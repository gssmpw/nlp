% Template for PLoS
% Version 3.6 Aug 2022
% Please do not include colors or graphics in the text.
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- eqS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your eqs and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline eqs, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display eqs when possible in order to fit size of the column. 
%
% For inline eqs, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}


\usepackage[numbers]{natbib}
\usepackage{booktabs}
\usepackage{multirow}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage[nopatch=eqnum]{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
\usepackage{setspace} 
\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}

\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Social media polarization during conflict: Insights from an ideological stance dataset on Israel-Palestine Reddit comments} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Hasin Jawad Ali\textsuperscript{1*},
Ajwad Abrar\textsuperscript{2},
S.M. Hozaifa Hossain\textsuperscript{3},
M. Firoz Mridha\textsuperscript{4}
\\
\bigskip
\textbf{1} Department of Business and Technology Management, Islamic University of Technology, Gazipur, Dhaka, Bangladesh
\\
\textbf{2} Department of Computer Science and Engineering, Islamic University of Technology, Gazipur, Dhaka, Bangladesh
\\
\textbf{3} Department of Electrical and Electronic Engineering, University of Dhaka, Nilkhet Road, Dhaka, Bangladesh
\\
\textbf{4} Department of Computer Science, American International University-Bangladesh, Kuratoli, Dhaka, Bangladesh
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.

% Current address notes
% \textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address



% Use the asterisk to denote corresponding authorship and provide email address in note below.
* hasinjawad@iut-dhaka.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
In politically sensitive scenarios like wars, social media serves as a platform for polarized discourse and expressions of strong ideological stances. While prior studies have explored ideological stance detection in general contexts, limited attention has been given to conflict-specific settings. This study addresses this gap by analyzing 9,969 Reddit comments related to the Israel-Palestine conflict, collected between October 2023 and August 2024. The comments were categorized into three stance classes: Pro-Israel, Pro-Palestine, and Neutral. Various approaches, including machine learning, pre-trained language models, neural networks, and prompt engineering strategies for open source large language models (LLMs), were employed to classify these stances. Performance was assessed using metrics such as accuracy, precision, recall, and F1-score. Among the tested methods, the Scoring and Reflective Re-read prompt in Mixtral 8x7B demonstrated the highest performance across all metrics. This study provides comparative insights into the effectiveness of different models for detecting ideological stances in highly polarized social media contexts. The dataset used in this research is publicly available for further exploration and validation.


% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
%%\section*{Author summary}
%%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

%%\begin{keyword}
 %%Ideological Stance Detection \sep Machine Learning \sep Large Language Models \sep Israel-Palestine Conflict \sep Social Media Analysis
%%\end{keyword}
% \linenumbers

% Use "equation" instead of "equation" for equation citations.
\section*{Introduction}
Ideological stances on the part of one or both parties can increase the likelihood of conflicts between two nations. If the stances are sufficiently strong, they even render mutually acceptable transfers insufficient to prevent war \citep{jackson2007political}. And when a war has already broken out, these stances come to surface extensively on social media. \citep{kubin2021role} states that social media reinforce these existing positions through the fragmentation of the news media and facilitate the spread of misinformation. In the context of ongoing Israel-Palestine conflict, social media is a hub for the exchange of opposing political ideologies. And due to the historical nature of this conflict, this manifestation is often highly polarized from both sides.

Numerous studies have been conducted in order to develop methodologies for identifying polarizing stances. Those studies have employed numerous machine learning algorithms, ranging from classical supervised algorithms to neural networks to pre-trained language models. Large Language Models (LLMs) have been used in sentiment detection of political texts \citep{kuila2024deciphering}. A study \citep{kwon2024sentiment} has also compared LLMs, neural networks, pre-trained language models and classical supervised algorithms in the context of ideological sentiment detection. Another study \citep{ansari2020analysis} compares LSTM and classical supervised algorithms in ideological stance detection. However, no studies have yet been conducted that use LLMs in ideological stance detection or compare LLMs to neural networks or pre-trained language models in this context. Moreover, no studies have also applied prompt engineering techniques in this context. 

To address this research gap, we present a comparative analysis of the performances of various open-access large language models (LLMs) incorporating multiple prompt engineering techniques, neural network architectures, and pre-trained language models in detecting ideological stance. We also propose the method that is identified as the best performing by the evaluation metrics. We specifically evaluated four open-access LLMs (Mixtral 8x7B, Mistral 7B, Gemma 7B and Falcon 7B), four neural network architectures (RNN, LSTM, GRU and BiLSTM), and seven pre-trained language models (BERT Cased, BERT Uncased, XLM-RoBERTa, DistilBERT Cased, DistilBERT Uncased, ELECTRA-Small and ELECTRA-Base). We utilized a manually annotated dataset of Reddit comments related to the Israel-Palestine conflict, spanning from October 2023 to August 2024. The models were evaluated based on accuracy, F1-score, recall, and precision on the test dataset. The final methodology was selected by systematically weighing these performance metrics. This comprehensive evaluation not only highlights the capabilities of various models in detecting ideological stance but also identifies the most suitable approach for this task. The contributions of this study are as follows:

\begin{itemize}
    \item A meticulously manually annotated, publicly available dataset containing the ideological stances of 9,969 Reddit comments in the context of the Israel-Palestine conflict.\footnote{The data is publicly available at \url{https://github.com/jami78/Conflict-Bias-Eval}}
    \item A comprehensive comparative analysis of multiple models for ideological stance detection, particularly in politically sensitive and charged scenarios.
    \item The establishment of benchmark models for this specific task, determined by evaluating and weighing various performance metrics.
\end{itemize}
\label{sec1}
The remainder of the paper is organized as follows:  Section \ref{rw} provides a comprehensive review of the existing literature; Section \ref{data} outlines the details of the dataset; Section \ref{method} demonstrates the proposed methodology including the models that were used; Section \ref{es} details the hardware setup and evaluation metrics used in our experiments; Section \ref{pe} presents the performance scores of the models; and finally, Section \ref{conclusion} provides a conclusion with a summary of the findings and a few potential future directions.


\section*{Related Work} \label{rw}
Ideological stance detection in our context refers to the identification of the underlying ideological viewpoint that a text might have conveyed in response to a particular political scenario. Traditionally, this domain used to leverage classical machine learning techniques until the emergence of neural networks and pre-trained language models significantly changed the landscape. More recently, large language models (LLMs) have advanced this field along with several prompt engineering strategies as they can capture textual nuances that the previous models failed to do. This section reviews the progression of methodologies in ideological stance detection, organized into three categories: neural networks, pre-trained language models, and state-of-the-art LLMs. The previous studies of stance detection in the context of the Israel-Palestine conflict are also discussed. 

\subsection*{\textbf{Neural Networks}}
Neural networks emerged as an improvement over the classical supervised model in the domain of ideological stance detection as it could capture patterns better. For example, \citep{ansari2020analysis} demonstrated that LSTM models outperform classical supervised models in ideological stance detection due to their ability to capture the temporal dependencies and context of tweets as observed in its performance on the Twitter dataset for 2019 Indian General Elections. According to \citep{khatua2020predicting}, Bi-LSTM is even better than LSTM, RNN, and SVM with a motion-dependent debate model for this particular task. When an RNN model is enhanced with word embeddings (W2V), it outperforms traditional Logistic Regression that uses Bag of Words (BoW) and word embeddings (W2V) \citep{iyyer2014political}. \citep{hamborg2021news} proposes a BiGRU model with multiple embeddings that outperformed previous state-of-the-art models for target-dependent sentiment classification in news articles. The model's success can be attributed to its ability to handle longer texts and multi-target sentences, offering a better solution for real-world sentiment analysis tasks in news media.

\subsection*{\textbf{Pre-trained Language Models}}
Pre-trained language models further improved performance over the neural networks by offering better handling of context. They have been used extensively over the years in ideological stance detection. According to \citep{ozturk2024ideology}, BERT demonstrates superior performance compared to classical supervised models, LSTM, and BiLSTM in this domain. However, \citep{abercrombie2020parlvote} reported that a supervised SVM classifier with a motion-dependent debate model outperformed the BERT+MLP model in detecting ideological sentiment in a large UK parliamentary debate corpus known as ParlVote. Furthermore, ELECTRA, another pre-trained language model, has been shown to outperform BERT in the detection of ideological stance \citep{ozturk2024ideology}. BERT, which utilizes transfer learning and logistic regression, has outperformed rule-based VADER and TextBlob in the detection of sentiment related to Sustainable Development Goals (SDGs) \citep{rosenberg2023sentiment}. BERT-based models have also been shown to be more socially conservative compared to GPT models, effecting their classification performance downstream. As a result, these models might exhibit certain biases, particularly in hate speech detection, unless they are pre-trained on left-leaning or right-leaning corpora \citep{feng2023from}. In a separate study, \citep{abrar2025religious} concluded that language models such as BERT, RoBERTa, and DistilBERT continue to exhibit significant religious biases. RoBERTa-Base has also performed better than LSTM, VADER, XLM-R, and SVM in monolingual ideological sentiment classification \citep{antypas2023negativity}. However, according to \citep{ferracane2023integrated}, a RoBERTa-based model specifically fine-tuned for classification in the political domain, named POLITICS, outperformed RoBERTa-Base in terms of detecting ideological stance by leveraging discourse-based, structure-aware representations. Another multi-task learning model named CLoSE, introduced in \citep{kim2022close}, was shown to outperform both RoBERTa and BERT due to its capability of embedding sentence framing language and predicting ideological stance simultaneously. The study \citep{bhatia2018topic} showed that the TSM (Topic-Sentiment Matrix) model performed comparably to state-of-the-art distributional text representation models such as GloVe-d2v, achieving an accuracy of 65.04\% compared to 64.30\% with GloVe-d2v. DistilBERT has shown better performance than ELMo in the classification of protest news and sentiment analysis in cross-context settings \citep{buyukoz2020analyzing}.

\subsection*{\textbf{Large Language Models (LLMs)}}
Large Language Models (LLMs) are the latest innovation in stance detection which are capable of understanding contexts and textual nuances better than any previous model. For example- LLMs showed better classification performance than BERT, CNN, traditional supervised algorithms and neural networks in ideological sentiment classification \citep{kwon2024sentiment}. Similarly, \citep{kuila2024deciphering} evaluated the performance of several open-access large language models (LLMs) in detecting ideological sentiments which identified Falcon 40B as the best-performing model, followed by Mistral 7B and Falcon 7B. However, LLMs are not without challenges. LLMs have been shown to have a left-leaning tendency, which might affect their classification accuracy \citep{hernandes2024llms, pit2023whose}. Additionally, their high computational requirements might restrict their efficiency. 

\subsection*{\textbf{Stance Detection in the Israel-Palestine Conflict}}
The Israel-Palestine war has instigated a few ideological stance detection studies, each with its own strengths and limitations. \citep{AlSarraj2018} investigated media stance in the coverage of the 2014 Israeli war on Gaza by Western news outlets. The study employed text mining techniques and supervised machine learning algorithms, such as SVM, to classify stances into binary categories of Pro-Palestine and Pro-Israel. The models showed high performance, achieving an accuracy and recall of approximately 90\% in classifying these ideological stances. However, this study did not consider neutrality as a stance category, which limits its capacity to capture balanced stances in politically sensitive content. On the other hand, \citep{imtiaz2022takingsidespublicopinion} incorporated neutrality as a stance category, broadening the range of perspectives considered. The study also achieved incredible model performance using XLNet. Nonetheless, it utilized a Twitter dataset, which may have limited the text's ability to capture nuanced expressions due to the 280-character limit. Additionally, the study did not include state-of-the-art Large Language Models (LLMs) for its analysis.

Building on this foundation, our work broadens the scope by integrating neural networks, pre-trained language models, and open source large language models (LLMs) with prompt engineering to analyze Reddit comments related to the Israel-Palestine conflict. Unlike traditional machine learning techniques, these advanced models enable a deeper understanding of contextual nuances, leading to the dynamic classification of ideological stance into three categories: Pro-Palestine, Pro-Israel, and Neutral.

\section*{Dataset} \label{data}
The dataset used in this study is a filtered version of the 'Daily Public Opinion on Israel-Palestine War' dataset \citep{asaniczka2024daily}, obtained from Kaggle. It contains comments from Reddit posts related to the ongoing Israel-Palestine war collected over a year from September 2023 to September 2024. This dataset has an advantage over previous datasets used for ideological stance detection because the context is set during a politically charged situation.
\subsection*{\textbf{Data Collection and Filtering}}
The original dataset comprises 1,825,545 comments collected from various subreddits. In this study, we filtered the dataset using specific keywords to ensure contextual relevance for accurate classification. This filtering process resulted in a refined subset of 9,969 comments focused on terms such as ``Hamas terrorist," ``Zionist terrorists," ``StandWithIsrael," ``FreePalestine," and ``Israel occupation," among others. Subsequently, duplicate comments were removed to ensure that the dataset consisted of unique and unambiguous entries.

\subsection*{\textbf{Data Annotation}}
After filtering the comments, they were manually annotated for sentiment by three independent annotators. Each comment was assigned one of the three ideological stance labels: Pro-Israel, Pro-Palestine, or Neutral. The final label for a comment was determined based on majority agreement among the annotators; if two or more annotators assigned the same label, it was accepted as the final label. Notably, no instances occurred in which all three annotators assigned different labels to the same comment.

The labeling process utilized specific keywords for categorization:
\begin{itemize}
    \item \textbf{Pro-Israel:} Comments containing keywords that expressed support for Israel and opposition to Palestine or Hamas, such as \textit{“Hamas terrorist,” “StandWithIsrael,” “Hamas terror,” “SupportIsrael,” “IsraelStrong,” “IsraelForever,” “IsraelUnderAttack”}.
    \item \textbf{Pro-Palestine:} Comments containing keywords that expressed support for Palestine and opposition to Israel, such as \textit{“Zionist terrorists,” “Zionist terror,” “FreePalestine,” “boycottisrael,” “Palestine will be free,” “Palestinewillbefree”}.
    \item \textbf{Neutral:} Comments that did not convey a strong sentiment towards either side were categorized as Neutral.
\end{itemize}

This systematic annotation process ensured that the dataset was accurately labeled for stance detection. This annotation process resulted in a total of 2,431 comments labeled Neutral, 4,947 comments labeled Pro-Israel, and 2,591 comments labeled Pro-Palestine. This balanced distribution of labels reflects a diverse range of stances and sentiments, ensuring a comprehensive dataset for analyzing ideological stance.

To measure inter-annotator agreement, we utilized Fleiss' Kappa \citep{fleiss1971measuring}. The calculated score for the three annotators is 0.93, indicating a high level of agreement.


\subsection*{\textbf{Dataset Statistics}}
As shown in Figure~\ref{fig:keywords}, \textit{“Hamas”} and \textit{“Israel”} emerge as the most frequently used keywords, surpassing others by a significant margin. The temporal distribution of comments (Figure~\ref{fig:time-distribution}) reveals that the number of comments is generally centered around 1,000 per month, with the highest volume observed in May 2024 and the lowest during the first and last two months of the dataset's time span.
\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{top-keywords.png}
\caption{Top 5 keywords within the comments}
\label{fig:keywords}
\end{figure}


\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{time-series.png}
\caption{Time Distribution of Comments}
\label{fig:time-distribution}
\end{figure} 

\subsection*{\textbf{Data Processing}} 
To prepare the dataset for model training and evaluation, a series of preprocessing steps were performed. These steps ensured that the data were clean, consistent, and in a format suitable for the application of machine learning and deep learning models. The following key techniques were employed during the data processing phase:

\begin{itemize}
    \item \textbf{Label Encoding:} Sentiment labels were converted into a numerical format for compatibility with neural networks: Neutral = 0, Pro-Palestine = 1, and Pro-Israel = 2.
    
    \item \textbf{Text Vectorization:} In order to make the data suitable for neural networks, multiple vectorization techniques were employed to transform text into numerical representations. These included Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), FastText, and Word2Vec. The most effective technique for each model type was experimentally determined to optimize performance.
    
    \item \textbf{Truncation:} To manage computational efficiency and memory usage, texts were truncated to a maximum length of 700 characters, aligning with recommendations for handling lengthy sequences in transformer-based models \citep{devlin2019bert}. Truncated texts were used in all models. 
    
\end{itemize}

\subsection*{\textbf{Dataset Split}}

The dataset was randomly divided into three subsets to facilitate model training, evaluation, and validation. Specifically, 6,978 samples (70\%) were allocated for training, 1,496 samples (15\%) for testing, and 1,495 samples (15\%) for validation. This 70:15:15 split ensured a balanced distribution of sentiment labels across all subsets, maintaining consistency for reliable performance assessment.

\section*{Methodology} \label{method}
Our study utilized various Neural Networks, pre-trained language models and open source Large Language Models (LLMs) to identify the ideological stances (Pro-Palestine, Pro-Israel, or Neutral) of Reddit comments. The following subsections provide a detailed description of the models used.
\subsection*{\textbf{Neural Network Architectures}}

Deep learning models, including the RNN, LSTM \citep{Sherstinsky_2020}, Bi-LSTM \citep{cui2019deepbidirectionalunidirectionallstm}, and GRU \citep{dey2017gru}, were trained and evaluated for the task. Among the various vectorization techniques explored, Bag of Words (BoW) has emerged as the most effective for these architectures. To enhance model performance, hyperparameter tuning was conducted, focusing on optimizing the configurations of the neural network architectures. Table \ref{tab:hyperparameter_ranges} lists the tested hyperparameters and their respective ranges.

\begin{table}[htbp]
\centering
\caption{Hyperparameter Ranges for Neural Network Architectures}
\label{tab:hyperparameter_ranges}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{@{}lccccc@{}}
\toprule
 & \multicolumn{5}{c}{\textbf{Hyperparameters}} \\ \cmidrule(lr){2-6}
       \textbf{Model}        & \textbf{Dropout Rate} & \textbf{Number of Layers} & \textbf{Units per Layer} & \textbf{Batch Size} & \textbf{Learning Rate} \\ \midrule
Bi-LSTM        & 0.2, 0.3, 0.5         & 1, 2, 3                   & 64, 128, 256             & 16, 32, 64          & 0.001, 0.01, 0.1       \\
LSTM           & 0.2, 0.3, 0.4         & 1, 2                      & 64, 128                  & 16, 32              & 0.001, 0.01            \\
Bi-GRU         & 0.2, 0.3, 0.5         & 1, 2, 3                   & 64, 128, 256             & 16, 32, 64          & 0.001, 0.01, 0.1       \\
RNN/GRU        & 0.2, 0.3, 0.5         & 1, 2, 3                   & 64, 128, 256             & 16, 32, 64          & 0.001, 0.01, 0.1       \\ \bottomrule
\end{tabular}
}
\end{table}
\subsection*{\textbf{Pre-trained Language Models}}
Pre-trained language models were fine-tuned on the dataset to create task-specific architectures that act as the mediator between traditional neural networks and large language models (LLMs). These models effectively capture text patterns and understand the context and relationships within a text. In addition, they are computationally more efficient than LLMs and can be easily adapted for specific tasks through fine-tuning. In our study, seven fine-tuned models were used to evaluate the task performance.
\begin{itemize}
    \item \textbf{BERT (Cased and Uncased):} BERT is a transformer-based model which is pre-trained using a masked language model objective in order to capture bidirectional context in text. The cased version preserves case sensitivity, whereas the uncased version ignores it \citep{devlin2019bert}.
    \item \textbf{XLM-RoBERTa:} XLM-RoBERTa extends RoBERTa to cross-lingual tasks by leveraging massive multilingual datasets that allow it to achieve enhanced performance on multilingual tasks ranging across 100 languages \citep{conneau2020xlmr}.
    \item \textbf{DistilBERT (Cased and Uncased):} This is the distilled version of BERT which offers faster processing speeds and reduced model size while retaining 97 \% of BERT's performance \citep{sanh2019distilbert}.
    \item \textbf{ELECTRA (Small and Base):} This model employs a unique token-replacement detection mechanism for efficient pre-training which enables the model to train faster and learn from smaller datasets \citep{clark2020electra}.
\end{itemize}
\subsection*{\textbf{Large Language Models}}
In politically sensitive datasets such as ours, nuanced language patterns and implicit stances often go unnoticed by traditional models. However, Large Language Models (LLMs) offer a promising solution, as they are pre-trained on extensive and diverse datasets, enabling them to generalize effectively to new contexts with minimal task-specific fine-tuning. Their ability to handle zero-shot, one-shot, and few-shot scenarios makes them particularly advantageous in low-resource settings where labeled data are limited. Additionally, LLMs are well-suited for processing longer and more complex narratives, which is especially relevant for our dataset comprising detailed Reddit comments with subtle political leanings. In this study, we conducted experiments using four Large Language Models, all accessed via the Huggingface platform.\footnote{Huggingface platform: \url{https://huggingface.co}}

\begin{itemize}
    \item \textbf{Mistral:} Mistral is a dense and efficient transformer-based language model with 7 billion parameters. It is trained on a mixture of high-quality datasets using techniques such as group-query attention and multi-query key-value caching for efficiency. It demonstrates advanced reasoning and comprehension capabilities, often outperforming Llama 2 13B on standard benchmarks \citep{jiang2023mistral7b}.
    \item \textbf{Mixtral:} Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) model based on Mistral 7B which uses 8 experts per layer while activating only 13B parameters at a time \citep{jiang2024mixtralexperts}.
    \item \textbf{Gemma:} Gemma is a family of lightweight open models based on the Gemini framework that incorporates multi-modal and multi-task training \citep{gemmateam2024gemmaopenmodelsbased}. It offers two parameter settings- 2 and 7 billions. We used the 7B parameter version in our study.
    \item \textbf{Falcon:} Falcon is a series of causal decoder-only models with 7B, 40B, and 180B parameters, trained on over 3.5 trillion tokens of high-quality web data \citep{Almazrouei2023TheFS}. The 7B version was utilized in our study.
\end{itemize}

\subsubsection*{\textbf{Prompting Strategies}}
We applied zero-shot, one-shot, three-shot, and five-shot prompting strategies \citep{brown2020languagemodelsfewshotlearners} to the truncated text column of the dataset to identify the underlying ideological stance in each entry. These strategies evaluated the models' abilities to handle varying levels of contextual information and infer classifications for the text.
\begin{itemize}

    \item \textbf{Zero-Shot Prompting:} In the zero-shot setting, the models were not provided with contextual information prior to the task. Instead, they were given only a task description and asked to generate an appropriate response. This approach evaluates the models' ability to generalize knowledge from their pre-training to this specific context. This strategy was applied to all LLMs. 
    
    \begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Zero-and-One-Shot.png}
    \caption{Zero and One Shot Prompting Demonstration}
    \label{fig:zero}
    \end{figure*} 
    
    \item \textbf{One-Shot Prompting:} One-shot prompting involves providing the model with a single example to guide its understanding of the task. This approach leverages the model's capacity to generalize from minimal guidance to improve task performance. This strategy was applied to all LLMs. Figure \ref{fig:zero} illustrates the application of zero and one shot prompting in our study.

    \item \textbf{Few-Shot Prompting:} Few-shot prompting involves providing the model with a small number of examples to help the model learn the task and generalize to new inputs. By offering multiple examples, this strategy allows the model to better understand the task context and the underlying relationships. In our experiments, we specifically used three-shot and five-shot prompting strategies to evaluate the model's performance with additional examples. These strategies were applied to all models except Mixtral, as the API limit on the Huggingface Hub restricted its use. Figure \ref{fig:three} illustrate the application of three-shot and five-shot prompting in our study.

    
    \begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Three-and-Five-Shots.png}
    \caption{Three and Five Shots Prompting Demonstration}
    \label{fig:three}
    \end{figure*}
    
\end{itemize}

\subsubsection*{\textbf{Novel Prompting Strategies}}
This section explores the impact of various novel prompting techniques on the model performance. We used five novel prompting strategies, consisting of Re-read Zero-Shot and One-Shot, Meta-Prompting, Scoring \& Reflective Re-read and Context Extraction. We evaluated these strategies only with Mixtral because its zero- and one-shot performance yielded the best results.
\begin{itemize}
    \item \textbf{Re2 or Re-read:} Re2 or re-read prompting strategy is designed to make the model check its classification and the text itself twice to reassess its initial output. This approach involves careful analysis and minimizes the likelihood of overlooking subtle contextual cues in its initial classification. We extended this approach to a one-shot setting where it reassesses its classification after being presented with an example during the initial classification. This addition combines the iterative strength of Re-read with the guidance provided in a one-shot setting. Figures \ref{fig:re2} and \ref{fig:re2-one} demonstrate the application of the Re-read prompting strategy in our context.
    \begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Re2-ZeroShot.png}
    \caption{Re-read Prompting Demonstration}
    \label{fig:re2}
    \end{figure*}
    \begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Re2-OneShot.png}
    \caption{Re-read-One Shot Prompting Demonstration}
    \label{fig:re2-one}
    \end{figure*}  
    \item \textbf{Meta-Prompting (Self-Critique):} Meta-Prompting, in the form of self-critique, refers to a two-step prompting strategy aimed at enhancing the model performance through a refinement process. The approach begins with an initial classification of the input text where it predicts a stance category. After that, the model is asked to critique its own classification and provide a refined final classification. The refined classification can be either the initial classification or a changed one. Figure \ref{fig:meta} demonstrates the use of Meta-Prompting (self-critique) in our context.
    \begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Meta-Prompt.png}
    \caption{Meta-Prompting (Self-Critique) Demonstration}
    \label{fig:meta}
    \end{figure*} 
    \item \textbf{Context Extraction:} The context extraction strategy was used to identify any contextual elements and embedded stances in the text. The prompt was formulated to be explicit yet open-ended, which may allow the model to effectively capture nuances without adhering to strict guidelines. The prompt allowed for the identification of the textual content first which might allow a better understanding of the potential stance present in it. By keeping the prompt simple and focused, we ensured that the model remained aligned with the primary task of extracting meaningful insights. Figure \ref{fig:context} demonstrates the use of Context Extraction in our context.
    \begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Context-Extraction.png}
    \caption{Context Extraction Demonstration}
    \label{fig:context}
    \end{figure*}

    \item \textbf{Scoring and Reflective Re-read:} The Scoring and reflective re-read strategy is a two-step prompting approach designed to encourage the model to assess the input text both qualitatively and quantitatively. The method begins with an initial scoring phase followed by a reflective re-read phase to refine the classification.
    In the initial scoring phase, the model was asked to assign numerical scores (ranging from 1 to 5) to three stance categories: Pro-Israel, Pro-Palestine, and Neutral. This step allows the model to understand the strength of alignment of the text with each category.
    Following the initial scoring phase, the model reevaluates the text and provides a classification based on the initial scores assigned. The scores serve as the primary metric, and the text allows for a further understanding of the context. This allows the model to safeguard against any contextual stances introduced in the first phase while also allowing the model to understand the relative strength of each category for the input text. Figure \ref{fig:score} demonstrates the use of Scoring and Reflective Re-read in our context.
    \begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Scoring+Re2.png}
    \caption{Scoring and Reflective Re-read Demonstration}
    \label{fig:score}
    \end{figure*}
\end{itemize}

\section*{Experimental Setup} \label{es}
\subsection*{\textbf{Hardware Details}}
The experiments were conducted on the Kaggle and Google Colab platform utilizing the free NVIDIA Tesla P100 and A100 GPU. This facilitated the training and evaluation of the models, particularly in handling computationally expensive processing.
\subsection*{\textbf{Evaluation Metrics}}
To evaluate the performance of the models implemented for sentiment classification, we employed four widely recognized metrics: Accuracy, Precision (Macro), Recall (Macro), and F1 Score (Macro). These metrics were chosen to ensure a comprehensive analysis, particularly given the multi-class nature of the task.

\textbf{Accuracy:} Accuracy refers to the proportion of correctly predicted instances among the total instances evaluated and serves as an overall indicator of a model's performance \citep{agustian2024new}.
\begin{equation}
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation} Where
\begin{itemize}
    \item TP: True Positives
    \item TN: True Negatives
    \item FP: False Positives
    \item FN: False Negatives
\end{itemize}
    In this study, accuracy provides a baseline understanding of model performance across the three stance categories \citep{hote2023open}.

\textbf{Precision (macro):} Precision evaluates the proportion of correctly predicted instances of a specific class relative to the total predicted instances for that class. Macro-averaging aggregates precision scores across all classes by assigning equal weights to each class, regardless of the number of instances \citep{cahya2024improving,shakith2024enhancing}.
\begin{equation}
\text{Macro Precision} = \frac{1}{N} \sum_{i=1}^{N} \frac{TP_i}{TP_i + FP_i}
\end{equation}
Where
\begin{itemize}
    \item N: Total number of classes
    \item \(TP_i\): True positives for the i-th class
    \item \(FP_i\): False positives for the i-th class
\end{itemize}
Precision (Macro) ensures that smaller classes, such as potentially underrepresented sentiments, are given equal importance during the evaluation \citep{hote2023open}.

\textbf{Recall (macro):} Recall, or sensitivity, measures the proportion of correctly identified instances of a class among all actual instances for that class \citep{cahya2024improving,shakith2024enhancing}.
\begin{equation}
\text{Macro Recall} = \frac{1}{N} \sum_{i=1}^{N} \frac{TP_i}{TP_i + FN_i}
\end{equation}

Where
\begin{itemize}
    \item \(FN_i\): False negatives for the i-th class
\end{itemize}
Recall (Macro) is particularly critical in this study to evaluate the model's ability to identify each sentiment class comprehensively, especially when class distributions are uneven \citep{hote2023open}.

\textbf{F1 Score (macro):} The F1 Score combines precision and recall into a single metric by calculating the harmonic mean. Macro F1 averages the F1 scores for all classes \citep{berger2020threshold}.

\begin{equation}
\text{Macro F1 Score} = \frac{2 \cdot \text{Macro Precision} \cdot \text{Macro Recall}}{\text{Macro Precision} + \text{Macro Recall}}
\end{equation}

Where
\begin{itemize}
    \item \(Precision_i\): Precision for the i-th class
    \item \(Recall_i\): Recall for the i-th class
\end{itemize}

Recall (Macro) is particularly critical in this study to evaluate the model's ability to identify each sentiment class comprehensively, especially when class distributions are uneven \citep{hote2023open}.

\section*{Performance Evaluation} \label{pe}
\subsection*{\textbf{Baseline Models}}

The classification performances of all models are listed in Table~\ref{tab:model_performance}. Neural networks, pre-trained language models, and some Large Language Models (LLMs) struggled to achieve an accuracy of 50\%.  Among the pre-trained language models, DistilBERT-Uncased achieved the highest accuracy of 56.95\% with respective macro precision, recall, and F1 scores of 57\%, 58\%, and 56\%. BiLSTM demonstrated the highest accuracy of 54\% among the neural network models while GRU had a greater macro-recall.

Among the LLMs, only Mixtral 8x7B and Gemma 7B crossed the 60\% accuracy threshold. The Mixtral 8x7B Zero Shot had higher accuracy, macro recall, and F1-score, while the Mixtral 8x7B One Shot had the highest macro precision.


\begin{table}[htbp]
\centering
\caption{Performance of models across different architectures. The bold-marked numbers indicate the highest values for each metric.}
\label{tab:model_performance}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Category} & \textbf{Model} & \textbf{Test Accuracy} & \textbf{Test Precision (macro)} & \textbf{Test Recall (macro)} & \textbf{Test F1 Score (macro)} \\ \midrule 
\multirow{7}{*}{\shortstack{\textbf{Pre-trained language} \\ \textbf{Models}}} 
    & BERT Cased          & 0.4345  & 0.4442  & 0.4476  & 0.4368  \\
    & BERT Uncased        & 0.5020  & 0.5017  & 0.4358  & 0.4264  \\
    & XLM-RoBERTa         & 0.5314  & 0.5521  & 0.5039  & 0.4452  \\
    & DistilBERT-Uncased  & 0.5695  & 0.5748  & 0.5842  & 0.5649  \\
    & DistilBERT-Cased    & 0.5448  & 0.5315  & 0.5483  & 0.5217  \\
    & ELECTRA-Small       & 0.4251  & 0.5078  & 0.4539  & 0.4236  \\
    & ELECTRA-Base        & 0.4051  & 0.4829  & 0.4715  & 0.4198  \\ \midrule 
\multirow{4}{*}{\shortstack{\textbf{Neural} \\ \textbf{Networks}}} 
    & RNN                 & 0.4993  & 0.1664 & 0.3333 & 0.2220 \\
    & LSTM                & 0.5354  & 0.3982 & 0.3949 & 0.3387 \\
    & GRU                 & 0.5394  & 0.3753 & 0.4159 & 0.3678 \\
    & BiLSTM              & 0.5414  & 0.4053 & 0.4041 & 0.3516 \\ \midrule
\multirow{16}{*}{\shortstack{\textbf{Large} \\ \textbf{Language} \\ \textbf{Models}}} 
    & Mixtral 8x7B Zero Shot  & \textbf{0.6830}  & 0.6612 & \textbf{0.6650} & \textbf{0.6628} \\
    & Mixtral 8x7B One Shot   & 0.6802  & \textbf{0.7169} & 0.6555 & 0.6482 \\
    & Mistral 7B Zero Shot    & 0.5207  & 0.6516 & 0.5488 & 0.5212 \\
    & Mistral 7B One Shot     & 0.5174  & 0.6442 & 0.5791 & 0.5109 \\
    & Mistral 7B Three Shots  & 0.5560  & 0.6779 & 0.5942 & 0.5673 \\
    & Mistral 7B Five Shots   & 0.5256  & 0.6625 & 0.5956 & 0.5526 \\
    & Gemma 7B Zero Shot         & 0.4739  & 0.4302 & 0.4344 & 0.4282 \\
    & Gemma 7B One Shot          & 0.6161  & 0.6161 & 0.6161 & 0.6161 \\
    & Gemma 7B Three Shots       & 0.5946  & 0.5432 & 0.5345 & 0.5173 \\
    & Gemma 7B Five Shots        & 0.6219  & 0.5814 & 0.5941 & 0.5901 \\
    & Falcon 7B Zero Shot     & 0.5319  & 0.5569 & 0.5525 & 0.5276 \\
    & Falcon 7B One Shot      & 0.4912  & 0.5549 & 0.5524 & 0.4912 \\
    & Falcon 7B Three Shots   & 0.5434  & 0.5511 & 0.5428 & 0.5288 \\
    & Falcon 7B Five Shots    & 0.5029  & 0.5979 & 0.5029 & 0.5229 \\ \bottomrule
\end{tabular}%
}
\end{table}

Table~\ref{tab:class_performance} lists the precision, recall, and F1 score of the best-performing Mixtral 7B Zero Shot. The model was observed to make a significant number of errors in classifying Neutral texts. It showed the best performance in classifying Pro-Israel texts, although the Pro-Palestine classifications were also substantially better than Neutral classifications.

\begin{table}[htbp]
\centering
\caption{Class-wise performance of Mixtral 7B Zero Shot.}
\label{tab:class_performance}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\ \midrule
Neutral & 0.4934 & 0.5068 & 0.5000 \\ 
Pro-Israel & 0.7975 & 0.7621 & 0.7794 \\ 
Pro-Palestine & 0.6928 & 0.7260 & 0.7090 \\ \bottomrule
\end{tabular}
\end{table}


Considering the performance scores of Mixtral 8x7B, we tested some prompt engineering techniques on the model to improve its base performance.

\subsection*{\textbf{Novel Methods}}
Table~\ref{tab:ablation_performance} presents the results of the novel prompt engineering methods used. Among them, Scoring \& Reflective Re-read achieved the highest performance with an accuracy of 74.08\%, macro precision of 73.25\%, macro recall of 73.09\%, and a macro F1 score of 72. 93\%. Although the one-shot re-read had an accuracy close to 70\%, it fell short of the Scoring \& Reflective Re-read across all metrics.


\begin{table}[htbp]
\centering
\caption{Performance of Mixtral 7B with different prompt engineering techniques.}
\label{tab:ablation_performance}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision (Macro)} & \textbf{Recall (Macro)} & \textbf{F1-Score (Macro)} \\ \midrule
Mixtral 8x7B - Re2 & 0.6374 & 0.7246 & 0.6634 & 0.6432 \\ 
Mixtral 8x7B - Re2 (One-Shot) & 0.6965 & 0.7144 & 0.6880 & 0.6896 \\ 
Mixtral 8x7B Meta-Prompting (Self-Critique) & 0.6357 & 0.6086 & 0.6138 & 0.6101 \\ 
Mixtral 8x7B Scoring + Re2 & 0.7408 & 0.7325 & 0.7309 & 0.7293 \\ 
Mixtral 8x7B (Context Extraction) & 0.6094 & 0.5969 & 0.6000 & 0.5971 \\ \bottomrule
\end{tabular}%
}
\end{table}


Table~\ref{tab:class_performance_proposed} lists the class-wise performance of the Scoring \& Reflective Re-read method. We can observe that the classification performance has increased significantly for all classes compared to the base zero-shot model. The most significant improvements were observed in classifying the neutral classes, where the most error occurred previously. These improvements suggest that the Scoring and Reflective Re-read achieves the best possible outcome out of all tested methods, thus making it our proposed method based on those experimental results. 


\begin{table}[htbp]
\centering
\caption{Class-wise performance of the proposed method.}
\label{tab:class_performance_proposed}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\ \midrule
Neutral & 0.5637 & 0.6769 & 0.6151 \\ 
Pro-Israel & 0.8460 & 0.7847 & 0.8142 \\ 
Pro-Palestine & 0.7879 & 0.7312 & 0.7585 \\ \bottomrule
\end{tabular}
\end{table}

\section*{Conclusion} \label{conclusion}
This study provides a comprehensive analysis of ideological stance detection in social media data in the context of the Israel-Palestine conflict. By leveraging a unique manually annotated dataset of Reddit comments, we compared the performances of neural networks, pre-trained language models, and large language models (LLMs) using various evaluation metrics. The findings reveal the nuanced nature of ideological sentiment in conflict zones and underscore the importance of advanced models and prompt engineering techniques to achieve higher classification accuracy.

Among the models tested, Mixtral 8x7B with Scoring and Reflective Re-read prompting strategies demonstrated superior performance, achieving significant improvements in accuracy, precision, recall, and F1-scores across all stance categories. This research highlights the limitations of traditional models in capturing subtle and often implicit stances in politically sensitive datasets, and demonstrates the potential of novel prompting techniques to enhance classification outcomes.

Future work can build upon this foundation by exploring multilingual datasets, integrating cross-cultural stances, and evaluating the ethical implications of ideological stance detection in highly polarized scenarios. The proposed methodologies and findings contribute to the broader discourse on the role of machine learning and natural language processing in understanding and addressing social media polarization in conflict zones.

\section*{Author Contributions}
\textbf{Conceptualization: } Hasin Jawad Ali. \newline
\textbf{Methodology: } Hasin Jawad Ali, S.M. Hozaifa Hossain. \newline
\textbf{Data Curation: } Hasin Jawad Ali, S.M. Hozaifa Hossain, Ajwad Abrar. \newline
\textbf{Formal analysis: } Hasin Jawad Ali, S.M. Hozaifa Hossain.\newline
\textbf{Writing – Original Draft Preparation: } Hasin Jawad Ali, S.M. Hozaifa Hossain.\newline
\textbf{Writing – Review \& Editing: } Ajwad Abrar. \newline
\textbf{Visualization: } Hasin Jawad Ali. \newline
\textbf{Validation: } Ajwad Abrar, M. Firoz Mridha. \newline
\textbf{Supervision: } Ajwad Abrar, M. Firoz Mridha. \newline
\textbf{Project Administration:} Ajwad Abrar, M. Firoz Mridha. \newline
\textbf{Investigation: } M. Firoz Mridha. \newline


\section*{Declaration of competing interest}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\section*{Acknowledgements}
The authors would like to thank Md Farhan Ishmam for his encouragement and support.


\bibliographystyle{plos2015}
\bibliography{references}



\end{document}

