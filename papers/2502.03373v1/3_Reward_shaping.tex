

\section{Impact of Reward Design on Long CoT}\label{sec:reward-design}

This section examines reward function design, with a focus on its influence on CoT length and model performance.

\subsection{CoT Length Stability}
\label{result:reward-length-stability}

Recent studies on long CoT \cite{deepseekai2025r1, kimi2025k15, hou2025advancinglanguagemodelreasoning} suggest that models naturally improve in reasoning tasks with increased thinking time. Our experiments confirm that models fine-tuned on long CoT distilled from \texttt{QwQ-32B-Preview} tend to extend CoT length under RL training, albeit sometimes unstably. This instability, also noted by \citet{kimi2025k15, hou2025advancinglanguagemodelreasoning}, has been addressed using techniques based on length and repetition penalties to stabilize training.

 \noindent\textbf{Setup.} We used two different models fine-tuned on long CoT data distilled from \texttt{QwQ-32B-Preview} using the MATH train split, with a context window size of 16K. The models were \texttt{Llama3.1-8B} and \texttt{Qwen2.5-Math-7B}. We used a rule-based verifier along and a simple reward of 1 for correct answers. We shall refer to this as the \textit{Classic Reward}. More details can be found in Appendix \ref{app:exp-hyperparams-reward-length-stability}.

\noindent\textbf{Results.} We observed that both models increased their CoT length during training, eventually reaching the context window limit. This led to a decline in training accuracy due to CoTs exceeding the allowable window size. Additionally, different base models exhibited distinct scaling behaviors. The weaker \texttt{Llama-3.1-8B} model showed greater fluctuations in CoT length compared to \texttt{Qwen-2.5-Math-7B}, as illustrated in Figure \ref{fig:reward-qwen-llama}.

We also found that the rate at which CoTs exceeded the context window size leveled off at a certain threshold below 1 (Figure \ref{fig:reward-qwen-llama}). This suggests that exceeding the limit started to apply significant downward pressure on the CoT length distribution, and highlights the context window size's role in implicit length penalization. Notably, a trajectory might be penalized even without an explicit exceed-length penalty due to reward or advantage normalization, both of which are standard in RL frameworks.

\begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{result:reward-length-stability} for CoT Length Stability}
CoT length does not always scale up in a stable fashion. (Figure \ref{fig:reward-qwen-llama}) 
\end{AIbox}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/reward_functions_row.pdf}
    \vspace{-20pt}
    \caption{The Classic and Cosine Reward functions. The Cosine Reward varies with generation length.}
    \vspace{-10pt}
    \label{fig:reward-cosine}
\end{figure}

\subsection{Active Scaling of CoT Length}
\label{result:reward-length-scaling}

We found that reward shaping can be used to stabilize emergent length scaling. We designed a reward function to use CoT length as an additional input and to observe a few ordering constraints. Firstly, correct CoTs receive higher rewards than wrong CoTs. Secondly, shorter correct CoTs receive higher rewards than longer correct CoTs, which incentivizes the model to use inference compute efficiently. Thirdly, shorter wrong CoTs should receive higher penalties than longer wrong CoTs. This encourages the model to extend its thinking time if it is less likely to get the correct answer.

We found it convenient to use a piecewise cosine function, which is easy to tune and smooth. We refer to this reward function as the \textit{Cosine Reward}, visualized in Figure \ref{fig:reward-cosine}. This is a \textit{sparse} reward, only awarded once at the end of the CoT based on the correctness of the answer. The formula of \textbf{CosFn} can be found in equation \ref{eqn:cosine-lr} in the appendix.

\vspace{-10pt}
\begin{equation*}
\small
\label{eqn:reward-cosine}
\begin{aligned}
& R(C, L_{\text{gen}}) = 
&\begin{cases} 
    \text{CosFn}(L_{\text{gen}}, L_{\text{max}}, r_0^c, r_L^c),  & \text{if } C = 1, \\
    \text{CosFn} (L_{\text{gen}}, L_{\text{max}}, r_0^w, r_L^w),  & \text{if } C = 0, \\
    r_e, &\text{if } L_{\text{gen}} = L_{\text{max}}.
\end{cases}
\end{aligned}
\end{equation*}
\vspace{-25pt}

{\small
\begin{flalign*}
&\textbf{Hyperparameters:} && \\
&\quad r_0^c / r_0^w: \text{Reward (correct/wrong) for } L_{\text{gen}} = 0, && \\
&\quad r_L^c/r_L^w: \text{Reward (correct/wrong) for } L_{\text{gen}} = L_{\text{max}}, \\
&\quad r_e: \text{Exceed length penalty}, \\
&\textbf{Inputs:} && \\
&\quad C: \text{Correctness (0 or 1)}, && \\
&\quad L_{\text{gen}}: \text{Generation length.} &&
\end{flalign*}
}

\noindent\textbf{Setup.} We ran experiments with the Classic Reward and the Cosine Reward. We used the \texttt{Llama3.1-8B} fine-tuned on long CoT data distilled from \texttt{QwQ-32B-Preview} using the MATH train split, as our starting point. For more details, see Appendix \ref{app:exp-hyperparams-reward-length-scaling}.

\noindent\textbf{Result.} We found that the Cosine Reward significantly stabilized the length scaling behavior of the models under RL, thereby also stabilizing the training accuracy and improving RL efficiency (Figure \ref{fig:reward-llama-classic}). We also observed improvements in model performance on downstream tasks (Figure \ref{fig:reward-eval}).



\begin{figure}[tbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/llama_classic_acc_res_len.pdf}
    \vspace{-20pt}
    \caption{\texttt{Llama3.1-8B} trained with length shaping using the Cosine Reward exhibited more stable (a) training accuracy and (b) response length. This stability led to improved performance on downstream tasks (Figure \ref{fig:reward-eval}). Red points on the charts indicate iterations where training accuracy dropped to near zero.}
    \vspace{-10pt}
    \label{fig:reward-llama-classic}
\end{figure}

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/train-proc-rew-type.pdf}
    \vspace{-20pt}
    \caption{Performance of \texttt{Llama-3.1-8B} trained with different reward functions on a variety of evaluation benchmarks.}
    \vspace{-10pt}
    \label{fig:reward-eval}
\end{figure*}

\begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{result:reward-length-scaling} for Active Scaling of CoT Length}
Reward shaping can be used to stabilize and control CoT length while improving accuracy. (Figure \ref{fig:reward-llama-classic}, \ref{fig:reward-eval})
\end{AIbox}

\subsection{Cosine Reward Hyperparameters}
\label{result:reward-cosine-hyperparams}

The Cosine Reward hyperparameters can be tuned to shape CoT length in different ways.

\noindent\textbf{Setup.} We set up RL experiments with the same model fine-tuned on long CoT distilled from \texttt{QwQ-32B-Preview}, but with different hyperparameters for the Cosine Reward function. We tweaked the correct and wrong rewards $r_0^c, r_L^c, r_0^w, r_L^w$ and observed their impact on the CoT lengths. For more details, see Appendix \ref{app:exp-hyperparams-reward-cosine-hyperparams}.

\noindent\textbf{Result.} We see from Figure \ref{fig:reward-cosine-hyperparams} in the Appendix that if the reward for a correct answer increases with CoT length ($r_0^c < r_L^c$), the CoT length increases explosively. We also see that the lower the correct reward relative to the wrong reward, the longer the CoT length. We interpret this as a kind of trained risk aversion, where the ratio of the correct and wrong rewards determines how confident the model has to be about an answer for it to derive a positive expected value from terminating its CoT with an answer.

\begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{result:reward-cosine-hyperparams} for Cosine Reward Hyperparameters}
Cosine Reward can be tuned to incentivize various length scaling behaviors. (Appendix Figure \ref{fig:reward-cosine-hyperparams})
\end{AIbox}

\subsection{Context Window Size}
\label{result:reward-context-window}

We know that longer contexts give a model more room to explore, and with more training samples, the model eventually learns to utilize more of the context window. This raises an interesting question -- are more training samples necessary to learn to utilize a larger context window?

\noindent\textbf{Setup.} We set up 3 experiments using the same starting model fine-tuned on long CoT data distilled from \texttt{QwQ-32B-Preview} with the MATH train split. We also used the latter as our RL prompt set. Each ablation used the Cosine Reward and repetition penalty with a different context window size (4K, 8K, and 16K). For more details, see Appendix \ref{app:exp-hyperparams-reward-context-window}.

\noindent\textbf{Result.} We found that the model with a context window size of 8K performed better than the model with 4K, as expected. However, we observed performance was better under 8K than 16K. Note that all three experiments used the same number of training samples (Figure \ref{fig:reward-context}). We see this as an indication that models need more training compute to learn to fully utilize longer context window sizes, which is consistent with the findings of \cite{hou2025advancinglanguagemodelreasoning}.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/viz-ctx-len-scaling.pdf}
    \vspace{-20pt}
    \caption{Performance of \texttt{Llama-3.1-8B} trained with different context window sizes. All experiments used the same number of training samples.}
    \vspace{-10pt}
    \label{fig:reward-context}
\end{figure*}

\begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{result:reward-context-window} for Context Window Size}
Models might need more training samples to learn to utilize larger context window sizes. (Figure \ref{fig:reward-context})
\end{AIbox}

\subsection{Length Reward Hacking}
\label{result:reward-hacking}

We observed that with enough training compute, the model started to show signs of reward hacking, where it increased the lengths of its CoTs on hard questions using repetition rather than learning to solve them. We also noted a fall in the branching frequency of the model, which we estimated by counting the number of times the pivot keyword "\texttt{alternatively,}" appeared in the CoT (Figure \ref{fig:reward-cosine-branching}).

We mitigated this by implementing a simple $N$-gram repetition penalty (Algorithm \ref{alg:reward-repetition-penalty}). We observed that the penalty was most effectively applied on repeated tokens, rather than as a sparse reward for the entire trajectory. Similarly, we found that discounting the repetition penalty when calculating the return was effective. Specific feedback about where the repetition occurred presumably made it easier for the model to learn not to do it (see more in \textsection\ref{result:optimal-discount}).

\noindent\textbf{Setup.} We used the \texttt{Llama3.1-8B} model fine-tuned on long CoT data distilled from \texttt{QwQ-32B-Preview}. We ran two RL training runs, both using the Cosine Reward, but with and without the repetition penalty. For more details, please refer to Appendix \ref{app:exp-hyperparams-reward-hacking}.

\noindent\textbf{Result.} The repetition penalty resulted in better downstream task performance and also shorter CoTs, meaning there was better utilization of inference compute (Figure \ref{fig:reward-eval}).

\textbf{Observation.} Our experiments revealed a relationship between the repetition penalty, training accuracy, and the Cosine Reward. When training accuracy was low, the Cosine Reward exerted greater upward pressure on CoT length, leading to increased reward hacking through repetition. This, in turn, required a stronger repetition penalty. Future work could further investigate these interactions and explore dynamic tuning methods for better optimization.

\begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{result:reward-hacking} for Length Reward Hacking}
Length rewards will be hacked with enough compute (Figure \ref{fig:reward-cosine-branching}), but this can be mitigated using a repetition penalty. (Figure \ref{fig:reward-eval})
\end{AIbox}

\subsection{Optimal Discount Factors}
\label{result:optimal-discount}

We hypothesized that applying the repetition penalty with temporal locality (i.e., a low discount factor) would be most effective, as it provides a stronger learning signal about the specific offending tokens. However, we also observed performance degradation when the discount factor for the correctness (cosine) reward was too low.

To optimally tune both reward types, we modified the GAE formula in PPO to accommodate multiple reward types, each with its own discount factor $\gamma$: $\hat{A}_t = \sum_{l=0}^{L} \sum_{m}^{M}\gamma_{m}^l r_{m, t + l} - V(s_t)$. For simplicity, we set $\lambda = 1$, which proved effective, though we did not extensively tune this parameter.

\noindent\textbf{Setup.} We ran multiple RL experiments with the same \texttt{Llama3.1-8B} model fine-tuned on \texttt{QwQ-32B-Preview} distilled long CoT data. We used the Cosine Reward and repetition penalty but with different combinations of discount factors. For more details, please see Appendix \ref{app:exp-hyperparams-optimal-discount}.

\noindent\textbf{Result.} A lower discount factor effectively enforces the repetition penalty, whereas a higher discount factor enhances the correctness reward and the exceed-length penalty. The higher factor allows the model to be adequately rewarded for selecting a correct answer earlier in the CoT (Figure \ref{fig:multiple-gamma}).

We observed a rather interesting phenomenon where decreasing the discount factor $\gamma$ of the correctness (cosine) reward increased the branching frequency in the model's CoT, making the model quickly give up on approaches that did not seem to lead to a correct answer immediately (Figure \ref{fig:reward-indecisive}, Extract in Appendix \ref{extract:reward-short-term}). We hypothesize that this short-term thinking was due to a relatively small number of tokens preceding the correct answer receiving rewards, which means stepping stones to the right answer are undervalued. Such behavior degraded performance (Figure \ref{fig:multiple-gamma}). However, we think this qualitative result might be of potential interest to the research community, due to its similarity to the relationship between behaviors like delayed gratification and the distribution of rewards given to the biological brain \cite{doi:10.1126/sciadv.abg6611}.



\begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{result:optimal-discount} for Optimal Discount Factors}
Different kinds of rewards and penalties have different optimal discount factors. (Figure \ref{fig:multiple-gamma})
\end{AIbox}

