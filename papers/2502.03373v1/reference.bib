# Adapted from DART-Math
@misc{alpaca,
  author       = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
  journal      = {GitHub repository},
  publisher    = {GitHub},
  title        = {Stanford Alpaca: An Instruction-following LLaMA model},
  year         = {2023}
}
@inproceedings{ansel2024pytorch,
  author    = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and others},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages     = {929--947},
  title     = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
  year      = {2024}
}

@misc{anthropic2023claude,
  author  = {Anthropic},
  title   = {Introducing Claude},
  url     = {https://www.anthropic.com/index/introducing-claude},
  urldate = {2023-05-31},
  year    = 2023
}


@inproceedings{azerbayev2024llemma,
  author    = {Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen Marcus McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {Llemma: An Open Language Model for Mathematics},
  year      = {2024}
}

@article{bai2022hhrlhf,
  author  = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal = {arXiv preprint arXiv:2204.05862},
  title   = {Training a helpful and harmless assistant with reinforcement learning from human feedback},
  year    = {2022}
}

@article{brown2020language,
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  pages   = {1877--1901},
  title   = {Language models are few-shot learners},
  volume  = {33},
  year    = {2020}
}

@article{bubeck2023sparks,
  author  = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal = {arXiv preprint arXiv:2303.12712},
  title   = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  year    = {2023}
}



@article{burns2023weak,
  author  = {Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal = {arXiv preprint arXiv:2312.09390},
  title   = {Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision},
  year    = {2023}
}

@article{cao2023instructionmining,
  author  = {Cao, Yihan and Kang, Yanbin and Wang, Chi and Sun, Lichao},
  journal = {arXiv preprint arXiv:2307.06290},
  title   = {Instruction Mining: When Data Mining Meets Large Language Model Finetuning},
  year    = {2023}
}

@article{chen2016gradcheckpoint,
  author  = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal = {arXiv preprint arXiv:1604.06174},
  title   = {Training Deep Nets with Sublinear Memory Cost},
  year    = {2016}
}

@inproceedings{chen2023theoremqa,
  author    = {Wenhu Chen and Ming Yin and Max Ku and Pan Lu and Yixin Wan and Xueguang Ma and Jianyu Xu and Xinyi Wang and Tony Xia},
  booktitle = {The 2023 Conference on Empirical Methods in Natural Language Processing},
  title     = {Theorem{QA}: A Theorem-driven Question Answering Dataset},
  year      = {2023}
}

@inproceedings{chen2024alpagasus,
  author    = {Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {AlpaGasus: Training a Better Alpaca with Fewer Data},
  year      = {2024}
}

@article{chen2024alphamath,
  author  = {Chen, Guoxin and Liao, Minpeng and Li, Chengxi and Fan, Kai},
  journal = {arXiv preprint arXiv:2405.03553},
  title   = {AlphaMath Almost Zero: process Supervision without process},
  year    = {2024}
}

@article{chowdhery2023palm,
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal = {Journal of Machine Learning Research},
  number  = {240},
  pages   = {1--113},
  title   = {Palm: Scaling language modeling with pathways},
  volume  = {24},
  year    = {2023}
}

@article{cobbe2021gsm8k,
  author  = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal = {arXiv preprint arXiv:2110.14168},
  title   = {Training Verifiers to Solve Math Word Problems},
  year    = {2021}
}
@article{cobbe2021training,
  author  = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal = {arXiv preprint arXiv:2110.14168},
  title   = {Training verifiers to solve math word problems},
  year    = {2021}
}
@article{dong2023raft,
  author  = {Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and KaShun, SHUM and Zhang, Tong},
  journal = {Transactions on Machine Learning Research},
  title   = {RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment},
  year    = {2023}
}

@inproceedings{flan,
  abstract  = {We study the design decision of publicly available instruction tuning methods, by reproducing and breaking down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17% across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, chain-of-thought) actually yields equivalent or stronger (2%) performance in all settings. In further experiments we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks â€“ motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available.},
  author    = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and Roberts, Adam},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = {23--29 Jul},
  pages     = {22631--22648},
  pdf       = {https://proceedings.mlr.press/v202/longpre23a/longpre23a.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  volume    = {202},
  year      = {2023}
}

@inproceedings{fu2023complexity,
  author    = {Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
  booktitle = {The Eleventh International Conference on Learning Representations },
  title     = {Complexity-Based Prompting for Multi-step Reasoning},
  year      = {2023}
}

@article{google2024gemini,
  author  = {Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal = {arXiv preprint arXiv:2403.05530},
  title   = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  year    = {2024}
}

@inproceedings{gou2024tora,
  author    = {Zhibin Gou and Zhihong Shao and Yeyun Gong and yelong shen and Yujiu Yang and Minlie Huang and Nan Duan and Weizhu Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {To{RA}: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving},
  year      = {2024}
}

@article{gulcehre2023rest,
  author  = {Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal = {arXiv preprint arXiv:2308.08998},
  title   = {Reinforced self-training (rest) for language modeling},
  year    = {2023}
}

@inproceedings{hao2023rap,
  abstract  = {Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs{'} absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monte Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, properly balancing exploration v.s. exploitation to achieve a high-reward reasoning path efficiently. We apply RAP to a variety of challenging reasoning problems, such as plan generation, math reasoning, and logical inference. Empirical results demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency, e.g., RAP on LLaMA-33B surpasses CoT on GPT-4 with 33{\%} relative improvement in plan generation.},
  address   = {Singapore},
  author    = {Hao, Shibo  and
               Gu, Yi  and
               Ma, Haodi  and
               Hong, Joshua  and
               Wang, Zhen  and
               Wang, Daisy  and
               Hu, Zhiting},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  doi       = {10.18653/v1/2023.emnlp-main.507},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  month     = dec,
  pages     = {8154--8173},
  publisher = {Association for Computational Linguistics},
  title     = {Reasoning with Language Model is Planning with World Model},
  year      = {2023}
}

@article{he2024olympiadbench,
  author  = {He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
  journal = {arXiv preprint arXiv:2402.14008},
  title   = {OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems},
  year    = {2024}
}

@inproceedings{hendrycks2021math,
  author    = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  editor    = {J. Vanschoren and S. Yeung},
  pages     = {},
  title     = {Measuring Mathematical Problem Solving With the MATH Dataset},
  volume    = {1},
  year      = {2021}
}

@article{hosseini2024vstar,
  author  = {Hosseini, Arian and Yuan, Xingdi and Malkin, Nikolay and Courville, Aaron and Sordoni, Alessandro and Agarwal, Rishabh},
  journal = {arXiv preprint arXiv:2402.06457},
  title   = {V-STaR: Training Verifiers for Self-Taught Reasoners},
  year    = {2024}
}

@article{huang2022selfimprove,
  author  = {Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal = {arXiv preprint arXiv:2210.11610},
  title   = {Large language models can self-improve},
  year    = {2022}
}

@article{huang2024kpmath,
  author  = {Huang, Yiming and Liu, Xiao and Gong, Yeyun and Gou, Zhibin and Shen, Yelong and Duan, Nan and Chen, Weizhu},
  journal = {arXiv preprint arXiv:2403.02333},
  title   = {Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning},
  year    = {2024}
}

@misc{jiang2023mistral,
  archiveprefix = {arXiv},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and LÃ©lio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and TimothÃ©e Lacroix and William El Sayed},
  eprint        = {2310.06825},
  primaryclass  = {cs.CL},
  title         = {Mistral 7B},
  year          = {2023}
}


@misc{jiang2024mixtral,
  archiveprefix = {arXiv},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and LÃ©lio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and ThÃ©ophile Gervet and Thibaut Lavril and Thomas Wang and TimothÃ©e Lacroix and William El Sayed},
  eprint        = {2401.04088},
  primaryclass  = {cs.LG},
  title         = {Mixtral of Experts},
  year          = {2024}
}

@inproceedings{jimenez2024swebench,
  author    = {Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
  year      = {2024}
}

@article{kalamkar2019bf16,
  author  = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and others},
  journal = {arXiv preprint arXiv:1905.12322},
  title   = {A Study of BFLOAT16 for Deep Learning Training},
  year    = {2019}
}

@article{kaplan2020scaling,
  author  = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal = {arXiv preprint arXiv:2001.08361},
  title   = {Scaling laws for neural language models},
  year    = {2020}
}

@article{krell2021sequence,
  author  = {Krell, Mario Michael and Kosec, Matej and Perez, Sergio P and Fitzgibbon, Andrew},
  journal = {arXiv preprint arXiv:2107.02027},
  title   = {Efficient Sequence Packing without Cross-contamination: Accelerating Large Language Models without Impacting Performance},
  year    = {2021}
}

@inproceedings{kwon2023vllm,
  author    = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  pages     = {611--626},
  title     = {Efficient memory management for large language model serving with pagedattention},
  year      = {2023}
}

@inproceedings{lewkowycz2022solving,
  author    = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and Wu, Yuhuai and Neyshabur, Behnam and Gur-Ari, Guy and Misra, Vedant},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {3843--3857},
  publisher = {Curran Associates, Inc.},
  title     = {Solving Quantitative Reasoning Problems with Language Models},
  volume    = {35},
  year      = {2022}
}


@article{li2023ifd,
  author  = {Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing},
  journal = {arXiv preprint arXiv:2308.12032},
  title   = {From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning},
  year    = {2023}
}

@article{li2023nuggets,
  author  = {Li, Yunshui and Hui, Binyuan and Xia, Xiaobo and Yang, Jiaxi and Yang, Min and Zhang, Lei and Si, Shuzheng and Liu, Junhao and Liu, Tongliang and Huang, Fei and others},
  journal = {arXiv preprint arXiv:2312.10302},
  title   = {One shot learning as instruction data prospector for large language models},
  year    = {2023}
}

@article{li2024common,
  author  = {Li, Chen and Wang, Weiqi and Hu, Jingcheng and Wei, Yixuan and Zheng, Nanning and Hu, Han and Zhang, Zheng and Peng, Houwen},
  journal = {arXiv preprint arXiv:2403.04706},
  title   = {Common 7B Language Models Already Possess Strong Math Capabilities},
  year    = {2024}
}

@article{li2024gsm,
  author  = {Li, Qintong and Cui, Leyang and Zhao, Xueliang and Kong, Lingpeng and Bi, Wei},
  journal = {arXiv preprint arXiv:2402.19255},
  title   = {GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers},
  year    = {2024}
}

@misc{liao2024mario,
  archiveprefix = {arXiv},
  author        = {Minpeng Liao and Wei Luo and Chengxi Li and Jing Wu and Kai Fan},
  eprint        = {2401.08190},
  primaryclass  = {cs.CL},
  title         = {MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline},
  year          = {2024}
}

@inproceedings{lightman2023verifystep,
  author    = {Hunter Lightman and Vineet Kosaraju and Yuri Burda and Harrison Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {Let's Verify Step by Step},
  year      = {2024}
}

@inproceedings{lin2018gradacc,
  author    = {Yujun Lin and Song Han and Huizi Mao and Yu Wang and Bill Dally},
  booktitle = {International Conference on Learning Representations},
  title     = {Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},
  year      = {2018}
}


@article{lin2024criticbench,
  author  = {Lin, Zicheng and Gou, Zhibin and Liang, Tian and Luo, Ruilin and Liu, Haowei and Yang, Yujiu},
  journal = {arXiv preprint arXiv:2402.14809},
  title   = {CriticBench: Benchmarking LLMs for Critique-Correct Reasoning},
  year    = {2024}
}
@inproceedings{liu2024deita,
  author    = {Wei Liu and Weihao Zeng and Keqing He and Yong Jiang and Junxian He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning},
  year      = {2024}
}

@misc{liu2024mmiqc,
  archiveprefix = {arXiv},
  author        = {Haoxiong Liu and Yifan Zhang and Yifan Luo and Andrew Chi-Chih Yao},
  eprint        = {2401.09003},
  primaryclass  = {cs.CL},
  title         = {Augmenting Math Word Problems via Iterative Question Composing},
  year          = {2024}
}

@inproceedings{lu2024instag,
  author    = {Keming Lu and Hongyi Yuan and Zheng Yuan and Runji Lin and Junyang Lin and Chuanqi Tan and Chang Zhou and Jingren Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {\#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models},
  year      = {2024}
}

@article{luo2023wizardmath,
  author  = {Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal = {arXiv preprint arXiv:2308.09583},
  title   = {WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct},
  year    = {2023}
}

@misc{meta2023llama3,
  author = {Meta},
  title  = {Introducing meta llama 3: The most capable openly available llm to date.},
  url    = {https://ai.meta.com/blog/meta-llama-3},
  year   = 2024
}

@article{meurer2017sympy,
  author    = {Meurer, Aaron and Smith, Christopher P and Paprocki, Mateusz and {\v{C}}ert{\'\i}k, Ond{\v{r}}ej and Kirpichev, Sergey B and Rocklin, Matthew and Kumar, AMiT and Ivanov, Sergiu and Moore, Jason K and Singh, Sartaj and others},
  journal   = {PeerJ Computer Science},
  pages     = {e103},
  publisher = {PeerJ Inc.},
  title     = {SymPy: symbolic computing in Python},
  volume    = {3},
  year      = {2017}
}

@inproceedings{micikevicius2018mixed,
  author    = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
  booktitle = {International Conference on Learning Representations},
  title     = {Mixed Precision Training},
  year      = {2018}
}

@article{mitra2024orcamath,
  author  = {Mitra, Arindam and Khanpour, Hamed and Rosset, Corby and Awadallah, Ahmed},
  journal = {arXiv preprint arXiv:2402.14830},
  title   = {Orca-Math: Unlocking the potential of SLMs in Grade School Math},
  year    = {2024}
}

@article{neal2003slice,
  author    = {Neal, Radford M},
  journal   = {The annals of statistics},
  number    = {3},
  pages     = {705--767},
  publisher = {Institute of Mathematical Statistics},
  title     = {Slice sampling},
  volume    = {31},
  year      = {2003}
}

@article{ning2024can,
  author  = {Ning, Xuefei and Wang, Zifu and Li, Shiyao and Lin, Zinan and Yao, Peiran and Fu, Tianyu and Blaschko, Matthew B and Dai, Guohao and Yang, Huazhong and Wang, Yu},
  journal = {arXiv preprint arXiv:2406.14629},
  title   = {Can LLMs Learn by Teaching? A Preliminary Study},
  year    = {2024}
}

@misc{nvidia2020tf32,
  author = {NVIDIA},
  title  = {TensorFloat-32 in the A100 GPU Accelerates AI Training, HPC up to 20x},
  url    = {https://blogs.nvidia.com/blog/tensorfloat-32-precision-format},
  year   = 2020
}

%  Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others
@article{openai2023gpt4,
  author  = {OpenAI}, 
  title   = {Gpt-4 technical report},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023}
}

@inproceedings{patel2021svamp,
  abstract  = {The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered {``}solved{''} with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.},
  address   = {Online},
  author    = {Patel, Arkil  and
               Bhattamishra, Satwik  and
               Goyal, Navin},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  doi       = {10.18653/v1/2021.naacl-main.168},
  editor    = {Toutanova, Kristina  and
               Rumshisky, Anna  and
               Zettlemoyer, Luke  and
               Hakkani-Tur, Dilek  and
               Beltagy, Iz  and
               Bethard, Steven  and
               Cotterell, Ryan  and
               Chakraborty, Tanmoy  and
               Zhou, Yichao},
  month     = jun,
  pages     = {2080--2094},
  publisher = {Association for Computational Linguistics},
  title     = {Are {NLP} Models really able to Solve Simple Math Word Problems?},
  year      = {2021}
}

@article{radford2019gpt2,
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  number  = {8},
  pages   = {9},
  title   = {Language Models are Unsupervised Multitask Learners},
  volume  = {1},
  year    = {2019}
}

@inproceedings{rajbhandari2020zero,
  author       = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle    = {SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  organization = {IEEE},
  pages        = {1--16},
  title        = {ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
  year         = {2020}
}

@inproceedings{rasley2020deepspeed,
  author    = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages     = {3505--3506},
  title     = {Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  year      = {2020}
}

@inproceedings{saxton2018dmmath,
  author    = {David Saxton and Edward Grefenstette and Felix Hill and Pushmeet Kohli},
  booktitle = {International Conference on Learning Representations},
  title     = {Analysing Mathematical Reasoning Abilities of Neural Models},
  year      = {2019}
}

@inproceedings{self-instruct,
  abstract  = {Large {``}instruction-tuned{''} language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33{\%} absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5{\%} absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  address   = {Toronto, Canada},
  author    = {Wang, Yizhong  and
               Kordi, Yeganeh  and
               Mishra, Swaroop  and
               Liu, Alisa  and
               Smith, Noah A.  and
               Khashabi, Daniel  and
               Hajishirzi, Hannaneh},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  doi       = {10.18653/v1/2023.acl-long.754},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  month     = jul,
  pages     = {13484--13508},
  publisher = {Association for Computational Linguistics},
  title     = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  year      = {2023}
}

@article{shao2024deepseekmath,
  author  = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, YK and Wu, Y and Guo, Daya},
  journal = {arXiv preprint arXiv:2402.03300},
  title   = {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models},
  year    = {2024}
}

@inproceedings{shi2023language,
  author    = {Freda Shi and Mirac Suzgun and Markus Freitag and Xuezhi Wang and Suraj Srivats and Soroush Vosoughi and Hyung Won Chung and Yi Tay and Sebastian Ruder and Denny Zhou and Dipanjan Das and Jason Wei},
  booktitle = {The Eleventh International Conference on Learning Representations },
  title     = {Language models are multilingual chain-of-thought reasoners},
  year      = {2023}
}

@article{singh2023restem,
  author  = {Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Liu, Peter J and Harrison, James and Lee, Jaehoon and Xu, Kelvin and Parisi, Aaron and others},
  journal = {arXiv preprint arXiv:2312.06585},
  title   = {Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models},
  year    = {2023}
}

@article{sorscher2022beyond,
  author  = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari},
  journal = {Advances in Neural Information Processing Systems},
  pages   = {19523--19536},
  title   = {Beyond neural scaling laws: beating power law scaling via data pruning},
  volume  = {35},
  year    = {2022}
}


@article{tang2024mathscale,
  author  = {Tang, Zhengyang and Zhang, Xingxing and Wan, Benyou and Wei, Furu},
  journal = {arXiv preprint arXiv:2403.02884},
  title   = {MathScale: Scaling Instruction Tuning for Mathematical Reasoning},
  year    = {2024}
}

@article{toomarian1992learning,
  author    = {Toomarian, Nikzad Benny and Barhen, Jacob},
  journal   = {Neural Networks},
  number    = {3},
  pages     = {473--484},
  publisher = {Elsevier Science Ltd. Oxford, UK, UK},
  title     = {Learning a trajectory using adjoint functions and teacher forcing},
  volume    = {5},
  year      = {1992}
}

@article{toshniwal2024openmathinstruct,
  author  = {Toshniwal, Shubham and Moshkov, Ivan and Narenthiran, Sean and Gitman, Daria and Jia, Fei and Gitman, Igor},
  journal = {arXiv preprint arXiv:2402.10176},
  title   = {OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset},
  year    = {2024}
}

@misc{touvron2023llama,
  archiveprefix = {arXiv},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and TimothÃ©e Lacroix and Baptiste RoziÃ¨re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  eprint        = {2302.13971},
  primaryclass  = {cs.CL},
  title         = {LLaMA: Open and Efficient Foundation Language Models},
  year          = {2023}
}

@misc{touvron2023llama2,
  archiveprefix = {arXiv},
  author        = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  eprint        = {2307.09288},
  primaryclass  = {cs.CL},
  title         = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  year          = {2023}
}

@article{van2008tsne,
  author  = {Van der Maaten, Laurens and Hinton, Geoffrey},
  journal = {Journal of machine learning research},
  number  = {11},
  title   = {Visualizing Data using t-SNE},
  volume  = {9},
  year    = {2008}
}

@inproceedings{wang2024mathcoder,
  author    = {Wang, Ke and Ren, Houxing and Zhou, Aojun and Lu, Zimu and Luo, Sichun and Shi, Weikang and Zhang, Renrui and Song, Linqi and Zhan, Mingjie and Li, Hongsheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning},
  year      = {2024}
}

@inproceedings{wei2022chain,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {24824--24837},
  publisher = {Curran Associates, Inc.},
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  volume    = {35},
  year      = {2022}
}

@article{wolf2019transformers,
  author  = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal = {arXiv preprint arXiv:1910.03771},
  title   = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  year    = {2019}
}

@article{workshop2022bloom,
  author  = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal = {arXiv preprint arXiv:2211.05100},
  title   = {Bloom: A 176b-parameter open-access multilingual language model},
  year    = {2022}
}

@article{xia2024less,
  author  = {Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi},
  journal = {arXiv preprint arXiv:2402.04333},
  title   = {Less: Selecting influential data for targeted instruction tuning},
  year    = {2024}
}

@inproceedings{xie2024self,
  author    = {Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {41618--41650},
  publisher = {Curran Associates, Inc.},
  title     = {Self-Evaluation Guided Beam Search for Reasoning},
  volume    = {36},
  year      = {2023}
}

@article{xu2024chatglmmath,
  author  = {Xu, Yifan and Liu, Xiao and Liu, Xinghan and Hou, Zhenyu and Li, Yueyan and Zhang, Xiaohan and Wang, Zihan and Zeng, Aohan and Du, Zhengxiao and Zhao, Wenyi and others},
  journal = {arXiv preprint arXiv:2404.02893},
  title   = {ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline},
  year    = {2024}
}

@inproceedings{yao2024tree,
  author    = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {11809--11822},
  publisher = {Curran Associates, Inc.},
  title     = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  volume    = {36},
  year      = {2023}
}

@inproceedings{yu2024metamath,
  author    = {Longhui Yu and Weisen Jiang and Han Shi and Jincheng YU and Zhengying Liu and Yu Zhang and James Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  year      = {2024}
}

@article{yuan2023rft,
  author  = {Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Tan, Chuanqi and Zhou, Chang},
  journal = {arXiv preprint arXiv:2308.01825},
  title   = {Scaling Relationship on Learning Mathematical Reasoning with Large Language Models},
  year    = {2023}
}

@inproceedings{yue2024mammoth,
  author    = {Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {{MA}mmo{TH}: Building Math Generalist Models through Hybrid Instruction Tuning},
  year      = {2024}
}

@article{yue2024mammoth2,
  author  = {Yue, Xiang and Zheng, Tuney and Zhang, Ge and Chen, Wenhu},
  journal = {NeurIPS 2024},
  title   = {MAmmoTH2: Scaling Instructions from the Web},
  year    = {2024}
}

@article{zelikman2022star,
  author  = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal = {Advances in Neural Information Processing Systems},
  pages   = {15476--15488},
  title   = {Star: Bootstrapping reasoning with reasoning},
  volume  = {35},
  year    = {2022}
}

@inproceedings{zeng2023glm130b,
  author    = {Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Zhiyuan Liu and Peng Zhang and Yuxiao Dong and Jie Tang},
  booktitle = {The Eleventh International Conference on Learning Representations },
  title     = {{GLM}-130B: An Open Bilingual Pre-trained Model},
  year      = {2023}
}

@inproceedings{zhang2018adam,
  author       = {Zhang, Zijun},
  booktitle    = {2018 IEEE/ACM 26th international symposium on quality of service (IWQoS)},
  organization = {IEEE},
  pages        = {1--2},
  title        = {Improved Adam Optimizer for Deep Neural Networks},
  year         = {2018}
}

@inproceedings{zheng2022minif2f,
  author    = {Kunhao Zheng and Jesse Michael Han and Stanislas Polu},
  booktitle = {International Conference on Learning Representations},
  title     = {miniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
  year      = {2022}
}

@inproceedings{zhou2024csv,
  author    = {Aojun Zhou and Ke Wang and Zimu Lu and Weikang Shi and Sichun Luo and Zipeng Qin and Shaoqing Lu and Anya Jia and Linqi Song and Mingjie Zhan and Hongsheng Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {Solving Challenging Math Word Problems Using {GPT}-4 Code Interpreter with Code-based Self-Verification},
  year      = {2024}
}

@inproceedings{wei2022cot,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {24824--24837},
 publisher = {Curran Associates, Inc.},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 volume = {35},
 year = {2022}
}

@misc{chen2021humaneval,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{rein2024gpqa,
title={{GPQA}: A Graduate-Level Google-Proof Q\&A Benchmark},
author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
booktitle={First Conference on Language Modeling},
year={2024},
}

@misc{openai2024o1,
  author = {OpenAI},
  title  = {Learning to reason with LLMs},
  url    = {https://openai.com/index/learning-to-reason-with-llms/},
  year   = 2024
}

@misc{deepseekai2025r1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}


@misc{kimi2025k15,
      title={Kimi k1.5: Scaling Reinforcement Learning with LLMs}, 
      author={{Kimi Team}},
      year={2025},
      eprint={2501.12599},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@inproceedings{lamb2016professorforcing,
 author = {Lamb, Alex M and ALIAS PARTH GOYAL, Anirudh Goyal and Zhang, Ying and Zhang, Saizheng and Courville, Aaron C and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Professor Forcing: A New Algorithm for Training Recurrent Networks},
 volume = {29},
 year = {2016}
}

@misc{schulman2017ppo,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@book{sutton2018reinforce,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}

@inproceedings{wang2024mmlupro,
title={{MMLU}-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
author={Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
}


@misc{qwen2024qwq,
  author = {{Qwen Team}},
  title  = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
  url    = {https://qwenlm.github.io/blog/qwq-32b-preview/},
  year   = 2024
}

@misc{qwen2024qwen25math,
      title={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement}, 
      author={{Qwen Team}},
      year={2024},
      eprint={2409.12122},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{hou2025advancinglanguagemodelreasoning,
      title={Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling}, 
      author={Zhenyu Hou and Xin Lv and Rui Lu and Jiajie Zhang and Yujiang Li and Zijun Yao and Juanzi Li and Jie Tang and Yuxiao Dong},
      year={2025},
      eprint={2501.11651},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}


@misc{tinyzero,
author       = {Jiayi Pan and Junjie Zhang and Xingyao Wang and Lifan Yuan},
title        = {TinyZero},
url = {https://github.com/Jiayi-Pan/TinyZero},
note         = {Accessed: 2025-01-24},
year         = {2025}
}

@article{hu2025reinforce++,
  title={REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models},
  author={Hu, Jian},
  journal={arXiv preprint arXiv:2501.03262},
  year={2025}
}


@misc{zeng2025simplerl,
  title={7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient},
  author={Weihao Zeng and Yuzhen Huang and Wei Liu and Keqing He and Qian Liu and Zejun Ma and Junxian He},
  year={2025},
  howpublished={\url{https://hkust-nlp.notion.site/simplerl-reason}},
  note={Notion Blog},
}

@misc{loshchilov2017sgdrstochasticgradientdescent,
      title={SGDR: Stochastic Gradient Descent with Warm Restarts}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2017},
      eprint={1608.03983},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{
doi:10.1126/sciadv.abg6611,
author = {Zilong Gao  and Hanqing Wang  and Chen Lu  and Tiezhan Lu  and Sean Froudist-Walsh  and Ming Chen  and Xiao-Jing Wang  and Ji Hu  and Wenzhi Sun },
title = {The neural basis of delayed gratification},
journal = {Science Advances},
volume = {7},
number = {49},
pages = {eabg6611},
year = {2021},
doi = {10.1126/sciadv.abg6611},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abg6611},
abstract = {Sustained ramping of dopaminergic activation helps individuals resist impulsivity and wait for a later but larger reward. Balancing instant gratification versus delayed but better gratification is important for optimizing survival and reproductive success. Although delayed gratification has been studied through human psychological and brain activity monitoring and animal research, little is known about its neural basis. We successfully trained mice to perform a waiting-for-water-reward delayed gratification task and used these animals in physiological recording and optical manipulation of neuronal activity during the task to explore its neural basis. Our results showed that the activity of dopaminergic (DAergic) neurons in the ventral tegmental area increases steadily during the waiting period. Optical activation or silencing of these neurons, respectively, extends or reduces the duration of waiting. To interpret these data, we developed a reinforcement learning model that reproduces our experimental observations. Steady increases in DAergic activity signal the value of waiting and support the hypothesis that delayed gratification involves real-time deliberation.}}

@inproceedings{broder1998minhash,
  title={Min-wise independent permutations},
  author={Broder, Andrei Z and Charikar, Moses and Frieze, Alan M and Mitzenmacher, Michael},
  booktitle={Proceedings of the thirtieth annual ACM symposium on Theory of computing},
  pages={327--336},
  year={1998}
}

@misc{hu2024openrlhfeasytousescalablehighperformance,
      title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework}, 
      author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},
      year={2024},
      eprint={2405.11143},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@misc{lambert2024tulu,
    title={Tulu 3: Pushing Frontiers in Open Language Model Post-Training},
    author={Nathan Lambert and Jacob Morrison and Valentina Pyatkin and Shengyi Huang and Hamish Ivison and Faeze Brahman and Lester James V. Miranda and Alisa Liu and Nouha Dziri and Shane Lyu and Yuling Gu and Saumya Malik and Victoria Graf and Jena D. Hwang and Jiangjiang Yang and Ronan Le Bras and Oyvind Tafjord and Chris Wilhelm and Luca Soldaini and Noah A. Smith and Yizhong Wang and Pradeep Dasigi and Hannaneh Hajishirzi},
    year={2024},
    eprint={2411.15124},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{tong2024dartmath,
title={{DART}-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving},
author={Yuxuan Tong and Xiwen Zhang and Rui Wang and Ruidong Wu and Junxian He},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
}

@misc{ouyang2022training,
    title={Training language models to follow instructions with human feedback},
    author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
    year={2022},
    eprint={2203.02155},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{feng2023alphazerolike,
    title={Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training},
    author={Xidong Feng and Ziyu Wan and Muning Wen and Stephen Marcus McAleer and Ying Wen and Weinan Zhang and Jun Wang},
    year={2023},
    eprint={2309.17179},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{snell2024scaling,
    title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters},
    author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
    year={2024},
    eprint={2408.03314},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{wang2024multistep,
    title={Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision},
    author={Zihan Wang and Yunxuan Li and Yuexin Wu and Liangchen Luo and Le Hou and Hongkun Yu and Jingbo Shang},
    year={2024},
    eprint={2402.02658},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{
zheng2024sglang,
title={{SGL}ang: Efficient Execution of Structured Language Model Programs},
author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
}

@misc{paster2023openwebmathopendatasethighquality,
      title={OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text}, 
      author={Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba},
      year={2023},
      eprint={2310.06786},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@Misc{cc:Rana:2010:Common-Crawl-open-web-scale-crawl,
  author       = "Ahad Rana",
  year         = "2010",
  title        = "Common Crawl â€“ Building an open web-scale crawl using Hadoop",
  URL          = "https://www.slideshare.net/hadoopusergroup/common-crawlpresentation",
  cc-author-affiliation = "Common Crawl",
  cc-class     = "web-crawling, big data, Hadoop",
}

@INPROCEEDINGS{666900,
  author={Broder, A.Z.},
  booktitle={Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171)}, 
  title={On the resemblance and containment of documents}, 
  year={1997},
  volume={},
  number={},
  pages={21-29},
  keywords={Sampling methods;Web sites;Digital systems;Particle measurements;Fingerprint recognition;Explosions;Algorithm design and analysis;Clustering algorithms;Costs;Testing},
  doi={10.1109/SEQUEN.1997.666900}}

@misc{wang2025r1waitcount,
  author = {Peiyi Wang},
  title = {Wait Count in R1 Training Process},
  year = {2025},
  url = {https://x.com/sybilhyz/status/1884290930355888589},
  note = {Accessed: 2025-02-04}
}

@article{su2024rope,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@misc{Chung2024slides,
  author       = {Hyung Won Chung},
  title        = {Donâ€™t teach. Incentivize.},
  year         = {2024},
  howpublished = {Presentation slides},
  url          = {https://t.co/2sjhynKxzJ},
  note         = {Slide 48}
}