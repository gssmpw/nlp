% \section{Conclusion}

\section{Discussions and Future Work}
In this work, we demystify long CoT reasoning in LLMs. In this section, we outline potential future directions.


\subsection{Scaling up Model Size}
We believe that model size is the primary factor limiting the emergence of the behavior observed in \autoref{result:base-reflections-existence}. Hyung Won Chung~\cite{Chung2024slides} recently shared a similar perspective, suggesting that smaller models may struggle to develop high-level reasoning skills and instead rely on heuristic-based pattern recognition. Future research could investigate RL using a larger base model.

\subsection{RL Infrastructure Is Still in Its Infancy
}
While attempting to scale up the model size, we encountered significant challenges in expanding to 32B, ultimately determining that the required number of GPUs was too large to proceed. We observe that open-source RL frameworks (e.g., OpenRLHF~\cite{hu2024openrlhfeasytousescalablehighperformance}) often coordinate multiple systems optimized for different training and inference workloads, leading to multiple copies of model parameters being stored in memory. Additionally, algorithms like PPO alternate between these workloads synchronously and sequentially, further limiting efficiency. These factors contribute to low hardware utilization, an issue that is particularly exacerbated in long CoT scenarios due to the higher variance in CoT length, which leads to stragglers during inference~\cite{kimi2025k15}. We look forward to advancements in machine learning and systems research that will help overcome these limitations and accelerate progress in long CoT modeling.

\subsection{REINFORCE Is More Tricky to Tune than PPO}
\label{result:reward-reinforce}

We also explored REINFORCE++~\cite{hu2025reinforce++} as a faster alternative to PPO for scaling up data. However, we found it to be significantly more unstable than PPO, leading to lower training accuracies (Figure \ref{fig:reinforce-instability}). As this instability may be due to an untuned setup (Appendix \ref{app:exp-hyperparams-reward-reinforce}), we refrain from making general claims about the algorithm. We present this as an observation that may be useful to the community.

\subsection{Scaling up Verification}

While our findings demonstrate that combining rule-based verifiers with prompt set filtering is highly effective, designing such rules and curating prompt sets across different domains remains labor-intensive. More fundamentally, this approach embeds human-designed heuristics into the RL environment, reflecting how we think rather than allowing for emergent learning. As highlighted in The Bitter Lesson\footnote{http://www.incompleteideas.net/IncIdeas/BitterLesson.html}, manually encoding human intuition tends to be an inefficient long-term strategy. This raises an intriguing question: how can verification signals be scaled effectively? Is there an equivalent of pretraining in the context of designing RL environments? We look forward to future research on silver supervision signals and the potential for self-supervised approaches in RL verification.

\subsection{Latent Capabilities in Base Models}

Reasoning is a latent capability in base models that has only recently been unlocked. Our analysis suggests that one possible source of this emergent thinking is human dialogue on Internet discussion forums. This raises a broader question: what other abilities exist, waiting to be elicited from the vast reservoir of human knowledge and experience embedded in pre-training data? We look forward to more detailed analyses tracing model behaviors back to their data origins, which could yield new insights and help uncover hidden capabilities within base models.