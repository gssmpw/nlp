\section{Problem Formulation}
\label{sec:problemform}

In this section, we define the notation, followed by an overview of SFT and RL methods for eliciting long CoTs.

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Research Aim}]
Our goal is to \textit{demystify long chain-of-thought reasoning} in LLMs. Through systematic analysis and ablations, we extract key insights and offer practical strategies to enhance and stabilize its performance.
\end{tcolorbox}
\vspace{-10pt}


\subsection{Notation}
\label{sec:notation}

Let \(x\) be a query, and let \(y\) be the output sequence. We consider a LLM parameterized by \(\theta\), which defines a conditional distribution over output tokens: $
\pi_\theta(y_t \mid x, y_{1:t-1}).
$

We denote by \(\text{CoT}(y)\subseteq y\) the tokens in the generated output that constitute the \emph{chain-of-thought}, which is often a reasoning trace or explanatory sequence. The final “answer” can be a separate set of tokens or simply the last part of \(y\).

In this work, we use the term \emph{long chain-of-thought} (\emph{long CoT}) to describe an extended sequence of reasoning tokens that not only exhibits a larger-than-usual token length but also demonstrates more sophisticated behaviors such as:

\noindent\textbf{1) Branching and Backtracking}: The model systematically explores multiple paths (branching) and reverts to earlier points if a particular path proves wrong (backtracking).

\noindent\textbf{2) Error Validation and Correction}: The model detects inconsistencies or mistakes in its intermediate steps and takes corrective actions to restore coherence and accuracy.


\subsection{Supervised Fine-Tuning (SFT)}
\label{sec:sft}

A common practice is to initialize the policy \(\pi_\theta\) via SFT~\citep{lamb2016professorforcing} on a dataset \(\mathcal{D}_{\text{SFT}} = \{(x_i, y_i)\}_{i=1}^N\), where \(y_i\) can be normal or long CoT reasoning tokens. 


\subsection{Reinforcement Learning (RL)}
\label{sec:rl-formulation}

After optional SFT initialization, we can further optimize the generation of long CoT with reinforcement learning.

\noindent\textbf{Reward Function.} 
We define a scalar reward \(r_t\) designed to encourage correct and verifiable reasoning. We only consider the outcome-based reward for the final answer produced, and do not consider process-based reward for the intermediate steps. We denote
the term \(r_{\text{answer}}(y)\) to capture the correctness of the final solution.

\noindent\textbf{Policy Update.} We adopted Proximal Policy Optimization (PPO) \citep{schulman2017ppo} as the default policy optimization method in our experiments. We also briefly discuss
REINFORCE \citep{sutton2018reinforce} method in \autoref{result:reward-reinforce}. We adopt a rule-based verifier as the reward function, which compares the predicted answer with the ground truth answer directly. The resulting updates push the policy to generate tokens that yield higher reward. 


\subsection{Training Setup}

We adopt \texttt{Llama-3.1-8B}~\cite{meta2023llama3} and \texttt{Qwen2.5} \texttt{-7B-Math}~\cite{qwen2024qwen25math} as the base models, which are representative general and math-specialized models respectively. For both SFT and RL, we use the 7,500 training sample prompt set of MATH \citep{hendrycks2021math} by default, with which verifiable ground truth answers are provided. For SFT when ground truth answers are available, we synthesize responses by rejection sampling \citep{zelikman2022star, dong2023raft, yuan2023rft, gulcehre2023rest, singh2023restem,yue2024mammoth, tong2024dartmath}. Specifically, we first sample a fixed number $N$ of candidate responses per prompt and then filter by only retaining ones with final answers consistent with the corresponding ground truth answers. We also discuss data like WebInstruct \cite{yue2024mammoth2} that is more diverse but without gold supervision signals like ground truth answers in \textsection\ref{sec:silver-data}. We train the models with the OpenRLHF framework~\cite{hu2024openrlhfeasytousescalablehighperformance}. 


\subsection{Evaluation Setup}\label{sec:eval-setup}

We focus on four representative reasoning benchmarks: MATH-500, AIME 2024, TheoremQA \citep{chen2023theoremqa}, and MMLU-Pro-1k \citep{wang2024mmlupro}. Given that our training data is primarily in the mathematical domain, these benchmarks provide a comprehensive framework for both in-domain (MATH-500 test set) and out-of-domain evaluations (AIME 2024, TheoremQA, MMLU-Pro-1k). By default, we generate from the models using a temperature of $t = 0.7$, a top-$p$ value of 0.95, and a maximum output length of 16,384 tokens. Please refer to Appendix \ref{app:eval-setup} for further details on the evaluation setup.
