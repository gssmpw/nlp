\section{Introduction}

Large language models (LLMs) \citep{brown2020language,
touvron2023llama,anthropic2023claude,openai2023gpt4} have demonstrated remarkable reasoning abilities in domains like mathematics \citep{cobbe2021gsm8k} and programming~\citep{chen2021humaneval}. A key technique for enabling reasoning abilities in LLMs is chain-of-thought (CoT) prompting \citep{wei2022cot}, which guides models to generate intermediate reasoning steps before arriving at a final answer.

Despite these advancements, LLMs still struggle with highly complex reasoning tasks, such as mathematical competitions \citep{hendrycks2021math}, PhD-level scientific QA \citep{rein2024gpqa}, and software engineering \citep{jimenez2024swebench}, even with CoT. Recently, OpenAI's o1 models \citep{openai2024o1} have demonstrated significant breakthroughs in these tasks. A key distinguishing feature of these models is their ability to scale up inference compute with long CoTs, which include strategies such as recognizing and correcting mistakes, breaking down difficult steps, and iterating on alternative approaches, leading to substantially longer and more structured reasoning processes.

Several efforts have attempted to replicate the performance of o1 models by training LLMs to generate long CoTs~\citep{qwen2024qwq, deepseekai2025r1, kimi2025k15, tinyzero, zeng2025simplerl}. Most of these approaches rely on verifiable rewards, such as accuracy based on ground-truth answers, which helps to avoid reward hacking in reinforcement learning (RL) at scale. However, a comprehensive understanding of how models learn and generate long CoTs remains limited. In this work, we systematically investigate the underlying mechanics of long CoT generation. Specifically, we explore:

1) \textit{Supervised fine-tuning (SFT) for long CoTs} -- the most direct way to enable long CoT reasoning. We analyze its scaling behavior and impact on RL, finding that long CoT SFT allows models to reach higher performance and also facilitates easier RL improvements than short CoT.

2) \textit{Challenges in RL-driven CoT scaling} -- we observe that RL does not always stably extend CoT length and complexity. To address this, we introduce a cosine length-scaling reward with a repetition penalty, which stabilizes CoT growth while encouraging emergent reasoning behaviors such as branching and backtracking.

3) \textit{Scaling up verifiable signals for long CoT RL} -- Verifiable reward signals are essential for stabilizing long CoT RL. However, scaling them up remains challenging due to the limited availability of high-quality, verifiable data. To address this, we explore the use of data containing noisy, web-extracted solutions~\cite{yue2024mammoth2}. While these ``silver'' supervision signals introduce uncertainty, we find that, with an appropriate mixture in SFT and filtration in RL, they show promise, especially in out-of-distribution (OOD) reasoning scenarios such as STEM problem-solving.

4) \textit{Origins of Long CoT Abilities and RL Challenges} â€“ Core skills like branching and error validation are inherently present in base models, but effective RL-driven incentivization demands careful designs. We examine RL incentives on long CoT generation, trace reasoning patterns in pre-training data, and discuss nuances in measuring their emergence.
