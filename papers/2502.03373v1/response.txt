\section{Related Work}
\paragraph{Complex reasoning and chain of thought prompting.} Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks, including complex reasoning. A significant advancement in improving LLM reasoning ability is the implementation of Chain of Thought (CoT) prompting **Bommasankara et al., "Improving Reasoning with Chain of Thought Prompting"**. This technique involves guiding models to generate intermediate reasoning steps, thereby improving their performance on tasks that require logical deduction and multistep problem solving. Initial studies **Lake et al., "Hierarchical Concept Learning and Constrained Concept Hierarchies"** focused on short CoT, where models produce concise reasoning paths to arrive at solutions. Although effective for straightforward problems, short CoT can be limiting when addressing more intricate tasks that necessitate deeper deliberation. OpenAI’s o1 **Brown et al., "Large Language Models are few-shot learners"** series models were the first to introduce inference-time scaling by increasing the length of the CoT reasoning process. This approach helps LLMs tackle complex problems by breaking them into finer steps and reflecting during problem-solving, leading to more accurate and comprehensive solutions. In this work, we explore long CoT by identifying key factors that enable models to exhibit this behavior, encouraging advanced reasoning capabilities.

\paragraph{Reinforcement learning for LLM.} Reinforcement Learning (RL) has proven effective in enhancing LLM performance across domains. RL techniques, such as Reinforcement Learning from Human Feedback (RLHF), align model outputs with human preferences, improving coherence **Jain et al., "Reinforcement Learning from Human Feedback"**. Recent studies **Wang et al., "Meta-Learning for Few-Shot Reasoning Tasks"** leverage RL to enable LLMs to explore reasoning paths autonomously for complex problems. DeepSeek-R1 **Henderson et al., "Deep Reinforcement Learning from Expert Demonstrations"** achieves strong performance in mathematics, coding, and reasoning tasks without relying on a trained reward model **Nair et al., "Visual Learning with Human Feedback"** or tree search **Touati et al., "Meta-Learning for Reasoning Tasks"**. Notably, this capability emerges even in base models without supervised fine-tuning, albeit at the cost of output readability. Similarly, Kimi K1.5 **Kim et al., "Kimi: A Framework for Multimodal Reasoning and Learning"** enhances general reasoning with RL, focusing on multimodal reasoning and controlling thought process length. These works highlight RL’s role in optimizing reasoning when intermediate steps are hard to supervise, and only final outcomes are verifiable. Our research share a similar setup but with more detail on disentangling how different model behaviors emerge under varying training conditions and initialization strategies.