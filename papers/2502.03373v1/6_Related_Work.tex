\section{Related Work}
\paragraph{Complex reasoning and chain of thought prompting.} Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks, including complex reasoning. A significant advancement in improving LLM reasoning ability is the implementation of Chain of Thought (CoT) prompting \cite{wei2022cot}. This technique involves guiding models to generate intermediate reasoning steps, thereby improving their performance on tasks that require logical deduction and multistep problem solving. Initial studies \cite{lambert2024tulu, wei2022cot, flan, yu2024metamath} focused on short CoT, where models produce concise reasoning paths to arrive at solutions. Although effective for straightforward problems, short CoT can be limiting when addressing more intricate tasks that necessitate deeper deliberation. OpenAI’s o1 \cite{openai2024o1} series models were the first to introduce inference-time scaling by increasing the length of the CoT reasoning process. This approach helps LLMs tackle complex problems by breaking them into finer steps and reflecting during problem-solving, leading to more accurate and comprehensive solutions. In this work, we explore long CoT by identifying key factors that enable models to exhibit this behavior, encouraging advanced reasoning capabilities.

\paragraph{Reinforcement learning for LLM.} Reinforcement Learning (RL) has proven effective in enhancing LLM performance across domains. RL techniques, such as Reinforcement Learning from Human Feedback (RLHF), align model outputs with human preferences, improving coherence \cite{ouyang2022training}. Recent studies \cite{kimi2025k15, deepseekai2025r1, lambert2024tulu} leverage RL to enable LLMs to explore reasoning paths autonomously for complex problems. DeepSeek-R1 \cite{deepseekai2025r1} achieves strong performance in mathematics, coding, and reasoning tasks without relying on a trained reward model \cite{lightman2023verifystep, wang2024multistep} or tree search \cite{feng2023alphazerolike, snell2024scaling}. Notably, this capability emerges even in base models without supervised fine-tuning, albeit at the cost of output readability. Similarly, Kimi K1.5 \cite{kimi2025k15} enhances general reasoning with RL, focusing on multimodal reasoning and controlling thought process length. These works highlight RL’s role in optimizing reasoning when intermediate steps are hard to supervise, and only final outcomes are verifiable. Our research share a similar setup but with more detail on disentangling how different model behaviors emerge under varying training conditions and initialization strategies.