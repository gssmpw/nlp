\section{Exploration on RL from the Base Model}
\label{sec:rl-from-base}

DeepSeek-R1 \citep{deepseekai2025r1} has demonstrated that long chain-of-thought reasoning can emerge by scaling up reinforcement learning compute on a base model. Recent studies \citep{zeng2025simplerl, tinyzero} have attempted to replicate this progress by running a relatively small number of RL iterations to observe the emergence of long CoT behavior (e.g., the ``aha moment''~\citep{deepseekai2025r1}, an emergent realization moment that enables critical functions like self-validation and correction).
We also explore the method of RL from the base model in this section.

% \subsection{RL from the Base Model}
\subsection{Nuances in Analysis Based on Emergent Behaviors}
\label{result:base-reflections-existence}

Self-validation behaviors are sometimes flagged as emergent behaviors or ``aha-moment'' by the model's exploration, since such patterns are rare in short CoT data. However, we notice that sometimes self-validation behaviors already exist in the base model  and reinforcing them through RL requires strict conditions, such as a strong base model.

\noindent\textbf{Setup.}
We follow the setup from \citet{zeng2025simplerl} to train \texttt{Qwen2.5-Math-7B} using PPO  with a rule-based verifier on approximately 8k MATH level 3-5 questions, but we use our own rule-based verifier implementation. For inference, we adopt temperature $t = 0$ (greedy decoding), as our preliminary experiments show that $t=0$ usually significantly outperforms $t>0$ for models obtained by direct RL from \texttt{Qwen2.5-Math-7B}. We use the maximum output length of 4096 tokens considering the training context length of 4096 tokens. Note that we use zero-shot prompting for the base model to avoid introducing biases to the output pattern. We select five representative keywords, ``wait'', ``recheck'', ``alternatively'', ``retry'' and ``however'' from long CoT cases in previous works \citep{openai2024o1,deepseekai2025r1,tinyzero,zeng2025simplerl}, and calculate their frequencies to quantify the extent to which the model does self-validation. Further details about the RL hyperparameters can be found in Appendix \ref{app:exp-hyperparams-rl-from-base}.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/simple-rl-acc-patterns-rate-dynamics.pdf}
    \vspace{-20pt}
    \caption{Dynamics of accuracies and reflection keyword rates on different benchmarks during our RL from the base model \texttt{Qwen2.5-Math-7B}. We do not see the keyword rates of ``wait'', ``alternatively'', and ``recheck'' get significantly improved during the RL training even though the accuracy is steadily increasing. }
    % \vspace{-10pt}
    \label{fig:reflection-acc-keywords-rate}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/simple-rl-math500-output-len-coding-rate-train-kl.pdf}
    \vspace{-20pt}
    \caption{Dynamics of the output token lengths and the coding rate on MATH-500 and the KL divergence of the policy over the base model on MATH Lv3-5 (training data) during our RL from \texttt{Qwen2.5-Math-7B}.}
    \vspace{-10pt}
    \label{fig:code-rate-output-len}
\end{figure*}

\noindent\textbf{Result.} Figure \ref{fig:reflection-acc-keywords-rate} shows that our RL from \texttt{Qwen2.5-} \texttt{Math-7B} effectively boosts the accuracies, but does not increase the frequency of the ``recheck`` pattern existing in the output of the base model,
nor effectively incentivize other reflection patterns such as ``retry'' and ``alternatively''. This indicates that RL from the base model does not necessarily incentivize reflection patterns, though significantly boosting the performance. Sometimes such behaviors exist in the base model's output and RL does not substantially enhance them. So we might need to be more careful about recognizing emergent behaviors.

% \begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{result:base-reflections-existence} for Analysis on Emergent Behaviors}
% RL from a small base model improves accuracy but struggles to incentivize behaviors like error validation in challenging tasks, even when such behaviors are already present in the base model.
% (Figure \ref{fig:reflection-acc-keywords-rate}).
% \end{AIbox}

\subsection{Nuances in Analysis Based on Length Scaling}
\label{sec:analysis-length-scaling} 

The length scaling up is recognized as another important feature of the effective exploration of the model. However, we notice that sometimes length scaling up can be accompanied by the KL divergence decreasing, which raises the possibility that the length is influenced by the KL penalty and is just reverting back to the base model's longer outputs, rather than reflecting the acquisition of long CoT ability.

\noindent\textbf{Setup.} The setup is the same as in \textsection\ref{result:base-reflections-existence}. Besides the output token length, we also calculate the ``coding rate''. We classify the model's output as ``coding'' if it contains the ``\texttt{```python}'', since \texttt{Qwen2.5-Math-7B} uses both natural language and coding to solve mathematical problems. Note that the ``coding'' output here is actually a special form of natural language output, where the code in it is not executed, and the code's output is generated by the model.

\noindent\textbf{Result.} Figure \ref{fig:code-rate-output-len} (1) shows that the length of the output token increases after an initial drop, but never exceeds the initial length of the base model.

\citet{zeng2025simplerl} suggest that the initial drop may be due to the model transitioning from generating long coding outputs to shorter natural language outputs. However, Figure \ref{fig:code-rate-output-len} (2) indicates that natural language outputs are actually longer than coding outputs, and the initial drop in length occurs in both types of output. Furthermore, Figure \ref{fig:code-rate-output-len} (3) shows that the coding rate subsequently increases again, suggesting that the distinction between coding and natural language may not significantly impact the optimization process.

Moreover, we suspect that the subsequent length scaling up is not from the model's exploration, since when the length scales up, the KL divergence of the policy over the base model drops, as shown in Figure \ref{fig:code-rate-output-len} (4). This might indicate that it is the KL penalty influencing length. If that is the case, there is little potential for the policy output length to exceed the base model's since the exploration is limited by the KL constraint.

% \begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{sec:analysis-length-scaling} for Analysis on Length Scaling}
% Length scaling is accompanied by a decrease in KL divergence, suggesting that the increase in length may be driven by the KL penalty rather than the model learning to think longer (Figure \ref{fig:code-rate-output-len}).
% \end{AIbox}

\subsection{Potential Reasons Why Emergent Behavior is Not Observed with \texttt{Qwen2.5-Math-7B}}

Our detailed analysis of RL from \texttt{Qwen2.5-Math-7B}, as presented in \textsection\ref{result:base-reflections-existence} and \textsection\ref{sec:analysis-length-scaling}, suggests that it fails to fully replicate the training behavior of \texttt{DeepSeek-R1}. We identify the following potential causes: 1) The base model, being relatively small (7B parameters), may lack the capacity to quickly develop such complex abilities when incentivized. 2) The model might have been overexposed to MATH-like short instruction data during (continual) pre-training and annealing, leading to overfitting and hindering the development of long CoT behaviors.

\subsection{Comparison between RL from the Base Model and RL from Long CoT SFT}
\label{sec:rl-from-base-vs-long-cot-sft}

We compare the performance of RL from the base model and RL from long CoT SFT and find that RL from long CoT SFT generally performs better.

\noindent\textbf{Setup.}
We compare using the base model \texttt{Qwen2.5-} \texttt{Math-7B}. The results of RL from the base model are from the model trained in \textsection\ref{result:base-reflections-existence}. For RL from long CoT SFT, we adopt a setup similar to \textsection\ref{sec:sft-init-for-rl}. Specifically, we choose the 7.5k MATH training set as the prompt set, curate the SFT data by rejection sampling with 32 candidate responses per prompt using \texttt{QwQ-32B-Preview}, and perform PPO using our cosine length-scaling reward with repetition penalty and our rule-based verifier, sampling 8 responses per prompt and training for 8 epochs. To adapt \texttt{Qwen2.5-Math-7B} with a pre-training context length of only 4096 tokens to long CoT SFT and RL, we multiply its RoPE \citep{su2024rope} $\theta$ by 10 times. We don't report the results of RL with classic reward from long CoT SFT since it collapses. For evaluation, we adopt our default temperature sampling setup for RL from long CoT SFT as in \textsection\ref{sec:eval-setup} and greedy decoding setup for RL from the base model as in \textsection\ref{result:base-reflections-existence} for the best performance. Further details about the distillation, SFT hyperparameters and RL hyperparameters can be found in Appendix \ref{app:distill} \& \ref{app:sft-setup} \& \ref{app:exp-hyperparams-rl-from-base}, respectively.

\begin{table}[htbp]
\caption{
Performance of different models based on \texttt{Qwen2.5-Math-7B}. The SFT data here is distilled with rejection sampling from \texttt{QwQ-32B-Preview}.
% ``MATH-QwQ'' here is curated by rejection sampling with 32 candidate responses per prompt in the 7.5k MATH training set using \texttt{QwQ-32B-Preview}. The ``MATH (Lv3-5, ~8k)'' prompt set here includes part of the original test split.
}
\label{tab:rl-from-base-vs-from-long-sft-rl}
% \vspace{-15pt}
\vskip 0.1in
\centering
\small
% \resizebox{\linewidth}{!}{
\begin{tabular}{@{}lccccc@{}}
\toprule
\multirow{2}{*}{Setup} & MATH & AIME & Theo. & MMLU & \multirow{2}{*}{AVG}\\
& 500 & 2024 & QA & Pro-1k \\
\midrule
Base (0-shot) & 52.0 & 13.3 & 17.1 & 2.4 & 21.2 \\
(Direct) RL & 77.4 & 23.3 & 43.5 & 19.7 & 41.0 \\
SFT  & 84.0 & 24.4 & 42.2 & 38.5 & 47.3 \\
SFT + RL & \textbf{85.9} & \textbf{26.9} & \textbf{45.4} & \textbf{40.6} & \textbf{49.7} \\
\bottomrule
\end{tabular}
% }
\end{table}

\noindent\textbf{Result.} Table \ref{tab:rl-from-base-vs-from-long-sft-rl} shows that, on \texttt{Qwen2.5-Math-7B}, RL initialized from the long CoT SFT model significantly outperforms RL from the base model and further improves upon the long CoT SFT itself. Specifically, RL from long CoT SFT with our cosine reward surpasses RL from the base model by a substantial 8.7\% on average and improves over the SFT initialization by 2.6\%. Notably, simply applying SFT with long CoT distilled from \texttt{QwQ-32B-Preview} already yields strong performance.
% Figure \ref{fig:rl-from-base-vs-long-cot-sft} 

% \begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{sec:rl-from-base-vs-long-cot-sft} for RL from Base or SFT}
% RL initialized from the long CoT SFT  model outperforms RL from the base model (Table \ref{tab:rl-from-base-vs-from-long-sft-rl}).
% \end{AIbox}


\subsection{Long CoT Patterns in Pre-training Data}
\label{result:base-cot-origin}

Based on the results in \textsection\ref{result:base-reflections-existence}, we hypothesize that incentivized behaviors, such as the model revisiting its solutions, may have already been partially learned during pre-training. To examine this, we employed two methods to investigate whether such data are already present on the web.

Firstly, we used a generative search engine Perplexity.ai to identify webpages explicitly containing problem-solving steps that approach problems from multiple angles or perform verification after providing an answer. The query we used and the examples we identified are in Appendix \ref{webpage:explicit-revision-correct}).

Secondly, we used \texttt{GPT-4o} to generate a list of phrases that are characteristic of the ``aha moment'' (Appendix \ref{app:open-web-math-queries}), then used the MinHash algorithm \cite{666900} to search through  OpenWebMath \cite{paster2023openwebmathopendatasethighquality}, a dataset filtered from the CommonCrawl \cite{cc:Rana:2010:Common-Crawl-open-web-scale-crawl} frequently used in pre-training. We found that there was a significant number of matches in discussion forum threads, where the dialogue between multiple users showed similarity to long CoT, with multiple approaches being discussed along with backtracking and error correction (Appendix \ref{app:open-web-math-matches}). This raises the intriguing possibility that long CoT originated from human dialogue, although we should also note that discussion forums are a common source of data in OpenWebMath.

Based on these observations, we hypothesize that RL primarily guides the model to recombine skills it already internalized during pre-training towards new behaviors to improve performance on complex problem-solving tasks. Given the broad scope of this paper, we leave a more in-depth investigation of this behavior to future work.


% \begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{result:base-cot-origin} for Origin of long CoT ability}
% Commonly used pre-training data contain content that shares similar properties (e.g., branching and error validation)  to long CoT (Appendix \ref{webpage:explicit-revision-correct}, \ref{app:open-web-math-matches}).
% \end{AIbox}


