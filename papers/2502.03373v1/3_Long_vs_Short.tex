\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.96\linewidth]{figs/viz-cot-scaling-direct.pdf}
    \vspace{-6pt}
    \caption{Scaling curves of SFT and RL on \texttt{Llama-3.1-8B}  with long CoTs and short CoTs. SFT with long CoTs can scale up to a higher upper limit and has more potential to further improve with RL.}
        \vspace{-10pt}

    \label{fig:cot-scaling}
\end{figure*}

\section{Impact of SFT on Long CoT}

In this section, we compare long and short CoT data for SFT and in the context of RL initialization.

\subsection{SFT Scaling}\label{subsec:sft-scaling}

To compare long CoT with short CoT, the first step is to equip the model with the corresponding behavior. The most straightforward approach is to fine-tune the base model on CoT data. Since short CoT is common, curating SFT data for it is relatively simple via rejection sampling from existing models. However, how to obtain high-quality long CoT data remains an open question. 
% We begin by distilling from the open-weight \texttt{QwQ-32B-Preview}  \citep{qwen2024qwq} because it is cheaper and produce better performance, which will be further discussed in \textsection\ref{sec:long-cot-pattern}.

\noindent\textbf{Setup.} To curate the SFT data, for long CoT, we distill from \texttt{QwQ-32B-Preview} (we discuss other long CoT data construction methods in \textsection\ref{sec:long-cot-pattern}). For short CoT, we distill from \texttt{Qwen2.5-Math-72B-Instruct}, which is a capable short CoT model in math reasoning. Specifically, we perform rejection sampling by first sampling $N$ candidate responses per prompt and then filtering for ones with correct answers. For long CoT, we use $N \in \{32, 64, 128, 192, 256\}$, while for short CoT, we use $N \in \{32, 64, 128, 256\}$, skipping one $N$ for efficiency. 
% The SFT datasets with smaller $N$'s are subsets of ones with larger $N$'s. 
In each case, the number of SFT tokens is proportional to $N$. We use the base model \texttt{Llama-3.1-8B} \citep{meta2023llama3}. Please refer to Appendix \ref{app:sft-setup} for more details about the SFT setup.

\noindent\textbf{Result.} The dashed lines in Figure \ref{fig:cot-scaling} illustrate that as we scale up the SFT tokens, long CoT SFT continues to improve model accuracy, whereas short CoT SFT saturates early at a lower accuracy level. For instance, on MATH-500, long CoT SFT achieves over 70\% accuracy and has yet to plateau even at 3.5B tokens. In contrast, short CoT SFT converges below 55\% accuracy, with an increase in SFT tokens from approximately 0.25B to 1.5B yielding only a marginal absolute improvement of about 3\%.

\begin{AIbox}{Takeaway 3.1 for SFT Scaling Upper Limit}
SFT with long CoT can scale up to a higher performance upper limit than short CoT. (Figure \ref{fig:cot-scaling})
\end{AIbox}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figs/qwen_llama_row.pdf}
    \vspace{-20pt}
    \caption{Both \texttt{Llama3.1-8B} and \texttt{Qwen2.5-Math-7B} models trained under RL with the Classic Reward manifested emergent CoT length scaling past the context window size, resulting in a decline in MATH-500 accuracy. The red points on the charts correspond to the iteration where the accuracy dropped to near zero. ``Terminated CoTs'' refer to responses that conclude within the context length.}
        \vspace{-10pt}

    \label{fig:reward-qwen-llama}
\end{figure*}

\subsection{SFT Initialization for RL}\label{sec:sft-init-for-rl}

Since RL is reported to have a higher upper limit than SFT, we compare long CoT and short CoT as different SFT initialization approaches for RL.

\noindent\textbf{Setup.}
We initialize RL using SFT checkpoints from \textsection\ref{subsec:sft-scaling}, and train for four epochs, sampling four responses per prompt. Our approach employs PPO \citep{schulman2017ppo} with a rule-based verifier from the MATH dataset, using its training split as our RL prompt set.
We adopt our cosine length scaling reward with the repetition penalty, which will be detailed in \textsection\ref{sec:reward-design}.
% The reward function follows a simple binary scheme: 1 if the answer is correct, 0 otherwise. We refer to this straightforward correctness-based reward as the Classic Reward. 
Further details about our RL setup and hyperparameters can be found in Appendix \ref{app:rl-setup} \& \ref{app:exp-hyperparams-sft-init-for-rl} respectively.

\noindent\textbf{Result.} The gap between solid and dashed lines in Figure \ref{fig:cot-scaling} shows that models initialized with long CoT SFT can usually be further significantly improved by RL, while models initialized with short CoT SFT see little gains from RL. For example, on MATH-500, RL can improve long CoT SFT models by over 3\% absolute, while short CoT SFT models have almost the same accuracies before and after RL.

\begin{AIbox}{Takeaway 3.2 for SFT Initialization for RL}
SFT with long CoTs makes further RL improvement easier, while short CoTs do not. (Figure \ref{fig:cot-scaling})
\end{AIbox}

\subsection{Sources of Long CoT SFT Data}\label{sec:long-cot-pattern}

To curate long CoT data, we compare two approaches: (1) \textbf{Construct} long CoT trajectories by prompting short CoT models to generate primitive actions and sequentially combining them; (2) \textbf{Distill} long CoT trajectories from existing long CoT models that exhibit emergent long CoT patterns.

\noindent\textbf{Setup.} To construct long CoT trajectories, we developed an Action Prompting framework (Appendix \ref{app:action-prompting}) which defined the following primitive actions: \texttt{clarify}, \texttt{decompose}, \texttt{solution\_step}, \texttt{reflection}, and \texttt{answer}. We employed multi-step prompting with a short CoT model (e.g., \texttt{Qwen2.5-72B-Instruct}) to sequence these actions, while a stronger model, \texttt{o1-mini-0912}, generates reflection steps incorporating self-correction. For distilling long CoT trajectories, we use \texttt{QwQ-32-Preview} as the teacher model. In both approaches, we adopt the MATH training set as the prompt set and apply rejection sampling. To ensure fairness, we use the same base model (\texttt{Llama-3.1-8B}), maintain approximately 200k SFT samples, and use the same RL setup as in \textsection\ref{sec:sft-init-for-rl}.

\noindent\textbf{Result.} Table \ref{tab:constructed-underperforms-emergent} shows that the model distilled from emergent long CoT patterns generalizes better than the constructed pattern, and can be further significantly improved with RL, while the model trained on constructed patterns cannot. Models trained with the emergent long CoT pattern achieve significantly higher accuracies on OOD benchmarks AIME 2024 and MMLU-Pro-1k, improving by 15-50\% relatively. 
Besides, on the OOD benchmark TheoremQA, RL on the long CoT SFT model significantly improves its accuracy by around 20\% relative, while the short CoT model's performance does not change. 
This is also why we conduct most of our experiments based on distilled long CoT trajectories.

\begin{AIbox}{Takeaway 3.3 for Long CoT Cold Start}
SFT initialization matters: high-quality, emergent long CoT patterns lead to significantly better generalization and RL gains. (Table \ref{tab:constructed-underperforms-emergent})
\end{AIbox}

\begin{table}[htbp]
\vspace{-15pt}
\caption{Emergent long CoT patterns outperform constructed ones. All the models here are fine-tuned from the base model \texttt{Llama-3.1-8B} with the MATH training prompt set.
% The SFT stages use data of different long CoT patterns, 
% while the RL stages use exactly the same setup.
}
\label{tab:constructed-underperforms-emergent}
\vskip 0.1in
\centering
\small
\begin{tabular}{@{}llcccc@{}}
\toprule
Training & Long CoT & MATH & AIME & Theo. & MMLU \\
Method & SFT Pattern & 500 & 2024 & QA & Pro-1k \\
\midrule
\multirow{2}{*}{SFT} & Constructed & 48.2 & 2.9 & 21.0 & 18.1 \\
& Emergent & \textbf{54.1} & \textbf{3.5} & \textbf{21.8} &\textbf{ 32.0} \\
\midrule
\multirow{2}{*}{SFT+RL} & Constructed & 52.4 & 2.7 & 21.0 & 19.2 \\
& Emergent & \textbf{59.4} & \textbf{4.0} & \textbf{25.2} & \textbf{34.6} \\
\bottomrule
\end{tabular}
\end{table}

% \subsection{RL from Base v.s RL from SFT Initialization}


