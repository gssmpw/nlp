\newpage
\appendix
\onecolumn
% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

\input{6_Related_Work}

\newpage
\section{Figures and Tables}

\begin{figure}[H]
    \centering
    \subfigure[Response lengths under different Cosine Rewards]
    {\includegraphics[width=0.24\textwidth]{figs/llama_cosine_hyperparams.pdf}}
    \subfigure[Reward A]
    {\includegraphics[width=0.24\textwidth]{figs/cosine_function_w0_c10.pdf}}
    \subfigure[Reward B]{\includegraphics[width=0.24\textwidth]{figs/cosine_function_w10_c6.pdf}}
    \subfigure[Reward C]{\includegraphics[width=0.24\textwidth]{figs/cosine_function_w10_c10.pdf}}
    \caption{(a) Tuning the hyperparameters of the Cosine Reward results in different length scaling behavior. Note that Reward A results in some performance degradation on downstream tasks due to the model's reduced ability to stop within the context window. (b) Reward A: $r_0^c=0, r_L^c=10, r_0^w=r_L^w=0$, (c) Reward B: $r_0^c = 6, r_L^c = 5, r_0^w = -10, r_L^w = 0$ (d) Reward C: $r_0^c = 10, r_L^c = 9, r_0^w = -10, r_L^w = 0$.}
    \label{fig:reward-cosine-hyperparams}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/alternatively_keyword_count_plot.pdf}
    \vspace{-10pt}
    \caption{CoT branching frequency, estimated by the keyword count of the pivot word "alternatively,", decreased under the Cosine Reward with more training compute. We attributed this, along with increased repetition, to reward hacking.}
    \label{fig:reward-cosine-branching}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/model_indecisiveness_alt.pdf}
    \vspace{-10pt}
   \caption{Branching frequency in CoT at different $\gamma_c$ values. Lowering the discount factor increased branching frequency, causing the model to abandon problem-solving approaches more quickly.}
    \label{fig:reward-indecisive}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/llama_cosine_repetition.pdf}
    \caption{Training response length of models trained with Cosine Reward with and without repetition penalty. We see that repetition penalty reduced the length.}
    \label{fig:reward-repetition-penalty-length-effect}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/reinforce_acc_res_len.pdf}
    \caption{Reinforce with classic reward shows signs of training instability.}
    \label{fig:reinforce-instability}
\end{figure}

%\begin{figure}[H]
%    \centering
%    \includegraphics[width=1\linewidth]{figs/viz-judge-rule.pdf}
%    \vspace{-20pt}
%    \caption{Downstream task performance when trained on clean data}
%    \label{fig:reward-verifier-clean}
%\end{figure}

\begin{table}[H]
\small
\caption{Performance of model trained with different discount factors for the correctness (cosine) reward and repetition penalty. We see that different reward types have different optimal values.}
\vspace{10pt}
\centering
\begin{tabular}{@{}cccccc@{}}
\toprule
\begin{tabular}[c]{@{}c@{}}Correctness \\ Discount\end{tabular} & \begin{tabular}[c]{@{}c@{}}Repetition\\ Discount\end{tabular} & \begin{tabular}[c]{@{}c@{}}MATH\\ -500\end{tabular} & \begin{tabular}[c]{@{}c@{}}AIME \\ 2024\end{tabular} & \begin{tabular}[c]{@{}c@{}}Theo.\\ QA\end{tabular} & \begin{tabular}[c]{@{}c@{}}MMLU\\ -Pro-1k\end{tabular} \\ \midrule
\multicolumn{2}{c}{SFT} & 50.4 & 3.5 & 20.6 & 32.4 \\ \midrule
\multirow{3}{*}{1.000} & 1.000 & 55.7 & \textbf{5.0} & 25.7 & 34.5 \\
 & 0.999 & \textbf{58.0} & 4.6 & \textbf{26.0} & \textbf{36.5} \\
 & 0.99 & 57.8 & 3.8 & 24.5 & 33.3 \\ \midrule
\multirow{2}{*}{0.999} & 0.999 & 53.5 & 2.1 & 19.5 & 30.7 \\
 & 0.99 & 55.2 & 1.7 & 18.5 & 32.0 \\ \midrule
0.99 & 0.99 & 47.9 & 0.2 & 15.6 & 25.5 \\ \bottomrule
\end{tabular}%
\label{fig:multiple-gamma}
\end{table}

\newpage
\section{Algorithms and Formulas}

\subsection{Cosine Reward Formula}

\begin{equation}
\label{eqn:cosine-lr}
\textbf{CosFn}(t, T, \eta_{min}, \eta_{max}) = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t\pi}{T}))
\end{equation}

The formula above is commonly used as the learning rate schedule during gradient descent optimization. It was introduced by \cite{loshchilov2017sgdrstochasticgradientdescent}.

\subsection{N-gram Repetition Penalty}

\begin{algorithm}[H]
\caption{N-gram Repetition Penalty}\label{alg:reward-repetition-penalty}
\begin{algorithmic}[1]
    \STATE {\bfseries Input:} 
    \STATE \ \ \ \ $s$ : sequence of tokens
    \STATE \ \ \ \ $l$ : sequence length
    \STATE \ \ \ \ $N$ : n-gram size
    \STATE \ \ \ \ $P$ : penalty value
    \STATE \ \ \ \ $m$ : maximum sequence length
    \STATE {\bfseries Output:} $r \in \mathbb{R}^m$
    \STATE $seq \gets s[1:l]$ \COMMENT{Extract subsequence of length $l$}
    \STATE $ngrams \gets \emptyset$ \COMMENT{Set of observed n-grams}
    \STATE $r \gets \vec{0} \in \mathbb{R}^m$ \COMMENT{Initialize reward vector}
    \FOR{$j \gets 1$ to $|seq| - N + 1$}
        \STATE $ng \gets (seq[j], seq[j+1], ..., seq[j+N-1])$ \COMMENT{Current n-gram}
        \IF{$ng \in ngrams$}
            \FOR{$t \gets j$ to $j + N - 1$}
                \STATE $r[t] \gets P$ \COMMENT{Apply penalty}
            \ENDFOR
        \ENDIF
        \STATE $ngrams \gets ngrams \cup \{ng\}$
    \ENDFOR
    \STATE {\bfseries Output:} $r$
\end{algorithmic}
\end{algorithm}

\newpage
\section{Extracts}

\begin{tcolorbox}[label=extract:reward-short-term, colback=lightgray!10, colframe=black, title={Short-Term Thinking ($\gamma_c = 0.99$)}]
\textbf{But} in this case, we saw that the fixed points don't satisfy the equation 
\[
p(p(x)) = x.
\]

\textbf{Wait}, maybe the solutions to \( p(p(x)) = x \) are not the fixed points of \( p \), but something else.

\textbf{Alternatively}, perhaps I can consider that \( p(p(x)) = x \) implies that \( p \) is a bijection, 
but since \( p(x) \) is quadratic, it's not necessarily invertible over all real numbers.

\textbf{Wait}, but the problem says that \( p(p(x)) = x \) has four real solutions, so perhaps there are four 
values of \( x \) that satisfy this equation.

\textbf{Alternatively}, perhaps I need to find the roots of 
\[
p(p(x)) - x = 0,
\]
and solve for \( x \). But that seems complicated. Maybe there's a better way.
\end{tcolorbox}

\newpage
\section{Experimental Setup}

\subsection{Evaluation Setup} \label{app:eval-setup}

\paragraph{Benchmarks} Below are details of our evaluation benchmarks:

\begin{itemize}
    \item \textbf{MATH-500} \citep{hendrycks2021math}: an in-domain mathematical reasoning benchmark. MATH consists of 12,500 problems from American high school math competitions. For efficiency, we adopt MATH-500, a widely-used i.i.d. subset of its test split.
    \item \textbf{AIME 2024}: an out-of-domain mathematical reasoning benchmark consisting of the 30 problems from American Invitational Mathematics Examination (AIME) 2024.
    \item \textbf{TheoremQA} \citep{chen2023theoremqa}: an out-of-domain STEM reasoning benchmark consisting of 800 samples. It covers 350+ theorems spanning across Math, EE\&CS, Physics and Finance.
    \item \textbf{MMLU-Pro-1k} \citep{wang2024mmlupro}: an out-of-domain general reasoning benchmark. MMLU-Pro comprises over 12,000 questions from academic exams and textbooks, spanning 14 diverse domains including Biology, Business, Chemistry, Computer Science, Economics, Engineering, Health, History, Law, Math, Philosophy, Physics, Psychology, and Others. For efficiency, we adopt an 1,000-sample i.i.d. subset of its test split, called MMLU-Pro-1k. We tried to keep the distribution identical to the original one. Figure \ref{fig:mmlu-pro-test-downsample} shows the distribution before/after the downsampling.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/mmlu-pro-test-downsample.pdf}
    \vspace{-20pt}
    \caption{MMLU-Pro test distribution before/after downsampling for the MMLU-Pro-1k subset. The subset is i.i.d. to the full set.}
    \label{fig:mmlu-pro-test-downsample}
\end{figure}

\paragraph{Statistical Metrics} We calculate the average accuracy with at least 4 random seeds. To tame the variance caused by the small size of AIME 2024, we sample 16 responses per prompt.

\paragraph{Implementation} We adopt the vLLM library to accelerate the inference and SymEval\footnote{\url{https://github.com/tongyx361/symeval}}, an elaborate answer grader capable of processing complex mathematical objects like matrices and functions, keeping consistent with the sampling and reward implementation in our RL setup. Note that a few RL experiments are carried out with an earlier version of the grader, causing nuanced performance differences.

\subsection{Details about Distillation}\label{app:distill}

To distill long CoT trajectories from \texttt{QwQ-32B-Preview}, we adopt the temperature $t=1.0$, the top-$p$ value of 0.95 and the maximum output length of 8192 tokens. Our preliminary experiments show that 8192 tokens show almost the same accuracy with \texttt{QwQ-32B-Preview} on MATH-500 as 16384 tokens, while costing significantly less time.

To distill short CoT trajectories from \texttt{Qwen2.5-Math-72B-Instruct}, we adopt the temperature $t=0.7$, the top-$p$ value of 0.95 and the maximum output length of 4096 tokens, since \texttt{Qwen2.5-Math-72B-Instruct} has a context limit of 4096 tokens and our preliminary experiments observe a non-negligible ratio of nonsense output when using $t=1.0$.

Note the data is distilled with SGLang \citep{zheng2024sglang} with an early version of our code.

When applying rejection sampling, we adopt the SymEval verifier as the grader.

\subsection{Details abour SFT Setup}\label{app:sft-setup}

We use OpenRLHF \citep{hu2024openrlhfeasytousescalablehighperformance} for our SFT experiments. By default, we adopt the SFT hyperparameters in Table \ref{tab:sft-hyperparams}.

For efficiency, we utilize Flash Attention 2 \citep{dao2023flashattention2} and ZeRO \citep{rajbhandari2020zero} based on the DeepSpeed library \citep{rasley2020deepspeed}. We uniformly set the micro batch size as 1 since we don't observe acceleration when increasing it.

\begin{table}[H]
\small
\caption{SFT Hyperparameters}
% \vspace{10pt}
\centering
\begin{tabular}{@{}cccccc@{}}
\toprule
    % \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Batch Size\end{tabular} &
    % \begin{tabular}[c]{@{}c@{}}Micro BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} \\
    \midrule
        % \makecell{Llama-3.1-8B \\ Llama-3.2-1B \\ Qwen2.5-32B \\ Qwen2.5-Math-7B} &
        256 &
        % 1 &
        128K &
        5e-6 &
        2 \\
    \midrule
\end{tabular}
\label{tab:sft-hyperparams}
\end{table}

\subsection{Details about RL Setup}\label{app:rl-setup}

We use OpenRLHF \cite{hu2024openrlhfeasytousescalablehighperformance} for our RL experiments. When describing hyperparameters, we adopt the same naming conventions as OpenRLHF.

\subsection{Experiment Hyperparameters}\label{app:exp-hyperparams}

Note that the \texttt{BS} column below refers to both \texttt{rollout\_batch\_size} (the number of prompts used in a sampling-training iteration) and \texttt{train\_batch\_size} (the number of samples used in a training update) because we adopt the same number for these two hyperparameters in most of our RL setups. Also, the \texttt{Samples} column refers to the number of samples per prompt.

\subsubsection{Details of Section \ref{sec:sft-init-for-rl} (SFT Initialization for RL)}
\label{app:exp-hyperparams-sft-init-for-rl}

SFT Data: CoT data distilled from \texttt{QwQ-32B-Preview} or \texttt{Qwen2.5-Math-72B-Instruct} with the MATH train split with different number of candidate responses per prompt.

\begin{table}[H]
\small
\caption{Hyperparameters}
\vspace{10pt}
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}GAE\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}KL\end{tabular} \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
\end{tabular}
\label{tab:exp-hyperparams-sft-init-for-rl}
\end{table}

\subsubsection{Details of Section \ref{result:reward-length-stability} (CoT Length Stability)}
\label{app:exp-hyperparams-reward-length-stability}

SFT Data: Long CoT data distilled from \texttt{QwQ-32B-Preview} with the MATH train split.

\begin{table}[H]
\small
\caption{Hyperparameters}
\vspace{10pt}
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}GAE\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}KL\end{tabular} \\
    \midrule
        Llama3.1-8B &
        Correct: $+1$ &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        8 & 8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 4.5e-6} &
        0.01 \\
    \midrule
        Qwen2.5-Math-7B &
        Correct: $+1$ &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        8 & 8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 4.5e-6} &
        0.01 \\
    \midrule
\end{tabular}
\label{fig:exp-hyperparams-reward-length-stability}
\end{table}

\newpage
\subsubsection{Details of Section \ref{result:reward-length-scaling} (Active Scaling of CoT Length)}
\label{app:exp-hyperparams-reward-length-scaling}

SFT Data: Long CoT data distilled from \texttt{QwQ-32B-Preview} with the MATH train split.

\begin{table}[H]
\small
\caption{Hyperparameters}
\vspace{10pt}
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}GAE\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}KL\end{tabular} \\
    \midrule
        Llama3.1-8B &
        Correct: $+1$ &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        8 & 8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 4.5e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{Cosine: \\ $r_0^c=+2$ \\ $r_L^c=+1$ \\ $r_0^w=-10$ \\ $r_L^w=0$ \\ $r_e=-10$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        8 & 8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 4.5e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        Correct: $+1$ &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        8 & 16 & 512 & 2 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        8 & 16 & 512 & 2 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        8 & 16 & 512 & 2 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
\end{tabular}
\label{fig:exp-hyperparams-reward-length-scaling}
\end{table}

\newpage
\subsubsection{Details of Section \ref{result:reward-cosine-hyperparams} (Cosine Reward Hyperparameters)}
\label{app:exp-hyperparams-reward-cosine-hyperparams}

SFT Data: Long CoT data distilled from \texttt{QwQ-32B-Preview} with the MATH train split.

\begin{table}[H]
\small
\caption{Hyperparameters}
\vspace{10pt}
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}GAE\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}KL\end{tabular} \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=0$ \\
            $r_L^c=+10$ \\
            $r_0^w=0$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+6$ \\
            $r_L^c=+5$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+10$ \\
            $r_L^c=+9$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
\end{tabular}
\label{fig:exp-hyperparams-reward-cosine-hyperparams}
\end{table}

\newpage
\subsubsection{Details of Section \ref{result:reward-context-window} (Context Window Size)}
\label{app:exp-hyperparams-reward-context-window}

SFT Data: Long CoT data distilled from \texttt{QwQ-32B-Preview} with the MATH train split.

\begin{table}[H]
\small
\caption{Hyperparameters}
\vspace{10pt}
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}GAE\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}KL\end{tabular} \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        8 & 8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 2048} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        8 & 8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 6144} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        8 & 8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
\end{tabular}
\label{fig:exp-hyperparams-reward-context-window}
\end{table}

\subsubsection{Details of Section \ref{result:reward-hacking} (Length Reward Hacking)}
\label{app:exp-hyperparams-reward-hacking}

SFT Data: Long CoT data distilled from \texttt{QwQ-32B-Preview} with the MATH train split.

\begin{table}[H]
\small
\caption{Hyperparameters}
\vspace{10pt}
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}GAE\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}KL\end{tabular} \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        8 & 16 & 512 & 2 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        8 & 16 & 512 & 2 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
\end{tabular}
\label{fig:exp-hyperparams-reward-hacking}
\end{table}

\subsubsection{Details of Section \ref{result:optimal-discount} (Optimal Discount Factors)}
\label{app:exp-hyperparams-optimal-discount}

SFT Data: Long CoT data distilled from \texttt{QwQ-32B-Preview} with the MATH train split.

\begin{table}[H]
\small
\caption{Hyperparameters}
%\vspace{5pt}
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}GAE\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}KL\end{tabular} \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=1$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.999$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=0.999$ \\
            $\gamma_p=0.999$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=0.999$ \\
            $\gamma_p=0.99$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=0.99$ \\
            $\gamma_p=0.99$} &
        4 & 4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
\end{tabular}
\label{fig:exp-hyperparams-optimal-discount}
\end{table}


\subsubsection{Details of Section \ref{result:reward-verify-clean} (RL with Noisy Verifiable Data)}
\label{app:exp-hyperparams-reward-verify-clean}

SFT Data: 115k filtered from 462k instances of long CoT data distilled from \texttt{QwQ-32B-Preview} with WebInstruct.

\begin{table}[H]
\small
\caption{Hyperparameters}
\vspace{10pt}
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}RL Prompt Set \\ Verifier\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}GAE\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes \\ Instances\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR \\ KL\end{tabular}\\
    \midrule
        Llama3.1-8B &
        \makecell{
            Unfiltered \\
            (30k sampled) \\
            Symeval} &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        \makecell{
            1 \\
            30k instances} &
        4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6 \\
        KL: 0.01} \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Unfiltered \\
            (30k sampled) \\
            LLM-as-a-judge} &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        \makecell{
            1 \\
            30k instances} &
        4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6 \\
        KL: 0.01} \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Filtered \\
            (30k sampled) \\
            Symeval} &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        \makecell{
            1 \\
            30k instances} &
        4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6 \\
        KL: 0.01} \\
    \midrule
        Llama3.1-8B &
        \makecell{
            Filtered \\
            (30k sampled) \\
            LLM-as-a-judge} &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma_c=1$ \\
            $\gamma_p=0.99$} &
        \makecell{
            1 \\
            30k instances} &
        4 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6 \\
        KL: 0.01} \\
    \midrule
\end{tabular}
\label{fig:exp-hyperparams-reward-verify-clean}
\end{table}

\subsubsection{Details of Section \ref{sec:rl-from-base} (Exploration on RL from the Base Model)}
\label{app:exp-hyperparams-rl-from-base}

\begin{table}[H]
\small
\caption{Hyperparameters}
\vspace{10pt}
\centering
\begin{tabular}{@{}cccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}GAE\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}KL\end{tabular} \\
    \midrule
        Qwen2.5-Math-7B &
        \makecell{
            Correct: $+1$ \\
            Wrong: $-0.5$ \\
            No Answer: $-1$} &
        \makecell{
            $\lambda=0.95$ \\
            $\gamma=1$} &
        20 & 8 & 
        \makecell{
            1024 \\
            (Train: 128)
        }
        & 1 &
        \makecell{Prompt: 1024 \\ Gen: 3072} &
        \makecell{Actor: 5e-7 \\ Critic: 9e-6} &
        0.01 \\
    \midrule
        Qwen2.5-Math-7B &
        Correct: $+1$ &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        8 & 8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336 } &
        \makecell{Actor: 5e-7 \\ Critic: 4.5e-6} &
        0.01 \\
    \midrule
        Qwen2.5-Math-7B &
        \makecell{
            Cosine: \\
            $r_0^c=+2$ \\
            $r_L^c=+1$ \\
            $r_0^w=-10$ \\
            $r_L^w=0$ \\
            $r_e=-10$ \\
            Rep. Penalty: \\
            $P=-0.05$ \\
            $N=40$} &
        \makecell{
            $\lambda=1$ \\
            $\gamma=1$} &
        8 & 8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        \makecell{Actor: 5e-7 \\ Critic: 4.5e-6} &
        0.01 \\
    \midrule
\end{tabular}
\label{tab:exp-hyperparams-rl-from-base}
\end{table}

\subsubsection{Details of Section \ref{result:reward-reinforce} (REINFORCE is more tricky to tune than PPO)}
\label{app:exp-hyperparams-reward-reinforce}

SFT Data: Long CoT data distilled from \texttt{QwQ-32B-Preview} with the MATH train split.

\begin{table}[H]
\small
\caption{Hyperparameters}
\vspace{10pt}
\centering
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
    \begin{tabular}[c]{@{}c@{}}Base Model\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Rewards\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Gamma\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Episodes\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Samples\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}BS\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Epochs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Context Length\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}LR\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}KL\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Clip\end{tabular} \\
    \midrule
        Llama3.1-8B &
        Correct: $+1$ &
        $\gamma=1$ &
        \makecell{8 \\ (stopped early)} &
        8 & 512 & 1 &
        \makecell{Prompt: 2048 \\ Gen: 14336} &
        5e-7 &
        0.01 &
        0.1 \\
    \midrule
\end{tabular}
\label{fig:exp-hyperparams-reward-reinforce}
\end{table}

\subsection{Implementation of the Model-Based Verifier}\label{app:model-based-verifier}

We used \texttt{Qwen2.5-7B-Instruct} as our model-based verifier. It was provided with both the reference answer and the suffix of the long CoT. We truncated the long CoT to avoid confusing the verifier. We used the following prompt.

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Prompt Template for Model-Based Verifier}]
\begin{lstlisting}[columns=flexible,breaklines=true,basicstyle=\small\tt]
Given the following last 20 lines of the LLM response to a math question
and the reference solution to that question, evaluate if the LLM response is correct based only on the LLM's final answer.

LLM response (last 20 lines):
...
{out}

Reference solution:
{ref}

Explain your thought process step-by-step before responding with `Judgement: <correct/wrong/not_found>`
\end{lstlisting}
\end{tcolorbox}

\subsection{Implementation of Short-Form Answer Extraction}\label{app:ans-extract}

We use the \texttt{Llama-3.1-8B-Instruct} model to extract short-form answer from QA pairs in WebInstruct, with the following prompt template:

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Prompt Template for Short-Form Answer Extraction}]
\begin{lstlisting}[columns=flexible,breaklines=true,basicstyle=\small\tt]
Problem: {Problem}

Solution: {Solution}

Based on the Problem and the Solution, extract a short final answer that is easy to check.
Provide the short final answer in the format of "The final answer is $$
\boxed{...}
$$" 
- If the answer is a mathematical object, write it in LaTeX, e.g., "The final answer is $$
\boxed{\frac{1}{2}}
$$"
- If the answer is a boolean, write it as "True" or "False", e.g., "The final answer is $$
\boxed{True}
$$"
- If the Problem can't be answered in a short form, respond with "" like "The final answer is $$
\boxed{}
$$"
\end{lstlisting}
\end{tcolorbox}

For generation parameters, we use temperature $t=0$ (greedy decoding) and set the maximum output length as 512 tokens.

After generation, we simply extract the short-form answer from within the \texttt{\textbackslash boxed\{...\}}.

\subsection{Action Prompting Framework}
\label{app:action-prompting}

We studied the publicly released CoTs of \texttt{o1-preview} and identified that its thoughts could be categorized into a few types of actions (listed below). To construct long CoTs, we designed prompts for each of these actions and implemented a multi-step prompting framework to sequence them. The framework ceded control flow of the CoT to the LLM, with the LLM making branching or looping decisions while the framework acted more passively as a state machine reacting to the LLM outputs. The framework took care of the boilerplate around constructing the CoT with an append-only log and managed all of the orchestration.

\begin{itemize}
\item \texttt{clarify}: Making some observations about the problem in order to identify an approach to solve it.
\item \texttt{decompose}: Breaking the current problem down into smaller and easier sub-problems to solve.
\item \texttt{solution\_step}: Computing a single step in the solution. In the context of math, this could be doing some arithmetic or symbolic manipulation.
\item \texttt{reflection}: Evaluating the current approach and partial solution to see if any mistakes were made, any sub-goals were achieved, or if alternative approaches should be considered instead. Note that we used a strong teacher model \texttt{o1-mini} for the \texttt{reflection} action as that one was a more difficult prompt to respond to correctly as it requires self-correction.
\item \texttt{answer}: Responding with a final answer and terminating the CoT.
\end{itemize}

\subsubsection{Control Flow}

Simplified description of the interaction between the framework and LLM.

\begin{algorithm}[H]
\caption{Action Prompting State Machine}\label{alg:problem-solving-state-machine}
\begin{algorithmic}[1]
    \STATE {\bfseries Input:} $prompt$
    \STATE {\bfseries Output:} $chain\_of\_thought$ sequence
    \STATE $chain\_of\_thought \leftarrow \text{[$prompt$]}$ \COMMENT{Initialize singleton chain of thought sequence from prompt}
    \STATE $state \leftarrow \text{ ``clarify''}$
    \WHILE{$\text{True}$}
        \IF{$state = \text{``clarify''}$}
            \STATE $output \leftarrow \text{prompt\_action\_clarify}()$
            \STATE $(state, thought) \leftarrow \text{parse}(output)$
            \STATE $chain\_of\_thought.\text{append}(thought)$
        \ELSIF{$state = \text{``decompose''}$}
            \STATE $output \leftarrow \text{prompt\_action\_decompose}()$
            \STATE $(state, thought) \leftarrow \text{parse}(output)$
            \STATE $chain\_of\_thought.\text{append}(thought)$
        \ELSIF{$state = \text{``solution\_step''}$}
            \STATE $output \leftarrow \text{prompt\_action\_solution\_step}()$
            \STATE $(state, thought) \leftarrow \text{parse}(output)$
            \STATE $chain\_of\_thought.\text{append}(thought)$
        \ELSIF{$state = \text{``reflection''}$}
            \STATE $output \leftarrow \text{prompt\_action\_reflection}()$
            \STATE $(state, thought) \leftarrow \text{parse}(output)$
            \STATE $chain\_of\_thought.\text{append}(thought)$
        \ELSIF{$state = \text{``answer''}$}
            \STATE $output \leftarrow \text{prompt\_action\_answer}()$
            \STATE $(state, thought) \leftarrow \text{parse}(output)$
            \STATE $chain\_of\_thought.\text{append}(thought)$
            \STATE {\bfseries return} $chain\_of\_thought$ \COMMENT{Terminate after answer action}
        \ENDIF
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsubsection{Action Prompting Templates}

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Action: Clarify}]
\begin{lstlisting}[columns=flexible,breaklines=true,basicstyle=\small\tt]
You are a very talented mathematics professor.
In a few sentences, VERY CONCISELY rephrase the problem to clarify its meaning and explicitly state what needs to be solved. Highlight any assumptions, constraints and potential misinterpretations.
Do NOT attempt to solve the problem yet -- you are just clarifying the problem in your mind.

<problem>
{goal}
</problem>

Answer in the following format:

<clarification>
Problem clarification as instructed above
</clarification>
<goal>
Summarize the problem into a single statement describing the goal, e.g. Find the value of the variable w.
</goal>
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Action: Decompose}]
\begin{lstlisting}[columns=flexible,breaklines=true,basicstyle=\small\tt]
You are a talented mathematics professor.
You already have a partial solution to a problem.
In a single sentence, propose candidates for the next subgoal as the next step of the partial solution that will help you make progress towards the current goal.
Do not repeat any subgoal, we don't want any infinite loops!
Do not suggest using a computer or software tools.

<current goal>
{current_goal}
</current goal>
<parent goal>
{parent_goal}
</parent goal>
<partial solution>
{solution}
</partial solution>

Format your answer as follows:

<thinking>
step-by-step thinking of what the next possible subgoal should be, as well as some other alternatives that might also work
remember, we want to solve the parent goal WITHOUT repeating the subgoals that are already DONE.
do not suggest verification or checking.
{parent_goal}
</thinking>
<sentence>
single sentence describing the subgoal
phrase it as if you were thinking to yourself and are considering this as a hypothesis (don't express too much certainty)
</sentence>
<sentence>
single sentence describing an *ALTERNATIVE* subgoal, without repeating previous ones
start off with "Alternatively,"
</sentence>
<sentence>
single sentence describing an *ALTERNATIVE* subgoal, without repeating previous ones
start off with "Alternatively,"
</sentence>
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Action: Solution Step}]
\begin{lstlisting}[columns=flexible,breaklines=true,basicstyle=\small\tt]
You are an extremely PEDANTIC mathematics professor who loves to nitpick.
You already have a partial solution to a problem. Your task is to solve *only* the current goal.
You should include symbols and numbers in every sentence if possible.

<current goal>
{current_goal}
</current goal>
<partial solution>
{solution}
</partial solution>

BE VERY CONCISE. Include calculations and equations in your response if possible, and make sure to solve them instead of just describing them.
DO NOT SOLVE THE WHOLE QUESTION, JUST THE CURRENT GOAL: {current_goal}
Do not repeat any calculations that were already in this prior step:
{prior_step}
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Action: Reflection}]
\begin{lstlisting}[columns=flexible,breaklines=true,basicstyle=\small\tt]
You are a talented mathematics professor.
You already have a partial solution to a math problem.
Verify whether the current subgoal has been achieved.

<current goal>
{current_goal}
</current goal>
{parent_goal}
<partial solution>
{solution}
</partial solution>

Format your answer as follows:

<verification>
Come up with a quick, simple and easy calculation to double check that the solution is correct.
This calculation should not re-compute the solution in the same way, as that would defeat the purpose of double-checking.
Use one of the following strategies:
- An easier, alternative method to arrive at the answer
- Substituting specific values into equations and checking for consistency
- Working backwards from the answer to derive the given inputs and then checking for consistency
Be consise. Do not suggest using a computer.
At the end of your verification, restate the answer from the current solution. Do not calculate it if it hasn't been solved.
Phrase it as if you are reflecting as you solve the problem.
</verification>
<current_goal_achieved>
true or false, depending on whether the solution is correct and the current goal has been achieved: {current_goal}
</current_goal_achieved>
<parent_goal_achieved>
true or false, depending on whether the parent goal has been achieved:
{parent_goal.target}
</parent_goal_achieved>
<new_goal>
If the solution is not correct or the current goal has not been achieved, suggest an alternative current goal here in a single sentence.
Start off with "Alternatively,"
Your goal should be sufficiently different from subgoals that have been solved or that have timed out:
{parent_goal_tree}
</new_goal>
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Action: Answer}]
\begin{lstlisting}[columns=flexible,breaklines=true,basicstyle=\small\tt]
Extract the final answer, making sure to obey the formatting instructions.
Solution:
{solution}

Formatting instructions:
{format}
\end{lstlisting}
\end{tcolorbox}
\newpage

\section{Long CoT Patterns in Pre-training Data}
\subsection{Snapshot of webpages}

Source: \href{https://brilliant.org/wiki/verify-solutions/}{brilliant.org}

The following two examples demonstrate how explicit verification after answering a question can naturally exist on a webpage.

\begin{tcolorbox}[label=webpage:explicit-revision-correct, colback=lightgray!10, colframe=black, title={Explicit verification}]
\(x+7=10\)

This problem can be solved by subtracting 7 from each side.

\(x+7-7=10-7\)

\(x=3\)

Once the problem is solved, the solution can be verified by rewriting the problem with 3 substituted for \(x\).

\(3+7=10\)

\(10=10\)

Both sides are equal, verifying that \(x=3\) is a valid solution.
\end{tcolorbox}

% Source: https://brilliant.org/wiki/verify-solutions/

\begin{tcolorbox}[label=webpage:explicit-revision-wrong, colback=lightgray!10, colframe=black, title={Explicit verification that found an error}]
\(x+7=10\)
A student rushing through her homework might mistakenly write \(x=2\) as the solution to this problem. If she takes a moment to rework the equation with her answer, she will realize the answer is incorrect.

\(x+7=10\)

\(2+7=10\)

\(9=10\)

Since \(9\neq10\), the student knows she needs to go back and find a different solution to the problem.
\end{tcolorbox}

\newpage

Source: \href{https://kidswholovemath.substack.com/}{kidswholovemath.substack.com}

\begin{tcolorbox}[label=webpage:multi-solutions, colback=lightgray!10, colframe=black, title={Attempt the question from different perspective}]
\textbf{The Double Check Game} \\
Regardless of the scenario, we can play the double check game! \\
The game is simple: we try to solve the problem in as many different ways as possible.

\textbf{Elementary School Example} \\
Math problem is: $78 - 57 = ?$

To play the game, we try to solve the problem in as many different ways as possible.

\textbf{The first solution:} \\
$? = 78 - 57$ \\
Break apart the 57: \\
$? = 78 - 50 - 7$ \\
$? = 28 - 7$ \\
$? = 21$

\textbf{A second solution:} \\
$? = 78 - 57$ \\
Subtract an easier number from 78: \\
$? = 78 - 60 + 3$ \\
$? = 18 + 3$ \\
$? = 21$

\textbf{A third solution:} \\
$? = 78 - 57$ \\
Subtract 57 from an easier number: \\
$? = 80 - 57 - 2$ \\
$? = 23 - 2$ \\
$? = 21$

...
\end{tcolorbox}

\newpage

\subsection{OpenWebMath}
\label{app:open-web-math}

\subsubsection{Queries}
\label{app:open-web-math-queries}

We used \texttt{GPT-4o} to generate examples of typical pivot keywords found in long CoT. These were used to find documents in OpenWebMath that have interesting properties characteristic of long CoT trajectories.
\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={"Aha" Phrases}]
\begin{verbatim}
"Let's think step by step."
"Let's go through this one step at a time."
"Breaking it down step by step..."
"Thinking about it logically, first..."
"Step 1: Let's figure out the starting point."
"If we follow the steps carefully, we get..."
"To solve this, lets analyze it piece by piece."
"Going through this systematically, we have..."
"Okay, lets solve this gradually."
"Does that make sense?"
"Is this correct?"
"Wait, does that check out?"
"Am I missing something?"
"Hmm does that work?"
"Let me verify that."
"That makes sense, right?"
"Hold on, is this right?"
"Lets double-check this."
"Wait, actually..."
"Oh, hold on..."
"Wait a second..."
"Actually, let me rethink that."
"Hmm, let me go back for a moment."
"I might need to check this again."
"Let's pause and reassess."
"Lets check by doing the reverse."
"Let's verify by working backward."
"Can we check this by reversing the process?"
"To confirm, let's undo the steps."
"A good way to verify is by reversing it."
"If we undo the operations, do we get the same result?"
...
\end{verbatim}
\end{tcolorbox}

\newpage

\subsubsection{Matches}
\label{app:open-web-math-matches}


Source: \href{https://discourse.mc-stan.org/t/interpretation-of-multilevel-parameters/20846}{MC Stan Discussion Forum}

The discussion below took place on a message board for the probabilistic programming framework MC Stan. The user Tiny has a question about how to interpret some data and multiple other users are responding. We can see the usual pivot keywords (highlighted in \textbf{bold}) characteristic of long CoT, including branching, self-correction and even an assessment of the feasibility of an approach.

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Discussion on message board}]
% \begin{verbatim}
% \begin{Verbatim}[breaklines=true]
% \begin{minted}[breaklines=true, fontsize=\small,escapeinside=||]{text}
\begin{lstlisting}
So the question is then to find the right prediction task, looking at your setup, those may include:

...

 For a hypothetical future serial drawn from the same population as the observed serials. (i.e. include the varying intercept via a new level and sample_new_levels = uncertainty)
 For the true or average underlying system (i.e. ignore the varying intercept)
 In the experiments you actually observed (i.e. include the fitted varying intercepts for your experiments)

But you could also ask other stuff, like:

 What is the expected difference in some of the constants (or anything else) between two future experiments?

All of those (and more) should be answerable using the posterior of the model. But you still need to figure out which questions do you actually want to ask, as there is a lot of options

|\textbf{Does that make sense?}|

Best of luck with your model!

...

I am not sure I follow your thought here, but |\textbf{maybe thats just because I would have worded it differently}|?

...

|\textbf{An alternative approach would be to try}| to find a different parametrization of the model where the parameters are interpretable separately, |\textbf{but that might be hard.}|

Also, if this is the parametrization of the process used by many in the field, than maybe poeple would expect you to report as (\frac {L} {mol})^{n-1} s^{-1}, because thats what everybody has been doing (although possibly with fixed n)?

|\textbf{Does that make sense?}|

|\textbf{Can you not just}| recast the model (with modified parameters) as

...
\end{lstlisting}
% \end{minted}
% \end{Verbatim}
\end{tcolorbox}

\newpage

Source: \href{https://www.physicsforums.com/threads/cylinder-in-3-d.934285/}{physicsforums.com}

The discussion below took place on a physics forum. The user Songoku is asking for help with homework and another user BvU is trying to assist without revealing the solution directly. We see the usual pivot keywords indicating self-reflection, expression of uncertainty and formulation of hypotheses.

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Discussion on a physics forum}]
% \begin{minted}[breaklines=true, fontsize=\small,escapeinside=||]{text}
\begin{lstlisting}
# Cylinder in 3 D
1. Dec 13, 2017

### songoku
1. The problem statement, all variables and given/known data
Let r be a positive constant. Consider the cylinder x2 + y2 <= r2, and let C be the part of the cylinder that satisfies
0 <= z <= y.
(1) Consider the cross section of C by the plane x = t (-r <= t <= r), and express its area in terms of r, t.
(2) Calculate the volume of C, and express it in terms of r.

...

5. Dec 13, 2017
### BvU
Simple case: x = 0. So -1 <= y <= 1. In the yz plane 0 <= z <= y is a triangle.
What about y ?

6. Dec 13, 2017
### songoku
|\textbf{I think I am missing something here}| because I feel I can't really grasp the hint given.
Let me start from the basic again:
1. Let the x - axis horizontal, y - axis vertical and z - axis in / out of page. I imagine there is circle on xy plane with radius r then it extends out of page (I take out of page as z+) to form 3 D cylinder. |\textbf{Is this correct?}|
2. Plane x = t is like the shape of a piece of paper hold vertically with the face of paper facing x - axis (I mean x - axis is the normal of the plane). |\textbf{Is this correct?}|
Thanks

7. Dec 14, 2017
### BvU
Yes

8. Dec 14, 2017
### songoku

"Consider the cross section of C by plane x = t" means plane x = t cuts the cylinder?
And the intersection will be rectangle?
...
\end{lstlisting}
% \end{minted}
\end{tcolorbox}

\newpage

Source: \href{https://math.stackexchange.com/questions/2938153/probability-that-we-stop-flipping-after-exactly-ten-flips-in-a-biased-coin-flipp/2938356}{StackExchange}

The user Baymax is asking for help on a probability problem and we see dialogue with another user Lulu. We see that the quick back-and-forth between them is similar to the kind of nimble branching behavior in long CoT where multiple solutions are quickly assessed and considered. We also see an expression of realization which can be easily re-cast as self-verification in a long CoT.  

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Discussion on Stack Exchange}]
% \begin{minted}[breaklines=true, fontsize=\small,escapeinside=||]{text}
\begin{lstlisting}
# probability that we stop flipping after exactly ten flips in a biased coin flipping?

...

I thought that let us fix of getting a third head at last that is at 10th flip, so that we would stop there, and the remaining - getting two heads can be accommodated in the 9 trials. so there are $$9$$ choose 2 ways of getting two heads so the probability that we stop flipping after exactly ten flips is $$^9C_{2}$$ . $$\frac{1}{4}^3$$.$$\frac{3}{4}^7$$. |\textbf{Is this correct?}|

EDIT - Now the probability of getting exactly 3 heads? I got it to be $$^{10} C_{3} \frac{1}{4}^3 \frac{3}{4}^7$$. Should we get the same as the previous one? any reason why they should/should not be same?

 I think you switched $P(H),P(T)$ |\textbf{but the approach is good.}|  lulu Oct 1 '18 at 16:13
 |\textbf{oh i see now!}| thanks!  BAYMAX Oct 1 '18 at 16:13
 @lulu please see the edit  BAYMAX Oct 1 '18 at 16:30
 Your probability for exactly 3 heads is right as well. It should be obvious why the results have to be different. In the first case the outcome of the last flip is fix and in the second case the outcome of the last flip is not fix.  callculus Oct 1 '18 at 16:31

...
\end{lstlisting}
% \end{minted}
\end{tcolorbox}

\newpage

Source: \href{https://puzzling.stackexchange.com/questions/274/choosing-units-for-drug-testing}{StackExchange}

User88 interacts with multiple other users. Observe that they are helping to clarify each others' doubts, which is reminiscent of self-correction in long CoT trajectories.

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Discussion on Stack Exchange}]
% \begin{minted}[breaklines=true, fontsize=\small,escapeinside=||]{text}
\begin{lstlisting}
Choosing units for drug testing

Here's a third puzzle that I found in a book, slightly paraphrased because I don't entirely remember the format of the original.

...

How can he arrange the dosage amounts so that he ends up using all 25 test packages, and the total units of dosage used in the tests are as low as possible?

The book had the answer, but one, it didn't explain how the answer was arrived at, and two, I don't remember what the answer was and no longer have that book with me.

 |\textbf{Am I missing something}|, or is the goal just to find 25 coprime numbers from 25 to 50?  Aza May 20 '14 at 4:33
 They don't have to be coprime. There just can't be any two where one is a factor of the other. And the range is from 1 to 50, not 25 to 50.  Joe Z. May 20 '14 at 4:34
 |\textbf{Wouldn't a single test}| of 1 unit technically satisfy the requirement? Or |\textbf{am I missing something? Ah, I guess you have to}| perform exactly 25 tests.  arshajii May 20 '14 at 14:28
 |\textbf{Yea.}| Wouldn't 1 win?  awesomepi May 20 '14 at 19:24
 You have to use all 25 tests.  Joe Z. May 20 '14 at 19:31

By logically starting from 26-50 and trying to shrink them one by one you can easily show: $8,12,14,17,18,19,20,21,22,23,25,26,27,29,30,31,33,35,37,39,41,43,45,47,49$

Which equals $711$

...
\end{lstlisting}
% \end{minted}
\end{tcolorbox}