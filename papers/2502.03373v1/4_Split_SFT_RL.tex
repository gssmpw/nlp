\section{Scaling up Verifiable Reward}\label{sec:silver-data}

Verifiable reward signals like ones based on ground-truth answers are essential for stabilizing long CoT RL for reasoning tasks. However, it is difficult to scale up such data due to the limited availability of high-quality human-annotated verifiable data for reasoning tasks. As an attempt to counter this, we explore using other data that is more available despite more noise, like reasoning-related QA pairs extracted from web corpora. Specifically, we experiment with the WebInstruct dataset \citep{yue2024mammoth2}. For efficiency, we construct WebInstruct-462k, a deduplicated subset derived via MinHash \citep{broder1998minhash}. 

\subsection{SFT with Noisy Verifiable Data}\label{sec:sft-with-noisy-verifiable-data}

We first explore adding such diverse data to SFT. Intuitively, despite less reliable supervision signals, diverse data might facilitate the model’s exploration during RL.

\noindent\textbf{Setup.} We experiment with three setups, varying the proportion of data without gold supervision signals: 0\%, 100\%, and approximately 50\%. We conduct long CoT SFT by distilling from \texttt{QwQ-32B-Preview}. For data with gold supervision signals (MATH), ground truth answers are used for rejection sampling. In contrast, for data from WebInstruct without fully reliable supervision signals but with a much larger scale, we sample one response per prompt from the teacher model without filtration. For RL here, we adopt the same setup as in \textsection\ref{sec:sft-init-for-rl}, using the MATH training set.

\noindent\textbf{Result.} Table \ref{tab:diverse-silver-improve-general-reasoning} shows that incorporating silver-supervised data improves average performance. Adding WebInstruct data to long CoT SFT yields a substantial 5–10\% absolute accuracy gain on MMLU-Pro-1k over using MATH alone. Furthermore, mixing MATH and WebInstruct data achieves the best average accuracy across benchmarks.

\begin{table}[htbp]
\vspace{-10pt}
\caption{
Adding data with a silver supervision signal is often beneficial.
``WebIT'' is the abbreviation of WebInstruct.
}
\vspace{5pt}
\label{tab:diverse-silver-improve-general-reasoning}
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}llccccc@{}}
\toprule
Long CoT & Training & MATH & AIME & Theo. & MMLU & \multirow{2}{*}{AVG} \\
SFT Data & Method & 500 & 2024 & QA & Pro-1k \\
\midrule
% \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}MATH\\-RS-32\end{tabular}}
\multirow{2}{*}{100\% MATH}
 & SFT & 54.1 & 3.5 & 21.8 & 32.0 & 27.9 \\
 & SFT + RL & \textbf{59.4} & 4.0 & \textbf{25.2} & 34.6 & 30.8 \\
\midrule
% \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}WebIT-231k\\-Distill\end{tabular}}
\multirow{2}{*}{100\% WebIT}
& SFT & 41.2 & 0.8 & 21.9 & 41.1 & 26.3 \\
& SFT + RL & 44.6 & 1.9 & 22.5 & \textbf{43.3} & 28.1 \\
\midrule
50\% MATH & SFT & 53.6 & \textbf{4.4} & 23.5 & 41.7 & 30.8 \\
+ 50\% WebIT & SFT + RL & 57.3 & 3.8 & 25.1 & 42.0 & \textbf{32.1} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{AIbox}{Takeaway 5.1 for SFT with Noisy Verifiable Data}
Adding noisy but diverse data to SFT leads balanced performance across different tasks. (Table \ref{tab:diverse-silver-improve-general-reasoning})
\end{AIbox}

\subsection{Scaling up RL with Noisy Verifiable Data}
\label{result:reward-verify-clean}

We compare two main approaches to obtain rewards from noisy verifiable data: 1) to extract short-form answers and use a rule-based verifier; 2) to use a model-based verifier capable of processing free-form responses. 

Here a key factor is whether the QA pair can have a short-form answer. So we also compare whether the dataset is filtered by only retaining samples with short-form answers. 

\noindent\textbf{Setup.}
We implement the model-based verifier by prompting \texttt{Qwen2.5-Math-7B-Instruct} with the raw reference solution.
To extract short-form answers, we first prompt \texttt{Llama-3.1-8B-Instruct} to extract from the raw responses and then apply rejection sampling with \texttt{QwQ-32B-Preview}. Specifically, we generate two responses per prompt from WebInstruct-462k and discard cases where neither response aligns with the extracted reference answers. This process yields approximately 189k responses across 115k unique prompts.
Our case studies show that the rejection sampling drops many prompts due to:
1) many WebInstruct prompts lack short-form answers that our rule-based verifier can process effectively,
and 2) some prompts are too difficult even for \texttt{QwQ-32B-Preview}.
For SFT we train \texttt{Llama-3.1-8B} on the filtered dataset as initialization for reinforcement learning (RL).
In the RL stage, we use the full 462k prompt set in the unfiltered setup and the 115k subset in the filtered setup, training with 30k prompts and 4 responses per prompt.
Further details about the model-based verifier, the answer extraction and the RL hyperparameters can be found in Appendix  \& \ref{app:exp-hyperparams-reward-verify-clean} \& \ref{app:model-based-verifier} \& \ref{app:ans-extract} respectively.

\begin{table}[htbp]
\vspace{-15pt}
\caption{Performance of RL with different verifiers and prompt filtering methods. All the models here are fine-tuned from \texttt{Llama-3.1-8B}. The ``MATH Baseline'' is the model trained with SFT and RL on MATH only in Table \ref{tab:diverse-silver-improve-general-reasoning}. The other models are trained with SFT by distillation from \texttt{QwQ-32B-Preview} and RL with different setups.}
\label{tab:verification-types}
\vskip 0.1in
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}llccccc@{}}
\toprule
Prompt & Verifier & MATH & AIME & Theo. & MMLU \\
Set & Type & 500 & 2024 & QA & Pro-1k \\
\midrule
\multicolumn{2}{c}{MATH Baseline} & 59.4 & 4.0 & 25.2 & 34.6 \\
\midrule
\multicolumn{2}{c}{SFT Initialization} & 46.6 & 1.0 & 23.0 & 28.3 \\
\midrule
\multirow{2}{*}{Unfiltered} & Rule-Based & 45.4 & 3.3 & 25.9 & 35.1 \\
 & Model-Based & 47.9 & 3.5 & 26.2 & 40.4 \\
\midrule
\multirow{2}{*}{Filtered} & Rule-Based & \textbf{48.6} & 3.3 & \textbf{28.1} & \textbf{41.4} \\
 & Model-Based & 47.9 & \textbf{3.8} & 26.9 & \textbf{41.4} \\
\bottomrule
\end{tabular}
}
\end{table}

\noindent\textbf{Result.} \autoref{tab:verification-types} shows that RL with the rule-based verifier on the filtered prompt set with short-form answers achieves the best performance across most benchmarks under the same number of RL samples. This might indicate that rule-based verifier after appropriate filtration can produce the highest-quality reward signals from noisy verifiable data.
Moreover, compared to the model trained on human-annotated verified data (MATH), leveraging noisy yet diverse verifiable data still significantly boosts performance on O.O.D. benchmarks, with absolute gains of up to 2.9\% on TheoremQA and 6.8\% on MMLU-Pro-1k. In contrast, applying a rule-based verifier to unfiltered data results in the worst performance.
This might be caused by its low training accuracy on free-form answers, while the model-based verifier achieves much better performance.

\begin{AIbox}{Takeaway \hypersetup{hidelinks}\ref{result:reward-verify-clean} for RL with Noisy Verifiable Data}
To obtain reward signals from noisy verifiable data, the ruled-based verifier after filtering the prompt set for short-form answers works the best. (Table \ref{tab:verification-types})
\end{AIbox}
