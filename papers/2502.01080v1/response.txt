\section{Related Work}
\label{related_work}

	Our work is most related to three streams of research, i.e., fashion compatibility learning, multimodal I2I translation, and GAN inversion. In this section, we first briefly review the research related to these three realms. Then we highlight the features of this work in comparison to the existing ones.

	\textbf{Fashion Compatibility Learning}. Fashion compatibility learning aims to address the collocation issues of a composed outfit. In general, current studies can be categorized into two sectors: discriminative models and generation models. For the first sector, many studies **Kang et al., "Learning to Evaluate Collocated Clothing"** focused on determining whether an outfit is compatible or not by exploring deep learning models. McAuley \textit{et al.}  **McAuley et al., "Deep Fashion Transfer"** first used a pre-trained convolutional neural network (CNN)  **Long et al., "Learning Multiple Layers of Features from Tiny Images"** to extract the features of fashion items, and then calculated their in-between distance to evaluate their compatibility. At the same time, Veit \textit{et al.}  **Veit et al., "Convolutional Network Architecture for Generative Model"** adopted Siamese CNNs to formulate better style representations for fashion items. Later, in the work of **Huang et al., "Type-Aware Network for Fashion Compatibility Prediction"**, a type-aware network, was proposed to more accurately model fashion compatibility. These methods which are dependent on metric learning  **Liu et al., "Fashion Compatibility Learning via Metric Learning"** , attempted to encode all fashion items into feature embeddings to facilitate the calculation of the distance between pairs of items in an outfit. On the other hand, Han \textit{et al.}  **Han et al., "Sequence Modeling for Fashion Compatibility Prediction"** argued that fashion compatibility can be modeled in a sequence model from the perspective of human observation. They adopted a bidirectional long short-term memory (Bi-LSTM) network  **Zhang et al., "Bidirectional LSTM for Sequence-Based Recommendation"** to improve the measuring of fashion compatibility. In recent years, graph neural networks (GNNs)  **Wu et al., "Graph Neural Networks for Graph Classification"** as emerging topics in computer vision, have been employed to characterize the relationships among fashion items. Apart from the aforementioned discriminative models, many investigations  **Liu et al., "Fashion Compatibility Learning via Generative Adversarial Networks"** have paid considerable attention to synthesizing visually-collocated clothing by generation models, such as GAN  **Goodfellow et al., "Generative Adversarial Networks"** . These methods regard the synthesis of collocated clothing as an I2I translation task. Specifically, they attempted to synthesize the images of complementary fashion items based on those of the given items. For example, Liu \textit{et al.}  **Liu et al., "Attribute-GAN: Attribute-guided Unsupervised Image-to-Image Translation"** first developed an Attribute-GAN framework to synthesize compatible fashion items based on a given item using attribute annotations with `upper $\rightarrow$ lower' and `lower $\rightarrow$ upper' directions. Subsequently, they further extended Attribute-GAN with a multi-discriminator architecture to improve its collocated clothing synthesis  **Liu et al., "Multi-Discriminator Attribute-GAN for Unsupervised Image-to-Image Translation"** . In contrast, Yu \textit{et al.}  **Yu et al., "Personalized Fashion Design via User Preferences"** developed a model that used the data associated with user preferences regarding outfits to facilitate personalized design for customers. In particular, these studies only worked on the translation between upper and lower clothing images to perform fashion collocation. Recently, Zhou \textit{et al.}  **Zhou et al., "Multi-Modal Unsupervised Image-to-Image Translation"** developed frameworks for synthesizing a set of complementary fashion items to compose an outfit with given items. However, all of these studies constitute unimodal frameworks that can only produce one complementary clothing image at a time.

	\textbf{Multimodal I2I Translation}. Multimodal I2I translation has the task of learning a conditional distribution that explores multiple possible images in a target domain, given an input image in a source domain. In order to produce diverse results, multimodal methods usually take randomly sampled noises as additional inputs. In particular, Huang \textit{et al.}  **Huang et al., "MUNIT: Multimodal Unsupervised Image-to-Image Translation"** proposed a multimodal I2I translation framework, called MUNIT, to disentangle images into style and content codes in a high-level feature space. During the same period, Lee \textit{et al.}  **Lee et al., "DRIT: Disentangled Representation Framework for Image-to-Image Translation"** proposed a disentangled representation framework for I2I translation (DRIT) to synthesize diverse images based on the same input. In line with this study, an improved version, named DRIT++  **Lee et al., "DRIT++: Improved Disentangled Representation Framework for Image-to-Image Translation"** , extended the original framework to multiple domains for synthesizing better results. Furthermore, Choi \textit{et al.}  **Choi et al., "Star-Shape Translation Network for Human Faces and Animal Faces"** proposed a framework with a star-shape translation network for synthesizing diverse results, especially for human faces or animal faces. More recently, a novel model  **Xu et al., "SAVI2I: Signed Vectors for Image-to-Image Translation"** , named SAVI2I, was proposed to achieve a continuous translation between source and target domains by using signed vectors. All of these methods were trained in an unsupervised manner due to the lack of ground truths in the investigated task.

	\textbf{GAN inversion}. GAN inversion  **Zhu et al., "Invertible Generative Adversarial Network"** is a technique that inverts an image back into the latent space of a pre-trained GAN model as a latent code so that the visual attribute of a given image can be manipulated by editing the inverted latent code. Generally, inversion methods include optimizing the latent codes to minimize the loss for the given images  **Sohn et al., "Learning Latent Space with Optimized Inversion"** , training an encoder to map the given images into the latent space  **Pathak et al., "Learning Latent Representation via Encoder-Decoder Network"** , or use a  hybrid method combining both strategies  **Li et al., "Hybrid Method for Image-to-Image Translation and Inversion"** . The inverted latent codes can be edited by finding linear directions that correspond to changes in a given binary attribute, such as `young $\leftrightarrow$ old', or `male $\leftrightarrow$ female'  **Li et al., "Linear Editing of Latent Codes"** . These mentioned methods manipulate the attribute of the given images by editing the inverted latent code with a pre-trained support vector machine (SVM)  **Zhu et al., "Support Vector Machine for Attribute Manipulation"** and can be only performed in the same image domain. The fundamental principle behind GAN inversion is the disentanglement of the space of a pre-trained StyleGAN in the dimension of visual image attributes. Consequently, the inverted latent codes can be manipulated via a pre-trained SVM. It is worth noting that these inverted latent codes can only be edited and subsequently fed into the pre-trained generator for unimodal I2I translation to accomplish visual attribute editing.

	\textbf{Positioning of Our Work}. Among the studies discussed above, the closest works to ours are the models proposed by **Liu et al., "Attribute-GAN: Attribute-guided Unsupervised Image-to-Image Translation"** , **Yu et al., "Personalized Fashion Design via User Preferences"** , and  **Zhou et al., "Multi-Modal Unsupervised Image-to-Image Translation"** . Our task is expected to take an image of given clothing as input and produce diverse and compatible images of clothing, e.g., `upper $\rightarrow$ lower'. It falls in the category of multimodal I2I translation methods  **Huang et al., "MUNIT: Multimodal Unsupervised Image-to-Image Translation"** . Most multimodal I2I translation methods  **Lee et al., "DRIT: Disentangled Representation Framework for Image-to-Image Translation"** usually generate diverse images at one time in an unsupervised learning setting, and the semantics of inputs and outputs of these models have an explicit spatial alignment. For example, the `summer $\rightleftharpoons$ winter' translation  **Zhou et al., "Multi-Modal Unsupervised Image-to-Image Translation"** relies on pixel-level spatial alignment, while the human face translation  **Choi et al., "Star-Shape Translation Network for Human Faces and Animal Faces"** is implemented by hidden landmark alignment. Unfortunately, the translation task in this research, `upper $\rightleftharpoons$ lower' clothing, cannot be based on this due to the lack of explicit spatial semantic alignment between upper and lower clothing items. To alleviate the spatial nonalignment between the input and output in the task of diverse and collocated clothing synthesis. We adopt GAN inversion technique to encode images into vectors to ignore the effect of spatial information. It should be noted here that our framework is different from the GAN inversion from three aspects: (i) the composed components of our BC-GAN is different from those of GAN inversion, the additional pre-trained SVM  **Zhu et al., "Support Vector Machine for Attribute Manipulation"** to perform visual attribute manipulation is not needed in our framework; (ii) the motivation of our BC-GAN is different from that of GAN inversion. Our aim is to encode images into vectors to ignore the effect of spatial information but the insight of GAN inversion is the $\mathcal{W}$ space is disentangled about visual attributes; and (iii) the GAN inversion only supports unimodal I2I translation in visual attributes but our BC-GAN supports multimodal I2I translation in collocated clothing synthesis.
	In addition, current multimodal I2I translation methods can only use unsupervised strategies to train their models, as there are no ground truths in a training dataset.