\section{Related Work}
\label{related_work}
	
	Our work is most related to three streams of research, i.e., fashion compatibility learning, multimodal I2I translation, and GAN inversion. In this section, we first briefly review the research related to these three realms. Then we highlight the features of this work in comparison to the existing ones.
	
	\textbf{Fashion Compatibility Learning}. Fashion compatibility learning aims to address the collocation issues of a composed outfit. In general, current studies can be categorized into two sectors: discriminative models and generation models. For the first sector, many studies \cite{mcauley2015image,veit2015learning,vasileva2018learning,han2017learning,cui2019dressing,li2020hierarchical} focused on determining whether an outfit is compatible or not by exploring deep learning models. McAuley \textit{et al.} \cite{mcauley2015image} first used a pre-trained convolutional neural network (CNN) \cite{zhao2020scgan,xie2021dual,peng2022robust} to extract the features of fashion items, and then calculated their in-between distance to evaluate their compatibility. At the same time, Veit \textit{et al.} \cite{veit2015learning} adopted Siamese CNNs to formulate better style representations for fashion items. Later, in the work of \cite{vasileva2018learning}, a type-aware network, was proposed to more accurately model fashion compatibility. These methods which are dependent on metric learning \cite{chopra2005learning}, attempted to encode all fashion items into feature embeddings to facilitate the calculation of the distance between pairs of items in an outfit. On the other hand, Han \textit{et al.} \cite{han2017learning} argued that fashion compatibility can be modeled in a sequence model from the perspective of human observation. They adopted a bidirectional long short-term memory (Bi-LSTM) network \cite{bi_lstm} to improve the measuring of fashion compatibility. In recent years, graph neural networks (GNNs) \cite{cui2019dressing,li2020hierarchical} as emerging topics in computer vision, have been employed to characterize the relationships among fashion items. Apart from the aforementioned discriminative models, many investigations \cite{liu2019toward,liu2019collocating,yu2019personalized,outfitgan,coutfitgan} have paid considerable attention to synthesizing visually-collocated clothing by generation models, such as GAN \cite{goodfellow2014generative}. These methods regard the synthesis of collocated clothing as an I2I translation task. Specifically, they attempted to synthesize the images of complementary fashion items based on those of the given items. For example, Liu \textit{et al.} \cite{liu2019toward} first developed an Attribute-GAN framework to synthesize compatible fashion items based on a given item using attribute annotations with `upper $\rightarrow$ lower' and `lower $\rightarrow$ upper' directions. Subsequently, they further extended Attribute-GAN with a multi-discriminator architecture to improve its collocated clothing synthesis \cite{liu2019collocating}. In contrast, Yu \textit{et al.} \cite{yu2019personalized} developed a model that used the data associated with user preferences regarding outfits to facilitate personalized design for customers. In particular, these studies only worked on the translation between upper and lower clothing images to perform fashion collocation. Recently, Zhou \textit{et al.} \cite{outfitgan,coutfitgan} developed frameworks for synthesizing a set of complementary fashion items to compose an outfit with given items. However, all of these studies constitute unimodal frameworks that can only produce one complementary clothing image at a time.
	
	
	\textbf{Multimodal I2I Translation}. Multimodal I2I translation has the task of learning a conditional distribution that explores multiple possible images in a target domain, given an input image in a source domain. In order to produce diverse results, multimodal methods usually take randomly sampled noises as additional inputs. In particular, Huang \textit{et al.} \cite{huang2018multimodal} proposed a multimodal I2I translation framework, called MUNIT, to disentangle images into style and content codes in a high-level feature space. During the same period, Lee \textit{et al.} \cite{lee2018diverse} proposed a disentangled representation framework for I2I translation (DRIT) to synthesize diverse images based on the same input. In line with this study, an improved version, named DRIT++ \cite{lee2020drit++}, extended the original framework to multiple domains for synthesizing better results. Furthermore, Choi \textit{et al.} \cite{choi2020starganv2} proposed a framework with a star-shape translation network for synthesizing diverse results, especially for human faces or animal faces. More recently, a novel model \cite{mao2022continuous}, named SAVI2I, was proposed to achieve a continuous translation between source and target domains by using signed vectors. All of these methods were trained in an unsupervised manner due to the lack of ground truths in the investigated task.
	
	\textbf{GAN inversion}. GAN inversion \cite{zhu2020domain,richardson2021encoding,tov2021designing,xia2022gan} is a technique that inverts an image back into the latent space of a pre-trained GAN model as a latent code so that the visual attribute of a given image can be manipulated by editing the inverted latent code. Generally, inversion methods include optimizing the latent codes to minimize the loss for the given images \cite{tewari2020stylerig}, training an encoder to map the given images into the latent space  \cite{zhu2020domain,richardson2021encoding,tov2021designing}, or use a  hybrid method combining both strategies \cite{bau2019seeing}. The inverted latent codes can be edited by finding linear directions that correspond to changes in a given binary attribute, such as `young $\leftrightarrow$ old', or `male $\leftrightarrow$ female' \cite{zhu2020domain,richardson2021encoding,tov2021designing}. These mentioned methods manipulate the attribute of the given images by editing the inverted latent code with a pre-trained support vector machine (SVM)  \cite{shen2020interfacegan} and can be only performed in the same image domain. The fundamental principle behind GAN inversion is the disentanglement of the space of a pre-trained StyleGAN in the dimension of visual image attributes. Consequently, the inverted latent codes can be manipulated via a pre-trained SVM. It is worth noting that these inverted latent codes can only be edited and subsequently fed into the pre-trained generator for unimodal I2I translation to accomplish visual attribute editing.
	
	\textbf{Positioning of Our Work}. Among the studies discussed above, the closest works to ours are the models proposed by \cite{liu2019toward}, \cite{liu2019collocating}, and \cite{yu2019personalized}. Our task is expected to take an image of given clothing as input and produce diverse and compatible images of clothing, e.g., `upper $\rightarrow$ lower'. It falls in the category of multimodal I2I translation methods. Most multimodal I2I translation methods \cite{huang2018multimodal,lee2018diverse,lee2020drit++,choi2020starganv2,mao2022continuous} usually generate diverse images at one time in an unsupervised learning setting, and the semantics of inputs and outputs of these models have an explicit spatial alignment. For example, the `summer $\rightleftharpoons$ winter' translation \cite{huang2018multimodal,lee2018diverse,lee2020drit++,mao2022continuous} relies on pixel-level spatial alignment, while the human face translation \cite{lee2018diverse,lee2020drit++,choi2020starganv2,mao2022continuous} is implemented by hidden landmark alignment. Unfortunately, the translation task in this research, `upper $\rightleftharpoons$ lower' clothing, cannot be based on this due to the lack of explicit spatial semantic alignment between upper and lower clothing items. To alleviate the spatial nonalignment between the input and output in the task of diverse and collocated clothing synthesis. We adopt GAN inversion technique to encode images into vectors to ignore the effect of spatial information. It should be noted here that our framework is different from the GAN inversion from three aspects: (i) the composed components of our BC-GAN is different from those of GAN inversion, the additional pre-trained SVM \cite{shen2020interfacegan} to perform visual attribute manipulation is not needed in our framework; (ii) the motivation of our BC-GAN is different from that of GAN inversion. Our aim is to encode images into vectors to ignore the effect of spatial information but the insight of GAN inversion is the $\mathcal{W}$ space is disentangled about visual attributes; and (iii) the GAN inversion only supports unimodal I2I translation in visual attributes but our BC-GAN supports multimodal I2I translation in collocated clothing synthesis.
	In addition, current multimodal I2I translation methods can only use unsupervised strategies to train their models, as there are no ground truths in a training dataset. On the contrary, the availability of existing matching pairs of clothing items can be fully exploited by transforming them as a type of weakly supervised information in comparison with those unsupervised methods.