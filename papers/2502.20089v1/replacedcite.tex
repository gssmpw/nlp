\section{Related Works}
Imitation learning (IL) and inverse reinforcement learning (IRL) ____ are foundational paradigms for training agents to mimic expert behavior from demonstrations. Behavioral Cloning (BC) ____, the simplest IL approach, treats imitation as a supervised learning problem by directly mapping states to expert actions. While computationally efficient, BC is prone to compounding errors ____ due to covariate shift during deployment. The Maximum Entropy IRL framework ____ addresses this limitation by probabilistically modeling expert behavior as reward maximization under an entropy regularization constraint, establishing a theoretical foundation for modern IRL methods.

The advent of adversarial training marked a pivotal shift in IL methodologies. ____ introduced Generative Adversarial Imitation Learning (GAIL), which formulates imitation learning as a generative adversarial game ____ where an agent learns a policy indistinguishable from the expertâ€™s by minimizing the Jensen-Shannon divergence between their state-action distributions. This framework was generalized by ____ in f-GAIL, which replaces the Jensen-Shannon divergence with arbitrary \( f \)-divergences to broaden applicability. Concurrently, ____ proposed Discriminator Actor-Critic (DAC), improving sample efficiency via off-policy updates and terminal-state reward modeling while mitigating reward bias.

Recent advances have shifted toward methods that bypass explicit reward function estimation. ____ introduced ValueDICE, an offline IL method that leverages an inverse Bellman operator to avoid adversarial optimization. Similarly, ____ developed IQ-Learn, which circumvents the challenges of MaxEnt IRL by optimizing implicit rewards derived directly from expert Q-values. A parallel research direction simplifies reward engineering by assigning fixed rewards to expert and agent samples. ____ pioneered this approach with Soft Q Imitation Learning (SQIL), which assigns binary rewards to transitions from expert and agent trajectories. Most recently, ____ proposed Least Squares Inverse Q-Learning (LSIQ), enhancing regularization by minimizing chi-squared divergence between expert and mixture distributions while explicitly managing absorbing states through critic regularization. Our work builds on these foundations by integrating distributional RL ____ and introducing a squared TD regularizer with adaptive targets.