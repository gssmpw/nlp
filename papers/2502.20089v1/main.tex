\documentclass[11pt]{article}

\usepackage{ mathpazo } 
\usepackage[scaled=0.92]{helvet} 
\usepackage{courier} 
\usepackage{microtype} 
\usepackage{amsmath, amssymb, amsthm, amsfonts}
\usepackage{mathpazo}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{indentfirst}
\usepackage{graphicx} 
\usepackage{booktabs}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{wrapfig}

\usepackage{geometry}
\geometry{
	a4paper,
	left=0.8in, 
	right=0.8in, 
	top=1in,
	bottom=1in,
	headheight=12pt,
	headsep=0.3in,
	footskip=0.5in
}

\usepackage{setspace}
\setstretch{1.2}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.7em}

\usepackage{titlesec}
\titleformat{\section}
{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titlespacing*{\section}{0pt}{12pt}{6pt}

\titleformat{\subsection}
{\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titlespacing*{\subsection}{0pt}{10pt}{4pt}

\usepackage{amsmath}
\usepackage{array}
\usepackage{ragged2e}

\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[section]

\usepackage{enumitem}
\setlist[itemize]{topsep=-10pt, itemsep=0pt, parsep=5pt} 

\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{hyperref}
\hypersetup{
	colorlinks=false,
}

\usepackage{ragged2e}

\date{}

\begin{document}


\title{RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning}
\author{Adib Karimi \\
	{\small Amirkabir University of Technology} \\
	\texttt{adibkarimi23@aut.ac.ir}
	\and
	Mohammad Mehdi Ebadzadeh \\
	{\small Amirkabir University of Technology} \\
	\texttt{ebadzadeh@aut.ac.ir}
}
\maketitle

\begin{abstract}
	We introduce a novel Inverse Reinforcement Learning (IRL) approach that overcomes limitations of fixed reward assignments and constrained flexibility in implicit reward regularization. By extending the Maximum Entropy IRL framework with a squared temporal-difference (TD) regularizer and adaptive targets, dynamically adjusted during training, our method indirectly optimizes a reward function while incorporating reinforcement learning principles. Furthermore, we integrate distributional RL to capture richer return information. Our approach achieves state-of-the-art performance on challenging MuJoCo tasks, demonstrating expert-level results on the \textit{Humanoid} task with only \textit{3 demonstrations}. Extensive experiments and ablation studies validate the effectiveness of our method, providing insights into adaptive targets and reward dynamics in imitation learning.\footnote{The code is available at \url{https://github.com/adibka/RIZE}.}
\end{abstract}


\section{Introduction}

Inverse Reinforcement Learning (IRL) \citep{Abbeel2004} is a foundational paradigm in artificial intelligence, enabling agents to acquire complex behaviors by observing expert demonstrations. This approach has catalyzed progress in diverse domains such as robotics \citep{Osa2018}, autonomous driving \citep{Knox2023}, and drug discovery \citep{Ai2024}. By eliminating the need for explicitly defined reward functions, IRL provides a practical framework for training agents in environments where designing such functions is infeasible.

A prominent framework in IRL is Maximum Entropy (MaxEnt) IRL \citep{Ziebart2010}, which underpins many state-of-the-art (SOTA) IL methods. Prior works have combined MaxEnt IRL with adversarial training \citep{Ho2016, Fu2018} to minimize divergences between agent and expert distributions. However, these adversarial methods often suffer from instability during training. To address this, recent research has introduced implicit reward regularization, which indirectly represents rewards via Q-value functions by inverting the Bellman equation. For instance, IQ-Learn \citep{Garg2021} unifies reward and policy representations using Q-functions with an \( L_2 \)-norm regularization on rewards, while LSIQ \citep{Al-Hafez2023} minimizes the chi-squared divergence between expert and mixture distributions, resulting in a squared temporal difference (TD) error objective analogous to SQIL \citep{Reddy2020}. Despite its effectiveness, this method has limitations: LSIQ assigns fixed rewards (e.g., +1 for expert samples and -1 for agent samples), which constrains flexibility by treating all tasks and state-action pairs uniformly, limiting performance and requiring additional gradient steps for convergence.

We present an extension of implicit reward regularization under the MaxEnt IRL framework through two methodological innovations: (1) \textbf{adaptive target rewards} (\( \lambda^{\pi_E}, \lambda^{\pi} \)) that dynamically adjust during training to replace static targets, enabling context-sensitive alignment of expert and policy data, and (2) \textbf{distributional RL integration}, where value distributions \( Z^{\pi}(s,a) \) \citep{Bellemare2017} are trained to capture richer return information while preserving theoretical consistency with Q-values, as the expectation of the value distribution is used for policy and value function optimization. By unifying adaptive target regularization with distributional insights \citep{Zhou2023}, our framework addresses prior rigidity in reward learning and empirically outperforms online IL baselines on MuJoCo benchmarks \citep{Todorov2012}, marking a novel integration of distributional RL into non-adversarial IRL.

Our contributions are as follow:
%\vspace{-10pt}
\begin{itemize}
	\item We introduce adaptive target rewards for implicit reward regularization to enable dynamic adjustments during training.
	\item We leverage value distributions instead of classical Q-functions to capture richer insights from return distributions.
	\item We demonstrate the effectiveness of our approach through extensive experiments on MuJoCo tasks, providing detailed analyses of implicit rewards and adaptive target mechanisms.
\end{itemize}


\section{Related Works}

Imitation learning (IL) and inverse reinforcement learning (IRL) \citep{Watson2023} are foundational paradigms for training agents to mimic expert behavior from demonstrations. Behavioral Cloning (BC) \citep{Pomerleau1991}, the simplest IL approach, treats imitation as a supervised learning problem by directly mapping states to expert actions. While computationally efficient, BC is prone to compounding errors \citep{Ross2011} due to covariate shift during deployment. The Maximum Entropy IRL framework \citep{Ziebart2010} addresses this limitation by probabilistically modeling expert behavior as reward maximization under an entropy regularization constraint, establishing a theoretical foundation for modern IRL methods.

The advent of adversarial training marked a pivotal shift in IL methodologies. \citet{Ho2016} introduced Generative Adversarial Imitation Learning (GAIL), which formulates imitation learning as a generative adversarial game \citep{Goodfellow2014} where an agent learns a policy indistinguishable from the expertâ€™s by minimizing the Jensen-Shannon divergence between their state-action distributions. This framework was generalized by \citet{Ghasemipour2019} in f-GAIL, which replaces the Jensen-Shannon divergence with arbitrary \( f \)-divergences to broaden applicability. Concurrently, \citet{Kostrikov2019} proposed Discriminator Actor-Critic (DAC), improving sample efficiency via off-policy updates and terminal-state reward modeling while mitigating reward bias.

Recent advances have shifted toward methods that bypass explicit reward function estimation. \citet{Kostrikov2020} introduced ValueDICE, an offline IL method that leverages an inverse Bellman operator to avoid adversarial optimization. Similarly, \citet{Garg2021} developed IQ-Learn, which circumvents the challenges of MaxEnt IRL by optimizing implicit rewards derived directly from expert Q-values. A parallel research direction simplifies reward engineering by assigning fixed rewards to expert and agent samples. \citet{Reddy2020} pioneered this approach with Soft Q Imitation Learning (SQIL), which assigns binary rewards to transitions from expert and agent trajectories. Most recently, \citet{Al-Hafez2023} proposed Least Squares Inverse Q-Learning (LSIQ), enhancing regularization by minimizing chi-squared divergence between expert and mixture distributions while explicitly managing absorbing states through critic regularization. Our work builds on these foundations by integrating distributional RL \citep{Dabney2018} and introducing a squared TD regularizer with adaptive targets.


\section{Background} 

\subsection{Preliminary}

We consider a Markov Decision Process (MDP) \citep{Puterman2014} to model policy learning in Reinforcement Learning (RL). The MDP framework is defined by the tuple \(\langle \mathcal{S}, \mathcal{A}, p_0, P, R, \gamma \rangle\), where \(\mathcal{S}\) denotes the state space, \(\mathcal{A}\) the action space, \(p_0\) the initial state distribution, \(P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0, 1]\) the transition kernel with \(P(\cdot \mid s,a)\) specifying the likelihood of transitioning from state \(s\) given action \(a\), \(R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) the reward function, and \(\gamma \in [0, 1]\) the discount factor which tempers future rewards. A stationary policy \(\pi \in \Pi\) is characterized as a mapping from states \(s \in \mathcal{S}\) to distributions over actions \(a \in \mathcal{A}\). The primary objective in RL \citep{Sutton2018} is to maximize the expected sum of discounted rewards, expressed as \(\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)\right]\). Furthermore, the occupancy measure \(\rho_{\pi}(s, a)\) for a policy \(\pi \in \Pi\) is given by \((1 - \gamma) \pi(a \mid s) \sum_{t=0}^{\infty} \gamma^t P(s_t = s \mid \pi)\). The corresponding measure for an expert policy, \(\pi_E\), is similarly denoted by \(\rho_E\). In Imitation Learning (IL), it is posited that the expert policy \(\pi_E\) remains unknown, and access is restricted to a finite collection of expert demonstrations, represented as \((s, a, s')\).


\subsection{Distributional Reinforcement Learning}

Maximum Entropy (MaxEnt) RL \citep{Haarnoja2018} focuses on addressing the stochastic nature of action selection by maximizing the entropy of the policy, while Distributional RL \citep{Bellemare2017} emphasizes capturing the inherent randomness in returns. Combining these perspectives, the distributional soft value function \(Z^{\pi} : \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{Z}\) \citep{Ma2020} for a policy \(\pi \in \Pi\) encapsulates uncertainty in both rewards and actions. It is formally defined as: 
\begin{equation}
	Z^{\pi}(s,a) = \sum_{t=0}^{\infty} \gamma^t [R(s_{t}, a_{t}) + \alpha \mathcal{H}(\pi(\cdot \mid s_t))],
\end{equation}
where \(\mathcal{Z}\) represents the space of action-value distributions, \(\mathcal{H}(\pi) = \mathbb{E}_{\pi}[-\log \pi(a \mid s)]\) denotes the entropy of the policy, and \(\alpha > 0\) balances entropy with reward.

The distributional soft Bellman operator \(\mathcal{B}_{D}^\pi : \mathcal{Z} \rightarrow \mathcal{Z}\) for a given policy \(\pi\) is introduced as \( (\mathcal{B}_{D}^\pi Z)(s, a) \overset{D}{=} R(s, a) + \gamma [Z(s', a') - \alpha \log \pi(a' \mid s')] \), where \(s' \sim P (\cdot \mid s, a)\), \(a' \sim \pi (\cdot \mid s')\), and  \(\overset{D}{=}\) signifies equality in distribution. Notably, this operator exhibits contraction properties under the p-Wasserstein metric, ensuring convergence to the optimal distributional value function.

A practical approach to approximating the value distribution \(Z\) involves modeling its quantile function \(F^{-1}_{Z}(\tau)\), evaluated at specific quantile levels \( \tau \in [0,1]\) \citep{Dabney2018a}. The quantile function is defined as \(F^{-1}_{Z}(\tau) = \inf \{ z \in \mathbb{R} : \tau \leq F_{Z}(z) \}\), where \(F_{Z}(z) = \mathbb{P}(Z \leq z)\) is the cumulative distribution function of \(Z\). For simplicity, we denote the quantile-based representation as \(Z_{\tau}^{\pi}(s,a) := F^{-1}_{Z}(\tau)\). To discretize this representation, we define a sequence of quantile levels, denoted as \(\{\tau_i\}_{i=0,...,N-1}\), where \(0 = \tau_0 < ... < \tau_{N-1} = 1\). These quantiles partition the unit interval into \(N\) fractions. Sampling uniformly from this interval \(\tau \sim U(0,1)\), we obtain an approximation of the value distribution \(Z^{\pi}_{\tau}(s,a)\), which provides quantile values at these specified levels.

\subsection{Inverse Reinforcement Learning}

Given expert trajectory data, Maximum Entropy (MaxEnt) Inverse RL \citep{Ziebart2010} aims to infer a reward function \( R(s,a) \) from the family \( \mathcal{R} = \mathbb{R}^{S \times A} \). Instead of assuming a deterministic expert policy, this method optimizes for stochastic policies \( \pi \in \Pi \) that maximize \( R \) while matching expert behavior. \citet{Ho2016} extend this framework by introducing a convex reward regularizer \( \psi : \mathbb{R}^{S \times A} \rightarrow \bar{\mathbb{R}} \), leading to the adversarial objective:
\begin{equation} \label{eq:maxent-irl}
	\max_{R \in \mathcal{R}} \min_{\pi \in \Pi} L(\pi, R) = \mathbb{E}_{\rho_E} [R(s, a)] - \mathbb{E}_{\rho_\pi} [R(s, a)] - \mathcal{H}(\pi) - \psi(R)
\end{equation}

IQ-Learn \citep{Garg2021} departs from adversarial training by implicitly representing rewards through Q-functions \( Q \in \Omega \) \citep{Piot2014}. It leverages the inverse soft Bellman operator \( \mathcal{T}^\pi \), defined as:
\begin{equation}
	(\mathcal{T}^{\pi} Q)(s, a) = Q(s, a) - \gamma \mathbb{E}_{s' \sim P (\cdot \mid s, a), a' \sim \pi(\cdot \mid s')} [Q(s', a') - \alpha \log \pi(a'|s')] 
\end{equation}

For a fixed policy \( \pi \), \( \mathcal{T}^\pi \) is bijective, ensuring a one-to-one correspondence between \( Q \)-values and rewards: \( \mathcal{T}^{\pi} Q = R \) and \( Q = (\mathcal{T}^{\pi})^{-1}R \). This allows reframing the MaxEnt IRL objective in Q-policy space as \( \max_{Q \in \Omega} \min_{\pi \in \Pi} \mathcal{J}(\pi, Q) \). IQ-Learn simplifies the problem by defining the implicit reward \( R_Q(s,a) = \mathcal{T}^{\pi}Q(s,a) \) and applying an L2 regularizer \( \psi(R_Q) \). The final objective becomes:
\begin{equation}
	\begin{aligned}
		\mathcal{J}(\pi, Q) &= \mathbb{E}_{\rho_E} [R_{Q}(s,a)] - \mathbb{E}_{\rho_\pi} [R_{Q}(s,a)] - \mathcal{H}(\pi) - c \left( \mathbb{E}_{\rho_E} [R_{Q}^2] + \mathbb{E}_{\rho_{\pi}} [R_{Q}^2] \right).
	\end{aligned}
\end{equation}


\section{Methodology}

In this section, we present a systematic framework for developing our proposed approach. We begin by integrating Distributional RL with Imitation Learning (IL). Next, we introduce a novel regularization technique that enhances existing Implicit Reward methods through an adaptive reward mechanism. Finally, we propose our algorithm, which leverages the strengths of both Distributional RL and IL to achieve superior performance in complex environments.

\subsection{Distributional Value Function}

Our method diverges from traditional imitation learning by modeling value distributions as critics in an actor-critic framework \citep{Zhou2023}. We propose that learning the full return distribution \( Z^{\pi}(s,a) \)â€”rather than point estimates like \( Q^{\pi}(s,a) \)â€”enables better decision-making in complex environments. By optimizing policies using the expectation of \( Z^{\pi}(s,a) \), we derive a more robust learning signal. This robustness stems from explicitly capturing environmental stochasticity through distributional modeling, avoiding the limitations of Q-networks that collapse variability into deterministic estimates. In Distributional RL, the action-value function is defined as \( Q^{\pi}(s,a) = \mathbb{E}[Z^{\pi}(s,a)] \), preserving the mathematical properties required by the IQ-Learn framework, ensuring theoretical consistency (see Lemma~\ref{lem:expectation} for more details).


\subsection{Implicit Reward Regularization}

In this section, we propose a novel regularizer for Inverse Reinforcement Learning (IRL) that refines existing implicit reward formulations \citep{Garg2021}. We define the implicit reward as \( R_{Q}(s,a) = Q^{\pi}(s, a) - \gamma V^{\pi}(s') \), where \( V^{\pi}(s') = Q^{\pi}(s',a') - \alpha \log \pi(a'|s') \). Prior work often regularizes implicit rewards using \(L2\)-norms, treating them as squared Temporal Difference (TD) errors between rewards and fixed targets \citep{Reddy2020, Al-Hafez2023}. While adopting a similar squared TD framework \citep{Mnih2015}, we introduce adaptive targets \(\lambda^{\pi_E}\) (for expert policy \(\pi_E\)) and \(\lambda^{\pi}\) (for imitation policy \(\pi\)) to derive our convex regularizer \(\Gamma: \mathbb{R}^{S \times A} \rightarrow \bar{\mathbb{R}}\):
\begin{equation} \label{eq:regularizer}
	\Gamma(R_{Q}, \lambda) = \mathbb{E}_{\rho_{E}} \left[(R_{Q} - \lambda^{\pi_E})^2\right] + \mathbb{E}_{\rho_{\pi}} \left[(R_{Q} - \lambda^{\pi})^2\right].
\end{equation}
\begin{lemma} \label{lem:regularizer}
	The implicit reward structure \( R_{Q} \) and regularizer \(\Gamma\) (Equation~\ref{eq:regularizer}) guarantee: 
	\begin{itemize}
		\item \textbf{Bounded Rewards:} \(R_{Q}\) converges to a convex combination of adaptive targets \(\lambda^{\pi_E}\) and \(\lambda^{\pi}\).
		\item \textbf{Temporal Consistency:} 	 Ensures \( |Q(s,a) - \gamma Q(s',a')| \leq |\lambda| + |\epsilon| + \gamma\alpha |\log \pi(a'|s')| \), \\where \(\epsilon\) is the optimization tolerance.
	\end{itemize}
	\vspace{10pt}
	(Proof: Appendix~\ref{proof:regularizer}.)
\end{lemma}

This framework stabilizes learning by tethering rewards \(R_{Q}\) to adaptive targets that evolve with the policy, unlike static methods (e.g., LSIQ/SQIL) that use fixed rewards (\(\pm 1\)). By co-optimizing \(R_{Q}\) and \(\lambda\), we establish dynamic equilibriumâ€”targets adapt to the current policy to avoid brittle regularization while maintaining bounded Q-values. The temporal consistency property ensures smooth value function updates, which directly translates to robust policy updatesâ€”reducing variance and improving the likelihood of convergence to the optimal policy. This dual adaptability distinguishes our approach from prior rigid regularization schemes. 

\subsection{Algorithm}

We introduce \textbf{RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning}, an algorithm enhancing implicit reward IRL through a convex regularizer inspired by TD error minimization. Unlike standard RL that uses environmental rewards, RIZE employs self-updating adaptable targets, creating a somewhat reinforcement paradigm where rewards automatically align with their moving targets through regularization (see Algorithm~\ref{algo:rize}).

\textbf{Critic Updates}

Let \(Z_{\phi, \tau}^{\pi}(s,a)\) denote the return distribution and \(\pi_{\theta}(a|s)\) the policy. We compute Q-values as:  
\begin{equation}
	Q^{\pi}(s, a) = \sum_{i=0}^{N-1} (\tau_{i+1} - \tau_i) Z_{\phi, \tau_i}^{\pi}(s, a)	 \label{eq:expec-z}
\end{equation}  
where \(\tau_i\) are quantile fractions and \(Z_{\phi, \tau_i}^{\pi}(s, a)\) represents quantile-specific return distributions.
 
The core objective integrates a squared-error regularizer \(\Gamma(R_Q, \lambda)\) (Equation~\ref{eq:regularizer}) to stabilize target alignment:  
\begin{equation}
	\begin{aligned} 
		\mathcal{L}(\pi, Q) = &\mathbb{E}_{\rho_E} [R_{Q}] - \mathbb{E}_{\rho_{\pi}} [R_{Q}] - \mathcal{H}(\pi) \\
		& - c\left[\mathbb{E}_{\rho_{E}} [(R_{Q} - \lambda^{\pi_E})^2] + \mathbb{E}_{\rho_{\pi}} [(R_{Q} - \lambda^{\pi})^2]\right]
		\label{eq:critic-loss}
	\end{aligned}
\end{equation}  
where implicit rewards \(R_{Q}\) derive from:  
\( R_{Q}(s,a) = Q^{\pi}(s, a) - \gamma \left[ Q^{\pi}(s',a') - \alpha \log \pi(a'|s') \right] \)
 

\textbf{Adaptive Target Updates}
 
Targets \(\lambda^{\pi_E}\) and \(\lambda^{\pi}\) self-update via the following objectives, creating a feedback loop where reward estimates continuously adapt to match their moving targets. 
\begin{equation}
	\underset{\lambda^{\pi_E}}{\min} \: \mathbb{E}_{\rho_{E}} \left[ R_{Q} - \lambda^{\pi_E} \right]^2, \quad \underset{\lambda^{\pi}}{\min} \: \mathbb{E}_{\rho_{\pi}} \left[ R_{Q} - \lambda^{\pi} \right]^2
	\label{eq:lambda-loss}
\end{equation}  

\begin{algorithm}[H]
	\caption{Regularized Imitation Learning via Distributional Reinforcement Learning}
	\label{algo:rize}
	\begin{algorithmic}[1]
		\STATE \textbf{Initialize} value distribution $Z_{\phi}$, policy $\pi_{\theta}$, and target rewards \(\lambda^{\pi_E}\), \(\lambda^{\pi}$
		\FOR{step $i$ in $\{1, \dots, N\}$}
		\STATE \textbf{Calculate} \(Q(s, a) = \mathbb{E}[Z_{\phi}(s, a)]\) using Equation~\ref{eq:expec-z}
		
		\STATE \textbf{Update value} \(Z_{\phi}\) using objective from Equation~\ref{eq:critic-loss}
		\STATE \[
		\phi_{t+1} \leftarrow \phi_{t} - \beta_Q \nabla_{\phi} [-J(\phi)]
		\]
		
		\STATE \textbf{Update policy} \(\pi_{\theta}\) (like SAC)
		\STATE \[
		\theta_{t+1} \leftarrow \theta_{t} + \beta_{\pi} \nabla_{\theta} \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_{\theta}(\cdot|s)}[\min_{k=1,2} Q_k(s, a) - \alpha \log \pi_{\theta}(a|s)]
		\]
		
		\STATE \textbf{Update target rewards} \(\lambda^{\pi}\) and \(\lambda^{\pi_E}\) using objectives in ~\ref{eq:lambda-loss}
		\STATE \[
		\lambda^{\pi}_{t+1} \leftarrow \lambda^{\pi}_{t} - \beta_{\lambda} \nabla_{\lambda^{\pi}} \Gamma(R_Q, \lambda)
		\]
		\STATE \[
		\lambda^{\pi_E}_{t+1} \leftarrow \lambda^{\pi_E}_{t} - \beta_{\lambda} \nabla_{\lambda^{\pi_E}} \Gamma(R_Q, \lambda)
		\]
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\section{Experiments}

We evaluate our algorithm on four MuJoCo \citep{Todorov2012} benchmarks (HalfCheetah-v2, Walker2d-v2, Ant-v2, Humanoid-v2) against state-of-the-art imitation learning methods: IQ-Learn \citep{Garg2021}, LSIQ \citep{Al-Hafez2023}, and SQIL \citep{Reddy2020}. All experiments are conducted with five seeds for statistical significance \citep{Henderson2018}. To assess sample efficiency, we test each method using three and ten expert trajectories. For ten trajectories, we retain baseline hyperparameters; for three trajectories (not reported in prior works), we adapt configurations from single-demonstration settings. IQ-Learn and LSIQ use their official implementations, while SQIL is evaluated using the LSIQ codebase. Results report mean performance across five seeds, with half a standard deviation to indicate uncertainty, and lines are smoothed for better readability. We normalize episode returns based on expert performance.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{bar.png}
	\caption{Normalized return of RIZE vs. online imitation learning baselines on Gym MuJoCo tasks. We depict the sorted top 25\% episodic returns across five seeds to evaluate convergence to expert-level behavior. We evaluate with three and ten expert trajectories.}
	\label{fig:bar}
\end{figure}

\subsection{Main Results}

Our method outperforms LSIQ and SQIL across tasks, with IQ-Learn as the only competitive baseline. Notably, in the most complex environment, Humanoid-v2, our approach is the sole method achieving expert-level performance, while all baselines fail (see Figure~\ref{fig:bar}). This demonstrates our algorithmâ€™s scalability to high-dimensional control problems. Additionally, our method shows superior sample efficiency, requiring fewer gradient steps to match expert performance compared to SOTA algorithms. These results highlight the robustness and efficiency of our approach in tackling complex tasks (see Figure~\ref{fig:main-demo10}).
 
\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{main_curves_demo10.png}
	\caption{Normalized return of RIZE vs. online imitation learning baselines on Gym MuJoCo tasks. We use 10 expert trajectories for all tasks.}
	\label{fig:main-demo10}
\end{figure}


\subsection{Reward Dynamics Analysis}

Our implicit reward regularizer \(\Gamma(R_Q, \lambda)\) (Equation~\ref{eq:regularizer}), ensures proximity between the learned \(R_Q\) and target rewards \(\lambda\). As illustrated in Figure~\ref{fig:reward-demo10}, when provided with sufficient expert trajectories (ten demonstrations), the rewards for both the expert and the policy stabilize around consistent values, as observed in environments like HalfCheetah and Ant. While our method is not Adversarial IL, it extends MaxEnt IRL principles. This alignment is evident as the policy increasingly mirrors expert behavior, causing the discriminator to converge and provide similar reward signals for both the expert and policy samples. This convergence reflects an equilibrium between policy optimization and reward learning, demonstrating the stability and effectiveness of our approach.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{reward-demo10.png}
	\caption{Implicit reward curves for expert and policy samples on Gym MuJoCo tasks. We use 10 expert trajectories for all tasks.}
	\label{fig:reward-demo10}
\end{figure}

\subsection{Ablation: Regularization Strategies }

We investigate the effect of squared TD error regularization on the Ant-v2 task using three distinct formulations: \textit{Expert-focused TD:} \(\mathbb{E}_{\rho_E}[R_Q - \lambda^{\pi_E}]^2 + \mathbb{E}_{\rho_\pi}[R_Q]^2\), \textit{Policy-focused TD:} \(\mathbb{E}_{\rho_E}[R_Q]^2 + \mathbb{E}_{\rho_\pi}[R_Q - \lambda^\pi]^2\), \textit{Baseline L2:} \(\mathbb{E}_{\rho_E}[R_Q]^2 + \mathbb{E}_{\rho_\pi}[R_Q]^2\).  

\begin{figure}[H]
	\begin{minipage}[t]{0.5\textwidth}
		\vspace{0pt}
		\justifying 
		Our results in Figure~\ref{fig:td} demonstrate that applying squared TD regularization to both expert and policy samples improves robustness. While the expert-focused TD formulation outperforms the policy-focused variant, neither consistently achieves expert-level behavior. Over time, both exhibit gradual performance degradation, underscoring the challenges of maintaining stable imitation. This highlights the need for more refined regularization strategies to ensure consistent and reliable learning in imitation learning.
	\end{minipage}
	\begin{minipage}[t]{0.5\textwidth}
		\vspace{0pt}
		\centering
		\includegraphics[width=0.9\linewidth]{td.png}
		\caption{Ablation study on the effect of various regularization strategies on the Ant-v2 task, evaluated using three demonstrations. Turquoise represents RIZE using the convex regularizer \(\Gamma\).}
		\label{fig:td}
	\end{minipage}
\end{figure}


\section{Conclusion}

We present a novel IRL framework that overcomes the rigidity of fixed reward mechanisms through dynamic reward adaptation via context-sensitive regularization and probabilistic return estimation with distributional RL. By introducing adaptive target rewards that evolve during trainingâ€”replacing static assignmentsâ€”our method enables nuanced alignment of expert and agent trajectories, while value distribution integration captures richer return dynamics without sacrificing theoretical consistency. Empirical evaluations on MuJoCo benchmarks demonstrate state-of-the-art performance, achieving expert-level proficiency on the \textit{Humanoid} task with only \textit{three demonstrations}, alongside ablation studies confirming the necessity of applying our regularization mechanism. This work advances imitation learning by unifying flexible reward learning with probabilistic return representations, offering a scalable, sample-efficient paradigm for complex decision making. Future directions include extending these principles to offline IL and risk-sensitive robotics, where adaptive rewards and uncertainty-aware distributions could further enhance robustness and generalization.


\newpage

\bibliography{references}

\newpage

\appendix

\section{Proofs}

\begin{lemma} \label{lem:expectation}
	Consider an MDP with discount factor \(\gamma \in [0, 1)\) and rewards bounded by some constant \(C > 0\) such that \(|R(s, a)| \leq C\). Define the return distribution as
	\[
	Z(s, a) = \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t),
	\]
	where the trajectory \(\{(s_t, a_t)\}\) is generated by following policy \(\pi\) under dynamics \(\mathcal{P}\). Then the classical action-value function is given by
	\[
	Q^\pi(s, a) = \mathbb{E}_{\pi, \mathcal{P}}[Z(s, a)].
	\]
	Moreover, if we approximate \(Z(s, a)\) by a learned distribution \(Z_\theta(s, a)\) with expectation \(\tilde{Q}(s, a) = \mathbb{E}[Z_\theta(s, a)]\), then under the conditions of the Dominated Convergence Theorem (DCT), \(\tilde{Q}(s, a)\) converges to \(Q^\pi(s, a)\).
\end{lemma}


\begin{proof}[\textbf{Proof of Lemma~\ref{lem:expectation}}] \label{proof:expectation}
	\mbox{}

	\noindent \textbf{Existence of the Expectation:}  
	Since the rewards are bounded, we have \(|R(s_t, a_t)| \leq C\), and the discounted series satisfies
	
	\[
	\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t |R(s_t, a_t)|\right] \leq \sum_{t=0}^{\infty} \gamma^t C = \frac{C}{1 - \gamma} < \infty.
	\]
	
	This absolute convergence allows us to apply DCT:
	
	\[
	\mathbb{E}[Z(s, a)] = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)\right] = \sum_{t=0}^{\infty} \gamma^t \mathbb{E}[R(s_t, a_t)].
	\]
	
	\noindent \textbf{Equivalence to Q-value:}  
	By definition,
	
	\[
	Q^\pi(s, a) = \mathbb{E}_{\pi,\mathcal{P}}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)\right] = E[Z(s,a)].
	\]
	
	\noindent \textbf{Convergence of the Learned Distribution:}  
	In distributional reinforcement learning, if we represent the return distribution using \(Z_\theta(s,a)\), then its mean is given by 
	
	\[
	\tilde{Q}(s,a) = E[Z_\theta(s,a)].
	\]
	
	Under DCT conditions, we have 
	
	\[
	\tilde{Q}(s,a) \to Q^\pi(s,a).
	\]
	\qedhere
\end{proof}


\begin{proof}[\textbf{Proof of Lemma~\ref{lem:regularizer}}] \label{proof:regularizer}
	\mbox{}
	
	\noindent \textbf{1. Bounded Rewards}
	
	\noindent The expert and policy reward targets \(\lambda^{\pi_E}, \lambda^\pi\) are learned via:
	\[
	\lambda^{\pi_E} \gets \mathbb{E}_{\rho_E}[R_Q], \quad \lambda^\pi \gets \mathbb{E}_{\rho_\pi}[R_Q].
	\]
	This ensures \(\lambda^{\pi_E}\) and \(\lambda^\pi\) track the empirical mean of \(R_Q\).
	
	Fixing $\lambda^{\pi_E}$ and $\lambda^\pi$, solving $\nabla_{R_{Q}}\Gamma = 0$ yields:
	\[
	R_{Q} = \frac{\rho_E\lambda^{\pi_E} + \rho_\pi\lambda^\pi}{\rho_E + \rho_\pi},
	\]
	This is a convex combination of \(\lambda^{\pi_E}\) and \(\lambda^\pi\). Since \(\lambda^{\pi_E}, \lambda^\pi\) are empirical means, \(|R_Q| \leq \max(|\lambda^{\pi_E}|, |\lambda^\pi|)\), ensuring boundedness.
	
	\vspace{10pt}
	\noindent \textbf{2. Temporal Consistency}
	
	\noindent The reward \(R_Q\) (for either expert or policy samples) is defined via the soft Q-function:
	\[
	R_Q = Q(s,a) - \gamma \mathbb{E}_{s'}\left[Q(s',a') - \alpha \log \pi(a'|s')\right].
	\]
	Rearranging gives:
	\[
	Q(s,a) = R_Q + \gamma \mathbb{E}_{s'}\left[Q(s',a') - \alpha \log \pi(a'|s')\right] + \epsilon,
	\]
	where \(\epsilon\) is the Bellman optimization error (due to approximation or finite samples). Substituting \( R_Q = \lambda \):
	\[
	Q(s,a) - \gamma Q(s',a') = \lambda + \epsilon - \gamma\alpha \log \pi(a'|s').
	\]
	Taking absolute values:
	\[
	|Q(s,a) - \gamma Q(s',a')| \leq |\lambda| + |\epsilon| + \gamma\alpha |\log \pi(a'|s')|.
	\]
	\noindent This bounds successive Q-values, ensuring temporal consistency.
	
	\vspace{10pt}
	\noindent \textbf{Conclusion.}
	
	\noindent The adaptive regularizer $\Gamma$ (Equation~\ref{eq:regularizer}) enforces bounded rewards via convex combinations of empirical targets and ensures temporal consistency through entropy-regularized Q-value recursion. Both properties stabilize learning by preventing divergence.
\end{proof}



\newpage

\section{Experiments}

\subsection{MuJoCo Control Suite}

We evaluate our imitation learning approach, \textbf{RIZE}, on four benchmark Gym \citep{Brockman2016} MuJoCo \citep{Todorov2012} environments: \textit{HalfCheetah-v2}, \textit{Walker2d-v2}, \textit{Ant-v2}, and \textit{Humanoid-v2}. Expert trajectories are generated using a pretrained Soft Actor-Critic (SAC) \citep{Haarnoja2018} agent, with each trajectory consisting of 1,000 state-action transitions. To facilitate performance comparisons, episode returns are normalized relative to expert performance: \textit{HalfCheetah} (5100), \textit{Walker2d} (5200), \textit{Ant} (4700), and \textit{Humanoid} (5300).

\subsection{Implementation Details}

Our architecture integrates components from \textbf{Distributional SAC (DSAC)}\footnote{\url{https://github.com/xtma/dsac}} \citep{Ma2020} and \textbf{IQ-Learn}\footnote{\url{https://github.com/Div99/IQ-Learn}} \citep{Garg2021}, with hyperparameters tuned through systematic search and ablation studies. Key configurations for experiments involving three demonstrations are summarized in Table~\ref{tab:demo3}, while Table~\ref{tab:demo10} provides settings for scenarios with ten demonstrations. Our implementation can be found at \url{https://github.com/adibka/RIZE}.

\textbf{Distributional SAC Components:} The critic network is implemented as a three-layer multilayer perceptron (MLP) with 256 units per layer, trained using a learning rate of \(3 \times 10^{-4}\). The policy network is a four-layer MLP, also with 256 units per layer. Environment-specific learning rates are applied: \(1 \times 10^{-5}\) for the \textit{Humanoid} environment and \(5 \times 10^{-5}\) for all others. To enhance training stability, we employ a target policyâ€”a delayed version of the online policyâ€”and sample next-state actions from this module. For value distribution training \(Z^{\pi}_{\phi, \tau}\), we adopt the Implicit Quantile Networks (IQN) \citep{Dabney2018} approach by sampling quantile fractions \(\tau\) uniformly from \(\mathcal{U}(0,1)\). Additionally, dual critic networks with delayed updates are used, which empirically improve training stability.

\textbf{IQ-Learn Adaptations:} Key adaptations from IQ-Learn include adjustments to the regularizer coefficient \(c\) and entropy coefficient \(\alpha\). Specifically, for the regularizer coefficient \(c\), we find that \(c=0.5\) yields robust performance on the \textit{Humanoid} task, while \(c=0.1\) works better for other tasks. For the entropy coefficient \(\alpha\), smaller values lead to more stable training. Unlike RL, where exploration is crucial, imitation learning relies less on entropy due to the availability of expert data. Across all tasks, we set initial target reward parameters as \(\lambda^{\pi_E}=10\) and \(\lambda^{\pi}=5\). Furthermore, we observe that lower learning rates for target rewards improve overall learning performance.

Previous implicit reward methods such as IQLearn, ValueDICE, and LSIQ\footnote{\url{https://github.com/robfiras/ls-iq/tree/main}} have employed distinct modifications to the loss function. In our setup, two main loss variants are defined:

\begin{enumerate}
	\item \textbf{Value loss:}
	\begin{equation}
		\mathcal{L}(\pi, Q) = \mathbb{E}_{\rho_E} [Q^{\pi}(s, a) - \gamma V^{\pi}(s')] - \mathbb{E}_{\rho} [V^{\pi}(s) - \gamma V^{\pi}(s')] - c \Gamma(R_{Q}, \lambda)
	\end{equation}
	\item \textbf{v0 loss:}
	\begin{equation}
		\mathcal{L}(\pi, Q) = \mathbb{E}_{\rho_E} [Q^{\pi}(s, a) - \gamma V^{\pi}(s')] - (1-\gamma) \mathbb{E}_{p_0} [V^{\pi}(s_0)] - c \Gamma(R_{Q}, \lambda)
	\end{equation}
\end{enumerate}

Here, \( \rho \) is a mixture distribution, \( p_0 \) denotes the initial distribution, \( R_{Q}(s,a) \) is the implicit reward defined as \( R_{Q}(s,a) = Q^{\pi}(s,a) - \gamma V^{\pi}(s') \), the state-value function is given by \( V^{\pi}(s') = Q^{\pi}(s',a') - \alpha \log \pi(a'|s') \), and lastly, our convex regularizer is expressed as \( \Gamma(R_{Q}, \lambda) = \mathbb{E}_{\rho_E} [(R_{Q} - \lambda^{\pi_E})^2] + \mathbb{E}_{\rho_{\pi}} [(R_{Q} - \lambda^{\pi})^2] \).

The choice between \textit{v0} or \textit{value} loss variants depends on environment complexity: we find that for a complex task like \textit{Humanoid-v2}, the \textit{v0} variant demonstrates greater robustness. And, for simpler tasks such as \textit{HalfCheetah-v2}, \textit{Walker2d-v2}, and \textit{Ant-v2}, the \textit{value} variant performs better.


\begin{table}[h]
	\centering
	\caption{Three Demonstrations Hyperparameters}
	\label{tab:demo3}
	\begin{tabular}{lccccccc}
		\toprule
		Environment & \(\alpha\) & \(c\) & lr \(\pi\) & \(\lambda^{\pi_E}\) & lr \(\lambda^{\pi_E}\) & \(\lambda^{\pi}\) & lr \(\lambda^{\pi_E}\) \\
		\midrule
		Ant-v2 & 0.05 & 0.1 & \(5\times10^{-5}\) & 10 & \(1\times10^{-4}\) & 5 & \(1\times10^{-5}\) \\
		HalfCheetah-v2 & 0.05 & 0.1 & \(5\times10^{-5}\) & 10 & \(1\times10^{-4}\) & 5 & \(1\times10^{-5}\) \\
		Walker2d-v2 & 0.05 & 0.1 & \(5\times10^{-5}\) & 10 & \(1\times10^{-4}\) & 5 & \(1\times10^{-5}\) \\
		Humanoid-v2 & 0.05 & 0.5 & \(1\times10^{-5}\) & 10 & \(1\times10^{-4}\) & 5 & \(5\times10^{-5}\) \\
		\bottomrule
	\end{tabular}
\end{table}

\vspace{0.5cm}

\begin{table}[h]
	\centering
	\caption{10 Demonstrations Hyperparameters}
	\label{tab:demo10}
	\begin{tabular}{lccccccc}
		\toprule
		Environment & \(\alpha\) & \(c\) & lr \(\pi\) & \(\lambda^{\pi_E}\) & lr \(\lambda^{\pi_E}\) & \(\lambda^{\pi}\) & lr \(\lambda^{\pi_E}\) \\
		\midrule
		Ant-v2 & 0.1 & 0.1 & \(5\times10^{-5}\) & 10 & \(1\times10^{-4}\) & 5 & \(1\times10^{-4}\) \\
		HalfCheetah-v2 & 0.1 & 0.1 & \(5\times10^{-5}\) & 10 & \(1\times10^{-4}\) & 5 & \(1\times10^{-4}\) \\
		Walker2d-v2 & 0.1 & 0.1 & \(5\times10^{-5}\) & 10 & \(1\times10^{-4}\) & 5 & \(1\times10^{-4}\) \\
		Humanoid-v2 & 0.1 & 0.5 & \(1\times10^{-5}\) & 10 & \(1\times10^{-4}\) & 5 & \(1\times10^{-5}\) \\
		\bottomrule
	\end{tabular}
\end{table}


\subsection{Extended Main Results}

In this section, we showcase the performance of our method, RIZE, using both ten and three expert demonstrations. RIZE outshines LSIQ and SQIL in various tasks, with IQ-Learn as its only competitor. Remarkably, it reaches expert-level performance in the challenging Humanoid-v2 environment with just three trajectories, proving its scalability. Plus, it requires fewer training steps than other algorithms, emphasizing its efficiency and robustness in complex situations. Our findings demonstrate that using fewer expert samples does not negatively impact our imitation learning approach (see Figure~\ref{fig:main-demo3&10}). 

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{main_curves.png}
	\caption{Normalized return of RIZE vs. online imitation learning baselines on Gym MuJoCo tasks. n denotes the number of expert trajectories.}
	\label{fig:main-demo3&10}
\end{figure}

\subsection{Extended Recovered Rewards}

Here, we investigate the implicit reward dynamics of our method using both three and ten expert demonstrations. As illustrated in Figure~\ref{fig:reward}, with ample expert trajectories (n=10), the rewards for both the expert and the policy stabilize to consistent values across most environments. Notably, even with a three demonstrations, which presents a significant learning challenge, rewards still converge to similar values in tasks such as HalfCheetah and Humanoid.

Our implicit reward regularization term, \(\Gamma(R_Q, \lambda)\), ensures the learned rewards stay close to the target rewards. Looking at Figure X, you can see that optimizing \(\lambda^{\pi_E}\) and \(\lambda^{\pi}\) using Equations X effectively bounds both the implicit and target rewards, boosting overall stability. Interestingly, for certain tasks like HalfCheetah-v2, Walker2d-v2, and Ant-v2, \(\lambda^{\pi}\) doesn't change much. This is because we deliberately used smaller learning rates, which proved crucial for maintaining training stability.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{reward.png}
	\caption{Implicit reward curves for expert and policy samples on Gym MuJoCo tasks. n represents the number of expert demonstrations.}
	\label{fig:reward}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{lambda.png}
	\caption{Target reward values for expert (\(\lambda^{\pi_E}\)) and policy (\(\lambda^{\pi}\)) on Gym MuJoCo tasks. n represents the number of expert demonstrations.}
	\label{fig:lambda}
\end{figure}

\subsection{Hyperparameter Fine-Tuning}

We present our analysis and comparison of important hyperparameters utilized in our algorithm. As before, all experiments use five seeds, and we show mean and half a standard deviation of all the seeds. As a challenging task among MuJoCo environments, we only experiment with Ant-v2. 

\subsubsection*{Target Reward \(\lambda\)}

Selecting appropriate initial values and learning rates for the automatic fine-tuning of \(\lambda^{\pi_E}\) and \(\lambda^{\pi}\) is critical in our approach. First, we observe that a suitable learning rate is essential for the stable training of our imitation learning agent, as illustrated in Figure~\ref{fig:lambda-lr}. Our findings indicate that \(\lambda^{\pi}\) must be optimized very slowly; using larger learning rates can destabilize training and hinder progress. In contrast, \(\lambda^{\pi_E}\) demonstrates greater resilience when optimized with higher learning rates. Additionally, \(\lambda^{\pi_E}\) remains robust even with varying initial values. However, as shown in Figure~\ref{fig:lambda-values}, failing to select an appropriate initial value for \(\lambda^{\pi}\) can negatively impact learning. Overall, Figures~\ref{fig:lambda-lr} and~\ref{fig:lambda-values} highlight the need for careful selection of both the learning rate and initial value when optimizing \(\lambda^{\pi}\), while \(\lambda^{\pi_E}\) exhibits considerable robustness in this regard.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{lambda-lr.png} 
		\caption{}
		\label{fig:lambda-lr}
	\end{subfigure}
	\hspace{0.05\textwidth} 
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{lambda-values.png}
		\caption{}
		\label{fig:lambda-values}
	\end{subfigure}
	\caption{Fine-tuning of target parameters. (a) Turquoise represents our method's primary result with learning rates of \(1e^{-4}\) for \(\lambda^{\pi_E}\) and \(1e^{-5}\) for \(\lambda^{\pi}\). Orange and blue lines indicate higher learning rates (e.g., 0.001) for \(\lambda^{\pi_E}\) and \(\lambda^{\pi}\), respectively. (b) Turquoise shows the main result with initial values of 10 for \(\lambda^{\pi_E}\) and 5 for \(\lambda^{\pi}\), while other lines explore different starting values. Only one parameter is varied at a time, and three trajectories are used throughout the analysis.}
	\label{fig:lambda-sensitivity}
\end{figure}

\subsubsection*{Regularization Coefficient \(C\)}

Our experiments on the regularizer coefficient, \(C\), reveal that smaller values of \(C\) encourage expert-like performance, while larger values overly constrain rewards and targets, limiting learning. This finding highlights the critical role of selecting an appropriate \(C\), as it directly impacts the balance between learning from expert data and regularization: higher values prioritize regularization at the cost of learning, whereas smaller values favor learning but reduce regularization (see Figure~\ref{fig:c}).

\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{c.png}
	\caption{Fine-tuning of the regularizer coefficient (\(C\)). Turquoise shows our method's primary result with \(C = 0.1\) for Ant-v2, while gray indicates a larger value (\(C = 0.6\)). Only one parameter is varied at a time, and three trajectories are used throughout the analysis.}
	\label{fig:c}
\end{figure}

\subsubsection*{Entropy Coefficient \(\alpha\)}

We observe that the entropy coefficient is a crucial hyperparameter in inverse reinforcement learning (IRL) problems. As shown in Figure~\ref{fig:alpha}, IRL methods typically require small values for \(\alpha\), a point previously noted by Reference X. With expert demonstrations available, an imitation learning (IL) policy does not need to explore for optimal actions, as these are provided by the demonstrations. Consequently, higher values of \(\alpha\) can lead to training instability, ultimately resulting in policy collapse.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{alpha.png}
	\caption{Fine-tuning of the temperature parameter (\(\alpha\)). Turquoise shows our method's primary result with \(\alpha = 0.05\) for Ant-v2, while gray indicates a larger value (\(\alpha = 0.5\)). Only one parameter is varied at a time, and three trajectories are used throughout the analysis.}
	\label{fig:alpha}
\end{figure}


\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
