\section{Related Works}
Imitation learning (IL) and inverse reinforcement learning (IRL) \citep{Watson2023} are foundational paradigms for training agents to mimic expert behavior from demonstrations. Behavioral Cloning (BC) \citep{Pomerleau1991}, the simplest IL approach, treats imitation as a supervised learning problem by directly mapping states to expert actions. While computationally efficient, BC is prone to compounding errors \citep{Ross2011} due to covariate shift during deployment. The Maximum Entropy IRL framework \citep{Ziebart2010} addresses this limitation by probabilistically modeling expert behavior as reward maximization under an entropy regularization constraint, establishing a theoretical foundation for modern IRL methods.

The advent of adversarial training marked a pivotal shift in IL methodologies. \citet{Ho2016} introduced Generative Adversarial Imitation Learning (GAIL), which formulates imitation learning as a generative adversarial game \citep{Goodfellow2014} where an agent learns a policy indistinguishable from the expertâ€™s by minimizing the Jensen-Shannon divergence between their state-action distributions. This framework was generalized by \citet{Ghasemipour2019} in f-GAIL, which replaces the Jensen-Shannon divergence with arbitrary \( f \)-divergences to broaden applicability. Concurrently, \citet{Kostrikov2019} proposed Discriminator Actor-Critic (DAC), improving sample efficiency via off-policy updates and terminal-state reward modeling while mitigating reward bias.

Recent advances have shifted toward methods that bypass explicit reward function estimation. \citet{Kostrikov2020} introduced ValueDICE, an offline IL method that leverages an inverse Bellman operator to avoid adversarial optimization. Similarly, \citet{Garg2021} developed IQ-Learn, which circumvents the challenges of MaxEnt IRL by optimizing implicit rewards derived directly from expert Q-values. A parallel research direction simplifies reward engineering by assigning fixed rewards to expert and agent samples. \citet{Reddy2020} pioneered this approach with Soft Q Imitation Learning (SQIL), which assigns binary rewards to transitions from expert and agent trajectories. Most recently, \citet{Al-Hafez2023} proposed Least Squares Inverse Q-Learning (LSIQ), enhancing regularization by minimizing chi-squared divergence between expert and mixture distributions while explicitly managing absorbing states through critic regularization. Our work builds on these foundations by integrating distributional RL \citep{Dabney2018} and introducing a squared TD regularizer with adaptive targets.