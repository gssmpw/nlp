@inproceedings{Barron2017,
	title = {Fast fourier color constancy},
	isbn = {978-1-5386-0457-1},
	doi = {10.1109/CVPR.2017.735},
	abstract = {We present Fast Fourier Color Constancy (FFCC), a color constancy algorithm which solves illuminant estimation by reducing it to a spatial localization task on a torus. By operating in the frequency domain, FFCC produces lower error rates than the previous state-of-the-art by 13 − 20\% while being 250 − 3000× faster. This unconventional approach introduces challenges regarding aliasing, directional statistics, and preconditioning, which we address. By producing a complete posterior distribution over illuminants instead of a single illuminant estimate, FFCC enables better training techniques, an effective temporal smoothing technique, and richer methods for error analysis. Our implementation of FFCC runs at ∼ 700 frames per second on a mobile device, allowing it to be used as an accurate, real-time, temporally-coherent automatic white balance algorithm.},
	booktitle = {Proceedings - 30th {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Barron, Jonathan T. and Tsai, Yun Ta},
	year = {2017},
	pages = {6950--6958},
	file = {PDF:files/1145/Barron和Tsai - 2017 - Fast fourier color constancy.pdf:application/pdf},
}

@inproceedings{Bianco2015,
	title = {Color constancy using {CNNs}},
	isbn = {978-1-4673-6759-2},
	doi = {10.1109/CVPRW.2015.7301275},
	abstract = {In this work we describe a Convolutional Neural Network (CNN) to accurately predict the scene illumination. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max pooling, one fully connected layer and three output nodes. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating scene illumination. This approach achieves state-of-the-art performance on a standard dataset of RAW images. Preliminary experiments on images with spatially varying illumination demonstrate the stability of the local illuminant estimation ability of our CNN.},
	language = {en-US},
	booktitle = {{IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Bianco, Simone and Cusano, Claudio and Schettini, Raimondo},
	year = {2015},
	keywords = {Estimation, Feature extraction, Image color analysis, Kernel, Lighting, Standards, Training},
	pages = {81--89},
	file = {PDF:files/1143/Bianco 等 - 2015 - Color constancy using CNNs.pdf:application/pdf},
}

@article{Buchsbaum1980a,
	title = {A spatial processor model for object colour perception},
	volume = {310},
	issn = {00160032},
	doi = {10.1016/0016-0032(80)90058-7},
	abstract = {A comprehensive mathematical model to account for colour constancy is formulated. Since the visual system is able to measure true object colour in complex scenes under a broad range of spectral compositions, for the illumination; it is assumed that the visual system must implicitly estimate and illuminant. The basic hypothesis is that the estimate of the illuminant is made on the basis of spatial information from the entire visual field. This estimate is then used by the visual system to arrive at an estimate of the (object) reflectance of the various subfields in the complex visual scene. The estimates are made by matching the inputs to the system to linear combinations of fixed bases and standards in the colour space. The model provides a general unified mathematical framework for related psychophysical phenomenology. © 1980.},
	number = {1},
	journal = {Journal of the Franklin Institute},
	author = {Buchsbaum, G.},
	year = {1980},
	pages = {1--26},
}

@article{Cheng2014a,
	title = {Illuminant estimation for color constancy: why spatial-domain methods work and the role of the color distribution},
	volume = {31},
	issn = {1084-7529},
	doi = {10.1364/josaa.31.001049},
	abstract = {Color constancy is a well-studied topic in color vision. Methods are generally categorized as (1) low-level statistical methods, (2) gamut-based methods, and (3) learning-based methods. In this work, we distinguish methods depending on whether they work directly from color values (i.e., color domain) or from values obtained from the image's spatial information (e.g., image gradients/frequencies). We show that spatial information does not provide any additional information that cannot be obtained directly from the color distribution and that the indirect aim of spatial-domain methods is to obtain large color differences for estimating the illumination direction. This finding allows us to develop a simple and efficient illumination estimation method that chooses bright and dark pixels using a projection distance in the color distribution and then applies principal component analysis to estimate the illumination direction. Our method gives state-of-the-art results on existing public color constancy datasets as well as on our newly collected dataset (NUS dataset) containing 1736 images from eight different highend consumer cameras. ? 2014 Optical Society of America.},
	language = {en-US},
	number = {5},
	journal = {Journal of the Optical Society of America A},
	author = {Cheng, Dongliang and Prasad, Dilip K. and Brown, Michael S.},
	year = {2014},
	pages = {1049--1058},
	file = {PDF:files/1126/Cheng 等 - 2014 - Illuminant estimation for color constancy why spatial-domain methods work and the role of the color.pdf:application/pdf},
}

@inproceedings{Gehler2008,
	title = {Bayesian color constancy revisited},
	doi = {10.1109/CVPR.2008.4587765},
	abstract = {Computational color constancy is the task of estimating the true reflectances of visible surfaces in an image. In this paper we follow a line of research that assumes uniform illumination of a scene, and that the principal step in estimating reflectances is the estimation of the scene illuminant. We review recent approaches to illuminant estimation, firstly those based on formulae for normalisation of the reflectance distribution in an image - so-called grey-world algorithms, and those based on a Bayesian formulation of image formation. In evaluating these previous approaches we introduce a new tool in the form of a database of 568 high-quality, indoor and outdoor images, accurately labelled with illuminant, and preserved in their raw form, free of correction or normalisation. This has enabled us to establish several properties experimentally. Firstly automatic selection of grey-world algorithms according to image properties is not nearly so effective as has been thought. Secondly, it is shown that Bayesian illuminant estimation is significantly improved by the improved accuracy of priors for illuminant and reflectance that are obtained from the new dataset.},
	language = {en-US},
	booktitle = {26th {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Gehler, Peter Vincent and Rother, Carsten and Blake, Andrew and Minka, Tom and Sharp, Toby},
	year = {2008},
	pages = {1--8},
	file = {Gehler 等 - 2008 - Bayesian color constancy revisited.pdf:files/850/Gehler 等 - 2008 - Bayesian color constancy revisited.pdf:application/pdf},
}

@inproceedings{Hu2017,
	title = {{FC4}: {Fully} convolutional color constancy with confidence-weighted pooling},
	isbn = {978-1-5386-0457-1},
	doi = {10.1109/CVPR.2017.43},
	abstract = {Improvements in color constancy have arisen from the use of convolutional neural networks (CNNs). However, the patch-based CNNs that exist for this problem are faced with the issue of estimation ambiguity, where a patch may contain insufficient information to establish a unique or even a limited possible range of illumination colors. Image patches with estimation ambiguity not only appear with great frequency in photographs, but also significantly degrade the quality of network training and inference. To overcome this problem, we present a fully convolutional network architecture in which patches throughout an image can carry different confidence weights according to the value they provide for color constancy estimation. These confidence weights are learned and applied within a novel pooling layer where the local estimates are merged into a global solution. With this formulation, the network is able to determine "what to learn" and "how to pool" automatically from color constancy datasets without additional supervision. The proposed network also allows for end-to-end training, and achieves higher efficiency and accuracy. On standard benchmarks, our network outperforms the previous state-of-the-art while achieving 120× greater efficiency.},
	booktitle = {Proceedings - 30th {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Hu, Yuanming and Wang, Baoyuan and Lin, Stephen},
	year = {2017},
	pages = {330--339},
	file = {PDF:files/1144/Hu 等 - 2017 - FC4 Fully convolutional color constancy with confidence-weighted pooling.pdf:application/pdf},
}

@article{VandeWeijer2007c,
	title = {Edge-based color constancy},
	volume = {16},
	issn = {10577149},
	doi = {10.1109/TIP.2007.901808},
	abstract = {Color constancy is the ability to measure colors of objects independent of the color of the light source. A well-known color constancy method is based on the gray-world assumption which assumes that the average reflectance of surfaces in theworld is achromatic. In this paper, we propose a new hypothesis for color constancy namely the gray-edge hypothesis, which assumes that the average edge difference in a scene is achromatic. Based on this hypothesis, we propose an algorithm for color constancy. Contrary to existing color constancy algorithms, which are com- puted from the zero-order structure of images, our method is based on the derivative structure of images. Furthermore, we pro- pose a framework which unifies a variety of known (gray-world, max-RGB, Minkowski norm) and the newly proposed gray-edge and higher order gray-edge algorithms. The quality of the various instantiations of the framework is tested and compared to the state-of-the-art color constancy methods on two large data sets of images recording objects under a large number of different light sources. The experiments show that the proposed color constancy algorithms obtain comparable results as the state-of-the-art color constancy methods with the merit of being computationally more efficient.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {van de Weijer, Joost and Gevers, Theo and Gijsenij, Arjan},
	year = {2007},
	keywords = {Color constancy, Object recognition, Photometric invariance},
	pages = {2207--2214},
	file = {PDF:files/1078/van de Weijer 等 - 2007 - Edge-based color constancy.pdf:application/pdf},
}

@inproceedings{Yang2015,
	title = {Efficient illuminant estimation for color constancy using grey pixels},
	isbn = {978-1-4673-6964-0},
	doi = {10.1109/CVPR.2015.7298838},
	abstract = {color-biased image에 기반해 gray pixel로 부터 illuminant를 estimate 하는 법 제안},
	booktitle = {Proceedings of the {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Yang, Kai Fu and Gao, Shao Bing and Li, Yong Jie},
	year = {2015},
	pages = {2254--2263},
	file = {Yang 等 - 2015 - Efficient illuminant estimation for color constanc.pdf:files/834/Yang 等 - 2015 - Efficient illuminant estimation for color constanc.pdf:application/pdf},
}

@inproceedings{afifi_deep_2020,
	title = {Deep {White}-{Balance} {Editing}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-7168-5},
	url = {https://ieeexplore.ieee.org/document/9156468/},
	doi = {10.1109/CVPR42600.2020.00147},
	abstract = {We introduce a deep learning approach to realistically edit an sRGB image’s white balance. Cameras capture sensor images that are rendered by their integrated signal processor (ISP) to a standard RGB (sRGB) color space encoding. The ISP rendering begins with a white-balance procedure that is used to remove the color cast of the scene’s illumination. The ISP then applies a series of nonlinear color manipulations to enhance the visual quality of the ﬁnal sRGB image. Recent work by [3] showed that sRGB images that were rendered with the incorrect white balance cannot be easily corrected due to the ISP’s nonlinear rendering. The work in [3] proposed a k-nearest neighbor (KNN) solution based on tens of thousands of image pairs. We propose to solve this problem with a deep neural network (DNN) architecture trained in an end-to-end manner to learn the correct white balance. Our DNN maps an input image to two additional white-balance settings corresponding to indoor and outdoor illuminations. Our solution not only is more accurate than the KNN approach in terms of correcting a wrong white-balance setting but also provides the user the freedom to edit the white balance in the sRGB image to other illumination settings.},
	language = {en},
	urldate = {2025-01-18},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Afifi, Mahmoud and Brown, Michael S.},
	month = jun,
	year = {2020},
	pages = {1394--1403},
	file = {PDF:files/1216/Afifi和Brown - 2020 - Deep White-Balance Editing.pdf:application/pdf},
}

@inproceedings{afifi_when_2019,
	title = {When {Color} {Constancy} {Goes} {Wrong}: {Correcting} {Improperly} {White}-{Balanced} {Images}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-7281-3293-8},
	shorttitle = {When {Color} {Constancy} {Goes} {Wrong}},
	doi = {10.1109/CVPR.2019.00163},
	abstract = {This paper focuses on correcting a camera image that has been improperly white-balanced. This situation occurs when a camera’s auto white balance fails or when the wrong manual white-balance setting is used. Even after decades of computational color constancy research, there are no effective solutions to this problem. The challenge lies not in identifying what the correct white balance should have been, but in the fact that the in-camera white-balance procedure is followed by several camera-speciﬁc nonlinear color manipulations that make it challenging to correct the image’s colors in post-processing. This paper introduces the ﬁrst method to explicitly address this problem. Our method is enabled by a dataset of over 65,000 pairs of incorrectly white-balanced images and their corresponding correctly white-balanced images. Using this dataset, we introduce a k-nearest neighbor strategy that is able to compute a nonlinear color mapping function to correct the image’s colors. We show our method is highly effective and generalizes well to camera models not in the training set.},
	language = {en},
	urldate = {2024-12-28},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Afifi, Mahmoud and Price, Brian and Cohen, Scott and Brown, Michael S.},
	month = jun,
	year = {2019},
	pages = {1535--1544},
	file = {PDF:files/1149/Afifi 等 - 2019 - When Color Constancy Goes Wrong Correcting Improperly White-Balanced Images.pdf:application/pdf},
}

@article{barnard_comparison_2002,
	title = {A comparison of computational color constancy algorithms. {I}: {Methodology} and experiments with synthesized data},
	volume = {11},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1057-7149},
	shorttitle = {A comparison of computational color constancy algorithms. {I}},
	url = {http://ieeexplore.ieee.org/document/1036047/},
	doi = {10.1109/TIP.2002.802531},
	abstract = {We introduce a context for testing computational color constancy, specify our approach to the implementation of a number of the leading algorithms, and report the results of three experiments using synthesized data. Experiments using synthesized data are important because the ground truth is known, possible confounds due to camera characterization and pre-processing are absent, and various factors affecting color constancy can be efficiently investigated because they can be manipulated individually and precisely.},
	language = {en},
	number = {9},
	urldate = {2024-12-27},
	journal = {IEEE Transactions on Image Processing},
	author = {Barnard, K. and Cardei, V. and Funt, B.},
	month = sep,
	year = {2002},
	pages = {972--984},
	file = {PDF:files/1136/Barnard 等 - 2002 - A comparison of computational color constancy algorithms. I Methodology and experiments with synthe.pdf:application/pdf},
}

@article{beigpour_multi-illuminant_2014,
	title = {Multi-{Illuminant} {Estimation} {With} {Conditional} {Random} {Fields}},
	volume = {23},
	issn = {1941-0042},
	doi = {10.1109/TIP.2013.2286327},
	abstract = {Most existing color constancy algorithms assume uniform illumination. However, in real-world scenes, this is not often the case. Thus, we propose a novel framework for estimating the colors of multiple illuminants and their spatial distribution in the scene. We formulate this problem as an energy minimization task within a conditional random field over a set of local illuminant estimates. In order to quantitatively evaluate the proposed method, we created a novel data set of two-dominant-illuminant images comprised of laboratory, indoor, and outdoor scenes. Unlike prior work, our database includes accurate pixel-wise ground truth illuminant information. The performance of our method is evaluated on multiple data sets. Experimental results show that our framework clearly outperforms single illuminant estimators as well as a recently proposed multi-illuminant estimation approach.},
	number = {1},
	urldate = {2024-02-16},
	journal = {IEEE Transactions on Image Processing},
	author = {Beigpour, Shida and Riess, Christian and van de Weijer, Joost and Angelopoulou, Elli},
	month = jan,
	year = {2014},
	keywords = {Color constancy, CRF, Estimation, Image color analysis, Labeling, Lighting, Materials, Minimization, multi-illuminant, Robustness},
	pages = {83--96},
	file = {IEEE Xplore Abstract Record:files/777/6637091.html:text/html;IEEE Xplore Full Text PDF:files/776/Beigpour 等 - 2014 - Multi-Illuminant Estimation With Conditional Rando.pdf:application/pdf},
}

@article{bianco_single_2017,
	title = {Single and {Multiple} {Illuminant} {Estimation} {Using} {Convolutional} {Neural} {Networks}},
	volume = {26},
	issn = {1941-0042},
	doi = {10.1109/TIP.2017.2713044},
	abstract = {In this paper, we present a three-stage method for the estimation of the color of the illuminant in RAW images. The first stage uses a convolutional neural network that has been specially designed to produce multiple local estimates of the illuminant. The second stage, given the local estimates, determines the number of illuminants in the scene. Finally, local illuminant estimates are refined by non-linear local aggregation, resulting in a global estimate in case of single illuminant. An extensive comparison with both local and global illuminant estimation methods in the state of the art, on standard data sets with single and multiple illuminants, proves the effectiveness of our method.},
	number = {9},
	urldate = {2024-02-18},
	journal = {IEEE Transactions on Image Processing},
	author = {Bianco, Simone and Cusano, Claudio and Schettini, Raimondo},
	month = sep,
	year = {2017},
	keywords = {Color, Color constancy, convolutional neural networks, Estimation, illuminant estimation, Image color analysis, Lighting, Neural networks, Training, Training data},
	pages = {4347--4362},
	file = {已提交版本:files/799/Bianco 等 - 2017 - Single and Multiple Illuminant Estimation Using Co.pdf:application/pdf;IEEE Xplore Abstract Record:files/798/7942101.html:text/html},
}

@article{cardei_estimating_2002,
	title = {Estimating the scene illumination chromaticity by using a neural network},
	volume = {19},
	copyright = {© 2002 Optical Society of America},
	issn = {1520-8532},
	doi = {10.1364/JOSAA.19.002374},
	abstract = {A neural network can learn color constancy, defined here as the ability to estimate the chromaticity of a scene’s overall illumination. We describe a multilayer neural network that is able to recover the illumination chromaticity given only an image of the scene. The network is previously trained by being presented with a set of images of scenes and the chromaticities of the corresponding scene illuminants. Experiments with real images show that the network performs better than previous color constancy methods. In particular, the performance is better for images with a relatively small number of distinct colors. The method has application to machine vision problems such as object recognition, where illumination-independent color descriptors are required, and in digital photography, where uncontrolled scene illumination can create an unwanted color cast in a photograph.},
	language = {EN},
	number = {12},
	urldate = {2024-12-27},
	journal = {Journal of the Optical Society of America A},
	author = {Cardei, Vlad C. and Funt, Brian and Barnard, Kobus},
	month = dec,
	year = {2002},
	keywords = {Camera calibration, CCD cameras, Machine vision, Neural networks, Physiology, Spatial resolution},
	pages = {2374--2386},
	file = {已提交版本:files/1139/Cardei 等 - 2002 - Estimating the scene illumination chromaticity by using a neural network.pdf:application/pdf},
}

@article{domislovic_color_2023,
	title = {Color constancy for non-uniform illumination estimation with variable number of illuminants},
	volume = {35},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-023-08487-z},
	doi = {10.1007/s00521-023-08487-z},
	abstract = {Image white-balancing is an integral part of every camera’s processing pipeline. White-balancing is used to remove illumination chromaticity from an image. Most research in this ﬁeld has been limited to images with a single uniform illuminant. In this paper, we introduce a novel method for illumination estimation for situations where the scene is illuminated by a variable number of different illuminants and where the illumination in the scene can be non-uniform. The proposed method uses a lightweight convolutional neural network that achieves state-of-the-art results. The method performs illumination estimation on a patch-by-patch basis. We use the assumption that only one illuminant affects each patch since they are so small. Unlike other such methods, our method uses features extracted from the entire image to perform patch illumination estimation. The paper also shows how the image features improve method accuracy with a minimal increase in complexity. The proposed method has around 42 k parameters, and it was tested on three different cameras from the Large-Scale Multi-Illuminant dataset.},
	language = {en},
	number = {20},
	urldate = {2025-01-18},
	journal = {Neural Computing and Applications},
	author = {Domislović, Ilija and Vršnjak, Donik and Subašić, Marko and Lončarić, Sven},
	month = jul,
	year = {2023},
	pages = {14825--14835},
	file = {PDF:files/1211/Domislović 等 - 2023 - Color constancy for non-uniform illumination estimation with variable number of illuminants.pdf:application/pdf},
}

@misc{entok_pixel-wise_2024,
	title = {Pixel-{Wise} {Color} {Constancy} via {Smoothness} {Techniques} in {Multi}-{Illuminant} {Scenes}},
	url = {http://arxiv.org/abs/2402.02922},
	abstract = {Most scenes are illuminated by several light sources, where the traditional assumption of uniform illumination is invalid. This issue is ignored in most color constancy methods, primarily due to the complex spatial impact of multiple light sources on the image. Moreover, most existing multi-illuminant methods fail to preserve the smooth change of illumination, which stems from spatial dependencies in natural images. Motivated by this, we propose a novel multi-illuminant color constancy method, by learning pixel-wise illumination maps caused by multiple light sources. The proposed method enforces smoothness within neighboring pixels, by regularizing the training with the total variation loss. Moreover, a bilateral filter is provisioned further to enhance the natural appearance of the estimated images, while preserving the edges. Additionally, we propose a label-smoothing technique that enables the model to generalize well despite the uncertainties in ground truth. Quantitative and qualitative experiments demonstrate that the proposed method outperforms the state-of-the-art.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Entok, Umut Cem and Laakom, Firas and Pakdaman, Farhad and Gabbouj, Moncef},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02922 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Entok 等 - 2024 - Pixel-Wise Color Constancy via Smoothness Techniqu.pdf:files/882/Entok 等 - 2024 - Pixel-Wise Color Constancy via Smoothness Techniqu.pdf:application/pdf},
}

@inproceedings{finlayson_shades_2004,
	title = {Shades of gray and colour constancy},
	abstract = {Colour constancy is a central problem for any visual system performing a task which requires stable perception of the colour world. To solve the colour constancy problem we estimate the colour of the prevailing light and then, at the second stage, remove it. Two of the most commonly used simple techniques for estimating the colour of the light are the Grey-World and Max-RGB algorithms. In this paper we begin by observing that this two colour constancy computations will respectively return the right answer if the average scene colour is grey or the maximum is white (and conversely, the degree of failure is proportional to the extent that these assumptions hold). We go on to ask the following question: "Would we perform better colour constancy by assuming the scene average is some shade of grey?". We give a mathematical answer to this question. Firstly, we show that Max-RGB and Grey-World are two instantiations of Minkowski norm. Secondly, that for a large calibrated dataset L6 norm colour constancy works best overall (we have improved the performance achieved by a simple normalization based approach). Surprisingly we found performance to be similar to more elaborated algorithm.},
	booktitle = {Final {Program} and {Proceedings} - {IS} and {T}/{SID} {Color} {Imaging} {Conference}},
	author = {Finlayson, Graham D and Trezzi, Elisabetta},
	year = {2004},
	keywords = {colour constancy},
	pages = {37--41},
}

@article{gao_combining_2019,
	title = {Combining {Bottom}-{Up} and {Top}-{Down} {Visual} {Mechanisms} for {Color} {Constancy} {Under} {Varying} {Illumination}},
	volume = {28},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/document/8678788},
	doi = {10.1109/TIP.2019.2908783},
	abstract = {Multi-illuminant-based color constancy (MCC) is quite a challenging task. In this paper, we proposed a novel model motivated by the bottom-up and top-down mechanisms of human visual system (HVS) to estimate the spatially varying illumination in a scene. The motivation for bottom-up based estimation is from our finding that the bright and dark parts in a scene play different roles in encoding illuminants. However, handling the color shift of large colorful objects is difficult using pure bottom-up processing. Thus, we further introduce a top-down constraint inspired by the findings in visual psychophysics, in which high-level information (e.g., the prior of light source colors) plays a key role in visual color constancy. In order to implement the top-down hypothesis, we simply learn a color mapping between the illuminant distribution estimated by bottom-up processing and the ground truth maps provided by the dataset. We evaluated our model on four datasets and the results show that our method obtains very competitive performance compared with the state-of-the-art MCC algorithms. Moreover, the robustness of our model is more tangible considering that our results were obtained using the same parameters for all the datasets or the parameters of our model were learned from the inputs, that is, mimicking how HVS operates. We also show the color correction results on some real-world images taken from the web.},
	number = {9},
	urldate = {2024-02-16},
	journal = {IEEE Transactions on Image Processing},
	author = {Gao, Shao-Bing and Ren, Yan-Ze and Zhang, Ming and Li, Yong-Jie},
	month = sep,
	year = {2019},
	keywords = {biologically inspired vision, Color constancy, Estimation, illuminant estimation, Image coding, Image color analysis, Image segmentation, Light sources, Lighting, Visualization},
	pages = {4387--4400},
	file = {IEEE Xplore Abstract Record:files/765/8678788.html:text/html;IEEE Xplore Full Text PDF:files/766/Gao 等 - 2019 - Combining Bottom-Up and Top-Down Visual Mechanisms.pdf:application/pdf},
}

@inproceedings{gijsenij_color_2007,
	title = {Color {Constancy} using {Natural} {Image} {Statistics}},
	isbn = {978-1-4244-1179-5 978-1-4244-1180-1},
	url = {http://ieeexplore.ieee.org/document/4270231/},
	doi = {10.1109/CVPR.2007.383206},
	abstract = {Although many color constancy methods exist, they are all based on speciﬁc assumptions such as the set of possible light sources, or the spatial and spectral characteristics of images. As a consequence, no algorithm can be considered as universal. However, with the large variety of available methods, the question is how to select the method that induces equivalent classes for different image characteristics. Furthermore, the subsequent question is how to combine the different algorithms in a proper way.},
	language = {en},
	urldate = {2024-07-09},
	booktitle = {2007 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gijsenij, Arjan and Gevers, Theo},
	month = jun,
	year = {2007},
	pages = {1--8},
	file = {Gijsenij 和 Gevers - 2007 - Color Constancy using Natural Image Statistics.pdf:files/848/Gijsenij 和 Gevers - 2007 - Color Constancy using Natural Image Statistics.pdf:application/pdf},
}

@article{gijsenij_color_2012,
	title = {Color {Constancy} for {Multiple} {Light} {Sources}},
	volume = {21},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1057-7149, 1941-0042},
	url = {http://ieeexplore.ieee.org/document/5986707/},
	doi = {10.1109/TIP.2011.2165219},
	abstract = {Color constancy algorithms are generally based on the simplifying assumption that the spectral distribution of the light source is uniform across the scene. However, in reality, this assumption is often violated due to the presence of multiple light sources. In this paper, we will address more realistic scenarios where the uniform light source assumption is too restrictive. First, a methodology is proposed to extend existing algorithms by applying color constancy locally to image patches rather than globally to the entire image. After local (patch-based) illuminant estimation, these estimates are combined into more robust estimations and a local correction is applied based on a modiﬁed diagonal model.},
	language = {en},
	number = {2},
	urldate = {2024-12-30},
	journal = {IEEE Transactions on Image Processing},
	author = {Gijsenij, A. and {Rui Lu} and Gevers, T.},
	month = feb,
	year = {2012},
	pages = {697--707},
	file = {PDF:files/1162/Gijsenij 等 - 2012 - Color Constancy for Multiple Light Sources.pdf:application/pdf},
}

@article{gijsenij_improving_2012,
	title = {Improving {Color} {Constancy} by {Photometric} {Edge} {Weighting}},
	volume = {34},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6042872/},
	doi = {10.1109/TPAMI.2011.197},
	abstract = {Edge-based color constancy methods make use of image derivatives to estimate the illuminant. However, different edge types exist in real-world images such as material, shadow and highlight edges. These different edge types may have a distinctive inﬂuence on the performance of the illuminant estimation.},
	language = {en},
	number = {5},
	urldate = {2025-01-20},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gijsenij, A. and Gevers, T. and Van De Weijer, J.},
	month = may,
	year = {2012},
	pages = {918--929},
	file = {PDF:files/1230/Gijsenij 等 - 2012 - Improving Color Constancy by Photometric Edge Weighting.pdf:application/pdf},
}

@article{joze_exemplar-based_2014,
	title = {Exemplar-{Based} {Color} {Constancy} and {Multiple} {Illumination}},
	volume = {36},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2013.169},
	abstract = {Exemplar-based learning or, equally, nearest neighbor methods have recently gained interest from researchers in a variety of computer science domains because of the prevalence of large amounts of accessible data and storage capacity. In computer vision, these types of technique have been successful in several problems such as scene recognition, shape matching, image parsing, character recognition, and object detection. Applying the concept of exemplar-based learning to the problem of color constancy seems odd at first glance since, in the first place, similar nearest neighbor images are not usually affected by precisely similar illuminants and, in the second place, gathering a dataset consisting of all possible real-world images, including indoor and outdoor scenes and for all possible illuminant colors and intensities, is indeed impossible. In this paper, we instead focus on surfaces in the image and address the color constancy problem by unsupervised learning of an appropriate model for each training surface in training images. We find nearest neighbor models for each surface in a test image and estimate its illumination based on comparing the statistics of pixels belonging to nearest neighbor surfaces and the target surface. The final illumination estimation results from combining these estimated illuminants over surfaces to generate a unique estimate. We show that it performs very well, for standard datasets, compared to current color constancy algorithms, including when learning based on one image dataset is applied to tests from a different dataset. The proposed method has the advantage of overcoming multi-illuminant situations, which is not possible for most current methods since they assume the color of the illuminant is constant all over the image. We show a technique to overcome the multiple illuminant situation using the proposed method and test our technique on images with two distinct sources of illumination using a multiple-illuminant color constancy dataset. The concept proposed here is a completely new approach to the color constancy problem and provides a simple learning-based framework.},
	language = {en-US},
	number = {5},
	urldate = {2024-02-16},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Joze, Hamid Reza Vaezi and Drew, Mark S.},
	month = may,
	year = {2014},
	keywords = {Color constancy, Color Constancy, Estimation, exemplar based learning, Exemplar Based Learning, Feature extraction, Image color analysis, Light sources, Lighting, multiple illuminants, Multiple Illuminants, Surface treatment, Training},
	pages = {860--873},
	file = {IEEE Xplore Abstract Record:files/769/6588227.html:text/html;IEEE Xplore Full Text PDF:files/768/Joze 和 Drew - 2014 - Exemplar-Based Color Constancy and Multiple Illumi.pdf:application/pdf},
}

@inproceedings{kim_attentive_2024,
	title = {Attentive {Illumination} {Decomposition} {Model} for {Multi}-{Illuminant} {White} {Balancing}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5300-6},
	doi = {10.1109/CVPR52733.2024.02410},
	abstract = {White balance (WB) algorithms in many commercial cameras assume single and uniform illumination, leading to undesirable results when multiple lighting sources with different chromaticities exist in the scene. Prior research on multi-illuminant WB typically predicts illumination at the pixel level without fully grasping the scene’s actual lighting conditions, including the number and color of light sources. This often results in unnatural outcomes lacking in overall consistency. To handle this problem, we present a deep white balancing model that leverages the slot attention, where each slot is in charge of representing individual illuminants. This design enables the model to generate chromaticities and weight maps for individual illuminants, which are then fused to compose the final illumination map. Furthermore, we propose the centroid-matching loss, which regulates the activation of each slot based on the color range, thereby enhancing the model to separate illumination more effectively. Our method achieves the state-of-theart performance on both single- and multi-illuminant WB benchmarks, and also offers additional information such as the number of illuminants in the scene and their chromaticity. This capability allows for illumination editing, an application not feasible with prior methods.},
	language = {en},
	urldate = {2024-12-30},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEE},
	author = {Kim, Dongyoung and Kim, Jinwoo and Yu, Junsang and Kim, Seon Joo},
	month = jun,
	year = {2024},
	pages = {25512--25521},
	file = {PDF:files/1168/Kim 等 - 2024 - Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing.pdf:application/pdf},
}

@inproceedings{kim_large_2021,
	title = {Large {Scale} {Multi}-{Illuminant} ({LSMI}) {Dataset} for {Developing} {White} {Balance} {Algorithm} under {Mixed} {Illumination}},
	isbn = {978-1-6654-2812-5},
	doi = {10.1109/ICCV48922.2021.00241},
	urldate = {2024-02-18},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Kim, Dongyoung and Kim, Jinwoo and Nam, Seonghyeon and Lee, Dongwoo and Lee, Yeonkyung and Kang, Nahyup and Lee, Hyong-Euk and Yoo, ByungIn and Han, Jae-Joon and Kim, Seon Joo},
	month = oct,
	year = {2021},
	pages = {2390--2399},
	file = {PDF:files/1174/Kim 等 - 2021 - Large Scale Multi-Illuminant (LSMI) Dataset for Developing White Balance Algorithm under Mixed Illum.pdf:application/pdf},
}

@misc{li_mimt_2023,
	title = {{MIMT}: {Multi}-{Illuminant} {Color} {Constancy} via {Multi}-{Task} {Local} {Surface} and {Light} {Color} {Learning}},
	shorttitle = {{MIMT}},
	url = {http://arxiv.org/abs/2211.08772},
	doi = {10.48550/arXiv.2211.08772},
	abstract = {Multi-illuminant color constancy is a challenging problem with only a few existing methods. For example, one prior work used a small set of predeﬁned white balance settings and spatially blended among them, limiting the solution to predeﬁned illuminations. Another method proposed a generative adversarial network and an angular loss, yet the performance is suboptimal due to the lack of regularization for multi-illumination colors. This paper introduces a transformer-based multi-task learning method to estimate single and multiple light colors from a single input image. To help our deep learning model have better cues of the light colors, achromatic-pixel detection, and edge detection are used as auxiliary tasks in our multi-task learning setting. By exploiting extracted content features from the input image as tokens, illuminant color correlations between pixels are learned by leveraging contextual information in our transformer. Our transformer approach is further assisted via a contrastive loss deﬁned between the input, output, and ground truth. We demonstrate that our proposed model achieves 40.7\% improvement compared to a state-of-the-art multi-illuminant color constancy method on a multi-illuminant dataset (LSMI). Moreover, our model maintains a robust performance on the single illuminant dataset (NUS-8) and provides 22.3\% improvement on the state-of-the-art single color constancy method.},
	language = {en},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Li, Shuwei and Wang, Jikai and Brown, Michael S. and Tan, Robby T.},
	month = aug,
	year = {2023},
	note = {arXiv:2211.08772 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:files/1206/Li 等 - 2023 - MIMT Multi-Illuminant Color Constancy via Multi-Task Local Surface and Light Color Learning.pdf:application/pdf},
}

@inproceedings{lo_clcc_2021,
	title = {{CLCC}: {Contrastive} {Learning} for {Color} {Constancy}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-6654-4509-2},
	shorttitle = {{CLCC}},
	url = {https://ieeexplore.ieee.org/document/9577539/},
	doi = {10.1109/CVPR46437.2021.00796},
	abstract = {In this paper, we present CLCC, a novel contrastive learning framework for color constancy. Contrastive learning has been applied for learning high-quality visual representations for image classiﬁcation. One key aspect to yield useful representations for image classiﬁcation is to design illuminant invariant augmentations. However, the illuminant invariant assumption conﬂicts with the nature of the color constancy task, which aims to estimate the illuminant given a raw image. Therefore, we construct effective contrastive pairs for learning better illuminant-dependent features via a novel raw-domain color augmentation. On the NUS-8 dataset, our method provides 17.5\% relative improvements over a strong baseline, reaching state-of-the-art performance without increasing model complexity. Furthermore, our method achieves competitive performance on the Gehler dataset with 3× fewer parameters compared to top-ranking deep learning methods. More importantly, we show that our model is more robust to different scenes under close proximity of illuminants, signiﬁcantly reducing 28.7\% worstcase error in data-sparse regions. Our code is available at https:// github.com/ howardyclo/ clcc-cvpr21.},
	language = {en},
	urldate = {2025-01-15},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lo, Yi-Chen and Chang, Chia-Che and Chiu, Hsuan-Chao and Huang, Yu-Hao and Chen, Chia-Ping and Chang, Yu-Lin and Jou, Kevin},
	month = jun,
	year = {2021},
	pages = {8049--8059},
	file = {PDF:files/1209/Lo 等 - 2021 - CLCC Contrastive Learning for Color Constancy.pdf:application/pdf},
}

@article{luo_estimating_2021,
	title = {Estimating {Polynomial} {Coefficients} to {Correct} {Improperly} {White}-{Balanced} {sRGB} {Images}},
	volume = {28},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1070-9908, 1558-2361},
	url = {https://ieeexplore.ieee.org/document/9508814/},
	doi = {10.1109/LSP.2021.3102527},
	abstract = {When taking photos, the illuminant of a scene can bring undesirable color casts to an image and ordinary users mainly rely on automatic white-balance to discount the effect of the illuminant. Since automatic white-balance is a nontrivial problem, many images can be wrongly white-balanced and rendered into standard RGB (sRGB) color space. In this paper, we propose to construct a residual image to correct wrongly white-balanced sRGB images by applying a linear transform matrix on a polynomially expanded image, in which the coefﬁcients are inferred from wrongly white-balanced images by a deep convolutional neural network. The proposed method has been comprehensively investigated and evaluated, and experimental results have shown the superiority of the proposed method.},
	language = {en},
	urldate = {2025-01-18},
	journal = {IEEE Signal Processing Letters},
	author = {Luo, Hang and Wan, Xiaoxia},
	year = {2021},
	pages = {1709--1713},
	file = {PDF:files/1220/Luo和Wan - 2021 - Estimating Polynomial Coefficients to Correct Improperly White-Balanced sRGB Images.pdf:application/pdf},
}

@inproceedings{qian_finding_2019,
	title = {On {Finding} {Gray} {Pixels}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-7281-3293-8},
	doi = {10.1109/CVPR.2019.00825},
	abstract = {We propose a novel grayness index for ﬁnding gray pixels and demonstrate its effectiveness and efﬁciency in illumination estimation. The grayness index, GI in short, is derived using the Dichromatic Reﬂection Model and is learning-free. GI allows to estimate one or multiple illumination sources in color-biased images. On standard singleillumination and multiple-illumination estimation benchmarks, GI outperforms state-of-the-art statistical methods and many recent deep methods. GI is simple and fast, written in a few dozen lines of code, processing a 1080p image in ∼ 0.4 seconds with a non-optimized Matlab code.},
	language = {en},
	urldate = {2024-12-25},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Qian, Yanlin and Kamarainen, Joni-Kristian and Nikkanen, Jarno and Matas, Jiri},
	month = jun,
	year = {2019},
	pages = {8054--8062},
	file = {PDF:files/1134/Qian 等 - 2019 - On Finding Gray Pixels.pdf:application/pdf},
}

@inproceedings{tang_transfer_2022,
	title = {Transfer {Learning} for {Color} {Constancy} via {Statistic} {Perspective}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	doi = {10.1609/aaai.v36i2.20135},
	abstract = {Color Constancy aims to correct image color casts caused by scene illumination. Recently, although the deep learning approaches have remarkably improved on single-camera data, these models still suffer from the seriously insufficient data problem, resulting in shallow model capacity and degradation in multi-camera settings. In this paper, to alleviate this problem, we present a Transfer Learning Color Constancy (TLCC) method that leverages cross-camera RAW data and massive unlabeled sRGB data to support training. Specifically, TLCC consists of the Statistic Estimation Scheme (SE-Scheme) and Color-Guided Adaption Branch (CGA-Branch). SE-Scheme builds a statistic perspective to map the camera-related illumination labels into camera-agnostic form and produce pseudo labels for sRGB data, which greatly expands data for joint training. Then, CGA-Branch further promotes efficient transfer learning from sRGB to RAW data by extracting color information to regularize the backbone's features adaptively. Experimental results show the TLCC has overcome the data limitation and model degradation, outperforming the state-of-the-art performance on popular benchmarks. Moreover, the experiments also prove the TLCC is capable of learning new scenes information from sRGB data to improve accuracy on the RAW images with similar scenes.},
	language = {en},
	urldate = {2024-01-28},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Tang, Yuxiang and Kang, Xuejing and Li, Chunxiao and Lin, Zhaowen and Ming, Anlong},
	month = jun,
	year = {2022},
	keywords = {Computer Vision (CV)},
	pages = {2361--2369},
	file = {Full Text PDF:files/747/Tang 等 - 2022 - Transfer Learning for Color Constancy via Statisti.pdf:application/pdf},
}

@article{woo_deep_2021,
	title = {Deep {Dichromatic} {Guided} {Learning} for {Illuminant} {Estimation}},
	volume = {30},
	issn = {19410042},
	doi = {10.1109/TIP.2021.3062729},
	abstract = {A new dichromatic illuminant estimation method using a deep neural network is proposed. Previous methods based on the dichromatic reflection model commonly suffer from inaccurate separation of specularity, thus being limited in their use in a real-world. Recent deep neural network-based methods have shown a significant improvement in the estimation of the illuminant color. However, why they succeed or fail is not explainable easily, because most of them estimate the illuminant color at the network output directly. To tackle these problems, the proposed architecture is designed to learn dichromatic planes and their confidences using a deep neural network with novel losses function. The illuminant color is estimated by a weighted least mean square of these planes. The proposed dichromatic guided learning not only achieves compelling results among state-of-the-art color constancy methods in standard real-world benchmark evaluations, but also provides a map to include color and regional contributions for illuminant estimation, which allow for an in-depth analysis of success and failure cases of illuminant estimation.},
	journal = {IEEE Transactions on Image Processing},
	author = {Woo, Sung Min and Kim, Jong Ok},
	year = {2021},
	pmid = {33729924},
	keywords = {color constancy, Illuminant estimation, chroma histogram, dichromatic reflection model, explainable deep learning, specular reflection},
	pages = {3623--3636},
	file = {PDF:files/1173/Woo和Kim - 2021 - Deep Dichromatic Guided Learning for Illuminant Estimation.pdf:application/pdf},
}

@article{yue_robust_2024,
	title = {Robust pixel-wise illuminant estimation algorithm for images with a low bit-depth},
	volume = {32},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-32-15-26708},
	doi = {10.1364/OE.528359},
	abstract = {Conventional illuminant estimation methods were developed for scenes with a uniform illumination, while recently developed methods, such as pixel-wise methods, estimate the illuminants at the pixel level, making them applicable to a wider range of scenes. It was found that the same pixel-wise algorithm had very different performance when applied to images with different bit-depths, with up to a 30\% decrease in accuracy for images having a lower bit-depth. Image signal processing (ISP) pipelines, however, prefer to deal with images with a lower bit-depth. In this paper, the analyses show that such a reduction was due to the loss of details and increase of noises, which were never identified in the past. We propose a method combining the L1 loss optimization and physical-constrained post-processing. The proposed method was found to result in around 40\% higher estimation accuracy, in comparison to the state-of-the-art DNN-based methods.},
	language = {en},
	number = {15},
	urldate = {2024-08-26},
	journal = {Optics Express},
	author = {Yue, Shuwei and Wei, Minchen},
	month = jul,
	year = {2024},
	pages = {26708--26718},
	file = {Yue 和 Wei - 2024 - Robust pixel-wise illuminant estimation algorithm .pdf:files/870/Yue 和 Wei - 2024 - Robust pixel-wise illuminant estimation algorithm .pdf:application/pdf},
}

