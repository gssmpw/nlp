% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{mhamdi2020job,
  title={Job recommendation based on job profile clustering and job seeker behavior},
  author={Mhamdi, D and Moulouki, Reda and others},
  journal={Procedia Computer Science},
  volume={175},
  pages={695--699},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{zhang2021attentive,
  title={Attentive heterogeneous graph embedding for job mobility prediction},
  author={Zhang, Le and Zhou, Ding and others},
  booktitle={Proc. SIGKDD, 2021},
  pages={2192--2201},
  year={2021}
}
@article{zha2024towards,
  title={Towards Unified Representation Learning for Career Mobility Analysis with Trajectory Hypergraph},
  author={Zha, Rui and Sun, Ying and others},
  journal={ACM Transactions on Information Systems},
  volume={42},
  number={4},
  pages={1--28},
  year={2024},
  publisher={ACM New York, NY}
}
@article{lu2022human,
  title={A human resource demand forecasting method based on improved BP algorithm},
  author={Lu, Xingguang},
  journal={Computational Intelligence and Neuroscience},
  volume={2022},
  number={1},
  pages={3534840},
  year={2022},
  publisher={Wiley Online Library}
}
@inproceedings{zhang2021talent,
  title={Talent demand forecasting with attentive neural sequential model},
  author={Zhang, Qi and Zhu, Hengshu and Sun, Ying and Liu, Hao and Zhuang, Fuzhen and Xiong, Hui},
  booktitle={Proc. SIGKDD, 2021},
  pages={3906--3916},
  year={2021}
}
@inproceedings{zhang2019job2vec,
  title={Job2Vec: Job title benchmarking with collective multi-view representation learning},
  author={Zhang, Denghui and Liu, Junming and others},
  booktitle={Proc. CIKM, 2019},
  pages={2763--2771},
  year={2019}
}
@inproceedings{kaya2021effectiveness,
  title={Effectiveness of job title based embeddings on r{\'e}sum{\'e} to job ad recommendation},
  author={Kaya, Mesut and Bogers, Toine},
  booktitle={CEUR Workshop Proceedings},
  volume={2967},
  year={2021},
  organization={CEUR Workshop Proceedings}
}
@article{DBLP:journals/corr/abs-2107-00221,
  author       = {Jing Zhao and
                  Jingya Wang and
                  others},
  title        = {Embedding-based Recommender System for Job to Candidate Matching on
                  Scale},
  journal      = {CoRR},
  volume       = {abs/2107.00221},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.00221},
  eprinttype    = {arXiv},
  eprint       = {2107.00221},
  timestamp    = {Wed, 07 Jul 2021 15:23:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-00221.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{laosaengpha-etal-2024-learning,
    title = "Learning Job Title Representation from Job Description Aggregation Network",
    author = "Laosaengpha, Napat  and
      Tativannarat, Thanit  and
      others",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "ACL findings, 2024",
    month = aug,
    year = "2024",
    url = "https://aclanthology.org/2024.findings-acl.77",
    doi = "10.18653/v1/2024.findings-acl.77",
    pages = "1319--1329",
    abstract = "Learning job title representation is a vital process for developing automatic human resource tools. To do so, existing methods primarily rely on learning the title representation through skills extracted from the job description, neglecting the rich and diverse content within. Thus, we propose an alternative framework for learning job titles through their respective job description (JD) and utilize a Job Description Aggregator component to handle the lengthy description and bidirectional contrastive loss to account for the bidirectional relationship between the job title and its description. We evaluated the performance of our method on both in-domain and out-of-domain settings, achieving a superior performance over the skill-based approach.",
}
@article{DBLP:journals/corr/abs-2109-09605,
  author       = {Jens{-}Joris Decorte and
                  Jeroen Van Hautte and
                  others},
  title        = {JobBERT: Understanding Job Titles through Skills},
  journal      = {CoRR},
  volume       = {abs/2109.09605},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.09605},
  eprinttype    = {arXiv},
  eprint       = {2109.09605},
  timestamp    = {Mon, 27 Sep 2021 15:21:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-09605.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bhola2020retrieving,
  title={Retrieving skills from job descriptions: A language model based extreme multi-label classification framework},
  author={Bhola, Akshay and Halder, Kishaloy and Prasad, Animesh and Kan, Min-Yen},
  booktitle={Proc. COLING, 2020},
  pages={5832--5842},
  year={2020}
}
@inproceedings{goyal-etal-2023-jobxmlc,
    title = "{J}ob{XMLC}: {EX}treme Multi-Label Classification of Job Skills with Graph Neural Networks",
    author = "Goyal, Nidhi  and
      Kalra, Jushaan  and
      others",
    booktitle = "EACL findings 2023",
    month = may,
    year = "2023",
    url = "https://aclanthology.org/2023.findings-eacl.163",
    doi = "10.18653/v1/2023.findings-eacl.163",
    pages = "2181--2191",
    abstract = "Writing a good job description is an important step in the online recruitment process to hire the best candidates. Most recruiters forget to include some relevant skills in the job description. These missing skills affect the performance of recruitment tasks such as job suggestions, job search, candidate recommendations, etc. Existing approaches are limited to contextual modelling, do not exploit inter-relational structures like job-job and job-skill relationships, and are not scalable. In this paper, we exploit these structural relationships using a graph-based approach. We propose a novel skill prediction framework called JobXMLC, which uses graph neural networks with skill attention to predict missing skills using job descriptions. JobXMLC enables joint learning over a job-skill graph consisting of 22.8K entities (jobs and skills) and 650K relationships. We experiment with real-world recruitment datasets to evaluate our proposed approach. We train JobXMLC on 20,298 job descriptions and 2,548 skills within 30 minutes on a single GPU machine. JobXMLC outperforms the state-of-the-art approaches by 6{\%} in precision and 3{\%} in recall. JobXMLC is 18X faster for training task and up to 634X faster in skill prediction on benchmark datasets enabling JobXMLC to scale up on larger datasets.",
}
@inproceedings{lin2023skill,
  title={Skill Graph Construction From Semantic Understanding},
  author={Lin, Shiyong and Yuan, Yiping and Jin, Carol and Pan, Yi},
  booktitle={Companion Proceedings of the ACM Web Conference 2023},
  pages={978--982},
  year={2023}
}
@inproceedings{javed2017large,
  title={Large-scale occupational skills normalization for online recruitment},
  author={Javed, Faizan and Hoang, Phuong and Mahoney, Thomas and McNair, Matt},
  booktitle={Proc. AAAI, 2017},
  volume={31},
  number={2},
  pages={4627--4634},
  year={2017}
}
@article{DBLP:journals/corr/abs-2207-00494,
  author       = {Rabih Zbib and
                  Lucas Lacasa Alvarez 
                  and others},
  title        = {Learning Job Titles Similarity from Noisy Skill Labels},
  journal      = {CoRR},
  volume       = {abs/2207.00494},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2207.00494},
  doi          = {10.48550/ARXIV.2207.00494},
  eprinttype    = {arXiv},
  eprint       = {2207.00494},
  timestamp    = {Tue, 21 Mar 2023 21:05:18 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2207-00494.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Bocharova_2023,
   title={VacancySBERT: the approach for representation of titles and skillsfor semantic similarity search in the recruitment domain},
   volume={6},
   ISSN={2663-7723},
   url={http://dx.doi.org/10.15276/aait.06.2023.4},
   DOI={10.15276/aait.06.2023.4},
   number={1},
   journal={Applied Aspects of Information Technology},
   publisher={Odessa Polytechnic National University},
   author={Bocharova, Maiia Y. and Malakhov, Eugene V. and Mezhuyev, Vitaliy I.},
   year={2023},
   month=apr, pages={52â€“59} }
@inproceedings{fang2023recruitpro,
  title={Recruitpro: A pretrained language model with skill-aware prompt learning for intelligent recruitment},
  author={Fang, Chuyu and Qin, Chuan and others},
  booktitle={Proc. SIGKDD, 2023},
  pages={3991--4002},
  year={2023}
}
@inproceedings{cao2024tarot,
  title={TAROT: A Hierarchical Framework with Multitask co-pretraining on Semi-Structured Data Towards Effective Person-Job fit},
  author={Cao, Yihan and Chen, Xu and others},
  booktitle={Proc. ICASSP, 2024},
  pages={12046--12050},
  year={2024},
  organization={IEEE}
}
@inproceedings{qin2018enhancing,
  title={Enhancing person-job fit for talent recruitment: An ability-aware neural network approach},
  author={Qin, Chuan and Zhu, Hengshu and Xu, Tong and Zhu, Chen and Jiang, Liang and Chen, Enhong and Xiong, Hui},
  booktitle={SIGIR, 2018},
  pages={25--34},
  year={2018}
}
@inproceedings{zhang-etal-2023-escoxlm,
    title = "{ESCOXLM}-{R}: Multilingual Taxonomy-driven Pre-training for the Job Market Domain",
    author = "Zhang, Mike  and
      van der Goot, Rob  and
      Plank, Barbara",
    booktitle = "ACL 2023",
    month = jul,
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.662",
    doi = "10.18653/v1/2023.acl-long.662",
    pages = "11871--11890",
    abstract = "The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some approaches have been developed that are specific to the job market domain, there is a lack of generalized, multilingual models and benchmarks for these tasks. In this study, we introduce a language model called ESCOXLM-R, based on XLM-R-large, which uses domain-adaptive pre-training on the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27 languages. The pre-training objectives for ESCOXLM-R include dynamic masked language modeling and a novel additional objective for inducing multilingual taxonomical ESCO relations. We comprehensively evaluate the performance of ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and find that it achieves state-of-the-art results on 6 out of 9 datasets. Our analysis reveals that ESCOXLM-R performs better on short spans and outperforms XLM-R-large on entity-level and surface-level span-F1, likely due to ESCO containing short skill and occupation titles, and encoding information on the entity-level.",
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@inproceedings{conneau-etal-2020-unsupervised,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      others",
    booktitle = "ACL 2020",
    month = jul,
    year = "2020",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}
@inproceedings{feng-etal-2022-language,
    title = "Language-agnostic {BERT} Sentence Embedding",
    author = "Feng, Fangxiaoyu  and
      Yang, Yinfei  and
      others",
    booktitle = "ACL 2022",
    month = may,
    year = "2022",
    url = "https://aclanthology.org/2022.acl-long.62",
    doi = "10.18653/v1/2022.acl-long.62",
    pages = "878--891",
    abstract = "While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80{\%}. Composing the best of these methods produces a model that achieves 83.7{\%} bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5{\%} achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at \url{https://tfhub.dev/google/LaBSE}.",
}
@inproceedings{yang-etal-2020-multilingual,
    title = "Multilingual Universal Sentence Encoder for Semantic Retrieval",
    author = "Yang, Yinfei  and
      Cer, Daniel  and
      others",
    booktitle = "ACL 2020",
    month = jul,
    year = "2020",
    url = "https://aclanthology.org/2020.acl-demos.12",
    doi = "10.18653/v1/2020.acl-demos.12",
    pages = "87--94",
    abstract = "We present easy-to-use retrieval focused multilingual sentence embedding models, made available on TensorFlow Hub. The models embed text from 16 languages into a shared semantic space using a multi-task trained dual-encoder that learns tied cross-lingual representations via translation bridge tasks (Chidambaram et al., 2018). The models achieve a new state-of-the-art in performance on monolingual and cross-lingual semantic retrieval (SR). Competitive performance is obtained on the related tasks of translation pair bitext retrieval (BR) and retrieval question answering (ReQA). On transfer learning tasks, our multilingual embeddings approach, and in some cases exceed, the performance of English only sentence embeddings.",
}
@inproceedings{chen-etal-2024-m3,
    title = "{M}3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
    author = "Chen, Jianlyu  and
      Xiao, Shitao  and
      others",
    booktitle = "ACL findings, 2024",
    month = aug,
    year = "2024",
    url = "https://aclanthology.org/2024.findings-acl.137",
    doi = "10.18653/v1/2024.findings-acl.137",
    pages = "2318--2335",
    abstract = "In this paper, we introduce a new embedding model called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3-Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross-lingual, and long-document retrieval benchmarks.",
}
@inproceedings{DBLP:journals/corr/KingmaB14,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {ICLR, 2015},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{lowphansirikul2022large,
  title={A large English--Thai parallel corpus from the web and machine-generated text},
  author={Lowphansirikul, Lalita and Polpanumas, Charin and Rutherford, Attapol T and Nutanong, Sarana},
  journal={Language Resources and Evaluation},
  volume={56},
  number={2},
  pages={477--499},
  year={2022},
  publisher={Springer}
}

@inproceedings{roy-etal-2020-lareqa,
    title = "{LAR}e{QA}: Language-Agnostic Answer Retrieval from a Multilingual Pool",
    author = "Roy, Uma  and
      Constant, Noah  and
      others",
    booktitle = "EMNLP, 2020",
    month = nov,
    year = "2020",
    url = "https://aclanthology.org/2020.emnlp-main.477",
    doi = "10.18653/v1/2020.emnlp-main.477",
    pages = "5919--5930",
    abstract = "We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for {``}strong{''} cross-lingual alignment, requiring semantically related \textit{cross}-language pairs to be closer in representation space than unrelated \textit{same}-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target {``}weak{''} alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at \url{https://github.com/google-research-datasets/lareqa}.",
}
@inproceedings{yang-etal-2021-simple,
    title = "A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations",
    author = "Yang, Ziyi  and
      Yang, Yinfei  and
      others",
    booktitle = "EMNLP, 2021",
    month = nov,
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.470",
    doi = "10.18653/v1/2021.emnlp-main.470",
    pages = "5825--5832",
    abstract = "Language agnostic and semantic-language information isolation is an emerging research direction for multilingual representations models. We explore this problem from a novel angle of geometric algebra and semantic space. A simple but highly effective method {``}Language Information Removal (LIR){''} factors out language identity information from semantic related components in multilingual representations pre-trained on multi-monolingual data. A post-training and model-agnostic method, LIR only uses simple linear operations, e.g. matrix factorization and orthogonal projection. LIR reveals that for weak-alignment multilingual systems, the principal components of semantic spaces primarily encodes language identity information. We first evaluate the LIR on a cross-lingual question answer retrieval task (LAReQA), which requires the strong alignment for the multilingual embedding space. Experiment shows that LIR is highly effectively on this task, yielding almost 100{\%} relative improvement in MAP for weak-alignment models. We then evaluate the LIR on Amazon Reviews and XEVAL dataset, with the observation that removing language information is able to improve the cross-lingual transfer performance.",
}
@inproceedings{xie-etal-2022-discovering,
    title = "Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations",
    author = "Xie, Zhihui  and
      Zhao, Handong  and
      others",
    booktitle = "EMNLP, 2022",
    month = dec,
    year = "2022",
    url = "https://aclanthology.org/2022.emnlp-main.379",
    doi = "10.18653/v1/2022.emnlp-main.379",
    pages = "5617--5633",
    abstract = "Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the subspace is found, we can directly project the original embeddings into the null space to boost language agnosticism without finetuning. We systematically evaluate our method on various tasks including the challenging language-agnostic QA retrieval task. Empirical results show that applying our method consistently leads to improvements over commonly used ML-LMs.",
}
@inproceedings{utpala-etal-2024-language,
    title = "Language Agnostic Code Embeddings",
    author = "Utpala, Saiteja  and
      Gu, Alex  and
      Chen, Pin-Yu",
    booktitle = "NAACL, 2024",
    month = jun,
    year = "2024",
    url = "https://aclanthology.org/2024.naacl-long.38",
    doi = "10.18653/v1/2024.naacl-long.38",
    pages = "678--691",
    abstract = "Recently, code language models have achieved notable advancements in addressing a diverse array of essential code comprehension and generation tasks. Yet, the field lacks a comprehensive deep dive and understanding of the code embeddings of multilingual code models. In this paper, we present a comprehensive study on multilingual code embeddings, focusing on the cross-lingual capabilities of these embeddings across different programming languages. Through probing experiments, we demonstrate that code embeddings comprise two distinct components: one deeply tied to the nuances and syntax of a specific language, and the other remaining agnostic to these details, primarily focusing on semantics. Further, we show that when we isolate and eliminate this language-specific component, we witness significant improvements in downstream code retrieval tasks, leading to an absolute increase of up to +17 in the Mean Reciprocal Rank (MRR).",
}

@inproceedings{DBLP:conf/emnlp/ConneauKSBB17,
  author       = {Alexis Conneau and
                  Douwe Kiela and
                  others},
  title        = {Supervised Learning of Universal Sentence Representations from Natural
                  Language Inference Data},
  booktitle    = {EMNLP 2017},
  pages        = {670--680},
  year         = {2017},
  url          = {https://doi.org/10.18653/v1/d17-1070},
  doi          = {10.18653/V1/D17-1070},
  timestamp    = {Fri, 06 Aug 2021 00:40:27 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/ConneauKSBB17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{gao-etal-2021-simcse,
    title = "{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings",
    author = "Gao, Tianyu  and
      Yao, Xingcheng  and
      Chen, Danqi",
    booktitle = "EMNLP, 2021",
    month = nov,
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.552",
    doi = "10.18653/v1/2021.emnlp-main.552",
    pages = "6894--6910",
    abstract = "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using {``}entailment{''} pairs as positives and {``}contradiction{''} pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3{\%} and 81.6{\%} Spearman{'}s correlation respectively, a 4.2{\%} and 2.2{\%} improvement compared to previous best results. We also show{---}both theoretically and empirically{---}that contrastive learning objective regularizes pre-trained embeddings{'} anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
}
