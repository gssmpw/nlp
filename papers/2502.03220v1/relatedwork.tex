\section{Background and Related Work}
\subsubsection{Pretraining on the Job Recruitment Domain} Several works have studied pretrained language models in the job recruitment domain. \citeauthor{qin2018enhancing} \shortcite{qin2018enhancing} and \citeauthor{cao2024tarot} \shortcite{cao2024tarot} utilized user activity data from recruitment platforms to enhance language model performance for job recommendation. \citeauthor{zhang-etal-2023-escoxlm} \shortcite{zhang-etal-2023-escoxlm} is a recent effort in multilingual pretraining across 16 European languages, leveraging ESCO taxonomy as a training data for the language model. However, this data require human-labeled training data, which is challenging to obtain for low-resource language. 

To the best of our knowledge, \citeauthor{fang2023recruitpro} \shortcite{fang2023recruitpro} is the only work that has attempted to pretrain a language model by using multiple information on job-related domain. However, this work focus on a monolingual evaluation in English. The effectiveness of the model in a bilingual setting remains unexplored, particularly in terms of cross-lingual capability and language bias in sentence representation. 

% \subsubsection{Sentence Representation} Recent work has utilized pretrained language models (PLMs) such as BERT \cite{vaswani2017attention} and XLM-R \cite{conneau-etal-2020-unsupervised} to encode sentences by combining the embeddings of individual words to represent the entire sentence. Alternative state-of-the art models, such as LaBSE \cite{feng-etal-2022-language}, mUSE \cite{yang-etal-2020-multilingual}, and BGE-M3 \cite{chen-etal-2024-m3}, aim to pretrain on sentence-level representations.

\subsubsection{Language Bias} \citeauthor{roy-etal-2020-lareqa} \shortcite{roy-etal-2020-lareqa} studied the language bias problem inside the sentence representations on a multilingual question-answering retrieval task. They found that the language bias influences performance, as queries often prefer candidates from the corresponding language while neglecting their semantic meaning. The following works proposed various methods to mitigate this language bias on the representation through post-processing matrix transformations \cite{yang-etal-2021-simple,xie-etal-2022-discovering}. Furthermore, the language bias problem also occurs in coding language embeddings \cite{utpala-etal-2024-language}. However, there is a lack of evaluation metrics to quantify the amount of language bias hidden in representations, especially in retrieval settings.