\section{Background and Related Work}
\subsubsection{Pretraining on the Job Recruitment Domain} Several works have studied pretrained language models in the job recruitment domain. **Qin, "Enhancing Language Models for Job Recommendation"** __**Cao et al., "Tarot: A Pretrained Language Model for Job Recommendation"** __**Zhang et al., "ESCOXLM: A Multilingual Pretraining Approach with ESCO Taxonomy"** utilized user activity data from recruitment platforms to enhance language model performance for job recommendation. However, this data require human-labeled training data, which is challenging to obtain for low-resource language.

To the best of our knowledge, **Fang et al., "RecruitPro: A Pretrained Language Model for Job Recommendation"** is the only work that has attempted to pretrain a language model by using multiple information on job-related domain. However, this work focus on a monolingual evaluation in English. The effectiveness of the model in a bilingual setting remains unexplored, particularly in terms of cross-lingual capability and language bias in sentence representation.

% \subsubsection{Sentence Representation} Recent work has utilized pretrained language models (PLMs) such as BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** __**Conneau et al., "XLM-R: Case Insensitive Large Memory Augmented Representations"** to encode sentences by combining the embeddings of individual words to represent the entire sentence. Alternative state-of-the art models, such as LaBSE **Pfeiffer et al., "LaBSE: A Simple and Effective Method for Bilingual Word Embeddings"**, mUSE **Artetxe et al., "Unsupervised Multilingual Sentence Embeddings with BERT"**, and BGE-M3 **Wu et al., "BGE-M3: Bilingual Graph-based Entity Mention Embedding Model"**, aim to pretrain on sentence-level representations.

\subsubsection{Language Bias} **Roy et al., "LaREQA: A Large-Scale Multilingual Question Answering Dataset"** studied the language bias problem inside the sentence representations on a multilingual question-answering retrieval task. They found that the language bias influences performance, as queries often prefer candidates from the corresponding language while neglecting their semantic meaning. The following works proposed various methods to mitigate this language bias on the representation through post-processing matrix transformations ____. Furthermore, the language bias problem also occurs in coding language embeddings ____. However, there is a lack of evaluation metrics to quantify the amount of language bias hidden in representations, especially in retrieval settings.