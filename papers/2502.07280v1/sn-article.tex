%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
% \documentclass[pdflatex,sn-mathphys-num,iicol]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
\documentclass[pdflatex,sn-apa,iicol]{sn-jnl}% APA Reference Style 
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{MIGT: Memory Instance Gated Transformer Framework for Financial Portfolio Management}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1,2]{\fnm{Fengchen} \sur{Gu}}\email{Fengchen.Gu19@student.xjtlu.edu.cn}

\author[1]{\fnm{Angelos} \sur{Stefanidis}}\email{Angelos.Stefanidis@xjtlu.edu.cn}
% \equalcont{These authors contributed equally to this work.}
\author[2]{\fnm{Ángel} \sur{García-Fernández}}\email{angel.garcia-fernandez@liverpool.ac.uk}

\author*[1]{\fnm{Jionglong} \sur{Su}}\email{Jionglong.Su@xjtlu.edu.cn}

\author*[1]{\fnm{Huakang} \sur{Li}}\email{Huakang.Li@xjtlu.edu.cn}

\affil*[1]{\orgdiv{School of AI and Advanced Computing, XJTLU Entrepreneur College (Taicang)}, \orgname{Xi’an Jiaotong-Liverpool University}, \orgaddress{\street{Taicang Road}, \city{Taicang, Suzhou}, \postcode{215400}, \state{Jiangsu}, \country{China}}}

\affil[2]{\orgdiv{Department of Electrical Engineering and Electronics}, \orgname{University of Liverpool}, \orgaddress{\street{Brownlow Hill}, \city{Liverpool}, \postcode{L69 7ZX}, \state{Merseyside}, \country{United Kingdom}}}


%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Deep reinforcement learning (DRL) has been applied in financial portfolio management to improve returns in changing market conditions. However, unlike most fields where DRL is widely used, the stock market is more volatile and dynamic as it is affected by several factors such as global events and investor sentiment. Therefore, it remains a challenge to construct a DRL-based portfolio management framework with strong return capability, stable training, and generalization ability. This study introduces a new framework utilizing the Memory Instance Gated Transformer (MIGT) for effective portfolio management. By incorporating a novel Gated Instance Attention module, which combines a transformer variant, instance normalization, and a Lite Gate Unit, our approach aims to maximize investment returns while ensuring the learning process's stability and reducing outlier impacts. Tested on the Dow Jones Industrial Average 30, our framework's performance is evaluated against fifteen other strategies using key financial metrics like the cumulative return and risk-return ratios (Sharpe, Sortino, and Omega ratios). The results highlight MIGT's advantage, showcasing at least a 9.75$\%$ improvement in cumulative returns and a minimum 2.36$\%$ increase in risk-return ratios over competing strategies, marking a significant advancement in DRL for portfolio management.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Portfolio Management, Deep reinforcement learning, Transformer, Stock Market.}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{s1}

Portfolio management involves determining the allocation of funds to multiple financial assets and continuously changing the portfolio weights to maximize the investment returns \citep{Avramov2002}. The financial investment market is volatile because it is influenced by economic, political, and technological factors \citep{Poon2003}. The financial market data are complex, often consisting of a large volume of historical information such as price, volume, and technical indicators \citep{Liu2021}. The varying conditions of the market environment together with its inherent data complexity, makes it difficult for traditional investment methods to effectively analyze and inform investor decisions \citep{Gao2020, Ren2021}.

Deep learning is a branch of machine learning that uses multiple layers of neural networks to learn representations of data with higher levels of abstraction  \citep{2015Deep}. It has revolutionized the fields of computer vision, speech recognition, and natural language processing \citep{2013Deep, Xia2020Fully}. Reinforcement learning is a machine learning technique in which an agent learns how to operate in an environment by trial and error while maximizing rewards and minimizing penalties \citep{1998Reinforcement, Charpentier2020Reinforcement}. It allows machines and software agents to discover optimal strategies within a specific context automatically \citep{2016Neural}.

Deep reinforcement learning (DRL), combining the perception capabilities of deep learning with decision-making using current knowledge of reinforcement learning, is a practical approach to automated portfolio management \citep{Jiang2017}. The DRL framework learns using feedback from a complex environment and is well suited to addressing dynamic decision-making problems. In portfolio management, stock trading is about making dynamic trading decisions in a complex stock market environment, i.e., deciding when and what to trade, at what price, and the transaction volume \citep{Jiang2018, Yao2022}. 

Recently, researchers applied DRL to portfolio management in the cryptocurrency markets and the stock markets. The cryptocurrency market is particularly volatile in the short term, and high-frequency trading can lead to short-term gains. \citet{Jiang2018} used the Ensemble of Identical Independent Evaluators (EIIE) and policy gradients for cryptocurrency portfolio management. Others subsequently constructed the EIIE framework by modifying the neural networks and adding features as input \citep{Gu2021, Sun2021, Yang2022, Qin2022}. The stock market tends to trade on a single day or over a longer period of time, requiring a long-term portfolio management strategy \citep{Gao2020}. The relatively early framework Investor-Imitator (IMIT) uses reinforcement learning to mimic investor behavior to extract trading knowledge rather than directly using reinforcement learning for portfolio management \citep{2018Investor}. For compatibility with more reinforcement learning algorithms, the FinRL framework presented by \citet{Liu2021} provides reinforcement learning-based stock trading strategies that support single stock trading and portfolio management containing multiple stocks. It supports a variety of reinforcement learning algorithms, e.g., Deep Deterministic Policy Gradient (DDPG) \citep{2018CONTINUOUS}, Soft Actor-Critic (SAC) \citep{2018Soft}, and Proximal Policy Optimization (PPO) \citep{Schulman2017}. Based on FinRL framework, the Ensemble Strategy (ES) uses the DDPG, SAC, and PPO that are selectively applied to different time intervals according to some decision rule \citep{Yang2020}. In addition to FinRL, there is also TradeMaster, an open-source platform for quantitative trading through reinforcement learning, which covers the entire process of designing, implementing, evaluating, and deploying reinforcement learning-based algorithms \citep{sun2023trademaster}. Other works such as SARL augment asset information with price movement prediction and are based on reinforcement learning \citep{Ye2020ReinforcementLearningBP}.

While existing frameworks demonstrate the efficacy of reinforcement learning in portfolio management, some limitations remain. First, the stock market is a complex and volatile environment, with significant fluctuations occurring during trading. This makes training challenging in converging to obtain a reliable policy and update asset portfolio weights effectively \citep{Jos2019, Guan2021}. This may lead to the inability of DRL to train effective strategies to rationally allocate portfolios, resulting in large fluctuations in returns. Second, the existing DRL portfolio management frameworks have insufficient generalization capabilities \citep{Cui2022, Koziarski2020}. Portfolio management should focus more on long-term trends and withstand the impact of short-term volatility, such as "Black swan" events that lead to sharp fluctuations in stock prices. Outliers from such events and other causes can disrupt reinforcement learning's modeling of the normal pattern of stock price movements, leading to the learning of inaccurate associations. Reinforcement learning models may fail to deal with the different features of different market situations, reducing the generalization ability of the strategy \citep{Onireti2016, Kandanaarachchi2020}. Third, the profitability of existing portfolio management models targeting stock markets is not optimal, primarily due to the lack of processing measures for the characteristics of stocks \citep{Gao2022, Soleymani2020}. They contain simple networks with fewer layers or existing networks from other domains without sufficient optimization for portfolio management. In either case, they are unable to adequately handle complex stock data, leading to limited feature extraction capabilities for capturing complex and non-linear relationships in stock data \citep{Zhang2022, Liu101145}. 

The key contributions of our work are threefold. First, the stability of DRL training is enhanced by using the newly constructed Lite Gate Unit (LGU) gating layer as the fan-in layer \citep{Dai2020}, which allows the training to converge smoother and faster, resulting in more effective portfolio management strategies. Second, we propose the use Instance Normalization to balance the scale difference between different feature dimensions of each state, avoiding certain feature dimensions that dominate the model training. Such an approach can reduce the negative impact of single sample outliers and short-term volatility, allowing the network to learn more discriminative feature representations, and improve the generalization ability of the model \citep{Ulyanov2014, Kandanaarachchi2020}. Third, we construct a new Transformer variant (Gated Instance Attention module) to handle complex stock data by leveraging the ability of the model to capture long-range dependencies as well as dependencies among different periods in stock data, which improves the profitability of the DRL portfolio management strategies \citep{Vaswani2017}. It achieves parallel computation through a multi-headed self-attention mechanism, which enables it to efficiently process high-dimensional features of stock data, and eventually improve the profitability of the portfolio management strategies.

The remainder of this paper is organized as follows. In Section \ref {s2}, we define the trading period, make some assumptions about the experimental setting, and give the portfolio management objectives. Section \ref {s3} presents the DRL environment for portfolio management. In Section \ref {s4}, the Memory Instance Gated Transformer (MIGT) policy network is presented. In Section \ref {s5}, we conduct the comparative and ablation experiments as well as offer an interpretation of the results. Finally, conclusions and future work are given in Section \ref {s6}.

\section{Definition}\label{s2}
\subsection{Definition of Data Input, Trading Period and Process}\label{}

Under the DRL framework, the agent reallocates capital into different asset classes at each period $t$, $t\in \mathbb{N}^{+}$. We first consider the process of portfolio management in Figure \ref {fig1}. The input to the portfolio management model is a high-dimensional tensor with dimensions of historical time, stock assets, and historical data including price and technical indicators. We set the length of each trading period $t$ to one day. At the end of each period $t$, the agent trades on the portfolio with ${\ V}_t$, the vector of closing prices of all assets in period $t$, and the portfolio value is $P_t$. Portfolio value is the total value of all the securities in the portfolio and the cash sum, i.e., to obtain the total value of the entire portfolio. 

% Figure
\begin{figure*}
	\centering
	\includegraphics[width=14.5cm]{figureperiod.pdf}
	\caption{Interaction Process for Time Series}\label{fig1}
\end{figure*}


\subsection{Assumptions about the Experimental Environment}\label{}
In order for the constructed trading environment to be as realistic as possible, the following assumptions are made:


% Unnumbered list
\begin{itemize}
	\item Because the trading simulation is based on historical data, we assume our transactions will not affect the price. This is due to the amount traded being tiny relative to the market's overall size, and as such, the impact on stock prices is negligible \citep{Soleymani2020, Jiang2018}.
	\item Since the trading frequency is set to one day, the price of each trade is the previous day's adjusted closing price \citep{Liu2021}. Based on the actions in period $t$, we define the trading actions as sell, buy, and hold. Trading actions by agents are limited to the balance in the account and do not include buying and selling short. 
	\item Different transaction costs are incurred in the stock market, such as trading and execution fees. For compatibility with different trading situations, we assume that our transaction fee rate $c$ is 0.1$\%$ of the value of each trade (buy or sell) \citep{Liu2021}.
\end{itemize}  

\subsection{Portfolio Management Objective}\label{}

The objective of our strategy is to maximize the return of the final portfolio. We define $P_t\in \mathbb{R}^+$ to be the portfolio value at the end of period $t$. The variable $P_t$ contains the available cash $A_t$ and the stock values $E_t$. The available cash in the portfolio at this period is:
\begin{equation}
	A_t=\,\,A_{t-1}+\,\,M_t^TV_t^{\,\,}\,\,\left( 1-c \right) -\,\,B_t^TV_t^{\,\,}\left( 1+c \right), \label{e2}
\end{equation}
where $M_t$ is the share of stock sold in period $t$, $B_t$ represents the proportion of stock bought in period $t$, and $V_t$ is the closing price vector in period $t$. The dimensions of the vectors $V_t$, $M_t$ and $B_t$ are the number of stocks $n$ in the portfolio. The portfolio share vector $W_t$ represents the share of each asset in the portfolio in period $t$. At each time period t, the portfolio share vector $W_t$ is $W_t-1$ from the previous time period adds the shares purchased $B_t$ and subtracts the shares sold $M_t$,
\begin{equation}
	W_t= W_{t-1}+\,\,B_t-\,\,M_t. \label{e3-}
\end{equation}
The value of stock assets $E_t$ at period $t$ is the product of the transposition of the portfolio share vector $W_t$, where the share of each stock is non-negative, and the vector of all assets‘ closing prices $V_t$:
\begin{equation}
	E_t=\,\,W_t^TV_t^{\,\,}=\,\,\left( W_{t-1}+\,\,B_t-\,\,M_t \right) ^TV_t^{\text{
	}}. \label{e3}
\end{equation}
The portfolio's value in period $t$, $P_t$, can be obtained by summing the equations (\ref {e2}) and (\ref {e3}), i.e.,
\begin{equation}
\begin{split}
	P_t =\ A_t+\ E_t \\
        &=\ A_{t-1}+\ {W_{t-1}}^TV_t-\ c{\ \left(B_t+M_t\right)}^TV_t.\label{e4}
\end{split}
\end{equation}
From equation (\ref {e2}), we obtain the value of the stocks in the portfolio in the last period $t-1$,
\begin{equation}
\begin{split}
	P_{t-1}&=\ A_{t-1}+\ E_{t-1}\\&=\ A_{t-1}+\ {W_{t-1}}^TV_{t-1}. \label{e5}
\end{split}
\end{equation}
Subtracting equation (\ref {e5}) from equation (\ref {e4}), we obtain the change in the portfolio value in period $t$, i.e.,
\begin{equation}
\begin{split}
	\bigtriangleup P_t&=P_t-P_{t-1}\\&=W_{t-1}^T\left( V_t-V_{t-1} \right) -c\left( B_t+M_t \right) ^TV_t. \label{e6}
\end{split}
\end{equation}
The change in portfolio value in period $t$ is the increase or decrease in returns at that period, and the goal of portfolio management is to maximize positive returns $\bigtriangleup P_t$ or minimize negative losses during each period $t$.


\section{DRL Environment}\label{s3}
\subsection{The Markov decision process of portfolio management}\label{}

The stock market is stochastic, i.e., the movement of stock prices is random and unpredictable \citep{M.2022Recurrence}. We model the portfolio management task as a Markov decision process problem (Figure \ref {fig2}) to choose an action based on the current state and then randomly move to a new state \citep{Liu2021, Baxter1995}. The training process involves observing changes in state $s_t$, taking actions $a_t$, and calculating rewards $r_t$, allowing the agent to adjust its strategy during the training process. Through this iterative reward-driven learning and feedback process, the DRL agent can train the optimal trading strategy that generates the highest returns for a given environment.
\begin{figure}
	\centering
	\includegraphics[width=8cm]{figure2tp.pdf}
	\caption{The Markov decision process of the DRL Environment, reflects the interaction of state, reward, action, DRL agents and the stock market environment.}\label{fig2}
\end{figure}
\subsection{State, Action, Reward Function, and transaction agent}\label{}

The state $s_t$ is a representation of a particular situation or context that describes the environment \citep{Naeem2020A}. Transaction agents observe many features in an interactive market environment to make sequential decisions \citep{Arulkumaran2017}. To make the model adaptive to stock trends and volatility data, we use different features to help agents to make informed decisions and use them to construct the state. Previous frameworks have generally used the following items as states:
\begin{itemize}
	\item $A_t$, available cash assets of stocks in period $t$;
	\item $W_t$, the share vector of each stock in the portfolio in period $t$; and
	\item $V_t$, the closing price vector in period $t$.
\end{itemize} 

To help portfolio management strategies identify uptrends and downtrends in each stock, the following trend indicators are  included as part of the state: 
\begin{itemize}
	\item Bollinger bands (BOLL) can be used to determine the range of stock price fluctuations and future movements, and to show the safe high and low-price levels of a stock \citep{Murphy1999};
	\item Commodity Channel Index (CCI) mainly measures the variability out of the normal range of prices \citep{Altan2022};
	\item Relative Strength Index (RSI) is a momentum indicator that measures the speed and magnitude of price movements \citep{Altan2022}; and
	\item True Range of Trading (TR) is used to measure the intensity of market volatility \citep{Chang2019}.
\end{itemize} 
Overbought and oversold indicators can reflect the short-term volatility of each stock and help portfolio management strategies perceive risk. Therefore we also include the following volatility indicators: 
\begin{itemize}
	\item Directional Movement Index (DMI) is used to determine the movement of stock prices by analyzing the change in the equilibrium point between buyers and sellers during the rise and fall of stock prices \citep{Seyma2020};
	\item Moving Average Convergence Divergence (MACD)  is a technical indicator that uses the convergence and divergence between the short-term exponential moving average and the long-term exponential moving average of the closing price to make a judgment on the timing of a trade  \citep{Hung2016}; and
	\item Money Flow Index (MFI)  is a technical indicator that uses trading volume and price to determine overbought and oversold \citep{Singleton2014}.
\end{itemize} 
The state tensor in period $t$ is $s_t=\left[f_{1,t},\ f_{2,t},\ \ldots,\ f_{n,t}\right]$, where $n$ the number of stocks in the portfolio. Each element of $s_t$, $f_{k,t}=\left[A_{k,t},\ W_{k,t},\ V_{k,t},\  BOLL_{k,t},\ \ldots,\ MFI_{k,t}\right]$, is the feature vector of $k$-th stock in period $t$, $k=1,2,3,\ldots,\ n$.

To allow the model to deal directly with portfolios rather than individual stock trades, the process of transaction is the portfolio in period $t$ for each asset in $E_t$ after a buy, sell or hold operation \citep{Poon2003, Kumar2021A}. The action $a_t$ is a portfolio share vector $W_t$ that reflects the buying and selling behavior of the transaction agent.

The role of the reward function $r\left(s_t,a_t,\ s_{t+1}\right)$ is to define the goals of DRL and evaluate the transaction agent’s behavior \citep{Lehnert2020Reward-predictive, Arulkumaran2017}. The change in the portfolio value in period $t$ from the previous period $t-1$ is presented in equation (\ref {e6}). Therefore, the reward in period $t$ is:
\begin{equation}
\begin{split}
	r\left( s_t,a_t,s_{t+1} \right) =\bigtriangleup P_t &=W_{t-1}^T\left( V_t-V_{t-1} \right) \\&-c\left( B_t+M_t \right) ^TV_t.
\end{split}
\end{equation}
PPO \citep{Schulman2017} controls the policy gradient update and ensures that the new policy will be numerically close to the previous one so that it can learn incrementally without destabilizing its own learning process. It uses an objective function:

\begin{equation}
\begin{split}
L^{CLIP}\left( \theta \right) = \hat{\mathbb{E}}_t [ &\min \left( r_t\left( \theta \right) \right) \hat{A}_t,\\&clip\left( r_t\left( \theta \right) ,1-\epsilon ,1+\epsilon \right) \hat{A}_t ], 
\end{split}
\end{equation}
where $\theta$ is the policy parameter, ${\hat{\mathbb{E}}}_t$ denotes the empirical expectation over period $t$, $r_t\left(\theta\right)=\ \frac{\pi_\theta\left({a_t\ |\ s}_t\right)}{\pi_{\theta_{old}}\left({a_t\ |\ s}_t\right)}$ is the probability ratio of the new policy to the old one, ${\hat{A}}_t$ is the estimated advantage function in period $t$, and $clip\left(r_t\left(\theta\right),\ 1\ -\ \epsilon,\ 1\ +\ \epsilon\right) $ truncates the ratio to $r_t\left(\theta\right)$ in the range $[1\ -\ \epsilon,\ 1\ +\ \epsilon]$. The $clip$ operation is used to limit the step size of the policy update to prevent the policy update from exceeding a certain threshold, enabling the PPO to be more stable and reliable \citep{Zhu2021A}.

\section{Policy Network}\label{s4}
\subsection{Research Problem}\label{}
DRL is successfully applied to many domains, especially those with restricted state and action spaces that are conducive to exploration. The environments of most DRL tasks are relatively stable, with rules that persist and do not change frequently, i.e., chess and other confrontational games \citep{Chasparis2012Distributed, Khader2021Learning}. The stock market is challenging for DRL due to its instability and dynamics \citep{Parisi2020Reinforcement}. Portfolio management tasks face a more dynamic and volatile environment than typical DRL application domains, being influenced by many external factors such as the interplay between corporate earnings, macroeconomics, and policies \citep{Nasir2014Aspects, Sharif2020COVID-19, Byrne2021The}. Irrational investor behaviors and information asymmetry further destabilize the market, causing drastic fluctuations in individual stock prices \citep{Paule-Vianez2020A}.


In such a complex and dynamic environment, DRL faces three main challenges:
\begin{itemize}
	\item The stock market environment is always changing, but DRL relies on a stable environment to accumulate experience \citep{Khader2021Learning}. Existing DRL models can hardly deal with high-dimensional, non-linear, and dynamic stock data. They usually use fixed-length information, which can lead to truncation or zeroing of the sequence, destroying the original time-series structure and potentially losing valuable historical data \citep{Cao2019Deep}. In addition, basic DRL models have difficulty effectively learning and modeling the long-term dependencies present in stock time series data, which makes it challenging to understand underlying market trends, and the profitability of portfolio strategies is compromised \citep{Gershman2010Context, Hu2021A}. 
    \item In the context of the stock market, company information, policy environment, investment psychology, etc. in the stock market are changing constantly, making the training of DRL models challenging. The frequent emergence of new information in the stock market leads to the continuous need for adaptation of prices and trading strategies. However, typical DRL is inefficient in adapting to new environments, making it difficult to train a stable and reliable strategy \citep{Dayan2008Reinforcement, Gupta2021Deep}. This is manifested in the difficulty of convergence of the training process and the eventual under-training that leads to poor strategy results \citep{Zhang2021A}. 
    \item There are various types of short-term market disturbances and noisy trades in the stock market, and these disturbances flood the data, interfering with the DRL algorithm's identification of effective signals. Normal fluctuations and "Black swan" incidents can expose DRL susceptible to overfitting problems, resulting in strategies that excel at short-term stage but have poor generalization ability \citep{Song2019Observational, Whiteson2011Protecting, Ale2020Dragons}.
\end{itemize}


\subsection{MIGT Framework}\label{}

To address these challenges, we propose the MIGT Framework (Figure \ref {fig3}), as the neural network for PPO input. The input to the portfolio management model is historical data in a high-dimensional tensor with historical period $t$, stock assets number $n$ and and historical data $f$ comprising price and technical indicators. Each period $t$ is a trading day, so the historical time dimension of the input tensor is the number of periods $t$, which is the number of trading days. The stock assets dimension defines the number of stocks $n$. The input data first passes through a standard FC layer, followed by memory trajectory processing. Subsequently, the tensor is processed by the Gated Instance Attention module (Figure \ref {fig5}). The next module is a Position-wise Multilayer Perceptron (PW-MLP) with normalization and fan-in layer. Finally, the data from the PW-MLP are fed into the Logits Multilayer Perceptron (Logits MLP).

\begin{figure}
	\centering
	\includegraphics[width=7cm]{figure3tpn.pdf}
	\caption{the structure of MIGT policy}\label{fig3}
\end{figure}

\subsection{Gated Instance Attention}\label{}

Figure \ref {fig5} gives the structure of the Gated Instance Attention. The Transformer's self-attention mechanism can weigh each position in the input sequence to focus on the information of other positions so that the multi-dimensional information in the stock history data can be processed \citep{Vaswani2017}. As given in Figure \ref {fig4}, the input to the scaled dot-product attention module consists of the query vector $Q$ and key vector $K$ of the dimension and the value vector $V$ of the dimension $d_V$ \citep{Vaswani2017}. We compute the dot product of the query and all keys, dividing each key by the root $d_K$, and use the SoftMax function to derive the weights of the values. In this network, we compute the attention function for a set of queries and store them in the query vector $Q$. The keys and values are also stored in a key vector $K$ and a value vector $V$. We compute the output vector as \citep{Vaswani2017}:
\begin{equation}
Attention\left( Q,K,V \right) =\,\,softmax\left( \frac{Q\,\,K^T}{\sqrt{d_K}} \right) \,\,V
\end{equation}
\\where $\sqrt{d_K}$ represents the scaling factor that prevents inflation in calculations from causing unstable values. The correlation between query and key is calculated by using nonlinear matching functions including dot product, which can deal with the nonlinear relationship and the complex dynamic stock market environment. The model receives all available data in a scaled dot-product attention layer, but it only accesses one representation space. We divide the model into multiple heads, forming multiple subspaces so that the model can learn relevant information in different representation subspaces. Through the multi-head mechanism, different features of multi-dimensional stock data can be learned from different subspaces, which helps to deal with high-dimensional inputs. In order to focus on the different parts of the input sequence, we use four heads to perform the attention computation simultaneously, without sharing parameters before each other and eventually stitching the results together \citep{Vaswani2017}. Simultaneous computation of multiple attentions can focus on different parts of the input, which can help to capture the key characteristic information of the stock time series to deal with changes in its dynamics.

\begin{figure}
	\centering
	\includegraphics[width=6cm]{figure5tp.pdf}
	\caption{the structure of Gated Instance Attention}\label{fig5}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=3.2cm]{figure4tp.pdf}
	\caption{the Scaled Dot-Product Attention structure}\label{fig4}
\end{figure}

Transformers use layer normalization in Natural Language Processing and other fields after calculating self-attention and after the feedforward portion of each encoder block, which is usually applied to the same sample \citep{1214993, Vaswani2017}. In contrast, instance normalization \citep{Ulyanov2014} is used in the attention module as it is applied to each channel data, such as the multi-dimensional time series. Layer Normalization normalizes the whole layer network and loses the specific information of each moment of the time series, while Instance Normalization retains the original differences between each sample, such as the time series information, but only reduces the differences within each sample. Preventing instance-specific mean and covariance shifts mitigates the effect of outliers in stock price and feature data \citep{Kandanaarachchi2020, Ulyanov2014}. Instance normalization reduces the bias of individual samples and allows the network to focus more on global information. This improves the robustness of the model against normal fluctuations and unexpected occurrences, leading to improved overall performance. Instance normalization also facilitates the network to learn more discriminative feature representations, which leads to a further improvement in the return ability of the portfolio management strategy. Therefore, the normalization layer is moved to the front of the attention module so that the normalization layer operation can be applied to the input of the sub-module. The advantage of this reordering is that the reordered network supports the mapping of identities from the input of the first transformer layer to the output after the last layer \citep{Dai2020}. The layer canonical reordering results in a path where the two linear layers are applied sequentially, the non-linear ReLU activation is applied to the output stream.

We propose a novel approach to construct the LGU gating layer ($g$ in function \ref {e12}) as a fan-in layer to replace the remaining connections after the attention module and PW-MLP. Unlike the Gated Recurrent Unit \citep{Chung2014}, we use an update gate $z$, avoiding the offsetting effect of the gated outputs. This simplifies the computation process and reduces the number of derivable parameters, thus resulting in a smoother gradient. We use $sigmoid$ as the activation function in the update gate $z$ and the hidden layer $h$, which has a smooth function derivative curve with non-zero gradients at both ends to avoid vanishing gradients. LGU can be represented by equations (\ref {e10}), (\ref {e11}) and (\ref {e12}), where the update gate $z$ replaces the information in the previous state selectively instead of discarding it,
\begin{equation}
	z=sigmoid\left(W_zy+\ U_zx-\ b_g\right). \label{e10}
\end{equation}
The hidden layer $h$ is used to store the calculation results, which makes it easier for the model to capture information at more distant moments and to learn long-term dependencies:
\begin{equation}
	h=sigmoid\left(W_gy+\ U_g(z\odot x)\right). \label{e11}
\end{equation}
The final calculation result is output by the output layer $g$, i.e.,
\begin{equation}
	g\left( x,y \right) =\left( 1-z \right) \odot x^{\,\,}+z\odot h^{\,\,} , \label{e12}
\end{equation}
where $g$ is a gating layer function, $y$ is the input, $x$ is the residual value, and $\odot$ is the elemental multiplication. $W$ and $U$ are parameters for each gating neuron. Using LGU as gating layer can reduce the possibility of the model learning irrelevant information and, as such, improve the model’s attention to important information, thus improving the stability of learning.

\subsection{PW-MLP, Logits MLP and Memory Trajectory}\label{}

The PW-MLP retains the dimensionality of the inputs and outputs while increasing the expressiveness of the model. As given in Figure \ref {fig6}, it applies two standard FC layers to each input position individually \citep{Dai2020}. The input size of the first layer is attention dimension size 64, and the output dimension is PW-MLP dimension 32. The input and output sizes of the second layer are the opposite of that of the first layer, i.e., its input and output sizes are PW-MLP dimension, and attention dimension, respectively. This allows the PW-MLP to learn non-linear transformations at each position independently, which enhances the model's ability to capture complex relationships in the data, i.e., increasing the expressiveness of the model. The final layer used to output values is the Logits MLP in Figure \ref {fig6}, which consists of two FC layers \citep{Goodfellow2014}. The first layer has the same input dimension as the attention module, and the output outputs have the same size as the attention. The output of the final layer will be used by the PPO algorithm to update the model.

\begin{figure}
	\centering
	\includegraphics[width=6cm]{figure6tp.pdf}
	\caption{the PW-MLP (left) and Logits MLP (right)}\label{fig6}
\end{figure}

The basic attention operation does not explicitly consider the sequence order because it is reciprocally invariant \citep{Vaswani2017}. As such, we incorporate a novel recursive mechanism into the Transformer architecture to address the limitation of using fixed time length information. In each step of the recursion, the model uses the information of the current time step and the previous time steps to generate output, and the output of the previous step is used as the input of the next step, which continues to generate the output until the end of the sequence. To allow the model to fuse current input and historical memory to deepen its learning ability, we cache a predetermined length of old hidden states across multiple segments and refer to them as memory. In the memory trajectory scheme, there is an additional $t$-step memory tensor $M_t(I,D)$, where $I$ is memory inference and $D$ refers to past memory, whose value is the attention dimension. Memory is used as an additional input to the attention layer during each training session. By using memory states as additional input to the neural network, the attention mechanism can consider current and historical information, improving the model’s inference and comprehension.

\section{Experiments}\label{s5}
\subsection{Data Selection}\label{}

To assess the performance of the MIGT framework, we conduct experiments using historical data, using 30 stocks from the Dow Jones Industrial Average (DJIA) \citep{DJIA} as the portfolio. We use three datasets of calendar years 2019, 2020, and 2021, as the test sets for Experiments 1, 2, and 3, respectively. The training set for each experiment is the data from the three years prior to their test set. The years of these datasets represent the pre-COVID-19, early, and mid-term COVID-19 years, respectively. The specific time composition of the training set and test set are given in Table \ref{tbl1}.

\begin{table*}
        \centering
	\caption{Time composition of the training and test set}\label{tbl1}
	\begin{tabular}{l l l l}
		\toprule
		Dataset & Data purpose& Training data set& Back-test data interval  \\% Table header row
		\midrule
		1 & Back-test 2019 & 2016.01.01– 2018.12.31 & 2019.01.01– 2019.12.31  \\
		2 & Back-test 2020 & 2017.01.01– 2019.12.31 & 2020.01.01– 2020.12.31  \\
		3 & Back-test 2021 & 2018.01.01– 2020.12.31 & 2021.01.01– 2021.12.31  \\
		\bottomrule
	\end{tabular}
\end{table*}


\subsection{Performance Measures}\label{}
The most direct and effective way to evaluate portfolio management frameworks is to compare cumulative rates. We use the cumulative rate of return as the metric, i.e.,
\begin{equation}
	Cumulative\ rate\ of\ return\ =\ \frac{P_T\ -P_0}{P_0}\,
\end{equation}
where $P_0$ is the initial money of the portfolio, $T$ is the total number of periods $t$ and $P_T$ is its final value. The cumulative rate of return provides a direct representation of the absolute level of return of an investment strategy, which gives how much profit the investment strategy can generate. However, the cumulative rate of return does not reflect the level of risk as it only calculates all periodic returns without considering fluctuations over the investment \citep{Bollerslev2020Good}. The Sharpe ratio \citep{Sharpe1994} is the most widely used risk-return indicator,

\begin{equation}
	Sharpe\ Ratio\ =\ \frac{R_p-R_f}{\sigma_p},
\end{equation}
where $p$ is the portfolio, $R_p$ is the return on the asset, $R_f$ is the risk-free return, which value was set to 3$\%$ per year (the highest level from 2019 to 2021) in our experiments, and $\sigma_p$ is the standard deviation of the asset. This ratio measures the amount of one receives for every unit of risk taken. Therefore, the higher the value of the Sharpe ratio, the higher the portfolio's investment return for the same risk.

When evaluating a portfolio, it is important to consider not only the general level of risk but also the potential for downside risk. The potential for downside risk refers to the possibility of experiencing losses when returns are negative. The Sortino ratio \citep{Rollinger2015} is a risk-adjusted indicator to determine the additional return that an investment generates per unit of downside risk, 
\begin{equation}
	Sortino\ Ratio\ =\ \frac{R_p-r}{\sqrt{\frac{1}{T}\sum_{t=0}^{T}\left(R_{p_t}-r\right)^2}},
\end{equation}
where $r$, whose value was set to 3$\%$ per year in our experiments, is the minimum acceptable return we are considering, $T$ is the total number of periods $t$, and $R_{p_t}$ rate of return value for stock $p$ in period $t$. A higher Sortino ratio means that the investor will receive a higher return per unit of downside risk.

To comprehensively evaluate the risk-return characteristics of the portfolio, we also use the Omega ratio \citep{Keating2002}. It does not depend on the overall distribution of portfolio returns, but is calculated directly using the cumulative distribution function, i.e.,
\begin{equation}
	Omega\,\,Ratio\,\,=\,\,\frac{\int_r^{\infty}{\left( 1-F\left( x \right) \right)}dx}{\int_{-\infty}^r{F}\left( x \right) dx},
\end{equation}
where $F$ is the cumulative distribution function of the returns. This ratio indicates the extent to which the gain component exceeds the loss component and measures the odds of winning versus losing. A higher Omega ratio means that the investment provides a higher return relative to the downside risk assumed.



\subsection{Comparative Strategies}\label{}
To assess the performance of our strategy, traditional statistical strategies and DRL strategies are used as comparative strategies. The traditional statistical strategies we use are based on mean reversion and trend following.


The mean reversion strategies \citep{Fil2020Pairs, Mousavi2021A} aim to take advantage of stock price fluctuations, as stock prices constantly fluctuate over a certain period. The strategies based on mean reversion are:

\begin{itemize}
	\item Confidence Weighted Mean Reversion (CWMR) \citep{Li2011} models the portfolio vector as a Gaussian distribution and updates the distribution sequentially by following the mean reversion trading principle.
	\item Online Moving Average Reversion (OLMAR) \citep{Li2012} uses multi-period moving average regression.
	\item Passive Aggressive Mean Reversion (PAMR) \citep{Lii2012} relies on the mean reversion relationship of financial markets and utilizes online passive-aggressive learning techniques in machine learning.
	\item Robust Median Reversion (RMR) \citep{Huang2016} takes advantage of the mean reversion properties of financial markets and uses a technique called "robust L1-median estimation" to solve the outlier problem in mean reversion.
	\item Transaction costs optimization (TCO) \citep{Li2018} is a strategy for non-zero transaction costs that combines the L1 parametrization of the difference between two consecutive allocations with the principle of maximizing expected logarithmic returns.
	\item Weighted Moving Average Mean Reversion (WMAMR) \citep{Gao2013} is calculated by taking a weighted moving average of stock prices to predict the upward or downward trend of stock prices.
\end{itemize} 

Trend-following strategies \citep{Fousekis2021Returns, Takada2022Trend-following}, on the other hand, aim to take advantage of stock price trends. Strategies based on the trend following are:

\begin{itemize}
	\item The algorithm selects the asset with the best performance on the last day (BEST) \citep{Jiang2017},
	\item Nearest neighbor-based strategy (BNN) \citep{Laszlo2006} uses proximity to classify or predict groups of individual data points.
	\item Correlation-driven nonparametric learning approach (CORN) \citep{Lii2011} is a correlation-based nonparametric learning approach that uses correlations to infer relationships between variables.
\end{itemize} 

 EIIE, IMIT, FinRL, ES, TradeMaster, and SARL are used in our experiments as DRL comparative frameworks. EIIE has performed well in the cryptocurrency portfolio management space \citep{Jiang2017}, and we have migrated their framework to the stock portfolio space and optimized it for the stock market. IMIT formalizes trading knowledge by mimicking investor behavior using a set of logical descriptors and introduces a Rank-Invest model that learns to optimize different evaluation metrics to maintain the diversity of logical descriptors \citep{2018Investor}. FinRL is a well-structured and effective basic framework for automated trading using DRL \citep{Liu2021}. ES combines the best features of three actor-critic-based algorithms and is a novel portfolio management framework \citep{Yang2020}.  In addition to enabling DRL-based quantitative trading (including portfolio management), TradeMaster introduces automated machine learning techniques to tune the hyperparameters that train the reinforcement learning algorithms \citep{sun2023trademaster}, which we will use in conjunction with the PPO algorithm, TradeMaster\_PPO (TMP), for comparison. SARL uses asset information and price trend prediction as additional states to incorporate heterogeneous data and enhance robustness to environmental uncertainty, while price trend prediction can be based on financial data \citep{Ye2020ReinforcementLearningBP}. 



\subsection{Comparative Results}\label{Results}


The results of the experiments are given in Table \ref {tbl2}, where the best results for each group are given in bold. The experimental results show that MIGT performs best in all three experiments. The MIGT outperforms the comparative strategies by at least 9.75$\%$, suggesting that our strategy has a stronger ability to capture returns. In terms of the Sharpe ratio, our framework is at least 0.2072 higher than the comparative strategy, indicating that our framework can generate higher risk-adjusted returns per unit level of risk. The Sortino ratio of our strategy is at least 0.3858 higher than the comparative strategy, suggesting that our framework can generate higher excess returns per unit of downside risk. MIGT's Omega ratio remains the highest of the three experiments, with an margin of at least 0.0286 in the experiment, although it has a slight advantage over the other metrics. This demonstrates that, for the same task and dataset, our framework has a higher probability of obtaining positive returns and stronger sustained profitability.

\begin{table*}
        \centering
	\caption{Results of the comparative experiments, where the best results for each metric are in bold.}\label{tbl2}
	
	\begin{tabular}{l l l l l}
		\toprule
		Strategies& Cumulative returns & Sharpe ratio & Omega ratio & Sortino ratio \\  
		\toprule
		Dataset 1\\
		\midrule
		\pmb{MIGT}  &\pmb{0.38443}  & \pmb{1.72498}  &  \pmb{1.33987} &  \pmb{2.74069}       \\
        EIIE & 0.17211 & 0.95176 & 1.19001 & 1.65438 \\
        IMIT  & -0.00403 &	-0.06185 &	0.98952 &	-0.08255 \\
        Finrl & 0.09105 & 0.42895 & 1.07678 & 0.82861 \\
        ES & 0.17291 & 1.35619 & 1.27595 & 2.30452 \\
        TMP & 0.21951 &	1.51779 &	1.30771 &	2.14405 \\
        SARL & 0.20178 &	1.36616 &	1.25836 &	1.93005\\
        BEST & -0.10451 & -0.50668 & 0.91478 & -0.54222 \\
        BNN & -0.42753 & -3.09789 & 0.59106 & -3.54361 \\
        CORN & -0.37279 & -2.38758 & 0.64868 & -2.65702 \\
        CWMR & -0.28261 & -1.28733 & 0.79902 & -1.47289 \\
        OLMAR & -0.32590 & -1.48064 & 0.77743 & -1.72265 \\
        PAMR & -0.28734 & -1.30985 & 0.79574 & -1.49827 \\
        RMR & -0.30868 & -1.39968 & 0.78643 & -1.64853 \\
        TCO1 & -0.03966 & -0.32699 & 0.94653 & -0.20877 \\
        WMAMR & -0.05699 & -0.25328 & 0.95863 & -0.17016 \\
		\toprule
		Dataset 2\\  % Table header row
		\midrule
		\pmb{MIGT}     & \pmb{0.24440}      & \pmb{0.70689}      & \pmb{1.14642}     & \pmb{1.18089}       \\
		EIIE & 0.14689 & 0.47497 & 1.10464 & 0.79507 \\
        IMIT & -0.25268 &	-0.51091 &	0.90696 &	-0.67185 \\
        Finrl & 0.01942 & 0.19664 & 1.04048 & 0.37425 \\
        ES & 0.05654 & 0.22474 & 1.04322 & 0.50492 \\
        TMP & 0.07834 &	0.30417 &	1.06416 &	0.42602 \\
        SARL & 0.08555 &	0.32463 &	1.06734 &	0.46560 \\
        BEST & -0.44279 & -0.64628 & 0.88068 & -0.90213 \\
        BNN & -0.31112 & -0.48298 & 0.90252 & -0.68164 \\
        CORN & -0.04534 & 0.00672 & 1.00147 & 0.11417 \\
        CWMR & -0.60932 & -1.33623 & 0.74598 & -1.62408 \\
        OLMAR & -0.29166 & -0.21499 & 0.95504 & -0.22308 \\
        PAMR & -0.61574 & -1.35462 & 0.74277 & -1.64354 \\
        RMR & -0.25559 & -0.20466 & 0.95724 & -0.20269 \\
        TCO1 & -0.50058 & -0.97505 & 0.79868 & -1.18684 \\
        WMAMR & -0.15039 & 0.04687 & 1.01031 & 0.12636 \\

		\toprule
		Dataset 3\\  % Table header row
		\midrule
		\pmb{MIGT}      & \pmb{0.28336}            & \pmb{1.30133}      & \pmb{1.24129}     & \pmb{2.18922}       \\
		EIIE & 0.13370 & 0.54816 & 1.10077 & 0.97863 \\
        IMIT & 0.14955  &	0.76728 &	1.13437 &	1.10643 \\
        Finrl & 0.11807 & 0.56680 & 1.09693 & 1.08441 \\
        ES & 0.16076 & 0.89057 & 1.16002 & 1.63785 \\
        TMP & 0.16901  &	1.16026 &	1.21270 &	1.67973 \\
        SARL & 0.14715 &	0.93386 &	1.16735 &	1.33449 \\
        BEST & -0.37901 & -1.58421 & 0.74931 & -1.90757 \\
        BNN & 0.12454 & 0.48104 & 1.08307 & 0.95923 \\
        CORN & -0.26964 & -1.18858 & 0.80911 & -1.36825 \\
        CWMR & -0.41946 & -2.23410 & 0.67878 & -2.70346 \\
        OLMAR & -0.24008 & -1.07469 & 0.83282 & -1.32658 \\
        PAMR & -0.41617 & -2.21117 & 0.68143 & -2.67586 \\
        RMR & -0.29132 & -1.33161 & 0.79731 & -1.64067 \\
        TCO1 & -0.13407 & -0.92436 & 0.85288 & -1.07723 \\
        WMAMR & -0.21895 & -1.02360 & 0.83832 & -1.27236 \\

		\bottomrule
	\end{tabular}
\end{table*}

Figures \ref {figc1}, \ref {figc2} and \ref {figc3} give the ratios of the portfolio value to the initial value for each day in the experiments. The backtest result of MIGT does not initially exhibit an obvious advantage in the first 2 months, maintaining performance comparable to the top three strategies, as the DRL strategy needs time to adjust, optimize, and obtain the best investment allocation. However, after about 3-8 months of adaptation to the market, the effectiveness starts to become evident. In approximately the last four months, our strategy shows a clear advantage and outperforms the other strategies substantially until the end of the backtest. Our strategy adapts over time to changes and complexities in the market environment through DRL. The initial period where the performance is comparable to other strategies reflects the time required to learn effective policies tailored to the market. Once learned, the DRL strategy enables the portfolio to leverage opportunities and risks, demonstrating robust returns. In contrast, the other strategies appear limited to static rules that fail to fully adapt to the dynamic market, ultimately limiting their performance.


\begin{figure}
	\centering
	\includegraphics[width=8.5cm]{figuref1.png}
	\caption{Results of the comparative experiments of data 2019.}\label{figc1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=8.5cm]{figuref2.png}
	\caption{Results of the comparative experiments of data 2020.}\label{figc2}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=8.5cm]{figuref3.png}
	\caption{Results of the comparative experiments of data 2021.}\label{figc3}
\end{figure}

To further investigate how our MIGT can achieve higher cumulative returns, we visualize the portfolio vector and the daily returns of individual stocks in Figure \ref {figr}, using the dataset 2 as an example. As can be seen in Figure \ref {figr}(a), from approximately the 90th trading day (late April/early May) to approximately the 190th trading day (end of August/beginning of September), our transaction agent increased its holdings of `APPL' stocks and maintained the highest position. We looked at the share prices of each stock on 1 May 2020 versus 1 September 2020 and calculated the return on each stock for that period. The `APPL' stock had a return of 86.51$\%$ during this period, which is the highest among the stocks in the portfolio and much higher than the average return of 19.38$\%$ of the stocks in the portfolio. Combined with the Figure \ref {figr}(b), we can see that a large increase in holdings of `APPL' led to a significant increase in our agent's investment returns, and when it experienced shocks and declines later in the year, our agent reduced a large portion of those holdings, allowing for the majority of the returns to be retained. These show that our portfolio management strategy has been successful in giving higher weights to stocks with good return capacity in the position vector and succeeded in stopping losses promptly in a declining market without relying on stock price forecasts. 

\begin{figure*}
    \centering
    \includegraphics[width=17cm]{stockreturnsnew.pdf}
    \caption{Schematic diagram of the process of adjusting the stock shares by the transaction agent during each trading period $t$, and the gain or loss incurred by each stock in the portfolio.}\label{figr}
\end{figure*}

To validate MIGT's performance over the long term, we use the 2019-2021 three-year data, i.e., combining dataset 1, 2, and 3, as a comparative experiment for the test set. As given in table \ref {tbla}, MIGT has at least 126.21$\%$ more cumulative returns than the comparative strategies. This suggests a greater advantage in the long-term return capacity of our strategy. As we can see from Figure \ref {figa}, MIGT begins to show a large advantage (about 50$\%$) in the early-middle period, and this advantage grows larger and larger (over 100$\%$) over time.

\begin{table*}
        \centering
	\caption{Three years of data from 2019-2021 were used as a test set to compare the results of the experiments, with the best results for each metric given in bold.}\label{tbla}
	
	\begin{tabular}{l l l l l}
		\toprule
		Strategies& Cumulative returns & Sharpe ratio & Omega ratio & Sortino ratio \\  
		\midrule
		\pmb{MIGT} & \pmb{1.88143} & \pmb{1.29061} & \pmb{1.28029} & \pmb{2.06325} \\
        EIIE & 0.55154 & 0.60861 & 1.14241 & 1.05018 \\
        IMIT  & 0.23495 &	0.28639 &	1.06158 &	0.41472 \\
        Finrl & 0.61929 & 0.52287 & 1.11752 & 0.92588 \\
        ES & 0.40192 & 0.57728 & 1.11972 & 1.03286 \\
        TMP & 0.53258 &	0.62228 &	1.14459 &	0.87164 \\
        SARL & 0.46130 &	0.54132 &	1.12693 &	0.75890 \\
        BEST & -0.61385 & -0.63775 & 0.88001 & -0.80744 \\
        BNN & -0.72627 & -1.25120 & 0.78986 & -1.49930 \\
        CORN & -0.12466 & -0.07156 & 0.98656 & 0.03114 \\
        CWMR & -0.82279 & -1.29828 & 0.75522 & -1.58723 \\
        OLMAR & -0.60072 & -0.53739 & 0.89345 & -0.61732 \\
        PAMR & -0.82657 & -1.31035 & 0.75346 & -1.59889 \\
        RMR & -0.57034 & -0.51821 & 0.89802 & -0.59114 \\
        TCO1 & -0.54912 & -0.63553 & 0.86083 & -0.73330 \\
        WMAMR & -0.08868 & 0.07072 & 1.01535 & 0.19846 \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{figure}
	\centering
	\includegraphics[width=8cm]{figuref4.png}
	\caption{Results of the comparative experiments of data from 2019-2021.}\label{figa}
\end{figure}

\subsection{Ablation Study}\label{}

Section \ref {Results} illustrates the performance of the MIGT framework. To analyze the impact of the individual modules on the effectiveness of our framework, we perform ablation experiments. The experiments provide a principled approach to understanding how and why the proposed framework works. To investigate the effectiveness of the Gated Instance Attention module, we modify it for the following version branches: the version with instance normalization removed (MIGT\_w/o\_Norm), the version with the gating layer removed (MIGT\_w/o\_Gating), and the version with the Transformer variant removed (MIGT\_w/o\_Transformer).


\begin{table*}
        \centering
	\caption{Results of the ablation experiments, where the best results for each metric are in bold.}\label{tbl3}
        \begin{tabular}{l l l l l}
		\toprule
		Strategies& Cumulative returns & Sharpe ratio & Omega ratio & Sortino ratio \\  
		\toprule
		Dataset 1\\
		\midrule
		\pmb{MIGT}  &\pmb{0.38443}  & \pmb{1.72498}  &  \pmb{1.33987} &  \pmb{2.74069}       \\
		MIGT\_w/o\_Norm   & 0.30139            & 1.46608       & 1.28920     & 2.39840        \\
		MIGT\_w/o\_Gating   & 0.24648            & 1.46140      & 1.27858     & 2.39581       \\
		MIGT\_w/o\_Transformer & 0.14537            & 0.73047      & 1.12926     & 1.29738       \\
		\toprule
		Dataset 2\\  % Table header row
		\midrule
		\pmb{MIGT}     & \pmb{0.24440}      & \pmb{0.70689}      & \pmb{1.14642}     & \pmb{1.18089}       \\
		MIGT\_w/o\_Norm   & 0.22806 &	0.59541	& 1.12202	& 0.93648        \\
		MIGT\_w/o\_Gating   & 0.20915 &	0.59697	& 1.12509	& 0.93752       \\
		MIGT\_w/o\_Transformer & 0.06440 &	0.30429	& 1.05949	& 0.55714       \\
		\toprule
		Dataset 3\\  % Table header row
		\midrule
		\pmb{MIGT}        & \pmb{0.28336} &	\pmb{1.30133}	& \pmb{1.24129}	& \pmb{2.18922}       \\
		MIGT\_w/o\_Norm   & 0.26727 &	1.19381	& 1.22241	& 1.93993       \\
		MIGT\_w/o\_Gating   & 0.25453 &	1.25659	& 1.22870	& 2.10863       \\
		MIGT\_w/o\_Transformer & 0.07674 &	0.34320	& 1.05727	& 0.74882       \\
				\bottomrule
	\end{tabular}
\end{table*}

\subsubsection{Overall ablation experiments with cumulative returns}\label{}
The results of the ablation experiments in Table \ref {tbl3} show that the return and ratio indicators for each group that underwent ablation are worse than MIGT. MIGT\_w/o\_Norm's impact on cumulative returns averaged -5.79$\%$, which suggests that the use of instance normalization to mitigate the impact of outliers has an obvious effect. However, this gap narrows to 1.61$\%$ in the experiments with 2021 data, suggesting that the outliers all have different degrees of impact on return capacity under different data sets. MIGT\_w/o\_Gating has a higher impact (-8.67$\%$), where the cumulative return in the experiment for the dataset 1 was 30.14$\%$, the difference with the full framework amounted to -8.39$\%$. This suggests that improving the stability of DRL training enhances the portfolio strategy's profitability. MIGT\_w/o\_Transformer model reduced both the return and the risk-return ratio to the lowest levels in the ablation experiment. The three-year average annual rate of return decreased by 22.76$\%$. The strategy applying Attention and Memory Trajectory increases the return by an average of 5.25$\%$ compared to the basic reinforcement learning framework. This shows that using the Gated Instance Attention module with a Transformer variant to handle stock data adequately has better performance.

\subsubsection{Ablation experiments for instance normalization}\label{}
We introduce pseudo-data into the training dataset to test the effectiveness of instance normalization in mitigating the impact of outliers. Figure \ref {figab1} shows that when pseudo-data is not added, the average cumulative return of the MIGT strategy with instance normalization is 0.3027, which is higher than that without 0.2652. This shows the effect of instance normalization itself on the raw data. As the percentage of pseudo-data added increases, the average cumulative returns of both strategies decrease. This indicates that the outliers do have a negative impact on strategy performance, but the average cumulative return of the MIGT strategy using normalization decreases to a lesser extent. When 5$\%$ pseudo-data is added, the MIGT strategy loses 4.91$\%$ of its return and the MIGT\_w/o\_Norm strategy loses up to 22.67$\%$. When 10$\%$ pseudo-data is added, the MIGT strategy return only decreases to 0.2512 with 17.01$\%$ loss, while the unused one decreases to 0.1390 with 47.60$\%$ loss. This suggests that using instance normalization can significantly mitigate the negative impact of outliers and help the strategy maintain higher and more stable cumulative returns.

\begin{figure*}
	\centering
	\includegraphics[width=15cm]{ablation1a.pdf}
	\caption{The effect of adding outlier pseudo-data on the cumulative return capacity of the strategy. Figure (a) shows the average cumulative return and Figure (b) shows the percentage of return lost compared to when no pseudo-data is added.}\label{figab1}
\end{figure*}

\subsubsection{Ablation experiments for LGU Gating Layer}\label{}
We conducted ablation experiments, as given in Figure \ref{figab2}, to determine if the LGU Gating Layer improves the training of DRL. The cumulative return of the MIGT strategy increases steadily with the increase of training steps, reaching a peak of 0.238166 at 10,000 steps, and then remains stable at about 0.2429. The cumulative return of the MIGT\_w/o\_Gating strategy also rises with the increase of training steps but with a much smaller increase. The cumulative return of the MIGT strategy is significantly higher than that of the MIGT\_w/o\_Gating strategy, especially at the early stage when the number of training steps is small, and the difference between the two strategies is even larger. This illustrates the effect of using the LGU Gating Layer in the MIGT strategy. At the later stage, when there are more training steps, the cumulative returns of the two strategies stabilize, and the growth slows down, indicating that the strategies have basically converged. However, MIGT can converge to a higher level of cumulative returns than the strategy without a gating layer, suggesting that using the LGU gating layer results in a faster and more efficient output of the strategy.

\begin{figure*}
	\centering
	\includegraphics[width=15cm]{ablation2a.pdf}
	\caption{Line plots of training steps and cumulative returns for DRL portfolio management strategies. Figure (a) is by average cumulative return. Figure (b) is the ratio of cumulative return to this number at the completion of training.}\label{figab2}
\end{figure*}

\section{Conclusions and Future Work}\label{s6}

In this work, we have introduced MIGT, a novel framework for portfolio management using DRL, which enhances the performance of the portfolio framework with Gated Instance Attention, memory Trajectory, and MLP. The proposed Gated Instance Attention module combines a variant of Transformer, instance normalization, and LGU gating layer, which is crucial for increased profitability, and improved convergence for effective strategies. Our framework overperforms previous reinforcement learning portfolio frameworks in handling multi-dimensional data by resulting in higher stability of reinforcement learning, less influence of outliers, and a larger range of time series data processing. Experiments show the superiority of MIGT, as demonstrated by achieving higher return and risk-return metrics.

However, our current work has the following limitations. First, while our MIGT strategy achieves higher returns compared to other strategies, its advantage is less significant in terms of the Omega ratio which accounts for risk-adjusted returns. Our portfolio management framework utilizes some risk-related factors as input, but more comprehensive risk modeling could further improve risk-adjusted performance \citep{Maeso2020Maximizing}. Second, the Gated Instance Attention mechanism enhances our strategy's performance, but the network architecture and reinforcement learning algorithms can be further optimized for stock data and stock market characteristics. Third, short-selling operation is not allowed in our research assumptions. However, it provides valuable hedging and profit opportunities in practice, especially during market downturns \citep{Wang2020Shorting, Umar2021Media}. The constrained long-only formulation limits the strategy space and practical applicability in real-world portfolio management involving shorts. 

For future research, we suggest the following directions. First, we expand the risk modeling capabilities of our framework by incorporating additional risk factors as inputs. A more comprehensive set of risk metrics could further improve the risk-adjusted returns of our portfolio strategies. Second, we continue to optimize the neural architecture, with a focus on the attention components. By customizing the attention design for stock data properties, such as periodicity and volatility clustering, we can potentially improve performance. We will explore modifications to the Scaled Dot-Product Attention and other attentive mechanisms. Third, we enable flexible mixed long/short positions to allow our reinforcement learning agents to learn more advanced strategies with improved performance in both bull and bear regimes. At the same time, risk control mechanisms need to be in place to prevent massive losses from overleveraged short positions. For example, the reinforcement learning formulation should include additional guardrails and constraints around shorting as part of the action space. 

\section*{Declarations}

\pmb{Funding} No funding was received for this work.\\
\\
\pmb{Data Availability} Data will be made available on request.\\
\\
\pmb{Conflict of interest} All authors declare that he has no conflict of interest.\\
\\
\pmb{Ethical approval} This article does not contain any studies with human participants or animals performed by any of the authors.\\
\\
\pmb{Informed consent} Informed consent was obtained from all individual participants included in the study

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{references}% common bib file
% \bibliographystyle{apacite}
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
