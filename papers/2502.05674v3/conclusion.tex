
\paragraph{Main findings} In general, novel test conditions are unavoidable, for example, due to the fact that new synthesis methods are continually being developed.  We find that state-of-the-art synthetic speech detectors are vulnerable to a wide range of distribution shifts. Given these results, one may wonder whether simply collecting more training data in more varied conditions offers a potential solution. Surprisingly, our results suggest the opposite: to generalize well in unknown test conditions, training on \emph{less} diverse training data---a single vocoder and ensemble of as few as four speaker models \emph{without augmentations}---appears to be the more effective strategy (\autoref{sec:itw}). As evident from inconsistent performance gain across different distribution shifts in \autoref{sec:augmentation}. We suspect that a partial explanation of better detection performance using single vocoder system is that, by not including older, worse generation systems in the training set, the detector has no opportunity to overfit shortcuts specific to those systems. 

\paragraph{Future work} Our findings suggest two main directions for the future aiming to improve the robustness of synthetic speech detectors. First, we suspect that more sophisticated training objectives for detectors, for example, based on identifying specific pairs of contrasting examples of real and fake speech, could enable more successful training of detectors on more heterogeneous datasets. Second, while our study focuses on the zero-shot setting, in certain applications it is reasonable to assume that a small sample of synthetic speech is available, which opens the door to building few-shot detectors that could adapt to evolving test conditions~\cite{soto2024fewshotdetectionmachinegeneratedtext}. 

\paragraph{Limitations} We perform systematic experiments with varying training and test conditions to understand the impact of distribution shifts on detection performance. However, due to space and computing limitations, we are naturally unable to consider all possible conditions. Specifically, we focus on a relatively narrow set of languages that are well-supported by existing synthesis methods, including English, Chinese and Japanese. We further focus our attention on state-of-the-art synthesis methods, at the possible expense of older methods that may exhibit different generalization patterns. The data augmentations considered in~\autoref{sec:augmentation} focus on a limited number of noise types. Finally, we choose to focus on a small number of state-of-the-art detector architectures that use self-supervised representations, specifically SSL-AASIST, as we expect this architecture to be the most robust to the kinds of distribution shifts we study here.

\section*{Acknowledgment}
This work was supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the ARTS Program under contract D2023-2308110001. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.

We thank Rafael Rivera-Soto, Rachel Wicks and Andrew Wang for their valuable discussions and feedback, which contributed to this work.
