\section{TTS distribution-shifts}\label{sec:tts_shifts_section}
In this section, we study distribution-shifts relative to TTS systems.

\subsection{Training on Vocoded speech vs training on TTS speech}\label{sec:exp-9}
For this experimental setup, detectors are trained on LJSpeech dataset such that TTS system used to generate synthetic speech utilize HFG vocoder. For the TTS systems, we use the transcripts from LJSpeech dataset to generate corresponding samples. Each of the TTS system we utilize are pre-trained on LJSpeech dataset with HFG as the vocoder. Information about pre-trained models can be found in \autoref{tab:train_details_tts}. For the vocoder, we convert the genuine waveform into acoustic features and then reconstruct the waveform using the HFG vocoder trained on LJSpeech  \footnote{\url{https://huggingface.co/speechbrain/tts-hifigan-ljspeech}}. While we train individual detectors with different TTS systems, we also train detectors with synthetic speech consisting of just the vocoded speech. Additionally, we also train a system using both TTS + vocoded speech. Please note that the set of TTS systems used in training as well as evaluations are -- Grad-TTS, VITS and Glow-TTS. Evaluation data is a test split of LJSpeech dataset (in-domain). 

Detectors are trained with a learning rate of 1e-6, weight decay of 0.0001 and batch size of 64. In particular, the detectors trained on TTS speech are trained for 50 epochs, the detector trained on both TTS and vocoded speech is trained for 30 epochs, and the detector trained on vocoded speech is trained for 20 epochs.


\begin{table}[ht!]
    \caption{\textbf{TTS distribution-shift} Detectors are trained using generated samples from given TTS systems with transcripts from LJSpeech dataset. Poor generalization when the detection model is trained using TTS generated samples. Slight performance improvement when synthetic samples generated with HFG are used in training.}
    \label{tab:d_tts}
    \vskip 0.15in
\begin{center}

\scriptsize
\begin{sc}
    \begin{tabular}{lcccc}
    \toprule
    Training data & Grad-TTS & VITS & Glow-TTS & Vocoded \\ 
    \midrule
      Grad-TTS & 0.45& 38.55 & 1.90 & 38.32\\ 
        \hline
        VITS-TTS &43.81 &0.15  & 0.0 & 46.71 \\ 
        \hline
         Glow-TTS &44.58 & 31.60 & 0.0 & 45.34 \\ 
        \hline
      Vocoded-HFG  &28.39 & 24.50&0.0 & 0.0\\ 
      \hline
      TTS+Vocoded &0.30& 0.30 & 0.0 & 0.38 \\ 
         \bottomrule 
    \end{tabular}
      \end{sc}
\end{center}
\vskip -0.1in
\end{table}

Based on the results from \autoref{tab:d_tts}, it can be observed that training on synthetic speech generated using one end-to-end TTS system generalizes poorly to other end-to-end methods of generation as well as vocoded speech. For example, a model trained on synthetic audio samples from Grad-TTS achieved an EER of 38.55 \% on utterances generated from VITS dataset and 38.32 \% EER on vocoded speech from HFG. Similar can be observed for models trained on VITS and Glow-TTS. On the other hand training on just the vocoded speech led to a slight improvement in generalization with EER of 28.39 \% and 24.50 \% on Grad-TTS and VITS test datasets respectively. However, regardless of the generation system used to generate fake audio samples, Glow-TTS achieved an almost perfect detection score in all cases. 

We also compare the above results with one of the best models trained with fake audio samples generated from more than one vocoder as reported in \autoref{tab:leave-one-out} ($\textbf{D}_{Leave-WaveGlow}$). EER dropped from 28.39 \% using just HFG vocoded speech to 23.43 \% using a detector trained with multi-source vocoded speech. Similarly, for VITS test data EER dropped to 20.45 \%. 
Training on both the TTS and Vocoded speech results in the best overall generalization results. This highlights the effect of distribution-shift with respect to different TTS systems during test time.

\paragraph{Discussion:} Poor generalization to unknown generation system reflects yet another test-time shifts to which detection models are vulnerable. This can also be regarded as one of most the common distribution-shift possible during test time. While the performance gain when the model is trained with fake and real samples generated using all the systems is significant, this is not an ideal real-world scenario. It can also be noted that performance gain using fake audio samples generated using vocoders is significant as compared to training using TTS generated samples.


\subsection{Language distribution-shift in TTS systems}\label{sec:exp-10}
\begin{table}[ht!]
    \caption{\textbf{Language distribution-shift:} Models trained on utterances in Chinese and English, generated using XTTS system. The test set only includes samples generated using XTTS system with corresponding real audio used as reference. The near perfect score was obtained using both detectors.}
    \label{tab:lang_tts_1}
    \vskip 0.15in
\begin{center}
\begin{small}
\scriptsize
\begin{sc}
    \begin{tabular}{lccc}
      \toprule
    Train set & AISH-1(zh) & JSUT (ja) \\
  \hline 
  AISH-1 (zh) & 0.0 & 0.0\\ 
  \hline
  VCTK (en) & 0.04 & 0.06  \\ 
 \bottomrule 
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
Detection systems are largely trained on English data. However, in real-world scenarios, many different languages are possible during test time. In this section, we systematically study this train-test mismatch by two experiments. For the following experiments, we use XTTS \footnote{\url{https://github.com/coqui-ai/TTS/tree/dev}} multi-lingual pre-trained model for generation. First, we train a detection model with generated utterances using the Chinese language dataset AISHELL as source. During test time we evaluate on English language dataset -- VCTK. The same experiment is repeated by training on VCTK corpus and evaluating on AISHELL dataset. This setup allows us to study cross-language detection performance. Since we only want to account for distribution-shift due to language, we generate utterances with real speakers as reference thus carefully creating the evaluation set. Models are trained with a learning rate of 1e-6, batch size of 128 and weight decay of 0.0001.

It can be observed from \autoref{tab:lang_tts_1} that the model trained on the Chinese dataset generalized well to the English test set, while a model trained on the English dataset also achieves near perfect detection performance on both the English and Chinese datasets. We further extend the evaluation dataset to the Japanese language dataset -- JSUT. While both models can be seen to have a good detection performance, the model trained on the Chinese dataset achieved perfect detection performance on this dataset. 

We further expand the scope of this experiment by utilizing test data for more languages derived from MLAAD \cite{muller2024mlaad} dataset and corresponding real audio samples sourced from M-AILABS \cite{mai} dataset. Surprisingly, the overall generalization of the model trained on Chinese utterances is better than that of the model trained with English. While the overall trend is mostly similar. For example, for both the models, Polish and German are easier to detect. Russian is seemingly harder to detect with EER of 11.50\% using model trained on AISHELL and 10.40\% using a model trained on VCTK corpus. 
\paragraph{Discussion} While we were able to train a detector using utterances solely from the Chinese corpus, data for other languages are not as readily available for training. We observed better generalization in new languages when the model is trained with Chinese language. This indicates that for a better generalization performance on test-time languages training on data other than English should be explored. 
Although, switching the training data from English to Chinese helped reduce the EER \%, the overall trend of detection performance remained the same. For example, some languages like Russian are harder to detect while Polish and Italian are easier to detect. Additional related experiments can be found in \autoref{sec:tts-shift}

\begin{table*}[H]
    \centering
    \scriptsize
    \caption{\textbf{Language distribution-shift:} Models trained on utterances in Chinese and English, generated using XTTS system.}
    \label{tab:lang_tts}
    \begin{tabular}{lcc}
      \toprule
    Train set $\downarrow$ Test set $\rightarrow$ & AISHELL & VCTK \\
  \hline 
  AISHELL (zh) & 0.0 & 0.18\\ 
  \hline
  VCTK (en) & 0.04 & 0.02\\ 
 \bottomrule 
    \end{tabular}
\end{table*}


\begin{table}[t!]
    \caption{\textbf{Language distribution-shift:} Models trained on audio samples in Chinese and English, generated using XTTS system. Real audio samples are derived from MLADD dataset and corresponding fake samples are generated using XTTS system. Difficulty in generalization on Russian and Spanish language across both detectors. }
    \label{tab:lang_tts}
    \vskip 0.15in
\scriptsize
\begin{center}
\begin{sc}
    \begin{tabular}{llllllll}
      \toprule
      Train set $\downarrow$ & \multicolumn{6}{c}{Test set} \\ 
\cmidrule{2-7}
         & pl & it & fr & de & es & ru \\ \hline
  AISH-1 (zh) &0.20 & 2.90 & 4.80& 5.40& 4.90& 11.50\\ 
  \hline
  VCTK (en) &1.20 & 6.70 & 6.60 & 5.40& 7.90 & 10.40\\ 
 \bottomrule 
    \end{tabular}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}
