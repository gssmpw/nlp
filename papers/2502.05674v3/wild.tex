\begin{table}[htb!]
    \caption{Comparison of EER (\%) on In-the-Wild benchmark. The proposed models based on SSL-AASIST outperform previous results by careful curation of the training data.}
    \label{tab:in_the_wild}
    \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{lc}
    \toprule
Detector & EER (\%) \\ 
    \midrule
    \cite{müller2024doesaudiodeepfakedetection}  & 37.81\\
    \cite{wang2023detectioncrossdatasetfakeaudio} & 36.84 \\ 
RawNet2~\cite{yi2023audiodeepfakedetectionsurvey} & 36.74 \\ 
   Res2Net~\cite{yi2023audiodeepfakedetectionsurvey} & 36.62 \\ 
     AASIST~\cite{yi2023audiodeepfakedetectionsurvey} &34.81 \\ 
   \cite{yang2024robustaudiodeepfakedetection} & 24.27 \\ 
    \cite{kulkarni24_asvspoof} & 18.84 \\ 
    \cite{lu2023oneclassknowledgedistillationspoofing} & 7.68 \\ 
    \cite{wang2023spoofedtrainingdataspeech} & 7.55 \\ 
    
\cite{pascu2024generalisablecalibratedsyntheticspeech} & 7.20 \\ 

\cite{wang2023largescalevocodedspoofeddata}  & 6.10\\ 
\midrule
  Ensemble (HFG-vocoded, 4-Spk, No Aug) & \textbf{5.70}\\
     \bottomrule 
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Recently, the In-The-Wild \cite{müller2024doesaudiodeepfakedetection} (ITW) benchmark has been used to measure out-of-domain generalization to spoofing attacks for which the speech synthesis or voice conversion methods are unknown. The data consist of authentic and synthetic speech sourced from online and publicly available resources, including deepfakes of public figures, politicians and celebrities.

Prior synthetic speech detectors rely on multi-domain training in conjunction with SSL models pre-trained on large amounts of real data to better generalize to the unknown synthesis methods in the ITW corpus. For instance, \cite{müller2024doesaudiodeepfakedetection,lu2023oneclassknowledgedistillationspoofing,pascu2024generalisablecalibratedsyntheticspeech} trained their detection systems on ASVspoof, while \citet{wang2023largescalevocodedspoofeddata} use large scale vocoded speech for training. 

Furthermore, in addition to training on ASVspoof \citet{wang2023detectioncrossdatasetfakeaudio, yi2023audiodeepfakedetectionsurvey,yang2024robustaudiodeepfakedetection,kulkarni24_asvspoof,lu2023oneclassknowledgedistillationspoofing,pascu2024generalisablecalibratedsyntheticspeech} also use Wav2vec 2.0, XLS-R, WavLM, Hubert or a fusion of these front-ends. Performance of these detection systems on the ITW dataset has gradually improved, (see \autoref{tab:in_the_wild}), often by training on increasingly large amounts of data, or making better use of SSL front-ends.

Surprisingly, we observe (Table \ref{tab:in_the_wild}) that an ensemble of synthetic speech detectors trained merely on vocoded speech generated from a single HiFiGAN (HFG) vocoder achieved an EER of 5.70\%, without using any data augmentations. We train the detection systems on four speakers based on results in \autoref{sec:exp-5} where we observed diminishing returns beyond four speakers in training, and to avoid making an arbitrary choice of speakers, we ensemble five detectors trained independently with different sets of four speakers. Each selected speaker contributes an equal number of audio samples during training. Notably, these results outperform the prior systems trained with comparatively large more amount of data. The zero-shot detector we train is SSL-AASIST\footnote{\url{https://github.com/TakHemlata/SSL_Anti-spoofing}} proposed in \citet{tak2022automaticspeakerverificationspoofing}. It processes raw waveforms using wav2vec 2.0\footnote{\url{https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec}} XLSR \cite{babu2021xlsrselfsupervisedcrosslingualspeech} as front-end and spectro-temporal graph attention network as backend \cite{jung2021aasistaudioantispoofingusing}. Pre-trained wav2vec 2.0 XLSR is trained with data covering 128 languages and 437k hours of speech. HFG generated audio samples are derived from WaveFake \cite{frank2021wavefakedatasetfacilitate} dataset with corresponding real samples from LJSpeech \cite{ljspeech17}. Augmentation used to train SSL-AASIST is SSL additive noise as part of RawBoost \cite{tak2022rawboostrawdataboosting}. 



In the remainder of the paper, we explain how we arrived at this result via a systematic analysis of the detection systems' performance under controlled distribution shifts and varying training conditions. 
