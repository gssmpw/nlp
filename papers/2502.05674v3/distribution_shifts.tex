Speech vocoders are being developed at a rapid pace,  producing increasingly convincing speech. Generally, vocoders are used in conjunction with synthesis methods, including TTS, playing a key role in synthetic speech generation. For the initial set of experiments, we hypothesize that focusing on vocoded speech will facilitate the development of an improved synthetic speech detector. We also conduct experiments with end-to-end TTS systems and controlled distribution shifts (\autoref{sec:tts_shifts_section}). 

\begin{table*}[htb]
    \caption{Distribution-shifts for vocoders considered in this paper. \texttt{ShiftySpeech} includes generated samples corresponding to each listed distribution-shifts for all test-time vocoders (\autoref{tab:train_test_vocoders}). For CommonVoice dataset following vocoders are excluded -- WaveGrad, APNet2 and iSTFTNet (see \autoref{sec:age_and_accent} for age, accent and gender related experiments).}
    \label{tab:shifty_speech_vocoders}
    \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{lcr}
    \toprule
Distribution-shift  & Source & Hrs\\ 
    \midrule
    Language (ja) & JSUT & 6.7h\\ 
    \hline
     Language (en) & AISHELL & 10h\\ 
    \hline 
    Celeb / Speaking-style & VoxCeleb & 11.19h\\ 
    \hline 
    Reading-style & LibriSpeech & 21.23h\\
    \hline
    Emotive Speech; Spontaneous & MSP-Podcast & 31.71h \\ 
    \hline 
    Background noise; Near and far field; Spontaneous  & GigaSpeech-YouTube & 24.15h \\
    \hline 
    Age;Accent;Gender &CommonVoice &246h \\
    \hline 
    \textbf{Total hours} & & \textbf{350.98h} \\ 
      \bottomrule 
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[htb]
    \caption{Distribution-shifts for TTS systems considered in this paper. See \autoref{sec:tts_shifts_section} for experiment details}
    \label{tab:shifty_speech_tts}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
  \begin{tabular}{lcccc}
    \toprule
  Distribution-shift &  TTS system & Source& Hrs\\ 
    \midrule
   Language + TTS    & VITS & VCTK&2.95h& \\ 
   TTS  & & LJSpeech &25.54h & \\
      \hline
   Language + TTS   &  FastPitch & VCTK&2.69h & \\ 
      \hline
   Language + TTS   & YourTTS & VCTK & 3.47h & \\ 
      \hline
    Language + TTS &  XTTS & AISHELL (train)&167.63h & \\
      & & AISHELL (dev) &19.99h & \\ 
     &  & AISHELL (test) &10.38h & \\
      & & VCTK & 32.59h & \\
      \hline
      Language & XTTS & JSUT & 7.54h \\ 
      \hline
    TTS &  Glow-TTS &LJSpeech&25.70h & \\ 
      \hline
     TTS & Grad-TTS &LJSpeech & 22.64h& \\
      \hline 
    TTS vs Vocoder &  LJSpeech trained HFG & LJSpeech & 24.36 h& \\
      \hline
      \textbf{Total hours} & & & \textbf{345.48h}\\
         \bottomrule 
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
 
\subsection{The choice of synthetic speech at training time}\label{sec:exp-1}
In the following experiments, we train detection systems with synthetic speech generated using varying numbers of synthesis models. We aim to understand the impact of different training conditions on generalization, and in particular to identify which training conditions lead to more robust detectors. 

\begin{table*}[htb!]
    \caption{Average EER on 12 test vocoders (see \autoref{tab:train_test_vocoders}) for each distribution-shift. Models are trained on LJSpeech dataset with audio samples generated using each train vocoder. The model trained only on HFG generated speech outperforms other models on all test conditions.}
    \label{tab:indv_voc}
     \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{lccccccc}
    \toprule
     & \multicolumn{6}{c}{Test set} \\
      \cmidrule(lr){2-8}
Train set & JSUT & AISHELL &VoxCeleb & Audiobook & Podcast & YouTube & aEER\\ 
    \midrule
    PWG & 3.19 & 17.96& 18.12&18.38&21.58 &26.55 & 17.63\\
    \hline
    HFG & \textbf{1.22} &\textbf{10.67} &\textbf{8.40} &\textbf{9.08} &\textbf{8.35}&\textbf{17.17} & \textbf{9.14}\\
    \hline
    MB-Mel & 3.35 &15.57 &12.98 &16.10 &17.14&22.46 & 14.60\\
    \hline
    FB-Mel &4.89  &12.61 &15.55 &16.18 &21.27& 27.14& 16.27\\
    \hline
    Mel & 11.16 &21.36 &15.65 &16.86 &16.87& 26.09& 17.99\\
    \hline
   Mel-L & 6.97 &20.83&19.81 &18.19&17.58&30.26 & 18.94\\
   \hline
   WaveGlow &15.98 &27.36 &38.27 &34.22 &36.20& 42.23& 32.37\\
         \bottomrule 
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[htb!]
    \caption{Average EER on 12 test vocoders for each distribution-shift. Models are trained with audio samples generated using multiple vocoders excluding one of the vocoders with LJSpeech as the source dataset.}
    \label{tab:leave-one-out}
     \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{lccccccc}
    \toprule
      & \multicolumn{6}{c}{Test set} \\
      \cmidrule(lr){2-8}
Train set & JSUT & AISHELL-1  &VoxCeleb & Audiobook & Podcast & YouTube & aEER\\ 
    \midrule
       Leave HFG &2.18 & 19.73&12.46 &14.28 &14.96 &25.15 & 14.79\\ 
       \hline
       Leave PWG &2.06 & 18.05&13.17&12.35 &14.54&26.75 & 14.48\\ 
       \hline
       Leave Mel &2.07&16.82 &13.83 &12.49 &14.99&26.09 & 14.38\\ 
       \hline
       Leave Mel-L &1.95 &21.08 &13.50 &12.42 &14.44 & 24.50 & 14.64\\ 
       \hline
       Leave MB-Mel &1.93 &15.58 &11.89 &12.53 &13.90& 25.61 & 13.57\\ 
       \hline
        Leave FB-Mel &2.12 &15.51 &12.78 &12.14 &13.75 & 25.22 & 13.58 \\ 
        \hline
         Leave WaveGlow &1.95 &17.84 &12.13&11.47&13.39 &25.92 & 
        13.78\\ 
         \bottomrule 
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

We train seven separate detectors each one trained on data generated from exactly one vocoder used in training from Table \ref{tab:train_test_vocoders}. The training data of vocoders is derived from the WaveFake \cite{frank2021wavefakedatasetfacilitate} dataset with LJSpeech as the source for real audio samples (see \autoref{sec:bes_vocoders} for more details). Evaluations are then conducted on \methodName \ (\autoref{tab:shifty_speech_vocoders}).

We observe that for each of the distribution shifts, training on speech vocoded using the HiFiGAN (HFG) vocoder seems to be most effective, achieving the lowest average EER (aEER) of 9.14\%. However, training on WaveGlow alone leads to the highest aEER of 32.37\%. Results are summarized in \autoref{tab:indv_voc}. This indicates that training on HFG is enough to train a detector with comparable generalization performance in each of the distribution shifts. 

\paragraph{Leave-one-out} We further investigate if training on vocoded speech derived from multiple systems results in improved generalization. Detectors are trained by excluding one of the vocoders from the set of training vocoders. More details can be found in \autoref{sec:bes_vocoders}. Results from \autoref{tab:leave-one-out} indicate that holding out HFG and MelGAN-L degrades the performance on most of the distribution-shifts.  Further, based on the leave-one-out experiment, holding out HFG leads to the highest aEER of 14.79\%. While holding out MB-Mel, FB-Mel and WaveGlow improves generalization (aEER of 13.78\%). \autoref{tab:indv_voc} and \autoref{tab:leave-one-out} also highlight the relative difficulty of individual distribution-shifts. The EER range observed for the JSUT dataset is between 1.22\% and 15.98\%, while for the the Youtube dataset it is 17.17\% - 42.23\%. A similar trend was observed for the leave-one-out experiment.
\paragraph{Discussion} Certain distribution shifts pose greater challenges for generalization than others. For example, when evaluating data with complex background conditions and YouTube-style content, our best-performing model (HFG) achieves an EER of 17.17\%, while the worst-performing model (WaveGlow) demonstrates an EER of 42.23\%. While incorporating additional data during training generally enhances performance, the minimum EER achieved when training with audio samples from multiple systems remains at 24.50 \%, 25.15 \% and 25.22\% for this specific distribution shift. Notably, including more vocoders does not necessarily improve the modelâ€™s performance on distribution shifts. In fact, despite HFG being included in most leave-one-out experiments, the models trained with multiple vocoders underperform compared to the model trained solely on HFG data. This highlights the critical importance of carefully selecting training data, rather than indiscriminately utilizing all available data.  Similar trends were observed across other distribution shifts. Lastly, the above results provide further evidence supporting the findings from \autoref{sec:itw}.
\subsection{Data augmentations} \label{sec:augmentation}
Data augmentation techniques like RawBoost \cite{tak2022rawboostrawdataboosting} applied in \cite{tak2022automaticspeakerverificationspoofing} have proven helpful in improving detection performance on ASVspoof Logical Access (LA) dataset and In-The-Wild dataset as observed in \autoref{sec:itw}. Here, we investigate if these waveform-level augmentations help improve detection performance under specific distribution shifts. Scores are reported in \autoref{tab:exp_aug}. We train the detection system with generated utterances from HFG vocoder with and without augmentations. One of the models is trained using a combination of augmentations related to transmission and microphone: (I) Linear and non-linear convolutive noise and (II) Impulsive signal-dependent additive noise. Another model was trained using data augmentation related to compression: (III) SSL additive noise. This augmentation has proved helpful especially in the detection of deepfakes as noted by \cite{tak2022automaticspeakerverificationspoofing}. More details can be found in \autoref{sec:appendix_data_aug}.

We note that there was no overall gain for all the distribution-shifts when augmentations I + II were applied. Whereas, utilizing augmentation III alone leads to a reduction in EER from 1.22\% to 0.78\% in the case of the JSUT dataset and from 9.08\% to 8.44\% in the case of the Audiobook dataset. However, no performance gain was observed for Podcast, YouTube and VoxCeleb dataset. This inconsistent behavior is surprising, especially for Podcast and YouTube datasets with data characteristics matching with the augmentations. For instance, audio compression effects are likely to be present in streaming YouTube audio. In addition, various microphone related artifacts can possibly be present in Podcast style data. Despite this, we observe an increase in EER\% when the data augmentations are applied.

\begin{table*}[htb!]
    \caption{\textbf{Data Augmentation:} Detectors are trained on utterances generated using HFG vocoder. Augmentations applied are -- (I) Linear and non-linear convolutive noise, (II) Impulsive signal dependent additive noise, (III) SSL additive noise}
    \label{tab:exp_aug}
    \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{lcccccc}
      \toprule
      Augmentation & \multicolumn{6}{c}{Test set }\\ 
      \cmidrule{2-7} & JSUT & AISHELL-1 & Audiobook & Podcast & YouTube & VoxCeleb \\
      \hline
      None & 1.22 & 10.67 & 9.08 & 8.35 & 17.17 & 8.40 \\ 
      \hline
       I + II & 1.36 & 9.64 & 9.50 &  12.00 & 18.15 & 9.26  \\
      \hline
      III  & 0.78 & 15.74 & 8.44 & 8.97 & 18.70 & 10.09  \\
      \bottomrule 
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\paragraph{Discussion} The above results suggest that data augmentation does not benefit every distribution-shift equally. These results are in contrast with standard practices of data augmentations aimed at improving performance. This highlights distribution-shifts as even a greater concern if common practices for improvement are not globally effective.

\subsection{Impact of training on more speakers} \label{sec:exp-5} 
In general, exposing a detection system to a variety of data during training is expected to help generalization. For example, including speakers with various speaking styles, pitch, timbre, and so on, may help generalize well to unseen speakers. Even though it is possible to access multiple-speakers during training, the number of audio samples per speaker are usually limited. In the following experiments, we explore the extent to which increasing the number of speakers in training helps generalization in this setting. 

In order to understand the importance of including different numbers of speakers in training, we increase the number of speakers from 1 to 10 in training while keeping the total number of training samples the same. For each setting, we repeat the experiment 5 times selecting different sets of speakers at random to account for possible variance due to speaker properties. For instance, when training a model with 5 speakers, we randomly select 5 speakers for each of the 5 experiments and train 5 different detection systems. Each of the 5 experiments builds on the previous one, with the 5-speaker training set expanding on the 4-speaker set by adding one new speaker. This overlap ensures that performance gains are solely due to the newly added speakers. (see \autoref{sec:more_spks} for more details) 

Models are trained on the \texttt{train-clean-360} subset of LibriTTS and synthetic speech is generated using HFG vocoder. We observed that for each of the distribution shifts, EER\% decreased as number of speakers in training are increased from 1 to 4 (\autoref{fig:hfg_in-domain}). While performance on certain datasets like AISHELL exhibits inconsistent behavior, the aEER obtained on this dataset with 4 speakers in training is comparable to including ten speakers in training (see~\autoref{sec:more_spks} for more details). These experimental results suggest that a dataset with four speakers may be sufficient for training, and increasing the number of speakers beyond this threshold might not provide substantial benefits to the modelâ€™s performance. 

For JSUT dataset, aEER of model trained with one speaker is 3.52\% and it further decreases to 1.34\% with 4 speakers in training. While training with all 10 speakers results in slight increase in aEER to 1.36\%. Although training on 8 speakers led to a reduction in aEER of 11\% in case of audiobook dataset; 35\% in case of JSUT dataset; 16\% in case of Podcast dataset. Only 3\% and 7\% reduction was observed for YouTube and VoxCeleb dataset respectively. Moreover, we observed similar results when evaluation was done on an out-of-domain vocoder PWG (see \autoref{fig:hfg_out-of-domain}). Detailed scores can be found in appendix \autoref{sec:more_spks}.

\begin{figure}[t]
\vskip 0.2in 
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Images/hfg_train_hfg_test_spks.pdf}}
    \caption{Average EERs reported with models trained on increasing number of speakers with HFG generated audio samples (LibriTTS, train-clean-360). For each dataset average EER on all 12 test vocoders are plotted. Test samples are also generated using HFG vocoder. aEER drops significantly when the number of speakers is increased to four. No significant performance gain was observed thereafter, for most cases.}
     \label{fig:hfg_in-domain}
\end{center}
\vskip -0.2in 
\end{figure}
\paragraph{Discussion} These results suggest that there are quickly diminishing returns from including more than 4 speakers in training. This is a positive finding, since it suggests that fewer resources need to be spent identifying a large number of speakers. 

\subsection{Vocoder detection difficulty}\label{sec:exp-2} We further extend the previous experiment discussed in \autoref{sec:exp-1} to identify vocoders particularly degrading overall generalization performance.
For each of the 7 detectors trained on synthetic speech generated using train-time vocoders in \autoref{tab:shifty_speech_vocoders}, we note the average EER on each of the distribution-shifts and report the scores in \autoref{tab:harder_detect}. Scores on a particular detection system can be found in \autoref{sec:bes_vocoders}.

\paragraph{Discussion} Detection performance on test vocoders varies significantly, with aEER values ranging from 7\% to 34\%. For example, PWG is relatively easier to detect, as all trained models achieve the lowest aEER on the PWG test set, with an average aEER of 7.05\%. On the other hand, BigVGAN is more challenging to detect, with higher EER values across all distribution-shifts and aEER of 34.64\%. Additionally, other vocoders show varying detection difficulty, resulting in intermediate aEER values. Overall, in addition to HFG, vocoders released in later years (iSTFTNet, APNet2, Vocos and BigVGAN) achieved high aEER in the range of 20\% -- 34\%. This depicts the generation quality being improved over the years. In addition to the findings in \autoref{sec:exp-1}, while using HFG vocoded samples in training helps improve generalization, detecting the HFG vocoder during test-time poses a significant challenge. 

\begin{table*}[htb!]
    \caption{For a given test vocoder EER is averaged over scores from six train vocoders. See \autoref{tab:train_test_vocoders} for the list of train vocoders. See \autoref{sec:bes_vocoders} for a detailed score obtained using each train vocoder. Detection performance is worse on BigVGAN and best on PWG test vocoder. }
    \label{tab:harder_detect}
    \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{lccccccc}
    \toprule
    & \multicolumn{7}{c}{Source}\\
    \cmidrule(lr){2-8}
Test Vocoder   & JSUT & AISHELL-1  &VoxCeleb & Audiobook & Podcast & YouTube & aEER\\ 
    \midrule
    \rowcolor{green!45}
   PWG & 0.02 & 5.09 & 6.19 & 4.80 & 7.39 & 18.85 &7.05
   \\
   \hline
    MB-Mel & 0.30 & 8.79 & 7.34 & 6.65 & 9.57 & 19.91 & 8.76\\ 
   \hline
    Style-Mel & 1.02 & 11.09 & 11.14 & 11.06 & 13.15 & 21.38& 11.47 \\ 
   \hline
   WaveGrad  & 1.07 & 11.81 & 7.34 & 11.26 & 14.32 & 23.31 & 11.51\\ 
   \hline
    Univ-1 & 3.88 & 8.79 & 15.67 & 15.63 & 17.23 & 23.93 &14.18\\ 
   \hline
   Univ-2 & 7.09 & 17.37 & 18.97 & 19.74 & 20.99 & 26.50 & 18.44\\ 
   \hline 
    BigVSAN & 4.18 & 21.35 & 18.83 & 20.08 & 21.39 & 30.65 &19.41\\ 
   \hline
   HFG & 10.20 & 19.03 & 20.83 & 22.07 & 24.20 & 26.68& 20.50 \\ 
   \hline
    iSTFTNet & 10.11 & 20.43 & 20.46 & 22.52 & 23.23 & 29.79 & 21.09\\ 
    \hline
    APNet2 & 10.01 & 25.08 & 24.79 & 26.82 & 26.56 & 33.01 &24.37 \\ 
   \hline
   Vocos &  5.96 & 29.45 & 27.09 & 23.03 & 23.93 & 35.03 &24.08\\ 
   \hline
   \rowcolor{red!45}
   BigVGAN & 26.33 & 33.08 & 34.62 & 37.50 & 36.38 & 39.94 &34.64\\ 
   \bottomrule
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Exploring generalization on newly released vocoders}\label{sec:exp-3}

Newly released vocoder systems with new architectures, features, and loss functions (\autoref{sec:appendix-train-details}), can be expected to generate waveforms of better quality. Given this improved generation quality, it is quite possible that a detection system trained on older sets of generation methods is unable to distinguish speech synthesized using new methods from real speech. One possible solution to improve generalization in this scenario is to continuously update the training data. Can we improve detection performance by including new vocoders in the training data as they are released? 

\begin{table}[ht]
    \caption{Models trained on samples generated using vocoders released in the year -- 2018 to 2021. Average EER is reported on samples generated using set of vocoders released in later years from 2022 to 2024 with JSUT dataset as source. Vocoders considered for each year can be found in \autoref{tab:voc_years}. aEER\% dropped significantly as new generation vocoders are added in training. The EER reduction is slow for vocoders released in year 2022. }
    \label{tab:gen_voc}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{lcccccc}
    \toprule
Train set  & 2022 aEER & 2023 aEER & 2024 aEER \\ 
    \midrule
    2018 & 18.42 & 7.21 & 5.20 \\ 
    \hline
    2018 -- 2019 & 12.38 & 2.83 & 1.46 \\ 
    \hline
    2018 -- 2020 & 10.69 & 1.74 & 1.12 \\
    \hline
    2018 -- 2021 & 8.41 & 1.35 & 1.08 \\ 
    \hline
    HFG (best) & 5.42 & 1.20 & 0.72 \\ 
         \bottomrule 
    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


From our existing set of vocoders, we include vocoders released in 2018 in training and study the performance on vocoders released in the following years.\footnote{We use the publication year of the corresponding paper for these experiments, noting that this imperfectly captures the actual development time frame of the corresponding systems.} Similarly, we gradually include vocoders released in later years as part of training. Four detection systems are thus trained by including vocoders from 2018 -- 2021 sequentially. The test set includes out-of-domain vocoders released between 2022 to 2024. Detection performance on in-domain vocoders is additionally reported in \autoref{tab:detail_scores_gen_voc}.  Information about vocoders and year of release can be found in appendix \autoref{tab:voc_years}.

The training data is derived from the WaveFake dataset. For vocoders not included in WaveFake, we generate fake audio samples using the respective vocoder with LJSpeech as the source. Further training details can be found in \autoref{sec:appendix-new-gen}. For evaluation, JSUT is used as the source dataset. Scores are reported in Table \ref{tab:gen_voc}. Detailed scores can be found in appendix Table \ref{tab:detail_scores_gen_voc}. 

Although there is continuous improvement in generalization performance, it can be noted that vocoders from 2022 pose a challenge to detection performance with low reduction rates as compared to the 2023 evaluation set. In particular, this difficulty in generalization can be attributed to BigVGAN vocoder in evaluation set of 2022, which was previously found to be hard to detect (\autoref{sec:exp-2}).
APNet2 and Vocos from 2023 evaluation set also achieved higher aEER on all distribution-shifts (see \autoref{tab:harder_detect}). However, this set achieved an overall better generalization performance with aEER of 1.35\%. This suggests that newer vocoders may still share similar artifacts with older ones, enabling the models to generalize effectively to them. Similar trends were observed for 2024 evaluation set; however, this test consists of only one vocoder and hence it might not be a good indicator of systems released in 2024. 

We also include the results from a model trained solely on utterances generated by HFG vocoder. We note that training only on HFG vocoder yields comparable performance to a model trained on a large set of vocoders. These results are consistent with observations in \autoref{sec:exp-1}.
