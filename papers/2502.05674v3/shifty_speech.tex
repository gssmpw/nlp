This section details the creation of evaluation data for benchmarking distribution shifts. We first introduce the dataset used for training the detection system, followed by a brief description of the generation systems used to create synthetic speech for evaluation and the source datasets that introduce variations in language, speaker characteristics and background complexity.

\paragraph{Training data} Similar to \autoref{sec:itw}, the WaveFake dataset is used as training data for the following experiments. It is a single speaker dataset with six GAN-based vocoders and LJSpeech as the source. Vocoders that appear in training are listed in \autoref{tab:train_test_vocoders} 

\paragraph{Evaluation data}  
We introduce a new dataset, \methodName, which includes 7 major distribution shifts for evaluation: it includes vocoded speech with test-time shifts due to different conditions, including:
\begin{itemize}
    \item Language variations: JSUT (Japanese) \cite{sonobe2017jsutcorpusfreelargescale} and AISHELL-1 (Chinese)\cite{bu2017aishell1opensourcemandarinspeech}
    \item Speaker diversity: VoxCeleb2 \cite{Chung18b}
    \item Reading style: Audiobook \cite{7178964}
    \item Emotive speech: MSP-Podcast \cite{Lotfian_2019_3}
    \item Noisy or real world environment conditions: Youtube \cite{chen2021gigaspeechevolvingmultidomainasr}
    \item Age/gender related shifts: CommonVoice \cite{ardila2020commonvoicemassivelymultilingualspeech}
\end{itemize}

In addition, we also include synthetic data generated using several end-to-end TTS systems \cite{kim2020glowttsgenerativeflowtexttospeech,kim2021conditionalvariationalautoencoderadversarial,popov2021gradttsdiffusionprobabilisticmodel,casanova2024xttsmassivelymultilingualzeroshot}  using LJSpeech \cite{ljspeech17} and VCTK transcripts, which are discussed further in~\autoref{sec:tts_shifts_section}.\footnote{The front-end feature extractor Wav2Vec 2.0 XLSR was pretrained on a multilingual corpus, which does not include the test data used in this study. Apart from the CommonVoice dataset detailed in \autoref{sec:age_and_accent}, the pretraining data does not overlap with any of the evaluation datasets.}



In order to generate synthetic speech using above mentioned source datasets we utilize several GAN based vocoders: MelGAN (Mel) \cite{kumar2019melgangenerativeadversarialnetworks}, Parallel-WaveGAN (PWG) \cite{yamamoto2020parallelwaveganfastwaveform}, HiFiGAN (HFG) \cite{kong2020hifigangenerativeadversarialnetworks},  Multi-band MelGAN (MB-Mel) \cite{yang2020multibandmelganfasterwaveform}, Style-MelGAN (Style-Mel) \cite{mustafa2021stylemelganefficienthighfidelityadversarial}, UniVNET \cite{jang2021univnetneuralvocodermultiresolution}, BigVGAN \cite{lee2023bigvganuniversalneuralvocoder}, BigVSAN \cite{takida2024saninducingmetrizabilitygan}, iSTFTNet \cite{kaneko2022istftnetfastlightweightmelspectrogram}, VOCOS \cite{siuzdak2024vocosclosinggaptimedomain} and APNet2 \cite{du2023apnet2highqualityhighefficiencyneural}.  Apart from that we also explore a flow-based vocoder, WaveGlow \cite{prenger2018waveglowflowbasedgenerativenetwork} and diffusion-based vocoder WaveGrad \cite{chen2020wavegradestimatinggradientswaveform}.

For each of the distribution shifts we re-synthesize real data using the 12 test vocoders described in \autoref{tab:train_test_vocoders}. All of the vocoders are trained on LibriTTS dataset. Training details can be found in \autoref{tab:train_details_voc} and \autoref{tab:train_details_tts}.
\begin{table}[htbp!]
\caption{Train and test time vocoders. Note that out of 16 vocoders, 3 vocoders are used both in training and as part of the evaluation set to report the average EER.}
    \label{tab:train_test_vocoders}
    \scriptsize
    \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
 \begin{tabular}{lcc}
    \toprule
Vocoders & Train-time & Test-time\\ 
    \midrule
PWG & $\surd$  & $\surd$  \\
\hline 
HFG & $\surd$  & $\surd$ \\ 
\hline 
Mel & $\surd$  & $\times$ \\ 
\hline 
Mel-L & $\surd$  & $\times$ \\ 
\hline 
MB-Mel & $\surd$  & $\surd$  \\  
\hline 
FB-Mel & $\surd$ & $\times$ \\ 
\hline 
WaveGlow & $\surd$  & $\times$ \\ 
\hline 
Style-Mel &$\times$ & $\surd$ \\ 
\hline
WaveGrad &$\times$ &$\surd$ \\ 
\hline 
UniVNET v1 and v2 &$\times$ &$\surd$  \\ 
\hline 
BigVGAN & $\times$ & $\surd$  \\ 
\hline 
BigVSAN &$\times$ & $\surd$  \\ 
\hline 
iSTFTNet &$\times$ &$\surd$  \\ 
\hline 
APNet2 & $\times$ &$\surd$  \\ 
\hline
Vocos & $\times$ &$\surd$  \\ 
     \bottomrule 
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


Speech generation methods are evaluated using various quality metrics like Mean Opinion Score (MOS). However, employing metrics like MOS to determine quality is cost intensive and time consuming. In this work, we use automatic speech quality metric, UTMOS \cite{saeki2022utmosutokyosarulabvoicemoschallenge} submitted as a part of VoiceMOS challenge 2022 \cite{huang2022voicemoschallenge2022} to determine the quality of our dataset. (see \autoref{tab:utmos-test-vocoders} for more details).

\paragraph{Evaluation metric}
In order to evaluate performance on different systems we use the standard Equal Error Rate (EER) metric. EER is defined as the value where the false acceptance rate (FAR) equals the false rejection rate (FRR). Lower EER represents better detection performance.

\paragraph{Model selection} 
During training, each model is validated on a held-out set that remains strictly separate from test set conditions. Specifically, model selection is performed on held out audio samples from the same dataset as the training vocoders. For the multi-speaker training, validation is conducted on held-out samples from the training speakers and vocoders. Similarly, for multi-vocoder training, held-out audio samples from training vocoders are used.
Models are evaluated under diverse conditions without performing any condition-specific hyperparameter tuning of the detectors (\autoref{sec:experiments}). The model evaluated on the ITW dataset in~\autoref{sec:itw} is selected  based on performance across various distribution shifts.
