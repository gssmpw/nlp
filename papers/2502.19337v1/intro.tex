
\section{INTRODUCTION}\label{sec:intro}

Probabilistic clustering models, also known as mixture models, play a crucial role in 
many scientific domains and are extensively used in various downstream tasks. These models aim to learn the underlying data structure by grouping similar data points into clusters, with cluster assignments encoded in  
the posterior distribution of  discrete latent variables.
%
\subimport{./figures/}{invariance_ecdf_mnist}
Given a generative model for the clusters, traditional posterior inference methods, such as Markov Chain Monte Carlo (MCMC) \citep{neal:2000:MCMC} and their fast implementations~\citep{Chang:NIPS:2013:ParallelSamplerDP, dinari:2019:distributed, dinari2022cpu}, yield samples from the posterior distribution, providing an asymptotically exact solution given enough samples. However, they often struggle with high-dimensional data or large datasets. Conversely, variational inference \citep{Blei:2004:VMD} is more suitable for large datasets; however, it incurs a trade-off in accuracy as a result of its reliance on approximations. 
%
% Deep learning methods: 

Over the last decade, with the increasing intricacy of data, a variety of clustering solutions have been proposed that utilize deep neural models. 
%
A popular family of models, reviewed in~\cite{ren2024deep} and~\cite{zhou:2022:survey1}, and often referred to as performing {\it deep clustering}, are  unsupervised classifiers trained to discover a finite number of categories. Among these models, methods such as DCN \citep{yang:2017:DCN} and ClusterGAN \citep{mukherjee:2019:clstrGAN} are designed to simultaneously tackle both data representation and clustering tasks.
%
Another approach involves a two-step process in which feature learning and clustering are decoupled, with SCAN \citep{van:2020:SCAN} and DDC \citep{ren:2020:DDC} being notable examples. 
%
Importantly, at test time, these models are limited to generating (soft) assignments for {\it individual} data points, since they are restricted by their reliance
on pre-learned categories and their inability to model point interactions. Note that this is also the case for DeepDPM~\citep{ronen:2022:DeepDPM}, despite its ability to infer the number 
of clusters present in the training (but not test) data. 

Our focus in this paper is on a different category of models, which 
address the more ambitious task of {\it jointly} modeling the 
clustering posterior for set-structured data of arbitrary size. 
%
This task is more challenging, 
as it involves not only learning the structure of individual data points but also the correlation structure among the points of a dataset of any size.
%
Motivated by this need, there has been consistent progress 
in recent years  in amortized inference methods within the framework of probabilistic clustering, accommodating a set-structured input.
%
% Amortized inference~\citep{Gershman:2014:amortized} refers to the process of training a neural network that, given observations drawn from a generative model containing latent variables, can infer their posterior distribution to produce new observations. 
%
Amortized inference~\citep{Gershman:2014:amortized} refers to the process of training a neural network that can infer the posterior distribution of latent variables based on observations drawn from a generative model.
%
In this setting, \cite{lee2019set} introduced the Set Transformer~(ST), an attention-based network specifically designed to model point interactions within a set. 
%
This architecture was used to amortize the inference 
over the {\it parameters} of a mixture model, but was restricted to  
a fixed number of Gaussian components. 
%
To avoid these limitations, other models directly amortize the posterior over the joint clustering {\it labels} of the dataset.
%
DAC~\citep{DAC} and CCP~\citep{pakman2020} build on this concept by sequentially generating full clusters, 
%without imposing a fixed number of clusters or a likelihood model, 
thus enabling more complex prior distributions.
%(see also \cite{liu:2021:CHiGAC}, \cite{liu:2022:AMCP}). 
% in which the proposed model employs two strategies to explore relationships among data points.
%
% A disadvantage of these methods is that they do not produce probabilities for the clustering configurations they generate.
%
% This drawback is addressed by an

An alternative approach, named Neural Clustering Process (NCP)~\citep{pakman2020}, sequentially assigns cluster labels to data points.
%
Unlike previous methods, NCP provides assignment probabilities, thereby offering deeper insights into clustering outputs through uncertainty quantification.
%
However, these probabilities are highly sensitive to data-order permutations, failing to preserve a fundamental symmetry of the posterior distribution.
%
The limitations of existing work, as outlined, motivate our current research.


% Our model
In this paper we propose the GFlowNet-based Clustering Process (GFNCP), a posterior generative clustering model that, given a set-structured input sampled from a (possibly infinite) mixture model, yields clustering assignment samples along with their associated probabilities.
%
% In GFNCP, we exploit the framework of GFlowNet to model the clustering problem as a generative process of forming assignment estimations, which are interpreted as final-state objects, and apply marginal consistency flaw
%
In GFNCP, we exploit the framework of GFlowNets~\citep{bengio2021flow} to model the clustering task as a sequential generative process in which the final object, a full-data assignment, is constructed by sampling from a learned policy at intermediate states. 
%
Notably, we formulate the policy and the learned rewards  as energy-based models with shared parameters in an end-to-end framework.
%
Our work differs from previous approaches in several aspects (see \autoref{tab:checklist}); in particular, unlike NCP, our model encourages invariance under data order permutations (see \autoref{fig:invariance_ecdf_mnist}).

\subimport{./tables/}{checklist_prop}


% Contributions:
Overall, \textbf{our contributions are as follows}: (1)~We present GFNCP, an amortized clustering method formulated as a GFlowNet sequential generative model, which uses an energy-based joint policy and reward function, allowing an unlimited number of components; 
(2) We demonstrate that GFNCP surpasses existing methods in clustering performance and also generalizes better to unseen classes; 
%
%(2) We demonstrate that GFNCP not only surpasses existing methods in clustering performance but also supports an unlimited number of components and generates assignment probabilities. 
%
(3)~We show that GFNCP exhibits greater consistency across different data orders; 
%
(4) Unlike previous works on cluster label amortization, we show that training can be performed without true labels via instance discrimination.\footnote{Our code is available at  
\url{https://github.com/BGU-CS-VIL/GFNCP}.
}

% Overall, \textbf{our contributions are as follows}: (1) We diagnose a consistency flaw in previous approaches to point-wise amortized clustering models. (2) We formulate amortized clustering as a sequential generative model via GFlow networks with an energy-based policy function. (3) We show that point-wise amortized clustering can be trained without known labels.

