

\section{RELATED WORK}\label{sec:related_work}

% \paragraph{Markov Chain Monte Carlo} 

% \paragraph{Variational Inference}

% \paragraph{The psychology of clustering}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph{Unsupervised classification.} 
% %
% %Different methods utilize various techniques to learn discriminative data representations.
% %
% Having the ability to approximate complex functions, various methods use different techniques to learn discriminative data representations.
% %
% In \cite{yang:2017:DCN}, auto-encoder network serves as a feature extractor, with the clustering objective integrated into the feature learning process.
% %
% Other methods, such as ClusterGAN \citep{mukherjee:2019:clusterGAN}, adopt adversarial learning for its ability to effectively capture the latent distribution of data.
% %
% In \cite{zhong:2020:DRC}, DRC adds an extra task to train a feature extractor, using contrastive learning with the goal of reducing intra-class variance while increasing inter-class variance. A similar approach is applied in JULE \citep{yang:2016:JULE}, using a triplet loss in a recurrent network. 
% %
% DDC \citep{ren:2020:DDC} and DeepDPM \citep{ronen:2022:DeepDPM} propose a deep-clustering solution, where the number of clusters is unknown. 
% %
% %In [SAC?, N2D?] feature representations and clustering objective are handled separately, aiming to reduce the reliance of clustering on low-level features.
% %
% The methods above, often termed {\it deep-clustering} solutions, assign data to discrete classes and, during inference, map each point to a pre-learned mixture model.
% %
% Our approach differs by assuming a set-structured input during both training and inference, relying on interactions between points within the set.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Amortizing discrete variables.}
Beyond clustering, models exist for posteriors over  
permutations~\citep{mena2018learning,pakman2020neural} 
and network communities~\citep{wang2024amortized}, {\it inter alia.} 
When both a generative and an inference model over discrete latents are learned, reparametrization gradients can be used via continuous relaxations~\citep{maddison2017concrete, jang2017categorical}. 
To amortize distributions 
over discrete factor graphs, \cite{buesing2020approximate} 
formulate sampling  as a MaxEnt Markov Decision Process. However, this approach fails when there are many ways to generate the same object, as in our setting.
%
Other approaches that leverage nonparametric Bayesian models within neural networks, such as those proposed in \citep{nalisnick2016stick} and \citep{jiang2016variational}, develop generative models with latent discrete labels.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Generative Flow Networks (GFlowNets).} Introduced in \citep{bengio2021flow} and 
reviewed in~\autoref{sec:gflonet_review}, this set of algorithms is designed to train a stochastic policy for sampling composite objects from a target distribution, following a sequence of actions structured as a directed acyclic graph (DAG).
GFlowNets address the challenging setting in which different trajectories in the space of actions can yield the same final state.
%
% was first introduced as reinforcement-learning algorithm aimed at learning a stochastic policy for generating an object (final state) through a sequence of actions, with the probability proportional to a specified reward function. GFlowNet aims to solve the challenging setting in which different trajectories can yield the same final state.
%
GFlowNets have connections to variational inference~\citep{malkin:2022:GFlowNets_VI} and entropy-regularized reinforcemnt learning~\citep{tiapkin2024generative, deleudiscrete}, and have been applied to problems 
such as biological and natural language sequences~\citep{jain2022biological,huamortizing} and  combinatorial optimization~\citep{zhang2023let}. 
Models similar to ours, were GFlowNets are conditioned
on data, include~\cite{deleu2022bayesian,hu2023gflownet}. 
%
%
% \cite{zhang2022generative} amortizes the MCMC algorithm by introducing a framework that jointly trains a GFlowNet with an energy function, which serves as a learned reward function, across two different models.
