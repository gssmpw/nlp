

\section{DATA GENERATION AND EXPERIMENTAL SETUP}
\label{sec:data_gen_and_experimental_setup}

Here we provide technical details about the experiments mentioned in the paper, including input-data generation and training procedures. Recall that our model consists of four functions, $h,g,u$ and $f$. We use for them architectures similar to the ones used in the NCP model~\citep{pakman2020}.

\subsection{Set-structured Input Generation}
\label{sec:data_generation}
\textbf{MNIST, Fashion-MNIST and ImageNet}. Each data point in our training and test data is a \textit{set} of images or image representations drawn from the original data set, without using the ground-truth labels. We apply self-labeling, as outlined below in step 4, for methods that require labeled data.
%
We use the training split as the data source for generating datasets during training, and the validation split for testing.

The training data is generated as follows:
\begin{itemize}
    \item[1.] A data size $N \sim (N_{\text{min}}, N_{\text{max}})$ is sampled.
    \item[2.] A clustering mixture $\bc$ is generated using the Chinese Restaurant Process (CRP), which determines the number of clusters, $K$, and the distribution of data points across each group.
    \item[3.] We then sample $K$ data points from the dataset.
    \item[4.] Instance discrimination is applied, where each cluster contains the sampled data point and a number of augmentations to the fill up each cluster size, according to the sampled mixture $\bc$. 
\end{itemize}

For these experiments, we set $(N_{\text{min}}, N_{\text{max}}) = (100, 1000)$, and choose $\alpha=1$, the hyperparameter controlling the data distribution in the CRP.

Test data is generated by setting $N=300$ for the data size, following steps 2 and 3, and then sampling original images from the dataset's categories based on the sampled mixture. 

As explained in the paper (\autoref{seq:obj_func} and \autoref{alg:training}), during training our model performs space exploration by using policies sampled from a mixture of $p_{\text{data}}(\bc|\bx)$ and 
a uniform random sample of $\bc$,
using $l \sim \text{Bernoulli}(\beta)$ to control that mixture.
%
\noindent For MNIST and FMNIST we set $\beta=0.99$, while for datasets with fewer images per category, such as in IN50/100/200, we set $\beta=0.999$.
%

\textbf{2D Mixure of Gaussians (MoG)}. We create a synthetic dataset using the following procedure for both training and testing data. Similar to the image datasets, for a fixed or sampled data size $N$, a clustering mixture is generated via the CRP, which determines the number of clusters, $K$, and the data distribution. We then sample $K$ centroids from the Normal distribution, where each centroid is generated as follows: 
    \begin{equation}
        \mu_k \sim \mathcal{N}([0]_{2\times 1}, \sigma I_{2\times 2}),
    \end{equation}
followed by 2D data-point generation based on the CRP mixture: 
    \begin{equation}
        x_i \sim \mathcal{N}(\mu_k, I_{2\times 2}),
    \end{equation}
using $\sigma = 10$.

For data size values,
we set $(N_{\text{min}}, N_{\text{max}}) = (100, 1000)$ for training and $N=300$ for testing.
Additionally, we set $\alpha=6$ for the CRP, and $\beta=0.999$ for space exploration. 

\subsection{Training Procedure}
\label{sec:training_procedure}
For both the toy-data and real-data experiments we train GFNCP with $5K$ iterations using Adam optimizer, a batch size of $64$, no weight decay and a cosine scheduler with an initial learning rate of $5\cdot 10^{-4}$ and a minimum learning rate of $1\cdot 10^{-6}$.
During training, each data point in the batch is a set-structured input, generated by following the procedure outlined in \autoref{sec:data_generation}.
%
During testing, we calculate the average NMI, ARI, and MC metrics (when applicable) across $M$ input sets drawn from the test data source
%, with each set containing $300$ data points. 
%
For MoG we set $M=3K$; for MNIST and Fashion-MNIST, $M$ is set to $10K$. For ImageNet, we set $M=2.5K$ due to the lower number of data points in each category.
%
As for the competitor's training (DAC, Set Transformer and NCP), we set the default hyper-parameters used in their published code or mentioned in their papers.

Each model was trained until convergence based to its objective function.
%
For each experiment we report the metrics' average and standard deviation over three different runs (using three different seeds), 
selecting the best model based on the lowest loss value.

