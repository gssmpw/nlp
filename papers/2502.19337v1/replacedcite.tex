\section{RELATED WORK}
\label{sec:related_work}

% \paragraph{Markov Chain Monte Carlo} 

% \paragraph{Variational Inference}

% \paragraph{The psychology of clustering}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph{Unsupervised classification.} 
% %
% %Different methods utilize various techniques to learn discriminative data representations.
% %
% Having the ability to approximate complex functions, various methods use different techniques to learn discriminative data representations.
% %
% In ____, auto-encoder network serves as a feature extractor, with the clustering objective integrated into the feature learning process.
% %
% Other methods, such as ClusterGAN ____, adopt adversarial learning for its ability to effectively capture the latent distribution of data.
% %
% In ____, DRC adds an extra task to train a feature extractor, using contrastive learning with the goal of reducing intra-class variance while increasing inter-class variance. A similar approach is applied in JULE ____, using a triplet loss in a recurrent network. 
% %
% DDC ____ and DeepDPM ____ propose a deep-clustering solution, where the number of clusters is unknown. 
% %
% %In [SAC?, N2D?] feature representations and clustering objective are handled separately, aiming to reduce the reliance of clustering on low-level features.
% %
% The methods above, often termed {\it deep-clustering} solutions, assign data to discrete classes and, during inference, map each point to a pre-learned mixture model.
% %
% Our approach differs by assuming a set-structured input during both training and inference, relying on interactions between points within the set.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Amortizing discrete variables.}
Beyond clustering, models exist for posteriors over  
permutations____ 
and network communities____, {\it inter alia.} 
When both a generative and an inference model over discrete latents are learned, reparametrization gradients can be used via continuous relaxations____. 
To amortize distributions 
over discrete factor graphs, ____ 
formulate sampling  as a MaxEnt Markov Decision Process. However, this approach fails when there are many ways to generate the same object, as in our setting.
%
Other approaches that leverage nonparametric Bayesian models within neural networks, such as those proposed in ____ and ____, develop generative models with latent discrete labels.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Generative Flow Networks (GFlowNets).} Introduced in ____ and 
reviewed in~\autoref{sec:gflonet_review}, this set of algorithms is designed to train a stochastic policy for sampling composite objects from a target distribution, following a sequence of actions structured as a directed acyclic graph (DAG).
GFlowNets address the challenging setting in which different trajectories in the space of actions can yield the same final state.
%
% was first introduced as reinforcement-learning algorithm aimed at learning a stochastic policy for generating an object (final state) through a sequence of actions, with the probability proportional to a specified reward function. GFlowNet aims to solve the challenging setting in which different trajectories can yield the same final state.
%
GFlowNets have connections to variational inference____ and entropy-regularized reinforcemnt learning____, and have been applied to problems 
such as biological and natural language sequences____ and  combinatorial optimization____. 
Models similar to ours, were GFlowNets are conditioned
on data, include____. 
%
%
% ____ amortizes the MCMC algorithm by introducing a framework that jointly trains a GFlowNet with an energy function, which serves as a learned reward function, across two different models.