

 
\section{GFLOWNET CLUSTERING}
Given $N$ data points, we formalize sampling from the clustering posterior using the GFlowNet framework as follows: the initial action uniformly samples a data order, $\rho$, and assigns the first data point to the first cluster.
Then, a sequence of $N-1$ sampled actions 
allow each successive data point to join an existing cluster or create a new one. A terminal state $z=s_N$ represents a fully clustered dataset. As illustrated in~\Cref{fig:dag}, the initial state~$s_0$ edges out into~$N!$ branches, all of which  meet again in each of the terminal states. 

 The highly symmetric nature of our setting implies 
an additional property not present in general GFlowNets. 
For each terminal state $z$,
there are $N!$ trajectories~$\tau$
which differ by the data order. 
However, conditional exchangeability (\autoref{eq:exchange}) implies that all these trajectories are of equal probability.
Thus, 
 \autoref{eq:R_GFN} becomes:
\begin{align}
R(z) \varpropto F(\tau) \qquad \forall z \forall \tau: z  \in \tau    \,.
\label{eq:R_F}
\end{align}
The reward itself, $R(z) \varpropto p(\bc|\bx)$, is learned from the data. 
This setting was studied
in~\cite{zhang2022generative}, where an energy-based model
for the reward was learned together with the sampling policy. 
% We follow a similar approach here, with a major difference;
% instead of training separate networks for the policy and the reward, we use a common parametrization for both.
We take a similar approach with one key difference: rather than training separate networks for the policy and the reward, we use a shared parametrization for both.
% Such a possibility, originally suggested in~\cite{bengio2023gflownet}, seems not to have been made concrete before. 
This idea, originally suggested in~\cite{bengio2023gflownet}, does not appear to have been concretely developed before.
%To achieve this, we depart from the typical 
%approach in GFlowNet models
%to parameterize the forward policy $P_F(-|s)$
%as a neural network that takes a representation of $s$ as input and produces the logits of a distribution over its children as outputs. 
%
To achieve this, we deviate from the standard GFlowNet approach, where the forward policy $P_F(-|s)$ is parameterized as a neural network
that takes 
% a representation of 
$s$ as input,
and outputs 
%the logits of a distribution over its children.
logits for its children's distribution.
%
Instead, we exploit the NCP energy function~(\autoref{eq:unnormalized_marginal}) 
and use the same object to parameterize
state flows, edge flows and the reward. 
%
Concretely, given $N$ data points, let us define:
\begin{align}
\hat{E}[c_{1:n}] 
= 
\begin{cases}
0 & n=1,
\\
E[c_{1:n}] 
- 
\underset{c_n'}{\min}
{E}[c_{1:n-1},c_{n}']
& {}_{2 \leq n < N},  
\\
E[c_{1:N}] & n=N,  
\end{cases}
\nonumber
\end{align}
where $E[c_{1:n}] = f(G_{c_{1:n}},U_{n+1})$ is the energy function given the labels and the data, as described in~\autoref{eq:unnormalized_marginal}. 
We omit indicating the data $x_{1:N}$ in the energy arguments to simplify the notation. 
We define a state $s_{1 \leq n < N}$ as  $(\rho, c_{1:n})$, containing both the data order $\rho$ and the $n$ initial label assignments. 
The initial transition 
$s_0 \rightarrow s_1$ is special 
because it uniformly samples a permutation~$\rho$ for the data order. 
We parameterize the initial edge flow as $F[ s_0 \rightarrow s_1 = (\rho, c_{1} =1)]
= 1$, yielding a uniform forward transition~(\autoref{eq:forward})
$P_F[ c_1=1, \rho |s_0 ] = \frac{1}{N!}$.
% \begin{align}
%   P_F[ c_1=1, \rho |s_0 ] = \frac{1}{N!} \,.
% \end{align}
For non-initial states, we model both edge and state flows using the {\it same} function (for $2 \leq n \leq N$): 
\begin{align}
F[ c_{1:n-1} \rightarrow
c_{1:n}, \rho] & = F[ c_{1:n}, \rho] = e^{-\hat{E}[c_{1:n}]}  \,.
% \\
% F[ c_{1:n}, \rho] & = e^{-\hat{E}[c_{1:n}]}  
% \quad n=2 \ldots N.
\label{eq:FFE}
\end{align}
This yields the forward transition (\autoref{eq:forward})
\begin{align}
   P_F[ c_n | c_{1:n-1}, \rho] =  
   \frac{e^{-\hat{E}[c_{1:n}]}}
   {\sum_{c_n'}  
   e^{-\hat{E}[c_{1:n-1},c_n']} }\,,
\label{eq:forward_clustering}
\end{align}
for $2 \leq n \leq N$, and the order-independent reward
\begin{align}
% p(\bc|\bx) \varpropto
R[c_{1:N}] & \varpropto F[c_{1:N}] = e^{-{E}[c_{1:N}]} \,. 
\label{eq:reward}
\end{align}

\subsection{Objective function}\label{seq:obj_func} 
To train the model, we consider a loss function over the network parameters $\btheta$ that consists of three terms:

{\bf Marginal Consistency loss.} 
Considering \autoref{eq:FFE} and the fact that 
non-terminal states have a single parent, 
the flow matching equations (\autoref{eq:flow_match}) become in our case
\begin{align}
F [c_{1:n-1}, \rho] = \sum_{c_{n}} F [c_{1:n}, \rho],
\label{eq:flow_match_for_us}
\end{align}
and our goal is to minimize the discrepancy between the two terms.
By substituting $F[ c_{1:n}, \rho] = e^{-\hat{E}[c_{1:n}]}$ (as shown in~\autoref{eq:FFE}), and transitioning to log space, we aim to minimize the following loss function:
%We approximate this equation in log space along a trajectory~$\bc$ via the 
%loss 
%($\btheta$ are the parameters of $\hat{E}$)
% \begin{align}
% \mathcal{L}_{\btheta}^{mc}(\bc,\bx)& =\Sigma_{n=2}^{N} \mathcal{L}_{\btheta}^{mc}(c_{1:n-1})\,,
% \label{eqn:mc_loss}
% \end{align}
% where
% \begin{align}
% \mathcal{L}_{\btheta}^{mc}(c_{1:n-1}) = 
% \left( \hat{E}[c_{1:n-1}]  +\log \sum_{c_{n}}
% e^{-\hat{E}[c_{1:n}]}
% \right)^2.
% \nonumber
% \end{align}
\begin{align}
\mathcal{L}_{\btheta}^{\text{mc}}(\bc,\bx)& \!=\!\sum_{n=2}^{N} 
\left( \hat{E}[c_{1:n-1}]  
+\log \Sigma_{c_{n}}
e^{-\hat{E}[c_{1:n}]}
\right)^2\!\!\!.
\label{eqn:mc_loss}
\end{align}
This is the Flow Matching loss~\citep{bengio2021flow}, which we dub {\it Marginal Consistency} loss.
In our model it is preferred over the other 
losses mentioned 
in~\autoref{sec:gflonet_review} (Detailed Balance, Trajectory Balance and SubTB($\lambda$))
because \autoref{eqn:mc_loss} directly enforces the consistency of marginal distributions, which is central to the order invariance 
proof in~\autoref{subsec:order_invariance}. The other losses would only enforce this condition asymptotically, achieving equivalence only in the limit of large model capacity and training data.


%%%%%% Algorithm (training) %%%%%%%%
\begin{algorithm}[t!]
\caption{GFNCP training framework}
\label{alg:training}
\textbf{Input:} Training distribution  $p_{\text{data}}(\bc,\bx)$ over data $\bx =\{x_i\}_{i=1}^N$ and assignments $\bc =\{c_i\}_{i=1}^N$ (using instance discrimination, \autoref{seq:self_sup}), hyperparameter $\beta \in [0,1]$. 
\begin{algorithmic}[1]
    \State Initialize $u, h, g,f$ networks with parameters $\btheta$
    \Repeat
        \State $l \sim \text{Bernoulli}(\beta)$
        \If{$l = 1$}   \algorithmiccomment{Data policy}
             \State Sample $(\bc,\bx) \sim p_{\text{data}}(\bc,\bx)$
            \State \parbox[t]{180pt}{Update $\btheta$ with gradient 
            \\
            $\nabla_{\btheta}[\mathcal{L}^{\text{mc}}_{\btheta}(\bc,\bx) + \delta 
            \mathcal{L}^{\text{reg}}_{\btheta}(\bc,\bx) + \lambda \mathcal{L}^{\text{cd}}_{\btheta}(\bc,\bx)]$ 
            \strut}
        \Else  \algorithmiccomment{Space exploration}
             \State Sample $\bx \sim p_{\text{data}}(\bx)$
            \State \parbox[t]{180pt}{Uniformly sample $\bc^U$.  \strut}
            \State \parbox[t]{180pt}{Update $\btheta$ with gradient 
            \\
            $\nabla_{\btheta}[\mathcal{L}_{\btheta}^{\text{mc}}(\bc^U,\bx) + \delta  
            \mathcal{L}_{\btheta}^{\text{reg}}(\bc^U,\bx)]$ }
        \EndIf
    \Until{convergence criterion}
\end{algorithmic}
\end{algorithm}
%%%%%% END Algorithm (training) %%%%%%%%


{\bf Reward loss.}
The normalized reward is: 
\begin{align}
    p_{\btheta}(c_{1:N}|\bx) = \frac{e^{-E[c_{1:N}]}}{Z},
\end{align}
where $Z$ is a normalization constant.  
To learn this function we minimize the negative log-likelihood,
\begin{equation}
\mathcal{L}_{\btheta}^{\text{cd}}(\bc,\bx)=-  \  \log p_{\btheta}(c_{1:N}|\bx) \,,
\label{eqn:cd_loss}
\end{equation}
whose contrastive divergence 
gradient is: 
\begin{align}
    \nabla_{\theta} \mathcal{L}_{\btheta}^{\text{cd}}(\bc,\bx) 
    = \nabla_{\theta} E[c_{1:N}]  - 
    \mathbb{E}_{p_{\btheta}(\tilde{c}_{1:N}|\bx) }
    E[\tilde{c}_{1:N}] \,.
\label{eq:cd_gradient}
\end{align}
In the second term we approximate the expectation using a sample $\tilde{c}_{1:N}$ generated by the learned policy. 

{\bf Regularization.} To regularize the value of the energy function we add the following term:
\begin{equation}
\mathcal{L}_{\btheta}^{\text{reg}}(\bc,\bx)
=(E[c_{1:N}])^2. 
\label{eqn:reg_equation}
\end{equation}
{\bf Training data and policy.}   
We create a training distribution $p_{\text{data}}(\bc,\bx)$ as described below in~\autoref{seq:self_sup}. Since each training data point $(\bc,\bx)$ contains a full trajectory, we found it convenient to optimize the losses  {\it off-policy} (except for $\tilde{\bc}$ in \autoref{eq:cd_gradient}). 
The full objective  is:
\begin{align}
\mathcal{L}_{\btheta} 
& =
\mathbb{E}_{\pi(\bc|\bx)p_{\text{data}}(\bx) }
[\mathcal{L}_{\btheta}^{\text{mc}}(\bc,\bx)
+ \delta \mathcal{L}_{\btheta}^{\text{reg}}
(\bc,\bx)
] 
\label{eqn:combined_loss}
\\ & + 
\nonumber
\lambda
\mathbb{E}_{p_{\text{data}}(\bc,\bx) }
 [\mathcal{L}_{\btheta}^{\text{cd}}(\bc,\bx)], 
\end{align}
where $\delta, \lambda$ are hyperparameters and the policy $\pi(\bc|\bx)$ is a 
mixture of $p_{\text{data}}(\bc|\bx)$ and 
a uniform random sample of $\bc$, to assure full support 
for the flow matching equations~\autoref{eq:flow_match_for_us}. See~\autoref{alg:training}. 



% \RED{
% Suggestion (Irit):

% Marginal-consistency loss:
% \begin{align}
% \mathcal{L}_{\btheta}^{mc}(\bc,\bx)& =\Sigma_{n=1}^{N-1} (log\ \tilde{p_{\btheta}}(c_{1:n-1}|\bx)\\
% & - log \Sigma_{c_n}\tilde{p_{\btheta}}(c_{1:n-1},c_n|\bx))^2 
% \end{align}

% Contrastive-divergence loss:
% \begin{equation}
% \mathcal{L}_{\btheta}^{cd}(\bc,\bx)=- \ \lambda (log\ p_{\btheta}(c_{1:N-1}|\bx)) 
% \end{equation}

% Regularization:
% \begin{equation}
% \mathcal{L}_{\btheta}^{reg}(\bc,\bx)=\delta (log\ \tilde{p_{\btheta}}(c_{1:N-1}|\bx))
% \end{equation}

% Full objective function:

% $$
% \mathcal{L}_{\btheta}(\bc,\bx)=\mathcal{L}_{\btheta}^{mc} + \mathcal{L}_{\btheta}^{cd} + \mathcal{L}_{\btheta}^{reg}
% $$

% }

% \subsection{Data order in the state definition}
% Similar to~\cite{zhang2023let}, we fix a uniform backward policy $P_B(q_n|s_n) = \frac{1}{n}$. 

\subsection{Order invariance}
\label{subsec:order_invariance}
{\bf Lemma:} when \autoref{eqn:mc_loss} is zero for all trajectories, the probability of reaching a final state is independent of the selected data order $\rho$, and thus \autoref{eq:R_F} holds. 
\begin{proof}
The probability of any trajectory $\tau$ is 
\begin{align}
\label{eq:prob_trajectory}
P(\tau) = & P_F(c_1 ) P_F(c_2| c_1 ) 
 \ldots P_F(c_N|c_{1:N-1})
\\
 & = \frac{1}{N!} 
\frac{ \cancel{ F(c_{1:2}, \rho) }}
{ \cancel{ \sum\limits_{c_2'} F(c_1, c_2', \rho) }}
\cdots
\frac{ F(c_{1:N}) } 
{ \cancel{ \sum\limits_{c_N'}
F(c_{1:N-1},c_N', \rho)}}  
\nn 
\\
& \varpropto F(c_{1:N}) = e^{-E[c_{1:N}] },
\label{eq:result_cancellations}    
\end{align}
where in all the pairwise cancellations we used \autoref{eq:flow_match_for_us},
which holds when \autoref{eqn:mc_loss} is zero. 
The proof is completed by noting that 
$E[c_{1:N}]$ is invariant 
under identical simultaneous permutations of $x_{1:N}$
and $c_{1:N}$.
\end{proof}
% The proof is similar to that of Proposition 2 in  \citealt{bengio2021flow}, 

\subsection{Self-supervised learning} \label{seq:self_sup}   
While traditional amortized clustering models require ground-truth 
labels~\citep{pakman2018amortized,pakman2020, DAC}, we propose a self-supervised approach using instance discrimination~\citep{dosovitskiy2014discriminative,wu2018unsupervised,chen2020simple,he2020momentum}, 
which assigns unique labels to each data point based on data augmentations. 

\paragraph{Computational cost.}
Like NCP, our model has an inference cost of $O(N)$. Faster models, which sample cluster members in parallel with $O(K)$ cost \citep{pakman2020}, introduce latent continuous variables and do not yield sample probabilities,  as this requires expensive marginalizations.





