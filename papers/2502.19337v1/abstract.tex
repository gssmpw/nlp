
% Neural models for amortized probabilistic clustering  yield samples of cluster labels for full datasets while avoiding lengthy Markov chain  runs and the need for explicit data likelihoods. In this work we revisit the Neural Clustering Process, an energy-based amortized clustering model, and 
% show that its cluster assignments 
% can be highly dependent on the order in which the data points are visited. 
 % We address this problem via a new objective obtained by formulating the model as a Generative Flow Network with a shared 
 % energy-based parametrization of policy and reward. We show that the flow matching conditions are equivalent to consistency  of the clustering posterior under marginalization, which in turn implies 
 % order invariance. 
 % We also show that xxx and that yyy. 


% \RED{
% [IRIT:]
% Neural models for amortized probabilistic clustering yield samples of cluster labels given a set-structured input, while avoiding lengthy Markov-chain runs and the need for explicit data likelihoods.
% %
% Existing methods, like the Neural Clustering Process, rely on a sequential framework, often leading to cluster assignments that are highly dependent on the data order.
% Moreover, non-sequential approaches are unable to generate assignment probabilities, and experience performance degradation as the set size grows.
% %
% In this paper, we introduce GFNCP, a novel generative clustering framework for solving amortized clustering.
% %
% GFNCP is formulated as a Generative Flow Network with a shared energy-based parametrization of policy and reward.
% %
% We show that the flow matching conditions are equivalent to consistency of the clustering posterior under marginalization, which in turn implies order invariance. 
% %
% GFNCP also outperforms existing methods in clustering performance on both synthetic and real-world data.
% %
% Our code will be made publicly available upon acceptance.
% }


Neural models for amortized probabilistic clustering yield samples of cluster labels given a set-structured input, while avoiding lengthy Markov chain runs and the need for explicit data likelihoods.
%
Existing methods which label each data point 
sequentially, like the Neural Clustering Process, often lead to cluster assignments highly dependent on the data order.
Alternatively, methods that sequentially create full clusters, 
do not provide assignment probabilities.
%
In this paper, we introduce GFNCP, a novel framework for  amortized clustering.
%
GFNCP is formulated as a Generative Flow Network with a shared energy-based parametrization of policy and reward.
%
We show that the flow matching conditions are equivalent to consistency of the clustering posterior under marginalization, which in turn implies order invariance. 
%
GFNCP also outperforms existing methods in clustering performance on both synthetic and real-world data.
%Our code will be made publicly available upon acceptance.



