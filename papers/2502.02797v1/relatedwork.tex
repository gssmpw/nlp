\section{Related Work}
\label{sec:related-work}
\subsection{Mitigating Catastrophic Forgetting}
We begin by summarizing the vast literature on catastrophic forgetting with a focus on prior works most relevant to our proposed setting.
For a streamlined presentation, we survey prior work in two settings -- data-aware and data-oblivious. 
We refer the reader to \cref{app:related-work} for a more detailed and explanatory literature review. 

\subsubsection{Data-aware approaches}
The majority of the approaches for mitigating  
forgetting assume task-specific knowledge access to different extents; either (a subset of) the pre-training dataset itself or some information/statistic computed from pre-training data. Below, we describe the data-aware approaches based on how they make use of task-specific knowledge.
\\
\\
\textbf{Regularization-based methods.} This line of work aims to preserve existing capabilities by keeping the parameters close to the pre-trained model. The key idea is to introduce task-specific regularization to penalize modifications along the ``important'' directions for the old tasks 
\citep{ahn2019uncertainty}. 
\citet{kirkpatrick2016overcoming} introduces the elastic weight consolidation (EWC) algorithm, which estimates the important directions by approximating the Fisher information matrix. 
Several variants of EWC have been proposed \citep{schwarz2018progress, ritter2018online, Lee2020ContinualLW, liu2018rotate}. \citet{zenke2017continual, aljundi2018memory} infer the importance of each parameter by their variational effect on the outputs. In a similar spirit, \citet{lee2017overcoming} aims to match the posteriors of the pre-trained and fine-tuned models. 
\\
\\
\textbf{Optimization-driven methods.} Another perspective to mitigating forgetting is guiding the optimization process by constraining the algorithms directly as opposed to manipulating the loss function. 
The core idea is to keep track of \enquote{important directions} for the old tasks, and train on the new task \enquote{orthogonally.} 
This could be done by storing prior data samples or gradients in a buffer \citep{lopezpaz2017gradient, farajtabar2020orthogonal, chaudhry2018efficient} or by incrementally expanding the subspace of important directions without storing task-specific information \citep{zeng2019continual, wang2021training, wang2023orthogonal}.
\\
\\
\textbf{Replay-based methods.} A more direct approach is to store old task samples in buffers and introduce them into the training process for the new task to refresh task-specific representations periodically.
There are several components to such methods. Some prior work focus on \emph{data selection} based on the nature of old data access \citep{rebuffi2017icarl, aljundi2019gradient, Bang2021RainbowMC, chaudhry2019continual, isle2018selective, delange2021continual, borsos2020coresets, tiwari2021gcr} (e.g., streaming versus on-demand). Another important perspective is the \emph{re-introduction strategy} of the stored information into the fine-tuning process \citep{silver2002task, li2016learning, Triki2017EncoderBL, Lee2019OvercomingCF, dhar2019learning, rebuffi2017icarl, riemer2019learning, chaudhry2019continual, delange2021continual, tiwari2021gcr}.
\\
\\
\textbf{Architecture-driven methods.} Another technique to limit interference between tasks is to allocate a separate trainable set of parameters per task. 
This could be done by initializing a sub-networks per new task \citep{Rusu2016ProgressiveNN, aljundi2017expertgate, patrick2020routing, rajasegaran2019random, Ramesh2021ModelZA, wang2023incorporating, wang2022coscl}, gradually expanding the parameters of a base network \citep{yoon2018lifelong, Ostapenko2019LearningTR, hung2019compacting}, or segregating a fixed model into task-specific subsets \citep{mallya2018piggyback, kang2022forgetfree, serra2018overcoming, worstman2020supermasks, Mallya2017PackNetAM, gurbuz2022nispa, jung2020continual}. 
The main downside with this line of work is that task identities must be known for inference to (de)activate relevant sub-networks \citep{aljundi2017expertgate}. 

\subsubsection{Data-oblivious approaches}
In the less-explored data-oblivious setting, it is particularly challenging to devise a principled approach, as there is no access to any data-specific information, except for the pre-trained model. 
One line of work explores the simple idea of ``model averaging'' (MA) which essentially does a convex combination of the parameters of the pre-trained model and that of the fully fine-tuned model for the new task. 
MA and more sophisticated model merging variants have been studied in relevant context to forgetting \citep{lubana2021quadratic, wortsman2021robust, ilharco2023editing, lin2023mitigating, kleiman2025soupgomitigatingforgetting}. 
Some recent works \citep{chen2024mofo,panda2406lottery} introduce different strategies to selectively update a subset of parameters in a pre-training data-agnostic manner.
Finally, \citet{biderman2024lora} has shown that LoRA \cite{hu2022lora} could be effective for mitigating catastrophic forgetting in transformers. 
Unlike the methods discussed above which focus on the parameter or gradient space, ours focuses on the sample space.

\subsection{Sample Selection and Weighting}
Sample-wise importance selection/weighting has been studied in optimization papers \citep{needell2014stochastic, zhao2015stochastic, Alain2015VarianceRI, stich2017safe} and ML papers \citep{Loshchilov2015OnlineBS, shrivastava2016training, katharopoulos2017biased, katharopoulos2018not, kawaguchi2020ordered, dasunderstanding} to speed up the optimization/training process by reducing the variance of the gradient updates. 
Such papers advocate focusing on \enquote{hard} samples with high-gradient norms or losses. In contrast, we focus on \enquote{easy} samples to mitigate forgetting. Another line of work focuses on robust learning under uncertain data distributions. Distributionally robust optimization (DRO) proposes to minimize the worst-case weighted loss, where the sample weights are constrained or regularized \citep{ben2013robust,levy2020large,duchi2021learning,qi2021online}. 
Some recent works \citep{xie2024doremi,chen2024take,anonymous2025dynamic} propose dynamic sample-weighting strategies for LLM training based on the previously discussed ideas.