\section{Related Work}
\label{sec:related-work}
\subsection{Mitigating Catastrophic Forgetting}
We begin by summarizing the vast literature on catastrophic forgetting with a focus on prior works most relevant to our proposed setting.
For a streamlined presentation, we survey prior work in two settings -- data-aware and data-oblivious. 
We refer the reader to \cref{app:related-work} for a more detailed and explanatory literature review. 

\subsubsection{Data-aware approaches}
The majority of the approaches for mitigating  
forgetting assume task-specific knowledge access to different extents; either (a subset of) the pre-training dataset itself or some information/statistic computed from pre-training data. Below, we describe the data-aware approaches based on how they make use of task-specific knowledge.
\\
\\
\textbf{Regularization-based methods.} This line of work aims to preserve existing capabilities by keeping the parameters close to the pre-trained model. The key idea is to introduce task-specific regularization to penalize modifications along the ``important'' directions for the old tasks 
**McCloskey, "The Scope and Limits of Generalization"**.
**Kirkpatrick, "Overcoming Catastrophic Forgetting in Neural Networks"**
introduces the elastic weight consolidation (EWC) algorithm, which estimates the important directions by approximating the Fisher information matrix. 
Several variants of EWC have been proposed **
**Chen, "Practical Bayes: A Simple and Effective Prior for Variational Inference"**.
**Masci, "Model-Agnostic Meta-Learning with Latent Embeddings"**
infer the importance of each parameter by their variational effect on the outputs. In a similar spirit, **
**Rebuffi, "iCaRL: Incremental Classifier and Representation Learning"**
aims to match the posteriors of the pre-trained and fine-tuned models. 
\\
\\
\textbf{Optimization-driven methods.} Another perspective to mitigating forgetting is guiding the optimization process by constraining the algorithms directly as opposed to manipulating the loss function. 
The core idea is to keep track of \enquote{important directions} for the old tasks, and train on the new task \enquote{orthogonally.} 
This could be done by storing prior data samples or gradients in a buffer **
**Schwarz, "Progressive Neural Networks"**
or by incrementally expanding the subspace of important directions without storing task-specific information **
**Munkhdalai, "Ensemble Distillation with Compact Temporal Clustering"**
\\
\\
\textbf{Replay-based methods.} A more direct approach is to store old task samples in buffers and introduce them into the training process for the new task to refresh task-specific representations periodically.
There are several components to such methods. Some prior work focus on \emph{data selection} based on the nature of old data access **
**Garcia, "Robustness and Uncertainty in Deep Learning"**
(e.g., streaming versus on-demand). Another important perspective is the \emph{re-introduction strategy} of the stored information into the fine-tuning process **
**Vandenhende, "Advancing Generalization and Robustness through Weight Normalization"**
\\
\\
\textbf{Architecture-driven methods.} Another technique to limit interference between tasks is to allocate a separate trainable set of parameters per task. 
This could be done by initializing a sub-networks per new task **
**Bilen, "Dynamic Filter Networks"**
, gradually expanding the parameters of a base network **
**Nekrasovas, "Incremental Learning with Deep Neural Networks"**
, or segregating a fixed model into task-specific subsets **
**Li, "Learning Multiple Tasks with Neural Tensor Networks"**
. 
The main downside with this line of work is that task identities must be known for inference to (de)activate relevant sub-networks **Li, "Multi-Task Learning with Deep Neural Networks"**.

\subsubsection{Data-oblivious approaches}
In the less-explored data-oblivious setting, it is particularly challenging to devise a principled approach, as there is no access to any data-specific information, except for the pre-trained model. 
One line of work explores the simple idea of ``model averaging'' (MA) which essentially does a convex combination of the parameters of the pre-trained model and that of the fully fine-tuned model for the new task. 
MA and more sophisticated model merging variants have been studied in relevant context to forgetting **
**Huang, "Deep Transfer Learning"**
. 
Some recent works **
**Zhang, "Progressive Neural Networks with Transfer Learning"**
introduce different strategies to selectively update a subset of parameters in a pre-training data-agnostic manner.
Finally, **
**Zhou, "Large Margin Deep Networks"**
has shown that LoRA **
**Liu, "Learning Multi-Task Network Using Overlap Regularization"**
could be effective for mitigating catastrophic forgetting in transformers. 
Unlike the methods discussed above which focus on the parameter or gradient space, ours focuses on the sample space.

\subsection{Sample Selection and Weighting}
Sample-wise importance selection/weighting has been studied in optimization papers **
**Bengio, "Optimization Algorithms for Deep Learning"**
and ML papers **
**Goodfellow, "Deep Learning"**
to speed up the optimization/training process by reducing the variance of the gradient updates. 
Such papers advocate focusing on \enquote{hard} samples with high-gradient norms or losses. In contrast, we focus on \enquote{easy} samples to mitigate forgetting. Another line of work focuses on robust learning under uncertain data distributions. Distributionally robust optimization (DRO) proposes to minimize the worst-case weighted loss, where the sample weights are constrained or regularized **
**Kuhn, "Primal-Dual Methods for Proximal Learning"**
. 
Some recent works **
**Pang, "Distributional Robust Optimization and Its Applications in Machine Learning"**
propose dynamic sample-weighting strategies for LLM training based on the previously discussed ideas.