@article{Alain2015VarianceRI,
  title={Variance Reduction in SGD by Distributed Importance Sampling},
  author={Guillaume Alain and Alex Lamb and Chinnadhurai Sankar and Aaron C. Courville and Yoshua Bengio},
  journal={ArXiv},
  year={2015},
  volume={abs/1511.06481},
  url={https://api.semanticscholar.org/CorpusID:6546520}
}

@article{Bang2021RainbowMC,
  title={Rainbow Memory: Continual Learning with a Memory of Diverse Samples},
  author={Jihwan Bang and Heesu Kim and Young Joon Yoo and Jung-Woo Ha and Jonghyun Choi},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={8214-8223},
  url={https://api.semanticscholar.org/CorpusID:232427874}
}

@article{Lee2019OvercomingCF,
  title={Overcoming Catastrophic Forgetting With Unlabeled Data in the Wild},
  author={Kibok Lee and Kimin Lee and Jinwoo Shin and Honglak Lee},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={312-321},
  url={https://api.semanticscholar.org/CorpusID:201314887}
}

@article{Lee2020ContinualLW,
  title={Continual Learning With Extended Kronecker-Factored Approximate Curvature},
  author={Janghyeon Lee and Hyeong Gwon Hong and Donggyu Joo and Junmo Kim},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={8998-9007},
  url={https://api.semanticscholar.org/CorpusID:215786151}
}

@article{Loshchilov2015OnlineBS,
  title={Online Batch Selection for Faster Training of Neural Networks},
  author={Ilya Loshchilov and Frank Hutter},
  journal={ArXiv},
  year={2015},
  volume={abs/1511.06343},
  url={https://api.semanticscholar.org/CorpusID:5324823}
}

@article{Mallya2017PackNetAM,
  title={PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning},
  author={Arun Mallya and Svetlana Lazebnik},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={7765-7773},
  url={https://api.semanticscholar.org/CorpusID:35249701}
}

@article{Ostapenko2019LearningTR,
  title={Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning},
  author={Oleksiy Ostapenko and Mihai Marian Puscas and Tassilo Klein and Patrick J{\"a}hnichen and Moin Nabi},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={11313-11321},
  url={https://api.semanticscholar.org/CorpusID:102353035}
}

@inproceedings{Ramesh2021ModelZA,
  title={Model Zoo: A Growing Brain That Learns Continually},
  author={Rahul Ramesh and Pratik Chaudhari},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:245007201}
}

@article{Rusu2016ProgressiveNN,
  title={Progressive Neural Networks},
  author={Andrei A. Rusu and Neil C. Rabinowitz and Guillaume Desjardins and Hubert Soyer and James Kirkpatrick and Koray Kavukcuoglu and Razvan Pascanu and Raia Hadsell},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.04671},
  url={https://api.semanticscholar.org/CorpusID:15350923}
}

@article{Triki2017EncoderBL,
  title={Encoder Based Lifelong Learning},
  author={A. Triki and Rahaf Aljundi and Matthew B. Blaschko and Tinne Tuytelaars},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={1329-1337},
  url={https://api.semanticscholar.org/CorpusID:12253672}
}

@inproceedings{ahn2019uncertainty,
 author = {Ahn, Hongjoon and Cha, Sungmin and Lee, Donggyu and Moon, Taesup},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Uncertainty-based Continual Learning with Adaptive Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2c3ddf4bf13852db711dd1901fb517fa-Paper.pdf},
 volume = {32},
 year = {2019}
}

@INPROCEEDINGS{aljundi2017expertgate,
  author={Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Expert Gate: Lifelong Learning with a Network of Experts}, 
  year={2017},
  volume={},
  number={},
  pages={7120-7129},
  keywords={Training;Data models;Logic gates;Training data;Load modeling;Neural networks},
  doi={10.1109/CVPR.2017.753}
}

@inproceedings{aljundi2018memory,
author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
title = {Memory Aware Synapses: Learning What (not) to Forget},
year = {2018},
isbn = {978-3-030-01218-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-01219-9_9},
doi = {10.1007/978-3-030-01219-9_9},
booktitle = {Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September 8–14, 2018, Proceedings, Part III},
pages = {144–161},
numpages = {18},
keywords = {Previous Task, Catastrophic Forgetting, Limited Capacity Model, Online Manner, Importance Weights},
location = {Munich, Germany}
}

@inbook{aljundi2019gradient,
author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
title = {Gradient based sample selection for online continual learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A continual learning agent learns online with a non-stationary and never-ending stream of data. The key to such learning process is to overcome the catastrophic forgetting of previously seen data, which is a well known problem of neural networks. To prevent forgetting, a replay buffer is usually employed to store the previous data for the purpose of rehearsal. Previous works often depend on task boundary and i.i.d. assumptions to properly select samples for the replay buffer. In this work, we formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. The goal is to select a fixed subset of constraints that best approximate the feasible region defined by the original constraints. We show that it is equivalent to maximizing the diversity of samples in the replay buffer with parameters gradient as the feature. We further develop a greedy alternative that is cheap and efficient. The advantage of the proposed method is demonstrated by comparing to other alternatives under the continual learning setting. Further comparisons are made against state of the art methods that rely on task boundaries which show comparable or even better results for our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1058},
numpages = {10}
}

@article{ben2013robust,
  title={Robust solutions of optimization problems affected by uncertain probabilities},
  author={Ben-Tal, Aharon and Den Hertog, Dick and De Waegenaere, Anja and Melenberg, Bertrand and Rennen, Gijs},
  journal={Management Science},
  volume={59},
  number={2},
  pages={341--357},
  year={2013},
  publisher={INFORMS}
}

@inproceedings{borsos2020coresets,
 author = {Borsos, Zal\'{a}n and Mutny, Mojmir and Krause, Andreas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {14879--14890},
 publisher = {Curran Associates, Inc.},
 title = {Coresets via Bilevel Optimization for Continual Learning and Streaming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/aa2a77371374094fe9e0bc1de3f94ed9-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{chaudhry2018efficient,
title={Efficient Lifelong Learning with A-{GEM}},
author={Arslan Chaudhry and Marc’Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Hkf2_sC5FX},
}

@inproceedings{chaudhry2019continual,
  edition = {},
  number = {},
  journal = {Workshop on Multi-Task and Lifelong Reinforcement Learning},
  pages = {},
  publisher = {},
  school = {},
  title = {Continual learning with tiny episodic memories},
  volume = {},
  author = {Chaudhry, A and Rohrbach, M and Elhoseiny, M and Ajanthan, T and Dokania, P and Torr, P and Ranzato, M},
  editor = {},
  year = {2019},
  organizer = {},
  series = {}
}

@article{chen2024mofo,
  title={MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning},
  author={Chen, Yupeng and Wang, Senmiao and Lin, Zhihang and Qin, Zeyu and Zhang, Yushun and Ding, Tian and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2407.20999},
  year={2024}
}

@article{chen2024take,
  title={Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization},
  author={Chen, Xuxi and Wang, Zhendong and Sow, Daouda and Yang, Junjie and Chen, Tianlong and Liang, Yingbin and Zhou, Mingyuan and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2402.14270},
  year={2024}
}

@inproceedings{dasunderstanding,
  title={Understanding the Training Speedup from Sampling with Approximate Losses},
  author={Das, Rudrajit and Chen, Xi and Ieong, Bertram and Bansal, Parikshit and others},
  year={2024},
  booktitle={Forty-first International Conference on Machine Learning}
}

@InProceedings{delange2021continual,
    author    = {De Lange, Matthias and Tuytelaars, Tinne},
    title     = {Continual Prototype Evolution: Learning Online From Non-Stationary Data Streams},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {8250-8259}
}

@inproceedings{dhar2019learning,
  author       = {Prithviraj Dhar and
                  Rajat Vikram Singh and
                  Kuan{-}Chuan Peng and
                  Ziyan Wu and
                  Rama Chellappa},
  title        = {Learning Without Memorizing},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
                  2019, Long Beach, CA, USA, June 16-20, 2019},
  pages        = {5138--5146},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2019},
  url          = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Dhar\_Learning\_Without\_Memorizing\_CVPR\_2019\_paper.html},
  doi          = {10.1109/CVPR.2019.00528},
  timestamp    = {Mon, 30 Aug 2021 17:01:14 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/DharSPWC19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{duchi2021learning,
  title={Learning models with uniform performance via distributionally robust optimization},
  author={Duchi, John C and Namkoong, Hongseok},
  journal={The Annals of Statistics},
  volume={49},
  number={3},
  pages={1378--1406},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}

@InProceedings{farajtabar2020orthogonal,
  title = 	 {Orthogonal Gradient Descent for Continual Learning},
  author =       {Farajtabar, Mehrdad and Azizan, Navid and Mott, Alex and Li, Ang},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3762--3773},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/farajtabar20a/farajtabar20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/farajtabar20a.html},
  abstract = 	 {Neural networks are achieving state of the art and sometimes super-human performance on learning tasks across a variety of domains. Whenever these problems require learning in a continual or sequential manner, however, neural networks suffer from the problem of catastrophic forgetting; they forget how to solve previous tasks after being trained on a new task, despite having the essential capacity to solve both tasks if they were trained on both simultaneously. In this paper, we propose to address this issue from a parameter space perspective and study an approach to restrict the direction of the gradient updates to avoid forgetting previously-learned data. We present the Orthogonal Gradient Descent (OGD) method, which accomplishes this goal by projecting the gradients from new tasks onto a subspace in which the neural network output on previous task does not change and the projected gradient is still in a useful direction for learning the new task. Our approach utilizes the high capacity of a neural network more efficiently and does not require storing the previously learned data that might raise privacy concerns. Experiments on common benchmarks reveal the effectiveness of the proposed OGD method.}
}

@article{gurbuz2022nispa,
author = {Mustafa B Gurbuz, Constantine Dovrolis},
title = {NISPA: Neuro-Inspired Stability-Plasticity Adaptation for Continual Learning in Sparse Networks}, 
url = {https://par.nsf.gov/biblio/10389701}, 
abstractNote = {}, 
journal = {Proceedings of the 39th International Conference on Machine Learning}, 
volume = {162}, 
year = {2022}
}

@inbook{hung2019compacting,
author = {Hung, Steven C. Y. and Tu, Cheng-Hao and Wu, Cheng-En and Chen, Chien-Hung and Chan, Yi-Ming and Chen, Chu-Song},
title = {Compacting, picking and growing for unforgetting continual learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning. Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1225},
numpages = {11}
}

@inproceedings{ilharco2023editing,
title={Editing models with task arithmetic},
author={Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=6t0Kwf8-jrj}
}

@inproceedings{isle2018selective,
author = {Isele, David and Cosgun, Akansel},
title = {Selective experience replay for lifelong learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Deep reinforcement learning has emerged as a powerful tool for a variety of learning tasks, however deep nets typically exhibit forgetting when learning multiple tasks in sequence. To mitigate forgetting, we propose an experience replay process that augments the standard FIFO buffer and selectively stores experiences in a long-term memory. We explore four strategies for selecting which experiences will be stored: favoring surprise, favoring reward, matching the global training distribution, and maximizing coverage of the state space. We show that distribution matching successfully prevents catastrophic forgetting, and is consistently the best approach on all domains tested. While distribution matching has better and more consistent performance, we identify one case in which coverage maximization is beneficial - when tasks that receive less trained are more important. Overall, our results show that selective experience replay, when suitable selection algorithms are employed, can prevent catastrophic forgetting.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {404},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{jung2020continual,
author = {Jung, Sangwon and Ahn, Hongjoon and Cha, Sungmin and Moon, Taesup},
title = {Continual learning with node-importance based adaptive group sparse regularization},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each neural network node based on its the importance, which is adaptively updated after learning each task. By utilizing the proximal gradient descent method, the exact sparsity and freezing of the model is guaranteed during the learning process, and thus, the learner explicitly controls the model capacity. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to facilitate efficient learning and prevent the negative transfer. Throughout the extensive experimental results, we show that our AGS-CL uses orders of magnitude less memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative benchmarks for both supervised and reinforcement learning.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {308},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@InProceedings{kang2022forgetfree,
  title = 	 {Forget-free Continual Learning with Winning Subnetworks},
  author =       {Kang, Haeyong and Mina, Rusty John Lloyd and Madjid, Sultan Rizky Hikmawan and Yoon, Jaehong and Hasegawa-Johnson, Mark and Hwang, Sung Ju and Yoo, Chang D.},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {10734--10750},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/kang22b/kang22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/kang22b.html},
  abstract = 	 {Inspired by Lottery Ticket Hypothesis that competitive subnetworks exist within a dense network, we propose a continual learning method referred to as Winning SubNetworks (WSN), which sequentially learns and selects an optimal subnetwork for each task. Specifically, WSN jointly learns the model weights and task-adaptive binary masks pertaining to subnetworks associated with each task whilst attempting to select a small set of weights to be activated (winning ticket) by reusing weights of the prior subnetworks. The proposed method is inherently immune to catastrophic forgetting as each selected subnetwork model does not infringe upon other subnetworks. Binary masks spawned per winning ticket are encoded into one N-bit binary digit mask, then compressed using Huffman coding for a sub-linear increase in network capacity with respect to the number of tasks.}
}

@article{katharopoulos2017biased,
  title={Biased importance sampling for deep neural network training},
  author={Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1706.00043},
  year={2017}
}

@InProceedings{katharopoulos2018not,
  title = 	 {Not All Samples Are Created Equal: Deep Learning with Importance Sampling},
  author =       {Katharopoulos, Angelos and Fleuret, Francois},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2525--2534},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/katharopoulos18a/katharopoulos18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/katharopoulos18a.html},
  abstract = 	 {Deep Neural Network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on "informative" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%.}
}

@inproceedings{kawaguchi2020ordered,
  title={Ordered sgd: A new stochastic optimization framework for empirical risk minimization},
  author={Kawaguchi, Kenji and Lu, Haihao},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={669--679},
  year={2020},
  organization={PMLR}
}

@article{kirkpatrick2016overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={James Kirkpatrick and Razvan Pascanu and Neil C. Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and Agnieszka Grabska-Barwinska and Demis Hassabis and Claudia Clopath and Dharshan Kumaran and Raia Hadsell},
  journal={Proceedings of the National Academy of Sciences},
  year={2016},
  volume={114},
  pages={3521 - 3526},
  url={https://api.semanticscholar.org/CorpusID:4704285}
}

@misc{kleiman2025soupgomitigatingforgetting,
      title={Soup to go: mitigating forgetting during continual learning with model averaging}, 
      author={Anat Kleiman and Gintare Karolina Dziugaite and Jonathan Frankle and Sham Kakade and Mansheej Paul},
      year={2025},
      eprint={2501.05559},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.05559}, 
}

@inproceedings{lee2017overcoming,
author = {Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},
title = {Overcoming catastrophic forgetting by incremental moment matching},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4655–4665},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{levy2020large,
  title={Large-scale methods for distributionally robust optimization},
  author={Levy, Daniel and Carmon, Yair and Duchi, John C and Sidford, Aaron},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8847--8860},
  year={2020}
}

@inproceedings{li2016learning,
  added-at = {2020-12-31T00:00:00.000+0100},
  author = {Li, Zhizhong and Hoiem, Derek},
  biburl = {https://www.bibsonomy.org/bibtex/2b0bcacbcf9df582e13b217c7db73b0c2/dblp},
  booktitle = {ECCV (4)},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  ee = {https://doi.org/10.1007/978-3-319-46493-0_37},
  interhash = {6859db3a4dc1a86458a19ced2aa051f6},
  intrahash = {b0bcacbcf9df582e13b217c7db73b0c2},
  isbn = {978-3-319-46492-3},
  keywords = {dblp},
  pages = {614-629},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  timestamp = {2024-04-10T00:28:16.000+0200},
  title = {Learning Without Forgetting.},
  url = {http://dblp.uni-trier.de/db/conf/eccv/eccv2016-4.html#LiH16},
  volume = 9908,
  year = 2016
}

@article{lin2023mitigating,
  title   = {Mitigating the Alignment Tax of RLHF},
  author  = {Lin, Yong and Lin, Hangyu and Xiong, Wei and Diao, Shizhe and Liu, Jianmeng and Zhang, Jipeng and Pan, Rui and Wang, Haoxiang and Hu, Wenbin and Zhang, Hanning and others},
  journal = {CoRR},
  year    = {2023}
}

@INPROCEEDINGS{liu2018rotate,
  author={Liu, Xialei and Masana, Marc and Herranz, Luis and Van de Weijer, Joost and López, Antonio M. and Bagdanov, Andrew D.},
  booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, 
  title={Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting}, 
  year={2018},
  volume={},
  number={},
  pages={2262-2268},
  keywords={Task analysis;Training;Training data;Neural networks;Data models;Computer vision;Standards},
  doi={10.1109/ICPR.2018.8545895}}

@inproceedings{lopezpaz2017gradient,
author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},
title = {Gradient episodic memory for continual learning},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6470–6479},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{lubana2021quadratic,
  title     = {How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation},
  author    = {Ekdeep Singh Lubana and Puja Trivedi and Danai Koutra and R. Dick},
  journal   = {COLLAS},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/90a0bea2b19957e9f9d1c920b3f5885ce2323d69}
}

@inproceedings{mallya2018piggyback,
author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
title = {Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights},
year = {2018},
isbn = {978-3-030-01224-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-01225-0_5},
doi = {10.1007/978-3-030-01225-0_5},
abstract = {This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that “piggyback” on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Our performance is agnostic to task ordering and we do not suffer from catastrophic forgetting or competition between tasks.},
booktitle = {Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part IV},
pages = {72–88},
numpages = {17},
keywords = {Incremental learning, Binary networks},
location = {Munich, Germany}
}

@inproceedings{needell2014stochastic,
author = {Needell, Deanna and Srebro, Nathan and Ward, Rachel},
title = {Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We improve a recent guarantee of Bach and Moulines on the linear convergence of SGD for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for SGD can improve convergence also in other scenarios. Our results are based on a connection between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1017–1025},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@article{panda2406lottery,
  title={Lottery ticket adaptation: Mitigating destructive interference in llms, 2024},
  author={Panda, Ashwinee and Isik, Berivan and Qi, Xiangyu and Koyejo, Sanmi and Weissman, Tsachy and Mittal, Prateek},
    year={2024},
  journal={https://arxiv. org/abs/2406.16797}
}

@inproceedings{patrick2020routing,
title	= {Routing Networks with Co-training for Continual Learning},
author	= {Mark Patrick Collier and Effrosyni Kokiopoulou and Andrea Gesmundo and Jesse Berent},
year	= {2020},
booktitle	= {ICML 2020 Workshop on Continual Learning}
}

@article{qi2021online,
  title={An online method for a class of distributionally robust optimization with non-convex objectives},
  author={Qi, Qi and Guo, Zhishuai and Xu, Yi and Jin, Rong and Yang, Tianbao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10067--10080},
  year={2021}
}

@inbook{rajasegaran2019random,
author = {Rajasegaran, Jathushan and Hayat, Munawar and Khan, Salman and Khan, Fahad Shahbaz and Shao, Ling},
title = {Random path selection for incremental learning},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Incremental life-long learning is a main challenge towards the long-standing goal of Artificial General Intelligence. In real-life settings, learning tasks arrive in a sequence and machine learning models must continually learn to increment already acquired knowledge. Existing incremental learning approaches, fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing. Since the reuse of previous paths enables forward knowledge transfer, our approach requires a considerably lower computational overhead. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity. Through extensive experiments, we demonstrate that the proposed method surpasses the state-of-the-art performance on incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1135},
numpages = {11}
}

@INPROCEEDINGS{rebuffi2017icarl,
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={iCaRL: Incremental Classifier and Representation Learning}, 
  year={2017},
  volume={},
  number={},
  pages={5533-5542},
  keywords={Training;Training data;Prototypes;Feature extraction;Memory management;Classification algorithms;Computer vision},
  doi={10.1109/CVPR.2017.587}
}

@inproceedings{riemer2019learning,
  author       = {Matthew Riemer and
                  Ignacio Cases and
                  Robert Ajemian and
                  Miao Liu and
                  Irina Rish and
                  Yuhai Tu and
                  Gerald Tesauro},
  title        = {Learning to Learn without Forgetting by Maximizing Transfer and Minimizing
                  Interference},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=B1gTShAct7},
  timestamp    = {Thu, 12 Sep 2019 14:49:23 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/RiemerCALRTT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ritter2018online,
author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
title = {Online structured laplace approximations for overcoming catastrophic forgetting},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90\% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {3742–3752},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@InProceedings{schwarz2018progress,
  title = 	 {Progress \& Compress: A scalable framework for continual learning},
  author =       {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4528--4537},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/schwarz18a/schwarz18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/schwarz18a.html},
  abstract = 	 {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress &amp; compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.}
}

@InProceedings{serra2018overcoming,
  title = 	 {Overcoming Catastrophic Forgetting with Hard Attention to the Task},
  author =       {Serra, Joan and Suris, Didac and Miron, Marius and Karatzoglou, Alexandros},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4548--4557},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/serra18a/serra18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/serra18a.html},
  abstract = 	 {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks’ information without affecting the current task’s learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.}
}

@INPROCEEDINGS{shrivastava2016training,
  author={Shrivastava, Abhinav and Gupta, Abhinav and Girshick, Ross},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Training Region-Based Object Detectors with Online Hard Example Mining}, 
  year={2016},
  volume={},
  number={},
  pages={761-769},
  keywords={Training;Detectors;Object detection;Support vector machines;Convergence;Computer vision;Computational modeling},
  doi={10.1109/CVPR.2016.89}
}

@inproceedings{silver2002task,
author = {Silver, Daniel L. and Mercer, Robert E.},
title = {The Task Rehearsal Method of Life-Long Learning: Overcoming Impoverished Data},
year = {2002},
isbn = {354043724X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The task rehearsal method (TRM) is introduced as an approach to life-long learning that uses the representation of previously learned tasks as a source of inductive bias. This inductive bias enables TRM to generate more accurate hypotheses for new tasks that have small sets of training examples. TRM has a knowledge retention phase during which the neural network representation of a successfully learned task is stored in a domain knowledge database, and a knowledge recall and learning phase during which virtual examples of stored tasks are generated from the domain knowledge. The virtual examples are rehearsed as secondary tasks in parallel with the learning of a new (primary) task using the MTL neural network algorithm, a variant of multiple task learning (MTL). The results of experiments on three domains show that TRM is effective in retaining task knowledge in a representational form and transferring that knowledge in the form of virtual examples. TRM with MTL is shown to develop more accurate hypotheses for tasks that suffer from impoverished training sets.},
booktitle = {Proceedings of the 15th Conference of the Canadian Society for Computational Studies of Intelligence on Advances in Artificial Intelligence},
pages = {90–101},
numpages = {12},
series = {AI '02}
}

@inproceedings{stich2017safe,
author = {Stich, Sebastian U. and Raj, Anant and Jaggi, Martin},
title = {Safe adaptive importance sampling},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Importance sampling has become an indispensable strategy to speed up optimization algorithms for large-scale applications. Improved adaptive variants—using importance values defined by the complete gradient information which changes during optimization—enjoy favorable theoretical properties, but are typically computationally infeasible. In this paper we propose an efficient approximation of gradient-based sampling, which is based on safe bounds on the gradient. The proposed sampling distribution is (i) provably the best sampling with respect to the given bounds, (ii) always better than uniform sampling and fixed importance sampling and (iii) can efficiently be computed—in many applications at negligible extra cost. The proposed sampling scheme is generic and can easily be integrated into existing algorithms. In particular, we show that coordinate-descent (CD) and stochastic gradient descent (SGD) can enjoy significant a speed-up under the novel scheme. The proven efficiency of the proposed sampling is verified by extensive numerical testing.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4384–4394},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{tiwari2021gcr,
author = {Tiwari, Rishabh and Killamsetty, Krishnateja and Iyer, Rishabh and Shenoy, Pradeep},
journal={CVPR},
year = {2021},
month = {11},
pages = {},
title = {GCR: Gradient Coreset Based Replay Buffer Selection For Continual Learning},
doi = {10.48550/arXiv.2111.11210}
}

@inproceedings{wang2021training,
author = {Wang, Shipeng and Li, Xiaorong and Sun, Jian and Xu, Zongben},
year = {2021},
month = {06},
pages = {184-193},
title = {Training Networks in Null Space of Feature Covariance for Continual Learning},
doi = {10.1109/CVPR46437.2021.00025}
}

@inproceedings{wang2022coscl,
author = {Wang, Liyuan and Zhang, Xingxing and Li, Qian and Zhu, Jun and Zhong, Yi},
title = {CoSCL: Cooperation of Small Continual Learners is Stronger Than a Big One},
year = {2022},
isbn = {978-3-031-19808-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-19809-0_15},
doi = {10.1007/978-3-031-19809-0_15},
abstract = {Continual learning requires incremental compatibility with a sequence of tasks. However, the design of model architecture remains an open question: In general, learning all tasks with a shared set of parameters suffers from severe interference between tasks; while learning each task with a dedicated parameter subspace is limited by scalability. In this work, we theoretically analyze the generalization errors for learning plasticity and memory stability in continual learning, which can be uniformly upper-bounded by (1) discrepancy between task distributions, (2) flatness of loss landscape and (3) cover of parameter space. Then, inspired by the robust biological learning system that processes sequential experiences with multiple parallel compartments, we propose Cooperation of Small Continual Learners (CoSCL) as a general strategy for continual learning. Specifically, we present an architecture with a fixed number of narrower sub-networks to learn all incremental tasks in parallel, which can naturally reduce the two errors through improving the three components of the upper bound. To strengthen this advantage, we encourage to cooperate these sub-networks by penalizing the difference of predictions made by their feature representations. With a fixed parameter budget, CoSCL can improve a variety of representative continual learning approaches by a large margin (e.g., up to 10.64\% on CIFAR-100-SC, 9.33\% on CIFAR-100-RS, 11.45\% on CUB-200-2011 and 6.72\% on Tiny-ImageNet) and achieve the new state-of-the-art performance. Our code is available at .},
booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVI},
pages = {254–271},
numpages = {18},
keywords = {Continual learning, Catastrophic forgetting, Ensemble model},
location = {Tel Aviv, Israel}
}

@article{wang2023incorporating,
author = {Wang, Liyuan and Zhang, Xingxing and Li, Qian and Zhang, Mingtian and Su, Hang and Zhu, Jun and Zhong, Yi},
year = {2023},
month = {11},
pages = {1-13},
title = {Incorporating neuro-inspired adaptability for continual learning in artificial intelligence},
volume = {5},
journal = {Nature Machine Intelligence},
doi = {10.1038/s42256-023-00747-w}
}

@inproceedings{wang2023orthogonal,
    title = "Orthogonal Subspace Learning for Language Model Continual Learning",
    author = "Wang, Xiao  and
      Chen, Tianze  and
      Ge, Qiming  and
      Xia, Han  and
      Bao, Rong  and
      Zheng, Rui  and
      Zhang, Qi  and
      Gui, Tao  and
      Huang, Xuanjing",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.715/",
    doi = "10.18653/v1/2023.findings-emnlp.715",
    pages = "10658--10671",
    abstract = "Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and efficient approach for continual learning in language models, effectively mitigating catastrophic forgetting while learning new tasks. Specifically, O-LoRA learns tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Our method induces only marginal additional parameter costs and requires no user data storage for replay. Experimental results on continual learning benchmarks show that our method outperforms state-of-the-art methods. Furthermore, compared to previous approaches, our method excels in preserving the generalization ability of LLMs on unseen tasks."
}

@inproceedings{worstman2020supermasks,
author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
title = {Supermasks in Superposition},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1272},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{wortsman2021robust,
  title={Robust fine-tuning of zero-shot models},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2109.01903},
  note={\url{https://arxiv.org/abs/2109.01903}},
  year={2021}
}

@article{xie2024doremi,
  title={Doremi: Optimizing data mixtures speeds up language model pretraining},
  author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy S and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{yoon2018lifelong,
title={Lifelong Learning with Dynamically Expandable Networks},
author={Jaehong Yoon and Eunho Yang and Jeongtae Lee and Sung Ju Hwang},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Sk7KsfW0-},
}

@article{zeng2019continual,
author = {Zeng, Guanxiong and Chen, Yang and Cui, Bo and Yu, Shan},
year = {2019},
month = {08},
pages = {364-372},
title = {Continual learning of context-dependent processing in neural networks},
volume = {1},
journal = {Nature Machine Intelligence},
doi = {10.1038/s42256-019-0080-x}
}

@InProceedings{zenke2017continual,
  title = 	 {Continual Learning Through Synaptic Intelligence},
  author =       {Friedemann Zenke and Ben Poole and Surya Ganguli},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3987--3995},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/zenke17a/zenke17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/zenke17a.html},
  abstract = 	 {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.}
}

@inproceedings{zhao2015stochastic,
author = {Zhao, Peilin and Zhang, Tong},
title = {Stochastic optimization with importance sampling for regularized loss minimization},
year = {2015},
publisher = {JMLR.org},
abstract = {Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization, including prox-SMD and prox-SDCA, with importance sampling, which improves the convergence rate by reducing the stochastic variance. We theoretically analyze the algorithms and empirically validate their effectiveness.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1–9},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

