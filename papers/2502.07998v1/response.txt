\section{Related works}
\vspace{-5pt}
\paragraph{Neural networks as kernel machines.} 

In certain initialization and parameterization schemes, taking the width of a neural network to infinity leads the model to learn a kernel machine, with a kernel that depends only on the initial task-independent features which do not change during training **Neal et al., "Bayesian Learning for Neural Networks"**.
This ``lazy training" regime has been extensively studied, particularly in the context of infinitely wide networks **Glorot and Bengio, "Understanding the Difficulty of Training Deep Feedforward Neural Networks"**. However, neural networks outside of the lazy limit (in the so-called ``rich regime") often perform better than their corresponding initial NTKs  and are not obviously related to kernel machines. **Chizat and Bach, "On Lazy Training in Differentiable Learning Methods"** argues that gradient descent training for any deep network corresponds to regression with a history-dependent ``path kernel", though this definition does not satisfy the standard representer theorem where coefficients are only functions of the training data as in Equation \eqref{eq:kernel_predictor_rep_thm}. In general, the learned function of a network trained with gradient flow can always be written as an integral over the history of error coefficients on training data and a time evolving NTK. In the present work, however, we are focused on when the final solution a network converges to satisfies a \textit{history independent} representer theorem as in Equation \ref{eq:kernel_predictor_rep_thm}. Some experimental and theoretical works have indicated that this is often the case, where regression with the \textit{final} NTK (computed with gradients at the final parameters) of a network provides an accurate approximation of the learned neural network function during rich training **Chizat et al., "On Lazy Training in Neural Networks"**. A complete theoretical understanding of when and why the correspondence between final NTK and final network function holds is currently lacking.

\vspace{-5pt}
\paragraph{Adaptive kernels.}

In Bayesian neural networks, several works have identified methods that describe learned network solutions beyond the lazy infinite-width description of NNGP regression. Some works pursue perturbative approximations to the posterior in powers of $1/\text{width}$ **Gal and Ghahramani, "Dropout as a Bayesian Approximation"** or alternative cumulant expansions of the predictor statistics **Wilson et al., "Deep Kernel Learning"**. Others analyze the Bayesian posterior for deep linear networks, which are more analytically tractable  and actually also capture the behavior of deep Bayesian nonlinear student-teacher learning in a particular scaling limit . Several works on Bayesian deep learning have argued in favor of a proportional limit where the samples and width are comparable **Hron et al., "Bayesian Neural Networks for Deep Learning"**. These works claim that the mean predictor under the posterior is the same as in the lazy learning limit, but that the predictor variance changes as a function of $P/N$. In our work, however, we take $N \to \infty$ first at fixed $P$ in the rich regime and find that network predictions are not well described by the lazy limit, either in theory or in experiments (see Figures \ref{fig::lambdas} \& \ref{fig::fig1}).  More recent versions of these theories explore variational approximations of the hidden neuron activation densities **Bui et al., "Variational Autoencoders"** and have begun to explore other parameterizations . These methods have been used to explain sharp transitions in the behavior of the posterior as hyperparameters, such as prior weight variance, are varied . Alternatively, some works have developed non-neural adaptive kernel algorithms, which filter information based on gradients of a kernel solution with respect to the input variables , which exhibit improvements in performance over the initial kernel and can capture interesting feature learning phenomena such as grokking **Dermatas et al., "Kernel Methods for Classification"**.