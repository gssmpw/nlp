@article{Aiudi2023LocalKR,
  title={Local kernel renormalization as a mechanism for feature learning in overparametrized convolutional neural networks},
  author={Riccardo Aiudi and Rosalba Pacelli and Alessandro Vezzani and Raffaella Burioni and Pietro Rotondo},
  journal={Nature Communications},
  year={2023},
  volume={16},
  url={https://api.semanticscholar.org/CorpusID:260125263}
}

@article{Baglioni2024,
  title = {Predictive Power of a Bayesian Effective Action for Fully Connected One Hidden Layer Neural Networks in the Proportional Limit},
  author = {Baglioni, P. and Pacelli, R. and Aiudi, R. and Di Renzo, F. and Vezzani, A. and Burioni, R. and Rotondo, P.},
  journal = {Phys. Rev. Lett.},
  volume = {133},
  issue = {2},
  pages = {027301},
  numpages = {7},
  year = {2024},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.133.027301},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.133.027301}
}

@article{Geiger_2020,
   title={Disentangling feature and lazy training in deep neural networks},
   volume={2020},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/abc4de},
   DOI={10.1088/1742-5468/abc4de},
   number={11},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
   year={2020},
   month=nov, pages={113301} }

@article{Lee_2020,
   title={Wide neural networks of any depth evolve as linear models under gradient descent
                  *},
   volume={2020},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/abc62b},
   DOI={10.1088/1742-5468/abc62b},
   number={12},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
   year={2020},
   month=dec, pages={124002} }

@article{Li_2021,
   title={Statistical Mechanics of Deep Linear Neural Networks: The Backpropagating Kernel Renormalization},
   volume={11},
   ISSN={2160-3308},
   url={http://dx.doi.org/10.1103/PhysRevX.11.031059},
   DOI={10.1103/physrevx.11.031059},
   number={3},
   journal={Physical Review X},
   publisher={American Physical Society (APS)},
   author={Li, Qianyi and Sompolinsky, Haim},
   year={2021},
   month=sep }

@article{Pacelli_2023,
   title={A statistical mechanics framework for Bayesian deep neural networks beyond the infinite-width limit},
   volume={5},
   ISSN={2522-5839},
   url={http://dx.doi.org/10.1038/s42256-023-00767-6},
   DOI={10.1038/s42256-023-00767-6},
   number={12},
   journal={Nature Machine Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Pacelli, R. and Ariosto, S. and Pastore, M. and Ginelli, F. and Gherardi, M. and Rotondo, P.},
   year={2023},
   month=dec, pages={1497–1507} }

@article{Zavatone_Veth_2022,
   title={Asymptotics of representation learning in finite Bayesian neural networks*},
   volume={2022},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/ac98a6},
   DOI={10.1088/1742-5468/ac98a6},
   number={11},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Zavatone-Veth, Jacob A and Canatar, Abdulkadir and Ruben, Benjamin S and Pehlevan, Cengiz},
   year={2022},
   month=nov, pages={114008} }

@misc{aitchison2020biggerbetterfiniteinfinite,
      title={Why bigger is not always better: on finite and infinite neural networks}, 
      author={Laurence Aitchison},
      year={2020},
      eprint={1910.08013},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1910.08013}, 
}

@misc{atanasov2021neuralnetworkskernellearners,
      title={Neural Networks as Kernel Learners: The Silent Alignment Effect}, 
      author={Alexander Atanasov and Blake Bordelon and Cengiz Pehlevan},
      year={2021},
      eprint={2111.00034},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2111.00034}, 
}

@misc{bassetti2024featurelearningfinitewidthbayesian,
      title={Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers}, 
      author={Federico Bassetti and Marco Gherardi and Alessandro Ingrosso and Mauro Pastore and Pietro Rotondo},
      year={2024},
      eprint={2406.03260},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2406.03260}, 
}

@misc{chizat2020lazytrainingdifferentiableprogramming,
      title={On Lazy Training in Differentiable Programming}, 
      author={Lenaic Chizat and Edouard Oyallon and Francis Bach},
      year={2020},
      eprint={1812.07956},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1812.07956}, 
}

@inproceedings{cui2023bayes,
  title={Bayes-optimal learning of deep random networks of extensive-width},
  author={Cui, Hugo and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={International Conference on Machine Learning},
  pages={6468--6521},
  year={2023},
  organization={PMLR}
}

@article{domingos2020every,
  title={Every model learned by gradient descent is approximately a kernel machine},
  author={Domingos, Pedro},
  journal={arXiv preprint arXiv:2012.00152},
  year={2020}
}

@article{hanin2023bayesian,
  title={Bayesian interpolation with deep linear networks},
  author={Hanin, Boris and Zlokapa, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={23},
  pages={e2301345120},
  year={2023},
  publisher={National Acad Sciences}
}

@misc{jacot2020neuraltangentkernelconvergence,
      title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks}, 
      author={Arthur Jacot and Franck Gabriel and Clément Hongler},
      year={2020},
      eprint={1806.07572},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1806.07572}, 
}

@misc{lee2018deepneuralnetworksgaussian,
      title={Deep Neural Networks as Gaussian Processes}, 
      author={Jaehoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein},
      year={2018},
      eprint={1711.00165},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1711.00165}, 
}

@article{mallinar2024emergence,
  title={Emergence in non-neural models: grokking modular arithmetic via average gradient outer product},
  author={Mallinar, Neil and Beaglehole, Daniel and Zhu, Libin and Radhakrishnan, Adityanarayanan and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2407.20199},
  year={2024}
}

@article{naveh2021self,
  title={A self consistent theory of gaussian processes captures feature learning effects in finite cnns},
  author={Naveh, Gadi and Ringel, Zohar},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21352--21364},
  year={2021}
}

@article{radhakrishnan2022mechanism,
  title={Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features},
  author={Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2212.13881},
  year={2022}
}

@book{roberts2022principles,
  title={The principles of deep learning theory},
  author={Roberts, Daniel A and Yaida, Sho and Hanin, Boris},
  volume={46},
  year={2022},
  publisher={Cambridge University Press Cambridge, MA, USA}
}

@article{seroussi2023separation,
  title={Separation of scales and a thermodynamic description of feature learning in some cnns},
  author={Seroussi, Inbar and Naveh, Gadi and Ringel, Zohar},
  journal={Nature Communications},
  volume={14},
  number={1},
  pages={908},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@misc{vanmeegen2024codingschemesneuralnetworks,
      title={Coding schemes in neural networks learning classification tasks}, 
      author={Alexander van Meegen and Haim Sompolinsky},
      year={2024},
      eprint={2406.16689},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.16689}, 
}

@article{vyas2022limitations,
  title={Limitations of the ntk for understanding generalization in deep learning},
  author={Vyas, Nikhil and Bansal, Yamini and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2206.10012},
  year={2022}
}

@misc{wei2022toyrandommatrixmodels,
      title={More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize}, 
      author={Alexander Wei and Wei Hu and Jacob Steinhardt},
      year={2022},
      eprint={2203.06176},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.06176}, 
}

@inproceedings{yang2023theory,
  title={A theory of representation learning gives a deep generalisation of kernel methods},
  author={Yang, Adam X and Robeyns, Maxime and Milsom, Edward and Anson, Ben and Schoots, Nandi and Aitchison, Laurence},
  booktitle={International Conference on Machine Learning},
  pages={39380--39415},
  year={2023},
  organization={PMLR}
}

@article{zavatone2022contrasting,
  title={Contrasting random and learned features in deep Bayesian linear regression},
  author={Zavatone-Veth, Jacob A and Tong, William L and Pehlevan, Cengiz},
  journal={Physical Review E},
  volume={105},
  number={6},
  pages={064118},
  year={2022},
  publisher={APS}
}

