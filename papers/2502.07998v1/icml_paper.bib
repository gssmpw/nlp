@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{naveh2021self,
  title={A self consistent theory of gaussian processes captures feature learning effects in finite cnns},
  author={Naveh, Gadi and Ringel, Zohar},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21352--21364},
  year={2021}
}

@article{seroussi2023separation,
  title={Separation of scales and a thermodynamic description of feature learning in some cnns},
  author={Seroussi, Inbar and Naveh, Gadi and Ringel, Zohar},
  journal={Nature Communications},
  volume={14},
  number={1},
  pages={908},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{mallinar2024emergence,
  title={Emergence in non-neural models: grokking modular arithmetic via average gradient outer product},
  author={Mallinar, Neil and Beaglehole, Daniel and Zhu, Libin and Radhakrishnan, Adityanarayanan and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2407.20199},
  year={2024}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{rotskoff2018trainability,
  title={Trainability and accuracy of artificial neural networks: An interacting particle system approach},
  author={Rotskoff, Grant and Vanden-Eijnden, Eric},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={9},
  pages={1889--1935},
  year={2022},
  publisher={Wiley Online Library}
}

@article{lewkowycz2020training,
  title={On the training dynamics of deep networks with $ L\_2 $ regularization},
  author={Lewkowycz, Aitor and Gur-Ari, Guy},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4790--4799},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@book{roberts2022principles,
  title={The principles of deep learning theory},
  author={Roberts, Daniel A and Yaida, Sho and Hanin, Boris},
  volume={46},
  year={2022},
  publisher={Cambridge University Press Cambridge, MA, USA}
}

@article{vyas2022limitations,
  title={Limitations of the ntk for understanding generalization in deep learning},
  author={Vyas, Nikhil and Bansal, Yamini and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2206.10012},
  year={2022}
}

@article{hanin2023bayesian,
  title={Bayesian interpolation with deep linear networks},
  author={Hanin, Boris and Zlokapa, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={23},
  pages={e2301345120},
  year={2023},
  publisher={National Acad Sciences}
}

@inproceedings{yang2023theory,
  title={A theory of representation learning gives a deep generalisation of kernel methods},
  author={Yang, Adam X and Robeyns, Maxime and Milsom, Edward and Anson, Ben and Schoots, Nandi and Aitchison, Laurence},
  booktitle={International Conference on Machine Learning},
  pages={39380--39415},
  year={2023},
  organization={PMLR}
}

@inproceedings{
rubin2024grokking,
title={Grokking as a First Order Phase Transition in Two Layer Networks},
author={Noa Rubin and Inbar Seroussi and Zohar Ringel},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=3ROGsTX3IR}
}

@article{radhakrishnan2022mechanism,
  title={Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features},
  author={Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2212.13881},
  year={2022}
}

@inproceedings{
rubin2024a,
title={A Unified Approach to Feature Learning in Bayesian Neural Networks},
author={Noa Rubin and Zohar Ringel and Inbar Seroussi and Moritz Helias},
booktitle={High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning},
year={2024},
url={https://openreview.net/forum?id=ZmOSJ2MV2R}
}

@misc{wei2022toyrandommatrixmodels,
      title={More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize}, 
      author={Alexander Wei and Wei Hu and Jacob Steinhardt},
      year={2022},
      eprint={2203.06176},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.06176}, 
}z

@article{domingos2020every,
  title={Every model learned by gradient descent is approximately a kernel machine},
  author={Domingos, Pedro},
  journal={arXiv preprint arXiv:2012.00152},
  year={2020}
}

@article{lee2020finite,
  title={Finite versus infinite neural networks: an empirical study},
  author={Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15156--15172},
  year={2020}
}

@article{cho2009kernel,
  title={Kernel methods for deep learning},
  author={Cho, Youngmin and Saul, Lawrence},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}

@article{lecun:hal-04206682,
  TITLE = {{Deep learning}},
  AUTHOR = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  URL = {https://hal.science/hal-04206682},
  JOURNAL = {{Nature}},
  PUBLISHER = {{Nature Publishing Group}},
  VOLUME = {521},
  NUMBER = {7553},
  PAGES = {436-444},
  YEAR = {2015},
  MONTH = May,
  DOI = {10.1038/nature14539},
  PDF = {https://hal.science/hal-04206682v1/file/Lecun2015.pdf},
  HAL_ID = {hal-04206682},
  HAL_VERSION = {v1},
}

@misc{lee2018deepneuralnetworksgaussian,
      title={Deep Neural Networks as Gaussian Processes}, 
      author={Jaehoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein},
      year={2018},
      eprint={1711.00165},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1711.00165}, 
}
@misc{matthews2018gaussianprocessbehaviourwide,
      title={Gaussian Process Behaviour in Wide Deep Neural Networks}, 
      author={Alexander G. de G. Matthews and Mark Rowland and Jiri Hron and Richard E. Turner and Zoubin Ghahramani},
      year={2018},
      eprint={1804.11271},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1804.11271}, 
}

@misc{jacot2020neuraltangentkernelconvergence,
      title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks}, 
      author={Arthur Jacot and Franck Gabriel and Cl√©ment Hongler},
      year={2020},
      eprint={1806.07572},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1806.07572}, 
}

@inproceedings{cho,
 author = {Cho, Youngmin and Saul, Lawrence},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Kernel Methods for Deep Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf},
 volume = {22},
 year = {2009}
}


@misc{arora2019exactcomputationinfinitelywide,
      title={On Exact Computation with an Infinitely Wide Neural Net}, 
      author={Sanjeev Arora and Simon S. Du and Wei Hu and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang},
      year={2019},
      eprint={1904.11955},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.11955}, 
}

@misc{arora2019convergenceanalysisgradientdescent,
      title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks}, 
      author={Sanjeev Arora and Nadav Cohen and Noah Golowich and Wei Hu},
      year={2019},
      eprint={1810.02281},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1810.02281}, 
}

@article{Li_2021,
   title={Statistical Mechanics of Deep Linear Neural Networks: The Backpropagating Kernel Renormalization},
   volume={11},
   ISSN={2160-3308},
   url={http://dx.doi.org/10.1103/PhysRevX.11.031059},
   DOI={10.1103/physrevx.11.031059},
   number={3},
   journal={Physical Review X},
   publisher={American Physical Society (APS)},
   author={Li, Qianyi and Sompolinsky, Haim},
   year={2021},
   month=sep }


@misc{atanasov2021neuralnetworkskernellearners,
      title={Neural Networks as Kernel Learners: The Silent Alignment Effect}, 
      author={Alexander Atanasov and Blake Bordelon and Cengiz Pehlevan},
      year={2021},
      eprint={2111.00034},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2111.00034}, 
}

@misc{avidan2024connectingntknngpunified,
      title={Connecting NTK and NNGP: A Unified Theoretical Framework for Wide Neural Network Learning Dynamics}, 
      author={Yehonatan Avidan and Qianyi Li and Haim Sompolinsky},
      year={2024},
      eprint={2309.04522},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.04522}, 
}

@article{Geiger_2020,
   title={Disentangling feature and lazy training in deep neural networks},
   volume={2020},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/abc4de},
   DOI={10.1088/1742-5468/abc4de},
   number={11},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
   year={2020},
   month=nov, pages={113301} }
@misc{bordelon2022selfconsistentdynamicalfieldtheory,
      title={Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks}, 
      author={Blake Bordelon and Cengiz Pehlevan},
      year={2022},
      eprint={2205.09653},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.09653}, 
}

@misc{yang2022featurelearninginfinitewidthneural,
      title={Feature Learning in Infinite-Width Neural Networks}, 
      author={Greg Yang and Edward J. Hu},
      year={2022},
      eprint={2011.14522},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.14522}, 
}

@misc{yang2022tensorprogramsvtuning,
      title={Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer}, 
      author={Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki and Weizhu Chen and Jianfeng Gao},
      year={2022},
      eprint={2203.03466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.03466}, 
}

@article{Lee_2020,
   title={Wide neural networks of any depth evolve as linear models under gradient descent
                  *},
   volume={2020},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/abc62b},
   DOI={10.1088/1742-5468/abc62b},
   number={12},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
   year={2020},
   month=dec, pages={124002} }

@misc{bordelon2021spectrumdependentlearningcurves,
      title={Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks}, 
      author={Blake Bordelon and Abdulkadir Canatar and Cengiz Pehlevan},
      year={2021},
      eprint={2002.02561},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.02561}, 
}

@misc{chizat2020lazytrainingdifferentiableprogramming,
      title={On Lazy Training in Differentiable Programming}, 
      author={Lenaic Chizat and Edouard Oyallon and Francis Bach},
      year={2020},
      eprint={1812.07956},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1812.07956}, 
}

@phdthesis{neal,
author = {Neal, Radford M.},
advisor = {Hinton, Geoffrey},
title = {Bayesian learning for neural networks},
year = {1995},
isbn = {0612026760},
publisher = {University of Toronto},
address = {CAN},
note = {AAINN02676}
}

@misc{vyas2023featurelearningnetworksconsistentwidths,
      title={Feature-Learning Networks Are Consistent Across Widths At Realistic Scales}, 
      author={Nikhil Vyas and Alexander Atanasov and Blake Bordelon and Depen Morwani and Sabarish Sainathan and Cengiz Pehlevan},
      year={2023},
      eprint={2305.18411},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18411}, 
}

@misc{bordelon2024infinitelimitsmultiheadtransformer,
      title={Infinite Limits of Multi-head Transformer Dynamics}, 
      author={Blake Bordelon and Hamza Tahir Chaudhry and Cengiz Pehlevan},
      year={2024},
      eprint={2405.15712},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2405.15712}, 
}

@misc{sohldickstein2020infinitewidthlimitneural,
      title={On the infinite width limit of neural networks with a standard parameterization}, 
      author={Jascha Sohl-Dickstein and Roman Novak and Samuel S. Schoenholz and Jaehoon Lee},
      year={2020},
      eprint={2001.07301},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.07301}, 
}

@article{Pacelli_2023,
   title={A statistical mechanics framework for Bayesian deep neural networks beyond the infinite-width limit},
   volume={5},
   ISSN={2522-5839},
   url={http://dx.doi.org/10.1038/s42256-023-00767-6},
   DOI={10.1038/s42256-023-00767-6},
   number={12},
   journal={Nature Machine Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Pacelli, R. and Ariosto, S. and Pastore, M. and Ginelli, F. and Gherardi, M. and Rotondo, P.},
   year={2023},
   month=dec, pages={1497‚Äì1507} }

@misc{vanmeegen2024codingschemesneuralnetworks,
      title={Coding schemes in neural networks learning classification tasks}, 
      author={Alexander van Meegen and Haim Sompolinsky},
      year={2024},
      eprint={2406.16689},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.16689}, 
}

@misc{bordelon2024featurelearningimproveneural,
      title={How Feature Learning Can Improve Neural Scaling Laws}, 
      author={Blake Bordelon and Alexander Atanasov and Cengiz Pehlevan},
      year={2024},
      eprint={2409.17858},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2409.17858}, 
}

@book{MacKay,
author = {MacKay, David J. C.},
title = {Information Theory, Inference \& Learning Algorithms},
year = {2002},
isbn = {0521642981},
publisher = {Cambridge University Press},
address = {USA}
}

@article{annurev:/content/journals/10.1146/annurev-conmatphys-031119-050745,
   author = "Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S. and Sohl-Dickstein, Jascha and Ganguli, Surya",
   title = "Statistical Mechanics of Deep Learning", 
   journal= "Annual Review of Condensed Matter Physics",
   year = "2020",
   volume = "11",
   number = "Volume 11, 2020",
   pages = "501-528",
   doi = "https://doi.org/10.1146/annurev-conmatphys-031119-050745",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-conmatphys-031119-050745",
   publisher = "Annual Reviews",
   issn = "1947-5462",
   type = "Journal Article",
   keywords = "chaos",
   keywords = "interacting particle systems",
   keywords = "random matrix theory",
   keywords = "nonequilibrium statistical mechanics",
   keywords = "machine learning",
   keywords = "jamming",
   keywords = "dynamical phase transitions",
   keywords = "neural networks",
   keywords = "spin glasses",
   abstract = "The recent striking success of deep neural networks in machine learning raises profound questions about the theoretical principles underlying their success. For example, what can such deep networks compute? How can we train them? How does information propagate through them? Why can they generalize? And how can we teach them to imagine? We review recent work in which methods of physical analysis rooted in statistical mechanics have begun to provide conceptual insights into these questions. These insights yield connections between deep learning and diverse physical and mathematical topics, including random landscapes, spin glasses, jamming, dynamical phase transitions, chaos, Riemannian geometry, random matrix theory, free probability, and nonequilibrium statistical mechanics. Indeed, the fields of statistical mechanics and machine learning have long enjoyed a rich history of strongly coupled interactions, and recent advances at the intersection of statistical mechanics and deep learning suggest these interactions will only deepen going forward.",
  }


@article{Zavatone_Veth_2022,
   title={Asymptotics of representation learning in finite Bayesian neural networks*},
   volume={2022},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/ac98a6},
   DOI={10.1088/1742-5468/ac98a6},
   number={11},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Zavatone-Veth, Jacob A and Canatar, Abdulkadir and Ruben, Benjamin S and Pehlevan, Cengiz},
   year={2022},
   month=nov, pages={114008} }

@misc{aitchison2020biggerbetterfiniteinfinite,
      title={Why bigger is not always better: on finite and infinite neural networks}, 
      author={Laurence Aitchison},
      year={2020},
      eprint={1910.08013},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1910.08013}, 
}

@misc{saxe2014exactsolutionsnonlineardynamics,
      title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}, 
      author={Andrew M. Saxe and James L. McClelland and Surya Ganguli},
      year={2014},
      eprint={1312.6120},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1312.6120}, 
}

@misc{jacot2022saddletosaddledynamicsdeeplinear,
      title={Saddle-to-Saddle Dynamics in Deep Linear Networks: Small Initialization Training, Symmetry, and Sparsity}, 
      author={Arthur Jacot and Fran√ßois Ged and Berfin ≈ûim≈üek and Cl√©ment Hongler and Franck Gabriel},
      year={2022},
      eprint={2106.15933},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2106.15933}, 
}

@misc{advani2017highdimensionaldynamicsgeneralizationerror,
      title={High-dimensional dynamics of generalization error in neural networks}, 
      author={Madhu S. Advani and Andrew M. Saxe},
      year={2017},
      eprint={1710.03667},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1710.03667}, 
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@inproceedings{welling_2011,
author = {Welling, Max and Teh, Yee Whye},
title = {Bayesian learning via stochastic gradient langevin dynamics},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {681‚Äì688},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@misc{bassetti2024featurelearningfinitewidthbayesian,
      title={Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers}, 
      author={Federico Bassetti and Marco Gherardi and Alessandro Ingrosso and Mauro Pastore and Pietro Rotondo},
      year={2024},
      eprint={2406.03260},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2406.03260}, 
}

@article{Aiudi2023LocalKR,
  title={Local kernel renormalization as a mechanism for feature learning in overparametrized convolutional neural networks},
  author={Riccardo Aiudi and Rosalba Pacelli and Alessandro Vezzani and Raffaella Burioni and Pietro Rotondo},
  journal={Nature Communications},
  year={2023},
  volume={16},
  url={https://api.semanticscholar.org/CorpusID:260125263}
}

@article{Baglioni2024,
  title = {Predictive Power of a Bayesian Effective Action for Fully Connected One Hidden Layer Neural Networks in the Proportional Limit},
  author = {Baglioni, P. and Pacelli, R. and Aiudi, R. and Di Renzo, F. and Vezzani, A. and Burioni, R. and Rotondo, P.},
  journal = {Phys. Rev. Lett.},
  volume = {133},
  issue = {2},
  pages = {027301},
  numpages = {7},
  year = {2024},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.133.027301},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.133.027301}
}


@article{zavatone2022contrasting,
  title={Contrasting random and learned features in deep Bayesian linear regression},
  author={Zavatone-Veth, Jacob A and Tong, William L and Pehlevan, Cengiz},
  journal={Physical Review E},
  volume={105},
  number={6},
  pages={064118},
  year={2022},
  publisher={APS}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/jax-ml/jax},
  version = {0.3.13},
  year = {2018},
}


@inproceedings{cui2023bayes,
  title={Bayes-optimal learning of deep random networks of extensive-width},
  author={Cui, Hugo and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={International Conference on Machine Learning},
  pages={6468--6521},
  year={2023},
  organization={PMLR}
}


@article{bordelon2024dynamics,
  title={Dynamics of finite width kernel and prediction fluctuations in mean field neural networks},
  author={Bordelon, Blake and Pehlevan, Cengiz},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}



@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}

@book{kardar2007statistical,
  title={Statistical physics of particles},
  author={Kardar, Mehran},
  year={2007},
  publisher={Cambridge University Press}
}

@book{Rasmussen2006Gaussian,
  added-at = {2019-03-04T22:26:50.000+0100},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  biburl = {https://www.bibsonomy.org/bibtex/21bf82350cc051367b8c7828a86c9dc0a/rwhender},
  crossref = {026218253X},
  file = {:gaussian process book.pdf:PDF},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {1bf82350cc051367b8c7828a86c9dc0a},
  keywords = {imported},
  owner = {wesley},
  publisher = {The MIT Press},
  timestamp = {2019-03-04T22:29:38.000+0100},
  title = {Gaussian Processes for Machine Learning},
  year = 2006
}

@book{Scholkopf,
author = {Scholkopf, Bernhard and Smola, Alexander J.},
title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
year = {2001},
isbn = {0262194759},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {From the Publisher:In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.   Learning with Kernels  provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.}
}

@article{
Mei2018,
author = {Song Mei  and Andrea Montanari  and Phan-Minh Nguyen },
title = {A mean field view of the landscape of two-layer neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {33},
pages = {E7665-E7671},
year = {2018},
doi = {10.1073/pnas.1806579115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1806579115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1806579115},
abstract = {Multilayer neural networks have proven extremely successful in a variety of tasks, from image classification to robotics. However, the reasons for this practical success and its precise domain of applicability are unknown. Learning a neural network from data requires solving a complex optimization problem with millions of variables. This is done by stochastic gradient descent (SGD) algorithms. We study the case of two-layer networks and derive a compact description of the SGD dynamics in terms of a limiting partial differential equation. Among other consequences, this shows that SGD dynamics does not become more complex when the network size increases. Multilayer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires optimizing a nonconvex high-dimensional objective (risk function), a problem that is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the former case, does this happen because local minima are absent or because SGD somehow avoids them? In the latter, why do local minima reached by SGD have good generalization properties? In this paper, we consider a simple case, namely two-layer neural networks, and prove that‚Äîin a suitable scaling limit‚ÄîSGD dynamics is captured by a certain nonlinear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows for ‚Äúaveraging out‚Äù some of the complexities of the landscape of neural networks and can be used to prove a general convergence result for noisy SGD.}}



@article{dmft,
  title = {Statistical Dynamics of Classical Systems},
  author = {Martin, P. C. and Siggia, E. D. and Rose, H. A.},
  journal = {Phys. Rev. A},
  volume = {8},
  issue = {1},
  pages = {423--437},
  numpages = {0},
  year = {1973},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.8.423},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.8.423}
}

@article{deDominicis,
  title = {Dynamics as a substitute for replicas in systems with quenched random impurities},
  author = {De Dominicis, C.},
  journal = {Phys. Rev. B},
  volume = {18},
  issue = {9},
  pages = {4913--4919},
  numpages = {0},
  year = {1978},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.18.4913},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.18.4913}
}

@article{zippelius,
  title = {Dynamic Theory of the Spin-Glass Phase},
  author = {Sompolinsky, H. and Zippelius, Annette},
  journal = {Phys. Rev. Lett.},
  volume = {47},
  issue = {5},
  pages = {359--362},
  numpages = {0},
  year = {1981},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.47.359},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.47.359}
}

@article{Arous1995LargeDF,
  title={Large deviations for Langevin spin glass dynamics},
  author={G{\'e}rard Ben Arous and Alice Guionnet},
  journal={Probability Theory and Related Fields},
  year={1995},
  volume={102},
  pages={455-509},
  url={https://api.semanticscholar.org/CorpusID:189918256}
}

@misc{arous2004cugliandolokurchanequationsdynamicsspinglasses,
      title={Cugliandolo-Kurchan equations for dynamics of Spin-Glasses}, 
      author={Gerard Ben Arous and Amir Dembo and Alice Guionnet},
      year={2004},
      eprint={math/0409273},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url={https://arxiv.org/abs/math/0409273}, 
}


@article{Mignacco_2021,
   title={Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem},
   volume={2},
   ISSN={2632-2153},
   url={http://dx.doi.org/10.1088/2632-2153/ac0615},
   DOI={10.1088/2632-2153/ac0615},
   number={3},
   journal={Machine Learning: Science and Technology},
   publisher={IOP Publishing},
   author={Mignacco, Francesca and Urbani, Pierfrancesco and Zdeborov√°, Lenka},
   year={2021},
   month=jul, pages={035029} }


@article{Agoritsas_2018,
   title={Out-of-equilibrium dynamical mean-field equations for the perceptron model},
   volume={51},
   ISSN={1751-8121},
   url={http://dx.doi.org/10.1088/1751-8121/aaa68d},
   DOI={10.1088/1751-8121/aaa68d},
   number={8},
   journal={Journal of Physics A: Mathematical and Theoretical},
   publisher={IOP Publishing},
   author={Agoritsas, Elisabeth and Biroli, Giulio and Urbani, Pierfrancesco and Zamponi, Francesco},
   year={2018},
   month=jan, pages={085002} }

@article{Sarao_Mannelli_2020,
   title={Marvels and Pitfalls of the Langevin Algorithm in Noisy High-Dimensional Inference},
   volume={10},
   ISSN={2160-3308},
   url={http://dx.doi.org/10.1103/PhysRevX.10.011057},
   DOI={10.1103/physrevx.10.011057},
   number={1},
   journal={Physical Review X},
   publisher={American Physical Society (APS)},
   author={Sarao Mannelli, Stefano and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov√°, Lenka},
   year={2020},
   month=mar }


@article{Mignacco_2021_2,
   title={Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification*},
   volume={2021},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/ac3a80},
   DOI={10.1088/1742-5468/ac3a80},
   number={12},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Mignacco, Francesca and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov√°, Lenka},
   year={2021},
   month=dec, pages={124008} }

@misc{novak2018sensitivitygeneralizationneuralnetworks,
      title={Sensitivity and Generalization in Neural Networks: an Empirical Study}, 
      author={Roman Novak and Yasaman Bahri and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-Dickstein},
      year={2018},
      eprint={1802.08760},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.08760}, 
}

@article{Bahri_2024,
   title={Explaining neural scaling laws},
   volume={121},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.2311878121},
   DOI={10.1073/pnas.2311878121},
   number={27},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
   year={2024},
   month=jun }
