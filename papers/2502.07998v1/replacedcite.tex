\section{Related works}
\vspace{-5pt}
\paragraph{Neural networks as kernel machines.} 

In certain initialization and parameterization schemes, taking the width of a neural network to infinity leads the model to learn a kernel machine, with a kernel that depends only on the initial task-independent features which do not change during training ____.
This ``lazy training" regime has been extensively studied, particularly in the context of infinitely wide networks ____. However, neural networks outside of the lazy limit (in the so-called ``rich regime") often perform better than their corresponding initial NTKs ____ and are not obviously related to kernel machines. ____ argues that gradient descent training for any deep network corresponds to regression with a history-dependent ``path kernel", though this definition does not satisfy the standard representer theorem where coefficients are only functions of the training data as in Equation \eqref{eq:kernel_predictor_rep_thm}. In general, the learned function of a network trained with gradient flow can always be written as an integral over the history of error coefficients on training data and a time evolving NTK. In the present work, however, we are focused on when the final solution a network converges to satisfies a \textit{history independent} representer theorem as in Equation \ref{eq:kernel_predictor_rep_thm}. Some experimental and theoretical works have indicated that this is often the case, where regression with the \textit{final} NTK (computed with gradients at the final parameters) of a network provides an accurate approximation of the learned neural network function during rich training ____. A complete theoretical understanding of when and why the correspondence between final NTK and final network function holds is currently lacking.

\vspace{-5pt}
\paragraph{Adaptive kernels.}

In Bayesian neural networks, several works have identified methods that describe learned network solutions beyond the lazy infinite-width description of NNGP regression. Some works pursue perturbative approximations to the posterior in powers of $1/\text{width}$ ____ or alternative cumulant expansions of the predictor statistics ____. Others analyze the Bayesian posterior for deep linear networks, which are more analytically tractable ____ and actually also capture the behavior of deep Bayesian nonlinear student-teacher learning in a particular scaling limit ____. Several works on Bayesian deep learning have argued in favor of a proportional limit where the samples and width are comparable ____. These works claim that the mean predictor under the posterior is the same as in the lazy learning limit, but that the predictor variance changes as a function of $P/N$. In our work, however, we take $N \to \infty$ first at fixed $P$ in the rich regime and find that network predictions are not well described by the lazy limit, either in theory or in experiments (see Figures \ref{fig::lambdas} \& \ref{fig::fig1}).  More recent versions of these theories explore variational approximations of the hidden neuron activation densities ____ and have begun to explore other parameterizations ____. These methods have been used to explain sharp transitions in the behavior of the posterior as hyperparameters, such as prior weight variance, are varied ____. Alternatively, some works have developed non-neural adaptive kernel algorithms, which filter information based on gradients of a kernel solution with respect to the input variables ____, which exhibit improvements in performance over the initial kernel and can capture interesting feature learning phenomena such as grokking ____. 

\vspace{-10pt}