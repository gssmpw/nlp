\section{Related works}
\vspace{-5pt}
\paragraph{Neural networks as kernel machines.} 

In certain initialization and parameterization schemes, taking the width of a neural network to infinity leads the model to learn a kernel machine, with a kernel that depends only on the initial task-independent features which do not change during training \cite{jacot2020neuraltangentkernelconvergence, lee2018deepneuralnetworksgaussian}.
This ``lazy training" regime has been extensively studied, particularly in the context of infinitely wide networks \cite{chizat2020lazytrainingdifferentiableprogramming, jacot2020neuraltangentkernelconvergence, lee2018deepneuralnetworksgaussian, Lee_2020}. However, neural networks outside of the lazy limit (in the so-called ``rich regime") often perform better than their corresponding initial NTKs \cite{vyas2022limitations} and are not obviously related to kernel machines. \citet{domingos2020every} argues that gradient descent training for any deep network corresponds to regression with a history-dependent ``path kernel", though this definition does not satisfy the standard representer theorem where coefficients are only functions of the training data as in Equation \eqref{eq:kernel_predictor_rep_thm}. In general, the learned function of a network trained with gradient flow can always be written as an integral over the history of error coefficients on training data and a time evolving NTK. In the present work, however, we are focused on when the final solution a network converges to satisfies a \textit{history independent} representer theorem as in Equation \ref{eq:kernel_predictor_rep_thm}. Some experimental and theoretical works have indicated that this is often the case, where regression with the \textit{final} NTK (computed with gradients at the final parameters) of a network provides an accurate approximation of the learned neural network function during rich training \cite{Geiger_2020, atanasov2021neuralnetworkskernellearners, wei2022toyrandommatrixmodels}. A complete theoretical understanding of when and why the correspondence between final NTK and final network function holds is currently lacking.

\vspace{-5pt}
\paragraph{Adaptive kernels.}

In Bayesian neural networks, several works have identified methods that describe learned network solutions beyond the lazy infinite-width description of NNGP regression. Some works pursue perturbative approximations to the posterior in powers of $1/\text{width}$ \cite{Zavatone_Veth_2022, roberts2022principles} or alternative cumulant expansions of the predictor statistics \cite{naveh2021self}. Others analyze the Bayesian posterior for deep linear networks, which are more analytically tractable \cite{aitchison2020biggerbetterfiniteinfinite, hanin2023bayesian, zavatone2022contrasting, bassetti2024featurelearningfinitewidthbayesian} and actually also capture the behavior of deep Bayesian nonlinear student-teacher learning in a particular scaling limit \cite{cui2023bayes}. Several works on Bayesian deep learning have argued in favor of a proportional limit where the samples and width are comparable \cite{Li_2021,Pacelli_2023,Aiudi2023LocalKR, vanmeegen2024codingschemesneuralnetworks,Baglioni2024}. These works claim that the mean predictor under the posterior is the same as in the lazy learning limit, but that the predictor variance changes as a function of $P/N$. In our work, however, we take $N \to \infty$ first at fixed $P$ in the rich regime and find that network predictions are not well described by the lazy limit, either in theory or in experiments (see Figures \ref{fig::lambdas} \& \ref{fig::fig1}).  More recent versions of these theories explore variational approximations of the hidden neuron activation densities \cite{seroussi2023separation, yang2023theory} and have begun to explore other parameterizations \cite{rubin2024a}. These methods have been used to explain sharp transitions in the behavior of the posterior as hyperparameters, such as prior weight variance, are varied \cite{rubin2024grokking}. Alternatively, some works have developed non-neural adaptive kernel algorithms, which filter information based on gradients of a kernel solution with respect to the input variables \cite{radhakrishnan2022mechanism}, which exhibit improvements in performance over the initial kernel and can capture interesting feature learning phenomena such as grokking \cite{mallinar2024emergence}. 

\vspace{-10pt}