%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%\PassOptionsToPackage{ruled,vlined}{algorithm2e}
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amsfonts,bm}
%\usepackage{algorithmic}

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\bb}[1]{\textcolor{purple}{([BB]: #1)}}
\newcommand{\cl}[1]{\textcolor{violet}{([CL]: #1)}}
\newcommand{\cp}[1]{\textcolor{red}{([CP]: #1)}}

% Use the accepted option so that author names appear.
\usepackage[accepted]{icml2025}

% Immediately override the conference notice to create a preprint version.
\makeatletter
\renewcommand{\ICML@appearing}{}
\renewcommand{\Notice@String}{\textit{Preprint}}
\makeatother

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{braket}
\usepackage{bm}

\def\x{\bm x}
\def\X{\bm X}
\def\h{\bm h}
\def\v{\bm v}
\def\y{\bm y}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development.
\usepackage[textsize=tiny]{todonotes}
%\usepackage{algorithm2e}


% The short running title for the header:
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Adaptive kernel predictors from feature-learning infinite limits of neural networks}

% List of affiliations:
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Clarissa Lauditi}{seas}
\icmlauthor{Blake Bordelon}{seas,centerbs,kempner}
\icmlauthor{Cengiz Pehlevan}{seas,centerbs,kempner}
\end{icmlauthorlist}

\icmlaffiliation{seas}{John A. Paulson School of Engineering and Applied Sciences, Harvard University}
\icmlaffiliation{centerbs}{Center for Brain Sciences}
\icmlaffiliation{kempner}{Kempner Institute}

\icmlcorrespondingauthor{Clarissa Lauditi}{clauditi@g.harvard.edu}
\icmlcorrespondingauthor{Blake Bordelon}{blake\_bordelon@g.harvard.edu}
\icmlcorrespondingauthor{Cengiz Pehlevan}{cpehlevan@seas.harvard.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% This prints the affiliations and the overridden notice.
\printAffiliationsAndNotice{}


\begin{abstract}
Previous influential work showed that infinite width limits of neural networks in the lazy training regime are described by kernel machines. Here, we show that neural networks trained in the rich, feature-learning infinite-width regime, across two different settings, are also described by kernel machines, but with data-dependent kernels. For both cases, we provide explicit expressions for the kernel predictors and prescriptions for their numerical calculation. To derive the first predictor, we study the large-width limit of feature-learning Bayesian networks, showing how feature learning leads to task-relevant adaptation of layer kernels and preactivation densities. The saddle point equations governing this limit result in a min-max optimization problem that defines the kernel predictor. To derive the second predictor, we study gradient flow training of randomly initialized networks trained with weight decay in the infinite-width limit using dynamical mean field theory (DMFT). The fixed-point equations of the resulting DMFT define the task-adapted internal representations and the kernel predictor. We compare our kernel predictors to kernels derived from the lazy regime and demonstrate that our adaptive kernels achieve lower test loss on benchmark datasets.  
%Understanding what types of solutions neural networks converge to after training is pivotal for both theoretical insights and practical applications. In this study, we argue that in both lazy and rich training regimes, neural networks trained in a variety of settings are well described by kernel methods with (possibly) task-adapted kernels. This correspondence between network training and (adaptive) kernel predictors becomes exact in the limit of infinite width. We consider (1) deep networks sampled from a Bayesian posterior (2) gradient flow training of randomly initialized networks trained with weight decay. For (1) we derive the large width limit for Bayesian networks, showing how feature learning leads to task-relevant adaptation of features and non-Gaussian preactivation densities. The saddle point equations governing this limit result in a min-max optimization problem, which we solve numerically. For (2) we provide fixed point equations for the final hidden layer features and network outputs in the infinite time limit. We compare these solutions to lazy learning and demonstrate that they achieve lower test loss on benchmark datasets. 
\end{abstract}

\section{Introduction}
As neural-network-based artificial intelligence is increasingly impacting many corners of human life, advancing the theory of learning in neural networks is becoming more and more important, both for intellectual and safety reasons. Progress in this endeavor is limited but promising. An influential set of results that motivates this paper
%
%Despite the great success achieved by deep and wide neural networks in various domains~\cite{he2015deepresiduallearningimage,brown2020languagemodelsfewshotlearners}, it is only for a few years that theoretical foundations of feature learning for sufficiently wide networks have been developed~\cite{lecun:hal-04206682, Goodfellow-et-al-2016,lee2018deepneuralnetworksgaussian,matthews2018gaussianprocessbehaviourwide}.A promising approach to interpretability 
identifies infinitely-wide neural networks under certain initializations as nonparametric kernel machines \cite{neal, cho,lee2018deepneuralnetworksgaussian,matthews2018gaussianprocessbehaviourwide,arora2019exactcomputationinfinitelywide, jacot2020neuraltangentkernelconvergence,Lee_2020}. This is important because kernels are theoretically well-understood \cite{Rasmussen2006Gaussian,Scholkopf}, offering rich mathematical frameworks for analyzing expressivity and generalization properties of neural networks. A drawback of this identification is that it works in the \textit{lazy} regime of neural network training ~\cite{chizat2020lazytrainingdifferentiableprogramming}, i.e. when data representations are fixed at initialization and do not evolve during learning. However, state-of-the-art deep networks operate in the \textit{rich}, feature learning regime \cite{Geiger_2020, vyas2022limitations, yang2022featurelearninginfinitewidthneural,vyas2023featurelearningnetworksconsistentwidths}, where they adapt their internal representations to the
structure of the data.


Motivated by these observations, here, we ask whether infinitely wide neural networks still admit nonparametric kernel predictors in the rich domain. And if so, what characterizes these predictors? Since in the rich regime data representations evolve dynamically according to the training dynamics, identifying the nature of predictors in this regime is crucial for uncovering the principles behind feature learning. Further, as wider networks are believed to perform better on the same amount of data~\cite{hestness2017deep, novak2018sensitivitygeneralizationneuralnetworks,kaplan2020scaling, hoffmann2022training}, it opens the possibility of directly training infinitely-wide feature learning networks through their corresponding kernel machines for obtaining the best performing instances of a given model architecture.

In this paper, we derive kernel predictors for infinitely wide multilayer perceptrons (MLPs) and convolutional neural networks (CNNs) in the \textit{rich} regime. They are kernel predictors in the sense that
%
\begin{equation}\label{eq:kernel_predictor_rep_thm}
    f(\bm x) = \sigma\left( \sum_{\mu = 1}^P a_{\mu} K(\bm x, \bm x_{\mu})\right) 
\end{equation}
%
 where $\sigma$ is a nonlinear function, $a^{\mu}$ are scalar coefficients, $\{\bm x_{\mu}\}_{\mu=1}^P$ are training data, and $\bm x$ is the test point. The kernel $K$ depends on the architecture of the network, and, importantly, adapts to the training data, unlike the Neural Tangent Kernel (NTK) \cite{jacot2020neuraltangentkernelconvergence} and Neural Network Gaussian Process Kernel (NNGPK) \cite{cho2009kernel, lee2018deepneuralnetworksgaussian,matthews2018gaussianprocessbehaviourwide} derived from lazy infinite limits. 


     \begin{figure*}
        \centering
        \subfigure[Gradient flow $\lambda >0$]{\includegraphics[width=0.26\linewidth]{figures/ntk_scatterplot_lambda0_mnist.png}}
        \subfigure[aNTK]{\includegraphics[width=0.26\linewidth]{figures/ntk_scatterplot_lambda1_mnist.png}}
        \subfigure[aNBK]{\includegraphics[width=0.26\linewidth]{figures/bayes_scatterplot_lambdabeta_mnist.png}}
         \caption{Test network predictors of a two-layer MLP (width $N=5000$) trained with $P=300$ data of two-classes of CIFAR10  compared with the theoretical kernel regression predictors. Three panels are for different regimes of regularization $\lambda$ and temperature $1/\beta$. In all the three cases of $\lambda$, $K_{\text{init}}$ represent the network predictors at initialization, while $K_{\text{final}}/\Phi_{\text{final}}$ correspond either to aNTK at convergence for (a)/(b) or aNBK that at convergence for (c). (a) When $\lambda = 0$ the NN predictor of gradient flow with weight decay is not a kernel predictor; (b) instead, when $\lambda > 0$ the test prediction is well captured by $f_{\text{aNTK}}$ (Eq.~\eqref{eq::pred_NTK}). (c) Bayesian empirical predictor is well-described by aNBK predictor at converge. We expect these matches to be exact for adaptive kernels at infinite width.} 
        \label{fig::lambdas}
    \end{figure*}
    
To arrive at these predictors, we start from existing work characterizing the feature-learning infinite-width limits of deep neural networks under gradient flow dynamics. Previously, \citet{bordelon2022selfconsistentdynamicalfieldtheory} adopted the dynamical mean-field theory (DMFT) \cite{dmft,deDominicis,zippelius,arora2019convergenceanalysisgradientdescent,arous2004cugliandolokurchanequationsdynamicsspinglasses} approach to the training dynamics of deep MLPs and CNNs under the \textit{maximal update parameterization} ($\mu$P) \cite{yang2022featurelearninginfinitewidthneural,yang2022tensorprogramsvtuning,Mei2018, rotskoff2018trainability}, which leads to a feature-learning infinite-width limit. This DMFT results in a set of stochastic integro-differential equations in terms of summary statistics, which define the deterministic training time ($t$) evolution of the deterministic network predictor $f(\boldsymbol{x},t)$. Performing inference with this framework requires solving these equations over the training time using sophisticated Monte Carlo techniques. Further, complexity and history dependence of the equations make the predictor interpretation challenging.

Our main observation is that we can analyze the network predictor at convergence $f(\bm x, t=\infty)$ in two ways to obtain deterministic adaptive kernel predictors:  (1) by interpreting the dynamics of gradient flow with added white noise as sampling the weights from a Bayesian posterior, (2) by studying the fixed points of DMFT equations for gradient flow with weight decay. Specifically, our contributions in this work are the following: 
\begin{enumerate}    
    \item We study the noisy gradient-flow dynamics with weight decay in the rich regime for MLPs and CNNs. We identify two novel infinite-width limits (\cref{table:limits}, \cref{fig::lambdas}) that lead to adaptive kernel machine interpretations of neural networks. We name the corresponding kernels \textit{adaptive Neural Bayesian Kernel} (aNBK) and \textit{adaptive Neural Tangent Kernel} (aNTK) for reasons that will be apparent below.
    
    \item To analyze the first of these limits, we introduce a novel Bayesian interpretation of feature-learning neural networks, where the posterior distribution at infinite width characterizes the network’s state after training. Analyzing this posterior in the infinite width limit using statistical mechanics methods, we identify a min-max optimization problem, arising from a saddle point argument, that defines the aNBK predictor. %The new key aspect is that it allows to study how neural networks transition between the \textit{lazy} and \textit{rich} training regimes~\cite{chizat2020lazytrainingdifferentiableprogramming} when predictions are made using the Bayesian posterior. To do so, a fundamental aspect is the scaling of the readout as $1/\gamma = 1/(\gamma_0 \sqrt{N})$, with $\gamma_0$ modulating the degree of feature evolution~\cite{vyas2023featurelearningnetworksconsistentwidths, bordelon2024infinitelimitsmultiheadtransformer}. Our theory recovers the lazy learning NNGP limit~\cite{neal,lee2018deepneuralnetworksgaussian,matthews2018gaussianprocessbehaviourwide} as $\gamma_0 \to 0$.
    \item To analyze the second limit, we invoke the DMFT analysis of gradient-flow dynamics \cite{bordelon2022selfconsistentdynamicalfieldtheory}. We show that when weight decay is added to gradient flow dynamics the final learned network predictors behave as kernel predictors. We provide the DMFT fixed point equations that define the aNTK predictor. 
    %we review DMFT from~\cite{bordelon2022selfconsistentdynamicalfieldtheory}, highlight its non-Markovian nature, and show that \textit{only} when weight decay is added to gradient flow dynamics the final learned predictors behave as kernel predictors. This is not true in general (see Fig.~\ref{fig::lambdas}).
%    \item We give a list of these predictors in both settings for fully connected and convolutional architectures.
    \item We develop numerical methods to solve for our predictors. %In gradient-flow with weight decay, fixed points determine the first two moments of pre-activation density distribution (and then the kernels), but require tracking the full training trajectory to get the full marginals. In contrast, the Bayesian setting directly yields full pre-activation densities at each layer by solving a min-max optimization problem that encodes the network's statistical structure. 
    \item For kernels arising from deep infinitely-wide linear Bayesian networks, we solve the saddle point equations exactly via recursion, bypassing sampling strategies needed for the nonlinear case. We analyze the behavior of kernel-task overlap parameters across depth for whitened data.
    \item We provide comparisons of our adaptive kernel machines to trained networks and $\textit{lazy}$ NTK and NNGPK predictors for MLPs and CNNs. We demonstrate that our adaptive-kernels are descriptive of feature-learning neural network training on a variety of metrics, including test loss, intermediate feature kernels, and pre-activation densities. In addition, they outperform $\textit{lazy}$ kernel predictors on benchmark datasets.
\end{enumerate}


%Overall, the key achievement of this work is a precise characterization of when the fixed points of real deep neural networks — trained under gradient flow, with either weight decay or injected noise - converge to adaptive kernel methods in the infinite-width limit and still learn features from data. Because these theories remain consistent across network widths~\cite{vyas2023featurelearningnetworksconsistentwidths}, this insight enables a potential shift: instead of training finite width networks directly, one can solve an iterative procedure over adaptive kernels to predict its performance.

\vspace{-5pt}

\subsection{Related works}
\vspace{-5pt}
\paragraph{Neural networks as kernel machines.} 

In certain initialization and parameterization schemes, taking the width of a neural network to infinity leads the model to learn a kernel machine, with a kernel that depends only on the initial task-independent features which do not change during training \cite{jacot2020neuraltangentkernelconvergence, lee2018deepneuralnetworksgaussian}.
This ``lazy training" regime has been extensively studied, particularly in the context of infinitely wide networks \cite{chizat2020lazytrainingdifferentiableprogramming, jacot2020neuraltangentkernelconvergence, lee2018deepneuralnetworksgaussian, Lee_2020}. However, neural networks outside of the lazy limit (in the so-called ``rich regime") often perform better than their corresponding initial NTKs \cite{vyas2022limitations} and are not obviously related to kernel machines. \citet{domingos2020every} argues that gradient descent training for any deep network corresponds to regression with a history-dependent ``path kernel", though this definition does not satisfy the standard representer theorem where coefficients are only functions of the training data as in Equation \eqref{eq:kernel_predictor_rep_thm}. In general, the learned function of a network trained with gradient flow can always be written as an integral over the history of error coefficients on training data and a time evolving NTK. In the present work, however, we are focused on when the final solution a network converges to satisfies a \textit{history independent} representer theorem as in Equation \ref{eq:kernel_predictor_rep_thm}. Some experimental and theoretical works have indicated that this is often the case, where regression with the \textit{final} NTK (computed with gradients at the final parameters) of a network provides an accurate approximation of the learned neural network function during rich training \cite{Geiger_2020, atanasov2021neuralnetworkskernellearners, wei2022toyrandommatrixmodels}. A complete theoretical understanding of when and why the correspondence between final NTK and final network function holds is currently lacking.

\vspace{-5pt}
\paragraph{Adaptive kernels.}

In Bayesian neural networks, several works have identified methods that describe learned network solutions beyond the lazy infinite-width description of NNGP regression. Some works pursue perturbative approximations to the posterior in powers of $1/\text{width}$ \cite{Zavatone_Veth_2022, roberts2022principles} or alternative cumulant expansions of the predictor statistics \cite{naveh2021self}. Others analyze the Bayesian posterior for deep linear networks, which are more analytically tractable \cite{aitchison2020biggerbetterfiniteinfinite, hanin2023bayesian, zavatone2022contrasting, bassetti2024featurelearningfinitewidthbayesian} and actually also capture the behavior of deep Bayesian nonlinear student-teacher learning in a particular scaling limit \cite{cui2023bayes}. Several works on Bayesian deep learning have argued in favor of a proportional limit where the samples and width are comparable \cite{Li_2021,Pacelli_2023,Aiudi2023LocalKR, vanmeegen2024codingschemesneuralnetworks,Baglioni2024}. These works claim that the mean predictor under the posterior is the same as in the lazy learning limit, but that the predictor variance changes as a function of $P/N$. In our work, however, we take $N \to \infty$ first at fixed $P$ in the rich regime and find that network predictions are not well described by the lazy limit, either in theory or in experiments (see Figures \ref{fig::lambdas} \& \ref{fig::fig1}).  More recent versions of these theories explore variational approximations of the hidden neuron activation densities \cite{seroussi2023separation, yang2023theory} and have begun to explore other parameterizations \cite{rubin2024a}. These methods have been used to explain sharp transitions in the behavior of the posterior as hyperparameters, such as prior weight variance, are varied \cite{rubin2024grokking}. Alternatively, some works have developed non-neural adaptive kernel algorithms, which filter information based on gradients of a kernel solution with respect to the input variables \cite{radhakrishnan2022mechanism}, which exhibit improvements in performance over the initial kernel and can capture interesting feature learning phenomena such as grokking \cite{mallinar2024emergence}. 

\vspace{-10pt}
\section{Preliminaries}\label{preliminaries}
\vspace{-5pt}
We start by describing our setup. Here, for ease of presentation, we discuss the fully connected MLP setting, referring to \cref{appendix::CNNs} for the case of CNNs. 

For an empirical training dataset $\mathcal{D}=\{\boldsymbol{x}_{\mu},y_{\mu}\}_{\mu=1}^P$ of size $P$, input vectors $\boldsymbol{x}_{\mu} \in \mathbb{R}^D$ and labels $y_{\mu}$, we define the output of an MLP with $L$ layers as
\begin{equation}\label{eq::defs}
\begin{split}
    &f_{\mu} =\sigma \left( \frac{1}{\gamma_0 N_{L}}\boldsymbol{w}^{(L)}\cdot\phi (\boldsymbol{h}_{\mu}^{L})\right),\\
    &\boldsymbol{h}_{\mu}^{\ell+1}	=\frac{1}{\sqrt{N_{\ell}}}\boldsymbol{W}^{\ell}\phi(\boldsymbol{h}_{\mu}^{\ell}), \quad \boldsymbol{h}_{\mu}^{1}	=\frac{1}{\sqrt{D}}\boldsymbol{W}^{(0)}\boldsymbol{x}_{\mu},
\end{split}
\end{equation}
where $\phi (a\bm\theta) = a^{\kappa} \phi (\bm \theta)$ represents a homogeneous transfer function, $\boldsymbol{h}^{\ell}_{\mu} \in \mathbb{R}^{N_{\ell}}$ at layer $\ell$ is the pre-activation vector and $\boldsymbol{W}^{\ell} \in \mathbb{R}^{N_{\ell +1}\times N_{\ell}}$ is the matrix of weights to be learned. We initialize each trainable parameter as a Gaussian random variable $W_{ij}^{\ell}\sim \mathcal{N}(0,1)$ with unit variance, in such a way that in the infinite width limit $N_{\ell} = N \to \infty, \,\, \forall \ell \in \{L\} $ the pre-activations at each layer will remain $\Theta_N(1)$. At the same time, we scale the network output by a factor $\gamma_0 \sqrt{N} $ in order to study feature learning. This allows to interpolate between a lazy limit description of NNs when $\gamma_0 \to 0$, and a rich regime description when $\gamma_0 = \Theta_N (1)$. This parameterization, known as \textit{maximal update parameterization} ($\mu$P) \cite{yang2022featurelearninginfinitewidthneural}, allows feature learning by enabling preactivations to evolve from their initialization during training even in the infinite-width limit. 

%Originally introduced in~\cite{yang2022featurelearninginfinitewidthneural}, $\mu$P also provides a well-defined framework for feature learning across a wide range of architectures, including MLPs, CNNs, and RNNs, by keeping consistent learning dynamics across the network widths~\cite{vyas2023featurelearningnetworksconsistentwidths}. Thus, we are confident that representation learning will be effectively captured, and the infinite-width limit we analyze theoretically will remain meaningful and predictive for finite-width architectures. 

      
\section{Adaptive kernel limits of training dynamics}

Next, we study the rich training dynamics of the NN defined as in Eq.~\eqref{eq::def} to arrive at adaptive kernel predictors. In particular, we study the infinite time limit $t \to \infty$ of the noisy gradient-flow dynamics 
%
\begin{equation}\label{eq::dyns}
     d\boldsymbol{\theta}(t)=-\eta \gamma^2 \nabla_{\boldsymbol{\theta}}\mathcal{L}\,dt-\eta\lambda \beta^{-1} \boldsymbol{\theta}(t)\,dt + \sqrt{2\eta\beta^{-1}} d \boldsymbol{\epsilon} (t)
  \end{equation}
  %
  for the collection of weights $\boldsymbol{\theta} = \boldsymbol{\text{Vec}}\{ \boldsymbol{W}^{(0)}, \ldots, \boldsymbol{w}^{(L)} \}$, a loss function $\mathcal{L}(\boldsymbol{\theta})$ and for a ridge $\lambda$. Here, $\eta$ stands for the learning rate, $\gamma^2 = \gamma_0^2 N$ ensures the feature updates to be $\Theta_N(1)$ in the infinite width limit~\cite{bordelon2022selfconsistentdynamicalfieldtheory}, and $d\boldsymbol{\epsilon}$ is a Brownian motion with covariance structure $\langle d\boldsymbol{\epsilon} (t) d\boldsymbol{\epsilon}(t')\rangle = \delta (t-t')\boldsymbol{I}$, whose contribution to the dynamics can be switched off by tuning the temperature $T = \frac{1}{\beta} \to 0$. When the Brownian motion is on, this dynamics can be interpreted as sampling from a Bayesian posterior, which will be detailed below. When it is turned off, this is gradient-flow dynamics with weight decay.
  
It is well know that infinite limits of this training dynamics in the lazy regime ($\gamma_0\to0$) lead to kernel machines defined by the NTK~\cite{jacot2020neuraltangentkernelconvergence} and NNGPK \cite{cho2009kernel, lee2018deepneuralnetworksgaussian,matthews2018gaussianprocessbehaviourwide}. Here, we show that there are infinite limits in the rich regime that also lead to kernel predictors, but this time these kernels adapt to data. The order of limits for width, time, temperature, and feature learning strength parameters $\{N, t, \beta, \gamma_0\}$ to get either the already know \textit{lazy} (NNGPK, NTK) or novel adaptive kernel predictors is shown in Table~\ref{table:limits}. The latter kernels correspond either to the infinite width limit of a NN at convergence (i.e. $t \to \infty$) that learns with $\beta >0$ (aNBK) or to the infinite time limit of an infinitely wide NN learning with gradient flow and weight decay (i.e. $\beta\to\infty$) (aNTK).  %Given these, NNGPK corresponds to the $\gamma_0 \to 0$ of aNBKK, while NTK is the $\gamma_0 \to 0$ limit of aNTK  when $t = \Theta_N (1)$.

Next, we give the functional forms of our novel aNTK and aNBK predictors and briefly discuss their derivations. Details are given in Appendix~\ref{sec::full_bayes_derivation}.
\vspace{-5pt}
\begin{table}[h!]
        \label{table:comparison} 
      \begin{center}
    \begin{tabular}{|p{3.9cm}||p{3.5cm}|}
      \hline
      \textbf{NNGPK} & \textbf{aNBK} (ours)\\
        \hline
        $\lim_{\gamma_0 \to 0} \lim_{ N \to \infty} \lim_{t\to \infty}$ & $\lim_{ N \to \infty} \lim_{t\to \infty}$\\
        $\beta = \Theta_N(1)$ & \{$\gamma_0, \beta\} $= $\Theta_N (1)$\\
      \hline
    \end{tabular}
    \end{center}
    \begin{center}
    \begin{tabular}{|p{3.7cm}||p{3.7cm}|}
      \hline
      \textbf{NTK} & \textbf{aNTK} (ours)\\
      \hline
      $\lim_{\gamma_0 \to 0} \lim_{N\to \infty} \lim_{\beta \to \infty}$ & $\lim_{t \to \infty} \lim_{N \to \infty} \lim_{\beta \to \infty}$ \\
      $t = \Theta_N (1)$ & $\gamma_0 = \Theta_N (1)$\\
      \hline
    \end{tabular}
    \caption{\label{table:limits} Limiting orders for $\{N,t,\beta,\gamma_0\}$ in the dynamics~\eqref{eq::dyns} to get either (known) static kernels NNGPK \& NTK (left column), or (new) adaptive kernel predictors (right column).\footnote{We note that there is no variance in these predictors in the $N\to\infty$ limit over either random initial conditions or the Bayesian posterior due to our $\mu$P scaling with respect to $N$. This differs from works where the posterior (in standard / NTK parameterization) induces a Gaussian process over the output function \cite{lee2018deepneuralnetworksgaussian}. } }
    \end{center}
\end{table}
\vspace{-10pt}

\subsection{aNBK}
As described in \cref{table:limits}, if we take first the $t\to\infty$ limit, and then the $N\to\infty$ limit with temperature ($1/\beta$) and feature learning strength ($\gamma_0$) fixed, we arrive at  
%Here, we first give a pseudocode for computing the Bayes $\mu$P-AK predictor that arises when we take the order of limits described in \ref{table:limits} for the dynamics of Eq.~\eqref{eq::dyns}. This takes the form of 
an adaptive kernel predictor (see Fig.~\ref{fig::lambdas})
\begin{align}
f_{\text{aNBK}} (\bm x) = \sigma\left( \frac{\beta}{\lambda_L}\sum_{\mu=1}^P \Delta_{\mu}\Phi^L (\bm x_{\mu}, \bm x) \right),
\end{align}
with $\Delta_{\mu} = - \frac{\partial \mathcal{L}}{\partial s_{\mu}}$ being the error signal, $s_{\mu} = \frac{1}{\gamma \sqrt{N}_{L}}\boldsymbol{w}^{(L)}\cdot\phi (\boldsymbol{h}_{\mu}^{L})$ the pre-readout as in Eq.~\ref{eq::defs}, and $\Phi^{L}_{\mu} = \frac{1}{N}  \phi(\boldsymbol{h}_{\mu}^{L})\cdot\phi(\boldsymbol{h}^{L})$ the train-test feature kernel at the last layer $L$. In this notation, the kernel matrix element is given by $\Phi_{\mu\nu} = \Phi (\bm x_{\mu},\bm x_{\nu})$. 
In the squared loss case with a linear readout ($\sigma(s)\equiv s$), the predictor $f(\bm x)$ becomes a kernel regression predictor of the form
\begin{equation}\label{eq::predictor_bayes}
        f_{\text{aNBK}}(\boldsymbol{x}) = (\boldsymbol{\Phi}^{L}(\bm x))^{\top}\left[\boldsymbol{\Phi}^{L}+\lambda_L\frac{\boldsymbol{I}}{\beta}\right]^{-1}\boldsymbol{y},
\end{equation}
%
where ${\Phi}^{L}(\bm x)_{\mu} = {\Phi}^{L}(\bm x, \bm x_{\mu})$.
    
In Appendix~\ref{sec::full_bayes_derivation}, we show that the kernel $\Phi^L$ is given by a solution to a min-max optimization problem that involves the data-adaptive kernel $\bm \Phi^L \in \mathbb{R}^{P\times P}$, intermediate layer adaptive-kernels $\bm\Phi^{\ell} \in \mathbb{R}^{P\times P}$ and  dual adaptive-kernel variables $\hat{\bm \Phi}^{\ell} \in \mathbb{R}^{P\times P}$. Here, we present this min-max problem for the squared loss and linear readout for simplicity, see Appendix~\ref{sec::full_bayes_derivation} for the full expressions. First, we define the action:
\begin{align}\label{eq:action_formula}
S =& -\frac{1}{2}\sum_{\ell=1}^L \text{Tr} \ \bm\Phi^{\ell}\hat{\bm\Phi}^{\ell}+\frac{\gamma_0^2}{2}\bm y^{\top}\Big(\frac{\bm I}{\beta}+\frac{\bm \Phi^{L}}{\lambda_{L}}\Big)^{-1}\bm y \nonumber
\\
&-\sum_{\ell=1}^{L-1}\ln\mathcal{Z}_{\ell}[\bm\Phi^{\ell -1},\hat{\bm \Phi}^{\ell}].
\end{align}
where the functions $\mathcal Z_\ell[\bm\Phi^{\ell-1},\hat{\bm\Phi}^\ell]$ are defined as
\begin{equation}\label{eq::preact_distr}
    \begin{split}
        \mathcal Z_\ell[\bm\Phi^{\ell-1},\hat{\bm\Phi}^\ell] = \int d\h^\ell &\exp\left( -\frac{\lambda_{\ell-1}}{2} \left( \bm h^\ell \right)^{\top}(\bm \Phi^{\ell-1})^{-1} \bm h^\ell\right) 
        \\
        &\exp\left( -\frac{1}{2} \phi(\bm h^\ell)^{\top}\hat{\bm \Phi}^{\ell}\phi(\bm h^\ell)  \right),
    \end{split}
    \end{equation}
with base case $\Phi^0_{\mu \nu} \equiv \frac{1}{D} \x_\mu \cdot \x_\nu$. Then, the saddle point that dominates the distribution is
%
\begin{align}
\{ \bm\Phi^{\ell}, \hat{\bm\Phi}^\ell \}_{\ell=1}^L = \arg \min_{\{\bm\Phi^\ell\}} \max_{\{\hat{\bm\Phi}^\ell\}} S(\{ \bm\Phi^{\ell}, \hat{\bm\Phi}^\ell \}).
\end{align}
%

\begin{proof}[Derivation sketch.] Taking $t \to \infty$ at fixed temperature $\beta = \frac{1}{T}$ and finite width $N$ in Eq.~\eqref{eq::dyns} converges to a stationary distribution \cite{kardar2007statistical,welling_2011} over the trainable parameters $\bm \theta$ given the dataset $\mathcal{D}$, which can be interpreted as a Bayesian posterior with log-likelihood $-\beta \gamma^2 \mathcal{L}(\bm \theta)$ and a Gaussian prior of scale $\lambda^{-1/2}$
    %
\begin{equation}\label{eq::posterior_main}
        p(\boldsymbol{\theta}|\mathcal{D}) = \frac{1}{Z} \exp \left[-\beta \gamma^2 \mathcal{L}(\boldsymbol{\theta}) -\frac{\lambda}{2}||\boldsymbol{\theta}||^2\right]. 
    \end{equation}
    The distribution of Eq.~\eqref{eq::posterior_main} can be studied in the overparameterized thermodynamic limit where the width at each layer $N \to \infty$ while $P = \Theta_N (1)$. When $\beta \to \infty$, the posterior is dominated by the set of global minimizers of the loss for the training data (ie solutions $\bm\theta$ that minimize $\mathcal{L}(\boldsymbol{\theta})$). In general, Eq.~\eqref{eq::posterior_main} is function of the order parameters $\{ \bm \Phi^{\ell}, \hat{\bm \Phi}\}_{\ell =1}^L$ since it has the form $p(\bm \theta|\mathcal{D}) \propto \int \prod_{\ell =1}^L {d\bm \Phi^{\ell} d \hat{\bm \Phi}^{\ell}}\, \exp\left( -NS(\bm \Phi^{\ell}, \hat{\bm \Phi}^{\ell}) \right)$ where $S(\bm \Phi^{\ell}, \hat{\bm \Phi}^{\ell})$ is the Bayesian action given in \cref{eq:action_formula}. When $N$ is large,  $p(\bm \theta | \mathcal{D})$ is exponentially dominated by the saddle points of $S$, and looking for the values of $\{ \bm \Phi^{\ell}, \hat{\bm \Phi}\}_{\ell =1}^L$ which makes $S$ locally stationary means to solving the $\min$-$\max$ optimization problem in~\cref{alg:kernel_convergence}. In the lazy learning limit $\gamma_0 \to 0$, the dual kernels vanish $\hat{\bm\Phi}^\ell \to 0$ and we recover Gaussian preactivation densities, consistent with the NNGP theory \cite{lee2018deepneuralnetworksgaussian}. 
    %In contrast with previous works~\cite{lee2018deepneuralnetworksgaussian}, where to compute $\bm \Phi^{\ell}$ it is sufficient to know the distribution $h_{\mu i}^{\ell} \sim \mathcal{N}(0, \Phi^{\ell -1}_{\mu \nu}\delta_{ij})$ at initialization because the task-dependence vanishes when $N\to \infty$ and no representation learning takes place, here the pre-activations are non-Gaussian and task dependent since we scale the likelihood in Eq.~\eqref{eq::posterior_main} proportionally to $\gamma_0^2 N$. From that, the predictor is just $\langle f(\boldsymbol{x};\bm \theta)\rangle_{\bm \theta \sim p(\bm \theta |\mathcal{D})}$. 
    A full account of this derivation is given in Appendix~\ref{appendix::bayes_regression}.
\end{proof}
\vspace{-10pt}
We present a numerical algorithm to calculate the aNBK Regression Predictor from data in Alg.~\ref{alg:kernel_convergence}. %To do so, the algorithm calculates the data-adaptive kernel $\bm \Phi^L \in \mathbb{R}^{P\times P}$, intermediate layer adaptive-kernels $\bm\Phi^{\ell} \in \mathbb{R}^{P\times P}$ and  dual adaptive-kernel variables $\hat{\bm \Phi}^{\ell} \in \mathbb{R}^{P\times P}$ through solving a minmax problem. Another important quantity is a non-Gaussian density measure defined by these kernels which describe the statistics of pre-activations in the limit we consider


%The derivation of this algorithm is detailed in Appendix \cp{cite} and a summary is given below. 

%Deriving the kernels means to solve a set of fixed point equations self-consistently as reported in Alg.~\ref{alg:kernel_convergence}.

\iffalse
\begin{algorithm}[H]
\caption{aNBK Regression Predictor}
\label{alg:kernel_convergence}
\KwIn{Dataset $\mathcal{D} = \{\bm x_\mu, y_\mu\}_{\mu=1}^P$ with covariance $\Phi^0_{\mu \nu} = \frac{1}{D} \x_\mu \cdot \x_\nu$, hyperparameters $\{\gamma_0, \beta, \lambda \}$, step size $\delta$.}
\KwOut{%Converged kernels $\{\bm\Phi^{\ell}, \hat{\bm\Phi}^{\ell}\}_{\ell =1}^L$ and 
Predictor $f(\bm x)$ for any test point $\bm x \in \{P_{\text{test}}\}$.}

\textbf{Generate:} Initial guesses for $\{\bm\Phi^{\ell}, \hat{\bm\Phi}^{\ell}\}_{\ell =1}^L$: \\
$\bm\Phi^{\ell} = \left< \phi(\h^\ell) \phi(\h^\ell)^\top \right>_{\h^\ell \sim \mathcal{N}(0, \bm\Phi^{\ell-1})}$, and $\hat{\bm \Phi}^{\ell} = 0 \quad \forall \ell$.

\While{\textit{Kernels do not converge}}{
    \State Define action the action $S$ of Equation \eqref{eq:action_formula} as differentiable function of $\{ \bm\Phi^\ell, \hat{\bm\Phi}^\ell \}$, using importance sampling to estimate the functions $\mathcal Z_{\ell}$. 
    
    \State Solve the inner optimization problem $$\underset{\hat{\bm\Phi}^{\ell}}{\max} \,S(\{ \bm\Phi^{\ell}, \hat{\bm\Phi}^\ell \})$$ with gradient ascent \;
    
    \State Perform gradient updates on feature kernels
    $$\bm\Phi^{\ell} \leftarrow \bm\Phi^\ell - \delta \ \frac{\partial}{\partial \bm\Phi^\ell} S(\{\bm\Phi^{\ell}, \hat{\bm\Phi}\}^{\ell})$$
}
	\textbf{Compute:} For a test point $\bm x$, $\bm \Phi^{\ell}(\bm x) = \langle \phi (h^\ell(x)) \phi (\h^\ell) \rangle_{p(h,\bm h)}$\; with importance sampling and $p(h, \bm h)$ from Eq.~\eqref{eq::preact_distr} (see~\ref{appendix::gen_error}).


	\Return{$f_{\text{aNBK}}(\bm x)$ as in Eq.~\eqref{eq::predictor_bayes}\;}
\end{algorithm}
\fi

\begin{algorithm}
\caption{aNBK Regression Predictor}
\label{alg:kernel_convergence}
\begin{algorithmic}
  \REQUIRE Dataset $\mathcal{D} = \{\bm x_\mu, y_\mu\}_{\mu=1}^P$ with covariance 
  $\Phi^0_{\mu \nu} = \frac{\bm x_\mu \cdot \bm x_\nu}{D}$, hyperparameters $\{\gamma_0,\beta,\lambda\}$, step size $\delta$.
  \ENSURE Predictor $f(\bm x)$ for any test point $\bm x \in \{P_{\text{test}}\}$.
  
  \STATE \textbf{Generate:} Initial guesses for $\{\bm\Phi^\ell,\,\hat{\bm\Phi}^\ell\}_{\ell=1}^L$: $\bm \Phi^{\ell} = \langle \phi (\bm h^{\ell})\phi(\bm h^{\ell})^{\top}\rangle_{\bm h^{\ell}\sim \mathcal{N}(0,\bm \Phi^{\ell -1})}$ and $\hat{\bm \Phi}^{\ell} =0\quad \forall \ell$.
  
  \WHILE{\textit{Kernels do not converge}}
    \STATE Define the action $S$ from Equation~\eqref{eq:action_formula} as a differentiable function of $\{\bm\Phi^\ell,\,\hat{\bm\Phi}^\ell\}$, using importance sampling to estimate $\mathcal{Z}_\ell$.
    \STATE Solve the inner optimization problem 
    \[
      \max_{\hat{\bm\Phi}^\ell}\; S\bigl(\{\bm\Phi^\ell,\,\hat{\bm\Phi}^\ell\}\bigr)
    \]
    with gradient ascent.
    \STATE Perform gradient updates on feature kernels
    \[
      \bm\Phi^\ell \gets \bm\Phi^\ell - \delta\,\frac{\partial}{\partial \bm\Phi^\ell}S(\{\bm\Phi^\ell,\,\hat{\bm\Phi}^\ell\})
    \]
  \ENDWHILE
  
  \STATE \textbf{Compute:} For a test point $\bm x$, $\bm\Phi^\ell(\bm x) = \langle \phi(h_0^\ell(x))\,\phi(\bm h^\ell) \rangle_{p(h_0,\bm h)}$ where $p(h_0, \bm{h}) = \frac{1}{\mathcal{Z}_\ell} e^{-\frac{1}{2}\sum_{\mu,\nu=0}^{P} h_\mu \left(\frac{\tilde{\tilde{\Phi}}_{\mu\nu}^{\ell-1}}{\lambda_{\ell-1}}\right)^{-1} h_\nu - \frac{1}{2}\sum_{\mu,\nu=1}^{P} \phi(h_\mu)\,\hat{\Phi}_{\mu\nu}^{\ell}\,\phi(h_\nu)}$.
  
  \STATE \textbf{Return:} $f_{\text{aNBK}}(\bm x)$ as in Eq.~\eqref{eq::predictor_bayes}.
\end{algorithmic}
\end{algorithm}


    %Here $f(\bm x)$ has just the mean part, while the variance $\langle \delta f(\bm x; \bm \theta)^2 \rangle_{\bm \theta \sim p(\bm \theta |\mathcal{D})}$ is subleading due to the readout scaling as in Eq.~\eqref{eq::defs}. In Appendix [?] we report all the details of the derivation and extend the theory to CNNs. 
    
    \iffalse
    Here, we report the expression of the pre-activation density, which decouple over the neuron index
    \begin{equation}\label{eq::preact_distr}
    \begin{split}
        p(\bm h^{\ell}) =& \frac{1}{\mathcal{Z}} \frac{e^{-\frac{1}{2}\bm h^{\top}(\frac{\bm \Phi^{\ell-1}}{\lambda_{\ell-1}})^{-1}\bm h -\frac{1}{2} \phi(\bm h)^{\top}\hat{\bm \Phi}^{\ell}\phi(\bm h)}}{\sqrt{2\pi\det(\frac{\bm \Phi^{\ell-1}}{\lambda_{\ell-1}})}}
    \end{split}
    \end{equation}
    being the non-Gaussian part proportional to $\hat{\bm \Phi}^{\ell}$. 
    \fi 

    \subsection{aNTK}
    The second order of limits we study is when $\beta \to \infty$ in Eq.~\eqref{eq::dyns}. We are interested in the infinite time limit $t \to \infty$ of the dynamics when $N\to \infty$, which leads to a predictor 
    \begin{align}\label{eq::antk_pred}
    f_{\text{aNTK}}(\bm x)  = \sigma\left(\frac{1}{\kappa \lambda_L}\sum_{\mu=1}^P \Delta_{\mu} K^{a\text{NTK}} (\bm x_{\mu}, \bm x)\right),
    \end{align}
   where again $\Delta_{\mu} = - \frac{\partial \mathcal{L}}{\partial s_{\mu}}$, $s_{\mu}$ the output pre-activation and $K_{\mu \alpha}^{\text{aNTK}}= \lim_{t \to \infty} \frac{\partial f_{\mu}(t)}{\partial \boldsymbol{\theta}}\cdot \frac{\partial f_{\alpha}(t)}{\partial \boldsymbol{\theta}} = \sum_{\ell} G^{\ell+1}(t,t) \Phi^\ell(t,t)$ is the \textit{adaptive Neural Tangent Kernel}. The gradient kernel $G^\ell_{\mu\nu}(t,t) = \frac{1}{N} \bm g^\ell_\mu(t) \cdot \bm g^\ell_\nu(t)$ represents the inner products of gradient vectors $\bm g^\ell_\mu(t) \equiv N \gamma_0 \frac{\partial s_\mu(t)}{\partial \h^\ell_\mu(t)}$ which are usually computed with back-propagation.
    
    For a squared loss and a linear readout, a homogenous activation function $\phi(\cdot)$, the final predictor has the form   \begin{equation}\label{eq::pred_NTK}
        f_{\text{aNTK}}(\boldsymbol{x}_{\star}) = \boldsymbol{k}^{\text{aNTK}}(\boldsymbol{x}_{\star})^{\top} [\boldsymbol{K}^{\text{aNTK}}+ \lambda \kappa \boldsymbol{I}]^{-1} \boldsymbol{y}.
    \end{equation}
    %
     The factor $\kappa$ appears in the expressions from the weight decay contribution to the dynamics~\eqref{eq::dyns}, when we consider a $\kappa$ degree homogeneous network as specified in~\ref{preliminaries}.
     
     In the infinite width limit $N \to \infty$, the neurons in each hidden layer become independent and the $\Phi, G$ kernels can be computed as averages \cite{bordelon2022selfconsistentdynamicalfieldtheory}
     \begin{equation}
     \begin{split}
         &\Phi_{\mu\nu}^{\ell}(t,t) = \langle \phi (h^{\ell}_{\mu}(t)) \phi (h^{\ell}_{\nu}(t)) \rangle \\
         &G_{\mu\nu}^{\ell}(t,t) = \langle g_{\mu}^{\ell}(t) g_{\nu}^{\ell}(t)\rangle 
         \end{split}
     \end{equation}
    where $\langle \cdot \rangle$ denotes the averages over a stochastic process for pairs $\{ h_\mu^\ell, z^\ell_\mu \}_{\mu=1}^P$ which obey the dynamics
    \begin{equation}\label{eq::dmft_dyn_main}
     \begin{split}
         &h_{\mu}^{\ell}(t) = e^{-\lambda t} \xi_{\mu}^{\ell}(t)\\ 
         &+ \gamma_0 \int_0^t dt' \, e^{-\lambda (t-t')}\sum_{\nu} \Delta_{\nu}(t') \,g_{\nu}^{\ell}(t') \,\Phi^{\ell -1}_{\mu\nu}(t,t') \\
         &z_{\mu}^{\ell}(t) = e^{-\lambda t} \psi^{\ell}_{\mu}(t) \\
         &+ \gamma_0 \int_0^t dt' \,e^{-\lambda (t-t')}\sum_{\nu} \Delta_{\nu}(t')\phi (h^{\ell}_{\nu}(t'))G_{\mu\nu}^{\ell +1}(t,t')
         \\
         &g^\ell_\mu(t) = \dot\phi(h^\ell_\mu(t)) z^\ell_\mu(t)
     \end{split}
 \end{equation}  
 %
 where $\xi^\ell_\mu(t), \psi_\mu^\ell(t)$ are stochastic processes inherited from the initial conditions, which become suppressed at large times. These equations are one way (but not the only way) to converge to a set of fixed point condition for the final features and final predictor (see Appendix~\ref{appendix::fixed_points_dmft}). 
     Alg. ~\ref{alg:kernel_convergence_muPAK} provides pseudocode for calculating the predictor in this setting.  
\vspace{-3pt}
   \begin{proof}[Derivation sketch.]
    A derivation to obtain the DMFT dynamics as in Eq.~\eqref{eq::dmft_dyn_main} can be found in~\cite{bordelon2022selfconsistentdynamicalfieldtheory}. Here, we want to stress the difference between the $\lambda = 0$ (disccused in~\cite{bordelon2022selfconsistentdynamicalfieldtheory}) and $\lambda > 0$ cases, the last of which leads to the kernel predictor as in Eq.~\eqref{eq::antk_pred}. In the case of gradient flow with weight decay, since the predictor dynamics can be written by the chain rule $\frac{df_{\mu}}{dt}= \frac{df_{\mu}}{ds_{\mu}}\frac{ds_{\mu}}{dt}$, we can just track the dynamics of the output pre-activation
    \begin{equation}
        \frac{d s_{\mu}}{dt} =\sum_{\alpha=1}^P K^{\text{aNTK}}_{\mu \alpha}(t,t)\Delta_{\alpha} (t) -\lambda_L \kappa s_{\mu}
    \end{equation}
    if we suppose $\sigma' (s_{\mu}) \neq 0$ \footnote{As specified in the Appendix~\ref{appendix::dmft}, we restrict the readout activations to those with $\sigma'(s_{\mu})\neq 0$, otherwise, the gradient signal does not backpropagate through the network, preventing convergence to a kernel predictor.}. How to arrive at this formula can be found in Appendix~\ref{appendix::dmft}. Here, at the fixed point the output pre-activation is $s_{\mu} =\frac{1}{\lambda_L \kappa} \sum_{\mu} \Delta_{\mu} K^{\text{aNTK}}(t,t)$, which recovers the kernel predictor of Eq.~\eqref{eq::antk_pred}. 
     \end{proof}
    \vspace{-15pt}   
    In principle, for any given value of $\lambda$, in order to have an estimate of $\bm K^{a\text{NTK}}$ at convergence, one has to simulate the stochastic non-Markovian field dynamics as shown in Algorithm \ref{alg:kernel_convergence_muPAK}. When $\lambda >0$, the contribution from initial conditions $\xi_{\mu}(t),\psi_{\mu}^{\ell}(t)$ (see Eq.~\eqref{eq::dmft_dyn_main}), is exponentially suppressed at large time, while the second term of Eq.~\eqref{eq::dmft_dyn_main} contributes the most only when the system has reached convergence.  This is not true if we switch-off the regularization $\lambda$, in which case the contribution from $\xi^\ell_\mu(t),\psi^\ell_\mu(t)$ persist late in training, since without the weight decay term, the initial conditions prevent the dynamics from converging to a fixed kernel predictor.
    
    In Fig.~\ref{fig::lambdas} we clearly demonstrate this, by comparing  the predictor of a two-layer MLP trained on a subset of CIFAR10 with the theoretical predictor of Eq.~\eqref{eq::pred_NTK}. For the first case when $\lambda = 0$, the network predictor at convergence is not the kernel predictor aNTK. Instead, when $\lambda >0$, the network dynamics is well-described by Eq.~\eqref{eq::pred_NTK}. We refer to Appendix~\ref{appendix::sec_dnns_antk} for the case of CNNs.
   
 \iffalse    
        \begin{algorithm}[H]
\caption{aNTK Regression Predictor}
\label{alg:kernel_convergence_muPAK}
\KwIn{Data $\bm \Phi^0$, $\bm y$ and hyperparameters $\{\gamma_0, \lambda \}$.}
\KwOut{%Converged kernels $\{\bm\Phi^{\ell}, \bm G^{\ell}\}_{\ell =1}^L$ and 
Predictor $f_{\text{aNTK}}(\bm x)$ for any test point $\bm x \in \{ P_{\text{test}} \}$.}

\textbf{Generate:} Initial guesses for $\{\bm\Phi^{\ell}, \bm G^{\ell}\}_{\ell=1}^L$: $\bm\Phi^0$, $\bm G^{L+1} = \bm 1 \bm 1^{\top}$.

\textbf{Draw} $\mathcal{S}$ samples for random fields at initialization $\xi^{\ell}_{\mu,s}(t) = \frac{1}{\sqrt{N}} W^{\ell}(0)\phi(h^{\ell -1}_{\mu,s})(t)$ and $\psi^{\ell}_{\mu,s}(t)=\frac{1}{\sqrt{N}}W^{\ell}(0) g_{\mu,s}^{\ell +1}(t)$

\While{\textit{Kernels do not converge} $\forall \ell \in \{L\}, \forall s \in \{\mathcal{S}\}$}{
    \State Implement the non-Markovian dynamics of Eq.~\eqref{eq::dmft_dyn_main}

    \\
    \State Compute new $\{\bm\Phi^{\ell}, \bm G^{\ell}\}$
}
%\State \Return{$\{\bm\Phi^{\ell}, \bm G^{\ell}\}$}

\textbf{Compute} $K^{\text{aNTK}}_{\mu\nu} =  \lim_{t\to\infty} \sum_{\ell=0}^L G_{\mu\nu}^{\ell +1}(t,t) 
 \Phi^{\ell}_{\mu\nu}(t,t)$
 
 \Return{$f_{\text{aNTK}}(\bm x)$ as in Eq.~\eqref{eq::pred_NTK}}
\end{algorithm}
\fi

\begin{algorithm}
\caption{aNTK Regression Predictor}
\label{alg:kernel_convergence_muPAK}
\begin{algorithmic}
  \REQUIRE Data $\bm \Phi^0$, $\bm y$, and hyperparameters $\{\gamma_0, \lambda\}$.
  \ENSURE Predictor $f_{\text{aNTK}}(\bm x)$ for any test point $\bm x\in \{P_{\text{test}}\}$.
  
  \STATE \textbf{Generate:} Initial guesses for $\{\bm\Phi^\ell,\, \bm G^\ell\}_{\ell=1}^L$: $\bm\Phi^0$, $\bm G^{L+1} = \bm 1\,\bm 1^\top$.
  
  \STATE \textbf{Draw:} $\mathcal{S}$ samples for random fields at initialization $\xi^{\ell}_{\mu,s}(t) = \frac{1}{\sqrt{N}}\,W^{\ell}(0)\,\phi(h^{\ell-1}_{\mu,s}(t))$ and $\psi^{\ell}_{\mu,s}(t) = \frac{1}{\sqrt{N}}\,W^{\ell}(0)\,g^{\ell+1}_{\mu,s}(t)$
  \WHILE{\textit{Kernels do not converge} $\forall \ell \in \{L\}$, $s \in \{\mathcal{S}\}$}
    \STATE Implement the non-Markovian dynamics of Eq.~\eqref{eq::dmft_dyn_main}.
    \STATE Compute new  $\{\bm\Phi^\ell,\,\bm G^\ell\}$.
  \ENDWHILE
  
  \STATE \textbf{Compute:}$K^{\text{aNTK}}_{\mu\nu}= \lim_{t\to\infty} \sum_{\ell=0}^L G^{\ell+1}_{\mu\nu}(t,t)\,\Phi^{\ell}_{\mu\nu}(t,t)$
  
  \STATE \textbf{Return:} $f_{\text{aNTK}}(\bm x)$ as in Eq.~\eqref{eq::pred_NTK}.
\end{algorithmic}
\end{algorithm}


    \begin{figure*}
    \centering
    \subfigure[Kernel-Label Overlaps]{\includegraphics[width=0.29\linewidth]{figures/c_ell_depth_16_vary_gamma.pdf}}
    \subfigure[Overlap vs $\gamma_0$]{\includegraphics[width=0.29\linewidth]{figures/c_L_vs_depth_gamma_asymptotics.pdf}}
    \subfigure[Low Rank Spiked Kernels]{\includegraphics[width=0.38\linewidth]{figures/final_kernels_bayes_gamma_4.0_L_8.pdf}}
    \caption{Linear networks with whitened data are determined by a set of kernel-label overlap matrices. (a) The overlap variables $c_\ell$ increase exponentially with $\ell$ with rate that depends on $\gamma$. Solid lines taken from Langevin dynamics on $N = 1024$ network. (b) The alignment of the final layer $c_L$ as a function of $\gamma_0$ and $L$ exhibits three distinct scaling regimes. (c) Examples of learned kernels in depth $L=8$, $\gamma_0 =4.0$ and finite width $N$ networks compared to the $N \to \infty$ theory. }
    \label{fig::fig2}
    \end{figure*}  
    
%\paragraph{Degeneracy of preactivation fixed points.} 
%  When $\lambda > 0$ the fields obey the coupled fixed point equations 
%
 % \begin{equation}\label{eq::dmft_fp}
 %       h^\ell_\mu = \frac{\gamma_0}{\lambda} \sum_\nu \Delta_\nu  \Phi^{\ell-1}_{\mu\nu}  \dot\phi(h_\nu) z_\nu    \ , \  z^\ell_\mu = \frac{\gamma_0}{\lambda} \sum_\nu \Delta_\nu \phi(h^\ell_\nu) G^{\ell+1}_{\mu\nu}.
%    \end{equation}
 %    In principle, Eq.~\eqref{eq::dmft_fp} has infinitely many solutions, meaning there exist multiple pairs $(h_\mu^\ell, z_\mu^\ell)$ that obey the constraints and whose averages recover the correlation functions $\Phi^\ell, G^\ell$. The fixed points set the first two moments of the distributions, which by definition are enough to determine the kernels. In the \cl{SM}, we analytically derive some simple conditions on $p(\bm h^{\ell})$ for a two-layer MLP with white covariance structure $\bm K^{x} = \bm I$, and demonstrate that these reproduce the first two moments of the pre-activation distribution of a network trained according to the dynamics of Eq.~\eqref{eq::dyns}. 
 %    To get the full densities one still needs to track the entire history trajectory (see~\ref{alg:kernel_convergence_muPAK}).

\section{Infinitely-wide feature learning deep linear networks}\label{sec::dln}

     \begin{figure*}[t]
        \centering
        \includegraphics[width=0.9\linewidth]{figures/2layerMLP.pdf}
        \caption{Feature learning theories outperform lazy predictors for a two-layer MLP trained  with Squared Loss (SL) on two classes of CIFAR10 (airplane vs automobile). (a) Test losses as a function of sample size $P$. Solid lines refer to theories, dashed lines to numerical simulations on a $N=5000$ network. \textit{Blue} is the NNGP lazy predictor; \textit{green} is the deterministic NNGPK kernel predictor; \textit{orange} is aNTK with feature learning strength $\gamma_0 = 0.3$; \textit{red} is aNBK predictor with the same $\gamma_0$. (b) Non-Gaussian pre-activation densities as a function of $\gamma_0 $ for (\textit{top}) aNTK and (\textit{bottom}) aNBK. Black dashed lines are theoretical predictions. (c) Learned feature kernels of the adaptive theories closely match their relative finite width $N$ network trainings and evolve with $\gamma_0$.}
        \label{fig::fig1}
    \end{figure*}
    Deep linear networks $(\phi(h) \equiv h)$ provide a simpler framework for analysis than their nonlinear counterparts~\cite{saxe2014exactsolutionsnonlineardynamics,advani2017highdimensionaldynamicsgeneralizationerror,arora2019convergenceanalysisgradientdescent,aitchison2020biggerbetterfiniteinfinite,Li_2021,jacot2022saddletosaddledynamicsdeeplinear,Zavatone_Veth_2022}, yet they still converge to non-trivial feature aligned solutions. In deep linear networks, preactivations remain Gaussian at each layer when $P = \Theta_N (1)$ for the Central Limit Theorem (CLT), and this greatly simplifies  the saddle point equations to algebraic formulas which close in terms of the kernels for both aNBK and aNTK theories. This means that we can solve for the adaptive kernels in both cases without any refined sampling strategy. The deep linear case of gradient flow dynamics can be found in~\cite{bordelon2022selfconsistentdynamicalfieldtheory}. 
    Here, we report the solution to the saddle point equations that define the kernels in the feature-learning Bayesian setting, specializing to the regression problem (the generic loss case can be found in Appendix~\ref{sec::full_bayes_derivation})
    \begin{equation}\label{eq::dlns_main}
        \begin{split}
            &\bm \Phi^{\ell}-\frac{\bm \Phi^{\ell-1}}{\lambda_{\ell-1}}\Big(\bm I+\frac{\bm \Phi^{\ell-1}}{\lambda_{\ell-1}}\hat{\bm \Phi}^{\ell}\Big)^{-1}=0\qquad\forall\ell=1,\ldots,L\\
            &\hat{\bm \Phi}^{\ell}-\frac{\hat{\bm \Phi}^{\ell+1}}{\lambda_{\ell}}\Big(\bm I+\frac{\bm \Phi^{\ell}}{\lambda_{\ell}}\hat{\bm \Phi}^{\ell+1}\Big)^{-1}=0\qquad\forall\ell=1,\ldots,L-1\\
            &\hat{\bm \Phi}^{L}+\frac{\gamma_{0}^{2}}{\lambda_{L}}\Big(\frac{\bm I}{\beta}+\frac{\bm \Phi^{L}}{\lambda_{L}}\Big)^{-1}\bm y \bm y^{\top}\Big(\frac{\bm I}{\beta}+\frac{\bm \Phi^{L}}{\lambda_{L}}\Big)^{-1}=0.
        \end{split}
    \end{equation}
%
Here, as a consistency check, it is easy to see that in the lazy limit $\gamma_0 \to 0$ the dual kernels $\hat{\bm \Phi}^{\ell} = 0 \quad \forall \ell \in \{L\}$, and as a consequence all the kernels $\bm \Phi^{\ell}$ will stay equal to the data covariance matrix $\bm \Phi^{0}$, consistent with lazy learning. However, for the rich regime where $\gamma_0 > 0$ the $\hat{\bm\Phi}^\ell$ kernels do not vanish and alter the fixed point kernels $\bm\Phi^\ell$ with target dependent information in the form of low rank spikes (by definition of $\hat{\bm \Phi}^L$). 

To illustrate this spike effect in the learned kernels in the rich limit, we specialize to whitened input data $\bm K^{\bm x} = \bm I $. We show in Appendix~\ref{appendix::dln} that the equations get simplified even further, since now the kernels $\bm \Phi^{\ell}$ only grows in the rank-one $\bm y \bm y^{\top}$ direction. By defining some set of scalar variables $\{c_{\ell}, \hat{c}_{\ell}\}_{\ell = 1}^L$, which are the overlaps with the label direction $\bm y^{\top} \bm \Phi^{\ell} \bm y = c_{\ell}$ and $\bm y^{\top} \hat{\bm \Phi}^{\ell} \bm y = \hat{c}_{\ell}$ we find 
\begin{equation}\label{eq::cl}
    c_{\ell} = \left( 1 + \frac{\gamma_0^2 c_L }{( \beta^{-1} + c_L )^2 }\right)^{\ell} \quad \forall \ell \in \{L\}
\end{equation}
which means that there is an exponential dependence of the overlap on the layer index $\ell$ (full derivation can be found in the Appendix) as Fig.~\ref{fig::fig2}(a) shows. 
From Eq.~\eqref{eq::cl} we derive the scalings for lazy, large depth, and large feature strength limits. For the last layer overlap $c_L$ these are
\begin{equation}
    \begin{split}
        & c_L \sim 1 + L \gamma_0^2  \qquad   \gamma_0^2 L \to 0\\
        & c \sim \gamma_0^{2 L /(L + 1)} \qquad \gamma_0 \to \infty, L \ \text{fixed} \\
        & c\sim \frac{L\gamma_0^2}{\ln (L\gamma_0^2)} \qquad  L \to \infty, \gamma_0 \ \text{fixed}
    \end{split}
\end{equation}
which closely match the theory in their respective regimes plotted in Fig.~\ref{fig::fig2}(b). In Fig.~\ref{fig::fig2}(c) we show examples of learned kernels for a $L=8$ network and $\gamma_0 = 4.0$ matching the finite width $N = 1028$ network trained with Langevin dynamics. 

\section{Numerical Results}

\paragraph{Two-layer MLPs.}
%    Once we have the predictors of the adaptive theories (Eqs.~\eqref{eq::predictor_bayes} and~\eqref{eq::pred_NTK}), computing performance metrics as test loss is easy. In general, this is the mean squared error (MSE) over an unseen test point $\{ \bm x, y\}$
  %  \begin{equation}
  %      \epsilon_{g}(\boldsymbol{x},y)=\langle(y-f(\boldsymbol{x};\bm\theta))^{2}\rangle
   % \end{equation}
   % being the average $\langle \cdot \rangle$ dependent over a measure $p(\bm \theta)$ which is dominated by the fixed points of Eq.~\eqref{eq::dyns} in each specific case. 
   
In Fig.~\ref{fig::fig1}(a) we compare test losses of lazy vs feature learning kernels for a two-layer MLP trained on a $P$ subset of two classes of CIFAR10 in a regression task.
The \textit{green} curve is the performance of NNGPK, \textit{Orange} is the aNTK, \textit{red} is the aNBK. There is a gap in performance between the lazy predictors and the adaptive feature learning predictors. However, when sample size $P$ is small, feature learning in a data-limited scenario can let the model to overfit on test points and lazy learning can be beneficial in a small window of $P$.

 In this plot, we also include the Neural Network Gaussian Process in \textit{blue} \cite{neal, lee2018deepneuralnetworksgaussian,matthews2018gaussianprocessbehaviourwide}, where for a number of patterns $P$ the solution space is sampled from the posterior of Eq.~\eqref{eq::posterior_main} by taking $\gamma = \Theta_N (1)$. Here the mean predictor is equivalent to the NNGPK predictor, however there is also a variance term, which comes from the fact that we are averaging over all possible random weights.
 
   %Instead, as soon as sample size increases, feature learning helps to target good performing solutions. In order to compare the predictions with numerical experiments on a real $N=5000$ network, we train the two-layer model with the corresponding dynamics of the limiting cases as shown in Table~\ref{table:comparison}. 
  \vspace{-5pt}  
    In the rich scenarios,
    we derive the preactivation distributions $p(\boldsymbol{h})$ as a function of $\gamma_0$ (Fig.~\ref{fig::fig1}(b) \textit{top} and \textit{bottom}) at convergence. At initialization, $p(\bm h)$ follows $\mathcal{N}(0, \bm \Phi^{0})$. However, as learning proceeds and features are learned, the densities accumulate non-Gaussian contributions which are identified by our theories (e.g. Eq.~\eqref{eq::preact_distr}). Fig.~\ref{fig::fig1}(c) shows that there is a clustering of $P=100$ data points by category in the feature space defined by the adaptive kernels. 

\vspace{-9.5pt}
\paragraph{Deep MLPs.} In Fig.~\ref{fig::dnns} we show simulations of a Bayesian $L=5$ ReLU MLP and compare to our infinite-width predictors. In order to study how feature learning propagates though depth, we plot the kernel alignment $\mathcal{A}(\bm \Phi^{\ell}, \bm y \bm y^{\top}) =  \frac{\bm{y}^{\top} \bm{\Phi}^{\ell} \bm{y}}{\|\bm{y} \bm{y}^{\top}\| \|\bm{\Phi}^{\ell}\|}$, which is the cosine similarity between the kernel $\bm \Phi^{\ell}$ and the label covariance $\bm y \bm y^{\top}$ at each layer. We show that by increasing feature strength $\gamma_0$, the alignments increase. As expected, the last layer kernel aligns first with the labels as $\gamma_0$ increases, followed by the previous ones. Fig.~\ref{fig::dnns}(b) shows that the perturbed kernels of the theory retrieve the final NN kernels as locally stable fixed points. 

To produce this figure, %Extracting the fixed point kernels from \cref{alg:kernel_convergence} in the deep case is hard, because solving $\underset{\hat{\bm \Phi}}{\max} \,S$ ($S$ is the action defined in~\cref{alg:kernel_convergence}) means to find some values for the dual kernels $\{\hat{\bm \Phi}^{\ell}\}_{\ell =1}^L$ which tilt the non-Gaussian measure in Eq.~\eqref{eq::preact_distr} at each layer. However, we can study the stability of these fixed points if 
we initialized the solver~\cref{alg:kernel_convergence}  with the empirical NN kernels $\{\bm \Phi^{\ell}\}_{\ell =1}^L$ at convergence perturbed with a multiplicative Gaussian noise, and solved for the dual variables. This warm start allowed faster convergence.
\vspace{-4pt}
    \begin{figure}[h!]
        \centering
        \subfigure[Kernel alignments]{\includegraphics[width=0.69\linewidth]{figures/alignments_L4.png}}
        \subfigure[Hidden layer kernels]{\includegraphics[width=0.67\linewidth]{figures/kernels_DNN_L4.pdf}}
        \caption{Bayesian $L=5$ ReLU MLP trained on $P=1000$ data of CIFAR10. Colored curve are experiments, dashed lines are the predictors calculated from~\cref{alg:kernel_convergence}. (a) Kernel alignments $\mathcal{A}(\bm \Phi^{\ell}, \bm y \bm y^{\top})$ at each layer vs feature strength. (b) Theory vs empirical kernels at each layer. }
        \label{fig::dnns}
        \end{figure}
\vspace{-4pt}
\paragraph{CNNs.} 
DMFT for infinite-width CNNs under gradient-flow was previously derived in~\cite{bordelon2022selfconsistentdynamicalfieldtheory}, which we solve numerically here for the first time. In Fig.~\ref{fig::fig_cnn} we show comparisons of DMFT kernel predictors at convergence for a two-layer MLP and a two-layer CNN with kernel size $k = 8$ and stride $8$. Black dashed curves, which are the theories, closely match the full colored lines, which are the network predictors on $N= 1028$ width networks. CNN outperform the MLP at large sample size $P$ for the same $\gamma_0$.  

See  Appendix~\ref{appendix::sec_dnns_antk} and Figure~\ref{fig::fig_cnn} for  simulations of adaptive convolutional kernels derived from the feature-learning setting. 

  \begin{figure}
        \centering
        \includegraphics[width=0.69\linewidth]{figures/mlp_vs_cnn.png}
        \caption{Test Loss as a function of sample size $P$ for DMFT theories at convergence: two-layer MLP vs two-layer CNN. Dashed lines are for theory, full-colored curves for empirical kernels.}
        \label{fig::fig_cnn}
    \end{figure}

\section{Discussion}

%In this paper, we derive theories of non-parametric feature kernel predictors for both MLP and CNN architectures initialized in $\mu$P parameterization. We study the fixed points of a dynamics which is gradient flow with weight decay and white noise contributions. This dynamics leads to different kernel predictors in the infinite width limit $N\to \infty$ depending on which order or limits one looks at. We get either a Bayesian description of DNNs by looking at the infinite wide limit of a network at convergence ($\lim_{N\to \infty, t\to \infty}$), or the kernel predictor corresponding to the fixed points of a gradient flow dynamics when the white noise contribution is tuned off ($\lim_{t\to \infty, N\to \infty}$). We name Bayes $\mu$P-AK the first while $\mu$P- AK the second, which was previously derived in~\cite{bordelon2022selfconsistentdynamicalfieldtheory}. For $\mu$P-AK, we show that without weight decay, gradient flow dynamics fail to yield a kernel predictor, as fixed points depend on initialization and history dynamics, while regularization ($\lambda > 0$) aligns convergence to $\mu$P-AK.

%In this paper, we develop a theory of non-parametric feature kernel predictors for MLP and CNN architectures in $\mu$P/mean-field parameterization. By analyzing gradient flow dynamics with weight decay and/or white noise, we identify two distinct infinite-width adaptive kernel (AK) predictors: Bayes $\mu$P-AK, representing a Bayesian description of DNNs, and $\mu$P-AK, which  corresponds to the fixed points of gradient flow dynamics with weight decay as in~\cite{bordelon2022selfconsistentdynamicalfieldtheory}.

%Unlike the static NNGP and NTK predictors, these kernels evolve through adaptive mechanisms governed by fixed-point equations that depend on data. The degree of feature learning is controlled by the hyperparameter $\gamma_0$, with $\gamma_0 \to 0$ recovering lazy predictors training regime (NNGP, NTK). For $\gamma_0 >0$, we analyze how these AK predictors shape representations in different architectures and benchmark tasks on real datasets (CIFAR10 or MNIST).

%Unlike static NNGP~\cite{neal,lee2018deepneuralnetworksgaussian} and NTK~\cite{jacot2020neuraltangentkernelconvergence} predictors, these AK evolve via data-dependent fixed-point equations. The feature learning strength, controlled by $\gamma_0$, recovers the lazy training when $\gamma_0 \to 0$, and enables richer representations when $\gamma_0 >0$~\cite{bordelon2024featurelearningimproveneural}. We study their impact on performance across architectures and benchmark tasks on real datasets.

%First, we start with deep linear network (DLN) models. In this case, the saddle point equations for deriving Bayes $\mu$P-AK predictor get simplified and if one assumes a white data covariance matrix ($\bm K^{\bm x} = \bm I$), the order parameters become scalar values $c_{\ell}$ which stand for the overlap between kernels and labels at each layer $\ell$. We discover that there is an exponential dependency of this overlaps $c_{\ell}$ from the layer index $\ell$, meaning that at fixed $\gamma_0$ DLNs learn aligning first the last layer kernels to the labels, and then proceeding backward the first ones. We derive some scalings for these overlaps in the lazy, large width, and large depth regimes.
%We first analyze deep linear networks (DLNs), where the saddle point equations for Bayes $\mu$P-AK simplify. Assuming a white data covariance matrix, the order parameters reduce to scalar overlaps $c_{\ell}$ between kernels and task labels at each layer $\ell$. We find that $c_{\ell}$ exhibits an exponential dependence on $\ell$, implying that for fixed $\gamma_0$, DLNs first align the last-layer kernels to the labels and then propagate the alignment backward. We derive scaling laws for these overlaps in the lazy, large-width, and large-depth regimes. 

%Then, we analyze a two-layer non-linear MLP by comparing the predictor performances of Bayes $\mu$P-AK and $\mu$P-AK to lazy predictors. We show that, when feature learning is tuned, they outperform both NNGP and NTK at large sample size $P$ and accurately reproduce the performance of a NN trained with the corresponding dynamics. Our theory predicts non-Gaussian pre-activation densities at convergence and data-clustered feature kernels with alignments increasing with $\gamma_0$, remaining consistent even for finite-width $N$~\cite{vyas2023featurelearningnetworksconsistentwidths}.

%For the deep non-linear Bayesian case, we verify that a warm start—initializing kernels closely to those of a converged NN—recovers the correct fixed points. However, solving the $\min-\max$ optimization problem for deep kernels is computationally expensive, and an improvement of the solver is left for future works.

%Lastly, we study the effect of feature learning in a two-layer convolutional neural network (CNN) by solving our $\mu$P-AK theory. We show how the CNN outperforms the two-layer MLP on a animate/inanimate task on CIFAR10, and that as one increases sample size $P$, increasing feature learning strength $\gamma_0$ is beneficial for test loss.

%Future work could aim to reduce the solver's computational overhead, either by considering data averaged theories, or \cl{add something}.


%Then, we study the case of a two-layer non-linear MLP by comparing the predictor performances of Bayes $\mu$P-AK and $\mu$P-AK to the lazy predictors. We show that they outperform both NNGP and NTK at large sample size $P$ when feature learning is tuned, and that they well-reproduce the performance at convergence of a NN trained with the respective dynamics. Our theories predict non-Gaussian pre-activation densities at convergence and data-clustered feature kernels whose alignments increase with $\gamma_0$. We show in the appendix how such theories that we derive here are consistent across finite width $N$ effects, remaining descriptive of real NN performance not only when the width is large.  


%For the deep non-linear Bayesian case, we check that with a warm start, where the kernels at each layers are initialized with the final kernels of a NN at convergence, we recover the right fixed points. Indeed, solving the $\min-\max$ optimization problem for the kernels in the deep case scales badly with compute time (\cl{give an order of magnitude}), and an improvement of the solver is left for future works. 

In this paper, we develop a theory of non-parametric feature kernel predictors for MLP and CNN architectures in $\mu$P/mean-field parameterization. By analyzing gradient flow dynamics with weight decay and/or white noise, we identify two distinct infinite-width adaptive kernel predictors: aNBK, representing a Bayesian description of DNNs, and aNTK, corresponding to the fixed points of gradient flow with weight decay~\cite{bordelon2022selfconsistentdynamicalfieldtheory}.  Unlike static NNGP~\cite{neal,lee2018deepneuralnetworksgaussian} and NTK~\cite{jacot2020neuraltangentkernelconvergence} predictors, our kernels adapt to data. The feature learning strength, controlled by $\gamma_0$, recovers lazy training when $\gamma_0 \to 0$ and enables richer representations for $\gamma_0 >0$~\cite{bordelon2024featurelearningimproveneural}. We study their impact across architectures and benchmark tasks on real datasets.  

We also analyze infinitely wide deep linear networks in the feature-learning regime, where the saddle point equations simplify. Assuming a white data covariance matrix, the order parameters reduce to scalar overlaps $c_{\ell}$ between kernels and labels at each layer $\ell$, exhibiting an exponential dependence on $\ell$. This implies that for fixed $\gamma_0$, deep linear networks align last-layer kernels first and propagate alignment backward. We derive scaling laws for these overlaps in the lazy, large-width, and large-depth regimes.  

Our numerical results show that our adaptive kernels %Next, we study a two-layer non-linear MLP, comparing aNBK and aNTK predictors to lazy models. When feature learning is tuned, they 
outperform NNGP and NTK at large sample size and match the performance of a trained NN in the feature-learning regime even in moderate widths (e.g. $N=5000$). Our theory predicts non-Gaussian pre-activation densities at convergence and data-clustered feature kernels, whose alignment with label covariance increases with $\gamma_0$. %, remaining consistent even for finite-width $N$~\cite{vyas2023featurelearningnetworksconsistentwidths}. 

Future work could focus on reducing solver computational costs by developing more efficient optimization techniques.  

%For the deep non-linear Bayesian case, we confirm that a warm start—initializing kernels near those of a converged NN—recovers the correct fixed points. However, solving the $\min-\max$ optimization for deep kernels is computationally expensive, leaving improvements to future work.  

%Lastly, we examine feature learning in a two-layer CNN through aNTK theory, showing that it outperforms a two-layer MLP on an animate/inanimate CIFAR-10 task. Increasing $\gamma_0$ improves test loss as $P$ grows.  
\section*{Acknowledgements}
We thank Enrico M. Malatesta and Hugo Cui for useful and engaging discussions. B.B. is supported by a Google PhD Fellowship and NSF CAREER Award IIS-2239780. C.L. is supported by DARPA Award DIAL-FP-038, and The William F. Milton Fund from Harvard University. C.P. is supported by NSF grant DMS-2134157, NSF CAREER Award IIS-2239780, DARPA Award DIAL-FP-038, a Sloan Research Fellowship, and The William F. Milton Fund from Harvard University. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence.


\iffalse
\section*{Impact Statement}

This paper presents work whose goal is to advance the theoretical understanding of deep neural networks. There are many computational/algorithmic consequences 
\fi

\bibliography{icml_paper}
\bibliographystyle{icml2025}
%\printbibliography


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Multi-layer deep Bayesian MLPs}\label{sec::full_bayes_derivation}
As mentioned in the main text, we would like to study feature learning when the solution space is sampled from a posterior that is a Gibbs distribution with a likelihood $\mathcal{L}(\bm \theta ; \mathcal{D})$ and a Gaussian prior $\frac{\lambda}{2}||\bm \theta||^2$. We can do this computation for generic loss and non-linear activation functions as Eq.~\eqref{eq::defs} shows. The representer theorem for Bayesian network is indeed independent on the activation choice. Here, the posterior takes the form
\begin{equation}\label{eq::posterior}
        p(\boldsymbol{\theta}|\mathcal{D}) = \frac{1}{Z} \exp \left[-\beta \gamma^2 \mathcal{L}(\boldsymbol{\theta}; \mathcal{D}) -\frac{\lambda}{2}||\boldsymbol{\theta}||^2\right]. 
    \end{equation}
    being $\boldsymbol{\theta} = \boldsymbol{\text{Vec}}\{ \boldsymbol{W}^{(0)}, \ldots, \boldsymbol{w}^{(L)} \}$ the collection of weights, $\mathcal{D}= \{\bm x_{\mu}, y_{\mu} \}_{\mu=1}^P $ the dataset with $P$ patterns, and $\beta = \frac{1}{T}$ the temperature inverse.
Again, we are interested in the infinitely overparameterized limit where $N\to \infty, P=\mathcal{O}_N(1)$. Here, when $\beta \to \infty$ the posterior becomes the uniform distribution over the set of global minimizers $\theta^{\star} \in \underset{\boldsymbol{\theta}}{\arg \min}\,\mathcal{L}(\boldsymbol{\theta})$.  In this setting, one needs to rescale the loss function $\mathcal{L} \to \gamma^2 \mathcal{L}$ with $\gamma = \gamma_0 \sqrt{N}$ in order to avoid for the Gaussian prior to dominate over the likelihood when $N\to \infty$, suppressing any interaction with the learning task $\mathcal{D}$. From the normalization factor in Eq.~\eqref{eq::posterior}, the partition function reads
\begin{equation}
\begin{split}
    Z&=\int\prod_{\ell=0}^{L-1}dW^{\ell}\int d\boldsymbol{w}^{L}e^{-\frac{\beta}{2}\gamma_{0}^{2}N\sum_{\mu}\mathcal{L}(y^{\mu},f^{\mu})-\sum_{\ell=0}^{L}\frac{\lambda_{\ell}}{2}||W^{\ell}||^{2}}
\end{split}
\end{equation}
and since we consider the dataset as fixed, we wish to integrate out the weights and move to a description in the space of representations. This can be done by simply enforcing the definitions of Eq.~\eqref{eq::deltas} through the integral representations of some Dirac-delta functions
\begin{equation}\label{eq::deltas}
    \int \prod_{\mu,\ell}d\boldsymbol{h}_{\mu}^{\ell +1} ds_{\mu} \langle \prod_{\mu,\ell}\delta \left( \bm h_{\mu}^{\ell +1} - \frac{1}{\sqrt{N_{\ell}}}\boldsymbol{W}^{\ell}\phi(\boldsymbol{h}_{\mu}^{\ell})\right)\prod_{\mu} \delta \left( s_{\mu} -\frac{1}{\gamma \sqrt{N}_{L}}\boldsymbol{w}^{(L)}\cdot\phi (\boldsymbol{h}_{\mu}^{L})\right)\rangle_{\bm \theta \sim \mathcal{N}(0, \lambda^{-1} \bm I)}
\end{equation}
getting 
\begin{equation}\label{eq::partition_func}
    \begin{split}
    Z&=\int\prod_{\mu}\prod_{\ell=0}^{L-1}\frac{d\boldsymbol{h}_{\mu}^{\ell+1}d\hat{\boldsymbol{h}}_{\mu}^{\ell+1}}{2\pi}\int\prod_{\mu}\frac{ds_{\mu}d\hat{s}_{\mu}}{2\pi N_{L}^{-1}}e^{i\sum_{\mu,\ell}\boldsymbol{h}_{\mu}^{\ell+1}\cdot\hat{\boldsymbol{h}}_{\mu}^{\ell+1}-\frac{1}{2}\sum_{\mu,\nu}\sum_{\ell}(\hat{\boldsymbol{h}}_{\mu}^{\ell+1}\cdot\hat{\boldsymbol{h}}_{\nu}^{\ell+1})\Big(\frac{\phi(\boldsymbol{h}_{\mu}^{\ell})\cdot\phi(\boldsymbol{h}_{\nu}^{\ell})}{N_{\ell}\lambda_{\ell}}\Big)+iN_{L}\sum_{\mu}s_{\mu}\hat{s}_{\mu}}\\
    &\quad \times e^{-\frac{1}{2}\sum_{\mu,\nu}\hat{s}_{\mu}\hat{s}_{\nu}\Big(\frac{\phi(\boldsymbol{h}_{\mu}^{L})\cdot\phi(\boldsymbol{h}_{\mu}^{L})}{\lambda_{L}}\Big)-\frac{\beta}{2}N\gamma_0^2\sum_{\mu}\mathcal{L}\left(y^{\mu},g(s^{\mu})\right)}\\
    &=\int\prod_{\mu\nu}\prod_{\ell=1}^{L}\frac{d\Phi_{\mu\nu}^{\ell}d\hat{\Phi}_{\mu\nu}^{\ell}}{2\pi N^{-1}}\int\prod_{\mu}\frac{ds_{\mu}d\hat{s}_{\mu}}{2\pi N_{L}^{-1}}e^{\frac{N}{2}\sum_{\mu\nu}\sum_{\ell}\Phi_{\mu\nu}^{\ell}\hat{\Phi}_{\mu\nu}^{\ell}-N\sum_{\mu}s_{\mu}\hat{s}_{\mu}+\frac{N}{2}\hat{s}_{\mu}\frac{\Phi_{\mu\nu}^{L}}{\lambda_{L}}\hat{s}_{\nu}-\frac{\beta}{2}N\gamma_0^2\sum_{\mu}\mathcal{L}(y_{\mu},g(s_{\mu}))}\\
    &\quad \times e^{N\sum_{\ell=0}^{L-1}\ln\mathcal{Z}[\Phi_{\mu \nu}^{\ell -1},\hat{\Phi}_{\mu \nu}^{\ell}]}.
    \end{split}
\end{equation}
In the last expression, we introduced the feature adaptive kernels as
\begin{equation}\label{eq::nngp_kernel}
        \Phi^{\ell}_{\mu\nu} = \frac{1}{N}  \phi(\boldsymbol{h}^{\ell}_{\mu })\cdot\phi(\boldsymbol{h}^{\ell}_{\nu })
    \end{equation}
    and, again, we enforced their definitions in $Z$ with some conjugated variables $\hat{\Phi}_{\mu\nu}^{\ell}$. Both $\{\Phi_{\mu\nu}^{\ell}, \hat{\Phi}_{\mu\nu}^{\ell} \}$ will become deterministic quantities in the $N\to \infty$ limit. 
    
    The single-site density in Eq.~\eqref{eq::partition_func} is given by
\begin{equation}\label{eq::ss}
    \begin{split}
        \mathcal{Z}_{\ell}&=\int\prod_{\mu}\frac{dh_{\mu}d\hat{h}_{\mu}}{2\pi}e^{i\sum_{\mu}h_{\mu}\hat{h}_{\mu}-\frac{1}{2}\sum_{\mu\nu}\hat{h}_{\mu}\frac{\Phi_{\mu\nu}^{\ell-1}}{\lambda_{\ell-1}}\hat{h}_{\nu}-\frac{1}{2}\sum_{\mu\nu}\phi(h_{\mu})\hat{\Phi}_{\mu\nu}^{\ell}\phi(h_{\nu})}\\
        &=\int \frac{\prod_{\mu}dh_{\mu}}{\sqrt{2\pi\det \left( \frac{\Phi^{\ell -1}}{\lambda_{\ell -1}}\right)}}e^{-\frac{1}{2}\sum_{\mu\nu}h_{\mu} \left( \frac{\Phi_{\mu\nu}^{\ell -1}}{\lambda_{\ell -1}}\right)^{-1} h_{\nu}-\frac{1}{2}\sum_{\mu\nu}\phi(h_{\mu})\hat{\Phi}_{\mu\nu}^{\ell}\phi(h_{\nu})}.
    \end{split}
\end{equation}
At each layer, this decouples over the neuron index because we supposed the hidden layers having the same width dimension $N$ for $\ell=1,\ldots,L$, and represents the normalization factor of a non-Gaussian pre-activation density distribution $p(h_{\mu}^{\ell})$ where the non-Gaussian part is proportional to $\hat{\bm\Phi}^{\ell}$, while the Gaussian contribution has a covariance that is the feature kernel at previous layer $\bm \Phi^{\ell -1}$. In the large $N\to \infty$ limit, if we extremize the exponent in Eq.~\ref{eq::partition_func}, we get the saddle points

\begin{subequations}
    \begin{align}
        &\Phi_{\mu\nu}^{\ell} = \langle \phi (h_{\mu}^{\ell}) \phi (h_{\nu}^{\ell})\rangle \quad \forall \ell \in \{L\}\\
        &\hat{\Phi}_{\mu\nu}^{\ell} =\lambda_{\ell} (\Phi_{\mu\nu}^{\ell})^{-1} - \lambda_{\ell} \sum_{\alpha \beta}(\Phi_{\mu \alpha}^{\ell})^{-1} \langle h_{\alpha} h_{\beta}\rangle (\Phi_{\beta \nu}^{\ell})^{-1} \quad \forall \ell \in \{L-1\}\\
        &\hat{\Phi}_{\mu\nu}^L = -\frac{1}{\lambda_L} \hat{s}_{\mu} \hat{s}_{\nu}\\
        & \hat{s}_{\mu} =-\beta \gamma_0 \frac{\partial \mathcal{L}}{\partial s_{\mu}} \\
        & s_{\mu} =\frac{1}{\gamma_0 \lambda_L}\sum_{\nu}\Phi_{\mu\nu}^L \hat{s}_{\nu} 
    \end{align}
\end{subequations}
from which we get a kernel predictor on a unseen test point $\bm x$, since $f (\bm x) = \sigma\left( \frac{\beta}{\lambda_L}\sum_{\mu=1}^P \Delta_{\mu}\Phi^L (\bm x_{\mu}, \bm x) \right)$ being $\Delta_{\nu} = - \frac{\partial \mathcal{L}}{\partial s_{\nu}}$ the pattern error signal for that given loss.
\subsection{Regression problem}\label{appendix::bayes_regression}
In this specific case, where the form of the loss function is known $\mathcal{L} = \sum_{\mu =1}^P (y_{\mu}-f_{\mu})^2$ and the readout is linear, i.e. $f(s_{\mu}) = s_{\mu} \quad \forall \mu \in \{P\}$, we can integrate over the output pre-activations and its conjugated parameter $\{s_{\mu},\hat{s}_{\mu}\}$. After integrating, we obtain
\begin{equation}
    \begin{split}
        Z &=\int\prod_{\mu\nu}\prod_{\ell=1}^{L}\frac{d\Phi_{\mu\nu}^{\ell}d\hat{\Phi}_{\mu\nu}^{\ell}}{2\pi N^{-1}}e^{-N S(\{\Phi_{\mu\nu}^{\ell}, \hat{\Phi}_{\mu\nu}^{\ell}\}_{\ell =1}^L)}\\
        &=\int\prod_{\mu\nu}\prod_{\ell=1}^{L}\frac{d\Phi_{\mu\nu}^{\ell}d\hat{\Phi}_{\mu\nu}^{\ell}}{2\pi N^{-1}}e^{-N\sum_{\mu\nu}\sum_{\ell}\Phi_{\mu\nu}^{\ell}\hat{\Phi}_{\mu\nu}^{\ell}+\frac{N}{2}\sum_{\mu\nu}y^{\mu}\Big(\frac{\mathbb{I}_{\mu\nu}}{\beta}+\frac{\Phi_{\mu\nu}^{L}}{\lambda_{L}}\Big)^{-1}y^{\nu}-N\sum_{\ell=0}^{L-1}\ln\mathcal{Z}[\Phi_{\mu \nu}^{\ell -1},\hat{\Phi}_{\mu \nu}^{\ell}]}
    \end{split}
\end{equation}
where the important quantity to be extremized in the limit $N\to \infty$ is the intensive action 
\begin{equation}\label{eq::action}
    S(\Phi_{\mu\nu}^{\ell}, \hat{\Phi}_{\mu\nu}^{\ell}) = -\frac{1}{2}\sum_{\mu\nu}\sum_{\ell}\Phi_{\mu\nu}^{\ell}\hat{\Phi}_{\mu\nu}^{\ell}+\frac{\gamma_0^2}{2}\sum_{\mu\nu}y^{\mu}\Big(\frac{\mathbb{I}_{\mu\nu}}{\beta}+\frac{\Phi_{\mu\nu}^{L}}{\lambda_{L}}\Big)^{-1}y^{\nu}-\sum_{\ell=0}^{L-1}\ln\mathcal{Z}[\Phi_{\mu \nu}^{\ell -1},\hat{\Phi}_{\mu \nu}^{\ell}].
\end{equation}
Here, the saddle points which render the action $S$ locally stationary $\delta S = 0$ with respect to these $2L$ matrix order parameters can be collected as
\begin{subequations}\label{eq::sp}
    \begin{align}
        &\Phi_{\mu\nu}^{\ell}=\langle\phi(h_{\mu}^{\ell})\phi(h_{\nu}^{\ell})\rangle\quad\forall\ell=1,\ldots,L\\
        &\hat{\Phi}_{\mu\nu}^{\ell}=\frac{1}{\lambda_{\ell}}\langle\hat{h}_{\mu}^{\ell+1}\hat{h}_{\nu}^{\ell+1}\rangle= (\Phi_{\mu\nu}^{\ell})^{-1} - \sum_{\alpha \beta}\frac{1}{\lambda_{\ell}}\left(\frac{\Phi_{\mu\alpha}^{\ell}}{\lambda_{\ell}} \right)^{-1} \langle h_{\alpha} h_{\beta}\rangle \left(\frac{\Phi_{\beta\nu}^{\ell}}{\lambda_{\ell}}  \right)^{-1} \quad\forall\ell=1,\ldots,L-1\\
        &\hat{\Phi}_{\mu\nu}^{L}=-\frac{\gamma_0^2}{\lambda_{L}}\sum_{\alpha \beta}\Big(\frac{\mathbb{I}_{\mu\alpha}}{\beta}+\frac{\Phi_{\mu\alpha}^{L}}{\lambda_{L}}\Big)^{-1}y^{\alpha}y^{\beta}\Big(\frac{\mathbb{I}_{\beta\nu}}{\beta}+\frac{\Phi_{\beta\nu}^{L}}{\lambda_{L}}\Big)^{-1}
    \end{align}
\end{subequations}
being $\Phi_{\mu \nu}^0 = \frac{\boldsymbol{x}_\mu \cdot \boldsymbol{x}_\nu}{\lambda_0 D}$ the data matrix covariance in this notation. 

Notice that the last layer dual's kernel $\hat{\Phi}_{\mu\nu}^L$ vanishes in the lazy limit $\gamma_0 \to 0$ and so do all the dual kernels at previous layers $\hat{\Phi}_{\mu\nu}^{\ell} = 0$, while for non-negligible $\gamma_0$ we see that the each hidden layer features are non-Gaussian from Eq.~\eqref{eq::ss}. Details on the numerical implementation of the numerical solver for Eqs.~\eqref{eq::ss} can be found in Sec.~\ref{sec::minmax}.
\subsection{Generalization error}\label{appendix::gen_error}
Knowing the posterior distribution makes it easy to compute the test error on a new (unseen) example $(\bm x_0, y_0)$, which is defined as
\begin{equation}
    \epsilon_{g}(\boldsymbol{x}_{0},y_{0})=\langle(y_{0}-f_{0}(\boldsymbol{x}_{0};\theta))^{2}\rangle_{\bm \theta \sim p(\bm \theta |\mathcal{D})}
\end{equation}
where the sampling measure corresponds to Eq.~\eqref{eq::posterior}. Here, we can include with an index $\mu = 0$ the test pattern contribution, and just compute
\begin{equation}
    \begin{split}
        \epsilon_{g}(\boldsymbol{x}_{0},y_{0})&=\frac{1}{Z}\int\prod_{\mu\nu=0}^P\prod_{\ell=1}^{L}\frac{d\tilde{\Phi}_{\mu\nu}^{\ell}d\hat{\tilde{\Phi}}_{\mu\nu}^{\ell}}{2\pi N^{-1}}\int\prod_{\mu=0}^{P}\frac{ds_{\mu}d\hat{s}_{\mu}}{2\pi N_{L}^{-1}}e^{\sum_{\mu\nu=0}^P\sum_{\ell=1}^{L}N_{\ell}\tilde{\Phi}_{\mu\nu}^{\ell}\hat{\tilde{\Phi}}_{\mu\nu}^{\ell}+\frac{N_{L}}{2\lambda_{L}}\sum_{\mu,\nu=1}^{P}\Phi_{\mu\nu}^{L}\hat{s}_{\mu}\hat{s}_{\nu}+\frac{N_{L}}{2\lambda_{L}}\Phi_{00}^{L}(\hat{s}_{0})^{2}}\times \\
        &\quad \times e^{\frac{N_{L}}{\lambda_{L}}\hat{s}_{0}\sum_{\mu}\Phi_{\mu}\hat{s}_{\mu}-N_{L}\sum_{\mu=0}^{P}s_{\mu}\hat{s}_{\mu}-\frac{\beta}{2}N\sum_{\mu=1}^{P}\left(y^{\mu}-s^{\mu}\right)^{2}+N\sum_{\ell=0}^{L}\ln\tilde{\mathcal{Z}_{\ell}}}\times \left( y_0 - s_0 \right)^2
    \end{split}
\end{equation}
where the single-site action contains now all the possible interactions with the test point at each layer $\ell$ in the test-test kernel $\Phi_{00}^{\ell} = \langle\phi(h_{0})^{2}\rangle $ and the test-train kernel $\Phi_{\mu}^{\ell} = \langle \phi(h_{0})\phi(h_{\mu})\rangle $
\begin{equation}
    \begin{split}
    \tilde{\mathcal{Z}}&=\int\prod_{\mu=0}^{P}\frac{dh_{\mu}d\hat{h}_{\mu}}{2\pi}e^{i\sum_{\mu=0}^{P}h_{\mu}\hat{h}_{\mu}-\frac{1}{2}\sum_{\mu\nu=1}^{P}\hat{h}_{\mu}\frac{\Phi_{\mu\nu}^{\ell-1}}{\lambda_{\ell-1}}\hat{h}_{\nu}-\sum_{\mu\nu=1}^{P}\phi(h_{\mu})\hat{\Phi}_{\mu\nu}^{\ell}\phi(h_{\nu})-\frac{1}{2}\frac{\Phi_{00}^{\ell-1}}{\lambda_{\ell-1}}\hat{h}_{0}^{2}-\hat{\Phi}_{00}^{\ell}\phi(h_{0})^{2}-\sum_{\mu=1}^{P}\frac{\Phi_{\mu}^{\ell-1}}{\lambda_{\ell-1}}\hat{h}_{0}\hat{h}_{\mu}}\\
    &\quad \times e^{-\sum_{\mu=1}^{P}\hat{\Phi}_{\mu}^{\ell}\phi(h_{0})\phi(h_{\mu})}.
    \end{split}
\end{equation}
Exploiting the saddle point equations, we realize that the dual kernels concerning the test point $\hat{\Phi}_{\mu}^{\ell}, \hat{\Phi}_{00} = 0$, and that
\begin{subequations}
    \begin{align}
        &\Phi_{\mu\nu}^{\ell}=\langle\phi(h_{\mu})\phi(h_{\nu})\rangle\quad\forall\ell=1,\ldots,L\\
        &\hat{\Phi}_{\mu\nu}^{\ell}=\frac{1}{2\lambda_{\ell}}\langle\hat{h}_{\mu}^{\ell+1}\hat{h}_{\nu}^{\ell+1}\rangle\quad\forall\ell=1,\ldots,L-1\\
        &\hat{\Phi}_{\mu\nu}^{L}+\frac{1}{2\lambda_{L}}\hat{s}_{\mu}\hat{s}_{\nu}=0\\
        &\hat{\Phi}_{\mu}^{\ell}=\frac{1}{\lambda_{\ell}}\langle\hat{h}_{0}^{\ell+1}\hat{h}_{\mu}^{\ell+1}\rangle\quad\forall\ell=1,\ldots,L-1\\
        &\hat{s}_{0}=\hat{\Phi}_{\mu}^{\ell}= \hat{\Phi}_{00}=0\\
        &s_{0}=\frac{1}{\lambda_{L}}\sum_{\mu}\Phi_{\mu}^{L}\hat{s}_{\mu}\\
        &\hat{s}_{\mu}-\beta\Big(y_{\mu}-s_{\mu})=0\\
        &s_{\mu}=\frac{1}{\lambda_{L}}\sum_{\nu}\Phi_{\mu\nu}^{L}\hat{s}_{\nu}.
    \end{align}
\end{subequations}
This allows to rewrite the single site density in a much simpler form, where the non-Gaussian contribution includes just the train points, while the Gaussian part has a $(P+1) \times (P+1)$ covariance matrix $\tilde{\Phi} = 
\begin{pmatrix}
\Phi_{00} & \Phi_{\mu}^{\top} \\
\Phi_{\mu} & \Phi_{\mu\nu}
\end{pmatrix}$
\begin{equation}\label{eq::single_site_appendix}
    \mathcal{\tilde{Z}} = \int \frac{\prod_{\mu}dh_{\mu}}{\sqrt{2\pi\det \left( \frac{\tilde{\Phi}^{\ell -1}}{\lambda_{\ell -1}}\right)}}e^{-\frac{1}{2}\sum_{\mu\nu=0}^P h_{\mu} \left( \frac{\tilde{\tilde{\Phi}}_{\mu\nu}^{\ell -1}}{\lambda_{\ell -1}}\right)^{-1} h_{\nu}-\frac{1}{2}\sum_{\mu\nu=1}^P\phi(h_{\mu})\hat{\Phi}_{\mu\nu}^{\ell}\phi(h_{\nu})}
\end{equation}
This means that once we solved for Eq.~\eqref{eq::sp} we can marginalize Eq.~\eqref{eq::single_site_appendix} to get $p(h_0^{\ell}|\bm h^{\ell})$ and hence the test-train vector kernel $\bm \Phi_{\mu}$. The test error expression is 
\begin{equation}
    \epsilon_g (\bm x_0,y_0 ) =\Big(y_{0}-\frac{1}{\lambda_{L}}\sum_{\mu\nu}\Phi_{\mu}^{L}\Big[\frac{\Phi_{\mu\nu}^{L}}{\lambda_{L}}+\frac{\mathbb{I}_{\mu\nu}}{\beta}\Big]^{-1}y_{\nu}\Big)^{2}
\end{equation}
meaning that the predictor $f_{\mu} = \frac{1}{\lambda_{L}}\sum_{\mu\nu}\Phi_{\mu}^{L}\Big[\frac{\Phi_{\mu\nu}^{L}}{\lambda_{L}}+\frac{\mathbb{I}_{\mu\nu}}{\beta}\Big]^{-1}y_{\nu} $ is again a kernel predictor, with adaptive kernels from Eqs.~\eqref{eq::sp}. 

\subsection{Deep linear case}\label{appendix::dln}
In the deep linear case $\phi(h^{\ell}) = h^{\ell}$, the action of Eq.~\eqref{eq::action} gets simplified because the single-site density is now Gaussian, and this leads to  
\begin{equation}
    S(\{\Phi_{\mu\nu}^{\ell}, \hat{\Phi}_{\mu\nu}^{\ell}\})=-\frac{1}{2}\sum_{\mu\nu}\sum_{\ell}\Phi_{\mu\nu}^{\ell}\hat{\Phi}_{\mu\nu}^{\ell}+\frac{\gamma_{0}^{2}}{2}\sum_{\mu\nu}y^{\mu}\Big(\frac{\mathbb{I}_{\mu\nu}}{\beta}+\frac{\Phi_{\mu\nu}^{L}}{\lambda_{L}}\Big)^{-1}y^{\nu}+\frac{1}{2}\sum_{\ell=1}^{L}\ln\mathcal{\det}\Big(\mathbb{I_{\mu\nu}}+\frac{\Phi_{\mu\nu}^{\ell-1}}{\lambda_{\ell-1}}\hat{\Phi}_{\mu\nu}^{\ell}\Big)
\end{equation}
with the following saddle point equations
\begin{subequations}
    \begin{align}
        &\Phi^{\ell}-\frac{\Phi^{\ell-1}}{\lambda_{\ell-1}}\Big(\mathbb{I}+\frac{\Phi^{\ell-1}}{\lambda_{\ell-1}}\hat{\Phi}^{\ell}\Big)^{-1}=0\qquad\forall\ell=1,\ldots,L\\
        &\hat{\Phi}^{\ell}-\frac{\hat{\Phi}^{\ell+1}}{\lambda_{\ell}}\Big(\mathbb{I}+\frac{\Phi^{\ell}}{\lambda_{\ell}}\hat{\Phi}^{\ell+1}\Big)^{-1}=0\qquad\forall\ell=1,\ldots,L-1\\
        &\hat{\Phi}^{L}+\frac{\gamma_{0}^{2}}{\lambda_{L}}\Big(\frac{\mathbb{I}}{\beta}+\frac{\Phi^{L}}{\lambda_{L}}\Big)^{-1}yy^{\top}\Big(\frac{\mathbb{I}}{\beta}+\frac{\Phi^{L}}{\lambda_{L}}\Big)^{-1}=0.
    \end{align}
\end{subequations}
In principle, this is a closed set of equations, which can be iteratively solved as mentioned in Sec.~\ref{sec::dln}. If we choose input data that are whitened, with $\bm \Phi^0 = \bm K^x = \bm I $ and label norm $|\bm y| = 1$, the equations can be simplified even further, since feature kernels at each layer $\bm \Phi^{\ell}$ only evolve in the rank-one direction $\bm y \bm y^{\top}$. This allows to define the variables $\{ c_\ell, \hat c_\ell \}$ which are the overlaps with the label direction $\bm y$ 
\begin{align}
    \bm y^\top \bm\Phi^\ell \bm y \equiv c^\ell  \ , \ \bm y^\top \bm{\hat \Phi}^\ell \bm y \equiv \hat c^\ell.  
\end{align}
%
In this setting, the reduced saddle point equations become
\begin{align}
    c_1 &= \frac{1}{1 + \hat c_1}  \ , \ c_{\ell+1} = \frac{c_{\ell}}{1 + c_\ell \ \hat c_{\ell+1} } 
    \\
    \hat c_L &= - \frac{\gamma^2}{(\beta^{-1} + c_L)^2} \ , \ \hat{c}_\ell = \frac{\hat c_{\ell+1}}{1 + c_\ell \  \hat{c}_{\ell+1} }
\end{align}
%
which are $2L$ scalar equations for the overlaps $c_{\ell}$ at each layer. We note the following conservation
\begin{align}
    c_{\ell} \hat c_\ell = c_{\ell+1} \hat c_{\ell+1} = - \frac{\gamma^2 c_L }{( \beta^{-1} + c_L )^2 } \equiv \chi(c_L)
\end{align}
which implies that 
\begin{align}
    1 = \frac{ c_\ell }{ c_{\ell+1} + c_\ell \chi(c_L) } \implies  c_{\ell+1} = c_\ell \left( 1 - \chi(c_L)  \right) = \left( 1 - \chi(c_L) \right)^{\ell} c_1.
\end{align}
Since we have $ c_1 = \frac{1}{1 + \chi / c_1} \implies c_1 = 1-\chi(c_L)$, hence we find 
\begin{align}\label{eq::dlns}
    c_\ell = \left( 1 - \chi(c_L) \right)^\ell 
\end{align}
which means that here is an exponential dependence of the overlap on layer index. In practice, we can solve Eq.~\eqref{eq::dlns} for the last layer overlap $c_L$ since $\chi (c_L)$ and then move backward in computing all the previous layer overlaps. 

We can also extract the following small $\gamma$ or small $L$ asymptotics. Precisely, when $L$ is fixed and $\gamma_0 \to 0$ we recover a perturbative feature learning regime
\begin{align}
    c_L \sim 1 + \frac{L \gamma^2}{c_L} \implies c_L \sim 1 + L \gamma^2 \ , \  \gamma^2 L \to 0
\end{align}
where correction are $\mathcal{O}(\gamma^2 L)$. Similarly, at large $\gamma$ with fixed $L$, which stands for a shallow but very rich regime, the overlaps scale as
\begin{align}
    &c = \left[ 1 + \gamma^2 c^{-1} \right]^L  \implies c \sim \gamma^{2 L /(L + 1)}
\end{align}
while, alternatively the large $L$ asymptotics for a very deep network have the form
\begin{equation}
    c^{1+1/L} = c+\gamma^2 \implies c\ln c \sim L \gamma^2 \implies c\sim \frac{L\gamma^2}{\ln (L\gamma^2)}
\end{equation}
    which we show to be predictive in Fig.\ref{fig::fig1} of the main text. 
\subsection{Non-gaussian pre-activation density}
In the non-linear case, as Eq.~\eqref{eq::ss} shows, when $\gamma_0 >0$ the pre-activation density at each layer is non-Gaussian, with $\gamma_0$ entering in the saddle point equation for the dual kernel at the last layer $\hat{\bm \Phi}^L$ (see Eq.~\eqref{eq::sp}). This means that, once we have $\{\bm\Phi^{\ell}, \hat{\bm \Phi}^{\ell}\}_{\ell = 1}^L$ from the solver (Alg~\ref{alg:kernel_convergence}), we can evaluate Eq.~\eqref{eq::ss} with importance sampling and compute $p(h_{\mu})$ for a given pattern $\mu$. In Fig.~\ref{fig::preact}(a) we show that, while in the lazy regime where no feature learning enters, the hidden layer pre-activation of the NNGP predictors are Gaussian, this is not the case in our setting. 
\iffalse
    \begin{subequations}
        \begin{align}
        \Phi_{\mu \nu} &= \langle \phi (h^{\mu})\phi(h^{\nu})\rangle\\
        \hat{\Phi}_{\mu \nu } &= -\frac{\gamma_0^2}{2\lambda_1}\sum_{\alpha,\beta} \Big(\frac{\mathbb{I}_{\mu \alpha}}{\beta} + \frac{\Phi_{\mu \alpha}}{\lambda_1} \Big)^{-1}y^{\alpha}y^{\beta} \Big( \frac{\mathbb{I}_{\beta \nu}}{\beta} + \frac{\Phi_{\beta \nu}}{\lambda_1}\Big)^{-1} 
        \end{align}
    \end{subequations}
and $\langle \bullet \rangle  = \frac{\int \frac{\prod_{\mu}dh^{\mu}}{\sqrt{2\pi \text{det}C}}e^{-\sum_{\mu \nu}\hat{\Phi}_{\mu \nu}\phi(h^{\mu})\phi(h^{\nu})-\frac{1}{2}\sum_{\mu \nu}h^{\mu}(C_{\mu \nu})^{-1}h^{\nu}}\bullet}{\mathcal{Z}}$.
\fi
\begin{figure}\label{fig::preact}
    \centering
    \subfigure[]{
    \includegraphics[width=0.45\linewidth]{figures/preact_bayes_thVSexp_final.png}}
    \subfigure[]{
    \includegraphics[width=0.45\linewidth]{figures/bayesian_densities.pdf}}
    \caption{(a) Bayesian two-layer MLP trained on a whitened covariance matrix $\bm \Phi^0 = \bm I$ on $P=4$ train points. Feature learning ($\gamma_0 >0$) leads to a non-Gaussian pre-activation distributions. Black-dashed curve is the lazy NNGP when $\bm h \sim \mathcal{N}(0, \bm \Phi^0)$; \textit{red} curve is the aNBK theory when $\bm h$ is sampled from Eq.~\ref{eq::preact_distr}; \textit{blue} is the empirical pre-activation distribution of a $N=5000$ network trained in the rich regime.  (b) Non-gaussian pre-activation distribution as a function of feature learning strength $\gamma$ for a Bayesian 2-layer MLP trained with Squared Error (SE) on 0-1 classes of MNIST dataset. Here sample size $P=100$.}
\end{figure}
\subsection{Perturbative approximation}
In the $\gamma_0 \to 0$ limit, we recover the static kernels of NNGP predictor~\cite{neal, lee2018deepneuralnetworksgaussian}. Corrections to this lazy limit can be extracted at small but finite $\gamma_0$. In order to do so, we can expand each macroscopic variable $q(\gamma_0)$ in power series of $\gamma_0$, such as $q =  q^{(0)} + \gamma_0^2 q^{(1)} + \gamma_0^4 q^{(2)} + \ldots  $, and compute the corrections up to $\mathcal{O}(\gamma_0^2)$. First of all, we notice that at leading order in $\gamma^2_0$
\begin{align}
    \hat{\bm\Phi}^L &= - \gamma_0^2 \left( \bm\Phi_0^L + \beta^{-1} \right)^{-1} \bm y \bm y^\top  \left( \bm\Phi_0^L + \beta^{-1} \right)^{-1}
\end{align}
where we set each $\lambda_{\ell} = 1$ for clarity of notation. For each dual kernel at previous layer $\ell = 1,\ldots, L-1$ we have instead a recursion
\begin{align}
    \frac{1}{2} \hat\Phi^{\ell} = - \frac{\partial}{\partial \Phi^\ell} \ln \mathcal{Z}(\Phi^{\ell}, \hat{\Phi}^{\ell +1})
\end{align}
where non-perturbatively
\begin{align}
    \frac{1}{2} \hat\Phi^{\ell} &= - \frac{1}{\mathcal Z} \int dh \frac{\partial}{\partial \bm\Phi} \exp\left( - \frac{1}{2} h (\Phi^{\ell})^{-1} h -  \frac{1}{2} \phi(h) \hat\Phi^{\ell+1} \phi(h)  \right)
    \\
    &= \frac{1}{2} \frac{1}{ \left< \exp\left( -\frac{1}{2} \phi(h) \hat\Phi^{\ell +1} \phi(h)  \right) \right>_0  } \times  \left< \frac{\partial^2}{\partial \h^2 } \exp\left( -\frac{1}{2} \phi(h) \hat\Phi^{\ell +1} \phi(h)  \right) \right>_0 
\end{align}
being $\left< \right>_0$ the Gaussian average with covariance $\Phi^{\ell}$. With a little bit of algebra the numerator can be written as
\begin{align}
    \left< \frac{\partial^2}{\partial h_\mu \partial h_\nu } \exp\left( -\frac{1}{2} \phi(h) \hat\Phi \phi(h)  \right) \right>_0
    &= -  \left< \frac{\partial}{\partial h_\nu } \left[\exp\left( -\frac{1}{2} \phi(h) \hat\Phi \phi(h)  \right)  \dot\phi(h_\mu) \hat{\Phi}_{\mu\alpha} \phi(h_\alpha) \right] \right>_0 \nonumber
    \\
    &=  \sum_{\alpha\beta} \hat\Phi_{\mu\alpha } \hat\Phi_{\nu \beta} \left< \dot\phi(h_\mu)\dot\phi(h_\nu)\phi(h_\alpha) \phi(h_\beta)  \exp\left( -\frac{1}{2} \phi(h) \hat\Phi \phi(h)  \right) \right>_0 \nonumber
    \\
    &- \delta_{\mu\nu} \sum_{\alpha} \hat\Phi_{\mu\alpha }  \left< \ddot\phi(h_\mu) \phi(h_\alpha) \exp\left( -\frac{1}{2} \phi(h) \hat\Phi \phi(h)  \right) \right>_0 \nonumber
    \\
    &- \left< \dot\phi(h_\mu)\dot\phi(h_\alpha) \exp\left( -\frac{1}{2} \phi(h) \hat\Phi \phi(h)  \right)  \right>_0 \hat\Phi_{\mu\alpha}.
\end{align}
Under the leading order approximation, we find the following relationship between successive layers
\begin{align}
     \hat{\bm\Phi}^{\ell}
     &\sim \frac{1}{2} \left< \frac{\partial^2}{\partial \h \partial \h^\top} \phi(\h)^\top \hat{\bm\Phi}^{\ell+1} \phi(\h)  \right> 
\end{align}
The entries of this Hessian matrix can be computed in terms of derivatives of the activation function
\begin{align}
    \frac{\partial }{ \partial h_\mu } \frac{\partial }{ \partial h_\nu } \sum_{\alpha \beta }  \phi(h_\alpha) \phi(h_\beta) \hat\Phi &=  \frac{\partial }{ \partial h_\mu }  \left[ \dot\phi(h_\nu) \phi(h_\beta) \hat\Phi_{\nu\beta} + \dot\phi(h_\nu) \phi(h_\alpha) \hat\Phi_{\nu \alpha} \right] 
    \\
    &= 2 \left<\dot\phi(h_\mu) \dot\phi(h_\nu) \right> \hat\Phi_{\mu\nu }  + 2 \delta_{\mu \nu}\left<  \ddot\phi(h_\mu) \sum_\beta \phi(h_\beta) \right>  \hat\Phi_{\mu\beta}  
\end{align}
all of these Gaussians can be evaluated at the unperturbed NNGP kernels Gaussian densities. Once these $\hat{\bm\Phi}^\ell$ matrices have been computed, the $\bm\Phi^\ell$ matrices can be asymptotically approximated as
\begin{align}
    \bm\Phi^\ell \sim \bm\Phi^\ell_0 - \frac{1}{2} \left< \phi(\h)\phi(\h)^\top  \left(  \phi(\h)^\top \hat{\bm\Phi}^{\ell}  \phi(\h) \right)   \right>_0 - \bm\Phi^{\ell}_0  \left< \phi(\h)^\top \bm{\hat \Phi}^\ell \phi(\h) \right>_0
\end{align}
which agree with the perturbative treatment of \cite{Zavatone_Veth_2022} obtained in the NTK parameterization which is similar to our small $\gamma_0$ expansion (neglecting finite width fluctuations). 

\section{Finite-width effects }
In principle, in $\mu$P, the kernels $\Phi^\ell_{\mu\nu}$ and predictions $f_\mu$ at width $N$ exhibit $\mathcal{O}\left(\frac{1}{\sqrt N} \right)$ fluctuations around their limiting values \cite{bordelon2024dynamics}. Because these fluctuations generically add variance to the predictor, they increase the test loss compared to the large $N$ limit as we show in Figure \ref{fig:finite_width_effect}. Instead, in NTK/standard parameterization from which the NNGP was derived~\cite{neal,lee2018deepneuralnetworksgaussian}, 
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/test_loss_finitewidth_final.png}
    \caption{Finite-width effects in NTK and $\mu$P parameterizations. \textit{Blue} curves are for different $N$ values, showing that NNGP predictor is not consistent across network widths. We compare Langevin dynamics in width $N$ networks to the NNGP infinite width limit. Finite width effects are instead negligible in $\mu$P parameterization, where finite $N$ networks consistently lie on the theory full \textit{orange} and \textit{red} curves. }
    \label{fig:finite_width_effect}
\end{figure}

When $N$ is comparable to $P$ in either parameterization, the kernels $\bm\Phi^\ell$ should actually be thought of as \textit{random matrices} with significant deformations to their spectra compared to the $N \to \infty$ limit. 
\newpage
\section{Deep Bayesian CNNs}\label{appendix::CNNs}
In this section, we describe the Bayesian posterior of a Deep convolutional neural network (CNN) for infinitely many channels. Here, we need to add an index $a$ for each weight $W_{ija}^{\ell}$ in order to account for the filter value at spacial displacement $a$ from the filter center at each layer, while $\mathcal{S}^{\ell}$ is the spatial receptive field at layer $\ell$. The $L-1$ hidden layers of the CNN can be expressed as
\begin{subequations}
    \begin{align}
        h_{\mu ia}^{1}&=\frac{1}{\sqrt{D}}\sum_{j=1}^{D}\sum_{b\in\mathcal{S}^{0}}W_{ijb}^{0}x_{\mu,j,a+b}\\
        h_{\mu ia}^{\ell+1}&=\frac{1}{\sqrt{N}}\sum_{j=1}^{N}\sum_{b\in\mathcal{S}^{\ell}}W_{ijb}^{\ell}\phi(h_{\mu,j,a+b}^{\ell})\\
        f_{\mu}&=\frac{1}{\gamma_{0}N}\sum_{i=1}^{N}\sum_{a}w_{ia}^{L}\phi(h_{\mu ia}^{L})
    \end{align}
\end{subequations}
From these definitions, the partition function turns out to be
\begin{equation}
        Z=\int\prod_{\ell=0}^{L-1}\prod_{ijb}dW_{ijb}^{\ell}\prod_{ia}dw_{ia}^{L}e^{-\frac{\beta}{2}\gamma_{0}N^{2}\sum_{\mu}(y_{\mu}-\frac{1}{\gamma_{0}N}\sum_{i=1}^{N}\sum_{a}w_{ia}^{L}\phi(h_{\mu ia}^{L}))^{2}+\frac{\lambda}{2}\sum_{\ell=0}^{L-1}\sum_{ijb}(W_{ijb}^{\ell})^{2}+\frac{\lambda}{2}\sum_{ia}(w_{ia}^{L})^{2}}
\end{equation}
By imposing the pre-activation definitions with the use of the integral representation of some Dirac delta functions as we did in Eq.~\eqref{eq::deltas}, we can integrate out the weights contribution and just move in the space of representations, and get
\begin{equation}
    \begin{split}
        Z&=\int\prod_{\ell=0}^{L-1}\prod_{\mu ia}\frac{dh_{\mu ia}^{\ell+1}d\hat{h}_{\mu ia}^{\ell+1}}{2\pi}\int\prod_{\mu}\frac{ds_{\mu}d\hat{s}_{\mu}}{2\pi}e^{i\sum_{\ell}\sum_{\mu ia}h_{\mu ia}^{\ell+1}\hat{h}_{\mu ia}^{\ell+1}+i\sum_{\mu}s_{\mu}\hat{s}_{\mu}} \int\prod_{\ell}\prod_{ijb}dW_{ijb}^{\ell}\prod_{ia}dw_{ia} e^{-\frac{\beta}{2}\gamma_{0}N^{2}\sum_{\mu}(y_{\mu}-s_{\mu})^{2}}\\
        &\quad \times e^{\frac{\lambda}{2}\sum_{\ell}\sum_{ijb}(W_{ijb}^{\ell})^{2}+\frac{\lambda}{2}\sum_{ia}(w_{ia}^{L})^{2}-i\sum_{\ell}\sum_{\mu ia}\hat{h}_{\mu ia}^{\ell+1}\Big(\frac{1}{\sqrt{N}}\sum_{j=1}^{N}\sum_{b\in\mathcal{S}^{\ell}}W_{ijb}^{\ell}\phi(h_{\mu,j,a+b}^{\ell})\Big)-i\sum_{\mu}\hat{s}_{\mu}\Big(\frac{1}{\gamma_{0}N}\sum_{i=1}^{N}\sum_{a}w_{ia}^{L}\phi(h_{\mu ia}^{L})\Big)}
    \end{split}
\end{equation}
\begin{equation}
\begin{split}
     Z&=\int\prod_{\ell=1}^{L-1}\prod_{\mu\nu}\prod_{aa'}\frac{d\Phi_{\mu\nu,aa'}^{\ell}d\hat{\Phi}_{\mu\nu,aa'}^{\ell}}{2\pi}\int\prod_{\mu\nu}\frac{d\Phi_{\mu\nu}^{L}d\hat{\Phi}_{\mu\nu}^{L}}{2\pi}e^{N\sum_{\ell=1}^{L-1}\sum_{\mu\nu,aa'}\Phi_{\mu\nu,aa'}^{\ell}\hat{\Phi}_{\mu\nu,aa'}^{\ell}+N\sum_{\mu\nu}\Phi_{\mu\nu}^{L}\hat{\Phi}_{\mu\nu}^{L}}\\
        &\quad \times \int\prod_{\mu}\frac{ds_{\mu}d\hat{s}_{\mu}}{2\pi}e^{+i\gamma_{0}N\sum_{\mu}s_{\mu}\hat{s}_{\mu}-\frac{\beta}{2}\gamma_{0}N^{2}\sum_{\mu}(y_{\mu}-s_{\mu})^{2}-\frac{1}{2}\sum_{\mu\nu}\hat{s}_{\mu}\hat{s}_{\nu}\frac{1}{\lambda}\Phi_{\mu\nu}^{L}}\\
        &\quad \times \Big[\int\prod_{\ell=1}^{L}\prod_{\mu a}\frac{dh_{\mu a}^{\ell}d\hat{h}_{\mu a}^{\ell}}{2\pi}e^{\sum_{\ell=1}^{L}\sum_{\mu a}h_{\mu a}^{\ell}\hat{h}_{\mu a}^{\ell}-\frac{1}{2}\sum_{\ell=1}^{L}\sum_{\mu\nu}\sum_{aa'}\hat{h}_{\mu a}^{\ell}\hat{h}_{\nu a'}^{\ell}\frac{\Phi_{\mu\nu,aa'}^{\ell-1}}{\lambda}}e^{-\sum_{\ell=1}^{L-1}\sum_{\mu\nu,aa'}\hat{\Phi}_{\mu\nu,aa'}^{\ell}\Big(\sum_{b}\phi(h_{\mu,a+b}^{\ell})\phi(h_{\nu,a'+b}^{\ell})\Big)}\\
        &\qquad e^{-\sum_{\mu\nu}\hat{\Phi}_{\mu\nu}^{L}\Big(\sum_{a}\phi(h_{\mu a}^{L})\phi(h_{\nu a}^{L})\Big)}\Big]^{N}\\
        &=\int\prod_{\ell=1}^{L-1}\prod_{\mu\nu,aa'}\frac{d\Phi_{\mu\nu,aa'}^{\ell}d\hat{\Phi}_{\mu\nu,aa'}^{\ell}}{2\pi}\int\prod_{\mu\nu}\frac{d\Phi_{\mu\nu}^{L}d\hat{\Phi}_{\mu\nu}^{L}}{2\pi}e^{N\sum_{\ell=1}^{L-1}\sum_{\mu\nu,aa'}\Phi_{\mu\nu,aa'}^{\ell}\hat{\Phi}_{\mu\nu,aa'}^{\ell}+N\sum_{\mu\nu}\Phi_{\mu\nu}^{L}\hat{\Phi}_{\mu\nu}^{L}}
        \\&\quad \times e^{-\gamma_{0}^{2}\frac{N}{2}\sum_{\mu\nu}y^{\mu}\Big(\frac{\mathbb{I}_{\mu\nu}}{\beta}+\frac{\Phi_{\mu\nu}^{L}}{\lambda_{L}}\Big)^{-1}y^{\nu}+N\ln\mathcal{Z}}
    \end{split}
\end{equation}
With saddle point equations
\begin{subequations}
    \begin{align}
        &\Phi_{\mu\nu,aa'}^{\ell}=\langle\sum_{b}\phi(h_{\mu,a+b}^{\ell})\phi(h_{\nu,a'+b}^{\ell})\rangle\quad\forall\ell=1,\ldots,L-1\\
        &\hat{\Phi}_{\mu\nu,aa'}^{\ell}=\frac{1}{2\lambda}\langle\hat{h}_{\mu a}^{\ell}\hat{h}_{\nu a'}^{\ell}\rangle\quad\forall\ell=1,\ldots,L-1\\
        &\Phi_{\mu\nu}^{L}=\langle\sum_{a}\phi(h_{\mu a}^{L})\phi(h_{\nu a}^{L})\rangle\\
        &\hat{\Phi}_{\mu\nu}^{L}=-\frac{\gamma_{0}^{2}}{2\lambda}\sum_{\alpha \beta}\Big(\frac{\mathbb{I}_{\mu \alpha}}{\beta} + \frac{\Phi_{\mu \alpha}}{\lambda}\Big)^{-1}y_{\alpha}y_{\beta}\Big(\frac{\mathbb{I}_{\mu \alpha}}{\beta} + \frac{\Phi_{\mu \alpha}}{\lambda}\Big)^{-1}
    \end{align}
\end{subequations}
\section{DMFT review}\label{appendix::dmft}
In this section, we briefly recall the dynamical mean field theory derivation of a gradient flow dynamics for a multi-layer fully connected neural network. As clarified in the main text, we are interested in a fully connected feedforward network with $L$ layers, defined as
\begin{equation}\label{eq::def}
    f_{\mu} =\frac{1}{\gamma \sqrt{N}_{L}}\boldsymbol{w}^{(L)}\cdot\phi (\boldsymbol{h}_{\mu}^{L}),\qquad \boldsymbol{h}_{\mu}^{\ell+1}	=\frac{1}{\sqrt{N_{\ell}}}\boldsymbol{W}^{\ell}\phi(\boldsymbol{h}_{\mu}^{\ell}), \qquad \boldsymbol{h}_{\mu}^{1}	=\frac{1}{\sqrt{D}}\boldsymbol{W}^{(0)}\boldsymbol{x}_{\mu}
\end{equation}
where each trainable parameter $\bm W^{\ell}$ is initialized as a Gaussian random variable $W_{ij}^{\ell}\sim \mathcal{N}(0,1)$ with unit variance. Here, the gradient updates for the weights $\boldsymbol{W}^{\ell}(t)$ and the output function $f_{\mu}$ are given by
\begin{equation}
    \frac{d \boldsymbol{W}^{\ell}(t)}{dt} = -\frac{\gamma^2}{N}\sum_{\mu} \Delta_{\mu}(t)\boldsymbol{g}_{\mu}^{\ell +1}(t) \phi(\boldsymbol{h}_{\mu}^{\ell}(t))^{\top} -\lambda \boldsymbol{W}^{\ell}(t)\,, \,\,\frac{d f_{\mu}}{dt} = \sum_{\alpha=1}^P K^{\text{NTK}}_{\mu \alpha}(t,t)\Delta_{\alpha} (t) -\lambda \kappa f_{\mu}
\end{equation}
where $\Delta_{\mu} (t) = -\frac{\partial \mathcal{L}}{\partial f_{\mu}(t)}$ represents the pattern error signal for a pattern $\mu$, and $\boldsymbol{g}_{\mu}^{\ell} (t) =  \frac{\partial \boldsymbol{h}_{\mu}^{L+1}(t)}{\partial \boldsymbol{h}_{\mu}^{\ell}(t)} $ captures the backpropagated gradients flowing from the downstream layers. Instead, the term $\phi(\boldsymbol{h}_{\mu}^{\ell}(t))$ involves the activations of the $\ell$-th layer and reflects the forward pass contribution to the weight update, which is proportional to $\gamma^2 = \gamma_0^2 N $ so that in the infinite width limit $N\to \infty$ the pre-activation updates of Eq.~\eqref{eq::def} remain $\Theta_N(1)$.

As specified in the main text, for the representer theorem to be valid in this case, we restrict to $\kappa$ degree homogeneous network, whose output scales as $f (a \boldsymbol{\theta}) = a^{\kappa} f(\boldsymbol{\theta})$. Here, the predictor dynamics is governed by the driving force of the error signal propagated through the \textit{adaptive Neural Tangent Kernel} $K^{\text{aNTK}}_{\mu\alpha}(t,t') = \frac{\partial f_{\mu}(t)}{\partial \boldsymbol{\theta}}\cdot \frac{\partial f_{\alpha}(t')}{\partial \boldsymbol{\theta}}$. This quantifies the interaction between parameter gradients for outputs of pattern pairs $(\mu, \nu )$ at times $(t,t')$. The homogeneity factor $\kappa$ appears in the second term from the weight decay contribution. Some quantities of interest to define are the forward and gradient kernels at each layer
\begin{equation}
    \Phi_{\mu\nu}^{\ell}(t,t') = \frac{1}{N}\phi (\boldsymbol{h}_{\mu}^{\ell}(t))\cdot \phi (\boldsymbol{h}_{\nu}^{\ell}(t'))\,, \quad G_{\mu\nu}^{\ell}(t,t') = \frac{1}{N}\boldsymbol{g}_{\mu}^{\ell}(t)\cdot \boldsymbol{g}_{\nu}^{\ell}(t')
\end{equation}
which allows to rewrite $K^{\text{aNTK}}_{\mu\nu}(t,t') = \sum_{\ell=0}^L G_{\mu\nu}^{\ell +1}(t,t') 
 \Phi^{\ell}_{\mu\nu}(t,t')$. If we take care of the initial conditions over the weights, in the DMFT limit where $N, \gamma \to \infty $ with $\gamma = \gamma_0 \sqrt{N}$, we can determine the final kernels by solving the field dynamics
 \begin{equation}\label{eq::dmft_dyn}
     \begin{split}
         &h_{\mu}^{\ell}(t) = e^{-\lambda t} \xi_{\mu}^{\ell}(t) + \gamma_0 \int_0^t dt' \, e^{-\lambda (t-t')}\sum_{\nu} \Delta_{\nu}(t') \,g_{\nu}^{\ell}(t') \,\Phi^{\ell -1}_{\mu\nu}(t,t') \\
         &z_{\mu}^{\ell}(t) = e^{-\lambda t} \psi^{\ell}_{\mu}(t) + \gamma_0 \int_0^t dt' \,e^{-\lambda (t-t')}\sum_{\nu} \Delta_{\nu}(t')\phi (h^{\ell}_{\nu}(t'))G_{\mu\nu}^{\ell +1}(t,t').
     \end{split}
 \end{equation}
 In Eq.~\eqref{eq::dmft_dyn}, both pre-activations $h^{\ell}_{\mu}(t)$ and pre-gradient signals $z^{\ell}_{\mu} = \frac{1}{\sqrt{N}}W^{\ell} g_{\mu}^{\ell +1}$ decouple over the neuron index and factorize over the layer index, and the contribution from initial conditions $\xi^{\ell}_{\mu}(t) = \frac{1}{\sqrt{N}} W^{\ell}(0)\phi(h^{\ell -1}_{\mu})(t)$ and $\psi^{\ell}_{\mu}(t)=\frac{1}{\sqrt{N}}W^{\ell}(0) g_{\mu}^{\ell +1}(t)$ is exponentially suppressed at large time $t$. Simulating a stochastic process like Eq.~\eqref{eq::dmft_dyn} requires keeping track of the entire history trajectory, and computing at each step produces of kernel matrices that have dimension $PT \times PT$. This scales cubically in both sample and time dimensions $\mathcal{O}_N(P^3 T^3)$, allowing in principle to solve for the field dynamics when $P,T= \mathcal{O}_N (1)$. A sketch of an iterative algorithm procedure can be found in Algorithm \ref{alg:kernel_convergence}, where given an initial guess on $\{\bm \Phi^{\ell}, \bm G^{\ell}\}_{\ell = 1}^L$ one can compute $\bm K^{\text{aNTK}}$ and solve for the predictor dynamics of Eq.~\eqref{eq::def} once drawn a number $\mathcal{S}$ of samples $\{\xi_{\mu,n}^{\ell}(t)\}_{n=1}^{\mathcal{S}} \sim \mathcal{N}(0, \bm \Phi^{\ell -1})$, $\{\psi_{\mu,n}^{\ell}(t)\}_{n=1}^{\mathcal{S}}\sim \mathcal{N}(0, \bm G^{\ell+1})$ and solved Eq.~\eqref{eq::dmft_dyn} for each $\{h_{\mu,n}^{\ell}(t), z^{\ell}_{\mu,n}(t)\}_{n=1}^{\mathcal{S}}$. A sketch of the solver can also be found in the main text Algorithm \ref{alg:kernel_convergence_muPAK}.
 \iffalse
 \begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/losses_dmf_cifar_P_200.png}
    \caption{Train (\textit{dashed}) and test (\textit{full}) losses as a function of gradient updates for different richness parameters $\gamma$. The model is a 2-layer MLP train to solve a regression problem with Squared Loss on two-classes of CIFAR10. Sample size is $P=200$. Black lines corresponds to theory predictions, colored curves to simulations.}\label{fig::losses_dmft}
\end{figure}
 \begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/preact_dmft_cifar_P_200.png}
    \caption{Non-Gaussian pre-activation and pre-gradient signal distributions as a function of $\gamma$ for a 2-layer MLP on two classes of CIFAR10. Black curves are the theory, colored lines are the simulations. } \label{fig::distr_dmft}
\end{figure}
\fi
\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{figures/mnist.pdf}
    \includegraphics[width=0.4\linewidth]{figures/cifar.pdf}
    \caption{The training dynamics of two layer ReLU MLPs trained with weight decay. The richly trained networks achieve lower training and test errors at equal levels of regularization. Our theory can reproduce the final preactivation and pregradient densities in each setting. }
     \label{fig::dmft_appendix_fig}
\end{figure}
As can be noticed in Figures~\ref{fig::dmft_appendix_fig}, solving for the field dynamics gives a good agreement with simulations. Precisely, once knowing the predictor, one can study how both train and test losses updates during the training dynamics and for different values of $\gamma$. Fig.~\ref{fig::dmft_appendix_fig}(b) (top left panel) shows that the $\textit{lazy}$ learning regime when $\gamma = 0$ does not allow the network to interpolate on the training data and leads to a higher test loss compared to the rich cases with $\gamma >0$. Increasing $\gamma$ has also the effect of speed up learning, while for that given sample size value ($P=200$ here) there exists an optimal degree of feature learning (i.e. $\gamma $) concerning the test loss. The specifics of the learning task can be found in the figure caption. Fig.~\ref{fig::dmft_appendix_fig} also shows the non-Gaussianity of the pre-activation and pre-gradient distributions once the system has thermalized, at the end of training. As mentioned above, because of the weight initialization as $W_{ij}^{\ell} \sim \mathcal{N}(0, 1)$, both $\{h,z \}$ are Gaussian distributed when the training starts. Then feature learning has the effects of accumulating non-Gaussian contributions as the training proceeds.

\subsection{DMFT for Convolutional 
Networks}\label{appendix::sec_dnns_antk}
In Fig.~\ref{fig::cnn_appendix} we show how our DMFT theory can be extended to a two-layer CNN which is trained on CIFAR10 images on a subset of $P=100$ (\textit{left}) or $P=1000$ (\textit{right}) points. When data sample is small, for any value of $\gamma_0$ there is an optimal early stopping time for the best Test Loss performance.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/cnn.pdf}
    \caption{Training dynamics for a two layer CNN for varying richness $\gamma_0$ on CIFAR-10 images. The images are turned into patches before computing cross-spatial correlations in the data. The training dynamics for infinite width networks (black) is compared to training finite width networks. For small training set sizes, richer training can result in faster overfitting, while this effect is less severe when there is more data.}
    \label{fig::cnn_appendix}
\end{figure}

The theory to get the predictor is an easy extension to the multi-layer fully-connected setting and can be found in~\cite{bordelon2022selfconsistentdynamicalfieldtheory}. However, we recall here for the sake of clarity how the field dynamic equations get modified in this setting. Again, in our notation $a$ is the spatial displacement from the center of the filter at each layer where $b \in \mathcal{S}^{\ell}$ is the spatial relative field at layer $\ell$ as in Eq.~\eqref{appendix::CNNs}. The pre-activation definitions still remain the same as it is in Appendix~\ref{appendix::CNNs}. In the same way, the gradient signal are now defined as 
\begin{equation}
    \bm g_{\mu,a}^{\ell} = \gamma_0 N \sum_b \frac{\partial f}{\partial \bm h_{\mu,b}^{\ell +1}}\cdot \frac{\partial \bm h_{\mu,b}^{\ell +1}}{\partial \bm h_{\mu,a}^{\ell}}.
\end{equation}  
while the weight dynamics per filter is 
\begin{equation}
    \frac{d}{dt}\bm W_b^{\ell}(t) = \frac{\gamma_0}{\sqrt{N}}\sum_{\mu,a} \Delta_{\mu} \bm g_{\mu,a}^{\ell +1} \phi (\bm h_{\mu,a+b})^{\top}-\lambda \bm W_b^{\ell}(t).
\end{equation}
Given that, the stochastic dynamics for the pre-activation and pre-gradient signals (similar to Eq.~\eqref{eq::dmft_dyn}) becomes
\begin{equation}
    \begin{split}
        &\bm h_{\mu,a}^{\ell +1}(t) = e^{-\lambda t}\bm \xi_{\mu,a}^{\ell +1}(t) + \gamma_0 \int_0^t dt' e^{-\lambda (t-t')}\sum_{\nu,b,c}\Delta_{\nu}(t') \Phi_{\mu \nu, a+b,a+c}^{\ell} (t,t')\bm g_{\nu,c}^{\ell+1}(t')\\
        &\bm z_{\mu,a}^{\ell}(t) = e^{-\lambda t}\bm \psi^{\ell}_{\mu a}(t) + \gamma_0 \int_0^t dt' e^{-\lambda(t-t')}\sum_{\nu,b,c}\Delta_{\nu}(t')G_{\mu\nu,a-b,c-b}(t,t')\phi(\bm h_{nu,c^{\ell}})
    \end{split}
\end{equation}
where again $\bm \xi_{\mu,a}^{\ell+1}(t) = \frac{1}{\sqrt{N}}\bm W^{\ell}(0)\phi (\bm h^{\ell}_{\mu a}(t))$ and $\bm\psi_{\mu a}^{\ell}(t) = \frac{1}{\sqrt{N}}\bm W^{\ell}(0) g_{\mu,a}^{\ell +1}(t)$ from the initial conditions. At large time $t$, as it is for the fully connected dynamics, the contribution form initial condition get suppressed and the fixed point predictor is a kernel predictor, being the feature and gradient kernels now
\begin{equation}
    \Phi_{\mu a, \nu b}^{\ell}(t,t') = \frac{1}{N}\phi (\bm h_{\mu a}^{\ell}(t)) \cdot \phi (\bm h_{\nu b}^{\ell}(t')), \quad G_{\mu a, \nu b}^{\ell}(t,t') = \frac{1}{N}\bm g_{\mu a}^{\ell}(t) \cdot \bm g_{\nu b}^{\ell}(t').
\end{equation}
The Neural Tangent Kernel is instead $K^{\text{aNTK}}_{\mu\nu}(t,t) = \sum_{\ell}\sum_{ab}\Phi_{\mu a,\nu b}^{\ell}(t,t) \,G_{\mu a,\nu b}^{\ell+1}(t,t)$ and the predictor again at convergence $f (\bm x)= \frac{1}{\kappa \lambda_L}\sum_{\nu} \Delta_{\nu}K^{\text{aNTK}}(\bm x, \bm x_{\nu})$.
\section{Fixed point structure of GD with Weight Decay}\label{appendix::fixed_points_dmft}
In what follows, we will be interested in the infinite time limit of a dynamics such that in Eq.~\eqref{eq::dmft_dyn}. Prior work on the dynamics of L2 regularization in the kernel regime revealed that training a wide network for infinite time leads to collapse of the features and network predictor to zero \cite{lewkowycz2020training}. However, if one instead adopts a $\mu$P scaling, then it is \textit{possible} to have a non-trivial fixed point at infinite width as the feature learning updates and regularization updates are of the same order \cite{bordelon2022selfconsistentdynamicalfieldtheory}. This is because from Eq. , we realize that in the setting where $\lambda > 0 $, not only the initial contribution of the fields dynamics are suppressed at large time $t$, but also the second terms contribute the most when the system has equilibrated, leading to a predictor which is a kernel predictor 
\begin{equation}
    f(\boldsymbol{x}_{\star}) = \boldsymbol{k}(\boldsymbol{x}_{\star})^{\top} [\boldsymbol{K}+ \lambda \kappa \boldsymbol{I}]^{-1} \boldsymbol{y}
\end{equation}
Because of the simple interpretation of DNNs in this regime, we wish to say something about the fixed point structure of the field dynamics, which are
\begin{equation}\label{eq::dmft_fp}
        h^\ell_\mu = \frac{\gamma_0}{\lambda} \sum_\nu \Delta_\nu  \Phi^{\ell-1}_{\mu\nu}  \dot\phi(h_\nu) z_\nu    \ , \  z^\ell_\mu = \frac{\gamma_0}{\lambda} \sum_\nu \Delta_\nu \phi(h^\ell_\nu) G^{\ell+1}_{\mu\nu}.
    \end{equation}
In what follows, we specialize to simple solvable cases to gain intuition for these fixed point constraints. In general, these constraints imply a set of possible joint densities over $h, z$ as well as determine the final feature and gradient kernels and the predictor.  
\subsection{Two Layer MLP with whitened data }
Let's consider a single data point with a white covariance matrix $K^x = 1$, label $y=1$ and a transfer function $\phi(x) = \text{ReLU}(x)$. In this setting, the dynamics of training for the pre-activation and pre-gradient signals are
\begin{align}
    \frac{d}{dt} h(t) = \gamma_0 \Delta(t) g(t) -\lambda h(t) \ , \ \frac{d}{dt} z(t) = \gamma_0 \Delta(t) \phi(h(t)) - \lambda z(t).
\end{align}
At the fixed point, the following conditions are satisfied
\begin{align}
    h = \frac{\gamma_0}{\lambda} \Delta \dot\phi(h) z \ , \ z = \frac{\gamma_0}{\lambda} \Delta \phi(h). 
\end{align}
In principle, there are infinitely many solutions to these equations, and combining them gives the following constraint on $h$
\begin{align}
    h = \frac{\gamma_0^2}{\lambda^2} \Delta^2 \dot\phi(h) \phi(h) = \frac{\gamma_0^2}{\lambda^2} \Delta^2 \phi(h). 
\end{align}
Since we know that here $\phi(h) = \max (0,h)$, this means that the following two constraints on the pre-activation density must be satisfied
\begin{align}
   \forall h<0  \quad  p(h) = 0   \ , \ \Delta = \frac{\lambda}{\gamma}. 
\end{align}
Lastly, we have the equation that fixes the value of the pattern error signal $\Delta$ through the predictor definition, which is
\begin{align}
    \Delta = 1 - \frac{1}{\gamma_0} \left<  z \phi(h) \right> = 1 - \gamma_0^{-1} \left< h^2 \right> 
\end{align}
implying that the second moment of $h$ must give
\begin{align}\label{eq::constr}
    \left< h^2 \right> = \gamma_0 - \lambda.
\end{align}
The solution here is correct if $\gamma_0 > \lambda$. Otherwise $\left< h^2 \right> = 0$ and $p(h) = \delta(h)$. We can verify that this is true by comparing the pre-activation density of a two-layer MLP trained until interpolation with the theoretical predictions of the fixed points. As shown in Fig.~\ref{fig:weight_decay_densities_single}, there is a sharp phase transition in the limiting density right when $\gamma_0 = \lambda$. Precisely, when $\gamma_0 < \lambda$, the effect of feature learning here is to kill the left-side of the distribution and adjusting the variance of $h>0$ such that the constraint of Eq.~\eqref{eq::constr} is satisfied. Another way of saying this is that in the infinite time limit $t \to \infty$, the $\{\gamma_0, \lambda_0 \} \to 0$ limits do not commute. In the first case of Fig.\ref{fig:weight_decay_densities_single}, when $\lim_{\gamma_0 \to 0, \lambda \to 0}$ we get a stable non-Gaussian behavior for the $p(h)$. In the second case of Fig.~\ref{fig:weight_decay_densities_single} we see a collapse when $\lim_{\lambda, \gamma  \to 0}$ and nothing is learned by the network. In the same way, in the limit where we fix $t = \mathcal{O}_N (1)$ and study the ridge-less limit of a lazy network ($\gamma_0 =0$), we recover the Neural Tangent Kernel predictor, and consequently the Gaussian pre-activation density at initialization.  

\begin{figure}[h]
    \centering
        \includegraphics[width=0.45\textwidth]{figures/gamma_g_lambda.png}
        \includegraphics[width=0.45\textwidth]{figures/gamma_l_lambda.png}
    \caption{Pre-activation densities of a two-layer MLP trained with GD and weight decay at different times. Ligther colors represent the end of training. Dashed blue line is the theoretical prediction from the fixed point in the infinite time limit. }
    \label{fig:weight_decay_densities_single}
\end{figure}
\subsection{Linear case}
In principle, the constraints one gets by looking at fixed points of Eq.~\eqref{eq::dmft_fp} fix the first two moments of the pre-activation density distribution, but are not enough to determine the full marginal $p(h)$. Indeed, this remains history dependent and one in principle has to track the entire update dynamics in order to get the full description. One possible way of understanding this is by looking at the simplest, linear case. Here, we initialize the weights of a two-layer MLP to be Laplace distributed. Here, we expect the distribution $p(h)$ to be Gaussian distributed if we train with weight decay until interpolation. Fig.~\ref{fig::laplace} shows that training with weight decay to the fixed point does not recover a Gaussian single site density. But it does have the properties demanded by the saddle point equations, which are again $\Delta = \lambda/\gamma_0; \, \langle h^2 \rangle = \gamma_0 -\lambda$.
\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{figures/laplace.png}
    \caption{Pre-activation density of a two-layer MLP trained with $P=1$ with a white covariance matrix. Dark colors correspond to early time training, being the weights initialized as Laplace distributed at $t=0$. Light colors coincide with the end of training, when the system has thermalized. Blue dashed line is the theory prediction from the fixed point equations.}
    \label{fig::laplace}
\end{figure}

%
\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{figures/nocollapse_lamb0.png}
    \includegraphics[width=0.4\linewidth]{figures/collapse_lamb_g_0.png}
    \caption{Weight decay in the lazy training regime can cause a model to ``unlearn" and reduce its output after the features start to decay. Provided $\gamma_0$ is sufficiently large compared to $\lambda$, however, the final predictor will still be nontrivial, unlike the zero predictor obtained in NTK parameterization \cite{lewkowycz2020training}. }
    \label{fig:enter-label}
\end{figure}
%

\section{Algorithmic implementation}
\subsection{Min-Max optimization for Bayesian DNNs}\label{sec::minmax}

In this section, we provide more detail on the solver for the aNBK algorithm. We use automatic differentiation to compute gradients of the action $S$ with respect to the order parameters $\{ \bm\Phi^\ell,\hat{\bm\Phi}^\ell \}$ \cite{jax2018github}. The key challenge for this is to estimate the single site moment generating functions $\mathcal Z_\ell$ as a differentiable function of both variables $\bm\Phi^\ell$ and $\hat{\bm\Phi}^\ell$. To do so, we use importance sampling, by recognizing that, conditional on $\bm\Phi^{\ell-1}$, the $\mathcal Z_\ell$ can be expressed as an average of a nonlinear function with respect to a Gaussian with covariance $\bm\Phi^{\ell-1}$

\begin{align}
    \mathcal{Z}_\ell  &= \left<  \exp\left( - \frac{1}{2} \bm\phi(\h)^\top \hat{\bm\Phi}^\ell \bm\phi(\h) \right) \right>_{\h \sim \mathcal{N}(0, \bm\Phi^{\ell-1})} \nonumber
    \\
    &\approx \frac{1}{B} \sum_{k=1}^B \exp\left( - \frac{1}{2} \phi(\h_k)^\top \hat{\bm\Phi}^\ell \phi(\h_k) \right)  
\end{align}
where each of the vectors $\h_k$ are iid draws from $\mathcal{N}(0, \bm\Phi^\ell)$. 

At each step of the iteration scheme for the min-max solver, we resample a new batch of $B$ vectors $\h_k$ and use these to estimate $\mathcal Z_\ell$, which provides fresh samples at each iteration of the algorithm. 

We run the inner maximization over all $\hat{\bm\Phi}^\ell$ until they reach a convergence criterion based on a fixed tolerance for the update sizes and we run the outer gradient descent step on $\bm\Phi^\ell$. 

\section{Glossary}

Here we provide more explanation of our choice of terminology for the various kernel predictors. We use the letter K at the end of a name to indicate that this predictor is a kernel method with \textit{no variance from random weight prior or initialization}. We use the prefix letter ``a" to indicate an \textit{adaptive kernel method} where the feature kernels adapt to the structure of the learning task. 
\begin{itemize}
    \item NNGP: the Gaussian process with $\Theta_N(1)$ variance for the network outputs under both the prior and the posterior. The mean of this process is a kernel method with the matrix $\bm\Phi^\ell$ in the lazy limit. 
    \item NNGPK: the mean kernel predictor for lazy training with the initial final kernel neglecting. This corresponds to $N \to \infty$ first followed by $\gamma_0 \to 0$ in our parameterization. In this limit, there is no variance of the predictor under either prior or posterior.
    \item NTK: a kernel method for the initial neural tangent kernel at infinite width without any randomness or variability in the predictor from initialization. This is the predictor obtained in the NTK parameterization if the initial output of the model is subtracted off (ie if a centering operation is performed where $f(\bm\theta,\x) \to f(\bm \theta, \x) - f(\bm\theta_0,\x)$). 
    \item aNBK: the adapted Bayesian kernel method in our scaling limit. This corresponds to regression with the adapted $\bm\Phi^L$ kernel.
    \item aNTK: the adapted NTK kernel in our feature learning scaling limit. This corresponds to regression with the adapted $\bm K$ kernel.
\end{itemize}

\end{document}




% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
