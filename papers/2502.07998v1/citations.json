[
  {
    "index": 0,
    "papers": [
      {
        "key": "jacot2020neuraltangentkernelconvergence",
        "author": "Arthur Jacot and Franck Gabriel and Cl\u00e9ment Hongler",
        "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"
      },
      {
        "key": "lee2018deepneuralnetworksgaussian",
        "author": "Jaehoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein",
        "title": "Deep Neural Networks as Gaussian Processes"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chizat2020lazytrainingdifferentiableprogramming",
        "author": "Lenaic Chizat and Edouard Oyallon and Francis Bach",
        "title": "On Lazy Training in Differentiable Programming"
      },
      {
        "key": "jacot2020neuraltangentkernelconvergence",
        "author": "Arthur Jacot and Franck Gabriel and Cl\u00e9ment Hongler",
        "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"
      },
      {
        "key": "lee2018deepneuralnetworksgaussian",
        "author": "Jaehoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein",
        "title": "Deep Neural Networks as Gaussian Processes"
      },
      {
        "key": "Lee_2020",
        "author": "Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey",
        "title": "Wide neural networks of any depth evolve as linear models under gradient descent\n*"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "vyas2022limitations",
        "author": "Vyas, Nikhil and Bansal, Yamini and Nakkiran, Preetum",
        "title": "Limitations of the ntk for understanding generalization in deep learning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "domingos2020every",
        "author": "Domingos, Pedro",
        "title": "Every model learned by gradient descent is approximately a kernel machine"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Geiger_2020",
        "author": "Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu",
        "title": "Disentangling feature and lazy training in deep neural networks"
      },
      {
        "key": "atanasov2021neuralnetworkskernellearners",
        "author": "Alexander Atanasov and Blake Bordelon and Cengiz Pehlevan",
        "title": "Neural Networks as Kernel Learners: The Silent Alignment Effect"
      },
      {
        "key": "wei2022toyrandommatrixmodels",
        "author": "Alexander Wei and Wei Hu and Jacob Steinhardt",
        "title": "More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "Zavatone_Veth_2022",
        "author": "Zavatone-Veth, Jacob A and Canatar, Abdulkadir and Ruben, Benjamin S and Pehlevan, Cengiz",
        "title": "Asymptotics of representation learning in finite Bayesian neural networks*"
      },
      {
        "key": "roberts2022principles",
        "author": "Roberts, Daniel A and Yaida, Sho and Hanin, Boris",
        "title": "The principles of deep learning theory"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "naveh2021self",
        "author": "Naveh, Gadi and Ringel, Zohar",
        "title": "A self consistent theory of gaussian processes captures feature learning effects in finite cnns"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "aitchison2020biggerbetterfiniteinfinite",
        "author": "Laurence Aitchison",
        "title": "Why bigger is not always better: on finite and infinite neural networks"
      },
      {
        "key": "hanin2023bayesian",
        "author": "Hanin, Boris and Zlokapa, Alexander",
        "title": "Bayesian interpolation with deep linear networks"
      },
      {
        "key": "zavatone2022contrasting",
        "author": "Zavatone-Veth, Jacob A and Tong, William L and Pehlevan, Cengiz",
        "title": "Contrasting random and learned features in deep Bayesian linear regression"
      },
      {
        "key": "bassetti2024featurelearningfinitewidthbayesian",
        "author": "Federico Bassetti and Marco Gherardi and Alessandro Ingrosso and Mauro Pastore and Pietro Rotondo",
        "title": "Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "cui2023bayes",
        "author": "Cui, Hugo and Krzakala, Florent and Zdeborov{\\'a}, Lenka",
        "title": "Bayes-optimal learning of deep random networks of extensive-width"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Li_2021",
        "author": "Li, Qianyi and Sompolinsky, Haim",
        "title": "Statistical Mechanics of Deep Linear Neural Networks: The Backpropagating Kernel Renormalization"
      },
      {
        "key": "Pacelli_2023",
        "author": "Pacelli, R. and Ariosto, S. and Pastore, M. and Ginelli, F. and Gherardi, M. and Rotondo, P.",
        "title": "A statistical mechanics framework for Bayesian deep neural networks beyond the infinite-width limit"
      },
      {
        "key": "Aiudi2023LocalKR",
        "author": "Riccardo Aiudi and Rosalba Pacelli and Alessandro Vezzani and Raffaella Burioni and Pietro Rotondo",
        "title": "Local kernel renormalization as a mechanism for feature learning in overparametrized convolutional neural networks"
      },
      {
        "key": "vanmeegen2024codingschemesneuralnetworks",
        "author": "Alexander van Meegen and Haim Sompolinsky",
        "title": "Coding schemes in neural networks learning classification tasks"
      },
      {
        "key": "Baglioni2024",
        "author": "Baglioni, P. and Pacelli, R. and Aiudi, R. and Di Renzo, F. and Vezzani, A. and Burioni, R. and Rotondo, P.",
        "title": "Predictive Power of a Bayesian Effective Action for Fully Connected One Hidden Layer Neural Networks in the Proportional Limit"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "seroussi2023separation",
        "author": "Seroussi, Inbar and Naveh, Gadi and Ringel, Zohar",
        "title": "Separation of scales and a thermodynamic description of feature learning in some cnns"
      },
      {
        "key": "yang2023theory",
        "author": "Yang, Adam X and Robeyns, Maxime and Milsom, Edward and Anson, Ben and Schoots, Nandi and Aitchison, Laurence",
        "title": "A theory of representation learning gives a deep generalisation of kernel methods"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "rubin2024a",
        "author": "Noa Rubin and Zohar Ringel and Inbar Seroussi and Moritz Helias",
        "title": "A Unified Approach to Feature Learning in Bayesian Neural Networks"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "rubin2024grokking",
        "author": "Noa Rubin and Inbar Seroussi and Zohar Ringel",
        "title": "Grokking as a First Order Phase Transition in Two Layer Networks"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "radhakrishnan2022mechanism",
        "author": "Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail",
        "title": "Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "mallinar2024emergence",
        "author": "Mallinar, Neil and Beaglehole, Daniel and Zhu, Libin and Radhakrishnan, Adityanarayanan and Pandit, Parthe and Belkin, Mikhail",
        "title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product"
      }
    ]
  }
]