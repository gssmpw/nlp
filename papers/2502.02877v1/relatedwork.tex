\section{Related Work}
DP has been widely studied as a means of preventing extraction of privacy-sensitive information from datasets or model parameters. Multiple DP mechanisms have been developed to provide guarantees on a certain level of DP, including the Laplacian mechanism \cite{dwork2006calibrating}, Gaussian mechanism \cite{Dwork2014DP}, functional mechanism \cite{zhang2012functional}, and exponential mechanism\cite{mcsherry2007mechanism}. 
The Laplacian, functional, and exponential mechanisms enforce $\epsilon$-DP, which often results in aggressive noise injection for ML tasks \cite{pan2024differential}. The Gaussian mechanism, by contrast, aims to enforce $(\epsilon,\delta)$-DP (see Sec.~\ref{ssec:DP}), a relaxed version of $\epsilon$-DP that is more suitable for learning tasks.


DP has been introduced into FL as a means of preventing servers and external eavesdroppers from extracting private information.
This has traditionally followed two paradigms: (i) central DP (CDP), involving noise addition at the main server~\cite{kon2017federated,Xiong2022CDP}, and (ii) local DP (LDP), which adds noise at each edge device~\cite{Zhao2021LDP,Shen2022imp,mobi2023Qiao,Liu2023mobi}. 
CDP generally leads to a more accurate final model, but it hinges on the trustworthiness of the main server. Conversely, LDP forgoes this trust requirement but requires a higher level of noise addition at each device to compensate~\cite{naseri2022local}.

There have been a few recent works dedicated to integrating these two paradigms into HFL, i.e., the special case of MFL with two layers. Specifically, \cite{Shi2021HDP,Zhou2023HDL} adapted the LDP strategy to the HFL structure, utilizing moment accounting to obtain strict privacy guarantees across the system. \cite{Wainakh2020HLDP} explored the advantages of flexible decentralized control over the training process in HFL and examined its implications on participant privacy. 
More recently, a third paradigm called hierarchical DP (HDP) has been introduced~\cite{chandrasekaran2022HDP}. 
HDP assumes that intermediate nodes present within the network can be trusted even if the main server cannot. These nodes are assigned the task of adding calibrated DP noise to the aggregated models prior to passing them upstream. The post-aggregation DP addition requirement to meet a given privacy budget becomes smaller as a result. However, there has not yet been a comprehensive analytical study on HDP and the convergence behavior in these systems, without which control algorithm design remains elusive. Moreover, no work has attempted to extend the concept of HDP towards more general multi-tier networks found in MFL settings, where the challenges of noise injection coupling and heterogeneous trust models become further exacerbated.