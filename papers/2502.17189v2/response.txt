\section{Background and Related Work}
\paragraph{Causal Discovery and LLMs.} The causal discovery task involves learning causal relationships from observed empirical data **Pearl, "Causality: Models, Reasoning and Inference"**. Many proposed algorithms exist **Spirtes et al., "Causal and Associative Induction"** attempting to solve the causal discovery problem. However, these methods are known to struggle on real world graphs where observations are noisy or common structural assumptions are violated **Verma and Pearl, "Equivalence and Synthesis of Causal Models"**.

Recently, LLMs have emerged as an alternative approach to causal discovery **Liao et al., "Causal Discovery with Large Margin"**. **Goyal et al., first investigated the capability of LLMs to act as zero-shot causal discovery agents using only semantic information and pairwise prompting on each variable pair. Follow-up work **Tashipour et al., further improves LLM predictions with observational data by selecting for predictions which maximize data likelihood. **Yates et al., utilize triplet prompting to prevent cycles when the causal graph is acyclic. They show only a topological ordering on variables is required for many common causal reasoning tasks **Liao et al.,**. Other works **Zhang and Chen, benchmark LLMs across a range of causality related tasks including causal discovery and causal inference confirming that LLMs struggle with integrating numerical data.

Another line of work more related to our proposed interactive causal discovery problem  studies how to incorporate background knowledge into causal discovery algorithms **Goyal et al.,**. Define a set of \textit{background knowledge} as the tuple $\mathcal{K} = (F,R)$, where $F$ specifies a set of ``forbidden'' graph edges and $R$ specifies a set of ``required'' graph edges. **Tashipour et al., presents an algorithm for constructing a causal graph consistent with $\mathcal{K}$ by leveraging an assumed structural directed acyclic graph (DAG) property. Building on **Goyal et al., **Yates et al., proposes a greedy search algorithm that performs well in practice.

Most related are statistical methods from the causal discovery literature which aim to efficiently choose a sequence of interventions to discover causal structure **Peters et al., Gradient based Interventional Targeting (GIT)**. **Liao et al., utilizes existing neural causal discovery methods **Tashipour et al.,** to learn a distribution over possible graph structures and variable assignments. For each round of intervention, GIT prioritizes variables whose simulated interventional distribution have large gradient with respect to the structural training loss.

In contrast to these works, our proposed algorithm utilizes LLMs to reason about the semantic/physical, as opposed to formal/structural, relationships between variables and edges in causal graphs. For this reason we are not required to make any structural assumptions on an underlying DAG, as is common in the causal discovery literature. This is desirable as in practice many real-world causal graphs are cyclic and poorly structured **Pearl and Dechter, "Network-Based Causal Reasoning"**. Additionally our method does not rely on observational or interventional data for real world graphs which may be expensive to acquire but crucial for good performance with statistical methods.

\paragraph{LLMs as Optimizers.} Another growing line of work utilizes LLMs as black-box optimizers **Liu et al.,**. **Dinan et al., introduce the notion of an LLM as a generic optimizer and use it to optimize performance objectives stemming from a range of tasks including linear regression and mathematical word problems **Miao et al.,**. Other works **Chen and Liu, examine the self-refinement capabilities of LLMs where the LLM must reason and self-improve on earlier responses. A growing number of papers apply LLMs to optimal experiment design and discovery **Liu et al.,**. **Zhu and Zhang both present benchmarks evaluating the ability of LLMs to perform real-world and synthesized discovery tasks.