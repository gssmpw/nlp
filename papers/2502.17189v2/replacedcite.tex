\section{Background and Related Work}
\paragraph{Causal Discovery and LLMs.} The causal discovery task involves learning causal relationships from observed empirical data ____. Many proposed algorithms exist ____ attempting to solve the causal discovery problem. However, these methods are known to struggle on real world graphs where observations are noisy or common structural assumptions are violated ____. 

Recently, LLMs have emerged as an alternative approach to causal discovery ____. ____ first investigated the capability of LLMs to act as zero-shot causal discovery agents using only semantic information and pairwise prompting on each variable pair. Follow-up work ____ further improves LLM predictions with observational data by selecting for predictions which maximize data likelihood. ____ utilize \textit{triplet prompting} to prevent cycles when the causal graph is acyclic. They show only a topological ordering on variables is required for many common causal reasoning tasks ____. Other works ____ benchmark LLMs across a range of causality related tasks including causal discovery and causal inference confirming that LLMs struggle with integrating numerical data. 

Another line of work more related to our proposed interactive causal discovery problem  studies how to incorporate background knowledge into causal discovery algorithms ____. Define a set of \textit{background knowledge} as the tuple $\mathcal{K} = (F,R)$, where $F$ specifies a set of ``forbidden'' graph edges and $R$ specifies a set of ``required'' graph edges. ____ presents an algorithm for constructing a causal graph consistent with $\mathcal{K}$ by leveraging an assumed structural directed acyclic graph (DAG) property. Building on ____, ____ proposes a greedy search algorithm that performs well in practice.

Most related are statistical methods from the causal discovery literature which aim to efficiently choose a sequence of interventions to discover causal structure ____. In particular, Gradient based Interventional Targeting (\textbf{GIT}) ____ utilizes existing neural causal discovery methods ____ to learn a distribution over possible graph structures and variable assignments. For each round of intervention, GIT prioritizes variables whose simulated interventional distribution have large gradient with respect to the structural training loss.

In contrast to these works, our proposed algorithm utilizes LLMs to reason about the semantic/physical, as opposed to formal/structural, relationships between variables and edges in causal graphs. For this reason we are not required to make any structural assumptions on an underlying DAG, as is common in the causal discovery literature. This is desirable as in practice many real-world causal graphs are cyclic and poorly structured ____. Additionally our method does not rely on observational or interventional data for real world graphs which may be expensive to acquire but crucial for good performance with statistical methods.

\paragraph{LLMs as Optimizers.} Another growing line of work utilizes LLMs as black-box optimizers ____. ____ introduce the notion of an LLM as a generic optimizer and use it to optimize performance objectives stemming from a range of tasks including linear regression and mathematical word problems ____. Other works ____ examine the self-refinement capabilities of LLMs where the LLM must reason and self-improve on earlier responses. A growing number of papers apply LLMs to optimal experiment design and discovery ____. ____ apply LLMs to gene discovery tasks which aim to find highly-influential parent genes affecting the regulation of a downstream target gene. ____ both present benchmarks evaluating the ability of LLMs to perform real-world and synthesized discovery tasks.


%Misc: 
%____