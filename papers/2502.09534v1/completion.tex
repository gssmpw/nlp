\section{Sampling Methods for Tensor Completion}
\label{sec:sampling-for-completion}
We are now ready to efficiently solve the \emph{unstructured} least-squares problem \eqref{eqn:input_regression} induced by ALS for tensor completion, i.e., for $\mat{P}\in\R^{|\Omega|\times R}$ and observations $\mat{q} \in \R^{|\Omega|}$, find
\[
    \mat{x}^* = \argmin_{\mat{x} \in \R^R} \,\norm{\mat{P} \mat{x} - \mat{q}}_{2}^2\,.
\]
As in Algorithm~\ref{alg:approximate-lifting},
we lift this problem to higher dimension to get a structured design matrix $\mat{A}$,
and use a known fast algorithm for approximately solving the structured least-squares problem in each step of approximate-mini-ALS.
For a given $\widehat{\epsilon} \in (0,1/\beta^2)$, the approximate solver computes a solution $\overline{\mat{x}}\in \R^R$ in time $O(T_{\widehat{\epsilon}})$ such that 
\[
\norm{\mat A \overline{\mat x} - \mat b}_2^2 \leq (1+\widehat \epsilon)\,\min_{\mat x} \norm{\mat A \mat x -\mat b}_2^2\,.
\]
Therefore, for a desired $\varepsilon_1 \in (0,1)$, we set $\widehat \epsilon = \Theta(\varepsilon_1 / \beta^2)$ and use a sufficiently small $\epsilon \gets \varepsilon_2$ in \cref{thm:approximate-richardson}.
Putting everything together, Algorithm~\ref{alg:approximate-lifting} finds an approximate solution $\widetilde{\mat x}\in \R^R$ in time $O(\beta T_{\varepsilon_1\beta^{-2}}\log\frac{\beta}{\epsilon_2})$ that satisfies
\begin{align}
    \norm{\mat P \widetilde{\mat x} - \mat q}_2^2
    &\leq
    (1+\varepsilon_1)\,\norm{\pi_{\mat P^\perp }\mat q}_2^2 + \varepsilon_2\, \norm{\mat \pi_{\mat P}\mat q}_2^2\,,\label{eq:tc-approx-sol}
\end{align}
where $\mat{\pi}_{\mat{P}}$ and $\mat{\pi}_{\mat{P}^\perp}$ are the orthogonal projection matrices into the column space and null space of $\mat{P}$, respectively~(see \Cref{app:least-square-regression}).
With this in hand, we now present the running times of our lifted iterative method for TC problems
by combining \cref{thm:approximate-richardson} with state-of-the-art tensor decomposition results based on leverage score sampling.

\subsection{CP Completion}
\label{subsec:CP-completion}
Each ALS update step for CP completion solves a regression problem where the design matrix is the Khatri--Rao product:
for $\mat{A}^{(k)} \in \R^{I_k \times R}$,
$\mat A^{\neq k} := \bigodot_{n=1,n\neq k}^N \mat A^{(n)} \in \R^{I_{\neq k}\times R}$,
and $\mat Q = (\mat X^\top_{(k)})_\Omega \in \R^{|\Omega| \times I_k}$,
\[
    \mat A^{(k)}
    \gets
    \argmin_{\mat{A} \in \R^{I_k \times R}} \, \bigl\lVert  (\mat A^{\neq k})_\Omega \,\mat{A}^\top - \mat Q \bigr\rVert_{\frobenius}\,.
\]
The design matrix $(\mat A^{\neq k})_\Omega$ does not necessarily have any structure,
so a direct method relies on solving the normal equation, which takes $O(R^\omega +R |\Omega|(R+I_k))$ time.
Thus, the running time of \emph{one round} of ALS, i.e., updating all $N$ factors,
is
\[
    O\parens*{N\parens*{R^\omega +R^2 |\Omega|}+ R |\Omega|\sum_{n=1}^N I_n}.
\]

Previous work on CP tensor decomposition \citep{cheng2016spals, larsen2022practical, bharadwaj2023fast}
developed fast methods for efficiently computing the leverage scores of a Khatri--Rao product matrix.
In particular, \citet{bharadwaj2023fast} designed a data structure for computing and maintaining the
leverage scores of $\mat A^{\neq k}$ during ALS updates. 
This approach requires sampling $\tilde{O}(R/\varepsilon)$ rows of $\mat A^{\neq k}$.
Due to the Khatri--Rao product structure, each row of $\mat A^{\neq k}$ can be mapped to a sequence of one choice from the rows of each $\mat{A}^{(n)}$ for $n\in [N]\backslash \{k\}$.
Hence, sampling a row from $\mat{A}^{\neq k}$ is equivalent to the following:
for each $n \in [N]\backslash\{k\}$, sample a row from $\mat A^{(n)}$ according to some conditional distribution given sampled rows from $\mat{A}^{(1)}, \dots, \mat{A}^{(n-1)}$, and then compute the Hadamard product of $N-1$ sampled rows.
Maintaining the full $I_n$-dimensional vector for a conditional probability for each $n$ is costly,
so \citet{bharadwaj2023fast} developed a binary tree-based data structure to speed up leverage-score sampling for $\mat A^{\neq k}$.

Applying \citet[Corollary 3.3]{bharadwaj2023fast}, one round of ALS runs in time
$
    \tilde{O} \parens{ \varepsilon^{-1} \sum_{n=1}^N \parens*{ I_n R^2 + NR^3} }.
$
Using their CP TD algorithm as the approximate solver in Algorithm~\ref{alg:approximate-lifting}, and combining its guarantee with \cref{thm:approximate-richardson},
we can extend their approach to CP completion.

\begin{corollary}
There is an ALS CP completion algorithm such that
(i) after a factor matrix update, each row of $\mat{A}^{(n)}$ satisfies \eqref{eq:tc-approx-sol},
and (ii) the total running time of one round is
\[
    \tilde{O} \parens*{\frac{\beta^2}{\varepsilon_1} \sum_{n=1}^N \parens*{I_n R^2 + NR^3} \log \frac{1}{\epsilon_2}}\,.
\]
\end{corollary}

\noindent
There is \emph{no dependence} on $|\Omega|$ in the running time due to leverage score sampling, i.e., it runs in sublinear time.

\subsection{Tucker Completion}

\citet{fahrbach2022subquadratic} designed block-sketching techniques and fast Kronecker product-matrix multiplication algorithms
to exploit the ALS structure for Tucker decomposition.

\subsubsection{Core Tensor Update}
Recall that for a Tucker decomposition we use the notation $I=\prod_{n\in[N]} I_n$ and $R=\prod_{n\in[N]} R_n$.
The ALS core tensor update in the Tucker completion problem is
\[
    \tensor{G}
    \leftarrow
    \argmin_{\tensor{G}' \in\R^{R_1 \times \cdots \times R_N}} \norm*{\parens*{\bigotimes_{n=1}^N\mat{A}^{(n)}}_{\Omega} \hspace{-0.2cm}\vvec(\tensor{G}') - \vvec(\tensor{X})_{\Omega}}_2.
\]
The design matrix above restricted to $\Omega$ is exactly $\mat P$ in our general setup.

We compare the running times of the direct method and our lifting approach.
In the former, we can compute an exact solution to a least-squares problem $\mat x^* = (\mat P^\top \mat P)^{-1} \mat P^\top \mat q$ in time $O(|\Omega| R^2 + R^\omega)$.

To achieve a fast lifted method, we solve the second step of Algorithm~\ref{alg:approximate-lifting}
using the leverage score sampling-based
core tensor update algorithm in \citep[Theorem 1.2]{fahrbach2022subquadratic}
with running time
\[
    \tilde{O} \parens*{
        \sum_{n=1}^N \parens*{ I_n R_n  + \frac{R_n^\omega N^2}{\varepsilon^2}} + \frac{R^{2-\theta^*}}{\varepsilon}
    }\,,
\]
where $\theta^*>0$ is an optimizable constant depending on $\{R_n\}_{n\in[N]}$.
Using this as the \ApproximateSolve subroutine in \Cref{thm:approximate-richardson}, we achieve the following.

\begin{corollary}
\label{cor:fast_tucker_completion_core_tensor_update}
There is an algorithm that computes an ALS Tucker completion core tensor update satisfying \eqref{eq:tc-approx-sol}
in time
\begin{equation}
\label{eqn:fast_tucker_completion_core_tensor_update}
    \tilde{O}\parens*{\parens*{
         \sum_{n=1}^N \parens*{I_nR_n  + \beta^4 R_n^\omega N^2 \varepsilon_1^{-2}}
         +
         \frac{\beta^2 R^{2-\theta^*}}{\varepsilon_1}} \beta\log\frac{1}{\varepsilon_2}
    }\,.
\end{equation}
\end{corollary}

\subsubsection{Factor Matrix Update}
The ALS factor matrix update for $\mat{A}^{(k)}$ in the Tucker completion problem is
\[
    \mat{A}^{(k)}
    \leftarrow
    \hspace{-0.1cm}
    \argmin_{\mat{A}\in \R^{I_k \times R_k}} \norm*{ \parens*{
        \parens*{\bigotimes_{n=1,n\neq k}^N \hspace{-0.10cm} \mat{A}^{(n)} } \mat{G}_{(k)}^\top }_\Omega  \hspace{-0.10cm} \mat{A}^\top \hspace{-0.10cm} - \mat Q}_\frobenius,
\]
where $\mat Q= (\mat X^\top_{(k)})_\Omega \in \R^{|\Omega| \times I_k}$ is a sparse matrix of observations.
The running time of a direct method that solves the normal equation is
$O(R_k^\omega + R_k |\Omega| (R_{\neq k} + R_k + I_k))$, where $R_{\neq k} = R/R_k$.

The running time of the sampling-based factor-matrix update for $\mat{A}^{(k)}$ in \citet[Theorem 1.2]{fahrbach2022subquadratic}
for the full decomposition problem is
\[
    \tilde{O}\parens*{
        \sum_{n=1}^N \parens*{I_nR_n + \frac{R_n^\omega N^2}{\varepsilon^2} + I_k R R_n} + \frac{I_k R_{\neq k}^{2-\theta^*}}{\varepsilon}
    }\,.
\]

\noindent
Combining this result with \cref{thm:approximate-richardson},
Algorithm~\ref{alg:approximate-lifting} has the following running time for a factor matrix update.

\begin{corollary}
There is an algorithm that computes an ALS Tucker completion factor matrix update for $\mat{A}^{(k)}$,
with each row of $\mat{A}^{(k)}$ satisfying \eqref{eq:tc-approx-sol},
in time
\[
\textstyle
    \tilde{O} \parens*{
        \parens*{
            \sum_{n=1}^N \parens*{I_nR_n  + \beta^4 R_n^\omega N^2 \epsilon_1^{-2}}
            +
            \frac{\beta^2 I_k R_{\neq k}^{2-\theta^*}}{\epsilon_1} + I_k R\sum_{n=1}^N R_n } \beta \log \frac{1}{\epsilon_2}
    }\,.
\]
\end{corollary}

\subsection{TT Completion}

Each ALS step for TT decomposition solves the following least-squares problem
with a Kronecker product-type design matrix:
for $\mat{A}^{\neq k} := \mat{A}_{< k}\otimes \mat{A}^\top_{> k} \in \R^{I_{\neq k} \times (R_{k-1}R_k)}$ and $\mat Q = (\mat{X}_{(k)}^\top)_\Omega \in \R^{|\Omega| \times I_k}$,
\[
    \tensor{A}^{(k)}
    \gets
    \argmin_{\tensor{B} \in \R^{R_{k-1}\times I_k \times R_k}}
    \norm*{\parens*{\mat{A}^{\neq k}}_\Omega\, (\mat{B}_{(2)})^\top - \mat Q }_{\frobenius}\,.
\]
Solving this directly with the normal equation takes
$O(\bar R_k^\omega +\bar R_k |\Omega| (\bar R_k + I_k))$ time for $\bar R_k := R_{k-1} R_k$.
Thus, the time for one round of ALS is
\[
    O\parens*{
        \sum_{n=1}^N \parens*{\bar R_n^\omega + \bar R_n |\Omega| \parens*{\bar R_n +I_n}}
    }.
\]

\citet{bharadwaj2024efficient} proposed a sampling-based ALS algorithm that crucially relies a \emph{canonical form} of the TT decomposition with respect to the index $k$. Any TT decomposition can be converted to this form through a QR decomposition, and this form ensures that $(\mat A^{\neq k})^\top \mat A^{\neq k} = I_{R_{k-1} R_k}$.
It follows that the leverage scores of $\mat A^{\neq k}$ are simply the diagonal entries of $\mat A^{\neq k} (\mat A^{\neq k})^\top = (\mat A_{< k}\mat A_{<k}^\top) \otimes (\mat A_{> k}^\top\mat A_{>k})$.

It follows from properties of the Kronecker product~\citep{diao2019optimal} that
$
    \ell_{i^{\neq k}}(\mat A^{\neq k})
    =
    \ell_{i_{< k}}(\mat A_{< k}) \cdot \ell_{i_{> k}}(\mat A^\top_{> k})\,,
$
where $i^{\neq k} = \underline{i_1\cdots i_{k-1} i_{k+1}\cdots i_N}$, $i_{< k} = \underline{i_1\cdots i_{k-1}}$, and $i_{> k} = \underline{i_{k+1}\cdots i_N}$ (see \Cref{app:tt-decomposition-details} for notation).
Therefore, efficient leverage score sampling for $\mat A^{\neq k}$ reduces to that for $\mat A_{< k}$ and $\mat A_{> k}$.
To this end, \citet{bharadwaj2024efficient} adopt an approach similar to \citet{bharadwaj2023fast} for leverage score-based CP decomposition.
Each row of $\mat A_{< k}$ corresponds to a series of one slice for each third-order tensor $\mat A^{(n)}$ for $n<k$, which results in a series of conditional sampling steps using a data structure adapted from the one used for CP decomposition.
In contrast, \citet[Corollary 4.4]{bharadwaj2024efficient} show that one round of approximate TT-core updates,
if $R_n = R$ for all $n \in [N-1]$, can run in time
$\tilde{O} \parens{ R^4 \varepsilon^{-1} \sum_{n = 1}^N \parens*{N + I_n}}$,
which leads to the following result.

\begin{corollary}
There is an ALS TT completion algorithm such that
(i) after a TT-core update, each row fiber of $\tensor{A}^{(n)}$
satisfies \eqref{eq:tc-approx-sol},
and (ii) the total running time of one round is
\[
    \tilde{O} \parens*{
        \frac{\beta^2 R^4}{\epsilon_1} \sum_{n=1}^N \parens*{N + I_n} \log\frac{1}{\epsilon_2}
    }\,.
\]
\end{corollary}
