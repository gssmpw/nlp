\section{Preliminaries}
\label{sec:preliminaries}

\paragraph{Notation.}
The \emph{order} of a tensor is the number of its dimensions $N$.
We denote scalars by normal lowercase letters $x \in \R$,
vectors by boldface lowercase letters $\mat{x} \in \R^n$,
matrices by boldface uppercase letters $\mat{X} \in \R^{m \times n}$,
and higher-order tensors by boldface script letters $\tensor{X} \in \R^{I_1 \times \dots \times I_N}$.
We use normal uppercase letters for the size of an index set,
e.g., $[N] = \{1, 2, \dots, N\}$.
We define $I_{\neq k}:= I / I_k$ for $k\in [N]$ and similarly $R_{\neq k := R / R_k}$.
We denote the $i$-th entry of $\mat{x}$ by $x_i$,
the $(i,j)$-th entry of $\mat{X}$ by $x_{ij}$, and the
$(i,j,k)$-th entry of a third-order tensor $\tensor{X}$ by $x_{ijk}$.

\paragraph{Linear algebra.}
A symmetric matrix $\mat{A}\in \mathbb{R}^{n\times n}$ is \emph{positive semi-definite} (PSD) if $\mat{v}^\top \mat{A}\mat{v}\geq 0$ for any $\mat{v}\in\mathbb{R}^n$.
For two symmetric matrices $\mat{A}$, $\mat{B}$, we use $\mat{A} \preccurlyeq \mat{B}$ to indicate that $\mat{B}-\mat{A}$ is PSD.
For a PSD matrix $\mat{M}\in \R^{n\times n}$ and vector $\mat{v}\in \R^n$, we define $\norm{\mat{v}}_{\mat{M}}:= (\mat{v}^\top \mat{M}\mat{v})^{1/2}$.
For $\mat{A}\in \R^{m\times n}$, $\mat{b}\in \R^m$, and $\Omega \subseteq [m]$, we use $\mat{A}_{\Omega}\in \R^{|\Omega|\times n}$ and $\mat{b}_{\Omega}\in\R^{|\Omega|}$ to denote the submatrix and subvector with rows indexed by $\Omega$.
We let $\otimes$ denote the Kronecker product and $\odot$ denote the Khatri--Rao product.

\paragraph{Tensor algebra.}
The fibers of a tensor are the vectors obtained by fixing all but one index,
e.g., if $\tensor{X} \in \R^{I \times J \times K}$,
the column, row and tube fibers are
$\mat{x}_{:jk}$, $\mat{x}_{i:k}$, and $\mat{x}_{ij:}$, respectively.
The \emph{mode-$n$ unfolding} of a
tensor $\tensor{X} \in \R^{I_1 \times \dots \times I_N}$ is the matrix
$\mat{X}_{(n)} \in \R^{I_n \times (I_1 \cdots I_{n-1} I_{n+1}\cdots I_N)}$
that arranges the mode-$n$ fibers of $\tensor{X}$ as rows of $\mat{X}_{(n)}$ sorted lexicographically by index. The \emph{vectorization} $\vvec{(\tensor{X})} \in \R^{I_1\cdots I_N}$ of $\tensor{X}$ stacks the elements of $\tensor{X}$ lexicographically by index.

For $n\in[N]$, we denote the \emph{mode-$n$ product} of a tensor
$\tensor{X} \in \R^{I_1 \times \dots \times I_N}$ and matrix
$\mat{A} \in \R^{J \times I_n}$ by
$\tensor{Y} = \tensor{X} \times_n \mat{A}$, where
$\tensor{Y} \in \R^{I_1 \times \dots \times I_{n-1} \times J \times I_{n+1} \times \dots \times I_{N}}$.
This operation multiplies each mode-$n$ fiber of $\tensor{X}$ by $\mat{A}$, and is expressed elementwise as
\[
    (\tensor{X} \times_{n} \mat{A})_{i_1 \dots i_{n-1} j i_{n+1} \dots i_{N}}
    =
    \sum_{i_n=1}^{I_n} x_{i_1 i_2 \dots i_N} a_{j i_n}.
\]
The inner product of two tensors $\tensor{X}, \tensor{Y} \in \R^{I_1 \times \dots \times I_N}$ is
\[
    \inner{\tensor{X}, \tensor{Y}}
    =
    \sum_{i_1=1}^{I_1}
    \sum_{i_2=1}^{I_2}
    \dots
    \sum_{i_N=1}^{I_N}
    x_{i_1 i_2 \dots i_N}
    y_{i_1 i_2 \dots i_N}.
\]
The Frobenius norm of a tensor $\tensor{X}$ is 
$\norm{\tensor{X}}_{\frobenius} = \sqrt{\inner{\tensor{X}, \tensor{X}}}$.


\subsection{Tensor Decompositions}

The tensor decompositions of $\tensor{X} \in \R^{I_1 \times \dots \times I_N}$ below can be viewed as higher-order analogs of low-rank matrix factorization.
See \citet{kolda2009tensor} for a comprehensive survey on this topic.

\paragraph{CP decomposition.}
A rank-$R$ \emph{CP decomposition} represents $\tensor{X}$
with $\boldsymbol{\lambda} \in \R_{\geq 0}^R$ and $N$ factors $\mat{A}^{(n)} \in \R^{I_n \times R}$, for $n\in [N]$,
where each column of $\mat{A}^{(n)}$ has unit norm.
The reconstructed tensor $\widehat{\tensor{X}}$ is defined elementwise as:
\[
    \widehat{x}_{i_1 \ldots i_N} = \sum_{r=1}^R \lambda_r \, a^{(1)}_{i_1 r} \cdots a^{(N)}_{i_N r}\,.
\]

\paragraph{Tucker decomposition.}
A rank-$\mat{r}$ \emph{Tucker decomposition} represents $\tensor{X}$ as a \emph{core tensor} $\tensor{G} \in \R^{R_1 \times \dots \times R_N}$ and $N$ \emph{factor matrices} $\mat{A}^{(n)} \in \R^{I_n \times R_n}$, for $n\in [N]$,
where $\mat{r} = (R_1,\dots,R_N)$ is the multilinear rank~\citep{ghadiri2023approximately}.
The reconstructed tensor
$\widehat{\tensor{X}}=\tensor{G} \times_{1} \mat{A}^{(1)} \times_{2} \dots \times_{N} \mat{A}^{(N)}$
is defined elementwise as:
\[
    \widehat{x}_{i_1 \ldots i_N}
    =
    \sum_{r_1=1}^{R_1} \cdots \sum_{r_N=1}^{R_N}
    g_{r_1 \ldots r_N} a_{i_1 r_1}^{(1)} \cdots a_{i_N r_N}^{(N)}\,.
\]

\paragraph{TT decomposition.}
A rank-$\mat{r}$ \emph{tensor train (TT) decomposition}~\citep{oseledets2011tensor} represents $\tensor{X}$
using $N$ third-order \emph{TT-cores} $\tensor{A}^{(n)}\in \R^{R_{n-1}\times I_n\times R_n}$, for $n\in[N]$, with the convention $R_0 = R_N = 1$.
The reconstructed tensor $\widehat{\tensor{X}}$ is defined elementwise as:
\begin{align*}
    \widehat{x}_{i_1 \ldots i_N}
    &= \underbrace{\mat{A}^{(1)}_{:i_1:}}_{1\times R_1}
       \underbrace{\mat{A}^{(2)}_{:i_2:}}_{R_1\times R_2}
       \cdots
       \underbrace{\mat{A}^{(N-1)}_{:i_{N-1}:}}_{R_{N-2}\times R_{N-1}}
       \underbrace{\mat{A}^{(N)}_{:i_N:}}_{R_{N-1}\times 1}\,.
\end{align*}

\begin{remark}
All three of these TDs are instances of the more general \emph{tensor network} framework (see~\Cref{app:tensor-networks}).
\end{remark}

\subsection{ALS Formulations}
\label{subsec:ALS-for-different-TDs}

\emph{Alternating least squares} (ALS) methods are the gold standard for low-rank tensor decomposition, e.g., they are the first techniques mentioned in the MATLAB Tensor Toolbox \citep{matlab}.
ALS cyclically minimizes the original least-squares problem~\eqref{eq:GTC} with respect to one block of factor variables while keeping all others fixed.
Repeating this process converges to a nontrivial local optimum
and reduces the original nonconvex problem to a series of (convex) linear regression problems in each step (see \cref{app:least-square-regression}).

Updating a block of variables in an ALS step for a TD problem
is typically a \emph{highly structured} regression problem
that can be solved very fast with problem-specific algorithms.
We describe the induced structure of each ALS update for the tensor decomposition types above.

\paragraph{CP factor matrix update $\equiv$ Khatri--Rao regression.}
In each ALS step for CP decomposition, all factor matrices are fixed except for one, say $\mat{A}^{(n)}$.
ALS solves the following linear least-squares problem:
\begin{equation}\label{eq:CP-ALS}
    \mat{A}^{(n)}
    \leftarrow
    \argmin_{\mat{A} \in \R^{I_n \times R}} \, \norm*{ \parens*{ \bigodot_{i=1,i\neq n}^{N} \mat{A}^{(i)} }\,\mat{A}^\top - \mat{X}_{(n)}^\top }_{\frobenius}\,.    
\end{equation}
Then, we set $\lambda_r = \norm{\mat{a}^{(n)}_{:r}}_2$ for each $r\in[R]$ and normalize the columns of $\mat{A}^{(n)}$.
Each row of $\mat{A}^{(n)}$ can be optimized independently, so \eqref{eq:CP-ALS}
solves $I_n$ linear regression problems where the design matrix is a \emph{Khatri--Rao product}.

\paragraph{Tucker core update $\equiv$ Kronecker regression.}
The core-tensor ALS update solves: with all factors $\mat{A}^{(n)}$ fixed, 
\[
    \tensor{G}
    \leftarrow
    \argmin_{\tensor{G}' \in\R^{R_1 \times \cdots \times R_N}}\,
    \norm*{\parens*{ \bigotimes_{n=1}^N\mat{A}^{(n)} } \vvec(\tensor{G}') - \vvec(\tensor{X}) }_2\,,
\]
where the design matrix is a \emph{Kronecker product} of the factors.

\paragraph{Tucker factor update $\equiv$ Kronecker-matrix regression.}
When ALS updates $\mat{A}^{(n)}$ with all the other factor matrices
and core tensor fixed, it solves:
\[
    \mat{A}^{(n)}
    \leftarrow
    \argmin_{\mat{A}\in \R^{I_n \times R_n}} \,
    \norm*{\parens*{\bigotimes_{i=1,i\neq n}^N \mat{A}^{(i)}} \mat{G}_{(n)}^\top \mat{A}^\top - \mat{X}_{(n)}^\top }_{\frobenius}\,,
\]
where $\mat{G}_{(n)}$, $\mat{X}_{(n)}$ are the mode-$n$ unfoldings of $\tensor{G}$ and $\tensor{X}$.
This is equivalent to solving $I_n$ independent linear regression problems, where the design matrix is the product of a Kronecker product and another matrix.
It can also be viewed as solving $I_n$ instances of structured but \emph{constrained linear regression}~\citep{fahrbach2022subquadratic}.

\paragraph{TT-core update $\equiv$ Kronecker regression.}
Given a TT decomposition $\{\tensor{A}^{(n)}\}_{n=1}^{N}$ and $n\in[N]$,
the \emph{left chain} $\mat{A}_{<n} \in \R^{(I_1 \cdots I_{n-1}) \times R_{n-1}}$
and
\emph{right chain} $\mat{A}_{> n}\in \R^{R_n \times (I_{n+1} \cdots I_N)}$
are matrices depending on the cores $\tensor{A}^{(n')}$ for $n' < n$ and $n' > n$, respectively (see \Cref{app:tt-decomposition-details} for details).
When ALS updates $\tensor{A}^{(n)}$ with all other TT-cores fixed, it solves
\[
    \tensor{A}^{(n)} \!\!
    \gets \!\!
    \argmin_{\tensor{B} \in \R^{R_{n-1}\times I_n \times R_n}}
    \bigl\lVert (\mat{A}_{<n}\kron \mat{A}^\top_{>n})\,\mat{B}_{(2)}^\top - \mat{X}_{(n)}^\top \bigr\rVert_{\frobenius}\,,
\]
which is equivalent to solving $I_n$ Kronecker regression problems in $\R^{I_{\neq n}}$.
