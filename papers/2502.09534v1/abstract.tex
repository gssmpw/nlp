\begin{abstract}
We study tensor completion (TC) through the lens of low-rank tensor decomposition (TD).
Many TD algorithms use fast alternating minimization methods, which solve \emph{highly structured} linear regression problems at each step (e.g., for CP, Tucker, and tensor-train decompositions).
However, such algebraic structure is lost in TC regression problems, making direct extensions unclear. 
To address this, we propose a \emph{lifting} approach that approximately solves TC regression problems using structured TD regression algorithms as blackbox subroutines,
enabling sublinear-time methods.
We theoretically analyze the convergence rate of our approximate Richardson iteration-based algorithm,
and we demonstrate on real-world tensors that its running time can be 100x faster than direct methods for CP completion.
\end{abstract}
