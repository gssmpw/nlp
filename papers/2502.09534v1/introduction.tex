\section{Introduction}
\label{sec:introduction}

\emph{Tensor completion} (TC) is the higher-order generalization of matrix completion.
It has many applications across data mining, machine learning, signal processing, and statistics
(see \citet{song2019tensor} for a detailed survey of applications).
In this problem, we are given a partially observed tensor as input
(i.e., only a subset of its entries is known),
and the goal is to impute the missing values.
Under low-rank and other statistical assumptions,
we can hope to recover the missing values by minimizing a loss function over only the observed entries.

Consider a tensor $\tensor{X} \in \mathbb{R}^{I_1 \times \cdots \times I_N}$
with a subset of observed indices $\Omega \subseteq [I_1] \times \cdots \times [I_N]$.
The general TC problem is
\begin{equation}
\label{eq:GTC}
    \min_{\bth}\, \CLoss_{\Omega} \parens{\widehat{\tensor{X}}(\bth), \tensor{X}} + \CReg(\bth)\,,
\end{equation}
where $\bth$ are the learnable parameters, $\widehat{\tensor{X}}(\bth) \in \mathbb{R}^{I_1 \times \cdots \times I_N}$ is the reconstructed tensor, $\CLoss_{\Omega}$ is the loss defining the error between $\widehat{\tensor{X}}(\bth)$ and $\tensor{X}$ on entries in $\Omega$,
and $\CReg(\bth)$ is the regularization term. For brevity, we write $\widehat{\tensor{X}}$ without explicit dependence on $\bth$.
Rank constraints can be incorporated into \eqref{eq:GTC} by including appropriate penalty terms in $\CReg(\bth)$.

This work focuses on the sum of squared errors
\begin{equation}
\label{eq:sum-of-square}
    \CLoss_{\Omega}(\widehat{\tensor{X}}, \tensor{X})
    =
    \sum_{(i_1,\ldots,i_N) \in \Omega} (\widehat{x}_{i_1\ldots i_N} - x_{i_1\ldots i_N})^2\,.
\end{equation}
To introduce our approach, we use a running example where
$\widehat{\tensor{X}} = \tensor{G} \times_1 \mat{A}^{(1)} \times_2 \mat{A}^{(2)} \cdots \times_N \mat{A}^{(N)}$
is a \emph{low-rank Tucker decomposition}
with core tensor $\tensor{G} \in \R^{R_1 \times \dots \times R_N}$ and
factor matrices $\mat{A}^{(n)} \in \R^{I_n \times R_n}$, and $\times_i$ is the mode-$i$ product.

In general, TC is a non-convex and NP-hard problem~\citep{hillar2013most}.
The special case where $\Omega = [I_1] \times \cdots \times [I_N]$
is the widely studied \emph{tensor decomposition} (TD) problem,
for which \emph{alternating least squares} (ALS) methods are regularly used to compute $\tensor{G}, \mat{A}^{(1)}, \ldots, \mat{A}^{(N)}$.

Each ALS step fixes all but one of $\tensor{G}, \mat{A}^{(1)}, \dots, \mat{A}^{(N)}$, and optimizes the unfixed component.
In the case of TD, each ALS step is a \emph{highly structured} linear regression problem.
For example, in the core tensor update for $\tensor{G}$, ALS solves
\begin{equation}
\label{eq:kron-reg}
    \min_{\tensor{G}' \in\mathbb{R}^{R_1 \times \cdots \times R_N}
    }
    \norm*{\parens*{\bigotimes_{n=1}^N\mat{A}^{(n)}} \vvec(\tensor{G}') - \vvec(\tensor{X})}_2\,,
\end{equation}
where $\kron$ is the \emph{Kronecker product} and
$\vvec(\tensor{G}') \in \R^{R}$ is the flattened version of $\tensor{G}'$
with $R := \prod_{n=1}^N R_n$.

\citet{diao2019optimal} and \citet{fahrbach2022subquadratic}
recently exploited the Kronecker product structure in~\eqref{eq:kron-reg} to design algorithms with running times that are sublinear in the size of the full tensor $I := \prod_{n=1}^N I_n$
by using \emph{leverage score sampling} to approximately solve~\eqref{eq:kron-reg}.

In TC, however, only a subset of observations appear in the loss function.
Letting $\mat{A} = \bigotimes_{n=1}^N\mat{A}^{(n)}$ and $\mat{b}=\vvec(\tensor{X})$,
the core tensor update becomes
\begin{equation}
\label{eq:vec-completion-prob}
    \vvec(\tensor{G})
    \gets
    \argmin_{\mat{x} \in \R^{R}}\, \norm{\mat{A}_{\Omega} \mat{x} - \mat{b}_{\Omega}}_2\,,
\end{equation}
where $\mat{A}_{\Omega}$ is a submatrix of $\mat{A}$
whose rows correspond to the indices in $\Omega$.
Observe that the Kronecker product structure in \eqref{eq:kron-reg} \emph{no longer exists} for $\mat{A}_{\Omega}$ in~\eqref{eq:vec-completion-prob},
hence fast TD methods do not immediately extend to $\Omega$-masked TC versions.


A natural idea to overcome the lack of structure in the $\Omega$-masked updates is to lift \eqref{eq:vec-completion-prob} to a higher-dimensional problem by introducing variables $\mat{b}_{\overline{\Omega}}$, where $\overline{\Omega}$ is the complement of $\Omega$,
i.e., letting the unobserved entries in $\tensor{X}$ be \emph{free variables}.
This is a convex problem with much of the same structure as the design matrix in the full TD problem, and gives the same solution as the TC update in \eqref{eq:vec-completion-prob}:
\begin{equation}
\label{eq:lifted-prob}
    (\mat{x}^*, \mat{b}_{\overline{\Omega}}^*) = \argmin_{\mat{x},\mat{b}_{\overline{\Omega}}}\, \norm{\mat{A} \mat{x} - \mat{b}}_2\,.
\end{equation}

To our knowledge, such ideas date back to \citet{healy1956missing} in the experimental design and causal inference literature.
However, it is not clear a priori that lifting is helpful for \emph{computational efficiency}---it restores the structure of the design matrix,
but the new problem \eqref{eq:lifted-prob} is larger and higher dimensional.
Our work proposes to solve \eqref{eq:lifted-prob} with the following two-step procedure,
called \emph{mini-ALS}.
Given $\mat x^{(k)}$, it repeats the following:
\begin{enumerate}
    \item Set $\mat{b}^{(k)}_{\overline{\Omega}} \gets \mat{A}_{\overline{\Omega}} \mat{x}^{(k)}$ and $\mat{b}^{(k)}_{\Omega} \gets \mat{b}_{\Omega}$.
    \item Set $\mat{x}^{(k+1)} \gets \argmin_{\mat{x}} \norm{\mat{A} \mat{x} - \mat{b}^{(k)}}_{2}$.
\end{enumerate}

Mini-ALS iterations can still be expensive since ${\mat{A} \in \R^{I \times R}}$ is a tall-and-skinny matrix (i.e., $I\gg R$).
However, step two is a structured ALS for TD,
so we propose solving it \emph{approximately} with row sampling,
allowing us to tap into a rich line of work on leverage score sampling
for CP decomposition~\citep{cheng2016spals,larsen2022practical,bharadwaj2023fast},
Tucker decomposition~\citep{diao2019optimal,fahrbach2022subquadratic},
and tensor-train decomposition~\citep{bharadwaj2024efficient}.
Returning to the running Tucker completion example,
updating the core tensor in step two of mini-ALS via~\eqref{eq:kron-reg}
only requires sampling $\tilde{O}(R)$ rows of $\mat{A}$,
which leads to a substantially \emph{smaller} problem.
Further, we can compute $\mat{b}^{(k)}$ \emph{lazily}, i.e.,
only the entries corresponding to sampled rows.
We call our lifted iterative method \emph{approximate-mini-ALS}.

\subsection{Our Contributions}
We summarize our main contributions as follows:
\begin{itemize}
\item In \Cref{sec:lifted_regression}, we propose using the mini-ALS algorithm
for each step of ALS for TC, and show that it simulates the \emph{preconditioned Richardson iteration}~(\Cref{lemma:richardson-simulation}).
Our main theoretical contribution is proving that the second step of a mini-ALS iteration can be performed \emph{approximately}.
We quantify how small this approximation error must be for approximate-mini-ALS to converge
at the same rate as the Richardson iteration~(\Cref{thm:approximate-richardson}).
This lets us to extend a recent line of work
on leverage score sampling-based TD ALS algorithms to the TC setting.

\item In \Cref{sec:sampling-for-completion},
we use state-of-the-art TD ALS algorithms
for CP,
Tucker, 
and tensor-train decompositions 
as \emph{blackbox subroutines} in Algorithm~\ref{alg:approximate-lifting}
to obtain novel sampling-based TC algorithms.
Hence, our lifting approach for TC also benefits from future TD algorithmic improvements.

\item In \Cref{sec:experiments}, 
we compare the empirical performance of
our lifted algorithm to direct methods for a \emph{low-rank CP completion} task on synthetic and real-world tensors.
We observe that mini-ALS can be orders of magnitude faster than direct ALS methods,
while achieving comparable reconstruction errors.
Finally, we propose an \emph{accelerated} version with adaptive step sizes that extrapolates the trajectory of $\mat{x}^{(k)}$ and converges in fewer total iterations.

\end{itemize}

\subsection{Related Work}
\label{sec:related-work}

\paragraph{CP completion.}
\citet{tomasi2005parafac} proposed an ALS algorithm for CP completion that repeats the following two-step process:
(1) fill in the missing values using the current CP decomposition, and 
(2) update one factor matrix.
Their algorithm is equivalent to running \emph{one iteration} of mini-ALS in each step of ALS.
As \citet{tomasi2005parafac} discuss, this can lead to slower convergence and an increased likelihood of converging to suboptimal local minima because of errors introduced by the imputed missing values.

In contrast, approximate-mini-ALS runs \emph{until convergence} in each step of ALS.
By doing this, we establish a connection to the Richardson iteration
and build on its convergence guarantees.
Further, \citet{tomasi2005parafac} explicitly fill in \emph{all missing values} using the current decomposition, whereas we only impute missing values \emph{required by row sampling},
allowing us to achieve sublinear-time updates in the size of the tensor.
In general, iteratively fitting to imputed missing values falls under the umbrella of \emph{expectation-maximization} (EM) algorithms~\citep[Chapter 8]{little2019statistical}.

\paragraph{Statistical assumptions.}
Similar to minimizing the nuclear norm for matrix completion \citep{fazel2002matrix, candes2012exact}, a line of research in noisy TC suggests minimizing a convex relaxation of rank and identifies statistical assumptions under which the problem is recoverable \citep{barak2016noisy}.
Two standard assumptions are \emph{incoherence} and the \emph{missing-at-random} assumption. %i.e., observations are drawn uniformly at random.
In \Cref{sec:lifted_regression}, we discuss how these assumptions provide a bound on the number of steps required for mini-ALS.
Alternating minimization approaches have also been applied
in this line of noisy TC research~\citep{jain2014provable,liu2020tensor}.
It is also consistently observed that methods based on TD and tensor unfoldings are more practical and computationally efficient \citep{acar2011scalable,montanari2018spectral,filipovic2015tucker,shah2019iterative,shah2023robust}.