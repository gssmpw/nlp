\section{Approximate Richardson Iteration}
\label{sec:lifted_regression}

We now present our main techniques for reducing tensor completion to tensor decomposition.
When we use ALS to solve a TC problem,
we must efficiently solve least-squares problems $\min_{\mat x} \, \norm{\mat{Px}-\mat q}_2$.
The rows of the design matrix $\mat{P}$ correspond to the \emph{subset of observations} in the TC problem,
so $\mat{P}$ does not necessarily have the structure of the design matrix in the full TD problem.

A direct approach is to compute the closed-form solution $\mat x^* = (\mat P^\top \mat P)^{-1}\mat P^\top \mat q$, but computing $(\mat P^\top \mat P)^{-1}$ is often impractical.
Two techniques are commonly used to overcome this:
(1) iterative methods and (2) row sampling.
Iterative methods repeat the same relatively cheap per-step computation
\emph{many times} to approximate the original expensive computation.
Row sampling methods (e.g., leverage score sampling) randomly pick rows of $\mat P$
and solve a least-squares problem on the sampled rows to obtain an approximate solution to the original problem with high probability.
Directly computing leverage scores for a general $\mat{P}$, however,
is \emph{also prohibitively expensive} since it requires computing
the same matrix $(\mat P^\top \mat P)^{-1}$ (see \Cref{app:leverage-score} for details).

We show that our novel \emph{approximate-mini-ALS} method
is a principled approach for tensor completion.
In \Cref{ssec:lifting}, we show how lifting \emph{restores the structure} of the full TD ALS update step,
enabling fast least-squares methods for a larger (but equivalent) problem.
In \Cref{subsec:iterative_methods}, we show that iteratively solving this lifted problem (i.e., mini-ALS) is connected to an iterative method called the \emph{Richardson iteration}~\citep{richardson1911approximate},
which we can also view as a matrix-splitting method.
In other words, mini-ALS and the Richardson iteration with a certain preconditioner give the same sequence of iterates $\{\mat{x}^{(k)}\}_{k \ge 0}$.
Lastly in~\Cref{sec:approx-solve-lifted},
we prove novel convergence guarantees for \emph{approximately} solving the lifted problem
(i.e., for approximate-mini-ALS).
This allows us to directly use fast leverage-score sampling algorithms for
CP decomposition~\citep{cheng2016spals,larsen2022practical,bharadwaj2023fast},
Tucker decomposition~\citep{diao2019optimal,fahrbach2022subquadratic},
and TT decomposition~\citep{bharadwaj2024efficient}
as blackbox subroutines.

\subsection{Lifting to a Structured Problem}
\label{ssec:lifting}
Consider the linear regression problem with $\mat{P}\in\R^{|\Omega| \times R}$ and $\mat{q} \in \R^{|\Omega|}$ given by
\begin{equation}
\label{eqn:input_regression}
    \mat{x}^* = \argmin_{\mat{x} \in \R^R} \,\norm{\mat{P} \mat{x} - \mat{q}}_{2}^2\,.
\end{equation}
If there exists a tall structured matrix $\mat{A} \in \R^{I \times R}$
with a subset of rows $\Omega \subseteq [I]$ such that $\mat{A}_{\Omega} = \mat{P}$
(permutations of the rows allowed),
then we can lift \eqref{eqn:input_regression} to a higher-dimensional problem while preserving the optimal solution.

\begin{restatable}[]{lemma}{LiftedRegression}
\label{lem:lifted_regression}
Let $\mat{b} \in \R^I$ be the lifted response such that $\mat{b}_{\Omega} = \mat{q}$
and $\mat{b}_{\overline{\Omega}}$ is a free variable. If
\begin{equation}
\label{eqn:lifted_regression}
    (\mat{x}^*, \mat{b}^*_{\overline{\Omega}})
    =
    \argmin_{\mat{x} \in \R^R, \mat{b}_{\overline{\Omega}} \in \R^{I - |\Omega|}}\, \norm*{\mat{A} \mat{x} - \mat{b}}_{2}\,,
\end{equation}
then $\mat{x}^*$ also minimizes \eqref{eqn:input_regression},
i.e., the original linear regression problem $\min_{\mat{x} \in \R^R}\, \norm{\mat{P} \mat{x} - \mat{q}}_{2}^2$.
\end{restatable}

\begin{proof}
For any $\mat{x}$, we have
\[
    \norm{\mat{A}\mat{x}-\mat{b}}_2^2
    =
    \norm{\mat{A}_{\Omega}\mat{x} - \mat{b}_{\Omega}}_{2}^2
    +
    \norm{\mat{A}_{\overline\Omega}\mat{x} - \mat{b}_{\overline\Omega}}_{2}^2\,,
\]
so
\[
\min_{\mat{x}}\norm{\mat{P} \mat{x} - \mat{q}}_2^2 \leq \min_{\mat{x},\mat{b}_{\overline{\Omega}}} \norm{\mat{A}\mat{x}-\mat{b}}_2^2\,.
\]
Moreover, for any $\mat{x}$, taking $\mat{b}_{\overline \Omega} = \mat{A}_{\overline \Omega}\mat{x}$ gives us $\norm{\mat{A}_{\overline\Omega}\mat{x} - \mat{b}_{\overline\Omega}}_{2}^2=0$, which implies that
\[
\min_{\mat{x}} \norm{\mat{P} \mat{x} - \mat{q}}_2^2 \geq \min_{\mat{x},\mat{b}_{\overline{\Omega}}} \norm{\mat{A}\mat{x}-\mat{b}}_2^2\,.
\]
Therefore,
\[
\min_{\mat{x}}\norm{\mat{P} \mat{x} - \mat{q}}_2^2 = \min_{\mat{x},\mat{b}_{\overline{\Omega}}} \norm{\mat{A}\mat{x}-\mat{b}}_2^2\,,
\]
and $\mat{x}^*$ also minimizes \eqref{eqn:input_regression}.
\end{proof}

For the rest of this section,
let $\widetilde{\mat{P}} \in \R^{I\times R}$, $\widetilde{\mat{q}}\in \R^{I}$
be the zero-masked lifted matrix and vector such that
\[
    (\widetilde{\mat{P}}_{\Omega}, \widetilde{\mat{P}}_{\overline{\Omega}}) = (\mat{A}_{\Omega}, \boldsymbol{0})
    \quad
    \text{and}
    \quad
    (\widetilde{\mat{q}}_{\Omega}, \widetilde{\mat{q}}_{\overline{\Omega}})
    =
    (\mat{b}_{\Omega}, \boldsymbol{0}).
\]

\begin{restatable}{lemma}{ConvexQuadratic}
\label{lem:lifted_problem_is_convex_quadratic}
Problem~\ref{eqn:lifted_regression} is a convex quadratic program.
\end{restatable}

\begin{proof}
Since $\widetilde{\mat{q}}\in \R^{I}$ is defined as $\widetilde{\mat{q}}_{\Omega} = \mat{b}_{\Omega}$ and $\widetilde{\mat{q}}_{\overline{\Omega}} = \boldsymbol{0}$,
we can write \eqref{eqn:lifted_regression} in the following equivalent manner:
\[
    (\mat{x}^*, \mat{b}^*_{\overline{\Omega}})
    =
    \argmin_{\mat{x} \in \R^R, \mat{b}_{\overline{\Omega}}\in\R^{I-|\Omega|}}
    \norm*{
    \begin{bmatrix}
        \mat{A} & -\mat{I}_{:,\overline{\Omega}}
    \end{bmatrix}
    \begin{bmatrix}
        \mat{x} \\ \mat{b}_{\overline{\Omega}}
    \end{bmatrix}
    -
    \widetilde{\mat{q}}
    }_{2}^2\,,
\]
where $\mat{I}$ is the $I\times I$ identity matrix.
\end{proof}

\begin{remark}
Problem~\ref{eqn:lifted_regression} is  not a linear regression problem
with (structured) design matrix~$\mat{A}$ since there are $\mat{b}_{\overline{\Omega}}$ variables in the response.
There is, however, enough structure to employ block minimization to alternate between minimizing $\mat{x}$ and $\mat{b}_{\overline{\Omega}}$.
\end{remark}

\subsection{Iterative Methods for the Lifted Problem}
\label{subsec:iterative_methods}

Iterative methods for solving linear systems and regression problems have a long history and have been used to expedite several algorithms in theory and practice.
The algorithms we consider use the exact arithmetic model, but all of these methods can be carried out with numbers with $\log \nicefrac{\kappa}\epsilon$ bits, where $\kappa$ is the condition number of the matrix (see, e.g., \citet{ghadiri2023bit,ghadiri2024improving}).
There is a literature on \emph{inexact} Richardson iteration for solving linear systems, 
but they require the error $\widehat{\epsilon}$ to be smaller than than $1/\kappa$,
which is not achievable with leverage-score sampling \cite{golub1988convergence,golub1997closer}.

\begin{lemma}[{Preconditioned Richardson iteration, \citep[Lemma 6.1]{lee2024techniques}}]
\label{lem:richardson_iteration}
Consider the least-squares problem $\mat{x}^* = \argmin_{\mat{x} \in \R^R}\, \norm{\mat{P}\mat{x} - \mat{q}}$.
Let $\mat{M}$ be a matrix such that $\mat{P}^\top \mat{P} \preccurlyeq \mat{M} \preccurlyeq \beta \cdot \mat{P}^\top \mat{P}$
for some $\beta \ge 1$,
and consider the Richardson iteration:
\[
    \mat{x}^{(k+1)} = \mat{x}^{(k)} - \mat{M}^{-1}(\mat{P}^\top \mat{P} \mat{x}^{(k)} - \mat{P}^\top \mat{q})\,.
\]
Then, we have that
\[
    \norm{\mat{x}^{(k+1)} - \mat{x}^*}_{\mat{M}}
    \le
    \Bigl(1 - \frac{1}{\beta}\Bigr)
    \norm{\mat{x}^{(k)} - \mat{x}^*}_{\mat{M}}\,.
\]
\end{lemma}

We now present a key lemma showing that alternating minimization between $\mat{x}$ and $\mat{b}_{\overline{\Omega}}$ corresponds to preconditioned Richardson iterations
on the original least-squares problem.
Below, one can easily check that $\mat{A}$, $\widetilde{\mat{P}}$, and $\widetilde{\mat{q}}$
in our lifted approach satisfy this condition.

\begin{restatable}{lemma}{RichardsonSimulation}
\label{lemma:richardson-simulation}
Let $\mat{A}, \widetilde{\mat{P}} \in \R^{I \times R}$, $\widetilde{\mat{q}}\in\R^{I}$ such that $\widetilde{\mat{P}}-\mat{A}$ and
$\bigl[\begin{matrix}
\widetilde{\mat{P}} & \widetilde{\mat{q}}
\end{matrix}\bigr]$
are orthogonal, i.e., $(\widetilde{\mat{P}}-\mat{A})^\top \bigl[\begin{matrix} \widetilde{\mat{P}} & \widetilde{\mat{q}} \end{matrix}\bigr] = \mat{0}$.
Then, the iterative method
\begin{align*}
    \widetilde{\mat{q}}^{(k)} & = \widetilde{\mat{q}} + (\mat{A} - \widetilde{\mat{P}})\, \mat{x}^{(k)}\,, \\
    \mat{x}^{(k+1)} & = \argmin_{\mat{x} \in \R^{R}}\, \norm{\mat{A} \mat{x} - \widetilde{\mat{q}}^{(k)}}_2^2\,,
\end{align*}
simulates Richardson iterations with preconditioner $\mat{A}^\top \mat{A}$
for the regression problem $\min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x} - \widetilde{\mat{q}}}_2^2$,
i.e.,
\begin{equation}
    \label{eq:update-rule}
    \mat{x}^{(k+1)}
    =
    \mat{x}^{(k)} - (\mat{A}^\top \mat{A})^{-1} (\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \mat{x}^{(k)} - \widetilde{\mat{P}}^\top \widetilde{\mat{q}})\,.
\end{equation}
\end{restatable}

\begin{proof}
Assume that $\mat{A}^\top \mat{A}$ is full rank.
Solving the normal equation for $\mat{x}^{(k+1)}$,
\begin{align*}
\mat{x}^{(k+1)}
&= (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top \widetilde{\mat{q}}^{(k)}\\
&= (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top (\widetilde{\mat{q}} + (\mat{A} - \widetilde{\mat{P}})\, \mat{x}^{(k)})
\\ & =
\mat{x}^{(k)} - (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top (\widetilde{\mat{P}} \mat{x}^{(k)} - \widetilde{\mat{q}})\,.
\end{align*}
Since $\widetilde{\mat{P}}-\mat{A}$ and $\bigl[\begin{matrix}
\widetilde{\mat{P}} & \widetilde{\mat{q}}
\end{matrix}\bigr]$ are orthogonal, 
\begin{align*}
    \mat{A}^\top (\widetilde{\mat{P}} \mat{x}^{(k)} - \widetilde{\mat{q}})
    &=
    \bigl(\widetilde{\mat{P}} - (\widetilde{\mat{P}} - \mat{A})\bigr)^\top \bigl[\begin{matrix}
        \widetilde{\mat{P}} & \widetilde{\mat{q}}
    \end{matrix}\bigr] \begin{bmatrix}
    \mat{x}^{(k)} \\ -1
    \end{bmatrix} \\
    &= 
    \widetilde{\mat{P}}^\top (\widetilde{\mat{P}} \mat{x}^{(k)} - \widetilde{\mat{q}})\,.
\end{align*}
Therefore,
\[
    \mat{x}^{(k+1)}
    =
    \mat{x}^{(k)} - (\mat{A}^\top \mat{A})^{-1} (\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \mat{x}^{(k)} - \widetilde{\mat{P}}^\top \widetilde{\mat{q}})\,,
\]
which completes the proof.
\end{proof}

\begin{remark}
In the tensor completion setting, $\mat{A}-\widetilde{\mat{P}}$ vanishes over $\Omega$,
so $\widetilde{\mat{q}}^{(k)}$ only updates entries in $\overline{\Omega}$ while maintaining $\mat{q}$ on $\Omega$.
Thus, computing $\mat{x}^{(k+1)}$ corresponds to
\begin{align*}
    \mat{x}^{(k+1)}
    &=
    \argmin_{\mat{x} \in \R^R}\, \norm{\mat{Ax}-\widetilde{\mat{q}}^{(k)}}^2_2
    = 
    \argmin_{\mat{x} \in \R^R}\, \bigl\{\norm{\mat{Px}-\mat{q}}^2_2 + \norm{\mat{A}_{\overline \Omega}\,(\mat x - \mat x^{(k)})}_2^2\bigr\}\,.
\end{align*}
\end{remark}

\subsection{Approximately Solving the Lifted Problem}
\label{sec:approx-solve-lifted}

We have shown that alternating minimization for the lifted problem~\eqref{eqn:lifted_regression}
has strong connections to preconditioned Richardson iteration
and inherits its convergence guarantees.
However, for this observation to be useful,
we need to use fast regression algorithms for the $\mat{x}^{(k+1)}$ updates that \emph{exploit the structure} of $\mat{A}$,
i.e.,
when solving $\min_{\mat x}\, \norm{\mat{Ax} - \widetilde{\mat{q}}^{(k)}}_2$.

This is where leverage score sampling comes in to play.
We exploit the structure of $\mat{A}$ to efficiently compute its leverage scores,
and then we solve the regression problem efficiently but \emph{approximately}.
By using a sketching method, our work deviates from the standard (exact) Richardson iteration.

Our next result shows how using approximate least-squares solutions
\emph{in each step of block minimization}
affects the convergence guarantee of our lifted iterative method.

\begin{algorithm2e}[t]
    \caption{\LiftedApproximateSolver}
    \label{alg:approximate-lifting}
	\BlankLine
	\KwData{$\mat{A},\widetilde{\mat{P}} \in \R^{I \times R}$, $\widetilde{\mat{q}}\in\R^{I}$, $\beta \geq 1$, $\epsilon \in (0,1)$, $\widehat{\epsilon} \in [0, 1/\beta^2)$ with $\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \preceq \mat{A}^\top \mat{A} \preceq \beta \cdot \widetilde{\mat{P}}^\top \widetilde{\mat{P}}$}
	\KwResult{$\widetilde{\mat{x}} \in \R^{R}$}
	\BlankLine
	Initialize $\mat{x}^{(0)}=\boldsymbol{0}$ \\
        
        \For{$k = 0, 1, \dots, \ceil*{\frac{\log(\sfrac{2\beta}{\epsilon})}{2\,(\sfrac{1}{\beta} - \sqrt{\widehat{\epsilon}})} }$
        }{  
            Set $\widetilde{\mat{q}}^{(k)} \gets \widetilde{\mat{q}} + (\mat{A} - \widetilde{\mat{P}})\, \mat{x}^{(k)}$
            \hfill {\color{Navy} \tcp{\texttt{Implicit}}}
            Set $\mat{x}^{(k+1)}$ to a vector such that $ \norm{\mat{A} \mat{x}^{(k+1)} - \widetilde{\mat{q}}^{(k)}}_2^2 \leq(1+\widehat{\epsilon})\,\min_{\mat{x}}\, \norm{\mat{A} \mat{x} - \widetilde{\mat{q}}^{(k)}}_2^2$
        }
    \Return $\mat{x}^{(k)}$
\end{algorithm2e}

\begin{restatable}{theorem}{ApproximateRichardson}
\label{thm:approximate-richardson}
Let $\mat{A},\widetilde{\mat{P}} \in \R^{I \times R}$, $\widetilde{\mat{q}}\in\R^{I}$, and $\beta \ge 1$
such that
$\widetilde{\mat{P}}-\mat{A}$ and $\bigl[\begin{matrix} \widetilde{\mat{P}} & \widetilde{\mat{q}} \end{matrix}\bigr]$ are orthogonal, and
\[
    \widetilde{\mat{P}}^\top \widetilde{\mat{P}}
    \preceq
    \mat{A}^\top \mat{A}
    \preceq
    \beta \cdot \widetilde{\mat{P}}^\top \widetilde{\mat{P}}\,.
\]
Let $\epsilon \in (0,1), \widehat{\epsilon} \in [0,1/\beta^2)$ and
\ApproximateSolve be an algorithm that for any
$\widehat{\mat{x}}\in\R^{R}$ and $\mat{f}=\widetilde{\mat{q}}+(\mat{A}-\widetilde{\mat{P}})\, \widehat{\mat{x}}$,
computes $\overline{\mat{x}} \in \R^{R}$ in time $O(T)$ such that
\[
\norm{\mat{A}\overline{\mat{x}}-\mat{f}}_2^2\leq (1+\widehat{\epsilon})\,\min_{\mat{x}}\, \norm{\mat{A}\mat{x}-\mat{f}}_2^2\,.
\]
Then, Algorithm~\ref{alg:approximate-lifting} returns an approximate solution
$\widetilde{\mat{x}} \in \R^{R}$, using  \ApproximateSolve as a subroutine, such that
\begin{align*}
    \norm{\widetilde{\mat{P}} \widetilde{\mat{x}}-\widetilde{\mat{q}}}_2^2
    &\leq
    \parens*{1 + \frac{2 \widehat{\epsilon}}{(\sfrac{1}{\beta} - \sqrt{\widehat{\epsilon}})^2}}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2 
    + \epsilon\, \norm{\widetilde{\mat{P}}\,(\widetilde{\mat{P}}^\top \widetilde{\mat{P}})^{-1} \widetilde{\mat{P}}^\top \widetilde{\mat{q}}}_2^2\,,
\end{align*}
in $O\parens*{\frac{\beta}{1-\sqrt{\widehat{\epsilon}} \beta} \cdot  T \log \sfrac{\beta}\epsilon}$ time.
\end{restatable}

\begin{proof}
We show that Algorithm~\ref{alg:approximate-lifting} gives the desired output.
Suppose \ApproximateSolve yields $\mat{x}^{(k+1)}$ for given inputs $\mat{A},\widetilde{\mat{P}},\widetilde{\mat{q}}$, and $\widetilde{\mat{q}}^{(k)}$ (i.e., $\widehat{\mat{x}} \gets \mat{x}^{(k)}$, $\mat{f}\gets \widetilde{\mat{q}}^{(k)}$, and $\overline{\mat{x}} \gets \mat{x}^{(k+1)}$), which satisfies
\[
    \norm{\mat{A}\mat{x}^{(k+1)}-\widetilde{\mat{q}}^{(k)}}_2^2\leq (1+\widehat{\epsilon})\,\min_{\mat{x}} \norm{\mat{A}\mat{x}-\widetilde{\mat{q}}^{(k)}}_2^2
    = (1+\widehat{\varepsilon})\,\norm{\pi_{\mat A ^\perp}\widetilde{\mat{q}}^{(k)}}_2^2\,.
\]
We can also decompose the LHS using $\widetilde{\mat{q}}^{(k)} = \pi_{\mat A} \widetilde{\mat{q}}^{(k)} + \pi_{\mat A ^\perp} \widetilde{\mat{q}}^{(k)}$ as follows:
\[
    \norm{\mat{A}\mat{x}^{(k+1)}-\widetilde{\mat{q}}^{(k)}}_2^2
    = 
    \norm{\mat{A}\mat{x}^{(k+1)} - \pi_{\mat{A}}\widetilde{\mat{q}}^{(k)}}_2^2 + \norm{\pi_{\mat{A}^{\perp}} \widetilde{\mat{q}}^{(k)}}_2^2\,.
\]
Combining the above, we get
\[
    \norm{\mat{A}\mat{x}^{(k+1)} - \pi_{\mat{A}}\widetilde{\mat{q}}^{(k)}}_2^2
    \leq
    \widehat{\epsilon}\, \norm{\pi_{\mat{A}^{\perp}}\widetilde{\mat{q}}^{(k)}}_2^2\,.
\]
Denoting $\mat{x}^* = (\widetilde{\mat{P}}^\top \widetilde{\mat{P}})^{-1} \widetilde{\mat{P}}^\top \widetilde{\mat{q}} = \argmin_{\mat x}\,\norm{\mat{Bx}-\widetilde{\mat{q}}}_2$ and using the triangle inequality,
\begin{align}
\nonumber
    \norm{\mat{A}\mat{x}^{(k+1)} - \mat{A}\mat{x}^*}_2
    &
    \leq \norm{\mat{A}\mat{x}^{(k+1)} - \pi_{\mat A}\widetilde{\mat{q}}^{(k)}}_2 + \norm{\pi_{\mat A}\widetilde{\mat{q}}^{(k)} - \mat{A} \mat{x}^*}_2
    \\ & \leq \label{eq:total-bound-on-a-norm}
    \sqrt{\widehat{\epsilon}}\, \norm{\pi_{\mat{A}^{\perp}}\widetilde{\mat{q}}^{(k)}}_2 + \norm{\pi_{\mat A}\widetilde{\mat{q}}^{(k)} - \mat{A} \mat{x}^*}_2\,.
\end{align}

We now bound each term in the RHS. As for the second term, since $\widetilde{\mat{q}}^{(k)} = \widetilde{\mat{q}} + (\mat{A} - \widetilde{\mat{P}})\, \mat{x}^{(k)}$ and $(\widetilde{\mat{P}}-\mat{A})^\top \begin{bmatrix}
\widetilde{\mat{P}} & \widetilde{\mat{q}} \end{bmatrix} = 0$, by \Cref{lemma:richardson-simulation},
\begin{align*}
    (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top\widetilde{\mat{q}}^{(k)}
    &=
    \argmin_{\mat x}\,\norm{\mat{Ax}-\widetilde{\mat{q}}^{(k)}}_2 \\
    &=
    \mat{x}^{(k)} - (\mat{A}^\top \mat{A})^{-1} (\widetilde{\mat{P}}^\top\widetilde{\mat{P}}\mat{x}^{(k)} - \widetilde{\mat{P}}^\top\widetilde{\mat{q}})\,,
\end{align*}
which is exactly a Richardson iteration with preconditioner $\mat M\gets \mat{A}^\top \mat{A}$ in \Cref{lem:richardson_iteration} (satisfying $\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \preceq \mat{A}^\top \mat{A}\preceq \beta\,\widetilde{\mat{P}}^\top \widetilde{\mat{P}}$). 
Thus, $\norm{(\mat{A}^\top \mat{A})^{-1} \mat{A}^\top\widetilde{\mat{q}}^{(k)} - \mat{x}^*}_{\mat{A}^\top \mat{A}} \leq (1-\beta^{-1})\,\norm{\mat{x}^{(k)} - \mat{x}^*}_{\mat{A}^\top \mat{A}}$, and
\begin{equation}
    \label{eq:second-term-bound-on-a-norm}
    \norm{\pi_{\mat A}\widetilde{\mat{q}}^{(k)} - \mat{A} \mat{x}^*}_2
    \leq
    \parens*{1-\frac 1 \beta}\, \norm{\mat{A} \mat{x}^{(k)} - \mat{A}\mat{x}^*}_2\,.
\end{equation}

Regarding the first term in \eqref{eq:total-bound-on-a-norm}, since $\mat{Ax}^{(k)}$ is in the column space of $\mat{A}$,
\[
\pi_{\mat A^\perp}\widetilde{\mat{q}}^{(k)} 
= 
\pi_{\mat A^\perp}\bigl(\widetilde{\mat{q}} +(\mat{A-B})\,\mat x^{(k)}\bigr)
=
\pi_{\mat A^\perp}(\widetilde{\mat{q}} - \widetilde{\mat{P}} \mat{x}^{(k)})\,.
\]
Therefore,
\begin{align*}
\norm{\pi_{\mat A^\perp}\widetilde{\mat{q}}^{(k)}}_2^2
& \leq 
\norm{\widetilde{\mat{q}} - \widetilde{\mat{P}} \mat{x}^{(k)}}_2^2
\\ & =
\norm{\widetilde{\mat{P}} \mat{x}^{*} - \widetilde{\mat{P}} \mat{x}^{(k)}}_2^2 + \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2
\\ & \leq 
\norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(k)}}_2^2 + \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2\,,
\end{align*}
where the last inequality follows from $\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \preceq \mat{A}^\top \mat{A}$.
Thus,
\begin{equation}
\label{eq:first-term-bound-on-a-norm}
\norm{\pi_{\mat A^\perp}\widetilde{\mat{q}}^{(k)}}_2
\leq 
\norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(k)}}_2 + \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2\,.
\end{equation}

Combining \eqref{eq:total-bound-on-a-norm}, \eqref{eq:second-term-bound-on-a-norm}, and \eqref{eq:first-term-bound-on-a-norm}, we have
\[
    \norm{\mat{A}\mat{x}^{(k+1)} - \mat{A}\mat{x}^*}_2
    \leq
    \parens*{1-\frac{1}{\beta} + \sqrt{\widehat{\epsilon}}}\, \norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(k)}}_2 + \sqrt{\widehat{\epsilon}}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2\,.
\]
Denoting $\alpha=1-\frac{1}{\beta} + \sqrt{\widehat{\epsilon}}\,$, by induction, we have
\begin{align}
\nonumber
\norm{\mat{A}\mat{x}^{(k)} - \mat{A}\mat{x}^*}_2 
& \leq 
\alpha^k\, \norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(0)}}_2 + (1+\alpha+\alpha^2 + \cdots + \alpha^{k-1}) \times \sqrt{\widehat{\epsilon}}\,\min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2
\\ & =
\label{eq:col-space-bound-for-a}
\alpha^k\, \norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(0)}}_2 + \frac{1-\alpha^k}{1-\alpha}\times \sqrt{\widehat{\epsilon}}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2\,.
\end{align}

We also have
\begin{align}
\nonumber
\norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{q}}}_2^2 
&= \norm{\widetilde{\mat{P}} \mat{x}^{(k)} - \pi_{\widetilde{\mat{P}}}\widetilde{\mat{q}}}_2^2 + \norm{\pi_{\widetilde{\mat{P}}^\perp}\widetilde{\mat{q}}}_2^2
\\ & = 
\label{eq:total-error}
\norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{P}} \mat{x}^*}_2^2 + \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x} - \widetilde{\mat{q}}}_2^2\,.
\end{align}
We then bound the first term by using $\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \preceq \mat{A}^\top \mat{A} \preceq \beta\, \widetilde{\mat{P}}^\top \widetilde{\mat{P}}$ and \eqref{eq:col-space-bound-for-a} as follows:
\begin{align}
    \nonumber
    \norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{P}} \mat{x}^*}_2^2 
    &\leq
    \norm{\mat{A} \mat{x}^{(k)}-\mat{A} \mat{x}^*}_2^2 \\
    &\leq
    \nonumber
    2\alpha^{2k}\, \norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(0)}}_2^2 + 2\, \parens*{\frac{1-\alpha^k}{1-\alpha}}^2 \times \widehat{\epsilon}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2 \\
    &\leq 
    \nonumber
    2 \beta\alpha^{2k}\, \norm{\widetilde{\mat{P}} \mat{x}^{*} - \widetilde{\mat{P}} \mat{x}^{(0)}}_2^2 + 2\, \parens*{\frac{1-\alpha^k}{1-\alpha}}^2 \times \widehat{\epsilon}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2\,.
\end{align}
Putting this bound back into \eqref{eq:total-error},
\[
\norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{q}}}_2^2 \leq 2 \beta\alpha^{2k}\, \norm{\widetilde{\mat{P}} \mat{x}^{*} - \widetilde{\mat{P}} \mat{x}^{(0)}}_2^2 + \parens*{1 + 2\widehat{\epsilon}\, \parens*{\frac{1-\alpha^k}{1-\alpha}}^2}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2\,.
\]

Setting
\[
k = \ceil*{\frac{\log(\sfrac{2\beta}{\epsilon})}{2\,(\nicefrac 1 \beta -\sqrt{\widehat{\epsilon}})}}\,,
\]
we have
\[
    \norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{q}}}_2^2
    \leq
    \epsilon\, \norm{\widetilde{\mat{P}} \mat{x}^{*} - \widetilde{\mat{P}} \mat{x}^{(0)}}_2^2 + \parens*{1+ \frac{2 \widehat{\epsilon}}{(\nicefrac 1  \beta - \sqrt{\widehat{\epsilon}})^2} }\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2\,,
\]
which completes the proof with $\mat{x}^{(0)}=\boldsymbol{0}$.
\end{proof}

\begin{remark}
To better understand the theorem, observe that
$\widetilde{\mat{P}}^\top \widetilde{\mat{P}} = \mat{P}^\top \mat{P}$ is a $\beta$-spectral approximation of $\mat{A}^\top \mat{A}$,
$\varepsilon$ controls the reducible error $\varepsilon\, \norm{\widetilde{\mat{P}} \mat{x}^*}_{2}^2$,
and $(1 + \widehat{\varepsilon})$ is the error in the approximate least-square update for each $\mat{x}^{(k)}$.
\end{remark}

\paragraph{Bounding $\beta$.}
First, observe that in the case of TD,
we have $\mat{P} = \mat{A}$, so $\beta = 1$.
More generally, if $\textnormal{rank}(\mat{A}) = s \le \min\{I, R\}$
and $\mat{A} = \mat{U}\mat{\Sigma}\mat{V}^\top$ is a compressed SVD, then $\mat{A}$ is said to satisfy  the \emph{standard incoherence condition} with parameter $\mu$ \citep{chen2015incoherence} if
\[
    \max_{i\in[I]} \norm*{\mat{e}_{i}^\top \mat{U}}_2 \leq \sqrt{\frac{\mu s}{I}}\,,
    \quad
    \max_{r\in[R]} \norm*{\mat{V} ^\top\mat{e}_r}_2 \leq \sqrt{\frac{\mu s}{R}}\,.
\]
The $\norm{\mat{e}_{i}^\top \mat{U}}_2^2$ and $\norm{\mat{V} ^\top\mat{e}_r}_2^2$ values are the leverage scores of the rows and columns of $\mat{A}$.
Applying \citet[Lemma 4]{cohen2015uniform},
if each row of $\mat{A}$ is observed with probability $p$ such that $p \geq \frac{c\mu s \log s}{I}$ for some absolute constant $c$, then 
\[
    \frac{1}{2} \,\mat{A}^\top \mat{A}
    \preceq
    \frac{1}{p} \, \mat{A}_{\Omega}^\top \mat{A}_{\Omega}
    \preceq
    \frac{3}{2}\, \mat{A}^\top \mat{A}\,,
\]
which gives $\beta=2/p$.
Let $\zeta = \max_{i\in[I]} \norm{\mat{a}_i}_2$, where $\mat{a}_i$ is row $i$ of $\mat{A}$.
Then, the $\alpha \zeta^2$-ridge leverage scores of $\mat{A}$ (i.e., $\mat{a}_i^\top (\mat{A}^\top \mat{A} + \alpha \zeta^2 \cdot\mat{I}_{R})^{-1} \mat{a}_i$), for $\alpha\geq 1$, are at most $1/\alpha$. If $p$ is the observation rate, taking $\alpha = \frac{c\log s}{p}$ gives the required incoherence condition. This can be done by introducing an $\ell_2$-regularization term to the TC optimization problem (i.e., solving a ridge regression problem in each ALS step).
Note that usually $\alpha$ can be chosen to be much smaller in practice.
