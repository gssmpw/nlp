\section{Experiments}
\label{sec:experiments}

\subsection{Warm-Up: Coupled Matrix Problem}
We study the \emph{coupled matrix problem} $\mat{A} \mat{X} \mat{B}^\top + \mat{C} \mat{Y} \mat{D}^\top = \mat{E}$, where $\mat{A},\mat{B},\mat{C},\mat{D}$ are given, $\mat{X},\mat{Y}$ are unknown, and $\mat{E}$ is a matrix with half of its entries randomly revealed~\citep{baksalary1980matrix}.
For a fixed $\mat{Y}$, we can compute $\mat{X}$ by solving $\min_{\mat{x}} \norm{(\mat{B} \kron \mat{A}) \mat{x} - \vvec(\mat{E} - \mat{C} \mat{Y} \mat{D}^\top)}_2$.
Thus, we can apply an alternating minimization algorithm for computing $\mat{X}$ and $\mat{Y}$.
We present the results in~\Cref{fig:kronecker} and give the full details in \Cref{app:kronecker_regression_experiments}.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Kronecker/time_comparison_plot_coupled_2000_10_seed1234.png}
    \end{subfigure}
    \hspace{0.90cm}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Kronecker/error_comparison_plot_coupled_2000_10_seed1234.png}
    \end{subfigure}
    \caption{Coupled matrix results for $\mat{E} \in \R^{n \times n}$, $\mat{X}, \mat{Y} \in \R^{d \times d}$ with $n=2000$ and $d=10$
    comparing the direct method, mini-ALS, and approximate-mini-ALS via leverage score sampling.
    }
    \label{fig:kronecker}
\end{figure}

\subsection{CP Completion}
For a given tensor $\tensor{X}$ and sample ratio $p \in [0, 1]$,
let $\tensor{X}_{\Omega}$ be a partially observed tensor with a random $p$ fraction of entries revealed.
We fit $\tensor{X}_{\Omega}$ with a rank-$R$ CP decomposition
by minimizing the training \emph{relative reconstruction error} (RRE)
$\norm{(\widehat{\tensor{X}} - \tensor{X})_{\Omega}}_{\frobenius} / \norm{\tensor{X}_{\Omega}}_{\frobenius}$
using different ALS algorithms.

\paragraph{Datasets.}
\textsc{cardiac-mri} is a $256 \times 256 \times 14 \times 20$ tensor
of MRI measurements indexed by $(x,y,z,t)$ where $(x,y,z)$ is a point in space and $t$ corresponds to time.
\textsc{hyperspectral} is $1024 \times 1344 \times 33$ tensor of time-lapse hyperspectral radiance images~\citep{nascimento2016spatial}.
We also consider random low-rank CP and Tucker tensors in \Cref{app:synthetic_experiments}.

\paragraph{Algorithms.}
We compare
\texttt{direct}, \texttt{parafac}, \texttt{mini-als}, and \texttt{accelerated-mini-als} methods.
\texttt{direct} solves the normal equation in each ALS step
and has running time $O(|\Omega| R^2 + R^3)$ since it computes $(\mat{P}^\intercal \mat{P})^{-1}$.
\texttt{mini-als} is our lifting approach in Algorithm~\ref{alg:approximate-lifting}
with $\widehat{\varepsilon} = 0$ and $\varepsilon > 0$.
\texttt{parafac-als} is the EM algorithm of \citet{tomasi2005parafac}.
\texttt{accelerated-mini-als} uses \emph{adaptive} step sizes based on the trajectory of $\mat{x}^{(k)}$ (see \Cref{app:acceleration} for details).

\paragraph{Results.}
In \Cref{fig:experiments}, we present plots for \textsc{cardiac-mri} in the top row and \textsc{hyperspectral} in the bottom row.

\begin{itemize}
\item In the first column,
we set $p = 0.1$ and sweep over the rank $R$ using \texttt{direct} to demonstrate how train RRE (solid line) and test RRE $\norm{\widehat{\tensor{X}} - \tensor{X}}_{\frobenius} / \norm{\tensor{X}}_{\frobenius}$ (dashed line) decrease as a function of
$R$ and the ALS step count.
Note that test RRE is the loss on the entire (mostly unobserved) tensor and is slightly above the train RRE curves at all times.

\item In the second column, we fix $(p,R)=(0.1,16)$ and
study the solution quality of \texttt{mini-als} for different $\varepsilon$
compared to the \texttt{direct} and \texttt{parafac} benchmarks.
As $\varepsilon$ decreases, we indeed recover solutions in each ALS step that are as good as \texttt{direct},
i.e., lifting simulates the preconditioned Richardson iteration (\Cref{lemma:richardson-simulation}).

\item In the third column, we sweep over $p$ for $R = 16$ and plot the
\emph{total running time} of \texttt{direct}, \texttt{parafac}, and \texttt{mini-als} for $10$ rounds of ALS.
The running time of \texttt{direct} increases linearly in $p \propto |\Omega|$ as expected.
Interestingly, the running time of \texttt{mini-als} decreases as $|\Omega|$ increases, for $\varepsilon < 0.1$,
since the lifted (structured) matrix $\mat{A}^\top \mat{A}$ becomes a better preconditioner for the TC design matrix $\mat{A}_{\Omega}$ (i.e., $\beta \rightarrow 1$ as $p \rightarrow 1$).
This means \texttt{mini-als} needs fewer iterations to converge.
Finally, \texttt{parafac} performs one mini-ALS iteration in each ALS step and is therefore faster but converges at a slower rate.

\item In the fourth column, we compare the total running time of \texttt{accelerated-mini-als} (dash-dot lines)
to \texttt{mini-als} in the third column.
Our accelerated method extrapolates the trajectory of $\mat{x}^{(k)}$ during mini-ALS
using a geometric series,
which allows us to solve collinear iterates in one step (see \Cref{fig:extrapol} for an illustration).
\texttt{accelerated-mini-als} achieves better solution quality than \texttt{mini-als} for a given $\varepsilon$ and always runs faster, especially for small values of $p$.
\end{itemize}

% --------------------------------------------------------------------
\input{experiments_figures}
% --------------------------------------------------------------------
