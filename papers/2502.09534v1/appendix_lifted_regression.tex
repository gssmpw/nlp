\section{Missing Analysis for \Cref{sec:lifted_regression}}
\label{app:lifted_regression}

%\todo{Fix restatable bug where restated lemmas using appendix subsection numbers.}
% Relevant: https://tex.stackexchange.com/questions/516147/why-is-restatable-giving-a-new-number-to-restated-theorems

% --------------------------------------------------------
\subsection{Proof of \Cref{lem:lifted_regression}}
% --------------------------------------------------------

\LiftedRegression*

\begin{proof}
For any $\mat{x}$, we have
\[
    \norm{\mat{A}\mat{x}-\mat{b}}_2^2
    =
    \norm{\mat{A}_{\Omega}\mat{x} - \mat{b}_{\Omega}}_{2}^2
    +
    \norm{\mat{A}_{\overline\Omega}\mat{x} - \mat{b}_{\overline\Omega}}_{2}^2\,,
\]
so
\[
\min_{\mat{x}}\norm{\mat{P} \mat{x} - \mat{q}}_2^2 \leq \min_{\mat{x},\mat{b}_{\overline{\Omega}}} \norm{\mat{A}\mat{x}-\mat{b}}_2^2\,.
\]
Moreover, for any $\mat{x}$, taking $\mat{b}_{\overline \Omega} = \mat{A}_{\overline \Omega}\mat{x}$ gives us $\norm{\mat{A}_{\overline\Omega}\mat{x} - \mat{b}_{\overline\Omega}}_{2}^2=0$, which implies that
\[
\min_{\mat{x}} \norm{\mat{P} \mat{x} - \mat{q}}_2^2 \geq \min_{\mat{x},\mat{b}_{\overline{\Omega}}} \norm{\mat{A}\mat{x}-\mat{b}}_2^2\,.
\]
Therefore,
\[
\min_{\mat{x}}\norm{\mat{P} \mat{x} - \mat{q}}_2^2 = \min_{\mat{x},\mat{b}_{\overline{\Omega}}} \norm{\mat{A}\mat{x}-\mat{b}}_2^2\,,
\]
and $\mat{x}^*$ also minimizes \eqref{eqn:input_regression}.
\end{proof}

% --------------------------------------------------------
\subsection{Proof of \Cref{lem:lifted_problem_is_convex_quadratic}}
% --------------------------------------------------------

\ConvexQuadratic*

\begin{proof}
Since $\widetilde{\mat{q}}\in \R^{I}$ is defined as $\widetilde{\mat{q}}_{\Omega} = \mat{b}_{\Omega}$ and $\widetilde{\mat{q}}_{\overline{\Omega}} = \boldsymbol{0}$,
we can write \eqref{eqn:lifted_regression} in the following equivalent manner:
\[
    (\mat{x}^*, \mat{b}^*_{\overline{\Omega}})
    =
    \argmin_{\mat{x} \in \R^R, \mat{b}_{\overline{\Omega}}\in\R^{I-|\Omega|}}
    \norm*{
    \begin{bmatrix}
        \mat{A} & -\mat{I}_{:,\overline{\Omega}}
    \end{bmatrix}
    \begin{bmatrix}
        \mat{x} \\ \mat{b}_{\overline{\Omega}}
    \end{bmatrix}
    -
    \widetilde{\mat{q}}
    }_{2}^2\,,
\]
where $\mat{I}$ is the $I\times I$ identity matrix.
\end{proof}

% --------------------------------------------------------
\subsection{Proof of \Cref{lemma:richardson-simulation}}
% --------------------------------------------------------

\RichardsonSimulation*

\begin{proof}
Assume that $\mat{A}^\top \mat{A}$ is full rank.
Solving the normal equation for $\mat{x}^{(k+1)}$,
\begin{align*}
\mat{x}^{(k+1)}
&= (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top \widetilde{\mat{q}}^{(k)}\\
%&= (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top \mat{d}^{(k)} \\
%&= (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top (\mat{d}^{(k)} - \mat{A} \mat{x}^{(k)} + \mat{A} \mat{x}^{(k)}) \\
&= (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top (\widetilde{\mat{q}} + (\mat{A} - \widetilde{\mat{P}})\, \mat{x}^{(k)})
\\ & =
\mat{x}^{(k)} - (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top (\widetilde{\mat{P}} \mat{x}^{(k)} - \widetilde{\mat{q}})\,.
\end{align*}
%Let $\widetilde{\mat{d}}^{(k)} \mat{A} \mat{x}^{(k)}$. %\kook{Feels like we don't really need to define this notation? It's only used once in (7) below.}
% Therefore,
% \begin{align*}
% \end{align*}
Since $\widetilde{\mat{P}}-\mat{A}$ and $\bigl[\begin{matrix}
\widetilde{\mat{P}} & \widetilde{\mat{q}}
\end{matrix}\bigr]$ are orthogonal, 
\begin{align*}
    \mat{A}^\top (\widetilde{\mat{P}} \mat{x}^{(k)} - \widetilde{\mat{q}})
    &=
    \bigl(\widetilde{\mat{P}} - (\widetilde{\mat{P}} - \mat{A})\bigr)^\top \bigl[\begin{matrix}
        \widetilde{\mat{P}} & \widetilde{\mat{q}}
    \end{matrix}\bigr] \begin{bmatrix}
    \mat{x}^{(k)} \\ -1
    \end{bmatrix} \\
    &= 
    \widetilde{\mat{P}}^\top (\widetilde{\mat{P}} \mat{x}^{(k)} - \widetilde{\mat{q}})\,.
\end{align*}
Therefore,
\[
    \mat{x}^{(k+1)}
    =
    \mat{x}^{(k)} - (\mat{A}^\top \mat{A})^{-1} (\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \mat{x}^{(k)} - \widetilde{\mat{P}}^\top \widetilde{\mat{q}})\,,
\]
which completes the proof.
\end{proof}

% --------------------------------------------------------
\subsection{Proof of \Cref{thm:approximate-richardson}}
% --------------------------------------------------------

\ApproximateRichardson*

\begin{proof}
% \iffalse
% We first analyze what happens for $\widehat{\varepsilon} = 0$, and then we consider the case when $\widehat{\varepsilon} > 0$.

% \paragraph{Case 1: $\widehat{\varepsilon} = 0$.}
% By \Cref{lemma:richardson-simulation}, \Cref{alg:lifted_block_coordinate_descent}
% simulates the preconditioned Richardson's iteration for solving $\min_{\mat{x}}\norm{\mat{B} \mat{x} - \mat{d}}$ with $\mat{A}^\top \mat{A}$ as the preconditioner.
% Therefore, by \Cref{lem:richardson_iteration}, starting from $\mat{x}^{(0)} = \mat{0}$ after $k$ iterations,
% \[
%     \norm{\mat{x}^{(k)} - \mat{x}^*}_{\mat{A}^\top \mat{A}} \leq \parens*{1-\frac{1}{\beta}}^k \norm{\mat{x}^*}_{\mat{A}^\top \mat{A}},
% \]
% where $\mat{x}^*=\argmin_{\mat{x}}  \norm{\mat{B} \mat{x}-\mat{d}}_2^2$.
% Rewrite $\mat{d}$ as
% \[
%     \mat{d} = \mat{B} (\mat{B}^\top \mat{B})^{-1} \mat{B}^\top \mat{d} + (\mat{I} - \mat{B} (\mat{B}^\top \mat{B})^{-1} \mat{B}^\top) \mat{d}.
% \]
% Since $\mat{B}$ is orthogonal to $(\mat{I} - \mat{B} (\mat{B}^\top \mat{B})^{-1} \mat{B}^\top)$, i.e., $\mat{B}^\top (\mat{I} - \mat{B} (\mat{B}^\top \mat{B})^{-1} \mat{B}^\top) = \mat{0}$,
% and recalling that $\mat{x}^* = (\mat{B}^\top \mat{B})^{-1} \mat{B}^\top \mat{d}$, we have
% \begin{align*}
% \norm{\mat{B} \mat{x}^{(k)}-\mat{d}}_2^2 & = \norm{\mat{B} \mat{x}^{(k)}-\mat{B} (\mat{B}^\top \mat{B})^{-1} \mat{B}^\top \mat{d}}_2^2 + \norm{(\mat{I} - \mat{B} (\mat{B}^\top \mat{B})^{-1} \mat{B}^\top) \mat{d}}_2^2
% \\ & = 
% \norm{\mat{B} \mat{x}^{(k)}-\mat{B} \mat{x}^*}_2^2 + \min_{\mat{x}} \norm{\mat{B} \mat{x} - \mat{d}}_2^2.
% \end{align*}
% Moreover,
% \begin{align*}
% \norm{\mat{B} \mat{x}^{(k)}-\mat{B} \mat{x}^*}_2^2 
% & =
% (\mat{x}^{(k)} - \mat{x}^*)^\top \mat{B}^\top \mat{B} (\mat{x}^{(k)} - \mat{x}^*)
% \\ & \leq
% (\mat{x}^{(k)} - \mat{x}^*)^\top \mat{A}^\top \mat{A} (\mat{x}^{(k)} - \mat{x}^*)
% \\ & \leq 
% (1-1/\beta)^{2k} (\mat{x}^{*})^\top \mat{A}^\top \mat{A} \mat{x}^{*}
% \\ & \leq 
% \beta (1-1/\beta)^{2k} (\mat{x}^{*})^\top \mat{B}^\top \mat{B} \mat{x}^{*}
% \\ & \leq 
% \beta \exp(-2k/\beta) (\mat{x}^{*})^\top \mat{B}^\top \mat{B} \mat{x}^{*}.
% \end{align*}
% Therefore, taking $k = \lceil \frac{\beta \log(\beta/\epsilon)}{2} \rceil$, we have
% \[
% \norm{\mat{B} \mat{x}^{(k)}-\mat{d}}_2^2 \leq \min_{\mat{x}} \norm{\mat{B} \mat{x}-\mat{d}}_2^2 + \epsilon \cdot \norm{\mat{B} (\mat{B}^\top \mat{B})^{-1} \mat{B}^\top \mat{d}}_2^2.
% \]

% \todo{The following should become a separate lemma and change the above to work for any $\widehat{\epsilon}$.}

% \paragraph{Case 2: }$\widehat{\varepsilon} > 0$.
% \fi
% \iffalse
% We claim that
% \[
%     \norm{\mat{x}^{(k)} - \mat{x}^*}_{\mat{A}^\top \mat{A}}
%     \leq
%     \parens*{1-\frac{1}{\beta} + \widehat{\epsilon}}^k \norm{\mat{x}^{(0)} - \mat{x}^*}_{\mat{A}^\top \mat{A}}\,.
% \]
% \fi

We show that Algorithm~\ref{alg:approximate-lifting} gives the desired output.
Suppose \ApproximateSolve yields $\mat{x}^{(k+1)}$ for given inputs $\mat{A},\widetilde{\mat{P}},\widetilde{\mat{q}}$, and $\widetilde{\mat{q}}^{(k)}$ (i.e., $\widehat{\mat{x}} \gets \mat{x}^{(k)}$, $\mat{f}\gets \widetilde{\mat{q}}^{(k)}$, and $\overline{\mat{x}} \gets \mat{x}^{(k+1)}$), which satisfies
\[
    \norm{\mat{A}\mat{x}^{(k+1)}-\widetilde{\mat{q}}^{(k)}}_2^2\leq (1+\widehat{\epsilon})\,\min_{\mat{x}} \norm{\mat{A}\mat{x}-\widetilde{\mat{q}}^{(k)}}_2^2
    = (1+\widehat{\varepsilon})\,\norm{\pi_{\mat A ^\perp}\widetilde{\mat{q}}^{(k)}}_2^2\,.
    %= (1+\widehat{\varepsilon})\,\bigl\lVert\bigl(\mat{I} - \mat{A}\,(\mat{A}^\top \mat{A})^{-1} \mat{A}^\top\bigr)\, \mat{d}^{(k)}\bigr\rVert_2^2\,,
\]
%where the equality follows from $(\mat{A}^\top\mat{A})^{-1}\mat{A}^\top \mat{d}^{(k)} = \argmin\,\norm{\mat{Ax}-\mat{d}^{(k)}}_2$. 
%Since $P_{\mat{A}} := \mat{A}\,(\mat{A}^\top \mat{A})^{-1} \mat{A}^\top$ and $P_{\mat{A}^\perp} := \mat{I} - P_{\mat{A}}$ are orthogonal to each other,
We can also decompose the LHS using $\widetilde{\mat{q}}^{(k)} = \pi_{\mat A} \widetilde{\mat{q}}^{(k)} + \pi_{\mat A ^\perp} \widetilde{\mat{q}}^{(k)}$ as follows:
\[
    \norm{\mat{A}\mat{x}^{(k+1)}-\widetilde{\mat{q}}^{(k)}}_2^2
    = 
    \norm{\mat{A}\mat{x}^{(k+1)} - \pi_{\mat{A}}\widetilde{\mat{q}}^{(k)}}_2^2 + \norm{\pi_{\mat{A}^{\perp}} \widetilde{\mat{q}}^{(k)}}_2^2\,.
\]
Combining the above, we get
\[
    \norm{\mat{A}\mat{x}^{(k+1)} - \pi_{\mat{A}}\widetilde{\mat{q}}^{(k)}}_2^2
    \leq
    \widehat{\epsilon}\, \norm{\pi_{\mat{A}^{\perp}}\widetilde{\mat{q}}^{(k)}}_2^2\,.
\]
Denoting $\mat{x}^* = (\widetilde{\mat{P}}^\top \widetilde{\mat{P}})^{-1} \widetilde{\mat{P}}^\top \widetilde{\mat{q}} = \argmin_{\mat x}\,\norm{\mat{Bx}-\widetilde{\mat{q}}}_2$ and using the triangle inequality,
\begin{align}
\nonumber
    \norm{\mat{A}\mat{x}^{(k+1)} - \mat{A}\mat{x}^*}_2
    &
    \leq \norm{\mat{A}\mat{x}^{(k+1)} - \pi_{\mat A}\widetilde{\mat{q}}^{(k)}}_2 + \norm{\pi_{\mat A}\widetilde{\mat{q}}^{(k)} - \mat{A} \mat{x}^*}_2
    \\ & \leq \label{eq:total-bound-on-a-norm}
    \sqrt{\widehat{\epsilon}}\, \norm{\pi_{\mat{A}^{\perp}}\widetilde{\mat{q}}^{(k)}}_2 + \norm{\pi_{\mat A}\widetilde{\mat{q}}^{(k)} - \mat{A} \mat{x}^*}_2\,.
\end{align}
% \begin{align}
% \nonumber
%     \norm{\mat{A}\mat{x}^{(k+1)} - \mat{A}\mat{x}^*}_2
%     &
%     \leq \norm{\mat{A}\mat{x}^{(k+1)}-\mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top\mat{d}^{(k)}}_2 + \norm{\mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top\mat{d}^{(k)} - \mat{A} \mat{x}^*}_2
%     \\ & \leq \label{eq:total-bound-on-a-norm}
%     \sqrt{\widehat{\epsilon}} \cdot \norm{(\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top) \mat{d}^{(k)}}_2 + \norm{\mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top\mat{d}^{(k)} - \mat{A} \mat{x}^*}_2\,.
% \end{align}

We now bound each term in the RHS. As for the second term, since $\widetilde{\mat{q}}^{(k)} = \widetilde{\mat{q}} + (\mat{A} - \widetilde{\mat{P}})\, \mat{x}^{(k)}$ and $(\widetilde{\mat{P}}-\mat{A})^\top \begin{bmatrix}
\widetilde{\mat{P}} & \widetilde{\mat{q}} \end{bmatrix} = 0$, by \Cref{lemma:richardson-simulation},
\begin{align*}
    (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top\widetilde{\mat{q}}^{(k)}
    &=
    \argmin_{\mat x}\,\norm{\mat{Ax}-\widetilde{\mat{q}}^{(k)}}_2 \\
    &=
    \mat{x}^{(k)} - (\mat{A}^\top \mat{A})^{-1} (\widetilde{\mat{P}}^\top\widetilde{\mat{P}}\mat{x}^{(k)} - \widetilde{\mat{P}}^\top\widetilde{\mat{q}})\,,
\end{align*}
which is exactly a Richardson iteration with preconditioner $\mat M\gets \mat{A}^\top \mat{A}$ in \Cref{lem:richardson_iteration} (satisfying $\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \preceq \mat{A}^\top \mat{A}\preceq \beta\,\widetilde{\mat{P}}^\top \widetilde{\mat{P}}$). 
Thus, $\norm{(\mat{A}^\top \mat{A})^{-1} \mat{A}^\top\widetilde{\mat{q}}^{(k)} - \mat{x}^*}_{\mat{A}^\top \mat{A}} \leq (1-\beta^{-1})\,\norm{\mat{x}^{(k)} - \mat{x}^*}_{\mat{A}^\top \mat{A}}$, and
% Multiplying $\mat{A}$ both sides and noting $\mat{B}^\top\mat{B}\mat{x}^* = \mat{B}^\top \mat{d}$,
% \begin{align*}
% P_{\mat A}\mat{d}^{(k)} 
% & = 
% \mat{A} \mat{x}^{(k)} -  \mat{A}\, (\mat{A}^\top \mat{A})^{-1} (\mat{B}^\top\mat{B}\mat{x}^{(k)} - \mat{B}^\top\mat{d})
% \\ & = 
% \mat{A} \mat{x}^{(k)} - \mat{A}\, (\mat{A}^\top \mat{A})^{-1} \mat{B}^\top\mat{B}\, (\mat{x}^{(k)} -\mat{x}^*)\,.
% \end{align*}
% %\kook{The plus sign in the second light should be minus?} \mehrdad{just fixed it.}
% Hence,
% \[
%     P_{\mat A}\mat{d}^{(k)} - \mat{A} \mat{x}^*
%     =
%     \mat{A}\, \bigl(\mat{I} - (\mat{A}^\top \mat{A})^{-1} \mat{B}^\top\mat{B}\bigr) (\mat{x}^{(k)} - \mat{x}^*)\,.
% \]
% Thus by \Cref{lem:richardson_iteration},
\begin{equation}
    \label{eq:second-term-bound-on-a-norm}
    \norm{\pi_{\mat A}\widetilde{\mat{q}}^{(k)} - \mat{A} \mat{x}^*}_2
    \leq
    \parens*{1-\frac 1 \beta}\, \norm{\mat{A} \mat{x}^{(k)} - \mat{A}\mat{x}^*}_2\,.
\end{equation}

Regarding the first term in \eqref{eq:total-bound-on-a-norm}, since $\mat{Ax}^{(k)}$ is in the column space of $\mat{A}$, %$\mat{A}$ and $\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top$ are orthogonal,
\[
\pi_{\mat A^\perp}\widetilde{\mat{q}}^{(k)} 
= 
\pi_{\mat A^\perp}\bigl(\widetilde{\mat{q}} +(\mat{A-B})\,\mat x^{(k)}\bigr)
=
\pi_{\mat A^\perp}(\widetilde{\mat{q}} - \widetilde{\mat{P}} \mat{x}^{(k)})\,.
\]
Therefore, %since $\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top$ is an orthogonal projection matrix,
\begin{align*}
\norm{\pi_{\mat A^\perp}\widetilde{\mat{q}}^{(k)}}_2^2
& \leq 
\norm{\widetilde{\mat{q}} - \widetilde{\mat{P}} \mat{x}^{(k)}}_2^2
\\ & =
\norm{\widetilde{\mat{P}} \mat{x}^{*} - \widetilde{\mat{P}} \mat{x}^{(k)}}_2^2 + \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2
\\ & \leq 
\norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(k)}}_2^2 + \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2\,,
\end{align*}
where the last inequality follows from $\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \preceq \mat{A}^\top \mat{A}$.
Thus,
\begin{equation}
\label{eq:first-term-bound-on-a-norm}
\norm{\pi_{\mat A^\perp}\widetilde{\mat{q}}^{(k)}}_2
\leq 
\norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(k)}}_2 + \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2\,.
\end{equation}

Combining \eqref{eq:total-bound-on-a-norm}, \eqref{eq:second-term-bound-on-a-norm}, and \eqref{eq:first-term-bound-on-a-norm}, we have
\[
    \norm{\mat{A}\mat{x}^{(k+1)} - \mat{A}\mat{x}^*}_2
    \leq
    \parens*{1-\frac{1}{\beta} + \sqrt{\widehat{\epsilon}}}\, \norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(k)}}_2 + \sqrt{\widehat{\epsilon}}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2\,.
\]
Denoting $\alpha=1-\frac{1}{\beta} + \sqrt{\widehat{\epsilon}}\,$, by induction, we have
\begin{align}
\nonumber
\norm{\mat{A}\mat{x}^{(k)} - \mat{A}\mat{x}^*}_2 
& \leq 
\alpha^k\, \norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(0)}}_2 + (1+\alpha+\alpha^2 + \cdots + \alpha^{k-1}) \times \sqrt{\widehat{\epsilon}}\,\min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2
\\ & =
\label{eq:col-space-bound-for-a}
\alpha^k\, \norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(0)}}_2 + \frac{1-\alpha^k}{1-\alpha}\times \sqrt{\widehat{\epsilon}}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2\,.
\end{align}

We also have
\begin{align}
\nonumber
\norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{q}}}_2^2 
&= \norm{\widetilde{\mat{P}} \mat{x}^{(k)} - \pi_{\widetilde{\mat{P}}}\widetilde{\mat{q}}}_2^2 + \norm{\pi_{\widetilde{\mat{P}}^\perp}\widetilde{\mat{q}}}_2^2
\\ & = 
\label{eq:total-error}
\norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{P}} \mat{x}^*}_2^2 + \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x} - \widetilde{\mat{q}}}_2^2\,.
\end{align}
We then bound the first term by using $\widetilde{\mat{P}}^\top \widetilde{\mat{P}} \preceq \mat{A}^\top \mat{A} \preceq \beta\, \widetilde{\mat{P}}^\top \widetilde{\mat{P}}$ and \eqref{eq:col-space-bound-for-a} as follows:
\begin{align}
    \nonumber
    \norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{P}} \mat{x}^*}_2^2 
    &\leq
    \norm{\mat{A} \mat{x}^{(k)}-\mat{A} \mat{x}^*}_2^2 \\
    &\leq
    \nonumber
    2\alpha^{2k}\, \norm{\mat{A} \mat{x}^{*} - \mat{A} \mat{x}^{(0)}}_2^2 + 2\, \parens*{\frac{1-\alpha^k}{1-\alpha}}^2 \times \widehat{\epsilon}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2 \\
    &\leq 
    \nonumber%\label{eq:col-space-bound}
    2 \beta\alpha^{2k}\, \norm{\widetilde{\mat{P}} \mat{x}^{*} - \widetilde{\mat{P}} \mat{x}^{(0)}}_2^2 + 2\, \parens*{\frac{1-\alpha^k}{1-\alpha}}^2 \times \widehat{\epsilon}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2\,.
\end{align}
Putting this bound back into \eqref{eq:total-error},% and \eqref{eq:col-space-bound},
\[
\norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{q}}}_2^2 \leq 2 \beta\alpha^{2k}\, \norm{\widetilde{\mat{P}} \mat{x}^{*} - \widetilde{\mat{P}} \mat{x}^{(0)}}_2^2 + \parens*{1 + 2\widehat{\epsilon}\, \parens*{\frac{1-\alpha^k}{1-\alpha}}^2}\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2\,.
\]

Setting
\[
k = \ceil*{\frac{\log(\sfrac{2\beta}{\epsilon})}{2\,(\nicefrac 1 \beta -\sqrt{\widehat{\epsilon}})}}\,,
\]
we have
\[
    \norm{\widetilde{\mat{P}} \mat{x}^{(k)}-\widetilde{\mat{q}}}_2^2
    \leq
    \epsilon\, \norm{\widetilde{\mat{P}} \mat{x}^{*} - \widetilde{\mat{P}} \mat{x}^{(0)}}_2^2 + \parens*{1+ \frac{2 \widehat{\epsilon}}{(\nicefrac 1  \beta - \sqrt{\widehat{\epsilon}})^2} }\, \min_{\mat{x}}\, \norm{\widetilde{\mat{P}} \mat{x}-\widetilde{\mat{q}}}_2^2\,,
\]
which completes the proof with $\mat{x}^{(0)}=\boldsymbol{0}$.
% \iffalse
% Further, with $\widetilde{\mat{d}}^{(k)}=\mat{A} \mat{x}^{(k)}$,
% \begin{align*}
% (\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top) \mat{d}^{(k)} 
% & = 
% (\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top) (\mat{d}^{(k)} - \widetilde{\mat{d}}^{(k)} + \widetilde{\mat{d}}^{(k)})
% \\ & =
% (\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top) (\mat{d}^{(k)} - \widetilde{\mat{d}}^{(k)} ) + (\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top) \mat{A} \mat{x}^{(k)}
% \\ & = 
% (\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top) (\mat{d}^{(k)} - \widetilde{\mat{d}}^{(k)} )
% \\ & = 
%  (\mat{d}^{(k)} - \widetilde{\mat{d}}^{(k)}) - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top (\mat{d} + (\mat{A} - \mat{B})\mat{x}^{(k)} - \mat{A}\mat{x}^{(k)} ).
% \end{align*}
% Therefore since $\mat{B}-\mat{A}$ and $\begin{bmatrix}
% \mat{B} & \mat{d}
% \end{bmatrix}$ are orthogonal,
% \begin{align*}
% (\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top) \mat{d}^{(k)} 
% & =
% (\mat{d}^{(k)} - \widetilde{\mat{d}}^{(k)}) - \mat{A} (\mat{A}^\top \mat{A})^{-1} (\mat{B}^\top \mat{d} - \mat{B}^\top \mat{B}\mat{x}^{(k)} )
% \\ & = 
% (\mat{d}^{(k)} - \widetilde{\mat{d}}^{(k)}) - \mat{A} (\mat{A}^\top \mat{A})^{-1} (\mat{B}^\top \mat{B} \mat{x}^* - \mat{B}^\top \mat{B}\mat{x}^{(k)} )
% \end{align*}
% Now since $(\mat{A} - \mat{B})^\top(\mat{d}^{(k)} - \widetilde{\mat{d}}^{(k)}) = \mat{0}$, by the triangle inequality:
% \begin{align*}
% \norm{(\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top) \mat{d}^{(k)}}_2 
% & \leq 
% \norm{\mat{A}(\mat{I} - (\mat{A}^\top \mat{A})^{-1} \mat{B}^\top\mat{B}) (\mat{x}^* - \mat{x}^{(k)})}_2
% \\ & + 
% \norm{\mat{d}^{(k)} - \mat{A} \mat{x}^*}_2
% \\ & \leq 
% (1-1/\beta) \norm{\mat{A} \mat{x}^{(k)} - \mat{A}\mat{x}^*}_2 + 
% \norm{\mat{A} \mat{x}^* - \mat{d}^{(k)}}_2
% \end{align*}
% Therefore,
% \[
%     \norm{\mat{A}\mat{x}^{(k+1)} - \mat{A}\mat{x}^*}_2
%     \leq
%     \parens*{1- \frac{1}{\beta} + 2\sqrt{\widehat{\epsilon}} - \frac{\sqrt{\widehat{\epsilon}}}{\beta}} \norm{\mat{A}\mat{x}^{(k)} - \mat{A}\mat{x}^*}_2.
% \]
% The result then follows similar to the first part of the proof.
% \fi
\end{proof}

% \subsection{Discussion about $\beta$}\label{subsec:remark-on-beta}
% % \begin{remark}
% % \label{remark:not-knowing-beta}
% \todo{Fix this}

% Fix $\widehat{\epsilon}<1$ and any $c\in(0,1)$.
% If $\widetilde{\mat{P}}^\top \widetilde{\mat{P}}
%     \preceq
%     \mat{A}^\top \mat{A}
%     \preceq
%     \beta\, \widetilde{\mat{P}}^\top \widetilde{\mat{P}}$,
% then
% \[
%     c\widetilde{\mat{P}}^\top \widetilde{\mat{P}} + (1-c) \mat{A}^\top \mat{A} \preceq   \mat{A}^\top \mat{A} \preceq c\beta\widetilde{\mat{P}}^\top \widetilde{\mat{P}}+(1-c) \mat{A}^\top \mat{A}\,.
% \]
% We note that $\widetilde{\mat{P}}^\top \widetilde{\mat{P}}\preceq c\widetilde{\mat{P}}^\top \widetilde{\mat{P}} + (1-c)\mat A^\top \mat A$ follows from $\widetilde{\mat{P}}^\top \widetilde{\mat{P}}
%     \preceq
%     \mat{A}^\top \mat{A}$.
% \begin{align*}
%     \mat{A}^\top \mat{A}
%     &\preceq c\beta\widetilde{\mat{P}}^\top \widetilde{\mat{P}}+(1-c) \mat{A}^\top \mat{A}\\
%     &= c(\beta-1)\,\widetilde{\mat{P}}^\top \widetilde{\mat{P}}  +c\widetilde{\mat{P}}^\top \widetilde{\mat{P}}+(1-c) \mat{A}^\top \mat{A}\\
%     &\preceq (c(\beta-1)+1)(c\widetilde{\mat{P}}^\top \widetilde{\mat{P}} + (1-c) \mat{A}^\top \mat{A})\,.
% \end{align*}
% Since $c<1$, if $\beta>1$, we have $c(\beta-1)+1 < \beta$.
% In particular, for $c=\frac{1-3\sqrt{\widehat{\epsilon}}}{3\sqrt{\widehat{\epsilon}} (\beta - 1)}$ (which is less than one if $\widehat{\epsilon} > \frac{1}{9\beta^2}$),
% \[
% c\widetilde{\mat{P}}^\top \widetilde{\mat{P}} + (1-c) \mat{A}^\top \mat{A} \preceq   \mat{A}^\top \mat{A} \preceq \frac{1}{3\sqrt{\widehat{\epsilon}}}\cdot(c\widetilde{\mat{P}}^\top \widetilde{\mat{P}} + (1-c) \mat{A}^\top \mat{A})\,.
% \]
% %\kook{Hey Mehrdad, could you read your writing below and clarify this? I'm not sure if the below is correct or not. For instance, shouldn't we know $\beta$ anyway to get $\widehat{\epsilon}$-close approximate solution through leverage-score sampling anyway?}
% Now note that $\widehat{\epsilon} \in [0,9\widehat{\epsilon})$. Therefore, by \Cref{thm:approximate-richardson}, we can use approximate solutions for the regression on $\mat{A}$ to find an approximate solution for regression on $ \begin{bmatrix}
% c \widetilde{\mat{P}}^\top \mid (1-c) \mat{A}^\top \end{bmatrix}^\top$. This can already be viewed as a backward stability argument, but further note that
% \[
% \widetilde{\mat{P}}^\top \widetilde{\mat{P}} \preceq c\widetilde{\mat{P}}^\top \widetilde{\mat{P}} + (1-c) \mat{A}^\top \mat{A} \preceq (\beta(1-c) + c) \widetilde{\mat{P}}^\top \widetilde{\mat{P}}\,.
% \]
% Since $c<1$, if $\beta>1$, $\beta(1-c) + c < \beta$. The above discussion implies an algorithm of the following form: use approximate solutions for $\mat{A}$ to find approximate solutions for $\begin{bmatrix}
% c \widetilde{\mat{P}}^\top \mid (1-c) \mat{A}^\top \end{bmatrix}^\top$ and use such approximate solutions to find approximate solutions for $\widetilde{\mat{P}}^\top \widetilde{\mat{P}}$. This can be extended to multiple middle steps instead of one. Consequently we do not necessarily need to know the value of~$\beta$ and can use such a scheme with sufficient number of iterations to find approximate solutions.
% \end{remark}





























\iffalse
\todo{Delete old statements + proofs if above are correct.}

\begin{lemma}
\label{lemma:old-lifting}
Let $\mat{A}\in\R^{I\times R}$, $\mat{b} \in \R^I$ be fixed.
Let $\Omega\subseteq [I]$, and $\widehat{\mat{b}} \in \R^{I}$ be a vector such that for $i\in \Omega$, $\widehat{b}_{i}=b_{i}$ and the rest of the entries (i.e., $i\in\overline{\Omega} := [I] \setminus \Omega$) are variables. Suppose $\mat{A}_\Omega$ is full (column) rank. Let
    \begin{align}
    \label{eq:prox_problem}
        (\widehat{\mat{b}}^*, \mat{x}^*) = \argmin & ~~ \norm{\mat{A} \mat{x} - \widehat{\mat{b}}}_2^2
        \\ \nonumber \text{s.t.} & ~~
        \mat{x} \in\R^{R}
        \\ \nonumber & ~~ 
        \widehat{b}_{i} \in \R, \forall i \in \overline{\Omega}.
    \end{align}
Then,
\[
    \mat{x}^* = \argmin_{\mat{x}\in\R^R} \norm{\mat{A}_{\Omega} \mat{x} - \mat{b}_{\Omega}}_{2}^2 =  \argmin_{\mat{x}\in\R^R} \sum_{i\in \Omega} (\mat{A} \mat{x} - \mat{b})_i^2
\]
\end{lemma}

\begin{proof}
We show that for any choice of $\mat{x}$, there exists 1) $\widehat{\mat{b}}$ such that $\norm{\mat{A}_{\Omega} \mat{x} - \mat{b}_{\Omega}}_{2}^2 = \norm{\mat{A} \mat{x} - \widehat{\mat{b}}}_2^2$, and 2) there is no $\widehat{\mat{b}}$ such that $\norm{\mat{A}_{\Omega} \mat{x} - \mat{b}_{\Omega}}_{2}^2 > \norm{\mat{A} \mat{x} - \widehat{\mat{b}}}_2^2$. Before proving these, we first show that this implies the desired result.

Let $(\widehat{\mat{b}}^*, \mat{x}^*)$ be a solution to \eqref{eq:prox_problem}. By Claim (2), there is no $\widehat{\mat{b}}$ such that $\norm{\mat{A}_{\Omega} \mat{x}^* - \mat{b}_{\Omega}}_{2}^2 > \norm{\mat{A} \mat{x}^* - \widehat{\mat{b}}}_2^2$. However, the choice of $\widehat{b}_i = \mat{a}_i^\top \mat{x}^*$, for $i\in\overline{\Omega}$ gives $\norm{\mat{A}_{\Omega} \mat{x}^* - \mat{b}_{\Omega}}_{2}^2 = \norm{\mat{A} \mat{x}^* - \widehat{\mat{b}}}_2^2$ which is the minimum error for $\norm{\mat{A} \mat{x}^* - \widehat{\mat{b}}}_2^2$. It is easy to see that this is the only optimal choice of $\widehat{\mat{b}}^*$ given $\mat{x}^*$.

Now suppose $\mat{x}^*$ is not the solution to $\argmin_{\mat{x}\in\R^R} \norm{\mat{A}_{\Omega} \mat{x} - \mat{b}_{\Omega}}_{2}^2$. Then there exists $\widehat{\mat{x}}$ such that, $\norm{\mat{A}_{\Omega} \mat{x}^* - \mat{b}_{\Omega}}_{2}^2 > \norm{\mat{A}_{\Omega} \widehat{\mat{x}} - \mat{b}_{\Omega}}_{2}^2$. Therefore by Claim (2) there exists $\widehat{\mat{b}}$ such that $\norm{\mat{A}_{\Omega} \widehat{\mat{x}} - \mat{b}_{\Omega}}_{2}^2 = \norm{\mat{A} \widehat{\mat{x}} - \widehat{\mat{b}}}_2^2$. This is in contradiction with the optimality of $(\widehat{\mat{b}}^*, \mat{x}^*)$ because
\begin{align}
\norm{\mat{A} \widehat{\mat{x}} - \widehat{\mat{b}}}_2^2 = \norm{\mat{A}_{\Omega} \widehat{\mat{x}} - \mat{b}_{\Omega}}_{2}^2 < \norm{\mat{A}_{\Omega} \mat{x}^* - \mat{b}_{\Omega}}_{2}^2 = \norm{\mat{A} \mat{x}^* - \widehat{\mat{b}}^*}_{2}^2.
\end{align}
To prove Claim (1), note that the choice of $\widehat{b}_i = \mat{a}_i^\top \mat{x}$, for $i\in\overline{\Omega}$ makes $(\mat{A} \mat{x} - \widehat{\mat{b}})_i=0$. For Claim (2), note that for $i\in\Omega$, $\mat{a}_i^\top \mat{x} - \mat{b}_i = \mat{a}_i^\top \mat{x} - \widehat{\mat{b}}_i$, and for $i\in\overline{\Omega}$, $(\mat{a}_i^\top \mat{x} - \widehat{\mat{b}}_i)^2 \geq 0$.
\end{proof}

\begin{lemma}
Problem \eqref{eq:prox_problem} is convex.
\end{lemma}

% \begin{proof}
% We show that the Hessian is a PSD matrix. Let $\widetilde{\mat{b}}\in\R^{|\overline{\Omega}|}$ be a vector containing only the entries of $\widehat{\mat{b}}$ that are free variables.
% Then we define 
% \begin{align*}
% f(\mat{x},\widetilde{\mat{b}}) & := \frac{1}{2}\norm{\mat{A} \mat{x} - \widehat{\mat{b}}}_2^2 = \frac{1}{2} (\mat{A} \mat{x} - \widehat{\mat{b}})^\top (\mat{A} \mat{x} - \widehat{\mat{b}})
% \\ &
% = \frac{1}{2} \mat{x}^\top \mat{A}^\top \mat{A} \mat{x} - \widehat{\mat{b}}^\top \mat{A} \mat{x} + \frac{1}{2} \widehat{\mat{b}}^\top \widehat{\mat{b}}
% \end{align*}
% We show that $\nabla^2 f(\mat{x},\widetilde{\mat{b}}) \succeq 0$.
% Observe that
% \[
% \nabla^2 f(\mat{x},\widetilde{\mat{b}}) =
% \begin{bmatrix}
% \mat{A}^\top \mat{A} & -\mat{A}_{\overline{\Omega}}^\top \\
% -\mat{A}_{\overline{\Omega}} & \mat{I}
% \end{bmatrix}
% \]
% By a block decomposition, we have
% \[
% \nabla^2 f(\mat{x},\widetilde{\mat{b}}) = \begin{bmatrix}
% \mat{I} & \mat{0} \\ -\mat{A}_{\overline{\Omega}} (\mat{A}^\top \mat{A})^{-1} & \mat{I}
% \end{bmatrix}
% \begin{bmatrix}
%     \mat{A}^\top \mat{A} & \mat{0} \\ \mat{0} & \mat{I} - \mat{A}_{\overline{\Omega}} (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top
% \end{bmatrix}
% \begin{bmatrix}
%     \mat{I} & -(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \\
%     \mat{0} & \mat{I}
% \end{bmatrix}
% \]
% Now we only need to show that $\mat{A}^\top \mat{A}$ and $\mat{I} - \mat{A}_{\overline{\Omega}} (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top$ are PSD. The former is obviously correct. For the latter, note that $\mat{I} - \mat{A}_{\overline{\Omega}} (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top$ is a principle submatrix of $\mat{I} - \mat{A} (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top$ and a principle submatrix of any PSD matrix is PSD.
% \end{proof}

If we have a regularization of the form $\frac{1}{2}\norm{\mat{A} \mat{x} - \widehat{\mat{b}}}_2^2 + \lambda\norm{\mat{x}}_2^2$,
then the Hessian is
\[
\nabla^2 f(\mat{x},\widetilde{\mat{b}}) =
\begin{bmatrix}
\mat{A}^\top \mat{A} + \lambda \mat{I} & -\mat{A}_{\overline{\Omega}}^\top \\
-\mat{A}_{\overline{\Omega}} & \mat{I}
\end{bmatrix}
\]
which makes the function strongly convex.
\fi

\iffalse
Now we update $\tensor{G}$ by alternating between updating $\mat{x}$ and $\widehat{\mat{b}}$. This can be viewed as a block Newton algorithm. \todo{Analyze the convergence.} Look at \cite{warga1963minimizing} and \cite{tseng2001convergence} for convergence. 
\todo{Write the regularized version.}


\subsection{Better convergence}
Let
\[
\mat{x}^* = (\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}_{\Omega}^\top \mat{b}_{\Omega}.
\]
We have
\[
\mat{A}_{\Omega}^\top \mat{b}_{\Omega} = \mat{A}_{\Omega}^\top \mat{A}_{\Omega} \mat{x}^* .
\]
Therefore,
\begin{align*}
\mat{x}^{(k+1)} - \mat{x}^{(k)} & = -(\mat{A}^\top \mat{A})^{-1} \mat{A}^\top (\widetilde{\mat{b}}^{(k)} - \mat{b}^{(k)})
\\ & =
-(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top (\widetilde{\mat{b}}^{(k)}_{\Omega} - \mat{b}^{(k)}_{\Omega})
\\ & =
-(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} (\mat{x}^{(k)} - \mat{x}^* )
\\ & =
-(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} (\mat{x}^{(k-1)} - (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top (\widetilde{\mat{b}}_{\Omega}^{(k-1)} - \mat{b}_{\Omega}^{(k-1)}) - \mat{x}^* )
\\ & =
-(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} (\mat{x}^{(k-1)} - (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} (\mat{x}^{(k-1)}-\mat{x}^* ) - \mat{x}^* )
\\ & = 
-(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} (\mat{I} - (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega}) (\mat{x}^{(k-1)}-\mat{x}^* )
\\ & = 
-(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}} (\mat{x}^{(k-1)}-\mat{x}^* )
\end{align*}
By induction we have
\begin{align*}
\mat{x}^{(k+1)} - \mat{x}^{(k)} 
& = 
-(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} ((\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}})^{k} (\mat{x}^{(0)}-\mat{x}^* )
\\ & =
-(\mat{A}^\top \mat{A})^{-1} (\mat{A}^\top \mat{A} - \mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}} )((\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}})^{k} (\mat{x}^{(0)}-\mat{x}^* )
\\ & = 
\left( ((\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}})^{k} - ((\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}})^{k+1} \right)(\mat{x}^* - \mat{x}^{(0)})
\\ & = 
(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \left( (\mat{A}_{\overline{\Omega}} (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top)^{k-1} - (\mat{A}_{\overline{\Omega}} (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top)^{k} \right) \mat{A}_{\overline{\Omega}} (\mat{x}^* - \mat{x}^{(0)})
\end{align*}
Therefore
\begin{align*}
\norm{\mat{x}^{(k+1)} - \mat{x}^{(k)} }_{\mat{A}^\top \mat{A}}^2 = 
\end{align*}
Suppose $\mat{U} \mat{\Sigma} \mat{V}^\top$ is the SVD of $\mat{A}$. Then $\mat{A}_{\overline{\Omega}} = \mat{U}_{\overline{\Omega}} \mat{\Sigma} \mat{V}^\top$. Therefore
\[
(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}} = \mat{V} \mat{\Sigma}^{-2} \mat{V}^\top \mat{V} \mat{\Sigma} \mat{U}_{\overline{\Omega}}^\top \mat{U}_{\overline{\Omega}} \mat{\Sigma} \mat{V}^\top = \mat{V} \mat{\Sigma}^{-1} \mat{U}_{\overline{\Omega}}^\top \mat{U}_{\overline{\Omega}} \mat{\Sigma} \mat{V}^\top
\]
By induction we have
\begin{align*}
\mat{x}^{(k+1)} - \mat{x}^{(k)} 
& = 
-(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} ((\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}})^{k} (\mat{x}^{(0)}-\mat{x}^* )
\\ & =
-(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top ( \mat{A}_{\overline{\Omega}} (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top)^{k-1} \mat{A}_{\overline{\Omega}} (\mat{x}^{(0)}-\mat{x}^* )
\end{align*}
Moreover
\[
\mat{x}^{(k-1)}-\mat{x}^* = -(\mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}})^{-1} \mat{A}^\top \mat{A} (\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}^\top \mat{A} (\mat{x}^{(k+1)} - \mat{x}^{(k)})
\]
Therefore
\[
\mat{x}^{(k+1)} - \mat{x}^{(k)} = (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} ((\mat{A}^\top \mat{A})^{-1} \mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}})^{k-1} (\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}^\top \mat{A} (\mat{x}^{(2)}-\mat{x}^{(1)} )
\]

\iffalse
We analyze the convergence in the following.
We have
\begin{align*}
\mat{x}^{(k+1)} - \mat{x}^* & = (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top (\mat{b}^{(k)} - \widetilde{\mat{b}}^{(k)}) + \mat{x}^{(k)} - \mat{x}^*
\\ & =
(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top (\mat{b}^{(k)}_{\Omega} - \widetilde{\mat{b}}^{(k)}_{\Omega}) + \mat{x}^{(k)} - \mat{x}^*
\\ & =
(\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}_{\Omega}^\top (\mat{b}^{(k)}_{\Omega} - \widetilde{\mat{b}}^{(k)}_{\Omega}) + \mat{x}^{(k)} - \mat{x}^*
\\ & - 
(\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1} 
\mat{A}_{\overline{\Omega}}^\top (\mat{I} + \mat{A}_{\overline{\Omega}}  (\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1}\mat{A}_{\overline{\Omega}}^\top)^{-1}
\mat{A}_{\overline{\Omega}}
(\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1} 
\mat{A}_{\Omega}^\top (\mat{b}^{(k)}_{\Omega} - \widetilde{\mat{b}}^{(k)}_{\Omega})
\end{align*}
Now note that $(\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}_{\Omega}^\top \mat{b}^{(k)}_{\Omega} = \mat{x}^*$, and therefore,
\begin{align*}
(\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}_{\Omega}^\top (\mat{b}^{(k)}_{\Omega} - \widetilde{\mat{b}}^{(k)}_{\Omega}) + \mat{x}^{(k)} - \mat{x}^* & = 
\mat{x}^* - (\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega} \mat{x}^{(k)} + \mat{x}^{(k)} - \mat{x}^* = 0.
\end{align*}
Thus
\begin{align*}
\mat{x}^{(k+1)} - \mat{x}^* & = - 
(\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1} 
\mat{A}_{\overline{\Omega}}^\top (\mat{I} + \mat{A}_{\overline{\Omega}}  (\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1}\mat{A}_{\overline{\Omega}}^\top)^{-1}
\mat{A}_{\overline{\Omega}}
(\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1} 
\mat{A}_{\Omega}^\top (\mat{b}^{(k)}_{\Omega} - \widetilde{\mat{b}}^{(k)}_{\Omega})
\\ & = 
(\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1} 
\mat{A}_{\overline{\Omega}}^\top (\mat{I} + \mat{A}_{\overline{\Omega}}  (\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1}\mat{A}_{\overline{\Omega}}^\top)^{-1}
\mat{A}_{\overline{\Omega}} (\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1} 
\mat{A}_{\Omega}^\top (\mat{A}_{\Omega} \mat{x}^{(k)} - \mat{b}^{(k)}_{\Omega})
\\ & =
(\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1} 
\mat{A}_{\overline{\Omega}}^\top (\mat{I} + \mat{A}_{\overline{\Omega}}  (\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1}\mat{A}_{\overline{\Omega}}^\top)^{-1}
\mat{A}_{\overline{\Omega}} (\mat{x}^{(k)} - \mat{x}^*)
\end{align*}
Therefore
\begin{align*}
& \norm{\mat{x}^{(k+1)} - \mat{x}^*}_{\mat{A}_{\Omega}^\top \mat{A}_{\Omega}}^2 \\ & = (\mat{x}^{(k)} - \mat{x}^*)^\top \mat{A}_{\overline{\Omega}}^\top (\mat{I} + \mat{A}_{\overline{\Omega}}  (\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1}\mat{A}_{\overline{\Omega}}^\top)^{-1} \mat{A}_{\overline{\Omega}} (\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1} 
\mat{A}_{\overline{\Omega}}^\top (\mat{I} + \mat{A}_{\overline{\Omega}}  (\mat{A}_{\Omega}^\top \mat{A}_{\Omega} )^{-1}\mat{A}_{\overline{\Omega}}^\top)^{-1}
\mat{A}_{\overline{\Omega}} (\mat{x}^{(k)} - \mat{x}^*)
\\ & \leq
\frac{1}{4} \norm{\mat{x}^{(k)} - \mat{x}^*}_{\mat{A}_{\overline{\Omega}}^\top \mat{A}_{\overline{\Omega}}}^2
\end{align*}
Now we have
\begin{align*}
\mat{x}^{(k+1)} - \mat{x}^* & = (\mat{A}^\top \mat{A})^{-1} \mat{A}^\top (\mat{b}^{(k)} - \widetilde{\mat{b}}^{(k)}) + \mat{x}^{(k)} - \mat{x}^*
\\ & =
(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top (\mat{b}^{(k)}_{\Omega} - \widetilde{\mat{b}}^{(k)}_{\Omega}) + \mat{x}^{(k)} - \mat{x}^*
\\ & = 
(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top (\mat{b}^{(k)}_{\Omega} - \mat{A}_{\Omega}\mat{x}^{(k)}) + \mat{x}^{(k)} - \mat{x}^*
\\ & = 
(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top (\mat{A}_{\Omega} (\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}_{\Omega}^\top\mat{b}^{(k)}_{\Omega} - \mat{A}_{\Omega}\mat{x}^{(k)}) + \mat{x}^{(k)} - \mat{x}^*
\\ & +
(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top (\mat{I} - \mat{A}_{\Omega} (\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}_{\Omega}^\top) \mat{b}^{(k)}_{\Omega}
\\ & = 
(\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top (\mat{A}_{\Omega} \mat{x}^* - \mat{A}_{\Omega}\mat{x}^{(k)}) + \mat{x}^{(k)} - \mat{x}^*
\\ & +
(\mat{A}^\top \mat{A})^{-1} (\mat{A}_{\Omega}^\top - \mat{A}_{\Omega}^\top\mat{A}_{\Omega} (\mat{A}_{\Omega}^\top \mat{A}_{\Omega})^{-1} \mat{A}_{\Omega}^\top) \mat{b}^{(k)}_{\Omega}
\\ & = 
(\mat{I} - (\mat{A}^\top \mat{A})^{-1} \mat{A}_{\Omega}^\top \mat{A}_{\Omega}) (\mat{x}^{(k)} - \mat{x}^*)
\end{align*}
\fi
\fi

% \section{Cases with Fast Convergence}

% \paragraph{Low coherence with uniformly selected $\Omega$.} In practice, data often has low coherence \cite{mohri2011can}. Moreover for low-coherence data, uniform sampling of rows gives a spectral approximation \cite{cohen2015uniform}.

% \paragraph{Ridge-Regression.} Consider the ridge-regression problem:
% \[
% \argmin_{\mat{x}\in\R^R} \norm{\mat{A}_{\Omega} \mat{x} - \mat{b}_{\Omega}}_{2}^2 + \lambda \norm{\mat{x}}_2^2.
% \]
% In this case, for the second step of each iteration, we need to solve the following problem:
% \[
% \mat{x}^{(k+1)} = \min_{\mat{x}} \norm{\mat{A} \mat{x} - \mat{b}^{(k)}}_2^2 + \lambda \norm{\mat{x}}_2^2.
% \]
% Therefore the convergence rate is $(1-1/\beta)$ for $\beta> 0$ such that 
% \[
% \mat{A}_{\Omega}^\top \mat{A}_{\Omega} + \lambda \mat{I} \preceq \mat{A}^\top \mat{A} + \lambda \mat{I} \preceq \beta \cdot (\mat{A}_{\Omega}^\top \mat{A}_{\Omega} + \lambda \mat{I})
% \]
% Note that this improves the convergence rate.














% \section{Vector Completion for Kronecker Product}

% \subsection{Pure Kronecker Product}

% For $n\in[N]$, let $\mat{A}^{(n)}\in \R^{I_n \times R_n}$ and $I=\prod_{n\in[N]} I_n$ and $R=\prod_{n\in[N]} R_n$.
% Consider solving a regression problem of the following form
% \begin{align}
%     \min_{\mat{g}} \norm{(\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})_{\Omega} \mat{g} - \mat{t}_{\Omega}}_2^2.
% \end{align}

% We consider the running time of the naive algorithm and the algorithm based on lifting. \mehrdad{for lifting I'm just bounding the cost for each iteration for now.}

% \textbf{Running time of the naive algorithm.} Since $(\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})_{\Omega}$ does not have a particular structure, the naive algorithm would compute the following.
% \[
% ((\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})_{\Omega}^\top (\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})_{\Omega})^{-1} (\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})_{\Omega}^\top \mat{t}_{\Omega}
% \]
% The running time of this computation is $O(\textnormal{MM}(R,|\Omega|,R)+R^{\omega})$. One can use leverage score sampling to improve this to $\widetilde{O}(R |\Omega| + \frac{ R^{\omega}}{\epsilon^2})$.

% \textbf{Running time of the lifting approach.} We are given a $\mat{g}^{(0)}$ and we define $\overline{\mat{t}}$ as the following. For $i\in\Omega$, we set $\overline{\mat{t}}_i = \mat{t}_i$ and for $i\in\overline{\Omega}$, we set $\overline{\mat{t}}_i = (\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})_i \mat{g}^{(0)}$. We then need to solve 
% \begin{align}
% \label{eq:lifted-kronecker-regression}
%     \min_{\mat{g}} \norm{(\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)}) \mat{g} - \overline{\mat{t}}}_2^2
% \end{align}
% We have a few different approaches to solving this. Let $\widetilde{\mat{t}} = (\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)}) \mat{g}^{(0)}$.

% \begin{enumerate}
%     \item The first approach is to compute the following.
%     \[
%     \mat{g}^*=((\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})^\top (\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)}))^{-1} (\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})^{\top} \overline{\mat{t}}
%     \]
%     We have the following. \mehrdad{I think we can use the following calculation to argue about the convergence as well.}
%     \begin{align*}
%     \mat{g}^* & = \left(((\mat{A}^{(1)\top} \mat{A}^{(1)})^{-1} \mat{A}^{(1)\top}) \kron \cdots \kron ((\mat{A}^{(N)\top} \mat{A}^{(N)})^{-1} \mat{A}^{(N)\top})\right) \overline{\mat{t}}
%     \\ & =
%     \left(((\mat{A}^{(1)\top} \mat{A}^{(1)})^{-1} \mat{A}^{(1)\top}) \kron \cdots \kron ((\mat{A}^{(N)\top} \mat{A}^{(N)})^{-1} \mat{A}^{(N)\top})\right) (\overline{\mat{t}} - \widetilde{\mat{t}} + \widetilde{\mat{t}})
%     \\ & =
%     \left(((\mat{A}^{(1)\top} \mat{A}^{(1)})^{-1} \mat{A}^{(1)\top}) \kron \cdots \kron ((\mat{A}^{(N)\top} \mat{A}^{(N)})^{-1} \mat{A}^{(N)\top})\right) (\overline{\mat{t}} - \widetilde{\mat{t}})
%     \\ & +
%     \left(((\mat{A}^{(1)\top} \mat{A}^{(1)})^{-1} \mat{A}^{(1)\top}) \kron \cdots \kron ((\mat{A}^{(N)\top} \mat{A}^{(N)})^{-1} \mat{A}^{(N)\top})\right) (\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)}) \mat{g}^{(0)}
%     \\ & =
%     \left(((\mat{A}^{(1)\top} \mat{A}^{(1)})^{-1} \mat{A}^{(1)\top}) \kron \cdots \kron ((\mat{A}^{(N)\top} \mat{A}^{(N)})^{-1} \mat{A}^{(N)\top})\right) (\overline{\mat{t}} - \widetilde{\mat{t}})
%     \\ & +
%     \left(((\mat{A}^{(1)\top} \mat{A}^{(1)})^{-1} \mat{A}^{(1)\top} \mat{A}^{(1)}) \kron \cdots \kron ((\mat{A}^{(N)\top} \mat{A}^{(N)})^{-1} \mat{A}^{(N)\top} \mat{A}^{(N)})\right) \mat{g}^{(0)}
%     \\ & =
%     \left(((\mat{A}^{(1)\top} \mat{A}^{(1)})^{-1} \mat{A}^{(1)\top}) \kron \cdots \kron ((\mat{A}^{(N)\top} \mat{A}^{(N)})^{-1} \mat{A}^{(N)\top})\right) (\overline{\mat{t}} - \widetilde{\mat{t}}) + \mat{g}^{(0)}
%     \end{align*}
% \end{enumerate}
% The last step is assuming that for all $n\in N$, $\mat{A}^{(n)}$ is full rank, but even if not, the corresponding term can be computed quickly. Note that $(\overline{\mat{t}} - \widetilde{\mat{t}})$ has only $|\Omega|$ nonzeros. Therefore the running time of this computation is
% \[
% O(R |\Omega|+\sum_{n=1}^{N} \textnormal{MM}(R_n,I_n,R_n))
% \]
% Note that even compared to the naive leverage score sampling approach, we have improved the $R^{\omega}/\epsilon^2$ factor. \todo{Check whether the $R|\Omega|$ term can be improved using fast sparse Kronecker vector multiplication to $\textnormal{MM}(\sqrt{R}, |\Omega|,\sqrt{R})$. }

% Another approach is to solve \eqref{eq:lifted-kronecker-regression} using leverage score sampling. That gives a running time of 
% \[
% \widetilde{O}(\frac{R^{\omega}}{\epsilon^2} + \sum_{n=1}^{N} I_n R_n + R_n^{\omega}).
% \]
% Note that in this case, we have saved the factor of $R|\Omega|$.

% \subsection{Product of Kronecker Product and a Matrix}

% For $n\in[N]$, let $\mat{A}^{(n)}\in \R^{I_n \times R_n}$ and $I=\prod_{n\in[N]} I_n$ and $R=\prod_{n\in[N]} R_n$. Moreover, let $\mat{M} \in \R^{R_1\cdots R_N \times P}$.

% We are interested in solving the following regression problem
% \begin{align}
% \min_{\mat{g}} \norm{((\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})\mat{M})_{\Omega} \mat{g} - \mat{t}_{\Omega}}_2^2
% \end{align}

% \paragraph{Running time of the naive algorithm.} Since $(\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})_{\Omega}$ does not have a particular structure, the naive algorithm would compute the following.
% \[
% (((\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})\mat{M})_{\Omega}^\top ((\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})\mat{M})_{\Omega})^{-1} ((\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})\mat{M})_{\Omega}^\top \mat{t}_{\Omega}
% \]
% The naive running time of this computation is $O(RP|\Omega|+\textnormal{MM}(P,|\Omega|,P)+P^{\omega})$.

% \paragraph{Running time of the lifting approach.} We are given a $\mat{g}^{(0)}$ and we define $\overline{\mat{t}}$ as the following. For $i\in\Omega$, we set $\overline{\mat{t}}_i = \mat{t}_i$ and for $i\in\overline{\Omega}$, we set $\overline{\mat{t}}_i = (\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)})_i \mat{g}^{(0)}$. We then need to solve 
% \begin{align}
% \label{eq:lifted-kronecker-m-regression}
%     \min_{\mat{g}} \norm{(\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)}) \mat{M} \mat{g} - \overline{\mat{t}}}_2^2
% \end{align}
% We have a few different approaches to solving this. Let $\widetilde{\mat{t}} = (\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)}) \mat{g}^{(0)}$.

% \subsection{Old}

% In this section, we consider tensor completion using Tucker decomposition.

% \iffalse
% Given a tensor $\tensor{T}\in \R^{I_1 \times \cdots \times I_N}$, a subset $\Omega \in [I_1] \times \cdots \times [I_N]$, and a class of functions $f_{\theta}:\R\rightarrow\R$ parameterized with $\theta$, the tensor completion problem asks for
% \[
% \argmin_{\theta} \sum_{(i_1,\ldots,i_N)\in\Omega} f_{\theta}(t_{i_1,\ldots,i_N}),
% \]
% where $t_{i_1,\ldots,i_N}$ is the entry $(i_1,\ldots,i_N)$ of $\tensor{T}$.
% \fi

% Recall that the Tucker decomposition of a tensor $\tensor{T}$ is parameterized by $N$ factor matrices $\mat{A}^{(1)}\in \R^{I_1\times R_1},\ldots,\mat{A}^{(N)}\in \R^{I_N\times R_N}$, and a core tensor $\tensor{G} \in \R^{R_1\times \cdots \times R_N}$ that
% \begin{align*}
%     \min & ~~ \norm{\tensor{T} - \tensor{G} \times_1 \mat{A}^{(1)} \times_2 \cdots \times_N \mat{A}^{(N)}}_{\fro}^2,
%     \\ \text{s.t.} & ~~
%     \tensor{G} \in \R^{R_1\times \cdots \times R_N},
%     \\ & ~~
%     \mat{A}^{(n)}\in \R^{I_n\times R_n} , \forall n\in[N].
% \end{align*}
% We define the tensor completion with Tucker decomposition as follows:
% \begin{align*}
%     \min & ~~ \sum_{(i_1,\ldots,i_N)\in\Omega}(t_{i_1,\ldots,i_N} - \tensor{G} \times_1 \mat{a}^{(1)}_{i_1} \times_2 \cdots \times_N \mat{a}^{(N)}_{i_N})^2,
%     \\ \text{s.t.} & ~~
%     \tensor{G} \in \R^{R_1\times \cdots \times R_N},
%     \\ & ~~
%     \mat{A}^{(n)}\in \R^{I_n\times R_n} , \forall n\in[N],
% \end{align*}
% where $\mat{a}^{(n)}_{i}$ is row $i$ of $\mat{A}^{(n)}$. Note that for $\Omega = [I_1] \times \cdots \times [I_N]$, this is just Tucker decomposition. 

% Note there are $N+1$ components in the Tucker decomposition: $N$ factor matrices and one core tensor. The alternating least square algorithm in each iteration fixes all but one of these components and optimizes the unfixed component, and thus is a least-squares linear regression problem.

% \subsubsection{Updating the Core Tensor}

% To update the core tensor, we fix all factor matrices, and the core tensor is updated to the solution of the following regression problem:
% \begin{align*}
%     \argmin_{\tensor{T} \in\R^{R_1 \times \cdots \times R_N}}
%     \norm{(\mat{A}^{(1)} \kron \cdots \kron \mat{A}^{(N)}) \mat{t} - \mat{x}}_2^2,
% \end{align*}
% where $\kron$ is the Kronecker product, and $\mat{x}$ and $\mat{t}$ are the vectorizations of $\tensor{X}$ and $\tensor{T}$, respectively.
% We can consider the tensor completion version of this as the following:
% \begin{align*}
% \argmin_{\tensor{T} \in\R^{R_1 \times \cdots \times R_N}}
% \sum_{(i_1,\ldots,i_N)\in\Omega}((\mat{a}^{(1)}_{i_1} \kron \cdots \kron \mat{a}^{(N)}_{i_N}) \mat{t} - x_{i_1,\ldots,i_N})^2.
% \end{align*}

% \subsubsection{Updating the Factor Matrices}

% \subsubsection{Special case: Higher-order orthogonal iteration (HOOI)}

% \cite{de2000best} introduced HOOI.