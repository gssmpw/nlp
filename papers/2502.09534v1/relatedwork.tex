\section{Related Work}
\label{sec:related-work}

\paragraph{CP completion.}
\citet{tomasi2005parafac} proposed an ALS algorithm for CP completion that repeats the following two-step process:
(1) fill in the missing values using the current CP decomposition, and 
(2) update one factor matrix.
Their algorithm is equivalent to running \emph{one iteration} of mini-ALS in each step of ALS.
As \citet{tomasi2005parafac} discuss, this can lead to slower convergence and an increased likelihood of converging to suboptimal local minima because of errors introduced by the imputed missing values.

In contrast, approximate-mini-ALS runs \emph{until convergence} in each step of ALS.
By doing this, we establish a connection to the Richardson iteration
and build on its convergence guarantees.
Further, \citet{tomasi2005parafac} explicitly fill in \emph{all missing values} using the current decomposition, whereas we only impute missing values \emph{required by row sampling},
allowing us to achieve sublinear-time updates in the size of the tensor.
In general, iteratively fitting to imputed missing values falls under the umbrella of \emph{expectation-maximization} (EM) algorithms~\citep[Chapter 8]{little2019statistical}.

\paragraph{Statistical assumptions.}
Similar to minimizing the nuclear norm for matrix completion \citep{fazel2002matrix, candes2012exact}, a line of research in noisy TC suggests minimizing a convex relaxation of rank and identifies statistical assumptions under which the problem is recoverable \citep{barak2016noisy}.
Two standard assumptions are \emph{incoherence} and the \emph{missing-at-random} assumption. %i.e., observations are drawn uniformly at random.
In \Cref{sec:lifted_regression}, we discuss how these assumptions provide a bound on the number of steps required for mini-ALS.
Alternating minimization approaches have also been applied
in this line of noisy TC research~\citep{jain2014provable,liu2020tensor}.
It is also consistently observed that methods based on TD and tensor unfoldings are more practical and computationally efficient \citep{acar2011scalable,montanari2018spectral,filipovic2015tucker,shah2019iterative,shah2023robust}.