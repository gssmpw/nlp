\section{Related Work}
\label{sec:related-work}

\paragraph{CP completion.}
____ proposed an ALS algorithm for CP completion that repeats the following two-step process:
(1) fill in the missing values using the current CP decomposition, and 
(2) update one factor matrix.
Their algorithm is equivalent to running \emph{one iteration} of mini-ALS in each step of ALS.
As ____ discuss, this can lead to slower convergence and an increased likelihood of converging to suboptimal local minima because of errors introduced by the imputed missing values.

In contrast, approximate-mini-ALS runs \emph{until convergence} in each step of ALS.
By doing this, we establish a connection to the Richardson iteration
and build on its convergence guarantees.
Further, ____ explicitly fill in \emph{all missing values} using the current decomposition, whereas we only impute missing values \emph{required by row sampling},
allowing us to achieve sublinear-time updates in the size of the tensor.
In general, iteratively fitting to imputed missing values falls under the umbrella of \emph{expectation-maximization} (EM) algorithms____.

\paragraph{Statistical assumptions.}
Similar to minimizing the nuclear norm for matrix completion ____, a line of research in noisy TC suggests minimizing a convex relaxation of rank and identifies statistical assumptions under which the problem is recoverable ____.
Two standard assumptions are \emph{incoherence} and the \emph{missing-at-random} assumption. %i.e., observations are drawn uniformly at random.
In \Cref{sec:lifted_regression}, we discuss how these assumptions provide a bound on the number of steps required for mini-ALS.
Alternating minimization approaches have also been applied
in this line of noisy TC research____.
It is also consistently observed that methods based on TD and tensor unfoldings are more practical and computationally efficient ____.