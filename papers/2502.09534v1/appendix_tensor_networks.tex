\section{Missing Details for \Cref{sec:preliminaries}}

\subsection{Least-Squares Linear Regression}
\label{app:least-square-regression}

For a design matrix $\mat{A}\in \R^{n\times d}$ and response $\mat{b}\in \R^n$, consider the least-squares problem
\[
    \mat{x}^* = \argmin_{\mat x\in \R^d}\,\norm{\mat{Ax}-\mat{b}}_2\,.
\]
The solution $\mat{x}^*$ can be obtained by solving the \emph{normal equation} $\mat{A}^\top\mat{A}\mat{x}^*=\mat{A}^\top \mat{b}$.
Therefore, $\mat{x}^* = (\mat{A}^\top\mat{A})^{-1}\mat{A}^\top \mat{b}$.
If $\mat{A}^\top\mat{A}$ is singular, then we can use the \emph{pseudoinverse} $\mat{A}^+$.

The \emph{orthogonal projection matrix} $\pi_{\mat{A}}:\R^n \to \R^n$ onto the image space of $\mat{A}$ is defined by
$\pi_{\mat{A}} = \mat{A}\,(\mat{A}^\top\mat{A})^{-1}\mat{A}^\top$, and satisfies $\pi_{\mat{A}}^2 = \pi_{\mat{A}}$ and $\norm{\pi_{\mat A}\mat v}_2 \leq \norm{\mat v}_2$ for any $\mat{v} \in \R^n$. 
Recall that any $\mat{v}\in \R^n$ can be uniquely decomposed as $\mat v = \pi_{\mat{A}}\mat v + \pi_{\mat{A}^\perp}\mat v$, where $\pi_{\mat{A}^\perp}=\mat{I}_{n} - \pi_{\mat{A}}$ is the orthogonal projection to the orthogonal subspace of $\textnormal{colsp}(\mat{A})$.

Given $\mat{b} = \pi_{\mat{A}}\mat b + \pi_{\mat A ^\perp}\mat b$, the first term is the \emph{reducible error} by regressing $\mat b$ on $\mat x$,
i.e., taking the optimum $\mat{x}^*$ so that $\mat{Ax}^* = \pi_{\mat{A}}\mat{b}$.
The second term  $\pi_{\mat{A}^\perp}\mat{b}$ is the \emph{irreducible error}, i.e., $\min\,\norm{\mat{Ax}-\mat b}_2 =\norm{\pi_{\mat A^\perp}\mat b}_2$.


\subsection{Leverage Score Sampling for Tensor Decomposition}
\label{app:leverage-score}

ALS formulations show how each tensor decomposition step reduces to solving a least-squares problem of the form
$\min_{\mat{x}}\,\norm{\mat{A}\mat x - \mat{b}}_2$ with a highly structured $\mat{A}$.
While we can find the optimum in closed form via $(\mat{A}^\top \mat{A})^{+} \mat{A}^\top \mat{b}$, matrix $\mat{A}$ has $I = I_1 \cdots I_N$ rows corresponding to each entry of the tensor (i.e., it is a tall skinny matrix), which can make using the normal equation challenging in practice.

Randomized sketching methods are a popular approach to approximately solving this problem with faster running times with high probability.
In general, these approach sample rows of $\mat{A}$ according to the probability distribution defined by the \emph{leverage scores} of rows, resulting in a random sketching matrix $\mat{S}$ whose height is much smaller than that of $\mat{A}$. For a matrix $\mat{A} \in \R^{I\times R}$ with ($I\gg R$), the leverage scores of $\mat{A}$ is the vector $\ell\in [0,1]^I$ defined by
\[
\ell_i \defeq \bigl(\mat{A}\,(\mat{A^\top A})^+\mat{A}^\top\bigr)_{ii}\,.
\]
Then, for a given $\varepsilon, \delta \in (0,1)$, the sketching algorithm samples $\tilde{O}(\nicefrac{R}{\varepsilon\delta})$ many rows, where the $i$-th row is drawn with probability $\ell_i  / \sum_i \ell_i = \ell_i / \text{rank}\,(\mat{A})$.
With probability at least $1-\delta$, we can guarantee that
\[
\min_{\mat{x}} \,\norm{\mat{S}\mat{A}\mat{x}-\mat{S}\mat{b}}_2 \leq (1+\varepsilon)\, \min_{\mat{x}}\,\norm{\mat{A}\mat x - \mat{b}}_2\,.
\]
The reduced number of rows in $\mat{SA}$ leads to better running times for the least-squares solves.
However, na\"ively computing leverage scores takes as long as computing the closed-form optimum since we need to compute $(\mat{A}^\top \mat{A})^{+}$.
This is where the \emph{structure} of the design matrix $\mat{A}$ comes in to play, i.e., to speed up the leverage score computations.


\subsection{Tensor-Train Decomposition}
\label{app:tt-decomposition-details}

Given a tensor-train (TT) decomposition $\{\tensor{A}^{(n)}\}_{n=1}^{N}$ and index $n\in[N]$, define the
\emph{left chain} $\mat{A}_{<n} \in \R^{(I_1 \cdots I_{n-1}) \times R_{n-1}}$
and
the \emph{right chain} $\mat{A}_{> n}\in \R^{R_n \times (I_{n+1} \cdots I_N)}$ as:
\begin{align*}
    a_{< n}(\underline{i_1\dots i_{n-1}}, r_{n-1})
    &=
    \sum_{r_0,\dots,r_{n-1}} \prod_{k=1}^{n-1} a^{(k)}_{r_{k-1}i_kr_k}
    \\
    a_{> n}(r_n, \underline{i_{n+1}\dots i_N})
    &=
    \sum_{r_{n+1},\dots,r_N} \prod_{k=n+1}^N a^{(k)}_{r_{k-1}i_kr_k}\,,
\end{align*}
where for any $i_s \in [I_s]$ with $s (\neq n)\in [N]$, $\underline{i_1\dots i_{n-1}}:= 1+ \sum_{k=1}^{n-1} (i_k -1)\prod_{j=1}^{k-1} I_j$ and $\underline{i_{n+1}\dots i_N}:= 1+ \sum_{k=n+1}^{N} (i_k -1)\prod_{j=n+1}^{k-1} I_j$.
When ALS optimizes $\tensor{A}^{(n)}$ with all other TT-cores fixed, it solves the regression problem:
\[
    \tensor{A}^{(n)} \!\!
    \gets \!\!
    \argmin_{\tensor{B} \in \R^{R_{n-1}\times I_n \times R_n}}
    \bigl\lVert (\mat{A}_{<n}\kron \mat{A}^\top_{>n})\,\mat{B}_{(2)}^\top - \mat{X}_{(n)}^\top \bigr\rVert_{\frobenius}\,,
\]
which is equivalent to solving $I_n$ Kronecker regression problems in $\R^{\prod_{k\neq n} I_k}$.


\subsection{Tensor Networks}
\label{app:tensor-networks}

A \emph{tensor network} (TN) is a powerful framework that can represent any factorization of a tensor,
so it can recover the three decompositions above as special cases.
A TN decomposition $\text{TN}(\tensor A^{(1)},\dots,\tensor A^{(N)})$ represents a given tensor $\tensor X$ with $N$ tensors $\tensor{A}^{(1)},\dots, \tensor A^{(N)}$ and a \emph{tensor diagram}. As in the above decompositions, the goal is to compute
\[
\argmin_{\tensor A^{(1)},\dots,\tensor A^{(N)}}\,\norm{\tensor X - \text{TN}(\tensor A^{(1)},\dots,\tensor A^{(N)})}_\frobenius^2\,.
\]
A tensor diagram\footnote{We refer readers to \url{https://tensornetwork.org/diagrams/} for more details.} consists of nodes with dangling edges, where a node indicates a tensor, and its dangling edge represents a mode, so that the number of dangling edges is the order of the tensor. For example, a node without an edge indicates a scalar, one with one dangling edge is a vector, and one with two dangling edges is a matrix.

When two dangling edges of two nodes are connected, we say that the two tensors are \emph{contracted} along that mode (i.e., a mode product of those two tensors). For example, when a node with two dangling edges shares one edge with another node with one dangling edge, it indicates a matrix-vector multiplication. Hence, the number of unmatched dangling edges in a tensor diagram corresponds to the order of its representing tensor.

\paragraph{ALS for TN decomposition.}
Given a TN decomposition $\{\tensor{A}^{(n)}\}_{n\in[N]}$, when ALS optimizes a tensor $\tensor A^{(n)}$ with all others fixed, it solves a linear regression problem of the form
\[
\tensor{A}^{(n)} \gets \argmin_{\tensor{B}} \, \lVert \mat A_{\neq n}\mat{B} - \mat{X}\rVert_{\frobenius}\,,
\]
where $\mat A_{\neq n}$ is an appropriate matricization depending on $\tensor A^{(1)},\dots,\tensor A^{(n-1)}, \tensor A^{(n+1)},\dots,\tensor A^{(N)}$, and $\mat B$ and $\mat X$ are suitable matricizations of $\tensor B$ and $\tensor X$, respectively. Structure of $\mat A_{\neq n}$ can be specified through a new tensor diagram obtained by removing the node of $\tensor A^{(n)}$ from the original tensor network diagram.

Just as the ALS approaches for other decomposition algorithms, \citet{malik2022sampling} proposed a sampling-based approach via leverage scores. First of all, they pointed out that $\mat{A}_{\neq n}^\top \mat A_{\neq n}$ can be efficiently computed by exploiting inherent structure of $\mat A_{\neq n}$ (i.e., contract a series of matched edges in a tensor diagram in an appropriate order). They then presented a leverage-score sampling method that draws rows of $\mat A_{\neq n}$ without materializing a full probability vector, and in spirit this approach is similar to one used for the CP decomposition in \cref{subsec:CP-completion}.

