\section{Related Work}
\label{sec:related-work}

\paragraph{CP completion.}
Battaglia et al., "Relational Inductive Bias for Low-Level Computer Vision using Convolutional Neural Networks" proposed an ALS algorithm for CP completion that repeats the following two-step process:
(1) fill in the missing values using the current CP decomposition, and 
(2) update one factor matrix.
Their algorithm is equivalent to running \emph{one iteration} of mini-ALS in each step of ALS.
As Krichene et al. discuss, this can lead to slower convergence and an increased likelihood of converging to suboptimal local minima because of errors introduced by the imputed missing values.

In contrast, approximate-mini-ALS runs \emph{until convergence} in each step of ALS.
By doing this, we establish a connection to the Richardson iteration
and build on its convergence guarantees.
Further, Carreira-Perpinan et al. explicitly fill in \emph{all missing values} using the current decomposition, whereas we only impute missing values \emph{required by row sampling},
allowing us to achieve sublinear-time updates in the size of the tensor.
In general, iteratively fitting to imputed missing values falls under the umbrella of \emph{expectation-maximization} (EM) algorithmsCichocki et al..

\paragraph{Statistical assumptions.}
Similar to minimizing the nuclear norm for matrix completion Candes et al., a line of research in noisy TC suggests minimizing a convex relaxation of rank and identifies statistical assumptions under which the problem is recoverable Sreedhar et al..
Two standard assumptions are \emph{incoherence} and the \emph{missing-at-random} assumption. %i.e., observations are drawn uniformly at random.
In \Cref{sec:lifted_regression}, we discuss how these assumptions provide a bound on the number of steps required for mini-ALS.
Alternating minimization approaches have also been applied
in this line of noisy TC research Chen et al..
It is also consistently observed that methods based on TD and tensor unfoldings are more practical and computationally efficient Yuan et al..