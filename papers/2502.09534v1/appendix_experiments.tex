\section{Additional Details for \Cref{sec:experiments}}
\label{app:experiments}

All experiments are implemented with NumPy~\citep{harris2020array} and Tensorly~\citep{tensorly}
on an Apple M2 chip with 8 GB of RAM.

\subsection{Warm-Up: Coupled Matrix Problem}
\label{app:kronecker_regression_experiments}
We consider the coupled matrix problem of the following form \citep{baksalary1980matrix}:
\[
\mat{A} \mat{X} \mat{B}^\top + \mat{C} \mat{Y} \mat{D}^\top = \mat{E}\,,
\] 
where $\mat{A},\mat{B},\mat{C},\mat{D}$ are given $n\times d$ matrices, $\mat{X},\mat{Y}$ are unknown $d\times d$ matrices and $\mat{E}$ is an $n\times n$ matrix.
For a fixed $\mat{Y}$, we can compute $\mat{X}$ by solving the following regression problem 
\begin{equation}
\label{eq:coupled-matrix-als}
\argmin_{\mat{X} \in \R^{d\times d}} \norm{(\mat{B} \kron \mat{A}) \vvec(\mat{X}) - \vvec(\mat{E} - \mat{C} \mat{Y} \mat{D}^\top)}_2\,.
\end{equation}
Matrix $\mat{Y}$ can be updated solving a similar regression problem.
Thus, we can apply an alternating minimization algorithm for computing $\mat{X}$ and $\mat{Y}$. 
For our experiments, we initialize $\mat{X}=\mat{Y}=\mat{I}$.
The results illustrated in \Cref{fig:kronecker} are averaged over five trials.

\paragraph{Data generation.}
The entries of $\mat{A},\mat{B},\mat{C},\mat{D},\mat{X},\mat{Y}$ are picked independently from a uniform distribution on $[0,1)$, and $\mat{E}$ is set to $\mat{A} \mat{X} \mat{B}^\top + \mat{C} \mat{Y} \mat{D}^\top$. We consider the case where only half of the entries of $\mat{E}$ (selected uniformly at random) are observed.
We set $n=2000$ and $d=10$. Note that since we only observe a subset of entries of $\mat{E}$, \emph{the Kronecker regression in \eqref{eq:coupled-matrix-als} is lost}.

\paragraph{Algorithms.} 
We compare
\texttt{direct}, \texttt{mini-als}, and \texttt{approximate-mini-als} methods. The latter two use \emph{adaptive} step sizes based on the trajectory of $\mat{x}^{(k)}$ (see \Cref{app:acceleration} for details). \texttt{direct} solves the normal equation in each ALS step
and has running time $O(|\Omega| R^2 + R^3)$ since it computes $(\mat{P}^\intercal \mat{P})^{-1}$.
\texttt{mini-als} is our lifting approach in Algorithm~\ref{alg:approximate-lifting}
with $\widehat{\varepsilon} = 0$ and $\varepsilon > 0$. \texttt{mini-als} uses properties of Kronecker product such as $((\mat{B} \kron \mat{A})^\top (\mat{B} \kron \mat{A}))^{-1} = (\mat{B}^\top \mat{B})^{-1} \kron (\mat{A}^\top \mat{A})^{-1}$ and $(\mat{B} \kron \mat{A} )\vvec(\mat{X}) = \vvec(\mat{A}\mat{X} \mat{B}^\top)$ to achieve a better efficiency.
\texttt{approximate-mini-als} uses leverage score sampling for Kronecker product similar to \citet{fahrbach2022subquadratic,diao2019optimal}, which is a direct application of \Cref{cor:fast_tucker_completion_core_tensor_update}. For leverage score sampling, we sample $1\%$ of rows in each iteration of \texttt{approximate-mini-als},
and then we run the algorithm for $7$ iterations.

\subsection{CP Completion}

\subsubsection{Synthetic Tensors}
\label{app:synthetic_experiments}

We run the same set of experiments as in \Cref{sec:experiments}
on two different random low-rank tensors:
\begin{itemize}
    \item \textsc{random-cp} is a $100 \times 100 \times 100$ tensor formed by reconstructing a random rank-16 CP decomposition.
    \item \textsc{random-tucker} is a $100 \times 100 \times 100$ tensor formed by reconstructing a random rank-$(4,4,4)$ Tucker decomposition.
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/random-cp_shape-100-100-100_rank-16_seed-1234/cp-completion/step_vs_both-loss_fixed-sample-ratio.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/random-cp_shape-100-100-100_rank-16_seed-1234/lifted-comparison/step_vs_both-losses.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/random-cp_shape-100-100-100_rank-16_seed-1234/lifted-comparison/step_vs_solve-time_2.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/random-cp_shape-100-100-100_rank-16_seed-1234/lifted-comparison/step_vs_solve-time_2_accelerated.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/random-tucker_shape-100-100-100_rank-4-4-4_seed-1234/cp-completion/step_vs_both-loss_fixed-sample-ratio.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/random-tucker_shape-100-100-100_rank-4-4-4_seed-1234/lifted-comparison/step_vs_both-losses.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/random-tucker_shape-100-100-100_rank-4-4-4_seed-1234/lifted-comparison/step_vs_solve-time_2.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/random-tucker_shape-100-100-100_rank-4-4-4_seed-1234/lifted-comparison/step_vs_solve-time_2_accelerated.png}
    \end{subfigure}
    \caption{Algorithm comparison for a low-rank CP completion task on the \textsc{random-cp} and \textsc{random-tucker} tensors.}
\end{figure}


\subsubsection{Accelerated Methods}
\label{app:acceleration}

We explain how to accelerate the Richardson iteration.
\begin{enumerate}
    \item For odd iterations (e.g., the first iteration), run mini-ALS normally.
    \item For even iterations, compute $\widehat{\mat{x}}^{(k+1)}$ using normal mini-ALS, but then set
    \[
        \mat{x}^{(k+1)} = \mat{x}^{(k)} + \frac{1}{1-\alpha}\parens*{\widehat{\mat{x}}^{(k+1)} - \mat{x}^{(k)}},
    \]
    where $\alpha = \frac{\norm{\widehat{\mat{x}}^{(k+1)} - \mat{x}^{(k)}}_2}{\norm{\mat{x}^{(k)} - \mat{x}^{(k-1)}}_2}$.
\end{enumerate}
Note that setting $\alpha=0$ is equivalent to running mini-ALS normally. We explain the intuition behind this accelration with the example in \Cref{fig:extrapol}, which illustrates the case where $x$ and $b_{\overline{\Omega}}$ both have only one variable. As mentioned previously, the lifted problem is a convex quadratic problem (see \cref{lem:lifted_problem_is_convex_quadratic}), and the iterations of mini-ALS alternate between optimizing $x$ and optimizing $b_{\overline{\Omega}}$. 
In \Cref{fig:extrapol}, the star ($\star$) denotes the optimal point, and the ellipses denote the level sets of the quadratic function. In the steps where we optimize $x$, we search for the point on the line that is parallel to the $x$ axis that crosses our current point and touches the smallest ellipse among the points on the line. Similarly, when we update $b_{\overline{\Omega}}$ we search on the line that is parallel to the $b_{\overline{\Omega}}$ axis.

Following the points in \Cref{fig:extrapol}, one can see that the points obtained through our iterations in the one-variable case form similar triangles, where the ratio of corresponding sides for every two consecutive triangles are the same. 
Therefore, if we denote the side length of the first triangle with $1$, then the side length for the next triangles are $\alpha,\alpha^2,\alpha^3,\ldots$.
Using the notation in \Cref{fig:extrapol}, $\alpha=\frac{\ell^{(2)}}{\ell^{(1)}}$. The sum over this geometric series is equal to $\frac{1}{1-\alpha}$, which inspires our \emph{adaptive step size} in even iterations.

Note that in the one-variable case, we can recover the optimal solution with only two iterations as the lines connecting $(x^{(k)},b_{\overline{\Omega}}^{(k-1)})$ go through the optimal solution. While this does not necessarily hold in higher-dimensional settings, our experiments demonstrate that acceleration improves the number of iterations and the running time of our approach significantly, especially when the number of observed entries are small (i.e., when $\beta$ is large).

\begin{figure*}
\centering
\begin{tikzpicture}[scale=0.5]

    \draw[->] (-3,-2.5) -- (7,-2.5) node[right] {$x$};
    \draw[->] (-3,-2.5) -- (-3,7.5) node[above] {$b_{\overline{\Omega}}$};

    \draw[-] (-3,6.7) -- (7.5,6.7);
    \draw[-] (5.0,7.0) -- (5.0,-2.5);
    \draw[-] (-3,2.0) -- (5.5,2.0);
    \draw[-] (1.5,2.2) -- (1.5,-2.5);
    \draw[-] (-3,0.6) -- (2.2,0.6);
    \draw[-] (0.45,0.9) -- (0.45,-2.5);
    
    \draw[-] (0.0,0.0) -- (6,8);

    \begin{scope}[rotate=30]
        \draw[thick, black] (0,0) ellipse [x radius=10.1, y radius=5.05];
        
        \draw[thick, black] (0,0) ellipse [x radius=5.54, y radius=2.77];
    
        \draw[thick, black] (0,0) ellipse [x radius=3, y radius=1.5];
        
        \draw[thick, black] (0,0) ellipse [x radius=1.64, y radius=0.82];

        \draw[thick, black] (0,0) ellipse [x radius=0.9, y radius=0.45];

    \end{scope}

    \fill[black] (7.0, 6.7) circle (5pt) node[anchor=south]{\tiny $(x^{(0)},b_{\overline{\Omega}}^{(0)})$};
    \fill[black] (5.0, 6.7) circle (5pt) node[anchor=south east]{\tiny $(x^{(1)},b_{\overline{\Omega}}^{(0)})$};
    \fill[black] (5.0, 2.0) circle (5pt) node[anchor=south west]{\tiny $(x^{(1)},b_{\overline{\Omega}}^{(1)})$};
    \fill[black] (1.5, 2.0) circle (5pt) node[anchor=south east]{\tiny $(x^{(2)},b_{\overline{\Omega}}^{(1)})$};
    \fill[black] (1.5, 0.6) circle (5pt) node[anchor=south west]{\tiny $(x^{(2)},b_{\overline{\Omega}}^{(2)})$};
    \fill[black] (0.45, 0.6) circle (5pt) node[anchor=south east]{\tiny $(x^{(3)},b_{\overline{\Omega}}^{(2)})$};

    \node[star, star points=5, star point ratio=0.3, fill=black, draw=black] at (0, 0) {};

    \draw[<->, dashed] (5.0, 2.3) -- (1.5, 2.3) node[midway, above] {$\tiny\ell^{(1)}$};
    \draw[<->, dashed] (1.5, 0.9) -- (0.45, 0.9) node[midway, above] {$\tiny\ell^{(2)}$};
\end{tikzpicture}
\caption{In the one-variable case, our approach proceeds in the following order: $(x^{(0)}, b^{(0)}_{\overline \Omega}) \to (x^{(1)}, b^{(0)}_{\overline \Omega}) \to (x^{(1)}, b^{(1)}_{\overline \Omega}) \to \cdots$.}
\label{fig:extrapol}
\end{figure*}
