\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/algorithm_summary.pdf}
    \caption{
    Algorithm Overview of DEFT.
    In the initialization stage (Section \ref{sec:BDLO model}), DEFT begins by separating the BDLO into a parent DLO and one or more children DLOs. 
    Each DLO is discretized into vertices and represented as elastic rods. 
    This setup allows DEFT to capture the geometric and physical properties required for dynamic simulation.
    To improve computational efficiency, DEFT then predicts the dynamics of each branch in parallel (Section \ref{section:parallelprogramming}). 
    During this process, analytical gradients are provided to solve \eqref{eq:innerloop}, as detailed in Section \ref{section:gradientopt}, ensuring efficient and stable convergence.
    Next, to address numerical errors, DEFT employs a GNN designed to learn the BDLO’s residual dynamics (Section \ref{section:residual learning}). 
    By modeling discrepancies between simulated and observed behavior, the GNN refines predictions and enhances overall accuracy.
    After integration, DEFT enforces constraints (Section \ref{section:constratins}) to enforce physical realism.
    Inextensibility constraints are applied to each branch, while junction-level constraints ensure proper attachment at branch junctions. 
    Additionally, edge orientation constraints enable the propagation of dynamics across these junctions.
    Throughout the entire pipeline, all components remain fully differentiable, allowing for efficient parameter learning from real-world data.
    }
    \label{fig:deft_contribution_diagram}
\end{figure*}

\section{Methodology}

Our objective is to model the dynamic behavior of a BDLO under robotic control.
Traditional modeling approaches simplify the problem by assuming that the DLO’s endpoints are each held by separate robot end-effectors~\cite{DEFORM, IRP, diminishingridgidity, directionalrigidity, inlstm, gelsim}. 
Although this assumption reduces complexity, it limits the potential for dexterous manipulation (e.g., inserting the middle of a BDLO into a plug).
This work relaxes this assumption and considers scenarios where the BDLO is grasped at an intermediate point, rather than exclusively at its ends.

This section introduces the key components of BDLO modeling and overviews the various components of DEFT, which are illustrated in Figure \ref{fig:deft_contribution_diagram}. 
The key components of DEFT are:
1) analytical gradients to accelerate the optimization procedure used to solve \eqref{eq:opt_theta};
2) a GNN architecture tailored for residual learning in BDLO simulations;
3) an inextensible constraint formulation that enables the propagation of dynamics at the junction between branches while conserving momentum, which improves the numerical stability of the simulation; 
and 4) a parallel programming strategy that ensures efficient simulation and gradient computation during training and simulation.
Please note that to simplify the presentation, we omit the time subscript $t$ in Sections \ref{section:gradientopt} and \ref{section:constratins}.


\subsection{Branched Deformable Linear Object Model}
\label{sec:BDLO model}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/BDLO_model.png}
    \caption{
    An illustration of a branched deformable linear object (BDLO) model.
    Left: An example configuration of BDLO manipulation.
    Right: Relevant notation and structure at a junction.}
    \label{fig:BDLO}
\end{figure}

To model a BDLO, we decompose its structure into a hierarchical arrangement of DLOs. 
The parent DLO is defined as the minimum-length path through the BDLO that connects the two end-effectors (i.e., the points where the structure is manipulated or anchored). 
All other branches diverging from this primary path are modeled as children DLOs that maintain their physical connection to the parent DLO at their attachment points, which we refer to as \emph{junction points}.
The parent and children DLOs are each modeled as DLOs and inherit all previously introduced notation.
% To model a BDLO, we first identify a parent DLO  and branches as child DLOs. 
% The parent DLO is the shortest path along the BDLO between two end-effectors, while other branches are treated as child DLOs. 
% Each branch is modeled as a DLO and inherits all previously introduced notation.
For conciseness, we describe the junction point using only three edges at each time instance: two from the parent DLO and one from a child DLO. 
In principle, the methods in DEFT can be extended to handle junctions with more than three edges. 

We distinguish between the parent and child DLOs using superscripts $p$ and $c$, respectively. 
For instance, if a junction is located at the $i$-th vertex, the associated edges are denoted by $\timevertind{e}{p, i-1}{t}$, $\timevertind{e}{p, i}{t}$, and $\timevertind{e}{c, 1}{t}$. 
An illustration of a BDLO can be found in Figure \ref{fig:BDLO}.
To capture the influence of bending and twisting, we introduce the moment of inertia of the $i$-th edge  which is denoted by $\moi\in \mathbb{R}^{3\times3}$.
It represents how the segment’s mass distribution affects its resistance to rotational deformations.
The orientation of $\timevertind{e}{i}{t}$ is denoted by an angle axis vector  $\mathbf{\Omega}^i_t\in \mathbb{R}^{3}$.
% as demonstrated in Figure \Yizhou{to do}. 

Let the orientation and position of the two end effectors at time $t$ be denoted by $\vertindu_t \in \R^{12}$.
We refer to $\vertindu_t$ as the input.
We assume that the initial state of the BDLO is fully observable, and that it can be accurately estimated using image segmentation~\cite{SAM} in combination with Gaussian Mixture Models~\cite{BDLO_perception}.


\subsection{DEFT Overview}

At each time $t$, DEFT simulates the behavior of a BDLO when given predictions of the vertices and their velocities under a specified input as described in Algorithm \ref{alg:long time horizon}. 
It begins by first associating the grasped vertices with the robot end effector inputs  $\vertindu_t$.
DEFT then processes the parent and children DLOs in parallel to compute their respective material frames using \eqref{eq:m1} \eqref{eq:m2}, which are subsequently used to calculate each branch’s potential energy.
DEFT then minimizes this potential energy using analytical gradients to determine the optimal parameters $\bm{\theta}_t^*$ and updates the potential energy accordingly (Line 2). 
% The negative gradient of the updated potential energy provides the twisting and bending forces acting on the branches.
These forces are integrated forward in time using a Semi-Implicit Euler method combined with residual learning to update the vertex velocities and positions (Lines 3 and 4). 

However, due to numerical integration errors, branch lengths may change in length.
In addition, up to this point, DEFT treats each branch’s internal dynamics in isolation, without accounting for junction interactions. 
This omission can hinder the propagation of dynamics across junctions.
For example, children DLOs may detach from their parent branches, or deformation effects (e.g., bending and twisting) may not propagate through the junction.
To address these issues, DEFT enforces additional constraints to ensure each branch remains inextensible, maintain branch connections, and allow deformation effects to propagate across the junction (Line 5).
% This approach results in the prediction algorithm described in Algorithm~\ref{alg:long time horizon}.
By applying Algorithm \ref{alg:long time horizon} over multiple time increments, DEFT can accurately predict the BDLO’s state over extended time horizons.
An algorithm overview is illustrated in Figure \ref{fig:deft_contribution_diagram}.

\begin{algorithm}[t]
\caption{$(\pred{X}{t+1}, \pred{V}{t+1}) = \textbf{DEFT}(\pred{X}{t}, \pred{V}{t}, \textbf{u}_t$)} 
\label{alg:long time horizon}
% \textbf{Inputs}: $\pred{X}{t}, \pred{V}{t}, \textbf{u}_t$
\begin{algorithmic}[1]
\State Associate grasped vertices with $\textbf{u}_t$
\State Calculate Material Frames $\MaterialFrame(\textbf{X}_t, \bm{\theta}_t)$ \Comment{\eqref{eq:m1},\eqref{eq:m2}}
\State $\bm{\theta}^*_t(\textbf{X}_t,\materialp) = \argmin_{\bm{\theta}_t} P(\MaterialFrame(\textbf{X}_t, \bm{\theta}_t),\materialp)$ \Comment{\secref{section:gradientopt}}
\State $\hat{\textbf{V}}_{t+1} = \hat{\textbf{V}}_t-\Delta_t \Massmatrix^{-1}\frac{\partial P(\MaterialFrame(\textbf{X}_t, \bm{\theta}^*_t(\textbf{X}_t,\materialp)),\bm{\alpha})}{\partial \textbf{X}_{t}}$ \Comment{\eqref{eq:Semi-Euler1}}   
\State $\hat{\mathbf{X}}_{t+1} = \hat{\mathbf{X}}_t + \Delta_t\hat{\textbf{V}}_{t+1} + \text{GNN}$ \Comment{\secref{section:residual learning}} \label{alg1:residual}
\State Constraints enforcement on $\hat{\mathbf{X}}_{t+1}$ 
\Comment{Algorithm \ref{alg:constraints enforcement}}
\State $\hat{\textbf{V}}_{t+1} = (\hat{\mathbf{X}}_{t+1} - \hat{\mathbf{X}}_t)/\Delta_t$ \Comment{Velocity Update}
\end{algorithmic}
\Return $\pred{X}{t+1}, \pred{V}{t+1}$
\end{algorithm}

\subsection{Analytical Gradient Derivation}
\label{section:gradientopt}

The goal of this subsection is to present the analytical gradient of $P(\MaterialFrame(\mathbf{X}_t, \bm{\theta}_t), \materialp)$ with respect to $\bm{\theta}_t$. 
To improve the paper’s readability, we summarize the result in the following theorem whose proof can be found in Appendix~\ref{appendix: Theorem 1 Proof}.
\begin{thm}
\label{thm:potential_energy_gradient}
Let $\MaterialFrame(\mathbf{X}_t, \bm{\theta}_t)$ be the Material Frame at time $t$, and let 
$\materialp$ contain the bending stiffness $\mathbf{B} \in \mathbb{R}^{3\times3} $ and twisting stiffness $\beta \in \mathbb{R}$. 
Suppose the total potential energy is the sum of bending and twisting potential energies:
\begin{equation}
\begin{split}
  P(\MaterialFrame(\mathbf{X}_t, \bm{\theta}_t), \materialp) 
  \;=\; P_\text{bend}\bigl(\MaterialFrame(\mathbf{X}_t, \bm{\theta}_t), \mathbf{B}\bigr) + \\
     P_\text{twist}\bigl(\MaterialFrame(\mathbf{X}_t, \bm{\theta}_t), \beta\bigr).
  \label{eq:potentialP}
\end{split}
\end{equation}
Then the partial derivative of the bending and twisting potential energies with respect to $\theta^i$ are given by
\begin{align}
    \begin{split}
  \frac{\partial P_{\mathrm{bend}}(\MaterialFrame(\mathbf{X}_t, \bm{\theta}_t), \materialp\bigr)}{\partial \theta^i}
  \;=\; 
  \sum_{k=i}^{\,i+1}
  \bigl(\mathbf{B}^k\, (&\bm{\omega}^{(k,i)}  - \bar{\bm{\omega}}^{(k,i)})\bigr)^{T}
  \cdot \\
    &\begin{bmatrix}0 & 1\\[6pt]-1 & 0\end{bmatrix}
  \bm{\omega}^{(k,i)},
      \end{split}
  \label{eq:potentialbendgradient_paper}
\end{align}
\begin{equation}
\begin{split}
  \frac{\partial P_{\mathrm{twist}}(\MaterialFrame(\mathbf{X}_t, \bm{\theta}_t), \materialp\bigr)}{\partial \theta^i}
  \;=\;
  \beta^i \,\bigl(\theta^i - \theta^{i-1}\bigr)\;-\;
  \\ 
  \beta^{i+1}\,\bigl(\theta^{i+1} - \theta^i\bigr),
  \label{eq:potentialtwistgradient_paper}
  \end{split}
\end{equation}
respectively, where the material curvature $\bm{\omega}^{(i,j)}$ is defined as
\begin{equation}
  \bm{\omega}^{(i,j)} \;=\; 
  \bigl(\bcurvature \cdot \mathbf{m}_1^j,\;\bcurvature \cdot \mathbf{m}_2^j\bigr)^T
  \quad \text{for} \quad j \in \{i-1,\,i\},
  \label{eq:materialcurvature_paper}
\end{equation}
\(\overline{\bm{\omega}}^{(i,j)}\) denotes the undeformed material curvature, 
and the curvature vector $\bcurvature$ in \eqref{eq:materialcurvature_paper} is
\begin{equation}
  \kappa \,\mathbf{b}^i 
  \;=\; 
  \frac{2\,\bigl(\mathbf{e}^{i-1} \times \mathbf{e}^i\bigr)}
  {\|\mathbf{e}^{i-1}\|_2\,\|\mathbf{e}^{i}\|_2 \;+\;\mathbf{e}^{i-1} \cdot \mathbf{e}^{i}}.
  \label{eq:curvature}
\end{equation}
\end{thm}
% In practice, we supply sum of \ref{eq:potentialbendgradient_paper} and \ref{eq:potentialtwistgradient_paper} to Theseus \cite{theseus} to solve \eqref{eq:opt_theta} using Levenberg–Marquardt solver.

\subsection{Integration Method with Residual Learning}
\label{section:residual learning}

This subsection constructs the GNN for residual learning in \eqref{eq:NN-Semi-Euler3}. 
The motivation for employing a GNN lies in its ability to represent a BDLO as a graph, where each vertex is treated as a node and edges capture both intra-branch connections and junctions. 
This graph-based formulation translates into an adjacency matrix \(\mathbf{A} \in \mathbb{R}^{(n^p + n^c) \times (n^p + n^c)}\), which links individual branch nodes and explicitly encodes node information across branch junctions.
This adjacency matrix is defined as:
\begin{equation}
\mathbf{A}_{i_1, i_2} =
\begin{cases}
1 & \text{if there is an edge between } i_1 \text{ and } i_2, \\
0 & \text{otherwise,}
\end{cases}
\end{equation}
An illustration of $\mathbf{A}$ can be found in Figure \ref{fig:matrix_A_illustration}. 
In practice,  DEFT constructs $\mathbf{A}$ for each BDLO whose behavior a user wants to predict.
This adjacency matrix is utilized within a Graph Convolutional Network (GCN)~\cite{GCN} to aggregate learned features. 
A single GCN layer is defined as
\begin{equation}
  \text{GCN}(\hat{\mathbf{X}}_t, \hat{\mathbf{V}}_t, \bm{\alpha}) 
  = \mathbf{\tilde{A}} \,\mathbf{F}_t(\hat{\mathbf{X}}_t, \hat{\mathbf{V}}_t, \bm{\alpha}) \,\mathbf{W},
  \label{eq:GCN}
\end{equation}
where \(\mathbf{\tilde{A}}\) is the normalized adjacency matrix 
\begin{equation}
\mathbf{\tilde{A}} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}.
\label{eq:normalizedA}
\end{equation}
Note that $\mathbf{\tilde{A}}$ is the normalization of \(\mathbf{A}\) using the degree matrix \(\mathbf{D} = \mathrm{diag}(d_1, d_2, \ldots, d_n)\)  where \(d_{i1} = \sum_{i2} A_{i1,i2}\).
This normalization ensures that nodes with a high degree do not disproprtionately influence the overall performance. 
This ensures consistent feature magnitudes across the learned representation~\cite{GNNreview}.
The node feature matrix, \(\mathbf{F}_t(\hat{\mathbf{X}}_t, \hat{\mathbf{V}}_t, \bm{\alpha}) \in \mathbb{R}^{(n^p + n^c) \times h}\), is constructed by concatenating its arguments together. 
The trainable weight matrix \(\mathbf{W} \in \mathbb{R}^{h \times 3}\) is implemented via a multi-layer perceptron (MLP)~\cite{MLP}, which projects input features into latent representations.

% By normalizing \(\mathbf{A}\) t degree matrix \(\mathbf{D} = \mathrm{diag}(d_1, d_2, \ldots, d_n)\) with \(d_{i1} = \sum_{i2} A_{i1,i2}\), nodes with a high degree do not disproportionately influence the overall aggregation, thereby preserving consistent feature magnitudes and promoting well-scaled learned representations~\cite{GNNreview}.


\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/graph_matrix.png}
    \caption{
    The adjacency matrix $\mathbf{A}$ for the junction in Figure~\ref{fig:BDLO} 
    captures both self-loops (diagonal entries) and inter-node connections 
    (off-diagonal entries). By embedding each node’s local dynamics 
    alongside its coupling to neighboring nodes, $\mathbf{A}$ enables 
    an graph representation of the BDLO’s behavior.}
    \label{fig:matrix_A_illustration}
\end{figure}

\subsection{Enforcing Constraints}
\label{section:constratins}

After performing numerical integration, DEFT applies constraints to ensure branch inextensibility, ensure that branches are attached at the junctions, and ensure that deformations are propagated throughout the BDLO. 
To accomplish this, DEFT updates the positions of each vertex using correction terms $\Delta \pred{X}{}$.
One could just update the positions of each vertex by just projecting directly onto the constraint set. 
% One could enforce these constraints by utilizing the gradient of the constraint function with respect to $\Delta \pred{X}{}$.
However, this induces highly nonlinear changes in momentum and can lead to simulation instability and unrealistic behavior. 
% Furthermore, this approach can result in a non-smooth gradient during parameter learning.

Motivated by the PDB method \cite{PBD}, DEFT enforces the aforementioned constraints while trying to preserve momentum.
DEFT does this by formulating the computation of the correction term as the solution to an optimization problem.
As we illustrate through our experimental evaluation in Section \ref{sec:experiments}, this leads to more stable simulations, which corresponds to more realistic predictions. 
Finally, note that DEFT could formulate the optimization problem to compute the correction terms as a single large optimization problem that considers constraints between all vertices in the BDLO simultaneously. 
However, this large optimization problem can be difficult to solve rapidly. 
Instead DEFT enforces these constraints sequentially between neighboring nodes.
Next, we summarize each of these optimization problems.


% To describe our approach, suppose we have a constraint function $C: $.
% If this constraint function evaluates to zero, then the constraint is satisfied.
% To enforce the constraints
% In particular, we propose the following general formulation for the resulting optimization problem:
% \begin{align}
%     \label{eq:optimization}
%     & &\underset{\Delta \pred{X}{}}{\min}& \hspace{0.3cm} \sum \frac{1}{2}\text{C}(\pred{X}{}, \Delta \pred{X}{})^2  \\
%     &&\text{subject to} & \hspace{0.3cm} \sum_{i=1}^{n} \textit{\textbf{M}}^i \cdot \Delta \hat{\mathbf{x}}^i = \textbf{0}
%     \label{eq:linearm} \\
%     && &\hspace{0.3cm} \sum_{i=1}^{n-1}
%     \textit{\textbf{I}}^i \cdot \Delta \hat{\boldsymbol{\Omega}}^i (\hat{\mathbf{x}}^i, \hat{\mathbf{x}}^{i+1}, \Delta \hat{\mathbf{x}}^i, \Delta \hat{\mathbf{x}}^{i+1}) = \textbf{0}
%     \label{eq:angularm} 
% \end{align}
% Where \(\mathbf{C}\) represents the constraint functions, $\Delta \hat{\boldsymbol{\Omega}}^i (\hat{\mathbf{x}}^i, \hat{\mathbf{x}}^{i+1}, \Delta \hat{\mathbf{x}}^i, \Delta \hat{\mathbf{x}}^{i+1}) \in \mathbb{R}^{3}$ represents the orientation change of $i$th edge arised from $\Delta \hat{\mathbf{x}}^i, \Delta \hat{\mathbf{x}}^{i+1}$. 
% Detailed explaination of \eqref{eq:angularm} can be found in Appendix.\ref{sec:appendix_angular_momentum}.
% \eqref{eq:linearm} and \eqref{eq:angularm} represent the standard forms of the linear and angular momentum conservation equations, respectively.
% Because simultaneously solving the optimization problem in Equation~\eqref{eq:optimization}, while satisfying both momentum constraints \eqref{eq:linearm} and \eqref{eq:angularm}, is inherently high-dimensional and computationally challenging, iterative methods such as Gauss-Seidel \cite{Gaussseidel} are commonly used. 
% By sequentially projecting these constraints, such methods produce efficient, approximate solutions without fully resolving the entire large-scale optimization at once.
% This approach simplifies the problem to a set of pairwise constraints, as demonstrated in the following. 
% The constraints enforcement is summarized in Algorithm \ref{alg:constraints enforcement}.

\subsubsection{Inextensibility Enforcement}
To enforce inextensibility of each branch, we define a constraint function between successive vertices. 
This function penalizes deviations of each segment’s length from its undeformed length.
By applying the following theorem, whose proof can be found in  \cite[Appendix B.2]{DEFORM}, one can compute $\Delta \hat{\mathbf{x}}^i, \Delta \hat{\mathbf{x}}^{i+1}$ explicitly:
\begin{thm}[Computing the Corrections to Enforce Inextensibility]
    \label{theorem:inextensibility}
    Consider two successive vertices \(i\) and \(i+1\) along the same branch. 
    Let \(\overline{\mathbf{e}}_i\) denote the edge between these vertices when the BDLO is undeformed. 
    Let the constraint function to enforce inextensibility be defined as:
    \begin{equation}
   \hspace*{-0.1cm} C^i_{I}(\hat{\mathbf{X}},\Delta \hat{\mathbf{X}} ) = 
    % \text{C}(\hat{\mathbf{x}}^i, &\hat{\mathbf{x}}^{i+1}, \Delta \hat{\mathbf{x}}^i, \Delta \hat{\mathbf{x}}^{i+1}) = \\
    ||(\hat{\mathbf{x}}^i+\Delta \hat{\mathbf{x}}^i) - (\hat{\mathbf{x}}^{i+1}+\Delta \hat{\mathbf{x}}^{i+1})||_2 -  ||\overline{\textbf{e}}_i||_2.
    \label{eq:constraint_inextensibility}
    \end{equation}
    The correction $\Delta \hat{\mathbf{X}}$ to enforce inextensibility at joint $i$ can be computed by solving the following optimization problem:
    \begin{align}
    % \label{eq:optimization_simplified}
    &&\underset{\Delta \hat{\mathbf{X}}}{\min} & \hspace{0.3cm} \frac{1}{2}\left(C^i_{I}(\hat{\mathbf{X}},\Delta \hat{\mathbf{X}} ) \right)^2   & \label{eq:cost} \\
    &&\text{s.t.} &\hspace{0.3cm}  \Delta \hat{\mathbf{x}}^j = 0 &  \forall j \notin \{i,i+1\}, \notag  \\
    && & \hspace{0.3cm}  \mathbf{M}^i  \Delta \hat{\mathbf{x}}^i + \mathbf{M}^{i+1}  \Delta \hat{\mathbf{x}}^{i+1}= \mathbf{0}, &  \notag \\
    % \label{eq:linearm_simplified} \\
    && &\hspace{0.3cm} 
    \mathbf{I}^i   \hat{\boldsymbol{\Omega}}^i_\Delta (\hat{\mathbf{X}},\Delta \hat{\mathbf{X}} ) = \mathbf{0}, & \notag
    % \label{eq:angularm_simplifiedblah} 
\end{align}
% \begin{align}
%     \label{eq:optimization_simplified}
%     & &\underset{\Delta \hat{\mathbf{x}}^i, \Delta \hat{\mathbf{x}}^{i+1}}{\min}& \text{C}(\hdat{\mathbf{x}}^i, \hat{\mathbf{x}}^{i+1}, \Delta \hat{\mathbf{x}}^i, \Delta \hat{\mathbf{x}}^{i+1})  \label{eq:cost} \\
%     &&\text{s.t.} & \hspace{0.3cm}  \textit{\textbf{M}}^i \cdot \Delta \hat{\mathbf{x}}^i + \textit{\textbf{M}}^{i+1} \cdot \Delta \hat{\mathbf{x}}^{i+1}= \textbf{0}
%     \label{eq:linearm_simplified} \\
%     && &\hspace{0.3cm} 
%     \textit{\textbf{I}}^i \cdot  \hat{\boldsymbol{\Omega}}^i_\Delta (\hat{\mathbf{x}}^i, \hat{\mathbf{x}}^{i+1}, \Delta \hat{\mathbf{x}}^i, \Delta \hat{\mathbf{x}}^{i+1}) = \textbf{0},
%     \label{eq:angularm_simplifiedblah} 
% \end{align}
where $\hat{\boldsymbol{\Omega}}^i_\Delta (\hat{\mathbf{X}},\Delta \hat{\mathbf{X}} ) \in \mathbb{R}^{3}$ is defined in Appendix \ref{sec:appendix_angular_momentum} and represents the orientation change of the $i$th edge arising from $\Delta \hat{\mathbf{x}}^i$ and $\Delta \hat{\mathbf{x}}^{i+1}$ and $\mathbf{0}$ correspond to the zero vector of appropriate size. 
% Because of the first constraint in the optimization problem, the only correction term that is non-zero are the $i$\ts{th} and $(i+1)$\ts{st}.
The only non-zero elements of the minimizer to this optimization problem can be computed explicitly and are equal to:
\begin{equation}
    \label{eq:momentum_solution1_inext} 
    \Delta \hat{\mathbf{x}}^i 
    =
    \mathbf{M}^{i+1}\bigl(\mathbf{M}^i + \mathbf{M}^{i+1}\bigr)^{-1}
    C^i_{I}(\hat{\mathbf{X}}, \mathbf{0}) 
    \frac{ \bigl(\hat{\mathbf{x}}^{i+1} - \hat{\mathbf{x}}^{i}\bigr)
         }{\|\hat{\mathbf{x}}^{i+1} - \hat{\mathbf{x}}^{i}\|_2},
\end{equation}
\begin{equation}
    \label{eq:momentum_solution2_inext} 
    \Delta \hat{\mathbf{x}}^{i+1} 
    =
    \mathbf{M}^i(\mathbf{M}^i + \mathbf{M}^{i+1}\bigr)^{-1}  
       C^i_{I}(\hat{\mathbf{X}}, \mathbf{0})
    \frac{ \bigl(\hat{\mathbf{x}}^{i} - \hat{\mathbf{x}}^{i+1}\bigr)
         }{\|\hat{\mathbf{x}}^{i+1} - \hat{\mathbf{x}}^{i}\|_2}.
\end{equation}
\end{thm}
\noindent Note that the inextensibility constraint at vertex $i$ is satisfied when $C^i_{I}(\hat{\mathbf{X}}, \mathbf{0})^2 = 0$. 
For convenience, let the function that computes the minimizer of optimization problem \eqref{eq:cost} using $\hat{\mathbf{X}}$ be denoted by $D^i_I$ (i.e., $D^i_I: \hat{\mathbf{X}} \mapsto \Delta \hat{\mathbf{X}}$). 

\subsubsection{Enforcing Attachment at the Junction}
\label{sec:Attachment Enforcement at the Junction}
To ensure the child branch remains attached to the parent branch, we define a constraint function between theirjunction vertices. 
% This function penalizes the distance discrepancy between $\hat{\mathbf{x}}^{p,i}, \hat{\mathbf{x}}^{c,1}$ shown in the Figure.\ref{fig:matrix_A_illustration}.
By applying the following theorem, whose proof can be found in Appendix~\ref{appendix:theorem_junction1}, one can compute $\Delta \hat{\mathbf{x}}^{p,i}, \Delta \hat{\mathbf{x}}^{c,1}$ explicitly:
\begin{thm} 
\label{theorem:constraint_attachement}
Consider the parent-branch vertex $\hat{\mathbf{x}}^{p,i}$ and child-branch vertex $\hat{\mathbf{x}}^{c,1}$ at the junction such as the one depicted in Figure \ref{fig:BDLO}.
\label{thm:attachment}
Let the constraint function to enforce attachment at the junction be defined as:
\begin{equation}
\label{eq:constraint_attachement}
 C^{p,i}_A(\hat{\mathbf{X}}, \Delta \hat{\mathbf{X}}) = ||\bigl(\hat{\mathbf{x}}^{p,i} + \Delta \hat{\mathbf{x}}^{p,i}\bigr) - \bigl(\hat{\mathbf{x}}^{c,1} + \Delta \hat{\mathbf{x}}^{c,1}\bigr)||_2. 
    \end{equation}
% \begin{align}
%     \begin{split}
% \label{eq:constraint_attachement}
% & \text{C}(\hat{\mathbf{x}}^{p,i}, \hat{\mathbf{x}}^{c,1}, \Delta \hat{\mathbf{x}}^{p,i}, \Delta \hat{\mathbf{x}}^{c,1}) = \\ & ||\bigl(\hat{\mathbf{x}}^{p,i} + \Delta \hat{\mathbf{x}}^{p,i}\bigr) - \bigl(\hat{\mathbf{x}}^{c,1} + \Delta \hat{\mathbf{x}}^{c,1}\bigr)||_2. 
%     \end{split}
% \end{align}
Suppose that the correction $\Delta \hat{\mathbf{X}}$ to account for the junction attachment constraint is obtained by solving an optimization problem that is identical to \eqref{eq:cost} with the objective function replaced with \eqref{eq:constraint_attachement} and with a requirement that the only elements of $\Delta \hat{\mathbf{X}}$ are those associated vertices $p,i$ and $c,1$.
The only non-zero elements of the minimzer to this optimization problem can be computed explicitly and are equal to: 
\begin{equation*}
    \label{eq:momentum_solution1_attach} 
  \Delta \hat{\mathbf{x}}^{p, i} =  \mathbf{M}^{c, 1} (\mathbf{M}^{p, i}+\mathbf{M}^{c, 1})^{-1}  C^{p,i}_A(\hat{\mathbf{X}},\mathbf{0}) \frac{(\hat{\mathbf{x}}^{c, 1} - \hat{\mathbf{x}}^{p, i})}{||\hat{\mathbf{x}}^{c, 1} - \hat{\mathbf{x}}^{p, i}||_2},
\end{equation*}
\begin{equation*}
    \label{eq:momentum_solution2_attach} 
     \Delta \hat{\mathbf{x}}^{c, 1} =  \mathbf{M}^{p, i}  (\mathbf{M}^{p, i}+\mathbf{M}^{c, 1})^{-1}
    \ C^{p,i}_A(\hat{\mathbf{X}},\mathbf{0}) \frac{(\hat{\mathbf{x}}^{p, i} - \hat{\mathbf{x}}^{c, 1})}{||\hat{\mathbf{x}}^{c, 1} - \hat{\mathbf{x}}^{p, i}||_2}.
\end{equation*}
\end{thm}
\noindent  Note that the junction attachment constraint is satisfied at vertex $p,i$ when $C^{p,i}_{I}(\hat{\mathbf{X}}, \mathbf{0})^2 = 0$.
For convenience, let the function that computes the minimizer of optimization problem \eqref{eq:cost} using $\hat{\mathbf{X}}$ be denoted by $D^{p,i}_A$ (i.e., $D^{p,i}_A: \hat{\mathbf{X}} \mapsto \Delta \hat{\mathbf{X}}$). 

\subsubsection{Edge Orientation Constraints at the Junction}
Many practical wire-harnesses feature junctions that are nearly rigid. 
This near-rigidity arises because manufacturers often use rigid plastics or other stiff materials at these connection points \cite{wireharnesinstruction}.
This results in minimal rotational or bending freedom. 
Consequently, the dynamics between the parent and child branches propagate through the junction in an almost rigid manner.
To capture this behavior, we model each edge as a rigid body and define a constraint function that computes the relative orientation of each body.
We then require that this constraint function is approximately satisfied where ap
By applying the following theorem, whose proof can be found in Appendix \ref{appendix:theorem_junction2}, one can compute $\Delta \hat{\mathbf{x}}^{p,i}, \Delta \hat{\mathbf{x}}^{c,1}$ explicitly:
\begin{thm}
    \label{thm:junctionconstraint}
    Consider the parent-branch vertex $\hat{\mathbf{x}}^{p,i}$ and child-branch vertex $\hat{\mathbf{x}}^{c,1}$ at the junction such as the one depicted in Figure \ref{fig:BDLO}.
    Let $\epsilon > 0$ be a user specified parameter. 
    Let the constraint function to enforce the orientation of the junction be defined as:
    \begin{multline}
    \label{eq:constraints_orietation}
     C^{p,i}_O(\hat{\mathbf{X}}, \Delta \hat{\mathbf{X}}) =  \|\big(\hat{\boldsymbol{\Omega}}^{p, i}(\hat{\mathbf{X}}, \Delta \hat{\mathbf{X}})  + \hat{\boldsymbol{\Omega}}_{\Delta}^{p, i}(\hat{\mathbf{X}}, \Delta \hat{\mathbf{X}}) \big)
    - \\  \big( \hat{\boldsymbol{\Omega}}^{c, i}(\hat{\mathbf{X}}, \Delta \hat{\mathbf{X}}) + \hat{\boldsymbol{\Omega}}_{\Delta}^{c, i}(\hat{\mathbf{X}}, \Delta \hat{\mathbf{X}}\big) \|_2 - \epsilon,
    \end{multline}
    where $\hat{\boldsymbol{\Omega}}_{\Delta}$ and $\hat{\boldsymbol{\Omega}}$ are defined in Appendix \ref{sec:appendix_angular_momentum} and \(\epsilon > 0\) bounds the mismatch between the updated parent--child orientation and the original orientation, ensuring that each junction remains \emph{almost} rigid. 
    The orientation correction at the junction is obtained by solving the following optimization problem:
\begin{align}
    &&  
    \underset{\Delta \hat{\mathbf{X}}}{\min} & \hspace{0.3cm}
    \frac{1}{2}\left(C^{p,i}_O(\hat{\mathbf{X}}, \Delta \hat{\mathbf{X}})\right)^2 &
    \label{eq:constraint_rigid_body}\\
    &&  \text{s.t.} &  \hspace{0.3cm} 
    \mathbf{I}^{p, i} \hat{\boldsymbol{\Omega}}_{\Delta}^{p, i}  + \mathbf{I}^{c, 1} \hat{\boldsymbol{\Omega}}_{\Delta}^{c, 1}  = \mathbf{0} \notag &
    % \label{eq:angularm_simplified2} 
\end{align}
When $\Delta_t$ is sufficiently small, then one can compute explicit formulas for $\Delta \hat{\mathbf{X}}$ which can be found in Appendix \ref{appendix:theorem_junction2}.
\end{thm}
\noindent Note that the junction orientation constraint is satisfied at vertex $p,i$ when $C^{p,i}_{O}(\hat{\mathbf{X}}, \mathbf{0})^2 = 0$.
For convenience, let the function that computes the minimizer of optimization problem \eqref{eq:cost} using $\hat{\mathbf{X}}$ be denoted by $D^{p,i}_O$ (i.e., $D^{p,i}_O: \hat{\mathbf{X}} \mapsto \Delta \hat{\mathbf{X}}$). 

% \begin{thm}
%     \label{thm:junctionconstraint}
%     Consider the parent edge $\hat{\mathbf{e}}^{p,i}$ (or $\hat{\mathbf{e}}^{p,i-1}$) and child-branch vertex $\hat{\mathbf{e}}^{c,1}$ at the junction shown in the Figure.\ref{fig:BDLO}.
%     The orientation correction at the junction is obtained by solving the following optimization problem:
% \begin{align}
%     && & 
%     \underset{\Delta \hat{\boldsymbol{\Omega}}^{p, i}, \Delta \hat{\boldsymbol{\Omega}}^{c, 1}}{\min}
%     \frac{1}{2}(||(  \hat{\boldsymbol{\Omega}}^{p, i} + \Delta\hat{\boldsymbol{\Omega}}^{p, i}) - (  \hat{\boldsymbol{\Omega}}^{c, 1} + \Delta \hat{\boldsymbol{\Omega}}^{c, 1})||_2 - \epsilon)^2
%     \label{eq:constraint_rigid_body}\\
%     && & \text{subject to}  \hspace{0.3cm} 
%     \textit{\textbf{I}}^{p, i} \cdot \Delta \hat{\boldsymbol{\Omega}}^{p, i}  + \textit{\textbf{I}}^{c, 1} \cdot \Delta \hat{\boldsymbol{\Omega}}^{c, 1}  = \textbf{0}
%     \label{eq:angularm_simplified2} 
% \end{align}
% $\hat{\boldsymbol{\Omega}}^{p, i}(\hat{x}^{p,i}, \hat{x}^{p,i+1},\Delta \hat{x}^{p,i}, \Delta\hat{x}^{p,i+1}) $
% where \(\epsilon > 0\) bounds the mismatch between the updated parent--child orientation and the original orientation, ensuring each junction remains \emph{almost} rigid. 
% Note that rotational changes naturally preserve linear momentum, we omit the linear momentum constraint from the above optimization.
% One can find
% \begin{align}
%     \label{eq:delta_omega1}
%     \begin{split}
% \Delta \hat{\boldsymbol{\Omega}}^{p,i} 
% =& \mathbf{I}^{c, 1}\cdot(\mathbf{I}^{c, 1}+\mathbf{I}^{p,i})^{-1} \\
% \cdot & \text{C}(\hat{\boldsymbol{\Omega}}^{p, i}, \hat{\boldsymbol{\Omega}}^{c, 1}, \Delta \hat{\boldsymbol{\Omega}}^{p, i}, \Delta \hat{\boldsymbol{\Omega}}^{c, 1}) \frac{ \bigl(\hat{\boldsymbol{\Omega}}^{c, 1} - \hat{\boldsymbol{\Omega}}^{p, i}\bigr)
%          }{\|\hat{\boldsymbol{\Omega}}^{c, 1} - \hat{\boldsymbol{\Omega}}^{p, i}\|_2}
%     \end{split}
% \end{align}
% \begin{align}
%     \label{eq:delta_omega2}
%     \begin{split}
% \Delta \hat{\boldsymbol{\Omega}}^{c,1} =& \mathbf{I}^{p, i}\cdot(\mathbf{I}^{c,1}+\mathbf{I}^{p,i})^{-1} \\
% \cdot & \text{C}(\hat{\boldsymbol{\Omega}}^{p, i}, \hat{\boldsymbol{\Omega}}^{c, 1}, \Delta \hat{\boldsymbol{\Omega}}^{p, i}, \Delta \hat{\boldsymbol{\Omega}}^{c, 1})
% \frac{ \bigl(\hat{\boldsymbol{\Omega}}^{p, i} - \hat{\boldsymbol{\Omega}}^{c, 1}\bigr)
%          }{\|\hat{\boldsymbol{\Omega}}^{c, 1} - \hat{\boldsymbol{\Omega}}^{p, i}\|_2}
%     \end{split}
% \end{align}
% where the constraint function $\text{C}(\hat{\mathbf{x}}^i, \hat{\mathbf{x}}^{i+1}, \Delta   \hat{\mathbf{x}}^i, \Delta \hat{\mathbf{x}}^{i+1})$ is defined as following
% \begin{align}
%     \begin{split}
%     &\text{C}(\hat{\boldsymbol{\Omega}}^{p, i}, \hat{\boldsymbol{\Omega}}^{c, 1}, \Delta \hat{\boldsymbol{\Omega}}^{p, i}, \Delta \hat{\boldsymbol{\Omega}}^{c, 1} ) = \\
%     &||(\Delta \hat{\boldsymbol{\Omega}}^{p, i} + \hat{\boldsymbol{\Omega}}^{p, i}) - ( \Delta \hat{\boldsymbol{\Omega}}^{c, 1} + \hat{\boldsymbol{\Omega}}^{c, 1})||_2 - \epsilon
%     \end{split}
% \end{align}
% Once \(\Delta \hat{\boldsymbol{\Omega}}^{p,i}\) and \(\Delta \hat{\boldsymbol{\Omega}}^{c,1}\) are computed, the edges \(\hat{\mathbf{e}}^{p,i}\) and \(\hat{\mathbf{e}}^{c,1}\) undergo the corresponding rotational updates, and the related vertices corrections \(\Delta \hat{\mathbf{x}}^{p, i}, \Delta \hat{\mathbf{x}}^{p, i+1}, \Delta \hat{\mathbf{x}}^{c, 1}, \Delta \hat{\mathbf{x}}^{c, 2}\) can be adjusted accordingly.
% \end{thm}

\subsubsection{Algorithm to Implement Corrections Due To Constraint Enforcement}
As described earlier, we run each of these optimization problems sequentially to generate a correction to the output of the integration method (Line \ref{alg1:residual} in Algorithm \ref{alg:long time horizon}). 
This is summarized in Algorithm \ref{alg:constraints enforcement}.
Note that one begins by checking whether any constraint is violated by more than some user-specified parameter $\kappa$. 
If any of the constraints are violated by more than $\kappa$, then one applies the correction described in the previous subsections to modify the prediction. 
This repeats until all constraints are satisfied. 

\begin{algorithm}[t]
\caption{Momentum Preserving Constraint Enforcement} 
\label{alg:constraints enforcement}
\begin{algorithmic}[1]
\Require $\pred{X}{t+1}, \kappa > 0$
\While {$\exists (i, j) \text{ such that } C^i_j(\hat{\mathbf{X}}_{t+1},\mathbf{0})^2  > \kappa$}
    \For{$i = 1$ \textbf{to} $n - 1$}
        \State $\hat{\mathbf{X}}_{t+1} = \hat{\mathbf{X}}_{t+1} + D^i_I(\hat{\mathbf{X}}_{t+1})$
        \Comment{Theorem~\ref{theorem:inextensibility}}
        \If{$i$th segments is a junction}
        \State $\hat{\mathbf{X}}_{t+1} = \hat{\mathbf{X}}_{t+1} + D^i_A(\hat{\mathbf{X}}_{t+1})$ \Comment{Theorem \ref{theorem:constraint_attachement}}
        \State $\hat{\mathbf{X}}_{t+1} = \hat{\mathbf{X}}_{t+1} + D^i_O(\hat{\mathbf{X}}_{t+1})$ \Comment{Theorem \ref{thm:junctionconstraint}}
        \EndIf
    \EndFor
\EndWhile
% \Until{\textbf{all} $||\hat{\textbf{x}}_{t+1}^i - \hat{\textbf{x}}_{t+1}^\text{i+1}|| - ||\overline{\textbf{e}}_i||_2 < threshold$}
\State \Return: $\hat{\textbf{X}}_{t}$ \Comment{Updated Vertices}
\end{algorithmic}
\end{algorithm}

\subsection{Parallel Programming}
\label{section:parallelprogramming}

We implement DEFT using PyTorch~\cite{pytorch}, taking advantage of its batch operations to execute Algorithm \ref{alg:long time horizon}. 
PyTorch's support for batch-wise computations enables parallel processing of multiple parent and child branch instances, significantly enhancing computational efficiency as we illustrate empirically in Section \ref{sec:computationaltime}.
Specifically, the first batch is dedicated to executing the parent branch's algorithm, while the remaining batches handle executions for the children branches. 

One challenge arises when the number of nodes in the parent branch ($n^p$) differs from the number in a child branch ($n^c$). 
This dimensional mismatch prevents the direct construction of uniform matrices for batch operations.
To overcome this, we assume $n^p > n^c$ and introduce padded matrices and vertices by appending zeros to the child branch's structure to match the parent branch's dimensions. 
This strategy ensures uniform matrix construction across the batches, which facilitates seamless batch-wise operations. 
During parameter updates, we mask out the gradients from padded elements to prevent them from influencing the learned parameters.


