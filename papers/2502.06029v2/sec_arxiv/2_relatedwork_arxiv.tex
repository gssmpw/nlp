\section{Related Work}
\label{sec:relatedwork}
Our work lies at the intersection of four research areas: multi-task vision  learning, parameter-efficient fine-tuning, neural network adaptation, and diffeomorphisms. We discuss each area with a focus on how they relate to weight matrix and singular value adaptation. 

\paragraph{Multi-Task Learning in Vision.} 
Multi-task learning models in vision typically adopt an encoder-decoder structure, where a shared encoder captures common features, and task-specific decoders handle individual objectives~\citep{misra2016cross}. This approach has proven particularly effective for dense prediction tasks, with different decoders handling semantic segmentation, depth estimation, and surface normal prediction while sharing a common feature backbone~\citep{kendall2018multi}. While recent work has extended this framework to Vision Transformers (ViTs)~\citep{dosovitskiy2021an}, these adaptations face unique challenges due to their large parameter count and the complex interactions between task-specific updates. 
To address these issues, \ourmethod\ introduces an encoder adaptation strategy that modulates singular values while retaining pre-trained singular vectors, rather than modifying full weight matrices or adding task-specific layers. This leverages the encoder-decoder framework for efficient adaptation of the shared encoder.

\paragraph{Parameter-Efficient Fine-tuning Methods.} 
These methods reduce computational overhead for model adaptation using various strategies. Adapter-based approaches~\citep{houlsby2019parameterefficienttransferlearningnlp} insert small trainable modules between transformer layers. Visual Prompt Tuning (VPT)~\citep{jia2022visual} introduces trainable parameters in ViT input tokens, distinguishing it from standard adaptation approaches. Compactor~\citep{mahabadi2021parameter} and BitFit~\citep{zaken2021bitfit} add minimal trainable parameters, enabling lightweight gradients via Kronecker products of rank-one and low-rank matrices and fine-tuning only biases, respectively. Hyperformers~\citep{mahabadi2021parameter} use hypernetworks to generate adapter layers for multi-task language learning, while Polyhistor~\citep{liu2022polyhistor} adapts this for dense vision tasks. VL-Adapter~\citep{sung2022vl} introduces adapters for joint Vision-Language (VL) multi-task fine-tuning.

With the introduction of GPT-2~\citep{radford2019language} and the rise in parameter sizes and task variety, methods like Low-Rank Adaptation (LoRA)\citep{hu2022lora} have gained traction in both language and vision. These methods restrict weight updates to fixed low-dimensional subspaces, enhancing resource efficiency. Other approaches, such as SVFT\citep{lingam2024svft}, enforce sparse singular values, while DoRA~\citep{liu2024dora} decouples the magnitude and direction of weight update columns. In contrast, ReFT~\citep{wuandarora2024reft} uses low-rank approximation to adapt representations rather than weights.

Although most of these methods are not specifically designed for multi-task learning (MTL), MTLoRA~\citep{agiza2024mtlora} has applied LoRA within an MTL framework. By enabling both task-agnostic and task-specific adaptations along with joint inference, MTLoRA has achieved state-of-the-art results on dense vision tasks. However, these methods, while effective in preserving pre-trained knowledge, still impose subspace constraints that can degrade performance relative to single-task baselines. 

\ourmethod\ takes a fundamentally different approach: instead of adding modules or modifying weights directly, we preserve the pre-trained singular vectors while enabling flexible adaptation via diffeomorphisms of singular values. This allows us to maintain both architectural efficiency and rich feature adaptability, without altering the input-output mapping learned during pre-training.
\vspace{-2pt}
\paragraph{Multi-Task Learning Dynamics and Challenges.} 
A recent study of Transformer training dynamics reveals an incremental learning phenomenon where weight update ranks gradually increase during training~\cite {NEURIPS2023_4d69c1c0}. This finding challenges the effectiveness of fixed-rank adaptation methods in MTL settings. While MTLoRA~\citep{agiza2024mtlora} attempts to address this through task-agnostic and task-specific modules, it still operates within fixed low-rank constraints. Similarly, gradient interference mitigation techniques like PCGrad address conflicts between task updates but do not provide the flexibility needed for diverse task adaptations. In contrast, \ourmethod\ aligns with natural learning dynamics by preserving the full-rank structure through singular vector retention while enabling task-specific adaptations via neural diffeomorphisms~\citep{freifeld2015highly, freifeld2017transformations}, uniquely combining parameter efficiency with the adaptability needed for effective multi-task learning.

\paragraph{Diffeomorphisms in Neural Networks.}
A bijective mapping \( f:\gM \to \gN \) between two differentiable manifolds \(\gM\) and \(\gN\) is a \emph{diffeomorphism} if both \( f \) and its inverse \( f^{-1}:\gN \to \gM \) are differentiable. Learning diffeomorphisms poses computational challenges, as early methods were constrained by complex, infinite-dimensional spaces~\citep{NEURIPS2018_68148596}, and later Markov Chain Monte Carlo methods remained demanding~\citep{allassonniere2010construction, allassonniere2015bayesian, zhang2016bayesian}. \citet{freifeld2015highly, freifeld2017transformations} addressed these issues with Continuous Piecewise-Affine Based (CPAB) transformations, a finite-dimensional approach suited for precise 1D diffeomorphisms, ideal for neural activation functions. CPAB offers linear complexity, supports parallelization, and achieves sub-linear performance in practice~\cite{freifeld2017transformations}. Initially developed for alignment and regression, CPAB is now widely applied in neural networks. For instance, \citet{detlefsen2018deep} enhanced spatial transformer layers with CPAB, \citet{martinez2022closed} applied CPAB to temporal alignment, \citet{weber2023regularization} developed a CPAB-free loss for time-series analysis, \citet{wang2024continuous} used CPAB for image animation, and \citet{Chelly2024DiTAC} proposed a trainable CPAB-based activation function.
\iffalse
A bijective mapping 
$f:\gM \to \gN$ between two differentiable manifolds $\gM$ and $\gN$ qualifies as a \emph{diffeomorphism} if both 
$f$ and its inverse $f^{-1}:\gN \to \gM$ are differentiable. Learning diffeomorphisms presents computational challenges, as initial approaches were often constrained by complex, infinite-dimensional spaces~\citep{NEURIPS2018_68148596}. Later developments explored Markov Chain Monte Carlo methods, though these methods also remained computationally demanding~\citep{allassonniere2010construction, allassonniere2015bayesian, zhang2016bayesian}.
To mitigate these computational burdens, \citet{freifeld2015highly, freifeld2017transformations} introduced the Continuous Piecewise-Affine Based (CPAB) transformation, a more practical approach that uses a finite-dimensional representation. This method supports precise diffeomorphism calculations in one dimension, which is advantageous in our context since activation functions are inherently 1D. Notably, CPAB offers linear complexity and supports parallelization, which can yield sub-linear performance in practical scenarios~\citep{freifeld2017transformations}. 
Initially developed for tasks such as alignment and regression, CPAB has recently found success in a range of neural network applications, establishing it as an effective framework for learning transformations. For example, \citet{detlefsen2018deep} enhanced spatial transformer layers by incorporating CPAB transformations for greater flexibility, \citet{martinez2022closed} combined CPAB with neural networks for aligning temporal data, \citet{weber2023regularization} introduced a novel loss function that negates the need for CPAB regularization in time-series analysis, \citet{wang2024continuous} applied CPAB to model complex spatial transformations in image animation and motion modeling, and \citet{Chelly2024DiTAC} proposed a trainable activation function based on CPAB.
\fi