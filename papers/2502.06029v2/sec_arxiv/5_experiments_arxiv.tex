\section{Experiments}
\label{sec:exp}
    \begin{table*}[t]
      \centering
      \scriptsize
      \setlength{\tabcolsep}{2.5pt}
    \renewcommand{\arraystretch}{0.75}
    \scriptsize
      \caption{\textbf{Performance Comparison on PASCAL Context for Multi-Task Learning:} This table summarizes the performance of our proposed 
    \ourmethod\ method compared to various fine-tuning approaches, including full fine-tuning, adapter-based methods, and fixed-rank approaches. Results are reported for four dense prediction tasks: Semantic Segmentation (\textsc{SemSeg}), Human Part Segmentation, Saliency Detection, and Surface Normal Estimation. Metrics include \textsc{mIoU} for segmentation tasks and \textsc{rmse} for normal estimation, with $\Delta m$ indicating the percentage improvement over single-task fine-tuning. Our method achieves superior performance with minimal trainable parameters, enabling efficient single inference across all tasks.}
      \vspace{-5pt}
    \small
      \begin{tabular}{l  cccc  cc  c }
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \textbf{\textsc{SemSeg}} & \textbf{\textsc{Human Parts}} & \textbf{\textsc{Saliency}} & \textbf{\textsc{Normals}} & 
          \multirow{2}{*}{$\Delta m (\%) $} & \textbf{Trainable Swin} & \textbf{Single Inference}   \\
          & ($\textsc{mIoU} \uparrow$) & ($\textsc{mIoU} \uparrow$) & ($\textsc{mIoU} \uparrow$) & ($\textsc{rmse} \downarrow$) & & 
          \textbf{Parameters} (M) &
          \textbf{For All Tasks}   \\
        \midrule
        \textbf{\textsc{Full Fine-Tuning}} & \\
        Single Task & 67.21 & 61.93 & 62.35 & 17.97 & \(\phantom{-}0.00\) & 112.62   & $\times$ \\
        MTL - Dec. Only  & 65.09 & 53.48 & 57.46 & 20.69 & \(-\phantom{0}9.95\)  & \phantom{0}\phantom{0}0\phantom{0}\phantom{0}  & \checkmark  \\
        MTL - Full  &  67.56 & 60.24 & 65.21 & 16.64 & \(+\phantom{0}2.23\) &  \phantom{0}28.12    & \checkmark  \\
        \midrule
        \textbf{\textsc{Adapter-Based}} & \\
        VPT-shallow \cite{jia2022visual}  & 62.96 &  52.27 & 58.31 & 20.90 & \(-11.18\)  & \phantom{0}\phantom{0}0.63  & $\times$  \\
        VPT-deep \cite{jia2022visual}  & 64.35 & 52.54 & 58.15 & 21.07 & \(-10.85\) & \phantom{0}\phantom{0}1.49  & $\times$ \\
        Compactor++ \cite{karimi2021compacter}  & 67.26 & 55.69 & 59.47 & 19.54 & \(-\phantom{0}5.84\) & \phantom{0}\phantom{0}0.72    & $\times$ \\
        Bitfit \cite{zaken2021bitfit} & 68.57 & 55.99 & 60.64 & 19.42 & \(-\phantom{0}4.60\) & \phantom{0}\phantom{0}0.91   & $\times$  \\
        Compactor \cite{karimi2021compacter} & 68.08 & 56.41 & 60.08 & 19.22 & \(-\phantom{0}4.55\) & \phantom{0}\phantom{0}0.84   & $\times$  \\
        
        Adapter \cite{he2021towards} &  69.21 &  57.38 & 61.28 & 18.83  & \(-\phantom{0}2.71\) & \phantom{0}\phantom{0}9.26  & $\times$ \\
        VL-Adapter \cite{sung2022vl} & 70.21 & 59.15 & 62.29 & 19.26 & \(-\phantom{0}1.83\)  & \phantom{0}\phantom{0}2.80  & $\times$ \\
        Polyhistor \cite{liu2022polyhistor} & 70.87  & 59.15  & 65.54 &  17.77 & \(+\phantom{0}2.34\)  & \phantom{0}\phantom{0}7.02  & $\times$ \\
        HyperFormer \cite{mahabadi2021parameter}  & 71.43 & 60.73 & 65.54 &  17.77 & \(+\phantom{0}2.64\)  & \phantom{0}70.83  & $\times$ \\
        \midrule
        \textbf{\textsc{Fixed-Rank}} & \\
        MTL - DoRA~\citep{liu2024dora} & 52.36 & 50.82 & 63.53 & 18.32 & \(-10.02\) & \phantom{0}\phantom{0}6.40 & \checkmark\\
        MTL - ReFT~\citep{wuandarora2024reft} & 68.75 & 56.49 & 58.76 & 20.54 & \(-\phantom{0}6.63\) & \phantom{0}16.16 & \checkmark \\
        MTL - SVFT~\citep{lingam2024svft} & 64.44 & 55.94 & 63.03 & 17.86 & \(-\phantom{0}3.02\) & \phantom{0}\phantom{0}3.83 & \checkmark\\
    LoRA \cite{hu2022lora} & 70.12 & 57.73 & 61.90 & 18.96 & \(-\phantom{0}2.17\) & \phantom{0}\phantom{0}0.93   & $\times$ \\    
        MTLoRA~\citep{agiza2024mtlora} (\(r = 16\)) & 68.19 & 58.99 & 64.48 & 17.03 & \(+\phantom{0}1.35\) & \phantom{0}\phantom{0}3.01  & \textbf{\checkmark} \\
        MTLoRA~\citep{agiza2024mtlora} ($r = 32$) & 67.74 & 59.46 & 64.90 & 16.59 & \(+\phantom{0}2.16\) & \phantom{0}\phantom{0}4.14  & \textbf{\checkmark} \\
        MTLoRA~\citep{agiza2024mtlora} ($r = 64$) & 67.90 & 59.84 & 65.40 & 16.60 & \(+\phantom{0}2.55\) & \phantom{0}\phantom{0}6.40  & \textbf{\checkmark} \\
        \midrule
        Single Task - \ourmethod & \textbf{72.20} & \textbf{62.33} & \textbf{65.70} & \textbf{16.55} & \(\mathbf{+\phantom{0}5.33}\) & \phantom{0}\phantom{0}1.60 ($\times$4) & $\times$\\
        MTL - \ourmethod\ (Ours) & 69.66 & 62.02 & 65.00 & 17.10 & \(\mathbf{+\phantom{0}3.22}\) & \phantom{0}\phantom{0}1.61 & \checkmark\\
        \bottomrule
      \end{tabular}
      \label{tab:results}
    \end{table*}

We evaluate \ourmethod{} on multiple MTL benchmarks and compare it with other adaptation methods. \Cref{subsec:expsetup} details the experimental setup and baselines,   \Cref{subsec:results} discusses our main findings, and \Cref{sec:ablation} presents few ablation studies of \ourmethod.
 We seek to address three key research questions: 
\begin{itemize}
    \item (RQ1) Can a selective modification of singular values while preserving singular vectors bridge the performance gap between decoder-only and full fine-tuning?
    
    \item (RQ2) Does full-rank preservation by singular value modulation outperform low-rank adaptation methods?
    
    \item (RQ3) How does \ourmethod\ compare to other parameter-efficient adaptation methods?

\end{itemize}

\subsection{Experimental Setup}
\label{subsec:expsetup}
\noindent\textbf{Datasets and Tasks.} We evaluate \ourmethod\ on the PASCAL-MTL dataset~\citep{pascal} (PASCAL-Context split) following the literature on multi-task dense prediction~\citep{vandenhende2020mti, xu2018pad, ye2022inverted}. This dataset includes 4,998 training and 5,105 validation images with annotations for semantic segmentation (21 classes), human part segmentation (7 classes), surface normal estimation, and saliency detection. Additional experiments on the  NYUD dataset~\citep{silberman2012indoor} are provided in \Cref{tab:nyud}.
\\
\noindent\textbf{Implementation Details.} We use the Swin-Tiny~\citep{liu2021swin} architecture, pre-trained on ImageNet-21k \cite{russakovsky2015imagenet} as the backbone encoder, similar to \cite{agiza2024mtlora}. To ensure a fair comparison, we incorporate the HRNet~\citep{hr_net} decoder, which accounts for 6\% of the total parameters, as in previous works \cite{agiza2024mtlora,vandenhende2020mti}.  Following MTINet~\citep{vandenhende2020mti}, we apply cross-entropy loss for segmentation tasks, $\ell_1$ loss for surface normals, and balanced cross-entropy \cite{cui2019classbalancedlossbasedeffective} for saliency detection, with same task weights $\{\lambda_k\}_{k=1}^K$ used in MTINet. In all experiments, we follow the evaluation protocol from \citet{agiza2024mtlora}.

\noindent\textbf{Baselines.} We benchmark our \ourmethod{} with three categories of methods:
\begin{table*}[t]
  \centering
  \scriptsize
   \setlength{\tabcolsep}{7.5pt}
  \caption{Effect of Backbone Size  -- Relative improvement increases with increasing capacity of the backbone}
  \vspace{-5pt}
\small
  \begin{tabular}{l  cccc  c  c  }
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \textbf{\textsc{SemSeg}} & \textbf{\textsc{Human Parts}} & \textbf{\textsc{Saliency}} & \textbf{\textsc{Normals}} &
      \multirow{2}{*}{$\Delta m (\%) $} & \textbf{Trainable Swin} \\
      & ($\textsc{mIoU} \uparrow$) & ($\textsc{mIoU} \uparrow$) & ($\textsc{mIoU} \uparrow$) & ($\textsc{rmse} \downarrow$) & & 
      \textbf{Parameters} (M)    \\
    \midrule
    \ourmethod\ + Swin-Tiny & 69.66 & 62.02 & \textbf{65.00} & 17.10 & \(+3.22\) & 1.61 \\
    \ourmethod\ + Swin-Small & 74.49 & 63.20 & 64.58 & 17.58 & \(+4.68\) & 1.66 \\
    \ourmethod\ + Swin-Base & 75.86 & 65.97 & 64.18 & 17.29 & \(+6.52\) & 3.14 \\
    \ourmethod\ + Swin-Large & \textbf{76.23} & \textbf{67.53} & 64.07 & \textbf{16.90} & \(\mathbf{+7.79}\) & 7.13 \\
    \bottomrule
  \end{tabular}
  \label{tab:backbone}
\end{table*}
\begin{itemize}
    \item \textit{Full Fine-Tuning}: Methods that modify all weight matrices (single-task, MTL decoder, full MTL)
    \item \textit{Adapter-based}: Methods that add task-specific layers while preserving main weights (Adapter~\citep{houlsby2019parameterefficienttransferlearningnlp}, BitFit~\citep{zaken2021bitfit}, VPT~\citep{jia2022visual}, Compactor~\citep{karimi2021compacter}, VL-Adapter~\citep{sung2022vl}, HyperFormer~\citep{mahabadi2021parameter}, Polyhistor~\citep{liu2022polyhistor})
    \item \textit{Fixed-Rank}: Methods that constrain updates to low-rank subspaces (LoRA~\citep{hu2022lora}, DoRA~\citep{liu2024dora}, SVFT~\citep{lingam2024svft}, ReFT~\citep{wuandarora2024reft}, MTLoRA~\citep{agiza2024mtlora})
\end{itemize}

\subsection{Results and Discussion}
  \label{subsec:results}
Our main results, summarized in \Cref{tab:results}, address (RQ1)-(RQ3) with respect to the effectiveness of \ourmethod{} in MTL, as discussed below.
\\
\noindent\textbf{Bridging the Efficiency-Performance Gap.} \ourmethod\ achieves a \(\mathbf{+3.22\%}\) relative improvement over single-task baselines using only 1.61M trainable encoder parameters. This significantly outperforms both decoder-only tuning (\(-9.95\%\)) and full fine-tuning (\(+2.23\%\)), demonstrating that selective modulation of singular values while preserving pre-trained feature directions effectively balances parameter efficiency and performance.
\\
\noindent\textbf{Comparison with Fixed Low-Rank Methods.}
By maintaining full-rank structure through singular value modulation, \ourmethod\ achieves a \(26.27\%\) improvement in \(\Delta m\) over the state-of-the-art MTLoRA, using \(\mathbf{4\times}\) fewer parameters (see \Cref{fig:parameterallocation}). Most fixed low-rank methods show negative \(\Delta m\), as restricting updates to shared low-rank subspaces causes task interference. Allowing singular values to be modulated independently for each task, our approach mitigates this interference effectively. Similar conclusions can drawn for NYUD based on \Cref{tab:nyud}.
\\
\noindent\textbf{Parameter Efficiency and Task Performance.}
Among all evaluated parameter-efficient methods, \ourmethod\ consistently yields positive improvements over single-task baselines, confirming that preserving pre-trained singular vectors with task-specific singular value modulation balances parameter efficiency and performance effectively.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/ParameterAllocation.pdf}
    \caption{Comparison of adaptation parameter budget of the shared encoder (excluding attention mask and layerNorm)}
    \label{fig:parameterallocation}
\end{figure}

\subsection{Ablation Study}
\label{sec:ablation}
We analyze key components of \ourmethod\ to understand their contributions to multi-task performance.
\\
\noindent\textbf{Computational Efficiency.}
\ourmethod\ uses only 8.5K adaptation parameters, compared to MTLoRA's 4.86M, while achieving 22\% faster batch processing (27.31s vs 35.20s) and reduced memory usage (see \Cref{tab:runtime}). The efficiency gains stem from our compact CPAB parameterization of singular value transformations.
\begin{figure}[t]
  \centering
\includegraphics[width=1.0\linewidth]{figs/TATSAblation.pdf} 
  \caption{Effect of task-specific and task-agnostic components}
  \label{fig:ab1}
\end{figure}
\\
\noindent\textbf{Joint vs Task-Wise Singular Value Modulation.}
We evaluate three configurations of singular value modulation, shown in  \Cref{fig:ab1}. Joint Adaptation alone, with shared modulation across tasks, achieves an improvement of 2.77\%. Task-Wise Adaptation alone, which modulates singular values independently for each task, yields 2.85\%. The Combined approach, \ourmethod, incorporates both strategies and reaches the highest improvement at 3.22\%, supporting the use of both shared and task-specific transformations.

\noindent\textbf{Model Scaling Analysis.}
The effectiveness of \ourmethod{} scales with model capacity (\Cref{tab:backbone}). This scaling effect indicates that larger models provide richer singular value spaces, enabling more effective task-specific adaptation.
\begin{table}[t]
    \centering
    \scriptsize
    \caption{Computational efficiency comparison between \ourmethod\ and MTLoRA. \ourmethod{} achieves faster inference (\(\approx 22\%\) reduction in batch runtime) and lower memory footprint while requiring \(500 \times\) fewer adaptation parameters.}
    \label{tab:runtime}
    \begin{adjustbox}{width=\columnwidth}
    \begin{tabular}{lccc}
      \toprule
      Method & Batch Runtime & Max GPU Mem  & \# Adaptation \\
      & (seconds) & (Megabytes) & Params\\
      \midrule
        MTLoRA & 35.20$\pm$1.05 & 23509 & 4.86\textsc{m}\\
        \ourmethod\ (ours) & 27.31$\pm$5.29 & 22906 & 8.5$\textsc{k}$\\
      \bottomrule
    \end{tabular}
  \end{adjustbox}
\end{table}
\\
\noindent\textbf{Qualitative Comparison.} In \Cref{fig:viz} we present a visual comparison between MTLoRA and our \ourmethod\ on the semantic segmentation task from PASCAL MTL. \ourmethod\ demonstrates tighter and clearer boundaries, more consistent segmentation, and improved class separation. For example, in the dimly lit sofa scene in the second column, MTLoRA struggles to correctly predict the sofa boundary, while \ourmethod\ offers a better prediction.

\begin{table}[t]
    \centering
    \begin{adjustbox}{width=\columnwidth}
  \begin{tabular}{l cccc}
    \toprule
         Method & \textbf{\textsc{SemSeg}} & \textbf{\textsc{Depth}} & ${\Delta m (\%)}$ & \textbf{Trainable Params} \\
         & $(\textsc{mIoU}) \uparrow$ & $(\textsc{rmse}) \downarrow$ & & (in M)\\
         \midrule
         Single Task & 33.18 & 0.667 & 0 & 112.62 \\
         MTL - Dec. Only & 28.37 & 0.832 & -19.61 & 1.00\\
         MTL - Full & 35.29 & 0.734 & -1.84 & 28.5\\
         \midrule
         MTLoRA & 37.18 & 0.635 & +8.42 & 6.26 \\
         \midrule
         Single Task - \ourmethod & 44.01 & 0.644 & +18.04 & 1.61\\
         \ourmethod\ (ours) & 43.85 & 0.606 & +\textbf{20.65} & 1.61\\
         \bottomrule
    \end{tabular}
  \end{adjustbox}
    
    \caption{Comparison of \ourmethod\ with MTLoRA on NYUD~\citep{silberman2012indoor} MTL dataset for Semantic Segmentation (\textsc{SemSeg)} and Depth Estimation tasks. We use \textsc{mIoU} for segmentation and \textsc{rmse} for depth estimation as metrics.}
    \label{tab:nyud}
\end{table}