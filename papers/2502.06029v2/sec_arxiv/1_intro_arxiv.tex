\begin{figure}[!t]
    \centering
\includegraphics[width=
\linewidth]{figs/teaser.pdf}
    \caption{Performance-efficiency trade-off in multi-task learning (MTL) across parameter-efficient fine-tuning methods on PASCAL MTL tasks. Our method \ourmethod\ achieves superior relative improvement over single-task fine-tuning while using significantly fewer parameters compared to fixed-rank approaches (LoRA~\citep{hu2022lora}) and other PEFT methods (Adapter~\citep{houlsby2019parameterefficienttransferlearningnlp}). The gray region indicates negative relative improvement versus single-task fine-tuning, illustrating the fundamental trade-off in MTL: methods with fewer parameters struggle to match single-task performance while achieving positive improvements typically requires substantial parameter overhead.
    }
    \label{fig:teaser}
\end{figure}

\section{Introduction}
\label{sec:intro}
Vision Transformers (ViTs) have emerged as compelling models for computer vision, achieving remarkable performance across various tasks by leveraging self-attention mechanisms~\citep{vaswani2017attention,dosovitskiy2021an} and large-scale pre-training~\citep{steiner2021train, russakovsky2015imagenet} on extensive datasets like ImageNet-21k~\citep{deng2009imagenet}. These models encode essential visual knowledge in their weight matrices through singular vectors that characterize the principal directions in the input space and output spaces~\citep{NEURIPS2018_a7a3d70c, saxe2014exactsolutionsnonlineardynamics}.
However, existing fine-tuning methods based on low-rank adaptation (LoRA) \cite{hu2022lora, agiza2024mtlora} may not preserve these learned singular vectors. In \Cref{fig:motivation}, we illustrate the importance of preserving the singular vectors; recovering a noisy image by adjusting only its singular values while keeping singular vectors yields \emph{superior} performance than using LoRA, which may not preserve the singular vectors of the image.  \revision{Singular vectors encode crucial image details and low-rank denoising methods can lose these details, resulting in poor denoising quality. Preserving singular vectors serves as an implicit regularizer}. This motivating example suggests that useful adaptations can be achieved through singular value modulation and inspires us to design a mathematically-ground method to achieve this goal.

\begin{figure}[t]
    \centering
\includegraphics[width=0.9\linewidth]{figs/motivation.pdf}
    \caption{Image recovery with LoRA vs \ourmethod. The preservation of singular vectors in \ourmethod{} offers an improved recovery with a higher (better) peak signal-to-noise ratio (PSNR).}
    \label{fig:motivation}
\end{figure}
Fine-tuning of pre-trained ViTs for downstream tasks, such as few-shot learning~\citep{Hu_2022_CVPR,Park_2024_CVPR}, typically involves adapting the learned representations to the target application. However, the substantial size of these models calls for a resource-efficient fine-tuning technique. Parameter-efficient and low-rank optimization approaches, such as LoRA~\citep{hu2022lora}, offer promising solutions by reducing the number of trainable parameters. This strategy not only enhances resource efficiency (e.g., lower VRAM and FLOPS requirements), 
but also helps keep valuable pre-trained filters that capture essential visual knowledge~\citep{burns2023makespretrainedvisualrepresentations} by preventing catastrophic forgetting~\citep{luo2023investigatingforgettingpretrainedrepresentations, he2023preservingpretrainedfeatureshelps}.

In the context of single-task fine-tuning, parameter-efficient and low-rank methods have proven highly successful~\citep{hu2022lora, houlsby2019parameterefficienttransferlearningnlp, mahabadi2021parameter, karimi2021compacter}.
However, in {\textit{\textbf{Multi-Task Learning (MTL)}}} settings, these methods were shown to struggle \cite{ agiza2024mtlora}. Particularly, MTL typically requires isolating task gradients to prevent task interference (e.g., enforcing gradient orthogonality~\citep{pcgrad,NIPS1995_bdb106a0}), a requirement that becomes increasingly challenging as the dimension or rank of the gradients decreases.
Thus, we face a trade-off between lowering the number of trainable parameters and maintaining task performance. Evidently, as shown in  
\Cref{fig:teaser}, existing methods can offer performance enhancement in MTL settings only when significantly increasing the number of trainable, effectively undermining their purpose of lightweight adaptation. 

We conjecture, inspired by recent studies~\citep{NEURIPS2023_4d69c1c0} that identified an \textit{incremental learning phenomenon} in transformers, that the underperformance of LoRA methods in MTL settings happens because of the direct and non-principled modification of weight matrices' singular vectors. These updates force distinct tasks to compete within the same constrained subspace. The key insight of our approach is realizing that while low-rank representations offer sufficient dimensionality for individual tasks, constraining multiple tasks to share the same low-rank subspace creates interference. An optimal subspace for one task may be suboptimal for others. Yet, existing methods force this competition by maintaining a single shared low-rank space, while a unique subspace for each task, like in MTLoRA~\citep{agiza2024mtlora}, may not take advantage of task synergies.

\noindent\textbf{Our approach.} 
To address the limitations of existing low-rank fine-tuning methods, we introduce \textbf{Diffeomorphic Multi-Task Adaptation} (\textbf{\ourmethod}), a novel approach leveraging neural diffeomorphisms parameterized by \textit{Continuous Piecewise Affine-Based} (CPAB) transformations~\citep{freifeld2015highly, freifeld2017transformations}. Specifically, \ourmethod\ preserves pre-trained feature patterns by retaining the singular vectors of learned weight matrices while dynamically adjusting singular values through neural diffeomorphic transformations at each optimization step. 
Our diffeomorphic transformations are parameterized by Continuous Piecewise Affine (CPA)~\cite{freifeld2015highly,freifeld2017transformations} velocity fields over the weight matrix domain, ensuring continuous, invertible deformations. This design enables adaptation across multiple tasks while preserving the knowledge embedded during pre-training.
We validate \ourmethod\ on the PASCAL MTL dataset~\citep{pascal}, a widely used benchmark for dense prediction tasks such as semantic segmentation, edge detection, and human body part segmentation. Our method achieves a \(26.27\%\) improvement in average task performance while reducing parameter usage by \(4 \times\) compared to state-of-the-art methods like MTLoRA~\citep{agiza2024mtlora}, demonstrating the effectiveness of preserving pre-trained feature directions for efficient multi-task fine-tuning.

\begin{figure*}[!t]
    \centering
    \begin{subfigure}[c]{0.6\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figs/method.pdf}
        \caption{Input feature $\rvx$ is transformed through the weight matrix $\rmW_A$ obtained after modulating the singular values of pre-trained weight $\rmW$ using a neural diffeomorphism $f^{\vtheta}$ to obtain updated features $\rmW_A \rvx$.}
        \label{fig:left}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.3\linewidth}
        \centering
        \vspace{-0.75em}
        \includegraphics[width=0.8\linewidth]{figs/method_2.pdf}
        \caption{We use two sets of \ourmethod\ modules, a joint transformation module to learn task synergies, and a set of task-wise modules to adapt to each task. The outputs are used as multi-task inputs to the decoder.} 
        \label{fig:overview_right}
    \end{subfigure}
    \caption{\textbf{Overview of our \ourmethod\ within Multi-Task Learning (MTL)}. \ourmethod\ leverages learned neural diffeomorphic transformations for both joint and task-wise adaptations. This approach achieves multi-task adaptation, preserving the singular vectors of pre-trained weights while enabling flexible, parameter-efficient fine-tuning.}
    \label{fig:side_by_side}
\end{figure*}

\noindent
\textbf{Our contributions} are as follows:
\begin{itemize}
    \item We propose a novel approach to MTL fine-tuning that leverages neural diffeomorphisms for singular value adaptation, enabling fine-tuning weight updates while preserving pre-trained representation structure.
    
    \item We develop an efficient parametrization using learnable CPA velocity fields that require only 32 additional trainable parameters per layer.

    \item We theoretically analyze the properties of \ourmethod{}, showing that it requires less memory than other low-rank Transformer adaptation methods.
    
    \item \ourmethod{} demonstrates state-of-the-art performance on PASCAL MTL tasks, achieving \(26.27\%\) improvement over existing methods like MTLoRA. 
\end{itemize}
The rest of the paper is organized as follows: In \Cref{sec:relatedwork}, we discuss the related work on parameter-efficient fine-tuning and MTL. In \Cref{sec:background}, we provide the mathematical background to understand \ourmethod{}. \Cref{sec:method} details \ourmethod\, for learning diffeomorphic  singular value adjustment, and \Cref{sec:exp} presents experimental results and analyses. 