\section{Preliminaries and Background}
\label{sec:background}
We combine two key components: the weights in ViTs, and the mathematical framework of CPAB diffeomorphisms. We now provide the background for understanding the coupling of these concepts in \ourmethod.

\subsection{Learned Weights in ViTs}
\label{sec:learned_weights_vits}
\begin{figure}[t]
    \centering
\includegraphics[width=1.0\linewidth]{figs/swin.pdf}
    \caption{\textbf{\ourmethod\ in $i$-th Swin Transformer Stage for Multi-Task Learning.} Task-agnostic modules are applied in all blocks except the last, enabling shared feature learning across tasks. In the last block, Task-Specific modules capture task-dependent features.
    }
    \label{fig:mtlora}
\end{figure}
ViTs encode visual knowledge in their weight matrices, which can be analyzed through Singular Value Decomposition (SVD). Assume we have a weight matrix that transforms inputs from $c_1$  to $c_2$ channels, denoted by $\mathbf{W} \in \mathbb{R}^{c_2 \times c_1}$. Its SVD reads:
\begin{equation}
    \label{eq:svd_weight_matrix}
    \mathbf{W} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top,
\end{equation}
where $\mathbf{U} \in \mathbb{R}^{c_2 \times c_2}, \ \mathbf{\Sigma} \in \mathbb{R}^{c_2 \times c_1}, \ \mathbf{V} \in \mathbb{R}^{c_1 \times c_1}$. 
We refer to $\mathbf{U}$ as the \emph{left} singular vectors, $\mathbf{\Sigma}$ as the singular \emph{values}, and $\mathbf{V}$ as the \emph{right} singular vectors. Notably, $\mathbf{U}$ and $\mathbf{V}$ form orthonormal bases for the output and input spaces of the linear map represented by $\rmW$, respectively. These bases characterize how the network transforms visual features across layers and hidden dimensions. 
\\
\noindent\textbf{Problem Setup.} Throughout this paper, we assume  $K$ tasks $\mathcal{T} = \{T_1, \cdots, T_K\}$. Each task $T_k$ has an associated dataset $\mathcal{D}_k = \{(x_i^k, y_i^k)\}_{i=1}^{N_k}$ where $x_i^k \in \mathcal{X}$ and $y_i^k \in \mathcal{Y}_k$ represent the input-output pairs. The goal is to train a neural network to minimize a supervised loss between the prediction and ground-truth output. Following \cite{agiza2024mtlora}, we use a Swin Transformer~\cite{liu2021swin}, which processes input images by dividing them into non-overlapping patches and progressively merging patches to reduce spatial resolution while increasing feature dimension across stages. 
Formally, the Swin Transformer processes images through $L$ stages, with the $l$-th stage operating on feature maps of resolution $\frac{H}{2^l} \times \frac{W}{2^l}$ with $C_l$ channels. The key operation in each stage is a Window-based Multi-head Self-Attention (W-MSA):
\begin{equation}
    \text{W-MSA}(Q, K, V) = \text{Softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)\,V,
\end{equation}
where $Q, K, V \in \sR^{M \times d}$ are query, key, and value matrices for windows of size $M$. The corresponding weight matrices $\rmW_l^{(q)}, \rmW_l^{(k)}, \rmW_l^{(v)} \in \sR^{d \times d}$ for these projections in layer $l$ are central to our \ourmethod{}, because in Section \ref{sec:method} we dynamically adjust their singular values for multi-task adaptations. The MTL objective minimizes a weighted combination of $K$ task losses:
\begin{equation}
\label{eqn:mtlloss}
    \min_{\Theta} \sum\limits_{k=1}^K \lambda_k \mathcal{L}_k(\Theta; \mathcal{D}_k),
\end{equation}
where $\mathcal{L}_k$ and $\lambda_k$ represent the loss function and weight for the $k$-th task, respectively. In \Cref{fig:mtlora}, we illustrate the overall architecture used in this paper.


\subsection{CPAB Diffeomorphisms for Weight Transformation}
\label{subsec:cpab}
As we show later in Section \ref{sec:method}, our \ourmethod{} modifies singular values $\mathbf{\Sigma}$ while preserving the structure of weight matrices. To achieve that, transformations that are smooth, invertible, and monotone, are required. Continuous Piecewise Affine-Based (CPAB) \cite{freifeld2015highly, freifeld2017transformations} diffeomorphisms satisfy these conditions, while being efficient in terms of parameters and computing diffeomorphisms. We now turn to define CPAB formally. 

\begin{definition}[CPAB Transformation]
    Given a closed interval $\Omega = [a, b]$ partitioned into $\mathcal{N}_{\mathcal{P}}$ intervals, a CPAB transformation is defined through a velocity field $v^\vtheta: \Omega \to \sR$ that is continuous and piecewise-affine within each interval. This velocity field, parameterized by $\vtheta \in \sR^{\gN_{\gP} - 1}$, yields a diffeomorphism $f^{\vtheta}$ of the form:
\begin{equation}
\label{eq:cpab}
    f^{\vtheta}(x) = x + \int_0^1 v^\vtheta(f^{\vtheta}(x, \tau))\, d\tau.
\end{equation}
\end{definition}
\noindent For practical implementation, we use the Python package from \citet{martinez2022closed}, and provide additional details in the supplementary material. 
\noindent
CPAB transformations are \emph{ideal for our settings}, shown in Section \ref{sec:method}, because they provide: (i) \textit{Low parameter overhead.} Only $\gO(\gN_\gP)$ parameters, where typically \(\gN_\gP \leq 128\); (ii) \textit{Differentiability.} Enables gradient-based optimization; (iii) \textit{Computational efficiency.} Computing \Cref{eq:cpab} is of linear complexity; (iv) \textit{Stability.} CPAB transformations are Lipschitz continuous \cite{freifeld2017transformations}.