 \appendix
% \setcounter{page}{1}
% \setcounter{section}{0} % Reset section numbering for the supplementary material
% \renewcommand{\thesection}{S\arabic{section}} % Use 'S' as a prefix for section numbers
% \renewcommand{\thefigure}{S\arabic{figure}}   % Use 'S' as a prefix for figure numbers
% \renewcommand{\thetable}{S\arabic{table}}     % Use 'S' as a prefix for table numbers
\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{\ourmethod\ Adaptation Process}
In this section, we present the pseudocode for our \ourmethod. We repeat this adaptation process in every training iteration for every transformer stage in the encoder, as shown in \Cref{fig:mtlora}.
\begin{quote} % Optional: Use quote to separate the algorithm visually
\noindent\texttt{\underline{\ourmethod($\rmW, \vtheta_j, \{\vtheta_k\}_{k=1}^K, f, \rvx, \{\rvx_k\}_{k=1}^K$)}}
\begin{steps}
    \item Compute the Singular Value Decomposition (SVD) of \( \rmW \):
    \begin{steps}
        \item \( \rmW = \rmU \mathbf{\Sigma} \rmV^\top \), where:
         \( \rmU \in \mathbb{R}^{c_2 \times c_2}, \mathbf{\Sigma} = \text{diag}(\sigma_1, \ldots, \sigma_p), \rmV \in \mathbb{R}^{c_1 \times c_1}. \)
    \end{steps}
    \item[\textit{//}] \textit{Joint Adaptation}
    \item  \( \mathbf{\Sigma}_J = \text{diag}(f^{\vtheta_j}(\sigma_1), \cdots, f^{\vtheta_j}(\sigma_p)). \)
    \item Construct \( \rmW_J = \rmU \mathbf{\Sigma}_J \rmV^\top. \)
    \item $\rvh = \rmW_J \, \rvx$
    \item[\textit{//}] \textit{Task-Specific Adaptation}
    \item For $k = 1, \cdots, K$ do
    \begin{steps}
            \item $\rvx_k = \rvx$ if not last block, else $\rvx_k$
            \item \( \mathbf{\Sigma}_k = \text{diag}(f^{\vtheta_k}(\sigma_1), \cdots, f^{\vtheta_k}(\sigma_p)). \)
            \item Construct \( \rmW_k = \rmU \mathbf{\Sigma}_k \rmV^\top. \)
            \item $\rvh_k = \rmW_k\,\rvx_k$
    \end{steps}
    \item \textbf{Return} $\rvh, \{\rvh_k\}_{k=1}^K$
\end{steps}
\end{quote}


\section{Gradient Analysis}
We analyze the memory requirements for low-rank adaptation methods, such as LoRA, and compare them with \ourmethod\ in terms of gradient storage.

LoRA adapts a pre-trained weight matrix $\rmW \in \sR^{c_2 \times c_1}$ using two learnable low rank matrices $\rmB \in \sR^{c_2 \times r}$ and $\rmA \in \sR^{r \times c_1}$. During backpropagation, gradients need to be stored for both $\rmB, \rmA$, and the input $\rvx$, resulting in a memory requirement that scales with $rc_2(c_2+c_1) + c_1 c_2$. This scaling depends directly on the rank $r$, which can make LoRA memory-intensive when $r$ is large or when the dimensions $c_1$ and $c_2$ are significant.

In contrast, \ourmethod\ leverages the singular value decomposition (SVD) of $\rmW = \rmU \mathbf{\Sigma}\rmV^\top$, where $\mathbf{\Sigma} = \text{diag}(\sigma_1, \cdots, \sigma_p), p=\min(c_1, c_2)$. \ourmethod\ adapts $\rmW$ by learning transformations on the singular values $\mathbf{\Sigma}$ using a small set of parameters $\vtheta$. This reduces the gradient storage requirement to $\gN_\gP + p + c_1 c_2$, where $\gN_\gP$ is the number of intervals over which CPA velocity field is defined. Unlike LoRA, \ourmethod\ avoids gradients tied to low-rank matrices, significantly reducing memory usage for tasks with high-rank requirements or large input dimensions.

By operating directly on the singular values, \ourmethod\ achieves a more memory-efficient adaptation strategy while retaining the ability to make task-specific updates. This efficiency makes it particularly advantageous for large-scale models.


\section{Experimental and Implementation Details}
\label{sec:appendix:hyper}
\noindent\textbf{Training.} We train using AdamW~\citep{loshchilov2019decoupledweightdecayregularization} optimizer with StepLR scheduler for 300 epochs on 8 NVIDIA A6000 GPUs (batch size 64 per GPU).

\noindent\textbf{Code.} Our implementation closely follows the codebase of MTLoRA~\citep{agiza2024mtlora} (MIT License), which we modified for our requirements. We refactored the code to allow distributed training.

\noindent\textbf{Hyperparameters.} The hyperparameters specific to \ourmethod\ are the tessellation size $\gN_\gP$ for each joint and task-wise transformations. In all our experiments, we perform a hyperparameter grid search using Weights \& Biases framework~\cite {wandb}. All our experiments were performed on a single node with 8 NVIDIA A6000 Ada GPUs using distributed data-parallel (DDP) training
\begin{itemize}
    \item \textbf{PASCAL MTL:} We used the tessellation size of CPAB transformations in $\{16, 32, 64, 128\}$, dropout in $\{0.0, 0.05, 0.5\}$, learning rate in $\{0.005, 0.0005, 0.00005\}$, warmup epochs for StepLR in $\{20, 30, 40\}$, weight decay in $\{0.05, 0.005, 0.0005, 0.00005\}$ and a total training epochs of 300 with a batch size of 64 per GPU.
    \item \textbf{NYUD:} We used the tessellation size of CPAB transformations in $\{16, 32, 64, 128\}$, dropout in $\{0.0, 0.05, 0.5\}$, learning rate in $\{0.005, 0.0005\}$, warmup epochs for StepLR in $\{10, 20, 30\}$, weight decay in $\{0.05, 0.005, 0.0005, 0.00005\}$ and a total training epochs of 100 with a batch size of 64 per GPU.
\end{itemize}


\noindent\textbf{Evaluation Metrics.} We follow the evaluation protocol of   MTLoRA \cite{agiza2024mtlora}:
\begin{itemize}
    \item Task-specific metrics: \textsc{mIoU} for segmentation tasks and \textsc{rmse} for surface normals and depth estimation.
    \item Average relative improvement across tasks:
    \begin{equation}
    \Delta m = \frac{1}{K} \sum\limits_{k=1}^K (-1)^{l_k}\,\frac{(M_k - M_{st, k})}{M_{st, k}},
    \end{equation}
    where $M_k$ is the performance on task $k$, $M_{st, k}$ is the single-task baseline. $l_k=1$ for metrics where lower is better, and 0 otherwise.
\end{itemize}

\section{Additional Ablations}
\noindent\textbf{CPAB Parameterization.} 
\revision{The CPAB transformations are parameterized by the number of subintervals $\gN_\gP$ of the domain $\Omega$. From \Cref{fig:ab5}, we observe that a moderate-sized $\gN_\gP = 32$ provides strong and stable performance.}

\begin{figure}[t]
  \centering\includegraphics[width=0.9\linewidth]{figs/Tessellation_Ablation.pdf}
  \caption{Effect of tessellation size using \ourmethod's performance on PASCAL MTL}
  \label{fig:ab5}
\end{figure}

\noindent\textbf{Pre-training Scale.} Models pre-trained on ImageNet-21k mostly outperform their ImageNet-1k counterparts (\Cref{tab:pretrainsize}), suggesting that models pre-trained on larger datasets learn novel representations and, by preserving them, make \ourmethod\ more effective.
\begin{table}[ht]
    \centering
     \caption{MTL Performance using \ourmethod\ on PASCAL for varying pre-training dataset scale}
    \begin{tabular}{lcc}
       \toprule
       \textbf{Task} $\downarrow$ / \textbf{Dataset} $\rightarrow$ & \textbf{ImageNet-1k} & \textbf{ImageNet-21k} \\
       \midrule
       \textsc{SemSeg} & 70.09 & 69.06 \\
       \textsc{Human Parts} & 59.03 & 62.02\\
       \textsc{Saliency} & 64.55 & 65.00\\
       \textsc{Normals} & 17.47 & 17.10\\
       \bottomrule
    \end{tabular}
    \label{tab:pretrainsize}
\end{table}

\revision{\noindent\textbf{VTAB Benchmark.} To understand the single-task generalization capabilities of our method, we compare \ourmethod\ and LoRA~\citep{hu2022lora} using the ViT~\citep{dosovitskiy2021an} backbone on the VTAB Benchmark~\citep{zhai2020largescalestudyrepresentationlearning}. Shown in \Cref{fig:VTAB}, \ourmethod\ achieves competitive performance with an order of magnitude fewer learnable parameters presenting itself as a strong fine-tuning method for vision transformers.}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/vtab.pdf}
    \caption{Pareto optimal curve on VTAB benchmark using \ourmethod\ and LoRA. \ourmethod\ achieves competitive performance using $\sim 10 \times$ fewer parameters.}
    \label{fig:VTAB}
\end{figure}
\revision{\noindent\textbf{Additional Experiments. }}
\revision{To understand the robustness of our design for different backbones, we evaluate \ourmethod\ using the Pyramid Vision Transformer (PVT)~\citep{wang2021pyramid} backbone against various parameter budgets of MTLoRA and LoRA. From \Cref{tab:diff_vit}, we conclude that our \ourmethod  achieves 2.5$\times$ improvement in multi-task performance over best performing baseline using 4.4$\times$ fewer trainable backbone parameters.}
\begin{table}[t]
  \centering
  \scriptsize
   \setlength{\tabcolsep}{2.5pt}
  \caption{
  MTL Performance of selected baselines vs. \ourmethod\ using Pyramid Vision Transformer (PVT) and Swin-Tiny backbones with different parameter budgets.
  }
  \vspace{-5pt}
  \begin{tabular}{l  c  c}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & 
      \multirow{2}{*}{$\Delta m (\%) $} & \textbf{Trainable Backbone} \\
       & &
      \textbf{Parameters} (M)    \\
    \midrule
   PVT +  LoRA ($r=4$)  & -1.35 & 2.41 \\
   Swin-Tiny + LoRA ($r=4$) & -2.17 & 0.93\\
   Swin-Tiny + LoRA ($r=8$) &  +4.93 & 1.31 $(\times 4)$ \\
   \midrule 
    PVT + MTLoRA ($r = 64$) & +1.2 & 8.69 \\    
     Swin-Tiny + MTLoRA ($r = 16 $) & +1.35 & 3.01\\
     Swin-Tiny + MTLoRA ($r = 32 $) & +2.16 & 4.14\\
     Swin-Tiny + MTLoRA ($r = 64 $) & +2.55 & 6.40\\
     \midrule 
       PVT + \ourmethod &  \textbf{+3.01} & 1.96 \\
        Swin-Tiny + \ourmethod &  \textbf{+3.22} & 1.61\\
        (Single Task) Swin-Tiny + \ourmethod & \textbf{+5.33} & 1.61 $(\times 4)$\\
    \bottomrule
  \end{tabular}
  \label{tab:diff_vit}
  \vspace{-12pt}
\end{table}\\

\begin{figure}[!htbp]
  \centering
  \newcommand{\imwidth}{0.12\textwidth}
  \setlength{\tabcolsep}{0.5pt}
  \begin{tabular}{cccc}
    \toprule
    \multicolumn{4}{c}{Semantic Segmentation} \\ 
    \midrule
    \shortstack{Input Image} &
    \shortstack{MTLoRA} & 
    \shortstack{{\ourmethod}} & 
    \shortstack{{Ground Truth}} \\
    \midrule 
    \includegraphics[width=\imwidth]{semseg/2010_001049_gt_mtlora.pdf} & 
    \includegraphics[width=\imwidth]{semseg/2010_001049_semseg_mtlora.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_001049_semseg_ditask.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_001049_mtlora.pdf}\\

    \includegraphics[width=\imwidth]{semseg/2010_000175_gt_mtlora.pdf} & 
    \includegraphics[width=\imwidth]{semseg/2010_000175_semseg_mtlora.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000175_semseg_ditask.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000175_mtlora.pdf}\\

    \includegraphics[width=\imwidth]{semseg/2010_000309_gt_mtlora.pdf} & 
    \includegraphics[width=\imwidth]{semseg/2010_000309_semseg_mtlora.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000309_semseg_ditask.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000309_mtlora.pdf}\\

    \includegraphics[width=\imwidth]{semseg/2010_000139_gt_mtlora.pdf} & 
    \includegraphics[width=\imwidth]{semseg/2010_000139_semseg_mtlora.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000139_semseg_ditask.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000139_mtlora.pdf}\\

    \includegraphics[width=\imwidth]{semseg/2010_000669_gt_mtlora.pdf} & 
    \includegraphics[width=\imwidth]{semseg/2010_000669_semseg_mtlora.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000669_semseg_ditask.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000669_mtlora.pdf}\\

    \includegraphics[width=\imwidth]{semseg/2010_000993_gt_mtlora.pdf} & 
    \includegraphics[width=\imwidth]{semseg/2010_000993_semseg_mtlora.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000993_semseg_ditask.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000993_mtlora.pdf}\\

    \includegraphics[width=\imwidth]{semseg/2010_000097_gt_mtlora.pdf} & 
    \includegraphics[width=\imwidth]{semseg/2010_000097_semseg_mtlora.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000097_semseg_ditask.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000097_mtlora.pdf}\\

    \includegraphics[width=\imwidth]{semseg/2010_000099_gt_mtlora.pdf} & 
    \includegraphics[width=\imwidth]{semseg/2010_000099_semseg_mtlora.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000099_semseg_ditask.pdf} &
    \includegraphics[width=\imwidth]{semseg/2010_000099_mtlora.pdf}\\
    \bottomrule
  \end{tabular}
  \captionof{figure}{Qualitative comparison of semantic segmentation on representative samples from the PASCAL MTL dataset with MTLoRA and our \ourmethod}
  \label{fig:viz3}
\end{figure}
 \vspace{-5pt}
\section{Qualitative Comparison}
\Cref{fig:viz3} shows semantic segmentation results on the PASCAL MTL~\citep{pascal} dataset, comparing MTLoRA and \ourmethod\ against the ground truth. \ourmethod\ consistently produces sharper and more accurate segmentations. In the first row, it captures the structure and boundaries of bicycles more precisely, whereas MTLoRA oversmooths the outputs, failing to recover fine details. Similarly, in the third row, \ourmethod\ segments smaller objects, such as zebras, with greater detail, avoiding omissions observed in MTLoRA. For complex indoor scenes, such as rows five and six, \ourmethod\ distinguishes multiple objects effectively and maintains segmentation coherence, whereas MTLoRA generates fragmented outputs. These results highlight \ourmethod’s ability to adapt pre-trained weights through diffeomorphic transformations, enabling accurate segmentation across diverse object categories.

\Cref{fig:viz2} demonstrates depth estimation results on the NYUD MTL~\citep{silberman2012indoor} dataset, comparing MTLoRA and \ourmethod. \ourmethod\ captures fine-grained depth variations and preserves object boundaries more effectively than MTLoRA. In the second row, \ourmethod\ separates the hallway’s foreground and background accurately, closely matching the ground truth, while MTLoRA produces oversimplified and blurred outputs. In the fourth row, \ourmethod\ preserves depth discontinuities and object structures, where MTLoRA fails to capture these transitions. Even in challenging scenes, such as the last row, \ourmethod\ achieves detailed and consistent depth predictions, outperforming MTLoRA. These results validate the effectiveness of \ourmethod’s singular value transformations in producing precise, task-specific depth estimates.

{\section{Additional Related Work}

\noindent\textbf{\revision{Hard Parameter Sharing and Task Dynamics in Multi-Task Learning. }
} \revision{Hard parameter sharing is a widely used approach in multi-task learning (MTL), where most layers of a neural network are shared among tasks, while task-specific layers are restricted to the output heads. This technique, introduced in \citep{caruana1997multitask} is computationally efficient but presents challenges due to task interference, where conflicting task gradients degrade performance. Approaches like PCGrad~\citep{yu2020gradient} mitigate this by enforcing gradient orthogonality, but they do not always address the full extent of task competition for limited shared parameters. Despite these challenges, task synergies can be harnessed through careful parameter modulation, allowing shared features to benefit related tasks. In this context, methods like our \ourmethod\ enhance positive transfer by preserving crucial pre-trained feature structures, enabling both task-agnostic and task-specific adaptations through diffeomorphic transformations. }
\begin{figure}[t]
\centering
	\newcommand{\imwidth}{0.12\textwidth}
	\setlength{\tabcolsep}{0.2pt}
	\begin{tabular}{ccccccc}
	\toprule
\multicolumn{4}{c}{Depth Estimation} \\ 
\midrule
\shortstack{Input Image} &
\shortstack{MTLoRA} & 
\shortstack{{\ourmethod}} & 
\shortstack{{Ground Truth}} \\
\midrule 
\includegraphics[width=\imwidth]{depths/val_112_gt_ditask.pdf} & \includegraphics[width=\imwidth]{depths/val_112_depth_mtlora.pdf}&
\includegraphics[width=\imwidth]{depths/val_112_depth_ditask.pdf}
& 
\includegraphics[width=\imwidth]{depths/val_112_depth_gt_ditask.pdf}\\
\includegraphics[width=\imwidth]{depths/val_4_gt_ditask.pdf} & \includegraphics[width=\imwidth]{depths/val_4_depth_mtlora.pdf}&
\includegraphics[width=\imwidth]{depths/val_4_depth_ditask.pdf}
& 
\includegraphics[width=\imwidth]{depths/val_4_depth_gt_ditask.pdf}\\
\includegraphics[width=\imwidth]{depths/val_509_gt_ditask.pdf} & \includegraphics[width=\imwidth]{depths/val_509_depth_mtlora.pdf}&
\includegraphics[width=\imwidth]{depths/val_509_depth_ditask.pdf}
& 
\includegraphics[width=\imwidth]{depths/val_509_depth_gt_ditask.pdf}\\
\includegraphics[width=\imwidth]{depths/val_148_gt_ditask.pdf} & \includegraphics[width=\imwidth]{depths/val_148_depth_mtlora.pdf}&
\includegraphics[width=\imwidth]{depths/val_148_depth_ditask.pdf}
& 
\includegraphics[width=\imwidth]{depths/val_148_depth_gt_ditask.pdf}\\
\includegraphics[width=\imwidth]{depths/val_451_gt_ditask.pdf} & \includegraphics[width=\imwidth]{depths/val_451_depth_mtlora.pdf}&
\includegraphics[width=\imwidth]{depths/val_451_depth_ditask.pdf}
& 
\includegraphics[width=\imwidth]{depths/val_451_depth_gt_ditask.pdf}\\
\includegraphics[width=\imwidth]{depths/val_502_gt_ditask.pdf} & \includegraphics[width=\imwidth]{depths/val_502_depth_mtlora.pdf}&
\includegraphics[width=\imwidth]{depths/val_502_depth_ditask.pdf}
& 
\includegraphics[width=\imwidth]{depths/val_502_depth_gt_ditask.pdf}\\
\includegraphics[width=\imwidth]{depths/val_633_gt_ditask.pdf} & \includegraphics[width=\imwidth]{depths/val_633_depth_mtlora.pdf}&
\includegraphics[width=\imwidth]{depths/val_633_depth_ditask.pdf}
& 
\includegraphics[width=\imwidth]{depths/val_633_depth_gt_ditask.pdf}\\

\bottomrule
\end{tabular}
\captionof{figure}{Qualitative comparison of depth estimation on representative samples from the NYUD MTL dataset with MTLoRA and our \ourmethod}
\label{fig:viz2}
\end{figure}

\noindent\textbf{\revision{Paradigms in Multi-Task Learning: Model Design and Optimization Strategies. }}
\revision{Research on MTL can be categorized into two main paradigms: optimization-driven and model design-based strategies. Optimization approaches focus on balancing task-specific loss functions or modifying task gradients to reduce interference, as seen in works like PCGrad~\citep{yu2020gradient}. In contrast, model design-based methods, such as adapters~\citep{he2021towards} and LoRA~\citep{hu2022lora}, introduce parameter-efficient layers that balance shared and task-specific features. However, these methods often restrict updates to low-rank subspaces, limiting adaptability. MTLoRA~\citep{agiza2024mtlora} extends LoRA by incorporating task-specific subspaces, yet still faces trade-offs between task isolation and synergy. Our \ourmethod\ addresses these limitations by preserving full-rank features and enabling dynamic, parameter-efficient adaptations, achieving superior performance on MTL benchmarks.}
