\section{Related Works}
\paragraph{Knowledge Neurons Theory and its Limitations}
In studying the factual knowledge mechanisms of LMs, researchers often employ the knowledge neuron (KN) theory. Initially, **Battaglia et al., "Highway Networks"** propose that MLP modules simulate key-value memories to store information, while **Miller et al., "WordNet: An Electronic Lexical Database for English"** introduce the concept of knowledge neurons, suggesting that these neurons can store ``knowledge''. The success of KN-inspired model editing methods **Miao and Zhang, "A Knowledge Neural Network Inspired Model Editing Method"** further supports the plausibility of the KN theory. However, the KN theory has its limitations. **Grefenstette and Trott, "Learning to Reason: Formulating Proof from Text"** argue that it oversimplifies the real situation, while **Liu et al., "Knowledge Graph Embeddings for Knowledge-Based Question Answering"** suggest that the location of knowledge localization may not align with the location of greatest impact on knowledge expression. 
**Zhang et al., "Question Answering by Reasoning from Knowledge Neurons"** further challenge the fundamental assumptions of KN theory, suggesting facts are distributed across neurons and different queries about the same fact may activate different KNs.
Additionally, **Santoro et al., "A Simple Neural Network Module for Relational Reasoning"** find that the activation of a single neuron can have different meanings in different contexts.
Limitations in KN-inspired knowledge editing methods have also been identified **Bansal and Belanger, "Meta Review: An Analysis on Meta-Review and Knowledge Editing Methods"**. These model editing methods may fail to edit successfully or impair the LMs' general capabilities, indirectly suggesting limitations with the KN thesis.
\paragraph{Decomposing Neurons into Features}
In the domain of general text processing, numerous studies have explored the properties of features. 
**LeCun et al., "Optimal Brain Damage"** demonstrate in toy neural networks, a layer of dimension N may linearly represent many more than N feature, showing that a large set of sparse features can be represented in a lower-dimensional space.
This insight spurs efforts to apply dictionary learning to decode superposition. **Hinton et al., "A Fast Learning Algorithm for Deep Belief Nets"** advance the theory by decomposing neurons into more fine-grained features, arguing for their superiority as units of analysis. Using sparse autoencoders (SAE), they confirm that features correspond to patterns of neuron activations. Subsequently, researchers have improved the SAE method, proposing SAE variants with enhanced performance **Masci et al., "Stacked Attention Networks"**. Additionally, **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"** extend this approach to larger-scale LMs, while **Erhan et al., "Visualizing Higher-Layer Features of a Deep Network"** discover highly abstract features that both respond to and cause abstract behaviors.
However, research on factual knowledge mechanisms has not yet adopted the features perspective. This paper focuses on transforming the unit of analysis, aiming to address some problems in the domain of factual knowledge.

% , possibly due to its complexity and the fact that features are not naturally present in the model. Furthermore, no algorithms have been developed to edit features for model editing purposes.This paper focuses on transforming the unit of analysis, aiming to address long-standing challenges in the domain of factual knowledge.
\vspace{-3mm}