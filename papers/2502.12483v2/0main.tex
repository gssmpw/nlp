
\begin{abstract}
Previous studies primarily utilize MLP neurons as units of analysis for understanding the mechanisms of factual knowledge in Language Models (LMs); however, neurons suffer from polysemanticity, leading to limited knowledge expression and poor interpretability. In this paper, we first conduct preliminary experiments to validate that Sparse Autoencoders (SAE) can effectively decompose neurons into features, which serve as alternative analytical units. With this established, our core findings reveal three key advantages of features over neurons: (1) Features exhibit stronger influence on knowledge expression and superior interpretability. (2) Features demonstrate enhanced monosemanticity, showing distinct activation patterns between related and unrelated facts. (3) Features achieve better privacy protection than neurons,  demonstrated through our proposed FeatureEdit method, which significantly outperforms existing neuron-based approaches in erasing privacy-sensitive information from LMs.\footnote{Code and dataset will be available.}.

\end{abstract}


\section{Introduction}
\label{section:Introduction}
Language Models (LMs) have demonstrated remarkable capabilities in storing and expressing factual knowledge \citep{TheC3, openai2024gpt4technicalreport,gemmateam2024gemma2improvingopen}. However, the underlying mechanisms remains unclear. Mechanistic interpretability of factual knowledge in neural networks aims to decompose these systems into interpretable units to understand how facts are stored and retrieved \cite{chen2024knowledgelocalizationmissionaccomplished}. The critical first step in this investigation is to identify the appropriate analytical units.
One mainstream approach is the neuron-based research method \citep{geva2021key-value,dai2022knowledge,chen2024journey}, which posits that LMs recall facts through multilayer perceptron (MLP) weights and conceptualizes the responsible knowledge storage units as \textit{knowledge neurons}.
Although using neurons as the research unit is highly intuitive, this kind of approach still has some notable limitations \citep{hase2023does,niu2024what,chen2024knowledgelocalizationmissionaccomplished}.



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/introduction/acl-introduction-figure.drawio.pdf}
\caption{Comparison of research units for factual knowledge mechanisms in LMs: (a) neurons and (b) features. Colors in neurons (or features) correspond to the facts they store, illustrating how specific facts are encoded in particular units.}
    \label{fig:introduction_figure1}
\end{figure}
In particular, a significant issue with neuron-based approaches is the phenomenon of polysemanticity \citep{bricken2023monosemanticity,Cunningham2023SparseAF}, where neurons  respond to mixtures of seemingly unrelated  facts. Intuitively, the number of factual knowledge items stored in LMs often exceeds the number of neurons\footnote{For example, Gemma-2 2B has $26\times9216 \approx 230k$ neurons but is trained on approximately 2 trillion tokens of data.}, necessitating that a single neuron must be associated with multiple facts.
As shown in Figure \ref{fig:introduction_figure1}(a), a fact may be dispersedly stored in a fragmented manner across numerous neurons, resulting in inseparable sets of neurons corresponding to $Fact_1$ and $Fact_2$. This fundamental characteristic leads to two challenges.

(1) \textbf{Limited Knowledge Expression:} 
Since factual knowledge can be dispersedly stored across numerous neurons, with some neurons potentially contributing only minimal information, these weak and distributed signals result in identified neurons having limited impact on knowledge expression.
(2) \textbf{Poor Interpretability:} The coupling of neurons representing different facts, makes it difficult to accurately describe the function of individual neurons, hindering our ability to gain deep insights into the mechanisms of factual knowledge.

\citet{bricken2023monosemanticity} suggests that polysemanticity arises from superposition, where neural networks represent more independent features through linear combinations of neurons. 
Their work demonstrates that Sparse Autoencoders (SAE) can effectively decompose neurons into interpretable features. 
As shown in Figure \ref{fig:introduction_figure1}, SAE transforms a ``low-dimensional'' neuronal space (a) into a ``high-dimensional'' feature space (b), making previously inseparable problems separable. This motivates us to explore whether such transformation could benefit the understanding of factual knowledge, where the transformed feature-level units might exhibit both stronger impact on knowledge expression and superior interpretability compared to their neuron-level counterparts.

Building on this foundation, we first investigate a preliminary question: Can neurons be effectively decomposed into features in the domain of factual knowledge, and is SAE a suitable technique for this decomposition? Then, we further explore three core research questions:

\textbf{Q1}: Can feature-based research methods address the dual challenges of Limited Knowledge Expression and Poor Interpretability?
\textbf{Q2}: Given that the key limitation of neurons lies in their polysemanticity, do features in the factual domain exhibit better monosemanticity?
\textbf{Q3}: Do features outperform neurons in downstream tasks?


Our investigation into these questions yields one preliminary finding and three core findings:

\textbf{Preliminary Finding} (\textbf{\textsection\ref{section:preliminary}}):  \textbf{SAE demonstrates superior effectiveness in decomposing neurons into features compared to other methods}, making them suitable alternative research units for studying factual knowledge mechanisms.

\textbf{Core Findings:}
(1) \textbf{Features as research units address the challenges of Limited Knowledge Expression and Poor Interpretability (\textsection\ref{subsection:Post-MLP Features Have the Strongest Impact on Knowledge Expression} and \textsection\ref{subsection:Features Demonstrate Superior Interpretability Compared to Neurons})}.  Through comparison of different modules (post-attention residuals, MLP activations, and post-MLP residuals), we find that features consistently show better interpretability than neurons, with post-MLP residual features having the greatest impact on knowledge expression.

(2) \textbf{Features exhibit stronger monosemanticity than neurons (\textsection\ref{subsection: Features Exhibit Stronger Monosemanticity}}).
Features strongly activate only when encountering related facts and remain inactive for unrelated ones, resulting in distinct separation in their activation distributions. In contrast, neurons lack such separation, indicating their susceptibility to activation by unrelated facts and weaker monosemanticity, while features better align with the ideal scenario in Figure \ref{fig:introduction_figure1}(b).

These two findings complement each other: the superior interpretability of features  naturally arises from their stronger monosemanticity property.

(3) \textbf{Feature-based method demonstrates superior performance in knowledge erasure for privacy protection (\textbf{\textsection\ref{section:Privacy Protection}})}. 
We propose \textbf{FeatureEdit}, the first feature-based model editing method, and evaluate it on our newly constructed privacy knowledge dataset \textbf{PrivacyParaRel}. Compared to neuron-based approaches, FeatureEdit achieves higher success rates in erasing privacy-sensitive knowledge from LMs while maintaining better generalization across semantically equivalent rephrase queries. Moreover, it causes less damage to the model's general capabilities.




\section{Dataset, Models and Evaluation metrics}
\label{section:Dataset, Models and Evaluation metrics}
Our experiments leverage Gemma Scope \citep{lieberum-etal-2024-gemma}, a comprehensive suite of SAEs trained on Gemma-2 models \citep{gemmateam2024gemma2improvingopen}. We study the 2B and 9B variants as they have SAEs for all layers.  Regarding the dataset, consistent with other neuron-based methods \citep{dai2022knowledge, chen2024journey}, we employ the ParaRel dataset \citep{elazar2021measuring-dataset}. For details to the dataset, see Table \ref{appendix:tab:relation_examples} in Appendix \ref{section:appendix-dataset}.

We introduce two evaluation metrics most frequently used in this paper. (1) $\Delta Prob$, the decreasing value of answer probability after features/neurons ablation, which assesses the impact of knowledge storage units on knowledge expression. For neurons, we directly set their activations to zero. For features, we aim to perform a similar operation. However, since features do not exist explicitly in LMs, we propose a reconstruction-based method (detailed in Appendix \ref{subsection:appendix:Feature Ablation Proces}). Briefly, given an activation ($\mathbf{h}$), we obtain its corresponding features, set target features to zero, reconstruct the activation ($\mathbf{h}'$), and replace $\mathbf{h}$ with $\mathbf{h}'$. We then measure the change in the probability of the correct answer before ($Prob_b$) and after ($Prob_a$) ablation: $\Delta Prob = \frac{Prob_b - Prob_a}{Prob_b}$.

(2) $IS$, Interpretability Score, which measures the interpretability of features. We  modify the method from \citet{bills2023language} to adapt it to our task (detailed in Appendix \ref{subsection:appendix:Autointerpretation Protocol}). Briefly, for features or neurons, we ask Large LMs (this paper uses gpt-4o-mini) to predict their activations. The correlation between the model's predicted activations and the actual activations is the interpretability score.



\section{Preliminary Experiment}
\label{section:preliminary}
\subsection{SAE Shows Superior Performance}
We first address the preliminary question raised in \textsection\ref{section:Introduction}: whether neurons can be decomposed into features when studying the mechanism of factual knowledge, and which method performs best. The candidate methods include: Sparse Autoencoders (SAE), Principal Component Analysis (PCA), Independent Component Analysis (ICA), and random directions (RD).
The hyperparameters and methodological details are provided in Appendix \ref{appendix:Details of SAE, PCA and ICA}. 
Both PCA and ICA perform the decomposition using the same amount of data used for training SAEs. 
\paragraph{Experiment settings}
The preliminary experiments focus on decomposing MLP activations to obtain features. We use pre-trained SAEs from Gemma Scope \cite{lieberum-etal-2024-gemma}. Given MLP activations $\mathbf{h}$ at layer $l$, features are obtained through the encoder function:
\begin{equation}
    \mathbf{f}(\mathbf{h}) := \sigma(\mathbf{W}_{enc}\mathbf{h} + \mathbf{b}_{enc})
\end{equation}
where $:=$ denotes definition, $\sigma$ is the JumpReLU activation, $\mathbf{W}_{enc}$ is the encoder weight matrix, and $\mathbf{b}_{enc}$ is the encoder bias vector. Each element $f_{l,p}(\mathbf{h})$ represents the activation of the feature at layer $l$ and position $p$.

Structurally, these features in SAEs parallel the role of intermediate neurons in LLMs, as SAEs are trained to reconstruct LLMs in a higher-dimensional space. Taking one MLP layer as an example, both architectures follow a similar encoder-intermediate-decoder pattern:
\begin{align}
\label{eq:architecture_comparison}
    \text{LLMs}: & \mathbf{x} \xrightarrow{\text{encoder}} \text{intermediate neurons} \xrightarrow{\text{decoder}} \mathbf{y} \\
    \text{SAEs}: & \mathbf{h} \xrightarrow{\text{encoder}} \text{SAE  features} \xrightarrow{\text{decoder}} \mathbf{h}'
\end{align}
For a given input, we select highly activated features ($\mathbf{F_{a}}$) based on their spatial locations:
\begin{equation}
\label{equation:threshold1}
\mathbf{F_{a}} = \{(l,p) \mid f_{l,p}(\mathbf{h}) > \tau_1 \cdot \max_{l',p'} f_{l',p'}(\mathbf{h})\}
\end{equation}
where $\tau_1$ is the threshold parameter. Using the metrics ($\Delta Prob$ and $IS$) defined in \textsection\ref{section:Dataset, Models and Evaluation metrics}, we compare our SAE-based approach with baseline methods. The results are presented in Figure \ref{fig:Preliminary: Feature Acquisition:compare features}.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/preliminary/PreliminaryFeatureAcquisition_mixed.pdf}
\caption{Evaluation of features obtained by different methods. Top: $\Delta$ Prob after feature ablation. Bottom: Interpretation scores ($IS$). Higher values indicate better performance in both metrics.}
    \label{fig:Preliminary: Feature Acquisition:compare features}
\end{figure}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/preliminary/2B_feature_distribution_wo_title.pdf}
\caption{Distribution plots of activated features under different feature number settings ($n\times 9216, n=1,2,4,8$) for Gemma-2 2B. 
The similar distribution patterns across different $n$ suggest that features consistently fall into similar regions. It should be noted that these four pictures are not exactly the same, but they are very similar.}
    \label{fig:Preliminary: Feature splitting}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/featureVSneurons/top2bottom_ablate_features_VS_neurons_both_models.pdf}
\caption{The impact on $\Delta Prob$ when ablating features from different transformer components and neurons. Values show mean $\pm$ standard error across 5 bootstrap iterations, with higher values indicating greater influence on knowledge expression. Note that while $\Delta Prob \in [0,1]$, the plots may exceed 1 due to $+$ std.}
\label{fig:top2bottom_ablate_features_VS_neurons_both_models}
\end{figure*}


\paragraph{Findings}
\textbf{SAE demonstrates superior effectiveness in decomposing neurons into features compared to other methods.} For example, as shown in Figure \ref{fig:Preliminary: Feature Acquisition:compare features}, in Gemma-2 9b, SAE features demonstrate superior performance in both $\Delta Prob$ and $IS$, achieving $\Delta Prob$ of $\sim0.78$ and $IS$ of $\sim0.64$, showing increases of $\sim1.3\times$ in $\Delta Prob$ and $\sim2\times$ in $IS$ over the strongest baseline (ICA). Paired t-tests also confirm that SAE features are more effective for studying factual knowledge mechanisms (see Table \ref{table:appendix:statistical_tests:preliminary} in Appendix \ref{section:appendix:Paired T-test Results for Preliminary Experiment}).

\subsection{Feature Distribution Patterns Remain Consistent Across Feature Numbers}
When extracting features using SAE, we need to determine how many features to use for reconstructing LLM representations. As shown in Equation \ref{eq:architecture_comparison}, while LLMs use a fixed number of neurons, SAEs map these representations to a higher-dimensional feature space. The number of features ($N$) determines this dimensionality, typically set as $N = n \times d_{model}$ where $n$ is a multiplier and $d_{model}$ is the model's hidden dimension ($9216$ for Gemma-2 2B). Adjusting $N$ requires resource-intensive retraining. Notably, we observe consistent feature clustering patterns across different values of $N$.

In Figure \ref{fig:Preliminary: Feature splitting}, we visualize feature distributions across four different settings ($N = n \times 9216$) using Gemma-2 2B. The vertical axis represents layers, while the horizontal axis shows positions. The color intensity indicates the feature density in each bin, with darker blue representing higher values.  Using 500 randomly sampled facts\footnote{This sampling is necessary as visualizing features from the entire dataset would result in near-complete coverage of the feature space.}, we observe that \textbf{features consistently cluster in similar regions} when fixing the number of bins. As $N$ increases, these features undergo hierarchical decomposition within their original clusters, with larger $N$ values enabling finer-grained representations while maintaining the same overall distribution structure.

The results for Gemma-2 9B, along with a comprehensive quantitative analysis of the entire dataset presented in Appendix \ref{section:appendix-feature-stability} (Table \ref{appendix: tab:feature-stability} and Figure \ref{fig:appendix: 9B_feature_distribution}), corroborate these findings. One possible explanation could be that SAE features are insensitive to $N$. Based on this hypothesis, we fix $N=4 \times 9216$ for subsequent experiments, eliminating the need to compare different $N$ values in each experiment.

\section{Features vs. Neurons}
\label{section:Features vs. Neurons}
Building upon \textsection\ref{section:preliminary}, this section delves into the mechanism of factual knowledge using features obtained through sparse autoencoders (SAE). Our findings address the research questions Q1 (\textsection\ref{subsection:Post-MLP Features Have the Strongest Impact on Knowledge Expression} and \textsection\ref{subsection:Features Demonstrate Superior Interpretability Compared to Neurons}) and Q2 (\textsection\ref{subsection: Features Exhibit Stronger Monosemanticity}) raised in \textsection \ref{section:Introduction}.


\subsection{Post-MLP Features Have the Strongest Impact on Knowledge Expression}
\label{subsection:Post-MLP Features Have the Strongest Impact on Knowledge Expression}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/featureVSneurons/01impact_analysis.pdf}
\caption{The impact on $\Delta Prob$ when ablating features from different transformer components and neurons.}
    \label{fig:featureVSneurons_overall_Prob}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/featureVSneurons/top2bottom_features_VS_neurons_IS.pdf}
\caption{Per-unit interpretation scores ($IS$) for features from different transformer components and neurons. We use the same bootstrap samples as Figure \ref{fig:top2bottom_ablate_features_VS_neurons_both_models}. Higher scores indicate better interpretability.}
    \label{fig:top2bottom_features_VS_neurons_IS}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/featureVSneurons/02interpretability_analysis.pdf}
\caption{The average interpretability scores ($IS$) for features from different components and neurons.}
\label{fig:featureVSneurons_overall_IS}
\end{figure}
\paragraph{Experiment Settings}
Following \textsection\ref{section:preliminary}, we extend our analysis to three components in transformer: post-attention residual, MLP activation, and post-MLP residual.
We apply SAE to extract features from each component and compare them with knowledge neurons identified using the localization method proposed by \citet{chen2024journey} (detailed in Appendix \ref{appendix:section:Knowledge Localization Method}), as their approach achieves state-of-the-art performance. We employ the $\Delta Prob$ metric (i.e., the decrease in model prediction probability) from \textsection\ref{section:Dataset, Models and Evaluation metrics} and conduct two complementary analyses to compare how features and neurons impact knowledge expression, with results shown in Figure \ref{fig:featureVSneurons_overall_Prob} and Figure \ref{fig:top2bottom_ablate_features_VS_neurons_both_models}.


(1) Figure \ref{fig:featureVSneurons_overall_Prob}: We select features and neurons through a thresholding method on the full dataset, then ablate them and calculate the $\Delta Prob$. 
For features, the selection follows Equation \ref{equation:threshold1}, and a similar thresholding technique with $\tau_1$ is applied for neurons.

(2) Figure \ref{fig:top2bottom_ablate_features_VS_neurons_both_models}: We perform a fine-grained analysis by ranking features based on their activations in descending order and progressively ablating them, calculating $\Delta Prob$ at each step. This approach allows us to observe the continuous impact of feature ablation without being constrained by any predetermined threshold. 
Since  progressive feature ablation on the full dataset is computationally intensive, we employ bootstrap sampling with 5 independent iterations (300 instances each, with replacement).

\paragraph{Findings}
\textbf{Post-MLP features have the strongest impact on knowledge expression.}
In Figure \ref{fig:featureVSneurons_overall_Prob}, the post-MLP features show a substantial impact with $\Delta Prob$ of $\sim0.85$, which is approximately $10\%$ higher than the strongest baseline (MLP features) and $\sim1.9\times$ that of neurons. Figure \ref{fig:top2bottom_ablate_features_VS_neurons_both_models} provides more granular evidence, showing that ablating just a few highly-activated features significantly impairs the model's ability to express knowledge. Ablating a single post-MLP feature yields a $\Delta Prob$ of $\sim0.6$, substantially higher than the strongest baseline (MLP features) and $\sim3\times$ that of neurons. The statistical significance test results are in Appendix \ref{subsection:appenidx:Paired T-test Results for prob and is} (Table \ref{appendix: tab:feature-stability}), confirming that the superior knowledge expression capabilities of post-MLP features over other features (or neurons) are significant.



\subsection{Features Demonstrate Superior Interpretability Compared to Neurons}

\label{subsection:Features Demonstrate Superior Interpretability Compared to Neurons}


\paragraph{Experiment Settings}
We employ the interpretability score ($IS$) metric introduced in \textsection\ref{section:Dataset, Models and Evaluation metrics} to evaluate the features and neurons, with results shown in Figure \ref{fig:featureVSneurons_overall_IS} and Figure \ref{fig:top2bottom_features_VS_neurons_IS}.

(1) Figure \ref{fig:featureVSneurons_overall_IS}: After obtaining features and neurons through the thresholding technique, we evaluate the interpretability scores ($IS$) of these selected units on the full dataset.

(2) Figure \ref{fig:top2bottom_features_VS_neurons_IS}: We first rank features in descending order based on their activations using the same 1000 sampled facts, then evaluate $IS$ for each feature/neuron individually. Unlike the batch ablation analysis in Figure \ref{fig:top2bottom_ablate_features_VS_neurons_both_models}, this approach assesses one unit at a time.
In this analysis, evaluating units individually holds greater significance beyond merely eliminating the influence of threshold. Since a single fact typically activates a larger number of neurons (can reach 20 or more) compared to features,  averaging $IS$ across neurons would bias the score toward lower values.  Therefore, evaluating each unit individually ensures a more equitable comparison. Note that we only evaluate up to the 50th unit, as the $IS$ approaches or falls below zero near this point, making further evaluation unnecessary.
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/featureVSneurons/features_facts_one_to_one.pdf}
\caption{Post-MLP feature activations corresponding to relation-facts under varying input compositions (0\% to 100\% relation-facts). Top: Activation score distributions. Bottom: Mean activation values.}
    \label{fig:features_facts_one_to_one}
\end{figure*}
\paragraph{Findings}
(1) \textbf{Features demonstrate superior interpretability compared to neurons.} In Figure \ref{fig:featureVSneurons_overall_IS}, post-MLP features achieve $IS$ values of $\sim0.6$, $\sim4\times$ that of neurons. A fine-grained analysis in Figure \ref{fig:top2bottom_features_VS_neurons_IS} further validates this conclusion, showing that even when compared individually, highly activated neurons consistently exhibit lower $IS$ values ($<0.4$) than highly activated features ($\sim0.7$).

(2) {Post-MLP features are a better choice when considering both metrics.} While MLP and post-attention features show comparable interpretability scores ($IS$ $\sim0.6$ and $\sim0.5$ respectively), post-MLP features consistently perform well in both interpretability and knowledge expression. The statistical significance test results are presented in Appendix \ref{subsection:appenidx:Paired T-test Results for prob and is} (Table \ref{appendix: tab:feature-stability}), showing that post-MLP features significantly outperform neurons in interpretability, and perform similarly to other features.

Let's review Q1: \textsection\ref{subsection:Post-MLP Features Have the Strongest Impact on Knowledge Expression} and \textsection\ref{subsection:Features Demonstrate Superior Interpretability Compared to Neurons} demonstrate that \textbf{features, as research units, effectively address the dual challenges of limited knowledge expression and poor interpretability.}

\subsection{Features Exhibit Stronger Monosemanticity}
\label{subsection: Features Exhibit Stronger Monosemanticity}
\paragraph{Motivation}
While our previous analyses demonstrate features' superior performance in both knowledge expression and interpretability, these findings indirectly suggest stronger monosemanticity of features. We now seek direct evidence to verify whether features better align with our desired scenario illustrated in Figure \ref{fig:introduction_figure1}(b). This would further support our findings in \textsection\ref{subsection:Post-MLP Features Have the Strongest Impact on Knowledge Expression} and \textsection\ref{subsection:Features Demonstrate Superior Interpretability Compared to Neurons}\footnote{Based on the analysis in \textsection\ref{subsection:Post-MLP Features Have the Strongest Impact on Knowledge Expression} and \textsection\ref{subsection:Features Demonstrate Superior Interpretability Compared to Neurons}, we focus our comparison specifically on post-MLP features and neurons.}.

\paragraph{Experiment Settings} To evaluate monosemanticity, we expect features corresponding to specific facts to show high activation values for those facts and low activation values for others. However, this evaluation presents two key challenges:

(1) Individual fact analysis is unreliable because each fact activates only a small subset of features. This sparsity means features corresponding to different facts might appear separated by chance, rather than due to true monosemanticity.
(2)  Analyzing all facts simultaneously would involve too many features, likely producing high activation values for some features regardless of input facts. This noise would mask the underlying feature-fact relationships we aim to study.

We address these challenges through a three-step approach:
(1) \textit{Relation Selection}: We select 5 relations (P39, P264, P37, P108, P131; see Appendix \ref{section:appendix-dataset}) and designate these facts as \textit{relation-facts}.

(2) \textit{Input Construction}: We maintain a constant total input size (2,591 facts) while varying the proportion of relation-facts from 0\% to 100\% in 20\% increments. For example, the ``40\%'' configuration contains 1,036 randomly sampled relation-facts and 1,555 non-relation facts.

(3) \textit{Activation Analysis}:
For each configuration, we:
(a) Record activation values from features (or neurons) associated with relation-facts.
(b) Visualize distributions using kernel density estimation (KDE), , where the $x$-axis is activation scores and the $y$-axis shows feature density.
(c) Plot mean activation values for clearer interpretation.


In this setup, stronger monosemanticity is characterized by clear distribution separation across different input configurations and higher activation scores when relation-fact proportions increase. Results are shown in Figure \ref{fig:features_facts_one_to_one}.


\paragraph{Findings}
\textbf{We resolve Q2: features exhibit superior monosemanticity compared to neurons.} 
(1) Features display distinct, well-separated activation waves that correlate with relation-fact proportions. In contrast, neurons show overlapping distributions and activation even without relevant inputs, i.e., $0\%$ condition (Top of Figure \ref{fig:features_facts_one_to_one}).

(2) Feature activation values increase systematically with relation-fact proportion, while neuron mean activation values start notably above zero and show minimal variation, especially between $60\%$ and $100\%$ (Bottom of Figure \ref{fig:features_facts_one_to_one}).

These findings demonstrate that features exhibit stronger fact-specific correspondence by remaining unresponsive to irrelevant inputs. Statistical analysis confirms this separation (all $p < 0.001$, Cohen's d$ > 0.8$; see Appendix \ref{subsection:appenidx:Paired T-test Results for Monosemanticity}).

\section{Feature-based Knowledge Erasure}

\label{section:Privacy Protection}
We evaluate our feature-based method against neuron-based approaches in erasing privacy-related information from LMs. This downstream application addresses Q3 raised in \textsection \ref{section:Introduction}, further validates our previous analysis, and demonstrates the practical value of our findings and analysis.

\subsection{Dataset} 
We construct \textbf{PrivacyParaRel}, a dataset containing synthetic privacy-sensitive information, following the triple format used by \citet{elazar2021measuring-dataset}.  Each entry is structured as \textit{$\langle$subject, relation, object$\rangle$}, such as \textit{$\langle$Alice, Social Security Number, 123-45-6789$\rangle$}, with multiple query variations generated for each fact. This format maintains consistency with factual knowledge datasets, enabling direct method transfer while addressing privacy concerns through synthetic data. We generate 1,500 different facts, each accompanied by six different query variations, resulting in 9,000 total entries. Further details in Appendix \ref{appendix:privacy-dataset}.


\subsection{Experiment Settings}

To erase specific knowledge from LMs, we first perform incremental fine-tuning on our privacy dataset, allowing the model to learn the private information. For erasure, we extract neurons and post-MLP features, then explore two approaches based on neurons and features respectively. In both approaches, we modify weights in the MLP layers to ensure fair comparison, as neuron-based methods operate on MLP weights. Note that this weight modification differs from the activation-zeroing approach used in $\Delta Prob$ calculation.

\paragraph{Neuron-based approach} Following existing neuron-based knowledge editing methods \citep{dai2022knowledge,chen2024knowledgelocalizationmissionaccomplished}, for each identified neuron $n^i_l$ in layer $l$, we set the $i$-th column to zero in $\mathbf{W}^{(2)}_l \in \mathbb{R}^{d_{io} \times d_m}$, where $\mathbf{W}^{(2)}_l$ is the second linear transformation matrix in the $l$-th MLP layer. Here, $d_{io}$ denotes the input/output dimension, and $d_m$ represents the intermediate dimension.


\paragraph{Feature-based approach}
We propose \textbf{FeatureEdit}, a reconstruction-based approach for feature-based model editing. Since features are not naturally exist in LMs, directly using them for model editing poses challenges.  To address this, \textbf{FeatureEdit} presents the first feature-based editing method, inspired by the activation reconstruction method used for $\Delta Prob$ (Appendix \ref{subsection:appendix:Feature Ablation Proces}).
For each identified feature $f^i_l$ in the $l$-th MLP layer, we create a one-hot probe vector ($\mathbf{p}^{i}_j$):
\begin{equation}
   \mathbf{p}^{i}_j = \begin{cases} 
   1 & \text{if} \quad j = i \\
   0 & \text{otherwise}
   \end{cases}
\end{equation}
Let $\mathbf{W}_e \in \mathbb{R}^{d_f \times d_m}$ be the encoding matrix learned by SAE. By reconstructing through its transpose (decoder matrix) $\mathbf{W}_e^T$, we obtain the feature's contribution pattern in the original MLP activation space:
\begin{equation}
   \mathbf{h}^{i} = \mathbf{W}_e^T\mathbf{p}^{i},\quad \mathbf{h}^{i} \in \mathbb{R}^{d_m}
\end{equation}
The reconstructed vector $\mathbf{h}^{i}$ reveals the feature's distributed influence. We traverse all features $f^i_l$, and identify significant positions in the weight matrix $\mathbf{W}^{(2)}_l$ by thresholding:
\begin{equation}
P = \{(l, c, i) \Big| |\mathbf{h}^{i}_c| > \tau_2, \text{ for all } i,l\}
\end{equation}
where $(l, c, i)$ represents the position in layer $l$, the $c$-th column and $i$-th row of $\mathbf{W}^{(2)}_l$, $\mathbf{h}^{(i)}_c$ is the $c$-th value of $\mathbf{h}^{(i)}$, and $\tau_2$ is a hyperparameter (Appendix \ref{subsection:appendix,Method-specific Parameters}). Finally, we zero out these specific weights: 
\begin{equation}
   \mathbf{W}^{(2)}_{l,c} = 0, \quad \forall (l,c,i) \in P
\end{equation}
Notably, this method achieves finer granularity than neuron-based approaches by enabling selective modification of specific weight positions rather than entire column vectors.


\paragraph{Evaluation Metrics} We employ four metrics to assess knowledge erasure performance \citep{yao2024knowledge,chen2024knowledgelocalizationmissionaccomplished}:
(1) Reliability (Rel): The probability that the model fails to correctly answer privacy-related queries after erasure.
(2) Generalization (Gen): The probability that the model fails to answer privacy-related queries with different phrasings. This metric is crucial as high Reliability with low Generalization indicates potential ``jailbreak'' \citep{NEURIPS2023_fd661313} phenomenon where models reveal private data in specific contexts.
(3) Locality (Loc): The probability that the model correctly answers unrelated queries. This metric ensures that knowledge erasure maintains other model capabilities.
(4) Perplexity: Measures the impact on the model's general text generation ability. We use $\Delta$PPL to quantify the perplexity change before (b) and after (a) erasure: $\Delta \text{PPL} = \frac{\text{PPL}_b-\text{PPL}_a}{\text{PPL}_b}$.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/privacy_erase/evaluation_metrics.pdf}
    \caption{Results of knowledge erasure for privacy protection. For the radar chart metrics (Rel, Gen and Loc), higher values indicate better performance, while lower values in the bar chart indicate better performance.}
    \label{fig:privacy_erase}
    \vspace{-3mm}
\end{figure*}
\subsection{Findings}
\textbf{We resolve Q3: Feature-based model editing outperforms neuron-based methods in privacy knowledge erasure}. As shown in Figure \ref{fig:privacy_erase}, features achieve higher Rel scores ($\sim0.8$) compared to neurons ($\sim0.65$), indicating better erasure effectiveness. The substantially higher Gen score for features ($\sim0.7$ vs. $\sim0.25$) demonstrates significant mitigation of the ``jailbreak'' phenomenon. Moreover, features exhibit fewer side effects, evidenced by higher Loc scores ($\sim0.7$ vs. $\sim0.2$ for neurons), indicating minimal impact on other facts, and lower $\Delta$PPL ($\sim0.1$ vs. $\sim0.3$), suggesting better preservation of model generation capabilities. These findings align with our previous conclusions: features exert stronger influence on knowledge expression (higher Rel and Gen) while exhibiting superior monosemanticity, thus requiring fewer editing locations (better Loc and $\Delta$PPL).



\section{Related Works}
\paragraph{Knowledge Neurons Theory and its Limitations}
In studying the factual knowledge mechanisms of LMs, researchers often employ the knowledge neuron (KN) theory. Initially, \citet{geva-etal-2021-transformer} propose that MLP modules simulate key-value memories to store information, while \citet{dai2022knowledge} introduce the concept of knowledge neurons, suggesting that these neurons can store ``knowledge''. The success of KN-inspired model editing methods \citep{meng2022locating, meng2023massediting} further supports the plausibility of the KN theory. However, the KN theory has its limitations. \citet{niu2024what} argue that it oversimplifies the real situation, while \citet{hase2023does} suggest that the location of knowledge localization may not align with the location of greatest impact on knowledge expression. 
\citet{chen2024knowledgelocalizationmissionaccomplished} further challenge the fundamental assumptions of KN theory, suggesting facts are distributed across neurons and different queries about the same fact may activate different KNs.
Additionally, \citet{bricken2023monosemanticity} find that the activation of a single neuron can have different meanings in different contexts.
Limitations in KN-inspired knowledge editing methods have also been identified \citep{li2024unveiling, yao-etal-2023-editing, cohen2024evaluating, hoelscher2023detecting, pinter2023emptying, zhao2023unveiling}. These model editing methods may fail to edit successfully or impair the LMs' general capabilities, indirectly suggesting limitations with the KN thesis.
\paragraph{Decomposing Neurons into Features}
In the domain of general text processing, numerous studies have explored the properties of features. 
\citet{elhage2022superposition} demonstrate in toy neural networks, a layer of dimension N may linearly represent many more than N feature, showing that a large set of sparse features can be represented in a lower-dimensional space.
% This insight spurs efforts to apply dictionary learning to decode superposition.
\citet{bricken2023monosemanticity} advance the theory by decomposing neurons into more fine-grained features, arguing for their superiority as units of analysis. Using sparse autoencoders (SAE), they confirm that features correspond to patterns of neuron activations. Subsequently, researchers have improved the SAE method, proposing SAE variants with enhanced performance \citep{rajamanoharan2024improving,gao2024scaling}. Additionally, \citet{huben2024sparse} extend this approach to larger-scale LMs, while \citet{templeton2024scaling} discover highly abstract features that both respond to and cause abstract behaviors.
However, research on factual knowledge mechanisms has not yet adopted the features perspective. This paper focuses on transforming the unit of analysis, aiming to address some problems in the domain of factual knowledge.

% , possibly due to its complexity and the fact that features are not naturally present in the model. Furthermore, no algorithms have been developed to edit features for model editing purposes.This paper focuses on transforming the unit of analysis, aiming to address long-standing challenges in the domain of factual knowledge.
\vspace{-3mm}
\section{Conclusion}
We investigate the mechanism of factual knowledge in LMs and propose a shift from neuron-based to feature-based analysis. Drawing inspiration from feature extraction methods, we first validate the effectiveness of SAE in extracting features for factual knowledge. Based on this new analysis unit, we make several key findings: Features exhibit greater influence on knowledge expressing than neurons, offer enhanced interpretability, and demonstrate superior monosemanticity. Additionally, our feature-based approach demonstrates better performance in erasing privacy-sensitive knowledge from LMs.

\section{Limitations}
A significant challenge lies in the increased complexity of feature-based methods compared to neuron-based approaches, as features do not constitute natural units of analysis. While neurons are inherent components of LLM architecture, features require additional training of SAE for extraction. Consequently, although we can derive deeper insights using features, translating these insights into model performance improvements presents considerable challenges. For instance, while feature ablation is straightforward, such operations do not modify model parameters. Mapping external features back to model parameters may require an additional mapping layer to establish correspondence between features and weights. Our preliminary approach is based on reconstruction methods, consistent with activation ablation, but this is suboptimal. Therefore, mapping back to weights likely requires further algorithmic innovation.


Additionally, we observe an intriguing clustering phenomenon of SAE features that merits further investigation. While our current analysis confirms that features undergo progressive decomposition with increasing $N$ while maintaining their cluster structure, more fundamental questions remain unexplored. Specifically, we aim to investigate whether the SAE feature space possesses an inherent stable structure and whether it is truly insensitive to $N$. Such investigations could provide deeper insights into the nature of SAE feature representations.