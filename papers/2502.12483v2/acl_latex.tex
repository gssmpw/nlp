% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
 \usepackage{amsmath} 
 \usepackage{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{amssymb}    % For \mathbb and other mathematical symbols
\usepackage{amsfonts}   % For additional mathematical fonts
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{The Knowledge Microscope: \\ Features as Better Analytical Lenses than Neurons}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
\author{Yuheng Chen,
    Pengfei Cao,
    Kang Liu,
    Jun Zhao\\
The Laboratory of Cognition and Decision Intelligence for Complex Systems, \\
Institute of Automation, Chinese Academy of Sciences, Beijing, China\\
School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\\
\texttt{chenyuheng2022@ia.ac.cn, \{pengfei.cao,kliu,jzhao\}@nlpr.ia.ac.cn}}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\input{0main}


\bibliography{custom}

\appendix

\input{1appendix}

\end{document}


Let's still assume that you are GPT2. I now have a relatively complete experimental idea. I am considering conducting the first experiment on GPT2.
Assuming that you are GPT2. I will explain my experimental settings and ask you to simulate the output. There is no need to give the code for the time being. We now assume that we can get relatively good experimental results. At present, we will first review the experimental process completely to see if there are any unreasonable settings or impossible results.
I will explain my experiment and expected results. If the expected results are possible, please simulate them. If not, please point them out.
Thesis topic: Features are better research units than neurons, with stronger interpretability and greater impact on knowledge expression.
Experiment 1: Feature acquisition.
I now have several methods to obtain features. First, the SAE method. Although there are many SAE variants, the core purpose of our research is not to improve the SAE method, so we chose vanilla SAE. If in this case, it is enough to prove the superiority of the features obtained by SAE over other methods, it can further prove our conclusion. Second, PCA method. Third, ICA method. Fourth, randomly selected features.

The evaluation of the quality of features is also evaluated from two aspects: interpretability and expression of knowledge impact.

First, knowledge expression. This is easy to understand.

My dataset is similar to: ["query": "China's capital is", "answer": "Beijing"].

For example, I obtained the features of the post-mlp residual flow, and obtained the corresponding PCA features and ICA features. Then I suppress these features and calculate the change in the probability of the model answer before and after suppression. Expected result: SAE features have the largest change value.

Then I explain the interpretability. From other papers, I borrow here.

We use the automated
approach introduced in Bills et al. (2023) because it scales well to measuring interpretability on the
thousands of dictionary features our autoencoders learn. In summary, the autointerpretability process takes samples of text where the dictionary feature activates, asks a language model to write a human-readable interpretation of the dictionary feature, and then prompts the language model to use this description to predict the dictionary feature’s activation on other samples of text. The correlation between the model’s predicted activations and the actual activations is that feature’s interpretability score. The autointerpretability process consists of five steps and yields both an interpretation and an au tointerpretability score: 1. On each of the first 50,000 lines of OpenWebText, take a 64-token sentence-fragment, and measure the feature’s activation on each token of this fragment. Feature activations are rescaled to integer values between 0 and 10. 2. Take the 20 fragments with the top activation scores and pass 5 of these to GPT-4o, along with the rescaled per-token activations. Instruct GPT-4 to suggest an explanation for when the feature (or neuron) fires, resulting in an interpretation. 3. Use GPT-4o to simulate the feature across another 5 highly activating fragments and 5 randomly selected fragments (with non-zero variation) by asking it to provide the per-token
activations.
4. Compute the correlation of the simulated activations and the actual activations. This correlation is the autointerpretability score of the feature. The texts chosen for scoring a feature
can be random text fragments, fragments chosen for containing a particularly high activationa
tion of that feature, or an even mixture of the two. We use a mixture of the two unless otherwise noted, also called ‘top-random’ scoring.
5. If, amongst the 50,000 fragments, there are fewer than 20 which contain non-zero variation in activation, then the feature is skipped entirely.

On my dataset, I need to make some corrections to its method and propose a new method. Please first give your suggestions for the new method.

Let's say we're evaluating a feature from the SAE applied to GPT-2's post-MLP residual flow. We'll call this Feature X.

Data Selection:
From your dataset of ["query": "China's capital is", "answer": "Beijing"] pairs, we select 20 samples. These would include various countries and capitals.
Feature Activation Measurement:
We run each sample through GPT-2 and measure the activation of Feature X for each sample. Let's say the activations range from 0 to 1.
Top Activation Sample Selection:
We select the top 3 samples where Feature X had the highest activation. For example:
Sample 1: "query": "the capital of China is", "answer": "Beijing" (Activation: 1.0)
Sample 2: "query": "Where is the capital of China", "answer": "Beijing" (Activation: 0.93)
Sample 3: "query": "China's capital is", "answer": "Beijing" (Activation: 0.92)
...
Generate Interpretation:
We provide 3 of these high-activation samples to Claude (or GPT-4), along with their activation values, and ask for an interpretation. For example:
Human: Here are 3 samples where a particular feature in a language model shows high activation. Please provide an interpretation of what this feature might represent:

"the capital of China is Beijing" (Activation: 1.0)
"Where is the capital of China? Beijing" (Activation: 0.93)
"China's capital is Beijing" (Activation: 0.92)

Assistant: Based on these samples, it appears that this feature strongly activates for statements that express "China, capital, Beijing" This feature likely represents the model's encoding of geographical knowledge, specifically the association between nations and their capital cities.

Activation Prediction:
We then ask Claude to predict the activation of this feature for 6 new samples (3 high-activation, 3 random) based on the interpretation. For example:
Human: Based on your interpretation, please predict the activation level (0-1) of this feature for the following samples:

"Germany's capital is Berlin"
"The capital of China is Beijing"
"Australia's capital is Canberra"
"The weather in Tokyo is"
"Brazil's capital is Brasília"
...

Assistant: Based on the interpretation that this feature activates strongly for statements expressing the relationship between a country and its capital city, here are my predictions:

"Germany's capital is Berlin" - Predicted activation: 0.44
"The capital of China is Beijing" - Predicted activation: 0.95
"Australia's capital is Canberra" - Predicted activation: 0.41
"The weather in Tokyo is" - Predicted activation: 0.15
"Brazil's capital is Brasília" - Predicted activation: 0.49
...


Correlation Calculation:
We compare these predictions with the actual activations from GPT-2 for these samples. Let's say the correlation coefficient comes out to 0.85.

Repeat and Average:
We repeat this process for multiple features and calculate the average score. If we did this for 100 features and the average correlation was 0.78, this would be our "Average score" for the SAE method.