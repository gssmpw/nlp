\section{Related Works}
\paragraph{Knowledge Neurons Theory and its Limitations}
In studying the factual knowledge mechanisms of LMs, researchers often employ the knowledge neuron (KN) theory. Initially, \citet{geva-etal-2021-transformer} propose that MLP modules simulate key-value memories to store information, while \citet{dai2022knowledge} introduce the concept of knowledge neurons, suggesting that these neurons can store ``knowledge''. The success of KN-inspired model editing methods \citep{meng2022locating, meng2023massediting} further supports the plausibility of the KN theory. However, the KN theory has its limitations. \citet{niu2024what} argue that it oversimplifies the real situation, while \citet{hase2023does} suggest that the location of knowledge localization may not align with the location of greatest impact on knowledge expression. 
\citet{chen2024knowledgelocalizationmissionaccomplished} further challenge the fundamental assumptions of KN theory, suggesting facts are distributed across neurons and different queries about the same fact may activate different KNs.
Additionally, \citet{bricken2023monosemanticity} find that the activation of a single neuron can have different meanings in different contexts.
Limitations in KN-inspired knowledge editing methods have also been identified \citep{li2024unveiling, yao-etal-2023-editing, cohen2024evaluating, hoelscher2023detecting, pinter2023emptying, zhao2023unveiling}. These model editing methods may fail to edit successfully or impair the LMs' general capabilities, indirectly suggesting limitations with the KN thesis.
\paragraph{Decomposing Neurons into Features}
In the domain of general text processing, numerous studies have explored the properties of features. 
\citet{elhage2022superposition} demonstrate in toy neural networks, a layer of dimension N may linearly represent many more than N feature, showing that a large set of sparse features can be represented in a lower-dimensional space.
% This insight spurs efforts to apply dictionary learning to decode superposition.
\citet{bricken2023monosemanticity} advance the theory by decomposing neurons into more fine-grained features, arguing for their superiority as units of analysis. Using sparse autoencoders (SAE), they confirm that features correspond to patterns of neuron activations. Subsequently, researchers have improved the SAE method, proposing SAE variants with enhanced performance \citep{rajamanoharan2024improving,gao2024scaling}. Additionally, \citet{huben2024sparse} extend this approach to larger-scale LMs, while \citet{templeton2024scaling} discover highly abstract features that both respond to and cause abstract behaviors.
However, research on factual knowledge mechanisms has not yet adopted the features perspective. This paper focuses on transforming the unit of analysis, aiming to address some problems in the domain of factual knowledge.

% , possibly due to its complexity and the fact that features are not naturally present in the model. Furthermore, no algorithms have been developed to edit features for model editing purposes.This paper focuses on transforming the unit of analysis, aiming to address long-standing challenges in the domain of factual knowledge.
\vspace{-3mm}