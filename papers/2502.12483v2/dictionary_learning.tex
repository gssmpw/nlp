\label{section:sparse_autoencoder}
To take network features out of superposition, we employ techniques from \emph{sparse dictionary learning} \citep{olshausen1997sparse, lee2006efficient}. Suppose that each of a given set of vectors $\{\mathbf{x}_i\}_{i=1}^{n_{\text{vec}}} \subset \R^d$ is composed of a sparse linear combination of unknown vectors $\{\mathbf{g}_j\}_{j=1}^{n_{\text{gt}}} \subset \R^d$, i.e. $\mathbf{x}_i = \sum_j a_{i,j}\mathbf{g}_j$ where $\mathbf{a_i}$ is a sparse vector. In our case, the data vectors $\{\mathbf{x}_i\}_{i=1}^{n_{\text{vec}}}$ are internal activations of a language model, such as Pythia-70M \citep{biderman2023pythia}, and $\{\mathbf{g}_j\}_{j=1}^{n_{\text{gt}}}$ are unknown, ground truth network features. We would like learn a dictionary of vectors, called dictionary features, $\{\mathbf{f}_k\}_{k=1}^{n_{\text{feat}}} \subset \R^d$ where for any network feature $\mathbf{g}_j$ there exists a dictionary feature $\mathbf{f}_k$ such that $\mathbf{g}_j \approx \mathbf{f}_k$. 

%Using gradient descent to $\mathbf{x}_i \approx \sum_j c_{i,j}\mathbf{f}_j$
% find a dictionary of vectors, called dictionary features, $\{\mathbf{f}_j\}_{j=1}^{n_{\text{feat}}} \subset \R^d$ such that each of a given set of vectors $\{\mathbf{x}_i\}_{i=1}^{n_{\text{vec}}} \subset \R^d$ can be represented as a sparse linear combination of features $\mathbf{x}_i=\sum_j c_{i,j}\mathbf{f}_j$. It can be shown that linear combinations of ground truth features can be recovered by $\ell^1$ minimisation \citep{wright_ma_2022}. This is done by minimising the $\ell^1$ norm of the coefficients which acts as a proxy for minimising the number of active features while being more computationally tractable \citep{wright_ma_2022}.
%In our case, the data vectors $\{\mathbf{x}_i\} \subset \R^d$ are internal activations of a language model such as Pythia-70M \citep{biderman2023pythia}. Under the `linear with superposition' hypothesis,  we assume that the $\{\mathbf{x}_i\}$ are actually a linear combination of unknown, ground-truth network features, which we try to identify using sparse dictionary learning. 
%would form a sparse dictionary. We therefore expect that sparse dictionary learning will be able to learn a set of features $\{\mathbf{f}_j\}$ similar to the ground-truth features, while being human-understandable.
To learn the dictionary, we train an autoencoder with a sparsity penalty term on its hidden activations. The autoencoder is a neural network with a single hidden layer of size $d_{\text{hid}}=R d_{\text{in}}$, where $d_{\text{in}}$ is the dimension of the language model internal activation vectors\footnote{We mainly study residual streams in Pythia-70M and Pythia 410-M, for which the residual streams are of size $d_{\text{in}}=512$ and $d_{\text{in}}=1024$, respectively \citep{biderman2023pythia}}, and $R$ is a hyperparameter that controls the ratio of the feature dictionary size to the model dimension. We use the ReLU activation function in the hidden layer \citep{fukushima1975relu}. We also use tied weights for our neural network, meaning the weight matrices of the encoder and decoder are transposes of each other.\footnote{We use tied weights because (a) they encode our expectation that the directions which detect and define the feature should be the same or highly similar, (b) they halve the memory cost of the model, and 
(c) they remove ambiguity about whether the learned direction should be interpreted as the encoder or decoder direction. They do not reduce performance when training on residual stream data but we have observed some reductions in performance when using MLP data.}  Thus, on input vector $\mathbf{x} \in \{\mathbf{x}_i\}$, our network produces the output $\mathbf{\hat x}$, given by
\begin{eqnarray}
\mathbf{c} &=& \mathrm{ReLU}(M\mathbf{x}+\mathbf{b}) \label{eq:encoder}\\
\mathbf{\hat x} &=& M^T \mathbf{c} \label{eq:decoder}\\
&=& \sum_{i = 0}^{d_{\text{hid}}-1} c_i \mathbf{f}_i
\end{eqnarray}

where $M \in \R^{d_{\text{hid}} \times d_{\text{in}}}$ and $\mathbf{b} \in \R^{d_{\text{hid}}}$ are our learned parameters, and $M$ is normalised row-wise\footnote{Normalisation of the rows (dictionary features) prevents the model from reducing the sparsity loss term $||\mathbf{c}||_1$ by increasing the size of the feature vectors in $M$.}. Our parameter matrix $M$ is our feature dictionary, consisting of $d_{\text{hid}}$ rows of dictionary features $\mathbf{f}_i$. The output $\mathbf{\hat x}$ is meant to be a reconstruction of the original vector $\mathbf x$, and the hidden layer $\mathbf c$ consists of the coefficients we use in our reconstruction of $\mathbf x$. 


% In this architecture, the parameter matrix $M$ consists of the $d_{\text{hid}}$ feature vectors which we wish to train and interpret, 

% \begin{figure} \label{fig:Architecture}
% \includegraphics[width=0.9\textwidth]{images/Architecture v3.png}
% \caption{Architecture and loss function of our sparse autoencoder}
% \centering
% \end{figure}

Our autoencoder is trained to minimise the loss function

\begin{equation}
\label{eqn:loss_function}
    \mathcal L(\mathbf x)= \underbrace{||\mathbf{x}- \mathbf{\hat x}||_2^2}_{\text{Reconstruction loss}} + \underbrace{\alpha ||\mathbf{c}||_1}_{\text{Sparsity loss}}
\end{equation}

where $\alpha$ is a hyperparameter controlling the sparsity of the reconstruction. The $\ell^1$ loss term on $\mathbf c$ encourages our reconstruction to be a sparse linear combination of the dictionary features. It can be shown empirically \citep{sharkey2023technical} and theoretically \citep{wright2022high} that reconstruction with an $\ell^1$ penalty can recover the ground-truth features that generated the data. For the further details of our training process, see Appendix \ref{appendix:hparam_selection}.

