

\section{Experimental Dataset Introduction}
\label{section:appendix-dataset}
In our experiments, we selected the ParaRel dataset \citet{elazar2021measuring-dataset}, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. We further conducted a basic filtering, excluding 2 relations that had no paraphrases. Table \ref{appendix:tab:relation_examples} displays these relations and corresponding example data.
\begin{table*}[t!]
% \small
\centering
\begin{tabular}{l l l} 
\toprule
\multicolumn{1}{c}{\multirow{3}{*}{{\textbf{Relation}}}} & \multicolumn{2}{c}{{\textbf{Example data}}} \\
\cmidrule(lr){2-3}
& {\textbf{Example Query}} & {\textbf{Answer}} \\ 
\cmidrule(lr){2-2} \cmidrule(lr){3-3}
\cmidrule(lr){1-1}
{P39}   & Adrian IV has the position of & pope \\
{P264}  & Purple Hearts is represented by music label & Sunshine \\
{P37}   & The official language of Republic of Ingushetia is & Russian \\
{P108}  & Henry Swanzy works for & BBC \\
{P131}  & Heaton Park is located in & Manchester \\
{P103}  & The native language of Francis Ponge is & French \\
{P176}  & Fiat Grande Punto is produced by & Fiat \\
{P30}   & Somalia is located in & Africa \\
{P178}  & Gain Ground is developed by & Sega \\
{P138}  & International Day for Biological Diversity is named after & biodiversity \\
{P47}   & Ukraine shares border with & Poland \\
{P17}   & Media Development Authority is located in & Singapore \\
{P413}  & Joe Torre plays in [MASK] position. & catcher \\
{P27}   & Edward Wollstonecraft is [MASK] citizen. & Australia \\
{P463}  & Chuck Schuldiner is a member of & Death \\
{P364}  & The original language of NU.nl is & Dutch \\
{P495}  & The Creepshow was created in & Canada \\
{P449}  & Yes Minister was originally aired on & BBC \\
{P20}   & Margaret Cavendish, Duchess of Newcastle-upon-Tyne died in & England \\
{P1376} & Rumbek is the capital of & Lakes \\
{P1001} & Minister for Foreign Affairs is a legal term in & Australia \\
{P361}  & propellant is part of & cartridge \\
{P36}   & The capital of Flanders is & Brussels \\
{P1303} & Ludovico Einaudi plays & piano \\
{P530}  & Brunei maintains diplomatic relations with & Australia \\
{P19}   & Lopo Soares de Albergaria was born in & Lisbon \\
{P190}  & Bratislava and [MASK] are twin cities. & Dublin \\
{P740}  & Shirehorses was founded in & Manchester \\
{P136}  & Frank Mantooth plays [MASK] music. & jazz \\
{P127}  & AVCHD is owned by & Sony \\
{P1412} & Karl Bodmer used to communicate in & French \\
{P407}  & Zarez was written in & Croatian \\
{P140}  & Leo IX is affiliated with the [MASK] religion. & Christianity \\
{P279}  & quinquina is a subclass of & wine \\
{P276}  & Al-Rifa'i Mosque is located in & Cairo \\
{P159}  & The headquarter of Allied Command Transformation is in & Norfolk \\
{P106}  & Giuseppe Saracco is a [MASK] by profession. & politician \\
{P101}  & Aleksei $N$. Leontiev works in the field of & psychology \\
{P937}  & Joseph Chamberlain used to work in & London \\
\bottomrule
\end{tabular}
\caption{Example data of the ParaRel dataset \citep{elazar2021measuring-dataset}.}
\label{appendix:tab:relation_examples}
\end{table*}

\section{Experimental Hardware Specification and Environment}
All experiments are conducted using a high-performance computing system with an Intel(R) Xeon(R) CPU E5-2680 v4 (2.40GHz, 56 cores) processor and 10 NVIDIA GeForce RTX 3090 GPUs, each equipped with 24576 MiB of memory. The software environment consists of Python 3.10.10 and PyTorch 2.0.0+cu117 for deep learning implementations.

\section{Feature Ablation Process and Autointerpretation Protocol}
\label{section:appendix:Autointerpretation Protocol}

Here we will introduce in detail how we obtain $\Delta Prob$ and $IS$.
\subsection{Feature Ablation Process}

\label{subsection:appendix:Feature Ablation Proces}
Let $\mathbf{h} \in \mathbb{R}^{d_m}$ denote the original component activation (e.g., MLP activation) at a specific layer. Through SAE, we obtain the encoding matrix $\mathbf{W}_e \in \mathbb{R}^{d_f \times d_m}$ and feature vector $\mathbf{f} = \sigma(\mathbf{W}_e\mathbf{h}) \in \mathbb{R}^{d_f}$, where $d_f$ is the number of features and $\sigma$ is the activation function.
The feature ablation process follows these steps:

1. Given a set of target features to ablate $S$, we create a masked feature vector $\mathbf{f}'$:
\begin{equation}
   \mathbf{f}'_i = \begin{cases} 
   0 & \text{if } i \in S \\
   \mathbf{f}_i & \text{otherwise}
   \end{cases}
\end{equation}

2. We reconstruct the activation using the decoder matrix $\mathbf{W}^T_e$:
   \begin{equation}
   \mathbf{h}' = \mathbf{W}^T_e\mathbf{f}' \in \mathbb{R}^{d_m}
\end{equation}

3. Replace the original activation $\mathbf{h}$ with the reconstructed activation $\mathbf{h}'$ in the model's forward computation to obtain the modified probability $Prob_a$.
This process allows us to measure how specific features influence the model's knowledge expression by comparing the original probability $Prob_b$ (using $\mathbf{h}$) with the modified probability $Prob_a$ (using $\mathbf{h}'$) through the $\Delta Prob$ metric.


\subsection{Autointerpretation Protocol}
\label{subsection:appendix:Autointerpretation Protocol}
We adapt the interpretability evaluation method from \citet{bills2023language} for our factual knowledge dataset, which consists of triples in various domains. This method is applied to features extracted by Sparse Autoencoders (SAE) from LLMs' post-MLP residual flow (this paper uses Gemma 2 2B and Gemma 2 9B). The process for each feature is as follows:

1. We select 20 diverse samples from our dataset of factual knowledge triples. Each sample is run through LLMs, measuring the feature's activation (range 0-1).

2. We identify the top 3 samples with highest feature activation. These high-activation samples are provided to a large language model. (we use gpt-4o-mini here\footnote{Any large language model can be used, but it is required that this LLMs can output logprobs.}.)

3. Based on this interpretation, we ask gpt-4o-mini to predict activation levels for 6 new samples: 3 high-activation and 3 random samples from our dataset.

4. We calculate the correlation between these predictions and the actual Gemma 2 2B activations, yielding an interpretability score for the feature.
\section{Details of SAE, PCA, ICA and FeatureEdit}
\label{appendix:Details of SAE, PCA and ICA}

This section details the four methods used for extracting interpretable features: Sparse Autoencoders (SAE), Principal Component Analysis (PCA), Independent Component Analysis (ICA), and random directions. Assume that the input is MLP activation. Other inputs are similar.

\subsection{JumpReLU Sparse Autoencoders (SAEs)}
JumpReLU SAEs are neural networks that learn sparse representations through a threshold-based activation mechanism \citep{lieberum-etal-2024-gemma}. Given MLP activations $\mathbf{h} \in \mathbb{R}^{d_m}$, the encoder and decoder functions are defined by:
\begin{equation}
    \mathbf{f}(\mathbf{h}) := \sigma(W_{enc}\mathbf{h} + \mathbf{b}_{enc})
\end{equation}
\begin{equation}
    \hat{\mathbf{h}}(\mathbf{f}) := W_{dec}\mathbf{f} + \mathbf{b}_{dec}
\end{equation}
where $W_{enc} \in \mathbb{R}^{d_f \times d_m}$, $W_{dec} \in \mathbb{R}^{d_m \times d_f}$, $\mathbf{b}_{enc} \in \mathbb{R}^{d_f}$, $\mathbf{b}_{dec} \in \mathbb{R}^{d_m}$.

The JumpReLU activation $\sigma$ is defined as:
\begin{equation}
    \sigma(\mathbf{z}) = \text{JumpReLU}_{\theta}(\mathbf{z}) := \mathbf{z} \odot H(\mathbf{z} - \theta)
\end{equation}
where $\theta > 0$ is the learnable threshold parameter and $H$ is the Heaviside step function.

The loss function combines reconstruction error with an L0 sparsity penalty:
\begin{equation}
    \mathcal{L} = \|\mathbf{h} - \hat{\mathbf{h}}(\mathbf{f}(\mathbf{h}))\|_2^2 + \lambda\|\mathbf{f}(\mathbf{h})\|_0
\end{equation}
where $\lambda$ controls the sparsity penalty weight.

Features are obtained through:
\begin{equation}
    \mathbf{F} = \{\mathbf{f}(\mathbf{h}) := \sigma(W_{enc}\mathbf{h} + \mathbf{b}_{enc})\}
\end{equation}

Selected features are identified using:
\begin{equation}
\label{identifyFa}
    \mathbf{F_{a}} = \{f \in \mathbf{F} \mid a(f) > \tau_1 \cdot \max_{f \in \mathbf{F}} a(f)\}
\end{equation}
In this equation, $a(f)$ represents the activation value of feature $f_i$, while $\tau_1$ serves as the threshold parameter controlling feature selection sensitivity. The term $\max_{f \in \mathbf{F}} a(f)$ denotes the maximum activation value across all features.

\subsection{Principal Component Analysis (PCA)}
PCA finds orthogonal directions that capture maximum variance in the data. For MLP activations $\mathbf{H} = [\mathbf{h}_1, ..., \mathbf{h}_{d_m}]^T$, the process involves several key steps. First, we center the data by computing $\mathbf{H}_c = \mathbf{H} - \mathbb{E}[\mathbf{H}]$. Next, we compute the covariance matrix $\mathbf{C} = \frac{1}{n}\mathbf{H}_c^T\mathbf{H}_c$. We then perform eigendecomposition $\mathbf{C} = \mathbf{V}\Lambda\mathbf{V}^T$, where $\mathbf{V} = [\mathbf{v}_1, ..., \mathbf{v}_{d_m}]$ contains eigenvectors. Finally, we project the data using $\mathbf{F} = \mathbf{H}_c\mathbf{V}_{d_f}$, where $\mathbf{V}_{d_f}$ contains top $d_f$ eigenvectors.

Features are obtained through:
\begin{equation}
    \mathbf{F} = \{\mathbf{f}(\mathbf{h}) := \mathbf{h}^T\mathbf{V}_{d_f}\}
\end{equation}

Selected features are identified using Equation \ref{identifyFa}.
\subsection{Independent Component Analysis (ICA)}
ICA seeks to find statistically independent components by maximizing non-Gaussianity. The process begins with whitening, where we transform the data to have unit variance in all directions:
\begin{equation}
    \mathbf{H}_w = \mathbf{H}_c\mathbf{V}\Lambda^{-1/2}
\end{equation}
We then find the unmixing matrix $\mathbf{W} \in \mathbb{R}^{d_f \times d_m}$ that maximizes non-Gaussianity:
\begin{equation}
    \mathbf{F} = \mathbf{H}_w\mathbf{W}
\end{equation}
The optimization typically uses approximations of negentropy:
\begin{equation}
    J(\mathbf{w}) = [E\{G(\mathbf{w}^T\mathbf{h}_w)\} - E\{G(\nu)\}]^2
\end{equation}
where $G$ is a non-quadratic function and $\nu$ is a standard Gaussian variable.

Features are obtained through:
\begin{equation}
    \mathbf{F} = \{\mathbf{f}(\mathbf{h}) := \mathbf{h}_w^T\mathbf{W}\}
\end{equation}


Selected features are identified using Equation \ref{identifyFa}.

\subsection{Random Directions (RD)}
Random directions serve as a baseline method through a three-step process. Initially, we generate a random matrix $\mathbf{R} \in \mathbb{R}^{d_m \times d_f}$ with entries drawn from $\mathcal{N}(0, 1/\sqrt{d_m})$. We then apply QR decomposition to obtain an orthonormal basis:
\begin{equation}
    \mathbf{R} = \mathbf{Q}\mathbf{R}_{upper}
\end{equation}
where $\mathbf{Q} \in \mathbb{R}^{d_m \times d_f}$ is an orthonormal matrix and $\mathbf{R}_{upper} \in \mathbb{R}^{d_f \times d_f}$ is an upper triangular matrix. Finally, we project the data using the orthonormal matrix: $\mathbf{F} = \mathbf{H}\mathbf{Q}$.

Features are obtained through:
\begin{equation}
    \mathbf{F} = \{\mathbf{f}(\mathbf{h}) := \mathbf{h}^T\mathbf{Q}\}
\end{equation}


Selected features are identified using Equation \ref{identifyFa}.


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/preliminary/2B_feature_distribution.pdf}
\caption{Feature cluster Results for Gemma 2 2B.}
    \label{fig:appendix: 2B_feature_distribution}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/preliminary/9B_feature_distribution.pdf}
\caption{Feature cluster Results for Gemma 2 9B.}
    \label{fig:appendix: 9B_feature_distribution}
\end{figure*}

\subsection{Method-specific Parameters}
\label{subsection:appendix,Method-specific Parameters}
The implementation of each method involves specific parameter settings. For SAE, we use $\beta = 3$, $\rho = 0.05$, sigmoid activation, and feature dimension $d_f = n \times d_m$, with $n=4$ in this paper. The training process employs the Adam optimizer with learning rate $1e^{-3}$, batch size 256, and runs for 100 epochs. Early stopping is triggered if validation loss does not improve for 10 consecutive epochs. Input activations are standardized to zero mean and unit variance before training.

PCA employs an explained variance ratio threshold of 0.95, which determines the resulting $d_f$ features. The input data is centered but not scaled, as variance information is crucial for principal component identification. ICA utilizes the FastICA algorithm with cubic $G$ function and feature dimension $d_f = 4d_m$, with input data whitened during preprocessing. The Random method uses Gaussian initialization with variance scaling and maintains a feature dimension of $d_f = 4d_m$.

For feature selection across all methods, we employ a threshold $\tau_1 = 0.3$ to identify significant features, striking a balance between feature coverage and selectivity. This threshold was determined through preliminary experiments examining the distribution of feature activations across different knowledge categories. Specifically, $\tau_1 = 0.3$ ensures capture of features that demonstrate substantial activation (at least 30\% of maximum activation) while filtering out noise and weakly activated features.


For FeatureEdit, we set the reconstruction threshold $\tau_2 = 0.1$ to identify significant weight positions. This threshold was chosen based on the empirical observation of weight contribution distributions in the reconstructed activation space, ensuring that we capture meaningful feature influences while maintaining editing precision. The relatively small threshold value allows us to identify subtle but important feature contributions in the distributed representations.

\section{Paired T-test Results for Preliminary Experiment}
\label{section:appendix:Paired T-test Results for Preliminary Experiment}
To rigorously validate the superiority of SAE features over baseline methods, we conduct paired t-tests using full dataset for each method. For both models (Gemma 2 2B and Gemma 2 9B) and both metrics ($\Delta Prob$ and $IS$), we compare SAE with each baseline method (PCA, ICA, and random baseline). The statistical significance of the differences is assessed using paired t-tests, as we compare different methods on the same set of instances. Table \ref{table:appendix:statistical_tests:preliminary} presents the detailed statistical analysis results.

Notably, we include Cohen's d effect size alongside traditional significance testing because p-values alone may not reflect the practical significance of the differences, especially with large sample sizes (all p<0.001). Cohen's d measures the standardized difference between two means, where values above 0.8 indicate large effects. Our results show substantial effect sizes (Cohen's d ranging from 0.38 to 3.81, with most values exceeding 0.8), confirming not only the statistical significance but also the practical importance of SAE's improvements over baseline methods. Particularly strong effects are observed when comparing SAE with the random baseline (Cohen's d > 1.6), and in the $IS$ metric where almost all comparisons show large effect sizes (Cohen's d > 0.7).

\begin{table*}[h]
\centering
\begin{tabular}{llccc}
\toprule
\multicolumn{5}{c}{\textbf{$\Delta Prob$}} \\
\toprule
\textbf{Model} & \textbf{Method} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Cohen's d} \\
\midrule
Gemma 2 2B & SAE vs. PCA & 85.15 & $<0.001$ & 0.70 \\
Gemma 2 2B & SAE vs. ICA & 46.33 & $<0.001$ & 0.38 \\
Gemma 2 2B & SAE vs. Random & 359.48 & $<0.001$ & 2.94 \\
\midrule
Gemma 2 9B & SAE vs. PCA & 120.76 & $<0.001$ & 0.99 \\
Gemma 2 9B & SAE vs. ICA & 77.18 & $<0.001$ & 0.63 \\
Gemma 2 9B & SAE vs. Random & 466.02 & $<0.001$ & 3.81 \\
\midrule
\multicolumn{5}{c}{\textbf{$IS$}} \\
\toprule
\textbf{Model} & \textbf{Method} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Cohen's d} \\
\midrule
Gemma 2 2B & SAE vs. PCA & 157.64 & $<0.001$ & 1.29 \\
Gemma 2 2B & SAE vs. ICA & 87.19 & $<0.001$ & 0.71 \\
Gemma 2 2B & SAE vs. Random & 200.49 & $<0.001$ & 1.64 \\
\midrule
Gemma 2 9B & SAE vs. PCA & 155.98 & $<0.001$ & 1.27 \\
Gemma 2 9B & SAE vs. ICA & 104.44 & $<0.001$ & 0.85 \\
Gemma 2 9B & SAE vs. Random & 255.96 & $<0.001$ & 2.09 \\
\bottomrule
\end{tabular}
\caption{Statistical analysis of feature acquisition methods. We report t-statistics, p-values from paired t-tests, and Cohen's d effect sizes for comparing SAE with baseline methods (PCA, ICA, and random baseline) across both metrics ($\Delta Prob$ and $IS$).}
\label{table:appendix:statistical_tests:preliminary}
\end{table*}



\section{Quantitative Analysis of Feature Stability Across Different $N$ Values}
\label{section:appendix-feature-stability}

To further validate our observation that the impact of the number of features ($N$) is less significant than anticipated, we conduct a comprehensive quantitative analysis on the entire dataset. This analysis aims to support our conclusion that our findings are stable across different values of the hyperparameter $N$.

\subsection{Methodology}

We define $N$ as $N = n \times \text{len(MLP activation)}$, where $n$ is a positive integer. We use $n = 1$ as the baseline for comparison. This approach yields $\text{layer} \times N$ features for each model.

Using Gemma 2 2B as an example, our methodology is as follows:

1. For a given fact, when $n = 1$, we record the positions of activated features as $[\text{layer}, \text{position}]$.

2. For any integer $n > 1$, based on the $n = 1$ case, we expect features to fall within the range $[\text{layer}, \text{position} \times n, (\text{position} + 1) \times n - 1]$.

3. We then compare the actual positions of features for $n > 1$ with these expected positions and calculate the overlap ratio.

4. We repeat this process for the entire dataset and compute the average overlap ratio.

We apply this methodology to both Gemma 2 2B and Gemma 2 9B models, using $n$ values of 1, 2, 4, and 8.

\subsection{Results}
Table \ref{appendix: tab:feature-stability} presents the average overlap ratios for different $n$ values across both models. Additionally, Figure \ref{fig:appendix: 9B_feature_distribution} complements the results shown in Figure \ref{fig:Preliminary: Feature splitting} from the main text. While Figure \ref{fig:Preliminary: Feature splitting} only presents the results for Gemma 2 2B, Figure \ref{fig:appendix: 9B_feature_distribution} displays the results for both Gemma 2 2B and Gemma 2 9B.


The results in Table \ref{appendix: tab:feature-stability} demonstrate a high degree of overlap between the expected and actual feature positions across different $n$ values. For both Gemma 2 2B and Gemma 2 9B, we observe that even as $n$ increases to 8, the overlap ratio remains above 0.87, indicating a strong consistency in feature localization.

\begin{table*}[htbp]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c}{$n$} \\
\cmidrule(l){2-5}
& 1 & 2 & 4 & 8 \\
\midrule
Gemma 2 2B    & 1.000 & 0.927 & 0.891 & 0.893 \\
Gemma 2 9B & 1.000 & 0.935 & 0.908 & 0.879 \\
\bottomrule
\end{tabular}
\caption{Average overlap ratios for different $n$ values.}
\label{appendix: tab:feature-stability}
\end{table*}
This quantitative analysis supports our earlier observation that as $N$ increases, the original features are further decomposed but remain aggregated in consistent regions. The high overlap ratios suggest that our conclusions about feature behavior and importance are indeed stable and relatively insensitive to changes in the hyperparameter $N$.

These findings have important implications for future research in this area, as they suggest that the choice of $N$, within a reasonable range, does not significantly alter the fundamental patterns of feature activation and localization in relation to factual knowledge representation in language models.

\section{Knowledge Localization Method}

\label{appendix:section:Knowledge Localization Method}
We compare the precision of knowledge neuron localization across different research papers and select Architecture-adapted Multilingual Integrated Gradients \citep{chen2024journey} as our baseline method, as it demonstrates superior performance in knowledge neuron localization.

Given a query \(q\), they  define the probability of the correct answer predicted by a PLMs as follows:
\begin{equation}
\label{eq:1}
    \operatorname{F}(\hat{w}^{(l)}_{j}) = p(y^* | q, w^{(l)}_{j}=\hat{w}^{(l)}_{j})
\end{equation}
Here, \(y^*\) represents the correct answer, \(w^{(l)}_{j}\) denotes the \(j\)-th neuron in the \(l\)-th layer, and \(\hat{w}^{(l)}_{j}\) is the specific value assigned to \(w^{(l)}_{j}\). To calculate the attribution score for each neuron, they employ the technique of integrated gradients.
To compute the attribution score of a neuron \(w^{(l)}_{j}\), they consider the following formulation:
\begin{equation}
    \Delta w^{(l)}_{j} = \overline{w}^{(l)}_{j} - {w'}^{(l)}_{j}
\end{equation}
\begin{equation}
    \label{eqution:attribute}
     \operatorname{Attr}({w}^{(l)}_{j}) = \Delta w^{(l)}_{j} \int_{0}^{1} \frac{\partial \operatorname{F}({w'}^{(l)}_{j} + \alpha\Delta w^{(l)}_{j})}{\partial {w}^{(l)}_{j}}  \, d\alpha
\end{equation}Here, \(\overline{w}^{(l)}_{j}\) represents the actual value of \(w^{(l)}_{j}\),
\(w'^{(l)}_{j}\) serves as the baseline vector for \(w^{(l)}_{j}\). The term \(\frac{\partial \operatorname{F}(w'^{(l)}_{j} + \alpha\Delta w^{(l)}_{j})}{\partial w^{(l)}_{j}}\) computes the gradient with respect to \(w^{(l)}_{j}\).  
Next, they aim to obtain ${w'}^{(l)}_{j}$. Starting from the sentence $q$, they acquire a baseline sentence and then encode this sentence as a vector.
Let the baseline sentence corresponding to $q_i$ be $q'_i$, and $q'_i$ consists of $m$ words, maintaining a length consistent with $q$, denoted as $q'_i=(q'_{i1} \ldots q'_{ik} \ldots q'_{im})$. Since they are using auto-regressive models, according to \citet{chen2024journey}, $q'_{ik}=\langle \text{eos}\rangle$, where $\langle \text{eos}\rangle$ represents ``end of sequence'' in auto-regressive models.
The attribution score \(Attr_i(w_j^{(l)})\) for each neuron, given the input \(q_i\), can be determined using Equation \eqref{eqution:attribute}. For the computation of the integral, the Riemann approximation method is employed:
\begin{equation}
    {Attr_i(w_j^l)} \approx \frac{\overline{w}^{(l)}_{j}}{N} \sum_{k=1}^{N} \frac{ \partial F({w'}^{(l)}_{j} + \frac{k}{N} \times \Delta w^{(l)}_{j}}{\partial {w}^{(l)}_{j}}
\end{equation}where $N$ is the number of approximation steps.
Then, the attribution scores for each word \(q_i\) are aggregated and subsequently normalized:
\begin{equation}
    Attr(w_j^l) = \frac{\sum_{i=1}^{m} Attr_i(w_j^l)}{\sum_{j=1}^{n} \sum_{i=1}^{m} Attr_i(w_j^l)}
\end{equation}

Let \( \mathcal{N} \) be the set of neurons classified as knowledge neurons based on their attribution scores exceeding a predetermined threshold \( \tau \), for a given input \( q \). This can be formally defined as:

\begin{equation}
\mathcal{N} = \left\{ w_j^{(l)} \,\middle|\, Attr(w_j^{(l)}) > \tau \right\}
\end{equation}where \(l\) encompassing all layers and \(j\) including all neurons within each layer.

\begin{table*}[h]
\centering
\begin{tabular}{llccc}
\toprule
\multicolumn{5}{c}{\textbf{$\Delta Prob$}} \\
\toprule
\textbf{Model} & \textbf{Method} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Cohen's d} \\
\midrule
Gemma 2 2B & Post-MLP F vs. Post-Att F & 152.29 & $<0.001$ & 1.24 \\
Gemma 2 2B & Post-MLP F vs. MLP F & 56.95 & $<0.001$ & 0.46 \\
Gemma 2 2B & Post-MLP F vs. Neurons & 138.18 & $<0.001$ & 1.13 \\
\midrule
Gemma 2 9B & Post-MLP F vs. Post-Att F & 95.91 & $<0.001$ & 0.78 \\
Gemma 2 9B & Post-MLP F vs. MLP F & 35.27 & $<0.001$ & 0.29 \\
Gemma 2 9B & Post-MLP F vs. Neurons & 146.42 & $<0.001$ & 1.20 \\
\bottomrule
\toprule
\multicolumn{5}{c}{\textbf{$IS$}} \\
\toprule
\textbf{Model} & \textbf{Method} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Cohen's d} \\
\midrule
Gemma 2 2B & Post-MLP F vs. Post-Att F & 28.41 & $<0.001$ & 0.23 \\
Gemma 2 2B & Post-MLP F vs. MLP F & 12.20 & $<0.001$ & 0.10 \\
Gemma 2 2B & Post-MLP F vs. Neurons & 158.37 & $<0.001$ & 1.29 \\
\midrule
Gemma 2 9B & Post-MLP F vs. Post-Att F & 12.55 & $<0.001$ & 0.10 \\
Gemma 2 9B & Post-MLP F vs. MLP F & -12.17 & $<0.001$ & -0.10 \\
Gemma 2 9B & Post-MLP F vs. Neurons & 162.13 & $<0.001$ & 1.32 \\
\bottomrule
\end{tabular}
\caption{Statistical significance test results comparing Post-MLP features with other features or neurons. For each comparison, we report the t-statistic from paired t-tests, corresponding p-value, and Cohen's d effect size.}
\label{appendix:table:statistical_tests:main}
\end{table*}

\section{Paired T-test Results for Main Experiment: Features vs. Neurons}
\subsection{For $\Delta Prob$ and $IS$}
\label{subsection:appenidx:Paired T-test Results for prob and is}
To rigorously validate the comparisons between Post-MLP features and other approaches (Post-Attention features, MLP features, and neurons), we conduct paired t-tests using the full dataset. For both metrics ($\Delta Prob$ and $IS$), we compare Post-MLP features with each alternative method across both models (Gemma 2 2B and Gemma 2 9B). We assess the statistical significance using paired t-tests, as we compare different methods on the same instances.

The results in Table \ref{appendix:table:statistical_tests:main} show varied effect sizes across different comparisons. For $\Delta Prob$, Post-MLP features demonstrate strong advantages over Post-Attention features (Cohen's d: 0.78-1.24) and neurons (Cohen's d > 1.1), while showing more modest advantages over MLP features (Cohen's d: 0.29-0.46). For interpretability ($IS$), we observe particularly strong effects when comparing Post-MLP features with neurons (Cohen's d > 1.2), while comparisons with other feature types show smaller effects ($\lvert\text{Cohen's d}\rvert \leq$ 0.23). All differences are statistically significant (p < 0.001), though the practical significance varies as indicated by the effect sizes.


\subsection{For Monosemanticity}
\label{subsection:appenidx:Paired T-test Results for Monosemanticity}

To rigorously validate the separation phenomenon in activation distributions, we conduct paired t-tests on two types of comparisons: adjacent ratios (e.g., 0\% vs 20\%) and comparisons with the full relation-facts condition (100\%).

The results in Table \ref{appendix: table:monosemanticity_statistics} demonstrate strong and consistent separation patterns, particularly in feature-based representations. For adjacent ratio comparisons, features show large effect sizes (Cohen's d ranging from 0.62 to 5.29) between consecutive ratios, with particularly strong separation in the middle ranges (20\% to 80\%). In contrast, neurons exhibit decreasing effect sizes as the ratio increases, with some comparisons showing small effects (Cohen's d < 0.8) in higher ratios.

When compared against the 100\% baseline, features maintain substantial separation across all ratios (Cohen's d ranging from 1.61 to 13.03), indicating clear distinctions in activation patterns even at high ratios. Neurons, while showing strong separation at lower ratios (Cohen's d > 5.0 for 0\% vs 100\%), demonstrate notably smaller effects at higher ratios (Cohen's d < 1.0 for 80\% vs 100\%). These patterns quantitatively support the superior monosemanticity of features, as they maintain clearer separation between different proportions of relation facts.

\begin{table*}[h]
\centering
\begin{tabular}{llccc}
\toprule
\multicolumn{5}{c}{\textbf{Adjacent Ratio Comparisons}} \\
\toprule
\textbf{Model} & \textbf{Comparison} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Cohen's d} \\
\midrule
Gemma 2 2B Feature & 20 vs. 0 & 27.99 & $<0.001$ & 1.25 \\
Gemma 2 2B Feature & 40 vs. 20 & 118.21 & $<0.001$ & 5.29 \\
Gemma 2 2B Feature & 60 vs. 40 & 57.74 & $<0.001$ & 2.58 \\
Gemma 2 2B Feature & 80 vs. 60 & 67.87 & $<0.001$ & 3.04 \\
Gemma 2 2B Feature & 100 vs. 80 & 35.94 & $<0.001$ & 1.61 \\
\midrule
Gemma 2 2B Neuron & 20 vs. 0 & 50.81 & $<0.001$ & 2.27 \\
Gemma 2 2B Neuron & 40 vs. 20 & 70.10 & $<0.001$ & 3.14 \\
Gemma 2 2B Neuron & 60 vs. 40 & 72.44 & $<0.001$ & 3.24 \\
Gemma 2 2B Neuron & 80 vs. 60 & -2.82 & $0.005$ & -0.13 \\
Gemma 2 2B Neuron & 100 vs. 80 & 12.58 & $<0.001$ & 0.56 \\
\midrule
Gemma 2 9B Feature & 20 vs. 0 & 13.83 & $<0.001$ & 0.62 \\
Gemma 2 9B Feature & 40 vs. 20 & 106.59 & $<0.001$ & 4.77 \\
Gemma 2 9B Feature & 60 vs. 40 & 67.33 & $<0.001$ & 3.01 \\
Gemma 2 9B Feature & 80 vs. 60 & 29.32 & $<0.001$ & 1.31 \\
Gemma 2 9B Feature & 100 vs. 80 & 88.83 & $<0.001$ & 3.97 \\
\midrule
Gemma 2 9B Neuron & 20 vs. 0 & 56.45 & $<0.001$ & 2.53 \\
Gemma 2 9B Neuron & 40 vs. 20 & 29.68 & $<0.001$ & 1.33 \\
Gemma 2 9B Neuron & 60 vs. 40 & 19.10 & $<0.001$ & 0.85 \\
Gemma 2 9B Neuron & 80 vs. 60 & 9.25 & $<0.001$ & 0.41 \\
Gemma 2 9B Neuron & 100 vs. 80 & 17.24 & $<0.001$ & 0.77 \\
\midrule
\bottomrule
\multicolumn{5}{c}{\textbf{Comparisons with 100\% Baseline}} \\
\toprule
\textbf{Model} & \textbf{Comparison} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Cohen's d} \\
\midrule
Gemma 2 2B Feature & 100 vs. 0 & 255.40 & $<0.001$ & 11.43 \\
Gemma 2 2B Feature & 100 vs. 20 & 220.35 & $<0.001$ & 9.86 \\
Gemma 2 2B Feature & 100 vs. 40 & 148.82 & $<0.001$ & 6.66 \\
Gemma 2 2B Feature & 100 vs. 60 & 104.99 & $<0.001$ & 4.70 \\
Gemma 2 2B Feature & 100 vs. 80 & 35.94 & $<0.001$ & 1.61 \\
\midrule
Gemma 2 2B Neuron & 100 vs. 0 & 153.64 & $<0.001$ & 6.87 \\
Gemma 2 2B Neuron & 100 vs. 20 & 112.58 & $<0.001$ & 5.04 \\
Gemma 2 2B Neuron & 100 vs. 40 & 66.82 & $<0.001$ & 2.99 \\
Gemma 2 2B Neuron & 100 vs. 60 & 11.51 & $<0.001$ & 0.51 \\
Gemma 2 2B Neuron & 100 vs. 80 & 12.58 & $<0.001$ & 0.56 \\
\midrule
Gemma 2 9B Feature & 100 vs. 0 & 272.33 & $<0.001$ & 12.18 \\
Gemma 2 9B Feature & 100 vs. 20 & 291.28 & $<0.001$ & 13.03 \\
Gemma 2 9B Feature & 100 vs. 40 & 162.05 & $<0.001$ & 7.25 \\
Gemma 2 9B Feature & 100 vs. 60 & 120.44 & $<0.001$ & 5.39 \\
Gemma 2 9B Feature & 100 vs. 80 & 88.83 & $<0.001$ & 3.97 \\
\midrule
Gemma 2 9B Neuron & 100 vs. 0 & 111.78 & $<0.001$ & 5.00 \\
Gemma 2 9B Neuron & 100 vs. 20 & 74.64 & $<0.001$ & 3.34 \\
Gemma 2 9B Neuron & 100 vs. 40 & 38.81 & $<0.001$ & 1.74 \\
Gemma 2 9B Neuron & 100 vs. 60 & 25.39 & $<0.001$ & 1.14 \\
Gemma 2 9B Neuron & 100 vs. 80 & 17.24 & $<0.001$ & 0.77 \\
\midrule
\bottomrule
\end{tabular}
\caption{Statistical analysis of activation distribution separation. We report t-statistics, p-values, and Cohen's d effect sizes for both adjacent ratio comparisons and comparisons with the 100\% condition. Adjacent ratio comparisons show the separation between consecutive ratios, while baseline comparisons demonstrate the differences from the full relation-facts condition.}
\label{appendix: table:monosemanticity_statistics}
\end{table*}



\section{Synthetic Privacy Dataset Construction and Characteristics}
\label{appendix:privacy-dataset}

Our synthetic privacy dataset comprises 1,500 structured entries of privacy-sensitive information, distributed equally across three relation types: phone numbers (P001), home addresses (P002), and email addresses (P003). Each entry contains a universally unique identifier (UUID), a natural language prompt, the corresponding value, and a relation code. The dataset is specifically designed for privacy-focused machine learning research while ensuring zero risk to individual privacy through complete synthetic generation.

\subsection{Dataset Components}
The dataset construction employs carefully curated component lists to ensure both consistency and variability. We organize our foundational elements into three main categories: identity components, location information, and contact details.

\begin{table*}[t]
\centering
\begin{tabular}{llll}
\toprule
Category & Component Type & Count & Examples \\
\midrule
\multirow{2}{*}{Identity} & First Names & 30 & Alex, Bailey, Casey, Dana, Ellis \\
& Last Names & 30 & Smith, Johnson, Williams, Brown, Jones \\
\midrule
\multirow{3}{*}{Location} & Street Names & 30 & Maple, Oak, Pine, Cedar, Elm \\
& Cities & 30 & Springfield, Rivertown, Lakeside, Hillview \\
& State Codes & 20 & AA, BB, CC, DD, EE \\
\midrule
Contact & Email Domains & 10 & example.com, sample.net, test.org \\
\bottomrule
\end{tabular}
\caption{Dataset generation components.}
\label{tab:components}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{llp{0.3\linewidth}p{0.4\linewidth}}
\toprule
Code & Type & Format Template & Example Query Templates \\
\midrule
P001 & Phone & 555-XXX-XXXX & 
"[name]'s phone number is" \newline
"What is [name]'s phone number?" \newline
"How can I reach [name] by phone?" \\
\midrule
P002 & Address & [Number] [Street] St,\newline [City], [State] [ZIP] & 
"[name]'s home address is" \newline
"Where does [name] live?" \newline
"What is [name]'s residential address?" \\
\midrule
P003 & Email & [firstname].[lastname]\newline[number]@[domain] & 
"[name]'s email address is" \newline
"What's [name]'s email?" \newline
"How can I contact [name] by email?" \\
\bottomrule
\end{tabular}
\caption{Relation types and query templates.}
\label{tab:relation-types}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{llll}
\toprule
Field & Type & Description & Example \\
\midrule
uuid & string & Unique identifier & "550e8400-e29b-41d4-a716-446655440000" \\
sentence & string & Natural language prompt & "Casey Thompson's phone number is" \\
answer & string & Corresponding value & "555-234-5678" \\
relation & string & Relation type code & "P001" \\
\bottomrule
\end{tabular}
\caption{Dataset entry structure.}
\label{tab:entry-structure}
\end{table*}

\subsection{Generation Process}
The dataset generation follows a systematic process to ensure consistency and quality. Names are created by combining first and last names from predefined lists, ensuring unique combinations. Values are generated according to type-specific rules: phone numbers follow the "555-XXX-XXXX" format with random digits, addresses combine random street numbers (1-9999) with component elements, and email addresses merge usernames with random numbers (1-999) and domains.

For each privacy fact, we create six variations of natural language queries, covering both declarative statements and questions. This approach expands our 1,500 unique facts into 9,000 total query-answer pairs. The dataset maintains equal distribution across relation types (500 facts each) and ensures no duplicate entries within each type.

Our quality control process focuses on three key aspects. First, we maintain consistent formatting across all entries to ensure data uniformity. Second, we establish strong referential integrity between names and their associated information to maintain data coherence. Third, we ensure reproducibility through systematic component combination, allowing for dataset regeneration when needed.

\subsection{Research Applications}
This dataset supports various research objectives in privacy-preserving machine learning. It enables thorough model evaluation for information retention and leakage, facilitating the development and evaluation of privacy-protecting mechanisms. The dataset also supports analysis of natural language understanding in the context of structured personal information, while enabling assessment of format learning and consistency in generated content. The synthetic nature of the dataset eliminates privacy concerns while maintaining realistic data patterns and relationships, making it ideal for academic research in privacy-preserving technologies.
