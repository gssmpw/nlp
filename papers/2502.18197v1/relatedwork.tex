\section{Related Work}
Since the introduction of Consistency Models in \citep{song2023consistency, songimproved}, several strategies have been proposed to improve training stability. The work from \citep{geng2024consistency} proposes Easy Consistency Models (ECM) a novel training strategy where time steps are sampled in a continuous fashion and the discretization step is adjusted during training, as opposed to the discrete time grid used in iCT. It further shows the benefits of initializing the network weights with the ones from a pretrained score model, achieving superior performance with smaller training budget. \citep{wang2024stable} builds on top of ECM, introducing additional improvements and framing consistency training as value estimation in Temporal Difference learning \citep{sutton2018reinforcement}. Truncated consistency models, introduced in \citep{lee2024truncated}, proposes to add a second training stage on top of ECM, to allow the model to focus its capacity on the later time steps, resulting in improved few-steps generation performance. Other recent contributions to the consistency model literature are works such as \citep{kimconsistency, heek2024multistep} where the focus is on improving multistep sample quality, \citep{lee2024stabilizing} which trains a model with both consistency and score loss to reduce variance, and \citep{lu2024simplifying}, which proposes several improvements to the continuous-time training of consistency models. Our work can be seen as a parallel contribution to the aforementioned methods, as we focus on learning the data-noise coupling, which can be used as drop-in replacement to the standard independent coupling.

There are several works showing the benefit of using coupling in Flow Matching \citep{pooladian2023multisample, tong2023conditional, liu2023flow, lee2023minimizing, albergostochastic, kim2024simple}. Among these, \citep{lee2023minimizing} shares the most similarities with our method, as they also use an encoder to learn a probability distribution over the noise conditioned on the data. Their method results in improved performance compared to equivalent Flow Matching models, while requiring less function evaluations. Our method consists of a similar procedure but applied to CT, resulting in improved few-steps generation performance and confirming the effectiveness of learning the data-noise coupling. A different coupling strategy for CT is proposed in \citep{issenhuth2024improving}, where the data-noise coupling is extracted directly from the prediction of the consistency model during training. Compared to our method, they do not need the additional encoder to learn the coupling, but their generator-induced coupling needs to be alternated with the standard independent coupling to avoid instabilities. The Flow Matching formulation in Consistency Models with linear interpolation kernel was previously used in \citep{douunified, yang2024consistency}, where the former also explores the use of minibatch OT coupling, while the latter trains the model to learn the velocity field and adds a regularization term to enforce constant velocity. In our work, we use the Flow Matching formulation, but keep most of the CT building blocks, resulting in a simpler formulation with superior performance.