%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{caption}
\usepackage{subcaption}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}



% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{array}
\input{math_commands.tex}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage[normalem]{ulem}

\newcommand{\mean}[2]{\mathbb{E}_{#2} \! \left[ #1 \right]}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\jcc}[1]{{\color{orange}{[\textbf{Jesse:} #1}]}}
\newcommand{\jc}[1]{{\color{orange}{#1}}}
\newcommand{\gl}[1]{{\color{cyan}{[\textbf{GL:} #1}]}}
\newcommand{\yuhta}[1]{{\color{magenta}{#1}}}
\newcommand{\yuhtc}[1]{{\color{magenta}{[\textbf{Yuhta:} #1}]}}

\newcommand{\luca}[1]{{\color{blue}{[\textbf{Luca:} #1}]}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Training Consistency Models with Variational Noise Coupling}

\begin{document}

\twocolumn[
\icmltitle{Training Consistency Models with Variational Noise Coupling}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Gianluigi Silvestri}{oneplanet,donders}
\icmlauthor{Luca Ambrogioni}{donders}
\icmlauthor{Chieh-Hsin Lai}{sony}
\icmlauthor{Yuhta Takida}{sony}
\icmlauthor{Yuki Mitsufuji}{sony,sony2}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{oneplanet}{OnePlanet Research Center, imec-the Netherlands, Wageningen, the Netherlands}
\icmlaffiliation{donders}{Donders Institute for Brain, Cognition and Behaviou, Nijmegen, the Netherlands}
\icmlaffiliation{sony}{Sony AI, Tokyo, Japan}
\icmlaffiliation{sony2}{Sony Group Corporation}

\icmlcorrespondingauthor{Gianluigi Silvestri}{gianluigi.silvestri@imec.nl, gianlu.silvestri@gmail.com}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML, Generative Models, Consistency Models, Variational Inference}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{$^*$Work done during an internship at Sony AI} % otherwise use the standard text.

\begin{abstract}

Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving competitive performance in image generation tasks. However, non-distillation consistency training often suffers from high variance and instability, and analyzing and improving its training dynamics is an active area of research. In this work, we propose a novel CT training approach based on the Flow Matching framework. Our main contribution is a trained noise-coupling scheme inspired by the architecture of Variational Autoencoders (VAE). By training a data-dependent noise emission model implemented as an encoder architecture, our method can indirectly learn the geometry of the noise-to-data mapping, which is instead fixed by the choice of the forward process in classical CT. Empirical results across diverse image datasets show significant generative improvements, with our model outperforming baselines and achieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and attaining FID on par with SoTA on ImageNet at $64 \times 64$ resolution in 2-step generation. Our code is available at \url{https://github.com/sony/vct}.




% Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving state-of-the-art (SoTA) generation performance with one or few sampling steps, without the need for a pre-trained model. However, its training dynamics often suffer from high variance and instability, and it has only been tested on certain diffusion processes. In this work, we propose a novel CT training method based on variational noise coupling, inspired by the Variational Autoencoders framework. By modeling a data-dependent probability distribution over the noise during training, our method significantly improves performance compared to standard CT. Our approach is flexible and applicable to various diffusion kernels under the Flow Matching formulation. Empirical results on image datasets demonstrate that our method stabilizes training and outperforms baseline methods, including standard CT and CT with minibatch Optimal Transport coupling, achieving SoTA FID performance in 2-step generation on CIFAR-10.
\end{abstract}

%\jcc{Let's sort out the main claims first: We want to claim that with trainable variational encoding in consistency training can:
%\begin{enumerate}
%    \item reducing the variance during training $\rightarrow$ improve the generation (reflected by FIDs)
%    \item due to the trainable encoding, we can sort out the coupling between input-noise $\rightarrow$ ???
%\end{enumerate}
%If these two are our main claims, 
%\begin{itemize}
%    \item are they related to each other?
%    \item can we summarize individual (theoretical/empirical) evidences to support them? Are these evidence implicit or explicit?
%    \item trust me, we really need to make a clear differentiations between our method with Minimizing Curvature paper other than performance gain (claims? principled stuffs that our method can achieve but not w/ Minimizing Curvature?). 
%\end{itemize}
%}

%\yuhta{Sorry for this basic question at this time, but I want to be clearer on \textit{technical} differences between ours and Minimizing curvature.}

%\luca{The Minimizing curvature paper is not about consistency models. Of course, they are theoretically (very) related but there is a very substantial difference between consistency and diffusion/flow-matching training. 
%Of course, we need to connect our work with theirs, but I do not see any novelty issue here. Apart from the KL term, the loss we use is very different.

%Concerning the curvature claim, it is difficult to evaluate for us since a consistency models does not easily provide trajectories.
%Their motivation to minimize curvature comes from the fact that they need to use ODE solvers, we do not have that, at least not directly (of course you can argue that a solver is embedded in the loss function).
%}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/toy/1_step_ind.png}
    \includegraphics[width=\linewidth]{figures/toy/1_step_vae.png}
    \caption{Comparison of 1-step generation on toy data for independent and variational coupling. The data is sampled from a 2-d mixture of Gaussians with means $\vect{\mu}_1=(0 , 0.5)$ and $\vect{\mu}_2=(0, -0.5)$. On the bottom plot (our approach), we show the posterior probabilities learned by the encoder (in blue and green) corresponding to $p(\vect{z}\mid \vect{\mu}_1)$ and $p(\vect{z}\mid \vect{\mu}_2)$, and their cumulative sum approximately recovers the prior distribution. The gray lines connect the samples from the trained models to the corresponding input noise. More details about the toy experiments are given in Appendix \ref{app:toy}.}
    \label{fig:toy_samples_1}
\end{figure}
\section{Introduction}
Generative Models are deep learning algorithms designed to learn the underlying probability distribution of a given dataset, in order to then generate samples coming from such a distribution. Some widely used models are Generative Adversarial Networks \citep{goodfellow2014generative}, Variational Autoencoders \citep[VAE;][]{kingma2013auto, rezende2014stochastic}, and Normalizing Flows \citep{chen2018neural, papamakarios2021normalizing, kobyzev2020normalizing}. More recently, Diffusion Models \citep{sohl2015deep, ho2020denoising, songscore} have achieved state-of-the-art (SoTA) results in several domains, including images, videos, and audio \citep{dhariwal2021diffusion, rombach2022high, karras2022elucidating, karras2024analyzing, ho2022video, kongdiffwave}. However, a weakness of diffusion models is the need for an iterative sampling procedure, which can require hundreds of network evaluations. Therefore, substantial effort has been made to develop methods that can maintain similar generation quality while requiring fewer sampling iterations \citep{songdenoising, jolicoeur2021gotta, salimansprogressive, liupseudo, lu2022dpm}. Among such methods, a recent and promising direction is given by Consistency Models (CMs) \citep{song2023consistency}. CMs, while sharing many similarities with DMs, use a different training procedure as they directly learn the probability flow equations rather than the score function. CMs can be either trained by distilling the ODE trajectories of a pre-trained diffusion model (Consistency Distillation, CD), or completely from scratch through a bootstrap loss (Consistency Training, CT), which results in a novel generative modeling framework. However, the CT objective can be subject to high variance, making it difficult to train. A follow-up work \citep{songimproved} analyses the training dynamics of CMs and proposes several improvements which result in a more stable CT procedure, achieving SoTA results in few-step image generation. Since then, several works have proposed additional strategies to further improve CT \citep{geng2024consistency, wang2024stable, lee2024truncated, yang2024consistency}. 

% \jcc{Perhaps here we need to clarify our reasoning more -- is the logic: as in CT, there is not coupling between data and the noise sampled from the prior, so the ODE trajectory may be hard to learn as it increases the "numbers of bundles of trajectories" that the network need to learn? but once we sort out certain coupling between data and noise, then we can focus more on these trajectories without shifting around during training?}

A possible source of instability of CT training comes from the fact that different noise masks are applied to the same data point, creating ambiguity the target corresponding to the given noisy state, especially early during training with coarse discretization steps. From a more mathematical perspective, training can be destabilized by sharp boundaries in the ODE flow mapping, which can be hard to learn for the model and give raise to high variance in the stochastic gradient estimator. For example, the ODE flow for mixture of delta distributions is defined by a tessellation of the initial noise space, with discontinuities along all the borders. Since the standard CT approach with fixed forward process cannot alter the target ODE flow, the optimum of the standard CT training can potentially be highly singular. The existence of these singularities depend on topological reasons (i.e. non-injectivity of the ODE flow mapping at $t \rightarrow 0$ \citep{cornish2020relaxing}). However, the issue can likely be ameliorated by altering the forward process during training, which can be used to change the location of the singularities.

A way to implement this approach is to sample noise using a conditional coupling function between data and noise. The concept of using a coupling function to reduce variance during training was successfully used in Flow Matching, with works such as \citep{pooladian2023multisample, tong2023conditional, lee2023minimizing}, with the main objective of obtaining straighter ODE trajectories for faster sampling. Forms of coupling in CMs were proposed in works such as \citep{douunified, issenhuth2024improving}, but their formulations do not match the performance of standard CMs. In this work, we introduce a variational training of the forward transition kernel with a coupling function between data and noise, which results in a loss function similar to the one used in VAEs. By learning a data-dependent probability distribution over the noise, regularized with an additional Kullback-Leibler divergence loss, we develop an end-to-end training procedure and show how the resulting coupling is effective at improving generation performance and is scalable to high-dimensional data. A simple and intuitive example is shown in Figure \ref{fig:toy_samples_1}, where with the same settings, the model trained with the learned coupling generates samples closer to the data distribution. From this figure, we can see how the learned coupling partitions the data differently from how the standard CT does, effectively changing the form of the underlying ODE, likely resulting in an easier training objective as the prior noise is partitioned according to the learned the data-dependent coupling distribution. 

The reminder of the paper is organized as follows: we first discuss relevant related CT methods and flow-based works employing coupling strategies. We then formulate CT from the Flow Matching perspective, which is a generalization of the diffusion framework and it offers a more natural way to introduce the noise-coupling distribution. Finally, we describe our method, deriving similarities with Variational Autoencoders, and report our experimental results on common image benchmarks.

% \jcc{also, a potential issue of CT is it only been testified with EDM (i.e., VE) kernel? so can we provide a more general perspective of ODE-trajectory learning with CT, and tested on different kernels, including FM, EDM? -- but as this is not super new in the community, so we need to discuss about our claim}

% \gl{let's first see the results on imagenet and then decide how much enphasis we want to put in the FM kernel}

\section{Related Work}
Since the introduction of Consistency Models in \citep{song2023consistency, songimproved}, several strategies have been proposed to improve training stability. The work from \citep{geng2024consistency} proposes Easy Consistency Models (ECM) a novel training strategy where time steps are sampled in a continuous fashion and the discretization step is adjusted during training, as opposed to the discrete time grid used in iCT. It further shows the benefits of initializing the network weights with the ones from a pretrained score model, achieving superior performance with smaller training budget. \citep{wang2024stable} builds on top of ECM, introducing additional improvements and framing consistency training as value estimation in Temporal Difference learning \citep{sutton2018reinforcement}. Truncated consistency models, introduced in \citep{lee2024truncated}, proposes to add a second training stage on top of ECM, to allow the model to focus its capacity on the later time steps, resulting in improved few-steps generation performance. Other recent contributions to the consistency model literature are works such as \citep{kimconsistency, heek2024multistep} where the focus is on improving multistep sample quality, \citep{lee2024stabilizing} which trains a model with both consistency and score loss to reduce variance, and \citep{lu2024simplifying}, which proposes several improvements to the continuous-time training of consistency models. Our work can be seen as a parallel contribution to the aforementioned methods, as we focus on learning the data-noise coupling, which can be used as drop-in replacement to the standard independent coupling.

There are several works showing the benefit of using coupling in Flow Matching \citep{pooladian2023multisample, tong2023conditional, liu2023flow, lee2023minimizing, albergostochastic, kim2024simple}. Among these, \citep{lee2023minimizing} shares the most similarities with our method, as they also use an encoder to learn a probability distribution over the noise conditioned on the data. Their method results in improved performance compared to equivalent Flow Matching models, while requiring less function evaluations. Our method consists of a similar procedure but applied to CT, resulting in improved few-steps generation performance and confirming the effectiveness of learning the data-noise coupling. A different coupling strategy for CT is proposed in \citep{issenhuth2024improving}, where the data-noise coupling is extracted directly from the prediction of the consistency model during training. Compared to our method, they do not need the additional encoder to learn the coupling, but their generator-induced coupling needs to be alternated with the standard independent coupling to avoid instabilities. The Flow Matching formulation in Consistency Models with linear interpolation kernel was previously used in \citep{douunified, yang2024consistency}, where the former also explores the use of minibatch OT coupling, while the latter trains the model to learn the velocity field and adds a regularization term to enforce constant velocity. In our work, we use the Flow Matching formulation, but keep most of the CT building blocks, resulting in a simpler formulation with superior performance.

\section{Background} 

% \jcc{let's discuss what would be a better notations for the flow map: there are $\psi_t(\vx_0)$, $\psi_t(\vx_0, \vx_1)$, and for inverse $\psi_t^{-1}(\vx_0, t)$--which has $t$. same for the $\vu_t$. in FM paper, they use $\psi_t(\vx_0)$ to denote the ODE flow starting from $\vx_0$; while $\psi_t(\vx_0, \vx_1)$ means the interpolant, which cannot/shouldn't be the ODE flow--i guess we somehow mix here}

% LUCA: I do not think we need to introduce the diffusion formalism. We can simply introduce the consistency approach from the flow matching formulation.

%\subsection{Generative diffusion Models}

%Diffusion Models (DMs) \citep{sohl2015deep, ho2020denoising, songscore} define a forward process in which small amount of noise is gradually added to data, following the SDE:
%\begin{align}
%    d\vect{x}_t = \vect{f}(\vect{x}_t,t)dt + g(t)d\vect{w}_t,
%\end{align}
%where $\vect{f}$ and $g$ are respectively the drift and the diffusion coefficients of $\vect{x}(t)$, and $\vect{w}$ is the standard Weiner process. Under this process, as $t\rightarrow\infty$, the data will be converted to gaussian noise. The intermediate states can be expressed as $\vect{x}_t = \alpha_t \vect{x} + \beta_t \vect{x}_1$, where $\vect{x}_1\sim\mathcal{N}(\rmzr, \rmI)$. A result from \citep{anderson1982reverse} is that the process can be reversed to recover the original data distribution starting from noise, and the reverse-time SDE is:
%\begin{align}
%    d\vect{x}_t = [\vect{f}(\vect{x}, t) - g(t)^2\nabla_x\log p_t(\vect{x})]dt + g(t)d\Bar{\vect{w}}.
%\end{align}
%In DMs, a neural network $s_\theta(\vect{x}_t,t)$ is trained to learn the intractable score function $\nabla_x\log p_t(\vect{x})$ using the score matching loss \citep{hyvarinen2005estimation}:
%\begin{align}
%    \vect{\theta}^* = \arg\min_{\theta} \mathbb{E}_t \left\{ \lambda(t) \mathbb{E}_{\mathbf{x}(0)} \mathbb{E}_{\mathbf{x}(t) | \mathbf{x}(0)} \left[ \left\| s_{\theta}(\mathbf{x}(t), t) - \nabla_{\mathbf{x}(t)} \log p_{t}(\mathbf{x}(t) | \mathbf{x}(0)) \right\|_2^2 \right] \right\}
%\end{align} 

%Once trained, the network can be used to approximate the score in the reverse process, which can be solved with standard SDE solvers. The reverse process can be also expressed as an ODE with the Probability Flow ODE formulation \citep{songscore}:
%\begin{align}
%    d\vect{x}_t = \left[\vect{f}(\vect{x}, t) - \frac{1}{2}g(t)^2\nabla_x\log p_t(\vect{x})\right]dt.
%\end{align}
Flow Matching provides a general framework that generalizes diffusion and score-matching models \citep{lipmanflow, albergo2023building}. In the Flow Matching formalism, a deterministic flow function $\vect{\psi}_t$ with initial condition $\rvx_0$ is used to build an interpolating map between two distributions $\vect{\psi}_t(\vect{x}_0) = \vect{x}_t$, such that the data distribution $p_0(\vect{x}_0)$ is mapped into a distribution $p_1(\vect{x}_1)$, commonly chosen to be Gaussian noise distribution $p_1(\vect{x}_1):=\mathcal{N}(\vect{x}_1;\bm{0},\vect{I})$,  by the pushforward operator. From this quantity, we can define the vector field $\vect{u}_t(\vect{x}_t)$ as the infinitesimal generator of $\vect{\psi}_t$:
\begin{align*}
    \frac{\mathrm{d}}{\mathrm{d} t} \vect{\psi}_t(\vect{x}_0) &= \vect{u}_t(\vect{x}_t) = \vect{u}_t(\vect{\psi}_t(\vect{x}_0)) ~, 
\end{align*}
In a diffusion model, the flow $\vect{\psi}_t(\vect{x}_0)$ is the inverse of the probability ODE flow determined by the forward SDE. Instead, in standard Flow Matching, the mapping is specified as a conditional flow $\vect{\psi}_t(\vect{x}_0; \vect{x}_1)$, which is typically taken as a simple linear interpolation between samples from the two densities $p_0(\vect{x}_0)$ and $p_1(\vect{x}_1)$. Some common examples are $\vect{\psi}_t(\vect{x}_0; \vect{x}_1) = \vect{x}_t = (1-t) \vect{x}_0 + t\vect{x}_1$ as seen in \citep{lipmanflow}, and $\vect{\psi}_t(\vect{x}_0; \vect{x}_1) = \vect{x}_t = \vect{x}_0 + t\vect{x}_1$ as in \citep{karras2022elucidating}. This conditional flow is analogous to the formal solution kernel of the forward process at time $t$ in the generative diffusion framework. While it is difficult to directly obtain the flow function $\vect{\psi}_t(\vect{x}_0)$ from the conditional flow $\vect{\psi}_t(\vect{x}_0; \vect{x}_1)$, it is possible to give a formal expression for the resulting vector field:
\begin{align}\label{eq: velocity from conditional}
    \vect{u}_t(\vect{x}_t) =  \mean{\vect{u}_t(\vect{x}_t; \vect{x}_1)}{\vect{x}_1 \mid \vect{x_t}}~,
\end{align}
where $\vect{u}_t(\vect{x}_t; \vect{x}_1) = \frac{\mathrm{d}}{\mathrm{d} t} \vect{\psi}_t(\vect{x}_0; \vect{x}_1)$ is the vector field that generates the conditional flow $\vect{\psi}_t(\vect{x}_0; \vect{x}_1)$. In the case of the simple interpolation conditional flows $\vect{\psi}_t(\vect{x}_0; \vect{x}_1) = (1-t) \vect{x}_0 + t \vect{x}_1$, the formula specializes as follows:
%%
\begin{align}\label{eq: velocity from conditional}
    \vect{u}_t(\vect{x}_t) =  \mean{\vect{x}_1 - \vect{x}_0}{\vect{x}_1 \mid \vect{x_t}} \,.
\end{align}
%%
Readers who are familiar with generative diffusion will immediately recognize that this expression is directly related to the standard expression for the score function. From this connection, it is clear that the conditional vector field can be estimated with a regression objective
\begin{align*}
    \mathbb{E}_{t,\vect{x}_0, \vect{x}_1} || \vect{f}_{\theta}(\vect{\psi}_t(\vect{x}_0; \vect{x}_1), t) - \vect{u}_t(\vect{\psi}_t(\vect{x}_0; \vect{x}_1) ; \vect{x}_1)||^2_2.
\end{align*}




\subsection{Noise coupling}
An advantage of the Flow Matching formalism over SDE diffusion is that, as shown in \citep{pooladian2023multisample, tong2023conditional}, it is straightforward to introduce a probabilistic coupling $
\pi(\vect{x}_1 \mid \vect{x}_0)$ between the data and the noise distribution. In this case, we require that $\int \pi(\vect{x}_1 \mid \vect{x}_0) \mathrm{d} \vect{x}_0$ should follow a standard normal distribution, at least approximately. 
The use of a non-trivial noise coupling does not alter the form of the conditional velocity fields $\vect{u}_t(\vect{x}_t; \vect{x}_1)$ as far as the coupling is time-independent. However, it does alter the total velocity field $\vect{u}_t(\vect{x}_t)$ since it affects the conditional distribution $p(\vect{x}_1, \vect{x}_t)$~, which determines the expectation in Eq.~\ref{eq: velocity from conditional}.

%Note that for simplicity, in the above presentation we used the convention $t \in [0,1]$, while in most Diffusion and Consistency works the time is generally set to be between to two scalars $t \in [\sigma_{\mathrm{min}}, \sigma_{\mathrm{max}}]$. 

\section{Continuous consistency models from a Flow Matching perspective}
As explained above, the flow function $\vect{\psi}_t(\vect{x}_0)$ maps a noiseless state $\vect{x}_0$ to the noisy state $\vect{x}_t$. Its inverse $\vect{\psi}_t^{-1}(\vect{x}_t)$ can then be interpreted as a denoiser, as it maps each noisy state to a uniquely defined noiseless state $\vect{x}_0$. This function is often referred to as a consistency map, and it follows the identity:
\begin{align} \label{eq: total derivative}
    \frac{\mathrm{d}}{\mathrm{d}t} \vect{\psi}_t^{-1}(\vect{\psi}_t(\vect{x}_0))=0, \quad \text{on}\quad t\in[0,1],
\end{align}
together with the boundary condition $\vect{\psi}_0^{-1}(\vect{x}_0) = \vect{x}_0$. Eq.~\ref{eq: total derivative} is a consequence of the fact that all noisy states in an ODE trajectory $\vect{\psi}_t(\vect{x}_0)$ share the same initial point $\vect{x}_0$, which implies that $\vect{\psi}_t^{-1}(\vect{\psi}_t(\vect{x}_0))$ is constant along the trajectory. This property can be used to define a continuous loss for a network $\vect{f}_{\vect{\theta}}(\vect{x}_t, t)$, trained to approximate the  inverse flow $\vect{\psi}_0^{-1}(\vect{x}_t)$:

\begin{align} \label{eq: continuous loss}
    \mathcal{L}_{\text{cont}}^{\text{tot}}(\vect{\theta}) \equiv  \mean{\int_0^1 \lambda(t) \norm{\frac{\mathrm{d}}{\mathrm{d}t} \vect{f}_{\vect{\theta}}(\vect{\psi}_t(\vect{x}_0), t)}^2 \mathrm{d} t}{\vect{x}_0} 
\end{align}
%% OLD (discrete)
%& \sum_{i=1}^N \lambda(t) \mean{\norm{\vect{f_\theta}(\vect{x}_{t_{i+1}},t_{i+1}) - \vect{f}_{\vect{\theta}^-}(\vect{x}_{t_i},t_i)}^2}{\vect{x}_0}, 
%%
%for a given discretized ODE trajectory $\vect{x}_t$ with times $t_{min}=t_1 < t_2 < \dots < t_N=T$ and 
where $\lambda(t)$ is a positive-valued function that weights the loss for different time points. This loss should be used together with the identity boundary condition $\vect{f}_{\vect{\theta}}(\vect{x}_0, 0) = \vect{x}_0$, which we will discuss later.
 In distillation training, the deterministic trajectories $\vect{x}_t = \vect{\psi}_t(\vect{x}_0)$ are obtained by integrating the ODE flow obtained from a pre-trained diffusion or flow matching model. Alternatively, the consistency network can be trained directly by re-writing the total derivative in terms of the conditional flow: 
\begin{align} \label{eq: conditional consistency}
\begin{split}
    & \frac{\mathrm{d}}{\mathrm{d}t} \vect{\psi}_t^{-1}(\vect{\psi}_t(\vect{x}_0)) = \nabla \vect{\psi}_t^{-1}(\vect{x}_t)   \frac{\mathrm{d} x_t}{\mathrm{d} t} +  \partial_t \vect{\psi}_t^{-1}(\vect{x}_t) \\
    & = \nabla \vect{\psi}_t^{-1}(\vect{x}_t)   \mean{\vect{u}_t(\vect{x}_t; \vect{x}_1)}{\vect{x}_1 \mid \vect{x_t}} +  \partial_t \vect{\psi}_t^{-1}(\vect{x}_t) \\
    & = \mean{\nabla \vect{\psi}_t^{-1}(\vect{x}_t)   \vect{u}_t(\vect{x}_t; \vect{x}_1) +  \partial_t \vect{\psi}_t^{-1}(\vect{x}_t)}{\vect{x}_1 \mid \vect{x_t}} \\
    & = \mean{\frac{\mathrm{d}}{\mathrm{d}t} \vect{\psi}_t^{-1}(\vect{\psi}_t(\vect{x}_0; \vect{x}_1))}{\vect{x}_1 \mid \vect{x_t}}~.
\end{split}
\end{align}
From this equality, together with the fact that the squared Euclidean norm is a convex function, it follows that
\begin{align*}
    \begin{split}
     &\mathcal{L}_{\text{cont}}^{\text{tot}}(\vect{\theta}) \leq \mathcal{L}_{\text{cont}}^{\text{cond}}(\vect{\theta})~, ~~~~\text{with}\\ 
     & \mathcal{L}_{\text{cont}}^{\text{cond}}(\vect{\theta}) \equiv \mean{ \int_0^1 \lambda(t) \norm{\frac{\text{d}}{\text{d}t} \vect{f}_{\vect{\theta}}(\vect{\psi}_t(\vect{x}_0; \vect{x}_1), t)}^2 \mathrm{d} t}{\vect{x}_1, \vect{x_0}}~,
     \end{split}
\end{align*}
where we moved the expectation outside of the squared norm using Jensen's inequality. Therefore, we can optimize the tractable "conditional loss" $\mathcal{L}_{\text{cont}}^{\text{cond}}(\vect{\theta})$ instead of $\mathcal{L}_{\text{cont}}^{\text{tot}}(\vect{\theta})$, which contains the unknown flow function $\vect{\psi}_t(\vect{x}_0)$. 
%By discretizing this loss, we obtain the flow matching consistency loss:
%\begin{align} \label{eq: discretized loss}
%\begin{split}
%    & \mathcal{L}(\vect{\theta}) = \sum_{i=1}^N \lambda(t_i) \mathbb{E}_{\vect{x}_0,\vect{x}_1}\left[||\vect{f_\theta}(\vect{\psi}_{t_{i+1}}(\vect{x}_0; \vect{x}_1),t_{i+1}) \right. \\ & \left. - \vect{f}_{\vect{\theta}^-}(\vect{\psi}_{t_{i}}(\vect{x}_0; \vect{x}_1),t_{i})||^2\right]~.
%\end{split}
%\end{align}
%where $\vect{x}_0, \vect{x}_1$ are sampled according to the noise-coupling $\pi(\vect{x}_0, \vect{x}_1)$. 
%\jcc{the structure of this part need to switch a bit. we first introduce the derivation of Eq. 9, and then put the part about discretization in Eq. 6 here. as we only care about $\psi(\vx_0, \vx_1)$. i think it is good to wrap this entire paragraph as a proposition? and clarify the message of its. }



%Using standard results of Bayesian decision theory we show that, in the non-parametric limit, the loss in Eq.~\ref{eq: discretized loss} is optimized when $\vect{f_\theta}(\vect{\psi}_{t_{i+1}}(\vect{x}_0; \vect{x}_1),t_{i+1})$ is equal to $\mean{\vect{f}_{\vect{\theta}^-}(\vect{\psi}_{t_{i}}(\vect{x}_0; \vect{x}_1),t_{i})}{\vect{x_0}, \vect{x_1}}$, which converges to the correct solution $ $ in the continuous limit due to Eq.~\ref{eq: conditional consistency}. This is consistent with the results for consistency models derived from diffusion presented in \citep{song2023consistency}, where the consistency loss was shown to be unbiased in the continuous limit. 



%\jcc{do we want to claim this part as one of our contribution (though I think some other work more or less talked about this)? or as a reinterpretation of two prominent works?}

%\luca{This part is somewhat of a contribution in the sense that the derivations are cleaner and the math is more general than what is currently in the literature. However, I do not think we should make big claims as it still is a re-statment of what is already known.}

\section{Discretized consistency training}
The continuous loss can be directly minimized in expectation by sampling the time $t$ from a uniform distribution and by computing the total derivative $\frac{\mathrm{d}}{\mathrm{d}t} \vect{f}_{\vect{\theta}}(\vect{\psi}_t(\vect{x}_0;\vect{x}_1), t)$ by automatic differentiation. However, in practice it is often more convenient to instead optimize a time-discretized loss with a finite difference approximation for the total derivative:
%%
\begin{align} \label{eq: discretized loss}
\begin{split}
    & \mathcal{L}_{\text{disc}}^{\text{cond}}(\vect{\theta}) = \sum_{i=1}^N \lambda(t_i) \mathbb{E}_{\vect{x}_0 \sim p_0(\vect{x}_0),\vect{x}_1 \sim \pi(\vect{x}_1 \mid \vect{x}_0)} \left[|| \Delta \vect{f}_{\vect{\theta}} ||^2\right]~, \\ 
    &~~~ \text{with} \\ 
    & \Delta \vect{f}_{\vect{\theta}} = \vect{f_\theta}(\vect{\psi}_{t_{i+1}}(\vect{x}_0; \vect{x}_1),t_{i+1}) - \vect{f}_{\vect{\theta}^-}(\vect{\psi}_{t_{i}}(\vect{x}_0; \vect{x}_1),t_{i})~.
\end{split}
\end{align}
where $\vect{x}_1 \mid \vect{x}_0$ are sampled according to the noise-coupling $\pi(\vect{x}_1 \mid \vect{x}_0)$. In this expression, $\vect{\theta}^-$ denotes a frozen copy of the parameters which does not require gradients. This loss is in fact unbiased for $\Delta t \rightarrow 0$, as it was shown in \cite{song2023consistency}.
The boundary condition can be enforced through the parametrization introduced in \citep{karras2022elucidating}:
\begin{equation*}
\label{eq:boundary}
    \vect{f_{\theta}}(\vect{x},t) = c_{\mathrm{skip}}(t)\vect{x} + c_{\mathrm{out}}(t)\vect{F_\theta}(\vect{x},t),
\end{equation*}
where $\vect{F_\theta}$ is a neural network and $c_{\mathrm{skip}}$ and $c_{\mathrm{out}}$ are specified such that $c_{\mathrm{skip}}(0)=1$ and $c_{\mathrm{out}}(0)=0$.


\section{Consistency Models with learned variational coupling}
Our method consists in learning a conditional coupling $q_{\vect{\phi}}(\vect{x}_1 \mid \vect{x}_0)$ with a neural network $\vect{g_\phi}(\vect{x})$ parametrized by $\vect{\phi}$, which we refer to as the encoder in the following given its analogy with VAEs. During training, we can sample noise conditionally from $\pi(\vect{x}_1 \mid \vect{x}_0)=q_{\vect{\phi}}(\vect{x}_1 \mid \vect{x}_0)p_0(\vect{x}_0)$ instead of the independent noise commonly used in CT, and obtain noisy states for a given time step $t$ as follows:
\begin{align*}
    \begin{split}
    \vect{x}_t &= \vect{\psi}_t(\vect{x}_0; \vect{x}_1), \\
    \quad \vect{x}_1 \sim q_{\vect{\phi}}(\vect{x}_1 \mid \vect{x}_0) &= \mathcal{N}( \vect{x}_1;\vect{g}_{\vect{\phi}}^{\mu}(\vect{x}_0),~\vect{g}_{\vect{\phi}}^{\sigma}(\vect{x}_0)^2\rmI),
    \end{split}
\end{align*}
where we express the corresponding coupled noise $\vect{x}_1$ using the Gaussian reparameterization formula:
\begin{equation*}
    \vect{x}_1 = \vect{g}_{\vect{\phi}}^{\mu}(\vect{x}_0) + ~\vect{g}_{\vect{\phi}}^{\sigma}(\vect{x}_0) \cdot \vect{\epsilon}, \quad \vect{\epsilon}\sim\mathcal{N}(\vect{\epsilon};\bm{0},\vect{I})
\end{equation*}
Here, we restricted our attention to linear forward models of the form $\vect{\psi}_t(\vect{x}_0; \vect{x}_1) = a_t \vect{x}_0 + b_t \vect{x}_1$, which encompasses most models used in the diffusion and flow-matching literature. Moreover $\vect{g}_{\vect{\phi}}^{\mu}(\vect{x}_0)$ and $\vect{g}_{\vect{\phi}}^{\sigma}(\vect{x}_0)$ denote the mean and scale output of the encoder, which define the signal-noise coupling (see Appendix \ref{app:diag} for a visual representation). Both $\vect{g}_{\vect{\phi}}^{\mu}(\vect{x}_0)$ and $\vect{g}_{\vect{\phi}}^{\sigma}(\vect{x}_0)$ preserve the same dimensionality of the input signal.
The encoder network that produces the coupling distribution $q_{\vect{\phi}}(\vect{x}_1 \mid \vect{x}_0)$ can be trained end-to-end alongside the consistency model, as shown in Algorithm \ref{alg:train-vae}. This results in a joint optimization where the consistency network adjusts its constancy to minimize its total derivative along the trajectories while the encoder implicitly moves the trajectories towards the space of constancy of the model. In fact, the velocity field $\vect{u}_t(\vect{x}_t)$ depends on the coupling, since $\pi(\vect{x}_1 \mid \vect{x}_0)$  affects the expectation in Eq.~\eqref{eq: velocity from conditional}.
\begin{algorithm}[tb]
   \caption{Variational Consistency Training}
   \label{alg:train-vae}
\begin{algorithmic}
   \STATE \textbf{Input:} data distribution $p_{\mathrm{data}}$, initial model parameter $\vect{\theta}$, initial encoder parameter $\vect{\phi}$, learning rate $\eta$, EMA rate $\mu$, distance function $d(\cdot,\cdot)$, consistency weighting $\lambda_{\mathrm{ct}}(\cdot)$, KL weighting $\lambda_{\mathrm{kl}}$
   \STATE $\vect{\theta}_{\mathrm{EMA}} \leftarrow \vect{\theta}$, $\vect{\phi}_{\mathrm{EMA}} \leftarrow \vect{\phi}$ and $k \leftarrow 0$
   \REPEAT
   \STATE Sample $\vect{x}_0 \sim p_{\mathrm{data}}$,  $t\sim p(t)$, $r=t-\Delta t$
   \STATE Sample $\vect{\epsilon} \sim N(\rmzr,\rmI)$
   \STATE $\vect{x}_1 \leftarrow \vect{g}_{\vect{\phi}}^{\mu}(\vect{x}_0) + ~\vect{g}_{\vect{\phi}}^{\sigma}(\vect{x}_0) \vect{\epsilon}$
   \STATE $\vect{x}_{t} \leftarrow a_t\vect{x}_0+b_t\vect{x}_1$
   \STATE $\vect{x}_{r} \leftarrow a_r\vect{x}_0+b_r\vect{x}_1$
   \STATE $\mathcal{L}_{\text{disc}}^{\text{cond}}(\vect{\theta, \phi})\leftarrow \lambda_{\mathrm{ct}}(t)d(\vect{f_{\theta}}(\vect{x}_{t}, t),\vect{f_{\theta^-}}(\vect{x}_{r}, r))$
   \STATE $\mathcal{L}_{\mathrm{kl}}(\vect{\phi})\leftarrow \KL(\mathcal{N}(\vect{g}_{\vect{\phi}}^{\mu}(\vect{x}_0),~\vect{g}_{\vect{\phi}}^{\sigma}(\vect{x}_0)^2\rmI) || \mathcal{N}(\rmzr, \rmI))$
   \STATE $\mathcal{\tilde{L}}(\vect{\theta, \phi})\leftarrow \mathcal{L}_{\text{disc}}^{\text{cond}}(\vect{\theta, \phi}) + \lambda_{\mathrm{kl}}\mathcal{L}_{\mathrm{kl}}(\vect{\phi})$
   \STATE $\vect{\theta} \leftarrow \vect{\theta} - \eta \nabla_{\vect{\theta}}\mathcal{\tilde{L}}(\vect{\theta}, \vect{\phi})$
   \STATE $\vect{\phi} \leftarrow \vect{\phi} - \eta \nabla_{\vect{\phi}}\mathcal{\tilde{L}}(\vect{\theta}, \vect{\phi})$
   \STATE $\vect{\theta}_{\mathrm{EMA}} \leftarrow$ \texttt{stopgrad}$(\mu \vect{\theta}_{\mathrm{EMA}} + (1-\mu)\vect{\theta})$
   \STATE $\vect{\phi}_{\mathrm{EMA}} \leftarrow$ \texttt{stopgrad}$(\mu \vect{\phi}_{\mathrm{EMA}} + (1-\mu)\vect{\phi})$
   \STATE $k \leftarrow k + 1$
   \UNTIL convergence
\end{algorithmic}
\end{algorithm}
This formulation results in a viable generative model as long as the noise at time $1$ remains approximately $p_1(\vect{x}_1)$, since severe deviation from the prior induced by the coupling would result in improper initialization for the sampling procedure and consequently in reduced sample quality. We therefore add a Kullback-Leibler divergence as a regularizer, $\KL(q_{\vect{\phi}}||p_1)$, resulting in a loss resembling the Evidence Lower Bound loss of Variational Autoencoders \cite{kingma2013auto}:
\begin{align}
    \label{eq:loss}
    \tilde{\mathcal{L}}(\vect{\theta}, \vect{\phi}) &= \mathcal{L}_{\text{disc}}^{\text{cond}}(\vect{\theta}, \vect{\phi}) \\ \nonumber &+ \mean{\KL(q_{\vect{\phi}}(\vect{x}_1 \mid \vect{x}_0)||\mathcal{N}(\vect{x}_1; \rmzr, \rmI))}{\vect{x}_0}.
\end{align}
While using an encoder to learn the data-noise coupling requires additional computation during training, we empirically find that a relatively small encoder is enough to learn an effective coupling, which results only in a minor increase of training time (see Appendix \ref{app:exp}). At sampling, the speed and computational requirements are identical to vanilla CMs for the one-step procedure, while for multistep sampling we need to account for additional forward passes of the encoder as shown in Algorithm \ref{alg:sampling-vae}.

% \jcc{Comparing to WAE's style loss:
% \begin{align*}
%     & \tilde{\mathcal{L}}(\vect{\theta}, \vect{\phi}) = \mathcal{L}(\vect{\theta}) + \KL\big(q_{\phi}(\vect{x}_1)||\mathcal{N}(\vect{x}_1; \rmzr, \rmI)\big) \\
%     & \mathrm{where } q_{\phi}(\vect{x}_1):=\mean{q_{\phi}(\vect{x}_1 \mid \vect{x}_0)}{\vect{x}_0},
% \end{align*}
% our loss function is actually an upper bound of that as
% \begin{align}
%     & D_{\mathrm{KL}}(q_\phi(\vect{x}_1) \| \mathcal{N}(\vect{x}_1; \rmzr, \rmI)) \\ & \leq \mathbb{E}_{\vect{x}_0 \sim p_{\mathrm{data}}(x)} \left[ D_{\mathrm{KL}}(q_\phi(\vect{x}_1 \mid \vect{x}_0) \| \mathcal{N}(\vect{x}_1; \rmzr, \rmI)) \right].
% \end{align}
%  This means, our coupling indeed are more implicit than WAE one, which is somehow explicitly related to OT of Wasserstein distance between $p_{\mathrm{data}}$ and the decoded distribution. A advantageous point of using this upper bound is we are imposing local coupling. But, really need to differentiate our objective or potentially any core difference with the Minimizing Curvature paper... 
% }
\subsection{Connection with variational autoencoders}

In this section, we will consider the special case with constant unit time weighting $\lambda_{\mathrm{ct}}(t) = 1, \forall t$.
\paragraph{ELBO perspective.}
First, we demonstrate the relationship between our model and VAE in terms of their objective functions. Specifically, our loss function in Eq.~\ref{eq:loss} serves as an upper bound on a standard VAE loss, where the latent vector $\vect{x}_1$ is regularized to be close to the prior $p_1$. Using the triangle inequality, we can establish the following bound for $\mathcal{L}_{\text{disc}}^{\text{cond}}$:
\begin{align} \label{eq: VAE1 loss}
   & \norm{\vect{x}_0 - \vect{f}_{\vect{\theta}}(\vect{x}_1,1)}^2 \leq \\ \nonumber
   & \sum_{i=0}^N \norm{\vect{f_\theta}(\vect{\psi}_{t_{i+1}}(\vect{x}_0; \vect{x}_1),t_{i+1}) - \vect{f}_{\vect{\theta}^-}(\vect{\psi}_{t_{i}}(\vect{x}_0; \vect{x}_1),t_{i})}^2.
\end{align}
Given that the KL terms in Eq.~\ref{eq:loss} and the VAE loss are identical, our loss function serves as an upper bound on the loss of a VAE with an encoder $(\vect{g}_{\vect{\phi}}^{\mu}(\vect{x}_0),\vect{g}_{\vect{\phi}}^{\sigma}(\vect{x}_0))$ and a prior $\mathcal{N}(\vect{x}_1; \rmzr, \rmI)$. Since the VAE loss represents an evidence lower bound, it follows that the consistency loss also provides a lower bound on the model evidence (proof in Appendix \ref{app:elbo}). Compared to traditional VAEs, our method can be viewed as a time-dependent modification where the transition kernel smoothly interpolates between delta distributions centered at datapoints and a Gaussian distribution.
\paragraph{Varying $\beta$.}
In both our model and VAE, the latent vector needs to approximately follow a normal distribution to avoid deviating from the prior. However, previous studies~\citep{hoffman2016elbo,rosca2018distribution,aneja2021contrastive} have observed that VAE's aggregated posterior fails to match the prior. The same problem could occur in our model without additional tricks (see Sec.~\ref{sec:beta}).
To mitigate this prior-posterior mismatch, we introduce a scalar hyperparameter $\beta$ to control the strength of the KL regularization. This was first introduced by \citet[$\beta$-VAE][]{higgins2022beta} for a different purpose (inducing disentangled latent representation) in the VAE context.
\begin{align}
 \min_{\vect{\theta}, \vect{\phi}}\mathcal{L}_{\text{disc}}^{\text{cond}}(\vect{\theta}, \vect{\phi}) + \beta \mathbb{E}_{\vx_0}[\KL(q_{\vect{\phi}}(\vect{x}_1 \mid \vect{x}_0)||\mathcal{N}(\vect{x}_1; \rmzr, \rmI))].
 \label{eq:beta_loss}
\end{align}
By carefully selecting the value of $\beta$, we can achieve a proper balance between flexibility and proximity to the prior.
To understand the effect of $\beta$ in our model, we present an alternative form of our objective function. Formally, we can view the minimization of Eq.~\ref{eq:beta_loss} as the relaxed Lagrangian problem of the following optimization problem:
\begin{align*}
 &\min_{\vect{\theta}, \vect{\phi}}\mathcal{L}_{\text{disc}}^{\text{cond}}(\vect{\theta}, \vect{\phi}) \\ \nonumber &\mathrm{s.t.} \quad \KL(q_{\vect{\phi}}(\vect{x}_1 \mid \vect{x}_0)||\mathcal{N}(\vect{x}_1; \rmzr, \rmI)) < \delta,
\end{align*}
As $\delta$ approaches 0, the coupling becomes $\pi(\vect{x}_1 \mid \vect{x}_0) =p_0(\vx_0)p_1(\vx_1)$, indicating that our model encompasses the standard CT.
In VAE, selecting appropriate values of $\beta$ to achieve reasonable generation performance is generally challenging. Values too close to zero result in strong deviation from the prior, while extremely large values cause over-smoothed decoders~\citep{takida2022preventing}, leading to blurry samples. However, our model does not suffer from this over-regularization issue. While tuning $\beta$ remains crucial in our model, as demonstrated in Section~\ref{sec:beta}, unlike VAE, increasing values of $\beta$ does not cause the oversmoothing problem but simply reduces our model to the standard CT. Consequently, the CT training objective enables sharp sample generation even when the posterior approximation is nearly a normal distribution.



\begin{algorithm}[tb]
   \caption{Multistep Variational Consistency Sampling}
   \label{alg:sampling-vae}
\begin{algorithmic}
   \STATE \textbf{Input:} Consistency model $\vect{f_\theta}$, encoder $\vect{g_{\phi}}$, sequence of time points $\tau_1 > \tau_2 > \dots > \tau_{N-1}$, initial noise $\vect{\hat{x}}_T$
   \STATE $\vect{x} \leftarrow \vect{f_\theta}(\vect{\hat{x}}_T, T)$
   \FOR{$n=1$ \textbf{to} $N-1$} 
   \STATE Sample $\vect{\epsilon}\sim \mathcal{N}(\rmzr, \rmI)$
   \STATE $\vect{x}_1 \leftarrow \vect{g}_{\vect{\phi}}^{\mu}(\vect{x}) + ~\vect{g}_{\vect{\phi}}^{\sigma}(\vect{x}) \vect{\epsilon}$
    \STATE $\vect{\hat{x}}_{\tau_n} \leftarrow a_{\tau_n}\vect{x} + b_{\tau_n}\vect{x}_1$
   
   \STATE $\vect{x} \leftarrow \vect{f_\theta}(\vect{\hat{x}}_{\tau_n}, \tau_n)$
   \ENDFOR
   \STATE \textbf{Output}: $\vect{x}$
\end{algorithmic}
\end{algorithm}

\subsection{Choosing the $\beta$ parameter}
 The most common weighting function $\lambda_{\mathrm{ct}}(t)$ for CMs is an adaptive weighting scheme that changes over training based on how fine grained the discretization is, or equivalently the distance between consecutive time steps in ECM. At a given training iteration, for two adjacent time steps $t_i$ and $t_{i+1}$, we have:
 \begin{align*}
    \lambda_{\mathrm{ct}}(t_{i+1}) = \frac{1}{\Delta_{t_{i+1}}}, \quad \Delta_{t_{i+1}}=t_{i+1} - t_i
 \end{align*}
 For the models trained with such an adaptive weighting function, we found it hard to tune $\beta$ to a single scalar value. The magnitude of the weights increases during training as the discretization scheme becomes more fine-grained and $\Delta_{t_{i+1}}$ becomes smaller, changing the balance between consistency loss and KL regularization, resulting in a very strong regularization at the early stages of training, or a too weak one at the later stages. A simple yet effective solution is to use an adaptive scaling for the KL regularization that changes according to the discretization scheme. To do so, we take as a reference the weighting of the consistency loss at the last step $t_N=\sigma_{\mathrm{max}}$, and define the adaptive KL weighting as:
\begin{align*}
\lambda_{\mathrm{kl}} = \beta \lambda_{\mathrm{ct}}(t_N) = \frac{1}{\Delta_{t_{N}}}.
\end{align*}
This way, we only need to specify the scalar hyperparameter $\beta$, and it will have a consistent regularization strength over training, as it increases whenever the discretization scheme is changed, which reflects in $\Delta_{t_{N}}$ becoming smaller.
For ECM models trained on ImageNet, which use the EDM-style weighting function \begin{align*}
    \lambda_{\mathrm{ct}}(t) = \frac{1}{t^2} + \frac{1}{\sigma_{\mathrm{data}}^2},
\end{align*}
we simply select a fixed $\beta$ scalar for the whole training, as the discretization scheme does not affect the magnitude of the weights.

\section{Experiments}
\label{sec:experiments}
In the following, we show that learning the data-noise coupling with our method is a simple yet effective improvement for CT. We pair our Variational Coupling with two established methods as baselines, namely improved Consistency Training (iCT) from \citep{songimproved} and Easy Consistency Tuning (ECM) from \citep{geng2024consistency}. Note that for the latter, the model is initialized with the weights of a pretrained score model from \citep{karras2022elucidating}, while in the former the weights are initialized at random. More details about the baselines are provided in Appendix \ref{app:baselines}. In this work, we consider only the framework of CT, where the unbiased vector field estimator $\vect{u}_t(\vect{x}_t)$ from equation \ref{eq: velocity from conditional} is used to approximate the noisy states $\vect{x}_t$ during training, as opposed to Consistency Distillation that uses a pretrained score model as a teacher. We evaluate the models on the image datasets Fashion-MNIST \citep{xiao2017/online}, CIFAR-10 \citep{krizhevsky2009learning}, FFHQ $64 \times 64$ \citep{karras2019style} and (class-conditional) ImageNet $64 \times 64$ \citep{deng2009imagenet}. To learn the coupling, we add a smaller version of the neural network used for CT, without time conditioning and with weights always initialized at random. For all the models, we use the variance exploding transition kernel (iCT-VE and ECM-VE) used in \citep{karras2022elucidating} and \citep{songimproved}, with $a_t = 1$, $b_t=t$, and the linear interpolation kernel (iCT-LI and ECM-LI) commonly used in Flow Matching \citep{lipmanflow}, with $a_t= 1-t/\sigma_{\mathrm{max}}$ and $b_t= t/\sigma_{\mathrm{max}}$ (details in Appendix \ref{app:fmbc}). For both kernels, we set $\sigma_{\mathrm{min}}=0.002$ and $\sigma_{\mathrm{max}}=80$. More experimental details can be found in Appendix \ref{app:exp}, while samples obtained with our best models are shown in \ref{app:qualitative}.

\subsection{Baselines and models}
As baselines, we re-implement the iCT and ECM models, corresponding to our iCT-VE and ECM-VE. As an additional model, we add CT with the minibatch Optimal Transport Coupling (-OT) proposed in \citep{pooladian2023multisample, tong2023conditional} and used in \citep{issenhuth2024improving}, to compare the effectiveness and scalability of the OT coupling with the learned one. Finally, we combine the baselines with our proposed Variational Coupling (-VC). For the models with learned coupling, we use gradient clipping with a large value (200 in all the experiments) to avoid instabilities at the early stages of training. 

\subsection{Ablation for different $\beta$}
\label{sec:beta}
To see the effect of $\beta$ on the generation performance, we compare the results for different values on the FashionMNIST dataset for iCT-VE-VC in Table \ref{tab:beta}. As expected, with small values of $\beta$, the coupling distribution deviates from the sampling distribution and the performance degenerates, while increasing $\beta$ to high values reduces the benefits of the learned coupling.

\begin{table}[h]
    \centering
    \resizebox{0.45\columnwidth}{!}{\begin{tabular}{lll}
    \multicolumn{3}{c}{\textbf{FID for different $\beta$}} \\ 
    \toprule
          & 1 step & 2 steps\\
         %& & \multicolumn{3}{c}{Nr. of latent dimensions} \\
        \midrule
        $\beta=5$ & $12.53$ & $5.69$ \\ 
        \midrule
        $\beta=15$ & $4.97$ & $\bm{2.34}$\\
        \midrule
        $\beta=30$ & $\bm{3.88}$ & $2.37$ \\
        \midrule
        $\beta=60$ & $3.90$ & $2.67$ \\
    \bottomrule        
    \end{tabular}
    }
    \caption{Comparison of FID performance (lower is better) for one and two sampling steps, for varying values of $\beta$. The models are iCT-VE-VC and trained on the FashionMNIST dataset with the same settings described in appendix \ref{app:exp}. Best entries in bold.}
    \label{tab:beta}
\end{table}
\begin{table}[h]
    \centering
    \begin{tabular}{c}
        \textbf{1-step / 2-step FID for iCT-based models} \\
    \end{tabular}
    \vspace{0.5cm}
    \resizebox{0.8\columnwidth}{!}{\begin{tabular}{lll}
    \toprule
         Model & Fashion-MNIST & CIFAR10\\
         %& & \multicolumn{3}{c}{Nr. of latent dimensions} \\
        \midrule
        iCT-VE$^*$ & - & $\bm{2.83}$ / $2.46$ \\ 
        \midrule
        iCT-VE & $4.79$ / $3.54$ & $3.61$ / $2.79$ \\
        \midrule
        iCT-LI & $4.75$ / $3.46$ & $3.81$ / $2.87$ \\
        \midrule
        iCT-VE-OT & $4.42$ / $2.82$ & $3.28$ / $2.66$ \\
        \midrule
        iCT-LI-OT & $4.41$ / $2.91$ & $3.42$ / $2.77$ \\
        \midrule
        iCT-VE-VC (ours) & $3.88$ / $2.37$ & $2.86$ / $\bm{2.32}$ \\
        \midrule
        iCT-LI-VC (ours) & \bm{$3.62$} / \bm{$2.22$} & $2.94$ / $\bm{2.32}$ \\
    \bottomrule  
    \end{tabular}
    }
    \caption{Comparison of FID (lower is better, reported as 1-step / 2-step performance) for different models based on iCT. The model marked with a $*$ is the baseline as reported in \citep{songimproved}. All the other models are from our re-implementation. The best entries are highlighted in bold.}
    \label{tab:ict}
\end{table}

\begin{table*}[h]
\centering
\small 
\begin{tabular}{llll}
\toprule
Model & CIFAR10 & FFHQ ($64\times 64$) & ImageNet ($64\times 64$) \\ 
\midrule
ECM-VE$^*$ & 3.60 / 2.11 & - & 5.51$^\dagger$ / 3.18$^\dagger$ \\
ECM-VE & 3.68 / 2.14 & 5.99 / 4.39 & 5.26 / 3.22 \\
ECM-LI & 3.65 / 2.14 & 6.42 / 4.73 & 5.13 / 3.20 \\
ECM-VE-OT & 3.46 / 2.13 & 6.11 / 4.68 & 6.02 / 4.27 \\
ECM-LI-OT & 3.49 / 2.13 & 6.19 / 4.73 & 5.63 / 4.09 \\
ECM-VE-VC (ours) & \textbf{3.26} / \textbf{2.02} & \textbf{5.47} / \textbf{4.16} & 5.08 / 3.15 \\
ECM-LI-VC (ours) & 3.39 / 2.09 & 5.57 / 4.29 & \textbf{4.93} / \textbf{3.07} \\
\bottomrule
\end{tabular}
\caption{Comparison of FID (lower is better, reported as 1-step / 2-step performance) for different models based on ECM. The model marked with a $*$ is the baseline as reported in \citep{geng2024consistency}. All the other models are from our re-implementation. The best entries are highlighted in bold. For ImageNet, the results marked with $\dagger$ are obtained with models trained for $100k$ iterations, while the others use $200k$ iterations.}
\label{tab:ecm}
\end{table*}
\subsection{Results}
\begin{figure}
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.48\linewidth]{figures/samples/imnet_ind_1.png}
        \includegraphics[width=0.48\linewidth]{figures/samples/imnet_ind_2.png}
        \caption{1-step (FID=5.13, left) and 2-step (FID=3.20, right) samples from ECM-LI.}
        \label{fig:top_row}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.48\linewidth]{figures/samples/imnet_vae_1.png}
        \includegraphics[width=0.48\linewidth]{figures/samples/imnet_vae_2.png}
        \caption{1-step (FID=$4.93$, left) and 2-step (FID=$3.07$, right) samples from ECM-LI-VC.}
        \label{fig:bottom_row}
    \end{subfigure}
    \caption{Visual comparison of generated class-conditional samples on ImageNet $64\times64$.}
    \label{fig:imnet_main1}
\end{figure}
\begin{figure}[ht!]
    \centering
        \centering
        \includegraphics[width=0.85\linewidth]{figures/variance/gradient_variance.pdf}
        \includegraphics[width=0.85\linewidth]{figures/variance/fid.pdf}
    \caption{The top graph shows a comparison of gradient variance during training for iCT-VE and iCT-VE-VC on CIFAR10. We plot the variance for each epoch (shaded) and its exponential moving average with smoothing factor 0.9. Especially later during training, the model with learned coupling exhibits lower variance, which results in improved performance, shown in terms of 1-step FID in the bottom graph. For a fair comparison, we did not use gradient clipping for iCT-VC in this run.}
    \label{fig:variance}
\end{figure}
In tables \ref{tab:ict} and \ref{tab:ecm} we report the 1 and 2 step sample quality evaluated with Frechet Inception Distance (FID) \citep{heusel2017gans}, for both the results reported in the original papers and our re-implementations. For high-dimensional data, we only use models based on ECM, as they require lower computational budget, while for FashionMNIST we only use models based on iCT as there is no available pretrained EDM model.\\
\textbf{FashionMNIST}: We choose FashionMNIST as a first benchmark to test the performance of iCT. On this dataset, we use a small version of DDPM++, with $64$ model channels instead of $128$ and no attention, and batch size $128$. Our variant with Variational Coupling outperforms both iCT and iCT-OT, with best performance obtained with the LI transition kernel, showing the benefit of the learned coupling. \\
\textbf{CIFAR-10}: For all the CIFAR-10 experiments, we use the DDPM++ architecture from \citep{songscore} as implemented in \citep{karras2022elucidating}, with EMA rate $0.9999$ as in \citep{geng2024consistency}. While this differs from the settings in \citep{songimproved}, we found it to work better in our re-implementation. The remaining hyperparameters are the same as used in the respective baselines. From the results, we can see how using the learned coupling results in improved performance for both one and two steps generation, outperforming all the re-implemented baselines.
The 1-step result from the original iCT is superior to our model. However, to the best of our knowledge, there is no open-source implementation that can reproduce the results reported in the paper. The learned coupling outperforms the minibatch OT coupling in all cases, as it is less affected by the effective (per device) batch size and the data dimensionality. Finally, our 2-step sampling performance for ECM-VE-VC is on par with the current SoTA achieved by other methods with similar settings \citep{wang2024stable, lee2024truncated, lu2024simplifying}.
We empirically compare the variance of the gradients for iCT-VE and iCT-VE-VC, and show in Figure \ref{fig:variance} how the resulting reduced variance corresponds to improved FID score. In particular, in early training the model with VC exhibits higher variance, which can be attributed to the fact that the encoder is still learning the coupling. As training continues, the coupling becomes effective at providing better data-noise pairs to the model, which results in reduced gradient variance and improved generative performance.\\
\textbf{FFHQ $64 \times 64$}: We use FFHQ $64 \times 64$ as an additional dataset to assess our method on higher-dimensional data. We reuse the same training settings used for CIFAR10, without additional tuning, and with the same network architecture used in EDM. While the results are worse than current SoTA generative models (e.g. 2.39 FID from EDM), they confirm the benefit of using the learned coupling over the baselines. Moreover, the results highlight the limits of using the minibatch OT coupling, which scales poorly with increased data dimensionality and in some cases performs worse than the independent coupling.\\
\textbf{ImageNet $64 \times 64$} (class conditional): As a baseline, we reuse the settings from ECM with the EDM2-S architecture and batch size $128$. While the baseline is trained for $100k$ iteration, we found that our models with Variational Coupling needed more time to converge properly, as the encoder weights are not pretrained and initialized at random. We therefore train our re-implemented baselines and models for $200k$ iterations instead. In this case, the models with Variational Coupling outperform the other models, with the LI kernel obtaining the best overall FID, while the OT coupling performs poorly due to the small batch size and high data dimensionality. In Figure \ref{fig:imnet_main1} we compare samples from ECM-LI and ECM-LI-VC, where we can see how the images generated with VC are more clear and detailed.




% \begin{table*}[h]
% \centering
% \begin{tabular}{@{}lcc|lcc@{}}
% \toprule
% \multicolumn{5}{c}{\textbf{CIFAR-10 (Unconditional)}} \\ \midrule
% \textbf{Method} & \textbf{FID ($\downarrow$)} & & \textbf{Method} & \textbf{FID ($\downarrow$)} \\ \midrule
% \multicolumn{5}{l}{\textit{1-step}} \\ \midrule
% $\mathrm{iCT}^*$\citep{songimproved} & $\bm{2.83}$ & & ECM$^*$ \citep{geng2024consistency} & $3.60$ \\
% iCT & $3.61$ & & ECM & $3.68$ \\
% iCT-FM & $3.81$ & & ECM-FM & $3.65$ \\
% iCT-OT & $3.28$ & & ECM-OT & $3.46$ \\
% iCT-FM-OT & $3.42$ & & ECM-FM-OT & $3.48$ \\
% iCT-VAE (ours) & $2.86$ & & ECM-VAE (ours) & $\bm{3.26}$ \\
% iCT-FM-VAE (ours) & $2.94$ & & ECM-FM-VAE (ours) & $3.39$ \\ \midrule
% \multicolumn{5}{l}{\textit{2-step}} \\ \midrule
% $\mathrm{iCT}^*$ \citep{songimproved} & $2.46$ & & ECM$^*$ \citep{geng2024consistency} & $2.11$ \\
% iCT & $2.79$ & & ECM & $2.14$ \\
% iCT-FM & $2.87$ & & ECM-FM & $2.14$ \\
% iCT-OT & $2.66$ & & ECM-OT & $2.13$ \\
% iCT-FM-OT & $2.77$ & & ECM-FM-OT & $2.15$ \\
% iCT-VAE (ours) & $\bm{2.32}$ & & ECM-VAE (ours) & $\bm{2.02}$ \\
% iCT-FM-VAE (ours) & $\bm{2.32}$ & & ECM-FM-VAE (ours) & $2.09$ \\ \bottomrule
% \end{tabular}
% \caption{Comparison of different models on FID (lower is better) for unconditional CIFAR-10. The model marked with a $*$ correspond to the results reported in the original papers. All the other models are from our reimplementation.}
% \label{tab:cifar}
% \end{table*}

% \begin{table*}[h]
% \centering
% \begin{tabular}{@{}lcc|lcc@{}}
% \toprule
% \multicolumn{5}{c}{\textbf{CIFAR-10 (Unconditional)}} \\ \midrule
% \textbf{Method} & \textbf{FID ($\downarrow$)} & & \textbf{Method} & \textbf{FID ($\downarrow$)} \\ \midrule
% \multicolumn{5}{l}{\textit{1-step}} \\ \midrule
% $\text{iCT}^*$\citep{songimproved} & $\bm{2.83}$ & & ECM$^*$ \citep{geng2024consistency} & $3.60$ \\
% iCT & $3.61$ & & ECM & $3.68$ \\
% iCT-FM & $3.81$ & & ECM-FM & $3.65$ \\
% iCT-OT & $3.28$ & & ECM-OT & $3.46$ \\
% iCT-FM-OT & $3.42$ & & ECM-FM-OT & $3.48$ \\
% iCT-VAE (ours) & $2.86$ & & ECM-VAE (ours) & $\bm{3.26}$ \\
% iCT-FM-VAE (ours) & $2.94$ & & ECM-FM-VAE (ours) & $3.39$ \\ \midrule
% \multicolumn{5}{l}{\textit{2-step}} \\ \midrule
% $\text{iCT}^*$ \citep{songimproved} & $2.46$ & & ECM$^*$ \citep{geng2024consistency} & $2.11$ \\
% iCT & $2.79$ & & ECM & $2.14$ \\
% iCT-FM & $2.87$ & & ECM-FM & $2.14$ \\
% iCT-OT & $2.66$ & & ECM-OT & $2.13$ \\
% iCT-FM-OT & $2.77$ & & ECM-FM-OT & $2.15$ \\
% iCT-VAE (ours) & $\bm{2.32}$ & & ECM-VAE (ours) & $\bm{2.02}$ \\
% iCT-FM-VAE (ours) & $\bm{2.32}$ & & ECM-FM-VAE (ours) & $2.09$ \\ \bottomrule
% \end{tabular}
% \caption{Comparison of different models on FID (lower is better) for unconditional CIFAR-10. The model marked with a $*$ correspond to the results reported in the original papers. All the other models are from our reimplementation.}
% \label{tab:cifar}
% \end{table*}



\section{Conclusions}
In this work, we introduced a novel approach to Consistency Training (CT) by incorporating a variational noise coupling mechanism. Our method leverages an encoder-based coupling function to learn a data-dependent noise distribution, which results in improved generative performance. By framing CT within the Flow Matching perspective, we provided a principled way to introduce adaptive noise coupling while maintaining the efficiency of standard CT. Empirical results on multiple image benchmarks, demonstrate that our approach consistently outperforms baselines in one and two-step generation settings. Our findings highlight the potential of learned coupling in CT and suggest several promising directions for future work. These include exploring more expressive posterior distributions, extending our method to the continuous-time CT formulation, and integrating variational consistency training with other recent CT improvements. We hope this work contributes to the broader understanding of the effect of coupling in CT and inspires further advancements in efficient generative sampling techniques.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}
\bibliography{citations}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Consistency Models with Linear Interpolation Kernel}
\label{app:fmbc}
In addition to the variance exploding forward process commonly used in CT, here we propose to use the linear interpolation kernel commonly used in Flow Matching:
\begin{align}
    \vect{x}_t = (1-t)\vect{x}_0 + t \vect{x}_1.
\end{align}
We reuse all of the building blocks from iCT and ECM and make only the necessary adjustments. Accounting for the boundary conditions, the transition kernel becomes: 

\begin{align}
   \vect{x}_t = \left( 1-\frac{t}{\sigma_{\mathrm{max}}}\right)\vect{x}_0 + \left(\frac{t}{\sigma_{\mathrm{max}}}\right) \vect{x}_1 \sigma_{\mathrm{max}}. 
\end{align}

Other crucial components for stability during training are the scaling factors $c_{\mathrm{in}}$, $c_{\mathrm{skip}}$ and $c_{\mathrm{out}}$, and we derive them for the linear interpolation kernel following the same procedure used in \citep{karras2022elucidating}, also accounting for the boundary conditions when $\sigma_{\mathrm{min}}\neq0$:

\begin{align}
    c_{\mathrm{in}}(\sigma) &= \cfrac{1}{\sqrt{\sigma_{\mathrm{data}}^2  (1-\frac{\sigma}{\sigma_{\mathrm{max}}})^2 + \sigma^2}} \\
    c_{\mathrm{skip}}(\sigma) &= \cfrac{\sigma_{\mathrm{data}}^2 (1-\frac{\sigma - \sigma_{\mathrm{min}}}{\sigma_{\mathrm{max}} - \sigma_{\mathrm{min}}})}{(\sigma - \sigma_{\mathrm{min}})^2 + \sigma_{\mathrm{data}}^2 (1-\frac{\sigma - \sigma_{\mathrm{min}}}{\sigma_{\mathrm{max}} - \sigma_{\mathrm{min}}})^2} \\
    c_{\mathrm{out}}(\sigma) &= (\sigma - \sigma_{\mathrm{min}})  \sigma_{\mathrm{data}} c_{\mathrm{in}}(\sigma)
\end{align}

\subsection{Derivations}

We report the derivations for the scaling factors used for the linear interpolation transition kernel. We follow the same derivations from \citep{karras2022elucidating} (appendix B.6), where the score matching objective is written as:
\begin{align}
    E||D_{\vect{\theta}} (\vect{y} + \vect{n}; \sigma) - \vect{y} ||_2^2
\end{align}
Where $\vect{y}$ is data sampled from the data distribution with standard deviation $\sigma_\mathrm{data}$ and $\vect{n}$ is a sample from noise distribution with standard deviation $\sigma$. Given this objective, they propose to derive the scaling factors $c_{\mathrm{in}}(\sigma)$, $c_{\mathrm{skip}}(\sigma)$, $c_{\mathrm{out}}(\sigma)$ as follows:
\begin{align}
c_{\mathrm{in}}(\sigma) &= \frac{1}{\sqrt{\mathrm{Var}_{\vect{y},\vect{n}}[\vect{y}+\vect{n}]}} \\
c_{\mathrm{out}}(\sigma)^2 & = \mathrm{Var}_{\vect{y},\vect{n}}[\vect{y} - c_{\mathrm{skip}}(\sigma)(\vect{y}+\vect{n})] \\
c_{\mathrm{skip}}(\sigma) & = \mathrm{arg min}_{c_{\mathrm{skip}}(\sigma)}c_{\mathrm{out}}(\sigma)^2.
\end{align}

In our formulation, we only need to rescale $\vect{y}$ by $1 - \frac{\sigma}{\sigma_{\mathrm{max}}}$ and perform the same derivations. For simplicity, we define $\alpha = 1 - \frac{\sigma}{\sigma_{\mathrm{max}}}$ (omitting the dependence on $\sigma$), and proceed as follows:
\begin{align}
c_{\mathrm{in}}(\sigma) &= \frac{1}{\sqrt{\mathrm{Var}_{\vect{y},\vect{n}}[\alpha \vect{y}+\vect{n}]}} \\
c_{\mathrm{out}}(\sigma)^2 & = \mathrm{Var}_{\vect{y},\vect{n}}[\vect{y} - c_{\mathrm{skip}}(\sigma)(\alpha \vect{y}+\vect{n})] \\
c_{\mathrm{skip}}(\sigma) & = \mathrm{arg min}_{c_{\mathrm{skip}}(\sigma)}c_{\mathrm{out}}(\sigma)^2.
\end{align}

The factor $c_{\mathrm{in}}(\sigma)$ simply becomes:
\begin{align}
c_{\mathrm{in}}(\sigma) &= \frac{1}{\sqrt{\sigma_\mathrm{data}^2 * \alpha^2 + \sigma^2}}.
\end{align}

To derive $c_{\mathrm{out}}(\sigma)$ we can proceed as:
\begin{align}
    c_{\mathrm{out}}(\sigma)^2 & = \mathrm{Var}_{\vect{y},\vect{n}}[\vect{y} - c_{\mathrm{skip}}(\sigma)(\alpha \vect{y}+\vect{n})] \\
    c_{\mathrm{out}}(\sigma)^2 & = \mathrm{Var}_{\vect{y},\vect{n}}[(1 - \alpha c_{\mathrm{skip}}(\sigma))\vect{y} + c_{\mathrm{skip}}(\sigma)\vect{n}] \\
    c_{\mathrm{out}}(\sigma)^2 & = (1 - \alpha c_{\mathrm{skip}}(\sigma))^2\sigma_\mathrm{data}^2 + c_{\mathrm{skip}}(\sigma)^2\sigma^2.
\end{align}

We can use this result to solve for $c_\mathrm{skip}(\sigma)$:
\begin{align}
    0 &= d[c_{\mathrm{out}}(\sigma)^2] / dc_{\mathrm{skip}}(\sigma) \\
    0 &= d[(1 - \alpha c_{\mathrm{skip}}(\sigma))^2\sigma_\mathrm{data}^2 + c_{\mathrm{skip}}(\sigma)^2\sigma^2] / dc_{\mathrm{skip}}(\sigma) \\
    0 &= \sigma_\mathrm{data}^2d[(1 - \alpha c_{\mathrm{skip}}(\sigma))^2]/ dc_{\mathrm{skip}}(\sigma) + \sigma^2d[c_{\mathrm{skip}}(\sigma)^2]/dc_{\mathrm{skip}}(\sigma) \\
    0 &= \sigma_\mathrm{data}^2[2\alpha ^2c_{\mathrm{skip}}(\sigma)-2\alpha] + \sigma^2[2c_{\mathrm{skip}}(\sigma)] \\
    0 &= (\sigma^2 + \alpha ^2\sigma_\mathrm{data}^2)c_{\mathrm{skip}}(\sigma) - \alpha \sigma_\mathrm{data}^2 \\
    c_{\mathrm{skip}}(\sigma) &= \alpha \sigma_\mathrm{data}^2/(\sigma^2 + \alpha^2\sigma_\mathrm{data}^2).
\end{align}

Finally, we can compute $c_\mathrm{out}(\sigma)$:
\begin{align}
    c_{\mathrm{out}}(\sigma)^2 &= (1 - \alpha c_{\mathrm{skip}}(\sigma))^2\sigma_\mathrm{data}^2 + c_{\mathrm{skip}}(\sigma)^2\sigma^2 \\
    c_{\mathrm{out}}(\sigma)^2 &= \left(1 - \left[\frac{\alpha^2\sigma_\mathrm{data}^2}{(\sigma^2 + \alpha^2\sigma_\mathrm{data}^2)}\right]\right)^2\sigma_\mathrm{data}^2 +\left[\frac{\alpha\sigma_\mathrm{data}^2}{(\sigma^2 + \alpha^2\sigma_\mathrm{data}^2)}\right]^2 \sigma^2 \\
    c_{\mathrm{out}}(\sigma)^2 &= \left[\frac{\sigma^2 \sigma_\mathrm{data}}{(\sigma^2 + \alpha^2\sigma_\mathrm{data}^2)}\right]^2 +\left[\frac{\alpha\sigma_\mathrm{data}^2 + \sigma}{(\sigma^2 + \alpha^2\sigma_\mathrm{data}^2)}\right]^2 \\
    c_{\mathrm{out}}(\sigma)^2 &= \frac{(\sigma^2 \sigma_\mathrm{data})^2 + (\sigma \alpha\sigma_\mathrm{data}^2)^2}{(\sigma^2 + \alpha^2\sigma_\mathrm{data}^2)^2} \\
    c_{\mathrm{out}}(\sigma)^2 &= \frac{(\sigma \sigma_\mathrm{data})^2 + (\alpha^2\sigma_\mathrm{data}^2 + \sigma^2)}{(\sigma^2 + \alpha^2\sigma_\mathrm{data}^2)^2} \\
    c_{\mathrm{out}}(\sigma)^2 &= \frac{(\sigma \sigma_\mathrm{data})^2}{(\sigma^2 + \alpha^2\sigma_\mathrm{data}^2)} \\
    c_{\mathrm{out}}(\sigma) &= \frac{\sigma \sigma_\mathrm{data}}{\sqrt{\sigma^2 + \alpha^2\sigma_\mathrm{data}^2}}.
\end{align}

If we want to use the boundary conditions for $\sigma_{\mathrm{min}} \neq 0$, then we can modify $c_\mathrm{skip}(\sigma)$ and  $c_\mathrm{out}(\sigma)$ as:
\begin{align}
     c_{\mathrm{skip}}(\sigma) &= \cfrac{\sigma_{\mathrm{data}}^2  (1-\frac{\sigma - \sigma_{\mathrm{min}}}{\sigma_{\mathrm{max}} - \sigma_{\mathrm{min}}})}{(\sigma - \sigma_{\mathrm{min}})^2 + \sigma_{\mathrm{data}}^2 (1-\frac{\sigma - \sigma_{\mathrm{min}}}{\sigma_{\mathrm{max}} - \sigma_{\mathrm{min}}})^2} \\
    c_{\mathrm{out}}(\sigma) &= (\sigma - \sigma_{\mathrm{min}}) \sigma_{\mathrm{data}} c_{\mathrm{in}}(\sigma),
\end{align}

which satisfy the condition $c_\mathrm{skip}(\sigma_{\mathrm{min}})=1$ and $c_\mathrm{out}(\sigma_{\mathrm{min}})=0$.

\section{Consistency Lower Bound}
\label{app:elbo}
In this section we prove the result from equation \ref{eq: VAE1 loss}, where we show how CT with variational noise coupling is also a lower bound of the evidence. To begin with, we start by rewriting the evidence lower bound derivation from VAEs:
\begin{align}
    \log p_{\vect{\theta}}(\vect{x}_0) &= \log \int p_{\vect{\theta}}(\vect{x}_0, \vect{z}) \, d\vect{z} = \log \int p_{\vect{\theta}}(\vect{x}_0|\vect{z})p(\vect{z}) \, d\vect{z} \\
    &= \log \int q_{\vect{\phi}}(\vect{z}|\vect{x}_0) \frac{p_{\vect{\theta}}(\vect{x}_0, \vect{z})}{q_{\vect{\phi}}(\vect{z}|\vect{x}_0)} \, d\vect{z} \\
    & \leq \mathbb{E}_{q_{\vect{\phi}}(\vect{z}|\vect{x}_0)} \left[ \log \frac{p_{\vect{\theta}}(\vect{x}_0, \vect{z})}{q_{\vect{\phi}}(\vect{z}|\vect{x}_0)} \right] \\
    \mathcal{L}(\vect{x}_0; \vect{\theta}, \vect{\phi}) &= \mathbb{E}_{q_{\vect{\phi}}(\vect{z}|\vect{x}_0)} \left[ \log p_{\vect{\theta}}(\vect{x}_0|\vect{z}) \right] - \text{KL}(q_{\vect{\phi}}(\vect{z}|\vect{x}_0) \| p(\vect{z}))
\end{align}
where $\log p_{\vect{\theta}}(\vect{x}_0|\vect{z})$ is the Gaussian log-likelihood, which up to multiplicative constants, simplifies to:
\begin{align}
    \log p_{\vect{\theta}}(\vect{x}_0|\vect{z}) = \norm{\vect{x}_0 - \vect{f}_{\vect{\theta}}(\vect{z})}^2
\end{align}
where $\vect{z}$ is a sample obtained from the posterior distribution with the reparametrization trick:
\begin{align}
    \vect{z} = \vect{g}_{\vect{\phi}}^{\mu}(\vect{x}_0) + \vect{g}_{\vect{\phi}}^{\sigma}(\vect{x}_0) \vect{\epsilon}
\end{align}
In discretized CT, for the triangular inequality, we have that:
\begin{align}
   \norm{\vect{x}_0 - \vect{f}_{\vect{\theta}}(\vect{z},1)}^2 \leq 
    \sum_{i=0}^N \norm{\vect{f_\theta}(\vect{\psi}_{t_{i+1}}(\vect{x}_0; \vect{x}_1),t_{i+1}) - \vect{f}_{\vect{\theta}^-}(\vect{\psi}_{t_{i}}(\vect{x}_0; \vect{x}_1),t_{i})}^2.
\end{align}
It is now easy to see that the consistency loss with variational noise coupling is also a lower bound on the evidence:
\begin{align}
    \log p(\vect{x}_0) & \leq \mathbb{E}_{q(\vect{z}|\vect{x}_0)} \left[ \norm{\vect{x}_0-\vect{f}_{\vect{\theta}}(\vect{z})}^2 \right] - \text{KL}(q(\vect{z}|\vect{x}_0) \| p(\vect{z})) \\ 
    &\leq \mathbb{E}_{q(\vect{z}|\vect{x}_0),p(t_i)} \left[ \norm{\vect{f_\theta}(\vect{\psi}_{t_{i+1}}(\vect{x}_0; \vect{x}_1),t_{i+1}) - \vect{f}_{\vect{\theta}^-}(\vect{\psi}_{t_{i}}(\vect{x}_0; \vect{x}_1),t_{i})}^2 \right] - \text{KL}(q(\vect{z}|\vect{x}_0) \| p(\vect{z}))
\end{align}
\section{Experimental details}
\subsection{Baselines}
\label{app:baselines}
Here we recap the detials about the two baselines used in this work, the Improved Consistency Training from \citep{songimproved} and Easy Consistency Models from \citep{geng2024consistency}.

\textbf{iCT}: the training procedure uses a discretization of time steps between two values $\sigma_{\mathrm{min}}=0.002$ and $\sigma_{\mathrm{max}}=80$, with the equation from \citep{karras2022elucidating}:
\begin{align}
    \sigma_i = \left( \sigma_{\min}^{1/\rho} + \frac{i-1}{N(k)-1} \left( \sigma_{\max}^{1/\rho} - \sigma_{\min}^{1/\rho} \right) \right)^\rho, \text{ where } i \in [[1, N(k)]],
\end{align}
where $\rho=7$ and $N(k)$ is a scheduler that defines the number of discretization steps at the $k$-th training iteration. $N(k)$ is chosen to be an exponential schedule which starts from $s_0=10$ steps and reaches $s_1=1280$ steps at the end of the training, and is defined as:
\begin{align}
    N(k) = \min(2^{\lceil k / K' \rceil}, s_1) + 1, \quad K' = \left\lceil \frac{K}{\log_2(s_1 / s_0) + 1} \right\rceil.
\end{align}
During training, time steps $t_i$ (or equivalently $\sigma_i$) are sampled following a discrete lognormal distribution:
\begin{align}
    p(\sigma_{i}) \propto \text{erf}\left(\frac{\log(\sigma_{i+1}) - P_{\text{mean}}}{\sqrt{2}P_{\text{std}}}\right) - \text{erf}\left(\frac{\log(\sigma_{i}) - P_{\text{mean}}}{\sqrt{2}P_{\text{std}}}\right),
\end{align}
with $P_{\text{mean}}=-1.1$ and $P_{\text{std}}=2.0$. Then, the steps $t_i$ and $t_{i+1}$ are used in the loss:
\begin{align}
    \mathcal{L}_{\mathrm{ct}}(\vect{\theta, \phi})\leftarrow \lambda_{\mathrm{ct}}(t_i)d(\vect{f_{\theta}}(\vect{x}_{t_{i+1}}, t+1),\vect{f_{\theta^-}}(\vect{x}_{t_i}, t_i)),
\end{align}
whith the time dependent weighting function $\lambda_{\mathrm{ct}}(t_i)=\frac{1}{t_{i+1}-{t_i}}$, and $d(.,.)$ is the Pseudo-Huber loss:
\begin{align}
    d(\vect{x},\vect{y}) = \sqrt{||\vect{x}-\vect{y}||_{2}^{2}+c^{2}} - c.
\end{align}
\\
\textbf{ECM}: ECM aims to simplify and improve the training procedure from iCT. We report here the main differences. Instead of using a discretized grid of time steps, it samples time steps $t$ from a continuous lognormal distribution with  $P_{\text{mean}}=-1.1$ and $P_{\text{std}}=2.0$ ($-0.8$ and $1.6$ for ImageNet). The second time step $r$ used in the discretized training objective is then obtained with a mapping function 
\begin{align}
    p(r|t, \text{iters}) = 1 - \frac{1}{q^a}n(t) = 1 - \frac{1}{q^{\lceil \text{iters}/d \rceil}}n(t),
\end{align}
where $n(t) = 1 + k\sigma(-bt) = 1 + \frac{k} {1+e^{bt}}$, $\sigma(.)$ is the sigmoid function, \textit{iter} is the current training iteration, $k=8$, $b=1$, and $q=2$ for all the models but ImageNet, where $q=4$. The discretization step is made smaller for eight times over training (four times for ImageNet). The loss function is a generalization of the Pseudo-Huber loss, which consists of the L2 loss and an adaptive weighting function $w(\Delta)$. The models are initialized with the weights of pretrained diffusion models, which is shown to greatly improve stability during training and generation performance.

\subsection{Training details}
\label{app:exp}

We report the training details for our models in Tables \ref{app:tab_ict} and \ref{app:tab_ecm}. Note that the baselines are the ones from our reimplementation. The models have the same number of parameters and training hyperparameters regardless of the transition kernel used. In the following, we report additional information important for reproducing out experiments:

\textbf{ECM-LI}: In ECM, the time steps $t$ ar sampled from a lognormal distribution, as done in \citep{karras2022elucidating}. This means that time steps $t>\sigma_{\mathrm{max}}$ can be sampled during training. While this works well when using the variance exploding Kernel, in the linear interpolantion case the time step $t$ cannot exceed $\sigma_{\mathrm{max}}$, and we therefore clip $t$ to be at most $\sigma_{\mathrm{max}}$.

\textbf{Random seeds}: All the training runs are initialized with random seed 42. For sampling and FID computation, we always set the random seed to 32, which was randomly chosen. This differs from what commonly done in EDM, where three different seeds are used to evaluate FID and the best result is reported. While our evaluation can lead to slightly worse results, the evaluation is consistent between our models and reimplemented baselines.

\textbf{2-steps generation}: Like in the original iCT baseline, all the models use $t=0.821$ for CIFAR10 and all the other datasets but ImageNet, where $t=1.526$ is used insetad.

\textbf{Data augmentation}: We scale all the images to have values between $-1$ and $1$. For CIFAR10 we apply random horizontal flip with $50\%$ probability.

\textbf{Differences for ImageNet}: The training procedure for ECM on ImageNet differs slightly from the one for the other datasets. The Adam optimizer is used instead of RAdam, with betas$=(0.9,0.99)$, and inverse square root learning rate decay defined as a function of the current training iteration $i$:
\begin{align}
    \alpha(i) = \frac{\alpha_{\text{ref}}}{\sqrt{\max(i/i_{\text{ref}},1)}},
\end{align}
with $\alpha_{\text{ref}}=0.001$ (the initial learning rate) and $i_{\text{ref}}=2000$ iterations. The Exponential Moving Average uses the power function averaging profile introduced in \citep{karras2024analyzing}. In ECM, three different EMA profiles are tracked during training, with rates $0.01$, $0.05$, and $0.1$. In our reimplementation, we only use the rate $0.1$. The number of times in which the discretization interval changes is reduced from $8$ to $4$, and the loss constant $c$ is set to $0.06$.


\begin{table}[h!]
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{lcc}
\textbf{Model Setups} & \textbf{FashionMNIST} & \textbf{CIFAR10} \\
\hline
Model Architecture & DDPM++ & DDPM++\\
Model Channels & 64 & 128 \\
N$^\circ$ of ResBlocks & 4 & 4 \\
Attention Resolution & - & 16 \\
Channel multiplyer & $[2, 2, 2]$ & $[2, 2, 2]$ \\
Model capacity & 13.6M & 55.7M\\
\hline
\textbf{Training Details} & & \\
Minibatch size & 128 & 1024\\
Batch per device & 128 & 512\\
Iterations & 400k & 400k\\
Dropout probability & 30\% & 30\% \\
Optimizer & RAdam & RAdam \\
Learning rate & 0.0001 & 0.0001 \\
EMA rate & 0.9999 & 0.9999 \\
\hline
\textbf{Training Cost} & & \\
Number of GPUs & 1 & 2\\
GPU types & H100 & H100\\
Training time (hours) & 28 & 92 \\
Training time with OT (hours) & 29 & 95 \\
\hline
\textbf{Encoder Details} & & \\
Model Architecture & DDPM++ & DDPM++ \\
Model Channels & 32 & 32 \\
N$^\circ$ of ResBlocks & 1 & 1 \\
Attention Resolution & - & 16 \\
Channel multiplyer & $[2, 2, 2]$ & $[2, 2, 2]$ \\
$\beta$ regularizer & 30 & 30 \\
Encoder Params & 1.5M & 1.6M\\
Training time with Encoder (hours) & 34 & 102 \\
\end{tabular}%
}
\caption{Model Configurations and Training Details for iCT on FashionMNIST and CIFAR10}
\label{app:tab_ict}
\end{table}

\begin{table}[h!]
\centering
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lccc}
\textbf{Model Setups} & \textbf{CIFAR10} & \textbf{FFHQ $64\times46$} & \textbf{ImageNet $64 \times 64$}\\
\hline
Model Architecture & DDPM++ & DDPM++ & EDM2-S\\
Model Channels & 64 & 128 & 192 \\
N$^\circ$ of ResBlocks & 4 & 4 & 3\\
Attention Resolution & [16] & [16] & [16, 8] \\
Channel multiplyer & $[2, 2, 2]$ & $[1, 2, 2, 2]$ & $[1, 2, 3, 4]$\\
Model capacity & 55.7M & 61.8M & 280M\\
\hline
\textbf{Training Details} & & \\
Minibatch size & 128 & 128 & 128\\
Batch per device & 128 & 128 & 128\\
Iterations & 400k & 400k & 200k\\
Dropout probability & 20\% & 20\% & 40\% (res $\leq16$)\\
Optimizer & RAdam & RAdam & Adam \\
Learning rate & 0.0001 & 0.0001 & 0.001 \\
EMA rate & 0.9999 & 0.9999 & 0.1 \\
\hline
\textbf{Training Cost} & & \\
Number of GPUs & 1 & 1 & 1\\
GPU types & H100 & H100 & H100\\
Training time (hours) & 37 & 95 & 51 \\
Training time with OT (hours) & 38 & 96 & 52 \\
\hline
\textbf{Encoder Details} & & \\
Model Architecture & DDPM++ & DDPM++ & EDM2-S \\
Model Channels & 32 & 32 & 32\\
N$^\circ$ of ResBlocks & 1 & 1 & 2\\
Attention Resolution & [16] & [16] & [16, 8] \\
Channel multiplyer & $[2, 2, 2]$ & $[1, 2, 2, 2]$ & $[1, 2, 3, 4]$\\
$\beta$ regularizer & 10 & 10 & 100 (VE), 90 (LI)\\
Encoder Params & 1.6M & 1.6M & 6M \\
Training time with Encoder (hours) & 49 & 110 & 58\\
\end{tabular}%
}
\caption{Model Configurations and Training Details for ECM on CIFAR10 , FFHQ $64\times46$ and ImageNet $64 \times 64$}
\label{app:tab_ecm}
\end{table}
\clearpage
\section{Toy experiments}
\label{app:toy}
To gain a visual understanding of the benefits of the variational coupling, we use the model to learn the distribution of a mixture of two Gaussians, with means $\mu_1=(0, 0.5)$ and $\mu_2=(0, -0.5)$, and standard deviation $\sigma=0.05$. We use iCT-LI so that the perturbed data reaches the prior even with small $\sigma_{\mathrm{max}}$, with $\sigma_\mathrm{min}=0.002$, $\sigma_{\mathrm{max}}=0.1$ and $\sigma_{\mathrm{data}}=0.05$. The models are trained for 40k iterations, with $s_0=10$ and $s_1=80$, and EMA rate 0.999. We use a simple four-layers MLP with GeLU activation and Positional time embedding, with batch size 256 and learning rate $1e^{-4}$. For iCT-LI-VC we use $\beta=0.001$. The results are shown in figures \ref{fig:toy_samples_1} and \ref{app:toy_samples_2} (one and two step generation respectively).

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/toy/2_step_ind.png}
    \includegraphics[width=0.7\linewidth]{figures/toy/2_step_vae.png}
    \caption{2-step generation result on the toy data, with $t=0.07$.}
    \label{app:toy_samples_2}
\end{figure}
\clearpage
\section{Model diagram}
\label{app:diag}
In figure \ref{app:graph} we show the difference between the forward process for standard Consistency Training and for our method with learned noise coupling, for a given time step $t$ and transition kernel characterized by the coefficients $a_t$ and $b_t$.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/graph/vct.pdf}
    \caption{Diagram for Consistency Training with independent coupling (left) and variational coupling (right).}
    \label{app:graph}
\end{figure}
\section{Qualitative Results}
\label{app:qualitative}
Here we report samples from our best models, iCT-LI-VC for FashionMNIST (figure \ref{app:samples_fmnist}), iCT-VE-VC and ECM-VE-VC on CIFAR10 (figures \ref{app:samples_cifar1} and \ref{app:samples_cifar2}), ECM-VE-VC on FFHQ $64\times 64$ (figure \ref{app:samples_ffhq}) and ECM-LI-VC on class conditional Imagenet $64\times64$ (figure \ref{app:samples_imnet}). In figure \ref{fig:rec}, we show the mean and standard deviation learned by the encoder for some images from the CIFAR10 dataset. While the values are very close to a standard Gaussian, the model still retains information from the original input. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/reconstructions/reconstructions.pdf}
    \caption{Visualization of the predicted mean and standard deviation for a trained iCT-VE-VC model for different input images. For visualization purpose, we perform min-max rescaling for the predicted mean and standard deviation, as they tend to have most values close to zero and one respectively. We also turn the predicted 3 channels standard deviations to a single channel with grayscale transform.}
    \label{fig:rec}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.49\linewidth]{figures/samples/fmnist1.png}
    \includegraphics[width=0.49\linewidth]{figures/samples/fmnist2.png}
    \caption{1-step (FID=$3.62$, left) and 2-step (FID=$2.22$, right) generation from iCT-LI-VC trained on FashionMNIST.}
    \label{app:samples_fmnist}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.49\linewidth]{figures/samples/cifar1_1.png}
    \includegraphics[width=0.49\linewidth]{figures/samples/cifar1_2.png}
    \caption{1-step (FID=$2.86$, left) and 2-step (FID=$2.32$, right) generation from iCT-VE-VC trained on CIFAR10.}
    \label{app:samples_cifar1}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.49\linewidth]{figures/samples/cifar2_1.png}
    \includegraphics[width=0.49\linewidth]{figures/samples/cifar2_2.png}
    \caption{1-step (FID=$3.26$, left) and 2-step (FID=$2.02$, right) generation from ECM-VE-VC trained on CIFAR10.}
    \label{app:samples_cifar2}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.49\linewidth]{figures/samples/ffhq1.png}
    \includegraphics[width=0.49\linewidth]{figures/samples/ffhq2.png}
    \caption{1-step (FID=$5.47$, left) and 2-step (FID=$4.16$, right) generation from ECM-VE-VC trained on FFHQ$64\times64$.}
    \label{app:samples_ffhq}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.49\linewidth]{figures/samples/imagenet_1.png}
    \includegraphics[width=0.49\linewidth]{figures/samples/imagenet_2.png}
    \caption{1-step (FID=$4.93$, left) and 2-step (FID=$3.07$, right) generation from ECM-LI-VC trained on class conditional ImageNet $64\times64$.}
    \label{app:samples_imnet}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[ht!] 
%     \centering
%     \includegraphics[width=0.48\linewidth]{figures/toy/toy_ot_ve.png}
%     \includegraphics[width=0.48\linewidth]{figures/toy/toy_ot_fm.png}
%     \caption{\jcc{At the first glance, this toy example is kinda arguing which forward kernel is better for coupling, but it is not showing our method can sort out coupling automatically comparing to fixed encoder?}   
%     \jcc{perhaps the purpose of the toy example is to show our VCT training can sort out coupling compared to fixed encoder baselines. Is there anything we can say comparing our VCT with \cite{pooladian2023multisample}??}  Toy problem where the data is sampled from the distribution $p(\vect{x}_0) = 0.5*\delta(\vect{x} - 1) + 0.5*\delta(\vect{x} +1)$. A simple consistency model is set up with $\sigma_{\text{max}}=1$. We sample $\vect{x}\sim p(\vect{x}_0)$ and $\vect{x}_1\sim\mathcal{N}(\rmzr,\sigma_{\text{max}}^2\rmI)$ and use the OT coupling $\pi(\vect{x}, \vect{x}_1)$ from \cite{pooladian2023multisample} to pair data with noise. Finally, we use the Variance Exploding (left) and Conditional OT (right) transition kernels for $t=\sigma_{\text{max}}$. From the figure it is clear that the coupling is wrong for the VE kernel, as the data can never be mapped to the interval $(-1,1)$.}
%     \label{fig:toy_ot}
% \end{figure}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
