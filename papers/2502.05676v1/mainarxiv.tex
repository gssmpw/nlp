%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
 

 
\usepackage{thmtools}
\usepackage{enumitem}
\usepackage{amsfonts}


\usepackage{bbm} 

 
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\essinf}{ess\,inf}
 
\DeclareMathOperator*{\logit}{logit}
\DeclareMathOperator*{\expit}{expit}
\renewcommand{\cite}{\citet}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Generalized Venn and Venn-Abers Calibration}

\begin{document}

\twocolumn[
\icmltitle{Generalized Venn and Venn-Abers Calibration \\
with Applications in Conformal Prediction}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lars van der Laan}{uw}
\icmlauthor{Ahmed Alaa}{ucb}
\end{icmlauthorlist}

\icmlaffiliation{uw}{Department of Statistics, University of Washington}
\icmlaffiliation{ucb}{Computational Precision Health, UC Berkeley and UCSF}
 

\icmlcorrespondingauthor{Lars van der Laan}{lvdlaan@uw.edu}

 

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Calibration, multicalibration, Venn-Abers, conformal prediction,  isotonic calibration, distribution-free, epistemic uncertainty}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

 \ 

\begin{abstract}%
 
%Model calibration ensures reliable predictions and supports informed decision-making. Distribution-free binning calibrators, such as histogram binning and isotonic calibration, provide only asymptotic calibration and may fail in finite samples. 

Ensuring model calibration is critical for reliable predictions, yet popular distribution-free methods, such as histogram binning and isotonic regression, provide only asymptotic guarantees. We introduce a unified framework for \textit{Venn} and \textit{Venn-Abers} calibration, generalizing Vovk's binary classification approach to arbitrary prediction tasks and loss functions.~\textit{Venn} calibration leverages binning calibrators to construct prediction sets that contain at least one marginally perfectly calibrated point prediction in finite samples, capturing epistemic uncertainty in the calibration process. The width of these sets shrinks asymptotically to zero, converging to a conditionally calibrated point prediction. Furthermore, we propose \textit{Venn multicalibration}, a novel methodology for finite-sample calibration across subpopulations. For quantile loss, group-conditional and multicalibrated conformal prediction arise as special cases of \textit{Venn} multicalibration, and Venn calibration produces novel conformal prediction intervals that achieve quantile-conditional coverage. As a separate contribution, we extend distribution-free conditional calibration guarantees of histogram binning and isotonic calibration to general losses.

\end{abstract}

 \footnotetext[1]{Department of Statistics, University of Washington}
\footnotetext[2]{Computational Precision Health, UC Berkeley and UCSF}

\section{Introduction}
Calibration is essential for ensuring that machine learning models produce reliable predictions and enable robust decision-making across diverse applications. Model calibration aligns predicted probabilities with observed event frequencies and predicted quantiles with the specified proportion of outcomes. In safety-critical sectors such as healthcare, it is crucial to ensure that model-driven decisions are reliable under minimal assumptions \citep{mandinach2006theoretical, veale2018fairness, vazquez2022conformal}. \textit{Point calibrators}, which map a single model prediction to a single calibrated prediction (e.g., histogram binning and isotonic regression), can provide distribution-free calibration conditional on the calibration data. However, their guarantees are asymptotic, achieving zero calibration error only in the limit, and their performance may degrade in finite samples.


To address these limitations, \textit{set calibrators} transform~a~single model prediction into a set of calibrated point predictions, explicitly capturing epistemic uncertainty in the calibration process. This class of methods includes Venn and Venn-Abers calibration \citep{vovk2003self, vovk2012venn, vanself} for classification and regression, and Conformal Prediction (CP) for quantile regression and predictive inference \citep{vovk2005algorithmic}. Set calibrators provide prediction sets with finite-sample marginal calibration guarantees, while these sets generally converge asymptotically to conditionally calibrated~point~predictions. For example, Venn calibration produces a set of calibrated probabilities, and conformal prediction generates a set of calibrated quantiles (from which an interval can be derived), ensuring that at least one prediction in the set is perfectly calibrated marginally over draws of calibration data.

\begin{figure}[t]
\centering
\includegraphics[width=0.475\linewidth]{vennabers_calibration_plot.pdf}\includegraphics[width=0.475\linewidth]{venn_multicalibration_plot.pdf}
\vspace{-0.125in}
\caption{Prediction bands capturing epistemic uncertainty in the calibration of point predictions generated by our proposed methods with a squared error loss.}
\vspace{-0.25in}
\end{figure}
 
\textbf{Our Contributions.} ~We introduce a unified~framework~for Venn and Venn-Abers calibration, generalizing \cite{vovk2012venn} to arbitrary prediction tasks and loss functions. This framework extends point calibrators, e.g., histogram binning and isotonic regression, to produce prediction sets with finite-sample marginal and large-sample conditional calibration guarantees to capture epistemic uncertainty. We further propose \textit{Venn multicalibration}, ensuring finite-sample calibration across subpopulations. For quantile loss, we show that Venn calibration corresponds to a novel CP procedure with quantile-conditional coverage, and that multicalibrated CP \citep{gibbs2023conformal} is a special case of~Venn~multicalibration, unifying and extending existing calibration methods.

 
\section{Preliminaries for loss calibration}
\subsection{Notation}
We consider the problem of predicting an outcome \( Y \in \mathcal{Y} \) from a context \( X \in \mathcal{X} \), where \( Z := (X, Y) \) is drawn from an unknown distribution \( P := P_X P_{Y \mid X} \), on which we impose no distributional assumptions. Let \( f: \mathcal{X} \to \mathcal{Y} \) denote a predictive model trained to minimize a loss function \( (f(x), z) \mapsto \ell(f(x), z) \) using a machine learning algorithm on training data. For example, $\ell(f(x),z)$ could be the squared error loss $\{y - f(x)\}^2$ or the $(1-\alpha)$-quantile loss $\mathbbm{1}\{y \geq f(x)\}  \alpha (y - f(x)) + \mathbbm{1}\{y < f(x)\} (1-\alpha) (f(x) - y)$. We assume access to a calibration dataset \( \mathcal{C}_n = \{(X_i, Y_i)\}_{i=1}^{n} \) of \( n \) i.i.d. samples drawn from the same distribution \( P \), independent from the training data. Throughout this work, we treat the model \( f \) as fixed, implicitly conditioning on its training process.  

\subsection{Defining calibration for general losses}
 
Machine learning models, such as neural networks and gradient-boosted trees, are powerful predictors but often produce biased point predictions due to model misspecification, distribution shifts, or limited data \citep{zadrozny2001obtaining, niculescu2005obtaining, bella2010calibration, guo2017calibration, davis2017calibration}. To ensure reliable decision-making, we seek \emph{calibrated} models \citep{roth2022uncertain}. Informally, calibration means that the predictions are optimal conditional on the prediction itself, so they cannot be improved by applying a transformation to reduce the loss. Formally, a model $f$ is perfectly $\ell$-calibrated if \citep{noarov2023scope, whitehouse2024orthogonal}
\begin{equation}
    E_P[\ell(f(X), Z)] = \inf_{\theta} E_P[\ell(\theta(f(X)), Z)],
\end{equation}
where the infimum is taken over all one-dimensional transformations of $f$. Perfect calibration implies that $f(x) = \argmin_{c \in \mathbb{R}} E_P[\ell(c, Z) \mid f(X) = f(x)]$ for all $x \in \mathcal{X}$. We assume that the loss \( \ell \) is smooth, such that \(\ell\)-calibration is equivalent to satisfying the first-order conditions \citep{whitehouse2024orthogonal}:
\begin{equation}
    E_P[\partial \ell(f(X), Z) \mid f(X) = f(x)] = 0   \text{ for all } x \in \mathcal{X},
\end{equation}
where \( \partial \ell(f(x), y) := \frac{d}{d\eta} \ell(\eta, y) \mid_{\eta = f(x)} \) denotes the partial derivative of \( \ell \) with respect to its first argument \( f(x) \).

In regression, with $\ell$ taken as the squared error loss, \( f \) is \emph{perfectly calibrated} if \( E_P[Y \mid f(X) = f(x)] = f(x) \) for all \( x \in \mathcal{X} \) \citep{lichtenstein1977calibration, gupta2020distribution}, a property also known as self-consistency \citep{flury1996self}. This property addresses systematic over- or under-estimation and ensures that \( f(X) \) is a conditionally unbiased proxy for \( Y \) in decision making. In binary classification, calibration ensures that the score \( f(X) \) can be interpreted as a probability, making decision rules such as assigning label $1$ when \( f(X) > 0.5 \) valid on average, since \( P(Y=1 \mid f(X) > 0.5) > 0.5 \).


For a model $\widehat{f}$, which depends randomly on the calibration set $\mathcal{C}_n$, we define two types of calibration: marginal and conditional \citep{vovk2012conditional}. A random model $\widehat{f}$ is \textit{conditionally} (perfectly) $\ell$-calibrated if it is calibrated conditional on the data $\mathcal{C}_n$: $ E_P[\partial \ell(\widehat{f}(X), Z) \mid \widehat{f}(X), \mathcal{C}_n]  = 0$ almost surely. The model $\widehat{f}$ is \textit{marginally} $\ell$-calibrated if it is calibrated marginally over $\mathcal{C}_n$: $\mathbb{E}[\partial \ell(\widehat{f}(X), Z) \mid \widehat{f}(X)]  = 0,$
where $\mathbb{E}$ is taken over the randomness in both $(X,Y)$ and $\mathcal{C}_n$.



\subsection{Post-hoc calibration via point calibrators}

Models trained for predictive accuracy often require post-hoc calibration, typically using an independent calibration dataset or, in some cases, the training data \citep{gupta2021distribution}. Post-hoc methods treat prediction and calibration as distinct tasks, each optimized for a different yet complementary objective. Since perfect calibration is unattainable in finite samples, the goal of calibration is to obtain a model \(\widehat{f}\) with minimal calibration error relative to a chosen metric, such as the conditional $\ell^2$ calibration error $\text{Cal}_{\ell^2}(\widehat{f})$, defined as \citep{whitehouse2024orthogonal}:
\begin{equation*}
 \int \left\{E_P[\partial \ell(\widehat{f}(X), Z) \mid \widehat{f}(X) = \widehat{f}(x), \mathcal{C}_n]\right\}^2 dP_X(x).
\end{equation*}


A \textit{point calibrator}, following \citet{van2023causal} and \citet{gupta2020distribution}, is a post-hoc procedure that learns a transformation \( \theta_n: \mathbb{R} \rightarrow \mathbb{R} \) of a black-box model \( f \) from $\mathcal{C}_n$ such that: (i) \( \theta_n(f(X)) \) is well-calibrated with low calibration error, and (ii) \( \theta_n(f(X)) \) remains comparably predictive to \( f(X) \) in terms of the loss $\ell$. Common point calibrators include Platt scaling \citep{platt1999probabilistic, cox1958two}, histogram binning \citep{zadrozny2001obtaining}, and isotonic calibration \citep{zadrozny2002transforming}.  

A~calibrated predictor can be constructed from $\mathcal{C}_n$ by learning the calibrator \( \theta_n \) via minimizing the empirical risk \( \sum_{i=1}^n \ell( \theta(f(X_i)), Z_i) \).~For regression, this involves regressing outcomes \( \{Y_i\}_{i=1}^n \) on predictions \( \{f(X_i)\}_{i=1}^n \) \citep{mincer1969evaluation}.~If \( \theta_n \) accurately estimates the conditional calibration function \( \min_{\theta} E_P[\ell(\theta(f(X)), Z) \,| \, \mathcal{C}_n] \), the predictor is well-calibrated by the tower property. This approach is not distribution-free and typically requires correct model specification. Methods like kernel smoothing and Platt scaling impose smoothness or parametric assumptions on the calibration function \citep{jiang2011smooth}.

%Assuming we can exchange the order of integration and differentiation,  Theorem \ref{theorem::multiVennGeneralLoss} implies that  
%\begin{align*}
%  \frac{\partial}{\partial t}   \mathbb{E}\left[ \ell(Z_{n+1},f_{n+1}^* + g) \right]  \Big |_{t = 0}   \,=\,0\, \text{ for all } g \in \mathcal{G}.
%\end{align*}
%In view of these first-order optimality conditions, we find that $f_{n+1}^*$ satisfies the self-consistency condition:
 


\subsection{Distribution-free calibration via histogram binning}
A simple class of distribution-free calibration methods is histogram binning, such as uniform mass (quantile) binning \citep{gupta2020distribution, gupta2021distribution}. Here, the prediction space \( f(\mathcal{X}) \) is partitioned into \( K \) bins $\{B_k\}_{k=1}^K$ in an outcome-agnostic manner, often by taking quantiles of the predictions \( \{f(X_i)\}_{i=1}^n \) in \( \mathcal{C}_n \). The binning calibrator \( \theta_n \) is a step function obtained via histogram regression:
\[
\theta_n(t) := \min_{c \in \mathbb{R}} \sum_{i=1}^n \mathbbm{1}\{f(X_i) \in B_{k(t)}\} \ell(c, Z_i),
\]
where \( k(t) \) indexes the bin containing \( t \in f(\mathcal{X}) \). For the squared error loss, \( \theta_n(t) \) is the empirical mean of the outcomes \( \{Y_i : i \in [n], f(X_i) \in B_{k(t)}\} \) within the bin \( B_{k(t)} \). The histogram regression property ensures that the resulting calibrated model \( f_n^* := \theta_n \circ f \) is \textit{$\ell$-empirically calibrated} \citep{van2024automatic}, meaning that, for each \( x \in \mathcal{X} \),
\begin{equation}
\sum_{i=1}^n \mathbbm{1}\{f_n^*(X_i) = f_n^*(x)\} \partial \ell(f_n^*(x), Z_i) = 0,\label{eqn::perfectcal}
\end{equation}
implying that its empirical risk cannot be improved by any transformation of its predictions.

Empirical calibration does not guarantee good out-of-sample calibration or predictive performance. With a maximal partition \( K = n \), perfect empirical calibration leads to overfitting, resulting in poor out-of-sample performance. Conversely, with a minimal partition \( K = 1 \), the model is well-calibrated but poorly predictive, yielding a constant predictor \( \min_{c \in \mathbb{R}} \sum_{i=1}^n \ell(c, Z_i) \). This illustrates a trade-off: too few bins reduce predictive power, while too many increase variance and degrade calibration. Histogram binning asymptotically provides conditionally calibrated predictions, with the conditional $\ell^2$ calibration error satisfying \( \text{Cal}_{\ell^2}(f_n^*) = O_p\left(\frac{K \log (n/K)}{n}\right) \) \citep{whitehouse2024orthogonal}. Thus, tuning of \( K \), for example via cross-validation, is crucial to balance calibration and predictiveness. 

\section{Generalized Venn calibration framework}

\subsection{Venn calibration for general losses}

Suppose we are interested in obtaining an $\ell$-calibrated prediction $f(X_{n+1})$ for an unseen outcome $Y_{n+1}$ from a new context $X_{n+1}$, where $(X_{n+1}, Y_{n+1})$ is drawn from $P$ and independent of the calibration data $\mathcal{C}_n$. Let $\mathcal{A}_{\ell}$ be any point calibration algorithm that takes a model $f$ and calibration data $\mathcal{C}_n$ and outputs a refined model $f_n^* := \mathcal{A}_{\ell}(f, \mathcal{C}_n)$ that is $\ell$-empirically calibrated in the sense of \eqref{eqn::perfectcal}. For example, $\mathcal{A}_{\ell}(f, \mathcal{C}_n)$ could be defined as $\theta_n \circ f$, where $\theta_n$ is learned using an outcome-agnostic histogram binning method, such as uniform mass binning, or an outcome-adaptive method, such as a regression tree or isotonic regression. We assume that the algorithm $\mathcal{A}_{\ell}$ processes input data exchangeably, ensuring the calibrated predictor is invariant to permutations of the calibration data.

Distribution-free binning-based point calibrators, like histogram binning and isotonic regression, achieve perfect empirical calibration on calibration data but may exhibit poor conditional calibration in finite samples, attaining population-level calibration only asymptotically. To address this, \citet{vovk2003self} proposed Venn calibration for binary classification, which transforms point calibrators into set calibrators that ensure finite-sample marginal calibration while retaining conditional calibration asymptotically.

In this section, we extend Venn calibration to general prediction tasks defined by loss functions. We demonstrate that Venn calibration can be applied to any point calibrator that achieves perfect $\ell$-\textit{empirical} calibration to ensure finite-sample \textit{marginal} calibration. Unlike traditional point calibrators, which produce a single calibrated prediction for each context, Venn calibration generates a prediction set that is guaranteed to contain a marginally perfectly $\ell$-calibrated prediction. This set reflects epistemic uncertainty by covering a range of possible calibrated predictions, each of which remains asymptotically conditionally $\ell$-calibrated. A prominent example is Venn-Abers calibration, which uses isotonic regression as the underlying point calibrator \citep{vovk2012venn, vanself}. 

Our generalized Venn calibration procedure is detailed in Alg.~\ref{alg::venn}. A distinctive aspect of Venn calibration is that it adapts the model $f$ specifically for the given context $X_{n+1}$, unlike point calibrators, which produce a single calibrated model intended to be (asymptotically) valid across all contexts. For a given context $X_{n+1}$, the algorithm iteratively considers imputed outcomes \( y \in \mathcal{Y} \) for \( Y_{n+1} \) and applies the calibrator $\mathcal{A}_{\ell}$ to the augmented dataset \mbox{\footnotesize\(\mathcal{C}_n \cup \{(X_{n+1}, y)\}\)}. This process yields a set of point predictions:
\[
f_{n, X_{n+1}}(X_{n+1}) := \{f_{n}^{(X_{n+1},y)}(X_{n+1}) : y \in \mathcal{Y}\}.
\]
When the outcome space \(\mathcal{Y}\) is continuous, Alg.~\ref{alg::venn} may be computationally infeasible to execute exactly and can instead be approximated by discretizing \(\mathcal{Y}\). Nonetheless, the range of the prediction set \(f_{n, x}(x)\) can often be computed by iterating over the extreme points \(\{y_{\text{min}}, y_{\text{max}}\}\) of \(\mathcal{Y}\).

\begin{algorithm}[htb!]
\caption{Venn loss calibration}
\label{alg::venn}
\begin{algorithmic}[1]{\footnotesize
\REQUIRE Calibration data~\mbox{\footnotesize $\mathcal{C}_n= \{(X_i, Y_i)\}_{i=1}^n$}, model \mbox{\footnotesize $f$}, context \mbox{\footnotesize $x \in \mathcal{X}$}, loss calibrator $\mathcal{A}_{\ell}$.

\vspace{.02in}
 
\vspace{.05in}
\FOR{each $y \in \mathcal{Y}$}
\vspace{.025in}
\STATE \mbox{\footnotesize augment dataset: $\mathcal{C}_{n}^{(x,y)} := \mathcal{C}_n \cup \{ (x, y)\}$}; \\
\STATE \mbox{\footnotesize calibrate model: $f_{n}^{(x,y)} := \mathcal{A}_{\ell}(f, \mathcal{C}_{n}^{(x,y)})$;} \\
\vspace{.025in}
\ENDFOR
\STATE set $f_{n, x}(x) := \{f_{n}^{(x,y)}(x) : y \in \mathcal{Y} \}$;
\ENSURE \mbox{\footnotesize prediction set $f_{n, x}(x)$}}.
\end{algorithmic}
\end{algorithm}


To establish the validity of Venn calibration, we impose the following conditions, which ensure that the data are exchangeable ---   a common assumption in conformal prediction \citep{vovk2005algorithmic}, particularly satisfied when the data are i.i.d --- and that the derivative of the loss has a finite second moment. 

 \begin{enumerate}[label=\bf{C\arabic*)}, ref={C\arabic*}, series=theorem]
  \item \textit{Exchangeability:} $\{(X_i, Y_i)\}_{i=1}^{n+1}$ are exchangeable. \label{cond::exchange}
 \item \textit{Finite variance:}\label{cond::variance}   $\mathbb{E}[\{\partial \ell(f_{n+1}^*(X_{n+1}), Z_{n+1})\}^2] < \infty$.  
    \item \textit{Perfect empirical calibration:} $ \sum_{i=1}^{n+1} \mathbbm{1}\{f_{n+1}^*(X_i) = f_{n+1}^*(x)\} \partial \ell(f_{n+1}^*(X_i), Z_i) = 0$ almost surely for each \( x \in \mathcal{X} \). \label{cond::empcal}
\end{enumerate}


The finite-sample validity of the Venn calibration procedure can be established through an \textit{oracle} procedure that assumes knowledge of the unseen outcome \( Y_{n+1} \). In this oracle procedure, a perfectly empirically $\ell$-calibrated prediction {\footnotesize \( f_{n+1}^*(X_{n+1}) := \mathcal{A}_{\ell}(f, \mathcal{C}_{n+1}^*)(X_{n+1}) \)} is obtained by calibrating \( f \) using \( \mathcal{A}_{\ell} \) on the oracle-augmented calibration set \( \mathcal{C}_{n+1}^* := \mathcal{C}_n \cup \{(X_{n+1}, Y_{n+1})\} \). By leveraging exchangeability, the empirical calibration of the oracle prediction \( f_{n+1}^*(X_{n+1}) \) ensures \textit{marginal} perfect $\ell$-calibration, such that \( \mathbb{E}[\partial \ell(f_{n+1}^*(X_{n+1}), Z_{n+1}) \mid f_{n+1}^*(X_{n+1})] = 0 \) almost surely. Since the oracle prediction is, by construction, contained in the Venn prediction set \( f_{n, X_{n+1}}(X_{n+1}) \), we conclude the following theorem.

\begin{theorem}[Marginal calibration of Venn prediction]
      \label{theorem::VennGeneral} Under \ref{cond::exchange}-\ref{cond::empcal}, the Venn prediction set $f_{n, X_{n+1}}(X_{n+1})$ contains the marginally perfectly $\ell$-calibrated prediction, $f_{n+1}^* (X_{n+1}) = f_n^{(X_{n+1}, Y_{n+1})}$, which satisfies 
   $$\mathbb{E} \left[\left\{\mathbb{E}[\partial \ell(f_{n+1}^*(X_{n+1}), Z_{n+1}) \mid f_{n+1}^*(X_{n+1})]  \right\}^2 \right] = 0.$$
 
\end{theorem}


 
   
 
 

In order to satisfy \ref{cond::empcal}, Algorithm~\ref{alg::venn} should be applied with a binning-based calibrator \( \mathcal{A}_{\ell} \) that achieves perfect empirical calibration, such as uniform mass binning, a regression tree, or isotonic regression. Importantly, this condition does not impose restrictions on how the bins are selected, allowing pre-specified and data-adaptive binning schemes. While the choice of binning calibrator does not affect the marginal perfect calibration guarantee in Theorem \ref{theorem::VennGeneral}, it influences both the width of the Venn prediction set and the conditional calibration of the point predictions. In general, using a point calibrator that ensures conditionally well-calibrated predictions, such as histogram binning with appropriately tuned bins or isotonic calibration, is recommended. 

For example, in the regression setting with squared error loss, Theorem \ref{theorem::VennGeneral} ensures that the set $f_{n, X_{n+1}}(X_{n+1})$ contains \( f_{n+1}^*(X_{n+1}) = \mathbb{E}[Y_{n+1} \mid f_{n+1}^*(X_{n+1})] \). However, using histogram binning with one observation per bin results in \( f_{n, X_{n+1}}(X_{n+1}) = \mathcal{Y} \), an uninformative set that poorly reflects conditional calibration despite including \( f_{n+1}^*(X_{n+1}) = Y_{n+1} \). In contrast, using a single bin produces a set containing the marginally perfectly calibrated prediction \( f_{n+1}^*(X_{n+1})  = \frac{1}{n+1}\sum_{i=1}^{n+1} Y_i \), where each prediction is close to the sample mean \( \frac{1}{n}\sum_{i=1}^n Y_i \), ensuring conditional calibration but resulting in poor predictiveness.



Suppose that $\mathcal{A}_{\ell}(f, \mathcal{C}_n \cup \{x,y\})$ outputs a transformed model $\theta_n^{(x,y)} \circ f$, where $\theta_n^{(x,y)}$ is learned using an outcome-agnostic or outcome-adaptive binning calibrator, such as quantile binning or a regression tree. The following theorem shows that each calibrated model \(f_{n}^{(x,y)} = \theta_{n}^{(x,y)} \circ f\), used to construct the Venn prediction set, is asymptotically conditionally calibrated, provided the calibration data are i.i.d. and the number of bins in the calibrator \( \theta_{n}^{(x,y)} \) does not grow too quickly.






 \begin{enumerate}[label=\bf{C\arabic*)}, ref={C\arabic*}, resume=theorem]
  \item \textit{Independence:} $\{(X_i, Y_i)\}_{i=1}^{n+1}$ are i.i.d. \label{cond::iid}
 \item  \textit{Boundedness:} \label{cond::bounded}   \( \esssup_{z'= (x',y')} |\partial \ell(\theta_{n}^{(x,y)}(f(x')), z')|  \) and \( \esssup_{x'} | \theta_{n}^{(x,y)}(x')|  \) are bounded by a constant \( M < \infty \).
  \item  \textit{Lipschitz derivative:} \label{cond::lipschitz} There exists $L < \infty$ such that $|\partial \ell(\eta_1,z) - \partial \ell(\eta_2,z)| \leq L |\eta_1 - \eta_2|$ for all $z, \eta_1, \eta_2$.
  \item  \textit{Finite number of bins:} $\theta_{n}^{(x,y)}$ is piecewise constant taking at most $k(n) < \infty$ values.  \label{cond::bins}

\end{enumerate}
 

 


\begin{theorem}[Conditional calibration of Venn calibration]
     \label{theorem::condcalVA} Under \ref{cond::iid}-\ref{cond::bins}, we have $\text{Cal}_{\ell^2}(f_{n}^{(x,y)}) = O_p(\frac{k(n) \log (n / k(n))}{n})$.
\end{theorem}



For stable point calibrators, as the calibration set size \( n \) increases, the Venn prediction set narrows and converges to a single perfectly $\ell$-calibrated prediction \citep{vovk2012venn}. In large-sample settings, where standard point calibrators perform reliably, the Venn prediction set becomes narrow, closely resembling a point prediction. In contrast, in small-sample settings, where overfitting can undermine the reliability of point calibrators such as histogram binning and isotonic calibration, the Venn prediction set widens, reflecting increased uncertainty about the true calibrated prediction \citep{johansson2023well}. Consequently, Venn calibration improves the robustness of the point calibration procedure \( \mathcal{A}_{\ell} \) by explicitly representing uncertainty through a set of possible calibrated predictions.




 
 
 
 


\subsection{Isotonic and Venn-Abers calibration}



Venn calibration can be applied with any loss calibrator \( \mathcal{A}_{\ell} \) that provides empirically calibrated predictions. While histogram binning requires pre-specifying the number of bins, isotonic calibration \citep{zadrozny2002transforming, niculescu2005obtaining} addresses this limitation by adaptively determining bins through isotonic regression, a nonparametric method for estimating monotone functions \citep{barlow1972isotonic}. Instead of fixing \( K \) in advance, isotonic calibration selects bins by minimizing an empirical MSE criterion, ensuring the calibrated predictor is a non-decreasing monotone transformation of the original predictor. Isotonic calibration allows the number of bins to grow with sample size, ensuring good calibration while preserving predictive performance. \citet{van2023causal} show that, in the context of treatment effect estimation, the conditional $\ell^2$ calibration error of isotonic calibration for i.i.d. data asymptotically satisfies \( \text{Cal}_{\ell^2}(f_n^*) = O_p(n^{-2/3}) \). 


  \begin{algorithm}[htb!]
\caption{Venn-Abers loss calibration}
\label{alg::VAgen}
\begin{algorithmic}[1]{\footnotesize
\REQUIRE Calibration data~\mbox{\footnotesize $\mathcal{C}_n= \{(X_i, Y_i)\}_{i=1}^n$}, model \mbox{\footnotesize $f$},  loss $\ell$, context \mbox{\footnotesize $x \in \mathcal{X}$}.

\vspace{.02in}
 
\vspace{.05in}
\FOR{each $y \in \mathcal{Y}$}
\vspace{.025in}
\STATE \mbox{\footnotesize augment dataset: $\mathcal{C}_{n}^{(x,y)} := \mathcal{C}_n \cup \{(X_{n+1}, Y_{n+1}) := (x, y)\}$}; \\
\STATE calibrate model using generalized isotonic regression: 

    \vspace{.05in}
    \,\mbox{\footnotesize $\theta_{n}^{(x,y)} := \argmin_{\theta \in \Theta_{\text{iso}}} \sum_{i \in \mathcal{C}_{n}^{(x,y)}} \ell( \theta(f(X_i)), Z_i)$}.

    
    $f_{n}^{(x,y)}: = \theta_{n}^{(x,y)} \circ f$.
    \vspace{.025in}\\
\vspace{.025in}
\ENDFOR
\STATE set $f_{n, x}(x) := \{f_{n}^{(x,y)}(x) : y \in \mathcal{Y} \}$;
\ENSURE \mbox{\footnotesize prediction set $f_{n, x}(x)$}}.
\end{algorithmic}
\end{algorithm}



In this section, we propose Venn-Abers calibration for general loss functions, a special instance of Venn calibration that employs isotonic regression as the underlying point calibrator, thereby generalizing the original procedure for classification and regression \citep{vovk2012venn, vanself}. Our generalized Venn-Abers calibration procedure is outlined in Alg.~\ref{alg::VAgen}. Isotonic regression is a stable algorithm, meaning small changes in the training set do not significantly affect the solution, ensuring that the Venn-Abers prediction set converges to a point prediction as the sample size grows \citep{caponnetto2006stability, bousquet2000algorithmic}. Consequently, the Venn-Abers prediction set inherits the marginal calibration guarantee of Venn calibration, while each point prediction in the set is conditionally calibrated in large samples.

Let \( f_{n, X_{n+1}}(X_{n+1}) \) denote the Venn-Abers prediction set obtained by applying Alg.~\ref{alg::VAgen} with \( x = X_{n+1} \). The following theorem follows directly from Theorem \ref{theorem::VennGeneral}.
 

  
 
 

\begin{theorem}[Marginal calibration of Venn-Abers]
      \label{theorem::vennabers} Under \ref{cond::exchange} and \ref{cond::variance}, the Venn prediction set $f_{n, X_{n+1}}(X_{n+1})$ contains the marginally perfectly $\ell$-calibrated prediction, $ f_{n+1}^*(X_{n+1}) := f_n^{(X_{n+1}, Y_{n+1})}$, which satisfies 
   $$\mathbb{E} \left[\left\{\mathbb{E}[\partial \ell(f_{n+1}^*(X_{n+1}), Z_{n+1}) \mid f_{n+1}^*(X_{n+1})]  \right\}^2 \right] = 0.$$
 
\end{theorem}

The following theorem establishes that each isotonic calibrated model \( f_{n}^{(x,y)} \) used to construct the Venn-Abers prediction set is asymptotically conditionally calibrated.




 

 \begin{enumerate}[label=\bf{C\arabic*)}, ref={C\arabic*}, resume=theorem]
  \item \textit{Best predictor of gradient has finite variation:} There exists an $B < \infty$ such that {\footnotesize $t \mapsto E_P[\partial \ell(f_{n}^{(x,y)}(X), Y) \mid f(X) = t, \mathcal{C}_n]$} has total variation norm that is almost surely bounded by $B$. \label{cond::TV}
\end{enumerate} 


\begin{theorem}[Conditional calibration of Venn-Abers]
     \label{theorem::condcalVA2} Under \ref{cond::iid}-\ref{cond::lipschitz}, and \ref{cond::TV}, we have $\text{Cal}_{\ell^2}(f_{n}^{(x,y)}) = O_p(n^{-2/3})$.
\end{theorem}

This theorem generalizes the distribution-free conditional calibration guarantees for isotonic calibration of \cite{van2023causal} and \cite{van2024stabilized} for regression and inverse probabilities to general losses.






As discussed in \cite{vanself}, the main computational cost of Alg.~\ref{alg::VAgen} lies in the isotonic calibration step for each $y \in \mathcal{Y}$. Isotonic regression \citep{barlow1972isotonic} can be efficiently computed using \texttt{xgboost} \citep{xgboost} with monotonicity constraints. Similar to Full CP \citep{vovk2005algorithmic}, Alg.~\ref{alg::VAgen} may be infeasible for non-discrete outcomes and can be approximated by iterating over a finite subset of $\mathcal{Y}$ with linear interpolation for ${ f_n^{(x,y)}(x)}$. Like Full and multicalibrated CP \citep{gibbs2023conformal}, this algorithm must be applied per context $x \in \mathcal{X}$. Since the algorithms depend on $x$ only through $f(x)$, we can approximate outputs for all $x \in \mathcal{X}$ by running the algorithms on a finite grid of the 1D output space $f(\mathcal{X}) \subset \mathbb{R}$.




\subsection{Venn multicalibration for finite-dimensional classes}


Standard calibration ensures that predicted outcomes cannot be improved by any transformation of the predictions, making them optimal on average for contexts with the same predicted value. However, this aggregate guarantee can mask systematic errors within subgroups. Multicalibration extends standard calibration by enforcing calibration within specified subpopulations, ensuring fairness and reliability across groups \citep{jung2008moment, roth2022uncertain, noarov2023scope, deng2023happymap}. In this section, we introduce Venn multicalibration, a generalization of Venn calibration that provides calibration guarantees across multiple subpopulations. This approach produces prediction sets that contain a perfectly multicalibrated prediction in finite samples.


We say a model \( \widehat{f} \) is \textit{marginally perfectly $\ell$-multicalibrated} with respect to a function class \( \mathcal{G} \) if the following holds:
\begin{align}
    \mathbb{E}\left[\frac{\partial}{\partial t}\ell((\widehat{f} + t g)(X_{n+1}), Z_{n+1}) \mid_{t=0} \right] = 0  \text{ for all } g \in \mathcal{G},  \label{eqn::multicalgeneral}
\end{align}
where the expectation is taken over \( (X_{n+1}, Y_{n+1}) \) as well as randomness of \( \widehat{f} \). Assuming the order of integration and differentiation can be exchanged, this condition implies that
\begin{align*}
\mathbb{E}\left[\ell( \widehat{f}(X_{n+1}), Z_{n+1}) \right] =  \min_{g \in \mathcal{G}} \mathbb{E}\left[\ell((\widehat{f} +g)(X_{n+1})  , Z_{n+1}) \right].
\end{align*}
In other words, the loss \( \ell(\widehat{f}(X_{n+1}), Z_{n+1}) \) incurred~for~the new data point cannot be improved in expectation by adjusting the calibrated model \( \widehat{f} \) using functions from \( \mathcal{G} \).

Multicalibration for classification and regression requires that \citep{hebert2018multicalibration, kim2019multiaccuracy}:  
\begin{align}
    \mathbb{E}\left[g(X_{n+1})\{Y_{n+1} - \widehat{f}(X_{n+1})\} \right] = 0  \text{ for all } g \in \mathcal{G}. \label{eqn::multical}
\end{align}  
The function \mbox{\( g \in \mathcal{G} \)} is sometimes interpreted as representing covariate shift. When \mbox{\( g \)} is nonnegative, it follows that \mbox{\small \( \mathbb{E}_g[\widehat{f}(X_{n+1})] = \mathbb{E}_g[Y_{n+1}] \)}, where the weighted expectation is defined as \mbox{\small \( \mathbb{E}_g[\widehat{f}(X_{n+1})] = \mathbb{E} \left[\frac{g(X_{n+1})}{\mathbb{E}[g(X_{n+1})]} \widehat{f}(X_{n+1}) \right] \)}. For example, when \mbox{\small \( \mathcal{G} = \{x \mapsto \mathbbm{1}(x \in B): B \in \mathcal{B}\} \)} consists of all set indicators for (possibly intersecting) subgroups in \mbox{\small $\mathcal{B}$}, multicalibration implies that the model \mbox{\small \( \widehat{f}(X_{n+1}) \)} is calibrated for \mbox{\small \( Y_{n+1} \)} within each subgroup, meaning that \mbox{\small \( \mathbb{E}[Y_{n+1} \mid X_{n+1} \in B] = \mathbb{E}[\widehat{f}(X_{n+1}) \mid X_{n+1} \in B] \)}.  


\begin{algorithm}[htb!]
\caption{Venn loss multicalibration}
\label{alg::multical}
\begin{algorithmic}[1]{\footnotesize
\REQUIRE Calibration data~\mbox{\footnotesize $\mathcal{C}_n= \{(X_i, Y_i)\}_{i=1}^n$}, model \mbox{\footnotesize $f$}, loss $\ell$, context \mbox{\footnotesize $x \in \mathcal{X}$}, function class $\mathcal{F}$.

\vspace{.02in}
 
\vspace{.05in}
\FOR{each $y \in \mathcal{Y}$}
\vspace{.025in}
\STATE \mbox{\footnotesize augment dataset: $\mathcal{C}_{n}^{(x,y)} := \mathcal{C}_n \cup \{(X_{n+1}, Y_{n+1}) := (x, y)\}$}; \\
\STATE multicalibrate model using offset loss minimization: 

    \vspace{.05in}
    \,\mbox{\footnotesize $g_{n}^{(x,y)} := \argmin_{g \in \mathcal{G}} \sum_{i \in \mathcal{C}_{n}^{(x,y)}} \ell(f(X_i) + g(X_i), Z_i)$}.

    
    $f_{n}^{(x,y)}: = f + g_{n}^{(x,y)}$.
    \vspace{.025in}\\
\vspace{.025in}
\ENDFOR
\STATE set $f_{n, x}(x) := \{f_{n}^{(x,y)}(x) : y \in \mathcal{Y} \}$;
\ENSURE \mbox{\footnotesize prediction set $f_{n, x}(x)$}}.
\end{algorithmic}
\end{algorithm}


For a finite-dimensional function class \( \mathcal{G} \), we propose Venn multicalibration for a generic loss function \( \ell \) in Algorithm~\ref{alg::multical}. For mean multicalibration with squared error loss, this algorithm can be computed efficiently using the Sherman-Morrison formula to update linear regression solutions with new datapoints \citep{shermen1949adjustment, yang2023forster}. Under monotonicity of $y \mapsto f_{n}^{(x,y)}$, the range of the Venn prediction set \( f_{n,x}(x) \) can be computed by iterating over the extreme points \(\{y_{\text{min}}, y_{\text{max}}\}\) of \(\mathcal{Y}\).
 
The following theorem establishes that the~prediction~set {\footnotesize \( f_{n, X_{n+1}}(X_{n+1})\)} in Alg. \ref{alg::multical} contains the \( \ell \)-multicalibrated prediction {\footnotesize $f_{n+1}^*(X_{n+1})$}, where {\footnotesize $f_{n+1}^*  := f_n^{(X_{n+1}, Y_{n+1})}$}.

 \begin{enumerate}[label=\bf{C\arabic*)}, ref={C\arabic*}, resume=theorem]
    \item \textit{Empirical~$\ell$-multicalibration:} $\sum_{i=1}^{n+1} \frac{\partial}{\partial t}   \ell((f_{n+1}^* + t g)(X_i), Z_i) \mid_{t = 0} = 0$ almost surely for each $g \in \mathcal{G}$. \label{cond::ellempcal}
\end{enumerate}

\begin{theorem}[Perfect calibration of Venn multicalibration]
\label{theorem::multiVennGeneralLoss}Under \ref{cond::exchange} and \ref{cond::ellempcal}, the Venn prediction set $f_{n, X_{n+1}}(X_{n+1})$ contains the marginally perfectly calibrated prediction $f_{n+1}^*(X_{n+1}) = f_n^{(X_{n+1}, Y_{n+1})}(X_{n+1})$, which satisfies
\begin{align*}
   \mathbb{E}\left[\frac{\partial}{\partial t}  \ell((f_{n+1}^* + t g)(X_{n+1}), Z_{n+1}) \mid_{t = 0} \right]  \,=\,0,\, \forall g \in \mathcal{G}.
\end{align*}
\end{theorem}
  
In the special case where \( \ell \) is the squared error loss,~the~next corollary shows that Venn prediction sets contain a perfectly multicalibrated regression prediction,~as~defined~in~\eqref{eqn::multical}.

\begin{corollary}[Regression Venn Multicalibration]
   Suppose that $\ell(f(x), z))$ is $\{y - f(x)\}^2$. Under \ref{cond::exchange} and \ref{cond::variance}, the Venn prediction set $f_{n, X_{n+1}}(X_{n+1})$ contains the perfectly multicalibrated prediction, denoted as $f_{n+1}^* (X_{n+1}) := f_n^{(X_{n+1}, Y_{n+1})}(X_{n+1})$, which satisfies $\mathbb{E}\left[g(X_{n+1})\{Y_{n+1} - f_{n+1}^*(X_{n+1})\} \right] = 0  \text{ for all } g \in \mathcal{G}$.
   \label{corollary::multical}
\end{corollary}

\section{Applications to conformal prediction}
 
\subsection{Conformal prediction via Venn quantile calibration}

\label{section::CP}
Conformal prediction (CP) \citep{vovk2005algorithmic} is a flexible approach for predictive inference that can be applied post-hoc to any black-box model. It constructs prediction intervals \(\widehat{C}_{n}(X_{n+1})\) that are guaranteed to {\it cover} the true outcome \(Y_{n+1}\) with probability \(1-\alpha\). The standard CP method ensures prediction intervals satisfy the marginal coverage guarantee:  $\mathbb{P}(Y_{n+1} \in \widehat{C}_{n}(X_{n+1})) \geq 1-\alpha, $ where the probability \(\mathbb{P}\) accounts for the randomness in \(\mathcal{C}_n\) and \((X_{n+1}, Y_{n+1})\). In this section, we show how our generalized Venn calibration framework, when combined with the quantile loss, enables the construction of perfectly calibrated quantile predictions and CP intervals in finite samples.


Let $\{S_i\}_{i=1}^{n+1}$ be conformity scores, where $S_i = \mathcal{S}(X_i,Y_i)$ for some scoring function $\mathcal{S}$. For example, we could set $\mathcal{S}$ equal to the absolute residual scoring function $z \mapsto |y - \mu(x)|$, where $\mu$ is a model to predict $y$ from $x$. Intuitively, conformity scores quantify how well a predicted outcome aligns with the true outcome based on the model's predictions. Let \( f: \mathcal{X} \rightarrow \mathbb{R} \) be a machine learning model trained to predict the $1-\alpha$ quantile of the conformity scores. For a quantile level $\alpha \in (0,1)$, we denote the quantile loss \( \ell_{\alpha}(q,y) \) by $\mathbbm{1}(y \geq q) \cdot \alpha (y - q) + \mathbbm{1}(y < q) \cdot (1-\alpha) (q - y).\)

Let $f_{n, X_{n+1}}(X_{n+1}) = \{f_n^{(X_{n+1}, y)}(X_{n+1}): y \in \mathcal{Y}\}$ be the Venn quantile prediction set of $S_{n+1}$ obtained by applying Algorithm~\ref{alg::VAgen} with the conformal quantile loss $\ell_{\mathcal{S}, \alpha}: (f, z) \mapsto \ell_{\alpha}( f, \mathcal{S}(z))$, and let $f_{n+1}^*(X_{n+1})$ be the perfectly calibrated prediction in this set, where $f_{n+1}^* := f_n^{(X_{n+1}, Y_{n+1})}$ equals $\mathcal{A}_{\ell}(f, \{(X_i, Y_i)\}_{i=1}^{n+1})$. The Venn prediction set $f_{n, X_{n+1}}(X_{n+1})$ induces the Venn CP interval:
\[\widehat{C}_n(X_{n+1}) := \{y \in \mathcal{Y}: \mathcal{S}((X_{n+1}, y)) \leq f_{n}^{(X_{n+1}, y)}(X_{n+1}) \}\]

  \begin{enumerate}[label=\bf{C\arabic*)}, ref={C\arabic*}, resume=theorem]
    \item \textit{Empirical calibration:} $\sum_{i=1}^{n+1} \ell_{\mathcal{S}, \alpha}(f_{n+1}^*, Z_i) = \min_{\theta} \sum_{i=1}^{n+1} \ell_{\mathcal{S}, \alpha}(\theta \circ f_{n+1}^*, Z_i)$ . \label{cond::quantileempcal}
\end{enumerate}

\begin{theorem}[Calibration of Venn Quantile Prediction]
   Assume \ref{cond::exchange}, \ref{cond::quantileempcal}, and that $S_i \neq f_{n+1}^*(X_i)$ almost surely for each $i \in [n+1]$. Then, the Venn prediction set $f_{n, X_{n+1}}(X_{n+1})$ contains the marginally perfectly calibrated prediction $f_{n+1}^* (X_{n+1}) = f_n^{(X_{n+1}, Y_{n+1})}(X_{n+1})$, where $\mathbb{P}( S_{n+1}  \leq f_{n+1}^*(X_{n+1})\mid f_{n+1}^*(X_{n+1})) = 1 - \alpha$. %Thus, the Venn CP interval $\widehat{C}_n(X_{n+1}) = \{y \in \mathcal{Y}: \mathcal{S}((X_{n+1}, y), f) \leq f_{n}^{(X_{n+1}, y)}(X_{n+1}) \}$ achieves perfectly calibrated coverage, satisfying $\mathbb{P}(Y_{n+1} \in \widehat{C}_n(X_{n+1}) \mid f_{n+1}^*(X_{n+1}))  = 1 - \alpha.$ 
   \label{theorem::pointquantile}
\end{theorem}
 


Theorem \ref{theorem::pointquantile} implies that the Venn CP interval $\widehat{C}_n(X_{n+1})$ constructed using Venn quantile calibration is perfectly calibrated in that $\mathbb{P}(Y_{n+1} \in \widehat{C}_n(X_{n+1}) \mid f_{n+1}^*(X_{n+1}))  = 1 - \alpha.$ This result follows directly from the perfect quantile calibration of $f_{n+1}^* (X_{n+1})$ because, by definition,
\begin{align*}
    \mathbb{P}(&Y_{n+1}  \in \widehat{C}_n(X_{n+1}) \mid f_{n+1}^*(X_{n+1})) \\
    &= \mathbb{P}(\mathcal{S}((X_{n+1}, Y_{n+1})) \leq f_{n}^{(X_{n+1}, Y_{n+1})} \mid f_{n+1}^*(X_{n+1})) \\
    &= \mathbb{P}(S_{n+1} \leq f_{n+1}^*(X_{n+1}) \mid f_{n+1}^*(X_{n+1})) \\
    &= 1 - \alpha.
\end{align*}
As a consequence, the CP interval satisfies a form of \textit{threshold calibration} \citep{jung2022batch}, meaning its coverage is valid conditional on the quantile $f_{n+1}^*(X_{n+1})$ used to define the interval. The law of total expectation implies that $\widehat{C}_n(X_{n+1})$ also satisfies the marginal calibration condition $\mathbb{P}( S_{n+1}  \leq f_{n+1}^*(X_{n+1})) = 1 - \alpha$. We note that, without assuming $S_i \neq f_{n+1}^*(X_i)$ almost surely for each $i \in [n+1]$, we can still establish the lower bound $\mathbb{P}(Y_{n+1} \in \widehat{C}_n(X_{n+1}) \mid f_{n+1}^*(X_{n+1})) \geq 1 - \alpha$ using arguments from \cite{gibbs2023conformal}, though we do not pursue this here for simplicity.


 
 

 \subsection{Conformal prediction as Venn multicalibration}

 
In this section, we show that conformal prediction is a special case of Venn multicalibration with the quantile loss.


Suppose Alg.~\ref{alg::multical} is applied with the conformal quantile loss \( (f(x), z) \mapsto \ell_{\alpha}(f(x), \mathcal{S}(z)) \), model $f$, and \( x := X_{n+1} \), and let \( f_{n, X_{n+1}}(X_{n+1}) \) be the corresponding Venn set prediction. As in Section \ref{section::CP}, we define the multicalibrated Venn CP interval as  
\(
\widehat{C}_n(X_{n+1}) := \{y \in \mathcal{Y}: \mathcal{S}((X_{n+1}, y)) \leq f_{n}^{(X_{n+1}, y)}(X_{n+1}) \}.
\)
This multicalibrated CP interval is identical to the interval proposed in the conditional CP framework of \cite{gibbs2023conformal}. 


The following theorem shows that Venn multicalibration outputs a prediction set containing a marginally multicalibrated quantile prediction \citep{deng2023happymap}, ensuring that the CP interval is multicalibrated in the sense of \cite{gibbs2023conformal}.



 

\begin{theorem}[Quantile Multicalibration]
   \label{theorem::multicalquantile} Assume \ref{cond::exchange} holds and \( S_i \neq f_{n+1}^*(X_i) \) almost surely for all \( i \in [n+1] \). Then, the Venn prediction set \( f_{n, X_{n+1}}(X_{n+1}) \) contains the perfectly multicalibrated prediction \( f_{n+1}^*(X_{n+1}) := f_n^{(X_{n+1}, Y_{n+1})}(X_{n+1}) \), which satisfies
   \(
   \mathbb{E}\left[g(X_{n+1})\{(1 - \alpha) - \mathbb{P}(S_{n+1} \leq f_{n+1}^*(X_{n+1}) \mid X_{n+1})\} \right] = 0 \quad \text{for all } g \in \mathcal{G}.
   \)    
\end{theorem}

 

 


  By definition, Theorem \ref{theorem::multicalquantile} implies the multicalibrated coverage of the conformal interval: for  all $g \in \mathcal{G}$, 
\[
\mathbb{E}\left[g(X_{n+1})\{(1 - \alpha) - \mathbb{P}(Y_{n+1} \in \widehat{C}_n(X_{n+1}) \mid X_{n+1})\} \right] = 0  ,
\]
which agrees with Theorem 2 of \cite{gibbs2023conformal}.



As a consequence, the multicalibrated CP framework for finite-dimensional covariate shifts proposed in Section 2.2 of \cite{gibbs2023conformal} can be interpreted as a special case of Venn multicalibration. Similarly, the standard marginal CP approach \citep{vovk2005algorithmic, lei2018distribution} and Mondrian (or group-conditional) CP \citep{vovk2005algorithmic, romano2020malice} are special cases of this algorithm, with \( \mathcal{G} \) consisting of constant functions and subgroup indicators, respectively.  








\section{Numerical experiments}

The utility of Venn and Venn-Abers calibration for classification and regression, as well as Venn multicalibration with the quantile loss in the context of conformal prediction (CP), has been demonstrated through synthetic and real data experiments in various works \citep{vovk2012venn, nouretdinov2018inductive, johansson2019calibrating, johansson2019efficient, johansson2023well, vanself, vovk2005algorithmic, lei2018distribution, romano2019conformalized, bostrom2020mondrian, romano2020malice, gibbs2023conformal}. In this section, we evaluate two novel instances of these methods: CP using Venn-Abers calibration with the quantile loss (Section \ref{section::CP}) and Venn multicalibration for regression using the squared error loss.



\subsection{Venn-Abers conformal quantile calibration}

We evaluate conformal prediction intervals constructed using Venn-Abers quantile calibration on real datasets, including the Medical Expenditure Panel Survey (MEPS) dataset \citep{cohen2009medical, MEPS_Panel_21}, as well as the \textit{Concrete}, \textit{Community}, \textit{STAR}, \textit{Bike}, and \textit{Bio} datasets from \cite{romano2019conformalized}, which are available in the \texttt{cqr} package. Each dataset is split into a training set (50\%), a calibration set (30\%), and a test set (20\%). We implement Venn-Abers quantile calibration (\textbf{VA}) using absolute residual error as the conformity score and train the $1-\alpha$ quantile model $f(\cdot)$ of the conformity score using \texttt{xgboost} \citep{xgboost}. The baselines include \textbf{uncalibrated} intervals derived from $f(\cdot)$, a symmetric variant of conformalized quantile regression (\textbf{CQR}) \citep{romano2019conformalized}, \textbf{Marginal} conformal prediction (\textbf{CP}) \citep{vovk2005algorithmic, lei2018distribution}, and Mondrian conformal prediction (\textbf{VM}) \citep{romano2020malice}, with categories based on bins of the estimated $1-\alpha$ quantiles. \textbf{VM} corresponds to \textbf{V}enn calibration with \textbf{M}ondrian histogram binning. For direct comparability, all baselines are based on the absolute residual error score $|y - \mu(x)|$, where $\mu(x)$ is a \texttt{xgboost} predictor of the conditional median of $y$, and intervals are thus centered around $\mu(x)$.


Averaged over 100 random data splits, Table \ref{tab:dataset-results} summarizes Monte Carlo estimates of marginal coverage, and conditional $\ell^1$-calibration error (CCE), and average interval width. For $f_n^*$, the isotonic calibration of $f$, the CCE is defined as 
\[
E_P\left[\max\{0, P(Y \not\in \widehat{C}_n(X) \mid f_n^*(X), \mathcal{C}_n) - \alpha\}\mid \mathcal{C}_n \right].
\] 
All calibrated methods achieve adequate marginal coverage, as guaranteed by theory, while the uncalibrated intervals exhibit poor coverage. \textbf{VA} consistently achieves the lowest or comparable CCE across datasets, as expected from Theorems \ref{theorem::condcalVA2} and \ref{theorem::pointquantile}, outperforming or matching the state-of-the-art \textbf{CQR} in terms of coverage, CCE, and width.~Although \textbf{VM} CP improves with more bins, its CCE remains higher than that of \textbf{VA}, highlighting the advantage of data-adaptive binning via isotonic regression.










%As baselines, we consider conformalized quantile regression (\textbf{CQR}) \citep{romano2019conformalized}, which provides varying-width prediction bands with marginal coverage guarantees, \textbf{Marginal} conformal prediction \cite{vovk2005algorithmic, lei2018distribution}, which uses the absolute residual conformity score to produce constant-width prediction bands, and \textbf{VM} conformal prediction with categories defined by bins of the estimated conditional $1-\alpha$ quantile of the conformity score \cite{vovk2005algorithmic, bostrom2021mondrian}. This form of \textbf{VM} CP is equivalent to \textbf{Venn quantile calibration} using quantile binning as the base calibrator.

\begin{table}[t]
\centering
\caption{Metrics for each dataset: Marginal Coverage, Conditional Calibration Error (CCE), and Average Width.}
\label{tab:dataset-results}
\begin{minipage}{\columnwidth}
\centering
\small
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Method} & \textbf{Bike} & \textbf{Bio} & \textbf{Star} & \textbf{Meps} & \textbf{Conc} & \textbf{Com} \\
\midrule
\multicolumn{7}{c}{\textbf{Marginal Coverage}} \\
\midrule
Uncalibrated        & 0.81 & 0.85 & 0.80 & 0.86 & 0.71 & 0.74 \\
Venn-Abers          & 0.90 & 0.90 & 0.90 & 0.90 & 0.90 & 0.90 \\
CQR                 & 0.90 & 0.90 & 0.90 & 0.90 & 0.90 & 0.90 \\
Marginal            & 0.90 & 0.90 & 0.90 & 0.90 & 0.90 & 0.90 \\
VM (5 bin)          & 0.90 & 0.90 & 0.89 & 0.90 & 0.89 & 0.90 \\
VM (10 bin)         & 0.90 & 0.90 & 0.89 & 0.90 & 0.88 & 0.89 \\
\midrule
\multicolumn{7}{c}{\textbf{Conditional Calibration Error (CCE)}} \\
\midrule
Uncalibrated        & 0.11 & 0.088 & 0.11 & 0.053 & 0.20 & 0.17 \\
Venn-Abers          & \textbf{0.019} & \textbf{0.017} & 0.024 & \textbf{0.018} & \textbf{0.035} & \textbf{0.028} \\
CQR                 & 0.031 & 0.020 & 0.026 & 0.020 & 0.037 & 0.031 \\
Marginal            & 0.10 & 0.053 & \textbf{0.020} & 0.052 & 0.057 & 0.058 \\
VM (5 bin)          & 0.033 & 0.025 & 0.023 & 0.026 & 0.044 & 0.030 \\
VM (10 bin)         & 0.022 & 0.020 & 0.028 & 0.022 & 0.049 & 0.030 \\
\midrule
\multicolumn{7}{c}{\textbf{Average Width}} \\
\midrule
Uncalibrated        & 83 & 12 & 620 & 2.4 & 9.4 & 0.22 \\
Venn-Abers          & 100 & 14 & 780 & 2.8 & 17 & 0.40 \\
CQR                 & 98 & 14 & 780 & 2.7 & 16 & 0.38 \\
Marginal            & 140 & 15 & 780 & 2.9 & 18 & 0.46 \\
VM (5 bin)          & 99 & 14 & 770 & 2.8 & 16 & 0.38 \\
VM (10 bin)         & 100 & 14 & 770 & 2.8 & 16 & 0.38 \\
\bottomrule
\end{tabular}
\end{minipage}
\end{table}





 

\subsection{Venn mean multicalibration}

We evaluate Venn multicalibration for regression with squared error loss on the same datasets as the prior experiment. Each dataset is split into a training set (40\%), a calibration set (40\%), and a test set (20\%). We train the model $f$ using median regression with \texttt{xgboost}, such that the model is miscalibrated for the mean when the outcomes are skewed. We apply Alg.~\ref{alg::multical} with $\mathcal{G}$ defined as the linear span of an additive spline basis of the features, aiming for multicalibration over additive functions. Specifically, for continuous features, we generate cubic splines with five knot points, and for categorical features, we apply one-hot encoding. As baselines, we consider the uncalibrated model and the point-calibrated model obtained by adjusting $f$ via offset linear regression on $\mathcal{C}_n$ based on $\mathcal{G}$. All outcomes are rescaled to lie in $[0,1]$ for comparability across datasets.


Averaged averaged over 100 random data splits, Table \ref{tab:exp2} summarizes the sample size $(n)$, feature dimension $(p)$, conditional multicalibration errors for the uncalibrated, point calibrated, and oracle Venn-calibrated predictions, and the average Venn prediction set width. The oracle Venn-calibrated prediction is the marginally perfectly calibrated prediction $f_{n+1}^{(X_{n+1}, Y_{n+1})}$ necessarily contained in the prediction set. For the basis $\{b_j(\cdot)\}_{j=1}^m$ of $\mathcal{G}$, the multicalibration error of a model $\widehat{f}$ is the $\ell^2$ norm of the test set empirical errors:
\begin{align*}
    \Big(\frac{1}{n_{\text{test}}} \sum_{i=1}^{n_\text{test}} b_j(X_i) \{Y_i - \widehat{f}(X_i)\} : j \in [n] \Big).
\end{align*}

 
\begin{table}[ht]
\vspace{-0.75cm}
\centering
\caption{Sample size and feature dimension $(n, p)$, calibration errors for the uncalibrated model, calibrated model, Venn-calibrated model, and mean prediction set width for datasets.}
\begin{tabular}{@{\hskip 2pt}l@{\hskip 5pt}c|ccc|c@{\hskip 2pt}}
\hline
 & Dim.  & \multicolumn{3}{c|}{Calibration Error} & Width \\
\hline
 &   $(n, p)$       & Uncal & Calibr & Venn & Venn \\
\hline
Bike       & (4354, 18)   & 0.0019 & 0.0015 & \textbf{0.0015} & 0.0086 \\
Bio        & (18292, 9)   & \textbf{0.0073} & 0.0100 & 0.0094 & 0.010 \\
Star       & (864, 39)    & 0.0098 & 0.0113 & \textbf{0.0078} & 0.326 \\
Meps       & (6262, 139)  & 0.0032 & 0.0017 & \textbf{0.0016} & 0.0088 \\
Conc   & (412, 8)     & 0.0077 & 0.0081 & \textbf{0.0064} & 0.143 \\
Comm  & (797, 101)   & 0.0099 & 0.0209 & \textbf{0.0055} & 0.665 \\
\hline
\end{tabular}
\label{tab:exp2}
\end{table}


The oracle Venn-calibrated model consistently achieves smaller calibration errors than the point-calibrated model across all datasets and outperforms the uncalibrated model in all but one dataset. Its improvement is more pronounced in settings with wider Venn prediction sets, which correspond to smaller effective sample sizes $\frac{n}{p}$. In these cases, naive multicalibration is more variable and prone to overfitting. This aligns with expectations, as wider prediction sets reflect greater uncertainty in the finite-sample calibration of point-calibrated predictions. 

 


 
\newpage

  

\bibliographystyle{plainnat}

 
\bibliography{ref}


\appendix

\onecolumn



\section{Background on isotonic calibration}

Isotonic calibration \citep{zadrozny2002transforming, niculescu2005obtaining} is a data-adaptive histogram binning method that learns the bins using isotonic regression, a nonparametric method traditionally used for estimating monotone functions \citep{barlow1972isotonic, groeneboom1993isotonic}. Specifically, the bins are selected by minimizing an empirical MSE criterion under the constraint that the calibrated predictor is a non-decreasing monotone transformation of the original predictor. Isotonic calibration is motivated by the heuristic that, for a good predictor $f$, the calibration function $\theta_{P,f}$ should be approximately monotone as a function of $f$. For instance, when $f(\cdot) = E_P[Y \mid X = \cdot]$, the mapping $f \mapsto \theta_{P,f} = f$ is the identity function. Isotonic calibration is distribution-free --- it does not rely on monotonicity assumptions --- and, in contrast with histogram binning, it is tuning parameter-free and naturally preserves the mean-square error of the original predictor (as the identity transform is monotonic) \citep{van2023causal}. 


For clarity, we focus on the regression case where $\ell$ denotes the squared error loss. Formally, isotonic calibration takes a predictor $f$ and a calibration dataset $\mathcal{C}_n$ and produces the calibrated model $f_n^* := \theta_n \circ f$, where $\theta_n: \mathbb{R} \rightarrow \mathbb{R}$ is an isotonic step function obtained by solving the optimization problem:
\begin{equation}
    \theta_n \in \argmin_{\theta \in \Theta_{\text{iso}}} \sum_{i=1}^n \left\{Y_i - \theta(f(X_i))\right\}^2,
\end{equation}
where $\Theta_{\text{iso}}$ denotes the set of all univariate, piecewise constant functions that are monotonically nondecreasing. Following \citet{groeneboom1993isotonic}, we consider the unique c\`{a}dl\`{a}g piecewise constant solution to the isotonic regression problem, which has jumps only at observed values in \( \{f(X_i): i \in [n]\} \). The first-order optimality conditions of the convex optimization problem imply that the isotonic solution $\theta_n$ acts as a binning calibrator with respect to a data-adaptive set of bins determined by the jump points of the step function $\theta_n$. Thus, isotonic calibration provides perfect empirical calibration. Specifically, for any transformation $g: \mathbb{R} \rightarrow \mathbb{R}$, the perturbed step function $\varepsilon \mapsto \theta_n + \varepsilon (g \circ \theta_n)$ remains isotonic for all sufficiently small $\varepsilon$ such that $|\varepsilon| \sup_{t \in f(\mathcal{X})}| (g \circ \theta_n)(t)|$ is less than the maximum jump size of $\theta_n$, given by $\sup_{t \in f(\mathcal{X})}|\theta_n(t) - \theta_n(t-)|$. Since $\theta_n$ minimizes the empirical mean square error criterion over all isotonic functions, it follows that, for each function $g: \mathbb{R} \rightarrow \mathbb{R}$, the following condition holds:
\begin{align*}
    \frac{d}{d\varepsilon} \frac{1}{2} \sum_{i=1}^n \left\{Y_i - \theta_n(f(X_i)) - \varepsilon g(\theta_n(f(X_i)))\right\}^2 \Big|_{\varepsilon = 0}  =  \sum_{i=1}^n g(f_n^*(X_i))\{Y_i - f_n^*(X_i)\} = 0.
\end{align*}
These orthogonality conditions are equivalent to perfect empirical calibration. In particular, by taking $g$ as the level set indicator $t \mapsto \mathbb{I}(t = \theta_n(f(x)))$, we conclude that the isotonic calibrated predictor $f_n^*$ is empirically calibrated. 



\section{Proofs}


\subsection{Proofs for Venn calibration}

\begin{proof}[Proof of Theorem \ref{theorem::VennGeneral}]
From \ref{cond::empcal}, we know that
    \[
 \sum_{i=1}^n \mathbbm{1}\{f_{n+1}^*(X_i) = f_{n+1}^*(x)\} \partial \ell(f_{n+1}^*(X_i), Z_i) = 0.
\]
This condition implies, for every transformation $g: \mathbb{R} \rightarrow \mathbb{R}$, that
    \[
 \sum_{i=1}^n g(f_{n+1}^*(X_i)) \partial \ell(f_{n+1}^*(X_i), Z_i) = 0.
\]
Taking the expectation of both sides, we find that
\begin{align*}
   0 &= \mathbb{E} \left[ \sum_{i=1}^n g(f_{n+1}^*(X_i))  \partial \ell(f_{n+1}^*(X_i), Z_i)  \right]\\
   &=  \sum_{i=1}^n \mathbb{E} \left[  g(f_{n+1}^*(X_i)) \partial \ell(f_{n+1}^*(X_i), Z_i)  \right].
\end{align*}
Note that $f_{n+1}^*$ is trained on all of $\mathcal{C}_{n+1}^*$ and is thus invariant to permutations of $\{(X_i, Y_i)\}_{i=1}^{n+1}$. Since $\{(X_i, Y_i)\}_{i=1}^{n+1}$ are exchangeable by \ref{cond::exchange}, it follows that $ g(f_{n+1}^*(X_i)) \partial \ell(f_{n+1}^*(X_i), Z_i) $ is exchangeable over $i \in [n+1]$. Thus, the previous display implies, for every transformation $g: \mathbb{R} \rightarrow \mathbb{R}$, that
\begin{align*}
   0 &=  \sum_{i=1}^n \mathbb{E} \left[  g(f_{n+1}^*(X_{n+1})) \partial \ell(f_{n+1}^*(X_{n+1}), Z_{n+1})  \right]\\
    0 &=  \mathbb{E} \left[  g(f_{n+1}^*(X_{n+1})) \partial \ell(f_{n+1}^*(X_{n+1}), Z_{n+1})  \right]\\
     0 &=  \mathbb{E} \left[  g(f_{n+1}^*(X_{n+1})) \mathbb{E}[\partial \ell(f_{n+1}^*(X_{n+1}), Z_{n+1}) \mid f_{n+1}^*(X_{n+1})]  \right],
\end{align*}
where the final equality follows from the law of total expectation.
Taking $g$ such that $ g(f_{n+1}^*(x)) = \mathbb{E}[\partial \ell(f_{n+1}^*(X_{n+1}), Z_{n+1}) \mid f_{n+1}^*(X_{n+1}) = f_{n+1}^*(x)]$, which exists by \ref{cond::variance}, we conclude that
$$ \mathbb{E} \left[\left\{\mathbb{E}[\partial \ell(f_{n+1}^*(X_{n+1}), Z_{n+1}) \mid f_{n+1}^*(X_{n+1})]  \right\}^2 \right] = 0.$$


\end{proof}




\begin{proof}[Proof of Theorem  \ref{theorem::condcalVA} ]

For a uniformly bounded function class $\mathcal{F}$, let $N(\epsilon,\mathcal{F},L_2(P))$ denote the $\epsilon-$covering number \citep{van1996weak} of $\mathcal{F}$ with respect to $L_2(P)$ and define the uniform entropy integral of $\mathcal{F}$ by  
\begin{equation*}
\mathcal{J}(\delta,\mathcal{F})\coloneq \int_{0}^{\delta} \sup_{Q}\sqrt{\log N(\epsilon,\mathcal{F},L_2(Q))}\,d\epsilon\ ,
\end{equation*}
where the supremum is taken over all discrete probability distributions $Q$. For two quantities $x$ and $y$, we use the expression  $x \lesssim y$ to mean that $x$ is upper bounded by $y$ times a universal constant that may only depend on global constants that appear in our conditions. 



 We know that \( \theta_{n}^{(x,y)} \) almost surely belongs to a uniformly bounded function class \( \mathcal{F}_n \) consisting of 1D functions with at most $k(n)$ constant segments. Then, \( \mathcal{F}_n \)  has finite uniform entropy integral with \( \mathcal{J}(\delta, \mathcal{F}_n) \lesssim \delta \sqrt{k(n) \log (1/\delta)} \). Define $\mathcal{F}_{f,n} := \{\theta \circ f: \theta \in \mathcal{F}_n\}$. We claim that \( \mathcal{J}(\delta, \mathcal{F}_{f,n}) \lesssim \delta \sqrt{k(n) \log (1/\delta)} \). This follows since, by the change-of-variables formula,
\begin{align*}
\mathcal{J}(\delta, \mathcal{F}_{f,n}) &=  \int_0^{\delta} \sup_Q \sqrt{N\big(\varepsilon, \mathcal{F}_{f,n}, \|\,\cdot\,\|_Q \big)}\,\mathrm{d}\varepsilon \\
&=  \int_0^{\delta} \sup_Q \sqrt{N\big(\varepsilon, \mathcal{F}_{n}, \|\,\cdot\,\|_{Q \circ f} \big)}\, \mathrm{d}\varepsilon \\
&= \mathcal{J}(\delta, \mathcal{F}_{n}).
\end{align*}
where, with a slight abuse of notation, \(Q \circ f\) is the push-forward probability measure for the random variable \(f(X)\).



 
 By assumption, we have perfect empirical calibration: for all \( g: \mathbb{R} \rightarrow \mathbb{R} \),
\begin{align*}
    \sum_{i=1}^n g(\theta_{n}^{(x,y)}(f(X_i))) \partial \ell(\theta_{n}^{(x,y)}(f(X_i), Z_i)) +   g(\theta_{n}^{(x,y)}(f(x))) \partial \ell(\theta_{n}^{(x,y)}(f(x)), z) = 0.
\end{align*}
Take \( g \) such that \( g \circ \theta_{n}^{(x,y)} \) equals \( t \mapsto E_P[\partial \ell(\theta_{n}^{(x,y)}(f(x)), z) \mid \theta_{n}^{(x,y)}(f(X)) = t, \mathcal{C}_n] \). Then, denoting \( \gamma_f(\theta_{n}^{(x,y)}, \cdot): x' \mapsto E_P[\partial \ell(\theta_{n}^{(x,y)}(f(x')), z') \mid \theta_{n}^{(x,y)}(f(X)) = \theta_{n}^{(x,y)}(f(x')), \mathcal{C}_n] \), we find that
\begin{align*}
  \sum_{i=1}^n \gamma_f(\theta_{n}^{(x,y)}, X_i) \partial \ell(\theta_{n}^{(x,y)}(f(X_i), Z_i)) + \gamma_f(\theta_{n}^{(x,y)}, x) \partial \ell(\theta_{n}^{(x,y)}(f(x)), z) = 0.
\end{align*}
By assumption, $\gamma_f(\theta_{n}^{(x,y)}, X) \partial \ell(\theta_{n}^{(x,y)}(f(x)), z)$ is uniformly bounded, such that
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n \gamma_f(\theta_{n}^{(x,y)}, X_i) \partial \ell(\theta_{n}^{(x,y)}(f(X_i), Z_i))  = O(n^{-1}).
\end{align*}


Adding and subtracting, we have that
\begin{align*}
 P_n \gamma_f(\theta_{n}^{(x,y)}, \cdot) \partial \ell(\theta_{n}^{(x,y)}(f(\cdot), \cdot))  &= O(n^{-1})\\
 P \gamma_f(\theta_{n}^{(x,y)}, \cdot) \partial \ell(\theta_{n}^{(x,y)}(f(\cdot), \cdot)) +  (P_n - P) \gamma_f(\theta_{n}^{(x,y)}, \cdot) \partial \ell(\theta_{n}^{(x,y)}(f(\cdot), \cdot))  &= O(n^{-1})\\
 P \{\gamma_f(\theta_{n}^{(x,y)}, \cdot)\}^2 +  (P_n - P) \gamma_f(\theta_{n}^{(x,y)}, \cdot) \partial \ell(\theta_{n}^{(x,y)}(f(\cdot), \cdot))  &= O(n^{-1}) ,
\end{align*}
 where, in the final equality, we used that  $P \gamma_f(\theta_{n}^{(x,y)}, \cdot) \partial \ell(\theta_{n}^{(x,y)}(f(\cdot), \cdot)) =  P \{\gamma_f(\theta_{n}^{(x,y)}, \cdot)\}^2$ by the law of total expectation.

The random quantity we wish to bound by $ \widehat{\delta}_n^2  P \{\gamma_f(\theta_{n}^{(x,y)}, \cdot)\}^2$. Then, the previous display implies
\begin{align*}
    \widehat{\delta}_n^2 &\leq \sup_{\theta \in \mathcal{F}_n: \|\gamma_f(\theta, \cdot)\|_P \leq \widehat{\delta}_n} \left|(P_n - P) \gamma_f(\theta, \cdot) \partial \ell(\theta(f(\cdot), \cdot)) \right| + O(n^{-1}).
\end{align*} 
By boundedness of $ \partial \ell(\theta(f(\cdot), \cdot))$, $\|\gamma_f(\theta, \cdot) \partial \ell(\theta(f(\cdot), \cdot))\|_P \leq K \|\gamma_f(\theta, \cdot) \|_P$ for some $K < \infty$. Thus,
\begin{align*}
    \widehat{\delta}_n^2 &\leq \sup_{g \in \mathcal{G}_{f}: \|g\|_P \leq K \widehat{\delta}_n} \left|(P_n - P)g\right| + O(n^{-1}),
\end{align*} 
where $\mathcal{G}_{f} := \{g_1 g_2: g_1 \in \mathcal{G}_{1,f}, g_2 \in \mathcal{G}_{2,f}\}$ with $\mathcal{G}_{1,f} := \left\{\partial \ell(\theta(f(\cdot), \cdot)): \theta \in \mathcal{F}_n \right\}$ and $\mathcal{G}_{2,f} := \left\{\gamma_f(\theta , \cdot): \theta \in \mathcal{F}_n \right\}$.

We claim that $\mathcal{J}(\delta, \mathcal{G}_f) \lesssim \mathcal{J}(\delta, \mathcal{F}_{n}) \lesssim \delta \sqrt{k(n) \log (1/\delta)}$ By assumption, the following Lipschitz condition holds almost surely:
\(
\left|\partial \ell(\theta_1(f(X)), Y) - \partial \ell(\theta_2(f(X)), Y) \right| \lesssim \left|\theta_1(f(X)) - \theta_2(f(X)) \right|.
\)
It follows that \( \mathcal{J}(\delta, \mathcal{G}_{1,f}) \lesssim \mathcal{J}(\delta, \mathcal{F}_{f,n}) \). Moreover, \( \mathcal{J}(\delta, \mathcal{G}_{2,f}) \lesssim \mathcal{J}(\delta, \mathcal{F}_{f,n}) \), since \( \gamma_f(\theta, \cdot) \in \mathcal{F}_{f,n} \) is a piecewise constant function with at most \( k(n) \) constant segments for each \( \theta \in \mathcal{F}_n \). Therefore, \( \mathcal{J}(\delta, \mathcal{G}_f) \lesssim \mathcal{J}(\delta, \mathcal{F}_{f,n})  \lesssim \mathcal{J}(\delta, \mathcal{F}_{n}) \) and the claim  follows.


Define \( \phi_{n}(\delta) := \sup_{g \in \mathcal{G}_{f}: \|g\|_P \leq K \delta} \left|(P_n - P)g\right| \). Then, we can write
\[
\widehat{\delta}_n^2 \leq \phi_{n}(\widehat{\delta}_n) + O(n^{-1}).
\]

Applying Theorem 2.1 in \cite{van2011local}, we have that for any \( \delta \) satisfying \( \sqrt{n} \delta^2 \gtrsim \mathcal{J}(\delta, \mathcal{G}_f) \),
\[
\mathbb{E}[\phi_{n}(\delta)] \lesssim n^{-\frac{1}{2}} \mathcal{J}(\delta, \mathcal{G}_f).
\]
Consequently, since \( \mathcal{J}(\delta, \mathcal{G}_f) \lesssim \mathcal{J}(\delta, \mathcal{F}_{n}) \lesssim \delta \sqrt{k(n) \log (1/\delta)} \), it follows that for any \( \delta \geq \sqrt{\frac{k(n) \log (1/ \delta)}{n}} \),
\[
\mathbb{E}[\phi_{n}(\delta)] \lesssim  \delta \sqrt{k(n) \log (1/\delta) /n}.
\]




It can be shown that  \( \delta_n^2 := k(n) \log (n/ k(n)) \) satisfies the critical inequality $\delta_n \geq \sqrt{\frac{k(n) \log (1/ \delta_n)}{n}}$, such that the previous identifies can be applied with $\delta := \delta_n$. Showing the asserted stochastic order, \( \widehat{\delta}_n^2 = O_p(\delta_n^2) \) with \( \delta_n^2 := k(n) \log (n / k(n)) \), is equivalent to demonstrating that for all \( \epsilon > 0 \), there exists a sufficiently large \( 2^S \) such that 
\[
\limsup_{n \to \infty} \mathbb{P}( \delta_n^{-2} \widehat{\delta}_n^2 > 2^S) < \epsilon.
\]
To this end, we need to show \( \lim_{n \to \infty} \mathbb{P}(\delta_n^{-2} \widehat{\delta}_n^2 > 2^S) \to 0 \) as \( S \to \infty \). Define the event \( A_s \coloneq \left\{ \delta_n^{-2} \widehat{\delta}_n^2 \in (2^s, 2^{s+1}] \right\} \) for each \( s \). Using a peeling argument and Markov's inequality, we obtain
\begin{align*} 
    \mathbb{P}\big(\delta_n^{-2} \widehat{\delta}_n^2 > 2^S \big) 
    &\leq \sum_{s=S}^\infty \mathbb{P} \big(2^{s+1} \geq \delta_n^{-2} \widehat{\delta}_n^2 > 2^s \big)   \\
    &\leq \sum_{s=S}^\infty \mathbb{P} \big(A_s, \widehat{\delta}_n^2 \leq \phi_{n}(\widehat{\delta}_n) + O(n^{-1})   \big)  \\
    &\leq \sum_{s=S}^\infty \mathbb{P} \big(A_s, \widehat{\delta}_n^2 \leq \phi_{n}(\widehat{\delta}_n)  + O(n^{-1}) \big)  \\
    &\leq \sum_{s=S}^\infty \mathbb{P} \big( \delta_n^{2} 2^s < \widehat{\delta}_n^2 \leq \phi_{n}\big(\delta_n 2^{\frac{s+1}{2}}\big)  + O(n^{-1})  \big)  \\
    &\leq \sum_{s=S}^\infty \mathbb{P} \big( \delta_n^{2} 2^s < \phi_{n}\big(\delta_n 2^{\frac{s+1}{2}}\big)   + O(n^{-1})  \big)   \\
    &\leq \sum_{s=S}^\infty \frac{\mathbb{E}\big[\phi_{n}\big(\delta_n 2^{\frac{s+1}{2}}\big)\big]  + O(n^{-1}) }{\delta_n^{2} 2^s}  
    \leq \sum_{s=S}^\infty \frac{n^{-1/2}\delta_n 2^{\frac{s+1}{2}} \sqrt{k(n) \log (n / k(n))} + O(n^{-1})  }{\delta_n^{2} 2^s}  \\
    &\leq \sum_{s=S}^\infty \frac{2^{\frac{s+1}{2}} + O(1)}{2^s} \rightarrow_{S \rightarrow \infty} 0.
\end{align*}
Thus, \( \widehat{\delta}_n^2 = O_p(\delta_n^2) \) and the result follows.

\end{proof}


 


\begin{proof}[Proof of Theorem \ref{theorem::condcalVA2} ]
This proof follows from a generalization of the proofs of Theorem 1 for treatment effect calibration and propensity score calibration in \cite{van2023causal} and \cite{van2024stabilized}.

Recall that \( f_{n}^{(x,y)} = \theta_n^{(x,y)} \circ f \). Under \ref{cond::TV}, up to a change of notation, the proof of Lemma 3 establishes that the map \( t \mapsto E_P[\partial \ell(f_{n}^{(x,y)}(X), Z) \mid f_{n}^{(x,y)}(X) = \theta_n^{(x,y)}(t), \mathcal{C}_n] \) has a total variation norm almost surely bounded by \( 3B \). Consequently, the function \( \gamma_f(\theta_n^{(x,y)}, \cdot): x \mapsto E_P[\partial \ell(f_{n}^{(x,y)}(X), Z) \mid f_{n}^{(x,y)}(X) = f_{n}^{(x,y)}(x), \mathcal{C}_n] \) is a transformation of \( f \) with a total variation norm almost surely bounded by \( 3B \).

Let \( \mathcal{F}_{TV} \) denote the space of 1D functions with total variation norm bounded by \( 3B \). Let \( \mathcal{F}_{iso} \) denote the space of isotonic functions that are uniformly bounded, such that the isotonic regression solution \( \theta_n^{(x,y)} \) belongs to this set. Note that \( \mathcal{J}(\delta, \mathcal{F}_{TV}) \lesssim \sqrt{\delta} \) and \( \mathcal{J}(\delta, \mathcal{F}_{iso}) \lesssim \sqrt{\delta} \).

Proceeding exactly as in the proof of Theorem \ref{theorem::condcalVA}, we can show that the quantity we wish to bound, \( \widehat{\delta}_n^2 := P \{\gamma_f(\theta_{n}^{(x,y)}, \cdot)\}^2 \), satisfies
\[
\widehat{\delta}_n^2 \leq \sup_{\theta \in \mathcal{F}_n: \|\gamma_f(\theta, \cdot)\|_P \leq \widehat{\delta}_n} \left|(P_n - P) \gamma_f(\theta, \cdot) \partial \ell(\theta(f(\cdot), \cdot)) \right| + O(n^{-1}).
\]
By the boundedness of \( \partial \ell(\theta(f(\cdot), \cdot)) \), we have \( \|\gamma_f(\theta, \cdot) \partial \ell(\theta(f(\cdot), \cdot))\|_P \leq K \|\gamma_f(\theta, \cdot)\|_P \) for some \( K < \infty \). Thus,
\[
\widehat{\delta}_n^2 \leq \sup_{g \in \mathcal{G}_{f}: \|g\|_P \leq K \widehat{\delta}_n} \left|(P_n - P)g\right| + O(n^{-1}),
\]
where \( \mathcal{G}_{f} := \{g_1 g_2: g_1 \in \mathcal{G}_{1,f}, g_2 \in \mathcal{G}_{2,f}\} \) with \( \mathcal{G}_{1,f} := \left\{\partial \ell(\theta(f(\cdot), \cdot)): \theta \in \mathcal{F}_{TV} \right\} \) and \( \mathcal{G}_{2,f} := \left\{\gamma_f(\theta, \cdot): \theta \in \mathcal{F}_{iso} \right\} \). An argument similar to the proof of Theorem \ref{theorem::condcalVA} shows that \( \mathcal{J}(\delta, \mathcal{G}_{f}) \lesssim \mathcal{J}(\delta, \mathcal{F}_{TV}) + \mathcal{J}(\delta, \mathcal{F}_{iso}) \lesssim \sqrt{\delta} \).

The result now follows by applying an argument identical to the proof of Theorem \ref{theorem::condcalVA}, where we set \( \delta_n^2 := n^{-2/3} \) and use \( \mathcal{J}(\delta, \mathcal{G}_{f}) \lesssim \sqrt{\delta} \).
 

 
\end{proof}

\subsection{Proofs for Venn multicalibration}



\begin{proof}[Proof of Theorem \ref{theorem::multiVennGeneralLoss}]
By \ref{cond::ellempcal}, we have almost surely for each \( g \in \mathcal{G} \) that
\[
\sum_{i=1}^{n+1} \frac{\partial}{\partial t} \ell((f_{n+1}^* + t g)(X_i), Z_i) \bigg|_{t = 0} = 0.
\]
Taking the expectations of both sides above and leveraging \ref{cond::exchange}, we have
\begin{align*}
\mathbb{E} \left[\sum_{i=1}^{n+1} \frac{\partial}{\partial t} \ell((f_{n+1}^* + t g)(X_i), Z_i) \bigg|_{t = 0} \right] &= 0, \\ 
\sum_{i=1}^{n+1} \mathbb{E} \left[ \frac{\partial}{\partial t} \ell((f_{n+1}^* + t g)(X_i), Z_i) \bigg|_{t = 0} \right] &= 0, \\
\sum_{i=1}^{n+1} \mathbb{E} \left[ \frac{\partial}{\partial t} \ell((f_{n+1}^* + t g)(X_{n+1}), Z_{n+1}) \bigg|_{t = 0} \right] &= 0, \\
\mathbb{E} \left[ \frac{\partial}{\partial t} \ell((f_{n+1}^* + t g)(X_{n+1}), Z_{n+1}) \bigg|_{t = 0} \right] &= 0,
\end{align*}
as desired.
\end{proof}


 \begin{proof}[Proof of Corollary  \ref{corollary::multical}]
This result is a direct consequence of Theorem \ref{theorem::multiVennGeneralLoss}, but we provide an independent proof for clarity and completeness.

Define $f_{n+1}^* := f_n^{(X_{n+1}, Y_{n+1})}$, and note that $f_{n+1}^* (X_{n+1})$ is an element of $f_{n, X_{n+1}}(X_{n+1})$ by construction. The first-order optimality conditions of the empirical risk minimizer $g_n^{(X_{n+1}, Y_{n+1})}$ imply that, for each $g \in \mathcal{G}$,
\[
\frac{1}{n+1} \sum_{i=1}^{n+1} g(X_i) \left\{Y_i - f_{n+1}^*(X_i)\right\} = 0.
\]
Taking the expectation of both sides, which we can do by \ref{cond::variance}, and leveraging \ref{cond::exchange}, we find that
\begin{align*}
0 &= \frac{1}{n+1} \sum_{i=1}^{n+1} \mathbb{E} \left[g(X_i) \left\{Y_i - f_{n+1}^*(X_i)\right\} \right] \\
&= \frac{1}{n+1} \sum_{i=1}^{n+1} \mathbb{E} \left[g(X_{n+1}) \left\{Y_{n+1} - f_{n+1}^*(X_{n+1})\right\} \right] \\
&= \mathbb{E} \left[g(X_{n+1}) \left\{Y_{n+1} - f_{n+1}^*(X_{n+1})\right\} \right].
\end{align*}

\end{proof}

\subsection{Proofs for conformal prediction}

 \begin{proof}{Proof of Theorem \ref{theorem::pointquantile}}
   Under the assumption that \( S_i \neq f_{n+1}^*(X_i) \) almost surely for all \( i \in [n+1] \), it is shown in Section 2.2 of \cite{gibbs2023conformal} that the quantile loss is differentiable almost surely. Moreover, its derivative is given by
   $$\partial \ell_{\alpha}(f(x), S(z))   = (1 - \alpha) - \mathbbm{1}\{S(x) \geq f(x)\}.$$
   The result now follows by application of Theorem \ref{theorem::VennGeneral}.
\end{proof}



  \begin{proof}{Proof of Theorem \ref{theorem::multicalquantile}}
   Under the assumption that \( S_i \neq f_{n+1}^*(X_i) \) almost surely for all \( i \in [n+1] \), it is shown in Section 2.2 of \cite{gibbs2023conformal} that the quantile loss is differentiable almost surely. Moreover, its derivative is given by
   $$\frac{d}{d\varepsilon} \ell_{\alpha}((f + g\varepsilon)(x), S(z)) \big |_{\varepsilon = 0}   = g(x) \left[(1 - \alpha) - \mathbbm{1}\{S(x) \geq f(x)\}\right].$$
   The result now follows by application of Theorem \ref{theorem::multiVennGeneralLoss}.
\end{proof}

 

\end{document}

 \begin{comment}
\begin{table}[ht]
\centering
\begin{tabular}{@{\hskip 2pt}l@{\hskip 5pt}c|ccc|c@{\hskip 2pt}}
\hline
 & Dim.  & \multicolumn{3}{c|}{Calibration Errors} & Width \\
\hline
 &   $(n, p)$       & Orig & Calibr & Venn & Venn \\
\hline
Bike       & (4354, 18)   & 0.0077 & 0.0061 & 0.0059 & 0.034 \\
Bio        & (18292, 9)   & 0.012  & 0.016  & 0.015  & 0.019 \\
Star       & (864, 39)    & 0.038  & 0.043  & 0.030  & 1.3   \\
Meps   & (6262, 139)  & 0.25   & 0.12   & 0.11   & 0.74  \\
Concr.   & (412, 8)     & 0.026  & 0.027  & 0.022  & 0.50  \\
Commu.  & (797, 101)   & 0.038  & 0.080  & 0.021  & 2.6   \\
\hline
\end{tabular}
\caption{Sample size $(n, p)$, calibration errors for the original model, point-calibrated model, Oracle Venn-calibrated model, and average Venn prediction set width for the Bike, Bio, Star, Meps\_21, Concrete, and Community datasets.}
\label{tab:example}
\end{table}
\end{comment}



\begin{table}[ht]
\centering
\begin{tabular}{@{\hskip 2pt}l@{\hskip 5pt}c|ccc|c@{\hskip 2pt}}
\hline
 & Dim.  & \multicolumn{3}{c|}{Calibration Error} & Rel.~Width \\
\hline
 &   $(n, p)$       & Uncal & Calibr & Venn & Venn \\
\hline
Bike       & (4354, 18)   & 0.0077 & 0.0061 & \textbf{0.0059} & 0.0046 \\
Bio        & (18292, 9)   & \textbf{0.012}  & 0.016  & 0.015  & 0.014  \\
Star       & (864, 39)    & 0.038  & 0.043  & \textbf{ 0.030}  & 0.0003 \\
Meps       & (6262, 139)  & 0.26   & 0.12   & \textbf{0.11}   & 1.2    \\
Conc.     & (412, 8)     & 0.026  & 0.027  & \textbf{0.022}  & 0.021  \\
Com.     & (797, 101)   & 0.038  & 0.080  & \textbf{0.021}  & 190    \\
\hline
\end{tabular}
\caption{Sample size and feature dimension $(n, p)$, calibration errors for the uncalibrated model, point-calibrated model, Oracle Venn-calibrated model, and average relative prediction set width {\scriptsize $\frac{\mathrm{diam}(f_{n,X_{n+1}})}{f_{n+1}^*(X_{n+1})}$} for the Bike, Bio, Star, Meps, Concrete, and Community datasets.}
\label{tab:example}
\end{table}
