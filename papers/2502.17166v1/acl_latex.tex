% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{url}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{enumerate}
% for table
\usepackage{caption}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{paralist}
\usepackage{multirow} 

\newcommand{\hqz}[1]{\textcolor{magenta}
{$H^{Q}_Z:$ #1}}

\newcommand{\todo}[1]{\textcolor{blue}
{todo: #1}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{\bf
Huanghai Liu\textsuperscript{\rm 1}\thanks{These authors contributed equally to this work.},
Quzhe Huang\textsuperscript{\rm *},
Qingjing Chen\textsuperscript{\rm 3},
Yiran Hu\textsuperscript{\rm 1},\\
\bf Jiayu Ma\textsuperscript{\rm1},
Yun Liu\textsuperscript{\rm 1},
Weixing Shen\textsuperscript{\rm 1}\thanks{Corresponding Author.},
Yansong Feng\textsuperscript{\rm 2$\dagger$}\\
\textsuperscript{\rm 1}School of Law, Tsinghua University \\
\textsuperscript{\rm 2}Wangxuan Institute of Computer Technology, Peking University \\
\textsuperscript{\rm 3}Department of Legal Studies, University of Bologna\\
% \textsuperscript{\rm 4}David R. Cheriton School of Computer Science, University of Waterloo\\
\texttt{\{liuhh23,chenqj21,huyr21,ma-jy24\}@mails.tsinghua.edu.cn}\\
\texttt{\{huangquzhe,fengyansong\}@pku.edu.cn}
\texttt{\{liuyun89,wxshen\}@mail.tsinghua.edu.cn}
}



% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{ \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

The Four-Element Theory is a fundamental framework in criminal law, defining the constitution of crime through four dimensions: Subject, Object, Subjective aspect, and Objective aspect. This theory is widely referenced in legal reasoning, and many Large Language Models (LLMs) attempt to incorporate it when handling legal tasks. However, current approaches rely on LLMs' internal knowledge to incorporate this theory, often lacking completeness and representativeness. 
To address this limitation, we introduce JUREX-4E, an expert-annotated knowledge base covering 155 criminal charges. It is structured through a progressive hierarchical annotation framework that prioritizes legal source validity and employs diverse legal interpretation methods to ensure comprehensiveness and authority.
We evaluate JUREX-4E on the Similar Charge Distinction task and apply it to Legal Case Retrieval, demonstrating its effectiveness in improving LLM performance. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. Code: https://github.com/THUlawtech/JUREX

\end{abstract}

\section{Introduction}
%  将大语言模型应用于特定领域时，例如法律领域，常常需要引入对应的domain knowledge，以补足通用大模型在该领域上的能力缺陷~\cite{}。这类domain knowledge 通常是专家花费大量精力，构建的comprehensive expert knowledge~\cite{}，它们准确、且尽可能的完备。这样的知识标注非常的昂贵，并且再迁移到新的任务时，也难以快速获取。

% 为了减少对完备专家知识的依赖，研究者们尝试让大模型利用易于获取的碎片化专家知识去模仿专家的思考方式，以达到和引入完备专家知识相似的效果。例如在罪名预测时，jiang cong要求模型按照三阶层理论的思考方式进行推理，XX 则要求模型按照四要件理论的思考方式进行推理。其中，三阶层和四要件理论均是中国主流的犯罪构成理论，被中国的学者和法官、律师等广泛使用。这类模拟法学专家思考方式的方法，在法律领域的任务中，展现了比其他通用CoT推理模式更好的效果。

% 这种模仿领域专家思考方式，是否真的能替代真实的专家知识，or 只是comprehensive expert knowledge 难以获取时的无奈之举，目前仍未可知。在本文中，我们希望探究如下两个问题：1）推理时模仿专家思考方式 是否能取得和引入真实专家知识comparable 的效果；2）If not， 我们是否能找到comprehensive expert knowledge 的一种替代，既能利用专家的知识，又不需要过于高昂/复杂的标注/构建代价。

% 我们以四要件理论为例，对比了利用大模型内部知识生成的四要件和专家carefully curated knowledge 之间的区别。四要件是XXXX。通过human eval 我们发现，尽管大模型生成的四要件是较为规范和准确的，但是它缺乏对具体案件的理解/关联。这motivate us 去引入一种更具有代表性知识，以补足大模型本身的不足。

% 具体来说，我们引入了notes-type knowledge。他是XXX，和comprehensive 相比显著易于获得。在human eval 中我们发现它XX不好，但是代表性好，能和llm 本身蕴含的知识互补。因此我们进一步提出了notes-guided method，结合 llm 和 notes 的优势。

% 我们在相似罪名辨析和类案检索上评测了notes-guuided knowledge的质量。我们在相似罪名辨析公开的数据集\cite{liuxiao} 上的实验显示，单独引入LLM-generated 四要件，和引入专家生成的四要件相比，相差XX个点，说明让大模型模仿专家并不能达到真实专家的效果。而引入notes-guided knowledge，则能取得比引入专家知识更好的效果，在F1上进一步提升了XX点。这说明notes-guided XX。我们进一步在类案检索任务上evaluate notes-guided方法的有效性。我们发现引入 notes-guided 知识后，能帮助模型从事实描述中抽取出更准确的细节/案件四要件？，从而在公开数据集XX 上，相对于指利用模型自身能力提升了XX点。

% 我们的贡献如下：
% 1. 我们验证了在法律领域，利用大模型模拟专家的方式和真正的专家知识还有差距。
% 2. 我们提出了一种新的专家经验形式，notes-type knowledge。它并不是像传统的comprehensive expert knowledge 一样，非常规范、准确，但是它获取成本低，并且包含大模型缺乏的XX？
% 我们提出了大模型和notes-guided knowledge结合的方法/框架，并在XX和XX任务上验证了它能超过大模型本身，并且能达到和comprehensive expert knowledge comparable的效果。


In legal AI tasks, enhancing the accuracy and interpretability of Large Language Models (LLMs) in the legal domain often requires the incorporation of legal theories as a support~\cite{jiang2023legal, servantez2024chain,yuan2024can,deng2023syllogistic}. One important theory is the Four-Element Theory of Crime Constitution in Chinese criminal law~\cite{liang2017vicissitudes}. This theory deconstructs criminal conduct into four elements: Subject, Object, Subjective aspect, and Objective aspect, providing clear standards for judicial authorities to determine criminal behavior and helping to prevent the abuse of penal power.

However, most current approaches do not provide additional knowledge but rather rely on the LLM's internal knowledge to incorporate the Four-Element Theory. A common method is to guide LLMs in mimicking expert reasoning processes. For example, designing four separate prompts to guide the LLM outputs in the form of four elements\cite{deng2023syllogistic}.

These methods assume that the model has a solid grasp of the Four-Element Theory, which has not yet been verified. We had LLMs generate the four elements of several complicated crimes in Chinese judicial practice\cite{ouyang1999confusing}, and then asked legal experts to score them. We found that, although LLMs can generate formally standardized and relatively accurate legal descriptions when provided with legal theoretical frameworks and references, the model still underperformed in terms of completeness and representativeness. This shortcoming could affect the accuracy and soundness of subsequent reasoning.

To help LLMs better utilize the Four-Element Theory in legal tasks, we propose \textbf{JUREX-4E: JUR}idical  \textbf{EX}pert-annotated  \textbf{4-E}lement knowledge base for legal reasoning. This knowledge base is annotated using a progressive hierarchy: Article → Judicial Interpretations → Guiding Cases → Academic Discourses, which is built upon the pyramid structure of legal source validity. It incorporates various legal interpretation methods, including textual, systematic, sociological, and purposive interpretations. The knowledge base covers the four elements of 155 high frequency charges, annotated by legal experts over a period of seven months. Each crime's four elements are described in an average of 472.5 words.

To assess the quality of the annotations, we sampled several crimes for human evaluation. The expert annotations achieved an average score of 4.60 on a 5-point scale, while the LLM-generated four elements scored only 3.96, indicating that the expert annotations were of higher quality. To further evaluate the annotations objectively and comprehensively, a direct way is to judge whether different charges can be distinguished according to the four-element definition of crime constitution. Therefore, we introduced the Similar Charge Distinction task~\cite{liu2021everything}. For each case, we provided the four elements of the candidate confused charges and combined them with the case facts as model input. The experimental results showed that injecting expert annotations helped the model better differentiate between similar charges, improving performance with a 0.65 increase in average accuracy and a 0.70 increase in average F1-score, underscoring the superior quality and reliability of expert annotations compared to those generated by the LLM.

We also applied the expert annotations in a specific legal task: Legal Case Retrieval. It is an important step in the practice of analyzing cases and making judgments, requiring the precise application of the four-element theory to compare the criminal composition of cases. We designed a simple retrieval framework guided by expert knowledge, in which the charge's four elements was used to generate four-element descriptions for both the query and candidate cases, and then match similar cases based on their vector similarity. Experiments demonstrated that incorporating expert-annotated four elements improved retrieval performance, as the model became better at focusing on the legal features and key details.

Our contributions are as follows:
\begin{compactenum}[(1)]
\item We verify that LLMs have gaps in understanding the legal theory, highlighting the inadequacy of relying solely on LLM-driven reasoning for legal AI tasks. 

\item We built the JUREX-4E knowledge base, which is the first to incorporate the pyramid structure of legal source validity and covers the four elements of 155 criminal charges under Chinese Criminal Law.

\item  We demonstrated the significance of incorporating criminal composition elements in the Similar Charge Distinction task and proved the superior quality of the expert-annotated four-element knowledge base.

\item We applied JUREX-4E to the Legal Case Retrieval task, found that they do indeed contribute to downstream tasks.
\end{compactenum}

\section{Related Work}

In legal AI, much work has introduced legal theories to enhance reasoning and improve model accuracy and interpretability. For example, legal syllogism prompting (LoT)\cite{jiang2023legal} teaches LLMs for legal judgment prediction by instructing legal syllogism, Chain of Logic\cite{servantez2024chain} guides models in reasoning about compositional rules by decomposing logical statements based on the IRAC (Issue, Rule, Application, Conclusion) paradigm.  Among these, the Four-Elements Theory (FET) of Crime Constitution is a widely adopted framework\cite{yuan2024can,deng2023syllogistic}.

The Four-Element Theory is one of the most widely recognized criminal theories in Chinese judicial practice~\cite{liang2017vicissitudes}. It specifies four essential elements that must be satisfied to establish criminal liability: \textbf{Subject, Object, Subjective aspect, and Objective aspect}. For example, the four elements of the Crime of Affray can be briefly summarized as follows:

(1) Subject: Principal organizers and other active participants who have reached the age of criminal responsibility. (2) Object: Public order. (3) Objective Aspect: The act of assembling brawl, engaging in a brawl, resulting in the following consequences of serious injury. (4) Subjective Aspect: Direct intent, where the person knowingly and willfully engages in organizing or participating in the act of assembling brawl.

Before discussing the Four-Element Theory (FET), it is necessary to briefly compare it with another key theory in Chinese criminal law, the Hierarchical Theory of Crime Constitution\cite{zhou2017hierarchical, zhang2010justification}, and the main distinction between these theories lies in whether a hierarchical structure is considered, with ongoing debates in practice\cite{gao2009rationality, chen2010crime, chen2017comparative, zhou2017debate}. We chose FET as our foundational template for following reasons: 1) its dominance in Chinese judicial practice aligns with real-world criminal judgments; (2) its clear distinction between objective aspects and subjective intent offers direct reasoning checkpoints compared to the Three-Tier Theory; (3) its four-element annotation is flexible and can be adapted to the Three-Tier Theory by prioritizing objective analysis before subjective evaluation\cite{li2006no_reconstruction, zhang2017judicial}.

Recent approaches have leveraged the FET framework to model expert reasoning. For example, breaking down legal rules into FET-aligned components using automated planning techniques \cite{yuan2024can}. Employing model-generated four-element structures as minor premises in legal judgment analysis \cite{deng2023syllogistic}. While these methods have demonstrated improved performance on downstream tasks, they generally assume that the LLMs inherently understand the FET, without systematically validating this assumption. Notably, prior research on criminal charge prediction \cite{an2022charge} suggests that the models may misinterpret key legal concepts and may not be sensitive enough to the subtle differences in fact descriptions of confusing charges, highlighting the need to incorporate expert annotations to support LLM reasoning.

\section{Dataset Construction}

\subsection{Hierarchical Legal Interpretation System}

\begin{figure*}[tp]
%待替换
    \centering
    \includegraphics[width=\textwidth]{fig/main.jpg}
    \caption{Hierarchical Legal Interpretation System base on legal source validity. The legal sources follow a hierarchical order of validity. Thick arrows indicate the primary level where a particular interpretive method is applied, while dashed arrows represent its supplementary use at that level.}
    \vspace{-2ex}
    \label{fig:main}
    
\end{figure*}

Annotating the four elements of crime constitution is essentially a process of legal interpretation, which can be analyzed in two aspects:

(1) \textbf{What law is being interpreted.} This involves identifying the sources of law, including statutory provisions corresponding to a specific charge, their associated judicial interpretations, case precedents, and academic discourses. In legal studies, these sources are categorized based on their legal validity into formal sources (which carry legal forces in judgments) and informal sources (which serve as references without legal forces)\cite{pound1925jurisprudence,watson1982legal,pound1932hierarchy}. Articles and judicial interpretations are considered formal sources, whereas case precedents and academic discourses are regarded as informal sources under the Chinese legal system\cite{zhang2007jurisprudence}.

(2) \textbf{How the law is interpreted.} This pertains to legal interpretation methods, including literal interpretation, systematic interpretation, purposive interpretation, etc. These methods follow a hierarchical order in legal reasoning\cite{sutherland1891statutes,kim2008statutory,eig2014statutory}. Legal interpretation should begin with literal interpretation (textual analysis). If the intended meaning cannot be clearly derived from the article alone, systematic interpretation and purpose interpretation should be applied first. If ambiguity remains, historical interpretation and comparative law interpretation may be used to further clarify the legal meaning. The specific definition is in Appendix\ref{appendix: interpretain}.

Based on these principles, our annotation follows a pyramid structure of Hierarchical Legal Interpretation System base on legal source validity. As shown in Figure \ref{fig:main}, the system is divided into two parts: Legal Source and Legal Interpretation Methods. The main structure of legal source follows a hierarchical order of validity: Article → Judicial Interpretations → Guiding Cases → Academic Discourses, where various legal interpretation methods are applied across different levels. Thick arrows indicate the primary level at which a particular method is used, while thin arrows denote the cross applications.

\subsection{Hierarchical Annotation Path of Legal Sources}

Our Annotators are experts who have all passed the National Uniform Legal Profession Qualification Examination and are familiar with the Four-Element Theory. The entire annotation process took a total of 7 months and involved 4 rounds of annotation according to the validity of the legal source from high to low level.

\paragraph{The First Level: Article.} Legal elements can be seen as an interpretation and refinement of the statutory provisions corresponding to a particular crime. Using \textbf{literal interpretation} as the primary method, the statute is broken down based on its semantic meaning and common usage, ensuring that the interpretation does not extend beyond the possible meaning of the text: 
(1) linguistic analysis follows the subject-predicate-object structure of the provision. (2) To maintain consistency, terms are systematically classified and mapped(e.g: subjective aspect is classified as either intentional or negligent. )(3) Only when it is impossible to make an explicit inclusion or exclusion judgment for an element based on the rules of language use (neutral option field), other interpretation methods should be used. This initial phase takes almost 2 months.

For example, in the crime of robbery, the object "public or private property" represents the protected legal interest. The phrase "forcibly seizing public or private property through violence, coercion, or other means" describes the objective aspect. Since no subject is specified, it is assumed to involve a general subject, and the adverbs "violence" and "coercion" indicate an intentional act. Preliminarily interpret `violence' in the objective aspect as `Use of physical force or power', but the specific forms and subjects of violence need further clarification.

\paragraph{The Second Level: Judicial Interpretation.}
In the 3rd and 4th months, the second level of the hierarchical annotation path focuses on refining legal elements through judicial interpretation. The primary method used for interpreting these materials is \textbf{systematic interpretation}.  This approach examines the position of the corresponding articles within the legal system by analyzing their placement within the structure of laws, including parts, chapters, sections, articles, clauses, and subclauses, as well as their relationship to other statutes and judicial interpretations. Additionally, other interpretative methods, such as sociological interpretation and teleological interpretation, are referenced based on judicial interpretations, related statutory provisions, or bar exam questions.The goal of this level is to clarify the legislative intent by considering the contextual relevance of each provision within the broader legal framework.

For example, in the first level, the objective aspect of "violence" in the crime of robbery requires further clarification, specifically regarding whether violence must be directed exclusively at persons or could also apply to property. Article 289 of Chinese Criminal Law\cite{npc2017criminal} stipulates that in cases of "smashing, looting, and robbing" committed by a group, the ringleaders shall be convicted of robbery if they destroy or seize public or private property. This provision demonstrates that violence against property can also constitute robbery under Chinese law.
% Additionally, bar exam questions were referenced to compare commonly confused situations, highlighting key differences in legal elements. 

\paragraph{The Third Level: Guiding Cases.}
In the 5th to 6th month, \textbf{purposive interpretation and sociological interpretation} are applied to the guiding cases and landmark judgments from the Supreme Court. By examining the social significance of real-world cases, these methods bridge the subtle gap between abstract legal theory and practical cases. This approach enables dynamic adaptation and integration of empirical insights and emerging controversies within the dataset.

For example, in Criminal Trial Reference Case No.159\cite{Zou2002}, the perpetrator lured the victim into a room, locked the door, and seized 170,000 RMB intended for a transaction. The court determined that although the detention did not endanger personal safety, it was sufficient to suppress the victim's resistance, thus constituting "violence" in in the objective aspects of robbery. 
Another example is the "Molestation and Theft Case" \cite{Ma2021Robbery}, where the perpetrator bound the victim, committed molestation, and stole the victim's phone. Since the ongoing molestation reinforced coercion, it constitutes a new act of violence. Thus, the annotation includes "molestation" as an additional method.

\paragraph{The Fourth Level: Academic Discourses.}
In the 7th month, the final stage involves academic expansion. Academic controversies are introduced by employing multiple interpretive methods such as \textbf{comparative law interpretation, purposive interpretation, and sociological interpretation}. These methods include inserting conflict markers at key points of controversy, highlighting the distinctions between mainstream consensus and minority theories, while providing brief annotations of their legal reasoning. This approach ensures the extensibility and academic depth of the dataset.

For example, regarding the crime of robbery, for the main view in China, Soviet Union, North Korea, and Japan explicitly holds that the violence must be severe enough to endanger the  victim’s life or health\cite{Zhang2007}. But some scholars argue that any violence that can forcibly impact the victim's body is sufficient to constitute violence in robbery, no need to endanger the  victim's life or health\cite{Yang2010}. 


\section{Data Distribution}
\begin{table}[h]
\centering    
\resizebox{\linewidth}{!}{
\begin{tabular}{l|llll}
\hline
\multirow{2}{*}{Metric} & \multicolumn{2}{l}{\textbf{LLM}} & \multicolumn{2}{l}{\textbf{Expert}} \\ \cline{2-5} 
 & Mean & Median & Mean & Median \\ \hline
Avg. Length & 115.43 & - & 472.53 & - \\
SB & 23.12 & 27 & 51.64 & 17 \\
OB & 15.86 & 15 & 36.01 & 25 \\
SA & 28.00 & 30 & 42.38 & 21 \\
OA & 48.45 & 45 & 342.5 & 230 \\ \hline
\end{tabular}
}
\caption{Comparison of Legal Element Lengths: LLM vs. Expert. SB = Subject, OB = Object, SA = Subjective Aspect, OA = Objective Aspect. }
\label{tab:element_length_comparison}
\end{table}

As shown in Table \ref{tab:element_length_comparison}, we compare the length of legal elements between expert-annotated descriptions in JUREX-4E and LLM-generated outputs across 105 charges that overlap with the Lecard-V2 dataset~\cite{li2024lecardv2}, which is one of the most comprehensive legal datasets, covering 184 criminal charges. We find that:

(1) The average total length of expert annotations (472.53) is more than four times longer than that of LLM-generated outputs (115.43), indicating that the former include more detailed information.

(2) The median difference between the Subject (SB), Object (OB), and Subjective Aspect (SA) is relatively small, as these elements are typically fixed. For example, the SB is often a general entity, and the SA is often intent or negligence.

(3) The median and mean values for SB and SA in the expert annotations differ, especially for SB (17 v.s. 51.64). This discrepancy arises because certain specialized charges may require more detailed explanations. For example, in the crime of copyright infringement, the definition of ``work'' under the subject element has 9 occasions. Detailed data distribution for each element is provided in Appendix \ref{appendix:Detailed Data Distribution for Each Element}.

(4) The main difference between Expert and LLM is in the Objective Aspect (342.5 v.s. 48.45 in Mean). This is because the OA includes a range of factual elements describing the criminal behavior, such as the conduct, object, result, time, and location, which are most emphasized in legal provisions and are central to various legal interpretive theories.


\section{Human Evaluation}
\label{human eval}
% \hqz{In order to evaluate whetehr the LLM have already handle FEt perfectly, we ask legal experts to compare the FE generated by LLM and two kinds of experts knowledge. (NEEDS polish)}
We selected 6 complicated crimes in Chinese judicial practice\cite{ouyang1999confusing} to evaluate whether the LLM can handle the Four-element Theory. Drawing from previous work\cite{deng2023syllogistic,cui2024chatlaw,zhou2023boosting}, we define LLM-generated knowledge as information produced by the LLM based on its pre-trained knowledge and contextual prompts. For detail, we provide the LLM with legal articles and the definition of each element in FET, prompting it to generate the four-elements base on these metrical. The LLM is expected to autonomously identify and generate the four elements based on its learned understanding of legal concepts. 

We invite legal experts to assess the four elements generated by the LLM from four dimensions: \textbf{Precision, Completeness, Representativeness, and Standardization}. 

\begin{compactitem}
  \item Precision: Whether the key components of each element are accurately identified. This dimension mainly evaluates whether the four elements faithfully represent the legal provisions.
  \item Completeness: Whether all necessary information of each element is included. This assesses whether any essential content is missing, such as the omission of a description for specific subjects, like government officials. 
  \item Representativeness: Whether the annotations highlight the most critical scenarios in judicial practice. For example, in crimes of intentional injury, this would involve describing the representative means of harm.
  \item Standardization: Whether the four elements are clearly defined, ensuring consistency in the expression of identical elements across different crimes (e.g., consistent description of general subjects), with concise and easily understandable explanations, free from legal ambiguities or misunderstandings.
\end{compactitem}

% \hqz{Do they pass the bar examination？ If they do, we should emphasis that point to show they are real legal experts. }
Each dimension was scored by two types of experts: one group with a pure legal background and another group with a combined background in law and Artificial Intelligence, all of whom have passed the bar examination. The experts were selected to balance domain expertise and interdisciplinary perspectives. Scores were averaged across the two groups. Details about 1-5 scale criteria and annotator background are provided in Appendix \ref{appendix: human eval guidance}.


\begin{table}[tp]
    \centering
    \small
    % \resizebox{\linewidth}{!}{%
    \begin{tabular}{l|ccc}
    \hline
    \textbf{Dimension} & \textbf{LLM} & \textbf{Expert} & \textbf{$\delta$} \\
    \hline
    Precision          & 4.12  & 4.69  & + 0.57  \\
    Completeness       & 3.79  & 4.65  & + 0.86  \\
    Representativeness & 3.60  & 4.48  & + 0.88  \\
    Standardization    & 4.33  & 4.56  & + 0.23  \\
    \hline
    \end{tabular}%
    % }
    \caption{Performance comparison of four elements across methods. $\delta$ represents the score difference between expert and LLM-generated four-elements, with experts outperforming LLMs in all dimensions.}
    \label{tab:Legal Dimension Metric-res}
\end{table}

As shown in Table \ref{tab:Legal Dimension Metric-res}, expert annotations consistently outperform LLM-generated elements across all four dimensions, highlighting the limitations of LLMs in understanding legal elements. The most pronounced deficiencies are observed in Completeness (+0.86) and Representativeness (+0.88). This suggests that while LLMs can generate formally standardized and relatively accurate four elements, their description are not specific enough and do not adequately reflect the representative features of a charge's criminal composition.


\section{Evaluate Expert Knowledge on Charge Disambiguation}
% \hqz{See the Chinese comments below}

% 在上文中，human eval 展示了结合notes 和 模型内部只是，有望生成更好的四要件只是。在本小节中，我们将借助“相似罪名辨析”任务，进一步的定量分析notes-guieded knowledge 是否能帮助模型更好的掌握/理解四要件理论？ 以及它和专家carefully curated knowledge 之间的差异。

% 需要一段描述，来说明为什么相似罪名辨析任务的表现能够反映四要件的好坏。我出不构想的结构式，先简单介绍相似罪名辨析这个任务，然后去说想要做好这个任务，需要分辨相似罪名之间的要件差异，所以能体现要件的质量/理解？我记得之前有一个文章也是引入四要件来做相似罪名辨析的，可以参考一下它怎么说的，以及引用一下人家。-section4

In the preceding section, the human evaluation demonstrated that experts annotated higher-quality four-elements. To further quantitatively evaluate the annotations, a direct way is to judge whether different charges can be distinguished according to the four-element definition of crime constitution. Therefore, we introduce the Similar Charge Disambiguation (SCD) task\cite{yuan2024can,li2024graph}. 

\subsection{Experiment Settings}
% \hqz{How to use notes-guided knowledge to do the Classification? The description below is too simple to understand. Explain it in detail, and perhaps put a prompt here.}

% 为了检验notes-guided 四要件的有效性，我们也对比了引入其他类型四要件的方法，这些方法引入四要件的方式和notes-guided 相同，只不过把XX中的四要件替换成了XX？能这么说吗，我记得不同的四要件，引入的prompt好像还不一样。-生成不同的四要件的prompt不一样，但是引入的prompt一样；四要件和COT引入四要件之类的方法的prompt不一样


\subsubsection{Dataset}
% \hqz{Why choose this dataset? You should say some advantages of this dataset. Why not use the 3-label-classification? And you shouldn't use abbreviation like F, E here. You should describe them explicitly.}

We chose the dataset released by \cite{liu2021everything}, which includes five charge sets with the largest number of cases. To evaluate performance on representative tasks, we selected three 2-label classification groups commonly examined in other datasets \cite{yuan2024can}: Fraud \& Extortion (F\&E), Embezzlement \& Misappropriation of Public Funds (E\&MPF), and Abuse of Power \& Dereliction of Duty (AP\&DD). Each crime has over 1.9k cases, with a total of 13,962 cases. The details of the classification groups are shown in Appendix \ref{app:scd}. Following previous work~\cite{liu2021everything, yuan2024can}, we use Average Accuracy (Acc) and macro-F1 (F1) as evaluation metrics. 


 % \hqz{For binary classification, is it possible to have multi-labels? And what's the meaning of the two numbers in the column Cases?}
 
% \hqz{The baselines are not just listing all the methods we compare with. We need to give some reasons that why we choose them.}
% For traditional models on SCD, we evaluate some typical models with causal inference chains from prior work, including GCI, CausalChain~\cite{liu2021everything}, Bi-LSTM~\cite{zhou2016attention}, Bi-LSTM+Att, and Bi-LSTM+Att+Cons. For LLMs, we employ the latest GPT-4o model\cite{achiam2023gpt} as baseline and introduce GPT-4o+Article, which directly incorporates legal statutes. Additionally, we consider two reasoning-based approaches: Legal-CoT, a variant of Chain-of-Thought incorporating FET steps\cite{kojima2022large}, and MALR~\cite{yuan2024can}, a multi-agent framework that decomposes complex legal tasks into four-element types to extract legal insights.
\subsubsection{Baselines and Methods}

To evaluate SCD tasks, we consider two ways of incorporating legal knowledge. The first directly integrates legal statutes, represented by GPT-4o~\cite{achiam2023gpt} as the baseline and GPT-4o+Article, which explicitly provides relevant legal articles to the model. The second adopts structured legal reasoning to enhance interpretability and accuracy. We consider Legal-CoT, a Chain-of-Thought~\cite{kojima2022large} variant that conducts a stepwise analysis based on the FET, and MALR~\cite{yuan2024can}, a multi-agent framework that decomposes legal tasks into sub-tasks in four-element structures. Details of each baseline are provided in Appendix \ref{appendix: SCD baselines}. 

We use an unified approach to introduce four-element descriptions. For each group of similar charges, the model receives charges' four-elements from JUREX-4E or generated by LLM to aid classification. Specifically, GPT-4o+FET\textsubscript{Expert} relies on expert-annotated four-elements, while GPT-4o+FET\textsubscript{LLM} relies on LLM-generated four-elements. As shown in Appendix \ref{app:scd}, the instruction format is consistent across methods, with only the \textit{[Four Elements of candidate charges]} varying based on the source. All experiments are conducted in a zero-shot setting, with the max\_tokens set to 3,000 (or 10,000 for COT and MALR reasoning) and temperature set to 0 or 0.0001(In repeated experiments). 



\subsection{Results}
% \hqz{See the Chiense comments below}
% 实验结果是有套路的，先总的说overall，我们的方法在XX上表现罪好，说明XX。然后后面跟着具体的分析。
% 具体分析：1）对比大模型和之前的方法，说明大模型内部知识是有用的。2）对比GPT-4o，GPT-4o_law 和 legal-CoT，后两个比第一个好，说明引入法律知识是有价值的。比较4o-law 和 CoT，后者效果不好，说明大模型对四要件的理解不够好？可以拉踩一下之前的工作，说之前这样只引入四要件的框架，但是完全依赖模型内部知识不好？3）对比FET_Notes，FET_LLM 和 FET_Expert，FET_Expert 表现最好，符合人工评分，而FET_Notes 不如FET_expert 也符合预期，毕竟后者是更详细的标注；4）最后再说FET_Notes_guided，达到了最好的效果，甚至超越了Expert。再结合Notes 没有超过LLM，但是Notes-guided 超过了LLM，说明Notes 和 LLM 之间的知识有互补？
\begin{table*}[]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{l|cc|cc|cc|cc}
\hline
\textbf{Model} & \multicolumn{2}{c|}{\textbf{F\&E}} & \multicolumn{2}{c|}{\textbf{E\&MPF}} & \multicolumn{2}{c|}{\textbf{AP\&DD}} & \multicolumn{2}{c}{\textbf{Average}} \\ \hline
 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\
GPT-4o & 94.36 & 95.81 & 86.49 & 89.76 & 85.54 & 87.12 & 88.72 & 90.07 \\
GPT-4o+Article & 95.34 & 96.30 & \textbf{92.64} & 93.03 & 88.30 & 89.33 & 92.09 & 92.89 \\
Legal-COT & 94.99 & 96.27 & 90.50 & 90.99 & 87.81 & 88.14 & 89.95 & 90.85 \\
MALR &94.62&95.82&86.99&86.98& 87.86	& 88.68 &89.82 &90.49\\
GPT-4o+FET$_{LLM}$ & 95.73 & 96.56 & 91.87 & 92.01 & 89.61 & 89.69 & 92.40 & 92.75 \\
GPT-4o+FET$_{Expert}$ & \textbf{96.06} & \textbf{96.69} & 92.57 & \textbf{93.05} & \textbf{90.53} & \textbf{90.62} & \textbf{93.05} & \textbf{93.45} \\
\hline
\end{tabular}}
  \caption{\label{tab:charge-level-res}
   Results of Charge Disambiguation. FET means introducing the Four-element theory with knowledge obtained from experts and LLM method. Highest results are in bold. 
  }
\end{table*}
% 结论：expert knowledge 在相似罪名辨析上表现最好，说明专家标注优于其他利用大模型直接或者间接引入四要件的方式。具体而言，
% 1）对比GPT-4o，GPT-4o_law 和 legal-CoT和MALR，后3个比第一个好，说明引入具体的法律知识是有价值的。
% 2）比较4o-law 和 CoT/MALR，后者效果不好，说明大模型对四要件的理解不够好 
% 3）对比GPT4+FET_LLM 和 FET_Expert，FET_Expert 表现更好，说明专家标注的质量更高。


As shown in Table \ref{tab:charge-level-res}, the GPT-4o+FET\textsubscript{Expert} performs best in discriminating similar charges, indicating that expert annotation is superior to other methods of directly or indirectly introducing FET with LLMs. Specifically, we can derive the following observation:

\paragraph{Effectiveness of Domain-Specific Legal Knowledge:} Among all approaches, those that explicitly incorporate domain-specific legal knowledge, such as GPT-4o+Article, Legal-CoT, and MALR, outperform GPT-4o alone. This highlights the importance of integrating legal knowledge.

\paragraph{Importance of Concrete Four-element Knowledge:} 
The accuracy of both Legal-CoT and MALR is still lower than GPT-4o+FET methods. This suggests that, compared to embedding the Four-Element Theory into LLMs' reasoning process, providing concrete charge four-elements enables the model to better understand the different crimes' composition.

\paragraph{Superiority of Expert Annotations:} Compared with the indirect introduction of FET reasoning, the method of directly introducing four-elements to the model (GPT-4o+FET) achieves better results. Notably, GPT-4o+FET\textsubscript{Expert} surpassing the GPT-4o+FET\textsubscript{LLM}  by 0.65 in average accuracy and 0.70 in average F1-score, underscoring the superior quality and reliability of expert annotations in legal tasks, aligning with human evaluations in Table \ref{tab:Legal Dimension Metric-res} and reaffirming the critical role of human expertise in legal decision-making.


\section{Can Expert Knowledge Benefit More Downstream Tasks?}
% \hqz{The same question, why we choose legal case retrieval? As an application of FET? See the Chinese comments below}
% 说明LCR 的重要性确实是有必要的，但更重要的是，你要说清楚为什么这个任务能用来验证notes-guided FET的好坏。至少可以说两点，一个是这个任务需要FET，另一个是这个任务的搜索池很大，需要对所有罪名对四要件都给出标记。这种情况下专家标注很困难，更需要像notes这样的低成本标注手段？-section4

In this section, we design a simple framework to apply the expert-annotated four elements to Legal Case Retrieval (LCR), a task in which relevant cases are retrieved based on given facts. It is an important step in the practice of analyzing cases and making judgments, and it requires the precise application of the four-element theory to matches cases with similar criminal compositions.

\begin{figure}[tp]
    \centering
    \includegraphics[width=1\columnwidth]{fig/1111_05.jpg}
    \caption{An expert-guided FET method to enhance legal case retrieval by incorporating expert four elements.}
    \label{fig:Legal Case Retrieval}
\end{figure}

\subsection{Method}

\begin{table*}[h!]
\resizebox{\linewidth}{!}{
\begin{tabular}{l|rrr|rrrrr|r}
\hline
\textbf{Model}  & \textbf{NDCG@10} & \textbf{NDCG@20} & \textbf{NDCG@30} & \textbf{R@1}    & \textbf{R@5}    & \textbf{R@10}   & \textbf{R@20}   & \textbf{R@30}   & \textbf{MRR}    \\ \hline
% QL               & 0.4438           & 0.4965           & 0.5372           & 0.0977          & 0.2831          & 0.4158          & 0.5517          & 0.6421          & 0.1969          \\
% BM25             & 0.4046           & 0.4650            & 0.5095           & 0.0681          & 0.2608          & 0.3889          & 0.5384          & 0.6467          & 0.1719          \\ \hline
BERT             & 0.1511           & 0.1794           & 0.1978           & 0.0199          & 0.0753          & 0.1299          & 0.2157          & 0.2579          & 0.1136          \\
Legal-BERT       & 0.1300           & 0.1487           & 0.1649           & 0.0186          & 0.0542          & 0.1309          & 0.1822          & 0.2172          & 0.0573          \\
Lawformer        & 0.2684           & 0.3049           & 0.3560           & 0.0432          & 0.1479          & 0.2330          & 0.3349          & 0.4683          & 0.1096          \\
ChatLaw & 0.2049           & 0.2328           & 0.2745           & 0.0353          & 0.1306          & 0.1913          & 0.2684          & 0.3751          & 0.1285          \\
SAILER           & 0.3142           & 0.4133           & 0.4745           & 0.0539          & 0.1780          & 0.3442          & 0.5688          & 0.7092          & 0.1427          \\
GEAR             & *                & *                & *                & 0.0630           & 0.1706          & 0.3142          & 0.4625          & *               & 0.2162          \\ \hline
BGE              & 0.4737           & 0.5539           & 0.5937           & 0.0793          & 0.2945          & 0.4298          & 0.6500          & 0.7394          & 0.1926          \\
FET$_{LLM}$          & 0.5139           & 0.5862           & 0.6291           & 0.0980           & 0.2967          & 0.4769 & 0.6802          & 0.7828          & 0.2140          \\
\hspace{2em}\small\textit{{- base}}              & \small{0.3583}           & \small{0.4293}           & \small{0.4798}           & \small{0.0506}          & \small{0.2240}          & \small{0.3644}          & \small{0.5383}          & \small{0.6652}          & \small{0.1453}          \\ 
FET$_{Expert\_guided}$       & \textbf{0.5211}  & \textbf{0.5920}  & \textbf{0.6379}  & \textbf{0.1024} & \textbf{0.3049} & \textbf{0.4883}   & \textbf{0.6885} & \textbf{0.7967} & \textbf{0.2155} \\
\hspace{2em}\small\textit{{- base}}           & \small{0.3766}           & \small{0.4584}          & \small{0.5111}         & \small{0.0715}         & \small{0.1894}          & \small{0.3709}          & \small{0.5891}          & \small{0.7203}          & \small{0.1624}          \\\hline
\end{tabular}}
  \caption{SCR results. Bold fonts indicate leading results in each setting. * denotes that the indicator is not applicable to the current model. }
  \label{tab:SCR results}
\end{table*}
% \hqz{How to use notes-guided knowledge to do the LCR?}

We implement a standard dense retrieval approach \textbf{BGE} using BGE-m3 \cite{bge_m3}, an advanced embedding model for dense retrieval. Given a query \( q \) and a candidate case \( c \), their vector representations  \( \mathbf{v}_q \) and \( \mathbf{v}_c \) are obtained through shared encoder \( E \): \(\mathbf{v}_q = E(q), \quad \mathbf{v}_c = E(c)\).
We used the BGE-m3 model without fine-tuning as the shared encoder. Next, the relevance score is computed via cosine similarity:
\begin{equation}
\text{sim}_{\text{base}}(q,c) = \frac{\mathbf{v}_q \cdot \mathbf{v}_c}{\|\mathbf{v}_q\| \|\mathbf{v}_c\|}
\end{equation}
To retrieve the top-k most similar cases, we rank the candidates based on their cosine similarity to the query. Denote the set of candidate cases as \( C = \{c_1, c_2, \dots, c_n\} \), where \( n \) is the total number of candidate cases. We compute the similarity for each \( c_i \in C \), and select the top-k candidates with the highest similarity scores.

 As shown in Figure \ref{fig:Legal Case Retrieval}, to leverages expert-annotated four elements of charges, we introduce an \textbf{BGE+FET\textsubscript{Expert\_guided}} method for the retrieval process, consisting of three steps: (1) Predicting charges, a LLM $\mathcal{M}_p$ predicts potential charges $Z=\{z_1,...,z_k\}$ from case facts. (2) Matching elements, retrieving corresponding charge's four-elements $\{f_z\}_{z\in Z}$ in JUREX-4E. (3) Analyzing case facts. Guided by $\{f_z\}$, another LLM $\mathcal{M}_g$ generates case-specific four elements $a_c$ for candidate $c$. The final similarity score combines factual and theoretical alignment:
\begin{equation}
    \text{sim}_{\text{final}}(q,c) = \alpha \cdot \text{sim}_{\text{base}}(q,c) + (1-\alpha) \cdot \text{sim}_{\text{f}}(a_q,a_c)
\end{equation}
where $\alpha=0.7$ and \(\text{sim}_{\text{f}}\) measures the similarity between the generated four-element descriptions.
 
 To facilitate comparison, we also design a \textbf{BGE+FET\textsubscript{LLM}} method that directly prompt the LLM $\mathcal{M}_g$ with the concept of Four-Element Theory to generate case-specific four elements $a_c$.


\subsection{Dataset}
% \hqz{See the comments below}
% % 为什么选这个数据集？因为它是widely-used？引一些用了这个数据集的文章？
% % 以及在全集上做实验，不是test the effect of increase the size of the dataset 吧？应该说在这个数据集上有两种常见的评测方式，一种是使用一个子集\cite{}，另一个使用全集\cite{}。然后说我们在这两个设置上都进行了实验。
LeCaRDv2\cite{li2024lecardv2} is the latest version of LeCaRD\cite{ma2021lecard}, which is widely used in LCR task \cite{li2024delta,zhou2023boosting}. It comprises 800 queries and 55,192 candidates extracted from 4.3 million criminal case documents. There are two common evaluation settings for this dataset: one uses a subset \cite{qin2024explicitly} with a candidate pool size of 1,390, while the other uses the full set \cite{li2024lecardv2} with a candidate pool size of 55,000. We conducted experiments under both settings.

% \hqz{Need citations. Following previous work, we use XXX as evaluation metrics}
Following previous work\cite{feng2024legal,qin2024explicitly}, we adopt commonly used evaluation metrics. For the subset, we use NDCG@10, 20, 30, Recall@1, 5, 10, 20, and MRR. For the full dataset, we use Recall@100, Recall@200, Recall@500, and Recall@1000.

\subsection{Baselines}

Consistent with earlier work\cite{li2024lecardv2,qin2024explicitly}, we compare some dense retrieval methods, including: BERT\cite{devlin2018bert}, Lawformer\cite{xiao2021lawformer}, ChatLaw-Text2Vec\footnote{\url{https://modelscope.cn/models/fengshan/ChatLaw-Text2Vec}}\cite{cui2023chatlaw}, SAILER\cite{li2023sailer}, GEAR\cite{qin2024explicitly}. Details of each baseline is shown in Appendix \ref{appendix:LCR baselines}. These baselines are implemented using the FlagEmbedding Toolkit\footnote{\url{https://github.com/FlagOpen/FlagEmbedding}} with a RTX 3090.

\subsection{Results}
% \hqz{See the comments below}
% 1）对比BGE 和 其他方法，说明我们用了一个很强的baseline。2）基于FET的SCR模型优于其他baseline，说明我们提出的引入四要件的框架是有效的。3）对比expert 和 LLM，说明单纯LLM的知识不够，专家知识是有必要的。最后可以提一嘴expert  和 expert -guided 的对比，说仅有expert 也不够，需要和大模型内部知识做融合，体现我们方法的优越性。

The LCR results are shown in Table \ref{tab:SCR results}, where we can observe that:

\paragraph{FET Works Well in LCR.} The baseline model BGE achieves strong performance across most metrics compared to previous methods. Introducing the Four-Element Theory (FET) further improves its results, with relative MRR improvements of 11.11\% for FET\textsubscript{LLM} and 11.89\% for FET\textsubscript{Expert\_guided}, indicating that introducing legal theory is effective.

\paragraph{Expert Knowledge is Necessary.} By leveraging external knowledge, FET\textsubscript{Expert\_guided} achieves significant improvements across all of the metrics. Specifically, using expert-guided case four-elements (FET\textsubscript{Expert\_guided}\textit{-base}) outperforms LLM-generated case four-elements (FET\textsubscript{LLM}\textit{-base}) by an average of 11.77\% in MRR, demonstrating the critical role of expert knowledge in enhancing retrieval precision. A case study in Appendix \ref{appendix:lcr-case} shows that the expert four-element for charges provide practical judgment points and key narratives (e.g., the special subject of the Crime of Embezzlement) that help the LLM focus on essential facts to analyze the case.

We also evaluated the FET method on the full set, as shown in Table \ref{tab:full set SCR results} , and the results remain consistent, with the expert-guided method still performing best.


\section{Conclusion}
In this paper, we propose an expert-annotated knowledge base, evaluate its quality in the Similar Charge Distinction task, and apply it to the Legal Case Retrieval task. Our results demonstrate that expert annotations significantly enhance LLMs' understanding of the Four-Element Theory. The four-element annotations, enriched with professional legal interpretations, provide strong support for LLMs' reasoning capabilities. This approach can be extended to other legal AI tasks, such as legal document analysis and contract interpretation.


\section{Ethical Considerations}

The datasets used in our evaluation are sourced from publicly available legal datasets, with all defendant information anonymized to ensure privacy. 

\section{Limitations}
% 虽然我们请专家按照层级金字塔结构进行了详细的标注，但由于标注的耗费较大，本工作还是局限于中国刑法领域的155个罪名上。但本工作提出的annotation方法和四级annotaion结构可以为之后民法、商法等其他领域的迁移提供参考。虽然本工作只是在做刑法的四要件理论，但是标注四要件过程中使用的基于法源效力的法律解释体系（文义解释、体系解释、社会学解释、学理解释）是法理学层面的设计，可以扩展到不同国家、不同法系、不同类别的法律。
As a limitation, this knowledge base focuses on the Four-Element Theory within the context of 155 crimes under Chinese Criminal Law. However, the four-level hierarchical pyramid annotation structure based on the legal interpretation system proposed in this work provides valuable insights for future expansion to other legal domains, as it represents a theoretical framework in the field of jurisprudence. The interpretative methods within the legal interpretation system, including textual, systematic, sociological, and doctrinal interpretations, are universally recognized in international law field and can be applied to different laws, countries, and legal systems. 


\section*{Acknowledgments}
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Detailed Data Distribution for each Element}
\label{appendix:Detailed Data Distribution for Each Element}

\begin{figure*}[]
    \centering
    \begin{minipage}{0.40\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/dd/ex_total.png}
        \caption{The average length distribution of total four elements annotated by experts.}
    \end{minipage}%
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/dd/ex_single.png}
        \caption{The length distribution of each element annotated by experts.}
    \end{minipage}
    
    \vspace{1cm}
    
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/dd/llm_total.png}
        \caption{The average length distribution of total four elements generated by LLM.}
    \end{minipage}%
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/dd/llm_single.png}
        \caption{The length distribution of each element generated by LLM.}
    \end{minipage}
    
\end{figure*}


\section{Interpretation Methods}
\label{appendix: interpretain}
1. Literal Interpretation

A strict textual analysis method that adheres to the ordinary meaning of words as understood by a reasonable person at the time of enactment, excluding subjective intent inference

2. Systematic Interpretation

An approach interpreting legal provisions through their position within the codified legal hierarchy and logical connections with related norms, maintaining the integrity of the legal system (aligned with Dworkin's "law as integrity" theory).

3. Purposive Interpretation

A method discerning the objective legislative purpose through analysis of statutory structure and functional goals, distinct from subjective legislative intent (following Hart \& Sacks' legal process school).

4. Historical Interpretation

Interpretation based on legislative history materials including drafts, debates and official commentaries, while distinguishing original meaning from framers' subjective intentions (as per Brest's original understanding theory).

5. Comparative Interpretation

A methodology referencing functionally comparable legal systems sharing common juridical traditions, employing analogical reasoning while considering local legal culture (developed through Gottfried Wilhelm Leibniz's comparative law framework).

6. Sociological Interpretation

Interpretation evaluating social efficacy through empirical analysis of implementation effects, guided by Pound's sociological jurisprudence principle that "law must be measured by its achieved results".

\section{Human Evaluation Guidance}
\label{appendix: human eval guidance}

The annotators included three postgraduate students specializing in criminal law and one master's student in legal science and technology.
The annotators scored independently, without knowledge of each other's results. Before scoring, they were asked to read the descriptions and scoring guidelines (as shown in Table \ref{tab: four-dimension score details}) for each evaluation dimension. In order to ensure the fairness of the evaluation, they do not know the source of each four elements, and even do not know that these four elements include those generated by LLMs. 

When assigning scores, they were also required to provide brief justifications. For example, for the Completeness dimension: 3 (The description of Objective Aspect is too brief, and does not specify the intent of illegal possession).

\begin{table*}[h]
\centering
\small
\renewcommand{\arraystretch}{1.5} % Adjust row height
\begin{tabularx}{\linewidth}{l|XXXX}
\hline
\multicolumn{1}{c|}{\textbf{Dimension}} & \multicolumn{1}{c}{\textbf{Precision}} & \multicolumn{1}{c}{\textbf{Completeness}} & \multicolumn{1}{c}{\textbf{Representativeness}} & \multicolumn{1}{c}{\textbf{Standardization}} \\ \hline
\textbf{Definition} & Whether there are errors in key elements & Whether the four elements are complete & Whether key elements and scenarios are emphasized & Whether language and format are clear and standardized \\ \hline
\textbf{Score 1} & Contains numerous obvious errors, severely impeding the judgment of culpability, exculpation, and conviction, leading to significant deviations. & Severe omission of key content, unable to present a complete picture of the crime structure, greatly hindering analysis of criminal behavior. & Completely fails to mention any key elements or scenarios, unable to highlight essential points for crime recognition, offering no assistance in conviction. & Language is extremely chaotic and obscure; format lacks any standardization, greatly hindering comprehension and application. \\
\textbf{Score 2} & Contains multiple noticeable errors, significantly interfering with culpability, exculpation, and conviction judgments, potentially leading to partial errors. & Noticeable omissions in content, failing to comprehensively cover crime elements, affecting thorough analysis of criminal behavior. & Only highlights a minimal and unimportant portion of the key elements, providing weak support for understanding key crime features. & Language is relatively vague and inaccurate, with a casual format that makes content comprehension significantly challenging. \\
\textbf{Score 3} & Contains a few errors, but the overall accuracy in determining culpability, exculpation, and conviction is relatively unaffected, unlikely to lead to judgment errors. & Some key content descriptions are incomplete, but they generally present the framework of the crime structure. & Highlights some relatively important key elements but lacks comprehensiveness and prominence, offering limited assistance in crime identification. & Language is generally clear but may have minor deviations in phrasing or formatting. \\
\textbf{Score 4} & Almost error-free, key elements accurately serve culpability, exculpation, and conviction judgments, ensuring the accuracy of results. & Key elements are mostly complete, with only very slight and non-critical deficiencies that do not hinder a comprehensive analysis of the crime. & Clearly and relatively comprehensively highlights key elements, aiding in accurately identifying crucial aspects of criminal behavior. & Language is clear and accurate, format is relatively standardized, facilitating comprehension and application of relevant content. \\
\textbf{Score 5} & Completely error-free, key elements are precisely defined, achieving highly accurate culpability, exculpation, and conviction judgments without any flaws. & All four elements are complete and detailed, covering every aspect of the crime, perfectly presenting the crime structure. & Precisely and comprehensively highlights all crucial elements, enabling immediate grasp of the core aspects of the crime, significantly aiding conviction. & Language is extremely clear, standardized, and concise; format perfectly meets requirements, with no barriers to understanding, ensuring efficient information delivery. \\ \hline
\end{tabularx}
\caption{The four dimensions of the human evaluation and the specific score description.}
\label{tab: four-dimension score details}
\end{table*}

\section{Details for Similar Charge Disambiguation}
\label{app:scd}


\begin{table}
\renewcommand{\arraystretch}{1.5} % Adjust row height
  \centering
  \small
\begin{tabularx}{\linewidth}{l|X|l}
\hline
\textbf{Charge Sets} & \textbf{Charges} & \textbf{Cases} \\
\hline
% Personal Injury & Intentional Injury \& Murder \& Involuntary Manslaughter & 6377 / 2282 / 1989 \\
% Violent Acquisition & Robbery \& Seizure \& Kidnapping & 5020 / 2113 / 622 \\
F\&E & Fraud \& Extortion & 3536 / 2149 \\
E\&MPF & Embezzlement \& Misappropriation of Public Funds & 2391 / 1998 \\
AP\&DD & Abuse of Power \& Dereliction of Duty & 1950 / 1938 \\
\hline
\end{tabularx}
  \caption{\label{tab:GCI-dataset}
    Distribution of charges in the GCI dataset. Cases denotes the number of cases in each category. Following \cite{liu2021everything}, for a case with both confusable charges, the prediction of any one of the charges is considered correct. 
  }
\end{table}

\begin{table*}[tp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|}
\hline
\textbf{Prompt:} \\
You are a lawyer specializing in criminal law. Based on Chinese criminal law, \\
please determine which of the following candidate charges the given facts align with. \\
\textcolor{red}{The candidate charges and their corresponding four elements are as follows:} \\
\textit{\textcolor{red}{[Four Elements of Candidate Charges]}}. \\
The four elements represent the core factors for determining the constitution of a criminal charge.\\
\textit{[The basic concepts of the Four-Element Theory]} \\
Please Compare the case facts to determine which charge's four elements they align with, thereby identifying the charge. \\
\hline
\end{tabular}}
\caption{Prompt template for adding the Four-Element Theory and specific four elements of crime in charge disambiguation.}
\label{tab:SCD prompt}
\end{table*}

\label{appendix: SCD baselines}

For LLM baselines, we evaluate both general-purpose and task-specific methods.  
% \hqz{Why choose these baselines? }

\textbf{GPT-4o} is an optimized version of GPT-4\cite{achiam2023gpt} that has well performance in specific tasks through domain adaptation. 

To explore the effectiveness of notes-guided four elements in LLMs, we further consider other methods that introduced the Four-element theory into LLMs.

\textbf{GPT-4o\textsubscript{Law}}, which introduces articles related to corresponding charges into the instruction to provide legal context.  

\textbf{Legal-COT} is a variant of COT~\cite{kojima2022large} that guides the LLM to perform step-by-step legal reasoning by incorporating explanations of the Four-element theory into the instruction.

\textbf{MALR} is a up to date multi-agent framework designed to enhance complex legal reasoning \cite{yuan2024can}, enabling LLMs to autonomously decompose legal tasks and extract insights from legal rules. As its full implementation is not publicly available, we use the released code for the auto-planner module and implement the legal insight extraction following the specified steps and prompts, with necessary refinements. Experiments on the paper's reported examples show that our implementation produces task decompositions and outputs largely consistent with the original results.

As shown in Table \ref{tab:prompt of different method}, different methods differ in their prompts for generating and explaining the Four-Element Theory, but generally follow a similar process. For the SCD output, except for COT and MALR, which require reasoning processes and prediction results, all other methods only require the output of prediction results. 


\begin{table*}[htbp]
\resizebox{\linewidth}{!}{
\renewcommand{\arraystretch}{1.5} % Adjust row height
\centering
\begin{tabularx}{\linewidth}{l|XXXXX}
\toprule
\textbf{Method} &
  \textbf{GPT-4o} &
  \textbf{GPT-4o+Article} &
  \textbf{Legal-COT} &
  \textbf{GPT-4o+FET\textsubscript{LLM}} &
\textbf{GPT-4o+FET\textsubscript{Experts}}  \\
\midrule

Pre-task &
  None &
  None &
  None &
  LLM-generated four elements &
  Expert-annotated four elements  \\ 
\midrule

Prompt &
  \multicolumn{5}{p{13cm}}{\raggedright You are a lawyer specializing in criminal law. Based on Chinese criminal law, please determine which of the following candidate charges the given facts align with.} \\ \cline{2-6}

 &
  Candidate charges are as follows: \textit{\textcolor{red}{\#Candidate Charges}} &
  The candidate charges and relevant legal articles are as follows: \textit{\textcolor{red}{\#Candidate Charges + \#Articles}} &
  Please analyze using the Four Elements Theory step by step: \textit{\textcolor{gray}{\#details about each step. }}The candidate charges  are as follows: \textit{\textcolor{red}{\#Candidate Charges }} &
  \multicolumn{2}{p{5cm}}{\raggedright The candidate charges and their corresponding four elements are as follows: \textit{\textcolor{red}{\#Four Elements of candidate charges}}. The four elements represent the four core factors of a charge. Compare the case facts to determine which charge's four elements they align with, thereby identifying the charge.} \\  \cline{2-6}

 &
  \multicolumn{5}{p{14cm}}{\raggedright Output format: \textit{\textcolor{orange}{\#Format}}. Note: Only output the charge, no additional information. \\ Case facts: \textit{\textcolor{blue}{\#Case Facts.}}} \\

\bottomrule
\end{tabularx}}
\caption{Prompts of different methods in Similar Charge Disambiguation. \# represents a format input.}
\label{tab:prompt of different method}
\end{table*}

\section{Baselines in Legal Case Retrieval}
\label{appendix:LCR baselines}

% \paragraph{Sparse Retrieval Methods: }

% \textbf{QL}\cite{zhai2008statistical} is a probabilistic retrieval model that ranks documents by the relevance likelihood to the query. \textbf{BM25}\cite{robertson2009probabilistic} is a probabilistic retrieval model that calculates the doc-query relevance using term frequency and document length.

% \paragraph{Dense Retrieval Methods: }

\textbf{BERT}\cite{devlin2018bert} is a language model widely used in retrieval tasks. In this paper, we chose BERT-base-Chinese\footnote{\url{https://huggingface.co/google-bert/bert-base-chinese}}. \textbf{Legal-BERT}\footnote{\url{https://github.com/thunlp/OpenCLaP}}\cite{chalkidis2020legal} is a variant of BERT that is specifically trained on legal corpora. 
\textbf{Lawformer}\cite{xiao2021lawformer}is a Chinese legal pre-trained model based on Longformer\cite{beltagy2020longformer}, which is able to process long texts in the legal domain. 
\textbf{ChatLaw-Text2Vec}\footnote{\url{https://modelscope.cn/models/fengshan/ChatLaw-Text2Vec}}\cite{cui2023chatlaw} is a Chinese legal LLM trained on 936,727 legal cases for similarity calculation of legal-related texts. 
\textbf{SAILER}\cite{li2023sailer} is a structure-aware legal case retrieval model utilizing the structural information in legal case documents. 
\textbf{GEAR}\cite{qin2024explicitly} is a generative retrieval framework that explicitly integrates judgment prediction with legal document retrieval in a sequence-to-sequence manner. Since the output of GEAR cannot directly evaluate NDCG, the official results under the same setting are directly referenced in this paper. \textit{LLM} and \textit{Expert} represent the results of retrieval using only the four elements.


\section{SCR results on the full LeCaRDv2 Dataset}

As presented in Table \ref{tab:full set SCR results}, we selected several representative methods based on sparse retrieval and dense retrieval for experiments on the full LeCaRDv2 dataset. All language models were not fine-tuned. The notes-guided FET method achieved the best performance among all language models, attaining top results in both R@500 and R@1000. The results indicate that the conclusions drawn from the full dataset are consistent with those from the subset, and the notes-guided method demonstrates strong performance.

\begin{table*}[]
\centering
\begin{tabular}{l|llll}
\hline
Model & \textbf{R@100} & \textbf{R@200} & \textbf{R@500} & \textbf{R@1000} \\ \hline
% BM25 & \textbf{0.6262} & \textbf{0.6629} & 0.6949 & 0.7207 \\
% QLD & 0.5984 & 0.6576 & 0.7065 & 0.7424 \\ \hline
BERT & 0.1116 & 0.1493 & 0.2174 & 0.2819 \\
Lawformer & 0.2432 & 0.304 & 0.4054 & 0.4833 \\
ChatLaw & 0.1045 & 0.1628 & 0.2791 & 0.3999 \\
SAILER & 0.2834 & 0.4033 & 0.6104 & 0.7568 \\ \hline
BGE & 0.4085 & 0.5246 & 0.6855 & 0.7912 \\
FET\textsubscript{LLM} & 0.4167 & 0.5388 & 0.7006 & 0.7925 \\
FET\textsubscript{Expert\_guided} & 0.4201 & 	0.5396 & \textbf{0.7010	} & \textbf{0.7927} \\ \hline
\end{tabular}
  \caption{SCR results on the full set of LeCaRDv2. Bold fonts indicate leading results in each setting. The expert-guided FET method achieved the best performance among all language models and attained the top results in both R@500 and R@1000. }
  \label{tab:full set SCR results}
\end{table*}

\section{A Case Study of LCR}
\label{appendix:lcr-case}
Table \ref{tab:LCR case} presents a case study on the Crime of Embezzlement. By comparing the four elements annotated by experts for the crime in JUREX-4E, the case-specific four elements generated directly by the LLM, and those generated by the LLM with expert four elements of charge as guidance, we can observe that:

1) Incorporating expert fine-grained annotations enables the model to better grasp the elements of a crime, thereby providing more precise element comparison. For example, LLMs can identify the ``integrity of official duties'', and the subjective aspect ``Intentional'' can be interpreted as ``having the purpose of illegally possessing public or private property'', highlighting the characteristics of ``official duties''. Capturing the core information of the case is crucial for matching cases with similar facts.

2) LLMs can conduct case-tailored specific analysis based on the constitutive elements of a crime. Blue parts show the LLMs can better analyze the defendant's workplace and the actions taken in the case, which reflects the significance of specific and accurate legal knowledge.

\begin{table*}[]
\renewcommand{\arraystretch}{1.5} % 调整行高
\begin{tabularx}{\linewidth}{l|X|X|X}
\hline
\textbf{Document} & \multicolumn{3}{p{13cm}}{\textit{[Head of document]}...In April 201X, Company A appointed B as the Sales Manager and Deputy Manager of the Catering Department, responsible for collecting outstanding debts. From December 201X to April 201Y, B embezzled X yuan from six units' payments for personal investment and consumption. In May 201Y, knowing that the company required him to reconcile accounts with the finance department, B fled after changing his phone number. It was found that after the case broke out, B's father C had returned the X yuan to Company A.} \\ \hline
\textbf{Charge} & \multicolumn{3}{p{13cm}}{Crime of Embezzlement} \\ \hline
 & \textbf{Experts Four-elements} & \textbf{LLM-generated} & \textbf{Experts-guided} \\ \hline
\textbf{Four Elements} &
  \begin{tabular}[t]{@{}p{4cm}@{}}
  \textbf{Object:} The ownership of property and \textcolor{red}{integrity of duties} in companies, enterprises, or other organizations. The objects include movable and immovable, tangible and intangible property of the perpetrator’s organization.\\
  \textbf{Objective Aspect:} Using positional convenience to illegally appropriate organizational property in a significant amount.\\
  \textbf{Subject:} \textcolor{red}{Special subject}, namely personnel of companies, enterprises, or other organizations.\\
  \textbf{Subjective Aspect:}Intentional, \textcolor{red}{with the purpose of illegally possessing property}.
  \end{tabular} &
  \begin{tabular}[t]{@{}p{4cm}@{}}
  \textbf{Object:} The ownership of property of companies, enterprises, or other organizations.\\
  \textbf{Objective Aspect:} Using the convenience of one’s position to illegally appropriate the property of the organization, in a significant amount.\\
  \textbf{Subject:} Personnel of companies, enterprises, or other organizations. The defendant is B.\\
  \textbf{Subjective Aspect:} Intentional.
  \end{tabular} &
  \begin{tabular}[t]{@{}p{4cm}@{}}
  \textbf{Object:} The ownership of property of companies, enterprises, or other organizations and \textcolor{red}{the integrity of official duties}. In this case, crime objetcs are the properties of the company, enterprise, or other organization where the perpetrator is employed.\\
  \textbf{Objective Aspect:} The act of using one's position for personal gain by illegally appropriating the property of the organization, where the amount involved is significant. \textcolor{blue}{Specifically, B used his position to embezzle X yuan from the payments of six units for personal investment and consumption.}\\
  \textbf{Subject:} \textcolor{red}{Special subject}, namely personnel of companies, enterprises, or other organizations. \textcolor{blue}{Defendant B is the Sales Manager and Deputy Manager of the Catering Department of the company.}\\
  \textbf{Subjective Aspect:} Intentional, \textcolor{red}{with the purpose of illegally possessing public or private property}.
  \end{tabular} \\ \hline
\end{tabularx}
  \caption{The results of the case four elements obtained through different methods in LCR. \textbf{Notes} refer to expert notes related to the charge retrieved during the search. \textbf{LLM-generated} and \textbf{Experts-guided} indicate whether using JUREX-4E's four-elements of the crime to guide LLM in generating the four elements. \textcolor{red}{Red} parts mean the knowledge from JUREX-4E, while \textcolor{blue}{blue} parts show the LLM’s internal knowledge. By incorporating JUREX-4E, the model better emphasizes conviction and sentencing related information and provides more detailed descriptions of critical case facts.}
  \label{tab:LCR case}
\end{table*}

\end{document}


% In this section, we compare the annotation quality of the four elements of criminal charges through downstream task metrics and manual evaluation from a legal perspective.

% \begin{table}[]
% \resizebox{\linewidth}{!}{
% \begin{tabular}{l|cc|cc|cc|ll}
% \hline
% Model & \multicolumn{2}{c|}{F\&E} & \multicolumn{2}{c|}{E\&MPF} & \multicolumn{2}{c|}{AP\&DD} & \multicolumn{2}{c}{Average} \\ \hline
%  & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\
% Four-Element_{Notes} & 95.80 & 96.51 & 91.18 & 91.22 & 90.59 & 90.71 & 92.52 & 92.81 \\
% Four-Element_{LLM} & 95.73 & 96.56 & 91.87 & 92.01 & 89.61 & 89.69 & 92.40 & 92.75 \\
% Four-Element_{Expert} & \textbf{96.06} & \textbf{96.69} & \textbf{92.57} & \textbf{93.05} & \textbf{90.53} & \textbf{90.62} & \textbf{93.05} & \textbf{93.45} \\ \hline
% \end{tabular}}
%   \caption{\label{tab:preliminary-res}
%    Results of charge disambiguation using a large language model with four-element knowledge obtained from experts, layperson notes, and LLMs.
%   }
% \end{table}



% \subsection{Legal Dimension Metrics}

% We developed multidimensional evaluation metrics designed by legal experts. In legal studies, the evaluation criteria for elements can generally be divided into two categories: practicability and logicality. Practicability focuses on the applicability of the elements in real-world legal practice, ensuring the elements are useful and easy to apply. Logicality refers to the internal consistency and logical flow of the elements, ensuring that they follow a clear and structured approach in line with legal reasoning. 

% Based on these principles, we evaluate the four elements produced by each approach using the following dimensions: \textbf{Precision, Completeness, Representativeness, and Standardization}: 

% \begin{itemize}
%   \item Precision: Whether the key components are accurately identified.
%   \item Completeness: Whether all necessary elements of the four-element theory are included.
%   \item Representativeness: Whether the annotations highlight the most important legal elements and case details.
%   \item Standardization: Whether the annotations are clear, consistent, and adhere to established norms for easy interpretation.
% \end{itemize}

% Each dimension was scored by three postgraduate students specializing in criminal law, based on the 1-5 scale criteria outlined in Table \ref{tab: four-dimension score details}. To reduce bias, the order of legal elements was randomized for the annotators.  

% The average scores are shown in Table \ref{tab:Legal Dimension Metric-res}. The human evaluation results reveal that while both notes-type annotations and LLM-generated annotations fall short of expert-level performance (as reflected in prior experiments on accuracy and F1 scores), the reasons for their shortcomings differ:

% - Due to the loss of detail, notes-type annotations score the lowest across all dimensions, with particularly low scores in Completeness and Standardization. But focusing on key legal terms allows them to capture the most critical aspects of the charge, maintaining relatively well performance in Representativeness.

% - In contrast, LLMs excel in Precision and Standardization due to their focus on the literal decomposition and restatement of legal provisions but fall short in fully explaining or analyzing the underlying legal concepts.

% To address these complementary strengths and weaknesses, we propose the Notes-guided Four Element approach, which combines the organizational efficiency of LLMs with the representational focus of notes-type knowledge. The results show this hybrid method significantly improves performance across all dimensions, matching expert-level scores in Precision and Standardization (4.58) while achieving near-expert performance in Completeness (4.39) and Representativeness (4.50).

% This suggests that integrating the practical, common-sense knowledge from human experts with the organizational power of LLMs enhances both the precision and consistency of the generated elements, offering a more effective and scalable framework for legal analysis.

% Building on this success, we designed the Notes\_guide\_LLM framework, which integrates notes-type knowledge with LLM capabilities to address legal tasks at two levels: Charge-level tasks introduce the four elements of charges, representing the integration of abstract legal knowledge from notes and LLMs, while Case-level tasks analyze the four elements within specific cases, demonstrating how LLMs apply notes-based legal knowledge in more concrete scenarios.