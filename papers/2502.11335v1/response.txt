\section{Related Work}
\label{sec:related}
In this section, we review previous studies for ranking in recommendation systems: 1) representation learning methods and 2) graph ranking methods.

\subsection{Representation Learning Methods for Recommendation}
\label{sec:related:rl}
% overview for subsection level (abstract level 1)
Representation learning extracts latent representation vectors (or embeddings) of users and items from user-item interactions, using these embeddings to yield scores for recommendations.
% overview for MF and GNN (abstract level 2)
Early research on single-behavior recommendation modeled user-item interactions based on a specific behavior, such as viewing or rating, using Matrix Factorization (MF) or Graph Neural Networks (GNNs) within the framework of collaborative filtering.
% detail of MF (detail level 1)
MF**He et al., "SpeedUp: Optimizing Matrix Factorization for Recommendation Systems"** decomposes a user-item interaction matrix into low-dimensional user and item representations, optimizing Bayesian Personalized Ranking (BPR)**Rendle et al., "BPR: Bayesian Personalized Ranking from Implicit Feedback"** loss to rank a userâ€™s consumed items higher than unconsumed ones.
% detail of GNN (detail level 1)
GNNs**Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks"** learn the embeddings from a bipartite graph of the user-item interactions through message-passing, which aggregates embeddings from neighboring nodes.
% detail of GNN (detail level 2)
For example, NGCF**Wang et al., "Neural Graph Collaborative Filtering"** employs GNNs to extract user and item embeddings, effectively modeling high-order connectivity through multiple rounds of message-passing. 
LightGCN**He et al., "LightGCN: Simplifying and Opening Up Graph Neural Networks for Recommendation"** simplifies the message-passing mechanism used in NGCF by removing several learning components (e.g., feature transformation and non-linear activation) to reduce overfitting and improve  efficiency.

% method for multi-behavior
However, relying solely on a single behavior is limited in effectively capturing user preferences, especially in multi-behavior recommendation, because such interaction data is insufficient, and it does not account for the distinct semantics associated with each behavior.
To fully leverage rich interactions from  multiple behaviors, recent studies**Feng et al., "Graph Neural Networks for Multi-Task Recommendation"** have extended GNN-based approaches to model multi-behavior interactions.
For example, MBGCN**Wang et al., "Multi-Behavior Graph Convolutional Networks"** constructs a unified graph from multi-behavior interactions, assigning a specific behavior type to each edge, and then uses heterogeneous GNNs to learn representations from the graph.
MB-HGCN**Feng et al., "Hierarchical Graph Learning for Multi-Task Recommendation"** hierarchically learns representations by using GNNs, starting from a unified graph and progressing to behavior-specific graphs.
MuLe**Li et al., "Multi-Grained Graph Learning for Recommendation"** proposes a multi-grained graph learning framework to capture diverse aspects between behaviors, exploiting graph attention to denoise uncertain auxiliary interactions.

Several researchers**Wang et al.** have exploited \textit{cascading pattern} as an inductive bias, a natural sequence of user behaviors (e.g., a user first views an item, adds it to a cart, and then makes a purchase), where the later a behavior occurs in the sequence, the greater its influence on the target behavior.
Specifically, MB-CGCN**Feng et al.** directly leverages the cascading pattern in its representation learning with GNNs, sequentially refining user and item embeddings by learning from each behavior graph in the order of the sequence.
PKEF**Wang et al.** enhances the cascading graph learning process by incorporating signals from each behavior graph.
HEC-GCN**Li et al.** learns the structure of behavior-specific hypergraphs of users or items, and performs a cascading learning on those hypergraphs at the coarse-grained level, combining this with the cascading learning of behavior-specific graphs at the fine-grained level.

% limitation (common)
However, the existing methods are likely to produce over-smoothed embeddings due to the assumptions in collaborative filtering and GNNs**Kipf and Welling** that encourage the embeddings of connected nodes to be similar.
Furthermore, they optimize a likelihood across all users, which causes a bias toward heavy users with a large number of interactions in their learning**Rendle et al.**.
Due to these issues, the representation learning methods produce embeddings with limited expressiveness, hindering their ability to precisely capture user preferences from multi-behavior interactions.

\subsection{Graph Ranking Methods for Recommendation}
\label{sec:related:graph}
% overview for subsection level
Graph ranking aims to directly produce ranking scores on items by analyzing the relationships between nodes (e.g., users and items) within a graph, and it has been widely used for recommending items to querying users on a graph of user-item interactions**Kunegis et al.**, as well as in other domains such as search**Rendle et al.**, social network analysis**Wang et al.**, and more.

% overview of paragraph level (ranking for general graphs)
Numerous ranking models**Feng et al.** for graphs have been proposed, each with its own assumptions about ranking scores, most of which focus on single-behavior interactions (e.g., clicks or ratings) for recommendation**Rendle et al.**.
%
%RWR
For example, random walk with restart (RWR)**Tong and Faloutsos**, a variant of PageRank**Brin and Page**, exploits a random surfer that stochastically either performs a random walk to neighboring nodes or restarts at a querying node, yielding personalized ranking scores, with more frequently visited nodes being ranked higher, across various types of graphs**Kunegis et al.**.
%HITS
Hyperlink-induced topic search (HITS)**Kleinberg** assumes that good hubs links to good authorities, and iteratively updates hub and authority scores for each node based on the graph structure.

% overview of paragraph level (ranking for bipartite graphs)
Several methods have been developed specifically for bipartite graphs (e.g., user-item graphs).
%CoHITS
For example, Deng et al.**Deng et al.** developed Co-HITS, a generalized method combining HITS and RWR for bipartite graphs, controlled by flexible personalized parameters, which results in scores suited for the input graph.
%BiRank
BiRank**Fang et al.** follows smoothness convention**Kunegis et al.** on bipartite graphs, i.e., a node on one side should be ranked higher if it is linked to higher-ranked nodes on the other side, while incorporating prior information (e.g., consumed items) and diminishing the influence of high-degree nodes during the ranking process.

% limitations of graph ranking
However, the previous methods have dealt with ranking on graphs of single-behavior interactions, and relying on a single behavior (e.g., clicks) struggles to capture the genuine intent of users regarding the target behavior (e.g., purchases).
Someone might naively apply those methods to a unified graph of all behaviors, but this approach loses the semantics of each behavior and overlooks important patterns, such as cascading sequences, resulting in limited ranking scores.

% special mention on BPMR (similar, but outside to this category)
While it is not a graph ranking method, Li et al.**Li et al.** have recently proposed BPMR, a pattern-mining approach that enumerates diverse patterns of behavioral paths and estimates the probabilities of items being purchased in a Bayesian manner.
Although BPMR outperforms GNN-based representation learning methods, its performance is limited because it considers only a few steps of paths, ignoring the overall graph structure, and requires heavy computational costs for enumeration.