\section{Experiments}
\label{sec:experiments}
In this section, we conducted experiments to address the following questions:

\begin{enumerate}[leftmargin=9mm,noitemsep]
    \item[Q1.] {
        \textbf{Recommendation performance. } 
        How effective is the personalized ranking provided by \method for multi-behavior recommendation compared to its competitors?
    }
    \item[Q2.] {
        \textbf{Ablation study.} How does each module of \method affect its recommendation performance?
    }
    \item[Q3.] {
        \textbf{Effect of hyperparameters.} 
        How do the hyperparameters of \method, such as smoothness, query fitting, and cascading alignment, influence its performance?
    }
    \item[Q4.] {
        \textbf{Convergence.} 
        Does the iterative algorithm of \method converge as the number of iterations increases?
    }
    \item[Q5.] {
        \textbf{Computational Efficiency} Does the \method yield linear computational complexity w.r.t. the number of total interactions?
    }
\end{enumerate}

\def\arraystretch{1.1} 
\setlength{\tabcolsep}{12pt}
\begin{table}[t]
\small
\caption{
    Data statistics of multi-behavior interactions.
}
\label{tab:data}
\centering
\begin{tabular}{crrrrrr}
    \hline
    \toprule
    \textbf{Dataset} & Users & Items & Views& 
    \makecell[r]{Collects$^{*}$\\or Shares$^{\dagger}$}
     & \makecell[r]{Carts$^{*}$\\or Likes$^{\dagger}$} & \makecell[r]{Buys$^{*}$\\or Follows$^{\dagger}$}\ \\
    \midrule
    \texttt{Taobao}$^{*}$ & 15,449 & 11,953 & 873,954 & - & 195,476  & 92,180 \\
    % \texttt{Beibei}$^{*}$ & 21,716 & 7,977 & 2,412,586   & - & 642,622 & 282,860 \\
    \texttt{Tenrec}$^{\dagger}$ & 27,948 & 15,440 & 1,489,997  & 13,947  & 1,914  & 1,307  \\
    \texttt{Tmall}$^{*}$ & 41,738 & 11,953 & 1,813,498  & 221,514 & 1,996  & 255,586 \\
    %\texttt{Jdata}$^{*}$ & 93,334 & 24,624 & 1,681,430   & 45,613 & 49,891 & 321,883 \\
    \bottomrule
    \hline
\end{tabular}
\begin{tablenotes}[flushleft]\footnotesize
{\item[] $\dagger$: Note that \tenrec includes shares, likes, and follows as corresponding behaviors.}
\end{tablenotes}
\end{table}

\subsection{Experimental Setting}
\label{sec:exp:setting}
We describe the setup for our experiments on multi-behavior recommendation.

\smallsection{Datasets}
We conducted experiments on three real-world multi-behavior datasets:  
\taobao~\cite{MengZGGLZZTLZ23, LeeLKSJ24xxaw}, \tenrec~\cite{abs-2210-10629, ZhangBCSYGWHH24} and \tmall~\cite{YanCGSLSL24, MengZGGLZZTLZ23} which are publicly available and have been used as standard benchmarks in previous studies~\cite{LeeLKSJ24xxaw, MengZGGLZZTLZ23, YanCGSLSL24, ZhangBCSYGWHH24, YanYCSSP23}.
The detailed statistics of the datasets are summarized in Table~\ref{tab:data}.
%
The \texttt{Tmall} dataset has four behavior types: \textit{view}, \textit{add-to-collect}, \textit{add-to-cart}, and \textit{buy}, while \texttt{Taobao}, apart from add-to-collect, consists of three types.
The behavior types of \tenrec are \textit{view}, \textit{share}, \textit{like} and \textit{follow}.
Following the previous studies~\cite{LeeLKSJ24xxaw, MengZGGLZZTLZ23, ZhangBCSYGWHH24}, we set the target behavior as \textit{buy} for $\{\taobao, \tmall\}$ and \textit{follow} for $\{\tenrec\}$, and preprocessed duplicate interactions by retaining only the earliest occurrence for each behavior.

\smallsection{Competitors}
We compared \method with traditional graph ranking methods such as \textbf{RWR}~\cite{TianJ13}, \textbf{CoHITS}~\cite{DengDLK09kqpg}, \textbf{BiRank}~\cite{HeHGKW17}, as well as single-behavior representation learning methods such as \textbf{LightGCN}~\cite{HeHDWLZW20} and \textbf{MF-BPR}~\cite{RendleRFGS12}.
Note that these methods were designed to operate on a single behavior graph. 
For these, we combined all behavior graphs into a unified graph\footnote{We performed an element-wise union operation across the adjacency matrices of all behaviors to construct the adjacency matrix of the unified graph.}, on which these methods were applied. 
Results based solely on the target behavior interactions are provided in~\ref{sec:appendix:single}.
We further compare \method with \textbf{NRank}, an extended version of \textbf{BiRank}~\cite{HeHGKW17} for multi-partite graphs, where the interactions of each behavior form a subgraph between users and items. In this graph, the set of users is shared across behaviors, while the set of items is distinct for each behavior.
We also compared our method with state-of-the-art representation learning methods for multi-behavior recommendation, including: 1) non-cascading approaches such as \textbf{MB-HGCN}~\cite{YanYCSSP23} and \textbf{MuLe}~\cite{LeeLKSJ24xxaw}, and 2) cascading approaches such as \textbf{PKEF}~\cite{MengMZYZL23} and \textbf{HEC-GCN}~\cite{yin2024hecgcn}.
Finally, we evaluated our method against \textbf{BPMR}~\cite{LiLCYLLD24}, a pattern-mining-based method that achieves state-of-the-art accuracy in the recommendation task.


\smallsection{Training and evaluation protocol}
We follow the leave-one-out setting, which has been broadly used in previous studies~\cite{MengZGGLZZTLZ23,LeeLKSJ24xxaw,ChengCHLZGP23fqvn}, where the test set consists of the last interacted item for each users. 
The second most recently interacted item for each user forms the validation set for tuning hyperparameters, while the remaining interactions are used for training.
%
In the evaluation phase, items within the test set for each user are ranked based on ranking or predicted scores by models. where its top-$k$ ranking quality is measured by HR@$k$ and NDCG@$k$~\cite{MengZGGLZZTLZ23,LeeLKSJ24xxaw,ChengCHLZGP23fqvn,ZhangBCSYGWHH24}. Note that target behavior items(i.e., buy) in the training interaction are excluded during testing.
HR@$k$ measures how often relevant items, on average, appear in the recommendation for each user.
NDCG@$k$ considers both relevance and order of relevant items in a ranking, averaged across all users.

\smallsection{Hyperparameter tuning}
For each dataset, we conducted a grid search to tune hyperparameters on the validation set and reported the test performance with the validated hyperparameters. 
The hyperparameters $\alpha$ and $\beta$ of \method are tuned in $[0,1]$ such that $0 \leq \alpha + \beta \leq 1$.
The validated values of $\alpha$ and $\beta$ for each dataset are provided in Appendix~\ref{sec:appendix:Effect of Hyperparameters_detail}.
For the cascading sequence $\mathcal{C}$, we follow a natural sequence of user behaviors based on their semantics, as used in previous studies~\cite{yin2024hecgcn, MengMZYZL23, LiuXWY00024, YanCGSLSL24, ChengCHLZGP23fqvn}.
We set $\mathcal{C}$ to $(\texttt{view}\rightarrow\texttt{collect}\rightarrow\texttt{cart}\rightarrow\texttt{buy})$ for \tmall, $(\texttt{view}\rightarrow\texttt{cart}\rightarrow\texttt{buy})$ for \taobao, and $(\texttt{view}\rightarrow\texttt{share}\rightarrow\texttt{like}\rightarrow\texttt{follow})$ for \tenrec (see Appendix~\ref{sec:appendix:Performance_for_Different_Cascading_Sequences} for results with different cascading sequences).
For each competitor, we followed the range of its hyperparameters, as reported in the corresponding paper.

\smallsection{Machine and implementation}
We used a workstation with AMD 5955WX and RTX 4090 (24GB VRAM).
Our method \method was implemented using Pytorch 2.0 in Python 3.9. 
Note that the BMPR's algorithm~\cite{LiLCYLLD24} is designed to run sequentially on a CPU and it is hard to parallelize, while other methods are easily parallelizable due to matrix operations and execute on a GPU.
For the other methods, we used their open-source implementations, with detailed information provided in Appendix~\ref{sec:appendix:competitors:information}.


\def\arraystretch{1.1} 
\setlength{\tabcolsep}{6pt}
\begin{table}[t]
\caption{
Multi-behavior recommendation performance in terms of HR@10 and NDCG@10, with the best results in bold and the second-best results underlined.
`\% impv.’ indicates the percentage improvement of the best model over the second-best model.
RL stands for representation learning, GR for graph ranking, and PM for pattern-mining methods.
\textbf{Our \method shows the best recommendation performance among all the tested methods across the datasets.}
}
\small
\label{tab:performance}
\centering
\begin{tabular}{cc|ccc|ccc}
\hline
\toprule
\multirow{2}{*}{\bf Methods} & \multirow{2}{*}{\bf Type} & \multicolumn{3}{c|}{\bf HR@10}       & \multicolumn{3}{c}{\bf NDCG@10}      \\
             & & \taobao  & \tenrec  & \tmall & \taobao  & \tenrec   & \tmall  \\ \midrule
MF-BPR  & RL & 0.0758  & 0.1244 & 0.0855 & 0.0387  & 0.0575  & 0.0423  \\
LightGCN & RL & 0.1025  & 0.1069 & 0.1162 & 0.0566  & 0.0526  & 0.0625  \\ \midrule
MB-HGCN & RL       & 0.1261  & 0.1133 & 0.1413 & 0.0666  & 0.0618  & 0.0753  \\ 
MuLe   & RL        & 0.1949  & 0.1920  & 0.2097 & 0.1128  & 0.1100  & 0.1175  \\ \midrule
PKEF   & RL        & 0.1349  & 0.0968 & 0.1222 & 0.0763  & 0.0530  & 0.0696  \\
HEC-GCN & RL       & 0.1905  & 0.2673 & 0.1806 & 0.1038  & 0.1565  & 0.1000  \\ \midrule
RWR & GR     & 0.2130  & 0.2074 & 0.2712 & 0.0988  & 0.0962  & 0.1284  \\
CoHITS & GR  & 0.2128  & 0.2074 & 0.2713 & 0.0988  & 0.0957  & 0.1284  \\
BiRank & GR   & \underline{0.3034}  & 0.2949 & \underline{0.3550} & \underline{0.1517}  & 0.1257  & \underline{0.1819}  \\ 
NRank & GR         & 0.2989  & \underline{0.4562} & 0.3477 & 0.1419  & 0.2508  & 0.1726  \\
\midrule
BPMR  & PM        & 0.2846  & 0.4286 & 0.3289 & 0.1429  & \underline{0.2579}  & 0.1598  \\ \midrule
\textbf{\method} & GR  & \textbf{0.3324}  & \textbf{0.4747} & \textbf{0.3751} & \textbf{0.1626}  & \textbf{0.2723}  & \textbf{0.1871}  \\ \midrule
\% impv.   & -     & 9.56\%  & 5.67\% & 5.67\% & 7.16\%  & 5.57\%  & 2.85\%  \\
\bottomrule
\hline
\end{tabular} 
\end{table}

\subsection{Recommendation Performance (Q1)} 
We evaluate the effectiveness of \method in multi-behavior recommendation by comparing it to its competitors.

\smallsection{Top-10 performance}
We first examine the recommendation quality of the top 10 highest-ranked items by each method, measured in terms of HR@10 and NDCG@10.
From Table~\ref{tab:performance}, we observe the following:
\begin{itemize}[leftmargin=9mm,noitemsep]
    \item{
        % overall comparison
        Our proposed \method achieves superior performance compared to its competitors across all datasets, providing \textbf{up to 9.56\% improvement in HR@10 and up to 7.16\% in NDCG@10 over the second-best model}, with the most notable gains observed on the \taobao dataset.
    }
    \item{
        The representation learning (RL) methods generally underperform compared to the graph ranking (GR) or the pattern-mining (PM) methods.
        Specifically, naive GR models on a unified graph such as BiRank surpass state-of-the-art RL models exploiting multi-behavior interactions such as MuLe and HEC-GCN on \taobao and \tmall, highlighting the RL approach’s limitations in providing accurate recommendations due to their limited expressiveness, mainly caused by over-smoothing and bias issues.
    }
    \item{
        % our apporach is better than simple graph ranking
        For the GR methods, exploiting a cascading sequence in measuring ranking scores, as in \method, is beneficial because it outperforms other GR methods, such as BiRank and NRank, which use all interactions but do not fully exploit the semantics of behaviors.
    }
    \item{
        % mention on cascading approach on RL
        It matters more to precisely encode embeddings than to use additional information, such as the cascading pattern, in the RL methods. 
        As evidence of this, HEC-GCN, a cascading method, performs better than MuLe, a non-cascading method, on \tenrec, while the opposite is observed on the other datasets.
    }
    \item {
        % mention on BPMR
        Our \method outperforms BPMR, which is recognized for achieving state-of-the-art accuracy and surpassing the RL methods. 
        The main difference is that BPMR considers only a few steps of paths, while our method accounts for the global structure of each behavior graph and the cascading effect, enabling it to generate higher-quality scores.      Furthermore, BPMR is significantly slower than \method, as discussed in Section~\ref{sec:exp:efficiency}.
    }
\end{itemize}

\begin{figure}[t]
    \centering
    %\hspace{-6mm}
    \includegraphics[width=0.95\linewidth]{All_k_score_legend.pdf}\\
    \subfigure[\taobao]{
        \hspace{-7mm}
        \includegraphics[width=0.318\linewidth]{All_k_score_taobao.pdf}
        %\hspace{-3.5mm}
        \label{fig:appendix:All_k_score:Taobao}
    }
    \subfigure[\tenrec]{
        \hspace{-3mm}
        \includegraphics[width=0.300\linewidth]{All_k_score_tenrec.pdf}
        %\hspace{-1mm}
        \label{fig:experiments:All_k_score:Tenrec}
    }
    \subfigure[\tmall]{
        \hspace{-4.5mm}
        \includegraphics[width=0.30\linewidth]{All_k_score_tmall.pdf}
        %\hspace{-3mm}
        \label{fig:appendix:All_k_score:Tmall}
    }
    
    \caption{
        \label{fig:experiments:varik}
        Recommendation performance in HR@$k$ and NDCG@$k$, where $k$ varies in \{10, 30, 50, 100, 200\}.
        \textbf{Our \method provides better ranking scores than its competitors in both metrics.}
    }
\end{figure}

\smallsection{Top-$k$ performance}
We further evaluate the ranking quality of \method by comparing it with its competitors in terms of HR@$k$ and NDCG@$k$ for various values of $k$ ranging from 10 to 200.
As shown in Figure~\ref{fig:experiments:varik}, \textbf{our \method achieves the highest ranking quality for items that test users are likely to purchase}, outperforming its competitors across all datasets.
Specifically, NDCG@$k$ of \method is the highest across all $k$, significantly outperforming the RL methods, indicating that the target items are more likely to be ranked higher in our results.
It is worth noting that HR@$k$ of the GR methods increases significantly as $k$ grows, compared to that of the RL methods, indicating that the rankings produced by the GR methods are more likely to contain the target items.
This implies that the GR methods are more suitable for generating candidate items, which can then be refined through re-ranking process~\cite{ValizadeganJZM09}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{ablation_study_Q2_legend.pdf}
    
    \subfigure[\taobao]{
        \hspace{-3mm}
        \includegraphics[width=0.320\linewidth]{ablation_study_Q2_taobao.pdf}
        % \hspace{-3mm}
        \label{fig:experiments:ablation:Taobao}
    }
    \subfigure[\tenrec]{
        \hspace{-3mm}
        \includegraphics[width=0.312\linewidth]{ablation_study_Q2_tenrec.pdf}
        % \hspace{-3mm}
        \label{fig:experiments:ablation:Tenrec}
    }
    \subfigure[\tmall]{
        \hspace{-3mm}
        \includegraphics[width=0.320\linewidth]{ablation_study_Q2_tmall.pdf}
        \label{fig:experiments:ablation:Tmall}
        \hspace{-3mm}
    }
    \caption{
        \label{fig:experiments:ablationstudy}
        Effect of behaviors in the cascading sequence, where \texttt{B4} is used for \method, \texttt{B7} for \tenrec, and \texttt{B8} for \tmall to produce the final ranking scores of \method.
        \textbf{Note that utilizing all behaviors in the sequence is beneficial for recommendation}, as the performance degrades when earlier behaviors are excluded from the sequence.
    }
\end{figure}


\subsection{Ablation Study (Q2)}
We investigate the effectiveness of our design choices in \method through ablation studies.

\smallsection{Effect of auxiliary behaviors in the cascading sequence}
We conducted an ablation study to verify the impact of auxiliary behaviors in the cascading sequences used in our method for each dataset.
For this experiment, we sequentially excluded each auxiliary behavior from the cascading sequence $\mathcal{C}$, where it was initially set to $\texttt{B8}: (\texttt{view}\rightarrow\texttt{collect}\rightarrow\texttt{cart}\rightarrow\texttt{buy})$ for \tmall, $\texttt{B4}:(\texttt{view}\rightarrow\texttt{cart}\rightarrow\texttt{buy})$ for \taobao, and $\texttt{B7}:(\texttt{view}\rightarrow\texttt{share}\rightarrow\texttt{like}\rightarrow\texttt{follow})$ for \tenrec.
As shown in Figure~\ref{fig:experiments:ablationstudy}, using all behaviors in the cascading sequence leads to better recommendations than the variants that exclude auxiliary behaviors.
Specifically, using only the graph of interactions of the target behavior shows the worst performance across all datasets, and the performance improves as auxiliary behaviors are added in the order of the cascading sequence.
This indicates that incorporating all auxiliary behaviors in the order of the cascading sequence is essential for achieving optimal performance.

% 2. CascadingCoHITS v.s. CascadingRank: normalization - column-norm / symmetric -> birank 논문
\def\arraystretch{1.1} 
\setlength{\tabcolsep}{6.7pt}
\begin{table}[t]
\caption{
Effect of normalization on measuring the ranking scores of \method.
\textbf{Ranking scores with symmetric normalization provides more accurate recommendation than those with column normalization.}
}
\label{tab:ablation:normalization}
\centering
\small
\begin{tabular}{c|ccc|ccc}
\hline
\toprule
\multirow{2}{*}{\bf Variants} & \multicolumn{3}{c|}{\bf HR@10}       & \multicolumn{3}{c}{\bf NDCG@10}      \\
             & \taobao  & \tenrec  & \tmall & \taobao  & \tenrec   & \tmall  \\ 
\midrule
\methodcol       & 0.2919  & 0.4439 & 0.3270  & 0.1465  & 0.2559  & 0.1605  \\
\methodsym       & \bf 0.3324  & \bf 0.4747 & \bf 0.3751  & \bf 0.1626  & \bf 0.2723  & \bf 0.1871  \\
\midrule
\% impv.                 & 13.86\% & 6.93\% & 14.74\% & 10.97\%  & 6.39\%  & 16.60\% \\ 
\bottomrule
\hline
\end{tabular} 
\end{table}

\smallsection{Effect of normalization}
We check the effect of normalization for estimating ranking scores in Equation~\eqref{eq:cascrank:vect}. 
For this experiment, we compared the following:
\begin{itemize}[leftmargin=9mm,noitemsep]
    \item {
        \methodcol: it uses column normalization on each adjacency matrix, i.e., $\Abnorm = \Ab \Dib^{-1}$ and $\Abnorm^{\top} = \Ab^{\top} \Dub^{-1}$.
    }
    \item {
        \methodsym: it uses symmetric normalization on each adjacency matrix, i.e., $\Abnorm = \Dub^{-1/2} \Ab \Dib^{-1/2}$ and $\Abnorm^{\top} =  \Dib^{-1/2} \Ab^{\top} \Dub^{-1/2}$.
    }
\end{itemize}
%
As shown in Table~\ref{tab:ablation:normalization}, the symmetric normalization achieves better recommendation performance than the column normalization, with improvements of up to 14.74\% in HR@$10$ and 16.60\% in NDCG@$10$ on the \tmall dataset.
This indicates that reducing the impact of both users and items in terms of size is more beneficial than reducing that of either one alone for scoring, especially when recommending items in long-tail distributions.

% \smallsection{Effect of query setting}
\begin{figure}[t]
    \centering
    \subfigure[$\alpha$: query fitting]{
        %\hspace{-7mm}
        \includegraphics[width=0.3145\linewidth]{hyperparam_sens_alpha_HR.png}
        \label{fig:experiments:hyper_sens_split_view:alpha}
        \hspace{-4mm}
    }
    \subfigure[$\beta$: cascading alignment]{
        %\hspace{-7mm}
        \includegraphics[width=0.3\linewidth]{hyperparam_sens_beta_HR.png}
        \label{fig:experiments:hyper_sens_split_view:beta}
        \hspace{-4mm}
        
    }
    \subfigure[$\gamma$: smoothing]{
        %\hspace{-7mm}
        \includegraphics[width=0.3\linewidth]{hyperparam_sens_gamma_HR.png}
        \label{fig:experiments:hyper_sens_split_view:gamma}
    }
    \caption{
    \label{fig:experiments:hyper_sens_split_view}
    Effect of the hyperparameters $\alpha$, $\beta$, and $\gamma$ of \method on the recommendation performance in HR@10, where $\gamma = 1 - \alpha - \beta$ is the strength of smoothing, and $\alpha$ and $\beta$ are the strengths of query fitting and cascading alignment, respectively.
    %$0 \leq \alpha + \beta \leq 1 $ and $\gamma = 1 - (\alpha  + \beta )$. Note that, All hyperparameters in our method have an impact on accuracy.
    }
\end{figure}

\subsection{Effect of Hyperparameters (Q3)}
\label{sec:experiments:hyper_sens_split_view}
We investigate the impact of the hyperparameters $\alpha$, $\beta$, and $\gamma$ in \method on the performance of multi-behavior recommendation.
For this experiment, we varied $\alpha$ and $\beta$ from 0 to 1 with a step size of 0.1, and measured HR@10 of \method with them such that $\alpha + \beta$ is between 0 and 1, where the results of all possible combinations are provided in Appendix~\ref{sec:appendix:details:hyperparams}.
For each value of a hyperparameter\footnote{For better visualization, we excluded the results when the value of each hyperparameter is $1$, as the accuracies were significantly low in all cases.}, varied in increments of 0.1, we reported the maximum accuracy it achieved along with the possible values of the others, to analyze its effect.

Figure~\ref{fig:experiments:hyper_sens_split_view} shows the effects of $\alpha$, $\beta$, and $\gamma$ on the recommendation performance.
As shown in Figure~\ref{fig:experiments:hyper_sens_split_view:beta}, the accuracy improves with an increase in the strength $\beta$ of cascading alignment across all tested datasets, highlighting the importance of leveraging cascading information\footnote{However, relying solely on the cascading information, such as setting $\beta = 1$, results in poor performance, as shown in Figure~\ref{fig:appendix:Hyper-sens-detail}.}.
The effects of $\alpha$ and $\gamma$ depend on the datasets. 
For the strength $\alpha$ of query fitting, a smaller value works better on \taobao and \tenrec, while a moderately large value performs better on \tmall.
For the strength $\gamma$ of smoothing, the accuracy remains relatively high for values between 0.1 and 0.7 on \taobao and \tenrec, whereas the accuracy significantly drops after 0.1. 
This indicates that query information is far more crucial on \tmall compared to \taobao and \tenrec, whereas the smoothing plays a more significant role on the latter datasets.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{Reg-legend.pdf}\\
    \subfigure[\taobao]{
        %\hspace{-2mm}
        \includegraphics[width=0.32\linewidth]{Reg-taobao.pdf}
        \hspace{-1mm}
    }
    \subfigure[\tenrec]{
        \hspace{-3mm}
        \includegraphics[width=0.320\linewidth]{Reg-tenrec.pdf}
        \hspace{-1mm}
    }
    \subfigure[\tmall]{
        \hspace{-3mm}
        \includegraphics[width=0.32\linewidth]{Reg-tmall.pdf}
        \hspace{-1mm}
    }
    
    \caption{
        \label{fig:experiments:reg_convergence}
        Convergence analysis on the regularization values of Equation~\eqref{eq:reg:obj} and the residuals in Algorithm~\ref{alg:method}.
        \textbf{As the number of iterations increases, the values of all regularization terms and residuals decrease and eventually converge.}
    }
\end{figure}

\begin{figure}[t]
    \centering
    \subfigure[\taobao]{
        \hspace{-5mm}
        \includegraphics[width=0.302\linewidth]{Convergence-iteration-taobao.pdf}
        \hspace{-1mm}
        \label{fig:experiments:iteration:Taobao}
    }
    \subfigure[\tenrec]{
        \hspace{-3mm}
        \includegraphics[width=0.300\linewidth]{Convergence-iteration-tenrec.pdf}
        \hspace{-1mm}
        \label{fig:experiments:iteration:Tenrec}
    }
    \subfigure[\tmall]{
        \hspace{-3mm}
        \includegraphics[width=0.30\linewidth]{Convergence-iteration-tmall.pdf}
        \hspace{-1mm}
        \label{fig:experiments:iteration:Tmall}
    }
    
    \caption{
        \label{fig:experiments:iteration}
        Effect of the hyperparameters $\alpha$ and $\beta$ of \method on the number of iterations to converge, where $\epsilon$ is set to $10^{-5}$.
        \textbf{Our algorithm for \method converges for all combinations of $\alpha$ and $\beta$ such that $\alpha + \beta \in (0, 1)$, with faster convergence for larger values of $\alpha + \beta$.}
        %All combinations of $\alpha$
    }
\end{figure}

\subsection{Convergence Analysis (Q4)}
\label{sec:experiments:convergence}
In this section, we analyze the convergence of the iterative algorithm for \method.

\smallsection{Analysis on regularization and residual}
We measured the average residuals of Algorithm~\ref{alg:method} and regularization values of Equation~\eqref{eq:reg:obj} for smoothing, query, and cascading across all querying users as the number of iterations increased.
To broadly observe the convergence of these terms, we set $\alpha = 0.3$ and $\beta = 0.4$ and randomly initialized the ranking vectors in Algorithm~\ref{alg:method}.
Figure~\ref{fig:experiments:reg_convergence} shows the results of this analysis, with the left $y$-axis representing the log values of regularizations and the right $y$-axis representing the log values of residuals.
The values of all regularization terms and residuals decrease and converge as the number of iterations increases sufficiently.
This indicates that Algorithm~\ref{alg:method} ensures convergence of the residuals, and the resulting scores minimize the objective function of Equation~\eqref{eq:reg:obj}, i.e., they adhere to ranking smoothness while aligning with the querying and cascading vectors. 
Note that convergence is guaranteed for any valid value of $\alpha$ and $\beta$, as discussed in Section~\ref{sec:proposed:analysis}.

\smallsection{Analysis on the number of iterations}
We further analyzed the number of iterations to convergence for various values of $\alpha$ and $\beta$, where the threshold $\epsilon$ for convergence is set to $10^{-5}$.
As shown in Figure~\ref{fig:experiments:iteration}, all valid combinations of $\alpha$ and $\beta$ where $\alpha + \beta \in (0, 1]$ result in convergence. 
Note that larger values of $\alpha + \beta$ lead to faster convergence because $\gamma = 1 - \alpha - \beta$ becomes smaller, which shrinks the range of the eigenvalues of $\mat{S}$.

\subsection{Computational Efficiency (Q5)}
\label{sec:exp:efficiency}
%overview
We evaluated the efficiency of our proposed \method in terms of scalability and the trade-off between  accuracy and running time. 

\begin{figure}[t]
    \centering  
    \vspace{3mm}
    \subfigure[\taobao]{
        \includegraphics[width=0.45\linewidth]{SCALABILITY_Taobao.pdf}
        %\hspace{-3.5mm}
    }
    \subfigure[\tenrec]{
        %\hspace{-3mm}
        \includegraphics[width=0.4473447598\linewidth]{SCALABILITY_Tenrec.pdf}
        %\hspace{-1mm}
    }
    \caption{
        \label{fig:scalability}
        Scalability of \method.
        \textbf{Our iterative algorithm for \method scales linearly with respect to the number of edges (or interactions).}
    }
\end{figure}

\begin{figure}[t!]
    \centering
    \hspace{5mm}\includegraphics[width=0.7\linewidth]{Trade-off-legend.pdf}\vspace{-2mm}\\
    \subfigure[\taobao]{
        \hspace{-7mm}
        \includegraphics[width=0.3238536585\linewidth]{Trade-off-Taobao.pdf}
        \hspace{-1mm}
    }
    \subfigure[\tenrec]{
        \hspace{-3mm}
        \includegraphics[width=0.300\linewidth]{Trade-off-Tenrec.pdf}
        \hspace{-1mm}
        \label{fig:experiments:tradeoff:Tenrec}
    }
    \subfigure[\tmall]{
        \hspace{-3mm}
        \includegraphics[width=0.30\linewidth]{Trade-off-Tmall.pdf}
        \hspace{-3mm}
        \label{fig:experiments:tradeoff:Tmall}
    }
    
    \caption{
        \label{fig:experiments:trade_off}
        Trade-off between accuracy and running time for the graph ranking methods.
        \textbf{Note that our \method achieves the highest accuracy while maintaining competitive runtime performance compared to other methods.}
    }
\end{figure}

\smallsection{Scalability}
To assess scalability, we measured the running time of \method by varying the number of interactions, using the \taobao and \tenrec datasets, which contain a large number of interactions. 
For each dataset, we first apply the same random permutation to all adjacency matrices, and then extract principal submatrices from each by slicing the upper-left part, ensuring that the number of interactions ranges from about $10^{4.5}$ to the original number of interactions.
Since bipartite graphs usually have different row and column sizes, we applied the same ratio (i.e., from 0 to 1) to both dimensions.
Figure \ref{fig:scalability} demonstrates that the running time of \method increases linearly with the number of interactions on both datasets, consistent with the theoretical analysis in Theorem~\ref{sec:proposed:analysis_complexity}.

\smallsection{Trade-off between accuracy and running time}
%
We also analyzed the trade-off between accuracy and running time. 
Note that \method is categorized as a graph ranking method. 
Therefore, we compared it with other graph ranking methods in this experiment (refer to Appendix~\ref{sec:appendix:EfficiencyComparisonwithRepresentationLearningMethods} for a comparison with representation learning methods).
%
Figure~\ref{fig:experiments:trade_off} demonstrates that our \method achieves the highest accuracy while maintaining competitive runtime performance compared to other graph ranking methods.
The traditional graph ranking methods, such as BiRank and NRank, demonstrate either competitive speed or accuracy compared to \method, but they fail to achieve both simultaneously.
Note that BPMR, a state-of-the-art method, performs worse than the graph ranking methods including \method in running time due to its sequential CPU-based algorithmic design (i.e., it is hard to parallelize), while others leverage parallelizable matrix operations on a GPU.
