\section{Proposed Method}
\label{sec:proposed}
In this section, we propose \method, a novel graph ranking method for effective multi-behavior recommendation.
%The overall architecture of \method is described in Figure~\ref{}.

\subsection{Overview}
\label{sec:proposed:overview}
We summarize the technical challenges addressed in this work for accurate multi-behavior recommendation as follows:

\begin{enumerate}[leftmargin=8mm,noitemsep]
    \item[C1.]{
        \textbf{Achieving accurate recommendations.}
        The representation learning methods for multi-behavior recommendation have limited expressive power, leading to suboptimal accuracy. 
        How can we enhance recommendation accuracy in multi-behavior settings, especially compared to these representation learning methods?
    }
    \item[C2.]{
        \textbf{Leveraging multi-behavior characteristics.}
        Graph ranking can be a promising alternative to representation learning, but the traditional ranking methods are limited in multi-behavior settings because they are designed for single-behavior graphs.
        Which aspects of multi-behavior interactions can be leveraged for effective graph ranking?
    }
    \item[C3.]{
        \textbf{Ensuring stability and scalability.}
        Ensuring stable ranking results and scalable computation is crucial when designing a personalized ranking method on graphs.
        How can we develop a graph ranking algorithm tailored for multi-behavior recommendation that guarantees both stability and scalability?
    }
\end{enumerate}

We propose the following ideas that addresses the aforementioned technical challenges:

\begin{enumerate}[leftmargin=8mm,noitemsep]
    \item[A1.]{
        \textbf{Personalized graph ranking.}
        We adopt a graph ranking approach for accurate multi-behavior recommendation. 
        Our ranking design adheres to collaborative filtering by smoothing ranking scores and fitting queries, while mitigating the impact of high-degree nodes on our ranking scores.
    }
    \item[A2.]{
        \textbf{Cascading behavior graph and cascading alignment.}
        We leverage a cascading sequence of user behaviors, constructing a cascading behavior graph that sequentially connects behavior graphs, and measure rankings on it. 
        This smoothly aligns the ranking scores of the current behavior with those of the previous one, capturing the cascading effect.
    }
    \item[A3.]{
        \textbf{Iterative computation.}
        We iteratively refine the ranking scores of \method for a querying user, guiding them to converge (i.e., reach a stable state) while ensuring scalable computation w.r.t. the number of interactions.
    }
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{OVERVIEW.pdf}
    \caption{
        \label{fig:overview}
        Example of a cascading behavior graph $\GC=(\mathcal{G}_{b_{\texttt{view}}} \rightarrow \mathcal{G}_{b_{\texttt{cart}}} \rightarrow \mathcal{G}_{b_{\texttt{buy}}})$, given a cascading sequence $\C = (b_{\texttt{view}} \rightarrow b_{\texttt{cart}} \rightarrow b_{\texttt{buy}})$ of behaviors where $b_{\texttt{buy}}$ is the target behavior (i.e., $b_t$), and $q$ is the querying user node.
        Starting from $\mathcal{G}_{b_{\texttt{view}}}$, \method calculates ranking scores w.r.t. $q$ along $\GC$, and returns ranking scores of items at the target behavior $b_t$.
    }
\end{figure}

To model a cascading sequence of behaviors, we first construct a \textit{cascading behavior graph} by connecting behavior graphs in the order of the sequence, as shown in Figure~\ref{fig:overview}.
Then, we measure the ranking scores along the cascading behavior graph while considering the querying user’s past history for each behavior. 
We use the ranking scores on the target (or last) behavior to recommend items to the querying user.

\subsection{Cascading Behavior Graph Construction}
\label{sec:proposed:cbg}

% motivation
To design a graph ranking model for multi-behavior interactions, one might simply merge the interactions, resulting in a unified multigraph where each edge represents either a unified or distinct type of behavior, and then measure the ranking scores on the graph. 
However, this naive approach may fail to fully capture the semantics of user behaviors. 
Especially, it is worth noting that behaviors often occur in a certain sequence~\cite{LiuXWY00024, yin2024hecgcn, ChengCHLZGP23fqvn, YanCGSLSL24}; for example, a user first views an item, adds it to a cart, and then purchases it.
We aim to inject this information of user behaviors into our ranking scores. 
%
% details
For this purpose, we first define a cascading sequence as follows:
\begin{definition}[Cascading Sequence]
\label{def:cascading_sequence}
Given a set $\B$ of user behaviors, a cascading sequence $\C$ is defined as follows:
\begin{equation*}
    \C \coloneqq (b_1 \rightarrow b_2 \rightarrow \cdots \rightarrow b_{|\B|}), 
\end{equation*}
where each behavior $b_{i} \in \B$ in the sequence is distinct. 
In general, a sequence of user behaviors leads to the target behavior $b_t$ (e.g., buy); thus, the last behavior $b_{|\B|}$ of $\C$ represents $b_t$ in this case.
\hfill\qedsymbol
\end{definition}

We consider a natural cascading sequence $\C$ of behaviors, i.e., $\C = (\texttt{view} \rightarrow \texttt{cart} \rightarrow \texttt{buy})$ for the aforementioned case. 
From the cascading sequence, we construct the cascading behavior graph $\GC$ as follows:
\begin{definition}[Cascading Behavior Graph]
Given a cascading sequence $C = (b_1 \rightarrow b_2 \rightarrow \cdots \rightarrow b_{t-1} \rightarrow b_t)$ and a set $\{\Gb \: | \: b \in \B \}$ of behavior graphs, the cascading behavior graph $\GC$ is defined as follows:
\begin{align}
    \label{eq:cbg}
    \GC \coloneqq (\Gind{b_1} \rightarrow \Gind{b_2} \rightarrow \cdots \rightarrow
    \Gind{b_{t-1}} \rightarrow \Gind{b_t}), 
\end{align}
where each $\Gind{b'} \rightarrow \Gind{b}$ indicates a node-wise connection that forwardly links each node in $\Gind{b'}$ of the previous behavior $b'$ to the corresponding node in  $\Gind{b}$ of the next behavior $b$.
\hfill\qedsymbol
\end{definition}

Figure~\ref{fig:overview} presents an example of the cascading behavior graph $\GC$. 
By doing so, we ensure that information from a previous behavior influences that of the subsequent behavior in the sequence $\C$, thereby modeling temporal dynamics similar to those observed in dynamic graphs~\cite{LeeLJ22}.
Note that for this purpose, we do not allow reverse-direction connections such as $\Gind{b'} \leftarrow \Gind{b}$ in $\GC$.

\subsection{\method: Personalized Ranking on a Cascading Behavior Graph}
\label{sec:proposed:cascrank}

In this section, we design our ranking model \method on the cascading behavior graph $\GC$ for multi-behavior recommendation. 
Our main idea for estimating ranking scores along the cascading sequence is to smooth the ranking scores of neighboring nodes while incorporating query-specific information for the current behavior $b$, as inspired by Equation~\eqref{eq:birank}, and to propagate and smooth the ranking scores from the previous behavior $b'$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{FORMULATION.pdf}
    \caption{
        \label{fig:formulation}
        Illustration of the computation on the ranking score $\rub(u)$ of user $u$ of Equation~\eqref{eq:cascrank:inst}, with details of the notations provided in Section~\ref{sec:proposed:cascrank}. 
        %The illustration of the computation of $\rib(i)$ is similar to this figure.”
    }
\end{figure}Suppose we consider $\Gind{b'} \rightarrow \Gind{b}$ in the cascading behavior graph $\GC$ to obtain ranking scores $\rub(u)$ and $\rib(i)$ of user $u$ and item $i$ for the current behavior $b$ with respect to the querying user $q$. 
These scores are recursively represented as follows:
\begin{align}
    \label{eq:cascrank:inst}
    \begin{split}
    \rub(u) &= (1-\alpha-\beta) \cdot
    \biggl(
         \sum_{i \in \NIb{u}} \wuib \cdot \rib(i) \
    \biggr) + \alpha\cdot\qub(u) + \beta\cdot\rubp(u), \\
    \rib(i) &= \underbrace{(1-\alpha-\beta) \cdot \biggl(
    \sum_{u \in \NUb{i}}{\wiub \cdot \rub(u)} \biggr)}_{_{\substack{\text{Ranking score} \\ \text{smoothing}}}}
    + \underbrace{\alpha \cdot \qib(i)
        {\color{white}\Biggl(_{B_{B_{B}}}\!\!\!\!\!\!\!\!}
    }_{\substack{\text{Query} \\ \text{fitting}}}  + \underbrace{\beta \cdot \ribp(i), {\color{white}\Biggl(_{B_{B_{B}}}\!\!\!}}_{\substack{\text{Cascading} \\ \text{alignment}}}
    \end{split}
\end{align}
where the equation of $\rub(u)$ is illustrated in Figure~\ref{fig:formulation}, and that of $\rib(i)$ is similar to the figure.
The first term in $\rub(u)$ follows the smoothness assumption by aggregating the ranking scores of each neighbor in $\NIb{u}$ of node $u$ with weight $\wuib$, where it is adjusted by $\gamma \coloneqq 1 - \alpha -\beta$, referred to as the strength of the smoothness.
The second term aims to align with $\qub(u)$ and $\qib(i)$, which encapsulate the information associated with nodes $u$ and $i$ with respect to the querying user $q$ for behavior $b$, where $\alpha$ controls the strength of the query fitting.
The third term, called \textit{cascading alignment}, smoothly aligns $\rub(u)$ with $\rubp(u)$ from the previous behavior $b’$, where $\beta$ controls the strength of this alignment.
The hyperparameters $\alpha$ and $\beta$ are within the range $(0, 1)$, satisfying $0 < \alpha + \beta < 1$.
%
%These constraints enforce that the scale of ranking score smoothing remains within $[0,1]$.
Note that the equation of $\rib(i)$ is interpreted similarly to that of $\rub(u)$, and we set $\alpha = \lambda_{\mathcal{U}} = \lambda_{\mathcal{I}}$, unlike in Equation~\eqref{eq:birank}, to reduce the number of hyperparameters.

\smallsection{Normalization weights}
As discussed in Section~\ref{sec:preliminaires:birank}, the design choice for the normalization edge weights $\wuib$ and $\wiub$ is one of the crucial factors in designing ranking models.
In this work, we adopt a symmetric normalization that sets these weights when there is an interaction between $u$ and $i$ (i.e., $\Ab(u,i)=1$ and $\Ab^{\top}(i,u)=1$), as follows:
\begin{equation*}
    \wuib = \frac{\Ab(u,i)}{\sqrt{d_b(u)}\sqrt{d_b(i)}} \quad \text{and} \quad \wiub = \frac{\Ab^{\top}(i,u)}{\sqrt{d_b(i)}\sqrt{d_b(u)}}, 
\end{equation*}
where $\Ab$ is the bi-adjacency matrix of $\Gb$, and $d_b(u)$ and $d_b(i)$ are the degrees of user $u$ and item $i$, respectively.
If $\Ab(u, i)$ is $0$, then $\wuib = \wiub$ is set to $0$.
%
The main reason we choose the symmetric normalization is its effectiveness in eliminating the impact of high-degree nodes when aggregating the ranking scores, especially compared to a stochastic normalization, such as $\wuib = \Ab(u,i)/d_b(i)$.
By doing so, this prevents the estimated ranking scores from being biased toward high-degree nodes (or items) and avoids penalizing low-degree items in a long-tail distribution. For this reason, it has been widely used in graph ranking for recommendation in previous studies~\cite{HeHDWLZW20, HeHGKW17}.
We empirically verify the effectiveness of the symmetric normalization over the stochastic normalization, as shown in Table~\ref{tab:ablation:normalization}.

\smallsection{Query information}
We use the information of $q$ and its interaction history for each behavior $b$ to set $\qub$ and $\qib$ for multi-behavior recommendation. 
Specifically, we set $\qub$ and $\qib$ as follows:
\begin{equation*}
    \qub(u) = \begin{cases}
        1, & \text{if } u = q, \\
        0, & \text{otherwise, }
    \end{cases}
    \quad \text{ and } \quad
    \qib(i) = \begin{cases}
        |\NIb{q}|^{-1}, & \text{if } i \in \NIb{q}, \\
        0, & \text{otherwise, }
    \end{cases}
\end{equation*}
where $\NIb{q}$ denotes the set of items that $q$ has interacted with under behavior $b$.
In other words, $\qub$ aims to enhance personalization for $q$, while $\qib$ incorporates the user's interaction history into the ranking scores.

\smallsection{Vectorization} 
We vectorize the entry-wise form of Equation~\eqref{eq:cascrank:inst} using the following matrix-vector multiplications:
\begin{align}
    \label{eq:cascrank:vect}
    \begin{split}
    \rub &= \gamma \cdot \Abnorm \cdot \rib \
    + \alpha\cdot\qub + \beta\cdot\rubp, \\
    \rib &= \gamma \cdot
        \Abnorm^{\top} \cdot \rub \
     + \alpha\cdot\qib + \beta\cdot\ribp,
    \end{split}
\end{align}
where $\gamma = 1 - \alpha - \beta$, and
$\Abnorm = \Dub^{-1/2} \Ab \Dib^{-1/2}$ is the symmetrically normalized bi-adjacency matrix. 
$\Dub$ and $\Dib$ are the diagonal degree matrices of $\Ub$ and $\Ib$, respectively, defined as follows:
\begin{equation*}
    \Dub = \texttt{diag}\Big(d_{b}(u) \: | \: \forall u \in \Ub \Big), \quad \text{and} \quad 
    \Dib = \texttt{diag}\Big(d_{b}(i) \: | \: \forall i \in \Ib \Big)
\end{equation*}
where $\texttt{diag}(\cdot)$ returns a diagonal matrix with the input values on its diagonal.
Then, Equation~\eqref{eq:cascrank:vect} can be further compactly represented as follows:
\begin{align}
    \label{eq:cascrank:mat}
    \underset{\rb}
    {\begin{bmatrix}
        \rub\\\rib
    \end{bmatrix}}=
    \gamma \cdot 
    \underset{\tilde{\mathbf{\mathcal{A}}}_b}{\begin{bmatrix}
        \mat{0}_{\Ub} & \Abnorm \\
        \Abnorm^{\top} & \mat{0}_{\Ib}
    \end{bmatrix}}
    \underset{\rb}
    {\begin{bmatrix}
        \rub\\\rib
    \end{bmatrix}}+
    \alpha \cdot
    \underset{\qb}
    {\begin{bmatrix}
        \qub\\\qib
    \end{bmatrix}}+
    \beta\cdot
    \underset{\vect{r}_{b'}}
    {\begin{bmatrix}
        \rubp \\ \ribp
    \end{bmatrix}},
\end{align}
where $\mat{0}_{\Ub} \in \mathbb{R}^{|\Ub| \times |\Ub|}$ and $\mat{0}_{\Ib} \in \mathbb{R}^{|\Ib| \times |\Ib|}$ are zero matrices, and $\rb$ contains the ranking scores for all users and items, whose closed-form solution is given as follows:
\begin{align}
    \label{eq:cascrank:closed}
    \begin{split}
    \big(\mat{I} - \gamma \cdot \tilde{\mathbf{\mathcal{A}}}_b \big) \cdot \rb &= \alpha \cdot \qb + \beta \cdot \vect{r}_{b'}, \\
    \mat{r}_{b} &= \mat{L}_b^{-1} \cdot
    \big(
        \alpha \cdot \qb + \beta \cdot \vect{r}_{b'}
    \big),
    \end{split}
\end{align}
where 
$\mat{I}$ is an identity matrix, and
$\mat{L}_b = \mat{I} - \gamma \cdot \tilde{\mathbf{\mathcal{A}}}_b$ is invertible for $0 < \gamma < 1$ because it is positive definite
\footnote{
The symmetrically normalized adjacency matrix $\mathcal{A}_b$ is symmetric with eigenvalues $\lambda$ in $[-1, 1]$, and the eigenvalues of $\mathbf{L}_b$, represented as $1 - \gamma \cdot \lambda \in [1 - \gamma, 1 + \gamma]$, are all positive for $0 < \gamma < 1$, indicating that $\mathbf{L}_b$ is positive definite.
}.
Given $\GC = (\Gind{b_1} \rightarrow \Gind{b_2} \rightarrow \cdots \rightarrow \Gind{b_t})$, we first compute $\vect{r}_{b_1}$, initializing the previous score vector $\vect{r}_{b'}$ to $\vect{q}_{b_1}$. 
We then proceed along the chain, repeatedly computing $\rb$ for each behavior, until finally obtaining $\vect{r}_{b_t}$ for the target behavior $b_t$.

\subsection{Iterative Algorithm for \method}
\label{sec:proposed:alg}
Although the ranking scores can be directly computed using Equation~\eqref{eq:cascrank:closed}, the matrix inversion operation of $\mat{L}_{b}^{-1}$ is intractable for large-scale graphs due to its cubic time complexity.
Instead, we employ an iterative approach, such as power iteration, similar to other graph ranking methods~\cite{HeHDWLZW20, HeHGKW17, BrinB98}. 
The main idea is to repeatedly update Equation~\eqref{eq:cascrank:vect} starting from initial values until convergence, which is encapsulated in Algorithm~\ref{alg:method}.
It first initializes $\rubp$ and $\ribp$ as the query vectors (line~\ref{alg:method:init}), as there is no previous behavior for the first behavior in $\GC$.
Then, it computes $\rub$ and $\rib$ for each behavior $b$ along the chain in $\GC$ (lines~\ref{alg:method:outer:for}-\ref{alg:method:outer:forend}).
Suppose we consider $\Gind{b}$ of the link $\Gind{b'} \rightarrow \Gind{b}$ in the chain of $\GC$ (line~\ref{alg:method:outer:for}). 
Then, it symmetrically normalizes $\Ab$ (line~\ref{alg:method:norm}), and then performs the power iteration to obtain $\rub$ and $\rib$ (lines~\ref{alg:method:inner:for}-\ref{alg:method:inner:forend}).
It repeats Equation~\eqref{eq:cascrank:vect} at most $K$ times until the convergence criteria is satisfied (line~\ref{alg:method:convergence}).
After updating $\rubp$ and $\ribp$ for the next iteration (line~\ref{alg:method:update:next}), it continues the power iterations until reaching the last (or target) behavior.
Since we only need the ranking scores of items for the target behavior $b_t$ for Problem~\ref{prob:mbr}, it returns $\vect{r}_{\items_{b_t}}$ at the end (line~\ref{alg:method:return}).
Note that the power iteration converges when $\alpha$, $\beta$, and $\gamma$ are in $(0, 1)$, as guaranteed by Theorem~\ref{theorem:convergence}.

\begin{algorithm}[t]
    %\setstretch{1.1}
    \caption{Iterative algorithm of \method}
    \label{alg:method}
    \begin{algorithmic}[1]
        \small
        \Require 
        \item[] Cascading behavior graph: $\GC$
        \item[] Querying user: $q$
        \item[] Strength of query fitting: $\alpha$
        \item[] Strength of cascading alignment: $\beta$
        \item[] Number of iterations: $K$
        \item[] Convergence threshold: $\epsilon$
        %Set of cascading sequence of behaviors $\C$, set of bi-adjacency matrices for each behavior, set of query vectors for users and items on query $q$
        \Ensure 
        \item[] Ranking score vector $\vect{r}_{\items_{b_t}}$ for items on the target behavior $b_t$ w.r.t. $q$
        \vspace{1mm} 
        \State Let $b'$ and $b$ denote the previous and current behaviors, respectively
        \State Set $\rubp \leftarrow \vect{q}_{\users_{b_1}}$ and $\ribp \leftarrow \vect{q}_{\items_{b_1}}$ \label{alg:method:init}{\Comment{\textbf{\scriptsize
        Initialization for the cascading alignment}}}
        
        \For{\textbf{each} $\Gind{b} \in \mathcal{\GC}$} \label{alg:method:outer:for}
            \State Symmetrically normalize $\Ab$, i.e., $\Abnorm \leftarrow \Dub^{-1/2} \Ab \Dib^{-1/2}$  \label{alg:method:norm}
            \State Set $\rub' \leftarrow \qub$ and $\rib' \leftarrow \qib$ {\Comment{\textbf{\scriptsize Initialization for the power iteration}}} 
            
            \For {$k \leftarrow 1 $ to $K$} \label{alg:method:inner:for} {\Comment{\textbf{\scriptsize Power iteration}}} 
            \label{alg:method:poweriteration}
                \State $\rub \leftarrow \gamma \cdot \Abnorm \cdot \rib' + \alpha\cdot\qub + \beta\cdot\rubp$ \label{alg:method:userscore}
                {\Comment{\textbf{\scriptsize $\gamma = 1-\alpha-\beta$}}} 
                \State $\rib \leftarrow \gamma \cdot\Abnorm^{\top} \cdot \rub'  + \alpha\cdot\qib + \beta\cdot\ribp$
                \label{alg:method:itemscore}
                \If{$\lVert \rub - \rub' \rVert_{1}+ \lVert \rib - \rib' \rVert_{1} \leq \epsilon$} \label{alg:method:convergence}
                    \State \textbf{break}
                \EndIf
                \State Set $\rub' \leftarrow \rub$ and $\rib' \leftarrow \rib$ {\Comment{\textbf{\scriptsize Update for the power iteration}}} 
            \EndFor \label{alg:method:inner:forend}
            \State Set $\rubp \leftarrow \rub$ and $\ribp \leftarrow \rib$ \label{alg:method:update:next}
            {\Comment{\textbf{\scriptsize Update for the cascading alignment}}} 
        \EndFor \label{alg:method:outer:forend}
    \State \textbf{return} $\vect{r}_{\items_{b_t}} \leftarrow \ribp$ \label{alg:method:return}
    \end{algorithmic}
\end{algorithm}

\subsection{Interpretation from the Perspective of Optimization}
\label{sec:proposed:reg}
We interpret the ranking design of \method in the form of an optimization problem, inspired by manifold optimization~\cite{ZhouZBLWS03,AgarwalA06, HeHGKW17}. 
Based on the notations in Equation~\eqref{eq:cascrank:mat}, our ranking model minimizes the following objective function:
\begin{align}
    \label{eq:reg:obj}
    \mathcal{J}(\rb; \mathcal{A}_{b}, \qb, \rbp) = \underbrace{\rb^{\top}(\mat{I} - \mathcal{A}_{b})\rb}_{_{\substack{\text{Ranking score} \\ \text{smoothing}}}} + \underbrace{\theta\lVert\rb - \qb \rVert_{2}^{2}}_{\substack{\text{Query} \\ \text{fitting}}} + \underbrace{\omega\lVert\rb - \rbp \rVert_{2}^{2}}_{\substack{\text{Cascading} \\ \text{alignment}}}, 
\end{align}
where the first term, $\rb^{\top}(\mat{I} - \mathcal{A}_{b})\rb = \sum_{i,j}\mathcal{A}_b(i, j)\cdot\lVert\rb(i) - \rb(j)\rVert_{2}^{2}$, is referred to as \textit{graph Laplacian smoothing}, which aims to measure the difference in the ranking scores between two connected nodes.
$\theta$ and $\omega$ controls the strength of the regularizations of query fitting and cascading alignment, respectively.

The solution $\rb$ obtained by \method minimizes the optimization problem, effectively smoothing the scores of neighboring nodes while also fitting the query information in $\qb$ and the previous scores in $\rbp$.
To prove the claim, we rewrite the objective function as follows:
\begin{align*}
    %\label{eq:reg:obj}
    \begin{split}
    \mathcal{J}(\rb; \mathcal{A}_{b}, \qb, \rbp)  &= 
    \begin{bmatrix}
        \rub^{\top} & \rib^{\top}
    \end{bmatrix}
    \begin{bmatrix}
        \mat{I} & -\tAb \\
        -\tAb^{\top} & \mat{I} 
    \end{bmatrix}
    \begin{bmatrix}
        \rub \\ \rib
    \end{bmatrix} \\
    &+ \theta \cdot \left( \lVert \rub - \qub \rVert_{2}^{2} + \lVert \rib - \qib \rVert_{2}^{2} \right) \\
    &+ \omega \cdot \left(\lVert \rub - \rubp \rVert_{2}^{2} + \lVert \rib - \ribp \rVert_{2}^{2} \right). 
    \end{split}
\end{align*}

Note that the optimization problem of graph Laplacian smoothing with strictly convex regularization terms (e.g., $\lVert \rub - \qub \rVert_{2}^{2}$) is strictly convex~\cite{HeHGKW17}, and thus it has a unique global minimum.
The first-order derivatives with respect to $\rub$ and $\rib$ are represented as follows: 
\begin{align*}
    %\label{eq:reg:grad}
    \begin{split}
    \frac{\partial\mathcal{J}}{\partial\rub} = 
    2\rub - 2\tAb\rib + 2\theta (\rub - \qub) + 2\omega(\rib - \ribp), \\
    \frac{\partial\mathcal{J}}{\partial\rib} = 
    2\rib - 2\tAb^{\top}\rub + 2\theta (\rib - \qib) + 2\omega(\rib - \ribp).
    \end{split}
\end{align*}

By setting each derivative to $0$, it provides the following solutions:
\begin{align*}
    %\label{eq:reg:closed}
    \begin{split}
        \rub = \frac{1}{1+\theta+\omega}\tAb\rib + 
        \frac{\theta}{1+\theta+\omega}\qub + \frac{\omega}{1+\theta+\omega}\rubp, \\
        \rib = \frac{1}{1+\theta+\omega}\tAb^{\top}\rub + 
        \frac{\theta}{1+\theta+\omega}\qib + \frac{\omega}{1+\theta+\omega}\ribp,
    \end{split}
\end{align*}
which are the same as Equation~\eqref{eq:cascrank:vect} if $\theta=\frac{\alpha}{1-\alpha-\beta}$ and $\omega = \frac{\beta}{1-\alpha-\beta}$.

\subsection{Theoretical Analysis}
\label{sec:proposed:analysis}
In this section, we theoretically analyze \method and its iterative algorithm with the following questions:
\begin{enumerate}[leftmargin=9mm,noitemsep]
    \item[T1.] {
        \textbf{Cascading effect.}
        Does our ranking design adhere to the cascading effect, where a behavior occurring later in the sequence has a greater influence on the target behavior?
    }
    \item[T2.] {
        \textbf{Convergence guarantee.}
        Is the convergence of our iterative algorithm guaranteed? 
    }
    \item[T3.] { 
        \textbf{Computational complexity.} Does our iterative algorithm scale linearly with the number of interactions?
    }
\end{enumerate}

\subsubsection{Analysis on Cascading Effect (T1)}
\label{sec:proposed:analysis:casc}
In this section, we investigate the cascading effect of \method by analyzing the ranking score vector $\vect{r}_{b_t}$ on the target behavior $b_t$ based on Equation~\eqref{eq:cascrank:closed} in the following theorem.

\begin{theorem}[Cascading Effect] 
\label{theorem:decay}
The ranking score vector $\vect{r}_{b_t}$ on the target behavior $b_t$ is represented as follows:
\begin{align}
    \label{eq:casc_effect}
    \vect{r}_{b_t} = \left(\sum_{i=0}^{t-1}\beta^{i} \cdot \mat{\mathcal{L}}_{b_t \leftsquigarrow b_{t-i}} \cdot \hat{\vect{q}}_{b_{t-i}} \right) + \beta^{t}\cdot\mat{\mathcal{L}}_{b_t \leftsquigarrow b_1}\cdot\vect{r}_{b_0},
\end{align}
where $\hat{\vect{q}}_{b_i} = \alpha \cdot \vect{q}_{b_i}$, $\mat{\mathcal{L}}_{b_i} = \mat{L}_{b_i}^{-1} = (\mat{I} - \gamma \cdot \tilde{\mathbf{\mathcal{A}}}_{b_i})^{-1}$, $\mat{\mathcal{L}}_{b_j\leftsquigarrow b_i} = \mat{\mathcal{L}}_{b_j} \cdot \mat{\mathcal{L}}_{b_{j-1}}\cdots\mat{\mathcal{L}}_{b_i}$, 
$\Linvprod{i}{i} = \mat{\mathcal{L}}_{b_i}$,
and $\vect{r}_{b_0}$ is an initial vector of ranking scores (i.e., $\vect{q}_{b_1}$).
\end{theorem}
\begin{proof}
    We prove the claim by mathematical induction.
    As a base case when $b_t = b_1$,  $\vect{r}_{b_1}$ is represented as follows:
    \begin{align*}
        \vect{r}_{b_1} = \Linvprod{1}{b_1}\cdot\qindhat{1} + \beta\cdot\Linvprod{1}{1}\cdot\vect{r}_{b_0} \Leftrightarrow \vect{r}_{b_1} = \L^{-1}_{b_1}(\alpha \cdot \vect{q}_{b_1}+\beta \cdot \vect{r}_{b_0}).
    \end{align*}
    Thus, it obviously holds for Equation~\ref{eq:cascrank:closed} when applied to $\vect{r}_{b_1}$.
    Suppose the claim holds at behavior $b_t$. 
    Based on Equation~\eqref{eq:cascrank:closed}, $\vect{r}_{b_{t+1}}$ is represented as follows:
    \begin{align*}
        \mat{r}_{b_{t+1}} &= \mat{L}_{b_{t+1}}^{-1} \cdot
    \big(
        \alpha \cdot \vect{q}_{b_{t+1}} + \beta \cdot \vect{r}_{b_{t}}
    \big) \Leftrightarrow
    \vect{r}_{b_{t+1}} = \Linv_{b_{t+1}}\cdot\qindhat{t+1} + \beta \cdot \Linv_{b_{t+1}}\cdot\vect{r}_{b_t}
    \end{align*}
    By replacing $\vect{r}_{b_t}$ of Equation~\eqref{eq:casc_effect} into the above equation, it is represented as follows: 
    \begin{align*}
        \vect{r}_{b_{t+1}} &= \Linv_{b_{t+1}}\cdot\qindhat{t+1} + \beta\cdot \Linv_{b_{t+1}}\cdot\Biggl(\sum_{i=0}^{t-1}\beta^{i}\cdot\Linvprod{t}{t-i}\cdot\qindhat{t-i} + \beta^{t}\cdot\Linvprod{t}{1}\cdot\vect{r}_{b_0} \Biggl),\\
         &= \sum_{i=0}^{t}\beta^{i}\cdot\Linvprod{t+1}{t+1-i}\cdot\qindhat{t+1-i} + \beta^{t+1}\cdot\Linvprod{t+1}{1}\cdot\vect{r}_{b_0}.
    \end{align*}
    If we replace $t+1$ with $k$, then it is represented as follows:
    \begin{align*}
        \vect{r}_{b_k} = \sum_{i=0}^{k-1} \beta^{i}\cdot\Linvprod{k}{k-i}\cdot\qindhat{k-i} + \beta^{k}\cdot\Linvprod{k}{1}\cdot\vect{r}_{b_0}. 
    \end{align*}
    This equation has exactly the same form as Equation~\eqref{eq:casc_effect}. 
    Thus, it holds for $b_{t+1}$, and therefore, the claim is true by mathematical induction.
\end{proof}

From Theorem~\ref{theorem:decay}, the ranking score vector $\vect{r}_{b_t}$ on the target behavior can be represented as follows:
\begin{align*}
    \vect{r}_{b_t} \propto \beta^{0}\cdot\Linvprod{t}{t}\cdot\qindhat{t} + \beta^{1}\cdot\Linvprod{t}{t-1}\cdot\qindhat{t-1} + \cdots + \beta^{t-1}\cdot\Linvprod{t}{1}\cdot\qindhat{1}, 
\end{align*}
where the term $\Linvprod{t}{t-i}$ is interpreted as the diffused propagation of the information in $\qindhat{t-i}$ from $\mathcal{G}_{b_{t-i}}$ to $\mathcal{G}_{b_{t}}$ along the chain, and the result decays as $i$ increases, with a factor of $0\leq \beta \leq 1$.
%
This aligns with the cascading effect, indicating that behaviors with stronger preference signals (e.g., purchases) are emphasized more than those with weaker preference signals (e.g., views).
A smaller value of $\beta$ further attenuates the impact of earlier behaviors in the chain on the ranking scores. 

\subsubsection{Analysis on Convergence (T2)}
\label{sec:proposed:analysis:convergence}
Inspired by~\cite{HeHGKW17}, we theoretically analyze the convergence of the iterative algorithm in Algorithm~\ref{alg:method} for \method in the following theorem.

\begin{theorem}[Convergence] 
\label{theorem:convergence}
The power iteration in Algorithm~\ref{alg:method} converges when $\gamma$ is in $(0, 1)$.
\end{theorem}
\begin{proof}
    Suppose we consider $\Gind{b}$ of the link $\Gind{b'} \rightarrow \Gind{b}$ in the chain of $\GC$.
    In Equation~\eqref{eq:cascrank:vect}, we substitute $\rub$ into the equation for $\rib$, which is represented as follows:
    \begin{align*}
        \rib &\leftarrow 
        \underbrace{\gamma^2 \cdot \Abnorm^{\top}\cdot\Abnorm \cdot\rib}_{\text{Variant term}}
        + \underbrace{\alpha\gamma \cdot \Abnorm^{\top}\cdot\qub + \beta\gamma\cdot \Abnorm^{\top}\cdot\rubp + \alpha \cdot \qib + \beta \cdot \ribp}_{\text{Invariant term}}, \\
        \rib &\leftarrow \mat{S}\cdot\rib + \vect{x},
    \end{align*}
where $\gamma = 1-\alpha-\beta$, and $\rubp$ and $\ribp$ are fixed at this step.
Let $\mat{S}$ represent $\gamma^{2} \cdot \Abnorm^{\top} \cdot \Abnorm$ in the variant term, and let $\vect{x}$ denote the invariant term.
Suppose $\rib^{(k)}$ represents the result after $k$ iterations of the above equation, starting from $\rib^{(0)}$. It can then be represented as follows:
\begin{align*}
    \rib^{(k)} = \mat{S}^{k}\cdot\rib^{(0)} + \sum_{t=0}^{k-1} \mat{S}^{t} \cdot \vect{x}.
\end{align*}
Assuming the eigenvalues of $\mat{S}$ lie within the range $(-1, 1)$, the infinite iterations for $\rib^{(k)}$ converge as follows:
\begin{align*}
\lim_{k\rightarrow\infty}\mat{S}^{k}\cdot\rib^{(0)} = 0, \quad \text{and} \quad \lim_{k\rightarrow\infty}\sum_{t=0}^{k-1}\mat{S}^{k} = (\mat{I}-\mat{S})^{-1},
\end{align*}
which is guaranteed by the Neumann series (i.e., the geometric series for matrices).
To analyze the range of the eigenvalues of $\mat{S}$, we represent it as follows:
\begin{align*}
    \mat{S} &= \gamma^{2}\cdot \Abnorm^{\top}\cdot \Abnorm, \\
    &= \gamma^{2}\cdot\Big(\mat{D}_{\users_b}^{-1/2}\Ab\mat{D}_{\items_b}^{-1/2}\Big)^{\top}\cdot\Big(\mat{D}_{\users_b}^{-1/2}\Ab\mat{D}_{\items_b}^{-1/2} \Big), \\
    &= \gamma^2\cdot \Big(\mat{D}_{\items_b}^{-1/2}\Ab^{\top}\mat{D}_{\users_b}^{-1}\Ab\mat{D}_{\items_b}^{-1/2} \Big), \\
    &= \mat{D}_{\items_b}^{-1/2}\mat{S}_{v}\mat{D}_{\items_b}^{1/2},
\end{align*}
where $\mat{S}_{v} = \gamma^{2}(\Ab^{\top}\mat{D}_{\users_b}^{-1}\cdot\Ab\mat{D}_{\items_b}^{-1})$. 
Note that the largest absolute eigenvalue of  $\Ab^{\top}\mat{D}_{\users_b}^{-1}\cdot\Ab\mat{D}_{\items_b}^{-1}$ is $1$ because it is a column-stochastic matrix, and thus, the eigenvalues of $\mat{S}_{v}$ are in the range $[-\gamma^2, \gamma^2]$.
In addition, the eigenvalues of $\mat{S}_{v}$ are identical to those of $\mat{S}$. 
The reason is as follows:
\begin{align*}
    |\mat{S}_{v} - \lambda_{v}\mat{I}| = |\mat{D}_{\items_b}^{1/2}(\mat{S} - \lambda_{v}\mat{I})\mat{D}_{\items_b}^{-1/2}| = 
    |\mat{D}_{\items_b}^{1/2}|\cdot|\mat{S} - \lambda_{v}\mat{I}|\cdot |\mat{D}_{\items_b}^{-1/2}| = |\mat{S} - \lambda_{v}\mat{I}| = 0,
\end{align*}
where $|\cdot|$ indicates the determinant of a given matrix. 
The largest absolute eigenvalue of  $\Ab^{\top}\mat{D}_{\users_b}^{-1}\cdot\Ab\mat{D}_{\items_b}^{-1}$ is $1$ because it is column stochastic.
Therefore, the eigenvalues of $\mat{S}$ lie within the range $[-\gamma^2, \gamma^2]$, and the condition $\gamma \in (0, 1)$ ensures that this range is bounded within $(-1, 1)$.
Note that the convergence of $\rib$ leads to that of $\rub$. 
%and when $\gamma$ in $(0,1)$ then the algorithm always converges.
%Without loss of generality, we prove the convergence of $\rub$ and $\rib$ on each behavior $b \in \B$.
%Note that lower $\gamma$ leads to faster convergence.
\end{proof}


\subsubsection{Complexity Analysis  (T3)}
\label{sec:proposed:analysis_complexity}

We analyze the time complexity of Algorithm~\ref{alg:method} for \method. 
Suppose $m \coloneqq \sum_{b \in \B} |\Eb|$ is the total number of interactions, and $n \coloneqq |\users| + |\items|$ is the number of nodes in each behavior graph.
Note that each bi-adjacency matrix $\Ab$ is sparse and is therefore stored in a sparse matrix format, such as compressed sparse row (CSR)~\cite{daglib}.

\begin{theorem}[Time Complexity]
The time complexity of Algorithm~\ref{alg:method} for \method is $O(K  (m + |\B| n))$, where $K$ is the maximum number of iterations and $|\B|$ is the number of behaviors.
\end{theorem}

\begin{proof}
Algorithm~\ref{alg:method} repeats the power iteration in its inner loop for each behavior, following the symmetric normalization and initialization, both of which require $O(|\Eb| + |\Ub| + |\Ib|)$ time.
The power iteration repeats at most $K$ times, with its main bottleneck arising from sparse matrix-vector multiplications~\cite{daglib}, requiring $O(K(|\Eb| + |\Ub| + |\Ib|))$ time for all iterations.
The outer loop repeats $|\B|$ times, as the length of the cascading chain is $|\B|$.
Putting everything together, the total time complexity is $\sum_{b \in \B} (K+1)(|\Eb| + |\Ub| + |\Ib|) = (K+1)(m + |\B| n) \in O(K  (m + |\B| n))$, where $|\Ub| = |\users|$ and $|\Ib| = |\items|$.
\end{proof}

Note that in real-world multi-behavior datasets, the main factor is the number $m$ of interactions, since $|\B|$ is small (e.g., 3 or 4), $K$ is a fixed constant, and the number $n$ of nodes is much smaller than $m$, as shown in Table~\ref{tab:data}. 
This indicates that the algorithm for \method primarily takes $O(m)$ time, i.e., it scales linearly with the number of interactions, as empirically verified in Figure~\ref{fig:scalability}.