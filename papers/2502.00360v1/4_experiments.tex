\section{Experiments}
\paragraph{Implementation Details}
In the first stage of our pipeline, we adopt the configuration of DreamGaussian~\cite{tang2023dreamgaussian} and conduct training for $500$ iterations. Each Gaussian is initialized as a sphere centered at $(0,0,0)$, while the cameras are distributed on a sphere with a radius of $2.5$. During training, we randomly sample camera positions within a 20-degree range around each primary view. 
For image restoration, we employ the ComfyUI integrated version of SUPIR and directly input Gaussian renderings from the primary views. 
For video generation, we leverage the original code of DimensionX. We remove the background from the SUPIR results and input them into the model, selecting 20 frames closest to the primary view in the generated videos as satellite views.
All experiments are conducted on an NVIDIA A100 (80GB) GPU, with the first and third stage training requiring less than 10 GB of GPU memory. The only exception is that when compared with other methods, we run Fantasia3D~\cite{chen2023fantasia3d} using 8$\times$ 3090 GPUs, which is its official configuration. For the Marching Cubes Algorithm, we sample the SDF on a grid with a $1024^3$ resolution. In the following parts, the case indexes in the tables refer to  Fig.~\ref{fig:main}.


\input{tables/tab_score}

\paragraph{Main Results}
We use our method to generate various multi-semantic textured meshes. The rendering results and normal maps are shown in Fig.~\ref{fig:main}. The meshes are created using diverse semantic and view inputs, demonstrating the rich creativity of our method. The outcomes align well with expected semantics in both geometry and rendering quality. To validate the manufacturability of the generated shapes, we 3D-printed several examples using both colored and white materials: the white prints make it easy to observe the geometric details, while the colored prints verify the final appearance. The results are shown in Fig.~\ref{fig:fab}. The fabricated objects closely match the intended designs and exhibit an aesthetic appeal.

\paragraph{Quantitative Evaluation}
To evaluate the consistency between the generated results and the input semantics, we render the generated shapes from various primary views and use the CLIP score~\cite{clip} to measure their semantic similarity to the input. Table~\ref{tab:clip_score} presents the CLIP scores for textured and non-textured cases. For each primary view, we allow random variations within a 20-degree latitude and longitude range to render the images. The CLIP model then evaluates the captured results 1,000 times, and the average score is taken as the score for that primary view. The results indicate that the generated shapes effectively convey semantic information, regardless of whether textures are applied. Observers can also discern the semantic representation of these geometric shapes even with slight changes in perspective.


In addition, we conducted a user study to further validate our method. The participants were shown the rendering of our generated shapes one at a time, and asked to sequentially answer the following questions with a score from 0 to 10:
\begin{itemize}[leftmargin=*]
\item Q1: How semantically accurate is our textureless rendering?
\item Q2: How semantically accurate is our textured rendering?
\end{itemize}
The last two questions focus on participants' preferences between the results of ours and \cite{Fabricable3DWireArt} under the same semantics. A score closer to 10 indicates a stronger preference for our results:
\begin{itemize}[leftmargin=*]
\item Q3: Which result better aligns with semantics? 
\item Q4: Which overall result do you prefer? 
\end{itemize}
We randomly and fairly selected participants, collecting 102 samples. The average scores from each primary view of each case are presented in Tab.~\ref{tab:user}. The results indicate that our shape design receives considerable recognition in terms of rendering quality. However, our geometry scores were relatively lower, likely due to the need to account for compatibility across different semantics. This might lead to a degree of shape deformation, which may have impacted realism. Nevertheless, in preference scoring, our overall performance in semantics and aesthetics is significantly higher compared to the methods we benchmarked against. This suggests that the combination of our rendering and geometry maintains strong semantic expressiveness. Moreover, when compared to monochrome or outline results, our overall design proved to be highly appealing.

\input{tables/tab_user}

\paragraph{Qualitative Comparison}
As far as we know, no previous research has been conducted with the same purpose as ours. Therefore, we make comparisons with similar works like Shadow Art~\cite{MP09} and Wire Art~\cite{Fabricable3DWireArt}. Similar to us, they aim to represent diverse semantics from different views. Initially, we provided images after restoration to both of them and compared their results with our reconstruction. As shown in Fig.~\ref{fig:ShadowArt}, both methods convey information solely through contours and silhouettes, lacking color representation and meaningful geometric expression. In contrast, our shape representation integrates rendering, enabling the depiction of more intricate and complex shapes while enhancing the presentation of geometry. Fig.~\ref{fig:wireArt} further illustrates our comparison with Wire Art under the same semantic inputs. While both approaches align with the given semantics, our generated shapes are more intricate and refined. Additionally, as shown in the first row of Fig.~\ref{fig:wireArt}, our method can utilize the occlusion of sight to convey different semantics on the front and back, which is challenging for simpler representations based mainly on contours.

To demonstrate the advantages of 3D GS, we experiment with different 3D representations during the design stage. Results are shown in Fig.~\ref{fig:presentation}. Compared to 3DGS, designs of other methods appear relatively dull, with excessive local geometric noise. Additionally, they require significantly more computational resources and longer generation time than Gaussian. 3DGS's flexible and efficient rendering makes it more suitable for our task.

\input{figures/experiments/fig_presentation}

We also conduct an ablation study to show the effectiveness of dilation loss and multi-view Gaussian mask loss in stage 3 reconstruction. As illustrated in Fig.~\ref{fig:abl_sdf}, the absence of dilation loss leads to distorted local geometry. Meanwhile, without Gaussian mask constraints, the overall shape becomes excessively inflated. 