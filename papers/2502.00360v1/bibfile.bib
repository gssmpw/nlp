@article{Hybridimages,
author = {Oliva, Aude and Torralba, Antonio and Schyns, Philippe G.},
title = {Hybrid images},
year = {2006},
issue_date = {July 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1141911.1141919},
doi = {10.1145/1141911.1141919},
abstract = {We present hybrid images, a technique that produces static images with two interpretations, which change as a function of viewing distance. Hybrid images are based on the multiscale processing of images by the human visual system and are motivated by masking studies in visual perception. These images can be used to create compelling displays in which the image appears to change as the viewing distance changes. We show that by taking into account perceptual grouping mechanisms it is possible to build compelling hybrid images with stable percepts at each distance. We show examples in which hybrid images are used to create textures that become visible only when seen up-close, to generate facial expressions whose interpretation changes with viewing distance, and to visualize changes over time within a single picture.},
journal = {ACM Trans. Graph.},
month = jul,
pages = {527–532},
numpages = {6},
keywords = {scale space, hybrid images, human perception}
}

@ARTICLE{GeneratingAmbiguousFigure-GroundImages,
  author={Kuo, Ying-Miao and Chu, Hung-Kuo and Chi, Ming-Te and Lee, Ruen-Rone and Lee, Tong-Yee},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Generating Ambiguous Figure-Ground Images}, 
  year={2017},
  volume={23},
  number={5},
  pages={1534-1545},
  keywords={Shape;Art;Compounds;Computational modeling;Shape measurement;Visualization;Figure-ground perception;partial shape matching;curve deformation;image cropping;image binarization},
  doi={10.1109/TVCG.2016.2535331}}

@article{Reliefsasimages,
author = {Alexa, Marc and Matusik, Wojciech},
title = {Reliefs as images},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/1778765.1778797},
doi = {10.1145/1778765.1778797},
abstract = {We describe how to create relief surfaces whose diffuse reflection approximates given images under known directional illumination. This allows using any surface with a significant diffuse reflection component as an image display. We propose a discrete model for the area in the relief surface that corresponds to a pixel in the desired image. This model introduces the necessary degrees of freedom to overcome theoretical limitations in shape from shading and practical requirements such as stability of the image under changes in viewing condition and limited overall variation in depth. The discrete surface is determined using an iterative least squares optimization. We show several resulting relief surfaces conveying one image for varying lighting directions as well as two images for two specific lighting directions.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {60},
numpages = {7},
keywords = {sculpture, relief, geometry generation}
}

@article{ShadowPix,
author = {Bermano, Amit and Baran, Ilya and Alexa, Marc and Matusk, Wojciech},
title = {ShadowPix: Multiple Images from Self Shadowing},
year = {2012},
issue_date = {May 2012},
publisher = {The Eurographs Association \& John Wiley \& Sons, Ltd.},
address = {Chichester, GBR},
volume = {31},
number = {2pt3},
issn = {0167-7055},
url = {https://doi.org/10.1111/j.1467-8659.2012.03038.x},
doi = {10.1111/j.1467-8659.2012.03038.x},
abstract = {ShadowPixare white surfaces that display several prescribed images formed by the self-shadowing of the surface when lit from certain directions. The effect is surprising and not commonly seen in the real world. We present algorithms for constructing ShadowPixthat allow up to four images to be embedded in a single surface. ShadowPixcan produce a variety of unusual effects depending on the embedded images: moving the light can animate or relight the object in the image, or three colored lights may be used to produce a single colored image. ShadowPixare easy to manufacture using a 3D printer and we present photographs, videos, and renderings demonstrating these effects. © 2012 Wiley Periodicals, Inc.},
journal = {Comput. Graph. Forum},
month = may,
pages = {593–602},
numpages = {10},
keywords = {I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Shadowing}
}

@inproceedings{ConstructabilityOT,
  title={Constructability of triplets},
  author={Jeroen J. A. Keiren and Freek van Walderveen and Alexander Wolff},
  year={2010},
  url={https://api.semanticscholar.org/CorpusID:15150353}
}

@article{Colorchangingeffects,
author = {Pjanic, Petar and Hersch, Roger D.},
title = {Color changing effects with anisotropic halftone prints on metal},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2816795.2818083},
doi = {10.1145/2816795.2818083},
abstract = {We propose a color reproduction framework for creating specularly reflecting color images printed on a metallic substrate that change hue or chroma upon in-plane rotation by 90°. This framework is based on the anisotropic dot gain of line halftones when viewed under specular reflection. The proposed framework relies on a spectral prediction model specially conceived for predicting the color of non-rotated and of 90° in-plane rotated cross-halftones formed of superpositions of horizontal and vertical cyan, magenta and yellow line halftones. Desired non-rotated and rotated image colors are mapped onto the sub-gamut allowing for the desired hue or chroma shift and then, using a 6D correspondence table, converted to optimal cross-halftone ink surface coverages. The proposed recolorization and decolorization framework is especially effective for creating surprising effects such as image parts whose hues change, or gray regions that become colorful. It can be adapted to commercial printers capable of printing with cyan, magenta and yellow inks on substrates formed by an ink attracting polymer lying on top of a metallic film layer. Applications may include art, advertisement, exhibitions and document security.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {167},
numpages = {12},
keywords = {anisotropic dot gain, color prints on metal, color reproduction, cross-halftone, decolorization, document security, hue shifts, in-plane rotation, recolorization, spectral prediction model}
}

@inproceedings{Fabricable3DWireArt,
author = {Tojo, Kenji and Shamir, Ariel and Bickel, Bernd and Umetani, Nobuyuki},
title = {Fabricable 3D Wire Art},
year = {2024},
isbn = {9798400705250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641519.3657453},
doi = {10.1145/3641519.3657453},
abstract = {This paper presents a computational method for automatically creating fabricable 3D wire sculptures from various input modalities, including 3D models, images, and even text. There are several challenges to wire art creation. For example, artists must express the desired visual as a sparse wire representation. It is also difficult to manually bend wires in the air without guidance to fabricate the designed 3D curves. Our workflow solves these challenges by using two core techniques. First, we present an algorithm that automatically generates a fabricable 3D curve representation of the target based on a loss function that measures the semantic distance between the rendered curve and the target. The loss function can be defined using different pre-trained vision-language neural networks to generate wire art from different input types. The loss function is then optimized using differentiable rendering specifically targeting 3D parametric curves. Our method can incorporate various fabrication constraints on the wire as additional regularization terms in the optimization process. Second, we present an algorithm to generate a 3D printable jig structure that can be used to fabricate the generated wire path. The major challenge in the jig generation stems from the design of an intersection-free surface mesh for 3D printing, which we address with our inflation algorithm. The experimental results indicate that our method can handle a wider range of input types and can produce physically fabricable wire shapes compared to previous wire generation methods. Various wire arts have been fabricated using our 3D-printed jig to demonstrate its effectiveness in 3D wire bending.},
booktitle = {ACM SIGGRAPH 2024 Conference Papers},
articleno = {134},
numpages = {11},
keywords = {Curve optimization, Differentiable rendering, Shape modeling},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{MP09,
author = {Mitra, Niloy J. and Pauly, Mark},
title = {Shadow art},
year = {2009},
isbn = {9781605588582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1661412.1618502},
doi = {10.1145/1661412.1618502},
abstract = {"To them, I said, the truth would be literally nothing but the shadows of the images." - Plato, The RepublicShadow art is a unique form of sculptural art where the 2D shadows cast by a 3D sculpture are essential for the artistic effect. We introduce computational tools for the creation of shadow art and propose a design process where the user can directly specify the desired shadows by providing a set of binary images and corresponding projection information. Since multiple shadow images often contradict each other, we present a geometric optimization that computes a 3D shadow volume whose shadows best approximate the provided input images. Our analysis shows that this optimization is essential for obtaining physically realizable 3D sculptures. The resulting shadow volume can then be modified with a set of interactive editing tools that automatically respect the often intricate shadow constraints. We demonstrate the potential of our system with a number of complex 3D shadow art sculptures that go beyond what is seen in contemporary art pieces.},
booktitle = {ACM SIGGRAPH Asia 2009 Papers},
articleno = {156},
numpages = {7},
location = {Yokohama, Japan},
series = {SIGGRAPH Asia '09}
}

@InProceedings{STR22,
    author    = {Sadekar, Kaustubh and Tiwari, Ashish and Raman, Shanmuganathan},
    title     = {Shadow Art Revisited: A Differentiable Rendering Based Approach},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2022},
    pages     = {29-37}
}

@article{Hsiao:2018:MVWA:,
 author = {Hsiao, Kai-Wen and Huang, Jia-Bin and Chu, Hung-Kuo},
 title = {Multi-view Wire Art},
 journal = {ACM Trans. Graph.},
 volume = {37},
 number = {6},
 year = {2018},
 pages = {242:1--242:11},
 articleno = {242},
 numpages = {11}
}

@inproceedings{qu2023dreamwire,
  title={Wired Perspectives: Multi-View Wire Art Embraces Generative AI},
  author={Qu, Zhiyu and Yang, Lan and Zhang, Honggang and Xiang, Tao and Pang, Kaiyue and Song, Yi-Zhe},
  booktitle={CVPR},
  year={2024}
}

@article{Generationofviewdependentmodels,
author = {Sela, Guy and Elber, Gershon},
title = {Generation of view dependent models using free form deformation},
year = {2007},
issue_date = {February 2007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-006-0095-2},
doi = {10.1007/s00371-006-0095-2},
abstract = {We present a scheme which, given two 3D geometric models, creates a third, synergetic model with resemblance to one input model from one viewing direction and the other input model from another, orthogonal, viewing direction. Our scheme automatically calculates the necessary constraints needed to deform the first model’s silhouette into the second model’s in 2D, and creates a 3D deformation function based on these constraints while minimizing the object’s distortion in all areas but the silhouette. The motivation of this work stems from the artwork of conceptual artists such as Shigeo Fukuda [9] and Markus Raetz [19].},
journal = {Vis. Comput.},
month = feb,
pages = {219–229},
numpages = {11},
keywords = {Non-photorealistic rendering, Free form deformation, Computer graphics art, 3D modeling}
}

@article{reflectors,
author = {Sakurai, Kaisei and Dobashi, Yoshinori and Iwasaki, Kei and Nishita, Tomoyuki},
title = {Fabricating reflectors for displaying multiple images},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201400},
doi = {10.1145/3197517.3201400},
abstract = {A great deal of attention has been devoted to the fabrication of reflectors that can display different color images when viewed from different directions not only in industry but also for the arts. Although such reflectors have previously been successfully fabricated, the number of images displayed has been limited to two or they suffer from ghosting artifacts where mixed images appear. Furthermore, the previous methods need special hardware and/or materials to fabricate the reflectors. Thus, those techniques are not suitable for printing reflectors on everyday personal objects made of different materials, such as name cards, letter sheets, envelopes, and plastic cases. To overcome these limitations, we propose a method for fabricating reflectors using a standard ultraviolet printer (UV printer). UV printer can render a specified 2D color pattern on an arbitrary material and by overprinting the printed pattern can be raised, that is, the printed pattern becomes a microstructure having color and height. We propose using these micro structures to formulate a method for designing spatially varying reflections that can display different target images when viewed from different directions. The microstructure is calculated by minimizing an objective function that measures the differences between the intensities of the light reflected from the reflector and that of the target image. We show several fabricated reflectors to demonstrate the usefulness of the proposed method.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {158},
numpages = {10},
keywords = {ultraviolet printer, spatially varying anisotropic BRDF, reflector, digital fabrication}
}

@article{Mirror,
author = {Wu, Kang and Chen, Renjie and Fu, Xiao-Ming and Liu, Ligang},
title = {Computational Mirror Cup and Saucer Art},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3517120},
doi = {10.1145/3517120},
abstract = {In the mirror cup and saucer art created by artists Yul Cho and Sang-Ha Cho, part of the saucer is directly visible to the viewer, while the other part of the saucer is occluded and can only be seen as a reflection through a mirror cup. Thus, viewers see an image directly on the saucer and another image on the mirror cup; however, the existing art design is limited to wavelike saucers. In this work, we propose a general computational framework for mirror cup and saucer art design. As input, we take from the user one image for the direct view, one image for the reflected view, and the base shape of the saucer. Our algorithm then generates a suitable saucer shape by deforming the input shape. We formulate this problem as a constrained optimization for the saucer surface. Our framework solves for the fine geometry details on the base shape along with its texture, such that when a mirror cup is placed on the saucer, the user-specified images are observed as direct and reflected views. Through extensive experiments, we demonstrate the effectiveness of our framework and the great design flexibility that it offers to users. We further validate the produced art pieces by fabricating the colored saucers using three-dimensional printing.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {174},
numpages = {15},
keywords = {sparse spike movement, black-white shape enhancement, differentiable rendering, Mirror cup and saucer art}
}

@article{Attenuators,
author = {Baran, Ilya and Keller, Philipp and Bradley, Derek and Coros, Stelian and Jarosz, Wojciech and Nowrouzezahrai, Derek and Gross, Markus},
title = {Manufacturing Layered Attenuators for Multiple Prescribed Shadow Images},
year = {2012},
issue_date = {May 2012},
publisher = {The Eurographs Association \& John Wiley \& Sons, Ltd.},
address = {Chichester, GBR},
volume = {31},
number = {2pt3},
issn = {0167-7055},
url = {https://doi.org/10.1111/j.1467-8659.2012.03039.x},
doi = {10.1111/j.1467-8659.2012.03039.x},
abstract = {We present a practical and inexpensive method for creating physical objects that cast different color shadow images when illuminated by prescribed lighting configurations. The input to our system is a number of lighting configurations and corresponding desired shadow images. Our approach computes attenuation masks, which are then printed on transparent materials and stacked to form a single multi-layer attenuator. When illuminated with the input lighting configurations, this multi-layer attenuator casts the prescribed color shadow images. Alternatively, our method can compute layers so that their permutations produce different prescribed shadow images under fixed lighting. Each multi-layer attenuator is quick and inexpensive to produce, can generate multiple full-color shadows, and can be designed to respond to different types of natural or synthetic lighting setups. We illustrate the effectiveness of our multi-layer attenuators in simulation and in reality, with the sun as a light source. © 2012 Wiley Periodicals, Inc.},
journal = {Comput. Graph. Forum},
month = may,
pages = {603–610},
numpages = {8},
keywords = {shadowing, shading, and texture, I.4.0 [Image Processing and Computer Vision]: General—Image Displays, I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Color, I.3.3 [Computer Graphics]: Picture/Image Generation—Display algorithms}
}

@inproceedings{LenticularObjects,
author = {Zeng, Jiani and Deng, Honghao and Zhu, Yunyi and Wessely, Michael and Kilian, Axel and Mueller, Stefanie},
title = {Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces That Can Change their Appearance Depending on the Viewpoint},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474815},
doi = {10.1145/3472749.3474815},
abstract = {In this paper, we present a method that makes 3D objects appear differently under different viewpoints. We accomplish this by 3D&nbsp;printing lenticular lenses across the curved surface of objects. By calculating the lens distribution and the corresponding surface color patterns, we can determine which appearance is shown to the user at each viewpoint. We built a 3D editor that takes as input the 3D model, and the visual appearances, i.e. images, to show at different viewpoints. Our 3D&nbsp;editor then calculates the corresponding lens placements and underlying color pattern. On export, the user can use ray tracing to live preview the resulting appearance from each angle. The 3D model, color pattern, and lenses are then 3D printed in one pass on a multi-material 3D printer to create the final 3D object. To determine the best fabrication parameters for 3D printing lenses, we printed lenses of different sizes and tested various post-processing techniques. To support a large number of different appearances, we compute the lens geometry that has the best trade-off between the number of viewpoints and the protrusion from the object geometry. Finally, we demonstrate our system in practice with a range of use cases for which we show the simulated and physical results side by side.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {1184–1196},
numpages = {13},
keywords = {design tools., lenticular lenses, multi-material 3D printing, optics},
location = {Virtual Event, USA},
series = {UIST '21}
}

@article{caustics,
author = {Schwartzburg, Yuliy and Testuz, Romain and Tagliasacchi, Andrea and Pauly, Mark},
title = {High-contrast computational caustic design},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2601097.2601200},
doi = {10.1145/2601097.2601200},
abstract = {We present a new algorithm for computational caustic design. Our algorithm solves for the shape of a transparent object such that the refracted light paints a desired caustic image on a receiver screen. We introduce an optimal transport formulation to establish a correspondence between the input geometry and the unknown target shape. A subsequent 3D optimization based on an adaptive discretization scheme then finds the target surface from the correspondence map. Our approach supports piecewise smooth surfaces and non-bijective mappings, which eliminates a number of shortcomings of previous methods. This leads to a significantly richer space of caustic images, including smooth transitions, singularities of infinite light density, and completely black areas. We demonstrate the effectiveness of our approach with several simulated and fabricated examples.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {74},
numpages = {11},
keywords = {3D optimization, caustics, computational design, inverse surface design}
}

@misc{sun2024endtoendsurfaceoptimizationlight,
      title={End-to-end Surface Optimization for Light Control}, 
      author={Yuou Sun and Bailin Deng and Juyong Zhang},
      year={2024},
      eprint={2408.13117},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2408.13117}, 
}

@inproceedings{PrintableSurface,
author = {Perroni-Scharf, Maxine and Rusinkiewicz, Szymon},
title = {Constructing Printable Surfaces with View-Dependent Appearance},
year = {2023},
isbn = {9798400701597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588432.3591526},
doi = {10.1145/3588432.3591526},
abstract = {We present a method for the digital fabrication of surfaces whose appearance varies based on viewing direction. The surfaces are constructed from a mesh of bars arranged in a self-occluding colored heightfield that creates the desired view-dependent effects. At the heart of our method is a novel and simple differentiable rendering algorithm specifically designed to render colored 3D heightfields and enable efficient calculation of the gradient of appearance with respect to heights and colors. This algorithm forms the basis of a coarse-to-fine ML-based optimization process that adjusts the heights and colors of the strips to minimize the loss between the desired and real surface appearance from each viewpoint, deriving meshes that can then be fabricated using a 3D printer. Using our method, we demonstrate both synthetic and real-world fabricated results with view-dependent appearance.},
booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
articleno = {51},
numpages = {10},
keywords = {Digital Fabrication, Ultraviolet Printer},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{softshadow,
booktitle = {Computational Aesthetics},
editor = {Holger Winnemoeller and Lyn Bartram},
title = {{Soft Shadow Art}},
author = {Min, Sehee and Lee, Jaedong and Won, Jungdam and Lee, Jehee},
year = {2017},
publisher = {Association for Computing Machinery, Inc (ACM)},
ISSN = {1816-0859},
ISBN = {978-1-4503-5080-8},
DOI = {10.1145/3092912.3092915}
}

@article{PFS*19,
  title={DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation},
  author={Jeong Joon Park and Peter R. Florence and Julian Straub and Richard A. Newcombe and S. Lovegrove},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={165-174},
  url={https://api.semanticscholar.org/CorpusID:58007025}
}

@article{MST*21,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{WLL*21,
  title={Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction},
  author={Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  journal={arXiv preprint arXiv:2106.10689},
  year={2021}
}

@inproceedings{LLL*23,
  title={Neuraludf: Learning unsigned distance fields for multi-view reconstruction of surfaces with arbitrary topologies},
  author={Long, Xiaoxiao and Lin, Cheng and Liu, Lingjie and Liu, Yuan and Wang, Peng and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20834--20843},
  year={2023}
}

@InProceedings{MON*19,
author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{3DGauss,
author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkuehler, Thomas and Drettakis, George},
title = {3D Gaussian Splatting for Real-Time Radiance Field Rendering},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592433},
doi = {10.1145/3592433},
abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {139},
numpages = {14},
keywords = {novel view synthesis, radiance fields, 3D gaussians, real-time rendering}
}

@inproceedings{neus2,
title={NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction}, 
author={Wang, Yiming and Han, Qin and Habermann, Marc and Daniilidis, Kostas and Theobalt, Christian and Liu, Lingjie},
year={2023},
booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}
}

@inproceedings{li2023neuralangelo,
  title={Neuralangelo: High-Fidelity Neural Surface Reconstruction},
  author={Li, Zhaoshuo and M\"uller, Thomas and Evans, Alex and Taylor, Russell H and Unberath, Mathias and Liu, Ming-Yu and Lin, Chen-Hsuan},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},
  year={2023}
}

@InProceedings{Niemeyer2021Regnerf,
author    = {Michael Niemeyer and Jonathan T. Barron and Ben Mildenhall and Mehdi S. M. Sajjadi and Andreas Geiger and Noha Radwan},  
title     = {RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs},
booktitle = {Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
year      = {2022},
}

@inproceedings{yu2020pixelnerf,
      title={{pixelNeRF}: Neural Radiance Fields from One or Few Images},
      author={Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},
      year={2021},
      booktitle={CVPR},
}

@InProceedings{Jain_2021_ICCV,
  author = {Jain, Ajay and Tancik, Matthew and Abbeel, Pieter},
  title = {Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month = {October},
  year = {2021},
  pages = {5885-5894}
}

@InProceedings{Yang2023FreeNeRF,
    author    = {Jiawei Yang and Marco Pavone and Yue Wang}, 
    title     = {FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization},
    booktitle = {Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2023},
}

@inproceedings{wynn-2023-diffusionerf,
 title   = {DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models},
 author  = {Jamie Wynn and
            Daniyar Turmukhambetov
           },
 booktitle = {CVPR},
 year = {2023}
}

@misc{liu2023zero1to3,
      title={Zero-1-to-3: Zero-shot One Image to 3D Object}, 
      author={Ruoshi Liu and Rundi Wu and Basile Van Hoorick and Pavel Tokmakov and Sergey Zakharov and Carl Vondrick},
      year={2023},
      eprint={2303.11328},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{poole2022dreamfusion,
  author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  title = {DreamFusion: Text-to-3D using 2D Diffusion},
  journal = {arXiv},
  year = {2022},
}

@article{wang2023prolificdreamer,
      title={ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation},
      author={Zhengyi Wang and Cheng Lu and Yikai Wang and Fan Bao and Chongxuan Li and Hang Su and Jun Zhu},
      journal={arXiv preprint arXiv:2305.16213},
      year={2023}
}

@inproceedings{lin2023magic3d,
  title={Magic3D: High-Resolution Text-to-3D Content Creation},
  author={Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},
  year={2023}
}

@InProceedings{chen2023fantasia3d,
  author={Chen, Rui and Chen, Yongwei and Jiao, Ningxin and Jia, Kui},
  title={Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month={October},
  year={2023},
  pages={22246-22256}
}

@INPROCEEDINGS{kilonerf,
    author = {Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger},
    title = {KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs},
    booktitle = {International Conference on Computer Vision (ICCV)},
    year = {2021}
}

@article{hedman2021snerg,
    title={Baking Neural Radiance Fields for
           Real-Time View Synthesis},
    author={Peter Hedman and Pratul P. Srinivasan and
            Ben Mildenhall and Jonathan T. Barron and
            Paul Debevec},
    journal={ICCV},
    year={2021}
}

@article{merf,
    title={MERF: Memory-Efficient Radiance Fields for
        Real-time View Synthesis in Unbounded Scenes},
    author={Christian Reiser and Richard Szeliski and 
        Dor Verbin and Pratul P. Srinivasan and Ben Mildenhall and Andreas Geiger and Jonathan T. Barron and Peter Hedman},
    journal={SIGGRAPH},
    year={2023}
}

@inproceedings{Instant3D,
author = {Li, Sixu and Li, Chaojian and Zhu, Wenbo and Yu, Boyang (Tony) and Zhao, Yang (Katie) and Wan, Cheng and You, Haoran and Shi, Huihong and Lin, Yingyan (Celine)},
title = {Instant-3D: Instant Neural Radiance Field Training Towards On-Device AR/VR 3D Reconstruction},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589115},
doi = {10.1145/3579371.3589115},
abstract = {Neural Radiance Field (NeRF) based 3D reconstruction is highly desirable for immersive Augmented and Virtual Reality (AR/VR) applications, but achieving instant (i.e., < 5 seconds) on-device NeRF training remains a challenge. In this work, we first identify the inefficiency bottleneck: the need to interpolate NeRF embeddings up to 200,000 times from a 3D embedding grid during each training iteration. To alleviate this, we propose Instant-3D, an algorithm-hardware co-design acceleration framework that achieves instant on-device NeRF training. Our algorithm decomposes the embedding grid representation in terms of color and density, enabling computational redundancy to be squeezed out by adopting different (1) grid sizes and (2) update frequencies for the color and density branches. Our hardware accelerator further reduces the dominant memory accesses for embedding grid interpolation by (1) mapping multiple nearby points' memory read requests into one during the feed-forward process, (2) merging embedding grid updates from the same sliding time window during back-propagation, and (3) fusing different computation cores to support the different grid sizes needed by the color and density branches of Instant-3D algorithm. Extensive experiments validate the effectiveness of Instant-3D, achieving a large training time reduction of 41\texttimes{} - 248\texttimes{} while maintaining the same reconstruction quality. Excitingly, Instant-3D has enabled instant 3D reconstruction for AR/VR, requiring a reconstruction time of only 1.6 seconds per scene and meeting the AR/VR power consumption constraint of 1.9 W.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {6},
numpages = {13},
keywords = {hardware accelerator, neural radiance field (NeRF)},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@article{instant3d2023,
    author = {Jiahao Li and Hao Tan and Kai Zhang and Zexiang Xu and Fujun Luan and Yinghao Xu and Yicong Hong and Kalyan Sunkavalli and Greg Shakhnarovich and Sai Bi},
    title = {Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model},
    journal = {https://arxiv.org/abs/2311.06214},
    year = {2023},
}

@article{Yu2023MipSplatting,
  author    = {Yu, Zehao and Chen, Anpei and Huang, Binbin and Sattler, Torsten and Geiger, Andreas},
  title     = {Mip-Splatting: Alias-free 3D Gaussian Splatting},
  journal   = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2024},
}

@INPROCEEDINGS {mssplatting,
author = { Yan, Zhiwen and Low, Weng Fei and Chen, Yu and Lee, Gim Hee },
booktitle = { 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering }},
year = {2024},
volume = {},
ISSN = {},
pages = {20923-20931},
abstract = { 3D Gaussians have recently emerged as a highly efficient representation for 3D reconstruction and rendering. Despite its high rendering quality and speed at high resolutions, they both deteriorate drastically when rendered at lower resolutions or from far away camera position. During low resolution or far away rendering, the pixel size of the image can fall below the Nyquist frequency compared to the screen size of each splatted 3D Gaussian and leads to aliasing effect. The rendering is also drastically slowed down by the sequential alpha blending of more splatted Gaussians per pixel. To address these issues, we propose a multi-scale 3D Gaussian splatting algorithm, which maintains Gaussians at different scales to represent the same scene. Higher-resolution images are rendered with more small Gaussians, and lower-resolution images are rendered with fewer larger Gaussians. With similar training time, our algorithm can achieve 13%-66% PSNR and 160%-2400% rendering speed improvement at 4 × -128 × scale rendering on Mip-NeRF360 dataset compared to the single scale 3D Gaussian splatting. More results and code are released on our project page. },
keywords = {Training;Degradation;Computer vision;Three-dimensional displays;Image resolution;Codes;Filtering algorithms},
doi = {10.1109/CVPR52733.2024.01977},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.01977},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@inproceedings{scaffoldgs,
  title={Scaffold-gs: Structured 3d gaussians for view-adaptive rendering},
  author={Lu, Tao and Yu, Mulin and Xu, Linning and Xiangli, Yuanbo and Wang, Limin and Lin, Dahua and Dai, Bo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20654--20664},
  year={2024}
}

@inproceedings{dynamicgs,
  title={Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis},
  author={Luiten, Jonathon and Kopanas, Georgios and Leibe, Bastian and Ramanan, Deva},
  booktitle={3DV},
  year={2024}
}

@InProceedings{CFGS,
    author    = {Fu, Yang and Liu, Sifei and Kulkarni, Amey and Kautz, Jan and Efros, Alexei A. and Wang, Xiaolong},
    title     = {COLMAP-Free 3D Gaussian Splatting},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {20796-20805}
}

@misc{zhu2023FSGS, 
title={FSGS: Real-Time Few-Shot View Synthesis using Gaussian Splatting}, 
author={Zehao Zhu and Zhiwen Fan and Yifan Jiang and Zhangyang Wang}, 
year={2023},
eprint={2312.00451},
archivePrefix={arXiv},
primaryClass={cs.CV} 
}

@article{guedon2023sugar,
    title={SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering},
    author={Gu{\'e}don, Antoine and Lepetit, Vincent},
    journal={CVPR},
    year={2024}
}

@article{tang2023dreamgaussian,
  title={DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation},
  author={Tang, Jiaxiang and Ren, Jiawei and Zhou, Hang and Liu, Ziwei and Zeng, Gang},
  journal={arXiv preprint arXiv:2309.16653},
  year={2023}
}

@article{ho2020denoising,
  title={Denoising Diffusion Probabilistic Models},
  author={Jonathan Ho and Ajay Jain and Pieter Abbeel},
  year={2020},
  journal={arXiv preprint arxiv:2006.11239}
}

@INPROCEEDINGS {LatentDiffusion,
author = { Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn },
booktitle = { 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ High-Resolution Image Synthesis with Latent Diffusion Models }},
year = {2022},
volume = {},
ISSN = {},
pages = {10674-10685},
abstract = { By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. },
keywords = {Training;Visualization;Image synthesis;Computational modeling;Noise reduction;Superresolution;Process control},
doi = {10.1109/CVPR52688.2022.01042},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52688.2022.01042},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@misc{DALLE,
      title={Hierarchical Text-Conditional Image Generation with CLIP Latents}, 
      author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
      year={2022},
      eprint={2204.06125},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.06125}, 
}

@inproceedings{Photorealistic,
author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Lit, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Gontijo-Lopes, Raphael and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
title = {Photorealistic text-to-image diffusion models with deep language understanding},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, GLIDE and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2643},
numpages = {16},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@article{jain2021dreamfields,
  author = {Jain, Ajay and Mildenhall, Ben and Barron, Jonathan T. and Abbeel, Pieter and Poole, Ben},
  title = {Zero-Shot Text-Guided Object Generation with Dream Fields},
  journal = {CVPR},
  year = {2022},
}

@InProceedings{MAkeit3D,
    author    = {Tang, Junshu and Wang, Tengfei and Zhang, Bo and Zhang, Ting and Yi, Ran and Ma, Lizhuang and Chen, Dong},
    title     = {Make-It-3D: High-fidelity 3D Creation from A Single Image with Diffusion Prior},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {22819-22829}
}

@article{objaverse,
  title={Objaverse: A Universe of Annotated 3D Objects},
  author={Matt Deitke and Dustin Schwenk and Jordi Salvador and Luca Weihs and
          Oscar Michel and Eli VanderBilt and Ludwig Schmidt and
          Kiana Ehsani and Aniruddha Kembhavi and Ali Farhadi},
  journal={arXiv preprint arXiv:2212.08051},
  year={2022}
}

@article{objaverseXL,
  title={Objaverse-XL: A Universe of 10M+ 3D Objects},
  author={Matt Deitke and Ruoshi Liu and Matthew Wallingford and Huong Ngo and
          Oscar Michel and Aditya Kusupati and Alan Fan and Christian Laforte and
          Vikram Voleti and Samir Yitzhak Gadre and Eli VanderBilt and
          Aniruddha Kembhavi and Carl Vondrick and Georgia Gkioxari and
          Kiana Ehsani and Ludwig Schmidt and Ali Farhadi},
  journal={arXiv preprint arXiv:2307.05663},
  year={2023}
}

@inproceedings{wu2023omniobject3d,
    author = {Tong Wu and Jiarui Zhang and Xiao Fu and Yuxin Wang and Jiawei Ren, 
    Liang Pan and Wayne Wu and Lei Yang and Jiaqi Wang and Chen Qian and Dahua Lin and Ziwei Liu},
    title = {OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, 
    Reconstruction and Generation},
    booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2023}
}

@InProceedings{ABC,
author = {Koch, Sebastian and Matveev, Albert and Jiang, Zhongshi and Williams, Francis and Artemov, Alexey and Burnaev, Evgeny and Alexa, Marc and Zorin, Denis and Panozzo, Daniele},
title = {ABC: A Big CAD Model Dataset For Geometric Deep Learning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{sweetdreamer,
author    = {Weiyu Li and Rui Chen and Xuelin Chen and Ping Tan},
title     = {SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D},
journal   = {arxiv:2310.02596},
year      = {2023},
}

@inproceedings{qiu2024richdreamer,
  title={Richdreamer: A generalizable normal-depth diffusion model for detail richness in text-to-3d},
  author={Qiu, Lingteng and Chen, Guanying and Gu, Xiaodong and Zuo, Qi and Xu, Mutian and Wu, Yushuang and Yuan, Weihao and Dong, Zilong and Bo, Liefeng and Han, Xiaoguang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9914--9925},
  year={2024}
}

@article{long2023wonder3d,
  title={Wonder3D: Single Image to 3D using Cross-Domain Diffusion},
  author={Long, Xiaoxiao and Guo, Yuan-Chen and Lin, Cheng and Liu, Yuan and Dou, Zhiyang and Liu, Lingjie and Ma, Yuexin and Zhang, Song-Hai and Habermann, Marc and Theobalt, Christian and others},
  journal={arXiv preprint arXiv:2310.15008},
  year={2023}
}

@article{wang2024crm,
  title={CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model},
  author={Zhengyi Wang and Yikai Wang and Yifei Chen and Chendong Xiang and Shuo Chen and Dajiang Yu and Chongxuan Li and Hang Su and Jun Zhu},
  journal={arXiv preprint arXiv:2403.05034},
  year={2024}
}

@article{shi2023MVDream,
  author = {Shi, Yichun and Wang, Peng and Ye, Jianglong and Mai, Long and Li, Kejie and Yang, Xiao},
  title = {MVDream: Multi-view Diffusion for 3D Generation},
  journal = {arXiv:2308.16512},
  year = {2023},
}

@InProceedings{dust3r,
    author    = {Wang, Shuzhe and Leroy, Vincent and Cabon, Yohann and Chidlovskii, Boris and Revaud, Jerome},
    title     = {DUSt3R: Geometric 3D Vision Made Easy},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {20697-20709}
}

@article{sun2024dimensionx,
title={DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion},
author={Sun, Wenqiang and Chen, Shuo and Liu, Fangfu and Chen, Zilong and Duan, Yueqi and Zhang, Jun and Wang, Yikai},
journal={arXiv preprint arXiv:2411.04928},
year={2024}
}

@InProceedings{SUPIR,
    author    = {Fanghua Yu and Jinjin Gu and Zheyuan Li and Jinfan Hu and Xiangtao Kong and Xintao Wang and Jingwen He and Yu Qiao and Chao Dong},
    title     = {Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
}

@article{depth_anything_v2,
  title={Depth Anything V2},
  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  journal={arXiv:2406.09414},
  year={2024}
}

@inproceedings{depth_anything_v1,
  title={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}, 
  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  booktitle={CVPR},
  year={2024}
}

@article{he2024lotus,
    title={Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction},
    author={He, Jing and Li, Haodong and Yin, Wei and Liang, Yixun and Li, Leheng and Zhou, Kaiqiang and Liu, Hongbo and Liu, Bingbing and Chen, Ying-Cong},
    journal={arXiv preprint arXiv:2409.18124},
    year={2024}
}
@article{ye2024stablenormal,
  title={StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal},
  author={Ye, Chongjie and Qiu, Lingteng and Gu, Xiaodong and Zuo, Qi and Wu, Yushuang and Dong, Zilong and Bo, Liefeng and Xiu, Yuliang and Han, Xiaoguang},
  journal={ACM Transactions on Graphics (TOG)},
  year={2024},
  publisher={ACM New York, NY, USA}
}
@article{trellis,
    title   = {Structured 3D Latents for Scalable and Versatile 3D Generation},
    author  = {Xiang, Jianfeng and Lv, Zelong and Xu, Sicheng and Deng, Yu and Wang, Ruicheng and Zhang, Bowen and Chen, Dong and Tong, Xin and Yang, Jiaolong},
    journal = {arXiv preprint arXiv:2412.01506},
    year    = {2024}
}

@article{tang2024lgm,
  title={LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation},
  author={Tang, Jiaxiang and Chen, Zhaoxi and Chen, Xiaokang and Wang, Tengfei and Zeng, Gang and Liu, Ziwei},
  journal={ECCV},
  year={2024}
}

@article{10.1145/37402.37422,
author = {Lorensen, William E. and Cline, Harvey E.},
title = {Marching cubes: A high resolution 3D surface construction algorithm},
year = {1987},
issue_date = {July 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {0097-8930},
url = {https://doi.org/10.1145/37402.37422},
doi = {10.1145/37402.37422},
abstract = {We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.},
journal = {SIGGRAPH Comput. Graph.},
month = aug,
pages = {163–169},
numpages = {7}
}

@inproceedings{marchingcubes,
author = {Lorensen, William E. and Cline, Harvey E.},
title = {Marching cubes: A high resolution 3D surface construction algorithm},
year = {1987},
isbn = {0897912276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/37401.37422},
doi = {10.1145/37401.37422},
booktitle = {Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {163–169},
numpages = {7},
series = {SIGGRAPH '87}
}

@misc{yu2024scaling,
  title={Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild}, 
  author={Fanghua Yu and Jinjin Gu and Zheyuan Li and Jinfan Hu and Xiangtao Kong and Xintao Wang and Jingwen He and Yu Qiao and Chao Dong},
  year={2024},
  eprint={2401.13627},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@inproceedings{dust3r_cvpr24,
      title={DUSt3R: Geometric 3D Vision Made Easy}, 
      author={Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud},
      booktitle = {CVPR},
      year = {2024}
}

@misc{stable-dreamfusion,
    Author = {Jiaxiang Tang},
    Year = {2022},
    Note = {https://github.com/ashawkey/stable-dreamfusion},
    Title = {Stable-dreamfusion: Text-to-3D with Stable-diffusion}
}

@inproceedings{shen2021dmtet,
title = {Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis},
author = {Tianchang Shen and Jun Gao and Kangxue Yin and Ming-Yu Liu and Sanja Fidler},
year = {2021},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@incollection{igr,
 author = {Gropp, Amos and Yariv, Lior and Haim, Niv and Atzmon, Matan and Lipman, Yaron},
 booktitle = {Proceedings of Machine Learning and Systems 2020},
 pages = {3569--3579},
 title = {Implicit Geometric Regularization for Learning Shapes},
 year = {2020}
}

@article{colmap1,
author = {Snavely, Noah and Seitz, Steven M. and Szeliski, Richard},
title = {Photo tourism: exploring photo collections in 3D},
year = {2006},
issue_date = {July 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1141911.1141964},
doi = {10.1145/1141911.1141964},
abstract = {We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.},
journal = {ACM Trans. Graph.},
month = jul,
pages = {835–846},
numpages = {12},
keywords = {structure from motion, photo browsing, image-based rendering, image-based modeling}
}

@INPROCEEDINGS{colmap2,
  author={Schönberger, Johannes L. and Frahm, Jan-Michael},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Structure-from-Motion Revisited}, 
  year={2016},
  volume={},
  number={},
  pages={4104-4113},
  keywords={Image reconstruction;Robustness;Cameras;Internet;Image registration;Transmission line matrix methods;Pipelines},
  doi={10.1109/CVPR.2016.445}}


@INPROCEEDINGS{SFM,
  author={Moulon, Pierre and Monasse, Pascal and Marlet, Renaud},
  booktitle={2013 IEEE International Conference on Computer Vision}, 
  title={Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion}, 
  year={2013},
  volume={},
  number={},
  pages={3248-3255},
  keywords={Cameras;Robustness;Tensile stress;Estimation;Pipelines;Three-dimensional displays;Accuracy;Calibration;Structure-from-Motion;robust estimation},
  doi={10.1109/ICCV.2013.403}}

@INPROCEEDINGS{MVS,
  author={Goesele, Michael and Snavely, Noah and Curless, Brian and Hoppe, Hugues and Seitz, Steven M.},
  booktitle={2007 IEEE 11th International Conference on Computer Vision}, 
  title={Multi-View Stereo for Community Photo Collections}, 
  year={2007},
  volume={},
  number={},
  pages={1-8},
  keywords={Image reconstruction;Surface reconstruction;Layout;Stereo image processing;Internet;Robustness;Cameras;Iterative algorithms;Image sampling;Image resolution},
  doi={10.1109/ICCV.2007.4408933}}

@INPROCEEDINGS{kfusion,
  author={Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
  booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, 
  title={KinectFusion: Real-time dense surface mapping and tracking}, 
  year={2011},
  volume={},
  number={},
  pages={127-136},
  keywords={Surface reconstruction;Cameras;Image reconstruction;Real time systems;Simultaneous localization and mapping;Iterative closest point algorithm;Three dimensional displays;Real-Time;Dense Reconstruction;Tracking;GPU;SLAM;Depth Cameras;Volumetric Representation;AR},
  doi={10.1109/ISMAR.2011.6092378}}

@article{bfusion,
author = {Dai, Angela and Nie\ss{}ner, Matthias and Zollh\"{o}fer, Michael and Izadi, Shahram and Theobalt, Christian},
title = {BundleFusion: Real-Time Globally Consistent 3D Reconstruction Using On-the-Fly Surface Reintegration},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/3054739},
doi = {10.1145/3054739},
abstract = {Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed reality and robotic applications. However, scalability brings challenges of drift in pose estimation, introducing significant errors in the accumulated model. Approaches often require hours of offline processing to globally correct model errors. Recent online methods demonstrate compelling results but suffer from (1) needing minutes to perform online correction, preventing true real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation, resulting in many tracking failures; or (3) supporting only unstructured point-based representations, which limit scan quality and applicability. We systematically address these issues with a novel, real-time, end-to-end reconstruction framework. At its core is a robust pose estimation strategy, optimizing per frame for a global set of camera poses by considering the complete history of RGB-D input with an efficient hierarchical approach. We remove the heavy reliance on temporal tracking and continually localize to the globally optimized frames instead. We contribute a parallelizable optimization framework, which employs correspondences based on sparse features and dense geometric and photometric matching. Our approach estimates globally optimized (i.e., bundle adjusted) poses in real time, supports robust tracking with recovery from gross tracking failures (i.e., relocalization), and re-estimates the 3D model in real time to ensure global consistency, all within a single framework. Our approach outperforms state-of-the-art online systems with quality on par to offline methods, but with unprecedented speed and scan completeness. Our framework leads to a comprehensive online scanning solution for large indoor environments, enabling ease of use and high-quality results.1},
journal = {ACM Trans. Graph.},
month = may,
articleno = {24},
numpages = {18},
keywords = {RGB-D, global consistency, real-time, scalable, scan}
}

@article{yu2024viewcrafter,
    title={ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis},
    author={Yu, Wangbo and Xing, Jinbo and Yuan, Li and Hu, Wenbo and Li, Xiaoyu and Huang, Zhipeng and Gao, Xiangjun and Wong, Tien-Tsin and Shan, Ying and Tian, Yonghong},
    journal={arXiv preprint arXiv:2409.02048},
    year={2024}
  }

@InProceedings{pd,
  author    = {Jianyuan Wang and Christian Rupprecht and David Novotny},
  title     = {{PoseDiffusion}: Solving Pose Estimation via Diffusion-aided Bundle Adjustment},
  journal   = {ICCV},
  year      = {2023}
}

@misc{lin2024diffbir,
      title={DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior}, 
      author={Xinqi Lin and Jingwen He and Ziyan Chen and Zhaoyang Lyu and Bo Dai and Fanghua Yu and Wanli Ouyang and Yu Qiao and Chao Dong},
      year={2024},
      eprint={2308.15070},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{yang2023pasd,
    title={Pixel-Aware Stable Diffusion for Realistic Image Super-Resolution and Personalized Stylization},
    author={Tao Yang, Rongyuan Wu, Peiran Ren, Xuansong Xie, and Lei Zhang},
    booktitle={The European Conference on Computer Vision (ECCV) 2024},
    year={2023}
}

@article{wang2024exploiting,
  author = {Wang, Jianyi and Yue, Zongsheng and Zhou, Shangchen and Chan, Kelvin C.K. and Loy, Chen Change},
  title = {Exploiting Diffusion Prior for Real-World Image Super-Resolution},
  article = {International Journal of Computer Vision},
  year = {2024}
}

@inproceedings{sd35,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{voleti2024sv3d,
  author    = {Voleti, Vikram and Yao, Chun-Han and Boss, Mark and Letts, Adam and Pankratz, David and Tochilkin,  Dmitrii and Laforte, Christian and Rombach, Robin and Jampani, Varun},
  title     = {{SV3D}: Novel Multi-view Synthesis and {3D} Generation from a Single Image using Latent Video Diffusion},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2024},
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{bini2022cao,
  title={Bilateral Normal Integration},
  author={Cao, Xu and Santo, Hiroaki and Shi, Boxin and Okura, Fumio and Matsushita, Yasuyuki},
  booktitle=ECCV,
  year={2022}
}

@INPROCEEDINGS{Kadambi15,
  author={Kadambi, Achuta and Taamazyan, Vage and Shi, Boxin and Raskar, Ramesh},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Polarized 3D: High-Quality Depth Sensing with Polarization Cues}, 
  year={2015},
  volume={},
  number={},
  pages={3370-3378},
  keywords={Shape;Three-dimensional displays;Robustness;Sensors;Cameras;Azimuth;Lighting},
  doi={10.1109/ICCV.2015.385}}



