\section{Introduction}
Reconstructing 3D geometry from given information is a fundamental problem in computer graphics and computer vision, with broad applications in areas such as cultural heritage digitization, film, and architecture. Conventional ``Shape from X'' methods typically rely on specific visual data such as RGB images, depth maps or surface normals to reconstruct 3D geometry and renderings that closely match the target. However, while these inputs provide sufficient information for accurate reconstruction, they introduce strict constraints that hinder the generation of imaginative and novel 3D assets, which are critical for applications in AR/VR and art.

To address these limitations, we propose a novel ``Shape from Semantics'' problem that utilizes textual descriptions, or semantics, to guide the generation of 3D geometry. In this context, semantics refer to high-level, human-interpretable concepts or attributes that describe the desired appearance or characteristics of an object. By using semantics as input, our approach enables creating 3D models that convey the intended visual properties from multiple viewpoints, introducing a new level of flexibility and creativity to the ``Shape from X'' reconstruction paradigm. Unlike traditional methods that aim to reconstruct a specific object based on captured visual data, our approach allows users to generate 3D models that match a broader set of desired attributes described by the input semantics. This expands the design space and empowers users to create imaginative and compelling 3D objects that align with their creative intent. The resulting 3D models can be directly observed from different views, providing a more intuitive and immersive experience compared to 2D designs or projections. Moreover, semantics-based operations are inherently more user-friendly, allowing users to generate intricate textures and detailed meshes with just a few text prompts. This significantly lowers the barrier to artistic creation, enabling non-professionals to produce stunning works of art. 


Our proposed problem is non-trivial because we aim to match the shape geometry and appearance from different viewpoints with input semantics rather than specific input images, making existing multi-view reconstruction approaches inapplicable. Moreover, current text-to-3D generation models are not suitable either, as they typically use a single prompt to describe a single object, while our approach uses multiple input prompts that describe different objects for the appearance of the shape in different view directions. 
The most similar current research to ours explores utilizing information such as shadows and contours as the basis for reconstruction and design. For instance, Shadow Art~\cite{MP09} aims to create objects whose projections under specific lighting conditions match the given constraints. Wire Art~\cite{Hsiao:2018:MVWA:,qu2023dreamwire,Fabricable3DWireArt} focuses on generating wireframe geometries that align with the input 2D line drawings or that can outline shapes consistent with the provided semantic inputs. However, the visual experience provided by these designs is primarily two-dimensional. When directly observing the objects, it is often challenging to perceive the embedded semantic information that the designs aim to convey. Furthermore, these techniques often rely on specific setups, such as light sources and projection planes, while also facing challenges in fabrication, which limits their practical applications.


To solve the novel problem proposed above, our key idea is to utilize the image- and text-understanding capabilities of generative models to create a 3D shape that matches the input semantics when observed from different directions. 
Our structure-to-detail approach is divided into three stages. 
In the first stage, we utilize a semantic-guided diffusion model prior~\cite{poole2022dreamfusion,tang2023dreamgaussian} to construct coarse geometry and texture represented as 3D Gaussian Splatting (3DGS)~\cite{3DGauss}. We employ multi-semantics Score Distillation Sampling (SDS) to distill 3D geometry and appearance from 2D diffusion models, ensuring that the initial shape aligns with the semantic input. In the second stage, we refine the generated textures using an image diffusion model~\cite{SUPIR}. At the same time, we employ a video diffusion model~\cite{sun2024dimensionx} to estimate rendering results from various satellite views, which provide priors for training neural implicit representations in the subsequent stage. This multi-view approach helps ensure that the model's geometry is consistent and perceptually coherent from different views. Finally, in the third stage, we represent the refined 3D model using neural implicit representations of Signed Distance Functions (SDF)~\cite{PFS*19,WLL*21}. The training of this SDF model is guided by the coarse 3DGS geometry masks and the detailed texture maps derived from satellite views. This process enables the neural implicit representation to capture both the final shape and texture of the object. From this representation, we extract a fabricable high-quality mesh, which preserves the geometry, textures, and smooth transitions required for practical use.


Our results demonstrate that specific viewpoints of the same object effectively integrate multiple semantic elements, which are easily discernible to observers. The models produced by our method exhibit a high degree of creativity, often surpassing what can be achieved through traditional spatial intuition or non-semantic inputs. The final high-quality meshes feature intricate details, well-constructed geometry, cohesive textures, and smooth transitions, resulting in visually engaging and compelling designs. Moreover, these models are manufacturable, significantly enhancing their practical applicability and expanding their potential for real-world use.
In summary, our contributions include:
\begin{itemize}[leftmargin=*]
    \item We introduce the ``Shape from Semantics'' problem, where semantic input guides the generation and design of 3D models with perceptual 3D characteristics. Unlike traditional ``Shape from X'' approaches, our focus is on creating highly imaginative and visually compelling geometric objects.
    \item We employ a range of generative models to address this problem, bridging the gap between multi-view constraints and perceptual 3D understanding. This approach opens new possibilities for generating semantically rich and visually engaging 3D designs.
    \item Our method enables the creation of high-quality meshes with detailed textures, superior geometry, intricate designs, and smooth transitionsâ€”all from just a few prompts. Furthermore, the results produced by our approach are fabricable and do not rely on specific setups, significantly enhancing their practical value and broadening their potential applications.
\end{itemize}