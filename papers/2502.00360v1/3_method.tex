\section{Method}
Our input consists of $n$ semantic labels $\mathcal{T} = \{t_i\}$ where each $t_i$ represents a textual prompt, along with their corresponding view directions $\mathcal{V} = \{v_i\}$ with $v_i\in \mathrm{SO\left(3\right)}$. We refer to $\mathcal{V}$ as the primary views, which can either be predefined or initialized randomly. Our purpose is to generate a colored 3D shape $\mathcal{S}$ such that its texture and geometry when observed from any primary view align with the associated semantic class. Formally, this can be expressed as:
\begin{equation}
%\begin{aligned}
\min_{\mathcal{S}} ~~ \mathcal{L}_{\mathrm{reg}}(\mathcal{S}), 
\qquad \text{s.t.} ~~ P(\mathcal{S}, v_{i}) \in C\left(t_{i}\right), \quad i = 1, \ldots, n,
%\end{aligned}
\label{eq:main}
\end{equation}
where $P(\mathcal{S}, v_{i})$ denotes the appearance of $\mathcal{S}$ when observed from the direction $v_{i}$, and $C\left(t_{i}\right)$ denotes the class of images consistent with the prompt $t_{i}$. $\mathcal{L}_{\mathrm{reg}}$ is a regularization loss that enforces the following conditions for $\mathcal{S}$ in addition to the semantic constraints:
\begin{itemize}[leftmargin=*]
\item \textit{Efficient Structure}: The design of $\mathcal{S}$ should be simple, intuitive, and compact to facilitate fabrication while maintaining geometric features that contribute to its appearance.
\item \textit{Natural Transitions}: The parts of $\mathcal{S}$ corresponding to each semantics should transition smoothly in both color and shape. This helps to generate aesthetically appealing results not only for the primary views but also for adjacent views. 
\item \textit{Aesthetic Appeal}: The generated shape should be highly recognizable and visually elegant.
\end{itemize}

\input{figures/method/fig_treecat}
\input{figures/pipeline}
Designing a visually appealing and semantically consistent 3D shape requires substantial creativity and imagination. Although recent advancements in generative models provide a powerful foundation for addressing this challenge, directly using such models often produces unsatisfactory results. As shown in the top row of Fig.~\ref{fig:naive_failure}, existing text-to-3D models like DreamGaussian~\cite{tang2023dreamgaussian} struggle to generate 3D shapes from text prompts that describe different semantics under different view directions. This might be because the same prompt are used to describe the orientation in supervision of different views, which makes it ambiguous. At the same time, generating additional elements in shapes does not seriously violate the semantics, making undesirable blending of all semantics into a single object a trivial solution. Alternatively, one could first generate an image for each semantic label using a text-to-image model such as Stable Diffusion 3.5~\cite{sd35}, and then use the generated images as input to create a 3D shape with multi-view reconstruction. However, as shown in the bottom row of Fig.~\ref{fig:naive_failure}, such an approach can lead to an unnatural transition of geometry between the primary view directions. This is because each image is generated independently, which fail to ensure the compatibility of their corresponding geometry on the final shape. 

We propose to gradually generate the geometry and appearance from multi-view semantics in a structure-to-detail manner in three stages with the help of generative models (see Fig.~\ref{fig:pipeline}). First, we leverage 2D generative priors to design the overall structure of the shape. Next, we use an image restoration model to improve the quality of rendered images, and use a video diffusion model to produce additional views based on the rendered results, which serve as supervision for subsequent optimization. Finally, we train a neural signed distance function that represents the refined shape with detailed geometry and texture, and extract a mesh surface from it as the final fabricable result.

\subsection{Multi-Semantics SDS based Shape Design}
\label{3_1}
Designing the structure of the shape is key to the entire task, as it directly affects the smoothness of transition between different views. To this end, we utilize generative models capable of understanding semantics. While recent 3D generation techniques can generate more realistic shapes, they are typically based on 3D consistency priors to create a globally consistent shape from a single semantic. In contrast, since our input semantics describe different objects, we need to allow for 3D inconsistencies between different views to accommodate multiple semantics in a single object. Therefore, we follow~\cite{poole2022dreamfusion, wang2023prolificdreamer} and adopt the Score Distillation Sampling (SDS) loss to supervise the generation of 3D shapes through a text-to-image model, and extend the SDS process to our multi-semantics case. Using this loss, we optimize a coarse shape represented using 3D Gaussian Splatting.

\paragraph{Multi-Semantics SDS Loss}
We choose Stable Diffusion~\cite{LatentDiffusion} as our 2D prior. In each iteration, for each semantics $t_i$, we randomly sample a camera pose $p_i$  around its primary view $v_i$ and render an RGB image $I_i$ of the current view. Then $I_i$ is input into 
the text-to-image model as the image to be denoised. For each $I_i$, we compute the gradient of original SDS loss with respect to $\Theta$, the parameters that encode $\mathcal{S}$, with semantics $t_i$:
\begin{equation}
    \begin{aligned}
        \nabla_{\Theta} \mathcal{L}_{\mathrm{SDS}}(t_i)= \mathbb{E}_{t,  \epsilon}\left[w(t)\left(\epsilon_{\phi}\left(I_{i} ; t, e_i\right)-\epsilon\right) \frac{\partial I_{i}}{\partial \Theta}\right],
    \end{aligned}
    \label{eq:SDS}
\end{equation}
where $\epsilon_{\phi}$ and $\epsilon$ are the predicted and added noise respectively, and $e_i$ is the CLIP embeddings of $t_i$.
Note that our sampled camera poses $\{p_i\}$ can deviate from the primary views, which brings two advantages. First, it ensures that geometric shapes maintain their intended semantics even when not strictly observed from the primary view directions to make our generated geometries without texture should also look meaningful. Moreover, expanding the camera's selection range allows capturing more overlapping regions between different semantics in $\mathcal{S}$ during rendering, thereby enabling the SDS process to facilitate natural transitions between adjacent parts better. Then, the gradient of multi-semantics SDS loss is defined as the average loss under different semantics: 
\begin{equation}
    \begin{aligned}
        \nabla_{\Theta} \mathcal{L}_{\mathrm{SDS}}(\mathcal{S})= \frac{1}{n}\sum\nolimits^n_{i=1}\nabla_{\Theta} \mathcal{L}_{\mathrm{SDS}}(t_i).
    \end{aligned}
    \label{eq:multiSDS}
\end{equation}

\textit{3D Gaussian Splatting Representation. }
3D Gaussian Splatting (3DGS)~\cite{3DGauss, tang2023dreamgaussian} is an efficient 3D modeling method that represents scenes as a collection of Gaussian primitives. Each Gaussian primitive represents a 3D Gaussian distribution and is represented with the following components: the center $\mathbf{x}\in\mathbb{R}^{3}$, a scaling factor $\mathbf{s} \in \mathbb{R}^{3}$, a rotation quaternion $\mathbf{q} \in \mathbb{R}^{4}$, an opacity value $\alpha \in \mathbb{R}$ and a color feature $\mathbf{c} \in \mathbb{R}^{3}$. To ensure that the shape color is independent of views in the final fabrication, spherical harmonics are disabled in our task. Therefore, the model parameter $\Theta$ can be presented as: 
\begin{equation}
    \Theta=\{\Theta_{k}\}=\left\{\mathbf{x}_{k}, \mathbf{s}_{k}, \mathbf{q}_{k}, \alpha_{k}, \mathbf{c}_{k}\right\}.
\end{equation}
To render 3D Gaussians into images, the primitives are first projected onto the image plane. Then, volume rendering is applied to each pixel in front-to-back order based on depth to compute the final color and alpha values.
Compared to other representations, the results generated with 3DGS guided by SDS capture semantic information more effectively, render faster, and exhibit fewer failure cases. We observe that regions of different semantics within the same shape tend to adopt similar color styles (see Fig.~\ref{fig:pipeline}, the green tortoise and hare), which facilitates smooth transitions between regions. 
Moreover, the unstructured nature of 3DGS makes it more flexible in representing 3D shapes with different topologies, which is highly suitable for our creative task.
To enforce the structure efficiency required in Eq.~\eqref{eq:main}, we penalize the standard deviation $\sigma(\cdot)$ of the depth value $d_i(\cdot)$ for the Gaussian centers $\{\mathbf{x}_k\}$ under each primary view $v_i$ to keep the overall volume compact:
\begin{equation}
% \mathcal{L}_{\mathrm{reg}}=\frac{1}{n}\sum^n_{i=1}\sigma(\{d_i\left(x_j\right)\}).
% \mathcal{L}_{\mathrm{reg}}=\sum_i \sigma(\{d_i\left(\textbf{x}_k\right)\}).
\mathcal{L}_{\mathrm{reg}}=\sum\nolimits_i \sigma(\{d_i\left(\mathbf{x}_k\right)\}),
\end{equation}
The total loss is:
\begin{equation}
\mathcal{L}=\mathcal{L}_{\mathrm{SDS}}+\lambda\mathcal{L}_{\mathrm{reg}}.
\end{equation}
At this stage, we have obtained a shape that generally meets the requirements of Eq.~\eqref{eq:main}. However, it still lacks sufficient details. Therefore, we subsequently use more generative models to introduce additional details based on the 3DGS results, which in turn guide the optimization of a more refined shape.



\subsection{Generation of Detailed and Multi-View Supervision}
\label{3_2}
We augment the details based on Gaussian renderings from $n$ primary views, as they best represent the visuals and geometric features of $\mathcal{S}$. These renderings currently have certain limitations: insufficient image clarity and a limited number of views. Therefore, we utilize corresponding generative models to address these issues.

\paragraph{Improving Resolution through Image Restoration} 
Although the rendering of Gaussians in the first stage is blurry, it guides the overall shape structure and how different parts merge together. 
Therefore, it is necessary to refine the rendering while maintaining the original image structure and color style. To achieve this goal, we adopt the image restoration work SUPIR~\cite{SUPIR}. We input the Gaussian rendering images from the primary views into the model to obtain the restored images, and provide semantic guidance in this process to ensure that the result makes sense.

\begin{figure}[t]
    % \raggedleft
    \centering
    \includegraphics[width=0.45\textwidth]{figures/method/SUPIR2.pdf}
    \caption{\textbf{Example for Image Restoration.} Prompt: "A detailed waterfall".}
    \label{fig:SUPIR}
\end{figure}

\paragraph{Generating Satellite-view Images} 
The generated high-quality images are insufficient to provide comprehensive constraints. Thus, alternative methods are required to supervise the geometry of various semantics effectively. A straightforward approach is leveraging depth or normal estimation~\cite{depth_anything_v1,depth_anything_v2,ye2024stablenormal,he2024lotus} or employing image-to-3D model techniques~\cite{tang2024lgm,trellis}. However, due to the property of our task, geometry in a specific view must consider the influence of surrounding semantics and make certain deformations to adapt accordingly. Consequently, constraining geometry one view at a time proves overly rigid and may lead to geometric inconsistencies. 

Compared to direct supervision methods, a more flexible alternative is to use images captured near the primary views for supervision. This approach offers the advantage of adjustable supervision scope, achieved by controlling the viewing angles of the capturing camera. Additionally, supervision intensity can be reduced for areas farther from the primary viewpoint by adding weights, enabling compatibility across different semantic geometries. We employ DimensionX~\cite{sun2024dimensionx} to accomplish this task. The restored images are fed into the model, guided by semantics, to generate videos simulating camera movement around the primary views. Subsequently, Dust3R~\cite{dust3r} is used to estimate the camera parameters for each frame of the generated video. Due to the continuity between video frames, the geometry generated within a small angular range achieves better consistency than models like zero-123~\cite{liu2023zero1to3}. These surrounding views, termed as \textit{satellite views}, provide abundant geometric information when used as supervision later on, proving particularly effective for representing convex geometric structures, as illustrated in Fig.~\ref{fig:sat}.


\subsection{Reconstruction of Refined Shapes with Neural SDF}
\label{3_3}
In the final stage, we derive a more refined shape $\mathcal{S}$ under the supervision of previous results. Since the multi-view images already contain sufficient semantic information, we only consider reconstruction at this stage. We use the neural implicit SDF function $\mathcal{F}$ as the shape representation to benefit from its capability to represent detailed geometric shapes and easily calculate geometric information such as normals. To achieve high-quality rendering, we follow the SDF-based volume rendering framework~\cite{WLL*21, MST*21}, using an SDF network for geometry and an RGB network for color presentation. During rendering, we collect rays from both the primary views and satellite views, and use the volume rendering formula to calculate the color loss for each ray:
\begin{equation}
	\begin{aligned}
		\mathcal{L}_{\mathrm{rgb}} = \|\widehat{C} - C\|
		, \quad
		\widehat{C}=\sum\nolimits_{k=1}^{n} \alpha_{k} c_{k}\prod\nolimits_{l=1}^{k-1}\left(1-\alpha_{l} \right),\\
	\end{aligned}
	\label{eq:nerf}
\end{equation}
where $\alpha_{k}$ is the opacity derived from the SDF values of sample points along the ray, and $c_k$ is predicted by the color network. For satellite views, to lead to natural shape transition, we design additional weights for each satellite view light when calculating the loss:
\begin{equation}
	\begin{aligned}
		w_{i,j}=\exp\left(-\mu\sin\langle v_{i,j},v_i\rangle\right),
	\end{aligned}
\end{equation}
where $v_{i,j}$ is the $j$-th satellite view of $v_i$, and $\mu$ is a hyperparameter. In this way, images farther from the primary view will have less influence on the regions corresponding to other semantics, helping to avoid geometric incompatibility.

% \setlength{\intextsep}{2pt}
\begin{figure}[t]
    % \raggedleft
    \centering
    \includegraphics[width=0.48\textwidth]{figures/method/sat2.pdf}
    %\setlength{\abovecaptionskip}{2pt}
    \caption{\textbf{Example for Satellite Views.} During reconstruction in the third stage, the generated satellite-view images can mitigate geometric flattening.}
    \label{fig:sat}
\end{figure}

In addition, to leverage the geometric features of the first stage without introducing local inaccuracies, we sample a set of camera poses to render opacity images of 3DGS. These images were used as masks to guide the optimization of SDF, ensuring that the cumulative opacity along each ray closely aligns with the mask values. This strategy ensures the overall structure remains reasonable while allowing the SDF sufficient flexibility for detail refinement. Meanwhile, since the supervised view focuses on areas near the primary view, less captured regions often appear overly compact and deflated. To address this, we introduce a dilation loss on a random point set $P$ near the surface to make the overall geometry well-rounded:
\begin{equation}	
\mathcal{L}_{\mathrm{dil}} = \frac{1}{|P|} \sum\nolimits_{p \in P} \max\{\mathcal{F}(p), 0\}.
\end{equation}
We give it a lower weight to ensure the overall structure remains efficient. In addition, eikonal loss~\cite{igr} $\mathcal{L}_{\mathrm{eik}}$ is adopted to maintain $\|\nabla \mathcal{F}\|=1$. The regularization loss term is:
\begin{equation}
	\mathcal{L}_{\mathrm{reg}} = \gamma_1\mathcal{L}_{\mathrm{dil}} + \gamma_2\mathcal{L}_{\mathrm{eik}}.
\end{equation}
In summary, our loss in this stage is:
\begin{equation}
	\mathcal{L} = \mathcal{L}_{\mathrm{rgb}} + \alpha \mathcal{L}_{\mathrm{mask}} + \beta \mathcal{L}_{\mathrm{reg}}.
\end{equation}
%~\\\indent

In this stage, the Gaussian masks and regularization loss make the structure of $\mathcal{S}$ efficient, and the high-resolution images generated in the second stage ensure sufficient details and beautiful appearance. Since the generated supervision and the final reconstruction are loyal to the design in the first stage, the color and geometry of different parts of $\mathcal{S}$ can transition naturally and conform to the semantics under the input primary views. At this point, we get the expected shape defined in Eq.~\eqref{eq:main}. We then use the Marching Cubes algorithm~\cite{marchingcubes} to extract a mesh, and use the color network to add the color for mesh vertices. During the rendering process, in order to ensure the geometry extraction convenience, we make the color no longer related to the view directions but still related to the normals, so that the change of rendering will also lead to the change of local geometry fluctuations, thus producing a relief-sculpture-like effect. 