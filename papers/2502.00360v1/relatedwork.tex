\section{Related work}
\paragraph{Shape from X} The previous works in the ``Shape from X'' series primarily focus on high-precision reconstruction of existing objects based on known specific visual data, such as RGB images \cite{colmap1,colmap2,SFM,MVS,WLL*21}, depth \cite{kfusion,bfusion} and normals \cite{bini2022cao,Kadambi15}. 

Constructing a single fixed object with multiple visual interpretations is a widely explored topic within the "Shape from X" domain. Researchers have explored a variety of methods to achieve diverse visual perceptions, leveraging factors such as viewing distance \cite{Hybridimages}, figure-ground organization \cite{GeneratingAmbiguousFigure-GroundImages}, illumination from different directions \cite{Reliefsasimages,ShadowPix,Attenuators}, light reflections \cite{reflectors,Mirror}, viewing angles \cite{Generationofviewdependentmodels,LenticularObjects,Hsiao:2018:MVWA:,Fabricable3DWireArt,qu2023dreamwire}, and shadow casting on external planar surfaces \cite{MP09,STR22}. 
% In computational art, the essential reasons for rich visual perceptions are diverse, such as optical properties \cite{MP09,STR22,Mirror,caustics}, application of optical materials\cite{LenticularObjects,PrintableSurface}, perceptual optimization of image content\cite{Hybridimages,GeneratingAmbiguousFigure-GroundImages}, and optimization of structure\cite{Reliefsasimages,Attenuators,ShadowPix,softshadow}. 
However, the goal of the above works is to produce different 2D information perceptions based on an object, whether it is a contour \cite{Hsiao:2018:MVWA:,Fabricable3DWireArt,qu2023dreamwire}, a projection \cite{MP09,STR22}, or a picture \cite{Mirror,caustics,softshadow}. Our work differs in that it provides direct 3D information perception, allowing the characteristics of 3D objects to be experienced firsthand. To the best of our knowledge, no work has yet explored creating multiple 3D interpretations of a single object. In addition, we leverage semantics as a substitute for traditional inputs, similar to \cite{Fabricable3DWireArt} and \cite{qu2023dreamwire}, significantly expanding the creative space. 

\paragraph{3D Data Representations.} The expression of 3D data is a core topic in computer graphics and vision. Besides traditional point cloud and mesh representations, neural implicit functions~\cite{MON*19,PFS*19,WLL*21,MST*21} and 3D Gaussian Splatting (3DGS)~\cite{3DGauss} have shown great advantages in various tasks recently. 
Mildenhall et al. proposed Neural Radiance Fields (NeRF) that represent a scene with a neural implicit function guided by neural rendering~\cite{MST*21}. This method has been widely applied to multi-view reconstruction \cite{WLL*21,neus2,li2023neuralangelo}, sparse reconstruction \cite{Niemeyer2021Regnerf,yu2020pixelnerf,Jain_2021_ICCV,wynn-2023-diffusionerf,liu2023zero1to3}, and generation tasks \cite{jain2021dreamfields,poole2022dreamfusion,wang2023prolificdreamer,lin2023magic3d,chen2023fantasia3d,MAkeit3D}, thanks to its capability in representing scenes with rich details. However, its optimization process can be time-consuming and computationally intensive. Recent work on speeding up NeRF have mainly focused on improving rendering  \cite{kilonerf,hedman2021snerg,merf,Instant3D} and reconstruction speeds \cite{neus2,li2023neuralangelo}, with only a few works applied to 3D generation \cite{instant3d2023}.

At present, 3DGS \cite{3DGauss}, as a new 3D data expression, brings new possibilities for rendering \cite{Yu2023MipSplatting,mssplatting,scaffoldgs} and reconstruction problems \cite{dynamicgs,CFGS,zhu2023FSGS,guedon2023sugar} due to the fast optimization brought by flexible model design and efficient differentiable rendering framework. Furthermore, Tang et al. incorporated a generative model into 3DGS for 3D generation tasks, enabling generation of textured meshes \cite{tang2023dreamgaussian}. However, the geometry generated by 3DGS often suffers from significant detail loss, excessive surface undulations, and suboptimal mesh quality. We integrate these two promising 3D representations in our pipeline to produce innovative and manufacturable geometries.


\paragraph{Diffusion Models in Vision.} In recent years, generative models have gained widespread attention in computer vision and graphics. They have achieved remarkable success in 2D generation for a wide range of tasks, including generating detailed images from rough ones \cite{SUPIR,yang2023pasd,lin2024diffbir,wang2024exploiting} and prompts \cite{DALLE,LatentDiffusion,Photorealistic}, and creating videos from images \cite{sun2024dimensionx,yu2024viewcrafter}, estimating depths, normals \cite{depth_anything_v1, depth_anything_v2, ye2024stablenormal,he2024lotus} and viewpoints \cite{dust3r,pd}.
On the other hand, 3D generation presents unique challenges, due to the limited availability of large, high-quality 3D datasets~\cite{ABC,wu2023omniobject3d,objaverse,objaverseXL}. Recently, most 3D generation methods \cite{jain2021dreamfields,poole2022dreamfusion,wang2023prolificdreamer,lin2023magic3d,chen2023fantasia3d,MAkeit3D} utilized 2D information as supervision to guide 3D generation, using neural implicit function as the primary representation of 3D data. DreamFields \cite{jain2021dreamfields} pioneered the use of diffusion models for semantic-based 3D generation. DreamFussion~\cite{poole2022dreamfusion} introduced the score distillation sampling (SDS) loss, which leverages semantic information and 2D rendering results, and this approach has since been widely adopted. DreamGaussian~\cite{tang2023dreamgaussian} used 3DGS to represent 3D data, significantly improving generation speed. However, as these methods inherently rely on supervision from 2D rendering results, they often face challenges with multi-view inconsistency. While many existing works aim to mitigate such inconsistencies \cite{liu2023zero1to3,shi2023MVDream}, our approach leverages such potential inconsistency to generate creative objects with multiple visual interpretations. Moreover, researchers incorporate various priors (normal, depth, etc.) into 3D generation tasks to enhance the realism of models. SweetDreamer~\cite{sweetdreamer} and RichDreamer \cite{qiu2024richdreamer}~integrate canonical coordinate maps and normal-depth priors into the loss function, respectively. Meanwhile, Wonder3D \cite{long2023wonder3d} and CRM \cite{wang2024crm} directly utilize these priors to construct corresponding meshes.