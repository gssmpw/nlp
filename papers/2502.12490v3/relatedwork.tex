\section{Related Work}
Most code generation models adopt the Seq2Seq paradigm to output a continuous sequence of tokens. Recognizing the importance of structural information in code, recent approaches have begun integrating this information into input sequences to enhance code representations. For example, GraphCodeBERT~\cite{DBLP:conf/iclr/GuoRLFT0ZDSFTDC21} first incorporates code structure (\ie data flow graphs extracted from ASTs) into input during pre-training. %TreeBERT~\cite{DBLP:conf/uai/JiangZLLL21} exploits AST paths with two structure-aware pre-training objectives: tree-masked language modeling and node order prediction. 
CodeT5~\cite{DBLP:conf/emnlp/0034WJH21} leverages node type information from ASTs and introduces an identifier-aware pre-training objective. UniXcoder~\cite{DBLP:conf/acl/GuoLDW0022} incorporates ASTs through a one-to-one mapping method that transforms an AST into a sequence.

In contrast to the aforementioned pre-trained models that integrate ASTs into the input, some models~\cite{DBLP:conf/aaai/XieSGLCYW21, DBLP:conf/acl/JiangZM00H0S20} focus on learning AST structures at the decoding stage. These models adopt the Seq2Tree paradigm, generating code as the corresponding pre-order traversal of ASTs, allowing them to better capture code syntax during generation. For instance, Xie \etal ~\cite{DBLP:conf/aaai/XieSGLCYW21} leverage mutual distillation across various AST traversal strategies to enhance action prediction accuracy. Jiang \etal~\cite{DBLP:conf/acl/JiangZM00H0S20} improve the Seq2Tree paradigm by designing a context-based Branch Selector to determine optimal expansion orders for multi-branch nodes. Note that existing Seq2Tree approaches are trained from scratch (\ie they are not trained/fine-tuned from pre-trained code models).


% \yanfu {add details of two more representative work for the seq2tree paradigm}
% StructCoder\cite{DBLP:journals/corr/abs-2206-05239} utilizes the syntax tree and data flow graph of the source code and introduces two auxiliary tasks to train a structure-aware decoder. 
% In the downstream task, \citep{DBLP:conf/iwpc/WangDLZ22} leverage two encoders, one of which learns the representation from AST while the other comes from a pre-trained program model, to learn a hybrid representation as input.

Unlike previous works, our paper proposes a novel model architecture that allows the model to effectively master both paradigms, enhancing its generative capabilities through a multi-task learning strategy, a distillation strategy, and a paradigm selector. To the best of our knowledge, this is the first work to unify the Seq2Seq and Seq2Tree paradigms in a single model.
Recently, we notice the similarity between \cite{10.1145/3609437.3609465} and our study, which also realizes the complementary advantages of the two paradigms through distillation loss. However, our work further enhances the efficiency of the fusion of the two paradigms by introducing a pre-trained model, designing a shared decoder, and developing a contrastive learning selector.



\begin{figure}[t]
%\vspace{-0.5cm} 
\centering
\footnotesize
\includegraphics[width=0.51\textwidth, trim=104 135 80 145, clip]{figs/overview.pdf}
\setlength{\abovecaptionskip}{0pt}
\caption{The overall architecture of our UniGenCoder model. The token embedding matrix and linear layer for token prediction are designed for the Seq2Seq paradigm, while the action embedding matrix and linear layer for action prediction are tailored to the Seq2Tree paradigm. Note that the token embedding matrix is included in the action embedding matrix, as output actions in the Seq2Tree paradigm can be either tokens or rules. Likewise, the linear layer for token prediction is contained by the linear layer for action prediction.} % The overall architecture of our UniGenCoder model. The token embedding matrix and linear layer for token prediction are proposed for Seq2Seq paradigm, and the action embedding matrix and linear layer for action prediction are designed for the Seq2Tree paradigm. Note that the token embedding matrix is also included in the action embedding matrix, because the output actions are either tokens or rules in the Seq2Tree paradigm. Likewise, the linear layer for token prediction is also contained by the linear layer for action prediction.}
\label{fig:overview}
\vspace{-0.4cm}
\end{figure}