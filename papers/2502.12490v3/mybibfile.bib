%%% ====================================================================
%%%  BibTeX-file{
%%%     author          = "David Rhead",
%%%     version         = "1.00",
%%%     date            = "17 Feb 1990",
%%%     time            = "17:00 GMT",
%%%     filename        = "test.bib",
%%%     address         = "Cripps Computing Centre,
%%%                        University of Nottingham,
%%%                        University Park,
%%%                        Nottingham,
%%%                        NG7 2RD,
%%%                        United Kingdom",
%%%     telephone       = "+44 602 484848 Ext 2670",
%%%     FAX             = "+44 602 588138",
%%%     checksum        = "05151 839 2908 25082",
%%%     email           = "David_Rhead at uk.ac.nott.vme (JANET)",
%%%     codetable       = "ISO/ASCII",
%%%     keywords        = "bibliography, citation, references",
%%%     supported       = "no",
%%%     docstring       = "This BibTeX database file contains entries
%%%                        designed for testing whether a BibTeX style
%%%                        file lays references out as recommended by
%%%                        certain authorities.  (Note, however, that
%%%                        the BS 1629 examples are from the 1976
%%%                        edition.  The file needs updating to use
%%%                        examples from the 1989 edition instead.)
%%%
%%%                        The checksum field above contains a CRC-16
%%%                        checksum as the first value, followed by the
%%%                        equivalent of the standard UNIX wc (word
%%%                        count) utility output of lines, words, and
%%%                        characters.  This is produced by Robert
%%%                        Solovay's checksum utility.",
%%%  }
%%% ====================================================================
%% @COMMENT{Some other standard works describing conventions for citations
%%      and bibliographies}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% TreeBERT，预训练模型

@INPROCEEDINGS{8668043,
  author={White, Martin and Tufano, Michele and Martínez, Matías and Monperrus, Martin and Poshyvanyk, Denys},
  booktitle={SANER}, 
  title={Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities}, 
  year={2019},
  volume={},
  number={}}


@ARTICLE{8827954,
  author={Chen, Zimin and Kommrusch, Steve and Tufano, Michele and Pouchet, Louis-Noël and Poshyvanyk, Denys and Monperrus, Martin},
  journal={TSE}, 
  title={SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair}, 
  year={2021}}


@article{DL-survey, author = {Watson, Cody and Cooper, Nathan and Palacio, David Nader and Moran, Kevin and Poshyvanyk, Denys}, title = {A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research}, year = {2022}, journal = {ACM TOSEM} }

@INPROCEEDINGS{7180092,
  author={White, Martin and Vendome, Christopher and Linares-Vasquez, Mario and Poshyvanyk, Denys},
  booktitle={IEEE/ACM MSR}, 
  title={Toward Deep Learning Software Repositories}, 
  year={2015},
}


@inproceedings{10.1145/2970276.2970326, 
author = {White, Martin and Tufano, Michele and Vendome, Christopher and Poshyvanyk, Denys}, 
title = {Deep learning code fragments for code clone detection},
year = {2016}, 
abstract = {Code clone detection is an important problem for software maintenance and evolution. Many approaches consider either structure or identifiers, but none of the existing detection techniques model both sources of information. These techniques also depend on generic, handcrafted features to represent code fragments. We introduce learning-based detection techniques where everything for representing terms and fragments in source code is mined from the repository. Our code analysis supports a framework, which relies on deep learning, for automatically linking patterns mined at the lexical level with patterns mined at the syntactic level. We evaluated our novel learning-based approach for code clone detection with respect to feasibility from the point of view of software maintainers. We sampled and manually evaluated 398 file- and 480 method-level pairs across eight real-world Java systems; 93\% of the file- and method-level samples were evaluated to be true positives. Among the true positives, we found pairs mapping to all four clone types. We compared our approach to a traditional structure-oriented technique and found that our learning-based approach detected clones that were either undetected or suboptimally reported by the prominent tool Deckard. Our results affirm that our learning-based approach is suitable for clone detection and a tenable technique for researchers.}, 
booktitle = {ASE}
}



@inproceedings{DBLP:conf/uai/JiangZLLL21,
  author       = {Xue Jiang and
                  Zhuoran Zheng and
                  Chen Lyu and
                  Liang Li and
                  Lei Lyu},
  editor       = {Cassio P. de Campos and
                  Marloes H. Maathuis and
                  Erik Quaeghebeur},
  title        = {TreeBERT: {A} tree-based pre-trained model for programming language},
  booktitle    = {{UAI} 2021},
  year         = {2021},
  url          = {https://proceedings.mlr.press/v161/jiang21a.html},
  timestamp    = {Fri, 17 Dec 2021 17:06:27 +0100},
  biburl       = {https://dblp.org/rec/conf/uai/JiangZLLL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% 代码分类，非transformer
@inproceedings{DBLP:conf/aaai/MouLZWJ16,
  author       = {Lili Mou and
                  Ge Li and
                  Lu Zhang and
                  Tao Wang and
                  Zhi Jin},
  editor       = {Dale Schuurmans and
                  Michael P. Wellman},
  title        = {Convolutional Neural Networks over Tree Structures for Programming
                  Language Processing},
  booktitle    = {AAAI 2016},
  year         = {2016},
  url          = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11775},
  timestamp    = {Wed, 10 Feb 2021 08:45:05 +0100},
  biburl       = {https://dblp.org/rec/conf/aaai/MouLZWJ16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


% tranx前作，非transformer
@inproceedings{DBLP:conf/acl/YinN17,
  author       = {Pengcheng Yin and
                  Graham Neubig},
  title        = {A Syntactic Neural Model for General-Purpose Code Generation},
  booktitle    = {ACL},
  year         = {2017},
}


% tranx，非transformer
@inproceedings{DBLP:conf/emnlp/YinN18,
  author       = {Pengcheng Yin and
                  Graham Neubig},
  title        = {{TRANX:} {A} Transition-based Neural Abstract Syntax Parser for Semantic
                  Parsing and Code Generation},
  booktitle    = {EMNLP},
  year         = {2018},
}



% 代码生成 炉石传说，非transformer
@inproceedings{DBLP:conf/acl/RabinovichSK17,
  author       = {Maxim Rabinovich and
                  Mitchell Stern and
                  Dan Klein},
  title        = {Abstract Syntax Networks for Code Generation and Semantic Parsing},
  booktitle    = {ACL},
  year         = {2017},
}

% ICLR2019 ast输入 代码摘要 非Transformer
@inproceedings{DBLP:conf/iclr/AlonBLY19,
  author       = {Uri Alon and
                  Shaked Brody and
                  Omer Levy and
                  Eran Yahav},
  title        = {code2seq: Generating Sequences from Structured Representations of
                  Code},
  booktitle    = {ICLR},
  year         = {2019},
}

% 学姐的互蒸馏，非transformer
@inproceedings{DBLP:conf/aaai/XieSGLCYW21,
  author       = {Binbin Xie and
                  Jinsong Su and
                  Yubin Ge and
                  Xiang Li and
                  Jianwei Cui and
                  Junfeng Yao and
                  Bin Wang},
  title        = {Improving Tree-Structured Decoder Training for Code Generation via
                  Mutual Learning},
  booktitle    = { AAAI },
  year         = {2021},
}

% 学长的动态扩展结点，非transformer
@inproceedings{DBLP:conf/acl/JiangZM00H0S20,
  author       = {Hui Jiang and
                  Chulun Zhou and
                  Fandong Meng and
                  Biao Zhang and
                  Jie Zhou and
                  Degen Huang and
                  Qingqiang Wu and
                  Jinsong Su},
  title        = {Exploring Dynamic Selection of Branch Expansion Orders for Code Generation},
  booktitle    = {ACL},
  year         = {2021},
}

% 代码生成、对生成的代码进行编码(但是解码生成的是ast相关的话decoder输入肯定是ast编码的 不应该分这个)、decoder生成的也是ast、transformer
@inproceedings{DBLP:conf/aaai/SunZXSMZ20,
  author       = {Zeyu Sun and
                  Qihao Zhu and
                  Yingfei Xiong and
                  Yican Sun and
                  Lili Mou and
                  Lu Zhang},
  title        = {TreeGen: {A} Tree-Based Transformer Architecture for Code Generation},
  booktitle    = {AAAI},
  year         = {2020},
}

% 代码生成、transformer
@inproceedings{DBLP:conf/icse/KimZT021,
  author       = {Seohyun Kim and
                  Jinman Zhao and
                  Yuchi Tian and
                  Satish Chandra},
  title        = {Code Prediction by Feeding Trees to Transformers},
  booktitle    = {ICSE},
  year         = {2021},
}

% 代码生成、解码端生成语法规则、非transformer
@inproceedings{DBLP:conf/aaai/SunZMXLZ19,
  author       = {Zeyu Sun and
                  Qihao Zhu and
                  Lili Mou and
                  Yingfei Xiong and
                  Ge Li and
                  Lu Zhang},
  title        = {A Grammar-Based Structural {CNN} Decoder for Code Generation},
  booktitle    = {AAAI},
  year         = {2019},
}

% 提出代码位置编码、代码摘要and代码补全、既使用ast编码 也使用ast解码、transformer
@inproceedings{DBLP:conf/emnlp/Peng0ZJ22,
  author       = {Han Peng and
                  Ge Li and
                  Yunfei Zhao and
                  Zhi Jin},
  title        = {Rethinking Positional Encoding in Tree Transformer for Code Representation},
  booktitle    = {EMNLP},
  year         = {2022},
}

% 代码补全、预训练模型、没有应用ast，是"热门课题"引用
@inproceedings{DBLP:conf/kbse/LiuLZJ20,
  author       = {Fang Liu and
                  Ge Li and
                  Yunfei Zhao and
                  Zhi Jin},
  title        = {Multi-task Learning based Pre-trained Language Model for Code Completion},
  booktitle    = {ASE},
  year         = {2020},
}

% "热门课题"引用
@inproceedings{DBLP:conf/sigsoft/SvyatkovskiyDFS20,
  author       = {Alexey Svyatkovskiy and
                  Shao Kun Deng and
                  Shengyu Fu and
                  Neel Sundaresan},
  title        = {IntelliCode compose: code generation using transformer},
  booktitle    = {{ESEC/FSE} '20},
  year         = {2020},
}

% 代码摘要、使用图注意力神经网络和预训练的混合表征来代码的混合表示 在图注意力神经网络中引入了AST和代码片段的控制流、预训练下游任务、ast编码
@inproceedings{DBLP:conf/iwpc/WangDLZ22,
  author       = {Yu Wang and
                  Yu Dong and
                  Xuesong Lu and
                  Aoying Zhou},
  title        = {GypSum: learning hybrid representations for code summarization},
  booktitle    = {ICPC},
  year         = {2022},
}

% 预训练模型，代码克隆检测
@inproceedings{DBLP:conf/acl/DingBPMRC22,
  author       = {Yangruibo Ding and
                  Luca Buratti and
                  Saurabh Pujar and
                  Alessandro Morari and
                  Baishakhi Ray and
                  Saikat Chakraborty},
  title        = {Towards Learning (Dis)-Similarity of Source Code from Program Contrasts},
  booktitle    = {ACL},
  year         = {2022},
}

% 预训练模型、代码理解+代码生成系列任务、在预训练阶段代码用ast表示
@inproceedings{DBLP:conf/emnlp/0034WJH21,
  author       = {Yue Wang and
                  Weishi Wang and
                  Shafiq R. Joty and
                  Steven C. H. Hoi},
  title        = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models
                  for Code Understanding and Generation},
  booktitle    = {EMNLP},
  year         = {2021},
}

% 预训练模型、预训练阶段：ast表示 encoder有ast注意力 decoder通过特殊任务structure aware
@article{DBLP:journals/corr/abs-2206-05239,
  author       = {Sindhu Tipirneni and
                  Ming Zhu and
                  Chandan K. Reddy},
  title        = {StructCoder: Structure-Aware Transformer for Code Generation},
  journal      = {CoRR},
  year         = {2022},
}

% 预训练模型、预训练阶段ast编码
@inproceedings{DBLP:conf/acl/GuoLDW0022,
  author       = {Daya Guo and
                  Shuai Lu and
                  Nan Duan and
                  Yanlin Wang and
                  Ming Zhou and
                  Jian Yin},
  title        = {UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  booktitle    = {ACL},
  year         = {2022},
}

% transformer

% 预训练
@inproceedings{DBLP:conf/iclr/GuoRLFT0ZDSFTDC21,
  author       = {Daya Guo and
                  Shuo Ren and
                  Shuai Lu and
                  Zhangyin Feng and
                  Duyu Tang and
                  Shujie Liu and
                  Long Zhou and
                  Nan Duan and
                  Alexey Svyatkovskiy and
                  Shengyu Fu and
                  Michele Tufano and
                  Shao Kun Deng and
                  Colin B. Clement and
                  Dawn Drain and
                  Neel Sundaresan and
                  Jian Yin and
                  Daxin Jiang and
                  Ming Zhou},
  title        = {GraphCodeBERT: Pre-training Code Representations with Data Flow},
  booktitle    = {{ICLR}},
  year         = {2021},
}

% codex
@article{DBLP:journals/corr/abs-2107-03374,
  author       = {Mark Chen and
                  Jerry Tworek and
                  Heewoo Jun and
                  Qiming Yuan and
                  Henrique Pond{\'{e}} de Oliveira Pinto and
                  Jared Kaplan and
                  Harrison Edwards and
                  Yuri Burda and
                  Nicholas Joseph and
                  Greg Brockman and
                  Alex Ray and
                  Raul Puri and
                  Gretchen Krueger and
                  Michael Petrov and
                  Heidy Khlaaf and
                  Girish Sastry and
                  Pamela Mishkin and
                  Brooke Chan and
                  Scott Gray and
                  Nick Ryder and
                  Mikhail Pavlov and
                  Alethea Power and
                  Lukasz Kaiser and
                  Mohammad Bavarian and
                  Clemens Winter and
                  Philippe Tillet and
                  Felipe Petroski Such and
                  Dave Cummings and
                  Matthias Plappert and
                  Fotios Chantzis and
                  Elizabeth Barnes and
                  Ariel Herbert{-}Voss and
                  William Hebgen Guss and
                  Alex Nichol and
                  Alex Paino and
                  Nikolas Tezak and
                  Jie Tang and
                  Igor Babuschkin and
                  Suchir Balaji and
                  Shantanu Jain and
                  William Saunders and
                  Christopher Hesse and
                  Andrew N. Carr and
                  Jan Leike and
                  Joshua Achiam and
                  Vedant Misra and
                  Evan Morikawa and
                  Alec Radford and
                  Matthew Knight and
                  Miles Brundage and
                  Mira Murati and
                  Katie Mayer and
                  Peter Welinder and
                  Bob McGrew and
                  Dario Amodei and
                  Sam McCandlish and
                  Ilya Sutskever and
                  Wojciech Zaremba},
  title        = {Evaluating Large Language Models Trained on Code},
  journal      = {CoRR},
  year         = {2021},
}

% codeT
@article{DBLP:journals/corr/abs-2207-10397,
  author       = {Bei Chen and
                  Fengji Zhang and
                  Anh Nguyen and
                  Daoguang Zan and
                  Zeqi Lin and
                  Jian{-}Guang Lou and
                  Weizhu Chen},
  title        = {CodeT: Code Generation with Generated Tests},
  journal      = {CoRR},
  year         = {2022},
}

@inproceedings{DBLP:conf/emnlp/IyerKCZ18,
  author       = {Srinivasan Iyer and
                  Ioannis Konstas and
                  Alvin Cheung and
                  Luke Zettlemoyer},
  title        = {Mapping Language to Code in Programmatic Context},
  booktitle    = {EMNLP},
  year         = {2018},
}

@inproceedings{DBLP:conf/nips/HendrycksBKMAGB21,
  author       = {Dan Hendrycks and
                  Steven Basart and
                  Saurav Kadavath and
                  Mantas Mazeika and
                  Akul Arora and
                  Ethan Guo and
                  Collin Burns and
                  Samir Puranik and
                  Horace He and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Coding Challenge Competence With {APPS}},
  booktitle    = {NeurIPS Datasets and Benchmarks},
  year         = {2021},
}

@inproceedings{DBLP:conf/iclr/PoesiaP00SMG22,
  author       = {Gabriel Poesia and
                  Alex Polozov and
                  Vu Le and
                  Ashish Tiwari and
                  Gustavo Soares and
                  Christopher Meek and
                  Sumit Gulwani},
  title        = {Synchromesh: Reliable Code Generation from Pre-trained Language Models},
  booktitle    = {ICLR},
  year         = {2022},
}

@inproceedings{DBLP:journals/corr/KingmaB14,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {ICLR},
  year         = {2015},
}

@inproceedings{DBLP:conf/nips/LuGRHSBCDJTLZSZ21,
  author       = {Shuai Lu and
                  Daya Guo and
                  Shuo Ren and
                  Junjie Huang and
                  Alexey Svyatkovskiy and
                  Ambrosio Blanco and
                  Colin B. Clement and
                  Dawn Drain and
                  Daxin Jiang and
                  Duyu Tang and
                  Ge Li and
                  Lidong Zhou and
                  Linjun Shou and
                  Long Zhou and
                  Michele Tufano and
                  Ming Gong and
                  Ming Zhou and
                  Nan Duan and
                  Neel Sundaresan and
                  Shao Kun Deng and
                  Shengyu Fu and
                  Shujie Liu},
  title        = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding
                  and Generation},
  booktitle    = {NeurIPS Datasets and Benchmarks},
  year         = {2021},
}

@inproceedings{DBLP:conf/naacl/AhmadCRC21,
  author       = {Wasi Uddin Ahmad and
                  Saikat Chakraborty and
                  Baishakhi Ray and
                  Kai{-}Wei Chang},
  title        = {Unified Pre-training for Program Understanding and Generation},
  booktitle    = {NAACL},
  year         = {2021},
}

@article{athena,
author = {Yan, Yanfu and Cooper, Nathan and Moran, Kevin and Bavota, Gabriele and Poshyvanyk, Denys and Rich, Steve},
title = {Enhancing Code Understanding for Impact Analysis by Combining Transformers and Program Dependence Graphs},
year = {2024},
journal = {FSE},
}

@inproceedings{DBLP:conf/cvpr/SchroffKP15,
  author       = {Florian Schroff and
                  Dmitry Kalenichenko and
                  James Philbin},
  title        = {FaceNet: {A} unified embedding for face recognition and clustering},
  booktitle    = {CVPR},
  year         = {2015},
}

@inproceedings{lin2004orange,
  title={Orange: a method for evaluating automatic evaluation metrics for machine translation},
  author={Lin, Chin-Yew and Och, Franz Josef},
  booktitle={COLING},
  year={2004}
}

@article{DBLP:journals/corr/abs-2009-10297,
  author       = {Shuo Ren and
                  Daya Guo and
                  Shuai Lu and
                  Long Zhou and
                  Shujie Liu and
                  Duyu Tang and
                  Neel Sundaresan and
                  Ming Zhou and
                  Ambrosio Blanco and
                  Shuai Ma},
  title        = {CodeBLEU: a Method for Automatic Evaluation of Code Synthesis},
  journal      = {CoRR},
  year         = {2020},
}

@inproceedings{zheng2023codegeex,
  title={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X},
  author={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2023}
}

@article{DBLP:journals/corr/abs-2108-07732,
  author       = {Jacob Austin and
                  Augustus Odena and
                  Maxwell I. Nye and
                  Maarten Bosma and
                  Henryk Michalewski and
                  David Dohan and
                  Ellen Jiang and
                  Carrie J. Cai and
                  Michael Terry and
                  Quoc V. Le and
                  Charles Sutton},
  title        = {Program Synthesis with Large Language Models},
  journal      = {CoRR},
  year         = {2021},
}

@article{DBLP:journals/corr/abs-2308-12950,
  author       = {Baptiste Rozi{\`{e}}re and
                  Jonas Gehring and
                  Fabian Gloeckle and
                  Sten Sootla and
                  Itai Gat and
                  Xiaoqing Ellen Tan and
                  Yossi Adi and
                  Jingyu Liu and
                  Tal Remez and
                  J{\'{e}}r{\'{e}}my Rapin and
                  Artyom Kozhevnikov and
                  Ivan Evtimov and
                  Joanna Bitton and
                  Manish Bhatt and
                  Cristian Canton{-}Ferrer and
                  Aaron Grattafiori and
                  Wenhan Xiong and
                  Alexandre D{\'{e}}fossez and
                  Jade Copet and
                  Faisal Azhar and
                  Hugo Touvron and
                  Louis Martin and
                  Nicolas Usunier and
                  Thomas Scialom and
                  Gabriel Synnaeve},
  title        = {Code Llama: Open Foundation Models for Code},
  journal      = {CoRR},
  year         = {2023},
}

@article{li2023starcodersourceyou,
  title={Starcoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={CoRR},
  year={2023}
}


@inproceedings{DBLP:conf/nips/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention is All you Need},
  booktitle    = {NeurIPS},
  year         = {2017},
}

@inproceedings{DBLP:conf/emnlp/FengGTDFGS0LJZ20,
  author       = {Zhangyin Feng and
                  Daya Guo and
                  Duyu Tang and
                  Nan Duan and
                  Xiaocheng Feng and
                  Ming Gong and
                  Linjun Shou and
                  Bing Qin and
                  Ting Liu and
                  Daxin Jiang and
                  Ming Zhou},
  title        = {CodeBERT: {A} Pre-Trained Model for Programming and Natural Languages},
  booktitle    = {EMNLP Findings},
  year         = {2020},
}

@inproceedings{DBLP:conf/naacl/DevlinCLT19,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  booktitle    = {NAACL},
  year         = {2019},
}

@article{DBLP:journals/jmlr/RaffelSRLNMZLL20,
  author       = {Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu},
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                  Transformer},
  journal      = {J. Mach. Learn. Res.},
  year         = {2020},
}


@inproceedings{10.1145/3609437.3609465,
author = {Zhao, Yunfei and Dong, Yihong and Li, Ge},
title = {Seq2Seq or Seq2Tree: Generating Code Using Both Paradigms via Mutual Learning},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609465},
doi = {10.1145/3609437.3609465},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {238–248},
numpages = {11},
keywords = {neural networks, mutual learning, code generation, abstract syntax tree},
location = {Hangzhou, China},
series = {Internetware '23}
}