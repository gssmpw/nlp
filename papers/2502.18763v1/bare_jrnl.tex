 %% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/
%\usepackage{indentfirst}
\setlength{\parindent}{2em}

%\documentclass[journal]{IEEEtran}

\documentclass[journal]{IEEEtran}
%\documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}
\usepackage{amsthm,amssymb}
\usepackage{amsmath}
\usepackage{easyReview}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}%解决公式在表格中位置太小的问题
\usepackage{booktabs}%解决表格横向粗细问题
\setcellgapes{3pt}
\usepackage{multirow}
\setcellgapes{3pt}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage[justification=centering]{caption}
\usepackage{color}
\usepackage{verbatim}
\usepackage{amssymb}
%\usepackage[table,xcdraw]{xcolor}
\usepackage{tabularx}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}

% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
%\usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
%\usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{CommGPT: A Graph and Retrieval-Augmented Multimodal Communication Foundation Model}

\author{Feibo Jiang, \textit{Senior Member, IEEE}, Wanyun Zhu, Li Dong,  Kezhi Wang, \textit{Senior Member, IEEE}, Kun Yang, \textit{Fellow, IEEE}, Cunhua Pan, \textit{Senior Member, IEEE}, Octavia A. Dobre, \textit{Fellow, IEEE}
	%\thanks{
	
	%	Feibo Jiang and Wanyun Zhu is with Hunan Normal University, China.
	%	Li Dong is with the Hunan University of Technology and Business, China.
	%	Kezhi Wang is with Brunel University, UK.
	%	Yubo Peng and Kun Yang are with Nanjing University, China.
	%	Cunhua Pan is with Southeast University, China.
	%	Robert Schober is with the Friedrich-Alexander University (FAU) Erlangen-Nürnberg, Germany.}
}

\maketitle
\begin{abstract}
Large Language Models (LLMs) possess human-level cognitive and decision-making capabilities, making them a key technology for 6G.
%addressing the complex challenges of next-generation wireless communication systems. 
However, applying LLMs to the communication domain faces three major challenges: 1) Inadequate communication data; 2) Restricted input modalities; and 3) Difficulty in knowledge retrieval. To overcome these issues, we propose CommGPT, a multimodal foundation model designed specifically for communications.
First, we create high-quality pretraining and fine-tuning datasets tailored in communication, enabling the LLM to engage in further pretraining and fine-tuning with communication concepts and knowledge. Then, we design a multimodal encoder to understand and process information from various input modalities.
Next, we construct a Graph and Retrieval-Augmented Generation (GRG) framework, efficiently coupling Knowledge Graph (KG) with Retrieval-Augmented Generation (RAG) for multi-scale learning. 
%In this framework, the fine-tuned LLM is integrated with the KG to understand and retrieve large-scale communication knowledge, while RAG is used to handle smaller-scale communication knowledge. The combination of these two strategies further improves CommGPT's accuracy and reliability in understanding and generating responses based on communication-related knowledge. 
Finally, we demonstrate the feasibility and effectiveness of the CommGPT through experimental validation.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.

\IEEEpeerreviewmaketitle

\section{Introduction}
The sixth-generation mobile communication technology (6G) aims to build an intelligent world of interconnected devices, offering society an unprecedented information transmission experience. In the 6G architecture, intelligence is one of its key driving forces \cite{9237460}. 6G networks will leverage adaptive resource management, intelligent network optimization, and efficient spectrum utilization to achieve self-learning and self-optimization, allowing dynamic adaptation to complex and changing communication environments in real time.

Large Language Models (LLMs), such as GPT \cite{achiam2023gpt} and LLaMA \cite{touvron2023llama}, represent the cutting edge of generative AI research. With their advanced neural network architectures and training on vast amounts of corpus data, these models exhibit human-level understanding and cognitive abilities. Introducing LLMs into 6G networks will significantly enhance their level of intelligence. This integration will enable the networks to more rapidly perceive, simulate, and respond to the fast-changing communication environment, generalize more efficiently, and understand and adapt to unseen communication scenarios \cite{10700707}. As such, LLMs are poised to become an essential tool for addressing the complex challenges of future wireless communication systems.
\begin{comment}
	内容...

\subsection{Advantages of applying LLMs in communication}
\subsubsection{Accurate Contextual Understanding and Reasoning}
The knowledge system in communication is vast and complex, encompassing numerous standards, protocols, technical details, and the intricate interrelationships between them. Traditional knowledge processing methods rely on manual parsing or rule-based systems, which are not only inefficient but also prone to misunderstandings and errors. LLMs possess powerful knowledge representation and understanding capabilities, enabling them to quickly extract key information from communication documents and reason based on context to generate precise solutions. %With a deep understanding of the interdependencies between communication protocols, LLMs can help engineers rapidly identify and locate critical technical issues, thus offering effective technical support solutions.

\subsubsection{Enhanced System Adaptability and Flexibility}
Communication systems need frequent updates and optimizations when dealing with new technologies and complex scenarios. LLMs demonstrate strong generalization and adaptability, capable of quickly adjusting to new communication standards and technical requirements through contextual learning and customized fine-tuning. When applied to 6G communication systems, LLMs can flexibly adjust their knowledge structures, efficiently mastering the principles and applications of emerging communication technologies. They can rapidly adapt and provide technical support based on the latest standards, continuously optimizing the system’s ability to handle new knowledge and scenarios.

\subsubsection{Cognitive-Level Decision Making and Optimization}
LLMs not only address specific communication problems but also offer cognitive-level logical reasoning and intelligent decision-making support in complex decision-making processes. By conducting in-depth analysis of communication data, LLMs can identify potential issues and optimization opportunities, providing decision-making support for network planning, resource allocation, and performance optimization. Additionally, LLMs can automatically analyze the operational status and logs of communication devices, offering real-time decision support for engineers, providing high-quality decision-making, making network management more intelligent and efficient.
\end{comment}
\setcounter{subsubsection}{0}
\subsection{Related work}
Several LLMs have already been applied in communication. For instance, TSpec-LLM leverages open-source datasets combined with  Retrieval-Augmented Generation (RAG) to enhance LLMs’ understanding of 3GPP standards, improving their precision in document analysis and response \cite{nikbakht2024tspec}. Tele-LLM is a specialized LLM tailored for telecommunications, leveraging customized datasets to improve its expertise in this field \cite{RN17}. Similarly, TelecomGPT introduces a telecommunications-specific LLM training framework that fine-tunes general LLMs to achieve exceptional performance on telecom-specific tasks \cite{zou2024telecomgpt}. These advancements underscore the significant potential of LLMs in enhancing the intelligence of 6G networks.
\subsection{Challenges of applying LLMs in communication}
Despite their promising performance across many tasks in communication, developing  foundational models suited for the communication field faces several challenges as follows:
\subsubsection{Inadequate communication data}
Mainstream LLMs lack specialized knowledge of communications, limiting their ability to understand and accurately respond to domain-specific concepts and problems. Most LLMs are trained on large-scale general-purpose datasets, while high-quality, domain-specific datasets in communication are relatively scarce. This deficiency restricts LLMs’ ability to provide accurate and reliable responses tailored to the communication field \cite{jiang2024large}.

\subsubsection{Restricted input modalities}
Many standards, protocols, and documents in communication include tables, images, and other non-textual content. Thus, multimodal information is a critical component that communication foundational models need to address. However, current LLMs in communication are primarily designed to process textual data, limiting their ability to understand multimodal content. In scenarios requiring the integration of multiple modalities for comprehensive responses, this limitation can lead to suboptimal performance.

\subsubsection{Difficulty in knowledge retrieval}
 The communication knowledge is highly specialized and complex, requiring retrieval and analysis at different scales. Two primary approaches are currently used for enabling LLMs to access external communication knowledge:
(1) Knowledge Graphs (KGs) enable the organization of complex knowledge networks and facilitate the understanding of communication tasks from a global (large-scale) perspective. (2) RAG tends to focus on a localized (small-scale) understanding and analysis of communication knowledge due to the retrieval of a series of document fragments. However, no approach has been able to effectively balance the demands for both local and global knowledge retrieval\cite{fatehkia2024t}.
%\begin{itemize}
%\item \emph{Knowledge graph}:
%By constructing specialized Knowledge Graphs (KGs) in communication, communication-specific knowledge can be structured and stored in graph form. 



%\item \emph{Retrieval-augmented generation}:
%By segmenting communication documents and storing them as vectorized representations, RAG allows LLMs to retrieve and generate responses to communication-related queries \cite{10531073} \cite{bornea2024telco}. However, due to the document segmentation, this approach biases models toward a localized (small-scale) understanding and analysis of communication knowledge.

%\end{itemize}
%However, no approach has been able to effectively balance the demands for both local and global knowledge retrieval.
%Both methods have limitations in terms of accuracy and comprehensiveness, as neither can effectively balance the demands for global and local knowledge understanding and analysis. This imbalance presents a significant challenge to achieving precise and holistic knowledge understanding and processing of LLMs in communication.

\setcounter{subsubsection}{0}
\subsection{Contributions}
To address the aforementioned challenges, we propose CommGPT, a multimodal foundational model in communication, enhanced with multimodal encoder, RAG and KG. Our contributions are summarized as follows:
\subsubsection{High-quality communication dataset}
We construct and release CommData, a high-quality dataset tailored in communication. CommData consists of a pretraining dataset and a fine-tuning dataset. The pretraining dataset includes state-of-the-art communication-related resources, such as recent communication papers, patents, codebases, and 3GPP and IEEE protocol standards. These materials encompass a wide range of topics, including networking technologies, signal processing, and wireless communication protocols, enabling comprehensive knowledge acquisition for LLMs. The fine-tuning dataset is curated and structured to generate question-answer pairs for instruction tuning, improving the LLM's ability to follow communication-specific instructions and accurately understand diverse communication-related commands.

\subsubsection{Multimodal data encoding}
Communication documents often contain tables and images, which traditional LLMs struggle to process effectively. We develop a multimodal encoder that utilizes multimodal embedding models, such as Bootstrapped Language-Image Pretraining (BLIP) \cite{li2023blip} and QOCR \cite{mithe2013optical}, to parse data from various modalities, including images and tables. This design allows the LLM to better understand user intent comprehensively and accurately. By leveraging multimodal encoder, we enhance the accuracy and reliability of communication-related question-answering in multimodal scenarios, reducing hallucinations when handling cross-modal tasks.

\subsubsection{Graph and Retrieval-Augmented Generation (GRG)}
We introduce a multi-scale analysis framework that integrates KG and RAG. This framework creates an efficient pipeline that couples graph knowledge with vector knowledge to improve the accuracy of external communication knowledge retrieval and reasoning. The pipeline employs a fine-tuned LLM to generate a graph-based knowledge base, which is then combined with the vector database in RAG. This enables the LLM to adaptively fuse knowledge for tackling complex problems at different scales, significantly enhancing the completeness and accuracy of communication-related knowledge queries.

The remainder of this paper is organized as follows. In Section II, the key technologies for integrating communication knowledge with foundational models are presented. In Section III, the CommGPT framework and its key components are described. In Section IV, the detailed design of the CommGPT framework is provided. In Section V, experimental results are presented, followed by a discussion of open issues in Section VI, and conclusions are drawn in Section VII.

\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\section{Learning process of communication foundation model}
For learning communication knowledge, LLMs adopt two primary learning approaches. The first involves pretraining and fine-tuning, embedding communication knowledge directly into the foundation model's parameters. However, this method is time-intensive and unsuitable for accommodating knowledge that requires frequent updates. The second approach integrates RAG and KG, leveraging external vector and graph databases to incorporate communication knowledge into the foundation model’s context learning. This method does not require updates to the foundation model’s parameters, making it better suited for learning rapidly evolving knowledge.
\subsection{Internal learning: pretraining and fine-tuning}
\subsubsection{Unsupervised pretraining}
The objective of pretraining is to equip the foundation model with expert-level communication knowledge, enhancing its ability to comprehend specialized concepts and structured information in communication. First, we load a corpus encompassing knowledge in communication, such as communication protocols, channel models, and communication standards. Next, open-source LLMs, such as LLaMA \cite{touvron2023llama} or Gemma \cite{team2024gemma} are trained on the communication corpus using unsupervised learning tasks, such as masked language modeling and causal language modeling \cite{10766891}. Through these tasks, the LLM learns to predict contextual relationships or fill in missing information. Pretraining enables the foundation model to grasp the underlying structure and patterns of communication knowledge, significantly improving its adaptability to domain-specific terminology and document structures, thereby establishing a solid foundation for subsequent fine-tuning.

\subsubsection{Supervised fine-tuning}
Fine-tuning is an additional optimization step for the pretrained foundation model, focusing on learning from communication data to improve performance on specialized tasks. Through fine-tuning, the LLM can better meet the demands of communication-specific tasks while retaining the general features learned during pretraining. The process begins by restructuring communications datasets to generate instruction datasets tailored to various tasks. The pretrained foundation model is then fine-tuned on these instruction datasets using supervised learning. Fine-tuning typically involves training only a subset of the model's parameters. To avoid catastrophic forgetting, it is essential to design high-quality, high-coverage instruction datasets, which can significantly enhance the LLM's precision and performance in communication-specific tasks.
\subsection{External learning: RAG and KG}
\subsubsection{Vector data-based RAG}
RAG is a method that integrates information retrieval with foundation models to provide accurate contextual knowledge, enhancing both knowledge coverage and response accuracy in complex communication tasks. By retrieving relevant communication knowledge from vector databases before generating outputs, RAG dynamically complements the foundation model’s existing knowledge. This technique addresses challenges related to knowledge updating and detail precision, significantly improving the model’s comprehension and expertise in communication. Moreover, it eliminates the need for frequent large-scale retraining of foundation models to incorporate updated knowledge, offering notable flexibility and scalability. %However, effective implementation of RAG requires ensuring the accuracy and authority of the communication knowledge base content, as well as the precision of retrieval strategies.

\subsubsection{Graph data-based KG}
KGs represent entities and their relationships through a graph structure, systematically capturing and storing communications knowledge via entities  and the relationships between them. This structured knowledge enhances the foundation model's reasoning capabilities and ensures consistency in knowledge representation. Through pretraining and fine-tuning, foundation models can effectively identify entities and explicit relationships in communication, improving the accuracy of KG construction. Furthermore, leveraging KGs enables the LLM to infer implicit relationships between entities, facilitating high-quality relation and event extraction. 
This greatly enhances the LLM's understanding and reasoning capabilities for complex communication knowledge. %Moreover, it supports interpretable outputs, aiding in the analysis of the accuracy and scientific validity of the generated results.
The comparison of pretraining, fine-tuning, RAG, and KG for communication foundation model is illustrated in TABLE \ref{tab:Comp}.

%\subsection{Comparison of Pretraining, Fine-Tuning, RAG, and KG}
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[ht] % 使用 table* 环境创建跨栏表格
	\renewcommand{\arraystretch}{1.5} 
	\caption{Comparison of pretraining, fine-tuning, RAG, and KG}
	\label{tab:Comp}
	\begin{center}
		\begin{tabularx}{\textwidth}{|l|X|X|X|X|}
			\hline
			\textbf{Aspect} & \textbf{Pretraining} & \textbf{Fine-tuning} & \textbf{RAG} & \textbf{Knowledge Graph} \\
			\hline
			Goal & Learning general language patterns & Optimizing performance for specific tasks & Retrieving external documents to enhance LLM & Constructing structured knowledge to enhance LLM \\
			\hline
			Data Type & Large-scale unlabeled text & Labeled instruction data & External documents/vector database & Structured graph database \\
			\hline
			Learning Method & Unsupervised learning & Supervised learning & Retrieval and in-context learning & Graph reasoning \\
			\hline
		%	Model Input & Raw text & Task-specific text & Text query + retrieved information & Knowledge graph nodes and edges \\
		%	\hline
		%	Model Output & Language model probabilities or representations & Task-specific output (e.g., classification, translation) & Generated text, incorporating retrieved content & Reasoning results or query answers from the knowledge graph \\
		%	\hline
		Tuning	Parameters  & All parameters of LLMs & Task-specific parameters of LLMs & Parameters of retrieval modules & Graph embedding and structure \\
			\hline
			Time Complexity & High & Medium & Low & Medium \\
			\hline
		Disadvantages & Lack of task-specific optimization &  Labeled instruction data requirement & Retrieval suboptimality & Difficulty in generating high-quality graph structure. \\
			\hline
		\end{tabularx}
	\end{center}
\end{table*}

\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\section{CommGPT framework}
%The direct application of LLMs to the communications domain faces several challenges: 1) Inadequate communication data; 2) Restricted input modalities; 3) Difficulty in knowledge retrieval. To address these issues, 
In this paper, we proposes CommGPT, a graph and retrieval-augmented multimodal communication foundation model. The CommGPT framework establishes an efficient pipeline that combines pretraining, fine-tuning, RAG, and KG to improve the accuracy of the foundation model's understanding, analysis, and decision-making capabilities in communication. The overall design process of the CommGPT framework is illustrated in Fig. \ref{fig:fig1}.

%The efficient pipeline includes construction of the communication dataset, pretraining and fine-tuning of the foundation model, development of the multimodal encoder, and implementation of the GRG. Through synergistic improvements in data quality, model training, and knowledge retrieval, CommGPT establishes a comprehensive pipeline that spans from data preprocessing to task inference. This end-to-end process significantly enhances the foundation model’s ability to understand and analyze tasks in communication.
\begin{figure*}[htpb]
	\centering
	\includegraphics[width=17cm]{1.png}
	\caption{Design framework of the CommGPT system}
	\label{fig:fig1}
\end{figure*}
\subsection{Construction of communication datasets}
We develop a communication-specific dataset, CommData, which includes two components: the continuing PreTraining dataset (CommData-PT) and the instruction Fine-Tuning dataset (CommData-FT). The CommData-PT dataset consolidates communication protocols and standards (such as 3GPP and IEEE standards), along with publicly available communication-related data (e.g., Wiki, papers, patents, and codes), enabling the foundation model to learn and internalize communication knowledge. The CommData-FT dataset focuses on optimizing the foundation model for various communication tasks. It includes task-specific instruction data designed around communication protocols and standards, aiming to further enhance the foundation model’s understanding and execution of communication tasks, significantly improving its response accuracy.

\subsubsection{CommData-PT}
To design the CommData-PT dataset, we collected the following types of data:

\begin{itemize}
	\item \emph{3GPP standard data}:
	This dataset contains 15,016 processed 3GPP standard documents (from Rel8 to Rel19).%, with a total size of approximately 2.09 GB.
	
	\item \emph{IEEE standard data}:
	This dataset includes 40 protocol documents related to communications extracted from IEEE standards (such as 802.3, 802.11, 802.15, and C951).%, with a total size of approximately 8.59 MB.
	
	\item \emph{Communication patents}:
This dataset consists of 697,717 communication-related patents spanning from 1975 to 2024.%, with a total size of about 404 MB. %This covers areas like wireless communications, network technologies, and information processing.
	
	\item \emph{Communication papers}:
This dataset consists of 90,310 communication-related papers sourced from the Arxiv platform.%, with a total size of approximately 3.9 GB.
	
	\item \emph{Communication code}:
We extract 14,128 code entries related to communications from GitHub. This dataset aims to improve the LLM’s capacity to comprehend and generate communication-related code.
	
	\item \emph{Wiki data}:
	We select 19,543 entries related to communications from Wikipedia, drawn from a total of 6,407,849 records.%, with a total size of approximately 121 MB.

\end{itemize}

	We employ two methods to extract high-quality communication data from public datasets: (1) \emph{LLM-based Filtering}: Using existing LLMs (e.g., GPT, LLaMA) to identify communication-related documents and filter out irrelevant or low-quality content. (2) \emph{Keyword-based Filtering}: Customizing a set of communication-specific keywords and extracting documents based on these keywords, while removing HTML tags, hyperlinks, and template content as interfering elements. After data collection, preprocessing becomes a crucial step for constructing a high-quality pretraining dataset. The preprocessing steps included eliminating noise, redundancy, irrelevant data, and potentially harmful content to ensure that CommGPT performs optimally. We propose a standardized data preprocessing pipeline for the communications domain, as shown in Fig. \ref{fig:fig2}. 
	%This pipeline includes the following steps:

%\begin{itemize}
%	\item \emph{Selection of Communication Documents}:
%	We employed two methods to extract high-quality communication data from public datasets: (1)Voice Model-based Filtering: Using existing language models (e.g., GPT, LLaMA) to identify communication-related documents and filter out irrelevant or low-quality content. (2)Keyword-based Filtering: Customizing a set of communication-specific keywords and extracting documents based on these keywords, while removing HTML tags, hyperlinks, and template content as interfering elements.
	
%	\item \emph{Deduplication of Communication Documents}:
%	Redundant text can reduce the diversity of the communication corpus and affect the quality and stability of foundational model training. We addressed this issue through the following methods: (1)Sentence-level Deduplication: Identifying duplicate sentences based on word overlap. (2)Paragraph-level Deduplication: Removing redundant content using n-gram overlap and vector database techniques.
	
%	\item \emph{Data Sanitization}:
%	To protect privacy, sensitive information such as personal details, organization or device identifiers, names, trademarks, URLs, addresses, phone numbers, device IDs, and specific confidential content were removed, minimizing the risk of privacy leakage.
	
%	\item \emph{Data Format Conversion}:
%	Before model training, it was essential to ensure that all data files adhered to predefined format standards to guarantee stable training and enhanced model performance. This included ensuring the integrity of data structures, completeness of fields, and consistency of formats, to avoid encoding errors and parsing issues, while ensuring that the necessary field information was clear and semantically accurate.
	
%\end{itemize}

\begin{figure*}[htpb]
	\centering
	\includegraphics[width=19cm]{2.png}
	\caption{Data preprocessing for CommData-PT}
	\label{fig:fig2}
\end{figure*}

\subsubsection{CommData-FT}
%The communication instruction fine-tuning dataset refers to a specialized dataset designed to optimize the foundational model’s ability to understand and respond to communication task instructions. 
The goal of CommData-FT dataset is to enable the foundation model to generate expected outputs when given clear task instructions, thereby enhancing its task adaptation, contextual understanding, and accuracy in generating results. A complete communication instruction consists of the following components: \emph{Instruction, Input, Output, and Metadata}. The \emph{Instruction} clearly describes the task; the \emph{Input} provides the contextual information for the instruction; the \emph{Output} offers a high-quality result based on the instruction and input, meeting the task requirements; and the \emph{Metadata} (optional) includes additional details such as task notes or difficulty level.
We use LLaMA 3-8B-Instruct model to generate the CommData-FT dataset based on the 3GPP and IEEE standards, aiming to enhance the foundation model’s understanding and representation of communication knowledge. The specific steps are as follows:

\begin{itemize}
	\item \emph{Generating questions and answers}:
	Using the LLaMA 3-8B-Instruct model, we refine different questions from the CommData-PT dataset based on instructions and guide the model to generate accurate answers.
	
	\item \emph{Building instruction data}:
	The instructions, generated questions, and corresponding answers are paired to form the instruction dataset. Each question is ensured to meet the requirements of the instruction, with accurate and relevant answers.
	
	\item \emph{Quality assessment and filtering}:
	The generated instructions are assessed for quality, and high-quality instructions are selected. This ensures the dataset's diversity and broad coverage of content and task types.
	
\end{itemize}

A specific example of the instruction data is as follows:
\emph{\{"Instruction":” This is a Question and Answer task related to 3GPP.", "Input": "What is the purpose of the SIP-based protocol framework?", "Output": "The SIP-based protocol framework serves as a means of user configuration of supplementary services in the IM CN subsystem.", "Metadata": "Section 4.1, General description in 24238-c00"\} }


In this example, the Instruction is a Question and Answer task, the Input and Output are generated from the LLaMA 3-8B-Instruct model, and the Metadata is the data source.

%asks the model to provide an explanation of the role of the SIP protocol framework. Through the Inputs generated from the instruction and the automatically generated answers, the model learns how this protocol supports user service configuration in communication networks. This question-answer pair not only helps the model understand complex communication protocols but also enhances its performance in real-world tasks.

%Through this systematic and precise fine-tuning process, we are able to generate a wide-ranging, highly accurate fine-tuning dataset that further improves the model’s performance in the communication domain.
\vspace{-2pt}
\subsection{Pretraining and fine-tuning}
Using CommData, we design a two-stage training scheme. First, we perform continued pretraining on the base model to enhance its understanding of foundational concepts and unique structures in the communication domain. Then, we apply instruction fine-tuning to further improve its performance on specific communication tasks. This combination of communication knowledge adaptation and task optimization ensures that the base model strikes a balance between broad communication knowledge and specialized expertise in various tasks.

\subsubsection{Continued pretraining}
We perform  continued pretraining to further train a foundation model on the CommData-PT dataset. This method allows the foundation model to retain general knowledge while acquiring specialized communication knowledge. We use open-source LLMs (such as LLaMA, Gemma, etc.) as the foundation model, and employ an autoregressive architecture for unsupervised training. The self-attention mechanism helps the foundation model capture long-range dependencies and complex contextual information. By predicting the probability of each token in its context, the foundation model acquires expertise in communication knowledge, improving its understanding of communication concepts and standards. After the pretraining phase, the foundation model is evaluated to ensure the learning quality in communication. %The pretrained model is then saved as the foundation for subsequent fine-tuning.

\subsubsection{Instruction fine-tuning}
We further fine-tune the pretrained foundation model using the CommData-FT dataset to optimize its performance on communication tasks. This step enables the foundation model to adapt to specific tasks such as answering questions, multiple-choice questions, and generating codes. We perform supervised efficient fine-tuning on the pretrained foundation model, employing the Low-Rank Adaptation (LoRA) \cite{zheng2024llamafactory}. The LoRA parameters are adjusted through backpropagation, and task-specific loss functions such as cross-entropy loss are used for optimization. Fine-tuning typically uses a smaller learning rate to avoid disrupting the communication knowledge acquired during pretraining. Finally, the fine-tuned foundation model is saved in preparation for subsequent tasks.

\subsection{Multimodal encoder}
To enable the understanding of tables and images in communication documents, we integrate two multimodal encoders into the input layer of the foundation model, where the BLIP encoder is used to extract high-level semantic features from images \cite{li2023blip}, while the QOCR encoder extracts lower-level text information in infographics \cite{mithe2013optical}, thereby providing comprehensive support for multimodal data in communication documents.

\subsubsection{BLIP encoder}
BLIP is a generative model designed for image semantic understanding. It captures high-level features from images using a self-attention mechanism and converts them into high-quality textual descriptions, enabling textual encoding of images in communication documents. The process of generating image descriptions with the BLIP encoder is as follows: First, BLIP uses a vision encoder based on the Vision Transformer architecture to extract high-level semantic features from the image. Then, through the image-text fusion module, it aligns the image and text features, allowing BLIP to better understand the textual meaning associated with the image. Finally, BLIP generates descriptive text for the image using a Transformer-based autoregressive generator in the text decoder \cite{li2023blip}.

\subsubsection{QOCR encoder}
QOCR is a deep learning model for detecting and recognizing text information in infographics. It converts the image files into grayscale by performing scaling and color space transformations. Next, Convolutional Neural Networks (CNNs) are employed to extract features from the image and identify text regions within it. Then, Long Short-term Memory Networks (LSTMs) are used to model the sequences in the text regions, precisely mapping the character sequences extracted from the image into readable text. Finally, a confidence threshold is applied to filter the recognition results, discarding those with confidence levels below the set threshold, ensuring that only high-confidence recognition results are retained, thereby enhancing the overall accuracy and reliability of the output \cite{mithe2013optical}.

\subsection{Graph and retrieval-augmented generation}
To efficiently learn and apply knowledge in communication, we propose a multi-scale learning framework that integrates KG and RAG. We use the fine-tuned foundation model to generate a KG of communication corpora, creating a global structured representation of entities and their relationships in communication documents. Simultaneously, we employ RAG to chunk communication documents, build a vector database, and enable local retrieval of knowledge from these documents. The combination of these two approaches enables the foundation model to efficiently acquire and generate knowledge in various communication task scenarios, demonstrating exceptional adaptability and accuracy. The workflow of the GRG process is as follows:

\subsubsection{Building the vector database}
After preprocessing the input communication documents, they are first chunked into smaller units. Each document chunk corresponds to an independent unit of information, such as a paragraph or technical concept. The document chunks are then encoded into high-dimensional vector embeddings using an embedding model and stored in a vector database. These vectorized representations retain the deep semantic features of the document chunks, making them suitable for large-scale document retrieval. A vector index is constructed in the database to facilitate fast retrieval of the most relevant document chunks by comparing query vectors with stored vectors.

\subsubsection{Building the graph database}
Using CommGPT, entities, attributes, and relationships in the communication documents are identified. By aligning entities, we construct a relationship network between different entities and attributes, forming a structured KG. In this KG, nodes represent entities, and edges represent the relationships between entities. The Neo4j graph database is used to store and manage the KG, supporting efficient graph queries \cite{9085182}. This allows the foundation model to analyze and process the complex network of communication entities and relationships, further enhancing its global understanding of specialized knowledge in communication.

\subsubsection{Joint retrieval of vectors and graphs}
The image encoder interprets the semantic content of the image and converts it into the text information needed for the query when the user's input contains image information. Then, the foundation model converts the user's query into a query vector and retrieves the most relevant document chunks from the vector database. The model compares the query vector with stored vectors to select the document chunk whose corresponding vector is the most similar to the query vector. The result of the vector retrieval represents the local knowledge relevant to the query.
Subsequently, the foundation model extracts key entities from the user's query and performs reasoning based on the relationships between entities in the graph database. It retrieves the attributes of these entities and their relationships with other entities. The result of the graph retrieval represents the global knowledge related to the query.

\subsubsection{Context-augmented generation}
The relevant document chunks retrieved from the vector database and the entity attributes and relationships inferred from the graph database are merged to form enriched contextual information, which is input into the fine-tuned CommGPT. At this stage, CommGPT is able to understand the terminology and technical background of the communications, reducing hallucinations and generating more accurate responses. For instance, when answering questions that involve multiple entities and technical relationships, the CommGPT is able to clearly display the connections between entities and provide accurate technical interpretations.

Examples of CommGPT applied to 3GPP queries are presented in Fig. \ref{fig:fig3}. 
\begin{figure}[htbp]
	\centering
	\includegraphics[width=9cm]{5.png}
	\caption{Examples of CommGPT applied to 3GPP queries.}
	\label{fig:fig3}
\end{figure}

\vspace{-10pt} % 增加10pt的垂直间距，可以调整大小

\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\section{Experiments}
\subsection{Experimental setup}
During the training phase, CommGPT utilizes the open-source Gemma 2-9b-instruct model \cite{team2024gemma} from Google as its base model, which is then pretrained and fine-tuned on the CommData dataset. The learning rate scheduler is set to cosine annealing, with the initial learning rate of the pretraining phase set to $5 \times 10^{-6}$ and the fine-tuning phase set to $1 \times 10^{-5}$. The optimizer is set to Adam. The rank for LoRA is set to 8, with a scaling factor of 16. Mixed-precision training with BF16 is employed.
Moreover, BLIP is used as the high-level image encoder, QOCR as the low-level image encoder, QAnything as the RAG system, and Milvus as the vector database. The KG is generated using GraphRAG in combination with CommGPT, and Neo4j is used as the graph database. 3GPP\_TR \cite{nikbakht2024tspec} dataset, which includes questions of three difficulty levels (easy, intermediate, and hard), is applied to evaluate the CommGPT and its competitors. The accuracy is defined as the ratio of correctly answered questions by LLMs to the total number of questions in 3GPP\_TR.

%The software environment for the experiments is as follows: PyTorch 2.2.2, PEFT 0.11.1, Transformers 4.42.3. The hardware environment consists of: CPU: 4 x Intel Xeon Platinum 8358, GPU: 6 x NVIDIA A800-SXM4-80GB, CUDA: 11.8, and the operating system: Ubuntu 22.04.3 LTS.

\subsection{Ablation experiment}
The competitors used in this experiment are as follows:

\subsubsection{BaseModel (Gemma 2-9b)}
The open-source foundation model with no pretraining or fine-tuning on CommData.

\subsubsection{CommGPT}
The communication foundation model pretrained and fine-tuned using CommData.

\subsubsection{CommGPT-R}
The CommGPT enhanced with RAG.

\subsubsection{CommGPT-GRG}
The CommGPT enhanced with both RAG and KG.

%The accuracy is defined as the ratio of correctly answered questions by the model to the total number of questions in 3GPP\_TR\_Questions. 


\begin{figure}[htbp]
	\centering
	\includegraphics[width=9cm]{3.png}
	\caption{The accuracy of LLMs in the ablation experiment.}
	\label{fig:fig4}
\end{figure}

The accuracy for each LLM is shown in Fig. \ref{fig:fig4}. The experimental results demonstrate a significant improvement in the accuracy of CommGPT for communication question-answering tasks. Initially, the accuracy of the BaseModel is only 37\%. After pretraining and fine-tuning on the CommData dataset, the accuracy of CommGPT increases to 54\%. This indicates that fine-tuning the BaseModel on a specialized communication dataset enables it to learn features and patterns more closely aligned with communication knowledge, thereby improving its ability to handle communication question-answering tasks. Furthermore, after integrating KG with RAG, the accuracy of the CommGPT-GRG model increases further to 91\%. This highlights that the GRG can provide multi-scale external knowledge retrieval, effectively supplementing the CommGPT’s inherent communication knowledge deficiencies and greatly enhancing its contextual understanding capability.

\subsection{Comparative experiment}
The competitors used in this experiment are as follows:
\subsubsection{LLaMA2-7B Series}
LLaMA 2-7B and LLaMA 2-7B-Instruct are second-generation open-source LLMs proposed by Meta, with 7 billion parameters.

\subsubsection{LLaMA3-8B Series}
LLaMA 3-8B and LLaMA 3-8B-Instruct are third-generation open-source LLMs proposed by Meta, with 8 billion parameters.

\subsubsection{GPT Series}
GPT-3.5 and GPT-4 are closed-source LLMs developed by OpenAI.

\subsubsection{Gemini 1.0}
A closed-source multimodal LLM developed by Google.

\subsubsection{Tele-LLM}
An LLM optimized for telecommunications, developed by Yale University, with 8 billion parameters \cite{RN17}.

\subsubsection{Tspec-LLM Series}
GPT-3.5-T, Gemini 1.0-T, and GPT-4-T are telecom-specific LLMs based on GPT-3.5, Gemini 1.0, and GPT-4, respectively, enhanced with the RAG \cite{nikbakht2024tspec}.

%\subsubsection{CommonGPT}
%The communication-specific foundation model proposed by us, combining RAG and KG techniques.

%The performance of these models was tested on the telecom question-answering dataset, 3GPP\_TR\_Questions.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=9cm]{4.png}
	\caption{The accuracy of LLMs in the comparative experiment.}
	\label{fig:fig5}
\end{figure}


The detailed experimental results are presented in Fig. \ref{fig:fig5}, which illustrates that the accuracy of open-source LLMs (e.g., the LLaMA2 and LLaMA3 series) is the lowest, while closed-source LLMs (e.g., the GPT series and Gemini 1.0) exhibit higher accuracy, with GPT-4 achieving the highest performance. Among the three groups of telecom-specific LLMs, the open-source Tele-LLM demonstrated performance comparable to that of the closed-source models, suggesting that training on specialized telecom data enhances the performance of open-source LLMs in communication. Furthermore, the accuracy of the Tspec-LLM series (e.g., GPT-3.5-T, Gemini 1.0-T, and GPT-4-T), which integrates RAG, shows significant improvement, indicating that RAG technology, by incorporating external telecom knowledge, enhances the LLM's ability to address telecom-related questions. Finally, CommGPT, which utilizes the GRG, achieved the highest accuracy, highlighting that the combination of KG and RAG facilitates a better understanding of domain-specific terminology and complex issues in communication, thereby providing more accurate and efficient responses.

%\section{Open Issues}
%\subsubsection{Model Resource Demands and Deployment Challenges}
%Although CommGPT performs excellently in communication applications, its large computational and storage requirements pose significant challenges for deployment on edge devices. To optimize deployment, future research should focus on model compression techniques (such as model distillation and quantization) and lightweight architectures, ensuring efficient operation in resource-constrained environments while reducing computational overhead.

%\subsubsection{Real-time inference and low latency requirements}
%In communication applications, particularly in scenarios with high real-time requirements like 6G networks, the inference latency of CommGPT could become a bottleneck. To address this, future research could explore low-latency inference optimization techniques, such as multi-query attention mechanisms and inference acceleration methods, to ensure fast response times and real-time processing capabilities.

%\subsubsection{Cross-domain knowledge integration and inference}
%The communication field encompasses specialized knowledge across multiple sub-domains, and efficiently integrating and reasoning over this cross-domain knowledge is a challenge. Future research could explore cross-domain knowledge fusion techniques, enabling CommGPT to flexibly understand and apply various communication technologies, thereby enhancing the accuracy and effectiveness of decision support.

%\subsubsection{Long-term memory and reasoning capabilitiess}
%In communication systems, especially in complex multi-turn dialogues and historical data analysis, enabling CommGPT to retain and utilize historical information for reasoning remains a challenge. Future research could focus on designing methods with long-term memory capabilities, allowing CommGPT to perform effective reasoning and decision-making across sessions and tasks.

\section{Conclusion}
In this paper, we proposed a multimodal foundational model for communications, named CommGPT. First, we introduced CommData, a specially curated dataset for CommGPT, which encompasses various types of data, including protocols, standards, papers, patents, and codes. Then, we employed CommData to pretrain and fine-tune the foundational model. Additionally, we designed multimodal encoders to enhance the CommGPT's ability to handle multimodal data in communication documents. Finally, we proposed a multi-scale learning approach that combines KG and RAG, assisting CommGPT in generating more precise and comprehensive responses for communications. Experimental results demonstrated that CommGPT outperforms other competitors in communication question-answering tasks, exhibiting the highest accuracy.

%\nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{EndNote}

\section*{Biographies}
\textbf{Feibo Jiang} (jiangfb@hunnu.edu.cn) is currently an Associate Professor at Hunan Normal University, China.

\textbf{Wanyun Zhu} (zhuwanyun@hunnu.edu.cn) is currently pursuing the master’s degree at Hunan Normal University, China.

\textbf{Li Dong} (Dlj2017@hunnu.edu.cn) is currently an Associate Professor at Hunan University of Technology and Business, China.



\textbf{Kezhi Wang} (Kezhi.Wang@brunel.ac.uk) is a Professor with the Department of Computer Science, Brunel University London, U.K.


\textbf{Kun Yang} (kyang@ieee.org) 
%received his PhD from the Department of Electronic \& Electrical Engineering of University College London (UCL), U.K. He 
is currently a Chair Professor in the School of Intelligent Software and Engineering, Nanjing University, China.


\textbf{Cunhua Pan} (cpan@seu.edu.cn) is currently a full professor in Southeast University, China. 

\textbf{Octavia A. Dobre} (odobre@mun.ca) is a professor and Canada Research Chair
Tier 1 at Memorial University, Canada.
%He serves as Lead Guest Editor of IEEE JSTSP special issue on RIS, Editor of IEEE CL, IEEE WCL, and IEEE ACCESS.

%\textbf{Robert Schober} (robert.schober@fau.de) is an Alexander von Humboldt Professor and the Institute for Digital Communications (IDC) in Friedrich-Alexander University (FAU) Erlangen-Nürnberg, Erlangen, Germany.
%\textbf{Jiangzhou Wang} (j.z.wang@seu.edu.cn) is currently a full professor in Southeast University, China.
%\textbf{Xiaohu You} (xhyu@seu.edu.cn) is currently a full professor in Southeast University, China.  He is also an Academician of the Chinese Academy of Sciences.

\end{document}





