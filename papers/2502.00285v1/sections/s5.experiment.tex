\section{Experiments}\label{sec:exp}

\subsection{Experimental Settings}\label{subsec:exp:setting}

\paragraph{Datasets} We use three widely used large trajectory datasets: Porto~\cite{porto}, Xian~\cite{didi}, and Germany~\cite{osmplanet}.
Following previous studies~\cite{neutraj,t3s,trajgat,trajcl}, we remove consecutive, duplicated points and filter out trajectories with fewer than 20 points or more than 200 points. 
We randomly sample 10,000 trajectories from each dataset, which is further split by 7:1:2 for training, validation, and test.
More details about the datasets can be found in Appendix~\ref{app:exp_setup}.

\paragraph{Competitors.}
We compare our \model\ model with six latest models: \textbf{NeuTraj}~\cite{neutraj}, \textbf{T3S}~\cite{t3s}, \textbf{TMN}~\cite{tmn}, \textbf{TrajGAT}~\cite{trajgat}, 
\textbf{TrajCL}~\cite{trajcl}, and 
\textbf{KGTS}~\cite{kgts}, which have been described in Section~\ref{sec:relatedwork}.


\paragraph{Implementation Details} 
We set the embedding dimensionality $d$ to 128. Parameter $\lambda$ in the loss function (Eq.~\eqref{eq:loss}) is set to 0.2. The trajectory encoder uses a one-layer self-attention.
We optimize \model\ by Adam~\cite{adam} with a maximum of 40 epochs and a batch size of 128. The early stop patience is set to 10.
The learning rate is initialized to 0.002 and decayed by 50\% after every 15 epochs. 
We report the average results of 3 runs for each experiment.

For the baseline models, we use their released code and default  settings, except for T3S which is not publicly available. We implement T3S following its proposal. The side length of grid cells is set as 100 meters for all baseline models.


\paragraph{Evaluation Metrics}
Following previous studies~\cite{neutraj,t3s,trajgat}, we use hit ratios \textbf{HR@10} and \textbf{HR@50} and  recall \textbf{R10@50} to evaluate model  performance.
HR@$x$ is the overlapping ratio between the ground-truth top-$x$ results and the predicted top-$x$ trajectories in test sets.
R10@50 is the recall of the ground-truth top-10 results in the predicted top-50 trajectories.


\input{tables/tab1.overall_results}

\subsection{Results}\label{subsec:exp:overall}

\paragraph{Overall Results} For each set of experiments, we follow the evaluation steps used by the competitors: (1)~train the models to approximate a non-learned measure (DTW, EDwP, or Fr\'echet); (2)~compute trajectories embeddings for a test set; (3)~compute the similarity between any two trajectories in the test set based on the embeddings; and (4) compute evaluation metrics by comparing the predicted similarity values
with the ground-truth (computed with the non-learned measure).


The overall results are shown in Table~\ref{tab:exp:overall}.
We see that \model\ is consistently the most accurate, achieving 32\%, 26\%, and 7\% improvements on average over the best baseline models in HR@10, HR@50, and R10@50, respectively. 
We made the following observations:

(1) \model\ achieves high R10@50 ($> 0.99$), ensuring that it can effectively filter out irrelevant results for $k$NN queries.


(2) The second best model is TrajCL on Porto and Germany, while this varies on Xian. This is because the density of trajectories in Xian is higher than that in Porto, making learning more difficult. DTW is better approximated by NeuTraj, EDwP by KGTS, and Fr\'echet by TrajCL.


(3) KGTS mostly has lower (i.e., worse) HR@50 than HR@10. This is because KGTS is trained without using any non-learned measure, and such unlabeled data is hard to contribute to approximate specific non-learned measures. 


\begin{figure}[ht]
 \vspace{-2mm}
    \hspace*{2mm}
    \includegraphics[width=0.46\textwidth]{plots/num_points_xian_legend.pdf} \\
     \vspace{-5mm}
     
     \hspace*{-2mm}
     \subfloat[{On Xian}~\label{fig:exp_num_points_xian_hr10}]{
        \includegraphics[width=0.23\textwidth]{plots/num_points_xian_hr10_large.pdf}
    }\hspace*{-2mm}
    \subfloat[{On Germany}~\label{fig:exp_num_points_germany_hr10}]{
        \includegraphics[width=0.23\textwidth]{plots/num_points_germany_hr10_large.pdf}
    } 
    \vspace{-2mm}
    \caption{Varying the number of points on trajectories to approximate Fr\'echet (OOM: out-of-memory error).}\label{fig:exp_num_points}
\end{figure}
\vspace{-3mm}

\input{tables/tab2.apdx.ablation}

\paragraph{Varying the Number of Points on Trajectories ($n$)} 
We investigate the effectiveness of TSMini on longer trajectories.
We randomly sample another two datasets of 10,000 trajectories from Xian and Germany (Porto does not have as many long trajectories), with 200 to 400, and 400 to 800 points on trajectories, respectively.
We report HR@10 on approximating Fr\'echet. Similar result patterns are observed on other measures and in other metrics, and thus omitted.

Figure~\ref{fig:exp_num_points} shows that, overall, the accuracy of all methods decreases as the length of trajectories increases, since longer trajectories bring challenges in representation learning.

\model\ consistently outperforms all baselines, especially achieving higher HR@10 when $n=(400,800]$ compared to other methods at $n=[20,200]$. This highlights the superior capability of~\model\ on learning long trajectories. 
TMN and TrajGAT incur out-of-memory (OOM) errors when $n=(400,800]$. This is because TMN requires computing the correlation between all point pairs in two trajectories, while TrajGAT constructs large graphs to represent long trajectories, both of which are memory-intensive.

We also study the impact of training set size, showing that \model\ is robust against it.  See Appendix~\ref{app:training_set_size} for the results. 

\paragraph{Ablation Study }\label{subsec:exp:ablation}

We compare \model\ with three model variants: 
(1)~\textbf{\model-w/o-S} replaces the sub-view encoder SVEnc with the cell encoder used in T3S and TrajCL;
(2)~\textbf{\model-w/o-K} removes the $k$NN-guided loss from Eq.~\eqref{eq:loss};
(3)~\textbf{\model-w/o-SK} uses the cell encoder and the MSE loss. 
We repeat the experiments as above.


As Table~\ref{tab:apdx:ablation} shows, both the $k$NN-guided loss and SVEnc 
are important to the overall model performance. They improve the hit ratios by 88\% (\model-w/o-K vs. \model) and 41\% (\model-w/o-S vs. \model) in HR@10 on average, respectively.
In most cases, the $k$NN-guided loss plays a more critical role, especially in learning DTW and EDwP, whereas the sub-view modeling is more effective in learning Fr\'echet.

We further investigate the impact of SVEnc and the self-attention module in TrajEnc. As the results in Appendix~\ref{app:diff_attention} show, SVEnc is highly effective, and our adapted self-attention module outperforms the vanilla self-attention.



\paragraph{Parameter Study}\label{subsec:exp:parameter}
We study the impact of  batch size $N$, the hyper-parameter $\lambda$ in the loss function, and the number of self-attention layers, by
repeating the experiments as before and reporting HR@10 on Xian. 
Full results of parameter study can be found in Appendix~\ref{app:parameterstudy}.

\begin{figure}[ht]
    \centering
    \vspace{-5mm}
    \hspace*{-2mm}
    \subfloat[{Batch size $N$ in Eq.~\eqref{eq:knnloss_6}}~\label{fig:paramstudy:exp_batchsize_xian_hr10}]{
        \includegraphics[width=0.24\textwidth]{plots/batchsizes_xian_legend.pdf}
    } 
    \subfloat[{$\lambda$ in Eq.~\eqref{eq:loss}}~\label{fig:paramstudy:exp_lambda_xian_hr10}]{
        \includegraphics[width=0.24\textwidth]{plots/lambdas_xian.pdf}
    } 
    \vspace{-2mm}
    \caption{Parameter study results}\label{fig:paramstudy}
    \vspace{-3mm}
\end{figure}

Figure~\ref{fig:paramstudy:exp_batchsize_xian_hr10} shows the results on varying $N$. The HR@10 results generally increase with $N$. A larger batch size (i.e., a larger $N$ in Eq.~\eqref{eq:knnloss_6}) allows \model\ to learn more relative similarity values from the $k$NN-guided loss.
We use $N = 128$ by default to keep inline with the baseline models.


Figure~\ref{fig:paramstudy:exp_lambda_xian_hr10} shows the results on varying $\lambda$ in Eq.~\eqref{eq:loss}. Except when $\lambda$ is 0 or 1, $\lambda$ has a marginal impact on \model.
Such results indicate that both loss terms contribute to the accuracy of \model, while using the $k$NN-guided loss alone ($\lambda=0$) is more effective than using the MSE loss ($\lambda=1$).

\paragraph{Model Efficiency}
We further study the time and space efficiency of \model.
Table~\ref{tab:exp:efficiency} shows the results for learning to approximate Fr\'echet, where the reported time is end-to-end. 
\model\ is efficient in both space and time: 
(1)~It is one of the models with the fewest parameters, trailing behind NeuTraj and TMN that solely rely on RNNs, and T3S that is based on self-attention without FFNs.
These models have low accuracy as reported above. 
(2)~It is the fastest in training, benefiting from the $k$NN-guided loss that leads to fast convergence.
(3)~It is among the fastest in inference, due to the highly efficient parallelization of self-attention computation on GPUs.
TMN cannot be used as an encoder on its own (Section~\ref{sec:relatedwork}) and hence is omitted here.

\input{tables/tab3.efficiency}

