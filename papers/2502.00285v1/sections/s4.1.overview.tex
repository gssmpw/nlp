
Figure~\ref{fig:overview} illustrates our overall model structure.
\model\ first encodes trajectories into sub-view embeddings through a \emph{sub-view encoder} (Section~\ref{subsec:method_patch}) and then  maps the sub-view embeddings into trajectory embeddings through a \emph{trajectory encoder} (Section~\ref{subsec:traj_enc}).
Finally, \model\ leverages a \emph{$k$NN-guided loss} for model training (Section~\ref{subsec:method_loss}).


\subsection{Sub-view Encoder}\label{subsec:method_patch}


Given a raw trajectory input $T$, the sub-view encoder (denoted as \textbf{SVEnc}) first transforms $T$ into sub-sequences,
which we call \emph{sub-views}, instead of modeling individual points as in previous works~\cite{neutraj,t3s,trajcl}.
Trajectory sub-views can capture not only the spatial features of individual points but also local movement patterns. They provide multi-grained spatial features while reducing the input length for the subsequent trajectory encoder, hence leading to embeddings that better preserve trajectory features.

SVEnc encodes the sub-views of $T$ (augmented as below) into embeddings $\mathbf{X} \in \mathbb{R}^{m\times d}$, where $m$ denotes the number of sub-views and $d$ is the embedding dimensionality as before. 








\begin{figure}[th]
    \centering
    \includegraphics[width=0.98\columnwidth]{figures/sub-view_3.pdf}
    \vspace{-2mm}
    \caption{Multi-grained trajectory sub-view modeling.}\label{fig:subview}
    \vspace{-3mm}
\end{figure}



\paragraph{Feature Preparation} 
Given an input raw trajectory $T$, we augment it by adding spatial features to each point. 
A point $p_i \in T$ is expanded to a vector of  seven elements, including the longitude, the latitude, the lengths of segments $\overline{p_{i-1} p_i}$ and $\overline{p_i p_{i+1}}$, the angle between the $x$-axis and $\overline{p_{i-1} p_i}$, the angle between the $x$-axis and $\overline{p_i p_{i+1}}$, and the interior angle between $\overline{p_{i-1} p_i}$ and $\overline{p_i p_{i+1}}$.
We use $\mathbf{S} \in \mathbb{R}^{n\times 7}$ to denote the augmented representation of $T$.

\paragraph{SVEnc}
Figure~\ref{fig:subview} shows the structure of SVEnc. 
Inspired by time series representation learning~\cite{patchfy,timellm}, we stack 
three convolutional sub-modules sandwiched between two linear layers. Each convolutional sub-module contains a \emph{1D convolutional} (Conv1D) layer, a \emph{batch normalization}~\cite{batchnorm} layer, and a \emph{LeakyReLU} activation function.
For all Conv1Ds, the kernel size $w_{k}$ is set as 3 and the stride $w_{s}$ is set as 1. The dimensionality of each layer is labeled in Figure~\ref{fig:subview}. 


Note that we do not explicitly create sub-views. Instead, we feed the entire trajectory $\mathbf{S}$ into SVEnc, where the sub-views are created and encoded simultaneously.
The convolution kernels move  along the dimension of trajectory points, which can be seen as first forming a sequence of sub-views of every $w_{k}$ points and then producing embeddings for the sub-views. 
The first Conv1D layer processes sub-views of points, while the second and third Conv1D layers aggregate every $w_{k}$ sub-views from the previous layer, summarizing the movement patterns of longer sub-sequences.  

The output $\mathbf{X} \in \mathbb{R}^{m\times d}$ ($m < n$) of SVEnc  is fed into the trajectory encoder to generate the trajectory embedding $\mathbf{h}$.


\subsection{Trajectory Encoder}\label{subsec:traj_enc}
The trajectory encoder (denoted as \textbf{TrajEnc}) can be any existing sequence embedding model. 
We adapt the self-attention-based backbone model from Llama-2~\cite{llama2} as TrajEnc for  \model. This choice comes with two advantages: (1) Self-attention is both effective and efficient for trajectory similarity learning~\cite{t3s,trajsimi_survey}. 
(2) The self-attention-based backbone model in Llama-2 is shown to be more effective than the vanilla self-attention~\cite{transformer} for 
capturing sequential correlation (see  Appendix~\ref{app:diff_attention} in supplementary material; same below).


Next, we detail TrajEnc. 
The right part of Figure~\ref{fig:overview} illustrates TrajEnc.
The sub-view embeddings $\mathbf{X}$ first go through a \emph{root mean square normalization} layer (RMSNorm)~\cite{rmsnorm} to obtain normalized input, for more stable model training.
We abuse the notation slightly and also use  $\mathbf{X}$ to denote the normalized input for conciseness, when there is no ambiguity.

Then, the normalized $\mathbf{X}$ is fed into a \emph{multi-head self-attention} layer (MHSA) to learn the hidden correlations between trajectory sub-views and generate new embeddings $\mathbf{H}$: 
{\small
\begin{equation}\label{eq:selfattention}
\begin{split}
    \mathbf{Q}^{j}, \mathbf{K}^{j}, \mathbf{V}^{j} & = \mathbf{X}\mathbf{W}_{Q}^{j}, \mathbf{X}\mathbf{W}_{K}^{j}, \mathbf{X}\mathbf{W}_{V}^{j}, \\
    \mathbf{H}^j & = \mathrm{Softmax} (\alpha_{A} \cdot
    \mathrm{RoPE}(\mathbf{Q}^{j}, \mathbf{K}^{j})) \mathbf{V}^{j}, \\
    \mathbf{H} & = \mathrm{Concat}(\mathbf{H}^{1},\cdots, \mathbf{H}^{j},\cdots, \mathbf{H}^{h})\mathbf{W}_{H}.
\end{split}
\end{equation}
}
Here, $\mathbf{W}_Q^j$, $\mathbf{W}_K^j$, and $\mathbf{W}_V^j$ (all in $\mathbb{R}^{d\times (d/h)}$) are learnable weights of the $j$-th self-attention head ($h$ heads in total), $\alpha_{A}$ is a scaling factor, and $\mathbf{W}_{H} \in \mathbb{R}^{d\times d}$. $\mathrm{RoPE}(\cdot)$ computes the attention coefficients and integrates with positional encodings. We adopt the \emph{Rotary Position Embedding} (RoPE)~\cite{rope} for its capability in exploiting the relative position dependency among the sub-views.  

Then, we apply a \emph{residual connection}~\cite{resnet} to the output of the self-attention layer and the initial input $\mathbf{X}$ (before normalization), i.e., \emph{adding} the two values. This alleviates gradient vanishing and yields smooth gradients. 


Next, the sum from the residual connection is fed into another sub-module that consists of an RMSNorm, a feed forward network (FFN) with an \emph{SwiGLU} activation function~\cite{glu}, and a residual connection, which brings the nonlinear representation learning capability.
The output of this sub-module, $\widetilde{\mathbf{H}}$, has the same shape as $\mathbf{H}$ and $\mathbf{X}$.

All layers above are stacked (i.e., repeated) in Llama-2, while we found that such repetition is not necessary for our task (see~Appendix~\ref{app:parameterstudy}) and thus we do not stack the layers.

Finally, we apply \emph{average pooling} to the hidden output $\widetilde{\mathbf{H}}$ to obtain the final trajectory embedding of $T$, i.e., $\mathbf{h} \in \mathbb{R}^{d}$. 

\paragraph{Discussion} 
TrajEnc in \model\ is not exactly the self-attention model in Llama-2. 
TrajEnc is an encoder, while the self-attention model in Llama-2 is closer to a Transformer decoder. 
Computing a trajectory embedding by our model can be viewed as encoding a prompt in LLMs without the inference step. To adapt a ``decoder'' into an ``encoder'', we made two main changes as follows:

(1) TrajEnc does \emph{not} adopt the \emph{grouped-query attention} (GQA)~\cite{gqa} used by Llama-2 for fast inference, which shares attention coefficients among attention heads.
TrajEnnc has much few parameters already and hence we do not need GQA which sacrifices accuracy for efficiency.

(2) TrajEnc does \emph{not} use \emph{causal masking} that prevents future token inputs from impacting attention computation in LLMs. 
This is because an entire trajectory is known for encoding in our task. To encode a  point, 
it is more beneficial to consider both proceeding and subsequent (i.e., future) points on the trajectory, instead of just the preceding ones.

