\subsection{$K$NN-Guided Optimization}\label{subsec:method_loss}


Next, we present our $k$NN-guided loss. 
To simplify the symbols, we abuse the notation slightly and use the same symbol to represent both a random variable and an observed value of the variable, e.g., $\mathbf{x}$, when the context is clear.  

\begin{figure}[th]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/probability.pdf}
    \vspace{-3mm}
    \caption{Variable dependency for the $k$NN-guided loss.}\label{fig:probability}
    \vspace{-2mm}
\end{figure}


\paragraph{$K$NN-guided Loss}
We formulate the $k$NN-guided loss $\mathcal{L}_{knn}$ in a probabilistic manner, as shown in Figure~\ref{fig:probability}. Let $\mathbf{x}$ be a given variable for a list of size $N$ of the predicted similarity scores between a trajectory and all trajectories in the training set.
Let $k \in \mathcal{K}$ be the query parameter $k$ of $k$NNs, where $\mathcal{K} = \mathbb{N}_{\leq N}$.
We introduce $\phi \in \Phi$  as a hidden variable determined by $\mathbf{x}$ and $k$. 
Here, $\phi$ denotes a partially sorted list of size $N$, where the first $k$ elements are larger than the rest, i.e., the first $k$ elements in $\phi$ form the $k$NN result set.
We use $\mathbf{y}$ to represent the ground-truth list of similarity scores of size $N$, which are impacted by the input variables $\mathbf{x}$, $k$, and $\phi$. 
The likelihood function for $\mathbf{y}$ conditioned on $\mathbf{x}$ is:
{\small
\begin{equation}\label{eq:knnloss_1}
    P(\mathbf{y} | \mathbf{x}) =  \sum_{k \in \mathcal{K}} P(\mathbf{y} | \mathbf{x}, k) P(k).
\end{equation}
}

Given a set of $N$ training instances $\{(x, y)\}$, where $x$ is a list of predicted similarity scores between a trajectory and all trajectories in the training trajectory set, and $y$ is the corresponding ground-truth list,
the $k$NN-guided loss $\mathcal{L}_{knn} :(\mathbf{y}, \mathbf{x}) \rightarrow \mathbb{R}$ is designed to minimize the negative log-likelihood, as shown in Eq.~\eqref{eq:knnloss_1}:
{\small
\begin{equation}\label{eq:knnloss_2}
    \mathcal{L}_{knn}(\mathbf{y}, \mathbf{x}) = -\log_2 \sum_{k \in \mathcal{K}} P(\mathbf{y} | \mathbf{x}, k) P(k).
\end{equation}
}

This log-sum loss is difficult to optimize directly due to its poor numerical stability (when the summation of probabilities stays close to 1, the loss will remain near 0 causing the gradient vanishing problem).  To address this issue, recognizing the convexity of the negative logarithm function, we leverage \emph{Jensen's inequality} to derive an upper bound for $\mathcal{L}_{knn}$. Specifically, for a convex function $\psi$, Jensen's inequality states that $\psi(\sum x_i) \le \sum (\psi(x_i) )$, which allows us to construct a surrogate upper bound for Eq.~\eqref{eq:knnloss_2}, as shown in Eq.~\eqref{eq:knnloss_2_1}. Further, since $P(k)$ represents the probability of $k$, its value always lies between 0 and 1. Thus, the upper bound simplifies to Eq.~\eqref{eq:knnloss_2_2}, which we denote as $\mathcal{L}_{knn}^{+}$.
{\small
\begin{align}
    \mathcal{L}_{knn}(\mathbf{y}, \mathbf{x}) & = -\log_2 \sum_{k \in \mathcal{K}} P(\mathbf{y} | \mathbf{x}, k) P(k) \nonumber\\
    & \le -\sum_{k \in \mathcal{K}} \log_2 P(\mathbf{y} | \mathbf{x}, k) P(k) \label{eq:knnloss_2_1}\\
    & \le -\sum_{k \in \mathcal{K}} \log_2 P(\mathbf{y} | \mathbf{x}, k).\label{eq:knnloss_2_2}
\end{align}
}

To proceed with Eq.~\eqref{eq:knnloss_2_2}, we expand  $P(\mathbf{y} | \mathbf{x}, k)$ as:
{\small
\begin{align}
    P(\mathbf{y} | \mathbf{x}, k) & = \frac{P(\mathbf{y},\mathbf{x},k)}{P(\mathbf{x})P(k)} = \frac{1}{P(\mathbf{x})P(k)} \sum_{\phi \in \Phi} P(\mathbf{y}, \mathbf{x}, k, \phi) \nonumber\\
    & = \frac{1}{P(\mathbf{x})P(k)} \sum_{\phi \in \Phi} P(\mathbf{x}) P(k) P(\phi | \mathbf{x}, k) P(\mathbf{y} | \mathbf{x}, k, \phi) \nonumber\\
    & = \sum_{\phi \in \Phi} P(\phi | \mathbf{x}, k) P(\mathbf{y} | \mathbf{x}, k, \phi) \nonumber\\
    & =  P(\bar{\phi}| \mathbf{x}, k) P(\mathbf{y} | \mathbf{x}, k, \bar{\phi}) + \hspace{-3mm}\sum_{\phi \in \Phi \setminus \bar{\phi}} P(\phi | \mathbf{x}, k) P(\mathbf{y} | \mathbf{x}, k, \phi) \nonumber\\
    & = P(\mathbf{y} | \mathbf{x}, k, \bar{\phi}).\label{eq:knnloss_3}
\end{align}
}
Here, in the first step, we exploit Bayes' rule and the fact that $\mathbf{x}$ and $k$ are independent. The subsequent steps use the dependency between $\mathbf{y}$, $\mathbf{x}$, $k$, and $\phi$ as discussed at the start of this section.  
For the final step, we reduce the summation over all possible $\phi$ to a special $\bar{\phi}$, which represents the case where all the elements in $\phi$ are sorted in descending order. Therefore, $P(\bar{\phi} | \mathbf{x}, k)=1$ and $P(\Phi \setminus \bar{\phi} | \mathbf{x}, k)=0$, since we only examine sorted $\phi$ in \model.


By substituting Eq.~\eqref{eq:knnloss_3} into Eq.~\eqref{eq:knnloss_2}, $\mathcal{L}_{knn}^{+}$ becomes: 
{\small
\begin{equation}\label{eq:knnloss_4}
    \mathcal{L}_{knn}^{+}(\mathbf{y}, \mathbf{x})  = -\sum_{k \in \mathcal{K}} \log_2  P(\mathbf{y} | \mathbf{x}, k, \bar{\phi}).
\end{equation}
}

We now formulate the likelihood $P(\mathbf{y} | \mathbf{x}, k, \phi)$ using the  Bradley-Terry model with a reward function $w(\mathbf{y})$~\cite{bradleyterry}.
The Bradley-Terry model estimates the outcome of pairwise comparisons, determining the probability that one item is preferred over another. Formally, 
{\small
\begin{equation}\label{eq:knnloss_bt}
P(y_i > y_j | x_i, x_j, k, \phi) = \sigma(x_i - x_j),
\end{equation}
}
where $\sigma$ is the \emph{sigmoid} function, and $x_i$ (or $y_i$) denotes the $i$-th element in $x$ (or $y$), $i \in [1, k]$ and $j \in (k, N]$. The Bradley-Terry model aligns well with the $k$NN query-based objective. They both aim to find a list where  the probability that the $i$-th trajectory is more similar to the query trajectory than the $j$-th trajectory based on the predicted similarity scores. 


Substituting the Bradley-Terry reward function in Eq.~\eqref{eq:knnloss_bt} into Eq.~\eqref{eq:knnloss_4}, we can further derive an upper bound for $\mathcal{L}_{knn}^{+}$.
{\small
\begin{align}
    \mathcal{L}_{knn}^{+}(\mathbf{y}, \mathbf{x}) & = -\sum_{k \in \mathcal{K}} \log_{2} \prod_{i = 1}^{k} \prod_{j = k}^{N} P(y_i > y_j | x_i, x_j, k, \bar{\phi})^{w(\mathbf{y})} \nonumber\\
    & = -\sum_{k \in \mathcal{K}} \sum_{i = 1}^{k} \sum_{j = k}^{N} \log_{2} P(y_i > y_j | x_i, x_j, k, \bar{\phi})^{w(\mathbf{y})} \nonumber\\
    & = -N \sum_{i = 1}^{k} \sum_{j = k}^{N} \log_{2} P(y_i > y_j | x_i, x_j,  \bar{\phi})^{w(\mathbf{y})} \label{eq:knnloss_5_1}\\
    & \leqslant -N\sum_{y_i > y_j} \log_{2} P(y_i > y_j | x_i, x_j, \bar{\phi})^{w(\mathbf{y})}. \label{eq:knnloss_5}
\end{align}}
The term in Eq.\eqref{eq:knnloss_5} turns out to be a scaled version of the \emph{LambdaLoss}~\cite{lambdaloss}, a  loss  often used in \emph{learning-to-rank} problems. This connection, though not immediately obvious, implies a potential synergy between $k$NN guidance and learning-to-rank techniques.

We omitted a few steps in the derivation of Eqs.~\eqref{eq:knnloss_5_1} and \eqref{eq:knnloss_5} due to space limit. These can be found in Appendix~\ref{app:eq_steps}. 


The upper bound introduced in Eq.~\eqref{eq:knnloss_5}, denoted as $\mathcal{L}_{knn}^{++}$, is derived based on the constraint that for any given $k$,  we compare elements only between those ranked before and after the $k$-th position.  As a result, the total number of comparisons is limited to a maximum of $N^2$. Note also that $\mathcal{L}_{knn}^{++}$ is simpler to implement, while it still captures the essential characteristics of the $k$NN guidance.


\paragraph{The Overall Loss Function} We implement the reward function $w(\mathbf{y})$ in $\mathcal{L}_{knn}^{++}$ using the approximated Normalized Discounted Cumulative Gain (NDCG) (Eq.~(16) by~\citeauthor{lambdaloss}~\citeyear{lambdaloss}), which is an  effective ranking metric.  Combining Eq.~\eqref{eq:knnloss_bt} with the NDCG reward function, and substituting them into Eq.~\eqref{eq:knnloss_5}, we obtain 
the $\mathcal{L}_{knn}^{++}$ over all training samples by calculating $\mathbb{E}_{\{(x, y)\}}\mathcal{L}_{knn}^{++}(y, x)$ as follows:
{\small
\begin{equation}\label{eq:knnloss_6}
    \mathcal{L}_{knn}^{++}  =  \mathop{\mathbb{E}}_{\{(x, y)\}} -N\sum_{y_i > y_j} \delta_{i,j}(G_i - G_j) \log_{2} \sigma(x_i - x_j).
\end{equation}
}
Here, {\small$G_i = \frac{2^{y_i}-1}{\mathrm{maxDCG}}$} with  {\small$ \mathrm{maxDCG}=\sum_{i}\frac{2^{y_i}-1}{\log_2(i+1)}$}, and the difference of discounts {\small$ \delta_{i,j} = \frac{1}{\log_2(j-i+1)} - \frac{1}{\log_2(j-i+2)}$}.

Our \emph{overall loss function} $\mathcal{L}$ combines the $k$NN-guided loss with a weighted version of the MSE loss, denoted as $\mathcal{L}_{mse}$:
{\small
\begin{equation}\label{eq:mse_loss}
\mathcal{L}_{mse} = \displaystyle \mathop{\mathbb{E}}_{ T_i, T_j \in \mathcal{D}} w_{i,j} \cdot \big(f(T_i, T_j) - f'(T_i, T_j)\big)^2.
\end{equation}
}

We use $f(T_i, T_j)$ as the weights $w_{i,j}$. The MSE loss allows \model\ to also learn from the absolute similar scores.  We balance the two loss terms with  parameter $\lambda \in (0,1)$:
\begin{equation}\label{eq:loss}
    \mathcal{L} = \lambda\mathcal{L}_{mse} + (1-\lambda)\mathcal{L}_{knn}^{++}.
\end{equation}

