\section{LLMs for Trajectory Similarity Learning}\label{sec:llm}
LLMs~\cite{gpt4,llama2,gemini,llama1} have reported state-of-the-art results for a variety of natural language processing (NLP) tasks, such as question answering and summary generation. They have been applied to trajectory data to predict the next point~\cite{llm4poi,trajllm}. We present the first study on applying LLMs for trajectory similarity learning.



\subsection{Model Design Considerations}\label{subsec:llm_discussion}
\textbf{Advantages of LLMs.} LLMs have a large number of layers (e.g., over 100) and parameters (e.g., billions). 
This leads to a few benefits when applying LLMs to trajectory similarity learning:
 
(1)~Pre-trained LLMs have the capability to analyze numerical inputs~\cite{timellm,lagllama}, which enables them to process trajectory data. They can provide a high-level understanding of trajectories, such as summarizing the movement patterns (cf. Figure~\ref{fig:prompt}). These features are crucial for trajectory similarity learning.

(2)~The large model size enables LLMs to understand a long context,  capturing dependencies between distant points within a trajectory and the global movement patterns.


\textbf{Limitations of LLMs.} There are also a few \emph{Limitations}:

(1)~LLMs require significant computational resources and could be slow at inference, which defeats the purpose of efficient trajectory similarity computation using learning-based approaches. 

(2)~The tokenizers of LLMs may split a number (e.g., a trajectory point coordinate) into multiple tokens. This disrupts the integrity of the coordinates and hinders  comprehension of trajectories.
 
(3)~LLMs are pre-trained on textual data and are not directly applicable to trajectory data. 
To adapt them for trajectory data, the LLMs need to be fine-tuned with substantial domain knowledge.

Due to the issues above, applying LLMs directly (i.e., as the trajectory encoder) has both efficiency (Limitation~1) and effectiveness (Limitations~2 and~3) issues. Instead, we propose to exploit the advantages of LLMs in summarizing the movement patterns of trajectories (i.e., producing additional knowledge about the trajectories). The movement pattern summaries are then learned by our \model\ to obtain trajectory embeddings that enhance the trajectory similarity prediction accuracy. 
By doing so, LLMs are only utilized to train \model\ while it is detached at test time such that we retain the high inference efficiency of \model. 


\begin{figure}[th]
    \centering
    \includegraphics[width=0.98\columnwidth]{figures/llm_pipeline_6.pdf}
    \caption{The pipeline of \model\ integrated with LLMs, a.k.a., the pre-training stage.}\label{fig:llm_pipeline}
\end{figure}


\subsection{Powering \model\ with LLMs}\label{subsec:llm_empirical}
We utilize LLMs to help train \model\ in two stages: a pre-training stage and a fine-tuning stage. 
(1) The pre-training stage is designed to learn general knowledge about trajectory similarity.
(2) The fine-tuning stage adapts the pre-trained model to approximate a given non-learned trajectory similarity measure. 
Here, we  focus on the pre-training stage, as the fine-tuning stage mainly follows the process described in Section~\ref{subsec:method_loss}.

\textbf{Pre-training Stage.}
Figure~\ref{fig:llm_pipeline} shows the overview of  the pre-training stage, which  integrates the trajectory knowledge of LLMs into our model~\model.



Overall, we use an LLM as a \emph{knowledge enhancer} to generate a textual \emph{summary} describing an input trajectory $T$, which is then converted into 
a \emph{summary embedding} by an \emph{LLM text embedder}. We compute the similarity between the summary embeddings. The similarity values are then used as supervision signals for \model\ pre-training, just like the similarity values computed by some non-learned measure are used for training \model\ in the experiments in Section~\ref{sec:exp}. 
In parallel, we encode $T$ and its augmented variants $T'$ and $T''$ with \model. Using $T'$ and $T''$ together with a contrastive loss enables learning more robust trajectory representations with contrastive learning (CL), following TrajCL~\cite{trajcl}. 


Algorithm~\ref{algo:llm} elaborates the procedure of this stage.
Given a batch of input trajectories, we first run a CL forward process. We generate two types of augmented trajectories of the input trajectories by randomly masking and randomly shifting some points on the trajectories. The augmented trajectories are turned into embeddings using \model. Both types of augmented trajectories are considered to be \emph{positive samples}, i.e., trajectories that are similar should share similar embeddings with the corresponding input trajectories. \emph{Negative samples}, which are supposed to be different from the trajectories of the current input batch, are obtained from 
a queue that stores the trajectory embeddings from previous batches.
We adopt the InfoNCE loss~\cite{infonce} to guide our model to maximize the similarity  between a trajectory and positive samples and minimize the similarity between  the trajectory and its negative samples.

Next, we run the LLM forward process. 
For the batch of input trajectories, we leverage an LLM as a knowledge enhancer to produce trajectory summaries in text.
Then, an LLM text embedder encodes the trajectory summaries together with the corresponding original trajectories into the summary embeddings. 
Our intuition is to use the trajectory summaries generated by the LLM knowledge enhancer as supervision signals for \model\ at the pre-training stage. The similarity between the generated summaries can be considered as knowledge from the LLM knowledge enhancer on what makes two trajectories similar.
We compute such similarity values between the summary embeddings. These similarity values form the ground-truth similarity, while the  similarity computed by the embeddings generated by \model\ is the model prediction. We apply the \emph{triplet margin loss} for model optimization based on the supervision signals from the LLM knowledge enhancer.


The InfoNCE loss and the triplet margin loss   together form the overall loss function for model training, weighted by a hyper-parameter $\alpha$




\input{sections/algo1.llm}

\textbf{Fine-tuning Stage.} 
The pre-trained~\model\  is fine-tuned with a given non-learned measure, following the procedures described in Section~\ref{subsec:method_loss}. 
Note that LLMs do not participate in the fine-tuning stage or model inference (i.e., model testing), to satisfy the efficiency requirement of~\model\ for trajectory similarity computation.


\subsection{Experiments}\label{subsec:llm_exp}

\textbf{Experimental Settings and Implementation Details.} 
We denote the \model\ powered by LLMs and contrastive learning as described above as \textbf{\model-LLM*}.
In~\model-LLM*, the knowledge enhancer should be an advanced LLM that is able to understand and summarize the key features of an input trajectory, while the text embedder can be a smaller LLM as its task is just to convert a text summary into an embedding. We implement knowledge enhancer and the text embedder with LangChain\footnote{\url{https://www.langchain.com/}} and Ollama\footnote{\url{https://www.ollama.com/}}. We use the pre-trained Llama3-70b\footnote{\url{https://ollama.com/library/llama3:70b-instruct-q4_0}} with the Q4\_0 quantization as the knowledge enhancer and the pre-trained 
mxbai-embed-large\footnote{\url{https://ollama.com/library/mxbai-embed-large:335m-v1-fp16}} with the F16 quantization as the text embedder. The inference temperature of the LLM knowledge enhancer is set as 0.7.


We randomly sample 10,000 trajectories that have not been used in the experiments in Section~\ref{sec:exp} for pre-training \model-LLM*.
We use \emph{point shifting} and \emph{point masking} as the augmentation methods~\cite{trajcl} in CL, where the maximum shift distance is set as 200 meters and the masking ratio is set as 0.3.  All other parameters related to CL are the same as the default parameters of TrajCL. 
The hyper-parameter $\alpha$ in the loss function is 0.2.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/prompt-wide.pdf}
    \vspace{-2mm}
    \caption{A prompt and response example for the LLM  knowledge enhancer to generate trajectory summaries.}\label{fig:prompt}
    \vspace{-1mm}
\end{figure*}

We also introduce two model variants of~\model-LLM* to study the effectiveness of each type of training signals. 
\textbf{\model-CL} denotes the model variant with contrastive learning only. 
\textbf{\model-LLM} denotes the variant that uses the LLMs only.

\textbf{Prompts for the LLM Knowledge Enhancer.}
We use an example to show the prompt and the response of the LLM knowledge enhancer, which are used to produce trajectory summaries for \model-LLM* and \model-LLM. 
As shown in Figure~\ref{fig:prompt}, the LLM knowledge enhancer summarizes an input trajectory from four aspects, including the endpoints, the minimum bounding  box, the significant points, and the movement.
These four features together represent a unique pattern of the trajectory.
Here, the endpoints and the minimum bounding box of a trajectory are deterministic, while the significant points and the movement can vary if the trajectory is analyzed by different techniques -- we exploit the unique insight of LLMs to detect these features.





\textbf{Performance Results.} 
We repeat the experiments in Section~\ref{subsec:exp:overall} and report the results of approximating Fr\'echet on all the datasets in Table~\ref{tab:llm}, as similar results are observed under the other settings.

\input{tables/tab5.llm}

We observe minor accuracy improvements using either LLMs or contrastive learning. When both optimization techniques are used, slightly larger improvements are obtained, at the cost of a high model training time. These findings are consistent with those reported in a 
recent study on using LLMs for learning time series which is also sequential and numerical data~\cite{llmts_exppaper}.

Several factors contribute to such results, including the limitations discussed in Section~\ref{subsec:llm_discussion}. Besides, the knowledge acquired from LLMs at the pre-training stage is difficult to transfer to similarity predictions. 
Nevertheless, \model-LLM* achieves over 2\% improvement in the hit ratios over \model, showing the potential of an LLM-based solution. 
These results call for further investigations on the best approach to exploit LLMs for trajectory data analysis. 

