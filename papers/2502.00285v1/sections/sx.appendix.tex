\appendix


\section{Additional Steps in Eqs.~\eqref{eq:knnloss_5_1} and \eqref{eq:knnloss_5}}\label{app:eq_steps}
We elucidate the omitted steps in the derivation of Eqs.~\eqref{eq:knnloss_5_1} and \eqref{eq:knnloss_5}.  
Here, we adopt a uniform assumption on $k\in[1, N]$, i.e., $P(k)=\frac{1}{N}$. 
Recall that in Eq.~\eqref{eq:knnloss_3}, we have shown the interchangeability between $P(\mathbf{y} | \mathbf{x}, k, \bar{\phi})$ and $P(\mathbf{y} | \mathbf{x}, k)$. Thus, we can derive:
% {\small
\begin{align}
    P(\mathbf{y} | \mathbf{x}, k, \bar{\phi}) &= P(\mathbf{y}|\mathbf{x}, k) = \frac{P(\mathbf{y},\mathbf{x}, k)}{P(\mathbf{x})P(k)} \nonumber\\
    &= \frac{N}{P(\mathbf{x})}\cdot P(\mathbf{y},\mathbf{x}, k) \label{eq:knnloss_5.1.1}\\
    &= \frac{N}{P(\mathbf{x})}\cdot \frac{P(\mathbf{y},\mathbf{x})}{N} \label{eq:knnloss_5.1.2}\\
    & = P(\mathbf{y}|\mathbf{x}) = P(\mathbf{y}|\mathbf{x}, \bar{\phi}), \label{eq:knnloss_5.1}
\end{align}
% }
where Eq.~\eqref{eq:knnloss_5.1.1} uses $P(k)=\frac{1}{N}$, and Eq.~\eqref{eq:knnloss_5.1.2} follows from the law of total probability $P(\mathbf{y},\mathbf{x})  = \sum_{k\in\mathcal{K}} P(\mathbf{y},\mathbf{x}, k) = N\cdot P(\mathbf{y},\mathbf{x}, k)$, given that the probabilities conditioned on $k$ are uniform with respect to any possible combination of $\mathbf{x}$ and $\mathbf{y}$. Finally, we derive Eq.~\eqref{eq:knnloss_5_1} by aggregating the contributions from all $k$.  

\section{Additional Details on Experimental Settings}\label{app:exp_setup}
We use three widely used large trajectory datasets:

(1) \textbf{Porto}~\cite{porto} contains 1.7 million taxi trajectories collected from Porto, Portugal, between July 2013 and June 2014. 

(2) \textbf{Xian}~\cite{didi} contains 2.1 million ride-hailing trajectories from Xiâ€™an, China, in the first two weeks of October 2018.

(3) \textbf{Germany}~\cite{osmplanet} contains 0.4 million trajectories publicly owned by OpenStreetMap collected across Germany before 2013.

Table~\ref{tab:apdx:dataset_statistics} summarizes the dataset statistics.

\input{tables/tab.apdx.dataset_statistics}

All experiments are run on a machine with an Intel Xeon Gold 6326 CPU, an NVIDIA A100 80 GB GPU and 64 GB RAM. 



\section{Additional Experimental Results}\label{app:exp_results}

\subsection{Impact of Training Set Size}\label{app:training_set_size}
We also investigate the impact of training set size on model accuracy. Each default dataset used in Section~\ref{subsec:exp:overall} contains 7,000, 1,000 and 2,000 trajectories for training, validation and testing, respectively. We increase the training set sizes to 14,000 and 28,000 while keeping the validation and test sets unchanged. These expanded datasets are referred to with ``-2x'' and ``-4x'' in Table~\ref{xtab:vary_train_set_size}, respectively.

In the table, we report the results of \model\ (repeating the experiments as before) comparing with the overall best baseline TrajCL to approximate Fr\'echet on Xian. \model\ consistently outperforms TrajCL, while additional training data does not lead to significant accuracy improvements in learning to approximate Fr\'echet. Notably, \model\ achieves higher accuracy on Xian compared to TrajCL does on Xian-4x, despite Xian-4x is a larger training set. Such results indicate that \model\ remains effective even with a smaller training set, making it highly practical.

\input{tables/xtab.train_set_size}

\subsection{Additional Results on Ablation Study} \label{app:diff_attention}
 
\paragraph{Impact of Self-Attention Modules}
We investigate the impact of different self-attention modules.
\model\ adapts the self-attention module from Llama-2~\cite{llama2}. 
We compare it with a model variant that uses the vanilla self-attention of Transformer~\cite{transformer}, denoted as \textbf{\model-vanillaSA}. 
We repeat the experiments of the ablation study and report the HR@10 results. Similar result patterns are recorded with the other metrics.

\input{tables/tab6.apdx.self-attention}

Table~\ref{tab:apdx:self-attention} shows the results. 
\model\ outperforms \model-vanillaSA consistently, showing that the adapted self-attention from Llama-2 is more effective than the vanilla self-attention for trajectory similarity learning. Meanwhile, the advantage is just 1.4\% on average. This suggests that the main performance gain of \model\ compared with the existing models comes from the sub-view encoder and the $k$NN-guided loss, rather than the self-attention from Llama-2.


\paragraph{Embedding Quality of the Sub-View Encoder}
\input{tables/xtab.rand_index}
Apart from the ablation study on SVEnc in Section~\ref{subsec:exp:overall}, we further conduct experiments to investigate the  effectiveness of SVEnc in capturing movement patterns. 

We evaluate the clustering accuracy of \model\ (with sub-view-based inputs) and \model\-w/o-S (using the cell-based inputs as in T3S and TrajCL) based on the generated trajectory embeddings, as this can indicate how SVEnc learns trajectory representations between similar trajectories.
We follow the experimental settings used in a recent study~\cite{trajsimi_survey}, applying the $k$-medoids algorithm ($k$=10) and measuring cluster accuracy using the Rand Index (RI). RI (higher the better) measures the percentage of ground-truth similar trajectory pairs (based on non-learned measures) being correctly assigned to the same cluster. We report the results on Xian in Table~\ref{xtab:ri}.

We see that \model\ (using SVEnc) achieves higher RI consistently than the model variant using cell-based inputs, verifying that SVEnc helps \model\ produce more accurate trajectory representations.

\subsection{Additional Results on Parameter Study}\label{app:parameterstudy}

We report the full results of parameter study in this section.
We investigate how (1) the batch size $N$, (2) the hyper-parameter $\lambda$ in the loss function, and (3) the number of self-attention layers $\#layers$ in the trajectory encoder of \model\ impact model performance. We repeat the experiments as in Section~\ref{subsec:exp:overall} and only report HR@10. Similar patterns are observed on the other metrics. 

\begin{figure}[ht]
    % \hspace*{2mm}
    % \includegraphics[width=0.26\textwidth]{plots/layers_legend.pdf} \\
    % \vspace*{-3mm}
     
    \hspace*{-2mm}
    \subfloat[{On Porto}~\label{fig:exp_batchsize_porto_hr10}]{
        \includegraphics[width=0.22\textwidth]{plots/batchsizes_porto_N.pdf}
    } \hspace*{-2mm}
    \subfloat[{On Xian}~\label{fig:exp_batchsize_xian_hr10}]{
        \includegraphics[width=0.22\textwidth]{plots/batchsizes_xian_N.pdf}
    } \\ \vspace{-2mm}

    \hspace*{-2mm}
    \subfloat[{On Germany}~\label{fig:exp_batchsize_germany_hr10}]{
        \includegraphics[width=0.22\textwidth]{plots/batchsizes_germany_N.pdf}
    } 
    \hspace*{5mm}
    \includegraphics[width=0.22\textwidth]{plots/layers_legend_fullsize.pdf} \\

    \caption{HR@10 vs. the batch size ($N$ in Eq.~\eqref{eq:knnloss_6}).}\label{fig:exp_batchsizes}
\end{figure}

\paragraph{Impact of Batch Size} 
Figure~\ref{fig:exp_batchsizes} shows the results on varying $N$. The HR@10 results generally increase as $N$ increases. This is expected, since a larger batch size (i.e., a larger $N$ in Eq.~\eqref{eq:knnloss_6}) allows \model\ to learn more relative similarity values from the $k$NN-guided loss.
We use a batch size of 128 by default to keep inline with the baselines for ease of comparison, although a larger batch size could further improve the accuracy of our model.


\begin{figure}[ht]
    % \hspace*{2mm}
    % \includegraphics[width=0.26\textwidth]{plots/layers_legend.pdf} \\
    % \vspace*{-3mm}
     
    \hspace*{-2mm}
    \subfloat[{On Porto}~\label{fig:exp_lambda_porto_hr10}]{
        \includegraphics[width=0.22\textwidth]{plots/lambdas_porto.pdf}
    } \hspace*{-2mm}
    \subfloat[{On Xian}~\label{fig:exp_lambda_xian_hr10}]{
        \includegraphics[width=0.22\textwidth]{plots/lambdas_xian.pdf}
    } \\ \vspace{-2mm}
    \hspace*{-2mm}
    \subfloat[{On Germany}~\label{fig:exp_lambda_germany_hr10}]{
        \includegraphics[width=0.22\textwidth]{plots/lambdas_germany.pdf}
    }     
    \hspace*{5mm}
    \includegraphics[width=0.22\textwidth]{plots/layers_legend_fullsize.pdf}  \\

    \caption{HR@10 vs. $\lambda$ in the loss ($\lambda$ in Eq.~\eqref{eq:loss}).}\label{fig:exp_lambdas}
\end{figure}

\paragraph{Impact of $\lambda$.} 
Figure~\ref{fig:exp_lambdas} shows the results on varying $\lambda$ in Eq.~\eqref{eq:loss}. Except when $\lambda$ is 0 or 1, the value of $\lambda$ has a light impact on \model.
Such results indicate that both loss terms contribute to the accuracy of \model, while using the $k$NN-guided loss alone ($\lambda=0$) is more effective than using the MSE loss ($\lambda=1$).

\begin{figure}[ht]
    % \hspace*{2mm}
    % \includegraphics[width=0.26\textwidth]{plots/layers_legend.pdf} \\
    % \vspace*{-3mm}
     
    \hspace*{-2mm}
    \subfloat[{On Porto}~\label{fig:exp_layers_porto_hr10}]{
        \includegraphics[width=0.22\textwidth]{plots/layers_porto.pdf}
    } \hspace*{-2mm}
    \subfloat[{On Xian}~\label{fig:exp_layers_xian_hr10}]{
        \includegraphics[width=0.22\textwidth]{plots/layers_xian.pdf}
    } \\ \vspace{-2mm}

    \hspace*{-2mm}
    \subfloat[{On Germany}~\label{fig:exp_layers_germany_hr10}]{
        \includegraphics[width=0.22\textwidth]{plots/layers_germany.pdf}
    } 
    \hspace*{5mm}
    \includegraphics[width=0.22\textwidth]{plots/layers_legend_fullsize.pdf} \\

    \caption{HR@10 vs. the number of self-attention layers}\label{fig:exp_layers}
\end{figure}

\paragraph{Impact of the Number of Self-attention Layers.} 
Figure~\ref{fig:exp_layers} shows the results on varying $\#layers$. When $\#layers$ increases, HR@10 first  increases slightly and then decreases slightly. More layers initially help improve the model accuracy, as more learnable parameters are introduced, while an excessive number of parameters can lead to the overfitting problem and hence negatively impact the model accuracy. We use 1-layer \model\ by default for efficiency, as the model accuracy is already high with this setup.



\subsection{Additional Results on Model Training Time}\label{app:training_time}
We  further report the average training time per epoch and the maximum number of training epochs on the Xian dataset in Table~\ref{xtab:training_time}, extending the results in Table~\ref{tab:exp:efficiency}. 

It is worth mentioning that the reported total training time is end-to-end, including, e.g., preprocessing time. For example, TrajGAT preprocesses to convert raw trajectories into graphs, which has a high time cost. Besides, the maximum number of training epochs (i.e., Maximum \#epochs) is \emph{not} the actual number of epochs each model is trained for, as most models apply early stopping.
Thus, the average training time per epoch has no direct correlation with the total time or the maximum number of epochs.

\input{tables/xtab.training_time}