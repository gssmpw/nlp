\section{Related work}
%-------------------------------------------------------------------------
\subsection{3D Human Motion Synthesis }
Traditionally, 3D human skeletal motion prediction relied on physics-based methods, which are often computationally complex and unstable~\cite{loi2023machine}. 
Recently, machine learning (ML) has leveraged large-scale datasets to achieve more efficient and accurate predictions of 3D motion trajectories. 
Specifically, early efforts primarily employed recurrent neural networks (RNNs) to address this task~\cite{martinez2017human, li2018convolutional, liu2019towards}. However, RNN models are susceptible to the accumulation of prediction errors, resulting in discontinuities within the predicted motion sequences~\cite{gui2018adversarial}. In~\cite{ma2022progressively}, a network composed of Spatial Dense GCNs (S-DGCN) and Temporal Dense GCNs (T-DGCN) is proposed, which alternates to extract spatiotemporal features across the global receptive field of the pose sequence. The work in~\cite{aksan2021spatio} utilizes a self-attention mechanism to learn high-dimensional embeddings for skeletal joints and generate temporally coherent poses. 

The motion diffusion model (MDM)~\cite{tevet2022humanmotiondiffusionmodel} firstly apply the classifier-free diffusion-based method to human motion generation, which has inspired most diffusion-based motion generation methods.
MoFusion~\cite{dabral2023mofusion} combines diffusion models with cross-attention mechanisms to effectively manage various input signals for motion synthesis tasks, such as generating motions from audio or text.
Although the above works have made substantial progress in improving the quality and diversity of generated motions, dance generation remains a challenging task due to the strict choreography rules such as matching skeletal movements with musical beats and ensuring the style aligns with the genre of the music, both of which contribute to the complexity of the task.
%-------------------------------------------------------------------------
\subsection{Music Driven Dance Generation}
Extensive research has been conducted on music-conditioned dance generation. Early works~\cite{shiratori2006dancing,ofli2008audio,fukayama2015music} approach this task as a similarity-based retrieval problem due to limited training data, which consequently constrains the diversity and creativity of the generated dances.
With the development of deep learning, many approaches instead synthesize dance segments from scratch using motion prediction and have proposed numerous modeling techniques, implementing generation tasks based on Convolutional Neural Network (CNN)~\cite{holden2016deep,holden2015learning}, RNN~\cite{butepage2017deep,chiu2019action,du2019bio}, and Transformers~\cite{fan2022bi,huang2022genre,li2022danceformer}. However, these frame-by-frame prediction approaches often face challenges such as error accumulation and motion freezing~\cite{zhuang2022music2dance}. Recent research has shifted towards a generative pipeline. Based on VQ-VAE, TM2D~\cite{gong2023tm2d} incorporates both music and text instructions to generate dance segments that are coherent with the given music while retaining semantic information. Bailando~\cite{siyao2022bailando} quantizes meaningful dance units into a quantized codebook and employs a reinforcement learning-based evaluator to improve the alignment between generated movement and musical beats. Despite their outstanding performance, these systems are highly complex and involve multiple sub-networks. EDGE~\cite{tseng2023edge} is the first to employ a diffusion-based dance generation framework featuring a single-model design optimized for a single objective. It also introduces a novel evaluation approach focusing on physical plausibility. However, existing models are typically trained on datasets containing only $24$ body joints and overlook the quality of hand motion generation. To address this limitation, Li~\textit{et al.}~\cite{li2023finedance} proposes FineNet and introduced a new dataset with $52$ joints. It is also worth mentioning that the vast majority of models rely on handcrafted musical features such as MFCC, chroma, or one-hot beat features, which may not fully capture the intricate details needed for fine-grained dance movement correlation.


%--------------------------------------------------------------------

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{Figures/overview2.jpg}
  \caption{An overview of the proposed GCDance method. On the left, we show the data preprocessing pipeline and the training process at diffusion timestep $t$. On the right, the figure illustrates the sampling process, where a sequence of dance motions is generated iteratively.}
  \label{f1}
\end{figure*}

%------
\subsection{Diffusion Models}
Diffusion models~\cite{ho2020denoising,nichol2021improved,dhariwal2021diffusion} are a type of deep generative model and have made significant progress in recent years~\cite{croitoru2023diffusion}.
They have been widely applied across multiple fields of research, such as image generation~\cite{austin2021structured,ho2022cascaded,ruiz2023dreambooth,ramesh2022hierarchical}, semantic segmentation~\cite{baranchuk2021label,brempong2022denoising,poole2022dreamfusion}, audio synthesis~\cite{liu2024audioldm2,liu2023audioldm,kong2020diffwave}
and text generation~\cite{austin2021structured,lovelace2024latent,he2022diffusionbert}. For conditional generation, existing approaches often employ classifier guidance~\cite{chung2022improving,dhariwal2021diffusion} or classifier-free guidance~\cite{nichol2021glide,rombach2022high} to enhance the quality of sample generation, which is applicable to any pretrained diffusion model to improve performance without retraining. Furthermore, the growing interest in diffusion models is attributed to their remarkable ability for controllable generation. The work in~\cite{avrahami2022blended} presents a text-conditional image generation model, utilizing the CLIP model~\cite{radford2021learning} to guide the diffusion process to produce images that conform to the target prompt. GMD~\cite{karunratanakul2023guided} applies diffusion to the task of text-to-motion trajectory generation, integrating spatial constraints to improve the alignment between spatial information and local poses. The work in~\cite{alexanderson2023listen} proposes an audio-driven motion generation focusing on gestures and dance, and also implemented style control and strength adjustment of stylistic expression. However, their dance generation is limited to only four genres. Dance motion generation is a more complex task and suffers from lower data availability due to its specialized nature~\cite{tseng2023edge}. In our work, we present a classifier-free diffusion-based method that can not only generate 16 different dance genres conditioned on music, but also control the type of dance through genre-specific prompts.



%-------------------------------------------------------------------------