%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{float}
\usepackage{capt-of}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{graphicx}  % 用于 \resizebox
\usepackage{multirow}  % 用于单元格合并
\usepackage{array}     % 表格增强
\usepackage{lscape}    % 横向页面
\usepackage{makecell}
\usepackage{setspace}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}


\begin{icmlauthorlist}
\icmlauthor{Xinran Liu}{yyy}
\icmlauthor{Xu Dong}{yyy}
\icmlauthor{Diptesh Kanojia}{yyy}
\icmlauthor{Wenwu Wang}{yyy}
\icmlauthor{Zhenhua Feng}{comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{University of Surrey, UK}
\icmlaffiliation{comp}{Jiangnan University, China}

\icmlcorrespondingauthor{Xinran Liu}{xinran.liu@surrey.ac.uk}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\begin{center}
\begin{minipage}{0.9\textwidth}
  {\protect\includegraphics[trim={22cm 2.5cm 0 0},clip, width=1.02\textwidth]{Figures/f1-top.jpg}}
  \captionof{figure}{Given an audio input and genre-specific textual prompt, GCDance generates 3D dance sequences that align well with the musical melody and beat while adhering to textual instructions.}
\end{minipage}
\end{center}
\vskip 0.2in

]


\printAffiliationsAndNotice{} 

\begin{abstract}
Generating high-quality full-body dance sequences from music is a challenging task as it requires strict adherence to genre-specific choreography. Moreover, the generated sequences must be both physically realistic and precisely synchronized with the beats and rhythm of the music. To overcome these challenges, we propose GCDance, a classifier-free diffusion framework for generating genre-specific dance motions conditioned on both music and textual prompts. Specifically, our approach extracts music features by combining high-level pre-trained music foundation model features with hand-crafted features for multi-granularity feature fusion. To achieve genre controllability, we leverage CLIP to efficiently embed genre-based textual prompt representations at each time step within our dance generation pipeline. Our GCDance framework can generate diverse dance styles from the same piece of music while ensuring coherence with the rhythm and melody of the music. Extensive experimental results obtained on the FineDance dataset demonstrate that GCDance significantly outperforms the existing state-of-the-art approaches, which also achieve competitive results on the AIST++ dataset. Our ablation and inference time analysis demonstrate that GCDance provides an effective solution for high-quality music-driven dance generation.
%The source code is publicly available online.\footnote{The source code and demos are available at \url{https://anonymous.4open.science/r/GC_Dance-5A36}.}
\end{abstract}

\section{Introduction}
\label{Introduction}

Dancing plays a significant role in all cultures and serves as an important medium for emotional expression among people~\cite{bannerman2014dance,huang2022genre}. However, choreography is an artistic skill that significantly relies on a professional foundation. During the process of dance composition, the body movements of the choreographer need to respond to musical beats and synchronize with the beats as per the genre of the music~\cite{li2021ai}. As a result, leveraging artificial intelligence for music-driven choreography demonstrates immense research potential. 

In recent years, numerous deep learning-based approaches have been applied to the task of dance generation. However, many of these treat dance generation as a \textit{matching} or a \textit{retrieval} problem. For instance, some approaches predict future dance movements based on past motion sequences~\cite{huang2020dance,li2021ai,kim2022brand}, but they frequently encounter challenges such as motion freezing during long-term generation. Other methods such as~\cite{siyao2022bailando,gong2023tm2d}, build the codebook of dance units using vector quantized variational autoencoder (VQ-VAE), but this reliance on a predefined discrete codebook tends to restrict the diversity and creativity of the generated dances. Currently, diffusion models~\cite{ho2020denoising} have demonstrated remarkable performance in generative tasks, including the generation of dance motion sequences~\cite{tseng2023edge, qi2023diffdance, liu2024dgfm, zhang2024bidirectional}. 
However, since these methods do not incorporate any genre information, they tend to generate single dance styles for a given piece of music. In practice, music within the same genre can inspire various styles of dance. This limitation significantly restricts the diversity of the generated dance movements. Therefore, exploring how to generate varied dance styles that align with the rhythm and choreography rules of the same piece of music is crucial for enhancing the expressiveness and diversity of dance generation.
Additionally, these methods are typically trained on datasets with limited joint representations (\textit{e.g.}, $24$ joints), neglecting finer hand motion details, which are essential for the realism and expressiveness of generated dances. 

Furthermore, the task of generating music-driven dance motion requires the generative model to have an outstanding performance in the extraction of music features. Most existing dance generation approaches rely on hand-crafted musical features such as Mel-Frequency Cepstral Coefficient (MFCC)~\cite{abdul2022mel}, chroma, or one-hot beat features, which do not fully capture the intricate link between music and dance movements~\cite{qi2023diffdance}. In contrast, recent advances in music foundation models have demonstrated the potential of modern machine learning techniques to better understand and process music in more sophisticated ways~\cite{li2023mert}. The potential of music foundation models in music-driven dance generation remains insufficiently explored, making it intriguing to investigate how these models can contribute to the quality and expressiveness of the dance generation task.

In this paper, we focus on generating diverse dance segments by proposing a Genre-Controlled 3D Full Body Dance Generation (GCDance) network, which leverages a transformer-based diffusion model conditioned on both music and text prompts. 
To address the limitations of music feature representations, we incorporate classical hand-crafted Short-Time Fourier Transform (STFT) audio features with deep features obtained from the music foundation model Wav2CLIP~\cite{wu2022wav2clip}. Furthermore, we utilize the contrastive language image pretraining (CLIP) model~\cite{radford2021learning} to extract text representations from genre descriptions. Since the Wav2CLIP audio encoder shares an embedding space with text, this alignment leads to a unified understanding of the musical and genre characteristics that drive dance movements, significantly enhancing the motion generation capability of the model. By conditioning on both the music input and the text prompt, we employ a classifier-free diffusion framework to generate dance sequences, where music serves as the primary driving input, while the text prompt provides additional control for genre-specific generation. To address the limitations of previous methods that lack controllability over dance style, we use a dense Feature-wise Linear Modulation (FiLM) layer to modulate the dance generation process based on genre information from the text. The FiLM layer generates scaling and shifting parameters from the text prompt, which are applied to intermediate features of the model, ensuring that the generated results are synchronized with the music while also adhering to the desired genre. 


Our contributions can be summarized as follows: 
1) We introduce GCDance, a classifier-free diffusion-based framework for generation of multi-genre dance sequences. Our framework incorporates fusing cross-attention to generate fine-grained (52-joint) dance movements, encompassing hand gestures and body motion.
2) GCDance integrates music embeddings extracted from a pretrained music foundation model with hand-crafted features, effectively leveraging the advantages of high-level semantic information and low-level temporal details to improve the quality of generated dance sequences. 
3) GCDance achieves controllable dance generation by conditioning on both music and textual prompts. By incorporating genre-specific information, the model can generate dances in specified genres, additionally supporting localized motion editing. 
4) We perform a comprehensive evaluation to assess existing approaches on the 52-joint dataset—FineDance, conducting separate evaluation for hand and body movements, consistently achieving state-of-the-art performance across metrics. Our ablation test, inference time analysis, and additional results on the 24-joint AIST++ dataset, confirm that GCDance provides an effective solution to music-driven dance generation.


%-------------------------------------------------------------------------

\section{Related work}


%-------------------------------------------------------------------------
\subsection{3D Human Motion Synthesis }
Traditionally, 3D human skeletal motion prediction relied on physics-based methods, which are often computationally complex and unstable~\cite{loi2023machine}. 
Recently, machine learning (ML) has leveraged large-scale datasets to achieve more efficient and accurate predictions of 3D motion trajectories. 
Specifically, early efforts primarily employed recurrent neural networks (RNNs) to address this task~\cite{martinez2017human, li2018convolutional, liu2019towards}. However, RNN models are susceptible to the accumulation of prediction errors, resulting in discontinuities within the predicted motion sequences~\cite{gui2018adversarial}. In~\cite{ma2022progressively}, a network composed of Spatial Dense GCNs (S-DGCN) and Temporal Dense GCNs (T-DGCN) is proposed, which alternates to extract spatiotemporal features across the global receptive field of the pose sequence. The work in~\cite{aksan2021spatio} utilizes a self-attention mechanism to learn high-dimensional embeddings for skeletal joints and generate temporally coherent poses. 

The motion diffusion model (MDM)~\cite{tevet2022humanmotiondiffusionmodel} firstly apply the classifier-free diffusion-based method to human motion generation, which has inspired most diffusion-based motion generation methods.
MoFusion~\cite{dabral2023mofusion} combines diffusion models with cross-attention mechanisms to effectively manage various input signals for motion synthesis tasks, such as generating motions from audio or text.
Although the above works have made substantial progress in improving the quality and diversity of generated motions, dance generation remains a challenging task due to the strict choreography rules such as matching skeletal movements with musical beats and ensuring the style aligns with the genre of the music, both of which contribute to the complexity of the task.
%-------------------------------------------------------------------------
\subsection{Music Driven Dance Generation}
Extensive research has been conducted on music-conditioned dance generation. Early works~\cite{shiratori2006dancing,ofli2008audio,fukayama2015music} approach this task as a similarity-based retrieval problem due to limited training data, which consequently constrains the diversity and creativity of the generated dances.
With the development of deep learning, many approaches instead synthesize dance segments from scratch using motion prediction and have proposed numerous modeling techniques, implementing generation tasks based on Convolutional Neural Network (CNN)~\cite{holden2016deep,holden2015learning}, RNN~\cite{butepage2017deep,chiu2019action,du2019bio}, and Transformers~\cite{fan2022bi,huang2022genre,li2022danceformer}. However, these frame-by-frame prediction approaches often face challenges such as error accumulation and motion freezing~\cite{zhuang2022music2dance}. Recent research has shifted towards a generative pipeline. Based on VQ-VAE, TM2D~\cite{gong2023tm2d} incorporates both music and text instructions to generate dance segments that are coherent with the given music while retaining semantic information. Bailando~\cite{siyao2022bailando} quantizes meaningful dance units into a quantized codebook and employs a reinforcement learning-based evaluator to improve the alignment between generated movement and musical beats. Despite their outstanding performance, these systems are highly complex and involve multiple sub-networks. EDGE~\cite{tseng2023edge} is the first to employ a diffusion-based dance generation framework featuring a single-model design optimized for a single objective. It also introduces a novel evaluation approach focusing on physical plausibility. However, existing models are typically trained on datasets containing only $24$ body joints and overlook the quality of hand motion generation. To address this limitation, Li~\textit{et al.}~\cite{li2023finedance} proposes FineNet and introduced a new dataset with $52$ joints. It is also worth mentioning that the vast majority of models rely on handcrafted musical features such as MFCC, chroma, or one-hot beat features, which may not fully capture the intricate details needed for fine-grained dance movement correlation.


%--------------------------------------------------------------------

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{Figures/overview2.jpg}
  \caption{An overview of the proposed GCDance method. On the left, we show the data preprocessing pipeline and the training process at diffusion timestep $t$. On the right, the figure illustrates the sampling process, where a sequence of dance motions is generated iteratively.}
  \label{f1}
\end{figure*}

%------
\subsection{Diffusion Models}
Diffusion models~\cite{ho2020denoising,nichol2021improved,dhariwal2021diffusion} are a type of deep generative model and have made significant progress in recent years~\cite{croitoru2023diffusion}.
They have been widely applied across multiple fields of research, such as image generation~\cite{austin2021structured,ho2022cascaded,ruiz2023dreambooth,ramesh2022hierarchical}, semantic segmentation~\cite{baranchuk2021label,brempong2022denoising,poole2022dreamfusion}, audio synthesis~\cite{liu2024audioldm2,liu2023audioldm,kong2020diffwave}
and text generation~\cite{austin2021structured,lovelace2024latent,he2022diffusionbert}. For conditional generation, existing approaches often employ classifier guidance~\cite{chung2022improving,dhariwal2021diffusion} or classifier-free guidance~\cite{nichol2021glide,rombach2022high} to enhance the quality of sample generation, which is applicable to any pretrained diffusion model to improve performance without retraining. Furthermore, the growing interest in diffusion models is attributed to their remarkable ability for controllable generation. The work in~\cite{avrahami2022blended} presents a text-conditional image generation model, utilizing the CLIP model~\cite{radford2021learning} to guide the diffusion process to produce images that conform to the target prompt. GMD~\cite{karunratanakul2023guided} applies diffusion to the task of text-to-motion trajectory generation, integrating spatial constraints to improve the alignment between spatial information and local poses. The work in~\cite{alexanderson2023listen} proposes an audio-driven motion generation focusing on gestures and dance, and also implemented style control and strength adjustment of stylistic expression. However, their dance generation is limited to only four genres. Dance motion generation is a more complex task and suffers from lower data availability due to its specialized nature~\cite{tseng2023edge}. In our work, we present a classifier-free diffusion-based method that can not only generate 16 different dance genres conditioned on music, but also control the type of dance through genre-specific prompts.



%-------------------------------------------------------------------------
\section{Proposed Method}


The goal of our model is to generate genre-specific dance sequences conditioned on both music sequences and textual descriptions. Given a long music piece, we first divide it into $N$ segments. For each segment, we extract \( k \) samples, where each sample is represented by a $D_{m}$-dimensional feature vector, forming a feature matrix $C_{M}\in \mathbb{R}^{k \times D_{m}}$. Simultaneously, we extract a text feature vector $C_{E}\in \mathbb{R}^{D_{e}}$ to facilitate genre-controlled dance generation, where $D_{e}$ denotes the dimensionality of the text features. 

\subsection{Preliminaries of Diffusion Models}

We employ a diffusion-based model for dance generation, which consists of two stages: the diffusion process and the reverse process. 
In the diffusion process, we follow the methodology outlined in Denoising Diffusion Probabilistic Models (DDPM)~\cite{ho2020denoising}, which formulates a Markov chain that incrementally injects Gaussian noise to the ground truth data $\textbf{\textit{m}}_{0}$, while enabling the sampling of $\textbf{\textit{d}}_{t}$ at any timestep $\mathit{t}$:
\begin{equation}  
q\left(\textbf{\textit{d}}_{t} \mid \textbf{\textit{m}}_{0}\right)=\mathcal{N}\left(\textbf{\textit{d}}_{t} ; \sqrt{\bar{\alpha}_{t}} \textbf{\textit{m}}_{0}, (1-\bar{\alpha}_{t}) \textit{\textbf{I}}\right)
\end{equation}
where $\bar{\alpha}_{t}$ is a constant within the range $(0,1)$ that decreases monotonically. As $\mathit{t}$ increases and $\bar{a}_{t}$ approaches $0$, the distribution of $\textbf{\textit{d}}_{t}$ gradually converges to the standard normal distribution $\mathcal{N}(0,\textit{\textbf{I}})$. In our model, we set timestep $\textit{T}=1000$.

In the denoising process, we build upon EDGE~\cite{tseng2023edge} and design an attention-based network $f_{rev}$ to reverse the forward diffusion process.
The music features $C_{M}$ and text features $C_{E}$ serve as conditioning inputs to predict the dance movements at all $t$. 
We adopt the loss function $\mathcal{L}_{S}$ in DDPM as our objective and optimize it by training the model to estimate $f_{rev} (\textbf{\textit{d}}_{t}, t, C_{M}, C_{E}  ) \approx  \textbf{\textit{m}}_{0}$. In this process, the model progressively refines the noisy latent variable to approximate the true data distribution. Hence, the training objective is:
\begin{equation}  
\label{equ_2}
\mathcal{L}_{\text {S}}=
\mathbb{E}_{\boldsymbol{ \textbf{\textit{m}}_{0}}, t}\left[\left\| \textit{\textbf{m}}_{0}- f_{rev} (\textit{\textbf{d}}_{t}, t, 
C_{M}, C_{E}  )\right\|_{2}^{2}\right]
\end{equation}
 
\subsection{Pose Representations}
For the motion representation, according to the Skinned Multi-Person Linear (SMPL) format~\cite{loper2023smpl}, we define three components: ($1$) Human joint positions: We employ a $6$ DOF rotation representation to transform the $52$ joint positions into a $312$ dimensional space, denoted as $\mathit{p} \in \mathbb{R}^{52\times 6=312}$.
($2$) Root translation: A 3-dimensional vector is used to describe the global translation of the root joint. ($3$) Foot-ground contact: Following the approach in EDGE~\cite{tseng2023edge}, we incorporate a $4$-dimensional foot-ground contact label to represent the binary states of heel and toe ground contact for each foot, given by $\mathit{f} \in \mathbb{R}^{2\times 2=4}$. Consequently, the complete representation of the pose sequence is $\mathit{m} \in \mathbb{R}^{k\times 319}$, where $k$ represents the number of frames.

\subsection{Conditional Generation}

\textbf{Music Representations.}
The existing approaches usually neglect the importance of music feature representation. To address this, we explore using Wav2CLIP~\cite{wu2022wav2clip} as our music encoder, which is an audio-visual correspondence model that distills from the CLIP framework \cite{radford2021learning}. Specifically, it works by freezing the CLIP vision model to process visual streams from videos, while training a new model to predict the corresponding CLIP embeddings from the audio stream alone. For hand-crafted music features, we employ the STFT, a variant of the Fourier Transform that decomposes music signals into time-frequency components, capturing frequency information within short time windows~\cite{gourisaria2024comparative}. It is widely used for feature extraction in audio-related tasks. In our approach, we extract STFT features using the Librosa toolbox~\cite{mcfee2015librosa}.

\textbf{Text Representations.} For music genre labeling, we apply a prompt learning method~\cite{zhou2022learning} to expand the label into a full sentence. For example, given the genre label ``Jazz," the generated sentence is  ``This is a Jazz type of music." We then use CLIP~\cite{radford2021learning} to extract features from this sentence.


However, a significant domain gap still exists between the raw conditional features and the dance motions. To bridge this gap, we introduce an adapter module to process the extracted music and text representations and effectively align them with the latent space. The adapter is implemented with the following structure:

\begin{equation}  
Adapter = W_{1} \left ( \delta  \left( W_{2}\left ( C \right )  \right )  \right ) 
\end{equation}  

where $W$ represents the fully connected (FC) layers and $\delta $ denotes the Rectified Linear Unit (ReLU) activation function, and $C$ represents the input condition.



\subsection{GCDance: The Proposed Framework}

In this section, we formulate the Genre-Controlled 3D Full Body Dance Generation Driven by Music (GCDance), a framework designed for generating full-body dance motions. The overall architecture is illustrated in Figure~\ref{f1}. 

\paragraph{Architecture.} The inputs of GCDance model include the noise slice $\textbf{\textit{d}}_{T}$, the music condition $C_{M}$, the text condition $C_{E}$ and diffusion timestep $t$. These inputs are then fed into a Transformer-based denoising network. As illustrated in Figure~\ref{f2}, we employ two expert downsampling modules to separately model the distributions of body motion and hand motion inspired by~\cite{li2023finedance}. This approach is motivated by the distinct differences in the range of motion and degrees of freedom between the body and hands. By learning their unique feature spaces independently, the model can generate dance sequences with enhanced detail and expressiveness. 
To elaborate on the process, the motion sequences are separately fed into the two Transformer-based networks. They consist of a self-attention module~\cite{vaswani2017attention}, a cross-attention module~\cite{saharia2022photorealistic}, multilayer perceptrons, and time-embedding FiLM layers~\cite{perez2018film}. 

Additionally, to incorporate the music conditioning input, we utilize a cross-attention mechanism to process music features projected into the embedding space, following the approach of~\cite{saharia2022photorealistic}. 
Genre information is incorporated through Feature-wise Linear Modulation (FiLM)~\cite{perez2018film}, which modulates the output of a neural network through multiplicative interaction, allowing the model to dynamically adjust representations based on contextual information. In our model, we use the output from the previous layer $Y$ and the text embedding $C_{E}$ as inputs. The embeddings are processed as follows:

\begin{equation}  
\quad\gamma  =\theta_{w} (\alpha(C_{E})), \quad\varepsilon  = \theta_{b} (\alpha(C_{E})) 
\end{equation}  


\begin{equation}  
FiLM_{t}(Y) = \gamma Y + \varepsilon,
\end{equation}  
where $\alpha$ is a text embedding adapter used to adjust the embedding representation. $\theta_{w}$ and $\theta_{b}$ represent the linear projections responsible for computing the weights and biases, respectively. 


\begin{figure}[!t]
  \centering
  \includegraphics[width=.48\textwidth]{Figures/f2.jpg}
  \caption{An overview of the Decoder of the GCDance method.}
  \label{f2}
\end{figure}



\paragraph{Training Objectives.} In order to generate fluent and physically-plausible motion sequences, apart from the reconstruction loss in Equ.~\ref{equ_2}, we incorporate several auxiliary losses frequently used in motion generation tasks adopted from the EDGE~\cite{tseng2023edge} and MDM~\cite{tevet2022humanmotiondiffusionmodel}. 
These auxiliary losses encourage alignment in three key aspects: joint positions ( Equ.~\ref{eq:4-3}), velocities (Equ.~\ref{eq:4-4}), and foot contact (Equ.~\ref{eq:4-5}). Similarly to previous studies~\cite{tevet2023human}, we use the forward kinematic function $FK(\cdot)$ to transform the joint angles into their corresponding joint positions, calculating the joint loss:
\begin{equation}  
\mathcal{L}_{\text {J}}=\frac{1}{k} \sum_{j=1}^{k}\left\|F K\left(\boldsymbol{\textbf{\textit{m}}}^{j}\right)-F K\left(\hat{\boldsymbol{\textbf{\textit{m}}}}^{j}\right)\right\|_{2}^{2}
\label{eq:4-3}
\end{equation}  
where $j$ represents the frame index and $\hat{\boldsymbol{\textbf{\textit{m}}}}^{j}$ represents the predicted pose for this frame.
We also compute velocity and acceleration, introducing the velocity loss:
\begin{equation}  
\mathcal{L}_{\text {V}}=\frac{1}{k-1} \sum_{j=1}^{k-1}\left\|\left(\boldsymbol{\textbf{\textit{m}}}^{j+1}-\boldsymbol{\textbf{\textit{m}}}^{j}\right)-\left(\hat{\boldsymbol{\textbf{\textit{m}}}}^{j+1}-\hat{\boldsymbol{\textbf{\textit{m}}}}^{j}\right)\right\|_{2}^{2}
\label{eq:4-4}
\end{equation}  
Lastly, we apply contact loss $\mathcal{L}_{\text {C}}$ which leverages binary foot-ground contact labels to optimize the consistency in foot contact during motion generation:
\begin{equation}
\mathcal{L}_{\text {C}}=\frac{1}{k-1} \sum_{j=1}^{k-1}\left\| \left( F K\left(\hat{\boldsymbol{\textbf{\textit{m}}}}^{j+1}\right)-F K\left(\hat{\boldsymbol{\textbf{\textit{m}}}}^{j}\right) \right) \cdot \hat{\textbf{\textit{b}}}^{j} \right\|_{2}^{2}
\label{eq:4-5}
\end{equation}
where $\hat{\textbf{\textit{b}}}^{j}$ denotes the predicted binary foot-ground contact labels. 
The \textbf{overall training loss} is defined by the weighted sum of the above loss functions:
\begin{equation}
\mathcal{L}=\mathcal{L}_{\text {S}}+\lambda_{\text {J }} \mathcal{L}_{\text {J }}+\lambda_{\text {V }} \mathcal{L}_{\text {V }}+\lambda_{\text {C }} \mathcal{L}_{\text {C }}
\end{equation}
where the weights $\lambda$ are the balancing parameters.

\subsection{Sampling}
The sampling process can be shown as the right of Figure~\ref{f1}. At each denoising timestep $t$, unlike conventional generation models based on diffusion, which reconstruct the output by predicting the noise term $d_{t}$, our model directly predicts the dance $\hat{m}$. The predicted pose is then re-noised back to timestep $t-1$ as illustrated in Equ.~\ref{eq:4-2}.

\begin{equation}
\bm{d_{t-1}}\sim q(\bm{\hat{m}}(\bm{d_{t}},C_{E},C_{M}),t-1) 
\label{eq:4-2}
\end{equation}

Repeat this process until $t$ reaches zero. 

\subsection{Editing}
Building on previous methods, our approach enhances diversity by incorporating diffusion inpainting techniques.  In practice, our model allows users to apply a wide range of constraints. Users can specify conditions for generating in-between movements in the temporal domain or for editing specific joint parts in the spatial domain. Based on these defined constraints, our method generates tailored dance outcomes, offering fine-grained control over the generated dance sequences. This process occurs only during sampling and is not included in the training process.
For detailed mathematical formulations and implementation specifics, including examples of editing based on joint-wise or temporal constraints, please refer to Appendix~\ref{s0}.






%-------------------------------------------------------------------------
\section{Experiments}

In this section, we present the dataset, evaluation metrics, and comprehensive experimental results. For additional implementation details of the model, please refer to Appendix~\ref{s1}.


%---
\begin{table*}[!t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
& \multicolumn{2}{c}{\textbf{Motion Quality}}          & \multicolumn{2}{c}{\textbf{Motion Diversity}}        & \multirow{2}{*}{\textbf{PFC}$\downarrow$  } & \multirow{2}{*}{\textbf{BAS}$\uparrow$ } \\  \cmidrule(lr{0.35em}){2-3}  \cmidrule(lr{0.35em}){4-5}
 & FID\_hand$\downarrow$             & FID\_body$\downarrow$            & Div\_hand$\uparrow$            & Div\_body$\uparrow$            &                         &                                \\
\midrule
DanceRevolution~\cite{huang2020dance}      & 219.52 ± 18.32    & 99.83 ± 7.79      & 1.85 ± 0.60        & 4.49 ± 0.25         & 6.81 ± 0.81     & 0.2104 ± 0.0057       \\
MNET~\cite{Kim_2022_CVPR}                  & 195.56 ± 5.04     & 154.79 ± 2.80       & 6.79 ± 0.20     & 8.25 ± 0.39\textsuperscript{*}     & 2.98 ± 0.11       & 0.1792 ± 0.0014       \\
Bailando~\cite{siyao2022bailando}          & 55.60 ± 8.15      & 57.77 ± 6.01         & 6.40 ± 0.68     & 4.27 ± 0.43        & 0.34 ± 0.01       & 0.2152 ± 0.0028       \\
EDGE~\cite{tseng2023edge}                  &  \underline{25.37 ± 3.24}     & 51.56 ± 3.62           & \underline { 8.29 ± 0.30}   &  \underline{5.88 ± 0.32}         & 0.21 ± 0.03       &  \underline{0.2171±0.0056}      \\
FineNet~\cite{li2023finedance}             & 27.77 ± 3.26      &  \underline{27.39 ± 2.48}                & 8.04 ± 0.35   & 5.80 ± 0.41     & \textbf{0.10 ± 0.01}       & 0.2037 ± 0.0059
\\
GCDance (Ours)                               & \textbf{18.48±4.52} & \textbf{22.61±3.64} & \textbf{8.77±0.41} & \textbf{6.77±0.75} &  \underline{0.17±0.01} & \textbf{0.2188±0.0042} \\
\bottomrule
\end{tabular}
}
\caption{Performance comparison on the FineDance Dataset. We use \textbf{bold} to indicate the best values and  \underline{underline} to denote the second-best values. $\downarrow$ indicates that lower values are better, while $\uparrow$ indicates that higher values are better. $*$Note that the 
generated results exhibit abnormally high Div due to discontinuous and jittery motions.}
\label{table_1}

\end{table*}
%---
\subsection{Dataset}
 We evaluate the proposed method on the FineDance dataset~\cite{li2023finedance}, which contains $7.7$ hours of paired music and dance, totaling $831,600$ frames at $30$ fps across different $16$ genres. The average dance length is $152.3$ seconds. 
The skeletal data of FineDance is stored in a 3D space and is represented by the standard $52$ joints, including the finger joints.
We train all the methods on the $183$ pieces of music from the training set and generate $270$ dance clips across $18$ songs from the test set, using the corresponding real dances as ground truth.


\subsection{Evaluation Method} 




We evaluate our approach based on three aspects: motion quality, generation diversity, and motion-music correlation.
\textbf{1) Motion Quality}: Following the previous works~\cite{li2021ai,li2020learning}, we evaluate the motion quality using Fréchet Inception Distance (FID)~\cite{heusel2017gans}. This metric measures the dissimilarity between the feature distributions of generated dance sequences and ground truth dance sequences by computing the distribution distance in the feature space. 
\textbf{2) Generation Diversity}: To assess the capacity to generate diverse dance motions,   we follow the approach outlined in prior work~\cite{siyao2022bailando}, which quantifies diversity by calculating the average Euclidean distance of the kinetic features across the generated motions.
\textbf{3) Motion-Music Correlation}: 
Furthermore, to evaluate the alignment between musical beats and motion transition beats, we employ the Beat Alignment Score (BAS) following~\cite{siyao2022bailando}. This metric assesses the correlation between motion and music by calculating the average temporal distance between each kinematic beat and its nearest musical beat. 
\textbf{4) Physical Plausibility}: 
Additionally, we utilize the Physical Foot Contact (PFC) metric proposed in~\cite{tseng2023edge}, which is inspired by the principles of center-of-mass (COM) motion and its relationship with foot-ground contact, to evaluate the physical plausibility of generated dance sequences.




\subsection{Qualitative Results}

To verify the controllability of our model in generating dances of specific genres, we conducted experiments using the same pieces of music with different genre labels. 
In Figure~\ref{fig_5}, we input a segment of popular modern music and provided four different genre labels: Popping, Hip-hop, Breaking, and Korean. Then we visualized the generated dance sequences as shown. Similarly, in Figure~\ref{fig_6}, we input the same piece of classical music but applied three different genre labels: Miao, Dai, and Classical dance. These visualization results demonstrated that our model is capable of generating distinct dance styles corresponding to different input genre labels.
For example, the Popping dance sequence demonstrates numerous sharp hits and precise poses, accompanied by smooth transitional movements such as waving. While the Breaking dance sequence is characterized by dynamic leg movements. Classical dance features extensive arm movements and graceful rotations, all of which are effectively captured in our visualization results. These results indicate the capability of our model for controllable dance generation, enabling the creation of diverse dance styles from the same piece of music.

\begin{figure}[!t]
  \centering
  \includegraphics[width=.49\textwidth]{Figures/vis-1.jpg}
  \caption{Four different genres of dance generated based on a piece of popular music.}
  \label{fig_5}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=.49\textwidth]{Figures/vis-2.jpg}
  \caption{Three different genres of dance generated based on a piece of classical music.}
  \label{fig_6}
\end{figure}
\vspace{-8pt}




\subsection{Quantitative Results}

As shown in Table~\ref{table_1}, we compare our method with the advanced existing works: DanceRevolution~\cite{huang2020dance}, MNET~\cite{Kim_2022_CVPR}, Bailando~\cite{siyao2022bailando}, EDGE~\cite{tseng2023edge} and FineNet~\cite{li2023finedance}. 
To evaluate the robustness and accuracy of the models, we retrained the baseline models on the FineDance dataset. For each model, we generated $10$ sets of dance sequences, with each set randomly sampled from $270$ dance clips in the test set. Each generated sequence contains $T=120$ frames, corresponding to $4$ seconds of motion. We then calculated the mean and standard deviation of key performance metrics to assess their performance.

The results show that our model outperforms the baseline model EDGE with FID\_body and FID\_hand scores of 6.89 and 28.95, respectively. In terms of motion diversity, our model achieves improvements as indicated by higher Div scores for both hand and body movements. Regarding the PFC metric, our model achieves a competitive score of $0.17\pm0.001$, surpassing the baseline model while being slightly lower than FineNet. However, our model strikes a better overall balance between sample quality and diversity, ensuring a more robust performance. Furthermore, our model attains the highest Beat Alignment Score of $0.2188 \pm 0.0042$, indicating that the generated dances are the most aligned with the music beats. We also tested the quality of the generated dances after randomly changing the input genre as shown in Table~\ref{table_2}, which demonstrates that even when the type of generated dance is changed, our model is still able to produce high-quality dances that are rhythmically aligned with the music.
\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
& \multicolumn{2}{c}{\textbf{Motion Quality}}          & \multicolumn{2}{c}{\textbf{Motion Diversity}}        & \multirow{2}{*}{\textbf{PFC}$\downarrow$  } & \multirow{2}{*}{\textbf{BAS}$\uparrow$ } \\  \cmidrule(lr{0.35em}){2-3}  \cmidrule(lr{0.35em}){4-5}
 & FID\_hand$\downarrow$             & FID\_body$\downarrow$            & Div\_hand$\uparrow$            & Div\_body$\uparrow$            &                         &                                \\
\midrule

Ours                                        & 14.43 & 22.51 & 8.41 & 6.92 & 0.23 & 0.24 \\
\bottomrule
\end{tabular}
}
\caption{Controllable Dance Generation - Quantitative Results on Random Changing Genre Examples. }
\label{table_2}
\end{table}

Additionally, we trained our method on the publicly available AIST++ dataset~\cite{li2021ai}, as shown in Table~\ref{table_3}. Following~\cite{li2021ai}, we utilized the FID\_m and Div\_m metrics, which evaluate the distributional spread of generated body part dances within the geometric feature space~\cite{muller2005efficient}. (Since ~\cite{muller2005efficient} does not provide geometric information for hand skeletons, it cannot be applied to the FineDance dataset.) Due to the absence of genre information and hand motion data in the AIST++ dataset, our model does not achieve the best results. Nevertheless, GCDance shows improvements in multiple metrics compared to the baseline model EDGE.

\begin{table}[!t]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{cccccc}
\toprule
& \multicolumn{2}{c}{\textbf{Motion Quality}}          & \multicolumn{2}{c}{\textbf{Motion Diversity}}        & \multirow{2}{*}{\textbf{BAS}$\uparrow$ } \\  \cmidrule(lr{0.35em}){2-3}  \cmidrule(lr{0.35em}){4-5}
 & FID\_k$\downarrow$             & FID\_m$\downarrow$            & Div\_k$\uparrow$            & Div\_m$\uparrow$            &                                \\
\midrule
FACT~\cite{li2021ai} & 86.43& 43.46& 6.85& 3.32& 0.1607\\
DanceNet~\cite{zhuang2020music2dance}& 69.18 & 25.49& 2.86 & 2.85 & 0.1430\\
DanceRevolution~\cite{huang2020dance} & 73.42& 25.92&  3.52 & 4.87 & 0.1950\\
Bailando~\cite{siyao2022bailando} & \textbf{28.16} & \textbf{9.62}& \textbf{7.83}& 6.34 &  0.2332\\
EDGE~\cite{tseng2023edge}
 & 42.16 & 22.12& 3.96& 4.61&  0.2334\\
GCDance (Ours)   & 36.81 & 20.10& 6.44& \textbf{6.47}& \textbf{0.2346}\\
\bottomrule
\end{tabular}
}
\centering
\caption{Performance comparison on the 24-joint AIST++ Dataset}
\label{table_3}
\end{table}
\vspace{-5pt}


\subsection{Ablation Studies}
As illustrated in Table~\ref{table_4}, we present the results of our ablation studies, analyzing the impact of modifications to the adapter, the composition of music features, and the influence of text information. 
Our results indicate that the adapters for both text and music contribute to improvements across multiple metrics. 
We also observe that removing the features from the music foundation model (\textit{i.e.}, relying solely on handcrafted music feature STFT) achieves a better FID score but a worse PFC score. Conversely, using only the foundation model features results in better PFC, diversity scores, and BAS but a worse FID score. This suggests that Wav2Clip features contribute to greater physical realism, diversity, and music alignment, while handcrafted features improve similarity to ground-truth dance motions. Additionally, the model without text input exhibits higher body diversity due to the absence of genre constraints, but suffers from a worse FID score. In contrast, our full model not only achieves superior overall performance, but also retains the ability to control the genre of the generated dances.


\begin{table}[!t]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{ccccccc}
\hline
& \multicolumn{2}{c}{\textbf{Motion Quality}}          & \multicolumn{2}{c}{\textbf{Motion Diversity}}        & \multirow{2}{*}{\textbf{PFC}$\downarrow$  } & \multirow{2}{*}{\textbf{BAS}$\uparrow$ } \\  \cmidrule(lr{0.35em}){2-3}  \cmidrule(lr{0.35em}){4-5}
                           & FID\_hand  $\downarrow$           & FID\_body $\downarrow$            & Div\_hand$\uparrow$            & Div\_body$\uparrow$            & \multicolumn{1}{c}{}                     &                      \\ \hline
\textbf{GCDance}                      & 18.48            & 22.61            & 8.77          & 6.77               &  0.17& 0.2188\\
w/o FM                     &                      18.15&                      25.94&                      8.86&                      6.88&                                          0.20&                      0.2160\\
w/o HM         &      20.81&         50.15&                      8.07&                      8.85&                                          0.18&                      0.2201\\
w/o music adapter   &     19.48&         27.98&      8.40&   6.51&        0.21&                      0.2037\\
w/o text adapter  &       21.97&      24.87&       8.01&      5.24&       0.18&           0.2121\\
 w/o text &                      20.41&                       28.43&                      8.29&                      7.07&                                          0.17&                      0.2152\\ \hline                                 
\end{tabular}
}
\centering
\caption{Ablation study where ‘FM’ represents foundation model-based features, and ‘HM’ represents handcrafted music features.}
\label{table_4}
\end{table}






\subsection{User study}
To better understand the impact of different music features on dance quality, we evaluate results generated using music features extracted by using various models including CLAP~\cite{wu2023large}, Wav2Vec2.0~\cite{baevski2020wav2vec}, Jukebox~\cite{dhariwal2020jukebox}, Wav2CLIP~\cite{wu2022wav2clip}, and the $35$-Feature Set, a $35$-dimensional feature representation provided by the FineDance dataset~\cite{li2023finedance}. Table~\ref{tab:table_5} shows that our method, which incorporates music features extracted from a music foundation model and handcrafted features, achieves the best overall performance. It effectively balances all key metrics, demonstrating superior motion quality, diversity, and rhythm consistency compared to the dance generated by other music feature-based methods.

\begin{table}[!t]
\centering


\resizebox{0.5\textwidth}{!}{
\begin{tabular}{ccccccc}
\hline
 & \multicolumn{2}{c}{\textbf{Motion Quality}}                                      & \multicolumn{2}{c}{\textbf{Motion Diversity}}        & \multirow{2}{*}{\textbf{PFC} $\downarrow$ }        & \multirow{2}{*}{\textbf{BAS} $\uparrow$} \\ \cline{2-5}
& FID\_hand $\downarrow$                        & FID\_body $\downarrow$                        & Div\_hand$\uparrow$& Div\_body$\uparrow$&                                &                                \\ \hline
\makecell{CLAP\\~\cite{wu2023large}}                         &                                    29.64& \multicolumn{1}{c}{27.52}               &                          8.11&                                6.10&                                0.23& 0.2076\\
   \makecell{Wav2Vec2.0\\~\cite{baevski2020wav2vec}}                        &                                    17.76&                                    33.82&                          8.57&                                6.44&                                0.20&                                0.2026\\
\makecell{Jukebox\\~\cite{dhariwal2020jukebox}}                     &                                    23.02&                                    32.26&                          7.41&                                6.38&                                0.24&                                0.2238\\
 \makecell{35-Feature Group*\\~\cite{li2023finedance}}   &                                    20.61&                                    25.41&                          8.22&                                5.84&                                0.15&                                0.2028\\ \hline
GCDance (Ours) & 18.48            & 22.61            & 8.77          & 6.77               &  0.17                & 0.2188\\ \hline
\end{tabular}
}

\caption{Impact of Music Features on Generated Dance Quality}
\label{tab:table_5}
\end{table}





\subsection{Model Efficiency}

We also compared the efficiency of different models in Table~\ref{table_6}. During the inference phase, we evaluated the model parameters and inference times for generating $4$-second dance sequences. The table shows that our model achieves an inference time of $0.22$ seconds with $87M$ parameters, achieving a strong balance between model size and speed. It closely matches FineNet in speed while utilizing fewer parameters. Compared to Bailando and FACT, our model significantly reduces the computational cost with a smaller parameter count and much faster inference times. Although slightly slower than EDGE, which achieves the fastest inference time with smaller parameters, our model offers a better balance between efficiency and motion quality.


\begin{table}[h!]
\centering
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{ccc}
\hline
\multicolumn{1}{c}{\textbf{Model}} & \textbf{Parameters} & \textbf{Inference Time} \\ \hline
FACT~\cite{li2021ai}                      & 120M       &       33.20s        \\
Bailando~\cite{siyao2022bailando}                  & 152M       &       0.94s         \\
EDGE~\cite{tseng2023edge}                      & 50M        &       0.13s           \\
FineNet~\cite{li2023finedance}                  & 94M        &       0.23s       \\ \hline
GCDance (Ours)                      & 87M        &      0.22s         \\ \hline
\end{tabular}
}
\caption{Model parameters and \textit{per-instance} inference time}
\label{table_6}
\end{table}




\section{Conclusion and Future Work}

In this paper, we have presented GCDance, a diffusion-based $3$D dance generation framework conditioned on both music and text prompts. Our approach enables genre-controllable dance generation by integrating genre information, allowing for the synthesis of dance motions that align with specified genres while maintaining diversity and high quality. Extensive experiments conducted on a $52$-joint full-body dataset demonstrate the superiority of our method from both qualitative and quantitative perspectives. In future, we would like to investigate the framework performance over fine-grained localized edits to dance motion generation, varying the input text prompts at each decoding time step. 





\bibliography{paper}
\bibliographystyle{icml2025}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Editing Framework Details}
\label{s0}

To enable flexible editing of dance sequences, our method applies a diffusion inpainting mechanism during the sampling process, which allows users to apply a wide range of
constraints as shown in Figure~\ref{edit}.
Given a subset of joint-wise or temporal constraint inputs 
$\bm m^{\bm known}  $, with positions indicated by a binary mask $\textit{B}$, the model performs the following denoising steps during sampling. 
\begin{equation}
\bm{d}_{t-1} := \textit{B} \odot q(\bm{m}^{\bm{known}}, t-1) + (1 - \textit{B}) \odot \bm{d}_{t-1}
\end{equation}
where $\odot $ denotes the Hadamard product, an element-wise operation that substitutes the known part of the motion with noisy samples based on the specified constraint.

Taking the example of editing  dance sequence based on key joints, if we want to generate suitable hand joint motion
based on body movements. User can provide a reference motion $\bm m^{\bm known} \in \mathbb{R}^{k \times 319} $ along with a mask $\textit{B}\in \left \{0,1\right \}^{k\times 319}$, where $\textit{B}$ has all $0$ for the hand joint features and all $1$ for the body joint features. 
This setup will generate a sequence of $k$ frames, where the body joint movements are based on the user-provided reference, and the hand joint regions are filled with consistent and coherent hand dance movements. The editing framework serves as a robust tool for downstream applications, offering flexible control over both temporal and spatial elements to create dance sequences that precisely conform to a variety of user-defined constraints.


\begin{figure}[h!]
  \centering
  \includegraphics[width=.99\textwidth]{Figures/edit1.jpg}
  \caption{GCDance allows users to generate joint-specific and temporally-specific dance segments. In the left example, the constrained body joints are shown in gray, while the generated hand joints are depicted in cyan. In the middle example, the constrained upper- body joints are shown in purple, and the generated leg joints are depicted in yellow. In the right example, the constrained first second is shown in red, while the generated last three seconds are depicted in green.}
  \label{edit}
\end{figure}


\section{Implementation details}
\label{s1}
The training of our proposed approach is conducted on two NVIDIA GeForce RTX $3090$ GPUs. We use the Adan optimizer with a learning rate of $2e-4$ and optimize the model using the $L2$ loss function.
The batch size was set to $512$, and the total number of training epochs was $2000$. The hidden dimension is set to $512$.
All training segments of the music-dance data are uniformly divided into fixed durations of $120$ frames, corresponding to $4$ seconds at $30$ frames per second (FPS). During inference, the guidance weight was set to $2.7$.  





\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.