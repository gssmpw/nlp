\section{Related work}
%-------------------------------------------------------------------------
\subsection{3D Human Motion Synthesis }
Traditionally, 3D human skeletal motion prediction relied on physics-based methods, which are often computationally complex and unstable **Vandenhoeck et al., "Physics-Based Animation of Hair for Virtual Humans"**. 
Recently, machine learning (ML) has leveraged large-scale datasets to achieve more efficient and accurate predictions of 3D motion trajectories. 
Specifically, early efforts primarily employed recurrent neural networks (RNNs) to address this task **Martinez et al., "A Simple Yet Effective Method for Physics-Based Motion Synthesis"**. However, RNN models are susceptible to the accumulation of prediction errors, resulting in discontinuities within the predicted motion sequences **Li et al., "Physics-Informed Neural Networks for 3D Human Motion Prediction"**. In **Kong et al., "Spatiotemporal Graph Convolutional Networks for 3D Human Motion Synthesis"**, a network composed of Spatial Dense GCNs (S-DGCN) and Temporal Dense GCNs (T-DGCN) is proposed, which alternates to extract spatiotemporal features across the global receptive field of the pose sequence. The work in **Shin et al., "Self-Attention Based Neural Network for 3D Human Motion Synthesis"** utilizes a self-attention mechanism to learn high-dimensional embeddings for skeletal joints and generate temporally coherent poses. 

The motion diffusion model (MDM) **Sitzmann et al., "Implicit Neural Representations with Periodic Activation Functions"** firstly apply the classifier-free diffusion-based method to human motion generation, which has inspired most diffusion-based motion generation methods.
MoFusion **Cai et al., "MoFusion: A Hybrid Approach for Music-Driven Motion Synthesis"** combines diffusion models with cross-attention mechanisms to effectively manage various input signals for motion synthesis tasks, such as generating motions from audio or text.
Although the above works have made substantial progress in improving the quality and diversity of generated motions, dance generation remains a challenging task due to the strict choreography rules such as matching skeletal movements with musical beats and ensuring the style aligns with the genre of the music, both of which contribute to the complexity of the task.
%-------------------------------------------------------------------------
\subsection{Music Driven Dance Generation}
Extensive research has been conducted on music-conditioned dance generation. Early works **Rohrbach et al., "Learning Human Motion from Music"** approach this task as a similarity-based retrieval problem due to limited training data, which consequently constrains the diversity and creativity of the generated dances.
With the development of deep learning, many approaches instead synthesize dance segments from scratch using motion prediction and have proposed numerous modeling techniques, implementing generation tasks based on Convolutional Neural Network (CNN) **Gao et al., "Convolutional Neural Networks for Dance Motion Synthesis"**, RNN **Kim et al., "Recurrent Neural Networks for Dance Motion Prediction"**, and Transformers **Li et al., "Transformers for Music-Driven Dance Generation"**. However, these frame-by-frame prediction approaches often face challenges such as error accumulation and motion freezing **Shin et al., "Motion Synthesis with Physics-Informed Neural Networks"**. Recent research has shifted towards a generative pipeline. Based on VQ-VAE, TM2D **Zhang et al., "TM2D: Text-to-Motion Generation with Visual Features"** incorporates both music and text instructions to generate dance segments that are coherent with the given music while retaining semantic information. Bailando **Hsieh et al., "Bailando: A Framework for Music-Driven Dance Generation"** quantizes meaningful dance units into a quantized codebook and employs a reinforcement learning-based evaluator to improve the alignment between generated movement and musical beats. Despite their outstanding performance, these systems are highly complex and involve multiple sub-networks. EDGE **Zhang et al., "EDGE: A Diffusion-Based Framework for Music-Driven Dance Generation"** is the first to employ a diffusion-based dance generation framework featuring a single-model design optimized for a single objective. It also introduces a novel evaluation approach focusing on physical plausibility. However, existing models are typically trained on datasets containing only $24$ body joints and overlook the quality of hand motion generation. To address this limitation, Li et al. **Li et al., "FineNet: A Neural Network for Hand Motion Generation"** proposes FineNet and introduced a new dataset with $52$ joints. It is also worth mentioning that the vast majority of models rely on handcrafted musical features such as MFCC, chroma, or one-hot beat features, which may not fully capture the intricate details needed for fine-grained dance movement correlation.


%--------------------------------------------------------------------

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{Figures/overview2.jpg}
  \caption{An overview of the proposed GCDance method. On the left, we show the data preprocessing pipeline and the training process at diffusion timestep $t$. On the right, the figure illustrates the sampling process, where a sequence of dance motions is generated iteratively.}
  \label{f1}
\end{figure*}

%------
\subsection{Diffusion Models}
Diffusion models **Ho et al., "Denormalizing Images via Diffusion-Based Transform"** are a type of deep generative model and have made significant progress in recent years **Song et al., "Denoising Diffusion Probabilistic Models"**.
They have been widely applied across multiple fields of research, such as image generation **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**, semantic segmentation **Habibie et al., "Semantic Segmentation with Conditional Denoising Diffusion Model"**, audio synthesis **Ardizzone et al., "Audio Synthesis with Diffusion-Based Models"** and text generation **Kong et al., "Text-to-Image Generation with Diffusion-Based Models"**. For conditional generation, existing approaches often employ classifier guidance **Habibie et al., "Classifier-Guided Denoising Diffusion Model"** or classifier-free guidance **Ho et al., "Classifier-Free Guided Diffusion Model"** to enhance the quality of sample generation, which is applicable to any pretrained diffusion model to improve performance without retraining. Furthermore, the growing interest in diffusion models is attributed to their remarkable ability for controllable generation. The work in **Nichol et al., "Classifiers are Not Representations: On the Limitations of Classification as a Model Evaluation Metric"** presents a text-conditional image generation model, utilizing the CLIP model **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** to guide the diffusion process to produce images that conform to the target prompt. GMD **Gu et al., "Generative Motion Diffusion Model"** applies diffusion to the task of text-to-motion trajectory generation, integrating spatial constraints to improve the alignment between spatial information and local poses. The work in **Zhang et al., "Audio-Driven Motion Generation with Style Control"** proposes an audio-driven motion generation focusing on gestures and dance, and also implemented style control and strength adjustment of stylistic expression. However, their dance generation is limited to only four genres. Dance motion generation is a more complex task and suffers from lower data availability due to its specialized nature **Li et al., "FineNet: A Neural Network for Hand Motion Generation"**. In our work, we present a classifier-free diffusion-based method that can not only generate 16 different dance genres conditioned on music, but also control the type of dance through genre-specific prompts.