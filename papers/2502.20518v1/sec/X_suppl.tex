\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{The Correlation between GIAA and PIAA Performance}\label{sec:som:giaa_upbound}

\renewcommand{\thetheorem}{3.1}
\begin{theorem}
\label{thm:giaa_piaa_loss}
Let $s$ represent the aesthetic score, $\hat{P}(s)$ denote the predicted score distribution produced by an IAA model that takes only images as input, and $\delta_{i}(s)$ denote the ground-truth score distribution for user $i$, expressed as a one-hot vector for an individual score. Let $n$ be the total number of users.

The GIAA and PIAA loss functions are defined as
\begin{align}
    \mathcal{L}_{GIAA} &= \left| \hat{P}(s) - \frac{1}{n} \sum_{i=1}^{n} \delta_{i}(s) \right|,\\
    \mathcal{L}_{PIAA} &= \frac{1}{n} \sum_{i=1}^{n} \left| \hat{P}(s) - \delta_{i}(s) \right|,
\end{align}
respectively. Then, we have
\begin{equation}
\mathcal{L}_{GIAA} \leq \mathcal{L}_{PIAA}.
\end{equation}
This result holds not only when $\hat{P}(s)$ and $\delta_{i}(s)$ represent score distributions but also when they are scalar scores.
\end{theorem}


\begin{proof}
Given $\hat{P}(s)$ is the predicted score distribution by an IAA model, $\delta_{i}(s)$ is the score distribution for user $i$, and $s$ is the score, the GIAA loss function $\mathcal{L}_{GIAA}$ is
\begin{equation}
\begin{aligned}
    \mathcal{L}_{GIAA} &= | \hat{P}(s) - \frac{1}{n} \sum_{i=1}^{n} \delta_{i}(s) | 
    \\ &= \frac{1}{n} | \sum_{i=1}^{n} ( \hat{P}(s) - \delta_{i}(s) ) | 
    \\ &\leq \frac{1}{n} \sum_{i=1}^{n} | \hat{P}(s) - \delta_{i}(s) | = \mathcal{L}_{PIAA}
\end{aligned}
\end{equation}
where the inequality holds by the triangular inequality and $\mathcal{L}_{PIAA}$ is the PIAA loss function. This inequality suggests that IAA models perform better on GIAA tasks than on PIAA tasks when the model is unconditioned to the user, even when trained on PIAA data. Note that the same proof applies when predicting scores instead of score distributions. By replacing $\hat{P}(s)$ with the predicted score and $\delta_{i}(s)$ with the score for user $i$, the sketch of proof remains unchanged.
\end{proof}


\section{Data Splitting for IAA}\label{sec:som:data_split}
Existing PIAA studies~\cite{yang2022personalized,zhu2022personalized,shi2024personalized,zhu2020personalized,li2022transductive,yang2023multi} rely on pre-trained GIAA models, which can introduce data leakage when the same datasets are employed for both GIAA pre-training and PIAA fine-tuning. 
The data leakage issue arises because GIAA models typically split data by images, as shown in Figure~\ref{fig:data_split:giaa}, whereas PIAA separates training and test sets by users, as illustrated in Figure~\ref{fig:data_split:piaa}, followed by few-shot sampling. As a result, the same images may appear in both the GIAA training phase and the PIAA testing phase.

In this work, we adopt an alternative data-splitting strategy to evaluate models' zero-shot performance on unseen users, addressing data leakage and establishing a consistent evaluation framework for both GIAA and PIAA. We follow the standard GIAA approach by initially dividing the data into training, validation, and test images. Additionally, we separate training and test users based on demographics; for instance, females are selected as training users, while males are designated as test users. Specifically, the training set comprises (training images, training users), the validation set includes (validation images, training users), and the test set contains (test images, test users), as shown in Figure~\ref{fig:data_split:consistent}.


\begin{figure}[ht]
    \centering
    % First subfigure
    \begin{subfigure}[]{1.\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/data_split_GIAA.pdf}
        \caption{Data Splitting for GIAA. The training, validation, and test data are split according to images.}
        \label{fig:data_split:giaa}
    \end{subfigure}
    % Second subfigure
    \begin{subfigure}[]{1.\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/data_split_PIAA.pdf}
        \caption{Data Splitting for PIAA. The training and test data are split according to users, followed by few-shot image sampling for training and evaluation.}
        \label{fig:data_split:piaa}
    \end{subfigure}
    % Third subfigure
    \begin{subfigure}[]{1.\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/data_split.pdf}
        \caption{Data Splitting for the zero-shot evaluation scheme on unseen users. The training set comprises (training images, training users), the validation set includes (validation images, training users), and the test set contains (test images, test users).}
        \label{fig:data_split:consistent}
    \end{subfigure}

    \caption{An overview of the data splitting for (a) GIAA, (b) PIAA, and (c) our proposed consistent splitting for both GIAA and PIAA to prevent data leakage. \textit{Train}, \textit{Val}, and \textit{Test} represent training, validation, and test data, respectively.}
    \label{fig:data_split}
\end{figure}



\section{Example from the PARA and LAPIS Datasets}\label{sec:som:datasets}
A GIAA example of the PARA dataset~\cite{yang2022personalized} (photos) is shown in Figure~\ref{fig:dataset_samples:para}. The score distribution is assembled from individual scores, as detailed below (user IDs follow personal scores in the dataset):
\begin{itemize}
    \item \textbf{Score 2.0}: Acb3e21, Bcb3b4b
    \item \textbf{Score 2.5}: A64c0cc, B422745
    \item \textbf{Score 3.0}: Abdb7c8, A768d0b, B2eb88a
    \item \textbf{Score 3.5}: A3c6418, A50152a, A09eb7d, A27fab0, Ac9545a, A409131, A5aa4a1, Bab5779
    \item \textbf{Score 4.0}: A7497fb, A697287, A03efa2, A2ce68c, Bbc1ee7, B9042fc, B0fa3ef
    \item \textbf{Score 4.5}: A3f6a35, B6260a3
\end{itemize}
where these personal scores are the groundtruth of PIAA data, the same holds for the LAPIS datasets~\cite{maerten2024lapis}. An example of the LAPIS dataset (artworks) is shown in Figure~\ref{fig:dataset_samples:lapis}.


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/PARA_hito_component.pdf}
        \caption{GIAA Data on the PARA dataset~\cite{yang2022personalized}, which consists of an image and a score distribution assembled from individual data. The scores are scaled from 0 to 5 with a spacing of 0.5. The mean score of the score distribution is 3.46.}
        
        \label{fig:dataset_samples:para}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/LAPIS_hito_component.pdf}
        \caption{GIAA Data on LAPIS dataset~\cite{maerten2024lapis}, which consists of an image and score distribution assembled from individual data. The scores are scaled from 0-10. The mean score of the score distribution is 3.37.}
        \label{fig:dataset_samples:lapis}
    \end{subfigure}
    \caption{Comparative visualization of GIAA data on the PARA and LAPIS datasets, including score distributions and mean scores.}
    \label{fig:dataset_samples}
\end{figure}



\section{Details of Demography in the LAPIS Dataset}\label{sec:som:demo_detail}
The countries included in the LAPIS dataset are: 'British', 'South African', 'American', 'Portuguese', 'Hungarian', 'Malaysian', 'Belgian', 'Northern Irish', 'Polish', 'Slovenian', 'Spanish', 'Italian', 'Egyptian', 'Scottish', 'Mexican', 'Irish', 'South Korean', 'Greek', 'Czech', 'Brazilian', 'Canadian', 'Indian', 'Ugandan', 'Zimbabwean', 'Dutch', 'Welsh', 'French', 'Finnish', 'German', 'Bangladeshi', 'Lithuanian', 'Australian', 'Tunisian', 'Swiss', 'Romanian', 'Chilean', 'Austrian', 'Nigerien', 'Estonian', 'Bulgarian', 'Turkish', 'Vietnamese', 'Latvian', and 'Malawian'. 
% Besides, the gender categories include male and female as the majority, with non-binary and unknown as minority categories not detailed in the main text.
Besides, the gender categories include male, female, non-binary, and unknown. The latter two, being minority categories, are not mentioned in the main text.
  
\section{Benchmark of PIAA Baselines on PARA}\label{sec:som:mir}
This section discusses the benchmarking of our implementation of the PIAA baselines, including PIAA-MIR~\cite{zhu2022personalized} and PIAA-ICI~\cite{shi2024personalized} models, on the PARA dataset. Note that the original code for these works is not publicly available.
These studies employ a meta-learning scheme for evaluation~\cite{zhu2020personalized}, where a meta-learner is trained on training users and then evaluated on test users, both using few-shot samples. In both stages, the model is fine-tuned on few-shot images and evaluated on the remaining images. In the existing PIAA works on PARA~\cite{yang2023multi,zhu2022personalized,shi2024personalized}, 40 test users were sampled from a total of 438 users and remained fixed throughout the evaluation process. 

Our implemented PIAA-MIR and PIAA-ICI achieve SROCC values of 0.716 and 0.732 with 100-shot fine-tuning, showing comparable performance to the reported values of 0.716~\cite{zhu2022personalized} and 0.739~\cite{shi2024personalized}, respectively.  However, it should be noted that these results may not be strictly comparable, as the test users in this work may differ from those in the original study due to the unavailability of the original test users. Our code will be made publicly available upon publication.

% Additionally, the randomness of user sampling may also impact performance, although this factor was not addressed in previous PIAA studies~\cite{yang2022personalized,zhu2022personalized,shi2024personalized,zhu2020personalized,li2022transductive,yang2023multi}.

% The evaluation pipeline was repeated 10 times, demonstrating that variation due to image sampling was negligible\footnote{The relative standard deviation was less than 0.2\%}. 

% However, since these 40 test users are not publicly available, directly comparing our implementation with the original work is not feasible. Additionally, because personal aesthetic preferences can vary significantly based on factors such as individual traits, nationality, and genetics~\cite{bignardi2024genetic}, sampling different sets of test users leads to numerous combinations of diverse aesthetic profiles.
% To address these challenges, we adapted the evaluation scheme by omitting the meta-learning training phase and directly evaluating the models on 401 users who annotated more than 500 images. The performance of these models is presented in Fig.~\ref{fig:piaa_benchmark}. The results reveal substantial variation among users, with both models achieving performance comparable to the originally reported results for approximately one-quarter of the users.


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/PIAA_benchmark.jpg}
%     \caption{Benchmark of PIAA baselines under 100-shot training.}
%     \label{fig:piaa_benchmark}
% \end{figure}



\section{Model Architecture and Traits Encoding}\label{sec:som:models}
\textbf{NIMA-trait}. This section describes the details of the model and data encoding. Figure~\ref{fig:nima_trait} illustrates the architecture of NIMA-trait. The MLP component is a two-layer multilayer perceptron (MLP) with 512 and 10 units in the hidden and output layers, respectively. The input dimension of the MLP corresponds to the trait dimension, which varies based on the trait encoding method used, as discussed below.

\textbf{Trait Encoding}. For trait encoding, the trait dimensions on PARA are 25 and 70, and on LAPIS are 71 and 137, corresponding to conventional encoding and one-hot encoding setups for numeric traits, respectively. The same encoding is also applied to PIAA-MIR~\cite{zhu2022personalized} and PIAA-ICI~\cite{shi2024personalized} in this work.
Specifically, for PARA, the attributes gender, age, educational level, photography experience, and art experience are one-hot encoded into 2, 5, 5, 4, and 4 dimensions, respectively. Combined with the Big-5 personality traits, this results in a total dimension of 25. In the one-hot encoding setup for numeric traits (e.g., Big-5), each Big-5 trait is further one-hot encoded into 10 bins, resulting in a dimension of 50 for all Big-5 traits combined, bringing the total to 70.
For LAPIS, the attributes gender, color blindness, age, educational level, and nationality are one-hot encoded into 4, 2, 5, 5, and 44 dimensions, respectively.  Together with the 11 VAIAK scores, this gives a total dimension of 71. In the one-hot encoding setup for numeric traits (e.g., VAIAK scores), each VAIAK score is further one-hot encoded into 7 bins, yielding a dimension of 77 for all VAIAK scores combined, resulting in a total dimension of 137.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/NIMA_trait.pdf}
    \caption{Model architecture of NIMA-trait, which utilize a ResNet-50~\cite{he2016deep} model as the image encoder. The variable \textit{img} represents images, while \textit{P(trait)} and \textit{P(score)} denote the trait distribution and score distribution, respectively.}
    \label{fig:nima_trait}
\end{figure}


\section{GIAA and PIAA Performance on Unseen Users}\label{sec:som:benchmark}
We evaluate the performance of NIMA, PIAA-MIR, and PIAA-MIR (Onehot-enc.) across different demographic splits to explore the model generalization to unseen users. 
For the GIAA performance, NIMA, PIAA-MIR (Onehot-enc.) trained on both GIAA and PIAA are evaluated. Their performance is depicted in Figure~\ref{fig:srocc:para_giaa} and Figure~\ref{fig:srocc:lapis_giaa} on PARA and LAPIS datasets, respectively. 
We observe a trend similar to the EMD results shown in Figure 2 of the main text. For the PARA dataset, the SROCC values are low for experts in both photography and art, as well as for users with only junior high school education, highlighting their distinct aesthetic preferences for photos. For the LAPIS dataset, the SROCC values are low for older users, users with either doctoral or primary-level education, and those with extensive art experience (especially for 2VAIAK1-4), indicating their distinct aesthetic preferences for artworks.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/trait/PARA_SROCC_BarPlot_TestGIAASROCC.jpg}
    \caption{GIAA SROCC between users with specific demographic characteristics and all other users on the PARA dataset.}
    \label{fig:srocc:para_giaa}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/trait/PARA_SROCC_BarPlot_TestPIAASROCC.jpg}
    \caption{PIAA SROCC between users with specific demographic characteristics and all other users on the PARA dataset.}
    \label{fig:srocc:para_piaa}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/trait/LAPIS_SROCC_BarPlot_TestGIAASROCC.jpg}
    \caption{GIAA SROCC between users with specific demographic characteristics and all other users on the LAPIS dataset.}
    \label{fig:srocc:lapis_giaa}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/trait/LAPIS_SROCC_BarPlot_TestPIAASROCC.jpg}
    \caption{PIAA SROCC between users with specific demographic characteristics and all other users on the LAPIS dataset.}
    \label{fig:srocc:lapis_piaa}
\end{figure}

For the PIAA performance, PIAA-MIR is additionally included. The model performances are depicted in Figure~\ref{fig:srocc:para_piaa} and Figure~\ref{fig:srocc:lapis_piaa} show their performance on PARA and LAPIS datasets, respectively.
Compared to the GIAA evaluation scheme, we observe similar PIAA performance across various demographic splits, suggesting that model inference under the PIAA evaluation scheme may be more robust for unseen users. Although the underlying rationale is not yet fully clear, it is possible that individual subjectivity is crucial for the model’s generalization to unseen users, while such subjectivity is suppressed by averaging individual data in GIAA.
%
Moreover, we observe similar performance between the PIAA-MIR (Onehot-enc.) models trained on GIAA and PIAA, suggesting that the interpolation and extrapolation discussed in our transfer learning theory do not hold here. The reason is that our theory addresses the difference between individual distributions and averaged distributions within a single convex hull (either in trait space or score space), where all users are accessible to the model during the training phase. However, with an unseen user setup, this configuration splits the convex hull into two parts: one for training users and one for test users, thus rendering the discussion of interpolation and extrapolation inapplicable. We aim to further explore our theory under these conditions in future work.
