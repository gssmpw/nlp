\section{Experimental Results}\label{sec:results}
\subsection{Evaluation Scheme of PIAA}~\label{ssec:results:eval_scheme}
%
Existing PIAA works~\cite{yang2022personalized,zhu2022personalized,shi2024personalized,zhu2020personalized,li2022transductive,yang2023multi} employ pre-trained GIAA models, which can lead to data leakage if the same datasets are used for both pre-training and PIAA fine-tuning. This risk arises because GIAA splits data by images, whereas PIAA divides training and test sets by users, potentially resulting in the same images being present in both the GIAA training phase and the PIAA testing phase. Here we use a different setup to split the data to prevent such data leakage and establish a consistent evaluation scheme for both GIAA and PIAA. 
% 
We follow the conventional GIAA approach, splitting the data into training, validation and test images. To further ensure a fair comparison between GIAA and PIAA on the same images and users, we omit few-shot sampling and instead use the full training data for PIAA. Additionally, PIAA models are trained collectively on data from all users, rather than independently for each user as in the meta-learning setup.
%
While this evaluation scheme involves the same users in both training and testing, it ensures consistent training data between GIAA and PIAA, allowing us to validate our transfer learning theorem without addressing model generalization to unseen users.

As for the model generalization on unseen users, we evaluate the models' zero-shot performance by maintaining the same image split while further separating training and test users based on demographics. For example, choosing females as training users and males as the test users.
Specifically, the train set consists of (training images, training users), the validation set of (validation images, training users), and the test set of (test images, test users), as depicted in Section 2 of the supplementary material. 

\subsection{Datasets}~\label{ssec:results:dataset}
% There abundant IAA datasets~\cite{yang2022personalized,kong2016photo,murray2012ava,kang2020eva,he2022rethinking,huang2022aesthetic,Ren_2017_ICCV,yi2023towards} while few provide personal aesthetic score. Flicker-AES~\cite{Ren_2017_ICCV} provide the personal aesthetic score while lacking the trait of users. 
Despite the abundance of image aesthetics resources in GIAA~\cite{yang2022personalized, kong2016photo, murray2012ava, kang2020eva, he2022rethinking, huang2022aesthetic, Ren_2017_ICCV, yi2023towards}, only a few datasets provide personal aesthetic scores for PIAA, such as FLICK-AES~\cite{Ren_2017_ICCV}, PARA~\cite{yang2022personalized}, and LAPIS~\cite{maerten2024lapis}. In this work, we demonstrate our method on the PARA and LAPIS datasets, which include photos and artworks, respectively. A sample from these datasets is shown in Section 3 of supplementary material.
Notably, FLICK-AES is excluded from this study as it lacks personal trait data. %, providing only personal aesthetic scores.

The \textbf{PARA} dataset~\cite{yang2022personalized} includes 31,220 photos and 438 users, with each image labeled by an average of 25 users. It provides personal aesthetic scores along with 8 aesthetic attributes, such as contrast, quality, lighting, and composition, for each (image, user) pair. Additionally, the dataset includes detailed personal traits for each user, including age, gender, education, Big-5 personality traits, and experience in art and photography.
Existing works~\cite{yang2022personalized, zhu2022personalized, shi2024personalized} adopt different approaches for creating the test set, either by splitting according to images, as in GIAA, or by splitting according to users, as in PIAA. 

%
Here, we instead use a consistent evaluation scheme for both GIAA and PIAA, as described in Section~\ref{ssec:results:eval_scheme}. Specifically, we follow the conventional GIAA approach, dividing the data into 25,398 training images, 2,822 validation images, and 3,000 test images. Users are segmented based on demographic characteristics as follows: Gender (male/female); age groups (18–21, 22–25, 26–29, 30–34, and 35–40); educational levels, including junior high school, senior high school, technical secondary school, junior college, and university; and photo and art experience, categorized as beginner, competent, proficient, or expert.

The \textbf{LAPIS} dataset~\cite{maerten2024lapis} includes 11,723 artworks and 578 users, with each image labeled by an average of 24 users. It provides personal aesthetic scores along with art style for each (image, user) pair. The dataset includes detailed personal traits for each user, including age, gender, education, and the 11 Vienna Art Interest and Art Knowledge (VAIAK) values~\cite{specker2020vienna}~(VAIAK1-7 and 2VAIAK1-4). Here, art interest reflects an individual’s engagement with art such as visiting galleries or engaging with art materials, while art knowledge captures familiarity with art styles, artists, and historical facts.
%
In this work, we use the same splitting strategy as with the PARA dataset: the images are divided into 7,074 for training, 2,358 for validation, and 2,358 for testing. Users are segmented based on demographic characteristics as follows: Gender is categorized as female or male. Age groups are divided into 18–27, 28–38, 39–49, 50–60, and 61–71. Educational levels include primary education, secondary education, Bachelor's or equivalent, Master's or equivalent, and Doctorate. Nationalities: 44 countries are detailed in Section 4 of the supplementary material. VAIAK levels are classified as low or high from the original scale ranging from 0 to 6, with low values ($\leq 3$) and high values ($> 3$).

\subsection{Models}
We select the state-of-the-art PIAA models, including PIAA-MIR~\cite{zhu2022personalized} and PIAA-ICI~\cite{shi2024personalized}, as baseline models capable of zero-shot performance on unseen users. We have implemented these baseline models and reproduced the reported results, as detailed in Section 5 of the supplementary material. 
%
In addition, we select NIMA~\cite{talebi2018nima} as the GIAA baseline model for comparison, rather than using state-of-the-art GIAA models~\cite{he2022rethinking,yi2023towards}, for two reasons. First, the PIAA models evaluated in this work are based on NIMA. Second, these state-of-the-art GIAA models~\cite{he2022rethinking,yi2023towards} incorporate additional attributes, such as theme~\cite{he2022rethinking} and style~\cite{yi2023towards}, which would make a comparison between GIAA and PIAA models less equitable.

We adapt NIMA, PIAA-MIR, and PIAA-ICI to onehot encoding, enabling inference for both GIAA and PIAA, which we refer to as NIMA-trait, PIAA-MIR (Onehot enc.), and PIAA-ICI (Onehot enc.), respectively. Specifically, we encode all traits with onehot encoding, including numeric traits such as the Big-5 on the PARA dataset and the VAIAKs on the LAPIS dataset. For NIMA-trait, these traits are integrated into NIMA's predictions via an additional 
two-layer multilayer perceptron (MLP). The model architecture is detailed in Section 6 of supplementary material.
% simple neural network. 
All models use ResNet-50~\cite{he2015deep} pretrained on ImageNet~\cite{deng2009imagenet} as the backbone. 
%
For the GIAA inference of NIMA-trait, PIAA-MIR (Onehot enc.), and PIAA-ICI (Onehot enc.), we adjust the inference method that models receive the average trait distribution across all training users during evaluation. This ensures a fair comparison to the GIAA scenario, where images are the only input. 
%
All IAA models are evaluated using Spearman's Rank Correlation Coefficient (SROCC) as the evaluation metric.

\subsection{Model Evaluation on overlapped users}
We evaluate NIMA, PIAA-ICI, PIAA-MIR, and their associated \textbf{onehot-encoded models} that utilize onehot encoding for all traits. Tables~\ref{table:para_piaa} and~\ref{table:lapis_piaa} present the results. 
When trained on GIAA, onehot-encoded models achieves performance comparable to the NIMA baseline\footnote{Note that the reported SROCC for NIMA on the PARA dataset is 0.8790~\cite{yang2022personalized}, and our implementation achieves a comparable score.}.
On the other hand, \textbf{when trained on PIAA, these models even outperform the PIAA baselines such as PIAA-MIR and PIAA-ICI.} These results demonstrate that our simple yet effective approach performs well in both the GIAA and PIAA settings.
%
Moreover, the generalization between GIAA and PIAA is evident, where the zero-shot PIAA performance denoted in \textcolor{blue}{blue} is significantly worse than the PIAA baselines. On the other hand, the zero-shot GIAA performance denoted in \textcolor{red}{red} is generally comparable to the GIAA baseline, except for the onehot-encoded PIAA-ICI on the LAPIS dataset. This observation aligns with our analysis in Section~\ref{sec:geometry}.

\begin{table}[]
\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{ccccc}
% Model & TrainSet & GIAA & GIAA (Prior) & PIAA \\  \hline \hline
Model & TrainSet & GIAA & PIAA \\  \hline \hline
NIMA & GIAA & 0.875 & 0.570 \\ 
NIMA-trait & GIAA & 0.883 & \textcolor{blue}{0.514} \\
PIAA-MIR (Onehot-enc.) & GIAA & 0.870 &  \textcolor{blue}{0.562} \\
PIAA-ICI (Onehot-enc.) & GIAA & 0.873 & \textcolor{blue}{0.578} \\ \hline
PIAA-MIR* & PIAA & - & 0.717 \\ 
PIAA-ICI* & PIAA & - & 0.731 \\ 
NIMA-trait & PIAA & \textcolor{red}{0.844} & 0.708 \\ 
PIAA-MIR (Onehot-enc.) & PIAA & \textcolor{red}{0.860} & 0.741 \\
PIAA-MIR (Onehot-enc.)* & PIAA & \textcolor{red}{0.871} & \textbf{0.741} \\ 
PIAA-ICI (Onehot-enc.) & PIAA & \textcolor{red}{0.837} & 0.729 \\
PIAA-ICI (Onehot-enc.)* & PIAA & \textcolor{red}{0.818} & 0.728 
% NIMA & GIAA & 0.875 & - & 0.570 \\ \hline
% PIAA-MIR* & PIAA & - & - & 0.717 \\ \hline
% PIAA-ICI* & PIAA & - & - & 0.731 \\ \hline
% \multirow{2}{*}{NIMA-trait} & GIAA & 0.883 & 0.883 & \textcolor{blue}{0.514} \\
%  & PIAA & \textcolor{red}{0.843} & \textcolor{red}{0.844} & 0.708 \\ \hline
% \multirow{2}{*}{PIAA-MIR (Onehot-enc.)} & GIAA & 0.871 & 0.870 &  \textcolor{blue}{0.562} \\
%  & PIAA & \textcolor{red}{0.860} & \textcolor{red}{0.860} & 0.741 \\
% * & PIAA & \textcolor{red}{0.873} & \textcolor{red}{0.871} & \textbf{0.741} \\ \hline
% \multirow{2}{*}{PIAA-ICI (Onehot-enc.)} & GIAA & 0.874 & 0.873 & \textcolor{blue}{0.578} \\
%  & PIAA & \textcolor{red}{0.828} & \textcolor{red}{0.837} & 0.729 \\
% * & PIAA & \textcolor{red}{0.823} & \textcolor{red}{0.818} & 0.728 
\end{tabular}
}
\caption{SROCC on the PARA dataset with overlapping users in both the training and test sets. The symbol * indicates models with GIAA pre-training. The zero-shot PIAA performance (\ie trained on GIAA, tested on PIAA) is shown in \textcolor{blue}{blue}, while the zero-shot GIAA performance (\ie trained on PIAA, tested on GIAA) is shown in \textcolor{red}{red}. Onehot-enc. refers to models that encode all traits using onehot encoding.}
\label{table:para_piaa}
\end{table}

\begin{table}[]
\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{ccccc}
Model & TrainSet & GIAA & PIAA \\ \hline \hline
NIMA & GIAA & 0.806 & 0.384 \\ 
NIMA-trait & GIAA & 0.808 & \textcolor{blue}{0.392} \\
PIAA-MIR (Onehot-enc.) & GIAA & 0.796 & \textcolor{blue}{0.405} \\
PIAA-ICI (Onehot-enc.) & GIAA & 0.796 & \textcolor{blue}{0.393} \\ \hline
PIAA-MIR* & PIAA & - & 0.674 \\ 
PIAA-ICI* & PIAA & - & 0.682 \\ 
NIMA-trait & PIAA & \textcolor{red}{0.787} & 0.673 \\ 
PIAA-MIR (Onehot-enc.) & PIAA & \textcolor{red}{0.783} & \textbf{0.711} \\
PIAA-MIR (Onehot-enc.)* & PIAA & \textcolor{red}{0.807} & 0.700 \\ 
PIAA-ICI (Onehot-enc.) & PIAA & \textcolor{red}{0.720} & 0.681 \\
PIAA-ICI (Onehot-enc.)* & PIAA & \textcolor{red}{0.703} & 0.683
% 3 digit version
% NIMA & GIAA & 0.806 & - & 0.384 \\ \hline
% PIAA-MIR* & PIAA & - & - & 0.674 \\ \hline
% PIAA-ICI* & PIAA & - & - & 0.682 \\ \hline
% \multirow{2}{*}{NIMA-trait} & GIAA & 0.807 & 0.808 & \textcolor{blue}{0.392} \\
%  & PIAA & \textcolor{red}{0.783} & \textcolor{red}{0.787} & 0.673 \\ \hline
% \multirow{2}{*}{PIAA-MIR (Onehot-enc.)} & GIAA & 0.798 & 0.796 & \textcolor{blue}{0.405} \\
%  & PIAA & \textcolor{red}{0.793} & \textcolor{red}{0.783} & \textbf{0.711} \\
% * & PIAA & \textcolor{red}{0.815} & \textcolor{red}{0.807} & 0.700 \\ \hline
% \multirow{2}{*}{PIAA-ICI (Onehot-enc.)} & GIAA & 0.797 & 0.796 & \textcolor{blue}{0.393} \\
%  & PIAA & \textcolor{red}{0.672} & \textcolor{red}{0.720} & 0.681 \\
% * & PIAA & \textcolor{red}{0.698} & \textcolor{red}{0.703} & 0.683
\end{tabular}
}
\caption{SROCC on the LAPIS dataset with overlapping users in both the training and test sets. The format follows that of Table~\ref{table:para_piaa}.}
\label{table:lapis_piaa}
\end{table}

Building on the generalization discussed above, the number of users involved in GIAA likely affects generalization, as fewer users shift the training domain toward individual preferences, while more users provide greater confidence in the annotated scores. Although existing works have explored GIAA, the effect of group size has rarely been addressed. To investigate this, we propose sub-sampling the GIAA dataset, referred to as sGIAA, with user groups ranging from 2 to the maximum number of users per image, as a form of data augmentation. The results of this approach are presented in Table~\ref{sgiaa:para} and Table~\ref{sgiaa:lapis}. 
These results demonstrate that \textbf{sub-sampling significantly improves zero-shot PIAA performance} by up to 20.9\% while maintaining GIAA performance. 
This demonstrates how sub-sampling helps strike a balance between individual and group preferences, further emphasizing the importance of the number of users in the GIAA dataset.

\begin{table}[]
\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{ccccc}
Model & Train Set & GIAA & PIAA \\ \hline \hline
\multirow{2}{*}{NIMA-trait} & GIAA & 0.883 & \textcolor{blue}{0.514} \\
 & sGIAA  & 0.875 & \textcolor{blue}{\textbf{0.650}} \\ \hline 
\multirow{2}{*}{PIAA-MIR (Onehot-enc.)} & GIAA & 0.870 & \textcolor{blue}{0.562} \\
& sGIAA & 0.868 & \textcolor{blue}{0.635} \\ \hline  
\multirow{2}{*}{PIAA-ICI (Onehot-enc.)} & GIAA & 0.873 & \textcolor{blue}{0.578} \\
& sGIAA & 0.870 & \textcolor{blue}{0.596} \\
% \multirow{2}{*}{NIMA-trait} & GIAA & 0.8831 & 0.8831 & \textcolor{blue}{0.5138} \\
%  & sGIAA & 0.8778 & 0.8754 & \textcolor{blue}{\textbf{0.6501}} \\ \hline 
% \multirow{2}{*}{PIAA-MIR (Onehot-enc.)} & GIAA & 0.8713 & 0.8700 &  \textcolor{blue}{0.5623 } \\
% & sGIAA & 0.8695 & 0.8675 & \textcolor{blue}{0.6352} \\ \hline  
% \multirow{2}{*}{PIAA-ICI (Onehot-enc.)} & GIAA & 0.8738 & 0.8728 & \textcolor{blue}{0.5780} \\
% & sGIAA & 0.8710 & 0.8696 & \textcolor{blue}{0.5964} \\
\end{tabular}
}
\caption{SROCC of models trained with GIAA and the subsampled setting, sGIAA on the PARA Dataset.}
\label{sgiaa:para}
\end{table}

\begin{table}[]
\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{ccccc}
Model & Train Set & GIAA & PIAA \\ \hline \hline
\multirow{2}{*}{NIMA-trait} & GIAA & 0.808 & \textcolor{blue}{0.392} \\
 & sGIAA & 0.807 & \textcolor{blue}{\textbf{0.434}} \\ \hline 
\multirow{2}{*}{PIAA-MIR (Onehot-enc.)} & GIAA & 0.796 & \textcolor{blue}{0.405} \\
 & sGIAA & 0.794 & \textcolor{blue}{0.418} \\ \hline 
\multirow{2}{*}{PIAA-ICI (Onehot-enc.)} & GIAA & 0.796 & \textcolor{blue}{0.393} \\
 & sGIAA & 0.800 & \textcolor{blue}{0.406} \\
% \multirow{2}{*}{NIMA-trait} & GIAA & 0.8066 & 0.8078 & \textcolor{blue}{0.3919} \\
%  & sGIAA & 0.8087 & 0.8065 & \textcolor{blue}{\textbf{0.4340}} \\ \hline 
% \multirow{2}{*}{PIAA-MIR (Onehot-enc.)} & GIAA & 0.7984 & 0.7964 & \textcolor{blue}{0.4047} \\
%  & sGIAA & 0.7961 & 0.7939 & \textcolor{blue}{0.4179} \\ \hline 
% \multirow{2}{*}{PIAA-ICI (Onehot-enc.)} & GIAA & 0.7969 & 0.7956 & \textcolor{blue}{0.3925} \\
%  & sGIAA & 0.8013 & 0.7999 & \textcolor{blue}{0.4062} \\
\end{tabular}
}
\caption{SROCC of models trained with GIAA and sGIAA on the LAPIS dataset.}
\label{sgiaa:lapis}
\end{table}


\subsection{Individual Subjectivity of Image Aesthetic}
This section examines the variations in score distribution across users from distinct demographic groups. We utilize the Gini index to illustrate how the distribution of aesthetic scores varies across different demographic splits. A lower Gini index indicates a more effective demographic split, enabling clearer distinctions in the distribution of aesthetic scores among user groups. 
% We perform splits based on age, educational level, and gender for both the PARA and LAPIS datasets. Additionally, we consider splits based on art experience and photography experience in the PARA dataset, and VAIAK traits in the LAPIS dataset.
The results are presented in Table~\ref{table:gini}; the Gini index for VAIAK1-7 in the LAPIS dataset is excluded due to its consistently high and nearly constant value ($0.769 \pm 0.009$), suggesting limited relevance to aesthetic variation across users\footnote{The Gini index values for VAIAK1-7 are 0.764, 0.776, 0.779, 0.764, 0.777, 0.771, and 0.750, respectively}.
%
It is evident that in both datasets, the Gini index is high for gender, suggesting that it does not significantly impact aesthetic preferences. Conversely, a lower Gini index is observed for educational level and photography experience in the PARA dataset, while in the LAPIS dataset, lower indices are noted for educational level, age, and specific VAIAK traits (particularly 2VAIAK1 and 2VAIAK4).

% \begin{table}[]
% \centering
% \resizebox{0.6\columnwidth}{!}{
% \begin{tabular}{c|c}
% Trait & Gini Index ($\downarrow$) \\ \hline \hline
% Age & 0.553 \\
% ArtExperience & 0.550 \\
% EducationalLevel & \textbf{0.461} \\
% Gender & 0.677 \\
% PhotographyExperience & 0.495 \\

% % Age & 0.5525 \\
% % ArtExperience & 0.5498 \\
% % EducationalLevel & \textbf{0.4610} \\
% % Gender & 0.6774 \\
% % PhotographyExperience & 0.4948
% \end{tabular}
% }
% \caption{Gini index on different demographic splits on PARA dataset. A low Gini index indicates a more effective demographic split for distinguishing aesthetic preferences.}
% \label{table:gini:para}
% \end{table}

% \begin{table}[]
% \centering
% \resizebox{0.55\columnwidth}{!}{
% \begin{tabular}{c|c}
% Trait & Gini Index  ($\downarrow$) \\ \hline \hline
% Age & 0.489 \\
% EducationalLevel & \textbf{0.423} \\
% Gender & 0.770 \\
% VAIAK1 & 0.764 \\
% VAIAK2 & 0.776 \\
% VAIAK3 & 0.779 \\
% VAIAK4 & 0.764 \\
% VAIAK5 & 0.777 \\
% VAIAK6 & 0.771 \\
% VAIAK7 & 0.750 \\
% 2VAIAK1 & 0.466 \\
% 2VAIAK2 & 0.617 \\
% 2VAIAK3 & 0.779 \\
% 2VAIAK4 & 0.456 \\
% % Age & 0.4889 \\
% % EducationalLevel & \textbf{0.4231} \\
% % Gender & 0.7702 \\
% % VAIAK1 & 0.7638 \\
% % VAIAK2 & 0.7764 \\
% % VAIAK3 & 0.7788 \\
% % VAIAK4 & 0.7642 \\
% % VAIAK5 & 0.7772 \\
% % VAIAK6 & 0.7708 \\
% % VAIAK7 & 0.7499 \\
% % 2VAIAK1 & 0.4663 \\
% % 2VAIAK2 & 0.6171 \\
% % 2VAIAK3 & 0.7788 \\
% % 2VAIAK4 & 0.4561
% \end{tabular}
% }
% \caption{Gini index on different demographic splits on LAPIS dataset. A low Gini index indicates a more effective demographic split for distinguishing aesthetic preferences.}
% \label{table:gini:lapis}
% \end{table}


\begin{table}[]
\centering
\caption{Gini index values for different demographic splits on the PARA and LAPIS datasets. A low Gini index indicates a more effective demographic split for distinguishing aesthetic preferences.}
\label{table:gini}

\parbox{.49\linewidth}{
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c}
Trait & Gini Index ($\downarrow$) \\ \hline \hline
Age & 0.553 \\
ArtExperience & 0.550 \\
EducationalLevel & \textbf{0.461} \\
Gender & 0.677 \\
PhotographyExperience & 0.495 \\
\end{tabular}
}
\subcaption{PARA dataset}
\label{table:gini:para}
}
\hfill
\parbox{.45\linewidth}{
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c}
Trait & Gini Index  ($\downarrow$) \\ \hline \hline
Age & 0.489 \\
EducationalLevel & \textbf{0.423} \\
Gender & 0.770 \\
% VAIAK1 & 0.764 \\
% VAIAK2 & 0.776 \\
% VAIAK3 & 0.779 \\
% VAIAK4 & 0.764 \\
% VAIAK5 & 0.777 \\
% VAIAK6 & 0.771 \\
% VAIAK7 & 0.750 \\
2VAIAK1 & 0.466 \\
2VAIAK2 & 0.617 \\
2VAIAK3 & 0.779 \\
2VAIAK4 & 0.456 \\
\end{tabular}
}
\subcaption{LAPIS dataset.}
\label{table:gini:lapis}
}
\end{table}


\subsection{Model Evaluation on Disjoint Users across Demography}

\textbf{Analysis of demographic differences.} To further assess the aesthetic differences and model generalization across the demographic split, we select users with a specific trait (e.g., users aged 18-21) as the test users, while all other users serve as the training users. We then compute the Earth Mover's Distance (EMD) between the aesthetic score distributions of the train and test groups for various demographic splits, as shown in Figure~\ref{fig:emd}. A higher EMD indicates a greater distinction in the aesthetic preferences of the test users compared to the training users.
%
We observe a similar trend to the Gini index results, where EMD values split by gender are the lowest, while splits based on art experience, photography experience, and educational level show higher EMD values, reaching up to around 0.8. Specifically, experts in both photography and art, as well as users with only high school education, demonstrate the greatest aesthetic distinction.
For the LAPIS dataset, splits based on age, educational level, 2VAIAK1, and 2VAIAK4 yield even higher EMD values, reaching up to approximately 1.2. This suggests that aesthetic preferences for artworks are more subjective compared to photographs, consistent with previous findings~\cite{vessel2018stronger}. In particular, older users, individuals with either a doctorate or primary education, and those with higher art experience exhibit the most distinct aesthetic preferences for artworks.


\input{sec/EMD_figure}


\textbf{Generalization to new users.} For model generalization, we evaluate the performance of NIMA, PIAA-MIR, and PIAA-MIR (Onehot-enc.) across different demographic splits, as shown in Section 7 of the supplementary materials. The results reveal a similar trend to the Gini index and EMD analysis, with high SROCC observed across gender splits, while lower SROCC values are noted for education level, photography experience, and art experience splits. 
%
In Figure~\ref{fig:giaa}, we plot GIAA SROCC against EMD to demonstrate the aesthetic difference caused by demographic differences is significant across all models. 
We observe a significant variation in performance caused by demographic differences, with SROCC ranging from 0.486 to 0.835 on the PARA dataset, and similarly, SROCC values ranging from 0.292 to 0.746 in the LAPIS dataset. This means the model performance can vary by as much as 41.8\% and 60.9\% on the two datasets, respectively.

Moreover, the results demonstrate a strong negative correlation. Pearson's Linear Correlation Coefficients (PLCC) are -0.980 for the PARA dataset and -0.721 for the LAPIS dataset, further indicating that greater aesthetic distinction (higher EMD) corresponds to lower model generalization (lower SROCC). This clearly indicates that averaging the scores is insufficient to eliminate individual subjectivity.
For the LAPIS dataset, outliers were identified due to imbalanced data splits caused by VAIAK groupings. For instance, users with high values in 2VAIAK4 account for only 2\% of those with low values, and a similar imbalance is seen in 2VAIAK1. Excluding these data, the PLCC improves to -0.849. This high PLCC highlights intrinsic aesthetic differences even for GIAA, suggesting that existing GIAA methods may overlook demographic differences. 
It is also worth noting that class imbalance is more pronounced in the LAPIS dataset compared to the PARA dataset. For instance, users with primary education and doctorate degrees make up only 0.9\% and 1.5\% of the total data in LAPIS, respectively. In contrast, the rarest category in PARA—users with photography expertise—accounts for 2.0\% of the data, while all other demographic groups exceed 7\%. This greater imbalance in user demographics in LAPIS contributes to more outliers in the analysis. 

A similar analysis on PIAA is conducted and illustrated in Figure~\ref{fig:piaa}. We observe a smaller variation in SROCC values for the PARA dataset, ranging from 0.448 to 0.590, while the LAPIS dataset exhibits a significantly larger variation, with SROCC values ranging from 0.111 to 0.573. Namely, the model performance can vary by as much as 24.1\% and 80.6\% on the two datasets, respectively. 
The greater variation in performance on the LAPIS dataset further underscores the higher individual subjectivity associated with artworks compared to photographs in both GIAA and PIAA models. In particular, the LAPIS dataset shows stronger performance variation for PIAA than for GIAA, indicating a greater challenge in achieving model generalization for unseen users with diverse demographic profiles. 
% Besides, the weaker correlation between SROCC values and EMD might suggest a better model generalization for unseen users.
The weaker negative correlation between PIAA SROCC values and EMD, compared to GIAA, may indicate improved generalization to unseen users.

\begin{figure}[]
    \centering
    \begin{subfigure}[b]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/trait/PARA_DisJointUsers_TestGIAASROCC.jpg}
        \caption{PARA dataset}
        \label{fig:giaa:para}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/trait/LAPIS_DisJointUsers_TestGIAASROCC.jpg}
        \caption{LAPIS dataset. The symbol 'x' denotes data split by VAIAKs, with the same color as the 'o' symbols representing the same model.}
        \label{fig:giaa:lapis}
    \end{subfigure}
    \caption{Correlation between GIAA SROCC and EMD across various demographic splits.}
    \label{fig:giaa}
\end{figure}


\begin{figure}[]
    \centering
    \begin{subfigure}[b]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/trait/PARA_DisJointUsers_TestPIAASROCC.jpg}
        \caption{PARA dataset}
        \label{fig:piaa:para}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/trait/LAPIS_DisJointUsers_TestPIAASROCC.jpg}
        \caption{LAPIS dataset}
        \label{fig:piaa:lapis}
    \end{subfigure}
    \caption{Correlation between PIAA SROCC and EMD across various demographic splits. The format follows that of Figure~\ref{fig:giaa}.}
    \label{fig:piaa}
\end{figure}

