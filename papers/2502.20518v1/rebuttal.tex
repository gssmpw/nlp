\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{16983} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{On the Role of Individual Differences in Current Approaches to Computational Image Aesthetics}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
% \noindent\textbf{Experiemntal results }
We thank reviewers for their constructive comments. 

\noindent \textcolor{blue}{\textbf{Additional state-of-the-art models. (@R1, R2 and R3)}} 
We acknowledge the IAA models suggested by R1. However, [Xu23]\footnote{CLIP Brings Better Features to Visual Aesthetics Learners} uses CLIP for knowledge distillation rather than an IAA model, and [Yun24]\footnote{Scaling Up Personalized Image Aesthetic Assessment via Task Vector}, which ensembles pre-trained GIAA models for PIAA, is unsuitable for our work as our theory requires validation without aesthetic priors to ensure fair GIAA-PIAA comparisons.
We include state-of-the-art backbones, such as Swin-tiny and ViT-small, with results summarized in Table~\ref{sgiaa:para_lapis}, extending Tables 3 and 4 of the manuscript for the PARA and LAPIS datasets. We also \textcolor{blue}{include PLCC as an extra metric, as suggested by R2}, but only for the PARA dataset to save space.
Our results highlight the significance of sGIAA. For Swin-tiny and ViT-small, the SROCC of NIMA are 0.897 [Yang22]\footnote{Personalized Image Aesthetics Assessment With Rich Attributes} and 0.871 [Xu23], respectively, while our NIMA-trait achieves comparable GIAA performance with Swin-tiny (0.893) and higher performance with ViT-small (0.894) on the PARA dataset, along with strong zero-shot PIAA performance for both SROCC and PLCC. 
Moreover, we include the results with PIAA training (only for the models with ViT-small to save space) and the results align with our transfer learning findings shown in Table 1 and Table 2 of the manuscript.
% To conserve space, we present only the results of NIMA-trait with ViT-small and \textcolor{blue}{include PLCC as an additional metric, as suggested by R2}.
While R3 noted the limited benchmarks, many PIAA models are unavailable, as also noted by R1. We believe our implemented state-of-the-art PIAA models with diverse backbones provide good benchmarks.

\vspace{-0.2cm}
\begin{table}[h]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccccccc}
\hline
Model & Back- & Train  & \multicolumn{4}{c}{PARA} & \multicolumn{2}{c}{LAPIS} \\ \cline{4-9} 
 & bone & Set & \multicolumn{2}{c}{SROCC} & \multicolumn{2}{c}{PLCC} & \multicolumn{2}{c}{SROCC} \\ \hline \hline
 &  &  & GIAA & PIAA & GIAA & PIAA & GIAA & PIAA \\ \hline \hline
NIMA-trait & Swin & G & 0.894 & \textcolor{blue}{0.529} & \underline{0.928} & \underline{\textcolor{blue}{0.579}} & 0.813 & \textcolor{blue}{0.339} \\
 & -tiny & sG & \textbf{0.893} & \textcolor{blue}{\textbf{0.660}} & \underline{0.928} & \underline{\textcolor{blue}{\textbf{0.696}}} & 0.814 & \textcolor{blue}{0.454} \\ \hline
PIAA-MIR & Swin & G & 0.884 & \textcolor{blue}{0.574} & \underline{0.922} & \underline{\textcolor{blue}{0.631}} & 0.811 & \textcolor{blue}{0.400} \\
(Onehot-enc.) & -tiny & sG & 0.881 & \textcolor{blue}{0.648} & \underline{0.920} & \underline{\textcolor{blue}{0.688}} & 0.811 & \textcolor{blue}{0.439} \\ \hline
PIAA-ICI & Swin & G & 0.886 & \textcolor{blue}{0.572} & \underline{0.923} & \underline{\textcolor{blue}{0.630}} & 0.813 & \textcolor{blue}{0.398} \\
(Onehot-enc.) & -tiny & sG & 0.887 & \textcolor{blue}{0.617} & \underline{0.925} & \underline{\textcolor{blue}{0.667}} & 0.809 & \textcolor{blue}{0.414} \\ \hline \hline
NIMA-trait & ViT & G & 0.897 & \textcolor{blue}{0.544} & \underline{0.931} & \underline{\textcolor{blue}{0.602}} & 0.830 & \textcolor{blue}{0.338} \\
 & -small & sG & \textbf{0.894} & \textcolor{blue}{\textbf{0.658}} & \underline{0.927} & \underline{\textcolor{blue}{\textbf{0.695}}} & 0.827 & \textcolor{blue}{0.450} \\ 
 &  & P & \textcolor{red}{0.848} & 0.716 & \textcolor{red}{\underline{0.884}} & \underline{0.734} & \textcolor{red}{0.624} & 0.656 \\  
 \hline
PIAA-MIR & ViT & G & 0.888 & \textcolor{blue}{0.555} & \underline{0.925} & \underline{\textcolor{blue}{0.617}} & 0.820 & \textcolor{blue}{0.384} \\
(Onehot-enc.) & -small & sG & 0.887 & \textcolor{blue}{0.658} & \underline{0.924} & \underline{\textcolor{blue}{0.695}} & 0.819 & \textcolor{blue}{0.445} \\ 
 &  & P & \textcolor{red}{0.867} & 0.743 & \textcolor{red}{\underline{0.906}} & \underline{0.771} & \textcolor{red}{0.709} & 0.688 \\ \hline
PIAA-ICI & ViT & G & 0.891 & \textcolor{blue}{0.576} & \underline{0.927} & \underline{\textcolor{blue}{0.638}} & 0.814 & \textcolor{blue}{0.387} \\
(Onehot-enc.) & -small & sG & 0.887 & \textcolor{blue}{0.619} & \underline{0.923} & \underline{\textcolor{blue}{0.667}} & 0.821 & \textcolor{blue}{0.417} \\
&  & P & \textcolor{red}{0.871} & 0.737 & \textcolor{red}{\underline{0.911}} & \underline{0.766} & \textcolor{red}{0.722} & 0.662 \\ \hline
\end{tabular}
% \begin{tabular}{ccccccccc}
% \hline
% Model & Back- & Train  & \multicolumn{3}{c}{PARA} & \multicolumn{3}{c}{LAPIS} \\ \cline{4-9} 
%  & bone & Set & GIAA & GIAA & PIAA & GIAA & GIAA & PIAA \\ \hline \hline
%  &  &  & SROCC & PLCC & SROCC & SROCC & PLCC & SROCC \\ \hline \hline
% NIMA-trait & Swin & G & 0.894 & \underline{0.928} &	\textcolor{blue}{0.529} & 0.813 & \underline{0.818}  & \textcolor{blue}{0.339} \\
%  & -tiny & sG & \textbf{0.893} & \underline{0.928} & \textcolor{blue}{\textbf{0.660}} & 0.814 & \underline{0.815} & \textcolor{blue}{0.454} \\ \hline
% PIAA-MIR & Swin & G & 0.884 & \underline{0.922} & \textcolor{blue}{0.574} & 0.811 & \underline{0.815} & \textcolor{blue}{0.400} \\
% (Onehot-enc.) & -tiny & sG & 0.881 & \underline{0.920} & \textcolor{blue}{0.648} & 0.811 & \underline{0.816} & \textcolor{blue}{0.439} \\ \hline
% PIAA-ICI & Swin & G & 0.886 & \underline{0.923} & \textcolor{blue}{0.572} & 0.813 & \underline{0.817} & \textcolor{blue}{0.398} \\
% (Onehot-enc.) & -tiny & sG & 0.887 & \underline{0.925} & \textcolor{blue}{0.617} & 0.809 & \underline{0.815} & \textcolor{blue}{0.414} \\ \hline \hline
% NIMA-trait & ViT & G & 0.897 & \underline{0.931} & \textcolor{blue}{0.544} & 0.830 & \underline{0.834} & \textcolor{blue}{0.338} \\
%  & -small & sG & \textbf{0.894} & \underline{0.927} & \textcolor{blue}{\textbf{0.658}} & 0.827 & \underline{0.831} & \textcolor{blue}{0.450} \\ 
%  &  & P & \textcolor{red}{0.848} & \textcolor{red}{\underline{0.884}} & 0.716 & \textcolor{red}{0.624} & \textcolor{red}{\underline{0.635}} & 0.656 \\  
%  \hline
% PIAA-MIR & ViT & G & 0.888 & \underline{0.925} & \textcolor{blue}{0.555} & 0.820 & \underline{0.824} & \textcolor{blue}{0.384} \\
% (Onehot-enc.) & -small & sG & 0.887 & \underline{0.924} & \textcolor{blue}{0.658} & 0.819 & \underline{0.821} & \textcolor{blue}{0.445} \\ 
%  &  & P & \textcolor{red}{0.867} & \textcolor{red}{\underline{0.906}} & 0.743 & \textcolor{red}{0.709} & \textcolor{red}{\underline{0.720}} & 0.688 \\ \hline
% PIAA-ICI & ViT & G & 0.891 & \underline{0.927} & \textcolor{blue}{0.576} & 0.814 & \underline{0.819} & \textcolor{blue}{0.387} \\
% (Onehot-enc.) & -small & sG & 0.887 & \underline{0.923} & \textcolor{blue}{0.619} & 0.821 & \underline{0.826} & \textcolor{blue}{0.417} \\
% &  & P & \textcolor{red}{0.871} & \textcolor{red}{\underline{0.911}} & 0.737 & \textcolor{red}{0.722} & \textcolor{red}{\underline{0.732}} & 0.662 \\ \hline
% \end{tabular}
}
\vspace{-0.3cm}
% \caption{\textbf{SROCC of models trained with GIAA, sGIAA, and PIAA on the PARA and LAPIS datasets.}. The value of PLCC is denoted with underline.}
\caption{\textbf{Model performance of IAA models on the PARA and LAPIS datasets.} Train set abbreviations: G = GIAA, sG = sGIAA, P = PIAA. The text colors follow the format used in Table 1 of the manuscript. The value of PLCC is denoted with underline.}
\label{sgiaa:para_lapis}
\end{table}
\vspace{-0.4cm}

% \begin{table}[h]
% \centering
% \begin{minipage}{0.46\columnwidth}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{ccccc}
% \hline
% Train Set & \multicolumn{2}{c}{SROCC} & \multicolumn{2}{c}{PLCC} \\
%  & GIAA & PIAA & GIAA & PIAA \\
% \hline \hline
% GIAA & 0.897 & \textcolor{blue}{0.544} & 0.931 & \textcolor{blue}{0.602} \\
% PIAA & \textcolor{red}{0.848} & 0.716 & \textcolor{red}{0.884} & 0.734 \\
% \hline
% \end{tabular}
% }
% \subcaption{PARA Dataset}
% \label{tl:para}
% \end{minipage}
% \hspace{0.05\linewidth} % Adds space between the subtables
% \begin{minipage}{0.46\linewidth}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{ccccc}
% \hline
% \hline
% Train Set & \multicolumn{2}{c}{SROCC} & \multicolumn{2}{c}{PLCC} \\
%  & GIAA & PIAA & GIAA & PIAA \\
% \hline \hline
% GIAA & 0.830 & \textcolor{blue}{0.338} & 0.834 & \textcolor{blue}{0.341}\\
% PIAA & \textcolor{red}{0.624} & 0.656 & \textcolor{red}{0.635} & 0.664 \\ \hline
% \end{tabular}
% }
% \subcaption{LAPIS Dataset}
% \label{tl:lapis}
% \end{minipage}
% \vspace{-0.3cm}
% \caption{SROCC of the NIMA-trait model (ViT-small) trained with GIAA and PIAA on the PARA and LAPIS datasets. The format follows Table 1 of the manuscript.}
% \label{tl:combined}
% \end{table}
% \vspace{-0.5cm} % Use a negative vspace to pull the content closer

\noindent \textcolor{blue}{\textbf{Limited applicability of our model. (@R2)}} 
In terms of PIAA, most state-of-the-art models require trait data for training, and our model is no exception. For GIAA, while our method requires trait data during training, we demonstrate that it achieves comparable performance to the GIAA baseline when using the averaged trait distribution from the training data as a fixed input, allowing the model to rely solely on image inputs during inference on test data.

\noindent \textcolor{blue}{\textbf{Missing or unavailable traits. (@R1 and R3)}}
For GIAA, missing or unavailable traits can be replaced with averaged traits, as validated by our models' performance during GIAA inference. However, this approach's effectiveness for PIAA remains unclear due to greater aesthetic variation. 
Existing work learns trait embeddings via contrastive learning [Yang23]\footnote{Multi-level transitional contrast learning for personalized image aesthetics assessment} but is infeasible for zero-shot inference on unseen users (see lines 188–195 in our manuscript).

\noindent \textcolor{blue}{\textbf{Contribution of our method (@R3)}}
While our method may look like a simple modification of the baseline's trait encoding, we emphasize its theoretical contribution to IAA. Specifically, our method enables us to explore transfer learning between PIAA and GIAA, previously treated as independent tasks, and highlight their interconnection through factors like group size, as agreed by R1\footnote{R1: 'The reported quantitative results on PARA and LAPIS are convincing, particularly in showing how subgroup sampling (sGIAA) can boost zero-shot PIAA performance.'}. 
% On user splits by demographic factors, our aim is less about proposing a specific method and more about raising awareness of this overlooked issue. 

\noindent \textcolor{blue}{\textbf{Implementation details of model and trait encoding (@R2 and R3)}}
We provide the details of our modeling and trait encoding in section 6 of the supplementary. We will also release the source code upon publication. 

\noindent \textcolor{blue}{\textbf{Model maintenance. (@R1)}}
(a) For online learning, our model may still require the data of new demographic groups due to the subjectivity of IAA. Our method can be further enhanced by integrating active learning to identify new users and validate its significance through the EMD of score distribution between new and existing users.
(b) The dynamic trait dimension is currently infeasible but can be addressed with a score correction module that takes the new trait dimension as input, similar to how PIAA models adjust scores based on GIAA predictions.

\noindent \textcolor{blue}{\textbf{Accesibility of LAPIS dataset (@R3)}}
The authors of the LAPIS dataset gave us early access to their data and confirmed that it will be publicly available soon.

\noindent \textcolor{blue}{\textbf{Structure of article (@R3)}}
% We will improve the writing and would appreciate guidance on specific areas needing improvement.
We will refine the writing and welcome guidance on specific areas for improvement.

\noindent \textcolor{blue}{\textbf{Gini index (@R3)}}
% The Gini index is computed from score distributions by splitting users based on demographic factors, resulting in n score distributions per image, where n is the number of demographic categories, \eg n is 5 for the gini index over age\footnote{age of 18--21, 22--25, 26--29, 30--34, 35--40}. Then the averaged gini index over images is reported.
The Gini index is computed from score distributions by splitting users based on demographic factors, resulting in \(n\) distributions per image (\eg, \(n = 5\) for age\footnote{Categories: 18--21, 22--25, 26--29, 30--34, 35--40}). We report the average value across images. 

\noindent \textcolor{blue}{\textbf{Limitation of onehot encoding (@R2)}}
% The main limitation of one-hot encoding is that it mixes personal traits into the distribution, causing ambiguity. For example, [a 60-year-old female + a 30-year-old male] is treated the same as [a 60-year-old male + a 30-year-old female]. However, we observed no performance degradation in our method compared to state-of-the-art GIAA/PIAA models.
The main limitation of one-hot encoding is mixing traits, \eg swapping the gender of users within groups keeps the same distribution. Despite this, our method shows no performance drop compared to state-of-the-art IAA models.


%%%%%%%%% REFERENCES
% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }

\end{document}
