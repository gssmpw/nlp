\documentclass[10pt, conference]{IEEEtran}
\addtolength{\topmargin}{+0.5cm}
\setlength{\columnsep}{0.21 in}
\usepackage[utf8]{inputenc}
\usepackage[detect-all]{siunitx}
\usepackage{array,tabularx,calc}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{pbox}
\usepackage{url}
\usepackage[]{algorithm, algpseudocode}
\usepackage{tikz}

\DeclareSIUnit{\belmilliwatt}{Bm}
\DeclareSIUnit{\dBm}{\deci\belmilliwatt}
\DeclareSIUnit{\belisotropic}{Bi}
\DeclareSIUnit{\dBm}{\deci\belisotropic}
\DeclareSIUnit{\bit}{bit}

\setlength\abovedisplayskip{0pt}
\setlength\belowdisplayskip{0pt}

\ifCLASSOPTIONcompsoc	\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else					\usepackage[caption=false,font=footnotesize]{subfig}
\fi

\makeatletter
% ALGORITHM
\usepackage[]{algorithm, algpseudocode}
\makeatletter
\renewcommand{\ALG@beginalgorithmic}{\small}
\newcommand{\norm}[1]{\lVert#1\rVert_2}

% REFERENCES
\usepackage[capitalise]{cleveref}	% \cref{}
    \crefformat{equation}{(#2#1#3)}	% Redefine Equation labels to be typeset as "(1)", instead of "Eq. (1)".

\usepackage[noadjust]{cite}

\newcommand{\startcompact}[1]{\par\vspace{-0.75em}\begin{#1}%
\allowdisplaybreaks\ignorespaces}

\newcommand{\stopcompact}[1]{\end{#1}\ignorespaces}

\setlength{\textfloatsep}{1\baselineskip plus 0.2\baselineskip minus 0.5\baselineskip}

\begin{document}

\usetikzlibrary{arrows}
\usetikzlibrary{shapes}
\newcommand{\mymk}[1]{%
	\tikz[baseline=(char.base)]\node[anchor=south west, draw,rectangle, rounded corners, inner sep=0.1pt, minimum size=3.5mm,
	text height=2mm](char){\ensuremath{#1}} ;}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
	\node[shape=circle,draw,inner sep=0.1pt] (char) {#1};}}

\title{ A Framework to Develop and Validate RL-Based Obstacle-Aware UAV Positioning Algorithms}

\author{\IEEEauthorblockN{Kamran Shafafi, Manuel Ricardo, Rui Campos }
	\IEEEauthorblockA{INESC TEC and Faculdade de Engenharia, Universidade do Porto, Porto, Portugal\\
		\{kamran.shafafi, manuel.ricardo, rui.l.campos\}@inesctec.pt}}


\maketitle

\begin{abstract}

Unmanned Aerial Vehicles (UAVs) are increasingly being utilized to enhance the Quality of Service (QoS) in wireless networks due to their flexibility and cost-effectiveness. However, optimizing UAV placement in dynamic and obstacle-prone environments remains a research challenge. Reinforcement Learning (RL) has proven to be an effective approach that offers adaptability and robustness in such environments. 

This paper introduces RLpos-3, a novel framework that integrates standard RL techniques and existing libraries with Network Simulator 3 (ns-3) to facilitate the development and evaluation of UAV positioning algorithms. RLpos-3 serves as a supplementary tool for researchers, enabling the implementation, analysis, and benchmarking of UAV positioning strategies in different environmental settings while ensuring that user traffic demands are met. To validate its effectiveness, we present various use cases that demonstrate the performance of RLpos-3 in optimizing UAV placement under realistic conditions.

\end{abstract}


\begin{IEEEkeywords}
	Unmanned Aerial Vehicles,  Aerial Networks,	Reinforcement Learning, Positioning Algorithms, LoS Communications,
	Obstacle-aware Communication, Positioning Frameworks.
\end{IEEEkeywords}


\section{Introduction}

Unmanned Aerial Vehicles (UAVs) have special characteristics because of their mobility, low cost, and ability to hover anywhere and at any time. Compared to terrestrial infrastructure, UAV-based networks offer greater agility and ease of configuration. This is particularly beneficial in disaster management scenarios where conventional infrastructure may fail, such as during wildfires, earthquakes, floods, cyberattacks, and terrorist attacks \cite{shafafi2023uav}. Moreover, during festivals and crowded events, UAVs can enhance network capacity or temporarily replace traditional infrastructures as needed \cite{shafafi2023joint, jx7c-py87-25}. In both commercial and civilian domains, numerous new applications have emerged. These include weather monitoring, forest fire detection, traffic control, cargo transport, emergency search and rescue, and communications relay. Consequently, UAVs have gained significant traction in recent years for deploying mobile Base Stations (BSs) and Wi-Fi Access Points (APs) \cite{rs11121443}. In today's digital society, improving wireless network coverage is essential for everywhere Internet access, which allows for various online services and applications like augmented reality, online games, ultra-high definition videos, disaster safety, and event facilitation \cite{8875210}.

There are numerous challenges in the deployment of UAVs, encompassing environmental conditions and limitations such as distance, attenuation, obstacle effects, meeting user traffic demands, transmission power, and energy efficiency. Properly deploying UAVs at precise coordinates in real-world scenarios requires a flexible positioning system that can adapt to different conditions. Although various studies have investigated UAV positioning, most solutions often introduce algorithms designed to address specific cases. They are mainly focused on solutions such as optimization techniques, heuristic algorithms, and, more recently, Machine Learning (ML) approaches. In complex situations, such as obstacle-rich scenarios, relying on optimization and heuristic methods may be difficult and insufficient to address the problem. In this case, a fundamental gap remains due to the absence of a generic framework capable of handling challenges across multiple environments. Reinforcement Learning (RL) has emerged as an effective way to address such issues, providing adaptability and robustness in dynamic, obstacle-rich contexts. Although prior work optimizes UAV positioning for obstacle-rich urban environments using a customized DQN algorithm, it lacks flexibility for other contexts. RLpos-3 addresses this by providing a modular framework that adapts to both obstacle-rich and free-space scenarios, with user-configurable environments, algorithms, and performance metrics.

The main contribution of this paper is RLpos-3, a generic and modular simulation framework designed to implement, validate, and evaluate RL-based UAV positioning algorithms in aerial networks. RLpos-3 integrates standard RL libraries, such as TensorFlow Agents \cite{TensorFl16:online} and OpenAI Gym \cite{GitHubop15:online}, with Network Simulator 3 (ns-3.44) \cite{ns3adisc37:online}. RLpos-3 customizes the basic libraries to align with the functions of the UAV positioning algorithm in different scenarios. The ns3-gym interface \cite{GitHubtk28:online} serves as a bridge between the RL libraries and ns-3. This framework leverages the capabilities of RL to address complex scenarios, resulting in improved positioning accuracy, reduced delays, and improved network capacity by establishing line-of-sight (LoS) connections. Unlike algorithm-specific solutions like RLTOPA \cite{shafafi2024traffic}, which targets obstacle-rich urban settings with a fixed DQN approach, RLpos-3 supports diverse environments (obstacle-rich or free-space) and configurable RL algorithms (e.g., DQN, PPO) and is capable of functioning in various environmental settings and configurations, including urban scenarios. A key feature of RLpos-3 is its traffic awareness, which uses RL to determine UAV positions that ensure user traffic demands are met, followed by an evaluation of the achieved positions. 

The rest of this paper is organized as follows. Section \ref{sec:soa} provides an overview of state-of-the-art positioning approaches in flying networks. Section \ref{sec:RLpos-3 Framework} provides a detailed description and structure of RLpos-3. Section \ref{sec:performance_evaluation} delves into the performance evaluation of RLpos-3 and provides a sample simulation setup, performance metrics, and results. Section \ref{sec:Conclusions} summarizes the main conclusions and suggests directions for future research.




\section{State of the Art~\label{sec:soa}}

Much of the existing literature emphasizes the use of optimization and mathematical solutions to tackle the positioning problem. However, many of these studies do not provide comprehensive coverage of deployment challenges across various scenarios; instead, they often focus on specific contexts. Numerous works incorporate unrealistic assumptions, such as conducting experiments in obstacle-free scenarios. In this section, we review the literature concerning positioning algorithms and the tools employed to address them.

In \cite{8761897}, the authors introduce an optimization formulation aimed at enhancing up-link throughput and optimizing resource allocation within an obstacle-free environment. Reference \cite{8422376} presents a solution focused on optimizing the backhaul network while maximizing the coverage for ground users. The authors of \cite{9684492} put forth a UAV-aided ground positioning method utilizing a nonparametric Belief Propagation (NBP)-based probabilistic framework to address UAV localization. This paper offers a solution for positioning UAVs to ensure optimal coverage with minimum interference. The same environment is considered in \cite{8369021}, to optimize the number of UAVs used for covering the area. For a specific scenario that involves providing wireless connectivity to the User Equipments (UEs) inside a building by a single UAV, the authors of \cite{7921981} propose an optimization solution. In \cite{7510820}, the authors address a 3-D placement problem for UAVs to optimize network revenue, which is directly linked to the number of drone cell users. They introduce a bisection search algorithm that simultaneously determines the coverage area and drone altitude.

In recent years, researchers have introduced positioning algorithms leveraging ML and RL techniques in specific scenarios \cite{ML8121867, MLJiang2017MachineLP}. While these studies focus on individual solutions tailored to particular scenarios, they have not presented a generic template for broader community use in positioning strategies. For instance, \cite{43-bab6dcd317da4e3d82375669cbb45023} utilized deep learning to maximize throughput. In contrast, the authors of \cite{23-8644345} outlined a three-step approach for the 3D positioning and dynamic mobility of multi-UAVs. Furthermore, enhancing Quality of Service (QoS) and Quality of Experience (QoE) through Q-learning techniques stands out as the primary contribution of \cite{22-COLONNESE2019101872}.

In summary, while related works offer solutions tailored to their specific challenges, a universal algorithm adaptable across various scenarios is lacking. Even though optimization formulations are effective for particular contexts, ensuring compatibility across diverse scenarios poses challenges. Utilizing Artificial Intelligence (AI), such as RL, appears to be a more logical and reasonable approach.

In summary, while related works offer solutions tailored to their specific challenges, a universal algorithm adaptable across various scenarios is lacking. Even though optimization formulations are effective for particular contexts, ensuring compatibility across diverse scenarios poses challenges. Utilizing Artificial Intelligence (AI), such as RL, appears to be a more logical and reasonable approach. RLpos-3 integrates ns-3.44 with OpenAI Gym and TensorFlow Agents, offering a modular architecture. Its Environment module supports obstacle-rich (via BuildingModule) or free-space setups, configurable via parameters like venue size ($S_{\text{venue}}$) and obstacle density. The Agent module allows swapping RL algorithms (DQN, PPO, SAC) and reward functions, unlike RLTOPA \cite{shafafi2024traffic} fixed $nLoS_{\text{norm}}$. This modularity enables testing across diverse scenarios. Table~\ref{tab:comparison} contrasts RLpos-3 with RLTOPA, highlighting its generic, modular design.

\begin{table}[h]
\centering
\caption{Comparison of RLTOPA and RLpos-3}
\label{tab:comparison}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature} & \textbf{RLTOPA} & \textbf{RLpos-3} \\ \hline
Purpose & Specific Algorithm & Generic Framework \\ \hline
Environment & Obstacle-Rich Only & Obstacle-Rich/Free-Space \\ \hline
RL Algorithm & DQN Only & DQN, PPO, SAC, etc. \\ \hline
Configurability & Fixed & Modular (Env, Agent) \\ \hline
ns-3 Version & v3.38 & v3.44 \\ \hline
\end{tabular}
\end{table}



% \section{Modules and Structure ~\label{sec:smodules and structure}}
\begin{figure}
	\centering
		\includegraphics[width=\linewidth]{fig1.pdf}
	\caption{RLpos-3 main modules and interactions between them.}
	\label{fig1}
\end{figure}
  

\section{RLpos-3 Framework~\label{sec:RLpos-3 Framework}}



The RLpos-3 framework consists of four main modules: $Input/output$, $Environment$, $Agent$, and $gym$. Figure \ref{fig1} illustrates these modules and their interfaces. The \emph{Input/output} module is a JSON-based script developed to interface with users, manage configuration parameters, and facilitate communication with other modules. The \emph{Environment} contains the \( State \) (\( s \)) of the standard RL process. At each timeslot, \( t_s \), the \emph{Agent}, via the \emph{gym} interface, receives a state, \( s \), from the \emph{Environment} and executes an action, \( a \), based on the received state. Upon executing action \( a \), the \emph{Environment} provides the \emph{Agent} with a reward, \( r \), which \emph{gym} relays to the \emph{Agent} as immediate feedback on the action's outcome. Using the reward from the \emph{Environment}, the \emph{Agent} determines its next action for the subsequent \( t_s \). The essential hyperparameters for standard RL formulation—such as the number of training episodes, evaluation episodes, batch size, learning rate, epsilon-greedy value, and buffer size—are defined through the \emph{Input/output} module. 

Figure \ref{fig2} illustrates the architecture and detailed submodules developed within each module. \emph{Environment} module is developed using ns-3 and it extends the ns-3 modules to simulate a wireless network environment with UAVs and ground users. While ns-3 provides the core network simulation capabilities and protocols, the \emph{Environment} customizes the simulation for UAV-based network scenarios with wireless configurations, mobility patterns, and building scenarios. The \emph{Environment} configuration is managed through three main modules: $UtilsEnv$, which handles essential utility functions for configuration management, Wi-Fi channel setup, and command-line interface operations; $ConfigEnv$, which serves as the core configuration module for all network-related parameters, including Wi-Fi settings, mobility, and application deployment, allowing users to choose between UDP and TCP protocols via the \emph{Input/output} module while also managing building implementation and node positioning; and $LogsEnv$, which provides comprehensive logging functionality across all components with configurable debug levels based on \emph{Input/output} settings. Four log levels are defined: Error, Warning, Info, and Debug—designed to facilitate error detection.
and correction.

\begin{figure}
	\centering
		\includegraphics[width=0.8\linewidth]{fig2.pdf}
	\caption{RLpos-3 architecture block diagram.}
	\label{fig2}
\end{figure}

Due to the constraints of ns-3, this framework only considers buildings as obstacles. The system can incorporate buildings of different heights, number of floors, and room sizes. Users have the flexibility to define the coordinates of buildings and the dimensions of the venue using the \emph{Input/output} module. RLpos-3 is engineered to support a Wi-Fi medium and offers flexibility in selecting from various Wi-Fi generations. Additionally, support for different remote station manager mechanisms is guaranteed. The medium can operate in either STA-AP mode or Ad Hoc mode, with the selection made within the \emph{Input/output} module.  

The propagation loss model is defined based on the environment characterization. Users can specify their desired loss model within the \emph{Input/output} module, and the framework will adjust accordingly. The \(FriisPropagationLossModel\) is suitable for scenarios without obstacles. However, in environments with obstacles and Non Line of Sight (NLoS) connections, options such as \(HybridBuildingsPropagationLossModel\), \(ItuR1411LosPropagationLossModel\), and \(ItuR1411Nlos-OverRooftopPropagationLossModel\) are available.
 
The \emph{gym} module is developed on top of ns3-gym, extending the latter to create a specialized environment for UAV positioning optimization. While ns3-gym provides the foundational bridge between OpenAI Gym and ns-3, \emph{gym} customizes the environment interface for the specific requirements of UAV-based wireless network optimization. The main customizations are: 


\begin{itemize}
    \itemsep0em
	\item \textbf{MyGymEnv:} It is a customized \emph{gym} environment that inherits from OpenGymEnv, the standard \emph{gym} environment interface. This environment extends 
         the base functionality by implementing specific methods and parameters.
	\item \textbf{ScheduleNextStateRead:} Schedules the timestep \( t_s \). \( t_s \) is the time considered by the agent to execute new actions and collect state.
    \item \textbf{GetActionSpace:} An OpenAI Gym one-dimensional discrete space is employed to outline potential movement directions for the agent, as configured in the \emph{Input/output} module. The default directions for UAVs include up, down, forward, backward, left, right, and staying in the same position. The dimensions and types of actions are flexible and can be configured based on the target scenario.
    \item \textbf{GetObservationSpace:} An OpenAI Gym box space is configured to encompass parameters observable from the state, as defined in the \emph{Input/output} module. The default parameters include the position of the UAV, throughput, and the number of LoS connections. The configuration allows for the definition of any type and number of observations.
    \item \textbf{ExecuteActions:} The agent executes actions during each time step. Here, the limitation of the potential positioning zone can be adjusted. By default, it aligns with the scale of the venue. However, in scenarios with buildings, their height must be considered.
    \item \textbf{ReceivePacketRX:} Monitors the reception of packets.
    \item \textbf{ThroughputMonitor:} Computes the throughput.
    \item \textbf{DelayMonitor:} Observes the mean delay of individual packets.
    \item \textbf{TrafficDemandMonitor:} Assesses whether the potential UAV position proposed by the agent can meet the traffic demands of all UEs. The mechanism applies a Modulation Code Scheme (MCS) for each UE based on demanded traffic to enforce a minimum Signal-to-Noise Ratio (SNR). 
    \item \textbf{GetGameOver:} Checks for completion of training and evaluation.
\end{itemize}
 

The \emph{Agent} module is developed using the TF-Agents framework, extending its DQN (Deep Q-Network) implementation to interact with the ns3-gym environment. While TF-Agents provide the core RL algorithms and neural network architectures, this module includes customizations for UAV positioning optimization with specific training and evaluation procedures. 


The framework is executed using a main Python script designated as \(main.py\). Users of RLpos-3 have the option to select either the training or the evaluation mode. The main parameters of the \emph{Input/output} module can be specified directly via the command-line interface.


\section{Applying RLpos-3: Example-Based Testing in Action~\label{sec:performance_evaluation}}

This section presents a usage example of RLpos-3 for training and evaluating a UAV positioning algorithm across diverse scenarios. The objective is to optimally position the UAV to ensure LoS connectivity with all UEs while maximizing throughput and accommodating each UE's traffic demand. RLpos-3 is used to train and validate the algorithm, aiming to enable reliable broadband communication at higher frequencies. The achieved solution is then evaluated in terms of QoS, particularly throughput and delay, providing valuable insights into the performance, functional validation, and adaptability of RLpos-3 under different environmental and traffic demand conditions. The findings are also generalizable to more complex scenarios involving varying numbers of users and obstacles, as demonstrated in \cite{shafafi2024traffic}.

\begin{figure}
	\centering
		\includegraphics[width=0.8\linewidth]{fig3.pdf}
	\caption{Evaluation scenario, which involves four UEs positioned in non-LoS locations with a UAV. ($x_o, y_o, z_o$) is the optimal position, achieved by RLpos-3 for the UAV to establish LoS connections with all four UEs.}
   \label{fig3}
\end{figure}


\subsection{System Settings~\label{System Settings}}

To evaluate the performance of RLpos-3, we considered three different scenarios: A) a free space scenario where 20 UEs are distributed within a 100m x 100m venue with constant mobility and each demanding a traffic rate of $\lambda = 58.5$ Mbit/s associated with the MCS index 0; B) an obstacle-rich homogeneous scenario includes five buildings of varying heights and four UEs with constant mobility in the same size venue where $\lambda_0 =  \lambda_1 =  \lambda_2 =   \lambda_3$, associated with the MCS index 0, with traffic demand of 58.5 Mbit/s; C) an obstacle-rich heterogeneous scenario includes five buildings of varying heights and four UEs with constant mobility in the same size venue where  $\lambda_0 = 0.75 \times \lambda_1 = 2 \times \lambda_2 = 4 \times \lambda_3$, with traffic demand of 234, 175.5, 117, and 58.5 Mbit/s associated with the MCS index 3, 2, 1, and 0, respectively, as depicted in Figure \ref{fig3}. Consider that the UEs are placed at the coordinates \((x_u, y_u, z_u)\), where \(u \in \{0,...,3\}\). The UAV is equipped with a Network Interface Card (NIC) operating in ad hoc mode, utilizing the IEEE 802.11ac standard on channel 50, with a channel bandwidth of 160 MHz and a Guard Interval (GI) of 800 ns. A single spatial stream is used for all links between the UEs and the UAV. Each UE generates UDP traffic using the $OnOffApplicationModule$, directed to the UAV, which has a UDP sink receiver installed. Each UE is assigned a traffic demand \(\lambda_u\), where \(u \in \{0,...,3\}\), corresponding to a specific MCS index \cite{MCSTable16:online}. 
\begin{table}[ht]
    \caption{\textbf{Detail of RL parameters.}}
    \label{tab2}
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{p{100pt} p{130pt}}
        \hline
        Parameter & Amount \\  
        \hline    
        Number of training episodes & 10\\
        $w_1$ & 0.8\\
        $w_2$ & 0.2\\
        Number of evaluation episodes & 1\\        
        Duration of episodes& 100 s\\
        Decision interval, $t_k$ & 100 ms\\       
        Observations & ($x, y, z$), nLoS \\
        Action Space & one-dimensional discrete scaled integer \\
        ML library & TensorFlow\\
        Optimizer & Adam (learning rate of $10^{-2}$) \\
        Epsilon Greedy & 1 (random decision) \\ 
        Quadratic Loss & Mean Square Error (MSE) \\
        Q-function& Two fully connected layers, each with 32 units\\
        Memory Replay & buffer size is $10^6$ with a batch of 64\\
        % $\alpha, \beta$ & 0.8, 0.2\\
       \hline      
    \end{tabular}
\end{table}

\begin{table}[ht]
    \caption{\textbf{ ns-3 environment configuration }}
    \label{tab1}
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{p{120pt} p{100pt}}
        \hline
        Parameter & Amount  \\  % Use \multicolumn to center the header
        \hline
        Size of venue ($W\times D)$  & 100m  \\        
        Guard Interval $(GI)$ & 800 $ns$\\
        Wi-Fi channel & 50 \\
        Wi-Fi Standard & IEEE 802.11ac  \\
        Channel Bandwidth & 160 MHz  \\
        Antenna Gain & 0dBi  \\
        Tx Power& 20 $dBm$\\
        Noise Floor Power & -85 $dBm$\\
        LoS Propagation Loss Model & ItuR1411LosPropagationLossModel   \\
        NLoS Propagation Loss Model & ItuR1411NlosOverRooftopP ropagationLossModel  \\
        Remote Station Manager mechanism & IdealWifiManager  \\
        Application Traffic & UDP constant bitrate \\        
        Packet Size & 1400 bytes  \\
        \hline
    \end{tabular}
\end{table}   

In Scenario A, the initial position of the UAV \((x_i, y_i, z_i)\), which serves as the baseline, is a 3D coordinate at the center of the venue with an altitude of 10m. In Scenarios B and C, the initial UAV position and baseline are set at 5m above the central building, where a fixed base station is mounted. The configuration of the environment is summarized in Table \ref{tab1}. RLpos-3 users can define these parameters using the \emph{Input/output} module through the $JSON$ file or the command-line interface. A potential zone for UAV placement—referred to as the action space—has been defined to cover all areas above the venue. This zone can be configured in the $JSON$ file based on the requirements of the specific scenario. Furthermore, the observation space is defined by the UAV's position \((x, y, z)\) and the number of LoS connections between the UAV and UEs \(nLoS\) at each time step. The reward function is formulated in Equation \ref{reward} to maximize $nLoS$ and throughput with different weights. Additionally, RLpos-3 allows users to customize the reward function by setting up the \emph{gym} module. The main configuration parameters are detailed in Table \ref{tab2}.  


\begin{equation}\label{reward}
    \begin{aligned}
        & r = w_1 \cdot nLoS_{\text{norm}} + w_2 \cdot Throughput_{\text{norm}} & \\
        & \text{where} \quad nLoS_{\text{norm}} = \frac{nLoS}{N} & \\
        &  \quad\quad\quad Throughput_{\text{norm}} = \frac{Throughput}{\sum_{i=0}^{N} \lambda_i} &
    \end{aligned}
\end{equation}

where $N$ is the number of UEs.


\subsection{Simulation Results~\label{sec:simulation_Results}}

The optimal position achieved by RLpos-3 is evaluated in this section using ns-3. The results are derived from 30 simulations, while the algorithm is trained over 10 episodes, each lasting 100 seconds. All simulations were conducted using the \(SetRandomSeed()\) function and \(RngRun = \{1, 2, \dots, 30\}\) parameters under constant networking conditions. The aggregate throughput achieved by the UAV is illustrated using the complementary cumulative distribution function (CCDF) and the mean delay is depicted using the cumulative distribution function (CDF).
\begin{figure}
	\centering
		\includegraphics[width=9cm, height=4cm]{Fig3.png}
	\caption{Scenario A: Free-space aggregate throughput and mean delay measured on UAV, where $\lambda_0 =  \lambda_1 =  . . .  =  \lambda_{19}$. .}
   \label{fig3.1}
\end{figure}
\begin{figure}
	\centering
           \includegraphics[width=8cm, height=4cm]{fig4.png}
	\caption{Scenario B: Aggregate throughput measured on UAV, where $\lambda_0 =  \lambda_1 =  \lambda_2 =  \lambda_3$.}
	\label{fig4}
\end{figure}
\begin{figure}
	\centering
		\includegraphics[width=8cm, height=4cm]{fig5.png}
	\caption{Mean delay measured on UAV, where $\lambda_0 = \lambda_1 = \lambda_2 = \lambda_3$.}
	\label{fig5}
\end{figure}

Figure \ref{fig3.1} shows the evaluation results for Scenario A, where the aggregate throughput and delay at the optimal position achieved by RLpos-3 are compared with the baseline. The results indicate a 60\% improvement in throughput and a 40\% reduction in mean delay. For both Scenarios A and B, five additional positions—each located 10 meters away from the optimal position in different directions, denoted as positions 1–5—are evaluated against the optimal position determined by RLpos-3 and the baseline. Figures \cref{fig4} and \cref{fig5} illustrate the results of Scenario B, showing an 80\% increase in throughput and a 60\% reduction in delay. In heterogeneous Scenario C, the framework achieved a 60\% improvement in aggregate throughput and a 35\% reduction in median delay compared to other positioning methods as depicted in Figures \ref{fig6} and \ref{fig7}.

It is worth noting that, due to the multi-objective reward function, in Scenarios B and C, even though Position 4 maintains LoS with all UEs, the optimal position identified by RLpos-3 still achieves higher throughput and lower delay.

\section{Conclusions~\label{sec:Conclusions}}

RLpos-3 is a simulation framework designed to implement, validate, and evaluate adaptive, obstacle-aware, RL-based UAV positioning algorithms. It integrates existing reinforcement learning libraries with ns-3 by leveraging ns-3gym as a bridging interface. RLpos-3 supports standard RL libraries such as TensorFlow Agents and OpenAI Gym, customizing them to align with the specific requirements of UAV positioning algorithms across various target scenarios. By simply configuring the \emph{Input/Output} module, users can develop and evaluate positioning strategies tailored to diverse environments. The framework aims to optimize UAV placement to enhance network performance—most notably by increasing aggregate throughput and reducing delay. \looseness=-1

The results from a simple use case illustrate RLpos-3's potential as a valuable tool for advancing RL-based UAV positioning solutions. Future work will focus on integrating signal processing techniques \cite{5381162} and computer vision methods to detect the positions of ground users and obstacles. Furthermore, the framework will be extended to support multi-UAV scenarios.


% \section*{Acknowledgments}
% {This work is co-financed by Component 5 - Capitalization and Business Innovation, integrated in the Resilience Dimension of the Recovery and Resilience Plan within the scope of the Recovery and Resilience Mechanism (MRR) of the European Union (EU), framed in the Next Generation EU, for the period 2021 - 2026, within project Produtech\_R3, with reference 60.}

\begin{figure}
	\centering
		\includegraphics[width=8cm, height=4cm]{fig6.png}
	\caption{Aggregate throughput measured on UAV, where $\lambda_0 = 0.75 \times \lambda_1 = 2 \times \lambda_2 = 4 \times \lambda_3$.}
	\label{fig6}
\end{figure}
\vspace{0 cm}
\begin{figure}
	\centering
		\includegraphics[width=8cm, height=4cm]{fig7.png}
	\caption{Mean delay measured on UAV, where $\lambda_0 = 0.75 \times \lambda_1 = 2 \times \lambda_2 = 4 \times \lambda_3$.}
	\label{fig7}
\end{figure}


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,References}

\end{document}
