



\begin{figure*}[t]
\centering
\includegraphics[width=16cm]{Fig/framework.pdf}
\caption{
The overall framework of \name.
First, we generate the initial token sequences from both the attribute view and topology view. Then, we utilize the proposed token swapping operation to generate new token sequences for each target node. These generated token sequences are then fed into a Transformer-based backbone to learn node representations and generate predicted labels. Additionally, a center alignment loss is adopted to further constrain the representations extracted from different token sequences.
}
\label{fig:fw}
\end{figure*}



% original token sequence

In this section, we detail our proposed \name.
Specifically, \name conducts the token sampling operation based on different feature spaces to generate the initial token sequences.
Then, \name develops a novel token swapping operation that produces diverse token sequences based on the initial token sequences.
Finally, \name introduces a Transformer-based backbone with a center alignment loss to learn node representations from generated token sequences.
The overall framework of \name is shown in Figure \ref{fig:fw}.




%initial token generating
\subsection{Token Sampling}
Token sampling, which aims to select relevant nodes from input graph to construct a token sequence for each target node, is a crucial operation in recent GTs~\cite{ansgt,vcrgt}.
Generally speaking, the token sampling operation for the target node $v_i$ can be summarized as follows:
\begin{equation}
    N_i = \{v_j | v_j \in \mathrm{Top}(\mathbf{S}_i, k)\},
    \label{eq:token_sampling}
\end{equation}
where $N_i$ denotes the token set of node $v_i$.
$\mathrm{Top}(\cdot)$ denotes the top-$k$ sampling operation and $k$ is the number of sampled tokens. 
$\mathbf{S}_i \in \mathbb{R}^{1\times n}$ denotes the vector of similarity scores between $v_i$ and other nodes.
Obviously, different strategies for measuring node similarity will result in different sets of sampled tokens. 

According to empirical results in previous studies~\cite{ansgt,vcrgt}, measuring the node similarity from different feature spaces to sample different types of tokens can effectively enhance the model performance.
Hence, in this paper, we sample the tokens from both the attribute feature space and the topology feature space, resulting in two token sets $N^{A}_i$ and $N^{T}_i$, respectively.

Specifically, for $N^{A}_i$, we utilize the raw attribute features to calculate the cosine similarity of each node pair to obtain the similarity scores.
For $N^{T}_i$, we adopt the neighborhood features to represent the topology features of nodes.
The neighborhood feature matrix is calculated as $\mathbf{X}^{\prime}= \phi(\hat{\mathbf{A}}, \mathbf{X}, K)$ where $K$ denotes the number of propagation steps and $\phi(\cdot)$ denotes the personalized PageRank propagation~\cite{appnp}.
Then we leverage the neighborhood features to measure the similarity of nodes in the topology feature space via the cosine similarity.




% token swapping
\subsection{Token Swapping}\label{sec:swapping}
As discussed in \autoref{fig:motivation}, existing node token generators~\cite{gophormer,ansgt,vcrgt} could be regarded as selecting the 1-hop neighborhood nodes in the constructed $k$-NN graph, which is inefficient in fully leveraging the semantic relevance between nodes and restricts the diversity of the token sequences, further limiting the model performance. 
% A naive solution is to adopt a large value of $k$ to sample more nodes into the token sequences.
% However, this strategy is more likely to introduce irrelevant nodes, which can further degrade the model performance.

To effectively obtain diverse token sequences, \name introduces a novel operation based on the unique characteristic of token sequences, called token swapping.
The key idea is to swap tokens in different sequences to construct new token sequences.
Specifically, for each node token $v_j$ in the token set of node $v_i$, we generate a new node token $v^{new}= \zeta(N_j)$ based on the token set $N_j$ of $v_j$, where $\zeta(\cdot)$ denotes the random sampling operation.
Then, the new token set $N^{\prime}_i$ of $v_i$ is generated as follows:
\begin{equation}
    N^{\prime}_i=\{\zeta(N_j)|v_j\in N_i\}.
    \label{eq:condidate_token}
\end{equation}

\autoref{eq:condidate_token} indicates that for each node token $v_j$, we swap it with a random node $v^{new}$ in its node token set $N_j$ to construct the new token set $N^{\prime}_i$ for the target node $v_i$.
\autoref{fig:swap} provides a toy example to illustrate the token swapping operation.


\begin{figure}[t]
\centering
\includegraphics[width=7.5cm]{Fig/swap.pdf}
\caption{
Illustration of the token swapping, where node 1 is the target node. We first select node 3 and regard the tokens in its token sequences as the candidates.
Then we select node 6 from the candidates to swap node 3, and construct the new token sequence.
}
\label{fig:swap}
\end{figure}


Here, we provide deep insights for the token swapping operation.
According to \autoref{fig:motivation}, nodes in the token set are the 1-hop neighbors of the target node in the $k$-NN graph.
Therefore, the selected node $v^{new}$ is within the 2-hop neighborhoods of $v_i$.
Therefore, performing the swapping operation $t$ times is equal to enlarge the sampling space from 1-hop neighborhood to $(t+1)$-hop neighborhood.
Hence, the swapping operation effectively utilize the semantic relevance between nodes to generate diverse token sequences.
The overall algorithm of token swapping is summarized in Algorithm \ref{alg:tokenswap}.

\begin{algorithm}[tb]
   \caption{The Token Swapping Algorithm}
   \label{alg:tokenswap}
\begin{algorithmic}[1]
   \REQUIRE Sampled token set of all nodes $N\in \mathbb{R}^{n\times k}$; Target node $v_i$; Probability $p$; Swapping times $t$
   \ENSURE The new token set $N^{\prime}_i\in \mathbb{R}^{1\times k}$ of $v_i$

    \STATE Initialize $N^{\prime}_i=N_i$;
    \FOR{$t_0=1$ {\bfseries to} $t$}
    \STATE Initialize $N^{new}=\{\}$;
   \FOR{$v_j \in N^{\prime}_i$ }
   \IF{$random(0,1)>p$}
   \STATE $N^{new}=N^{new}\cup\{v_j\}$;
    \ELSE
    \STATE $v^{new}= \zeta(N_j)$;
    \STATE $N^{new}=N^{new}\cup\{v^{new}\}$;

   \ENDIF
   \ENDFOR
   \STATE $N^{\prime}_i = N^{new}$;
   \ENDFOR
   \RETURN $N^{\prime}_i$

\end{algorithmic}
\end{algorithm}

After generating various token sets, we utilize them to construct the input token sequence for representation learning.
Given a token set $N_i$, the input token sequence $\mathbf{Z}_i^s$ of node $v_i$ is as:
\begin{equation}
    \mathbf{Z}_i^s = [\mathbf{X}_i, \mathbf{X}_{N_{i,0}}, \dots, \mathbf{X}_{N_{i,k-1}}].
    \label{eq:input_token}
\end{equation}
%Suppose we perform 
By performing Algorithm \ref{alg:tokenswap} $s$ times, we can finally obtain $1+s$ token sets for the target node $v_i$.
We combine all token sequences generated by \autoref{eq:input_token} based on different token sets to obtain the final input token sequences $\mathbf{Z}_i\in \mathbb{R}^{(1+s)\times (1+k) \times d}$.
Following the same process, the input token sequences generated by $N^{A}_i$ and $N^{T}_i$ are denoted as $\mathbf{Z}^A_i$ and $\mathbf{Z}^T_i$, respectively.


% transformer-based backbone
\subsection{Transformer-based Backbone}
The proposed Transformer-based backbone aims to learn node representations and predict the node labels according to the input token sequences.
Take the input sequence $\mathbf{Z}^A_i$ for example, we first utilize the projection layer to obtain the model input:
\begin{equation}
    \mathbf{Z}^{A,(0)}_i = \rho(\mathbf{Z}^A_i),
    \label{eq:projection}
\end{equation}
where $\rho(\cdot)$ denotes the projection layer and $\mathbf{Z}^{A,(0)}_i \in \mathbb{R}^{(1+s)\times k \times d_0}$ denotes the model input of node $v_i$.

Then, a Transformer layer-based encoder is applied to learn node representations from the model input:
\begin{align}
\mathbf{Z}^{\prime A,(l)}_{i} &=\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{Z}^{A,(l-1)}_{i}\right)\right)+\mathbf{Z}^{A, (l-1)}_{i}, \\
\mathbf{Z}^{A,(l)}_{i} &=\operatorname{FFN}\left(\operatorname{LN}\left(\mathbf{Z}^{\prime A,(l)}_{i}\right)\right)+\mathbf{Z}^{\prime A,(l)}_{i}, 
\end{align}
where $l=1, \ldots, L$ indicates the $l$-th Transformer layer and $\mathrm{LN}(\cdot)$ denotes the LayerNorm operation.

Through the encoder, we obtain the representations of input token sequences $\mathbf{Z}^{A,(L)}_i \in \mathbb{R}^{(1+s)\times (1+k) \times d_L}$. 
Then, we take the representation of the first item in each token sequence as the final representation of the input sequence.
This is because the first item in each token sequence is the target node itself, and the output representation has learned necessary information from other sampling tokens in the input sequence via the Transformer encoder.

Hence, the final output of the Transformer encoder is denoted as $\mathbf{Z}^{A,i} \in \mathbb{R}^{(1+s) \times d_L}$.
Then, we utilize the following readout function to obtain the node representation learned from multi token sequences:
\begin{equation}
    \mathbf{Z}^{A}_i = \mathbf{Z}^{A,i}_0||( \frac{1}{s}\sum_{j=1}^{s} \mathbf{Z}^{A,i}_j),
    \label{eq:readout}
\end{equation}
where $||$ denotes the concatenation operation, $\mathbf{Z}^{A}_i\in \mathbb{R}^{1\times d_L}$ denotes the representation of $v_i$ learned from token sequences generated from the attribute feature space.
Similarly, we can obtain $\mathbf{Z}^{T}_i$ from the topology feature space.

To effectively utilize information of different feature spaces, we leverage the following strategy to fuse the learned representations:
\begin{equation}
    \mathbf{Z}^{F}_i = \alpha\cdot\mathbf{Z}^{A}_i + (1-\alpha)\cdot\mathbf{Z}^{T}_i,
    \label{eq:finalrepre}
\end{equation}
where $\alpha \in [0, 1]$ is a hyper-parameter to balance the contribution from attribute features and topology features on the final node representation.

At the end, we adopt Multi-Layer Perception-based predictor for label prediction and the cross-entropy loss for model training:
\begin{equation}
    \mathbf{Y}^{\prime}_i = \mathrm{MLP}(\mathbf{Z}^{F}_i),
    \label{eq:prediction}
\end{equation}
\begin{equation}
        \mathcal{L}_{ce} = -\sum_{i\in V_{L}}\sum_{j = 0}^{c} {\mathbf{Y}_{i,j}}\mathrm{ln}\mathbf{Y}^{\prime}_{i,j}.
    \label{eq:celoss}
\end{equation}

\input{Tab/nc_dense}


\subsection{Center Alignment Loss}
To further enhance the model's generalization, we develop a center alignment loss to constrain the representations learned from different token sequences for each node.
Specifically, given the representations of multi-token sequences $\mathbf{Z}^{i}\in \mathbb{R}^{(1+s) \times d_L}$, we first calculate the center representation $\mathbf{Z}^{i}_c = \frac{1}{(1+s)}\sum_{j=0}^{s} \mathbf{Z}^{i}_j$.
Then, the center alignment loss is calculated as follows:
\begin{equation}
        \mathrm{CAL}(\mathbf{Z}^{i}, \mathbf{Z}^{i}_c) = 1 - \frac{1}{(1+s)}\sum_{j=0}^{s} \mathrm{Cosine}(\mathbf{Z}^{i}_j, \mathbf{Z}^{i}_c),
    \label{eq:cal}
\end{equation}
where $\mathrm{Cosine}(\cdot)$ denotes the cosine similarity function.

The rationale of \autoref{eq:cal} is that the representations learned from different token sequences can be regarded as different views of the target node.
Therefore, these representations should naturally be close to each other in the latent space.
In practice, we separately calculate the center alignment loss of token sequences from different feature spaces:
\begin{equation}
        \mathcal{L}_{ca} = \mathrm{CAL}(\mathbf{Z}^{A,i}, \mathbf{Z}^{A,i}_c) + \mathrm{CAL}(\mathbf{Z}^{T,i}, \mathbf{Z}^{T,i}_c).
    \label{eq:calloss}
\end{equation}
The overall loss of \name is as follows:
\begin{equation}
        \mathcal{L} = \mathcal{L}_{ce} + \lambda\cdot\mathcal{L}_{ca},
    \label{eq:allloss}
\end{equation}
where $\lambda$ is a coefficient controlling the balance between the two loss functions.