

\subsection{Graph Neural Networks}
GNNs~\cite{gnn3,gnn2,rlp,pamt,ncn} have shown remarkable performance in this task. 
Previous studies \cite{gat,jknet,sgc,appnp} have primarily concentrated on the incorporation of diverse graph structural information into the message-passing framework. 
Classic deep learning techniques, such as the attention mechanism \cite{gat,gatv2} and residual connections \cite{jknet,gcnii}, have been exploited to enhance the information aggregation on graphs. 
Moreover, aggregating information from high-order neighbors \cite{appnp,mixhop,h2gnn} or nodes with high similarity across different feature spaces \cite{geomgcn} has been demonstrated to be efficacious in improving model performance.

Follow-up GNNs have focused on the utilization of complex graph features to extract distinctive node representations. 
A prevalent strategy entails the utilization of signed aggregation weights \cite{fagcn,gprgnn,acmgnn,glognn} to optimize the aggregation operation. 
In this way, positive and negative values are respectively associated with low- and high-frequency information, thereby enhancing the discriminative power of the learned node representations.
Nevertheless, restricted by the inherent limitations of message-passing mechanism, the potential of GNNs for graph data mining has been inevitably weakened.
Developing a new graph deep learning paradigm has attracted great attention in graph representation learning. 



\subsection{Graph Transformers}
GTs~\cite{polynormer,agt,cob} have emerged as a novel architecture for graph representation learning and have exhibited substantial potential in node classification. 
A commonly adopted design paradigm for GTs is the combination of Transformer modules with GNN-style modules to construct hybrid neural network layers, called hybrid GTs~\cite{nodeformer,sgformer,specformer}. 
In this design, Transformer is employed to capture global information, while GNNs are utilized for local information extraction~\cite{graphgps,polynormer,signgt}.
Despite effectiveness, directly utilizing Transformer to model the interactions of all node pairs could occur the over-globalization issue~\cite{cob}, inevitably weakening the potential for graph representation learning.

An alternative yet effective design of GTs involves transforming the input graph into independent token sequences termed tokenized GTs \cite{ansgt,nagphormer,vcrgt,polyformer,ntformer}, which are then fed into the Transformer layer for node representation learning. 
Neighborhood tokens~\cite{nagphormer,nag+,polyformer,vcrgt,ntformer} and node tokens~\cite{ansgt,ntformer,vcrgt} are two typical elements in existing tokenized GTs.
The former, generally constructed by propagation approaches, such as random walk~\cite{nagphormer,nag+} and personalized PageRank~\cite{vcrgt}.
The latter is generated by diverse sampling methods based different similarity measurements, such as PageRank score~\cite{vcrgt} and attribute similarity~\cite{ansgt}. 
Since tokenized GTs only focus on the generated tokens, they naturally avoiding the over-globalization issue.

As pointed out in previous study~\cite{vcrgt}, node token oriented GTs are more efficient in capturing various graph information, such as long-range dependencies and heterophily, compared to neighborhood token oriented GTs.
However, we identify that previous methods only leverage a small subset of nodes as tokens for node representation learning, which could limit the model ability of deeply exploring graph information.
In this paper, we develop a new method \name that introduces a novel token swapping operation to produce more informative token sequences, further enhancing the model performance.