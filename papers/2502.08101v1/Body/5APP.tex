\section{Experimental Settings}

\subsection{Dataset}\label{app:data}
Here we introduce datasets adopted for experiments. The detailed statistics of all datasets are reported in \autoref{tab:dataset}.
\begin{itemize}
    \item \textbf{Academic graphs}: This type of graph is formed by academic papers or authors and the citation relationships among them. Nodes in the graph represent academic papers or authors, and edges represent the citation relationships between papers or co-author relationships between two authors. The features of nodes are composed of bag-of-words vectors, which are extracted and generated from the abstracts and introductions of the academic papers. The labels of nodes correspond to the research fields of the academic papers or authors. ACM, Citeseer, WikiCS and UAI2010 belong to this type.
    \item \textbf{Co-purchase graphs}: This type of graph is constructed based on users' shopping behaviors. Nodes in the graph represent products. The edges between nodes indicate that two products are often purchased together. The features of nodes are composed of bag-of-words vectors extracted from product reviews. The category of a node corresponds to the type of goods the product belongs to. Computer and Photo belong to this type.

    \item \textbf{Social graphs}: This type of graph is formed by the activity records of users on social platforms. Nodes in the graph represent users on the social platform. The edges between nodes indicate the social relations between two users. Node features represent the text information extracted from the authors' homepage. The label of a node refers to the interest groups of users. BlogCatalog and Flickr belong to this type.
    
\end{itemize}

\input{Tab/dataset}
\subsection{Implementation Details}\label{app:imple}
For baselines, we refer to their official implementations and conduct a systematic tuning process on each dataset.
For \name, we employ a grid search strategy to identify the optimal parameter settings.
Specifically, We try the learning rate in $\{0.001, 0.005, 0.01\}$, dropout in $\{0.3, 0.5, 0.7\}$, dimension of hidden representations in $\{256, 512\}$,
$k$ in $\{4, 6, 8\}$, $\alpha$ in $\{0.1, \dots, 0.9\}$.
All experiments are implemented using Python 3.8, PyTorch 1.11, and CUDA 11.0 and executed on a Linux server with an Intel Xeon Silver 4210 processor, 256 GB of RAM, and a 2080TI GPU.


\section{Additional Experimental Results}\label{app:exp-results}
In this section, we provide the additional experimental results of ablation studies and parameter studies.




\subsection{Study of the center alignment loss}\label{app:exp-ca}
The experimental results of \name and \name-O on the rest datasets are shown in \autoref{fig:align-APP}.
We can observe that \name outperforms \name-O on most datasets.
Moreover, the effect of applying the center alignment loss on \name in sparse splitting is more significant than that in dense splitting.
The above observations are in line with those reported in the main text.
Therefore, we can conclude that the center alignment loss can effectively enhance the performance of \name in node classification.


\begin{figure}[ht]
\centering
\includegraphics[width=17cm]{Fig/alignloss-APP.pdf}
\caption{
Performances of \name with or without the center alignment loss.
}
\label{fig:align-APP}
\end{figure}




\subsection{Study of the token sequence generation}\label{app:exp-ts}
The experimental results of \name with different token sequence generation strategies on the rest datasets are shown in \autoref{fig:ts-APP}.
We can find that the additional experimental results exhibit similar observations shown in the main text.
This situation demonstrates the effectiveness of the token sequence generation with the proposed token swapping operation in enhancing the performance of tokenized GTs.
Moreover, we can also observe that the gains of introducing the token swapping operation vary on different graphs based on the results shown in \autoref{fig:ts} and \autoref{fig:ts-APP}.
This phenomenon may attribute to that different graphs possess unique topology and attribute information, which further impact the selection of node tokens. 
While \name applies the uniform strategy for selecting node tokens, which could lead to varying gains of token swapping.
This situation also motivates us to consider different strategies of token selection on different graphs as the future work.


\begin{figure}[ht]
\centering
\includegraphics[width=17cm]{Fig/ts-p-APP.pdf}
\caption{
Performances of \name with different token sequence generation strategies.
}
\label{fig:ts-APP}
\end{figure}



\subsection{Analysis of the swapping times $t$}\label{app:exp-t}
Here we report the rest results of \name with varying $t$, which are shown in \autoref{fig:t-APP}.
Similar to the phenomenons shown in \autoref{fig:t}, \name can achieve the best performance on all datasets when $t>2$.
Based on the results shown in \autoref{fig:t-APP} and \autoref{fig:t}, we can conclude that introducing tokens beyond first-order neighbors via the proposed token swapping operation can effective improve the performance of \name in node classification.


\begin{figure}[ht]
\centering
\includegraphics[width=17cm]{Fig/t-APP.pdf}
\caption{
Performances of \name with varying $t$.
}
\label{fig:t-APP}
\end{figure}



\subsection{Analysis of the augmentation times $s$}\label{app:exp-s}
Similar to analysis of $t$, the rest results of \name with varying $s$ are shown in \autoref{fig:s-APP}.
We can also observe the similar situations shown in \autoref{fig:s} that \name requires a larger value of $s$ under sparse splitting compared to dense splitting.
The situation demonstrates that introducing augmented token sequences can bring more significant performance gain in sparse splitting than that in dense splitting. 




\begin{figure}[t]
\centering
\includegraphics[width=17cm]{Fig/s-APP.pdf}
\caption{
Performances of \name with varying $s$.
}
\label{fig:s-APP}
\end{figure}