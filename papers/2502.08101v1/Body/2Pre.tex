

\subsection{Node Classification}
Suppose an attributed graph is denoted as $\mathcal{G}=(V, E, \mathbf{X})$ where $V$ and $E$ are the sets of nodes and edges in the graph.
$\mathbf{X} \in \mathbb{R}^{n \times d}$ is the attribute feature matrix, where $n$ and $d$ are the number of nodes and the dimension of the attribute feature vector, respectively.
We also have the adjacency matrix $\mathbf{A} = \{0, 1\}^{n\times n}$.
If there is an edge between nodes $v_i$ and $v_j$, $\mathbf{A}_{ij} = 1$; otherwise, $\mathbf{A}_{ij} = 0$. 
$\hat{\mathbf{A}}$ denotes the normalized version calculated as $\hat{\mathbf{A}}=(\mathbf{D}+\mathbf{I})^{-1/2}(\mathbf{A}+\mathbf{I})(\mathbf{D}+\mathbf{I})^{-1/2}$ where $\mathbf{D}$ and $\mathbf{I}$ are the diagonal degree matrix and the identity matrix, respectively.
In the scenario of node classification, each node is associated with a one-hot vector to identify the unique label information, resulting in a label matrix $\mathbf{Y} \in \mathbb{R}^{n \times c}$ where $c$ is the number of labels.
Given a set of labeled nodes $V_L$, the goal of the task is to predict the labels of the rest  nodes in $V-V_L$.


\subsection{Transformer}
Here, we introduce the design of the Transformer layer, which is the key module in most GTs. 
There are two core components of a Transformer layer~\cite{transformer}, named multi-head self-attention (MSA) and feed-forward network (FFN).
Given the model input $\mathbf{H}^{n\times d}$, the calculation of MSA is as follows:
\begin{equation}
    \mathrm{MSA}(\mathbf{H}) = (||_{i=1}^{m}head_i)\cdot \mathbf{W}_{o},
    \label{eq:msa}
\end{equation}
\begin{equation}
    head_i = \mathrm{softmax}\left(\frac{[(\mathbf{H}\cdot\mathbf{W}^{Q}_{i}) \cdot (\mathbf{H}\cdot\mathbf{W}^{K}_{i})^{\mathrm{T}}]}{\sqrt{d_k}}\right)\cdot (\mathbf{H}\cdot\mathbf{W}^{V}_{i}), 
    \label{eq:single-head}
\end{equation}
where $\mathbf{W}^{Q}_{i}$, $\mathbf{W}^{K}_{i}$ and $\mathbf{W}^{V}_{i}$ are the learnable parameter matrices of the $i$-th attention head.
$m$ is the number of attention heads.
$||$ denotes the vector concatenation operation.
$\mathbf{W}_{o}$ denotes a projection layer to obtain the final output of MSA.

FFN is constructed by two linear layers and one non-linear activation function:
\begin{equation}
    \mathrm{FFN}(\mathbf{H}) = \sigma(\mathbf{H}\cdot\mathbf{W}^{1})\cdot\mathbf{W}^{2},
    \label{eq:ffn}
\end{equation}
where $\mathbf{W}^{1}$ and $\mathbf{W}^{2}$ denote learnable parameters of the two linear layers and $\sigma(\cdot)$ denotes the GELU activation function.