% In this section, we first introduce the experimental settings, including datasets, baselines and implementation details. Then we conduct a series of experiments, involving performance comparison, parameter sensitiveness analysis and ablation study, to comprehensively evaluate our proposed \name.


% \input{Tab/nc_dense}

\subsection{Dataset}
We adopt eight widely used datasets, involving homophily and heterophily graphs: 
Photo~\cite{nagphormer}, ACM~\cite{acm}, Computer~\cite{nagphormer}, BlogCatalog~\cite{socialnets}, UAI2010~\cite{amgcn}, Flickr~\cite{socialnets} and Wiki-CS~\cite{roman}.
The edge homophily ratio~\cite{glognn} ${H}(\mathcal{G})\in[0,1]$ is adopted to evaluate the graph's homophily level. 
${H}(\mathcal{G}) \rightarrow 1$ means strong homophily, 
while ${H}(\mathcal{G}) \rightarrow 0$ means strong heterophily.
Statistics of datasets are summarized in Appendix \ref{app:data}.
To comprehensively evaluate the model performance in node classification, we provide two strategies to split datasets, called dense splitting and sparse splitting.
In dense splitting, we randomly choose 50\% of each label as the training set, 25\% as the validation set, and the rest as the test set, which is a common setting is previous studies~\cite{nodeformer,sgformer}.
While in sparse splitting~\cite{gprgnn}, we adopt 2.5\%/2.5\%/95\% splitting for training set, validation set and test set, respectively.


\subsection{Baseline}
We adopt eleven representative approaches as the baselines: SGC~\cite{sgc}, APPNP~\cite{appnp}, GPRGNN~\cite{gprgnn}, FAGCN~\cite{fagcn}, BM-GCN~\cite{bmgcn}, ACM-GCN~\cite{acmgnn}, NAGphormer~\cite{nagphormer}, SGFormer~\cite{sgformer}, Specformer~\cite{specformer}, VCR-Graphormer~\cite{vcrgt} and PolyFormer~\cite{polyformer}.
The first six are mainstream GNNs and others are representative GTs.



\input{Tab/nc_sparse}

% \input{Tab/nc_results}


\subsection{Performance Comparison}
To evaluate the model performance in node classification, we run each model ten times with random initializations. The results in terms of mean accuracy and standard deviation are reported in \autoref{tab:dense-ncre} and \autoref{tab:sparse-ncre}.

First, we can observe that \name achieves the best performance on all datasets with different data splitting strategies, demonstrating the effectiveness of \name in node classification.
Then, we can find that advanced GTs obtain more competitive performance than GNNs on over half datasets under dense splitting.
But under sparse splitting, the situation reversed.
An intuitive explanation is that Transformer has more learnable parameters than GNNs, which bring more powerful modeling capacity.
However, it also requires more training data than GNNs in the training stage to ensure the performance.
Therefore, when the training data is sufficient, GTs can achieve promising performance.
And when the training data is sparse, GTs usually leg behind GNNs.
Our proposed \name addresses this issue by introducing the token swapping operation to generate diverse token sequences. 
This operation effectively augments the training data, ensuring the model training even in the sparse data scenario.
In addition, the tailored center alignment loss also constrains the model parameter learning, further enhancing the model performance.


\begin{figure}[t]
\centering
\includegraphics[width=7.5cm]{Fig/alignloss-p.pdf}
\caption{
Performances of \name with or without the center alignment loss.
}
\label{fig:align}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=7.5cm]{Fig/ts-p.pdf}
\caption{
Performances of \name with different token sequence generation strategies.}
\label{fig:ts}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=7.3cm]{Fig/t-p.pdf}
\caption{
Analysis on the swapping times $t$.}
\label{fig:t}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=7.3cm]{Fig/s-p.pdf}
\caption{
Analysis on the augmentation times $s$.}
\label{fig:s}
\end{figure}



\subsection{Study on the center alignment loss}
The center alignment loss, proposed to constrain the representation learning from multiple token sequences, is a key design of \name.
Here, we validate the effectiveness of the center alignment loss in node classification.
Specifically, we develop a variant of \name by removing the center alignment loss, called \name-O.
Then, we evaluate the performance of \name-O on all datasets under dense splitting and sparse splitting.
Due to the space limitation, we only report the results on four datasets in \autoref{fig:align}, other results are reported in Appendix \ref{app:exp-ca}.
"Den." and "Spa." denotes the experimental results under dense splitting and sparse splitting, respectively.
Based on the experimental results, we can have the following observations:
1) \name beats \name-O on all datasets, indicating that the developed center alignment loss can effectively enhance the performance of \name.
2) Adopting the center alignment loss can bring more significant improvements in sparse setting than those in dense setting.
This situation implies that introducing the reasonable constraint loss function based on the property of node token sequences can effectively improve the model training when the training data is sparse.






\subsection{Study on the token sequence generation}\label{exp:ts}
The generation of token sequences is another key module of \name, which develops a novel token swapping operation can fully leverage the semantic relevance of nodes to generate informative token sequences.
In this section, we evaluate the effectiveness of the proposed strategy by comparing it with two naive strategies.
One is to enlarge the sampling size $k$. We propose a variant called \name-L by sampling $2k$ tokens to construct token sequences.
The other is to randomly sample $k$ tokens from the enlarged $2k$ token set to construct multiple token sequences, called \name-R.
Performance of these variants are shown in \autoref{fig:ts} and results on other datasets are reported in Appendix \ref{app:exp-ts}.

We can observe that \name-R outperforms \name-L on most cases, indicating that constructing multiple token sequences is better for node representation learning of tokenized GTs than generating single long token sequence. 
Moreover, \name surpasses \name-R on all cases, showcasing the superiority of the proposed token swapping operation in generation of multiple token sequences.
This observation also implies that constructing informative token sequences can effectively improve the performance of tokenized GTs.



\subsection{Analysis on the swapping times $t$}
As discussed in Section \ref{sec:swapping}, $t$ determines the range of candidate tokens from the constructed $k$-NN graph, further affecting the model performance.
To validate the influence of $t$ on model performance, we vary $t$ in $\{1,2,3,4\}$ and observe the changes of model performance.
Results are shown in \autoref{fig:t} and Appendix \ref{app:exp-t}.
We can clearly observe that \name can achieve satisfied performance on all datasets when $t$ is no less than 2.
This situation indicates that learning from tokens with semantic associations beyond the immediate neighbors can effectively enhancing the model performance.
This phenomenon also reveals that reasonably enlarging  the sampling space to seek more informative tokens is a promising way to improve the effect of node tokenized GTs.
%分析的

\subsection{Analysis on the augmentation times $s$}
The augmentation times $s$ determines how many token sequences are adopted for node representation learning. Similar to $t$, we vary $s$ in $\{1,2,\dots,8\}$ and report the performance of \name. 
Results are shown in \autoref{fig:s} and Appendix \ref{app:exp-s}.
Generally speaking, sparse splitting requires a larger $s$ to achieve the best performance, compared to dense splitting.
This is because \name needs more token sequences for model training in the sparse data scenario.
This situation indicates that a tailored data augmentation strategy can effectively improve the performance of tokenized GTs when training data is sparse.
Moreover, the optimal $s$ varies on different graphs. 
This is because different graphs exhibit different topology features and attribute features, which affects the generation of token sequences, further influencing the model performance.


