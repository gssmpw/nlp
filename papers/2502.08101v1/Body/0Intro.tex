
%nc --> GT
Node classification, the task of predicting node labels in a graph, is a fundamental  problem in graph data mining with numerous real-world applications. 
Graph Neural Networks (GNNs)~\cite{gnn1,gcn} have traditionally been the dominant approaches. 
However, the message passing mechanism inherent to GNNs suffers from some limitations, such as over-smoothing~\cite{oversm}, 
which prevents them from effectively capturing deep graph structural information and hinders their performance in downstream tasks. 


%category of GT, hybrid GTs and tokenized GTs
In contrast, Graph Transformers (GTs), which adapt the Transformer framework for graph-based learning, have emerged as a promising alternative, demonstrating impressive performance in node classification. Existing GTs can be broadly classified into two categories based on their model architecture: hybrid GTs and tokenized GTs. 

%hybrid GTs --> tokenzied GTs
Hybrid GTs combine the strengths of GNN and Transformer, using GNNs to capture local graph topology and Transformers to model global semantic relationships. 
However, recent studies have highlighted a key issue with this approach: 
directly modeling semantic correlations among all node pairs using Transformers can lead to the over-globalization problem~\cite{cob}, which compromises model performance. 

Tokenized GTs, on the other hand, generate independent token sequences for each node, which encapsulate both local topological and global semantic information.
Transformer models are then utilized to learn node representations from these token sequences. 
The advantage of tokenized GTs is that they limit the token sequences to a small, manageable number of tokens, naturally avoiding the over-globalization issue. 
In tokenized GTs, the token sequences typically include two types of tokens: neighborhood tokens and node tokens. Neighborhood tokens aggregate multi-hop neighborhood information of a target node, while node tokens are sampled based on the similarity between nodes.

% neighborhood tokens and node tokens
However, recent studies~\cite{vcrgt} have shown that neighborhood tokens often fail to preserve complex graph properties such as long-range dependencies and heterophily, limiting the richness of node representations. On the other hand, node tokens, generated through various sampling strategies, can better capture correlations between nodes in both feature and topological spaces~\cite{ansgt,ntformer}, making them more effective in preserving complex graph information. As a result, this paper focuses on node token-based GTs.

% summarize of node token --> top k sampling 
% top k sampling --> 1-hop neighbor in KNN graph
A recent study~\cite{ntformer} formalized the node token generation process as two key steps: similarity evaluation and top-$k$ sampling. 
In the first step, similarity scores between node pairs are calculated based on different similarity measures to preserve the relations of nodes in different feature spaces. While in the second step, the top $k$ nodes with the highest similarity scores are selected as node tokens to construct the token sequence. 
In this paper, we provide a new perspective on token generation in existing tokenized GTs. 
We identify that the token generation process can be viewed as a neighbor selection operation on the $k$-nearest neighbor ($k$-NN) graph.  
Specifically, a $k$-NN graph is constructed based on node pair similarities, and the neighbor nodes within the first-order neighborhood of each node are selected to form  the token sequence.


\begin{figure}[h]
    \centering    
    \includegraphics[width=7.5cm]{Fig/motivation2.pdf}
    \caption{The toy example of token generation on the $k$-NN graph. Previous methods only focus on 1-hop neighborhood to construct a single token sequence. While our method can flexibly select tokens from multi-hop neighborhoods to generate diverse token sequences.}
    \label{fig:motivation}
\end{figure}


\autoref{fig:motivation} illustrates this idea with a toy example.
We can observe that only a small subset of nodes is selected via existing token generation strategies, which indicates that existing methods have limited exploitation of the $k$-NN graph and are unable to comprehensively utilize the correlations between node pairs to explore more informative nodes with potential association to construct token sequences. 
This situation inevitably restricts the ability of tokenized GTs to capture informative node representations. 
Furthermore, in scenarios with sparse training data, relying on token sequences generated from a limited set of nodes may lead to over-fitting, as Transformers, being complex models, may struggle to generalize effectively.

% our method
This leads to the following research question: \textit{How can we more comprehensively and effectively exploit node pair correlations to generate diverse token sequences, thus improving the performance of tokenized GTs for node classification?} 
To address this, we introduce a novel method called \name. 
Specifically, \name introduces a new operation, token swapping, which leverages the  semantic correlations of nodes in the $k$-NN to swap tokens in different token sequences, generating more diverse token sequences.
By incorporating multiple token sequences, \name 
enables the model to learn more comprehensive node representations. 
Additionally, \name employs a Transformer-based backbone and introduces a tailored readout function to learn node representations from the generated token sequences.
To handle the case where a node is assigned multiple token sequences, we propose a center alignment loss to guide the training process. 
The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item We propose a novel token swapping operation that fully exploits semantic correlations of nodes to generate diverse token sequences.
    \item We develop a Transformer-based backbone with a center alignment loss to learn node representations from the generated diverse token sequences.
    \item Extensive experiments on various datasets with different training data ratios showcase the effectiveness of \name in node classification. 
\end{itemize}





