\section{Preliminaries}
\label{sec:problem-formulation}

\begin{figure*}
    \centering
    \setlength{\abovecaptionskip}{-0.05cm}
    \setlength{\belowcaptionskip}{-0.2cm}
    \includegraphics[width=1\linewidth]{Figure/overview-all-comic.png}
    \caption{ArchRAG consists of two phases: offline indexing and online retrieval. For the online retrieval phase, we show an example of using ArchRAG to answer a question in the HotpotQA dataset.}
    % ArchRAG retrieves the relevant communities and entities from the index and extracts the answer through adaptive filtering-based generation.}
    \label{fig:overview}
\end{figure*}


%Graph-based RAG typically leverages external structured knowledge graphs to enhance the contextual understanding of LLMs and produce more informed responses.

Existing graph-based RAG methods often follow a general framework that leverages external structured knowledge graphs to improve contextual understanding of LLMs and generate more informed responses \cite{peng2024graph}.
% 
Typically, it involves two phases: 
\begin{enumerate}
    \item {\bf Offline indexing:} Building a KG $G(V,E)$ from a given corpus $D$ where each vertex represents an entity and each edge denotes the relationship between two entities, and constructing an index based on the KG.

    \item {\bf Online retrieval:} Retrieving the relevant information (e.g., nodes, subgraphs, or textual information) from KG using the index and providing the retrieved information to the LLM to improve the accuracy of the response.
    
    %\item {\bf KG construction}: Building a KG $G(V,E)$ from a given corpus $D$;
    %\item {\bf Element retrieval}: Retrieving relevant graph elements (e.g., nodes, subgraphs, or textual information) from KG using the graph structure;
    %\item {\bf Enhanced generation:} Providing the retrieved information to the LLM to improve the accuracy of the response.
\end{enumerate}

% Graph-based RAG typically leverages external structured knowledge graphs to enhance the contextual understanding of LLMs and produce more informed responses.
% 
% It involves three key steps: (1) Knowledge Graph (KG) Construction, (2) Element Retrieval, and (3) Enhanced Generation. 
% 
%The constructed KG, also referred to as a Text-Attributed Graph (TAG), records the semantic relationships distributed throughout the corpus, with each node and relation containing textual descriptions. Formally, we define a TAG as follows:

%\begin{definition}[Text-Attributed Graph (TAG) \cite{he2024g}]
%A Text-Attributed Graph (TAG) is a directed graph \(G = (V, E, \psi, \phi)\), where \(V\) and \(E\) represent the sets of nodes and edges, respectively. Each node \(v \in V\) and edge \(e \in E\) is associated with textual attributes \(\psi(v)\) and \(\phi(e)\), respectively.
%\end{definition}

%To avoid ambiguity, we use ``node" to refer to vertices in the graph and ``entity" to refer to vertices with textual attributes in KG.

% Different methods retrieve varying elements, such as relevant entities \cite{munikoti2023atlantic,wang2024knowledge}, subgraphs\cite{li2024dalk}, paths \cite{sun2023think,luo2023reasoning}, connected components in the graph \cite{he2024g,mavromatis2024gnn}, or even original chunk text \cite{gutierrez2024hipporag} and additional information \cite{edge2024local,sarthi2024raptor} linked to the knowledge graph. 
% Different methods retrieve varying elements, such as relevant entities \cite{munikoti2023atlantic,wang2024knowledge}, subgraphs\cite{li2024dalk}, paths \cite{sun2023think,luo2023reasoning}, and e.t.c. \cite{he2024g,mavromatis2024gnn,gutierrez2024hipporag,edge2024local,sarthi2024raptor}.
% % 
% Among these, using language models (i.e., embedding models) to compute text relevance is the primary method for retrieval.

% \begin{definition}[Language Models (LMs)]
%     A language model (LM) is a probabilistic model of a sequence of words.
%     % 
%     Given a text with $n$ words $x \in D^{n}$, an LM encodes this sequence of words as:

%     \begin{equation}
%         z_n = LM(x) \in \mathbb{R}^d
%     \end{equation}

%     where $z_n$ is the output vector of the LM, i.e., the embedding, and $d$ is the dimension of the output vector.
% \end{definition}

% % Language models, also known as embedding models, play a crucial role in encoding textual content.
% % % 
% % The resulting vectors capture the semantic meaning and key features of the data. 
% % %  
% % Semantically similar texts are positioned closer together in the vector space, while dissimilar texts are placed farther apart.
% % 

% In a RAG system, embedding vectors are typically stored in a vector database, enabling the efficient retrieval of relevant data during the query phase. 
% % 
% To facilitate this, Approximate Nearest Neighbor Search (ANNS) can be employed to quickly identify the closest data points to a given query, even if they are not exact neighbors.
% % 
% Next, we will introduce an efficient ANNS technique for vector databases.

% \begin{definition}[Hierarchical Navigable Small World (HNSW) \cite{malkov2018efficient}]
%     HNSW is a graph-based ANNS algorithm that consists of a multi-layered index structure, where each node uniquely corresponds to a vector in the database.
%     % 
%     Given a set $S$ containing $n$ vectors, the constructed HNSW can be represented as a pair $\mathcal{H} = (\mathcal{\bf G}, \mathcal{C})$. 
%     $\mathcal{\bf G}=\{G_0, G_1, \dots, G_L\}$ is a set of simple graphs (also called layers) $G_i=(V_i, E_i)$, where $i \in \{0,1,\dots, L\}$ and $V_L \subset V_{L-1} \subset \dots \subset V_1 \subset V_0 = S$.
%     $\mathcal{C}$ records the inter-layer mappings of edges between the same node across adjacent layers $\mathcal{C} = \bigcup_{i=0}^{L-1} \{(v,\phi(v))|v \in V_i,\phi(v)\in V_{i+1} \}$, where $\phi(v):V_i \rightarrow V_{i+1}$ is the mapping function for the same node across two adjacent layers.
% \end{definition}

% The nodes in the multi-layer graph of HNSW are organized in a nested structure, where each node at each layer is connected to its nearest neighbors. 
% % 
% During a query, the search begins at the top layer and quickly identifies the node closest to $q$ through a greedy search.
% % 
% Then, through inter-layer mapping, the search proceeds to the next lower layer.
% % 
% This process continues until all approximate nearest neighbors are identified in $G_0$.


% \subsection{Problem formulation}

% We aim to enhance the quality and factuality of an LLM through the retrieval and analysis of relevant information from a large set of documents.
% % 
% Formally, the Graph-based RAG problem is defined as follows:


% \begin{problem}[Graph-based RAG]
% \label{prob:graphrag}
% Given a knowledge base $\mathcal{B}$ from a large text corpus or a large set of text documents, Graph RAG constructs a TAG $G$ from $\mathcal{B}$.
% % 
% Given a query $q$ related to $\mathcal{B}$, Graph-based RAG leverages relevant graph components from $G$ and/or pertinent data from $\mathcal{B}$ to enhance the accuracy and factuality of the LLM's responses.
% \end{problem}










% \begin{definition}[Text-Attributed Graph (TAG) \cite{he2024g}]
%     Formally, a Text-Attributed Graph (TAG) is a graph where nodes and edges possess textual attributes.
%     % 
%     Formally, it can be defined as $\mathcal{G} = (V, E, \{x_n\}_{n \in V},\\ \{x_e\}_{e \in E})$, where $V$ and $E$ represent the sets of nodes and edges, respectively. 
%     % 
%     Additionally, $x_n \in D^{L_n}$  and $x_e \in D^{L_e}$ denote sequential text associated with a node $n \in V$ or an edge $e \in E$, where $D$ represents the vocabulary, and $L_n$ and $L_e$ signify the length of the text associated with the respective node or edge.
% \end{definition}

% A TAG incorporates external data to enhance LLM with additional knowledge, forming the foundation for GraphRAG tasks. 
% % 
% In the real world, knowledge graphs and document graphs are examples of TAGs, which can be categorized into the following two types:
% % 
% (1) existing general open KGs collaboratively built by users and experts, such as WikiData \footnote{https://www.wikidata.org/wiki}, DBPeida \footnote{https://www.dbpedia.org/},
% % 
% and (2) self-constructed graphs generated from a given document by extracting key entities and relationships or by identifying relationships within segmented text chunks.
% % 
% The former type of TAG can be directly obtained from open-source data, while the latter can be constructed using methods such as OpenIE \cite{pei2022use} and document graph construction techniques \cite{ferragina2010tagme, Rincon-Yanez2022,sarthi2024raptor,baek2023knowledge,gutierrez2024hipporag}.

% In this paper, we focus on the latter type of TAG because most domain-specific data lacks well-constructed TAGs. 
% 
% Additionally, we address the most widely studied area in RAG, namely, text-based RAG \cite{gao2023retrieval}.

