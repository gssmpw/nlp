\section{Related Work}
\label{sec:related-work}

In this section, we review the related works, including Retrieval-Augmentation-Generation (RAG) approaches, and LLMs for graph mining and learning.


$\bullet$ \textbf{RAG approaches.} RAG has been proven
to excel in many tasks, including open-ended question answering \cite{jeong2024adaptive,siriwardhana2023improving}, programming context \cite{chen2024auto,chen2023haipipe}, SQL rewrite~\cite{lillm,sun2024r}, and data cleaning~\cite{naeem2024retclean,narayan2022can,qian2024unidm}.
%
The naive RAG technique relies on retrieving query-relevant contexts from external knowledge bases to mitigate the ``hallucination'' of LLMs.
% 
Recently, many RAG approaches~\cite{edge2024local,guo2024lightrag,gutierrez2024hipporag,sarthi2024raptor,wang2024knowledge,he2024g,ma2024think,ma2024think,li2024dalk} have adopted graph structures to organize the information and relationships within documents, achieving improved overall retrieval performance.
% , which is extensively reviewed in this paper. 
%
For more details, please refer to the recent survey of graph-based RAG methods~\cite{peng2024graph}.
%


$\bullet$ \textbf{LLM for graph mining.}  Recent advances in LLMs have offered opportunities to leverage LLMs in graph mining. 
These include using LLMs for KG construction~\cite{zhu2024llms}, and employing KG to enhance the LLM reasoning~\cite{wang2023knowledge,wang2024knowledge,luo2023reasoning,sun2023think,mavromatis2024gnn,jiang2023structgpt}.
% by fusing the information of subgraph structure in the KGi into the LLM. 
%
For instance, KD-CoT~\cite{wang2023knowledge} retrieves facts from an external knowledge graph to guide the CoT performed by LLMs.
RoG \cite{luo2023reasoning} proposes a planning-retrieval-reasoning framework that retrieves reasoning paths from KGs to guide LLMs conducting faithful reasoning.
To capture graph structure, GNNRAG \cite{mavromatis2024gnn} adopts a lightweight graph neural network to retrieve elements from KGs effectively. StructGPT \cite{jiang2023structgpt} and ToG~\cite{sun2023think} treat LLMs as agents that interact with KGs to find reasoning paths leading to the correct answers.
Besides, LLM is used to generate code to solve complex graph mining problems, e.g., the shortest path computation~\cite{chen2024graphwiz,tang2024grapharena,cao2024graphinsight,zhang2024gcoder}. 

$\bullet$ \textbf{LLM for graph learning.}
%
%Graph intelligence represents a crucial step toward achieving general artificial intelligence.
%
Recently, LLMs have demonstrated remarkable zero-shot capabilities in processing text-attributed graphs.
Following this trend, graph foundation models have emerged as a promising direction~\cite{liu2023one,zhang2024graphtranslator}. 
For instance, Liu et al.~\cite{liu2023one} proposed a unified model capable of handling various graph ML tasks, such as node classification and link prediction, thereby eliminating the need for task-specific architectures.