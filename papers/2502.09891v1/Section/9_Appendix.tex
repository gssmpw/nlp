\appendix
\clearpage
\section{Method details}

\subsection{LLM-based hierarchical clustering}
\label{sec:LHC}

For example, as shown in the second step of offline indexing in Figure \ref{fig:overview}, LLM-based hierarchical clustering first enhances the original KG at layer $L_0$ by adding similar edges.
% 
Next, the weight of each edge is calculated based on the strength of the relationship between the node embeddings, and a weighted clustering algorithm is applied to obtain four communities.
% 
Based on the existence of links between nodes within a community, the topology of communities at layer $L_1$ is constructed, resulting in a graph of communities. 
% 
This process is repeated, ultimately generating a clustering result consisting of three layers of hierarchical communities.


\subsection{More details of C-HNSW}
\label{sec:c_hnsw}

% In a RAG system, embedding vectors are typically stored in a vector database, enabling the efficient retrieval of relevant data during the query phase. 
% % 
% To facilitate this, Approximate Nearest Neighbor Search (ANNS) can be employed to quickly identify the closest data points to a given query, even if they are not exact neighbors.
% % 
% Next, we will introduce an efficient ANNS technique for vector databases.

$\bullet$ \textbf{Introduction of HNSW. }
We provide a brief introduction to HNSW, an efficient Approximate Nearest Neighbor Search (ANNS) technique for vector databases.

\begin{definition}[Hierarchical Navigable Small World (HNSW) \cite{malkov2018efficient}]
    HNSW is a graph-based ANNS algorithm that consists of a multi-layered index structure, where each node uniquely corresponds to a vector in the database.
    % 
    Given a set $S$ containing $n$ vectors, the constructed HNSW can be represented as a pair $\mathcal{H} = (\mathcal{\bf G}, \mathcal{C})$. 
    $\mathcal{\bf G}=\{G_0, G_1, \dots, G_L\}$ is a set of simple graphs (also called layers) $G_i=(V_i, E_i)$, where $i \in \{0,1,\dots, L\}$ and $V_L \subset V_{L-1} \subset \dots \subset V_1 \subset V_0 = S$.
    $\mathcal{C}$ records the inter-layer mappings of edges between the same node across adjacent layers $\mathcal{C} = \bigcup_{i=0}^{L-1} \{(v,\phi(v))|v \in V_i,\phi(v)\in V_{i+1} \}$, where $\phi(v):V_i \rightarrow V_{i+1}$ is the mapping function for the same node across two adjacent layers.
\end{definition}

The nodes in the multi-layer graph of HNSW are organized in a nested structure, where each node at each layer is connected to its nearest neighbors. 
% 
During a query, the search begins at the top layer and quickly identifies the node closest to $q$ through a greedy search.
% 
Then, through inter-layer mapping, the search proceeds to the next lower layer.
% 
This process continues until all approximate nearest neighbors are identified in $G_0$.


$\bullet$ \textbf{The construction of C-HNSW. } 
The construction of C-HNSW is illustrated in Algorithm \ref{alg:construction}.
% 
The construction algorithm of C-HNSW follows a top-down approach. 
% 
Using the query process of C-HNSW, we obtain the $M$ nearest neighbors of each node in its layer, and the inter-layer links are continuously updated during this process.
% 
When inserting a node $x$ at layer $i$ , if the nearest neighbor at the layer $i+1$ is $c_j$, the inter-layer link of $c_j$ is updated in the following two cases:
\begin{itemize}
    \item Node $c_j$ does not have a inter-layer link to the layer $i$.
    \item The distance from node $c_j$ to node $x$ is smaller than the distance from $c_j$ to its previous nearest neighbor $x'$, i.e., $d(c_j,x) < d(c_j,x')$.
\end{itemize}

After all nodes at layer $i$ ($i<L$) have been inserted, we check each node at layer $i+1$ to ensure it has a inter-layer link, confirming the traverse from the higher layer to the lower layer.
% 

\begin{algorithm}[ht]
  \caption{{C-HNSW construction}}
  \label{alg:construction}
  \small
   \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{The Hierarchical community $\mathcal{HC}$, KG $\mathcal{G}(V,E)$, maximum number of connections for each node $M$.}
    % \Output{C-HNSW $\mathcal{H}=({\bf G,C})$.}
    $\mathcal{H}\gets\emptyset$\;
    
    ${\bf V}\gets$ \{$\mathcal{HC} \cup V$\} \tcp{\textcolor{teal}{Get all nodes of each layer.}}
    \For{each layer $l \gets L \cdots 0$}{
        \For{each node $v \in V_l$}{
            \If{$l\neq L$}{
                $R\gets$ SearchLayer(${ G_l}=(V_l,E_l), q, s, 1, l$)\;
                $c\gets$ get the nearest node from R\;
                $s\gets$ node in layer $l-1$ via $c$'s inter-layer link\;
                \If{$s$ is null\texttt{ or }$d(c,s)>d(c,v)$}{
                update $v$ as $c$'s inter-layer link
                }
            }
            \lIf{$l=L$\texttt{ or }$s$ is null}{$s\gets $ random node in layer $l$}
            $R\gets$ SearchLayer(${ G_l}=(V_l,E_l), q, s, M, l$)\;
            add edges between $v$ and $R$, update $E_l$\; 
        }
        $\mathcal{H}\gets \mathcal{H}\cup { G_l}=(V_l, E_l)$
    }
    \Return{$\mathcal{H}$;}
\end{algorithm}


\subsection{Complexity analysis of ArchRAG}
\label{sec:complexity}

We now analyze the complexity of our ArchRAG approach.
% 
Since the token cost is more important in the era of LLM, we replace the space complexity with the token cost.

The offline indexing process of ArchRAG includes the KG construction, hierarchical clustering, and C-HNSW construction.
% 
The time complexity and token usage are as follows:

% We give proof of the time complexity and token usage for HCARAG.

\begin{lemma}
\label{lemma:index-time}
Given a large text corpus or a large set of text documents with a total of $D$ tokens, the time complexity of the offline indexing process of ArchRAG is $O(I\frac{D}{w} + \frac{1-a^L}{1-a}(n*t+I\frac{D}{w}+\pi(m)+n\log n))$, where ${\bf I}$ is the generation time of the LLM for a single inference, $w$ is the specified token size of one chunk, $n$ and $m$ are the number of entities and relations in the extracted KG, $L$ is the height of the resulting hierarchical community structure, and $a$ is the average ratio of the number of nodes between two consecutive layers, with $0<a<1$. For a given embedding model, the computation time for the embedding of an entity description is denoted by $t$, while a specific clustering method is typically a function of $m$, represented as $\pi(m)$.
\end{lemma}

\begin{proof}
    For the corpus $D$, we use the LLM to infer and extract the KG from each chunk of size $w$, resulting in a cost of $O(I\frac{D}{w})$ for constructing the KG. Since the size of all community summaries in a single layer would not exceed the length of the corpus, their LLM inference time is also less than $O(I\frac{D}{w})$. For each layer of clustering, the embedding of each point must be computed, and clustering is performed with time complexity of $\pi(m)$. In the C-HNSW construction, each point requires $O(nlogn)$ time to perform the k-nearest neighbor search and establish connections, which is similar to the proof in \cite{malkov2018efficient}. For an L-layer multi-layer graph structure, where the number of nodes decreases by a factor of $a$ between two consecutive layers, the increase in clustering and C-HNSW construction time is given by: $\frac{1 - a^L}{1 - a}$.
\end{proof}


\begin{lemma}
\label{lemma:index-cost}
Given a large text corpus or a large set of text documents with a total of $D$ tokens, the number of tokens used in the offline indexing process of ArchRAG is $O(D + D\frac{1-a^L}{1-a})$, where $L$ is the height of the resulting hierarchical community structure and $a$ is the average ratio of the number of nodes between two consecutive layers, with $0<a<1$.
\end{lemma}

\begin{proof}
    Based on the above proof, the token cost for constructing the KG is $O(D)$, while the token cost for the summaries does not exceed $O(D\frac{1-a^L}{1-a})$.
\end{proof}

Generally, $L$ is $O(\log n)$, where $n$ is the number of extracted entities. 
However, due to the constraints of community clustering, $L$ is typically constant, usually no greater than 5.


Next, we analyze the time complexity and token usage in the online query process of the ArchRAG.

\begin{lemma}
\label{lemma:query-time}
Given a C-HNSW with $L$ layers constructed from a large text corpus or a large set of text documents, the time complexity of a sequentially executed single online retrieval query in ArchRAG is $O(e+ LkI+Lk\log(n))$, where $I$ is the generation time of the LLM for a single inference, $e$ is the time cost of computing the query embedding, $k$ is the number of nodes retrieved at each layer, and $n$ is the number of nodes at the lowest layer in C-HNSW.
\end{lemma}

\begin{proof}
    ArchRAG first computes the embedding of the query, which takes $O(e)$ time. For each layer, querying one nearest neighbor takes no more than $O(\log(n))$ time, similar to the proof in \cite{malkov2018efficient}. In the Adaptive filtering-based generation, the content of each query is analyzed and inferred, requiring $O(LkI)$ time. Therefore, the total time for the online retrieval query is $O(e+ LkI+Lk\log(n))$.
\end{proof}

\begin{lemma}
\label{lemma:query-cost}
Given a C-HNSW with $L$ layers constructed from a large text corpus or a large set of text documents, the number of tokens used for a single online retrieval query in ArchRAG is $O(kL(c+P)))$, where $k$ is the number of neighbors retrieved at each layer, $c$ is the average length of the retrieved content, and $P$ is the length of the prompt.
\end{lemma}

\begin{proof}
    The token consumption for analyzing all retrieved information is $O(kL(c+P)))$, while the token consumption for generating the final response is of constant order. Therefore, the total token consumption for the online retrieval is $O(kL(c+P)))$.
\end{proof}

In practice, multiple retrieved contents are combined, allowing the LLM to analyze them together, provided the total token count does not exceed the token size limit. 
% 
As a result, the time and token usage for online retrieval are lower than those required for analysis.

\section{Experimental details}



\subsection{Metrics}
\label{sec:metrics}

This section provides additional details on the metrics.

$\bullet$ \textbf{Metrics for specific QA tasks. } We choose accuracy as the evaluation metric based on whether the gold answers are included in the model's generations rather than strictly requiring an exact match, following \cite{schick2024toolformer,mallen2022not,asai2023self}.
% 
This is because LLM outputs are typically uncontrollable, making it difficult for them to match the exact wording of standard answers.
% 
Similarly, we choose recall as the metric instead of precision, as it better reflects the accuracy of the generated responses.
% 
Additionally, when calculating recall, we adopt the same approach as previous methods \cite{gutierrez2024hipporag,asai2023self}: if the golden answer or the generated output contains ``yes'' or ``no'', the recall for that question is set to 0. 
% 
Therefore, the recall metric is not perfectly correlated with accuracy.

$\bullet$ \textbf{Metrics for abstract QA tasks. }
% 
Following existing works, we use an LLM to generate abstract questions, with the prompts shown in Figure \ref{fig:prompt_summary}, defining ground truth for abstract questions, particularly those involving complex high-level semantics, poses significant challenges.
% 
We build on existing works \cite{edge2024local,guo2024lightrag} to address this and adopt an LLM-based multi-dimensional comparison method (including comprehensiveness, diversity, empowerment, and overall).
% 
We employ a robust LLM, specifically GPT-4o-mini, to rank each baseline against our method.
% 
Figure \ref{fig:eval_summary} shows the evaluation prompt we use.



\begin{figure*}[t] 
\begin{AIbox}{Prompt for generating abstract questions}
{\bf Prompt:} \\
{%\scriptsize
Given the following description of a dataset:

\{description\}

Please identify 5 potential users who would engage with this dataset. For each user, list 5 tasks they would perform with this dataset. Then, for each (user, task) combination, generate 5 questions that require a high-level understanding of the entire dataset.

Output the results in the following structure:
\begin{description}
    \item[- User 1: \text{[user description]}]
    \begin{description}
        \item \item[- Task 1: \text{[task description]}]
        \begin{description}  
            \item \item  - Question 1:
            \item  - Question 2:
            \item  - Question 3:
            \item  - Question 4:
            \item  - Question 5:
        \end{description}  
       \item[- Task 2: \text{[task description]}]
            \item ...
       \item[- Task 5: \text{[task description]}]
    \end{description}   
\end{description}    

\begin{description}
    \item[- User 2: \text{[user description]}]
        \item...
\end{description}

\begin{description}
    \item[- User 5: \text{[user description]}]
        \item...
\end{description} 
    
    Note that there are 5 users and 5 tasks for each user, resulting in 25 tasks in total. Each task should have 5 questions, resulting in 125 questions in total.
    The Output should present the whole tasks and questions for each user.
    
    Output:


}

\end{AIbox} 
\caption{The prompt for generating abstract questions.}
\label{fig:prompt_summary}
\end{figure*}



\begin{figure*}[t] 
\begin{AIbox}{Prompt for LLM-based multi-dimensional comparison}
{\bf Prompt:} \\
{%\scriptsize
You will evaluate two answers to the same question based on three criteria: {\bf Comprehensiveness}, {\bf Diversity}, {\bf Empowerment}, and {\bf Directness}.
\begin{itemize}
    \item Comprehensiveness: How much detail does the answer provide to cover all aspects and details of the question?
    \item Diversity: How varied and rich is the answer in providing different perspectives and insights on the question?
    \item Empowerment: How well does the answer help the reader understand and make informed judgments about the topic?
    \item Directness: How specifically and clearly does the answer address the question?
\end{itemize}

For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these four categories.

Here is the {\bf question}: 
\begin{verbatim}
    Question: {query}
\end{verbatim}

Here are the two answers:
\begin{verbatim}
    Answer 1: {answer1}
    Answer 2: {answer2}
\end{verbatim}

Evaluate both answers using the four criteria listed above and provide detailed explanations for each criterion. Output your evaluation in the following JSON format:
\begin{verbatim}
{
    "Comprehensiveness": {
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide one sentence explanation here]"
    },
    "Diversity": {
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide one sentence explanation here]"
    },
    "Empowerment": {
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide one sentence explanation here]"
    },
    "Overall Winner": {
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Briefly summarize why this answer is the overall winner]"
    }
}
\end{verbatim}

Output:

}

\end{AIbox} 
\caption{The prompt for the evaluation of abstract QA.}
\label{fig:eval_summary}
\end{figure*}




$\bullet$ \textbf{Metrics of community quality. } We select the following metrics to evaluate the quality of the community:
\begin{enumerate}
    \item \textit{Calinski-Harabasz Index (CHI)} \cite{calinski1974CHIndex}: A higher value of CHI indicates better clustering results because it means that the data points are more spread out between clusters than they are within clusters. It is an internal evaluation metric where the assessment of the clustering quality is based solely on the dataset and the clustering results and not on external ground-truth labels. The CHI is calculated by between-cluster separation and within-cluster dispersion:
    \begin{equation}
    CHI = \frac{N-C}{C-1}\frac{\sum^{C}_{i=1}n_i||c_i-c||^2}{\sum^{C}_{i=1}\sum_{x \in C_i}||x-c_i||^2}.
    \end{equation}
    $N$ is the number of nodes. $C$ is the number of clusters.$n_i$ is the number of nodes in cluster $i$. $C_i$ is the $i-th$ cluster. $c_i$ is the centroid of cluster $C_i$. $c$ is the overall centroid of the datasets. $x$ is the feature of the target node.
    
    \item \textit{Cosine Similarity (Sim)} \cite{charikar2002similarity}: Cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. In this paper, for each cluster, we calculate the similarity between the centroid of this cluster and each node in this cluster:
    \begin{equation}
    Sim = \frac{1}{N}\sum^{C}_{i=1}\sum_{x \in C_i}Cosine(x, c_i).
    \end{equation}
    $N$ is the number of nodes. $C$ is the number of clusters. $C_i$ is the $i$-th cluster. $c_i$ is the centroid of cluster $C_i$. $x$ is the feature of the target node.
    \begin{equation}
    Cosine(x, y) = \frac{\sum_{i=1}^{n}x_i y_i}{\sqrt{\sum_{i=1}^{n}x_i^2}\sqrt{\sum_{i=1}^{n}y_i^2}}.
    \end{equation}
\end{enumerate}


% \subsection{Selected baselines}
% \label{sec:baselines}



\subsection{Implementation details}
\label{sec:imp}

% 
We implement our ArchRAG in Python, while C-HNSW is implemented in C++ and provides a Python interface for integration.
% 
We implement C-HNSW using the FAISS framework and employ the inner product metric to measure the proximity between two vectors. 
% 
All methods utilize 10 concurrent LLM calls, and to maintain consistency, other parallel computations in the method, such as embedding calculations, also use 10 concurrent threads. 
% 
All the experiments were conducted on a Linux operating system running on a machine with an Intel Xeon 2.0GHz CPU, 1024GB of memory, and 8 NVIDIA GeForce RTX A5000 GPUs, each with 24 GB of memory.
% 
Figure \ref{fig:prompt_afg} demonstrates the prompt used in Adaptive filtering-based generation.
% 
Please refer to our repository (\href{https://anonymous.4open.science/r/H-CAR-AG-1E0B}{anonymous.4open.science/r/H-CAR-AG-1E0B}) to view the detailed prompts.


\begin{figure*}[t] 
\begin{AIbox}{Prompt for Adaptive filtering-based generation}
{\bf Filger Prompt:} \\
{%\scriptsize
\# Role

You are a helpful assistant responding to questions about data in the tables provided.

\# Goal

Generate a response consisting of a list of key points that respond to the user's question, summarizing all relevant information in the input data tables.
You should use the data provided in the data tables below as the primary context for generating the response.
If you don't know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up.

Each key point in the response should have the following element:
\begin{itemize}
    \item Description: A comprehensive description of the point.
    \item Importance Score: An integer score between 0-100 that indicates how important the point is in answering the user's question. An 'I don't know' type of response should have a score of 0.
\end{itemize}

The response should be JSON formatted as follows:

\begin{verbatim}
{
    "points": [
        {"description": "Description of point 1 [Data: Reports (report ids)]", "score": score_value},
        {"description": "Description of point 2 [Data: Reports (report ids)]", "score": score_value},
    ]
}
\end{verbatim}
\# User Question
\begin{verbatim}
    {user_query}
\end{verbatim}

\# Data tables
\begin{verbatim}
    {context_data}
\end{verbatim}
Output:
}
\tcbline
{\bf Merge Prompt:} \\
{%\scriptsize
\# Role

You are a helpful assistant responding to questions and may use the provided data as a reference.

\# Goal

You should incorporate insights from all the reports from multiple analysts who focused on different parts of the dataset to support your answer. Please note that the provided information may contain inaccuracies or be unrelated. If the provided information does not address the question, please respond using what you know. namely,

\begin{itemize}
    \item A response that utilizes the provided information, ensuring that all irrelevant details from the analysts' reports are removed.
    \item A response to the user's query based on your existing knowledge when <Analyst Reports> is empty.
\end{itemize}

The final response should merge the relevant information into a comprehensive answer that clearly explains all key points and implications, tailored to the appropriate response length and format.
Note that the analysts' reports provided below are ranked in the {\it descending order of importance}. Do not include information where the supporting evidence for it is not provided.

\# Target response length and format 

\begin{verbatim}
    {response_format}
\end{verbatim}

\# User Question 

\begin{verbatim}
    {user_query}
\end{verbatim}

\# Analyst Reports 

\begin{verbatim}
    {report_data}
\end{verbatim}

Output:
}

\end{AIbox} 
\caption{The prompt for adaptive filtering-based generation.}
\label{fig:prompt_afg}
\end{figure*}




% \subsection{Datasets}
% \label{sec:app-datasets}

% We following RAPTOR \cite{sarthi2024raptor} modified the matrices used in NarrativeQA datasets, as following:
% \begin{itemize}
%     \item 
% \end{itemize}