\subsection{Setup}

\begin{table}[h]
    \centering
    \small
\caption{Datasets used in our experiments. BLEU, MET, and ROU denote BLEU-1, METEOT, and ROUGE-L F1.
% , respectively.
}
    \begin{tabular}{c|rrr}
        \toprule
        Dataset & Multihop-RAG & HotpotQA & NarrativeQA  \\
        \midrule
        Passages & 609 & 9,221 & 1,572 \\
        Tokens & 1,426,396  & 1,284,956 & 121,152,448 \\
        Nodes & 23,353 & 37,436 & 650,571  \\
        Edges & 30,716 & 30,758 & 679,426 \\
        Questions & 2,556  & 1,000 & 43,304  \\
        Metrics & Accuracy, Recall & Accuracy, Recall & BLEU, MET, ROU \\
        \bottomrule
    \end{tabular}
    \label{tab:dataset}
\end{table}

\paragraph{Datasets.}
We consider both specific and abstract QA tasks.
% 
For the specific QA task, we use the following datasets:
Multihop-RAG \cite{tang2024multihop}, HotpotQA \cite{yang2018hotpotqa}, and NarrativeQA \cite{kovcisky2018narrativeqa},
all of which are extensively utilized within the QA and Graph-based RAG research communities \cite{hu2022empowering,gutierrez2024hipporag,yao2022react,xu2024unsupervised,sarthi2024raptor,asai2023self}.
Multihop-RAG is designed to evaluate retrieval and reasoning across news articles in various categories.
% , including entertainment, business, sports, technology, health, and science.
% 
HotpotQA is a QA dataset featuring natural, multi-hop questions.
% with associated supporting facts.
% 
NarrativeQA is a dataset that comprises QA pairs based on the full texts of books and movie transcripts.
% 
Besides, we follow the GraphRAG \cite{edge2024local} method for the abstract QA task. 
% 
We reuse the Multihop-RAG corpus and prompt LLM to generate questions that convey a high-level understanding of dataset contents.
% 
The statistics of these datasets are reported in Table \ref{tab:dataset}.
% 
% Detailed dataset information can be found in the Appendix \ref{sec:app-datasets}.



\begin{table*}[ht]
    \centering    
    \caption{Performance comparison of different methods across various datasets for solving specific QA tasks. The best and second-best results are marked in bold and underlined, respectively.}
    % \begin{tabular}{ll cccc p{1cm} p{1.6cm} p{1.6cm}}
    \begin{tabular}{ll ccccccc}
        \toprule
        \multirow{2}{*}{Baseline Type} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Multihop-RAG} & \multicolumn{2}{c}{HotpotQA} & \multicolumn{3}{c}{NarrativeQA} \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-9}
        & &(Accuracy) & (Recall) & (Accuracy) & (Recall) & (BLEU-1) & (METEOR) & (ROUGE-L F1)\\
\midrule
\multirow{2}{*}{Inference-only} & {Zero-shot} & 47.7 & 23.6 & 28.0 & 31.8 & \underline{8.0} & 7.9 & \underline{8.6} \\
& {CoT} & 54.5 & 28.7 & 32.5 & 39.7 & 5.0 & 8.1 & 6.4 \\
\midrule
\multirow{2}{*}{Retrieval-only} 
& {BM25} & 37.6 & 19.4 & 49.7 & 53.6 & 2.0 & 4.9  & 2.8 \\
& {Vanilla RAG} & \underline{58.6} & \underline{31.4} & 50.6 & 56.1 & 2.0 & 4.9 & 2.8 \\
\midrule
\multirow{6}{*}{Graph-based RAG} 
& {HippoRAG} & 38.9 & 19.1 & \underline{51.3} & \underline{56.8} & 2.2 & 5.0 & 2.8 \\
& {LightRAG-Low} & 44.1 & 25.1 & 34.1 & 41.8 & 4.5 & 8.7 & 6.6 \\
& {LightRAG-High} & 48.5 & 28.7 & 25.6 & 33.3 & 4.4 & 8.1 & 6.1 \\
& {LightRAG-Hybrid} & 50.3 & 30.3 & 35.6 & 43.3 & 5.0 & \underline{9.4} & 7.0 \\
& {GraphRAG-Local} & 40.1 & 23.8 & 29.7 & 35.5 & 3.9 & 3.3 & 3.5 \\
& {GraphRAG-Global}& 45.9 & 28.4 & 33.5 & 42.6 & 2.9 & 3.7 & 3.3 \\
\midrule
Our proposed & {ArchRAG} & {\bf 68.8} & {\bf 37.2 }& {\bf 65.4} & {\bf 69.2} & {\bf 11.5} & {\bf 15.6} & {\bf 17.6} \\
        \bottomrule
    \end{tabular}
    \label{tab:docqa}
\end{table*}




\paragraph{Baselines.}
Our experiments consider three model configurations:
\begin{itemize}[left=0pt]
\item \textbf{Inference-only:} Using an LLM for direct question answering without any retrieval data, i.e., {Zero-Shot} and {CoT} \cite{kojima2022large}.

\item \textbf{Retrieval-only:} Retrieval models extract relevant chunks from all documents and use them as prompts for large models. We select strong and widely used retrieval models: {BM25} \cite{robertson1994some} and {vanilla RAG}.

\item \textbf{Graph-based RAG:} These methods leverage graph data during retrieval. We select {HippoRAG} \cite{gutierrez2024hipporag}, {GraphRAG}~\cite{edge2024local}, and {LightRAG}~\cite{guo2024lightrag}.
    %
Particularly, {GraphRAG} has two versions, i.e., {GraphRAG-Global} and {GraphRAG-Local}, which use global and local search methods respectively.
    % 
Similarly, {LightRAG} integrates local search, global search, and hybrid search, denoted by {LightRAG-Low}, {LightRAG-High}, and {LightRAG-Hybrid}, respectively.
\end{itemize}

In {GraphRAG-Global}, all communities below the selected level are first retrieved, and then the LLM is used to filter out irrelevant communities.
% 
This process can be viewed as utilizing the LLM as a {\it retriever} to find relevant communities within the corpus.
% 
According to the selected level of communities~\cite{edge2024local}, {GraphRAG-Global} can be further categorized into {C1} and {C2}, representing high-level and intermediate-level communities, respectively, with {C2} as the default.
% 
% We provide more details of the baseline methods in the appendix.
% \ref{sec:baselines}.

\paragraph{Metrics.}
For the specific QA tasks, we use Accuracy and Recall to evaluate performance on the first two datasets based on whether gold answers are included in the generations instead of strictly requiring exact matching, following \cite{schick2024toolformer,mallen2022not,asai2023self}.
% 
We also use the official metrics of BLEU, METEOR, and ROUGE-l F1 in the NarrativeQA dataset.
% 
For the abstract QA task, we follow prior work \cite{edge2024local} and adopt a head-to-head comparison approach using an LLM evaluator (GPT-4o).
% 
LLMs have demonstrated strong capabilities as evaluators of natural language generation, often achieving state-of-the-art or competitive results when compared to human judgments \cite{wang2023chatgpt, zheng2023judging}. 
% 
Overall, we utilize four evaluation dimensions: Comprehensiveness, Diversity, Empowerment, and Overall.
% 
Please see Appendix \ref{sec:metrics} for more details on metrics.
% \ref{sec:metrics}

\paragraph{Implementation details.}
We mainly use Llama 3.1-8B \cite{dubey2024llama} as the default LLM model and use nomic-embed-text \cite{nussbaum2024nomic} as the embedding model to encode all texts.
% 
We use KNN for graph augmentation and the weighted Leiden algorithm for community detection.
% 
For each retrieval item $k$, we search the same number of items at each layer, with $k=5$ as the default. Our codes are available at: \href{https://anonymous.4open.science/r/H-CAR-AG-1E0B}{https://anonymous.4open.science/r/H-CAR-AG-1E0B}.
% 
Additional details are provided in Appendix \ref{sec:imp}.