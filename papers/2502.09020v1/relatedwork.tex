\section{Related Works}
\label{sec::relatedWorks}

In this section, we review the related works on Scene Text Recognition, LLM-based OCR, and Event-based Vision. More related works can be found in the following surveys~\cite{long2021STRDsurvey, wang2023MMPTMsurvey, gallego2020eventsurvey} and paper list~\footnote{\url{https://github.com/Event-AHU/Event_Camera_in_Top_Conference}}. 


\subsection{Scene Text Recognition} 
Scene text recognition~\cite{wang2011end,shi2016end,liao2019scene,han2024spotlight} naturally involves both vision and language processing. 
% However, much of the previous research has concentrated on enhancing either visual feature extraction or language modeling independently. The challenge lies in effectively integrating these two modalities to reduce the reliance on one over the other. Recent approaches have sought to balance the contributions of both modalities, achieving a more robust and accurate recognition system across diverse scenarios. These methods aim to optimize the joint contributions of vision and language features, improving performance in varying contexts.
E2STR~\cite{zhao2024E2STR} enhances adaptability to diverse scenarios by introducing context-rich text sequences and applying a context training strategy, enabling flexibility in recognizing texts across different environments. 
% However, its complex inference process increases computational overhead, making it less suitable for real-time applications or resource-constrained environments. 
Guan et al. propose CCD~\cite{Guan2023CCD}, a self-supervised character-to-character distillation method, which learns robust text feature representations via a self-supervised segmentation module and flexible augmentation techniques. 
% While CCD improves recognition, it still struggles with extreme conditions like blurred or occluded text. 
SIGA~\cite{guan2023SIGA} optimizes self-supervised segmentation and implicit attention alignment to improve attention accuracy, though it is constrained when character-level annotations are insufficient. 
CDistNet~\cite{zheng2024cdistnet} incorporates visual and semantic positional embeddings into its transformer-based architecture, offering improvements but still facing difficulties with irregular or dense text layouts and complex backgrounds, limiting its generalization capacity. 
% PARSeq~\cite{bautista2022PARseq} presents a novel approach to scene text recognition by learning a shared-weight autoregressive language model ensemble through parallel language modeling. 
% % It unifies autoregressive and non-autoregressive inference while using bidirectional context for iterative optimization, enhancing recognition accuracy. However, this approach also introduces additional complexity in training and inference, which could lead to inefficiencies in deployment, particularly in real-time scenarios. 
% TPS++~\cite{zheng2023tps++} introduces an attention-enhanced transformation for scene text recognition, improving on traditional TPS by using content-aware correction to handle highly distorted text.
In contrast, another group of approaches focuses on using language models for iterative error correction in scene text recognition. These methods refine recognition results by correcting errors during inference, resulting in more robust and interpretable systems. Recent models, such as VOLTER~\cite{li2024volter}, BUSNet~\cite{Wei2024BUSNet}, MATRNet~\cite{na2022matrn}, LevOCR~\cite{da2022levocr}, and ABINet~\cite{fang2021ABINet}, integrate language models for this iterative correction. 
%% 
Inspired by these models, in this paper, we also exploit large language model based scene text recognition using an event camera. 




\subsection{LLM-based OCR} 
Recent advancements in scene text recognition have harnessed the power of large language models (LLMs)~\cite{zhao2023survey} to enhance text understanding and improve recognition accuracy. These models focus on optimizing the integration between visual features and linguistic context, offering significant improvements over traditional methods. TextMonkey~\cite{liu2024textmonkey} is a multimodal LLM optimized for text-centric tasks, providing enhanced interaction and interpretability through high-resolution inputs and location-aware responses. 
% With improved image feature filtering and extended task capabilities, TextMonkey demonstrates strong potential in downstream applications, including interactive app navigation. 
DocPedia~\cite{feng2023docpedia} is an advanced multimodal model designed for OCR-free document understanding, capable of processing high-resolution images directly in the frequency domain to capture both visual and textual information efficiently. 
Vary~\cite{wei2025vary} enhances the visual vocabulary of large vision-language models (LVLMs), specifically designed for tasks that require dense and fine-grained visual perception, such as document-level OCR and chart interpretation. 
% It excels in improving fine-grained perception and understanding, particularly for non-English contexts. 
mPLUG-DocOwl 1.5~\cite{hu2024mplug} introduces Unified Structure Learning, improving text-rich document image understanding in multimodal large language models (MLLMs). 
% Fox~\cite{liu2024focus} presents an effective pipeline and hybrid data strategy to enhance the fine-grained understanding of multi-page documents in large language models. 
% By focusing on document-level regions and integrating multi-visual vocabularies, it supports format- and page-agnostic comprehension. UReader~\cite{ye2023ureader} is a cost-effective, OCR-free visual language understanding model based on a multimodal LLM. It fine-tunes only 1.2\% of the parameters while achieving top performance in various visual language understanding tasks through joint fine-tuning and auxiliary tasks, without requiring downstream task fine-tuning. 
OCR2.0~\cite{wei2024OCR2.0} introduces an advanced end-to-end model with 580M parameters, leveraging large language models (LLMs) to handle a wide range of OCR tasks, from text to formulas and diagrams. 
% Its interactive OCR features, along with applications in dynamic resolution and multi-page OCR, highlight the power of LLMs in enhancing OCR performance.
However, each of these models has limitations when confronted with extreme conditions, such as poor lighting, low-resolution images, or complex noise. Under these challenging circumstances, maintaining high performance can become difficult.






\subsection{Event-based Vision}  
An event camera~\cite{gallego2020event}~\cite{jiang2024evcslr} is a vision sensor that captures dynamic scenes with microsecond-level time resolution by recording pixel-level brightness changes rather than fixed-frame images. 
% This enables exceptional performance in high-speed motion, low-light environments, and high-dynamic-range scenes. Event cameras offer low power consumption, high temporal resolution, and flexible responsiveness, making them widely used in fields such as autonomous driving, robotic vision, and medical imaging. 
In human activity recognition, ESTF~\cite{wang2024hardvs} leverages event camera data to capture high-speed and low-light motion by projecting event streams into spatial and temporal embeddings. 
% Using Transformer networks for feature fusion, ESTF enhances activity recognition accuracy in dynamic and challenging environments. 
For object tracking, EventVOT~\cite{wang2024eventvot} introduces the first large-scale high-resolution (1280$\times$720) event-based tracking dataset, containing 1141 videos across multiple categories such as pedestrians, vehicles, drones, and ping pong balls. 
% The paper proposes a novel hierarchical knowledge distillation framework, leveraging multimodal/multiview information during training to enable high-speed, low-latency event-only tracking during testing, addressing the limitations of existing tracking methods that either rely on aligned RGB and event data or suffer from noise and sparse resolution. 
Recurrent Vision Transformers (RVTs)~\cite{gehrig2023RVTs} leverage event cameras' strengths in capturing high temporal resolution and handling challenging lighting conditions to achieve robust detection in dynamic environments. 
% This approach provides new insights into harnessing event data for fast and efficient object detection, outperforming traditional methods in time-sensitive scenarios. 
SAFE~\cite{li2023semantic} introduces an innovative framework that integrates semantic labels, RGB frames, and event streams. By leveraging a large pre-trained vision-language model, this approach addresses the semantic gap and overcomes the limitations associated with small-scale backbone networks in traditional methods. However, event cameras have not been widely explored in scene text recognition. Leveraging their advantages, such as high temporal resolution and low power consumption, we propose a method to use event data for text recognition, aiming to improve performance in dynamic and challenging environments where traditional methods face difficulties.