

\section{Architecture}
\label{sec:arch}
This section introduces the architecture of \sys, %a platform that automates data extraction, linking, and high-level analysis in a user-friendly manner.
which comprises an Offline \scrapper\ %that transforms raw documents into a structured graph of \fact\ and \dimension\ nodes,
and an Online \surveyor. %that enables progressive exploration and LLM-powered report generation.

%We showcase a pre-extracted network of over 100,000 papers -- complete with their references -- demonstrating how \sys\ facilitates the paper-survey scenario, with focused exploration, analysis, and automatic generation of literature reviews. All code and datasets are open-sourced to encourage further research, adaptation, and community collaboration.

\subsection{Offline Scrapper}
\label{sec:scrapper}
%\sys’s \scrapper~ is built around \inspector\ and \navigator.

\stitle{Inspection}. Given a paper document as input, the \inspector\ processes it to produce a structured representation comprising \fact\ and \dimension\ nodes. The paper itself forms the \fact\ node, while its \dimension\ nodes are extracted through a Directed Acyclic Graph (DAG) of instructions. %This DAG may leverage LLMs to process raw text, with each subnode focusing on a specific dimension of the paper, such as its problem statement, challenges, or solutions.
%The definition of each subnode is tailored to the user’s specific needs. For dimensions that are easy to extract, such as ``abstract'', users can use rule-based methods like regular expressions.
%For dimensions requiring more sophisticated processing, LLMs can be configured individually, allowing users to balance cost and performance. For example, less complex dimensions can utilize affordable, locally deployed models~\cite{ollama}, while more complex dimensions may leverage advanced cloud-based models~\cite{gpt4o}. The LLM-based subnodes are built on a base functionality that extracts and chunks text from the PDF document, storing the chunks in a vector database. Using user-defined query for each node, only the most relevant chunks are retrieved and processed by the LLMs to extract the targeted dimensions. %A key technique we employ is inspired by the “Chain of Thought” (CoT) paradigm~\cite{wei2023cot}, where subnodes in the DAG can form chains to enable a context-aware extraction process. %This allows the configuration of a subnode to incorporate memory from upstream subnodes in the chain, enabling a context-aware extraction process.
Each subnode’s definition aligns with the user’s specific requirements. For simple dimensions (e.g., an ``abstract''), users can employ rule-based methods such as regular expressions. For more advanced tasks, the system supports individually configured LLM subnodes, allowing users to balance cost and performance. For instance, simpler processing can rely on local models~\cite{ollama}, whereas more complex extraction may involve sophisticated cloud-based models~\cite{gpt4o}. These LLM-based subnodes build on a common workflow that chunks PDF text, stores it in a vector database, and then retrieves only the most relevant chunks—based on user-defined queries, for final extraction by the LLM.

\begin{lstlisting}
  "dag": {
      "nodes": [ ...,
        {
          "name": "Abstract",
          "extract_from": { ... },  # the rule of extracting abstract
          "output_schema": { single_typed: ... }  # the output formats
        },
        {
          "name": "Challenges",
          "model" : { "name": "ollama/qwen2.5:7b", ... },
          "query": "Please summarize the challenges in this paper",
          "output_schema": { array_typed: ... }  # the output formats
        },
        {
          "name": "Solutions",
          "model" : { "name": "qwen-plus", ... },
          "query": "Please summarize the solutions in this paper \
              for addressing the above challenges.",
          "output_schema": { array_typed: ... }
        }, ...,
      ],
      "edges": [ ...,

        {"source": "Challenges", "target": "Solutions" }
      ]
  }
  \end{lstlisting}

A snippet of the \texttt{PaperInspection} DAG in \reffig{graphy} is shown above. The subnode \texttt{Abstract} is rule-based for extracting the paper’s ``abstract'', while two LLM-based subnodes form a chain: \texttt{Challenges} uses a locally deployed model (prefixed with “ollama/”) to identify challenges, and \texttt{Solutions} leverages a cloud-based model to extract solutions. Such chain formation allows the \texttt{Solutions} subnode to leverage the context provided by the \texttt{Challenges} subnode.


\stitle{Navigation}. The \navigator\ is responsible for establishing connections between \fact\ nodes,
and in this case particularly, linking papers through their references. Specifically,  a subnode can be
deployed in the above ``PaperInspection'' to extract references from a paper.
These references are then processed by the \navigator\ to fetch additional paper documents. Currently, we have implemented a \navigator\ to retrieve papers from Arxiv~\cite{arxiv}. For each reference, only those that can be matched and retrieved through the \navigator\ are retained. The corresponding documents are downloaded, and the \inspector\ workflow is repeated for these new papers.
In this demo, we only consider the \navigator\ that links papers to the referenced papers. The workflow is actually customizable. For instance, one could implement a \navigator\ to link papers to their associated GitHub repositories.   %Subsequently, an \inspector\ could process these repositories to extract \fact\ nodes representing GitHub projects and their corresponding \dimensions.
%We leave this as a future work as it must process a graph with different types of \fact\ nodes.


%It constructs a graph where each node represents a fact extracted from a paper, and each edge represents a relationship between two facts. The graph is built incrementally as new papers are processed, with each new fact being linked to existing facts based on similarity. The similarity between two facts is computed using a combination of textual similarity and semantic similarity, with the latter being calculated using pre-trained LLMs. The graph is then pruned to remove redundant or irrelevant facts, ensuring that only the most relevant information is retained.


\stitle{Graph Modelling}. The results of \inspector\ and \navigator\, as shown in \reffig{graphy}, naturally form a graph comprising \fact\ and \dimension\ nodes. Each \fact\ node represents a paper, while the outputs generated by subnodes in the \inspector\ form a set of \dimension\ nodes linked to their corresponding \fact\ node.
This graph is incrementally expanded as new papers are processed. Specifically, when a new \fact\ node $p_2$ is added, it is linked to an existing \fact\ node $p_1$ if $p_2$ is retrieved by the \navigator\ based on references extracted from $p_1$.

A notable feature of the \inspector\ is the customizable “output\_schema” field for each subnode in the DAG, which defines the schema (data fields and their data types) for the resulting \dimension\ nodes.
The output can be single-typed, such as abstract and title of the paper, which can be directly stored as attributes of the \fact\ node. Alternatively, array-typed outputs like challenges and solutions can be stored as separate \dimension\ nodes, each sharing the same schema.

\eat{
For now, there are tow types of outputs:
\begin{itemize}
	\item Single-typed outputs: These correspond to a single \dimension\ node, which may contain multiple data fields. Examples include fields such as ``abstract'', or ``metadata'' that contains all the metadata including ``title'' of a paper.
	\item Array-typed outputs: These represent multiple \dimension\ nodes, where each node shares the same schema. For example, a ``challenges'' subnode could produce an array of \dimension\ nodes, each representing a distinct challenge.
\end{itemize}

For Single-typed outputs like ``title'' or ``summary'', the information can be directly stored as properties of the \fact\ node while the property graph model is used, eliminating the need to create separate \dimension\ nodes.
}
