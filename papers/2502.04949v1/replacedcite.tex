\section{Related Work}
\label{sec:related_work}

\paragraph{Robust Neural SBI}
Robustness in neural SBI has become a rapidly growing area of research, with most approaches enhancing robustness for a single data set at the cost of amortization, e.g., due to additional MCMC runs or post-hoc corrections.
The majority of these approaches focuses on \textbf{Target 2} by incorporating an misspecification model ____, shifting observed summary statistics with low support ____, reducing the influence of unmodeled data shifts via generalized SBI ____, or using the single-data-set NPE-MMD variant previously discussed ____.
Focusing on \textbf{Target 1}, ____ highlighted the role of the approximator's latent space in domain shifts and proposed a latent space correction based on the observed data $\x_\tobs$.
Differently, ____ focus on \textbf{Target 3} by using an upfront ABC run to filter the part of the parameter space causing the highest discrepancy between $\x$ and $\x_\tobs$.

\paragraph{Robust ABI} 
In contrast, research on robustifying inference while retaining amortization has been sparse. 
Extending the scope of the training data via additive noise
____, such as the spike-and-slab noise approach of Noisy NPE ____, can be seen as a light modification to the simulator-implied likelihood as in \textbf{Target 2}, but requires strong assumptions about the misspecification-generating process.
____ also approach \textbf{Target 2} by framing domain shift as an optimal transport problem in summary space, but this requires \emph{observed} ``ground-truth'' parameters $\thetab_\tobs^*$ that are hard to obtain in most ABI settings.
____ provided evidence for the potential of NPE-MMD for robust ABI but focused their evaluation on a gravitational lensing application with synthetically added noise.
Finally, ____ proposed an efficient regularization technique that can increase robustness against adversarial attacks and thus attain more reliable performance under \textbf{Target 1}.