\PassOptionsToPackage{table, x11names}{xcolor} % required for tex live version 2023 (which is in turn required for arxiv)
\documentclass{article} % For LaTeX2e
\usepackage{arxiv_iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\input{preamble}

\title{Does Unsupervised Domain Adaptation Improve the Robustness of Amortized Bayesian Inference? A Systematic Evaluation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Lasse Elsemüller\textsuperscript{1}, Valentin Pratz\textsuperscript{1,2}, Mischa von Krause\textsuperscript{1}, Andreas Voss\textsuperscript{1}, \\ \textbf{Paul-Christian Bürkner\textsuperscript{3}  \& Stefan T. Radev\textsuperscript{4}} \\[5pt]
\textsuperscript{1} Heidelberg University, \textsuperscript{2} Zuse School ELIZA, \textsuperscript{3} TU Dortmund University, \\ \textsuperscript{4} Rensselaer Polytechnic Institute
}

% \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Neural networks are fragile when confronted with data that significantly deviates from their training distribution. 
This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. 
Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. 
However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. 
We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in both a controlled and a high-dimensional benchmark.
We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. 
However, the same alignment mechanism can lead to failures under prior misspecifications---a critical finding with practical consequences. 
Our results underscore the need for careful consideration of misspecification types when using UDA techniques to increase the robustness of ABI in practice.
\end{abstract}

\begin{tcolorbox}[boxsep=0mm,arc=0mm, left = 2mm, right = 2mm]
  This paper is work in progress and will be updated with additional content.
\end{tcolorbox}

\section{Introduction}
\label{sec:intro}

Synthetic data can augment numerous real-world applications \citep{savage2023synthetic}, including complex statistical workflows. In line with this perspective, amortized Bayesian inference \citep[ABI;][]{gershman2014amortized} redefines the classical sampling problem in Bayesian estimation by training generative neural networks on simulations derived from computational models \citep{burkner2023some}. The trained neural networks are then deployed to efficiently solve inference tasks as diverse as inferring evolutionary parameters \citep{avecilla2022neural} or gravitational waves \citep{pacilio2024simulation}. 

Evidently, the faithfulness of any simulation-based method rests on a critical assumption: That statistical patterns learned from simulated data can be extrapolated to real observations.
This assumption inevitably situates ABI in a domain-shift regime, exacerbated by the degree of potential mismatch between model simulations and reality.
As such, \textit{robustness to model misspecification} has been identified as the primary challenge for amortized methods in different fields \citep{dingeldein2024simulation, rainforth2024modern, cannon2022investigating}.

Unsupervised Domain Adaptation (UDA) studies the transfer of knowledge from a labeled source domain to an unlabeled target domain. It aims to mitigate domain shifts by aligning the \textit{embedding spaces} of the two domains. This property makes UDA a promising approach for addressing domain shifts in ABI, as the latter typically combines inference with embedding high-dimensional data into \textit{learned summary statistics} \citep{radev2020bayesflow, chan2018likelihood}. Indeed, recent research has underscored the critical role of \textit{in-distribution} summary statistics for achieving robust simulation-based inference \citep{schmitt2023detecting, frazier2024statistical, huang2024learning, wehenkel2024addressing}.

So far, only two pioneering studies \citep{swierc2024domain_npe, huang2024learning} have explored the potential of UDA methods for robustifying simulation-based inference. 
Both approaches align the embedding spaces by minimizing the maximum mean discrepancy \citep[MMD;][]{gretton2012} between simulated and observed summary statistics.
However, despite their promising results, several gaps remain. 
In particular, \citet{huang2024learning} did not make an explicit connection to UDA and explored a non-amortized approach.
While \citet{swierc2024domain_npe} acknowledged the connection to UDA, their work focused on a specific gravitational lensing application.
Both works mainly evaluated likelihood misspecification, leaving the behavior under prior shifts largely untapped.
Finally, the utility of the widely used UDA method \textit{domain-adversarial neural networks} \citep[DANN;][]{ganin2016domain} remains completely unexplored.
To address these gaps, we make the following contributions:
\begin{enumerate}
    \item We adapt domain-adversarial neural networks for neural posterior estimation (NPE) and evaluate their utility for robust amortized Bayesian inference.
    \item We categorize robust methods by inference targets, enabling a theoretical assessment of their strengths and limitations based on the source of misspecification.
    \item We evaluate the robustness of UDA-based ABI methods across multiple misspecification scenarios in two benchmarks, confirming the central role of the source of misspecification. 
\end{enumerate}

\section{Background}
\label{sec:background}

\paragraph{Amortized Bayesian Inference (ABI)} Amortized methods are a subset of the simulation-based inference \citep[SBI;][]{cranmer2020frontier} family. Their defining characteristic is the ability to perform zero-shot inference on model parameters $\thetab$ by learning a conditional distribution $q(\thetab \mid \x)$ that requires no further training or approximation algorithms (see Appendix \ref{app:abi_definition} for details). The \textit{amortized distribution} $q(\thetab \mid \x)$ is typically parameterized by a generative neural network that can generate random samples $\thetab \sim q(\thetab \mid \x)$, akin to a standard Markov chain Monte Carlo (MCMC) sampler, but orders of magnitude faster. Following a potentially expensive simulation-based training phase, the network can be queried with any \textit{new} data $\x_{\text{new}}$ to rapidly approximate the target distribution $p(\thetab \mid \x_{\text{new}})$.
Initially dismissed as inefficient compared to sequential methods optimized for a specific data set $\x_\tobs$ \citep{papamakarios2016fast}, amortized methods have since achieved notable successes across various domains \citep{burkner2023some, zammit2024neural}.

\paragraph{Unsupervised Domain Adaptation (UDA)} UDA is a subfield of transductive transfer learning where labeled data is only available for the source domain \smash{$\mathcal{D}_S = \{ (\x^i_S, \y^i_S) \}_{i=1}^{N_S}$}, distributed according to $p_S(\x, \y)$, but not for the target domain $\mathcal{D}_T = \{\x^i_T\}_{i=1}^{N_T}$, distributed according to $p_T(\x_T, \y_T)$ \citep{johansson2019support}.
UDA methods are based on the seminal theoretical works of \citet{ben2006analysis, ben2010theory}, who introduced generalization bounds for binary classification tasks that bound the risk in the target domain $R_T$ of a hypothesis $h \in \mathcal{H}$:
\begin{equation}\label{eq:uda_bound}
R_T(h) \leq R_S(h) + d_{\mathcal{H} \Delta \mathcal{H}} (p_S, p_T) + \lambda_\mathcal{H},
\end{equation}
where $R_S(h)$ is the source domain risk, $d_{\mathcal{H} \Delta \mathcal{H}} (p_S, p_T)$ measures the divergence between the domain distributions, and $\lambda_\mathcal{H}$ is the minimum combined risk of the optimal hypothesis, $\lambda_\mathcal{H} = \text{inf}_{h \in \mathcal{H}} | R_S(h) + R_T(h) |$ \citep{johansson2019support}.
This suggests that domain adaptation from $\mathcal{D}_S$ to $\mathcal{D}_T$ can be facilitated by minimizing the divergence between the marginal domain distributions. % \citep{redko2022survey}.
Although the domain distribution divergence cannot be reduced directly, the representation divergence $d(\phi(\x_S), \phi(\x_T))$ from a transformation $\phi: \mathcal{X} \to \mathcal{Z}$ can be readily minimized \citep{ben2006analysis}.
The core idea of UDA is thus twofold: (i) to minimize the source-domain error $R_S(h)$ during training, and (ii) to align the domain representations $\phi(\x_S)$ and $\phi(\x_T)$ to achieve \emph{domain-invariant} embeddings that generalize to the target domain. 
UDA methods include discrepancy-based approaches, which minimize statistical divergences like the MMD between source and target embeddings \citep{tzeng2014deep}, and, most prominently, adversarial-based approaches, such as Domain-Adversarial Neural Networks (DANN) \citep{ganin2016domain}, which learn domain-invariant embeddings via a minimax game between a feature extractor and a domain classifier.

The vast majority of UDA research, including its theoretical foundations, focuses on classification tasks \citep{redko2022survey, ben2010theory, liu2022deep}, with some works on regression tasks \citep{cortes2014domain, mansour2009domain} and only a few on generative tasks \citep{uppaal2024useful}.
More recently, UDA methods have been successfully applied to address simulation-to-reality (sim2real) problems \citep{ciprijanovic2020domain, swierc2023domain_classification} which seek to generalize patterns learned in a simulated source domain to a real-world target domain. 
These problems seem pertinent to any simulation-based method relying on data generation from imperfect models.
\begin{wrapfigure}[20]{r}{0.35\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/bayesian-denoising/ssdd-nrmse-sim-misspecification.pdf}
    \vspace{-6mm}
    \caption{\textbf{Experiment 2}: Summary space domain distance (SSDD) vs. normalized root mean squared error (NRMSE) for a scale shift. We observe a sweet spot of domain alignment without losing important information.}
    \label{fig:denoising/ssdd-nrmse}
\end{wrapfigure}

\paragraph{From Simulated to Real Domains}

The preceding discussion makes the connection between UDA and ABI immediately apparent: When the distance between the data distribution $p(\x_\tobs)$ and the model-implied distribution $p(\x) = \mathbb{E}_{p(\thetab)}\left[p(\x \mid \thetab)\right]$ is non-zero, the risk of extrapolation error for atypical data $\x_\tobs$ may increase.
Indeed, this behavior has been observed repeatedly in the context of SBI \citep{ward2022robust, schmitt2023detecting, huang2024learning, frazier2024statistical}.
In particular, \citet{frazier2024statistical} notes that ABI is especially prone to ``extrapolation bias'' for observed summary statistics $\phi(\x)$ that are far in the tails of the model-implied (i.e., prior predictive) density $p(\x)$.
The scenario can be equivalently stated by invoking the notion of a \textit{typical set} \citep{cover2012elements}, which denotes a subset of the support of $p(\x)$ where most of the probability mass concentrates around the entropy $H(p)$: 
\begin{equation}
A_{\epsilon} = \left\{ \x \in \mathcal{X} : \left| -\log p(\x) - H(p) \right| \leq \epsilon \right\}.
\end{equation}
Accordingly, for any problem-specific $\epsilon$, observed data $\x_\tobs \notin A_{\epsilon}$
%$\x_\tobs \sim p(\x_\tobs)$ 
may result in a biased posterior approximation $q(\thetab \mid \x_\tobs)$. As further noted in the comprehensive theoretical exposition by \citet{frazier2024statistical}, matching summary statistics $\phi(\x_\tobs)$ to the model-implied distribution of $\phi(\x)$ can be a useful heuristic for reducing extrapolation bias. 
This observation harmonizes with the UDA literature as well \citep{ben2010theory}.
Pre-asymptotically, the success of such matching depends on multiple factors, including (i) the type and hyperparameters of the matching method (see \autoref{fig:denoising/ssdd-nrmse}); (ii) the degree and nature of domain mismatch; (iii) the complexity of the learning problem; and (iv) even the choice of success metric.
Thus, a primary goal of this work is to systematically examine the effects of these factors on a variety of metrics that can index potential robustness gains. 

\section{Methods}
\label{sec:methods}

\subsection{Unsupervised Domain Adaptation for Amortized Bayesian Inference}

We start with the observation that model misspecification in ABI \citep{schmitt2023detecting}, and also more generally in neural SBI, can naturally be framed as an UDA problem: Ground-truth parameter values are only available for the simulated source domain \smash{$\mathcal{D} = \{(\x^i, \thetab^i)\}_{i=1}^{{N}}$} but not the observed target domain \smash{$\mathcal{D}_{\tobs} = \{\x_{\tobs}^i\}_{i=1}^{N_{\tobs}}$}.
In most machine learning applications, the collection of reliable ground-truth values is costly but feasible, whereas in SBI, collecting ground-truth parameter values $\thetab_{\tobs}$ of observed data is typically impossible.
A general optimization objective for NPE-UDA methods can be formulated by extending the standard negative log-posterior NPE objective:
\begin{align}\label{eq:npe_uda_loss}
    \mathcal{L}_{\text{NPE-UDA}}(q,\phi) :&= \mathcal{L}_{\text{NPE}} + \lambda \cdot \mathcal{L}_{\text{UDA}}  \\
     &= \mathbb{E}_{p(\thetab, \x)p(\x_{\tobs})}\big[- \log q(\thetab \given \phi(\x)) + \lambda \cdot d(\phi(\x), \phi(\x_{\tobs})) \big],
\end{align}
where $\lambda$ controls the regularization weight of the UDA loss  and $d(\cdot , \cdot)$ is a divergence measure that attains its global minimum if and only if $\phi(\x) = \phi(\x_{\tobs})$.

$\mathcal{L}_{\text{NPE-UDA}}$ incurs a trade-off between approximation performance in the simulated domain and domain divergence in the summary space, depending on the degree of domain mismatch.
In the well-specified case, $p(\x) = p(\x_{\tobs})$, $\mathcal{L}_{\text{NPE-UDA}}$ reduces to the standard NPE loss.
In the misspecified case, $p(\x) \neq p(\x_{\tobs})$, the summary network $\phi$ optimizes the summary statistics to both \textit{maximize information extraction} in the simulated domain and \textit{minimize domain shift} in summary space.
Thereby, the approximator $q(\thetab \given \phi(\x))$  needs to rely on domain-invariant information shared between the simulated and the observed domain.
The common UDA assumption that there exists a low-error hypothesis for both domains \citep[][cf.~Eq.\ref{eq:uda_bound}]{redko2022survey} suggests an upper bound on the amount of domain shift that can be handled by NPE-UDA methods.
Next, we formulate two NPE-UDA variants based on popular UDA methods with strong benchmark performance \citep{musgrave2021unsupervised}.

\subsection{NPE-MMD}

The maximum mean discrepancy \citep[MMD;][]{gretton2012} is a popular probability integral metric in SBI, since it can be efficiently estimated from a finite number of samples \citep{bischoff2024practical, schmitt2023detecting}.
For the same reason, it has been employed by various UDA works \citep{pan2010domain, tzeng2014deep, long2015learning} to measure the divergence between (transformed) samples from different domains.
We categorize the combination of NPE and UDA based on MMD, such as the variants of \citet{huang2024learning} and \citet{swierc2024domain_npe}, as NPE-MMD.
Choosing the MMD as $\mathcal{L}_{\text{UDA}}$, Eq.~\ref{eq:npe_uda_loss} becomes
\begin{equation}\label{eq:npe_mmd_loss}
    \mathcal{L}_{\text{NPE-MMD}}(q,\phi) := \mathbb{E}_{p(\thetab, \x)}\big[- \log q(\thetab \given \phi(\x))\big] + \lambda \cdot \text{MMD}^2 \big[\phi(\x)\,||\,\phi(\x_{\tobs})\big].
\end{equation}
The most important hyperparameter of NPE-MMD is the choice of kernel in the sample-based MMD estimator. In our experiments, we obtained good results with a sum of inverse multiquadric kernels \citep{ardizzone2018analyzing}, but other choices have been explored in the context of robust ABI as well, such as (sums of) Gaussian kernels \citep{schmitt2023detecting, huang2024learning}.

\subsection{NPE-DANN}

Domain-adversarial neural networks \citep[DANN;][]{ganin2016domain}, which have not been considered for NPE to date, introduce a domain classifier $\psi(\cdot)$ to reduce domain distance. Unlike typical adversarial training, which alternates between objectives, DANN achieves minimax optimization in a single-step update via a gradient reversal layer \citep{ganin2016domain}. This layer flips the gradient sign from the classifier to the feature extractor (e.g., summary network) $\phi$ during backpropagation, encouraging the feature extractor to generate less domain-specific summary statistics.
Similarly to NPE-MMD, DANN can be integrated into Eq.~\ref{eq:npe_uda_loss} to achieve NPE-DANN:
\begin{equation}\label{eq:npe_dann_loss}
    \mathcal{L}_{\text{NPE-DANN}}(q,\phi, \psi) := \mathbb{E}_{p(\thetab, \x)}\big[- \log q(\thetab \given \phi(\x))\big] + \lambda \cdot \mathcal{L}_D(\psi, \phi).
\end{equation}
The discriminator loss $\mathcal{L}_D$ is given by:
\begin{equation}
    \mathcal{L}_D(\psi, \phi) := -\mathbb{E}_{p(\x)} \big[\log(p(\psi(\phi(\x))) \big] - \mathbb{E}_{p(\x_{\tobs})}\big[\log (1 - p(\psi(\phi(\x_{\tobs}))) \big], 
\end{equation}
where $\psi$ is the domain classifier and the equation represents the binary cross-entropy loss on the domains, where a gradient reversal layer enables updating $\phi$ and $\psi$ in opposing directions.

While DANN is a powerful and popular UDA method \citep{zhou2022domain}, it has two important drawbacks.
First, the unstable training dynamics and convergence issues generally associated with adversarial learning can also occur with DANN \citep{sener2016learning, sun2019unsupervised}.
Second, adversarial training adds new hyperparameters, including the domain classifier architecture, an optional weight for gradient reversal balance \citep{ganin2016domain}, and stabilization techniques like label smoothing \citep{zhang2023free}.
Notably, although $\lambda$ is a shared hyperparameter in NPE-MMD and NPE-DANN, its effect on training dynamics will vary across applications due to differing $\mathcal{L}_{\text{UDA}}$ scales.


\subsection{What Is the Target of Robustness?} 
\label{met:what_goal}

To better understand the strengths and limitations of robust methods, including NPE-UDA, we suggest to distinguish between the following inference goals:

\begin{itemize}
    \item \textbf{Target 1}: The analytic (true) posterior $p(\thetab \mid \x_\tobs) \propto p(\x_\tobs \mid \thetab) \, p(\thetab)$ of the assumed probabilistic model given the observed data $\x_\tobs$. 
    \item \textbf{Target 2}: A posterior $p(\thetab \mid \tilde{\x}_\tobs) \propto p(\tilde{\x}_\tobs \mid \thetab) \, p(\thetab)$ of the assumed probabilistic model given \textit{adjusted data} $\tilde{\x}_\tobs$. 
    \item \textbf{Target 3}: A posterior $\tilde{p}(\thetab \mid \x_\tobs) \propto p(\x_\tobs \mid \thetab) \, \tilde{p}(\thetab)$ from an \textit{adjusted prior} $\tilde{p}(\thetab)$ given the observed data $\x_\tobs$.
\end{itemize}

\textbf{Target 1} is the most common target in Bayesian inference. 
Classical approximation methods such as MCMC almost always consider this target \citep{carpenter2017stan}. 
\textbf{Target 2}, an explicit deviation from the true posterior, is often targeted by methods that seek to improve the robustness of Bayesian inference.
Their goal is to reduce the influence of unmodeled phenomena in $\x_\tobs$, such as additional noise or external contamination, by approximating a target posterior $p(\thetab \mid \tilde{\x}_\tobs)$ based on denoised or uncontaminated data $\tilde{\x}_\tobs$.
This can be achieved either explicitly, by transforming $\x_\tobs$ into $\tilde{\x}_\tobs$, or implicitly, by using an \textit{adjusted (implicit) likelihood} $\tilde{p}(\x_\tobs \mid \thetab)$.

Since \textbf{Target 2} implies ignoring parts of the data that are in disagreement with the assumed probabilistic model, we expect corresponding methods to perform worse under prior misspecification: When a data-generating parameter $\thetab^*$ is impossible or highly unlikely under the assumed prior, \textit{ignoring conflicting information effectively reduces the amount of information available to counteract a poorly chosen prior}.
Generalized Bayes approaches \citep{bissiri2016general} also aim to reduce the influence of undesired parts of the data. They move away from the classical Bayes rule by replacing the likelihood with a loss function, which can be interpreted as an adjusted likelihood $\tilde{p}(\x_\tobs \mid \thetab)$ according to \textbf{Target 2}.
Lastly, \textbf{Target 3} can directly reduce the impact of prior misspecification by adjusting the prior based on $\x_\tobs$. However, compared to \textbf{Target 2}, it is more challenging to conceptualize the desired target priors $\tilde{p}(\thetab)$ and posteriors $\tilde{p}(\thetab \mid \x_\tobs)$ under model misspecification.

Given this categorization, what is the target of NPE-UDA?
Unsurprisingly, the classic NPE loss $\mathcal{L}_{\text{NPE}}$ aims at \textbf{Target 1}. In contrast, the additional $\mathcal{L}_{\text{UDA}}$ loss governs the alignment of the summary space between simulated and observed data, effectively adjusting the observed data seen by the model.
Thus, $\mathcal{L}_{\text{UDA}}$ introduces a shift towards \textbf{Target 2}, with $\lambda$ governing its relative importance compared to \textbf{Target 1}.
As hypothesized above, methods aiming at \textbf{Target 2} may not perform well under prior misspecification, which is confirmed for the NPE-UDA methods throughout our experiments.
While \citet{huang2024learning} suggested that their NPE-MMD variant is robust to prior mean shift, this conclusion was based on a single tested $\x_\tobs$ and our comprehensive evaluation could not replicate the result.

In line with our hypothesis and empirical results, \citet{huang2024learning} observed that increasing values of $\lambda$ encourage trading off the information content of $\x$ to minimize the domain distance in summary space, leading the posterior to converge to the assumed prior $p(\thetab)$.
Thus, the critical importance of the tunable hyperparameter $\lambda$ in UDA contexts \citep{zellinger2021balancing} directly translates to ABI applications, where $\lambda$ controls a trade-off between \textit{improving} approximation under likelihood misspecification and \textit{degrading} approximation under prior misspecification. 

\section{Related Work}
\label{sec:related_work}

\paragraph{Robust Neural SBI}
Robustness in neural SBI has become a rapidly growing area of research, with most approaches enhancing robustness for a single data set at the cost of amortization, e.g., due to additional MCMC runs or post-hoc corrections.
The majority of these approaches focuses on \textbf{Target 2} by incorporating an misspecification model \citep{ward2022robust}, shifting observed summary statistics with low support \citep{kelly2023misspecification}, reducing the influence of unmodeled data shifts via generalized SBI \citep{gao2023generalized}, or using the single-data-set NPE-MMD variant previously discussed \citep{huang2024learning}.
Focusing on \textbf{Target 1}, \citet{siahkoohi2023reliable} highlighted the role of the approximator's latent space in domain shifts and proposed a latent space correction based on the observed data $\x_\tobs$.
Differently, \citet{wang2024preconditioned} focus on \textbf{Target 3} by using an upfront ABC run to filter the part of the parameter space causing the highest discrepancy between $\x$ and $\x_\tobs$.

\paragraph{Robust ABI} 
In contrast, research on robustifying inference while retaining amortization has been sparse. 
Extending the scope of the training data via additive noise
\citep{cranmer2020frontier, bernaerts2023combined}, such as the spike-and-slab noise approach of Noisy NPE \citep[NNPE;][]{ward2022robust}, can be seen as a light modification to the simulator-implied likelihood as in \textbf{Target 2}, but requires strong assumptions about the misspecification-generating process.
\citet{wehenkel2024addressing} also approach \textbf{Target 2} by framing domain shift as an optimal transport problem in summary space, but this requires \emph{observed} ``ground-truth'' parameters $\thetab_\tobs^*$ that are hard to obtain in most ABI settings.
\citet{swierc2024domain_npe} provided evidence for the potential of NPE-MMD for robust ABI but focused their evaluation on a gravitational lensing application with synthetically added noise.
Finally, \citet{gloeckler2023adversarial} proposed an efficient regularization technique that can increase robustness against adversarial attacks and thus attain more reliable performance under \textbf{Target 1}.

\section{Experiments}
\label{sec:experiments}

The previous two NPE-MMD approaches mainly evaluated performance against contamination \citep{huang2024learning}, where a fraction of the sample is replaced with corrupted observations \citep{huber1981robust}, or noise applied to all observations \citep{swierc2024domain_npe}.
Both of these scenarios are cases of likelihood misspecification where ignoring noise is desirable (\textbf{Target 2}).
To obtain a clearer insight into the strengths and limitations of NPE-UDA methods, we systematically evaluate the behavior of NPE-MMD and NPE-DANN in various likelihood/data and prior misspecification scenarios.

Experiment \ref{exp:gaussian} starts with a simple and controllable setting that allows for comparing the NPE-UDA methods not only against standard NPE and NNPE \citep{ward2022robust}, but also the analytic posterior under \textbf{Target 1}.
Afterwards, Experiment \ref{exp:denoising} explores whether the results can be replicated in a challenging setting with a high-dimensional parameter space.
We evaluate a range of metrics: (i) normalized root mean squared error (NRMSE) to measure approximation error; (ii) expected calibration error (ECE) to measure probabilistic calibration; (iii) posterior contraction (PC) to measure information gain from prior to posterior; (iv) posterior predictive distance (PPD; via MMD), the only performance metric obtainable on $\x_\tobs$ in real-world settings; and (v) summary space domain distance (SSDD; via MMD) to measure domain alignment.
Please refer to the Appendix for details concerning the metrics (\ref{app:metrics}), Experiment \ref{exp:gaussian} (\ref{app:exp_gaussian}), and Experiment \ref{exp:denoising}  (\ref{app:bayesian_denoising}).

\subsection{Experiment 1 - 2D Gaussian Means: Controlled Setting}\label{exp:gaussian}

\paragraph{Setup} Inspired by \citet{schmitt2023detecting}, we set the stage with a simple and controllable task of modeling the means of a $2$-dimensional Gaussian model, enabling the comparison with an analytic posterior.
The well-specified setting uses a multivariate standard normal prior and an identity likelihood covariance matrix.
We evaluate performance under increasing misspecification in two prior misspecification scenarios -- prior location ${\boldsymbol{\mu_0}}$ and prior scale ${\boldsymbol{\Sigma_0}} = \tau_0 \mI_2$ -- and two likelihood misspecification scenarios -- likelihood scale ${\boldsymbol{\Sigma}} = \tau \mI_2$ and contamination $\epsilon$ (see \autoref{tab:exp1_overview}).
For the contamination misspecification, a fraction $\epsilon$ of the observations is replaced by negative and positive vectors of the constant $c=1.5$ to obtain atypical observations without affecting overall location or scale.
Each simulated data set contains $M = 100$ exchangeable observations.
All methods train on $N = 49\,920$ well-specified data sets, with the NPE-UDA methods additionally exposed to $N_\tobs = 49\,920$ observed data sets, and are evaluated on $N_\tobs = 100$ observed data sets (unseen by NPE-UDA methods).

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/gaussian/observed_data_gen_params_jet_weight_1.0.pdf}
    \vspace*{-7mm}
    \caption{\textbf{Experiment 1}. Metrics of the methods in all misspecification scenarios (columns), averaged across $3$ runs. 
    The first row shows the well-specified setting, with misspecification increasing from top to bottom within each column. 
    Metric values are centered at $0$ and normalized by each column's/scenario's maximum value, which is displayed below the metric name at the border of each radar plot.
    Lower values indicate better performance for all metrics but SSDD.
    $1 -$ PC $=$ $1 -$ Posterior Contraction.
    NRMSE = Normalized Root Mean Squared Error.
    SSDD = Summary Space Domain Distance (MMD; not applicable for Analytic Posterior).
    PPD = Posterior Predictive Distance (MMD).
    ECE = Expected Calibration Error.
    NPE-UDA methods fail under prior misspecification but can be advantageous under likelihood misspecification, especially contamination.
    }
\label{fig:gaussian/observed_data_gen_params_jet_weight_1}
\end{figure*}

\paragraph{Results} \autoref{fig:gaussian/observed_data_gen_params_jet_weight_1} displays the results for all misspecification scenarios.
We invert the meaning of the posterior contraction (PC) metric so that lower means better for all metrics and the performance of a method can mostly be inferred from its area. 
All methods perform reliably well in the well-specified case (first row), whereas we observe distinct but consistent patterns for the different methods under increasing mismatch.
In the prior misspecification scenarios, all NPE methods perform poorly compared to the analytic posterior, but the NPE-UDA methods perform especially poorly in terms of NRMSE, PPD, and ECE.

In the likelihood scale misspecification scenario, the NPE methods are less sensitive to the misspecification than the analytic posterior.
NPE-MMD successfully aligns the summary space between domains, leading to a slightly lower NRMSE but high ECE compared to NPE.
NPE-DANN, on the other hand, fails to align the summary space for both misspecification levels, which translates to poor performance.
This \textit{drastic failure in the observed domain is not detectable in the simulated domain}, where all methods, even NPE-DANN in the $\tau = 20$ scenario, perform well (see \autoref{fig:gaussian/simulated_data_gen_params_jet_weight_1}).
While NNPE mostly performs similarly to standard NPE, with lower posterior contraction resulting from its noisier training in most settings, it achieves better calibration and a slightly lower NRMSE than NPE for likelihood scale shifts.
In the contamination scenario, deviating from the true posterior via \textbf{Target 2} enables NPE-MMD and NPE-DANN to excel, achieving much lower NRMSE and ECE than NPE, NNPE, and even the analytic posterior.

Finally, PPD reliably detects NPE-UDA failures under prior misspecification.
We suspect that its indifference to likelihood misspecifications is due to the structure of the simple Gaussian mean model.
Nevertheless, SSDD reliably indicates NPE-UDA alignment failures, which translate to poor approximation performance in likelihood misspecification scenarios.

\subsection{Experiment 2 - Bayesian Denoising: High-Dimensional Setting}\label{exp:denoising}

\paragraph{Setup} We base our high-dimensional benchmark on a noisy camera model, similar to \citet{ramesh2022gatsbi}. The parameter vector $\thetab \in\mathbb{R}^{256}$ represents the original image, whereas the observation $\x\in\mathbb{R}^{256}$ is a blurred version of the original image generated by the noisy camera. 
The training data set consists of $N = 50\,000$ images from the MNIST data set \citep{lecun1998mnist}, downscaled to $16 \times 16$ pixels for compatibility with the USPS data set \citep{hull1994usps}. We test four different misspecification scenarios (see \autoref{tab:denoising_samples} for examples). In the prior misspecification scenario, we keep the settings of the noisy camera model constant but use images from the USPS data set \citep{hull1994usps}. While both data sets contain digits, the USPS data set features smaller margins, giving the priors different support. 
In the likelihood scale scenario, we increase the amount of blur. In the noise contamination scenario, we replace $10\%$ of the pixels with salt-and-pepper noise (i.e., set them to black or white). In the row contamination scenario, we randomly set two rows ($12.5\%$ of the pixels) of each observation to black.
We evaluate the performance on $N_\tobs = 1,000$ observed data sets (seen by NPE-UDA methods during training).

\paragraph{Results}

\begin{table*}[t]
    \scriptsize
    \setlength\tabcolsep{2pt} % default value: 6pt
    \centering
    \input{tables/bayesian-denoising/metrics_main}
    \caption{\textbf{Experiment 2}. Metrics of the methods in all misspecification scenarios, averaged across $3$ runs. NRMSE: Normalized Root Mean Squared Error (lower is better). PPD: Posterior Predictive Distance (NRMSE) (lower is better). SSDD: Summary Space Domain Distance (MMD). Lower values indicate better summary space alignment, but too much alignment (i.e., vanishing SSDD) can lead to an uninformative summary space (e.g., NPE-MMD with $\lambda=1.00$).}
    \label{tab:denoising_metrics}
\end{table*}

\autoref{tab:denoising_metrics} displays an overview of the metrics in all scenarios. \autoref{tab:denoising_samples} shows samples from the best run (lowest NRMSE) for each scenario and method.
We observe worse approximations for all robust methods compared to NPE in the prior misspecification scenario, even though the summary space domain distance (SSDD) is strongly diminished for NPE-DANN and NPE-MMD.
This is somewhat expected, as performance improvements would also require an adaptation of the approximator, which cannot be induced by the methods tested here.
NNPE is beneficial in the two contamination scenarios, whereas NPE-DANN and NPE-MMD improve performance in all three likelihood misspecification scenarios.
The results highlight the differences between the robust methods: While NNPE mainly excels in the noise contamination scenario, where its misspecification model matches the domain shift, NPE-UDA methods effectively adapt to different likelihood shifts.

Overall, NPE-DANN achieves good performance over a wide range of $\lambda$ values. In contrast, NPE-MMD is prone to overregularizing the summary space, leading to a complete loss of information in the summary space (see also \autoref{fig:denoising/ssdd-nrmse}). This is indicated by a huge drop in performance and vanishing SSDD. 
We found NPE-MMD highly sensitive to the chosen batch size, which we had to increase from $32$ to $128$ to achieve acceptable results. 
Thus, increasing the batch size and reducing $\lambda$ can counteract excessive regularization in higher-dimensional problems.
Finally, the close correspondence between the NRMSE and PPD metrics confirms our hypothesis that the limited diagnostic power of PPD in the likelihood misspecification scenarios of Experiment \ref{exp:gaussian} was caused by the limited informativeness of data simulated from a simple Gaussian model.

\begin{table*}[h]
    \scriptsize
    \setlength\tabcolsep{1pt} % default value: 6pt
    \centering
    \input{tables/bayesian-denoising/samples}
    \caption{\textbf{Experiment 2}. Parameters, observations, and samples from the run with the lowest NRMSE for each scenario and method. \textit{Train} shows a sample from the parameters $\thetab$ of the training distribution and the corresponding observations $\x_\tobs$. The observations are identical for NPE, NPE-DANN, and NPE-MMD, whereas spike-and-slab noise is added for NNPE. The similarity to the observations in the \textit{Contamination (Noise)} scenario explains the good performance of NNPE.}
    \label{tab:denoising_samples}
\end{table*}

\section{Conclusion}
\label{sec:conclusion}

We argued that introducing UDA to NPE methods shifts the inference goal from the standard analytic posterior $p(\thetab \mid \x_\tobs)$ to another posterior $p(\thetab \mid \tilde{\x}_\tobs)$ based on adjusted data $\tilde{\x}_\tobs$.
This implies potential robustness gains under likelihood misspecification, where ignoring unmodeled phenomena in the observed data can be desirable, but reduces the amount of information available to counteract prior misspecification.
We consistently found these patterns throughout our systematic evaluations for both the existing NPE-MMD and a new NPE-DANN method.
Whereas NPE-DANN was less stable than NPE-MMD in the low-dimensional benchmark, it excelled in the likelihood misspecification scenarios of the high-dimensional benchmark.
Lastly, we confirmed the existence of an application-specific optimal amount of UDA regularization \citep{zellinger2021balancing} in the NPE context.
In light of our results, we propose a two-step approach for diagnosing NPE-UDA methods in real-world applications relative to an NPE baseline: (1) assessing summary space alignment via summary space domain distance and (2) evaluating whether this alignment improves fit to empirical data via posterior predictive distance.

\FloatBarrier

\subsubsection*{Acknowledgments}
The authors acknowledge support by the state of Baden-Württemberg through bwHPC and the German Research Foundation (DFG) through grant INST 35/1597-1 FUGG. 
Additionally, LE was supported by a grant from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation; GRK 2277; project number 310365261) to the research training group Statistical Modeling in Psychology (SMiP).
VP acknowledges funding by the German Academic Exchange Service (DAAD) and the German Federal Ministry of Education and Research (BMBF) through the funding programme ``Konrad Zuse Schools of Excellence in Artificial Intelligence'' (ELIZA). 
PB acknowledges support of DFG Project 528702768 and DFG Collaborative Research Center 391 (Spatio-Temporal Statistics for the Transition of Energy and Transport) -- 520388526.



\subsubsection*{Author Contributions}
L.E. and S.T.R. conceived the initial idea, L.E., P.C.B., and S.T.R. developed the methodology, and A.V. and S.T.R. supervised the project. L.E., V.P., and M.V.K. implemented the experiments and wrote the experiment sections. L.E., P.C.B., and S.T.R. wrote the rest of the initial manuscript draft, which all authors reviewed and refined.

\bibliography{references}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\onecolumn

\appendix

\begin{center}
{\huge APPENDIX}\\[24pt]
\end{center}

\section{Theoretical details}
% Renew figure and table numbering - has to be adjusted manually for each section!
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{A.\arabic{table}}
\setcounter{table}{0}

\subsection{Defining Amortized Bayesian Inference} \label{app:abi_definition}

The term ``amortized'' has been used inconsistently throughout the literature, often denoting different generalization scopes. To clarify this concept for the discussion within this work, we offer the following definition:

\begin{definition} Let $\mathcal{A}$ denote a learner, $\y$ denote target variables, $\x$ represent input data, and $\bs{c}$ denote context variables. A learner $\y \sim \mathcal{A}(\x, \bs{c})$ is an \emph{amortized Bayesian approximator} of a target quantity $\y$ with respect to a joint distribution $p(\x, \y, \bs{c})$ if it can directly approximate $p(\y \mid \x, \bs{c})$ for any $(\x, \bs{c}) \sim p(\x, \bs{c})$ without requiring further training or additional approximation algorithms. 
\end{definition}

By this definition, sequential methods that necessitate further training for new data \citep{papamakarios2016fast, glockler2022variational} are not considered amortized. Similarly, neural likelihood estimation \citep[NLE;][]{papamakarios2016fast} and neural ratio estimation (NRE) \citep{hermans2020likelihood} which depend on MCMC algorithms do not qualify as amortized. In contrast, recent transformer-based \citep{gloeckler2024all} or context-aware methods \citep{elsemuller2024sensitivity} clearly fall within the scope of amortized neural posterior estimation (NPE).

\section{Experimental Details}
% Renew figure and table numbering - has to be adjusted manually for each section!
\renewcommand{\thefigure}{B\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{B.\arabic{table}}
\setcounter{table}{0}

Since the analytic posterior is only obtainable in Experiment \ref{exp:gaussian}, we measure performance relative to the data-generating parameters $\thetab^*$ to enable a direct comparison between the experiments. 
For likelihood misspecification settings, $\thetab^*$ is closely related to the posterior $p(\thetab \mid \tilde{\x}_\tobs)$ based on adjusted (e.g., decontaminated) data $\tilde{\x}_\tobs$ (\textbf{Target 2}).
Thus, the NPE-UDA posterior approximations being closer to $\thetab^*$ than the analytic posterior $p(\thetab \mid \x)$ in the contamination scenario of Experiment \ref{exp:gaussian} indicates that NPE-UDA methods indeed focus \textbf{Target 2}. 

In all experiments, we build upon the \texttt{BayesFlow} Python library for amortized Bayesian workflows using generative neural networks \citep{radev2023bayesflow}.

\subsection{Method Details}

\paragraph{NNPE} We implemented NNPE following the original implementation of \citet{ward2022robust} at \url{https://github.com/danielward27/rnpe}, who used a spike scale of $\sigma = 0.01$ and a slab scale of $\tau = 0.25$ for all experiments.
Whether spike (standard normal) or slab (standard Cauchy) noise is applied to a simulated data point is determined by sampling from a Bernoulli distribution with $p = 0.5$.

\paragraph{Sensitivities of NPE-UDA} In both experiments, we found the typical UDA phenomenon of sensitivity to higher learning rates \citep{perone2019unsupervised} in the form of unstable learning dynamics such as exploding gradients.
We also found sensitivity to short training times, suggesting that finding a stable optimum for the two-component NPE-UDA loss in Eq.~\ref{eq:npe_uda_loss} requires more gradient updates than usual.

\paragraph{Computational Cost of NPE-UDA} Since the NPE-UDA methods operate in the compressed summary space, the runtime increase during training is minimal compared to NPE.
For example, despite the relatively large ($32$-dimensional) summary space in Experiment \ref{exp:denoising}, NPE and NPE-MMD took $12 \text{s} / \text{epoch}$ and NPE-DANN $13 \text{s} / \text{epoch}$ during GPU training on a cluster.

\subsection{Metrics}\label{app:metrics}
We compute multiple metrics that measure the performance based on the approximation performance of $J$ data-generating parameters $\{\theta^*_{j}\}_{j=1}^J$ via $S$ posterior samples (we forego the $\text{obs}$ notation where possible for brevity here).
Depending on the metric, results are averaged across the $J$ parameters and/or $N$ observed data sets.

Normalized root mean squared error (NRMSE):
\begin{equation}
\text{NRMSE} = \frac{1}{N} \sum_{n=1}^{N} \left[ \frac{1}{J} \sum_{j=1}^{J} \frac{\sqrt{\frac{1}{S} \sum_{s=1}^{S} (\theta^*_{j,n} - \hat{\theta}^{(s)}_{j,n})^2}}{\max(\theta^*_{j}) - \min(\theta^*_{j})} \right].
\end{equation}

Expected calibration error (ECE) via the fraction of ground-truth inliers for $R$ linearly spaced $\alpha$-confidence intervals 
in $[0.005, 0.995]$ \citep{ardizzone2018analyzing, radev2020bayesflow}:
\begin{equation}
\text{ECE} = \frac{1}{J} \sum_{j=1}^{J} \text{median}_{r=1}^{R}  \left( \left|  \frac{1}{N} \sum_{n=1}^{N} \mathbb{I} \Big\{ Q_{\frac{1-\alpha_r}{2}}(\hat{\theta}^{(n)}_{j}) \leq \theta^*_{j} \leq Q_{1-\frac{1-\alpha_r}{2}}(\hat{\theta}^{(n)}_{j}) \Big\} - \alpha_r \right| \right),
\end{equation}
where $\text{median}_{r=1}^{R}$ represents the median fraction of inliers across the $R = 20$ credible intervals and \smash{$Q_k(\hat{\theta}^{(n)}_{j})$} represents the $k$-th quantile of the posterior samples for the $n$-th data set.
We estimate the ECE on all test data sets via the median calibration error of $R = 20$ linearly spaced credible intervals, averaged across $J$ model parameters.

Posterior contraction (PC) relative to the prior distribution \citep{betancourt2018calibrating}:

\begin{equation}
\text{PC} = \frac{1}{N} \sum_{n=1}^{N} \left[ \frac{1}{J} \sum_{j=1}^{J} \left( 1 - \frac{\text{Var}(\hat{\theta}^{(s)}_{j,n})}{\text{Var}(\theta^*_{j,n})} \right) \right].
\end{equation}

Posterior predictive distance (PPD):
\begin{equation}
\text{PPD} = \frac{1}{N} \sum_{n=1}^{N} \left[  \frac{1}{S} \sum_{s=1}^{S} d \left( \x_n,  \hat{\x}_n^{(s)} \right) \right].
\end{equation}
where $\hat{\x}^{(s)}$ represents a re-simulation based on a posterior sample of all parameters, $\hat{\thetab}^{(s)}$, and we use the MMD (Experiment \ref{exp:gaussian}) or NRMSE (Experiment \ref{exp:denoising}) for  $d(\cdot , \cdot)$.

The summary space domain distance (SSDD), which does not measure approximation performance but the degree of summary space alignment, is based on the biased sample-based $\widehat{\text{MMD}}^2$ estimator \citep{gretton2012}:
\begin{equation}
\text{SSDD} = \frac{1}{N} \sum_{n=1}^{N} \widehat{\text{MMD}}^2 \big[\{\phi(\x_n)\}\,||\, \{\phi(\x^{\tobs}_n)\}\big],
\end{equation}
where $\{\phi(\x_n)\}$ and $\{\phi(\x^{\tobs}_n)\}$ are sets of summary statistics over which the expectations are approximated.

\subsection{Experiment 1 - 2D Gaussian Means} \label{app:exp_gaussian}

\input{tables/gaussian/exp1_overview}

\autoref{tab:exp1_overview} provides an overview of the well-specified setting and the different misspecification scenarios inspired by \citet{schmitt2023detecting}

\subsubsection{Network Architecture}

We use a deep set architecture \citep{zaheer2017deep} for the summary network $\phi$, compressing the input to $4$-dimensional summary statistics.
For the generative inference network of the approximator $q$, we use an affine coupling flow architecture \citep{ardizzone2021conditional, kingma2018glow} with $3$ coupling layers.

For the domain classifier $\psi$ in NPE-DANN, we use a standard feedforward network with $2$ hidden layers of width $256$.
We do not use label smoothing or weight the gradient reversal balance.

\subsubsection{Training and Evaluation Details}

To rule out any overfitting effects, we use an online training approach where new data from the simulated and the observed domain is simulated at each training step, resulting in overall simulation budgets of $N = 49,920$ and $N_\tobs = 49,920$. Since we use a batch size of $32$, also for the observed data in NPE-UDA methods, online training amounts to $1560$ mini-batches and thus gradient updates.
We use an Adam optimizer with an initial learning rate of $5\cdot 10^{-4}$ and cosine decay. 

We use $S = 100$ posterior samples per method to limit the computational cost of the PPD calculation, where an MMD distance is calculated for each re-simulated data set and thus posterior sample. 
While we did not observe different result patterns with higher values of $S$, we will increase $S$ in the full version of this work. 

\subsubsection{Additional Results}

We provide additional results iterating over three factors: (i) performance in the simulated vs. the observed domain, (ii) $\lambda = [0.1, 1, 10]$, and (iii) comparison of the posterior approximations to the analytic posterior instead of the data-generating parameters $\thetab^*$.

\paragraph{Performance in the Simulated Domain} \autoref{fig:gaussian/simulated_data_gen_params_jet_weight_0.1}, \autoref{fig:gaussian/simulated_data_gen_params_jet_weight_1}, and \autoref{fig:gaussian/simulated_data_gen_params_jet_weight_10.0} show the performance in the simulated domain. 
Despite notable performance differences in the observed domain, all methods perform well in the simulated domain for the vast majority of settings, with the only exception being the failures of NPE-DANN for high regularization weights in \autoref{fig:gaussian/simulated_data_gen_params_jet_weight_10.0}.
NNPE performs worse in the simulated (noiseless) domain since it was optimized based on noisy training data.
Besides the NPE-DANN failures, we mostly do not observe a trade-off of the summary space alignment of the NPE-UDA methods. 
Only in the high regularization setting $\lambda = 10$, the ECE is systematically higher compared to NPE.

\paragraph{Performance in the Observed Domain} \autoref{fig:gaussian/observed_data_gen_params_jet_weight_0.1} and \autoref{fig:gaussian/observed_data_gen_params_jet_weight_10.0} confirm our finding of an application- and also method-specific $\lambda$ optimum: Whereas the difference of the NPE-UDA methods to NPE is often small for $\lambda = 0.1$, $\lambda = 10$ still leads to performance improvements of NPE-MMD in likelihood misspecification scenarios but renders NPE-DANN highly unstable when large domain shifts are present.

\paragraph{Performance Compared to the Analytic Posterior} \autoref{fig:gaussian/observed_analytical_posterior_jet_weight_0.1}, \autoref{fig:gaussian/observed_analytical_posterior_jet_weight_1}, and \autoref{fig:gaussian/observed_analytical_posterior_jet_weight_10.0} compare the different approximation algorithms to the analytic posterior under \textbf{Target 1}, also showing the clear separation between NPE and NNPE vs. the NPE-UDA methods as a function of $\lambda$.
The inference network latent distance (INLD) to its base distribution, a proxy of approximation quality \citep{siahkoohi2023reliable}, is closely related to a methods performance (compare for example \autoref{fig:gaussian/observed_data_gen_params_jet_weight_1} and \autoref{fig:gaussian/observed_analytical_posterior_jet_weight_1}).

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gaussian/simulated_data_gen_params_jet_weight_0.1.pdf}
    \vspace*{-7mm}
    \caption{\textbf{Experiment 1}: Performance metrics of the methods in all misspecification scenarios (columns) \textbf{on simulated (i.e., well-specified) data for $\mathbf{\lambda = 0.1}$ in NPE-MMD and NPE-DANN}, averaged across $3$ separate runs. 
    Lower values indicate better performance (for SSDD only for NPE-MMD and NPE-DANN).
    $1 -$ PC $=$ $1 -$ Posterior Contraction.
    NRMSE = Normalized Root Mean Squared Error.
    SSDD = Summary Space Domain Distance (MMD; not applicable for Analytic Posterior).
    PPD = Posterior Predictive Distance (MMD).
    ECE = Expected Calibration Error.
    }
    \label{fig:gaussian/simulated_data_gen_params_jet_weight_0.1}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gaussian/simulated_data_gen_params_jet_weight_1.0.pdf}
    \vspace*{-7mm}
    \caption{\textbf{Experiment 1}: Performance metrics of the methods in all misspecification scenarios (columns) \textbf{on simulated (i.e., well-specified) data for $\mathbf{\lambda = 1}$ in NPE-MMD and NPE-DANN}, averaged across $3$ separate runs. 
    Lower values indicate better performance (for SSDD only for NPE-MMD and NPE-DANN).
    $1 -$ PC $=$ $1 -$ Posterior Contraction.
    NRMSE = Normalized Root Mean Squared Error.
    SSDD = Summary Space Domain Distance (MMD; not applicable for Analytic Posterior).
    PPD = Posterior Predictive Distance (MMD).
    ECE = Expected Calibration Error.
    }
    \label{fig:gaussian/simulated_data_gen_params_jet_weight_1}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gaussian/simulated_data_gen_params_jet_weight_10.0.pdf}
    \vspace*{-7mm}
    \caption{\textbf{Experiment 1}: Performance metrics of the methods in all misspecification scenarios (columns) \textbf{on simulated (i.e., well-specified) data for $\mathbf{\lambda = 10}$ in NPE-MMD and NPE-DANN}, averaged across $3$ separate runs. 
    Lower values indicate better performance (for SSDD only for NPE-MMD and NPE-DANN).
    $1 -$ PC $=$ $1 -$ Posterior Contraction.
    NRMSE = Normalized Root Mean Squared Error.
    SSDD = Summary Space Domain Distance (MMD; not applicable for Analytic Posterior).
    PPD = Posterior Predictive Distance (MMD).
    ECE = Expected Calibration Error.
    }
    \label{fig:gaussian/simulated_data_gen_params_jet_weight_10.0}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gaussian/observed_data_gen_params_jet_weight_0.1.pdf}
    \vspace*{-7mm}
    \caption{\textbf{Experiment 1}: Performance metrics of the methods in all misspecification scenarios (columns) \textbf{for $\mathbf{\lambda = 0.1}$ in NPE-MMD and NPE-DANN}, averaged across $3$ separate runs. 
    Lower values indicate better performance (for SSDD only for NPE-MMD and NPE-DANN).
    $1 -$ PC $=$ $1 -$ Posterior Contraction.
    NRMSE = Normalized Root Mean Squared Error.
    SSDD = Summary Space Domain Distance (MMD; not applicable for Analytic Posterior).
    PPD = Posterior Predictive Distance (MMD).
    ECE = Expected Calibration Error.
    }
    \label{fig:gaussian/observed_data_gen_params_jet_weight_0.1}
\end{figure*}
 
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gaussian/observed_data_gen_params_jet_weight_10.0.pdf}
    \vspace*{-7mm}
    \caption{\textbf{Experiment 1}: Performance metrics of the methods in all misspecification scenarios (columns) \textbf{for $\mathbf{\lambda = 10}$ in NPE-MMD and NPE-DANN}, averaged across $3$ separate runs. 
    Lower values indicate better performance (for SSDD only for NPE-MMD and NPE-DANN).
    $1 -$ PC $=$ $1 -$ Posterior Contraction.
    NRMSE = Normalized Root Mean Squared Error.
    SSDD = Summary Space Domain Distance (MMD; not applicable for Analytic Posterior).
    PPD = Posterior Predictive Distance (MMD).
    ECE = Expected Calibration Error.
    }
    \label{fig:gaussian/observed_data_gen_params_jet_weight_10.0}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gaussian/observed_analytical_posterior_jet_weight_0.1.pdf}
    \vspace*{-7mm}
    \caption{\textbf{Experiment 1}: Performance metrics of the methods in all misspecification scenarios (columns) \textbf{compared to the analytic posterior for $\mathbf{\lambda = 0.1}$ in NPE-MMD and NPE-DANN}, averaged across $3$ separate runs. 
    Lower values indicate better performance (for SSDD only for NPE-MMD and NPE-DANN).
    MMD = Maximum Mean Discrepancy to analytic posterior.
    RMSE = Root Mean Squared Error to analytic posterior.
    SSDD = Summary Space Domain Distance (MMD; not applicable for Analytic Posterior).
    INLD = Inference Network Latent Distance (MMD) to base distribution.
    }
    \label{fig:gaussian/observed_analytical_posterior_jet_weight_0.1}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gaussian/observed_analytical_posterior_jet_weight_1.0.pdf}
    \vspace*{-7mm}
    \caption{\textbf{Experiment 1}: Performance metrics of the methods in all misspecification scenarios (columns) \textbf{compared to the analytic posterior for $\mathbf{\lambda = 1}$ in NPE-MMD and NPE-DANN}, averaged across $3$ separate runs. 
    Lower values indicate better performance (for SSDD only for NPE-MMD and NPE-DANN).
    MMD = Maximum Mean Discrepancy to analytic posterior.
    RMSE = Root Mean Squared Error to analytic posterior.
    SSDD = Summary Space Domain Distance (MMD; not applicable for Analytic Posterior).
    INLD = Inference Network Latent Distance (MMD) to base distribution.
    }
    \label{fig:gaussian/observed_analytical_posterior_jet_weight_1}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gaussian/observed_analytical_posterior_jet_weight_10.0.pdf}
    \vspace*{-7mm}
    \caption{\textbf{Experiment 1}: Performance metrics of the methods in all misspecification scenarios (columns) \textbf{compared to the analytic posterior for $\mathbf{\lambda = 10}$ in NPE-MMD and NPE-DANN}, averaged across $3$ separate runs. 
    Lower values indicate better performance (for SSDD only for NPE-MMD and NPE-DANN).
    MMD = Maximum Mean Discrepancy to analytic posterior.
    RMSE = Root Mean Squared Error to analytic posterior.
    SSDD = Summary Space Domain Distance (MMD; not applicable for Analytic Posterior).
    INLD = Inference Network Latent Distance (MMD) to base distribution.
    }
    \label{fig:gaussian/observed_analytical_posterior_jet_weight_10.0}
\end{figure*}


\subsection{Experiment 2}\label{app:bayesian_denoising}

\paragraph{Simulator (Noisy Camera Model)} We adopt a noisy camera model similar to the one presented in \citet{ramesh2022gatsbi}. First, the input image is clipped to the range $[-1, 1]$. Next, we use scikit-image \citep{scikit-image} to add Poisson noise to the image, then filter it using a Gaussian filter from SciPy \citep{2020SciPy-NMeth} with a standard deviation $\sigma$ for the Gaussian kernel. The result is a blurred image with identical size as the input image.

\paragraph{Data Preparation}

For each data set, we normalize the images to the range $[-1, 1]$. The MNIST \citep{lecun1998mnist} images are rescaled from $28\times 28$ to $16\times 16$ with anti-aliasing enabled. To produce the training data $\x$, the images are processed by the simulator, with $\sigma_0=1.4$. For NNPE, we then add noise to $\x$ using the spike-and-slab error model from \citet{ward2022robust}.

While the training data remains constant across scenarios, the observed data is generated in different ways. For the prior misspecification scenario, we use the USPS data set \citep{hull1994usps} instead of MNIST, but the parameters of the simulator remain identical (i.e., $\sigma=\sigma_0$). For the likelihood scale scenario, we use $\tilde\sigma=1.25 \cdot \sigma_0$, leading to an increased blur. For the noise contamination scenario, we randomly set $10\%$ of the pixels of each observation to black or white. For the row contamination scenario, we randomly set $2$ rows of each observation (i.e., $12.5\%$ of the pixels) to black. Refer to \autoref{tab:denoising_samples} for samples from each scenario.

\paragraph{Network Architecture}

For the summary network, we use a $4$-layer convolutional neural network, which outputs $32$ learned summary variables.

For the inference network, we use flow matching \citep{lipman2023flow, wildberger2023flow} to convert a multivariate Gaussian distribution to the approximate posterior distribution. We use a U-Net architecture \citep{ronneberger2015unet} to learn the flow field conditional on the summary variables.

For NPE-DANN, we use a domain classifier $\psi$ consisting of a standard feedforward network with $3$ hidden layers of width $256$, a gradient reversal layer (GRL) weight of $1$, and no label smoothing.

\paragraph{Training and Evaluation Details}

We use an AdamW optimizer with an initial learning rate of $5\cdot 10^{-4}$ and cosine decay. We use a batch size of $32$ and train for $20$ epochs, except for NPE-MMD, which required increasing the batch size to $128$. To keep the number of gradient updates constant, we also increased the number of epochs to $80$ for NPE-MMD. The training budget is $50\,000$ training images, and $1\,000$ observed images.
Training one neural network takes approximately 10 minutes on a GPU.


Similar to Experiment \ref{exp:gaussian}, we use a relatively low number of posterior samples (here: $S = 10$) per method to limit the computational cost of the experiment, allowing for a broader exploration of hyperparameters and the variance between multiple runs. 
While we observe a low variance of posterior samples and additionally average over observations and samples, we will increase $S$ in the full version of this work.

\paragraph{Additional Metrics}

\autoref{tab:denoising_metrics_in_distribution} displays the performance on a held-out in-distribution data set, to assess the influence on the loss on the in-domain observations.
\autoref{tab:denoising_metrics_std} displays the same data as \autoref{tab:denoising_metrics}, but with uncertainty indicators (standard deviation).

\begin{table*}[h]
    \scriptsize
    \setlength\tabcolsep{3pt} % default value: 6pt
    \centering 
    \input{tables/bayesian-denoising/metrics_in_domain_1}
    \input{tables/bayesian-denoising/metrics_in_domain_2}
    \caption{\textbf{Experiment 2}: Overview of the metrics on a held-out validation data set from the training distribution (mean and standard deviation of three runs). For NNPE and NPE-DANN we see reduced performance on the training distribution. For NPE-MMD, we see that for successful runs the performance on the training distribution improves. For settings with vanishing SSDD (compare \autoref{tab:denoising_metrics}) the performance drops massively, for both training distribution and observed distribution. This supports the notion that no meaningful information is learned in the summary space.}
    \label{tab:denoising_metrics_in_distribution}
\end{table*}

\begin{table*}[h]
    \scriptsize
    \setlength\tabcolsep{3pt} % default value: 6pt
    \centering
    \input{tables/bayesian-denoising/metrics_std_1}
    \input{tables/bayesian-denoising/metrics_std_2}
    \caption{\textbf{Experiment 2}: Overview of the metrics in the different misspecification scenarios (mean and standard deviation of three runs). Please refer to \autoref{tab:denoising_metrics} for a detailed description. Note that each standard deviation is given for a constant set of hyperparameters, so it only covers the computational uncertainty for a given setting. As shown by the performance changes when changing $\lambda$, hyperparameters have a large influence on the results, and different hyperparameter choices might lead to qualitative changes in the results.}
    \label{tab:denoising_metrics_std}
\end{table*}

\paragraph{Additional Figures}

\autoref{fig:denoising/app-ssd-nrmse} shows the plots corresponding to \autoref{fig:denoising/ssdd-nrmse} for the remaining three scenarios.

\begin{figure}

\centering
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\linewidth]{figures/bayesian-denoising/ssdd-nrmse-prior-misspecification.pdf}
    \caption{Prior (MNIST $\rightarrow$ USPS)}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\linewidth]{figures/bayesian-denoising/ssdd-nrmse-noise-misspecification.pdf}
    \caption{Contamination (Noise)}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\linewidth]{figures/bayesian-denoising/ssdd-nrmse-contamination-row-deletion.pdf}
    \caption{Contamination (Rows)}
\end{subfigure}
        
\caption{\textbf{Experiment 2}: Relationship of summary space domain distance (SSDD) and normalized root mean squared error (NRMSE, lower is better). For a) we see that despite the reduced SSDD, there is no gain in performance. For b) and c), we observe a sweet spot at a low SSDD value, before performance drops again when approaching zero. Refer to \autoref{tab:denoising_metrics} for numerical values.}
\label{fig:denoising/app-ssd-nrmse}

\end{figure}

\end{document}