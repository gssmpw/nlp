\section{Related Works}
We now discuss additional related works.


\paragraph{Benign overfitting in logistic regression.} 
A line of work shows the benign overfitting of the asymptotic GD (or the maximum $\ell_2$-margin estimator) in overparameterized logistic regression under a variety of assumptions \citep{montanari2019generalization,chatterji2021finite,cao2021risk,wang2022binary,muthukumar2021classification,shamir2023implicit}. 
Our results are not a violation of theirs, instead, we show an additional regularization of early-stopping, which brings statistical advantages of early-stopped GD over asymptotic GD such as calibration and a smaller sample complexity. 






\paragraph{M-estimators for logistic regression.}
In the classical finite $d$-dimensional setting, the sample complexity of the \emph{empirical risk minimizer} (ERM) for logistic regression is well-studied \citep{ostrovskii2021finite,kuchelmeister2024finite,hsu2024sample,chardon2024finite}, where the minimax rate is known to be $\Ocal(d/n)$.
Different from theirs, we focus on an overparameterized regime, where the ERM of logistic regression does not even exist. When specialized to their setting, our \Cref{thm:gd:fast-upper-bound} recovers the comparable $\tilde{\Ocal}(d/n)$ rate.

In the nonparametric setting, the works by \citep{bach2010self,marteau2019beyond} provided logistic risk bounds for $\ell_2$-regularized ERM. 
\citet{bach2010self} only considered a fixed design setting, whereas \citet{marteau2019beyond} required that $\|\wB^*\|<\infty$. 
Different from theirs, we aim to understand the benefits of the implicit regularization of early-stopping, instead of that of explicit $\ell_2$-regularization.
Moreover, we show that early-stopped GD achieves a vanishing excess logistic risk as long as $\|\wB^*\|_{\SigmaB}<\infty$, without assuming a finite $\|\wB^*\|$. In the regimes where our results are directly comparable, however, our risk bounds might be less tight than theirs. We leave it as a future work to improve our current bounds.

Finally, \citet{bach2014adaptivity} considered one-pass SGD for logistic regression assuming strong convexity around the true model parameter.
This strong convexity assumption, however, is prohibitive in our high-dimensional settings. 



\paragraph{Separable distribution.}
There are logistic risk bounds of early-stopped GD (and one-pass SGD) developed in the \emph{noiseless} cases, assuming a separable population distribution \citep{ji2018risk,shamir2021gradient,telgarsky2022stochastic,schliserman2024tight}. 
These results do not imply any benefits of early stopping, as their setting is noiseless. 
In comparison, we consider overparameterized logistic regression with a strictly non-separable population distribution, where the risk of overfitting is prominent.
In this case, our results suggest that early stopping plays a significant role in preventing overfitting.

\paragraph{Early stopping for classification.}  
In the boosting literature, an early work by \citet{zhang2005boosting}
showed that boosting methods (that can be interpreted as coordinate descent) with early stopping
are consistent in the classification sense;
related refined studies for boosting with the squared loss with
early stopping were also provided by \citet{buhlmann2003boosting}.  
The paper is also notable for giving the first proof of boosting methods converging to the maximum margin solution \citep[Appendix D]{zhang2005boosting}, which was later
refined with rates by \citep{telgarsky2013margins}. 
Their results can be converted to GD.
In particular, related concepts were used to prove consistency
of early-stopped GD for shallow networks in the lazy regime \citep{ji2021early}.
In contrast with the present work that focuses on high-dimensional cases, the preceding works only deal with finite-dimensional settings.  Moreover, none of those works provide lower bounds for interpolating estimators and tight links to the regularization path which are provided in the present work.







\paragraph{Classification calibration.}  \Cref{prop:risk-properties}
captures a very nice consequence of logistic loss minimization:
\emph{calibration} and \emph{classification-calibration}, respectively
recovery of the optimal conditional probability model and of the
optimal classifier.  For more general convex losses, the ability
to construct a general conditional probability model was
developed by \citet{zhang_consistency} as a conceptual tool in establishing
classification calibration, but without explicitly controlling
calibration error. A further abstract treatment of classification
calibratoin was later presented by \citet{bartlett_jordan_mcauliffe}.
The refined statistical rates, separations, and early-stopping
consequences studied in the present work were not considered in those
works.