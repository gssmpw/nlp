\documentclass[11pt]{article}

\usepackage{authblk}
\usepackage{times}
\usepackage{fullpage,natbib,mymath}
\bibliographystyle{ims}


\newcommand\JW[1]{\textcolor{orange}{[JW: #1]}}
\newcommand\MJT[1]{\textcolor{red!80!blue!90!black}{[MJT: #1]}}

\title{Benefits of Early Stopping in Gradient Descent for Overparameterized Logistic Regression}
\author[1]{Jingfeng Wu}
\author[1,3]{Peter L.\ Bartlett\thanks{Alphabetical order.}}
\author[2]{Matus Telgarsky$^*$}
\author[1]{Bin Yu$^*$}

\affil[1]{University of California, Berkeley\\ {\tt \{uuujf,peter,binyu\}@berkeley.edu}}
\affil[2]{New York University\\ {\tt mjt10041@nyu.edu}}
\affil[3]{Google DeepMind}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
In overparameterized logistic regression, gradient descent (GD) iterates diverge in norm while converging in direction to the maximum $\ell_2$-margin solution---a phenomenon known as the implicit bias of GD. This work investigates additional regularization effects induced by early stopping in well-specified high-dimensional logistic regression. We first demonstrate that the excess logistic risk vanishes for early-stopped GD but diverges to infinity for GD iterates at convergence. This suggests that early-stopped GD is well-calibrated, whereas asymptotic GD is statistically inconsistent. Second, we show that to attain a small excess zero-one risk, polynomially many samples are sufficient for early-stopped GD, while exponentially many samples are necessary for any interpolating estimator, including asymptotic GD. This separation underscores the statistical benefits of early stopping in the overparameterized regime. Finally, we establish nonasymptotic bounds on the norm and angular differences between early-stopped GD and $\ell_2$-regularized empirical risk minimizer, thereby connecting the implicit regularization of GD with explicit $\ell_2$-regularization.  
\end{abstract}


\section{Introduction}

Modern machine learning often operates in the \emph{overparameterized} regime, where the number of parameters exceeds the number of training data. Despite this, models trained by \emph{gradient descent} (GD) often generalize well even in the absence of explicit regularization \citep{zhang2021understanding,neyshabur2017exploring,bartlett2021deep}. The common explanation is that GD exhibits certain \emph{implicit regularization} effects that prevent overfitting. 

The implicit regularization of GD is relatively well understood in regression settings. Using overparameterized linear regression as an example, amongst all interpolators, GD asymptotically converges to the minimum $\ell_2$-norm interpolator \citep{zhang2021understanding}. Moreover, when the data covariance satisfies certain conditions, the minimum $\ell_2$-norm interpolator achieves vanishing excess risk while fitting training data with \emph{constant} amount of noise, 
a phenomenon known as \emph{benign overfitting} \citep[see][and references therein]{bartlett2020benign,tsigler2023benign}.
When the data covariance is general, although benign overfitting may not occur, early-stopped GD (and one-pass stochastic GD) can still achieve vanishing excess risk \citep{buhlmann2003boosting,yao2007early,lin2017optimal,dieuleveut2016nonparametric,zou2023benign,zou2022risk,wu2022last}. This suggests early stopping provides an additional regularization effect for GD in linear regression.
Moreover, the statistical effects of early stopping are known to be comparable to that of $\ell_2$-regularization in linear regression \citep{suggala2018connecting,ali2019continuous,zou2021benefits}.


However, the picture is less complete for classification, 
where the risk is measured by the logistic loss and the zero-one loss instead of the squared loss. 
In overparameterized logistic regression,
GD diverges in norm while converging in direction to the maximum $\ell_2$-margin solution \citep[see][and \Cref{prop:implicit-bias-asymp} in \Cref{sec:preliminary}]{soudry2018implicit,ji2018risk}, which is in contrast with GD's convergence to the (bounded!) minimum $\ell_2$-norm solution in the linear regression setting. 
In standard (finite-dimensional, low-noise, large margin) classification settings, the asymptotic implicit bias of GD implies generalization via classical margin theory \citep{bartlett1999generalization}.  More recently,
certain high-dimensional settings exhibit well-behaved
maximum margin solutions and benign overfitting \citep[see][for example]{montanari2019generalization}, but it is unclear if these
results apply more broadly or represent special cases.
Moreover, if the maximum $\ell_2$-margin solution generalizes poorly,
new techniques are required, as the aforementioned least squares techniques
cannot be easily adapted owing to their heavy dependence upon the explicit linear algebraic form
of GD's path specific to least squares.










\begin{figure*}[t]
    \centering
    \subfigure[Empirical and population logistic risk]{
    \label{fig:toy:logistic}
    \includegraphics[width=0.45\linewidth]{figures/risk.pdf}
    }
    \hfill
    \subfigure[Empirical and population zero-one error]{
    \label{fig:toy:error}
    \includegraphics[width=0.45\linewidth]{figures/error.pdf}
    }
    \caption{The logistic risk and zero-one error along the GD path for an overparameterized logistic regression problem. 
    Here $d=2000$, $n=1000$, $\lambda_i=i^{-2}$, $\wB^*_{0:100}=1$ and $\wB^*_{100:\infty}=0$. The optimization length is measured by $\eta t$. The plots show that the excess logistic risk and excess zero-one error are both small for GD with appropriate early stopping, and both grow larger when GD enters the interpolation regime. These demonstrate the regularization of early stopping in GD. }
    \label{fig:toy}
\end{figure*}

\paragraph{Contributions.} 
This work investigates the beneficial regularization effects of early stopping in GD for overparameterized logistic regression. We focus on a well-specified setting where the feature vector follows an anisotropic Gaussian design and the binary label conditional on the feature is given by a logistic model (see \Cref{assump:data-distribution} in \Cref{sec:preliminary}). 
We are particularly interested in the regime where the label contains a constant level of noise. 
We establish the following results.



\begin{enumerate}[leftmargin=*]
\item \textbf{Calibration via early stopping.} 
We first derive risk upper bounds for early-stopped GD that can be applied in the overparameterized regime. 
With an oracle-chosen stopping time, early-stopped GD achieves vanishing excess logistic risk and excess zero-one error (as the sample size grows) for \emph{every} well-specified logistic regression problem. Furthermore, its naturally induced conditional probability approaches the true underlying conditional probability model.
These properties suggest that early-stopped GD is \emph{consistent} and \emph{calibrated} for \emph{every} well-specified logistic regression problem, even in the overparameterized regime.  


\item \textbf{Advantages over interpolation.} We then provide negative results for GD without early stopping. 
We show that GD at convergence,
in contrast to the typical successes of maximum margin predictors,
suffers from an \emph{unbounded} logistic risk and a \emph{constant} calibration error in the overparameterized regime. 
Moreover, for a broad class of overparameterized logistic regression problems, to attain a small excess zero-one error, early-stopped GD only needs \emph{polynomially} many samples, whereas any interpolating estimators, including asymptotic GD, requires at least \emph{exponentially} many samples. 
These results underscore the statistical benefits of early stopping.

\item \textbf{Connections to $\ell_2$-regularization.} Finally, we compare the GD path (formed by GD iterates with all possible stopping times) with the $\ell_2$-regularization path (formed by $\ell_2$-regularized empirical risk minimizers with all possible regularization strengths). For general convex and smooth problems, including logistic regression, these two paths differ in norm by a factor between $0.585$ and $3.415$, and differ in direction by an angle no more than $\pi/4$. 
Specific to overparameterized logistic regression, the $\ell_2$-distance of the two paths is asymptotically zero in a widely considered situation but may diverge to infinity in the worst case. 
These findings partially explain the implicit regularization of early stopping via its connections with the explicit $\ell_2$-regularization.
\end{enumerate}




\paragraph{Notation.}
For two positive-valued functions $f(x)$ and $g(x)$, we write $f(x)\lesssim g(x)$ or $f(x)\gtrsim g(x)$
if there exists a constant $c>0$ such that $f(x) \le cg(x)$ or $f(x) \ge cg(x)$ for every $x$, respectively. 
We write $f(x) \eqsim g(x) $ if $f(x) \lesssim g(x) \lesssim f(x)$.
We use the standard big-O notation. 
For two vectors $\uB$ and $\vB$ in a Hilbert space, we denote their inner product by $\la\uB, \vB\ra$ or equivalently, $\uB^\top \vB$.
For two matrices $\AB$ and $\BB$ of appropriate dimension, we define their inner product as $\langle \AB, \BB \rangle := \tr(\AB^\top \BB)$.
For a positive semi-definite (PSD) matrix $\AB$ and a vector $\vB$ of appropriate dimension, we write $\|\vB\|_{\AB}^2 := \vB^\top \AB \vB$. In particular, we write $\|\vB \| := \|\vB \|_\IB$.
For a positive integer $n$, we write $[n]:=\{1,\dots,n\}$.




\section{Preliminaries}\label{sec:preliminary}


Let $(\xB,y)\in\Hbb\otimes \{\pm 1\}$ be a pair of features and the corresponding binary label sampled from an unknown population distribution.
Here $\Hbb$ is a finite or countably infinite dimensional Hilbert space.  
For a parameter $\wB\in\Hbb$, define its population \emph{logistic risk} as
\begin{align*}
    \risk(\wB):= \Ebb \ell(y \xB^\top \wB),\ \text{where} \ \ell(t) := \ln(1+e^{-t}),
\end{align*}
and define its population \emph{zero-one error} as
\begin{align*}
    \error(\wB):= \Ebb \ind{y\xB^\top \wB \le 0} 
    = \Pr(y\xB^\top \wB \le 0),
\end{align*}
where the expectation is over the population distribution of $(\xB,y)$.
It is worth noting that, different from the logistic risk $\risk(\wB)$, the zero-one error $\error(\wB)$ is insensitive to the parameter norm.
Moreover, we measure the \emph{calibration error} of a parameter $\wB\in\Hbb$ by
\begin{align*}
    \calibration(\wB):= \Ebb \big|p(\wB;\xB) - \Pr(y=1|\xB)\big|^2,
\end{align*}
where $p(\wB;\xB)$ is a naturally induced conditional probability given by 
\begin{align*}
    p(\wB;\xB) := \frac{1}{1+\exp(-\xB^\top\wB)}.
\end{align*}

We say an estimator $\hat \wB$ is \emph{consistent} (for classification) if it attains the Bayes zero-one error asymptotically, that is, $\error(\hat \wB) - \min \error \to 0$.
We say an estimator $\hat\wB$ is \emph{calibrated} if its induced conditional probability predicts the true one asymptotically \citep{foster1998asymptotic}, that is, $\calibration(\hat\wB) \to 0$.




\paragraph{Gradient descent.}
Let $(\xB_i, y_i)_{i=1}^n$ be $n$ independent copies of $(\xB,y)$. Define the empirical risk as 
\begin{equation*}
    \eRisk(\wB):= \frac{1}{n}\sum_{i=1}^n\ell(y_i\xB_i^\top \wB),\quad \wB\in\Hbb.
\end{equation*}
Then the iterates of \emph{gradient descent} (GD) are given by
\begin{equation}\label{eq:GD}\tag{GD}
 \wB_0 =0,\quad 
   \wB_{t+1} = \wB_t - \eta \grad \eRisk(\wB_t),\quad t\ge 0, 
\end{equation}
where $\eta>0$ is a fixed stepsize. We consider zero initialization to simplify the presentation, which does not cause the loss of generality.
We aim to compare asymptotic GD, that is, $\wB_{\infty}$, with early-stopped GD, that is, $\wB_t$ at a certain finite stopping time $t<\infty$.

\paragraph{Data model.}
We mainly focus on a \emph{well-specified} setting formalized by the following conditions. 
However, part of our results can also be applied to misspecified cases. 
\begin{assumption}[Well-specification]\label{assump:data-distribution}
Let $\SigmaB \in \Hbb^{\otimes 2}$ be positive semi-definite (PSD) and $\tr(\SigmaB) < \infty$. Let $\wB^*\in\Hbb$ be such that $\|\wB^*\|_{\SigmaB} < \infty$.
Assume that $(\xB, y)\in \Hbb \otimes \{\pm 1\}$ is given by
\begin{align*}
\xB\sim \Ncal(0,\SigmaB),\quad \Pr(y | \xB) = \frac{1}{1+\exp(-y\xB^\top \wB^*)}.
\end{align*}
\end{assumption}

Under this data model, we have the following standard properties for the logistic risk, zero-one error, and calibration error (see for example \citep{ji2021early} or Section 4.7 in \citep{mohri2018foundations}). The proof is included in \Cref{sec:proof:risk-properties} for completeness.

\begin{proposition}[Basic properties]\label{prop:risk-properties}
Under \Cref{assump:data-distribution}, we have
\begin{assumpenum}
\item $\wB^* = \arg\min\risk(\cdot)$ and $\wB^*\in \arg\min \error(\cdot)$;
\item 
for every $\wB\in\Hbb$, it holds that
\begin{align*}
    \error (\wB) - \min \error  
    \le 2  \sqrt{\calibration(\wB)} 
    \le\sqrt{2}\cdot \sqrt{\risk(\wB) - \min\risk};
\end{align*}
\item if additionally we have $\|\wB^*\|_{\SigmaB} \lesssim 1$, then 
\begin{align*}
    \min \risk  \gtrsim 1,\quad \min \error \gtrsim 1.
\end{align*}
\end{assumpenum}
\end{proposition}

\Cref{prop:risk-properties} suggests that the Bayes logistic risk and Bayes zero-one error are attained by the true model parameter $\wB^*$. Moreover, the excess zero-one error is controlled by the calibration error, which is further controlled by the excess logistic risk. 
Thus under \Cref{assump:data-distribution}, a calibrated estimator is also consistent for classification, and an estimator is calibrated if it attains the Bayes logistic risk asymptotically.
However, the reverse might not be true.
As we will show later, for overparameterized logistic regression, early-stopped GD is calibrated and consistent for both logistic risk and zero-one error. In contrast, asymptotic GD is poorly calibrated and attains an unbounded logistic risk, although it could be consistent for zero-one error. 


\paragraph{Noise and overparameterization.}
Most of our results should be interpreted in the \emph{noisy} and \emph{overparameterized} regime. Specifically, this means \[\|\wB^*\|_{\SigmaB} \lesssim 1\  \text{and}\ \rank(\SigmaB) \ge n.\] The first condition ensures the population distribution carries a constant amount of noise, as the Bayes logistic risk and Bayes zero-one error are lower bounded by a constant according to \Cref{prop:risk-properties}. In other words, the population distribution is strictly \emph{not} linearly separable. 
Despite so, the second condition ensures the \emph{linear separability} of the training data almost surely, as the number of effective parameters exceeds the number of training data. 
In this regime, estimators can \emph{interpolate} the training data, yet this interpolation inherently carries the risk of \emph{overfitting} and \emph{poor calibration}. 
Our setting aligns well with the prior setting for studying benign overfitting in linear regression \citep{bartlett2020benign,tsigler2023benign}.


\paragraph{Asymptotic implicit bias.}
When the training data is linearly separable (implied by overparameterization), prior works show that GD diverges to infinite in norm while converging in direction to the maximum $\ell_2$-margin direction \citep{soudry2018implicit,ji2018risk}. This characterizes the asymptotic \emph{implicit bias} of GD. See the following proposition for a precise statement.

\begin{proposition}[Asymptotic implicit bias]\label{prop:implicit-bias-asymp}
Assume that $\rank(\xB_1,\dots,\xB_n) \ge n$. Then the training data $(\xB_i, y_i)_{i=1}^n$ is linearly separable, that is, \[\max_{\|\wB\|=1} \min_{i\in [n]} y_i\xB_i^\top \wB > 0.\] 
Let $\tilde \wB$ be the maximum $\ell_2$-margin direction, that is,
\begin{align*}
    \tilde \wB := \arg\max_{\|\wB\|=1} \min_{i\in [n]} y_i\xB_i^\top \wB.
\end{align*}
Then $\tilde\wB$ is unique and the following holds for \Cref{eq:GD} with any stepsize $\eta>0$:
\begin{align*}
\|\wB_t\|\to \infty,\quad 
\frac{\wB_t}{\|\wB_t\|}\to \tilde\wB,
\end{align*}
\end{proposition}
\begin{proof}[Proof of \Cref{prop:implicit-bias-asymp}]
Let $\XB := (\xB_1,\dots,\xB_n)^\top$ and $\yB := (y_1,\dots,y_n)^\top$. 
Since $\rank(\XB) \ge n$, the ordinary least squares estimator
\[\hat \wB:= \XB^\top (\XB\XB^\top)^{-1}\yB\] is well-defined. This implies the linear separability of the training data as $\XB \hat \wB = \yB$. If two distinct directions $\tilde\wB_1$ and $\tilde\wB_2$ achieve the maximum $\ell_2$-margin, the direction of their average achieves a larger margin, which is a contradiction. So $\tilde\wB$ must be unique.

For logistic regression with linearly separable data, the implicit bias of GD is established by \citet{soudry2018implicit,ji2018risk} when the stepsizes are small such that $\eRisk(\wB_t)$ decreases monotonically. The same results can be extended to GD with any fixed stepsize using techniques from \citep{wu2023implicit,wu2024large}.
\end{proof}

\paragraph{Additional notation.}
The following notations are handy for presenting our results.
Let $(\lambda_i)_{i\ge 1}$ be the eigenvalues of the data covariance $\SigmaB$, sorted in non-increasing order. Let $\uB_i$ be the eigenvector of $\SigmaB$ corresponding to $\lambda_i$.
Let $\big(\pi(i)\big)_{i\ge 1}$ be resorted indexes
such that $\lambda_{\pi(i)}\big(\uB_{\pi(i)}^\top \wB^*\big)^2$ is non-increasing as a function of $i$. Define 
\[
\wB^*_{0:k} := \sum_{i\le k} \uB_{\pi(i)}\uB_{\pi(i)}^\top \wB^*,\quad 
\wB^*_{k:\infty} := \sum_{i> k} \uB_{\pi(i)}\uB_{\pi(i)}^\top \wB^*.
\]
It is clear that $\|\wB^*\|_{\SigmaB} < \infty$ implies that $\big\|\wB^*_{k:\infty}\big\|_{\SigmaB} = o(1)$ as $k$ increases.

\section{Upper Bounds for Early-Stopped GD}\label{sec:upper-bound}
In this section, we present two risk bounds for early-stopped GD for overparameterized logistic regression and a characterization of the implicit bias of early stopping in GD.

\subsection{A Bias-Dominating Bound}
We first provide a bias-dominating excess logistic risk bound for early-stopped GD in overparameterized logistic regression.
The proof is deferred to \Cref{sec:proof:gd:upper-bound}.

\begin{theorem}[A ``bias-dominating'' risk bound]\label{thm:gd:upper-bound}
Suppose that \Cref{assump:data-distribution} holds.
Let $k$ be an arbitrary index.
Suppose that the stepsize for \Cref{eq:GD} satisfies
\[\eta\le \frac{1}{C_0\big( 1+\tr(\SigmaB)+\lambda_1\ln(1/\delta)/n\big) },\]
where $C_0>1$ is a universal constant. Then with probability at least $1-\delta$, the empirical risk $\eRisk(\wB_t)$ decreases monotonically for $t\ge 0$. Thus there exists a stopping time $t$ such that 
\begin{align*}
\eRisk(\wB_t)\le \eRisk(\wB^*_{0:k}) \le \eRisk(\wB_{t-1}).
\end{align*}
Moreover, 
for \Cref{eq:GD} with this stopping time we have
\begin{align*}
{\risk(\wB_t)-\min\risk \lesssim } 
  \sqrt{\frac{ \max\big\{1,\, \tr(\SigmaB)\|\wB^*_{0:k}\|^2 \big\}\ln^2(n/\delta)}{n}} 
+\|\wB^*_{k:\infty}\|^2_{\SigmaB}.
\end{align*}
\end{theorem}

A few remarks on \Cref{thm:gd:upper-bound} are in order.

\paragraph{Calibration and consistency.}
\Cref{thm:gd:upper-bound} implies that early-stopped GD attains the Bayes logistic risk asymptotically for \emph{any} logistic regression problem satisfying \Cref{assump:data-distribution}.
To see this, we pick $k$ as an increasing function of $n$ such that 
\(
\|\wB^*_{0:k}\|=o(n).
\)
Then $\|\wB^*_{k:\infty}\|_{\SigmaB} = o(1)$ since $k$ increases as $n$ increases (recall that $\|\wB^*\|_{\SigmaB}$ is finite by \Cref{assump:data-distribution}). Hence the risk bound in \Cref{thm:gd:upper-bound} implies that
\[\risk(\wB_t)-\min\risk= o(1)\ \text{as $n$ increases}.\]
By \Cref{prop:risk-properties}, this also ensures that early-stopped GD induces a conditional probability that approaches the true one and achieves a vanishing excess zero-one error. Hence early-stopped GD is calibrated and consistent for any well-specified logistic regression problem.

As a concrete example, let us consider the following source and capacity conditions \citep{caponnetto2007optimal}, 
\begin{equation}\label{eq:source-capacity}
    \lambda_i \eqsim i^{-a},\quad \lambda_i (\uB_i^\top \wB^*_i)^2 \eqsim i^{-b},\quad a,b>1.
\end{equation}
Then \Cref{thm:gd:upper-bound} implies 
\begin{align*}
    \risk(\wB_t) -\min\risk = \begin{dcases}
    \tilde\Ocal\big(n^{-1/2}\big) & b>a+1, \\
        \tilde\Ocal(n^{\frac{1-b}{a+b-1}}) & b \le a+1.
    \end{dcases}
\end{align*}
This provides an explicit rate on the excess risk. Note that the obtained rate might not be the sharpest. An improved rate under stronger conditions is provided later in \Cref{thm:gd:fast-upper-bound}. Note that our main purpose here is to show the calibration and consistency of early-stopped GD for every well-specified logistic regression problem. 

\paragraph{Stopping time.}
Note that the stopping time $t$ relies on the oracle information of the true parameter $\wB^*$. Therefore the ``early-stopped GD'' in \Cref{thm:gd:upper-bound} is not a practical algorithm. Instead, we should view \Cref{thm:gd:upper-bound} as a guarantee for GD with an \emph{optimally tuned} stopping time. It will also be clear later in \Cref{sec:lower-bound} that the optimal stopping time $t$ must be finite for overparameterized logistic regression. 
Moreover, we point out that the stopping time $t$ is a function of $k$ and thus also depends on the sample size $n$.

\paragraph{Misspecification.}
For the simplicity of discussion, we state \Cref{thm:gd:upper-bound} in a well-specified case formalized by \Cref{assump:data-distribution}. Nonetheless, from its proof in \Cref{sec:proof:gd:upper-bound}, it is clear that the same results also hold in misspecified cases, where we define $\wB^* \in \arg\min \risk$ and assume $\SigmaB^{-1/2}\xB$ is subGaussian.
Here, we do not need to make assumptions on the true conditional probability $\Pr(y|\xB)$.
In those misspecified cases, however, \Cref{prop:risk-properties} may not hold. Thus \Cref{thm:gd:upper-bound} only provides a logistic risk bound but does not yield any bounds on calibration error or zero-one error.



\subsection{A Variance-Dominating Bound}
From \Cref{thm:gd:upper-bound}, we see that early-stopped GD is consistent and calibrated under the arguably weakest condition on the true parameter, $\|\wB^*\|_{\SigmaB} < \infty$. However, the attained bound decays at a rate no faster than $\Ocal(1/\sqrt{n})$ as long as $\|\wB^*\|_{\SigmaB} \gtrsim 1$.
In the simpler case where $\|\wB^*\|< \infty$, 
we can tune the stopping time to achieve an improved bound. This is presented in the following theorem. The proof is deferred to \Cref{sec:proof:gd:fast-upper-bound}.

\begin{theorem}[A ``variance-dominating'' risk bound]\label{thm:gd:fast-upper-bound}
Suppose that \Cref{assump:data-distribution} holds with $\|\wB^*\|<\infty$.
Let $k$ be an arbitrary index.
Suppose that the stepsize for GD satisfies the same condition as in \Cref{thm:gd:upper-bound} and the stopping time $t$ is such that
\[\eRisk(\wB_t) \le \eRisk(\wB^*) \le \eRisk(\wB_{t-1}).\]
Assume for simplicity that 
\(\|\wB^*\|\gtrsim 1\), \(\lambda_1 \lesssim 1\), and \(\tr(\SigmaB)\gtrsim 1.\)
Then with probability at least $1-\delta$, we have 
\begin{align*}
\risk (\wB_t) -  \min\risk \lesssim  \|\wB^*\| \bigg( \frac{k}{n} + \sqrt{\frac{\sum_{i>k}\lambda_i}{n}} +
 \frac{  \tr(\SigmaB)^{1/2}\ln\big(n \|\wB^*\|\tr(\SigmaB) /\delta\big)}{n} \bigg).
\end{align*}
\end{theorem}

\paragraph{Comparing \Cref{thm:gd:upper-bound,thm:gd:fast-upper-bound}.}
Compared to \Cref{thm:gd:upper-bound}, \Cref{thm:gd:fast-upper-bound} achieves a faster rate, but is only applicable when $\|\wB^*\|<\infty$.
Specifically, in the classical finite-dimensional setting where $\|\wB^*\| \eqsim 1$ and $\SigmaB=\IB_d$, the excess risk bound in \Cref{thm:gd:fast-upper-bound} decreases at the rate of $\tilde\Ocal(d/n)$ while that in \Cref{thm:gd:upper-bound} decreases at the rate of $\tilde\Ocal\big(\sqrt{d/n}\big)$.
For another example, under the source and capacity conditions of \Cref{eq:source-capacity}, \Cref{thm:gd:fast-upper-bound} provides an improved excess risk bound of $\tilde\Ocal\big(n^{-a/(1+a)}\big)$ when $b>a+1$, but is not applicable when $b\le a+1$.

The stopping time in \Cref{thm:gd:upper-bound} is designed to handle more general high-dimensional situations that even allow $\|\wB^*\|=\infty$. It tends to stop ``earlier'' so that the bias error tends to dominate the variance error. In comparison, \Cref{thm:gd:fast-upper-bound} is limited to simpler cases where $\|\wB^*\|<\infty$ and sets a ``later'' stopping time so that the variance error tends to dominate the bias error. Therefore \Cref{thm:gd:fast-upper-bound} achieves a faster rate. 


\paragraph{Future directions.}
\Cref{thm:gd:upper-bound,thm:gd:fast-upper-bound} are sufficiently powerful for our purpose of demonstrating the benefits of early stopping. 
However, we point out that neither Theorems \ref{thm:gd:upper-bound} nor \ref{thm:gd:fast-upper-bound} reveal the \emph{true} trade-off between the bias and variance errors induced by early stopping. 
This is unsatisfactory given that in linear regression, the exact trade-off between bias and variance errors has been settled for one-pass SGD \citep{zou2023benign,wu2022last,wu2022power} and $\ell_2$-regularization \citep{tsigler2023benign}, and has been partially settled for early-stopped GD \citep[assuming a Gaussian prior]{zou2022risk}.
We leave the improvement of these bounds for future work.


From a technical perspective, the gap in analysis between linear regression and logistic regression is significant.
All the prior sharp analyses of GD in linear regression make heavy use of explicit calculations with chains of equalities and closed-form solutions. 
But these fail to hold for GD in logistic regression since the Hessian is no longer fixed.
While one might suspect that a limiting analogy
can be made where least squares ideas are applied locally around an
optimum, a priori there is no reason to believe that the GD path,
which diverges to infinity, even passes near the population optimum,
let alone spends a reasonable amount of time there.  Moreover,
as our lower bounds in \Cref{sec:lower-bound} attest, the GD path exhibits significant curvature.
Due to these issues, we believe tools from linear regression can not
be merely ported over, and new approaches are required.
While we have provided some tools to this end,
as above \Cref{thm:gd:upper-bound,thm:gd:fast-upper-bound} do not tightly characterize
the GD path, and much is left to future work.



\subsection{Implicit Bias of Early Stopping}
In this part, we briefly discuss the proof ideas by introducing the following key lemma in our analysis.  
Variants of this lemma have appeared in \citep{ji2018risk,ji2019polylogarithmic,shamir2021gradient,telgarsky2022stochastic,wu2024large} for analyzing different aspects of GD. 
For completeness, we include a proof of it in \Cref{sec:proof:implicit-regularization}.

\begin{lemma}[Implicit bias of early stopping]\label{lemma:implicit-regularization}
Let $\eRisk(\cdot)$ be convex and $\beta$-smooth. Let $(\wB_t)_{t\ge 0}$ be given by \Cref{eq:GD} with stepsize $\eta \le 1/\beta$.
Then for every $\uB$, we have 
\begin{align*}
    \frac{\|\wB_t - \uB\|^2}{2\eta t} + \eRisk(\wB_t) \le \eRisk(\uB) + \frac{\|\uB\|^2}{2\eta t},\quad t>0.
\end{align*}
\end{lemma}
This lemma reveals an implicit bias of early-stopping, in which early-stopped GD \emph{attains a small empirical risk while maintaining a relatively small norm}. 
Specifically, consider a comparator $\uB$ and a stopping time $t$ such that 
\[
\eRisk(\wB_t)\le \eRisk(\uB) \le \eRisk(\wB_{t-1}).
\]
This stopping time together with \Cref{lemma:implicit-regularization} (applied to $t-1$) leads to
\begin{align*}
    \eRisk(\wB_t)\le \eRisk(\uB),\ \text{and}\ \|\wB_{t-1}- \uB\|\le \|\uB\|.
\end{align*}
By optimizing the choice of the comparator $\uB$, we see that early-stopped GD achieves a small empirical risk with a relatively small norm.

Besides \Cref{lemma:implicit-regularization}, the remaining efforts for proving \Cref{thm:gd:upper-bound,thm:gd:fast-upper-bound} are using classical tools of Rademacher complexity \citep{bartlett2002rademacher,kakade2008complexity} and local Rademacher complexity \citep{bartlett2005local}, respectively. More details can be found in \Cref{sec:proof:gd:upper-bound,sec:proof:gd:fast-upper-bound}.

Later in \Cref{sec:l2-regularization}, we will use \Cref{lemma:implicit-regularization} to show connections between early stopping and $\ell_2$-regularization.
We also note that the proof of \Cref{thm:gd:upper-bound,thm:gd:fast-upper-bound} can be easily adapted to $\ell_2$-regularized empirical risk minimizes.


\section{Lower Bounds for Interpolating Estimators}\label{sec:lower-bound}
In this section, we provide negative results for interpolating estimators by establishing risk lower bounds for them.


\subsection{Logistic Risk and Calibration Error}
The following theorem shows that GD without early stopping must induce an unbounded logistic risk and a positive calibration error in the overparameterized regime. The proof is deferred to \Cref{sec:proof:logistic:lower-bound}.

\begin{theorem}[Lower bounds for logistic risk and calibration error]\label{thm:logistic:lower-bound}
Suppose that \Cref{assump:data-distribution} holds. 
Let $\tilde\wB$ be a unit vector such that $\|\tilde \wB\|_{\SigmaB} > 0$ and let $(\wB_t)_{t\ge 0}$ be a sequence of vectors such that 
\begin{align*}
    \|\wB_t\|\to \infty,\quad 
    \frac{\wB_t}{\|\wB_t\|} \to \tilde\wB.
\end{align*}
Then we have
\begin{align*}
\lim_{t\to\infty}\calibration(\wB_t) \ge \exp(-C\|\wB^*\|_{\SigmaB}),\quad 
   \lim_{t\to\infty} \risk(\wB_t) = \infty,
\end{align*}
where $C>1$ is a constant.
\end{theorem}

\Cref{thm:logistic:lower-bound} shows that for every sequence of estimators that diverges in norm but converges in direction, their induced logistic risk must grow unboundedly and their induced calibration error must be bounded away from zero by a constant. 
Therefore, their limit is \emph{inconsistent} (for logistic risk) and \emph{poorly calibrated}. 
According to \Cref{prop:implicit-bias-asymp}, this applies to GD iterates in the overparameterized regime. 

Combining this with our preceding discussion, we see that for \emph{every} well-specified but overparameterized logistic regression problem, GD is calibrated and consistent (for logistic risk) when early stopped, but is poorly calibrated and inconsistent  (for logistic risk) at convergence.
This contrast demonstrates the benefit of early stopping.




\subsection{Zero-One Error} 
The preceding lower bounds in \Cref{thm:logistic:lower-bound} are tied to the divergence of the norm of the estimators. 
In this part, we show that even when properly normalized, interpolating estimators are still inferior to early-stopped GD. 
To this end, we consider the zero-one error that is insensitive to the estimator norm. We provide a lower bound on that for interpolating estimators in the next theorem. The proof is deferred to \Cref{sec:proof:zero-one:lower-bound}.

\begin{theorem}[A lower bound for zero-one error]\label{thm:zero-one:lower-bound}
Suppose that \Cref{assump:data-distribution} holds.
Let $C_2> C_1>1$ be two sufficiently large constants. 
Assume that $\SigmaB^{1/2}\wB^*$ is $k$-sparse and $1/C_1 \le \|\wB^*\|_{\SigmaB} \le C_1 $.
Assume that  
\begin{align*}
    n \ge C_1  k  \ln(k /\delta),\quad 
     C_1  \le \frac{\rank(\SigmaB) }{n \ln(n)\ln(1/\delta)}\le C_2 .
\end{align*} 
Then with probability at least $1-\delta$, for every interpolating estimator $\hat\wB$ such that $\min_{i\in[n]}y_i\xB_i^\top \hat\wB > 0$, we have
\begin{align*}
    \error(\hat \wB)-\min\error
    \gtrsim \frac{1}{\sqrt{\ln(n)\ln(1/\delta)}}.
\end{align*}
\end{theorem}

\Cref{thm:zero-one:lower-bound} characterizes a class of overparameterized logistic regression problems where every interpolating estimator needs at least an \emph{exponential} number of training data to achieve a small excess zero-one error. 
This applies to asymptotic GD as it converges to the maximum $\ell_2$-margin solution by \Cref{prop:implicit-bias-asymp}.
In contrast, \Cref{thm:gd:upper-bound,thm:gd:fast-upper-bound} suggests that early-stopped GD can achieve a small excess zero-one error using at most a \emph{polynomial} number of training data under weak conditions. These weak conditions can be, for example, $\|\wB^*\|<\infty$ or the sparsity parameter $k$ does not grow with $n$.
This separation underscores the benefits of early stopping for reducing sample complexity.

\paragraph{Future direcition.}
Note that \Cref{thm:zero-one:lower-bound} applies to \emph{every} interpolating estimator. When restricted to the maximum $\ell_2$-margin estimator, the one that GD converges to in direction, we conjecture that a \emph{constant} lower bound on the excess zero-one error can be proved, especially when the spectrum of the data covariance matrix decays fast. This is left for future investigation. 


\section{Early Stopping and \texorpdfstring{$\ell_2$}{l2}-Regularization}\label{sec:l2-regularization}
In this section, we establish connections between early stopping and $\ell_2$-regularization.
An $\ell_2$-regularized \emph{empirical risk minimizer} (ERM) is defined as
\begin{equation}\label{eq:l2-regularization}
    \uB_{\lambda} := \arg\min_{\uB} \eRisk(\uB) + \frac{\lambda}{2} \|\uB\|^2,
\end{equation}
where $\lambda>0$ is the regularization strength. Note that $\uB_{\lambda}$ is unique and well-defined as long as $\eRisk(\cdot)$ is convex, whereas $\eRisk(\cdot)$ does not have to have a finite minimizer. We refer to $(\uB_{\lambda})_{\lambda>0}$ 
given by \Cref{eq:l2-regularization} 
as the \emph{$\ell_2$-regularization path}. Similarly, we refer to $(\wB_t)_{t> 0}$ given by \Cref{eq:GD} as the \emph{GD path}.

In linear regression, prior works showed that the excess risk of early-stopped GD (and one-pass SGD) is comparable to that of $\ell_2$-regularized ERM \citep{ali2019continuous,zou2021benefits}. 
For strongly convex and smooth problems, \citet{suggala2018connecting} provided bounds on the $\ell_2$-distance between the GD and $\ell_2$-regularization paths. 
In what follows, we establish more connections between the GD and $\ell_2$-regularization paths. We first establish a relative but global connection in convex (not necessarily strongly convex) and smooth problems, then we establish an asymptotic but absolute connection in overparameterized logistic regression problems.


\subsection{A Global Connection}
The following theorem presents a global comparison of the norm and angle between the GD and $\ell_2$-regularization paths. The proof exploits the implicit regularization results in \Cref{lemma:implicit-regularization} and is included in \Cref{sec:proof:path:global-angle}.

\begin{theorem}[A global bound]\label{thm:path:global-angle}
Let $\eRisk(\cdot)$ be convex and $\beta$-smooth. Consider $(\wB_t)_{t\ge 0}$ given by \Cref{eq:GD} with stepsize $\eta \le 1/\beta$ and $(\uB_\lambda)_{\lambda > 0}$ given by \Cref{eq:l2-regularization}. 
Set $\lambda := 1/(\eta t)$. Then we have 
\begin{align*}
\text{for every $t>0$,} \quad   \|\wB_t - \uB_\lambda \|\le \frac{1}{\sqrt{2}}\|\wB_t\|.
\end{align*}
As a direct consequence, the following holds for every $t>0$: 
\begin{gather*}
\cos(\wB_t, \uB_\lambda) \ge \frac{1}{\sqrt{2}},\quad
    \frac{\sqrt{2}}{1+\sqrt{2}} \|\uB_\lambda\| \le \|\wB_t\| \le \frac{\sqrt{2}}{\sqrt{2}-1}\|\uB_\lambda\|.
\end{gather*}
\end{theorem}

\Cref{thm:path:global-angle} establish a global but relative connection between the GD and $\ell_2$-regularization paths for all convex and smooth problems. Specifically, starting from the same zero initialization, the angle between the two paths is no more than $\pi/4$, and the norm of the two paths differs by a factor within $0.585$ and $3.415$. We point out this relative connection holds \emph{globally} for every stopping time (with its corresponding regularization strength) and for every convex and smooth problem.
In particular, it applies to overparameterized logistic regression, which is smooth and convex but not strongly convex. We also note that using the norm bounds in \Cref{thm:path:global-angle}, the upper bounds in \Cref{thm:gd:upper-bound,thm:gd:fast-upper-bound} for early-stopped GD can be easily adapted to $\ell_2$-regularized ERM. 

\Cref{thm:path:global-angle} cannot be improved without making further assumptions. This is because the GD and $\ell_2$-regularization paths could converge to two distinct limits (as $t\to\infty$ and $\lambda\to 0$) in convex but non-strongly convex problems \citep[see][Section 4]{suggala2018connecting}. 
So in general, we cannot expect their distance to be small in the absolute sense.

\subsection{An Asymptotic Comparison}
We have established a global but relative connection between the GD and $\ell_2$-regularization paths in \Cref{thm:path:global-angle}. We now turn to logistic regression with linearly separable data and establish an absolute but asymptotic connection between the two paths. 

In logistic regression with linearly separable data, both GD and $\ell_2$-regularization paths diverge to infinite in norm (as $t\to\infty$ and $\lambda\to 0$) while converging in direction to the maximum $\ell_2$-margin solution \citep{rosset2004boosting,soudry2018implicit,ji2018risk,ji2020gradient}. 
Therefore their angle tends to zero asymptotically \citep{suggala2018connecting,ji2020gradient}. 
This characterization is more precise than the $\pi/4$ global angle bound from \Cref{thm:path:global-angle}.

However, it remains unclear how the $\ell_2$-distance between the two paths evolves in logistic regression with linearly separable data.
Quite surprisingly, we will show that their $\ell_2$-distance tends to zero under a widely used condition \citep{soudry2018implicit,ji2021characterizing,wu2023implicit}, but could diverge to infinity in the worst case. 

Let $\XB := (\xB_1,\dots,\xB_n)^\top$ and $\yB:=(y_1,\dots,y_n)^\top$ be a set of linearly separable data.
Then the Lagrangian dual of the margin maximization program in \Cref{prop:implicit-bias-asymp} is given by \citep[see][for example]{hsu2021proliferation} 
\begin{align*}
    \max_{\betaB \in \Rbb^n}\ \  -\frac{1}{2}\betaB^\top \XB\XB^\top\betaB + \betaB^\top \yB \quad
    \text{s.t.}\ \  y_i \betaB_i \ge 0,\ i\in[n].
\end{align*}
Here, $\betaB$ are the dual variables multiplied by $\yB$ entry-wise.
Let $\hat\betaB$ be the solution to the above problem. 
Let $\Scal := \{i\in [n]: y_i \hat\betaB_i > 0\}$ be the set of support vectors (with strictly positive dual variables). 
The following condition assumes the coverage of the support vectors.

\begin{assumption}
[Support vectors condition]
\label{assump:full-rank-support}
Assume that $\rank\{\xB_i: i \in \Scal\} = \rank\{\xB_i: i \in [n]\}$.
\end{assumption}

\Cref{assump:full-rank-support} has been widely used in the analysis of the implicit bias \citep{soudry2018implicit,ji2021characterizing,wu2023implicit}.
In particular, \Cref{assump:full-rank-support} holds if every data is a support vector, which is common in high-dimensional situations \citep{hsu2021proliferation,wang2022binary,cao2021risk}.

The following theorem provides an asymptotic bound on the $\ell_2$-distance between the GD and $\ell_2$-regularization paths under \Cref{assump:full-rank-support}. The proof is deferred to \Cref{sec:proof:path:point-wise-distance}.

\begin{theorem}[An asymptotic bound]\label{thm:path:point-wise-distance}
Let $(\xB_i, y_i)_{i=1}^n$ be a linearly separable dataset that satisfies \Cref{assump:full-rank-support}. 
Let $(\wB_t)_{t>0}$ and $(\uB_{\lambda})_{\lambda>0}$ be the GD and  $\ell_2$-regularization paths, respectively, for logistic regression with $(\xB_i, y_i)_{i=1}^n$.
Then there exists $\lambda$ as a function of $t$ such that 
\[\|\wB_t - \uB_{\lambda(t)}\| \to 0,
\quad \text{while}\ \ 
\|\wB_t\|, \|\uB_{\lambda(t)}\| \to\infty,
\]  
as $t\to\infty$.
\end{theorem}
For logistic regression with linearly separable data under \Cref{assump:full-rank-support}, \Cref{thm:path:point-wise-distance} shows that the $\ell_2$-distance between the GD and $\ell_2$-regularization paths tends to zero, despite that both paths diverge to infinity in their norm.
Note that this implies their angle converges to zero, and is more precise than the relative norm bound from \Cref{thm:path:global-angle}.

However, this sharp asymptotic connection is strongly tied to \Cref{assump:full-rank-support}. 
Surprisingly, when \Cref{assump:full-rank-support} fails to hold, the $\ell_2$-distance between the GD and $\ell_2$-regularization paths could tend to infinity instead. 
This is shown in the following theorem. The proof is deferred to \Cref{sec:proof:path:counter-example}.

\begin{theorem}[A counter example]\label{thm:path:counter-example}
Consider the following dataset
    \begin{align*}
        \xB_1 := (
            \gamma, 
            0)^\top
        ,\ y_1 := 1,\ \  
        \xB_2 :=(
            \gamma, 
            \gamma_2
      )^\top,\ y_2:=1,
\end{align*}
where $ 0< \gamma_2 < \gamma<1$.
Then $(\xB_i, y_i)_{i=1,2}$ is linearly separable but violates \Cref{assump:full-rank-support}.
Let $(\wB_t)_{t\ge 0}$ and $(\uB_{\lambda})_{\lambda \ge 0}$ be the GD and $\ell_2$-regularization paths respectively for logistic regression with $(\xB_i, y_i)_{i=1,2}$.
Then $\|\wB_t\| \to\infty$ as $t\to \infty$.
Moreover, for \emph{every} map $\lambda: \Rbb_{\ge 0} \to \Rbb_{\ge 0}$, we have 
\begin{align*}
    \|\wB_t - \uB_{\lambda(t)}  \| \gtrsim \ln\ln (\|\wB_t\|) \to \infty.
\end{align*}
\end{theorem}

This simple yet strong counter-example suggests that the $\ell_2$-distance between the GD and $\ell_2$-regularization path can diverge to infinity when \Cref{assump:full-rank-support} fails to hold. 

\paragraph{Future directions.}
We conjecture that for logistic regression with linearly separable data,
the limit of the $\ell_2$-distance between the GD and $\ell_2$-regularized paths is \emph{either zero or infinity}, and the phase transition is determined by a certain geometric property of the dataset (for example, \Cref{assump:full-rank-support}). This is left for future investigation.


\section{Related Works}
We now discuss additional related works.


\paragraph{Benign overfitting in logistic regression.} 
A line of work shows the benign overfitting of the asymptotic GD (or the maximum $\ell_2$-margin estimator) in overparameterized logistic regression under a variety of assumptions \citep{montanari2019generalization,chatterji2021finite,cao2021risk,wang2022binary,muthukumar2021classification,shamir2023implicit}. 
Our results are not a violation of theirs, instead, we show an additional regularization of early-stopping, which brings statistical advantages of early-stopped GD over asymptotic GD such as calibration and a smaller sample complexity. 






\paragraph{M-estimators for logistic regression.}
In the classical finite $d$-dimensional setting, the sample complexity of the \emph{empirical risk minimizer} (ERM) for logistic regression is well-studied \citep{ostrovskii2021finite,kuchelmeister2024finite,hsu2024sample,chardon2024finite}, where the minimax rate is known to be $\Ocal(d/n)$.
Different from theirs, we focus on an overparameterized regime, where the ERM of logistic regression does not even exist. When specialized to their setting, our \Cref{thm:gd:fast-upper-bound} recovers the comparable $\tilde{\Ocal}(d/n)$ rate.

In the nonparametric setting, the works by \citep{bach2010self,marteau2019beyond} provided logistic risk bounds for $\ell_2$-regularized ERM. 
\citet{bach2010self} only considered a fixed design setting, whereas \citet{marteau2019beyond} required that $\|\wB^*\|<\infty$. 
Different from theirs, we aim to understand the benefits of the implicit regularization of early-stopping, instead of that of explicit $\ell_2$-regularization.
Moreover, we show that early-stopped GD achieves a vanishing excess logistic risk as long as $\|\wB^*\|_{\SigmaB}<\infty$, without assuming a finite $\|\wB^*\|$. In the regimes where our results are directly comparable, however, our risk bounds might be less tight than theirs. We leave it as a future work to improve our current bounds.

Finally, \citet{bach2014adaptivity} considered one-pass SGD for logistic regression assuming strong convexity around the true model parameter.
This strong convexity assumption, however, is prohibitive in our high-dimensional settings. 



\paragraph{Separable distribution.}
There are logistic risk bounds of early-stopped GD (and one-pass SGD) developed in the \emph{noiseless} cases, assuming a separable population distribution \citep{ji2018risk,shamir2021gradient,telgarsky2022stochastic,schliserman2024tight}. 
These results do not imply any benefits of early stopping, as their setting is noiseless. 
In comparison, we consider overparameterized logistic regression with a strictly non-separable population distribution, where the risk of overfitting is prominent.
In this case, our results suggest that early stopping plays a significant role in preventing overfitting.

\paragraph{Early stopping for classification.}  
In the boosting literature, an early work by \citet{zhang2005boosting}
showed that boosting methods (that can be interpreted as coordinate descent) with early stopping
are consistent in the classification sense;
related refined studies for boosting with the squared loss with
early stopping were also provided by \citet{buhlmann2003boosting}.  
The paper is also notable for giving the first proof of boosting methods converging to the maximum margin solution \citep[Appendix D]{zhang2005boosting}, which was later
refined with rates by \citep{telgarsky2013margins}. 
Their results can be converted to GD.
In particular, related concepts were used to prove consistency
of early-stopped GD for shallow networks in the lazy regime \citep{ji2021early}.
In contrast with the present work that focuses on high-dimensional cases, the preceding works only deal with finite-dimensional settings.  Moreover, none of those works provide lower bounds for interpolating estimators and tight links to the regularization path which are provided in the present work.







\paragraph{Classification calibration.}  \Cref{prop:risk-properties}
captures a very nice consequence of logistic loss minimization:
\emph{calibration} and \emph{classification-calibration}, respectively
recovery of the optimal conditional probability model and of the
optimal classifier.  For more general convex losses, the ability
to construct a general conditional probability model was
developed by \citet{zhang_consistency} as a conceptual tool in establishing
classification calibration, but without explicitly controlling
calibration error. A further abstract treatment of classification
calibratoin was later presented by \citet{bartlett_jordan_mcauliffe}.
The refined statistical rates, separations, and early-stopping
consequences studied in the present work were not considered in those
works.

\section{Conclusion}
We show the benefits of early stopping in GD for overparameterized and well-specified logistic regression. We show that for every well-specified logistic regression problem, early-stopped GD is calibrated while asymptotic GD is not. 
Furthermore, we show that early-stopped GD achieves a small excess zero-one error with only a polynomial number of samples, in contrast to interpolating estimators, including asymptotic GD, which require an exponential number of samples to achieve the same.  
Finally, we establish nonasymptotic bounds on the differences between the GD and the $\ell_2$-regularization paths.
Altogether, we underscore the statistical benefits of early stopping, partially explained by its connection with $\ell_2$-regularization.


\section*{Acknowledgments}
We gratefully acknowledge the support of the NSF for FODSI through grant DMS-2023505, of the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and \#814639, of the NSF through grants DMS-2209975 and DMS-2413265, and of the ONR through MURI award N000142112431.
The authors are also grateful to the Simons Institute for hosting them during
parts of this work.




\bibliography{ref}


\newpage
\appendix






\section{Basic Results}
\subsection{Proof of \texorpdfstring{\Cref{prop:risk-properties}}{Proposition 2.1}}\label{sec:proof:risk-properties}

\begin{proof}[Proof of \Cref{prop:risk-properties}]
We first compute the logistic loss. Define 
\begin{align*}
    p_{\xB}(\wB) := \frac{1}{1+\exp(-\xB^\top \wB)},\quad p^*_{\xB} := \frac{1}{1+\exp(-\xB^\top \wB^*)}.
\end{align*}
Then under \Cref{assump:data-distribution} we have 
\begin{align*}
\risk(\wB) &= \Ebb_{\xB, y} \ln(1+\exp(-y \xB^\top \wB))  \\
&= - \Ebb_{\xB} \Big( p^*_{\xB} \ln p_{\xB}(\wB) + (1- p^*_{\xB} )\ln \big( 1- p_{\xB}(\wB) \big)  \Big)\\
&= \Ebb_{\xB} \big(  H(p^*_{\xB}) + \KL{p^*_{\xB}}{p_{\xB}(\wB)} \big),
\end{align*}
where the first term is the entropy of a Bernoulli distribution with a head probability of $p^*_{\xB}$, and the second term is the KL divergence between two Bernoulli distributions with head probabilities of $p^*_{\xB}$ and $p_{\xB}(\wB)$, respectively.
It is then clear that $\wB^*$ is the unique minimizer of $\risk(\wB)$.

We then compute the zero-one error under \Cref{assump:data-distribution}. A similar calculation can be found in, for example, Lemma 4.5 in \citep{mohri2018foundations}.
Note that $\error(0)= 1$.
If $\wB^*=0$, then for every $\wB$,
\begin{align*}
 \error(\wB) = \Ebb_{\xB,y}\ind{y \xB^\top \wB \le 0} 
 = \Ebb_{\xB} 0.5 \big( \ind{\xB^\top \wB \le 0} + \ind{\xB^\top \wB \ge 0}  \big)
 \ge 1 = \error(\wB^*),
\end{align*}
so $\wB^* \in \arg\min\error(\cdot)$.
If $\wB^*\ne 0$, we have $\xB^\top\wB^*=0$ is measure zero. Then we have
\begin{align*}
     \error(\wB^*) = \Ebb_{\xB} \big( p_{\xB}^* \ind{\xB^\top \wB^* \le 0} + (1-p_{\xB}^*) \ind{\xB^\top \wB^* \ge 0} \big) 
     = \Ebb_{\xB} \min\{p_{\xB}^*, 1-p_{\xB}^*\} 
     \le 0.5 < \error (0).
\end{align*}
It remains to check that $\error(\wB^*) \le \error(\wB)$ for all $\wB \ne 0$.
When $\wB^*$ and $\wB$ are both non-zero, we have $\xB^\top\wB^*=0$ and $\xB^\top\wB=0$ are measure zero, 
then we have 
\begin{align*}
    \error(\wB) -\error(\wB^*)
    &= \Ebb_{\xB, y} \ind{y \xB^\top \wB \le 0}-\ind{y \xB^\top \wB^* \le 0} \\
    &= \Ebb_{\xB} p^*_{\xB}\big(  \ind{\xB^\top \wB \le 0} - \ind{\xB^\top \wB^*\le 0} \big) +  (1-p^*_{\xB}) \big( \ind{\xB^\top \wB \ge 0} -\ind{\xB^\top \wB^* \ge 0} \big) \\
    &= \Ebb_{\xB} p^*_{\xB}\big(  \ind{\xB^\top \wB \le 0} - \ind{\xB^\top \wB^*\le 0} \big) +  (1-p^*_{\xB}) \big( \ind{\xB^\top \wB^* < 0} -\ind{\xB^\top \wB < 0} \big) \\ 
    &= \Ebb_{\xB}(2p^*_{\xB}-1) \big( \ind{\xB^\top \wB \le 0} -\ind{\xB^\top \wB^* \le 0} \big) \\
    &= \Ebb_{\xB}(2p^*_{\xB}-1)  \ind{\xB^\top \wB \le 0}\ind{\xB^\top \wB^* > 0} + (1-2p^*_{\xB}) \ind{\xB^\top \wB > 0} \ind{\xB^\top \wB^* \le 0}  \\
    &= \Ebb_{\xB}| 2p^*_{\xB}-1 | \ind{ \xB^\top \wB \xB^\top \wB^*\le 0},
\end{align*}
which is always non-negative. Therefore we have $\wB^* \in \arg\min\error(\wB)$. We complete the proof of the first claim.



The second claim follows from the following. The calculation is similar to the proof of Theorem 4.7 in \citep{mohri2018foundations}.
\begin{align*}
\error(\wB) - \error(\wB^*)    
&= 2 \Ebb_{\xB}|p^*_{\xB} - 1/2| \ind{ \xB^\top \wB \xB^\top \wB^*\le 0}\\
    &\le 2 \Ebb_{\xB}  | p^*_{\xB} - p_{\xB}(\wB) |\ind{ \xB^\top \wB \xB^\top \wB^*\le 0} \\
    &\le 2 \Ebb_{\xB}  | p^*_{\xB} - p_{\xB}(\wB) | \\
    &\le 2 \sqrt{\Ebb_{\xB}  | p^*_{\xB} - p_{\xB}(\wB) |^2 } \\
    &\le 2 \sqrt{\frac{1}{2} \Ebb_{\xB}  \KL{  p^*_{\xB}}{ p_{\xB}(\wB) } }\\
    &= \sqrt{2}\cdot \sqrt{L(\wB) - L(\wB^*)},
\end{align*}
where the last inequality is by Pinsker's inequality.
We complete the proof of the second claim.

We now prove the third claim under \Cref{assump:data-distribution}. 
Notice that
\begin{align*}
    \risk(\wB)
    = \Ebb \ln(1+\exp(-y\xB^\top \wB))  
    \ge \Ebb \ln(2) \ind{y \xB^\top \wB\le 0}  
    = \ln (2) \cdot \error(\wB),\quad \text{for all}\ \wB.
\end{align*}
Therefore a lower bound on $\error(\wB^*)$ implies a lower bound on $\risk(\wB^*)$.
We lower bound $\error(\wB^*)$ by
\begin{align*}
    \error(\wB^*) &= \Ebb\ind{y \xB^\top \wB^* \le 0} \\ 
    &= \Ebb \ind{\xB^\top\wB^* \le 0 } \frac{1}{1+\exp(-\xB^\top \wB^*)}+\ind{\xB^\top\wB^* \ge 0} \frac{1}{1+\exp(\xB^\top \wB^*)} \\
    &= 2 \Ebb_{g\sim \Ncal(0,1)}\frac{1}{1+\exp(g \|\wB^*\|_{\SigmaB})}\ind{g\ge 0} \\
    &= \frac{\sqrt{2}}{\pi} \int_{0}^\infty \frac{\exp(-g^2/2)}{1+\exp(g\|\wB^*\|_{\SigmaB})}\dif g \\
    &\ge \frac{\sqrt{2}}{2\pi} \int_{0}^\infty \exp(-g^2/2)\exp(-g\|\wB^*\|_{\SigmaB})\dif g \\
    &= \frac{\sqrt{2}}{2\pi} \cdot \exp\big(\|\wB^*\|_{\SigmaB}^2/2\big) \int_{0}^\infty\exp\big(-(g+\|\wB^*\|_{\SigmaB})^2/2\big)\dif g \\
    &\ge \frac{\sqrt{2}}{2\pi}\cdot  \frac{\sqrt{2}}{\|\wB^*\|_{\SigmaB}/\sqrt{2} + \sqrt{\|\wB^*\|^2_{\SigmaB}/2+2}} \\
&\ge \frac{1}{\sqrt{2}\pi(\|\wB^*\|_{\SigmaB} + 1)},
\end{align*}
where we use the following error bounds \citep{abramowitz1965handbook},
\begin{align*}
    \frac{1}{x+\sqrt{x^2+2}}\le e^{x^2}\int_{x}^{\infty}e^{-t^2}\dif t \le \frac{1}{x+\sqrt{x^2+4/\pi}},\quad x\ge 0.
\end{align*}
This completes the proof.
\end{proof}

\subsection{Proof of \texorpdfstring{\Cref{lemma:implicit-regularization}}{Lemma 3.3}}\label{sec:proof:implicit-regularization}

\begin{proof}[Proof of \Cref{lemma:implicit-regularization}]
For $\eta\le 1/\beta$, we have the descent lemma, that is,
\begin{align*}
    \eRisk(\wB_{t+1}) 
    \le \eRisk (\wB_t) - \eta \|\grad \eRisk (\wB_t)\|^2+ \frac{\eta^2 \beta}{2} \|\grad \eRisk (\wB_t)\|^2 
    \le \eRisk (\wB_t) - \frac{\eta}{2} \|\grad \eRisk (\wB_t)\|^2.
\end{align*}
Then for the quadratic potential centered at $\uB$, we have 
\begin{align*}
    \|\wB_{t+1}-\uB\|^2 
    &= \|\wB_{t}-\uB\|^2 + 2\eta \la\grad \eRisk (\wB_t), \uB - \wB_t \ra + \eta^2 \|\grad \eRisk(\wB_t)\|^2  \\
    &\le \|\wB_{t}-\uB\|^2 + 2\eta \big(\eRisk(\uB) - \eRisk (\wB_t)\big) + 2\eta \big( \eRisk (\wB_t)-\eRisk (\wB_{t+1})\big)  \\
    &=  \|\wB_{t}-\uB\|^2 + 2\eta \big(\eRisk(\uB) - \eRisk (\wB_{t+1})\big),
\end{align*}
where the inequality is due to the convexity and the descent lemma.
Telescoping the sum and rearranging, we have 
\begin{align*}
    \frac{\|\wB_t - \uB\|^2}{2\eta t} + \frac{1}{t}\sum_{k=1}^{t}\eRisk(\wB_k) \le \eRisk(\uB) + \frac{\|\wB_0 - \uB\|^2}{2\eta t}.
\end{align*}
Using the descent lemma and the initial condition $\wB_0=0$, we have 
\begin{align*}
    \frac{\|\wB_t - \uB\|^2}{2\eta t} + \eRisk(\wB_t) \le \eRisk(\uB) + \frac{\|\uB\|^2}{2\eta t}.
\end{align*}
This completes the proof.
\end{proof}

\section{Upper Bounds for Early-Stopped GD}
\subsection{Proof of \texorpdfstring{\Cref{thm:gd:upper-bound}}{Theorem 3.1}}\label{sec:proof:gd:upper-bound}
\begin{lemma}\label{lemma:norm-bound}
Let  
\(\beta:= C_0 \big( 1+ \tr(\SigmaB) + \lambda_1 \ln(1/\delta)/n \big)\),
where $C_0>1$ is a sufficiently large constant.
Assume that $\eta \le 1/\beta$ and $t$ is such that $\eRisk(\wB^*_{0:k})\le \eRisk(\wB_{t-1})$.
Then with probability at least $1-\delta$, we have $\|\wB_t-\wB^*_{0:k}\|\le 1+\|\wB_{0:k}\| $.
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:norm-bound}]
We first show that the following holds with probability at least $1-\delta$:
\begin{align*}
        \big\|\grad \eRisk(\wB) \big\| \le \sqrt{\beta},\quad 
        \big\|\grad^2 \eRisk(\wB) \big\| \le \beta.
\end{align*}
This is because 
\begin{align*}
    \big\|\grad \eRisk(\wB)\big\| = \bigg\| \frac{1}{n}\sum_{i=1}^n \ell'(y_i \xB_i^\top \wB) y_i \xB_i\bigg\| 
    \le \frac{1}{n} \sum_{i=1}^n \|\xB_i\| 
    \le \sqrt{\frac{1}{n} \sum_{i=1}^n \|\xB_i\|^2},
\end{align*}
and 
\begin{align*}
    \big\|\grad^2 \eRisk(\wB) \big\| = \bigg\|\frac{1}{n}\sum_{i=1}^n \ell''(y_i \xB_i^\top \wB) \xB_i \xB_i^\top \bigg\| 
    \le \frac{1}{n} \sum_{i=1}^n \|\xB_i\|^2.
\end{align*}
So it remains to bound $\sum_{i=1}^n \|\xB_i\|^2$.
Let $z_{ij}$'s be independent Gaussian random variables, then by \Cref{assump:data-distribution} and Bernstein's inequality, we have the following with probability at least $1-\delta$:
\begin{align*}
    \sum_{i=1}^n \|\xB_i\|^2 &= \sum_{i=1}^n \sum_{j}\lambda_j z_{ij}^2
    \le n \tr(\Sigma) + C_1\bigg( \sqrt{n \sum_{j}\lambda_j^2 \ln(1/\delta)} + \lambda_1 \ln(1/\delta) \bigg) \\
    &\le C_0 \big( n \tr(\Sigma) +\lambda_1 \ln(1/\delta) \big) \le \beta,
\end{align*}
where $C_0, C_1>1$ are constants.

So far we have shown $\eRisk$ is $\beta$-smooth and $\sqrt{\beta}$-Lipschitz for $\beta>1$. 
By the stopping criterion, we have $\eRisk(\wB^*_{0:k})\le \eRisk(\wB_{t-1})$.
Then by applying \Cref{lemma:implicit-regularization} to $\wB_{t-1}$ and $\uB=\wB^*_{0:k}$, we have 
\begin{align*}
  &  \|\wB_{t-1}-\wB^*_{0:k}\|^2 \le 2\eta (t-1) \big(\eRisk(\wB^*_{0:k}) - \eRisk(\wB_{t-1}) \big) + \|\wB^*_{0:k}\|^2 \le \|\wB^*_{0:k}\|^2 \\
    \quad \Rightarrow \quad &
    \|\wB_{t-1} - \wB^*_{0:k}\|\le \|\wB^*_{0:k}\|.
\end{align*}
Then by the Lipschitzness, we get 
\begin{align*}
   \|\wB_{t} - \wB^*_{0:k}\|\le \|\wB_{t} - \wB_{t-1}\| + \|\wB_{t-1} - \wB^*_{0:k}\|\le \eta \sqrt{\beta} + \|\wB^*_{0:k}\|
   \le \frac{1}{\sqrt{\beta}} + \|\wB^*_{0:k}\| \le 1 + \|\wB^*_{0:k}\|,
\end{align*}
where we use $\big\|\grad \eRisk(\wB)\big\|\le \sqrt{\beta}$, $\eta\le 1/\beta$, and $\beta\ge 1$. This completes the proof.
\end{proof}


\begin{lemma}\label{lemma:loss-tail}
Let $\wB^*\in \arg\min \risk(\wB)$, then for every $\wB$, we have 
\begin{align*}
\risk(\wB) \le \risk(\wB^*) + \frac{1}{2}\|\wB - \wB^*\|_{\SigmaB}^2.   
\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:loss-tail}]
Notice that 
\begin{align*}
    \grad^2 \risk(\wB) = \Ebb \ell''(y \xB^\top \wB) \xB \xB^\top 
    = \Ebb \frac{\xB \xB^\top}{\big(1+\exp(\xB^\top \wB)\big)\big(1+\exp(-\xB^\top \wB)\big)} \preceq \Ebb \xB\xB^\top = \SigmaB.
\end{align*}
Moreover, we have $\grad \risk(\wB^*) = 0$.
Then by the midpoint theorem, there exists a $\vB$ such that
\begin{align*}
    \risk(\wB) 
    - \risk(\wB^*) = \la \grad \risk(\wB^*), \wB-\wB^*\ra + \frac{1}{2} (\wB-\wB^*)^\top \grad^2 \risk(\vB)(\wB-\wB^*) 
    \le  \frac{1}{2}\|\wB-\wB^*\|^2_{\SigmaB}.
\end{align*}
This completes the proof.
\end{proof}


\begin{lemma}\label{lemma:rad-concentration}
Let $C_1>1$ be a sufficiently large constant. 
Then with probability at least $1-\delta$, 
\begin{align*}
\sup_{\|\wB\|\le W}    |\risk(\wB)-\eRisk(\wB)| \le C_1 W \sqrt{\frac{ (1+\tr(\SigmaB))\ln(n/\delta)\ln(1/\delta)}{n}}.
\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:rad-concentration}]
This is by standard Rademacher complexity arguments for $1$-Lipschitz loss and linear function class \citep{bartlett2002rademacher,kakade2008complexity}.

Similarly to the proof of \Cref{thm:gd:fast-upper-bound}, let $\tilde\xB:= \xB\ind{\|\xB\|\le X}$ where $X$ is a constant to be determined. 
Then for $\tilde\xB$ and hypothesis class $\{\wB: \|\wB\|\le W\}$, the loss $\ell(y\tilde\xB^\top \wB)$ is bounded by $|y\tilde\xB^\top \wB|\le WX$. By Corollary 4 in \citep{kakade2008complexity}, the following holds with probability at least $1-\delta$:
for all $\wB$ such that $\|\wB\|\le W$,
\begin{align*}
    \bigg| \Ebb\ell(y\tilde\xB^\top \wB) - \frac{1}{n}\sum_{i=1}^n \ell(y_i\tilde\xB_i^\top\wB) \bigg| 
    \le  2XW\sqrt{\frac{1}{n}} + XW\sqrt{\frac{\ln(1/(2\delta))}{2n}}.
\end{align*}
For
\[
X^2 = C\big(1+\tr(\SigmaB)\big)\ln(n /\delta),
\]
by \Cref{lemma:loss-small-prob}, we have 
\[
\sup_{\|\wB\|\le W} \big| L(\wB) - \Ebb\ell(y\tilde\xB^\top \wB) | \le \frac{W}{n}.
\]
Moreover, we have with probability $1-\delta$, $\xB_i=\tilde\xB_i$ for $i=1,\dots,n$, which implies \[\eRisk(\wB) = \frac{1}{n}\sum_{i=1}^n \ell(y_i\tilde\xB_i^\top\wB) .\]
Putting things together with union bound, with probability at least $1-\delta$, we have 
\begin{align*}
\sup_{\|\wB\|\le W} \big(   L(\wB) - \eRisk(\wB) \big) 
&\le \frac{W}{n} +  2XW\sqrt{\frac{1}{n}} + XW\sqrt{\frac{\ln(1/(3\delta))}{2n}} \\
&\le C_1 W \sqrt{\frac{\big(1+\tr(\SigmaB)\big)\ln(n/\delta)\ln(1/\delta)}{n}},
\end{align*}
where $C_1 >1$ is a constant.
\end{proof}



\begin{proof}[Proof of \Cref{thm:gd:upper-bound}]
By the stopping condition and \Cref{lemma:norm-bound}, with probability at least $1-\delta$, we have 
\begin{align*}
\eRisk(\wB_t)\le \eRisk(\wB^*_{0:k}),\quad \|\wB_t-\wB^*_{0:k}\|\le 1 + \|\wB^*_{0:k}\|.
\end{align*}
Let $W:= 1+2\|\wB^*_{0:k}\|$.
Then by \Cref{lemma:loss-tail,lemma:rad-concentration}, with probability at least $1-\delta$, we have
\begin{align*}
&\lefteqn{\risk(\wB_t)-\risk(\wB^*) }\\
&= \risk(\wB_t)-\eRisk(\wB_t) + \eRisk(\wB_t)-\eRisk(\wB^*_{0:k}) + \eRisk(\wB^*_{0:k})-\risk(\wB^*_{0:k}) + \risk(\wB^*_{0:k}) -  \risk(\wB^*) \\  
&\le C_1 W \sqrt{\frac{ \big(1+\tr(\SigmaB)\big) \ln(n/\delta)\ln(1/\delta)}{n}} + 0 + C_1 W \sqrt{\frac{ \big(1+\tr(\SigmaB)\big) \ln(n/\delta)\ln(1/\delta)}{n}} + \frac{1}{2} \|\wB^*_{k:\infty}\|^2_{\SigmaB} \\ 
&\le C (1+\|\wB^*_{0:k}\|) \sqrt{\frac{ \big(1+\tr(\SigmaB)\big) \ln(n/\delta)\ln(1/\delta)}{n}}  + \frac{1}{2} \|\wB^*_{k:\infty}\|^2_{\SigmaB}.
\end{align*}
This completes the proof.
\end{proof}


\subsection{Proof of \texorpdfstring{\Cref{thm:gd:fast-upper-bound}}{Theorem 3.2}}\label{sec:proof:gd:fast-upper-bound}
Suppose that \Cref{assump:data-distribution} holds throughout this subsection.
In this subsection, we define
\begin{align*}
    W& := 1+2\|\wB^*\|,\\
X^2 &: = 2\tr(\SigmaB) + 2C(1+\lambda_1) \ln\Big(4C_1 \big(1+\lambda_1^{3/2} W^3\big) n \sqrt{\tr(\SigmaB)}/\delta\Big) \\
    L &:= WX,\\ 
    B &:= \frac{4C_1(1+\lambda_1^{3/2} W^3)}{W^2 X^2} \lesssim \sqrt{\lambda_1} W,
\end{align*}
where $C_0, C_1 > 1$ are two sufficiently large constants.
We aim to use tools from \citep{bartlett2005local}. To this end, consider the following random variables, function class, and loss function:
\begin{equation*}
    \tilde\xB := \xB\ind{\|\xB\|\le X},\quad 
    \Fcal := \bigg\{\tilde\xB\mapsto \frac{\wB^\top \tilde\xB}{WX}: \|\wB\| \le W \bigg\},\quad 
    \tilde\ell : t\mapsto  \ell(WX t).
\end{equation*}
It is clear that $\|\tilde\xB\|\le X$ and $f(\tilde\xB) \in [-1,1]$ for every $f\in\Fcal$.
Recall 
that Rademacher complexity is defined as 
\begin{align*}
    \rad_n \Fcal := \sup_{f \in \Fcal} \frac{1}{n}\sum_{i=1}^n \sigma_i f(\tilde\xB_i),
\end{align*}
where $\sigma_i$'s are independent Rademacher random variables.
The following lemma is Corollary 5.3 in \citep{bartlett2005local} restated in our context.
\begin{lemma}[Corollary 5.3 in \citep{bartlett2005local}]\label{thm:risk-local-rad}
Let $\Fcal$ be a class of functions with ranges in $[-1, 1]$ and let $\tilde\ell$ be a loss function satisfying
\begin{enumerate}[leftmargin=*]
    \item There exists $f^*\in\Fcal$ such that $\Ebb \tilde\ell\big(y f^*(\tilde\xB) \big) = \inf_{f\in\Fcal}\Ebb \tilde\ell\big(y f(\tilde\xB) \big)$.
    \item There exists $L$ such that $\tilde\ell$ is $L$-Lipschitz.
    \item There exists $B\ge 1$ such that for every $f\in\Fcal$,
    \[
    \Ebb \big(f(\tilde\xB) - f^*(\tilde\xB) \big)^2 
\le B \Big(\Ebb \tilde\ell\big(y f(\tilde\xB)\big) -\Ebb \tilde\ell\big(y f^*(\tilde\xB)\big) \Big).
\]
\end{enumerate}
Let $\hat f \in \Fcal$ be such that 
\[
\frac{1}{n}\sum_{i=1}^n \tilde\ell(y_i \hat f(\tilde\xB_i)\big) \le \frac{1}{n}\sum_{i=1}^n \tilde\ell(y_i f^*(\tilde\xB_i)\big).
\]
Let $\phi$ be a sub-root function for which 
\begin{align*}
    \psi(r) \ge BL \Ebb \rad_n\{ f\in\Fcal: L^2 \Ebb \big(f(\tilde\xB) - f^*(\tilde\xB) \big)^2 \le r  \}.
\end{align*}
Then for any $r\ge \psi(r)$, with probability at least $1-\delta$, 
\begin{align*}
    \Ebb \tilde\ell\big(y \hat f (\tilde x)\big) -  \Ebb \tilde\ell\big(y  f^* (\tilde x)\big) \le 705 \frac{r}{B} + \frac{(11L + 27B)\ln(1/\delta)}{n}.
\end{align*}
\end{lemma}


The following lemma provides a classical upper bound on the Rademacher complexity for the linear function class.
\begin{lemma}[Theorem 6.5 in \citep{bartlett2005local}]\label{lemma:local-rad}
We have
\begin{align*}
     \Ebb \rad_n \{f \in \Fcal: 
     L^2 \Ebb (f(\tilde\xB) - f^*(\tilde\xB))^2 \le r \} 
& \le  
     \sqrt{\frac{2k}{nL^2}\cdot r + \frac{\sum_{i>k}\lambda_i}{nX^2} },
\end{align*}
where $k$ is an arbitrary index.
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:local-rad}]
The proof is an adaptation of the proof of Theorem 6.5 in \citep{bartlett2005local} in our context. We include it here for completeness. 
Assume $\SigmaB$ is diagonal without loss of generality. 
Let 
\[
\tilde\SigmaB := \max\bigg\{ \frac{1}{W^2}\IB, \ \frac{L^2}{2r W^2 X^2 }\SigmaB \bigg\},
\]
where $\max\{\cdot, \cdot\}$ is applied entry wise.
Recall that $0.5\SigmaB\preceq \Ebb\tilde\xB\tilde\xB^\top\preceq\SigmaB$ by \Cref{lemma:norm-tail}.
Then by definition, we have 
\begin{align*}
    & \Ebb \rad_n \{f \in \Fcal: 
     L^2 \Ebb (f(\tilde\xB) - f^*(\tilde\xB))^2 \le r \} \\
     &\le  \Ebb \sup_{\|\wB\|\le W, \frac{L^2}{W^2X^2} \|\wB - \wB^*\|^2_{\SigmaB} \le 2r}\bigg\la \frac{1}{n}\sum_{i=1}^n \sigma_i\tilde\xB_i, \frac{\wB}{WX}\bigg\ra 
     = \frac{1}{WX}  \Ebb \sup_{ \|\wB\|_{\tilde\SigmaB}\le 1 } \bigg\la \frac{1}{n}\sum_{i=1}^n \sigma_i\tilde\xB_i, \wB\bigg\ra \\
     &\le \frac{1}{WX}  \Ebb \bigg\|\frac{1}{n}\sum_{i=1}^n \sigma_i\tilde\xB_i\bigg\|_{\tilde\SigmaB^{-1}} 
     \le \frac{1}{WX}\sqrt{ \Ebb \bigg\|\frac{1}{n}\sum_{i=1}^n \sigma_i\tilde\xB_i\bigg\|^2_{\tilde\SigmaB^{-1}} }
    = \frac{1}{WX}\sqrt{ \frac{1}{n} \Ebb \big\|\tilde\xB\big\|^2_{\tilde\SigmaB^{-1}} }\\
     &\le  \frac{1}{WX}\sqrt{\frac{1}{n}\big\la \SigmaB, \tilde\SigmaB^{-1} \big\ra  } 
=  \frac{1}{WX}\sqrt{\frac{1}{n}\sum_{i}\min\bigg\{ \frac{2rW^2 X^2}{L^2},\ W^2 \lambda_i \bigg\}  } 
     \le \sqrt{\frac{2k}{nL^2}\cdot r + \frac{\sum_{i>k}\lambda_i}{nX^2} },
\end{align*}
where $k$ is an arbitrary index. This completes the proof.
\end{proof}

The following lemma establishes several basic effects of clipping the random variable $\xB$.
\begin{lemma}\label{lemma:norm-tail}
There exists constant $C>1$ such that for every $X^2 \ge 2\tr(\SigmaB) + C(1+\lambda_1)t$, we have 
\begin{align*}
    \Pr\big(\|\xB\| \ge X \big) \le \exp(-t),\quad t>1.
\end{align*}
In addition, for every $C_1>1$, by setting $t \ge 2 \ln(4C_1 (1+\lambda_1^{3/2} W^3))$ we have
\begin{align*}
    \Ebb\xB\xB^\top\ind{\|\xB\|> X} \preceq \frac{1}{2C_1(1+\lambda_1^{3/2} W^3)} \SigmaB \preceq \frac{1}{2}\SigmaB.
\end{align*}
In particular, this implies $\Ebb \tilde\xB\tilde\xB^\top = \SigmaB - \Ebb\xB\xB^\top\ind{\|\xB\|> X} \ge 0.5\SigmaB$.
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:norm-tail}]
By Bernstein's inequality, there is a constant $c>0$ such that
\begin{align*}
    \Pr\big(\|\xB\|^2 - \tr(\SigmaB)    > z \big) \le \exp\bigg( -c
\min\bigg\{\frac{z^2}{\tr(\SigmaB)},\ \frac{z}{\lambda_1}\bigg\} \bigg).
\end{align*}
We then obtain the first claim by setting $z = X^2 - \tr(\SigmaB)$ and adjusting the constant.






To prove the second claim, for any unit vector $\uB$, we have
\begin{align*}
 \Ebb \big(\xB^\top\uB\big)^2 \ind{\|\xB\|\ge X} 
 &\le \sqrt{\Ebb \big(\xB^\top\uB\big)^4} \cdot \sqrt{\Ebb\ind{\|\xB\|\ge X} }\\
 &\le \sqrt{3} \Ebb \big(\xB^\top\uB\big)^2 \cdot \sqrt{\Pr(\|\xB\|\ge X)} \\
 &=  \uB^\top \SigmaB \uB \cdot \sqrt{3}\exp(-t/2).
\end{align*}
The above implies
\begin{align*}
    \Ebb \xB\xB^\top \ind{\|\xB\|\ge X}  \preceq \sqrt{3}\exp(-t/2)\SigmaB.
\end{align*}
Setting $t \ge 2 \ln(4C_1 (1+\lambda_1 W^3))$ completes the proof.
\end{proof}

The following lemma is from \citep{chardon2024finite}.
\begin{lemma}\label{lemma:population-hessian}
There exists a constant $C_1>0$ such that for every $\wB$, we have
\begin{align*}
    \Ebb \frac{1}{1+\exp(\xB^\top \wB)} \frac{1}{1+\exp(-\xB^\top\wB)} \xB \xB^\top \succeq \frac{1}{C_1(1+\|\wB\|^3_{\SigmaB})} \SigmaB.
\end{align*}    
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:population-hessian}]
Let $s(t) := 1/(1+e^{-t})$. Define
\[\beta := \|\wB\|_{\SigmaB},\quad 
\uB_1 := \frac{1}{\beta}\SigmaB^{1/2}\wB,\quad 
\zB := \SigmaB^{-1/2}\xB\sim \Ncal(0,1).\]
Then we only need to verify that
\begin{align*}
    \Ebb s'(\beta \zB^\top \uB_1) \zB\zB^\top \ge \frac{1}{C (1+\beta^3)}.
\end{align*}
Hitting the left-hand side of the above with $\uB_1$, we get 
\begin{align*}
    \Ebb s'(\beta \zB^\top \uB_1) (\zB^\top \uB_1)^2 
    = \Ebb_{g\sim \Ncal(0,1)} s'(\beta g) g^2  
    \ge \sqrt{\frac{2}{\pi}}\frac{2^3}{3}\min\bigg\{\frac{1}{4e^4\beta^3}, \frac{s'(2)}{e^2}\bigg\} \ge \frac{1}{C_1 (1+\beta^3)},
\end{align*}
where the first inequality is by Lemma 25 in \citet{chardon2024finite}.
Hitting that again with a unit vector $\uB_2$ such that $\uB_1^\top\uB_2=0$, we get 
\begin{align*}
    \Ebb s'(\beta \zB^\top \uB_1) (\zB^\top \uB_2)^2 
= \Ebb_{g\sim \Ncal(0,1)} s'(\beta g) 
    \ge \sqrt{\frac{2}{\pi}}\frac{2^2}{2}\min\bigg\{\frac{1}{4e^4\beta}, \frac{s'(2)}{e^2}\bigg\} \ge \frac{1}{C_1(0.5+\beta)} \ge \frac{1}{C_1(1+\beta^3)},
\end{align*}
where the first inequality is again by Lemma 25 in \citet{chardon2024finite}.
Together, these two lower bounds and the properties of Gaussian complete the proof.
\end{proof}

The following lemma verifies a key condition in \Cref{thm:risk-local-rad}.
\begin{lemma}\label{lemma:population-convexity}
Let 
\(X^2\ge 2\tr(\SigmaB) + 2C(1+\lambda_1)\ln \big(4C_1 (1+\lambda_1^{3/2} W^3)\big)\)
and $ B \ge 4C_1(1+\lambda_1^{3/2} W^3) / (W^2 X^2)$.
Then for every $\|\wB\|\le W$, we have
\begin{align*}
    \Ebb \big(\tilde\xB^\top \wB - \tilde\xB^\top \wB^* \big)^2 \le W^2 X^2 B \big( \Ebb \ell(y \tilde\xB^\top \wB)-\Ebb \ell(y \tilde\xB^\top \wB^*) \big).
\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:population-convexity}]
Since $\Ebb\tilde\xB\tilde\xB^\top \preceq \SigmaB$, it suffices to show that 
\begin{align*}
     \|\wB - \wB^*\|^2_{\SigmaB} \le W^2 X^2 B \big( \Ebb \ell(y \tilde\xB^\top \wB)-\Ebb \ell(y \tilde\xB^\top \wB^*) \big).
\end{align*}
Recall that $\wB^*$ is the minimizer of $\Ebb_y \ell(y \tilde\xB^\top \wB)$ (where the expectation is conditional on $\tilde\xB$, see the proof of \Cref{prop:risk-properties} in \Cref{sec:proof:risk-properties}). By the midpoint theorem, there exists $\vB$ between $\wB$ and $\wB^*$ (thus $\|\vB\|\le W$)
such that 
\begin{align*}
    \Ebb \ell(y \tilde\xB^\top \wB)-\Ebb \ell(y \tilde\xB^\top \wB^*) 
    &= \frac{1}{2} \Ebb \ell''(y \tilde\xB^\top \vB) \big\la \tilde\xB^{\otimes 2}, (\wB - \wB^*)^{\otimes 2} \big\ra.
\end{align*}
Thus it suffices to show 
\begin{align*}
\text{for every $\vB$ such that $\|\vB\|\le W$},\quad 
    \Ebb \ell''(y \tilde\xB^\top \vB)  \tilde\xB \tilde\xB^\top \succeq \frac{2}{W^2X^2 B}\SigmaB.
\end{align*}
This is because
\begin{align*}
\Ebb \ell''(y \tilde\xB^\top \vB)  \tilde\xB \tilde\xB^\top 
   &=  \Ebb \frac{1}{1+\exp(\tilde\xB^\top \vB)} \frac{1}{1+\exp(-\tilde\xB^\top \vB)} \tilde\xB \tilde\xB^\top \\
    &= \Ebb \frac{1}{1+\exp(\xB^\top \vB)} \frac{1}{1+\exp(-\xB^\top \vB)} \xB \xB^\top\ind{\|\xB\|\le X} \\
    &\succeq \Ebb \frac{1}{1+\exp(\xB^\top \vB)} \frac{1}{1+\exp(-\xB^\top \vB)} \xB \xB^\top - \Ebb \xB \xB^\top\ind{\|\xB\|> X}.
\end{align*}
By \Cref{lemma:population-hessian}, we have 
\begin{align*}
    \Ebb \frac{1}{1+\exp(\xB^\top \vB)} \frac{1}{1+\exp(-\xB^\top \vB)} \xB \xB^\top 
    \succeq \frac{1}{C_1(1+\|\vB\|^3_{\SigmaB})} \SigmaB
    \succeq \frac{1}{C_1(1+\lambda_1^{3/2} W^3)} \SigmaB,
\end{align*}
where $C_1>1$ is a constant.
By \Cref{lemma:norm-tail}, we have 
\begin{align*}
    \Ebb \xB \xB^\top\ind{\|\xB\|> X} \preceq \frac{1}{2C_1(1+\lambda_1^{3/2} W^3)} \SigmaB.
\end{align*}
So we have 
\begin{align*}
    \Ebb \ell''(y \tilde\xB^\top \vB)  \tilde\xB \tilde\xB^\top \succeq \frac{1}{2C_1(1+\lambda_1^{3/2} W^3)} \SigmaB.
\end{align*}
We complete the proof by noting that $W^2 X^2 B \ge 4C_1(1+\lambda_1^{3/2} W^3)$.
\end{proof}




The following lemma controls the effect of clipping on population risk.
\begin{lemma}\label{lemma:loss-small-prob}
Let $X^2 \ge  2\tr(\SigmaB) + C(1+\lambda_1)\cdot 2 \ln\big(n \sqrt{\tr(\SigmaB)}  \big)$.
Then for every $\wB$ such that $\|\wB\|\le W$, we have
\begin{align*}
   \big| L(\wB) - \Ebb \ell(y \tilde\xB^\top \wB) \big|\le  \frac{W}{n}.
\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:loss-small-prob}]
By definition and $1$-Lipschitzness of $\ell$, we have 
\begin{align*}
\big| L(\wB) - \Ebb \ell(y \tilde\xB^\top \wB) \big|
&= \big| \Ebb\ell(y \xB^\top \wB) - \ell \big(y \xB^\top \wB \ind{\|\xB\|\le X}\big)  \big|\\
&\le \Ebb\big| y \xB^\top \wB\ind{\|\xB\|> X} \big|\\
&\le W \Ebb  \|\xB\|\ind{\|\xB\|> X} \\
    &\le  W\sqrt{\Ebb \|\xB\|^2} \sqrt{\Pr(\|\xB\|>X)} \\
    &\le  W\sqrt{\tr(\SigmaB)}\exp(-t/2)
    \le \frac{W}{n},
\end{align*}
where we use \Cref{lemma:norm-tail} and the choice of $X$.
This completes the proof.
\end{proof}

The following lemma shows the early-stopped GD has a small norm.
\begin{lemma}\label{lemma:fast-norm-bound}
Let  
\(\beta:= C_0 \big( 1+ \tr(\SigmaB) + \lambda_1 \ln(1/\delta)/n \big)\),
where $C_0>1$ is a sufficiently large constant.
Assume that $\eta \le 1/\beta$ and $t$ is such that $\eRisk(\wB^*)\le \eRisk(\wB_{t-1})$.
Then with probability at least $1-\delta$, we have $\|\wB_t-\wB^*\|\le 1+\|\wB\| $.
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:fast-norm-bound}]
This is the same as the proof of \Cref{lemma:norm-bound}.
\end{proof}

We are now ready to proof \Cref{thm:gd:fast-upper-bound} using \Cref{thm:risk-local-rad}.
\begin{proof}[Proof of \Cref{thm:gd:fast-upper-bound}]
It is clear that 
$\tilde \ell$ is $L$-Lipschitz and that $f^*:= \la \wB^*, \cdot \ra / (WX)$ 
satisfies 
\[f^*\in \arg\inf_{f\in\Fcal} \Ebb \tilde\ell \big(yf(\tilde\xB)\big).\]
Moreover, for every $f = \la\wB,\cdot \ra/(WX) \in\Fcal$, by \Cref{lemma:population-convexity}, we have
    \begin{align*}
    \Ebb \big(f(\tilde\xB) - f^*(\tilde\xB) \big)^2  
    &= \frac{1}{W^2X^2}\Ebb (y\xB^\top \wB - y\xB^\top \wB^*)^2 \\
&\le B \Big(\Ebb \ell\big(y \tilde\xB^\top \wB \big) -\Ebb \ell\big(y \tilde\xB^\top \wB^*\big) \Big) 
= B \Big(\Ebb \tilde\ell\big(y f(\tilde\xB)\big) -\Ebb \tilde\ell\big(y f^*(\tilde\xB)\big) \Big).
    \end{align*}
So far, we have verified the conditions on $\Fcal$ and $\tilde\ell$ for applying \Cref{thm:risk-local-rad}.
 
Consider the function 
\[\hat f:= \la \wB_t, \cdot \ra / (WX).\]
By \Cref{lemma:fast-norm-bound} we have $\|\wB_t\|\le W$, thus $\hat f\in \Fcal$.
Moreover, by the choice of $X$ and \Cref{lemma:norm-tail}, with probability $1-\delta$, we have $\xB_i = \tilde\xB_i$ for all $i=1,\dots,n$.
Thus the stopping criterion implies 
\begin{align*}
    \frac{1}{n}\sum_{i=1}^n\tilde\ell\big(y_i \hat f(\tilde\xB_i) \big)
    = \frac{1}{n}\sum_{i=1}^n \ell\big(y_i \xB_i^\top \wB_t \big)
    = \eRisk(\wB_t) \le \eRisk(\wB^*)
    = \frac{1}{n}\sum_{i=1}^n\tilde\ell\big(y_i f^*(\tilde\xB_i) \big).
\end{align*}
So we can apply \Cref{thm:risk-local-rad} to $\hat f$.
With probability at least $1-\delta$, we have 
\begin{align*}
    \Ebb \tilde\ell\big(y \hat f (\tilde \xB)\big) -  \Ebb \tilde\ell\big(y  f^* (\tilde\xB)\big) \le 705 \frac{r}{B} + \frac{(11L + 27B)\ln(1/\delta)}{n},
\end{align*}
where, by \Cref{thm:risk-local-rad,lemma:local-rad}, $r$ is such that 
\begin{align*}
    r \ge \psi(r):= BL \sqrt{\frac{2k}{nL^2}\cdot r + \frac{\sum_{i>k}\lambda_i}{nX^2} }.
\end{align*}
Choosing 
\begin{align*}
    r = \frac{4k B^2}{n} + \sqrt{\frac{2B^2L^2\sum_{i>k}\lambda_i}{nX^2} }
\end{align*}
we have
\begin{align*}
    \Ebb \tilde\ell\big(y \hat f (\tilde \xB)\big) -  \Ebb \tilde\ell\big(y  f^* (\tilde\xB)\big)
    &\lesssim \frac{r}{B} + \frac{(L+B)\ln(1/\delta)}{n} \\
    &\lesssim  \frac{B k}{n} + \frac{L}{X} \sqrt{\frac{\sum_{i>k}\lambda_i}{n}} + \frac{(L+B)\ln(1/\delta)}{n} .
\end{align*}
Then applying \Cref{lemma:loss-small-prob}, we get
\begin{align*}
    &\lefteqn{L(\wB_t) - L(\wB^*)
    \le  \Ebb \tilde\ell\big(y \hat f (\tilde \xB)\big) -  \Ebb \tilde\ell\big(y  f^* (\tilde\xB)\big) + \frac{2W}{n}} \\
    &\lesssim  \frac{B k}{n} + \frac{L}{X} \sqrt{\frac{\sum_{i>k}\lambda_i}{n}} + \frac{(L+B)\ln(1/\delta)}{n}+ \frac{2W}{n} \\
    &\lesssim \max\{\|\wB^*\|, 1\} \Bigg( \frac{k\lambda_1^{1/2}}{n} + \sqrt{\frac{\sum_{i>k}\lambda_i}{n}} + \frac{ \ln (1/\delta) \sqrt{ \big(1+\tr(\SigmaB)\big)\ln\Big(\big(1+\tr(\SigmaB)\|\wB^*\|\big) n /\delta\Big) }}{n} \Bigg)\\
    &\lesssim \|\wB^*\| \bigg( \frac{k}{n} + \sqrt{\frac{\sum_{i>k}\lambda_i}{n}} + \frac{ \ln (1/\delta) \sqrt{ \tr(\SigmaB)\ln\big(n \|\wB^*\|\tr(\SigmaB) /\delta\big) }}{n} \bigg),
\end{align*}
where we assume $\|\wB^*\|\gtrsim 1$, $\lambda_1 \lesssim 1$, and $\tr(\SigmaB)\gtrsim 1$.
This completes the proof.
\end{proof}

\section{Lower Bounds for Interpolating Estimators}
\subsection{Proof of \texorpdfstring{\Cref{thm:logistic:lower-bound}}{Theorem 4.1}}\label{sec:proof:logistic:lower-bound}
\begin{proof}[Proof of \Cref{thm:logistic:lower-bound}]
Consider a sequence of estimators $(\wB_t)_{t\ge 0}$ such that
\begin{align*}
    \|\wB_t\|\to \infty,\quad 
    \wB_t / \|\wB_t\| \to \tilde\wB,
\end{align*}
where $\tilde\wB$ is a fixed unit vector.
Fix a small constant $\gamma>0$. Define an event 
\begin{align*}
    \Fcal := \{\xB: |\xB^\top \wB^*|\le 10 \|\wB^*\|_{\SigmaB},\  \|\xB\|\le 10\sqrt{\tr(\SigmaB)},\ |\xB^\top \tilde\wB|\ge \gamma \}.
\end{align*}
Since $\xB \sim \Ncal(0, \SigmaB)$ and $ \|\tilde\wB\|_{\SigmaB} > 0$, we have $\Pr(\Fcal)>0$.
Let $t_0$ be such that
\begin{align*}
    \bigg\|\frac{\wB_t}{ \|\wB_t\|} - \tilde\wB\bigg\| \le \frac{\gamma}{20\sqrt{\tr(\SigmaB)}},\quad \text{for every $t\ge t_0$}.
\end{align*}
Then for every $\xB\in\Fcal$ and $t\ge t_0$, we have
\begin{align*}
     \frac{|\xB^\top \wB_t |}{\|\wB_t\|} \ge |\xB^\top\tilde\wB| - \|\xB\|\cdot\bigg\| \frac{\wB_t}{ \|\wB_t\|} - \tilde\wB\bigg\| \ge \gamma -  10\sqrt{\tr(\SigmaB)} \cdot \frac{\gamma}{20\sqrt{\tr(\SigmaB)}} \ge  \frac{\gamma}{2}.
\end{align*}
Then the population risk of $\wB_t$ is
\begin{align*}
    \risk(\wB_t) 
    &= \Ebb_{\xB} \Ebb_{y} \ln(1+\exp(-y\xB^\top \wB_t)) \\
    &= \Ebb \sum_{y\in\{\pm 1\}} \frac{1}{1+\exp(-y \xB^\top \wB^*)} \ln(1+\exp(-y\xB^\top \wB_t)) \\
    &\ge \Ebb \sum_{y\in\{\pm 1\}} \frac{1}{1+\exp(-y \xB^\top \wB^*)} \ln(1+\exp(-y\xB^\top \wB_t))\ind{\xB \in \Fcal} \\
    &\ge \Ebb \frac{\ln(1+\exp(|\xB^\top \wB_t|))}{1+\exp(|\xB^\top \wB^*|)}  \ind{\xB \in \Fcal}\\
    &\ge \Ebb \frac{ \|\wB_t\|\gamma / 2 }{1+\exp(10\|\wB^*\|_{\SigmaB})} \ind{\xB \in \Fcal} \\
    &\ge \frac{\|\wB_t\|\gamma / 2}{1+\exp(10\|\wB^*\|)}\Pr(\Fcal)  \to \infty,
\end{align*}
where the last inequality is because $\|\wB_t\|\to \infty$.

Now we consider the calibration error. Under event $\Fcal$, we have 
\begin{align*}
    p(\wB^*; \xB) = \frac{1}{1+\exp(-\xB^\top\wB^*)} 
& \in\bigg[ \frac{1}{1+\exp(10\|\wB^*\|_{\SigmaB})}, \ 
         \frac{1}{1+\exp(-10\|\wB^*\|_{\SigmaB})}\bigg] \\
         &\in \bigg[ \exp(-10\|\wB^*\|_{\SigmaB}), \ 
         1-\exp(-10\|\wB^*\|_{\SigmaB})\bigg].
\end{align*}
Moreover, under event $\Fcal$, for $t>t_0$, we have
\begin{align*}
    p(\wB_t; \xB) = \frac{1}{1+\exp(-\xB^\top\wB_t)} 
    \begin{dcases}
        \le \frac{1}{1+\exp(\gamma \|\wB_t\|/2)} \le \exp(-\gamma \|\wB_t\|/2)  & \xB^\top\wB_t<0, \\
        \ge \frac{1}{1+\exp(-\gamma \|\wB_t\|/2)} \ge 1- \exp(-\gamma \|\wB_t\|/2)  & \xB^\top\wB_t>0.
    \end{dcases}
\end{align*}
These together imply that 
\begin{align*}
    | p(\wB_t; \xB) - p(\wB^*; \xB) | &\ge \exp(-10\|\wB^*\|_{\SigmaB}) -\exp(-\gamma \|\wB_t\|/2) \to \exp(-10\|\wB^*\|_{\SigmaB}).
\end{align*}
Then for the calibration error, we have 
\begin{align*}
    \calibration(\wB_t) &= \Ebb |p(\wB_t; \xB) - p(\wB^*;\xB)|^2 \\
    &\ge \Ebb |p(\wB_t; \xB) - p(\wB^*;\xB)|^2\ind{\xB\in\Fcal} \\
    &\ge \big(\exp(-10\|\wB^*\|_{\SigmaB}) -\exp(-\gamma \|\wB_t\|/2) \big)^2 \Pr(\Fcal) \\
    & \to \exp(-20\|\wB^*\|_{\SigmaB})\Pr(\Fcal) > 0.
\end{align*}
This completes the proof.
\end{proof}

\subsection{Proof of \texorpdfstring{\Cref{thm:zero-one:lower-bound}}{Theorem 4.2}}\label{sec:proof:zero-one:lower-bound}


\begin{lemma}\label{lemma:zero-one:excess-error-and-angle}
For a non-zero vector $\wB$, let $\theta$ be the angle between $\SigmaB^{1/2}\wB$ and $\SigmaB^{1/2}\wB^*$. 
Then
\begin{align*}
\error(\wB) - \error(\wB^*)
\ge \begin{dcases}
        \frac{1}{4\sqrt{2\pi}\|\wB^*\|_{\SigmaB}}& \frac{\pi}{2}< \theta \le \pi, \\
        \frac{\|\wB^*\|_{\SigmaB}}{48\pi \max\{1, \|\wB^*\|_{\SigmaB}^3\} }(1-\cos(\theta)) & 0\le \theta \le \frac{\pi}{2}.
    \end{dcases}
\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:zero-one:excess-error-and-angle}]
Let $s(t)=1/(1+e^{-t})$.
Notice that for $t>0$, 
\begin{align*}
    s(t)-1/2 &= \frac{1-\exp(-t)}{2(1+\exp(-t))}
    \ge \frac{1-\exp(-t)}{4}
    \ge \frac{1-1/(t+1)}{4} 
    \ge \frac{t}{8}\ind{0<t<1}.
\end{align*}
Since $\xB$ and $-\xB$ are identically distributed, we have 
\begin{align*}
    &\lefteqn{\Pr(y\xB^\top\wB\le 0) - \Pr(y \xB^\top \wB^* \le 0)  }\\
    &= 2 \Ebb\ind{\xB^\top \wB<0,\ \xB^\top \wB^* > 0}|s(\xB^\top \wB^*)-1/2| \\
    &= 2 \Ebb\ind{\xB^\top \wB<0,\ \xB^\top \wB^* > 0}\big( s(\xB^\top \wB^*)-1/2 \big) \\
    &\ge \frac{1}{4} \Ebb \xB^\top \wB^* \ind{\xB^\top \wB<0,\ 0< \xB^\top \wB^* <1}.
\end{align*}
Without loss of generality, assume that $\|\wB\|_{\SigmaB}=1$.
We can write $\SigmaB^{1/2} \wB = \SigmaB^{1/2}\wB^*/ \|\wB^*\|_{\SigmaB} \cos\theta - \vB_{\perp} \sin\theta$, where $\vB_{\perp}$ is a unit vector such that $\la \vB_{\perp},  \SigmaB^{1/2}\wB^*\ra = 0$.
Since $\xB\sim\Ncal(0,\SigmaB)$, we have
\begin{align*}
 &\lefteqn{\Pr(y\xB^\top\wB\le 0) - \Pr(y \xB^\top \wB^* \le 0)  }\\
     &\ge \frac{\|\wB^*\|_{\SigmaB}}{4} \Ebb_{g_1, g_2 \sim \Ncal(0,1)} g_1 \ind{g_1\cos(\theta)- g_2\sin(\theta)<0,\ 0<g_1< 1/\|\wB^*\|_{\SigmaB} } \\
    &= \frac{\|\wB^*\|_{\SigmaB}}{8\pi} \underbrace{\int_{0< g_1< 1/\|\wB^*\|_{\SigmaB}, g_1\cos\theta < g_2 \sin \theta} g_1 \exp\big(-(g_1^2  + g_2^2)/2 \big)\dif g_1\dif g_2}_{\diamondsuit} .
\end{align*}
To proceed, we discuss two cases.
\begin{itemize}[leftmargin=*]
    \item 
{If $\pi/2 < \theta \le \pi$}, we have $\cos\theta<0$ and $\sin \theta\ge 0$. So we have 
\begin{align*}
    \diamondsuit &\ge  \int_{0< g_1< 1/\|\wB^*\|_{\SigmaB}, \  0< g_2 } g_1 \exp\big(-(g_1^2  + g_2^2)/2 \big)\dif g_1\dif g_2 \\
    &= \frac{\sqrt{2\pi}}{2} \int_{0< g_1< 1/\|\wB^*\|_{\SigmaB} } g_1 \exp\big(-g_1^2/2 \big)\dif g_1 \\
    &= \sqrt{2\pi} \big( 1- \exp(-1/\|\wB^*\|^2_{\SigmaB})\big) \\
    &\ge \frac{\sqrt{2\pi}}{\|\wB^*\|^2_{\SigmaB}}.
\end{align*}
So we have 
\begin{align*}
\Pr(y\xB^\top\wB\le 0) - \Pr(y \xB^\top \wB^* \le 0)  
 \ge \frac{\|\wB^*\|_{\SigmaB}}{8\pi }\cdot \diamondsuit  \ge \frac{1}{4\sqrt{2\pi}\|\wB^*\|_{\SigmaB}}.
\end{align*}

\item {If $0\le  \theta \le \pi/2$}, we have $\cos\theta\ge 0$. By changing of variables, we get
\begin{align*}
\diamondsuit &= \int_{0< g_1< 1/\|\wB^*\|_{\SigmaB}, \  g_1 < g_2\tan\theta } g_1 \exp\big(-(g_1^2  + g_2^2)/2 \big)\dif g_1\dif g_2 \\
    &= \int_{0< r\sin \psi < 1/\|\wB^*\|_{\SigmaB}, 0<\psi< \theta} r\sin \psi \exp\big(-r^2/2 \big) \cdot r \dif r \dif \psi \\
& \ge \int_{0< \psi <\theta} \sin\psi \bigg( \int_{0< r< 1/\|\wB^*\|_{\SigmaB}} r^2 \exp\big(-\frac{1}{2}r^2 \big)\dif r  \bigg) \dif \psi \\
&\ge \int_{0< \psi <\theta} \sin\psi \Bigg(\int_{0< r< \min\{ 1, 1/\|\wB^*\|_{\SigmaB}\} } r^2 \bigg(1-\frac{1}{2}r^2 \bigg)\dif r \Bigg) \dif \psi \\
     &\ge \int_{0< \psi <\theta} \sin\psi \Bigg( \frac{1}{2}\int_{0< r< \min\{ 1, 1/\|\wB^*\|_{\SigmaB}\} } r^2 \Bigg) \dif \psi \\
    & = \frac{1}{6\max\{1, \|\wB^*\|_{\SigmaB}^3\} } \int_{0< \psi <\theta} \sin\psi\dif \psi \\
    &= \frac{1}{6\max\{1, \|\wB^*\|_{\SigmaB}^3\} } \big( 1-\cos\theta\big).
\end{align*}
So we have 
\begin{align*}
\Pr(y\xB^\top\wB\le 0) - \Pr(y \xB^\top \wB^* \le 0)  
 \ge \frac{\|\wB^*\|_{\SigmaB}}{8\pi }\cdot \diamondsuit  
\ge  \frac{\|\wB^*\|_{\SigmaB}}{48\pi \max\{1, \|\wB^*\|_{\SigmaB}^3\} }(1-\cos(\theta)).
\end{align*}
\end{itemize}
This completes the proof.
\end{proof}




\begin{lemma}\label{lemma:zero-one:failure-prob}
Let $\zB\sim\Ncal(0, \IB)$ and $\Pr(y|\zB)=s(y \zB^\top\vB^*)$.
Then for any unit vector $\vB$, we have
\begin{align*}
     \Pr(y \zB^\top \vB < - 0.5) \ge \frac{0.25}{1+\exp(\|\vB^*\|)}.
\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:zero-one:failure-prob}]
This is by direct calculation.
\begin{align*}
    \Ebb \ind{y \zB^\top \vB < - 0.5 } 
    &= \Ebb \ind{y=1,\ \zB^\top \vB < -0.5} + \ind{y=-1,\ \zB^\top \vB > 0.5} \\
    &= \Ebb s(\zB^\top \vB^*) \ind{ \zB^\top \vB < -0.5} + s(-\zB^\top \vB^*) \ind{ \zB^\top \vB > 0.5} \\
    &\ge\Ebb  \frac{1}{1+\exp(|\zB^\top \vB^*|)} \ind{ |\zB^\top \vB| > 0.5} \\
    &\ge\Ebb \frac{1}{1+\exp(\|\vB^*\|)} \ind{ |\zB^\top \vB| > 0.5,\ | \zB^\top \vB^* | < \|\vB^*\| } \\
    &\ge \frac{1}{1+\exp(\|\vB^*\|)} \big( 1- \Pr(|\zB^\top \vB| \le 0.5) - \Pr(| \zB^\top \vB^* | \ge \|\vB^*\|) \big) \\
    &= \frac{1}{1+\exp(\|\vB^*\|)}\big(1- ( \Phi(0.5)-\Phi(-0.5))- 2 (1-\Phi(1))  \big) \\
    &\ge \frac{0.25}{1+\exp(\|\vB^*\|)},
\end{align*}
where $\Phi$ is the cumulative distribution function of normal distribution. We complete the proof.
\end{proof}

\begin{lemma}\label{lemma:zero-one:const-frac-failure-prob}
Let $(y_i,\zB_i)_{i=1}^n$ be independent copies of $(y,\zB)$. Assume that 
\begin{align*}
    n \ge C \big( 1+\exp(\|\vB^*\|)\big)  k  \ln(k /\delta)
\end{align*} 
for a sufficiently large constant $C>1$.
Let $\Scal:= \{\vB: \|\vB_{0:k}\|=1, \vB_{k:\infty}=0\}$.
Then \begin{align*}
    \Pr\bigg( \text{for every $\vB\in \Scal$},\ \#\{i\in[n]: y_i \zB_i^\top \vB \le -0.4 \}  \ge \frac{n/8}{1+\exp(\|\vB^*\|)} \bigg) \ge 1-\delta.
\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:zero-one:const-frac-failure-prob}]
For each unit vector $\vB$, define a binary random variable 
\[
\xi(\vB) := \ind{y \zB^\top \vB \le - 0.5} \in \{0, 1\}.
\]
Let $(\xi_i(\vB))_{i=1}^n$ be independent copies of $\xi(\vB)$. By the multiplicative Chernoff bound, we have 
\begin{align*}
    \Pr \bigg(\sum_{i=1}^n \xi_i(\vB) \le  0.5 n  \Ebb \xi(\vB) \bigg) \le \exp\big(- n  \Ebb \xi(\vB) / 8 \big).
\end{align*}
By \Cref{lemma:zero-one:failure-prob}, we have 
$\Ebb \xi(\vB)  \ge 0.25 / \big(1+\exp{\|\vB^*\|}\big)$.
Thus we have 
\begin{align*}
    \Pr \bigg(\sum_{i=1}^n \xi_i(\vB) \le  \frac{n/8}{1+\exp{\|\vB^*\|}}\bigg) 
    \le \exp\bigg(-  \frac{n/32}{1+\exp(\|\vB^*\|)} \bigg).
\end{align*}
Let $\Ccal$ be an $\epsilon$-$\ell_2$-covering of the $k$-dimensional unit sphere $\Scal$. Then $|\Ccal|=\Ocal((1/\epsilon)^k)$. 
By a union bound, we get 
\begin{align*}
  \Pr\bigg( \text{for every $\vB\in\Ccal$},\  \sum_{i=1}^n \xi_i(\vB) \ge \frac{n/8}{1+\exp(\|\vB^*\|)} \bigg) \ge 1- \exp\bigg(- \frac{n/32}{1+\exp(\|\vB^*\|)} + C k \ln(1/\epsilon)\bigg).
\end{align*}
Here $C>1$ is a sufficiently large constant and may vary line by line.
Moreover, with probability at least $1- 0.5 \delta$, we have 
\[
\Pr\bigg(  \max_{i\in[n]}\|\zB^{(i)}_{0:k}\| \le  C \sqrt{k\ln(n/\delta)} \bigg) \ge 1-0.5\delta.\]
Under the joint of the two events, for every $\vB\in\Scal$, there is a $\vB'\in\Ccal$ such that
\begin{align*}
\text{for every $i\in[n]$},\  y_i\zB_i^\top \vB \le y_i\zB_i^\top \vB' + \max_{i\in[n]}\|\zB^{(i)}_{0:k}\| \epsilon \le y_i\zB_i^\top \vB' + C \sqrt{k\ln(n/\delta)} \epsilon \le y_i\zB_i^\top \vB' - 0.1,
\end{align*}
where we set $\epsilon = 0.1/( C \sqrt{k\ln(n/\delta)})$.
Therefore we must have 
\begin{align*}
&\lefteqn{ \Pbb\bigg(    \text{for every $\vB\in\Scal$},\ \sum_{i=1}^n \ind{y_i\zB_i^\top \vB \le -0.4} \ge  \frac{n/8}{1+\exp(\|\vB^*\|)}\bigg)  } \\
& \ge 1- \exp\bigg(- \frac{n/32}{1+\exp(\|\vB^*\|)} + C k \ln(1/\epsilon)\bigg) - 0.5 \delta
\ge 1- \delta,
\end{align*}
where in the last inequality we use the assumption that 
\begin{align*}
    n \ge C \big( 1+ \exp(\|\vB^*\|) \big)  k  \ln(k /\delta)
\end{align*}
for a sufficiently large constant $C>1$. We have completed the proof.
\end{proof}














\begin{lemma}\label{lemma:zero-one:angle-lower-bound}
Assume that $\vB^*$ is $k$-sparse and 
\begin{align*}
    n \ge C \big( 1+\exp(\|\vB^*\|)\big)  k  \ln(k /\delta),\quad 
    d\ge C n\ln(n)\ln(1/\delta),
\end{align*} 
where $C>1$ is a large constant. 
Then with probability at least $1-\delta$, we have 
\begin{align*}
    \text{for every unit $\vB$ such that $\max_{i\in[n]} y_i\zB_i^\top \vB< 0$},\quad 1 -\frac{\la \vB,\vB^*\ra }{\|\vB^*\|} \ge \sqrt{ \frac{1}{C\big( 1+\exp(\|\vB^*\|) \big) }} \cdot \sqrt{\frac{n}{d}}.
\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:zero-one:angle-lower-bound}]




Let $\vB$ be an arbitrarily unit vector such that $\max_{i\in[n]} y_i\zB_i^\top \vB < 0$.
Since $\vB^*$ is $k$-sparse, without loss of generality, assume $\vB^*_{k:\infty} = 0$, then
\begin{align*}
    1- \frac{\la \vB, \vB^*\ra }{\|\vB^*\|}
    = 1- \frac{\la \vB_{0:k}, \vB^*_{0:k}\ra }{\|\vB^*_{0:k}\|} 
    \ge  1- \|\vB_{0:k}\|.
\end{align*}
It suffices to establish an upper bound on $\vB^*_{0:k}$.

Define a set of indexes of the data that is significantly incorrectly classified by $\vB_{0:k}$,
\begin{align*}
    \Ical:= \{i\in[n]: y_i\zB_i^\top \vB_{0:k} \le - 0.4 \|\vB_{0:k}\| \}
\end{align*}
Then for each $i\in\Ical$, we must have 
\begin{align*}
    0< y_i\zB_i^\top \vB = y_i \zB^{(i)}_{0:k} \vB_{0:k} + y_i \zB^{(i)}_{k:\infty} \vB_{k:\infty} \le  - 0.4 \|\vB_{0:k}\| + y_i \zB^{(i)}_{k:\infty} \vB_{k:\infty}.
\end{align*}

By \Cref{lemma:zero-one:const-frac-failure-prob}, we have 
\begin{align*}
\Pr \bigg( |\Ical| \ge     \frac{n/8}{1+\exp(\|\vB^*\|)} \bigg) \ge 1-\delta.
\end{align*}

According to \citep{hsu2021proliferation}, we have 
\begin{align*}
    \Pr\bigg(\text{all $\big(y_i, \zB^{(i)}_{k:\infty}\big)_{i \in\Ical}$ is support vector} \bigg) \ge 1-\delta
\end{align*}
assuming that $d\ge C |\Ical|\ln(\Ical) \ln(1/\delta)$.
Under this event, the max-margin direction is given by $ \hat \vB := \ZB_{k:\infty} (\ZB_{k:\infty} \ZB_{k:\infty}^\top)^{-1} \yB$, where $\ZB_{k:\infty} = (\zB^{(i)}_{k:\infty})_{i\in\Ical}$. It is clear that for every $i\in\Ical$, we have $ \zB^{(i)}_{k:\infty} \hat \vB = y_i$.
So we have 
\begin{align*}
    \max_{\|\uB\|=1 }\min_{i\in\Ical} y_i \zB^{(i)}_{k:\infty} \uB 
= \frac{1}{\|\hat \vB\|} 
    = \frac{1}{\sqrt{ \yB^\top (\ZB_{k:\infty} \ZB_{k:\infty}^\top)^{-1} \yB }} 
    \le \sqrt{\frac{\|\ZB_{k:\infty} \ZB_{k:\infty}^\top\|_2}{\|\yB\|^2}} 
    =  \sqrt{\frac{\|\ZB_{k:\infty} \ZB_{k:\infty}^\top\|_2 }{n}}.
\end{align*}
By Hoeffding's inequality, we have 
\begin{align*}
    \Pr\big(\ZB_{k:\infty} \ZB_{k:\infty}^\top \preceq 2 d \IB_n  \big) \ge 1- \exp(-C (d-n)) \ge 1-\delta.
\end{align*}
Under this event, we have $ \max_{\|\uB\|=1 }\min_{i\in\Ical} y_i \zB^{(i)}_{k:\infty} \uB \le \sqrt{2d/|\Ical|}$.
Then we get
\begin{align*}
    0.4 \|\vB_{0:k}\| \le \min_{i\in\Ical} y_i\zB^{(i)}_{k:\infty} \vB_{k:\infty} \le \|\vB_{k:\infty}\|\sqrt{2d/|\Ical|}.
\end{align*}
Since $\|\vB\|=1$, we must have 
\begin{align*}
    \|\vB_{0:k}\| \le \frac{1}{\sqrt{1+0.4 |\Ical|/(2d)}}
\end{align*}

Under the joint of the three events, which happens with probability at least $1-3\delta$, we have
\begin{align*}
    1- \frac{\la \vB, \vB^*\ra }{\|\vB^*\|}
    &= 1- \frac{\la \vB_{0:k}, \vB^*_{0:k}\ra }{\|\vB^*_{0:k}\|} \\ 
    &\ge  1- \|\vB^*_{0:k}\| \\
    &\ge  1- \frac{1}{\sqrt{1+0.4 |\Ical|/(2d)}} \\
    &\ge C \sqrt{\frac{|\Ical|}{d}} \\
    &\ge C \sqrt{ \frac{1}{1+\exp(\|\vB^*\|)} \cdot \frac{n}{d}}.
\end{align*}
Here $C>1$ is a large constant and may vary line by line. We complete the proof by rescaling $\delta$.
\end{proof}


\begin{proof}[Proof of \Cref{thm:zero-one:lower-bound}]
Define 
\[\vB = \SigmaB^{1/2}\wB,\quad \vB^* = \SigmaB^{1/2}\wB^*,\quad \zB = \SigmaB^{-1/2}\xB,\quad 
d:=\rank(\SigmaB).\]
Then $\xB^\top \wB =  \zB^\top \vB$ and $\xB^\top \wB^* =  \zB^\top \vB^*$.
Without loss of generality, let $\|\vB\|=1$.
Let $\theta$ be the angle between $\vB$ and $\vB^*$.
We can apply \Cref{lemma:zero-one:angle-lower-bound} to get 
\begin{align*}
    1- \cos \theta =  1-\frac{\la \vB, \vB^*\ra}{\|\vB^*\|} 
    \gtrsim \sqrt{\frac{n}{d}} \gtrsim \frac{1}{\sqrt{\ln(n)\ln(1/\delta)}}.
\end{align*}
We complete the proof by calling \Cref{lemma:zero-one:excess-error-and-angle}.
\end{proof}













\section{Early Stopping and \texorpdfstring{$\ell_2$}{l2}-Regularization}
\subsection{Proof of \texorpdfstring{\Cref{thm:path:global-angle}}{Theorem 5.1}}\label{sec:proof:path:global-angle}

\begin{proof}[Proof of \Cref{thm:path:global-angle}]
We apply the $\ell_2$-regularized ERM $\uB_{\lambda}$ with $\lambda= 1/(\eta t)$ as a comparator in \Cref{lemma:implicit-regularization}. 
Recall that by the first-order stationary point condition, we have 
\begin{align*}
    -\grad \risk(\uB_\lambda)  = \lambda \uB_\lambda = \frac{1}{\eta t} \uB_\lambda.
\end{align*}
Then we have 
\begin{align*}
    \frac{1}{2}\|\wB_t - \uB_t\|^2 - \frac{1}{2}\|\uB_t\|^2 
    &\le \eta t \big( \eRisk(\uB_{\lambda}) -  \eRisk(\wB_t)\big) \\
    &\le \eta t \la \grad \risk(\uB_t), \uB_t -\wB_t \ra \\
    &= -\la  \uB_t, \uB_t-\wB_t \ra,
\end{align*}
where the two inequalities are by \Cref{lemma:implicit-regularization} and convexity, respectively.
The above is equivalent to 
\begin{equation}\label{eq:opt-vs-regu-path}
   \frac{1}{2} \|\wB_t-\uB_\lambda \|^2 \le \la \uB_\lambda , \wB_t\ra - \frac{1}{2}\|\uB_\lambda \|^2 \quad \Leftrightarrow\quad   \|\wB_t-\uB_\lambda\|^2 \le \frac{1}{2} \|\wB_t\|^2.
\end{equation}

To get the angle bound, we reformulate \Cref{eq:opt-vs-regu-path} as 
\begin{align*}
    2 \la\uB_t,\wB_t \ra \ge \frac{1}{2}\|\wB_t\|^2 + \|\uB_t\|^2. 
\end{align*}
Then we get
\begin{align*}
    \cos(\wB_t, \uB_t) := \frac{\la \uB_t, \wB_t\ra }{\|\uB_t\| \cdot \|\wB_t\|} 
    \ge \frac{\frac{1}{2}\bigg(\frac{1}{2}\|\wB_t\|^2 + \|\uB_t\|^2\bigg)}{\|\uB_t\| \cdot \|\wB_t\|} 
    \ge \frac{1}{\sqrt{2}},
\end{align*}
where we use $(a+b)/2\ge \sqrt{ab}$ for $a,b\ge 0$ in the last inequality.
To get the norm bounds, we use triangle inequalities with \Cref{eq:opt-vs-regu-path} to get 
\begin{align*}
\frac{1}{\sqrt{2}} \|\wB_t\| \ge     \|\wB_t-\uB_\lambda\| \ge \begin{dcases}
    \|\wB_t\| - \|\uB_\lambda\|, \\
    \|\uB_\lambda\| - \|\wB_t\|,
\end{dcases} 
\end{align*}
which implies 
\[
\frac{\sqrt{2}}{\sqrt{2}+1} \|\uB_\lambda\| \le \|\wB_t\|\le \frac{\sqrt{2}}{\sqrt{2}-1}\|\uB_\lambda\|.
\]
We complete the proof.
\end{proof}




























\subsection{Proof of \texorpdfstring{\Cref{thm:path:point-wise-distance}}{Theorem 5.2}}\label{sec:proof:path:point-wise-distance}
Recall that the dataset $(\xB_i, y_i)_{i=1}^n$ is linearly separable.
Define the margin and the maximum $\ell_2$-margin direction as
\begin{align*}
    \gamma := \max_{\|\wB\|=1}\min_{i\in[n]} y_i\xB_i^\top \wB > 0,\quad \tilde\wB :=  \arg\max_{\|\wB\|=1}\min_{i\in[n]} y_i\xB_i^\top \wB.
\end{align*}
Both GD and $\ell_2$-regularization paths are rational invariant.
Without loss of generality, we can assume $\tilde\wB = \eB_1$, where $(\eB_i)_{i=1}^d$ denotes the canonical basis \citep{wu2023implicit}.
That is to say, we will use the first coordinate of a vector to refer to its projection along the $\tilde\wB$ direction. 
Under this convention, we use $(w_t, \bar \wB_t)_{t\ge 0}$ and $(u_\nu, \bar \uB_\nu)_{\nu \ge 0}$ to denote the optimization and regularization paths, respectively.
We denote the data by 
\begin{align*}
    \xB_i = (
        x_i,\,
        \bar \xB_i),\quad y_i x_i \ge \gamma,\quad \bar \xB_i \in \Rbb^{d-1}. 
\end{align*}
Let the set of support vectors be
\[
\Scal := \{i\in[n]: y_i\xB_i^\top\tilde\wB=\gamma \}.
\]
By \cite{wu2023implicit}, the dataset $(\bar\xB_i, y_i)_{i\in\Scal}$ is strictly non-separable under \Cref{assump:full-rank-support}. In particular, by Definition 2 in \citep{wu2023implicit}, there exists $b>0$ such that
\begin{equation}\label{eq:path:non-separable}
    \text{for every $\bar\wB\in\Rbb^{d-1}$, there exists $i\in\Scal$ such that}\ y_i\bar\xB_i^\top\bar\wB \le -b \|\bar\wB\|.
\end{equation}
Define 
\begin{equation*}
    G(\bar \wB):= \sum_{i\in \Scal}\exp(- y_i \bar \xB_i^\top \bar \wB).
\end{equation*}
Then $G(\cdot)$ is convex and $G(\bar \wB)\ge 1$.
By \Cref{eq:path:non-separable}, the level set of $G(\bar \wB)$ is compact and $\bar \wB^*:= \arg \min G(\bar \wB)$ is finite.

The following lemma for the limiting GD path is from \citep{wu2023implicit}.
\begin{lemma}
Let $\wB_t := (w_t, \bar \wB_t)$ for $t\ge 0$ be the GD path with any fixed stepsize $\eta>0$.
Then under \Cref{assump:full-rank-support}, we have $w_t$ is increasing and 
\begin{align*}
    \lim_{t\to\infty} w_t = \infty,\quad 
    \lim_{t\to\infty} \bar \wB_t = \bar \wB^*.
\end{align*}
\end{lemma}

The following lemma characterizes the limiting $\ell_2$-regularization path.
\begin{lemma}\label{lemma:path:l2-regu}
Let $\uB_{\lambda} = (u_{\lambda}, \bar \uB_{\lambda})$ for $\lambda>0$ be the $\ell_2$-regularization path.
Then under \Cref{assump:full-rank-support}, we have
\begin{align*}
    \lim_{\lambda\to0} u_{\lambda} = \infty,\quad 
    \lim_{\lambda\to0}\bar \uB_{\lambda} = \bar \wB^*.
\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lemma:path:l2-regu}]
By the first order condition, we have 
\begin{align*}
\lambda    u_{\lambda} &= -\frac{\dif}{\dif u} L(\uB_{\lambda}) = \frac{1}{n}\sum_{i=1}^n \frac{y_ix_i}{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )}, \\
\lambda    \bar\uB_{\lambda} &= -\frac{\dif}{\dif \bar\uB} L(\uB_{\lambda}) = \frac{1}{n}\sum_{i=1}^n \frac{y_i\bar \xB_i}{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )}.
\end{align*}

We first show that $u_{\lambda}\to\infty$. Recall that $y_i x_i \ge \gamma$. Then we have 
\begin{align*}
    \lambda    u_{\lambda} &=  \frac{1}{n}\sum_{i=1}^n \frac{y_ix_i}{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )} \\
    &\ge  \frac{\gamma}{n}\sum_{i=1}^n \frac{1}{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )} \\
    &\ge  \frac{\gamma}{2n}\sum_{i=1}^n \exp(-y_i x_i u_{\lambda} - y_i \bar\xB_i^\top \bar\uB_{\lambda} ) \\
    &\ge \frac{\gamma}{2n}\sum_{i\in\Scal} \exp(-y_i x_i u_{\lambda} - y_i \bar\xB_i^\top \bar\uB_{\lambda} ) \\
    &=  \frac{\gamma}{2n} \exp(-\gamma u_{\lambda}) G ( \bar\uB_{\lambda} ) \\
    &\ge \frac{\gamma}{2n} \exp(-\gamma u_{\lambda}),
\end{align*}
where the last inequality is because $G(\cdot)\ge 1$. The above bound implies that $u_{\lambda}\to\infty$ as $\lambda\to 0$.

We next show that $\bar\uB_{\lambda}$ is bounded. 
Define
\[
\Ncal:=\{i\in[n]: y_i\bar\xB_i^\top\bar\uB_{\lambda} < 0\},\quad 
\Pcal:=\{i\in[n]: y_i\bar\xB_i^\top\bar\uB_{\lambda} \ge 0\}.
\]
Then $\Ncal$ is nonempty by \Cref{eq:path:non-separable}. Moreover, there exists $i^*\in \Ncal\cap\Scal$ such that $y_{i^*}\bar\xB_{i^*}^\top\bar\uB_{\lambda}\le -b\|\bar\uB\|$ by \Cref{eq:path:non-separable}.
Note that $y_{i^*}x_{i^*}=\gamma$ by the definition of $\Scal$.
Then by definition, we have 
\begin{align*}
    \lambda\|\bar\uB\|^2 &= \frac{1}{n}\sum_{i=1}^n \frac{y_i\bar \xB_i^\top \bar\uB}{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )}\\
    &=\frac{1}{n}\sum_{i\in\Ncal} \frac{y_i\bar \xB_i^\top \bar\uB}{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )}+\frac{1}{n}\sum_{i\in\Pcal} \frac{y_i\bar \xB_i^\top \bar\uB}{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )} \\
    &\le \frac{1}{n}\sum_{i\in\Ncal} \frac{y_i\bar \xB_i^\top \bar\uB}{1+\exp(y_i x_i u_{\lambda} )} +\frac{1}{n}\sum_{i\in\Pcal} {y_i\bar \xB_i^\top \bar\uB}\exp(-y_i x_i u_{\lambda} - y_i \bar\xB_i^\top \bar\uB_{\lambda} ) \\
    &\le \frac{1}{n} \frac{y_{i^*}\bar \xB_{i^*}^\top \bar\uB}{1+\exp(y_{i^*} x_{i^*} u_{\lambda} )} +\frac{1}{n}\sum_{i\in\Pcal} \exp(-y_i x_i u_{\lambda}  ) \\
    &\le \frac{1}{n} \frac{-b\|\bar\uB\|}{1+\exp(\gamma u_{\lambda} )} + \exp(-\gamma u_{\lambda}  ) ,
\end{align*}
where the second inequality is by $e^{t}t\le 1$. The above inequality implies 
\begin{align*}
    b\|\bar\uB_{\lambda}\| \le n \big(1+\exp(\gamma u_{\lambda} ) \big) \exp(-\gamma u_{\lambda}  ) \le 2n,
\end{align*}
where the last inequality is because $u_{\lambda}>0$. This shows that $\bar\uB_{\lambda}$ is bounded.

Now we prove an upper bound on $u_{\lambda}$.
Since $\bar\uB_{\lambda}$ is bounded, we know $\sum_{i=1}^n \exp(-y_i \bar\xB_i^\top \bar\uB_{\lambda} )$ is bounded by a constant $F_{\max}$. Let $X:=\max_{i}\|\xB_i\|$.
Then we have 
\begin{align*}
    \lambda    u_{\lambda} &=  \frac{1}{n}\sum_{i=1}^n \frac{y_ix_i}{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )} \\
    &\le  \frac{X}{n}\sum_{i=1}^n \frac{1}{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )} \\
    &\le  \frac{X}{n}\sum_{i=1}^n \exp(-y_i x_i u_{\lambda} - y_i \bar\xB_i^\top \bar\uB_{\lambda} ) \\
    &\le \frac{X}{n}\sum_{i=1}^n \exp(-\gamma u_{\lambda} - y_i \bar\xB_i^\top \bar\uB_{\lambda} )\\
     &\le \frac{X F_{\max}}{n} \exp(-\gamma u_{\lambda}),
\end{align*}
which implies that 
\begin{align}
    \lambda  \exp(\gamma u_{\lambda}) \le \frac{X F_{\max}}{n u_{\lambda}} \to 0.\label{eq:path:upper-u}
\end{align}

Finally, we show $\bar\uB_{\lambda}\to\bar\wB^*$. Recall the definition of $\bar\uB_{\lambda}$. Let us take $\lambda\to 0$ and use \cref{eq:path:upper-u}, the boundedness of $\bar\uB_{\lambda}$, and that 
\[y_ix_i> \gamma \ \text{for}\  i\not\in\Scal,\quad y_ix_i=\gamma\ \text{for}\  i\in\Scal,\]
then we get 
\begin{align*}
0 &=
    \lim_{\lambda\to 0}\lambda\bar\uB_{\lambda}  \exp(\gamma u_{\lambda}) \\
    & =  \lim_{\lambda\to 0}\frac{1}{n}\sum_{i=1}^n \frac{y_i\bar \xB_i\exp(\gamma u_{\lambda}) }{1+\exp(y_i x_i u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )} \\
    &= \lim_{\lambda\to 0}\frac{1}{n}\sum_{i\in\Scal} \frac{y_i\bar \xB_i \exp(\gamma u_{\lambda}) }{1+\exp(\gamma u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )} \\
    &= \lim_{\lambda\to 0}\frac{1}{n}\sum_{i\in\Scal} \frac{y_i\bar \xB_i \exp(\gamma u_{\lambda}) }{\exp(\gamma u_{\lambda} + y_i \bar\xB_i^\top \bar\uB_{\lambda} )} \\
    &= \lim_{\lambda\to 0}\frac{1}{n}\sum_{i\in\Scal} \frac{y_i\bar \xB_i }{\exp( y_i \bar\xB_i^\top \bar\uB_{\lambda} )}.
\end{align*}
Thus $\bar\uB_{0}$ satisfies the first-order stationary condition of $G(\cdot)$. So we must have $\bar\uB_{0} = \bar\wB^*$. This completes our proof.
\end{proof}


\begin{proof}[Proof of \Cref{thm:path:point-wise-distance}]
The proof is a direct consequence of the above lemmas.

Note that $ u_{\lambda}$ is continous, $u_{\infty}=0$, and $\lim_{\lambda\to 0} u_{\lambda} = \infty$. Moreover $w_0=0$, $w_t$ is increasing, and $w_t\to\infty$.
Thus for each $t$ we can choose $\lambda(t)$ such that $ u_{\lambda} =  w_t$ and $\lambda(t)\to 0$. 
Then we have 
\begin{align*}
    \| \uB_{\lambda(t)} - \wB_t\| = \|\bar \uB_{\lambda(t)} - \bar \wB_t\| \le \|\bar\uB_{\lambda(t)} - \bar\wB^*\| +\| \bar \wB_t - \bar \wB^*\| \to 0. 
\end{align*}
This completes the proof.
\end{proof}



\subsection{Proof of \texorpdfstring{\Cref{thm:path:counter-example}}{Theorem 5.3}}\label{sec:proof:path:counter-example}

\begin{proof}[Proof of \Cref{thm:path:counter-example}]
As the dataset is linearly separable and we only care about the asymptotic, without loss of generality, we can consider the exponential loss instead of the logistic loss.
In what follows, we focus on analyzing gradient flow. Our argument applies to gradient descent with any fixed stepsize $\eta>0$.

Denote $\wB = (w^{(1)}, w^{(2)})$. Then the empirical risk can be written as 
\begin{align*}
    \eRisk(w^{(1)}, w^{(2)}):= \frac{1}{2} \big( \exp(-\gamma w^{(1)}) + \exp(-\gamma w^{(1)} - \gamma_2  w^{(2)}) \big).
\end{align*}
Note that the GF and $\ell_2$-regularization paths under the $\eRisk$ are the same as those under $\ln\eRisk$ up to a rescaling of the time and regularization strength.
It suffices to compared the GF and $\ell_2$-regularization paths under $\ln\eRisk$, where
\begin{align*}
    \ln\eRisk(w^{(1)}, w^{(2)}) = -\gamma w^{(1)} + \ln \big(1+\exp(-\gamma_2 w^{(2)}) \big) - \ln(2).
\end{align*}


The GF path is given by
\begin{align*}
    \dif w^{(1)}_t = \gamma \dif t,\quad 
    \dif w^{(2)}_t= \gamma_2 \frac{\exp(-\gamma_2 w^{(2)})}{1+\exp(-\gamma_2 w^{(2)})} \dif t,\quad w^{(1)}_0 = w^{(2)}_0= 0 .
\end{align*}
Solving the ODEs, we get 
\begin{align*}
    w^{(1)}_t = \gamma t,\quad 
    \bigg| w^{(2)}_t - \frac{\ln (1+\gamma_2^2 t)}{\gamma_2} \bigg| \le C,\quad t\ge 0,
\end{align*}
for some constant $C>0$.
The $\ell_2$-regularization path is given by 
\begin{align*}
   -\gamma + \lambda u^{(1)}_\lambda  = 0,\quad 
   -\gamma_2 \frac{\exp(-\gamma_2 u^{(2)})}{1+\exp(-\gamma_2 u^{(2)})} +  \lambda u^{(2)}_\lambda  = 0.
\end{align*}
Then we get 
\begin{align*}
     & u^{(1)}_{\lambda} = \frac{\gamma}{\lambda},\quad  \lambda \ge 0 \\
     \quad 
    & \bigg| u^{(2)}_{\lambda} - \frac{\ln (\gamma_2^2/ \lambda) - \ln\ln(\gamma^2/ \lambda)}{\gamma_2} \bigg| \le C,\quad \lambda\le \gamma^2_2/e,
\end{align*}

As $t \to \infty$ and $\lambda\to 0$, the two paths tend to infinity in both directions. However, due to the rate mismatch, it is impossible to match $\wB_t$ with $\uB_{\lambda}$ in both directions at the same time. So their $\ell_2$-distance has to be infinite asymptotically.  
\end{proof}
\end{document}