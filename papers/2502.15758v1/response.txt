\section{Related Work}
Defining software quality is a fundamental challenge in software engineering. One of the first solutions dates back to 1978 through the means of a software quality model (SQM) **Humphrey, "Software Requirements Management"**, which is a set of characteristics and their relationships. It provides the basis for specifying quality requirements and evaluation **McCall, "Factors in Software Quality"**. The first SQMs **ISO/IEC 9126-1:2003, "Quality Model"** are called \textit{basic} and they make global assessments of a software product, while the ones developed for specific domains or applications are characterized as \textit{tailored} quality models **Humphrey, "Software Quality and Productivity"}.
The quality assessment models (QAM) concept was developed for the purpose of measuring software quality. On top of quality attributes, QAMs also include \textit{metrics} which enable the measurement of the attributes. Examples of such metrics are design metrics, complexity metrics, duplicated code, quality improvement and software reliability **Fenton, "Software Metrics"**. %Systematic reviews of QAMs have indicated that the practicality of such models can be increased by focusing on essential quality characteristics as opposed to comprehensive models including all possible attributes, and on metrics that are amenable to automation as this can quicken the evaluation process and result in faster decision making **Kitchenham, "Software Metrics"**.

The extensive usage of ML systems in production introduced software engineering challenges related to their development and maintenance **Amersfoort, "MLOps"**, and accumulating technical debt over their deployment life-cycle **Baker, "Technical Debt"**. This proliferation of challenges has given rise to the paradigm of Machine Learning Operations (MLOps). 
MLOps introduces a set of best practices to operate ML systems in production at scale **Amersfoort, "MLOps"**. However, despite the rise of MLOps practices the field of quality management for ML systems has received less attention. While there are studies mapping traditional quality management activities to ML systems **Kitchenham, "Software Metrics"**, as well as frameworks for ML quality assessment **Harrison, "ML Quality Assessment"**, none of the existing work provides concrete metrics to be measured.


%\input{includes/example_mat}
%\input{includes/compressed_subchar_table}
%\subsection{Maturity models for Machine Learning}
A concept orthogonal to the quality assessment is the definition of quality standards to determine the maturity of an ML system. The notion of maturity for software applications dates back to 1988 when the first maturity framework was introduced **CMMI, "Capability Maturity Model"**, aiming to improve software development processes, leading to later version of the Capability Maturity Model **Humphrey, "Software Quality and Productivity"**. 
The maturity model for ML systems is based on the idea that an organization can achieve different levels of maturity in terms of its ability to develop and maintain ML systems **Amersfoort, "MLOps"**.

\begin{table*}
\begin{tblr}[
  caption = \textbf{Full description of quality assessment requirements},
  entry = {Short Caption},
  label = {tab:full_qa},
]{
  colspec = {
    p{0.01\linewidth}
    p{0.06\linewidth}
    p{0.07\linewidth}
    p{0.18\linewidth}
    p{0.36\linewidth}
    p{0.01\linewidth}
    p{0.01\linewidth}
    p{0.01\linewidth}
    p{0.01\linewidth}}
    , 
    cell{1}{1} = {c=2}{l},
  %colspec = {p{2.4cm}p{2.7cm}XXX}, 
  rowhead = 1,
  cells = {font = \fontsize{7pt}{7pt}\selectfont},
  hlines,
}
 \textbf{Sub-Characteristic} & &\textbf{Min req. \ckmark} & \textbf{Full req. \doubleckmark} & \textbf{Reasoning} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
 
% ----------
\SetCell[r=5]{l} \rotatebox{90}{\textbf{Responsibility}} & Explainability & - & The system's predictions are explainable  & Explain the mechanism with which the system outputs its predictions is key for gaining stakeholders' trust **Adadi, "Explainable Machine Learning"**. & - & - & - & \doubleckmark & \doubleckmark \\
% ---------
& Fairness & - & The system has been checked against undesired biases and none were identified **Barocas, "Fairness in AI"**. & ML systems' predictions can be used to take irreversible decisions on behalf of customers, hence it is important that their performance is not affected by undesired biases **Datta, "Algorithmic Fairness"**.& \doubleckmark & \doubleckmark & \doubleckmark & \doubleckmark & \doubleckmark \\
% ---------
& Ownership & - & A team is appointed for maintaining the ML system **Carrillo, "Ownership in Software Engineering"**. & Ownership ensures that there is always an appointed individual to maintain the system in case of issues **Hall, "Software Maintenance"**. & \doubleckmark & \doubleckmark & \doubleckmark & \doubleckmark & \doubleckmark \\
% % \midrule
& Standards \mbox{Compliance} & - &Compliance standards, such as PII data handling, are met **Noble, "Data Protection by Design"**. & Adherence to applied regulatory standards is essential for the long-term viability of an ML system **Erlingson, "Regulatory Compliance in AI"**.  & \doubleckmark & \doubleckmark & \doubleckmark & \doubleckmark & \doubleckmark \\
% % \midrule
& Vulnerability & - & Bots are filtered out from the input data **Lowd, "Bots and Adversarial Attacks"**. & Existence of bots in data adds noise to the system which harms its performance and pose security risks **Wu, "Adversarial Attacks on ML"**. & \doubleckmark & \doubleckmark & \doubleckmark & \doubleckmark & \doubleckmark \\
\end{tblr}
\end{table*}