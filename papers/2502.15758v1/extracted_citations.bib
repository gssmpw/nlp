@article{AIMatModelReview,
  title={Artificial intelligence maturity model: a systematic literature review},
  author={Sadiq, Raghad Baker and Safie, Nurhizam and Abd Rahman, Abdul Hadi and Goudarzi, Shidrokh},
  journal={PeerJ Computer Science},
  volume={7},
  pages={e661},
  year={2021},
  publisher={PeerJ Inc.}
}

@inproceedings{Boehm,
author = {Boehm, B. W. and Brown, J. R. and Lipow, M.},
title = {Quantitative Evaluation of Software Quality},
year = {1976},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {The study reported in this paper establishes a conceptual framework and some key initial results in the analysis of the characteristics of software quality. Its main results and conclusions are:• Explicit attention to characteristics of software quality can lead to significant savings in software life-cycle costs.• The current software state-of-the-art imposes specific limitations on our ability to automatically and quantitatively evaluate the quality of software.• A definitive hierarchy of well-defined, well-differentiated characteristics of software quality is developed. Its higher-level structure reflects the actual uses to which software quality evaluation would be put; its lower-level characteristics are closely correlated with actual software metric evaluations which can be performed.• A large number of software quality-evaluation metrics have been defined, classified, and evaluated with respect to their potential benefits, quantifiability, and ease of automation.•Particular software life-cycle activities have been identified which have significant leverage on software quality.Most importantly, we believe that the study reported in this paper provides for the first time a clear, well-defined framework for assessing the often slippery issues associated with software quality, via the consistent and mutually supportive sets of definitions, distinctions, guidelines, and experiences cited. This framework is certainly not complete, but it has been brought to a point sufficient to serve as a viable basis for future refinements and extensions.},
booktitle = {Proceedings of the 2nd International Conference on Software Engineering},
pages = {592–605},
numpages = {14},
keywords = {Software quality, Software reliability, Software engineering, Management by objectives, Software standards, Quality assurance, Testing, Software measurement and evaluation, Quality characteristics, Quality metrics},
location = {San Francisco, California, USA},
series = {ICSE '76}
}

@techreport{CMM,
author={Paulk, Mark and Curtis, William and Chrissis, Mary Beth and Weber, Charles},
title={Capability Maturity Model for Software (Version 1.1)},
month={Feb},
year={1993},
number={CMU/SEI-93-TR-024},
howpublished={Carnegie Mellon University, Software Engineering Institute's Digital Library},
url={https://insights.sei.cmu.edu/library/capability-maturity-model-for-software-version-11/},
note={Accessed: 2023-Nov-30}
}

@article{Dromey,
author = {Dromey, R. Geoff},
title = {A Model for Software Product Quality},
year = {1995},
issue_date = {February 1995},
publisher = {IEEE Press},
volume = {21},
number = {2},
issn = {0098-5589},
url = {https://doi.org/10.1109/32.345830},
doi = {10.1109/32.345830},
abstract = {A model for software product quality is defined. It has been formulated by associating a set of quality-carrying properties with each of the structural forms that are used to define the statements and statement components of a programming language. These quality-carrying properties are in turn linked to the high-level quality attributes of the International Standard for Software Product Evaluation ISO-9126. The model supports building quality into software, definition of language-specific coding standards, systematically classifying quality defects, and the development of automated code auditors for detecting defects in software.},
journal = {IEEE Trans. Softw. Eng.},
month = {feb},
pages = {146–162},
numpages = {17},
keywords = {software characteristics, quality-carrying properties., quality attributes, Software quality, maintainability, product evaluation, quality model, ISO-9126, quality defect classification, code auditing}
}

@misc{Google-Mat-Model,
 title = {MLOps: Continuous delivery and automation pipelines in machine learning},
  howpublished = {\url{https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning}},
  note = {Accessed: 2023-11-24},
 year={2023}
}

@book{Grady,
author = {Grady, Robert B.},
title = {Practical Software Metrics for Project Management and Process Improvement},
year = {1992},
isbn = {0137203845},
publisher = {Prentice-Hall, Inc.},
address = {USA}
}

@book{ISO9126,
  added-at = {2014-07-08T11:07:29.000+0200},
  author = {{ISO/IEC 9126}},
  biburl = {https://www.bibsonomy.org/bibtex/2a7b775cd08a9182a7b88d5ca68657a4a/andiv},
  interhash = {ef6f58a2241715facdfa7424be170ea3},
  intrahash = {a7b775cd08a9182a7b88d5ca68657a4a},
  keywords = {ISO functional non},
  publisher = {ISO/IEC},
  timestamp = {2014-07-08T11:07:50.000+0200},
  title = {ISO/IEC 9126. Software engineering -- Product quality},
  year = 2001
}

@inproceedings{Kohavi-rules-of-thumb,
author = {Kohavi, Ron and Deng, Alex and Longbotham, Roger and Xu, Ya},
title = {Seven Rules of Thumb for Web Site Experimenters},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
series = {KDD '14},
year = {2014},
isbn = {978-1-4503-2956-9},
location = {New York, New York, USA},
pages = {1857--1866},
numpages = {10},
url = {http://doi.acm.org/10.1145/2623330.2623341},
doi = {10.1145/2623330.2623341},
acmid = {2623341},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {a/b testing, controlled experiments, randomized experiments}, }

@article{MLOps-Overview,
  author={Kreuzberger, Dominik and Kühl, Niklas and Hirschl, Sebastian},
  journal={IEEE Access}, 
  title={Machine Learning Operations (MLOps): Overview, Definition, and Architecture}, 
  year={2023},
  volume={11},
  number={},
  pages={31866-31879},
  doi={10.1109/ACCESS.2023.3262138}}

@article{McCall,
author = {Cavano, Joseph P. and McCall, James A.},
title = {A Framework for the Measurement of Software Quality},
year = {1978},
issue_date = {November 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/953579.811113},
doi = {10.1145/953579.811113},
abstract = {Research in software metrics incorporated in a framework established for software quality measurement can potentially provide significant benefits to software quality assurance programs. The research described has been conducted by General Electric Company for the Air Force Systems Command Rome Air Development Center. The problems encountered defining software quality and the approach taken to establish a framework for the measurement of software quality are described in this paper.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {133–139},
numpages = {7}
}

@misc{Microsoft-Mat-Model,
 title = {Machine Learning operations maturity model},
  howpublished = {\url{https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/mlops-maturity-model}},
  note = {Accessed: 2023-11-24},
  year={2023}
}

@article{Miguel,
author = {Miguel, Jose and Mauricio, David and Rodriguez, Glen},
year = {2014},
month = {11},
pages = {31-54},
title = {A Review of Software Quality Models for the Evaluation of Software Products},
volume = {5},
journal = {International journal of Software Engineering \& Applications},
doi = {10.5121/ijsea.2014.5603}
}

@inproceedings{Rubric,
title	= {The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction},
author	= {Eric Breck and Shanqing Cai and Eric Nielsen and Michael Salib and D. Sculley},
year	= {2017},
booktitle	= {Proceedings of IEEE Big Data}
}

@article{SQAM-systematic-mapping,
  title={Software quality assessment model: a systematic mapping study},
  author={Meng Yan and Xin Xia and Xiaohong Zhang and Ling Xu and Dan Yang and Shanping Li},
  journal={Science China Information Sciences},
  year={2019},
  volume={62},
  url={https://api.semanticscholar.org/CorpusID:53378706}
}

@article{a-software-engineering-perspective,
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
journal = {Journal of Systems and Software},
volume = {180},
pages = {111031},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111031},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100128X},
author = {Görkem Giray},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review}
}

@inproceedings{bernardi2019150,
  title={150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com},
  author={Bernardi, Lucas and Mavridis, Themistoklis and Estevez, Pablo},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1743--1751},
  year={2019}
}

@inproceedings{booking2021personalization,
  title={Personalization in Practice: Methods and Applications},
  author={Goldenberg, Dmitri and Kofman, Kostia and Albert, Javier and Mizrachi, Sarai and Horowitz, Adam and Teinemaa, Irene},
  booktitle={Proceedings of the 14th International Conference on Web Search and Data Mining},
  year={2021}
}

@article{challenges-in-deploying-ML,
author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533378},
doi = {10.1145/3533378},
abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {114},
numpages = {29},
keywords = {Machine learning applications, sofware deployment}
}

@article{chouliaras2023best,
  title={Best Practices for Machine Learning Systems: An Industrial Framework for Analysis and Optimization},
  author={Chouliaras, Georgios Christos and Kie{\l}czewski, Kornel and Beka, Amit and Konopnicki, David and Bernardi, Lucas},
  journal={arXiv preprint arXiv:2306.13662},
  year={2023}
}

@article{code-readablity,
author = {Scalabrino, Simone and Linares‐Vásquez, Mario and Oliveto, Rocco and Poshyvanyk, Denys},
year = {2018},
month = {06},
pages = {},
title = {A comprehensive model for code readability},
volume = {30},
journal = {Journal of Software: Evolution and Process},
doi = {10.1002/smr.1958}
}

@article{concept-drift-adaptation,
author = {Gama, Jo\~{a}o and \v{Z}liobaitundefined, Indrundefined and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
title = {A Survey on Concept Drift Adaptation},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2523813},
doi = {10.1145/2523813},
abstract = {Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this article, we characterize adaptive learning processes; categorize existing strategies for handling concept drift; overview the most representative, distinct, and popular techniques and algorithms; discuss evaluation methodology of adaptive algorithms; and present a set of illustrative applications. The survey covers the different facets of concept drift in an integrated way to reflect on the existing scattered state of the art. Thus, it aims at providing a comprehensive introduction to the concept drift adaptation for researchers, industry analysts, and practitioners.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {44},
numpages = {37},
keywords = {data streams, Concept drift, adaptive learning, change detection}
}

@article{crisp-ML,
AUTHOR = {Studer, Stefan and Bui, Thanh Binh and Drescher, Christian and Hanuschkin, Alexander and Winkler, Ludwig and Peters, Steven and Müller, Klaus-Robert},
TITLE = {Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {3},
YEAR = {2021},
NUMBER = {2},
PAGES = {392--413},
URL = {https://www.mdpi.com/2504-4990/3/2/20},
ISSN = {2504-4990},
ABSTRACT = {Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.},
DOI = {10.3390/make3020020}
}

@article{data-poisoning,
title = {Data poisoning attacks against machine learning algorithms},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118101},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118101},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422012933},
author = {Fahri Anıl Yerlikaya and Şerif Bahtiyar},
keywords = {Cybersecurity, Machine learning, Adversarial attack, Data poisoning, Label flipping attack},
abstract = {For the past decade, machine learning technology has increasingly become popular and it has been contributing to many areas that have the potential to influence the society considerably. Generally, machine learning is used by various industries to enhance their performances. Moreover, machine learning algorithms are used to solve some hard problems of systems that may contain very critical information. This makes machine learning algorithms a target of adversaries, which is an important problem for systems that use such algorithms. Therefore, it is significant to determine the performance and the robustness of a machine learning algorithm against attacks. In this paper, we analyze empirically the robustness and performances of six machine learning algorithms against two types of adversarial attacks by using four different datasets and three metrics. In our experiments, we analyze the robustness of Support Vector Machine, Stochastic Gradient Descent, Logistic Regression, Random Forest, Gaussian Naive Bayes, and K-Nearest Neighbor algorithms to create learning models. We observe their performances in spam, botnet, malware, and cancer detection datasets when we launch adversarial attacks against these environments. We use data poisoning for manipulating training data during adversarial attacks, which are random label flipping and distance-based label flipping attacks. We analyze the performance of each algorithm for a specific dataset by modifying the amount of poisoned data and analyzing behaviors of accuracy rate, f1-score, and AUC score. Analyses results show that machine learning algorithms have various performance results and robustness under different adversarial attacks. Moreover, machine learning algorithms are affected differently in each stage of an adversarial attacks. Furthermore, the behavior of a machine learning algorithm highly depends on the type of the dataset. On the other hand, some machine learning algorithms have better robustness and performance results against adversarial attacks for almost all datasets.}
}

@article{de2021artificial,
  title={Artificial intelligence regulation: a framework for governance},
  author={de Almeida, Patricia Gomes R{\^e}go and dos Santos, Carlos Denner and Farias, Josivania Silva},
  journal={Ethics and Information Technology},
  volume={23},
  number={3},
  pages={505--525},
  year={2021},
  publisher={Springer}
}

@article{dixon2023principled,
  title={A principled governance for emerging AI regimes: lessons from China, the European Union, and the United States},
  author={Dixon, Ren Bin Lee},
  journal={AI and Ethics},
  volume={3},
  number={3},
  pages={793--810},
  year={2023},
  publisher={Springer}
}

@article{efficient-ml-review,
title = {Efficient Machine Learning for Big Data: A Review},
journal = {Big Data Research},
volume = {2},
number = {3},
pages = {87-93},
year = {2015},
note = {Big Data, Analytics, and High-Performance Computing},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000271},
author = {Omar Y. Al-Jarrah and Paul D. Yoo and Sami Muhaidat and George K. Karagiannidis and Kamal Taha},
keywords = {Big data, Green computing, Efficient machine learning, Computational modeling},
abstract = {With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years – in fact, as much as 90% of current data were created in the last couple of years – a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven – the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas' structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.}
}

@ARTICLE{explainable-ml-princinples,
AUTHOR={Belle, Vaishak and Papantonis, Ioannis},   
TITLE={Principles and Practice of Explainable Machine Learning},      
JOURNAL={Frontiers in Big Data},      
VOLUME={4},           
YEAR={2021},      
URL={https://www.frontiersin.org/articles/10.3389/fdata.2021.688969},       
DOI={10.3389/fdata.2021.688969},      
ISSN={2624-909X},   
ABSTRACT={Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.}
}

@inproceedings{google-data-validation,
title	= {Data Validation for Machine Learning},
author	= {Eric Breck and Marty Zinkevich and Neoklis Polyzotis and Steven Whang and Sudip Roy},
year	= {2019},
URL	= {https://mlsys.org/Conferences/2019/doc/2019/167.pdf},
booktitle	= {Proceedings of SysML}
}

@inproceedings{google-latency, series={CHIIR ’21},
   title={Impact of Response Latency on User Behaviour in Mobile Web Search},
   url={http://dx.doi.org/10.1145/3406522.3446038},
   DOI={10.1145/3406522.3446038},
   booktitle={Proceedings of the 2021 Conference on Human Information Interaction and Retrieval},
   publisher={ACM},
   author={Arapakis, Ioannis and Park, Souneil and Pielot, Martin},
   year={2021},
   month=mar, collection={CHIIR ’21} }

@book{huyen2022designing,
  title={Designing machine learning systems},
  author={Huyen, Chip},
  year={2022},
  publisher={" O'Reilly Media, Inc."}
}

@misc{ibm-maturity-framework,
      title={Characterizing machine learning process: A maturity framework}, 
      author={Rama Akkiraju and Vibha Sinha and Anbang Xu and Jalal Mahmud and Pritam Gundecha and Zhe Liu and Xiaotong Liu and John Schumacher},
      year={2018},
      eprint={1811.04871},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{kohavi2022b,
  title={A/b testing intuition busters: Common misunderstandings in online controlled experiments},
  author={Kohavi, Ron and Deng, Alex and Vermeer, Lukas},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={3168--3177},
  year={2022}
}

@article{large-scale-ML,
title = {Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions},
journal = {Information and Software Technology},
volume = {127},
pages = {106368},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106368},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301373},
author = {Lucy Ellen Lwakatare and Aiswarya Raj and Ivica Crnkovic and Jan Bosch and Helena Holmström Olsson},
keywords = {Machine learning systems, Software engineering, Industrial settings, Challenges, Solutions, SLR},
abstract = {Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.}
}

@INPROCEEDINGS{maintainability,
  author={Chen, Celia and Alfayez, Reem and Srisopha, Kamonphop and Boehm, Barry and Shi, Lin},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)}, 
  title={Why Is It Important to Measure Maintainability and What Are the Best Ways to Do It?}, 
  year={2017},
  volume={},
  number={},
  pages={377-378},
  doi={10.1109/ICSE-C.2017.75}}

@inproceedings{mat-model-for-analysis,
author = {Mateo-Casali, Miguel A. and Fraile, Francisco and Boza, Andrés and Nazarenko, Artem},
year = {2023},
month = {03},
title = {Maturity Model for Analysis of Machine Learning Operations in Industry},
isbn = {978-3-031-27914-0},
doi = {10.1007/978-3-031-27915-7_57}
}

@article{mat-model-software-product,
author = {Al-Qutaish, Rafa and Abran, Alain},
year = {2011},
month = {11},
pages = {307-327},
title = {A Maturity Model of Software Product Quality},
volume = {43},
journal = {Journal of Research and Practice in Information Technology}
}

@inproceedings{microsoft-ownership,
author = {Bird, Christian and Nagappan, Nachi and Murphy, Brendan and Gall, Harald and Devanbu, Premkumar},
title = {Don't Touch My Code! Examining the Effects of Ownership on Software Quality},
booktitle = {Proceedings of the the eighth joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
year = {2011},
month = {September},
abstract = {Ownership is a key aspect of large-scale software development. We examine the relationship between different ownership measures and software failures in two large software projects: Windows Vista and Windows 7. We find that in all cases, measures of ownership such as the number of low-expertise developers, and the proportion of ownership for the top owner have a relationship with both pre-release faults and post-release failures. We also empirically identify reasons that low-expertise developers make changes to components and show that the removal of low-expertise contributions dramatically decreases the performance of contribution based defect prediction. Finally we provide recommendations for source code change policies and utilization of resources such as code inspections based on our results.},
publisher = {ACM},
url = {https://www.microsoft.com/en-us/research/publication/dont-touch-my-code-examining-the-effects-of-ownership-on-software-quality/},
edition = {Proceedings of the the eighth joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
}

@inproceedings{ml-documentation,
author = {Chang, Jiyoo and Custis, Christine},
title = {Understanding Implementation Challenges in Machine Learning Documentation},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555301},
doi = {10.1145/3551624.3555301},
abstract = {The lack of transparency in machine learning (ML) systems makes it difficult to identify sources of potential risks and harms. In recent years, various organizations have proposed standardized frameworks and processes for documentation for ML systems. However, it remains unclear how practitioners should implement and operationalize ML documentation in their workflows. We conducted semi-structured interviews with 24 practitioners in various organizational contexts to identify key implementation challenges and strategies for alleviating these challenges. Our findings indicated that addressing the why, how, and what of documentation is critical for implementing robust documentation practices.},
booktitle = {Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {16},
numpages = {8},
keywords = {ML model evaluation, datasheets, model cards, documentation, standardization, implementation},
location = {, Arlington, VA, USA, },
series = {EAAMO '22}
}

@article{ml-fairness,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {115},
numpages = {35},
keywords = {machine learning, deep learning, Fairness and bias in artificial intelligence, natural language processing, representation learning}
}

@misc{ml-privacy-meter,
      title={ML Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning}, 
      author={Sasi Kumar Murakonda and Reza Shokri},
      year={2020},
      eprint={2007.09339},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{models-measurement-QA,
author = {Mohanty, Siba N.},
title = {Models and Measurements for Quality Assessment of Software},
year = {1979},
issue_date = {Sept. 1979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/356778.356783},
doi = {10.1145/356778.356783},
journal = {ACM Comput. Surv.},
month = {sep},
pages = {251–275},
numpages = {25}
}

@Inbook{modularity,
author="Dennis, Jack B.",
editor="Bauer, F. L.
and Dennis, J. B.
and Waite, W. M.
and Gotlieb, C. C.
and Graham, R. M.
and Griffiths, M.
and Helms, H. J.
and Morton, B.
and Poole, P. C.
and Tsichritzis, D.",
title="Modularity",
bookTitle="Software Engineering: An Advanced Course",
year="1975",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="128--182",
isbn="978-3-540-37502-9",
doi="10.1007/3-540-07168-7_77",
url="https://doi.org/10.1007/3-540-07168-7_77"
}

@misc{monitoring-article,
    title = {Monitoring Machine Learning Models in Production},
    url = {https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/},
    author = {Christopher Samiullah},
    year = {2020},
    note = {Accessed on December 22, 2023}
}

@inproceedings{poran2022one,
  title={With One Voice: Composing a Travel Voice Assistant from Repurposed Models},
  author={Poran, Shachaf and Amsalem, Gil and Beka, Amit and Goldenberg, Dmitri},
  booktitle={Companion Proceedings of the Web Conference 2022},
  pages={383--387},
  year={2022}
}

@misc{quality-assurance-challenges,
      title={Quality Assurance Challenges for Machine Learning Software Applications During Software Development Life Cycle Phases}, 
      author={Md Abdullah Al Alamin and Gias Uddin},
      year={2021},
      eprint={2105.01195},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{quality-management,
      title={Quality Management of Machine Learning Systems}, 
      author={P. Santhanam},
      year={2020},
      eprint={2006.09529},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{resilient-ml,
author = "Adarsh Prasad",
title = "{Towards Robust and Resilient Machine Learning}",
year = "2022",
month = "4",
url = "https://kilthub.cmu.edu/articles/thesis/Towards_Robust_and_Resilient_Machine_Learning/19552420",
doi = "10.1184/R1/19552420.v1"
}

@article{sculley2015hidden,
  title={Hidden technical debt in machine learning systems},
  author={Sculley, David and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{software-process-maturity-framework,
  author={Humphrey, W.S.},
  journal={IEEE Software}, 
  title={Characterizing the software process: a maturity framework}, 
  year={1988},
  volume={5},
  number={2},
  pages={73-79},
  doi={10.1109/52.2014}}

@article{software-testing,
  title={Software testing},
  author={Pan, Jiantao},
  journal={Dependable Embedded Systems},
  volume={5},
  number={2006},
  pages={1},
  year={1999},
  publisher={Citeseer}
}

@INPROCEEDINGS{sre,
  author={Lyu, Michael R.},
  booktitle={Future of Software Engineering (FOSE '07)}, 
  title={Software Reliability Engineering: A Roadmap}, 
  year={2007},
  volume={},
  number={},
  pages={153-170},
  doi={10.1109/FOSE.2007.24}}

@misc{symeonidis2022mlops,
      title={MLOps -- Definitions, Tools and Challenges}, 
      author={G. Symeonidis and E. Nerantzis and A. Kazakis and G. A. Papakostas},
      year={2022},
      eprint={2201.00162},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{systematic-mapping-maturity-models,
title = {The maturity of maturity model research: A systematic mapping study},
journal = {Information and Software Technology},
volume = {54},
number = {12},
pages = {1317-1339},
year = {2012},
note = {Special Section on Software Reliability and Security},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001334},
author = {Roy Wendler},
keywords = {Maturity models, Software management, Design-oriented research, Systematic mapping study},
abstract = {Context
Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research.
Objective
The objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps.
Method
A systematic mapping study was conducted. It included relevant publications of journals and IS conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications.
Results
The mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given.
Conclusion
The mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist.}
}

@article{systematic-open-source-SQAM,
author = {Misra, Sanjay and Adewumi, Adewole and Omoregbe, Nicholas and Crawford, Broderick},
year = {2016},
month = {11},
pages = {1936},
title = {A systematic literature review of open source software quality assessment models},
volume = {2016},
journal = {SpringerPlus},
doi = {10.1186/s40064-016-3612-4}
}

@inbook{towards-guidelines-for-assessing,
   title={Towards Guidelines for Assessing Qualities of Machine Learning Systems},
   ISBN={9783030587932},
   ISSN={1865-0937},
   url={http://dx.doi.org/10.1007/978-3-030-58793-2_2},
   DOI={10.1007/978-3-030-58793-2_2},
   booktitle={Quality of Information and Communications Technology},
   publisher={Springer International Publishing},
   author={Siebert, Julien and Joeckel, Lisa and Heidrich, Jens and Nakamichi, Koji and Ohashi, Kyoko and Namba, Isao and Yamamoto, Rieko and Aoyama, Mikio},
   year={2020},
   pages={17–31} }

@INPROCEEDINGS{towardsMLOps-a-framework,
  author={John, Meenu Mary and Olsson, Helena Holmström and Bosch, Jan},
  booktitle={2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={Towards MLOps: A Framework and Maturity Model}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/SEAA53835.2021.00050}}

@article{veale2021demystifying,
  title={Demystifying the Draft EU Artificial Intelligence Act—Analysing the good, the bad, and the unclear elements of the proposed approach},
  author={Veale, Michael and Zuiderveen Borgesius, Frederik},
  journal={Computer Law Review International},
  volume={22},
  number={4},
  pages={97--112},
  year={2021},
  publisher={Verlag Dr. Otto Schmidt}
}

@inproceedings{wamlm2023,
author = {Goldenberg, Dmitri and Ross, Chana and Meir Lador, Shir and Cheong, Lin Lee and Xu, Panpan and Sokolova, Elena and Mandelbaum, Amit and Vasilinetc, Irina and Jain, Ankit and Weil Modlinger, Amit and Potdar, Saloni},
title = {The Second Workshop on Applied Machine Learning Management},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599206},
doi = {10.1145/3580305.3599206},
abstract = {Machine learning applications are rapidly adopted by industry leaders in any field. The growth of investment in AI-driven solutions created new challenges in managing Data Science and ML resources, people and projects as a whole. The discipline of managing applied machine learning teams, requires a healthy mix between agile product development tool-set and a long term research oriented mindset. The abilities of investing in deep research while at the same time connecting the outcomes to significant business results create a large knowledge based on management methods and best practices in the field. The Second KDD Workshop on Applied Machine Learning Management brings together applied research managers from various fields to share methodologies and case-studies on management of ML teams, products, and projects, achieving business impact with advanced AI-methods.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5859–5860},
numpages = {2},
keywords = {machine learning management, ml product development, data science management},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@book{wieder2011service,
  title={Service Level Agreements for Cloud Computing},
  author={Wieder, P. and Butler, J.M. and Theilmann, W. and Yahyapour, R.},
  isbn={9781461416142},
  series={SpringerLink : B{\"u}cher},
  url={https://books.google.nl/books?id=z306GUfFL5gC},
  year={2011},
  publisher={Springer New York}
}

