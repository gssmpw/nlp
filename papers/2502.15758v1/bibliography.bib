@article{sculley2015hidden,
  title={Hidden technical debt in machine learning systems},
  author={Sculley, David and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}



@inproceedings{wamlm2023,
author = {Goldenberg, Dmitri and Ross, Chana and Meir Lador, Shir and Cheong, Lin Lee and Xu, Panpan and Sokolova, Elena and Mandelbaum, Amit and Vasilinetc, Irina and Jain, Ankit and Weil Modlinger, Amit and Potdar, Saloni},
title = {The Second Workshop on Applied Machine Learning Management},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599206},
doi = {10.1145/3580305.3599206},
abstract = {Machine learning applications are rapidly adopted by industry leaders in any field. The growth of investment in AI-driven solutions created new challenges in managing Data Science and ML resources, people and projects as a whole. The discipline of managing applied machine learning teams, requires a healthy mix between agile product development tool-set and a long term research oriented mindset. The abilities of investing in deep research while at the same time connecting the outcomes to significant business results create a large knowledge based on management methods and best practices in the field. The Second KDD Workshop on Applied Machine Learning Management brings together applied research managers from various fields to share methodologies and case-studies on management of ML teams, products, and projects, achieving business impact with advanced AI-methods.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5859–5860},
numpages = {2},
keywords = {machine learning management, ml product development, data science management},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{booking2021personalization,
  title={Personalization in Practice: Methods and Applications},
  author={Goldenberg, Dmitri and Kofman, Kostia and Albert, Javier and Mizrachi, Sarai and Horowitz, Adam and Teinemaa, Irene},
  booktitle={Proceedings of the 14th International Conference on Web Search and Data Mining},
  year={2021}
}


@article{chouliaras2023best,
  title={Best Practices for Machine Learning Systems: An Industrial Framework for Analysis and Optimization},
  author={Chouliaras, Georgios Christos and Kie{\l}czewski, Kornel and Beka, Amit and Konopnicki, David and Bernardi, Lucas},
  journal={arXiv preprint arXiv:2306.13662},
  year={2023}
}

@book{huyen2022designing,
  title={Designing machine learning systems},
  author={Huyen, Chip},
  year={2022},
  publisher={" O'Reilly Media, Inc."}
}

@article{coates2014sox,
  title={SOX after ten years: A multidisciplinary review},
  author={Coates, John C and Srinivasan, Suraj},
  journal={Accounting Horizons},
  volume={28},
  number={3},
  pages={627--671},
  year={2014},
  publisher={American Accounting Assocation}
}


@article{de2021artificial,
  title={Artificial intelligence regulation: a framework for governance},
  author={de Almeida, Patricia Gomes R{\^e}go and dos Santos, Carlos Denner and Farias, Josivania Silva},
  journal={Ethics and Information Technology},
  volume={23},
  number={3},
  pages={505--525},
  year={2021},
  publisher={Springer}
}

@inproceedings{wang2023text2topic,
  title={Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities},
  author={Wang, Fengjun and Beladev, Moran and Kleinfeld, Ofri and Frayerman, Elina and Shachar, Tal and Fainman, Eran and Assaraf, Karen Lastmann and Mizrachi, Sarai and Wang, Benjamin},
booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  pages={93–103},
  year={2023}
}

@article{chorev2022deepchecks,
  title={Deepchecks: A library for testing and validating machine learning models and data},
  author={Chorev, Shir and Tannor, Philip and Israel, Dan Ben and Bressler, Noam and Gabbay, Itay and Hutnik, Nir and Liberman, Jonatan and Perlmutter, Matan and Romanyshyn, Yurii and Rokach, Lior},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={12990--12995},
  year={2022},
  publisher={JMLRORG}
}

@inproceedings{albert2022commerce,
  title={E-Commerce Promotions Personalization via Online Multiple-Choice Knapsack with Uplift Modeling},
  author={Albert, Javier and Goldenberg, Dmitri},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={2863--2872},
  year={2022}
}


@inproceedings{wang2023mumic,
  title={MuMIC--Multimodal Embedding for Multi-Label Image Classification with Tempered Sigmoid},
  author={Wang, Fengjun and Mizrachi, Sarai and Beladev, Moran and Nadav, Guy and Amsalem, Gil and Assaraf, Karen Lastmann and Boker, Hadas Harush},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={13},
  pages={15603--15611},
  year={2023}
}

@article{veale2021demystifying,
  title={Demystifying the Draft EU Artificial Intelligence Act—Analysing the good, the bad, and the unclear elements of the proposed approach},
  author={Veale, Michael and Zuiderveen Borgesius, Frederik},
  journal={Computer Law Review International},
  volume={22},
  number={4},
  pages={97--112},
  year={2021},
  publisher={Verlag Dr. Otto Schmidt}
}

@article{paleyes2022challenges,
  title={Challenges in deploying machine learning: a survey of case studies},
  author={Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D},
  journal={ACM Computing Surveys},
  volume={55},
  number={6},
  pages={1--29},
  year={2022},
  publisher={ACM New York, NY}
}


@article{lwakatare2020large,
  title={Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions},
  author={Lwakatare, Lucy Ellen and Raj, Aiswarya and Crnkovic, Ivica and Bosch, Jan and Olsson, Helena Holmstr{\"o}m},
  journal={Information and software technology},
  volume={127},
  pages={106368},
  year={2020},
  publisher={Elsevier}
}


@inproceedings{Kohavi2014seven,
author = {Kohavi, Ron and Deng, Alex and Longbotham, Roger and Xu, Ya},
title = {Seven Rules of Thumb for Web Site Experimenters},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623341},
doi = {10.1145/2623330.2623341},
abstract = {Web site owners, from small web sites to the largest properties that include Amazon, Facebook, Google, LinkedIn, Microsoft, and Yahoo, attempt to improve their web sites, optimizing for criteria ranging from repeat usage, time on site, to revenue. Having been involved in running thousands of controlled experiments at Amazon, Booking.com, LinkedIn, and multiple Microsoft properties, we share seven rules of thumb for experimenters, which we have generalized from these experiments and their results. These are principles that we believe have broad applicability in web optimization and analytics outside of controlled experiments, yet they are not provably correct, and in some cases exceptions are known.To support these rules of thumb, we share multiple real examples, most being shared in a public paper for the first time. Some rules of thumb have previously been stated, such as 'speed matters,' but we describe the assumptions in the experimental design and share additional experiments that improved our understanding of where speed matters more: certain areas of the web page are more critical.This paper serves two goals. First, it can guide experimenters with rules of thumb that can help them optimize their sites. Second, it provides the KDD community with new research challenges on the applicability, exceptions, and extensions to these, one of the goals for KDD's industrial track.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1857–1866},
numpages = {10},
keywords = {controlled experiments, randomized experiments, a/b testing},
location = {New York, New York, USA},
series = {KDD '14}
}


@inproceedings{poran2022one,
  title={With One Voice: Composing a Travel Voice Assistant from Repurposed Models},
  author={Poran, Shachaf and Amsalem, Gil and Beka, Amit and Goldenberg, Dmitri},
  booktitle={Companion Proceedings of the Web Conference 2022},
  pages={383--387},
  year={2022}
}

@inproceedings{mavridis2020beyond,
  title={Beyond algorithms: Ranking at scale at Booking. com},
  author={Mavridis, Themis and Hausl, Soraya and Mende, Andrew and Pagano, Roberto},
  booktitle={Proceedings of the Fourth Workshop on Recommendation in Complex Scenarios. CEUR-WS},
  year={2020}
}


@article{ross2022democratizing,
  title={Democratizing Travel Personalization via Central Recommendation Platform},
  author={Ross, Chana and Ovadia, Tomer and Mooney, Jake and Meitin, Amit and Kabilou, Eytan and Kabalo, Mush and Goldenberg, Dmitri},
  year={2022}
}


@inproceedings{booking2021dataset,
  author =       {Goldenberg, Dmitri  and Levin, Pavel},
  title =        {Booking.com Multi-Destination Trips Dataset},
  booktitle =   {
Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’21)},
  year =      {2021},
   doi   = {10.1145/3404835.3463240}
}

@article{d2003non,
  title={Non-inferiority trials: design concepts and issues--the encounters of academic consultants in statistics},
  author={D'Agostino Sr, Ralph B and Massaro, Joseph M and Sullivan, Lisa M},
  journal={Statistics in medicine},
  volume={22},
  number={2},
  pages={169--186},
  year={2003},
  publisher={Wiley Online Library}
}

@inproceedings{kohavi2013online,
  title={Online controlled experiments at large scale},
  author={Kohavi, Ron and Deng, Alex and Frasca, Brian and Walker, Toby and Xu, Ya and Pohlmann, Nils},
  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1168--1176},
  year={2013}
}

@article{dixon2023principled,
  title={A principled governance for emerging AI regimes: lessons from China, the European Union, and the United States},
  author={Dixon, Ren Bin Lee},
  journal={AI and Ethics},
  volume={3},
  number={3},
  pages={793--810},
  year={2023},
  publisher={Springer}
}

@inproceedings{kohavi2022b,
  title={A/b testing intuition busters: Common misunderstandings in online controlled experiments},
  author={Kohavi, Ron and Deng, Alex and Vermeer, Lukas},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={3168--3177},
  year={2022}
}

@article{kaufman2017democratizing,
  title={Democratizing online controlled experiments at Booking. com},
  author={Kaufman, Raphael Lopez and Pitchforth, Jegar and Vermeer, Lukas},
  journal={arXiv preprint arXiv:1710.08217},
  year={2017}
}



@inproceedings{kohavi2007practical,
  title={Practical guide to controlled experiments on the web: listen to your customers not to the hippo},
  author={Kohavi, Ron and Henne, Randal M and Sommerfield, Dan},
  booktitle={Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={959--967},
  year={2007}
}


@inproceedings{bernardi2019150,
  title={150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com},
  author={Bernardi, Lucas and Mavridis, Themistoklis and Estevez, Pablo},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1743--1751},
  year={2019}
}


@inproceedings{gupta2020challenges,
  title={Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
  author={Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin and Mukherjee, Avijit},
  booktitle={Proceedings of the 13th International Conference on Web Search and Data Mining},
  pages={877--880},
  year={2020}
}%https://dl.acm.org/doi/pdf/10.1145/3336191.3371871



@article{large-scale-ML,
title = {Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions},
journal = {Information and Software Technology},
volume = {127},
pages = {106368},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106368},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301373},
author = {Lucy Ellen Lwakatare and Aiswarya Raj and Ivica Crnkovic and Jan Bosch and Helena Holmström Olsson},
keywords = {Machine learning systems, Software engineering, Industrial settings, Challenges, Solutions, SLR},
abstract = {Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.}
}

@article{a-software-engineering-perspective,
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
journal = {Journal of Systems and Software},
volume = {180},
pages = {111031},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111031},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100128X},
author = {Görkem Giray},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review}
}


@article{challenges-in-deploying-ML,
author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533378},
doi = {10.1145/3533378},
abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {114},
numpages = {29},
keywords = {Machine learning applications, sofware deployment}
}

@misc{quality-management,
      title={Quality Management of Machine Learning Systems}, 
      author={P. Santhanam},
      year={2020},
      eprint={2006.09529},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{MLOps-Overview,
  author={Kreuzberger, Dominik and Kühl, Niklas and Hirschl, Sebastian},
  journal={IEEE Access}, 
  title={Machine Learning Operations (MLOps): Overview, Definition, and Architecture}, 
  year={2023},
  volume={11},
  number={},
  pages={31866-31879},
  doi={10.1109/ACCESS.2023.3262138}}

@misc{symeonidis2022mlops,
      title={MLOps -- Definitions, Tools and Challenges}, 
      author={G. Symeonidis and E. Nerantzis and A. Kazakis and G. A. Papakostas},
      year={2022},
      eprint={2201.00162},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Google-Mat-Model,
 title = {MLOps: Continuous delivery and automation pipelines in machine learning},
  howpublished = {\url{https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning}},
  note = {Accessed: 2023-11-24},
 year={2023}
} 

@misc{Microsoft-Mat-Model,
 title = {Machine Learning operations maturity model},
  howpublished = {\url{https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/mlops-maturity-model}},
  note = {Accessed: 2023-11-24},
  year={2023}
} 

@misc{ibm-maturity-framework,
      title={Characterizing machine learning process: A maturity framework}, 
      author={Rama Akkiraju and Vibha Sinha and Anbang Xu and Jalal Mahmud and Pritam Gundecha and Zhe Liu and Xiaotong Liu and John Schumacher},
      year={2018},
      eprint={1811.04871},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{towardsMLOps-a-framework,
  author={John, Meenu Mary and Olsson, Helena Holmström and Bosch, Jan},
  booktitle={2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={Towards MLOps: A Framework and Maturity Model}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/SEAA53835.2021.00050}}

@misc{characterizing-ml-processes,
      title={Characterizing machine learning process: A maturity framework}, 
      author={Rama Akkiraju and Vibha Sinha and Anbang Xu and Jalal Mahmud and Pritam Gundecha and Zhe Liu and Xiaotong Liu and John Schumacher},
      year={2018},
      eprint={1811.04871},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{AIMatModelReview,
  title={Artificial intelligence maturity model: a systematic literature review},
  author={Sadiq, Raghad Baker and Safie, Nurhizam and Abd Rahman, Abdul Hadi and Goudarzi, Shidrokh},
  journal={PeerJ Computer Science},
  volume={7},
  pages={e661},
  year={2021},
  publisher={PeerJ Inc.}
}

@inproceedings{mat-model-for-analysis,
author = {Mateo-Casali, Miguel A. and Fraile, Francisco and Boza, Andrés and Nazarenko, Artem},
year = {2023},
month = {03},
title = {Maturity Model for Analysis of Machine Learning Operations in Industry},
isbn = {978-3-031-27914-0},
doi = {10.1007/978-3-031-27915-7_57}
}

@misc{quality-assurance-challenges,
      title={Quality Assurance Challenges for Machine Learning Software Applications During Software Development Life Cycle Phases}, 
      author={Md Abdullah Al Alamin and Gias Uddin},
      year={2021},
      eprint={2105.01195},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}


@article{crisp-ML,
AUTHOR = {Studer, Stefan and Bui, Thanh Binh and Drescher, Christian and Hanuschkin, Alexander and Winkler, Ludwig and Peters, Steven and Müller, Klaus-Robert},
TITLE = {Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {3},
YEAR = {2021},
NUMBER = {2},
PAGES = {392--413},
URL = {https://www.mdpi.com/2504-4990/3/2/20},
ISSN = {2504-4990},
ABSTRACT = {Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.},
DOI = {10.3390/make3020020}
}


@inbook{towards-guidelines-for-assessing,
   title={Towards Guidelines for Assessing Qualities of Machine Learning Systems},
   ISBN={9783030587932},
   ISSN={1865-0937},
   url={http://dx.doi.org/10.1007/978-3-030-58793-2_2},
   DOI={10.1007/978-3-030-58793-2_2},
   booktitle={Quality of Information and Communications Technology},
   publisher={Springer International Publishing},
   author={Siebert, Julien and Joeckel, Lisa and Heidrich, Jens and Nakamichi, Koji and Ohashi, Kyoko and Namba, Isao and Yamamoto, Rieko and Aoyama, Mikio},
   year={2020},
   pages={17–31} }

@article{software-process-maturity-framework,
  author={Humphrey, W.S.},
  journal={IEEE Software}, 
  title={Characterizing the software process: a maturity framework}, 
  year={1988},
  volume={5},
  number={2},
  pages={73-79},
  doi={10.1109/52.2014}}

@techreport{CMM,
author={Paulk, Mark and Curtis, William and Chrissis, Mary Beth and Weber, Charles},
title={Capability Maturity Model for Software (Version 1.1)},
month={Feb},
year={1993},
number={CMU/SEI-93-TR-024},
howpublished={Carnegie Mellon University, Software Engineering Institute's Digital Library},
url={https://insights.sei.cmu.edu/library/capability-maturity-model-for-software-version-11/},
note={Accessed: 2023-Nov-30}
}


@inproceedings{kornilova2021mining,
  title={Mining the stars: learning quality ratings with user-facing explanations for vacation rentals},
  author={Kornilova, Anastasiia and Bernardi, Lucas},
  booktitle={Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  pages={976--983},
  year={2021}
}

@article{systematic-mapping-maturity-models,
title = {The maturity of maturity model research: A systematic mapping study},
journal = {Information and Software Technology},
volume = {54},
number = {12},
pages = {1317-1339},
year = {2012},
note = {Special Section on Software Reliability and Security},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001334},
author = {Roy Wendler},
keywords = {Maturity models, Software management, Design-oriented research, Systematic mapping study},
abstract = {Context
Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research.
Objective
The objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps.
Method
A systematic mapping study was conducted. It included relevant publications of journals and IS conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications.
Results
The mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given.
Conclusion
The mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist.}
}

@article{mat-model-software-product,
author = {Al-Qutaish, Rafa and Abran, Alain},
year = {2011},
month = {11},
pages = {307-327},
title = {A Maturity Model of Software Product Quality},
volume = {43},
journal = {Journal of Research and Practice in Information Technology}
}

@inproceedings{google-data-validation,
title	= {Data Validation for Machine Learning},
author	= {Eric Breck and Marty Zinkevich and Neoklis Polyzotis and Steven Whang and Sudip Roy},
year	= {2019},
URL	= {https://mlsys.org/Conferences/2019/doc/2019/167.pdf},
booktitle	= {Proceedings of SysML}
}

@inproceedings{Kohavi-rules-of-thumb,
author = {Kohavi, Ron and Deng, Alex and Longbotham, Roger and Xu, Ya},
title = {Seven Rules of Thumb for Web Site Experimenters},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
series = {KDD '14},
year = {2014},
isbn = {978-1-4503-2956-9},
location = {New York, New York, USA},
pages = {1857--1866},
numpages = {10},
url = {http://doi.acm.org/10.1145/2623330.2623341},
doi = {10.1145/2623330.2623341},
acmid = {2623341},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {a/b testing, controlled experiments, randomized experiments}, }

@INPROCEEDINGS{sre,
  author={Lyu, Michael R.},
  booktitle={Future of Software Engineering (FOSE '07)}, 
  title={Software Reliability Engineering: A Roadmap}, 
  year={2007},
  volume={},
  number={},
  pages={153-170},
  doi={10.1109/FOSE.2007.24}}

@article{concept-drift-adaptation,
author = {Gama, Jo\~{a}o and \v{Z}liobaitundefined, Indrundefined and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
title = {A Survey on Concept Drift Adaptation},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2523813},
doi = {10.1145/2523813},
abstract = {Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this article, we characterize adaptive learning processes; categorize existing strategies for handling concept drift; overview the most representative, distinct, and popular techniques and algorithms; discuss evaluation methodology of adaptive algorithms; and present a set of illustrative applications. The survey covers the different facets of concept drift in an integrated way to reflect on the existing scattered state of the art. Thus, it aims at providing a comprehensive introduction to the concept drift adaptation for researchers, industry analysts, and practitioners.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {44},
numpages = {37},
keywords = {data streams, Concept drift, adaptive learning, change detection}
}

@inproceedings{microsoft-ownership,
author = {Bird, Christian and Nagappan, Nachi and Murphy, Brendan and Gall, Harald and Devanbu, Premkumar},
title = {Don't Touch My Code! Examining the Effects of Ownership on Software Quality},
booktitle = {Proceedings of the the eighth joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
year = {2011},
month = {September},
abstract = {Ownership is a key aspect of large-scale software development. We examine the relationship between different ownership measures and software failures in two large software projects: Windows Vista and Windows 7. We find that in all cases, measures of ownership such as the number of low-expertise developers, and the proportion of ownership for the top owner have a relationship with both pre-release faults and post-release failures. We also empirically identify reasons that low-expertise developers make changes to components and show that the removal of low-expertise contributions dramatically decreases the performance of contribution based defect prediction. Finally we provide recommendations for source code change policies and utilization of resources such as code inspections based on our results.},
publisher = {ACM},
url = {https://www.microsoft.com/en-us/research/publication/dont-touch-my-code-examining-the-effects-of-ownership-on-software-quality/},
edition = {Proceedings of the the eighth joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
}

@book{wieder2011service,
  title={Service Level Agreements for Cloud Computing},
  author={Wieder, P. and Butler, J.M. and Theilmann, W. and Yahyapour, R.},
  isbn={9781461416142},
  series={SpringerLink : B{\"u}cher},
  url={https://books.google.nl/books?id=z306GUfFL5gC},
  year={2011},
  publisher={Springer New York}
}


@article{Spark,
author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
title = {Apache Spark: A Unified Engine for Big Data Processing},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/2934664},
doi = {10.1145/2934664},
abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
journal = {Commun. ACM},
month = {oct},
pages = {56–65},
numpages = {10}
}

@INPROCEEDINGS{safety-critical,
  author={Knight, J.C.},
  booktitle={Proceedings of the 24th International Conference on Software Engineering. ICSE 2002}, 
  title={Safety critical systems: challenges and directions}, 
  year={2002},
  volume={},
  number={},
  pages={547-550},
  doi={}}

@misc{List_of_system_quality_attributes,
   author = "Wikipedia",
   title = "{List of system quality attributes} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2022",
   howpublished = {\url{https://en.wikipedia.org/wiki/List\_of\_system\_quality\_attributes}},
   note = "[Online; accessed 30-April-2022]"
 }
 

@inproceedings{Amershi,  author={Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},   title={Software Engineering for Machine Learning: A Case Study},   year={2019},  volume={},  number={},  pages={291-300},  doi={10.1109/ICSE-SEIP.2019.00042}}

@inproceedings{Serban,
author = {Serban, Alex and van der Blom, Koen and Hoos, Holger and Visser, Joost},
title = {Adoption and Effects of Software Engineering Best Practices in Machine Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410681},
doi = {10.1145/3382494.3410681},
abstract = {Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner.Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components.Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models.Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied.Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {3},
numpages = {12},
keywords = {survey, best practices, machine learning engineering},
location = {Bari, Italy},
series = {ESEM '20}
}

@misc{se_ml_website, 
title = {Software engineering for machine learning}, url = {https://se-ml.github.io/}, 
urldate = {2022-05-11},
journal = {SE}, 
year = {2022},
author = {Serban, Alex and van der Blom, Koen and Hoos, Holger and Visser, Joost}
}

@inproceedings{Rubric,
title	= {The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction},
author	= {Eric Breck and Shanqing Cai and Eric Nielsen and Michael Salib and D. Sculley},
year	= {2017},
booktitle	= {Proceedings of IEEE Big Data}
}

@misc{parzen-estimator,
      title={Tree-Structured Parzen Estimator: Understanding Its Algorithm Components and Their Roles for Better Empirical Performance}, 
      author={Shuhei Watanabe},
      year={2023},
      eprint={2304.11127},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{google-latency, series={CHIIR ’21},
   title={Impact of Response Latency on User Behaviour in Mobile Web Search},
   url={http://dx.doi.org/10.1145/3406522.3446038},
   DOI={10.1145/3406522.3446038},
   booktitle={Proceedings of the 2021 Conference on Human Information Interaction and Retrieval},
   publisher={ACM},
   author={Arapakis, Ioannis and Park, Souneil and Pielot, Martin},
   year={2021},
   month=mar, collection={CHIIR ’21} }

@article{efficient-ml-review,
title = {Efficient Machine Learning for Big Data: A Review},
journal = {Big Data Research},
volume = {2},
number = {3},
pages = {87-93},
year = {2015},
note = {Big Data, Analytics, and High-Performance Computing},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000271},
author = {Omar Y. Al-Jarrah and Paul D. Yoo and Sami Muhaidat and George K. Karagiannidis and Kamal Taha},
keywords = {Big data, Green computing, Efficient machine learning, Computational modeling},
abstract = {With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years – in fact, as much as 90% of current data were created in the last couple of years – a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven – the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas' structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.}
}

@article{resilient-ml,
author = "Adarsh Prasad",
title = "{Towards Robust and Resilient Machine Learning}",
year = "2022",
month = "4",
url = "https://kilthub.cmu.edu/articles/thesis/Towards_Robust_and_Resilient_Machine_Learning/19552420",
doi = "10.1184/R1/19552420.v1"
}

@INPROCEEDINGS{maintainability,
  author={Chen, Celia and Alfayez, Reem and Srisopha, Kamonphop and Boehm, Barry and Shi, Lin},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)}, 
  title={Why Is It Important to Measure Maintainability and What Are the Best Ways to Do It?}, 
  year={2017},
  volume={},
  number={},
  pages={377-378},
  doi={10.1109/ICSE-C.2017.75}}

@Inbook{modularity,
author="Dennis, Jack B.",
editor="Bauer, F. L.
and Dennis, J. B.
and Waite, W. M.
and Gotlieb, C. C.
and Graham, R. M.
and Griffiths, M.
and Helms, H. J.
and Morton, B.
and Poole, P. C.
and Tsichritzis, D.",
title="Modularity",
bookTitle="Software Engineering: An Advanced Course",
year="1975",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="128--182",
isbn="978-3-540-37502-9",
doi="10.1007/3-540-07168-7_77",
url="https://doi.org/10.1007/3-540-07168-7_77"
}

@article{software-testing,
  title={Software testing},
  author={Pan, Jiantao},
  journal={Dependable Embedded Systems},
  volume={5},
  number={2006},
  pages={1},
  year={1999},
  publisher={Citeseer}
}

@misc{monitoring-article,
    title = {Monitoring Machine Learning Models in Production},
    url = {https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/},
    author = {Christopher Samiullah},
    year = {2020},
    note = {Accessed on December 22, 2023}
}

@article{code-readablity,
author = {Scalabrino, Simone and Linares‐Vásquez, Mario and Oliveto, Rocco and Poshyvanyk, Denys},
year = {2018},
month = {06},
pages = {},
title = {A comprehensive model for code readability},
volume = {30},
journal = {Journal of Software: Evolution and Process},
doi = {10.1002/smr.1958}
}

skip to main content
ACM Digital Library home
 
ACM home
BrowseAbout
Sign in Register
Journals
Magazines
Proceedings
Books
SIGs
Conferences
People
Search ACM Digital Library
Search ACM Digital Library

 Advanced Search
Conference
Proceedings
Upcoming Events
Authors
Affiliations
Award Winners
HomeConferencesEAAMOProceedingsEAAMO '22Understanding Implementation Challenges in Machine Learning Documentation
RESEARCH-ARTICLE OPEN ACCESS
SHARE ON
Understanding Implementation Challenges in Machine Learning Documentation
Authors: 
Jiyoo Chang

, 
Christine Custis

 Authors Info & Claims
EAAMO '22: Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and OptimizationOctober 2022Article No.: 16Pages 1–8https://doi.org/10.1145/3551624.3555301
Published:17 October 2022Publication HistoryCheck for updates on crossmark
2citation340Downloads
 
eReaderPDF
EAAMO '22: Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization
Understanding Implementation Challenges in Machine Learning Documentation
Pages 1–8
PreviousNext
ABSTRACT
References
Cited By
Index Terms
Recommendations
Comments
ACM Digital Library
ABSTRACT
The lack of transparency in machine learning (ML) systems makes it difficult to identify sources of potential risks and harms. In recent years, various organizations have proposed standardized frameworks and processes for documentation for ML systems. However, it remains unclear how practitioners should implement and operationalize ML documentation in their workflows. We conducted semi-structured interviews with 24 practitioners in various organizational contexts to identify key implementation challenges and strategies for alleviating these challenges. Our findings indicated that addressing the why, how, and what of documentation is critical for implementing robust documentation practices.

References
Matthew Arnold, Rachel K. E. Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsilovic, Ravi Nair, Karthikeyan Natesan Ramamurthy, Darrell Reimer, Alexandra Olteanu, David Piorkowski, Jason Tsay, and Kush R. Varshney. 2019. FactSheets: Increasing Trust in AI Services through Supplier's Declarations of Conformity. arXiv:1808.07261 [cs] (Feb. 2019). http://arxiv.org/abs/1808.07261 arXiv: 1808.07261.Google Scholar
Jack Bandy and Nicholas Vincent. 2021. Addressing "Documentation Debt" in Machine Learning Research: A Retrospective Datasheet for BookCorpus. arXiv:2105.05241 [cs] (May 2021). http://arxiv.org/abs/2105.05241 arXiv: 2105.05241.Google Scholar
Emily M. Bender and Batya Friedman. 2018. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. Transactions of the Association for Computational Linguistics 6 (Dec. 2018), 587–604. https://doi.org/10.1162/tacl_a_00041Google Scholar
Show All References
Cited By
View all
Mysore S, Jasim M, Song H, Akbar S, Randall A and Mahyar N. How Data Scientists Review the Scholarly Literature. Proceedings of the 2023 Conference on Human Information Interaction and Retrieval. (137-152).
https://doi.org/10.1145/3576840.3578309

Nahar N, Zhang H, Lewis G, Zhou S and Kästner C. (2023). A Meta-Summary of Challenges in Building Products with ML Components – Collecting Experiences from 4758+ Practitioners 2023 IEEE/ACM 2nd International Conference on AI Engineering – Software Engineering for AI (CAIN). 10.1109/CAIN58948.2023.00034. 979-8-3503-0113-7. (171-183).
https://ieeexplore.ieee.org/document/10164747/

Index Terms
Understanding Implementation Challenges in Machine Learning Documentation
Social and professional topics

Professional topics

Management of computing and information systems

Implementation management

Recommendations
Understanding Machine Learning Practitioners' Data Documentation Perceptions, Needs, Challenges, and Desiderata
CSCW
Data is central to the development and evaluation of machine learning (ML) models. However, the use of problematic or inappropriate datasets can result in harms when the resulting models are deployed. To encourage responsible AI practice through more ...

Read More
Model Cards for Model Reporting
FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency
Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in ...

Read More
Strategically Using Applied Machine Learning for Accessibility Documentation in the Built Environment
Human-Computer Interaction – INTERACT 2021
Abstract
There has been a considerable amount of research aimed at automating the documentation of accessibility in the built environment. Yet so far, there has been no fully automatic system that has been shown to reliably document surface quality ...

Read More
Comments





32References



View Table Of Contents
Back
Close modal
Export Citations

BibTeX
@inproceedings{ml-documentation,
author = {Chang, Jiyoo and Custis, Christine},
title = {Understanding Implementation Challenges in Machine Learning Documentation},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555301},
doi = {10.1145/3551624.3555301},
abstract = {The lack of transparency in machine learning (ML) systems makes it difficult to identify sources of potential risks and harms. In recent years, various organizations have proposed standardized frameworks and processes for documentation for ML systems. However, it remains unclear how practitioners should implement and operationalize ML documentation in their workflows. We conducted semi-structured interviews with 24 practitioners in various organizational contexts to identify key implementation challenges and strategies for alleviating these challenges. Our findings indicated that addressing the why, how, and what of documentation is critical for implementing robust documentation practices.},
booktitle = {Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {16},
numpages = {8},
keywords = {ML model evaluation, datasheets, model cards, documentation, standardization, implementation},
location = {, Arlington, VA, USA, },
series = {EAAMO '22}
}

@ARTICLE{explainable-ml-princinples,
AUTHOR={Belle, Vaishak and Papantonis, Ioannis},   
TITLE={Principles and Practice of Explainable Machine Learning},      
JOURNAL={Frontiers in Big Data},      
VOLUME={4},           
YEAR={2021},      
URL={https://www.frontiersin.org/articles/10.3389/fdata.2021.688969},       
DOI={10.3389/fdata.2021.688969},      
ISSN={2624-909X},   
ABSTRACT={Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.}
}

@article{ml-fairness,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {115},
numpages = {35},
keywords = {machine learning, deep learning, Fairness and bias in artificial intelligence, natural language processing, representation learning}
}

@misc{ml-privacy-meter,
      title={ML Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning}, 
      author={Sasi Kumar Murakonda and Reza Shokri},
      year={2020},
      eprint={2007.09339},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{data-poisoning,
title = {Data poisoning attacks against machine learning algorithms},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118101},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118101},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422012933},
author = {Fahri Anıl Yerlikaya and Şerif Bahtiyar},
keywords = {Cybersecurity, Machine learning, Adversarial attack, Data poisoning, Label flipping attack},
abstract = {For the past decade, machine learning technology has increasingly become popular and it has been contributing to many areas that have the potential to influence the society considerably. Generally, machine learning is used by various industries to enhance their performances. Moreover, machine learning algorithms are used to solve some hard problems of systems that may contain very critical information. This makes machine learning algorithms a target of adversaries, which is an important problem for systems that use such algorithms. Therefore, it is significant to determine the performance and the robustness of a machine learning algorithm against attacks. In this paper, we analyze empirically the robustness and performances of six machine learning algorithms against two types of adversarial attacks by using four different datasets and three metrics. In our experiments, we analyze the robustness of Support Vector Machine, Stochastic Gradient Descent, Logistic Regression, Random Forest, Gaussian Naive Bayes, and K-Nearest Neighbor algorithms to create learning models. We observe their performances in spam, botnet, malware, and cancer detection datasets when we launch adversarial attacks against these environments. We use data poisoning for manipulating training data during adversarial attacks, which are random label flipping and distance-based label flipping attacks. We analyze the performance of each algorithm for a specific dataset by modifying the amount of poisoned data and analyzing behaviors of accuracy rate, f1-score, and AUC score. Analyses results show that machine learning algorithms have various performance results and robustness under different adversarial attacks. Moreover, machine learning algorithms are affected differently in each stage of an adversarial attacks. Furthermore, the behavior of a machine learning algorithm highly depends on the type of the dataset. On the other hand, some machine learning algorithms have better robustness and performance results against adversarial attacks for almost all datasets.}
}

@misc{critical-systems,
   author = "Wikipedia",
   title = "{Critical System} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2024",
   howpublished = {\url{https://en.wikipedia.org/wiki/Critical_system}},
   note = "[Online; accessed 11-January-2024]"
 }


@article{McCall,
author = {Cavano, Joseph P. and McCall, James A.},
title = {A Framework for the Measurement of Software Quality},
year = {1978},
issue_date = {November 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/953579.811113},
doi = {10.1145/953579.811113},
abstract = {Research in software metrics incorporated in a framework established for software quality measurement can potentially provide significant benefits to software quality assurance programs. The research described has been conducted by General Electric Company for the Air Force Systems Command Rome Air Development Center. The problems encountered defining software quality and the approach taken to establish a framework for the measurement of software quality are described in this paper.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {133–139},
numpages = {7}
}

@book{ISO9126,
  added-at = {2014-07-08T11:07:29.000+0200},
  author = {{ISO/IEC 9126}},
  biburl = {https://www.bibsonomy.org/bibtex/2a7b775cd08a9182a7b88d5ca68657a4a/andiv},
  interhash = {ef6f58a2241715facdfa7424be170ea3},
  intrahash = {a7b775cd08a9182a7b88d5ca68657a4a},
  keywords = {ISO functional non},
  publisher = {ISO/IEC},
  timestamp = {2014-07-08T11:07:50.000+0200},
  title = {ISO/IEC 9126. Software engineering -- Product quality},
  year = 2001
}

@inproceedings{Boehm,
author = {Boehm, B. W. and Brown, J. R. and Lipow, M.},
title = {Quantitative Evaluation of Software Quality},
year = {1976},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {The study reported in this paper establishes a conceptual framework and some key initial results in the analysis of the characteristics of software quality. Its main results and conclusions are:• Explicit attention to characteristics of software quality can lead to significant savings in software life-cycle costs.• The current software state-of-the-art imposes specific limitations on our ability to automatically and quantitatively evaluate the quality of software.• A definitive hierarchy of well-defined, well-differentiated characteristics of software quality is developed. Its higher-level structure reflects the actual uses to which software quality evaluation would be put; its lower-level characteristics are closely correlated with actual software metric evaluations which can be performed.• A large number of software quality-evaluation metrics have been defined, classified, and evaluated with respect to their potential benefits, quantifiability, and ease of automation.•Particular software life-cycle activities have been identified which have significant leverage on software quality.Most importantly, we believe that the study reported in this paper provides for the first time a clear, well-defined framework for assessing the often slippery issues associated with software quality, via the consistent and mutually supportive sets of definitions, distinctions, guidelines, and experiences cited. This framework is certainly not complete, but it has been brought to a point sufficient to serve as a viable basis for future refinements and extensions.},
booktitle = {Proceedings of the 2nd International Conference on Software Engineering},
pages = {592–605},
numpages = {14},
keywords = {Software quality, Software reliability, Software engineering, Management by objectives, Software standards, Quality assurance, Testing, Software measurement and evaluation, Quality characteristics, Quality metrics},
location = {San Francisco, California, USA},
series = {ICSE '76}
}


@book{Grady,
author = {Grady, Robert B.},
title = {Practical Software Metrics for Project Management and Process Improvement},
year = {1992},
isbn = {0137203845},
publisher = {Prentice-Hall, Inc.},
address = {USA}
}

@article{Dromey,
author = {Dromey, R. Geoff},
title = {A Model for Software Product Quality},
year = {1995},
issue_date = {February 1995},
publisher = {IEEE Press},
volume = {21},
number = {2},
issn = {0098-5589},
url = {https://doi.org/10.1109/32.345830},
doi = {10.1109/32.345830},
abstract = {A model for software product quality is defined. It has been formulated by associating a set of quality-carrying properties with each of the structural forms that are used to define the statements and statement components of a programming language. These quality-carrying properties are in turn linked to the high-level quality attributes of the International Standard for Software Product Evaluation ISO-9126. The model supports building quality into software, definition of language-specific coding standards, systematically classifying quality defects, and the development of automated code auditors for detecting defects in software.},
journal = {IEEE Trans. Softw. Eng.},
month = {feb},
pages = {146–162},
numpages = {17},
keywords = {software characteristics, quality-carrying properties., quality attributes, Software quality, maintainability, product evaluation, quality model, ISO-9126, quality defect classification, code auditing}
}

@article{Miguel,
author = {Miguel, Jose and Mauricio, David and Rodriguez, Glen},
year = {2014},
month = {11},
pages = {31-54},
title = {A Review of Software Quality Models for the Evaluation of Software Products},
volume = {5},
journal = {International journal of Software Engineering \& Applications},
doi = {10.5121/ijsea.2014.5603}
}

@article{models-measurement-QA,
author = {Mohanty, Siba N.},
title = {Models and Measurements for Quality Assessment of Software},
year = {1979},
issue_date = {Sept. 1979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/356778.356783},
doi = {10.1145/356778.356783},
journal = {ACM Comput. Surv.},
month = {sep},
pages = {251–275},
numpages = {25}
}

@article{SQAM-systematic-mapping,
  title={Software quality assessment model: a systematic mapping study},
  author={Meng Yan and Xin Xia and Xiaohong Zhang and Ling Xu and Dan Yang and Shanping Li},
  journal={Science China Information Sciences},
  year={2019},
  volume={62},
  url={https://api.semanticscholar.org/CorpusID:53378706}
}

@article{systematic-open-source-SQAM,
author = {Misra, Sanjay and Adewumi, Adewole and Omoregbe, Nicholas and Crawford, Broderick},
year = {2016},
month = {11},
pages = {1936},
title = {A systematic literature review of open source software quality assessment models},
volume = {2016},
journal = {SpringerPlus},
doi = {10.1186/s40064-016-3612-4}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{systematic_approach_benchmark,
title = {Toward developing a systematic approach to generate benchmark datasets for intrusion detection},
journal = {Computers \& Security},
volume = {31},
number = {3},
pages = {357-374},
year = {2012},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2011.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167404811001672},
author = {Ali Shiravi and Hadi Shiravi and Mahbod Tavallaee and Ali A. Ghorbani},
keywords = {Intrusion detection, Dataset generation, Network traffic profile},
abstract = {In network intrusion detection, anomaly-based approaches in particular suffer from accurate evaluation, comparison, and deployment which originates from the scarcity of adequate datasets. Many such datasets are internal and cannot be shared due to privacy issues, others are heavily anonymized and do not reflect current trends, or they lack certain statistical characteristics. These deficiencies are primarily the reasons why a perfect dataset is yet to exist. Thus, researchers must resort to datasets that are often suboptimal. As network behaviors and patterns change and intrusions evolve, it has very much become necessary to move away from static and one-time datasets toward more dynamically generated datasets which not only reflect the traffic compositions and intrusions of that time, but are also modifiable, extensible, and reproducible. In this paper, a systematic approach to generate the required datasets is introduced to address this need. The underlying notion is based on the concept of profiles which contain detailed descriptions of intrusions and abstract distribution models for applications, protocols, or lower level network entities. Real traces are analyzed to create profiles for agents that generate real traffic for HTTP, SMTP, SSH, IMAP, POP3, and FTP. In this regard, a set of guidelines is established to outline valid datasets, which set the basis for generating profiles. These guidelines are vital for the effectiveness of the dataset in terms of realism, evaluation capabilities, total capture, completeness, and malicious activity. The profiles are then employed in an experiment to generate the desirable dataset in a testbed environment. Various multi-stage attacks scenarios were subsequently carried out to supply the anomalous portion of the dataset. The intent for this dataset is to assist various researchers in acquiring datasets of this kind for testing, evaluation, and comparison purposes, through sharing the generated datasets and profiles.}
}

@misc{jie2022alleviating,
      title={Alleviating Representational Shift for Continual Fine-tuning}, 
      author={Shibo Jie and Zhi-Hong Deng and Ziheng Li},
      year={2022},
      eprint={2204.10535},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{how_to_evaluate_llms,
   author = "Microsoft",
   title = "How to Evaluate LLMs: A Complete Metric Framework",
   year = "2023",
   howpublished = {\url{https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/how-to-evaluate-llms-a-complete-metric-framework/}},
   note = "[Online; accessed 17-May-2024]"
 }

@misc{owasp_top_10,
   author = "The OWASP Foundation",
   title = "OWASP Top 10 for Large Language Model Applications",
   year = "2024",
   howpublished = {\url{https://owasp.org/www-project-top-10-for-large-language-model-applications/}},
   note = "[Online; accessed 17-May-2024]"
 }

@conference{Kluyver:2016aa,
	Author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing},
	Booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
	Editor = {F. Loizides and B. Schmidt},
	Organization = {IOS Press},
	Pages = {87 - 90},
	Title = {Jupyter Notebooks -- a publishing format for reproducible computational workflows},
	Year = {2016}}