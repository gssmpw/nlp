\section{Partial Counterfactual Inference via Canonical SCMs}
\citet{pmlr-v162-zhang22ab} introduced a family of canonical SCMs that can model all possible counterfactual distributions for any causal graph. \jl{Their partial counterfactual inference approach optimises over these canonical SCMs to identify counterfactual probability bounds.} In this section, we explain how their approach can be applied to MDPs, and how additional assumptions can be incorporated 
%
to ensure counterfactuals are realistic, addressing issues in existing work.

%
\subsection{Optimisation Procedure}
MDPs can be represented as SCMs with the following structural equations:
{%
\begin{equation}\label{eq:mdp_scm}
        S_{t+1} = f(S_{t},A_{t}, U_{t}); \ \ A_t = \pi(S_t); 
        \ \ S_0 = f_0(U_0)
\end{equation}
}
\begin{wrapfigure}{r}{0.35\columnwidth}
    \centering
    \vspace{-1.1\intextsep}
            \resizebox{0.2\columnwidth}{!}{%
            \begin{circuitikz}
    \tikzstyle{every node}=[font=\Huge]
    
    %
    \node[circle, draw, minimum size=2cm] (S_tp1) at (0, 0) {\Huge $S_{t+1}$};
    \node[circle, draw, minimum size=2cm] (S_t) at (-3, 1.5) {\Huge $S_t$};
    \node[circle, draw, minimum size=2cm] (A_t) at (-3, -1.5) {\Huge $A_t$};
    \node[circle, draw, dashed, minimum size=2cm] (U_t) at (0, 3) {\Huge $U_t$};

    %
    \draw[->, thick, >=Stealth] (S_t) -- (S_tp1);
    \draw[->, thick, >=Stealth] (S_t) -- (A_t);
    \draw[->, thick, >=Stealth] (A_t) -- (S_tp1);
    \draw[->, thick, >=Stealth] (U_t) -- (S_tp1);
\end{circuitikz}
        }
        \caption{MDP causal graph}
    \label{fig:MDP_DAG}
\end{wrapfigure}
\jl{We can convert this SCM to its equivalent canonical SCM to perform partial counterfactual inference.} To this end, we need to find the c-component of the exogenous variable $U_t$ in the MDP causal graph (see Figure \ref{fig:MDP_DAG}), which is defined as follows:
\begin{definition}[c-component \citep{tian2002general}]
    Given a causal graph $\mathcal{G}$, a subset of its endogenous variables $\mathbf{C} \subseteq \mathbf{V}$ is a \textbf{c-component} if any two variables $V_i, V_j \in \mathbf{C}$ are connected by a sequence of bi-directed edges $V_i \leftarrow U_k$ and $V_j \leftarrow U_k$, where each $U_k$ is an exogenous parent shared by $V_i$ and $V_j$.
\end{definition}
\begin{definition}[Canonical SCM \citep{pmlr-v162-zhang22ab}]
    A \textbf{canonical SCM} is a tuple $\mathcal{C} = \langle \boldsymbol{V}, \boldsymbol{U}, \mathcal{F}, P \rangle$, where:
    \begin{enumerate}
        \item For every endogenous variable $V \in \boldsymbol{V}$, its values $v$ are determined by a structural equation $v \leftarrow f_V(pa_V, u_V)$ where, for any $pa_V$ and $u_V$, $f_V(pa_V, u_V)$ is contained within a finite domain $\Omega_V$.
        \item For every exogenous $U \in \boldsymbol{U}$, its values $u$ are drawn from a finite domain $\Omega_U$, where the  cardinality of $\Omega_U$ is equal to the total number of functions that map all possible inputs $pa_V \in \Omega_{\mathit{PA}_V}$ to values $v \in V$ for every endogenous $V$ in the c-component covering $U$\footnotemark, i.e., $
        |\Omega_U| = \prod_{V \in \mathbf{C}(U)}|\Omega_{{PA}_V} \mapsto \Omega_V|
        $
    \end{enumerate}
\end{definition}
\footnotetext{$\Omega_{\mathit{PA}_V} \mapsto \Omega_V$ denotes the set of all possible mappings from values in $\Omega_{{\mathit{PA}_V}}$ to values in $\Omega_V$}
The c-component covering the single exogenous variable $U_t$ in an MDP is $\mathbf{C}(U_t) = \{S_{t+1}\}$. Therefore, for any MDP, \( |\Omega_U| \) is equal to the total number of functions that map all possible values of \( S_t \) and \( A_t \) (i.e., all possible combinations of states \( s \in \mathcal{S} \) and actions \( a \in \mathcal{A} \)) to all possible values of \( S_{t+1} \) (\( \mathcal{S} \)). Thus, the cardinality of \( U_t \) in a canonical MDP SCM is $|\Omega_{U_t}| = |\mathcal{S}|^{|\mathcal{S}| \times |\mathcal{A}|}
$. Each value $u_t \in U_t$ indexes a unique structural equation, which deterministically maps all possible state-action pairs $(s, a)$ to a next state $s'$.

%
%
Given the canonical SCM representation of an MDP and an observed transition $s_t, a_t \rightarrow s_{t+1}$, we can define an optimisation procedure to find the minimum and maximum counterfactual probabilities for every transition \citep{pmlr-v162-zhang22ab}. The first step is to define a mapping between exogenous values $u_t \in U_t$ and the structural equation they index. We define an indicator variable $\mu \in \{0,1\}^{\mathcal{S} \times \mathcal{A} \times |\Omega_{U_t}| \times \mathcal{S}}$ such that, for any $s_t, a_t, u_t$ and $s_{t+1}$:
\[
\mu_{s_t, a_t, u_t, s_{t+1}} = 
\begin{cases}
    1 & \text{if $f(s_t, a_t, u_t) = s_{t+1}$}\\
    0 & \text{otherwise}\\
\end{cases}
\]
Next, we define a vector $\theta \in \mathbb{R}^{|\Omega_{U_t}|}$, where each $\theta_{u_t}$ represents the probability $P(U_t = u_t)$. Each instantiation of $\theta$ defines a unique SCM. Given an observed transition $s_t, a_t \rightarrow s_{t+1}$, we can write the counterfactual probability $\tilde{P}_t(\tilde{s}' \mid \tilde{s}, \tilde{a})$ of any transition $\tilde{s}, \tilde{a} \rightarrow \tilde{s}'$ in terms of $\mu$ and $\theta$:
\[
    \tilde{P}_t(\tilde{s}' \mid \tilde{s}, \tilde{a}) = \frac{\sum_{u_t = 1}^{|U_t|} \mu_{\tilde{s}, \tilde{a}, u_t, \tilde{s}'} \cdot \mu_{s_t, a_t, u_t, s_{t+1}} \cdot \theta_{u_t}}{P(s_{t+1} \mid s_t, a_t)}
\]
To find the counterfactual probability bounds, we \jl{search over all values of $\theta$ consistent with the interventional data (i.e., the MDP's transition probabilities):}
%
%
{
%
\begin{equation}
\label{eq: optimisation}
\begin{aligned}
{\min / \max}_{\theta} \sum_{u_t = 1}^{|U_t|} \mu_{\tilde{s}, \tilde{a}, u_t, \tilde{s}'} \cdot \mu_{s_t, a_t, u_t, s_{t+1}} \cdot \theta_{u_t}\\
\textrm{s.t.} \sum_{u_t = 1}^{|U_t|} \mu_{s, a, u_t, s'} \cdot \theta_{u_t} = P(s' \mid s, a), \forall s, a, s'\\
0 \leq \theta_{u_t} \leq 1, \forall u_t, \qquad
\sum_{u_t = 1}^{|U_t|} \theta_{u_t} = 1\\
\end{aligned}
\end{equation}
}
%
\subsection{Incorporating Extra Assumptions}
While this optimisation correctly considers all SCMs consistent with the interventional and observational data, this can result in wide or trivial $[0,1]$ bounds. We now explain how assumptions can be added to the optimisation problem to tighten the counterfactual probability bounds, and produce more realistic counterfactual probabilities. The first assumption is \textit{counterfactual stability} \citep{oberst2019counterfactual}, which, in the context of an MDP, is defined as:
%
%
%

%
%
\begin{definition}[Counterfactual stability]\label{def: counterfactual stability} An MDP SCM \eqref{eq:mdp_scm} satisfies \textit{counterfactual stability} if, given we observed the next state $S_{t+1}=s_{t+1}$ after observing the state-action pair $(s_t, a_t)$, the counterfactual outcome under a different state-action pair $(\tilde{s}, \tilde{a})$ will not change to some $S_{t+1} = \tilde{s}'\neq s_{t+1}$ unless $\dfrac{P(s_{t+1} \mid \tilde{s}, \tilde{a})}{P(s_{t+1} \mid s_t, a_t)} < \dfrac{P(\tilde{s}' \mid \tilde{s}, \tilde{a})}{P(\tilde{s}' \mid s_t, a_t)}$.
\end{definition}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
%
\renewcommand{\arraystretch}{0.9} %

\begin{figure}[t]
    \centering
        \resizebox{0.6\columnwidth}{!}{%
            \begin{circuitikz}
            \tikzstyle{every node}=[font=\Huge]
            \draw  (10.75,18.5) circle (1.25cm) node {\Huge $s_1$} ;
            \draw [ fill={rgb,255:red,177; green,170; blue,170} ] (4.25,18.5) circle (1.25cm) node {\Huge $s_0$} ;
            \draw  (16.75,18.5) circle (1.25cm) node {\Huge $s_2$} ;
            \draw [ fill={rgb,255:red,177; green,170; blue,170} ] (10.75,14.75) circle (1.25cm) node {\Huge $s_1$} ;
            \draw  (4.25,14.75) circle (1.25cm) node {\Huge $s_0$} ;
            \draw  (16.75,14.75) circle (1.25cm) node {\Huge $s_2$} ;
            \draw [->, >=Stealth] (4.25,17.25) -- (4.25,16)node[pos=0.5, fill=white]{0.3};
            \draw [->, >=Stealth] (16.75,17.25) -- (16.75,16)node[pos=0.5, fill=white]{1.0};
            \draw [->, >=Stealth] (10,17.5) -- (5.5,15);
            \draw [->, >=Stealth] (11.5,17.5) -- (15.5,15);
            \draw [->, >=Stealth] (5.25,17.75) -- (15.5,14.75);
            \draw [->, >=Stealth] (5,17.5) -- (9.5,14.75);
            \node [font=\Huge] at (5.75,16.5) {0.4};
            \node [font=\Huge] at (9,17.5) {0.4};
            \node [font=\Huge] at (6.75,17.75) {0.3};
            \node [font=\Huge] at (12.5,17.5) {0.6};
            \end{circuitikz}
        }%
 \caption{Example MDP where Gumbel-Max produces unintuitive CF probabilities. The observed path is $s_0 \rightarrow s_1$.}
        \label{fig:gumbel-max-scm-unintuitive-probs}
\end{figure}

\begin{table}[b]
\centering
\caption{\jl{Counterfactual transition probabilities produced by the optimisation in \eqref{eq: optimisation} vs. the Gumbel-max SCM \eqref{eq:cf_mdp_probs} vs. the optimisation in \eqref{eq: optimisation} with the extra assumptions \eqref{eq: additional assumptions}.}}
%
\resizebox{0.95\columnwidth}{!}{ %
\begin{tabular}{|c|c|c|c|P{0.7cm}|P{0.7cm}|c|P{0.7cm}|P{0.7cm}|}
\hline
\multirow{2}{*}{\text{$s$}} & \multirow{2}{*}{\text{$a$}} & \multirow{2}{*}{\text{$s'$}} & \multirow{2}{*}{\text{$P(s' \mid s, a)$}} & \multicolumn{2}{c|}{\makecell{\text{Optimisation} \\ \text{\eqref{eq: optimisation}}}} & \multirow{2}{*}{\makecell{\text{Gumbel-} \\ \text{Max \eqref{eq:cf_mdp_probs}}}} & \multicolumn{2}{c|}{\makecell{\text{Optimisation} \\ \text{\eqref{eq: optimisation} + \eqref{eq: additional assumptions}}}} \\ \cline{5-6} \cline{8-9} 
                                &                                 &                 &                     & \text{LB}                             & \text{UB}                            &                                      & \text{LB}                              & \text{UB}                             \\ \Xhline{1pt}
0                               & 0                               & 0      & 0.3                             & 0.0                                     & 0.0                                    & 0.0                                  & 0.0                                      & 0.0                                     \\ \hline
0                               & 0                               & 1      & 0.4                             & 1.0                                     & 1.0                                    & 1.0                                  & 1.0                                      & 1.0                                     \\ \hline
0                               & 0                               & 2      & 0.3                              & 0.0                                     & 0.0                                    & 0.0                                  & 0.0                                      & 0.0                                     \\ \hline
1                               & 0                               & 0           & 0.4                        & 0.0                                     & 1.0                                    & 0.35                                 & 0.4                                      & 0.4                                     \\ \hline
1                               & 0                               & 1      & 0.0                             & 0.0                                     & 0.0                                    & 0.0                                  & 0.0                                      & 0.0                                     \\ \hline
\textbf{1}                      & \textbf{0}                      & \textbf{2}          & \textbf{0.6}                & \textbf{0.0}                            & \textbf{1.0}                           & \textbf{0.65}                        & \textbf{0.6}                             & \textbf{0.6}                            \\ \hline
2                               & 0                               & 0       & 0.0                            & 0.0                                     & 0.0                                    & 0.0                                  & 0.0                                      & 0.0                                     \\ \hline
2                               & 0                               & 1       & 0.0                            & 0.0                                     & 0.0                                    & 0.0                                  & 0.0                                      & 0.0                                     \\ \hline
2                               & 0                               & 2      & 1.0                             & 1.0                                     & 1.0                                    & 1.0                                  & 1.0                                      & 1.0                                     \\ \hline
\end{tabular}}
\label{tab:gumbel-max-scm}
\end{table}
However, even with counterfactual stability, the Gumbel-max SCM yields unrealistic counterfactual probabilities. In the example in Figure \ref{fig:gumbel-max-scm-unintuitive-probs}, the counterfactual probability of transition $s=1, a=0 \rightarrow s'=2$ (highlighted in Table \ref{tab:gumbel-max-scm}) is greater than its nominal probability, even though state $s'=2$ was reachable from the observed state $s_t=0$, but not observed (see Appendix \ref{app: unintuitive probs explanation} for further discussion). Arguably, a state that was possible but did not unfold in the factual world should not be more likely in the counterfactual world. To formalise this, we introduce \textit{monotonicity}\footnote{This is different from other definitions of monotonicity, which assume an ordering on interventions and outcomes, e.g., \citep{vlontzos2023estimating}.}:
\begin{definition}[Monotonicity]
    An MDP SCM \eqref{eq:mdp_scm} satisfies \textit{monotonicity} if, upon observing the transition $(s_t, a_t, s_{t+1})$, then, $\forall s \in \mathcal{S}, a \in \mathcal{A}$: 1) the counterfactual probability $\tilde{P}_t(s_{t+1} \mid s, a)$ cannot be smaller than the nominal probability $P(s_{t+1} \mid s, a)$ (i.e., observing an outcome cannot make it less likely in the counterfactual world), and 2) $\forall \tilde{s}'\neq s_{t+1}$ with $P(\tilde{s}' \mid s_t, a_t)>0$, the counterfactual probability $\tilde{P}_t(\tilde{s}' \mid s, a)$ cannot be bigger than the nominal probability $P(\tilde{s}' \mid s, a)$ \jl{(i.e., not observing a possible outcome cannot make it more likely in the counterfactual world)}. 
\end{definition}
The counterfactual stability (CS) and monotonicity assumptions (Mon1 + Mon2) can be added as linear constraints to the optimisation problem in \eqref{eq: optimisation} as follows:
{
%
\begin{multline}\label{eq: additional assumptions}
	\textbf{(CS) } \tilde{P}_t(\tilde{s}' \mid \tilde{s}, \tilde{a}) = 0 \\ 
    \text{ if } \frac{P(s_{t+1} \mid \tilde{s}, \tilde{a})}{P(s_{t+1} \mid s_t, a_t)}>\frac{P(\tilde{s}' \mid \tilde{s}, \tilde{a})}{P(\tilde{s}' \mid s_t, a_t)} \text{ and } P(\tilde{s}' \mid s_t, a_t) > 0 \\[2pt] 
    \shoveleft{\textbf{(Mon1) } \tilde{P}_t(s_{t+1} \mid \tilde{s}, \tilde{a}) \geq {P}(s_{t+1} \mid \tilde{s}, \tilde{a})} \\ 
    \shoveright{\text{ if } P(s_{t+1} \mid \tilde{s}, \tilde{a})>0}\\[2pt] 
    \shoveleft{\textbf{(Mon2) } \tilde{P}_t(\tilde{s}' \mid \tilde{s}, \tilde{a}) \leq {P}(\tilde{s}' \mid \tilde{s}, \tilde{a})}\\ 
    \text{ if } P(\tilde{s}' \mid \tilde{s}, \tilde{a})>0  \text{ and } P(\tilde{s}' \mid s_t, a_t)>0, \forall \tilde{s}'\neq s_{t+1}
\end{multline}
%
%
%
%
%
%
%
%
}
%
%
%
%
%
%
%
%
%
%
%
%
%


