\section{More Experiments}

In this section, we present experiments comparing Langevin-MWU  with classic MWU. We use Langevin-MWU and MWU to optimize several non-convex functions with many local minima. The experimental results show that MWU converges to bad local minima, but Langevin-MWU escapes such bad local minima and finds minima with smaller function values.

\paragraph{Test functions.}
We construct 6 non-convex functions to verify the efficiency of LMWU in finding global minima. The functions are given as follows,
\begin{itemize}
\item[(1)]In Figure \eqref{f_1}, 
\begin{align}
    f_1(x,y,z) &= -\ln (e^{ -10 (x - 0.3)^2 - 20(y - 0.5)^2 - 30(z - 0.2)^2 }\notag \\
    &+e^{ -30 (x - 0.4)^2 - 20(y - 0.2)^2 - 36(z - 0.4)^2 } )\notag\\ &+ y + 10.
\end{align}
\item[(2)]In Figure \eqref{f_2} 
\begin{align}
f_2(x,y,z) &= -\ln (e^{ -15 (x - 0.4)^2 - 60(y - 0.4)^2 - 10(z - 0.2)^2 }\notag\\
& +e^{ -3 (x - 0.4)^2 - 2(y - 0.2)^2 - 6(z - 0.4)^2 } ) + y.
\end{align}

\item[(3)]In Figure \eqref{f_3} 
\begin{align}
f_3(x,y,z) &= (x - 0.3)^2 (x - 0.9)^2 \notag\\ 
&+ (y - 0.2)^2(y - 0.7)^2\notag\\ 
&+ (z - 0.6)^2(z - 0.1)^2 + (x - 0.3)(y - 0.5)   .
\end{align}

\item[(4)] in Figure \eqref{f_4} 
\begin{align}
f_4(x,y,z) &= -(x - 0.6)^2 (x - 0.2)^2 \notag\\
&+ (y - 0.3)(y - 0.4)^3\notag\\ 
&+ (z - 0.2)^3(z - 0.8) - xy - 0.4z  .
\end{align}

\item[(5)]In (a) of Figure \eqref{f_56} 
\begin{align}
f_5(x,y,z,w,v) &= (x - 0.6)^2 (x - 0.2)^2 - xy  \notag\\
&+ (y - 0.3)^2(y - 0.4)^2 + (z - 0.2)^4
\notag\\
&- 0.5zw + (w - 0.5)^4 + (v - 0.3)^4   .
\end{align}

\item[(6)]In (b) of Figure \eqref{f_56} 
\begin{align}
f_6(x,y,z,w,v,h) &= (x - 0.6)^2 (x - 0.8)  \notag\\
&+ (y - 0.9)(y - 0.4)^2 \notag\\
&+ (z - 0.2)^2 + (v - 0.6)^2 
\notag\\
&+ (w - 0.5)^2 - 0.5vw + (h - 0.5)^2  .
\end{align}
\end{itemize}

For test functions (1)-(4), we present both trajectories of algorithms on the contour maps of text functions and the curve of convergence in the function values. With simplex constrains $x + y + z = 1, x,y,z \ge 0$, the values of a three variables function $f(x,y,z) $ constrained on a simplex are determined variables $x$ and $y$, thus we can draw trajectory and level curves of $f(x,y,z)$ on a $(x,y)$-plane. Note that since $x + y \le 1$, only the lower half part of the $(x,y)$-plane is meaningful, and algorithms' trajectories will only appear on lower half part of $(x,y)$-plane. For test functions (5) and (6) with more than three variables, we only show their curves of convergence in function values.
\paragraph{Parameter setting.} 
Since the behaviors of Langevin-MWU are controlled by the parameter $\beta$, in the experiments we choose different $\beta$ to show the power of using larger $\beta$'s, the choices of $\beta$ are denoted on the convergence curves graph. In experiments we set the parameters as follows:
\begin{itemize}
\item[(1)] Figure \eqref{f_1}: Initial point $(0.3,0.6,0.1)$, MWU's step size $10^{-3}$, LMWU's step size $10^{-4}$ , $\beta  =10,50,100 $.
\item[(2)] Figure \eqref{f_2}: Initial point $(0.4,0.1,0.5)$, MWU's step size $10^{-3}$, LMWU's step size $5 \times 10^{-5}$ , $\beta  =10,50,100 $.
\item[(3)] Figure \eqref{f_3}: Initial point $(0.2,0.75,0.05)$, MWU's step size $10^{-2}$, LMWU's step size $10^{-3}$ , $\beta  =10,2000,5000 $.
\item[(4)] Figure \eqref{f_4}: Initial point $(0.5,0.4,0.1)$, MWU's step size $10^{-2}$, LMWU's step size $2 \times 10^{-4}$ , $\beta  =1000,2000,8000 $.
\item[(5)] (a) of Figure \eqref{f_56} :  Initial point $(0.1,0.05,0.4,0.4,0.05)$, MWU's step size $5 \times 10^{-2}$, LMWU's step size $5 \times 10^{-3}$ , $\beta  =800,2000,3000 $.
\item[(6)] (b) of Figure \eqref{f_56} :  Initial point $(0.4,0.1,0.1,0.2,0.1,0.1)$, MWU's step size $10^{-4}$, LMWU's step size $10^{-4}$ , $\beta=300,3000,8000 $.
\end{itemize}
In each experiments MWU and LMWU starting from the same initial points and run the same number of steps.



\paragraph{Analysis of experimental results.} The curves of convergence in function values show that Langevin-MWU outperforms MWU when $\beta$ is small enough. This can be seen clearly from the trajectories of the algorithms on contour map : MWU is  attracted by a local minimum near initial points, but although starting from the same initial point, LMWU will escape the bad local minimum near initial points and go to the global minimum. The choice of $\beta$ have a great influence on behaviors of LMWU : as the experimental results show that larger $\beta$ will make LMWU find a better convergence point, but with a slower convergence rate, this is compatible with our theoretical analysis. Moreover, as shown in Figure \eqref{f_3} and (a) of Figure \eqref{f_56}, an inappropriate choice of $\beta$ will make LMWU underperform MWU. In fact, for different test functions, the range of suitable $\beta$ is very different, thus choosing an optimal $\beta$ is an important question for future research.

%\paragraph{Application beyond multi-agent learning.}We complete this section by applying Langevin MWU algorithm in a special polynomial optimization in portfolio selection problem which is a challenging task in financial literature. The modern portfolio theory (MPT), or mean-variance (MV) analysis by \cite{markowitz1952} established a mathematical framework for assembling a portfolio of assets such that the expected return is maximized for a given level of risk. A polynomial portfolio optimization problem can be defined as $\hat{\vec{x}}=\argmin_{\vec{x}\in \Delta}\mathbb{E}f(\vec{x},\vec{r})$, where $f(\vec{x},\vec{r})$ is polynomial loss function, $\vec{r}=(r_1,...,r_n)^{\top}$ denotes the vector of $n$ individual returns in the portfolio, and $\vec{x}$ stands for the weights we assign to each component in the portfolio. Here we assume that $\vec{x}$ is restricted to the simplex. Empirically the expectation is represented as $\mathbb{E}f(\vec{x},\vec{r})=\frac{1}{N}\sum_{i=1}^Nf(\vec{x},\vec{r}_i)$. So see the efficiency of LMWU, it suffices to verify the validity of LMWU on each $f(\vec{x},\vec{r}_i)$ since it is trivial to extend the code/program to linear combination of $f(\vec{x},\vec{r}_i)$ for $i\in[N]$. We leave more details in appendix due to space constraint. 

\begin{comment}
In line with \cite{yang2022}, we consider the following specification of the loss function $f(\vec{x},\vec{r})$ such that
$
f(\vec{x},\vec{r})=-\lambda_1m_1(\vec{x},\vec{r})+\lambda_2m_2(\vec{x},\vec{r})+...+(-1)^d\lambda_dm_d(\vec{x},\vec{r})$, where $m_1(\vec{x},\vec{r})=\vec{x}^{\top}\vec{r}$ and $m_i(\vec{x},\vec{r})=\left(m_1(\vec{x},\vec{r})-\mathbb{E}\left(m_1(\vec{x},\vec{r})\right)\right)^i$, for $i=2,...,d$.
\end{comment}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Tar_1.png}
    \caption{Trajectories for $f_1$}
    \label{f1_trajectories}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Value_1.png}
    \caption{Convergence curves for $f_1$}
    \label{f1_convergence}
\end{subfigure}
\caption{Test function $f_1$}
\label{f_1}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Tar_2.png}
    \caption{Trajectories for $f_2$}
    \label{f2_trajectories}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Value_2.png}
    \caption{Convergence curves for $f_2$}
    \label{f2_convergence}
\end{subfigure}
\caption{Test function $f_2$}
\label{f_2}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Tar_3.png}
    \caption{Trajectories for $f_3$}
    \label{f3_trajectories}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Value_3.png}
    \caption{Convergence curves for $f_3$}
    \label{f3_convergence}
\end{subfigure}
\caption{Test function $f_3$}
\label{f_3}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Tar_4.png}
    \caption{Trajectories for $f_4$}
    \label{f4_trajectories}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Value_4.png}
    \caption{Convergence curves for $f_4$}
    \label{f4_convergence}
\end{subfigure}
\caption{Test function $f_4$}
\label{f_4}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Value_5.png}
    \caption{Convergence curves of $f_5$}
    \label{f5_convergence}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Value_6.png}
    \caption{Convergence curves of $f_6$}
    \label{f6_convergence}
\end{subfigure}
\caption{Test functions $f_5$ and $f_6$}
\label{f_56}
\end{figure}


%\begin{figure}[htp]
%\centering
%\subfigure[Tar]{
%\includegraphics[clip,width=0.5\columnwidth]{Value_6.png}
%}
%\caption{Test function $f_6(x,y,z,w,v,h)$}
%\label{f_6}
%\end{figure}



\begin{comment}
\subsection{Single-agent potential game}

\[
-\ln\left(e^{-10(x-0.3)^2-20(y-0.5)^2-30(z-0.2)^2}+e^{-30(x-0.4)^2-20(y-0.2)^2-36(z-0.4)^2}\right)+y
\]

\subsection{Multi-agent potential game}
\end{comment}












