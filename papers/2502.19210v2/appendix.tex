\appendix
\onecolumn

\section{Multi-Agent L-MWU}
It is immediate to have the multi-agent version of Langevin MWU based on the algorithm given in main article.
\begin{algorithm}
\caption{Langevin-MWU (multi-agent) }
\label{alg:C2}
\begin{algorithmic}
\STATE Input: error bound $\delta>0$, $\beta>0$, step size $\epsilon>0$ 
\\
Initialize: $\vec{x}_i(0)\sim\rho_i(0)$ for all $i\in[N]$
\REPEAT
\STATE For $i=1,...,N$, 
\\
Compute 
$S_{\vec{x}_i}=\sum_{s=1}^{n_i}\frac{1}{x_{is}}$, 
\\
Sample $z_0^{is}\sim\mathcal{N}(0,1)$.
\\
Compute \[V_0^{is}=\frac{\epsilon}{2\beta}\left(n_i+1-(1+x_{is})S_{\vec{x}_i}\right)+\sqrt{2\epsilon\beta^{-1} x_{is}}z_0^{is}\]
\\
Set \[x_{is}\leftarrow\frac{x_{is}-\epsilon x_{is}\frac{\partial f}{\partial x_{is}}+V_0^{is}}{1-\epsilon\sum_{s=1}^{n_i}x_{is}\frac{\partial f}{\partial x_{is}}+\sum_{s=1}^{n_i}V_0^{is}}\]
\UNTIL{$k$ large enough}
\end{algorithmic}
\end{algorithm}

\section{Empirical Illustration on Polynomial Portfolio Management}
Supplementing to experiment of LMWU on polynomial portfolio management, we conduct empirical illustration of the non-convexity problem in polynomial portfolio optimization. We consider three representative stocks ($N=3$): (i) AAPL: Apple Inc.; (ii) AMT: American Tower Corp.; and (iii) COST: Costco Wholesale Corp., which are typical companies from the IT sector, the real estate sector, and the consumer discretionary sector, respectively. We collect the daily return data for the three stocks in year 2012 as an example. We consider a high order polynomial function of $d = 5$ in this illustration. We can even broaden out constraints on the weights by setting $w^L = -1$, $w^U = 2$, and $\lambda_i = 1/d$ for all $i$. 

We first consider a simple bivariate portfolio \{AAPL, AMT\}. We consider all the possible weights combination in the feasible set $\mathcal{W}$. Figure \ref{f1} plots the estimated $ -\hat{\mathbb{E}}[f(\vec{ w}, \vec{r})]$ against $w_1$.\footnote{Note that once $w_1$ is known, we immediately know the weight for the second stock due to $\sum^N_{i=1} w_i = 1$. } It is obvious that the figure consists of one global maximum along with a local maximum. The optimization problem to find the best weights is clearly non-convex. 

We include all three stocks \{AAPL, AMT, COST\} in the second portfolio. Similarly, we consider all the possible weights combination in the feasible set $\mathcal{W}$. The 3D Figure \ref{f2} plots the estimated $ -\hat{\mathbb{E}}[f(\vec{ w}, \vec{ r})]$ against $w_1$ and $w_2$. To have better visualization, we expand the interval of $w_1$ and $w_2$, although our estimation straightly follow the condition $\vec{ w}\in\mathcal{W}$. Again, the polynomial portfolio optimization problem is clearly non-convex. In fact, three maximums (one global and one local) appear in the graph. Although impossible to illustrate, we should expect more local maximums in such non-convexity as we include more stocks in the portfolio. 
Conventional convex optimization algorithm will clearly fail to deliver the global optima in the above illustration. This emphasizes the importance of our proposed algorithm that is capable of solving the non-convexity problem in high-order polynomial portfolio optimization.

\begin{figure}[H]
\centering
\includegraphics[scale = 0.6]{f1.eps}
\caption{An Illustration of Non-convexity in Polynomial Portfolio Optimization}\label{f1}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[scale = 0.6]{f2.eps}
\caption{A 3D Illustration of Non-convexity in Polynomial Portfolio Optimization}\label{f2}
\end{figure}


\input{experiment}


\section{More Background on Analysis on Manifolds}
The standard references for the necessary background are \cite{JLee,Hsu}.
\subsection{Covariant Derivative}
Given two vector fields $X$ and $Y$ on $\mathcal{M}$, the covariant derivative, $D_XY$ is a bilinear operator with the following properties:
\begin{align*}
D_{\alpha_1X_1+\alpha_2X_2}Y&=\alpha_1D_{X_1}Y+\alpha_2D_{X_2}Y,
\\
D_X(Y_1+Y_2)&=D_X(Y_1)+D_X(Y_2),
\\
D_X(\alpha Y_1)&=\alpha D_X(Y_1)+X(\alpha)Y_1.
\end{align*}
In local coordinate systems with variable $x$, the vector field $X$ can be represented as $X=\sum X_i\partial x_i$, where $\partial x_i$ are the basis vector fields, and $Y=\sum Y_i\partial x_i$, the covariant derivative is given by 
\[
D_XY=\sum_k\left(X(Y_k)+\sum_i\sum_jX_iY_j\Gamma^k_{ij}\right)\partial x_k.
\]
where the Christoffel symbols $\Gamma_{ij}^k$ will be used and computed explicitly in deriving the Langevin MWU algorithm. With the basis $\{\partial x_i\}$, the covariant derivative and Christoffel symbols are related as follows,
\[
D_{\partial x_j}\partial x_i=\sum_k\Gamma_{ij}^k\partial x_k
\]
and the $\Gamma_{ij}^k$ can be computed by
\[
\Gamma_{ij}^k=\frac{1}{2}\sum_mg^{km}(\partial_jg_{mi}+\partial_ig_{mj}-\partial_mg_{ij}).
\]

\subsection{Differential Operators on Manifold}
In local coordinate systems, denote $\abs{g}=\det{g_{ij}(x)}$, the differential operators on manifold can be written as

\[
\Div V=\frac{1}{\sqrt{\abs{g}}}\sum_i\frac{\partial}{\partial x_i}\left(\sqrt{\abs{g}}V_i\right)
\]

The Laplace-Beltrami operator is

\begin{align}
\Delta_{M}f&=\Div(\grad f)
\\
&=\frac{1}{\sqrt{\abs{g}}}\sum_i\frac{\partial}{\partial x_i}\left(\sqrt{\abs{g}}\sum_jg^{ij}\frac{\partial f}{\partial x_j}\right)
\\
&=\sum_{i}b_i\frac{\partial f}{\partial x_i}+\sum_{i,j}g^{ij}\frac{\partial^2f}{\partial x_i\partial x_j},
\end{align}
where
\[
b_i=\frac{1}{\sqrt{\abs{g}}}\sum_j\frac{\partial (\sqrt{\abs{g}}g^{ij})}{\partial x_j}=\sum_{j,k}g^{jk}\Gamma^i_{jk}
\]
\section{Derivation of Algorithm}
The standard geometry used in simplex is a special type of Riemannian geometry on the positive orthant and interior of simplex, called \emph{Shahshahani geometry} \cite{Shahshahani,HofSig}. The metric matrix $\{g_{ij}(\vec{x})\}$ on $\mathbb{R}_+^{d}=\{\vec{x}:x_i>0\text{ for all }i\in[d]\}$ is diagonal with $g_{ii}(\vec{x}=\frac{\abs{\vec{x}}}{x_i})$ where $\abs{\vec{x}}
=\sum_jx_j$. Use the Riemannian gradient \eqref{grad}, we have the explicit form of the Shahshahani gradient for $\vec{x}\in\mathbb{R}_+^d$ as follows:
\[
\grad f(\vec{x})=g^{-1}\nabla f(\vec{x})=\left(\frac{x_1}{\abs{\vec{x}}}\frac{\partial f}{\partial x_1},...,\frac{x_d}{\abs{\vec{x}}}\frac{\partial f}{\partial x_d}\right).
\]

Viewing $\Delta_+^{d-1}$ as a Riemannian submanifold of $\mathbb{R}_+^{d}$, we endow the simplex with a Riemannian metric whose matrix satisfies $g_{ii}(\vec{x})=\frac{1}{x_i}$ on diagonal and $g_{ij}=0$ on all other entries. The tangent space of $\Delta_+^{d-1}$ at $\vec{x}$ is denoted by $T_{\vec{x}}$ which consists of all the vectors $\vec{v}=(v_1,...,v_d)$ such that $\sum_jv_j=0$. Thus the tangent space $T_{\vec{x}}\Delta_+^{d-1}$ is identified with the hyperplane passing through $\vec{0}$ and parallel to $\Delta_+^{d-1}$. In the derivation of the L-MWU algorithm, the geometric property used most frequently is the othogonality in $\mathbb{R}_+^d$. Let $\langle\cdot,\cdot\rangle_{\vec{x}}$ be the Riemannian metric (a space-dependent inner product on $T_{\vec{x}}\Delta_+^{d-1}$), i holds that for all $\vec{u}\in T_{\vec{x}}\Delta_+^{d-1}$ and any $\lambda\ne 0$, we have $\langle\vec{u},\lambda\vec{x}\rangle_{\vec{x}}=0$. This means that the straight line passing through $\vec{0}$ and $\vec{x}$ is orthogonal to the tangent space of $\Delta_+^{d-1}$, with respect to the Shahshahani metric on $\mathbb{R}_+^d$. With these background in Shahshahani geometry of simplex, MWU can be viewed as the Riemannian gradient descent, especially the linear variant \eqref{MWUclassic} is the Riemannian gradient descent with retraction as the projection mapping from tangent space onto the base manifold.
\subsection{Brownian motion in Riemannian manifold}
In locally coordinate systems, Brownian motion can be written in the following way:
\[
dX_t=-\frac{1}{2}g^{ij}\Gamma^k_{ij}dt+\sqrt{g^{-1}}dB_t
\]




The standard Brownian motion in $\mathbb{R}^n$ can be generated by the diffusion equation 
\[
\frac{\partial \rho}{\partial t}=\frac{1}{2}\Delta \rho.
\]

The Brownian motion on manifold can be seen to be generated by the Laplace-Beltrami operator. Let $\sigma=\{\sigma_{ij}\}$ be the unique symmetric square root of $g^{-1}=\{g^{ij}\}$. In local coordinate, the solution of the stochastic differential equation for a process $x_t=(x_t^1,...,x_t^n)$:
\[
dx_t^i=\frac{1}{2}b_i(x_t)dt+\sum_j\sigma_{ji}(x_t)dB^j_t,
\]
that is a diffusion process generated by $\frac{1}{2}\Delta_M$, i.e. $x_t$ is a Brownian motion on $M$.
%\begin{align}
%\frac{\partial \rho}{\partial t}&=\frac{1}{2}\Delta_M\rho
%\\
%&=\frac{1}{\sqrt{\abs{g}}}\sum_{i=1}^n\frac{\partial}{\partial x_i}\left(\sqrt{\abs{g}}\sum_{i=1}^ng^{ij}\frac{\partial \rho}{\partial x_j}\right)
%\\
%&=\frac{1}{\sqrt{\abs{g}}}\sum_i^n\frac{\partial}{\partial x_i}\left(\sqrt{\abs{g}}g^{-1}\nabla\rho\right)_i
%\\
%&=\frac{1}{\sqrt{\abs{g}}}\nabla\cdot\left(\sqrt{\abs{g}}g^{-1}\nabla\rho\right)
%\end{align}

\begin{theorem}[Fokker-Planck Equation]
For any stochastic differential equation of the form
\[
dx_t=\mu(x_t,t)dt+\sqrt{A(x_t,t)}dB_t,
\]
the probability density of the SDE is given by the diffusion equation
\[
\frac{\partial\rho(x,t)}{\partial t}=-\sum_{i=1}^n\frac{\partial}{\partial x_i}\left(\mu_i(x,t)\rho(x,t)\right)+\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\frac{\partial^2}{\partial x_i\partial x_j}\left(A_{ij}(x,t)\rho(x,t)\right)
\]
\end{theorem}

\subsection{Langevin algorithm in Shahshahani manifold}
In this subsection we show that the algorithm \ref{alg:C} is discretization and approximation of Langevin based Riemannian gradient descent on Shahshahani manifold.
\begin{proposition}
Algorithm \ref{alg:C} that is given as follows,
\[
x_i\leftarrow\frac{x_i-\epsilon x_i\frac{\partial f}{\partial x_i}+V_0^i}{1-\epsilon\sum_{j=1}^nx_j\frac{\partial f}{\partial x_j}+\sum_{j=1}^nV_0^j}
\]
is Langevin gradient descent that scaled with Shahshahani geometry in simplex.
\end{proposition}
\begin{proof}
MWU with noise can be implemented by gradient descent on the simplex with Shahshahani metric with a Gaussian noise in the tangent space. Consider the open simplex $\Delta_n\subset\mathbb{R}^{n}$,

\[
\Delta_n=\left\{\vec{x}=(x_1,...,x_n)\in\mathbb{R}^n:x_i>0\ \ \text{for all}\ \ i,\sum_{i=1}^nx_i=1\right\}
\]
and its tangent spaces are identified with the hyperplane through origin,
\[
T_n=\left\{\vec{v}=(v_1,...,v_n)\in\mathbb{R}^n:\sum_{i=1}^n=0\right\}.
\]
The exponential map $\Exp:\Delta_n\times T_n\rightarrow\Delta_n$,
\[
(\vec{x},\vec{v})\mapsto\sum_{i=1}^n\frac{x_ie^{v_i}}{\sum_{j=1}^nx_je^{v_j}}\vec{e}_i.
\]
We will use the restrictions notation $\Exp_{\vec{x}}(\vec{v})$ as well. The coordinate system is given by 
\[
\varphi(\vec{x})=(x_1,...,x_{n-1}) 
\]
and the parametrization $\varphi^{-1}$ is
\[
\varphi^{-1}(x_1,...,x_{n-1})=(x_1,...,x_{n-1},1-\sum_{i=1}^{n-1}x_i).
\]
The metric matrix pulled back from $\Delta_n$ to the hypersurface $(x-1,...,x_{n-1},0)$ is given by
\[
\bar{g}_{ij}=(d\varphi^{-1})^{\top}g_{ij}d\varphi^{-1}, \ \ i,j\in [n-1].
\]
Next we derive the expression of noise based on the Brownian motion in the projected simplex $\varphi(\Delta_n)$. The Brownian motion (generated by full Laplace-Beltrami instead of $\frac{1}{2}\Delta_M$) pull back into the tangent space, according to the local expression, is
\[
dX_t=b(X_t)dt+\sqrt{2\bar{g}^{-1}}dB_t
\]
where the basis is $\vec{e}_1,...,\vec{e}_{n-1}$ and 
\[
b_i(X_t)=\sum_{j,k}\bar{g}^{jk}\Gamma^i_{jk}.
\]
Denote $\sigma_{ij}$ to be the unique matrix such that $\sigma^2=g^{-1}$, the Riemannian noise with stepsize $\epsilon$ can be written as
\[
W_0^i=\epsilon\sum_{j,k}\bar{g}^{jk}\Gamma^i_{jk}+\sqrt{2\epsilon}\sum_j\sigma_{ji}z^j_0
\]
where $z^j_0\sim\mathcal{N}(0,1)$. Then the push-forward noise of $W_0$ to $T_{\vec{x}}\Delta_n$ by $\varphi^{-1}$ is 
\[
\xi_0=d\varphi^{-1}(W_0).
\] 
One can compute the Christoffel symbols according to the rule
\[
\Gamma^i_{jk}=\frac{1}{2}\sum_m\left(\frac{\partial g_{km}}{\partial x_j}+\frac{\partial g_{mj}}{\partial x_k}-\frac{\partial g_{jk}}{\partial x_m}\right)g^{mi}.
\]
An simple approach to compute the noise is to think of the noise or Brownian motion on $\Delta_n$ as the projection of noise or Brownian motion in the ambient space $\mathbb{R}^n_+$ onto $\Delta_n$. Let $g_{ij}$ be the Shahshahani metric on $\mathbb{R}^n_+$, according to the previous derivation, denote $V_0$ as the noise, we have the following expression 
\[
V^i_0=\epsilon\sum_{j,k}g^{jk}\Gamma^i_{jk}+\sqrt{2\epsilon}\sum_j\sigma_{ji}z_0^j
\]
where $j\in[n]$ and $\Gamma^i_{jk}$ is the Christoffel symbol of $g_{ij}$. Next we compute them explicitly. Recall that $g_{ij}=\diag\{\frac{1}{x_i}\}$, and we have the following calculation:

One can compute the Christoffel symbols according to the rule
 \[
 \Gamma^i_{jk}=\frac{1}{2}\sum_m\left(\frac{\partial g_{km}}{\partial x_j}+\frac{\partial g_{mj}}{\partial x_k}-\frac{\partial g_{jk}}{\partial x_m}\right)g^{mi}
 \]
 Note that $g_{ij}=0$ for all $i\ne j$, then we have
 \begin{align}
 \Gamma^i_{jk}&=\frac{1}{2}\sum_m\left(\frac{\partial g_{km}}{\partial x_j}+\frac{\partial g_{mj}}{\partial x_k}-\frac{\partial g_{jk}}{\partial x_m}\right)g^{mi}
 \\
 &=\frac{1}{2}\left(\left(\sum_m\frac{\partial g_{km}}{\partial x_j}\right)g^{mi}+\left(\sum_m\frac{\partial g_{mi}}{\partial x_k}\right)g^{mi}-\left(\sum_m\frac{\partial g_{jk}}{\partial x_m}\right)g^{mi}\right)
 \\
 &=\frac{1}{2}\left(\frac{\partial g_{kk}}{\partial x_j}g^{kj}+\frac{\partial g_{jj}}{\partial x_k}g^{ji}-\frac{\partial g_{jk}}{\partial x_i}g^{ii}\right)
 \end{align} 
 
If $j=k\ne i$
\begin{align}
\Gamma_{jk}^i&=\Gamma_{jj}^i=\frac{1}{2}\left(\frac{\partial g_{jj}}{\partial x_j}g^{jj}+\frac{\partial g_{jj}}{\partial x_j}g^{ji}-\frac{\partial g_{jj}}{\partial x_i}g^{ii}\right)
\\
&=\frac{1}{2}\left(\frac{\partial g_{jj}}{\partial x_j}g^{jj}-\frac{\partial g_{jj}}{\partial x_i}g^{ii}\right)
\\
&=\frac{1}{2}\left(\frac{x_j-\abs{\vec{x}}}{x_j\abs{\vec{x}}}-\frac{x_i}{x_j\abs{\vec{x}}}\right)
\\
&=\frac{1}{2}\frac{x_j-x_i-\abs{\vec{x}}}{x_j\abs{\vec{x}}}
\end{align}

if $i=j\ne k$
\begin{align}
\Gamma_{jk}^i&=\Gamma_{ik}^i=\frac{1}{2}\left(\frac{\partial g_{ii}}{\partial x_k}g^{ii}-\frac{g_{ii}}{\partial x_i}g^{ii}\right)
\\
&=\frac{1}{2}\left(\frac{1}{\abs{\vec{x}}}-\frac{x_i-\abs{\vec{x}}}{x_i\abs{\vec{x}}}\right)
\\
&=\frac{1}{2}\left(\frac{x_i-x_j+\abs{\vec{x}}}{x_i\abs{\vec{x}}}\right)
\\
&=\frac{1}{2x_i}
\end{align}

if $i=k\ne j$

\begin{align}
\Gamma_{jk}^i&=\Gamma_{ji}^i=\frac{1}{2}\left(\frac{\partial g_{jj}}{\partial x_i}g^{ji}-\frac{\partial g_{ji}}{\partial x_i}g^{ii}\right)
\\
&=\frac{1}{2}\left(-\frac{\partial g_{ji}}{\partial x_i}g^{ii}\right)
\\
&=0
\end{align}

If $i\ne j\ne k$

\begin{align}
\Gamma_{jk}^i=0
\end{align}

If $i=j=k$

\begin{align}
\Gamma_{jk}^i=\frac{1}{2}\frac{x_i-\abs{\vec{x}}}{x_i\abs{\vec{x}}}.
\end{align}

Next we compute the terms in $V_0^i$. Since $g^{jk}=0$ if $j\ne k$, we have
\begin{align}
\sum_{j,k}g^{jk}\Gamma_{jk}^i&=\Gamma_{ii}^i+\sum_{j\ne i}g^{jj}\Gamma_{jj}^i
\\
&=\frac{1}{2}\frac{x_i-\abs{\vec{x}}}{x_i\abs{\vec{x}}}+\sum_{j\ne i}\frac{1}{2}\frac{x_j-x_i-\abs{\vec{x}}}{x_j\abs{\vec{x}}}
\\
&=\frac{1}{2\abs{\vec{x}}}\left(\frac{x_i-\abs{\vec{x}}}{x_i}+\sum_{j\ne i}\frac{x_j-x_i-\abs{\vec{x}}}{x_j}\right)
\\
&=\frac{1}{2\abs{\vec{x}}}\left(\frac{x_i-\abs{\vec{x}}}{x_i}+\sum_{j\ne i}\left(\frac{x_j-\abs{\vec{x}}}{x_j}-\frac{x_i}{x_j}\right)\right)
\end{align}
Since $\vec{x}$ is on the simplex, i.e., $\abs{\vec{x}}=1$, the above expression can be simplified to
\begin{align}
&\frac{1}{2}\left(\frac{x_i-1}{x_i}+\sum_{j\ne i}\left(1-\frac{1}{x_j}-\frac{x_i}{x_j}\right)\right)
\\
&=\frac{1}{2}\left(1-\frac{1}{x_i}+(n-1)-\sum_{j\ne i}\frac{1}{x_j}-\sum_{j\ne i}\frac{x_i}{x_j}\right)
\\
&=\frac{1}{2}\left(n-\frac{1}{x_i}-\sum_{j\ne i}\frac{1}{x_j}-\sum_{j\ne i}\frac{x_i}{x_j}\right)
\\
&=\frac{1}{2}\left(n-\sum_{j}\frac{1}{x_j}-\sum_{j\ne i}\frac{x_i}{x_j}-\frac{x_i}{x_i}+1\right)
\\
&=\frac{1}{2}\left(n+1-\sum_{j}\frac{1}{x_j}-\sum_{j}\frac{x_i}{x_j}\right),
\end{align} 
on the other hand, 
\[
\sigma_{ii}=\sqrt{x_i}\ \ \text{and}\ \ 0\ \text{for }j\ne i.
\]
So we have
\begin{align}
V_0^i&=\epsilon\cdot\frac{1}{2}\left(n+1-\sum_{j}\frac{1}{x_j}-\sum_{j}\frac{x_i}{x_j}\right)+\sqrt{2\epsilon}\cdot\sqrt{x_i}z_0^i
\\
&=\frac{\epsilon}{2}\left(n+1-\sum_{j}\frac{1}{x_j}-\sum_{j}\frac{x_i}{x_j}\right)+\sqrt{2\epsilon x_i}z_0^i
\end{align}


Recall the exponential map from tangent space of Shahshahani manifold to the base manifold is
$\Exp_{\vec{x}}\left(\vec{v}\right)$,
in order to simplify the algorithm, we use orthogonal projection from the tangent space to the underlying manifold, instead of using the exact exponential map. In the Euclidean space, the manifold gradient descent for a function defined on $M\subset\mathbb{R}^d$ we mean the algorithm
\[
\vec{x}_{t+1}=\Retr_{\vec{x}_t}\left(-\epsilon\mathcal{P}_{T_{\vec{x}}}\nabla f(\vec{x}_t)\right)
\]
where $\mathcal{P}_{T_{\vec{x}}}\nabla f(\vec{x}_t)$ is the orthogonal projection of $\nabla f(\vec{x}_t)$ onto the tangent space $T_{\vec{x}_t}M$ with respect to the Euclidean metric on the ambient space $\mathbb{R}^d=T_{\vec{x}_t}\mathbb{R}^d$. To derive the Langevin MWU, we firstly generalize the manifold gradient on submanifold of Euclidean space to the case when the ambient space is a general Riemannian manifold $N$. Let $\grad_{N}f$ be the Riemannian gradient of $f$ on $N$, then the generalized gradient descent on the submanifold $M\subset N$ is of the following form:
\[
\vec{x}_{t+1}=\Retr_{\vec{x}_t}\left(-\epsilon\mathcal{P}_{T_{\vec{x}_t}}\grad_Nf(\vec{x}_t)\right)
\]
where the orthogonal projection of $\grad_Nf(\vec{x})$ onto the tangent space $T_{\vec{x}}M$ is based on the inner product $\langle,\rangle$ on the ambient tangent space $T_{\vec{x}}N$. For the case of Shahshahani manifold, let $N=\mathbb{R}^d_+$ and $M=\Delta_+^{d-1}$, and the orthogonal projection in the tangent space is with respect to the Shahshahani metric. Then for small $\epsilon>0$, the orthgonal projection of $-\epsilon\grad_Nf(\vec{x})$ onto $T_{\vec{x}}M$ is the vector obtained form the difference between the point $G(\vec{x})$ that is the normalization of $\vec{x}-\epsilon\grad f(\vec{x})$ onto the simplex and the initial point $\vec{x}$:
\begin{align}
-\vec{v}=-\mathcal{P}_{T_{\vec{x}}M}\vec{v}
=\mathcal{P}_{T_{\vec{x}}M}( \vec{v})
=G(\vec{x})-\vec{x}
\end{align}
where
\[
G(\vec{x})=\left(\frac{x_1- v_1}{1-\sum_jv_j},...,\frac{x_d- v_d}{1- \sum_jv_j}\right).
\]
Combined with the calculation of Brownian motion, we need to project the gradient and noise onto Shahshahani manifold,
\[
x_i\leftarrow\frac{x_i-\epsilon x_i\frac{\partial f}{\partial x_i}+V_0^i}{1-\epsilon\sum_{j}x_j\frac{\partial f}{\partial x_j}+\sum_j V_0^j}.
\]
So far we have completed the derivation of Langevin MWU when the reverse temperature $\beta=1$, it is trivial to scale $V_0^i$ with general $\beta$ to get the general form of Langevin MWU. The proof completes.
\end{proof}

\section{Missing Proofs}

\section{Proof of Lemma \ref{lemma:convergence1}}

\begin{proof}
Let $\vec{x},\vec{y}$ be two points on the Shahshahani manifold $M$ and $\vec{1}:=\frac{1}{n}(1,...,1)$. By the definition of geodesic, we claim that there exists a unique geodesic connecting $\vec{x}$ and $\vec{y}$. Especially this geodesic can be parametrized by length, i.e., there exists a parametrized curve $\gamma(t)$ with unit speed defined on $t\in[0,\ell]$ such that $\gamma(0)=\vec{x}$ and $\gamma(\ell)=\vec{y}$, where $\ell = d(\vec{x},\vec{y})$. Recall that $\langle,\rangle_{\gamma(t)}$ is the Riemannian metric at point $\gamma(t)$, for this pair of $\vec{x}$ and $\vec{y}$, we have the following
\begin{equation}
\begin{split}
f(\vec{y})-f(\vec{x})&=\int_0^{\ell}\langle\grad f(\gamma(t)),\gamma'(t)\rangle_{\gamma(t)}dt
\\
&\le \int_0^{\ell}\abs{\langle\grad f(\gamma(t)),\gamma'(t)\rangle_{\gamma(t)}}dt
\\
&\le\int_0^{\ell}\norm{\grad f(\gamma(t))}_{\gamma(t)}\cdot\norm{\gamma'(t)}_{\gamma(t)}dt.
\end{split}
\end{equation}
Suppose at $t=\xi\in[0,\ell]$, function $\norm{\grad f(\gamma(t))}_{\gamma(t)}\cdot\norm{\gamma'(t)}_{\gamma(t)}$ reaches its maximum value, then the above inequality can be simplified so that
\begin{equation}\label{eq:MVT}
f(\vec{y})-f(\vec{x})\le\norm{\grad f(\gamma(\xi))}_{\gamma(\xi)}\int_0^{\ell}\norm{\gamma'(\xi)}_{\gamma(\xi)}dt=\norm{\grad f(\gamma(\xi))}_{\gamma(\xi)}\cdot d(\vec{x},\vec{y}),
\end{equation}
where the last equality holds because $\gamma(t)$ is a unit speed geodesic. The assumption on $\grad f$ in the statement of this lemma gives that
\[
\norm{\grad f(\gamma(\xi))}_{\gamma(\xi)}\le c_1d(\vec{1},\gamma(\xi))+c_2.
\]
Since $\vec{1}$, $\vec{x}$ and $\vec{y}$ consist a geodesic triangle on $M$ and $\gamma(\xi)$ lies on the geodesic connecting $\vec{x}$ and $\vec{y}$, triangle inequality of the metric induced by Riemannian metric implies the following inequality,
\begin{equation}
\begin{split}
d(\vec{1},\vec{x})+d(\vec{x},\gamma(\xi))&\ge d(\vec{1},\gamma(\xi))
\\
d(\vec{1},\vec{y})+d(\vec{y},\gamma(\xi))&\ge d(\vec{1},\gamma(\xi))
\end{split}
\end{equation}
and then
\begin{equation}
\begin{split}
2d(\vec{1},\gamma(\xi))&\le d(\vec{1},\vec{x})+d(\vec{1},\vec{y})+d(\vec{x},\gamma(\xi))+d(\vec{y},\gamma(\xi))
\\
&=d(\vec{1},\vec{x})+d(\vec{1},\vec{y})+d(\vec{x},\vec{y})
\\
&\le 2\left(d(\vec{1},\vec{x})+d(\vec{1},\vec{y})\right).
\end{split}
\end{equation}
Combining with \ref{eq:MVT} we have
\begin{equation}
\begin{split}
f(\vec{y})-f(\vec{x})&\le \norm{\grad f(\gamma(\xi))}_{\gamma(\xi)}\cdot d(\vec{x},\vec{y})
\\
&\le\left(c_1d(\vec{1},\vec{x})+c_1d(\vec{1},\vec{y})+c_2\right)\cdot d(\vec{x},\vec{y}).
\end{split}
\end{equation}
Let $P$ be the coupling of $\mu$ and $\nu$, so that $W_2(\mu,\nu)^2=\mathbb{E}_Pd(X,Y)^2$ where $X\sim\mu$ and $Y\sim\nu$ respectively. Taking expectations of $f(\vec{x})-f(\vec{y})$, we have
\begin{equation}
\begin{split}
\int_M f\mu d\vol-\int_M f\nu d\vol=\mathbb{E}_P[f(X)-f(Y)]\le\mathbb{E}_P\abs{f(X)-f(Y)}.
\end{split}
\end{equation}

Moreover, 
\begin{equation}
\begin{split}
\mathbb{E}_P\abs{f(X)-f(Y)}&\le \mathbb{E}_P[(c_1d(\vec{1},\vec{x})+c_1d(\vec{1},\vec{y})+c_2)\cdot d(\vec{x},\vec{y})]
\\
&\le \sqrt{\mathbb{E}_P(c_1d(\vec{1},\vec{x})+c_1d(\vec{1},\vec{y})+c_2)^2}\sqrt{\mathbb{E}_Pd(\vec{x},\vec{y})^2}
\\
&\le \left(c_1\left(\mathbb{E}_{\mu}d(\vec{1},\vec{x})^2\right)^{\frac{1}{2}}+c_1\left(\mathbb{E}_{\nu}d(\vec{1},\vec{y})^2\right)^{\frac{1}{2}}+c_2\right)\cdot\sqrt{\mathbb{E}_Pd(\vec{x},\vec{y})^2}
\\
&=(c_1\sigma+c_2)W_2(\mu,\nu)
\end{split}
\end{equation}
We complete the proof by letting $c_1=\frac{M}{2}$ and $c_2=B$.
\end{proof}


\subsection{Proof of Lemma \ref{lemma:convergence2}}
\begin{proof}
Let $\nu(\vec{x})=\frac{e^{-\beta f(\vec{x})}}{\Lambda}$ denote the density function of the Gibbs measure with respect to the measure induced by the Shahshahani metric in simplex, where
\[
\Lambda = \int_Me^{-\beta f(\vec{x})}d\vol
\]
is the normalization constant known as the partition function. The differential entropy $h(\nu)$ is computed as follows,
\begin{equation}
\begin{split}
h(\nu)&=-\int_M\nu(\vec{x})\log\nu(x)d\vol
\\
&=-\int_M\frac{e^{-\beta f(\vec{x})}}{\Lambda}\log\frac{e^{-\beta f(\vec{x})}}{\Lambda}d\vol
\\
&=-\int_M\frac{e^{-\beta f(\vec{x})}}{\Lambda}(-\beta f(\vec{x})-\log\Lambda)d\vol
\\
&=\int_M\frac{e^{-\beta f(\vec{x})}}{\Lambda}(\beta f(\vec{x})+\log\Lambda)d\vol
\\
&=\int_M\frac{\beta f(\vec{x})e^{-\beta f(\vec{x})}}{\Lambda}d\vol+\int_M\frac{e^{-\beta f(\vec{x})}}{\Lambda}\log\Lambda d\vol
\\
&=\frac{1}{\Lambda}\int_M\beta f(\vec{x})e^{-\beta f(\vec{x})}d\vol+\log\Lambda
\end{split}
\end{equation} 
which implies
\[
\frac{h(\nu)}{\beta}=\frac{1}{\Lambda}\int_Mf(\vec{x})e^{-\beta f(\vec{x})}d\vol+\frac{\log\Lambda}{\beta}.
\]
Since $\nu(x)=\frac{e^{-\beta f(\vec{x})}}{\Lambda}$, we can furthermore obtain 
\[
\mathbb{E}_{\nu}f=\int_Mf(\vec{x})\nu(\vec{x})d\vol=\frac{h(\nu)}{\beta}-\frac{\log\Lambda}{\beta}
\]
Let $\vec{x}^*$ be any point that minimizes $f(\vec{x})$. Then $\grad f(\vec{x}^*)=0$. Since $f$ is assumed to be geodesically smooth, we have $f(\vec{x})-f(\vec{x}^*)\le\frac{M}{2}d(\vec{x},\vec{x}^*)^2$, the lower bound of $\log\Lambda$ can be obtained by the following calculation,
\begin{equation}
\begin{split}
\log\Lambda&=\log\int_Me^{-\beta f(\vec{x})}d\vol
\\
&=-\beta f(\vec{x}^*)+\log\int_Me^{\beta (f(\vec{x}^*)-f(\vec{x}))}d\vol
\\
&\ge -\beta f(\vec{x}^*)+\log\int_Me^{-\frac{\beta M}{2}d(\vec{x},\vec{x}^*)^2}d\vol.
\end{split}
\end{equation}
Note that showing the boundedness of the integral $\int_M\exp\left(-\frac{\beta M}{2}d(\vec{x},\vec{x}^*)^2\right)d\vol$ can be reduced to the boundedness of $\int_M\exp\left(-\frac{\beta M}{2}d(\vec{1}_n,\vec{x})^2\right)d\vol$ by a translation on $M$. The translation is defined with parallel transport. Suppose $\gamma(t)$ is the geodesic connecting $\vec{1}$ and an arbitrary point $\vec{y}$, $\vec{v}$ is the vector in the tangent space at $\vec{1}$ such that $\Exp_{\vec{1}}(\vec{v})=\vec{x}^*$. Let $\Gamma_{\vec{1}}^{\vec{y}}\vec{v}$ be the parallel transport of $\vec{v}$ along $\gamma(t)$, and define the image of $\vec{y}$ to be the point $\Exp_{\vec{y}}(\Gamma_{\vec{1}}^{\vec{y}}\vec{v})$.  Lemma \ref{lemma:distance} provides us the value of $\int_M\exp\left(-\frac{\beta M}{2}d(\vec{1},\vec{x})^2\right)d\vol$ by letting $c=\frac{\beta M}{2}$, i.e., 
\begin{equation}
\begin{split}
&\int_M\exp\left(-\frac{\beta M}{2}d(\vec{1},\vec{x})^2\right)d\vol
\\
&=\alpha_{n-1}\frac{1}{2}\left(\frac{2}{\beta M}\right)^{\frac{n-1}{2}}\Gamma\left(\frac{n-1}{2}\right)
\\
&-\alpha_{n-1}\frac{\tau(R)}{6(n-1)}\frac{1}{2}\left(\frac{2}{\beta M}\right)^{\frac{n+1}{2}}\Gamma\left(\frac{n+1}{2}\right)
\\
&+\frac{\alpha_{n-1}}{360(n-1)(n+1)}(-3\norm{R}^2+8\norm{\rho(R)}^2+5\tau(R)^2-18\Delta R)\frac{1}{2}\left(\frac{2}{\beta M}\right)^{\frac{n+3}{2}}\Gamma\left(\frac{n+3}{2}\right)
\\
&+\alpha_{n-1}\left(\frac{2}{\beta M}\right)^{\frac{n+5}{2}}\Gamma\left(\frac{n+5}{2}\right)O(1),
\end{split}
\end{equation}
By denoting $\text{poly}\left(\frac{1}{\beta}\right)$ the right hand side for short, we have
\[
\log\Lambda\ge-\beta f(\vec{x}^*)+\log\left(\text{poly}\left(\frac{1}{\beta}\right)\right).
\]
Dividing both side by $\beta$ and rearranging, we have
\[
\frac{\log \Lambda}{\beta}\ge -f(\vec{x}^*)+\frac{1}{\beta}\log\left(\text{poly}\left(\frac{1}{\beta}\right)\right)
\]
and 
\[
-f(\vec{x}^*)\le\frac{\log\Lambda}{\beta}-\frac{1}{\beta}\log\left(\text{poly}\left(\frac{1}{\beta}\right)\right)=\frac{\log\Lambda}{\beta}+\frac{1}{\beta}\log\left(\text{poly}\left(\frac{1}{\beta}\right)^{-1}\right).
\]
Combined with $\mathbb{E}_{\nu}f=\frac{h(\nu)}{\beta}-\frac{\log\Lambda}{\beta}$ that has been proven before, we have 

\[
\mathbb{E}_{\nu}f-f(\vec{x}^*)\le \frac{h(\nu)}{\beta}+\frac{1}{\beta}\log\left(\text{poly}\left(\frac{1}{\beta}\right)^{-1}\right)\le\frac{K}{\beta}+\frac{1}{\beta}\log\left(\text{poly}\left(\frac{1}{\beta}\right)^{-1}\right).
\]
\end{proof}




\begin{lemma}\label{lemma:distance}
Let $M$ be the Shahshahani manifold, $\vec{1}:=(\frac{1}{n},...,\frac{1}{n})^{\top}$. Then for any $c>0$, the integral of $e^{-cd(\vec{1},\vec{x})^2}$ over $M$ with respect to the volume form induced by Shahshahani metric is bounded, i.e.,
\[
\int_Me^{-cd(\vec{1},\vec{x})^2}d\vol
\]
is bounded.
\end{lemma}

\begin{proof}
The distance between $\vec{1}_n$ and any other point $\vec{x}$ on $M$ can be computed by the exponential map on Shahshahani manifold. Recall that the exponential map on $M$ at $\vec{x}$ is given by
\[
\Exp_{\vec{x}}(\vec{v})=\left(\frac{x_1e^{v_1}}{S},...,\frac{x_ne^{v_n}}{S}\right)
\]
where $S=\sum_jx_je^{v_j}$. Consider the tangent space at $\vec{1}_n$, i.e., $T_{\vec{1}_n}M$, for any $\vec{x}\in M$, there exists a unique $\vec{v}\in T_{\vec{1}_n}M$ such that $\Exp_{\vec{1}_n}(\vec{v})=\vec{x}$. This can be done by solving equations given as follows, 
\[
\left(\frac{e^{v_1}}{S},...,\frac{e^{v_n}}{S}\right)=(x_1,...,x_n).
\]
We have $\frac{e^{\sum v_i}}{S^n}=\prod_{i=1}^n x_i$. Thus $S=\left(\frac{1}{\prod_{i=1}^nx_i}\right)^{\frac{1}{n}}$, and then
$
v_i=\ln x_iS$, for all $i\in[n]$. 

Furthermore, if the exponential map is defined on $T_{\frac{1}{n}\vec{1}_n}M$, $S=\frac{1}{n}$, and then $v_i=\ln x_i-\ln n$ for all $i\in[n]$. The distance $d(\frac{1}{n}\vec{1}_n,\vec{x})$ between $\frac{1}{n}\vec{1}_n$ and $\vec{x}$ can be obtained by evaluating the Shahshahani length of the vector $\vec{v}$ at $\frac{1}{n}\vec{1}_n$, where $\vec{v}$ is the one satisfying $\Exp_{\frac{1}{n}\vec{1}_n}(\vec{v})=\vec{x}$. Since we already have the expression of each component of $\vec{v}$ with respect to the exponential map on the tangent space at $\frac{1}{n}\vec{1}_n$, the square of the distance $d(\frac{1}{n}\vec{1}_n,\vec{x})$ can be computed as follows,
\begin{equation}
d\left(\frac{1}{n}\vec{1}_n,\vec{x}\right)^2=\norm{\vec{v}}_{\frac{1}{n}\vec{1}_n}^2=(v_1,...,v_2)\left(
\begin{array}{ccc}
n&&
\\
&\ddots&
\\
&&n
\end{array}\right)
\left(
\begin{array}{c}
v_1
\\
\vdots
\\
v_n
\end{array}
\right)=n\sum_{i=1}^nv_i^2=n\sum_{i=1}^n(\ln x_i-\ln n)^2.
\end{equation}
Therefore, the integral $\int_Me^{-cd(\frac{1}{n}\vec{1}_n,\vec{x})^2}d\vol$ can be explicitly written as
\[
\int_M\exp\left(-cd\left(\vec{1},\vec{x}\right)^2\right)d\vol=\int_M\exp\left(-cn\sum_{i=1}^n(\ln x_i-\ln n)^2\right)d\vol.
\]
Since the volume form $d\vol$ is induced from the Shahshahani metric on $M$, which is not compact. In fact, each geodesic of infinite length can be embedded into $M$, i.e., for any $\vec{v}\in T_{\frac{1}{n}\vec{1}_n}M$, its image $\left(\frac{x_1e^{tv_1}}{S},...,\frac{x_ne^{tv_n}}{S}\right)$ converges asymptotically to a point on the boundary of simplex as $t\rightarrow \infty$. The function $\exp\left(-cd\left(\vec{1},\vec{x}\right)^2\right)$ can be integrated along any geodesic starting from $\vec{1}$. In order to estimate the integral presented in the beginning, we consider $\vec{x}$ is obtained by mapping $t\vec{v}\in T_{\vec{1}}M$ to $M$, where $\vec{v}$ has Shahshahani norm of $1$ at $\frac{1}{n}\vec{1}_n$. Then the integral can be written in terms of $t$ and $\vec{v}$, which is a polar coordinate system in Shahshahani manifold. Therefore, the integral is computed by an integration over the geodesic from $\vec{1}$ followed by an integration over a sphere $\vec{v}\in S^{n-2}$. Let's elaborate it as follows by denoting $\gamma(t)$ the geodesic with initial velocity $\vec{v}$.

\begin{equation}
\begin{split}
\int_M\exp\left(-cd\left(\vec{1},\vec{x}\right)^2\right)d\vol&=\int_0^{\infty}\left(\int_{S_{n-2}(t)}e^{-ct^2d(\vec{1},\vec{v})^2}dS_{n-2}(t)\right)\norm{\gamma'(t)}_{\gamma(t)}dt
\end{split}
\end{equation}
Note that the integral inside can be computed as follows,
\[
\int_{S_{n-2}(t)}e^{-ct^2d(\vec{1},\vec{v})^2}dS_{n-2}(t)=e^{-ct^2d(\vec{1},\vec{v})^2}\text{Volume}(S_{n-2}(t)),
\]
and we need some notations to estimate the volume of $S_{n-2}(t)$. Let $R$ be Riemannian curvature tensor on $n-1$ dimensional manifold $M$, from \cite{Alfred} we can write
\[
\tau(R)=\sum_{i=1}^{n-1}R_{ii}, \ \ \ \norm{R}^2=\sum_{i,j,k,l=1}^{n-1}R_{ijkl}^2,
\]
\[
\norm{\rho(R)}^2=\sum_{i,j=1}^{n-1}R_{ij}^2,\ \ \ \Delta R=\text{Laplacian of }R=\sum_{i=1}^n\nabla_{ii}^2\tau(R).
\]

Denote
\[
\alpha_{n-1}=\frac{2\Gamma(\frac{1}{2})^{n-1}}{\Gamma(\frac{n-1}{2})},
\]
and then
\begin{equation}
\begin{split}
\text{Volume}(S_{n-2}(t))&=\alpha_{n-1}t^{n-2}\left(1-\frac{\tau(R)}{6(n-1)}t^2+\right.
\\
&\left.\frac{1}{360(n-1)(n+1)}\left(-3\norm{R}^2+8\norm{\rho(R)}^2+5\tau(R)^2-18\Delta R\right)t^4+O(t^6)\right).
\end{split}
\end{equation}
We are now ready to compute the integral
\begin{equation}
\begin{split}
\int_Me^{-cd(\vec{1},\vec{x})^2}d\vol&=\int_0^{\infty}e^{-ct^2}\text{Volume}(S_{n-2}(t))dt
\\
&=\int_0^{\infty}e^{-ct^2}\alpha_{n-1}t^{n-2}dt
\\
&-\int_0^{\infty}e^{-ct^2}\alpha_{n-1}\frac{\tau(R)}{6(n-1)}t^ndt
\\
&+\int_0^{\infty}\frac{\alpha_{n-1}}{360(n-1)(n+1)}\left(-3\norm{R}^2+8\norm{\rho(R)}^2+5\tau(R)^2-18\Delta R\right)t^{n+2}e^{-ct^2}dt
\\
&+\int_0^{\infty}\alpha_{n-1}t^{n-2}O(t^6)e^{-ct^2}dt
\end{split}
\end{equation}
It is immediate to calculate more general integral in the form of 
\[
\int_0^{\infty}e^{-ct^2}t^kdt
\]
by writing $r=ct^2$, the above integral equals
\begin{equation}
\begin{split}
\frac{1}{2}\left(\frac{1}{c}\right)^{\frac{k+1}{2}}\int_0^{\infty}e^{-r}r^{\frac{k-1}{2}}dr&=\frac{1}{2}\left(\frac{1}{c}\right)^{\frac{k+1}{2}}\int_0^{\infty}e^{-r}r^{\frac{k+1}{2}-1}dr
\\
&=\frac{1}{2}\left(\frac{1}{c}\right)^{\frac{k+1}{2}}\Gamma\left(\frac{k+1}{2}\right).
\end{split}
\end{equation}
Thus we have
\begin{equation}
\begin{split}
\int_Me^{-cd(\vec{1},\vec{x})^2}d\vol&=\alpha_{n-1}\frac{1}{2}\left(\frac{1}{c}\right)^{\frac{n-1}{2}}\Gamma\left(\frac{n-1}{2}\right)
\\
&-\alpha_{n-1}\frac{\tau(R)}{6(n-1)}\frac{1}{2}\left(\frac{1}{c}\right)^{\frac{n+1}{2}}\Gamma\left(\frac{n+1}{2}\right)
\\
&+\frac{\alpha_{n-1}}{360(n-1)(n+1)}(-3\norm{R}^2+8\norm{\rho(R)}^2+5\tau(R)^2-18\Delta R)\frac{1}{2}\left(\frac{1}{c}\right)^{\frac{n+3}{2}}\Gamma\left(\frac{n+3}{2}\right)
\\
&+\alpha_{n-1}\left(\frac{1}{c}\right)^{\frac{n+5}{2}}\Gamma\left(\frac{n+5}{2}\right)O(1).
\end{split}
\end{equation}


The proof completes.

\end{proof}



The next proposition shows that one can obtain a complete analogy of the conditions in \cite{RRT17}. 

\begin{proposition}\label{lemma:bound of f}
Suppose function $f$ satisfies Assumptions 1-5. It holds that
\[
\norm{\grad f(\vec{x})}_{\vec{x}}\le \frac{M}{2}d(\vec{1},\vec{x})+B
\]
and 
\[
\frac{m}{2}d(\vec{1},\vec{x})^2-\frac{b}{2}\ln 3\le f(\vec{x})\le A+\frac{M}{2}d(\vec{1},\vec{x})^2+Bd(\vec{1},\vec{x}).
\]
\end{proposition}

\begin{proof}
Direct estimate gives the bound of gradient 
\[
\norm{\grad f(\vec{x})}_{\vec{x}}\le\frac{M}{2}d(\vec{1},\vec{x})+B.
\]
Suppose $\gamma(t)$ is the geodesic connecting $\vec{1}$ and $\vec{x}$. Using fundamental theorem of calculus along geodesic, we have
\[
f(\vec{x})-f(\vec{1})=\int_0^1\langle\grad f(\gamma(t)),\gamma'(t)\rangle_{\gamma(t)}dt
\]
where $\langle\cdot,\cdot\rangle_{\gamma(t)}$ is the Riemannian metric at $\gamma(t)$. By assumption that $f(\vec{1})\le A$, we have
\begin{equation}
\begin{split}
f(\vec{x})&=f(\vec{1})+\int_0^1\langle\grad f(\gamma(t)),\gamma'(t)\rangle_{\gamma(t)}dt
\\
&\le A+\int_0^1\langle\grad f(\gamma(t)),\gamma'(t)\rangle_{\gamma(t)}dt
\\
&\le A+\int_0^1\norm{\grad f(\gamma(t))}_{\gamma(t)}\cdot\norm{\gamma'(t)}_{\gamma(t)}dt
\\
&\le A+\int_0^1\left(\frac{M}{2}d(\vec{1},\gamma(t))+B\right)\norm{\gamma'(t)}_{\gamma(t)}dt
\\
&\le A+\int_0^1\left(\frac{M}{2}d(\vec{1},\vec{x})+B\right)\norm{\gamma'(t)}_{\gamma(t)}dt
\\
&=A+\left(\frac{M}{2}d(\vec{1},\vec{x})+B\right)\int_0^1\norm{\gamma'(t)}_{\gamma(t)}dt
\\
&=A+\left(\frac{M}{2}d(\vec{1},\vec{x})+B\right)d(\vec{1},\vec{x})
\\
&=A+\frac{M}{2}d(\vec{1},\vec{x})^2+Bd(\vec{1},\vec{x}).
\end{split}
\end{equation}
To show the other inequality, we consider the geodesic $\gamma(t)$ connecting $\vec{1}$ and $\vec{x}$, suppose $c\in[0,1]$ is a point on $\gamma(t)$. For that we can use more geometric features of geodesic, we assume the geodesic to be of constant speed, i.e., $\norm{\gamma'(t)}_{\gamma(t)}=\norm{\vec{v}}_{\vec{1}}=d(\vec{1},\vec{x})$, and then the length of the geodesic connecting $\vec{1}$ and $\vec{x}$ equals $\norm{\vec{v}}_{\vec{1}}=d(\vec{1},\vec{x})$. We have that
\[
f(\vec{x})=f(\gamma(c))+\int_c^{1}\langle f(\gamma(t)),\gamma'(t)\rangle_{\gamma(t)}dt
\]
where $f(\gamma(c))\ge 0$. By the dissipative assumption, i.e.,
\[
\langle\grad f(\gamma(t)),\frac{\gamma'(t)}{\norm{\gamma'(t)}_{\gamma(t)}}d(\vec{1},\gamma(t))\rangle_{\gamma(t)}\ge md(\vec{1},\gamma(t))^2-b
\]
Then we can have the following estimate,
\begin{equation}
\begin{split}
f(\vec{x})&\ge\int_c^{1}\langle\grad f(\gamma(t)),\gamma'(t)\rangle_{\gamma(t)}dt
\\
&=\int_c^1\langle\grad f(\gamma(t)),\frac{\gamma'(t)}{\norm{\gamma(t)}_{\gamma(t)}}d(\vec{1},\gamma(t))\rangle_{\gamma(t)}\frac{\norm{\gamma'(t)}_{\gamma(t)}}{d(\vec{1},\gamma(t))}dt
\\
&\ge\int_c^1\left(md(\vec{1},\gamma(t))^2-b\right)\frac{\norm{\gamma'(t)}_{\gamma(t)}}{d(\vec{1},\gamma(t))}
\\
&=\int_c^1\left(mt^2\norm{\vec{v}}_{\vec{1}}^2-b\right)\frac{\norm{\vec{v}}_{\vec{1}}}{t\norm{\vec{v}}_{\vec{1}}}dt
\\
&=\int_c^1\left(mt\norm{\vec{v}}_{\vec{1}}^2-\frac{b}{t}\right)dt
\\
&=\frac{m(1-c^2)}{2}\norm{\vec{v}}_{\vec{1}}^2+b\ln c
\\
&=\frac{m(1-c^2)}{2}d(\vec{1},\vec{x})^2+b\ln c.
\end{split}
\end{equation}
Taking $c=\frac{1}{\sqrt{3}}$, we have 
\[
\frac{m}{2}d(\vec{1},\vec{x})^2-\frac{b}{2}\ln 3\le f(\vec{x})\le A+\frac{M}{2}d(\vec{1},\vec{x})^2+Bd(\vec{1},\vec{x}).
\]
The proof completes.

\end{proof}








