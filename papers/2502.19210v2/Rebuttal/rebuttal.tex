\documentclass{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{tikz}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[font=small,labelfont=bf]{caption}

\usepackage{caption}
\usepackage{graphicx}
\usepackage{float} 
%\usepackage{subfigure}

\usepackage{parskip}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{comment}
\usepackage{subfigure}
\usepackage{color}
\usepackage{cases}

\usepackage{tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.pathreplacing}

\usepackage{amscd,latexsym,amsthm,amsfonts,amssymb,amsmath,amsxtra}

\usepackage{tikz}
\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}
\RequirePackage{amssymb}
\definecolor{mygray}{gray}{0.85}

\usepackage{hyperref}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{
  colorlinks=true,
  linkcolor=mydarkblue ,    % 链接颜色
  filecolor=mydarkblue ,  % 文件颜色
  urlcolor=red ,     % URL颜色
  citecolor=mydarkblue       % 引用颜色
}



\usepackage{thmtools}
\usepackage{thm-restate}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}


\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}[theorem]{Example}


\newcommand{\diag}{\mathop{\mathbf{diag}}}
\newcommand{\abs}[1]{\ensuremath{\left|#1\right|}}
%\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\norm}[2][]{\ensuremath{\left\Vert #2 \right\Vert}}
\newcommand{\spec}[1]{\ensuremath{\mathrm{sp}\inp{#1}}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\argmin}{\mathop{\mathbf{argmin}}}
\newcommand{\argmax}{\mathop{\mathbf{argmax}}}
\newcommand{\vol}{\mathrm{Vol}}
\newcommand{\grad}{\mathrm{grad}}
\newcommand{\Proj}{\mathrm{Proj}}
\newcommand{\Retr}{\mathrm{Retr}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Hess}{\mathrm{Hess}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\Log}{\mathrm{Log}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\Div}{\mathrm{div}}
\newcommand\pder[2][]{\ensuremath{\frac{\partial#1}{\partial#2}}}
\newcommand{\be}{\beta^{-1}}
\newcommand{\BA}{{\mathbb {A}}} \newcommand{\BB}{{\mathbb {B}}}
    \newcommand{\BC}{{\mathbb {C}}} \newcommand{\BD}{{\mathbb {D}}}
    \newcommand{\BE}{{\mathbb {E}}} \newcommand{\BF}{{\mathbb {F}}}
    \newcommand{\BG}{{\mathbb {G}}} \newcommand{\BH}{{\mathbb {H}}}
    \newcommand{\BI}{{\mathbb {I}}} \newcommand{\BJ}{{\mathbb {J}}}
    \newcommand{\BK}{{\mathbb {K}}} \newcommand{\BL}{{\mathbb {L}}}
    \newcommand{\BM}{{\mathbb {M}}} \newcommand{\BN}{{\mathbb {N}}}
    \newcommand{\BO}{{\mathbb {O}}} \newcommand{\BP}{{\mathbb {P}}}
    \newcommand{\BQ}{{\mathbb {Q}}} \newcommand{\BR}{{\mathbb {R}}}
    \newcommand{\BS}{{\mathbb {S}}} \newcommand{\BT}{{\mathbb {T}}}
    \newcommand{\BU}{{\mathbb {U}}} \newcommand{\BV}{{\mathbb {V}}}
    \newcommand{\BW}{{\mathbb {W}}} \newcommand{\BX}{{\mathbb {X}}}
    \newcommand{\BY}{{\mathbb {Y}}} \newcommand{\BZ}{{\mathbb {Z}}}
    
    \newcommand{\CA}{{\mathcal {A}}} \newcommand{\CB}{{\mathcal {B}}}
    \newcommand{\CC}{{\mathcal {C}}}
    \newcommand{\CE}{{\mathcal {E}}} \newcommand{\CF}{{\mathcal {F}}}
    \newcommand{\CG}{{\mathcal {G}}} \newcommand{\CH}{{\mathcal {H}}}
    \newcommand{\CI}{{\mathcal {I}}} \newcommand{\CJ}{{\mathcal {J}}}
    \newcommand{\CK}{{\mathcal {K}}} \newcommand{\CL}{{\mathcal {L}}}
    \newcommand{\CM}{{\mathcal {M}}} \newcommand{\CN}{{\mathcal {N}}}
    \newcommand{\CO}{{\mathcal {O}}} \newcommand{\CP}{{\mathcal {P}}}
    \newcommand{\CQ}{{\mathcal {Q}}} \newcommand{\CR}{{\mathcal {R}}}
    \newcommand{\CS}{{\mathcal {S}}} \newcommand{\CT}{{\mathcal {T}}}
    \newcommand{\CU}{{\mathcal {U}}} \newcommand{\CV}{{\mathcal {V}}}
    \newcommand{\CW}{{\mathcal {W}}} \newcommand{\CX}{{\mathcal {X}}}
    \newcommand{\CY}{{\mathcal {Y}}} \newcommand{\CZ}{{\mathcal {Z}}}

\newcommand{\ab}{{\mathrm{ab}}}\newcommand{\Ad}{{\mathrm{Ad}}}
    \newcommand{\ad}{{\mathrm{ad}}}\newcommand{\al}{{\mathrm{al}}}
    \newcommand{\alg}{{\mathrm{alg}}}\newcommand{\Ann}{{\mathrm{Ann}}}
    \newcommand{\Aut}{{\mathrm{Aut}}}\newcommand{\Ar}{{\mathrm{Ar}}}
    \newcommand{\AI}{{\mathrm{AI}}}\newcommand{\Alb}{{\mathrm{Alb}}}
    \newcommand{\Art}{{\mathrm{Art}}} \newcommand{\bij}{{\mathrm{bij}}}
    \newcommand{\Br}{{\mathrm{Br}}}\newcommand{\BBC}{{\mathrm{BC}}}
    \newcommand{\Char}{{\mathrm{Char}}}\newcommand{\cf}{{\mathrm{cf}}}
    \newcommand{\Ch}{{\mathrm{Ch}}}\newcommand{\cod}{{\mathrm{cod}}}
    \newcommand{\cond}{\mathrm{cond^r}}\newcommand{\Cond}{{\mathrm{Cond}}}
    \newcommand{\cont}{{\mathrm{cont}}}\newcommand{\cris}{{\mathrm{cris}}}
    \newcommand{\corank}{{\mathrm{corank}}}
    \newcommand{\Cor}{{\mathrm{Cor}}}\newcommand{\cl}{{\mathrm{cl}}}
    \newcommand{\Cl}{{\mathrm{Cl}}}\newcommand{\can}{{\mathrm{can}}}
    \newcommand{\codim}{{\mathrm{codim}}}\newcommand{\Coker}{{\mathrm{Coker}}}
    \newcommand{\coker}{{\mathrm{coker}}}\newcommand{\cyc}{{\mathrm{cyc}}}
    \newcommand{\dR}{{\mathrm{dR}}}\newcommand{\depth}{{\mathrm{depth}}}
    \newcommand{\disc}{{\mathrm{disc}}}\newcommand{\Deg}{{\mathrm{Deg}}}
    \newcommand{\Def}{{\mathrm{Def}}}\newcommand{\der}{{\mathrm{der}}}
    \newcommand{\Dim}{{\mathrm{dim}}}
   \renewcommand{\div}{{\mathrm{div}}}
    \newcommand{\End}{{\mathrm{End}}} \newcommand{\Eis}{{\mathrm{Eis}}}
    \newcommand{\Ell}{{\mathrm{Ell}}}\newcommand{\Error}{{\mathrm{Errr}}}
    \newcommand{\Frac}{{\mathrm{Frac}}}\newcommand{\Fr}{{\mathrm{Fr}}}
    \newcommand{\Frob}{{\mathrm{Frob}}} \newcommand{\fin}{{\mathrm{fin}}}
    \newcommand{\forget}{{\mathrm{forget}}}
    \newcommand{\Gal}{{\mathrm{Gal}}} \newcommand{\GL}{{\mathrm{GL}}}
    \newcommand{\Groth}{{\mathrm{Groth}}}\newcommand{\GSp}{{\mathrm{GSp}}}
    \newcommand{\Hg}{{\mathrm{Hg}}}\newcommand{\Hom}{{\mathrm{Hom}}}
    \newcommand{\height}{{\mathrm{ht}}}\newcommand{\Hol}{{\mathrm{Hol}}}
    \newcommand{\id}{{\mathrm{id}}}\renewcommand{\Im}{{\mathrm{Im}}}
    \newcommand{\Ind}{{\mathrm{Ind}}}
    \newcommand{\Irr}{{\mathrm{Irr}}}
    \newcommand{\inv}{{\mathrm{inv}}}\newcommand{\Isom}{{\mathrm{Isom}}}
    \newcommand{\Jac}{{\mathrm{Jac}}}\newcommand{\Ker}{{\mathrm{Ker}}}
    \newcommand{\KS}{{\mathrm{KS}}}\newcommand{\length}{{\mathrm{length}}}
    \newcommand{\Lie}{{\mathrm{Lie}}}\newcommand{\LT}{{\mathrm{LT}}}
    \newcommand{\loc}{{\mathrm{loc}}}
    \newcommand{\mult}{{\mathrm{mult}}}\newcommand{\Meas}{{\mathrm{Meas}}}
    \newcommand{\Mor}{{\mathrm{Mor}}}
    \newcommand{\new}{{\mathrm{new}}} \newcommand{\NS}{{\mathrm{NS}}}
    \newcommand{\NT}{{\mathrm{NT}}} \newcommand{\old}{{\mathrm{old}}}
    \newcommand{\ord}{{\mathrm{ord}}} 
        \newcommand{\PGL}{{\mathrm{PGL}}} \newcommand{\Pic}{\mathrm{Pic}}
    \newcommand{\pr}{{\mathrm{pr}}}
    \renewcommand{\mod}{\ \mathrm{mod}\ }\renewcommand{\Re}{{\mathrm{Re}}}
    \newcommand{\Rep}{{\mathrm{Rep}}}\newcommand{\rec}{{\mathrm{rec}}}
    \newcommand{\ram}{{\mathrm{ram}}}\newcommand{\Rings}{{\mathrm{Rings}}}
    \newcommand{\red}{{\mathrm{red}}}\newcommand{\Rat}{{\mathrm{Rat}}}
    \newcommand{\reg}{{\mathrm{reg}}}
    \newcommand{\Sel}{{\mathrm{Sel}}} \newcommand{\Sch}{{\mathrm{Sch}}}
    \newcommand{\sep}{{\mathrm{sep}}}\newcommand{\sh}{{\mathrm{sh}}}
    \newcommand{\st}{{\mathrm{st}}}\newcommand{\supp}{{\mathrm{supp}}}
    \newcommand{\Sh}{{\mathrm{Sh}}}\newcommand{\Sets}{{\mathrm{Sets}}}
    \newcommand{\sign}{{\mathrm{sign}}}\renewcommand{\ss}{{\mathrm{ss}}}
    \newcommand{\Sim}{{\mathrm{Sim}}}\newcommand{\SL}{{\mathrm{SL}}}
    \newcommand{\Spec}{{\mathrm{Spec}}} \newcommand{\Spf}{{\mathrm{Spf}}}
    \newcommand{\SO}{{\mathrm{SO}}}\newcommand{\Sp}{{\mathrm{Sp}}}
    \newcommand{\St}{{\mathrm{St}}}\newcommand{\SU}{{\mathrm{SU}}}
    \newcommand{\Sym}{{\mathrm{Sym}}}\newcommand{\sgn}{{\mathrm{sgn}}}
    \newcommand{\Stab}{{\mathrm{Stab}}}\newcommand{\Symb}{{\mathrm{Symb}}}
    \newcommand{\Symm}{{\mathrm{Symm}}}\newcommand{\Tate}{{\mathrm{Tate}}}
    \newcommand{\Tgt}{{\mathrm{Tgt}}}
    \newcommand{\RTr}{{\mathrm{Tr}}}\newcommand{\univ}{{\mathrm{univ}}}
    \newcommand{\ur}{{\mathrm{ur}}}\newcommand{\val}{{\mathrm{val}}}
    \newcommand{\Vect}{{\mathrm{Vect}}}
    \newcommand{\Var}{{\mathrm{Var}}}
    \newcommand{\dist}{{\mathrm{dist}}}
    \newcommand{\WD}{{\mathrm{WD}}}\newcommand{\Cov}{{\mathrm{Cov}}}
    \newcommand{\md}{{\mbox{d}}}
     \newcommand{\KL}{{\mathrm{KL}}}
\newcommand{\tvdots}{%
    \vcenter{%
        \baselineskip = 4pt
        \hbox{.}\hbox{.}\hbox{.}
    }
}

    \font\cyr=wncyr10

    \newcommand{\Sha}{\hbox{\cyr X}}\newcommand{\wt}{\widetilde}
    \newcommand{\wh}{\widehat}
    \newcommand{\pp}{\frac{\partial\bar\partial}{\pi i}}
    \newcommand{\pair}[1]{\langle {#1} \rangle}
    \newcommand{\wpair}[1]{\left\{{#1}\right\}}
    \newcommand{\intn}[1]{\left( {#1} \right)}
    \newcommand{\ds}{\displaystyle}\newcommand{\ov}{\overline}
    \newcommand{\Gros}{Gr\"{o}ssencharak}
    \newcommand{\Poincare}{Poincar\'{e}}
    \newcommand{\incl}{\hookrightarrow}
    \newcommand{\sk}{\medskip}\newcommand{\bsk}{\bigskip}
    \newcommand{\lra}{\longrightarrow}\newcommand{\lla}{\longleftarrow}
    \newcommand{\ra}{\rightarrow} \newcommand{\imp}{\Longrightarrow}
    \newcommand{\lto}{\longmapsto}\newcommand{\bs}{\backslash}
    \newcommand{\nequiv}{\equiv\hspace{-9.5pt}/\ }
    \newcommand{\s}{\sk\noindent}\newcommand{\bigs}{\bsk\noindent}
     \newcommand{\tb}{\textbf}
     

\newcommand\coolover[2]{\mathrlap{\smash{\overbrace{\phantom{%
    \begin{matrix} #2 \end{matrix}}}^{\mbox{$#1$}}}}#2}
\newcommand\coolrightbrace[2]{%
\left.\vphantom{\begin{matrix} #1 \end{matrix}}\right\}#2}



    \theoremstyle{plain}
    %\renewcommand{\thechapter}{\Roman{chapter}}
    \newtheorem{thm}{Theorem}[section] \newtheorem{cor}[thm]{Corollary}
    \newtheorem{lem}[thm]{Lemma}  \newtheorem{prop}[thm]{Proposition}
    \newtheorem {conj}[thm]{Conjecture} \newtheorem{defn}[thm]{Definition}
     \newtheorem {rem}[thm]{Remark}
     \newtheorem {assu}[thm]{Assumption}

    \DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arccsc}{arccsc}
\DeclareMathOperator{\arccosh}{arccosh}
\DeclareMathOperator{\arcsinh}{arcsinh}
\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\arcsech}{arcsech}
\DeclareMathOperator{\arccsch}{arccsch}
\DeclareMathOperator{\arccoth}{arccoth}
\hypersetup{
colorlinks=true,
linkcolor=black
}

\definecolor{antiquegold}{RGB}{205,127,50} 
\definecolor{deepskyblue}{RGB}{0,191,255} 
\definecolor{crimson}{RGB}{220,20,60} 
\definecolor{orange}{RGB}{255,165,0} 
\definecolor{green}{RGB}{0,128,0}


\title{Rebuttal AAAI2025}



\begin{document}

\maketitle


\section{For R1}
Thank you for your support and comments. We agree that our assumptions are strong, especially Assumption 3 and 5 are not standard in optimization problems. Assumption 3 is a manifold version of the dissipative condition used in (Raginsky, Rakhlin and Telgarsky 2017), and Assumption 5 is a condition guaranteeing fast convergence for Langevin algorithms. Thanks for the comment, it is possible to make these conditions less strictive.

For classic MWU, the difference is from the derivation of the algorithm with geometric meanings. Linear MWU has a clear and simple Riemannian gradient descent formulation which allows one to add geometric Brownian motion easily. But for classic MWU, despite we can involve the noise and the outcome of the algorithm is expected to be similar, the geometric meaning, i.e., Riemannian Langevin algorithm formulation, is less clear. This is an important direction for future work since classic MWU and its linear variant are essentially different algorithms, further exploration will help understanding both algorithms better.

\section{For R2}
Thank you for your support and insightful questions. 

"Are there any application beyond portfolio optimization?"

While our primary application of the Langevin Multiplicative Weights Update (LMWU) algorithm is in polynomial portfolio optimization, its design is broadly adaptable to various nonconvex optimization problems over simplices. We focused on polynomial portfolio optimization as it addresses a key issue in finance and closely aligns with practical applications in the field.

LMWU's capability to escape local minima and converge toward global optima makes it highly valuable for more general applications:
(i) In neural network training, where nonconvex optimization with numerous local minima is common, LMWU could improve convergence to better solutions, especially in models with simplex constraints in their parameter space.
(ii) In game theory, LMWU can facilitate Nash equilibrium computation, as players' strategies often lie within constrained simplices and global convergence is crucial for stable equilibrium solutions.
(iii) In multi-agent systems, LMWU allows distributed agents to optimize their objectives over constrained domains without getting trapped in local optima, enhancing coordination among agents, particularly in high-dimensional, constrained settings.

These examples highlight LMWU's broader utility in tackling complex, nonconvex problems, indicating its potential for wider applications in constrained optimization challenges.

"Is it possible...problems?"
It is possible to have a similar algorithm for general polytopes with barrier functions defining the Riemannian metric in the interior. However, the analysis for optimization is not straightforward, since the simplex with Shahshahani metric has constant positive curvature, but one cannot expect the same property to hold for general constraints.

As pointed, the non-trivial projection for other constraints will be one of the main challenges. A standard approach for polytopes with Riemannian metric in the interior is the exponential map which requires solving a second order ordinary differential equation, i.e., the geodesic equation, and this will make the implementation and analysis more twisted. 

"How does... Euclidean space?"
The main difference of Theorem 3.1 and its Euclidean counterpart is the appearance of log-Sobolev constant $\alpha$, which is not needed in Euclidean Langevin algorithm. The main idea of bounding the expected error is the same as the Euclidean setting.

"Are there...constraints?"
It is important to note that our theoretical analysis and proof of Theorem 3.1 depend on the the simplex constraints. First of all, the simplex is locally isometric to a positively curved manifold with constant curvature, which makes the computation of volumes and entropy integrals manageble. Secondly, the topology of the interior of a simplex is essentially diffeomorphic to a Euclidean space, this is another important property in introducing the Assumption 3. In Euclidean space, Assumption 3 is called dissipative condition, which may not be clear what is should look like on compact or closed manifolds.

In the end, thanks for pointing out the typos, they have been fixed in updates.

\section{For R3}
Thanks a lot for your supportive and critical comments.
Weaknesses:

"The authors should discuss more about the comparison with Projected Langevin ... simple projection." The Projected Langevin Algorithm works as follows: In each step, the algorithm first performs a gradient descent step, then adds an IID Gaussian perturbation to the current point. If the point after perturbation lies outside the constraints, an additional convex projection step is applied to ensure the point satisfies the constraints. In practice, the problem of convex projection to the simplex is a non-trivial algorithms  when the dimension is very high. For example, a $\mathcal{O}(n)$ algorithm for convex projection into the simplex constrains algorithm was purposed in (Thai et al., 2015), where $n$ is the dimension parameter. For the portfolio management problem considered in this paper, $n$ is the constituent element of the portfolio, which can be very large in practice, thus the cost introduced by the projection step in the Projected Langevin Algorithm can not be ignored. However, the Langevin MWU algorithm purposed by the current paper is naturally defined on the simplex constrains, which avoid these computational cost of convex projection. 

In another aspect, we also observe that the Langevin MWU generally performs better on the simplex-constrained problem. For example, as shown in Figure 2 of this paper, Langevin MWU can escape local optima more quickly than Projected Langevin and exhibits more stable performance.

Reference: Thai et al., Projected sub-gradient with $\ell_1$ or simplex constraints via isotonic regression. CDC 2015


Further discussion about the distributed constraints. 
Thanks for pointing out the importance of exploring further applications in the distributed setting. Some natural applications might be game theory and multi-agent systems, where each simplex represents each agent's mixed strategies. This is a relatively young research topic of using Langevin algorithm in game theory, especially in zero-sum games (our setting is indeed a potential game). 

An immediate limitation of our theoretical analysis is the following. The payoff functions in a matrix game does not satisfy all our assumptions, especially assumption 3 and 5, which obviously fail if the function's gradient is linear. For polynomial payoffs, we are adding examples to see whether they agree with the polynomial portfolio settings.

"The proposed..."
The algorithm can be generalized to broader settings like polytopes, since equation (11) indicates that Riemannian geometry of simplex is actually the Hessian geometry. However, it is true that the theoretical analysis may not be generalized without difficulties, mainly because of the complicated dependence of  curvature. 

Minor comments
Yes, thanks for pointing this out. We will fix this redundancy of notations.

For R4:

Thank you for your careful reading and constructive comments. We first answer to the main concerns.

Contribution compared to Raginsky et al. 

The main technical contribution/challenge is the geometric version of the dissipative condition (A.3 in Raginsky et al.) and the analysis based on it, i.e., the proof of Lemma G.1 and G.2. The key technique is to apply the volume analysis of small geodesic ball (a classic result of Gray 1974.), which did not appear in previous works.

Writing of the paper. 
Thank you for letting us realize this issue, we will add more formal treatment, examples and references on the background, especially the dissipative condition, log-Sobolev inequality, volume analysis of small geodesic balls to the appendix.

We next address the 21 questions/issues.

1. The dimension is the $N(d-1)$ since the simplex is 1 dimension lower than the Euclidean space.

2. The word bias is from sampling with Langevin dynamics where discretization can only converge to biased distribution. We use it to emphasize that the gap in Theorem 3.1 cannot be reduced by increasing the iterations (different from deterministic algorithms).

3. Thank you, the reference is fixed.

4. Yes, we should've remove $\beta$ in $\nu$ or put $\beta^{-1}$ to entropy. $d(,)$ is the geodesic distance on the underlying manifold.

LSI can be expressed with KL divergence. Fisher information is the squared norm of the gradient of entropy. Simply speaking, KL dominates the $L^2$ norm of $log$-distribution. We will leave more explanation in revision due to space constraint here.

5. It should be $\nabla f(x)-\nabla f(y)$.

6. Thanks for noting it, Riemannian volume should've been stated earlier.

7. $\phi$ is just a convex function whose Hessain gives a Riemannian metric. And yes, it should be
$$\sum_{i=1}^n\left(1-\sum_{i-1}^{n-1}x_i\right)\ln\left(1-\sum_{i=1}^{n-1}x_i\right)$$.

8. Does it mean the second equality? Because the second term grad p(x,t) is derived from the gradient flow on Wasserstein space (p292, Optimal transport for applied mathematicians). If it refers to the second equality, that is a Riemannian version of "divergence of gradient field gives Laplacian" in Euclidean space.

Detailed derivation can be found on p292, Optimal transport for applied mathematicians. Simply, it's from
$$p\nabla(\frac{\delta h}{\delta p}(p))=p\frac{\nabla p}{p}=\nabla p$$
The evolution of $p$ follows the wasserstein gradient flow
$$\partial_tp=\nabla\cdot(p\nabla(\frac{\delta (h+\mathbb{E}_pf)}{\delta p}(p)))$$

9. We will uniform them as $h(p)$.

10. $M$ is smoothness constant of assumption 2. Thus, $M$ may not necessarily equal to 1.

11. $p$ is propotional to $\nu$ or differs by a normalization constant.

12. The update line 8 is interpreted as a manifold gradient descent (51), and the proof leverages the analysis on manifold. Despite the proof framework is general, our analysis heavily relies on geodesic ball volume computation (61)-(67), which depends on specific assumption 3 and constant positive curvature of Shahshahani metric.

13. In the minimization problem, the expectation is over the randomness in the returns vector \( r = [r_1, r_2, ..., r_n]^{\top} \), as the returns \( r_i \) are random variables, representing the individual returns of assets within the portfolio. The randomness of \( f(w, r) \), the polynomial loss function, arises from this vector of returns, as it depends on the variability of each asset's return within the portfolio. The weights \( w = [w_1, w_2, ..., w_n]^{\top} \) are deterministic, representing fixed portfolio allocations constrained to sum to one.

14. Thank you for the suggestion! We will plot the global optima in the figures in revised version. The global optima of the $6$ text functions are

    $f_1: (0.4049, 0.1969, 0.3981)$, $f_2:(0.3804, 0.3736, 0.2461)$, $f_3:(1, 0, 0)$, $f_4:(0.0008, 0.1464, 0.8527)$, $f_5:(0.5111, 0.4889, 0, 0,0)$, $f_6:(0, 0, 0.0182, 0.5309, 0.4509,0)$

    Some assumptions may not be satisfied by the test functions, these experiments shows that the algorithm works beyond the class the theoretical analysis focuses on.

15. $b_i$'s are general components of a vector field.

16. Thanks for your suggestion, this part needs more detailed derivation, based on the Shahshahani orthogonality.

17. Yes, just as all Langevin algorithms, basically it's Gibbs sampling. More comparisons will be added in revision.

18. It's called LMWU because the analysis, especially (61)-(67), cannot be simply generalized to any Riemannian manifold. For example, Assumption 3 cannot even hold on spheres.

19. Comparison is given in beginning, and for sure, more elaboration will be added to the update, this is an crucial issue in current writeup. 

20. As mentioned before, we do not have a reasonable formulation for assumption 3 on compact manifolds, so on many Lie groups and manifolds, the analysis cannot be immediately generalized. But generalizing to polytope is possible, as long as we can deal with the non-constant curvature issue.

21. Essentially, Hessian geometry/interior point methods approach on polytope constraint are more or less similar. The main difficulty here is to conquer the non-constant curvature case. 

    







\section{For R4}

\begin{itemize}

    \item For the 6 test functions, do they satisfy the assumptions 1-5? And the global optima should be plotted in the figures for reference.

    Thank you for the suggestion! We will plot the global optima in the figures in revised version. The global optima of the $6$ text functions are

    $f_1: (0.4049, 0.1969, 0.3981)$, $f_2:(0.3804, 0.3736, 0.2461)$, $f_3:(1, 0, 0)$, $f_4:(0.0008, 0.1464, 0.8527)$, $f_5:(0.5111, 0.4889, 0, 0,0)$, $f_6:(0, 0, 0.0182, 0.5309, 0.4509,0)$


    
\end{itemize}



\end{document}