\section{Binary Embedding Framework}
\label{sec:PLCEmbed}
This section introduces our proposed framework, \textbf{PLCEmbed}, a raw-byte embedding method for PLC binaries. 
PLCEmbed is designed to be compiler-agnostic and to handle heterogeneous binary formats without relying on specialized reverse-engineering tools.
We begin by outlining the overall approach and objectives (Sec.~\ref{subsec:plcembed_approach}), then formalize the two primary classification tasks that PLCEmbed addresses (Sec.~\ref{subsec:plcembed_tasks}).


\subsection{Overall Approach and Goals}
\label{subsec:plcembed_approach}

\paragraph{Context and Motivation.}
Most industrial control system (ICS) security and forensics efforts have historically focused on high-level source code analysis or network-layer monitoring. However, as discussed in Sec.~\ref{sec:related} and~\ref{sec:plcbead}, many legacy environments lack accessible source code and rely on diverse, proprietary compiler toolchains to produce PLC executables. 
Traditional reverse-engineering methods assume standard file formats, symbol tables, or specific CPU architectures, yet PLC binaries often break these assumptions\cite{song2008bitblaze,shoshitaishvili2016sok,keliris2019icsref, ICSFuzz}. 
Moreover, multiple compilers each introduce unique design choices, making it difficult to devise a one-size-fits-all reverse-engineering technique.
In response to these challenges, we developed PLCEmbed to operate \textit{directly on the raw bytes} of a PLC binary, minimizing the assumptions about its internal format.

\paragraph{High-Level Idea.}
Rather than parsing executable sections or relying on symbol information, PLCEmbed consumes the entire binary as a variable-length sequence of bytes.
A learnable embedding layer first transforms each byte into a vector representation, allowing the model to discover relationships between byte values automatically.
Subsequent neural network components (detailed in Sec.~\ref{subsec:plcembed_architecture}) capture both local patterns, such as repeated compiler inserts, and global dependencies that span distant parts of the binary.
By avoiding compiler-specific heuristics or file-structure assumptions, PLCEmbed can unify analysis across multiple toolchains.


\paragraph{ICS Security Relevance.}
Two central questions emerge when investigating suspicious binaries in Industrial Control Systems (ICS):

(1) \textit{Which toolchain produced this executable?}

(2) \textit{What functionality does it implement?}

Identifying the toolchain (for example, CoDeSys vs. GEB) can be crucial if a known vulnerability or exploit path exists for a particular compiler version.
Determining the functionality can verify whether the binary’s behavior aligns with the claimed control task (for example, a timer routine, network protocol handler, or building automation function).
PLCEmbed targets these questions by mapping raw bytes to higher-level insights, paving the way for automated forensics in ICS environments.

\paragraph{Goals of PLCEmbed.}
Building on the multi-toolchain dataset introduced in Sec.~\ref{sec:plcbead}, we aim to:
\begin{itemize}
    \item \textit{Provide a unified method} that handles binaries from different compilers without custom reverse-engineering routines.
    \item \textit{Facilitate classification tasks} such as toolchain provenance and functionality detection, which are vital to ICS forensics.
    \item \textit{Enable future ICS security research} by offering a reference architecture that is purely data-driven and thus extensible to emerging proprietary formats.
\end{itemize}

As we envision the development of more sophisticated tools in the future, with our dataset playing a pivotal role in this progression, our immediate goal is to establish an ML-based compiler-agnostic framework for binary analysis. 
In the following subsections, we formalize the classification tasks (Sec.~\ref{subsec:plcembed_tasks}), then present the detailed model design (Sec.~\ref{subsec:plcembed_architecture}).

\subsection{Problem Formulation and Classification Tasks}
\label{subsec:plcembed_tasks}
In order to clarify the PLCEmbed approach, we define the input space (raw binary bytes) and two separate classification objectives: \textbf{toolchain provenance} and \textbf{functionality}.

\paragraph{Input Representation.}
Let $X$ denote the set of all PLC binaries within our dataset.
Each binary $b \in X$ is represented as a sequence of bytes $\mathbf{x} = (x_1, x_2, \dots, x_L)$, where $x_i \in \{0,1,\dots,255\}$ and $L$ may vary across files.
When $L$ surpasses a maximum length (for example, 65,536 bytes), we truncate; when $L$ is shorter, we pad with a special token to indicate empty bytes.
Hence, every binary is mapped to a fixed-length sequence $\mathbf{x}$, which will be the input to our model.

\paragraph{Toolchain Provenance.}
We denote the toolchain label as $y_{\mathrm{tc}} \in \{\text{CoDeSys}, \text{GEB}, \text{OpenPLC-V2}, \text{OpenPLC-V3}\}$.
This classification objective seeks to identify which compiler produced the binary.
Formally, we learn a function $f_{\mathrm{tc}}: \mathbf{x} \mapsto y_{\mathrm{tc}}$ that assigns one of these four classes.
Identifying compiler origin is critical for ICS security because certain vulnerabilities might be exclusive to specific toolchains, and forensic analysts can narrow their search if they know the toolchain used \cite{keliris2019icsref}. 
Table~\ref{tab:vulstat} shows the numbers of vulnerabilities identified in well-known PLC development systems reported to the \textit{National Vulnerability Database} (NVD)~\cite{NVD} and the average \textit{Common Vulnerability Scoring System} (CVSS) severity of them.
When analyzing PLC binaries, knowledge of the toolchain provenance allows investigators to examine the origins of threats more effectively, especially if the toolchain in question has been previously associated with similar security issues. Furthermore, utilizing compiler-related information makes it possible to model the compilation chain, which can enhance defenses against cyberattacks by providing detailed semantic behavior insights ~\cite{ben2018detection,rahimian2015bincomp,otsubo2020o-glassesx,benoit2021binary, yang2022ratscope}.

\begin{table}[!hb]
    \centering
    \caption{The reported number of PLC toolchain vulnerabilities in NVD and the average of CVSS severity.}
    \begin{tabular}{c|c|c}
        \hline
        PLC Toolchain & \# of Vulnerabilities & Avg. CVSS Severity\\
        \hline
        \textit{CoDeSys} & 11 & 7.75 (High)\\
        OpenPLC & 2 & 7.10 (High)\\
        Siemens TIA Portal & 57 & 6.52 (Medium)\\
        TwinCAT & 6 & 7.52 (High)\\
        RSLogix & 10 & 6.84 (Medium)\\
        \hline
    \end{tabular}
    \label{tab:vulstat}
\end{table}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/PLCEmbed.png} 

    \caption{The overall architecture of \textit{PLCEmbed}}
    \label{fig:archi}
\end{figure*}
% \vspace{-5pt}

\paragraph{Functionality Classification.}
A second classification task focuses on the functional category $y_{\mathrm{func}} \in \{\text{Timing}, \text{Network}, \dots\}$, comprising 22 labels derived from Sec.~\ref{subsubsec:labeling} of our dataset construction.
We learn a function $f_{\mathrm{func}}: \mathbf{x} \mapsto y_{\mathrm{func}}$ that predicts which of the 22 categories best describes the binary’s core behavior.
This capability assists ICS operators in verifying that a given binary indeed implements (for example) a building automation block rather than an unauthorized network-scanning routine or a maliciously modified PID controller.
This approach complements any available metadata from engineering workstations, audit logs, or network captures, allowing forensic teams to focus on files that deviate from expected patterns. It can also reduce manual hex inspections or partial reverse-engineering tasks by surfacing immediate leads in the early stage of an investigation.



\subsection{Model Architecture}
\label{subsec:plcembed_architecture}

Building on the problem formulation, we now present the detailed architecture of \textbf{PLCEmbed}. Fig.~\ref{fig:archi} offers a conceptual view of the pipeline, while the subsections below discuss each component. 
% The overall design uses a combination of convolutional layers (for local byte patterns) and transformer-based self-attention (for global context).

\subsubsection{Byte Encoding Layer}
\label{subsubsec:byte_embedding}
Each PLC binary is read as a sequence of bytes $\mathbf{x} = (x_1, x_2, \dots, x_L)$, where $x_i \in \{0,\dots,255\}$ and $L$ is fixed by truncation or padding.
We map each byte $x_i$ to a learnable embedding vector $\mathbf{e}_i \in \mathbb{R}^{d}$, where $d$ is a hyperparameter (for example, $d=256$).
We learn these embeddings jointly with the rest of the model, allowing the network to identify meaningful relationships among different byte values during training.
The resulting sequence $(\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_L)$ serves as the input to the subsequent layers.

\subsubsection{CNN-Based Local Feature Extraction}
\label{subsubsec:cnn_extractor}

After we obtain the byte embeddings, 
we use a one-dimensional convolutional layer to capture local patterns in the byte sequence~\cite{raff2018malware}. For example, many compilers insert characteristic blocks or repeated sequences at regular intervals. A CNN can exploit these short-range dependencies to generate a set of feature maps:
\begin{equation}
    \mathbf{h} = \text{Conv1D}(\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_L),
\end{equation}
where the kernel size and stride can be tuned to reflect typical repeated segments in PLC binaries. This extracted representation $\mathbf{h}$ emphasizes local byte patterns that may reveal compiler-specific “fingerprints.”

\subsubsection{Transformer for Global Context}
\label{subsubsec:transformer}
To capture global, long-range relationships within the byte sequence, we feed $\mathbf{h}$ into a transformer encoder~\cite{vaswani2017attention}. The encoder’s multi-head self-attention learns how different positions in the byte sequence relate to one another, allowing the model to link distant parts of the binary that might encode function calls or configuration blocks. While the CNN captures local cues, the transformer integrates these cues into a broader context vector.

\subsubsection{Classification Head}
\label{subsubsec:classification_head}

The final transformer output is a sequence of context vectors $(\mathbf{z}_1, \dots, \mathbf{z}_M)$, where $M \leq L$ depends on any pooling or downsampling used in the CNN stage. We apply a global pooling operation as the summary vector $\mathbf{z}_{\mathrm{summary}}$. A fully connected layer then maps $\mathbf{z}_{\mathrm{summary}}$ to class logits:
\begin{equation}
    \mathbf{p} = \mathrm{Softmax}\bigl(\mathbf{W}\,\mathbf{z}_{\mathrm{summary}} + \mathbf{b}\bigr).
\end{equation}
For \textit{toolchain provenance}, the size of $\mathbf{p}$ is 4, while for \textit{functionality classification}, it is 22 (matching the number of categories).

\subsubsection{CNN-Only Baseline}
\label{subsubsec:cnn_baseline}

Although the transformer encoder should help capture long-range dependencies, we also implement a simpler \textbf{CNN-only} baseline that omits the transformer layers. In this variant, the model processes byte embeddings through one or more convolutional blocks, then applies a final pooling and softmax layer. The CNN-only approach is expected to be more efficient but may struggle with binaries whose distinctive features are distributed across large code segments. 
This design is similar to prior raw-binary classifiers for general-purpose Linux binaries (e.g., MalConv~\cite{raff2018malware}), which detect local n-gram patterns but cannot directly capture long-range dependencies within a file.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/DF.png}
    \caption{PLC DF process comprises four steps: data collection, examination, analysis, and reporting. To analyze security incidents, the investigator looks at raw evidence and leverages the forensic artifacts for the following stages. \textit{PLCEmbed} serves as a tool to help investigators extract information from raw evidence, making the process more efficient. }
    \label{fig:DF}
    % \vspace{-5pt}
\end{figure}


\subsection{Application to PLC Digital Forensics}
\label{subsec:plc-forensics}

Fig.~\ref{fig:DF} illustrates the complete PLC digital forensics (DF) process, which comprises four steps: data collection, examination, analysis, and reporting~\cite{kent2006guide, karabiyik2018forensic}. In a typical investigation, the analyst first gathers raw evidence from PLC systems. During the examination stage, forensic artifacts are extracted from this raw data. Here, \textit{PLCEmbed} serves as a tool to automatically extract meaningful information from the raw bytes, making the subsequent analysis phase more efficient. By predicting both toolchain provenance and functionality, PLCEmbed assists investigators in verifying whether the behavior of an executable aligns with its claimed purpose. This capability streamlines the reporting process and helps focus further analysis on suspicious or anomalous files.


\subsection{Implementation and Training}
\label{subsec:plcembed_training}
We implement the convolution-attention pipeline using PyTorch. Data loading converts each PLC binary to a tensor of bytes truncated or padded to a maximum length (commonly 64K). The embedding dimension $d$ is selected based on preliminary validation experiments; a larger dimension may capture more nuanced patterns but requires more memory. Training proceeds for a fixed number of epochs (often 30), with early stopping criteria based on validation accuracy. Different classification heads can be swapped in for toolchain versus functionality tasks, but the same underlying architecture is retained.

All experiments use a train-test split at the granularity of individual ST programs to avoid overlap across data partitions. Batch sizes of 16 or 32 are typical. Hyperparameters such as learning rate and weight decay are tuned on a small validation set. Class weighting or oversampling mitigates imbalance in some of the functionality categories. During inference, the pipeline receives a raw binary, prepares the byte sequence, and outputs the predicted label.
% \todo{Appx.~B provides details on kernel sizes, attention heads, and residual blocks for those seeking a deeper technical reference for reproducing the main results.}

% \paragraph{Hyperparameters.}
% \begin{itemize}
%     \item \textit{Embedding dimension} $d$: 128 by default.
%     \item \textit{CNN kernel size} $k$: 8 or 16 based on pilot tests, with stride $s$ set to 4.
%     \item \textit{Transformer heads} $H$: 4 or 8; \textit{layers} $N$: 2 or 3.
%     \item \textit{Batch size}: 16, balancing memory constraints with stable training.
%     \item \textit{Learning rate}: $10^{-4}$ with the Adam optimizer.
%     \item \textit{Epochs}: Typically 30, with early stopping based on validation accuracy.
% \end{itemize}



