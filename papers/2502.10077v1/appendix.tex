% \title{Appendix for Towards Empowerment Gain through Causal Structure Learning in Model-Based RL}

\maketitle


\clearpage
\appendix
% \begin{spacing}{3}
\tableofcontents
% \end{spacing}
\clearpage

\section{Broader Impact}
\label{Broader Impact}
Our work explores leveraging causal structure to enhance empowerment for efficient policy learning, enabling better control of the environment in MBRL. We propose a framework that can effectively combine diverse causal discovery methods. This holistic approach not only refines policy learning but also ensures that the causal model remains adaptable and accurate, even when faced with novel or shifting environmental conditions. \texttt{\textbf{ECL}} demonstrates improved learning efficiency and generalization compared to other causal MBRL methods across six different RL environments, including pixel-based tasks. 
Simultaneously, \texttt{\textbf{ECL}} achieves more accurate causal relationship discovery, overcoming spurious correlation present in the environment. 

While \texttt{\textbf{ECL}} demonstrated strengths in accurate causal discovery and overcoming spurious correlation, disentangling controllable behavioral dimensions remains a limitation. Our implicit empowerment approach enhances the policy's control over the environment, but does not explicitly tease apart different behavioral axes. Explicitly disentangling controllable behavioral dimensions could be an important future work to further improve behavioral control and empowerment. 
Additionally, our current approach involves substantial data collection and model optimization efforts, which can hinder training efficiency. Moving forward, we aim to further streamline our framework to enable more efficient policy training and causal structure learning. Enhancing computational performance while maintaining accuracy will be a key focus area for future iterations of this work. In the empowerment maximization described by Eq.~\ref{eq:emp_final}, we currently omit two entropy terms. In our future work, we plan to explore additional entropy relaxation methods to further optimize this causal empowerment learning objective. 

\section{Additional Related Works}
\label{Additional Related Works}
\subsection{Model-Based Reinforcement Learning}
MBRL involves training a dynamics model by maximizing the likelihood of collected transitions, known as the world model, as well as learning a reward model~\citep{moerland2023model,janner2019trust}. Based on learned models, MBRL can execute downstream task planning~\citep{nguyen2021temporal, zhao2021consciousness}, data augmentation~\citep{pitis2022mocoda, okada2021dreaming, yu2020mopo}, and Q-value estimation~\citep{wang2022sample,amos2021model}. MBRL can easily leverage prior knowledge of dynamics, making it more effective at enhancing policy stability and generalization. 
However, when faced with high-dimensional state spaces and confounders in complex environments, the dense models learned by MBRL suffer from spurious correlations and poor generalization~\citep{wang2022causal,bharadhwaj2022information}. To tackle these issues, causal inference approaches are applied to MBRL for state abstraction, removing unrelated components~\citep{hwang2023quantized,ding2022generalizing,wang2024building}. 

\subsection{Causality in MBRL}
Due to the exclusion of irrelevant factors from the environment through causality, the application of causal inference in MBRL can effectively improve sample efficiency and generalization~\citep{ke2021systematic,mutti2023provably,liu2024learning,urpicausal}. Wang~\citep{wang2021task} proposes a regularization-based causal dynamics learning method that explicitly learns causal dependencies by regularizing the number of variables used when predicting each state variable. 
GRADER~\citep{ding2022generalizing} execute variational inference by regarding the causal graph as a latent variable. IFactor~\citep{liu2024learning} is a general framework to model four distinct categories of latent state variables, capturing various aspects of information. CDL~\citep{wang2022causal} is a causal dynamics learning method based on conditional independence testing. CDL employs conditional mutual information to compute the causal relationships between different dimensions of states and actions, thereby explicitly removing unrelated components. However, it is challenging to strike a balance between explicit causal discovery and prediction performance, and the learned policy has lower controllability over the system. In this work, we aim to actively leverage learned causal structures to achieve effective exploration of the environment through empowerment, thereby learning controllable policies that generate data to further optimize causal structures. 

\section{Notations, Assumptions and Propositions}
\label{sec:ass}
\subsection{Notations}

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.3} % Adjust row height
    \begin{tabular}{@{}llp{5cm}@{}}
        \toprule
        \textbf{Symbol} & \textbf{Description} & \textbf{Details} \\ \midrule
        % Example rows (you can delete these)
        $s^i_t$ & $i$-th state at time $t$  & -- \\
        $a_t$ & action at time $t$ & -- \\
        $r_t$ & reward at time $t$ & -- \\
        $M^{s \to s}$ & Causal masks between actions and states & Trainable parameters in Eq.~\ref{eq:cau}\\
        $M^{a \to s}$ & Causal masks between actions and states & Trainable parameters in Eq.~\ref{eq:cau}\\
        $f$ & Dynamics function  & Dynamics function of the MDPs \\
        $R$ & Reward function  & Reward function of the MDPs \\
        $\mathcal{I}$ & Mutual information  & -- \\
        $\mathcal{E}$ & Empowerment gain & -- \\
        $\phi_c$ & Parameters of dynamics model & Trainable parameters in Eq.~\ref{eq:full} \\
        $\varphi_{r}$ & Parameters of reward model & Trainable parameters in Eq.~\ref{eq:rew} \\
        $\pi_e$ & Parameters of empowerment-driven policy &  Trainable parameters in Eq.~\ref{eq:7} \\
        $\pi$ & Parameters of task policy &  Trainable parameters for task policy learning \\
        % Add more rows as needed
        \bottomrule
    \end{tabular}
    \caption{Notations used throughout the paper.}
    \label{tab:notations}
\end{table}

\subsection{Detailed objective functions}
To better illustrate the trainable parameters in each objective function, we mark the trainable ones in red as follows. 
\begin{equation}
\mathcal{L}_{\texttt{dyn}}= \mathbb{E}_{{(s_t, a_t, s_{t+1})} \sim \mathcal{D} } \left[\sum_{i=1}^{d_S} \log P_{\phi_c}(s_{t+1}^{i} | s_t, a_t; \textcolor{red}{{\phi_c}}) \right]
\end{equation}
\begin{equation}
\mathcal{L}_{\rm{c-dyn}}= \mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D} } \left[\sum_{i=1}^{d_S} \log P_{\phi_{\rm{c}}}(s_{t+1}^{i} | \textcolor{red}{M^{s\to s^j}} \odot s_t, \textcolor{red}{M^{a\to s^j}} \odot a_t; \phi_{\rm{c}}) + \mathcal{L}_{\rm{causal}} \right]
\end{equation}
\begin{equation}
    \mathcal{L}_{\rm{rew}}= \mathbb{E}_{(s_t, a_t, r_t) \sim \mathcal{D}} \left[ \mathrm{log}P_{\textcolor{red}{\varphi_{r}}} \left(r_{t} | \phi_c(s_t \mid M),a_t\right)  
    \right]
\end{equation}
\begin{equation}
    \max_{a \sim \textcolor{red}{\pi_e}(a|s)} \mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D}} \left[\mathcal{E}_{\phi_c}(s|M) - \mathcal{E}_{\phi_c}(s) \right].
\end{equation}

\subsection{Assumptions and Propositions}
% \subsection{Definitions and Proofs}
\begin{assumption}
(d-separation~\citep{pearl2009causality}) d-separation is a graphical criterion used to determine, from a given causal graph, if a set of variables X is conditionally independent of another set Y, given a third set of variables Z. 
In a directed acyclic graph (DAG) $\mathcal{G}$, a path between nodes $n_1$ and $n_m$ is said to be blocked by a set $S$ if there exists a node $n_k$, for $k = 2, \cdots, m-1$, that satisfies one of the following two conditions:

(i) $n_k \in S$, and the path between $n_{k-1}$ and $n_{k+1}$ forms ($n_{k-1} \rightarrow n_k \rightarrow n_{k+1}$), ($n_{k-1} \leftarrow n_k \leftarrow n_{k+1}$), or ($n_{k-1} \leftarrow n_k \rightarrow n_{k+1}$). 

(ii) Neither $n_k$ nor any of its descendants is in $S$, and the path between $n_{k-1}$ and $n_{k+1}$ forms ($n_{k-1} \rightarrow n_k \leftarrow n_{k+1}$). 

In a DAG, we say that two nodes $n_a$ and $n_b$ are d-separated by a third node $n_c$ if every path between nodes $n_a$ and $n_b$ is blocked by $n_c$, denoted as $n_a  \! \perp \!\!\! \perp n_b|n_c$. 
\end{assumption}

\begin{assumption}
    (Global Markov Condition~\citep{spirtes2001causation, pearl2009causality}) The state is fully observable and the dynamics is Markovian. The distribution $p$ over a set of variables $\mathcal{V}=(s^1_{t},\cdots,s^d_{t},a^1_{t},\cdots,a^d_{t},r_t)^T$ satisfies the global Markov condition on the graph if for any partition $(\mathcal{S, A, R})$ in $\mathcal{V}$ such that if $\mathcal{A}$ d-separates $\mathcal{S}$ from $\mathcal{R}$, then $p(\mathcal{S},\mathcal{R}|\mathcal{A}) = p(\mathcal{S}|\mathcal{A})\cdot p(\mathcal{R}|\mathcal{A})$
\end{assumption}

\begin{assumption}
    (Faithfulness Assumption~\citep{spirtes2001causation, pearl2009causality}) 
For a set of variables $\mathcal{V}=(s^1_{t},\cdots,s^d_{t},a^1_{t},\cdots,a^d_{t},r_t)^T$, there are no independencies between variables that are not implied by the Markovian Condition.
\end{assumption}

\begin{assumption}
Under the assumptions that the causal graph is Markov and faithful to the observations, the edge $s^i_t \to  s^i_{t+1}$ exists for all state variables $s^i$.
\end{assumption}

\begin{assumption}
No simultaneous or backward edges in time.
\end{assumption}

\textbf{Theorem 1} \textit{Based on above $5$ assumptions, we define the conditioning set $\{ a_t, s_t  \setminus  s^i_t \} = \{ a_t,s^1_t, \dots s^{i-1}_t, s^{i+1}_t, \dots \}$. If $s^i_t \not \! \perp \!\!\! \perp s^j_{t+1} | \{ a_t, s_t  \setminus  s^i_t \}$, then $s^i_t \to s^j_{t+1}$. Similarly, if $a^i_t \not \! \perp \!\!\! \perp s^j_{t+1} | \{ a_t \setminus a^i_t, s_t \}$}, then $a^i_t \to s^j_{t+1}$.

\paragraph{Proposition 1}
\textit{Under the assumptions that the causal graph is Markov and faithful to the observations, there exists an edge from $a^i_t \to s^j_{t+1}$ if and only if $a^i_t \not \! \perp \!\!\! \perp s^j_{t+1} | \{ a_t \setminus a^i_t, s_t \}$, then $a^i_t \to s^j_{t+1}$.}

\textit{Proof.} We first prove that if there exists an edge from $a^i_t$ to $s^{j}_{t+1}$, then $a^i_t \not \! \perp \!\!\! \perp s^j_{t+1} | \{ a_t \setminus a^i_t, s_t \}$. We prove it by contradiction. Suppose that $a^i_t$ is independent of $s^{j}_{t+1}$ given $ \{ a_t \setminus a^i_t, s_t \}$. According to the faithfulness assumption, we can infer this independence from the graph structure. If $a^i_t$ is independent of $s^{j}_{t+1}$ given $ \{ a_t \setminus a^i_t, s_t \}$, then there cannot be a directed path from $a^i_t$ to $s^{j}_{t+1}$ in the graph. Hence, there is no edge between $a^i_t$ and $s^{j}_{t+1}$. This contradicts our initial statement about the existence of this edge. 

Now, we prove the converse: if $a^i_t \not \! \perp \!\!\! \perp s^j_{t+1} | \{ a_t \setminus a^i_t, s_t \} $, then there exists an edge from $a^i_t$ to $s^{j}_{t+1}$. Again, we use proof by contradiction. Suppose there is no edge between $a^i_t$ and $s^{j}_{t+1}$ in the graph. Due to the Markov assumption, the lack of an edge between these variables implies their conditional independence given $\{ a_t \setminus a^i_t, s_t \}$. This contradicts our initial statement that $a^i_t \not \! \perp \!\!\! \perp  s^j_{t+1} | \{ a_t \setminus a^i_t, s_t \}$. Therefore, there must exist an edge from $a^i_t$ to $s^{j}_{t+1}$.


\paragraph{Proposition 2} 
\textit{Under the assumptions that the causal graph is Markov and faithful to the observations, there exists an edge from $s^i_t \to s^j_{t+1}$ if and only if $s^i_t \not \! \perp \!\!\! \perp s^j_{t+1} | \{a_t, s_t  \setminus s^i_t\}$.}

The proof of Proposition 2 follows a similar line of reasoning to that of Proposition 1. Consequently, the two propositions collectively serve as the foundation for deriving Theorem 1.

% \subsection{Derivation} 
% \label{Derivation}

% We provide the full derivation of empowerment in ECL as follows:
% \begin{equation}
% \setlength{\jot}{8pt}
% \begin{aligned}
%      & I(s_{t+1}; a_t | M) -  I(s_{t+1}; a_t)
%     \\
%     & =  \mathcal{H}(s_{t+1}|s_t, \phi_{\rm{cau}}) - \mathcal{H}(s_{t+1}|a_t, s_t; \phi_{\rm{cau}}) - \mathcal{H}(s_{t+1}|s_t) + \mathcal{H}(s_{t+1}|a_t, s_t; \phi_{\rm{dyn}}) 
%     \\
%     & = \underbrace{\mathcal{H}(s_{t+1}|a_t, s_t; \phi_{\rm{dyn}}) - \mathcal{H}(s_{t+1}|a_t, s_t; \phi_{\rm{cau}})}_{X} + \underbrace{\mathcal{H}(s_{t+1}|s_t, \phi_{\rm{cau}})-\mathcal{H}(s_{t+1}|s_t) }_{Y}
%     \label{eq:emp}
% \end{aligned}
% \end{equation}
% Our objective is to enhance the agent's capability to better control over the environment by leveraging causal structures. The equation $Y$ remains unaffected by the agent's behavior. Consequently, we focus our analysis on the equation $X$. This equation can be derived as follows: 
% \begin{equation}
% \setlength{\jot}{8pt}
% \begin{aligned}
%     X & = \mathbb{E}_{(s_t, a_t, s_{t+1} \sim \mathcal{D})} \left[\log P(s_{t+1} | C^{s\to s} \odot s_t, C^{a\to s} \odot a_t ; \phi_{\rm{cau}})
%     \right]
%     \\
%      &  \quad - \mathbb{E}_{(s_t, a_t, s_{t+1} \sim \mathcal{D})} \left[ -\log P(s_{t+1} | s_t, a_t; \phi_{\rm{dyn}} )
%     \right]
%     \label{eq:emp_X}
% \end{aligned}
% \end{equation}
% % \clearpage
% Our goal is to achieve higher empowerment for models based on causal structure compared to models without causal structure, thereby actively utilizing causal relationships to improve learning efficiency. In practical implementation, we employ the policy $\pi_e$ to collect transition samples, maximizing the value of Eq.~\ref{eq:emp_X}. 



\section{Details on Experimental Design and Results}
\label{Details on Experimental Design and Results}

\subsection{Experimental Environments}
\label{Experimental environments}
We select three different types environments for basic experimental evaluation, as shown in Figure~\ref{fig:abl_envs}. 

\begin{figure}[h]
\centering
\begin{subfigure}{0.34\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/envs/chemical.pdf}
\caption{Chemical}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/envs/physical.pdf}
\caption{Physical}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/envs/mani.pdf}
\caption{Manipulation}
\end{subfigure}
\hfill
\caption{Three basic experimental environments.}
\label{fig:abl_envs}
\end{figure}


\paragraph{Chemical}
In chemical environment, we aim to discover the causal relationship (Chain, Collider \& Full) of chemical items which will prove the learned dynamics and explain the behavior without spurious correlations. Meanwhile, in the downstream tasks, we evaluate the proposed methods by episodic reward and success rate. The reward function is defined as follows:

\textbf{Match:} match the object colors with goal colors individually:
\begin{equation}
    r^{\mathrm{match}}= \sum_{i=1}^{10}\mathbbm{1} [m^i_t=g^i]
\end{equation}
where $\mathbbm{1}$ is the indicator function, $m^i_t$ is the current color of the $i$-object, and $g^i$ is the goal color of the $i$-object.

\paragraph{Manipulation} In the manipulation environment, we aim to prove the learned dynamics and policy for difficult settings with spurious correlations and multi-dimension action causal influence.  The state space consists of the robot end-effector (EEF) location ($\mathbb{R}^3$), gripper (grp) joint angles ($\mathbb{R}^2$), and locations of objects and markers (6 × $\mathbb{R}^3$). The action space includes EEF location displacement ($\mathbb{R}^3$) and the degree to which the gripper is opened ([0, 1]).  In each episode, the objects and markers are reset to randomly sampled poses on the table. The task reward functions of \textbf{Reach}, \textbf{Pick} and \textbf{Stack} are followed~\citep{wang2022causal}.

% \textbf{Reach:} move the end-effector to the goal position: 
% \begin{equation}
%     r^{\mathrm{reach}}=\mathrm{tanh}(10\cdot ||eef_t-g||_{1}) 
% \end{equation}
% where $||\cdot||_{1}$ is the L1 norm, $eef_t-g$ is the current end-effector position, and $g$ is the goal position.

% \textbf{Pick:} pick the movable object to the goal position: 
% \begin{equation}
%     r^{\mathrm{pick}}=\left\{\begin{matrix}
%   0.3+0.5 \cdot(1-\frac{||mov-g||_{1}}{1.2} ) & \mathrm{grasped} \\
%   0.1\cdot (1-\frac{||mov-g||_{1}}{1.2})\cdot \mathbbm{1}  & \mathrm{otherwise}
% \end{matrix}\right.
% \end{equation}
% \textbf{Stack:} stack the movable object on the top of the unmovable object: 
% \begin{equation}
%     r^{\mathrm{stack}}=\left\{\begin{matrix}
%  0.35+1.0\cdot (1-\frac{||mov_t-g||_{1}}{1.2}) & \mathrm{grasped}  \\
%  2.0 &  \mathrm{top} \\
%  0.1 \cdot (1-\frac{||eef_t-g||_{1}}{1.2})\cdot \mathbbm{1} & otherwise
% \end{matrix}\right.
% \end{equation}

\paragraph{Physical}
In addition to the chemical and manipulation environment, we also evaluate our method in the physical environment. In a 5 × 5 grid-world, there are 5 objects and each of them has a unique weight. The state space is 10-dimensional, consisting of x, y positions (a categorical variable over 5 possible values) of all objects. At each step, the action selects one object, moves it in one of 4 directions or lets it stay at the same position (a categorical variable over 25 possible actions). During the movement, only the heavier object can push the lighter object (the object won’t move if it tries to push an object heavier than itself). Meanwhile, the object cannot move out of the grid-world nor can it push other lighter objects out of the grid-world. Moreover, the object cannot push two objects together, even when both of them are lighter than itself (Dense model mode). The task reward function is defined as follows:

\textbf{Push:} calculate the average distance between the current node and the target location:
\begin{equation}
 r^{\mathrm{match}}= \frac{1}{5} \sum_{i=1}^{5} \mathrm{dis}(o_i, t_i)
\end{equation}
where $\mathrm{dis(\cdot)}$ is the distance between two objects position. $o_i$ is the position of current node and $t_i$ is the position of target node. 

\subsubsection{Pixel-Based Environments}
Importantly, to evaluate the performance of our proposed \texttt{\textbf{ECL}} framework in latent state environments, we select three distinct categories of pixel-based environments with distractors for assessment, as shown in Figure~\ref{fig:abl_envs_pixed}. We employ IFactor~\citep{liu2024learning} as our baseline method and used its encoders to process visual inputs. Subsequently, we apply the proposed \texttt{\textbf{ECL}} framework for policy learning. The parameter settings for these three environments are kept consistent with the default configurations of IFactor. 

\begin{figure}[h]
\centering
\begin{subfigure}{0.19\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/envs/cartpole.png}
\caption{Cartpole}
\end{subfigure}
\hfill
\begin{subfigure}{0.19\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/envs/robodesk.png}
\caption{Robodesk}
\end{subfigure}
\hfill
\begin{subfigure}{0.19\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/envs/cheetah.png}
\caption{Cheetah Run}
\end{subfigure}
\hfill
\begin{subfigure}{0.19\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/envs/reacher.png}
\caption{Reacher Easy}
\end{subfigure}
\hfill
\begin{subfigure}{0.19\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/envs/walker.png}
\caption{Walker Walk}
\end{subfigure}
\hfill
\caption{3 pixel-based experimental environments with $5$ tasks.}
\label{fig:abl_envs_pixed}
\end{figure}

\paragraph{Modified Cartpole}
We select a variant of the original Cartpole environment by incorporating two distractors~\citep{liu2024learning}, as shown in Figure~\ref{fig:abl_envs_pixed}(a). The first distractor is an uncontrollable Cartpole located in the upper portion of the image, which is irrelevant to the rewards. The second distractor is a controllable but reward-irrelevant green light positioned below the reward-relevant Cartpole in the lower part of the image.

\paragraph{Robodesk}
We select a variant of Robodesk \citep{kannan2021robodesk, wang2022robodesk}, which includes realistic noise element with a dynamic video background, as shown in Figure~\ref{fig:abl_envs_pixed}(b). In this task, the objective for the agent is to change the hue of a TV screen to green using a button press, while ignoring the distractions from the video background.

\paragraph{Deep Mind Control}
We also consider variants of DMC \citep{wang2022denoised, tassa2018deepmind}, where a dynamic video background is introduced to the original DMC environment as distractor. We select cheetah Run, reacher Easy and walker Walk three specific tasks for evaluation, as shown in Figure~\ref{fig:abl_envs_pixed}(c, d, e). 

\subsection{Experimental setup}
\label{Experimental setup}

\subsubsection{Dynamics Learning Implementation Details}
We present the architectures of the proposed method across all environments in Table~\ref{tab:arch}. For all activation functions, the Rectified Linear Unit (ReLU) is employed. Additionally, we summarize the hyperparameters for causal mask learning used in all environments for \texttt{\textbf{ECL-Con}} and \texttt{\textbf{ECL-Sco}} in Table~\ref{tab:dyn}. 
Regarding the other parameter settings, we adhered to the parameter configurations established in CDL~\citep{wang2022causal} and ASR~\citep{huang2022action}. Moreover, The policy $\pi_{collect}$ is trained with a reward function $r=\tanh ( {\textstyle \sum_{j=1}^{d_{\mathcal{S}}}}\log \frac{p(s^j_{t+1}|s_t,a_t)}{p(s^j_{t+1}|\mathrm{PA}_{s^j})} )$. This reward function measures the prediction difference between the dense predictor and the current causal predictor, following the approach described in CDL~\citep{wang2022causal}. 

% The $\mathcal{L}_{\rm{causal}}$ of constraint-based causal discovery method used in \texttt{\textbf{ECL}} is:
% \begin{equation}
%       \mathcal{L}^{\mathrm {Con}}_{\rm{causal}}=\sum_{j=1}^{d_S}\left[
% \log \hat{p}(s^j_{t+1}|\{a_t,s_t \setminus  s^i_t \})  \right]
% \end{equation}

% The $\mathcal{L}_{\rm{causal}}$ of score-based causal discovery method used in \texttt{\textbf{ECL}} is:
% \begin{equation}
%       \mathcal{L}^{\mathrm {Sco}}_{\rm{causal}}= \underset{\mathcal{D}}{\mathbb{E}} \log P(s_{t+1;t+H}|s_t,a_{t:t+H-1}-\lambda_{M}||M||_{1})
% \end{equation}
% where $\mathcal{D}$ is the transition data and $\lambda_{M}$ is regularization coefficient.
% \usepackage{multirow}
 chemical, manipulation, and physical environments, we utilize well-defined feature spaces for states and actions, which are explicitly designed for causal structure learning. For pixel-based environments such as DMC, Cartpole, and RoboDesk, ECL operates on latent states extracted by visual encoders. These encoders are supported by the identifiability theory proposed in IFactor~\citep{liu2024learning}, which ensures that these latent states can effectively map to the true states. While establishing identifiability is not the primary focus of our work, we leverage IFactor's encoders and include comparisons with IFactor in our experiments. Importantly, even in these settings, we can learn meaningful causal graphs. 


\begin{table}[h]
\caption{Architecture settings in all environments.}
\label{tab:arch}
\renewcommand{\arraystretch}{1.3}
\setlength{\tabcolsep}{7pt} % 设置列间距为4pt
\centering
\begin{tabular}{cccc}
\hline
\multirow{2}{*}{\textbf{Architecture}} & \multicolumn{3}{c}{\textbf{Environments}}           \\ \cline{2-4} 
                               & \textbf{Chemical}    & \textbf{Physical}     & \textbf{Manipulation}  \\ \hline
feature dimension              & 64          & 128          & 128           \\ 
predictive networks            & {[}64,32{]} & {[}128,128{]} & {[}128,64{]} \\ 
training steps                  & 500K        & 500K          & 32M          \\ 
% training step                  & 300K        & 1.5M           & 2M          \\ 
max step of environment                   & 50        & 100           & 250          \\ 
batch size                     & \multicolumn{3}{c}{64}                     \\ 
learing rate                   & \multicolumn{3}{c}{1e-4}                   \\ 
max sample time                 & \multicolumn{3}{c}{128}                   \\ 
prediction step during training               & \multicolumn{3}{c}{2}                      \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Hyperparameters for causal mask learning in all environments.}
\label{tab:dyn}
\renewcommand{\arraystretch}{1.3}
\setlength{\tabcolsep}{7pt} % 设置列间距为4pt
\centering
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{\textbf{Method}}  & \multirow{2}{*}{\textbf{hyperparameters}}             & \multicolumn{3}{c}{\textbf{Environments}}   \\ \cline{3-5} 
                         &                                              & \textbf{Chemical} & \textbf{Physical} & \textbf{Manipulation} \\ \hline
\multirow{6}{*}{\texttt{\textbf{ECL-Con}}} & CMI threshold                                & 0.02     & 0.01     & 0.002        \\
                         & optimization frequency                       & \multicolumn{3}{c}{10}             \\
                         & evaluation frequency                         & \multicolumn{3}{c}{10}             \\
                         & evaluation batch size                        & \multicolumn{3}{c}{32}             \\
                         & evaluation step                              & \multicolumn{3}{c}{1}              \\
                         & prediction reward weight                     & \multicolumn{3}{c}{1.0}            \\ \hline
\multirow{2}{*}{\texttt{\textbf{ECL-Sco}}} & coefficient                                  & 0.002    & 0.02     & 0.001        \\
                         & regularization starts after N steps & 100K     & 100K     & 750K         \\ \hline
\end{tabular}
\end{table}

\subsubsection{Task Learning Implementation Details}
\label{sec:pixel_setting}
We list the downstream task learning architectures of the proposed method across all environments in Table~\ref{tab:task}. We outline the parameter configurations for the reward predictor, as well as the settings employed for the cross-entropy method that is applied. 
For pixel-based task learning, we leverage the four distinct categories of latent state variables by IFactor to conduct empowerment maximization for policy learning. 
Moreover, we follow the same parameter settings in IFactor, and used the same video background in all tasks. 

% \usepackage{multirow}
\begin{table}[h]
\caption{Hyperparameters for downstream task learning in all environments.}
\label{tab:task}
\renewcommand{\arraystretch}{1.3}
\setlength{\tabcolsep}{7pt} % 设置列间距为4pt
\centering
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{\textbf{Method}}           & \multirow{2}{*}{\textbf{hyperparameters}} & \multicolumn{3}{c}{\textbf{Environments}}   \\ \cline{3-5} 
                                  &                                  & \textbf{Chemical} & \textbf{Physical} & \textbf{Manipulation} \\ \hline
\multirow{4}{*}{Reward Predictor} & training steps                    & 300K     & 1.5M     & 2M           \\
                                  & optimizer                        & \multicolumn{3}{c}{Adam}           \\
                                  & learing rate                     & \multicolumn{3}{c}{3e-4}           \\
                                  & batch size                       & \multicolumn{3}{c}{32}             \\ \hline
CEM                               & number of candidates             & 64       & \multicolumn{2}{c}{128} \\
                                  & number of iterations             & 5        & \multicolumn{2}{c}{10}  \\
                                  & number of top candidates         & \multicolumn{3}{c}{32}             \\
                                  & action\_noise                    & \multicolumn{3}{c}{0.03}           \\ \hline
\end{tabular}
\end{table}

\subsection{Results of Causal Dynamics Learning}
\label{Results of causal dynamics learning}

We compare the performance of causal dynamics learning with score-based method GRADER~\citep{ding2022generalizing}, CDL~\citep{wang2022causal} and constraint-based method REG~\citep{wang2021task} across different environments. The experimental results, presented in Table ~\ref{tab:abl_causal}, reveal that although GRADER exhibits superior performance in the chemical full environment, \texttt{\textbf{ECL}}-based methods overall achieve better results than GRADER across three chemical environments. 
In the accuracy assessment metrics, \texttt{\textbf{ECL-Con}} attains 100\% precision, and across the chain and collider environments, all evaluation metrics achieve perfect 100\% scores. 
Furthermore, in the physical environment, our proposed methods attain 100\% performance. The result of rigorous evaluation metrics substantiates that incorporating \texttt{\textbf{ECL}} has boosted the dynamics model performance. 
These experimental results further validate the effectiveness of the proposed \texttt{\textbf{ECL}} approach in both sparse and dense modal environments. 

Furthermore, we analyze the prediction accuracy performance of the causal dynamics constructed by our proposed method. The multi-step (1-5 steps) prediction experimental results across four environments are illustrated in Figure~\ref{fig:abl_prediction}. \texttt{\textbf{ECL-Con}} and CDL exhibit smaller declines in accuracy as the prediction steps increase, benefiting from the causal discovery realized based on conditional mutual information. Compared to REG, \texttt{\textbf{ECL-Sco}} achieves a significant improvement in accuracy under different settings. Concurrently, we find that the outstanding out-of-distribution experimental results further corroborate the strong generalization capability of our proposed method. 
By actively leveraging the learned causal structure for empowerment-driven exploration, \texttt{\textbf{ECL}} facilitates more accurate causal discovery. 
Overall, we can demonstrate that the proposed \texttt{\textbf{ECL}} framework realizes efficient and robust causal dynamics learning.

\begin{table}[t]
\caption{Compared results of causal graph learning on three chemical and physical environments.}
\label{tab:abl_causal}
\centering
\fontsize{9.5}{9.5}
\selectfont % 字体
\renewcommand{\arraystretch}{1.3}
\setlength{\tabcolsep}{13pt} % 设置列间距为4pt
\begin{tabular}{cccccc}
\hline
\textbf{Metrics}           & \textbf{Methods} & \textbf{Chain}       & \textbf{Collider}   & \textbf{Full}  & \textbf{Physical}     \\ \hline
\multirow{3}{*}{\textbf{Accuracy}}  & \texttt{\textbf{ECL-Con}}         & \textbf{1.00±0.00} & \textbf{1.00±0.00} &  \textbf{1.00±0.00} &  \textbf{1.00±0.00} \\
                           & \texttt{\textbf{ECL-Sco}}         & 0.99±0.00  & 0.99±0.00 &  0.99±0.01 &  \textbf{1.00±0.00} \\
                           & GRADER         & 0.99±0.00 & 0.99±0.00 &  0.99±0.00&  - \\
                           \hline

                           
\multirow{3}{*}{\textbf{Recall}}    & \texttt{\textbf{ECL-Con}}         &  \textbf{1.00±0.00}  & \textbf{1.00±0.00} &  \textbf{0.97±0.00} &  \textbf{1.00±0.00} \\
                           & \texttt{\textbf{ECL-Sco}}         &  \textbf{1.00±0.00}  &  0.99±0.01 &  0.90±0.02 &  \textbf{1.00±0.00} \\ 
                            & GRADER         & 0.96±0.03 & 0.99±0.02 &  0.96±0.02 & - \\
                            \hline
\multirow{3}{*}{\textbf{Precision}} & \texttt{\textbf{ECL-Con}}         & \textbf{1.00±0.00} & \textbf{1.00±0.00} & 0.96±0.02 &  \textbf{1.00±0.00} \\
                           & \texttt{\textbf{ECL-Sco}}         & 0.99±0.01 & 0.99±0.01 &  0.97±0.03 &  \textbf{1.00±0.00} \\ 
                            & GRADER         & 0.94±0.04 & 0.90±0.05 &  \textbf{1.00±0.00} & -\\\hline
\multirow{3}{*}{\textbf{F1 Score}}  & \texttt{\textbf{ECL-Con}}         & \textbf{1.00±0.00}  & \textbf{1.00±0.00} &  0.97±0.01 &  \textbf{1.00±0.00} \\
                           & \texttt{\textbf{ECL-Sco}}        &  0.99±0.00  &  0.99±0.00 &  0.93±0.02 &  \textbf{1.00±0.00} \\ 
                            & GRADER         & 0.95±0.03 & 0.94±0.03 &  \textbf{0.98±0.01} & - \\ \hline
\multirow{3}{*}{\textbf{ROC AUC}}   & \texttt{\textbf{ECL-Con}}         &  \textbf{1.00±0.00}  & \textbf{1.00±0.00} &  \textbf{0.98±0.01} &  \textbf{1.00±0.00} \\
                           & \texttt{\textbf{ECL-Sco}}         & 0.99±0.01  &  0.99±0.01 & 0.95±0.01 &  \textbf{1.00±0.00} 
                           \\
                           & GRADER         & 0.94±0.02  &  0.99±0.01 & 0.96±0.01 & - \\ 
                           \hline
\end{tabular}
\end{table}


\begin{figure}[h]
\centering
\begin{subfigure}{0.49\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/prediction/chain.pdf}
% \captionsetup{font=scriptsize}
\caption{Chemical (Chain)}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix/prediction/collider.pdf}
% \captionsetup{font=scriptsize}
\caption{Chemical (Collider)}
\label{sub2}
\end{subfigure}
\\
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix/prediction/full.pdf}
% \captionsetup{font=scriptsize}
\caption{Chemical (Full)}
\label{sub3}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix/prediction/manipulation.pdf}
% \captionsetup{font=scriptsize}
\caption{Manipulation}
\label{sub2}
\end{subfigure}
\caption{Multi-step prediction performance for four basic environments. (\textbf{Left}) prediction on in distribution states. (\textbf{Right}) prediction on OOD states.}
\label{fig:abl_prediction}
\end{figure}


\subsection{Visualization on the Learned Causal Graphs}
\label{Visualization on the learned causal graphs}

We conduct a detailed comparative analysis by visualizing the learned causal graphs. In each causal graph, these are $d_{\mathcal{S}}$ rows and $d_{\mathcal{S}}+1$ columns, and the element at the $j$-th row and $i$-th column represents whether the variable $s_{t+1}^{j}$ depends on the variable $s^{i}_{t+1}$ if $j < d_{\mathcal{S}}+1$ or $a_t$ if $j= d_{\mathcal{S}}+1$,  measured by CMI for score-based methods and Bernoulli success probability for Reg. 
First, the causal graph learning scenario in the chemical chain environment is shown in Figure \ref{fig:abl_chain_graph}. Compared to CDL and REG, \texttt{\textbf{ECL-Con}} accurately uncovers the causal relationships among crucial elements, such as all different dimensions between states and actions, outperforming the other two methods. Moreover, we achieve extensive elimination of causality between irrelevant factors. These results demonstrate the accuracy of the proposed method in causal inference within the chemical chain environment.

Furthermore, for the chemical collider environment, the compared causal graphs are depicted in Figure~\ref{fig:abl_collider_graph}. We can observe that both CDL and \texttt{\textbf{ECL-Con}} achieved optimal discovery of causal relationships. Moreover, in contrast to the REG method, \texttt{\textbf{ECL-Con}} is not impeded by interference from irrelevant causal factors. 
For the chemical full environment, the causal graph is illustrated in Figure~\ref{fig:abl_full_graph}. Compared to CDL, \texttt{\textbf{ECL-Con}} better excludes interference from irrelevant causal factors. In comparison with the REG method, \texttt{\textbf{ECL-Con}} attains superior overall performance in discovering causal relationships. Additionally, \texttt{\textbf{ECL-Con}} reaches optimal learning performance when provided the true causal graph.

Moreover, for the manipulation environment, the experimental results are presented in Figures~\ref{fig:abl_manipulation_graph_1} and~\ref{fig:abl_manipulation_graph_2}. From the results in Figure 6, we can discern that \texttt{\textbf{ECL-Con}} achieves around 90\% overall fitting degree with the true causal graph and accurately learns the causal association between state and action. Compared to CDL shown in Figure~\ref{fig:abl_manipulation_graph_2}, \texttt{\textbf{ECL-Con}} learns more causal associations from relevant causal components related to the gripper, movable states, and actions. Conversely, in contrast to REG, \texttt{\textbf{ECL-Con}} better excludes interference from irrelevant causal factors, such as unmovable and marker states. 
In summary, the proposed method achieves more accurate and efficient learning performance in causal dynamics learning. In the subsequent section, we will delve further into analyzing the enhanced performance of \texttt{\textbf{ECL}} in optimizing causal dynamics and reward models, and how these optimizations manifest in the learning policies for downstream tasks, including complex pixel-based tasks. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{appendix/graph/chain_true.pdf}
    \includegraphics[width=0.49\linewidth]{appendix/graph/chain_ECL.pdf}
    \includegraphics[width=0.49\linewidth]{appendix/graph/chain_cdl.pdf}
    \includegraphics[width=0.49\linewidth]{appendix/graph/chain_reg.pdf}
    
    \caption{Causal graph for the chemical chain environment learned by the \texttt{\textbf{ECL}}, CDL and REG.}
    \label{fig:abl_chain_graph}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{appendix/graph/collider_true.pdf}
    \includegraphics[width=0.49\linewidth]{appendix/graph/collider_ECL.pdf}
    \includegraphics[width=0.49\linewidth]{appendix/graph/collider_cdl.pdf}
    \includegraphics[width=0.49\linewidth]{appendix/graph/collider_reg.pdf}
    
    \caption{Causal graph for the chemical collider environment learned by the \texttt{\textbf{ECL}}, CDL and REG.}
    \label{fig:abl_collider_graph}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{appendix/graph/full_true.pdf}
    \includegraphics[width=0.49\linewidth]{appendix/graph/full_ECL.pdf}
    \includegraphics[width=0.49\linewidth]{appendix/graph/full_cdl.pdf}
    \includegraphics[width=0.49\linewidth]{appendix/graph/full_reg.pdf}
    
    \caption{Causal graph for the chemical full environment learned by the \texttt{\textbf{ECL}}, CDL and REG.}
    \label{fig:abl_full_graph}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{appendix/graph/manipulation_true.pdf}
    \includegraphics[width=0.95\linewidth]{appendix/graph/manipulation_ECL.pdf}
    % \includegraphics[width=0.95\linewidth]{appendix/graph/manipulation_true.pdf}
    % \includegraphics[width=0.95\linewidth]{appendix/graph/manipulation_true.pdf}
    
    \caption{Causal graph for the manipulation environment learned by the true graph and \texttt{\textbf{ECL}}.}
    \label{fig:abl_manipulation_graph_1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{appendix/graph/manipulation_cdl.pdf}
    \includegraphics[width=0.95\linewidth]{appendix/graph/manipulation_reg.pdf}
    % \includegraphics[width=0.95\linewidth]{appendix/graph/manipulation_true.pdf}
    % \includegraphics[width=0.95\linewidth]{appendix/graph/manipulation_true.pdf}
    
    \caption{Causal graph for the manipulation environment learned by CDL and REG.}
    \label{fig:abl_manipulation_graph_2}
\end{figure}



\clearpage
\subsection{Downstream Tasks Learning}
\label{Downstream tasks learning}

As illustrated in Figures \ref{fig:abl_reward} and \ref{fig:abl_rew_other}, \texttt{\textbf{ECL-Con}} attains the highest reward across three environments when compared to dense models like GNN and MLP, as well as causal approaches such as CDL and REG. Notably, \texttt{\textbf{ECL-Con}} outperforms other methods in intricate manipulation tasks.
Furthermore, \texttt{\textbf{ECL-Sco}} surpasses REG, enhancing model performance and achieving a reward comparable to CDL.
The proposed curiosity reward encourages exploration and avoids local optimality during the policy learning process. 
Moreover, \texttt{\textbf{ECL}} excels not only in accurately uncovering causal relationships but also in enabling efficient learning for downstream tasks.

\begin{figure}[t]
% \centering
\centering
\includegraphics[width=0.27\linewidth]{paper_figs/reward/chain.pdf}
\includegraphics[width=0.27\linewidth]{paper_figs/reward/full.pdf}
\includegraphics[width=0.27\linewidth]{paper_figs/reward/collider.pdf}
\caption{The task learning of episodic reward in three environments with \texttt{\textbf{ECL-Con}} (ECL-C), \texttt{\textbf{ECL-Sco}} (ECL-S) and baselines.} 
\label{fig:abl_reward}
% \vspace{-2mm}
\end{figure}

\begin{figure}[t]
% \centering
\centering
\includegraphics[width=0.24\linewidth]{paper_figs/reward/Reach.pdf}
\includegraphics[width=0.24\linewidth]{paper_figs/reward/Pick.pdf}
\includegraphics[width=0.24\linewidth]{paper_figs/reward/Stack.pdf}
\includegraphics[width=0.24\linewidth]{paper_figs/reward/physical.pdf}
\caption{The task learning of episodic reward in three manipulation and physical environments.}
\label{fig:abl_rew_other}
% \vspace{-5mm}
\end{figure}

\paragraph{Sample efficiency analysis.}
We perform comparative analysis of downstream tasks learning across all environments. As depicted in Figure~\ref{fig:abl_reward_curve_chemical}  for experiments in three chemical environments, we can find that \texttt{\textbf{ECL-Con}} and \texttt{\textbf{ECL-Sco}} achieve outstanding performance in all three environments. Furthermore, the policy learning exhibits relative stability, reaching a steady state after approximately $400$ episodes. Additionally, Figure~\ref{fig:abl_reward_curve_other} illustrates the reward learning scenarios in the other four environments. 
Within the intricate manipulation environment, \texttt{\textbf{ECL-Con}} facilitates more expeditious policy learning. Moreover, in the dense physical environment, \texttt{\textbf{ECL-Con}} and \texttt{\textbf{ECL-Sco}} also exhibit the most expeditious learning efficiency. 
The experimental results demonstrate that the proposed methods outperform CDL. Moreover, compared to CDL, \texttt{\textbf{ECL}} enhances sample efficiency, further corroborating the effectiveness of the proposed intrinsic-motivated empowerment method. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.32\linewidth]{appendix/reward/chemical_chain.pdf}
     \includegraphics[width=0.32\linewidth]{appendix/reward/chemical_collider.pdf}
      \includegraphics[width=0.32\linewidth]{appendix/reward/chemical_full.pdf}
   \caption{The task learning curves of episodic reward in three chemical environments and the shadow is the standard error.}
\label{fig:abl_reward_curve_chemical}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.24\linewidth]{appendix/reward/reach.pdf}
     \includegraphics[width=0.24\linewidth]{appendix/reward/Pick.pdf}
      \includegraphics[width=0.24\linewidth]{appendix/reward/stack.pdf}
      \includegraphics[width=0.24\linewidth]{appendix/reward/physical.pdf}
   \caption{The task learning curves of episodic reward in four environments and the shadow is the standard error.}
\label{fig:abl_reward_curve_other}
% \vspace{-5mm}
\end{figure}

\paragraph{Causal Discovery with FCIT} 
We further conduct causal discovery using the explicit conditional independence test, specifically the Fast Conditional Independence Test (FCIT) employed in GRADER~\citep{ding2022generalizing}, for task learning evaluation. The comparative task learning results are presented in Figure~\ref{fig:rebuttal}. These findings demonstrate that \texttt{\textbf{ECL-FCIT}}, achieves improved policy learning performance than GRADER, further validating the effectiveness of our proposed learning framework \texttt{\textbf{ECL}}. 

\begin{figure}[t]
% \centering
\centering
\includegraphics[width=0.32\linewidth]{paper_figs/rebuttal/chain.pdf}
\includegraphics[width=0.32\linewidth]{paper_figs/rebuttal/collider.pdf}
\includegraphics[width=0.32\linewidth]{paper_figs/rebuttal/full.pdf}
\caption{The task learning of episodic reward in three chemical environments. \texttt{\textbf{ECL-S}} represents \texttt{\textbf{ECL}} with score-based causal discovery. \texttt{\textbf{ECL-C}} represents \texttt{\textbf{ECL}} with L1-norm regularization of constrant-based causal discovery. \texttt{\textbf{ECL-F}} represents \texttt{\textbf{ECL}} with FCIT (used in GRADER for causal discovery).}
\label{fig:rebuttal}
\end{figure}


\subsection{Pixel-Based Tasks Learning}
\label{pixel-based results}
We evaluate \texttt{\textbf{ECL}} on $5$ pixel-input tasks across $3$ latent state environments. Figure~\ref{fig:pixel_cartpole} presents comparative experimental results and visualized trajectories in the modified cartpole task. Our findings reveal that \texttt{\textbf{ECL}} achieves superior sample efficiency compared to IFactor. Furthermore, the visualized results demonstrate \texttt{\textbf{ECL}}'s effectiveness in controlling the target cartpole, successfully overcoming distractions from both the upper cartpole and the lower green light, which are not controlled in the IFactor policy. 

Moreover, we conduct evaluations on three DMC tasks. The visualized results in Figure~\ref{fig:pixel_dmc} confirm effective control for all three agents. Moreover, as shown in Figure~\ref{fig:vis_dmc}, \texttt{\textbf{ECL}} achieves more stable average return results, corroborating the enhanced controllability provided by our proposed causal empowerment approach. 
Finally, we evaluate our method against DreamerV3~\citep{hafner2023mastering}, a current state-of-the-art approach, across three DMC tasks under noiseless settings. As shown in Figure~\ref{fig:vis_dmc_dreamer}, $\texttt{\textbf{ECL}}$ consistently outperforms DreamerV3 in all $3$ tasks.

\begin{figure}[h]
    \centering

\begin{subfigure}{0.36\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/pixel/cartpole.pdf}
\caption{Average reward}
\end{subfigure}
\hfill
\begin{subfigure}{0.58\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/pixel/vis/cart_vis.pdf}
\caption{Visualization}
\end{subfigure}
\hfill
   \caption{The results of average return compared with IFactor and visualized trajectories in Modified Cartpole environment.}
\label{fig:pixel_cartpole}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\linewidth}
\includegraphics[width=1\linewidth]{appendix/pixel/vis/cheetah.pdf}
\caption{Cheetah Run}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=1\linewidth]{appendix/pixel/vis/reacher_vis.pdf}
\caption{Reacher Easy}
\end{subfigure}
\hfill
\begin{subfigure}{0.5\linewidth}
\includegraphics[width=1\linewidth]{appendix/pixel/vis/walker_vis.pdf}
\caption{Walker Walk}
\end{subfigure}
\hfill
   \caption{The results of visualization in three pixel-based tasks of DMC environment.}
\label{fig:pixel_dmc}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.32\linewidth]{appendix/pixel/rebuttal/cheetah.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/pixel/rebuttal/reacher.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/pixel/rebuttal/walker.pdf}
   \caption{The results of average return compared with IFactor in three pixel-based tasks of DMC environment under video background setting.}
\label{fig:vis_dmc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.32\linewidth]{appendix/pixel/dreamer/cheetah.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/pixel/dreamer/reacher.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/pixel/dreamer/walker.pdf}
   \caption{The results of average return compared with Dreamer in three pixel-based tasks of DMC environment under noiseless setting.}
\label{fig:vis_dmc_dreamer}
\end{figure}



\subsection{Property Analysis}
\label{Property analysis}

\paragraph{Training steps analysis.}
For property analysis, we set different training steps for causal dynamics learning of \texttt{\textbf{ECL-Con}}. As depicted in Figure \ref{fig:abl_train_step}, in the chemical chain environment, we observe that the mean prediction accuracy reaches its peak at 300k training steps. A similar trend is observed in the collider environment, where the maximum accuracy is achieved at 150k training steps. Although in the full environment, \texttt{\textbf{ECL}} attains its maximum accuracy at 600k steps, which is higher than the 500k steps used for training CDL, we notice that at 500k steps, \texttt{\textbf{ECL}} has already achieved performance comparable to CDL. These results substantiate that our proposed causal action empowerment method effectively enhances sample efficiency and dynamics performance. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\linewidth]{appendix/ablation/training_step/chain.pdf}
     \includegraphics[width=0.32\linewidth]{appendix/ablation/training_step/collider.pdf}
      \includegraphics[width=0.32\linewidth]{appendix/ablation/training_step/full.pdf}
   \caption{The mean accuracy of prediction with different training steps in chemical environments.}
\label{fig:abl_train_step}
\end{figure}

\paragraph{Hyperparameter analysis.} 
We further analyze the impact of the hyperparameter $\lambda$ introduced in the downstream task reward function with CUR. We compare four different threshold settings, and the experimental results are depicted in Figure~\ref{fig:abl_reward_abl}. From the results, we observe that when the parameter is set to $1$, the policy learning performance is optimal. When the parameter is set to $0$, the introduced curiosity cannot encourage exploratory behavior in the policy. Nonetheless, it still achieves reward performance comparable to CDL. This finding further corroborates the effectiveness of our method for dynamics learning. Conversely, when this parameter is set excessively high, it causes the policy to explore too broadly, subjecting it to increased risks, and thus more easily leading to policy divergence. Through comparative analysis, we ultimately set this parameter to $1$. In our future work, we will further optimize the improvement scheme for the reward function.

\vspace{-3mm}

\paragraph{Computation cost.} 
To consider the computation cost, we calculate the computation time for two chemical tasks of Chain, and Collider. The experimental results shown in Figure~\ref{fig:appendix_time} demonstrate that ECL achieves its performance improvements with minimal additional computational burden - specifically less than $10\%$ increase compared to CDL and REG. These results demonstrate that ECL's enhanced performance comes without significant computational cost. All experiments were conducted on the same computing platform with the same computational resources detailed in Appendix~\ref{exp}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.32\linewidth]{appendix/ablation/chain.pdf}
     \includegraphics[width=0.32\linewidth]{appendix/ablation/collider.pdf}
      \includegraphics[width=0.32\linewidth]{appendix/ablation/full.pdf}
   \caption{The episodic reward with different hyperparameter $\lambda$ in three chemical environments.}
\label{fig:abl_reward_abl}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.32\linewidth]{appendix/time/chain.pdf}
     \includegraphics[width=0.32\linewidth]{appendix/time/collider.pdf}
   \caption{The computation time in two chemical environments.}
\label{fig:appendix_time}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.32\linewidth]{appendix/ablation/reward/chain_abl.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/ablation/reward/collider_abl.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/ablation/reward/full_abl.pdf}
    \caption{Learning curves of ablation studies in three chemical environments and the shadow is the standard error. w/ represents with. w/o represents without.}
    \label{fig:abl}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.32\linewidth]{appendix/pixel/distance/cheetah.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/pixel/distance/walker.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/pixel/distance/reacher.pdf}
    \caption{Learning curves of ablation studies in three DMC tasks and the shadow is the standard error. w/ represents with.}
    \label{fig:abl_distance_emp}
\end{figure}

\subsection{Ablation Studies}
\label{Ablation Studies}

To further validate the effectiveness of the various components comprising the proposed \texttt{\textbf{ECL}} method, we designed a series of ablation experiments for verification. 
First, we implement the method without the first-stage model learning, simultaneously conducting causal model and task learning (w/ Sim) to verify the effectiveness of the proposed three-stage optimization framework. 
Second, we replace the curiosity reward introduced in the task learning with a causality motivation-driven reward (w/ Cau): $r_{\mathrm{cau}}=\mathbb{E}_{(s_t, a_t, s_{t+1} \sim \mathcal{D})}\left[{\mathbb{KL}}\left({P}_{\rm{env}}||{P}_{{\phi_c},M}\right)-{\mathbb{KL}}\left({P}_{\rm{env}}||{P}_{\phi_c}\right)\right], $ and a method without reward shaping (w/o Sha), respectively, to verify the effectiveness of incorporating the curiosity reward.

The results presented in Figure~\ref{fig:abl} clearly demonstrate the superior performance of the \texttt{\textbf{ECL}} over all other comparative approaches. \texttt{\textbf{ECL}} achieves the highest reward scores among the evaluated methods. Moreover, when compared to the method with Sim, \texttt{\textbf{ECL}} not only attains higher cumulative rewards but also exhibits greater stability in its performance during training. 
Additionally, \texttt{\textbf{ECL}} significantly outperforms the methods with Cau and method without Sha, further highlighting the efficacy of our proposed curiosity-driven exploration strategy in mitigating overfitting issues. By encouraging the agent to explore novel states and gather diverse experiences, the curiosity mechanism effectively prevents the policy from becoming overly constrained.

We explore the difference between simply maximizing empowerment under the causal dynamics model (Eq.~\ref{eq:8}) versus maximizing the difference between causal and dense model empowerment (Eq.~\ref{eq:emp_final}). Comparing \texttt{\textbf{ECL}} with empowerment (w/ Emp) against \texttt{\textbf{ECL}} with distance (w/ Dis) across three DMC tasks, our results in Figure~\ref{fig:abl_distance_emp} show that \texttt{\textbf{ECL}} w/ Dis achieves superior performance, and \texttt{\textbf{ECL}} w/ Emp also demonstrates strong learning capabilities.

Furthermore, we conducted comparative experiments between \texttt{\textbf{ECL}} and \texttt{\textbf{ECL}} without curiosity reward. The learning curves for episodic reward and success rate, shown in Figure~\ref{fig:w_o_reward}, demonstrate that the curiosity reward plays a crucial role in preventing policy overfitting during the learning process. We also carried out experiments with different values of $\lambda$. The success rate shown in Figure~\ref{fig:w_o_reward} shows the effectiveness of the curiosity reward.

In summary, \texttt{\textbf{ECL}} facilitates effective and controllable policy learning for agents operating in complex environments. The curiosity-driven reward enables the agent to acquire a comprehensive understanding of the environment while simultaneously optimizing for the desired task objectives, resulting in superior performance and improved sample efficiency.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.32\linewidth]{appendix/w_o_reward/reach_reward.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/w_o_reward/reach_success.pdf}
    \includegraphics[width=0.32\linewidth]{appendix/w_o_reward/mani_success.pdf}
    \caption{Learning curves of ECL with and without curiosity reward in manipulation reach task, and with different $\lambda$ settings. The shadow is the standard error. w/ represents with.}
    \label{fig:w_o_reward}
\end{figure}

\section{Details on the Proposed Framework}
\label{Details on the Proposed Framework}

Algorithm~\ref{alg:algorithm1} lists the full pipeline of \texttt{\textbf{ECL}} below. 

\begin{algorithm}[h]
    \caption{Empowerment through causal structure learning for model-based RL}
    \label{alg:algorithm1}
    \textbf{Input}: policy network $\pi_{e}$, $\pi_{\theta}$, transition collect policy $\pi_{\rm{collect}}$, epoch length of dynamics model training, causal empowerment and downstream task policy learning $H_{\rm{dyn}}$, $H_{\rm{emp}}$, and $H_{\rm{task}}$, evaluation frequency for causal mask learning $f_{\rm{eval}}$
    \begin{algorithmic}[]
    \begin{tcolorbox}[colback=green!10!white,colframe=green!50!black,title=Step 1: Model Learning]
     \FOR{each environment step $t$}
     \STATE Collect transitions $\{(s_{i},a_{i}, r_{i}, s'_{i})\}_{i=1}^{|\mathcal{D}_{\rm{env}}|}$ with $\pi_{\mathrm{collect}}$ from environment
     \STATE Add transitions to replay buffer $\mathcal{D}_{\rm{collect}}$
     \ENDFOR
     
      \FOR{ $epoch = 1, \cdots, H_{\rm{dyn}}$ }
      \STATE  Sample transitions $\{(s_{i},a_{i}, s'_{i})\}_{i=1}^{|\mathcal{D}_{\rm{dyn}}|}$ from $\mathcal{D}_{\rm{collect}}$
      \STATE Train dynamics model $P_{\phi_c}$ with $\{(s_{i},a_{i}, s'_{i})\}_{i=1}^{|\mathcal{D}_{\rm{dyn}}|}$ followed Eq.~\ref{eq:full}
      \IF{ $epoch $ $\%$ $f_{\rm{eval}}$ == $0$}
      \STATE Sample transitions $\{(s_{i},a_{i}, s'_{i})\}_{i=1}^{|\mathcal{D}_{\rm{cau}}|}$ from $\mathcal{D}_{\rm{collect}}$
      \STATE Learn causal dynamics model with causal mask using different causal discovery methods followed Eq.~\ref{eq:cau}
      \ENDIF
      \STATE Sample transitions $\{(s_{i},a_{i}, r_i, s'_{i})\}_{i=1}^{|\mathcal{D}_{\rm{rew}}|}$ from $\mathcal{D}_{\rm{collect}}$
      \STATE Train reward model $P_{\varphi_{r}}$ with  $\{(s_{i},a_{i}, r_i, s'_{i})\}_{i=1}^{|\mathcal{D}_{\rm{rew}}|}$ and $\phi_c(\cdot \mid M)$ followed Eq.~\ref{eq:rew}
      \ENDFOR
     \end{tcolorbox}

      \begin{tcolorbox}[colback=orange!10!white,colframe=orange!60!black,title=Step 2: Model Optimization]
       \STATE Collect transitions $\{(s_{i},a_{i}, r_i, s'_{i})\}_{i=1}^{|\mathcal{D}_{\rm{emp}}|}$ with policy $\pi_{e}$ 
      \FOR {$epoch = 1, \cdots, H_{\rm{emp}}$}
    \STATE Maximize $ \left(\mathcal{E}_{\phi_c}(s_{t+1}\mid M)-\mathcal{E}_{\phi_c} (s_{t+1}) \right) $ with transitions sampled from $\mathcal{D}_{\rm{emp}}$ for policy $\pi_{e}$ learning 
    \STATE Add transitions sampled with $\pi_{e}$ to $\mathcal{D}_{\rm{emp}}$
     \IF{ $epoch $ $\%$ $f_{\rm{eval}}$ == $0$}
      \STATE Optimize causal mask $M$ and reward model with transitions sampled from $\mathcal{D}_{\rm{emp}}$ followed Eq.~\ref{eq:cau} and Eq.~\ref{eq:rew}
      \ENDIF
    \ENDFOR
    \end{tcolorbox}

      \begin{tcolorbox}[colback=purple!10!white,colframe=purple!50!black,title=Step 3: Policy Learning]
     \FOR{$epoch = 1, \cdots, H_{\mathrm{task}}$}
     
     \STATE Collect transitions $\{(s_{i},a_{i}, r_i, s'_{i})\}_{i=1}^{|\mathcal{D}_{\rm{task}}|}$ with $\pi_{\theta}$  
     \STATE Compute predicted rewards $r_{\rm{task}}$ by learned reward predictor
     
     \STATE Calculate curiosity reward $r_{\rm{cur}}$ by Eq.~\ref{eq:cur}

     \STATE Calculate $r \gets r_{\rm{task}} + \lambda r_{\rm{cur}}$
     
     \STATE Optimize policy $\pi_{\theta}$ by the CEM planning
     \ENDFOR
     \RETURN policy $\pi_{\theta}$
     \end{tcolorbox}

    \end{algorithmic}
\end{algorithm}

% \subsection{The framework dealing with causal empowerment}

% \subsection{The framework dealing with task learning by CARE}
% \subsection{Hyper-parameter selection}

\section{Experimental Platforms and Licenses}
\label{exp}
\subsection{Platforms}
All experiments of this approach are implemented on 2 Intel(R) Xeon(R) Gold 6444Y and 4 NVIDIA RTX A6000 GPUs.

\subsection{Licenses}
In our code, we have utilized the following libraries, each covered by its respective license agreements:
\begin{itemize}
    \item PyTorch (BSD 3-Clause "New" or "Revised" License)
    \item Numpy (BSD 3-Clause "New" or "Revised" License)
    \item Tensorflow (Apache License 2.0)
    \item Robosuite (MIT License)
    \item CausalMBRL (MIT License)
    \item OpenAI Gym (MIT License)
    \item RoboDesk (Apache License 2.0)
    \item Deep Mind Control (Apache License 2.0)
\end{itemize}

\maketitle

% \bibliographystyle{unsrtnat}
% \bibliography{ref}

