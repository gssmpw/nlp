
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{url}            % simple URL \usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}     
\PassOptionsToPackage{prologue,dvipsnames}{xcolor}
% \usepackage[usenames,dvipsnames]{xcolor}
\usepackage{colortbl}
% colors
\usepackage{graphicx}
\usepackage{amsmath}
% \D\texttt{\textbf{ECL}}areMathOperator*{\argmax}{argmax} %为了使用 \argmax
% \D\texttt{\textbf{ECL}}areMathOperator*{\argmin}{argmin} %为了使用 \argmin
\usepackage{multirow}
\usepackage{subcaption} % 导入 subcaption 包
% \usepackage{natbib}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{tcolorbox}

\usepackage{bbm}
\usepackage{enumitem}
\usepackage{amssymb}

\newtheorem{definition}{\bf Definition}
\newtheorem{assumption}{\bf Assumption}  % assumptions
\newtheorem{thm}{\bf Theorem}        % theorems
\newtheorem{corollary}{\bf Corollary}
\definecolor{MyDarkRed}{rgb}{0.8,0.02,0.02}
\definecolor{royalpurple}{rgb}{0.47, 0.32, 0.66}
\colorlet{mylinkcolor}{royalpurple} %violet
\colorlet{mycitecolor}{royalpurple}
\colorlet{myurlcolor}{MyDarkRed}
\hypersetup{
  citecolor  = mycitecolor,
  linkcolor = mylinkcolor,
  urlcolor = myurlcolor,
  colorlinks = true
}
\newcommand{\mf}[1]{\textcolor{orange}{MF: #1}}
\newcommand{\fan}[1]{\textcolor{blue}{Fan: #1}}

\newcommand{\codesite}{\url{https://sites.google.com/view/ecl-1429/}}
\newenvironment{compactitemize}{\begin{itemize}[nosep,leftmargin=*]}{\end{itemize}}
\newcommand{\tianpei}[1]{{\color{red}{[tp: #1]}}}


% \usepackage{ulem}


\title{Towards Empowerment Gain through Causal Structure Learning in Model-Based RL}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Hongye Cao$^{1}$\thanks{Equal contribution. Corresponding to Jing Huo (\texttt{huojing@nju.edu.cn}).} \quad Fan Feng$^{2,3	 \ast}$ \quad Meng Fang$^{4}$ \quad Shaokang Dong$^{1}$ \quad Tianpei Yang$^{1,5}$ \\
\textbf{Jing Huo}$^{1}$ \qquad \textbf{Yang Gao}$^{1,5}$\\
$^{1}$National Key Laboratory for Novel Software Technology, Nanjing University\\
$^{2}$University of California, San Diego \quad $^{3}$MBZUAI
\quad $^{4}$University of Liverpool \\
$^{5}$School of Intelligence Science and Technology, Nanjing University\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\iclrfinalcopy
\begin{document}


\maketitle

\begin{abstract}

In Model-Based Reinforcement Learning (MBRL), incorporating causal structures into dynamics models provides agents with the structured understanding of environments, enabling more efficient and effective decisions. 
% Empowerment and causal reasoning are crucial abilities for intelligence.
Empowerment, as an intrinsic motivation, enhances the ability of agents to actively control environments by maximizing mutual information between future states and actions. 
We posit that empowerment coupled with the causal understanding of the environment can improve the agent's controllability over environments, while enhanced empowerment gain can further facilitate causal reasoning. 
To this end, we propose the framework that pioneers the integration of empowerment with causal reasoning, Empowerment through Causal Learning (\texttt{\textbf{ECL}}), where an agent with the awareness of the causal dynamics model achieves empowerment-driven exploration and optimizes its causal structure for task learning. 
Specifically, we first train a causal dynamics model of the environment based on collected data. Next, we maximize empowerment under the causal structure for exploration, simultaneously using data gathered through exploration to update the causal dynamics model, which could be more controllable than dynamics models without the causal structure. We also design an intrinsic curiosity reward to mitigate overfitting during downstream task learning. 
Importantly, \texttt{\textbf{ECL}} is method-agnostic and can integrate diverse causal discovery methods. 
We evaluate \texttt{\textbf{ECL}} combined with $3$ causal discovery methods across $6$ environments including both state-based and pixel-based tasks, demonstrating its performance gain compared to other causal MBRL methods, in terms of causal structure discovery, sample efficiency, and asymptotic performance in policy learning. The project page is \codesite. 
\end{abstract}

\section{Introduction}
% \vspace{-2mm}
% \tianpei{If the references are used in the beginning of the sentence, use citet, otherwise, use citep.}
% model-based RL and causal RL, the benefits of current causal RL works - can better describe the system and improve generalization. In this paper, we focus on another benefits of causal models - improving the controllability of the env 
Model-Based Reinforcement Learning (MBRL) uses predictive dynamics models to enhance decision-making and planning~\citep{moerland2023model}. Recent advances in integrating causal structures into MBRL have provided a more accurate description of systems, achieve better adaptation~\citep{huang2021adarl, huang2022action, feng2023learning}, generalization~\citep{pitis2022mocoda, zhang2020learning, wang2022causal, richens2024robust, lu2021invariant}, and avoiding spurious correlations~\citep{ding2022generalizing, ding2024seeing, liu2024learning, mutti2023exploiting}. 
% \fan{Will add a few sentences to explain "passive"}%In this work, we explore how \textit{actively} leveraging causal structures can enhance environmental controllability and learning efficiency and whether this, in turn, can improve the learning of the causal structure in RL environments. 

% \mf{Challenging}
However, these methods often \textit{passively} rely on pre-existing or learned causal structures for policy learning or generalization. 
In this work, we aim to enable the agent to \textit{actively} leverage causal structures, guiding more efficient exploration of the environment. The agent can then refine its causal structure through newly acquired data, resulting in improvements in both the causal model and policy. This could further enhance the agent’s controllability over the environment and its learning efficiency.

We hypothesize that agents equipped with learned causal structures will have better controllability than those using traditional dynamics models without causal modeling. This is because causal structures inform agents to explore the environment more efficiently by nulling out the irrelevant system variables. 
 This assumption serves as intrinsic motivation to guide the policy in exploring higher-quality data, which in turn improves both causal and policy learning. Specifically, we employ empowerment gain, an information-theoretic framework where agents maximize mutual information between their actions and future states to improve control~\citep{leibfried2019unified, klyubin2005empowerment, klyubin2008keep, bharadhwaj2022information, eysenbach2018diversity, mohamed2015variational}, as the intrinsic motivation to measure the agent's controllability. Concurrently, through empowerment, agents develop a more nuanced comprehension of their actions' consequences, implicitly discovering the causal relationships within their environment. Hence, by iteratively \textit{improving empowerment gain with causal structure for exploration}, \textit{refining causal structure with data gathered through the exploration}, the agent should be able to develop a robust causal model for effective policy learning. 
%Exploring how agents can \textit{actively} leverage causal structure to better explore the environment, aiming to improve controllability and learning efficiency, is a compelling challenge. 

% \tianpei{do they have some limitations so that we need employ empowerment gain, or we just do the same thing as theirs?} 
% \fan{Before this claim, we may need to say sth to explain that why passive learning is not enough, and this is the core target of this work }
%To measure the controllability and efficiency during policy learning, we can employ empowerment gain as the intrinsic motivation, encouraging agents for more effective exploration to the causal structure of the environment.
% \tianpei{Switch this sentence with the former one. The logic should be like, Empowerment is ... To better.. we propose to use empowerment gain as .. BTW, is this firstly proposed by us? If not, we should reference them. }
% Hence it can serve as an intrinsic motivation, measuring an agent’s control and efficiency in the RL environment. 

\begin{figure}[t]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{figs/fig1.pdf}}
    \caption{(a). An example of a robot manipulation task with three trajectories and three nodes: one target node (movable) and two noisy nodes (one movable, one unmovable). (b). Underlying causal structures with a factored MDP. Different nodes represent different dimensional states and actions.}
    \label{fig:fig1}
    \vspace{-6mm}
\end{figure}

%Specifically, we explore how to \textit{actively} leverage causal structures to enhance empowerment, thereby improving causal discovery and learning efficiency. 
%Causal dynamics models empower agents to explore the environment more efficiently by masking irrelevant state dimensions. 
%Agents empowered by causal structures are better positioned to control transition outcomes with precision. 
% \tianpei{this sentence also looks repeated}
%Concurrently, empowerment gain serves as an intrinsic motivation, enhancing the ability to control its environment. Through empowerment, agents develop a more nuanced comprehension of their actions' consequences, implicitly discovering the causal relationships within their environment. 

% \tianpei{what's the motivation of this example? We should highlight here.} 
We give a motivating example (Fig.\ref{fig:fig1}(a)) in a manipulation task, where the robot aims to move a target node while avoiding noisy nodes. Three possible trajectories (rows 1-3) are shown with different levels of control, efficiency, and success. 
Row 1 (irrelevant states) represents the least effective trajectory that can not control nodes and find the target, while rows 2 and 3 (controllable states) demonstrate learned control and efficiency, with high empowerment focusing on movable objects.
% \tianpei{If the caption of the figure includes the task decription, we could shorten this part.}
In the corresponding causal graphs, represented as a Dynamics Bayesian Network in Fig.~\ref{fig:fig1}(b), $s^1$, $s^2$, $s^3$ denote the states of three objects. For simplicity and clarity, we assume each object is represented by a single variable. The graph illustrates the causal relationships between these states, actions, and rewards.
Assuming the agent follows the causal structure (Fig.\ref{fig:fig1}(b)), it will likely execute actions similar to rows 2 and 3 since there are causal relationships between actions and states of movable objects, effectively improving controllability. 
Through exploration with better control, agents can facilitate improved causal discovery of the task, leading to high-reward outcomes and resulting in more efficient task completion like row 3. 

% \textcolor{red}{TODO: give a shorter version of this}
To this end, we propose an Empowerment through Causal Learning (\texttt{\textbf{ECL}}) framework that \textit{actively} leverages causal structure to maximize empowerment gain, improving controllability and learning efficiency. \texttt{\textbf{ECL}} consists of three main steps: model learning, model optimization, and policy learning. In model learning (step 1), we learn the causal dynamics model with a causal mask and a reward model. 
% \tianpei{should we mention how we get the causal mask here?}
We then integrate an empowerment-driven exploration policy with the learned causal structure to better control the environment (step 2). We alternately update the causal structure with the collected data through exploration and policy of empowerment maximization. 
Finally, the optimized causal dynamics and reward models are used to learn policies for downstream tasks with a curiosity reward to maintain robustness and prevent overfitting (step 3). Importantly, \texttt{\textbf{ECL}} is method-agnostic, being able to integrate diverse causal discovery (i.e., score-based and constraint-based) methods. 
The main contributions of this work can be summarized as follows:
\begin{compactitemize}
\item To improve controllability and learning efficiency, we propose \texttt{\textbf{ECL}}, a novel method-agnostic framework that actively leverages causal structures to boost empowerment gain, facilitating efficient exploration and causal discovery. 
\item \texttt{\textbf{ECL}} leverages causal dynamics model to conduct empowerment-based exploration. It also utilizes controllable data gathered through exploration to optimize causal structure and reward models, thereby delving deeper into the causal relationships among states, actions, and rewards. 
\item We evaluate \texttt{\textbf{ECL}} combined with $3$ causal discovery methods across $6$ environments, encompassing both In-Distribution (ID) and Out-Of-Distribution (OOD) settings, as well as pixel-based tasks. Our results demonstrate that \texttt{\textbf{ECL}} outperforms other causal MBRL methods, exhibiting superior performance in terms of causal discovery accuracy, sample efficiency, and asymptotic performance. 
\end{compactitemize}


 
% \textcolor{red}{summarize contributions here}

%These policies are optimized to exploit the causal structure, thereby enhancing the agent’s empowerment and learning efficiency in dynamic environments.



%Our main contributions of this paper are as follows. First, we propose a method-agnistic learning framework of empowerment through causal learning that actively leverages causal reasoning to enhance the empowerment of agents, facilitating efficient policy learning. 
%A causal model is constructed to capture the causal dynamics for removing unnecessary dependencies between states and actions. 
%Based on this structured causal model, we enhance the agent's empowerment to better control the environment and discovery the causal relationships more effectively. Furthermore, We evaluate \texttt{\textbf{ECL}} with two causal discovery frameworks (score-based and constraint-based) across $3$ RL environments, considering in-distribution and out-of-distribution settings for causal dynamics learning and task learning, and it outperforms other causal MBRL methods, showing remarkable performance with more accurate causal discovery, higher sample efficiency, and improved episodic rewards. 

% describe the framework: 1-2 sentences on high-level idea; 3-4 sentences on pipelines;

% summarize the contributions (optional) 


%Especially under conditions of distribution shifts, causal relationships offer an efficient, interpretable understanding of the environment and a compact adaptation mechanism. 

%This enhances generalization capabilities across diverse tasks such as multi-domain transfer, out-of-distribution (OOD) scenarios~\cite{zhang2020learning, lu2021invariant, ding2024seeing}, temporal generalization, and compositional generalization. 

\vspace{-4mm}
\section{Preliminaries}
\vspace{-2mm}
% \fan{add one more sec on causal discovery, especially those used in rl}
%\subsection{Preliminaries}
\subsection{MDP with Causal Structures}
\vspace{-2mm}
\paragraph{Markov Decision Process}
In MBRL, the interaction between the agent and the environment is formalized as a Markov Decision Process (MDP). The standard MDP is defined by the tuple $ \mathcal{M} = \langle \mathcal{S}, \mathcal{A}, T, \mu_0, r, \gamma \rangle $, where $\mathcal{S}$ denotes the state space, $\mathcal{A}$ represents the action space, $T(s' | s, a)$ is the transition dynamics model, $r(s, a)$ is the reward function, and $\mu_0$ is the distribution of the initial state $s_0$. The discount factor $\gamma \in [0, 1)$ is also included. The objective of RL is to learn a policy $\pi: \mathcal{S} \times \mathcal{A} \to [0, 1]$ that maximizes the expected discounted cumulative reward ${\eta _\mathcal{M}}(\pi) := \mathbb{E}_{s_0 \sim \mu_0, s_t \sim T, a_t \sim \pi} \left[\sum\nolimits_{t = 0}^\infty {\gamma^t}r(s_t, a_t)\right]$. 
% The value function $V_\mathcal{M}^\pi(s): = \mathbb{E}_{s_t \sim T, a_t \sim \pi} \left[\sum\nolimits_{t = 0}^\infty {\gamma^t}r(s_t, a_t) \,|\, s_0 = s\right]$ represents the expected discounted return under policy $\pi$ when starting from the state $s$.
\vspace{-2mm}
\paragraph{Structural Causal Model}
A Structural Causal Model (SCM)~\citep{pearl2009causality} is defined by a distribution over random variables $\mathcal{V}=\{s_t^1, \cdots, s_t^d, a_t^1, \cdots, a_t^n, s_{t+1}^1, \cdots, s_{t+1}^d \}$ and a Directed Acyclic Graph (DAG) $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ with a conditional distribution $P(v_i|\mathrm{PA}(v_i))$ for node $v_i \in \mathcal{V}$. Then the distribution can be specified as: 
\begin{equation}
    p(v^1, \dots, v^{|\mathcal{V}|})= \prod_{i=1}^{|\mathcal{V}|}p(v^i|\mathrm{PA}(v_i) ) ,
\end{equation}
where $\mathrm{PA}(v_i)$ is the set of parents of the node $v_i$ in the graph $\mathcal{G}$. 
\vspace{-2mm}
\paragraph{Causal Structures in MDP} 
% \fan{add one version with observation, similar to world model; one option: add a few notes in the end of this paragraph (point to the appendix, identi proof in Liu et al., 2023)}
We model a factored MDP~\citep{guestrin2003efficient, guestrin2001multiagent} with the underlying SCM between states, actions, and rewards (Fig.\ref{fig:fig1}b). In this factored MDP, nodes represent system variables (different dimensions of the state, action, and reward), while edges denote their relationships within the MDP. We employ causal discovery methods to learn the structures of $\mathcal{G}$. 
We identify the graph structures in $\mathcal{G}$, which can be represented as the causal mask $M$. Hence, the dynamics transitions and reward functions in MDP with causal structures are defined as follows:
\begin{equation}
\left\{\begin{matrix}
s^i_{t+1} = f\left( M^{s \to s} \odot s_t, M^{a \to s} \odot a_t, \epsilon_{s,i,t} \right) \\
r_t = R(\phi_c(s_t\mid M), a_t)
\end{matrix}\right.
\label{eq:gen}
\end{equation}
{where \( s^i_{t+1} \) represents the next state in dimension $i$, $ M^{s \to s} \in \{0,1\}^{|s|\times |s|}$ and $ M^{a \to s} \in \{0,1\}^{|a|\times |s|}$ are the causal masks indicating the influence of current states and actions on the next state, respectively, \( \odot \) denotes the element-wise product, and \( \epsilon_{s,i,t} \) represents i.i.d. Gaussian noise. Each entry in the causal mask $M$ (represented as the adjacency matrix of the causal graph $\mathcal{G}$) indicates the presence ($1$) or absence ($0$) of a causal relationship between elements. 
The reward \( r_t \) is a function of the state abstraction \( \phi_c(\cdot \mid M) \) under the learned causal mask $M$, which filters out the state dimensions without direct edges to the target state dimension, and the action \( a_t \). We list the assumptions and propositions in Appendix~\ref{sec:ass}. 

%\subsection{Causal Discovery in RL}

%We primarily focus on two widely used categories of causal discovery methods: score-based and constraint-based. 

%\textbf{Constraint-based:} 
%constraint-based methods leverage conditional independence to reconstruct causal information from data~\citep{spirtes2013causal,huang2022action,wang2021task}. Under the assumptions of causal Markov condition and faithfulness, a correspondence can be established between causal graph structures and statistical independence. This allows for learning causal structures by determining conditional independence relationships among observed variables.

%\textbf{Score-based:} 
%To relax the causal faithfulness assumption, score-based methods operate under the causal sufficiency assumption. These methods perform greedy heuristic searches in the space of directed acyclic graphs~\citep{chickering2002optimal,wang2022causal,ding2022generalizing}. By optimizing a causal graph score, they aim to find the graph structure that best matches the observed data. 

\vspace{-2mm}
\subsection{Empowerment}
\vspace{-2mm}
Empowerment is to quantify the influence an agent has over its environment and the extent to which this influence can be perceived by the agent~\citep{klyubin2005empowerment,salge2014empowerment,jung2011empowerment}. Within our framework, the empowerment is the mutual information between the agent action ${a}_t$ and its subsequent state ${s}_{t+1}$ under the causal mask $M$ as follows: 
\begin{equation}
    \mathcal{E} := \max_{\pi(\cdot|s_t)} \mathcal{I}(s_{t+1};a_{t} \mid M),
\end{equation}
where $\mathcal{E}$ is used to represent the channel capacity from the action to state observation. $\pi(\cdot|s_t)$ is the conditional distribution of actions given states. 
%\textcolor{red}{We aim to enhance the empowerment gain under the causal structure of the environment for improving controllability.}
% We aim to enhance the empowerment gain under the causal understanding of the agent to the environment for improving controllability and causal reasoning. 
\vspace{-3mm}
\section{Empowerment through Causal Learning} 
\vspace{-2mm}
\label{sec:ECL}
% \subsection{Overview}
\begin{figure}[h]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{paper_figs/pipeline.pdf}}
    \caption{The framework overview of \texttt{\textbf{\texttt{\textbf{ECL}}}}. Gold lines: model learning. Blue lines: model optimization alternating with empowerment-driven exploration (yellow lines). Green lines: policy learning.}
    \label{fig:framework}
    \vspace{-3mm}
\end{figure}

% \textcolor{red}{Section 3 has four problems:
% 1. clarify dense model, causal dynamics model, and dynamics model. 
% 2. how to update dynamics model and causal dynamics model. 
% 3. Eq.7 to 10 is confusing. 
% 4. what is the relationship between $r_{cur}$ with empowerment?}

An illustration of the \texttt{\textbf{ECL}} framework is shown in Fig.~\ref{fig:framework}, comprising three main steps: model learning, model optimization, and policy learning. In model learning \textbf{(step 1)}, we learn causal dynamics model with the causal mask and reward model. This causal dynamics model is trained using collected data to identify causal structures (i.e., causal masks $M$) , by maximizing the likelihood of observed trajectories. The reward model is trained based on state abstraction that masks irrelevant state dimensions with the causal structure. 
% with\tianpei{unclear, based on the state abstraction generated by the causal mask and the orignial state?} causal mask and action. 
With the learned causal structure, we integrate empowerment-driven exploration for model optimization \textbf{(step 2)}. This process involves learning the empowerment policy $\pi_e$ that enhances the agent's controllability by actively leveraging the causal mask. We alternately update the policy $\pi_e$ for empowerment maximization and generate data with $\pi_e$ to optimize the causal mask $M$ and reward model $P_{\varphi_{\rm_{r}}}$. Finally, in \textbf{step 3}, the learned causal dynamics and reward models are used to learn policies for the downstream tasks. In addition to the task reward, to maintain robustness and prevent overfitting, an intrinsic curiosity reward is incorporated to balance the causality. 
% \tianpei{so the intrinsic reward is proposed to maintain robustness, prevent overfitting and balance the causality? Why should we balance the causality? Also, you didn't mention the reward can improve robustness in intro and abs.}. 
\vspace{-2mm}
\subsection{Step 1: Model Learning with Causal Discovery}
\label{sub:step1}
\vspace{-2mm}

We first learn causal dynamics model with the causal mask and reward model for the empowerment and downstream task learning. Specifically, a dynamics encoder is trained by maximizing the likelihood of observed trajectories $\mathcal{D}$. Then, the causal mask is learned based on the dynamics {model} and a reward model is trained with the state abstraction under the causal mask and action. 
% and a reward model . \tianpei{Specifically, should contain more information, here you only have one sentence. Maybe we could say: Sepcifically, the causal dynamics model is learned by maximzing ... as follows:}
\vspace{-2mm}
\paragraph{Causal Dynamics Model} 
The causal dynamics model is composed with a dynamics model $P_{\phi_c}$ and a causal mask $M$. The dynamics model maximizes the likelihood of observed trajectories $\mathcal{D}$ as follows:
\begin{equation}
\mathcal{L}_{\texttt{dyn}}= \mathbb{E}_{{(s_t, a_t, s_{t+1})} \sim \mathcal{D} } \left[\sum_{i=1}^{d_S} \log P_{\phi_c}(s_{t+1}^{i} | s_t, a_t; {\phi_c}) \right],
\label{eq:full}
\end{equation}
where \( d_S \) is the dimension of the state space, and \( \phi_c \) denotes the parameters of the dynamics model. We train the dynamics model as a dense dynamics model that incorporates all state dimensions to capture the state transitions within the environment, facilitating subsequent causal discovery and empowerment. Additionally, we assess the performance of the dense model, specifically the baseline MLP, within the experimental evaluations detailed in Section \ref{sec:exp}. 
Next, we use this learned dynamics model for causal discovery. 
% \tianpei{Then, we ... we need to connect these paragraphs.}
\vspace{-2mm}
\paragraph{Causal Discovery} For causal discovery, with the learned dynamics model \( P_{\phi_{c}} \), we further embed the causal masks $M^{s\to s}$ and $M^{a\to s}$ into the learning objective. To learn the causal mask, we employ both conditional independence testing (\textit{constraint-based})~\citep{wang2022causal} and mask learning by sparse regularization (\textit{score-based})~\citep{huang2022action}. We further maximize the likelihood of states by updating the dynamics model and learned masks. Thus, the learning objective for the causal dynamics model is as follows: 
\begin{equation}
\mathcal{L}_{\rm{c-dyn}}= \mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D} } \left[\sum_{i=1}^{d_S} \log P_{\phi_{\rm{c}}}(s_{t+1}^{i} | {M^{s\to s^j}} \odot s_t, {M^{a\to s^j}} \odot a_t; \phi_{\rm{c}}) + \mathcal{L}_{\rm{causal}} \right],
\label{eq:cau}
\end{equation}
where \( \mathcal{L}_{\rm{causal}} \) represents the objective term associated with learning the causal structure.
% \footnote{Detailed loss functions are given in Appendix~\ref{Experimental setup}}. 
$ \mathcal{L}^{\mathrm {Con}}_{\rm{causal}}=\sum_{j=1}^{d_S}\left[
\log \hat{p}(s^j_{t+1}|\{a_t,s_t \setminus  s^i_t \})  \right]$ and $\mathcal{L}^{\mathrm {Sco}}_{\rm{causal}}= -\lambda_{M}||M||_{1}$ represent constraint-based and score-based objectives respectively. $\lambda_{M}$ is regularization coefficient.
% where $\phi_{\rm{dyn}}$ is used to approximate the predictive model. $\mathcal{D}$ is the collected transition data. $d_S$ is the dimension of the state space. 
% \paragraph{Causal mask.} 
%Another learning objective is to execute the causal structure learning by maximizing the likelihood of the dynamics model with causal mask $M$ as:
\iffalse
\begin{equation}
\label{eq:cau}
    \mathcal{L}_{\rm{cau}} = \mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D} } \left[ \sum_{i=1}^{d_S}\mathrm{log} P_{\phi_{\rm{c}}}(s^j_{t+1}|\textcolor{red}{C^{s\to s^j}} \odot s_t, \textcolor{red}{C^{a\to s^j}} \odot a_t ; \phi_{\rm{c}}) \right],
\end{equation}
where $\phi_{\rm{cau}}$ is used to identify the causal structure of $\mathcal{G}$ by predicting the binary masks in Eq.~\ref{eq:gen}.
\fi 
\vspace{-2mm}
\paragraph{Reward Model}
After obtaining the causal dynamics model, we process states using the causal mask $M$ to derive state abstractions $\phi_c(\cdot \mid M)$ for the reward model learning, effectively filtering out irrelevant state dimensions. Simultaneously, the reward model $P_{\varphi_{\rm_{r}}}$ maximizes the likelihood of observed rewards sampled from trajectories $D$:
\begin{equation}
\label{eq:rew}
    \mathcal{L}_{\rm{rew}}= \mathbb{E}_{(s_t, a_t, r_t) \sim \mathcal{D}} \left[ \mathrm{log}P_{\varphi_{r}} \left(r_{t} | \phi_c(s_t \mid M),a_t\right)  
    \right].
\end{equation}
In this way, \texttt{\textbf{ECL}} leverages causal understanding to enhance both state representation and reward prediction accuracy. 
Finally, the overall objective of the model learning with the causal structure is to maximize $\mathcal{L} = \mathcal{L}_{\rm{dyn}} + \mathcal{L}_{\rm{c-dyn}} + \mathcal{L}_{\rm{rew}}$.

\vspace{-3mm}
\subsection{Step 2: Model Optimization with Empowerment-Driven Exploration}
\label{sub:step2}
\vspace{-5pt}
In Step 2, we optimize the learning of the causal structure and empowerment. As depicted in Fig.~\ref{fig:framework}, this procedure alternates between optimizing the empowerment-driven exploration policy $\pi_e$ and update the causal mask $M$ using data gathered through exploration. Furthermore, to ensure the stability, we update the reward model to adapt to changes in state abstraction induced by updates to the causal mask $M$. Note that the dynamics model $P_{\phi_c}$ learned in Step 1 remains fixed, allowing for a focused optimization of both the causal structure and the empowerment in an alternating manner. The causal structure is optimized by the causal mask M through maximizing  $\mathcal{L}_{causal}$ (Eq.~\ref{eq:cau}), while keeping the parameters of $\phi_c$ fixed during this learning step.

\vspace{-2mm}
\paragraph{Empowerment-driven Exploration} To enhance the agent's control and efficiency given the causal structure, instead of maximizing $\mathcal{I}\left(s_{t+1}, a_t | s_t\right)$ at each step, we consider a baseline that uses the dense dynamics model $P_{\phi_c}$ without the causal mask $M$. We then prioritize causal information by maximizing the difference in empowerment gain between the causal and dense dynamics models. 
% \tianpei{how to optimize? maximize the distance?}

We first denote the empowerment gain of the causal dynamics model and dense dynamics model as $\mathcal{E}_{\phi_c}(s|M) = \max_a  \mathcal{I}\left(s_{t+1}; a_t \mid s_t; \phi_c, M\right)$ and $\mathcal{E}_{\phi_c}(s) = \max_a  \mathcal{I}\left(s_{t+1}; a_t \mid s_t; \phi_c \right)$, respectively. Here, $\mathcal{E}_{\phi_c}(s)$ corresponds to the dynamics model without considering causal structures. 
% For this purpose, we separately train a well-tuned $\phi$ on offline data to serve as a baseline for optimization. 

Then, we have the following learning objective:
% \tianpei{what? optimize the distance?}
\begin{equation}
    \max_{a \sim \pi_e(a|s)} \mathbb{E}_{(s, a, s') \sim \mathcal{D}} \left[\mathcal{E}_{\phi_c}(s|M) - \mathcal{E}_{\phi_c}(s) \right].
\label{eq:7}
\end{equation}
In practice, we employ the estimated $\hat{\mathcal{E}}_{\phi_c}(s\mid M)$ and $\hat{\mathcal{E}}_{\phi_c}(s)$ with the policy $\pi_e$ for computing, specifically:
\begin{equation}
     \hat{\mathcal{E}}_{\phi_c}(s_t|M) = \max_{a \sim \pi_e(a|s)} \mathbb{E}_{\pi_e(a_t|s_t) p_{\phi_c}(s_{t+1}|s_t, a_t,M)} \left[\log P_{\phi_c}(s_{t+1} \mid s_t, a_t; M, \phi_c) - \log P(s_{t+1}|s_t) \right],
\label{eq:8}
\end{equation}
and: 
\begin{equation}
     \hat{\mathcal{E}}_{\phi_c}(s_t) = \max_{a \sim \pi_e(a|s)} \mathbb{E}_{\pi_e(a_t|s_t) p_{\phi_c}(s_{t+1}|s_t, a_t)} \left[\log P_{\phi_c}(s_{t+1} \mid s_t, a_t; \phi_c) - \log P(s_{t+1}|s_t) \right],
\end{equation}
where $P(s_{t+1}|s_t)$ is the conditional distribution of the current state. Hence, the objective function Eq.~\ref{eq:7} is derived as:
\begin{equation}
    \max_{a \sim \pi_e(a|s)} \mathcal{H}(s_{t+1} \mid s_t;M) - \mathcal{H}(s_{t+1} \mid s_t) + \mathbb{E}_{a \sim \pi_e(a|s)} \left[\mathbb{KL} \left(P_{\phi_c}(s_{t+1} \mid s_t, a_t; M) \| P_{\phi_c}(s_{t+1} \mid s_t, a_t) \right) \right], 
    \label{eq:emp_final}
\end{equation}
where $\mathcal{H}(s_{t+1} \mid s_t;M)$ and $\mathcal{H}(s_{t+1} \mid s_t)$ denote the entropy at time $t+1$ under the causal dynamics model and dense dynamics model, respectively. For simplicity, we update $\pi_e$ by optimizing the KL term. 

\paragraph{Model Optimization} In Step 2, we fix the dynamics model $P_{\phi_c}$ and further fine-tune the causal mask $M$ and the reward model $P_{\varphi_r}$. We adopt an alternating optimization with the policy $\pi_e$ to optimize the causal mask. Specifically, given $M$, we first optimize $\pi_e$. The policy $\pi_e$ is designed to collect controllable trajectories by maximizing the distance of empowerment between causal and dense models. These collected trajectories are then used to optimize both the causal structure $M$ and reward model $P_{\varphi_r}$.
% Then, we use data gathered through empowerment-driven exploration to update $M$ and . 

\subsection{Step 3: Policy Learning with Curiosity Reward}
\label{sub:step3}

We learn the downstream task policy based on the optimized causal structure. To mitigate potential overfitting of the causality learned in Steps 1\&2, we incorporate a curiosity-based reward as an intrinsic motivation objective or exploration bonus, in conjunction with a task-specific reward, to prevent overfitting during task learning: 
\vspace{-2mm}
\begin{equation}
\begin{aligned}
r_{\mathrm{cur}}(s,a) = \mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D}} \Bigg[
\mathbb{KL}\Big(P_{\rm{env}}{(s_{t+1}|s_t, a_t)} \,\Big\|\, P_{\phi_c, M}(s_{t+1}|s_t, a_t; \phi_c, M)\Big) \\
- \mathbb{KL}\Big(P_{\rm{env}}{(s_{t+1}|s_t, a_t)} \,\Big\|\, P_{\phi_c}(s_{t+1}|s_t, a_t; \phi_c)\Big) \Bigg]
\end{aligned}
\label{eq:cur}
\end{equation}

\iffalse
\begin{equation}
\begin{aligned}
    r_{\mathrm{cur}} & =\mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D}}\left[{\mathbb{KL}}\left({P}_{\rm{env}}||{P}_{{\phi_c},M}\right)-{\mathbb{KL}}\left({P}_{\rm{env}}||{P}_{\phi_c}\right)\right],
    % r_{\mathrm{cur}}=\mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D}}\left[ & {\mathbb{KL}}\left({P}_{\rm{env}}(s_{t+1}|s_t, a_t)||{P}_{{\phi_c},M}(s_{t+1}|s_t, a_t;\phi_c, M)\right) \right.
    % \\
    % & \left. -{\mathbb{KL}}\left({P}_{\rm{env}}(s_{t+1}|s_t, a_t)||{P}_{\phi_c}(s_{t+1}|s_t, a_t;\phi_c)\right)\right],
    % \\
    % & =\mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D}}\left[{\mathbb{KL}}\left({P}_{\rm{env}}(s_{t+1}|s_t, a_t)||{P}_{{\phi_c},M}(s_{t+1}|s_t, a_t;\phi_c, M)\right)-{\mathbb{KL}}\left({P}_{\rm{env}}(s_{t+1}|s_t, a_t)||{P}_{\phi_c}(s_{t+1}|s_t, a_t;\phi_c)\right)\right]
\end{aligned}
\label{eq:cur}
\end{equation}
\fi 
where ${P}_{\rm{env}}$ is the ground truth dynamics collected from the environment.  By taking account of $r_{\mathrm{cur}}$, we encourage the agent to explore states that the causal dynamics cannot capture but the dense dynamics can from the true environment dynamics, thus preventing the policy from being overly conservative due to model learning with trajectories. Hence, the shaped reward function is: 
\begin{equation}
    r(s,a)=r_{\mathrm{task}}(s,a)+\lambda r_{\mathrm{cur}}(s,a),
\label{eq:shaped_rew}
\end{equation}
where $r_{\rm{task}}(s,a)$ is the task reward, $\lambda$ is a balancing hyperparameter. In section~\ref{Ablation Studies}, we conduct ablation experiments to thoroughly analyze the impact of different shaped rewards, including curiosity, causality and original task rewards.   


\iffalse
where $\mathcal{P}_S^{\rm{cau}}$ is the prediction distribution of causal dynamics, $\mathcal{P}_S^{\rm{true}}$ is the distribution of the groundtruth, and $\mathcal{P}_S^{\rm{full}}$ is the prediction distribution of full dense dynamics. Our approach incentivizes the policy to take causal actions that diverge from the original strategy, thereby fostering broader exploration. 
Hence, the overall objective of the downstream task learning is to learn a policy $\pi_{\theta}$ that maximizes the expected discounted cumulative reward ${\eta _\mathcal{\hat{M}}}(\pi_{\theta}) := \mathbb{E}_{s_0 \sim \mu_0, s_t \sim T, a_t \sim \pi_{\theta}} \left[\sum\nolimits_{t = 0}^\infty {\gamma^t}(r_{\mathrm{task}}(s,a)+\lambda r_{\mathrm{cur}}(s,a))\right]$.
\fi 
\vspace{-4mm}
\section{Practical Implementation}
\vspace{-3mm}
We introduce the practical implementation of \texttt{\textbf{ECL}} for casual dynamics learning with empowerment-driven exploration and task learning. 
The proposed framework for the entire learning process is illustrated in Figure~\ref{fig:framework}, comprising three steps and the full pipeline is listed in Algorithm~\ref{alg:algorithm1}.
\iffalse
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{paper_figs/alg_1.pdf}
    \hspace{1mm}
    \includegraphics[width=0.52\linewidth]{paper_figs/framework.pdf}
    \caption{The proposed method of learning procedure and framework}
    \label{fig:framework}
\end{figure}
\fi

% \textbf{Step 1: Model Learning}\quad
\vspace{-3mm}
\paragraph{Step 1: Model Learning}
Initially, following \citep{wang2022causal}, we use a transition collection policy $\pi_{\text{collect}}$ by formulating a reward function that incentivizes selecting transitions that cover more state-action pairs to expose causal relationships thoroughly. 
We train the dynamics model $P_{\phi_c}$ by maximizing the log-likelihood $\mathcal{L}_{\rm{dyn}}$, following Eq.~\ref{eq:full}. 
Then, we employ the causal discovery approach for learning causal mask $M$ by maximizing the log-likelihood $\mathcal{L}_{\rm{c-dyn}}$ followed Eq.~\ref{eq:cau}. 
Subsequently, we train the reward model $P_{\varphi_{\rm{r}}}$ with the state abstraction $\phi_c(s\mid M)$ by maximizing the likelihood.
\vspace{-3mm}
\paragraph{Step 2: Model Optimization} We execute the empowerment-driven exploration by $\max_{a \sim \pi_e(a|s)} \mathbb{E}_{s_t, a_t, s_{t+1} \sim \mathcal{D}} \left[\mathcal{E}_{\phi_c}(s|M) - \mathcal{E}_{\phi_c}(s) \right]$ followed Eq.~\ref{eq:7} with causal dynamics model and dense dynamics model for policy $\pi_{e}$ learning. Furthermore, the learned policy $\pi_{e}$ is used to sample transitions for updating casual mask $M$ and reward model. We alternately perform empowerment-driven exploration for policy learning and causal model optimization. 
% This process actively leverages causal representations and structures to increase their control over the environment and more efficiently discover causal relationships. 

% \textcolor{red}{We explicitly conduct state abstraction with causal discovery to eliminate irrelevant state components and prioritize controllable ones, driven by the concept of empowerment through causal structure learning. We maximize $\hat{I}(s_{t+1}; a_t)$ outlined in Eq.~\ref{eq:emp} for policy optimization. Subsequently, we improve the controllability of the environment by maximizing $I(s_{t+1}; a_t)$ to update policy $\pi_{\theta}$ and 
% the dynamics model with causal mask $\phi_{\rm{cau}}$.}
\vspace{-3mm}
\paragraph{Step 3: Policy Learning} During downstream task learning, we incorporate the causal effects of different actions as curiosity rewards combined with the task reward, following Eq.~\ref{eq:shaped_rew}. We maximize the discounted cumulative reward to learn the policy by the cross entropy method (CEM)~\citep{rubinstein1997optimization}. Specifically, The causal model is used to execute dynamic state transitions defined in Eq.~\ref{eq:gen}. The reward model evaluates these transitions and provides feedback in the form of rewards. The CEM handles the planning process by leveraging the predictions from the causal and reward models to optimize the task's objectives effectively.


% Additionally, the policy $\pi_{\theta}$ is utilized to sample transitions to the replay buffer $\mathcal{D}_{\rm{buffer}}$ for model optimization and causal empowerment. 

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{figs/balance.pdf}
%     \caption{Exploration of diversity and causality.}
%     \label{fig:balance}
% \end{figure}

% \subsection{Overall Objective of \texttt{\textbf{ECL}}}
% % \textcolor{red}{Intrinsic-motivated causal dynamics empowerment + Causal action Reward empowerment.} 
% We now motivate the overall learning objective of \texttt{\textbf{ECL}}, which consists of maximizing two terms: the causal dynamics learning and the downstream task learning empowerment terms.

% \paragraph{Causal dynamics learning objective.} The causal dynamics learning aims to prioritize the encoding of forward-predictive, reward-predictive, and directable causal components. Therefore, we define the overall objective for causal dynamics learning as follows:

% \begin{equation}
%     \max_{s_{1:H}}\sum_{t=1}^{H} \left( I(s_{t+1}|s_t, a_t; \phi_{\rm{dyn}})+ I(s_{t+1}|s_t, a_t; \phi_{\rm{cau}}) + I(r_{t}|s_t, a_t; \phi_{\rm{return}})
% + I(r_{t}; s_t ) \right).
% \end{equation}


% \paragraph{Task learning objective.} The task learning objective aims to prioritize the causal action empowerment term and reward-based value term. We define the overall objective for downstream task learning over the horizon $H$ as follows:

% \begin{equation}
%     \max_{s_{1:H}}\sum_{t=1}^{H} \left( I(s_{t+1}|s_t, a_t; \theta_{\rm{cau}}) - I(s_{t+1}|s_t, a_t; \theta_{\rm{full}})\right)
% + V_{\theta}(s),
% \end{equation}
% where $V_{\theta}(s) = \mathbb{E}_{s_t\sim T_{\theta},a_t\sim \pi_{\theta }}\left[ 
%  {\textstyle \sum_{t=1}^{H}} \gamma^tr(s_t, a_t)|s_1=s
% \right] $.


% \clearpage
\vspace{-4mm}
\section{Experiments}
\label{sec:exp}
\vspace{-3mm}
% \fan{to be discussed: 1. different causal discovery approaches (todo); 2. effect of different strategies for collecting the offline data; 3. the optimization process}
We aim to answer the following questions in experimental evaluation: 
(i) How does the performance of \texttt{\textbf{ECL}} compare to other causal and dense models across different environments for tasks and dynamics learning, including pixel-based tasks?
(ii) Does \texttt{\textbf{ECL}} improve causal discovery by eliminating more irrelevant state dimensions interference, thereby enhancing learning efficiency and generalization towards the empowerment gain? 
(iii) Whether different causal discovery methods in step 1 and 2, impact policy performance? What are the effects when combine the step 1 and 2?
(iv) What are the effects of the components and hyperparameters in \texttt{\textbf{ECL}}? 
\vspace{-2mm}
\subsection{Setup}
\vspace{-2mm}
\paragraph{Environments.} We select 3 different environments for basic experimental evaluation.
\textbf{Chemical~\citep{ke2021systematic}:} The task is to discover the causal relationship (Chain, Collider \& Full) of chemical items 
which proves the learned dynamics and explains the behavior without spurious correlations. 
\textbf{Manipulation~\citep{wang2022causal}:} The task is to prove dynamics and policy for difficult settings with spurious correlations and multi-dimension action causal influence. 
% In each episode, the objects and markers are reset to randomly sampled poses on the table.
\textbf{Physical~\citep{ke2021systematic}:} a dense mode Physical environment. 
Furthermore, we also include $3$ pixel-based environments of \textbf{Modified Cartpole~\citep{liu2024learning}, Robedesk~\citep{wang2022denoised}} and \textbf{Deep Mind Control (DMC)~\citep{wang2022denoised}} for evaluation in latent state environments. 
For the details of the environment setup, please refer to Appendix~\ref{Experimental setup}. 
\vspace{-2mm}
\paragraph{Baselines.} We compare \texttt{\textbf{ECL}} with $4$ causal and 2 standard MBRL methods. 
\textbf{CDL}~\citep{wang2022causal}: infers causal relationships between the variables for dynamics learning with Conditional Independence Test (CIT) of constraint-based causal discovery. \textbf{REG}~\citep{wang2021task}: Action-sufficient state representation based on regularization of score-based causal discovery. \textbf{GRADER}~\citep{ding2022generalizing}: generalizing goal-conditioned RL with CIT by variational causal reasoning. 
\textbf{IFactor}~\citep{liu2024learning}: a causal framework to model four distinct categories of latent state variables within the RL system for pixel-based environments. 
\textbf{GNN}~\citep{ke2021systematic}: a graph neural network with dense dependence for each state variable. 
\textbf{Monolithic}~\citep{wang2022causal}: a Multi-Layer Perceptron (MLP) network that takes all state variables and actions for prediction. For \texttt{\textbf{ECL}}, we employ both conditional independence testing (constraint-based (\texttt{\textbf{ECL-Con}})) used in~\citep{wang2022causal} and mask learning by sparse regularization (score-based (\texttt{\textbf{ECL-Sco}})) used in~\citep{huang2022action}. We also combine IFactor~\citep{liu2024learning} for pixel-based tasks learning detailed in Appendix~\ref{sec:pixel_setting}. 

% \clearpage
\begin{figure}[t]
% \centering
\centering
\includegraphics[width=0.24\linewidth]{paper_figs/reward/chain.pdf}
\includegraphics[width=0.24\linewidth]{paper_figs/reward/full.pdf}
\includegraphics[width=0.24\linewidth]{paper_figs/reward/Stack.pdf}
\includegraphics[width=0.24\linewidth]{paper_figs/reward/physical.pdf}
\caption{The task learning of episodic reward in three environments of \texttt{\textbf{ECL-Con}} (\texttt{\textbf{ECL-C}}) and \texttt{\textbf{ECL-Sco}} (\texttt{\textbf{ECL-S}}).}
\label{fig:reward}
\end{figure}

\begin{figure}[t]
% \centering
\centering
\includegraphics[width=0.325\linewidth]{paper_figs/reward/curves/chemical_full.pdf}
\includegraphics[width=0.325\linewidth]{paper_figs/reward/curves/physical.pdf}
\includegraphics[width=0.325\linewidth]{paper_figs/reward/curves/Pick.pdf}
\caption{The learning curves of episodic reward in three different environments and the shadow is the standard error.}
\label{fig:reward_curves}
\end{figure}
\vspace{-2mm}
\paragraph{Evaluation Metrics.} In tasks learning, we utilize episodic reward and task success as evaluation criteria for downstream tasks. For causal dynamics learning, we employ five metrics to evaluate the learned causal graph and assess the mean accuracy for dynamics predictions of future states in both ID and OOD. For pixel-based tasks, we use average return and visualization results for evaluation\footnote{ We conduct each experiment using 4 random seeds.}.
% \vspace{-5pt}
\vspace{-2mm}
\subsection{Results}
\vspace{-2mm}
% \vspace{-5pt}
\subsubsection{Task Learning}
\vspace{-2mm}
We evaluate each method with the following $7$ downstream tasks in the chemical (C), physical (P) and the manipulation (M) environments. 
\textbf{Match} (C): match the object colors with goal colors individually. 
\textbf{Push} (P): use the heavier object to push the lighter object to the goal position. 
\textbf{Reach} (M): move the end-effector to the goal position. 
\textbf{Pick} (M): pick the movable object to the goal position. 
\textbf{Stack} (M):  stack the movable object on the top of the unmovable object.

As shown in Fig.~\ref{fig:reward}, compared to dense dynamics models GNN and MLP, as well as the causal approaches CDL and REG, \texttt{\textbf{ECL-Con}} attains the highest reward across $3$ environments. Notably, \texttt{\textbf{ECL-Con}} outperforms other methods in the intricate manipulation tasks. 
Furthermore, \texttt{\textbf{ECL-Sco}} surpasses REG, elevating model performance and achieving a reward comparable to CDL. 
The proposed curiosity reward encourages exploration and avoids local optimality during the policy learning process. For full results, please refer to Appendix~\ref{Downstream tasks learning}.


Additionally, Figure~\ref{fig:reward_curves} depicts the learning curves across three environments. Across these diverse settings, \texttt{\textbf{ECL}} exhibits elevated sample efficiency compared to CDL and higher reward attainment. The introduction of curiosity reward bonus enables efficient exploration of strategies, thus averting the risk of falling into local optima. Overall, our proposed intrinsic-motivated causal empowerment learning framework demonstrates improved stability and learning efficiency. We also evaluate the effect of combining steps 1 and 2, as shown in Appendix \ref{Ablation Studies}. 
For full experimental results in property analysis and ablation studies, please refer to Appendix~\ref{Property analysis} and~\ref{Ablation Studies}. 

% \vspace{-4mm}
\begin{wrapfigure}{r}{0.51\textwidth}
  \centering
  \vspace{-2.5mm} 
  \includegraphics[width=0.25\textwidth]{paper_figs/success/collider_success.pdf} \includegraphics[width=0.25\textwidth]{paper_figs/success/reach.pdf}
  \vspace{-6mm}
  \caption{Success rate in collider and manipulation environments and the shadow is the standard error.}
  \label{fig:success}
\end{wrapfigure}
% \vspace{-3mm}

\paragraph{Sample Efficiency Analysis.} 
After validating the effectiveness of \texttt{\textbf{ECL}} in reward learning, we further substantiate the improvements in sample efficiency of \texttt{\textbf{ECL}} for task execution. As depicted in Figure~\ref{fig:success}, we illustrate task success in both collider and manipulation reach tasks. The compared experimental results underscore the efficiency of our approach, demonstrating enhanced sample efficiency across different environments.


\subsubsection{Causal Dynamics Learning}

\paragraph{Causal Graph Learning.} 
% We compare the causal graph inferred with the ground truth graph, in terms of edge accuracy. The results shown in Table~\ref{tab:1} and Fig.~\ref{fig:causal_graph} demonstrate that the proposed method achieves better performance than compared causal discovery methods in causal learning. 
To evaluate the efficacy of our proposed method for learning causal relationships, we first conduct experimental analyses across three chemical environments, employing five evaluation metrics. 
We conduct causal learning based on the causal discovery with Con and Sco respectively. 
The comparative results using the same causal discovery methods are presented in Table~\ref{tab:1}, with each cell containing the comparative results for that method across different scenarios. 
These results demonstrate the superior performance of our approach in causal reasoning, exhibiting both effectiveness and robustness as evinced by the evaluation metrics of F1 score and ROC AUC~\citep{wang2022causal}. All results exceed 0.90. 
Notably, our approach exhibits exceptional learning capabilities in chemical chain and collider environments. Moreover, it significantly enhances models performance when handling more complex full causal relationships, underscoring its remarkable capability in grasping intricate causal structures. 
This proposed causal empowerment framework facilitates more precise uncovering of causal relationships by actively using the causal structure. 

\paragraph{Visualization.} Moreover, we visually compare the inferred causal graph with the ground truth graph in terms of edge accuracy. The results depicted in Figure~\ref{fig:causal_graph} illustrate the causal graphs of \texttt{\textbf{ECL-Sco}} compared to REG and GRADER in the collider environment. 
For nodes exhibiting strong causality, \texttt{\textbf{ECL-Sco}} achieves fully accurate learning and substantial accuracy enhancements compared to REG.
Concurrently, \texttt{\textbf{ECL-Sco}} elucidates the causality between action and state more effectively. Furthermore, \texttt{\textbf{ECL-Sco}} mitigates interference from irrelevant causal nodes more proficiently than GRADER. 
The causal graph learned in the complex manipulation environment shown in Figure~\ref{fig:abl_manipulation_graph_1}, demonstrates that \texttt{\textbf{ECL}} effectively excludes irrelevant state dimensions to avoid the influence of spurious correlations. 
These findings substantiate that the proposed method attains superior performance compared to other causal discovery methods in causal learning. 

\begin{table}[t]
\centering
\caption{Experimental results on causal graph learning in three chemical environments.}
\label{tab:1}
\fontsize{9}{9}
\selectfont % 字体
\renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{4pt} % 设置列间距为4pt
\begin{tabular}{ccccc}
\hline
\textbf{Metrics}           & \textbf{Methods} & \textbf{Chain}       & \textbf{Collider}   & \textbf{Full}       \\ \hline
\multirow{2}{*}{\textbf{Accuracy}}  & \texttt{\textbf{ECL}}/CDL         & 1.00±0.00/1.00±0.00 & 1.00±0.00/1.00±0.00 &  \textbf{1.00±0.00}/0.99±0.00 \\
                           & \texttt{\textbf{ECL}}/REG         & 0.99±0.00/0.99±0.00  & 0.99±0.00/0.99±0.00 &  \textbf{0.99±0.01}/0.98±0.00 \\ \hline
\multirow{2}{*}{\textbf{Recall}}    & \texttt{\textbf{ECL}}/CDL         &  \textbf{1.00±0.00}/0.99±0.01  & 1.00±0.00/1.00±0.00 &  \textbf{0.97±0.01}/0.92±0.02 \\
                           & \texttt{\textbf{ECL}}/REG         &  \textbf{1.00±0.00}/0.94±0.01  &  \textbf{0.99±0.01}/0.89±0.09 &  \textbf{0.90±0.02}/0.79±0.01 \\ \hline
\multirow{2}{*}{\textbf{Precision}} & \texttt{\textbf{ECL}}/CDL         & 1.00±0.00/1.00±0.00 & 1.00±0.00/1.00±0.00 & 0.96±0.02/ \textbf{0.97±0.02} \\
                           & \texttt{\textbf{ECL}}/REG         & 0.99±0.01/0.99±0.01  & 0.99±0.01/0.99±0.01 &  \textbf{0.97±0.03}/0.92±0.05 \\ \hline
\multirow{2}{*}{\textbf{F1 Score}}  & \texttt{\textbf{ECL}}/CDL         & \textbf{1.00±0.00}/0.99±0.01  & 1.00±0.00/1.00±0.00 &  \textbf{0.97±0.01}/0.94±0.01 \\
                           & \texttt{\textbf{ECL}}/REG         &  \textbf{0.99±0.00}/0.96±0.01  &  \textbf{0.99±0.00}/0.94±0.05 &  \textbf{0.93±0.02}/0.85±0.02 \\ \hline
\multirow{2}{*}{\textbf{ROC AUC}}   & \texttt{\textbf{ECL}}/CDL         &  \textbf{1.00±0.00}/0.99±0.01  & 1.00±0.00/1.00±0.00 &  \textbf{0.98±0.01}/0.96±0.01 \\
                           & \texttt{\textbf{ECL}}/REG         & 0.99±0.01/0.99±0.01  &  \textbf{0.99±0.01}/0.93±0.04 & 0.95±0.01/0.95±0.01 \\ \hline
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
% \centering
\includegraphics[width=1\linewidth]{paper_figs/causal_gragh/causal-graph.pdf}
\caption{The causal graph comparison in the chemical collider environment.}
\label{fig:causal_graph}
\vspace{-5mm}
\end{figure}

\begin{figure}[h]
\centering
\begin{subfigure}{0.49\linewidth}
% \centering
\includegraphics[width=1\linewidth]{paper_figs/prediction/chain.pdf}
% \captionsetup{font=scriptsize}
\caption{Chemical (Chain)}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{paper_figs/prediction/collider.pdf}
% \captionsetup{font=scriptsize}
\caption{Chemical (Collider)}
\label{sub2}
\end{subfigure}
\\
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{paper_figs/prediction/full.pdf}
% \captionsetup{font=scriptsize}
\caption{Chemical (Full)}
\label{sub3}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{paper_figs/prediction/manipulation.pdf}
% \captionsetup{font=scriptsize}
\caption{Manipulation}
\label{sub4}
\end{subfigure}
\caption{Prediction performance (\%) on ID and OOD states of \texttt{\textbf{ECL-Con}} (\texttt{\textbf{ECL-C}}) and \texttt{\textbf{ECL-Sco}} (\texttt{\textbf{ECL-S}}). The mean score is marked on the top of each bar.}
\label{fig:prediction}
\vspace{-3mm}
\end{figure}

\paragraph{Predicting Future States.} 
Given the current state and a sequence of actions, we evaluate the accuracy of each method’s prediction, for states both ID and OOD. 
We evaluate each method for one step prediction on 5K transitions, for both ID and OOD states. To create OOD states, we change object positions in the chemical environment and marker positions in the manipulation environment to unseen values, followed~\citep{wang2022causal}. 

Figure~\ref{fig:prediction} illustrates the prediction results across four environments.
In the ID settings, our proposed methods, based on both Sco and Con, achieve performance on par with GNNs and MLPs, while significantly elevating performance in the intricate manipulation environment. 
These findings validate the efficacy of our proposed approach for causal learning. 
For the OOD settings, our method attains comparable performance to the ID setting. These results demonstrate strong generalization and robustness capabilities compared to GNNs and MLPs. Moreover, it outperforms CDL and REG. 
The comprehensive experimental results substantiate the proficiency of our proposed method in accurately uncovering causal relationships and enhancing generalization abilities. 
For full results of causal dynamics learning, please refer to Appendix~\ref{Results of causal dynamics learning} and~\ref{Visualization on the learned causal graphs}.


% \begin{figure}[t]
% % \centering
% \centering
% \includegraphics[width=0.24\linewidth]{paper_figs/reward/Reach.pdf}
% \includegraphics[width=0.24\linewidth]{paper_figs/reward/Pick.pdf}
% \includegraphics[width=0.24\linewidth]{paper_figs/reward/Stack.pdf}
% \includegraphics[width=0.24\linewidth]{paper_figs/reward/physical.pdf}
% \caption{The task learning of episodic reward in three manipulation and pyhsical environments.}
% \label{fig:manipulation}
% \end{figure}

\subsubsection{Pixel-Based Task Learning}
In complex pixel-based robodesk task, where video backgrounds serve as distractors, \texttt{\textbf{ECL}} effectively learns controllable policies for changing background colors to green, as shown in Figure~\ref{fig:pixel_robodesk}. Additionally, \texttt{\textbf{ECL}} surpasses IFactor in terms of average return. These results further validate \texttt{\textbf{ECL}}'s efficacy in pixel-based tasks and its ability to overcome spurious correlations (video backgrounds). For more results in pixel-based tasks, please refer to Appendix~\ref{pixel-based results}.


\begin{figure}[h]
    \centering
\begin{subfigure}{0.37\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/pixel/robodesk.pdf}
\caption{Average reward}
\end{subfigure}
\hfill
\begin{subfigure}{0.58\linewidth}
% \centering
\includegraphics[width=1\linewidth]{appendix/pixel/vis/robodesk_vis.pdf}
\caption{Visualization}
\end{subfigure}
\hfill
   \caption{The compared results with IFactor and visualized trajectories in Robodesk environment.}
\label{fig:pixel_robodesk}
\vspace{-3mm}
\end{figure}


% \subsection{Property Analysis}
\vspace{-3mm}
\section{Related Work}
\paragraph{Causal MBRL}
MBRL involves training a dynamics model by maximizing the likelihood of collected transitions, known as the world model~\citep{moerland2023model,janner2019trust,nguyen2021temporal,zhao2021consciousness}.
Due to the exclusion of irrelevant factors from the environment through state abstraction, the application of causal inference in MBRL can effectively improve sample efficiency and generalization~\citep{ke2021systematic,mutti2023provably,hwang2023quantized}. 
Wang~\citep{wang2021task} proposes a constraint-based causal dynamics learning that explicitly learns causal dependencies by action-sufficient state representations. 
GRADER~\citep{ding2022generalizing} executes variational inference by regarding the causal graph as a latent variable. CDL~\citep{wang2022causal} is a causal dynamics learning method based on CIT. CDL employs conditional mutual information to compute the causal relationships between different dimensions of states and actions. For additional related work, please refer to Appendix~\ref{Additional Related Works}.
\vspace{-3mm}
\paragraph{Empowerment in RL} 
Empowerment is an intrinsic motivation to improve the controllability over the environment~\citep{klyubin2005empowerment,salge2014empowerment}. This concept is from the information-theoretic framework, wherein actions and future states are viewed as channels for information transmission. In RL, empowerment is applied to uncover more controllable associations between states and actions or skills~\citep{mohamed2015variational,bharadhwaj2022information,choi2021variational, eysenbach2018diversity}. By quantifying the influence of different behaviors on state transitions, empowerment encourages the agent to explore further to enhance its controllability over the system~\citep{leibfried2019unified,seitzer2021causal}. Maximizing empowerment $\max_{\pi} I$ can be used as the learning objective, empowering agents to demonstrate intelligent behavior without requiring predefined external goals. 
% Hence, grounded in the concept of empowerment, we propose a learning framework towards empowerment gain through causal structure learning to improve the controllability for environment. 
\vspace{-3mm}
\section{Conclusion}
\vspace{-3mm}
This paper proposes a method-agnostic framework of empowerment through causal structure learning in MBRL to improve controllability and learning efficiency by iterative policy learning and causal structure optimization. We maximize empowerment under causal structure to prioritize controllable information and optimize causal dynamics and reward models to guide downstream task learning. 
Extensive experiments across $6$ environments included pixel-based tasks substantiate the remarkable performance of the proposed framework.

\textbf{Limitation and Future Work}\quad $\texttt{\textbf{ECL}}$ implicitly enhances the controllability but does not explicitly tease apart different behavioral dimensions.
In our future work, we plan to extend this framework in several directions. First, we aim to disentangle directable behaviors and explore entropy relaxation methods to enhance empowerment, particularly for real-world robotics tasks~\citep{collaboration2023open}. Second, while the current framework does not account for changing dynamics, we intend to incorporate insights from recent advancements in local causal discovery~\citep{hwang2023quantized} and modeling non-stationary change factors~\citep{huang2020causal} to enhance the causal discovery component. Third, we plan to leverage pre-trained 3D or object-centric visual dynamics models~\citep{shi2024composing, wang2023manipulate, luo2025pre, team2024octo} to scale our approach to real-world robotics applications. These directions will be pursued in future work.




\vspace{-3mm}
\section*{Reproducibility Statement}
\vspace{-3mm}
We provide the source code of $\texttt{\textbf{ECL}}$ in the supplementary material. 
The implementation details of experimental settings and platforms are shown in Appendix~\ref{Details on Experimental Design and Results}.

\section*{Acknowledgment}
This work was supported in part by the National Natural Science Foundation of China under Grant 62276128, Grant 62192783; in part by the Jiangsu Science and Technology Major Project BG2024031; in part by the Natural Science Foundation of Jiangsu Province under Grant BK20243051; the Fundamental Research Funds for the Central Universities(14380128); in part by the Collaborative Innovation Center of Novel Software Technology and Industrialization.


% \clearpage
\bibliographystyle{iclr2025_conference}
\bibliography{ref}

% \bibliography{iclr2025_conference}
% \bibliographystyle{iclr2025_conference}

% \appendix
\input{appendix}
% \section{Appendix}
% You may include other additional sections here.


\end{document}
