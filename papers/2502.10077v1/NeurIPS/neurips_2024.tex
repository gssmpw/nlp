\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
   % \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL \usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}     
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{colortbl}
% colors
\usepackage{graphicx}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax} %为了使用 \argmax
\DeclareMathOperator*{\argmin}{argmin} %为了使用 \argmin
\usepackage{multirow}
\usepackage{subcaption} % 导入 subcaption 包
% \usepackage{natbib}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{tcolorbox}

\usepackage{bbm}
\usepackage{enumitem}
\usepackage{amssymb}

\newtheorem{definition}{\bf Definition}
\newtheorem{assumption}{\bf Assumption}  % assumptions
\newtheorem{thm}{\bf Theorem}        % theorems
\newtheorem{corollary}{\bf Corollary}
\definecolor{MyDarkRed}{rgb}{0.8,0.02,0.02}
\definecolor{royalpurple}{rgb}{0.47, 0.32, 0.66}
\colorlet{mylinkcolor}{royalpurple} %violet
\colorlet{mycitecolor}{royalpurple}
\colorlet{myurlcolor}{MyDarkRed}
\hypersetup{
  citecolor  = mycitecolor,
  linkcolor = mylinkcolor,
  urlcolor = myurlcolor,
  colorlinks = true
}
\newcommand{\mf}[1]{\textcolor{orange}{MF: #1}}
\newcommand{\codesite}{\url{https://ecl-9738.github.io/}}
\newenvironment{compactitemize}{\begin{itemize}[nosep,leftmargin=*]}{\end{itemize}}
% original: Causal Action Empowerment for Model-Based Reinforcement Learning
\title{Towards Empowerment Gain through Causal Structure Learning in Model-Based RL}

% option: Causality-guided Empowerment for Model-Based Reinforcement Learning


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.



\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  % \textbf{\textcolor{red}{(Model-based RL)} Model-based reinforcement learning can be divided into dynamics learning and task learning.}

  % \textbf{\textcolor{red}{(Motivation)} But dense dynamics model is vulnerable to spurious correlations and therefore generalizes poorly to unseen states. }

  % \textbf{\textcolor{red}{(Approach)}We introduce causal action empowerment for model-based RL to learn causal dynamics model and optimize task learning}

  % Recent empowerment papers:
  % https://openreview.net/pdf?id=rye16VSl8S
  % https://proceedings.neurips.cc/paper_files/paper/2015/file/e00406144c1e7e35240afed70f34166a-Paper.pdf 
  % https://arxiv.org/pdf/2106.01404
  % https://proceedings.mlr.press/v139/zhang21w.html
  % https://proceedings.mlr.press/v139/choi21b/choi21b.pdf
  % https://openreview.net/pdf?id=DfUjyyRW90
  % https://openreview.net/pdf?id=pmN5h1McUWV
   
Empowerment and causal reasoning are crucial abilities for intelligence. In reinforcement learning (RL), empowerment enhances agents’ ability to actively control their environments by maximizing the mutual information between future states and actions. In model-based RL (MBRL), incorporating causal structures into dynamics models provides agents with a structured understanding of the environment to better control outcomes. We posit that learning causal world models can enhance agents’ empowerment and, conversely, improved empowerment can facilitate causal reasoning. From this viewpoint, our goal is to enhance agents’ empowerment, aiming to improve controllability and learning efficiency, and their ability to learn causal world models. We propose a framework, Empowerment through Causal Learning (ECL), where an agent with the awareness of causal models achieves empowerment-driven exploration and utilize its structured causal perception and control for task learning. Specifically, we first train a causal dynamics model of the environment based on collected data. We then maximize empowerment under the causal structure for policy learning, simultaneously updating the causal model to be more controllable than dynamics model without causal structure. 
An intrinsic curiosity reward is also included to prevent overfitting in offline model learning. Importantly, our framework is method-agnostic, capable of integrating various causal discovery and policy learning techniques. We evaluate ECL combined with $2$ different causal discovery methods in $3$ environments, demonstrating its superior performance compared to other causal MBRL methods, in terms of causal discovery, sample efficiency, and episodic rewards. The anonymous project page is \codesite.

 
  %Model-Based Reinforcement Learning (MBRL) empowers agents to anticipate the consequences of different actions by training a predictive world model.\textcolor{red}{}
 % However,in complex environments, modeling the entire observation space can be fraught with challenges like spurious correlations and poor generalization. To tackle these issues, causal inference approaches have been applied in MBRL to explicitly disentangle unrelated variables. 
  %\textcolor{red}{But explicit causal discovery is arduous and constraining in terms of system controllability.} Hence, we propose a universal learning framework for MBRL that, in combination with intrinsic-motivated \textbf{C}ausal \textbf{A}ction \textbf{E}mpowerment (ECL), allow us to optimize causal dynamics model and downstream task policy that implicitly prioritizes directable factors without reconstruction. We propose the causal action empowerment terms for causal dynamics learning to enable implicit prioritization of directable causal relationships. Moreover, in downstream task learning, we consider the causal effects of different decision actions as \textbf{C}ausal \textbf{A}ction \textbf{R}eward \textbf{E}mpowerment (CARE), serving as an intrinsic-motivated exploration bonus. The proposed ECL framework enhances policy controllability over systems through implicit empowerment objectives optimization, without being limited to specific causal discovery methods. 
 % We evaluate ECL combined with two types of causal discovery technique on three environments for causal dynamics learning and task learning, demonstrating its superior performance compared to other causal MBRL methods, exhibiting more accurate causal discovery, higher sample efficiency and episodic rewards. 
\end{abstract}
\section{Introduction}
% model-based RL and causal RL, the benefits of current causal RL works - can better describe the system and improve generalization. In this paper, we focus on another benefits of causal models - improving the controllability of the env 
Model-based reinforcement learning (MBRL) uses predictive models to enhance decision-making and planning~\cite{moerland2023model}. Recent advances in integrating causal structures into MBRL have provided a more accurate description of systems, aiding adaptation and improving generalization amid environmental changes~\cite{wang2022causal, huang2022action, zhang2020learning, huang2021adarl, richens2024robust, lu2021invariant}, spurious correlations~\cite{ding2022generalizing, ding2024seeing, liu2024learning}, and systematic or compositional generalization challenges~\cite{mutti2023provably, mutti2023exploiting, pitis2020counterfactual, pitis2022mocoda, feng2023learning}. These studies show that agents with causal world models achieve robustness and adaptability across diverse scenarios. However, these methods often rely on \textit{passively} using the learned or given causal structures to improve RL generalization. %In this work, we explore how \textit{actively} leveraging causal structures can enhance environmental controllability and learning efficiency and whether this, in turn, can improve the learning of the causal structure in RL environments. 

% \mf{Challenging}
Exploring how agents can \textit{actively} leverage causal structure to better control the environment, aiming to improve controllability and learning efficiency, presents a compelling challenge. 
To measure the controllability and efficiency, we can employ empowerment gain as the intrinsic motivation for the agents. 
Empowerment is an information-theoretic framework where agents strive to maximize the mutual information between their actions and future states, conditioned on the initial state~\cite{leibfried2019unified, klyubin2005empowerment, klyubin2008keep, bharadhwaj2022information, eysenbach2018diversity}. 
% Hence it can serve as an intrinsic motivation, measuring an agent’s control and efficiency in the RL environment.

In this paper, we explore how to \textit{actively} leverage causal structures to enhance empowerment, thereby improving learning efficiency and whether this, in turn, can improve the learning of the causal structure in RL environments.
Conceptually, learning an accurate causal model for environments and improving empowerment are interdependent processes that reinforce each other. Causal models enable agents not only to predict but also to influence future states more effectively by utilizing variables that directly cause important state changes or reward maximization. Consequently, agents with causal world models are better positioned to manipulate state outcomes, resulting in a higher degree of control and efficiency in their actions. At the same time, by improving controllability, agents gain a better understanding of the consequences of their actions, thereby implicitly learning the causal model of their environment. 

The given example (Fig.\ref{fig:fig1}(a)) discusses robot manipulation, where the goal is to move the target node (movable) while avoiding noisy nodes (some movable and some not). Three possible trajectories (rows 1-3) are shown with different levels of control, efficiency, and success in finding the target. 
Row 1 represents the least effective trajectory, while rows 2 and 3 indicate that the agent has learned control and efficiency (high empowerment, as these behaviors tend to movable objects). 
However, row 2 fails to find the target, whereas row 3 successfully identifies it. Assuming the agent has the causal structure between states and actions (Fig.\ref{fig:fig1}(b)), it will likely execute actions similar to rows 2-3 since there are causal relationships between actions and movable objects, effectively optimizing empowerment. If the agent also knows the causal relationship between states and rewards, it would further prioritize actions leading to the target object. Conversely, when optimizing empowerment, the agent implicitly learns that action sequences like rows 2 and 3 have a greater impact, facilitating efficient control and implicitly learning the causal state-action relationship.


%Take robot manipulation as an example (Fig.\ref{fig:fig1}(a)). The goal of the robotic arm is to capture the target node while avoiding the noisy nodes. In Fig.\ref{fig:fig1}(b), the causal structures can guide the agent’s actions towards more controllable and efficient behaviors (row 3) instead of uncontrollable or inefficient actions (row 2), thereby enhancing its empowerment. Conversely, enhanced empowerment, by motivating agents to perform primitive actions, can also implicitly lead the agent to discover the causal relationships between states and actions. \textcolor{red}{TODO: add summary here}

%Building on this foundation, we advance a step further by exploring how agents can \textit{actively} leverage causal representations and structures to increase their control over the environment, thereby significantly enhancing their \textit{empowerment}.
%This empowerment through causality thereby facilitates more precise interventions, enabling agents to navigate and adapt to complex, dynamic environments.

%Empowerment is an information-theoretic framework in which agents strive to maximize the mutual information between their actions and future states, conditioned on the initial state, serving as an intrinsic motivation for assessing an agent’s control over its environment~\cite{leibfried2019unified, klyubin2005empowerment, klyubin2008keep, bharadhwaj2022information, eysenbach2018diversity}.

\begin{figure}[t]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{figs/fig1.pdf}}
    \caption{(a). Example of robot manipulation task with three trajectories and three nodes: one target node (movable) and two noisy nodes (one movable, one unmovable). (b). Underlying causal structures of the example.}
    \label{fig:fig1}
    \vspace{-15pt}
\end{figure}

\iffalse
\begin{figure}[t]
    \centering
    \subfloat[Manipulation task.]{\includegraphics[width=0.52\linewidth]{paper_figs/example.pdf}
    \label{sub_1}
    }
    \hfil
    \subfloat[Causal graphical model.]{\includegraphics[width=0.35\linewidth]{paper_figs/graph.pdf}
    \label{sub_2}
    }
    \caption{ (a) An illustrative example of the manipulation task. (b) Causal graphical model. \textcolor{red}{TODO: change the graphical model here (the current pairs are not aligned with each other; give def of the exact states in the graphical model; )}}
    \label{fig:illustration}
    \vspace{-5mm}
\end{figure}
\fi 

% \textcolor{red}{TODO: give a shorter version of this}
From this viewpoint, we introduce an Empowerment through Causal Learning (ECL) framework that actively leverages causal structure to maximize empowerment, improving controllability and learning efficiency. The ECL framework consists of three main steps: offline model learning, online model learning, and policy learning. In offline model learning (step 1), we learn the causal dynamics model with causal mask and reward encoder. With the learned causal structure, we then integrate empowerment-driven exploration in online model learning (step 2), to better control the environment, by alternating the updates of the causal structure and policy of empowerment maximization. 
Finally, the learned causal structure is used to learn policies for down-streaming task with a curiosity reward to maintain robustness and prevent overfitting in model learning (step 3). Importantly, our framework is method-agnostic, able to integrate diverse causal discovery and policy learning techniques. 


ECL not only refines policy learning but also ensures that the causal model remains adaptable and accurate, even in the face of novel or shifting environmental conditions. We evaluate ECL with two causal discovery techniques (conditional independence testing and regularization-based) across $3$ environments, considering in-distribution and out-of-distribution settings. ECL outperforms other causal MBRL methods, showing remarkable performance with more accurate causal discovery, higher sample efficiency, and improved episodic rewards. 

% \textcolor{red}{summarize contributions here}

%These policies are optimized to exploit the causal structure, thereby enhancing the agent’s empowerment and learning efficiency in dynamic environments.



%Our main contributions of this paper are as follows. First, we propose a method-agnistic learning framework of empowerment through causal learning that actively leverages causal reasoning to enhance the empowerment of agents, facilitating efficient policy learning. 
%A causal model is constructed to capture the causal dynamics for removing unnecessary dependencies between states and actions. 
%Based on this structured causal model, we enhance the agent's empowerment to better control the environment and discovery the causal relationships more effectively. Furthermore, We evaluate ECL with two causal discovery frameworks (score-based and constraint-based) across $3$ RL environments, considering in-distribution and out-of-distribution settings for causal dynamics learning and task learning, and it outperforms other causal MBRL methods, showing remarkable performance with more accurate causal discovery, higher sample efficiency, and improved episodic rewards. 

% describe the framework: 1-2 sentences on high-level idea; 3-4 sentences on pipelines;

% summarize the contributions (optional) 


%Especially under conditions of distribution shifts, causal relationships offer an efficient, interpretable understanding of the environment and a compact adaptation mechanism. 

%This enhances generalization capabilities across diverse tasks such as multi-domain transfer, out-of-distribution (OOD) scenarios~\cite{zhang2020learning, lu2021invariant, ding2024seeing}, temporal generalization, and compositional generalization. 


\section{Preliminaries}
%\subsection{Preliminaries}
\subsection{MDP with Causal Structures}
\paragraph{Markov Decision Process}
In RL, the interaction between the agent and the environment is formalized as an MDP. The standard MDP is defined by the tuple $ \mathcal{M} = \langle \mathcal{S}, \mathcal{A}, T, \mu_0, r, \gamma \rangle $, where $\mathcal{S}$ denotes the state space, $\mathcal{A}$ represents the action space, $T(s' | s, a)$ is the transition dynamic model, $r(s, a)$ is the reward function, and $\mu_0$ is the distribution of the initial state $s_0$. The discount factor $\gamma \in [0, 1)$ is also included. The objective of RL is to learn a policy $\pi: \mathcal{S} \times \mathcal{A} \to [0, 1]$ that maximizes the expected discounted cumulative reward ${\eta _\mathcal{M}}(\pi) := \mathbb{E}_{s_0 \sim \mu_0, s_t \sim T, a_t \sim \pi} \left[\sum\nolimits_{t = 0}^\infty {\gamma^t}r(s_t, a_t)\right]$. 
% The value function $V_\mathcal{M}^\pi(s): = \mathbb{E}_{s_t \sim T, a_t \sim \pi} \left[\sum\nolimits_{t = 0}^\infty {\gamma^t}r(s_t, a_t) \,|\, s_0 = s\right]$ represents the expected discounted return under policy $\pi$ when starting from the state $s$.
\vspace{-5pt}
\paragraph{Structural Causal Model}
A \textit{structural causal model} (SCM)~\cite{pearl2009causality} is defined by a distribution over random variables $\mathcal{V}=\{s_t^1, \cdots, s_t^d, a_t^1, \cdots, a_t^n, s_{t+1}^1, \cdots, s_{t+1}^d \}$ and a Directed Acyclic Graph (DAG) $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ with a conditional distribution $P(v_i|\mathrm{PA}(v_i))$ for node $v_i \in \mathcal{V}$. Then the distribution can be specified as: 
\begin{equation}
    p(v^1, \dots, v^{|\mathcal{V}|})= \prod_{i=1}^{|\mathcal{V}|}p(v^i|\mathrm{PA}(v_i) ) ,
\end{equation}
where $\mathrm{PA}(v_i)$ is the set of parents of the node $v_i$ in the graph $\mathcal{G}$. 
\vspace{-5pt}
\paragraph{Causal Structures in MDP}
We use a dynamic Bayesian network (DBN)\cite{murphy2002dynamic} (Fig.\ref{fig:fig1}b) denoted by $\mathcal{G}$, 
to model the MDP and the underlying causal structures between states, actions, and rewards. In the DBN, nodes represent system variables (different dimensions of the state, action, and rewards), while edges denote their relationships within the MDP. This model aligns with the factored MDPs~\cite{guestrin2003efficient, guestrin2001multiagent}, and we employ causal discovery methodologies to learn the structures of $\mathcal{G}$. We have the Markov conditions and faithfulness assumptions and the assumptions on edges in MDP (A1-A4):
%For example,in Fig.~\ref{fig:illustration}, $s^{3}_{t}$ is the ancestor of $s^{3}_{t+1}$, and the execution of action $a^{2}_{t}$ make this state transition and get the reward $r_{t+1}$ with another state transition of $s^{2}_{t}$ to $s^{2}_{t+1}$ by action $a^{1}_{t}$. 
%More importantly, we aim to improve the controllability over the system to facilitate more directed and efficient execution of behaviors (such as $a^2$).

% \begin{wrapfigure}{r}{0.42\textwidth}
% \vspace{-12pt}
%     \centering
%       \includegraphics[width=0.88\linewidth]{paper_figs/graph.pdf}
%     \caption{The causal graphical model with different state and action variables. }
%     \label{fig:my_Causal}
%     \vspace{-20pt}
% \end{wrapfigure}
%We use the causal graphical model. 

%Our objective is to remove unrelated state components (such as $s^1$) to prevent unsafe actions and erroneous transitions. 


%We can rewrite the transition probabilities as $\mathcal{P}(s_{t+1},|s_t, a_t)=p(s^3_{t+1}|s^3_{t}, s^2_{t},a^1_t, a^2_t)\cdot p(s^2_{t+1}|s^2_t,a^1_t)
%\cdot p(s^1_{t+1}|s^1_t, s^2_t)$, where the edges from $s^2_t$ to $s^1_{t+1}$ and $s^2_t$ to $s^3_{t+1}$ are optional as it does not change the split of state variables.

% $\{$ 
% The Causal RL Writing Format:
% 1. Description of MDP/ Factored MDP etc.
% 2. Causal Graph Models
% 3. Problem Definitions
% 4. Assumptions
% 5. Theorems
% 6. Approach with RL
% $\}$
\vspace{-5pt}
\begin{assumption}
    (Global Markov Condition~\cite{spirtes2001causation, pearl2009causality}) The state is fully observable and the dynamics is Markovian. The distribution $p$ over a set of variables $\mathcal{V}=(s^1_{t},\cdots,s^d_{t},a^1_{t},\cdots,a^d_{t},r_t)^T$ satisfies the global Markov condition on the graph if for any partition $(\mathcal{S, A, R})$ in $\mathcal{V}$ such that if $\mathcal{A}$ d-separates $\mathcal{S}$ from $\mathcal{R}$, then $p(\mathcal{S},\mathcal{R}|\mathcal{A}) = p(\mathcal{S}|\mathcal{A})\cdot p(\mathcal{R}|\mathcal{A})$
\end{assumption}
\vspace{-5pt}
\begin{assumption}
    (Faithfulness Assumption~\cite{spirtes2001causation, pearl2009causality}) 
For a set of variables $\mathcal{V}=(s^1_{t},\cdots,s^d_{t},a^1_{t},\cdots,a^d_{t},r_t)^T$, there are no independencies between variables that are not implied by the Markovian Condition.
\end{assumption}
\vspace{-5pt}
\begin{assumption}
Under the assumptions that the causal graph is Markov and faithful to the observations, the edge $s^i_t \to  s^t_{t+1}$ exists for all state variables $s^i$.
\end{assumption}
\vspace{-5pt}
\begin{assumption}
No simultaneous or backward edges in time.
\end{assumption}
\vspace{-5pt}
\textbf{Theorem 1} \textit{Assuming A1-A4, we define the conditioning set $\{ a_t, s_t  \setminus  s^i_t \} = \{ a_t,s^1_t, \dots s^{i-1}_t, s^{i+1}_t, \dots \}$. If $s^i_t \not \! \perp \!\!\! \perp s^j_{t+1} | \{ a_t, s_t  \setminus  s^i_t \}$, then $s^i_t \to s^j_{t+1}$. Similarly, if $a^i_t \not \! \perp \!\!\! \perp s^j_{t+1} | \{ a_t \setminus a^i_t, s_t \}$}, then $a^i_t \to s^j_{t+1}$.

With Assumptions 1-4 and Theorem 1, we can identify the graph structures in $\mathcal{G}$, which can be represented as the adjacency matrix $M$. Hence, the dynamic transitions and reward functions in MDP with structures are as follows:
\begin{equation}
\left\{\begin{matrix}
s^i_{t+1} = f\left( M^{s \to s} \odot s_t, M^{a \to s} \odot a_t, \epsilon_{s,i,t} \right) \\
r_t = R(\psi(s_t), a_t)
\end{matrix}\right.
\label{eq:gen}
\end{equation}
where \( s^i_{t+1} \) represents the next state, $ M^{s \to s} \in \{0,1\}^{|s|\times |s|}$ and $ M^{a \to s} \in \{0,1\}^{|a|\times |s|}$ are the adjacency matrices indicating the influence of current states and actions on the next state, respectively, \( \odot \) denotes the element-wise product, and \( \epsilon_{s,i,t} \) represents i.i.d. Gaussian noise. The reward \( r_t \) is a function of the current state \( \psi(s_t) \), which filters out the state without direct edges to the target, and the action \( a_t \).
\subsection{Empowerment in RL}
Empowerment is to quantify the influence an agent has over its environment and the extent to which this influence can be perceived by the agent~\cite{klyubin2005empowerment,salge2014empowerment,jung2011empowerment}. Within our framework, the empowerment is the channel capacity between the agent actions ${a}_t$ and its subsequent state ${s}_{t+1}$ given the causal mask $M$ as follows:
\begin{equation}
    \mathcal{E} := \max_{p(a_t)} \mathcal{I}(s_{t+1};a_{t} \mid M),
\end{equation}
where $\mathcal{E}$ is used to represent the channel capacity from the action to state observation. $p(a_t)$ is the distribution of actions.
\vspace{-5pt}
\section{Empowerment through Causal Learning}
\vspace{-5pt}
% \textcolor{blue}{Make the notations aligned with the ones in updated fig.2}
% \textcolor{red}{Use 3.1 to describe the model learning, including dynamics learning, structure learning, and reward prediction. Add one final objective function (sth like likelihood maximization) to summarize}
\label{sec:ECL}
\subsection{Overview}
\vspace{-5pt}
\begin{figure}[h]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{paper_figs/pipeline2.pdf}}
    \caption{Framework of \texttt{\textbf{ECL}}. Gold lines: offline model learning. Blue lines: online model learning alternating with empowerment-driven exploration (yellow lines). Green lines: policy learning.}
    \label{fig:framework}
\end{figure}
% \textcolor{red}{Causal structure, empowerment, objective functions
% We introduce the ECL framework (Fig.~\ref{fig:framework}) that simultaneously learns the causal structure and gains empowerment, enabling the agent to causally control the environment and efficiency learn the down-streaming policy. 
% ECL leverages causal structure to maximize empowerment, improving controllability and learning efficiency. 
This framework (Fig.~\ref{fig:framework}) consists of three main steps: offline model learning, online model learning, and policy learning. In offline model learning \textbf{(step 1)}, we learn the causal structures of the environment dynamics, capturing the causal dynamics and reward structures. This causal model is trained using offline collected data to identify the causal structures (i.e., causal masks), dynamics and reward models by maximizing the likelihood of observed trajectories. With the learned structured causal model in place, we then integrate empowerment-driven exploration in online model learning \textbf{(step 2)}, to learn policies that enhance the agent’s ability to control and influence its environment effectively. By alternating the updates of the causal structure and policy to achieve empowerment maximization, the overall optimization objective is to learn the policy that maximizes empowerment with the causal structure. Finally, in \textbf{step 3}, the learned causal structure is used as a model to learn policies for down-streaming task policy. In addition to the task reward, to maintain robustness and prevent overfitting in model learning, the curiosity reward is also incorporated.
\subsection{Step 1: Offline Model Learning with Causal Structures}
We learn the causal and model structures from the offline dataset $\mathcal{D}$. Specifically, we employ a causal dynamic encoder and a reward encoder to maximize the likelihood of observed trajectories.
\vspace{-5pt}
\paragraph{Causal Dynamics Encoder} 
The causal dynamics encoder consists of two parts: the dynamics encoder $P_{\phi_c}$ and causal mask $M$. The dynamics encoder $P_{\phi_c}$ maximizes the likelihood of observed states:
\begin{equation}
\mathcal{L}_{\texttt{dyn}}= \mathbb{E}_{{(s_t, a_t, s_{t+1})} \sim \mathcal{D} } \left[\sum_{i=1}^{d_S} \log P_{\color{MyDarkRed}{\phi_c}}(s_{t+1}^{i} | s_t, a_t; {\color{MyDarkRed}{\phi_c}}) \right],
\label{eq:full}
\end{equation}
where \( d_S \) is the dimension of the state space, and \( \phi_c \) denotes the parameters of the dynamics encoder.
\vspace{-5pt}
\paragraph{Causal Discovery} For causal discovery, with the learned dynamics model \( P_{\phi_{c}} \), we further embed the causal structure into the objective function. To learn the causal structure, we employ two off-the-shelf causal discovery methods: conditional independence testing in~\cite{wang2022causal} and regularization by sparse filters~\cite{huang2022action}. We also maximize the likelihood of states by updating the dynamics encoder and masks. Thus, the total objective for the causal dynamics encoder is:

\begin{equation}
\mathcal{L}_{\rm{c-dyn}}= \mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D} } \left[\sum_{i=1}^{d_S} \log P_{\phi_{\rm{c}}}(s_{t+1}^{i} | {\color{MyDarkRed}{M^{s\to s^j}}} \odot s_t, {\color{MyDarkRed}{M^{a\to s^j}}} \odot a_t; \phi_{\rm{c}}) + \mathcal{L}_{\rm{causal}} \right],
\label{eq:cau}
\end{equation}

where \( \mathcal{L}_{\rm{causal}} \) represents the objective term associated with learning the causal structure\footnote{Detailed loss functions are given in Appendix~\ref{Experimental setup}}.
% where $\phi_{\rm{dyn}}$ is used to approximate the predictive model. $\mathcal{D}$ is the collected transition data. $d_S$ is the dimension of the state space. 
% \paragraph{Causal mask.} 
%Another learning objective is to execute the causal structure learning by maximizing the likelihood of the dynamics model with causal mask $M$ as:
\iffalse
\begin{equation}
\label{eq:cau}
    \mathcal{L}_{\rm{cau}} = \mathbb{E}_{(s_t, a_t, s_{t+1}) \sim \mathcal{D} } \left[ \sum_{i=1}^{d_S}\mathrm{log} P_{\phi_{\rm{c}}}(s^j_{t+1}|\textcolor{red}{C^{s\to s^j}} \odot s_t, \textcolor{red}{C^{a\to s^j}} \odot a_t ; \phi_{\rm{c}}) \right],
\end{equation}
where $\phi_{\rm{cau}}$ is used to identify the causal structure of $\mathcal{G}$ by predicting the binary masks in Eq.~\ref{eq:gen}.
\fi 
\vspace{-5pt}
\paragraph{Reward Encoder}
Similarly, the reward encoder $P_{\varphi_{\rm_{r}}}$ aims to maximize the likelihood of the rewards:
\begin{equation}
\label{eq:rew}
    \mathcal{L}_{\rm{rew}}= \mathbb{E}_{(s_t, a_t, r_{t} \sim \mathcal{D})} \left[ \mathrm{log}P_{\textcolor{red}{\varphi_{r}}} \left(r_{t} | \psi(s_t),a_t\right)  
    \right],
\end{equation}
where $\psi(\cdot)$ is the operation to filter out the irrelevant states with causal dynamics model.
Hence, the overall objective of the offline model learning with causal structures is to maximize $\mathcal{L} = \mathcal{L}_{\rm{dyn}} + \mathcal{L}_{\rm{c-dyn}} + \mathcal{L}_{\rm{rew}}$.
\vspace{-5pt}
\subsection{Step 2: Online Model Learning with Empowerment-driven Exploration}
\vspace{-5pt}
In Step 2, we aim to simultaneously optimize the learning of the causal structure and empowerment. Specifically, as illustrated in Fig.~\ref{fig:framework}, we alternately optimize the empowerment-driven exploration policy $\pi_e$, the causal mask $M$, and the reward encoder $\varphi_r$. To ensure stable learning, we keep the dynamic encoder $\phi_c$ learned in Step 1 fixed, focusing solely on the alternating optimization of the causal structure and empowerment.
\vspace{-5pt}
\paragraph{Empowerment-driven Exploration} To enhance the agent's control and efficiency given the causal structure, instead of maximizing $\mathcal{I}\left(s_{t+1}, a_t | s_t\right)$ at every environment step, we consider a baseline that uses a dense dynamics model $\phi$ without causal structures. We then optimize the difference between the empowerment gain of the causal dynamics model and the baseline dense dynamics model. 

We first denote the empowerment gain of the causal dynamic model and dynamic model as $\mathcal{E}_{\phi_c}(s) = \max_a  \mathcal{I}\left(s_{t+1}; a_t \mid s_t; \phi_c, M\right)$ and $\mathcal{E}_{\phi}(s) = \max_a  \mathcal{I}\left(s_{t+1}; a_t \mid s_t; \phi\right)$, respectively. Here, $\phi$ corresponds to the dynamic model without considering causal structures. For this purpose, we separately train a well-tuned $\phi$ on offline data to serve as a baseline for optimization. 

Then, we have the following objective function:
\begin{equation}
    \max_{a \sim \color{MyDarkRed}{\pi_e(a|s)}} \mathbb{E}_{s_t, a_t, s_{t+1} \sim \mathcal{D}} \left[\mathcal{E}_{\phi_c}(s) - \mathcal{E}_{\phi}(s) \right].
\label{eq:7}
\end{equation}
In practice, we employ the estimated $\hat{\mathcal{E}}_{\phi_c}(s)$ and $\hat{\mathcal{E}}_{\phi}(s)$, specifically
\begin{equation}
     \hat{\mathcal{E}}_{\phi_c}(s) = \max_{a_t \sim \color{MyDarkRed}{\pi_e(a|s)}} \mathbb{E}_{\pi_e(a_t|s_t) p_{\phi_c}(s_{t+1}|s_t, a_t)} \left[\log P_{\phi_c}(s_{t+1} \mid s_t, a_t; M, \phi_c) - \log P(s_{t+1}|s) \right],
\end{equation}
and 
\begin{equation}
     \hat{\mathcal{E}}_{\phi}(s) = \max_{a_t \sim \color{MyDarkRed}{\pi_e(a|s)}} \mathbb{E}_{\pi_e(a_t|s_t) p_{\phi}(s_{t+1}|s_t, a_t)} \left[\log P_{\phi}(s_{t+1} \mid s_t, a_t; \phi) - \log P(s_{t+1}|s) \right],
\end{equation}
where $P(s_{t+1}|s)$ is the marginal distribution of the future state $s_{t+1}$. Hence, the objective function Eq.~\ref{eq:7} is derived as:
\begin{equation}
    \max_{a \sim \color{MyDarkRed}{\pi_e(a|s)}} \mathcal{H}(s^{\phi_c}_{t+1} \mid s_t) - \mathcal{H}(s^{\phi}_{t+1} \mid s_t) + \mathbb{E}_{a \sim \pi_e(a|s)} \left[\mathbb{KL} \left(P_{\phi_c}(s_{t+1} \mid s_t, a_t; M) \| P_{\phi}(s_{t+1} \mid s_t, a_t) \right) \right],
\end{equation}
where $s^{\phi_c}_{t+1}$ and $s^{\phi}_{t+1}$ denote the state at time $t+1$ under the causal dynamics and dynamics model, respectively. Since Computing $\mathcal{H}(s^{\phi_c}_{t+1} \mid s_t) - \mathcal{H}(s^{\phi}_{t+1} \mid s_t)$ requires integrating over actions. So for simplicity, we update $\pi_e$ by only optimizing the KL term. 
\vspace{-5pt}
\paragraph{Online Model Learning} In Step 2, we fix the dynamics encoder $\phi_c$ and further fine-tune the causal mask $M$ and the reward encoder $\varphi_r$. We adopt an alternating optimization with the policy $\pi_e$ to learn the model. Specifically, given $M$, we first optimize $\pi_e$. Then, using the actions from $\pi_e$, we collect new trajectories online to update $M$ and $\varphi_r$.

\iffalse
The empowerment enhances an agent's controllability over its environment by maximizing the mutual information between the agent's future states and its actions, as expressed by maximizing the following information term: $
    I(s_{t+1}; a_t) = \mathcal{H}(s_{t+1}) - \mathcal{H}(s_{t+1}|a_t) $. 
We seek to maximize mutual information as a form of empowerment $I(s_{t+1}; a_t | M) - I(s_{t+1}; a_t)$, where a causal dynamics model surpasses that of the dynamics model. This enhancement aims to enable the agent to better control the environment through actively leveraging the acquired causal structure. 
% \begin{equation}
% \begin{aligned}
%      I(s_{t+1}; a_t | M) - I(s_{t+1}; a_t)
%      =
%     \mathbb{E}_{(s_t, a_t, s_{t+1} \sim \mathcal{D})} \left[\log \frac{P(s_{t+1} | C^{s\to s} \odot s_t, C^{a\to s} \odot a_t ; \phi_{\rm{cau}})}{P(s_{t+1} |s_t, a_t; \phi_{\rm{dyn}} )} \right]
%     % & \le \mathbb{E}_{(s_t, a_t, s_{t+1} \sim \mathcal{D})} \left[ \mathrm{log}P \left(s_{t+1} | C^{s\to s} \odot s_t, C^{a\to s} \odot a_t; \phi_{\rm{cau}}\right) \right. \\
%     % & \left. - \mathrm{log}P(s_{t+1}|s_t, a_t; {\phi_{\rm{full}}}) 
%     % \right].
% \label{eq:emp}
% \end{aligned}
% \end{equation}

The learning objective of empowerment gain thorugh causal structure is $\max [ I(s_{t+1}; a_t | M) - I(s_{t+1}; a_t) ] $, which aims to prioritize capturing the controllable factors of the environment first based on the causal structure for improving the policy efficiently. For a full deviation of this maximization, please refer to Appendix C.2. 

\textcolor{red}{This objective maximizes the mutual information between the next state $s_{t+1}$ and the current action $a_t$, which encourages the agent to learn the direct effects of its actions on the environment. 
The intuition behind this objective is that if the agent can accurately predict the consequences of its actions, it will have a better understanding of how to achieve its goals and navigate the environment. By capturing the controllable factors first, the agent can focus on learning the most relevant and actionable aspects of the environment, rather than being distracted by uncontrollable or irrelevant factors. By explicitly prioritizing the controllable factors, the agent can disentangle the causal relationships and develop a more robust and effective control strategy.}
\fi 
\subsection{Step 3: Policy Learning with Curiosity Reward}
\vspace{-5pt}
We learn the downstream policy for the task given the causal structures. To mitigate the potential overfitting of the causal model learned in Steps 1\&2, we use a curiosity reward (CUR) to serve as an intrinsic motivation objective or exploration bonus, in conjunction with a task-specific reward, to prevent overfitting in model learning.
\begin{equation}
    r_{\mathrm{cur}}=\mathbb{E}_{(s_t, a_t, s_{t+1} \sim \mathcal{D})}\left[{\mathbb{KL}}\left({P}_{\rm{true}}||{P}_{{\phi_c}}\right)-{\mathbb{KL}}\left({P}_{\rm{true}}||{P}_{\phi}\right)\right],
\label{eq:cur}
\end{equation}
where ${P}_{\rm{true}}$ is the ground truth dynamics of the system. By taking account of $r_{\mathrm{cur}}$, we encourage the agent to explore states that the causal dynamics cannot capture but the dense dynamics can, thus preventing the policy from being overly conservative due to offline model learning. Hence The shaped reward is shown as follows:
\begin{equation}
    r(s,a)=r_{\mathrm{task}}(s,a)+\lambda r_{\mathrm{cur}}(s,a),
\label{eq:shaped_rew}
\end{equation}
where $r_{\rm{task}}(s,a)$ is the task reward, $\lambda$ is a balancing hyperparameter. 


\iffalse
where $\mathcal{P}_S^{\rm{cau}}$ is the prediction distribution of causal dynamics, $\mathcal{P}_S^{\rm{true}}$ is the distribution of the groundtruth, and $\mathcal{P}_S^{\rm{full}}$ is the prediction distribution of full dense dynamics. Our approach incentivizes the policy to take causal actions that diverge from the original strategy, thereby fostering broader exploration. 
Hence, the overall objective of the downstream task learning is to learn a policy $\pi_{\theta}$ that maximizes the expected discounted cumulative reward ${\eta _\mathcal{\hat{M}}}(\pi_{\theta}) := \mathbb{E}_{s_0 \sim \mu_0, s_t \sim T, a_t \sim \pi_{\theta}} \left[\sum\nolimits_{t = 0}^\infty {\gamma^t}(r_{\mathrm{task}}(s,a)+\lambda r_{\mathrm{cur}}(s,a))\right]$.
\fi 
\vspace{-5pt}
\section{Practical Implementation}
\vspace{-5pt}
We introduce the practical implementation of ECL for casual dynamics learning with empowerment-driven exploration and task learning. 
The proposed framework for the entire learning process is illustrated in Figure~\ref{fig:framework}, comprising three steps that step $2$ and $3$ are executed cyclically over time.
\iffalse
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{paper_figs/alg_1.pdf}
    \hspace{1mm}
    \includegraphics[width=0.52\linewidth]{paper_figs/framework.pdf}
    \caption{The proposed method of learning procedure and framework}
    \label{fig:framework}
\end{figure}
\fi



\textbf{Step 1: Offline Model Learning}\quad
Initially, following \cite{wang2022causal}, we establish a transition collection policy $\pi_{\text{collect}}$ by formulating a reward function that incentivizes selecting transitions that cover more state action pairs to expose causal relationships thoroughly. 
We train the dynamics model $\phi_c$ by maximizing the log-likelihood $\mathcal{L}_{\rm{dyn}}$, following Eq.~\ref{eq:full}. 
Then, we employ causal discovery approach for learning causal mask by maximizing the log-likelihood $\mathcal{L}_{\rm{c-dyn}}$ followed Eq.~\ref{eq:cau}. 
Subsequently, we train the reward predictor $\varphi_{\rm{r}}$ by maximizing the likelihood in accordance with Eq.~\ref{eq:rew}.

\textbf{Step 2: Online Model Learning}\quad
We execute empowerment-driven exploration by maximizing $\mathcal{I}(s_{t+1};a_t|M) - \mathcal{I}(s_{t+1};a_t)$ followed Eq.~\ref{eq:7} with causal dynamics model and dynamics model without causal mask for policy $\pi_{e}$ learning. Furthermore, the learned policy $\pi_{e}$ is used to sample transition for casual mask $M$ fine-tuning with fixed $\phi$. We alternately perform empowerment-driven exploration for policy learning and causal model learning for causal structure optimization. 
% This process actively leverages causal representations and structures to increase their control over the environment and more efficiently discover causal relationships. 

% \textcolor{red}{We explicitly conduct state abstraction with causal discovery to eliminate irrelevant state components and prioritize controllable ones, driven by the concept of empowerment through causal structure learning. We maximize $\hat{I}(s_{t+1}; a_t)$ outlined in Eq.~\ref{eq:emp} for policy optimization. Subsequently, we improve the controllability of the environment by maximizing $I(s_{t+1}; a_t)$ to update policy $\pi_{\theta}$ and
% the dynamics model with causal mask $\phi_{\rm{cau}}$.}


\textbf{Step 3: Policy Learning}\quad
During downstream task learning, we incorporate the causal effects of different actions as curiosity rewards combined with the task reward, following Eq.~\ref{eq:shaped_rew}. The causality introduced by CUR in task learning maintains essential exploration, thereby facilitating the learning of an optimal policy to maintain robustness and prevent overfitting in model learning. We maximize the discounted cumulative reward ${\eta _\mathcal{\hat{M}}}(\pi_{\theta})$ to learn the policy by the cross entropy entropy (CEM)~\cite{rubinstein1997optimization}.
% Additionally, the policy $\pi_{\theta}$ is utilized to sample transitions to the replay buffer $\mathcal{D}_{\rm{buffer}}$ for model optimization and causal empowerment. 

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{figs/balance.pdf}
%     \caption{Exploration of diversity and causality.}
%     \label{fig:balance}
% \end{figure}

% \subsection{Overall Objective of ECL}
% % \textcolor{red}{Intrinsic-motivated causal dynamics empowerment + Causal action Reward empowerment.} 
% We now motivate the overall learning objective of ECL, which consists of maximizing two terms: the causal dynamics learning and the downstream task learning empowerment terms.

% \paragraph{Causal dynamics learning objective.} The causal dynamics learning aims to prioritize the encoding of forward-predictive, reward-predictive, and directable causal components. Therefore, we define the overall objective for causal dynamics learning as follows:

% \begin{equation}
%     \max_{s_{1:H}}\sum_{t=1}^{H} \left( I(s_{t+1}|s_t, a_t; \phi_{\rm{dyn}})+ I(s_{t+1}|s_t, a_t; \phi_{\rm{cau}}) + I(r_{t}|s_t, a_t; \phi_{\rm{return}})
% + I(r_{t}; s_t ) \right).
% \end{equation}


% \paragraph{Task learning objective.} The task learning objective aims to prioritize the causal action empowerment term and reward-based value term. We define the overall objective for downstream task learning over the horizon $H$ as follows:

% \begin{equation}
%     \max_{s_{1:H}}\sum_{t=1}^{H} \left( I(s_{t+1}|s_t, a_t; \theta_{\rm{cau}}) - I(s_{t+1}|s_t, a_t; \theta_{\rm{full}})\right)
% + V_{\theta}(s),
% \end{equation}
% where $V_{\theta}(s) = \mathbb{E}_{s_t\sim T_{\theta},a_t\sim \pi_{\theta }}\left[ 
%  {\textstyle \sum_{t=1}^{H}} \gamma^tr(s_t, a_t)|s_1=s
% \right] $.


% \clearpage
\vspace{-5pt}
\section{Experiments}
\vspace{-5pt}
We aim to answer the following questions in the evaluation:
(i) How does the performance of ECL compared to other causal and dense causal models across different environments for tasks and dynamics learning?
(ii) Whether different causal discovery methods in step 1 and 2, impact policy performance? 
(iii) Does ECL improve the causal discovery and learning efficiency with the empowerment gain? 
(iv) What are the effects of the hyperparameters in ECL?
\vspace{-5pt}
\subsection{Setup}
\vspace{-5pt}
\paragraph{Environments.} We select 3 different environments for experimental evaluation.
\textbf{Chemical~\cite{ke2021systematic}:} The task is to discover the causal relationship (Chain, Collider \& Full) of chemical items 
which proves the learned dynamics and explains the behavior without spurious correlations. 
\textbf{Manipulation~\cite{wang2022causal}:} The task is to prove the learned dynamics and policy for difficult settings with spurious correlations and multi-dimension action causal influence. 
% In each episode, the objects and markers are reset to randomly sampled poses on the table.
\textbf{Physical~\cite{ke2021systematic}:} We also evaluate our method in the dense mode environment Physical. 
For the details of environment setup, please refer to Appendix~\ref{Experimental setup}. 
\vspace{-8pt}
\paragraph{Baselines.} We compare ECL with 3 causal and 2 dense dynamics methods.
CDL~\cite{wang2022causal}: infers causal relationships between the variables for dynamics learning with Conditional Independence Test (CIT).  ASR~\cite{huang2022action}: causal structure learning based on regularization, where the causal mask is learned as trainable parameters. GRADER~\cite{ding2022generalizing}: generalizing goal-conditioned RL with CIT by variational causal reasoning. 
GNN~\cite{ke2021systematic}: a graph neural network with dense dependence for each state variable. 
Monolithic~\cite{wang2022causal}: a multi-layer perceptron (MLP) network that takes all state variables and actions for prediction. For ECL, we employ both the conditional independence testing (same setup in CDL~\cite{wang2022causal}) and sparse regularization (same setup in ASR~\cite{huang2022action}) as the causal discovery modules. 
\vspace{-5pt}
\paragraph{Evaluation Metric.} In tasks learning, we utilize episodic reward and the task success as evaluation criteria for downstream tasks. For causal dynamics learning, we employ five metrics to evaluate the learned causal graph and assess the mean accuracy for dynamics predictions of future states both In-Distribution (ID) and Out-Of-Distribution (OOD). 
% \vspace{-5pt}
\subsection{Results}
% \vspace{-5pt}
\subsubsection{Task Learning}
We evaluate each method with the following downstream tasks in the chemical (C), physical (P) and the manipulation (M) environments. 
\textbf{Match} (C): match the object colors with goal colors individually. 
\textbf{Push} (P): use the heavier object to push the lighter object to goal position. 
\textbf{Reach} (M): move the end-effector to the goal position. 
\textbf{Pick} (M): pick the movable object to the goal position. 
\textbf{Stack} (M):  stack the movable object on the top of the unmovable object.

As shown in Fig.~\ref{fig:reward}, compared to dense models GNN and MLP, as well as the causal approaches CDL and REG, ECL-CIT attains the highest reward across $3$ environments. Notably, ECL-CIT outperforms other methods in the intricate manipulation tasks. 
Furthermore, ECL-ASR surpasses REG, elevating model performance and achieving a reward comparable to CDL. 
The proposed curiosity reward encourages exploration and avoids local optimality during the policy learning process. For full results, please refer to Appendix~\ref{Downstream tasks learning}.

Additionally, Figure~\ref{fig:reward_curves} depicts the learning curves across three environments. Across these diverse settings, ECL exhibits elevated sample efficiency compared to CDL and higher reward attainment. The introduction of curiosity reward bonus enables efficient exploration of strategies, thus averting the risk of falling into local optima. Overall, our proposed intrinsic-motivated causal empowerment learning framework demonstrates improved stability and learning efficiency. For full experimental results in property analysis and ablation studies, please refer to Appendix~\ref{Property analysis} and~\ref{Ablation Studies}.

\begin{figure}[t]
% \centering
\centering
\includegraphics[width=0.24\linewidth]{paper_figs/reward/chain.pdf}
\includegraphics[width=0.24\linewidth]{paper_figs/reward/full.pdf}
\includegraphics[width=0.24\linewidth]{paper_figs/reward/Stack.pdf}
\includegraphics[width=0.24\linewidth]{paper_figs/reward/physical.pdf}
\caption{The task learning of episodic reward in three environments.}
\label{fig:reward}
\vspace{-2mm}
\end{figure}

\begin{figure}[t]
% \centering
\centering
\includegraphics[width=0.325\linewidth]{paper_figs/reward/curves/chemical_full.pdf}
\includegraphics[width=0.325\linewidth]{paper_figs/reward/curves/physical.pdf}
\includegraphics[width=0.325\linewidth]{paper_figs/reward/curves/Pick.pdf}
\caption{The learning curves of episodic reward in three different environments and the shadow is the standard error.}
\label{fig:reward_curves}
\vspace{-5mm}
\end{figure}

\begin{wrapfigure}{r}{0.52\textwidth}
  \centering
  % \hspace{-1cm} 
  \includegraphics[width=0.25\textwidth]{paper_figs/success/collider.pdf}
  \includegraphics[width=0.25\textwidth]{paper_figs/success/reach.pdf}
  \caption{Task success in the collider and manipulation environments.}
  \label{fig:success}
  \vspace{-10pt}
\end{wrapfigure}
\vspace{-5pt}
\paragraph{Sample Efficiency Analysis.} 
After validating the effectiveness of ECL in reward learning, we further substantiate the improvements in sample efficiency of ECL during task learning. As depicted in Figure~\ref{fig:success}, we illustrate task success in both collider and manipulation reach tasks. The compared experimental results underscore the efficiency of our approach, demonstrating enhanced sample efficiency across different environments.


\vspace{-3mm}
\subsubsection{Causal Dynamics learning}

\paragraph{Causal Graph Learning.} 
% We compare the causal graph inferred with the ground truth graph, in terms of edge accuracy. The results shown in Table~\ref{tab:1} and Fig.~\ref{fig:causal_graph} demonstrate that the proposed method achieves better performance than compared causal discovery methods in causal learning. 
To evaluate the efficacy of our proposed method for learning causal relationships, we first conduct experimental analyses across three chemical environments, employing five evaluation metrics. 
We conduct causal learning based on the causal discovery with CIT and ASR respectively. 
The comparative results using the same causal discovery methods are presented in Table~\ref{tab:1}, with each cell containing the comparative results for that method across different scenarios. 
These results demonstrate the superior performance of our approach in causal inference, exhibiting both efficiency and robustness as evinced by the evaluation metrics of F1 score and ROC AUC. All results exceed 0.90. 
Notably, our approach exhibits exceptional learning capabilities in chemical chain and collider environments. Moreover, it significantly enhances models performance when handling more complex full causal relationships, underscoring its remarkable capability in grasping intricate causal structures. 
This proposed intrinsic-motivated causal empowerment framework facilitates more precise uncovering of causal relationships and augments the stability of causal inference. 
\vspace{-3mm}
\paragraph{Visualization.} Moreover, we visually compare the inferred causal graph with the ground truth graph in terms of edge accuracy. The results depicted in Figure~\ref{fig:causal_graph} illustrate the causal graphs of ECL-ASR compared to REG and GRADER in the collider environment. 
For nodes exhibiting strong causality, ECL-ASR achieves fully accurate learning and substantial accuracy enhancements compared to REG.
Concurrently, ECL-ASR elucidates the causality between action and state more effectively. Furthermore, ECL-ASR mitigates interference from irrelevant causal nodes more proficiently than GRADER. 
These findings substantiate that the proposed method attains superior performance compared to other causal discovery methods in causal learning. 

\begin{table}[t]
\centering
\caption{Compared results of causal graph learning on three chemical environments.}
\label{tab:1}
\fontsize{9}{9}
\selectfont % 字体
\renewcommand{\arraystretch}{1.4}
% \setlength{\tabcolsep}{4pt} % 设置列间距为4pt
\begin{tabular}{ccccc}
\hline
\textbf{Metrics}           & \textbf{Methods} & \textbf{Chain}       & \textbf{Collider}   & \textbf{Full}       \\ \hline
\multirow{2}{*}{\textbf{Accuracy}}  & ECL/CDL         & 1.00±0.00/1.00±0.00 & 1.00±0.00/1.00±0.00 &  \textbf{1.00±0.00}/0.99±0.00 \\
                           & ECL/REG         & 0.99±0.00/0.99±0.00  & 0.99±0.00/0.99±0.00 &  \textbf{0.99±0.01}/0.98±0.00 \\ \hline
\multirow{2}{*}{\textbf{Recall}}    & ECL/CDL         &  \textbf{1.00±0.00}/0.99±0.01  & 1.00±0.00/1.00±0.00 &  \textbf{0.97±0.01}/0.92±0.02 \\
                           & ECL/REG         &  \textbf{1.00±0.00}/0.94±0.01  &  \textbf{0.99±0.01}/0.89±0.09 &  \textbf{0.90±0.02}/0.79±0.01 \\ \hline
\multirow{2}{*}{\textbf{Precision}} & ECL/CDL         & 1.00±0.00/1.00±0.00 & 1.00±0.00/1.00±0.00 & 0.96±0.02/ \textbf{0.97±0.02} \\
                           & ECL/REG         & 0.99±0.01/0.99±0.01  & 0.99±0.01/0.99±0.01 &  \textbf{0.97±0.03}/0.92±0.05 \\ \hline
\multirow{2}{*}{\textbf{F1 Score}}  & ECL/CDL         & \textbf{1.00±0.00}/0.99±0.01  & 1.00±0.00/1.00±0.00 &  \textbf{0.97±0.01}/0.94±0.01 \\
                           & ECL/REG         &  \textbf{0.99±0.00}/0.96±0.01  &  \textbf{0.99±0.00}/0.94±0.05 &  \textbf{0.93±0.02}/0.85±0.02 \\ \hline
\multirow{2}{*}{\textbf{ROC AUC}}   & ECL/CDL         &  \textbf{1.00±0.00}/0.99±0.01  & 1.00±0.00/1.00±0.00 &  \textbf{0.98±0.01}/0.96±0.01 \\
                           & ECL/REG         & 0.99±0.01/0.99±0.01  &  \textbf{0.99±0.01}/0.93±0.04 & 0.95±0.01/0.95±0.01 \\ \hline
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
% \centering
\includegraphics[width=1\linewidth]{paper_figs/causal_gragh/causal-graph.pdf}
\caption{The compared causal gragh in the chemical collider environment.}
\vspace{-6pt}
\label{fig:causal_graph}
\end{figure}

\begin{figure}[h]
\centering
\begin{subfigure}{0.49\linewidth}
% \centering
\includegraphics[width=1\linewidth]{paper_figs/prediction/chain.pdf}
% \captionsetup{font=scriptsize}
\caption{Chemical (Chain)}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{paper_figs/prediction/collider.pdf}
% \captionsetup{font=scriptsize}
\caption{Chemical (Collider)}
\label{sub2}
\end{subfigure}
\\
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{paper_figs/prediction/full.pdf}
% \captionsetup{font=scriptsize}
\caption{Chemical (Full)}
\label{sub3}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{paper_figs/prediction/manipulation.pdf}
% \captionsetup{font=scriptsize}
\caption{Manipulation}
\label{sub2}
\end{subfigure}
\caption{Prediction performance (\%) on ID and OOD states of ECL-CIT (ECL-C) and ECL-ASR (ECL-A). The mean score is marked on the top of each bar.}
\vspace{-5mm}
\label{fig:prediction}
\end{figure}

\vspace{-3mm}
\paragraph{Predicting Future States.} 
Given the current state and a sequence of actions, we evaluate the accuracy of each method’s prediction, for states both in and out of distribution (denoted as ID and OOD states). 
We evaluate each method for one step prediction on 5K transitions, for both ID and OOD states. To create OOD states, we change object positions in the chemical environment and marker positions in the manipulation environment to unseen values, followed~\cite{wang2022causal}. 

Figure~\ref{fig:prediction} illustrates the prediction results across four environments.
In the ID settings, our proposed methods, based on both CIT and ASR, achieve performance on par with GNNs and MLPs, while significantly elevating performance in the intricate manipulation environment. 
These findings validate the efficacy of our proposed approach for causal learning. 
For the OOD settings, our method attains comparable performance to the ID setting. These results demonstrate strong generalization and robustness capabilities compared to GNNs and MLPs. Moreover, it outperforms CDL and REG. 
The comprehensive experimental results substantiate the proficiency of our proposed method in accurately uncovering causal relationships and enhancing generalization abilities. 
For full results of causal dynamics learning, please refer to Appendix~\ref{Results of causal dynamics learning} and~\ref{Visualization on the learned causal graphs}.


% \begin{figure}[t]
% % \centering
% \centering
% \includegraphics[width=0.24\linewidth]{paper_figs/reward/Reach.pdf}
% \includegraphics[width=0.24\linewidth]{paper_figs/reward/Pick.pdf}
% \includegraphics[width=0.24\linewidth]{paper_figs/reward/Stack.pdf}
% \includegraphics[width=0.24\linewidth]{paper_figs/reward/physical.pdf}
% \caption{The task learning of episodic reward in three manipulation and pyhsical environments.}
% \label{fig:manipulation}
% \end{figure}


% \subsection{Property Analysis}
\vspace{-3mm}
\section{Related Work}
\vspace{-2mm}
\subsection{Causal Model-based Reinforcement Learning}
MBRL involves training a dynamics model by maximizing the likelihood of collected transitions, known as the world model~\cite{moerland2023model,janner2019trust,nguyen2021temporal,zhao2021consciousness}.
Due to the exclusion of irrelevant factors from the environment through state abstraction, the application of causal inference in MBRL can effectively improve sample efficiency and generalization~\cite{ke2021systematic,mutti2023provably,hwang2023quantized}. 
Wang~\cite{wang2021task} proposes a constraint-based causal dynamics learning that explicitly learns causal dependencies by action-sufficient state representations. 
GRADER~\cite{ding2022generalizing} executes variational inference by regarding the causal graph as a latent variable. CDL~\cite{wang2022causal} is a causal dynamics learning method based on CIT. CDL employs conditional mutual information to compute the causal relationships between different dimensions of states and actions. For additional related work, please refer to Appendix~\ref{Additional Related Works}.
\vspace{-3mm}
\subsection{Empowerment in Reinforcement Learning} 
Empowerment is an intrinsic motivation to improve the controllability over the environment~\cite{klyubin2005empowerment,salge2014empowerment}. This concept is from the information-theoretic framework, wherein actions and future states are viewed as channels for information transmission. In RL, empowerment is applied to uncover more controllable associations between states and actions or skills~\cite{mohamed2015variational,bharadhwaj2022information,choi2021variational, eysenbach2018diversity}. By quantifying the influence of different behaviors on state transitions, empowerment encourages the agent to explore further to enhance its controllability over the system~\cite{leibfried2019unified,seitzer2021causal}. Maximizing empowerment $\max_{\pi} I$ can be used as the learning objective functions, empowering agents to demonstrate intelligent behavior without requiring predefined external goals and model reconstruction. 
% Hence, grounded in the concept of empowerment, we propose a learning framework towards empowerment gain through causal structure learning to improve the controllability for environment. 

\vspace{-3mm}
\section{Conclusion}
% This study proposes an offline model-based adversarial data-augmented optimization method. By combining adversarial game-theoretic techniques with the differential factor, the offline rollout procedure is modeled into a two-player zero-sum game. 
% MORAL proposes an adversarial framework based on the Markov zero-sum game, which achieves alternating biased sampling for data augmentation under stochastic perturbations, with the guarantee of monotonous policy improvement. The framework overcomes the dependency on ensemble models and rollout horizon settings across datasets and tasks in existing model-based offline RL approaches. 
This study propose an method-agnostic framework of empowerment through causal structure learning in MBRL to improve controllability and learning efficiency within environments. We maximize empowerment under causal structure to prioritize controllable information and optimize causal world models to guide downstream task learning. Further, we propose an intrinsic-motivated curiosity reward during task learning to prevent overfitting. 
Extensive experiments across $3$ environments substantiate the remarkable performance. 
For our future work, we will concentrate on extending this framework to disentangle directable behaviors. 
% \clearpage
\bibliographystyle{unsrtnat}
\bibliography{ref}

\clearpage
% \input{appendix}

\clearpage
\newpage
\section*{NeurIPS Paper Checklist}

% %%% BEGIN INSTRUCTIONS %%%
% The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and precede the (optional) supplemental material.  The checklist does NOT count towards the page
% limit. 

% Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
% \begin{itemize}
%     \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
%     \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). 
%    % \item {\bf The papers not including the checklist will be desk rejected.}
% \end{itemize}

% {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

% The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

% IMPORTANT, please:
% \begin{itemize}
%     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},
%     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
%     \item {\bf Do not modify the questions and only use the provided macros for your answers}.
% \end{itemize} 
 

% %%% END INSTRUCTIONS %%%


\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The claims accurately reflect the paper’s contributions and scope.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
    %     \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
    %     \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
    %     \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
    % \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We discuss the limitations of the proposed approach in disentangling agent actions.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
    %     \item The authors are encouraged to create a separate "Limitations" section in their paper.
    %     \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
    %     \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
    %     \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
    %     \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
    %     \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
    %     \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    % \end{itemize}

\item {\bf Theory Assumptions and Proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: All assumptions and proof are included in the paper
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper does not include theoretical results. 
    %     \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
    %     \item All assumptions should be clearly stated or referenced in the statement of any theorems.
    %     \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
    %     \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
    %     \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
    % \end{itemize}

    \item {\bf Experimental Result Reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We have provided the all the information needed to reproduce in the Appendix.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper does not include experiments.
    %     \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
    %     \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
    %     \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
    %     \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
    %     \begin{enumerate}
    %         \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
    %         \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
    %         \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
    %         \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
    %     \end{enumerate}
    % \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We provide anonymous code files in the form of compressed packages.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that paper does not include experiments requiring code.
    %     \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
    %     \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
    %     \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
    %     \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
    %     \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
    %     \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
    %     \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    % \end{itemize}


\item {\bf Experimental Setting/Details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We specify all the training and test details.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper does not include experiments.
    %     \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
    %     \item The full details can be provided either with the code, in appendix, or as supplemental material.
    % \end{itemize}

\item {\bf Experiment Statistical Significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: All figures in experiments include the error bars.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper does not include experiments.
    %     \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
    %     \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
    %     \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
    %     \item The assumptions made should be given (e.g., Normally distributed errors).
    %     \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
    %     \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
    %     \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
    %     \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    % \end{itemize}

\item {\bf Experiments Compute Resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We have introduced all the computer resources in the experiment.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper does not include experiments.
    %     \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
    %     \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
    %     \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
    % \end{itemize}
    
\item {\bf Code Of Ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We make sure to observe the NeurIPS Code of Ethics.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
    %     \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
    %     \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    % \end{itemize}


\item {\bf Broader Impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We discuss both potential positive societal impacts and negative societal impacts of the work performed in the Appendix.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that there is no societal impact of the work performed.
    %     \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
    %     \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
    %     \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
    %     \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
    %     \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    % \end{itemize}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: This paper has no such risks.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper poses no such risks.
    %     \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
    %     \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
    %     \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    % \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Our code is based on the open-sourced repositories, and we cite these works in our paper.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper does not use existing assets.
    %     \item The authors should cite the original paper that produced the code package or dataset.
    %     \item The authors should state which version of the asset is used and, if possible, include a URL.
    %     \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
    %     \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
    %     \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
    %     \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
    %     \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    % \end{itemize}

\item {\bf New Assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We have provided documentation to assist readers. 
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper does not release new assets.
    %     \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
    %     \item The paper should discuss whether and how consent was obtained from people whose asset is used.
    %     \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    % \end{itemize}

\item {\bf Crowdsourcing and Research with Human Subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
    %     \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
    %     \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
    % \end{itemize}

\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.
    % \item[] Guidelines:
    % \begin{itemize}
    %     \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
    %     \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
    %     \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
    %     \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    % \end{itemize}

\end{enumerate}

\end{document}

