\section{Related Work}
\label{related}
\textbf{Copyright infringement in text-to-image models.}
Recent research \cite{carlini2023extracting, somepalli2023diffusion, somepalli2023understanding, gu2023memorization, wang2024replication, wen2024detecting, chiba2024probabilistic} highlights the potential for copyright infringement in text-to-image models. These models are trained on vast datasets that often include copyrighted material, which could inadvertently be memorized by the model during training \cite{vyas2023provable, ren2024copyright}. Additionally, several studies have pointed out that synthetic images generated by these models might violate IP rights due to the inclusion of elements or styles that resemble copyrighted works \cite{poland2023generative, wang2024stronger}. Specifically, models like stable diffusion \cite{Rombach_2022_CVPR} may generate images that bear close resemblances to copyrighted artworks, thus raising concerns about IP infringement \cite{shi2024rlcp}. 

\textbf{Image infringement detection and mitigation.}
The current mainstream infringing image detection methods primarily measure the distance or invariance in pixel or embedding space \cite{carlini2023extracting, somepalli2023diffusion, shi2024rlcp, wang2021bag, wang2024image}. For example, \citeauthor{carlini2023extracting} uses the $L_2$ norm to retrieve memorized images. \citeauthor{somepalli2023diffusion} use SSCD \cite{pizzi2022self}, which learns the invariance of image transformations to identify memorized prompts by comparing the generated images with the original training ones. \citeauthor{zhang2018unreasonable} compare image similarity using the LPIPS distance, which aligns with human perception but has limitations in capturing certain nuances. \citeauthor{wang2024image} transform the replication level of each image replica pair into a probability density function. Studies \cite{wen2024detecting, wang2024evaluating} have shown that these methods have lower generalization capabilities and lack interpretability because they do not fully align with human judgment standards. For copyright infringement mitigation, the current approaches mainly involve machine unlearning to remove the model's memory of copyright information \cite{bourtoule2021machine, nguyen2022survey, kumari2023ablating, zhang2024forget} or deleting duplicated samples from the training data \cite{webster2023duplication, somepalli2023understanding}. These methods often require additional model training. On the other hand, \citeauthor{wen2024detecting} have revealed the overfitting of memorized samples to specific input texts and attempt to modify prompts to mitigate the generation of memorized data. Similarly, \citeauthor{wang2024evaluating} leverage LVLMs to detect copyright information and use this information as negative prompts to suppress the generation of infringing images.













