\input{figs/res1}
\section{Experiments}
\label{experiments}
\subsection{Infringement Identification Experiments}
\label{iden setup}
\textit{Dataset.} Firstly, we use D-Rep dataset \cite{wang2024image}, which contains realistic and generated image pairs scored by human from 0 to 5 according to their similarity. Following its setting, we consider samples with scores of 4 or above as infringement samples, while the rest were non-infringement samples. We use the 4,000 official test images. In addition, we also consider the specific IP infringement. Referring to \cite{ma2024dataset}, we select 10 well-known cartoon characters and 10 artworks from Wikipedia (all can be found in \ref{promptB}). 3 different text-to-image models—Stable Diffusion v2, Kandinsky2-2 \cite{razzhigaev2023kandinsky}, and Stable Diffusion XL \cite{podell2023sdxl}—are used to generate images. For each item, we manually select one infringing image and one non-infringing image from each model, resulting in a total of 60 positive samples and 60 negative samples. %a benchmark \cite{ma2024dataset}



\textit{Baselines.} We select 4 commonly used distance-based image copy detection metrics: $L_2$ norm \cite{carlini2023extracting}, which directly measures pixel-wise differences; LPIPS \cite{zhang2018unreasonable}, which captures perceptual similarity based on deep network features; SSCD \cite{pizzi2022self}, which learns transformation-invariant representations to match generated images with their original training counterparts; and RLCP \cite{shi2024rlcp}, which integrates semantic and perceptual scores for a more comprehensive similarity assessment. Additionally, we compare our approach with the state-of-the-art image copy detection method PDF-Emb \cite{wang2024image}, which models the similariy level of image pairs as a probability density function. We implement all methods using their official open-source code.

\textit{Metric.} Accuracy and F1 score are calculated as the criteria for infringement classification. We perform grid search to select the threshold that achieves the highest F1 score for each method.

\textit{Implementation detail.} Although our approach does not rely on a specific LVLM, we choose GPT-4o \cite{hurst2024gpt} as our agent by default. As for multi-agent debate, we use 3 agents with a maximum of 5 iterations. We use random 3 images from each level (0-5) in the D-Rep training set as human priors to present to the agent.



\textbf{Results.} From the results in Table \ref{table1}, it is evident that traditional image copy detection methods exhibit limitations in the copyright infringement identification task. Our approach significantly outperforms most methods. For the state-of-the-art method, PDF-Emb, which was trained on 36,000 samples from the D-Rep, our performance on D-Rep is slightly inferior. However, its poor performance on the Cartoon IP and Artwork dataset highlights its lack of generalization capability, whereas our method demonstrates equally excellent results across datasets. Figure \ref{fig:res1} illustrates the prediction score distributions for all methods, showing that our approach achieves a relatively more distinct boundary between infringing and non-infringing cases. More results and details on time cost can be found in appendix \ref{resultsA} and \ref{timeA}.

\begin{table}[t]
\caption{Infringement identification results on various dataset (Accuracy $\uparrow$ / F1-Score$\uparrow$ $\times 100$).}
\vspace{-0.1in}
\label{table1}
\vskip 0.15in
\begin{center}
\begin{footnotesize}
\begin{tabular}{l@{\hskip 4pt}c@{\hskip 4pt}c@{\hskip 4pt}c}
\toprule
{Method / Dataset} & D-Rep  & Cartoon & Artwork\\
\midrule
$L_2$ norm & 27.42 / 42.32 &51.67 / 67.42 & 50.00 / 66.67\\
LPIPS &26.85 / 42.29&56.67 / 68.29 &66.67 / 67.74\\
SSCD    &43.65 / 44.37&58.33 / 70.59 & 90.00 / 90.32\\
RLCP    &27.02 / 42.23& 50.00 / 66.67 &50.00 / 66.67\\
PDF-Emb    &\textbf{79.90} / \textbf{57.10}& 61.67 / 71.60 & 81.67 / 80.70\\
{CopyJudge (Ours)}  &75.67 / 51.04 & \textbf{91.67} / \textbf{90.91} & \textbf{93.33} / \textbf{92.86}\\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\vskip -0.3in
\end{table}

% \begin{table}[t]
% \caption{Infringement identification results on Cartoon IP dataset.}
% \label{table2}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{tabular}{lcc}
% \toprule
% {Method} & ACC $\uparrow$& F1-Score $\uparrow$\\
% \midrule
% $L_2$ norm &50.00&60.00\\
% LPIPS &53.33&45.10\\
% SSCD    &46.67&63.64\\
% PDF-Emb    &68.33&67.24\\
% {CopyJudge (Ours)}  &\textbf{92.56}&\textbf{92.04}\\
% \bottomrule
% \end{tabular}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}



\subsection{Infringement Mitigation Experiments}
To thoroughly test the effectiveness of our infringement mitigation, we consider both memorization and specific IP infringement mitigation. 

\subsubsection{Memorization Mitigation}
\citeauthor{wen2024detecting} have found that text-to-image model memorization can be mitigated through simple prompt modifications without additional model training. They iteratively adjust prompt embeddings through gradients based on text-conditional noise prediction to reduce memorization. To compare with it, we conduct our memorization mitigation test using the overfitted Stable Diffusion v1 model trained by them. We use the same 200 memorized images provided by them and record the average Infringement Score identified by CopyJudge before and after the mitigation. At the same time, we use the CLIP Score \cite{radford2021learning} to measure the alignment between the modified images and the original prompt. Since \citeauthor{wen2024detecting} have demonstrated that the generated copy images do not change with variations in latent space, we only use the prompt control (PC) method here. Both \citeauthor{wen2024detecting}’s and our method are set to perform 10 prompt optimization iterations.

\textbf{Results.}
From Table \ref{table3}, our approach could generate images that are less likely to cause infringement while maintaining a comparable, slightly reduced match accuracy. As shown in Figure \ref{fig:res2}, our method effectively avoids the shortcomings of \citeauthor{wen2024detecting}’s method, including failing to mitigate memorization or generating highly deviated images.

\input{figs/res2}




\begin{table}[t]
\caption{Memorization mitigation results.}
\vspace{-0.1in}
\label{table3}
\vskip 0.15in
\begin{center}
\begin{footnotesize}
\begin{tabular}{lcc}
\toprule
{Method} & Infringement Score $\downarrow$ & CLIP Score $\uparrow$\\
\midrule
Raw    &0.869&31.65\\
\citeauthor{wen2024detecting}'s Method  &0.605&\textbf{28.96}\\
PC (ours) &\textbf{0.468}&28.31\\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\vskip -0.1in
\end{table}

\input{figs/res3}



\subsubsection{IP Infringement Mitigation}
Compared to cases of exact replication (memorization), here we consider specific IP infringement, such as imitation of cartoon IPs and artistic elements. Based on whether the input prompt contains direct copyright information, we consider two types of infringement scenarios: \textit{explicit infringement} and \textit{implicit infringement}.

\textbf{Explicit infringement}. This refers to prompts that directly contain copyright information, such as \textit{“Generate an image of Mickey Mouse.”} We use the 20 cartoon and artwork samples collected in section \ref{iden setup} to generate infringing images using Stable Diffusion v2, where the prompt explicitly includes the names or author names of the work.

\textbf{Implicit infringement.} This occurs when the prompt does not explicitly contain copyright information, but the generated image still infringes due to certain infringing expressions. This type of scenario is more applicable to commercial text-to-image models, as they often include content detection modules that can effectively detect copyrighted information and thus reject the request. In this scenario, we use the same IP samples as above, but generate infringing images without any explicit copyright information using DALL·E 3 \cite{Betker2023ImprovingImageGeneration}, which has a safety detection module to reject prompts that trigger it.

\textbf{Automated attack.} Efficiently retrieving or generating infringing prompts has always been a challenge. \citeauthor{kim2024automatic} utilize large models to iteratively generate jailbreak prompts targeting commercial models, thereby inducing them to output copyrighted content. Drawing from it, we use our CopyJudge to generate infringing prompts. In contrast to mitigation, for attack, we only need to use an LVLM to progressively intensify the infringing expressions within the prompt. The prompt is iteratively adjusted, and once the infringement score exceeds 0.8 / 1.0, mitigation is activated, using the current prompt as the starting point.

For explicit infringement, we validate both prompt control (PC) and latent control (LC). For implicit infringement, due to the commercial model DALL·E's inability to customize latents, we only evaluate prompt control. In both scenarios, we compare our approach with the only prompt-based IP infringement mitigation method \cite{wang2024evaluating}, which detects potentially infringed works using LVLM and inputs the detected work information as a negative prompt (Neg-P) into the generative model to avoid infringement. We follow the same settings as in the original paper, but the paper does not address how to handle commercial models that cannot accept negative prompts. For such models, we simply appended the suffix "without [copyrighted work information (e.g., name, author, etc.)]" to the original prompt to simulate a negative prompt.

\begin{table}[t]
\caption{Explicit IP infringement mitigation results on Stable Diffusion v2 (Infringement Score $\downarrow$ / CLIP Score $\uparrow$).}
\vspace{-0.1in}
\label{table4}
\vskip 0.15in
\begin{center}
\begin{footnotesize}
\begin{tabular}{l@{\hskip 20pt}c@{\hskip 20pt}c}
\toprule
{Method / Dataset} & Cartoon & Artwork\\
\midrule
Raw    &0.810 / 32.39 &0.760 / 35.21 \\
Neg-P  &0.597 / 30.91  & 0.616 / 32.07\\
LC (Ours) &0.673 / \textbf{33.19}  &0.665 / \textbf{33.50}\\
PC (Ours) &0.450 / 29.48 &0.417 / 29.46 \\
PC + LC (Ours) &\textbf{0.350} / 28.67 & \textbf{0.353} / 28.82 \\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\vskip -0.2in
\end{table}

\begin{table}[t]
\caption{Implicit IP infringement mitigation results on DALL·E 3 (Infringement Score $\downarrow$ / CLIP Score $\uparrow$).}
\vspace{-0.1in}
\label{table5}
\vskip 0.15in
\begin{center}
\begin{footnotesize}
\begin{tabular}{l@{\hskip 20pt}c@{\hskip 20pt}c}
\toprule
{Method / Dataset} & Cartoon & Artwork\\
\midrule
Raw    &0.783 / 30.54 & 0.697 / 29.80\\
Neg-P &0.710 / \textbf{29.27} & 0.689 / \textbf{29.34}\\
PC (Ours) &\textbf{0.547} / 27.92&\textbf{0.431} / 28.81\\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\vskip -0.2in
\end{table}

\textbf{Results.}
From Tables \ref{table4} and \ref{table5}, it can be seen that our method significantly reduces the likelihood of infringement, both for explicit and implicit infringement, with only a slight drop in CLIP Score. The infringement score after only latent control is relatively higher than after prompt control because retrieving non-infringing latents without changing the prompt is quite challenging. However, we can still effectively reduce the infringement score while maintaining higher image-text matching quality. Figure \ref{fig:res3} shows visualization results, where it can be observed that we avoid the IP infringement while preserving user requirements. Additional results and detailed time cost analysis are provided in Appendix \ref{resultB} and \ref{timeB}.



% \begin{tabular}{lcc}
% \toprule
% {Method} & Infringement Score $\downarrow$ & CLIP Score $\uparrow$\\
% \midrule
% Raw    &0.777&\textbf{35.27}\\
% Prompt Control  &\textbf{0.502}&32.54\\
% \bottomrule
% \end{tabular}
% \end{small}
% \end{center}
% \vskip -0.1in

\begin{table}[t]
\caption{Ablation studies on different identification modules.}
\vspace{-0.1in}
\label{table6}
\vskip 0.15in
\begin{center}
\begin{footnotesize}
\begin{tabular}{c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c|c}
\toprule
LVLM & AFC  & MAD & DEM & ACC / F1-Score $\uparrow$  \\
\cmidrule(lr){1-5}
\checkmark & - & - & - & 73.70 $\pm$ 0.68 / 42.44 $\pm$ 1.08  \\
\checkmark & \checkmark & - & - & 76.50 $\pm$ 1.70 / 44.79 $\pm$ 2.30\\
\checkmark & - & \checkmark & - & 75.00 $\pm$ 1.14 / 43.48 $\pm$ 1.57 \\
\checkmark & - & - & \checkmark & 76.50 $\pm$ 1.14 / 48.35 $\pm$ 1.53 \\
\checkmark & \checkmark & \checkmark & - & 77.50 $\pm$ 1.70 / 47.62 $\pm$ 1.81 \\
\checkmark & \checkmark & \checkmark & \checkmark & \textbf{78.50} $\pm$ 2.00 / \textbf{51.70} $\pm$ 2.19\\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\vskip -0.3in
\end{table}

\subsection{Ablation Studies}
We separately explore the contributions of the abstraction-filtration-comparison test (AFC), multi-agent debate (MAD), and prior demonstration (DEM) to infringement identification. We randomly select 200 samples from the D-Rep dataset for ablation experiments, with each group undergoing five independent runs. As shown in Table 5, each module has a positive impact. Specifically, abstraction-filtration-comparison significantly improves accuracy, the demonstration of human priors effectively enhances the F1 score, and the multi-agent debate further boosts both metrics, ensuring reliable identification results.



