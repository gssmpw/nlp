\section{LVLM for Infringement Mitigation}
\label{mitigation}
\input{figs/mit}
Based on the identification results, we attempt to control the generation model to ensure its outputs do not infringe on copyright. As shown in Figure \ref{fig:mit}, we will discuss two methods separately depending on the control target: prompt control and latent control.
\subsection{Mitigation via LVLM-based Prompt Control}
\citeauthor{wen2024detecting} have proved that slightly modifying the overfitted prompts can effectively avoid generating memorized images. To achieve automated prompt modification aimed at eliminating infringement expressions, we use an LVLM as a prompt optimizer to iteratively adjust the infringing prompt until the final score falls below a threshold $\gamma$. Formally, given the source image $x_{{cr}}$, the generated image $x^t$ and the corresponding prompt $p^t$ at round $t$, control condition $p^c$, historical judgment score $s^t_{f}$, confidence $c^t_{f}$, and rationale $r^t_{f}$, the prompt modifier $\pi_{p}$ is tasked with providing the prompt for the next round, that is: 
\begin{equation}
\pi_{p}(x^t, x_{{cr}}, p^t, p^c, s^t_{f}, c^t_{f}, r^t_{f}) \to p^{t+1}.
\end{equation}
Here, $p^c$ requires the modifier to alter the infringing expression while preserving the original expression as much as possible to avoid generating meaningless images. This mitigation strategy does not require any knowledge of text-to-image models, making it suitable for general black-box scenarios. The algorithm and instruction prompts can be found in appendix \ref{algB} and \ref{promptB}.

\subsection{Mitigation via RL-based Latent Control}
For diffusion models, the output is influenced not only by the prompt but also by the latent noise. Latent noise represents encoded representations of the input that capture essential features in a lower-dimensional space. These latent variables guide the generation process, affecting the finer details of the resulting image. In this section, we propose a reinforcement learning (RL)-based latent control method to mitigate copyright infringement in diffusion-based generative models. Our method involves training an agent to search the input latent variables that yield lower infringement scores, ensuring that the generated outputs do not violate copyright.

Specifically, for latent variable \( z \), we define a policy \( \pi_{\omega} \) parameterized by \( \omega \), allowing us to sample latent noise \( \epsilon \sim \pi_{\omega}(z) \), which follows a Gaussian distribution. The sampled noise \( \epsilon \) is then passed through the pre-trained diffusion decoder \( f \) to produce the image \( x = f(z, \epsilon) \).

To assess the copyright infringement potential of the generated image, we employ our CopyJudge to obtain the infringement score $s_{f}$. Based on this score, we define a reward function:

\begin{equation}
    R(z) = -\log(s_{f}).
\end{equation}

This reward is designed to penalize outputs with higher infringement scores, thus encouraging the generation of non-infringing content. We optimize the parameters \( \omega \) by maximizing the expected reward, \( L(\omega) \), defined as:

\begin{equation}
    L(\omega) = \mathbb{E}_{z \sim \pi_{\omega}}[R(z)].
\end{equation}

The gradient of this objective is computed using the REINFORCE rule \cite{williams1992simple}, which is given by:

\begin{equation}
\nabla_{\omega}L(\omega) = \mathbb{E}_{z \sim \pi_{\omega}}[\nabla_{\omega}\log(\pi_{\omega}) R(z)].
\end{equation}

During the training process, the latent variable $z$ is updated according to the following rule:
\begin{equation}
 z' = z + \beta \epsilon, \quad \epsilon \sim \pi_{\omega}(z),
\end{equation}
where $\beta$ is the step size. We further conduct normalization for the latent variables to maintain stability and prevent extreme deviations. This  RL-based approach allows the agent to explore variations in the latent space, thereby improving its ability to generate non-infringing content. The detailed algorithm can be found in appendix \ref{algB}.
