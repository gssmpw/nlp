\section{LVLM for Infringement Identification}
\label{identification}
\input{figs/iden}
\subsection{Problem Formulation}
Our goal is to determine whether an image infringes the copyright of a known copyrighted image. Based on U.S. law \cite{roth_greeting_cards_wikipedia} and similar laws in other countries, given an image $x$ created with access to the copyrighted image $x_{cr}$, if $x$ and $x_{cr}$ exhibit \textit{substantial similarity}, then $x$ is deemed to infringe the copyright of $x_{cr}$. 
% That is,
% \begin{equation}
%     \text{IsInfringement}(x) = \text{IsSubstantiallySimilar}
% (x, x_\text{cr}),
% \end{equation}
% where $x \in \text{Access}(x_\text{cr})$.

Motivated by this, we aim to establish a substantial similarity identification model $f$, which takes 
$x$ and $x_{cr}$ as inputs and outputs a similarity score $s$. When $s$ exceeds a threshold $\gamma$, we determine that $x$ infringes on $x_{cr}$. This can be defined as:
\begin{equation}
\text{IsInfringement}(x) = \mathbb{I}(f(x, x_{{cr}}) > \gamma),
\end{equation}
where $x$ represents the AI-generated image, and $x_{cr}$ is the corresponding copyrighted image. 

% However, there is no unified understanding of the definition of substantial similarity. In this paper, we focus on three aspects of similarity:
% \begin{itemize}
%     \item \textbf{Memorization}: \citeauthor{carlini2023extracting} found that generative models may almost entirely replicate content from training data, which we consider the strictest form of copyright infringement.
%     \item \textbf{Multilevel Similarity}: \citeauthor{wang2024image} classified image similarity into six levels (0-5) through manual labeling, with varying degrees of similarity between different levels. We adopt this approach and consider a similarity level of 4 or higher as infringement.
%     \item \textbf{IP Resemblance}: We refer to \citeauthor{wang2024evaluating}'s definition, which considers similar copyrighted characters appearing in the image.
% \end{itemize}

\subsection{Abstraction-Filtering-Comparison Framework}
For the process of identifying substantial similarity, we refer to the \textit{abstraction-filtering-comparison} test method \cite{abramson2002promoting}, which has been widely adopted in practical court rulings on infringement cases, and propose an automated infringement identification framework using large vision-language models, as seen in Figure \ref{fig:iden}. In the \textit{copyright expression extraction stage}, we break down images into different elements (such as composition and color patterns), and filter out non-copyrightable parts, leaving copyrighted portions to assess substantial similarity. In the next \textit{copyright infringement determination} stage, multiple LVLMs debate and score the similarity of images given the copyrighted elements, with a final decision made by a meta-judge LVLM based on their consensus. Human priors are injected into the models through few-shot demonstrations to better align with human preferences. 

\textbf{Copyright expression extraction via image-to-text abstraction and filtration.} The process of distinguishing between the fundamental ideas and the specific expressions of an image is a crucial step in determining copyright protection. The core idea of our method is to break down the image into different layers or components, in order to examine the true copyright elements.

First, during the abstraction phase, the image is analyzed and decomposed into its fundamental building blocks. This involves identifying the core elements that contribute to the overall meaning or aesthetic of the image, such as composition, themes, color palette, or other unique visual elements. We can implement this using an LVLM $\pi_{abs}$, defined as:
\begin{equation}
\pi_{abs}(x, x_{{cr}}, p_{abs}) \to (z, z_{cr}),
\end{equation}
where $z$ and $z_\text{cr}$ represent the expressions of $x$ and $x_{{cr}}$ in text after decoupling, respectively.
%\begin{tcolorbox}[colback=gray!10, colframe=gray!80, rounded corners, boxrule=0.1mm]
%\small \textbf{Abstraction Prompt}: ``Please help decompose the given two images into their essential elements and abstract concepts. Identify key components such as composition, themes, color palette, textures, and any unique visual elements. Describe each component in detail."
%\end{tcolorbox}
The goal is to abstract away the superficial features of the image that do not hold significant creative value and instead focus on the underlying concepts that convey the essence of the work.

The next step is filtering. At this stage, elements of the image that are not eligible for copyright protection are removed from consideration. These can include generic concepts, common patterns, functional aspects, or elements derived from public domain sources.
%\begin{tcolorbox}[colback=gray!10, colframe=gray!80, rounded corners, boxrule=0.1mm]
%\small \textbf{Filtering Prompt}: ``Based on the image decomposition, please identify which elements are likely unprotectable under copyright law. These could include generic concepts, common patterns, functional aspects, or elements derived from public domain sources. Then, specify the remaining unique elements that are original and creative."
%\end{tcolorbox}
For example, standard design patterns or commonly used motifs in artwork may not be deemed original enough to warrant protection under copyright law. This process could be defined as:
\begin{equation}
\pi_{fil}(z, z_{{cr}}, p_{fil}) \to (z^c, z^c_{cr}),
\end{equation}
where $z^c$ and $z^c_{cr}$ are the filtered copyright expressions, and $\pi_{fil}$ is another independent LVLM. Filtering helps ensure that only the truly creative, original aspects of the image are preserved for comparison.

The following step is to conduct a comparison of the remaining abstracted elements to assess the degree of similarity. This process helps determine whether the image in question constitutes a derivative work or if it has enough original expression to qualify for copyright protection. To ensure the reliability of the results, we used multi-LVLM debates to perform the comparison and make the final infringement determination.

\textbf{Copyright infringement determination via multi-LVLM comparison.} 
Many studies \cite{du2023improving, chan2023chateval, lakara2024mad, liu2024groupdebate} have shown that multi-agent debate can effectively improve the reliability of responses generated by large models. At this stage, we utilize $N$ LVLMs $\pi_i (i = 1, 2, â€¦, N)$ to communicate with each other and evaluate overall similarity. In addition, to align with human judgment preferences, we employ few-shot in-context learning \cite{dong2022survey, agarwal2024many} by presenting multiple pairs of images scored by humans as references. Specifically, for a single agent $\pi_i$, given inputs including $x$, $x_{cr}$, the filtered copyright expressions $z^c$, $z^c_{cr}$,  an instruction $p_i$, and the set of human reference images $D_h$ and their corresponding score set $S_h$, the agent is required to output a score $s_i \in [0,1]$, confidence $c_i \in [0,1]$, and supporting rationale $r_i$. Specifically, the process can be represented as:
\begin{equation}
\pi_i(x, x_{{cr}}, z^c, z^c_{cr}, p_i, D_h, S_h) \to (s_i, c_i, r_i).
\end{equation}
Following \cite{du2023improving}, we adopt a fully connected synchronous communication debate approach, where each LVLM receives the responses ($s$, $c$, $r$) from the other $N-1$ LVLMs before making the next judgment. This creates a dynamic feedback loop that strengthens the reliability and depth of the analysis, as models adapt their evaluations based on new insights presented by their peers. Each LVLM can adjust its score based on the responses from the other LVLMs or keep it unchanged. We use the following consistency judgment criterion: 
\begin{equation}
\left| s_i - s_j \right| \leq \alpha \quad \forall i, j \in \{1, 2, \dots, N\}.
\end{equation}
If the difference in scores between all LVLMs is less than $\alpha$, we consider that all models have reached a consensus. Additionally, to avoid the models getting stuck in a meaningless loop, we set the maximum number of debate rounds to $M$.

After the debate, the agreed-upon results will be input into an independent meta-judge LVLM $\pi_{f}$, which synthesizes the results to give the final score on whether substantial similarity has occurred, defined as: 
\begin{equation}
\pi_{f}(x, x_{{cr}}, z^c, z^c_{cr}, p_{f}, D_h, S_h, S_m, C_m, R_m) \to (s_{f}, c_{f}, r_{f}),
\end{equation}
where $S_m$, $C_m$, and $R_m$ represent the set of scores, confidence levels, and rationales from $N$ LVLMs after reaching consensus in the $m$-th ($m \leq M$) debate. By combining the strengths of individual agents and iterative debating, the approach could achieve a reliable assessment of visual similarity.
Furthermore, we can determine whether the generated image constitutes infringement based on whether the final similarity score exceeds a specific threshold $\gamma$:
\begin{equation}
\text{IsInfringement}(x) = \mathbb{I}(s_{f}) > \gamma.
\end{equation}
The whole two-stage process ensures a comprehensive and reliable evaluation by integrating multiple perspectives and rigorous analysis. The complete algorithm and instruction prompts can be found in appendix \ref{algA} and \ref{promptA}.




