\section{Related Works}
\subsection{Character Image Animation}
Distinguished from GAN-based\cite{gan,wgan,stylegan} approaches\cite{fomm,mraa,ren2020deep,tpsmm,siarohin2019animating,zhang2022exploring,bidirectionally,everybody}, diffusion-based image animation methods\cite{dreampose,disco,aa,magicanimate,magicpose,mimicmotion,champ,unianimate,tcan,tpc} have emerged as the current research mainstream. As the most representative approach, Animate Anyone\cite{aa} designs its framework based on Stable Diffusion\cite{ldm}, and the denoising network is structured as a 3D UNet\cite{align,animatediff} for temporal modeling. It proposes ReferenceNet, a symmetric UNet\cite{unet} architecture, to preserve appearance consistency and employs pose guider to incorporate skeleton information as driving signals for stable motion control. The Animate Anyone framework achieves robust and generalizable character animation, from which we extensively drew inspiration.

Some works propose improvements upon foundational frameworks. MimicMotion\cite{mimicmotion} leverages pretrained image-to-video capabilities of Stable Video Diffusion\cite{svd}, designing a PoseNet to inject skeleton information. UniAnimate\cite{unianimate} stacks reference images across temporal dimensions, utilizing mamba-based\cite{mamba} temporal modeling techniques. Some works explore different motion control signals. DisCo\cite{disco} and MagicAnimate\cite{magicanimate} utilizes DensePose\cite{densepose} as human body representations. Champ\cite{champ} employs the 3D parametric human model SMPL\cite{smpl}, integrating multi-modal information including depth, normal, and semantic signals derived from SMPL.





%\subsection{Affordance-Aware Image/Video Generation}
%\subsection{Scene/Object-conditioned Generation}
\subsection{Human-environment Affordance Generation}
Numerous studies leverage diffusion models to generate human image or video that contextually integrate with scenes or interactive objects. Some studies\cite{putting,environment-specific,addme,text2place,invi} investigate inserting or inpainting human into given scenes to achieve scene affordance. \cite{putting} applies video self-supervised training to inpaint person into masked region with correct affordances. Text2Place\cite{text2place} aims to place a person in background scenes by learning semantic masks using text guidance for localizing regions. InVi\cite{invi} achieves object insertion by first conducting image inpainting and subsequently generating frames using extended-attention mechanisms.

Several works focus on character animation with scene or object interactions. MovieCharacter\cite{moviecharacter} composites the animated character results into person-removed video sequence. AnchorCrafter\cite{anchorcrafter}, focusing on human-object interaction, first perceives HOI-appearances and injects HOI-motion to generate anchor-style product promotion videos. MIMO\cite{mimo} introduces spatial decomposed diffusion, decomposing videos into human, background and occlusion based on 3D depth and subsequently composing these elements to generate character video.