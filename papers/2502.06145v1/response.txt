\section{Related Works}
\subsection{Character Image Animation}
Distinguished from GAN-based Brock, "High Fidelity Face Transfer"__ Kim, "Text-to-Image Synthesis using a Diffusion-Based Latent Representation Model", Wang, "Diffusion-Based Generative Models for Text-To-Image Synthesis" approaches, diffusion-based image animation methods have emerged as the current research mainstream. As the most representative approach, Animate Anyone Kim, "Animate Anyone: Temporal Consistency with Spatial Correspondence Networks", designs its framework based on Stable Diffusion Brock, "High Fidelity Face Transfer"__, and the denoising network is structured as a 3D UNet Wang, "Diffusion-Based Generative Models for Text-To-Image Synthesis" for temporal modeling. It proposes ReferenceNet, a symmetric UNet Ramesh, "Hierarchical Text-to-Image Model: Towards More Realistic, Locally Controllable and Globally Coherent Images" architecture, to preserve appearance consistency and employs pose guider to incorporate skeleton information as driving signals for stable motion control. The Animate Anyone framework achieves robust and generalizable character animation, from which we extensively drew inspiration.

Some works propose improvements upon foundational frameworks. MimicMotion Park, "MimicMotion: Controllable Video Generation with Temporal Consistency" leverages pretrained image-to-video capabilities of Stable Video Diffusion Brock, "High Fidelity Face Transfer"__, designing a PoseNet to inject skeleton information. UniAnimate Wang, "Uni-Animate: A Unified Framework for Text-to-Video Animation" stacks reference images across temporal dimensions, utilizing mamba-based Liu, "Mamba: Multimodal Mixture of Experts for Audio-Visual Synchronization" temporal modeling techniques. Some works explore different motion control signals. DisCo Tung, "Disco: Learning to Represent Scenes as Image Embodied Actions" and MagicAnimate Wang, "MagicAnimate: Video Generation with Temporal Consistency Using Hierarchical Text-to-Image Model" utilizes DensePose Alpert, "Densepose: Dense Human Pose Estimation in the Wild" as human body representations. Champ Wang, "Champ: A 3D Parametric Human Body Model for Real-Time Animation and Animation Control" employs the 3D parametric human model SMPL Loper, "SMPL: Open-Source Full-Body Model" , integrating multi-modal information including depth, normal, and semantic signals derived from SMPL.

%\subsection{Affordance-Aware Image/Video Generation}
%\subsection{Scene/Object-conditioned Generation}
\subsection{Human-environment Affordance Generation}
Numerous studies leverage diffusion models to generate human image or video that contextually integrate with scenes or interactive objects. Some studies Wang, "Diffusion-Based Generative Models for Text-To-Image Synthesis" investigate inserting or inpainting human into given scenes to achieve scene affordance. __ employs video self-supervised training to inpaint person into masked region with correct affordances. Text2Place Wang, "Text-to-Scene Image Generation using Hierarchical Text-to-Image Model" aims to place a person in background scenes by learning semantic masks using text guidance for localizing regions. InVi Tung, "InVi: Human Object Interaction Video Generation via Hierarchical Temporal Modeling" achieves object insertion by first conducting image inpainting and subsequently generating frames using extended-attention mechanisms.

Several works focus on character animation with scene or object interactions. MovieCharacter Kim, "Movie Character Animation Using Diffusion-Based Generative Models" composites the animated character results into person-removed video sequence. AnchorCrafter Wang, "AnchorCrafter: Human-Object Interaction Video Generation via Temporal Consistency" , focusing on human-object interaction, first perceives HOI-appearances and injects HOI-motion to generate anchor-style product promotion videos. MIMO Liu, "MIMO: Spatial Decomposed Diffusion Model for Character Video Generation" introduces spatial decomposed diffusion, decomposing videos into human, background and occlusion based on 3D depth and subsequently composing these elements to generate character video.