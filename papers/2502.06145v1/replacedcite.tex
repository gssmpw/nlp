\section{Related Works}
\subsection{Character Image Animation}
Distinguished from GAN-based____ approaches____, diffusion-based image animation methods____ have emerged as the current research mainstream. As the most representative approach, Animate Anyone____ designs its framework based on Stable Diffusion____, and the denoising network is structured as a 3D UNet____ for temporal modeling. It proposes ReferenceNet, a symmetric UNet____ architecture, to preserve appearance consistency and employs pose guider to incorporate skeleton information as driving signals for stable motion control. The Animate Anyone framework achieves robust and generalizable character animation, from which we extensively drew inspiration.

Some works propose improvements upon foundational frameworks. MimicMotion____ leverages pretrained image-to-video capabilities of Stable Video Diffusion____, designing a PoseNet to inject skeleton information. UniAnimate____ stacks reference images across temporal dimensions, utilizing mamba-based____ temporal modeling techniques. Some works explore different motion control signals. DisCo____ and MagicAnimate____ utilizes DensePose____ as human body representations. Champ____ employs the 3D parametric human model SMPL____, integrating multi-modal information including depth, normal, and semantic signals derived from SMPL.





%\subsection{Affordance-Aware Image/Video Generation}
%\subsection{Scene/Object-conditioned Generation}
\subsection{Human-environment Affordance Generation}
Numerous studies leverage diffusion models to generate human image or video that contextually integrate with scenes or interactive objects. Some studies____ investigate inserting or inpainting human into given scenes to achieve scene affordance. ____ applies video self-supervised training to inpaint person into masked region with correct affordances. Text2Place____ aims to place a person in background scenes by learning semantic masks using text guidance for localizing regions. InVi____ achieves object insertion by first conducting image inpainting and subsequently generating frames using extended-attention mechanisms.

Several works focus on character animation with scene or object interactions. MovieCharacter____ composites the animated character results into person-removed video sequence. AnchorCrafter____, focusing on human-object interaction, first perceives HOI-appearances and injects HOI-motion to generate anchor-style product promotion videos. MIMO____ introduces spatial decomposed diffusion, decomposing videos into human, background and occlusion based on 3D depth and subsequently composing these elements to generate character video.