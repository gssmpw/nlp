@inproceedings{nehme2023uncertainty,
	title        = {Uncertainty Quantification via Neural Posterior Principal Components},
	author       = {Nehme, Elias and Yair, Omer and Michaeli, Tomer},
	booktitle    = {Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS)},
	year         = {2023},
	organization = {NeurIPS}
}

@article{lloyd1982least,
	title   = {Least squares quantization in PCM},
	author  = {Lloyd, Stuart P},
	journal = {IEEE Transactions on Information Theory},
	year    = {1982}
}

@book{peyre2019computational,
	title     = {Computational Optimal Transport: With Applications to Data Science},
	author    = {Peyr{\'e}, Gabriel and Cuturi, Marco},
	year      = {2019},
	publisher = {Foundations and Trends in Machine Learning}
}

@inproceedings{ronneberger2015unet,
	title     = {U-net: Convolutional networks for biomedical image segmentation},
	author    = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
	year      = {2015},
	publisher = {Springer}
}

@article{falk2019unet,
	title     = {U-net: deep learning for cell counting, detection, and morphometry},
	author    = {Falk, Thorsten and Mai, Dominic and Bensch, Robert and Çiçek, Özgün and Abdulkadir, Ahmed and Marrakchi, Yassine and Böhm, Anton and Deubner, Jan and Jäckel, Zoe and Seiwald, Katharina and others},
	journal   = {Nature Methods},
	year      = {2019},
	publisher = {Springer Nature}
}

@inproceedings{bendel2024pcagan,
	title     = {pca{GAN}: Improving Posterior-Sampling c{GAN}s via Principal Component Regularization},
	author    = {Matthew C Bendel and Rizwan Ahmad and Philip Schniter},
	booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
	year      = {2024}
}

@inproceedings{kutiel2023conformal,
	title     = {Conformal Prediction Masks: Visualizing Uncertainty in Medical Imaging},
	author    = {Kutiel, Gilad and Cohen, Regev and Elad, Michael and Freedman, Daniel and Rivlin, Ehud},
	booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
	year      = {2023}
}

@inproceedings{gal2016dropout,
	title        = {Dropout as a Bayesian approximation: Representing model uncertainty in deep learning},
	author       = {Gal, Yarin and Ghahramani, Zoubin},
	booktitle    = {International Conference on Machine Learning},
	pages        = {1050--1059},
	year         = {2016},
	organization = {PMLR}
}

@inproceedings{blundell2015weight,
	title        = {Weight uncertainty in neural networks},
	author       = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	booktitle    = {International Conference on Machine Learning},
	pages        = {1613--1622},
	year         = {2015},
	organization = {PMLR}
}

@inproceedings{lakshminarayanan2017simple,
	title     = {Simple and scalable predictive uncertainty estimation using deep ensembles},
	author    = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	booktitle = {Advances in Neural Information Processing Systems},
	pages     = {6402--6413},
	year      = {2017}
}

@article{linde1980vector,
	title   = {An algorithm for vector quantizer design},
	author  = {Linde, Yoseph and Buzo, Andres and Gray, Robert M},
	journal = {IEEE Transactions on Communications},
	year    = {1980}
}

@inproceedings{pathak2016context,
	title     = {Context encoders: Feature learning by inpainting},
	author    = {Pathak, Deepak and Kr{\"a}henb{\"u}hl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	year      = {2016}
}

@inproceedings{kohonen1995clvq,
	author    = {Teuvo Kohonen},
	title     = {Learning vector quantization for pattern recognition},
	booktitle = {Proceedings of the International Conference on Neural Networks},
	year      = {1995}
}

@inproceedings{manor2024posterior,
	title        = {On the Posterior Distribution in Denoising: Application to Uncertainty Quantification},
	author       = {Hila Manor and Tomer Michaeli},
	booktitle    = {Proceedings of the International Conference on Learning Representations (ICLR)},
	year         = {2024},
	organization = {ICLR}
}

@inproceedings{angelopoulos2022image,
	title     = {Image-to-image regression with distribution-free uncertainty quantification and applications in imaging},
	author    = {Angelopoulos, Anastasios N and Kohli, Amit P and Bates, Stephen and Jordan, Michael I and Malik, Jitendra and Alshaabi, Thayer and Upadhyayula, Srigokul and Romano, Yaniv},
	booktitle = {International Conference on Machine Learning},
	year      = {2022}
}

@inproceedings{kendall2017uncertainties,
	title     = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
	author    = {Kendall, Alex and Gal, Yarin},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year      = {2017}
}

@techreport{bishop1994mixture,
	title       = {Mixture Density Networks},
	author      = {Bishop, Christopher M.},
	institution = {Neural Computing Research Group, Aston University},
	year        = {1994}
}

@inproceedings{langley00,
	author    = {P. Langley},
	title     = {Crafting Papers on Machine Learning},
	year      = {2000},
	pages     = {1207--1216},
	editor    = {Pat Langley},
	booktitle = {Proceedings of the 17th International Conference
	             on Machine Learning (ICML 2000)},
	address   = {Stanford, CA},
	publisher = {Morgan Kaufmann}
}

@techreport{mitchell80,
	author      = {T. M. Mitchell},
	title       = {The Need for Biases in Learning Generalizations},
	institution = {Computer Science Department, Rutgers University},
	year        = {1980},
	address     = {New Brunswick, MA}
}

@phdthesis{kearns89,
	author = {M. J. Kearns},
	title  = {Computational Complexity of Machine Learning},
	school = {Department of Computer Science, Harvard University},
	year   = {1989}
}

@book{MachineLearningI,
	editor    = {R. S. Michalski and J. G. Carbonell and T.
	             M. Mitchell},
	title     = {Machine Learning: An Artificial Intelligence
	             Approach, Vol. I},
	publisher = {Tioga},
	year      = {1983},
	address   = {Palo Alto, CA}
}

@book{DudaHart2nd,
	author    = {R. O. Duda and P. E. Hart and D. G. Stork},
	title     = {Pattern Classification},
	publisher = {John Wiley and Sons},
	edition   = {2nd},
	year      = {2000}
}

@misc{anonymous,
	title  = {Suppressed for Anonymity},
	author = {Author, N. N.},
	year   = {2021}
}

@incollection{Newell81,
	author    = {A. Newell and P. S. Rosenbloom},
	title     = {Mechanisms of Skill Acquisition and the Law of
	             Practice},
	booktitle = {Cognitive Skills and Their Acquisition},
	pages     = {1--51},
	publisher = {Lawrence Erlbaum Associates, Inc.},
	year      = {1981},
	editor    = {J. R. Anderson},
	chapter   = {1},
	address   = {Hillsdale, NJ}
}


@article{Samuel59,
	author  = {A. L. Samuel},
	title   = {Some Studies in Machine Learning Using the Game of
	           Checkers},
	journal = {IBM Journal of Research and Development},
	year    = {1959},
	volume  = {3},
	number  = {3},
	pages   = {211--229}
}

@book{Bogachev07,
	author     = {Bogachev, V. I.},
	title      = {Measure theory. {V}ol. {I}, {II}},
	publisher  = {Springer-Verlag, Berlin},
	year       = {2007},
	pages      = {Vol. I: xviii+500 pp., Vol. II: xiv+575},
	isbn       = {978-3-540-34513-8; 3-540-34513-2},
	mrclass    = {28-02 (28Axx 28Cxx 46G12 60G42 60G44)},
	mrnumber   = {2267655},
	mrreviewer = {Ren\'e\ L.\ Schilling},
	doi        = {10.1007/978-3-540-34514-5},
	url        = {https://doi.org/10.1007/978-3-540-34514-5}
}

@book{Graf&Luschgy,
	author     = {Graf, Siegfried and Luschgy, Harald},
	title      = {Foundations of quantization for probability distributions},
	series     = {Lecture Notes in Mathematics},
	volume     = {1730},
	publisher  = {Springer-Verlag, Berlin},
	year       = {2000},
	pages      = {x+230},
	isbn       = {3-540-67394-6},
	mrclass    = {60E99 (60F25 62H05 94A12)},
	mrnumber   = {1764176},
	mrreviewer = {Kalev\ P\"arna},
	doi        = {10.1007/BFb0103945},
	url        = {https://doi.org/10.1007/BFb0103945}
}

@article{GLP_Banach,
	author     = {Graf, Siegfried and Luschgy, Harald and Pag\`es, Gilles},
	title      = {Optimal quantizers for {R}adon random vectors in a {B}anach
	              space},
	journal    = {J. Approx. Theory},
	fjournal   = {Journal of Approximation Theory},
	volume     = {144},
	year       = {2007},
	number     = {1},
	pages      = {27--53},
	issn       = {0021-9045,1096-0430},
	mrclass    = {41A65 (46N30 60B11 94A29)},
	mrnumber   = {2287375},
	mrreviewer = {Tuomas\ P.\ Hyt\"onen},
	doi        = {10.1016/j.jat.2006.04.006},
	url        = {https://doi.org/10.1016/j.jat.2006.04.006}
}

@article{Pages98,
	author     = {Pag\`es, Gilles},
	title      = {A space quantization method for numerical integration},
	journal    = {J. Comput. Appl. Math.},
	fjournal   = {Journal of Computational and Applied Mathematics},
	volume     = {89},
	year       = {1998},
	number     = {1},
	pages      = {1--38},
	issn       = {0377-0427,1879-1778},
	mrclass    = {65D30 (65C05 94A34)},
	mrnumber   = {1625987},
	mrreviewer = {Pierre\ Hillion},
	doi        = {10.1016/S0377-0427(97)00190-8},
	url        = {https://doi.org/10.1016/S0377-0427(97)00190-8}
}

@article{procPages,
	author  = {Gilles Pagès},
	title   = {Introduction to vector quantization and its applications for numerics},
	journal = {ESAIM: Proceedings and Surveys},
	year    = {2015},
	volume  = {48},
	number  = {1},
	pages   = {29-79},
	url     = {http://www.sciengine.com/publisher/EDP Sciences/journal/ESAIM: Proceedings and Surveys 48/1/10.1051/proc/201448002},
	doi     = {https://doi.org/10.1051/proc/201448002}
}

@inproceedings{neuralPC,
	author    = {Nehme, Elias and Yair, Omer and Michaeli, Tomer},
	title     = {Uncertainty quantification via neural posterior principal components},
	year      = {2023},
	publisher = {Curran Associates Inc.},
	address   = {Red Hook, NY, USA},
	abstract  = {Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. Yet, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap around a pre-trained model that was trained to minimize the mean square error (MSE), or can be trained from scratch to output both a predicted image and the posterior PCs. We showcase our method on multiple inverse problems in imaging, including denoising, inpainting, super-resolution, colorization, and biological image-to-image translation. Our method reliably conveys instance-adaptive uncertainty directions, achieving uncertainty quantification comparable with posterior samplers while being orders of magnitude faster. Code and examples are available on our webpage https://eliasnehme.github.io/NPPC/.},
	booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
	articleno = {1613},
	numpages  = {14},
	location  = {New Orleans, LA, USA},
	series    = {NIPS '23}
}

@article{PagesBouton,
	author     = {Bouton, Catherine and Pag\`es, Gilles},
	title      = {About the multidimensional competitive learning vector
	              quantization algorithm with constant gain},
	journal    = {Ann. Appl. Probab.},
	fjournal   = {The Annals of Applied Probability},
	volume     = {7},
	year       = {1997},
	number     = {3},
	pages      = {679--710},
	issn       = {1050-5164,2168-8737},
	mrclass    = {60J20 (60F99 60J05 68T05 92B20)},
	mrnumber   = {1459266},
	mrreviewer = {Min\ Ping\ Qian},
	doi        = {10.1214/aoap/1034801249},
	url        = {https://doi.org/10.1214/aoap/1034801249}
}


@inproceedings{kynkaanniemi_improved_2019,
	title     = {Improved {Precision} and {Recall} {Metric} for {Assessing} {Generative} {Models}},
	abstract  = {The ability to automatically estimate the quality and coverage of the samples produced by a generative model is a vital requirement for driving algorithm research. We present an evaluation metric that can separately and reliably measure both of these aspects in image generation tasks by forming explicit, non-parametric representations of the manifolds of real and generated data. We demonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing several illustrative examples where existing metrics yield uninformative or contradictory results. Furthermore, we analyze multiple design variants of StyleGAN to better understand the relationships between the model architecture, training methods, and the properties of the resulting sample distribution. In the process, we identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods and identify an improved method. Finally, we extend our metric to estimate the perceptual quality of individual samples, and use this to study latent space interpolations.},
	urldate   = {2021-05-26},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems} ({NeurIPS} 2019), {Vancouver}, {Canada}.},
	author    = {Kynkäänniemi, Tuomas and Karras, Tero and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	month     = oct,
	year      = {2019},
	note      = {arXiv: 1904.06991},
	keywords  = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}


@inproceedings{heusel_gans_2017,
	title     = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	volume    = {30},
	url       = {https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
	abstract  = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fréchet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	urldate   = {2023-01-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author    = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	year      = {2017}
}


@inproceedings{brock2018large,
	title     = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
	author    = {Andrew Brock and Jeff Donahue and Karen Simonyan},
	booktitle = {International Conference on Learning Representations},
	year      = {2019},
	url       = {https://openreview.net/forum?id=B1xsqj09Fm}
}


@inproceedings{pmlr-v119-cornish20a,
	title     = {Relaxing Bijectivity Constraints with Continuously Indexed Normalising Flows},
	author    = {Cornish, Rob and Caterini, Anthony and Deligiannidis, George and Doucet, Arnaud},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	pages     = {2133--2143},
	year      = {2020},
	editor    = {III, Hal Daumé and Singh, Aarti},
	volume    = {119},
	series    = {Proceedings of Machine Learning Research},
	month     = {13--18 Jul},
	publisher = {PMLR},
	pdf       = {http://proceedings.mlr.press/v119/cornish20a/cornish20a.pdf},
	url       = {https://proceedings.mlr.press/v119/cornish20a.html},
	abstract  = {We show that normalising flows become pathological when used to model targets whose supports have complicated topologies. In this scenario, we prove that a flow must become arbitrarily numerically noninvertible in order to approximate the target closely. This result has implications for all flow-based models, and especially residual flows (ResFlows), which explicitly control the Lipschitz constant of the bijection used. To address this, we propose continuously indexed flows (CIFs), which replace the single bijection used by normalising flows with a continuously indexed family of bijections, and which can intuitively "clean up" mass that would otherwise be misplaced by a single bijection. We show theoretically that CIFs are not subject to the same topological limitations as normalising flows, and obtain better empirical performance on a variety of models and benchmarks.}
}



@inproceedings{pmlr-v189-verine23a,
	title     = {On the expressivity of bi-Lipschitz normalizing
	             flows},
	author    = {Verine, Alexandre and Negrevergne, Benjamin and Chevaleyre, Yann and Rossi, Fabrice},
	booktitle = {Proceedings of The 14th Asian Conference on Machine
	             Learning},
	pages     = {1054--1069},
	year      = {2023},
	editor    = {Khan, Emtiyaz and Gonen, Mehmet},
	volume    = {189},
	series    = {Proceedings of Machine Learning Research},
	month     = {12--14 Dec},
	publisher = {PMLR},
	pdf       = {https://proceedings.mlr.press/v189/verine23a/verine23a.pdf},
	url       = {https://proceedings.mlr.press/v189/verine23a.html},
	abstract  = {An invertible function is bi-Lipschitz if both the
	             function and its inverse have bounded Lipschitz
	             constants. Most state-of-the-art Normalizing Flows
	             are bi-Lipschitz by design or by training to limit
	             numerical errors (among other things). In this
	             paper, we discuss the expressivity of bi-Lipschitz
	             Normalizing Flows and identify several target
	             distributions that are difficult to approximate
	             using such models. Then, we characterize the
	             expressivity of bi-Lipschitz Normalizing Flows by
	             giving several lower bounds on the Total Variation
	             distance between these particularly unfavorable
	             distributions and their best possible
	             approximation. Finally, we show how to use the
	             bounds to adjust the training parameters, and
	             discuss potential remedies.}
}

@inproceedings{dinh2017density,
	title     = {Density estimation using Real {NVP}},
	author    = {Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},
	booktitle = {International Conference on Learning Representations},
	year      = {2017},
	url       = {https://openreview.net/forum?id=HkpbnH9lx}
}


@inproceedings{verine2023precisionrecall,
	title     = {Precision-Recall Divergence Optimization for Generative Modeling with {GAN}s and Normalizing Flows},
	author    = {Alexandre Verine and benjamin negrevergne and Muni Sreenivas Pydi and Yann Chevaleyre},
	booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
	year      = {2023},
	url       = {https://openreview.net/forum?id=SzYHu7EIwZ}
}