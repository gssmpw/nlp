\section{Related Works}
Understanding and modeling uncertainty in deep learning has been a long-standing challenge, with various approaches proposed to quantify and manage uncertainty across tasks.

\textbf{Pixel-based approaches}
Pixel-wise uncertainty quantification methods, such as per-pixel variance heatmaps~\citep{kendall2017uncertainties}, confidence intervals~\citep{angelopoulos2022image}, and conformal prediction masks~\citep{kutiel2023conformal}, often neglect spatial correlations between pixels, resulting in overconfident predictions and limited diversity in reconstructions. Many of these methods rely on Bayesian approaches~\cite {gal2016dropout, blundell2015weight}, which, while theoretically sound, are computationally intensive and difficult to scale. Methods that explore the conditional distribution by producing diverse reconstructions while accounting for pixel correlations are essential.

\textbf{PCA-based approaches}
Building on the idea of exploring conditional distributions, Principal Component Analysis (PCA) provides a form of uncertainty quantization~\citep{nehme2023uncertainty, manor2024posterior} that focuses on variability around the conditional mean \( \mathbb{E}(Y \mid X) \). A similar method has been used to regularize the top principal components of the conditional covariance for enhanced posterior sampling in generation with GANs~\citep{bendel2024pcagan}.
However, PCA-based methods face critical limitations: they capture only linear dependencies and local variability, neglecting global or non-linear structures.
This makes them inherently unsuited for modeling multimodal distributions. Furthermore, PCA assumes unimodal variability, quantifying uncertainty in \( \ell_2 \)-norm around the mean, which restricts their ability to provide diverse reconstructions.
%

\textbf{Ensemble of Experts}
Deep Ensembles~\citep{lakshminarayanan2017simple} provide a scalable alternative by training multiple independent networks, combining their outputs to capture both epistemic and aleatoric uncertainty, demonstrating robustness to dataset shifts and well-calibrated predictions.
Mixture Density Networks (MDN)~\citep{bishop1994mixture} predict the parameters of a mixture distribution, enabling them to capture multimodal outputs and quantify predictive uncertainty effectively. This makes them particularly suitable for tasks where the conditional distribution \( \mathcal{L}(Y \mid X) \) is complex or multimodal.
MDNs approximate complex distributions by predicting the parameters of probabilistic mixtures, while quantization methods achieve a similar goal by representing the distribution with a finite set of adaptive points.


\textbf{Quantization methods}
Lloyd's K-means algorithm~\citep{lloyd1982least} is a foundational quantization method that iteratively minimizes the distortion between data points and their nearest centroids by alternating between assignment and update steps. While effective, it requires exhaustive traversal of the dataset in each iteration, making it computationally expensive for large-scale data. Extensions of K-means, such as Competitive Learning Vector Quantization (CLVQ)~\citep{kohonen1995clvq, PagesBouton}, focus on adaptively learning quantization points through a competitive process.

In the following, we propose a method based on CLVQ, resembling an ensemble of experts, providing a recipe for training diverse networks to quantize simply the conditional distribution.