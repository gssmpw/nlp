\section{Related Work}
Composing language model calls with tool usage into complex applications is now popular, with the help of established language model programming libraries such as DSPy \cite{DBLP:journals/corr/abs-2212-14024,khattab2024dspy}, LangChain \cite{Chase_LangChain_2022}, or TextGrad \cite{yuksekgonul2024textgradautomaticdifferentiationtext}. Language programs compose and integrate knowledge and information flow and are often equipped with additional reasoning capabilities. For instance, RAG \cite{DBLP:conf/nips/LewisPPPKGKLYR020} combines a language model with external retrieval from a corpus for knowledge-intensive tasks; Multi-Agent Debate \cite{du2023improvingfactualityreasoninglanguage} harnesses multiple models debating each other to sharpen mathematical and strategic reasoning; Self-Refine \cite{madaan2023selfrefineiterativerefinementselffeedback} iterates on its own outputs for continuous improvement; and ReAct \cite{yao2023reactsynergizingreasoningacting} integrates step-by-step reasoning with actions to facilitate interactions with external environments. Recent research also focuses on scaling inference-time computation by generating multiple responses and verifying them with specialized models \cite{snell2024scalingllmtesttimecompute, chen2024llmcallsneedscaling}.


Recent work has introduced agentic benchmarks \cite{kapoor2024aiagentsmatter, zhou2024webarenarealisticwebenvironment, yang2024sweagentagentcomputerinterfacesenable, wang2024novelqabenchmarkingquestionanswering}, which are closely related to \tool. As agents are a form of complex language programs, these benchmarks can help examine some design choices in language programs and we intend to add some of them to future iterations of \tool. Unfortunately, such agentic tasks leave plenty of unstudied problem types and are somewhat hard to adapt for asking questions about broader categories of program architectures~ \cite{stroebl2024inferencescalingflawslimits}. For example, SWE-bench \cite{yang2024sweagentagentcomputerinterfacesenable} is helpful in evaluating software engineer-like agents, but it can be difficult to rely on it for testing general inference-time scaling or retrieval-augmented generation methods. %



\begin{table*}[t!]
    \centering \small
    \renewcommand{\arraystretch}{1.2} %
    \setlength{\tabcolsep}{10pt}       %
    \begin{tabular}{L ll}              %
        \toprule
        \textbf{Dataset Category} & \textbf{Datasets \& Tasks} & \textbf{Specialized Programs} \\
        \midrule
        \textbf{Code}         & HumanEval, SWEUnderspecified,  & GeneratorCriticRanker, GeneratorCriticFuser\\
                              & SWEValidity                    & \\
        \midrule
        \textbf{Reasoning}    & Judge, Scone                   & GeneratorCriticRanker, GeneratorCriticFuser\\
        \midrule
        \textbf{Agent}        & AppWorld                     & ReActBaseline, ReActAugmented\\
        \midrule
        \textbf{Knowledge}    & MMLU, HoVer, IReRa, HotpotQA,   & RAGBasedRank, RAG, MultiHopSummarize,\\
                              & HotpotQAConditional, RAGQAArena & SimplifiedBaleen \\
        \midrule
        \textbf{Classification} & HeartDisease, Iris           & CoTBasedVote, GeneratorCriticRanker, \\ & & GeneratorCriticFuser\\
        \midrule
        \textbf{Math}         & MATH, GSM8K                  & GeneratorCriticRanker, GeneratorCriticFuser \\
        \bottomrule
    \end{tabular}
    \caption{Dataset categories, the datasets associated with them, and the specialized language programs evaluated on each different category. We also provide a detailed description for each dataset, task, and language program in \Cref{sec:appendix:bench} and \Cref{sec:appendix:language_program}.}
    \label{tab:datasets_by_category}
\end{table*}