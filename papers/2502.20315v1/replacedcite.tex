\section{Related Work}
Composing language model calls with tool usage into complex applications is now popular, with the help of established language model programming libraries such as DSPy ____, LangChain ____, or TextGrad ____. Language programs compose and integrate knowledge and information flow and are often equipped with additional reasoning capabilities. For instance, RAG ____ combines a language model with external retrieval from a corpus for knowledge-intensive tasks; Multi-Agent Debate ____ harnesses multiple models debating each other to sharpen mathematical and strategic reasoning; Self-Refine ____ iterates on its own outputs for continuous improvement; and ReAct ____ integrates step-by-step reasoning with actions to facilitate interactions with external environments. Recent research also focuses on scaling inference-time computation by generating multiple responses and verifying them with specialized models ____.


Recent work has introduced agentic benchmarks ____, which are closely related to \tool. As agents are a form of complex language programs, these benchmarks can help examine some design choices in language programs and we intend to add some of them to future iterations of \tool. Unfortunately, such agentic tasks leave plenty of unstudied problem types and are somewhat hard to adapt for asking questions about broader categories of program architectures~ ____. For example, SWE-bench ____ is helpful in evaluating software engineer-like agents, but it can be difficult to rely on it for testing general inference-time scaling or retrieval-augmented generation methods. %



\begin{table*}[t!]
    \centering \small
    \renewcommand{\arraystretch}{1.2} %
    \setlength{\tabcolsep}{10pt}       %
    \begin{tabular}{L ll}              %
        \toprule
        \textbf{Dataset Category} & \textbf{Datasets \& Tasks} & \textbf{Specialized Programs} \\
        \midrule
        \textbf{Code}         & HumanEval, SWEUnderspecified,  & GeneratorCriticRanker, GeneratorCriticFuser\\
                              & SWEValidity                    & \\
        \midrule
        \textbf{Reasoning}    & Judge, Scone                   & GeneratorCriticRanker, GeneratorCriticFuser\\
        \midrule
        \textbf{Agent}        & AppWorld                     & ReActBaseline, ReActAugmented\\
        \midrule
        \textbf{Knowledge}    & MMLU, HoVer, IReRa, HotpotQA,   & RAGBasedRank, RAG, MultiHopSummarize,\\
                              & HotpotQAConditional, RAGQAArena & SimplifiedBaleen \\
        \midrule
        \textbf{Classification} & HeartDisease, Iris           & CoTBasedVote, GeneratorCriticRanker, \\ & & GeneratorCriticFuser\\
        \midrule
        \textbf{Math}         & MATH, GSM8K                  & GeneratorCriticRanker, GeneratorCriticFuser \\
        \bottomrule
    \end{tabular}
    \caption{Dataset categories, the datasets associated with them, and the specialized language programs evaluated on each different category. We also provide a detailed description for each dataset, task, and language program in \Cref{sec:appendix:bench} and \Cref{sec:appendix:language_program}.}
    \label{tab:datasets_by_category}
\end{table*}