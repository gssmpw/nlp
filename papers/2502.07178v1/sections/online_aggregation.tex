%!TEX root = ../root.tex
\section{Online Aggregation}
\label{sec:online-agg}

In this section, we present our framework for online aggregation of trajectory predictors. We first define the problem setup (\S\ref{sec:problem-setup}) and spend most of the content on presenting the algorithm for the case of convex loss and stationary environment (\S\ref{sec:convex-stationary}). After that, the extension to nonconvex loss and nonstationary environment becomes natural (\S\ref{sec:nonconvex-loss} and \S\ref{sec:nonstationary}). Fig.~\ref{fig:diagram} overviews our approach.
\vspace{-2mm}
\input{sections/fig-diagram}
\vspace{-6mm}

\subsection{Problem Setup}
\label{sec:problem-setup}
We are given $N$ trajectory predictors treated as black-box experts.
We assume each trajectory predictor outputs a multimodal distribution parameterized as a categorical normal prediction, \ie it predicts a probability vector $p \in \Delta_L$\footnote{$\Delta_n := \{ v \in \Real{n} : v_i \geq 0, i=1,\dots,n, \sum_{i=1}^n v_i =1 \}$ denotes the $n$-D probability simplex.} over $L$ possible modes of future behavior. The $j$-th mode is modeled as a Gaussian distribution with mean $\mu_j \in \mathbb{R}^{K\times3}$ (denoting the $xy$ position and heading $\theta$ of the agent in the next $K$ timesteps) and precision $h_j \in \mathbb{R}^{K\times3}$ (\eg $h_j(k,:) = [1/\sigma_x^2, 1/\sigma_y^2,1/\sigma_{\theta}^2]^T$ represents the reciprocal of the variances in $x,y,\theta$ coordinates at the $k$-th prediction step). 
Let $\Sigma_j$ be the covariance matrix corresponding to $h_j$,
we can write the output of the $i$-th predictor as a Gaussian Mixture Model (GMM):
\bea \label{eq:individual-GMM}
\cbrace{ (p_1^i, \mu_1^i, \Sigma_1^i),\cdots,(p_L^i, \mu_L^i, \Sigma_L^i)  }, \quad i=1,\dots,N
\eea
where $p^i_j \in \Delta_L, \mu^i_j \in \Real{K \times 3}, \Sigma_j^i \in (\pd{3})^K, j=1,\dots,L$ ($\pd{3}$ denotes the set of $3\times 3$ positive definite matrices). Note that we use the superscript $i$ to index the $i$-th expert (trajectory predictor) and the subscript $j$ to index the $j$-th mode in the GMM.

\begin{remark}[Generality]\label{remark:general}
    In principle, our framework only requires each expert to output a distribution of trajectories that can be sampled from, and the distribution needs not be a GMM, see Appendix~\ref{app:generality}. Here we choose to present our framework by assuming each expert outputs a GMM for two reasons. (i) Many recent state-of-the-art trajectory predictors use GMMs to represent multimodal distributions, such as Trajectron++~\cite{salzmann20eccv-trajectron++}, Wayformer~\cite{nayakanti2023wayformer}, and MTR++~\cite{shi2024mtr++}. (ii) GMMs are easier to interpret than arbitrary distributions, and do not require sampling for online aggregation.
\end{remark}

{\bf The Mixture-of-Experts Model}.
Given $N$ predictions of the form~\eqref{eq:individual-GMM}, we aggregate them by forming a mixture-of-experts (MoE) model. Particularly, the MoE model maintains a probability vector $\alpha \in \Delta_N$ where each $\alpha_i$ is used to weigh the importance of the $i$-th expert.  Given $\alpha$, the MoE model outputs a new GMM model that is the mixture of $N \times L$ Gaussians
\bea \label{eq:moe}
\Gamma(\alpha) := \cbrace{ (\alpha_i p^i_j, \mu^i_j, \Sigma^i_j) }_{i=1,j=1}^{i=N,j=L}.
\eea
The goal of online aggregation is then to use online information to dynamically update $\alpha$. 

\subsection{Convex Loss and Stationary Environment}
\label{sec:convex-stationary}
We use OCO to update $\alpha$ in the MoE model~\eqref{eq:moe}. Let $\alpha_t$ be the probability vector ``played'' by the MoE model at time $t$, and $x_t \in \Real{3}$ be the true agent state revealed to the MoE model before it plays the next probability vector. 
A first choice for updating $\alpha_t$ given $x_t$ is to seek an $\alpha$ that \emph{maximizes} the probability of $x_t$ given $\Gamma(\alpha_t)$:
\bea\label{eq:prob-x-given-Gamma}
p(x_t | \Gamma(\alpha_t)) = \sum_{i,j} \alpha_{i,t} p_{j,t}^i g(x_t | \mu^i_{j,t}, \Sigma^i_{j,t})
\eea
where $g(x_t | \mu^i_{j,t}, \Sigma^i_{j,t})$ denotes the probability density function (PDF) of a Gaussian distribution.
In \eqref{eq:prob-x-given-Gamma}, we used the fact that the PDF of a GMM can be written as the weighted sum of the PDFs of its individual Gaussian components. Note that in~\eqref{eq:prob-x-given-Gamma}
% -\eqref{eq:Gaussian-pdf} 
we added a subscript ``$t$'' to $(p,\mu,\Sigma)$ to make explicit their dependence on the time step. 
Finding the weights $\alpha$ that maximizes the probability $p(x_t | \Gamma(\alpha_t))$ is equivalent to finding the weights $\alpha$ that minimizes $-p(x_t | \Gamma(\alpha_t))$:
\bea \label{eq:probability-loss}
\ell_t(\alpha_t) = - p(x_t | \Gamma(\alpha_t)).
\eea 

Clearly, the loss function is linear in $\alpha_t$ and its gradient with respect to $\alpha_t$ is 
\bea\label{eq:gradient-prob}
\nabla_{\alpha_{t,i}} \ell_t(\alpha_t) = - \sum_{j} p^{i}_{j,t} g(x_t | \mu_{j,t}^i, \Sigma^i_{j,t}), \ i=1,\dots,N.
\eea

{\bf The \squint Algorithm}. The de-facto algorithm for online minimization of the linear function $\ell_t(\alpha_t)$ in~\eqref{eq:probability-loss} subject to $\alpha_t$ being a probability vector is the \emph{exponentiated gradient} (EG) algorithm~\cite{orabona19book-modern}. However, applying EG in the context of aggregating trajectory predictors is impractical because EG exhibits slow convergence (see Appendix~\ref{app:eg-squint} for a description of the EG algorithm and results showing its slow convergence). To mitigate this issue,
we introduce \squint~\cite{koolen2015secondorder} as the algorithm to update $\alpha$, whose pseudocode is given in Algorithm~\ref{alg:squint} and follows the official implementation.\footnote{\url{https://blog.wouterkoolen.info/Squint_implementation/post.html}}  Without going too deep into the theory, we provide some high-level insights.

\begin{itemize}
\item  {\bf Goal of learning}. Since the loss function is linear, the OCO problem reduces to \emph{hedging}~\cite{freund1997decision}, or \emph{learning with expert advice}~\cite{orabona19book-modern}. In this setup, the $i$-th entry of the gradient ${g_t} \in \Real{N}$ in line~\ref{line:clip-gradient} (for the discussion here let us first assume $g_t = \tilde{g}_t$) represents the loss of the $i$-th expert, for $i=1,\dots,N$. For online aggregation, one can check that the gradient in~\eqref{eq:gradient-prob} represents the probability of $x_t$ under the $i$-th GMM and measures how well the $i$-th expert predicts the ground truth. By playing a MoE model using the probability vector $\alpha_t$, we suffer a weighted sum of individual expert losses, namely $\alpha_t\tran g_t$. The \emph{instantaneous regret} $r_t$ defined in line~\ref{line:ins-regret} measures the \emph{relative performance} of the MoE model with respect to each expert at time $t$, and the cumulative regret $R_t = \sum_{\tau=1}^t r_t$ computed in line~\ref{line:updateRV} measures the relative performance over the time period until $T$. The goal of the learner (\ie the MoE model) is to minimize each entry of $R_t$, \ie to ensure that the regret with respect to \emph{any individual expert} is small. By doing so, the learner is guaranteed to perform \emph{as well as the best expert} (without knowing which expert is the best \emph{a priori}).

\item {\bf Bayes update}. To achieve the goal of small regret, the \squint algorithm essentially performs Bayes update. To see this, note that the algorithm starts with a prior distribution of expert weights $\alpha_1$, which represents the learner's belief of which expert is the best, before receiving any online information. After the game starts, \squint maintains the regret vector $R_t$, together with the \emph{cumulative regret squared} vector $V_t = \sum_{\tau=1}^t r_\tau \circ r_\tau$ computed in line~\ref{line:updateRV}. The main update rule is written in~\eqref{eq:squint-update-alpha} where the function $\xi(\cdot,\cdot)$ is a potential function called \squint evidence. Note that the crucial difference between \squint and the vanilla EG algorithm (see Appendix~\ref{app:eg-squint}) is that \squint uses second-order information, \ie $V_t$. With this interpretation, we recognize~\eqref{eq:squint-update-alpha} as an instance of Bayes' rule with prior $\alpha_1$ and evidence measured by $\xi$. The detailed proof of why~\eqref{eq:squint-update-alpha} guarantees small regret is presented in the original paper~\cite{koolen2015secondorder} and beyond the scope here due to its mathematical involvement. 

\item {\bf Gradient clipping} \ \ An assumption of the hedging setup is that individual expert losses need to be bounded, which may not hold for our problem. Therefore, in lines~\ref{line:Gt}-\ref{line:clip-gradient}, we employ the gradient clipping method proposed in~\cite{cutkosky2019artificial} to rescale the gradients. The gradient clipping technique has been a popular method in training deep neural networks~\cite{pascanu2012understanding,gorbunov2020stochastic,zhang2020adaptive}.


\end{itemize}

\input{sections/alg-squint}

The \squint algorithm guarantees sublinear regret~\cite[
Theorem 4]{koolen2015secondorder}. The loss function $\ell_t(\cdot)$ is linear in $\alpha$ and $\alpha$ lives in the probability simplex, which implies the optimal $\alpha$ in hindsight must be a one-hot vector that assigns $1$ to the best expert and $0$ to other experts~\cite{convexanalysis}. Therefore, we conclude that \squint guarantees convergence to the best expert in hindsight, as we will empirically show in \S\ref{sec:experiment}.


\subsection{Nonconvex Loss}
\label{sec:nonconvex-loss}


The probability loss~\eqref{eq:probability-loss} intuitively considers the ``average'' performance of the MoE model in the sense that all the Gaussian modes are included (penalized) in computing the loss. In practice, however, we may not care about all modes of the MoE model but rather its top-$k$ modes. Formally, let $\alpha_t$ be the decision of the MoE model at time $t$ and denote $\phi_t=(\phi_{1,t},\dots,\phi_{k,t}) \in [NL]$ as the indices of the top-$k$ modes in the MoE model~\eqref{eq:moe}
\bea \label{eq:argtopk}
\phi_t=(\phi_{1,t},\dots,\phi_{k,t}) = \mathrm{argtopk}\{\alpha_{i,t} p^i_{j,t} \}_{i=1,j=1}^{i=N,j=L},
\eea
\ie we rank the weights of all $NL$ modes --each mode has weight $\alpha_{i,t} p^i_{j,t}$-- and then choose the indices of the largest $k$ modes. It is worth noting that $\phi_t$ is a function of $\alpha_t$. Let $x_t$ be the true agent state revealed at time $t$, the minimum first displacement error (minFRDE$_k$) is computed as 
\bea \label{eq:minFRDEk}
\ell_t(\alpha_t) = \min_{i=1,\dots,k} \cbrace{ \norm{x_t - \mu(\phi_i)} }, 
\eea 
where $\mu(\phi_i)$ denotes the mean of the Gaussian distribution with index $\phi_i$.

The minFRDE$_k$ loss in~\eqref{eq:minFRDEk} is, unfortunately, not differentiable in $\alpha_t$ due to the non-differentiability of the ``$\mathrm{argtopk}$'' operation in~\eqref{eq:argtopk}. To fix this issue, we use the $\mathrm{softsort}$ surrogate introduced in~\cite{prillo2020softsort}, and additionally, employ a smooth approximation of the ``$\min$'' operator, \ie
$
\mathrm{softmin}(x) = -\frac{1}{\beta} \log\left(\sum_{i=0}^k \exp(-\beta x_i)\right) 
$
so that the gradients remain non-zero for all $\alpha_{i,t}$ that do not contain the top-$k$ modes ($\beta>0$ is known as the temperature scaling factor). 

In summary, for the nonconvex and nonsmooth minFRDE$_k$ loss~\eqref{eq:minFRDEk}, we perform a smooth approximation using $\mathrm{softsort}$ and $\mathrm{softmin}$, then we directly apply \squint in Algorithm~\ref{alg:squint}.


\subsection{Nonstationary Environment}
\label{sec:nonstationary}

The OCO framework presented so far (\ie the regret in~\eqref{eq:regret} and the \squint algorithm) implicitly assumes a \emph{static} environment. To see this, note that minimizing the regret~\eqref{eq:regret} makes sense if and only if a good comparator $u$ exists in hindsight, which is typically the assumption of a stationary environment. However, this may not be the case for trajectory prediction. Imagine a vehicle on a road trip from Boston to Pittsburgh, or a driver who usually drives in suburban areas but occasionally goes to the city: in both situations the environments are nonstationary. 

Fortunately, there exists an easy fix, as pointed out by~\cite{zhang2024discountedn}. The intuition there is simple: in nonstationary online learning, the key is to gracefully forget about the past and focus on data in a recent time window. Technically, \cite{zhang2024discountedn} shows any stationary online learning algorithm can be converted to a nonstationary algorithm by choosing a discount factor when computing the loss functions. We borrow this intuition and design a nonstationary algorithm by simply adding a discount factor $\lambda < 1$. In particular, the discounted \squint algorithm replaces line~\ref{line:updateRV} of Algorithm~\ref{alg:squint} with
\bea \label{eq:discounted-squint}
R_{t+1} = \lambda R_t + r_t, \quad V_{t+1} = \lambda^2 V_t + r_t \circ r_t.
\eea 
