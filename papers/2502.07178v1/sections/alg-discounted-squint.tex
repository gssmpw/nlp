%!TEX root = ../root.tex
\begin{algorithm}[hbt!]
\SetKwInput{KwInput}{Input}
\SetKwInput{KwInit}{Initialize}
\caption{Discounted \squint with Cutkosky Clipping}\label{alg:discounted-squint}
\KwInput{A prior distribution of the weights $\alpha_1$ for each expert, a discount factor $\lambda \leq 1$ }
\KwInit{Cumulative regret $R_1=0$, Cumulative regret squared $V_1=0$}
\For{$t=1,2,\dots, T$}{
    Obtain $\ell_t$ from the experts and define the gradient $\tilde{g}_t = \nabla_{\alpha_t}(\ell_t)$ 
    
    Set $G_t = \max (\max(|\tilde{g}_t|), G_{t-1})$, where ``$|\cdot|$'' and ``$\max$'' are applied element-wise \alex{How is max applied element wise? Shouldn't it be taking the max of the vector?}
    
    Define the clipped gradient $g_t = \frac{(\tilde{g}_t / G_t) + 1}{2}$ (element-wise division)
    
    Define the instantaneous regret $r_t = \alpha_t \cdot g_t - g_t$ (element-wise multiplication) \hy{@Alex, I am confused, is $r_t$ a scalar or a vector?} \alex{$r_t$ is a vector, basically each expert $i$ has their own $r^i_t, R^i_t$ and $V^i_t$}
    
    Update 
    \bea\label{eq:squint-update-alpha}
    \alpha_{t+1} = \frac{\alpha_1 \xi(R_t, V_t)}{\sum_i \alpha_1 \xi(R^i_t, V^i_t)}
    \eea
    where $\alpha_1 \xi(R_t, V_t)$ denotes the scalar multiplication of $\alpha_1$ and $\xi(R_t,V_t)$,
    \bea\label{eq:squint-error-function}
    \xi(R,V) = \sqrt{\pi} \exp{\left( \frac{R^2}{4V}\right)} \frac{\text{erfc}(\frac{-R}{2\sqrt{V}})-\text{erfc}(\frac{V-R}{2\sqrt{V}})}{2\sqrt{V}}
    \eea
    and erfc is the complementary error function $\text{erfc}(x) := 1 - \text{erf}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt$
    
    Update $R_{t+1} = \lambda R_{t} + r_t$ and $V_{t+1} = \lambda V_{t} + r_t^2$ \label{line:discounted-updateRV}
}
\end{algorithm}