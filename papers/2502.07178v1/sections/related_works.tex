%!TEX root = ../root.tex
\section{Related Works}
\label{sec:related}

{\bf Trajectory Prediction}. Early works on trajectory prediction focused on rule-based models, operating under the assumption that humans typically conform to a set of rules and customs \cite{6696982}. 
However, the performance of rule-based predictors relies heavily on the completeness and soundness of the predefined rules. Furthermore, the ``rigidity'' of the rules can fail to capture the nuanced and stochastic nature of human driving, leading to an inability to reason about uncertainty and adapt to multimodal outcomes in complex interactions. 
Learning-based methods can address these limitations by leveraging data to train generative models~\cite{salzmann20eccv-trajectron++,chen22cvpr-scept,yuan21iccv-agentformer} that reason about uncertainty and provide multimodal outcomes. 
Various architectures have been explored, \eg recurrent neural networks (RNNs)~\cite{7780479, kamenev2022predictionnet}, transformers~\cite{yuan21iccv-agentformer}, variational auto-encoders (VAEs) \cite{salzmann20eccv-trajectron++}, generative adversarial networks (GANs) \cite{gupta2018social}, and even large language models (LLMs) \cite{bae2024can}.  

{\bf Trajectory Predictor Fusion}. 
Despite their ability to model uncertainty and multimodal behaviors, learned predictors often struggle to generalize to OOD scenarios, where the test distribution differs from the training data.

This has led to recent works combining rule-based and learned predictors in series~\cite{song2021learning, li2023planninginspired, pmlr-v155-li21b} and in parallel~\cite{veer23arxiv-mpf, sun2021complementing, patrikar2024rulefuser}. While our proposed architecture is also in parallel, it does not use a Bayesian method for fusion and instead builds upon online convex optimization.

{\bf Online Learning in Robotics}.
A notable application of online convex optimization (OCO) in robotics is \emph{online control}, which dates back to at least~\cite{agarwal19icml-online}. This approach reformulates online optimal control as an OCO problem, leveraging algorithmic insights from the OCO literature~\cite{hazan22-introduction}. Building on these ideas, adaptive online learning has enabled parameter-free optimizers for lifelong reinforcement learning~\cite{muppidi2024pick}. OCO has also proven valuable for \emph{online adaptive conformal prediction}~\cite{gibbs21-adaptive}, where it helps construct prediction sets that guarantee coverage despite distribution shifts~\cite{zhang2024discountedn,bhatnagar2023improved}. Inspired by these successes, we investigate OCO's application to aggregating trajectory predictors.
