\section{Related Work}
\subsection{Vulnerability and Safety in LVLMs}
Large vision-language models (LVLMs) are vulnerable to various security risks, including susceptibility to malicious prompt attacks ____, which can exploit vision-only ____ or cross-modal ____ inputs to elicit unsafe responses. Prior studies identify two primary attack strategies for embedding harmful content. The first involves encoding harmful text into images using text-to-image generation tools, thereby bypassing safety mechanisms ____. For example, ____ demonstrate how malicious queries embedded in images through typography can evade detection. The second strategy employs gradient-based adversarial techniques to craft images that appear benign to humans but provoke unsafe model outputs ____. These methods leverage minor perturbations or adversarial patches to mislead classifiers ____.

% Beyond adversarial manipulations, LVLMs are also prone to hallucinations and inaccurate responses when encountering unanswerable, deceptive, or maliciously designed queries ____. For instance, ____ show that misleading prompts such as asking about "three dogs" in an image containing only twoâ€”can induce erroneous model outputs. Similarly, ____ employ diffusion models to generate images of harmful activities and then query LVLMs for guidance, exposing vulnerabilities to jailbreaking techniques.


\subsection{Efforts to Safeguard LVLMs}

To mitigate these risks, prior research has explored various alignment strategies, including reinforcement learning from human feedback (RLHF) ____ and fine-tuning LLMs with curated datasets containing both harmful and benign content ____. While effective, these approaches are computationally demanding. Other inference-time defenses include manually engineered safety prompts to specify acceptable behaviors ____, though these approaches frequently fail to generalize across diverse tasks. More recent methods transform visual inputs into textual descriptions for safer processing ____ or employ adaptive warning prompts ____. Additionally, ____ propose multimodal chain-of-thought prompting to enforce safer responses. However, many of these methods overlook intrinsic safety mechanisms within LVLMs, which is the main goal of our work.

\begin{figure*}[!t]   
\centering
        \includegraphics[width=1.05\linewidth]{figures/pipeline13.pdf}
        \caption{Overview of HiddenDetect. We calculate the safety score based on the cosine similarity between the mapped hidden states at the final token position in the vocabulary space of the most safety-aware layers and the constructed refusal vector, enabling effective and efficient safety judgment at inference time.}   
        \label{pipeline}
\end{figure*}