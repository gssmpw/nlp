@article{bagdasaryan2023ab,
  title={(Ab) using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs},
  author={Bagdasaryan, Eugene and Hsieh, Tsung-Yin and Nassi, Ben and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:2307.10490},
  year={2023}
}

@article{bailey2023image,
  title={Image hijacks: Adversarial images can control generative models at runtime},
  author={Bailey, Luke and Ong, Euan and Russell, Stuart and Emmons, Scott},
  journal={arXiv preprint arXiv:2309.00236},
  year={2023}
}

@article{chen2023dress,
  title={Dress: Instructing large vision-language models to align and interact with humans via natural language feedback},
  author={Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay},
  journal={arXiv preprint arXiv:2311.10081},
  year={2023}
}

@article{dong2023robust,
  title={How Robust is Google's Bard to Adversarial Image Attacks?},
  author={Dong, Yinpeng and Chen, Huanran and Chen, Jiawei and Fang, Zhengwei and Yang, Xiao and Zhang, Yichi and Tian, Yu and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2309.11751},
  year={2023}
}

@misc{du2024vlmguarddefendingvlmsmalicious,
      title={VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data}, 
      author={Xuefeng Du and Reshmi Ghosh and Robert Sim and Ahmed Salem and Vitor Carvalho and Emily Lawton and Yixuan Li and Jack W. Stokes},
      year={2024},
      eprint={2410.00296},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.00296}, 
}

@article{fu2023misusing,
  title={Misusing Tools in Large Language Models With Visual Adversarial Examples},
  author={Fu, Xiaohan and Wang, Zihan and Li, Shuheng and Gupta, Rajesh K and Mireshghallah, Niloofar and Berg-Kirkpatrick, Taylor and Fernandes, Earlence},
  journal={arXiv preprint arXiv:2310.03185},
  year={2023}
}

@article{gong2023figstep,
  title={Figstep: Jailbreaking large vision-language models via typographic visual prompts},
  author={Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun},
  journal={arXiv preprint arXiv:2311.05608},
  year={2023}
}

@article{gou2024eyes,
  title={Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation},
  author={Gou, Yunhao and Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu},
  journal={arXiv preprint arXiv:2403.09572},
  year={2024}
}

@misc{jiang2024rapguardsafeguardingmultimodallarge,
      title={RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting}, 
      author={Yilei Jiang and Yingshui Tan and Xiangyu Yue},
      year={2024},
      eprint={2412.18826},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.18826}, 
}

@article{liu2023query,
  title={Query-relevant images jailbreak large multi-modal models},
  author={Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  journal={arXiv preprint arXiv:2311.17600},
  year={2023}
}

@article{liu2024safety,
  title={Safety of Multimodal Large Language Models on Images and Text},
  author={Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  journal={arXiv preprint arXiv:2402.00357},
  year={2024}
}

@article{luo2024jailbreakv,
  title={Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks},
  author={Luo, Weidi and Ma, Siyuan and Liu, Xiaogeng and Guo, Xiaoyu and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2404.03027},
  year={2024}
}

@article{mad_bench,
  title={How Easy is It to Fool Your Multimodal {LLMs}? An Empirical Analysis on Deceptive Prompts},
  author={Qian, Yusu and Zhang, Haotian and Yang, Yinfei and Gan, Zhe},
  journal={arXiv preprint arXiv:2402.13220},
  year={2024}
}

@article{mmsafetybench,
  title={Query-relevant images jailbreak large multi-modal models},
  author={Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  journal={arXiv preprint arXiv:2311.17600},
  year={2023}
}

@article{qi2023visual,
  title={Visual Adversarial Examples Jailbreak Large Language Models},
  author={Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Wang, Mengdi and Mittal, Prateek},
  journal={arXiv preprint arXiv:2306.13213},
  year={2023}
}

@inproceedings{schlarmann2023adversarial,
  title={On the adversarial robustness of multi-modal foundation models},
  author={Schlarmann, Christian and Hein, Matthias},
  booktitle={ICCV},
  year={2023}
}

@article{selfaware,
  title={{MM-SAP}: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception},
  author={Wang, Yuhao and Liao, Yusheng and Liu, Heyang and Liu, Hongcheng and Wang, Yu and Wang, Yanfeng},
  journal={arXiv preprint arXiv:2401.07529},
  year={2024}
}

@article{shayegani2023plug,
  title={Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models},
  author={Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2307.14539},
  year={2023}
}

@article{tu2023many,
  title={How many unicorns are in this image? a safety evaluation benchmark for vision llms},
  author={Tu, Haoqin and Cui, Chenhang and Wang, Zijun and Zhou, Yiyang and Zhao, Bingchen and Han, Junlin and Zhou, Wangchunshu and Yao, Huaxiu and Xie, Cihang},
  journal={arXiv preprint arXiv:2311.16101},
  year={2023}
}

@inproceedings{wan-etal-2024-logicasker,
    title = "{L}ogic{A}sker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
    author = "Wan, Yuxuan  and
      Wang, Wenxuan  and
      Yang, Yiliu  and
      Yuan, Youliang  and
      Huang, Jen-tse  and
      He, Pinjia  and
      Jiao, Wenxiang  and
      Lyu, Michael",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.128/",
    doi = "10.18653/v1/2024.emnlp-main.128",
    pages = "2124--2155",
    abstract = "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite LLMs' prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs' learning of logical rules, with identified reasoning failures ranging from 29{\%} to 90{\%} across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5{\%}. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs' formal reasoning capabilities. We make our code, data, and results publicly available(https://github.com/yxwan123/LogicAsker) to facilitate further research and replication of our findings."
}

@article{wang2024adashield,
  title={Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting},
  author={Wang, Yu and Liu, Xiaogeng and Li, Yu and Chen, Muhao and Xiao, Chaowei},
  journal=ECCV,
  year={2024}
}

@article{wu2023jailbreaking,
  title={Jailbreaking gpt-4v via self-adversarial attacks with system prompts},
  author={Wu, Yuanwei and Li, Xiang and Liu, Yixin and Zhou, Pan and Sun, Lichao},
  journal={arXiv preprint arXiv:2311.09127},
  year={2023}
}

@article{zhao2024evaluating,
  title={On evaluating adversarial robustness of large vision-language models},
  author={Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man Man and Lin, Min},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

