% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J<?xml version='1.0' encoding='UTF-8'?><svg width='474px' height='493px' viewBox='0 0 474 493' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' xmlns:sketch='http://www.bohemiancoding.com/sketch/ns'><!-- Generator: Sketch 3.3.3 (12081) - http://www.bohemiancoding.com/sketch --><title>Slice 4</title><desc>Created with Sketch.</desc><defs><filter x='-50%' y='-50%' width='200%' height='200%' filterUnits='objectBoundingBox' id='filter-1'><feMorphology radius='1' in='SourceAlpha' operator='dilate' result='shadowSpreadOuter1'></feMorphology><feOffset dx='0' dy='0' in='shadowSpreadOuter1' result='shadowOffsetOuter1'></feOffset><feGaussianBlur stdDeviation='2' in='shadowOffsetOuter1' result='shadowBlurOuter1'></feGaussianBlur><feColorMatrix values='0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.35 0' in='shadowBlurOuter1' type='matrix' result='shadowMatrixOuter1'></feColorMatrix><feMerge><feMergeNode in='shadowMatrixOuter1'></feMergeNode><feMergeNode in='SourceGraphic'></feMergeNode></feMerge></filter></defs><g id='Page-1' stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' sketch:type='MSPage'><g id='Beta' sketch:type='MSLayerGroup' transform='translate(-0.706543, 2.562988)'><path d='M109.287849,0.488782773 L109.097497,278.669188 L238.013765,176.714543 L366.292447,279.757493 L366.187107,0.994749316 L474.319908,90.2606126 L474.075962,407.450084 L366.277663,489.561771 L238.013765,383.812854 L108.803653,489.166544 L0.986872821,407.474905 L0.523467653,90.2606126 L109.287849,0.488782773 Z' id='Line' fill='white' filter='url(#filter-1)' sketch:type='MSShapeGroup'></path><path d='M108.775315,406.994109 L108.775315,90.1275084 L1.04188027,1.01666865 L1.04188027,484.941633 L108.775315,406.994109 Z' id='Rectangle-2-Copy-103' fill='#EBCB6B' sketch:type='MSShapeGroup' transform='translate(54.908598, 242.979151) scale(-1, 1) translate(-54.908598, -242.979151) '></path><path d='M473.90304,406.994109 L473.90304,90.1275084 L366.169605,1.01666865 L366.169605,484.941633 L473.90304,406.994109 Z' id='Rectangle-2-Copy-25' fill='#EBCB6B' sketch:type='MSShapeGroup'></path><path d='M238.109007,176.881179 L366.323973,279.936095 L237.5,383.3247 L108.457881,279.936095 L238.109007,176.881179 Z' id='Rectangle-2-Copy-26' stroke='#B7B7B7' fill='#C0E1E7' sketch:type='MSShapeGroup'></path><path d='M237.644947,404.993835 L237.644947,366.561501 L130.43912,279.963167 L1.38975349,383.3247 L130.43912,484.144532 L237.644947,404.993835 Z' id='Rectangle-2-Copy-92' fill='#C0E1E7' sketch:type='MSShapeGroup' transform='translate(119.517350, 382.053849) scale(-1, 1) translate(-119.517350, -382.053849) '></path><path d='M473.642135,404.993835 L473.642135,366.561501 L366.436307,279.963167 L237.386941,383.3247 L366.436307,484.144532 L473.642135,404.993835 Z' id='Rectangle-2-Copy-93' fill='#C0E1E7' sketch:type='MSShapeGroup'></path><path d='M108.775315,406.994109 L108.775315,90.1275084 L1.04188027,1.01666865 L1.04188027,484.941633 L108.775315,406.994109 Z' id='Rectangle-2-Copy-94' fill='#F8D468' sketch:type='MSShapeGroup' transform='translate(54.908598, 242.979151) scale(-1, 1) translate(-54.908598, -242.979151) '></path><path d='M473.90304,406.994109 L473.90304,90.1275084 L366.169605,1.01666865 L366.169605,484.941633 L473.90304,406.994109 Z' id='Rectangle-2-Copy-95' fill-opacity='0.424847147' fill='#F09300' sketch:type='MSShapeGroup'></path><path d='M472.381094,408.536426 L473.308756,369.382188 L366.225098,279.924129 L237.943538,383.697684 L365.873912,489.456538 L472.381094,408.536426 Z' id='Rectangle-2-Copy-97' fill='#F5BB18' sketch:type='MSShapeGroup'></path><path d='M237.644947,408.073957 L237.644947,366.561501 L130.43912,279.963167 L1.38975349,383.3247 L130.43912,488.764714 L237.644947,408.073957 Z' id='Rectangle-2-Copy-96' fill='#FFE38F' sketch:type='MSShapeGroup' transform='translate(119.517350, 384.363940) scale(-1, 1) translate(-119.517350, -384.363940) '></path><path d='M238.019095,176.92765 L366.323973,279.936095 L108.457881,488.487984 L108.457881,352.607709 L108.457881,279.936095 L238.019095,176.92765 Z' id='Rectangle-2-Copy-99' fill='#F7CF5B' sketch:type='MSShapeGroup'></path><path d='M473.90304,406.994109 L473.90304,368.878491 L366.169605,279.767651 L366.169605,489.724244 L473.90304,406.994109 Z' id='Rectangle-2-Copy-100' fill='#F19C17' sketch:type='MSShapeGroup'></path><path d='M238.056597,177.004696 L366.280489,279.936095 L238.0566,383.976842 L108.086816,279.936095 L238.056597,177.004696 Z' id='Rectangle-2-Copy-102' fill='#F7CB4D' sketch:type='MSShapeGroup'></path></g><path d='M-180.5,299.5 L1305.50842,299.5' id='Line-Copy-5' stroke='#E9E9E9' stroke-linecap='square' opacity='0' sketch:type='MSShapeGroup'></path></g></svg>. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{chatgpt,
	author = {OpenAI},
	title = {Introducing ChatGPT},
        journal={Technical Report},
	year = {2022}}


@article{it-with-gpt4,
	author = {Baolin Peng and Chunyuan Li and Pengcheng He and Michel Galley and Jianfeng Gao},
	journal = {arXiv preprint arxiv:2304.03277},
	title = {Instruction Tuning with GPT-4},
	year = {2023}}

@article{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@article{Liu2023VisualIT,
  title={Visual Instruction Tuning},
  author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
  journal={arxiv preprint arxiv:2304.08485},
  year={2023},
}

@article{Zhu2023MiniGPT4EV,
  title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
  journal={arXiv preprint arxiv:2304.10592},
  year={2023},
}

@article{Dai2023InstructBLIPTG,
  title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Albert Li and Pascale Fung and Steven C. H. Hoi},
  journal={arXiv preprint arxiv:2305.06500},
  year={2023},
}

@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}

@article{wang2022super,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}

@article{Bai2023QwenVLAV,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{Zhao2023SVITSU,
  title={SVIT: Scaling up Visual Instruction Tuning},
  author={Bo Zhao and Boya Wu and Tiejun Huang},
  journal={arxiv preprint arxiv:2307.04087},
  year={2023},
}

@article{Li2023M3ITAL,
  title={M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning},
  author={Lei Li and Yuwei Yin and Shicheng Li and Liang Chen and Peiyi Wang and Shuhuai Ren and Mukai Li and Yazheng Yang and Jingjing Xu and Xu Sun and Lingpeng Kong and Qi Liu},
  journal={arxiv preprint arxiv:2306.04387},
  year={2023},
}

@inproceedings{Zhang2017ASO,
  title={A Survey on Multi-Task Learning},
  author={Yu Zhang and Qiang Yang},
  booktitle={TKDE},
  year={2017},
}

@article{Zhu2022UniPerceiverMoELS,
  title={Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs},
  author={Jinguo Zhu and Xizhou Zhu and Wenhai Wang and Xiaohua Wang and Hongsheng Li and Xiaogang Wang and Jifeng Dai},
  journal={arxiv preprint arxiv:},
  year={2022},
  volume={abs/2206.04674},
  url={https://api.semanticscholar.org/CorpusID:249538647}
}

@article{Lu2023AnES,
  title={An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models},
  author={Yadong Lu and Chunyuan Li and Haotian Liu and Jianwei Yang and Jianfeng Gao and Yelong Shen},
  journal={arxiv preprint arxiv:},
  year={2023},
  volume={abs/2309.09958},
  url={https://api.semanticscholar.org/CorpusID:262054900}
}

@inproceedings{Jacobs1991AdaptiveMO,
  title={Adaptive Mixtures of Local Experts},
  author={Robert A. Jacobs and Michael I. Jordan and Steven J. Nowlan and Geoffrey E. Hinton},
  booktitle={Neural Computation},
  year={1991},
}

@inproceedings{Jordan1993HierarchicalMO,
  title={Hierarchical Mixtures of Experts and the EM Algorithm},
  author={Michael I. Jordan and Robert A. Jacobs},
  booktitle={Neural Computation},
  year={1993},
}

@article{Shazeer2017OutrageouslyLN,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Noam M. Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc V. Le and Geoffrey E. Hinton and Jeff Dean},
  journal={arxiv preprint arxiv:1701.06538},
  year={2017},
}

@article{Hu2021LoRALA,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={J. Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  journal={arXiv preprint arxiv:2106.09685},
  year={2021},
}

@article{Houlsby2019ParameterEfficientTL,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  journal={arxiv preprint arxiv:},
  year={2019},
  volume={abs/1902.00751},
  url={https://api.semanticscholar.org/CorpusID:59599816}
}

@inproceedings{Zoph2022STMoEDS,
  title={ST-MoE: Designing Stable and Transferable Sparse Expert Models},
  author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248496391}
}

@article{Agrawal2019nocapsNO,
  title={Nocaps: novel object captioning at scale},
  author={Harsh Agrawal and Karan Desai and Yufei Wang and Xinlei Chen and Rishabh Jain and Mark Johnson and Dhruv Batra and Devi Parikh and Stefan Lee and Peter Anderson},
  journal={ICCV},
  year={2019},
}

@inproceedings{Young2014FromID,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Peter Young and Alice Lai and Micah Hodosh and J. Hockenmaier},
  booktitle={TACL},
  year={2014},
}

@article{Lu2021IconQAAN,
  title={IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning},
  author={Pan Lu and Liang Qiu and Jiaqi Chen and Tony Xia and Yizhou Zhao and Wei Zhang and Zhou Yu and Xiaodan Liang and Song-Chun Zhu},
  journal={arxiv preprint arxiv:2110.13214},
  year={2021},
}

@article{Lu2022LearnTE,
  title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
  author={Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and A. Kalyan},
  journal={arxiv preprint arxiv:2209.09513},
  year={2022},
}

@inproceedings{Liu2022VisualSR,
  title={Visual Spatial Reasoning},
  author={Fangyu Liu and Guy Edward Toh Emerson and Nigel Collier},
  booktitle={TACL},
  year={2022},
}

@inproceedings{Singh2019TowardsVM,
  title={Towards VQA Models That Can Read},
  author={Amanpreet Singh and Vivek Natarajan and Meet Shah and Yu Jiang and Xinlei Chen and Dhruv Batra and Devi Parikh and Marcus Rohrbach},
  booktitle={CVPR},
  year={2019},
}

@article{Alayrac2022FlamingoAV,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={arXiv preprint arxiv:2204.14198},
  year={2022},
}

@article{Li2023BLIP2BL,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven C. H. Hoi},
  journal={arxiv preprint arxiv:2301.12597},
  year={2023},
}

@article{Zhang2023LLaMAAdapterEF,
  title={LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention},
  author={Renrui Zhang and Jiaming Han and Aojun Zhou and Xiangfei Hu and Shilin Yan and Pan Lu and Hongsheng Li and Peng Gao and Yu Jiao Qiao},
  journal={arxiv preprint arxiv:2303.16199},
  year={2023},
}

@inproceedings{Riquelme2021ScalingVW,
  title={Scaling Vision with Sparse Mixture of Experts},
  author={Carlos Riquelme and Joan Puigcerver and Basil Mustafa and Maxim Neumann and Rodolphe Jenatton and Andr{\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},
  booktitle={NeurIPS},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235417196}
}

@article{Wu2022ResidualMO,
  title={Residual Mixture of Experts},
  author={Lemeng Wu and Mengchen Liu and Yinpeng Chen and Dongdong Chen and Xiyang Dai and Lu Yuan},
  journal={arxiv preprint arxiv:2204.09636},
  year={2022},
}

@article{Lepikhin2020GShardSG,
  title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam M. Shazeer and Z. Chen},
  journal={arxiv preprint arxiv:2006.16668},
  year={2020},
}

@inproceedings{Fedus2021SwitchTS,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={William Fedus and Barret Zoph and Noam M. Shazeer},
  booktitle={JMLR},
  year={2021},
}

@article{Mustafa2022MultimodalCL,
  title={Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts},
  author={Basil Mustafa and Carlos Riquelme and Joan Puigcerver and Rodolphe Jenatton and Neil Houlsby},
  journal={arxiv preprint arxiv:2206.02770},
  year={2022},
}

@article{Shen2023ScalingVM,
  title={Scaling Vision-Language Models with Sparse Mixture of Experts},
  author={Sheng Shen and Zhewei Yao and Chunyuan Li and Trevor Darrell and Kurt Keutzer and Yuxiong He},
  journal={arxiv preprint arxiv:2303.07226},
  year={2023},
}

@inproceedings{
liu2023taskcustomized,
title={Task-customized Masked Autoencoder via Mixture of Cluster-conditional Experts},
author={Zhili LIU and Kai Chen and Jianhua Han and Lanqing HONG and Hang Xu and Zhenguo Li and James Kwok},
booktitle={ICLR},
year={2023},
}

@article{Chen2023MixedAF,
  title={Mixed Autoencoder for Self-Supervised Visual Representation Learning},
  author={Kai Chen and Zhili Liu and Lanqing Hong and Hang Xu and Zhenguo Li and Dit-Yan Yeung},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
  pages={22742-22751},
  url={https://api.semanticscholar.org/CorpusID:257834069}
}

@article{Zadouri2023PushingMO,
  title={Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning},
  author={Ted Zadouri and A. Ustun and Arash Ahmadian and Beyza Ermics and Acyr Locatelli and Sara Hooker},
  journal={arxiv preprint arxiv:2309.05444},
  year={2023},
}

@article{Wang2022AdaMixMF,
  title={AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning},
  author={Yaqing Wang and Subhabrata Mukherjee and Xiaodong Liu and Jing Gao and Jianfeng Gao},
  journal={arxiv preprint arxiv:2210.17451},
  year={2022},
}

@inproceedings{Jang2023ExploringTB,
  title={Exploring the Benefits of Training Expert Language Models over Instruction Tuning},
  author={Joel Jang and Seungone Kim and Seonghyeon Ye and Doyoung Kim and Lajanugen Logeswaran and Moontae Lee and Kyungjae Lee and Minjoon Seo},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256627673}
}

@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@inproceedings{sidorov2020textcaps,
  title={Textcaps: a dataset for image captioning with reading comprehension},
  author={Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  year={2014},
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{schwenk2022okvqa,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={ECCV},
  year={2022},
}

@INPROCEEDINGS{8978122,
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle={ICDAR}, 
  title={OCR-VQA: Visual Question Answering by Reading Text in Images}, 
  year={2019},
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{kiela2020hateful,
  title={The hateful memes challenge: Detecting hate speech in multimodal memes},
  author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{xu2017video,
  title={Video Question Answering via Gradually Refined Attention over Appearance and Motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={ACM Multimedia},
  year={2017}
}

@inproceedings{yang2021just,
  title={Just ask: Learning to answer questions from millions of narrated videos},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  booktitle={JMLR},
  year={2008}
}

@article{chen2023minigpt,
  title={Minigpt-v2: large language model as a unified interface for vision-language multi-task learning},
  author={Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2310.09478},
  year={2023}
}

@inproceedings{li2022blip,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      booktitle={ICML},
}

@article{li2023evaluating,
  title={Evaluating object hallucination in large vision-language models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2305.10355},
  year={2023}
}

@article{yu2023mm,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{chen2023gaining,
  title={Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis},
  author={Chen, Kai and Wang, Chunwei and Yang, Kuo and Han, Jianhua and Hong, Lanqing and Mi, Fei and Xu, Hang and Liu, Zhengying and Huang, Wenyong and Li, Zhenguo and others},
  journal={arXiv preprint arXiv:2310.10477},
  year={2023}
}

@inproceedings{liu2022task,
	title = {Task-Customized Self-Supervised Pre-Training with Scalable Dynamic Routing},
	booktitle = {AAAI},
	author = {Liu, Zhili and Han, Jianhua and Chen, Kai and Hong, Lanqing and Xu, Hang and Xu, Chunjing and Li, Zhenguo},
	year = {2022}
}

@inproceedings{chen2021multisiam,
  title={Multisiam: Self-supervised multi-instance siamese representation learning for autonomous driving},
  author={Chen, Kai and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{chen2023mixed,
  title={Mixed Autoencoder for Self-supervised Visual Representation Learning},
  author={Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan},
  booktitle={CVPR},
  year={2023}
}

@article{han2021soda10m,
  title={SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous Driving}, 
  author={Jianhua Han and Xiwen Liang and Hang Xu and Kai Chen and Lanqing Hong and Chaoqiang Ye and Wei Zhang and Zhenguo Li and Xiaodan Liang and Chunjing Xu},
  journal={arXiv preprint arXiv:2106.11118},
  year={2021}
}

@article{li2022coda,
  title={CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving},
  author={Li, Kaican and Chen, Kai and Wang, Haoyu and Hong, Lanqing and Ye, Chaoqiang and Han, Jianhua and Chen, Yukuai and Zhang, Wei and Xu, Chunjing and Yeung, Dit-Yan and others},
  journal={arXiv preprint arXiv:2203.07724},
  year={2022}
}

@article{chen2023integrating,
  title   = {Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt},
  author  = {Chen, Kai and Xie, Enze and Chen, Zhe and Hong, Lanqing and Li, Zhenguo and Yeung, Dit-Yan},
  journal = {arXiv preprint arXiv:2306.04607},
  year    = {2023}
}

@article{gao2023magicdrive,
      title={MagicDrive: Street View Generation with Diverse 3D Geometry Control}, 
      author={Gao, Ruiyuan and Chen, Kai and Xie, Enze and Hong, Lanqing and Li, Zhenguo and Yeung, Dit-Yan and Xu, Qiang},
      year={2023},
      journal={arXiv preprint arXiv:2310.02601},
}

@article{liu2023geomerasing,
      title={Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion Models},
      author={Liu, Zhili and Chen, Kai and Zhang, Yifan and Han, Jianhua and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan and Kwok, James},
      year={2023},
      journal={arXiv preprint arXiv:2310.05873},
}

@inproceedings{zhili2023task,
  title={Task-customized Masked Autoencoder via Mixture of Cluster-conditional Experts},
  author={Zhili, LIU and Chen, Kai and Han, Jianhua and Lanqing, HONG and Xu, Hang and Li, Zhenguo and Kwok, James},
  booktitle={ICLR},
  year={2023}
}

@article{li2023trackdiffusion,
  title={TrackDiffusion: Multi-object Tracking Data Generation via Diffusion Models},
  author={Li, Pengxiang and Liu, Zhili and Chen, Kai and Hong, Lanqing and Zhuge, Yunzhi and Yeung, Dit-Yan and Lu, Huchuan and Jia, Xu},
  year={2023},
  journal={arXiv preprint arXiv:2312.00651},
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@InProceedings{Gou_2023_CVPR,
    author    = {Gou, Yunhao and Ko, Tom and Yang, Hansi and Kwok, James and Zhang, Yu and Wang, Mingxuan},
    title     = {Leveraging per Image-Token Consistency for Vision-Language Pre-Training},
    booktitle = {CVPR},
    year      = {2023},
}

@inproceedings{dou2022empirical,
  title={An empirical study of training end-to-end vision-and-language transformers},
  author={Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and others},
  booktitle={CVPR},
  year={2022}
}

@article{chen2023pali,
  title={PaLI-X: On Scaling up a Multilingual Vision and Language Model},
  author={Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and others},
  journal={arXiv preprint arXiv:2305.18565},
  year={2023}
}

@article{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{ye2023mplug,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.04257},
  year={2023}
}

@article{zhang2023internlm,
  title={Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition},
  author={Zhang, Pan and Wang, Xiaoyi Dong Bin and Cao, Yuhang and Xu, Chao and Ouyang, Linke and Zhao, Zhiyuan and Ding, Shuangrui and Zhang, Songyang and Duan, Haodong and Yan, Hang and others},
  journal={arXiv preprint arXiv:2309.15112},
  year={2023}
}

@article{chen2023sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv preprint arXiv:2311.12793},
  year={2023}
}

@article{gou2023mixture,
  title={Mixture of cluster-conditional lora experts for vision-language instruction tuning},
  author={Gou, Yunhao and Liu, Zhili and Chen, Kai and Hong, Lanqing and Xu, Hang and Li, Aoxue and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu},
  journal={arXiv preprint arXiv:2312.12379},
  year={2023}
}

@article{liu2024safety,
  title={Safety of Multimodal Large Language Models on Images and Text},
  author={Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  journal={arXiv preprint arXiv:2402.00357},
  year={2024}
}


@article{dong2023robust,
  title={How Robust is Google's Bard to Adversarial Image Attacks?},
  author={Dong, Yinpeng and Chen, Huanran and Chen, Jiawei and Fang, Zhengwei and Yang, Xiao and Zhang, Yichi and Tian, Yu and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2309.11751},
  year={2023}
}

@article{qi2023visual,
  title={Visual Adversarial Examples Jailbreak Large Language Models},
  author={Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Wang, Mengdi and Mittal, Prateek},
  journal={arXiv preprint arXiv:2306.13213},
  year={2023}
}

@article{tu2023many,
  title={How many unicorns are in this image? a safety evaluation benchmark for vision llms},
  author={Tu, Haoqin and Cui, Chenhang and Wang, Zijun and Zhou, Yiyang and Zhao, Bingchen and Han, Junlin and Zhou, Wangchunshu and Yao, Huaxiu and Xie, Cihang},
  journal={arXiv preprint arXiv:2311.16101},
  year={2023}
}

@inproceedings{
luo2024an,
title={An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models},
author={Haochen Luo and Jindong Gu and Fengyuan Liu and Philip Torr},
booktitle={ICLR},
year={2024},
}


@inproceedings{schlarmann2023adversarial,
  title={On the adversarial robustness of multi-modal foundation models},
  author={Schlarmann, Christian and Hein, Matthias},
  booktitle={ICCV},
  year={2023}
}

@article{bailey2023image,
  title={Image hijacks: Adversarial images can control generative models at runtime},
  author={Bailey, Luke and Ong, Euan and Russell, Stuart and Emmons, Scott},
  journal={arXiv preprint arXiv:2309.00236},
  year={2023}
}

@article{fu2023misusing,
  title={Misusing Tools in Large Language Models With Visual Adversarial Examples},
  author={Fu, Xiaohan and Wang, Zihan and Li, Shuheng and Gupta, Rajesh K and Mireshghallah, Niloofar and Berg-Kirkpatrick, Taylor and Fernandes, Earlence},
  journal={arXiv preprint arXiv:2310.03185},
  year={2023}
}

@article{liu2023query,
  title={Query-relevant images jailbreak large multi-modal models},
  author={Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  journal={arXiv preprint arXiv:2311.17600},
  year={2023}
}


@article{li2024red,
  title={Red teaming visual language models},
  author={Li, Mukai and Li, Lei and Yin, Yuwei and Ahmed, Masood and Liu, Zhenguang and Liu, Qi},
  journal={arXiv preprint arXiv:2401.12915},
  year={2024}
}

@article{chen2023dress,
  title={Dress: Instructing large vision-language models to align and interact with humans via natural language feedback},
  author={Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay},
  journal={arXiv preprint arXiv:2311.10081},
  year={2023}
}

@article{wu2023jailbreaking,
  title={Jailbreaking gpt-4v via self-adversarial attacks with system prompts},
  author={Wu, Yuanwei and Li, Xiang and Liu, Yixin and Zhou, Pan and Sun, Lichao},
  journal={arXiv preprint arXiv:2311.09127},
  year={2023}
}

@article{wang2024inferaligner,
  title={InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance},
  author={Wang, Pengyu and Zhang, Dong and Li, Linyang and Tan, Chenkun and Wang, Xinghao and Ren, Ke and Jiang, Botian and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2401.11206},
  year={2024}
}

@article{pi2024mllm,
  title={MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance},
  author={Pi, Renjie and Han, Tianyang and Xie, Yueqi and Pan, Rui and Lian, Qing and Dong, Hanze and Zhang, Jipeng and Zhang, Tong},
  journal={arXiv preprint arXiv:2401.02906},
  year={2024}
}

@article{chen2023can,
  title={Can language models be instructed to protect personal information?},
  author={Chen, Yang and Mendes, Ethan and Das, Sauvik and Xu, Wei and Ritter, Alan},
  journal={arXiv preprint arXiv:2310.02224},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}

@misc{taori2023stanford,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}


@article{wang2023aligning,
  title={Aligning large language models with human: A survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2307.12966},
  year={2023}
}

@article{liu2023languages,
  title={Languages are rewards: Hindsight finetuning using human feedback},
  author={Liu, Hao and Sferrazza, Carmelo and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2302.02676},
  year={2023}
}

@article{scheurer2022training,
  title={Training language models with natural language feedback},
  author={Scheurer, J{\'e}r{\'e}my and Campos, Jon Ander and Chan, Jun Shern and Chen, Angelica and Cho, Kyunghyun and Perez, Ethan},
  journal={arXiv preprint arXiv:2204.14146},
  year={2022}
}

@inproceedings{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle={NeruIPS},
  year={2022}
}


@article{yuan2023rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}

@article{dong2023raft,
  title={Raft: Reward ranked finetuning for generative foundation model alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  journal={arXiv preprint arXiv:2304.06767},
  year={2023}
}

@inproceedings{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  booktitle={NeurIPS},
  year={2023}
}

@article{dai2023safe,
  title={Safe rlhf: Safe reinforcement learning from human feedback},
  author={Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong},
  journal={arXiv preprint arXiv:2310.12773},
  year={2023}
}

@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}

@article{sun2023safety,
  title={Safety Assessment of Chinese Large Language Models},
  author={Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie},
  journal={arXiv preprint arXiv:2304.10436},
  year={2023}
}

@article{saunders2022self,
  title={Self-critiquing models for assisting human evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={arXiv preprint arXiv:2206.05802},
  year={2022}
}

@article{liu2023mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{wu2023v,
  title={V*: Guided visual search as a core mechanism in multimodal llms},
  author={Wu, Penghao and Xie, Saining},
  journal={arXiv preprint arXiv:2312.14135},
  year={2023}
}

@misc{dubois2023alpacafarm,
  title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}, 
  author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  year={2023},
  eprint={2305.14387},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{zhang2023multicot,
  title={Multimodal Chain-of-Thought Reasoning in Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  journal={arXiv preprint arXiv:2302.00923},
  year={2023}
}

@article{he2022synthetic,
  title={Is synthetic data from generative models ready for image recognition?},
  author={He, Ruifei and Sun, Shuyang and Yu, Xin and Xue, Chuhui and Zhang, Wenqing and Torr, Philip and Bai, Song and Qi, Xiaojuan},
  journal={arXiv preprint arXiv:2210.07574},
  year={2022}
}

@article{tian2023stablerep,
  title={StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners},
  author={Tian, Yonglong and Fan, Lijie and Isola, Phillip and Chang, Huiwen and Krishnan, Dilip},
  journal={arXiv preprint arXiv:2306.00984},
  year={2023}
}

@article{lu2023self,
  title={SELF: Language-Driven Self-Evolution for Large Language Model},
  author={Lu, Jianqiao and Zhong, Wanjun and Huang, Wenyong and Wang, Yufei and Mi, Fei and Wang, Baojun and Wang, Weichao and Shang, Lifeng and Liu, Qun},
  journal={arXiv preprint arXiv:2310.00533},
  year={2023}
}

@article{wang2024detdiffusion,
  title={DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception},
  author={Wang, Yibo and Gao, Ruiyuan and Chen, Kai and Zhou, Kaiqiang and Cai, Yingjie and Hong, Lanqing and Li, Zhenguo and Jiang, Lihui and Yeung, Dit-Yan and Xu, Qiang and Zhang, Kai},
  journal={arXiv preprint arXiv:2403.13304},
  year={2024}
}

@article{li2024automated,
  title={Automated evaluation of large vision-language models on self-driving corner cases},
  author={Li, Yanze and Zhang, Wenhua and Chen, Kai and Liu, Yanxin and Li, Pengxiang and Gao, Ruiyuan and Hong, Lanqing and Tian, Meng and Zhao, Xinhai and Li, Zhenguo and others},
  journal={arXiv preprint arXiv:2404.10595},
  year={2024}
}

@article{liu2024mixture,
  title={Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment},
  author={Liu, Zhili and Gou, Yunhao and Chen, Kai and Hong, Lanqing and Gao, Jiahui and Mi, Fei and Zhang, Yu and Li, Zhenguo and Jiang, Xin and Liu, Qun and others},
  journal={arXiv preprint arXiv:2405.00557},
  year={2024}
}

@article{gao2024magicdrive3d,
  title={MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes},
  author={Gao, Ruiyuan and Chen, Kai and Li, Zhihao and Hong, Lanqing and Li, Zhenguo and Xu, Qiang},
  journal={arXiv preprint arXiv:2405.14475},
  year={2024}
}

@article{zhao2024evaluating,
  title={On evaluating adversarial robustness of large vision-language models},
  author={Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man Man and Lin, Min},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lu2023less,
  title={Less is More: Understanding Word-level Textual Adversarial Attack via n-gram Frequency Descend},
  author={Lu, Ning and Liu, Shengcai and Zhang, Zhirui and Wang, Qi and Liu, Haifeng and Tang, Ke},
  journal={arXiv preprint arXiv:2302.02568},
  year={2023}
}

@article{wang2024adashield,
  title={Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting},
  author={Wang, Yu and Liu, Xiaogeng and Li, Yu and Chen, Muhao and Xiao, Chaowei},
  journal=ECCV,
  year={2024}
}

@article{zhang2024benchmarking,
  title={Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study},
  author={Zhang, Yichi and Huang, Yao and Sun, Yitong and Liu, Chang and Zhao, Zhe and Fang, Zhengwei and Wang, Yifan and Chen, Huanran and Yang, Xiao and Wei, Xingxing and others},
  journal={arXiv preprint arXiv:2406.07057},
  year={2024}
}

@article{zhao2024first,
  title={The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?},
  author={Zhao, Qinyu and Xu, Ming and Gupta, Kartik and Asthana, Akshay and Zheng, Liang and Gould, Stephen},
  journal=ECCV,
  year={2024}
}

@article{tenney2019bert,
  title={BERT rediscovers the classical NLP pipeline},
  author={Tenney, I},
  journal={arXiv preprint arXiv:1905.05950},
  year={2019}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@misc{zong2024safety,
  title={Safety fine-tuning at (almost) no cost: A baseline for vision large language models},
  author={Zong, Yongshuo and Bohdal, Ondrej and Yu, Tingyang and Yang, Yongxin and Hospedales, Timothy},
  year={2024}
}

@article{luo2024jailbreakv,
  title={Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks},
  author={Luo, Weidi and Ma, Siyuan and Liu, Xiaogeng and Guo, Xiaoyu and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2404.03027},
  year={2024}
}


@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{park2023linear,
  title={The linear representation hypothesis and the geometry of large language models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  journal={arXiv preprint arXiv:2311.03658},
  year={2023}
}

@article{wang2024concept,
  title={Concept algebra for (score-based) text-controlled generative models},
  author={Wang, Zihao and Gui, Lin and Negrea, Jeffrey and Veitch, Victor},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{nanda2023emergent,
  title={Emergent linear representations in world models of self-supervised sequence models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2309.00941},
  year={2023}
}

@inproceedings{qi2024visual,
  title={Visual adversarial examples jailbreak aligned large language models},
  author={Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Henderson, Peter and Wang, Mengdi and Mittal, Prateek},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={19},
  pages={21527--21536},
  year={2024}
}

@article{niu2024jailbreaking,
  title={Jailbreaking attack against multimodal large language model},
  author={Niu, Zhenxing and Ren, Haodong and Gao, Xinbo and Hua, Gang and Jin, Rong},
  journal={arXiv preprint arXiv:2402.02309},
  year={2024}
}

@article{kuhn2013introduction,
  title={An introduction to feature selection},
  author={Kuhn, Max and Johnson, Kjell and Kuhn, Max and Johnson, Kjell},
  journal={Applied predictive modeling},
  pages={487--519},
  year={2013},
  publisher={Springer}
}

@article{joulin2016bag,
  title={Bag of tricks for efficient text classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.01759},
  year={2016}
}





@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@inproceedings{rombach2021highresolution,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  year={2022}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{llava_v1,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{mmsafetybench,
  title={Query-relevant images jailbreak large multi-modal models},
  author={Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  journal={arXiv preprint arXiv:2311.17600},
  year={2023}
}

@article{selfaware,
  title={{MM-SAP}: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception},
  author={Wang, Yuhao and Liao, Yusheng and Liu, Heyang and Liu, Hongcheng and Wang, Yu and Wang, Yanfeng},
  journal={arXiv preprint arXiv:2401.07529},
  year={2024}
}

@inproceedings{slobodkin2023curious,
  title={The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models},
  author={Slobodkin, Aviv and Goldman, Omer and Caciularu, Avi and Dagan, Ido and Ravfogel, Shauli},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={3607--3625},
  year={2023}
}

@article{qian2024towards,
  title={Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models},
  author={Qian, Chen and Zhang, Jie and Yao, Wei and Liu, Dongrui and Yin, Zhenfei and Qiao, Yu and Liu, Yong and Shao, Jing},
  journal={arXiv preprint arXiv:2402.19465},
  year={2024}
}

@article{gurnee2023language,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@article{yang2023dawn,
  title={The dawn of {LMMs}: Preliminary explorations with {GPT-4V}(ision)},
  author={Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv preprint arXiv:2309.17421},
  xvolume={9},
  xnumber={1},
  xpages={1},
  year={2023}
}

@article{pope_benchmark,
  title={Evaluating object hallucination in large vision-language models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2305.10355},
  year={2023}
}

@article{mathvista,
  title={{MathVista}: Evaluating mathematical reasoning of foundation models in visual contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.02255},
  year={2023}
}

@article{mad_bench,
  title={How Easy is It to Fool Your Multimodal {LLMs}? An Empirical Analysis on Deceptive Prompts},
  author={Qian, Yusu and Zhang, Haotian and Yang, Yinfei and Gan, Zhe},
  journal={arXiv preprint arXiv:2402.13220},
  year={2024}
}

@inproceedings{lavis,
    title = "{LAVIS}: A One-stop Library for Language-Vision Intelligence",
    author = "Li, Dongxu  and
      Li, Junnan  and
      Le, Hung  and
      Wang, Guangsen  and
      Savarese, Silvio  and
      Hoi, Steven C.H.",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
    xmonth = jul,
    year = "2023",
    xaddress = "Toronto, Canada",
    xpublisher = "Association for Computational Linguistics",
    xurl = "https://aclanthology.org/2023.acl-demo.3",
    xpages = "31--41",
    xabstract = "We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.",
}

@misc{instructblip,
      title={{InstructBLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      journal={arXiv preprint arXiv:2305.06500},
      year={2023}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{durmus2023measure,
  title={Towards measuring the representation of subjective global opinions in language models},
  author={Durmus, Esin and Nyugen, Karina and Liao, Thomas I and Schiefer, Nicholas and Askell, Amanda and Bakhtin, Anton and Chen, Carol and Hatfield-Dodds, Zac and Hernandez, Danny and Joseph, Nicholas and others},
  journal={arXiv preprint arXiv:2306.16388},
  year={2023}
}

@article{tjuatja2023llms,
  title={Do {LLMs} exhibit human-like response biases? a case study in survey design},
  author={Tjuatja, Lindia and Chen, Valerie and Wu, Sherry Tongshuang and Talwalkar, Ameet and Neubig, Graham},
  journal={arXiv preprint arXiv:2311.04076},
  year={2023}
}

@inproceedings{gurari2018vizwiz,
  title={{VizWiz} grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3608--3617},
  year={2018}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  xorganization={PMLR}
}

@article{llava_1_5,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}

@article{ye2023mplug2,
  title={{mPLUG-Owl2}: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.04257},
  year={2023}
}

@article{llama_adapter_v2,
  title={{LLaMA-Adapter v2}: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@article{multimodalgpt,
      title={{MultiModal-GPT}: A Vision and Language Model for Dialogue with Humans}, 
      author={Tao Gong and Chengqi Lyu and Shilong Zhang and Yudong Wang and Miao Zheng and Qian Zhao and Kuikun Liu and Wenwei Zhang and Ping Luo and Kai Chen},
      journal={arXiv preprint arXiv:2305.04790},
      year={2023}
}

@article{minigpt4,
  title={{MiniGPT-4}: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{russakovsky2015imagenet,
  title={{ImageNet}: large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International Journal of Computer Vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{coco,
  title={{Microsoft COCO}: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{team2023gemini,
  title={{Gemini}: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{mllm_protector,
  title={{MLLM-Protector}: Ensuring {MLLM}'s Safety without Hurting Performance},
  author={Pi, Renjie and Han, Tianyang and Xie, Yueqi and Pan, Rui and Lian, Qing and Dong, Hanze and Zhang, Jipeng and Zhang, Tong},
  journal={arXiv preprint arXiv:2401.02906},
  year={2024}
}

@article{ging2024open,
  title={Open-ended {VQA} benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy},
  author={Ging, Simon and Bravo, Mar{\'\i}a A and Brox, Thomas},
  journal={arXiv preprint arXiv:2402.07270},
  year={2024}
}

@article{hu2021lora,
  title={{LoRA}: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{podell2023sdxl,
  title={{SDXL}: Improving latent diffusion models for high-resolution image synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  journal={arXiv preprint arXiv:2307.01952},
  year={2023}
}


@article{llama1,
  title={{LLaMA}: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{cobra,
  title={Cobra: Extending mamba to multi-modal large language model for efficient inference},
  author={Zhao, Han and Zhang, Min and Zhao, Wei and Ding, Pengxiang and Huang, Siteng and Wang, Donglin},
  journal={arXiv preprint arXiv:2403.14520},
  year={2024}
}

@article{discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@article{li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to AI transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@misc{alon2023detecting,
      title={Detecting Language Model Attacks with Perplexity}, 
      author={Gabriel Alon and Michael Kamfonas},
      year={2023},
      eprint={2308.14132},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.14132}, 
}

@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{hu2024gradientcuff,
      title={Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes}, 
      author={Xiaomeng Hu and Pin-Yu Chen and Tsung-Yi Ho},
      year={2024},
      eprint={2403.00867},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.00867}, 
}

@misc{fares2024mirror,
      title={MirrorCheck: Efficient Adversarial Defense for Vision-Language Models}, 
      author={Samar Fares and Klea Ziu and Toluwani Aremu and Nikita Durasov and Martin Takáč and Pascal Fua and Karthik Nandakumar and Ivan Laptev},
      year={2024},
      eprint={2406.09250},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.09250}, 
}

@misc{xu2024cross,
      title={Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models}, 
      author={Yue Xu and Xiuyuan Qi and Zhan Qin and Wenjie Wang},
      year={2024},
      eprint={2407.21659},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.21659}, 
}

@misc{zhang2024jailguard,
      title={JailGuard: A Universal Detection Framework for LLM Prompt-based Attacks}, 
      author={Xiaoyu Zhang and Cen Zhang and Tianlin Li and Yihao Huang and Xiaojun Jia and Ming Hu and Jie Zhang and Yang Liu and Shiqing Ma and Chao Shen},
      year={2024},
      eprint={2312.10766},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2312.10766}, 
}

@misc{du2024vlmguard,
      title={VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data}, 
      author={Xuefeng Du and Reshmi Ghosh and Robert Sim and Ahmed Salem and Vitor Carvalho and Emily Lawton and Yixuan Li and Jack W. Stokes},
      year={2024},
      eprint={2410.00296},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.00296}, 
}


@misc{jiang2024rapguardsafeguardingmultimodallarge,
      title={RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting}, 
      author={Yilei Jiang and Yingshui Tan and Xiangyu Yue},
      year={2024},
      eprint={2412.18826},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.18826}, 
}

@misc{li2024unveilingbackboneoptimizercouplingbias,
      title={Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning}, 
      author={Siyuan Li and Juanxi Tian and Zedong Wang and Luyuan Zhang and Zicheng Liu and Weiyang Jin and Yang Liu and Baigui Sun and Stan Z. Li},
      year={2024},
      eprint={2410.06373},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.06373}, 
}

@misc{röttger2024xstest,
      title={XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models}, 
      author={Paul Röttger and Hannah Rose Kirk and Bertie Vidgen and Giuseppe Attanasio and Federico Bianchi and Dirk Hovy},
      year={2024},
      eprint={2308.01263},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.01263}, 
}

@misc{zhou2024alignmentjailbreak,
      title={How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States}, 
      author={Zhenhong Zhou and Haiyang Yu and Xinghua Zhang and Rongwu Xu and Fei Huang and Yongbin Li},
      year={2024},
      eprint={2406.05644},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.05644}, 
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}@inproceedings{du2024sal,
  title={How Does Unlabeled Data Provably Help Out-of-Distribution Detection?},
  author={Du, Xuefeng and Fang, Zhen and Diakonikolas, Ilias and Li, Yixuan},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2024}
}
@inproceedings{du2021save,
  title={How to Save your Annotation Cost for Panoptic Segmentation?},
  author={Du, Xuefeng and Jiang, ChenHan and Xu, Hang and Zhang, Gengwei and Li, Zhenguo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={2},
  pages={1282--1290},
  year={2021}
}
@article{lan2020node,
  title={Node classification on graphs with few-shot novel labels via meta transformed network embedding},
  author={Lan, Lin and Wang, Pinghui and Du, Xuefeng and Song, Kaikai and Tao, Jing and Guan, Xiaohong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16520--16531},
  year={2020}
}
@inproceedings{du2021learning,
  title={Learning diverse-structured networks for adversarial robustness},
  author={Du, Xuefeng and Zhang, Jingfeng and Han, Bo and Liu, Tongliang and Rong, Yu and Niu, Gang and Huang, Junzhou and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={2880--2891},
  year={2021},
  organization={PMLR}
}
@article{du2021active,
  title={Active learning to classify macromolecular structures in situ for less supervision in cryo-electron tomography},
  author={Du, Xuefeng and Wang, Haohan and Zhu, Zhenxi and Zeng, Xiangrui and Chang, Yi-Wei and Zhang, Jing and Xing, Eric and Xu, Min},
  journal={Bioinformatics},
  volume={37},
  number={16},
  pages={2340--2346},
  year={2021},
  publisher={Oxford University Press}
}
@inproceedings{xie2022performance,
  title={Performance-aware mutual knowledge distillation for improving neural architecture search},
  author={Xie, Pengtao and Du, Xuefeng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11922--11932},
  year={2022}
}
@article{issaka2024ghanaian,
  title={The Ghanaian NLP Landscape: A First Look},
  author={Issaka, Sheriff and Zhang, Zhaoyi and Heda, Mihir and Wang, Keyi and Ajibola, Yinka and DeMar, Ryan and Du, Xuefeng},
  journal={arXiv preprint arXiv:2405.06818},
  year={2024}
}
 @inproceedings{du2022vos,
      title={VOS: Learning What You Don’t Know by Virtual Outlier Synthesis}, 
      author={Du, Xuefeng and Wang, Zhaoning and Cai, Mu and Li, Yixuan},
      booktitle={Proceedings of the International Conference on Learning Representations},
      year={2022}
}
 @inproceedings{du2022unknown,
      title={Unknown-Aware Object Detection: Learning What You Don’t Know from Videos in the Wild}, 
      author={Du, Xuefeng and Wang, Xin and Gozum, Gabriel and Li, Yixuan},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      year={2022}
}
@article{yang2022openood,
  title={Openood: Benchmarking generalized out-of-distribution detection},
  author={Yang, Jingkang and Wang, Pengyun and Zou, Dejian and Zhou, Zitang and Ding, Kunyuan and Peng, Wenxuan and Wang, Haoqi and Chen, Guangyao and Li, Bo and Sun, Yiyou and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32598--32611},
  year={2022}
}
@inproceedings{du2022siren,
  title={SIREN: Shaping Representations for Detecting Out-of-Distribution Objects},
  author={Du, Xuefeng and Gozum, Gabriel and Ming, Yifei and Li, Yixuan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
@journal{du2023noise,
title={Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions},
author={Xuefeng Du and Tian Bian and Yu Rong and Bo Han and Tongliang Liu and Tingyang Xu and Wenbing Huang and Yixuan Li and Junzhou Huang},
journal={Transactions on Machine Learning Research},
year={2023}
}
@inproceedings{tao2023nonparametric,
  title={Non-Parametric Outlier Synthesis},
  author={Tao, Leitian and Du, Xuefeng and Zhu, Xiaojin and Li, Yixuan},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2023}
}

@article{zhang2023openood,
  title={Openood v1. 5: Enhanced benchmark for out-of-distribution detection},
  author={Zhang, Jingyang and Yang, Jingkang and Wang, Pengyun and Wang, Haoqi and Lin, Yueqian and Zhang, Haoran and Sun, Yiyou and Du, Xuefeng and Zhou, Kaiyang and Zhang, Wayne and others},
  journal={arXiv preprint arXiv:2306.09301},
  year={2023}
}
 @inproceedings{du2023dream,
      title={Dream the Impossible: Outlier Imagination with Diffusion Models}, 
      author={Xuefeng Du and Yiyou Sun and Xiaojin Zhu and Yixuan Li },
      booktitle={Advances in Neural Information Processing Systems},
      year = {2023}
}
@article{yang2024generalized,
  title={Generalized Out-of-Distribution Detection: A Survey},
  author={Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  year={2024}
}
@inproceedings{du2024when,
      title={When and How Does In-Distribution Label Help Out-of-Distribution Detection?}, 
      author={Xuefeng Du and Yiyou Sun and Yixuan Li},
      booktitle = {International Conference on Machine Learning},
      year = {2024}
}
@article{liu2024exploring,
  title={Exploring Transition States of Protein Conformational Changes via Out-of-Distribution Detection in the Hyperspherical Latent Space},
  author={Liu, Bojun and Boysen, Jordan G and Unarta, Ilona Christy and Du, Xuefeng and Li, Yixuan and Huang, Xuhui},
  year={2024}
}
@inproceedings{bai2023feed,
      title={Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution Generalization and Detection}, 
      author={Haoyue Bai and Gregory Canal and Xuefeng Du and Jeongyeol Kwon and Robert D Nowak and Yixuan Li},
      booktitle = {International Conference on Machine Learning},
      year = {2023}
}
@article{bai2024out,
  title={Out-of-Distribution Learning with Human Feedback},
  author={Bai, Haoyue and Du, Xuefeng and Rainey, Katie and Parameswaran, Shibin and Li, Yixuan},
  journal={arXiv preprint arXiv:2408.07772},
  year={2024}
}
 @inproceedings{du2024haloscope,
      title={ HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection}, 
      author={Xuefeng Du and Chaowei Xiao and Yixuan Li},
      booktitle={Advances in Neural Information Processing Systems},
      year = {2024}
}
@article{shayegani2023plug,
  title={Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models},
  author={Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2307.14539},
  year={2023}
}
@article{klema1980singular,
  title={The singular value decomposition: Its computation and some applications},
  author={Klema, Virginia and Laub, Alan},
  journal={IEEE Transactions on automatic control},
  volume={25},
  number={2},
  pages={164--176},
  year={1980},
  publisher={IEEE}
}
@article{miao2021prevent,
  title={Prevent the language model from being overconfident in neural machine translation},
  author={Miao, Mengqi and Meng, Fandong and Liu, Yijin and Zhou, Xiao-Hua and Zhou, Jie},
  journal={arXiv preprint arXiv:2105.11098},
  year={2021}
}
@article{burns2022discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}
@article{gu2024agent,
  title={Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast},
  author={Gu, Xiangming and Zheng, Xiaosen and Pang, Tianyu and Du, Chao and Liu, Qian and Wang, Ye and Jiang, Jing and Lin, Min},
  journal={arXiv preprint arXiv:2402.08567},
  year={2024}
}
@article{gong2023figstep,
  title={Figstep: Jailbreaking large vision-language models via typographic visual prompts},
  author={Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun},
  journal={arXiv preprint arXiv:2311.05608},
  year={2023}
}
@article{li2024red,
  title={Red teaming visual language models},
  author={Li, Mukai and Li, Lei and Yin, Yuwei and Ahmed, Masood and Liu, Zhenguang and Liu, Qi},
  journal={arXiv preprint arXiv:2401.12915},
  year={2024}
}

@article{liu2023prompt1,
  title={Prompt injection attacks and defenses in llm-integrated applications},
  author={Liu, Yupei and Jia, Yuqi and Geng, Runpeng and Jia, Jinyuan and Gong, Neil Zhenqiang},
  journal={arXiv preprint arXiv:2310.12815},
  year={2023}
}
@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{schlarmann2024robust,
  title={Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models},
  author={Schlarmann, Christian and Singh, Naman Deep and Croce, Francesco and Hein, Matthias},
  journal={arXiv preprint arXiv:2402.12336},
  year={2024}
}
@article{carlini2024aligned,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Koh, Pang Wei W and Ippolito, Daphne and Tramer, Florian and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{liu2023autodan,
  title={Autodan: Generating stealthy jailbreak prompts on aligned large language models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2310.04451},
  year={2023}
}
@article{robey2023smoothllm,
  title={Smoothllm: Defending large language models against jailbreaking attacks},
  author={Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2310.03684},
  year={2023}
}
@article{russinovich2024great,
  title={Great, now write an article about that: The crescendo multi-turn llm jailbreak attack},
  author={Russinovich, Mark and Salem, Ahmed and Eldan, Ronen},
  journal={arXiv preprint arXiv:2404.01833},
  year={2024}
}
@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}
@article{yi2024jailbreak,
  title={Jailbreak Attacks and Defenses Against Large Language Models: A Survey},
  author={Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
  journal={arXiv preprint arXiv:2407.04295},
  year={2024}
}
@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}
@article{rossi2024early,
  title={An Early Categorization of Prompt Injection Attacks on Large Language Models},
  author={Rossi, Sippo and Michel, Alisia Marianne and Mukkamala, Raghava Rao and Thatcher, Jason Bennett},
  journal={arXiv preprint arXiv:2402.00898},
  year={2024}
}
@article{wang2024defending,
  title={Defending llms against jailbreaking attacks via backtranslation},
  author={Wang, Yihan and Shi, Zhouxing and Bai, Andrew and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2402.16459},
  year={2024}
}
@article{hines2024defending,
  title={Defending Against Indirect Prompt Injection Attacks With Spotlighting},
  author={Hines, Keegan and Lopez, Gary and Hall, Matthew and Zarfati, Federico and Zunger, Yonatan and Kiciman, Emre},
  journal={arXiv preprint arXiv:2403.14720},
  year={2024}
}
@inproceedings{greshake2023not,
  title={Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  booktitle={Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
  pages={79--90},
  year={2023}
}
@article{piet2023jatmo,
  title={Jatmo: Prompt injection defense by task-specific finetuning},
  author={Piet, Julien and Alrashed, Maha and Sitawarin, Chawin and Chen, Sizhe and Wei, Zeming and Sun, Elizabeth and Alomair, Basel and Wagner, David},
  journal={arXiv preprint arXiv:2312.17673},
  year={2023}
}
@article{chen2024struq,
  title={StruQ: Defending against prompt injection with structured queries},
  author={Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner, David},
  journal={arXiv preprint arXiv:2402.06363},
  year={2024}
}
@article{shi2024optimization,
  title={Optimization-based Prompt Injection Attack to LLM-as-a-Judge},
  author={Shi, Jiawen and Yuan, Zenghui and Liu, Yinuo and Huang, Yue and Zhou, Pan and Sun, Lichao and Gong, Neil Zhenqiang},
  journal={arXiv preprint arXiv:2403.17710},
  year={2024}
}

@article{liu2023prompt,
  title={Prompt Injection attack against LLM-integrated Applications},
  author={Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and others},
  journal={arXiv preprint arXiv:2306.05499},
  year={2023}
}
@article{xu2024defending,
  title={Defending Jailbreak Attack in VLMs via Cross-modality Information Detector},
  author={Xu, Yue and Qi, Xiuyuan and Qin, Zhan and Wang, Wenjie},
  journal={arXiv preprint arXiv:2407.21659},
  year={2024}
}
@article{huber1992robust,
  title={Robust estimation of a location parameter},
  author={Huber, Peter J},
  journal={Breakthroughs in statistics: Methodology and distribution},
  pages={492--518},
  year={1992},
  publisher={Springer}
}
@article{fares2024mirrorcheck,
  title={MirrorCheck: Efficient Adversarial Defense for Vision-Language Models},
  author={Fares, Samar and Ziu, Klea and Aremu, Toluwani and Durasov, Nikita and Tak{\'a}{\v{c}}, Martin and Fua, Pascal and Nandakumar, Karthik and Laptev, Ivan},
  journal={arXiv preprint arXiv:2406.09250},
  year={2024}
}
@article{zhang2023mutation,
  title={A mutation-based method for multi-modal jailbreaking attack detection},
  author={Zhang, Xiaoyu and Zhang, Cen and Li, Tianlin and Huang, Yihao and Jia, Xiaojun and Xie, Xiaofei and Liu, Yang and Shen, Chao},
  journal={arXiv preprint arXiv:2312.10766},
  year={2023}
}
@article{yi2023benchmarking,
  title={Benchmarking and defending against indirect prompt injection attacks on large language models},
  author={Yi, Jingwei and Xie, Yueqi and Zhu, Bin and Hines, Keegan and Kiciman, Emre and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},
  journal={arXiv preprint arXiv:2312.14197},
  year={2023}
}
@article{gou2024eyes,
  title={Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation},
  author={Gou, Yunhao and Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu},
  journal={arXiv preprint arXiv:2403.09572},
  year={2024}
}

@article{niu2024jailbreaking,
  title={Jailbreaking attack against multimodal large language model},
  author={Niu, Zhenxing and Ren, Haodong and Gao, Xinbo and Hua, Gang and Jin, Rong},
  journal={arXiv preprint arXiv:2402.02309},
  year={2024}
}
@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}
@article{rottger2023xstest,
  title={Xstest: A test suite for identifying exaggerated safety behaviours in large language models},
  author={R{\"o}ttger, Paul and Kirk, Hannah Rose and Vidgen, Bertie and Attanasio, Giuseppe and Bianchi, Federico and Hovy, Dirk},
  journal={arXiv preprint arXiv:2308.01263},
  year={2023}
}
@article{bagdasaryan2023ab,
  title={(Ab) using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs},
  author={Bagdasaryan, Eugene and Hsieh, Tsung-Yin and Nassi, Ben and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:2307.10490},
  year={2023}
}
@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}
@article{wang2024llms,
  title={From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking},
  author={Wang, Siyuan and Long, Zhuohan and Fan, Zhihao and Wei, Zhongyu},
  journal={arXiv preprint arXiv:2406.14859},
  year={2024}
}
@article{zeng2024autodefense,
  title={Autodefense: Multi-agent llm defense against jailbreak attacks},
  author={Zeng, Yifan and Wu, Yiran and Zhang, Xiao and Wang, Huazheng and Wu, Qingyun},
  journal={arXiv preprint arXiv:2403.04783},
  year={2024}
}
@article{zhang2024soft,
  title={Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions},
  author={Zhang, Tingwei and Zhang, Collin and Morris, John X and Bagdasaryan, Eugene and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:2407.08970},
  year={2024}
}
@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}
@article{pi2024mllm,
  title={MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance},
  author={Pi, Renjie and Han, Tianyang and Xie, Yueqi and Pan, Rui and Lian, Qing and Dong, Hanze and Zhang, Jipeng and Zhang, Tong},
  journal={arXiv preprint arXiv:2401.02906},
  year={2024}
}
@article{zou2024improving,
  title={Improving Alignment and Robustness with Short Circuiting},
  author={Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek and Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan and Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan},
  journal={arXiv preprint arXiv:2406.04313},
  year={2024}
}
@article{hu2024gradient,
  title={Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes},
  author={Hu, Xiaomeng and Chen, Pin-Yu and Ho, Tsung-Yi},
  journal={arXiv preprint arXiv:2403.00867},
  year={2024}
}
@article{xie2024gradsafe,
  title={GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis},
  author={Xie, Yueqi and Fang, Minghong and Pi, Renjie and Gong, Neil},
  journal={arXiv preprint arXiv:2402.13494},
  year={2024}
}

@inproceedings{
shayegani2024jailbreak,
title={Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models},
author={Erfan Shayegani and Yue Dong and Nael Abu-Ghazaleh},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=plmBsXHxgR}
}
@article{yin2024vlattack,
  title={Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models},
  author={Yin, Ziyi and Ye, Muchao and Zhang, Tianrong and Du, Tianyu and Zhu, Jinguo and Liu, Han and Chen, Jinghui and Wang, Ting and Ma, Fenglong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{cogvlm,
      title={Cog{VLM}: Visual Expert for Pretrained Language Models}, 
      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      journal={arXiv preprint arXiv:2311.03079}
}

@inproceedings{wan-etal-2024-logicasker,
    title = "{L}ogic{A}sker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
    author = "Wan, Yuxuan  and
      Wang, Wenxuan  and
      Yang, Yiliu  and
      Yuan, Youliang  and
      Huang, Jen-tse  and
      He, Pinjia  and
      Jiao, Wenxiang  and
      Lyu, Michael",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.128/",
    doi = "10.18653/v1/2024.emnlp-main.128",
    pages = "2124--2155",
    abstract = "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite LLMs' prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs' learning of logical rules, with identified reasoning failures ranging from 29{\%} to 90{\%} across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5{\%}. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs' formal reasoning capabilities. We make our code, data, and results publicly available(https://github.com/yxwan123/LogicAsker) to facilitate further research and replication of our findings."
}

@inproceedings{peng2024multi,
  title={Multi-modal auto-regressive modeling via visual tokens},
  author={Peng, Tianshuo and Li, Zuchao and Zhang, Lefei and Zhao, Hai and Wang, Ping and Du, Bo},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={10735--10744},
  year={2024}
}

@article{peng2024chimera,
  title={Chimera: Improving generalist model with domain-specific experts},
  author={Peng, Tianshuo and Li, Mingsheng and Zhou, Hongbin and Xia, Renqiu and Zhang, Renrui and Bai, Lei and Mao, Song and Wang, Bin and He, Conghui and Zhou, Aojun and others},
  journal={arXiv preprint arXiv:2412.05983},
  year={2024}
}

@misc{du2024vlmguarddefendingvlmsmalicious,
      title={VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data}, 
      author={Xuefeng Du and Reshmi Ghosh and Robert Sim and Ahmed Salem and Vitor Carvalho and Emily Lawton and Yixuan Li and Jack W. Stokes},
      year={2024},
      eprint={2410.00296},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.00296}, 
}

@misc{chao2024jailbreakingblackboxlarge,
      title={Jailbreaking Black Box Large Language Models in Twenty Queries}, 
      author={Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong},
      year={2024},
      eprint={2310.08419},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.08419}, 
}

@misc{niu2024jailbreakingattackmultimodallarge,
      title={Jailbreaking Attack against Multimodal Large Language Model}, 
      author={Zhenxing Niu and Haodong Ren and Xinbo Gao and Gang Hua and Rong Jin},
      year={2024},
      eprint={2402.02309},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.02309}, 
}

@misc{qi2023visualadversarialexamplesjailbreak,
      title={Visual Adversarial Examples Jailbreak Aligned Large Language Models}, 
      author={Xiangyu Qi and Kaixuan Huang and Ashwinee Panda and Peter Henderson and Mengdi Wang and Prateek Mittal},
      year={2023},
      eprint={2306.13213},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2306.13213}, 
}

@misc{li2024safetylayersalignedlarge,
      title={Safety Layers in Aligned Large Language Models: The Key to LLM Security}, 
      author={Shen Li and Liuyi Yao and Lan Zhang and Yaliang Li},
      year={2024},
      eprint={2408.17003},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2408.17003}, 
}