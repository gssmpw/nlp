[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2024safety",
        "author": "Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu",
        "title": "Safety of Multimodal Large Language Models on Images and Text"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liu2023query",
        "author": "Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu",
        "title": "Query-relevant images jailbreak large multi-modal models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "luo2024jailbreakv",
        "author": "Luo, Weidi and Ma, Siyuan and Liu, Xiaogeng and Guo, Xiaoyu and Xiao, Chaowei",
        "title": "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gong2023figstep",
        "author": "Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun",
        "title": "Figstep: Jailbreaking large vision-language models via typographic visual prompts"
      },
      {
        "key": "liu2023query",
        "author": "Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu",
        "title": "Query-relevant images jailbreak large multi-modal models"
      },
      {
        "key": "luo2024jailbreakv",
        "author": "Luo, Weidi and Ma, Siyuan and Liu, Xiaogeng and Guo, Xiaoyu and Xiao, Chaowei",
        "title": "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "gong2023figstep",
        "author": "Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun",
        "title": "Figstep: Jailbreaking large vision-language models via typographic visual prompts"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhao2024evaluating",
        "author": "Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man Man and Lin, Min",
        "title": "On evaluating adversarial robustness of large vision-language models"
      },
      {
        "key": "shayegani2023plug",
        "author": "Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael",
        "title": "Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models"
      },
      {
        "key": "dong2023robust",
        "author": "Dong, Yinpeng and Chen, Huanran and Chen, Jiawei and Fang, Zhengwei and Yang, Xiao and Zhang, Yichi and Tian, Yu and Su, Hang and Zhu, Jun",
        "title": "How Robust is Google's Bard to Adversarial Image Attacks?"
      },
      {
        "key": "qi2023visual",
        "author": "Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Wang, Mengdi and Mittal, Prateek",
        "title": "Visual Adversarial Examples Jailbreak Large Language Models"
      },
      {
        "key": "tu2023many",
        "author": "Tu, Haoqin and Cui, Chenhang and Wang, Zijun and Zhou, Yiyang and Zhao, Bingchen and Han, Junlin and Zhou, Wangchunshu and Yao, Huaxiu and Xie, Cihang",
        "title": "How many unicorns are in this image? a safety evaluation benchmark for vision llms"
      },
      {
        "key": "luo2024an",
        "author": "Haochen Luo and Jindong Gu and Fengyuan Liu and Philip Torr",
        "title": "An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models"
      },
      {
        "key": "wan-etal-2024-logicasker",
        "author": "Wan, Yuxuan  and\nWang, Wenxuan  and\nYang, Yiliu  and\nYuan, Youliang  and\nHuang, Jen-tse  and\nHe, Pinjia  and\nJiao, Wenxiang  and\nLyu, Michael",
        "title": "{L}ogic{A}sker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "bagdasaryan2023ab",
        "author": "Bagdasaryan, Eugene and Hsieh, Tsung-Yin and Nassi, Ben and Shmatikov, Vitaly",
        "title": "(Ab) using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs"
      },
      {
        "key": "schlarmann2023adversarial",
        "author": "Schlarmann, Christian and Hein, Matthias",
        "title": "On the adversarial robustness of multi-modal foundation models"
      },
      {
        "key": "bailey2023image",
        "author": "Bailey, Luke and Ong, Euan and Russell, Stuart and Emmons, Scott",
        "title": "Image hijacks: Adversarial images can control generative models at runtime"
      },
      {
        "key": "fu2023misusing",
        "author": "Fu, Xiaohan and Wang, Zihan and Li, Shuheng and Gupta, Rajesh K and Mireshghallah, Niloofar and Berg-Kirkpatrick, Taylor and Fernandes, Earlence",
        "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "mad_bench",
        "author": "Qian, Yusu and Zhang, Haotian and Yang, Yinfei and Gan, Zhe",
        "title": "How Easy is It to Fool Your Multimodal {LLMs}? An Empirical Analysis on Deceptive Prompts"
      },
      {
        "key": "chen2023dress",
        "author": "Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay",
        "title": "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback"
      },
      {
        "key": "mmsafetybench",
        "author": "Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu",
        "title": "Query-relevant images jailbreak large multi-modal models"
      },
      {
        "key": "selfaware",
        "author": "Wang, Yuhao and Liao, Yusheng and Liu, Heyang and Liu, Hongcheng and Wang, Yu and Wang, Yanfeng",
        "title": "{MM-SAP}: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "mad_bench",
        "author": "Qian, Yusu and Zhang, Haotian and Yang, Yinfei and Gan, Zhe",
        "title": "How Easy is It to Fool Your Multimodal {LLMs}? An Empirical Analysis on Deceptive Prompts"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "mmsafetybench",
        "author": "Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu",
        "title": "Query-relevant images jailbreak large multi-modal models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chen2023dress",
        "author": "Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay",
        "title": "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "MLLM_protector",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "du2024vlmguarddefendingvlmsmalicious",
        "author": "Xuefeng Du and Reshmi Ghosh and Robert Sim and Ahmed Salem and Vitor Carvalho and Emily Lawton and Yixuan Li and Jack W. Stokes",
        "title": "VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wu2023jailbreaking",
        "author": "Wu, Yuanwei and Li, Xiang and Liu, Yixin and Zhou, Pan and Sun, Lichao",
        "title": "Jailbreaking gpt-4v via self-adversarial attacks with system prompts"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gou2024eyes",
        "author": "Gou, Yunhao and Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu",
        "title": "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wang2024adashield",
        "author": "Wang, Yu and Liu, Xiaogeng and Li, Yu and Chen, Muhao and Xiao, Chaowei",
        "title": "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "jiang2024rapguardsafeguardingmultimodallarge",
        "author": "Yilei Jiang and Yingshui Tan and Xiangyu Yue",
        "title": "RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting"
      }
    ]
  }
]