\section{Related Work}
\subsection{Vulnerability and Safety in LVLMs}
Large vision-language models (LVLMs) are vulnerable to various security risks, including susceptibility to malicious prompt attacks \cite{liu2024safety}, which can exploit vision-only \cite{liu2023query} or cross-modal \cite{luo2024jailbreakv} inputs to elicit unsafe responses. Prior studies identify two primary attack strategies for embedding harmful content. The first involves encoding harmful text into images using text-to-image generation tools, thereby bypassing safety mechanisms \cite{gong2023figstep,liu2023query,luo2024jailbreakv}. For example, \citet{gong2023figstep} demonstrate how malicious queries embedded in images through typography can evade detection. The second strategy employs gradient-based adversarial techniques to craft images that appear benign to humans but provoke unsafe model outputs \cite{zhao2024evaluating,shayegani2023plug,dong2023robust,qi2023visual,tu2023many,luo2024an,wan-etal-2024-logicasker}. These methods leverage minor perturbations or adversarial patches to mislead classifiers \cite{bagdasaryan2023ab,schlarmann2023adversarial,bailey2023image,fu2023misusing}.

% Beyond adversarial manipulations, LVLMs are also prone to hallucinations and inaccurate responses when encountering unanswerable, deceptive, or maliciously designed queries \cite{mad_bench,chen2023dress,mmsafetybench,selfaware}. For instance, \citet{mad_bench} show that misleading prompts such as asking about "three dogs" in an image containing only twoâ€”can induce erroneous model outputs. Similarly, \citet{mmsafetybench} employ diffusion models to generate images of harmful activities and then query LVLMs for guidance, exposing vulnerabilities to jailbreaking techniques.


\subsection{Efforts to Safeguard LVLMs}

To mitigate these risks, prior research has explored various alignment strategies, including reinforcement learning from human feedback (RLHF) \cite{chen2023dress} and fine-tuning LLMs with curated datasets containing both harmful and benign content \cite{MLLM_protector, du2024vlmguarddefendingvlmsmalicious}. While effective, these approaches are computationally demanding. Other inference-time defenses include manually engineered safety prompts to specify acceptable behaviors \cite{wu2023jailbreaking}, though these approaches frequently fail to generalize across diverse tasks. More recent methods transform visual inputs into textual descriptions for safer processing \cite{gou2024eyes} or employ adaptive warning prompts \cite{wang2024adashield}. Additionally, \citet{jiang2024rapguardsafeguardingmultimodallarge} propose multimodal chain-of-thought prompting to enforce safer responses. However, many of these methods overlook intrinsic safety mechanisms within LVLMs, which is the main goal of our work.

\begin{figure*}[!t]   
\centering
        \includegraphics[width=1.05\linewidth]{figures/pipeline13.pdf}
        \caption{Overview of HiddenDetect. We calculate the safety score based on the cosine similarity between the mapped hidden states at the final token position in the vocabulary space of the most safety-aware layers and the constructed refusal vector, enabling effective and efficient safety judgment at inference time.}   
        \label{pipeline}
\end{figure*}