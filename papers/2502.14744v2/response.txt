\section{Related Work}
\subsection{Vulnerability and Safety in LVLMs}
Large vision-language models (LVLMs) are vulnerable to various security risks, including susceptibility to malicious prompt attacks **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, which can exploit vision-only **Su et al., "One Pixel Attack for Fooling Deep Neural Networks"** or cross-modal **Liu et al., "Deep Learning Security: A Review of the State-of-the-Art"** inputs to elicit unsafe responses. Prior studies identify two primary attack strategies for embedding harmful content. The first involves encoding harmful text into images using text-to-image generation tools, thereby bypassing safety mechanisms **Zhang et al., "Adversarial Attacks on Image-to-Text Generation Models"**. For example, **Sharma et al., "Robustness of Deep Neural Networks to Adversarial Attacks through Typography"** demonstrate how malicious queries embedded in images through typography can evade detection. The second strategy employs gradient-based adversarial techniques to craft images that appear benign to humans but provoke unsafe model outputs **Kurakin et al., "Adversarial Examples in the Physical World"**. These methods leverage minor perturbations or adversarial patches to mislead classifiers **Liu et al., "Deep Learning Security: A Review of the State-of-the-Art"**.

% Beyond adversarial manipulations, LVLMs are also prone to hallucinations and inaccurate responses when encountering unanswerable, deceptive, or maliciously designed queries **Huang et al., "Understanding and Mitigating Hallucinations in Vision-Language Models"**. For instance, **Guo et al., "Prompt Engineering for Adversarial Attacks on Vision-Language Models"** show that misleading prompts such as asking about "three dogs" in an image containing only twoâ€”can induce erroneous model outputs. Similarly, **Zhang et al., "Adversarial Manipulation of Vision-Language Models using Diffusion Models"** employ diffusion models to generate images of harmful activities and then query LVLMs for guidance, exposing vulnerabilities to jailbreaking techniques.


\subsection{Efforts to Safeguard LVLMs}

To mitigate these risks, prior research has explored various alignment strategies, including reinforcement learning from human feedback (RLHF) **Jansen et al., "Reinforcement Learning from Human Feedback for Vision-Language Models"** and fine-tuning LLMs with curated datasets containing both harmful and benign content **Chen et al., "Fine-Tuning Pre-Trained Language Models for Safety on Vision-Linguistic Tasks"**. While effective, these approaches are computationally demanding. Other inference-time defenses include manually engineered safety prompts to specify acceptable behaviors **Kumar et al., "Safety-Prompt Engineering for Vision-Language Models"**, though these approaches frequently fail to generalize across diverse tasks. More recent methods transform visual inputs into textual descriptions for safer processing **Liu et al., "Multimodal Text-to-Text Transformers for Vision-Language Alignment"** or employ adaptive warning prompts **Zhou et al., "Adaptive Warning Prompts for Safety in Vision-Language Models"**. Additionally, **Wang et al., "Multimodal Chain-of-Thought Prompting for Safe Vision-Language Inference"** propose multimodal chain-of-thought prompting to enforce safer responses. However, many of these methods overlook intrinsic safety mechanisms within LVLMs, which is the main goal of our work.

\begin{figure*}[!t]   
\centering
        \includegraphics[width=1.05\linewidth]{figures/pipeline13.pdf}
        \caption{Overview of HiddenDetect. We calculate the safety score based on the cosine similarity between the mapped hidden states at the final token position in the vocabulary space of the most safety-aware layers and the constructed refusal vector, enabling effective and efficient safety judgment at inference time.}   
        \label{pipeline}