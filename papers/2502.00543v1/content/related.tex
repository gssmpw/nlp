Transformers, initially proposed for language translation task, have demonstrated remarkable versatility across a spectrum of domains, including CV and robotics. This section provides an overview of key advancements in each of these areas, as well as existing work in data-driven off-road mobility. 

\subsection{Transformers in NLP and CV.} 
The \tr~architecture originated from the seminal work of \citet{vaswani2017attention} in machine translation. Subsequent research has explored the effects of different \tr~parts, including using only the \encoder~(BERT~\cite{devlin2019bert}) or \decoder~(GPT series~\cite{radford2018improving, radford2019language, brown2020language}). Other works explored optimization techniques such as adopting a warm-up phase for training Transformers~\cite{xiong2020layer}, specific initialization and optimization methods to train deep Transformers with limited data~\cite{xu2021optimizing}, as well as normalization techniques~\cite{loshchilov2024ngpt}.

Early explorations of Transformers in CV include iGPT~\cite{chen2020generative}. A significant breakthrough came with the introduction of Vision Transformers (ViT) by~\citet{dosovitskiy2021image}. Subsequent research focused on refining training methodologies and enhancing performance, such as incorporating auxiliary tasks~\cite{liu2021efficient} for spatial understanding, two-stage training (self-supervised view prediction followed by supervised label prediction)~\cite{gani2022how}, different token representations~\cite{mao2022discrete}, architectural modifications~\cite{zhai2022scaling}, working in embedding space by JEPA family~\cite{assran2023self, bardes2023mcjepa, bardes2024revisiting}, data augmentation and regularization~\cite{steiner2022how}, and Masked Autoencoders~\cite{he2022masked} with random patch encoding for training stabilization~\cite{chen2021empirical}. Similar to the autoregressive nature of NLP tasks,~\citet{rajasegaran2025empirical} provided empirical guidelines to train Transformers on large-scale video data autoregressively.
Despite the plethora of NLP and CV Transformers trained with internet-scale datasets, existing common training practices may not apply to robot learning with small real-world data, especially for off-road robot mobility. 

\subsection{Transformers in Robotics.}
Recent years have witnessed a surge in the application of Transformers to robotics, encompassing both perception and planning: 
Generalist robot policies based on Transformers, e.g., Octo~\cite{octomodelteam2024octo} and CrossFormer~\cite{doshi2024scaling}, with multi-modal sensory input~\cite{jones2025sight} and action tokenization~\cite{pertsch2025fast} aim to handle diverse tasks such as manipulation and navigation;
Studies in target-driven~\cite{du2021vtnet, wang2024navformer, nazeri2024vanp, huang2024goalguided} and image-goal navigation~\cite{pelluri2024transformers, liu2024citywalker} show that Transformers significantly outperform traditional behavior cloning baselines~\cite{pomerleau1988alvinn, bojarski2016end, nazeri2021exploring}; 
Reinforcement learning has been significantly enhanced by integrating the \tr~architecture, providing improved sequence modeling ~\cite{zhang2024naviformer} and decision-making capabilities~\cite{chen2021decision};  
Transformers have also been used in motion planning to guide long-horizon navigation tasks~\cite{lawson2023control} and reduce the search space for sampling-based motion planners~\cite{johnson2022motion};
In Unmanned Surface Vehicles (USV), MarineFormer~\cite{kazemi2024marineformer} utilizes Transformers to learn the flow dynamics around a USV and then learns a navigation policy resulting in better path length and completion rate.

A common characteristic of these models is their treatment of each sensor modality (e.g., vision, touch, and audio) as a distinct token, relying on the \tr~to learn the inter-modal correlations and their temporal dynamics. While this approach allows for flexible integration of diverse sensory information, it necessitates substantial amounts of training data to compensate for the lack of inductive bias inherent in Transformers~\cite{dosovitskiy2021image}. This data dependency poses a significant challenge, particularly in off-road robot mobility, where real-world, outdoor data acquisition can be expensive and time-consuming. Consequently, there remains a critical need for research focused on refining training methodologies and exploring architectural modifications specifically tailored to address the data scarcity often encountered in robotics.

\subsection{Learning Off-Road Mobility.}
While most learning approaches for off-road autonomy focus on perception tasks~\cite{xiao2022motion, wigness2019rugd, jiang2021rellis}, researchers have recently investigated off-road mobility to account for vehicle stability~\cite{bae2021curriculum, lee2023learning, datar2024learning, pokhrel2024cahsor}, wheel slippage~\cite{siva2019robot, siva2022nauts, sharma2023ramp}, and terrain traversability~\cite{fan2021step, triest2022tartandrive, castro2023does, seo2023learning, cai2024evora}. A relevant work by \citet{xiao2024anycar} aims to use Transformers to enable a universal forward kinodynamics model that can drive different ground vehicles. Most of these approaches adopted specific techniques designed to address one particular off-road mobility task with non-Transformer architectures. 

Focusing on kinodynamic representation for off-road mobility, our non-autoregressive \former~employs a novel variation of MM and NTP paradigms and a unified modality latent representation to predict the next pose, action, and terrain patch in order to simultaneously enable a variety of off-road mobility tasks, e.g., forward and inverse kinodynamics modeling, behavior cloning, and terrain patch reconstruction, without a specific training procedure for each.  