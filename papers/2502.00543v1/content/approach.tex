We introduce \former, a data-efficient multi-task \tr~model for kinodynamic representation and navigation on complex, vertically challenging, off-road terrain. We propose an efficient training methodology for training \former~utilizing limited (one hour) robotics data, including unified multi-modal latent representation, learnable masking, and non-autoregressive training to improve data efficiency by enabling multi-task learning. 

\begin{figure*}
  \centering
  \includegraphics[width=2\columnwidth]{figures/VertiFormer.pdf}
  \caption{\textbf{\former~Architecture}. \former~employs a TransformerEncoder (left) to receive a history of terrain patches, actions, and poses along with multiple context tokens. To predict future states, the model computes cross-attention between these context tokens and the masked upcoming actions or poses. Causal masking is implemented during this cross-attention computation to ensure that predictions are conditioned only on past and present information, preventing information leakage from future time steps.}
  \label{fig:former}
  % \vspace{-1.2em}
\end{figure*}


\subsection{\former~Training} 
\subsubsection{Unified Multi-Modal Latent Representation}
\label{sec:unified}
\former~consists of both \encoder~(\coder) and \decoder~(\vertidecoder), as illustrated in Fig.~\ref{fig:former} left and right, respectively. Consistent with established practices~\cite{datar2024terrainattentive,nazeri2024vertiencoder}, \former~receives a multi-modal sequence of actions $\mathbf{a_{\text{0:T}}}$, robot poses $\mathbf{p_{\text{0:T}}}$, and the underlying terrain patches $\mathbf{i_{\text{0:T}}}$. The \coder~first applies an independent linear mapping to each modality. Specifically, action commands $\mathbf{a_{\text{0:T}}}$ are projected into an embedding space via a linear function $f_a$, yielding $\mathbf{\hat{a}_{\text{0:T}}}$. Analogously, robot poses $\mathbf{p_{\text{0:T}}}$ and terrain patches $\mathbf{i_{\text{0:T}}}$ are transformed using linear mappings $f_p$ and $f_i$ respectively, producing a sequence of embeddings $\mathbf{\hat{p}_{\text{0:T}}}$ and $\mathbf{\hat{i}_{\text{0:T}}}$. This initial linear mapping can be formally expressed as:
\begin{align}
\hat{a}_{\text{t}} &= f_a(a_{\text{t}}) = W_a a_{\text{t}} + b_a , a_{\text{t}} \in  \mathbf{a_{\text{0:T}}}, \\
\hat{p}_{\text{t}} &= f_p(p_{\text{t}}) = W_p p_{\text{t}} + b_p , p_{\text{t}} \in \mathbf{p_{\text{0:T}}}, \\
\hat{i}_{\text{t}} &= f_i(i_{\text{t}}) = W_i i_{\text{t}} + b_i , i_{\text{t}} \in \mathbf{i_{\text{0:T}}},
\end{align}
where $W_a$, $W_p$, and $W_i$ represent the weight matrices, and $b_a$, $b_p$, and $b_i$ denote the bias vectors for each respective modality.

To facilitate effective cross-modal interaction within \former, it is crucial to establish a consistent distributional characteristic across the modality-specific embeddings. Therefore, a subsequent linear transformation, denoted by 
$f_s$, is applied to the concatenation ($\cdot$) of embeddings:
\begin{align}
z_{\text{t}} &= f_s(\hat{a}_{\text{t}}, \hat{p}_{\text{t}}, \hat{i}_{\text{t}}) = W_s(\hat{a}_{\text{t}} \cdot \hat{p}_{\text{t}} \cdot \hat{i}_{\text{t}}) + b_s , t \in [0:T],
\end{align}
with $W_s$ and $b_s$ denoting the weight matrix and bias vector for $f_s$, respectively. This shared linear mapping $f_s$ aims to project all embeddings into a unified latent space, minimizing potential discrepancies in statistical properties. The resulting unified tokens, $\mathbf{z_{\text{0:T}}}$, are then passed as input to the \coder~(Fig.\ref{fig:former} top left). This procedure ensures a homogeneous input representation for the subsequent encoding layers, crucial for effective multi-modal fusion of robotic data. Empirical results (Fig.~\ref{fig:unified_state}) supporting the importance of such a unified representation, in contrast to the conventional individual modality representations, will be presented in Section~\ref{sec:study}. 


\subsubsection{Learnable Masking for Multi-Task Learning}
Combined with our unified representation, we also propose a stochastic learnable MM technique (Fig.\ref{fig:former} top right) to allow \former~to perform multiple predictive tasks, including next pose prediction, action prediction, behavior cloning, and terrain patch prediction (Fig.\ref{fig:former} bottom right). This multi-task learning paradigm is hypothesized to enhance data efficiency by leveraging shared latent representations across related tasks, thereby mitigating the challenges associated with restricted data availability.
During training, we first warm up the model for a few epochs with all modalities, then two distinct data masking methods are applied with equal probability (Fig.\ref{fig:former} top right):
\begin{itemize}
    \item \textbf{Action-Conditioned Pose Prediction:} In 50\% of the training instances, actions generated by human demonstration $\tau$ steps into the future, denoted as $\mathbf{a_{\text{T+1:T}+\tau}}$, are provided as input. Concurrently, the corresponding future poses, $\mathbf{p_{\text{T+1:T}+\tau}}$, are replaced with a learnable mask. This configuration compels the model to predict future poses conditioned on the provided future actions and the preceding historical context, similar to the \emph{Forward Kinodynamic Modeling} (FKD) task in off-road mobility. 

    \item \textbf{Pose-Conditioned Action Prediction:} In the remaining 50\% of instances, the inverse scenario is implemented. Future poses, $\mathbf{p_{\text{T+1:T}+\tau}}$, are provided as input, while the corresponding future actions, $\mathbf{a_{\text{T+1:T}+\tau}}$, are masked using another learnable mask. This prompts the model to predict future actions conditioned on the provided future poses and the historical context, similar to the \emph{Inverse Kinodynamic Modeling} (IKD) task in off-road mobility. 
\end{itemize}
This alternating masking strategy along with our unified representation promotes the learning of a joint representation that is capable of decoding both action and pose information. The utilization of this novel learnable mask allows the model to dynamically adapt the masking pattern. The learnable mask can be conceptualized as a learnable gating mechanism that selectively filters information flow during training.

Furthermore, by extending this masking strategy to mask both future actions, $\mathbf{a_{\text{T+1:T}+\tau}}$, and future poses, $\mathbf{p_{\text{T+1:T}+\tau}}$, simultaneously, \former~is able to perform \emph{Behavior Cloning} (BC) in a zero-shot manner. In this configuration, the model predicts both actions and poses solely based on the historical context, effectively mimicking the demonstrated behavior without requiring explicit information about future actions and poses from a planner. 


\subsubsection{Non-Autoregressive Training}
Building upon the works by \citet{octomodelteam2024octo} and \citet{doshi2024scaling}, \former~employs multiple context tokens to represent a distribution of plausible future states. These context tokens serve to inform \vertidecoder~in predicting both the future ego state and the evolution of the environment. Having multiple context tokens allows \former~to predict the future non-autoregressively. The non-autoregressive nature of the proposed architecture is motivated by the potential computational bottlenecks inherent in autoregressive models, which require querying the model multiple times and are subject to drifting due to error propagation from earlier steps. By learning multi-context representations, the non-autoregressive approach aims to improve both training efficiency and inference speed---a critical consideration for real-time robotic control applications.

We train \former~by minimizing the Mean Squared Error (MSE) between the model's predictions and the corresponding ground truth values. Model evaluation is performed by calculating the error rate between the model's predictions and the ground truth values on a held-out, unseen dataset.


\subsection{\former~Inference} 
During FKD inference, \coder~receives the same historical input as training. \vertidecoder~receives sampled actions from an external sampling-based planner (e.g., MPPI~\cite{williams2017model}) while masking the corresponding poses, compelling the model to predict future poses based solely on the sampled actions (and the context tokens) so that the planner can choose the optimal trajectory to minimize a cost function. For IKD, a global planner generates desired future poses, and by masking the actions we encourage the model to predict future actions to achieve these globally planned poses. By masking both actions and poses, \former~can perform zero-shot BC.

As a reference, we examine the average error rate of \former's pose predictions across $\tau=3$ future time steps in one second (3 Hz). We focus on the average error rate across the three pose components, $\mathbf{X}$, $\mathbf{Y}$, and $\mathbf{Z}$. The performance of \former~is compared against two baseline models: \tal~\cite{datar2024terrainattentive} and \citet{nazeri2024vertiencoder}. Notice that \textsc{tal} is a highly accurate forward kinodynamic model specifically designed for vertically challenging terrain, and \citet{nazeri2024vertiencoder} only employs a \encoder~with random masking. 

\begin{table}[h]
    \centering
    % \vspace{0.5em}
    \begin{NiceTabular}{lccc}

    \toprule
    & \tal~\cite{datar2024terrainattentive} & \citet{nazeri2024vertiencoder} & \former \\
    \midrule                       %    TAL   Encoder   Former
    \textbf{{Error Rate}~$\downarrow$} & 0.528 & 0.516    & 0.495 \\
    \bottomrule
    \end{NiceTabular}
    
    \label{tab::reference}
    % \vspace{-2.0em}
\end{table}
We provide \former's architecture parameters in Appendix~\ref{app:architecture} and qualitative samples of FKD in Fig.~\ref{fig:qualitative} of Appendix~\ref{app:qualitative}. The implementation details along with the one-hour dataset description are provided in Appendix~\ref{app:implementation}.