In this work, we introduce \former, a novel data-efficient multi-task Transformer designed for learning kinodynamic representations on vertically challenging, off-road terrain. \former~demonstrates the capacity to simultaneously address forward kinodynamics learning, inverse kinodynamics learning, and behavior cloning tasks, only using one hour of training data. 
Key contributions include a unified latent space representation enhancing temporal understanding, multi-context tokens enabling multi-step prediction without autoregressive feedback, and a learned masked representation facilitating multiple off-road mobility tasks simultaneously and acting as a proxy for missing modalities during inference. All three contributions improve robustness and generalization of \former~to out-of-distribution environments.
We provide extensive experiment results and empirical guidelines for training Transformers under extreme data scarcity. 
Our evaluations across all three downstream tasks demonstrate that \former~outperforms baseline models, including \tal~\cite{datar2024terrainattentive}, \coder~\cite{nazeri2024vertiencoder}, \vertidecoder, and end-to-end approaches, while exhibiting reduced overfitting and improved generalization and highlighting the efficacy of the proposed architecture and training methodology for learning kinodynamic representations in data-constrained settings. Physical experiments also demonstrate that \former~can enable superior off-road robot mobility on vertically challenging terrain.