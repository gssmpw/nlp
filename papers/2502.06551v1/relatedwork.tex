\section{Related Work}
\label{sec:2}

\subsection{Language Models for Scientific Literature}%1311

The introduction of the transformer architecture \cite{attentionisallyouneed} revolutionized natural language processing, marking a new era in the field, with pretrained language models like BERT \cite{devlin-etal-2019-bert} significantly advancing performance across a wide range of tasks. This progress quickly extended to the scientific domain, leading to the development of domain-specific models such as SciBERT \cite{beltagy2019scibert}, which set new benchmarks on various scientific NLP tasks. SciBERT and similar models demonstrate clear advantages over general-purpose models \cite{biobert, song-etal-2023-matsci, rostam2024finetuninglargelanguagemodels}, and have therefore been applied to a variety of tasks within the scientific domain, including literature search and similarity assessment \cite{singh-etal-2023-scirepeval}, classification \cite{rostam2024finetuninglargelanguagemodels}, and summarization \cite{summarization}, with similar pretrained models having been trained for the general biomedical domain \cite{biobert, pubmedbert} as well as for the biodiversity domain \cite{abdelmageed2023biodivbert}.

More recently, the improved performance of autoregressive language models \cite{radford2019language} has driven a shift toward leveraging these models for a wide range of tasks. Openly available models, such as Llama-2 \cite{touvron2023llama2openfoundation}, alongside proprietary systems like ChatGPT, have established new state-of-the-art results in various scientific document processing tasks, including structured information extraction \cite{Rettenberger, dagdelen2024structured}, term extraction \cite{huang2024critical}, text classification, named entity recognition and and question answering \cite{choi2024accelerating}.

A range of benchmarks has been developed specifically for information extraction from scientific full texts, often accompanied by proposed models. These benchmarks target various tasks, including dataset mention detection \cite{dataset}, entity and relation extraction \cite{zhang-etal-2024-scier}, general information extraction \cite{jain-etal-2020-scirex}, and summarization \cite{deyoung-etal-2021-ms}.

\subsection{Language Models for Biodiversity Science}

In the specific domain of biodiversity science, transformer encoder architectures have been employed to tackle tasks such as hypothesis classification \cite{brinner-etal-2022-linking}, biodiversity analysis \cite{barcodebert}, named entity recognition and relation extraction \cite{abdelmageed2023biodivbert}, as well as hypothesis evidence localization \cite{weakClaims, brinner-zarriess-2024-rationalizing}. Additionally, autoregressive models have been applied to tasks such as literature review, question answering \cite{biodiversity_LLM}, and structured information extraction \cite{structuredExtractionEcology, infoextractBiodiv}, with further potential applications continuing to emerge \cite{osawa2023role}.