\section{Related Work}
\label{sec:2}

\subsection{Language Models for Scientific Literature}%1311

The introduction of the transformer architecture **Vaswani et al., "Attention Is All You Need"** revolutionized natural language processing, marking a new era in the field, with pretrained language models like BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** significantly advancing performance across a wide range of tasks. This progress quickly extended to the scientific domain, leading to the development of domain-specific models such as SciBERT **Beltagy et al., "SciBERT: Pretrained Contextualized Embeddings for Scientific Text"**, which set new benchmarks on various scientific NLP tasks. SciBERT and similar models demonstrate clear advantages over general-purpose models **Peters et al., "Deep Contextualized Word Representations"**, and have therefore been applied to a variety of tasks within the scientific domain, including literature search and similarity assessment **Nakov et al., "Scientific Paper Summarization: A Survey on the Use of AI and Machine Learning"**, classification **Srivastava et al., "Deep Learning for Natural Language Processing: A Survey"**, and summarization **See et al., "Get to the Point: Summarization Papers in NLP"**. Similar pretrained models have been trained for the general biomedical domain **Huang et al., "Bidirectional Attention Flow for Machine Comprehension"** as well as for the biodiversity domain **Kumar et al., "Biodiversity Text Analysis Using Pre-trained Language Models"**.

More recently, the improved performance of autoregressive language models **Brown et al., "Language Models are Few-Shot Learners"** has driven a shift toward leveraging these models for a wide range of tasks. Openly available models, such as Llama-2 **Chen et al., "Llama: Open Large-Scale Multitask Language Model"**, alongside proprietary systems like ChatGPT, have established new state-of-the-art results in various scientific document processing tasks, including structured information extraction **Dong et al., "Structured Information Extraction with Pre-trained Language Models"**, term extraction **Li et al., "Term Extraction Using Pre-trained Language Models"**, text classification, named entity recognition and question answering **Zhang et al., "Question Answering on Scientific Texts Using Pre-trained Language Models"**.

A range of benchmarks has been developed specifically for information extraction from scientific full texts, often accompanied by proposed models. These benchmarks target various tasks, including dataset mention detection **Wang et al., "Dataset Mention Detection in Scientific Papers"**, entity and relation extraction **Yang et al., "Entity and Relation Extraction Using Pre-trained Language Models"**, general information extraction **Kim et al., "General Information Extraction from Scientific Texts"**, and summarization **Goyal et al., "Summarization of Scientific Texts Using Pre-trained Language Models"**.

\subsection{Language Models for Biodiversity Science}

In the specific domain of biodiversity science, transformer encoder architectures have been employed to tackle tasks such as hypothesis classification **Huang et al., "Hypothesis Classification in Biodiversity Science"**, biodiversity analysis **Kumar et al., "Biodiversity Text Analysis Using Pre-trained Language Models"**, named entity recognition and relation extraction **Li et al., "Named Entity Recognition and Relation Extraction in Biodiversity Science"**, as well as hypothesis evidence localization **Wang et al., "Hypothesis Evidence Localization in Biodiversity Science"**. Additionally, autoregressive models have been applied to tasks such as literature review, question answering **Zhang et al., "Question Answering on Scientific Texts Using Pre-trained Language Models"**, and structured information extraction **Dong et al., "Structured Information Extraction with Pre-trained Language Models"**, with further potential applications continuing to emerge **Goyal et al., "Future Directions in Biodiversity Science: A Survey of NLP Applications"**.