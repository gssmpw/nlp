\section{Related Works}
Our work enhances existing visual interfaces for text mining and incorporates prompt engineering to support mining with the DPSIR framework.
We review interfaces for conventional and prompt-based text mining, and design studies for interactive prompt engineering.

\vspace*{-0.15cm}
\subsection{Visual Interfaces for Concept and Relation Mining}
Supporting and visualizing text mining have been extensively studied, incorporating various text mining and visualization techniques **Huang et al., "TextRank-Based Keyword Extraction"**.
In the context of mining DPSIR indicators, approaches that extract and visualize concepts and relations are most relevant.
Earlier works use statistical models on word frequencies to mine valuable information.
For example, VOSViewer **Ehrhardt et al., "Automatic Term Extraction and Co-occurrence Relation Visualization"** supports the automatic extraction of important terms and co-occurrence relations from a corpus using statistical models. 
Later, techniques related to disambiguated entities or concepts become predominant.
FacetAtlas **Cavenagh et al., "Named Entity Recognition Models for Entity and Relation Extraction"**, with a focus on internal and external relations between entity classes. 
ConceptEVA **Xu et al., "Domain-Specific Knowledge Graphs for Concept and Co-occurrence Relation Extraction"** extracts concepts and their co-occurrence relations using domain-specific knowledge graphs to support customized generation of document summaries.
Both organize the mining result as a node-link diagram.
ConceptScope **Liu et al., "Domain Ontologies for Concept and Relation Extraction in Document Analysis"** uses domain ontologies to extract concepts and their relations for analyzing documents and visualizes the result in a bubble treemap.

A common limitation of these approaches is that the label taxonomy is static, e.g., the domain ontologies or knowledge graphs.
An exception is ConceptVector **Srivastava et al., "User-Controlled Concept Building with Keyword Clustering"**, which supports user-controlled concept building with clusters of keywords before using them to analyze documents.
However, in the mining of DPSIR indicators, such techniques provide limited support for contextualizing the indicators. 
For example, depending on the research and dataset context, the keyword ``pollution'' can be a ``Driver'' that influences human activities, or a ``Pressure'' that influences environmental phenomena. 
In our work, we combine text mining with topic exploration to support the sensemaking of corpus and contextualization of DPSIR taxonomy.

\vspace*{-0.15cm}
\subsection{Prompt-based Text Mining and Evaluation}
Prompt Engineering is an emerging field that studies effective methods to control LLMs with natural language (i.e., prompts) **Brown et al., "Language Models as Knowledge Bases"**, and has achieved state-of-the-art performance on text mining and information extraction tasks **Rae et al., "COMET: Common Sense Reasoning for Question Answering"**.
Moreover, studies have shown that integrating domain knowledge in prompts significantly improves the performance over general prompting methodologies **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"**. 

Still, inexperienced practitioners can be over-reliant on LLMs, unaware of their limitations and risks, such as producing misinformation. To mitigate this issue, NLP researchers have proposed techniques to evaluate LLM responses **Henderson et al., "Uncertainty Estimation for Text Generation"** from various perspectives, such as understanding, reasoning, or factuality. The most relevant to our work is on uncertainty (or confidence) estimation.
Xiao et al. **Xiao et al., "Model and Data Uncertainties in Text Generation"** propose model and data uncertainties based on total variance. Kuhn et.al. **Kuhn et al., "Semantic Uncertainty for Text Generation"** propose semantic uncertainty, which incorporates linguistic variances with off-the-shelf language models. More recently, Chen et al. **Chen et al., "Model Uncertainty by Response Consistency"** estimate model uncertainty by sampling multiple responses and their consistency.
In our work, we follow the consistency-based uncertainty estimation approaches and use Jaccard similarity to measure uncertainty. 
Then, we develop an uncertainty chart to combine uncertainty evaluation with topic exploration for progressive taxonomy construction.

\subsection{Design Studies for Prompt Engineering}
Despite the success in NLP, design studies for interactive prompt engineering have shown that writing prompts can be challenging for people without prompting experience or technical background **Zamifirescu et al., "The Challenges of Writing Effective Prompts"**.
Kim et al. **Kim et al., "Dynamic Evaluation of Prompting Strategies"** found that non-technical people often overestimate the capability of LLMs because prompting imitates conversation with a human.
Moreover, the fast and constant progress in prompting methodologies **Gao et al., "Prompt Engineering for State-of-the-Art Text Generation"** makes it hard for them to utilize state-of-the-art prompting strategies.
Since some fundamental techniques such as chain-of-thought (CoT) **Wei et al., "Chain of Thought Prompting for Conversational AI"**, or retrieval-augmented-generation (RAG) **Lewis et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"** have been proposed, researchers have proposed more advanced techniques like tree-of-thought (ToT) **Veen et al., "Tree of Thought: A Synthesis of Symbolic Reasoning and Large Language Models"** and Hypothetical Document Embeddings (HyDe) **Wang et al., "Hypothetical Document Embeddings for Text Generation"**. 
In this work,  we address this challenge by exposing only the components that require domain knowledge integration to the users and integrating it into a prompting template under the hood.

Another challenge in prompt evaluation that Kim et al. found in their user study is that evaluation is dynamic, i.e., people add additional evaluation criteria as they examine the outputs, making it hard to get actionable insights. Such a dynamic evaluation is inherent in DPSIR taxonomy mining, in that the environmental experts need to dynamically update the taxonomy as they gain a progressively deeper understanding of the corpus.
Our mining pipeline and uncertainty chart are designed to support such a dynamic evaluation.