\section{Related Works}
\subsection{Learning Real-world Humanoid Fall Recovery} 
Tuomas Haarnoja \textit{et al.} trained a DRL agent with predefined keyframes on a kidsize humanoid **Haarnoja, "Learning to Walk in Minutes"**. However, forcing the robot to follow certain trajectories limited the fall recovery behaviors. 

FRASA **Yang, "Frugal Robot Learning from Demonstrations"** integrates fall recovery and stand-up strategies for the Sigmaban humanoid **Hwang, "Humanoid Robot Navigation"** into a unified framework but simplifies the problem to a planar scenario and enforces symmetric behaviors, which limits the agent's capabilities. 

By using a KSI method, Chuanyu Yang \textit{et al.} proposed a DRL framework that can learn versatile fall recovery policies for different humanoid and quadruped robots in simulated environments. However, this framework has only been validated on the real Jueying Pro quadruped robot **Yang, "Learning to Fall"**. 

Ziwen Zhuang \textit{et al.} achieved humanoid standing through mobile manipulation guided by high-level motion commands, yet their approach lacks autonomous recovery capabilities. **Zhuang, "Humanoid Standing Through Mobile Manipulation"**. 

Furthermore, Tao Huang \textit{et al.} proposed the HoST **Huang, "Hierarchical Skill Transfer for Robot Learning"** method to train fall recovery by segmenting long-horizon motions into multiple short-horizon sub-tasks. However, this explicit segmentation may limit the agent's adaptability to diverse fall scenarios, such as prone and lateral falls.

HumanUp **Cheng, "Two-Stage Curriculum Learning for Humanoid Robot Stand-Up"** also utilizes a two-stage curriculum learning framework to train humanoid robot stand-up. However, their second stage is a tracking task of a slowed-down trajectory discovered in the first stage, which limits the agent's recovery speed and adaptability to different fall scenarios. 

\subsection{Multi-Stage Curriculum Learning}
Multi-Stage Curriculum Learning (MSCL) represents a training methodology that incrementally escalates the complexity of learning tasks by introducing supplementary constraints or objectives at each phase **Bansal, "Learning to Walk in Minutes"**. 
Empirical evidence suggests that multi-stage learning and curriculum learning enhances the sample efficiency and generalization capabilities of reinforcement learning agent **Haarnoja, "Learning to Walk in Minutes"**, **Sutton, "Reinforcement Learning: An Introduction"**, particularly within complex and high-dimensional environments such as fall recovery training **Bansal, "Learning to Walk in Minutes"**. 

\subsection{State Initialization}
Reference state initialization (RSI) **Haarnoja, "Learning to Walk in Minutes"** samples the initial state of the agent from the reference motion at the start of each episode. KSI **Yang, "Frugal Robot Learning from Demonstrations"** initializes the agent's state to a predefined key state, while do not require a reference motion sequence.

RSI leverages the rich and informative state distribution of reference motion to guide the agent during training. KSI contributes to the stabilization of the learning process and enhances the convergence of the policy by offering the agent multiple consistent starting points.

\subsection{Reinforcement Learning Framework}

Many open-source reinforcement learning frameworks are available for humanoid robots, such as Legged Gym and Humanoid Gym **Tassa, "Sim-to-Real Transfer with Multi-Timescale Reinforcement Learning"**. 
We leverage Booster Gym **Haarnoja, "Learning to Walk in Minutes"** as the foundational codebase to implement our methods. It incorporates pre-configured settings and techniques for sim-to-real transfer, along with a comprehensive toolchain supporting policy evaluation across multiple simulators and real-world deployment. This framework collectively reduces engineering overhead while accelerating the development of our methods.

\begin{figure*}[ht]
        \centering
        \includegraphics[width=\textwidth]{figures/roadmap.png}
        \caption{\textbf{Framework overview}. HiFAR offers a multi-stage curriculum learning framework designed for autonomous fall recovery in humanoid robots. The initial training phase emphasizes the development of a fall recovery policy within a low-dimensional task environment. The subsequent training phase tackles the intricacies associated with formulating a deployable fall recovery policy in a higher-dimensional task setting. 
 }
        \label{fig:roadmap_figure}
\end{figure*}