\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\label{sec:appandix}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.83\textwidth]{images/partial_attention_block.pdf}
  \caption{Detailed of three PATConv blocks. Where $\odot$ and $\otimes$ denote element-wise multiplication and matrix multiplication respectively, and $C=C_p+C_{p^{'}}$.}
  \label{fig:attention_block}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.82\textwidth]{images/convnext_loss.pdf}
  \caption{The training process of ConvNext-tiny with and without PATConv (i.e., PAT\_ch).}
  \label{fig:abla_convnext}
\end{figure*}

\section{Overview}
In this supplementary material, we present more explanations and experimental results.
\begin{itemize}
  \item Firstly, we provide detailed explanations of our experimental setup, the specifics of the three PATConv blocks, and the different PartialNet variants.
  \item Secondly, we present a comprehensive comparison of the classification task on the ImageNet-1k benchmark, as well as object detection and instance segmentation tasks on the COCO dataset.
  \item Finally, we provide additional ablation studies for our proposed Partial Attention Convolution (PATConv) and show the training process of ConvNext-tiny with and without PATConv (i.e., PAT\_ch).
\end{itemize}

\begin{table}[ht]
  \small
  \centering
  \resizebox{1\linewidth}{!}{%
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{@{}l|ccc@{}}
      \toprule
      Variants           & \qquad S \qquad \qquad                                            & \qquad M \qquad \qquad & L      \\
      \midrule
      Train and test Res & \multicolumn{3}{c}{shorter side $=$ 800, longer side $\leq$ 1333}                                   \\
      Batch size         & \multicolumn{3}{c}{16 (2 on each GPU)}                                                              \\
      Optimizer          & \multicolumn{3}{c}{AdamW}                                                                           \\
      Train schedule     & \multicolumn{3}{c}{1$\times$ schedule (12 epochs)}                                                  \\
      Weight decay       & \multicolumn{3}{c}{0.0001}                                                                          \\
      Warmup schedule    & \multicolumn{3}{c}{linear}                                                                          \\
      Warmup iterations  & \multicolumn{3}{c}{500}                                                                             \\
      LR decay           & \multicolumn{3}{c}{StepLR at epoch 8 and 11 with decay rate 0.1}                                    \\
      LR                 & 0.0002                                                            & 0.0001                 & 0.0001 \\
      Stoch. Depth       & 0.15                                                              & 0.2                    & 0.3    \\
      \bottomrule
    \end{tabular}%
  }
  \caption{Experimental settings of object detection and instance segmentation on the COCO2017 dataset.}
  \vspace{-0.2in}
  \label{tab:coco_settings}
\end{table}

\section{Clarifications on Experimental Setting}
Firstly, we provide the ImageNet-1k training and evaluation settings in~\cref{tab:imagenet_settings}. These settings can be used to reproduce our main results in Figure 1 of the main paper. Different PartialNet variants vary in the magnitude of regularization and augmentation techniques. The magnitude increases as the model size increases to alleviate overfitting and improve accuracy. It is worth noting that most of the works compared in Figure 1 of the main paper, such as MobileViT, FastNet, ConvNeXt, Swin, etc., also adopt such advanced training techniques (ADT), with some even heavily relying on hyper-parameter search. For other models without ADT, such as ShuffleNetV2, MobileNetV2, and GhostNet, although the comparison is not entirely fair, we include them for reference. Moreover, for object detection and instance segmentation on the COCO2017 dataset, we equip our PartialNet backbone with the popular Mask R-CNN detector. We use ImageNet-1k pre-trained weights to initialize the backbone and Xavier initialization for the add-on layers. Detailed settings are summarized in~\cref{tab:coco_settings}.

\begin{table}[ht]
  \small
  \centering
  \resizebox{1\linewidth}{!}{
    \begin{tabular}{@{}l|cccccc@{}}
      \toprule
      Variants           & T0                                & T1           & T2          & S           & M       & L     \\
      \hline
      Train Res          & \multicolumn{6}{c}{Random select from \{128,160,192,224,256,288\}}                             \\
      Test Res           & \multicolumn{6}{c}{224}                                                                        \\
      \hline
      Epochs             & \multicolumn{6}{c}{300}                                                                        \\
      \# of forward pass & \multicolumn{6}{c}{188k}                                                                       \\
      \hline
      Batch size         & 4096                              & 4096         & 4096        & 4096        & 2048    & 2048  \\
      Optimizer          & \multicolumn{6}{c}{AdamW}                                                                      \\
      Momentum           & \multicolumn{6}{c}{0.9/0.999}                                                                  \\
      LR                 & 0.004                             & 0.004        & 0.004       & 0.004       & 0.002   & 0.002 \\
      LR decay           & \multicolumn{6}{c}{cosine}                                                                     \\
      Weight decay       & 0.005                             & 0.01         & 0.02        & 0.03        & 0.05    & 0.05  \\
      Warmup epochs      & \multicolumn{6}{c}{20}                                                                         \\
      Warmup schedule    & \multicolumn{6}{c}{linear}                                                                     \\
      \hline
      Label smoothing    & \multicolumn{6}{c}{0.1}                                                                        \\
      Dropout            & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      Stoch. Depth       & \ding{55}                         & 0.02         & 0.05        & 0.1         & 0.2     & 0.3   \\
      Repeated Aug       & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      Gradient Clip.     & \ding{55}                         & \ding{55}    & \ding{55}   & \ding{55}   & 1       & 0.01  \\
      \hline
      H. flip            & \multicolumn{6}{c}{\ding{51}}                                                                  \\
      RRC                & \multicolumn{6}{c}{\ding{51}}                                                                  \\
      Rand Augment       & \ding{55}                         & 3/0.5        & 5/0.5       & 7/0.5       & 7/0.5   & 7/0.5 \\
      Auto Augment       & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      Mixup alpha        & 0.05                              & 0.1          & 0.1         & 0.3         & 0.5     & 0.7   \\
      Cutmix alpha       & \multicolumn{6}{c}{1.0}                                                                        \\
      Erasing prob.      & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      Color Jitter       & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      PCA lighting       & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      \hline
      SWA                & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      EMA                & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      \hline
      Layer scale        & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      \hline
      CE loss            & \multicolumn{6}{c}{\ding{51}}                                                                  \\
      BCE loss           & \multicolumn{6}{c}{\ding{55}}                                                                  \\
      \hline
      Mixed precision    & \multicolumn{6}{c}{\ding{51}}                                                                  \\
      \hline
      Test crop ratio    & \multicolumn{6}{c}{0.9}                                                                        \\
      \hline
      Top-1 acc. (\%)    & 73.9                              & 78.1          & 80.2        & 82.1        & 83.1   & 83.9  \\
      \bottomrule
    \end{tabular}
  }
  \caption{ImageNet-1k training and evaluation settings for different PartialNet variants.}
  \label{tab:imagenet_settings}
\end{table}

\begin{table*}[ht]
  % \small
  \centering
  \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{@{}c|c|c|c|c|c|c|c|c|c@{}}
      \toprule
      Name                            & Output size                                & \multicolumn{2}{c|}{Layer specification}                                                                                                                                                                      & T0   & T1   & T2   & S     & M    & L           \\
      \hline
      Embedding                       & \large{$\frac{h}{4} \times \frac{w}{4}$}   & \begin{tabular}[c]{@{}c@{}}Conv\_4\_$c$\_4,\\ BN\end{tabular}                                                                                                                              & \# Channels $c$  & 32   & 48   & 64    & 96   & 128  & 160  \\
      \hline
      Stage 1                         & \large{$\frac{h}{4} \times \frac{w}{4}$}   & $\left[ \texttt{\begin{tabular}[c]{@{}c@{}}PAT\_ch\_3\_$c$\_1\_1/4,\\ Conv\_1\_$2c$\_1,\\ BN, Acti,\\ Conv\_1\_$c$\_1,\\ PAT\_sp\_1\_$c$\_1\_1/4 \end{tabular}}  \right] \times b_1 $      & \# Blocks $b_1$  & 1    & 1    & 2     & 2    & 2    & 2    \\
      \hline
      Merging                         & \large{$\frac{h}{8} \times \frac{w}{8}$}   & \begin{tabular}[c]{@{}c@{}}Conv\_2\_$2c$\_2,\\ BN\end{tabular}                                                                                                                             & \# Channels $2c$ & 64   & 96   & 128   & 192  & 256  & 320  \\
      \hline
      Stage 2                         & \large{$\frac{h}{8} \times \frac{w}{8}$}   & $\left[ \texttt{\begin{tabular}[c]{@{}c@{}}PAT\_ch\_3\_$2c$\_1\_1/4,\\ Conv\_1\_$4c$\_1,\\ BN, Acti,\\ Conv\_1\_$2c$\_1,\\ PAT\_sp\_1\_$2c$\_1\_1/4 \end{tabular}}  \right] \times b_2 $   & \# Blocks $b_2$  & 2    & 2    & 2     & 2    & 3    & 3    \\
      \hline
      Merging                         & \large{$\frac{h}{16} \times \frac{w}{16}$} & \begin{tabular}[c]{@{}c@{}}Conv\_2\_$4c$\_2,\\ BN\end{tabular}                                                                                                                             & \# Channels $4c$ & 128  & 192  & 256   & 384  & 512  & 640  \\
      \hline
      Stage 3                         & \large{$\frac{h}{16} \times \frac{w}{16}$} & $\left[ \texttt{\begin{tabular}[c]{@{}c@{}}PAT\_ch\_3\_$4c$\_1\_1/4,\\ Conv\_1\_$8c$\_1,\\ BN, Acti,\\ Conv\_1\_$4c$\_1,\\ PAT\_sp\_1\_$4c$\_1\_1/4 \end{tabular}}  \right] \times b_3 $   & \# Blocks $b_3$  & 8    & 8    & 6     & 9    & 16   & 20   \\
      \hline
      Merging                         & \large{$\frac{h}{32} \times \frac{w}{32}$} & \begin{tabular}[c]{@{}c@{}}Conv\_2\_$8c$\_2,\\ BN\end{tabular}                                                                                                                             & \# Channels $8c$ & 256  & 384  & 512   & 768  & 1024 & 1280 \\
      \hline
      Stage 4                         & \large{$\frac{h}{32} \times \frac{w}{32}$} & $\left[ \texttt{\begin{tabular}[c]{@{}c@{}}PAT\_ch\_3\_$8c$\_1\_1/4,\\ Conv\_1\_$16c$\_1,\\ BN, Acti,\\ Conv\_1\_$8c$\_1,\\ PAT\_sf\_1\_$8c$\_1\_1/4 \end{tabular}}  \right] \times b_4 $  & \# Blocks $b_4$  & 2    & 2    & 4     & 4    & 4    & 4    \\
      \hline
      Classifier                      & $1  \times 1$                              & \begin{tabular}[c]{@{}c@{}}Global average pool,\\ Conv\_1\_1280\_1,\\ Acti,\\ FC\_1000\end{tabular}                                                                                        & Acti             & GELU & GELU & ReLU  & ReLU & ReLU & ReLU \\
      \hline
      \multicolumn{4}{c|}{Params (M)} & 4.3                                        & 7.8                                                                                                                                                                                        & 12.6             & 29.0 & 61.3 & 104.4                      \\
      \hline
      \multicolumn{4}{c|}{FLOPs (G)}  & 0.25                                       & 0.55                                                                                                                                                                                       & 1.03             & 2.71 & 6.69 & 11.91                       \\
      \bottomrule
    \end{tabular}%
  }
  \caption{Configurations of different PartialNet variants. 'Conv\_$k$\_$c$\_$s$' refers to a convolutional layer with a kernel size of $k$, output channels of $c$, and a stride of $s$. 'PAT\_ch$\_k\_c\_s\_r$' refers to a partial attention convolution with an additional parameter, the split ratio $r$ of feature map channels, compared to a regular convolution. Similarly, 'PAT\_sp$\_k\_c\_s\_r$' and 'PAT\_sf$\_k\_c\_s\_r$' have the same configuration. Additionally, 'FC\_1000' refers to a fully connected layer with 1000 output channels. The $h \times w$ represents the input size, while $b\_i$ denotes the number of PartialNet blocks at stage $i$. FLOPs are calculated for an input size of $224 \times 224$.}
  \label{tab:configuration}
\end{table*}

Secondly, the configurations of different PartialNet variants are presented in~\cref{tab:configuration}. We also provide the detailed structures of the three different PATConv blocks, as shown in~\cref{fig:attention_block}.

\section{Full Comparison on the ImageNet-1k Benchmark and COCO Benchmark.}
For the full comparison of the classification task on the ImageNet-1k Benchmark, please refer to~\cref{tab:appendix_class_Benchmark}, which complements the results provided in Table~1 of the main paper. For the full Comparison of the object detection and instance segmentation tasks on the COCO2017 dataset please refer to~\cref{tab:appendix_dect_and_seg_Benchmark}, which complements the results provided in Table~2 of the main paper. 

\begin{table*}[ht] \small
  \centering
  \resizebox{1.0\linewidth}{!}{
  \begin{tabular}{@{}lcccccccc@{}}
    \toprule
    Network   &\makecell{Type}   & \makecell{Params\\(M)}	&\makecell{FLOPs\\(G)}	  &\makecell{Throughput\\V100 (FPS)$\uparrow$} &\makecell{Throughput\\MI250 (FPS)$\uparrow$}	&\makecell{Latency\\CPU (ms)$\downarrow$} &\makecell{Top-1\\(\%)$\uparrow$}  \\
    \hline
    ShuffleNetV2 x1.5\cite{Ma2018}      & CNN             & 3.5              & 0.30  & 5315       & 6642        & 13.7        & 72.6       \\
    MobileNetV2\cite{Sandler2018a}      & CNN             & 3.5              & 0.31  & 3924       & 7359        & 13.7        & 72.0       \\
    FasterNet-T0\cite{Chen2023}         & CNN             & 3.9              & 0.34  & {\bf 8546} & 10612       & {\bf 10.5}  & 71.9       \\
    MobileViT-XXS\cite{Mehta2021}       & Hybrid          & 1.3              & 0.42  & 2900       & 3321        & 16.7        & 69.0       \\
    MobileViTv2-0.5\cite{Mehta2022}     & Hybrid          & 1.4              & 0.46  & 3094       & 3135        & 15.8        & 70.2       \\
    PartialNet-T0(ours)                 & Hybrid          & 4.3              & 0.25  & 7777       & {\bf 11744} & 12.2        & {\bf 73.9} \\
    \hline
    EfficientNet-B0\cite{Tan2019}       & CNN             & 5.3              & 0.39  & 2934       & 3344        & 22.7        & 77.1       \\
    GhostNet x1.3\cite{Han2020}         & CNN             & 7.4              & 0.24  & 3788       & 3620        & 16.7        & 75.7       \\
    ShuffleNetV2 x2\cite{Ma2018}        & CNN             & 7.4              & 0.59  & 4290       & 5371        & 22.6        & 74.9       \\
    MobileNetV2 x1.4\cite{Sandler2018a} & CNN             & 6.1              & 0.60  & 2615       & 4142        & 21.7        & 74.7       \\
    FasterNet-T1\cite{Chen2023}         & CNN             & 7.6              & 0.85  & {\bf 4648} & 7198        & 22.2        & 76.2       \\
    EfficientViT-B1-192\cite{Cai2023b}  & Hybrid          & 9.1              & 0.38  & 4072       & 3912        & {\bf 19.3}  & 77.7       \\
    MobileViT-XS\cite{Mehta2021}        & Hybrid          & 2.3              & 1.05  & 1663       & 1884        & 32.8        & 74.8       \\
    PartialNet-T1(ours)                 & Hybrid          & 7.8              & 0.55  & 4403       & {\bf 7379}  & 21.5        & {\bf 78.1} \\
    \hline
    EfficientNet-B1\cite{Tan2019}       & CNN             & 7.8              & 0.70  & 1730       & 1583        & 35.5        & 79.1       \\
    ResNet50\cite{He2015}               & CNN             & 25.6             & 4.11  & 1258       & 3135        & 94.8        & 78.8       \\
    FasterNet-T2\cite{Chen2023}         & CNN             & 15.0             & 1.91  & 2455       & 4189        & 43.7        & 78.9       \\
    PoolFormer-S12\cite{Yu2022a}        & Hybrid          & 11.9             & 1.82  & 1927       & 3558        & 56.1        & 77.2       \\
    MobileViT-S\cite{Mehta2021}         & Hybrid          & 5.6              & 2.03  & 1219       & 1370        & 52.4        & 78.4       \\
    MobileViTv2-1.0\cite{Mehta2022}     & Hybrid          & 4.9              & 1.85  & 1391       & 1543        & 41.5        & 78.1       \\
    EfficientViT-B1\cite{Cai2023b}      & Hybrid          & 9.1              & 0.52  & 3072       & 3387        & {\bf 25.7}  & 79.4       \\
    PartialNet-T2(ours)                 & Hybrid          & 12.6             & 1.03  & {\bf 3074} & {\bf 4761}  & 35.2        & {\bf 80.2} \\
    \hline
    EfficientNet-B3\cite{Tan2019}       & CNN             & 12.0             & 1.80  & 768        & 926         & 73.5        & 81.6       \\
    ConvNeXt-T\cite{Liu2022b}           & CNN             & 28.6             & 4.47  & 902        & 1103        & 99.4        & {\bf 82.1} \\
    FasterNet-S\cite{Chen2023}          & CNN             & 31.1             & 4.56  & 1261       & 2243        & 96.0        & 81.3       \\
    PoolFormer-S36\cite{Yu2022a}        & Hybrid          & 30.9             & 5.00  & 675        & 1092        & 152.4       & 81.4       \\
    MobileViTv2-1.5\cite{Mehta2022}     & Hybrid          & 10.6             & 4.00  & 812        & 1000        & 104.4       & 80.4       \\
    MobileViTv2-2.0\cite{Mehta2022}     & Hybrid          & 18.5             & 7.50  & 551        & 684         & 103.7       & 81.2       \\
    Swin-T\cite{Liu2021c}               & Hybrid          & 28.3             & 4.51  & 808        & 1192        & 107.1       & 81.3       \\
    PartialNet-S(ours)                  & Hybrid          & 29.0             & 2.71  & {\bf 1559} & {\bf 2422}  & {\bf 72.5}  & {\bf 82.1} \\
    \hline
    EfficientNet-B4\cite{Tan2019}       & CNN             & 19.0             & 4.20  & 356        & 442         & 156.9       & 82.9       \\
    ConvNeXt-S\cite{Liu2022b}           & CNN             & 50.2             & 8.71  & 510        & 610         & 185.5       & {\bf 83.1} \\
    FasterNet-M\cite{Chen2023}          & CNN             & 53.5             & 8.74  & 621        & 1098        & 181.6       & 83.0       \\
    PoolFormer-M36\cite{Yu2022a}        & Hybrid          & 56.2             & 8.80  & 444        & 721         & 244.3       & 82.1       \\
    Swin-S\cite{Liu2021c}               & Hybrid          & 49.6             & 8.77  & 477        & 732         & 199.1       & 83.0       \\
    % EfficientViT-B2                   &Hybrid   & 24.3      & 1.57  &1429       & 1483        & {\bf 62.3}  & 82.7       \\
    PartialNet-M(ours)                  & Hybrid          & 61.3             & 6.69  & {\bf 799}  & {\bf 1280}  & {\bf 155.3} & {\bf 83.1} \\
    \hline
    EfficientNet-B5\cite{Tan2019}       & CNN             & 30.0             & 9.90  & 246        & 313         & 333.3       & 83.6       \\
    ConvNeXt-B\cite{Liu2022b}           & CNN             & 88.6             & 15.38 & 322        & 430         & 317.1       & 83.8       \\
    FasterNet-L\cite{Chen2023}          & CNN             & 93.5             & 15.52 & 384        & 709         & 312.5       & 83.5       \\
    PoolFormer-M48\cite{Yu2022a}        & Hybrid          & 73.5             & 11.59 & 335        & 556         & 322.3       & 82.5       \\
    Swin-B\cite{Liu2021c}               & Hybrid          & 87.8             & 15.47 & 315        & 520         & 333.8       & 83.5       \\
    PartialNet-L(ours)                  & Hybrid          & 104.3            & 11.91 & {\bf 426}  & {\bf 765}   & {\bf 272.5} & {\bf 83.9} \\
    \bottomrule
  \end{tabular}
  }
  \caption{Full comparison on ImageNet-1k Benchmark: models with similar top-1 accuracy are grouped together. The best results are in bold.}
  \label{tab:appendix_class_Benchmark}
\end{table*}

\begin{table*}[ht] \small
  \centering
  \resizebox{1.0\linewidth}{!}{
  \begin{tabular}{@{}lccccc cccc@{}}
    \toprule
    Backbone              & \makecell{Params\\(M)}	&\makecell{FLOPs\\(G)} &\makecell{Throughput\\MI250 (FPS)$\uparrow$}	&\makecell{$AP^{b}\uparrow$}  &\makecell{$AP^{b}_{50}\uparrow$} &\makecell{$AP^{b}_{75}\uparrow$} &\makecell{$AP^{m}\uparrow$} &\makecell{$AP^{m}_{50}\uparrow$} &\makecell{$AP^{m}_{75}\uparrow$}\\
    \hline
    ResNet50\cite{He2015}                 & 44.2    & 253   &121         & 38.0       & 58.6       & 41.4       & 34.4       & 55.1       & 36.7       \\
    PoolFormer-S24\cite{Yu2022a}          & 41.0    & 233   &68          & 40.1       & 62.2       & 43.4       & 37.0       & 59.1       & 39.6       \\
    PVT-Small x1.5\cite{Wang2021c}        & 44.1    & 238   &98          & 40.4       & 62.9       & 43.8       & 37.8       & 60.1       & 40.3       \\
    FasterNet-S\cite{Chen2023}            & 49.0    & 258   &121         & 39.9       & 61.2       & 43.6       & 36.9       & 58.1       & 39.7       \\
    PartialNet-S(ours)                    & 46.9    & 216   &{\bf 122}   & {\bf 42.7} & {\bf 64.9} & {\bf 46.5} & {\bf 39.3} & {\bf 61.8} & {\bf 42.2} \\
    \hline
    ResNet101\cite{Mehta2021}             & 63.2    & 329   &62          & 40.4       & 61.1       & 44.2       & 36.4       & 57.7       & 38.8       \\
    ResNeXt101-32$\times$4d\cite{Xie2017} & 62.8    & 333   &51          & 41.9       & 62.5       & 45.9       & 37.5       & 59.4       & 40.2       \\
    PoolFormer-S36\cite{Yu2022a}          & 50.5    & 266   &44          & 41.0       & 63.1       & 44.8       & 37.7       & 60.1       & 40.0       \\
    PVT-Medium\cite{Wang2021c}            & 63.9    & 295   &52          & 42.0       & 64.4       & 45.6       & 39.0       & 61.6       & 42.1       \\
    FasterNet-M\cite{Chen2023}            & 71.2    & 344   &62          & 43.0       & 64.4       & 47.4       & 39.1       & 61.5       & 42.3       \\
    PartialNet-M(ours)                    & 78.2    & 295   &{\bf 65}    & {\bf 44.3} & {\bf 65.8} & {\bf 48.5} & {\bf 40.6} & {\bf 63.3} & {\bf 43.7} \\
    \hline
    ResNeXt101-64$\times$4d\cite{Xie2017} & 101.9   & 487   &29          & 42.8       & 63.8       & 47.3       & 38.4       & 60.6       & 41.3       \\
    PVT-Large$\times$4d\cite{Wang2021c}   & 81.0    & 358   &26          & 42.9       & 65.0       & 46.6       & 39.5       & 61.9       & 42.5       \\
    FasterNet-L\cite{Chen2023}            & 110.9   & 484   &35          & 44.0       & 65.6       & 48.2       & 39.9       & 62.3       & 43.0       \\
    PartialNet-L(ours)                    & 122.0   & 397   &{\bf 39}    & {\bf 44.7} & {\bf 66.3} & {\bf 49.0} & {\bf 41.0} & {\bf 63.7} & {\bf 44.2} \\
    \bottomrule
  \end{tabular}
  }
  \caption{Results using PartialNet-S/M/L on object detection and instance segmentation benchmark in COCO dataset.}
  \label{tab:appendix_dect_and_seg_Benchmark}
\end{table*}

\section{The Complements of Ablation Studies}
\textbf{ Partial Attention vs. Classic Visual Attention:}
To further prove the superiority of our proposed PATConv, we present experiment results for the combination of our partial attention and classic visual attention networks, and the results are shown in \cref{tab:abla_convention_attention_acc}. The results demonstrate the effectiveness of our enhanced PATConv block, .e., PAT\_ch.

\textbf{Partial Attention Convolution (PATConv) vs. Partial Convolution (PConv) Under the Same Training Settings.}
In order to verify the effectiveness and ensure a fair comparison of our PartialNet, we reproduced the results of all FastNet variants on ImageNet-1k using our training experiment configuration. The results are shown in~\cref{tab:pat_vs_fasternet}. It can be seen from the results that our PartialNet still has significant advantages.

\begin{table*}[ht]
  \centering
  \resizebox{0.8\linewidth}{!}{
  \begin{tabular}{@{}lcccc c@{}}
    \toprule
    Visual type & \makecell{Params(M)} & \makecell{FLOPs(G)} & \makecell{Throughput(fps)$\uparrow$} & \makecell{latency(ms)$\downarrow$} & \makecell{Acc1(\%)$\uparrow$} \\
    \hline
    SRM~\cite{Lee2019a}               & 12.2       & 1.03     & 4751         & 35.2        & 79.6                          \\
    SE-NET~\cite{Hu2018}              & 12.3       & 1.04     & 4910         & 32.3        & 79.8                          \\
    PAT(ours)                         & 12.6       & 1.03     & 4761         & 35.2        & {\bf 80.2}                     \\
    \bottomrule
  \end{tabular}
  }
  \caption{Comparison on PartialNet-T2 of partial visual attention and conventional visual attention on ImageNet1K dataset.}
  \label{tab:abla_convention_attention_acc}
\end{table*}



\begin{table*}[ht] \small
  \centering
  \resizebox{0.87\linewidth}{!}{
  \begin{tabular}{@{}lcccccccc@{}}
    \toprule
    Network   &\makecell{Type}   & \makecell{Params\\(M)}	&\makecell{FLOPs\\(G)}	  &\makecell{Throughput\\V100 (FPS)$\uparrow$} &\makecell{Throughput\\MI250 (FPS)$\uparrow$}	&\makecell{Latency\\CPU (ms)$\downarrow$} &\makecell{Top-1\\(\%)$\uparrow$}  \\
    \hline
    FasterNet-T0\cite{Chen2023}         & CNN             & 3.9              & 0.34  & {\bf 8546} & 10612       & {\bf 10.5}  & 71.9       \\
    FasterNet-T0*\cite{Chen2023}        & CNN             & 3.9              & 0.34  & {\bf 8546} & 10612       & {\bf 10.5}  & 71.0       \\
    PartialNet-T0(ours)                 & Hybrid          & 4.3              & 0.25  & 7777       & {\bf 11744} & 12.2        & {\bf 73.9} \\
    \hline
    FasterNet-T1\cite{Chen2023}         & CNN             & 7.6              & 0.85  & {\bf 4648} & 7198        & 22.2        & 76.2       \\
    FasterNet-T1*\cite{Chen2023}        & CNN             & 7.6              & 0.85  & {\bf 4648} & 7198        & 22.2        & 76.5       \\
    PartialNet-T1(ours)                 & Hybrid          & 7.8              & 0.55  & 4403       & {\bf 7379}  & 21.5        & {\bf 78.1} \\
    \hline
    FasterNet-T2\cite{Chen2023}         & CNN             & 15.0             & 1.91  & 2455       & 4189        & 43.7        & 78.9       \\
    FasterNet-T2*\cite{Chen2023}        & CNN             & 15.0             & 1.91  & 2455       & 4189        & 43.7        & 79.2       \\
    PartialNet-T2(ours)                 & Hybrid          & 12.6             & 1.03  & {\bf 3074} & {\bf 4761}  & 35.2        & {\bf 80.2} \\
    \hline
    FasterNet-S\cite{Chen2023}          & CNN             & 31.1             & 4.56  & 1261       & 2243        & 96.0        & 81.3       \\
    FasterNet-S\cite{Chen2023}          & CNN             & 31.1             & 4.56  & 1261       & 2243        & 96.0        & 81.5       \\
    PartialNet-S(ours)                  & Hybrid          & 29.0             & 2.71  & {\bf 1559} & {\bf 2422}  & {\bf 72.5}  & {\bf 82.1} \\
    \hline
    FasterNet-M\cite{Chen2023}          & CNN             & 53.5             & 8.74  & 621        & 1098        & 181.6       & 83.0       \\
    FasterNet-M*\cite{Chen2023}         & CNN             & 53.5             & 8.74  & 621        & 1098        & 181.6       & 83.0       \\
    PartialNet-M(ours)                  & Hybrid          & 61.3             & 6.69  & {\bf 799}  & {\bf 1280}  & {\bf 155.3} & {\bf 83.1} \\
    \hline
    FasterNet-L\cite{Chen2023}          & CNN             & 93.5             & 15.52 & 384        & 709         & 312.5       & 83.5       \\
    FasterNet-L*\cite{Chen2023}         & CNN             & 93.5             & 15.52 & 384        & 709         & 312.5       & 83.6       \\
    PartialNet-L(ours)                  & Hybrid          & 104.3            & 11.91 & {\bf 426}  & {\bf 765}   & {\bf 272.5} & {\bf 83.9} \\
    \bottomrule
  \end{tabular}
  }
  \caption{Comparison on ImageNet-1k. The "*" denotes reproduced results based on our experimental setups.}
  \label{tab:pat_vs_fasternet}
\end{table*}
