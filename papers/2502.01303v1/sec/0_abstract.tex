\begin{abstract}
    Designing a module or mechanism that enables a network to maintain low parameters and FLOPs without sacrificing accuracy and throughput remains a challenge. To address this challenge and exploit the redundancy within feature map channels, we propose a new solution: {\bf p}artial {\bf c}hannel {\bf m}echanism ({\bf PCM}). Specifically, through the split operation, the feature map channels are divided into different parts, with each part corresponding to different operations, such as convolution, attention, pooling, and identity mapping. Based on this assumption, we introduce a novel {\bf p}artial {\bf at}tention {\bf conv}olution ({\bf PATConv}) that can efficiently combine convolution with visual attention. Our exploration indicates that the PATConv can completely replace both the regular convolution and the regular visual attention while reducing model parameters and FLOPs. Moreover, PATConv can derive three new types of blocks: Partial Channel-Attention block (PAT\_ch), Partial Spatial-Attention block (PAT\_sp) and Partial Self-Attention block (PAT\_sf). In addition, we propose a novel {\bf d}ynamic {\bf p}artial {\bf conv}olution ({\bf DPConv}) that can adaptively learn the proportion of split channels in different layers to achieve better trade-offs. Building on PATConv and DPConv, we propose a new hybrid network family, named {\bf PartialNet}, which achieves superior top-1 accuracy and inference speed compared to some SOTA models on ImageNet-1K classification and excels in both detection and segmentation on the COCO dataset. Our code is available at \url{https://github.com/haiduo/PartialNet}.
\end{abstract}