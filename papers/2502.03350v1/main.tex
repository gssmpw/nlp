\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
%\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[draft]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Additional packages
\usepackage{bm}
\bibliographystyle{unsrtnat}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% New Commands
\newcommand{\va}{\bm{a}}
\newcommand{\vb}{\bm{b}}
\newcommand{\vc}{\bm{c}}

\newcommand{\vg}{\bm{g}}
\newcommand{\vh}{\bm{h}}

\newcommand{\vr}{\bm{r}}
\newcommand{\vs}{\bm{s}}

\newcommand{\vw}{\bm{w}}
\newcommand{\vx}{\bm{x}}
\newcommand{\vy}{\bm{y}}
\newcommand{\vz}{\bm{z}}

\newcommand{\p}{\partial}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\tr}{\text{tr}}
\newcommand{\diag}{\text{diag}}
\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\tsum}{\textstyle{\sum}}
\newcommand{\Order}{\mathcal{O}}
\newcommand{\Real}{\mathbb{R}}

\newcommand{\llv}{\left\lVert}
\newcommand{\rrv}{\right\rVert}
\newcommand{\lla}{\left\langle}
\newcommand{\rra}{\right\rangle}

\newcommand{\nhc}[1]{\textcolor{cyan}{[\textit{#1}]}}



\title{Optimal Task Order for Continual Learning of Multiple Tasks}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Ziyan Li\\
  Department of Physics\\
  Washington University in St. Louis\\
  St. Louis, USA \\
  % examples of more authors
  \And
  Naoki Hiratani\thanks
  {corresponding author: \textit{hiratani@wustl.edu}}\\
  Department of Neuroscience \\
  Washington University in St. Louis \\
  St. Louis, USA \\
}

\begin{document}

\maketitle

\begin{abstract}
Continual learning of multiple tasks remains a major challenge for neural networks. Here, we investigate how task order influences continual learning and propose a strategy for optimizing it. Leveraging a linear teacher-student model with latent factors, we derive an analytical expression relating task similarity and ordering to learning performance. Our analysis reveals two principles that hold under a wide parameter range: (1) tasks should be arranged from the least representative to the most typical, and (2) adjacent tasks should be dissimilar. We validate these rules on both synthetic data and real-world image classification datasets (Fashion-MNIST, CIFAR-10, CIFAR-100), demonstrating consistent performance improvements in both multilayer perceptrons and convolutional neural networks. Our work thus presents a generalizable framework for task-order optimization in task-incremental continual learning.
\end{abstract}


\section{Introduction}
The ability to learn multiple tasks continuously is a hallmark of general intelligence. However, deep neural networks and its applications, including large language models, struggle with continual learning and often suffer from catastrophic forgetting of previously acquired knowledge \citep{mccloskey1989catastrophic, french1999catastrophic, hadsell2020embracing, luo2023empirical}. Although extensive work has been done to identify when forgetting is most prevalent \citep{ramasesh2020anatomy, lee2021continual} and how to mitigate it \citep{french1991using, robins1995catastrophic, kirkpatrick2017overcoming, shin2017continual, serra2018overcoming, rolnick2019experience}, it remains unclear how to prevent forgetting while simultaneously promoting knowledge transfer across tasks \citep{ke2020continual, lin2022beyond, ke2023sub, kontogianni2024continual}.

One important yet relatively underexplored aspect of continual learning is task-order dependence. Previous work has revealed that the order in which tasks are presented can significantly influence continual learning performance and also explored various approaches to optimize task order \citep{lad2009toward, pentina2015curriculum, guo2018dynamic, bell2022effect, lin2023theory, singh2023learning}. However, we still lack clear understanding on how ordering of tasks influences the learning performance and how to order tasks to achieve optimal performance. 
Figure~\ref{order_intro} illustrates this problem using a continual binary image classification example, where a neural network is trained on three tasks: \textit{A} (Cat vs. Ship), \textit{B} (Frog vs. Truck), and \textit{C} (Horse vs. Deer). Learning one task can influence performance on the others in a complex manner (Figs. \ref{order_intro}a and \ref{order_intro}b). Consequently, in this example, the \textit{C}\textrightarrow \textit{B}\textrightarrow \textit{A} task order achieves a higher average performance after training than the \textit{A}\textrightarrow \textit{B}\textrightarrow \textit{C} order (Fig. \ref{order_intro}c). 
The goal of this work is to understand this task-order dependence in continual learning.

\begin{figure*}[tb]
%\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=0.75\linewidth]{figs/fig_scheme1-1.pdf}}
\vskip -0.1in
\caption{Schematic figure of the task-order dependence. 
\textbf{a, b)} Continual learning of binary classification with two different task orders.
\textbf{c)} Average test accuracy on the three classification tasks at the end of learning under task orders depicted in panels \textbf{a} and \textbf{b}. 
Error bars represent the standard error of mean over 10 random seeds.  
}
\label{order_intro}
\end{center}
\vskip -0.2in
\end{figure*}

Task-order optimization requires data samples from all tasks beforehand, making it infeasible in a strictly online learning setting. Nevertheless, it remains highly relevant for many learning problems. For instance, if the current task creates unfavorable conditions, systems can postpone its learning to a more suitable time. In addition, understanding how the order of tasks impacts learning can serve as a tool for predicting the difficulty of online learning given a data stream.
Moreover, even when all tasks are known to the learner a priori, training them jointly in an offline manner can be impractical or costly, making a continual learning with the optimal task-order a desirable alternative.  
For instance, large language models are typically trained in an online fashion because the size of the training corpus is so vast that multiple epochs of training over the entire dataset is infeasible \citep{hoffmann2022training, chowdhery2023palm}. In such cases, how the corpus is organized for training can significantly impact learning efficiency, supporting the importance of optimizing task/corpus order.
Furthermore, in robotics applications \citep{lesort2020continual}, transitioning from one task to another demands time and effort, making simultaneous multi-task learning inefficient. Similar constraints arise in designing of machine-learning-based teaching curricula for schools or professional training where learners need to study multiple subjects sequentially \citep{rafferty2016faster, singh2023learning}. 

To explore the basic principle of task order optimization, here we analyze the task-order dependence of continual learning using a linear teacher-student model with latent factors. First, we derive an analytical expression for the average error after continual learning as a function of task similarity for an arbitrary number of tasks. Our theory shows that this error inevitably depends on the task order because it is a function of the upper-triangular components of the task similarity matrix, rather than of the entire matrix.

We then investigate how the similarity between tasks, when placed in various positions within the task order, affects the overall error. Through linear perturbation analysis, we find that the task-order effect decomposes into two factors. The first is absolute order dependence: similarity between two tasks influences the error differently depending on whether these tasks appear near the beginning or near the end of the sequence. We demonstrate that when tasks are on average positively correlated, the least representative tasks should be learned first, while the most typical task should be learned last (periphery-to-core rule).
The second factor is relative order dependence: the effect of task similarity on the error differs depending on whether two tasks are adjacent in the sequence or far apart. We show that a task order maximizing the path length in the task dissimilarity graph outperforms one that minimizes this path length (max-path rule), consistent with previous empirical observations \citep{bell2022effect}. 

We illustrate these two rules by applying them to tasks with simple similarity structures forming chain, ring, and tree graphs, revealing the presence of non-trivial task orders that robustly achieve the optimal learning performance, given a graph structure.
Moreover, we apply these rules to continual image classification tasks using the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets. We estimate task similarity by measuring the zero-shot transfer performance between tasks, and then implement the task-ordering rules based on these estimates. Our results show that both the periphery-to-core rule and the max-path rule hold robustly in both multilayer perceptrons and convolutional neural networks. This work thus provides a simple and generalizable theory to task-order optimization in task-incremental continual learning.

\section{Related work}
The effects of curriculum learning have been extensively studied in the reinforcement learning (RL) literature \citep{elman1993learning, krueger2009flexible, narvekar2020curriculum}. However, these studies primarily focus on learning a single challenging task by sequentially training on simpler tasks, leaving open the question of how to design a curriculum for learning multiple tasks of similar difficulty. A limited number of works have explored task-order optimization for continual/lifelong learning across multiple tasks by contrast.

\citet{lad2009toward} demonstrated that ordering tasks based on pairwise order preferences can lead to better classification performance compared to random task ordering. More recently, \citet{bell2022effect} investigated task-order optimization by examining Hamiltonian paths on a task dissimilarity graph (see Sec. \ref{sec_task_order_optim} for details). They hypothesized that the shortest Hamiltonian path would be optimal but instead found that the longest Hamiltonian path significantly outperformed both random task ordering and the shortest path in continual image classification tasks. Our work provides analytical insights into when and why this is the case.
\citet{lin2023theory} analyzed generalization error and task-order optimization in continual learning for linear regression. Our work advances this theoretical framework in several important ways. First, we introduce a latent structure model for considering the effect of input similarity and reveal how tasks' relative positions—not just their absolute positions as in \citet{lin2023theory}'s Equation 10—influence the model's final performance. We validate this theoretical finding through experiments on both synthetic data and image classification tasks. Furthermore, we extend beyond the synthetic task settings of \citet{lin2023theory} by demonstrating these effects in a general continual learning framework using data-driven similarity estimation.
Task-order effects on continual learning have also been analyzed in \citep{pentina2015curriculum, evron2023continual, singh2023learning}.


The linear teacher-student model used in this work is a widely adopted framework for analyzing the average properties of neural networks by explicitly modeling the data generation process through a teacher model \citep{gardner1989three, zdeborova2016statistical, bahri2020statistical}. Due to their analytical tractability, these models have offered deep insights into various aspects of statistical learning problems, including generalization \citep{seung1992statistical, advani2020high}, learning dynamics \citep{saad1995line, werfel2003learning, saxe2013exact}, and representation learning \citep{saxe2019mathematical, tian2021understanding}. Many studies have also applied this framework to explore various aspects of continual learning \citep{asanuma2021statistical, lee2021continual, evron2022catastrophic, goldfarb2023analysis, li2023statistical, lin2023theory, evron2024joint, hiratani2024disentangling, mori2024optimal}.

\begin{figure*}[tb]
\begin{center}
\centerline{\includegraphics[width=0.8\linewidth]{figs/fig_lst_theory-simul1-1.pdf}}
\vskip -0.1in
\caption{
\textbf{a)} Schematic of the teacher-student model. 
\textbf{b)} Comparison between the analytical and numerical evaluations of the error $\epsilon_f$ under various number of tasks. Each point represents the errors under a randomly sampled task similarity matrices $(C^{in}, C^{out})$ (see Appendix \ref{sec_implementation_details} for implementation details).
\textbf{c)} Optimal task order for three task learning. In the white regions, $C^{in}$ is not a positive-definite matrix, hence the tasks are not well-defined. 
}
\label{lin_theory_simul}
\end{center}
\vskip -0.2in
\end{figure*}


\section{Task-order dependence}
\subsection{Model setting}
Let us consider a sequence of $P$ tasks, where the inputs $\vx \in \Real^{N_x}$ and the target output $\vy^* \in \Real^{N_y}$ of the $\mu$-th task is generated by
\begin{align}
 \vs \sim \mathcal{N} \left(0, I \right), \quad
 \vx = A_{\mu} \vs, \quad
 \vy^* = B_{\mu} \vs.
\end{align}
Here, $\vs \in \Real^{N_s}$ is the latent factor that underlies both $\vx$ and $\vy^*$, $I$ is the identity matrix, and $A_{\mu} \in \Real^{N_x \times N_s}$ and $B_{\mu} \in \Real^{N_y \times N_s}$ are the mixing matrices that generate the input $\vx$ and the target $\vy^*$ from the latent $\vs$ (Fig.~\ref{lin_theory_simul}a).
Below we focus on $N_x \gg N_s$ regime. 
The introduction of this low-dimensional latent factor $\vs$ is motivated by the presence of low-dimensional latent structures in many real-world datasets \citep{yu2017compressing, cohen2020separability}. 

We sample elements of $\{A_{\mu}, B_{\mu} \}_{\mu=1}^P$ from a correlated Gaussian distribution. 
Denoting a vector consists of the ($i,j$)-th elements of $A_1,..,A_P$ by $\bm{a}_{ij} \equiv [A_{1,ij}, A_{2,ij}, ...., A_{P,ij}]^T$, we sample $\bm{a}_{ij} \in \Real^P$ from
\begin{align}
  \bm{a}_{ij}
  \sim \gauss 
  \left( \bm{0} , \tfrac{1}{N_s}C^{in} \right)
\end{align}
where $C^{in}$ is a $P \times P$ matrices that specify input correlation between tasks. Similarly, we sample the ($i,j$)-th elements of $B_1,..,B_P$, $\bm{b}_{ij} \equiv [B_{1,ij}, B_{2,ij}, ...., B_{P,ij}]^T$, by $\bm{b}_{ij} \sim \gauss (\bm{0}, \tfrac{1}{N_s} C^{out})$. 
Note that here correlation is introduced across tasks in an element-wise manner while keeping elements of each mixing matrix independent (i.e. $\lla A^{\mu}_{ij} A^{\nu}_{kl} \rra_A = \delta_{ik} \delta_{jl} \tfrac{C^{in}_{\mu\nu}}{N_s}$).
Here we generate the model from the task similarity matrices $\{C^{in}, C^{out} \}$ because previous work suggests the crucial impact of task similarity on continual learning \citep{ramasesh2020anatomy, lee2021continual}. 
In section \ref{sec_image_classification}, we consider the estimation of task similarity from datasets to ensure the applicability of our framework. 

Let us consider the training of a linear network, $\vy = W \vx$, in this set of $P$ tasks.
We evaluate the performance of the network for the $\mu$-th task using the mean squared error:
\begin{align}
  \epsilon_{\mu} [W] 
  \equiv \lla \llv \vy^* - \vy \rrv^2 \rra_{\vs}
  = \llv B_{\mu} - W A_{\mu} \rrv_F^2,
\end{align}
where $\lla \cdot \rra_{\vs}$ represents expectation over latent $\vs \sim \gauss(\bm{0}, I)$. 
In a task-incremental continual learning task \citep{van2019three}, we are mainly concerned with minimizing the total error on all tasks after learning all tasks.
Denoting the network parameter after learning of the last, $P$-th, task by $W_P$, the final error is defined by 
\vspace{-3pt}
\begin{align}
  \epsilon_f \equiv \frac{1}{N_y} \sum_{\mu=1}^P \epsilon_{\mu} [W_P].
\end{align}
\vspace{-3pt}
Below, we take the expectation over randomly generated mixing matrices $\{ A_{\mu}, B_{\mu} \}_{\mu=1}^P$ and derive the average final error $\bar{\epsilon}_f \equiv \lla \epsilon_f \rra_{\{ A_{\mu}, B_{\mu} \}}$ as a function of the input and output correlation matrices $C^{in}$ and $C^{out}$. 
Subsequently, we analyze how the task order influences $\bar{\epsilon}_f$ and how to optimize the order. 

\subsection{Analysis of the final error $\epsilon_f$}
We consider task incremental continual learning where $P$ tasks are learned in sequence one by one. 
Let us denote the weight after training on the $(\mu-1)$-th task as $W_{\mu-1}$. 
Considering learning of the $\mu$-th task from $W = W_{\mu-1}$ using gradient descent on task-specific loss $\epsilon_{\mu}[W]$, the weight after training follows (see Appendix \ref{appendix_task_sim}.2)
\begin{align}
  W_{\mu} 
  = W_{\mu-1} \left( I - U_{\mu} U_{\mu}^T \right) + B_{\mu} A_{\mu}^+,
\end{align}
where $U_{\mu}$ is defined by singular value decomposition (SVD) of $A_{\mu}$, $A_{\mu} = U_{\mu} \Lambda_{\mu} V_{\mu}^T$, and $A^+$ is the pseudo-inverse of $A$. 
Applying it recursively while assuming that $W$ is initialized as a zero matrix prior to the first task, we have
\begin{align}
  W_{\mu} = \sum_{\nu=1}^{\mu} (B_{\nu} A_{\nu}^+) \prod_{\rho=\nu+1}^{\mu} (I - U_{\rho} U_{\rho}^T).
\end{align}
If $N_x \gg N_s$, pseudo-inverse $A_{\mu}^+$ is approximated by a scaled transpose $\gamma A_{\mu}^T$, and $U_{\mu} U_{\mu}^T$ approximately follows $U_{\mu} U_{\mu}^T \approx \gamma A_{\mu} A_{\mu}^T$ with $\gamma = \tfrac{N_s}{N_x}$ (see Appendix \ref{appendix_task_sim}.4). Thus, under $\tfrac{N_s}{N_x} \ll 1$, we have
\begin{align}
  W_{\mu} \approx 
  \gamma \sum_{\nu=1}^{\mu} (B_{\nu} A_{\nu}^T) \prod_{\rho=\nu+1}^{\mu} (I - \gamma A_{\rho} A_{\rho}^T).
\end{align}
Under this approximation, there exists a simple expression of the final error as below (see Appendix \ref{appendix_task_sim}.3 for the proof). 
\begin{theorem} \label{theorem_epsilonf}
At $\tfrac{N_s}{N_x} \to 0$ limit, the final error asymptotically satisfies
\begin{align} \label{eq_epsilon_f_analytical}
  \bar{\epsilon}_f
  = \llv (C^{out})^{1/2} \left( I - (I + C^{in,U})^{-1} C^{in} \right) \rrv_F^2,
\end{align}
where $C^{in,U}$ is the strictly upper-triangle matrix generated from the input correlation matrix $C^{in}$ (see eq. \ref{eq_def_CinU}). 
\end{theorem} 
Importantly, the dependence on the upper-triangular components in Eq. \ref{theorem_epsilonf} implies that $\bar{\epsilon}_f$ is not permutation-invariance, and thus depends on the task-order. 
%Note that, our proof requires $P \ll \tfrac{N_x}{N_s}$ for the number of tasks $P$, \nhc{comment on limitation?}

\subsection{Numerical evaluation}
To check this analytical result, in Fig. \ref{lin_theory_simul}b, we compared $\bar{\epsilon}_f$ estimated from Eq. \ref{eq_epsilon_f_analytical} with its numerical estimation through learning via gradient descent, under various choices of the number of tasks $P$ and task correlation matrices $C^{in}$ and $C^{out}$ (each point in Fig. \ref{lin_theory_simul}b represents the errors under one randomly sampled $\{C^{in}, C^{out}\}$ pair). 
This result indicates that our simple analytical expression robustly captures the performance of continual learning in a linear teacher-student model under arbitrary task similarity and the number of tasks. 

To explore how task order influences the continual learning performance, we next calculated the optimal task order of three tasks under various input correlation $C^{in}$ using Eq. \ref{eq_epsilon_f_analytical} (Fig. \ref{lin_theory_simul}c). Here, we set the output correlation $C^{out}_{\mu\nu} = 1$ for all task pairs $(\mu, \nu)$ for simplicity, and parameterized the input correlation between tasks A, B, and C by
\begin{align}
  C^{in} = 
  \begin{pmatrix}
  1 & \rho_{AB} & \rho_{CA} \\
  \rho_{AB} & 1 & \rho_{BC} \\
  \rho_{CA} & \rho_{BC} & 1
  \end{pmatrix}.
\end{align}
Here, tasks A, B, and C are linear regression tasks with partial overlap in the input domain. If $\rho_{AB} = 1$, the input subspace for tasks A and B are the same, while they are independent if $\rho_{AB} = 0$.  
Figure \ref{lin_theory_simul}c revealed that the optimal task order depends on the combination of task similarity $(\rho_{AB}, \rho_{BC}, \rho_{CA})$ in a rich and complex manner. Some of the phase shifts represent trivial mirror symmetry (e.g., $x=y$ line), but many of them are non-trivial. To further gain insights into this complex task-order dependence, below, we consider the linear perturbation limit. 

\section{Task-order optimization} \label{sec_task_order_optim}
\subsection{Linear perturbation theory}
Theorem \ref{theorem_epsilonf} revealed a simple relationship between the task similarity and the final error of continual learning, but it remains unclear how to optimize the task order for continual learning.  
To gain insight into this question, we next add a small perturbation to the input similarity matrix and examine how the change in the similarity between various task pairs modifies the error. 
We parameterize the input correlation matrix by a combination of a constant factor and a small perturbation. 
\begin{align}
  C^{in}_{\mu\nu} = 
  \begin{cases} 
  1 & (\text{if} \quad \mu = \nu) \\
  m + \delta M_{\mu\nu} & (\text{otherwise}).
  \end{cases}
\end{align}
Here, we set the constant factor $m$ to be the same across all tasks for the analytical tractability of the matrix inversion, and perturbation $\delta M$ is constrained to the ones that keep $C^{in}$ to a correlation matrix. 
Similarly, we restricted the target output correlation matrix to be $C^{out}_{\mu\nu} = \rho_o$ for all non-diagonal components.  
In this setting, the error has the following decomposition. 

\begin{figure*}[tb]
%\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{figs/fig_lst_pert1-2.pdf}}
\vskip -0.1in
\caption{Linear perturbation analysis of task order dependence. 
\textbf{a)} Decomposition of the contribution of task similarity to the final error under $\rho_o = 1, m = 0.3$. 
\textbf{b)} Schematic representations of periphery-to-core ordering and core-to-periphery ordering. Circles A, B, ..., I represent tasks and their spatial positions represent similarity between tasks. Here, tasks A-C are central whereas tasks D-I are periphery. 
\textbf{c)} Schematic of minimum and maximum paths on a task dissimilarity graph. 
\textbf{d)} $G^{+}_{\mu\nu}$ as a function of $m$ under $P=7$. Colors indicate the indices such that the purple line on the top corresponds to $G^+_{1,2}$, while the yellow line at the bottom corresponds to $G^+_{6,7}$.
\textbf{e)} Relative error of core-to-periphery and periphery-to-core rules under various average task correlation $m$ at $P=7$. Error bars represent the standard error over random seeds (see Appendix \ref{sec_implementation_details} for details).
\textbf{f)} Coefficient $\alpha^-$ under $P=3, 5, 7$. Dark gray line corresponds to $\alpha^-$ at $P \to \infty$ limit. 
\textbf{g)} Relative error of min-path and max-path task orders under $P=7$. 
Here we took average over two task orders that follows the minimum pathway to estimate the error of min-path. The error of max-path was estimated in the same manner. 
}
\label{lin_pert}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{theorem} \label{theorem_linpert_decompose_main}
  Let us suppose that all elements of matrix $\delta M$ satisfies, $| \delta M_{\mu\nu} | < \delta_m$, where $\delta_m$ is a positive constant.  
  Then, the final error is written as below:
  \begin{align}
    \bar{\epsilon}_f [C^{in}, C^{out} ]
    = \bar{\epsilon}_f [m, \rho_o] 
    + \sum_{\mu=1}^P \sum_{\nu = \mu+1}^P G_{\mu\nu} \delta M_{\mu\nu} + \Order(\delta_m^2),
  \end{align}
  where $\bar{\epsilon}_f [m, \rho_o] $ is the error in the absence of perturbation, and $G_{\mu\nu}$ is a function of $m$, $\rho_o$, and $P$ (see Eqs. \ref{eq_def_go_gpm} in Appendix). At $\rho_o = 1$, $G_{\mu\nu}$ has a following simple expression:
  \begin{subequations}
  \begin{align}
  G_{\mu\nu} 
  &= G^+_{\mu\nu} + G^-_{\mu\nu}, 
  \\
  G^+_{\mu\nu} 
  &\equiv - (1-m)^{P+\mu-1} - (1-m)^{P+\nu-1}
  \nonumber \\
  &\quad + \tfrac{3-m}{2-m} (1-m)^{\mu+\nu-1}, \\
  G^-_{\mu\nu}
  &\equiv - \left( 1 - (1-m)^P \left( \tfrac{mP}{1-m} + \tfrac{3-m}{2-m} \right) \right) (1-m)^{P-(\nu-\mu)}. 
  \end{align}
  \end{subequations}
\end{theorem}
Note that, $P \times P$ matrix $G$ specifies the contribution of $(\mu,\nu)$-th task similarity to the final error. 
The proof of the theorem is provided in Appendix \ref{appendix_task_order}.2. 
Fig. \ref{lin_pert}a describes an example of $G_{\mu\nu}$ (here $P = 7$ and $m = 0.3$). In this case, $G_{12}$ is positive while $G_{17}$ is negative, meaning that if you increase the similarity between the first and the second tasks while keeping the rest the same, the total error $\bar{\epsilon}_f$ goes up, but if you increase the similarity between the first and the last tasks, the error instead decreases. 
To understand this task order dependence, we next analyze $G^+$ and $G^-$ separately. 

\subsection{Impact of task typicality}
Let us first consider the contribution of $G^+_{\mu\nu}$ term. 
Denoting $\alpha^+ \equiv \tfrac{2-m}{3-m} (1-m)^P$, $G^+_{\mu\nu}$ is rewritten as
\begin{align}
  G^+_{\mu\nu}
  &= \tfrac{3-m}{(2-m)(1-m)} \left( (1-m)^{\mu} - \alpha^+ \right) \left( (1-m)^{\nu} - \alpha^+ \right)
  \nonumber \\
  &\quad - \alpha^+ (1-m)^{P-1},
\end{align}
If $1 > m > 0$, $\tfrac{3-m}{(2-m)(1-m)} > 0$ and $(1-m)^{\mu} \geq (1-m)^P > \alpha^+$ for $\mu=1,2,...,P$. Therefore, $G^+_{\mu\nu}$ is a monotonically decreasing function of both $\mu$ and $\nu$ under $1 > m > 0$ (Fig. \ref{lin_pert}d). 
This means that, to minimize the error contributed from $G^+_{\mu\nu}$, $\delta \epsilon^+_f \equiv \sum_{\mu,\nu} G^+_{\mu\nu} \delta M_{\mu\nu}$, the tasks should be ordered in a way that the residual similarity $\delta M_{\mu\nu}$ takes a small (preferably negative) value for early task pairs and a large value for later task pairs.  
In other words, earlier pairs should be relatively dissimilar to each other, while later pairs should be mroe similar. 

One heuristic way to achieve this task order is to put the most atypical task at the beginning and the most typical one at the end. Denoting the relative typicality of the task by
$\delta t_{\mu} = \sum_{\nu \neq \mu} \delta M_{\nu\mu}$,
if we arrange tasks as $\delta t_1 \leq \delta t_2 \leq ... \leq \delta t_P$, on average, earlier pairs are dissimilar to each other while the latter ones are similar. 
Below, we denote this ordering as a periphery-to-core rule, as less representative periphery tasks are learned first and more central core tasks are learned later under this principle (Fig. 4b). 
Under a randomly generated input correlation matrix $C^{in}$, periphery-to-core task order robustly outperformed both random and core-to-periphery order, when the average correlation is large positive value (blue vs black and red line in Fig. \ref{lin_pert}e). This was not the case when the average correlation is a small positive value potentially due to contribution from $G^-$ factor. 
Note that, a similar rule was derived by \citet{lin2023theory} based on their analysis of linear regression model, where they proved that when there is one outlier task, the outlier task should be learned in the first half of the task sequence. 

\subsection{Impact of Hamiltonian path length}
Let us next focus on $G^-_{\mu\nu}$ term that governs the contribution of the relative distance between tasks in the task sequence. 
The error originating from this term is written as
\begin{align}
  \delta \epsilon^-_f 
  = \alpha^- \sum_{d=1}^{P-1} (1-m)^{P-d} \sum_{\mu=1}^{P-d} \delta M_{\mu,\mu+d},
\end{align}
where $\alpha^- \equiv - 1 + (1-m)^P \left( \tfrac{mP}{1-m} + \tfrac{3-m}{2-m} \right)$ is a coefficient. $\alpha^-$ is negative if $1>m>0$ and $P$ is sufficiently large (Fig. 4f). Thus, to minimize the error $\delta \epsilon^-_f$, the tasks should be arrange in a way that $\delta M_{\mu,\mu+d}$ is small for small $d$, while $\delta M_{\mu,\mu+d}$ is large for large $d$.
In other words, tasks following one another in the task order sequence should be dissimilar to each other, while distant pairs should be similar. 

Given a set of tasks, let us define a task dissimilarity graph by setting each task as a node and dissimilarity between two tasks as the weight of the edge between corresponding nodes (Fig. 4c). 
Then, a task order that learns each task only once forms a Hamiltonian path on the graph, a path that visits all nodes once but only once.  
We can then construct a heuristic solution for minimizing $\delta \epsilon^{-}_f$ by selecting a task order that yields the longest Hamiltonian path. 
When tasks have the same similarity with each other in terms of $C^{out}$, their similarity depends solely on $C^{in}$, allowing us to define dissimilarity as $d_{\mu\nu} \equiv 1 - C^{in}_{\mu\nu}$. 
Thus, the total length of the Hamiltonian path induced by a given task order follows 
$D_H = \sum_{\mu=1}^{P-1} C^{in}_{\mu, \mu+1}$.
Consequently, $\delta \epsilon^-_f$ is rewritten as
\begin{align}
  \epsilon^-_f
  &= -\alpha_- \left( (1-m)^{P-1} D_H
  + \sum_{d=2}^{P-1} (1-m)^{P-d} \sum_{\mu=1}^{P-d} d_{\mu\nu} \right)
  \nonumber \\
  & \quad + \text{const.}
\end{align}
Because $-\alpha_-$ is non-negative, small task dissimilarity $d_{\mu\nu}$ (i.e., large task correlation $C^{in}_{\mu\nu}$) generally helps minimizing the error. 
Moreover, we have $0 \leq (1-m)^{P-1} < (1-m)^{P-d}$ for $d=2,3,...$, indicating $D_H$ term has the smallest impact on the error. 
Therefore, by choosing the largest $d_{\mu\nu}$ for $D_H$, we can make $\delta \epsilon^-_f$ small on average. 
We observed this trend robustly even when we sampled $\{C^{in}, C^{out}\}$ randomly (Fig. \ref{lin_pert}g). 
Our work thus provides theoretical insights on why the maximum Hamiltonian path provides a preferable task order, strengthening previous empirical finding \citep{bell2022effect}. 
We call this rule as max-path rule below. 

\begin{figure*}[tb]
%\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{figs/fig_lst1_graph2-1.pdf}}
\vskip -0.1in
\caption{Optimal task orders for tasks with simple graph-like similarity structures.
\textbf{a,b)} Total error $\epsilon_f$ under all task orders when the similarity structure of five tasks follows chain (a) and ring (b) structures. Each gray line represents one of 120 (=5!) task order, while red, orange, and blue lines highlight three representative task orders depicted on the right. Thick black line is the average error over ordering.   
\textbf{c)} Numerically-estimated learning dynamics of the network when tasks have a chain-graph structure. Red and blue lines represents \textit{A\textrightarrow B\textrightarrow C\textrightarrow D\textrightarrow E} and \textit{A\textrightarrow E\textrightarrow C\textrightarrow D\textrightarrow B} orders depicted in the insets. Black line is the average learning trajectory under random task ordering. 
\textbf{d,e)} The same as panels a and b but for tasks with similarity matrices having tree (d), and tree-leaves similarity structure (e), respectively.
\textbf{f)} The same as panel c, but for tasks having tree-graph-like similarity structure. 
}
\label{lin_graph}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Application to tasks having simple graph structures}
The analyses above elucidated two principles underlying task order optimization.  
To illustrate these principles, we next examine task order optimization for a set of tasks with a simple task similarity structure.

Figure~\ref{lin_graph}a depicts the total error estimated using Eq.~\ref{eq_epsilon_f_analytical} in a continual learning scenario involving five tasks with a chain-like similarity. We configure the input correlation matrix $C^{in}$ such that tasks \textit{A} and \textit{B} are directly correlated, while \textit{A} and \textit{C} are correlated only indirectly through \textit{B}. Specifically, denoting the similarity between neighboring tasks on the task dissimilarity graph as $a$, we set $C^{in}_{AB} = C^{in}_{BC} = a$, $C^{in}_{AC} = a^2$, and so on (see Appendix~\ref{sec_implementation_details}). 
Here, tasks exhibit significant overlap when $a \lesssim 1$, while tasks become independent in the limit $a \to 0$ (x-axis of Figure~\ref{lin_graph}a). We set $C^{out}$ to one for all task pairs.
Each line in Figure~\ref{lin_graph}a represents the error under a specific task order. For example, the \textit{A\textrightarrow B\textrightarrow C\textrightarrow D\textrightarrow E} task order, depicted by the red line, consistently performed among the worst, regardless of the similarity $a$ between neighboring tasks. Surprisingly, several task orders robustly outperformed the others, such as \textit{A\textrightarrow C\textrightarrow E\textrightarrow D\textrightarrow B} (orange line) and \textit{A\textrightarrow E\textrightarrow C\textrightarrow D\textrightarrow B} (blue line).  
These task orders align with the two principles described earlier.  
First, the periphery-to-core rule suggests that the initial task should either be \textit{A} or \textit{E}, as these tasks are the least typical.\footnote{Due to mirror symmetry, the \textit{E\textrightarrow A\textrightarrow C\textrightarrow B\textrightarrow D} order exhibits equivalent performance to \textit{A\textrightarrow E\textrightarrow C\textrightarrow D\textrightarrow B}.}  
Second, the max-path rule indicates that subsequent tasks should be as dissimilar as possible. For instance, if the first task is \textit{A}, selecting \textit{E} as the second task, as in the blue line, maximizes the distance.  
Notably, there was approximately a seven-fold difference in performance between the best and worst task orders, underscoring the critical importance of task order in continual learning.

\begin{figure*}[tb]
%\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{figs/fig_nonlinear1-2.pdf}}
\vskip -0.1in
\caption{Task order preference in continuous image classification tasks.
\textbf{a–f)} Continual learning performance, defined as the average test accuracy across all the tasks after learning, under various task orders. Panels (a, c, e) compare the periphery-to-core rule against the core-to-periphery rule, whereas panels (b, d, f) compare the max-path rule with the min-path rule.
\textbf{g, h)} The ratio of task sets where the periphery-to-core rule outperforms the core-to-periphery rule (g), and where the max-path rule outperforms the min-path rule (h), under CIFAR-100. Different colors represent results for different numbers of tasks ($P=3,5,7$).
See Appendix \ref{sec_implementation_details}.3 for details.
}
\label{order_nonlin}
\end{center}
\vskip -0.2in
\end{figure*}

We observed analogous trends when applying the same analysis to tasks with ring, tree, and leaves structures (Figs.~\ref{lin_graph}b, \ref{lin_graph}d, and \ref{lin_graph}e, respectively).  
For tasks with a tree-like similarity structure, as shown in Figure~\ref{lin_graph}d, the error was minimized when tasks corresponding to leaf nodes were learned first, followed by tasks associated with root nodes (the orange and blue trees in Fig.~\ref{lin_graph}d). This result aligns with the periphery-to-core rule.  
When only the leaf nodes were considered as tasks, as illustrated in Figure~\ref{lin_graph}e, the optimal task order exhibited a complex pattern of hopping across tasks (blue tree in Fig.~\ref{lin_graph}e), consistent with the max-path rule.

Numerical simulations validated the analytical results (Figs.~\ref{lin_graph}c and \ref{lin_graph}f) and further revealed intricate learning dynamics.  
In Figure~\ref{lin_graph}f, the red task order initially outperformed the black line representing the average performance, while the blue task order performed worse. However, this trend reversed around the fourth task.  
These findings indicate that the optimal task order is often non-trivial, and a greedy approach optimizing task-by-task error may lead to suboptimal performance.


\section{Application to image classification tasks} \label{sec_image_classification}
Our analytical investigation in the linear teacher-student setting highlighted two principles for task order optimization: the periphery-to-core rule and the max-path rule. To evaluate the potential applicability of these principles to more general settings, we next explore continual learning of image classification tasks.

\subsection{Empirical estimation of task similarity}
To apply these principles, it is necessary to first measure the similarity between tasks. Here, we estimate the similarity between tasks $A$ and $B$ by measuring the zero-shot transfer performance between them (Fig. \ref{sim_mat} in the Appendix). 
Specifically, we train a network for task $A$, obtaining the learned weights $W_A$. We then measure the error of this trained network on task $B$, denoted as $\epsilon_B [W_A]$. Since the transfer performance from task $A$ to $B$ generally differs from that of $B$ to $A$\, we take the mean of both directions and define the similarity $\rho_{AB}$:
\begin{align} \label{eq_def_rhoAB_nonlin}
  \rho_{AB} = 1 - \frac{1}{2} \left( \sqrt{\frac{\epsilon_B [W_A]} { \epsilon_{B,sf} [W_A]}} + \sqrt{\frac{\epsilon_A [W_B]} { \epsilon_{A,sf} [W_B]}} \right).
\end{align}
Here, $\epsilon_{B,sf} [W_A]$ represents the error on task $B$ with label shuffling, which serves as the chance-level error. The square root is taken because the error scales with the squared value of task correlation in our linear model (see Appendix \ref{sec_implementation_details}.4). 

Although this method requires training the network on all $P$ tasks, the computational complexity of training is $\Order (P)$, which is significantly smaller than the naive task order optimization that requires a computational cost of $\Order (P!)$. 
Furthermore, this method only requires the inputs and outputs of the trained network, making it applicable even in situations where the model's internal details are inaccessible.


\subsection{Numerical results}
We estimated the performance of the periphery-to-core rule and max-path rule in task-incremental continual learning using Fashion-MNIST \citep{xiao2017fashion}, CIFAR-10, and CIFAR-100 dataset \citep{krizhevsky2009learning} (see Appendix \ref{sec_implementation_details} for the details). 
For the Fashion-MNIST and CIFAR-10 datasets, we randomly generated five binary image classification tasks by dividing 10 labels into 5 pairs without replacement. In the case of CIFAR-100, we selected 10 labels out of 100 labels randomly and generated 5 binary classifications. 
For Fashion-MNIST, we trained a multi-layered perceptron with two hidden layers, while for CIFAR-10/100, we used a convolutional neural network with two convolutional layers and one dense layer, to explore robustness against the model architecture.  

We found that the final performance was modulated by the estimated average similarity among tasks, $\bar{\rho} = \tfrac{1}{P(P-1)} \sum_{\mu \neq \nu} \rho_{\mu\nu}$, we thus plotted the performance of each task-order rule as a function of the average similarity (Fig. \ref{order_nonlin}). 
In all three settings, we found that the periphery-to-core rule robustly outperforms the core-to-periphery rule and average performance over random ordering (Fig. \ref{order_nonlin}a,c,e). Similarly, the max-path rule outperformed both the min-path rule and the random ordering (Fig. \ref{order_nonlin}b,d,e; see also \citet{bell2022effect}). 
Moreover, we observed consistent results under a continual learning of a multi-class classification (Fig.\ref{order_nonlin_supp}a and b in the Appendix). 
The periphery-to-core rule outperformed the max-path rule on average, bu the difference was small (red lines in Fig. \ref{order_nonlin} top vs bottom).
When we increased the number of binary classification tasks from 3 to 7 using CIFAR-100, the performance advantage periphery-to-core over core-to-periphery increased (Fig. \ref{order_nonlin}g) as expected from the linear model (Fig. \ref{lin_pert_supp}c in the Appendix).
This was not evident for max-path and min-path rules potentially because the difference was already high under $P=3$ (Fig. \ref{order_nonlin}h). 
These results suggest the robustness of the task order optimization principles found in our simple analysis. 

\section{Discussion}
In this work, we derived a simple analytical expression to explain how task similarity and ordering influence continual learning performance in a linear model with latent structure. Based on this result, we proposed two principles for task order optimization: the periphery-to-core rule and the max-path rule, the latter of which was predicted by \citet{bell2022effect}. We validated these principles in task-incremental continual image classification tasks using both multi-layer perceptrons and convolutional neural networks. Thus, this work proposes basic principles for task order optimization in the context of continual learning for multiple tasks.

\paragraph*{Limitations}
Our theoretical results were derived in a linear model under the assumption of random task generation, which limits their direct applicability. However, we numerically confirmed that the proposed ordering rules hold in both convolutional neural networks and multi-layer perceptrons trained for continual image recognition tasks. Future work should further evaluate these rules in domains closer to real-world applications, including deep-RL, robotics, and language models.
Additionally, in this work, we restricted the model setting to scenarios where each task is learned only once and trained to convergence. The first assumption can be readily relaxed as long as the total number of tasks remains small (see Appendix \ref{appendix_task_sim}.4). Relaxing the second assumption is an important direction for future work. 

\section*{Acknowledgment} 
This work was supported by Washington University in St Louis.
%\section*{Impact Statement}
%Task order optimization for continual learning of multiple tasks may potentially contribute beyond the field of machine learning from school curriculum design \citep{rafferty2016faster, zhu2018overview} to animal training protocol in neuroscience experiments \citep{krueger2009flexible}. Nevertheless, due to theoretical nature of this work, there are no specific societal consequence that we feel must be highlighted here.  

\bibliography{refs}
%\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
%\onecolumn

\section{Analysis of the impact of task similarity on continual learning} \label{appendix_task_sim}
\subsection{Model setting}
%Let us first reintroduce the model setting explained in the main text for completeness. 
Below, we analyze task-order dependence of continual learning using linear teacher-student models with a latent factor. 
In teacher-student models, the generative model of the task parameterized explicitly by the teacher model, making the learning dynamics and the performance analytically tractable \citep{gardner1989three, saad1995line, zdeborova2016statistical}. 
Here, the generative model for input $\vx \in \Real^{N_x}$ and the target output $\vy \in \Real^{N_y}$ is constructed as
\begin{align}
 \vs \sim \mathcal{N} \left(0, I\right), \quad
 \vx = A_{\mu} \vs, \quad
 \vy^* = B_{\mu} \vs,
\end{align}
where $\vs \in \Real^{N_s}$ is the latent variable that underlies $\vx$ and $\vy^*$, $I$ is the identity matrix, and $A_{\mu} \in \Real^{N_x \times N_s}$ and $B_{\mu} \in \Real^{N_y \times N_s}$ are mixing matrices for the input and the target output at task $\mu = 1,...,P$, respectively.

We generate matrices $\{ A_{\mu}, B_{\mu} \}_{\mu=1}^P$ randomly but with task-to-task correlation. 
We specify the element-wise correlation among input generation matrices $\{A_{\mu}\}_{\mu=1}^P$ by a $P\times P$ correlation matrix $C^{in}$ and specify the correlation among the target output generation matrices $\{ B_{\mu} \}_{\mu=1}^P$ by another $P \times P$ correlation matrix $C^{out}$.
$C^{in}$ and $C^{out}$ are constrained to be correlation matrices, but arbitrary otherwise. 
For all $1 \leq i \leq N_x$ and $1 \leq j \leq N_s$,
we generate $(i,j)$-th elements of matrices $A_1, ..., A_P$ by jointly sampling them from a Gaussian distribution with mean zero and covariance $\tfrac{1}{N_s} C^{in}$:
\begin{align}
    \begin{pmatrix} A^1_{ij} \\ \vdots \\ A^P_{ij}
    \end{pmatrix}
    \sim \gauss \left( \bm{0}, \tfrac{1}{N_s} C^{in} \right).
\end{align}
Similarly, we generate $(i,j)$-th elements of matrices $B_1, ..., B_P$ by $(B^1_{ij}, ..., B^P_{ij})^T \sim \gauss \left( \bm{0}, \tfrac{1}{N_s} C^{out} \right)$.
Note that, under this construction, two different elements in a matrix $A_{\mu}$ are independent with each other, but the same element in matrices for two different tasks are correlated with each other. 
Although the random task generation assumption limits direct applicability of our theory, it enables us to obtain insights into how task similarity influences the overall performance and optimal task order.   
Moreover, our analytical results up to Eq. \ref{eq_def_prod_IminusUUT} hold for arbitrary mixing matrices $\{ A_{\mu}, B_{\mu} \}$, as they don't assume the expectation over $\{ A_{\mu}, B_{\mu} \}$.

The student network that learns the task is specified to be a linear network:
\begin{align}
  \vy = W \vx,
\end{align}
where $W \in \Real^{N_y \times N_x}$ is the trainable weight. 
The mean-squared error between the output of the student network $\vy$ and the target for the $\mu$-th task $\vy^*$ is given by
\begin{align}
  \epsilon_{\mu} [W] 
  \equiv \tfrac{1}{N_y}\lla \llv \vy - \vy^* \rrv^2 \rra_{\vs}
  = \tfrac{1}{N_y} \llv B_{\mu} - W A_{\mu} \rrv_F^2 
\end{align}
The second equality follows from the Gaussianity of $\vs$. 
We consider task-incremental continual learning \citep{van2019three} where the network is trained for task $\mu= 1,...,P$ in sequence. 
During the training for the $\mu$-th task, weight $W$ is updated by gradient descent on error $\epsilon_{\mu}$:
\begin{align} \label{eq_lst_gd_discrete}
  W \leftarrow 
  W - \eta \frac{\p \epsilon_{\mu} [W]}{\p W}
  = W - \frac{2\eta}{N_y} (B_{\mu} - W A_{\mu}) A_{\mu}^T.
\end{align}
We denote the weight after training on task $\mu$ as $W_{\mu}$. The total error on all tasks at the end of all $P$ task learning becomes:
\begin{align}
  \epsilon_f
  \equiv \sum_{\mu=1}^P \epsilon_{\mu} [W_P] 
  = \frac{1}{N_y} \sum_{\mu=1}^P \llv B_{\mu} - W_P A_{\mu} \rrv_F^2.
\end{align}
To uncover how similarity between tasks influences the final error, in this work, we focus on the average performance over randomly generated mixing matrices $\{ A_{\mu}, B_{\mu} \}_{\mu=1}^P$ under a fixed pair of task correlation matrices $C^{in}, C^{out}$. 
We define the average of the final error $\epsilon_f$ over generative models $\{A_{\mu}, B_{\mu}\}_{\mu=1}^P$ by
\begin{align} 
  \bar{\epsilon}_f \equiv 
  \lla \epsilon_f \rra_{A,B}. 
\end{align}
Below, we first derive the analytical expression of $W_{\mu}$ then estimate the total final error $\bar{\epsilon}_f$. 
%\nhc{comment on why we care about $\epsilon_f$}

\subsection{The weight after continual learning of $P$ tasks}
Considering the gradient flow limit of learning dynamics, the weight update (Eq. \ref{eq_lst_gd_discrete}) is rewritten as
\begin{align}
  \frac{dW}{dt}
  = - (B_{\mu} - W A_{\mu}) A_{\mu}^T. 
\end{align}
Let us denote singular value decomposition (SVD) of $A_{\mu}$ by $A_{\mu} = U_{\mu} \Lambda_{\mu} V_{\mu}^T$, where $U_{\mu} \in \Real^{N_x \times N_o}$ and $V_{\mu} \in \Real^{N_s \times N_o}$ are semi-orthonormal matrices (i.e., $U^T U = V^T V = I$) and $\Lambda_{\mu} \in \Real^{N_o \times N_o}$ is a non-negative diagonal matrix. 
Then, at any point during learning, there exists a matrix $Q(t) \in \Real^{N_y \times N_o}$ such that $W(t)$ is written as $W(t) = W_{\mu-1} + Q(t) U_{\mu}^T$ because weight change during learning of the $\mu$-th task is constrained to the space spanned by $U_{\mu}^T$.
Thus, at the convergence of learning, $\frac{dW}{dt} = 0$, we have
\begin{align}
  \left( B_{\mu} - [W_{\mu-1} + Q_{\mu} U_{\mu}^T] A_{\mu} \right) A_{\mu}^T = 0.
\end{align}
Solving this equation with respect to $Q_{\mu}$, we get
$Q_{\mu} = B_{\mu} V_{\mu} \Lambda^{-1}_{\mu} - W_{\mu-1} U_{\mu}$.
Therefore, the weight after training on the $\mu$-th task becomes
\begin{align}
  W_{\mu} = W_{\mu-1} (I - U_{\mu} U_{\mu}^T) + B_{\mu} A_{\mu}^+,
\end{align}
where $A_{\mu}^+$ is the pseudo-inverse of $A_{\mu}$ ($A_{\mu}^+ = V_{\mu} \Lambda_{\mu}^{-1} U_{\mu}^T$). 
By applying this result iteratively from zero initialization, $W_{\mu}$ is rewritten as
\begin{align}
  W_{\mu} = \sum_{\nu=1}^{\mu} (B_{\nu} A_{\nu}^+) \prod_{\rho=\nu+1}^{\mu} (I - U_{\rho} U_{\rho}^T),
\end{align}
where $\prod_{\rho=\nu+1}^{\mu} (I - U_{\rho} U_{\rho}^T)$ is the identity matrix if $\mu = \nu$, otherwise,
\begin{align} \label{eq_def_prod_IminusUUT}
  \prod_{\rho=\nu+1}^{\mu} (I - U_{\rho} U_{\rho}^T)
  = (I - U_{\nu+1} U_{\nu+1}^T) (I - U_{\nu+2} U_{\nu+2}^T) \cdot\cdot\cdot (I - U_{\mu} U_{\mu}^T).
\end{align}

To further investigate how task similarity impacts continual learning performance, below we focus on the large $N_x$ regime, and analyze the learning behavior at $\tfrac{N_s}{N_x} \to 0$ limit.
This assumption of the presence of low-dimensional latent factor is consistent with many real-world datasets \citep{yu2017compressing, cohen2020separability}. 
If $A_{\mu}$ is a very-tall random matrix (i.e., $N_x \gg N_s$), pseudo-inverse $A_{\mu}^+$ is approximated by a scaled transpose $\gamma A_{\mu}^T$, and $U_{\mu} U_{\mu}^T$ approximately follows $U_{\mu} U_{\mu}^T \approx \gamma A_{\mu} A_{\mu}^T$, where $\gamma = \tfrac{N_s}{N_x}$ (see Appendix \ref{appendix_task_sim}.4).
Thus, we have
\begin{align} \label{eq_Wmu_approx}
  W_{\mu} \approx 
  \gamma \sum_{\nu=1}^{\mu} (B_{\nu} A_{\nu}^T) \prod_{\rho=\nu+1}^{\mu} (I - \gamma A_{\rho} A_{\rho}^T).
\end{align}
Using the approximation from Eq. \ref{eq_Wmu_approx}, the error on the $\mu$-th task after training on $\nu$-th task, $\epsilon_{\mu} [W_{\nu}]$, is
\begin{align} \label{eq_emu_Wnu}
  \epsilon_{\mu} [W_{\nu}] 
  = \llv B_{\mu} - W_{\nu} A_{\mu} \rrv_F^2
  \approx \llv B_{\mu} -\gamma \sum_{\rho=1}^{\nu} (B_{\rho} A_{\rho}^T) \prod_{\sigma=\rho+1}^{\nu} (I - \gamma A_{\sigma} A_{\sigma}^T) A_{\mu}\rrv_F^2. 
\end{align}

\subsection{Proof of Theorem \ref{theorem_epsilonf}}
Substituting $W_P$ with Eq. \ref{eq_Wmu_approx}, at $\tfrac{N_s}{N_x} \to 0$ limit, $\bar{\epsilon}_f$ is rewritten as
\begin{align} \label{eq_bar_ef_expansion}
  \bar{\epsilon}_f 
  &= \frac{1}{N_y} \sum_{\mu=1}^P 
  \lla \llv B_{\mu} 
  - \gamma \sum_{\rho=1}^P B_{\rho} A_{\rho}^T \prod_{\sigma=\rho+1}^P (I - \gamma A_{\sigma} A_{\sigma}^T) A_{\mu}\rrv_F^2 \rra
  \nonumber \\
  &= \frac{1}{N_y} \sum_{\mu=1}^P \lla \llv B_{\mu} \rrv_F^2 \rra
  - \frac{2\gamma}{N_y} \sum_{\mu=1}^P \sum_{\rho=1}^P 
  \lla \tr \left[ B_{\mu}^T B_{\rho} A_{\rho}^T \prod_{\sigma=\rho+1}^P (I - \gamma A_{\sigma} A_{\sigma}^T) A_{\mu} \right] \rra
  \nonumber \\
  &\quad + \frac{\gamma^2}{N_y} \sum_{\mu=1}^P
  \lla \llv \sum_{\rho=1}^P B_{\rho} A_{\rho}^T \prod_{\sigma=\rho+1}^P (I - \gamma A_{\sigma} A_{\sigma}^T) A_{\mu} \rrv_F^2 \rra
\end{align}

Taking expectation over $\{A_{\mu}, B_{\mu}\}_{\mu=1}^P$, the first term is $\lla \llv B_{\mu} \rrv_F^2 \rra = N_y$.
The second term is rewritten as
\begin{align}
  &\frac{\gamma}{N_y} \sum_{\mu=1}^P \sum_{\rho=1}^P 
  \lla \tr \left[ B_{\mu}^T B_{\rho} A_{\rho}^T \prod_{\sigma=\rho+1}^P (I - \gamma A_{\sigma} A_{\sigma}^T) A_{\mu} \right] \rra
  \nonumber \\
  &= \frac{1}{N_x} \sum_{\mu=1}^P \sum_{\rho=1}^P 
  C^{out}_{\mu\rho} \lla  \tr \left[ A_{\rho}^T (I - \gamma A_{\rho+1} A_{\rho+1}^T ) \cdot \cdot \cdot (I - \gamma A_P A_P^T ) A_{\mu} \right] \rra
  \nonumber \\
  &= \frac{1}{N_x} \sum_{\mu=1}^P \sum_{\rho=1}^P 
  C^{out}_{\mu\rho} \lla 
 \tr [A_{\rho}^T A_{\mu}]
 - \gamma \sum_{\sigma_1 = \rho+1}^P \tr [ A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T A_{\mu}]
 + \gamma^2 \sum_{\sigma_1 = \rho+1}^{P-1} \sum_{\sigma_2 = \sigma_1+1}^P \tr [A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T A_{\sigma_2} A_{\sigma_2}^T A_{\mu}] - ...
 \rra
 \nonumber \\
 &= \frac{1}{N_x} \sum_{\mu=1}^P \sum_{\rho=1}^P 
  C^{out}_{\mu\rho} \sum_{k=0}^{P-\rho} 
 (-\gamma)^k \sum_{\rho < \sigma_1 < ... < \sigma_k \leq P} \lla \tr \left[ A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T ... A_{\sigma_k} A_{\sigma_k}^T A_{\mu} \right] \rra.
\end{align}
In the first line, we took expectation over $\{ B_{\mu} \}$, which yields
$\tfrac{\gamma}{N_y} \lla \tr [B_{\mu}^T B_{\rho} M] \rra_B
= \tfrac{N_s/N_x}{N_y} \tfrac{N_y C^B_{\mu\rho}}{N_s} \tr[M]
= \tfrac{C^B_{\mu\rho}}{N_x} \tr[M]$ for arbitrary matrix $M$. 
In the third line, we rearranged the terms inside the trace based on $\gamma$ dependence.
The summation $\sum_{\rho < \sigma_1 < ... < \sigma_k \leq P}$ in the last line is summation over a set of indices $\sigma_1$, $\sigma_2$, ..., $\sigma_k$ that satisfy $\rho < \sigma_1 < \sigma_2 < ... < \sigma_k \leq P$ condition. 
Under $N_x \gg N_s$, the expectation term in the equation above follows (see Appendix \ref{appendix_task_sim}.4)
\begin{align} \label{eq_gamma_tr_A_ap2}
  \lla \gamma^k \tr \left[ A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T ... A_{\sigma_k} A_{\sigma_k}^T A_{\mu} \right] \rra
  = N_x \left( C^{in}_{\rho \sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_k \mu} + \Order \left( \tfrac{N_s}{N_x} \right) \right).
\end{align}
Moreover, if we define an upper-triangle matrix $C^{in,U} \in \Real^{P\times P}$ by
\begin{align} \label{eq_def_CinU}
  C^{in,U} = 
  \begin{cases} 
  C^{in}_{\mu \nu} & (\text{if} \quad \mu < \nu) \\
  0 & (\text{otherwise})
  \end{cases},
\end{align}
we have
\begin{align}
  \sum_{\rho < \sigma_1 < ... < \sigma_k \leq P} \lla \tr \left[ A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T ... A_{\sigma_k} A_{\sigma_k}^T A_{\mu} \right] \rra
  &= \sum_{\rho < \sigma_1 < ... < \sigma_k \leq P} C^{in}_{\rho \sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_k \mu}
  + \Order\left( \tfrac{N_s}{N_x} \right)
  \nonumber \\
  &= \left[ \left( C^{in,U} \right)^k C^{in} \right]_{\rho \mu}
  + \Order\left( \tfrac{N_s}{N_x} \right)
\end{align}
The last line follows because the upper triangle matrix $C^{in,U}$ satisfies
\begin{align}
  \sum_{\sigma_1=1}^P \sum_{\sigma_2=1}^P ... \sum_{\sigma_k=1}^P
  C^{in,U}_{\rho \sigma_1} C^{in,U}_{\sigma_1 \sigma_2} ... C^{in,U}_{\sigma_{k-1} \sigma_k} C^{in}_{\sigma_k \mu}
  &= \sum_{\sigma_1=\rho+1}^P \sum_{\sigma_2=\sigma_1+1}^P ... \sum_{\sigma_k=\sigma_{k-1}+1}^P C^{in,U}_{\rho \sigma_1} C^{in,U}_{\sigma_1 \sigma_2} ... C^{in,U}_{\sigma_{k-1} \sigma_k} C^{in}_{\sigma_k \mu}
  \nonumber \\
  &= \sum_{\rho < \sigma_1 < ... < \sigma_k \leq P} C^{in}_{\rho \sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_{k-1} \sigma_k} C^{in}_{\sigma_k \mu}.
\end{align}
Moreover, because $\left[ (C^{in,U})^k \right]_{\rho \nu} = 0$ for any $\nu$ if $\rho$ satisfies $\rho \geq P-k+1$, we have
\begin{align}
  \sum_{k=0}^{P-\rho} \left[ \left( C^{in,U} \right)^k C^{in} \right]_{\rho \mu}
  = \sum_{k=0}^P \left[ \left( C^{in,U} \right)^k C^{in} \right]_{\rho \mu}
\end{align}
Therefore, taking $\tfrac{N_s}{N_x} \to 0$ limit, it follows that
\begin{align}
 &\frac{1}{N_x} \sum_{\mu=1}^P \sum_{\rho=1}^P 
  C^{out}_{\mu\rho} \sum_{k=0}^{\nu-\rho} 
 (-\gamma)^k \sum_{\rho < \sigma_1 < ... < \sigma_k \leq P} \lla \tr \left[ A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T ... A_{\sigma_k} A_{\sigma_k}^T A_{\mu} \right] \rra
 \nonumber \\
 &\approx \sum_{\mu=1}^P \sum_{\rho=1}^P 
  C^{out}_{\mu\rho} \sum_{k=0}^P (-1)^k \left[ \left( C^{in,U} \right)^k C^{in} \right]_{\rho \mu} 
 \nonumber \\
 &= \sum_{\mu=1}^P \sum_{\rho=1}^P 
  C^{out}_{\mu\rho} \left[ \left( I + C^{in,U} \right)^{-1} C^{in} \right]_{\rho \mu}
 \nonumber \\
 &= \tr \left[ C^{out} \left( I + C^{in,U} \right)^{-1} C^{in} \right]. 
\end{align}
In the third line, we used 
\begin{align}
  (I + C^{in,U}) \sum_{k=0}^P (-1)^k \left(C^{in,U}\right)^k
  = I + (-1)^P \left(C^{in,U}\right)^{P+1} = I.
\end{align}

We can evaluate the third term of Eq. \ref{eq_bar_ef_expansion} in an analogous manner:
\begin{align}
  &\frac{\gamma^2}{N_y} \sum_{\mu=1}^P
  \lla \llv \sum_{\rho=1}^P B_{\rho} A_{\rho}^T \prod_{\sigma=\rho+1}^P (I - \gamma A_{\sigma} A_{\sigma}^T) A_{\mu} \rrv_F^2 \rra
  \nonumber \\
  &= \frac{\gamma^2}{N_y} \sum_{\mu=1}^P \sum_{\rho=1}^P \sum_{\rho'=1}^P \lla 
  \tr \left[ B_{\rho'}^T B_{\rho} A_{\rho}^T \prod_{\sigma=\rho+1}^P (I - \gamma A_{\sigma} A_{\sigma}^T ) A_{\mu} \left(  A_{\rho'}^T \prod_{\sigma'=\rho'+1}^P (I - \gamma A_{\sigma'} A_{\sigma'}^T ) A_{\mu} \right)^T \right]
  \rra 
  \nonumber \\
  &= \frac{N_s}{N_x^2} \sum_{\mu=1}^P \sum_{\rho=1}^P \sum_{\rho'=1}^P C^{out}_{\rho\rho'} 
  \lla \tr \left[ 
  A_{\rho}^T (I - \gamma A_{\rho+1} A_{\rho+1}^T) ... (I - \gamma A_P A_P^T) A_{\mu} A_{\mu}^T (I - \gamma A_P A_P^T) ... (I - \gamma A_{\rho'+1} A_{\rho'+1}^T) A_{\rho'} 
  \right] \rra.
\end{align}
The term inside the trace can be expanded as 
\begin{align}
  &\lla \tr \left[ 
  A_{\rho}^T (I - \gamma A_{\rho+1} A_{\rho+1}^T) ... (I - \gamma A_P A_P^T) A_{\mu} A_{\mu}^T (I - \gamma A_P A_P^T) ... (I - \gamma A_{\rho'+1} A_{\rho'+1}^T) A_{\rho'} 
  \right] \rra
  \nonumber \\
  &= \sum_{k=1}^{P-\rho} \sum_{k'=1}^{P-\rho'}
  \sum_{\rho < \sigma_1 < ... < \sigma_k \leq P}
  \sum_{\rho' < \sigma'_1 < ... < \sigma'_k \leq P}
  (-\gamma)^{k+k'} \lla \tr \left[ A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T ... A_{\sigma_k} A_{\sigma_k}^T A_{\mu} A_{\mu}^T A_{\sigma'_k} A_{\sigma'_k}^T ... A_{\sigma'_1} A_{\sigma'_1}^T A_{\rho'} \right] \rra,
\end{align}
and the expectation over $\{ A_{\mu} \}$ follows (Appendix \ref{appendix_task_sim}. 4)
\begin{align} \label{eq_gamma_tr_ap2_2}
  \gamma^{k+k'} \lla \tr \left[ A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T ... A_{\sigma_k} A_{\sigma_k}^T A_{\mu} A_{\mu}^T A_{\sigma'_k} A_{\sigma'_k}^T ... A_{\sigma'_1} A_{\sigma'_1}^T A_{\rho'} \right] \rra
  = \frac{N_x^2}{N_s} \left( 
  C^{in}_{\rho \sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_k \mu} C^{in}_{\mu \sigma'_{k'} } ... C^{in}_{\sigma'_2 \sigma'_1} C^{in}_{\sigma'_1 \rho'} 
  + \Order \left( \tfrac{N_s}{N_x} \right)
  \right).
\end{align}
Therefore, at $\tfrac{N_s}{N_x} \to 0$ limit, the squared term is evaluated as
\begin{align}
  &\frac{\gamma^2}{N_y} \sum_{\mu=1}^P
  \lla \llv \sum_{\rho=1}^P B_{\rho} A_{\rho}^T \prod_{\sigma=\rho+1}^P (I - \gamma A_{\sigma} A_{\sigma}^T) A_{\mu} \rrv_F^2 \rra
  \nonumber \\
  &= \sum_{\mu=1}^P \sum_{\rho=1}^P \sum_{\rho'=1}^P C^{out}_{\rho\rho'} 
  \left( \sum_{k=0}^{P-\rho} (-1)^k \sum_{\rho < \sigma_1 < ... < \sigma_k \leq P} C^{in}_{\rho \sigma_1} ... C^{in}_{\sigma_k \mu} \right)
  \left( \sum_{k'=0}^{P-\rho'} (-1)^{k'} \sum_{\rho' < \sigma'_1 < ... < \sigma'_{k'} \leq P} C^{in}_{\rho' \sigma'_1} ... C^{in}_{\sigma'_{k'} \mu} \right)
  \nonumber \\
  &= \sum_{\mu=1}^P \sum_{\rho=1}^P \sum_{\rho'=1}^P C^{out}_{\rho\rho'} 
  \left[ (I + C^{in,U})^{-1} C^{in} \right]_{\rho \mu}
  \left[ (I + C^{in,U})^{-1} C^{in} \right]_{\rho' \mu}
  \nonumber \\
  &= \tr \left[ C^{out} (I + C^{in,U})^{-1} C^{in} \left( (I + C^{in,U})^{-1} C^{in} \right)^T \right]. 
\end{align}

Noticing that the first term of Eq. \ref{eq_bar_ef_expansion} is rewritten as
\begin{align}
  \frac{1}{N_y} \sum_{\mu=1}^P \lla \llv B_{\mu} \rrv_F^2 \rra
  = P = \tr [C^{out}], 
\end{align}
at $\tfrac{N_s}{N_x} \to 0$ limit, the final error $\bar{\epsilon}_f$ is written as
\begin{align}
  \bar{\epsilon}_f 
  &= \tr [C^{out}] - 2 \tr[C^{out} (I + C^{in,U})^{-1} C^{in} ]
  + \tr \left[ C^{out} (I + C^{in,U})^{-1} C^{in} \left( (I + C^{in,U})^{-1} C^{in} \right)^T  \right]
  \nonumber \\
  &= \llv \left( C^{out} \right)^{1/2} \left( I - (I + C^{in,U})^{-1} C^{in} \right) \rrv_F^2.
\end{align}
Thus, we obtained the equality in Theorem \ref{theorem_epsilonf}.
Note that because $C^{out}$ is a correlation matrix, there exists a matrix $\left( C^{out} \right)^{1/2}$ such that $\left( C^{out} \right)^{1/2} \left( C^{out} \right)^{1/2} = C^{out}$. 
Because $C^{in} = I + C^{in,U} + (C^{in,U})^T$, $\bar{\epsilon}_f$ is also written as
\begin{align}
  \bar{\epsilon}_f 
  = \llv \left( C^{out} \right)^{1/2} (I + C^{in,U})^{-1} \left( C^{in,U} \right)^T  \rrv_F^2.
\end{align}
Note that, if $C^{in} = I$, the error is zero. This is consistent with previous results showing that in the absence of overlap between tasks, continual learning doesn't suffer from forgetting \citep{ramasesh2020anatomy, lee2021continual, peng2023ideal}.
Additionally, Eq. \ref{eq_gamma_tr_A_ap2} requires $P \ll \tfrac{N_x}{N_s}$ (see Appendix \ref{appendix_task_sim}).4 below), thus the obtained expression doesn't hold when the number of tasks is comparable to the network size.   

\subsection{Expectation over random correlated matrices $\{A_{\mu}\}$}
We first show that $A_{\mu}^+ \to \gamma A_{\mu}^T$ and $U_{\mu} U_{\mu}^T \to \gamma A_{\mu} A_{\mu}^T$ at $\tfrac{N_s}{N_x} \to 0$, where $\gamma = \frac{N_s}{N_x}$ and $U_{\mu}$ is defined by SVD of $A_{\mu}$, $A_{\mu} = U_{\mu} \Lambda_{\mu} V_{\mu}^T$. 
If $\Lambda_{\mu} = \tfrac{1}{\sqrt{\gamma}} I$, then we have $A_{\mu} A_{\mu}^T = U_{\mu} \Lambda_{\mu}^2 U_{\mu}^T = \tfrac{1}{\gamma} U_{\mu} U_{\mu}^T$, and
\begin{align}
A^+_{\mu} 
= V_{\mu} \Lambda_{\mu}^{-1} U_{\mu}^T
= \sqrt{\gamma} \left( U_{\mu} V_{\mu}^T \right)^T
= \gamma \left( \tfrac{1}{\sqrt{\gamma}} U_{\mu} V_{\mu}^T \right)^T
= A_{\mu}^T.
\end{align}
Thus, it is sufficient to show that $\Lambda_{\mu} \to \tfrac{1}{\sqrt{\lambda}} I$ at $\tfrac{N_s}{N_x} \to 0$. 
The mean and variance of $N_s \times N_s$ matrix $A_{\mu}^T A_{\mu}$ over randomly sampled $A_{\mu}$ obey
\begin{align}
  \lla A_{\mu}^T A_{\mu} \rra_A 
  &= \tfrac{N_x}{N_s} I
  \nonumber \\
  \lla [A_{\mu}^T A_{\mu} - \tfrac{1}{\gamma} I] \odot [A_{\mu}^T A_{\mu} - \tfrac{1}{\gamma} I]\rra
  &= \tfrac{N_x}{N_s^2} \left( I + \bm{1} \bm{1}^T \right), 
\end{align}
where $\bm{1}$ is a all-one vector and $\odot$ represents Hadamard product, indicating that the standard deviation of $A_{\mu}^T A_{\mu}$ shows $\Order \left( \tfrac{N_s}{N_x} \right)$ scaling with respect to the mean. Thus, $\gamma A_{\mu}^T A_{\mu} \to I$ at $\tfrac{N_s}{N_x} \to 0$, implying $\Lambda_{\mu} \to \tfrac{1}{\sqrt{\lambda}} I$ at $\tfrac{N_s}{N_x} \to 0$. 

Regarding expectation over $A$ in Eq. \ref{eq_gamma_tr_A_ap2}, expanding the equation up to the next to the leading order, we have
\begin{align}
  &\lla \tr \left[ 
  A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T ... A_{\sigma_k} A_{\sigma_k}^T A_{\mu} \right] \rra
  \nonumber \\
  &= \sum_{i_0,...,i_k} \sum_{j_0,...,j_k}
  \lla A^{\rho}_{i_0 j_0} A^{\sigma_1}_{i_0 j_1} A^{\sigma_1}_{i_1 j_1} A^{\sigma_2}_{i_1 j_2} A^{\sigma_1}_{i_2 j_2} ... A^{\sigma_k}_{i_{k-1} j_k} A^{\sigma_k}_{i_k j_k} A^{\mu}_{i_k j_0} \rra
  \nonumber \\
  &= \sum_{i_0,...,i_k} \sum_{j_0,...,j_k} \Big(
  \delta_{j_0, j_1, j_2, ..., j_k} \left( \tfrac{1}{N_s} \right)^{k+1} C^{in}_{\rho\sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_k \mu}
  \nonumber \\
  &\quad + \sum_l
  \delta_{i_{l-1} i_l} \delta_{j_0, j_1, ..., j_{l-1}, j_{l+1}, ..., j_k} \left( \tfrac{1}{N_s} \right)^{k+1} C^{in}_{\rho\sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_{l-2} \sigma_{l-1}} C^{in}_{\sigma_{l-1} \sigma_{l+1}} C^{in}_{\sigma_l \sigma_l} C^{in}_{\sigma_{l+1} \sigma_{l+2}} ... C^{in}_{\sigma_k \mu} + ... \Big)
  \nonumber \\
  &= N_x \left( \tfrac{N_x}{N_s} \right)^k C^{in}_{\rho\sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_k \mu}
  + N_s \left( \tfrac{N_x}{N_s} \right)^k \sum_l C^{in}_{\rho\sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_{l-1} \sigma_{l+1}} C^{in}_{\sigma_l \sigma_l} ... C^{in}_{\sigma_k \mu} 
  + \Order \left( \left( \tfrac{N_x}{N_s} \right)^k \right)
  \nonumber \\
  &= \gamma^{-k} N_x \left( C^{in}_{\rho\sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_k \mu} + \Order \left( \tfrac{N_s}{N_x} \right) \right)
\end{align}
$\delta_{j_0, j_1, ..., j_k}$ in the third line is the Kronecker delta function that returns 1 if $j_0 = j_1 = ... = j_k$, otherwise returns 0.
The third line follows from Isserlis' theorem, which states that the expectation over multivariate normal variables can be decomposed into summation over all pair-wise partitions \citep{helias2020statistical}. 
In the equation above, the partition that pairs neighboring matrices takes $\Order (N_x^{k+1})$ value, while all other partitions yield $\Order (N_x^k)$ value at most because of indices mismatch. 
Note that, the number of second order terms depends on $P$, as suggested by the summation over $l$ in the third line. Thus, we expect that our theory hold only when $P$ satisfies $P \ll \tfrac{N_x}{N_s}$. 

From a parallel argument with the one above, the expectation in Eq. \ref{eq_gamma_tr_ap2_2} is evaluated as
\begin{align}
 &\gamma^{k+k'} \lla \tr \left[ A_{\rho}^T A_{\sigma_1} A_{\sigma_1}^T ... A_{\sigma_k} A_{\sigma_k}^T A_{\mu} A_{\mu}^T A_{\sigma'_k} A_{\sigma'_k}^T ... A_{\sigma'_1} A_{\sigma'_1}^T A_{\rho'} \right] \rra
 \nonumber \\
 &= \left( \tfrac{N_s}{N_x} \right)^{k+k'} 
 \left( \tfrac{N_x}{N_s} \right)^{k+k'+2} N_s \left(
 C^{in}_{\rho \sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_k \mu} C^{in}_{\mu \sigma'_{k'} } ... C^{in}_{\sigma'_2 \sigma'_1} C^{in}_{\sigma'_1 \rho'} + \Order(\tfrac{N_s}{N_x}) \right)
 \nonumber \\
 &= \frac{N_x^2}{N_s} \left( 
 C^{in}_{\rho \sigma_1} C^{in}_{\sigma_1 \sigma_2} ... C^{in}_{\sigma_k \mu} C^{in}_{\mu \sigma'_{k'} } ... C^{in}_{\sigma'_2 \sigma'_1} C^{in}_{\sigma'_1 \rho'} 
 + \Order \left( \tfrac{N_s}{N_x} \right)
 \right).
\end{align}



\section{Analysis of the impact of task order on continual learning} \label{appendix_task_order}
\subsection{Linear perturbation analysis of the order dependence}
To further investigate the order-dependence of the final error $\bar{\epsilon}_f$, we aim to decompose the error into interpretable features of task similarity matrix.  
To this end, we further constrain the input similarity matrix $C^{in}$ to
\begin{align}
  C^{in}_{ij} = \begin{cases}
  1 & (\text{if} \quad i = j) \\
  m + \delta M_{ij} & (\text{otherwise}),
  \end{cases}
\end{align}
where $m$ is a constant satisfying $-1 < m < 1$. 
Here, $\delta M_{ij}$ is a small element-wise perturbation added in such a way that $C_{in}$ is a correlation matrix. We define an upper-triangular matrix that consists of the constant component as $\bar{M}$. $(i,j)$-th element of $\bar{M}$ takes $\bar{M}_{ij} = m$ if $j > i$, but $\bar{M}_{ij} = 0$ otherwise.
This constant $\bar{M}$ assumption enables us to evaluate the effect of inverse matrix term in $\bar{\epsilon}_f$ analytically owing to the following lemma:
\begin{lemma}
For any $m$ satisfying $-1 < m < 1$, an upper-triangle matrix $\bar{M}$ satisfies
\begin{align}
  \left[ (I + \bar{M})^{-1} \right]_{ij} 
  = \delta_{ij} - m [ j > i]_+ (1 - m)^{j-i-1},
\end{align}
where $[X]_+$ is the indicator function that returns 1 if $X$ is true, but returns 0 otherwise. 
\end{lemma}
\begin{proof}
\begin{align} 
  [(I+\bar{M}) (I+\bar{M})^{-1}]_{ij}
  &= \sum_{k=1}^P 
  \left( \delta_{ik} + m [k>i]_+ \right)
  \left( \delta_{kj} - m [j>k]_+ (1-m)^{j-k-1} \right)
  \nonumber \\
  &= \delta_{ij} + [j > i]_+ \left(
  -m(1-m)^{j-i-1} + m 
  -m^2 \sum_{k=i+1}^{j-1} (1 - m)^{j-k-1}
  \right) \nonumber \\
  &= \delta_{ij} + [j > i]_+ \left(
  -m(1-m)^{j-i-1} + m - m \left[ 1 - (1-m)^{j-i-1} \right] 
  \right) \nonumber\\
  &= \delta_{ij}.
\end{align}
\end{proof}

Let us consider the case when the output similarity is the same across all the tasks for simplicity. Then, $C^{out}$ is written as 
\begin{align}
  C^{out} = \rho_o \bm{1}\bm{1}^T + [1-\rho_o] I.
\end{align}
In this problem setting, assuming that $\delta M$ is sufficiently small compared to $\bar{M}$, we can rewrite the error $\bar{\epsilon}_f$ as a linear function of $\delta M$, which enables us to interpret the contribution of different pairwise similarities to the final error. 
The following theorem describes the exact decomposition of the impact of task order in this linear perturbation limit. 
\begin{theorem} \label{theorem_linpert_decompose}
  Let us suppose that all elements of a upper-triangle matrix with zero-diagonal components $\delta M_{ij}$ satisfies, $| \delta M_{ij} | < \delta_m$, where $\delta_m$ is a positive constant.  
  Then, the error $\bar{\epsilon}_f$ is rewritten as below:
  \begin{align}
    \bar{\epsilon}_f 
    = \bar{\epsilon}_f [m, \rho_o]
    + \sum_{i=1}^P \sum_{j = i+1}^P G_{ij} \delta M_{ij} + \Order(\delta_m^2),
  \end{align}
  where
  \begin{subequations}
  \begin{align}
    \bar{\epsilon}_f [m, \rho_o]
    &\equiv \llv \left( \rho_o \bm{1}\bm{1}^T + [1-\rho_o] I \right)^{1/2} (I + \bar{M})^{-1} \bar{M}^T \rrv_F^2 
    \\ \label{eq_def_Gij_full}
    G_{ij} &\equiv g_o (i) + g_o (j) + g_{+} (j+i) + g_{-} (j-i) ,
  \end{align}
  \end{subequations}
  and functions $g_o, g_{-}, g_{+}: \mathbb{Z} \to \mathbb{R}$ that constitute the coefficient $G$ are defined by
  \begin{subequations} \label{eq_def_go_gpm}
  \begin{align} 
    g_o (k) 
    &\equiv \tfrac{(1-\rho_o)m}{2-m} (1-m)^{P-k}
    - \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) (1-m)^{P+k-1},
    \\
    g_{+} (s)
   &\equiv \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) \tfrac{3-m}{2-m} (1-m)^{s-1} 
   - \tfrac{(1-\rho_o)m}{2-m} \left( Pm + 2(1-m) - \tfrac{(1-m)^2}{2-m} \right) (1-m)^{2P - s},
    \\ 
    g_{-}(d)
    &\equiv \tfrac{(1-\rho_o)m}{2-m} \tfrac{1}{2-m} (1-m)^{d-1}
   - \left[ \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) 
 \left( 1 - (1-m)^P \left( \tfrac{mP}{1-m} + \tfrac{3-m}{2-m} \right) \right) - \tfrac{(1-\rho_o)m}{2-m} \tfrac{1}{1-m} \right] (1-m)^{P-d}.
  \end{align}
  \end{subequations}
\end{theorem}

Matrix $G$ specifies the relative contribution of each task-to-task similarity to the final error. 
Notably, $\bar{\epsilon}_f [m, \rho_o]$ term is permutation invariant by construction. Thus, task-order dependence stems from the $\delta M$-dependent terms. 

Inserting $\rho_o = 1$ into Eqs. \ref{eq_def_go_gpm}, you get Theorem \ref{theorem_linpert_decompose_main} in the main text.

\subsection{Proof of Theorem \ref{theorem_linpert_decompose}}
The inverse of $(I + \bar{M} + \delta M)$ is rewritten as
\begin{align}
(I + \bar{M} + \delta M)^{-1}
= (I + \bar{M})^{-1} - (I + \bar{M})^{-1} \delta M (I + \bar{M})^{-1} + \Order (\delta M^2).
\end{align}
Thus, up to the leading order with respect to $\delta M$, we have
\begin{align}
  \bar{\epsilon}_f 
  &= \llv \left( \rho_o \bm{1}\bm{1}^T + [1-\rho_o] I \right)^{1/2} \left( I + \bar{M} + \delta M \right)^{-1} (\bar{M} + \delta M)^T \rrv_F^2
  \nonumber \\
  &= \llv \left( \rho_o \bm{1}\bm{1}^T + [1-\rho_o] I  \right)^{1/2} (I + \bar{M})^{-1} \bar{M}^T \rrv_F^2 
  \nonumber \\
  &\quad + 2 \tr \left[ \bar{M} (I+\bar{M})^{-T} \left( C^{out} \right)^{1/2} (I + \bar{M})^{-1} \left( \delta M^T - \delta M (I + \bar{M})^{-1} \bar{M}^T \right) \right] + \Order(\delta M^2).
\end{align}
The first term corresponds to $\bar{\epsilon}_f [\bar{M}, C^{out}]$, thus it is enough to show that the coefficients of $\delta M$ is written as Eqs. \ref{eq_def_Gij_full} and \ref{eq_def_go_gpm}. 

$\bar{M} (I+\bar{M})^{-T}$ term is rewritten as
\begin{align}
  [\bar{M} (I + \bar{M})^{-T}]_{ij}
  &= \sum_{k=1}^P m [k>i]_+ \left( \delta_{jk} - m [k>j]_+ (1-m)^{k-j-1} \right) 
  \nonumber\\
  &= m [j>i]_+ - m \left( (1-m)^{k_{ij} - j - 1} - (1-m)^{P-j} \right)
  \nonumber \\
  &= [j > i]_+ m \left( 1 - [1 - (1-m)^{P-j}] \right)
  - m [j \leq i]_+ \left( (1-m)^{i-j} - (1-m)^{P-j} \right)
  \nonumber \\
  &= m (1-m)^{P-j} - m[j \leq i]_+ (1-m)^{i-j}.  
\end{align}
In the second line, we defined $k_{ji}$ by $k_{ji} \equiv \max (i+1, j+1)$.
For the ease of notation, let us denote the last term in the equation above as
\begin{align}
  v_{ji} \equiv m (1-m)^{P-j} - m[j \leq i]_+ (1-m)^{i-j}. 
\end{align}

Next, $\left( \rho_o \bm{1}\bm{1}^T + [I - \rho_o] I \right) (I + \bar{M})^{-1}$ term becomes 
\begin{align}
&\left[ \left( \rho_o \bm{1}\bm{1}^T + [I - \rho_o] I \right) (I + \bar{M})^{-1} \right]_{ij}
\nonumber \\
&= \sum_{k} \left( \rho_o + (1-\rho_o) \delta_{ik} \right)
\left( \delta_{kj} - m [j>k]_+ (1-m)^{j-k-1} \right)
\nonumber \\
&= \rho_o \left( 1 - \sum_k m [j > k]_+ (1-m)^{j-k-1} \right)
+ (1 - \rho_o) \left( \delta_{ij} - m [j>i]_+ (1-m)^{j-i-1} \right)
\nonumber \\
&= \rho_o (1 - m)^{j-1} + (1 - \rho_o) \delta_{ij} 
- (1 - \rho_o) m [j>i]_+ (1-m)^{j-i-1}
\end{align}
In the last line, we used 
$\sum_k m [j>k]_+ (1-m)^{j-k-1} = 1 - (1-m)^{j-1}$.
As before, let us denote the coefficient by 
\begin{align}
  u_{ij} \equiv \rho_o (1-m)^{j-1} - (1 - \rho_o) m [j>i]_+ (1-m)^{j-i-1}.
\end{align}

Then, the first-order term with respect to $\delta M$ follows
\begin{align}
  &\tr \left[ \bar{M} (I+\bar{M})^{-T} \left( \rho_o \bm{1}\bm{1}^T + (1 - \rho_o) I \right) (I + \bar{M})^{-1} \left( \delta M^T - \delta M (I + \bar{M})^{-1} \bar{M}^T \right) \right]
  \nonumber \\
  &= \sum_{ijk} \left[ \bar{M} (I+\bar{M})^{-T} \right]_{ij}
  \left[ \left( \rho_o \bm{1}\bm{1}^T + [I - \rho_o] I \right) (I + \bar{M})^{-1} \right]_{jk} 
  \left( \delta M_{ik} - \sum_l \delta M_{kl} \left[ \bar{M} (I + \bar{M})^{-T} \right]_{il} \right)
  \nonumber \\
  &= \sum_{ijk} v_{ji} \left( (1-\rho_o)\delta_{jk} + u_{jk} \right) \left( \delta M_{ik} - \sum_l \delta M_{kl} v_{li} \right)
  \nonumber \\
  &= \sum_{kl} \delta M_{kl} 
  \sum_{ij} v_{ji} \left( 
  \delta_{ik} \left[ (1-\rho_o) \delta_{jl} + u_{jl} \right] - v_{li} \left[ (1-\rho_o) \delta_{jk} + u_{jk} \right] \right)
  \nonumber \\
  &= \sum_{kl} \delta M_{kl} G_{kl},
\end{align}
where the coefficient of $(k,l)$-th element is defined by 
\begin{align}
  G_{kl} \equiv \sum_{ij} v_{ji} \left( 
  \delta_{ik} \left[ (1-\rho_o) \delta_{jl} + u_{jl} \right] - v_{li} \left[ (1-\rho_o) \delta_{jk} + u_{jk} \right] \right).
\end{align}

$G_{kl}$ is decomposed into
\begin{align}
  G_{kl}
  = (1- \rho_o) v_{lk} + \sum_j v_{jk} u_{jl}
  - (1-\rho_o) \sum_i v_{ki} v_{li} - \sum_{ij} v_{ji} v_{li} u_{jk}. 
\end{align}
The first term $(1-\rho_o) v_{lk}$ is rewritten as
\begin{align}
  (1-\rho_o) v_{lk}
  &= (1-\rho_o) m (1-m)^{P-l} - m [l \leq k]_+ (1-m)^{k-l}
  \nonumber \\
  &= (1-\rho_o) m (1-m)^{P-l}.
\end{align}
Here, we dropped the second term, because $\delta M_{kl} = 0$ when $l \leq k$. 

Regarding the second term, summation over $j$ is evaluated as 
\begin{align}
  \sum_{j=1}^P v_{ji} u_{jk}
  &= \sum_j 
  \left( m (1-m)^{P-j} - m [j\leq i]_+ (1-m)^{i-j} \right)
  \left( \rho_o (1-m)^{k-1} - (1-\rho_o) m [k>j]_+ (1-m)^{k-j-1} \right)
  \nonumber \\
  &= \rho_o (1-m)^{k-1} 
  \left( m \sum_{j=1}^P (1-m)^{P-j} - m \sum_{j=1}^i (1-m)^{i-j} \right)
  \nonumber \\
  &\quad - (1 - \rho_o) m^2 \left( \sum_{j=1}^{k-1} (1-m)^{(P-j)+(k-j-1)} - \sum_{j=1}^{j_{ik}} (1-m)^{(i-j)+(k-j-1)} \right)
  \nonumber \\
  &= \rho_o (1-m)^{k-1} \left( (1-m)^i - (1-m)^P \right)
  \nonumber \\
  &\quad -  \frac{(1 - \rho_o) m}{2-m} \left( 
  \left[ (1-m)^{P-(k-1)} - (1-m)^{P+(k-1)} \right]
  - \left[ (1-m)^{i+k-1 - 2j_{ik}} - (1-m)^{i+k-1} \right]
  \right)
  \nonumber \\
  &= \left( \rho_o -  \frac{(1-\rho_o) m}{2-m} \right) \left( (1-m)^{i+k-1} - (1-m)^{P+k-1} \right)
  + \frac{(1-\rho_o)m}{2-m} \left( (1-m)^{i+k-1-2j_{ik}} - (1-m)^{P-(k-1)} \right).
\end{align}
In the third line, we defined $j_{ik}$ as $j_{ik} \equiv \min (k-1, i)$.
Thus, the second, $\sum_j v_{jk} u_{jl}$, term becomes
\begin{align}
  \sum_j v_{jk} u_{jl}
  &= - \left( \left[ \rho_o - \frac{(1-\rho_o)m}{2-m} \right] (1-m)^{P+l-1} + \frac{(1-\rho_o)m}{2-m} (1-m)^{P-l+1} \right)
  \nonumber \\
  &\quad + \left[ \rho_o - \frac{(1-\rho_o)m}{2-m} \right] (1-m)^{k+l-1} 
  + \frac{(1-\rho_o)m}{2-m} (1-m)^{l-k-1}.
\end{align}
The third term is, from a similar calculation, rewritten as
\begin{align}
  &\sum_i v_{li} v_{ki}
  \nonumber \\
  &= \sum_i
  \left( m(1-m)^{P-l} - m [l \leq i]_+ (1-m)^{i-l} \right) 
  \left( m (1-m)^{P-k} - m[k \leq i]_+ (1-m)^{i-k} \right)
  \nonumber \\
  &= P m^2 (1-m)^{2P-(k+l)}
  - m^2 \left( (1-m)^{P-k} \sum_{i=l}^P (1-m)^{i-l} + (1-m)^{P-l} \sum_{i=k}^P (1-m)^{i-k} \right)
  + m^2 \sum_{i=l}^P (1-m)^{2i-(k+l)}
  \nonumber \\
  &= \left( P m^2 + 2 m(1-m) - \frac{m (1-m)^2}{2-m} \right) (1-m)^{2P-(k+l)}
  - m \left( (1-m)^{P-k} + (1-m)^{P-l} \right)
  + \frac{m}{2-m} (1-m)^{l-k}. 
\end{align}

The last term is a little more complicated, so let us divide it into two terms:
\begin{align}
  \sum_i v_{li} \sum_j v_{ji} u_{jk}
  = \left( \rho_o - \frac{(1-\rho_o)m}{2-m} \right) T_1 + \frac{(1-\rho_o)m}{2-m} T_2,
\end{align}
where
\begin{subequations}
\begin{align}
  T_1 &= 
  m (1-m)^{P-l} \sum_{i=1}^P \left( (1-m)^{i+k-1} - (1-m)^{P+k-1} \right)
  - m \sum_{i=l}^P (1-m)^{i-l} \left( (1-m)^{i+k-1} - (1-m)^{P+k-1} \right),
  \nonumber \\
  T_2 &= 
  m (1-m)^{P-l} \sum_{i=1}^P \left( (1-m)^{i+k-1-2j_{ik}} 
  - (1-m)^{P-(k-1)} \right)
   - m\sum_{i=l}^P (1-m)^{i-l}\left( (1-m)^{i-(k-1)} - (1-m)^{P-(k-1)} \right).
\end{align}
\end{subequations}
Taking summation over index $i$, $T_1$ and $T_2$ are rewritten as
\begin{align}
  T_1 &= 
  m (1-m)^{P-l} \left( \frac{(1-m)^k}{m} \left[ 1 - (1-m)^P \right] - P (1-m)^{P+k-1} \right)
  \nonumber \\
  &\quad - \left( 
  \frac{1}{2-m} \left[ (1-m)^{l+k-1} - (1-m)^{2P+1+(k-l)} \right] 
  - (1-m)^{P+k-1} \left[ 1 - (1-m)^{P+1-l} \right]
  \right)
  \nonumber \\
  &= \left( 1 - (1-m)^P \left[ 2 + \frac{mP}{1-m} - \frac{1-m}{2-m} \right] \right) (1-m)^{P-(l-k)}
  - \frac{1}{2-m} (1-m)^{l+k-1} + (1-m)^{P+k-1}.
\end{align}
\begin{align}
  T_2 &= 
  (1-m)^{P-l} \left( 
  [1 - (1-m)^{k-1}] + [(1-m) - (1-m)^{P+2-k}] - mP(1-m)^{P-(k-1)}
  \right)
  \nonumber \\
  &\quad - \left( \frac{1}{2-m} [(1-m)^{l-k+1} - (1-m)^{2P+3 - (l+k)}]
  - [1 - (1-m)^{P+1-l}] (1-m)^{P-(k-1)}\right)
  \nonumber \\
  &= - \frac{1-m}{2-m} (1-m)^{l-k}
  - \frac{1}{1-m} (1-m)^{P-(l-k)}
  + \left( (2-m) (1-m)^{P-l} + (1-m)^{P-k+1} \right)
  \nonumber \\
  &\quad - (1-m)^{2P-(k+l)} \left( mP(1-m) + 2 (1-m)^2 - \frac{(1-m)^3}{2-m} \right).
\end{align}

The results above show that $G_{kl}$ is decomposed into four components:
\begin{align}
  G_{kl} = g_- (l-k) + g_+ (l+k) + g_L (k) + g_R (l). 
\end{align}
Summing over the terms that only depends on $k$, we have
\begin{align}
  g_L (k) &= 
  - (1- \rho_o) \left( -m (1-m)^{P-k} \right)
  - \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) (1-m)^{P+k-1}
  - \tfrac{(1-\rho_o)m}{2-m} (1-m)^{P-k+1}
  \nonumber \\
  &= \tfrac{(1-\rho_o)m}{2-m} (1-m)^{P-k}
  - \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) (1-m)^{P+k-1}
\end{align}
Similarly, the terms that only depend on $l$ are summed up to:
\begin{align}
  g_R (l) &= 
  (1 - \rho_o) m (1-m)^{P-l}
  - \left( \left[ \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right] (1-m)^{P+l-1}
  + \tfrac{(1-\rho_o)m}{2-m} (1-m)^{P-l+1} \right)
  \nonumber \\
  &\quad- (1-\rho_o) \left( -m (1-m)^{P-l} \right)
  - \tfrac{(1-\rho_o)m}{2-m} (2-m) (1-m)^{P-l}
  \nonumber \\
  &= \tfrac{(1-\rho_o)m}{2-m} (1-m)^{P-l}
  - \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) (1-m)^{P+l-1}.
\end{align}
Therefore, $g_L$ and $g_R$ have the same form. 
We denote this function as $g_o$ below. 

$g_+ (l+k)$ term has a slightly more complicated expression:
\begin{align}
  g_+ (l+k) &=
  \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) (1-m)^{k+l-1}
  - (1-\rho_o)m \left( Pm + 2(1-m) - \tfrac{(1-m)^2}{2-m} \right) (1-m)^{2P-(k+l)}
  \nonumber \\
  &\quad + \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) \tfrac{1}{2-m} (1-m)^{l+k-1}
  + \tfrac{(1-\rho_o)m}{2-m} (1-m) \left( Pm + 2(1-m) - \tfrac{(1-m)^2}{2-m} \right) (1-m)^{2P - (k+l)}
  \nonumber \\
  &= \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) \tfrac{3-m}{2-m} (1-m)^{l+k-1} 
  - \tfrac{(1-\rho_o)m}{2-m} \left( Pm + 2(1-m) - \tfrac{(1-m)^2}{2-m} \right) (1-m)^{2P - (k+l)}.
\end{align}
Lastly, $g_- (l-k)$ term becomes
\begin{align}
  &g_- (l-k) \nonumber \\
  &= \tfrac{(1-\rho_o)m}{2-m} (1-m)^{l-k-1}
  - \tfrac{(1-\rho_o)m}{2-m} (1-m)^{l-k}
  - \left( \rho_o - \tfrac{(1-\rho_o)m}{2-m} \right) 
  \left( 1 - (1-m)^P \left( 2 + \tfrac{mP}{1-m} - \tfrac{1-m}{2-m} \right) \right) (1-m)^{P-(l-k)}
  \nonumber \\
  &\quad + \tfrac{(1-\rho_o)m}{2-m} \left( \tfrac{1-m}{2-m} (1-m)^{l-k} + \tfrac{1}{1-m} (1-m)^{P-(l-k)} \right)
  \nonumber \\
  &= \tfrac{(1-\rho_o)m}{2-m} \tfrac{1}{2-m} (1-m)^{l-k-1}
  - \left[ \left( 1 - (1-m)^P \left( \tfrac{mP}{1-m} + \tfrac{3-m}{2-m} \right) \right) - \tfrac{(1-\rho_o)m}{2-m} \tfrac{1}{1-m} \right] (1-m)^{P-(l-k)}.
\end{align}
We thus obtain Eq. \ref{eq_def_go_gpm}. 
If we set $\rho_o = 1$, we recover Theorem \ref{theorem_linpert_decompose_main}. 
In Theorem \ref{theorem_linpert_decompose_main}, $G^+$ is defined as
$G^+_{ij} = g_+ (i+j) + g_L(i) + g_R(j)$.

\subsection{The impact of task typicality}
We can further analyze the impact of task typicality on the task order discuss in the main text.
Let us define $\bar{g}$ by
\begin{align}
  \bar{g} 
  \equiv \frac{1}{P} \sum_{\mu=1}^P (1-m)^{\mu}
  = \frac{1}{mP} \left( (1-m) - (1-m)^{P+1} \right),
\end{align}
then $G^+_{\mu\nu}$ is rewritten as
\begin{align}
  G^+_{\mu\nu}
  &= \left[ \tfrac{(3-m) \bar{g} }{(2-m) (1-m)} - (1-m)^{P-1} \right] \left[ (1-m)^{\mu} + (1-m)^{\nu} \right]
  \nonumber \\
  &\quad + \tfrac{3-m}{ (2-m) (1-m) } \left( (1-m)^{\mu} - \bar{g} \right)  \left( (1-m)^{\nu} - \bar{g} \right)
  - \tfrac{ (3-m) \bar{g}^2 }{ (2-m) (1-m) }.
\end{align}
Thus, the corresponding error term $\delta \bar{\epsilon}^+_f$ becomes
\begin{align}
  \delta \bar{\epsilon}^+_f
  &= \sum_{\mu=1}^P \sum_{\nu=\mu+1}^P G^+_{\mu\nu} \delta M_{\mu\nu}
  \nonumber \\
  &= P \left[ \tfrac{(3-m) \bar{g} }{(2-m) (1-m)} - (1-m)^{P-1} \right] \sum_{\mu=1}^P (1-m)^{\mu} \delta t_{\mu} 
  \nonumber \\
  &\quad + \tfrac{3-m}{ (2-m) (1-m) } \sum_{\mu=1}^P \sum_{\nu=\mu+1}^P \left( (1-m)^{\mu} - \bar{g} \right)  \left( (1-m)^{\nu} - \bar{g} \right) \delta M_{\mu\nu}
  + \text{const.}
\end{align}
If $\tfrac{(3-m) \bar{g} }{(2-m) (1-m)} > (1-m)^{P-1}$, from the rearrangement inequality, the first term is minimized under $\delta t_1 \leq \delta t_2 \leq ... \leq \delta t_P$ ordering. However, the second term may not be minimized under this task order.  

\begin{figure*}[tb]
%\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{figs/fig_lst_pert_supp1-2.pdf}}
\vskip -0.1in
\caption{
\textbf{a, b)} The same as Fig. \ref{lin_pert}e and g, but without normalization. The average correlation dominates the error $\epsilon_f$, but the order dependence is observed robustly.   
\textbf{c)} The ratio of networks where the periphery-to-core order outperforms core-to-periphery order. 
As before, we generated 1000 samples of $C^{in}$ matrices randomly while fixing $C^{out}$ to be all one, then binned $C^{in}$ based on the average off-diagonal correlation. For each average correlation value $m$, we then calculated the ratio of networks in which the periphery-to-core order achieved smaller error than the core-to-periphery order. 
\textbf{d)} The same as panel c, but the ratio of random seeds where the max-path orders achieved the better performance than the min-path orders. 
}
\label{lin_pert_supp}
\end{center}
\vskip -0.2in
\end{figure*}

\section{Implementation details} \label{sec_implementation_details}
Source codes for all the numerical experiments are available at \url{https://github.com/ziyan-li-code/optimal-learn-order}.

\subsection{Linear teacher-student model with latent variables}
In the simulations depicted in Figs. \ref{lin_theory_simul}b, \ref{lin_pert}eg, and \ref{lin_graph}ef, we set the latent vector size $N_s = 30$, input layer size $N_x = 3000$, and the output layer size $N_y = 10$. 
We initialized the weight matrix $W$ as the zero matrix, and then updated the weight using gradient descent (Eq. \ref{eq_lst_gd_discrete}) with learning rate $\eta = 0.001$ for 100 iterations per task.  
In Fig. \ref{lin_theory_simul}b, the input correlation matrix $C^{in} \in \Real^{P \times P}$ was generated randomly in the following manner. First, we generated a strictly upper-triangular matrix $C^{in,U}$ by sampling each element independently from a continuous uniform distribution between 0 and 1, $\mathcal{U}_{[0,1]}$. We then generated the full matrix $C^{in}$ by $C^{in} = C^{in,U} + (C^{in,U})^T + I$. If the resultant $C^{in}$ is a positive semi-definite matrix, we accepted the matrix, otherwise, we generated $C^{in}$ in the same manner, until we obtain a positive semi-definite matrix. 
Here, we limited the correlation to be positive mainly because continual learning is typically impractical when there exists a large negative correlation between tasks. 
We generated $C^{out}$ using the same method. 
In Fig. \ref{lin_theory_simul}c, we estimated the final error $\bar{\epsilon}_f$ using Eq. \ref{eq_epsilon_f_analytical} for each triplet $(\rho_{AB}, \rho_{BC}, \rho_{CA})$. We calculated the error for all six task orders and plotted the order that yielded the minimum error. 

In Figs. \ref{lin_pert}e and g, we generated input correlation matrix $C^{in}$ using the same method with Fig. \ref{lin_theory_simul}b, but we instead sampled the elements from a uniform distribution between -1 and 1, $\mathcal{U}_{[-1,1]}$. The average correlation $m$ was defined by the average of the off-diagonal components of $C^{in}$. The output correlation matrix $C^{out}$ was set to be the all one matrix (i.e. $C^{out}_{\mu\nu} = 1$ for all $(\mu,\nu)$ pairs) which corresponds to the $\rho_o = 1$ scenario.
We estimated the error under each task order for 1000 randomly generated input correlation matrices and binned the performance by the average correlation. The average performance (black lines in Figs. \ref{lin_pert}e and g) were estimated by taking the average over randomly sampled 100 task orders. The error bars, representing the standard error of mean, are larger for larger average correlation because we didn't generated many $C^{in}$ with a large average correlation under our random generation method. 
Because there are two task sequence that provides the max-path due to symmetry (e.g. \textit{A\textrightarrow C\textrightarrow E\textrightarrow B\textrightarrow D} and \textit{D\textrightarrow B\textrightarrow E\textrightarrow C\textrightarrow A} in Fig. \ref{lin_pert}c), we defined the error of the max-path rule as the average over these two task orders. 


\subsection{Generation of input similarity matrices having simple graph structures}
In Fig. \ref{lin_graph}, we introduced simple graph structures to the task similarity matrices.   
To this end, we first generated an unweighted bidirectional adjacency matrix $A_{dj} \in \{0, 1\}^{P \times P}$ given a graph structure, then calculated distance between nodes on the graph specified by $A_{dj}$, which we denote as $D$. 
From this distance matrix $D$, we generated the input similarity matrix $C^{in}$ by $C^{in}_{ij} = a^{D_{ij}}$ for each task pairs $(i,j)$.  
Here, $a$ is the constant that controls overall task similarity. $a \approx 1$ means that inputs for all tasks are highly correlated, whereas $a \approx 0$ means that they are mostly independent. 

For instance, in the case of chain graph, the adjacency matrix is given by $A_{dj,ij} = 1$ if $j = i \pm 1$ else $A_{dj,ij} = 0$. Thus, the distance between nodes $D$ follows $D_{ij} = |i-j|$ and the input correlation matrix becomes $C^{in}_{ij} = a^{|i-j|}$. 
In the ring graph, the distance between node is instead given by $D_{ij} = \min \{ |i-j|, P-|i-j| \}$.
For the tree graph, we used a tree where each non-leaf node has exactly two children nodes. 
The same structure was assumed for the leaves graph, except that we only used leaf nodes for constructing the task similarity matrix. 



\begin{figure*}[tb]
%\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{figs/fig_nonlinear_supp3-1.pdf}}
\vskip -0.1in
\caption{Task order preference in continuous image classification tasks.
\textbf{a,b)} Average classification performance after continual learning of CIFAR-100 where each task consists of 5 label classifications. 
Here, we randomly picked 25 labels from CIFAR-100 dataset and generated 5 tasks each requiring classification of 5 labels.  
\textbf{c,d)} Average classification performance after continual learning on CIFAR-10. Task similarity was estimated by evaluating zero-shot generalization performance from task $A$ to $B$, using the training data of $B$ instead of the test data (see Appendix \ref{sec_implementation_details}.4 for details).
}
\label{order_nonlin_supp}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[tb]
%\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\linewidth]{figs/fig_sigmat1-1.pdf}}
\vskip -0.1in
\caption{Schematic of the task similarity estimation. From the error in zero-shot transfer $\epsilon_{\mu} [W_{\nu}]$ (left), we estimated task similarity $\rho_{\mu\nu}$ (right).}
\label{sim_mat}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Convolutional and multi-layered non-linear neural networks for image classification}
We used convolutional neural networks (CNNs) for numerical experiments with the CIFAR-10 and CIFAR-100 datasets.
The network consisted of two convolutional layers and one dense layer, followed by an output layer.
The first convolutional layer had 32 filters with $3 \times 3$ kernels. The output was passed through a Rectified Linear Unit (ReLU) activation function and then downsampled using average pooling with a window size of $2 \times 2$ and a stride of 2. The second convolutional layer was similar to the first, except that we used 64 filters.
The dense layer following the two convolutional layers had 256 neurons, with ReLU as the activation function. For classification, we used a softmax activation function in the last layer. The weights of both convolutional and dense layers were initialized with LeCun normal initializers. 

For the Fashion-MNIST dataset, we used multi-layer perceptrons (MLPs) to evaluate the robustness of our findings against the neural architecture.
The MLP model had two hidden layers with 128 and 64 neurons, respectively. We used ReLU as the activation function for the hidden layers and softmax for the output layer.

In both CNN and MLP models, we studied binary classification with two output neurons, except in Figure \ref{order_nonlin_supp}a and b, where we considered a classification of 5 labels with five output neurons. 
All tasks were implemented as single-head continual learning where the output nodes are shared across tasks. The performance was evaluated by the average classification accuracy on the test datasets for all the tasks at the end of the entire training. 

The networks were trained by minimizing the cross-entropy loss using the Adam optimizer with a learning rate of $10^{-3}$ for five epochs per task. We set the batch size to 4 due to GPU memory constraints. The models were implemented using Flax \citep{flax2020github}, a JAX neural network library, and were trained on NVIDIA Tesla V100 GPUs 

In both Figures \ref{order_nonlin} and \ref{order_nonlin_supp}, we generated 100 task sets by randomly dividing 10 labels into a set of five binary classification tasks. In the case of CIFAR-100, we initially sampled 10 labels randomly, then partitioned them into five binary classification. 
We binned the task sets by the average similarity estimated from Eq. \ref{eq_def_rhoAB_nonlin}, then plotted the mean performance and the standard error of mean for each bin. 
The black lines representing the average performance of random task order were estimated by taking the mean of the classification performance under 30 random task orders for each task set. 
Because there are two task orders that provides the max-path by construction, we define the performance of the max-path rule as the mean of performance under these two task orders. 

\subsection{Estimation of task similarity}

In the main text, we inferred similarity between two tasks \textit{A} and \textit{B} using zero-shot transfer performance between the two tasks. 
Previous work on linear model indicates that, if the output similarity is one, the pairwise transfer performance $\Delta \epsilon_{TF} [\nu \to \mu] \equiv \epsilon_{\mu} [W_{\nu}] - \epsilon_{\mu} [W_o]$ is written as \citep{hiratani2024disentangling}
\begin{align}
  \Delta \epsilon_{TF} [\nu \to \mu] = \rho^{in}_{\mu\nu} (2 - \rho^{in}_{\mu\nu}), 
\end{align}
indicating that the input similarity between two tasks can be inferred as
\begin{align}
  \rho^{in}_{\mu\nu}  = 1 - \sqrt{1 - \Delta \epsilon_{TF}  [\nu \to \mu]}.
\end{align}
Motivated by this relationship, we defined similarity between tasks in general nonlinear networks by Eq. \ref{eq_def_rhoAB_nonlin}. 
A similar approach was implemented in \citep{lad2009toward}. 
Notably, this method only requires inputs/outputs of the trained network, and thus applicable to situation where the model details are inaccessible (e.g., human and animal brains, closed-LLM). 

We implemented the evaluation the zero-shot transfer performance from task \textit{A} to \textit{B} as follows: First, we trained a network on task \textit{A} for 5 epochs from a random initialization using the Adam optimizer on the cross-entropy loss with learning rate $10^{-3}$ as above. We then measured the cross-entropy loss on the test dataset of task \textit{B}, $\epsilon_B [W_A]$. 
To normalize the accuracy, we divided the obtained loss by the loss on task \textit{B} under a label shuffling, $\epsilon_{B,sf} [W_A]$. 
The resultant value $\frac{\epsilon_B[W_A]}{\epsilon_{B,sf} [W_A]}$ characterizes how well the network transfer to task \textit{B} compared to a random task with the same input statistics. 
Since evaluating similarity using the transfer performance on the test data may potentially introduce bias, in Fig. \ref{order_nonlin_supp}c and d, we estimated the transfer performance $\Delta \epsilon_B [W_A]$ using the training dataset for task $B$.
Even in this setting, we found results nearly identical to those in Fig. \ref{order_nonlin}c and d, confirming the robustness of our findings with respect to the details of the similarity evaluation method.

\end{document}