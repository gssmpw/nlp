\section{Related Work}
%Due to space constraints, we have not discussed many other interesting training phenomena that may also be related to symmetry. In fact, parameter symmetries have been extensively studied by previous literature. Permutation symmetries are found to induce connected critical points for overparameterized networks \cite{simsek2021geometry} and lead to linear model connectivity \cite{entezari2021role}. Rescaling symmetries can be used to explain the generalization of ReLU networks \cite{yi2019positively}. Rotational symmetries can cause the dimensional collapse in self-supervised learning \cite{ziyin2023what} and the low-rankness of attention layers \cite{brea2019weight}.

%Symmetries also motivate the design of optimization methods. One line of research utilizes parameter symmetries to accelerate optimization \cite{armenta2023neural,zhao2023improving}, and other work discusses how to remove symmetries to improve the model \cite{lim2024empirical,ziyin2024remove}.

%For examples, Ref.~\citep{ziyin2024parameter} showed that both progressive sharpening \citep{wu2018sgd, cohen2021gradient} and flattening (which is required for warmup \citep{kalra2024warmup}) may be due to symmetry.