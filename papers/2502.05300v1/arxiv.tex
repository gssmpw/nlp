\documentclass[10pt]{article}
\renewcommand{\baselinestretch}{1.0}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx, enumitem}
\usepackage{graphics}
\usepackage{centernot}
\usepackage{authblk}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem*{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem*{surfacecor}{Corollary 1}
\newtheorem{conjecture}{Conjecture}
\newtheorem{question}{Question} 
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}



\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}

 
\newcommand{\fsl}[1]{{\centernot{#1}}}
\setlength{\fboxsep}{0pt}
\setlength{\fboxrule}{.1pt}

\usepackage[bottom]{footmisc}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{subfloat}
%\usepackage{subfig}
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{color}
\usepackage{MnSymbol}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage[dvipsnames]{xcolor}
\usepackage{caption} 
\usepackage{natbib}
\usepackage{bbm}

%\usepackage{epsfig}
%\usepackage{subcaption}
%\usepackage{float}
\usepackage{textcomp}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{csquotes}



\usepackage[many]{tcolorbox}    	% for COLORED BOXES (tikz and xcolor included)
%\usepackage{mathspec} 			    % for FONTS
\usepackage{setspace}               % for LINE SPACING
\usepackage{multicol}               % for MULTICOLUMNS

                      % setting the font as Noto Sans

\definecolor{main}{HTML}{5989cf}    % setting main color to be used
\definecolor{sub}{HTML}{cde4ff}     % setting sub color to be used


\tcbset{
    sharp corners,
    colback = white,
    before skip = 0.2cm,    % add extra space before the box
    after skip = 0.5cm      % add extra space after the box
}                           % setting global options for tcolorbox

% You can copy any following box you like to your code.
\newtcolorbox{boxA}{
    fontupper = \bf,
    boxrule = 1.5pt,
    colframe = black % frame color
}

\newtcolorbox{boxB}{
    fontupper = \bf\color{main}, % font color
    boxrule = 1.5pt,
    colframe = main,
    rounded corners,
    arc = 5pt   % corners roundness
}

\newtcolorbox{boxC}{
    colback = sub, % background color
    boxrule = 0pt  % no borders
}

\newtcolorbox{boxD}{
    colback = sub, 
    colframe = main, 
    boxrule = 0pt, 
    toprule = 3pt, % top rule weight
    bottomrule = 3pt % bottom rule weight
}

\newtcolorbox{boxE}{
    enhanced, % for a fancier setting,
    boxrule = 0pt, % clearing the default rule
    borderline = {0.75pt}{0pt}{main}, % outer line
    borderline = {0.75pt}{2pt}{sub} % inner line
}

\newtcolorbox{boxF}{
    colback = sub,
    enhanced,
    boxrule = 1.5pt, 
    colframe = white, % making the base for dash line
    borderline = {1.5pt}{0pt}{main, dashed} % add "dashed" for dashed line
}

\newtcolorbox{boxG}{
    enhanced,
    boxrule = 0pt,
    colback = sub,
    borderline west = {1pt}{0pt}{main}, 
    borderline west = {0.75pt}{2pt}{main}, 
    borderline east = {1pt}{0pt}{main}, 
    borderline east = {0.75pt}{2pt}{main}
}


\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{{\rm Var}}
\newcommand{\Tr}{{\rm Tr}}
\newcommand{\sgn}{{\rm sgn}}




\begin{document}
%\[ \fbox{$\Box$} \fbox{$\hat\Box$} \fbox{$\tilde\Box$} \]

\title{Parameter Symmetry Breaking and Restoration Determines the Hierarchical Learning in AI Systems}
\author{Liu Ziyin$^{1,3}$, Yizhou Xu$^2$, Tomaso Poggio$^1$, Isaac Chuang$^1$\\
$^1$\textit{Massachusetts Institute of Technology}\\
$^2$\textit{École Polytechnique Fédérale de Lausanne}\\
$^3$\textit{NTT Research}
}
\maketitle


\begin{abstract}

The dynamics of learning in modern large AI systems is hierarchical, often characterized by abrupt, qualitative shifts akin to phase transitions observed in physical systems. While these phenomena hold promise for uncovering the mechanisms behind neural networks and language models, existing theories remain fragmented, addressing specific cases. In this paper, we posit that parameter symmetry breaking and restoration serve as a unifying mechanism underlying these behaviors. We synthesize prior observations and show how this mechanism explains three distinct hierarchies in neural networks: learning dynamics, model complexity, and representation formation. By connecting these hierarchies, we highlight symmetry -- a cornerstone of theoretical physics  -- as a potential fundamental principle in modern AI. 
\end{abstract}

%\vspace{-6mm}
\section{Introduction}
%\vspace{-1mm}
More and more phenomena that are virtually universal in the learning process have been discovered in contemporary AI systems. These phenomena are shared by models with different architectures, trained on different datasets, and with different training techniques. The existence of these universal phenomena calls for one or a few universal explanations. However, until today, most of the phenomena are instead described by narrow theories tailored to explain each phenomenon separately -- often focusing on specific models trained on specific tasks or loss functions and in isolation from other interesting phenomena that are indispensable parts of the deep learning phenomenology. Certainly, it is desirable to have a \textit{universal perspective}, if not a \textit{universal theory}, that explains as many phenomena as possible. In the spirit of science, a universal perspective should be independent of system details such as variations in minor architecture definitions, choice of loss functions, training techniques, etc. A universal theory would give the field a simplified paradigm for thinking about and understanding AI systems and a potential design principle for a new generation of more efficient and capable models.

Learning phenomena in deep learning can be roughly categorized into three types, each capturing a different kind of hierarchy hidden in neural networks:
\vspace{1mm}
\begin{itemize}[noitemsep,topsep=0pt, parsep=0pt,partopsep=0pt, leftmargin=12pt]
    \item \textbf{Hierarchy of Learning Dynamics}: distinct \textit{temporal} regimes arise during learning -- this includes abrupt complexity jumps \citep{simon2023stepwise, jacot2021saddle, abbe2023sgd}, progressive sharpening and flattening \citep{cohen2021gradient}, and beyond-linear dynamics of training \citep{zhu2022quadratic};
    \item \textbf{Hierarchy of Model Complexity}: the \textit{functional} complexity of models adapts to the target function -- this is exhibited in simplicity biases \citep{kalimeris2019sgd}, compressive coding through the information bottleneck \citep{tishby2000information, tishby2015deep}, and the ``blessing of dimensionality" in overparameterized nets \citep{galanti2021role, Zhang_rethink};
    \item \textbf{Hierarchy of Neural Representation}: distinct \textit{spatial} structures arise in the layers of neural networks, with progressively deeper layers tending to encode increasingly abstract information -- this is evidenced in the structured representations such as neural collapse \citep{papyan2020prevalence}, hierarchical encoding of features \citep{zeiler2014visualizing}, and universal alignment of representations across models \citep{huh2024platonic}.
\end{itemize}
\vspace{-1mm}


\begin{figure*}
    \centering
    \vspace{-2mm}
    \includegraphics[width=0.95\linewidth]{plot/symmetry_main.png}
    \vspace{-1em}
    \caption{\small The division of solution space into hierarchies given by distinct parameter symmetries. \textbf{Left}: Example solution space of a model with parameter symmetries can be divided into hierarchies with boundaries prescribed by symmetry-breaking conditions. The more symmetry there is, the more restricted the hypothesis space becomes. \textbf{Middle}: The temporal (learning dynamics) and spatial (layer-wise information processing) dynamics of AI models can be characterized by the transitions between different symmetry groups. The solid line shows a symmetry restoration dynamics when the parameter transitions from a low-symmetry state to a high one either through time or through layers. The dashed lines show compositional dynamics where the model follows two symmetry breaking and then a symmetry restoration. Actual learning dynamics of neural networks may involve the model first learning by breaking symmetries before regularization effects dominate \citep{li2021happens}, restoring symmetry. Similarly, for spatial processing, neural networks are found to break symmetries in early layers and restore symmetries in final layers (Section~\ref{sec: representation}). \textbf{Right}: The more symmetry a model has, the more spatial, temporal, and functional hierarchies it has. Changes in symmetries can induce transitions between these hierarchies.}
    \label{fig:main illustration}
%\vspace{-1em}
\end{figure*}

In this paper, we posit that
these three seemingly unrelated hierarchies of deep learning phenomena 
can be collectively understood as arising from a single concept regarding the behavior of model parameter symmetries during training: 
%The central hypothesis of this paper can be stated as the following:
\begin{boxC}
\paragraph{Main Hypothesis:} The hierarchies of learning dynamics, model complexity, and representation formation in AI systems are primarily determined by parameter symmetry breaking and restoration.
\end{boxC}
%\vspace{-2mm}
Prior art has certainly hinted at this idea, but here we draw together many separate threads to complete a single picture, as illustrated in Figure~\ref{fig:main illustration}.  The main idea is that parameter symmetry (defined in Section~\ref{sec:psymdef}) divides the solution space into hierarchies, such that three key effects result:
\begin{itemize}[noitemsep,topsep=0pt, parsep=0pt,partopsep=0pt, leftmargin=26pt]
%\vspace{-1mm}
    \item \textit{Temporal} jumps follow transitions between symmetry groups (Section~\ref{sec: dynamics});
    \item \textit{Functional} constraints arise adaptively along boundaries of symmetry (Section~\ref{sec: complexity});
    \item \textit{Spatial} hierarchy of abstract representations arise due to layered symmetries (Section~\ref{sec: representation}).
%\vspace{-1mm}
\end{itemize}
Given parameter symmetry as a unifying mechanism for deep learning, our understanding of the principles of AI can advance with explicit identification and engineering of symmetries in models, loss functions, and data. With the recent advances in how arbitrary parameter symmetries may be deliberately introduced or removed (Section~\ref{sec: mechanisms}), it is now possible to design symmetries matching practitioner intentions. Moreover, given widespread beliefs that hidden hierarchy phenomena of the kinds described above are beneficial, it follows one should be able to design models with intentional symmetries to introduce hierarchies into learning dynamics, the learned function complexities, and the layered structure of neural networks.

The next section introduces parameter symmetry. We then discuss existing supports for the three specific hypotheses given above. Section~\ref{sec: mechanisms} discusses mechanisms that cause and methods for controlling symmetry breaking and restoration. The last section discusses alternative views. Formal theorems, experimental details, and additional figures are presented in the appendix.





\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{c|ccc}
    \hline
        Symmetry & Model condition  & Symmetric State  & Example \\
        \hline
        translation & $f(w) = f( w + \lambda z)$ for fixed $z$ & none &  softmax, low-rank inputs \\ 
       scaling & $f(w) = f(\lambda w)$ & none & batchnorm, etc. \\ 
       rescaling   & $f(u,w) = f(\lambda u, \lambda^{-1} w)$  & $\|u\|=\|w\|$ &  ReLU neuron \\
       rotation   & $f(W) = f(RW)$ for orthogonal $R$  & low-rank solutions & self-supervised learning \\
       permutation  &  $f(u,w) = f(w,u)$ & identical neurons & fully connected layers, ensembles\\
       double rotation  &  $f(U,W) = f(UA,A^{-1}W) $ & low-rank solutions  
 & self-attention, matrix factorization\\
       sign flip & $f(w) = f(-w) $ & $w=0$ & tanh neuron\\
    \hline
    \end{tabular}
    \vspace{-0.8em}
    \caption{\small Common parameter symmetries in deep learning. We divide $\theta$ into three parts: $\theta=(w,u,v)$, where $w$ and $u$ are related to symmetry, while $v$ is symmetry-irrelevant and is omitted. Note that these symmetries are not mutually exclusive. For example, double rotation or rotation symmetry implies permutation symmetry and sign flip. Double rotation also implies rescaling. Some continuous groups do not have a discrete subgroup, such as the scaling and translation symmetry, which is also included for completeness. However, they still interact with regularizations in an interesting way: If there is a weight decay, the global minima are achieved at zero, which is ill-behaved for scaling symmetry but not for translation. Also, note that $Z_2$ subgroups are particularly common in these symmetries.}
    \label{tab:list of symmetries}
%\vspace{-0.8em}
\end{table*}

%\vspace{-2mm}
\section{Parameter Symmetry in Deep Learning}
\label{sec:psymdef}
%\vspace{-1mm}

\begin{definition}
    Let $G$ be a linear representation of a group. We say that there is a $G$-parameter symmetry in the model $f(\theta,x)$ if $\forall g\in G$ and $\forall x$, $f(\theta, x) = f(g\theta, x)$.
\end{definition}
\begin{definition}\label{def: symmetry breaking}
    Let $P_G= \frac{1}{|G|}\sum_{g\in G}g $. The model parameter $\theta$ is said to be in a $G$-symmetric state if $P_G \theta = \theta$. Otherwise, $\theta$ is in the symmetry-broken state.
\vspace{-1mm}
\end{definition}

$P_G$ is the projection matrix to the symmetry-invariant subspace of $G$ \citep{feit1982representation}. Note that if $P_G \theta = \theta$, then $g\theta = \theta$ for any $g$ because $gP_G=P_G$. A striking fact for those unfamiliar with parameter symmetries is that many such parameter symmetries exist in widely used loss functions and neural networks (see Table~\ref{tab:list of symmetries}). Understanding the implications of such symmetries has been an active area of research \citep{simsek2021geometry, entezari2021role, Dinh_SharpMinima, saxe2013exact, neyshabur2014search, tibshirani2021equivalences, ioffe2015batch,zhao2023improving,godfrey2022symmetries}, and this has been challenging since the number of groups induced by these symmetries often grows exponentially in the size of the model. Many prior works have explored the effect of parameter symmetry on constraining the model complexity or learning dynamics \citep{li2016symmetry, saxe2013exact, du2018algorithmic, hidenori2021noether, zhao2022symmetry, marcotte2023abide,li2020reconciling}. Because parameter symmetry naturally means that part of the solution space is redundant, having parameter symmetry can be seen as a form of ``overparameterization."

\vspace{-2mm}
\paragraph{Example:} Consider the parameter symmetries in the self-attention logit of a transformer, which can be written as (using notation from \citet{vaswani2017attention})
\vspace{-1mm}
\begin{equation}
    a_{ij}(W_Q, W_K) = X_i^T W_Q W_K X_j. 
    \vspace{-1mm}
\end{equation}
It has a double rotation symmetry: for any invertible $M$, $a_{ij}(W_Q M, M^{-1}W_K) =a_{ij}(W_Q, W_K)$, which implies that the loss function is also invariant to this transformation. This, in turn, implies that the symmetric states are those with low-rank $W_Q$ and $W_K$. Note that $a_{ij}$ can also be written as 
\begin{equation}
%\vspace{-2mm}
    a_{ij}(W_Q, W_K) = X_i^T \sum_{l} W_Q^l (W_K^l)^T X_j,
\end{equation}
where $W_Q^l$ is the $l$-th column of $W_Q$ and $W_K^l$ is the $l$-th row of $W_K$. This means that it also has the permutation symmetry such that for any $l$ and $l'$, $a_{ij}(W_Q^l, W_K^l, W_Q^{l'}, W_K^{l'}) = a_{ij}(W_Q^{l'}, W_K^{l'}, W_Q^{l}, W_K^{l})$. An alternative way to see this is that permutation symmetry is a subgroup of the double rotation symmetry (because permutation matrices are invertible). For each $l$, there is also the sign flip symmetry, $a_{ij}(W_Q^l, W_K^l) = a_{ij}(-W_Q^l, -W_K^l)$, and the rescaling symmetry, $a_{ij}(\lambda W_Q^l, \lambda^{-1}W_K^l)$. Note that if $X$ is low-rank in the $z$-direction, these matrices also have a translation symmetry, where $a_{ij}(W_K^l + \lambda z)= a_{ij}(W_K^l)$ for any $l$ and $\lambda \in \mathbb{R}$. Therefore, we have seen that even for a single self-attention layer, there are many symmetries. %\footnote{The permutation symmetry in a width-$n$ layer implies the symmetric group $S_n$, and the number of subgroups of $S_n$ is believed to grow as $e^{n^2}$ \citep{holt2010enumerating}.} 
Note that these symmetries are independent of the data distribution and apply to every pair of $X_i$ and $X_j$ -- this is the main reason these symmetries could significantly affect training neural networks. Recent works have shown that double rotation symmetry causes the self-attention layers to have a low-rank bias \citep{kobayashi2024weight}. 



%\clearpage
%\vspace{-2mm}
\section{Learning Dynamics is Symmetry-to- Symmetry}\label{sec: dynamics}
%\vspace{-1mm}


Symmetry has a direct influence on the loss landscape, and it thus affects the learning dynamics of neural networks through its effect on the landscape \citep{tatro2020optimizing, lim2024empirical}. One primary effect of symmetry on the loss landscape is that it creates extended saddle points from which SGD or GD cannot escape \citep{li2016symmetry, chen2023stochastic}. Meanwhile, it has been found that when a neural network is initialized with small norm weights, its learning dynamics is primarily saddle-to-saddle \citep{jacot2021saddle}. In fact, neural networks have been found to converge often to saddle points~\citep{alain2019negative, ziyin2023probabilistic}. Given that symmetries are the primary origins of saddle points, it is natural to hypothesize that the learning dynamics of neural networks are not only saddle-to-saddle but symmetry-to-symmetry: 
\begin{boxC}
\paragraph{Dynamics Hypothesis:} The learning dynamics of neural networks are dominated by jumps between symmetry groups, with parameters going from a larger to a smaller group (symmetry breaking) or from a smaller to a larger group (restoration).
\end{boxC}



%\vspace{-2mm}
As Theorem~\ref{theo: complexity} in the next section shows, at a $G$-symmetric solution, the number of effective model parameters is reduced by a number that matches the rank of the group. This means the change between symmetries naturally induces a change in model complexities. Therefore, these symmetry-to-symmetry jumps not only in terms of the loss function value but also in terms of complexity jumps.




Definition~\ref{def: symmetry breaking} can quantify the breaking and restoration of symmetry. %if we know the action of the symmetry on the parameters. 
We define the symmetry-breaking distance as:\vspace{-1mm}
\begin{equation}
%\vspace{-2mm}
    \Delta^G = \left \|\theta - P_G \theta \right\|_2^2 .
    %\vspace{-1mm}
\end{equation}
When $\Delta^G > \Delta^G_{\rm th}$ for some threshold $\Delta^G_{\rm th}$ ($\Delta^G_{\rm th} = 0.05\sim 0.2$ in experiments), we say the $G$-symmetry is broken. We care about how many symmetries are broken for a given layer, so we can count the number of such large $\Delta^G$. For example, for permutation symmetry in a fully connected layer, the group-invariant projection is the average of the input and output weights of any pair of neurons $i$ and $j$. One can identify each pairwise distance as a different $\Delta^G$, which we will denote as $\Delta_{ij}$ throughout this work. We only need to count the number of neighboring neurons with a large $\Delta^G$ because they form a generating set of the symmetric group -- we refer to this number as the \textit{degree of symmetry} $N_{\rm dos}$. The difference between $N_{\rm dos}$ and the number of neurons is the \textit{degree of symmetry breaking} ($N_{\rm dosb}$). See Section~\ref{app sec: measurement} for details on measuring this quantity.

\begin{figure}
    \centering
    \vspace{-1mm}
    %\includegraphics[width=0.38\linewidth]{plot/distances.png}
    \includegraphics[width=0.38\linewidth]{plot/loss.png}
    \vspace{-1.2em}
    \caption{\small DNN learning dynamics is symmetry-to-symmetry. Recent works suggested the learning of neural networks is primarily saddle-to-saddle \citep{jacot2021saddle}, and escaping these saddle points coincides with a sudden change in the complexity of the network \citep{abbe2023sgd}. At the same time, symmetries have been found to be the primary causes of the saddle points \citep{li2016symmetry, ziyin2024symmetry}. Once the symmetry is removed, saddle points seem to have disappeared when interpolating different solutions of the model \citep{lim2024empirical}. The figure shows that the loss jumps when symmetry breaking happens (black dotted lines) and plateaus when there is no symmetry breaking.}
    %\vspace{-1em}
    \label{fig:symmetry breaking dynamics}
\end{figure}


Figure~\ref{fig:symmetry breaking dynamics} shows an example where we train an MLP in a teacher-student setting using SGD. We initialize the model at a small norm, so the model's initial state is approximately in the symmetric state, where the average pairwise distance is of order $10^{-4}$. The black dashed lines show when a pairwise distance exceeds $\Delta^{G}_{\rm th}$, signaling a symmetry breaking. We see that the symmetry-breaking times coincide well with the periods where the network learns rapidly. This shows that the saddle-to-saddle learning dynamics is potentially caused by the symmetry-to-symmetry transitions during training. Examples with polynomial and sinusoidal nets are shown in Section~\ref{app sec: exp detail}.


%\subsection{Continuous Symmetry and Conservation Law}

\vspace{-2mm}
\paragraph{Decomposability into Quadratic Models} Another interesting point worth raising about symmetries is that if the model is close to a symmetric state $\theta^*$, then it can be decomposed into a sum of a quadratic model and a strictly smaller model. \citet{ziyin2024symmetry} showed that for a small $\Delta^G$,
 \vspace{-1mm}
\begin{equation}
    f(\theta,x) = f(\theta^*) + \theta_G^T \nabla^2 
 f(\theta^*,x)  \theta_G + o(\Delta^G),
\vspace{-1mm}
\end{equation}
where $\theta^* = P_G \theta$ is essentially a parameter projected to a $d-{\rm rank}(P_G)$ subspace, and $\theta_G = (I-P_G)\theta$ are the parameters projected to the symmetry breaking direction. Therefore, the second term can be seen as a quadratic model trained on a data transformation kernel of the form $ \nabla^2 f(x)$. This means that close to symmetry states, the learning dynamics must be ``diligent" and cannot be characterized by NTK \citep{jacot2018neural} or lazy training \citep{chizat2018lazy}. This implies that parameter symmetry also plays a significant role in feature learning \citep{yang2020feature}. In contrast, if a model has an antisymmetry, its expansion will only have odd order terms, and one may expect that such a model primarily behaves like a linear model.


\begin{figure*}[t!]
    \centering
    \vspace{-1mm}
    \hfill
    \includegraphics[width=0.26\linewidth]{plot/ViT_self_attention.png}
    \
   % \includegraphics[width=0.3\linewidth]{plot/units_scatter.png}
    \includegraphics[width=0.35\linewidth]{plot/rank.png}
    %\includegraphics[width=0.37\linewidth]{plot/generalization.png}
    \hfill
    \includegraphics[width=0.23\linewidth]{plot/ball_covering.png}
    \hfill
    \vspace{-1em}
    \caption{\small The complexity and generalization error of neural networks do not grow with width. A well-known observation in deep learning is that overparameterized networks not only work well, but also their generalization errors are empirically found to be essentially independent of width \citep{li2018neural, pinto2024generalization, galanti2023norm, mingard2025deep}, an observation at odds with conventional bounds based on the Rademacher complexity \citep{Zhang_rethink}. The existence of parameter symmetries may solve this problem because, with a fixed regularization, the maximum surviving neurons are upper bounded by a constant. The \textbf{left} figure shows that Imagenet-pretrained ViT-Base (80M) and ViT-Large (300M) have similar degrees of symmetry in their self-attention layers. Here, each dot is a self-attention layer. The \textbf{middle} figure shows that the weight rank does not grow with increasing model size. A mechanism for this is that permutation symmetry implies that neuron weights of distance $o(\gamma)$ to each other must collapse -- this means that within a fixed $n$-sphere, there can be at most $1/\gamma^n$ different neurons. This filling procedure is illustrated in the \textbf{right} figure: The orange circle denotes the parameter space, and the little circles are the neuron weights.}
    \label{fig:complexity upper bound}
    %\vspace{-1em}
\end{figure*}
%\clearpage
%\vspace{-2mm}
\section{Symmetry Adaptively Limits Model Complexity}\label{sec: complexity}
%\vspace{-1mm}

Another important implication of symmetries is that they control the effective number of parameters. When a system is symmetry-broken, new degrees of freedom usually emerge (known as a Goldstone mode in physics \citep{peskin2018introduction}), and the system becomes higher-dimensional and more complex. When it is symmetry-restored, the effective dimension is reduced. This means that the symmetry boundaries naturally correspond to boundaries of different hierarchies of model capacities. This idea has a place in machine learning. For example, equivariant networks can improve the sample efficiency of training, and existing equivariant networks almost always involve introducing new parameter symmetry to the neural network \citep{maron2018invariant, bronstein2021geometric}. In Bayesian learning, parameter symmetry (and its generalizations) have been found to directly determine the generalization scaling of the model \citep{watanabe2010asymptotic}. Prior works also indicate how neural networks may break symmetries to adapt to different tasks \citep{fok2017spontaneous}.

More broadly, combining the idea that symmetry classes have different model complexity and the common observation that SGD tends to learn a function whose complexity is proportional to the complexity of the target function \citep{kalimeris2019sgd, mingard2025deep}, it is natural to arrive at the following hypothesis:
\begin{boxC}
\paragraph{Complexity Hypothesis:}  Symmetry adaptively controls the model's capacity. The model converges to a symmetry class whose complexity matches the complexity of the target.
\end{boxC}
%\vspace{-3mm}
The direct correspondence between symmetry and model capacity has been justified by a recent result: at a $G$-symmetric state, the effective model dimension decreases by exactly ${\rm rank}(P_G)$ throughout training. Moreover, in the NTK limit, this reduction in parameter dimension implies a selection of input features, and being at a symmetric solution directly affects the input-output function map.
\vspace{-0mm}
\begin{theorem}\label{theo: complexity}
    (Informal, \citep{ziyin2024remove}) If the loss function has $G$-symmetry, and the initial $\theta\in \mathbb{R}^d$ is $G$-symmetric, then (1) there exists a model with $d-{\rm rank}(P_G)$ parameters whose learning dynamics is the same as $\theta$, and (2), in the lazy training regime, this is equivalent to applying a rank $d-{\rm rank}(P_G)$ mask to the NTK features.
\end{theorem}
Thus, symmetric solutions are low-capacity states from which gradient-based training methods cannot escape. More importantly, these symmetric solutions are preferred solutions when weight decay is used \citep{ziyin2024symmetry} or if the minibatch noise is strong due to a mechanism called ``stochastic collapse" \citep{chen2023stochastic}. 



\vspace{-2mm}
\paragraph{Parameter Symmetry as an Occam's Razor} In other words, parameter symmetries in the model may function like an Occam's razor, where simpler and low-complexity solutions are favored -- an observation that has been made across almost all modern neural models, whose generalization performances are independent of the size of the model \citep{Zhang_rethink, galanti2023norm}, which is in discrepancy with common generalization bounds that predict a $\sqrt{\psi}$ deterioration with respect to the width $\psi$ \citep{neyshabur2018towards}. One can test this hypothesis simply by training an MLP and a small transformer on different randomly generated teacher networks -- see Section~\ref{app sec: adaptive capacity}. For different tasks, the same network converges to solutions with different symmetry classes corresponding to different levels of complexity. Similarly, for a fixed task, networks with different levels of capacity converge to similar degrees of symmetry. See Figure~\ref{fig:complexity upper bound}-Left for vision transformers (ViT) \citep{dosovitskiy2020image} trained on Imagenet.


 
Here, we raise an insightful conjecture about the quantification of complexity control due to permutation symmetry, where $\gamma$ is the weight decay,  $\eta/S$ is the learning-rate-batch-size ratio. Recall that the distance between two neurons is $\Delta_{ij}$. 
\begin{conjecture}
(Space Quantization Conjecture, Informal) In every layer with permutation symmetry, for two nonidentical neurons $i$ and $j$, $\Delta_{ij} > O(\kappa^{\beta})$ after training, where $\beta >0$ and $\kappa$ is the regularization strength.

\label{conjecture}
\vspace{-1mm}
\end{conjecture}
We prove a special case in Section~\ref{app sec: space quantization conjecture}. A primary mechanism for this absolute upper limit is a combination of regularization and symmetry (Section~\ref{sec: mechanisms}): (1) using weight decay $\gamma$, the model parameters must lie within a ball of radius $O(1/\gamma)$ to the origin; (2) with symmetry any neuron whose weights are with a distance of $\gamma$ to each other must become identical. This means that the number of surviving neurons is upper bounded by the number of $n-$hyperspheres of radius $O(\gamma)$ that one can fit in a hypersphere of radius $O(1/\gamma)$; this number can be upper bounded by $O(\gamma^{-n})$. This exponential index can be improved to be independent of $n$ and so does not suffer the curse of dimensionality (Section~\ref{app sec: space quantization conjecture}). 
From a physics picture, the neurons start to form a ``lattice" in the parameter space with the lattice distance $\gamma$.


Thus, when regularized, there are, at most, a finite number of nonidentical neurons within a layer, however wide it is. This implies that the actual complexity of the trained network must decrease as a function polynomial in the regularization strength. %Note that this conjecture is quite strong in the sense that the scaling is independent of the dimension $n$ of these weights. 
See Figure~\ref{fig:complexity upper bound}-Middle, where we train a two-hidden-layer network with varying widths. When weight decay or stochastic training is used, we see that the dimension of the latent representation saturates at a large model size. This could imply that the actual model complexity is much smaller than its apparent dimension and may be a key mechanism for understanding why highly overparametrized networks can still generalize. %If this conjecture is true, it immediately implies that one can prove generalization bounds that are asymptotically independent of width, either implicitly or explicitly. 
For example, ~\citet{bartlett2019nearly} showed that the VC dimension of a two-layer piecewise linear network is $O(\psi)$. Our result implies that the nonidentical number of neurons must be $O(1)$ after training. This, in turn, means that the VC dimension of the model is independent of width.




%\vspace{-2mm}
\section{Representation Learning Requires Parameter Symmetry}\label{sec: representation}
%\vspace{-1mm}
Representation learning is believed to be the most essential aspect of deep learning \citep{bengio2013representation}. 
Learned representations of neural networks are found to take almost universally hierarchical forms, where earlier layers encode a large variety of low-level features and later layers learn a composed and abstract representation that is invariant to the changes in the low-level details \citep{zeiler2014visualizing}. One reason why symmetry may serve as a driving mechanism for learning these structured representations is that these structures almost always involve compressing information onto a few neurons and come with a low-rank structure in the hidden layer \citep{alain2016understanding, masarczyk2024tunnel, xu2023janus, papyan2020prevalence}, and a primary low-rank mechanism for deep neural networks is through the permutation symmetry of the layer or the rescaling symmetry of ReLU neurons \citep{fukumizu1996regularity, fukumizu2000local, ziyin2024symmetry}. As a primary example, neural collapses (NC) often happen in an image classification task, where the inner-class variations are found to disappear in the last and intermediate layers of the neural network \citep{papyan2020prevalence}, resulting in a representation whose rank matches the number of classes. This implies that the network has learned a hierarchical representation, where, in the later layers, only high-level features are encoded. These results motivate the following hypothesis:
\begin{boxC}
\paragraph{Representation Hypothesis:} Learning invariant, hierarchical and universal latent representations requires parameter symmetry. %With symmetry, the learned representations are universal across different models and hierarchically invariant to low-level irrelevant features.
\end{boxC}
\begin{figure}[t!]
\vspace{-0.5em}
    \centering
    \includegraphics[width=0.3\linewidth]{plot/unbiased_cov.png}
    \includegraphics[width=0.3\linewidth]{plot/biased_cov.png}
    \vspace{-1em}
    \caption{\small Neural collapse (NC) only happens when permutation symmetry is present. NC is a primary example of how invariant high-level representations emerge in neural networks \citep{papyan2020prevalence} and exist quite generally in classification, regression, and large language models \citep{andriopoulos2024prevalence, wu2024linguistic}. When NC happens, the learned representation must be low-rank; however, \citet{ziyin2024remove} showed that if the permutation symmetries are removed, the learned representation is always full-rank. This implies that permutation symmetry is a necessary condition for NC to happen. The figures show the representation alignment of $100$ CIFAR10 images across $10$ classes ($10$ images in each class). The color represents the correlation between representations. \textbf{Left}: The vanilla model exhibits neural collapse, where all neurons are similar for the same class. \textbf{Right}: The innerclass variation becomes significant when the permutation symmetry is removed.} %Also, the vanilla model exhibits a clear low-rank structure, which is not present in the symmetry-removed model.}
    \label{fig:neural collapse}
    %\vspace{-1em}
\end{figure}
\vspace{-4mm}
\paragraph{Invariant and Hierarchical Representation Learning} See Figure~\ref{fig:neural collapse}. Here, we train a standard ResNet18 on CIFAR-10, which is known to exhibit NC. In comparison, we also train a ResNet18 whose symmetries have all been removed using the method proposed in \citep{ziyin2024remove}. We see that after removing permutation symmetries, the innerclass variation of representations no longer vanishes. In Section~\ref{app sec: neural collapse}, we show that the spectral gap between class means and innerclass variations also becomes smaller.


More broadly, it is important to understand how representations build up as the input data passes through layers of a neural network. It seems likely that there are at least three distinctive regimes in the layers of a trained net: the first few layers of neural networks serve as an \textit{expansion} phase where the representation becomes linearly separable \citep{alain2016understanding}, which requires the layer to be wide and implies a high rank \citep{nguyen2018neural}; then, a ``reduction" phase happens where the irrelevant information is thrown away and the neurons encode more and more compact information \citep{xu2023janus, rangamani2023feature}; lastly, a ``transmission" phase where the layers do nothing except transmitting the signal it receives \citep{masarczyk2024tunnel}. The NC can be seen as an example of the transmission and the simplest case of such a hierarchical representation \citep{ziyin2024formation}. These results all suggest that the representation rank is a good metric of the complexity of the layer. 
As Figure~\ref{fig:hierachy} shows, these changes in the representation ranks match well the symmetry-breaking level of the layer, a strong evidence that symmetry may drive the formation of hierarchical representations.


\begin{figure}[t!]
    \centering
    \vspace{-0.5em}
    \includegraphics[width=0.45\linewidth]{plot/FCN_final.png}
    % \includegraphics[width=0.47\linewidth]{plot/FCN.png}
    % \includegraphics[width=0.51\linewidth]{plot/FCN_scatter.png}
    \vspace{-1em}
    \caption{\small Neural networks learn a hierarchical representation. Recent works on representation learning have suggested that the rank of the latent representation first increases and then decreases through the layers \citep{xu2023janus, masarczyk2024tunnel}. This is reasonable because, on the one hand, a network needs to be wide enough to learn disconnected decision regions \citep{nguyen2018neural}, while permutation symmetries drive towards low-rankness \citep{ziyin2024symmetry}. This figure shows the rank and degree of symmetry breaking, which can be seen as the simplest metrics of the representation complexity, of different layers in a 5-layer FCN trained on CIFAR-10. This experiment shows hierarchical representations may be due to symmetry changes: the beginning layers feature symmetry breaking, and later layers are primarily symmetry restoration.}
    \label{fig:hierachy}
    %\vspace{-1em}
\end{figure}

\vspace{-2mm}
\paragraph{Universal Representation Learning} 
The neural collapse is also a special case of what we will call ``universal representation learning." Recent works found that the representations of learned models are found to be universally aligned to different models trained on similar datasets \citep{bansal2021revisiting, kornblith2019similarity}, and to the biological brains \citep{yamins2014performance}. This interesting phenomenon has a rather philosophical undertone and has been termed ``Platonic Representation Hypothesis" \citep{huh2024platonic}. Here, we say that the two neural networks have learned a universal representation if for all $x_1,\ x_2$, 
\begin{equation}\label{eq: universal alignment}
    h_A(x_1)^T h_A(x_2) = h_B(x_1)^T h_B(x_2),
\end{equation}
where $h_A$ is the normalized activation of network $A$ in one of the hidden layers, and  $h_B$ for network B. This is an idealization of what people have observed -- and the difference between the two sides is the ``degree of alignment". The NC is a special case because when NC happens, the representations form a simplex tight frame, and so two networks with NC must have an aligned representation.

 
Recent results have suggested that symmetry may play a key role in shaping the representation of neural networks. The following theorem can be proved by leveraging Theorem 5.4 of \citet{ziyin2024parameter}, which shows that the double rotation symmetry in deep linear networks leads to the emergence of universal representations in different models, even when data and models are different.

\begin{theorem}\label{theo: alignment}
    (Informal) Let $\mathcal{D}_M = \{(Mx_i, y_i)\}_i$ be the training set, where $M$ is an invertible matrix. Let deep linear networks A and B have arbitrary depths and widths. Let model A train with SGD on $\mathcal{D}_M$ and model B with SGD on $\mathcal{D}_{M'}$ for arbitrary $M$, $M'$. Then, if the model converges and is at the global minimum, every layer $h_A$ of A is \textbf{perfectly} aligned with every layer $h_B$ of B for any $x$:\vspace{-2mm}
    \begin{equation}
        h_A(x)  \propto R h_B(x),
        \vspace{-2mm}
    \end{equation}
    where $R$ is an orthogonal matrix.
\vspace{-1mm}
\end{theorem}
See Figure~\ref{fig:representation alignment}. This is an extraordinary fact: due to the double rotation symmetry, there exist infinitely many global minima for a deep linear network such that the representations are not aligned. Yet, SGD training finds a special solution such that every layer of $A$ is aligned with every layer of $B$ despite having different initializations, different data transformations (controlled by the arbitrary matrix $M$), and different architectures (width and depth). This is only possible if the first layer transforms the representation into an input-independent form. See Figure~\ref{fig:representation alignment} for a demonstration. This theorem is a direct (perhaps the first) proof of the platonic representation hypothesis, implying that for any $x_1, x_2$, Eq.~\eqref{eq: universal alignment} holds. Importantly, the mechanism does not belong to any previously conjectured mechanisms (capacity, simplicity, multitasking \citep{huh2024platonic}). This example has nothing to do with multitasking. The result holds for any deep linear network, all having the same capacity and the same level of simplicity because all solutions parametrize the same input-output map. Here, the cause of the universal representation is symmetry alone: in the degenerate manifold of solutions, the training algorithm prefers a particular and universal one. This example showcases how symmetry is indeed an overlooked fundamental mechanism in deep learning.

%Certainly, our discussion only serves as proof of concept. 
An important future step is to generalize it to nonlinear networks. 
Due to extensive connectivity among the global minima of overparametrized networks \citep{nguyen2019connected}, there is a great potential that parameter symmetry (in exact or approximate forms) plays a similar role in overparameterized neural networks \citep{zhao2022symmetry, zhao2023understanding}. 






\begin{figure}[t!]
    \centering
    \vspace{-1mm}
    \includegraphics[width=0.45\linewidth]{plot/mnist_alignment_ver2.png}
    %\includegraphics[width=0.4\linewidth]{plot/mnist_alignment_tanh.png}
    \vspace{-1em}
    \caption{\small Universally aligned representations emerge in differently trained neural networks. Many recent works demonstrate that different trained neural networks learn representations that are similar to each other and even to the biological brain \citep{huh2024platonic, kornblith2019similarity, yamins2014performance}. Parameter symmetry may be a core mechanism for this intriguing phenomenon as it implies a universal alignment between representations \citep{ziyin2024parameter}. The figure shows the representations of two deep linear networks independently trained on randomly transformed MNIST become perfectly aligned for \textbf{every} pair of layers. The figure shows the average alignment between the same or different layers of two networks. This alignment does not weaken even if the input is arbitrarily transformed (Theorem~\ref{theo: alignment}). The black dashed line shows the average alignment to the input data, which is significantly weaker.}
    \label{fig:representation alignment}
    %\vspace{-1em}
\end{figure}


%\vspace{-2mm}
\section{Mechanism and Control}\label{sec: mechanisms}
%\vspace{-1mm}

%\vspace{-1mm}
\paragraph{Mechanism:} A primary known mechanism is regularization in explicit or implicit forms. Since symmetry breaking is just the lack of symmetry restoration, we may focus on symmetry restoration. In explicit form, a simple weight decay has been shown to universally turn symmetric solutions energetically favorable to symmetry-broken solutions \citep{ziyin2024symmetry}, in a manner similar to phase transition in physics \citep{landau2013statistical}. In implicit form, the stochasticity in SGD is known to lead to an ``implicit" regularization effect, sometimes similar to that of weight decay \citep{kunin2021rethinking, chen2023stochastic}. Another mechanism is data augmentation, as it can also be seen as a form of regularization \citep{dao2019kernel}. While understanding these mechanisms is an open problem, the easiest way to study it is to consider the effective loss landscape (also known as the ``modified loss") under SGD training \citep{geiping2021stochastic, smith2021origin}:%\vspace{-1mm}
\begin{equation}
    L(\theta) = \underbrace{\E_\epsilon [L_0 (\theta, x_\epsilon)]}_{\text{learning + data aug.}} + \underbrace{\gamma \|\theta\|^2}_{\text{weight decay}} + \underbrace{T\, {\rm Tr}[\Sigma(\theta)]}_{\text{training noise}},
        %\vspace{-2mm}
\end{equation}
where $L_0$ has the symmetry under consideration, $\Sigma$ is the gradient covariance matrix due to SGD sampling, and $T=\eta/S$ is the learning-rate-to-batch-size ratio. The data augmentation term can also be controlled by, say, a variance term $\sigma$. A careful inspection suggests that none of these terms breaks the $Z_2$ symmetries of $L_0$. So all $\sigma,\ \gamma,\ T$ can serve as regularizers that can induce a ``spontaneous symmetry breaking," which means that the symmetric states are energetically favored solutions at a critical regularization strength \citep{ziyin2024symmetry}. We experimentally demonstrate these effects in Section~\ref{app sec: mechanism}.  

With explicit regularization, whether symmetry breaking or restoration happens depends on the competition between regularization $\gamma$ and the data signal. The more curious situation is when $\gamma =0$ and $T>0$. This type of symmetry breaking is more like a thermodynamic phase transition in physics as it is induced by an entropic force due to the SGD noise. Why do noisy dynamics favor symmetric states? One intuition is that the ``temperature" of the dynamics is parameter-dependent, and stochastic dynamics tend to move to places that are ``cold," a common phenomenon in nature \citep{duhr2006molecules, anzini2019thermal}. \footnote{So, implicit regularization effects can be viewed as a thermal current obeying laws of nonequilibrium physics \citep{seifert2008stochastic}.} 
In other words, temperature gradient tends to create a flow. In the case of SGD dynamics on a neural network, the symmetric solutions are `` colder" because they have a lower-rank $\Sigma$ than the symmetry-broken solutions. This discussion offers a likely intuition for the mechanism, but a more formal treatment of the argument is still lacking. Different training techniques likely favor different types of symmetries, and one may leverage symmetry to understand the inductive biases of deep learning techniques.  

\vspace{-3mm}
\paragraph{Control:} If hierarchies and complexity adaptivity are desired, one can introduce artificial symmetries to the neural network. A straightforward way to introduce symmetries is by introducing an additional parameter $v$ and multiplying it with the parameters $W$ for which a hierarchy is desired. For example, changing a layer weight from $W$ to $vW$ introduces a new symmetry without affecting the model expressivity: $|v|=\|W\|=0$ is the symmetric state, and the model will naturally adapt to break or restore this symmetry. This is a special case of a class of algorithms that introduce artificial symmetries to constrain the model parameters. (for example, see the DCS algorithm in \cite{ziyin2024symmetry}; also, see \cite{kolb2025deep}). A simple way to remove symmetries is to add some static randomness to the model, which has been found to help with the model connectivity \citep{lim2024empirical} and avoid saddle points \citep{ziyin2024remove}. Finding the right symmetries to introduce or remove can be a fruitful future direction.



%\vspace{-3mm}
\section{Outlook and Alternative Views}\label{sec: conclusion}
%\vspace{-1mm}
In this work, we have argued and demonstrated that a wide range of seemingly unrelated hierarchies emergent in AI systems is actually related to, if not directly caused by, the symmetry of the trained models. Symmetry breaking and restoration are fundamental mechanisms in physics relevant to the dynamics of almost every scale of nature -- from scattering of the quarks to the large-scale structure formation of the universe \citep{miller1990charge, preskill1991cosmology}. If different symmetries govern the universal laws at different scales of nature \citep{anderson1972more}, it is natural to hypothesize that it may also lead to universal mechanisms and laws of learning in both artificial and biological systems. We suggest three primary hypothetical mechanisms that govern three fundamental aspects of hierarchical learning of neural networks, and validating or falsifying their related hypotheses is the most important next step. From a broader perspective, that symmetry can play such important roles in learning also calls for more interdisciplinary study of intelligence from the perspective of physics. 



There are certainly alternative views to our position. 

\vspace{-4mm}
\paragraph{Symmetry alone is insufficient:} %While we argue that symmetry is a major cause of these phenomena, it is not the only cause. 
In most of our examples, it is a combination of symmetry and some other effect that leads to the phenomenon. For example, the compression bias of SGD is a result of symmetry \textit{and} training noise. Neural collapse is due to symmetry \textit{and} regularization \citep{rangamani2022neural}. 
Therefore, it is possibly the case that symmetry \textit{plus} some form of explicit (weight decay) or implicit (noise, GD training, data augmentation) regularization is the dominating factor, and this may be an important direction of future research.

\vspace{-4mm}
\paragraph{Symmetry may not be necessary:} While symmetry is possibly a unified perspective to view many phenomena, there are cases where a subset of these phenomena are exhibited but does not require symmetry. For theoretical purposes, this suggests that there can be other concepts as important as symmetry. However, from a design and practice perspective, knowing only one way to achieve a design goal often suffices -- and the parameter symmetry may be sufficient. For example, there may be multiple ways for neural collapse to happen, but to introduce neural collapse to the desired model, one only needs to know one such way.

\vspace{-4mm}
\paragraph{Behind symmetry there may be more:} There are also possible more fundamental mechanisms than symmetry, which can explain what happens with symmetry and what happens without it. One such example could be the topology of the system. In fact, in theoretical physics, symmetry is often regarded as a special case of topology (for example, the rank of the Fisher information can be a good the topological invariant). This makes it possible to generalize our arguments to a wider range of systems. Some works have explored the role of topology in AI, but often in a scenario where symmetry is also present \citep{barannikov2020topological, nurisso2024topological}. Another interesting future direction is understanding what topology may achieve without requiring symmetries. 


\section*{Acknowledgment}
ILC acknowledges support in part from the Institute for Artificial Intelligence and Fundamental Interactions (IAIFI) through NSF Grant No. PHY-2019786. This work was also supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award  CCF - 1231216.


%\bibliography{ref}
%\bibliographystyle{icml2025}


\begin{thebibliography}{86}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe et~al.(2023)Abbe, Adsera, and Misiakiewicz]{abbe2023sgd}
Abbe, E., Adsera, E.~B., and Misiakiewicz, T.
\newblock Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics.
\newblock In \emph{The Thirty Sixth Annual Conference on Learning Theory}, pp.\  2552--2623. PMLR, 2023.

\bibitem[Alain(2016)]{alain2016understanding}
Alain, G.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock \emph{arXiv preprint arXiv:1610.01644}, 2016.

\bibitem[Alain et~al.(2019)Alain, Roux, and Manzagol]{alain2019negative}
Alain, G., Roux, N.~L., and Manzagol, P.-A.
\newblock Negative eigenvalues of the hessian in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.02366}, 2019.

\bibitem[Anderson(1972)]{anderson1972more}
Anderson, P.~W.
\newblock More is different: Broken symmetry and the nature of the hierarchical structure of science.
\newblock \emph{Science}, 177\penalty0 (4047):\penalty0 393--396, 1972.

\bibitem[Andriopoulos et~al.(2024)Andriopoulos, Dong, Guo, Zhao, and Ross]{andriopoulos2024prevalence}
Andriopoulos, G., Dong, Z., Guo, L., Zhao, Z., and Ross, K.
\newblock The prevalence of neural collapse in neural multivariate regression.
\newblock \emph{arXiv preprint arXiv:2409.04180}, 2024.

\bibitem[Anzini et~al.(2019)Anzini, Colombo, Filiberti, and Parola]{anzini2019thermal}
Anzini, P., Colombo, G.~M., Filiberti, Z., and Parola, A.
\newblock Thermal forces from a microscopic perspective.
\newblock \emph{Physical Review Letters}, 123\penalty0 (2):\penalty0 028002, 2019.

\bibitem[Bansal et~al.(2021)Bansal, Nakkiran, and Barak]{bansal2021revisiting}
Bansal, Y., Nakkiran, P., and Barak, B.
\newblock Revisiting model stitching to compare neural representations.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 225--236, 2021.

\bibitem[Barannikov et~al.(2020)Barannikov, Voronkova, Trofimov, Korotin, Sotnikov, and Burnaev]{barannikov2020topological}
Barannikov, S., Voronkova, D., Trofimov, I., Korotin, A., Sotnikov, G., and Burnaev, E.
\newblock Topological obstructions in neural networks learning.
\newblock \emph{arXiv preprint arXiv:2012.15834}, 2020.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and Mehrabian]{bartlett2019nearly}
Bartlett, P.~L., Harvey, N., Liaw, C., and Mehrabian, A.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0 (63):\penalty0 1--17, 2019.

\bibitem[Bengio et~al.(2013)Bengio, Courville, and Vincent]{bengio2013representation}
Bengio, Y., Courville, A., and Vincent, P.
\newblock Representation learning: A review and new perspectives.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 35\penalty0 (8):\penalty0 1798--1828, 2013.

\bibitem[Bronstein et~al.(2021)Bronstein, Bruna, Cohen, and Veli{\v{c}}kovi{\'c}]{bronstein2021geometric}
Bronstein, M.~M., Bruna, J., Cohen, T., and Veli{\v{c}}kovi{\'c}, P.
\newblock Geometric deep learning: Grids, groups, graphs, geodesics, and gauges.
\newblock \emph{arXiv preprint arXiv:2104.13478}, 2021.

\bibitem[Chen et~al.(2023)Chen, Kunin, Yamamura, and Ganguli]{chen2023stochastic}
Chen, F., Kunin, D., Yamamura, A., and Ganguli, S.
\newblock Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks.
\newblock \emph{arXiv preprint arXiv:2306.04251}, 2023.

\bibitem[Chizat et~al.(2018)Chizat, Oyallon, and Bach]{chizat2018lazy}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock \emph{arXiv preprint arXiv:1812.07956}, 2018.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and Talwalkar]{cohen2021gradient}
Cohen, J.~M., Kaur, S., Li, Y., Kolter, J.~Z., and Talwalkar, A.
\newblock Gradient descent on neural networks typically occurs at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2103.00065}, 2021.

\bibitem[Dao et~al.(2019)Dao, Gu, Ratner, Smith, De~Sa, and R{\'e}]{dao2019kernel}
Dao, T., Gu, A., Ratner, A., Smith, V., De~Sa, C., and R{\'e}, C.
\newblock A kernel theory of modern data augmentation.
\newblock In \emph{International conference on machine learning}, pp.\  1528--1537. PMLR, 2019.

\bibitem[{Dinh} et~al.(2017){Dinh}, {Pascanu}, {Bengio}, and {Bengio}]{Dinh_SharpMinima}
{Dinh}, L., {Pascanu}, R., {Bengio}, S., and {Bengio}, Y.
\newblock {Sharp Minima Can Generalize For Deep Nets}.
\newblock \emph{ArXiv e-prints}, March 2017.

\bibitem[Dosovitskiy(2020)]{dosovitskiy2020image}
Dosovitskiy, A.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du2018algorithmic}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Duhr \& Braun(2006)Duhr and Braun]{duhr2006molecules}
Duhr, S. and Braun, D.
\newblock Why molecules move along a temperature gradient.
\newblock \emph{Proceedings of the National Academy of Sciences}, 103\penalty0 (52):\penalty0 19678--19682, 2006.

\bibitem[Entezari et~al.(2021)Entezari, Sedghi, Saukh, and Neyshabur]{entezari2021role}
Entezari, R., Sedghi, H., Saukh, O., and Neyshabur, B.
\newblock The role of permutation invariance in linear mode connectivity of neural networks.
\newblock \emph{arXiv preprint arXiv:2110.06296}, 2021.

\bibitem[Feit(1982)]{feit1982representation}
Feit, W.
\newblock \emph{The representation theory of finite groups}.
\newblock Elsevier, 1982.

\bibitem[Fok et~al.(2017)Fok, An, and Wang]{fok2017spontaneous}
Fok, R., An, A., and Wang, X.
\newblock Spontaneous symmetry breaking in neural networks.
\newblock \emph{arXiv preprint arXiv:1710.06096}, 2017.

\bibitem[Fukumizu(1996)]{fukumizu1996regularity}
Fukumizu, K.
\newblock A regularity condition of the information matrix of a multilayer perceptron network.
\newblock \emph{Neural networks}, 9\penalty0 (5):\penalty0 871--879, 1996.

\bibitem[Fukumizu \& Amari(2000)Fukumizu and Amari]{fukumizu2000local}
Fukumizu, K. and Amari, S.-i.
\newblock Local minima and plateaus in hierarchical structures of multilayer perceptrons.
\newblock \emph{Neural networks}, 13\penalty0 (3):\penalty0 317--327, 2000.

\bibitem[Galanti et~al.(2021)Galanti, Gy{\"o}rgy, and Hutter]{galanti2021role}
Galanti, T., Gy{\"o}rgy, A., and Hutter, M.
\newblock On the role of neural collapse in transfer learning.
\newblock \emph{arXiv preprint arXiv:2112.15121}, 2021.

\bibitem[Galanti et~al.(2023)Galanti, Xu, Galanti, and Poggio]{galanti2023norm}
Galanti, T., Xu, M., Galanti, L., and Poggio, T.
\newblock Norm-based generalization bounds for compositionally sparse neural networks.
\newblock \emph{arXiv preprint arXiv:2301.12033}, 2023.

\bibitem[Geiping et~al.(2021)Geiping, Goldblum, Pope, Moeller, and Goldstein]{geiping2021stochastic}
Geiping, J., Goldblum, M., Pope, P.~E., Moeller, M., and Goldstein, T.
\newblock Stochastic training is not necessary for generalization.
\newblock \emph{arXiv preprint arXiv:2109.14119}, 2021.

\bibitem[Godfrey et~al.(2022)Godfrey, Brown, Emerson, and Kvinge]{godfrey2022symmetries}
Godfrey, C., Brown, D., Emerson, T., and Kvinge, H.
\newblock On the symmetries of deep learning models and their internal representations.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 11893--11905, 2022.

\bibitem[Huh et~al.(2024)Huh, Cheung, Wang, and Isola]{huh2024platonic}
Huh, M., Cheung, B., Wang, T., and Isola, P.
\newblock The platonic representation hypothesis.
\newblock \emph{arXiv preprint arXiv:2405.07987}, 2024.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock \emph{arXiv preprint arXiv:1806.07572}, 2018.

\bibitem[Jacot et~al.(2021)Jacot, Ged, {\c{S}}im{\c{s}}ek, Hongler, and Gabriel]{jacot2021saddle}
Jacot, A., Ged, F., {\c{S}}im{\c{s}}ek, B., Hongler, C., and Gabriel, F.
\newblock Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity.
\newblock \emph{arXiv preprint arXiv:2106.15933}, 2021.

\bibitem[Kalimeris et~al.(2019)Kalimeris, Kaplun, Nakkiran, Edelman, Yang, Barak, and Zhang]{kalimeris2019sgd}
Kalimeris, D., Kaplun, G., Nakkiran, P., Edelman, B., Yang, T., Barak, B., and Zhang, H.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Kobayashi et~al.(2024)Kobayashi, Akram, and Von~Oswald]{kobayashi2024weight}
Kobayashi, S., Akram, Y., and Von~Oswald, J.
\newblock Weight decay induces low-rank attention layers.
\newblock \emph{arXiv preprint arXiv:2410.23819}, 2024.

\bibitem[Kolb et~al.(2025)Kolb, Weber, Bischl, and Rügamer]{kolb2025deep}
Kolb, C., Weber, T., Bischl, B., and Rügamer, D.
\newblock Deep weight factorization: Sparse learning through the lens of artificial symmetries, 2025.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and Hinton]{kornblith2019similarity}
Kornblith, S., Norouzi, M., Lee, H., and Hinton, G.
\newblock Similarity of neural network representations revisited.
\newblock In \emph{International conference on machine learning}, pp.\  3519--3529. PMLR, 2019.

\bibitem[Kunin et~al.(2021)Kunin, Sagastuy-Brena, Gillespie, Margalit, Tanaka, Ganguli, and Yamins]{kunin2021rethinking}
Kunin, D., Sagastuy-Brena, J., Gillespie, L., Margalit, E., Tanaka, H., Ganguli, S., and Yamins, D.~L.
\newblock Rethinking the limiting dynamics of sgd: modified loss, phase space oscillations, and anomalous diffusion.
\newblock 2021.

\bibitem[Landau \& Lifshitz(2013)Landau and Lifshitz]{landau2013statistical}
Landau, L.~D. and Lifshitz, E.~M.
\newblock \emph{Statistical Physics: Volume 5}, volume~5.
\newblock Elsevier, 2013.

\bibitem[Li \& Wang(2018)Li and Wang]{li2018neural}
Li, S.-H. and Wang, L.
\newblock Neural network renormalization group.
\newblock \emph{Physical review letters}, 121\penalty0 (26):\penalty0 260601, 2018.

\bibitem[Li et~al.(2016)Li, Wang, Lu, Arora, Haupt, Liu, and Zhao]{li2016symmetry}
Li, X., Wang, Z., Lu, J., Arora, R., Haupt, J., Liu, H., and Zhao, T.
\newblock Symmetry, saddle points, and global geometry of nonconvex matrix factorization.
\newblock \emph{arXiv preprint arXiv:1612.09296}, 1:\penalty0 5--1, 2016.

\bibitem[Li et~al.(2020)Li, Lyu, and Arora]{li2020reconciling}
Li, Z., Lyu, K., and Arora, S.
\newblock Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 14544--14555, 2020.

\bibitem[Li et~al.(2021)Li, Wang, and Arora]{li2021happens}
Li, Z., Wang, T., and Arora, S.
\newblock What happens after sgd reaches zero loss?--a mathematical framework.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lim et~al.(2024)Lim, Putterman, Walters, Maron, and Jegelka]{lim2024empirical}
Lim, D., Putterman, T.~M., Walters, R., Maron, H., and Jegelka, S.
\newblock The empirical impact of neural parameter symmetries, or lack thereof.
\newblock \emph{arXiv preprint arXiv:2405.20231}, 2024.

\bibitem[Marcotte et~al.(2023)Marcotte, Gribonval, and Peyr{\'e}]{marcotte2023abide}
Marcotte, S., Gribonval, R., and Peyr{\'e}, G.
\newblock Abide by the law and follow the flow: Conservation laws for gradient flows.
\newblock 2023.

\bibitem[Maron et~al.(2018)Maron, Ben-Hamu, Shamir, and Lipman]{maron2018invariant}
Maron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y.
\newblock Invariant and equivariant graph networks.
\newblock \emph{arXiv preprint arXiv:1812.09902}, 2018.

\bibitem[Masarczyk et~al.(2024)Masarczyk, Ostaszewski, Imani, Pascanu, Mi{\l}o{\'s}, and Trzcinski]{masarczyk2024tunnel}
Masarczyk, W., Ostaszewski, M., Imani, E., Pascanu, R., Mi{\l}o{\'s}, P., and Trzcinski, T.
\newblock The tunnel effect: Building data representations in deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Miller et~al.(1990)Miller, Nefkens, and {\v{S}}laus]{miller1990charge}
Miller, G.~A., Nefkens, B. M.~K., and {\v{S}}laus, I.
\newblock Charge symmetry, quarks and mesons.
\newblock \emph{Physics Reports}, 194\penalty0 (1-2):\penalty0 1--116, 1990.

\bibitem[Mingard et~al.(2025)Mingard, Rees, Valle-P{\'e}rez, and Louis]{mingard2025deep}
Mingard, C., Rees, H., Valle-P{\'e}rez, G., and Louis, A.~A.
\newblock Deep neural networks have an inbuilt occam’s razor.
\newblock \emph{Nature Communications}, 16\penalty0 (1):\penalty0 220, 2025.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and Srebro]{neyshabur2014search}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock In search of the real inductive bias: On the role of implicit regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and Srebro]{neyshabur2018towards}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock Towards understanding the role of over-parametrization in generalization of neural networks.
\newblock \emph{arXiv preprint arXiv:1805.12076}, 2018.

\bibitem[Nguyen(2019)]{nguyen2019connected}
Nguyen, Q.
\newblock On connected sublevel sets in deep learning.
\newblock In \emph{International conference on machine learning}, pp.\  4790--4799. PMLR, 2019.

\bibitem[Nguyen et~al.(2018)Nguyen, Mukkamala, and Hein]{nguyen2018neural}
Nguyen, Q., Mukkamala, M.~C., and Hein, M.
\newblock Neural networks should be wide enough to learn disconnected decision regions.
\newblock In \emph{International conference on machine learning}, pp.\  3740--3749. PMLR, 2018.

\bibitem[Nurisso et~al.(2024)Nurisso, Leroy, and Vaccarino]{nurisso2024topological}
Nurisso, M., Leroy, P., and Vaccarino, F.
\newblock Topological obstruction to the training of shallow relu neural networks.
\newblock \emph{arXiv preprint arXiv:2410.14837}, 2024.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Papyan, V., Han, X., and Donoho, D.~L.
\newblock Prevalence of neural collapse during the terminal phase of deep learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (40):\penalty0 24652--24663, 2020.

\bibitem[Peskin(2018)]{peskin2018introduction}
Peskin, M.~E.
\newblock \emph{An introduction to quantum field theory}.
\newblock CRC press, 2018.

\bibitem[Pinto et~al.(2024)Pinto, Rangamani, and Poggio]{pinto2024generalization}
Pinto, A., Rangamani, A., and Poggio, T.
\newblock On generalization bounds for neural networks with low rank layers.
\newblock \emph{arXiv preprint arXiv:2411.13733}, 2024.

\bibitem[Preskill et~al.(1991)Preskill, Trivedi, Wilczek, and Wise]{preskill1991cosmology}
Preskill, J., Trivedi, S.~P., Wilczek, F., and Wise, M.~B.
\newblock Cosmology and broken discrete symmetry.
\newblock \emph{Nuclear Physics B}, 363\penalty0 (1):\penalty0 207--220, 1991.

\bibitem[Rangamani \& Banburski-Fahey(2022)Rangamani and Banburski-Fahey]{rangamani2022neural}
Rangamani, A. and Banburski-Fahey, A.
\newblock Neural collapse in deep homogeneous classifiers and the role of weight decay.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  4243--4247. IEEE, 2022.

\bibitem[Rangamani et~al.(2023)Rangamani, Lindegaard, Galanti, and Poggio]{rangamani2023feature}
Rangamani, A., Lindegaard, M., Galanti, T., and Poggio, T.~A.
\newblock Feature learning in deep classifiers through intermediate neural collapse.
\newblock In \emph{International Conference on Machine Learning}, pp.\  28729--28745. PMLR, 2023.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Seifert(2008)]{seifert2008stochastic}
Seifert, U.
\newblock Stochastic thermodynamics: principles and perspectives.
\newblock \emph{The European Physical Journal B}, 64\penalty0 (3-4):\penalty0 423--431, 2008.

\bibitem[Simon et~al.(2023)Simon, Knutins, Ziyin, Geisz, Fetterman, and Albrecht]{simon2023stepwise}
Simon, J.~B., Knutins, M., Ziyin, L., Geisz, D., Fetterman, A.~J., and Albrecht, J.
\newblock On the stepwise nature of self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2303.15438}, 2023.

\bibitem[Simsek et~al.(2021)Simsek, Ged, Jacot, Spadaro, Hongler, Gerstner, and Brea]{simsek2021geometry}
Simsek, B., Ged, F., Jacot, A., Spadaro, F., Hongler, C., Gerstner, W., and Brea, J.
\newblock Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9722--9732. PMLR, 2021.

\bibitem[Smith et~al.(2021)Smith, Dherin, Barrett, and De]{smith2021origin}
Smith, S.~L., Dherin, B., Barrett, D.~G., and De, S.
\newblock On the origin of implicit regularization in stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:2101.12176}, 2021.

\bibitem[Tanaka \& Kunin(2021)Tanaka and Kunin]{hidenori2021noether}
Tanaka, H. and Kunin, D.
\newblock Noether's learning dynamics: Role of symmetry breaking in neural networks.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  25646--25660. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/d76d8deea9c19cc9aaf2237d2bf2f785-Paper.pdf}.

\bibitem[Tatro et~al.(2020)Tatro, Chen, Das, Melnyk, Sattigeri, and Lai]{tatro2020optimizing}
Tatro, N., Chen, P.-Y., Das, P., Melnyk, I., Sattigeri, P., and Lai, R.
\newblock Optimizing mode connectivity via neuron alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15300--15311, 2020.

\bibitem[Tibshirani(2021)]{tibshirani2021equivalences}
Tibshirani, R.~J.
\newblock Equivalences between sparse models and neural networks.
\newblock \emph{Working Notes. URL https://www. stat. cmu. edu/\~{} ryantibs/papers/sparsitynn. pdf}, 2021.

\bibitem[Tishby \& Zaslavsky(2015)Tishby and Zaslavsky]{tishby2015deep}
Tishby, N. and Zaslavsky, N.
\newblock Deep learning and the information bottleneck principle.
\newblock In \emph{2015 ieee information theory workshop (itw)}, pp.\  1--5. IEEE, 2015.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{tishby2000information}
Tishby, N., Pereira, F.~C., and Bialek, W.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint physics/0004057}, 2000.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Watanabe \& Opper(2010)Watanabe and Opper]{watanabe2010asymptotic}
Watanabe, S. and Opper, M.
\newblock Asymptotic equivalence of bayes cross validation and widely applicable information criterion in singular learning theory.
\newblock \emph{Journal of machine learning research}, 11\penalty0 (12), 2010.

\bibitem[Wu \& Papyan(2024)Wu and Papyan]{wu2024linguistic}
Wu, R. and Papyan, V.
\newblock Linguistic collapse: Neural collapse in (large) language models.
\newblock \emph{arXiv preprint arXiv:2405.17767}, 2024.

\bibitem[Xu et~al.(2023)Xu, Galanti, Rangamani, Rosasco, and Poggio]{xu2023janus}
Xu, M., Galanti, T., Rangamani, A., Rosasco, L., and Poggio, T.
\newblock The janus effects of sgd vs gd: high noise and low rank.
\newblock 2023.

\bibitem[Yamins et~al.(2014)Yamins, Hong, Cadieu, Solomon, Seibert, and DiCarlo]{yamins2014performance}
Yamins, D.~L., Hong, H., Cadieu, C.~F., Solomon, E.~A., Seibert, D., and DiCarlo, J.~J.
\newblock Performance-optimized hierarchical models predict neural responses in higher visual cortex.
\newblock \emph{Proceedings of the national academy of sciences}, 111\penalty0 (23):\penalty0 8619--8624, 2014.

\bibitem[Yang \& Hu(2020)Yang and Hu]{yang2020feature}
Yang, G. and Hu, E.~J.
\newblock Feature learning in infinite-width neural networks.
\newblock \emph{arXiv preprint arXiv:2011.14522}, 2020.

\bibitem[Zeiler \& Fergus(2014)Zeiler and Fergus]{zeiler2014visualizing}
Zeiler, M.~D. and Fergus, R.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13}, pp.\  818--833. Springer, 2014.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{Zhang_rethink}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock 2017.
\newblock URL \url{https://arxiv.org/abs/1611.03530}.

\bibitem[Zhao et~al.(2022)Zhao, Dehmamy, Walters, and Yu]{zhao2022symmetry}
Zhao, B., Dehmamy, N., Walters, R., and Yu, R.
\newblock Symmetry teleportation for accelerated optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16679--16690, 2022.

\bibitem[Zhao et~al.(2023{\natexlab{a}})Zhao, Dehmamy, Walters, and Yu]{zhao2023understanding}
Zhao, B., Dehmamy, N., Walters, R., and Yu, R.
\newblock Understanding mode connectivity via parameter space symmetry.
\newblock In \emph{UniReps: the First Workshop on Unifying Representations in Neural Models}, 2023{\natexlab{a}}.

\bibitem[Zhao et~al.(2023{\natexlab{b}})Zhao, Gower, Walters, and Yu]{zhao2023improving}
Zhao, B., Gower, R.~M., Walters, R., and Yu, R.
\newblock Improving convergence and generalization using parameter symmetries.
\newblock \emph{arXiv preprint arXiv:2305.13404}, 2023{\natexlab{b}}.

\bibitem[Zhu et~al.(2022)Zhu, Liu, Radhakrishnan, and Belkin]{zhu2022quadratic}
Zhu, L., Liu, C., Radhakrishnan, A., and Belkin, M.
\newblock Quadratic models for understanding neural network dynamics.
\newblock \emph{arXiv preprint arXiv:2205.11787}, 2022.

\bibitem[Ziyin(2024)]{ziyin2024symmetry}
Ziyin, L.
\newblock Symmetry induces structure and constraint of learning.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Ziyin et~al.(2023)Ziyin, Li, Galanti, and Ueda]{ziyin2023probabilistic}
Ziyin, L., Li, B., Galanti, T., and Ueda, M.
\newblock The probabilistic stability of stochastic gradient descent, 2023.

\bibitem[Ziyin et~al.(2024)Ziyin, Wang, Li, and Wu]{ziyin2024parameter}
Ziyin, L., Wang, M., Li, H., and Wu, L.
\newblock Parameter symmetry and noise equilibrium of stochastic gradient descent.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024.

\bibitem[Ziyin et~al.(2025{\natexlab{a}})Ziyin, Chuang, Galanti, and Poggio]{ziyin2024formation}
Ziyin, L., Chuang, I., Galanti, T., and Poggio, T.
\newblock Formation of representations in neural networks.
\newblock \emph{International Conference on Learning Representations}, 2025{\natexlab{a}}.

\bibitem[Ziyin et~al.(2025{\natexlab{b}})Ziyin, Xu, and Chuang]{ziyin2024remove}
Ziyin, L., Xu, Y., and Chuang, I.
\newblock Remove symmetries to control model expressivity.
\newblock \emph{International Conference on Learning Representations}, 2025{\natexlab{b}}.

\end{thebibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


%\section{Related Work}
%Due to space constraints, we have not discussed many other interesting training phenomena that may also be related to symmetry. In fact, parameter symmetries have been extensively studied by previous literature. Permutation symmetries are found to induce connected critical points for overparameterized networks \cite{simsek2021geometry} and lead to linear model connectivity \cite{entezari2021role}. Rescaling symmetries can be used to explain the generalization of ReLU networks \cite{yi2019positively}. Rotational symmetries can cause the dimensional collapse in self-supervised learning \cite{ziyin2023what} and the low-rankness of attention layers \cite{brea2019weight}.

%Symmetries also motivate the design of optimization methods. One line of research utilizes parameter symmetries to accelerate optimization \cite{armenta2023neural,zhao2023improving}, and other work discusses how to remove symmetries to improve the model \cite{lim2024empirical,ziyin2024remove}.

%For examples, Ref.~\citep{ziyin2024parameter} showed that both progressive sharpening \citep{wu2018sgd, cohen2021gradient} and flattening (which is required for warmup \citep{kalra2024warmup}) may be due to symmetry.

\section{Experiments}
\subsection{Measurement of $\Delta^G$}\label{app sec: measurement}
\paragraph{Permutation Symmetry} For the permutation symmetry in fully connected layers, we have described the $\Delta^G$ for these pairwise symmetries. However, for a layer of width $\psi$, there are $O(\psi^2)$ many such pairs, but we do not have to care about every pair of these because, for most of our purposes, we only care about how many neurons are actually functional and how many neurons are useless. This fact can allow us to reduce the number of measurements to $\psi$. To achieve this, we first sort all the neurons according to the norm of the input and outgoing weights. Because for two neurons to become close, their norms also need to be close. Therefore, if two neurons have a large norm difference, their permutation symmetry must have been broken. Under this ordering, we measure the $\Delta^G$ for the pairs of neurons with the closest norms.

An alternative perspective to look at this is that these pairwise neuron distances form a generating set of the symmetric group, and we are counting the number of symmetry breaking of these subgroups generated by each generator.

\paragraph{Omission of G} We will write $\Delta^G$ as $\Delta$ throughout the appendix because we are often comparing $\Delta$ for different groups and so the superscript $G$ is different for most cases.

\paragraph{Double Rotation Symmetry} Computing the degree of symmetry for the double rotation symmetry is rather tricky. 
In the ViT experiment (Figure~\ref{fig:complexity upper bound}), we measured the degree of symmetry in the self-attention layers. Here, the symmetry is the double rotation symmetry:
\begin{equation}
    W_Q W_K = W_Q M M^{-1} W_K,
\end{equation}
for an arbitrary invertible matrix $M$. The symmetric states are the ones where $W_Q$ and $W_K$ both become low-rank in the same subspace. Namely, it happens when there exists a vector $n$ such that 
\begin{equation}
    W_Q n =0,\ W_K^T n =0.
\end{equation}
There are at most $k$ such $n$ where $k$ is the right dimension of $W_Q$. This motivates this group's following operational definition of $\Delta$. We first compute the eigenvalue decomposition of $W_Q^T W_Q$ to obtain its eigenvectors $U$:
\begin{equation}
    W_Q^T W_Q = U \Lambda_Q U^T.
\end{equation}
We then compute the following matrix:
\begin{equation}
    \Lambda_K =  U^T W_K W_K^T U,
\end{equation}
and the degree of symmetry is defined as 
\begin{equation}
    N_{\rm dos} = \sum_i^k \mathbbm{1}_{\Delta_i < \Delta_{\rm th}} ,
\end{equation}
where 
\begin{equation}
    \Delta_i = |(\Lambda_Q)_{ii} - (\Lambda_K)_{ii}|.
\end{equation}


\paragraph{Degree of Symmetry} The \textit{degree of symmetry} we compute is the number of all  $\Delta^G$ such that $\Delta^G> \Delta^G_{\rm th}$, where $\Delta^G_{\rm th}$ is a certain threshold, often between $0.05$ and $0.2$. If this layer has $h$ many neurons, we define
\begin{equation}
    N_{\rm dosb} = h - N_{\rm dos}\,,
\end{equation}
and by definition $N_{\rm dosb} \ge 0$.


\subsection{Learning Dynamics}\label{app sec: exp detail}
\begin{figure}
    \centering
    \includegraphics[width=0.35\linewidth]{plot/quadratic1.png}
    \includegraphics[width=0.35\linewidth]{plot/quadratic2.png}
    \caption{The same setting as Figure \ref{fig:symmetry breaking dynamics}, but for quadratic activation. \textbf{Left}: $\Delta$ during training. \textbf{Right}: loss curve during training. Black dotted lines represent the time when $\Delta$ exceeds $\Delta^G_{\rm th}$.}
    \label{fig:symmetry breaking dynamics2}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.35\linewidth]{plot/sin1.png}
    \includegraphics[width=0.35\linewidth]{plot/sin2.png}
    \caption{The same setting as Figure \ref{fig:symmetry breaking dynamics}, but for sin activation.}
    \label{fig:symmetry breaking dynamics3}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.35\linewidth]{plot/rank.png}
    \includegraphics[width=0.35\linewidth]{plot/generalization.png}
    \caption{\small The same as Figure \ref{fig:complexity upper bound}, and we report the rank of the first layer for completeness.}
    \label{fig:complexity upper bound2}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.3\linewidth]{plot/alpha_scatter.png}
    \includegraphics[width=0.3\linewidth]{plot/sigma_scatter.png}
    \includegraphics[width=0.3\linewidth]{plot/units_scatter.png}


    \includegraphics[width=0.3\linewidth]{plot/transformer_alpha_scatter.png}
    \includegraphics[width=0.3\linewidth]{plot/transformer_sigma_scatter.png}
    \includegraphics[width=0.3\linewidth]{plot/transformer_units_scatter.png}
%    \caption{Figure \ref{fig:degree_symmetry} for transformers.}\label{fig: transformer degree of symmetry}
    \caption{\small The degree of symmetry increases with $\alpha$, the inverse scaling of input (\textbf{Left}), $\sigma$, the noise level (\textbf{Middle}), and the degree of teacher symmetry ($64-$the number of teacher units, \textbf{Right}). Blue dots represent experiment results, and blue lines represent linear fitting. \textbf{Upper}: MLP. \textbf{Lower}: transformer.}
    \label{fig:degree_symmetry}
\end{figure}

In Figure \ref{fig:symmetry breaking dynamics}, we use a teacher-student setting, where both the teacher and student are five-layer fully connected networks (FCNs) with 64 units per layer and tanh activation. The input and output dimensions are $10$ and $1$, respectively. Teacher network weights follow Kaiming initialization, and the output is scaled by 10. The student network is initialized near zero. The training dataset consists of $300$ samples generated as 
$\sin(\mathcal{N}(0,\sigma^2))$, with $\sigma$ ranging from $0$ to $3$. The test dataset contains 500 samples from the same distribution. We train the student network with gradient descent (no weight decay) for $50,000$ iterations using the mean squared error (MSE) loss.

To calculate the symmetry-breaking distance, we sort the first-layer neurons by their norms and compute the $L_2$ distance between consecutive neurons. Figure \ref{fig:symmetry breaking dynamics} shows distances for $63$ neuron pairs in the first layer (see Section~\ref{app sec: measurement}). Results for quadratic and sin activations are presented in Figures \ref{fig:symmetry breaking dynamics2} and \ref{fig:symmetry breaking dynamics3}.

\subsection{Mechanisms for Symmetry Changes}\label{app sec: mechanism}

For the left panel of Figure \ref{fig:complexity upper bound}, we directly use the pretrained models of ViT-Base and ViT-Large from \url{https://pytorch.org/vision/stable/models.html}. The computation of the weight rank and $N_{\rm dosf}$ follows the outline in Section~\ref{app sec: measurement}.

In the middle panel of Figure \ref{fig:complexity upper bound}, we train a three-layer fully connected network (FCN) with swish activation on a synthetic dataset where inputs and outputs are identical, sampled from a 300-dimensional standard Gaussian distribution. For gradient descent (GD), the dataset size is $1000$, and weight decay is set to $10^{-3}$. For stochastic gradient descent (SGD), the batch size is $128$, with new data randomly generated for each batch. In both cases, the test dataset contains $1000$ samples, and the networks are trained using the Adam optimizer. For completeness, we report the rank of the first layer in Figure \ref{fig:complexity upper bound2}, which shows that both the rank and the generalization error remain constant when the model width gets larger.

In Figure \ref{fig:degree_symmetry}, we analyze how symmetry evolves with input scaling, label noise, and the teacher symmetry. We adopt the teacher-student framework from Figure \ref{fig:symmetry breaking dynamics}, using Gaussian inputs for the teacher network and the Adam optimizer with a weight decay of $10^{-2}$. Symmetry is quantified as the number of first-layer neuron pair distances below $0.1$ among $63$ pairs in the student network. Teacher symmetry is measured as $64-$the number of neurons in each teacher network layer. As expected, stronger noise, weaker inputs, or teachers with higher symmetry (simpler target functions) result in greater symmetry in the student network.

The second row of Figure \ref{fig:degree_symmetry} presents experiments on a simple transformer comprising two self-attention layers and one linear layer. We use an in-context learning dataset with input sequences of length $50$, where each sequence element is a $21$-dimensional token. The last dimension of each token is a linear combination of its first $20$ dimensions, sampled from a standard Gaussian. The label corresponds to the last dimension of the final token in the sequence. The transformer is trained with the AdamW optimizer (weight decay $10^{-2}$). The results for the transformer align with those for the MLP.

\subsection{Neural Collapse}\label{app sec: neural collapse}
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\linewidth]{plot/unbiased.png}
    \includegraphics[width=0.4\linewidth]{plot/biased.png}
    \caption{The spectrum of the representation correlation. \textbf{Left}: the vanilla model. \textbf{Right}: the syre model. While both models are low rank, the gaps between eigenvalues are smaller for the syre model.}
    \label{fig:neural collapse_eigs}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.35\linewidth]{plot/unbiased_cov_wd0.005.png}
    \includegraphics[width=0.35\linewidth]{plot/biased_cov_wd0.005.png}
    \caption{The same setting as Figure \ref{fig:neural collapse}, but with weight decay ten times larger.}
    \label{fig:neural collapse2}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\linewidth]{plot/unbiased_wd0.005.png}
    \includegraphics[width=0.4\linewidth]{plot/biased_wd0.005.png}
    \caption{The same setting as Figure \ref{fig:neural collapse_eigs}, but with weight decay ten times larger. The low-rank structure is largely suppressed.}
    \label{fig:neural collapse_eigs2}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.37\linewidth]{plot/FCN_svhn.png}
    \includegraphics[width=0.4\linewidth]{plot/FCN_svhn_scatter.png}
    \caption{\small The same setting as Figure \ref{fig:hierachy}, but on the SVHN dataset.}
    \label{fig:hierachy2}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\linewidth]{plot/mnist_alignment.png}
    \includegraphics[width=0.4\linewidth]{plot/mnist_alignment_tanh.png}
    \caption{\small The raw data for Figure \ref{fig:representation alignment}, but two networks have different number of neurons. The legend $(i,j)$ denotes the alignment between the $i$-th layer of network A and the $j$-th layer of network B. \textbf{Left}: linear network. \textbf{Right}: tanh network.}
    \label{fig:representation alignment2}
\end{figure}

In Figure \ref{fig:neural collapse}, we train ResNet18 on the CIFAR-10 dataset using vanilla weight decay and syre \citep{ziyin2024symmetry}. In both cases, the weight decay is set to $5\times10^{-4}$. Networks are trained for $200$ epochs, and results from the final epoch are reported. To generate Figure \ref{fig:neural collapse}, we randomly select 10 images per class and compute the correlation between their features (i.e., the input to the final layer). The corresponding eigenvalue spectrum is presented in Figure \ref{fig:neural collapse_eigs}, showing 10 prominent eigenvalues. Figures \ref{fig:neural collapse2} and \ref{fig:neural collapse_eigs2} extend these experiments with a weight decay of $5\times10^{-3}$, where the vanilla weight decay model exhibits a stronger low-rank structure compared to the syre model.

In Figure \ref{fig:hierachy}, we train a five-layer FCN with $512$ neurons per layer and swish activation on CIFAR-10. Symmetry breaking is evaluated as the number of pairwise distances (as in Figure \ref{fig:symmetry breaking dynamics}) exceeding $1$. Results are averaged over 5 independent runs. Figure \ref{fig:hierachy2} replicates this experiment on the SVHN dataset, where the same hierarchical representation effects can be observed.


In Figure \ref{fig:representation alignment}, we train two deep linear networks on MNIST using the Adam optimizer. Each network is a six-layer fully connected network (FCN) with 128 neurons per hidden layer. The average alignment to input data is computed as the average alignment between the data and the third layer during training. Figure \ref{fig:representation alignment2} explores a similar setting for both linear and tanh networks, and the two networks have hidden layers with $64$ and $128$ neurons, respectively, where the universal alignment effects still hold.

%\clearpage
\subsection{Adaptive Capacity}\label{app sec: adaptive capacity}
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.245\linewidth]{plot/trials.png}
    \includegraphics[width=0.245\linewidth]{plot/trials_large.png}
    \includegraphics[width=0.245\linewidth]{plot/transformer_trials.png}
    \includegraphics[width=0.245\linewidth]{plot/transformer_trials_largeinit.png}
    \caption{The final distance between the weights of neurons for different teachers and the same initialization. For different tasks, the model converged to a different symmetry class. Left to right: (1) MLP small init., (2) MLP large init., (3) transformer small init., (4) transformer large init.}
    \label{fig:final learning result}
\end{figure}

In Figure \ref{fig:final learning result}, we train two-layer MLPs and simple transformers on synthetic datasets. The tasks match Figure \ref{fig:degree_symmetry}, and we report the pairwise distances among six neurons. Figure \ref{fig:final learning result} suggests that symmetry adapts to the training data in each run.






\clearpage
\section{Theory}
\subsection{Formal Statement of Theorem \ref{theo: complexity}}
We consider the MSE loss for part 2 of the theorem.
\begin{equation}
    \ell(x,\theta)=||y(x)-f(x,\theta)||^2.
\end{equation}
Consider an empirical data distribution $P(x)$, where $x$ contains both the input and the label. The SGD iteration is defined as 
\begin{equation}
    \theta_{t+1}=\theta_t- \eta\nabla_\theta\ell(x,\theta_t),
\end{equation}
where $x \sim P(x)$ and $\eta$ is the learning rate.

The GD iteration is defined as
\begin{equation}
    \theta_{t+1}=\theta_t- \eta\nabla_\theta \E_{x \sim P(x)}[\ell(x,\theta_t)].
\end{equation}



\begin{theorem}
    Let $f$ have the $G$-symmetry for which $P_G^T=P_G$, and $\theta$ be intialized at $\theta_0$ such that $P_G\theta_0 =\theta_0$.
    \begin{enumerate}[noitemsep,topsep=0pt, parsep=0pt,partopsep=0pt, leftmargin=14pt]
    \item For all time steps $t$ under GD or SGD, there exists a model $f'(x,\theta')$ and sequence of parameters $\theta'_t$ such that for all $x$,
    \begin{equation}
        f'(x,\theta_t') = f(x, \theta_t),
    \end{equation}
    where ${\rm dim}(\theta') = {\rm dim}(P_G)$.
        \item The kernalized model, $g(x,\theta) = \lim_{\lambda\to 0} (\lambda^{-1} f(x, \lambda\theta+\theta_0) - f(x, \theta_0))$, converges to
    \begin{equation}
        \theta^* = A^+ \sum_x \nabla_\theta f(x,\theta_0)^T y(x) 
        \label{eq:LeastSquare}
    \end{equation}
    under GD for a sufficiently small learning rate. Here, $A:=P_G\sum_x \nabla_\theta f(x,\theta_0)^T\nabla_\theta f(x,\theta_0)P_G$ and $A^+$ denotes the Moore–Penrose inverse of $A$. 
    \end{enumerate}
\end{theorem}
The second part of the theorem means that in the kernel regime\footnote{Technically, this is the lazy training limit \citep{chizat2018lazy}.}, being at a symmetric solution implies that the feature kernel features are being masked by the projection matrix $
    \nabla_\theta f(x,\theta_0)  \to P_G\nabla_\theta f(x,\theta_0)$,
and learning can only happen given these masks. The proof is a slight generalization of Propostions 2 and 3 in \citep{ziyin2024remove}.

\begin{proof}

\begin{enumerate}[noitemsep,topsep=0pt, parsep=0pt,partopsep=0pt, leftmargin=14pt]
\item Note that $\ell$ has the $G-$symmetry when $f$ has the $G-$symmetry. For $P_G\theta_0=\theta_0$ and any $\theta$, we have $\ell(x,\theta)=\ell(x,\theta_0+P_G(\theta-\theta_0))$. Taking $\theta\to\theta_0$, we have
\begin{equation}
(I-P_G)\nabla_\theta \ell(x,\theta_0)=0,\label{eq:nable f}
\end{equation}
where we use $P_G^T=P_G$. Therefore, for $\theta_1:=\theta_0+\eta\nabla_\theta \ell(x,\theta_0)$, we still have $P_G\theta_1=\theta_1$.

As $G$ is linear, we can suppose that $\{a_i\}_{i=1}^{\text{dim}(P_G)}$ forms a basis of $\text{im}(P_G)$, and define $f'(x,\theta'):=f(x,\sum_{i=1}^{\text{dim}(V^G)}\theta_i'a_i)$ for ${\rm dim}(\theta') = \text{dim}(P_G)$. By choosing $\theta'_i=\theta^Ta_i$, we have $f'(x,\theta_t') = f(x, \theta_t)$.

\item  By \eqref{eq:nable f}, close to any symmetric point $\theta_0$ (any $\theta_0$ for which $P_G\theta_0 = \theta_0$), for all $x$, we have
\begin{align}
    f(x,\theta) - f(x,\theta_0)
    =&\nabla_\theta f(x,\theta_0)P_G\Delta  + O(\|\Delta\|)^2.
\end{align}
Therefore, $g(x,\theta)$ simplifies to a kernel model \begin{equation}
        g(x,\theta)=\nabla_{\theta_0} f(x,\theta_0) P_G\theta.
    \end{equation}
    Let us consider the squared loss $\ell(\theta)=\sum_x||y(x)-g(x,\theta)||^2$ and denote $A:=\sum_xP_G\nabla_{\theta_0} f(x,\theta_0)^T\nabla_{\theta_0}f(x,\theta_0)P_G$, $b:=P_G\sum_x\nabla_{\theta_0} f(x,\theta_0)^Ty(x)$. The GD iteraiton is 
    \begin{equation}
        \theta^{t+1}=\theta^t-2\eta(A\theta^t-b),
    \end{equation}
    where $\theta^0=0$. If
    \begin{equation}
        \eta<\frac{1}{2\lambda_{max}(A)},
    \end{equation}
    GD converges to
    \begin{equation}
    \begin{aligned}
        \theta^*&=\lim_{t\to\infty}\sum_{k=0}^t(I-2\eta A)^k*2\eta b\\
        &=A^+b,
    \end{aligned}
    \end{equation}
    which is the standard least square solution.
\end{enumerate}
\end{proof}

\subsection{Formal Statement of Theorem \ref{theo: alignment}}
We will consider training a deep linear network with SGD and MSE loss
\begin{equation}
\ell(\theta, x)=||W_D\cdots W_1x-y(x)||^2,
\end{equation}
on datasets $\mathcal{D}_M = \{(Mx_i, y_i)\}_i$, where $M$ is an invertible matrix, and $y_i=Vx_i+\epsilon_i$ for i.i.d. noise $\epsilon_i$. We make the same assumptions as Theorem 5.4 of  \citep{ziyin2024parameter} (the most important of which is the SDE approximation to the SGD). Also, we use the definition of the noise equilibrium in \citep{ziyin2024symmetry}, which essentially means that SGD reaches stationarity in the degenerate directions of the double rotation symmetry.


\begin{theorem}
Let $V'=\sqrt{\Sigma_\epsilon}V\sqrt{\Sigma_x}$, $\text{rank}(V')=d$ and $S'$ be a diagonal matrix containing singular values of $V'$. Consider two deep linear networks A and B with weights of arbitrary dimensions larger than $d$. Let model A train on $\mathcal{D}_M$ and model B on $\mathcal{D}_{M'}$. Then, at the global minimum and at the noise equilibrium, every hidden layer of A is \textbf{perfectly} aligned with every hidden layer of B for any $x$, in the sense that
\begin{equation}
h_A^{L_A}(x)=c_0Rh_B^{L_B}(x)
\end{equation}
for $1\leq L_A<D_A$ and $1\leq L_B<D_B$ and any $x$, where $c_0=\left(\frac{\Tr S'}{d}\right)^{\frac{2L_A-D_A}{2D_A}-\frac{2L_B-D_B}{2D_B}}$ is a constant and $R=U_1U_2^T$, satisfying $U_1^TU_1=U_2^TU_2=I_d$. $h_A^{L_A}(x):=\Pi_{i=1}^{L_A}W_i^AMx$, $h_A^{L_A}(x):=\Pi_{i=1}^{L_A}W_i^AM'x$ denote the output of the $L_A,L_B-$the layer of network A and B, respectively. 
\end{theorem}

\begin{proof}
Let $V':=\tilde{U}S'\tilde{V}$ be its SVD. According to Theorem 5.4 of \citep{ziyin2024parameter}. At the global minimum and noise equilibrium under SGD, the solution of a $D_A$-layer network for the dataset $\mathcal{D}_M$ is given by
\begin{equation}
\sqrt{\Sigma_\epsilon}M_2W^A_{D_A}=\tilde{U}\Sigma_DU_{D_A-1}^T,\ W^A_i=U_i\Sigma_iU_{i-1}^T,\ W^A_1M_1\sqrt{\Sigma_x}=U_1\Sigma_1\tilde{V}
\end{equation}
for $i=2,\cdots,D-1$, where $U_i$ are arbitrary matrices satisfying $U_i^TU_i=I_{d\times d}$, and
\begin{equation}
\Sigma_1=\Sigma_D=\left(\frac{d}{\Tr S'}\right)^{(D_A-2)/2D_A}\sqrt{S'},\ \Sigma_i=\left(\frac{\Tr S'}{d}\right)^{1/D_A}I_{d\times d}.
\end{equation}
We can verify that $\Pi_{i=1}^{D_A}W_i^AM=V$, or $h_A^{D_A}(x)=Vx$ as expected. The solution suggests that
\begin{equation}
\Pi_{i=1}^{L_A}W_i^AM=U_{L_A}\left(\frac{\Tr S'}{d}\right)^{\frac{2L_A-D_A}{2D_A}}\sqrt{S'}\tilde{V}.
\label{eq:solution_A}
\end{equation}
Similarly
\begin{equation}
\Pi_{i=1}^{L_B}W_i^BM'=U_{L_B}\left(\frac{\Tr S'}{d}\right)^{\frac{2L_B-D_B}{2D_B}}\sqrt{S'}\tilde{V}.
\label{eq:solution_B}
\end{equation}

The proof is complete by comparing \eqref{eq:solution_A} and \eqref{eq:solution_B}.
\end{proof}

\subsection{Space Quantization Conjecture}\label{app sec: space quantization conjecture}
\begin{theorem}
Consider the loss $\ell(\theta)=\ell_0(\theta)+\gamma||\theta||^2$  with $\theta:=(\theta_1,\cdots,\theta_k)$ and $\theta_i\in\mathbb{R}^n$ for $1\leq i\leq k$. Assume that $\ell_0(\theta_1,\cdots,\theta_k)$ has the permutation symmetry $\ell_0(\theta_1,\cdots,\theta_i,\cdots,\theta_j,\cdots,\theta_k)=\ell_0(\theta_1,\cdots,\theta_j,\cdots,\theta_i,\cdots,\theta_k)$
for any $1\leq i,j\leq k$ and satisfies the following $q-$Lipschitz condition
\begin{equation}
    ||\nabla\ell_0(\theta)-\nabla\ell_0(\theta')||\leq K||\theta-\theta'||^q
\end{equation}
for $q\geq1$. Moreover, assume that $\inf_\theta\ell_0(\theta)>-\infty$ and that $K$ scales with the number of active neurons $m$ as $K=K_0m^{-\alpha}$. Then, at any global minimum, 
\begin{equation}
    m\leq C\gamma^{-\frac{q+1}{2\alpha}}
\end{equation}
for $n$ large enough, where $C$ is some constant.
\end{theorem}

\begin{proof}
We first consider two vectors $\theta_1\neq\theta_2$. Suppose that the global minimum is at $\ell(\theta_1,\theta_2)$. We would like to compare the loss between $(\theta_1,\theta_2)$ and $(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2))$, which gives
\begin{equation}
\begin{aligned}
&\ell(\theta_1,\theta_2)-\ell\left(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2)\right)\\
=&\ell_0(\theta_1,\theta_2)-\ell_0\left(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2)\right)+\gamma(||\theta_1||^2+\gamma||\theta_2||^2-2\gamma||\frac{1}{2}(\theta_1+\theta_2)||^2)\\
=&\ell_0(\theta_1,\theta_2)-\ell_0\left(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2)\right)+\frac{1}{2}\gamma||\theta_1-\theta_2||^2.
\end{aligned}
\end{equation}
For the first term, we have
\begin{equation}
||\ell_0(\theta_1,\theta_2)-\ell_0\left(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2)\right)||\leq ||\theta_1-\theta_2||\max_{z\in[0,1]}|f'(z)|||\theta_1-\theta_2||,
\end{equation}
where
\begin{equation}
f(z):=\ell_0\left(\frac{1}{2}(\theta_1+\theta_2)+z(\theta_1-\theta_2),\frac{1}{2}(\theta_1+\theta_2)-z(\theta_1-\theta_2)\right).
\end{equation}
By permutation symmetry, we have
\begin{equation}
f'(0)=\left(\nabla_1\ell_0\left(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2)\right)-\nabla_2\ell_0\left(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2)\right)\right)^T(\theta_1-\theta_2),
\end{equation}
where $\nabla_1$ and $\nabla_2$ denote the derivative of $\ell_0$ w.r.t. its first and second variable. By the permutation symmetry, we have $\nabla_1\ell_0\left(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2)\right)=\nabla_2\ell_0\left(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2)\right)$, and thus $f'(0)=0$. 

As $\nabla\ell_0$ is $q-$Lipschitz, we have
\begin{equation}
f'(z)\leq Kz^q||\theta_1-\theta_2||^q,
\end{equation}
which gives
\begin{equation}
0\geq\ell(\theta_1,\theta_2)-\ell(\frac{1}{2}(\theta_1+\theta_2),\frac{1}{2}(\theta_1+\theta_2))\geq -K||\theta_1-\theta_2||^{q+1}+\frac{1}{2}\gamma||\theta_1-\theta_2||^2.
\label{eq:ell_theta1_theta_2}
\end{equation}
for the global minimum $\ell(\theta_1,\theta_2)$. Thus we have
\begin{equation}
||\theta_1-\theta_2||\geq\left(\frac{\gamma}{2K}\right)^{1/(q-1)}
\end{equation}
for $q>1$. We conclude that any two vectors should be separated by a distance at least $\left(\frac{\gamma}{2K}\right)^{1/(q-1)}$.

Meanwhile, we have
\begin{equation}
\ell_0(\theta)+\gamma||\theta||^2\leq \ell_0(0),
\end{equation}
which gives
\begin{equation}
||\theta_i||^2\leq\frac{\ell_0(0)-L^*}{\gamma},
\end{equation}
for $1\leq i\leq k$, where $L^*=\inf_\theta\ell_0(\theta)>-\infty$.


Therefore, the problem is to put $m$ points in a $n-$dimensional ball of radius $\sqrt{\frac{\ell_0(0)-L^*}{\gamma}}$,  with pair-wise distance greater than $\left(\frac{\gamma}{2K}\right)^{1/(q-1)}$. Thus we have
\begin{equation}
m\leq2^n\left(\frac{\ell_0(0)-L^*}{\gamma}\right)^{n/2}\left(\frac{\gamma}{2K}\right)^{-n/(q-1)}=C'm^{-\frac{\alpha n}{q-1}}\gamma^{-\frac{q+1}{2(q-1)}n},
\end{equation}
which gives
\begin{equation}
m\leq C\gamma^{-\frac{q+1}{2\alpha}+O(1/n)},
\end{equation}
where $C,C'$ denote some constants.

For $q=1$, \eqref{eq:ell_theta1_theta_2} gives
\begin{equation}
K_0m^{-\alpha}-\frac{1}{2}\gamma\geq0,
\end{equation}
and thus
\begin{equation}
m\leq\left(\frac{\gamma}{2K_0}\right)^{-\frac{1}{\alpha}}.
\end{equation}
\end{proof}






\end{document}