%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{array}
\usepackage{graphicx} % For scaling the table
\usepackage{subcaption}
\usepackage{float}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage[preprint]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\Autoref}[1]{%
  \begingroup%
  \def\chapterautorefname{Chapter}%
  \def\sectionautorefname{Section}%
  \def\subsectionautorefname{Subsection}%
  \autoref{#1}%
  \endgroup%
}

\newcommand{\dataname}{\textsc{NaturalReasoning}\xspace}
\newcommand{\xian}[1]{\noindent{\textcolor{red}{\{{\bf xian:} \em #1\}}}}

\newcommand{\wz}[1]{\textcolor{blue}{(Weizhe: #1)}}
\newcommand{\jane}[1]{\textcolor{purple}{(Jane: #1)}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\dataname :  Reasoning in the Wild with 2.8M Challenging Questions}

\begin{document}

\twocolumn[
% \icmltitle{NaturalReasoning: 2.8M Reasoning Questions in the Wild}
\icmltitle{\dataname :  Reasoning in the Wild with 2.8M Challenging Questions}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Weizhe Yuan}{meta,sch}
\icmlauthor{Jane Yu}{meta,equal}
\icmlauthor{Song Jiang}{meta,equal}
\icmlauthor{Karthik Padthe}{meta}
\icmlauthor{Yang Li}{meta}
\icmlauthor{Ilia Kulikov}{meta}
\icmlauthor{Kyunghyun Cho}{sch}
%\icmlauthor{}{sch}
\icmlauthor{Dong Wang}{meta}
\icmlauthor{Yuandong Tian}{meta}
\icmlauthor{Jason Weston}{meta,sch}
\icmlauthor{Xian Li}{meta}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{meta}{Meta}
\icmlaffiliation{sch}{New York University}

\icmlcorrespondingauthor{Xian Li}{xianl@meta.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% Scaling reasoning data beyond domains such as math and code is hindered by the lack of verifiable ground truth. We present a scalable method for generating diverse and challenging reasoning questions based on pretraining corpora. Without any human annotation on verifying ground truth, we demonstrate that synthetic rewards derived from self-scoring is sufficient to enable self-improvement on reasoning via reinforcement learning. We provide empirical results on our method can effectively leverage scaling test-time compute, where performance on challenging reasoning benchmarks such as GPQA, MMLU Pro improves when sampling more solutions, or sampling more verifications, or both. We release 2.8 million questions covering multiple domains in [place holder. STEM, finance, ...] which can be used as seed data to generate more synthetic data.

% Scaling reasoning capabilities beyond traditional domains such as mathematics and coding is hindered by the lack of verifiable ground truth. To overcome this limitation, we introduce a scalable approach for generating diverse and challenging reasoning questions, accompanied by reference answers, leveraging pretraining corpora. We present NaturalReasoning, a comprehensive dataset comprising 2.8 million questions that span multiple domains, including STEM fields (e.g., physics, biology), finance, social sciences, and more. The efficacy of our questions is validated through knowledge distillation from larger language models to smaller ones, utilizing their responses. Furthermore, we demonstrate the value of our reference answers by showing additional performance gains when incorporating them into training data curation. Our dataset offers a valuable resource for exploring new frontiers in large language model reasoning capabilities.

Scaling reasoning capabilities beyond traditional domains such as math and coding is hindered by the lack of diverse and high-quality questions. To overcome this limitation, we introduce a scalable approach for generating diverse and challenging reasoning questions, accompanied by reference answers. We present \dataname, a comprehensive dataset comprising 2.8 million questions that span multiple domains, including STEM fields (e.g., Physics, Computer Science), Economics, Social Sciences, and more. We demonstrate the utility of the questions in \dataname through knowledge distillation experiments which show that \dataname can effectively elicit and transfer reasoning capabilities from a strong teacher model. 
%We illustrate the utility of the questions in \dataname through distillation experiments that utilize responses from a larger model to finetune a smaller one, where a comparison with other datasets show that distillation is more effective with \dataname. 
Furthermore, we demonstrate that \dataname is also effective for unsupervised self-training using external reward models or self-rewarding.
%Furthermore, we demonstrate the value of our reference answers by showing additional performance gains when incorporating them into training data curation. 
% \dataname offers a valuable resource for exploring new frontiers in large language model reasoning capabilities.
\end{abstract}
\begin{figure*}[!t]
     \centering
    \includegraphics[width=1.0\linewidth]{fig/natural_reasoning_diagram_v2.pdf}
    \caption{An overview of the data creation approach of \dataname. First, we use an LLM to annotate the reasoning contents of documents in pretraining corpora. For documents which contain reasoning traces to solve complex problems, we use an LLM to synthesize a self-contained question which requires thinking and reasoning. We also prompt the LLM to return a reference answer to the question if the answer can be found in the document. We then conduct deduplication and decontamination. Sampling solutions to those questions from different LLMs enables knowledge distillation as well as self-rewarding training schemes.}
    \label{fig:data_pipeline}
\end{figure*}
\section{Introduction}
\label{intro}

 Large language models (LLMs) have recently demonstrated increased reasoning capabilities to solve complex tasks. Notable examples include OpenAI's o1 models~\cite{openai2024openaio1card} and DeepSeek's R1 models~\cite{guo2025deepseek}, which have demonstrated significant improvements in challenging reasoning benchmarks such as competition math
%AIME\footnote{\url{https://artofproblemsolving.com/online}}, Codeforces~\footnote{\url{https://codeforces.com/}}
competitive coding, and GPQA~\cite{rein2024gpqa}. These models are designed to devote more time to deliberation before responding, enabling them to tackle intricate tasks and solve more complex problems in science, coding, and mathematics.

However, existing reasoning datasets often have limitations, primarily consisting of problem-solving tasks that exclude domains with open-ended reasoning  and that lack diversity in scale and difficulty. To address this gap, we introduce \dataname, a comprehensive dataset curated from pretraining corpora, comprising 2.8 million reasoning questions spanning various topics, including Mathematics, Physics, Computer Science, Economics \& Business, etc. \dataname is compared to a wide range of typical reasoning datasets, showcasing its 
%beneficial 
advantageous
properties, in particular its
%exceptional 
diversity and difficulty.

\dataname possesses several desirable attributes as a dataset, serving multiple research purposes. Firstly, the questions are backtranslated from the pretraining corpora, ensuring that it represents real-world reasoning problems that people care about, as opposed to synthetic datasets derived from benchmark datasets like MetaMathQA~\cite{yumetamath} and OpenMathInstruct-2~\cite{toshniwal24openmathinstruct}. Secondly, it consists of questions with both verifiable and open-ended answers (e.g., theorem proving), providing a rich resource for developing learning algorithms to enhance LLMs' reasoning abilities beyond a narrow subset of verifiable questions. Lastly, we demonstrate that the questions in \dataname require more advanced reasoning than existing datasets, and thus can be instrumental in improving LLMs' reasoning capabilities through knowledge distillation from a stronger teacher model to a weaker student model or self-training based on external reward models or self-rewarding techniques~\cite{yuanself}.

Our contributions are threefold:
\begin{itemize}
    \item We create a large-scale and high-quality reasoning dataset by using pretraining data and LLMs alone without extra human annotation. The dataset contains challenging questions which require deliberate thinking accompanied with short reference answers.
    % to be used as solution verification. 
    \item We show that \dataname is a highly performant dataset to enhance reasoning capabilities in post-training. Specifically, using questions in \dataname in distillation is more sample efficient than existing datasets.% such as OpenMathInstruct-2 and MAmmoTH2~\cite{}.
    \item We explore self-training methods %, including RRFT and DPO, 
    using the model’s own responses. Our results show that the questions in our dataset effectively enable self-learning, where self-rewarding techniques can even outperform strong external reward models.
    
    % We also investigate how to curate reliable and verifiable gold answers without human annotations. We compare deriving reliable reward signals using self-consistency and self-scoring, and demonstrate that even without reliable gold answers, the model can still self-improve through self-scoring and curate contrastive pairs for DPO training.
\end{itemize}


\section{Data Collection}
\begin{table*}[t]
\footnotesize
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.3}

 \caption{Comparison of four large publicly available reasoning datasets with \dataname. ``Q. \#'' denotes the number of unique questions, and question length is measured by the number of words.}
  \label{tab:data_compare}
  
\begin{tabular}{p{2.cm}m{0.9cm}m{0.6cm}p{1cm}p{7.1cm}p{3.4cm}}
\toprule
\textbf{Dataset}   & \textbf{Domain} & \multicolumn{1}{l}{\textbf{Q. \#}} & \textbf{Q. Len.} & \textbf{Question source}                                & \textbf{Response model}             \\
\midrule
MetaMathQA         & Math            & 395K                                       & 41$\pm$24                 & Rephrased by GPT-3.5-Turbo from GSM8K+MATH              & GPT-3.5-Turbo           \\
% NuminaMath         & Math            & 860K                                       & 48$\pm$32                 & Grade-level and competition-level math datasets & GPT-4o     \\
OpenMath2 & Math            & 607K                                       & 46$\pm$44                 & Synthesized by Llama3-405B-Inst from GSM8K+MATH  & Llama3.1-405B-Instruct \\
WebInstruct           & Multi       & 13M                                    & 34$\pm$28                 & Recall and Extracted from Web                           & Mixtral-22B×8, Qwen-72B   \\


\midrule
{\small NaturalReasoning}  & Multi       & 2.8M                                     & 55$\pm$21                 & Synthesized by Llama3.3-70B-Inst grounded in Web                             & Llama3.3-70B-Instruct \\
\bottomrule
\end{tabular}
\end{table*}


Backtranslating questions based on pretraining corpora has shown to be a scalable approach to create instruction-following datasets~\cite{li2023self,yue2024mammoth2}. We take a similar approach to extract diverse and realistic reasoning questions grounded in pretraining data. An overview of the data creation approach is illustrated in \autoref{fig:data_pipeline}. A key differentiator of our approach is its emphasis on simplicity; we use LLMs throughout the entire cycle of synthetic data generation and curation, without any human annotation nor manual steps such as preprocessing to detect relevant documents or extracting questions with rule-based filtering.



\subsection{Annotate Reasoning}
\label{annotation_sec}
Inspired by the meta-cognitive capabilities of state-of-the-art LLMs~\cite{didolkar2024metacognitive}, we use an LLM to perform detailed annotation of pretraining documents to detect documents which contain sophisticated reasoning traces. We use the public pretraining corpora DCLM-baseline ~\cite{li2024datacomp} and FineMath ~\cite{liu2024finemath} as sources, which have demonstrated steeper scaling laws than alternative corpora. More specifically, given a document $d$ from the pretraining corpus, we prompt an LLM to rate the content in $d$ along multiple axes: Problem Completeness, Problem Complexity and Technical Depth, Technical Correctness and Accuracy, Thinking and Reasoning. Empirically, we found that the model could analyze the document well and is able to follow the format requirement in the instruction. Please refer to \autoref{app:prompts} for the detailed prompt we use. 




\subsection{Synthesize Questions, Answers and Responses}

For documents which are identified with a high degree of reasoning (e.g., scored full points on the axes of Problem Complexity and Technical Depth, Thinking and Reasoning), we further prompt an LLM to compose a self-contained and challenging reasoning question $q$ based on the content in $d$. Different from existing work, which extracts questions appearing in the document~\citep{yue2024mammoth2}, our approach allows us to synthesize more novel questions not directly contained in pretraining corpora. Additionally, we prompt the LLM to verify whether a correct reference answer $a$ to the synthesized question $q$ can be derived from $d$ and, if possible, include it as a reference answer. Besides questions and reference answers, we also augment each question in our dataset with a response generated by a relatively strong open-sourced model, Llama3.3-70B-Instruct. 


% the detailed prompts we use are shown in \autoref{app:prompts}.


\subsection{Question Deduplication and Decontamination}
% \wz{Mention that despite question and response, we also have extracted reference answer, and annotated quality/difficulty}
\paragraph{Deduplication}
% There are multiple sources of duplicates. Different pretraining corpora could have similar docs. Within a single corpus, even if two docs are not quite similar, they could cover the same reasoning task, thus the backtranslated questions are almost the same. 
% Our curation deduplication focuses on removing near matching questions by locality sensitive min-hashing each word. We choose a low threshold at $0.55$ to filter out near duplicates. This helps to remove cases where the core reasoning task stay the same, but the prompts vary.
Our deduplication process focuses on identifying and removing near-duplicate questions using locality-sensitive min-hashing at the word level. We apply a similarity threshold of 0.55 to filter out closely related variations, ensuring that questions with the same core reasoning task but different prompts are not redundantly included.

\paragraph{Decontamination}

We filter out questions that are similar to popular reasoning benchmarks including MATH~\cite{hendrycksmath2021}, GPQA, MMLU-Pro~\cite{wang2024mmlu} and MMLU-STEM~\cite{hendryckstest2021}. The standard 13-gram decontamination method from EleutherAI's llm-eval-harness~\cite{eval-harness} is used to identify and remove $0.026\%$ items from the dataset. %Due to their small number, our ablation shows that the effect of these items on SFT evaluation metrics is negligible.
% \paragraph{Reasoning Score.}



% \subsection{Bootstraping Reference Answer}
% \wz{I thought we only kept the backtranslated refernece answer as it is...} \xian{These sections are describing the methods, which we use to verify and compare the quality of extracted answer.}
% For each question $q$, we sample $K=32$ solutions $\{a_k\}$ from different models $\{M_i\}$. 

% \subsection{Generate Response}
% \wz{We use Opensourced SOTA LLMs for response generation. Ablation for 405B vs. 3.3-70B vs backtranslation from human CoT}
% \paragraph{Self Consistency.}



% \paragraph{Self Scoring.}

\section{Data Analysis}

We compare \dataname to the following representative existing datasets which were curated to boost reasoning capabilities.
\paragraph{MetaMathQA} is created by bootstrapping mathematical questions from GSM8K and MATH, rewriting them from multiple perspectives to form a new dataset~\cite{DBLP:conf/iclr/YuJSYLZKLWL24}. The responses are generated using GPT-3.5-Turbo.

%\paragraph{NuminaMath} is a comprehensive collection of 860K pairs of math problems and solutions~\citep{numina_math_datasets}. The questions cover multiple sources including grade-level questions and competition problems. The solutions in NuminaMath dataset are generated or rewritten by GPT-4o.

\paragraph{OpenMathInstruct-2} is a collection of 14M synthesized math questions and solutions, based on GSM8K and MATH~\citep{toshniwal2024openmath2}. The solutions are generated by Llama3.1-405B-Instruct~\cite{grattafiori2024llama3herdmodels} and curated through majority vote on the final answer.

\paragraph{WebInstruct} recalls relevant documents from Common Crawl using a fastText model trained on a diverse seed dataset of quiz websites. It then extracts question-answer pairs contained in recalled web pages and uses LLMs (Qwen-72B~\cite{bai2023qwentechnicalreport}, Mixtral-8×7B~\cite{jiang2024mixtralexperts}) to refine the answer~\cite{yue2024mammoth2}.
% \paragraph{WebInstruct} recalls relevant documents and extracts question-answer pairs contained in web pages in pretraining data, and uses LLMs (Qwen-72B~\cite{bai2023qwentechnicalreport}, Mixtral-8×7B~\cite{jiang2024mixtralexperts}) to refine the answer~\cite{yue2024mammoth2}.
% \begin{itemize}
%     \item NuminaMath~\citep{numina_math_datasets}.
%     \item OpenMathInstruct-2~\citep{toshniwal2024openmath2}.
% \end{itemize}
\subsection{Basic Statistics}
We present a comparison of key dataset statistics in \autoref{tab:data_compare}. Most large open reasoning datasets primarily focus on the math domain, with datasets such as OpenMathInstruct-2, 
%NuminaMath, 
and MetaMathQA containing only math-related questions. In contrast, \dataname covers reasoning problems from more diverse domains. Additionally, \dataname consists of 2.8M unique questions, significantly larger than OpenMathInstruct-2 (607K), 
%NuminaMath (860K), 
and MetaMathQA (395K), though smaller than WebInstruct (13M).

Moreover, our questions have an average length of 55 words, which is longer than those in OpenMathInstruct-2 (46), WebInstruct (34),
%NuminaMath (48), 
and MetaMathQA (41). The increased question length suggests that our dataset contains more complex and detailed problems, potentially requiring more advanced reasoning capabilities. This, combined with our diverse question sources and grounding in web-based knowledge, highlights the uniqueness and potential difficulty of our dataset.

% \wz{Num of questions, question length: mean+-std, question similarity (use the embedding to calculate on average, how similar is a question to other questions in the dataset)}
\subsection{Question Quality and Difficulty}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.93\linewidth]{fig/q_quality.pdf}
    \caption{Quality distribution of questions based on LLM-annotated scores: Low (0 to 6), High (7 to 10).}
    \label{fig:difficulty_distribution}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/response_length.pdf}
    \caption{Median response length (measured in words) of Llama3.3-70B-Instruct-generated responses across all datasets. Responses to questions in \dataname are generally much longer, suggesting a more challenging/complex nature.}
    \label{fig:response_length}
\end{figure}

% \wz{Response length and difficulty got by prompting}
\paragraph{Quality} To further evaluate questions in \dataname, we analyze them through the lens of both quality and difficulty. For each question in \dataname, we further annotate its quality (e.g., solvability, completeness) on a scale of 1 to 10 using Llama3.3-70B-Instruct. For the datasets we compare to, we randomly selected 10\% of their questions and applied the same annotation prompt to assess their quality. The quality scores range from 0 to 10, where we categorize scores between 0 and 6 as low quality and those between 7 and 10 as high quality. The results, presented in \autoref{fig:difficulty_distribution}, show that \dataname has the highest proportion of high-quality questions at 98\%, while the second-highest dataset contains 87\% high-quality questions.

% To estimate question difficulty, we use an off-the-shelf LLM to generate responses and measure their lengths, as longer responses generally indicate more complex questions. Specifically, we employ Llama3.3-70B-Instruct to generate answers for all questions for fair comparison. For the comparison datasets, we analyze responses for a randomly selected 10\% subset got using the same model. The results, shown in \autoref{fig:response_length}, indicate that our dataset has the longest median response length (434 words), significantly exceeding other datasets. The second-longest median response length is observed in OpenMathInstruct-2, which has a median of xx words. This suggests that our dataset contains more intricate and demanding questions compared to existing open reasoning datasets.

\paragraph{Difficulty} To estimate question difficulty, we leverage a strong LLM to generate responses and use response length as a proxy, as longer chain-of-thoughts typically correspond to more complex questions. Specifically, we randomly selected 10\% of questions from each dataset, and employ Llama3.3-70B-Instruct to generate responses for each question. As is shown in \autoref{fig:response_length}, \dataname exhibits the longest median response length (434 words), significantly surpassing all other datasets. The second-longest median response length is observed in OpenMathInstruct-2, with a median of 354 words. This suggests that our dataset contains more intricate and reasoning-demanding questions compared to existing open reasoning datasets.

\subsection{Question Diversity}
In addition to being difficult, questions in \dataname are also diverse. We analyze diversity of the questions in terms of question similarity and the topics, and compare to WebInstruct, an existing dataset covering multiple domains.
%\wz{1. plot all data's embedding together. 2. show the different topics in our dataset }

\paragraph{Embedding Clustering} 
\label{sec:clustering}
% For this method we use LLM to calculate embeddings for sample of MAmmoTH2 and \dataname, then UMAP~\cite{mcinnes2018umap} is used to project high dimensional data to 2D embeddings, then apply K-means to cluster these embedding and then we use 8B scale LLM to label clusters into highlevel categories by passing few examples from the cluster in prompt. From the \ref{fig:topic_clusters} we can see that along with Math data \dataname contains more dense representation from diverse non-math topics like Physics, Chemistry, computer science, law etc. Whereas MAmmoTH mainly skews towards Math content.
We use an off-the-shell sentence encoder\footnote{\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}} to generate embeddings for questions in WebInstruct and \dataname. We then apply UMAP~\cite{mcinnes2018umap} to project the high-dimensional embeddings into a 2D space, followed by K-means clustering~\cite{wu2012advances} to identify distinct groups. We use Mixtral-8B to assign high-level labels to these clusters, which is prompted with a few representative examples from each cluster. As is shown in Appendix \autoref{fig:topic_clusters}, \dataname contains a more diverse and dense representation of non-mathematical topics, including Physics, Chemistry, Computer Science, and Law, besides Math. In contrast, WebInstruct is primarily skewed toward mathematical content, highlighting the broader topic coverage of \dataname.
\paragraph{Classifier Categorization} To estimate the topic distribution, a multi-class topic classifier is used to classify each question into 16 knowledge classes. The class labels are motivated by \href{https://en.wikipedia.org/wiki/Outline_of_academic_disciplines}{Wikipedia academic disciplines}. Figure \ref{fig:topic_distribution} shows that \dataname is complementary to WebInstruct, where \dataname has better coverage on non-Math topics especially Physics, Computer Science, Social Science, etc.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/categorization.pdf}
    \vspace{-7mm}
    \caption{Topic distributions of \dataname and WebInstruct. \dataname shows much greater coverage on non-Math topics like Computer Science and Physics.}
    \label{fig:topic_distribution}
\end{figure}




% \begin{center}
% \begin{tabular}{|p{2.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}
% \hline
% Dataset & No. of questions & Mean question length & std question length\\
% \hline
% OpenMathInstruct-2 & 607324 & 239 & 235 \\ 
% MAmmoTH2 & 13275146 & 189 & 158 \\  
% NuminaMath & 859494 & 251 & 178 \\
% MetaMathQA & 395000 & 213 & 124 \\
% our data & 2790552 & 324 & 128 \\
% \hline
% \end{tabular}
% \end{center}

% \begin{center}
% \begin{table}[h]
%     \centering
%     \caption{Summary of datasets and their respective number of questions and mean/standard deviation of the length of the questions.}
%     \begin{tabular}{%
%         >{\raggedright\arraybackslash}p{2.8cm}
%         >{\centering\arraybackslash}p{1.9cm}
%         >{\centering\arraybackslash}p{0.8cm}
%         >{\centering\arraybackslash}p{0.8cm}}
%         \toprule
%         \textbf{Dataset} & \textbf{Num. of Questions} & \textbf{Mean Length} & \textbf{Std Length} \\
%         \midrule
%         OpenMathInstruct-2 & 607,324 & 239 & 235 \\ 
%         MAmmoTH2 & 13,275,146 & 189 & 158 \\  
%         NuminaMath & 859,494 & 251 & 178 \\
%         MetaMathQA & 395,000 & 213 & 124 \\
%         \dataname & 2,790,552 & 324 & 128 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:datasets}
% \end{table}
% \end{center}





% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{step_count_box.png}
%     \caption{Number of reasoning steps in responses to questions from different datasets. Responses are generated with Llama 3.3 70B.}
%     \label{fig:step_count}
% \end{figure}


\subsection{Reference Answer Analysis}

Among the 2.8 million questions we synthesized, 81.68\% have reference answers which could be derived from pretraining data. The distribution of reference answer lengths is illustrated in \autoref{fig:ref_length}, with single-word answers accounting for 10.7\%, short answers (2–9 words) making up 20.0\%, and long answers ($\geq$10 words) constituting the majority at 50.9\%.

We provide examples of questions with single-word, short, and long answers in \autoref{app:example_questions}. In general, we found that questions with single word answers typically involve numerical, factual, or definitional queries, while questions with long answers demand more free-form in-depth analysis. For questions with a long answer, the extracted reference answer is typically a short summary content from the original documents or useful clues to answer the question. 

While reference answers may contain some noise, we demonstrate their %potential usefulness for training data filtering in
usefulness in 
\Autoref{sec:ref_ans_use}.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\linewidth]{fig/ref_answer_len_dist.pdf}
    \caption{Distribution of reference answer lengths in \dataname, showing that the majority of questions have long reference answers ($\geq$10 words).}
    \label{fig:ref_length}
\end{figure}


\section{Experiment Setup}

\subsection{Training data}
We highlight the efficacy of \dataname in two primary ways: (1) Knowledge distillation and supervised finetuning, which enables steeper scaling trends than other existing reasoning datasets, and (2) using \dataname as a source to extract seed data to target a specific domain at hand. 

To demonstrate (2), we focus on science reasoning benchmarks, such as GPQA. To achieve this, we first sample 250 questions from the GPQA dataset, excluding those from the Diamond subset. For each selected question, we retrieve the 1,000 most similar questions from \dataname, which were already decontaminated against the entire GPQA datset. Similarity is computed using cosine similarity between two question embeddings. After obtaining this candidate set, we apply deduplication and perform clustering, grouping the questions into 15,000 clusters. From each cluster, we select the questions closest to the cluster center, ensuring a diverse and representative dataset for downstream science reasoning tasks. This process resulted in a pool of 15,000 questions, which we refer to as \texttt{science-reasoning-15k}. Models trained on this subset can still be evaluated on the GPQA-Diamond test set as those questions are not used for data selection.
\texttt{science-reasoning-15k} is used in
\autoref{sec:ref_ans_use} and
\autoref{sec:unsup_self_train}.

\subsection{Evaluation}
We evaluate our models on a diverse set of benchmarks that encompass both math and science reasoning: MATH, GPQA, GPQA-Diamond~\cite{rein2024gpqa}, and MMLU-Pro. To ensure a fair and consistent comparison, we adopt a zero-shot evaluation setting across all trained models. For inference we use vllm \citep{Kwon_2023} and employ greedy decoding to maintain determinism and eliminate variability introduced by stochastic generation. Unless mentioned otherwise, we report accuracy averaged over the last three saved model checkpoints during training.

% \paragraph{GPQA.} We evaluate on the Diamond subset.

\section{Steeper Scaling with Challenging and Diverse questions}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/scaling_curve_avg.pdf}
    \caption{Average performance across MATH, GPQA, and MMLU-Pro when using varying sizes of data for training Llama3.1-8B-Base. SFT with 1.5 million examples from \dataname is able to surpass Llama3.1-8B-Instruct.}
    \label{fig:scaling_base}
\end{figure}
Our hypothesis is that challenging and diverse questions which require thinking and reasoning are more sample efficient for post-training. To verify this, we run supervised finetuning (SFT) starting from a base model, and evaluate general reasoning capabilities and overall performance across MATH, GPQA, and MMLU-Pro.


% \subsection{From the base model}


% \wz{xian: could you add some training details about those models, like for different data size, the lr and bsz, how frequent is a ckpt saved etc.}
% We finetune Llama3.1-8B-Base using fairseq2 training recipes \cite{balioglu2023fairseq2}, with varying sizes of data (number of unique questions), and compare average performance across MATH, GPQA, and MMLU-Pro. 
We fine-tuned the Llama3.1-8B-Base model using fairseq2 training recipes \cite{balioglu2023fairseq2}, exploring the impact of varying dataset sizes (measured by the number of unique questions). Specifically, we trained the model on our dataset 
\dataname 
and the comparison datasets introduced in \autoref{tab:data_compare}, and evaluated its average performance across three benchmarks: MATH, GPQA, and MMLU-Pro. 
% The responses
% are generated by different models as is described in \autoref{tab:data_compare}. 
For all datasets, we train 3 epochs for each data size, using batch size 128, learning rate $5e^{-6}$, with cosine learning rate schedule where final learning rate is 1\% of peak learning rate. 
% We report the average performance of the last 3 checkpoints during the last 6000 steps of training. 

% Note that we didn't do data decontamination for other datasets.

\subsection{Results}
% As is shown in \autoref{fig:scaling_base}, \dataname is more sample efficient than existing datasets commonly used in reasoning post-training at different scales. More importantly, only training on 1.5 million questions from \dataname already outperforms Llama 3.1 8B Instruct. On the other hand, training using OpenMathInstruct-2 and MAmmoTH2 do not manage to surpass the Llama 3.1 8B Instruct, even with 2.8 million datapoints. A breakdown of these scores is enumerated in Table~\ref{tab:scaling_base_breakdown}, which shows that math-specific datasets like NuminaMath and OpenMathInstruct-2 perform well if evaluated on math-centric datasets like MATH but are much less performant on more general benchmarks like GPQA and MMLU-Pro than \dataname.
The scaling trends plotted by averaging performance on the three benchmarks are shown in \autoref{fig:scaling_base}, and \autoref{tab:scaling_base_breakdown} provides a detailed breakdown of model performance across different dataset sizes and benchmarks. 

% \wz{Should we mention that we did not do decontamination for other datasets and numina has MATH and MAmmoTH has MMLU-Pro}\xian{sure.}

% Overall, we make the following observations.

\paragraph{\dataname is significantly more sample-efficient than existing reasoning datasets.} As shown in \autoref{fig:scaling_base}, models trained on \dataname require fewer training examples to achieve superior performance. With just 1.5 million training examples, the model trained on \dataname already outperforms Llama3.1-8B-Instruct, which was extensively tuned for instruction-following with more data~\cite{grattafiori2024llama3herdmodels}. In contrast, other datasets, including OpenMathInstruct-2 and WebInstruct, fail to surpass Llama3.1-8B-Instruct even when trained on 2.8 million data points. This highlights that \dataname provides more useful reasoning supervision per training sample, making it a better choice for improving model reasoning abilities efficiently.

\paragraph{Math-specific datasets like OpenMathInstruct-2 excel at math reasoning but fail to generalize beyond math.} A closer look at \autoref{tab:scaling_base_breakdown} reveals that OpenMathInstruct-2 consistently achieves the highest scores on the MATH benchmark, with performance increasing from 50.83 (500K) to 59.25 (2.8M). This confirms that OpenMathInstruct-2 is well-optimized for pure math reasoning. However, its performance on GPQA and MMLU-Pro is significantly weaker, where GPQA accuracy plateaus around 27–26 as dataset size increases, and MMLU-Pro accuracy fluctuates without significant improvement. This suggests that while OpenMathInstruct-2 provides strong supervision in math reasoning, it lacks the diversity required to generalize to broader scientific and multidisciplinary reasoning tasks.

\paragraph{Some datasets show diminishing returns as training data increases, highlighting potential inefficiencies in data composition.} While scaling up dataset size generally improves performance, datasets like WebInstruct and OpenMathInstruct-2 exhibit inconsistent or plateauing performance trends. For example, WebInstruct’s GPQA performance peaks at 500K (29.02) but drops at 1.5M (25.37) and only marginally improves at 2.8M (26.12). Similarly, OpenMathInstruct-2’s GPQA accuracy fluctuates with increased training data, suggesting that simply adding more data does not always lead to better reasoning abilities. These observations imply that data quality and diversity matter more than data volume when training models for complex reasoning.

\begin{table}[t]
\footnotesize
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\centering
\caption{Performance breakdown of Figure~\ref{fig:scaling_base} by benchmark (MATH, GPQA, and MMLU-Pro), where the highest accuracy per data size is underlined.
% Math-specific datasets such as NuminaMath and OpenMathInstruct-2 perform well on MATH but poorly on on non-math specific tasks (GPQA and MMLU-Pro). \dataname generalizes well across all three tasks, suggesting high-quality and diverse training data.
}
\vspace{1mm}
\begin{tabular}{lcccc}
\toprule
\textbf{SFT Dataset} & \textbf{500K} & \textbf{1M} & \textbf{1.5M} & \textbf{2.8M} \\
\midrule
\multicolumn{5}{c}{\textit{MATH}} \\
\midrule
WebInstruct              & 9.60  & --     & 11.65 & 14.46 \\
% NuminaMath               & 49.53 & --     & --     & --     \\
OpenMathInstruct-2               & \underline{50.83} & \underline{54.58} & \underline{56.47} & \underline{59.25} \\
NaturalReasoning     & 45.17 & 48.55 & 52.49 & 55.55 \\
\midrule
\multicolumn{5}{c}{\textit{GPQA}} \\
\midrule
WebInstruct              & \underline{29.02} & --    & 25.37 & 26.12 \\
% NuminaMath               & 13.84 & --     & --     & --     \\
OpenMathInstruct-2               & 25.60 & 27.31 & 27.23 & 26.45 \\
NaturalReasoning     & 26.64 & \underline{29.91} & \underline{31.77} & \underline{30.13} \\
\midrule
\multicolumn{5}{c}{\textit{MMLU-Pro}} \\
\midrule
WebInstruct              & 29.44 & --     & 35.17 & 37.54 \\
% NuminaMath               & 42.36 & --     & --     & --     \\
OpenMathInstruct-2               & 32.16 & 34.03 & 34.30 & 31.99 \\
NaturalReasoning     & \underline{43.47} & \underline{45.16} & \underline{45.43} & \underline{46.71} \\
\bottomrule
\end{tabular}
\label{tab:scaling_base_breakdown}
\end{table}

% \subsection{From the instruct model}
% In addition to experiments finetuning the base model with \dataname, we investigate whether it can also further improve the reasoning capacity of the instruct models. Table~\ref{tab:distill-instruct} shows a comparison of using 50k samples from NuminaMath, MAmmoTH2, and \dataname. 

% \begin{table}[h]
%     \centering
%     \small
%     \begin{tabular}{lccc}
%         \toprule
%         \textbf{Dataset} & \textbf{MATH} & \textbf{GPQA} & \textbf{MMLU-Pro} \\
%         \midrule
%         NuminaMath & 56.25 & 31.99 & 46.84 \\
%         MAmmoTH2 & X & 32.59 & 52.88 \\
%         \dataname & X & 32.37 & 52.79 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Should we keep this?}
%     \label{tab:distill-instruct}
% \end{table}



% \section{Elicit Long Chain-of-Thought}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{fig/r1_distill.pdf}
%     \caption{
%     % R1-distilled results on GPQA-Diamond. The R1-distilled baseline is obtained by distilling DeepSeek-R1 to finetune Llama-3.3-70B-Instruct. The reported baseline uses a curated set of 800k questions, as reported in \citet{guo2025deepseek} whereas our baseline uses questions from \texttt{science-reasoning-15k}. Despite using 50x less data, our R1-distilled model is able to outperform the r baseline.
%     R1-distilled model performance on GPQA-Diamond. Our baseline (distilled from \texttt{science-reasoning-15k}) outperforms the reported baseline \cite{guo2025deepseek} (distilled from 800k questions) despite using 50x less data. Results are shown for the reported baseline and our baseline at different maximum token lengths during inference time.
%     }
%     \label{fig:r1_distill}
% \end{figure}

% % WEIZHE'S ORIGINAL FIRST PARAGRAPH:
% % Following the emergence of OpenAI-O1, Recent studies show that by allowing the model to think for longer by generating extended CoT, it is able to solve questions that it previously cannot be solved using short CoT. To show that questions from \dataname are difficult enough and has the potential to let model think for longer, we try distilling the recent Deepseek-R1 model to Llama3.3-70B-Instruct using SFT. 

% In addition to the emergence of OpenAI-o1 and DeepSeek-R1, several studies suggest that simpler tasks require fewer steps while complex tasks benefit  significantly from longer inference sequences or longer CoTs \cite{jin2024impact}; in encouraging the model to think for longer, it is able to solve questions that it previously could not. Motivated by this, we investigate whether the questions in \dataname are complex enough to benefit from longer CoTs from a stronger reasoning model. We do so by distilling the recent Deepseek-R1 model to Llama3.3-70B-Instruct under the assumption that performance can greatly benefit from this stronger teacher model. 

% % We use our dataset to SFT, obtaining our R1-distilled Llama and compare this to the R1-distilled LLama reported in \cite{guo2025deepseek}, which uses a curated set of 800k reasoning and non-reasoning data.

% % 1) our questions benefit more from a stronger reasoning model, 2) distill on our questions are better than 800k questions. 

% % We also show that questions from \dataname could elicit long CoT responses which could benefit distilling the reasoning performance from a strong reasoner to a weaker student model. 

% % \wz{sry, i'm thinking how could we better motivate this experiment... cite o1 etc} \jane{I tried to write something but now I don't feel like it makes sense...}


% We use SGLang \citep{zheng2023sglang} to generate responses from DeepSeek-R1 to questions from \texttt{science-reasoning-15k}. Resulting response lengths range from 400 to 15.5k tokens with an average length of 4710 tokens. We train Llama-3.3-70B-Instruct using supervised finetuning to 
% % verify the quality of the question. 
% show that our questions are challenging enough to potentially elicit long CoT from models.
% We only keep responses with 14,336 tokens or less as training examples due to compute constraints. 
% To keep the evaluation consistent with the setting used in \citet{guo2025deepseek} for GPQA-Diamond, we report pass@1 averaged across $n=16$ samples. Each sample is generated using temperature=$0.6$, top\_p=$0.95$. 

% The results are provided in \autoref{fig:r1_distill}, which shows that increasing the  max token length during inference does indeed improve performance, but beyond a point has diminishing returns. Furthermore, we compare our R1-distilled Llama to that of \citet{guo2025deepseek}, which uses a curated set of 800k reasoning and non-reasoning data. Our distilled model outperforms the results reported in \citet{guo2025deepseek} despite using 50x less training data. These results highlight the effectiveness of questions from \dataname when paired with long-form reasoning responses. 





\section{Reference Answer Usefulness}
\label{sec:ref_ans_use}



% \wz{TODO:List example questions in the appendix for sample extraction for question with answer and without answer.}
We demonstrate the potential usefulness of reference answers using questions from \texttt{science-reasoning-15k}. We remove the questions that we are not able to extract a reference answer for and conduct a comparison to understand the utility of reference answers. We fine-tune the Llama3.1-8B-Instruct model using data filtered by final answer verification and compare to a model trained on the unfiltered data.


% \subsection{Training Details}
For final answer verification, we use the prompt in Appendix \autoref{tab:prompt_self_score_with_ref} that prompts the model to judge whether the generated response using Llama3.3-70B-Instruct is in line with the reference final answer, using CoT reasoning. For training data filtering, we only keep the responses that have received a ``Yes'' final judgement. The training setup includes a learning rate of $1e^{-6}$, a batch size of 64, and training for three epochs, with checkpoints saved every 100 steps for the unfiltered experiment and 50 steps for the filtered experiment due to much smaller data size.

% \subsection{Results}
The results are shown in \autoref{tab:ref_ans_filter}. Filtering training data using reference answers leads to better performance despite a smaller training set. The filtered dataset contains 7,646 examples, significantly fewer than the 12,349 examples in the unfiltered dataset, yet achieves a higher score on both GPQA-Diamond (32.15 vs. 31.82) and MMLU-Pro (50.06 vs. 49.92). This suggests that higher-quality training data outweighs raw data quantity.

% The improvements are particularly notable given that the unfiltered data provides no measurable gain over the baseline on MMLU-Pro (31.82). This indicates that introducing lower-quality or noisy data can dilute training effectiveness, while filtering improves signal quality, leading to better model generalization. These results highlight the value of curating training data based on reference answers to enhance reasoning capabilities, even in non-mathematical domains.


\begin{table}[t]
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\caption{
% SFT results using the reference answers to filter. We finetune Llama3.1-8B-Instruct on a subsample of unfiltered data from \dataname and a filtered version of this set, where questions are excluded if the distilled response from Llama3.3-70B-Instruct does not agree with the reference final answer. The filtered version performs better despite being much smaller, indicating the higher quality of our reference answers.
SFT results using reference answer filtering. We fine-tune Llama3.1-8B-Instruct on both an unfiltered subsample of \texttt{science-reasoning-15k} and a filtered version, where questions are excluded if Llama3.3-70B-Instruct's response disagrees with the reference answer. Despite its smaller size, the filtered set performs better, highlighting the usefulness of our reference answers.
}
\label{tab:ref_ans_filter}
\begin{tabular}{llll}
\toprule
           & \textbf{Training size}             & \textbf{GPQA}$_\text{diamond}$                 & \textbf{MMLU}$_\text{pro}$                       \\

                          
%            \midrule
% \multicolumn{4}{l}{\textit{Training Llama3.1-8B-Instruct}}     
% \\
\midrule
Llama3.1-8B-Inst   & \multicolumn{1}{r}{--}                        & \multicolumn{1}{r}{31.82} & \multicolumn{1}{r}{49.79} \\
Unfiltered SFT & \multicolumn{1}{r}{12349} & \multicolumn{1}{r}{31.82} & \multicolumn{1}{r}{49.92} \\
Filtered SFT  & \multicolumn{1}{r}{7646}  & \multicolumn{1}{r}{32.15} & \multicolumn{1}{r}{50.06} \\
\bottomrule
\end{tabular}
\end{table}






% \subsection{Basic Statistics}
% We compare different approaches to derive ``ground-truth" answer: majority voting, self-scoring, and using off-the-shelf reward models.


\section{Unsupervised Self-Training}
\label{sec:unsup_self_train}




% \wz{Add RMs, self-scoring, self-consistency to construct DPO pairs}
% \wz{xian could you add some details in the Appendix regarding how we sample the responses?}
Since open-ended reasoning questions are difficult to evaluate using exact match with reference answers, we explore whether our dataset can facilitate self-training through either strong external reward models or self-reward mechanisms~\cite{yuanself}.

% self-training strategies based on self-consistency~\cite{prasad2024selfconsistencypreferenceoptimization} and self-reward mechanisms~\cite{yuanself}.  

We use the \texttt{science-reasoning-15k} questions and sample 32 responses per question using the Llama3.1-8B-Instruct model. We evaluate different unsupervised self-training methods based on Rejection-based sampling Fine-Tuning (RFT) and Direct Preference Optimization (DPO)~\cite{rafailov2023direct} across GPQA-Diamond and MMLU-Pro, focusing on the effectiveness of different reward scoring strategies. Each approach relies on sampling 32 candidate responses per question, followed by selecting responses based on reward scores. RFT employs rejection sampling, selecting the highest-scoring response for SFT training, while DPO constructs training pairs using both the highest and lowest-scoring responses. For external reward models, we consider \texttt{Qwen2.5-Math-RM-72B}~\cite{yang2024qwen25mathtechnicalreportmathematical} and \texttt{INF-ORM-Llama3.1-70B}\footnote{https://huggingface.co/infly/INF-ORM-Llama3.1-70B}.  

In addition to utilizing external reward models, we explore a self-rewarding framework where the model evaluates and assigns rewards to its own generated responses. Specifically, we consider the following self-rewarding strategies:
\begin{itemize}
    \item \textbf{Self-consistency}: Inspired by prior work such as \citet{prasad2024self}, the best response is selected based on response frequency, while the worst response is chosen randomly. To determine frequencies, we extract final answers formatted as \verb|\boxed{|X\verb|}| and compute their occurrence counts. Responses without a clearly extractable final answer are filtered out.
    \item \textbf{Self-scoring}: The model is prompted with both the question and a response and asked to assess whether the response is valid. We compute a reward score as the difference between the log probability of a “yes” judgement and a “no” judgement. The detailed prompt used for this evaluation is provided in \autoref{tab:prompt_self_score}.
    \item \textbf{Self-scoring with filtering}: on top of self-scoring, when applying RFT or DPO, we introduce an additional filtering mechanism to improve training quality. Specifically, for RFT, if the highest-ranked response has a self-score below zero, it is discarded. For DPO, if the preferred response in a pair has a self-score below zero, the pair is removed from training.
\end{itemize}



 

We train Llama3.1-8B-Instruct using RFT data and DPO data constructed through these four methods. The training setup includes a learning rate of $1e^{-6}$, a batch size of 64, and training for three epochs, with checkpoints saved every 50 steps.

% We evaluate our models on GPQA-Diamond and MMLU-Pro, taking the average performance across the last three checkpoints as the final reported result.  

\subsection{Results}
\begin{table}[t]
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\caption{Unsupervised self-training results. We employ RFT and DPO training of Llama3.1-8B-Instruct, using various reward scoring strategies. Self-reward methods generally exceed strategies requiring an external reward model, and are further improved when filtering out samples that are predicted as low-scoring. All self-training strategies improve over the instruction-tuned baseline, showing the utility of our dataset for self-training. }
\label{tab:unsup_self_train}
\begin{tabular}{lccc}
\toprule
\textbf{Model}                & \textbf{GPQA}$_\text{diamond}$                     & \textbf{MMLU}$_\text{pro}$        & \textbf{AVG}                    \\
\midrule
Llama3.1-8B-Instruct & \multicolumn{1}{r}{31.82} & \multicolumn{1}{r}{49.79} & \multicolumn{1}{r}{40.81} \\
\midrule
\multicolumn{3}{l}{\textit{RFT training using external Reward model}}                               \\
\midrule
INF-ORM-Llama3.1-70B              &        \multicolumn{1}{r}{32.66}	                          &            \multicolumn{1}{r}{50.95}   & \multicolumn{1}{r}{41.81}                   \\
Qwen2.5-Math-RM-72B                 &   \multicolumn{1}{r}{34.18}                               &   \multicolumn{1}{r}{49.84}           &   \multicolumn{1}{r}{42.01}                      \\
\midrule

\multicolumn{3}{l}{\textit{RFT training using self-reward}}                                         \\
\midrule
Self-consistency     & \multicolumn{1}{r}{34.18}        & \multicolumn{1}{r}{49.83}    & \multicolumn{1}{r}{41.91}      \\
Self-score           & \multicolumn{1}{r}{34.34} & \multicolumn{1}{r}{50.36}
 & \multicolumn{1}{r}{42.35}   \\
Self-score-filtered           & \multicolumn{1}{r}{\underline{35.02}} & \multicolumn{1}{r}{50.06}  & \multicolumn{1}{r}{42.54}  
\\
\midrule
\multicolumn{3}{l}{\textit{DPO training using external Reward model}}                               \\
\midrule
INF-ORM-Llama3.1-70B              &        \multicolumn{1}{r}{33.50}	                          &            \multicolumn{1}{r}{\underline{52.74}}       & \multicolumn{1}{r}{43.12}                 \\
Qwen2.5-Math-RM-72B                 &   \multicolumn{1}{r}{30.13}                               &   \multicolumn{1}{r}{49.17}     & \multicolumn{1}{r}{39.65}                          \\
\midrule
\multicolumn{3}{l}{\textit{DPO training using self-reward}}                                         \\
\midrule
Self-consistency     & \multicolumn{1}{r}{30.81}        & \multicolumn{1}{r}{48.60}     & \multicolumn{1}{r}{39.71}     \\
Self-score           & \multicolumn{1}{r}{34.34} & \multicolumn{1}{r}{52.11}  & \multicolumn{1}{r}{43.22}  
\\
Self-score-filtered           & \multicolumn{1}{r}{\underline{35.02}} & \multicolumn{1}{r}{52.31}  & \multicolumn{1}{r}{\underline{43.67}}  
\\
\bottomrule
\end{tabular}
\end{table}
The results are given in \autoref{tab:unsup_self_train}. 
% Overall we make the following observations.

\paragraph{Self-training improves performance over the baseline.} Llama3.1-8B-Instruct, serving as the baseline, achieves an average score of 40.81 across GPQA-Diamond and MMLU-Pro. Almost all self-training methods lead to improvements, demonstrating the effectiveness of fine-tuning on high-quality model-generated responses. 


\paragraph{Self-reward methods are highly competitive, often surpassing external reward models.} While using external reward models, such as INF-ORM-Llama3.1-70B and Qwen2.5-Math-RM-72B, could outperform the baseline, self-reward methods achieve comparable or even superior results. Notably, self-score-filtered SFT and self-score-filtered DPO deliver the best performance on GPQA-Diamond (35.02), with self-score-filtered DPO achieving the highest overall score (43.67 AVG). These results highlight that self-reward mechanisms can effectively guide self-training without relying on external reward models.

\paragraph{Self-score filtering further enhances performance by improving training data quality.} Among self-reward methods, applying simple filtering improves results across both RFT and DPO. In RFT, self-score-filtered (42.54 AVG) outperforms unfiltered self-scoring (42.35 AVG), while in DPO, self-score-filtered (43.67 AVG) surpasses unfiltered self-scoring (43.22 AVG). This suggests that filtering out low-confidence responses strengthens self-training by reducing noise in the training data.

% DPO using INF-ORM-Llama3.1-70B achieves the highest MMLU$_\text{pro}$ score, but self-reward performs better overall.

% While DPO with INF-ORM-Llama3.1-70B achieves the highest MMLU$_\text{pro}$ score (52.74), it falls short on GPQA$_\text{diamond}$ (33.50). In contrast, DPO with self-score filtering achieves a more balanced performance, obtaining the best overall average (43.67). This suggests that while external reward models can excel in specific benchmarks, self-reward methods provide more consistent improvements across tasks.







% Weizhe did:
% self-scoring with reference COT+judgement
% self-scoring without reference


% logp(yes), logp(no)
% logp(##judgement: yes)
% logp(##judgement: no)

% logp(\boxed{yes})
% logp(yes) -14
% logp(##judgement: yes): -2



% \subsection{Self-Consistency}



% \subsection{Self-scoring}


\section{Related Work}
% \citep{setlur2024rl} 

% \paragraph{Augmented Reasoning Data.} In light of the ever-increasing expectations for LLM reasoning capabilities and projections of exhausting public language data by as early as 2030 \cite{villalobos2022will}, synthetic data has emerged as a promising solution for pushing performance improvements and enabling new skills. Some approaches bootstrap new data by augmenting or reformatting existing seed data. For example, STaR \cite{zelikman2022star} augment with new CoT rationales, MetaMath \cite{yumetamath} rewrites the questions in MATH and GSM8K in several ways, and generation methods like Evol-Instruct \cite{xu2023wizardlm} and Code Evol-Instruct \cite{luowizardcoder} iteratively make the original data more challenging. However, these techniques depend on existing high-quality seed data with question-answer pairs, often requiring the answer to be easily verifiable (e.g., a single number). 

% \paragraph{Synthetic Reasoning Data.} Other techniques such as that of OpenMathInstruct-2 \cite{toshniwal24openmathinstruct}, NuminaMath \cite{numina_math_datasets}, and Xwin-Math \cite{li2024common}, and Self-Instruct \cite{wang2023self} generate new question-answer pairs from a few seed examples using an LLM but scaling to new domains or new content not previously seen in training remains a significant challenge. MMIQC \cite{liu2024augmenting} combines existing datasets and pre-training content from OpenWebMath \cite{pasteropenwebmath}, creating question-answer pairs using the highest ranked answer. However, few measures are taken to curate and ensure the quality of the questions or answers and the resulting dataset is specific to a single domain, similar to the previously mentioned works. MAmmoTH2 \cite{yue2024mammoth2} on the other hand harvests question-answer pairs from exam problems in Common Crawl, spanning multiple domains in math and science. 



\paragraph{Synthetic Reasoning Data.} Synthetic data has emerged as a promising solution for improving performance and enabling new skills. Some approaches bootstrap new data from existing annotated data (e.g., STaR \cite{zelikman2022star} augments with new CoT rationales and MetaMath \cite{yumetamath} rewrites the questions in MATH and GSM8K in several ways), but these techniques rely on the existence of a high-quality dataset. Other techniques such as that of OpenMathInstruct-2 \cite{toshniwal24openmathinstruct}, NuminaMath \cite{numina_math_datasets}, Xwin-Math \cite{li2024common}, and Self-Instruct \cite{wang2023self} generate new data from only a few seed examples using an LLM but scaling to new domains or new content not previously seen in training remains a significant challenge. MMIQC \cite{liu2024augmenting} parses QA pairs from Mathematics Stack Exchange, using the highest-ranked answer, but few measures are taken to curate for quality and the resulting dataset is  also specific to the math domain. Similar to our work, WebInstruct \cite{yue2024mammoth2} harvests question-answer pairs from pre-training corpora and spans multiple domains, but is dependent on carefully crafted rule-based filters. 



\paragraph{Unsupervised Self-training} Most prior works typically depend on human-annotated (gold) final answers \citep{zelikman2022star, chen2024self, pang2024iterative} or the use of an external reward model \citep{singh2023beyond, dong2023raft}. However, manually annotating or verifying final answers is particularly resource-intensive for complex, multi-step problems and training effective reward models for reasoning often requires human evaluation of LLM outputs \citep{cobbe2021training, uesato2022solving, lightman2023let}, making it similarly costly. Like works such as \citet{she2024mapo, yuan2024selfrewarding, rosset2024direct, viethoangtranduong}, our work explores self-training even in the absence of gold labels and does not limit itself to questions with short, easily verifiable targets.


% Metamath bootstraps (augments)
% GAIR-Abel (reformats)
% Xwin-Math (generate new)
% MMIQC (uses Mathematics Stack Exchange, take highest voted answer)

% Simulated environment for code (Shypula et al 2023, InterCode, )
% Self-instruct (needs seed)
% Code Evol-Instruct 
% OSS-Instruct
% star

\section{Conclusion}
We present \dataname, a dataset of 2.8 million questions for enhancing LLM reasoning capabilities. Our questions are challenging,  requiring more deliberate thinking than existing datasets. The dataset covers diverse reasoning problems across multiple domains including math, physics, computer science, economics, social sciences, etc. Using questions from \dataname in distillation experiments, we observe consistent improvement on reasoning benchmarks when scaling  the data size. 
% Using only 15k examples, distillation with an even more powerful teacher model, DeepSeek-R1, enables superior performance
% We also compare different approaches to evaluate the correctness of solutions.    
% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}
% \textbf{Do not} include acknowledgements in the initial version of the paper submitted for blind review.
We also demonstrate that \dataname is effective for enabling LLM unsupervised self-training using external reward models or self-rewarding.
% We also demonstrate that our reference answers can be useful for further curation and that \dataname is effective for enabling LLM unsupervised self-training even in the absence of reference answers.


\section*{Impact Statement}

This paper seeks to improve reasoning capabilities of large language models through leveraging pretraining corpora. While our efforts are focused on curating high-quality, diverse data, models trained using this data may exhibit undesirable behavior not examined in our work. Therefore, comprehensive evaluation would be needed to evaluate and address any potential pre-existing or existing biases in LLMs which leverage this data.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Clustering Results}

We present the results of our clustering in Figure~\ref{fig:topic_clusters}. The procedure for producing this clustering is described in Section~\ref{sec:clustering}.

\begin{figure*}[ht]
    \centering
    \subfigure[WebInstruct]{\includegraphics[width=0.48\textwidth]{fig/mammoth_topics.png}}
    % \subfigure[Topic distribution comparison]{\includegraphics[width=0.55\textwidth]{mamoth_ourdata_topic_comparision.png}}
    \subfigure[\dataname]{\includegraphics[width=0.48\textwidth]{fig/ourdata_topics.png}}
    % \begin{subfigure}{0.5\textwidth}
    %     \centering
    %     \includegraphics[width=0.4\linewidth]{mamoth_topics_kmeans_new.png}
    %     \caption{MAmmoTH2}
    %     \label{fig:ourdatacluster}        
    % \end{subfigure}
    % \begin{subfigure}{0.5\textwidth}
    %     \centering
    %     \includegraphics[width=0.4\linewidth]{ourdata_topics_kmeans_new.png}
    %     \caption{\dataname}
    %     \label{fig:openmath_cluster}        
    % \end{subfigure}
    \caption{Topic clustering of WebInstruct and \dataname.}
    \label{fig:topic_clusters}
\end{figure*}


\section{Example questions}
\label{app:example_questions}
Example questions with single answer, short answer, long answer are shown in \autoref{tab:example_questions}.

\begin{table*}[!ht]
    \centering
    \footnotesize
  % \setlength\tabcolsep{3.5pt}
  % \renewcommand{\arraystretch}{0.1}
  \caption{Example questions with single word, short, and long reference answers.}
  \begin{tabularx}{\textwidth}{@{}X@{}}
    \toprule
    \textbf{Example questions with single word answer} \\ \midrule
    (1) You have 5 cards with letters and numbers on either side. The rule is: If the letter on one side of a card is a vowel, the number on the other side is odd. What is the minimum number of cards you need to turn over to prove or disprove this rule?\\
    (2) What is the approximate number of grapes required to produce one bottle of wine, considering the conversion factors and variability in grape yields, and how does this relate to the overall wine production process? \\
    (3) A company is facing financial difficulties and is in danger of not meeting its obligations. In an effort to secure a bridge loan, the company's management decides to manipulate its financial statements by not reversing cancelled orders, thereby overstating its accounts receivable. This action allows the company to collateralize the loan and secure the necessary funding. However, this practice is in direct violation of revenue recognition rules. Analyze this situation and determine whether the company's actions constitute blatant fraud. Be sure to discuss the technical and ethical implications of such actions. \\
    (4) Evaluate the definite integral $\int_{0}^{\pi} \cos^2(x)\sin^7(x)dx$ using an appropriate substitution method and provide a step-by-step solution.\\
    (5) Speed of boat in still water is 10kmph. If it travels 24km downstream, 16km upstream in the same amount of time, what is the speed of the stream?
     \\
     \midrule
    \textbf{Example questions with short answer} \\ \midrule
    (1) What are the parameters that need to be estimated in the general equation of an ellipse, and how do the variables $x$ and $y$ differ from the constants $a$ and $b$ in this context? Provide a detailed explanation of your answer, including the roles of $a$, $b$, $x$, and $y$ in defining the ellipse.\\
    (2) Given a company's historical data on revenues, working capital, and net capital expenditures, is it acceptable to forecast change in working capital / net capex by regressing (linear) historical data on revenues? What are the limitations of this approach, and what alternative methods could be used to improve the accuracy of the forecast?\\
    (3) Solve the inequality 4(3w+4) $\geq$ 4(2w+12) using interval notation, and express the solution in set-builder notation.\\
    (4) Given an image with shape [1,28,28], what will be the shape of the output of a convolution layer with 10 5x5 kernels (filters) without padding? Assume the image dimensions follow the CHW (Channel, Height, Width) format.\\
     (5) A gas bubble, from an explosion under water, oscillates with a period T proportional to $P^a*d^b*E^c$. Where `P' is the static pressure, `d' is the density of water, and `E' is the total energy of the explosion. Find the values of a, b, and c, and explain the physical reasoning behind your answer.\\
     \midrule
     \textbf{Example questions with long answer} \\
     \midrule
     (1) Analyze the impact of errors in data analysis on the validity of research findings, using the example of Reinhart and Rogoff's research on the relationship between debt and economic growth. How do such errors affect the development of economic policies, and what are the implications for the field of economics?\\
     (2) Prove that the relation $R = \{(1,2), (1,3), (1,4), (2,1), (2,3), (2,4), (3,1), (3,2), (3,4), (4,1), (4,1), (4,3)\}$ is not transitive. Use the definition of transitivity to show that there exist elements $x_0, y_0, z_0$ such that $(x_0, y_0) \in R$ and $(y_0, z_0) \in R$ but $(x_0, z_0) \notin R$.\\
     (3) Astronomers use a new method to detect asteroids and measure their velocity. The method involves detecting energized electromagnetic waves at two different Earth times and using the relative motion of Earth with respect to the asteroid to calculate the velocity. Suppose the Earth spins about 360 degrees within 24 hours, and the asteroid moves in a straight path with respect to other stellar objects. If the angle between the flat Earth surface at point A0 and the direction of asteroid observable is ?, and the angle between the flat Earth surface at point A and the direction of asteroid observable is ?, derive an expression for the relative velocity of Earth with respect to the asteroid at position A0 and A. Use the relativistic Doppler formula to relate the frequencies of the electromagnetic waves detected at points A0 and A. Assume the velocity of the asteroid does not change within the time interval of two detections, and estimate the value of the asteroid's velocity. \\
     (4) Bob, a resident outside the US, has purchased a mobile app subscription from a California-based business for under \$200. However, due to a showstopper bug, the app is unusable for its main purpose. Bob has attempted to report the issue to the business's support team without success. Discuss the practicalities of Bob suing the California-based business from abroad, considering the requirements of Small Claims court in California and the potential application of consumer protection laws from Bob's home country. How might Bob's approach differ if he were to pursue litigation in a 'normal' court versus Small Claims court, and what are the implications of using an online store like Apple for the purchase?\\
     (5) Describe the differences in flow resistance between laminar and turbulent flows in a tube, and explain how the velocity profile changes in the transition from laminar to turbulent flow. Be sure to include the role of viscosity, density, and eddy viscosity in your answer.\\
\bottomrule
    \end{tabularx}
    \label{tab:example_questions}
\end{table*}


\section{Data Creation Details}

\subsection{Generation}
We use vllm for all generations. For annotating documents and  synthesizing questions, we use greedy decoding (i.e. temperature=$0$). For response generation for each question in \dataname, we use temperature=$0.7$ top\_p=$0.9$. Responses used in unsupervised self-training experiments are sampled using temperature=\{0, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.2\} to encourage response diversity.


\section{Prompts}
\label{app:prompts}
% The prompt we used for annotating reasoning from the document, generating a question, and reference answer are shown in \autoref{tab:prompt_annotate_reasoning_part1} and \autoref{tab:prompt_annotate_reasoning_part2}.
The prompt we used for annotating reasoning from the document is shown in \autoref{tab:prompt_annotate_reasoning}.
We additionally provide the prompt for annotating question validity and difficulty (\autoref{tab:prompt_annotate_quality}), the prompt used to check if a generated response matches the reference (\autoref{tab:prompt_self_score_with_ref}), and the prompt for self scoring (\autoref{tab:prompt_self_score}).


% \paragraph{Prompt for Annotating Reasoning and Synthesizing Questions}
% \paragraph{Prompt for Evaluating Reference Answer Matching}
% \paragraph{Prompt for Annotating Question Quality}
% \paragraph{Prompt for Self-scoring}



\begin{figure}[th!]
\centering
\small
\begin{tcolorbox}[colback=green!10!white, % Background color
                  colframe=green!30!white, % Frame color
                  width=0.99\textwidth, % Width of the tcolorbox
                  arc=4mm, % Radius of the rounded corners
                  auto outer arc,
                   sharp corners=south, % Ensure corners are well-rounded
                  fontupper=\ttfamily, % Use monospaced font for readability
                  before upper={\setlength{\parindent}{0em}} % Ensure consistent indentation
                  ]
\begin{lstlisting}[basicstyle=\ttfamily\tiny, breaklines=true,mathescape=true]
Evaluate the text below according to the scoring instruction and criteria. If the scores are high on each axis, derive an exam question following the instructions.

## Scoring Instruction
1. Evaluate the text on each criteria step by step. Provide your honest answer to each sub-question. If the answer to a sub-question is a confident Yes, add or subtract the points corresponding to the criteria.
2. Keep track of the running points from each criteria to get the total score.
3. Summarize your final evaluation results in a valid JSON object following the instruction below.

### Scoring Criteria

**Criteria 1: Problem Completeness**
* The content does not have clear main question, or enough clues to derive the correct answer. (0 point)
* The content includes a main question, and enough clues to derive the correct answer. (+1 point)
* The text shows evidence of engagement and discussion among multiple authors, including proposing answers, evaluating and reflecting on answers, responding to critiques, revising and editing answers. (+1 point)

**Criteria 2: Problem Complexity and Technical Depth**

* The difficulty of the content is college-level or below. (0 point)
* The difficulty of the content is graduate-level or above, and only domain experts can understand. (+1 point)
* The question being discussed is so challenging that even highly skilled non-experts would not be able to fully understand the question or provide a correct answer, even after spending 30 minutes searching the internet or reading up literature. (+1 point)

**Criteria 3: Technical Correctness and Accuracy**

* The text contains significant technical errors or inaccuracies. (-1 point)
* The text demonstrates some technical correctness, but with notable flaws or omissions (e.g., incorrect units, incomplete derivations). (0 point)
* The text demonstrates technical correctness, with some minor flaws or omissions (e.g., minor algebraic errors, incomplete explanations). (+0.5 point)
* The text demonstrates high technical correctness, with clear and accurate explanations (e.g., precise definitions, complete derivations). (+0.5 point)
* The text exemplifies exceptional technical correctness, with rigorous and precise explanations (e.g., formal proofs, precise calculations). (+1 point)

**Criteria 4: Thinking and Reasoning**

* The text lacks any evidence of thinking or reasoning. (-1 point)
* The text demonstrates some basic thinking and reasoning (+0.5 point), such as:
        + A straightforward application of a known technique.
        + A simple analysis of a problem.
* The text demonstrates some thinking and reasoning (+0.5 point), such as:
        + A consideration of multiple approaches to a problem.
        + A discussion of the trade-offs between different solutions.
* The text demonstrates significant thinking and reasoning (+1 points), such as:
        + Multi-step reasoning chains to solve a complex problem.
        + Advanced reasoning patterns often used in specialized science domains.
* The text exemplifies exceptional thinking and reasoning (+1 points), such as:
        + A highly innovative and creative approach to solving a complex problem in specialized domains.
        + Combining multiple reasoning and thinking techniques, with novel abstraction of the problem.

## Instruction on Exam Question and Final Report
* Step 1. If BOTH Criteria 1 and Criteria 2 scores above zero, transform the original question being discussed to an exam question. The question should focus on problem-solving and there should exist a correct answer to the question. The question should be descriptive, i.e. use the details and notations from the original text as much as possible. The question must be self-contained, concrete, well-defined, i.e. it should NOT contain any missing information nor should it contain any ambiguity or subjectiveness.
* Step 2. If BOTH Criteria 1 and Criteria 3 scores above zero, determine whether the text contains a correct solution to the question or not. If the discussion DOES contain a correct solution, try to extract the gists and important details from the correct answer. Then use those key information to derive a correct answer to the question. If there is a single final answer, conclude the correct answer with: "Therefore, the final answer is: \\boxed{{answer}}.", where [answer] is the number or expression that is the final answer.
* Step 3. If BOTH Criteria 2 and Criteria 4 have non-zero scores, write down a list of critical knowledge and reasoning steps which are required to derive a correct answer to the exam question. Each item in the list must be descriptive, specific and concrete.
* Step 4. Label the question difficulty with Easy, Medium, Hard, and Extra Hard.

Finally, copy all your analysis from above into a JSON object at the end of the final report. The JSON object should contain the following attributes:
- `"scores"`: a list of dictionary entries, where each entry contains the criteria name and corresponding score on that criteria.
- `"exam\_question"`: a string recording the full exam question derived from Step 1 analysis. DO NOT omit any details. If the scores for Criteria 1 and Criteria 2 are low and no exam question can be made out of the text, return an empty string.
- `"correct\_answer"`: a string recording the correct answer derived from Step 2. If the discussion does not contain a correct answer, return an empty string.
- `"knowledge\_and\_reasoning\_steps"`: a list of strings, where each entry copying the critical piece of knowledge or important reasoning steps derived from Step 3. If either Criteria 2 or Criteria 4 has score zero, return an empty list.
- `"question\_difficulty"`: a string recording the difficulty of the question derived from Step 4.

### Text
{text}

\end{lstlisting}

\end{tcolorbox}
\caption{Prompt for annotating reasoning from the document, generating a question and reference answer.}
\label{tab:prompt_annotate_reasoning}
\end{figure}


\begin{figure}[th!]
\centering
\small
\begin{tcolorbox}[colback=green!10!white, % Background color
                  colframe=green!30!white, % Frame color
                  width=0.99\textwidth, % Width of the tcolorbox
                  arc=4mm, % Radius of the rounded corners
                  auto outer arc,
                  sharp corners=south, % Ensure corners are well-rounded
                  fontupper=\ttfamily, % Use monospaced font for readability
                  before upper={\setlength{\parindent}{0em}} % Ensure consistent indentation
                  ]

\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
Your task is to verify and improve the quality of a question. 

A valid question must meet the following criteria:
* The question should contain a problem to be solved, instead of only presenting statements.
* The question should be well-defined and self-contained, i.e. have all the necessary information to derive an answer.
* The question should be specific and clear. There should be one correct answer to the question. 
* The question should not refer to external resources, such as figures, videos, etc.
* To derive an answer, multi-step reasoning and recalling relevant knowledge is required.
* The difficulty should be graduate-level or above.
* The question can contain LaTex but it should be correct.
If a question does not meet any of the criterion above, revise it till it meets all the criteria.

IMPORTANT: Put your final answer in a JSON object with two fields:
- "question_quality_score": rate how well the question meets all the criteria, on a scale of 1 to 10.
- "improved_question": revised question which will meet the criteria better and thus has a higher question quality score. 

Question:
{question}
\end{lstlisting}
\end{tcolorbox}
\caption{Prompt for annotating quality scores.}
\label{tab:prompt_annotate_quality}
\end{figure}


% \begin{figure}[th!]
% \centering
% \small
% \begin{tcolorbox}[colback=green!10!white, % Background color
%                   colframe=green!30!white, % Frame color
%                   width=0.99\textwidth, % Width of the tcolorbox
%                   arc=4mm, % Radius of the rounded corners
%                   auto outer arc,
%                   sharp corners=south, % Ensure corners are well-rounded
%                   fontupper=\ttfamily, % Use monospaced font for readability
%                   before upper={\setlength{\parindent}{0em}} % Ensure consistent indentation
%                   ]

% \begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
% Evaluate the text below according to the scoring instruction and criteria. 
% If the scores are high on each axis, derive an exam question following the instructions.

% ## Scoring Instruction
% 1. Evaluate the text on each criteria step by step. Provide your honest answer to each sub-question. If the answer to a sub-question is a confident Yes, add or subtract the points corresponding to the criteria.
% 2. Keep track of the running points from each criteria to get the total score.
% 3. Summarize your final evaluation results in a valid JSON object following the instruction below.

% ### Scoring Criteria

% **Criteria 1: Problem Completeness**
% - The content does not have a clear main question or enough clues to derive the correct answer. (0 point)
% - The content includes a main question and enough clues to derive the correct answer. (+1 point)
% - The text shows evidence of engagement and discussion among multiple authors, including proposing answers, evaluating and reflecting on answers, responding to critiques, revising and editing answers. (+1 point)

% **Criteria 2: Problem Complexity and Technical Depth**
% - The difficulty of the content is college-level or below. (0 point)
% - The difficulty of the content is graduate-level or above, and only domain experts can understand. (+1 point)
% - The question being discussed is so challenging that even highly skilled non-experts would not be able to fully understand the question or provide a correct answer, even after spending 30 minutes searching the internet or reading up literature. (+1 point)

% **Criteria 3: Technical Correctness and Accuracy**
% - The text contains significant technical errors or inaccuracies. (-1 point)
% - The text demonstrates some technical correctness, but with notable flaws or omissions (e.g., incorrect units, incomplete derivations). (0 point)
% - The text demonstrates technical correctness, with some minor flaws or omissions (e.g., minor algebraic errors, incomplete explanations). (+0.5 point)
% - The text demonstrates high technical correctness, with clear and accurate explanations (e.g., precise definitions, complete derivations). (+0.5 point)
% - The text exemplifies exceptional technical correctness, with rigorous and precise explanations (e.g., formal proofs, precise calculations). (+1 point)
% \end{lstlisting}
% \end{tcolorbox}
% \caption{Prompt for annotating reasoning from the document, generating a question, and reference answer (Part 1).}
% \label{tab:prompt_annotate_reasoning_part1}
% \end{figure}


% \begin{figure}[th!]
% \centering
% \small
% \begin{tcolorbox}[colback=green!10!white, % Background color
%                   colframe=green!30!white, % Frame color
%                   width=0.99\textwidth, % Width of the tcolorbox
%                   arc=4mm, % Radius of the rounded corners
%                   auto outer arc,
%                   sharp corners=south, % Ensure corners are well-rounded
%                   fontupper=\ttfamily, % Use monospaced font for readability
%                   before upper={\setlength{\parindent}{0em}} % Ensure consistent indentation
%                   ]

% \begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
% **Criteria 4: Thinking and Reasoning**
% - The text lacks any evidence of thinking or reasoning. (-1 point)
% - The text demonstrates some basic thinking and reasoning (+0.5 point), such as:
%     - A straightforward application of a known technique.
%     - A simple analysis of a problem.
% - The text demonstrates some thinking and reasoning (+0.5 point), such as:
%     - A consideration of multiple approaches to a problem.
%     - A discussion of the trade-offs between different solutions.
% - The text demonstrates significant thinking and reasoning (+1 point), such as:
%     - Multi-step reasoning chains to solve a complex problem.
%     - Advanced reasoning patterns often used in specialized science domains.
% - The text exemplifies exceptional thinking and reasoning (+1 point), such as:
%     - A highly innovative and creative approach to solving a complex problem in specialized domains.
%     - Combining multiple reasoning and thinking techniques, with novel abstraction of the problem.

% ## Instruction on Exam Question and Final Report
% 1. If **BOTH** Criteria 1 and Criteria 2 scores above zero, transform the original question being discussed to an exam question. The question should focus on problem-solving and there should exist a correct answer to the question. The question should be descriptive, using the details and notations from the original text.
% 2. If **BOTH** Criteria 1 and Criteria 3 scores above zero, determine whether the text contains a correct solution to the question. If the discussion **does** contain a correct solution, extract the key details and summarize the correct answer.
% 3. If **BOTH** Criteria 2 and Criteria 4 have non-zero scores, write a list of critical knowledge and reasoning steps required to derive a correct answer. The list should be descriptive and specific.
% 4. Label the question difficulty with **Easy, Medium, Hard, or Extra Hard**.

% Finally, copy all your analysis into a JSON object at the end of the final report. The JSON object should contain the following attributes:

% - `"scores"`: A list of dictionary entries, where each entry contains the criteria name and corresponding score.
% - `"exam_question"`: A string recording the full exam question derived from Step 1 analysis. If no valid question can be made, return an empty string.
% - `"correct_answer"`: A string recording the correct answer derived from Step 2. If the discussion does not contain a correct answer, return an empty string.
% - `"knowledge_and_reasoning_steps"`: A list of strings, each entry copying critical knowledge or reasoning steps from Step 3. If either Criteria 2 or Criteria 4 has a zero score, return an empty list.
% - `"question_difficulty"`: A string recording the difficulty derived from Step 4.

% ### Text
% {text}

% \end{lstlisting}
% \end{tcolorbox}
% \caption{Prompt for annotating reasoning from the document, generating a question, and reference answer (Part 2).}
% \label{tab:prompt_annotate_reasoning_part2}
% \end{figure}






\clearpage

\begin{figure}[th!]
\centering
\small
\begin{tcolorbox}[colback=green!10!white, % Background color
                  colframe=green!30!white, % Frame color
                  width=0.99\textwidth, % Width of the tcolorbox
                  arc=4mm, % Radius of the rounded corners
                  auto outer arc,
                  sharp corners=south, % Ensure corners are well-rounded
                  fontupper=\ttfamily, % Use monospaced font for readability
                  before upper={\setlength{\parindent}{0em}} % Ensure consistent indentation
                  ]

\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
You are an expert evaluator tasked with deciding whether a response meets the standards of quality and correctness for general reasoning tasks. Your evaluation must consider both the quality of the reasoning process (chain of thought, CoT) and the correctness or appropriateness of the final answer.

### Evaluation Criteria:
1. **Correctness of the Final Answer**: 
   - Does the final answer align with the reference answer or the expected outcome?
2. **Quality of the Thinking Process (CoT)**:
   - Is the reasoning logical, coherent, and free from significant errors?
   - Does the reasoning support the final answer in a clear and step-by-step manner?
3. **Completeness**: 
   - Does the response adequately address all aspects of the instruction or problem?

### Input Details:
- **Instruction**: {Describe the task, problem, or instruction here.}
- **Reference Answer**: {Provide the expected or ideal outcome here.}
- **Response**: {Include the response to be evaluated, containing both the CoT and the final answer.}

### Task:
Analyze the response provided and decide if it is a "Yes" or "No" based on the following:
- **"Yes"**: The response meets the required standards for correctness, reasoning, and completeness.
- **"No"**: The response fails to meet one or more of the standards.

Provide a brief explanation of your decision, highlighting specific strengths or weaknesses in the reasoning process (CoT), the final answer, or completeness.

**Response Format**:
1. **Explanation**:  
   - **Final Answer Evaluation**: (Discuss correctness and consistency.)  
   - **Chain of Thought Evaluation**: (Discuss logic and coherence.)  
   - **Completeness**: (Assess whether the response fully addresses the instruction.)  
2. **Judgment**: Yes/No  
---

[FEW-SHOT EXAMPLES HERE]
---

### Current Input:
**Instruction**: <INSTRUCTION>
**Reference Answer**: <ANSWER>
**Response**:  
<RESPONSE>

**Evaluation**:

\end{lstlisting}
\end{tcolorbox}
\caption{Prompt used to check if a response matches the reference answer.}
\label{tab:prompt_self_score_with_ref}
\end{figure}


\begin{figure}[th!]
\centering
\small
\begin{tcolorbox}[colback=green!10!white, % Background color
                  colframe=green!30!white, % Frame color
                  width=0.99\textwidth, % Width of the tcolorbox
                  arc=4mm, % Radius of the rounded corners
                  auto outer arc,
                  sharp corners=south, % Ensure corners are well-rounded
                  fontupper=\ttfamily, % Use monospaced font for readability
                  before upper={\setlength{\parindent}{0em}} % Ensure consistent indentation
                  ]

\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
You are an expert evaluator tasked with analyzing a response to a general reasoning problem. Your goal is to determine if the response demonstrates good reasoning (CoT) and whether the reasoning makes sense overall.

### **Evaluation Criteria**:
1. **Reasoning Quality (CoT)**:
   - Does the reasoning follow a logical and coherent sequence?
   - Are the steps valid and free of major errors?
   - Does the reasoning align with standard problem-solving practices?

2. **Accuracy**:
   - Does the reasoning lead to the correct or valid conclusion based on the instruction?

3. **Clarity**:
   - Is the response clear and easy to understand?

### Input Details:
- **Instruction**: {Describe the task, problem, or instruction here.}
- **Response**: {Include the response to be evaluated, containing the chain of thought (CoT).}

### **Task**:
Analyze the response and decide if it meets the standards for correctness, reasoning quality, and clarity. Provide your judgment as either **"Yes"** (the response is good) or **"No"** (the response is not good). Then, briefly explain your decision.

**Response Format**:
1. **Judgment**: Yes/No  
2. **Explanation**:  
   - **Reasoning Quality (CoT)**: (Assess the reasoning process in detail.)  
   - **Accuracy**: (Evaluate whether the reasoning leads to the correct conclusion.)  
   - **Clarity**: (Comment on the clarity of the response.)  

---
[FEW-SHOT EXAMPLES]

---

### Current Input:
**Instruction**: <INSTRUCTION>
**Response**:  
<RESPONSE>

**Evaluation**:
1. **Judgment**: 

\end{lstlisting}
\end{tcolorbox}
\caption{Prompt for self scoring.}
\label{tab:prompt_self_score}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

