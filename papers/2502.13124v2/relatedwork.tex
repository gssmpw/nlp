\section{Related Work}
% \citep{setlur2024rl} 

% \paragraph{Augmented Reasoning Data.} In light of the ever-increasing expectations for LLM reasoning capabilities and projections of exhausting public language data by as early as 2030 \cite{villalobos2022will}, synthetic data has emerged as a promising solution for pushing performance improvements and enabling new skills. Some approaches bootstrap new data by augmenting or reformatting existing seed data. For example, STaR \cite{zelikman2022star} augment with new CoT rationales, MetaMath \cite{yumetamath} rewrites the questions in MATH and GSM8K in several ways, and generation methods like Evol-Instruct \cite{xu2023wizardlm} and Code Evol-Instruct \cite{luowizardcoder} iteratively make the original data more challenging. However, these techniques depend on existing high-quality seed data with question-answer pairs, often requiring the answer to be easily verifiable (e.g., a single number). 

% \paragraph{Synthetic Reasoning Data.} Other techniques such as that of OpenMathInstruct-2 \cite{toshniwal24openmathinstruct}, NuminaMath \cite{numina_math_datasets}, and Xwin-Math \cite{li2024common}, and Self-Instruct \cite{wang2023self} generate new question-answer pairs from a few seed examples using an LLM but scaling to new domains or new content not previously seen in training remains a significant challenge. MMIQC \cite{liu2024augmenting} combines existing datasets and pre-training content from OpenWebMath \cite{pasteropenwebmath}, creating question-answer pairs using the highest ranked answer. However, few measures are taken to curate and ensure the quality of the questions or answers and the resulting dataset is specific to a single domain, similar to the previously mentioned works. MAmmoTH2 \cite{yue2024mammoth2} on the other hand harvests question-answer pairs from exam problems in Common Crawl, spanning multiple domains in math and science. 



\paragraph{Synthetic Reasoning Data.} Synthetic data has emerged as a promising solution for improving performance and enabling new skills. Some approaches bootstrap new data from existing annotated data (e.g., STaR \cite{zelikman2022star} augments with new CoT rationales and MetaMath \cite{yumetamath} rewrites the questions in MATH and GSM8K in several ways), but these techniques rely on the existence of a high-quality dataset. Other techniques such as that of OpenMathInstruct-2 \cite{toshniwal24openmathinstruct}, NuminaMath \cite{numina_math_datasets}, Xwin-Math \cite{li2024common}, and Self-Instruct \cite{wang2023self} generate new data from only a few seed examples using an LLM but scaling to new domains or new content not previously seen in training remains a significant challenge. MMIQC \cite{liu2024augmenting} parses QA pairs from Mathematics Stack Exchange, using the highest-ranked answer, but few measures are taken to curate for quality and the resulting dataset is  also specific to the math domain. Similar to our work, WebInstruct \cite{yue2024mammoth2} harvests question-answer pairs from pre-training corpora and spans multiple domains, but is dependent on carefully crafted rule-based filters. 



\paragraph{Unsupervised Self-training} Most prior works typically depend on human-annotated (gold) final answers \citep{zelikman2022star, chen2024self, pang2024iterative} or the use of an external reward model \citep{singh2023beyond, dong2023raft}. However, manually annotating or verifying final answers is particularly resource-intensive for complex, multi-step problems and training effective reward models for reasoning often requires human evaluation of LLM outputs \citep{cobbe2021training, uesato2022solving, lightman2023let}, making it similarly costly. Like works such as \citet{she2024mapo, yuan2024selfrewarding, rosset2024direct, viethoangtranduong}, our work explores self-training even in the absence of gold labels and does not limit itself to questions with short, easily verifiable targets.


% Metamath bootstraps (augments)
% GAIR-Abel (reformats)
% Xwin-Math (generate new)
% MMIQC (uses Mathematics Stack Exchange, take highest voted answer)

% Simulated environment for code (Shypula et al 2023, InterCode, )
% Self-instruct (needs seed)
% Code Evol-Instruct 
% OSS-Instruct
% star