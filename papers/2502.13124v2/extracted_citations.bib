@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{li2024common,
  title={Common 7b language models already possess strong math capabilities},
  author={Li, Chen and Wang, Weiqi and Hu, Jingcheng and Wei, Yixuan and Zheng, Nanning and Hu, Han and Zhang, Zheng and Peng, Houwen},
  journal={arXiv preprint arXiv:2403.04706},
  year={2024}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{liu2024augmenting,
  title={Augmenting math word problems via iterative question composing},
  author={Liu, Haoxiong and Zhang, Yifan and Luo, Yifan and Yao, Andrew Chi-Chih},
  journal={arXiv preprint arXiv:2401.09003},
  year={2024}
}

@inproceedings{luowizardcoder,
  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@misc{numina_math_datasets,
  author = {Jia Li and Edward Beeching and Lewis Tunstall and Ben Lipkin and Roman Soletskyi and Shengyi Costa Huang and Kashif Rasul and Longhui Yu and Albert Jiang and Ziju Shen and Zihan Qin and Bin Dong and Li Zhou and Yann Fleureau and Guillaume Lample and Stanislas Polu},
  title = {NuminaMath},
  year = {2024},
  publisher = {Numina},
  journal = {Hugging Face repository},
  howpublished = {\url{[https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)}}
}

@article{pang2024iterative,
  title={Iterative reasoning preference optimization},
  author={Pang, Richard Yuanzhe and Yuan, Weizhe and Cho, Kyunghyun and He, He and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2404.19733},
  year={2024}
}

@inproceedings{pasteropenwebmath,
  title={OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text},
  author={Paster, Keiran and Dos Santos, Marco and Azerbayev, Zhangir and Ba, Jimmy},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{rosset2024direct,
  title={Direct nash optimization: Teaching language models to self-improve with general preferences},
  author={Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang},
  journal={arXiv preprint arXiv:2404.03715},
  year={2024}
}

@article{setlur2024rl,
  title={RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold},
  author={Setlur, Amrith and Garg, Saurabh and Geng, Xinyang and Garg, Naman and Smith, Virginia and Kumar, Aviral},
  journal={arXiv preprint arXiv:2406.14532},
  year={2024}
}

@article{she2024mapo,
  title={{MAPO}: Advancing multilingual reasoning through multilingual alignment-as-preference optimization},
  author={She, Shuaijie and Huang, Shujian and Zou, Wei and Zhu, Wenhao and Liu, Xiang and Geng, Xiang and Chen, Jiajun},
  journal={arXiv preprint arXiv:2401.06838},
  year={2024}
}

@article{uesato2022solving,
  title={Solving math word problems with process-and outcome-based feedback},
  author={Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
  journal={arXiv preprint arXiv:2211.14275},
  year={2022}
}

@techreport{viethoangtranduong,
  author = {Tran, Hoang and Glaze, Chris and Hancock, Braden},
  title = {Iterative {DPO} Alignment},
  institution = {Snorkel AI},
  year = {2023},
}

@article{villalobos2022will,
  title={Will we run out of data? an analysis of the limits of scaling datasets in machine learning},
  author={Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson}
}

@inproceedings{wang2023self,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13484--13508},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@article{yue2024mammoth2,
  title={MAmmoTH2: Scaling Instructions from the Web},
  author={Yue, Xiang and Zheng, Tuney and Zhang, Ge and Chen, Wenhu},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@inproceedings{zelikman2022star,
  title={STaR: Bootstrapping Reasoning With Reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

