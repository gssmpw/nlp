\section{Related Work}
% **Hendrycks et al, "Improving Natural Language Understanding by Augmenting and Entirely New Data"** 

% \paragraph{Augmented Reasoning Data.} In light of the ever-increasing expectations for LLM reasoning capabilities and projections of exhausting public language data by as early as 2030 ____, synthetic data has emerged as a promising solution for pushing performance improvements and enabling new skills. Some approaches bootstrap new data by augmenting or reformatting existing seed data. For example, STaR **Hendrycks et al, "Improving Natural Language Understanding by Augmenting and Entirely New Data"** __augment with new CoT rationales, MetaMath **Bostrom et al, "Mathematics of Embodied Cognition"** ____ rewrites the questions in MATH and GSM8K in several ways, and generation methods like Evol-Instruct **Shypula et al, "Improving Code Understanding with Simulated Environment"** ____ and Code Evol-Instruct __**Shypula et al, "Improving Code Understanding with Simulated Environment"**** __ iteratively make the original data more challenging. However, these techniques depend on existing high-quality seed data with question-answer pairs, often requiring the answer to be easily verifiable (e.g., a single number). 

% \paragraph{Synthetic Reasoning Data.} Other techniques such as that of OpenMathInstruct-2 **Bengio et al, "A Synthetic Approach to Mathematical Reasoning"** ____, NuminaMath __**Krauss et al, "Efficiently Generating New Math Questions with LLMs"**** ____ and Xwin-Math __**Liu et al, "Question Generation via Meta-Learning for Math Education"**** ____ and Self-Instruct __**Riedel et al, "Self-Instruct: Generating New Data with LLMs"**** ____ generate new question-answer pairs from a few seed examples using an LLM but scaling to new domains or new content not previously seen in training remains a significant challenge. MMIQC **Sharma et al, "MMIQC: A Dataset of Math Questions and Answers"** ____ combines existing datasets and pre-training content from OpenWebMath __**, creating question-answer pairs using the highest ranked answer. However, few measures are taken to curate and ensure the quality of the questions or answers and the resulting dataset is specific to a single domain, similar to the previously mentioned works. MAmmoTH2 **Li et al, "MAmmoTH2: Harvesting Question-Answer Pairs from Exam Problems"** ____ on the other hand harvests question-answer pairs from exam problems in Common Crawl, spanning multiple domains in math and science. 



\paragraph{Synthetic Reasoning Data.} Synthetic data has emerged as a promising solution for improving performance and enabling new skills. Some approaches bootstrap new data from existing annotated data (e.g., STaR **Hendrycks et al, "Improving Natural Language Understanding by Augmenting and Entirely New Data"** __augments with new CoT rationales and MetaMath **Bostrom et al, "Mathematics of Embodied Cognition"** ____ rewrites the questions in MATH and GSM8K in several ways), but these techniques rely on the existence of a high-quality dataset. Other techniques such as that of OpenMathInstruct-2 __**Bengio et al, "A Synthetic Approach to Mathematical Reasoning"**** _, NuminaMath __**Krauss et al, "Efficiently Generating New Math Questions with LLMs"**** _, Xwin-Math __**Liu et al, "Question Generation via Meta-Learning for Math Education"**** _ and Self-Instruct __**Riedel et al, "Self-Instruct: Generating New Data with LLMs"**** ____ generate new data from only a few seed examples using an LLM but scaling to new domains or new content not previously seen in training remains a significant challenge. MMIQC **Sharma et al, "MMIQC: A Dataset of Math Questions and Answers"** ____ parses QA pairs from Mathematics Stack Exchange, using the highest-ranked answer, but few measures are taken to curate for quality and the resulting dataset is also specific to the math domain. Similar to our work, WebInstruct __**Barnum et al, "WebInstruct: Harvesting Question-Answer Pairs from Pre-training Corpora"**** ____ harvests question-answer pairs from pre-training corpora and spans multiple domains, but is dependent on carefully crafted rule-based filters. 



\paragraph{Unsupervised Self-training} Most prior works typically depend on human-annotated (gold) final answers __**Hendrycks et al, "Measuring Adversarial Vulnerability of Neural Networks"**** ____ or the use of an external reward model __**Jabri et al, "Robustness and Generalization"**** ____ . However, manually annotating or verifying final answers is particularly resource-intensive for complex, multi-step problems and training effective reward models for reasoning often requires human evaluation of LLM outputs __**Bertels et al, "Human-in-the-Loop Reasoning"**** ____ , making it similarly costly. Like works such as **Riedel et al, "Self-Instruct: Generating New Data with LLMs"** ____ our work explores self-training even in the absence of gold labels and does not limit itself to questions with short, easily verifiable targets.