\section{Related Work}
% ____ 

% \paragraph{Augmented Reasoning Data.} In light of the ever-increasing expectations for LLM reasoning capabilities and projections of exhausting public language data by as early as 2030 ____, synthetic data has emerged as a promising solution for pushing performance improvements and enabling new skills. Some approaches bootstrap new data by augmenting or reformatting existing seed data. For example, STaR ____ augment with new CoT rationales, MetaMath ____ rewrites the questions in MATH and GSM8K in several ways, and generation methods like Evol-Instruct ____ and Code Evol-Instruct ____ iteratively make the original data more challenging. However, these techniques depend on existing high-quality seed data with question-answer pairs, often requiring the answer to be easily verifiable (e.g., a single number). 

% \paragraph{Synthetic Reasoning Data.} Other techniques such as that of OpenMathInstruct-2 ____, NuminaMath ____, and Xwin-Math ____, and Self-Instruct ____ generate new question-answer pairs from a few seed examples using an LLM but scaling to new domains or new content not previously seen in training remains a significant challenge. MMIQC ____ combines existing datasets and pre-training content from OpenWebMath ____, creating question-answer pairs using the highest ranked answer. However, few measures are taken to curate and ensure the quality of the questions or answers and the resulting dataset is specific to a single domain, similar to the previously mentioned works. MAmmoTH2 ____ on the other hand harvests question-answer pairs from exam problems in Common Crawl, spanning multiple domains in math and science. 



\paragraph{Synthetic Reasoning Data.} Synthetic data has emerged as a promising solution for improving performance and enabling new skills. Some approaches bootstrap new data from existing annotated data (e.g., STaR ____ augments with new CoT rationales and MetaMath ____ rewrites the questions in MATH and GSM8K in several ways), but these techniques rely on the existence of a high-quality dataset. Other techniques such as that of OpenMathInstruct-2 ____, NuminaMath ____, Xwin-Math ____, and Self-Instruct ____ generate new data from only a few seed examples using an LLM but scaling to new domains or new content not previously seen in training remains a significant challenge. MMIQC ____ parses QA pairs from Mathematics Stack Exchange, using the highest-ranked answer, but few measures are taken to curate for quality and the resulting dataset is  also specific to the math domain. Similar to our work, WebInstruct ____ harvests question-answer pairs from pre-training corpora and spans multiple domains, but is dependent on carefully crafted rule-based filters. 



\paragraph{Unsupervised Self-training} Most prior works typically depend on human-annotated (gold) final answers ____ or the use of an external reward model ____. However, manually annotating or verifying final answers is particularly resource-intensive for complex, multi-step problems and training effective reward models for reasoning often requires human evaluation of LLM outputs ____, making it similarly costly. Like works such as ____, our work explores self-training even in the absence of gold labels and does not limit itself to questions with short, easily verifiable targets.


% Metamath bootstraps (augments)
% GAIR-Abel (reformats)
% Xwin-Math (generate new)
% MMIQC (uses Mathematics Stack Exchange, take highest voted answer)

% Simulated environment for code (Shypula et al 2023, InterCode, )
% Self-instruct (needs seed)
% Code Evol-Instruct 
% OSS-Instruct
% star