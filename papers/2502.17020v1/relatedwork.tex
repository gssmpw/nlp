\section{Related Work}
\subsection*{Stability of clusters}
Most clustering methods involve finding clusters after a random initialisation process~\cite{ahmedKmeansAlgorithmComprehensive2020, hosseiniAlternativeEMGaussian2020}.  
This stochastic element in the initialisation means that the resulting clusters may be different with different initialisations. Cluster stability measures how consistently an algorithm finds the same clusters within a dataset across the different random starting points.  Cluster stability is therefore a measure which may be used to assess the robustness and reliability of clustering methods, with the assumption being that if there is some underlying structure in the data, the algorithm should be able to recover this consistently.  




Cluster stability is central in the field of bioinformatics, where the high-dimensional nature of the data, coupled with biological variation, makes stability an indicator that a clustering algorithm has captured biologically meaningful clusters (e.g., distinct cell types) \cite{kiselevChallengesUnsupervisedClustering2019}. 
Importantly, in bioinformatics, the notion of stability is tied to the biological reproducibility of the findings: when clusters correspond to real biological phenomena, they should persist across different algorithms, initialisations, and even perturbations to the dataset \cite{handlComputationalClusterValidation2005, ronanAvoidingCommonPitfalls2016}. To test how stable the clusters found by an algorithm are, one must consider both the variance in the data, and also the variance in the dimensions used i.e. ensuring that the clusters are not just forming as a result of variance within a few data points, or from a few variables \cite{yu2022benchmarking}.  This work will draw on the approaches of bioinformatics, but applied to text analysis.


A key difference between bioinformatics and text clustering lies in the interpretability of the clusters themselves. While biological clusters often have clear interpretations based on known cell types or functions \cite{qiClusteringClassificationMethods2020}, the interpretability of text clusters relies heavily on human judgment \cite{JMLR:v18:17-069}. 
This adds another dimension to the utility of stability. In text clustering, stability might suggest that the algorithm has found coherent groupings, but it doesnâ€™t guarantee that these groupings are semantically meaningful or useful for the task at hand \cite{ahmedKmeansAlgorithmComprehensive2020}. Hence, an additional human interpretability step is often needed to evaluate whether the stable clusters align with meaningful categories or concepts \cite{kuncheva2006evaluation}.




Ultimately, stability serves as a valuable criterion for identifying meaningful clusters across fields, but the interpretation of stability can vary. In bioinformatics, stable clusters are likely to correspond to real, reproducible biological phenomena \cite{yu2022benchmarking}. In text data, stable clusters may reflect coherent patterns, but assessing their utility requires further interpretability checks, often involving human evaluation \cite{lord2017using}. Stability, in this sense, acts as a precursor to, but not a guarantee of, meaningfulness \cite{kuncheva2006evaluation}.




\subsection*{Gaussian Mixture Model Clustering}

This work uses Gaussian Mixture Models (GMMs) to find clusters in a high dimensional text embedding space.  GMMs are a probabilistic approach to clustering that assume the data are generated from a mixture of several Gaussian distributions \cite{21GaussianMixture}. Each Gaussian component represents a cluster, and the algorithm assigns probabilities of membership for each data point across these clusters, making GMM a soft clustering technique. This section outlines the theoretical foundation and practical implementation of GMM clustering \cite{mit_ml_notes_2015, JMLR:v25:23-1245, ghosh2018emnotes}.


The GMM assumes that the dataset $\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}$ is generated from $k$ Gaussian distributions, where each data point $\mathbf{x}_i \in \mathbb{R}^d$ belongs to a particular cluster with a certain probability.  These parameters are generally estimated through the use of the Expectation-Maximization (EM) algorithm (E-step) \cite{hosseiniAlternativeEMGaussian2020}, This paper uses the scikit learn implementation of GMM, so uses this particular algorithm \cite{21GaussianMixture}. In the E-step, the algorithm calculates the posterior probabilities (responsibilities) that each data point $\mathbf{x}_i$ belongs to each Gaussian component $j$, denoted by $\gamma(z_{ij})$, where $z_{ij}$ is the latent variable indicating the cluster membership of $\mathbf{x}_i$:

\begin{equation}
\gamma(z_{ij}) = \frac{\pi_j \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}{\sum_{l=1}^{k} \pi_l \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_l, \boldsymbol{\Sigma}_l)}.
\label{eq:e-step}
\end{equation}

\textbf{Definitions of Terms:}

\begin{itemize}
    \item \(\gamma(z_{ij})\): The posterior probability (responsibility) that data point \(\mathbf{x}_i\) belongs to the \(j\)-th Gaussian component.
    \item \(\pi_j\): The mixing coefficient (prior probability) for the \(j\)-th Gaussian component, satisfying \(\sum_{j=1}^{k} \pi_j = 1\) and \(0 \leq \pi_j \leq 1\).
    \item \(\mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)\): The multivariate Gaussian probability density function evaluated at \(\mathbf{x}_i\) with mean \(\boldsymbol{\mu}_j\) and covariance matrix \(\boldsymbol{\Sigma}_j\). It is defined as:
    \begin{equation}
    \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j) = \frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}_j|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_j)^\top \boldsymbol{\Sigma}_j^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_j) \right),
    \end{equation}
    where:
    \begin{itemize}
        \item \(d\) is the dimensionality of the data.
        \item \(|\boldsymbol{\Sigma}_j|\) denotes the determinant of the covariance matrix.
        \item \((\cdot)^\top\) represents the transpose of a vector.
    \end{itemize}
    \item \(\mathbf{x}_i\): The \(i\)-th data point in your dataset.
    \item \(\boldsymbol{\mu}_j\): The mean vector of the \(j\)-th Gaussian component.
    \item \(\boldsymbol{\Sigma}_j\): The covariance matrix of the \(j\)-th Gaussian component.
    \item \(k\): The total number of Gaussian components in the mixture model.
    \item \(z_{ij}\): The latent variable indicating the cluster membership of \(\mathbf{x}_i\); \(z_{ij} = 1\) if \(\mathbf{x}_i\) belongs to cluster \(j\), and \(z_{ij} = 0\) otherwise.
    \item \(\sum_{l=1}^{k}\): The summation over all \(k\) Gaussian components.
\end{itemize}

This step assigns soft cluster memberships, meaning that each data point is assigned a probability of belonging to each cluster.


\subsection*{Cluster stability using  Adjusted Mutual Information}

Adjusted Mutual Information (AMI) is a measure used to compare the similarity between two different clusterings of the same dataset. It builds upon the concept of Mutual Information (MI), which quantifies the amount of information shared between two clusterings. MI is able to distinguish between true agreement and agreement that happens purely by chance, by looking at how the knowledge of one set of clusters reduces the surprise at the other set.
However, MI does not account for the similarity that might occur by chance. AMI adjusts the MI score by accounting for the expected similarity between random clusterings, providing a more accurate and reliable metric \cite{JMLR:v11:vinh10a}. 
% Following sentence needs clarification
AMI is symmetric, meaning that the AMI between the two sets is the same regardless of the order of the sets. The AMI used in this paper uses SKlearn's implementation of AMI \cite{Adjusted_mutual_info_score}.

Mutual Information between two clusterings \( U \) and \( V \) is defined as:

\[
\text{MI}(U, V) = \sum_{u \in U} \sum_{v \in V} P(u, v) \log \left( \frac{P(u, v)}{P(u) \, P(v)} \right)
\]

Where:
\begin{itemize}
    \item \( U = \{u_1, u_2, \dots, u_{K_U}\} \) is the set of clusters in clustering \( U \).
    \item \( V = \{v_1, v_2, \dots, v_{K_V}\} \) is the set of clusters in clustering \( V \).
    \item \( P(u) \) is the probability that a randomly selected data point belongs to cluster \( u \) in \( U \).
    \item \( P(v) \) is the probability that a randomly selected data point belongs to cluster \( v \) in \( V \).
    \item \( P(u, v) \) is the joint probability that a data point belongs to cluster \( u \) in \( U \) and cluster \( v \) in \( V \).
\end{itemize}



The AMI adjusts the MI to account for chance, defined as:

\begin{equation}
\label{eq:AMI}
\text{AMI}(U, V) = \frac{\text{MI}(U, V) - \mathbb{E}[\text{MI}(U, V)]}{\text{AVG}[\text{H}(U), \text{H}(V)] - \mathbb{E}[\text{MI}(U, V)]}
\end{equation}

Where:
\begin{itemize}
    \item \( \text{MI}(U, V) \) is the Mutual Information between \( U \) and \( V \).
    \item \( \mathbb{E}[\text{MI}(U, V)] \) is the expected Mutual Information between \( U \) and \( V \) if the cluster assignments were random.
    \item \( \text{H}(U) \) and \( \text{H}(V) \) are the entropies of \( U \) and \( V \), respectively.
    \item \( \text{AVG}[\text{H}(U), \text{H}(V)] \) is the average of the entropies of \( U \) and \( V \).
\end{itemize}

Entropy of a set of clusters measures the uncertainty associated with the cluster assignments:

\[
\text{H}(U) = -\sum_{u \in U} P(u) \log P(u)
\]
\[
\text{H}(V) = -\sum_{v \in V} P(v) \log P(v)
\]



The AMI score ranges from 0 to 1, where close to 1 represents agreement between the two sets and close to 0 represents no relation between the two sets.


\subsection*{Multi-level approaches to clustering}
Previous work has explored multi-level approaches to clustering. One of which uses visualisations called "Clustering Trees" \cite{zappiaClusteringTreesVisualization2018c}. These trees illustrate the proportion of each cluster at a given $K$ that originates from clusters at lower resolutions. By applying this method to synthetic datasets with varying numbers of clusters, the visualisation aids in identifying stable clusters and can help inform the appropriate choice of $K$. Using this method helps determine when additional complexity (in the form of increasing $K$) leads to stable clusters and when further increasing $K$ does not provide meaningful insights.

Another approach to this is the use of MRTree \cite{pengCellTypeHierarchy2021}. Instead of focusing on a visual approach, MRtree uses a more quantifiable method. It determines the optimal number of clusters by analysing stability across different values of $K$ using the Adjusted Rand Index (ARI). Stability is calculated by comparing the clustering results from the reconciled hierarchical tree to the initial non-hierarchical clustering at each resolution. The optimal number of clusters corresponds to the resolution where stability is highest, as indicated by consistent and well-defined partitions.  A change point, where stability drops sharply with further increases in resolution, signifies the threshold for meaningful clustering. In the biological context, this approach generally indicates that the selected number of clusters reflects robust and biologically relevant structures in the data \cite{handlComputationalClusterValidation2005, ronanAvoidingCommonPitfalls2016}. %citation?