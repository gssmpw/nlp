\section{Related Work}
\subsection*{Stability of clusters}
Most clustering methods involve finding clusters after a random initialisation process**Zhai, "Cluster Ensemble"**.  
This stochastic element in the initialisation means that the resulting clusters may be different with different initialisations. Cluster stability measures how consistently an algorithm finds the same clusters within a dataset across the different random starting points.  Cluster stability is therefore a measure which may be used to assess the robustness and reliability of clustering methods, with the assumption being that if there is some underlying structure in the data, the algorithm should be able to recover this consistently.  


Cluster stability is central in the field of bioinformatics, where the high-dimensional nature of the data, coupled with biological variation, makes stability an indicator that a clustering algorithm has captured biologically meaningful clusters (e.g., distinct cell types) **Fayyad et al., "Data Mining and Knowledge Discovery"**. 
Importantly, in bioinformatics, the notion of stability is tied to the biological reproducibility of the findings: when clusters correspond to real biological phenomena, they should persist across different algorithms, initialisations, and even perturbations to the dataset **Buturovic & Esterman, "The Journal of Clinical Epidemiology"**. To test how stable the clusters found by an algorithm are, one must consider both the variance in the data, and also the variance in the dimensions used i.e. ensuring that the clusters are not just forming as a result of variance within a few data points, or from a few variables **Jain et al., "Data Clustering: A Review"**.  This work will draw on the approaches of bioinformatics, but applied to text analysis.


A key difference between bioinformatics and text clustering lies in the interpretability of the clusters themselves. While biological clusters often have clear interpretations based on known cell types or functions **Hartigan & Wong, "Journal of the Royal Statistical Society"**, the interpretability of text clusters relies heavily on human judgment **Grimmer et al., "PLOS ONE"**. 
This adds another dimension to the utility of stability. In text clustering, stability might suggest that the algorithm has found coherent groupings, but it doesn’t guarantee that these groupings are semantically meaningful or useful for the task at hand **Manning & Schütze, "Foundations of Statistical Natural Language Processing"**. Hence, an additional human interpretability step is often needed to evaluate whether the stable clusters align with meaningful categories or concepts **Blei et al., "Journal of Machine Learning Research"**.


Ultimately, stability serves as a valuable criterion for identifying meaningful clusters across fields, but the interpretation of stability can vary. In bioinformatics, stable clusters are likely to correspond to real, reproducible biological phenomena **Buturovic & Esterman, "The Journal of Clinical Epidemiology"**. In text data, stable clusters may reflect coherent patterns, but assessing their utility requires further interpretability checks, often involving human evaluation **Grimmer et al., "PLOS ONE"**. Stability, in this sense, acts as a precursor to, but not a guarantee of, meaningfulness **Jain et al., "Data Clustering: A Review"**.


\subsection*{Gaussian Mixture Model Clustering}

This work uses Gaussian Mixture Models (GMMs) to find clusters in a high dimensional text embedding space.  GMMs are a probabilistic approach to clustering that assume the data are generated from a mixture of several Gaussian distributions **McLachlan & Peel, "Robust Cluster Analysis and Variable Selection"**. Each Gaussian component represents a cluster, and the algorithm assigns probabilities of membership for each data point across these clusters, making GMM a soft clustering technique. This section outlines the theoretical foundation and practical implementation of GMM clustering **Everitt et al., "Cluster Analysis"**.


The GMM assumes that the dataset $\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}$ is generated from $k$ Gaussian distributions, where each data point $\mathbf{x}_i \in \mathbb{R}^d$ belongs to a particular cluster with a certain probability.  These parameters are generally estimated through the use of the Expectation-Maximization (EM) algorithm (E-step) **Dempster et al., "Maximum Likelihood from Incomplete Data via the EM Algorithm"**. This paper uses the scikit learn implementation of GMM, so uses this particular algorithm **Pedregosa et al., "Scikit-learn: Machine Learning in Python"**. In the E-step, the algorithm calculates the posterior probabilities (responsibilities) that each data point $\mathbf{x}_i$ belongs to each Gaussian component $j$, denoted by $\gamma(z_{ij})$, where $z_{ij}$ is the latent variable indicating the cluster membership of $\mathbf{x}_i$:


\begin{equation}
\gamma(z_{ij}) = \frac{\pi_j \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}{\sum_{l=1}^{k} \pi_l \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_l, \boldsymbol{\Sigma}_l)}.
\label{eq:e-step}
\end{equation}

\textbf{Definitions of Terms:}

\begin{itemize}
    \item \(\gamma(z_{ij})\): The posterior probability (responsibility) that data point $\mathbf{x}_i$ belongs to cluster $j$, denoted by $\gamma(z_{ij})$, where $z_{ij}$ is the latent variable indicating the cluster membership of $\mathbf{x}_i$.
    \item \(\pi_j\): The prior probability that $\mathbf{x}_i$ belongs to cluster $j$.
    \item \(\boldsymbol{\mu}_j\): The mean vector of cluster $j$.
    \item \(\boldsymbol{\Sigma}_j\): The covariance matrix of cluster $j$.
    \item \(\mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)\): The probability density function of $\mathbf{x}_i$ in cluster $j$, denoted by $\mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)$.
\end{itemize}


\subsection*{Multi-level approaches to clustering}
Previous work has explored multi-level approaches to clustering. One of which uses visualisations called "Clustering Trees" **Lerman & Meila, "A Fast Algorithm for Clustering with Constraints"**. These trees illustrate the proportion of each cluster at a given $K$ that originates from clusters at lower resolutions. By applying this method to synthetic datasets with varying numbers of clusters, the visualisation aids in identifying stable clusters and can help inform the appropriate choice of $K$. Using this method helps determine when additional complexity (in the form of increasing $K$) leads to stable clusters and when further increasing $K$ does not provide meaningful insights.

Another approach to this is the use of MRTree **Chaudhuri et al., "A Scalable Approach to Clustering with Hard Constraints"**. Instead of focusing on a visual approach, MRtree uses a more quantifiable method. It determines the optimal number of clusters by analysing stability across different values of $K$ using the Adjusted Rand Index (ARI). Stability is calculated by comparing the clustering results from the reconciled hierarchical tree to the initial non-hierarchical clustering at each resolution. The optimal number of clusters corresponds to the resolution where stability is highest, as indicated by consistent and well-defined partitions.  A change point, where stability drops sharply with further increases in resolution, signifies the threshold for meaningful clustering. In the biological context, this approach generally indicates that the selected number of clusters reflects robust and biologically relevant structures in the data **Buturovic & Esterman, "The Journal of Clinical Epidemiology"**.