\documentclass[12pt]{article}
\usepackage[round]{natbib}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,amsmath,amsfonts,amssymb,fullpage,url}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{array}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{bookmark}
\usepackage{hyperref}

\usepackage{tcolorbox}

\usepackage{parskip} % Package to handle paragraph spacing
%\setlist[enumerate]{topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt}
\usepackage{cmbright}
\usepackage[OT1]{fontenc}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MARK: colors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xcolor}
\definecolor{myred}{HTML}{E06666}
\definecolor{myblue}{HTML}{299DD3}
\definecolor{mygreen}{HTML}{159B27}
\definecolor{lightsteel}{HTML}{8f8f8c}
\definecolor{mediumsteel}{RGB}{82, 83, 94}
\definecolor{darksteel}{RGB}{54, 69, 79}
\definecolor{darkgreen}{RGB}{0,77,64}
\definecolor{prettycyan}{RGB}{0, 180, 180}  % A bright, pretty cyan
%\definecolor{darkblue}{RGB}{0, 0, 100}     % A deep, dark blue
\definecolor{olive}{HTML}{9CB380}     % A deep, dark blue
\definecolor{orange}{HTML}{FECDAA}     % A deep, dark blue
\definecolor{salmon}{HTML}{FFA3A5}     % A deep, dark blue

\newcommand{\paperarg}[1]{\textbf{\color{lightsteel} #1}}

\hypersetup{
    colorlinks=true,
    linkcolor=mygreen,  % Internal links (like table of contents)
    urlcolor=salmon,     % External links
    citecolor=prettycyan,  % Citations
    filecolor=darkblue     % Links to local files
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MARK: Setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtcolorbox{definitionbox}[1]{
  colback=lightsteel,
  colframe=darksteel,
  fonttitle=\bfseries\color{white},
  coltitle=white,
  title=#1,
  sharp corners,
  boxrule=1pt,
  toptitle=1mm,
  bottomtitle=1mm,
  top=3mm,
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MARK: New commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\multiLineCell}[2][c]{%
  \begin{tabular}[#1]{@{}l@{}}#2\end{tabular}%
}
\newcommand{\todo}[1]{[\textbf{\textcolor{myred}{TODO.}}~\textbf{#1}]}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MARK: Doc begins
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{
    \textbf{Naturalistic Computational Cognitive Science} \\[0.5em]
    {\Large Towards generalizable models and theories \\
     that capture the full range of natural behavior}
}
\author{
    \begin{minipage}[t]{0.48\textwidth}
        \textbf{Wilka Carvalho}$^\ast$ \\
        Kempner Institute \\
        Harvard University \\
        \texttt{wcarvalho@g.harvard.edu} \\
        $^\ast$equal contribution
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \textbf{Andrew Lampinen}$^\ast$ \\
        Google DeepMind \\
        \newline
        \texttt{lampinen@google.com}
    \end{minipage}\\
}

\date{}
\begin{document}

\maketitle

\begin{abstract}
   Artificial Intelligence increasingly pursues large, complex models that perform many tasks within increasingly realistic domains. How, if at all, should these developments in AI influence cognitive science? 
    We argue that progress in AI offers timely opportunities for cognitive science to embrace experiments with increasingly naturalistic stimuli, tasks, and behaviors; and computational models that can accommodate these changes. We first review a growing body of research spanning neuroscience, cognitive science, and AI that suggests that incorporating a broader range of naturalistic experimental paradigms (and models that accommodate them) may be necessary to resolve some aspects of natural intelligence and ensure that our theories generalize.
    We then suggest that integrating recent progress in AI and cognitive science will enable us to engage with more naturalistic phenomena without giving up experimental control or the pursuit of theoretically grounded understanding. We offer practical guidance on how methodological practices can contribute to cumulative progress in naturalistic computational cognitive science, and illustrate a path towards building computational models that solve the real problems of natural cognition---together with a reductive understanding of the processes and principles by which they do so.
    \end{abstract}

\section{Introduction}

Cognitive scientists build models to make our theories concrete  --- which offers testable predictions, eliminates ambiguities \citep{guest2021computational},  and can reveal unexpected properties of cognition and behavior~\citep{mcclelland2009place}. Cognitive models can range from simplified models of a high-level process \citep[e.g.][]{frank2012predicting,cohen1990control}, to task-performing models of a particular domain \citep{newell2012you}, or beyond \citep{newell1994unified}. Many cognitive paradigms---connectionism \citep{mcclelland1986appeal}, bayesian inference \citep{tenenbaum2001generalization}, and cognitive architectures \citep[e.g.][]{ritter2019act,laird2019soar}---have sought generalizable models that can explain a broad range of behavior from a simpler set of principles. Ultimately, these approaches could aspire to build towards  "unified theories of cognition" that can explain and predict natural behavior across many tasks and domains~\citep{newell1994unified}. However, we are not there yet. In recent years there have been substantial debates about the generalizability of our theories \citep{yarkoni2022generalizability,eckstein2021reinforcement}, whether our models and theories are adequately constrained \citep[e.g.][]{jones2011bayesian,rahnev2018suboptimality}, and calls to accommodate a broader scope of naturalistic phenomena \citep{wise2023naturalistic,cisek2024toward}. Our goal in this paper is to motivate and outline a path that we believe will begin to address many of these challenges.

The path we propose is motivated by recent practical and conceptual developments in AI. In the last ten years AI has progressed substantially in building models that can perform a wide range of tasks in various domains. We now have vision models that can classify and segment real-world images in objects without external supervision~\citep{he2020momentum,caron2021emerging}, we have reinforcement learning models that can learn new tasks in a domain with human-like efficiency~\citep{team2023human}, and we have language models which can produce human-like language and solve many language-specified tasks~\citep{radford2019language,wei2022emergent}. Surprisingly, such models produce internal representations that capture many features of brain representations \citep{yamins2014performance,schrimpf2021neural}. These findings have led to discussion of how such complex models can be explanatory \citep{cao2024explanatorypt1}, and how they should alter our theories \citep{hasson2020direct,perconti2020deep,piantadosi2023modern}. Yet they have also led to debate over how we test our models and interpret the results \citep{bowers2023deep,dicarlo2023let,dentella2024testing}. Thus, it remains an open question if and how the progress in AI can contribute to addressing the challenges of cognitive science.

\begin{figure}[htp]
    \begin{center}
        \includegraphics[width=\textwidth]{overview-main.pdf} 
        \caption{\textbf{Naturalistic computational cognitive science}: the what, why, and the how. The first section (\S\ref{sec:definitions}) provides an overview of ``naturalistic computational cognitive science''. Afterwards (\S\ref{sec:data-benefits}-\S\ref{sec:learning-benefits}), we motivate the utility of naturalistic experimental paradigms and learning-based approached for developing a more complete understanding of cognition. The rest of the paper (\S\ref{sec:building}-\S\ref{sec:theory}) focuses on \textbf{how} to achieve these goals; how to develop models for naturalistic settings, and how to use naturalistic experiments and models as part of explanatory cognitive theories. (Figures reproduced from \citealp{saxe2019mathematical,geirhos2018imagenet,doshi2023cortical}.)}
        \label{fig:overview}
    \end{center}
\end{figure}

In this work, we attempt to weave a thread of arguments that synthesizes these literatures, spanning from cognitive science's motivation and theory development to the practicalities of model engineering and reproducibility.
We articulate a research strategy that we believe will be helpful in enabling theory-driven cognitive science to achieve deeper
understanding of the full range of natural intelligence.
We present an overview of this framework and our arguments in Figure~\ref{fig:overview}.

Our paper proceeds as follows. First, in \S\ref{sec:definitions}, we unpack what we mean by ``naturalistic'' computational cognitive science, by providing a pragmatic outline of the features that we believe deserve more emphasis in our experimental paradigms and models. In particular, we argue that researchers should expand their task paradigms to to more closely approximate the breadth of the settings in which their theoretical constructs would be expected to generalize by both adding relevant variables that may interact with those in question, and by incorporating more variability within existing parameters. Likewise, researchers should expand their models to more faithfully approximate the computational task that the natural system is solving. 

In the next two sections, we motivate these goals, by arguing that increasing naturalism is necessary for developing generalizable understanding. First, in \S\ref{sec:data-benefits}, we review strands of evidence spanning neuroscience, cognitive science, and AI, showing that naturalistic experimental paradigms can lead to different behavior, engage brain systems differently, and expose  expose computational challenges that engage mechanisms differently. As an example, \citet{francis2017simulating} found that moral judgments on a trolley problem were dramatically different in a virtual reality presentation compared to text vignettes. Other works that we review have found that processes ranging from visual processing \citep{amme2024saccade}, learning \citep{rosenberg2021mice,collins2024rl}, memory \citep{helbing2020search} and more are changed when naturalistic elements are introduced. The works that we review in this section thus suggest that experiments that span a broader range of naturalistic settings are essential for drawing correct inferences about the system as a whole.
Next, in \S\ref{sec:learning-benefits}, we highlight the benefits of models that learn from naturalistic data---which can perform a wide range of tasks, and yield qualitatively different patterns of generalization than models trained in simplified settings. In turn, these differences can change the inferences we make from our models, and yield new insights about the origins of cognitive and neural phenomena. Together, these two sections of our paper argue that pursuing naturalistic experiments, and models trained on them, will be necessary for achieving complete understanding of cognition.

In the remainder of the paper, we turn to the practice of naturalistic computational cognitive science, and its  contributions to building generalizable cognitive theories. First, in \S\ref{sec:building}, we review key lessons computational cognitive scientists can adopt from AI in developing generalizable models that can operate in increasingly naturalistic data conditions: the benefits of frictionlessly reproducing prior research artifacts, the importance of generalizability-focused model development, and the value of benchmarking for concentrating research effort on important questions. Finally, in \S\ref{sec:theory}, we outline how one can leverage these potentially complicated and opaque models to contribute to generalizable theories of cognition---uniting task-performing predictive models with explanatory reductive understanding, using established techniques like rational analysis \citep{anderson1990adaptive} and new approaches to interpreting complex models \citep{geiger2021causal}. We close in \S\ref{sec:discussion} by discussing the broader context and implications of naturalistic computational cognitive science.

\begin{figure}[htp]
    \begin{center}
        \includegraphics[width=\textwidth]{overview-definition.pdf} 
        \caption{Overview of naturalistic computational cognitive science. Given a novel computational model, researchers generate predictions to test their model in a simplified setting. If this model successfully predicts human behavior, researchers see if their model continues to generalize human predictions in increasingly naturalisic conditions.}
        \label{fig:overview-defintion}
    \end{center}
\end{figure}

\section{What \textit{is} ``naturalistic'' computational cognitive science?} \label{sec:definitions}

Naturalistic computational cognitive science is a research strategy for theory-driven cognitive science that aims to predict human behavior across increasingly naturalistic stimuli and tasks. This approach gradually increases the ecological validity of experimental designs while ensuring models can accommodate both simplified and more natural conditions. As demonstrated in machine learning (\S\ref{sec:building}), this strategy has enabled models that can work with real-world data, albeit imperfectly.

As in other approaches within computational cognitive science, this approach starts with formulating models and designing controlled experiments to isolate core phenomena. Like prior work, we place an emphasis on \textit{model generalization}, i.e. predicting human behavior on data the model hasn't been exposed to---including data from new experimental conditions or tasks \citep{busemeyer2000model}. Therefore,
\begin{enumerate}
  \item Model parameters are estimated with a ``training'' distribution before being evaluated.
  \item Models should predict human behavior on held-out examples that neither the model nor humans have encountered.
\end{enumerate}

One key facet of naturalistic computational cognitive science is that we place emphasis on having models---in particular, the same model---predict human behavior across increasingly naturalistic conditions that cover a broader space of settings in which the theoretical construct would be expected to generalize. We see two main paths toward increasing naturalism in experimental paradigms:
\begin{enumerate}
    \item Changing existing experiment parameters to better cover the scope of natural distributions. 
    For example, in an object classification task, we might expand the category set to include the broad range of objects people encounter in daily their life.
    \item Adding ecologically motivated parameters that might interact with the theoretical constructs in question. For example, in a linguistic judgment task, we might test judgments across both spoken and written utterances.
\end{enumerate}
We provides examples in Figure \ref{fig:naturalism}.
\begin{figure}[!htb]
   \begin{center}
       \includegraphics[width=.82\textwidth]{increasing-naturalism-v5.pdf} 
       \caption{\textbf{Examples of increasingly naturalistic settings that we can now study in a theory-driven manner.} All of these are settings where tasks and stimuli can now be parametrically generated---i.e., thanks to ``generative AI'', we can now \textit{automate} the generation of photorealistic synthetic data and virtual worlds; thanks to virtual and augmented reality, we can now scan and parametrically manipulate real environments. Thanks to progress in AI,  we can now build task-performing models that can operate in these stimuli and task settings. 
       }
       \label{fig:naturalism}
   \end{center}
\end{figure}


Our goal is to encourage cognitive scientists to embrace experimental paradigms and models that capture a broader spectrum of the variability in inputs, interactions, and tasks present in the natural environment of humans or animals---including, for humans, the rich cultural constructs that form so much of our modern experience. That is, we identify a paradigm as more naturalistic if it incorporates a broader scope of the range of natural variability over which the theoretical constructs would be expected to generalize. Correspondingly, we identify a model as more naturalistic if it is capable of generalizing to make predictions across a broader range of contexts. Note that increasing naturalism does not always mean faithfully approximating all the joint statistics of the natural distribution---increasing the scope of variation along naturalistic dimensions can also enable moving outside the natural data distribution for controlled experiments (see \S\ref{sec:theory:controlled_experiments}).

Generally, identifying the parameters for increasing naturalism in a particular experiment is not well defined or obvious---it is task- and theory-specific.
To illustrate ``naturalistic'' more explicitly, we will therefore give some examples of dimensions along which naturalism can be increased.

\textbf{Task paradigm}
\begin{itemize}
\item Incorporating multiple paradigms across which a hypothesis would be expected to hold, rather than testing on a single task; for example testing multi-step RL tasks as well as bandit tasks.
\item Incorporating a broader set of stimuli, for example augmenting synthetic images with real images, or images with videos.
\item Having task stimuli generated by many varying latent factors, not just the ostensible variables of theoretical interest.
\end{itemize}



\textbf{Environment}
\begin{itemize}
\item Large state spaces that reflect the complexity of real-world environments~\citep{wise2023naturalistic}.
\item Including the presence of other social agents that continue to learn with the agent~\citep{hu1998multiagent} and that the organism must interact with~\citep{velez2021learning}.
\item A continually changing environment~\citep{abel2024definition}
\end{itemize}
\textbf{Model architecture}
\begin{itemize}
\item Architectures that can operate over sensory inputs  like natural images \citep{yamins2014performance}, speech \citep{kell2018task}, or natural language \citep{schrimpf2021neural} rather than simplified stimuli (e.g. low-dimensional or discrete inputs that contain only task-relevant features).
\item Architectures with naturalistic action spaces---for example, modeling low-level motor control and embodiment rather than abstract, symbolic actions \citep{merel2019hierarchical}.
\end{itemize}
\textbf{Learning algorithm}
\begin{itemize}
\item Unsupervised~\citep{higgins2021unsupervised}, self-supervised~\citep{konkle2022self}, or intrinsic~\citep{chentanez2004intrinsically,oudeyer2013intrinsically} learning algorithms.
\item Social~\citep{henrich2016secret} or cultural~\citep{cook2024artificial} learning algorithms.
\item Prospective learning algorithms that aim to model how the tasks evolve in continually changing environments~\citep{seligman2013navigating,de2023prospective}.
\end{itemize}

Of course, it is easy to say that increasing the naturalism of our experiments and models would be useful, but how can we actually achieve it? A key goal for the later sections of our paper (\S\ref{sec:building}-\ref{sec:theory}) will be to give practical perspectives on how to achieve these goals, and how to develop theories that generalize across a broader range of naturalistic behavior. We briefly sketch these perspectives here.

\paperarg{How do we scale up to naturalistic experiments?}
We believe that recent developments in computer science and engineering make increasing naturalism in our experimental paradigms much more accessible to researchers, as we can use natural language to programmatically generate stimuli such as 3D scenes~\citep{zhou2024scenex} and videos of objects~\citep{villegas2022phenaki}, and leverage haptic feedback in virtual reality to simulate textures~\citep{lu2022preference}. While the specific parameters will vary by domain, the core principle remains: gradually increasing ecological validity while maintaining experimental control.

\paperarg{How do we glean understanding from potentially opaque learning based models? (\S\ref{sec:theory})}
We believe that task performing models offer powerful new tools for building cognitive theories. In a sense, these tools amount to treating the cognitive model as a proxy object for study---but one that admits much more opportunity for experimentation and understanding. For example, we can use ``neuroscience''-like causal methods to probe a model's computations by intervening on them \citep[e.g.][]{geiger2021causal}. We can also interpret its behavior as an adaptation to the naturalistic properties of its training data \citep[e.g.]{chan2022data,prystawski2024think}---as a kind of \emph{rational analysis} \citep{anderson1990adaptive,lewis2014computational} of behavior and computations being a rational response to an environment. When applying these approaches to computational models, we have the ability to explicitly test our assumptions in ways we cannot with human subjects (e.g., how would the system behave if it were trained on only ungrammatical sentences). We can also attempt to distill our insights further, into analytic abstractions that illustrate the minimal instantiation of a phenomenon. By combining these routes, we can build rigorous links from an opaque task-performing model to conceptual understanding of cognition. 


\paperarg{In summary} naturalistic computational cognitive science seeks to (1) explain real-world intelligent behavior by developing models that operate over the scope of naturalistic inputs, produce naturalistic outputs, and are optimized according to the actual constraints and affordances of an person's environment, and (2) develop cognitive theories that unite these models with reductive understanding. %This contrasts with traditional approaches that often rely on simplified, artificial experimental settings. 
In the coming sections, we will first motivate more deeply how naturalistic experimental settings can help us develop more complete accounts of intelligence, before returning in more detail to the questions of how these goals can be achieved.



% MARK: benefits of naturalistic experimental
\section{The benefits of naturalistic experimental settings}\label{sec:data-benefits}

\begin{figure}[!htp]
  \begin{center}
      \includegraphics[width=\textwidth]{overview-experiments.pdf} 
      \caption{Overview of the benefits of increasingly naturalisic experimental conditions.}
      \label{fig:overview-experiments}
  \end{center}
\end{figure}

In this section, we outline the benefits of naturalistic experimental settings for cognitive science. Specifically, we argue that naturalistic experimental settings can engage mechanisms that are qualitatively different than those engaged in simpler settings --- and thereby change the scientific inferences we draw. We review examples where naturalistic experimental settings have changed our understanding of a system, and have helped to disentangle competing models that simpler settings could not. 

As an orienting conceptual example, imagine that we are doctors studying heart function, and we chose to only study it while the participants are lying down and resting. We could certainly learn interesting things thereby; the resting setting offers high test-retest reliability, and a highly-controlled environment to study the processes like how respiratory cycles affect heart rate. % This controlled setting would yield relatively high test-retest reliability for features like average heart rate and heart-rate variability.
However, it offers this control and reliability precisely by removing the many factors of natural variation that interact with heart rate, and indeed for which heart function evolved --- such as rapid adaptation to movement, stress, etc. Studying heart function across a broader range of naturalistic settings would be necessary to understand why the system is the way it is, and how all the physiological (and psychological) processes involved interact. For example, if we experimentally manipulated breathing, but did so across a range of different naturalistic activities (sitting, speaking, running, etc.) we would more effectively determine how breathing affects heart function, and how that effect depends on the state of the overall system. That is, by studying phenomena across a range of naturalistic settings we elaborate our understanding of the system, and can build more complete theories of its function.



\subsection{Behavior can be different in naturalistic experiments}\label{sec:benefits:behavior}

The starting point for many cognitive analyses is behaviors on a task designed to isolate some cognitive capability. However, there are many cases in which task-relevant behavior can be altered by seemingly-orthogonal features of the task---thus raising the risk that our theories will be overfit to a restricted task setting unless we also consider a broading range of variation. In this section, we review some examples illustrating changes in behavior or capabilities in naturalistic settings.

\paperarg{Example 1: moral behavior can change when moving from textual vignettes to virtual reality}.
Recent research has shown that behavioral choices can change when moving from text to virtual reality as it can mark a shift from moral \textit{judgment} (i.e. deciding whether things are moral) to moral \textit{action}~\citep{francis2017simulating}.
When presented with the classic trolley dilemma in text form, only 40\% of participants say they would pull a switch to divert a trolley that would kill one person to save five others.
However, when the same scenario is presented in virtual reality where participants must simulate the action, 55\% choose to pull the switch.
This difference becomes even more pronounced in the ``footbridge'' variant of the dilemma.
In the text version, only 10\% of participants say they would push someone off a bridge to save five others.
Yet when able to simulate this action in virtual reality, 65\% of participants choose to push the person.
These findings demonstrate that people's stated moral preferences in hypothetical scenarios can differ substantially from their actions in more naturalistic settings.

\paperarg{Example 2: memory performance differs between naturalistic search and explicit memorization}.
When participants are explicitly instructed to memorize objects in a 3D home environment, their subsequent recall accuracy was significantly lower than when they incidentally encounter the same objects during visual search tasks~\citep{helbing2020search}.
This effect extends to spatial memory as well.
Not only do participants better remember the identity of objects encountered during search, they also show more accurate memory for object locations—placing objects closer to their original positions when they were initially seen during search versus explicit memorization.
These findings demonstrate that tasks focused explicitly on memory may not capture all aspects of how memory naturally operates during everyday behavior.
Additionally, it suggests that computational models of memory may need to be revised to account for these differences in naturalistic settings.
This is supported by research showing that people display detailed memory of object and spatial information in real-world scenes~\citep{bainbridge2019drawings}.

\paperarg{Example 3: mice can learn $1000 \times$ faster during natural behavior, compared to two-alternative forced choice tasks}.
Recent research has shown that learning rates can dramatically differ between simplified laboratory tasks and more naturalistic settings~\citep{rosenberg2021mice}.
When mice are tested in traditional two-alternative forced choice (2AFC) tasks, they typically require 10,000 trials over 3-6 weeks of training to reach asymptotic performance of only about 67\% accuracy.
However, when allowed to \textit{freely explore} a labyrinth environment (similar to a natural burrow systems~\citep{small1901experimental}), mice demonstrate remarkably accelerated learning~\citep{rosenberg2021mice}.
In this setting, mice learned to navigate a complex maze with 63 T-junctions in just a few hours, discovering and optimizing their path to a water reward after only about 10 reward experiences.
The learning rate in the naturalistic setting was approximately $1000 \times$ times faster than in traditional 2AFC paradigms.
This dramatic difference in learning speed demonstrates how traditional simplified experimental paradigms may fundamentally underestimate an organism's learning capabilities, and suggests that natural learning mechanisms may operate differently when they are engaged through more naturalistic behavior.

\subsection{Neural systems can operate differently under naturalistic conditions}

In addition to behavioral differences like those reviewed above, naturalistic conditions can result in changes to neural coding and computation. Thus, in order to arrive at a complete understanding of neural function, we need to consider the responses of the system across a range of simpler and more naturalistic settings. In this section, we highlight some examples of different computations or computational roles that arise when moving from standard paradigms to more naturalistic ones.

\paperarg{Example 1: the model of superior colliculus shifted from a model of  eye movement to one of integrating sensory inputs for body control}.
\citet{cisek1999beyond} suggests that the focus on computational behavior as an input-output mapping neglects the fundamental fact that natural intelligence evolved not to produce single responses, but for closed-loop control in an environment. This setting yields a rather different interpretation of the system's representations and processes. More generally, many researchers have argued that cognition cannot be understood completely outside its embodiment and the environment in which cognitive processes are instantiated \citep[e.g.,][]{newen2018oxford}.
For example, \citet{cisek2024toward} argues that as we increased naturalism from head-fixed to head-free to body-free settings when studying monkeys, we expanded our theory of superior colliculus from controlling saccadic eye movement to generally integrating multimodal cues to guide bodily action-selection~\citep{cisek2024toward}. That is, as we increased the naturalism of the experimental conditions that we used to study monkeys, we arrived at a more complete model of superior colliculus.

\paperarg{Example 2: visual processing systems operate differently during natural viewing compared to passive viewing paradigms}.
Traditional research has shown that one of the earliest visual responses in the brain (known as P100/M100) occurs about 100ms after a person's eyes land and fixate on a new location~\citep{vinje2000sparse}.
However, this finding comes primarily from experiments where participants passively view images while keeping their eyes still.
In contrast, natural vision involves actively moving our eyes multiple times per second to sample information from different parts of a scene.
When the researchers examined brain responses during this more natural active viewing, they found that the P100/M100 response actually begins when the eye movement (saccade) starts, not when it ends at the new fixation point~\citep{amme2024saccade}.
Furthermore, they discovered that the neural patterns during active viewing were opposite (anti-correlated) to those seen during passive viewing, suggesting fundamentally different processing mechanisms.
These findings reveal that simplified experimental paradigms, while valuable, may not capture all aspects of how visual processing actually operates during natural behavior.

\paperarg{Example 3: model-based learning systems are used for learning moral judgments, but model-free learning are used for learning to avoid harming others}.
Despite moral reasoning being strongly associated with model-based learning systems in the prefrontal cortex, neural evidence shows a surprising shift when people actually learn to avoid harming others~\citep{lockwood2020model}. Moral reasoning commonly engages model-based systems in the lateral prefrontal cortex (LFPC)~\citep{spitzer2007neural,carlson2018lateral} with LPFC disruption leading to reduced norm compliance and enforcement~\citep{knoch2006diminishing,ruff2013changing}. However, when participants engage in tasks requiring them to learn moral behavior (rather than just reason about it), researchers instead observed neural activity consistent with model-free learning. Similar to our moral judgment vs. moral action example from \S\ref{sec:benefits:behavior}, this shows that when an experiment becomes more naturalistic (not just reasoning about morality but learning moral behavior), neural systems can operate in unexpected ways.


\subsection{Naturalistic experimental paradigms can expose computational challenges that engage mechanisms differently}

Finally, naturalistic paradigms can likewise introduce challenges that are important to understanding functioning of the system as a whole. In this section, we highlight examples where the challenges posed by increasing naturalism can engage computational mechanisms differently---which can be useful for disentangling underlying mechanisms, and is important for achieving generalizable understanding.

\paperarg{Example 1: Different learning mechanisms only yield distinct predictions under working memory load}. 
Overly-simplified tasks can yield \emph{aliasing} of different solutions, where many different computational approaches yield the same behavior. \citet{collins2024rl} presents an example, by showing that what seems superficially to be implemented as a standard reinforcement learning algorithm, on closer examination may instead be implemented through a combination of working memory and simpler value-free associative learning.\footnote{Whether this alternative counts as an alternative implementation of RL or not is orthogonal to our argument; the important point is that the two models yield distinct predictions in some regimes but not others.} However, the difference between these two implementations cannot be identified in standard task settings, as Collins notes: ``Even simple tasks designed to elicit a target process (such as bandit tasks for RL) recruit multiple other processes, but those processes may be unidentifiable in such tasks. Disentangling multiple processes requires considering more complex tasks to elicit differentiable behavior.'' The more complex tasks in question simply increase the stimulus set size within learning blocks to a handful of objects, rather than restricting to two or three---i.e., more broadly covering the range of set sizes more clearly disentangles the learning processes. Thus, by restricting to minimal paradigms, we enable a system to use many solutions and prevent ourselves from discriminating between them. By contrast, if we impose a broad range of evaluations on the system \citep[cf.][]{nau2024centering}---and especially naturalistic evaluations that increase demands along different axes of task difficulty---the increased constraints actually make it easier to map the model on to natural intelligence, as is highlighted in the \emph{contravariance principle} of \citet{cao2021explanatorypt2}.

\paperarg{Example 2: fear conditioning}. 
Traditional fear conditioning research has provided fundamental insights into how humans learn associations between stimuli and aversive outcomes~\citep{sehlmeyer2009human,maren2001neurobiology,britton2014looking}.
Through carefully controlled laboratory studies involving paired stimuli and outcomes, we have developed a rich mechanistic understanding of basic fear learning processes.
These paradigms have proven especially valuable for studying certain types of naturally occurring fear learning, such as immediate aversive responses to foods~\citep{riley1985conditioned} or traumatic events like combat exposure~\citep{engelhard2008memory}.
However, real-world fear learning often involves additional temporal dynamics that complement these well-studied processes.
For example, a student may develop fear of exams after receiving a single poor grade weeks after taking the test, without requiring repeated negative experiences~\citep{mobbs2021promises}.
This type of naturalistic learning involves delayed \textit{delayed} and \textit{sparse} feedback for choices, i.e. it relies on a {computational mechanism} for \textit{credit assignment}~\citep{sutton2018reinforcement}. Credit assignment describes how an agent credits choices (e.g. studying, socializing, diet, etc.) as responsible for future events (e.g. a poor grade).
This is not to say humans have optimal credit assignment but that they have \textit{some mechanism} for it and we are empowered to study it with appropriately naturalistic experimental settings.
Clinical researchers have additionally pointed out that increased ecological validity can increase the applicability of our experimental results as the gap between lab-based fear conditioning and real-world fear has been a barrier to successful treatment of pathological fears~\citep{beckers2013s,scheveneels2016validity,krypotos2018validity}.


\paperarg{Example 3: seemingly-similar tasks that engage brain mechanisms differently can be reconciled by naturalistic models}.
There has been a long debate about whether perceptual processes like ``oddity''  tasks (identifying an object that is different from the rest of the objects in a set) recruit the Medial Temporal Lobe (MTL). Specifically, some researchers \citep[e.g][]{murray2007visual} have suggested that MTL is recruited to discriminate among ``complex'' stimulus sets that visual cortex cannot discriminate on its own (and thus MTL lesions impair performance specifically on those stimuli); other researchers have found no such interaction and suggested it stems from methodological issues \citep{buffalo1998human,suzuki2009perception}. Recently, stimulus-computable models have shed new light on this debate. Specifically, \citet{bonnen2021ventral,bonnen2023inconsistencies} used pretrained convolutional vision models to directly process \emph{all} the \emph{raw} stimuli used in prior experiments, and evaluated whether the ``oddity'' stimulus could be distinguished from the rest of the set in the representation space of these models. By doing so, Bonnen et al. showed that some of the purportedly ``complex'' stimuli from some of the prior experiments could nevertheless be distinguished with the vision model alone, while the complex stimuli from other experiments could not. Furthermore, the  purportedly ``complex'' stimuli that in fact could be discriminated by the pretrained vision network alone did not require the MTL, while the complex stimuli that could not be distinguished by the vision model were precisely the ones that required MTL. Thus, the way that prior works operationalized ``complexity'' was underspecified; differences in operationalization that got abstracted away in conceptual descriptions of the experiments likely drove the seemingly-conflicting findings.
This example illustrates the value of building models that directly perform the same naturalistic tasks as the subjects, and testing these models on benchmarks that incorporate stimuli from many experimental paradigms. This naturalistic approach can elucidate important differences among experimental paradigms, and clarify conceptual understanding.

% MARK: benefits of learning
\section{The (surprising) benefits of learning with naturalistic data}\label{sec:learning-benefits}
\begin{figure}[htp]
    \begin{center}
        \includegraphics[width=\textwidth]{overview-learning.pdf} 
        \caption{Overview of the benefits of learning with naturalistic data. (Figures reproduced from \citealp{doshi2023cortical,manning2020emergent,team2023human}.)}
        \label{fig:overview-learning}
    \end{center}
\end{figure}
The standard lesson in science is that we must simplify an experimental setting so that we can better arrive at a causal conclusion. 
However, we have already seen examples illustrating  that more is different \citep{anderson1972more}---certain properties of intelligence may only emerge in more ``complicated'' naturalistic settings.
In this section, we detail examples where naturalistic data \emph{itself} seems to play an importance role in producing the empirical phenomena of intelligence that we wish to study.

Many of the findings we will discuss here will be modern results from AI. 
One common trend is that there is a positive interaction between learning-based systems and naturalistic data---learning-based systems can accommodate naturalistic data, and reciprocally, learning from naturalistic data results in qualitatively better results than learning in simpler settings. 
While we do not yet know the implications for natural intelligence, they open interesting questions and challenge prior assumptions.
In particular, we argue that cognitive science should consider the limitations placed on our models when we fail to consider naturalistic data.
(Note that while we draw many case studies from particular areas of deep learning, we believe that some of these factors could likewise contribute to generalization under other learning paradigms, such as program synthesis, or even evolution---and thus should be of interest even to those who do not work with deep learning models.)


\subsection{Learning enables models that can operate on natural data}

Perhaps the first surprising benefit of (deep) learning is that it enables a system to operate over natural data. Until AlexNet~\citep{krizhevsky2012imagenet}, the prevailing wisdom was that one had to design useful representations of natural data for models. However, AlexNet and subsequent work showed that a model could learn useful representations for naturalistic data simply by being trained to make predictions about this data. Surprisingly, these models learn representations that have notable correspondence to the internal representations in visual cortex~\citep{yamins2014performance,khaligh2014deep}. Here, we review some of this work and how learning-based methods can enable more flexible cognitive models that operate on natural data. 

\paperarg{Learning-based vision models can operate on natural data and predict neural data (surprisingly) well.} There is a rich history of learning-based methods revealing how neural networks can develop internal representations from pixel-based images that strongly correspond to neural data. A seminal example comes from~\citet{olshausen1996emergence}, who showed that V1-like Gabor filters could emerge from a convolutional neural network optimized to transmit input while minimizing overall activation (essentially, a sparse autoencoder). Prior to this, V1 activations were modeled using analytically defined Gabor filters. This breakthrough demonstrated that V1-like receptive fields could arise naturally from optimization to environmental statistics of edges at varying frequencies and orientations. Building on this foundation, \citet{lee2007sparse} demonstrated that V2-like convolutional filters could emerge from unsupervised optimization of sparse deep belief networks. A significant advance came when \citet{yamins2016using} showed that task-optimized deep convolutional neural networks trained for object classification developed intermediate layers that matched neural representations in V4 and IT better than any previous models. Recent research has revealed an even more striking finding: these task-optimized networks represent images in a latent space whose geometry has remarkable correspondence to brain \textit{topography}~\citep{doshi2023cortical}. When fitting a 2D sheet to image representations such that neighboring points correspond to nearby points in the original high-dimensional space, the resulting sheet shows striking similarities to actual brain topography. Thus, learning-based approaches, particularly neural networks, not only enable effective visual processing but also provide surprisingly accurate predictions of brain organization and function.

\paperarg{Integrating learning-based models with program synthesis enables flexible Bayesian models of human concept learning.} 
Recent work has shown how large language models can be combined with program synthesis to create more generalizable Bayesian models of human reasoning~\citep{ellis2024human}. Hand-designed symbolic hypothesis spaces don't always transfer across domains. ~\citet{ellis2024human} showed that one can circumvent these challenges by leveraging large language models to enable use of natural language as a flexible hypothesis space with a data-driven prior tuned to human judgments. By translating natural language hypotheses to executable programs for likelihood computation, such models can capture human learning dynamics in online concept acquisition tasks while continuing to predict human behavior robustly in new domains. This overcomes traditional limitations of pure symbolic program synthesis approaches which can fail to transfer due to incomplete symbolic vocabularies---a manifestation of the frame problem~\citep{shanahan2004frame},  where when deciding the relevant variables for a problem, we might accidentally choose a framing that doesn't generalize to other settings of interest. The integration of learning-based components with program synthesis thus enables Bayesian models that can flexibly adapt their hypothesis space while maintaining interpretable symbolic computation---a goal of many cognitive modellers.


\subsection{Learning from naturalistic data can improve generalization}

Studying how systems generalize is a key method in cognitive science and AI. Conceptual arguments often rely on empirical studies of generalization; however, these generalization properties are often studied in simple settings, with minimal models learning from minimal features \citep[e.g.][]{marcus1998rethinking}. A fundamental assumption underlying this approach is that the simplifications do not alter the generalization problem. However, here we review studies from AI and neuroscience showing that the variability of experience according to seemingly-orthogonal variables can alter how systems learn and generalize---thus implying that studying models within more restricted settings may be misleading about the naturalistic computational problem. 

\paperarg{Example 1: Compositional generalization of image classifiers vs. agents}. Naturalistic tasks can fundamentally change what models learn and how they generalize.~\citet{hill2019environmental} trained two models to do a vision-language grounding task, and tested their compositional generalization to held-out language instructions. Both models were trained on the same language examples, and tested on the same held-out examples. However, one model was trained as an agent that interacts with a simulated environment, while the other was a simple image-language classifier trained on screenshots from that environment. The authors found that the interacting agent exhibited perfect compositional generalization to the novel language utterances, while the image classifier was substantially worse in novel settings. The authors also explored a range of other settings, including generalization benefits of more naturalistic 3D (rather than 2D) environments, or egocentric embodiments. Their results illustrate how richer, more naturalistic settings can enhance the generalizability of the solutions a system learns. Thus, if we are interested in understanding how a system generalizes, we may need to build models that learn from appropriately naturalistic data.

\paperarg{Example 2: Generalization to novel syntactic structures is enhanced by variability in other structures' fillers}
Generalization is of deep interest in linguistics. Recently, \citet{misra2024language} performed controlled experiments in which challenging linguistic constructions are systematically held out from the language model training data---and showed that models trained on naturalistic language can generalize to held out constructions by composing pieces of simpler constructions. Critically, the authors also found that generalization depended on the variability of the semantic fillers observed in the structures in training---thus showing how incorporating naturalistic variation in model training data can impact our theoretical inferences about generalization. 


\paperarg{Example 3: Rats raised in enriched environments}.
Analogously, animals raised in more complex environments can be more skilled than those raised in simpler environments. 
This phenomenon has been well-documented in neuroscience, where rats raised in enriched environments --- with social interaction and/or more space or toys in their home cage --- show greater exploration, and improved learning and memory on novel tasks \citep{simpson2011impact}. This illustrates how, for natural as well as artificial intelligence, increasing the naturalistic variation in experience can alter the system's learning in ways that may impact our experimental and theoretical inferences.


\subsection{Learning with naturalistic data can yield good performance across a range of seemingly disparate tasks}
One interesting finding from the deep learning literature is that when deep learning architectures with many parameters are trained with a lot of naturalistic data and an appropriate ``basic'' learning objective, these architectures can develop mechanisms that go beyond the learning objective they were trained on. Crucially, these learning paradigms can allow models to transfer well (through initial performance or accelerated learning) on novel tasks beyond the training distribution.

Human learning may likewise benefit from transfer among the disparate tasks we learn. We argue that these AI findings motivate computational cognitive science research studying whether ``down-stream human behavior'' on a set of tasks can be recapitulated by a model which is trained on a large set of naturalistic tasks or stimuli representative of some of aspect of human experience---as in meta-learning approaches \citep[cf.][]{wang2021meta}.
Below, we provide examples of this kind of transfer spanning computer vision, reinforcement learning, and natural language processing.

\paperarg{Example 1: Computer Vision}.
One of the earliest successes from deep learning came in computer vision. Soon after AlexNet~\citep{krizhevsky2012imagenet} achieved strong results on the ImageNet dataset~\citep{deng2009imagenet}, researchers showed that the features discovered by AlexNet could be repurposed to novel tasks ranging from scene recognition to medical diagnosis~\citep{donahue2014decaf,sharif2014cnn,litjens2017survey}. This was striking because these features were trained (a) via supervision on a fixed set of objects but (b) enabled transfer to novel objects or even different types of tasks. For years this pattern continued, and researchers even showed that the representations learned by these models could (by 2022, unsurprisingly) transfer from Imagenet to both 3D dexterious manipulation tasks and 3D household navigation tasks, sometimes achieving \textit{better} performance than using ``ground-truth'' state information~\citep{parisi2022unsurprising,yuan2022pre}.
In addition to generally useful representations, deep learning vision models trained with naturalistic data also develop mechanisms they were not trained for.
For example, scene-oriented CNNs develop mechanisms for object detection~\citep{zhou2014object} and vision transformers develop  mechanisms for segmenting objects~\citep{caron2021emerging}---both without an explicit training signal.

\paperarg{Example 2: Reinforcement learning}. Likewise in reinforcement learning, researchers have found that reinforcement learning algorithms trained on many (e.g. billions) of tasks exhibit strong ``out-of-distribution'' generalization on unknown tasks~\citep{team2021open}, with the ability to generalize to novel tasks in the same number of samples as humans~\citep{team2023human}. Some algorithms can even generalize to collaborating with humans without any human data~\citep{strouse2021collaborating}. Recently, reinforcement learning algorithms for learning a modern variant of the successor representation~\citep{gershman2018successor,carvalho2024predictive} have shown that they develop mechanism for exploration and behavioral skills without explicit training signals~\citep{liu2024single}.

\paperarg{Example 3: Large language models}. Perhaps the most striking example of this general phenomenon comes from large language models. Large language models are only trained to predict the next ``token'' (or word) to appear following a sequence of tokens. However, the distribution of internet language effectively includes a broad mixture of many tasks \citep{radford2019language}. When trained on vast amounts of this naturalistic data, large language models develop mechanisms for disparate tasks such as modular arithithmic, solving word problems, etc.~\citep{wei2022emergent}---and even for adapting to new tasks from examples presented in context \citep{brown2020language}.
Moreover, the representations learned by these models can transfer to many superficially dissimilar downstream tasks \citep{lu2022frozen}, even ones as dissimilar as playing video games \citep{reid2022can}. These examples illustrate how learning from a broad naturalistic distribution can induce many abilities---and can provide important transfer to downstream tasks. 

\subsection{Learning from naturalistic data allows us to ask new questions about the origins of knowledge}

The points outlined above have an important consequences; using models that learn from naturalistic data can change our theoretical conclusions in cognitive science. For example, if a model fails to generalize when trained on simple data, we cannot be sure if the model or the data are inadequate. By contrast, using models that learn from naturalistic data can enable us to ask more precise questions about which features---of the models or the data---are necessary to reproduce the theoretically-relevant features of cognitive and neural processes.

\paperarg{Example 1: language models and the learnability of language}.
Recent progress in language modeling from naturalistic data has suggested challenges to prior assumptions about language learnability and innateness. We briefly sketch these developments; \citet{piantadosi2023modern} and \citet{futrell2025linguistics} give much more thorough accounts. Classical approaches to language processing focused on simple models and ideas \citep[e.g.][]{chomsky2014minimalist}, but in doing so imposed strong assumptions. Under certain strong assumptions, \citet{gold1967language} proved that even relatively simple languages cannot be learned from input alone. This proof contributed to arguments that there must be some innate universals such as recursion, and separation of syntax from semantics \citep[e.g.]{chomsky1957syntactic,chomsky1965aspects}---ideas that were subsequently influential in arguments about the nature of cognition more broadly \citep{fodor1975language,fodor1988connectionism}. These theories were based on studying particular language features in great detail\footnote{Or sometimes even studying the intuitions of linguists, with which the average language user would not necessarily even agree \citep{spencer1973differences}.}. Because of this focus, the theories did not attempt to model language processing in all its naturalistic detail. 

However, recent progress in deep learning based language models \citep[e.g.][]{brown2020language} shows that many of these assumptions may need revision. By learning from large amounts of naturalistic data, these models acquire both syntax \citep{manning2020emergent} and semantics \citep{li2021implicit}---and account for a broad range of behavioral and neuroscientific phenomena \citep{schrimpf2021neural}. Of course, these models often learn from inhuman quantities of language \citep{wilcox2024bigger}. However, even language models trained on a human-like amount of language data can learn complex syntactic features that have been previously considered evidence for innate gramatical knowledge \citep[e.g.][]{wilcox2023using}.
Likewise, while some ``impossible'' languages that (adult) humans struggle to learn had been considered evidence for universal grammar, \citet{kallini2024mission} show that language models can more easily learn real languages than impossible ones. 
Finally, controlled experiments---like those from \citet{misra2024language}, reviewed above---allow us to begin to understand which features of experience can contribute to generalizing to truly-novel structures, and how naturalistic variability contributes to this generalization. Thus, incorporating naturalistic data into has helped to reshape our understanding of the necessary and sufficient features for acquiring linguistic capabilities.


\paperarg{Example 2: naturalistic data can lead to brain-like functional specialization}.
Given the importance of perceiving faces in our social lives, it might seem likely that face recognition is innately encoded in the brain. Indeed, the discovery of the Fusiform Face Area (FFA)---a visual region specialized for face perception \citep{kanwisher2006fusiform}---would seem to lend further support to this hypothesis. However, recent computational work shows that this kind of specialization can emerge from a domain-general computer-vision model purely through training on a naturalistic distribution of object and face recognition tasks \citep{dobs2022brain}.
As the authors state: ``It may be that the only inductive bias humans need to develop their face system is the already well-established early preference of infants to look at faces.''
Of course, this does \emph{not} necessarily imply that the FFA is not innately specified. However, it does illustrate how training computational models over naturalistic data can help us to understand the natural constraints that affect neural organization---whether those effects occur over developmental or evolutionary timescales.


% MARK: Building moddels
\section{Building generalizable models} \label{sec:building}



ML research has excelled in developing generalizable models. While we focus on their utility for working on naturalistic stimuli and tasks, we believe cognitive science would broadly benefit from improved practices for developing generalizable models.
In this section, we highlight three key practices of empirical ML research that we believe computational cognitive science can adopt.
The first is frictionless reproducibility: the practice of developing research artifacts which can be reused, and repurposed with minimal effort (\S\ref{sec:build-frictionless}).
The second is a focus on developing generalizable models that can learn successfully across many datasets (\S\ref{sec:build-generalization}).
We argue that these two offer the foundation for having groups of research collectively work on important research problems through the utilization of benchmarks (\S\ref{sec:build-benchmarks}).

Before we continue, we acknowledge that modern empirical ML is a young field which, like other fields, has faced challenges in rigor and reproducibility~\citep{melis2017state,lucic2018gans,riquelme2018deep,henderson2018deep,recht2019imagenet,agarwal2021deep}.
Despite these challenges, we are optimistic that some of the practical knowledge ML has developed is valuable for improving rigor and reproducibility, as we highlight below.
\begin{figure}[htp]
  \begin{center}
      \includegraphics[width=\textwidth]{overview-engineering.pdf} 
      \caption{Overview of key strategies cognitive science can adopt to develop generalizable models. (Figures reproduced from \citep{geirhos2018imagenet,bowers2023deep})}
      \label{fig:overview-engineering}
  \end{center}
\end{figure}


\paperarg{Section summary.}
We envision a future where cognitive scientists identify a cognitive phenomenon of interest (e.g., theory of mind, working memory, or object recognition) and create a phenomena-oriented benchmark \textbf{(R8}) containing experiments specifically designed to test different theoretical aspects of that phenomenon.
Here, experiments are implemented with standardized evaluation protocols and interfaces \textbf{(R1)} that enable direct comparison between competing theoretical frameworks.
All experimental data and stimuli are made available through one-line download access \textbf{(R2)}.
Models are developed and released following frictionless reproducibility standards \textbf{(R1-R3)} for easy adoption and modification.
Researchers identify gaps in theoretical coverage and contribute new experiments to the benchmark, facilitating maintaining a separation between model and task design \textbf{(R6)}.
Thanks to the existence of a benchmark and frictionless reproducibility standards, models are evaluated comprehensively across all experimental paradigms with minimal additional human labor.
This helps mitigate the challenge of disconnected, incommensurate findings that resist integration across studies~\citep{almaatouq2024beyond}.
Thanks to the leaderboard principle \textbf{(R7)}, this also helps ensure that progress is defined by substantial rather than incremental advances.
This approach encourages the development of models that work across both simplified and naturalistic contexts \textbf{(R4)} and that generalize across multiple experimental paradigms \textbf{(R5)}.
Importantly, in addition to a model that predicts behavior across many tasks, we also have a model that can perform many tasks that humans can---\textbf{enabling the building of human-like models that scale and generalize}.


\subsection{Frictionless reproducibility: a superpower}\label{sec:build-frictionless}
Within cognitive science and psychology, there is currently substantial friction when using  data or code of prior work~\citep{nosek2015promoting,hardwicke2018data}.
Researchers may not share data in a standard or easy to use format, often don't fully report their analysis methods, or provide code that contains errors~\citep{poldrack2017scanning}.
In a large-scale replication analysis of open code and data, researchers found that $38\%$ of code was not usable and \textit{only $31\%$} of workflows had reproducible results~\citep{hardwicke2018data}.
These factors may have contributed to the replication crisis~\citep{lilienfeld2017psychology,shrout2018psychology}.

In recent years, data-centric sciences are experiencing a profound 
transition to what David Donoho calls ``\textit{frictionless reproducibility}''~(\citealp{donoho2024data}; cf.~\citealp{recht2024mechanics}).
Donoho identifies three key pillars to frictionless reproducibility: data sharing, code sharing, and competitive challenges.
We will focus on the first two in this section, and competitive challenges in \S\ref{sec:build-benchmarks}.
Critically, with frictionless reproducibility, the research artifacts that are produced allow future researchers to exactly re-execute the same \textit{complete} workflow with minimal effort.
With this, good ideas are spread, adopted, and improved with little friction.
Donoho argues that ML is ostensibly the \textit{most successful} at frictionless reproducibility.
Below we review some supporting strategies that may aid cognitive science.


\subsubsection*{Recommendations for cognitive science}

\paperarg{R1. Use a standardized evaluation protocol where models can be compared via a common interface}.
Machine learning's foundation of standardized evaluation protocols traces back to 1959, when Highleyman created the first alphanumeric dataset and the first ever ``train-test'' split for comparing pattern recognition~\citep{highleyman1959generalized}.
This strategy continued with the famous MNIST alphanumeric dataset~\citep{lecun1998gradient}. MNIST simplified data-sharing and evaluation by pre-processing data, fixing the train and test distributions, and sharing the data online.
This standardization and usage of a common interface enabled fair comparisons between diverse approaches, from boosted stumps~\citep{kegl2009boosting} to support vector machines~\citep{cristianini2002support} to nearest neighbor methods~\citep{khan2017mcs}.

Standardized evaluation protocols serve a critical function: \textbf{mitigating personal bias} in model assessment. The evolution from MNIST through Caltech 101~\citep{fei2004learning} to ImageNet~\citep{deng2009imagenet} demonstrates this principle, as increasingly naturalistic datasets 
enabled fair comparisons between diverse model families. This enabled AlexNet~\citep{krizhevsky2012imagenet} to validate deep learning in an era when many researchers doubted its potential.

Thus, we recommend that cognitive scientists establish clear \textit{hold-out} data for either model development or data analysis. This is especially critical for confirming exploratory results~\citep{poldrack2017scanning}.
Combined with standardized evaluation protocols, this will enable fair comparison between diverse theoretical frameworks ranging from program synthesis to deep neural networks to reinforcement learning and active inference.

\paperarg{R2. Make data accessible with a single line of code}. 
\textit{Fully processed} research data should be \textit{programmatically} accessible with a single line of code, requiring no permissions.
While uploading data to platforms like the Open Science Framework (\href{https://osf.io/}{OSF}) is valuable, we additionally recommend uploading data to platforms like \href{https://huggingface.co/}{Hugging Face} that offer version control, free hosting, authentication-free access, the ability to download data with one line of code, and to upload data with a few lines of code.
This removes friction from reusing and updating datasets.

\newcommand{\twoline}[2]{%
    \begin{tabular}{@{}l@{}}
        #1 \\
        #2
    \end{tabular}%
}

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|l|}
    \hline
    \textbf{Model} & \textbf{Citations} & \textbf{Age} & \textbf{GitHub Stars} & \textbf{Single file examples} \\
    \hline
    \twoline{ResNet}{\cite{he2016deep}} & 247,921 & 8 years & 2,300 & \href{https://github.com/facebookarchive/fb.resnet.torch/blob/master/models/resnet.lua}{model} \\
    %\hline
    %\twoline{RL World Model}{\cite{kipf2019contrastive}} & 315 & 5 years & 387 & \href{https://github.com/tkipf/c-swm/blob/master/modules.py#L9}{model}, \href{https://github.com/tkipf/c-swm/blob/master/train.py}{training} \\
    \hline
    \twoline{Contrastive Learning}{\cite{he2020momentum}} & 13,480 & 4 years & 4,800 & \href{https://github.com/facebookresearch/moco/blob/main/moco/builder.py}{model}, \href{https://github.com/facebookresearch/moco/blob/main/main_moco.py}{training} \\
    \hline
    \twoline{Object Discovery}{\cite{locatello2020object}} & 790 & 4 years & 394 & \href{https://github.com/lucidrains/slot-attention/blob/master/slot_attention/slot_attention.py}{model} \\
    \hline
    \twoline{RL Library}{\cite{huang2022cleanrl}} & 252 & 2 years & 5,600 & \href{https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py}{model 1}, \href{https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_trxl/ppo_trxl.py}{model 2} \\
    \hline
    \twoline{RLHF Method}{\cite{rafailov2024direct}} & 1,730 & 1 year & 2,200 & \href{https://github.com/eric-mitchell/direct-preference-optimization/blob/main/train.py}{train} \\
    \hline
    \twoline{Multi-agent RL}{\cite{rutherford2023jaxmarl}} & 24 & 0 years & 433 & \href{https://github.com/FLAIROx/JaxMARL/blob/main/baselines/IPPO/ippo_cnn_overcooked.py}{model} \\
    \hline
    \end{tabular}
    \caption{Comparison of ML libraries that adopted the single file philosophy.}
    \label{tab:ml-models}
\end{table}

\paperarg{R3. Adopt the ``single file'' philosophy}.
Ideally, key workflows (e.g. model or evaluation definititions)
should be defined in a single file. This has emerged as a powerful force for reproducibility in ML research (see Table~\ref{tab:ml-models} for examples).
While this approach contradicts traditional software engineering principles of modularity and abstraction, it offers distinct advantages for research code: (1) complete understanding of an important component (e.g. how a model works) requires reading only one file, (2) minimal abstraction layers and dependencies, and (3) easy adaptation of either complete implementations or specific components. One can simply \textit{copy} this single file into another codebase. Removing friction from future reuse also benefits us---as we can see from Table~\ref{tab:ml-models}, these methods get a large number of citations. 

% MARK: Generalization-oriented model development
\subsection{Developing models with an eye towards generalizability}\label{sec:build-generalization}

Task-performing computational models have been a goal of cognitive science since ~\citet{newell2012you}. 
This sentiment still exists today~\citep{kriegeskorte2018cognitive,almaatouq2024beyond} and is exemplified in conferences that explicitly seek to integrate cognitive science and neuroscience with AI
~\citep{naselaris2018cognitive}.
Cognitive scientists are well aware that they can't rely on a single paradigm for the studying an effect if they want to achieve generalizable conclusions~\citep{holzmeister2024heterogeneity,yarkoni2022generalizability}.
Ideally, the models we develop generalize beyond the setting (e.g. stimulus set or task-specification) they were designed for.
This requires two things.
First, that the model can \textit{operate} over new stimulus sets and generalize its behavior to new tasks.
Second, that the behavioral (and potentially neural) predictions made by our models capture human behavior on new tasks and new stimulus sets.
Clearly, the first is a requirement for the second.
Here, we argue that machine learning, with its emphasis on frictionless reproducibility, has made significant strides in developing models that can generalize.
We detail recommendations for cognitive science below.

\subsubsection*{Recommendations for cognitive science}

\paperarg{R4. Develop models with simplified stimluli but ensure they work with increasingly naturalistic stimuli}.
The frame problem~\citep{shanahan2004frame} and the ``simulation is doomed to succeed'' critique~\citep{grim2013simulations} highlight a fundamental challenge in model development: the evaluation tasks we design often fail to capture the full complexity of the phenomenon we care about.
Empirical machine learning addresses this by developing models that must succeed in both simplified and increasingly complex settings.
Below we detail two examples.


\textit{Example 1: Generative Models}. One of the most successful models of machine learning has been the variational autoencoder~\citep{kingma2013auto}, a generative model that learns to infer latent variables which describe an observation. When this model was released they showed results inferring latent variables correctly on both ``simplified'' MNIST digits and more complex images of real-world faces. Importantly, in this more complicated setting where the latent dimensions did not have obvious correspondence, they performed a qualitative analysis showing their model had learned a face manifold capturing semantically meaningful dimensions of faces such as sentiment and facial direction. 

\textit{Example 2: Reinforcement Learning}. 
The history of RL is one of progress towards increasing naturalism: initial environments were grid-worlds~\citep{sutton2018reinforcement}, later 2D video games like atari~\citep{bellemare2013arcade}, then virtual 3D home environments~\citep{szot2021habitat,li2021igibson}, and now an emphasis on open-world environments~\citep{fan2022minedojo,matthews2024craftax,hughes2024open}.
Despite the increased emphasis on naturalistic settings, there is still emphasis on the importance of toy settings for studying specific aspects of your model~\citep{osband2019behaviour,obando2024small}.
Indeed, the strategy is almost always to \textit{first} develop your model for simplified settings, but one should always then see that your model continues to ``work'' with more naturalistic  settings.
Collectively, this has allowed RL researchers to both advance a theoretical understanding of how these models work~\citep{dadashi2019value,grimm2020value,lyle2022understanding}, an empirical understanding of how they work~\citep{obando2024small,farebrother2024stop}, but also develop performant models for a wide range of ``complex'' settings including beating the world's best players in games like Go~\citep{silver2017mastering} and controlling robots in the real world~\citep{levine2016end,cheng2024extreme}.


\paperarg{R5. Develop models that work with many stimulus sets}.
The ultimate goal of ML has always been generalization~\citep{highleyman1959generalized,hardt2022patterns,recht2024mechanics}.
Initially, this meant models that generalize to new data.
Now, it means both models and learning procedures that generalize to new settings.
Thus, the practice of evaluating models across diverse datasets has become foundational in machine learning.
This has manifested with research papers where ML models are evaluated on numerous datasets to showcase their generality.
Early examples include ``dropout''~\citep{srivastava2014dropout}, which demonstrated generalization improvements across 6 distinct image and speech datasets, and MoCo~\citep{he2020momentum}, which validated its contrastive learning approach on 9 computer vision datasets. 
Another notable example is the seminal ``Deep Q-Network'' paper~\citep{mnih2015human}, which showed that a single neural network architecture with fixed hyperparameters\footnote{Hyperparameters are parameters that are set for a model and algorithm before training, such as the number of layers in a neural network or the learning rate of the optimizer} could master 50 different Atari games.
The field has since evolved to group environments by the cognitive capabilities they test (e.g., exploration, generalization, manipulation)~\citep{patterson2023empirical}, with researchers developing models on one set of tasks and validating on entirely different ones to ensure robust generalization~\citep{machado2018revisiting}.

We recommend cognitive scientists also test the models that support their theories more broadly. Importantly, we should develop our models with some stimulus sets or tasks, and then evaluate them (e.g. learn the parameters) on a different set of stimulus sets or tasks~\citep{machado2018revisiting}. This further helps prevent our computational model (and thereby theory) from ``overfitting'' to a particular paradigm or set of stimuli.

\paperarg{R6. Iterate between between (1) model-design (2) task design where the other is fixed}.
One crucial lesson from ML research is the importance of separating model development from task design.
\citet{patterson2023empirical} strongly caution against simultaneously developing both the problem setting (datasets or tasks) and the solution method, as researchers may inadvertently design evaluation environments that favor their proposed solution.
Historical examples illustrate this risk. When developing Q-learning algorithms, researchers modified the classic pendulum control problem by adding episode cutoffs and random state resets \citep{machado2018revisiting}. While these modifications made Q-learning more tractable by implicitly improving exploration, they obscured fundamental limitations in Q-learning's exploration capabilities compared to policy gradient methods.
Maintaining a strict separation between task design and model development enforces a clear delineation between hypothesis formation and testing, which is \textbf{important for reducing experimenter bias}.
This is especially important because researchers tend to find ``positive'' results~\citep{scheel2021excess,sarafoglou2022survey}, a bias exacerbated by an increasingly competitive academic landscape~\citep{lee2014publish, reithmeier201910, woolston2021researchers}.
Much of ML's progress can be attributed to researchers evaluating their models on independently developed datasets and tasks---a practice that not only simplifies the research process but also promotes scientific rigor.


Thus, we recommend that cognitive science also develop models on tasks/stimuli that have been developed by others.
This may seem to contradict science, where researchers develop a hypothesis and create an experiment to test their hypothesis. The data from prior work was for a different hypothesis!
We recommend that the field begin to explore phenomena-oriented benchmarks that aggregate stimuli and tasks designed to test specific theoretical predictions for broad phenomena.
This approach would allow researchers to leverage existing experimental paradigms while maintaining scientific rigor through independent validation.
We detail this more in the next section.


% MARK: Benchmarks
\subsection{Benchmarks: catalysts for progress and innovation}\label{sec:build-benchmarks}

Another challenge that cognitive science faces is fragmentation. This has been true since Newell's seminal critique \citep{newell2012you}, where his observation that piece-wise investigation of brain components would not yield a cohesive understanding remains relevant forty years later~\citep{schrimpf2020integrative,dicarlo2023let}. Current experimental paradigms often produce disconnected, incommensurate findings that resist integration across studies, even within the same domain \citep{almaatouq2024beyond}. 
This fragmentation manifests not only in methodology but also in how researchers distribute their attention across different problems, potentially diluting collective progress.

The history of machine learning offers an instructive parallel. In its early days, researchers worked on disparate, self-defined problems, leading to diffuse progress \citep{recht2024mechanics}. The introduction of shared benchmarks marked a crucial shift: researchers began focusing on standardized problems that others had identified as important.
We argue that this led subpopulations of researchers to converge on small subsets of problems.
Putting aside whether this is good or bad, it's been effective.
Indeed, this is the third pillar of frictionless reproducibility which~\citep{donoho2024data} argues has been a major catalyst for progress in ML: competitive challenges.
We argue that cognitive science can similarly stand to benefit from benchmarks.  

\subsubsection*{Recommendations for cognitive science}

\paperarg{R7. Embrace the leaderboard principle}.
A common concern with benchmarks is that researchers will ``overfit'' to test sets, which theoretically should underestimate true test error \citep{hastie2009elements}. Yet despite widespread iteration on benchmarks in machine learning---a practice that ostensibly violates statistical principles---the field has produced models with strong performance on held-out data. This apparent paradox is partially explained in a recent machine learning textbook~\citep{hardt2022patterns} by a phenomenon known as the ``leaderboard principle''. Researchers typically only publish and build upon models that demonstrate substantial improvements over prior state-of-the-art results, rather than minor variations. This selective pressure towards meaningful advances effectively constrains the degree of adaptation to test sets, as researchers focus on substantial improvements rather than exhaustively exploring the test set's peculiarities. 
Still, a risk of overfitting to data remains. We argue that the remedy to this for cognitive science is the creation of phenomena-oriented benchmarks, where researchers can contribute new challenges that identify a missing component of a phenomenon of interest. We detail this below.

\paperarg{R8. Develop \textit{dynamic} benchmarks to 
iteratively capture important phenomena}. 
ML benchmarks are not collected to test specific hypotheses~\citep{hardt2022patterns} and so may seem unscientific. 
However, we argue that cognitive science can embrace ``scientific'' benchmarks oriented toward testing specific hypotheses. We use the story of ``Brain-Score'' as an illustrative example of \textit{initial} steps in this direction.

Brain-Score~\citep{schrimpf2020integrative} began as a benchmark with a collection of behavioral and neural data from humans and monkeys on object recognition tasks.
One common critique of Brain-Score is that the behavioral analysis essentially boils down to whether models and humans found the same stimuli easy and challenging to identify.
However, it did not test \textit{hypotheses} about how humans identify objects, nor show that models identified objects in this same way.
This was perhaps best described by \citet{bowers2023deep}, who noted that matching predictions of humans was not enough to show that this was a good model of human object recognition.
In a scathing critique, they reference 9 experiments from the human psychophysics literature which presented stimuli generated to test specific hypothesis and showed that deep neural networks failed to capture human behavior across these.
Examples include stimuli sets that tested the hypothesis that object recognition was based on local shape features~\citep{baker2020local}, stimuli sets that tested the hypothesis that human object recognition is defined by shape more than by texture~\citep{geirhos2018imagenet}, and stimuli sets that tested the hypothesis that pairs of objects were judged to be similar based on shape contours~\citep{puebla2022can}.
In response,~\citet{dicarlo2023let} added many of these stimulus sets to Brain-Score so that when future models are tested, they are tested on whether they capture human behavior across all of these stimulus sets.

We believe this story illustrates what cumulative, dynamic naturalistic computational cognitive science research could look like: where researchers can develop models and experiments that easily integrate with prior findings on a phenomenon thanks to phenomena-oriented benchmarks, frictionless reproducibility, and the development of generalization-oriented models.
Of course, we must still continue to analyze and study our models to build comprehensive theories, as we discuss in the next section.


% MARK: scientifically studying
\section{Building from naturalistic experiments to cognitive theories}  \label{sec:theory}

\begin{figure}[htp]
    \begin{center}
        \includegraphics[width=\textwidth]{overview-theory.png} 
        \caption{Overview of how we can develop theories with potentially opaque models and unnatural manipulations of natural data. (Panel C bottom figure is reproduced from \citealt{saxe2019mathematical}.)}
        \label{fig:overview-theory}
    \end{center}
\end{figure}


A primary goal of cognitive science is understanding. Experiments and computational modeling are key constraints on theory building. By instantiating our theories in a model, we are forced to make our theories more precise by concretizing the ambiguous details of the mapping from a verbal hypothesis to its implementation~\citep{guest2021computational}.

From this perspective, building computational models that perform the task in as naturalistic a way as possible, across as wide a variety of settings as possible, imposes much stronger constraints on our theories than an abstracted model at a higher-level \citep[cf.][]{cao2021explanatorypt2}. These added constraints are important; otherwise, high-level theories that do not directly engage with the details of the implementation can be so unconstrained as to lack explanatory value \citep[cf.][]{erev20151800,jones2011bayesian,rahnev2018suboptimality,andrews2021math}, can be intractable for real problems \citep{van2008tractable}, or can elide important details, as we have outlined above. 

Yet moving towards naturalistic settings, and the complex models they require, alters the way that we need to approach experimentation and theory building.  In this section we discuss the consequences for experiment design (\S \ref{sec:theory:controlled_experiments}-\ref{sec:theory:incorporating_variation}), the role and interpretation of models (\ref{sec:theory:interpreting_models}), and theory building (\S \ref{sec:theory:combining_prediction_with_explanation}).



\subsection{Performing controlled experiments by parametrically manipulating naturalistic data (in unnatural ways)} \label{sec:theory:controlled_experiments}

\begin{figure}[h!t]
\centering
\includegraphics[width=0.6\textwidth]{dimensions-controlled-experiments.pdf}
\caption{Testing hypotheses by parametrically manipulating naturalistic data. In the natural data distribution (blue), features like shape and texture are highly correlated---cats usually have cat fur, and elephants usually have elephant skin. This confounding can make it difficult to discern how these variables independently contribute to processing in neural systems or models. One approach to deconfounding these variables is to use a narrow distribution of highly-simplified artificial stimuli (light green) that isolate the variables of interest. However, we advocate for (additionally) creating broader distributions of stimuli that preserve more of the breadth of the natural data distribution, while deconfounding or manipulating features parametrically in order to test hypotheses---as in the unnatural combinations of natural shapes and textures created by \citet{geirhos2018imagenet}. } \label{fig:data_dimensions_controlled_experiments}
\end{figure}

Developing and testing models on truly naturalistic data introduces new issues that are not present in simple artificial settings. Naturalistic data distributions may be too ``easy''---for example, they may only rarely include edge-cases that test key capabilities \citep{zhang2023deep}; thus, testing average performance over a naturalistic distribution may not identify important failures. Relatedly, in naturalistic data, features may be confounded, which can prevent readily disentangling how a model is solving a task \citep{rust2005praise}. For example, models trained on ImageNet \citep{deng2009imagenet} rely on features like texture more than humans do \citep{geirhos2018imagenet}, but 
still perform similarly to humans on ImageNet. These issues limit our ability to achieve deep understanding from experimenting with only the distribution of naturalistic data \citep{rust2005praise}. 

However, we believe that these challenges are surmountable by parametrically manipulating rich, naturalistic data (Fig. \ref{fig:data_dimensions_controlled_experiments})---preserving or enhancing its richness, while more carefully accounting for theoretically motivated constructs. For example, to effectively test processing of challenging syntactic structures, \citet{futrell2021natural} edited natural stories to increase the density of these rare constructions, while preserving the richness and naturalism of the original content. Similarly, the work of \citet{geirhos2018imagenet} engineered datasets that combine natural shapes and textures in unnatural ways. To do so, the authors exploited the contemporary ML technique of iterative style transfer \citet{gatys2016image}---combining the features of different images at different spatial scales---thus illustrating how progress in AI models unlocks new experimental approaches. These conflicting shape-texture datasets helped to inspire similar experiments on humans \citep{jagadeesh2022texture,jagadeesh2022human}; surprisingly, those studies found that human visual cortex likewise appears to use textural rather than shape-focused representations, and shape-driven behavior is thus mediated by downstream readout processes. Thus, manipulating features to test models can lead to new insights into brain function.

A growing range of studies have taken similar approaches to systematically manipulate or ablate features of a natural stimulus, while preserving as much of the natural variety as possible---whether introducing new parameters like modality manipulation of a stimulus \citep[e.g.][]{deniz2019representation} or systematically selecting stimuli from a broad distribution to drive particular responses while preserving variety \citep[e.g.][]{tuckute2024driving,hosseini2023teasing}. We likewise advocate for testing on tasks that augment natural data distributions with unnatural systematic manipulations in accordance with the hypothesis to be tested \citep[cf.][]{jain2024computational}. Experimenting with rich settings does not mean giving up theory-driven experimental manipulation; it simply means situating those controlled experiments in more complex, naturalistic settings. For example, there has been a growing research direction using games as richer settings for the experimental study of cognition \citep{allen2024using}. Moreover, naturalistic experiments \emph{enhance} our ability to develop generalizable theories, as we argue below.

\subsection{Incorporating naturalistic variation to enhance generalizability}   \label{sec:theory:incorporating_variation}


By testing hypotheses in settings where other aspects of the task and data generating process are sampled broadly from the naturalistic distribution, we address some of the challenges of generalizability that arise from testing on a much narrower distribution than the written hypothesis implies \citep[cf.][]{yarkoni2022generalizability}. For example, if the hypothesis is about visual processing in general, but the task paradigm relies solely on abstract shape stimuli, the conclusions may not generalize in the way the written results would suggest. Thus, testing our hypotheses across varied settings---``design[ing] with variability in mind'' \citep{yarkoni2022generalizability}---increases the likelihood of identifying robust effects that will generalize \citep[cf.][]{baribault2018metastudies}. In the language of regression, varying as many other data generating factors as possible brings us closer to estimating the ``main effect'' of an experimentally-manipulated construct, rather than estimating the ``simple effect'' of the manipulation in the single setting we have tested. 

For example, it is a long-standing principle of experimental design to test a hypothesis with multiple stimuli, and statistically quantify the stimulus effect to estimate the generalizability \citep[e.g.][]{clark1973language}. However, many other experiment features---even exact task formulations such as multi-arm bandit tasks---are often preserved within a paper, or even across an entire literature. Just like testing a hypothesis with a single stimulus, this lack of variation poses challenges for generalizability \citep[cf.][]{eckstein2022interpretation}---especially because the way that researchers operationalize a theory can substantially affect their conclusions \citep[e.g.][]{holzmeister2024heterogeneity,schweinsberg2021same,strickland2012experimenter}. While naturalistic variation cannot resolve this issue in full, we believe that incorporating a broader range of the natural variation of settings, stimuli, and tasks within which we expect those theories to hold, will help us to develop more generalizable theories. 

Incorporating variation into our experiments can also help us to revise the constructs underlying our theories. For example, \citet{eisenberg2019uncovering} study the putative construct of self-regulation by simultaneously testing a large variety of existing self-regulation measures. The authors find that these measures do not form a unitary construct. Rather, task- and survey-based measures each tap into distinct, multi-factorial constructs, with individual tasks loading primarily onto a subset of the factors in their domain. That is, two different studies that each use only a single measure for self-regulation may be tapping into entirely different constructs. By contrast, by substantially varying the tasks we use to measure a theoretical component like self-regulation, we can more fully disentangle the underlying constructs---and thereby improve the extent to which our theories generalize.


\subsection{Interpreting complex models to yield explanations} \label{sec:theory:interpreting_models}

If we perform experiments in naturalistic settings, and develop complex models that can predict human behavior across them, how does this support explaining the cognitive phenomena of interest? In this section we will review some paths to deriving explanations of phenomena from complex models.\footnote{N.B., not all complex systems admit simple, abstract explanations. For example, where natural intelligence incorporates chaotic \citep{freeman1995chaos} or critical \citep{obyrne2022critical} dynamics, they may alter the kind of explanations we can seek \citep[cf.][]{kellert1993wake}. A benefit of seeking predictive models as a route to explanation is that the models may be useful even if simple explanations do not follow.}

\paperarg{Data properties as explanation \& rational analysis.} One route to explaining model behavior is as a consequence of the properties of the data from which it learns. This idea stretches back through behaviorism as well as cognitive science; for example, connectionist research has often focused on how naturalistic data could drive cognitive phenomena \citep{mcclelland1991nature,elman1996rethinking,rogers2004semantic}. Analogously, in AI, various researchers have taken inspiration from behaviors that emerge in large pretrained models, and similarly attempted to identify minimal properties of naturalistic data that give rise to those behaviors. For example, \citet{chan2022data} explore how the bursty, long-tailed nature of natural data can give rise to in-context learning, and \citet{prystawski2024think} examine how local dependencies can yield certain kinds of sequential reasoning. These works illustrate how rich properties of natural data can be distilled down to the minimal elements that give rise to a behavior---and can thus likewise offer a candidate explanation for the origin of those capabilities. Moreover, explanations of behavior in terms of the property of the data from which a system learns offer a route toward \emph{normative} explanations of that behavior as a rational solution to constraints---precisely the perspective taken by \emph{rational analysis} \citep{anderson1991human}.

\paperarg{Mechanistic explanation.} Understanding that a behavior arises from properties of data does not explain how that behavior is implemented. Fortunately, the internal workings of task-performing computational models can generally be inspected and intervened upon. For example, in contemporary AI research, a variety of works have analyzed the mechanisms that implement model behaviors, either in simplified settings \citep[e.g.][]{ nanda2023progress,zhong2024clock}, or even in large models trained on natural data \citep{geiger2021causal,wu2024interpretability}. A particularly promising approach for cognitive science is illustrated by Distributed Alignment Search \citep{geiger2024finding}, which attempts to map postulated abstract causal models onto low-level features of the model that play corresponding causal roles---thus enabling the link between abstract models or hypotheses about the computational solution, and evidence about concrete implementational mechanisms.
Morever, mechanistic interventions can even be combined with data studies, to examine the causal role of different mechanisms over the course of learning \citep{singh2024needs}. Appropriate regularization of models can also produce representations that yield greater interpretability---e.g. \citet{miller2024cognitive} train networks to predict animal behavior while restricting their representations, and used these to identify non-optimal features influencing the decisions---thus allowing bottom-up discovery of possible implementations. Thus, various methods for mechanistic study of models can offer powerful routes to experimentally determining sufficient implementations of cognitive processes.

\paperarg{Formal theories.} From the way that behavior emerges from data or mechanisms, we can progress to formal theories. \citet{saxe2019mathematical} offers an illustrative example. Prior works observed empirically that pseudo-naturalistic data give rise to various aspects of human-like semantic development and representation in neural networks \citep{rogers2004semantic}, such as progressive differentiation. However, these works had not explained \emph{why} these parallels emerge. \citet{saxe2019mathematical} bridged this gap by formally deriving how the data properties combine with gradient-based learning to yield these cognitive phenomena. This illustrates how preliminary explanations in terms of data can be developed into more rigorous analytic theories. Thus, building naturalistic models can be a stepping stone toward more rigorous, normative theories of intelligence as a rational solution to a particular problem setting---much like prior work in cognitive science 
\citep[e.g.][]{anderson1991human,oaksford2009precis}.

\subsection{Theories that combine task-performing models with reductive explanation.} \label{sec:theory:combining_prediction_with_explanation} 
Above, we have argued for the value of embracing the richness of naturalistic tasks, even if it leads us to build more complex task-performing models that are hard to interpret, but that perhaps make more accurate predictions of behavior. In the previous section, we have outlined some of the possibilities (and challenges) of deriving understanding from these models. So what, ultimately, should we seek?

We argue that naturalistic computational cognitive science should seek theories of cognitive phenomena that consist of two components:
\begin{enumerate}
\item Task-performing (predictive) models that reproduce the phenomena across the same range of naturalistic stimuli and paradigms as the human/animal subjects.
\item Reductions of these task-performing models to simpler mechanisms, properties, and theories of \emph{why} these models reproduce the phenomena.
\end{enumerate}
These two components serve different purposes. The first helps to demonstrate that the models are real candidate models of the phenomena in question, rather than models that simply could not scale to the real problem or that are overfit to a particular instantiation of the task. We believe this component is necessary because otherwise the problem space is under-constrained, as we have argued above. Moreover, it provides a test-bed for performing experimentation that would be impossible in reality (e.g. beyond what would be feasible or ethical in the lab), which is useful for both scientific and practical reasons. The second component---where feasible---allows linking these predictions to the explanatory understanding that cognitive scientists have usually sought, and that may help us to generalize our insights beyond the current paradigms.  This perspective aligns with past arguments on how deep learning can contribute to understanding in cognitive neuroscience \citep{saxe2021if,kanwisher2023using,cao2021explanatorypt2,doerig2023neuroconnectionist}. However, we emphasize that combining these two components makes explicit links between the actual problem the system solves and the theorized constructs underlying that solution. It also ensures that our theories make testable predictions beyond the narrow settings of a particular task paradigm.

\section{Discussion}\label{sec:discussion}

In this paper, we have tried to outline a direction of research that we call ``naturalistic computational cognitive science'' that aims to build generalizable models of cognition that scale to naturalistic tasks, while still offering routes to explanatory and theoretical understanding. This perspective synthesizes a growing literature on the importance of naturalistic experiments and generalizable models, and grows out of a long-standing focus in cognitive science on explaining a broad range of phenomena. We outline these connections here.

\paperarg{The quest for building general models of natural intelligence.} 
A long history of cognitive science research has sought to develop frameworks with the generality to explain a wide scope of cognitive phenomena. Researchers in connectionist \citep[e.g.][]{mcclelland1986appeal,mcclelland2003developing}, Bayesian \citep[e.g.][]{tenenbaum2001generalization}, and cognitive architectures \citep[e.g.][]{ritter2019act,laird2019soar} paradigms have tried to identify underlying principles or mechanisms that explain many phenomena. However, while the modeling frameworks themselves are general, typical \emph{instantiations} of these frameworks have built specialized models focused on individual tasks.


Other recent studies have built upon foundation models from AI that can perform many tasks. Some works have applied cognitive paradigms to study the behaviors of these models \citep[e.g.][]{binz2023using,lampinen2024language,buschoff2025visual}. Other recent works have finetuned these models using cognitive data, to create generalizable cognitive models that can predict behavior on new experimental paradigms in areas like vision \citep[e.g.][]{muttenthaler2024improving,muttenthaler2024aligning,fu2024dreamsim}, or the broader space of cognitive tasks that can be presented in language \citep{binz2024turning,binz2024centaurfoundationmodelhuman}.  

We see the arguments that we have laid out in this paper as being broadly compatible with many of these approaches and frameworks, but offering a stronger emphasis on the virtuous cycle of expanding the scope and naturalism of our experimental designs, and expanding the generalizability of our models and theories---and anchoring these in the practicalities of achieving frictionless reproducibility.

\paperarg{Towards experiments that involve naturalistic behavior}
In emphasizing more naturalistic experiments, our perspective aligns with a variety of works that have likewise highlighted the importance of considering the scope of complex naturalistic behaviors. This perspective has been advocated frequently in neuroscience \citep[e.g.][]{mobbs2021promises,cisek2024toward}, where even prominent visual processing signals can be fundamentally different in active, naturalistic paradigms than in simpler classic ones \citep[e.g.][]{amme2024saccade}. Likewise, \citet{wise2023naturalistic} advocate for the need to explore more naturalistic environments and behaviors in reinforcement learning, to grapple with the complexity of behavior in environments closer to the real world. We concur with these works, and emphasize that advances in technology both enable richer experimental paradigms, and the models that can perform them. We differ from many of these works in emphasizing the role of models that generalize across task paradigms.

\paperarg{Linking between learning models and the brain}
A large recent literature has explored the surprising alignment between the representations learned by task-optimized deep learning models and those in the brain \citep{khaligh2014deep,yamins2014performance,yamins2016using,schrimpf2021neural,sucholutsky2023getting}. The observations of surprising alignment have led to substantial interest and debate. The reactions range from critical argument that failures to capture particular phenomena doom these models \citep[e.g.][]{bowers2023deep}, to suggestions that deep learning models upend the theoretical assumptions of broad areas of the field \citep{perconti2020deep,hasson2020direct}.
Other authors have written frameworks that attempt to integrate deep learning models within the existing scientific paradigms \citep{doerig2023neuroconnectionist}.
In this vein, several papers have proposed frameworks for understanding the role of deep learning in cognitive neuroscience \citep[e.g][]{richards2019deep,storrs2019deep,cichy2019deep}---and how these models can contribute to deriving explanatory and theoretical understanding \citep{cao2024explanatorypt1,cao2021explanatorypt2,saxe2021if}. In this context, \citet{feather2025brain} argue for the importance of benchmarks that place stronger constraints on models, by testing alignment of both behavior and representations. As above, our perspective aligns with many of these works, but we place greater emphasis on the complementary role of increasing naturalism of experimental paradigms, and learning from the engineering paradigms of AI, rather than simply adopting its models as artifacts.


\paperarg{Conclusions}
We have outlined a perspective that advocates for studying cognition through a broad spectrum of naturalistic experimental paradigms, building generalizable models of cognition that can perform naturalistic tasks, and deriving explanatory theories from these models and their simplifications. We have supported this perspective with examples illustrating the value of naturalistic settings and generalizable models, and concrete guidance on the practicalities of building generalizable models and using them to shape our theories. Many individual aspects of our argument align with prior works, but we hope that there is value in synthesizing these perspectives and highlighting their synergy. We hope that this perspective will inspire computational cognitive scientists to continue embracing richer naturalistic experimental paradigms and models.


\clearpage




\clearpage
\subsection*{Acknowledgements}
Sam Gershman for guidance and feedback on this article and on how to integrate cognitive science and machine learning methodology.
We thank Maria Eckstein, Tyler Bonnen, Jay McClelland, Mike Mozer, Rachel Calcott, Nick Blauch, and Shuze Liu for thoughtful comments and discussions. We thank Felix Hill for many inspirational conversations on generalization, cognition, and more.
\bibliographystyle{apalike}
\bibliography{bib}


\end{document}
