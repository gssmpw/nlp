\section{EXPERIMENTAL ANALYSIS}
Based on the constructed \method, we conduct experiments to explore the effectiveness of the reasoning strategies for zero-shot TSF across eight datasets and four settings.
As shown in Figure 2 and Table XXX, we make the following observations regarding this research question.
\newpage
\subsection{RQ1: Can TSF Benefit from Enhanced Reasoning Ability?}
\observationbox{Overall Answer: TSF performance can benefit from enhanced reasoning ability}{We observe that in all four TSF scenarios, i.e., unimodal short-term, unimodal long-term, multimodal short-term, and multimodal long-term, at least two reasoning strategies are effective, by outperforming the corresponding System 1 models with a probability of over 50\%; at least one reasoning strategy is significant, by outperforming the corresponding System 1 model with a probability of over 60\%.}
\observationbox{Short-term vs. Long-term Perspective: Long-term TSF benefits more consistently than short-term TSF from enhanced reasoning.} {We observe that long-term TSF, including both unimodal and multimodal cases, consistently benefits from all three System 1-based reasoning enhancement strategies across datasets and methods. Specifically, the CoT, Self-Consistency, and Self-Correction strategies outperform System 1 models with an average probability of 52.08\%, 58.33\%, and 54.17\%, respectively. In contrast, short-term TSF only consistently benefits from the Self-Consistency strategy. This aligns with the nature of TSF: long-term forecasting requires greater consideration of temporal and event influences, whereas short-term forecasting is more closely aligned with the lookback window.  }
\observationbox{Unimodal vs. Multimodal Perspective: Multimodal TSF benefits more significantly than unimodal TSF from enhanced reasoning.}{We observe that multimodal TSF, including both long-term and short-term cases, benefits significantly from reasoning enhancement strategies across datasets and methods. Specifically, the Self-Consistency and Self-Correction strategies outperform System 1 models with an average probability of 66.67\% and 56.25\%, respectively. In contrast, unimodal TSF only significantly benefits from the Self-Consistency strategy. This aligns with the current belief that multimodal TSF, which provides textual context for forecasting, is more reasoning-intensive.}

\section{RQ2: What Kind of Reasoning Strategies Does TSF Need?}
As shown in Figure 2 and Table XXX, we make the following observations regarding this research question:
\observationbox{Overall Answer: self-consistency is the current most
effective for TSF.}{We observe that the self-consistency strategy is consistently and significantly effective, outperforming the corresponding System 1 model with a probability ranging from 60\% to 80\%. We attribute this to self-consistency’s ability to select the most coherent reasoning path from diverse options, which aligns with the fundamental logic of forecasting—considering multiple possible scenarios in the future and adopt the most probable for prediction.}
\rejectionbox{System 1 vs System 2 Perspective: System 1-based reasoning strategies win}{We observe that System 1 with test-time reasoning enhancement achieves an average effectiveness of 66.67\%, significantly higher than the 33.33\% of System 2. This suggests that pure System 2 reasoning is not well-suited for TSF. In contrast, enhancing reasoning within System 1 is more suitable for TSF, as it still largely relies on intuition and rapid responses, such as recognizing periodicity and trends.~\cite{cleveland1990stl,liu2024lstprompt}}
\observationbox{Among System 2 Perspective: DeepSeek-R1 is the only effective reasoning model.}{We observe that DeepSeek-R1 is the only effective model among the three System 2 models, while the other two are o1-mini and Gemini-2.0-Flash-Thinking. Surprisingly, DeepSeek-R1 achieves significant improvements in 3/4 settings, surpassing the System 1 model, i.e., DeepSeek-V3, by an average probability of 60\%. We attribute this to DeepSeek-R1's unique reinforcement learning approach, named Group Relative Policy Optimization(GRPO) ~\cite{guo2025deepseek}, to incentivize reasoning. GRPO, as a pure reinforcement learning method, focuses solely on outcomes rather than labeled reasoning trajectories. Intuitively, GRPO is more suitable for TSF, where relying on accurate reasoning paths to forecast uncertain future numerical series is impractical.}
