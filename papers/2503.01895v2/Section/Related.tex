\vspace{-1mm}
\section{Related Work}

\par\noindent\textbf{(Zero-Shot) Time-Series Forecasting.} Time-series forecasting (TSF) is a fundamental task in time-series analysis with broad real-world applications, including economics, urban computing, and epidemiology~\cite{sezer2020financial, tabassum2021actionable, rodriguez2024machine}. The introduction of transformer-based architectures~\cite{vaswani2017attention} has significantly advanced TSF performance, setting new benchmarks in forecasting accuracy~\cite{zhou2021informer, zhou2022fedformer, Yuqietal-2023-PatchTST, liu2023itransformer}.  Building upon these advancements, recent research has explored foundation TSF models, which achieve competitive zero-shot forecasting performance comparable to supervised TSF models~\cite{das2024decoder, ansari2024chronos, goswami2024moment, shi2024time, ekambaram2025tiny, kamarthi2023large}. Trained on billions of data samples, these foundation models can generate accurate forecasts across diverse time series without requiring additional post-training or fine-tuning.

\par\noindent\textbf{Foundation Model Reasoning.} With the rise of foundation models, reasoning has become a critical research direction, enabling models to move beyond surface-level pattern recognition toward structured decision-making~\cite{wei2022chain, brown2020language}. Reasoning in foundation models primarily follows two paradigms: reasoning-enhanced System 1, which improves inference during test time for fast and intuitive thinking, such as Chain-of-Thought reasoning~\cite{wei2022chain, wang2022self, pan2023automatically, kumar2024training}, and System 2, which enables deep, analytical reasoning as an inherent capability of the model. System 2 reasoning is typically developed through post-training techniques such as reinforcement learning from human feedback (RLHF), reward shaping, knowledge distillation, and the more recent group-relative policy optimization (GRPO) in DeepSeek-R1~\cite{bai2022training, kwon2023reward, gou2021knowledge, xu2024survey, guo2025deepseek}, which allows foundation models to process more structured, deeper, and analytical thinking. Leveraging these advanced reasoning strategies, foundation models have demonstrated strong capabilities in handling complex tasks, including mathematical problem-solving, automated planning, and applications in engineering and science~\cite{bai2023qwen, zhou2024isr, sun2023adaplanner, song2023llm, guo2023can, goh2024large}.  

\par\noindent\textbf{Time-Series Reasoning.} Reasoning over time-series data is an important topic in time-series analysis~\cite{kauppinen2007modeling}. Conventional time-series reasoning primarily focuses on causal analysis, including causal discovery, feature selection, and graph-based analysis~\cite{eichler2012causal, sun2015using, runge2023causal, chen2004analyzing}. However, these conventional methods focus only on numerical modalities, making validation challenging and often limiting their application to synthetic or simplified scenarios. Recent advancements have broadened time-series reasoning beyond numerical data, incorporating multimodal perspectives. Notably, emerging research has explored reasoning over time-series data using LLMs~\cite{liu2024lstprompt, yan2025position} and vision-based approaches~\cite{liu2024picture}, expanding its scope and applicability. While these studies show promising potential for time series reasoning with advanced foundation models, they do not explicitly establish a connection between reasoning strategies and TSF tasks.

Importantly, existing studies and benchmarks focus on either (zero-shot) TSF ~\cite{aksu2024gift, wang2024deep, du2024tsi} or reasoning strategies~\cite{lin2024criticbench, hao2024llm, zhang2025benchmark}, no existing work systematically evaluates how different reasoning strategies impact zero-shot TSF. Our study bridges this gap and introduces a multimodal reasoning enhancement setup beyond conventional unimodal approaches. This multimodal setup incorporates textual context alongside numerical data, providing more comprehensive information for real-world forecasting, and setting a higher standardization for reasoning ability in zero-shot TSF tasks.  






\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{picture/KDD25_system12_1.jpg}  % æˆ– tsf_comparison.pdf
    \vspace{-3mm}
    \caption{The reasoning strategies included in the proposed \method benchmark. \method systematically includes three mainstream approaches: the direct System 1, i.e., directly using generative models such as GPT-4o for reasoning; the test-time-enhanced System 1, including Chain-of-Thought, Self-Consistency, and Self-Correction; the post-training-empowered System 2, which enables built-in reasoning capabilities through reinforcement learning, such as DeepSeek-R1 \cite{guo2025deepseek}.}
    \label{fig:reason_strategy}
\end{figure*}
