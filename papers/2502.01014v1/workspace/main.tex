\begin{abstract}
Recently, zeroth-order (ZO) optimization plays an essential role in scenarios where gradient information is inaccessible or unaffordable, such as black-box systems and resource-constrained environments. While existing adaptive methods such as \base{} have shown promise, they are fundamentally limited by their underutilization of moment information during optimization, usually resulting in underperforming convergence. To overcome these limitations, this paper introduces \textit{Refined Adaptive Zeroth-Order Optimization} (\ours{}). Specifically, we first show the untapped variance reduction effect of first moment estimate on ZO gradient estimation, which improves the accuracy and stability of ZO updates. We then refine the second moment estimate based on these variance-reduced gradient estimates to better capture the geometry of the optimization landscape, enabling a more effective scaling of ZO updates. We present rigorous theoretical analysis to show \textbf{(\rom{1})} \textit{the first analysis} to the variance reduction of first moment estimate in ZO optimization, \textbf{(\rom{2})} \textit{the improved second moment estimates} with a more accurate approximation of its variance-free ideal, \textbf{(\rom{3})} \textit{the first variance-aware convergence framework} for adaptive ZO optimizers, which may be of independent interest, and \textbf{(\rom{4})} \textit{the faster convergence} of \ours{} than existing baselines like \base{}. Our extensive experiments, including synthetic problems, black-box adversarial attack, and memory-efficient fine-tuning of large language models (LLMs), further verify the superior convergence of \ours{}, indicating that \ours{} offers an improved solution for real-world ZO optimization challenges. 
\end{abstract}


\section{Introduction}
Zeroth-order (ZO) optimization has emerged as an indispensable technique at the forefront of machine learning, addressing critical challenges where gradient information is either unavailable or computationally prohibitive. This necessity stems from the prevalence of black-box optimization problems, such as adversarial attacks \citep{RuCBG20, HiranandaniMNFK21}, and resource-constrained environments, like fine-tuning large language models (LLMs) on memory-limited devices \citep{mezo, revisit-mezo}. Consequently, ZO optimization algorithms, which rely solely on function evaluations, have become a crucial alternative to traditional gradient-based methods. Despite the growing body of research in ZO optimization, a significant portion of existing methods adapt stochastic gradient descent (SGD) updates to the ZO setting \citep{0001KCTCA18, 0001LCHA18, zord, fzoos}. This reliance on SGD, however, will lead to performance limitations, especially in complex and non-convex optimization landscapes. The need for more adaptive and versatile update mechanisms is hence evident. However, the exploration of adaptive strategies beyond SGD-based updates remains surprisingly limited. 

While adaptive methods such as \base{} \citep{zo-adamm, nazari2020adaptive} have demonstrated potential in addressing the missing adaptivity in zeroth-order optimization, they are fundamentally limited by their underutilization of moment information, often resulting in suboptimal convergence rates. This limitation in fact arises from their reliance on noisy and high-variance gradient estimates derived solely from function evaluationsâ€”a stark contrast to the first-order (FO) methods that leverage direct and more stable gradients. This issue becomes even more pronounced in high-dimensional and complex settings.

To address this critical limitation, we introduce \textit{Refined Adaptive Zeroth-Order Optimization} (\ours{}), a novel approach that effectively capitalizes on moment information through two key innovations. First, \ours{} is the first to analyze the untapped but inherent variance reduction effect of the first moment estimates on the gradient estimates in ZO optimization, leading to more accurate and stable ZO updates. This is accomplished through the integration of historical gradient estimates, which effectively averages out the estimation noise (Sec.~\ref{sec:1st-mo}). Second, \ours{} refines the second moment using these variance-reduced gradient estimates, enabling better adaptation to the underlying geometry of the optimization landscape and facilitating a more effective scaling of ZO updates (Sec.~\ref{sec:2nd-mo}).

Beyond simply presenting \ours{}, we provide a thorough analysis that combines rigorous theoretical guarantees with extensive empirical validation, demonstrating its effectiveness. Specifically, we first provide the assumptions used in our theoretical analysis (Sec.~\ref{sec:assumps}). We then theoretically analyze that incorporating first-moment estimates into ZO optimization significantly reduces the variance, leading to more stable and reliable ZO updates, and theoretically demonstrate that our refined second moment estimates provide a more accurate approximation of its variance-free ideal (Sec. \ref{sec:bounds}). We further introduce the first variance-aware framework to prove the convergence of adaptive ZO optimization methods, which is not limited to our specific method and can be used to analyze a wider range of similar algorithms, and theoretically prove that \ours{} converges faster than established baseline methods, such as \base{}, demonstrating its efficiency in optimization (Sec. \ref{sec:conv}). Through extensive experiments, including synthetic problems (Sec. \ref{sec:syn}), black-box adversarial attack (Sec. \ref{sec:attack}), and memory-efficient LLM fine-tuning (Sec. \ref{sec:tuning}), we demonstrate that \ours{} consistently outperforms existing methods in practice, exhibiting superior convergence.


To summarize, our contributions in this work include:
\begin{itemize}[topsep=0pt,leftmargin=3mm,itemsep=-2pt]
\item We propose \ours{} to enhance the utilization of moment information in ZO optimization and significantly improve the convergence of adaptive ZO optimizers.
\item We theoretically show \textbf{(\rom{1})} \textit{the first analysis} to the variance reduction of first moment estimates in ZO optimization, \textbf{(\rom{2})} \textit{the effects of our refined second moment estimates}, \textbf{(\rom{3})} \textit{the first variance-aware convergence framework} for adaptive ZO methods, which may be of independent interest, and \textbf{(\rom{4})} \textit{the improved convergence} of \ours{}.
\item We use extensive empirical validation to show the consistent performance gains of \ours{} over baselines.
\end{itemize} 

\section{Related Work}
% \textbf{ZO Optimization.} Zero-order (ZO) optimization methods include direct search, Bayesian optimization (BO), and gradient descent (GD) with estimated derivatives. Direct search, exemplified by \citep{pursuit, gld}, relies on comparing function values, which is query-inefficient. BO uses a Gaussian process (GP) model to balance exploitation and exploration \citep{ChowdhuryG17, SrinivasKKS10}, but its efficiency and computational cost degrade in high dimensions \citep{RasmussenW06}. Given the limitations of both direct search and BO, especially in high dimensions, GD-based algorithms with estimated derivatives offer a more practical alternative. GD-based algorithms with estimated derivatives are often preferred for high-dimensional problems. These algorithms estimate derivatives using finite difference (FD) approximations of directional derivatives, typically along sampled directions like random unit vectors \cite{bsg} and Gaussian vectors \citep{Nesterov2017}. 
Recent ZO optimization research focuses on two key areas: ZO gradient estimation and ZO update rules. 

\textbf{ZO Gradient Estimation.} Since ZO optimization only relies on function values, gradient estimation is essential for effective optimization. A common approach is to use finite difference approximations under input perturbations. \citet{Nesterov2017} propose to use Gaussian random noise perturbations, demonstrating theoretical convergence with smooth perturbations. Other methods also propose to use uniform sampling from the unit sphere \citep{bsg} or coordinate-wise perturbations \citep{coordinate}. These methods often have a noisy gradient estimation. To address this, \citep{prgf} introduces prior-guided gradient estimation, which leverages previous estimates to improve the current one, effectively smoothing the estimation noise. Recently, \citep{zord, fzoos} propose using kernel methods to learn a surrogate model of the objective function from historical function values, allowing for more accurate gradient estimation. Note that this paper does not aim to introduce a new gradient estimation approach, but focus on developing advanced update rules that are applicable to all these existing estimation methods.

\textbf{ZO Update Rules.} Building upon the estimated gradients from various ZO estimation methods, ZO optimizers often directly adopt update rules from first-order (FO) optimization. E.g., a large portion of existing ZO optimizers use stochastic gradient descent (SGD) and its variants as their update mechanism \citep{GhadimiL13a, GhadimiLZ16, Nesterov2017, 0001LCHA18, 0001KCTCA18, prgf, zord}. While simple to apply, the slow convergence of SGD has motivated few efforts \cite{zo-adamm, nazari2020adaptive, adamu} to explore the use of adaptive methods, such as Adam \citep{adam}, as the ZO update rule. However, these attempts often under-utilize the moment information inherent in adaptive methods when applied to ZO optimization, leading to suboptimal convergence. This paper addresses this critical issue by proposing refined update rules that are specifically designed to better leverage moment information, ultimately leading to more efficient ZO optimization.

% \textbf{ZO gradient estimation.} The field of adaptive gradient methods has seen rapid development, with algorithms such as AdaGrad \citep{adagrad}, RMSprop \citep{rmsprop}, and Adam \citep{adam} becoming widely used in optimizing complex machine learning models. These methods typically use per-parameter learning rates based on past gradient information, enabling efficient convergence in many scenarios. However, the application of these methods in zero-order optimization presents unique challenges, as gradient information must be estimated. Researchers have begun exploring the integration of adaptive gradient methods with finite-difference gradient approximations, seeking to replicate the benefits of adaptive learning rates in the absence of direct gradient access. This paper contributes to this emerging area by [mention your specific contribution/focus, e.g., a new adaptive method for ZO, analysis of existing methods, etc. ]. 



\section{Background}
This paper tackles a stochastic zeroth-order (ZO) optimization problem, aiming to minimize the expected value of a function, defined as:

\begin{equation}
\min_{\vtheta \in \sR^d} F(\vtheta) \triangleq \mathbb{E}_{\xi}\left[f(\vtheta; \xi)\right]
\end{equation}
where $\vtheta \in \mathbb{R}^d$ and $f(\vtheta; \xi)$ is a scalar-valued function whose evaluation depends on the parameters $\vtheta$ and a random variable $\xi$ sampled from an underlying distribution. Crucially, we have access only to function evaluations $f(\vtheta; \xi)$ and not its gradient $\nabla_{\vtheta} f(\vtheta; \xi)$. 
% This setting, where gradient information is unavailable, is called ZO optimization, and it is particularly relevant when gradient computation is infeasible. This infeasibility arises especially when the system includes black-box components that impede gradient calculation \citep{RuCBG20}; or memory constraints prevent storing intermediate results for backpropagation \citep{mezo}. 
Throughout this paper, we adopt the following notational conventions. Vectors are represented in boldface, e.g., $\vtheta$, and scalar constants are denoted by uppercase letters, e.g., $L$. All vector operations are assumed to be element-wise unless explicitly stated otherwise. We denote by $\nabla_i F$ the partial derivative of function $F$ with respect to the $i$-th coordinate.

\paragraph{ZO Gradient Estimation.} In ZO optimization, the absence of direct access to gradients, denoted as $\nabla_{\vtheta} f(\vtheta; \xi)$, necessitates the use of gradient estimation techniques that rely solely on function evaluations. A widely used method is to approximate gradients using finite differences. E.g., let random vectors $\{\vu_k\}_{k=1}^K$ be drawn uniformly from the sphere of a unit ball $\sS$, a common ZO gradient estimator, which is used throughout this paper, can be formed as:
\begin{equation}\label{eq:fd}
\hat{\nabla} f(\vtheta, \xi) \triangleq \frac{d}{K}\sum_{k=1}^K\frac{f(\vtheta + \mu \vu_k; \xi) - f(\vtheta; \xi)}{\mu} \vu_k
\end{equation}
where $\mu > 0$ is a smoothing parameter, and $K$ is the number of random vectors. While this paper utilizes this specific ZO gradient estimator as its foundation, the proposed method is extensible to other ZO gradient estimators as well.

\paragraph{Adaptive ZO Optimization.}

ZO optimization methods with a fixed step size typically suffer from slow convergence. To address this, adaptive methods like \base{} \citep{zo-adamm} are used, which incorporate momentum using first moment estimates and per-parameter learning rates using second moment estimates. Specifically, in \base{}, the parameter updates are computed as follows for every iteration $t$ (see also Algo.~\ref{alg:adam}):
\begin{equation}
\begin{aligned}
     &\vm_t \gets \beta_1 \vm_{t-1} + (1 - \beta_1) \vg_t & \text{(First Moment Est.)} \\
    &\vv_t \gets \beta_2 \vv_{t-1} + (1- \beta_2) \vg_t^{\smash{2}} & \text{(Second Moment Est.)}  \\[-3pt]
    &\vtheta_t \gets \vtheta_{t-1} - \eta\frac{\vm_t}{\sqrt{\vv_t + \zeta}} & \text{(ZO Update)}
\end{aligned}
\end{equation}
where $\vg_t = \hat{\nabla} f(\vtheta_{t-1})$ defined in \eqref{eq:fd},  $\beta_1,\beta_2\in(0,1)$ are exponential decay rates for moment estimates, and $\zeta$ is a small constant to prevent dividing by zero.


However, while these adaptive ZO approaches have shown promise, they often underutilize the moment information in the context of ZO optimization: \textbf{\textit{(a)}} They typically treat first moment estimate $\vm_t$ as standard velocity accumulation in FO optimization, failing to consider its underlying variance reduction effect in ZO optimization by accumulating information from previous gradient estimates. \textbf{\textit{(b)}} They fail to apply this variance-reduced gradient estimates to refine the second moment estimate $\vv_t$, causing a less effective scaling of ZO updates.


\begin{figure}[t]
\vspace{-2mm}
\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{\base{}}\label{alg:adam}
\KwIn{$\beta_1, \beta_2, \zeta, \eta, f$}
\textbf{Initialize:} $\vtheta_0, \vm_0, \vv_0$

\For{iteration $t \in [T]$}{

$\vg_t \gets \hat{\nabla} f(\vtheta_{t-1}, \xi_t)$

$\vm_t \gets \beta_1 \vm_{t-1} + (1 - \beta_1) \vg_t$

% $\vm_t \gets \vm_t / (1-\beta_1^t)$

$\vv_t \gets \beta_2 \vv_{t-1} + (1- \beta_2) \textcolor{blue}{\vg_t^{2}}$

% $\vv_t \gets \vv_t / (1-\beta_2^t)$

$\vtheta_t \gets \vtheta_{t-1} - \eta\frac{\vm_t}{\sqrt{\vv_t + \zeta}}$
}
\KwOut{$\vtheta_T$}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{\ours{}}\label{alg:ours}
\KwIn{$\beta_1, \beta_2, \zeta, \eta, f$}
\textbf{Initialize:} $\vtheta_0, \vm_0, \vv_0$

\For{iteration $t \in [T]$}{

$\vg_t \gets \hat{\nabla} f(\vtheta_{t-1}, \xi_t)$

$\vm_t \gets \beta_1 \vm_{t-1} + (1 - \beta_1) \vg_t$

% $\vm_t \gets \vm_t / (1-\beta_1^t)$

$\vv_t \gets \beta_2 \vv_{t-1} + (1- \beta_2) \textcolor{red}{\vm^{2}_t}$

% $\vv_t \gets \vv_t / (1-\beta_2^t)$

$\vtheta_t \gets \vtheta_{t-1} - \eta\frac{\vm_t}{\sqrt{\vv_t + \zeta}}$
}
\KwOut{$\vtheta_T$}
\end{algorithm}
\end{minipage}
\end{figure}


\section{Refined Adaptive ZO Optimization}
To address the underutilization of momentum information in existing adaptive ZO optimization methods, we introduce \ours{} (\textit{Refined Adaptive Zeroth-Order Optimization}). Specifically, we first analyze the untapped variance reduction effect of first moment estimates on ZO gradient estimation, which is important for accurate and stable ZO updates (Sec.~\ref{sec:1st-mo}). We then leverage these variance-reduced estimates to construct a refined second moment, enabling more effective scaling of ZO updates (Sec.~\ref{sec:2nd-mo}).

\subsection{Variance Reduction in First Moment Estimates}\label{sec:1st-mo}
First moment estimation, while conventionally used for convergence speedup, inherently serve as a variance reduction mechanism for noisy gradients. To show this, consider the following standard first moment estimate with $\beta_1 \in (0,1)$:
\begin{equation}\label{eq:1st-mo}
\vm_t \gets \beta_1\vm_{t-1} + (1-\beta_1)\vg_{t}
\end{equation}
where $\vm_t$ is the estimated first moment at iteration $t$ and $\vg_t$ is the gradient estimate at $\vtheta_{t-1}$ via \eqref{eq:fd}.  Intuitively, this update averages the current noisy gradient estimate with past, correlated estimates.
% a correlation that stems from the local nature of gradient-based methods. 
This averaging process effectively smooths out noise in gradient estimates, thereby reducing variance. For example, averaging two independent noisy gradient estimates (ie, $\vm_{t-1}$ and $\vg_t$) of variance $\sigma^{\smash{2}}$ results in a variance of $[\beta_1^{\smash{2}} + (1-\beta_1)^{\smash{2}}]\sigma^{\smash{2}}$, which is less than $\sigma^{\smash{2}}$. While current and past gradient estimates are not fully independent in practice, their local correlation still enables variance reduction through this averaging, which we will show theoretically in Sec.~\ref{sec:theory}.

While this variance reduction effect has been proven in FO optimization \citep{sgdm}, it is significantly more crucial in ZO optimization. Unlike FO methods that compute gradients directly with relatively low variance, ZO optimization approximates gradients using function evaluations (as in \eqref{eq:fd}), resulting in inherently noisier estimates. This disparity underscores the critical importance of the variance reduction effect of first moment estimates in ZO optimization, a connection we are the first to identify. We further provide theoretical support for this in Sec.~\ref{sec:theory}.

% This enhanced role of momentum as a variance reducer is therefore paramount to the stability and convergence of ZO algorithms. Our subsequent development of \ours{} directly capitalizes on this insight.


\subsection{Refinement to Second Moment Estimates}\label{sec:2nd-mo}

The second key innovation of \ours{} lies in its refined second moment estimate, which is crucial for the adaptivity in ZO optimization. Existing adaptive ZO methods \citep{zo-adamm, nazari2020adaptive} update the second moment estimate directly using the squared noisy gradient estimates:
\begin{equation}\label{eq:xcxv}
    \vv_t = \beta_2\vv_{t-1} + (1 - \beta_2)\textcolor{blue}{\vg_{t}^2} \ .
\end{equation}
However, this approach can be suboptimal in the ZO setting owing to the inherent high variance of the gradient estimates in \eqref{eq:fd}, which could lead to unstable and unreliable second moment estimates. We thus propose to address this issue by simply leveraging the variance-reduced gradient information from the first moment. That is, we update the second moment estimate as below, which interestingly shares similar form with RMSProp \citep{rmsprop}.
\begin{equation}\label{eq:2nd-mo}
    \vv_t = \beta_2\vv_{t-1} + (1 - \beta_2)\textcolor{red}{\vm_t^2} \ .
\end{equation}

The first moment estimate, as revealed in Sec.~\ref{sec:1st-mo}, acts as a variance reduction mechanism by averaging historical gradient information. Using the squared first moment estimate then probably provides a smoothed and more stable second moment estimate. This refinement therefore may enable a more accurate representation for the underlying geometry of the optimization landscape, resulting in more effective scaling of ZO updates and thus accelerated convergence. Specifically, consider a scenario where $\E[\vm_t] = \E[\vg_t]$ but $\vm_t$ has significantly lower variance than $\vg_t$ due to the smoothing effect, given the same $\vv_{t-1}$, we can see that the refined $\vv_t$ in \eqref{eq:2nd-mo} achieves a smaller expected value compared to the standard one in \eqref{eq:xcxv}. Hence, the update step (see \eqref{eq:update}) using this refined $\vv_t$ in \eqref{eq:2nd-mo} is likely to be larger, allowing the algorithm to move faster towards the optimum. This claim will be rigorously established in Sec.~\ref{sec:theory}.

\subsection{Final Algorithm}

Given the first and second moment estimates in \eqref{eq:1st-mo} and \eqref{eq:2nd-mo} respectively, \ours{} updates parameters by:
\begin{equation}\label{eq:update}
    \boldsymbol{\theta}_t = \boldsymbol{\theta}_{t-1} - \eta \frac{\vm_t}{\sqrt{\vv_t + \zeta}}
\end{equation}
where $\eta$ is the base learning rate and $\zeta$ is a small constant for numerical stability. This update rule adaptively scales the effective learning rate based on the local geometry while incorporating the variance-reduced gradient estimates. The complete \ours{} algorithm is detailed in our Algo.~\ref{alg:ours}.

\textbf{Computational and Memory Complexity.} \ours{} incurs the same computational cost of $\gO(Kd)$ per iteration for moment estimates and ZO updates (excluding function evaluations), and the same memory footprint of $\gO(d)$ as \base{} for moment estimates, where $K$ is the number of function evaluations and $d$ is the dimension of parameter $\vtheta$. 
% Note that the memory footprint is incurred by the gradient estimate $\vg_t$, the first moment $\vm_t$, and the second moment $\vv_t$, each with dimension $d$.

\textbf{Ease of Implementation.} A key advantage of \ours{} is its simple implementation. The core change involves updating the second moment estimate using the squared first moment estimate, a one-line change for existing adaptive ZO optimizers. This minimal change enables easy integration and fast deployment, while improving convergence.

\section{Theoretical Analysis}\label{sec:theory}

This section presents a theoretical foundation for the efficacy of \ours{}. We structure our analysis as follows: First, we introduce the required assumptions and preliminaries (Sec.\ref{sec:assumps}). Second, we prove the variance reduction in first moment estimate and the improvement of our refined second moment in \ours{} (Sec. \ref{sec:bounds}). Finally, we present the first variance-aware convergence framework for adaptive ZO methods and demonstrate the improved convergence of \ours{} over other baselines (Sec.~\ref{sec:conv}). 

\subsection{Assumptions and Preliminaries}\label{sec:assumps}
Our theoretical framework is built upon two fundamental assumptions concerning the non-convex function $F$. We impose a bounded function value as well as a coordinate-wise Lipschitz smoothness (Assump. \ref{assump:1}), with a bounded variance of function values (Assump. \ref{assump:2}). Of note, coordinate-wise Lipschitz smoothness is commonly used in the analysis of FO adaptive gradient methods, e.g., \citep{zhang2024convergence, wang2024closing}. 
% Note that, our bounded noisy gradient variance condition (Assump.~\ref{assump:2}) is more general than the bounded noisy gradient norm condition utilized in \citep{zo-adamm}, as our noisy gradient can be unbounded.

\begin{assumption}\label{assump:1}
$\forall \vtheta,\vtheta' \in \mathbb{R}^d$ and $\forall i \in [d]$,
\begin{align}
\big|f(\vtheta, \xi)\big| &\leq C \ , \\[1pt]
% \big|\nabla_i F(\vtheta)\big| &\leq G \ , \label{eq:continuity} \\
\left|\nabla_i F(\vtheta) - \nabla_i F(\vtheta')\right| &\leq L\left\|\vtheta - \vtheta'\right\| \ . \label{eq:smoothness}
\end{align}
\end{assumption} 
\begin{assumption}\label{assump:2}
$\forall{\vtheta} \in \mathbb{R}^d$,
\begin{align}
% \E_{\xi}\Big[\nabla_i f(\vtheta, \xi)\Big] &= \nabla_i F(\vtheta) \ , \label{eq:mean}\\
\E_{\xi}\left[\big|f(\vtheta, \xi) - F(\vtheta)\big|^2\right] &\leq \sigma^2 \ . \label{eq:variance}
\end{align}
\end{assumption}

% In fact, it is too complex to prove the convergence of \ours{} directly based on the objective function $F$ due to the bias introduced by gradient estimation in \eqref{eq:fd}.
Directly establishing the convergence of \ours{} through the function $F$ presents a primary challenge for adaptive ZO methods, due to the bias (i.e., $\E\left[\hat{\nabla}f(\vtheta,\xi)\right] \neq \nabla F(\vtheta)$) arising from the gradient estimation in \eqref{eq:fd}. Thus, we innovatively propose to prove the convergence of \ours{} with respect to the randomized smoothing function $F_{\mu}$ defined in \eqref{eq:fu} where $\vu$ is a random vector drawn uniformly from the sphere of a unit ball $\sS$ and $\mu > 0$ is a smoothing parameter.
\begin{equation}\label{eq:fu}
F_{\mu}(\vtheta) \triangleq \E_{\vu \sim \sS}\left[F(\vtheta + \mu \vu)\right] \ .
\end{equation}

We introduce the following Lemma \ref{le:connection} (proof in Appx.~\ref{proof:connection}) to justify why $F_{\mu}$, instead of $F$, servers as a better choice for the convergence framework of adaptive ZO methods.
\begin{lemma}\label{le:connection}
Given gradient estimator \eqref{eq:fd}, with Assump. \ref{assump:1}, $\forall \vtheta \in \mathbb{R}^d$ and $\forall i \in [d]$,
\begin{align}
\mathbb{E}\left[\hat{\nabla} f(\vtheta, \xi)\right] &= \nabla F_{\mu}(\vtheta) \ , \label{eq:skvj} \\
\mathbb{E}\left[\left\|\nabla F(\vtheta) - \nabla F_{\mu}(\vtheta)\right\|\right] &\leq \mu L\sqrt{d} \ . \label{eq:vine}
\end{align}
\end{lemma}
\textbf{Remark.} Lemma \ref{le:connection} establishes that \textbf{\textit{(a)}} $\nabla F_{\mu}$ is the expectation of the gradient estimate in \eqref{eq:fd}, thereby overcoming the bias challenge mentioned above, and \textbf{\textit{(b)}} the discrepancy between $\nabla F_{\mu}$ and $\nabla F$ is bounded above by $\mathcal{O}(\mu)$, implying that the convergence of \ours{} with respect to $\nabla F$ can be easily derived after obtaining the results with respect to $\nabla F_{\mu}$. In light of these, $F_{\mu}$ will be a good choice for the convergence framework of adaptive ZO methods.

In addition, we provide the following Lemma \ref{le:lips} (proof in Appx.~\ref{proof:lips}) to ease our proof.
\begin{lemma}\label{le:lips}
Given gradient estimator \eqref{eq:fd}, with Assump. \ref{assump:1}, \ref{assump:2}, $\forall \vtheta,\vtheta' \in \mathbb{R}^d$ and $\forall i \in [d]$,
\begin{align}
% \left|\nabla_i F_{\mu}(\vtheta)\right| &\leq G \ , \label{eq:continuity-fu} \\[4pt]
\left|\nabla_i F_{\mu}(\vtheta) - \nabla_i F_{\mu}(\vtheta')\right| &\leq L\left\|\vtheta - \vtheta'\right\| \ , \label{eq:smoothness-fu} \\
\E\left[\left|\hat{\nabla}_i f(\vtheta, \xi) - \nabla_i F_{\mu}(\vtheta)\right|^2\right] &\leq \frac{8(\sigma^2 + C^2)d}{K\mu^2} \ . \label{eq:var-fu}
\end{align}
\end{lemma}
\textbf{Remark.} Lemma \ref{le:lips} establishes that \textbf{\textit{(a)}} $F_{\mu}$ exhibits the same Lipschitz smoothness as $F$, and \textbf{\textit{(b)}} the gradient variance associated with ZO optimization can be substantially large, particularly when $K \ll d$ and $\mu$ is small. Therefore, variance reduction is critical for improved ZO optimization.

\subsection{Analysis on First and Second Moment Estimates}\label{sec:bounds}
We first theoretically show the underlying variance reduction effect of first moment estimate in \eqref{eq:1st-mo} using variance $\Sigma^2$ defined below in Thm.~\ref{thm:vr} (proof in Appx.~\ref{proof:vr}).
\begin{equation}\label{eq:sigma2}
\Sigma^2 \triangleq \frac{8(\sigma^2 + C^2)d}{K\mu^2} \ .
\end{equation}
\begin{theorem}\label{thm:vr}
Given first and second moment estimates \eqref{eq:1st-mo} and \eqref{eq:2nd-mo} respectively, with Assump.~\ref{assump:1}, \ref{assump:2}, $\forall{t}\geq1$ and $\forall{i}\in[d]$,
\begin{equation}
\begin{aligned}
&\E\left[\left|\vm_{t,i} - \nabla_i F_{\mu}(\vtheta_{t-1})\right|^2\right] \leq \\
&\underbrace{\frac{1-\beta_1}{1 + \beta_1} \Sigma^2}_{\text{\normalfont Variance}} + \underbrace{\frac{\beta_1(1+\beta_1)L^2\eta^2 d}{(1 - \beta_1)^2(1 - \beta_2)} + \beta_1^t \E\left[\left|\nabla_i F_{\mu}(\vtheta_{t-1})\right|^2\right]}_{\text{\normalfont Bias}} \ . \label{eq:jeke}
\end{aligned}
\end{equation}
\end{theorem}
\textbf{Remark.} To the best of our knowledge, this theorem provides the first fundamental variance-bias decomposition for the first moment estimate in adaptive ZO algorithms. The variance, given by $\frac{1-\beta_1}{1+\beta_1}\Sigma^2$, arises from the randomness in gradient estimator \eqref{eq:fd} and reduces $\Sigma^2$ in \eqref{eq:var-fu} by a factor of $\frac{1-\beta_1}{1+\beta_1}$, which can be further improved with a large $\beta_1$. This thus theoretically demonstrates the variance reduction effect of first moment estimate in \eqref{eq:1st-mo}, which goes beyond increasing $K$ to reduce variance. The bias, stemming from the difference between current and past estimates, can be reduced by using a small learning rate $\eta$, which limits the magnitude of update steps, or a small $\beta_1$, which reduces the influence of past estimates. So, this decomposition unveils a fundamental trade-off controlled by the utilization  (i.e., $\beta_1$) of past estimates between variance and bias. Particularly, when $\beta_1=0$, \eqref{eq:jeke} simplifies to \eqref{eq:var-fu}.
% Overall, this decomposition unveils a fundamental trade-off between variance and bias, guiding the selection of $\beta_1$ to enhance gradient estimation in adaptive ZO algorithms. For example, when $\Sigma^2$ is large and dominates the gradient estimation error, a larger $\beta_1$ can effectively reduce the variance term. On the contrary, when $\Sigma^2$ is small, a smaller $\beta_1$ should be used to reduce bias from past gradient discrepancies.

We then theoretically show that our refined second moment update in \eqref{eq:2nd-mo} is likely to be a more accurate approximation to its variance-free ideal in \eqref{eq:vsdb} and hence may better capture the underlying geometry of optimization landscape than \eqref{eq:xcxv} used in \base{}, with the following Thm.~\ref{thm:v} (proof in Appx.~\ref{proof:v}) and Cor.~\ref{cor:v-hat} (proof in Appx.~\ref{proof:v-hat}).
\begin{equation}\label{eq:vsdb}
\vv_{t,i} = \beta_2^t\,\vv_{0,i} + \sum_{\tau=1}^{t}(1-\beta_2)\beta_2^{t-\tau}\left|\nabla_i F_{\mu}(\vtheta_{\tau-1})\right|^2 \ .
\end{equation}
\begin{theorem}\label{thm:v}
Given second moment estimate \eqref{eq:2nd-mo}, with Assump.~\ref{assump:1}, \ref{assump:2}, $\forall{t}\geq1$ and $\forall{i}\in[d]$, 
\begin{equation}\label{eq:sv}
\begin{aligned}
&\E\left[ \vv_{t,i}\right] \leq \beta_2^t\,\vv_{0,i} + \coloredbox{green}{(1-\beta_1)}\Sigma^2 + \coloredbox{orange}{\frac{\beta_1(1+\beta_1)^2L^2\eta^2 d}{(1 - \beta_1)^2(1 - \beta_2)}} + \\
&\qquad\ \ \, \frac{(1+\beta_1)^2}{\beta_1}\sum_{\tau=1}^{t}(1-\beta_2)\beta_2^{t-\tau}\E\left[\left|\nabla_i F_{\mu}(\vtheta_{\tau-1})\right|^2\right] \ .
\end{aligned}
\end{equation}
\end{theorem}
\begin{corollary}\label{cor:v-hat}
Given second moment estimate in \eqref{eq:xcxv}, with Assump.~\ref{assump:1}, \ref{assump:2}, $\forall{t}\geq1$ and $\forall{i}\in[d]$,
\begin{equation}\label{eq:csvg}
\begin{aligned}
&\E\left[ \vv_{t,i}\right] \leq \beta_2^t\,\vv_{0,i} + \coloredbox{green}{(1+\beta_1)}\Sigma^2 \  +  \\
&\qquad \frac{(1+\beta_1)^2}{\beta_1}\sum_{\tau=1}^{t}(1-\beta_2)\beta_2^{t-\tau}\E\left[\left|\nabla_i F_{\mu}(\vtheta_{\tau-1})\right|^2\right] \ .
\end{aligned}
\end{equation}
\end{corollary}
\textbf{Remark.} Thm. \ref{thm:v} introduces a novel variance-dependent upper bound for our refined second moment estimate \eqref{eq:2nd-mo}. Compared with the bound \eqref{eq:csvg} in Cor. \ref{cor:v-hat} for the conventional estimate \eqref{eq:xcxv}, our \eqref{eq:2nd-mo} reduces the influence of gradient estimation variance $\Sigma^2$ (in $\coloredbox{green}{\text{green}}$) by a factor of $\frac{1-\beta_1}{1+\beta_1}$. This is crucial in ZO optimization where $\Sigma^2$ is typically large. While our estimate introduces a bias (in $\coloredbox{orange}{\text{orange}}$), it is small with a small learning rate $\eta$. Note that \eqref{eq:vsdb} represents the variance-free ideal, which the conventional estimate \eqref{eq:xcxv} and our refined estimate \eqref{eq:2nd-mo} aims to approximate. Comparing the bounds in \eqref{eq:sv} and \eqref{eq:csvg} with \eqref{eq:vsdb}, our refined estimate \eqref{eq:2nd-mo} better approaches this ideal than \eqref{eq:xcxv}, particularly when $\Sigma^2$ dominates, thanks to its reduced impact of $\Sigma^2$. This thus enables a better capture of geometry information during optimization and probably leads to improved optimization.


\subsection{Variance-Aware Convergence Analysis}\label{sec:conv}
This section presents the first variance-aware convergence framework for adaptive ZO methods, particularly focusing on the convergence of \ours{} and \base{}. We first bound the averaged gradient norm of the smoothed function, $F_{\mu}$, as a step towards bounding the averaged gradient norm of the original function $F$. Inspired by \citep{zhang2024convergence}, the core proof idea lies in applying H\"{o}lder's inequality to decomposes this target into two components (Lemma \ref{le:holder}): One involving the averaged square root norm of second moment estimate that will be variance-dependent and another involving a normalized gradient norm by second moment estimate. The subsequent analysis then focuses on bounding these two components using Lemma \ref{le:v-norm} and Thm. \ref{thm:grad/v}, respectively. 
% The first component, i.e., the averaged square root norm of second moment, can be upper-bounded with Thm.~\ref{thm:v} (see Lemma \ref{le:v-norm}). The second component, i.e., the normalized gradient norm with second moment, is shown to be of order $\gO(V^2\epsilon^2)$ (see Thm. \ref{thm:grad/v}). 
By combining these bounds and incorporating the connection between $\nabla F$ and $\nabla F_{\mu}$ in Lemma~\ref{le:connection}, we arrive at the final convergence results for \ours{} (Thm. \ref{thm:r-adazo}) and  \base{} (Cor. \ref{cor:zo-adamm}).

We first introduce Lemma \ref{le:holder} (proof in Appx.~\ref{proof:holder}).
\begin{lemma}\label{le:holder}
$\forall{t}\geq1$, we have that
\begin{equation}
\begin{aligned}
&\left(\frac{1}{T}\sum_{t=0}^{T-1}\E\left[\left\|\nabla
F_{\mu}(\vtheta_t)\right\|\right]\right)^2 \leq \\
& \underbrace{\frac{1}{T}\sum_{t=0}^{T-1} \E\left[\sqrt{\beta_2\left\|\vv_{t}\right\|+\zeta}\right]}_{\circled{A}}\underbrace{\frac{1}{T}\sum_{t=0}^{T-1}\E\left[\frac{\left\|\nabla
F_{\mu}(\vtheta_{t})\right\|^2}{\sqrt{\beta_2\left\|\vv_{t}\right\|+\zeta}}\right]}_{\circled{B}} \ .
\end{aligned}
\end{equation}
\end{lemma}
\textbf{Remark.} \citet{zo-adamm, nazari2020adaptive} bound $\circled{B}$ solely to demonstrate the convergence of adaptive ZO methods. However, we argue that this bound alone fail to include the effects of second moment estimate and therefore provides incomplete convergence information. In contrast, Lemma~\ref{le:holder} allows us to include the effects of second moment (i.e., $\circled{A}$) and directly bound a more relevant quantity, $\frac{1}{T}\sum_{t=0}^{T-1}\E\left[\left\|\nabla F_{\mu}(\vtheta_t)\right\|\right]$. Note that this metric is a more widely accepted convergence criteria in optimization theory, directly measuring the distance to a stationary point \citep{lower-bound, zhang2024convergence}. 
Overall, Lemma \ref{le:holder} enables us to provide a variance-aware convergence analysis, strengthening the understanding of convergence behavior for adaptive ZO methods.

Leveraging Lemma \ref{le:holder}, we then proceed to bound the terms $\circled{A}$ and $\circled{B}$ in Lemma \ref{le:v-norm} (proof in Appx.~\ref{proof:v-norm}) and Lemma \ref{thm:grad/v} (proof in Appx.~\ref{proof:grad/v}), respectively. These results rely on the following definition of $V$ resulted from Thm. \ref{thm:v}.
\begin{equation}\label{eq:V2}
\resizebox{0.5\textwidth}{!}{
$V^2 \triangleq \left\|\vv_0\right\| + \underbrace{(1-\beta_1)\frac{8(\sigma^2 + C^2)d}{K\mu^2}}_{\text{Variance}} + \underbrace{\frac{\beta_1(1+\beta_1)^2L^2\eta^2d}{(1 - \beta_1)^2(1 - \beta_2)}}_{\text{Bias}}$ \ .
}
\end{equation}

\begin{lemma}\label{le:v-norm}
Given first and second moment estimates \eqref{eq:1st-mo} and \eqref{eq:2nd-mo} respectively, with Assump.~\ref{assump:1},~\ref{assump:2}, $\forall{t} \geq 1$ and $\forall{i} \in [d]$,
\begin{equation}
\begin{aligned}
&\frac{1}{T}\sum_{t=0}^{T-1} \E\left[\sqrt{\beta_2\left\|\vv_{t}\right\|+\zeta}\right] \leq \\
&\qquad \sqrt{\zeta} + Vd + \frac{(1+\beta_1)\sqrt{d}}{\sqrt{\beta_1(1-\beta_2)}} \frac{1}{T} \sum_{t=0}^{T-1} \E\left[\big\|\nabla F_{\mu}(\vtheta_t)\right\|\big] \ .
\end{aligned}
\end{equation}
\end{lemma}
\textbf{Remark.} Lemma \ref{le:v-norm} demonstrates that $\circled{A}$ in Lemma \ref{le:holder} is variance-dependent. Specifically, $\circled{A}$ is asymptotically dominated by $V$ as $T \to \infty$, because $\frac{1}{T} \sum_{t=0}^{T-1} \E\left[\big\|\nabla F_{\mu}(\vtheta_t)\right\|\big]$ gradually decreases during optimization. This highlights that the asymptotic behavior of $\circled{A}$ is governed by both the bias and variance present in the first moment estimate \eqref{eq:1st-mo}.

\begin{theorem}[Informal]\label{thm:grad/v}
With Assump.~\ref{assump:1},~\ref{assump:2}, let $1-\beta_2 \sim \gO(\eps^2)$, $\eta \sim \gO(\eps^2)$, and $T \sim \gO(\eps^{-4})$. the following holds for \ours{} if $\beta_1 \leq \sqrt{\beta_2}, \beta_2 \geq \frac{1}{2}, \vm_{0,i}=0,\vv_{0,i}>0$,
\begin{equation}
\frac{1}{T}\sum_{t=0}^{T-1}\E\left[\frac{\left\|\nabla
F_{\mu}(\vtheta_{t-1})\right\|^2}{\sqrt{\beta_2\left\|\vv_{t}\right\|+\zeta}}\right] \leq \eps^2 \ .
\end{equation}
\end{theorem}
\textbf{Remark.} Of note, Thm. \ref{thm:grad/v} attains the same rate of $\gO(\frac{1}{\sqrt{T}})$ as \citep{zo-adamm, nazari2020adaptive} to achieve that $\frac{1}{T}\sum_{t=0}^{T-1}\E\left[\frac{\left\|\nabla
F_{\mu}(\vtheta_{t-1})\right\|^2}{\sqrt{\beta_2\left\|\vv_{t}\right\|+\zeta}}\right] \leq \eps$.
% , whereas our Thm. \ref{thm:grad/v} is more general. Because it explicitly accounts for $V$, a term derived from the bias-variance decomposition in Thm.~\ref{thm:vr}, offering a more nuanced analysis of the error.

By incorporating Lemma \ref{le:connection}, \ref{le:v-norm}, and Thm. \ref{thm:grad/v} into Lemma \ref{le:holder}, we derive the following convergence for \ours{} (Thm. \ref{thm:r-adazo}) and \base{} (Cor. \ref{cor:zo-adamm}), respectively.
\begin{theorem}[Informal]\label{thm:r-adazo}
Given Assump.~\ref{assump:1},~\ref{assump:2}, let $ 1-\beta_2 \sim \gO(\eps^2)$, $\eta \sim \gO(\eps^2)$, and $T \sim \gO(\eps^{-4})$. We have the following for \ours{} if $\beta_1 \leq \sqrt{\beta_2}, \beta_2 \geq \frac{1}{2}, \vm_{0,i}=0,\vv_{0,i}>0$,
\begin{equation}
\begin{aligned}
\frac{1}{T}\sum_{t=0}^{T-1}\E\big[\left\|\nabla
F(\vtheta_t)\right\|\big] & {\leq} \frac{(1+\beta_1)\sqrt{d}}{\sqrt{\beta_1(1-\beta_2)}} \eps^2 {+} \left(\sqrt[4]{\zeta} {+} \sqrt{Vd}\right) \eps \\
&\qquad\quad + \mu L\sqrt{d} \ .
\end{aligned}
\end{equation}
\end{theorem}
\begin{corollary}[Informal]\label{cor:zo-adamm}
Given Assump.~\ref{assump:1},~\ref{assump:2}, let $1-\beta_2 \sim \gO(\eps^2)$, $\eta \sim \gO(\eps^2)$, and $T \sim \gO(\eps^{-4})$. We have the following for \base{} if $\beta_1 \leq \sqrt{\beta_2}, \beta_2 \geq \frac{1}{2}, \vm_{0,i}\,{=}\,0,\vv_{0,i}>0$,
\begin{equation}
\begin{aligned}
\frac{1}{T}\sum_{t=0}^{T-1}\E\big[\left\|\nabla
F(\vtheta_t)\right\|\big] & {\leq} \frac{(1+\beta_1)\sqrt{d}}{\sqrt{\beta_1(1-\beta_2)}} \eps^2 {+} \left(\sqrt[4]{\zeta} {+} \sqrt{\hat{V}d}\right) \eps \\
&\qquad\quad + \mu L\sqrt{d} \ .
\end{aligned}
\end{equation}
where $\hat{V}^2 \triangleq \left\|\vv_0\right\| + \underbrace{(1+\beta_1)\frac{8(\sigma^2 + C^2)d}{K\mu^2}}_{\normalfont\text{Variance}}$.
\end{corollary}
\textbf{Remark.} To the best of our knowledge, our Thm.~\ref{thm:r-adazo} and Cor.~\ref{cor:zo-adamm} are the first analyses to explicitly incorporate the impact of second moment estimate (measured by $V$ or $\hat{V}$) that is variance-dependent into the convergence of adaptive ZO methods. Specifically, Thm. \ref{thm:r-adazo} and Cor.~\ref{cor:zo-adamm} demonstrate that the convergence of $\frac{1}{T}\sum_{t=0}^{T-1}\E\big[\left\|\nabla
F(\vtheta_t)\right\|\big]$ typically exhibits a dependence of $\gO(\sqrt{V}\eps)$ in adaptive ZO methods, highlighting the importance of an improved second moment estimate with reduced variance. This explains the advantage of \ours{} over other adaptive ZO methods like \base{} thanks to our refined second moment estimate \eqref{eq:2nd-mo} achieving a reduction of at most $\frac{1-\beta_1}{1+\beta_1}$ on $V$. Comparing Thm. \ref{thm:r-adazo} and Cor. \ref{cor:zo-adamm}, we observe that \ours{} achieves a speedup of $\gO\left(\sqrt[4]{\frac{1+\beta_1}{1-\beta_1}}\right)$ for the convergence of averaged gradient norm.
% , which will be empirically validated in Sec.~\ref{sec:exps}.
% , and (b) \base{} and \rms{} exhibit nearly identical convergence. These findings will be empirically validated in Sec.~\ref{sec:exps}.

%sy: add reference here.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{workspace/figs/synthetic.pdf}
    \caption{Convergence comparison among different adaptive ZO optimizers for various synthetic functions, in which $y$-axis represents the log-scale optimality gap $F(\vtheta) - \min_{\vtheta'} F(\vtheta')$ and $x$-axis is the number of iterations $T$. Each curve denotes the mean from 3 independent runs.}
    \label{fig:syn}
\end{figure*}

\section{Experiments}\label{sec:exps}
In this section, we conduct extensive experiments on various tasks, including synthetic functions (Sec.~\ref{sec:syn}), black-box adversarial attack (Sec.~\ref{sec:attack}), and memory-efficient LLM fine-tuning (Sec.~\ref{sec:tuning}), to show the efficacy of \ours{}.

\subsection{Synthetic Functions}\label{sec:syn}

\textbf{On Convergence.} We first compare the convergence of \ours{} with \base{} and \rms{}, an integration of RMSProp \citep{rmsprop} and ZO gradient estimator, using four synthetic functions with $d{=}10^4$, including Quadratic, Rosenbrock, Rastrigin, and Ackley function. We refer to Appx.~\ref{appx:syn} for more details. The results are in Fig. \ref{fig:syn}, showing that \ours{} consistently achieves significantly faster convergence and lower optimality gaps compared to \rms{} and \base{}. Specifically, \ours{} demonstrated approximately 3.75$\times$ for Quadratic, Rosenbrock, and Rastrigin (or 2.5$\times$ for Ackley) speedup in reducing the optimality gap to those achieved by \rms{} after $10^4$ iterations. This consistent gain across all functions suggests that \ours{} is robust to the structure of the underlying problem. Furthermore, Fig. \ref{fig:syn} reveals a notable similarity in the convergence behavior of \base{} and \rms{} across all four benchmark functions. In contrast, \ours{} consistently demonstrates a substantial speedup compared to \rms{}. These results imply that the first moment itself contributes minimally to the convergence gains for adaptive ZO optimization, and underscores the critical role of our refined second moment estimate in achieving the superior performance of \ours{}.

\begin{figure}[t]
\vspace{-2mm}
\centering
\includegraphics[width=0.5\textwidth]{workspace/figs/moments.pdf}
\caption{Effects of (a) first and (b) second moment under varying $\beta_1$ during the convergence of Quadratic function. Here, $\vg_t$ and $\vm_t$ corresponds to the results from the estimated gradient in \eqref{eq:fd} and the first moment in \eqref{eq:1st-mo}, and $\vv_t$ (ori) and $\vv_t$ (ours) are results of the second moment estimates defined in \eqref{eq:xcxv} and \eqref{eq:2nd-mo} respectively. The $y$-axis in (a) represents the cosine similarity between $\vg_t$ or $\vm_t$ and the true gradient $\nabla F(\vtheta_{t-1})$, while the $y$-axis in (b) denotes the relative error between $\vv_t$ in \eqref{eq:xcxv} or \eqref{eq:2nd-mo} and the $\vv_t$ computed using the square of the true gradient $\nabla F(\vtheta_{t-1})$.}
\label{fig:moments}
% \vspace{-3mm}
\end{figure}

\textbf{On First Moment.} 
We further conduct an experimental analysis to understand how $\beta_1$ affects first moment estimates during the optimization process of the Quadratic function. In Fig.~\ref{fig:moments} (a), we present the results, using cosine similarity to measure the alignment between the estimated gradient $\vg_t$ or the estimated first moment $\vm_t$, and the true gradient $\nabla F(\vtheta_{t-1})$. The results indicate that the estimated first moment $\vm_t$ exhibits better cosine similarity than $\vg_t$, resulting from its variance reduction effect, as proven in Thm.~\ref{thm:vr}.  Moreover, we observe that increasing $\beta_1$ generally enhances this variance reduction. However, excessively high values of $\beta_1$ result in a minor decrease in similarity. This trend is consistent with the trade-off discussed in Thm.~\ref{thm:vr}.

\textbf{On Second Moment.} We further conduct an experimental analysis to understand how $\beta_1$ affects second moment estimates during the optimization process of the Quadratic function. Figure~\ref{fig:moments}(b) compares the second moment estimates, $\vv_t(\text{ori})$ from \eqref{eq:xcxv} and $\vv_t(\text{ours})$ from \eqref{eq:2nd-mo}, using the relative error against the second moment estimate based on the squared ground truth $(\nabla F(\vtheta_{t-1}))^2$. The results demonstrate that our refined second moment estimate, $\vv_t(\text{ours})$, significantly reduces the relative error compared to the standard second moment estimate, $\vv_t(\text{ori})$, which therefore enables the capture of more accurate geometry information during optimization. Interestingly, increasing values of $\beta_1$ generally lead to a lower relative error, a trend that contrasts with the behavior of first moment estimates. This lack of a trade-off is likely due to the loose bound we derived for our refined second moment.

% We also investigate effects of first and second moment under varying $\beta_1$ during the convergence of Quadratic function above. The results are in Fig.~\ref{fig:moments}, in which we use cosine similarity to examine the variance reduction effects in the first moment \eqref{eq:1st-mo} and relative error between estimated second moment and the . Fig.~\ref{fig:moments}(a) demonstrates that (i) the improved cosine similarity between $\vm_t$ and $\nabla F(\vtheta_{t-1})$ verifies the variance reduction effect in first moment, (ii) this variance reduction effect incurs a trade-off with respect to $\beta_1$, a larger $\beta_1$ can generally improve the variance reduction effect whereas a too large $\beta_1$ will slighly decrease the similarity, which aligns with our theoretical results from Thm. \ref{thm:vr}. In addtion, Fig.~\ref{fig:moments}(b) reveals that (i) our refined second moment in \ref{eq:2nd-mo} can indeed offers an improved estimate that more closely approaches the variance-free second moment update, (ii) the scale of our refined second moment generally increase when $\beta_1$ is increasing. This does have the same trade-off as first moment, which is likely because the loose bound we derived for second moment.

\subsection{Black-Box Adversarial Attack}\label{sec:attack}
\begin{table}[t]
\caption{
Comparison of the number of iterations to achieve a successful black-box adversarial attack.
Each cell represents mean $\pm$ standard deviation from five independent runs.
}
\label{tab:attack}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
Measurement & \rms{} & \base{}  & \ours{} \\
\midrule 
\# Iters ($\times 10^3$) & 15.6$\pm$3.2 & 15.5$\pm$4.1 & \textbf{2.9}$\pm$\textbf{0.8} \\
Speedup & 1.0$\times$ & 1.0$\times$ & \textbf{5.4}$\times$ \\
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\end{table}

Following the practice in \citep{zord}, we also present a comparative analysis of the number of iterations required for successful black-box adversarial attacks on an image from the MNIST dataset \citep{lecun1998mnist}, using \rms{}, \base{}, and \ours{} in Tab.~\ref{tab:attack} (experimental setup in Appx.~\ref{appx:attack}).  As shown in the table, \rms{} and \base{} exhibit similar performance, requiring an average of approximately 15.6 and 15.5 thousand iterations, respectively. The standard deviations of the iteration counts were similar as well, about 3200 to 4100 iterations. These align with our results in Sec.~\ref{sec:syn}. On the other hand, \ours{} requires a significantly lower number of iterations with an average of only 2900, and a smaller standard deviation of 800 iterations, suggesting a faster and more stable convergence. The speedup achieved by \ours{}, i.e., a speedup of 5.4$\times$ compared to the baseline \rms{}, is also highlighted in Tab.~\ref{tab:attack}. These findings thus further underscore the superior efficacy of \ours{}.

\subsection{Memory-Efficient LLM Fine-Tuning}\label{sec:tuning}
\begin{figure}[t]
\vspace{-2mm}
\centering
\includegraphics[width=0.5\textwidth]{workspace/figs/mezo.pdf}
\caption{Training loss comparison among various adaptive ZO optimizers for the fine-tuning of LLMs under different model sizes on SST-2 dataset \citep{sst2}. Each curve denotes the mean from 3 independent runs.}
\label{fig:mezo}
% \vspace{-3mm}
\end{figure}

Recent interest in memory-efficient fine-tuning of large language models using ZO optimization \citep{mezo, revisit-mezo} motivates our use of this setting to further demonstrate the superiority of \ours{} over other adaptive ZO optimization algorithms (experimental setup in Appx.~\ref{appx:tuning}). The results in Fig.~\ref{fig:mezo} show that, for both OPT-1.3B and OPT-13B models \citep{zhang2022opt}, \ours{} converges significantly faster than \rms{} and \base{}, achieving a speedup of 4.29$\times$ on OPT-1.3B and 3.75$\times$ on OPT-13B to reach the same training loss. The optimization curves of \rms{} and \base{} are indistinguishable, indicating the similar convergence behavior we have seen in Sec.~\ref{sec:syn} and Sec.~\ref{sec:attack}. These empirical results strongly support \ours{} as a more efficient and effective adaptive ZO optimizer.

\section{Conclusion}
In conclusion, this work introduces \ours{}, a novel approach that addresses the critical limitations of existing adaptive ZO methods by effectively leveraging moment information. Through rigorous theoretical analysis, we have demonstrated the inherent variance reduction effect of first moment estimates on ZO gradient estimates, leading to more stable and accurate updates, as well as the improved accuracy of our refined second moment estimates. Furthermore, we establish the first variance-aware convergence framework for adaptive ZO methods and prove the superior convergence rate of \ours{}. The consistent empirical performance gains of \ours{} across diverse applications underscore its potential as a powerful and practical solution for real-world ZO optimization challenges. 

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of zeroth-order optimization and machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.