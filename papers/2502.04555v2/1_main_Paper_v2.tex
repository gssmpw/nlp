\documentclass[a4paper, superscriptaddress, amsfonts, amssymb, amsmath, reprint, showkeys, nofootinbib, twoside,aps,prl]{revtex4-1}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{bm}% bold math
\usepackage{graphicx}% Include figure files
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts,amssymb,amsthm}
\usepackage[colorinlistoftodos, color=green!40, prependcaption]{todonotes}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[left=23mm,right=13mm,top=35mm,columnsep=15pt]{geometry} 
\usepackage{adjustbox}
\usepackage{placeins}
\usepackage[T1]{fontenc}
\usepackage{lipsum}
\usepackage{csquotes}
%\usepackage{hyperref}
\usepackage[pdftex, pdftitle={Article}, pdfauthor={Author}]{hyperref} % For hyperlinks in the PDF

%\usepackage[paperwidth=210mm,paperheight=297mm,centering,hmargin=2cm,vmargin=2.5cm]{geometry}
\usepackage{geometry}
\geometry{a4paper, portrait, margin=0.7in}

\bibliographystyle{apsrev4-1}
\begin{document}
\title{Decomposing Multivariate Information Rates in Networks of Random Processes}

\author{Laura Sparacino}
    \affiliation{Department of Engineering, University of Palermo, Palermo, Italy}
\author{Gorana Mijatovic}
    \affiliation{Faculty of Technical Sciences, University of Novi Sad, Serbia}
\author{Yuri Antonacci}
    \affiliation{Department of Engineering, University of Palermo, Palermo, Italy}
\author{Leonardo Ricci}
    \affiliation{Department of Physics, University of Trento, Italy}
\author{Daniele Marinazzo}
    \affiliation{University of Ghent, Belgium}
\author{Sebastiano Stramaglia}
    \affiliation{University of Bari Aldo Moro and Istituto Nazionale
di Fisica Nucleare, Sezione di Bari, Italy}
\author{Luca Faes}
    \email[Correspondence email address: ]{luca.faes@unipa.it}% Your name
    \affiliation{Department of Engineering, University of Palermo, Palermo, Italy}
    \affiliation{Faculty of Technical Sciences, University of Novi Sad, Serbia}

\begin{abstract}
The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the  framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations.
By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. 
The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order \textcolor{black}{interactions} that are obscured in the time domain.
Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress.
Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology.

%Our results highlight the necessity of considering the dynamical structure of the investigated processes, as well as the importance of a spectral information decomposition in dynamic network analysis, paving the way for more accurate analyses of multivariate time series in neuroscience, biology, and engineering.


\end{abstract}

\keywords{partial information decomposition, mutual information rate, coarse graining, lattice theory, multivariate time series, network physiology}

\maketitle

\section{\label{sect:Intro}Introduction}
% 1) BACKGROUND: the study of many-body interactions in complex network systems composed by several interconnected units 
In the growing research area of Network Science \cite{barabasi2013network}, identifying and untangling the multifaceted many-body interactions occurring in complex network systems composed by several interconnected units has become a crucial task. Different approaches have been developed to deepen our understanding of interactions in complex networks, and have been exploited in applicative fields ranging from neuroscience and biology to sociology and engineering. Advances have come from the utilization of many disparate tools such as graph theory and network analysis \cite{newman2003structure}, dynamical systems approaches including nonlinear methods, chaos theory, synchronization and coupled oscillator models \cite{strogatz2015nonlinear,boccaletti2002synchronization,pikovsky2003synchronization,ricci2021experimental}, tools for the analysis of higher-order networks, hypergraph models and simplicial complexes \cite{courtney2016generalized,battiston2021physics,santoro2023higher}, as well as methods rooted in the application of information theory to network models and time series data \cite{mcgill1954interaction,sole2004information,timme2014synergy,novelli2021inferring,perinelli2018correlation,perinelli2021relationship}.

% among these, the concept of PID for random variables has acquired a remarkable importance
In this context, partial information decomposition (PID) has been developed as a comprehensive framework designed to understand how information is distributed in multivariate systems: the foundational work by Williams and Beer \cite{williams2010nonnegative} introduced PID as a method to decompose multivariate information in non-negative terms, addressing limitations of traditional measures like the interaction information \cite{mcgill1954interaction} which can yield both positive and negative values often obscuring the interpretation of informational relationships. Considering a \textit{target} random variable and a set of \textit{source} variables, the mathematical redundancy lattice structure defined for the PID \cite{williams2010nonnegative} identifies a set of atoms whose associated partial information (PI) amounts constitute the building blocks of the analyzed multivariate information shared between the target and the sources, quantified by the mutual information (MI). 
Moreover, to overcome the limitation that the number of atoms grows super-exponentially with the number of source variables \cite{gutknecht2021bits}, refinements have been introduced whereby the PI atoms are aggregated meaningfully to highlight how the MI is distributed among the sources \cite{rosas2020reconciling}; these refined approaches provide a coarse-grained decomposition with a small number of atoms that scale gracefully with the system size, highlighting the \textit{unique} information exclusively available from each source, the \textit{redundant} information obtained from at least two different sources, and the \textit{synergistic} information revealed only when multiple sources are considered simultaneously.
%As an alternative, the latter can be decomposed into components highlighting how such an information is distributed among the sources: the \textit{unique} information exclusively available from each source, the \textit{redundant} information obtained from at least two different sources, and the \textit{synergistic} information revealed only when multiple sources are considered simultaneously. Contrarily to the full PID based on computing the PI amounts, which yields a number of atoms growing super-exponentially with the number of sources \cite{gutknecht2021bits}, this approach provides a coarse-grained decomposition with a small number of atoms that scale gracefully with the system size \cite{rosas2020reconciling}.

Thanks to the peculiarities described above, the PID framework has become widely adopted as a main tool to assess high-order interdependencies among the units of network datasets collected in several applicative fields of physics, engineering and life sciences where understanding the specific contributions of individual variables to the collective information is of fundamental importance \cite{wibral2017partial, cang2020inferring, rosas2020reconciling, luppi2022synergistic, wollstadt2023rigorous, dissanayake2024quantifying}. For instance, it has been used to analyze neural information processing dynamics, revealing how different brain regions encode stimuli both redundantly and synergistically \cite{kay2022comparison}. In machine learning, PID has been exploited in feature selection to distinguish between information that is redundant across features and information that is uniquely informative \cite{barnett2009partial, taylor2018partial,wollstadt2023rigorous}. Recent applications have highlighted its value for network analysis also in computational neuroscience, biology and physiology \cite{luppi2024information,sherrill2021partial,faes2021information}.

% DRAWBACKS of the PID framework: 1) redundancy definitions not universally accepted
However, in spite of the universality of the problem posed by PID, the underlying analytical framework presents some inherent limitations that restrict its unambiguous utilization in different contexts. A first issue was recognized since the inception of PID \cite{williams2010nonnegative} noting that the information atoms of
unique, redundant, and synergistic information cannot be
defined using classical information theory, but rather require the introduction of new axioms whose definition is not yet universally accepted.
%Since it is known that the PID is solved once a redundancy function is assigned, how to define such function unequivocally represents an active area of research.
Consequently, several axiomatic definitions of redundancy have been proposed so far that differ depending on the philosophy followed to satisfy the desired properties (e.g., decision- \cite{pakman2021estimating}, game- \cite{ince2017measuring}, information-theoretic \cite{makkeh2021introducing}), on the nature (continuous \cite{barrett2015exploration, pakman2021estimating, ehrlich2024partial} or discrete \cite{williams2010nonnegative, bertschinger2014quantifying, ince2017measuring}) of the analyzed variables, and on assumptions made about their distribution (e.g., Gaussian \cite{barrett2015exploration}). Popular and simple approaches implement the so-called minimum MI (MMI) PID schemes, whereby redundancy is defined for a given atom as the minimum of the MI (or the specific MI) shared between the target (or a specific state of it) and each source \cite{williams2010nonnegative,barrett2015exploration}. However, these schemes are limited in the fact that they quantify the minimum amount of information that all variables carry but do not require that such information is the same for all variables \cite{harder2013bivariate,bertschinger2014quantifying,griffith2015quantifying,ince2017measuring}. To overcome this limitation, less conservative approaches have been proposed which typically define redundancy at the local or pointwise level, rather than at the level of ensemble averages \cite{ince2017measuring,makkeh2021introducing,ehrlich2024partial}; these approaches allow defining more refined redundancy quantities, but suffer in their turn from the limitation of potentially yielding negative information atoms (essentially because local MI can be negative), which hinders a strightforward interpretation of the results. Thus, the definition of a proper operalization of PID, merging computability and interpretability, is still an open problem in information theory.


% DRAWBACKS of the PID framework: 2) what happens when we consider random processes with temporal structure
Another crucial issue, which is the main question addressed in this paper, is how to apply PID to random processes with temporal statistical structure.
In fact, although specifically defined for random variables, PID is often needed in practice to analyze multivariate time series whose most proper statistical representation is the vector random process. Then, since both the target random process and the set of source processes constitute collections of %composed of several source processes $X=\{X_{1,t},\ldots,X_{M,t}\}$ and a target process $Y=\{Y_t\}$ ($t \in \mathbb{Z}$ for discrete-time processes). 
random variables, utilization of the PID in such dynamic case is not straightforward as it implies an arbitrary selection of the variables to be extracted from each process.
% zero-lag PID
In the literature, PID has been applied to the variables sampling the processes at the same time, performing a so-called "static PID", under the implicit assumption that the processes are stationary and memoryless (i.e., composed by independent and identically distributed - i.i.d. - variables).
%, setting $T\equiv Y_t$ and $S\equiv X_t$ as target and source variables, and thus disregarding the temporal statistical structure of the multivariate process.
However, the i.i.d. assumption is typically not tested in practice and is often violated in applications of information decomposition where the analyzed data exhibit nontrivial temporal correlations \cite{kay2022comparison, varley2023information, varley2023multivariate}.
% TE PID
Alternatively, the PID has been applied to random processes by selecting the variables to use in order to decompose the joint information transferred from all sources to the target \cite{krohova2019multiscale,luppi2022synergistic,koccillari2023behavioural,luppi2024synergistic}. Specifically, taking the present of one predefined process as the target variable and the past histories of all other processes as (vector) source variables, and conditioning the MI between target and sources on the past of the predefined process, yields the PID of the popular transfer entropy (TE) measure of information transfer \cite{schreiber2000measuring}.
Nevertheless, although considering the temporal statistical structure of the multivariate process, the PID of the TE cannot account for instantaneous interactions among the processes, nor for causal interactions occurring in the causal direction from the target to the sources. Hence, the current applications of PID to random processes provide only a partial, often misleading view of the interactions among the units of dynamic network systems. 

% 3) how we fill the gap in the literature: the PIRD framework
To deal with these significant limitations, the present work introduces a framework for the decomposition of the information shared dynamically between the target and the source units composing the analyzed network of random processes. Our idea is to shift the paradigm of PID from the use of random variables to the use of random processes as building blocks of information decomposition: leveraging the use of information rate quantities in place of standard information quantities, we replace the MI between random variables with the MI (MIR) between random processes, and use the same lattice backbone of PID to implement the so-called \textit{partial information rate decomposition} (PIRD).
PIRD dissects the information shared per unit of time between the designed target random process and the set of source processes, quantified by the MIR, into PI rate atoms adopting a full PID perspective, or into unique, redundant and synergistic information rates adopting a coarse-graining perspective.
The PIRD is solved introducing the new information-theoretic measure of \textit{redundancy rate} which generalizes the MIR over the lattice. Among several possible definitions, the redundancy rate is here formulated following a pointwise approach implemented in the frequency domain, i.e. defining a so-called spectral redundancy rate which quantifies the concept of redundancy among iso-frequency oscillatory components of the analyzed processes. This allows to achieve a non-negative decomposition of the spectral MIR between target and source processes at each specific frequency. Moreover, in the case of linear Gaussian processes \cite{geweke1982measurement, chicharro2011spectral}, the use of the minimum MI principle \cite{barrett2015exploration}  applied to the MIR between the target and each source process leads us to retrieve a non-negative time-domain PIRD through integration of the the PI rate atoms over the whole frequency spectrum.

%Spectral redundancy is then assessed following the MMI principle \cite{barrett2015exploration} applied to the spectral MIR computed between the target and each source process at the frequency of interest.
%In the linear signal processing framework, the latter can be obtained as a logarithmic combination of the terms appearing in the power spectral density (PSD) matrix of the analyzed set of stochastic processes, whose practical computation relies on a linear parametric representation of the observed dynamics \cite{geweke1982measurement, chicharro2011spectral}. Remarkably, the time and frequency domain formulations are linked through the spectral integration property valid for Gaussian processes \cite{geweke1982measurement, chicharro2011spectral}.

% --- simulations and exemplary application to physiology
The feasibility of the new framework is displayed exhaustively in benchmark simulations of linearly interacting processes, where we illustrate the benefits of PIRD over the static PID in the presence of non-trivial temporal correlations, investigate the differences with the PID applied to the TE, and highlight the peculiarity of the spectral approach which allows to perform PID restricting the analysis to predetermined frequency bands with practical meaning. The latter property is then exploited in the applicative context of Network Physiology \cite{ivanov2021new} reporting the application of the coarse-grained PIRD to a physiological network comprising cerebrovascular, cardiovascular and respiratory time series analyzed in patients prone to develop postural-related syncope.
%variables assessed in patients prone to develop postural-related syncope has demonstrated the great potential of the framework in assessing frequency-specific high-order interactions, as well as confirmed the redundant character of information sharing in these networks in the presence of external stressors.

\section{\label{sect:PID_variables}Decomposition of multivariate information}
Let us consider a static network system composed of $M+1$ nodes $\mathcal{V} = \{ \mathcal{T},\mathcal{S}_1,\ldots,\mathcal{S}_M \}$, where the activity at each node is described in terms of random variables $T,S_1,\ldots,S_M$, with $T$ assumed as the \textit{target} variable and $S = \{ S_1,\ldots,S_M \}$ as the vector of \textit{sources}. 
The identification of a target and several source variables naturally leads to use directed approaches to assess and decompose the multivariate information shared in the system, i.e., the mutual information (MI) between $T$ and $S$ defined as
%In the framework of information theory, the activity of the generic network node can be assessed through the information carried by the variable, e.g., $S_i$, quantified by its Shannon entropy, $H(S_i)=\mathbb{E} \left[ \log \frac{1}{p(s_i)} \right]$ \cite{shannon1948mathematical, cover1999elements},
%\begin{equation}
%   H(S_i)=\mathbb{E} \left[ \log \frac{1}{p(s_i)} \right],
%   \label{Entropy}
%\end{equation}
%while the interaction between the activities of two nodes, e.g., the $i^{\mathrm{th}}$ source and the target, can be assessed through the \textit{mutual information} (MI) between $S_i$ and $T$:
\begin{equation}
   I(T;S)=\mathbb{E} \left[ \log \frac{p(t,s)}{p(t)p(s)} \right],
   \label{MutualInformation}
\end{equation}
where $p(\cdot,\cdot)$ and $p(\cdot)$ denote joint and marginal probability, with $\mathbb{E}[\cdot]$ being the expectation operator.
Directed approaches to information decomposition expand the MI in (\ref{MutualInformation}) into terms related to the contributions that the individual sources $S_i$, $i=1,\ldots,M$, or a collection thereof, share with the target $T$. Such contributions are related in non-trivial ways to the marginal MI between each source and the target, $I(T;S_i)$. In this context, the partial information decomposition (PID) introduced by Williams and Beer (\cite{williams2010nonnegative}) provides one of the most popular approaches to study multivariate systems through the lens of a directed decomposition of multivariate information.



%The entropy quantifies the information contained in a random variable intended as the average uncertainty about its outcomes, while the MI quantifies the information shared by two variables intended as the uncertainty about one variable that is resolved by knowing the other variable. Remarkably, the MI is a symmetric measure (i.e., $I(T;S_i)=I(S_i;T)$), and is linked to the joint and individual entropies of the two variables by the relation $I(T;S_i)=H(T)+H(S_i)-H(T,S_i)$, where $H(T,S_i)$ is the joint entropy of $\{T,S_i\}$. The MI can be also expressed as $I(T;S_i)=H(T)-H(T|S_i)$, where $H(T|S_i)=H(T,S_i)-H(S_i)$ is the conditional entropy (CE) of $T$ given $S_i$, quantifying the information carried by one variable that is not shared with the other, intended as the residual uncertainty which remains in one variable when the other variable is known. 

\subsection{\label{subsect:PID_variables_PID}Partial Information Decomposition and the redundancy lattice}
Considering the target variable $T$ and the vector source variable $S$, the mathematical \textit{redundancy lattice structure} defined for the PID \cite{williams2010nonnegative} identifies a set of atoms whose associated partial information (PI) amounts constitute the building blocks of the analyzed multivariate information $I(T;S)$.
The lattice is identified by the collection $\mathcal{A}$ of all subsets of sources such that no source is a superset of any other, i.e., the set of anti-chains formed from the indices of the sources in $S$ under set inclusion \cite{williams2010nonnegative} 
(see Fig. \ref{fig:fig_lattice}a,b (left) for the cases of $M=2$ and $M=3$ sources; e.g, $\mathcal{A}=\{\{1\}\{2\},\{1\},\{2\},\{12\}\}$ if $M=2$). Formally, the PID expands (\ref{MutualInformation}) as
%clearly interpretable as non-negative informational quantities (Fig. \ref{fig:fig_lattice}a,b, left), can be applied to clarify the general structure of multivariate information among multiple variables. The PID representation has allowed to expand the MI as as the sum of contributions (\textit{information atoms} $I^{\delta}(\cdot;\cdot)$) identified by the lattice structure \cite{williams2010nonnegative}
\begin{equation}
    I(T;S) = \sum_{\alpha \in \mathcal{A}} I^{\delta}(T;S_{\alpha}),
    \label{PID_atoms}
\end{equation}
where $I^{\delta}(\cdot;\cdot)$ is the PI function defined over the atoms of the lattice, $\alpha=\{\alpha_1,\ldots,\alpha_J\} \in \mathcal{A}$, and $S_{\alpha}$ denotes the set of subsets of source variables indexed by the $\alpha^{\mathrm{th}}$ atom,
with $\alpha_j\subseteq \{1\cdots M\}$, $S_{\alpha_j} \subseteq S$, $j=1,\ldots,J$. 

To complete the PID besides the basic statement in (\ref{PID_atoms}) it is necessary to provide a set of so-called consistency equations which, relating atoms to mutual information, allow to derive the PI terms \cite{gutknecht2021bits}. The main consistency equations state that the marginal MI terms involving any individual source variable $S_i$ are constructed additively by summing the information of the atoms positioned at the level $\{i\}$ and downwards in the lattice, i.e.,
\begin{equation}
    I(T;S_i)=\sum_{\beta \preceq \{i\}} I^{\delta}(T;S_{\beta}),
    \label{marginalMI}
\end{equation}
where $\preceq$ identifies precedence based on the partial ordering imposed by the lattice structure \cite{williams2010nonnegative}. 
Moreover, as the equations (\ref{PID_atoms}) and (\ref{marginalMI}) do not suffice to solve the PID problem because they provide a number of constraints lower than the number of information atoms to be computed (i.e., $M+1<|\mathcal{A}|$, where $|\cdot|$ indicates cardinality), to complete the PID it is necessary to define a so-called \textit{redundancy function}, here denoted as $I^{\cap}(\cdot;\cdot)$, which generalizes the MI over the lattice. The redundancy function extends (\ref{marginalMI}) to each atom $\alpha \in \mathcal{A}$, fulfilling 
\begin{equation}
    I^{\cap}(T;S_{\alpha})=\sum_{\beta \preceq \alpha} I^{\delta}(T;S_{\beta}),
    \label{redundancy_function}
\end{equation}
where $\beta$ represents the atoms preceding or equal to $\alpha$ and structurally connected to it in the lattice.
Finally, the information associated to all atoms can be retrieved, starting from the knowledge of the redundancy function, either iteratively as
\begin{equation}
    I^{\delta}(T;S_{\alpha})=I^{\cap}(T;S_{\alpha})-\sum_{\beta \prec \alpha} I^{\delta}(T;S_{\beta}),
    \label{delta_from_red}
\end{equation}
or in a compact way via  M\"{o}bius inversion of (\ref{redundancy_function}) \cite{williams2010nonnegative}. Importantly, while the redundancy value of an atom $\alpha$, $I^{\cap}(T;S_{\alpha})$, measures the total amount of redundant information shared by all the sources included in that atom, the PI measures the unique information contributed only by that atom.

At this point is worth noting that, although the PID is solved once a redundancy function is assigned, there is no consensus in the literature about how to define such function unequivocally. This is an active area of research, with several axiomatic definitions of redundancy proposed so far that differ depending on the philosophy followed to satisfy the desired properties (e.g., decision- \cite{pakman2021estimating}, game- \cite{ince2017measuring}, information-theoretic \cite{makkeh2021introducing}), on the nature (continuous \cite{barrett2015exploration, pakman2021estimating, ehrlich2024partial} or discrete \cite{williams2010nonnegative, bertschinger2014quantifying, ince2017measuring}) of the analyzed variables, and on assumptions made about their distribution (e.g., Gaussian \cite{barrett2015exploration}). A popular and simple approach is the so-called minimum MI (MMI) PID, whereby redundancy is defined for the atom $\alpha=\{\alpha_1,\ldots,\alpha_J\}$ as the minimum of the information shared between the target and each individual information component \cite{barrett2015exploration}:
\begin{equation}
    I^{\cap}(T;S_{\alpha}) = \min\limits_{j=1,\ldots,J} I(T;S_{\alpha_j});
    \label{redundancy_MMI}
\end{equation}
for instance, considering $M=3$ sources and the atom $\alpha=\{\{1\},\{23\}\}$, the redundancy becomes the minimum between $I(T;S_1)$ and $I(T;S_2,S_3)$. 
The MMI PID is generally applied in case of Gaussian data, for which it subsumes several of the previously proposed PID schemes \cite{barrett2015exploration}; we will adopt this approach in the practical implementation of the redundancy rate for random processes defined in Sect. "Decomposition of Multivariate Information".


\begin{figure*}
\centering
\includegraphics{Figures_Paper/lattice_PIRD.pdf}
\caption{\textbf{Standard and coarse-grained partial information decomposition, superimposed on the redundancy lattices for $M$ sources.}
\textbf{(a)} Standard PID on the redundancy lattice for 3 variables ($M=2$). The alphabet of source combinations is $\mathcal{A}= \{ \{1\}\{2\}, \{1\}, \{2\}, \{12\} \}$, where $\{1\}$ denotes $S_1$ and $\{2\}$ denotes $S_2$. The MI between the target and the set of sources is decomposed into a redundant (magenta), a synergistic (blue) and two unique (gray) contributions, which are exclusively provided by different atoms of the lattice. 
\textbf{(b)} Standard and coarse-grained $1^{st}$ order PID on the redundancy lattice for 4 variables ($M=3$). The alphabet of source combinations is $\mathcal{A}= \{ \{1\}\{2\}\{3\}, \{1\}\{2\}, \{1\}\{3\}, \{2\}\{3\}, \ldots, \{123\} \}$, where $\{i\}$ denotes $S_i$, $i=1,\ldots,M$. The MI between the target and the set of sources is decomposed into a redundant (magenta), a synergistic (blue) and three unique (gray) contributions, which are exclusively provided by different set of atoms of the lattice.}
\label{fig:fig_lattice}
\end{figure*}

\subsection{Coarse-grained PID}

An important aspect with practical relevance is that, as an alternative to (\ref{PID_atoms}), the PID can be formulated in a meaningful way by making explicit the \textit{unique information} that each source $S_m$ holds about the target $T$ ($m=1,\ldots,M$), the \textit{redundant information} that all source variables in $S$ hold about $T$, and the \textit{synergistic information} about $T$ that only arises from knowing all the sources $S_1,\ldots,S_M$. This corresponds to expand multivariate information as
\begin{equation}
    I(T;S) =  \sum_{m=1}^{M} \mathcal{U}(T;S_m) + \mathcal{R}(T;S) + \mathcal{S}(T;S),
    \label{PID_contributions}
\end{equation}
where
\begin{equation}
    \mathcal{U}(T;S_m) = I(T;S_m) - \mathcal{R}(T;S).
    \label{UI_MI_equivalence}
\end{equation}
This approach provides a \textit{coarse-grained} decomposition with a small number of atoms that scale gracefully with the system size: while the full PID (\ref{PID_atoms}) yields a number of atoms $|\mathcal{A}|$ that grows super-exponentially with $M$ like the Dedekind numbers \cite{gutknecht2021bits}, the coarse-grained PID (\ref{PID_contributions}) decomposes the multivariate information into exactly $M+2$ quantities.
In particular, the two formulations coincide when $M=2$ source variables are considered, yielding $\mathcal{R}(T;S)=I^{\delta}(T;S_{\{1\}\{2\}})$, $\mathcal{U}(T;S_1)=I^{\delta}(T;S_{\{1\}})$, $\mathcal{U}(T;S_2)=I^{\delta}(T;S_{\{2\}})$, and $\mathcal{S}(T;S)=I^{\delta}(T;S_{\{12\}})$ (Fig. \ref{fig:fig_lattice}a, right). On the other hand, when $M \geq 3$ the coarse-graining is implemented by summing the PI of some of the atoms in (\ref{PID_atoms}).
This issue has been addressed in \cite{rosas2020reconciling}, where the construction for $M=2$ was generalized to $M=3$ sources through the so-called $k^{\mathrm{th}}$ order \textit{ coarse-grained} PID, which preserves the intuitive meaning that synergy, redundancy, and unique information have for $M=2$ sources (Fig. \ref{fig:fig_lattice}b, right).
Specifically, considering $k=1$, the $1^{\mathrm{st}}$-order synergy, $\mathcal{S}(T;S)$, corresponds to the information about the target that is provided by the whole $S$ but is not contained in any subset of sources when considered separately from the rest (atoms surrounded by the blue shade in Fig. \ref{fig:fig_lattice}).
%For example, for $M = 2$ we obtain the standard synergy related to the atom $\{ \{12\} \}$ (Fig. \ref{fig:fig_lattice}a), while for $M = 3$ synergy is computed from the atoms $\{ \{12\}, \{13\}, \{23\}, \{12\}\{13\}, \{12\}\{23\}, \{13\}\{23\}, \{12\}\{13\}\{23\}, \{123\} \}$ (Fig. \ref{fig:fig_lattice}b).
Similarly, the $1^{\mathrm{st}}$-order redundancy, $\mathcal{R}(T;S)$, is the information held by at least two different groups of size $1$ (magenta in Fig. \ref{fig:fig_lattice}).
Finally, the $1^{\mathrm{st}}$-order unique information provided by the $m^{\mathrm{th}}$ source, $\mathcal{U}(T;S_m)$, $m=1,\ldots,M$, is the information that $S_m$ has access to and no other subset of parts has access to on its own, although bigger groups of other parts may have (gray in Fig. \ref{fig:fig_lattice}). 




%The \textit{net information shared}, quantifying the balance between redundancy and synergy in the analysed network and herein referred to as $\Delta(T;S)$, can be obtained as
%\begin{equation}
%\begin{split}
%    I(T;S_1;\ldots;S_M) & = \sum_{m=1}^{M} I(T;S_m) - I(T;S) \\
%    & = \mathcal{R}(T;S) - \mathcal{S}(T;S),
%    \label{delta_PID}
%    \end{split}
%\end{equation}
%and has been previously referred to as \textit{interaction information} in the well-known framework of interaction information decomposition (IID) defined for triplets of random variables \cite{stramaglia2012expanding, stramaglia2014synergy, stramaglia2016synergetic, faes2021information}.
%Remarkably, the formulations in (\ref{PID_contributions}), (\ref{delta_PID}) coincide when $M=2$, while it has been demonstrated that the IID overestimates the term $I(T;S)$ when $M=3$ (\textit{double-counting redundancy}) \cite{rosas2020reconciling}. 

\subsection{\label{subsect:PID_variables_linear_formulation}Linear parametric formulation}
In the case in which the observed variables have a joint Gaussian distribution, the PID can be performed by exploiting linear parametric regression models. Specifically,  if $T \sim \mathcal{N}(m_T,\sigma^2_{T})$ and $S_i \sim \mathcal{N}(m_{S_i},\sigma^2_{S_i})$, $i=1,\ldots,M$, the target $T$ and the source $S_i$ are related by the following linear regression model:
\begin{equation}
    T = a S_i + b + U_i,
    \label{VAR_model_static}
\end{equation}
where $T$ is predicted using the coefficient $a$ weighing the regressor $S_i$, $b$ is the constant term and $U \sim \mathcal{N}(0,\sigma^2_{U_i})$ is a zero-mean Gaussian innovation with variance $\sigma^2_{U_i}$, uncorrelated with $S_i$.
Then, the MI between $T$ and $S_i$ can be estimated exploiting the relation between entropy and variance valid for Gaussian variables \cite{barrett2010}. Specifically, expressing the entropy of the predicted variable $T$ as $H(T) = \frac{1}{2} \log{( 2 \pi e \sigma^2_{T})}$, and the conditional entropy of the predicted variable $T$ given the predictor $S_i$ as $H(T|S_i) = \frac{1}{2} \log{( 2 \pi e \sigma^2_{U_i} )}$, yields to compute the MI as
\begin{equation}
    I(T;S_i) = H(T)-H(T|S_i) = \frac{1}{2} \log{\biggr( \frac{\sigma^2_{T}}{\sigma^2_{U}} \biggr)}.
    \label{MI_linear}
\end{equation}
Eq. (\ref{MI_linear}) can be applied to any combination of sources, allowing then to compute the redundancy function using (\ref{redundancy_MMI}) and the PI of each atom using (\ref{delta_from_red}), from which the coarse-grained terms are derived using (\ref{PID_contributions}).



\section{\label{sectPIRDprocesses}Decomposition of multivariate information rates} 
Let us consider a dynamic network system composed of $M+1$ nodes, $\mathcal{Z}=\{ \mathcal{Y},\mathcal{X}_1,\ldots,\mathcal{X}_M \}$, where the activity at each node is described in terms of the vector random process $Z=\{Y,X_1,\ldots,X_M\}$, with $Y$ assumed as \textit{target} and $X = \{ X_1,\ldots,X_M \}$ as the vector of \textit{sources}. % Here, the individual processes are taken as scalar, but our formulation extends intuitively to the case of vector sources.
Here we consider discrete-time random processes intended as time-ordered vector random variables: e.g., $Y_n\equiv Y(t_n)$ is the variable sampling the target process at the time $t_n$, where $n \in \mathbb{N}$ is the time index; typically, $t_n=n\Delta t$, where $\Delta t$ is the sampling period ($f_s=\Delta t^{-1}$ is the sampling frequency).
Since each random process is a collection of random variables, the application of the PID in the dynamic case is not straightforward as it implies an arbitrary selection of the variables to be extracted from each process. The most intuitive choice is to apply the PID to the variables sampling the processes at the same time $t_n$, setting $T=Y_n$ and $S=X_i$ as target and source variables and thus decomposing the static MI $I(Y_n;X_n)$ through a redundancy function based on zero-lag MI terms (e.g., $I^{\cap}(T;S_i)=I(Y_n;X_{i,n})$ according to the formalism presented in Sect. "Partial Information Decomposition and the redundancy lattice".
An alternative approach is to decompose the joint transfer entropy (TE) from all sources to the target, which is defined as $T_{X\rightarrow Y}=I(Y_n;X_{<n}|Y_{<n})$ \cite{schreiber2000measuring}, where $X_{<n}=\{X_{n-1},X_{n-2},\ldots\}$ and $Y_{<n}=\{Y_{n-1},Y_{n-2},\ldots\}$ denote the (potentially infinite-dimensional) vectors representing the past history of the source and target processes; in this case the PID is applied setting $T=Y_n$ and $S=X_{<n}$ as target and source variables, and the redundancy function is a conditional MI (e.g., $I^{\cap}(T;S_i)=I(Y_n;X_{i,<n}|Y_{<n}))$.
Although these two alternative applications of PID to random processes are common \cite{kay2022comparison,varley2023information,koccillari2023behavioural,luppi2024information}, they provide only a partial view of the dynamic interactions among the processes and are based on implicit assumptions which are often not fulfilled by dynamic network systems.
The zero-lag PID of $I(Y_n;X_n)$ presupposes to work with memoryless processes, a condition that is typically not satisfied in practice as the presence of a temporal statistical structure is inherently expected in time series data. On the other hand, the PID of the TE $I(Y_n;X_{<n}|Y_{<n})$ cannot account for instantaneous interactions among the processes, nor for causal interactions occurring in the causal direction from the target to the sources.
To overcome these limitations, in the following, we propose a framework for the decomposition of the information shared dynamically between the target and the source processes.


%The PID framework described in the previous section is applied to the variables sampling the target random process $Y=\{Y_n\}_{n \in \mathbb{Z}}$ and the vector source process $X=\{X_{1,n},\ldots,X_{M,n}\}_{n \in \mathbb{Z}}$ at the same time $n$, i.e., decomposing the MI between $Y_n$ and $X_n$, $I(Y_n;X_n)$, with $Y_n=T$, $X_n=S$. This is done implicitly assuming that these processes are composed by independent and identically distributed (i.i.d.) variables. Such an assumption, which is typically not tested in practice, presupposes to work with stationary and memory-less processes. However, while the property of stationarity can be often satisfied, the presence of a temporal statistical structure is inherently expected in time series data, so that the more realistic scenario is that of \textit{random processes} formed by identically distributed (i.d.) but correlated variables.

\subsection{Partial Information Rate Decomposition}

The framework proposed here for the decomposition of information in multivariate random processes makes use of the concepts of \textit{entropy rate} (ER) and \textit{mutual information rate} (MIR). The ER of a generic stationary random processes $X$ is defined as  \cite{cover1999elements}
\begin{equation}
   H_{X}=\lim_{m \to \infty}\frac{1}{m} H(X_{n:n+m}),
   \label{ER}
\end{equation}
and, being equivalent to the conditional entropy of the present state of the process given its past states (i.e., $H_{X}=H(X_n|X_{<n})$), quantifies the rate of generation of new information in the process. Then, considering another process $Y$, the MIR between $X$ and $Y$ is \cite{duncan1970calculation}
\begin{equation}
   I_{X;Y}=\lim_{m \to \infty} \frac{1}{m} I(X_n,\ldots,X_{n+m};Y_n,\ldots,Y_{n+m}),
   \label{MIR}
\end{equation}
quantifying the information shared by the two processes per unit of time; the MIR can be expressed in terms of entropy rates as $I_{X;Y}=H_{X}+H_{Y}-H_{X,Y}$, evidencing the analogy between the concepts of entropy and MI for random variables and the concepts of ER and MIR for random processes.


%In this framework, considering the statistical dependencies between the present and past states of the systems analyzed, it is possible to investigate the overall degree of association between the two discrete-time random processes $X_i=\{X_{i,n}\}_{n \in \mathbb{Z}}$ ($i=1,\ldots,M$) and $Y=\{Y_n\}_{n \in \mathbb{Z}}$ through the \textit{mutual information rate} (MIR), a symmetric information-theoretic measure of dynamic coupling quantifying the information shared by the two processes per unit of time \cite{duncan1970calculation} and extending to random processes the MI computed for random variables. The MIR is defined as the limit 
%where $I(\cdot;\cdot)$ denotes the MI of two (possibly vector) random variables, and can be obtained from the present and past information of the processes as $I_{Y;X_i}=H_{Y}+H_{X_i}-H_{Y,X_i}$, where $H_{X_i}$ and $H_{Y}$ are the entropy rates (ER) of the processes $X_i$ and $Y$, while $H_{Y,X_i}$ is their joint entropy rate. Specifically, the ER of the generic process $Y$ quantifies the rate of generation of new information in the process, and is defined as the CE of the variable representing the present of the process given the variables sampling its past history, i.e., $H_{X_i}=\lim_{m \to \infty}\frac{1}{m} H(X_{i,n:n+m})=H(X_{i,n}|X^-_{i,n})$ \cite{cover1999elements, chicharro2011spectral}.



%\subsection{\label{subsect_PIRD_processes_PIRD}The redundancy lattice structure: decomposition of the mutual information rate}
Exploiting this analogy, we use the MIR as a building block for assessing and decomposing the dynamic information shared between the scalar target process $Y$ and the vector source process $X=\{X_1,\ldots,X_M\}$ of the analyzed network system. Specifically, we formalize a so-called \textit{Partial Information Rate Decomposition} (PIRD) which makes use of the same lattice structure of the PID \cite{williams2010nonnegative} to expand the MIR between target and sources as:
\begin{equation}
    I_{Y;X} = \sum_{\alpha \in \mathcal{A}} I^{\delta}_{Y;X_{\alpha}},
    \label{PIRD_atoms}
\end{equation}
where $I^{\delta}_{\cdot;\cdot}$ is a PI rate function defined for each atom $\alpha=\{\alpha_1,\ldots,\alpha_J\}$ of the lattice, and $X_{\alpha}$ denotes the $\alpha^{\mathrm{th}}$ set of subsets of source processes, with $X_{\alpha_j} \subseteq X$.
As happens for the PID, to solve the PIRD it is necessary to define a so-called \textit{redundancy rate} function, here denoted as $I^{\cap}_{\cdot;\cdot}$, which generalizes the MIR over the lattice and replaces the concept of redundancy function generalizing the MI. The redundancy rate of the $\alpha^{\mathrm{th}}$ atom is obtained summing the PI rate of the same atom to the PI rates of the atoms positioned downwards in the lattice:  
\begin{equation}
    I^{\cap}_{Y;X_{\alpha}}=\sum_{\beta \preceq \alpha} I^{\delta}_{Y;X_{\beta}};
    \label{redundancy_rate_function}
\end{equation} 
then, once the redundancy rate is known, the information rate associated to all atoms can be retrieved via  M\"{o}bius inversion of  (\ref{redundancy_rate_function}). In Sect. "Frequency-domain PIRD" we will elaborate on the definition of redundancy rate functions.

Importantly, since the PIRD is built over the same lattice backbone as the PID, several concepts and relations defined for the PID still hold in the dynamic case (see Fig. \ref{fig:fig_lattice}, with the shrewdness of considering information rates in place of the static information distributed over the lattice).
For instance, as the PIRD satisfies the same consistency equations valid for the PID, the redundancy function computed for an atom composed by one single source reduces to the MIR between the target and that source, i.e. $I^{\cap}_{Y;X_{\alpha}}=I_{Y;X_i}$ when $\alpha=\{i\}$.
Moreover, the rate of information shared between the $M$ source processes $X_1,\ldots,X_M$ taken together and the target process $Y$ can be expanded in analogy to (\ref{PID_contributions}) as the sum of $M+2$ contributions: 
\begin{equation}
    I_{Y;X} =  \sum_{m=1}^{M} \mathcal{U}_{Y;X_m} + \mathcal{R}_{Y;X} + \mathcal{S}_{Y;X};
    \label{PIRD_contributions}
\end{equation}
eq. (\ref{PIRD_contributions}) achieves a so-called \textit{coarse-grained} PIRD, whereby each of the $M$ terms $\mathcal{U}_{Y;X_m} = I_{Y;X_m} - \mathcal{R}_{Y;X}$ identifies the \textit{unique} rate of information produced by $Y$ that is shared exclusively with $X_m$ ($m=1,\ldots,M$), the term $\mathcal{R}_{Y;X}$ identifies the \textit{redundant} rate of information produced by $Y$ that is shared simultanously with all the source processes in $X$, and the term $\mathcal{S}_{Y;X}$ identifies the \textit{synergistic} rate of information produced by $Y$ that only arises from knowing all the sources $X_1,\ldots,X_M$. 
These coarse-grained information rates correspond for the case of two sources to the PI rate of the four atoms of the redundancy rate lattice (Fig. \ref{fig:fig_lattice}a, right), while they can be obtained for the case $M=3$ by summing the PI rates of the atoms with redundant, unique or synergistic character (respectively, magenta, gray and blue shades in Fig. \ref{fig:fig_lattice}b, right).

As a further remark, we stress that the PIRD formulated in (\ref{PIRD_atoms}) decomposes the information rates shared by multivariate processes accounting for their full dynamical structure and, as such, it generalizes previous attempts to apply the PID to random processes \cite{kay2022comparison,varley2023information,koccillari2023behavioural,luppi2024information}. This aspect becomes apparent considering that the MIR (\ref{MIR}) can be expanded as the sum of three terms, two related to the causal (time-lagged) information transfer along the two directions $X \rightarrow Y$ and $Y \rightarrow X$, and the latter related to the instantaneous (zero-lag) information shared between $X$ and $Y$ (see, e.g., \cite{bara2023comparison}):
\begin{equation}
   I_{X;Y}=T_{X\rightarrow Y}+T_{Y\rightarrow X}+I_{X \cdot Y},
   \label{MIR_dec}
\end{equation}
where $T_{X\rightarrow Y}=I(Y_n;X_{<n}|Y_{<n})$ and $T_{Y\rightarrow X}=I(X_n;Y_{<n}|X_{<n})$ are the transfer entropies measuring causal information transfer, and $I_{X \cdot Y}=I(X_n;Y_n|X_{<n},Y_{<n})$ is the instantaneous information shared between the processes.
Now, if we consider the case in which the overall multivariate process $Z=\{X,Y\}$ is memoryless, meaning that its whole temporal statistical structure is absent, the two TEs vanish and the instantaneous term becomes the information shared instantaneously between $X$ and $Y$, showing how the PIRD reduces to a PID applied to the static MI $I(X_n;Y_n)$. If, on the other hand, we consider the case of strictly causal processes with exclusive (unidirectional) information transfer from the sources to the target, both the instantaneous term and the TE from $Y$ to $X$ vanish, and the PIRD reduces to a PID applied to the TE $T_{X\rightarrow Y}$. These properties will be illustrated in simulated examples in Sect. "Theoretical Examples". 




% Remarkably, as for the PID, the formulations in (\ref{PIRD_contributions}), (\ref{delta_PIRD}) coincide when $M=2$, while the IID overestimates the term $I_{Y;X}$ when $M=3$ (double-counting redundancy) \cite{rosas2020reconciling}. 
%When $M=2$ source processes are considered, the PIRD atoms uniquely identify the redundant, unique and synergistic information rates (respectively, $\mathcal{R}_{Y;X}=I^{\delta}_{Y;X_{\{1\}\{2\}}}$, $\mathcal{U}_{Y;X_1}=I^{\delta}_{Y;X_{\{1\}}}$, $\mathcal{U}_{Y;X_2}=I^{\delta}_{Y;X_{\{2\}}}$, and $\mathcal{S}_{Y;X}=I^{\delta}_{Y;X_{\{12\}}}$), as depicted in Fig. \ref{fig:fig_lattice}a. However, a well-known drawback of the lattice representation is that the number of atoms grows super-exponentially with the number of sources \cite{williams2010nonnegative, rosas2020reconciling} (Fig. \ref{fig:fig_lattice}b, left). This issue has already been addressed, e.g., in \cite{rosas2020reconciling}, where the construction for $M=2$ was generalized to $M=3$ sources through the $k^{\mathrm{th}}$-order \textit{coarse-grained} PID. The approach provides a decomposition with a small number of atoms that scale gracefully with system size, and, remarkably, preserve the intuitive meaning that synergy, redundancy, and unique information have for $M=2$ sources \cite{rosas2020reconciling} (Fig. \ref{fig:fig_lattice}b, right). Specifically, considering $k=1$, the $1^{\mathrm{st}}$-order synergy $\mathcal{S}^{(1)}_{Y;X}$ corresponds to the information about the target that is provided by the whole $X$ but is not contained in any subset of sources when considered separately from the rest \cite{rosas2020reconciling}. For example, for $M = 2$ we obtain the standard synergy related to the atom $\{ \{12\} \}$ (Fig. \ref{fig:fig_lattice}a), while for $M = 3$ synergy is computed from the atoms $\{ \{12\}, \{13\}, \{23\}, \{12\}\{13\}, \{12\}\{23\}, \{13\}\{23\}, \{12\}\{13\}\{23\}, \{123\} \}$ (Fig. \ref{fig:fig_lattice}b). Similarly, the $1^{\mathrm{st}}$-order UIR $U^{(1)}_{Y;X_m}$ is the information that $X_m$ has access to and no other subset of parts has access to on its own (although bigger groups of other parts may) \cite{rosas2020reconciling}: for $M = 2$, we compute the UIRs from the atoms $\{ \{i\} \}$ (Fig. \ref{fig:fig_lattice}a), while for $M = 3$ we resort to, e.g., $\{ \{1\}, \{1\}\{23\} \}$ (Fig. \ref{fig:fig_lattice}b). Finally, the $1^{\mathrm{st}}$-order redundancy $\mathcal{R}^{(1)}_{Y;X}$ is the information held by at least two different groups of size $1$. Again, if $M = 2$ we recover the standard redundancy from the atom $\{ \{1\}\{2\} \}$ (Fig. \ref{fig:fig_lattice}a), while for $M = 3$ redundancy is obtained from $\{ \{1\}\{2\}, \{1\}\{3\}, \{2\}\{3\}, \{1\}\{2\}\{3\} \}$ (Fig. \ref{fig:fig_lattice}b).\\

\subsection{\label{subsect:SpectralPIRD}Frequency-domain PIRD}
To solve the PIRD identified by (\ref{PIRD_atoms}), it is necessary to define a redundancy rate function taking values over the lattice underlying the decomposition. In principle, the redundancy rate can be defined following any of the several approaches formulated for the PID, adapting it to the calculation of the MIR between random processes in place of the MI between random variables. In the PID literature, after the seminal work by Williams and Beer who first recognized that the concept of redundancy cannot be defined using classical information theory and proposed an axiomatic definition of redundant information \cite{williams2010nonnegative}, a number of alternative redundancy measures adopting or modifying the original set of axioms have been proposed \cite{bertschinger2014quantifying, barrett2015exploration, ince2017measuring, pakman2021estimating,makkeh2021introducing, ehrlich2024partial}.
A particularly useful approach is to derive redundancy measures working on the specific realizations of the random variables at hand, rather than on the variable themselves, and then compute redundancy via statistical expectation. This \textit{pointwise} approach has been followed to put forth PID methods that define "local" redundancy  measures computed for single realizations of the variables, from which a "global" redundancy is obtained taking the ensemble average over all possible realizations \cite{ince2017measuring,finn2018pointwise,makkeh2021introducing,gutknecht2021bits}. The same approach can be followed for the PIRD, e.g., working on the local version of the MIR (\ref{MIR}) to formalize the notion of pointwise redundancy rate; the redundancy rate among processes could be then retrieved by ensemble averaging, which for stationary processes corresponds to time-domain averaging. 

Here, we propose an approach that is conceptually similar, but is implemented through a pointwise representation in frequency rather than in time.
Specifically, we characterize the analyzed network of random processes in the frequency domain, considering the information provided about a particular oscillatory component of the target process $Y$ by the iso-frequency oscillatory components of the source processes collected in $X$. The idea is to perform the entire PIRD on the pointwise level for a particular frequency, i.e., to decompose the spectral (frequency-specific) MIR denoted as $i_{Y;X}(\omega)$, where $\omega=2\pi \frac{f}{f_s}$ is the normalized circular frequency ($\omega \in [-\pi, \pi]$, $f \in [-\frac{f_s}{2},\frac{f_s}{2}]$); the spectral MIR is identified from the expansion of the MIR in the frequency domain as:
\begin{equation}
    I_{Y;X}=\frac{1}{2\pi}\int_{-\pi}^{\pi}i_{Y;X}(\omega) \mathrm{d}\omega.
    \label{spectralMIR_integral}
\end{equation}
While different integral transforms (e.g., the wavelet transform) could in principle be used to expand the MIR, in the next section we will use a definition of spectral MIR which satisfies (\ref{spectralMIR_integral}) for Gaussian processes to formalize the PIRD in the frequency domain. 
Crucially, (\ref{spectralMIR_integral}) connects the time- and frequency-domain representations of information-theoretic quantities for random processes, and is exploited here to relate the PIRD (\ref{PIRD_atoms}) to its frequency-domain extension. Such an extension is denoted as \textit{spectral} PIRD and is formulated, for the oscillatory components of the network process assessed at the frequency $\omega$, expressing the spectral MIR as:
\begin{equation}
    i_{Y;X}(\omega) = \sum_{\alpha \in \mathcal{A}} i^{\delta}_{Y;X_{\alpha}}(\omega),
    \label{Spectral_PIRD_atoms}
\end{equation}
where $i^{\delta}_{\cdot;\cdot}(\omega)$ is the \textit{spectral partial information rate} function defined over a lattice specifically identified on the oscillations with frequency $\omega$, and $X_{\alpha}$ denotes the set of subsets of source processes indexed by the atom $\alpha$. 
As happens with PID, to solve (\ref{Spectral_PIRD_atoms}) we need to identify a \textit{spectral redundancy rate} through the imposition of a certain number of reasonable axioms or constraints on the spectral MIR, and then follow the PID formalism to derive the pointwise (spectral) atoms of information at the frequency $\omega$. If $i^{\cap}_{Y;X_{\alpha}}(\omega)$ denotes the spectral redundancy rate computed for the $\alpha^{\mathrm{th}}$ node of the spectral lattice defined at the frequency $\omega$, 
the corresponding spectral PI rate is computed recursively, in analogy to (\ref{delta_from_red}), as:
\begin{equation}
    i^{\delta}_{Y;X_{\alpha}}(\omega) = i^{\cap}_{Y;X_{\alpha}}(\omega) - \sum_{\beta \prec \alpha} i^{\delta}_{Y;X_{\beta}}(\omega).
    \label{atom_information_rate_function_PIRD}
\end{equation}
Furthermore, once the atoms are identified via (\ref{atom_information_rate_function_PIRD}), they can be properly grouped to obtain a coarse-grained representation of the spectral PIRD which takes the form:
\begin{equation}
    i_{Y;X}(\omega) =  \sum_{m=1}^{M} u_{Y;X_m}(\omega) + r_{Y;X}(\omega) + s_{Y;X}(\omega),
    \label{spectralPIRD_contributions}
\end{equation}
where the $M+2$ atoms reflect the $M$ unique contributions of each source process, as well as the redundant and synergistic contributions of all sources, to the rate of information produced by the target process at the specific frequency $\omega$; the coarse-grained spectral PIRD is obtained adopting the same criteria already described for the PID \cite{rosas2020reconciling}, leading to the coarse-grained atoms illustrated in Fig. \ref{fig:fig_lattice} (right panels).


Eq. (\ref{atom_information_rate_function_PIRD}) provides a solution for the spectral PIRD up to the definition of a proper spectral redundancy rate function.
Here, we propose to assess spectral redundancy following the minimum mutual information (MMI) principle \cite{barrett2015exploration} applied to the spectral MIR computed between the target and each source process at the frequency of interest. Specifically, 
we define the frequency-specific redundancy rate function of the atom $\alpha=\{\alpha_1,\ldots,\alpha_J \}$ of the spectral redundancy lattice as:
\begin{equation}
    i^{\cap}_{Y;X_{\alpha}}(\omega) = \min\limits_{j=1,\ldots,J} i_{Y;X_{\alpha_j}}(\omega),
    \label{spectral_redundancy_rate_function_MMI}
\end{equation}
where $J=|\alpha|$ indicates the cardinality of the atom; e.g., $J=1$ if $\alpha = \{1\}$ or $\alpha = \{12\}$, while $J=2$ if $\alpha = \{\{3\},\{12\}\}$. 
Note that, in contrast with the classical MMI formulation performed for random variables \cite{barrett2015exploration}, here the minimum MIR is searched at the pointwise (frequency-specific) level. While a pointwise implementation of the MMI criterion would be cumbersome if performed in the time domain, and it is indeed avoided by the existing pointwise approaches  \cite{ince2017measuring,finn2018pointwise,makkeh2021introducing} because the local MI or the local MIR can take negative values, in our case is favored by the non-negativity of the spectral MIR. The properties of the spectral redundancy rate (\ref{spectral_redundancy_rate_function_MMI}) computed for Gaussian processes will be discussed in the following subsection.

Importantly, from the perspective of information decomposition based on lattice structures \cite{williams2010nonnegative}, the spectral PIRD
(\ref{Spectral_PIRD_atoms},\ref{atom_information_rate_function_PIRD})
is conceptually equivalent to the PIRD (\ref{PIRD_atoms},\ref{redundancy_rate_function}) and to the PID 
(\ref{marginalMI},\ref{redundancy_function}); what changes in the three formulations is only the quantity to be decomposed, from the MI $I(T;S)$ for the case of random variables to the MIR $I_{Y;X}$ for the case of random processes, and to the spectral MIR $i_{Y;X}(\omega)$ for the case of oscillatory components of random processes. Since these quantities maintain the same meaning and properties (i.e., they are non-negative measures of shared information), the three formulations of PID, PIRD and spectral PIRD will lead to a unique solution for the atoms once a redundancy function (respectively, the redundancy among collections of random variables, the redundancy rate among collections of random processes, and the spectral redundancy rate among collections of oscillatory components of random processes) is fixed over a lattice structure like that in Fig. \ref{fig:fig_lattice}.
Therefore, moving from the PIRD defined on the pointwise level (frequency-specific) to that defined on the process-level (time domain) is straightforward exploiting 
%the pointwise PIRD (\ref{Spectral_PIRD_atoms},\ref{atom_information_rate_function_PIRD}) identified at each specific frequency is closely linked to the time-domain PIRD (\ref{PIRD_atoms},\ref{redundancy_rate_function}) by
spectral integration applied to the redundancy rate and PI rate functions:
\begin{align}
    I^{\cap}_{Y;X_{\alpha}}=\frac{1}{2\pi}\int_{-\pi}^{\pi} i^{\cap}_{Y;X_{\alpha}}(\omega) \mathrm{d}\omega, \label{spectralred_integral}\\
    I^{\delta}_{Y;X_{\alpha}}=\frac{1}{2\pi}\int_{-\pi}^{\pi}i^{\delta}_{Y;X_{\alpha}}(\omega) \mathrm{d}\omega;
    \label{spectralPI_integral}
\end{align}
the same spectral integration property holds to relate the unique, redundant and synergistic atoms of the coarse-grained spectral PIRD (\ref{spectralPIRD_contributions}) to their corresponding time-domain atoms of the PIRD (\ref{PIRD_contributions}).

Remarkably, solving the frequency-specific PIRD to obtain the PI rate $i^{\delta}_{Y;X_{\alpha}}(\omega)$ for each atom $\alpha$ via (\ref{atom_information_rate_function_PIRD}) and then integrating these contributions along the whole frequency axis according to (\ref{spectralPI_integral}) to get time domain values, is equivalent to integrating the spectral redundancy rate functions $i^{\cap}_{Y;X_{\alpha}}(\omega)$ in the range $[ -\pi, \pi]$ via (\ref{spectralred_integral}) and then applying the PIRD in the time domain to get the contributions $I^{\delta}_{Y;X_{\alpha}}$. This is guaranteed by the property of linearity of the definite integrals, and allows also to develop band-specific PIRD schemes whereby the decomposition of multivariate information rates is achieved in the time domain but is limited to oscillatory components whose frequencies are confined within a specific band of the spectrum. This band-limited decomposition, which is useful to highlight and decompose multivariate interactions with focus on specific oscillations with practical meaning, will be illustrated in the next sections in both simulated and applicative settings.

%Redundancy, synergy and unique information rates are then computed straightforwardly by summing the time domain atom information rate functions of the lattice as shown in Fig. \ref{fig:fig_lattice} (right plots of panels a,b). The property can be demonstrated easily by considering basic features of definite integrals, i.e., given the interval $\left[a,b\right]$, the integral of a sum of functions integrable on $\left[a,b\right]$ is equivalent to the sum of the definite integrals on $\left[a,b\right]$: $\int_{a}^{b}\big( f(x) + g(x) \big) = \int_{a}^{b} f(x) + \int_{a}^{b} g(x)$.\\

%\textcolor{blue}{...here, briefly on the properties (in the case of random variables (no temporal correlations) the spectral MIR is flat so that the spectral PIRD becomes the PIRD via the integration properties, and in turn the PIRD is the zero-lag PID; in the case of absence of instantaneous effects and target not sending info to sources, the spectral MIR becomes 1/2 the spectral GC, which integrated is the time-domain GC)... see also the comments here below}

%\textcolor{black}{qui modificare togliendo la parte OIR:}the PIRD where the redundancy rate function is defined based on the MMI PID reduces to the standard PID between the random variables sampling the involved processes at the same time $n$. Let us consider an exemplary 3-VAR system where $Y$ is the target and $\{X_1,X_2\}$ is the group of sources, and assume that only zero-lag interactions are present, i.e., interactions occurring at the same time $n$ between the variables $Y_n$, $X_{1,n}$ and $X_{2,n}$ ($n \in \mathbb{Z}$). Whether we apply the PID to the set $\{ Y,X_1,X_2\}$, redundancy can be computed following the MMI PID approach \cite{barrett2015exploration} as $\mathcal{R}(Y;X_1;X_2)=min\{ I(Y;X_1),I(Y;X_2)\}$, while the unique information terms are retrieved as in (\ref{UI_MI_equivalence}). Thus, synergy results from (\ref{PID_contributions}), i.e., $\mathcal{S}(Y;X_1;X_2)=I(Y;X_1;X_2)-U(Y;X_1)-U(Y;X_2)-\mathcal{R}(Y;X_1;X_2)$. If the processes can be intended as white noises correlated at lag zero, the spectral MIRs $i_{Y;X_1}(\omega), i_{Y;X_2}(\omega)$ display flat patterns acquiring the same identical value at each frequency $\omega$. Then, assuming that, e.g., $i_{Y;X_1}(\omega) < i_{Y;X_2}(\omega)$, then $i^{min}_{Y;X_1;X_2}(\omega)=i_{Y;X_1}(\omega)$, $\mathcal{R}_{Y;X_1;X_2}=I_{Y;X_1}$ and $\mathcal{U}_{Y;X_1}=0$. It follows that the PIRD, when exploiting the definition of spectral redundancy based on the concept of MMI PID and shown in (\ref{spectral_redundancy_rate_function_MMI}), reduces to the PID in absence of temporal correlations.

%Besides the spectral MIRs, the spectral OIR $\nu_{Y;X_1;X_2}(\omega)$ has a flat profile, too. If $\nu_{Y;X_1;X_2}(\omega) > 0$, time domain redundancy is thus computed from (\ref{spectral_redundancy_rate_function_OI}) yielding $\mathcal{R}_{Y;X_1;X_2} = \Delta_{Y;X_1;X_2} = I_{Y;X_1} + I_{Y;X_2} - I_{Y;X_1;X_2}$, where $\Delta_{Y;X_1;X_2} = \frac{1}{2\pi} \int_{-\pi}^{\pi} \nu_{Y;X_1;X_2}(\omega) \mathrm{d}\omega$. It follows that $\mathcal{U}_{Y;X_1} = I_{Y;X_1}-\mathcal{R}_{Y;X_1;X_2} = I_{Y;X_1;X_2}-I_{Y;X_2}$, and $\mathcal{U}_{Y;X_2} = I_{Y;X_2}-\mathcal{R}_{Y;X_1;X_2} = I_{Y;X_1;X_2}-I_{Y;X_1}$; synergy is then computed as $\mathcal{S}_{Y;X_1;X_2} = I_{Y;X_1;X_2}-\mathcal{U}_{Y;X_1}-\mathcal{U}_{Y;X_2}-\mathcal{R}_{Y;X_1;X_2} = 0$. Conversely, if $\nu_{Y;X_1;X_2}(\omega) \leq 0$, then $\mathcal{R}_{Y;X_1;X_2}=0$, $\mathcal{U}_{Y;X_1}=I_{Y;X_1}$, $\mathcal{U}_{Y;X_2}=I_{Y;X_2}$ and $\mathcal{S}_{Y;X_1;X_2}=I_{Y;X_1;X_2}-I_{Y;X_1}-I_{Y;X_2}=-\Delta_{Y;X_1;X_2}$. It follows that the PIRD, when exploiting the definition of spectral redundancy based on the concept of common change in surprisal shared between variables at the local level and shown in (\ref{spectral_redundancy_rate_function_OI}), cannot reduce to the PID in absence of temporal correlations.

\subsection{\label{subsect_PIRD_processes_linearformulation}Formulation for Gaussian processes}
In the linear signal processing framework, the analyzed set of stochastic processes $Z=\{Y,X_1,\ldots,X_M\}$ can be described in terms of its power spectral density (PSD) matrix expressed as
\begin{equation}
    \textbf{P}_Z(\omega)=\begin{bmatrix}
    P_{Y}(\omega) & P_{YX_1}(\omega) & \cdots & P_{YX_M}(\omega)\\
    P_{X_1Y}(\omega) & P_{X_1}(\omega) & \cdots & P_{X_1X_M}(\omega)\\
    \vdots & \vdots & \ddots & \vdots\\
    P_{X_MY}(\omega) & P_{X_MX_1}(\omega) & \cdots & P_{X_M}(\omega)\\    
    \end{bmatrix},
    \label{PSDmatrixS}
\end{equation}
which contains the individual PSDs of the processes $\{ Y, X_1, \ldots, X_M \}$ as diagonal elements and the cross-PSDs between them as off-diagonal elements. Individual and cross-PSDs are defined as the Fourier Transform (FT) of the covariance functions of the processes: i.e., $P_{Y}(\omega)=\mathfrak{F}\{R_{Y}(k)\}$\textcolor{black}{, $P_{X_iX_j}(\omega)=\mathfrak{F}\{R_{X_iX_j}(k)\}$} and $P_{YX_i}(\omega)=\mathfrak{F}\{R_{YX_i}(k)\}$, where $R_{Y}(k)=\mathbb{E}[Y_{n}Y_{n-k}]$\textcolor{black}{, $R_{X_iX_j}(k)=\mathbb{E}[X_{i,n}X_{j,n-k}]$} and $R_{YX_i}(k)=\mathbb{E}[Y_{n}X_{i,n-k}]$ are the auto-correlation function of $Y$\textcolor{black}{, the cross-correlation function between $X_i$ and $X_j$} and the cross-correlation function between $Y$ and $X_i$  ($k$ is the correlation delay)\textcolor{black}{, respectively}; correlations are equivalent to covariances for zero-mean processes.

The PSD matrix is the central element for the implementation of the spectral PIRD. In fact, it is well-known that, for jointly Gaussian processes $X$ and $Y$, the MIR admits the following spectral expansion 
\cite{gelfand1959calculation, chicharro2011spectral}:
\begin{equation}
    I_{X;Y}=\frac{1}{4\pi}\int_{-\pi}^{\pi} \textcolor{black}{\log}\frac{|\textbf{P}_{X}(\omega)| P_{Y}(\omega)}{|\textbf{P}_{Z}(\omega)|}\mathrm{d}\omega,
    \label{spectralMIR_expansion}
\end{equation}
where \textcolor{black}{$\textbf{P}_{X}(\omega)$} is the PSD matrix pruned from the first row and column, and $|\cdot|$ denotes matrix determinant. The spectral MIR decomposed by PIRD then results immediately comparing (\ref{spectralMIR_expansion}) with (\ref{spectralMIR_integral}):
\begin{equation}
    i_{Y;X}(\omega)=\frac{1}{2}\log \frac{ |\textbf{P}_{X}(\omega)| P_{Y}(\omega)}{|\textbf{P}_{Z}(\omega)|}.
    \label{spectralMIR}
\end{equation}
Moreover, for any given atom $\alpha=\{\alpha_1,\ldots,\alpha_J \}$ of the spectral redundancy lattice, the application of (\ref{spectralMIR}) is particularized to the element $\alpha_j$ as follows:
\begin{equation}
    i_{Y;X_{\alpha_j}}(\omega)=\frac{1}{2}\log \frac{|\textbf{P}_{X_{\alpha_j}}(\omega)| P_{Y}(\omega) }{|\textbf{P}_{[YX_{\alpha_j}]}(\omega)|},
    \label{spectralMIR_alphaj}
\end{equation}
where
\begin{equation}
    \textbf{P}_{[YX_{\alpha_j}]}(\omega)=\begin{bmatrix}
    P_{Y}(\omega) & \textbf{P}_{YX_{\alpha_j}}(\omega)\\
    \textbf{P}_{X_{\alpha_j}Y}(\omega) & \textbf{P}_{X_{\alpha_j}}(\omega)\\
    \end{bmatrix};
    \label{PSDmatrixYXalphaj}
\end{equation}
the computation of (\ref{spectralMIR_alphaj}) for each $j=1,\ldots, J$ yields the spectral MIR
terms to be used in (\ref{spectral_redundancy_rate_function_MMI}) for the computation of the spectral redundancy function.



Remarkably, the spectral redundancy rate defined by (\ref{spectral_redundancy_rate_function_MMI},\ref{spectralMIR_alphaj}) is a proper redundancy function as it satisfies the axioms originally proposed by Williams and Beer, i.e. \textit{symmetry}: 
$i^{\cap}_{Y;X_{\{\alpha_1,\ldots,\alpha_J\}}}(\omega)$ is symmetric w.r.t. the $\alpha_j, j=1,\ldots,J$; \textit{self-redundancy}: $i^{\cap}_{Y;X_{\alpha}}(\omega)=i_{Y;X_{\alpha}}(\omega)$ when $|\alpha|=1$; \textit{monotonicity}: $i^{\cap}_{Y;X_{\{\alpha_1,\ldots,\alpha_{J-1},\alpha_J\}}}(\omega) \leq i^{\cap}_{Y;X_{\{\alpha_1,\ldots,\alpha_{J-1}\}}}(\omega)$, with equality if $\alpha_{J-1}\subseteq \alpha_J$.
These properties are derived straightforwardly from the minimum MIR definition  (\ref{spectral_redundancy_rate_function_MMI}) of the spectral redundancy rate, and from the properties of non-negativity and monotonicity of the spectral MIR (\ref{spectralMIR_alphaj}) \cite{nedungadi2011block}.


We conclude discussing the practical computation of the PSD matrix (\ref{PSDmatrixS}) whose elements are exploited to estimate all the MIR terms entering the PIRD.
There are several standard methods for computing the PSD matrix of multivariate time-series, including the averaged periodogram, multi taper techniques, and wavelet approaches \cite{von2020nonparametric}. Here, we use the computation that induces a linear parametric representation of the observed dynamics. Specifically, the analyzed stochastic process $Z=\{Y,X_1,\ldots,X_M\}$ is described as a vector autoregressive (VAR) process \cite{lutkepohl2005new}:
\begin{equation}
   Z_n = \sum_{k=1}^{p}\mathbf{A}_k Z_{n-k} + U_n
   \label{VAR_model},
\end{equation}
where $p$ is the model order, defining the maximum lag used to quantify interactions, $Z_n=[Y_{n} X_{1,n} \ldots X_{M,n}]^\intercal$ is a $M+1$-dimensional vector collecting the present state of all processes, $\mathbf{A}_k$ is the $(M+1) \times (M+1)$ matrix of the model coefficients relating the present with the past of the processes at lag $k$, and $U_n=[U_{Y,n} U_{X_1,n} \ldots U_{X_M,n}]^\intercal$ is a vector of $M+1$ zero-mean white innovations with $(M+1) \times (M+1)$ positive definite covariance matrix $\mathbf{\Sigma}_U=\mathbb{E}[U_n U_n^\intercal]$.
To analyze the VAR model (\ref{VAR_model}) in the frequency domain, the FT of (\ref{VAR_model}) is taken to derive
\begin{equation}
   Z(\omega) = [\mathbf{I} - \sum_{k=1}^{p}\mathbf{A}_k e^{-\mathbf{j} \omega k}]^{-1} U(\omega)=\mathbf{H}(\omega)U(\omega)
   \label{VAR_model_freq},
\end{equation}
where $Z(\omega)$ and $U(\omega)$ are the FTs of $Z_n$ and $U_n$, $\mathbf{j}=\sqrt{-1}$ and $\mathbf{I}$ is the $(M+1)$-dimensional identity matrix. The $(M+1) \times (M+1)$ matrix $\mathbf{H}(\omega)$ contains the transfer functions relating the FTs of the innovation processes in $U$ to the FTs of the processes in $Z$. Finally, the transfer matrix is exploited, together with the covariance of the VAR innovations, to derive the PSD matrix through spectral factorization \cite{wilson1972factorization}:
\begin{equation}
    \textbf{P}_Z(\omega)=\mathbf{H}(\omega)\mathbf{\Sigma}_{U}\mathbf{H}^*(\omega)
   \label{SpectralFactorization},
\end{equation}
where $^*$ stands for conjugate transpose. 

The practical computation of the PSD starts from the  parameters $\textbf{A}_k$ and $\mathbf{\Sigma}_U$ of the VAR model (\ref{VAR_model}), which can be easily estimated from realizations of the process $X$ available in the form of multivariate time series. While several procedures exist to perform VAR model identification \cite{barnett2014mvgc,antonacci2020information}, here we use a tool \cite{faes2012measuring} based on classical least squares estimation \cite{lutkepohl2005new} implemented optimizing the model order $p$ through the Akaike information criterion \cite{akaike1974new}; 
remarkably, high model orders denote a non-parsimonious representation of the multivariate time series at hand, which however may indicate that the linear representation induced by the VAR model can capture at least in part possible nonlinearities in the underlying processes, provided that the fitted VAR model is stable \cite{barnett2015granger}. 

In closing this section we note that alternative approaches for the computation of multivariate information measures, which are not dealt with here, can rely on the model-free computation of entropy measures, which guarantees high flexibility in the representation of the dynamics but also exposes to issues related to the data-efficient estimation of entropy measures \cite{runge2012escaping,vicente2011transfer,xiong2017entropy,ricci2021estimating}.
In the following sections, keeping the linear model-based approach, we will first illustrate the PIRD framework in theoretical VAR processes where the parameters are set to simulate controlled behaviors, and then apply the framework on real multivariate time series measured in the context of network physiology.









\section{\label{sect:simulations}Theoretical Examples}

In this section, we characterize the behavior of the measures proposed to dissect the multivariate information shared by random processes using linear VAR models; this allows for a theoretical analysis whereby the exact profiles of the measures are computed starting from the true values imposed for the model parameters. Such parameters are varied in different simulation settings to study the time-domain and spectral behavior of the PIRD measures and to compare them with different implementations of the standard PID applied to random processes.


\subsection{\label{subsect:simulations_2sources} Effects of temporal correlations}
First, we characterize the PIRD in both the time and frequency domains in simulated dynamic networks involving three processes ($Z=\{Y,X_1,X_2 \}$), comparing the time-domain measures with the standard PID decomposing instantaneous interactions among the processes. To do this we use a three-variate VAR process simulated with $f_s=1$, in which different regimes of dynamic interaction are set by varying the parameters related to zero-lag effects, lagged interactions, and autonomous dynamics. Specifically, the 3-VAR process is defined as:
\begin{equation}
\begin{aligned}
&Y_n = c X_{1,n-1} + c X_{2,n-2} + U_{Y,n}\\
&X_{1,n} = \sum_{k=1}^{2} a_{1,k} X_{1,n-k} + U_{X_1,n} \\
&X_{2,n} = \sum_{k=1}^{4} a_{2,k} X_{2,n-k} + U_{X_2,n}
\label{th_sim_3VAR_equations}
\end{aligned}
\end{equation}
where $U_{Y}$, $U_{X_1}$ and $U_{X_2}$ are Gaussian white noises with zero mean and unit variance. The covariance matrix of the residuals,
\begin{equation*}
    \mathbf{\Sigma}_U = \begin{pmatrix}
  1 & \sigma^2_{U_{YX_1}} & \sigma^2_{U_{YX_2}}\\
  \sigma^2_{U_{X_1Y}} & 1 & \sigma^2_{U_{X_1X_2}}\\
  \sigma^2_{U_{X_2Y}} & \sigma^2_{U_{X_2X_1}} & 1
\end{pmatrix},
\end{equation*}
is built in such a way to generate zero-lag cross-correlations among the processes modulated inversely by the parameter $c$, imposing $\sigma^2_{U_{YX_1}} = \sigma^2_{U_{YX_2}} = \sigma^2_{U_{X_1X_2}} = 0.8 - c$. The autonomous oscillations in the two source processes $X_1$ and $X_2$ are obtained placing pairs of complex-conjugate poles, with modulus $\rho$ and phase $2\pi{f}$, in the complex plane representation of each process; the AR coefficients resulting from this setting at lags $1,2$ are $a_1=2\rho \cos(2\pi f)$ and $a_2=-\rho^2$ \cite{faes2015information}. Here, we place a pair of poles for the process $X_1$, setting $\rho=c, f=0.1$ Hz so that the strength of the autonomous dynamics determined by the coefficients $a_{1,1}$ and $a_{1,2}$ depends on the parameter $c$; similarly, we place two pairs of poles for $X_2$, setting $\rho_{1}=c, f_{1}=0.1$ Hz and $\rho_{2}=1.125c, f_{2}=0.3$ Hz so that the strength of the autonomous dynamics of $X_2$ determined by the coefficients $a_{2,k}, k=1,\ldots,4$, depends on the parameter $c$. Moreover, causal interactions are set from $X_1$ to $Y$ at lag $k=1$ and from $X_2$ to $Y$ at lag $k=2$, with strength modulated by the parameter $c$. 
The parameter $c$ is varied in the range $\left[ 0 - 0.8 \right]$, thus allowing (i) progressive strengthening of the autonomous dynamics in the processes $X_1$, $X_2$ and of the causal interaction from $X_1$ and $X_2$ to $Y$, as well as (ii) progressive weakening of the zero-lag interactions among the three processes. The simulation design is shown in Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}a.

\begin{figure}
    \centering
\includegraphics{Figures_Paper/th_sim_PID_vs_PIRD_redundancy_final_version.pdf}
    \caption{Example of how the spectral redundancy rate function $i^{\cap}_{Y;X_{\{1\}\{2\}}}(f)=i^{min}_{Y;X_1;X_2}(f)$ is computed as the minimum of the interaction between each individual source and the target at the specific frequency $f$, $\min\limits_{i=1,2} i_{Y;X_{i}}(f)$. The spectral PIRD allows to overcome the drawback of the MMI-PID which sets to zero one of the two unique contributions, as well as to delve into the spectral content of the investigated processes and their interactions.}
    \label{fig:thsim_PID_vs_PIRD_redundancy}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Figures_Paper/th_sim_PID_vs_PIRD_final_version.pdf}
    \caption{\textbf{The presence of temporal correlations has a profound impact on the multivariate information shared at lag zero by multiple random processes.} \textbf{(a)} Simulation design, where $Y$ is the target process and $\{X_1,X_2\}$ is the group of sources; time-lagged interactions (solid black arrows) and zero-lag interactions (dashed gray lines) are set respectively to increase and decrease with the parameter $c \in [0-0.8]$.
    %Zero-lag effects (dashed gray lines) are set among the processes as off-diagonal elements of the covariance matrix $\mathbf{\Sigma}_U$ of the VAR residuals, and decrease with the parameter $c$. Time-lagged interactions from $X_1, X_2$ to $Y$ (continuous black lines) are set in such a way to vary in the range $\left[ 0 - 0.8\right]$ for increasing values of the parameter $c$. Autodependencies of the two sources $X_1$ and $X_2$ (continuous black lines) are set such that autonomous oscillations with strength increasing with the parameter $c$ emerge at specific frequencies (i.e., $0.1$ Hz for $X_1$, $0.1$ Hz and $0.3$ Hz for $X_2$).
    \textbf{(b-e)} Spectral profiles of the joint MIR between $Y$ and $\{X_1,X_2\}$, of the individual MIR between $Y$ and $X_1$ and between $Y$ and $X_2$, and of the redundancy rate $i^{min}_{Y;X_1;X_2}(f)$, obtained at varying the parameter $c$ from zero (continuous black lines) to $0.8$ (continuous pink lines).
    \textbf{(f)} Time-domain behavior of the total information shared between the target and the two sources ($I$), of the unique information shared between the target and each individual source ($\mathcal{U}_1$, $\mathcal{U}_2$), and of the redundant and synergistic information provided by the two sources to the target ($\mathcal{R}$, $\mathcal{S}$), measured using the the zero-lag PID (continuous black lines) and the PIRD (dashed gray lines) as a function of the parameter $c$.}
    \label{fig:thsim_PID_vs_PIRD_spectral_functions}
\end{figure*}

The PIRD was performed computing the spectral redundancy rate according to (\ref{spectral_redundancy_rate_function_MMI}), with the spectral MIR function computed as in (\ref{spectralMIR_alphaj}) after deriving the PSD matrix from the VAR parameters, and then obtaining the spectral PI rate through (\ref{atom_information_rate_function_PIRD}) which $-$in this case with $M=2$ sources$-$ yields immediately the coarse-grained terms in (\ref{spectralPIRD_contributions}); all these terms were then integrated over the full frequency axis to obtain the unique $\mathcal{U}_{Y;X_1}$, $\mathcal{U}_{Y;X_2}$, redundant $\mathcal{R}_{Y;X_1;X_2}$ and synergistic $\mathcal{S}_{Y;X_1;X_2}$ information rates in the time domain.
An example of how spectral redundancy is computed as the minimum MIR at each frequency $f$ is shown in Fig. \ref{fig:thsim_PID_vs_PIRD_redundancy} for one specific VAR configuration ($c$ = 0.5). Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions} reports the spectral MIR and redundancy rate functions, as well as the time-domain values of the PIRD terms, investigated at varying the simulation parameter $c$. In Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}f the time-domain PIRD is compared with the instantaneous PID which decomposes the MI $I(Y_n;X_{1,n},X_{2,n})$; the latter was computed following the formulation sketched in Subsect. "Linear parametric formulation" with $T=Y_n$ and $S_i=X_{i,n}$, after deriving the zero-lag covariance of the processes via solution of the Yule Walker equations of the VAR process (the procedure is illustrated in detail in \cite{mijatovic2024network}).


%The elements of the PID, i.e., the MI and the unique information shared between each individual source and the target ($I(Y;X_1)$, $I(Y;X_2)$, $\mathcal{U}(Y;X_1)$, $\mathcal{U}(Y;X_2)$), as well as the MI shared between the group of two sources and the target $I(Y;X_1;X_2)$, the redundant $\mathcal{R}(Y;X_1;X_2)$ and synergistic contributions $\mathcal{S}(Y;X_1;X_2)$, were computed exploiting the MI formulation valid for Gaussian variables in (\ref{MI_linear}). Their time domain behaviors are shown in Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}f as a function of the parameter $c$ (continuous black lines).
%Moreover, the spectral MIRs shared between each individual source and the target ($i_{Y;X_1}(f)$, $i_{Y;X_2}(f)$), as well as the total MIR shared between the target and the group of two sources ($i_{Y;X_1;X_2}(f)$), were computed for each value of the parameter $c$. Spectral profiles of these measures are shown in panels b-d of Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions} as a function of $c$. 

%Results suggest that the presence of temporal correlations has a profound impact on the multivariate information shared at lag zero by multiple random processes. 
The results suggest that the rate of dynamic information shared by multivariate processes is deeply affected by the balance between instantaneous and time-lagged interactions.
When the three processes interact only at lag zero and do not exhibit self-dependencies ($c=0$), the spectral profiles of the MIR and redundancy rate measures are flat (Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}b-e, black lines) and the time-domain PIRD and zero-lg PID measures coincide (Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}f with $c=0$). Increasing the parameter $c$ determines modifications of the spectral profiles of the MIR functions (Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}b-d), due to the emergence of spectral peaks around $0.1$ and $0.3$ Hz induced by the self-dependencies rising in $X_1$ and $X_2$, as well as of causal interactions along the directions $X_1 \rightarrow Y$ and $X_2 \rightarrow Y$. As a result, the profile of the spectral redundancy rate is also modulated by $c$ (Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}e), as are the time-domain MIR and PIRD terms (Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}f, dashed lines).
The modification of the VAR parameters affects also the zero-lag PID (Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}f, solid lines), whose information atoms were modified in a substantially different way than those of the PIRD.
In fact, both redundant and synergistic contributions computed via PID ($\mathcal{R}$ and $\mathcal{S}$, solid lines in Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}f)  decrease towards zero at increasing $c$, while we rather expect an increase of synergy due to the emerging common child structure of the simulated system (Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}a). This effect is well evidenced by the PIRD, whose synergistic contribution increases with $c$ becoming far higher than the redundant contribution ($\mathcal{R}$ and $\mathcal{S}$, dashed lines in Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}f).
Moreover, the PIRD detects a rise of both the unique information rates relevant to the two sources ($\mathcal{U}_1$ and $\mathcal{U}_2$, dashed lines in Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}f), which is expected due to the emergence of causal interactions along the directions $X_1 \rightarrow Y$ and $X_2 \rightarrow Y$ with strength modulated by $c$.
However, the same is not true if the unique information is measured by the zero-lag PID exploiting the time domain definition of redundancy (\ref{redundancy_MMI}) based on the MMI-PID, confirming a known limitation of such PID which always forces to zero the unique information of the source sharing the lowest information with the target (in this case $\mathcal{U}_2$, dashed line in Fig. \ref{fig:thsim_PID_vs_PIRD_spectral_functions}f).

\subsection{\label{subsect:simulations_PID_transfer_entropies}Effects of changes in the network topology}

Here, we investigate the time-domain behavior of the PIRD measures obtained decomposing the MIR between a target process $Y$ and two source processes $X_1,X_2$, compared with the same measures obtained from a PID applied to the joint TE from the two sources to the target, $T_{X_1,X_2 \rightarrow Y}$.
To this aim, we use a three-variate VAR process simulated with $f_s=1$ where the parameters related to lagged interactions from the sources to the target and vice versa are varied, in order to obtain different configurations of causal interactions.
Specifically, the 3-VAR process is defined as:
\begin{equation}
\begin{aligned}
&Y_n = (0.8-c) X_{1,n-1} + (1.6-2c) X_{2,n-1} + U_{Y,n}\\
&X_{1,n} = c Y_{n-1} + U_{X_1,n} \\
&X_{2,n} = 2c Y_{n-1} + U_{X_2,n}
\label{th_sim_TE_3VAR_equations}
\end{aligned}
\end{equation}
where $U_{Y}$, $U_{X_1}$ and $U_{X_2}$ are uncorrelated Gaussian white noises with zero mean and unit variance; the uncorrelation between the inputs ($\mathbf{\Sigma}_U=\mathbf{I}$) denotes absence of instantaneous correlations.
On the other hand, causal interactions are set at lag $k=1$, from $Y$ to both $X_1$ and $X_2$ with strength modulated directly by the parameter $c$, and from $X_1$ and $X_2$ to $Y$ with strength modulated inversely by the same parameter $c$. This setting allows for a progressive strengthening of the causal interactions directed from the target to the sources, and a progressive weakening of the causal interactions directed from the sources to the target, as $c$ increases in the range $[0-0.8]$. The simulation design is shown in Fig. \ref{fig:th_sim_PID_TE_vs_PIRD}a.

The PIRD was performed as in the first simulation, using the approach presented in Sect. "Formulation for Gaussian processes". Remarkably, since in this simulation of processes without self-dependencies the spectral MIR functions are flat, the time- and frequency-domain PIRD are equivalent and reduce to applying the MMI criterion to the MIR.
%Remarkably, the spectral PIRD reduces to integrating the spectral redundancy rate function in the time domain and then computing the PIRD atoms by applying the MMI-based formulation of the obtained redundancy value, since the considered processes do not have frequency-specific oscillations and their spectra are flat.
As regards the PID applied to the joint TE $I(Y_n;X_{1,<n},X_{2,<n}|Y_{<n})$, it was computed exploiting the formalism linking information-theoretic measures with Granger causality measures derived from linear parametric regression models developed for joint Gaussian processes \cite{barnett2009granger,barrett2010, faes2016information}. Specifically, the MMI criterion \cite{barrett2015exploration} was used to derive redundancy as the minimum information transferred from each individual source to the target; the latter was measured as half the value of the Granger causality (GC) estimated from the VAR processes \cite{barnett2009granger}, and all GC terms were computed from the VAR parameters using sub-models \cite{faes2016information}. The resulting redundant TE ($\mathcal{R}$) was then used to derive the unique ($\mathcal{U}_1$, $\mathcal{U}_2$) and synergistic ($\mathcal{S}$) amounts of information transferred from $X$ to $Y$ according to PID rules.

%Regarding PIRD, the MMI-based formulation of the spectral redundancy rate function $i^{\cap}_{Y;X_1;X_2}(f)$ in (\ref{spectral_redundancy_rate_function_MMI}) was exploited to retrieve unique $\mathcal{U}_1$, $\mathcal{U}_2$, redundant $\mathcal{R}$ and synergistic $\mathcal{S}$ information rates in the time domain, whose behavior at varying $c_1, c_2$ is shown in Fig. \ref{fig:th_sim_PID_TE_vs_PIRD}b-f (dashed gray lines).
The time domain behaviors of the PID terms are shown in Fig. \ref{fig:th_sim_PID_TE_vs_PIRD}b-f as a function of the parameters $c_1, c_2$ (continuous black lines).

The results evidence how the transition from the condition in which the target acts exclusively as a sink of information ($c=0$) to that in which it acts exclusively as a source of information ($c=0.8$) is thoroughly reflected by the PIRD but cannot be fully captured by the PID applied to the joint TE. Indeed, the latter cannot take time-lagged interactions directed from the target to the sources into account, and thus in the simulation where $c$ is increased it reflects only the decrease of the joint information transferred along the direction $X \rightarrow Y$ ($J$, Fig. \ref{fig:th_sim_PID_TE_vs_PIRD}d, solid line) and the consequent drop to zero of all PID terms ($\mathcal{U}_1$, $\mathcal{U}_2$, $\mathcal{R}$, $\mathcal{S}$, Fig. \ref{fig:th_sim_PID_TE_vs_PIRD}b,c,e,f, solid lines).
On the other hand, the PIRD is applied to the MIR shared between the sources and the target, a measure which is not directional as the TE and is thus sensible to the overall information flowing among the processes of the considered dynamic system. This is reflected by values of the MIR between $Y$ and $X_1,X_2$ which remain high as $c$ increases ($J$, Fig. \ref{fig:th_sim_PID_TE_vs_PIRD}d, dashed line), as also happens for the unique information rate of $X_2$ ($\mathcal{U}_2$, Fig. \ref{fig:th_sim_PID_TE_vs_PIRD}c, dashed line), and by values of redundancy rate and synergy rate shifting from the prevalence of synergy when $c=0$ to the prevalence of redundancy when $c=0.8$. The latter behavior reflects the modifications of the topological structure induced in the dynamic network, with links undergoing a transition from a common child configuration to a common drive configuration. 

\begin{figure}
    \centering
\includegraphics[width=\columnwidth]{Figures_Paper/th_sim_PID_TE_vs_PIRD_final_version.pdf}
    \caption{\textbf{Changes in the network topology have a profound impact on the multivariate information shared by multiple random processes.}    
    \textbf{a)} Simulation design, where $Y$ is the target process and $\{X_1,X_2\}$ is the group of sources; time-lagged interactions (solid black arrows) are set varying the parameter $c \in [0-0.8]$ to simulate a transition from purely unidirectional interactions directed from  $X_1, X_2$ to $Y$ when $c=0$ to purely unidirectional interactions directed from $Y$ to $X_1, X_2$ when $c=0.8$.
    (\textbf{b-e}) Profiles of the time domain MIR shared between the $Y$ and $\{X_1,X_2\}$ and the joint TE from $\{X_1,X_2\}$ to $Y$ (measuring overall interactions, here indicated as $J$) and of the unique ($\mathcal{U}_1$, $\mathcal{U}_2$), redundant ($\mathcal{R}$) and synergistic ($\mathcal{S}$) components of their decomposition measured using the PID applied to the TEs (continuous black lines) and the PIRD (dashed gray lines) as a function of the parameter $c$.}
    \label{fig:th_sim_PID_TE_vs_PIRD}
\end{figure}

\subsection{\label{subsect:simulations_3sources} Frequency-specific coarse-grained PIRD}

\begin{figure*}
    \centering
    \includegraphics{Figures_Paper/thsim_mix_new.pdf}
    \caption{\textbf{Coexistence of redundant and synergistic characters of interactions in different spectral bands elicited by frequency-specific PIRD.}
    \textbf{(a)} Network structure, with $Y$ receiving from $X_1$, oscillating at 0.3 Hz, and $X_3$, oscillating at 0.1 Hz, and $X_1$ sending to $X_2$, oscillating at 0.3 Hz; all coupling coefficients are set to 1.
    \textbf{(b)} Spectral profiles of the pairwise MIR measures computed between the target $Y$ and each source $X_i$, $i=1,2,3$. % $i_{Y;X_1}(f)$ (black continuous line), $i_{Y;X_2}(f)$ (gray continuous line), and $i_{Y;X_3}(f)$ (pink continuous line). \textbf{(c)} Mutual ($I_{Y;X_1}$, $I_{Y;X_2}$, $I_{Y;X_3}$, black, gray and pink bars)
    \textbf{(c,d)} Time-domain values of the pairwise MIR and of the unique ($\mathcal{U}_{Y;X_i}$, $i=1,2,3$), redundant ($\mathcal{R}_{Y;X}$) and synergistic ($\mathcal{S}_{Y;X}$) PIRD components
    %unique ($\mathcal{U}_{Y;X_1}$, $\mathcal{U}_{Y;X_2}$, $\mathcal{U}_{Y;X_3}$, black, gray and pink bars), redundant ($\mathcal{R}_{Y;X}$, magenta bars) and synergistic ($\mathcal{S}_{Y;X}$, blue bars) information rates
    integrated along the whole frequency axis ($\left[ 0 - f_s/2\right]$) (left bars), and withing the bands $B_1=\left[ 0.04 - 0.15\right]$ Hz (middle bars), $B_2=\left[ 0.15 - 0.4\right]$ Hz (right bars);
    The balance $\Delta_{Y;X}=\mathcal{R}_{Y;X}-\mathcal{S}_{Y;X}$ is also reported, indicating overall prevalence of redundancy and coexistence of redundancy and synergy in the two different bands $B_1$ and $B_2$.
    %(purple bars) is positive in the time domain ($\Delta_{Y;X}=0.075$) indicating an overall prevalence of redundancy, negative in $B_1$ ($\Delta_{Y;X}=-0.028$) and positive in $B_2$ ($\Delta_{Y;X}=0.094$) indicating coexistence of redundancy and synergy in different spectral bands. 
    }
    \label{fig:thsim_mix_new}
\end{figure*}

In the last simulation, we provide an illustrative example of the full-frequency and band-limited PIRD applied to a network of four nodes where different high-order behaviors emerge at different frequencies.
We consider a VAR process simulated with $f_s=1$, where the lagged interactions between the target $Y$ and the three sources $X=\left[X_1,X_2,X_3\right]$ are set to induce a network topology with both common drive and common child structures (Fig. \ref{fig:thsim_mix_new}a). The VAR process is defined as:
\begin{equation}
\begin{aligned}
&Y_n = X_{1,n-1} + X_{3,n-1} + U_{Y,n}\\
&X_{1,n} = \sum_{k=1}^{2} a_{1,k} X_{1,n-k} + U_{X_1,n} \\
&X_{2,n} = \sum_{k=1}^{2} a_{2,k} X_{2,n-k} + X_{1,n-1} + U_{X_2,n} \\
&X_{3,n} = \sum_{k=1}^{2} a_{3,k} X_{3,n-k} + U_{X_3,n}
\label{th_sim_4VAR_equations}
\end{aligned}
\end{equation}
where $U =\left[ U_{Y},U_{X_1},U_{X_2},U_{X_3} \right]$ is a vector of four zero-mean independent Gaussian white noises with unit variance. The autonomous oscillations in the three source processes $X_1$, $X_2$, $X_3$ are obtained placing a pair of complex-conjugate poles, with modulus $\rho$ and phase $2\pi{f}$, in the complex plane representation of each process. %; the AR coefficients resulting from this setting are $a_1=2\rho \cos(2\pi f)$ and $a_2=-\rho^2$ \cite{faes2015information}.
Here, we set $\rho=0.8, f=0.3$ Hz for the processes $X_1$ and $X_2$, so that their autonomous dynamics are determined by the coefficients $a_{1,1}=a_{2,1}=-0.494; a_{1,2}=a_{2,2}=-0.64$; similarly, we set $\rho=0.9, f=0.1$ Hz for $X_3$, so that its autonomous dynamics are determined by the coefficients $a_{3,1}=1.456; a_{3,2}=-0.81$. Moreover, causal interactions are set from $X_1$ and $X_3$ to $Y$ and from $X_1$ to $X_2$ at lag $k=1$, with unitary strength. 

The PIRD was applied computing the spectral MIRs between each individual source and the target ($i_{Y;X_i}(f)$, $i=1,2,3$; Fig. \ref{fig:thsim_mix_new}b), as well as between groups of sources and the target (e.g., $i_{Y;X_1,X_2}(f)$); the spectral redundancy function was then computed as in (\ref{spectral_redundancy_rate_function_MMI}), and was exploited to retrieve the unique ($\mathcal{U}_{Y;X_1}$, $\mathcal{U}_{Y;X_2}$, $\mathcal{U}_{Y;X_3}$), redundant ($\mathcal{R}_{Y;X}$) and synergistic ($\mathcal{S}_{Y;X}$) information rates in the time domain; the latter were obtained as the whole-band integral of the correspondent spectral functions, as well as the integral taken along  two spectral bands centered around the simulated stochastic oscillations (i.e., $B_1 = \left[ 0.04 - 0.15 \right]$ Hz and $B_2 = \left[ 0.15 - 0.4 \right]$ Hz). 

The resulting time-domain values of the individual MIR terms and of the PIRD components are shown in Fig. \ref{fig:thsim_mix_new}c,d. The comparison highlights how, contrary to the pairwise MIR, the coarse-grained PIRD allows to disentangle the underlying network structure.
Indeed, whilst non-zero MIR values are detected between $Y$ and $X_2$ (Fig. \ref{fig:thsim_mix_new}c), the unique contribution of $X_2$ to $Y$ was null, and non-zero unique contributions  are correctly identified only in the presence of direct links (i.e., $\mathcal{U}_{Y;X_1}$, $\mathcal{U}_{Y;X_3}$). 
Remarkably, such non-zero unique contributions are mainly visible when assessed within the frequency bands for which oscillatory components are imposed (i.e., $B_2$ for $\mathcal{U}_{Y;X_1}$ and $B_1$ for $\mathcal{U}_{Y;X_3}$), thus confirming the important role played by the spectral representation of PIRD in the analysis of rhythmic processes.

The PIRD also favors quantification of redundancy and synergy related to the full dynamical structure of the analyzed processes, or to oscillations confined within the bands $B_1$ and $B_2$ (Fig. \ref{fig:thsim_mix_new}d). Redundancy arises typically from common drive (sub)structures where multiple copies of the same information are distributed, providing robustness \cite{luppi2024information}; e.g., here $X_1$ sends redundant information at $\sim 0.3$ Hz to both $X_2$ and $Y$, which is correctly detected by the significant values of $\mathcal{R}_{Y;X}$ within $B_2$.
%Results in Fig. \ref{fig:thsim_mix_new}d suggest that prevalence of redundancy is typical of common drive (sub)structures where the two sources send multiple copies of the same information to the target; e.g., in the substructure including $Y, X_1, X_2$, both the sources send at $0.3$ Hz, thus determining prevalence of redundancy in $B_2$.
On the other hand, synergistic informational circuits generally emerge from common child configurations, requiring a high degree of coordination between multiple parts of the system \cite{varley2023multivariate}; here, since $X_1$ and $X_3$ send information to $Y$ at different frequencies, synergy is detected in both bands $B_1$ and $B_2$.
The balance between synergy and redundancy assessed across the full spectrum, $\Delta_{Y;X}=\mathcal{R}_{Y;X} - \mathcal{S}_{Y;X}$, indicates an an overall prevalence of redundancy within the network. Interestingly, $\Delta_{Y;X}$ indicates the presence of net synergy when assessed within $B_1$, due to the fact that only the source $X_3$ transfers information to the target $Y$ in this band, and of net redundancy when assessed within $B_2$, due to the common drive role of the source $X_1$ in this band. 
Therefore, the combination of common drive and common child substructures, with the source processes oscillating at different frequencies, leads to the coexistence of synergistic and redundant modes of interplay in distinct spectral bands. These complex behaviors emerging at different time scales can be detected only using frequency-specific measures of redundancy and synergy, and can be better characterized separating redundant and synergistic contributions as guaranteed by the PIRD.

\section{\label{sect:applications}Application to physiological networks}

This section presents the application of the proposed framework to the network of physiological processes involved in the homeostatic control of arterial pressure and cerebral blood flow. The applicative context is very popular in the field of computational physiology, where the spontaneous variability of heart rate, arterial pressure, respiration and cerebral blood flow is widely studied to assess non-invasively the mechanisms involved in cardiovascular and cerebrovascular regulation, using a variety of time series analysis techniques including information decomposition methods \cite{zhang2000spontaneous, malpas2002neural, faes2013, faes2016information, porta2017quantifying, gelpi2022dynamic, sparacino2023method, mijatovic2024assessing, sparacino2024measuring}.


\subsection{\label{subsect:applications_data_acquisition}Experimental protocol} 

The application involves physiological time series reflecting th spontaneous variability of cerebral blood flow (CBFV), arterial pressure (AP), heart period (HP) and respiration (RESP). The analyzed time series belong to a database previously collected to study the short-term cardiovascular and cerebrovascular control responses to orthostatic challenge in subjects prone to neurally-mediated syncope % and healthy controls
via the analysis of spontaneous variability of systemic variables \cite{faes2013, bari2016}. The study included 13 subjects (age: $28\pm{9}$ years; 5 males) with previous history of unexplained syncope (reporting $>$ 3 syncope events in the previous 2 years)%and 13 age-matched healthy subjects (nonSYNC, age: $27\pm{8}$ years; 5 males)
, enrolled at the Neurology Division of Sacro Cuore Hospital, Negrar, Italy. The protocol consisted of 10 minutes of recording in the resting supine position, followed by prolonged stay in the 60$^\circ$ position reached after passive head-up tilt. All subjects experienced presyncope signs (i.e., a vasovagal episode characterized by hypotension and reflex bradycardia leading to partial loss of consciousness) during the tilt session; when signs were reported, the subject was returned to the resting position and a spontaneous recovery occurred. % None of the nonSYNC subjects experienced presyncope symptoms during tilt.

The acquired signals were the electrocardiogram (ECG, lead II), the continuous AP measured at the level of middle finger through a photopletysmographic device (Finapres, Enschede, The Netherlands), the CBFV signal measured at the level of the middle cerebral artery by means of a transcranial doppler ultrasonographic device (Multi-Dop T, Compumedics, San Juan Capistrano, CA, USA), and the respiratory amplitude signal measured through a thoracic impedance belt. From these signals, physiological time series were synchronously extracted on a beat-to beat basis starting from the heart period (HP), which was measured as the temporal distance between two consecutive R peaks of the ECG detected through a template matching algorithm \cite{bari2016}. % A manual correction procedure was followed to mitigate the effects of ectopic or isolated arrhythmic beats or missing events using linear interpolation between the closest unaffected values. A low-pass sixth-order Butterworth filter with cut-off frequency of 10 Hz was applied to the CBFV and respiration signals.
Then, the $n^{\mathrm{th}}$ systolic AP ($SAP(n)$) was measured as the maximum of the AP signal inside the $n^{\mathrm{th}}$ HP ($HP(n)$). The $n^{\mathrm{th}}$ diastolic AP value ($DAP(n)$) was taken as the minimum AP between the occurrences of $SAP(n)$ and $SAP(n+1)$. The mean AP (MAP) values were computed by integrating the AP signal between the occurrences of $DAP(n-1)$ and $DAP(n)$ and, then, by dividing the result by the duration of the $n^{\mathrm{th}}$ diastolic interval (i.e., the time distance between the occurrences of $DAP(n-1)$ and $DAP(n)$). The mean CBFV (MCBFV) values were computed by integrating the CBFV signal between the diastolic values (i.e., the minima of the CBFV close to the occurrences of $DAP(n-1)$ and $DAP(n)$) and, then, by dividing the result by the time distance between the two diastolic values. Finally the $n^{\mathrm{th}}$ RESP value (i.e., $RESP(n)$) was computed sampling the respiration signal on the $n^{\mathrm{th}}$ R peak of the ECG.

The beat-to-beat variability series of HP, MAP, MCBFV and RESP, herein referred respectively as $H$, $M$, $F$ and $R$, were then produced as the sequences of consecutive values collected during three stationary time windows of length $L=250$ beats during the following physiological conditions \cite{faes2013, bari2016}: (i) supine rest (RS); (ii) early tilt (ET), starting after the onset of the head-up tilt maneuver, excluding transient changes of the physiological variables; and (iii) late tilt (LT), %starting at least 5 minutes after the onset of the head-up tilt maneuver for healthy subjects, or
occurring just before the pressure decrease due to presyncope (start at $16 \pm 8$ min after the head-up tilt). % for SYNC subjects. Selection of the sequences was performed randomly in each experimental condition and repeated if non-stationarities of the mean and the variance were present. The series were visually inspected and eventually corrected through cubic spline interpolation, with corrections not exceeding the 5\% of the overall length of the sequence.
Further information about the experimental protocol, signal acquisition and variability series extraction can be found in \cite{faes2013, bari2016}.

\subsection{\label{subsect:applications_data_analysis}Data and statistical analysis}
The four physiological time series described above were taken as a realization of the random processes \textit{F}, \textit{M}, \textit{H} and \textit{R}. These processes were investigated to decompose the rate of information shared dynamically between the target $Y=F$ and the set of source processes $X=\{ H,M,R \}$, with $H=X_1, M=X_2, R=X_3$, with the aim of investigating the mechanisms underlying the short-term dynamic regulation of the cerebral blood flow in resting and stress conditions.
Each series was first detrended with an AR high-pass filter with zero phase (cutoff frequency 0.015 cycles/beat) \cite{nollo2000}. Then, a VAR model in the form of (\ref{VAR_model}) was fitted to the four time series; model identification was performed via the ordinary least-squares approach \cite{lutkepohl2005new}, setting the model order $p$ according to the Akaike Information Criterion (AIC) for each subject (with maximum model order equal to 14) \cite{akaike1974}. After VAR identification, computation of time and frequency domain interaction measures of mutual, unique, redundant and synergistic information rates was performed from the estimated model parameters and spectra of the processes. Spectral analysis was performed assuming the series as uniformly sampled with the mean HP taken as the sampling period ($f_s=\frac{1}{<H_n>}$). Specifically, the spectral MIRs between the target and groups of sources were computed as in (\ref{spectralMIR}); then, according to coarse-graining PIRD, the unique, redundant and synergistic information rates calculated from the spectral redundancy rate in (\ref{spectral_redundancy_rate_function_MMI}) were computed through integration within the low frequency (LF, $\left[ 0.04 - 0.15 \right]$ Hz) and high frequency (HF, $\left[ 0.15 - 0.4 \right]$ Hz) bands of the spectrum to analyze specific rhythms with physiological meaning \cite{malpas2002neural}, as well as through integration over all frequencies, to get overall time-domain measures. % Specifically, the measures were averaged within the LF band conventionally adopted for studying MAP-MCBFV spectral interactions \cite{claassen2016}. Representative spectral MIR profiles $i_{F;H}(f), i_{F;M}(f), i_{F;R}(f)$ are shown in Fig. \ref{fig:appl_negrar}a.

Given the small size of the surveyed population, non-parametric statistical tests were applied to assess statistically significant differences between indexes evaluated in the three phases of the experimental protocol, i.e., RS, ET and LT conditions. Specifically, the Wilcoxon signed rank test for paired data was applied on the MIRs shared between the target and each source, as well as on the PIRD terms (i.e., the joint MIR shared between the target and all the sources, the unique, redundant and synergistic information rates), evaluated in the time domain and along the LF and HF bands of the spectrum during the three phases of the protocol. Further, the statistical differences among pairs of unique information rate measures, as well as between redundancy and synergy, were tested in the time domain, LF and HF bands for each experimental condition via Wilcoxon signed rank test for paired data with Bonferroni-Holm correction for multiple comparisons. For all the statistical tests, the significance level was set to $0.05$. 

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]
    {Figures_Paper/fig_appl_cerebro.pdf}
    \caption{\textbf{Coarse-grained PIRD applied to the physiological network of mean cerebral blood flow velocity ($F$), mean arterial pressure ($M$), heart period ($H$) and respiration ($R$) assessed in patients prone to develop postural-related syncope.}
    The mutual (left column: $I_{F;H}$ (black dots), $I_{F;M}$ (gray dots), $I_{F;R}$ (pink dots), $I_{F;H,M,R}$ (light blue dots)), unique (middle column: $\mathcal{U}_{F;H}$ (black dots), $\mathcal{U}_{F;M}$ (gray dots), $\mathcal{U}_{F;R}$ (pink dots)), redundant (right column: $\mathcal{R}_{F;H;M;R}$, magenta dots) and synergistic (right column: $\mathcal{S}_{F;H;M;R}$, blue dots) information rates shared between the target $F$ and the sources $\{ H,M,R \}$ are computed in the RS (left boxplots), ET (middle boxplots) and LT (right boxplots) conditions along \textbf{a)} the whole frequency axis (TIME), \textbf{b)} the LF and \textbf{c)} the HF bands of the spectrum, taking the spectral redundancy rate function as in (\ref{spectral_redundancy_rate_function_MMI}). The redundancy-synergy balance $\Delta$ is shown as boxplot distributions and individual values (purple dots) in the right column of panels a) (TIME), b) (LF) and c) (HF). The statistical differences among pairs of unique information rates (middle panels) were tested via Wilcoxon signed rank test for paired data ($p<0.05$) with Bonferroni-Holm correction for multiple comparisons: significant p-values are shown above the boxplot distributions with colors indicating the distribution to be compared with (i.e., black if compared with $\mathcal{U}_{F;H}$, gray if compared with $\mathcal{U}_{F;M}$). Wilcoxon signed rank test for paired data, $p<0.05$: $\ast$, RS vs. ET; $\S$, RS vs. LT; $\#$, ET vs. LT.}
    \label{fig:appl_negrar}
\end{figure*}

\subsection{\label{subsect:applications_results}Results and discussion}
Results are shown in Fig. \ref{fig:appl_negrar} as boxplot distributions and individual values of the PIRD measures computed in the RS, ET and LT conditions. 
The MIRs $(I_{F;H},I_{F;M},I_{F;R},I_{F;H,M,R})$ and unique information rates $(\mathcal{U}_{F;H}, \mathcal{U}_{F;M}, \mathcal{U}_{F;R})$ assessed in the time domain (panels \textbf{a)}) and within the LF (panels \textbf{b)}) and HF (panels \textbf{c)}) bands of the spectrum are depicted in the left and middle columns, respectively, while the redundant and synergistic contributions and their balance ($\Delta$) are shown in the right column.

% interaction pressure - cerebral flow
The transition to ET induced an increase of $I_{F;M}$ and $\mathcal{U}_{F;M}$ mainly visible in the LF band of the spectrum (Fig. \ref{fig:appl_negrar}a,b, left and middle columns), in line with previous observations indicating that the increase of the information transfer from \textit{M} to \textit{F} is related to altered cerebral autoregulation (CA) in syncope patients \cite{bari2017}. The LT-induced decrease of $I_{F;M}$ in HF is probably related to the modulating effect of respiration \cite{bari2016}, and thus is not detected by the unique contribution of arterial pressure ($\mathcal{U}_{F;M}$, panel c, middle column).
% interaction respiration - cerebral flow
As regards the interactions between cerebral blood flow and respiration, we found that the MIR $I_{F;R}$ increases during the early phase of tilt in LF probably due to confounding effects of $H$, $M$, while RS and ET values of $\mathcal{U}_{F;R}$ are very close to zero for most of subjects (Fig. \ref{fig:appl_negrar}b, left and middle columns), suggesting that the unique information shared between $R$ and $F$ may be negligible during this phase of the protocol. This result is further confirmed by the significantly lower values of $\mathcal{U}_{F;R}$ than $\mathcal{U}_{F;H}$ and $\mathcal{U}_{F;M}$ in the RS and ET phases. However, a significant increase of the unique information rate shared between the respiratory and the cerebral system in the LF band is observed in LT, thus shedding light on the remarkable role of respiration in influencing the dynamics of CBFV suddenly before the occurrence of syncope \cite{porta2008influence, bari2016}.
It is worth noting that the tilt-induced significant increase of cerebro-vascular and cerebro-respiratory interactions in the LF band is well visible as significant increases of the joint MIR $I_{F;H,M,R}$ (Fig. \ref{fig:appl_negrar}b, left column), which however is not a \textit{source-specific} measure and thus cannot distinct between the pathways of information flow.

% interaction cardiac system - cerebral flow
A major result is related to the interactions between cerebral and cardiac processes, which indeed do not change in response to the orthostatic challenge within the investigated network ($I_{F;H}$, $\mathcal{U}_{F;H}$, black distributions of left and middle panels), thus implying invariance of these relationships with the postural stress.
Nonetheless, besides being significantly lower than $\mathcal{U}_{F;M}$ during tilt as suggested by p-values $<0.05$ (Fig. \ref{fig:appl_negrar}a, middle column), a result that emerges better in the LF band (Fig. \ref{fig:appl_negrar}b, middle column), the role played by heart rate in influencing CBFV variability in this group of subjects cannot be marked as negligible throughout the experimental protocol outlined in the study. Instead, we remark here the prevalence of the cerebro-vascular interactions between cerebral blood flow and arterial pressure.
Overall, what emerges from the analysis of mutual and unique information rates is that the latter is able to capture direct mechanisms of interaction which can be masked by the presence of other unobserved variables in the MIR measure: looking at the unique contributions, only the $M-F$ interactions seem to be significantly affected by the early postural stress in the LF band, while respiration may play a role in shaping CBFV dynamics in the late phase of tilt.

In addition, the significant increase of the redundant and synergistic information rates assessed in the LF band moving from RS to ET and LT, with redundancy always significantly higher than synergy as documented by statistical tests, suggests that the orthostatic stress is responsible for the emergence of predominantly redundant patterns of interaction between cardiovascular and cardiorespiratory processes sharing information with the cerebral flow velocity taken as the target process. The significant increase of $\Delta$ in the early phase of tilt (Fig. \ref{fig:appl_negrar}b, right panel) remarks the importance of LF oscillations within the network and the LF-dependent nature of the interactions among the investigated signals.

Overall, these results confirm the redundant nature of cardiovascular and cerebrovascular interactions previously reported for similar triplets of physiological processes \cite{faes2016information, porta2017quantifying, faes2022new}, and document the relevance of separating LF and HF contributions to elicit the different roles of heart rate, arterial pressure and respiration on cardiovascular and cerebrovascular interactions. Moreover, the significant increase of redundancy with tilt suggests that these redundant effects are enhanced during postural stress, likely as a consequence of sympathetic activation and vagal withdrawal \cite{malpas2002neural,porta2017quantifying}.
Therefore, the evidence that cardiovascular and cerebrovascular interactions occur through the coupling of rhythms in different frequency bands with different physiological meaning \cite{porta2015wiener}, make the proposed spectral PIRD eligible to probe high-order interactions in these networks \cite{faes2021information}. 

\section*{Conclusions}
In this work, we introduced the partial information rate decomposition (PIRD) framework by extending partial information decomposition (PID) to random processes with temporal statistical structure. We provided a decomposition of the information shared dynamically between a target process and a set of source processes into unique, redundant, and synergistic contributions over time, by utilizing the concept of rate of mutual information shared instead of the well-known mutual information. A key innovation is the introduction of a spectral redundancy rate function, formulated exploiting a pointwise definition of redundancy in the spectral domain, thus enabling a frequency-specific decomposition of information interactions. Furthermore, we introduced a coarse-grained representation of information decomposition, which aggregates partial information atoms into a principled structure that remains scalable and interpretable even in high-dimensional dynamic systems. By integrating spectral analysis and coarse-graining, our framework provides a refined and computationally efficient partition of information flow in complex networks.

Through benchmark simulations, we demonstrated how conventional PID methods can fail to properly decompose high-order interactions in dynamic systems, due to a missing or partial account of temporal correlations. In contrast, our framework provides a more accurate and interpretable decomposition of multivariate information flow in dynamic systems of random processes. Additionally, the application of the PIRD to a physiological network involving cerebrovascular and cardiovascular variables revealed that frequency-dependent redundant information exchange plays a crucial role in the response to postural stress, as well as that cardiovascular and cerebrovascular interactions occur through the coupling of rhythms in different frequency bands with different physiological meaning. This emphasizes the importance of availing of spectrally resolved information decomposition methods in the presence of networks of random processes with relevant oscillatory content, which can exhibit the coexistence of redundant and synergistic higher-order interactions within different frequency bands.

Overall, by integrating temporal structure and frequency-specific dependencies, PIRD offers a more refined and comprehensive approach for studying multivariate interactions in neuroscience, biology, engineering, and beyond. Future works will explore further refinements in redundancy definitions and their applicability to real-world networked systems with nonlinear dependencies.


%\textcolor{black}{where we warn against the use of PID in processes exhibiting evident temporal statistical structure and show that the presence of temporal correlations has a profound impact on the multivariate information shared at lag zero by multiple random processes. Additionally, we investigate the effects of changes in the network topology on the PID measures computed by decomposing the TE from the sources to the target process, evidencing that the latter cannot take time-lagged interactions directed from the target to the sources into account and is thus limited to only one direction of interaction. Further, we show coexistence of redundant and synergistic characters of interplay in different spectral bands elicited by the frequency-specific PIRD, highlighting the major role played by the spectral PIRD in disentangling mechanisms occurring at specific scales and likely hidden in the time domain.}

\begin{acknowledgments}
This work was supported by the project HONEST - High-Order Dynamical Networks in Computational Neuroscience and Physiology: an Information-Theoretic Framework, Italian Ministry of University and Research (funded by MUR, PRIN 2022, code 2022YMHNPY, CUP: B53D23003020006).
\end{acknowledgments}

\nocite{*}

\bibliography{biblio_Paper}

%\appendix*
%\input{sections/appendix1.tex}

\end{document}