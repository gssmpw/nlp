\section{Introduction}
\label{sec:introduction}
\vspace{-0.1cm}
Tree-like structures such as hierarchies are key for knowledge representation, from biological taxonomies \citep{padial2010integrative} and phylogenetics \citep{kapli2020phylogenetic} to natural language \citep{miller1995wordnet, tifrea2018poincar, yang2016hierarchical}, social networks \citep{freeman2004development}, visual understanding \citep{desai2023hyperbolic} and more. To obtain faithful embeddings, Euclidean space is ill-equiped; even simple trees lead to high distortion \citep{sonthalia2020tree}. On the other hand, the exponential nature of hyperbolic space  makes it a natural geometry for embedding trees \citep{nickel2018learning}. This insight has led to rapid advances in hyperbolic learning, with superior embedding \citep{sala2018representation} and clustering \citep{chami2020trees} of tree-like data.

Recent literature has shown that hyperbolic tree embeddings are not only useful on their own, they also form powerful target embeddings on top of deep networks to unlock hierarchical representation learning \citep{peng2021hyperbolic,mettes2024hyperbolic}. Deep learning with hyperbolic tree embeddings has made it possible to effectively perform action recognition \citep{long2020searching}, knowledge graph completion \citep{kolyvakis2020hyperbolic}, hypernymy detection \citep{tifrea2018poincar} and many other tasks in hyperbolic space. These early adoptions of hyperbolic embeddings have shown a glimpse of the powerful improvements that hierarchically aligned representations can bring to deep learning.

The rapid advances in hyperbolic deep learning underline the need for hyperbolic tree embeddings compatible with GPU accelerated software. Current tree embedding algorithms can roughly be divided in two categories; optimization-based and constructive methods. The optimization-based methods, e.g. Poincar√© embeddings \citep{nickel2017poincare}, hyperbolic entailment cones \citep{ganea2018hyperbolic}, and distortion optimization \citep{yu2022skin}, train embeddings using some objective function based on the tree. While these approaches are flexible due to minimal assumptions, the optimization can be unstable, slow and result in heavily distorted embeddings. Conversely, constructive methods traverse a tree once, placing the children of each node on a hypersphere around the node's embedding \citep{sarkar2011low,sala2018representation}. These methods are fast, require no hyperparameter tuning and have great error guarantees. However, they rely on hyperspherical separation, a notoriously difficult problem \citep{saff1997distributing}, and on multiple precision floating point arithmetic, which is incompatible with GPUs and other accelerated hardware. 

Our goal is to embed trees in hyperbolic space with minimal distortion yet with the ability to operate on accelerated GPU hardware even when using higher precision. We do so in two steps. First, we outline HS-DTE, a new generalization of Delaunay tree embeddings \citep{sarkar2011low} to arbitrary dimensionality through hyperspherical separation. Second, we propose HypFPE, a floating point expansion arithmetic approach to enhance our constructive hyperbolic tree embeddings. We develop new routines for computing hyperbolic distances on floating point expansions and outline how to use these on hyperbolic embeddings. Furthermore, we provide theoretical results demonstrating the effectiveness of these floating point expansion routines. Floating point expansions allow for higher precision similar to multiple precision arithmetic. However, our routines can be implemented using standard floating point operations, making these compatible with GPUs. Experiments demonstrate that HS-DTE generates higher fidelity embeddings than other hyperbolic tree embeddings and that HypFPE further increases the embedding quality for HS-DTE and other methods. We will make two software libraries available, one for arbitrary-dimensional hyperbolic tree embeddings and one for GPU-compatible floating point expansions.






