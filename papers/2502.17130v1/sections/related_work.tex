\section{Preliminaries and related work}
\label{sec:related_work}

\subsection{Hyperbolic geometry preliminaries}
\vspace{-0.1cm}
To help explain existing constructive hyperbolic embedding algorithms and our proposed approach, we outline the most important hyperbolic functions here. For a more thorough overview, we refer to \citep{cannon1997hyperbolic,anderson2005hyperbolic}. Akin to \citep{nickel2017poincare,ganea2018hyperbolic,sala2018representation}, we focus on the Poincaré ball model of hyperbolic space. For $n$-dimensional hyperbolic space, the Poincaré ball model is defined as the Riemannian manifold $(\mathbb{D}^n, \mathfrak{g}^n)$, where the manifold and Riemannian metric are defined as
\begin{equation}
\begin{gathered}
    \mathbb{D}^n = \big\{ \bfx \in \mathbb{R}^n : ||\bfx||^2 < 1 \big\},\\
    \mathfrak{g}^n = \lambda_\bfx I_n, \quad \lambda_\bfx = \frac{2}{1 - ||x||^2}.
\end{gathered}
\end{equation}
Using this model of hyperbolic space, we can compute distances between $\bfx, \bfy \in \mathbb{D}^n$ either as
\begin{equation}\label{eq:poin_dist_acosh}
    d_{\mathbb{D}} (\bfx, \bfy) = \cosh^{-1} \bigg( 1 + 2 \frac{||\bfx - \bfy||^2}{(1 - ||\bfx||^2) (1 - ||\bfy||^2)} \bigg),
\end{equation}
or as
\begin{equation}\label{eq:poin_dist_atanh}
    d_{\mathbb{D}} (\bfx, \bfy) = 2 \tanh^{-1} \big( ||-\bfx \oplus \bfy|| \big),
\end{equation}
where
\begin{equation}\label{eq:mob_add}
    \bfx \oplus \bfy = \frac{(1 + 2 \langle \bfx, \bfy \rangle + ||\bfy||^2) \bfx + (1 - ||\bfx||^2) \bfy}{1 + 2 \langle \bfx, \bfy \rangle + ||\bfx||^2 ||\bfy||^2},
\end{equation}
is the Möbius addition operation. These formulations are theoretically equivalent, but suffer from different numerical errors. This distance represents the length of the straight line or geodesic between $\bfx$ and $\bfy$ with respect to the Riemannian metric $\mathfrak{g}^n$. Geodesics of the Poincaré ball are Euclidean straight lines through the origin and circular arcs perpendicular to the boundary of the ball.
We will use some isometries of hyperbolic space. More specifically, we will use reflections in geodesic hyperplanes. A geodesic hyperplane is an $(n-1)$-dimensional manifold consisting of all geodesics through some point $\bfx \in \mathbb{D}^n$ which are orthogonal to a normal geodesic through $\bfx$ or, equivalenty, orthogonal to some normal tangent vector $\bfv \in \mathcal{T}_\bfx \mathbb{D}^n$. For the Poincaré ball these are the Euclidean hyperplanes through the origin and the $(n-1)$-dimensional hyperspherical caps which are perpendicular to the boundary of the ball. We will denote a geodesic hyperplane by $H_{\bfx, \bfv}$. Reflection in a geodesic hyperplane $H_{\mathbf{0}, \bfv}$ through the origin can be defined as in Euclidean space, so as a Householder transformation
\begin{equation}
    R_{H_{\mathbf{0}, \bfv}}(\bfy) = (I_n - 2 \bfv \bfv^T) \bfy,
\end{equation}
where $||\bfv|| = 1$. Reflection in the other type of geodesic hyperplane is a spherical inversion:
\begin{equation}
    R_{H_{\bfx, \bfv}}(\bfy) = \bfm + \frac{r^2}{||\bfy - \bfm||^2} (\bfy - \bfm),
\end{equation}
with $\bfm \in \mathbb{R}^n$, $r > 0$ the center and radius of the hypersphere containing the geodesic hyperplane. We will denote a reflection mapping some point $\bfx \in \mathbb{D}^n$ to another point $\bfy \in \mathbb{D}^n$ by $R_{\bfx \rightarrow \bfy}$. The specific formulations and derivations of the reflections that we use are in Appendix \ref{sec:reflections}.

\vspace{-0.1cm}

\subsection{Related work}
\vspace{-0.15cm}
\paragraph{Hyperbolic tree embedding algorithms.}
Existing embedding methods can be divided into two categories: optimization-based methods and constructive methods. The optimization methods typically use the tree to define some loss function and use a stochastic optimization method such as SGD to directly optimize the embedding of each node, e.g. Poincaré embeddings \citep{nickel2017poincare}, hyperbolic entailment cones \citep{ganea2018hyperbolic} and distortion optimization \citep{sala2018representation,yu2022skin}. Poincaré embeddings use a contrastive loss where related nodes are pulled together and unrelated nodes are pushed apart. Hyperbolic entailment cones attach an outwards radiating cone to each node embedding and define a loss that forces children of nodes into the cone of their parent. Distortion optimization directly optimizes for a distortion loss to embed node pairs. Such approaches are flexible, but do not lead to arbitrarily low distortion and optimization is slow. Constructive methods are either combinatorial methods \citep{sarkar2011low,sala2018representation} or eigendecomposition methods \citep{sala2018representation}. Combinatorial methods first place the root of a tree at the origin of the hyperbolic space and then traverse down the tree, iteratively placing nodes as uniformly as possible on a hypersphere around their parent. \citep{sarkar2011low} proposes a 2-dimensional approach, where the points have to be separated on a circle; a trivial task. For higher dimensions, \citep{sala2018representation} place points on a hypercube inscribed within a hypersphere, which leads to suboptimal distribution. We also follow a constructive approach, where we use an optimization method for the hyperspherical separation, leading to significantly higher quality embeddings. The eigendecomposition method h-MDS \citep{sala2018representation} takes a graph or tree metric and uses an eigendecomposition of the corresponding distance matrix to generate low-distortion embeddings. However, it collapses nodes within some subtrees to a single point, leading to massive local distortion.

\vspace{-0.15cm}

\paragraph{Deep learning with hyperbolic tree embeddings.} In computer vision, a wide range of works have recently shown the potential and effectiveness of using a hyperbolic embedding space \citep{khrulkov2020hyperbolic}. Specifically, hierarchical prior knowledge can be embedded in hyperbolic space, after which visual representations can be mapped to the same space and optimized to match this hierarchical organization. \citep{long2020searching} show that such a setup improves hierarchical action recognition, while \citep{liu2020hyperbolic} use hierarchies with hyperbolic embeddings for zero-shot learning. Deep visual learning with hyperbolic tree embeddings has furthermore shown to improve image segmentation \citep{ghadimi2022hyperbolic}, skin lesion recognition \citep{yu2022skin}, video understanding \citep{li2024isolated}, hierarchical visual recognition \citep{ghadimi2021hyperbolic,dhall2020hierarchical}, hierarchical model interpretation \citep{gulshad2023hierarchical}, open set recognition \citep{dengxiong2023ancestor}, continual learning \citep{gao2023exploring}, few-shot learning \citep{zhang2022hyperbolic}, and more. Since such approaches require freedom in terms of embedding dimensionality, they commonly rely on optimization-based approaches to embed the prior tree-like knowledge. Similar approaches have also been investigated in other domains, from audio \citep{petermann2023hyperbolic} and text \citep{dhingra2018embedding,le2019inferring} to multimodal settings \citep{hong2023hyperbolic}. In this work, we provide a general-purpose and unconstrained approach for low-distortion embeddings with the option to scale to higher precisions without losing GPU-compatibility.

\vspace{-0.15cm}

\paragraph{Floating point expansions.}
Floating point expansions (FPEs) to increase precision in hyperbolic space was proposed by \citep{yu2021representing} and implemented in a PyTorch library \citep{yu2022mctensor}. However, their methodology is based on older FPE arithmetic definitions and routines by \citep{priest1991algorithms,priest1992properties,shewchuk1997adaptive}. In the field of FPEs, more efficient and stable formulations have been proposed over the years with improved error guarantees \mbox{\citep{joldes2014computation, joldes2015arithmetic, muller2016new}}. In this paper, we build upon the most recent arithmetic framework detailed in \citep{popescu2017towards}. We have implemented this framework for PyTorch and extend its functionality to work with hyperbolic embeddings.
