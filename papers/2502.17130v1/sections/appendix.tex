\newpage
\section{Geodesic hyperplane reflections}\label{sec:reflections}
In this paper we will make use of reflections in geodesic hyperplanes through the origin to align points on a hypersphere centered at the origin with some existing point on the hypersphere. More specifically, if we have points $\bfw, \bfz \in \mathbb{D}^n$ with $||\bfw|| = ||\bfz||$ and we want to reflect $\bfw$ to $\bfz$, then we can use a Householder reflection with $\bfv = \frac{(\bfz - \bfw)}{||\bfz - \bfw||}$, so
\begin{equation}
    R_{\bfw \rightarrow \mathbf{z}} (\bfy) = \bigg(I_n - \frac{2 (\bfz - \bfw) (\bfz - \bfw)^T}{||\bfz - \bfw||^2}\bigg) \bfy.
\end{equation}
To see that this maps $\bfw$ to $\bfz$, we can simply enter $\bfw$ into this map to see that
\begin{align}
    R_{\bfw \rightarrow \mathbf{z}} (\bfw) &= \bigg(I_n - \frac{2 (\bfz - \bfw) (\bfz - \bfw)^T}{||\bfz - \bfw||^2}\bigg) \bfw \\
    &= \bfw - \frac{2(\langle \bfz, \bfw \rangle - ||\bfw||^2)}{||\bfz||^2 - 2 \langle \bfz, \bfw \rangle + ||\bfw||^2} (\bfz - \bfw) \\
    &= \bfw + \frac{||\bfz||^2 - 2 \langle \bfz, \bfw \rangle + ||\bfw||^2 + ||\bfw||^2 - ||\bfz||^2}{||\bfz||^2 - 2 \langle \bfz, \bfw \rangle + ||\bfw||^2} (\bfz - \bfw) \\
    &= \bfw + \bigg(1 + \frac{||\bfw||^2 - ||\bfz||^2}{||\bfz||^2 - 2 \langle \bfz, \bfw \rangle + ||\bfw||^2}\bigg) (\bfz - \bfw) \\
    &= \bfw + \bfz - \bfw = \bfz.
\end{align}
We make use of reflections in geodesic hyperplanes not through the origin that reflect some given point $\bfw \in \mathbb{D}^n$ to the origin. This can be done through reflection in the hyperplane contained in the hypersphere with center $\bfm = \frac{\bfw}{||\bfw||^2}$ and radius $r = \sqrt{\frac{1}{||\bfw||^2} - 1}$. We easily verify that this hyperspherical inversion maps $\bfw$ to the origin.
\begin{align}
    R_{\bfw \rightarrow \mathbf{0}} (\bfw) &= \frac{\bfw}{||\bfw||^2} + \frac{\frac{1}{||\bfw||^2} - 1}{||\bfw - \frac{\bfw}{||\bfw||^2}||} \Big(\bfw - \frac{\bfw}{||\bfw||^2}\Big) \\
    &= \frac{\bfw}{||\bfw||^2} + \frac{1 - ||\bfw||^2}{||\bfw||^4 - 2 ||\bfw||^2 + 1} \Big( 1 - \frac{1}{||\bfw||^2} \Big) \bfw \\
    &= \frac{\bfw}{||\bfw||^2} + \frac{1}{1 - ||\bfw||^2} \cdot \frac{||\bfw||^2 - 1}{||\bfw||^2} \bfw \\
    &= \frac{\bfw}{||\bfw||^2} - \frac{\bfw}{||\bfw||^2} = \mathbf{0}.
\end{align}
To show that this is a reflection in a geodesic hyperplane and, therefore, an isometry, we need to show that the hypersphere defined by $\bfm$ and $r$ is orthogonal to the boundary of $\mathbb{D}^n$. This is the case when all the triangles formed by the line segments between $\mathbf{0}$, $\bfm$ and any point $\bfv$ in the intersection of the hypersphere and the boundary of $\mathbb{D}^n$ are right triangles. This is exactly the case when the Pythagorean theorem holds for each of these triangles. For each $\bfv$ we have that $||\bfv|| = 1$ and $||\bfv - \bfm|| = r$, so
\begin{align}
    ||\bfv - \mathbf{0}||^2 + ||\bfv - \bfm||^2 &= 1 + r^2 \\
    &= \frac{1}{||\bfw||^2} \\
    &= \frac{||\bfw||^2}{||\bfw||^4} \\
    &= ||\bfm - \mathbf{0}||^2,
\end{align}
which shows that the Pythagorean theorem holds and, thus, that this hyperspherical inversion is a geodesic hyperplane reflection, so an isometry.

\section{Evaluation metrics}
\label{sec:metrics}
We will use two distortion based evaluation metrics. The first one is the average relative distortion \citep{sala2018representation}, given as
\begin{equation}
    D_{ave} (\phi) = \frac{1}{N(N-1)} \sum_{u \neq v} \frac{|d_{\mathbb{D}} (\phi(u), \phi(v)) - d_T (u, v)|}{d_T (u, v)},
\end{equation}
where $N = |V|$ is the number of nodes. A low value for this metric is a necessary, but not sufficient condition for a high quality embedding, as it still allows for large local distortion. Therefore, we use a second distortion based metric, the worst-case distortion \citep{sarkar2011low}, given by
\begin{equation}
    D_{wc} (\phi) = \max_{u \neq v} \frac{d_\mathbb{D}(\phi(u), \phi(v))}{d_T (u, v)} \bigg( \min_{u \neq v} \frac{d_\mathbb{D}(\phi(u), \phi(v))}{d_T (u, v)} \bigg)^{-1}.
\end{equation}
$D_{ave}$ ranges from $0$ to infinity and $D_{wc}$ ranges from $1$ to infinity, with smaller values indicating strong embeddings. A large $D_{ave}$ indicates a generally poor embedding, while a large $D_{wc}$ indicates that at least some part of the tree is poorly embedded. Both values should be close to their minimum if an embedding is to be used for a downstream task. Lastly, another commonly used evaluation metric for unweighted trees is the mean average precision \citep{nickel2017poincare}, given by 
\begin{equation}
    \text{MAP}(\phi) = \frac{1}{N} \sum_{u \in V} \frac{1}{\text{deg}(u)} \sum_{v \in \mathcal{N}_V (u)} \frac{\Big|\mathcal{N}_V(u) \cap \phi^{-1} \Big(B_{\mathbb{D}}(u, v)\Big)\Big|}{\Big| \phi^{-1} \Big(B_{\mathbb{D}}(u, v)\Big) \Big|},
\end{equation}
where $\text{deg}(u)$ denotes the degree of $u$ in $T$, $\mathcal{N}_V (u)$ denotes the nodes adjacent to $u$ in $V$ and where $B_{\mathbb{D}} (u, v) \subset \mathbb{D}^n$ denotes the closed ball centered at $\phi(u)$ with hyperbolic radius $d_\mathbb{D}(\phi(u), \phi(v))$, so which contains $v$ itself. The MAP reflects how well we can reconstruct neighborhoods of nodes while ignoring edge weights, making it less appropriate for various downstream tasks. 

\section{Placing points on the vertices of a hypercube}\label{sec:maximal_hamming_dists}
The discussion here is heavily based on \citep{sala2018representation}. We include it here for completeness. When placing a point on the vertex of an $n$-dimensional hypercube, there are $2^n$ options, so each option can be represented by a binary sequence of length $n$. For example, on a hypercube where each vertex $v$ has $||v||_\infty = 1$, each vertex is of the form $(\pm 1, \ldots, \pm 1)^T$, so we can represent $v$ as some binary sequence $s$. The distance between two such vertices can then be expressed in terms of the Hamming distance between the corresponding sequences as $$d(v_1, v_2) = \sqrt{4d_{\text{Hamming}} (s_1, s_2)},$$ which shows that points placed on vertices of a hypercube are maximally separated if this Hamming distance is maximized. This forms an interesting and well studied problem in coding theory where the objective is to find $k$ binary sequences of length $n$ which have maximal pairwise Hamming distances. There are some specific combinations of $n$ and $k$ for which optimal solutions are known, such as the Hadamard code. However, for most combinations of $n$ and $k$, the solution is still an open problem \citep{macwilliams1977theory}. Therefore, properly placing points on the vertices of a hypercube currently relies on the solution to an unsolved problem, making it difficult in practice.

\section{Proof of Theorem \ref{thm:mhs_complexity}}\label{sec:mhs_proof}
\begin{proof}
For a tree $T = (V, E)$ with $N = |V|$, we know that the degrees of the vertices satisfy
\begin{equation}
    \sum_{v \in V} \text{deg}(v) = 2 |E| = 2(N-1).
\end{equation}
Suppose $W_{1}, \ldots, W_{p} \subset S^{n-1}$ are the sets of points on the hypersphere generated by the $p$ optimizations that need to be ran to perform the construction, then $|W_{i}| \neq |W_{j}|$, since we use the cached result whenever nodes have the same degree. Moreover, $|W_i|$ is equal to the degree of the node for which the points are generated, so
\begin{equation}
    \sum_{i = 1}^p |W_i| \leq \sum_{v \in V} \text{deg}(v) = 2(N-1).
\end{equation}
Given this constraint, the largest possible value of $p$ is when we can fit as many $|W_i|$'s in this sum as possible, which is when $|W_1|, \ldots, |W_p| = 1, \ldots, p$. In that case
\begin{equation}
    \sum_{i = 1}^p |W_i| = \sum_{i=1}^p i = \frac{p(p+1)}{2} \leq 2(N-1).
\end{equation}
Solving for integer $p$ yields
\begin{equation}
    p \leq \Big\lceil \frac{1}{2} (\sqrt{16N - 15} - 1) \Big\rceil.
\end{equation}
\end{proof}
Note that this bound can be sharpened slightly by observing that each node $v$ with $\text{deg}(v) > 1$ forces the existence of $\text{deg}(v) - 1$ leaf nodes with degree 1. However, the asymptotic behaviour remains $\mathcal{O}(\sqrt{N})$.

\section{Proof of Theorem \ref{thm:acosh_accuracy}}\label{sec:acosh_accuracy_proof}
\begin{theorem*}
    Given $\bfx, \bfy \in \mathbb{D}^n$ with $||\bfx|| < 1 - \epsilon^{t - 1}$ and $||\bfy|| < 1 - \epsilon^{t - 1}$, an approximation $d$ to equation \ref{eq:poin_dist_acosh} can be computed with FPE representations with $t$ terms and with a largest magnitude approximation to $\cosh^{-1}$ such that
    \begin{equation}
        \bigg|d - \cosh^{-1} \bigg( 1 + 2 \frac{||\bfx - \bfy||^2}{(1 - ||\bfx||^2) (1 - ||\bfy||^2)} \bigg)\bigg| < \epsilon^*,
    \end{equation}
    for some small $\epsilon^* > 0$.
\end{theorem*}

\begin{proof}
    We begin by noting that the accuracy of the largest magnitude approximation to $\cosh^{-1}$ depends on the underlying floating point algorithm used for computing the inverse hyperbolic cosine. While this function cannot be computed up to machine precision on its entire domain due to the large derivative near the lower end of its domain, it can still be computed quite accurately, i.e. there exists some small $\epsilon_1^* > 0$ such that
    \begin{equation}\label{eq:acosh_float_acc}
        \Big|\cosh^{-1}(x) - \cosh^{-1}(\tilde{x})\Big| < \epsilon_1^*,
    \end{equation}
    where $x \in [1, R]$, where $R$ is the greatest representable number and $\tilde{x}$ is the floating point approximation to $x$, so for which we have
    \begin{equation}
        \frac{|\tilde{x} - x|}{|x|} < \epsilon.
    \end{equation}
    For example, in PyTorch when using float64, we have $\epsilon_1^* \approx 2.107 * 10^{-8}$. If we can approximate the argument inside $\cosh^{-1}$ sufficiently accurately, then the largest magnitude approximation will be close enough to guarantee a small error. More specifically, let
    \begin{equation}
        z = 1 + 2 \frac{||\bfx - \bfy||^2}{(1 - ||\bfx||^2) (1 - ||\bfy||^2)},
    \end{equation}
    and let $\tilde{z} = \tilde{z}_1 + \ldots + \tilde{z}_t$ with $|\tilde{z}_i| > |\tilde{z}_j|$ for each $i \neq j$ be the approximation to $z$ obtained through FPE arithmetic. If
    \begin{equation}\label{eq:arg_approx}
        \frac{|z - \tilde{z}|}{|z|} = \frac{|z - \sum_{i=1}^t \tilde{z}_i |}{|z|} < 2\epsilon,
    \end{equation}
    where $\epsilon$ is the machine precision of the corresponding floating point format, then
    \begin{align}
        |z - \tilde{z}_1| &\leq |z - \tilde{z}| + \Big| \sum_{i=2}^t \tilde{z}_i \Big| \\
        &< 2 \epsilon + 2 \epsilon |\tilde{z}_1| \\
        &\leq 4\epsilon |z| + 2\epsilon |z - \tilde{z}_1|,
    \end{align}
    where we use that $|\tilde{z}_2| \leq \text{ulp} (\tilde{z}_1) = \epsilon |\tilde{z}_1|$, so that $|\sum_{i=2}^t \tilde{z}_i| < 2\epsilon |\tilde{z}_1|$. Now, we can rewrite to see that
    \begin{align}
        \frac{|z - \tilde{z}_1|}{|z|} < \frac{4 \epsilon}{1 - 2 \epsilon} < 8 \epsilon.
    \end{align}
    Therefore, by repeatedly using equation \ref{eq:acosh_float_acc}, we see that the largest magnitude approximation error is bounded by $16 \epsilon_1^*$. Our ability to approximate the argument $z$ as precisely as in equation \ref{eq:arg_approx} using FPEs follows from the error bounds of the FPE arithmetic routines from \citep{popescu2017towards}. This shows that the statement holds for $\epsilon^* = 16 \epsilon_1^*$.
\end{proof}

\section{Proof of Proposition \ref{thm:acosh_range}}\label{sec:acosh_range_proof}
\begin{proposition*}
    The range of the inverse hyperbolic tangent formulation increases linearly in the number of terms $t$ of the FPEs being used.
\end{proposition*}

\begin{proof}
    When we use FPEs with $t$ terms, we can represent points $\bfx, \bfy \in \mathbb{D}^n$ such that $||\bfx|| = 1 - \epsilon^{t-1}$ and $||\bfy|| = 1 - \epsilon^{t-1}$. If we set $-\bfy = \bfx = (1 - \epsilon^{t-1}, 0, \ldots, 0)^T$, then 
    \begin{align}
        \cosh^{-1} \Big( 1 + 2 \frac{||\bfx - \bfy||^2}{(1 - ||\bfx||^2) (1 - ||\bfy||^2)} \Big) &= \cosh^{-1} \Big( 1 + 4 \frac{(1 - \epsilon^{t-1})^2}{(1 - (1 - \epsilon^{t-1})^2)^2} \Big) \\
        &\geq \cosh^{-1} \Big( 1 + \frac{2}{4\epsilon^{2t - 2} - 4\epsilon^{3t - 3} + \epsilon^{4t-4}} \Big) \\
        &\geq \cosh^{-1} \bigg( 1 + \frac{2}{\epsilon^{2t-2}} \Big) \\
        &= \log \Big( 1 + \frac{1}{2\epsilon^{2t - 2}} + \sqrt{\Big(1 + \frac{1}{2\epsilon^{2t - 2}}\Big)^2 - 1} \bigg) \\
        &\geq \log \Big( \frac{1}{\epsilon^{t-1}} \Big) \\
        &= (1 - t) \log (\epsilon) \\
        &= (t - 1) |\log(\epsilon)|,
    \end{align}
    which shows that we can compute a distance that is bounded from below by $\mathcal{O}(t)$. Similar steps can be used to show that the distance is also bounded from above by a $\mathcal{O}(t)$ term.
\end{proof}

\section{Proof of Theorem \ref{thm:atanh_accuracy}}
\label{sec:atanh_accuracy_proof}

\begin{theorem*}
    Given a ulp-nonoverlapping FPE $x = \sum_{i=1}^t x_i \in [-1 + \epsilon^{t - 1}, 1 - \epsilon^{t - 1}]$ consisting of floating point numbers with a precision $b > t$, Algorithm \ref{alg:atanh} leads to an approximation $y$ of the inverse hyperbolic tangent of $x$ that satisfies
    \begin{equation}
        |y - \tanh^{-1} (x)| \leq \epsilon^*,
    \end{equation}
    for some small $\epsilon^* > 0$.
\end{theorem*}

\begin{proof}
    The accuracy of the $x \in (-0.5, 0.5)$ branch of the algorithm follows easily from the accuracy of the algorithm for normal floating point numbers and the error bounds of the FPE routines from \cite{popescu2017towards}, similar to the proof in Appendix \ref{sec:acosh_accuracy_proof}. The other branch can be a bit more problematic, due to the large derivatives near the boundary of the domain. For $0.5 \leq |x| < 1 - \epsilon^{t-1}$, we use
    \begin{equation}
        \tanh^{-1} (x) = 0.5 \cdot \sign(x) \cdot \log \Big( 1 + \frac{2 |x|}{1 - |x|} \Big).
    \end{equation}
    Let $z$ denote the argument of the logarithm, so
    \begin{equation}
        z = 1 + \frac{2|x|}{1 - |x|},
    \end{equation}
    and let $\tilde{z} = \tilde{z}_1 + \ldots + \tilde{z}_t$ denote that approximation of $z$ obtained through FPE operations. Due to the error bounds given in \citep{popescu2017towards}, for FPEs with $t$ terms on the domain $0.5 \leq |x| < 1 - \epsilon^{t-1}$ we can assume that
    \begin{equation}
        \frac{|z - \tilde{z}|}{|z|} < 2 \epsilon,
    \end{equation}
    where $\epsilon$ is the machine precision of the floating point terms. Now, since $|\tilde{z}_2| \leq \text{ulp}(\tilde{z}_1) = \epsilon |\tilde{z}_1|$, we can write
    \begin{align}
        |z - \tilde{z}_1| &\leq |z - \tilde{z}| + \Big| \sum_{i=2}^t \tilde{z}_i \Big| \\
        &\leq 2\epsilon |z| + 2 |\tilde{z}_2| \\
        &\leq 2\epsilon |z| + 2 \epsilon |\tilde{z}_1| \\
        &\leq 4\epsilon |z| + 2 \epsilon |\tilde{z}_1 - z|,
    \end{align}
    which can be rewritten as 
    \begin{equation}
        |z - \tilde{z}_1| \leq \frac{4\epsilon}{1 - 2\epsilon} |z| \leq 8\epsilon |z|.
    \end{equation}
    This shows that we can write $\tilde{z}_1 = (1 + \delta) z$, with $|\delta| < 8 \epsilon$. Now, the error of the largest magnitude term approximation of the logarithm is
    \begin{align}
        \Big| y - 0.5 \cdot \sign(x) \cdot \log (z) \Big| &= \Big| 0.5 \cdot \sign(\tilde{x}) \cdot \log (\tilde{z}_1) - 0.5 \cdot \sign(x) \cdot \log (z) \Big| \\
        &= 0.5 \cdot \Big| \log\Big(\frac{z}{\tilde{z}}\Big) \Big| \\
        &= 0.5 \cdot \Big| \log\Big(\frac{\tilde{z}_1}{z}\Big) \Big| \\
        & = 0.5 \cdot \Big| \log\Big(\frac{(1 + \delta) z}{z}\Big) \Big| \\
        &= 0.5 \cdot |\log(1 + \delta)| \\
        &\leq 0.5 \cdot |\delta| \\
        &\leq 4\epsilon.
    \end{align}
    Lastly, we introduce some error through the approximation of the natural logarithm. However, as long as no overflow occurs, this error is typically bounded by the machine precision. Therefore, if we can approximate $z$ well enough, then we can guarantee an accurate computation of $\tanh^{-1}$. So combining this result with the error bounds from \citep{popescu2017towards} concludes the proof.
\end{proof}

\section{Proof of Proposition \ref{thm:atanh_range}}
\label{sec:atanh_range_proof}
\begin{proposition*}
    The range of algorithm \ref{alg:atanh} increases linearly in the number of terms $t$.
\end{proposition*}

\begin{proof}
    The maximal values that we can encounter occur near the boundary of the domain, so set $x = 1 - \epsilon^{t-1}$. Then,
    \begin{align}
        0.5 \cdot \sign(x) \cdot \log\Big( 1 + \frac{2|x|}{1 - |x|} \Big) &= 0.5 \cdot \log\Big(1 + \frac{2 - 2\epsilon^{t-1}}{\epsilon^{t-1}}\Big) \\
        &\leq 0.5 \cdot \log\Big(\frac{\epsilon^{t-1} + 2}{\epsilon^{t-1}}\Big) \\
        &\leq 0.5 \cdot \log \Big( \frac{e}{\epsilon^{t-1}} \Big) \\
        &= 0.5 \cdot (1 - (t - 1) \log(\epsilon)) \\
        &= 0.5 \cdot (1 + (t - 1) |\log (\epsilon)|),
    \end{align}
    which shows that the range is bounded from above by $\mathcal{O}(t)$. A similar argument leads to a $\mathcal{O}(t)$ lower bound, showing that the range indeed increases linearly in the number of terms $t$.
\end{proof}



\section{Binary tree embedding results for varying dimensions}\label{sec:bin_tree_dim_res}
Table \ref{tab:bin_tree_dim_res} shows results of the embedding of a binary tree with float32 representations in 4, 7, 10 or 20 dimensions. Here, we have also tested an additional objective similar to MAM, where we use the cosines of the angles instead of the angles. We find that MAM generally leads to the best or close to the best results for each choice of dimensions.

\input{tables/binary_tree_embeddings}

\section{Embedding \texorpdfstring{$m$}{m}-ary trees in varying dimensions}\label{sec:m_tree_dim_res}
Tables \ref{tab:m_tree_dim_res_ave} and \ref{tab:m_tree_dim_res_wc} show results of the embedding of various $m$-ary trees in dimensions 4, 7, 10 and 20, similar to Table \ref{tab:n_h_tree_comp}. We find that MS-DTE gives the best results overall.

\input{tables/m_tree_dim_res}

\section{Embedding phylogenetic trees in varying dimensions}\label{sec:phylo_tree_dim_res}
Additional experiments involving the phylogenetic trees with embedding dimensions 4, 7, 10 and 20 are shown in Tables \ref{tab:phylo_tree_dim_res_ave_1}, \ref{tab:phylo_tree_dim_res_ave_2}, \ref{tab:phylo_tree_dim_res_wc_1} and \ref{tab:phylo_tree_dim_res_wc_2}. We observe that the precomputed points method struggles to separate points for higher dimensions, leading to higher distortion. Moreover, we find that HS-DTE gives the best results overall in every setting.

\newpage

\input{tables/phylo_tree_dim_res}

\section{Statistics of the trees used in the experiments}\label{sec:tree_analysis}
Some statistics of the trees that are used in the experiments are shown in Table \ref{tab:tree_analysis}. Most notably, these statistics show that the true number of optimizations that has to be performed is significantly lower than the worst-case number of optimizations given by Theorem \ref{thm:mhs_complexity}. To see this, note that an optimization step using MAM has to be performed each time a node is encountered with a degree that did not appear before. The result of this optimization step can then be cached and used for each node with the same degree.

\input{tables/tree_analysis}


\section{Graph and tree-like graph embedding results}\label{sec:graphs}
The graphs that we test our method on are a graph detailing relations between diseases \citep{goh2007human} and a graph describing PhD advisor-advisee relations \citep{de2018exploratory}. In order to embed graphs with the combinatorial constructions, the graphs need to be embedded into trees first. Following \citep{sala2018representation}, we use \citep{abraham2007reconstructing} for the graph-to-tree embedding. The results of the subsequent tree embeddings are shown in Table \ref{tab:tree_like_graphs}. These distortions are with respect to the tree metric of the embedded tree instead of with respect to the original graph. This is to avoid mixing the influence of the tree-to-hyperbolic space embedding method with that of the graph-to-tree embedding.

From these results we again see that HypFPE + HS-DTE outperforms all other methods. However, it should be noted that graphs cannot generally be embedded with arbitrarily low distortion in hyperbolic space and that the graph to tree embedding method will introduce significant distortion. Hyperbolic space is not a suitable target for embedding a graph that is not tree-like. Therefore, we define our method as a tree embedding method and not as a graph embedding method.

\input{tables/graph_embeddings}


\section{FPE arithmetic}\label{sec:fpe_arithmetic}
\begin{algorithm}
    \caption{FPEAddition}\label{alg:fpe_add}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} FPEs $x = x_1 + \ldots + x_n$, $y = y_1 + \ldots + y_m$ and number of output terms $r$.
        \STATE $f \gets \text{MergeFPEs}(x, y)$
        \STATE $s \gets \text{FPERenormalize}(f, r)$
        \RETURN $s = s_1 + \ldots + s_r$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{MergeFPEs}\label{alg:merge_fpe}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} FPEs $x = x_1 + \ldots + x_n$, $y = y_1 + \ldots + y_m$.
        \STATE $z \gets \text{Concatenate}(x, y)$
        \STATE Sort terms in $z$ in ascending order with respect to absolute value.
        \RETURN Sorted $z = \{z_1, \ldots, z_{n + m}\}$.
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{FPERenormalize}\label{alg:fpe_renorm}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} List of floating point numbers $x = x_1, \ldots, x_n$ and number of output terms $r$.
        \STATE $e \gets \text{VecSum}(x)$
        \STATE $y \gets \text{VecSumErrBranch}(e, r)$
        \RETURN $y = y_1 + \ldots + y_r$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{VecSum}\label{alg:vecsum}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} List of floating point numbers $x_1, \ldots, x_n$.
        \STATE $s \gets x_n$
        \FOR{$i \in \{n - 1, \ldots, 1\}$}
            \STATE $(s, e_{i + 1}) \gets \text{2Sum}(x_i, s)$
        \ENDFOR
        \STATE $e_1 \gets s$
        \RETURN $e_1, \ldots, e_n$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{VecSumErrBranch}\label{alg:vecsumerrbranch}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} List of floating point numbers $e_1, \ldots, e_n$ and number of output terms $m$.
        \STATE $j \gets 1$
        \STATE $\epsilon \gets e_1$
        \FOR{$i \in \{1, n - 1\}$}
            \STATE $(r_j, \epsilon) \gets \text{2Sum}(\epsilon, e_{i + 1})$
            \IF{$\epsilon \neq 0$}
                \IF{$j \geq m$}
                    \RETURN $r_1, \ldots, r_m$
                \ENDIF
                \STATE $j \gets j + 1$
            \ELSE
                \STATE $\epsilon \gets r_j$
            \ENDIF
        \ENDFOR
        \IF{$\epsilon \neq 0$ \AND $j \leq m$}
            \STATE $r_j \gets \epsilon$
        \ENDIF
        \RETURN $r_0, \ldots, r_m$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{2Sum}\label{alg:2sum}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} floating point numbers $x$ and $y$.
        \STATE $s \gets \text{RN}(x + y) \quad$ where RN is rounding to nearest
        \STATE $x' \gets \text{RN}(s - y)$
        \STATE $y' \gets \text{RN}(s - x')$
        \STATE $\delta_x \gets \text{RN}(x - x')$
        \STATE $\delta_y \gets \text{RN}(y - y')$
        \STATE $e \gets \text{RN}(\delta_x + \delta_y)$
        \RETURN $(s, e)$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Fast2Sum}\label{alg:fast2sum}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Floating point numbers $x$ and $y$ with $\lfloor \log_2 |x| \rfloor \geq \lfloor \log_2 |y| \rfloor$
        \STATE $s \gets \text{RN}(x + y)$
        \STATE $z \gets \text{RN}(s - x)$
        \STATE $e \gets \text{RN}(y - z)$
        \RETURN $(s, e)$
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
    \caption{FPEMultiplication}\label{alg:fpe_mult}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} FPEs $x = x_1 + \ldots + x_n, y = y_1 + \ldots + y_m$, number of output terms $r$, bin size $b$ and precision $p$ (for float64: $b = 45, p = 53$).
        \STATE $t_{x_1} \gets \lfloor \log_2 |x_1| \rfloor$
        \STATE $t_{y_1} \gets \lfloor \log_2 |y_1| \rfloor$
        \STATE $t \gets t_{x_1} + t_{y_1}$
        \FOR{$i \in \{1, \ldots, \lfloor r \cdot p / b \rfloor + 2\}$}
            \STATE $B_i \gets 1.5 \cdot 2^{t - ib + p - 1}$
        \ENDFOR
        \FOR{$i \in \{1, \ldots, \min(n, r + 1)\}$}
            \FOR{$j \in \{1, \ldots, \min(m, r + 1 - i)\}$}
                \STATE $(\pi', e) \gets \text{2Prod}(x_i, y_j)$
                \STATE $\ell \gets t - t_{x_i} - t_{y_i}$
                \STATE $sh \gets \lfloor \ell / b \rfloor$
                \STATE $\ell \gets \ell - sh \cdot b$
                \STATE $B \gets \text{Accumulate}(\pi', e, B, sh, \ell)$
            \ENDFOR
            \IF{$j < m$}
                \STATE $\pi' \gets x_i \cdot y_j$
                \STATE $\ell \gets t - t_{x_i} - t_{y_j}$
                \STATE $sh \gets \lfloor \ell / b \rfloor$
                \STATE $\ell \gets \ell - sh \cdot b$
                \STATE $B \gets \text{Accumulate}(\pi', 0, B, sh, \ell)$
            \ENDIF
        \ENDFOR
        \FOR{$i \in \{1, \ldots, \lfloor r \cdot p / b \rfloor + 2\}$}
            \STATE $B_i \gets B_i - 1.5 \cdot 2^{t - ib + p - 1}$
        \ENDFOR
        \STATE $\pi \gets \text{VecSumErrBranch}(B, r)$
        \RETURN $\pi_1 + \ldots + \pi_r$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Accumulate}\label{alg:accumulate}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Floating point numbers $\pi', e$, list of floating point numbers $B$ and integers $sh, \ell$.
        \STATE $c \gets p - b - 1$
        \IF{$\ell < b - 2c - 1$}
            \STATE $(B_{sh}, \pi') \gets \text{Fast2Sum}(B_{sh}, \pi')$
            \STATE $B_{sh + 1} \gets B_{sh + 1} + \pi'$
            \STATE $(B_{sh+1}, e) \gets \text{Fast2Sum}(B_{sh+1}, e)$
            \STATE $B_{sh + 2} \gets B_{sh + 2} + e$
        \ELSIF{$\ell < b - c$}
            \STATE $(B_{sh}, \pi') \gets \text{Fast2Sum}(B_{sh}, \pi')$
            \STATE $B_{sh + 1} \gets B_{sh + 1} + \pi'$
            \STATE $(B_{sh + 1}, e) \gets \text{Fast2Sum}(B_{sh + 1}, e)$
            \STATE $(B_{sh+2}, e) \gets \text{Fast2Sum}(B_{sh+2}, e)$
            \STATE $B_{sh + 3} \gets B_{sh + 3} + e$
        \ELSE
            \STATE $(B_{sh}, p) \gets \text{Fast2Sum}(B_{sh}, \pi')$
            \STATE $(B_{sh + 1}, \pi') \gets \text{Fast2Sum}(B_{sh + 1}, \pi')$
            \STATE $B_{sh + 2} \gets B_{sh + 2} + \pi'$
            \STATE $(B_{sh+2}, e) \gets \text{Fast2Sum}(B_{sh+2}, e)$
            \STATE $B_{sh + 3} \gets B_{sh + 3} + e$
        \ENDIF
        \RETURN $B$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{FPEReciprocal}\label{alg:fpe_reciprocal}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} FPE $x = x_1 + \ldots + x_{2^k}$ an number of output terms $2^q$.
        \STATE $r_1 = \text{RN}(\frac{1}{x_1})$
        \FOR{$i \in \{1, \ldots, q\}$}
            \STATE $v \gets \text{FPEMultiplication}(r, x, 2^{i + 1})$
            \STATE $w \gets \text{FPERenormalize}(-v_1, \ldots, -v_{2^{i + 1}}, 2.0, 2^{i + 1})$
            \STATE $r \gets \text{FPEMultiplication}(r, w, 2^{i + 1})$
        \ENDFOR
        \RETURN $r_1 + \ldots + r_{2^q}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{FPEDivision}\label{alg:fpe_division}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} FPEs $x = x_1 + \ldots + x_n$, $y = y_1 + \ldots + y_m$ and number of output terms $r$.
        \STATE $z \gets \text{FPEReciprocal}(y, m)$
        \STATE $\pi \gets \text{FPEMultiplication}(x, z, r)$
        \RETURN $\pi$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{FPE$\tanh^{-1}$}\label{alg:atanh}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} FPE $\tf = \tf_1 + \ldots + \tf_t$.
        \IF{$|\tf| > 1$}
            \RETURN NaN
        \ELSIF{$|\tf| = 1$}
            \RETURN $\infty$
        \ELSIF{$|\tf| < 0.5$}
            \RETURN $0.5 \cdot \sign(\tf) \cdot \log (1 + 2 |\tf| + \frac{2 |\tf| \cdot |\tf|}{1 - |\tf|})$\alglinelabel{ln:log_line_1}
        \ELSE
            \RETURN $0.5 \cdot \sign(\tf) \cdot \log (1 + \frac{2 |\tf|}{1 - |\tf|})$\alglinelabel{ln:log_line_2}
        \ENDIF
    \end{algorithmic}
\end{algorithm}

