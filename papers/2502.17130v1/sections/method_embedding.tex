\section{HS-DTE}%
\label{sec:method_embedding}
\vspace{-0.2cm}
\paragraph{Setting and objective.}
We are given a (possibly weighted) tree $T = (V, E)$, where the nodes in $V$ contain the concepts of our hierarchy and the edges in $E$ represent parent-child connections. The goal is to find an embedding $\phi: V \rightarrow \mathbb{D}^n$ that accurately captures the semantics of the tree $T$, so where $T$ can be accurately reconstructed from $\phi(V)$. An embedding $\phi$ is evaluated by first defining the graph metric $d_T (u, v)$ on the tree as the length of the shortest path between the nodes $u$ and $v$ and then checking how much $\phi$ distorts this metric. More specifically, for evaluation we use the average relative distortion \citep{sala2018representation}, the worst-case distortion \citep{sarkar2011low} and the mean average precision \citep{nickel2017poincare}. Further details on these metrics can be found in Appendix \ref{sec:metrics}.


\vspace{-0.2cm}

\paragraph{Constructive solution for hyperbolic embeddings.}
The starting point of our method is the Poincaré ball implementation of Sarkar's combinatorial construction \citep{sarkar2011low} as outlined by \citep{sala2018representation}. A generalized formulation of this approach is outlined in Algorithm \ref{alg:sarkar}. The scaling factor $\tau > 0$ is used to scale the tree metric $d_T$. A larger $\tau$ allows for a better use of the curvature of hyperbolic space, theoretically making it easier to find strong embeddings. Lower values can help avoid numerical issues that arise near the boundary of the Poincaré ball. When the dimension of the embedding space satisfies $n \leq \log (\text{deg}_{\max}) + 1$ and the scaling factor is set to
\begin{equation}\label{eq:tau_nd}
    \tau = \frac{1 + \epsilon} {\epsilon} \log \Big( 4 \; \text{deg}_{\max}{}^{\frac{1}{n-1}} \Big),
\end{equation}
with $\text{deg}_{\max}$ the maximal degree of $T$, then the construction leads to a worst-case distortion bounded by $1 + \epsilon$, given that the points on the hypersphere are sufficiently uniformly distributed \citep{sala2018representation}. When the dimension is $n > \log (\text{deg}_{\max}) + 1$, the scaling factor should be $\tau = \Omega(1)$, so it can no longer be reduced by choosing a higher dimensional embedding space \citep{sala2018representation}. The number of bits required for the construction is $\mathcal{O}(\frac{1}{\epsilon} \frac{\ell}{n} \log (\text{deg}_{\max}))$ when $n \leq \log (\text{deg}_{\max}) + 1$ and $\mathcal{O} (\frac{\ell}{\epsilon})$ when $n > \log (\text{deg}_{\max}) + 1$, where $\ell$ is the longest path in the tree.


\begin{algorithm}[h]
    \caption{Generalized Sarkar's Dalaunay tree embedding}\label{alg:sarkar}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Tree $T = (V, E)$ and scaling factor $\tau > 0$.
        \FOR{$v \in V$}
            \STATE $p \gets \text{parent}(v)$
            \STATE $c_1, \ldots, c_{\text{deg}(v) - 1} \gets \text{children}(v)$
            \STATE Reflect $\phi(p)$ with $R_{\phi(v) \rightarrow \mathbf{0}}$ \alglinelabel{ln:constr_refl_parent}
            \STATE Generate $\bfx_1, \ldots, \bfx_{\text{deg}(v)}$ uniformly distributed points on a hypersphere with radius $1$ \alglinelabel{ln:hyperspherical_gen}
            \STATE Get rotation matrix $A$ such that $R_{\phi(v) \rightarrow \mathbf{0}} \big(\phi(p)\big)$ is aligned with $A \bfx_{\text{deg}(v)}$ and rotate
            \STATE Scale points by $\gamma = \frac{e^\tau - 1}{e^\tau + 1}$ \alglinelabel{ln:constr_scale}
            \STATE Reflect rotated and scaled points back: $\phi(c_i) \gets R_{\phi(v) \rightarrow \mathbf{0}} (\gamma A\bfx_i), \quad i = 1, \ldots, \text{deg}(v) - 1$ \alglinelabel{ln:constr_refl_children}
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\paragraph{The difficulty of distributing points on a hypersphere.}
The construction in Algorithm \ref{alg:sarkar} provides a nice way of constructing embeddings in $n$-dimensional hyperbolic space with arbitrarily low distortion. However, the bound on the distortion for the $\tau$ in Equation \ref{eq:tau_nd} is dependent on our ability to generate uniformly distributed points on the $n$-dimensional hypersphere. More specifically, given generated points $\bfx_1, \ldots, \bfx_{\text{deg}_{\max}}$, the error bound relies on the assumption that
\begin{equation}\label{eq:min_angle}
    \min_{i \neq j} \; \sin \angle(\bfx_i, \bfx_j) \geq \text{deg}_{\max}{}^{-\frac{1}{n-1}}.
\end{equation}
While this assumption can theoretically always be achieved \citep{sala2018representation}, generating points that actually satisfy it is not straightforward. In practice it is important to keep the scaling factor $\tau$ as small as possible, since the required number of bits increases linearly with $\tau$. Increasing the minimal angle beyond the condition in Equation \ref{eq:min_angle} allows for a smaller $\tau$. This problem of maximizing the minimal pairwise angle is commonly known as the Tammes problem \citep{tammes1930origin,mettes2019hyperspherical} and it has been studied extensively, with many analytical and approximate solutions given for various specific combinations of dimensions $n$ and number of points that have to be placed \citep{cohn2024spherical}. However, there exists no general closed-form solution that encompasses all possible combinations.

\citep{sala2018representation} propose to generate points by placing them on the vertices of an inscribed hypercube. This approach comes with three limitations. First, the maximum number of points that can be generated with this method is $2^n$, which is limited for small $n$. Second, for most configurations this method results in a sub-optimal distribution, leading to an unnecessarily high requirement on $\tau$. Third, this method depends on finding binary sequences of length $n$ with maximal Hamming distances (see Appendix \ref{sec:maximal_hamming_dists}), which is in general not an easy problem to solve. Their solution is to use the Hadamard code. This can only be used when the dimension is a power of 2 and at least $\text{deg}_{\max}$; a severe restriction, often incompatible with downstream tasks. 

\vspace{-0.2cm}

\paragraph{Delaunay tree embeddings with separation.}
We propose to improve the construction by distributing the points on the hypersphere in step \ref{ln:hyperspherical_gen} of Algorithm \ref{alg:sarkar} through optimization. Specifically, we use projected gradient descent to find $\bfx_1, \ldots, \bfx_k \in S^{n - 1}$ such that
\begin{equation}
    \bfx_1, \ldots, \bfx_k = \argmin_{\bfw_1, \ldots, \bfw_k \; \in \; S^{n - 1}} L (\bfw_1, \ldots, \bfw_k),
\end{equation}
where $L: (S^{n-1})^{k} \rightarrow \mathbb{R}$ is some objective function. Common choices for this objective are the hyperspherical energy functions \citep{liu2018learning}, given by
\begin{equation}
    E_{s} (\bfw_1, \dots, \bfw_k) = 
    \begin{cases}
        \sum\limits_{i = 1}^k \sum\limits_{j \neq i} ||\bfw_i - \bfw_j||^{-s}, & s > 0, \\
        \sum\limits_{i = 1}^k \sum\limits_{j \neq i} \log\big( ||\bfw_i - \bfw_j||^{-1} \big), & s = 0,
    \end{cases}
\end{equation}
where $s$ is a nonnegative integer parameterizing this set of functions. Minimizing these objective functions pushes the hyperspherical points apart, leading to a more uniform distribution. However, these objectives are aimed at finding a large mean pairwise angle, allowing for the possibility of having a small minimum pairwise angle. Having a small minimum pairwise angle leads to the corresponding nodes and their descendants being placed too close together, leading to large distortion, as shown in the experiments. Therefore, we advocate the minimal angle maximization (MAM) objective, aimed at maximizing this minimal angle
\begin{equation}
    E(\bfw_1, \ldots, \bfw_k) = - \sum_{i = 1}^k \min_{j \neq i} \angle(\bfw_i, \bfw_j),
\end{equation}
which pushes each $\bfw_i$ away from its nearest neighbour and essentially optimizes directly for the objective of the Tammes problem. We find that this method results in strong separation when compared to highly specialized existing methods used for specific cases of the Tammes problem \citep{cohn2024spherical}. More importantly, it leads to better separation than the method used in current hyperbolic embeddings \citep{sala2018representation}, allowing for the use of a smaller $\tau$. Moreover, this optimization method places no requirements on the dimension, making it a suitable choice for downstream tasks. We refer to the resulting construction as the highly separated Delaunay tree embedding (HS-DTE).

When performing the construction using MAM, the output of the optimization can be cached and reused each time a node with the same degree is encountered. Using this approach, the worst-case number of optimizations that has to be performed is $\mathcal{O}(\sqrt{N})$ as shown by Theorem \ref{thm:mhs_complexity}. 

\begin{theorem}\label{thm:mhs_complexity}
    The worst-case number of optimizations $p$ that has to be performed when embedding a tree with the combinatorial construction in Algorithm \ref{alg:sarkar} with any objective using caching is
    \begin{equation}
        p \leq \Big\lceil \frac{1}{2} (1 + \sqrt{16N - 15}) \Big\rceil.
    \end{equation}
\end{theorem}

\vspace{-0.4cm}
\begin{proof}
    See Appendix \ref{sec:mhs_proof}.
\end{proof}
\vspace{-0.4cm}

In practice we find the number of optimizations to be lower due to frequent occurrence of low degree nodes for which cached points can be used, as shown in Appendix \ref{sec:tree_analysis}.

\textbf{MAM optimization details.} MAM is an easily optimizable objective, that we train using projected gradient descent for 450 iterations with a learning rate of 0.01, reduced by a factor of 10 every 150 steps, for every configuration. This optimization can generally be performed in mere seconds which, if necessary, can be further optimized through hyperparameter configurations, early stopping, parallelization or hardware acceleration. As a result, the increase in computation time of our method compared to \citep{sala2018representation} is minimal. Moreover, when compared to methods such as Poincaré embeddings \citep{nickel2017poincare} which use stochastic gradient descent to directly optimize the embeddings, we find that our method is orders of magnitude faster, while avoiding the need for costly hyperparameter tuning. 

