\section{HypFPE: High-precision GPU-compatible hyperbolic embeddings}
\label{sec:method_float_expansions}
While hyperbolic space enjoys numerous potential benefits, it is prone to numerical error when using floating point arithmetic. Especially as points move away from the origin, floating point arithmetic struggles to accurately represent or perform computations with these points. For larger values of $\tau$ or maximal path lengths $\ell$, the embeddings generated by the construction often end up in this problematic region of the Poincaré ball. As such, the precision required for hyperbolic embeddings is often larger than the precision provided by the floating point formats supported on GPUs. Increased precision can be attained by switching to arbitrary precision arithmetic. However, this makes the result incompatible with existing deep learning libraries.

Here, we propose HypFPE, a method to increase the precision of constructive hyperbolic approaches through floating point expansion (FPE) arithmetic. In this framework, numbers are represented as unevaluated sums of floating point numbers, typically of a fixed number of bits $b$. In other words, a number $f \in \mathbb{R}$ is represented by a floating point expansion $\tf$ as
\begin{equation}
    f \approx \tf = \sum_{i=1}^t \tf_i,
\end{equation}
where the $\tf_i$ are floating point numbers with a fixed number of bits and where $t$ is the number of terms that the floating point expansion $\tf$ consists of. Each term $\tf_i$ is in the form of a GPU supported float format, such as float16, float32 or float64. Moreover, to ensure that this representation is unique and uses bits efficiently, it is constrained to be ulp-nonoverlapping \citep{popescu2017towards}.
\begin{definition}
    A floating point expansion $\tf = \tf_1 + \ldots + \tf_t$ is ulp-nonoverlapping if for all $2 \leq i \leq t$, $|\tf_i| \leq \text{ulp}(\tf_{i - 1})$, where $\text{ulp}(\tf_{i - 1})$ is the \textit{unit in the last place} of $\tf_{i - 1}$.
\end{definition}
A ulp-nonoverlapping FPE consisting of $t$ terms each with $b$ bits precision has at worst $t(b-1) + 1$ bits of precision, since exactly $t - 1$ overlapping bits can occur.
The corresponding arithmetic requires completely different routines for computing basic operations, many of which have been introduced by \citep{joldes2014computation, joldes2015arithmetic, muller2016new, popescu2017towards}. An overview of these routines can be found in Appendix \ref{sec:fpe_arithmetic}. For an overview of the error guarantees we refer to \citep{popescu2017towards}. Each of these routines can be defined using ordinary floating point operations that exist for tensors in standard tensor libraries such as PyTorch, which are completely GPU compatible. Here, we have generalized all routines to tensor operations and implemented them in PyTorch by adding an extra dimension to each tensor containing the terms of the floating point expansion.

\vspace{-0.1cm}

\paragraph{Applying FPEs to the construction.}
In the constructive method the added precision is warranted whenever numerical errors lead to large deviations with respect to the hyperbolic metric. In other words, if we have some $\bfx \in \mathbb{D}^n$ and its floating point representation $\tilde{\bfx}$, then it makes sense to increase the precision if
\begin{equation}
    d_{\mathbb{D}} (\bfx, \tilde{\bfx}) \gg 0.
\end{equation}
For the Poincaré ball, this is the case whenever $\bfx$ lies somewhere close to the boundary of the ball. In our construction, this means that generation and rotation of points on the unit hypersphere can be performed in normal floating point arithmetic, since the representation error in terms of $d_{\mathbb{D}}$ will be negligible. However, for large $\tau$, the scaling of the hypersphere points and the hyperspherical inversion require increased precision as these map points close to the boundary of $\mathbb{D}^n$. Specifically, steps \ref{ln:constr_refl_parent}, \ref{ln:constr_scale} and \ref{ln:constr_refl_children} of Algorithm \ref{alg:sarkar} may require increased precision. Note that these operations can be performed using the basic operation routines shown in Appendix \ref{sec:fpe_arithmetic}.
From the basic operations, more complicated nonlinear operations can be defined through the Taylor series approximations that are typically used for floating point arithmetic. To compute the distortion of the resulting embeddings, the distances between the embedded nodes must be computed either through the inverse hyperbolic cosine formulation of Equation \ref{eq:poin_dist_acosh} or through the inverse hyperbolic tangent formulation of Equation \ref{eq:poin_dist_atanh}. We show how to accurately compute distances using either formulation.

\subsection{The inverse hyperbolic cosine formulation}
\vspace{-0.1cm}
For Equation \ref{eq:poin_dist_acosh}, normal floating point arithmetic may cause the denominator inside the argument of $\cosh^{-1}$ to become 0 due to rounding. To solve this, we can use FPE arithmetic to compute the argument of $\cosh^{-1}$ and then approximate the distance by applying $\cosh^{-1}$ to the largest magnitude term of the FPE. This allows accurate computation of distances even for points near the boundary of the Poincaré ball, as shown by Theorem \ref{thm:acosh_accuracy} and Proposition \ref{thm:acosh_range}. 

\begin{theorem}\label{thm:acosh_accuracy}
    Given $\bfx, \bfy \in \mathbb{D}^n$ with $||\bfx|| < 1 - \epsilon^{t - 1}$ and $||\bfy|| < 1 - \epsilon^{t - 1}$, an approximation $d$ to equation \ref{eq:poin_dist_acosh} can be computed with FPE representations with $t$ terms and with a largest magnitude approximation to $\cosh^{-1}$ such that, for some small $\epsilon^* > 0$,
    \begin{equation}
        \bigg|d - \cosh^{-1} \bigg( 1 + 2 \frac{||\bfx - \bfy||^2}{(1 - ||\bfx||^2) (1 - ||\bfy||^2)} \bigg)\bigg| < \epsilon^*.
    \end{equation}
\end{theorem}
\vspace{-0.4cm}
\begin{proof}
    See Appendix \ref{sec:acosh_accuracy_proof}.
\end{proof}
\vspace{-0.2cm}
\begin{proposition}\label{thm:acosh_range}
    The range of the inverse hyperbolic tangent formulation increases linearly in the number of terms $t$ of the FPEs being used.
\end{proposition}
\vspace{-0.4cm}
\begin{proof}
    See Appendix \ref{sec:acosh_range_proof}.
\end{proof}
\vspace{-0.2cm}
Theorem \ref{thm:acosh_accuracy} shows that we can accurately compute distances on a larger domain than with normal floating point arithmetic. Proposition \ref{thm:acosh_range} shows that the effective radius of the Poincaré ball in which we can represent points and compute distances increases linearly in the number of terms of our FPE expansions. Therefore, this effective radius increases linearly with the number of bits. The same holds for arbitrary precision floating point arithmetic, so FPE expansions require a similar number of bits for constructive methods as arbitrary precision floating point arithmetic.

\subsection{The inverse hyperbolic tangent formulation}
For Equation \ref{eq:poin_dist_atanh}, the difficulty lies in the computation of $\tanh^{-1}$. With normal floating point arithmetic, due to rounding errors, this function can only be evaluated on $[-1 + \epsilon, 1 - \epsilon]$, where $\epsilon$ is the machine precision. This severely limits the range of values, i.e., distances, that we can compute. Therefore, we need to be able to compute the inverse hyperbolic tangent with FPEs. Inspired by \citep{felker2024musl}, we propose a new routine for this computation, given in Algorithm \ref{alg:atanh} of Appendix \ref{sec:fpe_arithmetic}. Here, we approximate the logarithm in steps \ref{ln:log_line_1} and \ref{ln:log_line_2} as $\log(\tf) \approx \log(\tf_1)$, which is accurate enough for our purposes. This algorithm can be used to accurately approximate $\tanh^{-1}$ while extending the range linearly in the number of terms $t$ as shown by Theorem \ref{thm:atanh_accuracy} and Proposition \ref{thm:atanh_range}.



\begin{theorem}\label{thm:atanh_accuracy}
    Given a ulp-nonoverlapping FPE $x = \sum_{i=1}^t x_i \in [-1 + \epsilon^{t - 1}, 1 - \epsilon^{t - 1}]$ consisting of floating point numbers with a precision $b > t$, Algorithm \ref{alg:atanh} leads to an approximation $y$ of the inverse hyperbolic tangent of $x$ that, for small $\epsilon^* > 0$, satisfies
    \begin{equation}
        |y - \tanh^{-1} (x)| \leq \epsilon^*.
    \end{equation}
\end{theorem}
\vspace{-0.4cm}
\begin{proof}
    See Appendix \ref{sec:atanh_accuracy_proof}.
\end{proof}
\vspace{-0.2cm}

\begin{proposition}\label{thm:atanh_range}
    The range of algorithm \ref{alg:atanh} increases linearly in the number of terms $t$.
\end{proposition}
\vspace{-0.4cm}
\begin{proof}
    See Appendix \ref{sec:atanh_range_proof}.
\end{proof}
\vspace{-0.2cm}
Based on these results, either formulation could be a good choice for computing distances with FPEs. In practice, we find that the $\tanh^{-1}$ formulation leads to larger numerical errors, which is likely due to catastrophic cancellation errors in the dot product that is performed in Equation \ref{eq:mob_add}. Therefore, we use the $\cosh^{-1}$ formulation in our experiments.


