\section{Threat Analyses and limitations}
\label{threat}

Our selection of the two subject datasets, namely, Devign, and CodeChef with their associated code models, might threaten the external validity of our experimental conclusions. We tried to alleviate this threat by following efforts: 
(1) the two datasets are very popular and have been widely used in relevant research~\cite{lu2021codexglue,tian2023code,yang2022natural,carrot}; (2) their associated DL models are commonly used in SE tasks; (3) these datasets and models differ from each other by varying topics, %(e.g., BERT mainly focuses on NL data while codeBERT focuses on both NL-PL data), 
labels (from two to four), and model accuracies (from 61.56\% to 81.98\%), which make these subjects diverse and representative. Therefore, our experimental conclusion should generally hold, although specific data could be inevitably different for other subject. 


Threats to external validity may arise from the techniques selected for experimental comparisons, including uncertainty metrics\textcolor{blue}{~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline,gal2016dropout, alon2019code2vec,xiao2019quantifying,vasudevan2019towards,corbiere2019addressing, monarch2021human, steinhardt2016unsupervised,shannon1948mathematical}} and dissector~\cite{wang2020dissector}. Due to inherent differences, existing methods for image data (SelfChecker~\cite{xiao2021selfchecking} and InputReflector~\cite{xiao2022repairing}) cannot be directly applied to code data. Therefore, we chose dissector as a baseline for input validation, as it identifies out-of-scope inputs similarly to our focus. Additionally, we sampled a subset of uncertainty metrics to assess their effectiveness in identifying out-of-scope inputs \textcolor{blue}{~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline,gal2016dropout, alon2019code2vec,xiao2019quantifying,vasudevan2019towards,corbiere2019addressing, monarch2021human, steinhardt2016unsupervised,shannon1948mathematical}}.

%%%edited Threats to external validity may also arise from our selected techniques for the experimental comparisons, which include uncertainty metrics~\cite{hendrycks2018baseline, guo2017calibration, monarch2021human,shannon1948mathematical,steinhardt2016unsupervised}, and dissector~\cite{wang2020dissector}. We argued earlier, applying existing work image data cannot be directly done on code data due to the inherent differences among the two types of data. Existing work cannot be applied on code data directly (SelfChecker~\cite{xiao2021selfchecking} and InputReflector~\cite{xiao2022repairing}). Therefore, we select dissector as a baseline for input validation because it resembles our focus by identifying out-of-scope inputs. In addition to dissector, we sampled a subset of uncertainty metrics to analyze the effectiveness of identifying out-of-scope inputs~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline, li2021estimating, alon2019code2vec, vasudevan2019towards,corbiere2019addressing}.

%The work was published in 2019 as the representative of the state-of-the-art techniques. Note that dissector does not provide the source code for sub-model generation, therefore, we designed our sub-model generation on code data that utilizes code models. In addition to dissector, we sampled a subset of existing uncertainty metrics to analyze the effectiveness of identifying out-of-scope inputs. These sampled metrics are widely used in existing computer vision-based techniques~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline, li2021estimating, alon2019code2vec, vasudevan2019towards,corbiere2019addressing}
%Our primary internal threat stems from the lack of ground truths for distinguishing out-of-scope inputs from in-scope inputs, limitations on applying CodeImprove to different subjects, and the use of various program transformation rules. We simulated out-of-scope and in-scope inputs using mispredictions and correct predictions, respectively. While this estimation may be rough, the logic holds (RQ2).
%Regarding the lack of subject diversity, our goal was to assess our proposed technique across different SE tasks and code models. We selected a subset of SE tasks and code models for our evaluation (RQ1, RQ3, and RQ4). In the future, we plan to extend CodeImprove to other subjects such as clone detection and functionality classification.
%Currently, CodeImprove employs 15 semantic-preserving program transformation rules. In the future, we plan to add more transformation rules and update our project website with the latest results. Given the necessary diversity of our subjects, as discussed, we believe our experimental conclusions are generally applicable, and the techniques used in CodeImprove can be leveraged for future research.



Our internal threat mainly comes from the lack of ground truths for distinguishing out-of-scope inputs from in-scope inputs, limitations on applying CodeImprove in different subjects, and applying different program transformation rules. We used mispredictions and correct predictions to simulate out-of-scope inputs and in-scope inputs respectively. Such estimation can be rough, however the logic may holds (RQ2). Regarding the lack of subject matter, our assumption was to investigate how our propose technique performed on different SE tasks and code models. Therefore, we select a subset of SE tasks and code models (RQ1, RQ3 and RQ4). In future, we plan to extend and apply CodeImprove on other subjects matter (e.g. clone detection, functionality classification etc.). \textcolor{blue}{CodeImprove uses 15 semantic-preserving transformation rules, but we plan to add more and update our website with new results. Due to the diversity of our subjects, we believe our conclusions are generally applicable, and CodeImprove can support future research.}

%Currently, CodeImprove employs 15 semantic preserving program transformation rules. However, in future, we plan to add more transformation rules and update our project website with the latest results.
%Therefore, considering our subjects have necessary diversities as discussed above, our experimental conclusions can hold generally and we believe that the technique applied in CodeImprove can be used for future research.

%We believe that the technique applied in CodeImprove can be used for future research.% however, in future, we plan to add more transformation rules and update our project website with the latest results. 
%Therefore, considering our subjects have necessary diversities as discussed above, our experimental conclusions can hold generally and we believe that the technique applied in CodeImprove can be used for future research. %that can help to confront the code data shifts and improve our genetic based algorithm to manage large search space. Therefore, considering our subjects have necessary diversities as discussed above, our experimental conclusions can hold generally and we believe that the technique applied in CodeImprove can be used for future research. 

\vspace{-1em}

