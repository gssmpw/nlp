%\section{Detecting Numerical Bug}
\section{Evaluation}
\label{evaluation}
\input{tex/tables/performance}

We conducted all our experiments on a server equipped with an Intel Xeon E5-26 CPU and eight NVIDIA 1080 Ti GPUs. We set up 0.3 and 0.2 as the threshold based on our validation score on detecting out-of-scope inputs for vulnerability detection and defect prediction tasks, respectively, on all subjects. We set up maximum iteration to three and different crossover rates (i.e., 0.16, 0.33, 0.66, and 1)  when applying sequences of transformation operators on each subject matter described in Section~\ref{evaluation}.

%We set up 0.3 and 0.2 as the threshold based on our validity score on detecting out-of-scope inputs for vulnerability detection and defect prediction tasks respectively on all subjects. We set up maximum iteration to three, and crossover rate of 0.5 when applying program transformations on each subject matter. 

\subsection{Research Questions }
%We aim to answer the following research questions in the next section:

\noindent\textbf{RQ1. Overall Performance: }What is the overall performance of CodeImprove?

\noindent\textbf{RQ2. Input Validation: } How effective is the out-of-scope program data detection? 

\noindent\textbf{RQ3. Input Adaptation: } How effective to convert out-of-scope data to become in-scope data?

\noindent\textbf{RQ4. Setting Sensitivity: }
How sensitive is CodeImprove's performance under different experimental setups? 

%%\noindent\textcolor{blue}{\textbf{RQ5. Submodel Decomposition: }How effective is the submodel generation in detecting out-of-scope data detection?}

%\textbf{RQ3. Layer Selection:} Does the choice of layers for selection have an impact on out-of-scope data detection?
\noindent\textbf{RQ5. Semantic Preserving:} Does program transformations of CodeImprove preserve semantics?


%\textbf{RQ5. Transformation:} How effective are the program transformations on out-of-scope data?

\noindent\textbf{RQ6. Runtime Overhead:} What is the overhead of CodeImprove in adapting a program to DL models?

%\textbf{RQ6. Controlling Factors: } What factors contribute to the impact on the performance of DL models?





%\textbf{RQ2.} What is the accuracy of the model with transformed programs?


\subsection{Subjects}
\label{subjects}
\subsubsection{Datasets and Tasks}
CodeImprove is evaluated on two code-based classification tasks: vulnerability prediction with devign dataset ~\cite{zhou2019devign}, and defect prediction with codeChef dataset~\cite{phan2017conv}.~\textcolor{blue}{Although this study used two datasets, CodeImprove applies to all code model-based tasks, with plans for broader future evaluations.}

%{While these two datasets were selected for this study, CodeImprove is applicable to all code model-based tasks, and we plan to evaluate CodeImprove on a broader range of tasks in the future.}

%To evaluate CodeImprove, we consider two code-based classification tasks and two associated datasets (i.e., vulnerability prediction with devign dataset ~\cite{zhou2019devign}, and defect prediction with codeChef dataset~\cite{phan2017conv}).   


%The statistics of datasets are shown at the first four columns in Table~\ref{table:datasets}, each of which represents the task, the number of classes for the classification task, the class names, the number of samples in each class, the number of inputs in the training/validation/test set, and the programming language for the inputs. 

For \textbf{vulnerability detection}, the Devign dataset\cite{zhou2019devign} consists of 27,318 functions extracted from FFmpeg and Qemu open-source C projects. The functions are labeled as containing vulnerabilities or being clean, with 14,858 vulnerable and 12,460 clean samples. The dataset is split into train/validation/test sets with sizes 21,854/2,732/2,732.

For \textbf{defect prediction}, the CodeChef dataset includes 33,822 C/C++ functions from the CodeChef platform~\cite{phan2017conv}. \textcolor{blue}{Samples are labeled with categories such as}
%Samples are labeled as having 
no defect, wrong output, timeout error, or runtime error, with 11,362 no defect, 13,656 wrong output, 5,101 timeout error, and 3,703 runtime error samples. The dataset is divided into train/validation/test sets with sizes 21,647/5,411/6,764.








%The task of \textbf{vulnerability detection} aims to predict whether a given code snippet contains vulnerabilities. We use the dataset that was prepared by \cite{zhou2019devign}. The dataset is extracted from two popular open-sourced C projects: FFmpeg and Qemu, consisting of 27,318 functions that are labeled as either containing vulnerabilities or clean. The number of vulnerable samples are 14858 and clean samples are 12460. The dataset consists of 21854/2732/2732 as train/validation/test samples. The task of \textbf{defect prediction} aims to predict whether a given code snippet is defective and its defect type. We use the CodeChef dataset that was prepared by ~\cite{phan2017conv}. The dataset is extracted from the CodeChef platform that contains 33822 C/C++ functions that are labeled as no defect, wrong output, timeout, or runtime error. The dataset consists of 11362 no defect samples, 13656 wrong output samples, 5101 timeout error samples, and 3703 runtime error samples. The dataset consists of 21647/5411/6764 as train/valid/test samples. 


%This dataset is included as part of the CodeXGLUE benchmark~\cite{} that has been used to investigate the effectiveness of CodeBERT for vulnerability prediction. CodeXGLUE divides the dataset into training, development and test set that we reuse in this study. 
 


%\input{tex/tables/datasets}

\subsubsection{Models}
We employed state-of-the-art pre-trained models, namely CodeBERT~\cite{fengCodeBERTPreTrainedModel2020}, RoBERTa~\cite{Liu2019RoBERTa}, and GraphCodeBERT~\cite{guo2021graphcodebert} which have been widely utilized in previous studies~\cite{lu2021codexglue,tian2023code,yang2022natural,carrot}. These models were fine-tuned on our tasks using the corresponding datasets, adhering to the recommended settings proposed in previous literature~\cite{lu2021codexglue}. Hyper-parameters were set to match the original configurations. \textcolor{blue}{CodeImprove is designed for use with all types of code models.} \textcolor{blue}{Our study includes a diverse range of tasks, pre-trained models, and class numbers, ensuring a comprehensive evaluation of CodeImprove's performance.}
%Our study consists of a diverse range of subjects, spanning different tasks, pre-trained models, and class numbers. This diversity ensures a comprehensive evaluation of CodeImprove's performance.


%These models have been used in the existing work that adapts code models~\cite{lu2021codexglue,tian2023code,yang2022natural,carrot}. We fine-tuned these models on the two tasks based on the corresponding datasets respectively, following the settings recommended by the existing work ~\cite{lu2021codexglue}. We set the hyperparameters similar to the original setup. %Table~\ref{table:parameters} shows hyperparameters of each model on each task. For each task, we imply the same hyperparameters for each model. 

%Overall, the subjects used in our study are diverse, involving different tasks, different pre-trained models, and different numbers of classes, It is very helpful to sufficiently evaluate the performance of CodeImprove.


%%We employed state-of-the-art pre-trained models, namely CodeBERT [41], RoBERTa [42], and GraphCodeBERT [9], which have been widely utilized in previous studies that focus on code model adaptation [4], [33], [43], [44]. These models were fine-tuned on our tasks using the corresponding datasets, adhering to the recommended settings outlined in previous literature [4]. Hyperparameters were set to match the original configurations.

%%Our study encompasses a diverse range of subjects, spanning different tasks, pre-trained models, and class numbers. This diversity ensures a comprehensive evaluation of CodeImprove's performance.







%%%%ISSTA \subsection{Baseline Techniques: }
%%%%ISSTA \subsubsection{Techniques employed to evaluate overall performance }

%ISSTA We compared CodeImprove with other search based techniques namely, random search (\textbf{CodeImprove-rand}), Hill climbing algorithm (\textbf{CodeImprove-HC}),and \textcolor{red}{A-star search (\textbf{CodeImprove-A*})}. \textbf{CodeImprove-rand} is designed to apply random transformations until identifying the optimal candidate. \textbf{CodeImprove-HC} is designed based on the princliples of hill climbing algorithm. The search strategy begins with an initial solution (i.e., obtained through a random transformation). Subsequently, it iteratively applies single transformations to this solution while computing the fitness score. If the current solution surpasses the threshold for fitness score, the algorithm terminates, having achieved the optimal solution. 

%%%%ISSTA \subsubsection{Techniques employed to evaluate input validation }We compared CodeImprove's DSMD with the Cross-Layer Dissection (\textbf{CLD})~\cite{wang2020dissector}. Here, we utilized the validity score in DSMD, however employed the similar sub-model generation technique in ~\cite{wang2020dissector}.

%\textbf{CodeImprove-PSO} is designed based on the princliples of particle swarm optimization algorithm. The process begins by defining each particle as a transformation. We initialize a swarm of particles where each particle's position is a generated by applying the transformation. 












%\input{tex/tables/parameters}





\subsection{Evaluation Metrics }

We use a diverse set of metrics to measure CodeImprove’s effectiveness for our six RQs.   

%\textbf{Accuracy:} Accuracy = $\frac{TP+TN}{TP+TN+FP+FN}$. 
The \textbf{accuracy (A)} is the proportion of correctly classified samples out of all samples. %TN represents the number of true negatives and TP + TN + FN + FP represents the number of all samples.
 %\textbf{Precision:} Precision = $\frac{TP}{TP+FP}$. 
The \textbf{precision (P)} is the percentage of correctly predicted positive samples out of all positive predictions. %TP and FP denote the number of true positives and false positives, respectively. 
%\textbf{Recall:} Recall = $\frac{TP}{TP+FN}$. 
The \textbf{recall (R)} measures the percentage of correctly predicted positive samples that were retrieved out of all actual positive samples.  %TP and FN denote the number of true positives and false negatives, respectively.
%\textbf{F1-Score:} F1 = $\frac{2*Precision*Recall}{Precision+Recall}$ = $\frac{2*TP}{2*TP+FP+FN}$. 
The \textbf{F1 score (F1)} is the harmonic mean of precision and recall. 
\textbf{Relative improvement (RI)} quantifies the accuracy improvement relative to the difference between training and test accuracy.

%is measured by the amount of accuracy improved over the difference between training accuracy and test accuracy.

% \textbf{Relative Improvement: }RI = $\frac{\text{New Accuracy} - \text{Orginal Accuracy}}{\text{Training accuracy- Original Accuracy}}*100$

\textbf{ Correction success rate (CSR)}~\cite{tian2023code}
is the ratio of successfully corrected mispredictions to the total identified mispredictions. \textbf{Mis-correction rate (MCR)}~\cite{tian2023code} measures the negative effect caused, which is the ratio of correct predictions changed to mispredictions to the total number of correct predictions in the test set.

%measures the ability to correct mispredictions, which is the ratio of the number of inputs whose mispredictions are successfully corrected to the total number of identified mispredicted inputs. \textbf{Mis-correction rate (MCR)} measures the negative effect caused, which is the ratio of the number of inputs whose correct predictions are changed to mispredictions to the total number of inputs with correct predictions in the test set.

\textbf{ Correction validation rate (CVR)} is the ratio of successfully validated mispredictions to the total possible mispredicted inputs to be validated. \textbf{Mis-correction validation rate (MVR)} is the ratio of correct predictions validated as mispredictions to the total number of correct predictions in the test set. \textbf{AUC} score evaluates the effectiveness of the input validation process. \textbf{Transformations per Second (TPS)} measures the rate of transformations CodeImprove can apply per second. Next, we will describe the results of the experiments.


%%%measures the ability to validate correct mispredictions, which is the ratio of the number of inputs whose mispredictions are successfully validated to the total number of possible mispredicted inputs to be validated. \textbf{Mis-correction validation rate (MVR)} measures the negative effect caused, which is the ratio of the number of inputs whose correct predictions are validated as mispredictions to the total number of inputs with correct predictions in the test set. \textbf{AUC} score to evaluate our effectiveness of input validation process \textbf{AUC} score to evaluate our effectiveness of input validation process. \textbf{Transformations per Second (TPS)} measures the number of transformations that CodeImrpove is capable of apply per second.%We compute the AUC scores similar to the approach in Section~\ref{study}.


%%%ISSTA%%%%For RQ 1), We first measure the improvements in model performance using our technique. However, we realize that the training set up of the code models for the SE tasks posses few limitations. Despite utilizing the original implementations from \cite{lu2021codexglue} and maintaining consistency with the training process and hyperparameters, we observed that the training accuracy of the code models falls below our expectations. For example, on vulnerability detection dataset shows only a training accuracy of 70.45\%, 71.28\%, and 79.8\% in accuracy for GraphCodeBERT, RoBERTa, and CodeBERT respectively. This limitation on training accuracy can impose constraints on achieving higher test accuracy. While addressing this issue may be straightforward, we deliberately chose to conduct experiments using the original setup to minimize potential biases.


%However, we realize that the training set up of the code models for the SE tasks posses few limitations. Although we use the original implements~\cite{lu2021codexglue} without changing the training process or any hyper-parameters, we find that the code models training accuracy is less than what we expected. For example, on vulnerability detection dataset shows only a training accuracy of 70.45\%, 71.28\%, and 79.8\% in accuracy for GraphCodeBERT, RoBERTa, and CodeBERT respectively. Therefore, the highest possible test accuracy we could achieve is constrained. Although fixing this issue may be trivial, we wanted to experiment on the original set up to ignore any bias. 

%%%%ISSTA%%%%As a result, we design an metric to evaluate the effectiveness of our approach. We named it as relative improvement (RI). RI is measured by the amount of accuracy improved with CodeImprove over the difference between training accuracy and test accuracy. Training accuracy is necessary due to the limitations imposed by these models. Although our technique can be efficient, we may not achieve a better test accuracy than the training accuracy. Therefore, RI is formulated as;



%We use the following four commonly used metrics to measure CodeImprove’s overall performance (RQ 1):

%%%%OOOPSLA
% \textbf{Accuracy:} Accuracy = $\frac{TP+TN}{TP+TN+FP+FN}$. The accuracy measures the percentage of correctly classified samples out of all samples. TN represents the number of true negatives and TP + TN + FN + FP represents the number of all samples.

% \textbf{Precision:} Precision = $\frac{TP}{TP+FP}$. The precision measures the percentage of correctly predicted positive samples out of all the positive samples that were predicted that are retrieved. TP and FP denote the number of true positives and false positives, respectively. 


% \textbf{Recall:} Recall = $\frac{TP}{TP+FN}$. The recall measures the percentage of correct positive prediction sample that were retrieved out of all positive predictions. TP and FN denote the number of true positives and false negatives, respectively.

% \textbf{F1-Score:} F1 = $\frac{2*Precision*Recall}{Precision+Recall}$ = $\frac{2*TP}{2*TP+FP+FN}$. The F1 score
% is the harmonic mean of precision and recall metrics.

%%%%OOPSLA

%\noindent For RQ 2), we compute \textbf{AUC} score to evaluate our validity score metric. We compute the AUC scores similar to the approach in Section~\ref{study}.

%\textbf{Area Under Curve (AUC):} AUC is computed based on True Positive Rate (TPR) and False Positive Rate (FPR) data to measure how effective a technique is in distinguishing out-of-scope inputs from in-scope inputs. We compute the AUC scores similar to the approach in Section~\ref{study}.


%oopsla
% \noindent For RQ 4), we define the \textbf{Relative Decrease in Accuracy} as the evaluation metric:

% \textbf{Relative Decrease in Inaccuracy (RDI):} 

% RDI = $\frac{\text{New Accuracy} - \text{Orginal Accuracy}}{\text{Original Incorrectly Predicted Percentage}} * 100$. The RDA measures the percentage of the accuracy change out of the original incorrectly predicted input percentage. 
%end oopsla


%%%%OOOPSLA
%\subsection{Experimental Process}



% For RQ1 (performance): We compute the accuracy, precision, recall, and F1 score for each subject in Section~\ref{subjects} with CodeImprove. We compare CodeImprove with the evaluation of the original set-up and a CodeImprove-Random which applies 10 random transformations on out-of-scope data. 

% For RQ2 (Input Validation): We calculate AUC values to compare the validity score with the dissector for their effectiveness in distinguishing beyond-inputs from within-inputs. Also, we include more baseline approaches in our preliminary study. 

% For RQ3 (Layer Selection): We compute the number of out-of-scope data detected for a given threshold score by choosing sub-models with different layers. In total, all code models in our study contain 12 sub-models. We select one, four, seven, and twelve sub-models randomly for our evaluation. For each sub-model, we compute the AUC scores. 

% For RQ4 (Transformation): We compute the IDP for CodeImprove with CodeImprove-random and apply a random transformation on out-of-scope data. 

% For RQ5 (Overhead): We compute the average timing overhead to adapt out-of-scope data by CodeImprove on two SE tasks: vulnerability detection, and defect detection. We compute the average time with regard to the total time to adapt an input divided by the total number of inputs involved in the out-of-scope data. 

%%%%OOOPSLA



% In this section, we demonstrate the effect of data distribution shift on model performance under two programming language tasks that target different properties of source code: code summarization (CS) [18] and code completion (CC). They are both implemented using the datasets in Section III-B.

% For clarification, there are other programming language tasks such as authorship identification, API search or code clone detection, but the datasets used in these tasks requires large labelling effort. Since our study focus on distribution shift and our shifted data are manually generated, we only consider CS and CC in this paper. 
% Code Summarization. The first task is source code summarization and more specifically, we consider predicting method names according to method bodies. We follow [18] to configure a path-attention network architecture for code prediction tasks and evaluate by the accuracy metric following work [32]. 
% Code Completion. The second task is to predict the missing code based on existing context. We follow [33] to configure a multilayer perceptron (MLP) architecture for code prediction tasks and apply accuracy as the evaluation metric.




%\subsection{Experimental Process}


% Software engineering tasks: Defect detection, Clone detection

% Datasets: Defect detection: Devign dataset
% Clone detection: POJ-104 

% Models: CodeBERT, Roberta, Bert, distilbert

% Evaluation metric: defect detection - accuracy
% clone detection F1 score. 
%Next, we will describe the results of the experiments.

\section{Results and Analysis}
\label{results}
We report and analyze experimental results, and answer the preceding research questions in turn.

%\input{tex/tables/RI}



\subsection{RQ1: Overall Performance of CodeImprove}
%\input{tex/tables/evaluation}

%We discuss the results of the effectiveness of applying program transformations on the out-of-scope data. Table ~\ref{Tab:evaluation} represents the statistics of our experimental results. In addition to Table~\ref{Tab:evaluation}, we also show the statistics of number of inputs that program transformation has been applied, the number of correctly predicted and incorrectly predicted inputs of the total inputs by comparing those to the ground truth data, and number of transformed inputs that convert a correct prediction to incorrect vice versa in Table~\ref{Tab:stts}.


\textit{Baseline}: CodeImprove is the first technique to improve the code model's performance through program transformations. Thus, we cannot find direct baselines for comparison. To address this, we draw inspiration from a technique in the image domain for comparative analysis, i.e. the InputReflector \textcolor{blue}{\textbf{(IRef)}}~\cite{xiao2022repairing} that detects deviating inputs and substitutes them with the most similar sample from the training set. We are unable to include CodeDenoise~\cite{tian2023code} as a baseline due to the evaluation methods of this work, which splits the test set into two subsets as T1 and T2 and subsequently assesses results on T2 set. However, the splitting criteria of datasets is not provided in the project website. Moreover, the work is similar to the adversarial style, which denoises the program identifiers and corrects the program with the supervision of the model's predictions. CodeImprove demonstrates better performance compared to CodeDenoise~\cite{tian2023code} where it only fixes 20.45\% of inputs while CodeImprove fixes 32.8\% for defect prediction task on the CodeBERT model.

%We borrowed the ideas from two techniques in the image domain for comparison. Input reflector (IR)~\cite{xiao2022repairing} and interpretable technique (IT)~\cite{mohasel2023interpretable} identifies deviating inputs and replace with the samples in training set. %We ignore the work in ~\cite{tian2023fly} because the work identifies noisy identifiers in the dataset by applying perturbations and then clease these identifiers. Although, such technique can help in improving model performance, it requires to generate multiple samples for input validation by applying perturbations for multiple identifiers. Moreover, the work splits the test set into two sets, thus making the sample size even smaller. 

\textit{Process}: \textcolor{blue}{\textbf{IRef}} employs two models, namely the siamese network~\cite{he2018twofold} and the quadruple network~\cite{xiao2022repairing}, to detect deviating inputs and repair them, respectively. During training, these models rely on three datasets: the original set, the transformed (human recognizable) set, and the extremely transformed (human unrecognizable) set. These two transformed sets are generated by applying different degrees of transformation to the original training set. However, generating such datasets for code data poses challenges as code inputs cannot rely on a degree of transformation like in image data. Therefore, we utilized only two sets: the original set and its transformed version. We configured the loss functions employed in the \textcolor{blue}{IRef} accordingly for these two sets. We leverage the hidden layer outputs from the original model and feed them to the siamese network and quadruple network. Subsequently, we search for similar data in the training set to repair the out-of-scope inputs by exchanging the labels of the most similar training sample. For CodeImprove, we employed all 15 transformations during the crossover step. The effectiveness is measured using various metrics, including accuracy, precision, recall, F1-score, relative improvement (RI), correction success rate (CSR), and mis-correction rate (MCR).   
%%edited Table~\ref{Tab:performance} presents a detailed comparison between CodeImprove and \textcolor{blue}{IRef}, illustrating that CodeImprove consistently outperforms \textcolor{blue}{IRef} across various metrics. 

\textit{Result: }Table~\ref{Tab:performance} \textcolor{blue}{presents} the comparison \textcolor{blue}{between CodeImprove and \textcolor{blue}{IRef}, illustrating that CodeImprove consistently outperforms \textcolor{blue}{IRef}.}
%results comparing CodeImprove with IR. From this table, we found that CodeImprove always performs better than IR.
\textcolor{blue}{Notably}, we observe the following: (1) CodeImprove consistently achieved the best model improvements ranging upto 8.78\% in accuracy, 8.48\% in precision, 16.9\% in recall, and 13.5\% in F1-score on all the subjects; (2) CodeImprove is capable of correcting around 23.1\% to 39.9\% of the mispredicted inputs on both vulnerability detection and defect prediction tasks; (3) The RI measurement shows significant improvement for RoBERTa models on both the tasks. On other models, CodeImprove shows \textcolor{blue}{a RI ranging from} 21.1\% to 51.28\%; (4) Techniques employed in the image domain \textcolor{blue}{\textbf{(IRef)}} cannot be applied to code data\textcolor{blue}{. Our results indicate that \textcolor{blue}{IRef} negatively impacts the}, 
%as our results show that IR hurts
 model performance for the CodeBERT model on both vulnerability detection and defect prediction tasks. Moreover, the defect prediction task shows performance discrepancy on all models for \textcolor{blue}{\textbf{IRef}}. One of the reasons is that, \textcolor{blue}{\textbf{IRef}}'s analyzer component is better at detecting %uncertainty measurements~\cite{hu2023codes,li2021estimating} are for detecting 
out-of-distribution (i.e., data from a different distribution from training data) inputs, and is able to repair such inputs. %extremely transformed images.
However, our aim is to maintain the same distribution while identifying inputs that are prone to misprediction. Thus, \textcolor{blue}{\textbf{IRef}} fails to capture out-of-scope inputs adequately due to the similarity in syntax and semantics between transformed and original code; and
%So IR fails to capture out-of-scope inputs because a transformed version of a program can have similar syntax and semantics compared to original code; 
(5) The negative impact of CodeImprove is minimal. At most, CodeImprove will only mispredict 2.6\% of the correct predictions to become mispredictions. \textcolor{blue}{CodeImprove successfully adapts out-of-scope inputs to in-scope inputs, as demonstrated in Table~\ref{Tab:performance}.}
\begin{tcolorbox}[title=\textbf{RQ1} - What is the overall performance of CodeImprove?, left=2pt, right=2pt,top=2pt,bottom=2pt]
CodeImprove was effective in adapting out-of-scope inputs for both SE tasks on three subject models with higher accuracy, precision, recall, F1-score, CSR, and RI.(Table~\ref{Tab:performance}). 
\end{tcolorbox}
%We can conclude that CodeImprove is able to successfully adapt out-of-scope inputs to become in-scope-inputs as \textcolor{blue}{shown} in Table~\ref{Tab:performance}.

%IR trains two models, namely the siamese network, to identify deviating inputs and the quadruple network to repair the deviating input. During the training, these models require three datasets: original set, transformed set, and extremely transformed set. However, generating such datasets for code data is not easy. Therefore, we only employed two sets: an original set and a transformed version of it. We set up the triplet and quadruple loss functions for these two sets. For IT, we leverage marginal confidence (i.e., the retrieve of the highest two softmax values from the test set) as the uncertainty method to identify whether the prediction is certain or not. Then, we search for similar data for the identified out-of-scope input from the training set to fix the input. For CodeImprove, we ran the original setup by employing all 15 transformations during the crossover step. 

%Our subject consists of 18 experiments for two SE tasks. \textcolor{red}{For Input reflector, we changed th/....}


%We defined three variants of CodeImprove: CodeImprove-I1 represents applying identifier renaming transformation on the best identifier with highest importance score, CodeImprove-Iall represents applying identifier renaming transformation on all the identifiers in a code snippet, and CodeImrpove-S represents applying structure transformations on the input as described by algo~\ref{alg:GA}. 


%%%ISSTA In addition to measuring RI (Table~\ref{Tab:ri}), we evaluated the total number of out-of-scope inputs identified by CodeImprove. Although our guiding metric is designed to detect out-of-scope inputs, it is possible for in-scope inputs to be validated as out-of-scope. To provide a comprehensive analysis, we present the counts of correctly predicted and mispredicted inputs by comparing them with the ground truth labels. %Furthermore, we calculated the number of inputs that can transition from being mispredicted to correctly predicted, and vice versa, across the three CodeImprove variants.


%there can be inputs that are correctly predicted but fall into this category. Therefore, we show the number of correctly predicted and mispredicted inputs by comparing with the ground truth labels. Next, we compute the number of inputs that can convert from mispredicted to correctly predicted vice versa on the three variants.  



%%%ISSTA From the analysis of Table~\ref{Tab:ri}, we observe the following: (1) CodeImprove demonstrates notable efficacy in structure-based transformations for CodeBERT and RoBERTa models for both SE tasks (from 29.36\% to 48.63\%); (2) CodeBERT exhibits better improvements for defect detection tasks by applying identifier renaming transformation showing 25.69\% of RI for renaming one identifier and 25.52\% of RI for all identifiers; (3) On average GraphCodeBERT shows comparatively lower RI percentages for all identifier renaming transformation; (4) RoBERTa records the lowest RI value for renaming one identifier on defect detection task (i.e.; 0.68); (5) In general, structure-based transformations show promising results compared to identifier renaming transformations. Of course, renaming an identifier will only change the identifier name while maintaining the code structure same. However, these findings affirm that deep code models effectively learn code patterns, thereby indicating that structure based transformations yield better improvements. 
 

%%%issta From the analysis of Table~\ref{newstats}, we observe the following; (1) first, CodeImprove's validity score detected a significant number of out-of-scope inputs (ranging from 779 to 966); (2) Of the identified out-of-scope inputs majority of inputs are mispredicted inputs (from 571 to 779 mispredicted inputs) with a relatively lower number of correctly predicted inputs; (3) CodeImprove's structure based transformations on CodeBERT models actually make the model to change the predictions (i.e., higher changes for $M\rightarrow C$ and $C\rightarrow M$); (4) In all cases, number of transformations from $M\rightarrow C$ is higher than $C\rightarrow M$; (5) Notably, CodeImprove's program transformations lead the model to alter the predictions (as low as 13.4\% to as high as 48.6\%), thus confirming CodeImprove can adapt program inputs. 

%%%issta Our experimental results confirm that CodeImprove is better at validating and adapting out-of-scope inputs. We will continue to add more transformation rules and improve the CodeImprove's effectiveness on more diverse SE tasks with different datasets. 
%%%issta \input{tex/tables/newstats}



%FTable~\ref{Tab:ri} shows the RI \% of CodeImprove. Especially structure based transformations show higher RI percentage for both CodeBERT and RoBERTa models on two SE tasks. This confirms that the 

% oopsla We discuss the results of our study on the effectiveness of applying program transformations on out-of-scope data. Our experimental results are summarized in Table ~\ref{Tab:evaluation}, which provides statistics on various performance metrics. Additionally, Table~\ref{Tab:stts} shows the number of inputs on which program transformation has been applied, the number of correctly and incorrectly predicted inputs, and the number of transformed inputs that convert a correct prediction to incorrect and vice versa.

%% From the Table~\ref{Tab:evaluation}, we observe that: (1) for each of the subjects, CodeImprove always achieved the best accuracy values, e.g: from 62.74 to 68.99 and 81.98 to 84.15 on CodeBERT, from 61.56 to 63.99 and for 80.02 to 82.03 on RoBERTa, and from 59.95 to 63.47 and from 76.35 to 79.15 on BERT for both SE tasks respectively; (2) CodeImprove always achieved best precision values for both SE task on each subject, precision increase ranging from 1.71\% to 6.61\%; (3) CodeImprove achieved best recall on each subject, recall increase ranging from 1.44\% to 11.39\% ; (4) CodeImprove achieves best F1-score on each subject, F1-score increase from 2.10\% to 9.83\%;(5) Although CodeImprove-random achieved closer performance in accuracy, precision, recall, and F1-score compared to CodeImprove on both SE tasks,  CodeImprove still performed slightly better with the RoBERTa model, and (6) On CodeBERT and BERT models, CodeImprove achieved better performance compared to CodeImprove-random on both SE tasks.

%%% OOPSLA
%From the analysis of Table~\ref{Tab:evaluation}, we observe the following: (1) CodeImprove consistently achieved the best accuracy values for each of the subjects, with accuracy increasing from 62.74 to 68.99 and 81.98 to 84.15 on CodeBERT, from 61.56 to 63.99 and from 80.02 to 82.03 on RoBERTa, and from 59.95 to 63.47 and from 76.35 to 79.15 on BERT for both vulnerability detection and defect prediction tasks, respectively; (2) CodeImprove also achieved the best precision values for both SE tasks on each subject, with precision increasing by as much as 6.61\%; (3) CodeImprove achieved the best recall on each subject, with recall increasing by as much as 11.39\%; (4) CodeImprove achieved the best F1-score on each subject, with F1-score increasing by as much as 9.83\%; (5) Although CodeImprove-random achieved closer performance in accuracy, precision, recall, and F1-score compared to CodeImprove on both SE tasks for RoBERTa model,  CodeImprove still performed better than CodeImprove-random; and (6) On CodeBERT and BERT models, CodeImprove outperformed CodeImprove-random on both SE tasks. Overall, our results demonstrate that program transformations can be a highly effective technique for improving the accuracy, precision, recall, and F1-score of software engineering tasks.


%\input{tex/tables/stats}


%Table~\ref{Tab:stts} shows the comparison of the out-of-scope inputs detected by the validity score. These inputs can either be correctly predicted inputs or incorrectly predicted inputs. Therefore, we compare these inputs with their ground truth to confirm the classification of the inputs selected by our validity score. Next, we check the number of correctly predicted samples that made incorrect prediction vice versa after applying the program transformations of CodeImprove. From Table~\ref{Tab:stts}, we observe that: (1) validity score detected 942 and 948 inputs for CodeBERT, 930 and 1045 inputs for RoBERTa, and 1020 and 1306 for BERT on vulnerability detection and defect prediction tasks respectively; (2) out of the total inputs, majority of inputs are incorrectly predicted inputs (i.e., ranging from 557 to 977 incorrectly predicted inputs and from 242 to 373 correctly predicted inputs), resulting that validity score is better at validating mis-classified inputs; (3) of the transformed inputs, CodeImprove is better at transforming incorrectly predicted inputs to correctly predicted inputs on (i.e., 14.0\% to 33.4\% of I to C and 0.14\% to 35.5\% of C to I); (4) RoBERTa model on defect prediction task shows a higher percentage of transforming correct predicted sample to incorrect predicted samples than vice versa; and (5) in general CodeImprove shows promising results in terms of validating the inputs (i.e., detecting high number of incorrectly-predicted inputs) and adapting those inputs to become in-scope inputs (i.e., higher IDP values on each subject matter). 


%In Table~\ref{Tab:stts}, we compare the out-of-scope inputs detected by the validity score and confirm the classification of these inputs by comparing them with their ground truth. We also check the number of correctly predicted samples that made incorrect predictions and vice versa after applying the program transformations of CodeImprove. From the analysis of Table~\ref{Tab:stts}, we observe the following:
%(1) The validity score detected a significant number of inputs, ranging from 930 to 1306, for both vulnerability detection and defect prediction tasks across the three models (CodeBERT, RoBERTa, and BERT); (2) Majority of the inputs are incorrectly predicted inputs, ranging from 557 to 977, resulting in the validity score being better at validating incorrectly predicted inputs; 
%(3) CodeImprove is better at transforming incorrectly predicted inputs to correctly predicted inputs, with an increase ranging from 14.0\% to 33.4\% of incorrectly predicted to correctly predicted inputs and from 0.14\% to 35.5\% of correctly predicted to incorrectly predicted inputs;  (4) The RoBERTa model on the defect prediction task shows a higher percentage of transforming correctly predicted samples to incorrectly predicted samples than vice versa; and (5) In general, CodeImprove shows promising results in terms of validating the inputs (i.e., detecting a high number of incorrectly-predicted inputs) and adapting those inputs to become in-scope inputs (i.e., higher IDP values on each subject matter). Overall, our findings suggest that program transformations can significantly improve the performance of software engineering tasks, particularly in cases where the inputs are validated as out-of-scope inputs.
%%% end oopsla



%\textbf{Dataset and Models: }We continue using the datasets and well-trained DL models in Section IV.
 

%\textbf{Evaluation Metric: } We evaluate the effectiveness based on the model accuracy and the percentage of inaccuracy drop-down. The \% of inaccuracy drop-down is measured by the \% of the difference of accuracy of the models before and after applying program transformations over the original inaccuracy of the model.



\subsection{RQ2: Effectiveness of Out-of-Scope Data Detection}
\label{RQ2}

%\textcolor{red}{Add figures for uncertainty scores, and descriptions}
%%%We evaluate the effectiveness of out-of-scope data detection using the AUC value. Specifically, we compute the AUC values of three variants of dissector that are designed according to different weight growth types (i.e., Dissector-linear $(y = x)$, Dissector-log $(y = ln x)$, and Dissector-exp $(y = e^x )$ and apply the same three variants on our validity score analysis. Table~\ref{Tab:rq2} shows the statistics of the AUC comparison of the three variants of the Validity Score with the Dissector. 
\textit{Baseline:} We compared CodeImprove's DSMG with the Cross-Layer Dissection (\textbf{CLD})~\cite{wang2020dissector}. 
Additionally, we \textcolor{blue}{evaluated} the DSMG approach with the uncertainty metrics in our preliminary study (Section ~\ref{study}): Vanilla, temperature-scaling, predictive entropy, entropy, mutual information, least confidence, ratio confidence, and margin confidence, \textcolor{blue}{monte-carlo dropout, and deep ensemble}. %\textcolor{blue}{For} a more comprehensive study on other uncertainty metrics, \textcolor{blue}{please refer to our project website}~\cite{Data}. %is added to our project website~\cite{Data}. 


\textit{Process:} We compute the AUC score, CVR, and MVR to evaluate the effectiveness of out-of-scope data detection on all baseline approaches.  


\input{tex/tables/validation}

\input{tex/tables/submodel}%on different uncertainty methods compared to with DSMG approach. %Specifically, we compute the AUC values of three variants of dissector~\cite{wang2020dissector} that were designed according to different weight growth types (i.e., Dissector-linear $(y = x)$, Dissector-log $(y = ln x)$, and Dissector-exp $(y = e^x )$. We applied the same three variants on our validity score analysis. The statistics of the AUC comparison of the three variants of the validity score with the dissector are presented in Table ~\ref{Tab:rq2}.
\vspace{-1em}
\textit{Results: } Table ~\ref{Tab:validation} shows the results of out-of-scope data detection. Based on the results, we observe that: (1) CodeImprove achieved higher AUC scores across all models \textcolor{blue}{and tasks} (i.e., AUC 0.781- 0.924); (2) Although CLD obtained better AUC scores compared to other uncertainty metrics, CodeImprove still outperforms CLD; (3) The CVR on DSMG is higher than all other baselines. Moreover, CodeImprove can detect 70.4\% of the out-of-scope inputs for the CodeBERT model on vulnerability detection tasks. CodeImprove consistently outperforms other techniques in terms of CVR on each subject.; (4) The MVR on CodeImprove is lower than other approaches, concluding that CodeImprove is better at differentiating in-scope inputs. MVR for defect prediction task shows 3.0\%, 3.1\%, and 3.3\% for CodeBERT, RoBERTa, and GraphCodeBERT models; \textcolor{blue}{(5) MCD and DE average predictions over multiple forward passes through the same network. This approach limits diversity in the predictions, which may contribute to their poorer performance in AUC compared to CodeImprove}; and (6) other uncertainty metrics did not produce promising results on AUC, CVR, or MVR. For example, predictive entropy obtained a CVR of 43.5\% and an MVR of 35.3\%, which are not significant indicators \textcolor{blue}{of effective performance.} 

%for promising results. % really does not signify the results. 
%%%%\input{tex/tables/rq2}
%From the Table~\ref{Tab:rq2}, we observe that: (1) dissector-linear and validity score-linear obtained similar AUC scores for vulnerability detection task, however, validity-score linear obtained higher AUC score compared to dissector-linear for defect prediction task (i.e., 0.889 vs 0.891 for CodeBERT, 0.828 vs 0.903 for RoBERTa, and 0.875 vs 0.898 for BERT); (2) similar to observation (1), dissector-log obtained similar AUC scores as validity score-log for vulnerability detection task, however, validity score-linear obtained higher AUC scores for RoBERTa and BERT models for defect prediction task (i.e., 0.813 vs 0.819 and 0.789 vs 0.869); (3) dissector-exp on CodeBERT and RoBERTa for vulnerability detection tasks achieved the lowest AUC scores compared to validity score-exp (i.e., 0.523 vs 0.717, and 0.523 vs 0.855); (4) validity score-exp performed well on RoBERTa and BERT (0.887 vs 0.929, and 0.924 vs 0.928), and performed slightly lower on CodeBERT (0.932 vs 0.931) compared to dissector-exp on defect prediction task; (5) the three variants of validity score worked similarly satisfactory, and validity score-exp worked better for vulnerability detection task and validity score-linear for defect prediction task; and (6) compared to existing uncertainty methods discussed in Section~\ref{study}, both dissector and validity score achieved better AUC values (i.e, 0.391-0.601 from the study, 0.523-0.932 for dissector, 0.717-0.931 for validity score).  
%%%Based on the results in Table ~\ref{Tab:rq2}, we can observe that: (1) Dissector-linear and validity score-linear obtained similar AUC scores for the vulnerability detection task. However, validity score-linear obtained a higher AUC score compared to dissector-linear for the defect prediction task (0.889 vs 0.891 for CodeBERT, 0.828 vs 0.903 for RoBERTa, and 0.875 vs 0.898 for GraphCodeBERT); (2) Similar to observation (1), dissector-log obtained similar AUC scores as validity score-log on vulnerability detection task. However, validity score-log obtained higher AUC scores for RoBERTa and GraphCodeBERT models for the defect prediction task (0.819 vs 0.898 and 0.789 vs 0.869); (3) Dissector-exp on CodeBERT and RoBERTa for vulnerability detection tasks achieved the lowest AUC scores compared to validity score-exp (0.523 vs 0.717 and 0.523 vs 0.855); (4) Validity score-exp performed well on RoBERTa and GraphCodeBERT (0.887 vs 0.929 and 0.924 vs 0.928) and slightly lower on CodeBERT (0.932 vs 0.931) compared to dissector-exp on the defect prediction task; (5) the three variants of the validity score worked similarly satisfactorily, and validity score-exp worked better for the vulnerability detection task, while validity score-linear worked better for the defect prediction task; and (6) compared to existing uncertainty methods discussed in Section~\ref{study}, both dissector and validity score achieved better AUC values (0.391-0.601 from the study, 0.523-0.932 for dissector, 0.717-0.931 for validity score).
%Finding 2: Investigation on each layer's effectiveness on decision making is promising: The analysis of partially growing knowledge on the DL model helps in distinguishing out-of-scope- data. Our results show that dissector outperforms other uncertainty measurements on both subjects. Dissector evaluates the effectiveness of each sub-layer on the model's decision making by generating sub-models by confusing the predictions.


\textcolor{blue}{\textit{\textbf{Comparison of Using  Sub-models vs. Hidden State Outputs from the Original Model:}}}
%\textcolor{blue}{Although the submodels are trained by leveraging hidden representation of the original model, accessing the hidden representation directly is possible. Therefore, we conduct an experiment to show the effectiveness of using trained sub-models compared to direct use of hidden representations for out-of-scope data detection.} 
%\textcolor{red}{make it quite clear sub-model decomposition vs hidden states of original model. add epistemic: dropout and emsemble. }
\textcolor{blue}{While sub-models are trained using the layerwise hidden states of the original model, it is also feasible to access these hidden states directly. We conduct an experiment to evaluate the effectiveness of trained sub-models versus the direct use of hidden states from the original model in detecting out-of-scope inputs. Given the high dimensionality of hidden states in the original model, we employed a linear transformation and subsequently applied Equations ~\ref{eq:ic} - ~\ref{eq:pvscore}. Table ~\ref{Tab:hiddenvalidation} presents the statistics of the AUC comparison between these two methods.}

%%Consequently, we conducted an experiment to evaluate the effectiveness of trained sub-models compared to the direct use of hidden state representations for detecting out-of-scope data. Due to the high dimensionality of hidden state representations, we employed a linear transformation without additional training, and then applied equations ~\ref{eq:ic} - ~\ref{eq:pvscore}. Table ~\ref{Tab:hiddenvalidation} presents the statistics of the AUC comparison between these two methods.}


%%While sub-models are trained using the layer-wise hidden state representations of the original model, it is also feasible to access these hidden state representations directly without additional training. To evaluate the effectiveness of trained sub-models compared to the direct use of hidden state representations for detecting out-of-scope data, we conducted an experiment. Given the high dimensionality of hidden state representations, we employed a linear transformation without additional training and subsequently applied Equations 1 to 3. Table V presents the statistics of the AUC comparison between these two methods.




\textcolor{blue}{Based on the results in  Table~\ref{Tab:hiddenvalidation}, we observe that training sub-models (AUC 0.781-0.924) outperforms direct use of hidden states (AUC 0.451 - 0.607) across both software engineering tasks. These results signify the performance of trained sub-models for out-of-scope input validation. We summarize several factors contribute to the lower effectiveness of directly using hidden states: 1) \textbf{Ineffective Feature Utilization and Transformation}: Hidden states are in the form of \textit{(batch\_size, sequence\_length, hidden\_size)}. Effective input validation requires a mapping between the feature space and the class space. Without training a dense layer, this process would merely reduce dimensions without learning this mapping. Training a dense layer allows it to learn the most relevant features from the hidden states and establish an accurate mapping from the feature space to the class space. This reduces significant information loss and enhances overall performance; and 2) \textbf{Lack of Adaptability}: Trained dense layer in sub-models can adapt to the characteristics and distribution of the training data, making them more effective for each SE task. Without training, the model lacks this adaptability, resulting in poor effectiveness.}


%1) \textbf{Lack of feature representation}: The training process of a sub-model allows it to learn and emphasize the most relevant features from the hidden states. However, using hidden states from the original model directly may result in the loss of important features, 2)\textbf{Lack of adaptability: } Trained sub-models can adapt to the characteristics and distribution of the training data, making them more effective for each SE task. Without training, the model lacks this adaptability, resulting in poor effectiveness, and 3) \textbf{Lack of transformation:} The hidden states are in the form of \textit{(batch\_size, sequence\_length, hidden\_size)}. Reducing these high-dimensional features to a lower dimension without training a dense layer fails to capture the relationship between the feature space and the class space during transformation. This can result in significant information loss and poor task performance.}




%%%The training process of a submodel is capable of learn the layer-wise hidden state representations to emphasize the most relevant features while no-training approach may directly use the hidden state representations followed by linear layer for dimensional reduction hence loosing majority of the features, 2) \textbf{Lack of adaptability: } The submodel can adapt to characteristics and distribution of the training data making more effective for the task, while with no training the model lacks adaptability as it does not learn from the data, making it rarely effective, 3) \textbf{Lack of Transformation:} The outputs of each layer hidden state representations are in the form of \textit{(batch\_size, sequence\_length, hidden\_size). Reduing such higher dimension features to a lower dimension without training a linear layer will not capture the relationship between the feature space to class space during the transformation. This can result in a significant loss of information and poor task performance. } }


%The hidden state representations are in the form of \textit{(batch_size, sequence_length, hidden_size)}. Reducing these high-dimensional features to a lower dimension without training a linear layer fails to capture the relationship between the feature space and the class space during transformation. This can result in significant information loss and poor task performance.

%Trained Submodels: The model can adapt to the specific characteristics and distribution of the training data, making it more effective for the task at hand. No Training: The model lacks adaptability as it doesn't learn from the data, leading to a one-size-fits-all approach that is rarely effective.



\begin{tcolorbox}[title=\textbf{RQ2} - How effective is the out-of-scope program data detection?, left=2pt, right=2pt,top=2pt,bottom=2pt]
\textcolor{blue}{CodeImprove effectively distinguishes out-of-scope from in-scope inputs (AUC: 0.781-0.924, CVR: 47.2\%-70.4\%, MVR: 3.0\%-14.7\%), and is more suitable than existing techniques for various SE tasks.}
%CodeImprove was effective in distinguishing out-of-scope inputs from in-scope inputs (AUC: 0.781-0.924, CVR: 47.2\%-70.4\%, and MVR: 3.0\%-14.7\%), and more suitable than existing techniques for different SE tasks with varying accuracies.
\end{tcolorbox}
\vspace{-1em}



%\input{tex/tables/rq3}


\input{tex/tables/evaluation}
\subsection{RQ3: Effectiveness of Search Strategies to Adapt Out-of-Scope Inputs. }

\textit{Baseline:} We employed two search strategies; namely random search (\textbf{CodeImprove-rand})~\cite{zabinsky2009random}, and Hill climbing algorithm (\textbf{CodeImprove-HC})~\cite{selman2006hill}. \textbf{CodeImprove-rand} \textcolor{blue}{applies} random transformations until identifying the optimal candidate. \textbf{CodeImprove-HC} \textcolor{blue}{follows} the principles of the hill climbing algorithm. 


\textit{Process:} For CodeImprove-rand, we randomly apply transformation operators until the algorithm finds the best candidate. CodeImprove-HC begins with an initial solution (i.e., obtained through a random transformation). Subsequently, it iteratively applies a single transformation operator to this solution while computing the fitness score. If the current solution surpasses the threshold for fitness score, the algorithm terminates, having achieved the optimal solution. To be fair, we set up a number of transformations for a solution to 15 in all the techniques. For CodeImprove, each candidate solution \textcolor{blue}{undergoes} all 15 operators during crossover. 

\textit{Results: } Table~\ref{Tab:evaluation} shows the comparison of the three approaches. From this table, we observe that: (1) CodeImprove obtained the best accuracy across all subjects (up to 8.78\%) while both CodeImprove-rand and CodeImprove-HC did not achieve the performance of CodeImprove (up to 2.13\%); (2) Although CodeImprove-HC and CodeImprove-rand improve the model performance, we find that these search algorithms stop at the local minima (i.e., once the algorithm identifies a better candidate, the process terminates). However, CodeImprove will evolve for multiple generations i.e., in our case, is three to find the best candidate; (3) In terms of correcting mispredictions CodeImprove performs the best (i.e.,  CSR up to 39.9\%); (4) Although CodeImprove-rand and CodeImprove-HC shows lower values of MCR, note that its CSR values are really low, therefore, unable to correct mispredictions in a large scale; and (5) In conclusion, CodeImprove is a stable approach to adapt program inputs.  

\begin{tcolorbox}[title=\textbf{RQ3} - How effective to convert out-of-scope data to become in-scope data?, left=2pt, right=2pt,top=2pt,bottom=2pt]
CodeImprove is better at correcting out-of-scope inputs compared to other search algorithms such as random search and hill climbing. 
\end{tcolorbox}



\subsection{RQ4: Influence of Hyper-parameters}
\input{tex/tables/sensitivity}
\textit{Process:} We studies the influence of number of transformation operators ($N$) applied during the crossover for each candidate. We investigated the effectiveness and efficiency of Codeimprove under different settings, i.e., $N$ = \{2, 5, 10, 15\}. We applied variable renaming and one random transformation operator from Table~\ref{table:transformations} for $N=2$, operators 1-5 for $N$=5, operator 1-10 for $N$=10, and operators 1-15 for $N$=15. 


\textit{Results: }Table~\ref{Tab:sensitivity} show the results of CodeImprove under the hyper-parameter settings of $N$ in terms
of CSR, MCR, and RI. \textcolor{blue}{We observed that as N increases, more mispredicted inputs can be corrected, resulting in a larger RI.}
%We found with the setting of $N$ increasing, the more mispredicted inputs can be corrected resulting to a larger RI. 
Meanwhile more correctly predicted inputs are identified as mispredicted ones due to more transformations leading to slightly higher MCR. When $N$ = 5 for CodeBERT on vulnerability detection task, \textcolor{blue}{CodeImprove} achieved a higher CSR (i.e., 40.3\%) than $N$=10 (i.e., CSR of 38.7\%), however the RI \textcolor{blue}{was lower than} $N$ =10 due to higher MCR value. Therefore, it is necessary to maintain the balance between CSR and MCR during the transformation. Moreover, CodeImprove has achieved greater performance on all cases for $N$, \textcolor{blue}{indicating its practical applicability}. 


\begin{tcolorbox}[title=\textbf{RQ4} - How sensitive is CodeImprove’s performance under different experimental setups?, left=2pt, right=2pt,top=2pt,bottom=2pt]
CodeImprove is effective when different number of transformation operators are applied during the crossover (i.e., RI shows 48.4\% - 51.28\% for vulnerability detection and 26.1\%-32.8\% for defect detection tasks).
\end{tcolorbox}
%\vspace{-3mm}






%%%ISSTA\subsection{RQ3: Impact on Layer Selection for Out-of-Scope Data Detection}

 %Our goal is to analyze how out-of-scope data detection affects on growing layers. Therefore, we randomly selected four sub-models from our study to compute the number of out-of-scope data detected by our validity score. We selected sub-models of layers consisting of one, four, seven, and twelve. Note that for all models in the subject, the maximum number of layers is 12. Table~\ref{Tab:rq3} shows the statistics of the AUC comparison of validity score with different numbers of sub-models. Table~\ref{Tab:rq3}, column 2 shows the selected layer ids of each sub model. 

 %%%ISSTA \textit{Process} Our goal is to analyze how out-of-scope data detection affects on growing layers. We randomly selected sub-models consisting of one, four, seven, and twelve layers. Table~\ref{Tab:rq3} presents the statistics of the AUC comparison and Column 2 of Table~\ref{Tab:rq3} shows the selected layer ids of each sub-model. Note that for all models in the subject, the maximum number of layers is 12.
 
 
 %Table~\ref{Tab:rq3} presents the statistics of the AUC comparison of the validity score with different numbers of sub-models, which were randomly selected from the generated sub-models. The sub-models consist of one, four, seven, and twelve layers. Column 2 of Table~\ref{Tab:rq3} shows the selected layer ids of each sub-model. Note that for all models in the subject, the maximum number of layers is 12.



%From Table~\ref{Tab:rq3}, we observe that: (1) highest AUC scores are obtained in sub-models with less number of layers (i.e., sub-model-1 except for RoBERTa on vulnerability detection); (2) adding more layers will show a decreasing pattern of AUC scores (e.g. 0.798 vs 0.874 for vulnerability detection on BERT with sub-model-12 vs sub-model-1); (3) RoBERTa model on vulnerability detection shows lowest AUC score for sub-model 1,4,and 7 respectively; (4) defect prediction task with 1 sub-model achieved the highest AUC scores; and (5) although all sub-model performs satisfactorily, we can conclude that adding layers can affect the final model prediction. Therefore, different layers affect in overall model performance. 

%%%ISSTA From Table~\ref{Tab:rq3}, we observe that: (1) sub-models with fewer layers (i.e., sub-model-1 except for RoBERTa on vulnerability detection) achieved the highest AUC scores; (2) adding more layers showed a decreasing pattern of AUC scores (e.g. 0.798 vs 0.874 for vulnerability detection on GraphCodebERT with sub-model-12 vs sub-model-1); (3) RoBERTa model on vulnerability detection shows the lowest AUC score for sub-model-1, sub-model-4, and sub-model-7 respectively; (4) the defect prediction task with one sub-model achieved the highest AUC scores (i.e., 0.928-0.936); and (5) although all sub-models performed satisfactorily, we can conclude that adding layers can affect the final model prediction. Therefore, different layers have an impact on the overall model performance.

%%%issta\begin{tcolorbox}[title=\textbf{RQ3} - Does the choice of layers for selection have an impact on out-of-scope data detection?, left=2pt, right=2pt,top=2pt,bottom=2pt] Layer selection for detecting out-of-scope data is important for its best effectiveness (Table~\ref{Tab:rq3}). 
%%%\end{tcolorbox}


 





\input{tex/casestudy}

%We measure the effectiveness of applying semantic preserving program transformations to adapt out-of-scope inputs to become in-scope inputs. We compute IDP for each subject model with CodeImprove and CodeImprove-random. In particular, CodeImprove-random applies 10 randomly selected program transformation rules for out-of-scope inputs. We set the random transformation number to 10 in order to maintain the diversity of the transformed programs. Table~\ref{Tab:rq4} shows the statistics of IDP comparison for CodeImprove and CodeImprove-Random. 

%From Table~\ref{Tab:rq4}, we observe that: (1) CodeImprove performed better in terms of IDP in all subject matter compared to CodeImprove-random, suggesting that CodeImprove is more effective in converting an out-of-scope input to an in-scope input; (2) CodeBERT models on both SE tasks obtained the highest IDP values (i.e., 16.77 and 12.04); (3) CodeImprove-random showed similar IDP value for RoBERTa model on defect prediction task compared to CodeImprove (8.35 vs 10.06); and (4) overall, adapting an out-of-scope input to an in-scope input is satisfactory and promising by utilizing CodeImprove. 

% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.5\columnwidth}
%      \begin{minted}{C}
% for(iter = m.begin(); iter != m.end(); iter++){
% //
% }
% \end{minted}
% \caption{Defining \textit{for-loops} with before C++14}
% \label{fig:y equals x}
% \end{subfigure}
% \hfill
% \begin{subfigure}[b]{0.5\linewidth}
% \begin{minted}{C}
% for(auto:iter m){
% //
% }      
% \end{minted}
% \caption{Defining \textit{for-loops} with C++14}
% \label{fig:three sin x}
% \end{subfigure}
% \caption{Difference in the Definition of for-loops before and after C++14}
% \label{fig:forloops}
% \end{figure}
%-------------------------------------
% \noindent\begin{minipage}{.48\textwidth}
% \begin{minted}{C}
% static int null_filter_samples(AVFilterLink *link, 
% AVFilterBufferRef *samplesref){   
% return 0;}

% \end{minted}
% \end{minipage}\hfill
% \begin{minipage}{.45\textwidth}
% \begin{lstlisting}[caption= Transformed in-scope Input,frame=tlrb]{Name}
% void code()
% {

% }
% \end{lstlisting}
% \end{minipage}


% \noindent\begin{minipage}{.48\textwidth}
% \begin{lstlisting}[caption= An out-of-scope Input,frame=tlrb]{Name}
% void code()
% {

% }
% \end{lstlisting}
% \end{minipage}\hfill
% \begin{minipage}{.45\textwidth}
% \begin{lstlisting}[caption= Transformed in-scope Input,frame=tlrb]{Name}
% void code()
% {

% }
% \end{lstlisting}
% \end{minipage}






%\input{tex/tables/misclassfied}

%In addition to computing the IDP values, we sampled the incorrectly predicted inputs in Table~\ref{Tab:stts} to apply our program transformation. The results are shown in the Table~\ref{Tab:misclassified}. We find that CodeImprove is capable of improving the model performance of each subject in terms of accuracy, precision, recall, and F1-score compared to the performance of original set up (e.g: an increase of accuracy: 2.53\% - 8.49\%, precision: 2.29\%-9.15\%, recall: 1.04\%-14.43\%, and F1-score: 2.03\%-12.42\%). 


\begin{tcolorbox}[title=\textbf{RQ5} - Does program transformations of
CodeImprove preserve semantics?, left=2pt, right=2pt,top=2pt,bottom=2pt]
CodeImprove can generate semantic preserving program transformations. 
\end{tcolorbox}




\subsection{RQ6: Overhead of CodeImprove}

%\textcolor{red}{organize this section to show online offline stage.}
\textcolor{blue}{\textit{Process: } To apply CodeImprove in real-time, we compute the overhead of applying transformations for an input. We calculate the TPS, which measures the number of transformations applied per second by CodeImprove. This metric is averaged across all $N$ variants of CodeImprove studied under RQ4. Additionally, we evaluate both offline and online overhead. The offline stage involves the sub-model training, while the online stage measures the time required to adapt an input with CodeImprove. Table~\ref{Tab:rq5} shows the statistics of TPS, and Figure~\ref{fig:combined_figures} shows the time overhead of CodeImprove.   }


%\textit{Process: }  In order to apply CodeImprove in real-time, we compute the overhead of applying transformation for an input. We compute the TPS, which measures the number of transformations applied per second by CodeImprove.  This is the average of all $N$ variants of CodeImprove studied under RQ4. Table~\ref{Tab:rq5} shows the statistics of overhead for adapting an out-of-scope input by CodeImprove. %Note that we applied CodeImprove on all samples in Table~\ref{newstats}. 

\input{tex/tables/rq5}
\vspace{-1em}
\textcolor{blue}{Based on  Table~\ref{Tab:rq5}, CodeImprove is capable of applying transformations at a rate of 1.2 TPS to 2.04 TPS for each SE task across all code models. Figure~\ref{fig:figure2} confirms that CodeImprove takes approximately 49.92s to 59.4s to adapt an input across all models. We plan to further minimize the transformation times in future work.  Moreover, CodeImprove is more efficient and offers a practical, scalable solution to enhance model performance without the significant cost and time investment required by traditional methods such as retraining and replacement.}
\vspace{-1em}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{tex/images/offline_stage_time_comparison_similar_bold.pdf}
        \caption{\textcolor{blue}{Offline time overhead (in second)}}
        \label{fig:figure1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{tex/images/online_stage_time_comparison_similar_bold.pdf}
        \caption{\textcolor{blue}{Online time overhead (in seconds)}}
        \label{fig:figure2}
    \end{subfigure}
    \caption{\textcolor{blue}{Time Overhead of CodeImprove}}
    \label{fig:combined_figures}
\end{figure}
\vspace{-1em}
\textcolor{blue}{It is important to note that the sub-model training procedure is treated as an offline stage, minimizing its impact on overall performance. Figure~\ref{fig:figure1} shows that training a sub-model takes around 900s to 940s for the vulnerability detection  and 1200s to 1250s for the defect prediction across all models on a machine with an NVIDIA GeForce GTX 1080 GPU. This process only needs to be done once and incurs significantly lower costs compared to regular retraining or fine-tuning.}


%It is important to note that the sub-model training procedure is treated as an offline stage, minimizing its impact on overall performance. Training sub-models with 12 layers takes approximately three to five hours (i.e., 15 to 25 minutes per sub-model) on a machine with an NVIDIA GeForce GTX 1080 GPU and only needs to be done once. This process incurs significantly lower costs compared to regular retraining or fine-tuning.

%%%From the Table~\ref{Tab:rq5}, an average CodeImprove takes around one minute to adapt an input (1.2 TPS to 2.04 TPS), confirming that CodeImprove is capable of adapting inputs in practical use. \textcolor{blue}{We plan to further minimize the transformation times in future work. Moreover, CodeImprove is more efficient and offers a practical, scalable solution to enhance model performance without the significant cost and time investment required by traditional methods such as retraining and replacement. }

% \begin{figure}[H]
%     \centering
%     \begin{minipage}[b]{0.49\columnwidth}
%         \centering
%         \includegraphics[width=\columnwidth]{tex/images/offline_stage_time_comparison_similar_bold.pdf}
%         \caption{(a) Offline stage time comparison}
%         \label{fig:figure1}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.49\columnwidth}
%         \centering
%         \includegraphics[width=\columnwidth]{tex/images/online_stage_time_comparison_similar_bold.pdf}
%         \caption{(b) Online stage time comparison}
%         \label{fig:figure2}
%     \end{minipage}
%     \caption{Combined caption for both figures}
%     \label{fig:combined_figures}
% \end{figure}








%Therefore, our approach is more feasible and efficient, avoiding extensive overhead of retraining or replacement. We plan to further minimize these transformation times as a future work. By focusing on input adaptation, we offer a practical and scalable solution to enhance model performance without the significant costs and time investments of traditional methods. }



%our approach of adapting inputs is more feasible and efficient, avoiding the extensive overhead of retraining or replacement. We are also working on further minimizing these transformation times. By focusing on input adaptation, we offer a practical and scalable solution to enhance model performance without the significant costs and time investments of traditional methods.





%%Note that the sub-model training procedure is treated as an offline stage, minimizing the impact on overall performance. Each sub-model inherits the weights of the original model up to a certain layer, with minimal training on dense and dropout layers to introduce variability. Training sub-models with 12 layers takes approximately three to five hours (i.e., 15 to 25 minutes for each sub-model) on a machine with an NVIDIA GeForce GTX 1080 GPU and only needs to be done once. This process incurs much lower costs compared to regular retraining or fine-tuning. }

%The sub-model training procedure is treated as an offline stage, minimizing its impact on overall performance. Each sub-model inherits the weights of the original model up to a certain layer, with minimal training on dense and dropout layers to introduce variability. Training sub-models with 12 layers takes approximately three to five hours on a machine with an NVIDIA GeForce GTX 1080 GPU and only needs to be done once. This process incurs much lower costs compared to regular retraining or fine-tuning.


\begin{tcolorbox}[title=\textbf{RQ6} - What is the overhead of CodeImprove in adapting a program to DL models?, left=2pt, right=2pt,top=2pt,bottom=2pt]
CodeImprove was highly efficient in adapting an out-of-scope input through semantic preserving program transformations in real-time (1.2TPS - 2.04TPS).
\end{tcolorbox}


%In recent years, lots of researches about uncertainty measurement for DL models have been proposed. We select a subset of uncertainty metrics from the existing literature for their prevalence, scalability, and practical applicability. The selected work includes:

