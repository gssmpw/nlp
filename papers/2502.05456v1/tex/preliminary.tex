\section{Preliminary Study on Input Validation for Code Models}
\label{study}
% \textcolor{red}{
% RQ2.1: How effective are the existing uncertainty methods in distinguishing in/out-of-scope program inputs? 
% %Which method(s) perform(s) relatively well (or bad) and why?
% RQ2.2: Are the performance of existing uncertainty methods
% consistent on Software engineering tasks? %If not consistent, what are the possible reasons? 
% }
%\jiajun{The connection between ``out-of-scope'' and ``uncertainty'' was unclear. Why we should study the effectiveness of uncertainty metrics?}
%To understand the kinds of test cases (i.e., tests) that operator developers write and the limitations of their current testing practices, we study 50 open-source Kubernetes operator projects from GitHub and their tests.

%%% rephrased The initial step in code adaptation involves distinguishing between in-scope and out-of-scope inputs, a challenging task due to the absence of a definitive boundary delineating the handling scope of DL models. While a common approach is to leverage the model's confidence score, research indicates that the softmax confidence score alone is insufficient for identifying incorrect predictions. Consequently, multiple uncertainty metrics have been proposed to quantify prediction uncertainty accurately. In our preliminary study, we investigate the effectiveness of these uncertainty metrics in the context of code models. Our objective is to ascertain whether these metrics can establish a dependable threshold score for differentiating between in-scope and out-of-scope program inputs. To assess the suitability of existing uncertainty metrics for input validation in code data, we focus on a selection from the literature and calculate an uncertainty score for each data point in the test set. This analysis aims to determine the optimal threshold for delineating in-scope and out-of-scope program inputs.

%The initial step in code adaptation involves distinguishing between in-scope and out-of-scope inputs. This task is quite challenging due to the lack of oracle defining boundary on the DL model's handling scope. While a common approach is to leverage the model's confidence score (i.e.,  the output of the final softmax layer), existing work~\cite{guo2017calibration, hendrycks2018baseline} suggests that the softmax confidence score alone is insufficient for identifying incorrect predictions. Consequently, multiple uncertainty metrics~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline, li2021estimating, alon2019code2vec, vasudevan2019towards,corbiere2019addressing} have been proposed to quantify the level of uncertainty associated with a model's prediction. 

In our preliminary study,  we assess the applicability of existing uncertainty metrics within the realm of code models, aiming to identify a dependable threshold score that can differentiate in-scope and out-of-scope program inputs. We perform the study on a comprehensive set of metrics~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline, li2021estimating, alon2019code2vec} from the existing literature. 
%In particular, we calculate an uncertainty score for each test input to find the best threshold to distinguish between in-scope and out-of-scope program inputs.
\textcolor{blue}{Our goal is to} answer the research question: How effective are the existing uncertainty metrics in distinguishing in/out-of-scope program inputs? 

 %If not consistent, what are the possible reasons? 


%%%ISSTA-----The first step of code adaptation is to identify in-scope inputs and out-of-scope inputs. However, due to lack of oracle defining boundary on the DL model's handling scope, this task is quite challenging.  A naive approach is to use the model's confidence score (i.e.,  the output of the final softmax layer) to estimate the confidence of each model prediction. However, existing work~\cite{guo2017calibration, hendrycks2018baseline} suggests that softmax confidence score is not a good indicator of whether an example is an incorrect prediction. Therefore, researchers propose multiple uncertainty metrics~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline, li2021estimating, alon2019code2vec, vasudevan2019towards,corbiere2019addressing} to quantify the level of uncertainty associated with a model's prediction.  Therefore, we conduct a preliminary study on the effectiveness of uncertainty metrics on code models. Our aim is to determine whether these metrics can establish a reliable threshold score to differentiate the in-scope and out-of-scope program inputs. To gain insights into the efficacy of existing uncertainty metrics in the context of code data for input validation, we focus on a set of metrics~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline, li2021estimating, alon2019code2vec} from the existing literature. In particular, we calculate an uncertainty score for each data point in the test set to find the best threshold to distinguish between in-scope and out-of-scope program inputs. end issta%

% To understand the effectiveness of existing uncertainty metrics~\cite{} on code data for input validation, we select a subset of uncertainty metrics from the existing literature. Specifically, we assess the uncertainty score for each test data to differentiate between in-scope and out-of-scope data. %The summary of selected works is illustrated in the Table~\ref{table:uncertainty_metrics}.

%%%%\input{tex/tables/setup_uncertainty}

%We aim to explore the effectiveness of uncertainty methods in detecting out-of-scope data from the normal inputs for input validation. Specifically, we assess the uncertainty score for each test data to differentiate between in-scope and out-of-scope data. We select a subset of uncertainty metrics from the existing literature. The selected work includes:

%We explore the effectiveness of detecting out-of-scope data for DL models on defect detection tasks that play a crucial role in the field of SE. We select a subset of uncertainty metrics from the existing literature. The selected work includes: 
%Recently, many researchers have proposed uncertainty measurements for DL models. We select a subset of uncertainty metrics from the existing literature to differentiate the in-scope and out-of-scope data. The selected work includes:



% \begin{itemize}
%     \item \textbf{Vanilla: } Vanilla proposes that the maximum softmax probability could be used as the model confidence or uncertainty to distinguish the in- and OOD inputs.

%     \item \textbf{Temperature-Scaling: } Temp scale utilizes the post-hoc calibration named temperature scaling on a validation set to calibrate the gap between model predictive confidence and accuracy, the calibrated DL model’s predictive probability could represent better ground-truth correctness likelihood and thus be used for uncertainty measurement.

%     \item \textbf{Mahalanobis }Mahalanobis distance (MD) is a simple and popular post-processing method for detecting outof-distribution (OOD) inputs in neural networks. The Mahalanobis distance (MD) [] method uses intermediate feature maps of a trained deep neural network. 

%     \item\textbf{Predictive Entropy } Compute predictive entropy for a tensor

%     \item \textbf{Mutual Information }Compute the mutual information as defined in [3] given a number of predictions

%     \item\textbf{Least Confidence } Returns the uncertainty score of an array using least confidence sampling in a 0-1 range where 1 is the most uncertain

%     \item\textbf{Softmax confidence }Returns softmax array for array of scores. Converts a set of raw scores from a model (logits) into a  probability distribution via softmax. The probability distribution will be a set of real numbers
%     such that each is in the range 0-1.0 and the sum is 1.0.
    

%     \item\textbf{Margin Confidence } Returns the uncertainty score of a probability distribution using margin of confidence sampling in a 0-1 range where 1 is the most uncertain

%     \item\textbf{Ratio Confidence }Returns the uncertainty score of a probability distribution using ratio of confidence sampling in a 0-1 range where 1 is the most uncertain

%     \item\textbf{Dissector: } Dissector proposes a model-specific uncertainty evaluation approach based on assessing the model’s cross-layer confidence about a given input. Based on different weight growth types, Dissector was configured into three sub-techniques, named as, Dissector-linear, Dissector-log and Dissector-exp
% \end{itemize}
\subsection{Experimental Method and Setup}

%In order to comprehensively capture the uncertainty in the predictions of DL models, we utilize a selection of well-regarded methods from the existing literature based on their prevalence, scalability, and practical applicability in a real-world scenario.
%as shown in Table ~\ref{table:uncertainty_metrics}. 

\textbf{Uncertainty Metrics: } 
Our study includes eight different uncertainty metrics. We utilize vanilla~\cite{hendrycks2018baseline} that computes maximum softmax probability as the confidence. \textcolor{blue}{Temp Scale~\cite{guo2017calibration} is a post-hoc calibrated confidence metric applied to the validation set, where the BFGS optimizer~\cite{international1990bfgs} is used to train a calibration temperature with a learning rate of 0.01.} Our study includes confidence-based uncertainty metrics, \textcolor{blue}{such as least confidence~\cite{monarch2021human}, which calculates the difference between 100\% confidence and the most confidently predicted label; margin confidence~\cite{monarch2021human}, which determines the difference between the top two most confident softmax predictions; and ratio confidence~\cite{monarch2021human}, which computes the ratio between the top two most confident softmax predictions.} We also include uncertainty metrics that were designed using information theory~\cite{steinhardt2016unsupervised,shannon1948mathematical}. Entropy computes the average amount of surprise/ uncertainty for a given outcome. Predictive entropy quantifies the uncertainty associated with the outcomes for a given set of observed inputs. Mutual information measures the amount of information obtained from one random variable given another using entropy and conditional entropy. 
\textcolor{blue}{Monte-Carlo Dropout (MCD)~\cite{gal2016dropout} quantifies uncertainty by averaging the logits over multiple dropout samples. Deep Ensembles (DE)~\cite{lakshminarayanan2017simple} quantifies uncertainty by averaging the outputs from multiple independently trained models with different initial seeds. Both employ the sampled winning score (SWS) as the primary uncertainty metric.}
%\textcolor{blue}{Monte-Carlo Dropout (MCD)~\cite{gal2016dropout} and Deep Ensembles (DE)~\cite{lakshminarayanan2017simple} are methods used to measure the model uncertainty. MCD quantifies uncertainty by averaging the logits over multiple dropout samples, while DE quantifies uncertainty by averaging the outputs from multiple independently trained models with different initial seeds, both employing the sampled winning score (SWS) as the primary uncertainty metric.}

%in neural networks.
%Temp scale~\cite{guo2017calibration} is a post-hoc calibrated confidence on the validation set. For temp Scale, we use the BFGS optimizer~\cite{international1990bfgs} to train a calibration temperature on the validation (in-distribution) dataset with a learning rate of 0.01. Our study includes confidence-based uncertainty metrics such as least confidence~\cite{monarch2021human}, margin confidence~\cite{monarch2021human}, and ratio confidence~\cite{monarch2021human}. Least confidence computes the difference between the 100\% confidence and the most confidently predicted label. Margin confidence computes the difference between the top two most confidence softmax predictions. Ratio confidence computes the ratio between the top two most confident softmax predictions. Additionally, we also included uncertainty metrics that were designed using information theory, such as entropy, predictive entropy, and mutual information~\cite{steinhardt2016unsupervised,shannon1948mathematical}. Entropy computes the average amount of surprise/ uncertainty for a given outcome. Predictive entropy quantifies the uncertainty associated with the outcomes for a given set of observed inputs. Mutual information measures the amount of information obtained from one random variable given another using entropy and conditional entropy. 


%For Vanilla~\cite{hendrycks2018baseline}, no additional configuration is needed as it leverages the softmax probability of DL models. For Temp Scale, we use the BFGS optimizer~\cite{international1990bfgs} to train a calibration temperature on the validation (in-distribution) dataset with a learning rate of 0.01. For, confidence-based uncertainty metrics such as least confidence, margin confidence, and ratio confidence, we set it up to the original implementation~\cite{monarch2021human} as these metrics compute different confidence scores for model's prediction output. We also, leveraged information theory-based uncertainty metrics such as entropy, predictive entropy, and mutual information~\cite{steinhardt2016unsupervised,shannon1948mathematical}. Dissector requires selecting interior hidden layers for sub-model generation. We adapt the hidden features using the hidden state outputs of the pre-trained code models. For each sub-model, we select the embedding layers, hidden blocks containing attention layers, and dropout layers. Based on the growth pattern of sub-model generation, the Dissector is configured into linear, logarithmic, and exponential. We initially set up the number of sub-models to 12. Dissector is configured into linear, logarithmic and exponential.

%\input{tex/tables/uncertainty_results}


\textbf{Dataset and Models: } To evaluate the effectiveness of detecting out-of-scope data through uncertainty quantification, we consider two code tasks and two associated datasets (i.e., defect prediction with CodeChef dataset and vulnerability detection on Devign dataset) on three pre-trained models. More information on datasets and subject models is explained in Section~\ref{subjects}.
\input{tex/tables/preliminary}
%We continue using Devign dataset with CodeBERT model to retrieve the uncertainty scores for each test data. We select CodeBERT as our primary model because CodeBERT performs better accuracy for task defect detection. 
 
 %We report the best among the three sub-techniques.


%For each uncertainty metric, we use the initial set up. Note that, confidence score based uncertainty scores (e.g: Softmax confidence, margin confidence, and ration confidence) are used in active learning domains for uncertainty sampling. We include these to measure the uncertainty in our target test dataset based on hidden layer outputs. For dissector, since the original set up does not include the sub-model training phase, we added it in our study. 


%%%We evaluate the performance of different uncertainty methods for detecting out-of-scope data and predicting the F1-score for in-scope samples across various threshold values. To compute the AUC scores, we define positive and negative samples based on the correspondence between predicted outputs and ground truth labels. A positive sample indicates correct predictions (in-scope), labeled as 1, while a negative sample signifies misclassifications (out-of-scope), labeled as 0. For example, if a model predicts an input pair (x, y) as an exact match (ground truth label 1), it's considered in-scope; otherwise, it's labeled out-of-scope. During evaluation, each uncertainty method predicts a score reflecting the model's capability in handling the input. An effective method assigns higher scores to in-scope inputs and lower scores to out-of-scope ones. We compare the AUC scores of these methods for out-of-scope data detection and calculate the predicted F1-score for in-scope inputs using a threshold T. The effectiveness of an uncertainty method is gauged by its ability to achieve higher scores on in-scope code samples.

\textbf{Evaluation Metric: }We used the Area Under Curve (AUC)~\cite{davis2006relationship} based on
True Positive Rate (TPR) and False Positive Rate (FPR) data to measure how effective a technique is in distinguishing an out-of-scope inputs from in-scope inputs. AUC quantifies the probability that a positive example receives a higher predictive score than a negative sample. For example, a random classifier yields an AUC of 0.5, while a perfect classifier achieves an AUC of 1.

To compute the AUC scores, we define positive and negative samples based on the correspondence between predicted outputs and ground truth labels. A positive sample indicates correct predictions (in-scope), labeled as 1, while a negative sample signifies misclassified (out-of-scope), labeled as 0. For instance, given a well-trained model $f(\cdot)(·|\theta)$, an input pair $(x,y)$ has ground truth 1 if $f(x|\theta)$ is an exact match of $y$, and 0 otherwise. During evaluation, each uncertainty method predicts a score reflecting the model's capability in handling the input.
%AUC represents the probability that an positive example has a larger predictive score than a negative sample. For example, a random classifier corresponds to an AUC of 0.5 and a "perfect" classifier corresponds to 1.  

%We compare AUC scores of the selected uncertainty methods for out-of-scope data detection %and predicted F1-score for in-scope samples using various threshold values. 
%%%To compute the AUC scores, we define positive and negative samples based on the correspondence between predicted outputs and ground truth labels. A positive sample indicates correct predictions (in-scope), labeled as 1, while a negative sample signifies misclassified (out-of-scope), labeled as 0. For instance, given a well-trained model $f(\cdot)(·|\theta)$, an input pair $(x,y)$ has ground truth 1 if $f(x|\theta)$ is an exact match of $y$, and 0 otherwise. During evaluation, each uncertainty method predicts a score reflecting the model's capability in handling the input. An effective method assigns higher scores to in-scope inputs and lower scores to out-of-scope ones. We compare the AUC scores of these methods for out-of-scope data detection. % and calculate the predicted F1-score for in-scope inputs using a threshold T. 
%%%Intuitively, an effective uncertainty method should achieve higher scores on in-scope code samples. 

%We compare AUC scores of the selected uncertainty methods for out-of-scope data detection and predicted F1-score for in-scope samples using various threshold values. AUC computation requires positive and negative samples. Therefore, we defined the two samples as follows. For all predicted samples in both SE tasks, we compare the samples with the ground truth. We classify a positive sample if the input's prediction is similar to the ground truth; otherwise, it is a negative sample. Inputs that are correctly predicted (in-scope) are labeled 1, while those misclassified (out-of-scope) are labeled 0. For instance, given a well-trained model $f(\cdot)(·|\theta)$, an input pair $(x,y)$ has ground truth 1 if $f(x|\theta)$ is an exact match of $y$, and 0 otherwise. During the evaluation, an uncertainty method $U(f,x|\phi)$ takes $f(\cdot|\theta)$ and $x$, and predicts a score that reflects $f(\cdot|\theta)$’s handling ability of $x$. An effective method should predict larger scores for in-scope inputs and smaller scores for out-of-scope inputs. We compare AUC scores of the selected uncertainty methods for out-of-scope data detection. We calculate the predicted F1-score for in-scope inputs based on the threshold $T$. An in-scope input $x$ is $U(x) > T$ based on the measured uncertainty. Intuitively, an effective uncertainty method should achieve higher scores on in-scope code samples.






%---\textbf{Evaluation Metric: } We use the well-trained DL models and all the dataset in Section~\ref{subjects}. We define the ground truths for the test dataset as follows: Inputs that are correctly predicted (in-scope) are labeled 1, while those misclassified (out-of-scope) are labeled 0. For instance, given a well-trained model $f(\cdot)(·|\theta)$, an input pair $(x,y)$ has ground truth 1 if $f(x|\theta)$ is an exact match of $y$, and 0 otherwise. During the evaluation, an uncertainty method $U(f,x|\phi)$ takes $f(\cdot|\theta)$ and $x$, and predicts a score that reflects $f(\cdot|\theta)$’s handling ability of $x$. An effective method should predict larger scores for in-scope inputs and smaller scores for out-of-scope inputs except for dissector.We compare AUC scores of the selected uncertainty methods for out-of-scope data detection. In addition to AUC computation, we study how effective existing uncertainty methods can satisfy a model to resist from predicting out-of-scope inputs. Therefore, we compute the predicted F1-score for in-scope samples using various threshold values. 




 

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/vanilla.pdf}
%         \caption{Vanilla}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/predictive.pdf}
%         \caption{Predictive Entropy}
%     \end{subfigure}
%     %\centering
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/mutual.pdf}
%         \caption{Mutual Information}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/least.pdf}
%         \caption{Least Confidence}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/ratio.pdf}
%         \caption{Ratio Confidence}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/margin.pdf}
%         \caption{Margin Confidence}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/softmax.pdf}
%         \caption{Softmax Confidence}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/entropy.pdf}
%         \caption{Entropy}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/dissector.pdf}
%         \caption{Dissector}
%     \end{subfigure}
%     \caption{Distribution of uncertainty scores for each test data. }
%     \label{fig:dicussion3}
% \end{figure}

\subsection{Results and Analysis: }
%We use AUC values to measure the effectiveness of distinguishing out-of-scope inputs from in-scope inputs. 
 %and Figure~\ref{fig:f1} shows the predicted F1 score for varying threshold.
% From the table, we observe that: 1) vanilla always obtained stable AUC scores (i.e., AUC \textgreater{} 0.5) for both SE tasks for CodeBERT and RoBERTa, 2) other uncertainty metrics performed better depending on the SE tasks and the model. For example, entropy showed the higher AUC score for the vulnerability detection task on the RoBERTa model (AUC: 0.556) while obtaining the lowest AUC score for the same task on the CodeBERT model, and 





%From Figure~\ref{fig:f1}, first we observe that vanilla and temp scaling has higher F1-scores with RoBERTa model on vulnerability detection task. Second, both vanilla and temp. scaling shows a huge degradation in performance for GraphCodeBERT models on both tasks. This draws to the conclusion that each subject model can affect on models input validation for code data as vanilla and temp. scaling depicts opposite scenarios on RoBERTa and GraphCodeBERT on SE tasks. Third, entropy shows less effective on improving model performance for CodeBERT, given the decreasing pattern on F1-score on both tasks. Fourth, confidence based uncertainty metrics do not show any increasing or decreasing pattern of F1-score on each subject. Model's confidence scores are hugely affected by the softmax. We can conclude that such variability might be due to attributes in code data and the influence on model's handling capability on code data which affects the uncertainty measurements. 

%---From the table, we observe that: 1) dissector always obtained the best AUC scores for vulnerability detection task(e.g: 0.73-0.82), and defect prediction (e.g: 0.82-0.88); 2) other uncertainty methods obtained only 0.47-0.53 for CodeBERT, 0.45-0.55 for RoBERTa, and 0.47-0.52 for BERT on vulnerability detection task; and 0.40-0.59 on CodeBERT, 0.39-0.60 on RoBERTa, and 0.49-0.59 on BERT for defect prediction task. 

%---From Figure~\ref{fig:f1}, first we observe that dissector demonstrated higher F1-scores for all models in both vulnerability detection and defect prediction tasks. Secondly, we observe that vanilla and temp scaling has higher F1-scores with RoBERTa model on vulnerability detection task.  Thirdly, both vanilla and temp. scaling shows a huge degradation in performance for BERT models on both tasks. This draw to a conclusion that each subject model can affect on models input validation for code data as vanilla and temp. scaling depicts opposite scenarios on RoBERTa and BERT on SE tasks. Fourthly, entropy shows less effective on improving model performance for CodeBERT, given the decreasing pattern on F1-score on both tasks.  Fifthly, confidence based uncertainty metrics do not show any increasing or decreasing pattern of F1-score on each subject. Model's confidence scores are hugely affected by the softmax. We can conclude that such variability might be due to attributes in code data and the influence on model's handling capability on code data which affects the uncertainty measurements. 

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.49\columnwidth}
%     \includegraphics[width=\columnwidth]{tex/images/VulCB.pdf}
%         \caption{CB with V.D}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.49\columnwidth}
%         \includegraphics[width=\columnwidth]{tex/images/VulRB.pdf}
%         \caption{RB with V.D}
%     \end{subfigure}
    %\centering
    % \begin{subfigure}[b]{0.3\columnwidth}
    %     \includegraphics[width=\columnwidth]{tex/images/VulGCB.pdf}
    %     \caption{GCB with V.D}
    % \end{subfigure}
    % \begin{subfigure}[b]{0.49\columnwidth}
    %     \includegraphics[width=\columnwidth]{tex/images/DefectCB.pdf}
    %     \caption{CB with D.P}
    % \end{subfigure}
    % \begin{subfigure}[b]{0.3\columnwidth}
    %     \includegraphics[width=\columnwidth]{tex/images/DefectRB.pdf}
    %     \caption{RB with D.P}
    % \end{subfigure}
    % \begin{subfigure}[b]{0.49\columnwidth}
    %     \includegraphics[width=\columnwidth]{tex/images/F1SCORE.pdf}
    %     \caption{GCB with D.P}
    % \end{subfigure}
    % \caption{F1 Score \% with Different Thresholds. V.D stands for Vulnerability Detection and D.P stands for Defect Prediction. %Enlagred images are available in our website~\cite{CodeImprove}
% }
%     \label{fig:f1}
% \end{figure}

% We further investigate the reasons why existing uncertainty scores fail on code data. 
%\subsection{Findings on Uncertainty Metrics for Input Validation:}

%According to the Table~\ref{Tab:preliminary} distinguishing out-of-scope inputs from in-scope inputs using existing uncertainty measurements is challenging. Therefore, we further investigate the reasons why existing uncertainty scores fail on code data. 

%\textcolor{red}{rewrite in terms of computations}
%\textit{Finding 1: Effectiveness of softmax computation on uncertainty measurements: }
% \textit{\textbf{Finding 1. The Effectiveness of Uncertainty Metric Methods:}}
%%%ISSTA The aforementioned uncertainty scores compute the softmax values for hidden state layer outputs (e.g: confidence based uncertainties, entropy, predictive entropy etc.). Then, the metrics use these softmax values in two ways: 1) compute ratio difference, marginal difference between top two softmax values, or apply statistical equation such as entropy,  or 2) retrieve the maximum softmax value. In the situation of retrieving the maximum softmax value, the out-of-scope data will also have a high confidence value similar to the in-scope value resulting in an overlapping situation with in-scope data.  In the situation of using softmax values for further computation depends on the difference among the softmax values in the probability vector. Therefore, obtaining maximum softmax based values are not good indicators to compute uncertainty.

Table~\ref{Tab:preliminary} shows AUC scores for the evaluated uncertainty metrics.
The majority of these metrics exhibit AUC scores close to 0.5, akin to what one would expect from a random classifier, with none surpassing an AUC score of \textcolor{blue}{0.624}. From these results, it is inferred that the uncertainty metrics under consideration are ineffective at distinguishing between in-scope and out-of-scope code inputs.



% For example, when computing entropy based uncertainty the minimum and maximum uncertainty scores for in-scope input (0.01-0.944) and out-of-scope (0.04-0.944) inputs falls into the similar range, making it indistinguishable to each other. 

%However, the hidden states of code data may capture features or patterns in the data that are common to both correctly predicted or incorrectly predicted samples. 



%When we compute uncertainty using entropy for a correctly predicted input $x1$ and incorrectly predicted input $x2$, the respective hidden state outputs are $H1$ = [ 0.1034,  0.5225, -0.1192,  ...,  0.2330,  0.1357,  0.3067] and $H2$ = [-0.0074,  0.5445,  0.0509,  ...,  0.5250, -0.1328,  0.4963]. The entropy scores for these two inputs on hidden states are $E1$ = 0.85241187 and $E2$ = 0.86419624. Upon examination, it is observed that there is not much difference between these scores to effectively differentiate between the two scenarios. Therefore, such metrics are not a good indicator for computing uncertainty.  


%The hidden states may capture features or patterns in the input data that are common to both correctly and incorrectly predicted samples. If the data distribution for these samples overlaps significantly, it can result in similar distributions of hidden state values and, consequently, similar entropy scores.


%In all these situations, out-of-scope data can also have hidden state outputs similar to in-scope data resulting an overlapping situation. Therefore, such metrics are not a good indicator to compute uncertainty.  

%\textcolor{blue}{For example, given $x1$ as a correctly predicted sample and $x2$ as incorrectly predicted sample, and entropy as our uncertainty measurement. $H1$ = [ 0.1034,  0.5225, -0.1192,  ...,  0.2330,  0.1357,  0.3067] and $H2$ = [-0.0074,  0.5445,  0.0509,  ...,  0.5250, -0.1328,  0.4963]. The entropy scores for these two inputs on hidden states are $E1$ = 0.85241187 and $E2$ = 0.86419624. By taking a look at the scores, it is not much differentiable.} 
%\textcolor{red}{show statistics hidden output, explain with stats, finding 1 and 3. Change terms hidden state outputs.}
%\textcolor{red}{draw sub-model generation}

%---\textit{Finding 2: Investigation on each layer's effectiveness on decision making is promising:} The analysis of partially growing knowledge on the DL model helps in distinguishing out-of-scope- data. Our results show that dissector outperforms other uncertainty measurements on both subjects. Dissector evaluates the effectiveness of each sub-layer on the model's decision making by generating sub-models by confusing the predictions. 

%\textcolor{red}{rewrite in term of behavior of code data on models}
% ISSTA \textit{Finding 2: Effectiveness on model's handling capability on code data: } Our study finds out that different models behave differently on code data. For example, although metrics like vanilla and temp. scaling performs well in terms of predicted F1-score for RoBERTa model, both of the metrics show performance degradation on the GraphCodeBERT model. Therefore, we conclude that performance of uncertainty metrics also depend on the models behaviour on the code input.  

%%%%\textit{\textbf{Finding 2. Performance:} Model Behavior Disparity on Code Data: } Our study finds discrepancies in how models handle code data. For instance, while metrics such as vanilla and temperature scaling exhibit strong performance in terms of predicted F1-score for the RoBERTa model, both metrics demonstrate performance degradation when applied to the GraphCodeBERT model. Therefore, we conclude that efficacy of uncertainty metrics depend on the models behavior on the code input. 




%Our investigation reveals notable discrepancies in how various models handle code data. For instance, while metrics such as vanilla and temperature scaling exhibit strong performance in terms of predicted F1-score for the RoBERTa model, both metrics demonstrate performance degradation when applied to the GraphCodeBERT model. Consequently, we infer that the efficacy of uncertainty metrics is contingent upon the behavior of the model concerning the input code.


%In conclusion, distinguishing out-of-scope inputs from in-scope inputs for code data is challenging. Based on our insights,  we design a validity score metrics  that could perform better than existing uncertainty metrics (Section~\ref{validation}).

% \textcolor{blue}{\textit{\textbf{Finding 2. The Performance on Software Engineering tasks:}}
% % The studies in \cite{li2021estimating, hu2023codes} utilize uncertainty metrics to distinguish out-of-distribution data.
% Existing studies leverage uncertainty measurement~\cite{hu2023codes,li2021estimating}, a traditional approach to detect out-of-distribution (i.e., data from a different distribution from training data) inputs. %by interpreting the amount of uncertainty in a model prediction for a given input. 
% However, our aim is to maintain the same distribution while identifying inputs that are prone to misprediction (i.e., out-of-scope). 
% % However, our study focuses on detecting out-of-scope data from the same distribution. 
% This difference in focus could be a reason for uncertainty metrics to perform less effectively. For example, uncertainty metrics used to detect out of distribution data shows around 0.75 or above in AUC scores~\cite{li2021estimating}. Therefore, to correctly determine the out-of-scope inputs from the same data distribution, our study concludes that a more granular observation of data behavior is required.  }

%The studies in [17, 30] utilize uncertainty metrics to distinguish out-of-distribution data. However, our study focuses on detecting out-of-scope data from the same distribution. This difference in focus could be a reason for the uncertainty metrics to perform less effectively. For instance, the uncertainty metrics used to detect out-of-distribution data typically show AUC scores of around 0.75 or higher [17]. Therefore, to accurately determine out-of-scope inputs from the same data distribution, our study concludes that a more granular observation of data behavior is required.





%After computing uncertainty scores on each test data, we find that all of the above-aforementioned uncertainty metrics except the dissector show an overlapping nature between in-scope and out-of-scope data as shown in figure~\ref{fig:dicussion3}. Therefore, we further investigate the reasons on why existing uncertainty scores fail while approaches like dissectors perform well. 



%Confusion score second highest vs highest. 

 




%\textbf{Dissector as Uncertainty Measurement }

%After computing uncertainty scores, we find that all of the above aforementioned uncertainty metrics except dissector shows an overlapping nature between in-scope and out-of-scope data as shown in figure []. However, dissector is capable of distinguishing the out-of-scope data when the uncertainty score if less than 0.4 for a given test data as shown in figure []. This indicates that out-of-scope data detection using dissector is promising. 


%DISSECTOR benefits from the internal layer features that builds several sub-models that are retrained on top of internal layers. Dissector also computes the model predictions for each sub-model alleviating different predictions on each layer. Therefore, additional information may be learned by the training process. Other uncertainty metrics rely on softmax values or hidden state features of the model to quantify uncertainty scores. 


% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/dissector.drawio.pdf}
%         \caption{Distribution of Dissector's confidence scores for selected layers}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{tex/images/vanilla_scores.pdf}
%         \caption{Distribution of Vanilla uncertainty scores for selected layers }
%     \end{subfigure}
%     \caption{High level visualization of Dissector vs Vanilla  scores for an out-of-scope program data ID:35. Dissector's approach uses four sub-model generations and their corresponding sub-model predictions, and sub-model confidence score that helps to compute the overall confidence. Vanilla uncertainty scores are obtained for each corresponding layers }
%     \label{fig:disVSvanilla}
% \end{figure}


% \begin{figure}[!htbp]
% \centering

% \includegraphics[width=1\columnwidth]{tex/images/dissector.drawio.pdf}
% \caption{High level visualization of Dissector vs Vanilla uncertainty scores for an out-of-scope program data ID:35. Dissector's approach uses four sub-model generations and their corresponding sub-model predictions, logit values, and SVScore that helps to compute the PVScore. Vanilla uncertainty scores are obtained for each corresponding layer number of the dissector.  }
% \textcolor{red}{TODO: Working on vanilla method. bernstein is down and cannot run anything }
% \label{fig:dissector}
% \end{figure}

%\textcolor{red}{Working on this}
%DNN’s softmax layer to represent the confidence score, given that softmax layer is the output layer that is directly correlated with the model’s prediction, their calibrated confidence score has more reliance on the original model’s prediction accuracy than those uncertainty methods that focus more on the DNN’s interior layer activation values.

\label{design}
\begin{figure}[!htbp]
\centering
%\includegraphics[width=\columnwidth]{figs/jjj.JPG}
%\includegraphics[width=\linewidth, height=4cm]{figs/overviewupdated.drawio.pdf}
\includegraphics[width=\columnwidth]{tex/images/overview.pdf}
\caption{Overview of CodeImprove} 
% %\textcolor{red}{TODO: DRAW A FLOW DIAGRAM TO SHOW MANIFESTATION }
\label{fig:Overview}
\end{figure}