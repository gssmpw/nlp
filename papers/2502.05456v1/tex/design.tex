\section{Design of CodeImprove}


% Describe overview diagram. 

% Add the genetic algorithms psuedocode. 


% Explain into each section: 
%       -uncertainty measurement to find the threshold. 
%       - using the threshold as the fitness value in program transformation
%       -  Test with new transformed data

% \subsection{Overview}
Figure~\ref{fig:Overview} \textcolor{blue}{provides} an overview of CodeImprove. 
% Given a trained DL model and test instances, our goal is to develop a systematic method that can adapt out-of-scope inputs into in-scope inputs of DL models.  
% As shown in figure~\ref{fig:Overview}, 
CodeImprove consists of two main technical phases: \textit{Input validation} phase to detect out-of-scope inputs from normal inputs, and \textit{Input adaptation} phase to transform out-of-scope inputs to in-scope inputs. %\textit{Finetuning the DL Model, Identifying in-scope and out-of-scope data and the differentiable threshold, and Transforming out-of-scope inputs}. 



%Prior to applying CodeImprove on the problem domain, our first task is to identify the performance of the DL model. Therefore, given a trained DL model and test instances, we first gather the accuracy, precision, recall, and F1 score of the DL model by running the test instances. This helps us to know how well can the DL model perform on the target dataset. Next, we apply our CodeImprove's methodology. 



%%% ISSTA|||The goal of the \textit{Input adaptation} phase is to adapt the out-of-scope data into the DL model. This can be achieved by performing semantic preserving program transformations on the out-of-scope data. Our approach includes a transformation strategy based on genetic algorithms. The threshold score obtained from phase 1 serves as the fitness score to determine if the transformed program is more likely to be converted to in-scope data. %%% end ISSTA




%Finetuning the DL Model phase, consists a target dataset for downstream software engineering tasks. Then, for the testing dataset the finetuned model will do an effectiveness checking such as model accuracy. The next phase is to identify the threshold for in/out-scope data for the testing inputs. We use Dissector, as approach for input validation thorugh cross layer dissection. The input for dissector is the finetuned model and the training and testing datasets. Based on the dissectos PVScore computation, we will identify a threshold value that can differentiate the in-scope and out-of-scope data. For each out-of-scope data that is beyond the threshold value, we apply program transformation using CLONEGEN and apply genetics algorithm to convert the out-of-scope data to in-scope data. 

%edited \subsection{Phase 1: Finetuning the Pretrained Models}
%edited Given a pretrained model, and a target dataset for downstream tasks, the first step is to finetune the model. We include two software engineering downstream tasks: defect detection and clone detection. We include four pretrained models into our study: CodeBERT, RoBERTa, BERT, and DistilBERT. For finetuned model, we test with the testing dataset to analyze the effectiveness of the model in terms of accuracy. 

\subsection{Input Validation}
%%%\textcolor{red}{Dropout based Sub-Model Dissection (DSMD) }
%\textcolor{red}{general idea: how things should be added, like GA, mutation step, }
\label{validation}
Given a trained DL model and test dataset, the goal of this phase is to design a guiding metric for distinguishing in-scope and out-of-scope test data to differentiate the two sets of inputs. 
Our preliminary investigation (Section~\ref{study}) shows that traditional uncertainty metrics are not effective for SE tasks. The findings motivates us to observe the data behaviors in a granular fashion. 
% This involves identifying the contribution of each layer to the model's final prediction. 

%\textcolor{blue}{The encoding of syntax information in code models varies across different layers, as demonstrated by Ma et al.(2022)~\cite{ma2022code}. Their study reveals that certain early layers, such as layer 2, exhibit higher accuracy compared to shallower layers. Conversely, accuracy tends to decrease in later layers of both CodeBERT and GraphCodeBERT models. This observation marks the importance of examining the behavior of each layer in the process of learning code syntax information.}




% \begin{figure}[!htbp]
% \centering
% %\includegraphics[width=\columnwidth]{figs/jjj.JPG}
% %\includegraphics[width=\linewidth, height=4cm]{figs/overviewupdated.drawio.pdf}
% \includegraphics[width=\columnwidth]{tex/images/input_behavior.jpeg}
% \caption{Behavior on each layer's prediction on inputs } 
% % %\textcolor{red}{TODO: DRAW A FLOW DIAGRAM TO SHOW MANIFESTATION }
% \label{fig:behavior}
% \end{figure}

%%Our initial investigation (Section~\ref{study}) indicates that conventional uncertainty metrics may not be effective for software engineering tasks such as vulnerability detection and defect prediction. In light of these findings, our assumption is to scrutinize the data behavior in a more granular fashion. This involves identifying the contribution of each layer to the model's final prediction.
%while layer-wise methods such as dissectors demonstrate to be effective. This gives rise to the assumption that the confidence in a prediction may experience variability as an input undergoes interpretation by the model. 
%Therefore, we select dissector as our uncertainty metric for evaluation. 



We aim to identify an optimal hidden state representation that distinguishes between in-scope and out-of-scope inputs. When code inputs are fed into the DL model, the focus of the model's layers may shift across various aspects of the inputs, such as from structural information to variable names. Existing uncertainty metrics fall short for code inputs because they focus on hidden state outputs of the final few layers of the DL model,  rather than the layer-by-layer processing of inputs. To bridge this gap, we explore sub-models extracted from the original model to gain insights into the processing of inputs. We have devised a method for generating these sub-models and propose using the confidence levels of sub-model predictions as a measure of input validity. 

%\textcolor{red}{Add citation to support. 3.3.1: A survey of uncertainty in deep neural networks.}
%The motivation for using sub-models is driven by the analogy that a group of decision-makers tends to make better decisions than a single decision-maker~\cite{gawlikowski2023survey}
\textcolor{blue}{The intuition for using sub-models is based on the observation that a group of sub-models can reveal layerwise internal understanding of the model on a given input. Inspired by sub-ensemble methods~\cite{gawlikowski2023survey}, sub-models introduce diversity in predictions by employing varied network architectures. Training sub-models independently highlights uncertainties at different layers by isolating layer-wise processing stages (As suggested in Section~\ref{RQ2}, training sub-models performs significantly better than directly using hidden states of the original model). It is only necessary to train the dense layer while freezing the earlier layers, which provides stronger signals of the model's confidence in classifying each category. Moreover, research~\cite{maunveiling} has shown that shallower layers of code models often perform better than deeper layers, making sub-models particularly effective for input validation. This technique ensures that sub-models can focus on distinct aspects of the data, leading to more accurate differentiation between in-scope and out-of-scope inputs.}
%\textcolor{blue}{The use of sub-models improves both the granularity and interpretability of the DL model by enabling layer-by-layer analysis. Such an approach captures intermediate representations that are often overlooked by traditional uncertainty metrics, providing a finer resolution of how inputs are processed throughout the model. Furthermore, training sub-models independently highlights uncertainties at different layers by isolating layerwise processing stages, thus allowing for a more systematic method for input validation. Such a technique ensures that sub-models can focus on distinct aspects of the data, leading to more accurate differentiation between in-scope and out-of-scope inputs. }

\textbf{Sub-Model Generation.} 
Figure~\ref{fig:Sub_model} shows the overview of our Dropout-based Sub-Model Generation (DSMG). 
DSMG creates diverse sub-models to represent the hidden \textcolor{blue}{states of the} original model.
Each sub-model consists of two parts: \textcolor{blue}{the first part is derived from the original DL model, inheriting all structures and parameter values from the first layer up to an intermediate layer k (for Sub-Model$_k$), and the second part is a newly trained link from layer 
$k$ to the output layer, using training samples specific to each software engineering (SE) task.}
%one part is obtained from the original DL model (i.e., for Sub-Model$_k$ is the first layer to intermediate layer k) by inheriting all structures and parameter values, \textcolor{blue}{and} the other part is the link from layer $k$ to the output layer which is trained using training samples \textcolor{blue}{of each SE task in our subject}.
 To generate first part, we obtained dropout-based layerwise hidden representation (illustrated below) of each layer. To generate second part, we adopted a dense layer\textcolor{blue}{, which has been} proven to be effective in DNN for final layer prediction~\cite{huang2017densely,he2016deep} with labels \textcolor{blue}{from the} original DL models. These sub models are trained \textcolor{blue}{using} cross-entropy as its loss function. To mitigate overfitting and enhance generalization, we incorporated dropout regularization technique. \textcolor{blue}{Note that the sub-model generation is conducted only
once in an offline way and are specific to each SE task.}


\textit{Dropout-based layerwise hidden representation.}
In our hidden state representation, submodels of various depths are crafted by selectively activating or deactivating layers, coupled with dropout that randomly omits nodes within these layers. Such representation introduces a dual-layered diversity in the model's hidden states: variability in depth from the layer selection and randomness from node dropout. The introduced diversity helps better represent the model's ability to handle diverse coding patterns and structures by capturing model's understanding on code features. Through such hidden state representation, CodeImprove is better equipped to capture and distinguish the subtle differences between in-scope and out-of-scope inputs.


% Our selected technique generates a sequence of sub-models from the original DL model. Next we explain our sub-model generation process. %We named our technique as Dropout based Sub-Model Dissection(DSMD) that we designed for for code data.

\begin{figure}[!htbp]
\centering
%\includegraphics[width=\columnwidth]{figs/jjj.JPG}
%\includegraphics[width=\linewidth, height=4cm]{figs/overviewupdated.drawio.pdf}
\includegraphics[width=\columnwidth]{tex/images/submodel.pdf}
\caption{Overview of Sub-Model Generation} 
% %\textcolor{red}{TODO: DRAW A FLOW DIAGRAM TO SHOW MANIFESTATION }
\label{fig:Sub_model}
\end{figure}

% \textcolor{red}{Sub model generation: 
%    }

% First, given a deep code model $M$, we designed a novel approach that involves leveraging the hidden state outputs of each layer to construct a sequence of sub-models. Each sub-models represent the growing knowledge embedded within $M$ and consists of two key components.  Firstly, it incorporates the hidden state outputs of each layer in $M$. Secondly, we introduce an additional linear layer. 




%%%%It's crucial to emphasize that the sub-models are essentially enhanced representations of the original hidden states, enriched by the appended linear layer. These augmented sub-models are then trained using the available training set. In our methodology, we opted for linear regression (LR) structures, widely acknowledged for their effectiveness in deep neural networks (DNN), for the final layer prediction [1]. The choice of cross-entropy as the loss function further enhances the model's predictive capabilities.

%%%Notably, the flexibility of our approach allows for the generation of multiple sub-models, corresponding to the number of hidden layers in $M$. Additionally, to mitigate overfitting and enhance generalization, we incorporated dropout regularization techniques, particularly beneficial given the large size of the hidden states.




%%%%%ISSTA |||First, given a deep code model $M$, we utilized hidden state outputs of each layer to create a sequence of sub-models representing the growing knowledge encoded in this DL model. A generated sub-model consists of two parts: first part is the hidden state outputs of each layer of the model $M$ and the other part introduces an additional linear layer. This new layer takes the hidden states as inputs and generates an output. It's important to note that the sub-model is essentially the hidden state of each layer augmented by a new layer, which is trained using the training set. We adopted linear regression (LR) structure which is widely is proved to effective structures in DNN for final layer prediction~\cite{huang2017densely,he2016deep} with cross-entropy as its loss function. To mitigate overfitting and enhance generalization, we incorporated dropout regularization techniques, particularly beneficial given the large size of the hidden states. We can generate as many sub-models as the number of hidden layers. Our experiments generate all sub-models (i.e., 12 for deep code models on transformer based architectures). Later, these sub-models validate a given input interpreted by a deep learning model leading to an increased confidence in its final prediction result. 
%END ISSTAAA






%The DL models should interpret in-scope inputs with increasing confidence scores, while out-of-scope inputs could lead to a decreasing confidence score. As the test input is fed into the DL model, the focus of the model's layers may shift between the different parts of the inputs (e.g.: from structural information to variable names). However, traditional uncertainty metrics focus on the hidden state outputs of each layer or the final logits of the DL model deviate our assumption because such metrics only focus on certain parts of the input data. Therefore, we examine sub-models of the original model to understand how the input is processed layer by layer similar to the existing work dissector~\cite{wang2020dissector}. We define the sub-models prediction confidence as the validity score. 

%Therefore, we select layer-wise dissection-based uncertainty measurement similar to existing work dissector~\cite{} to produce better results. 

%%%oopsla First, our selected technique generates a sequence of sub-models from the original DL model. Since dissector has not provided the implementation of sub-model generation and dissector has applied only to image data, we designed our sub-model generation technique for code data. Next, for a given input, each sub-model will compute a layer-wise validity score with the intuition of higher validity score means the more confident the sub-model prediction and lower validity score if the prediction is likely to be an incorrect prediction. In order to increase or decrease the validity score, we use the difference between the top two predictions. Last, based on each sub-model's validity score, next our approach computes the validity score as a whole. Since each sub-model corresponds to a layer in the original DL model, each sub-model validity score is normalized in terms of increasing weights. Based on the final validity score of the input data, we identify a classification threshold that can be used to differentiate the in-scope or out-of-scope data.  






\textbf{Sub-model Validity Measurement.}
\textcolor{blue}{Equation~\ref{eq:ic} presents the sub-model validity measurement.} For each sub-model, we acquire corresponding probability vectors to understand how the input is processed layer by layer. Suppose for an input $X$ is fed to a DL model $M$ with $n$ labels, and $M$ predicts $X$ as label $l_x$. Let $submodel_k$ be the probability vector associated with the sub-model. There can be two cases: $l_x$ is %the highest probability in $submodel_k$, which 
 a correct prediction, or an incorrect prediction. Based on our intuition, we want to increase the validation score for correct predictions and decrease \textcolor{blue}{it for incorrect ones. To achieve this, we employ the Best Versus Second Best (BVSB) strategy, which evaluates the difference between the highest predicted probability and the second-highest predicted probability (i.e., probabilities for labels $l_h$ and $l_s$ respectively). This difference serves as a measure of the sub-model's confidence in its prediction. A large difference indicates high confidence and leads to an increase in the validity score for correct predictions. Conversely, a small difference suggests uncertainty, resulting in a decrease in the validity score for incorrect predictions. This approach, formulated in Equation~\ref{eq:bvsb}, effectively quantifies the confidence of sub-models in their predictions, thereby adjusting the validity score based on the confidence level of the sub-model's predictions.} 

 
%%% we want to increase the validation score of the correct prediction and decrease in the other case. We increase the validity score of the correct prediction by adding the difference between the top two prediction probabilities (i.e., probabilities for labels $l_h$ and $l_s$ respectively in \textcolor{blue}{equation} ~\ref{eq:bvsb}), while decreasing this for an incorrect prediction. %(equation ~\ref{eq:ic}). 

Once each sub-model computes the respective validity score for each input, we utilize the cross-layer dissection's approach~\cite{wang2020dissector}  (i.e., weight growth types) to compute the validity for the whole DL model (\textcolor{blue}{Equation} \ref{eq:pvscore}).

%%%% removed The existing approach~\cite{wang2020dissector} confuses the correct and incorrect prediction. We increase the validity score of the correct prediction by adding the difference between the top two prediction probabilities ($l_h$ and $l_s$ respectively in eq ~\ref{eq:bvsb}), while decreasing this for an incorrect prediction (equation ~\ref{eq:ic}). 

%we measure its prediction based on the validity score to identify how the corresponding input's final prediction result is uniquely supported by the probability vector in the sub-model. Suppose for input $X$ is fed to a DL model $M$ with $n$ labels, and $M$ predicts $X$ as label $l_x$. Let the sub-model under consideration be $submodel_k$ which is a probability vector. We can have two cases: $l_x$ is associated with highest probability in this vector, or otherwise. Our intuition is to increase the validity for instances that the submodel prediction is similar to $l_x$ while decreasing the validity in the other case. Therefore, we utilize the best-vs-second-best (BvSB) approach where we compute the probability difference between the highest prediction and the second highest prediction. For the first case, we add this difference to the predicted probability. However, for the second case, where $l_x$ is not associated with the highest probability, we negate the validity score as a penalization. Similarly, the larger the difference between the probabilities for best and second-best is, the less uniquely the prediction result $l_x$ is supported in this submodel. Based on this intuition we compute the validity score of each submodel for each input. 

{\small
\begin{gather}
\label{eq:ic}
\text{ValidityScore}_k(l_x, \text{submodel}_k) = \\
\resizebox{0.98\columnwidth}{!}{
$
    \begin{cases}
       \text{submodel}_k[l_x] + B(\text{submodel}_k), & %\text{if } 
        l_x  \text{ with the highest probability} \\[5pt]%\text{submodel}_k  \\[5pt]
        \text{submodel}_k[l_x] -B(\text{submodel}_k), & \text{otherwise}
    \end{cases}
$
} \nonumber
\end{gather}
}

{\small
\begin{equation} \label{eq:bvsb}
    B(\text{submodel}_k) = \text{submodel}_k[l_h] - \text{submodel}_k[l_s]
\end{equation}
}


{\small

\begin{equation}
\label{eq:pvscore}
\resizebox{\columnwidth}{!}{
    Final_{score}(l_x) = \frac{\sum_{k=1}^{n} {ValidityScore_k}(l_x, \textcolor{blue}{submodel_k}) \times weight_{k}}
{\sum_{k=1}^{n} weight_{k}}
}
\end{equation}

}  

%Suppose that an input I is fed to a DL model M (with m labels) and M predicts I as label lx . Dissector considers how a snapshot supports this prediction result. Let the snapshot under consideration be snapshotk , which is a probability vector, taking the form of {l0: prob0, l1: prob1, ..., lm−1: probm−1}. There are two cases: lx is already associated with the highest probability in this vector, or otherwise. For the first case, Dissector (encouragingly) measures the snapshot’s unique support by how much lx ’s associated probability exceeds the second highest probability in this vector. Let lSH the label having the best shot to confuse the prediction (i.e., with the second highest probability). Intuitively, the larger the difference between the probabilities for label lx and lSH is, the more uniquely the prediction result lx is supported in this snapshot. For the second case, lx is not associated with the highest probability. Dissector (penalizingly) measures the snapshot’s unique support by how much the actual highest probability (with label lH ) exceeds that of lx in this vector. Similarly, the larger the difference between the probabilities for label lH and lx is, the less uniquely the prediction result lx is supported in this snapshot. Following this intuition, we measure the snapshot validity as follows (let snapshot[l] return l’s associated probability)

%Consequently, as data traverse the model, the focus may shift between different parts of the inputs - from structural information to something like variable names. Under certain circumstances, the model might concentrate on inappropriate parts of the inputs, leading to high-confidence but incorrect results. In such instances, traditional uncertainty metrics fail to provide reliable measurements


%editied \subsection{Phase 2: In-scope/out-of-scope Input Detection}

%edited The goal of this phase is to identify the in-scope and out-of-scope test data and how we can differentiate the two categories for the given test data. To achieve this goal, we utilized dissector, an approach used to validate input by cross layer dissection. Then, we analyze the dissectors' PVScores for each test input to identify the classification threshold that differentiates in/out scope data.  

%editied: \subsubsection{Dissector to Identify Mis-classified Data}

%edited: Zhang et al.\cite{wang2020dissector} proposed that DL models should interpret within-inputs with increasing confidence, as beyond-inputs could lead to confused predictions during the prediction process. To address this issue, they introduce a lightweight technique named Dissector for automated validation of inputs to deep learning applications. Dissector was first designed to help DL models identify out-of-scope inputs in the task of image classification

%edited: Dissector generates a sequence of sub-models to represent different knowledge of the trained DL model, and these sub-models are used to track how a predicting sample is interpreted through them. If the confidence of the sample towards the final prediction result increases, it will be considered a within-input and the corresponding decision will be given. Otherwise, it will be classified as a beyond-input. %Experiments on four popular image classification datasets demonstrate that Dissector is effective and efficient in helping deep learning models, including LeNet4, WRN, ResNeXt, and ResNet101, identify beyond-inputs.

%As mentioned above, Dissector was first designed to help DL models identify out-of-scope inputs in the task of image classification. For the first time, 
%edited: We have extended the application of Dissector to Software Engineering (SE) tasks, aiming to identify out-of-scope inputs. Our methodology entails the creation of sub-models that are subsequently employed to monitor the interpretation of a predicting sample. Initially, we set up the number of sub-models to four because dissector claims that to achieve promising results four to seven sub-model generation would be sufficient. Since the Dissector only provided corresponding sub-models trained on image datasets, we implement our own sub-models generation strategy. Specifically, we extract the specific layer outputs (hidden states) of the given pretrained model and use them to train the final softmax layer. Then we need to figure out how the input is interpreted in these generated sub-models. The probability vectors obtained by the sub-models are called prediction snapshot. We caculate the validity for each snapshot in the corresponding profile (SV score), and based on the sequence on SV scores, we can finally determine the validity for the entire profile (PV Score). Since each snapshot corresponds to one particular intermediate layer in the original DL model, we normalize these SV scores with increasing weights from the first intermediate layer to the last one, echoing the aforementioned “increasing confidence". Dissector mentions three types of increasing weights, including linear, logarithmic, and exponential, and we only use the linear one in this work. Finally, PV score's value is normalized to [0, 1], the more the inputs is a in-scope to the model, the closer the PV score value is to one. Based on the PV score values for each test input, we identify a classification threshold that helps to differentiate the in/out scope data. 


\subsection{Input Adaptation} 

%\textcolor{red}{rewrite}
%\textcolor{red}{Add explanation about the transformation rules. }

Given an out-of-scope input, the goal of this phase is to covert the input to become an in-scope input. 
\textcolor{blue}{The challenge lies in identifying appropriate program transformations within the vast search space of possible modifications, which includes changes to code structure, logic, and data flow. }
%The challenge lies in identifying such program transformation among the vast search space of possible program transformations, which includes modifications to code structure, logic, and data flow. 
% Existing work on alter code for code models change code semantics~\cite{yuDataAugmentationProgram2022,tian2023code}.
To address \textcolor{blue}{this} challenge, we develop a list of basic semantic preserving transformations and leverage DSMG’s validation score as a guiding metric (Section~\ref{validation}) to combine these basic transformations into a composite transformation that effectively \textcolor{blue}{coverts} the the input from being out-of-scope to in-scope. This method emphasizes iterative refinement guided by the metric, ultimately aiming to find the best solution. We named our approach Adaptation by Evolutionary Search (AES). 

 {
\begin{algorithm}
\caption{High-Level AES Algorithm}
\label{alg:GA}
\small
\DontPrintSemicolon
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}

\KwInput{Pre-trained Model $M$, Testing Dataset $T$, Out-of-scope Input ids $ids$, Maximum Iterations $max\_iter$, Mutation Rate $rate$, Fitness Threshold $threshold$}
\KwOutput{New Test program dataset $N$}

$N \leftarrow []$\;

\ForEach{$sample$ in $T$}{
    \If{$sample.id$ is not in $ids$}{
        $N.\text{append}(sample.code)$\;
        \textbf{continue}
        
        
    }

    $i \leftarrow 0$\;
    $best\_candidate \leftarrow sample.code$\;
    $fitness \leftarrow [0,\ldots,0]$ 

    \While{$i$ \textless{} $max\_iter$ \textbf{and} $fitness$[$best\_candidate$] \textless{} $threshold$}{
        $initial\_pop \leftarrow \text{genpop}(best\_candidate)$
        
        $fitness \leftarrow \text{fitness}(initial\_pop)$ 
        
        $new\_pop \leftarrow \text{select}(initial\_pop)$ 
        
        $pop \leftarrow \text{evolve}(new\_pop, rate)$

        $fitness \leftarrow \text{fitness}(pop)$\;
        
        $best\_candidate \leftarrow \text{select\_best}(pop)$ 
        
        $i \leftarrow i+1$\;
    }

    \If{$fitness$[$best\_candidate$] \textgreater{}$threshold$}{
        $N.\text{append}(best\_candidate)$
    }
    \Else{
        $N.\text{append}(sample.code)$
    }
}

\Return{$N$}\;
\end{algorithm}
}


%One significant challenge is the vast search space of possible program transformations, which includes modifications to code structure, logic, and data flow. Identifying effective transformations that improve performance or correct errors while preserving program semantics is a non-trivial task. Furthermore, manual exploration of this search space is time-consuming, error-prone, and often impractical for large codebases. Genetic Algorithms (GA) offer a promising solution to these challenges by providing an automated and systematic approach to program transformation. By encoding potential program modifications as individuals in a population, GA iteratively evolves these solutions through processes such as selection, crossover, and mutation. This evolutionary process enables GAs to efficiently explore the search space, adapt to changing requirements, and discover effective program transformations that optimize performance or resolve defects. Moreover, GAs can leverage feedback from fitness evaluations, such as runtime profiling or test results, to guide the search towards more promising solutions. Overall, GAs offer a scalable and adaptive framework for addressing the challenges of program transformation by automating the search for effective code modifications while considering the constraints of program correctness and semantic preservation.

%%% After identifying an input as out-of-scope, next step involves adapting these out-of-scope inputs to become in-scope inputs. This phase aims to make semantic-preserving transformations to the input while that the altered input remains equivalent to the original input. Finding the appropriate code semantics for transformation with the objective of finding the optimal solution within the model's handling capability is challenging. 

%ISSTA After identifying an input as out-of-scope, next step involves adapting these out-of-scope inputs to become in-scope inputs. The goal of this phase is to perform transformations on the input such that the alterations are semantic preserving and equivalent to the original input. Therefore, one of the major obstacles is to find the appropriate code semantics to transform with the objective of finding the optimal solution within the model's handling capability. 






%After identifying an input as out-of-scope, the subsequent step involves adapting these out-of-scope inputs to bring them within the in-scope domain. The objective of this phase is to perform transformations on the input, ensuring that the alterations are semantic-preserving and result in an equivalent input compared to the original one.%This task is accomplished by applying semantic preserving program transformations on the out-of-scope inputs. 



% \begin{table}[h!]
% \caption{\textcolor{red}{add eg/ description}Description of code transformation}
% \label{table:transformations}
% \centering
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{ c | l l l}
% \hline
% &\textbf{Code Transformations} & \textbf{Type} & \textbf{Description} \\
% \hline
% 1 & changeRename & Identifier & Rename all variables \\
% 2 & changeCompoundForAndWhile & Structure & Swap for and while loops \\
% 3& changeCompoundWhile & Structure & Change while basic block \\
% 4 &changeCompoundDoWhile & Structure & Change Do-while basic block \\
% 5 &changeCompoundIf (if-else) & Structure &Convert if-else to if-else-if \\
% 6&changeCompoundIf (if-else-if) & Structure & Convert if-else-if to if-else \\
% 7& changeCompoundSwitch & Structure & Convert switch to if-else-if structure \\
% 8 & changeCompoundLogicalOperator & Structure& Change logical expressions \\
% 9& changeSelfOperator & Structure& Change SelfOperator (e.g., i++) \\
% 10 & changeCompoundIncrement & Structure& Change increment operation (e.g., +=) \\
% 11 & changeConstant & Structure& \textcolor{red}{Change Constant} \\
% 12 & changeVariableDefinitions & Structure& Change Variable and Function Definitions \\
% 13 & changeAddJunkCode & Structure& Insert Junk Code \\
% 14 & changeExchangeCodeOrder &Structure&\makecell[l]{Exchange the order of statements without\\dependencies (e.g., declaration statements)} \\
% 15 & changeDeleteCode & Structure&Delete code without semantics (e.g., printf()) \\
% \hline
% \end{tabular}}
% \end{table}

\input{tex/tables/transformation}


\textbf{Program Transformations.} To construct the set of basic program transformations, we considered all common kinds of code structures, \textcolor{blue}{including} loop structures, branch structures, operator, and expression \textcolor{blue}{changes} (Table ~\ref{table:transformations}). 
All transformations are carefully compiled to ensure that the adapted program not only undergoes transformation but also upholds the semantics of the original code. \textcolor{blue}{Our list of transformation operators is designed to be task-preserving by ensuring that the functionality and behavior of the code remain unchanged.} %maintaining the semantic equivalence of the code, ensuring that the functionality and behavior of the code remain unchanged.}
While there is potential to broaden this list of \textcolor{blue}{transformations}, CodeImprove already \textcolor{blue}{shows} significant improvement over existing techniques based on the current list.
\textcolor{blue}{Detailed semantics} with the input-output examples of these transformations can be found on our project website~\cite{CodeImprove}.
% The primary challenge in the code transformation is to inherently retain the semantics of the original code snippet. This necessitates a careful approach .   

%%%The transformation operators employed in CodeImprove manage to preserve the semantics of the original code snippets~\cite{Zhang2023Challenging,yuDataAugmentationProgram2022}. For operator 4, the body statement inside the do-while loop will execute once before entering the transformed while loop. Operator 13 inserts a junk code (e.g., if(0) return 0;) inside the block as a statement, not inside any statement, therefore, does not violate the syntax rules of the code. Operator 14 only reorders the statements without any data or control dependency. Operator 15 only deletes comments or statements that print debugging hints without changing the internal program logic or failing any test cases. For other transformations, they are either mathematically obvious equivalents (such as changing a \textgreater{} b to b\textless{} a) or have already been theoretically proven to be semantically equivalent ~\cite{yuDataAugmentationProgram2022}. 

\textit{Operator $1$.} As an example, we illustrate CodeImprove's implementation for operator $1$, i.e., identifier renaming. Contrary to scenarios in attack or testing where renaming can be more arbitrary~\cite{bertattack}, the process of renaming identifiers for enhancing model performance  is more complex. Replacing identifiers randomly may compromise code's characteristics and models' understanding of the code. This motivates a better renaming strategy for CodeImprove. 

First, CodeImprove \textcolor{blue}{extracts} all identifiers from a given input by parsing the code and \textcolor{blue}{filtering out keywords, macros, and special identifiers (e.g., main, stdio, etc.).}
%and capturing the identifiers by filtering out keywords, macros, and special identifiers (e.g., main, stdio, etc.). 
This step ensures that the identifiers are only variable/function names. Next, the process of renaming identifiers involves two steps: identifying and \textcolor{blue}{substituting} the most important identifier. Our approach is focused on obtaining the logit outputs by the target model for supervision. %Our approach is focused on a black-box scenario, thus the only available supervision is the logit output by the target model. 
We introduce a metric called the importance score.
To compute the importance score we retrieved the tokens of both original code $C = [t_0, \text{· · ·} , t_i,
\text{· · ·} ]$ and code after replacing the identifier with [MASK] at the $i^{th}$ token
 $C_i =[t_0,\text{ · ·} ,t_{i-1} , [MASK], t_{i+1}, \text{· · ·} ] $. The logit output of $C$ and $C_i$ denoted as $O_y(C)$ and $O_y(C_i)$ respectively. The importance score is denoted as: 
 
\begin{equation} \label{eq:idenscore}
    I_i = O_y(C) - O_y(C_i)
\end{equation}

Once the importance scores are computed, we select the top $K$ identifiers. For each identifier in $K$ we replace the original identifier by generating candidates using the RoBERTa masked language model. Then we selected the identifier with \textcolor{blue}{the} highest logit output value to replace the original code. Maintaining $K$ identifiers ensure to limit the large search space because the search space is limited to $K$ identifiers. %CodeImprove then validate the effectiveness of transformed code. 

For other operators, CodeImprove transforms the given source code by a set of atomic operations. Due to the
space limitation, we list all these specific atomic operations at our project homepage~\cite{CodeImprove}. Then, we illustrate how to apply these operators to search for the best solution that deep code models can adapt.

%\textbf{Searching for the Best Candidate Solution for Adaptation: }
\textbf{Adaptation by Evolutionary Search:}
One of the primary requirements before applying transformation operators is to identify syntactic features, i.e., the places of code fragments applicable for transformation. Finding appropriate syntactic features is essentially an optimization problem. CodeImprove addresses this problem by counting the number of code fragments present in each operator for a given code snippet. For example, if there are four identifiers in a source code snippet, then the count of the operator 1 is $K = 4$. 

After identifying the number of syntactic features to be transformed, CodeImprove needs a transformation strategy to generate a \textcolor{blue}{diverse} pool of candidates. CodeImprove achieves this by implementing a genetic algorithm-based~\cite{kumar2010genetic,forrest1996genetic,whitley1994genetic} strategy \textcolor{blue}{comprising} initialization, a fitness function, crossover and mutation operators, and termination criteria to guide the transformation process.  Algorithm~\ref{alg:GA} shows the overview of our transformation strategy. 
The inputs for the AES algorithm are the trained DL model $M$, the test data used to evaluate the $M$'s performance, the identified out-of-scope data, the maximum number of iterations that the AES should evolve for, and the required fitness score that the solution of AES should achieve. \textcolor{blue}{The output of our genetic algorithm is a new dataset that includes the transformed source code.} To avoid randomness when applying transformations, CodeImprove transforms all $K$ counts of features in each operator. 
%The output of our genetics algorithm is to create a new  dataset that includes the transformed source code. Note that to avoid randomness when applying transformations, CodeImprove transforms all $K$ counts of features in each operator. 

Initially, CodeImprove creates a starting population by applying 15 operators to each out-of-scope input (Line 10), with each individual in this population representing a potential solution. The fitness of each individual, determined by the DSMG's validation score (Section~\ref{validation}), reflects how closely a solution approaches the problem's target, with higher scores indicating better candidates (Line 11).




% \textcolor{red}{Applying traditional genetic algorithm for code data is challenging. First, genetic algorithm for code transformation should prioritize semantic preservation. Traditional approaches may operate on string binary, binary or numerical representations~\cite{kumar2010genetic,forrest1996genetic,whitley1994genetic} cannot be directly used as for code data, the focus is the specific code syntax that needs to be transformed. Hence the representation of code data should either be the code snippet itself or the abstract syntax trees (AST). Second, the fitness functions in genetics algorithms are are simple objective functions~\cite{forrest1996genetic}, however, for code data, it is required a metric to guide the transformation to enhance the model's performance. Third, traditional approaches, crossover techniques are focused on single-point, two-point, or uniform crossover~\cite{kumar2010genetic,forrest1996genetic,whitley1994genetic}. However, the crossover operator for code data should be the set of transformations applied to the code snippet and operate directly on the code structure, enabling transformations at statement, expression, or program construct level. Such variations poses challenges in leveraging genetics algorithms for code data. }



Then, the genetic algorithm iterates through cycles of evaluation, selection, and reproduction.  In the selection phase, the top 50\% of candidates are chosen based on their fitness scores (Line 12). During the reproduction phase, CodeImprove performs genetic operators (i.e., crossover and mutation) to generate new
solutions (Line 13). During crossover, for each candidate, CodeImprove applies a sequence of transformations. %(i.e., each candidate in the population will undergo 1, 5, 10, and 15 transformations). 
Then, we add the new samples to our population. For each crossover variant, we mutate the syntactic feature with a random transformation to maintain a diverse population. The algorithm terminates when the candidate code has reached a higher validation score based on our guiding metric or the fixed number of generations has reached. In the end, the algorithm returns the solution with the highest
fitness value (Line 15). %After obtaining the new testing dataset, we input the dataset to identify the performance of program transformation of CodeImprove.
Next, we will describe the experimental evaluation.






 











%%%%ISSTA
%%%ISSTA We employed 15 transformation operators. Table~\ref{table:transformations} describes the transformation rules in our study. Notably, these transformations fall into two sub-categories: identifier transformations that involves altering function/variable names in the code snippet, and code structure transformations that change the code structure but ensures semantic preservation.%For identifier renaming, we propose a  th

%%% ISSTA\subsubsection{Identifier Renaming Transformation}

%%%% ISSTA Given a code snippet, our goal is to rename identifiers while preserving the code structure. Distinguishing from natural language processing (NLP) that renames any word in the sentence~\cite{bertattack}, the process of renaming identifiers in code poses challenges due to the presence of keywords, operators, and macros. Therefore, this tasks requires careful implementation. Additionally, renamed identifier should contain meaningful and human-readable information. For example, identifier renaming in CLONEGEN\cite{Zhang2023Challenging} produces random identifiers that may not be easily readable by humans. This motivates our effort to implement a better renaming strategy in our study. 

%%%%ISSTA Inspired by the existing work~\cite{bertattack, tian2023code}, an identifier renaming transformation in CodeImprove refers to substituting the name of an identifier while preserving the semantics and ensuring the grammatical correctness. Algorithm~\ref{alg:identifier} shows an overview of the identifier renaming approach. However, renaming an identifier imposes aforementioned challenges. To address these challenges, our approach involves extracting all identifiers from a given input by parsing the code and capturing the identifiers (Line 1). During this step, we filter out keywords, macros, and special identifiers (e.g., main, stdio, etc.). This step ensures that the identified identifiers contain only variable/function names, thereby assuring that renaming these identifiers will not violate the semantics of the code. 

%%%issta
% {\small
% \begin{algorithm}
% \caption{Identifier Renaming Algorithm}
% \label{alg:identifier}

% \DontPrintSemicolon
% \SetKwInput{KwInput}{Input}
% \SetKwInput{KwOutput}{Output}

% \KwInput{Pre-trained Model $M$, Code $C$}
% \KwOutput{Transformed Code $T$}

% $V \leftarrow  \text{get\_only\_identifiers}($C$)$\

% \For{$\text{identifier}$ in $V$}{
%     $I \leftarrow \text{Compute\_Importance\_Score}(C, V, M)$
% }

% $K \leftarrow x$

% $Top\_K \leftarrow \text{sort}(I)$

% \For{$\text{identifier}$ in $Top\_K$}{
%     $candidate \leftarrow \text{Generate\_Candidate}(masked\_model, C, identifier)$

%     $Transformed\_Code \leftarrow \text{Replace}(C, candidate, identifier)$
% }

% \Return{$Tranformed\_Code$}\;
% \end{algorithm}
% }

%As a solution, we retrieved all identifier names of a code snippet by parsing the code and retrieving all identifier names. During this process, we made sure to eliminate any key words, macros, and special ids (e.g.: main, stdio etc.). This step guarantees that our identifiers contain only variable/ function names and renaming these identifiers will not break the semantics of the code. 

%%%%ISSTA The process of renaming a filtered identifier involves two steps: identifying the most important identifier for transformation and substituting the original identifier with this important identifier. Our approach is focused on a black-box scenario, thus the only available supervision is the logit output by the target model. To compute the importance score we retrieved the tokens of both original code $C = [t_0, \text{· · ·} , t_i, \text{· · ·} ]$ and code after replacing the identifier with [MASK] at the $i^{th}$ token $C_i =[t_0,\text{ · ·} ,t_{i-1} , [MASK], t_{i+1}, \text{· · ·} ] $. The logit output of $C$ and $C_i$ denoted as $O_y(C)$ and $O_y(C_i)$ respectively. We compute the importance score of each identifier similar to the work done in ~\cite{bertattack}. The importance score is denoted as: 

 %%%ISSTA
%  \vspace{-1em}
%  \begin{equation} \label{eq:idenscore}
%     I_i = O_y(C) - O_y(C_i)
% \end{equation}

%%%%%ISSTA Once the importance scores are computed, we select the top $K$ identifiers (Line 5). For each identifier in $K$ we replace the original identifier by generating candidates using the RoBERTa masked language model. Then we selected the identifier with highest logit value (Line 7 and 8) to replace the original code. Maintaining $K$ identifiers ensure to limit the large search space because the search space is limited to $K$ identifiers. CodeImprove then validate the effectiveness of transformed code. 














%%%Next renaming a filtered identifier involves two steps: finding the most important identifier to be transformed, and replacing the identifier with the most important identifier. We considered a balck box scenario, i.e., user cannot internal access to the deployed model which is very common in production. Therefore, the logit output by the target model is the only supervision we can get. 

%Renaming an identifier (i.e., a variable or function name) with a random name is not effective due to readability issue. Therefore, replacing with a meaningful identifier is effective. Unlike NLP, renaming an identifier in a code is challenging due to existence of key words, operators, and macros. Therefore, this steps needs to be done carefully. 



%Especially, we categorized 15 transformations into two sub-categories. First, identifier transformations that transforms the function/variable names in the code snippet. Second category is code structure  transformations. Rules 2 through 15 are structure transformations we applied in our study.

%%%We employed 15 transformation operators, and Table~\ref{table:transformations} provides an overview of the transformation rules utilized in our study. Notably, these transformations fall into two sub-categories: identifier transformations, which involve altering function/variable names in the code snippet, and code structure transformations. Rules 2 through 15 specifically pertain to structure transformations that were applied in our study.

%%Initially, we select the out-of-scope data that are beyond the threshold score obtained in section ~\ref{validation}. Then, for each out-of-scope input, we apply program transformation rules described in the CLONEGEN\cite{Zhang2023Challenging}. For a given code snippet, we apply each transformation rule once to generate a population of transformed code. Based on this population, we try to find the best version of the transformed code that is more likely to be classified as an in-scope program. We leverage genetics algorithms to find the optimal code snippet that can be adapted by the DL model.

%After identifying the classification threshold and out-of-scope data, the goal in this phase is to adapt out-of-scope inputs into the DL model. This task is accomplished by applying semantic preserving program transformations on the out-of-scope inputs. 



%%We applied 15 program transformation rules using  genetics algorithm as shown in Algorithm~\ref{alg:GA}. 









%After identifying the classification threshold in phase two, we plan to apply program transformations by designing a genetics algorithm. The goal of this phase is to convert out-of-scope data back to in-scope data. In order to transform code, we utilized CLONEGEN. Also, we designed a version of genetics algorithm. 

%%% ISSTA \subsubsection{Code Structure Transformation}CodeImprove aims to transform the code structures. The primary challenge in the code structure transformation is to inherently retain the semantics of the original code snippet. This necessitates a careful approach to ensure that the adapted program not only undergoes transformation but also uploads the semantic of the original code.

%However, the major challenge in transforming code structure is to maintain the semantic properties. Given the code snippet $C$, the transformed code at structure level $C_s$ should preserve semantic of $C$. 

%%%%% ISSTA Particularly, we considered all common kinds of code structures, i.e., loop structures, branch structures, operator and expression structures (Table ~\ref{table:transformations}). We utilized 14 structure transformation rules from CLONEGEN\cite{Zhang2023Challenging}. These rules are general to most popular programming languages (e.g., C, C++, and Java) ensuring the generality of CodeImprove to a large extent. 

%%%%%ISSTA According to Yu et al.~\cite{yuDataAugmentationProgram2022}, operators 5 and 6 are inverse transformations ensuring the semantic equivalence. This is because \textit{if-else-if} and nested \textit{if-else} statements are logically equivalent because they ensure only one code block is executed under a given set of conditions. This equivalence is due to the exclusivity and sequentiality of the conditions. Operator 11 ensures the expression's calculated value is equal to the original constant by applying numerical constraints. Operator 13 adds code that will never be executed. Operator 15 only removes comments and statements that print debugging hints or intermediate results. For other transformations, they are either mathematically obvious equivalents (such as changing a>b to b<a) or have already been theoretically proven to be semantically equivalent ~\cite{yuDataAugmentationProgram2022}.  

%\textcolor{red}{Transformation5 and Transformation6 are inverse transformations of each other, while ensuring semantic equivalence. This is because the if-else-if statement and the nested if-else statement are logically equivalent, as they both evaluate conditions in the same order and ensure that only one corresponding code block is executed under a given set of conditions. This equivalence is based on the exclusivity and sequentiality of the conditions. Transformation11 ensures the expression's calculated value is equal to the original constant through numerical constraints. Transformation13 adds code that will never be executed. Transformation15 only removes comments and statements that print debugging hints or intermediate results. For other transformations, they are either mathematically obvious equivalents (such as changing a>b to b<a) or have already been theoretically proven to be semantically equivalent in other literature~\cite{yuDataAugmentationProgram2022}.}

%%%%%ISSSTA \subsubsection{Genetic Algorithm} We followed a genetics algorithm to apply structure transformation for the code. Algorithm ~\ref{alg:GA} shows the overview of our genetics algorithm. The inputs for the algorithm is the pretrained model M, the testing dataset, and ids of the out-of-scope data, the maximum number of iterations, and the threshold score. The output of our genetics algorithm is to create a new testing dataset that includes the transformed source code. We set up 0.3 and 0.2 as the threshold based on our validity score on detecting out-of-scope inputs for vulnerability detection and defect prediction tasks respectively on all subjects. We set up maximum iteration to three, and crossover rate of 0.5 when applying program transformationson each subject matter described in Section ~\ref{evaluation}.





%Initially, we select the out-of-scope data that are beyond the threshold score obtained in section ~\ref{validation}. Then, for each out-of-scope input, we apply program transformation rules described in the CLONEGEN\cite{Zhang2023Challenging}. For a given code snippet, we apply each transformation rule once to generate a population of transformed code. Based on this population, we try to find the best version of the transformed code that is more likely to be classified as an in-scope program. We leverage genetics algorithms to find the optimal code snippet that can be adapted by the DL model.


%edited: \subsubsection{CLONEGEN to Apply Program Transformations}

%edited: CLONEGEN\cite{Zhang2023Challenging} is a code clone generator proposed recently. Aim to escape the modern clone detection techniques, CLONEGEN accepts a source code snippet as input, executes semantic-preserving transformations, and subsequently produces mutated code snippets. When given an input, CLONEGEN generates semantic-preserving clones designed to elude detection. It incorporates fifteen semantic-preserving code transformation operators. To decide where to apply these operators, CLONEGEN initially utilizes heuristics, including Random-Search (RS), Genetic Algorithm(GA), and Markov Chain Monte Carlo (MCMC). 
%It then proposes a Deep Reinforcement Learning-based strategy to guide the search process. Experimental results demonstrate that CLONEGEN's lightweight and simple transformations are effective and semantic-preserving in evaluating the robustness of clone detectors.

%edited: We utilize CLONEGEN to generate semantically equivalent program transformations to transform out-of-scope data into in-scope inputs. We follows the design of the CLONEGEN's 15 transformation operators. The rules of these 15 transformations are described in Table 1 in the \cite{Zhang2023Challenging}. We include the CLONEGEN's program transformation into our genetics algorithm to further improve the code transformations. 

% \subsubsection{Genetics Algorithm}
%For each out-of-scope data beyond the threshold score, our genetics algorithm contains five stages. We include the transformation rules in CLONEGEN into our genetics algorithm to improve the code generation process. Algorithm ~\ref{alg:GA} shows the overview of our genetics algorithm. The inputs for the algorithm is the pretrained model M, the testing dataset, and ids of the out-of-scope data, the maximum number of iterations, and the threshold score.  The output of our genetics algorithm is to create a new testing dataset that includes the transformed source code. We set up 0.43 and 0.27 as the threshold based on our validity score on detecting out-of-scope inputs for vulnerability detection and defect prediction tasks respectively on all subjects. We set up maximum iteration to three, and crossover rate of 0.2 when applying program transformations on each subject matter described in Section~\ref{evaluation}.

%%%% ISSTA \textbf{Initial Population:} The goal of initial population is to generate a population pool for each out-of-scope test data (line 10). First, we apply 14 transformations (from rule 2-15) for each out-of-scope data sample. Then, we check whether the code has applied the transformation rule because some transformations may not work for source code. For example, if the source code does not contain for-loops, the application of for-to-while-loop transformation will not work and will reproduce the original source code. Therefore, we cross-check the generated code for each transformation before adding the code to the transformation pool. 

%%%%ISSTA \textbf{Fitness Computation: } For each sample in the population pool, we compute the fitness score based on our validity score metric designed in section~\ref{validation} (line 11). Since, the input validation is achieved using the validity score metrics, we kept the same approach to detect the fitness score for our initial population. The validity score of each sample will be the fitness score. 

%%%%ISSTA \textbf{Selection: } Once the fitness score is computed, the goal of CodeImprove is to select the top samples (line 12). We select the top 50\% samples based on the fitness computation. 

%%%%ISSTA \textbf{Crossover:} We create crossover variants from the population by interchanging transformations (line 13). We apply a crossover rate of 0.5 to the list of transformations, i.e., each code in the population will undergo eight continous transformations (i.e., change logical expressions, change self operators, change constants,  change increment operators, change variable definitions, add junk code, change code order (e.g: only declarations), and delete comments/debugging statements). Unlike the other rules, these transformation rules can be applied to any source code (E.g: A for-to-while-loop transformation can be applied only to codes with for-loops etc.). Then we add the new samples into our population. 

%%%ISSTA \textbf{Mutation: } For the created crossover variants, we mutate a few of the components by applying a random transformation (line 13). The goal of mutation is to maintain a diverse population. 

%%%%ISSTA After applying the aforementioned steps generate variants, we compute the fitness score (line 14) for our final set of the pool. Based on the fitness score values, we select the best candidate (line 15). However, based on whether the candidate's fitness score is greater than the threshold score, we replaced the original code with the candidate code (line 18) , else we keep the original code in our dataset after the maximum number of iterations are exceeded.(line 20). Finally, the CodeImprove will output a new test dataset (line 21).

%%%%ISSTA After obtaining the new testing dataset, we input the dataset to identify the performance of program transformation of CodeImprove. Next, we will describe the experimental evaluation. 

%In this work, we need to apply semantically equivalent program transformations to transform out-of-scope inputs into in-scope inputs. So we follow the design of the CLONEGEN's fifteen transformation operators. When performing transformations to generate new code snippets from the original one, we apply all of the operators together.




% \begin{algorithm}
% \caption{High-Level Genetic Algorithm}
% \label{alg:diff}
% \textcolor{red}{This algorithm is naive, I have improved version. Going to add it after experimentation}
% \begin{algorithmic}[1]
% \STATE \textbf{Input:} Pre-trained Model $M$, Testing dataset $T$, Misclassified data ids $ids$
% \STATE \textbf{Output:} New Test program dataset $N$
% \STATE $N$ = []
% \WHILE {$T$ is not empty}
%     \IF {$T$[id] is in $ids$ }
%         \STATE $code = transform(T[code],1)$
%         \STATE $fitness\_score = uncertainty\_score(code)$
%         \IF {$fitness\_score > threshold\_value$}
%             \STATE $code = crossover(code)$
%             \STATE $rand = Random(2,15)$
%             \STATE $code = Mutate (code, rand)$
%             \STATE $add\ code\ to\ N$
        
%         \ELSE
%             \STATE $i = 2$
            
%             \WHILE {$i < 16$}
%                 \STATE $code = transform(T[code], i)$
%                 \STATE $fitness\_score = uncertainty\_score(code)$
%                 \IF {$fitness\_score > threshold\_value$}
%                     \STATE $repeat\ steps\ 9-12$
%                     \STATE $i++$
%                 \ELSE
%                     \STATE $i++$
%                     \STATE $go\ to\ line\ 15$
%                 \ENDIF
%             \ENDWHILE
%         \ENDIF
%         \STATE $add\ T[code]\ to\ N$
%     \ENDIF
% \ENDWHILE
      
% \RETURN $N$
% \end{algorithmic}
% \end{algorithm}

% \subsection{Dissector} 
% \textcolor{red}{zeqing and zijie. briefly describe dissector approach.}
% \zeqing{Zhang et al.\cite{wang2020dissector} propose that DL models should interpret within-inputs with increasing confidence, as beyond-inputs could lead to confused predictions during the prediction process. To address this issue, they introduce a lightweight technique named Dissector for automated validation of inputs to deep learning applications. Dissector generates a sequence of sub-models to represent different knowledge of the trained deep learning model, and these sub-models are used to track how a predicting sample is interpreted through them. If the confidence of the sample towards the final prediction result increases, it will be considered a within-input and the corresponding decision will be given. Otherwise, it will be classified as a beyond-input. Experiments on four popular image classification datasets demonstrate that Dissector is effective and efficient in helping deep learning models, including LeNet4, WRN, ResNeXt, and ResNet101, identify beyond-inputs.}

% \textcolor{red}{here, we need to give a brief overview on how dissector work and how we used dissector for our task}

% \zeqing{As mentioned above, Dissector was first designed to help DL models identify out-of-scope inputs in the task of image classification. For the first time, we have extended the application of Dissector to Software Engineering (SE) tasks, aiming to identify out-of-scope inputs. Our methodology entails the creation of sub-models that are subsequently employed to monitor the interpretation of a predicting sample. Should the confidence of the input towards the final prediction elevate, it is deemed an in-scope input; otherwise, it is classified as an out-of-scope input. This technique enables us to separate out-of-scope inputs from normal ones.}

% \subsection{CLONEGEN for Program Transformation}
% \textcolor{red}{zeqing and zijie. briefly describe clonegen's program transformation approach. }
% \zeqing{
% CLONEGEN\cite{Zhang2023Challenging} accepts a source code snippet as input, executes semantic-preserving transformations, and subsequently produces mutated code snippets. When given an input, CLONEGEN generates semantic-preserving clones designed to elude detection. It incorporates fifteen semantic-preserving code transformation operators. To decide where to apply these operators, CLONEGEN initially utilizes heuristics. It then proposes a Deep Reinforcement Learning-based strategy to guide the search process, aiming to generate clones more effectively.

% Table \ref{tab: Table1} summarizes all the operators used in CLONEGEN, which are lightweight and simple. For example, Op2 replaces for-loops with while-loops in code snippets, making the transformation easy to understand and implement.
% }

% \textcolor{red}{here, we need to give a brief overview on how clonegen work and how we used dissector for our task. We can remove table for now as we can refer the table in the paper itself. We only need the program transformation part of clonegen, also we need to mention that code for RS,GA,MCMC is not avaiable.}

% \begin{table*}
%   \centering
%   \caption{The descriptions and examples of 15 atomic transformation operators in CLONEGEN}
%   \label{tab: Table1}
%   \begin{tabular}{l p{5cm} l l}
%     \toprule
%     \addlinespace[1ex]
%     \textbf{Transformation Operator} & \textbf{Description} & \textbf{Original} & \textbf{Changed} \\
%     \midrule[\heavyrulewidth]
%     Op1-ChRename & Function name and variable name renaming. & int i; & int i1; \\
%     \cmidrule(lr){1-4}
%     Op2-ChFor & The for-loop is transformed into a while-loop. & \begin{lstlisting}[language=C,  columns=flexible]
% for(i=0;i<10;i++){
%     BodyA
% }
%     \end{lstlisting} & 
%     \begin{lstlisting}[language=C,  columns=flexible]
% i=0; while(i<10){
%     BodyA
%     i++;}   
%     \end{lstlisting}
%     \\
%     \cmidrule(lr){1-4}
%     Op3-ChWhile & The while-loop is transformed into a for-loop. & \begin{lstlisting}[language=C,  columns=flexible]
% while(i<10){
% BodyA}
%     \end{lstlisting} & 
%     \begin{lstlisting}[language=C,  columns=flexible]
% for(;i<10;){
% BodyA}
%     \end{lstlisting} \\
%     \cmidrule(lr){1-4}
%     Op4-ChDo & The do-loop is transformed into a while-loop. & 
%     \begin{lstlisting}[language=C,  columns=flexible]
% do{
% BodyA
% }while(i<10);
%     \end{lstlisting} & 
%     \begin{lstlisting}[language=C,  columns=flexible]
% BodyA
% while(i<10){
% BodyA}
%     \end{lstlisting}\\
%     \cmidrule(lr){1-4}
%     Op5-ChIfElseIF & Transformation of if elseif to if else. & 
%     \begin{lstlisting}[language=C,  columns=flexible]
% if(grad<60) BodyA
% else if(grad<80) BodyB
% else BodyC
%     \end{lstlisting}
%     &
%     \begin{lstlisting}[language=C,  columns=flexible]
% if(grad<60) BodyA
% else{ if(grad<80) BodyB
% else BodyC }
%     \end{lstlisting} \\
%     \cmidrule(lr){1-4}
%     Op6-ChIf & Transformation of if else to if elseif. &
%     \begin{lstlisting}[language=C,  columns=flexible]
% if(grad<60) BodyA
% else{ if(grad<80) BodyB
% else BodyC }
%     \end{lstlisting}
%     &
%     \begin{lstlisting}[language=C,  columns=flexible]
% if(grad<60) BodyA
% else if(grad<80) BodyB
% else BodyC }
%     \end{lstlisting} \\
%     \cmidrule(lr){1-4}
%     Op7-ChSwitch & Transformation of the Switch statement to the if elseif statement. & \begin{lstlisting}[language=C,  columns=flexible]
% switch(a){ case 60: BodyA
% case 70: BodyB
% default: BodyC }
%     \end{lstlisting}
%     & \begin{lstlisting}[language=C,  columns=flexible]
% if(a==60) BodyA
% else if(a==70) BodyB
% else BodyC
%     \end{lstlisting}
%     \\
%     \cmidrule(lr){1-4}
%     Op8-ChRelation & Transformation of relational expressions. &
%     \begin{lstlisting}[language=C,  columns=flexible]
% a<b
%     \end{lstlisting}
%     &\begin{lstlisting}[language=C,  columns=flexible]
% b>a
%     \end{lstlisting}
%     \\
%     \cmidrule(lr){1-4}
%     Op9-ChUnary & Modifications to unary operations. &\begin{lstlisting}[language=C,  columns=flexible]
% i++;
%     \end{lstlisting}
%     &\begin{lstlisting}[language=C,  columns=flexible]
% i=i+1;
%     \end{lstlisting}
%     \\
%     \cmidrule(lr){1-4}
%     Op10-ChIncrement & Modifications to incremental operations. &\begin{lstlisting}[language=C,  columns=flexible]
% i+=1;
%     \end{lstlisting}
%     &\begin{lstlisting}[language=C,  columns=flexible]
% i=i+1;
%     \end{lstlisting}
%     \\
%     \cmidrule(lr){1-4}
%     Op11-ChConstant & Modifying constants. &\begin{lstlisting}[language=C,  columns=flexible]
% 8
%     \end{lstlisting}
%     &\begin{lstlisting}[language=C,  columns=flexible]
% (a-b) //8=a-b
%     \end{lstlisting}
%     \\
%     \cmidrule(lr){1-4}
%     Op12-ChDefine & Modifications to variable definitions. &\begin{lstlisting}[language=C,  columns=flexible]
% int b=0;
%     \end{lstlisting}
%     &\begin{lstlisting}[language=C,  columns=flexible]
% int b;b=0;
%     \end{lstlisting}
%     \\
%     \cmidrule(lr){1-4}
%     Op13-ChAddJunk & Adding junk codes. & \begin{lstlisting}[language=C,  columns=flexible]
% if(a){
% BodyA}
%     \end{lstlisting}
%     & \begin{lstlisting}[language=C,  columns=flexible]
% if(a) { BodyA
% if(0) return 0; }
%     \end{lstlisting}
%     \\
%     \cmidrule(lr){1-4}
%     Op14-ChExchange & Change the order of the statements in a block without data and control dependency. & \begin{lstlisting}[language=C,  columns=flexible]
% a=b+10;
% c=d+10;
%     \end{lstlisting}
%     & \begin{lstlisting}[language=C,  columns=flexible]
% c=d+10;
% a=b+10;
%     \end{lstlisting}
%     \\
%     \cmidrule(lr){1-4}
%     Op15-ChDeleteComments & Deleting statements that print debugging hints and comment. & \begin{lstlisting}[language=C,  columns=flexible]
% printf("test");//comments
%     \end{lstlisting}
%     & 
%     \sout{printf("test"); //comments}
%     \\
%     \bottomrule
%   \end{tabular}
% \end{table*}


% \zeqing{Before applying the operators, CLONEGEN encodes the programs using term feature count to represent the number of code fragments that can perform the corresponding operators. The bit vector encodings are also attached to enable transformation strategies. For the transformation strategies, CLONEGEN firstly uses optimization algorithms such as Random-Search (RS), Genetic Algorithm (GA), and Markov Chain Monte Carlo (MCMC) to generate semantic clones with diversity. However, these strategies do not account for feedback from the evaluated clone detectors. To address this, a deep reinforcement learning-based sequence generation (DRLSG) strategy is proposed to effectively guide the search process of generating clones. Experimental results demonstrate that CLONEGEN's lightweight and simple transformations are effective and semantic-preserving in evaluating the robustness of clone detectors.

% We employ CLONEGEN to carry out semantic-preserving program transformations. Since the implementation code for CLONEGEN, based on RS, GA, and MCMC, is not available, we adopt the DRLSG strategy to guide the transformation process.}



      