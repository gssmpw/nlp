\section{Contributions}

In summary, this research makes the following contributions: 

\begin{itemize}
    \item \textbf{Development of Multi-Level Validation Metrics}: This work introduces a comprehensive set of validation metrics to detect out-of-scope inputs across diverse model architectures, including encoder, decoder, and encoder-decoder models. These metrics support tasks in code, NLP, and image domains, enabling precise identification of inputs prone to misprediction.
    %My research introduces a set of validation metrics designed to detect out-of-scope inputs across different model architectures, including encoder models, decoder models, and encoder-decoder models. These metrics address diverse tasks across code, NLP, and image domains, enabling more precise identification of inputs likely to result in mispredictions.
    \item \textbf{Automated Input Transformation Techniques}: The proposed framework includes semantic-preserving input transformation techniques tailored to each data type (discrete and continuous), effectively aligning out-of-scope inputs with model capabilities.
    %My proposed framework includes effective input transformation techniques that apply semantic-preserving transformations tailored to each data type (discrete and continous). 
    \item \textbf{Adaptive Search Strategy}: An innovative search strategy dynamically identifies the optimal refinements for out-of-scope inputs, converting them into in-scope representations and improving model performance without retraining.
    %My research introduces a novel search strategy to dynamically identify the optimal refinement for out-of-scope inputs, allowing the model to convert out-of-scope inputs into in-scope representations. 
    \item \textbf{Comprehensive Evaluation Framework}: Extensive testing across various model architectures and tasks demonstrates the framework’s scalability and adaptability in real-world applications.
    %Extensive testing is conducted across various model architectures and tasks, demonstrating the scalability and adaptability of the framework in real-world applications.
    \item \textbf{Scalable, Resource-Efficient Solution for Model Deployment}: By dynamically refining inputs at inference, this framework offers a scalable, resource-efficient alternative to frequent retraining, making it well-suited for agile development and large-scale deployment, while reducing operational costs and maintaining model performance.
    %By providing a framework that dynamically transforms inputs at inference, this research offers a scalable and resource-efficient alternative to frequent retraining. This solution is ideal for agile development environments and large-scale deployment scenarios, reducing costs and improving model performance on an ongoing basis.
    \item \textbf{Publications and Open Science Contributions}: This research advances the field through publications in leading AI and software engineering venues. Findings and public artifacts will be shared in alignment with open science principles, supporting reproducibility, transparency, and adherence to high research standards.
    %This work contributes to the field through publications in leading AI and software engineering venues. Research findings, along with public artifacts, are shared in alignment with open science principles, ensuring reproducibility, transparency, and adherence to high research standards.
\end{itemize}


% \begin{itemize}
%     \item \textbf{Development of Validation Metrics}: My research introduces a set of validation metrics specifically designed to identify out-of-scope inputs in CLM). These metrics are applicable across various CLM architectures, including encoder-only, decoder-only, and encoder-decoder models, and are suited for both classification tasks (e.g., vulnerability detection, defect prediction) and generation tasks (e.g., code completion).
%     \item \textbf{Automated Input Refinement Techniques}:My research proposes effective input refinement techniques that combine semantic-preserving transformations and optimized sampling strategies. These techniques are adaptable to different types of CLMs and are designed to improve model confidence and accuracy on previously out-of-scope inputs without altering model parameters.
%     \item \textbf{Comprehensive Evaluation Framework}: My research conducts extensive testing across diverse CLM architectures and tasks, demonstrating the framework’s scalability and adaptability in improve inference performance. This testing includes classification tasks like vulnerability detection and defect prediction, as well as generation tasks like code completion.
%     \item \textbf{Scalable, Resource-Efficient Solution for CLM Deployment}: By providing a framework that improves CLM performance at inference time, my research offers a scalable, cost-effective alternative to frequent retraining, making it highly suitable for agile development and large-scale deployment scenarios.
%     \item \textbf{Publications}: Contributes to the field by publishing research findings in top software engineering venues and sharing public artifacts, in adherence to the ACM’s open science policy to ensure reproducibility, openness, and high-quality research standards.
% \end{itemize}
