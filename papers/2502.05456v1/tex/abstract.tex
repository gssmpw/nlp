\begin{abstract}
%\textcolor{red}{}

Advancements in deep learning have significantly improved model performance across tasks involving code, text, and image processing. However, these models still exhibit notable mispredictions in real-world applications, even when trained on up-to-date data. Such failures often arise from slight variations in inputs such as minor syntax changes in code, rephrasing in text, or subtle lighting shifts in images that reveal inherent limitations in these models’ capability to generalize effectively. Traditional approaches to address these challenges involve retraining, a resource-intensive process that demands significant investments in data labeling, model updates, and redeployment.

This research introduces an adaptive, on-the-fly input refinement framework aimed at improving model performance through input validation and transformation. The input validation component detects inputs likely to cause errors, while input transformation applies domain-specific adjustments to better align these inputs with the model’s handling capabilities. This dual strategy reduces mispredictions across various domains, boosting model performance without necessitating retraining. As a scalable and resource-efficient solution, this framework holds significant promise for high-stakes applications in software engineering, natural language processing, and computer vision.



%Advancements in deep learning have significantly improved model performance for code, text, and image processing; however, these models still exhibit notable mispredictions in real-world applications, even when trained on current data. Such failures often stem from minor variations in inputs, such as minor syntax changes in code, rephrasing in text, or slight lighting adjustments in images which underscore inherent limitations in these models’ capability to generalize effectively. Traditional methods to address these issues involve retraining, an expensive endeavor requiring substantial resources for data labeling, model updates, and deployment. 

%This research proposes an adaptive, on-the-fly input refinement framework designed to improve model performance through input validation and transformation. The input validation component identifies inputs that are prone to cause model errors, while input transformation employs domain-specific transformations to adjust these inputs, thereby aligning them more closely with the model’s handling capabilities. This dual approach reduces mispredictions across varied domains, improving model performance without the need for retraining. As a scalable and resource-efficient solution, this framework offers particular promise for high-stakes applications in software engineering, natural language processing, and computer vision tasks.


%Despite advancements in deep learning, models for code, text, and image data often mispredict in real-world applications, even with up-to-date training. These failures arise from subtle input variations such as small syntax changes in code, rephrasing in text, or lighting differences in images which reveal fundamental limits in models' ability to generalize. Traditional solutions rely on retraining, a costly process in terms of data labeling, model updating, and deployment.  

%This research introduces an on-the-fly framework to improve model performance through input validation and input refinement. Input validation identifies inputs likely to cause errors, while input refinement applies domain-specific transformations to align these inputs with the model’s handling capability. This approach reduces mispredictions across diverse domains, improving model performance without retraining. The framework provides a scalable, resource-efficient solution to improve model performance, making it ideal for high-stakes applications in software engineering, NLP, and vision tasks.


%Deep learning-based code language models (CLMs) are increasingly used to tackle a wide array of software engineering tasks. However, these models frequently encounter performance degradation due to issues such as data shifts in code, limiting their effectiveness in real-world applications. Traditional solutions rely on retraining, a costly process in terms of data labeling, model updating, and deployment. My research explores a more affordable solution to enhance CLM performance directly at deployment. This approach consists of two core steps: (1) input validation, that focuses on identifying whether an input is an out-of-scope input that are beyond a model’s handling capability, and (2) input refinement which adapts these out-of-scope inputs to become in-scope through transformations and sampling techniques, tailored to generative and classification tasks. 

%My findings reveal that input validation for CLMs presents unique challenges, as traditional validation techniques for continuous data are often ineffective when applied to discrete code data. Additionally, input refinement demands an efficient exploration of large search spaces to apply semantic-preserving transformations or optimize sampling strategies. To address these challenges, I introduce a set of robust validation metrics designed for both classification and generation tasks, applicable across diverse CLM architectures. Furthermore, I propose optimized search strategies that integrate program transformations and targeted sampling to improve model confidence and reliability without the need for retraining.

%which adapts these out-of-scope inputs to become in-scope through transformations and sampling techniques, tailored to generative and classification tasks.(2) input refinement, which adapts these out-of-scope inputs to become in-scope. 



%%%Deep learning (DL)-based code analysis tools, aka, code language models (CLM) tools are becoming increasingly popular to solve software engineering tasks. CLMs often suffer performance degradation due to various reasons (e.g., code data shifts). Traditional approaches to solve these issues focus on retraining. However, frequent model updates are costly in labeling and deployment.  My research focus on identifying much more affordable solution to improve the CLM performance during deployment. This can be achieved by two steps: 1) input validation that focus on identifying whether the input is beyond the model's handling capability and 2) input refinement that adapts these out-of-scope inputs to become in-scope. Based on our findings, input validation is challenging as existing methods focus on continuous data often fail with discrete data, and input refinement require vast search spaces. My research introduces an effective validation metric for CLMs on both classification and generation task for all type of CLMs, effective search strategies to use program trasnformations or sampling techniques. 


% Leveraging deep learning (DL)-based code analysis tools to solve software engineering tasks is becoming increasingly popular. 
% %However, these tools face challenges related to performance limitations, scalability issues, and the potential for erroneous results (i.e., mispredictions). 
% \textcolor{blue}{Code models often suffer performance degradation due to various reasons (e.g., code data shifts).}
% %\textcolor{blue}{Existing approaches on improving the accuracy of these tools face challenges related to performance limitations, scalability issues, and the potential for erroneous results (i.e., mispredictions).}
% Retraining is often required to address these issues, but frequent model updates are costly in labeling and deployment. In this paper, we explore an alternative solution: Adapting the program inputs to the code models. This can be achieved by two steps: 1) input validation that focuses on identifying whether an input is an out-of-scope input program that are beyond a model’s handling capability, and 2) input adaptation that adapts out-of-scope inputs to become in-scope inputs.
% %%%Leveraging deep learning (DL) code models to solve software engineering (SE) tasks is becoming increasingly popular. However, code data shifts, often caused by software evolution, degrade the performance of deep code models by creating syntactic mismatches between training and testing code data. To address this issue, retraining is often required, but frequent model updates are costly in labeling and deployment. In this paper, we explore an alternative solution: Adapting the program inputs to the code models. This can be achieved by two steps: 1) input validation that focuses on identifying whether an input is an out-of-scope input program that are beyond a model’s handling capability, and 2) input adaptation that adapts out-of-scope inputs to become in-scope inputs.
% %On the one hand, validating input is challenging because the existing techniques are mainly focused on image data and cannot be used for code data due to different data shift patterns (e.g.: code syntax change versus image matrix transformation). On the other hand, adapting an out-of-scope program can be challenging due to large search spaces, i.e., an out-of-scope program can be modified in a significantly large number of ways while preserving the program semantics. Designing an input validation approach specifically for code data for input validation and applying a set of program transformations to limit the search space can solve the aforementioned challenges. 
% Validating program input is challenging, as current techniques focus on continuous inputs such as image data and fail with discrete inputs like code data, which have unique characteristics and are processed differently by deep learning models. Adapting out-of-scope programs is also challenging due to their vast search spaces.
% Therefore, in this paper, we propose CodeImprove, which distinguishes out-of-scope from normal inputs and converts such out-of-scope inputs back to in-scope inputs through program transformation. In particular,  we propose a validity score metric to identify out-of-scope inputs and leverage genetics algorithms to apply semantic preserving program transformation to convert out-of-scope inputs to in-scope inputs. Our experimental results show CodeImprove can enhance upto 8.78\% of accuracy, and 51.28\% of relative improvements in three code models on two SE tasks. Additionally, our input validation is promising in detecting out-of-scope inputs (AUC score of 0.924). 


% %\textcolor{red}{Our experimental results show improvements in accuracy (2.01\%-6.25\%), precision (1.71\%-6.61\%), recall (1.41\%-11.39\%), F1-score(2.10\%-9.83\%), and inaccuracy dropdown percentage (6.32\%-16.77\% ) on vulnerability detection and defect prediction tasks for CodeBERT, RoBERTa, and BERT models}. 


% % which works in the input space of the code models by utili

% % design an approach specifically for code data to improve the modelling of input handling scope of a model

% % in this paper, we propose CodeImprove which works in the input space of the code models. 

% % CodeImprove distinguishes those inputs that are beyond the model's handling capability (out-of-scope inputs) from normal inputs (in-scope inputs) and converts such out-of-scope inputs back to in-scope inputs through program transformation. In particular,  we propose an importance score metric to identify out-of-scope inputs and leverage genetic algorithm to apply semantic preserving program transformation to convert out-of-scope inputs to in-scope inputs. Our experimental results on the vulnerability detection task show a 6.8 percent improvement in the prediction accuracy for CodeBERT models.

% % Also, modeling an input
% % handling scope of the model is challenging due to the representation of code data that hardly comprehends all
% % syntactic and semantic information in the code. 

% % 1. changing the model is expensive. 
% % 2. so we need to change the input. 
% % 3. to change an input, there need to be done two things. 1) checking if the input is in the scope 2) adapting out-of-scope inputs to in-scope inputs.
% % 4. challenges in each step.
% % 5. what we propose. 
% %Also, handling code data by the existing DL models is hindered due to a limited understanding of the inherent code data’s nature.   

 
% %As a result, development teams rely on strategies such as retraining or replacing the DL model. However, such techniques impose limitations on 

% %Leveraging Deep Learning (DL) models in software engineering (SE) tasks (code clone detection, defect detection, etc.) is becoming increasingly popular. However, handling code data by the existing DL models is hindered due to a limited understanding of the code data’s nature and a lack of domain-specific knowledge on SE tasks. Therefore, not knowing what a DL model cannot handle impedes the adoption of DL-based code analysis. As a result, DL models have limited performance during inference due to evolving programs causing syntactic differences between training and testing code, as well as, placing higher weights on specific code syntax by the model. In this paper, we propose CodeImprove, to distinguish those inputs that represent out-of-scope inputs from normal inputs. CodeImprove applies semantically equivalent program transformations to transform out-of-scope inputs into in-scope inputs. This can be accomplished by leveraging evolutionary programming to identify variants of programs that are capable of becoming in-scope inputs. Our experimental results on the vulnerability detection task show a 6.8 percent improvement in the prediction accuracy for CodeBERT models. 

% %Leveraging Deep learning (DL) code models to solve software engineering (SE) tasks is becoming increasingly popular. However, code data shifts, often caused by software evolution, degrade the performance of deep code models by creating syntactic mismatches between training and testing code data. While retraining or replacing the DL model are promising solutions, working on the input space of the DL model can save costs in labeling, rebuilding, and redeploying. In this paper, we propose CodeImprove to distinguish those inputs that are beyond the model’s handling capability (out-of-scope inputs) from normal inputs (in-scope inputs), and convert such out-of-scope inputs back to in-scope inputs that are within the model’s handling capability. In particular, we propose an importance score metric to identify out-of-scope inputs and apply a genetic algorithm-based semantic preserving program transformation to convert out-of-scope inputs to in-scope inputs. Our experimental results on the vulnerability detection task show a 6.8 percent improvement in the prediction accuracy for CodeBERT models
\end{abstract}