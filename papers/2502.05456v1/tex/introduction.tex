\section{Introduction}


Deep learning models have achieved remarkable success across domains such as software engineering, natural language processing (NLP), and computer vision~\cite{tian2023fly, naturalattack, Zhang2023Challenging, lu2021codexglue,svyatkovskiy2020intellicode,Devlin2019BERT,Sanh2020DistilBERT,guo2021graphcodebert,Chen2021Evaluating,xiao2022repairing,xiao2021selfchecking}. Generally, these models are trained on large, current datasets and evaluated on test data that closely mirrors the training distribution. However, despite this alignment, models frequently mispredict in real-world applications, not only due to distribution shifts but also due to their inability to manage slight input variations within the same distribution.

%%Deep learning models have achieved remarkable success across diverse domains such as software engineering, natural language processing (NLP), and computer vision. These models are typically trained on large, up-to-date datasets and evaluated on test data that aligns closely with the training distribution. However, despite this alignment, models frequently mispredict in real-world applications. These failures occur not only due to shifts in data distribution but because models struggle to handle nuanced, subtle variations present even within the same data distribution. %This phenomenon highlights a fundamental limitation in deep learning: \textit{models learn patterns based on statistical correlations, yet they lack the contextual understanding and adaptability necessary to reliably interpret small but meaningful differences in input data}.

A fundamental limitation of deep learning models is their reliance on statistical correlations rather than true understanding, restricting their ability to consistently interpret even familiar inputs. For instance, code language models (CLMs) often mispredict when confronted with minor syntactic changes, such as variable renaming or slight control flow adjustments~\cite{tian2023fly, naturalattack, Zhang2023Challenging}. These models learn code as sequential patterns, lacking an understanding of underlying functionality, making them fragile to variations that are functionally equivalent.




%Deep learning models are fundamentally constrained by their dependence on statistical correlations rather than intrinsic understanding, limiting their ability to consistently interpret inputs—even those aligned with training data distributions. In code language models (CLMs), for instance, slight syntactic modifications, such as variable renaming or marginal control flow adjustments, can lead to mispredictions because these models lack a true grasp of underlying functionality and dependencies. They perceive code primarily as a sequence of patterns rather than a cohesive logical structure, resulting in fragile predictions sensitive to minor, functionally equivalent variations.

Similarly, NLP models capture linguistic patterns but lack human-like adaptive interpretation of context, tone, or intent~\cite{chowdhary2020natural}. Slight changes in phrasing or word emphasis, even within familiar contexts, can yield different meanings that these models fail to understand especially in nuanced tasks like sentiment analysis and dialogue systems. Image-based models~\cite{xiao2022repairing,xiao2021selfchecking, wang2020dissector} are equally vulnerable, as slight perturbations in lighting or angle can lead to mispredictions due to their reliance on precise feature mappings rather than broader perceptual generalization.



%%In natural language processing (NLP), models capture linguistic patterns but lack the adaptive interpretive capacity that human cognition applies to context, tone, and intent. Even when inputs resemble those seen in training, subtle shifts in phrasing or word emphasis can produce divergent meanings that these models fail to apprehend, particularly in nuanced applications like sentiment analysis and dialogue systems.

%%Similarly, image-based models are highly susceptible to minor perturbations in visual inputs, such as lighting shifts or angle adjustments, due to their dependence on precise feature mappings. Although trained on diverse visual data, these models often misclassify inputs that deviate slightly from learned representations, as they cannot generalize beyond narrowly defined perceptual patterns. 

These domain-specific limitations reveal a broader challenge: \textit{while deep learning models excel at reproducing complex statistical relationships, they lack the flexibility, contextual reasoning, and adaptability needed to handle the complexities inherent in real-world data}. \textbf{Retraining}~\cite{yuDataAugmentationProgram2022,xiao2021selfchecking,xiao2022repairing}, a traditional approach to addressing these issues which involves periodic model updates to account for new or rare patterns. However, retraining is resource-intensive, requiring substantial computational power, time, and labeled data. Additionally, retraining can lead to overfitting on new data and is, at best, a temporary solution for handling the infinite variations of real-world inputs. These limitations underscore the need for a more adaptive approach in deep learning systems.


%%These domain-specific limitations underscore a broader philosophical challenge: \textit{while deep learning models excel at reproducing complex statistical relationships, they lack the inherent flexibility, contextual reasoning, and conceptual adaptability required for reliable performance in the subtle complexities of real-world applications}.

%Deep learning models exhibit fundamental limitations across domains due to their reliance on pattern recognition rather than true contextual understanding. In code language models (CLMs), even slight syntactic changes—like renaming a variable or altering control flow—can disrupt the model’s interpretation, leading to mispredictions. These models identify syntactic patterns but lack a functional grasp of code logic and dependencies, which is essential for reliable analysis. As a result, CLMs frequently fail to handle minor code variations, undermining their effectiveness in tasks like code generation and vulnerability detection.

%Similarly, NLP models are challenged by the subtlety and variability of human language. Small shifts in phrasing, tone, or idiomatic expressions, while common in real-world language, can alter meaning significantly. Even with extensive data, NLP models lack the adaptive, context-driven interpretation that humans use to understand these variations. This results in unreliable predictions, particularly in sentiment analysis and conversational AI, where meaning is deeply context-dependent.

%For image-based models, the high-dimensional nature of visual data makes them vulnerable to slight environmental changes, such as variations in lighting, angle, or occlusions, which can alter perceived features and lead to misclassification. This sensitivity to minor visual shifts is particularly problematic in applications like autonomous driving and medical imaging, where stable, accurate interpretation is critical. These limitations reveal that while models can learn complex patterns, they lack the inherent flexibility and contextual reasoning needed to perform reliably in the nuanced conditions encountered in real-world settings.



%The limitations of deep learning models manifest uniquely in each domain. In code language models (CLMs), for instance, even minor syntactic or structural changes, such as renaming variables or slightly adjusting control flow, can alter program functionality or meaning entirely. CLMs are highly sensitive to these changes; although they may be within the model’s data distribution, such variations often lead to mispredictions. This is because CLMs are trained to recognize patterns within code syntax but lack the intrinsic understanding of dependencies and functionality that are critical for robust code analysis. In practical settings, this means that CLMs may frequently misinterpret inputs that are only subtly different from those they have seen, which poses a significant limitation for tasks like code generation and vulnerability detection.

%In NLP, similar issues arise from language’s inherent complexity and context dependence. NLP models are designed to interpret and generate human language, yet they often fail to account for small shifts in phrasing, tone, or idiomatic expressions that are well within the same data distribution but carry different contextual meanings. For example, minor changes in punctuation or word choice can drastically alter sentiment or intent, leading to mispredictions in applications such as customer service and sentiment analysis. While NLP models are trained on extensive datasets that cover a range of linguistic structures, they fundamentally lack the cognitive flexibility to process language as humans do, which results in unreliable performance when faced with subtle linguistic variations.

%Image-based models, on the other hand, contend with the high-dimensional variability of visual data. In real-world settings, slight changes in lighting, angle, or background elements can alter the visual features on which these models rely, causing incorrect classifications. This sensitivity is particularly problematic in fields like healthcare imaging and autonomous driving, where high accuracy and stability are essential. Although these models may be trained on diverse visual data, their architecture often limits their ability to generalize to minor visual perturbations, revealing a fragility that undermines consistent performance.

%%Despite advancements in model architecture, these failures persist due to fundamental architectural limitations. Deep learning models are highly reliant on statistical associations within data and lack mechanisms for understanding context, intent, or adaptability. As a result, even small variations in input can cause major shifts in predictions, especially in NLP and vision tasks. Moreover, models are designed to capture patterns from a fixed training distribution and struggle to generalize to inputs that deviate slightly, even when these inputs remain within the general distribution. This sensitivity to minor perturbations underscores the limitations of current architectures and suggests that retraining alone cannot fully address these issues.

%%%A traditional approach to addressing these limitations is \textbf{retraining}, where models are periodically updated with additional data or adjusted parameters to capture new or rare patterns. However, retraining is often impractical due to its high demands on computational power, time, and curated labeled data—resources that are costly and difficult to obtain. Additionally, retraining risks model degradation, as overfitting to new data can compromise general performance. Critically, retraining remains a limited solution: it cannot account for the infinite subtleties and variations inherent in real-world inputs. Thus, retraining offers, at best, a temporary fix rather than a scalable, enduring response to the complexities of dynamic environments, highlighting the need for a more adaptive approach in deep learning systems.

%A traditional approach to addressing these limitations is \textit{retraining}, where models are periodically updated with additional data or modified parameters to capture new or rare patterns. However, retraining is often an impractical solution for several reasons. First, it is  resource-intensive, demanding extensive computational power, time, and curated labeled data—resources that are costly.  Additionally, retraining introduces the risk of model degradation, where the model overfits to new data at the expense of general performance. Moreover, retraining is not a definitive solution; it cannot anticipate or cover the infinite subtleties and variations present in real-world inputs. Consequently, retraining provides, at best, a temporary measure rather than a scalable or enduring response to the nuanced demands of dynamic environments, underscoring the need for an alternative approach to achieve true adaptability in deep learning systems.

%%A common response to these challenges is \textit{retraining}, which involves augmenting the model with more data or adjusting its parameters to better capture new or rare patterns. However, retraining is often an impractical solution for several reasons. First, it is resource-intensive, requiring time, computational power, and labeled data. In many fields, such as autonomous driving or medical imaging, obtaining new labeled data is labor-intensive and costly. Additionally, retraining introduces the risk of model degradation, where the model overfits to new data at the expense of general performance. This approach also does not guarantee that every subtle variation encountered in real-world settings will be represented in the expanded data. As such, retraining fails to offer a scalable, long-term solution to these issues of input variability and sensitivity.

To address these challenges, this research introduces an adaptive framework for \textbf{input refinement}. Rather than relying on retraining, this framework improves model performance by dynamically performing \textbf{input validation} and \textbf{input transformation} during inference. Through a structured sequence of phases, this approach provides a scalable and resource-efficient solution for reducing mispredictions without modifying the model itself.

%%To overcome the limitations of traditional deep learning approaches in handling  input variability, this research presents a comprehensive framework for \textbf{input refinement}. This framework directly addresses the challenge of misprediction by equipping models with the capability to \textbf{input validation} and \textbf{input transformation} dynamically at inference. Through a structured sequence of phases, the framework provides a scalable, resource-efficient solution for enhancing model reliability without relying on retraining.

The framework consists of three main phases, each targeting specific challenges. The first phase, \textbf{Input Validation (P1)}, identifies inputs likely to cause errors, even if they are within the model’s training distribution. Tailored to various architectures, this phase includes sub-phases for validation encoder models \textbf{(P1.1)}, validation for decoder models \textbf{(P1.2)}, and validation for encoder-decoder models \textbf{(P1.3)}.



%%The framework consists of three main phases, each with specialized sub-phases designed to address specific challenges. \textbf{Input Validation (P1)} focuses on identifying inputs that are likely to produce wrong predictions, even if they technically fall within the model’s data distribution. This phase examines the unique architectural behavior of inputs in various model types, ensuring validation is tailored to the specific processing stages of each architecture. Input validation includes three sub-phases: validation for encoder models \textbf{(P1.1)}, validation for decoder models\textbf{(P1.2)}, and validation for encoder-decoder models \textbf{(P1.3)}.
%By evaluating layer-wise consistency, each sub-phase identifies subtle input variations that may destabilize the model’s interpretive accuracy.

The second phase, \textbf{Input Transformation (P2)}, adjusts inputs identified "out-of-scope" in the validation phase, applying domain-specific modifications to ensure alignment with the model's handling capabilities. This phase includes discrete transformations for code \textbf{(P2.1)} and text \textbf{(P2.2)}, as well as continuous transformations for images \textbf{(P2.3)}, preserving the intrinsic meaning or functionality of inputs while ensuring they align more closely with the model’s learned patterns.

%%\textbf{Input Transformation (P2)} targets the out-of-scope inputs from the validation phase, applying domain-specific transformations to adapt them at the data level. This phase addresses the unique properties of different data types through two primary sub-phases: discrete data transformation (for code \textbf{(P2.1)} and text \textbf{(P2.2)}) and continuous data refinement (for images \textbf{(P2.3)}). These refinements adjust inputs in a way that preserves their intrinsic meaning or functionality, ensuring they align more closely with the model’s learned patterns and processing strengths.

The final phase, \textbf{Optimal Search Strategy (P3)}, iterates over transformed input variations to find the best representation for model processing. This adaptive search further reduces mispredictions without changing model parameters or requiring retraining.

%%The final phase, \textbf{Optimal Search Strategy (P3)}, further enhances adaptability by iterating over refined versions of out-of-scope inputs to determine the best representation for model processing. This phase involves a search sub-phase that dynamically selects the optimal transformation, thereby reducing mispredictions without altering model parameters or requiring retraining.


This research hypothesizes that a framework incorporating input validation, targeted transformation, and optimal search will improve deep learning models' performance across code, NLP, and image domains. By dynamically aligning inputs with learned model patterns, this approach aims to significantly reduce mispredictions in real-world settings without requiring retraining, offering a scalable and efficient solution for robust AI performance in complex environments.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=\linewidth]{tex/images/ICSEDS.jpeg}
\caption{Overview of Proposed On the Fly Input Refinement Framework} 
% %\textcolor{red}{TODO: DRAW A FLOW DIAGRAM TO SHOW MANIFESTATION }
\label{fig:Overview}
\end{figure*}

%%My research hypothesizes \textbf{A novel framework incorporating input validation, targeted transformation, and an adaptive search strategy at inference time will improve deep learning models’ performance across code, NLP, and image domains. By dynamically adjusting inputs to align with learned model patterns, this approach will significantly reduce mispredictions in real-world settings without requiring retraining, providing a scalable and efficient solution for robust AI performance in complex environments}.



%The persistence of these mispredictions, even with well-aligned data and the limitations of retraining, motivates a shift from traditional approaches toward a more adaptable framework. This research proposes a novel solution to improve model reliability by implementing input validation and input refinement directly at inference time. Input validation identifies inputs that, while technically within the model’s learned distribution, are likely to lead to mispredictions due to their nuanced variations. By examining layer-wise consistency in model processing, input validation can effectively flag inputs that challenge the model’s interpretation capabilities. 

%For inputs flagged as potentially problematic, input refinement applies domain-specific transformations to adapt these inputs to the model’s learned processing scope. In code, this may involve syntactic or structural adjustments that preserve functionality; in NLP, rephrasing or standardizing informal language to align with expected linguistic patterns; and in images, adjustments to lighting or contrast to normalize visual variability. These transformations help align inputs more closely with the model’s expectations, reducing misprediction rates without altering model parameters or requiring costly retraining.

%By equipping models with a mechanism for input validation and refinement at deployment, this framework introduces adaptability and resilience directly into the inference process. This approach provides a scalable, resource-efficient solution to enhance model performance across diverse real-world inputs that may not strictly deviate from the training distribution but nonetheless challenge the model’s processing abilities. Ultimately, this research offers a pathway to developing more dependable AI systems, capable of robustly handling the complexities of real-world environments where input variability is inevitable.







%Deep learning-based code language models (CLMs) are increasingly adopted in software engineering tasks such as vulnerability detection, defect prediction, and code generation~\cite{tian2023fly, naturalattack, Zhang2023Challenging, lu2021codexglue,svyatkovskiy2020intellicode,Devlin2019BERT,Sanh2020DistilBERT,guo2021graphcodebert,Chen2021Evaluating}. While these models demonstrate impressive capabilities, they frequently experience performance degradation in real-world applications due to factors like data shifts, diverse programming contexts, and rapidly evolving codebases.~\cite{yang2024robustness, yefet2020adversarial,hu2023codes,li2021estimating,van2020tailoring,peng2018t}. This degradation can occur not only when inputs deviate from the training distribution but also with challenging test data that may be within the expected distribution. Such inconsistencies present a major challenge, as CLMs are expected to function reliably across varied code patterns and domains but may struggle to generate accurate predictions in these scenarios. Addressing these performance issues is essential to ensure the reliability and adaptability of CLMs in production settings.

%The conventional solution to these challenges is to retrain or replace the underlying CLMs, incorporating new data and models to improve performance~\cite{yuDataAugmentationProgram2022,xiao2021selfchecking,xiao2022repairing}. However, this approach is resource-intensive, requiring substantial investments in data labeling, computational power, and redeployment. Frequent retraining can also reduce model generalization and introduce compatibility issues, particularly in agile development environments where codebases are updated frequently. Moreover, new architectures or datasets may be needed to address limitations fully, resulting in high costs for redevelopment and deployment. Given these complexities, regular model retraining is often impractical for organizations that depend on continuous, adaptive code analysis.

%To address these limitations, my research proposes \textit{input refinement} as a cost-effective alternative that enhances model performance by adapting inputs rather than modifying the model itself. Input refinement begins with \textbf{input validation}, where inputs are analyzed to determine whether they lie within the model’s handling scope. This step is crucial for identifying out-of-scope inputs that are likely to produce incorrect outputs. Once out-of-scope inputs are identified, the process moves to \textbf{input refinement}, which adapts these inputs to make them more tractable for the model. Refinement techniques include semantic-preserving code transformations, which restructure inputs without altering their functionality, and sampling techniques like top-k sampling and temperature scaling, which adjust model output probabilities for more reliable predictions. Input refinement is particularly advantageous in scenarios with rapidly changing codebases, as it enables CLMs to handle a broader range of inputs effectively without requiring retraining. This approach is also more resource-efficient than continuous retraining, making it highly suitable for large-scale deployments. However, implementing input refinement in code models poses unique challenges due to the discrete, structured nature of code data.

%Despite its advantages, input refinement introduces significant challenges in both validating and refining inputs. 
%The process of validating and refining inputs for CLMs encompasses distinct challenges. In terms of input validation, it is difficult to develop a reliable metric that accurately identifies out-of-scope inputs for code data. Traditional uncertainty metrics~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline,gal2016dropout, alon2019code2vec,xiao2019quantifying,vasudevan2019towards,corbiere2019addressing, monarch2021human, steinhardt2016unsupervised,shannon1948mathematical}, designed primarily for continuous data like images, often fail to reliably capture the nuances of code inputs. Unlike continuous data, code is highly structured and discrete, with complex features such as control flows and data dependencies that can cause abrupt changes in model output. These characteristics make traditional uncertainty metrics unreliable, as they struggle to predict whether a model will produce correct or incorrect outputs for specific code inputs.

%Once out-of-scope inputs are identified, input refinement becomes challenging due to the need to balance semantic-preserving transformations and sampling techniques while maintaining code functionality. Code refinement requires navigating a vast search space of possible transformations and sampling strategies to ensure that the adapted input is compatible with the model while preserving its original intent. Additionally, transformations and sampling must be tuned carefully to avoid semantic drift, where modifications alter the input’s functionality or meaning. This balancing act between compatibility and functionality is complex, as refined inputs must align with the model’s handling capabilities without compromising the input’s original purpose. 

%\textbf{This research hypothesizes that an automated framework combining input validation and refinement, adapted to the structured nature of code, can enhance CLM accuracy and reliability at inference time without requiring retraining}. To achieve this, my research focuses on three main goals: (1) developing effective validation metrics for various CLM models and tasks, such as classification and generation, specifically suited to the discrete, structured characteristics of code; (2) creating effective refinement techniques that utilize semantic-preserving transformations and optimized sampling strategies; and (3) providing a scalable, resource-efficient framework to improve CLM performance during deployment.

% \begin{figure}[!htbp]
% \centering
% \includegraphics[width=\columnwidth]{tex/images/ds.jpeg}
% \caption{Overview of Proposed Framework} 
% % %\textcolor{red}{TODO: DRAW A FLOW DIAGRAM TO SHOW MANIFESTATION }
% \label{fig:Overview}
% \end{figure}

%\textbf{This research hypothesizes that an automated framework combining input validation and input refinement, specifically adapted for the structured nature of code, can improve CLMs' accuracy and reliability at inference time. By accurately identifying out-of-scope inputs and refining them through tailored semantic-preserving transformations and sampling techniques, this framework can enhance model performance without necessitating retraining.} Building on this hypothesis, my research aims to achieve three main goals: first, to develop a set of reliable validation metrics tailored for various CLM models and tasks, such as classification and generation, specifically addressing the discrete, structured nature of code data; second, to create effective input refinement techniques that combine semantic-preserving transformations and optimized sampling strategies; and third, to provide a scalable, resource-efficient framework that enhances CLM performance at inference time without the need for frequent retraining.





%To address these challenges, an alternate cost-effective solution is to refine the program inputs to CLMs without altering the original model. This strategy of input adaptation is particularly beneficial in scenarios like agile development, where the code base rapidly changes, as it maintains the model’s applicability without the need for frequent retraining. Additionally, exploring different input adaptation techniques is more resource-efficient than the continuous retraining of a model to discover the optimal approach.

%My research addresses these challenges by introducing an innovative, automated input validation and refinement framework specifically tailored for CLMs. This approach provides a practical, cost-effective alternative to traditional retraining by ensuring that models can handle out-of-scope inputs with greater accuracy and reliability. By improving CLM performance at inference time, my research offers a scalable solution for organizations and developers who rely on these models to maintain software quality in evolving environments. This work benefits industries and teams engaged in rapid software development, particularly those using CLMs in agile settings where codebases frequently change. Additionally, this approach enhances the practicality of deploying CLMs in production environments by reducing dependency on continuous retraining, thus minimizing resource expenditure and operational complexity.

%The process of validating and refining inputs for code models presents distinct challenges. First, in terms of input validation, creating a reliable metric to predict whether a model will generate a correct or incorrect output for a given input is difficult. Existing uncertainty metrics~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline,gal2016dropout, alon2019code2vec,xiao2019quantifying,vasudevan2019towards,corbiere2019addressing, monarch2021human, steinhardt2016unsupervised,shannon1948mathematical} , primarily designed for continuous data like images, often fail to identify out-of-scope inputs when applied to code data. Image data is typically continuous and dense, allowing for smooth gradients that make uncertainty metrics effective. By contrast, code data is discrete, highly structured, and contains complex patterns like control flows and data dependencies, leading to abrupt shifts in model outputs that make uncertainty predictions less stable and reliable. Additionally, code data undergoes unique transformations related to syntax and semantics, unlike the noise or pixel transformations typical of image data, further complicating validation.

%Once out-of-scope inputs are identified, refining them to align with the model’s handling scope introduces additional complexity. Effective refinement involves not only semantic-preserving code transformations but also sampling techniques such as top-k sampling and temperature scaling, particularly useful in generative tasks. These sampling techniques adjust the model's output probabilities to improve the likelihood of generating accurate predictions. However, applying these techniques in code models is challenging, as they require careful tuning to avoid semantic drift, where the refined input deviates from the original functionality. The combination of transformations and sampling thus creates a large search space of possible refinements. This requires balancing modifications that improve model compatibility with constraints that preserve the code’s original meaning, ensuring that each refinement remains both functional and aligned with the model’s processing capabilities.









%1.https://arxiv.org/pdf/2002.05442.pdf
%\textcolor{red}{Introduction to DL code models}
%\textcolor{red}{1.List down the challenges. 2. Explain the scope. }

% \textcolor{red}{
% para 1: Introduce DL models performance degradation due to code data shifts \newline
% para 2: existing work to improve the DL model using retraining and replacing strategy. Such strategies have limitations.  However, solutions are promising to work on the input space of the DL model while not changing the model. \newline 
% para 3: Challenges in working in the input space: \newline
% a. inherent code nature, \newline
% b. code sample expansions \newline
% c. Maintaining code properties \newline
% d. large search spaces \newline
% We can address these challenges in a two-step process. 1) input validation, and then 2) input improvement. \newline
% para 4: CodeImprove's input validation phase. \textit{In this paper, we first focus on input validation} \newline
% para 5: Code Improve's input improvement phase. \textit{Once an input is validated, the next step is to focus on input improvement for out-of-scope inputs.}  \newline
% para 6: highlight findings and contributions
% }

% \textcolor{blue}{1. changing the model is expensive. 
%  2. so we need to change the input. 
%  3. to change an input, there need to be done two things. 1) checking if the input is in the scope 2) adapting out-of-scope inputs to in-scope inputs.
%  4. challenges in each step.
%  5. what we propose. 
% Also, handling code data by the existing DL models is hindered due to a limited understanding of the inherent code data’s nature.}

% \begin{wrapfigure}{r}{0.6\textwidth}
% %\hspace{-2.8cm}
% \begin{minipage}[h]{\linewidth}
% \begin{minted}
% [highlightlines={}]
% {C}
% map<string, int> m{{"a",1},{"b",2},{"c",3}};
% //map iteration before C++14
% for(iter = m.begin(); iter != m.end(); iter++){
% //...
% }
% //map iteration after C++14
% for(auto:iter m){
% //...
% } 
% \end{minted}
% \end{minipage}
% \caption{Listing 1: Code Snippet for \textit{for} Loops}
% \label{fig:forloops}
%\end{wrapfigure}


%\textcolor{red}{split to two tasks }

%%%OOPSLA   Recently, Deep Learning (DL) has been widely used to solve software engineering tasks such as vulnerability detection~\cite{tian2023fly,lu2021codexglue}, defect prediction~\cite{tian2023fly, naturalattack}, and clone detection~\cite{tian2023fly, naturalattack, Zhang2023Challenging, lu2021codexglue} by leveraging the deep code models (i.e., DL models that are trained on a large number of code snippets)~\cite{Devlin2019BERT,Sanh2020DistilBERT,guo2021graphcodebert,Chen2021Evaluating}. Although Deep learning (DL) models have demonstrated advantage on various code analysis tasks (e.g., defect detection)~\cite{schuster2021you, radford2019language, lu2021codexglue, raychev2016probabilistic, allamanis2013mining, svajlenko2014towards, wang2020detecting, wang2016automatically, nam2015heterogeneous, li2017software, zhou2019devign, li2018vuldeepecker, li2021sysevr}, no DL models are perfect, and all models face out-of-scope inputs that are beyond models' handling capability. One of the main obstacles to adopting DL models in industrial practice to automate program analysis is the lack of confidence and control over what DL models can handle. Because DL models approximate solutions for tasks by learning the data patterns in training datasets, and the training datasets may not be representative of all the possible data, the approximation may not work for some data. This is especially true for DL models that take program data as inputs because the deep code models often suffer performance degradation after deployment due to code data shifts. The code data shifts occur for many reasons, and one main reason is software evolution (e.g., evolving API and library versions, evolving programmers). The effectiveness of the code models relies on the assumption that the test data is identically distributed with the training dataset~\cite{li2021estimating,hu2023codes}. However, code data shifts can violate this assumption by causing distribution shifts in program data. For example, as shown in Figure~\ref{fig:forloops}, if a code model is trained with C/C++ code before C++ 14, the model would have difficulty in making correct decisions on test programs that consist of C++14 syntax such as \textit{auto} in a \textit{for} loop. 

%\textcolor{red}{code analysis tools, differentiate AI based and symbolic reasoning}

%%%\textcolor{red}{adapting input is also very common in non AI software. You should say it's common for people to adapt input when it's difficult to change tools. And for AI-based tools, especially so. But no technique is ever developed for it. Our technique is proposed to address this emerging need}



%%%% older||| Recently, code analysis tools has been widely used to solve software engineering tasks such as vulnerability detection~\cite{van2020tailoring,peng2018t,tian2023fly,lu2021codexglue}, defect prediction~\cite{tian2023fly, naturalattack}, and clone detection~\cite{tian2023fly, naturalattack, Zhang2023Challenging, lu2021codexglue}. Despite their widespread adoption, these tools often encounter performance issues, scalability issues, and may generate false positive/negative results. Consequently, such shortcomings can slowdown the software development process and damage software quality. The process of improving these tools entails significant costs. For example, improving tool for specific dataset require adjustments to tool's algorithms, configuration resulting in additional development, testing, and maintaining costs. Moreover, such a modified tool may fail to perform better on different dataset. 

%%%%End older
% Code analysis tools has been widely utilized to solve software engineering tasks such as vulnerability detection, defect prediction, and clone detection~\cite{van2020tailoring,peng2018t, tian2023fly, naturalattack, Zhang2023Challenging, lu2021codexglue}. These tools, spanning from non-AI to AI-based tools suffer common issues such as performance limitations, scalability constraints, and the potential for generating false results. However, enhancing these tools involve two approaches: improving the tools themselves, or adapting the inputs. Improving the non-AI based tools can be costly, involving modifications from one version to another by integrating additional heuristics or rules. However, such a modified tool may fail to perform better on different dataset. As a result, a common practice to improve non-AI tools is to adapt the inputs when modifying the tools become challenging~\cite{van2020tailoring,peng2018t}. 

% In the field of software engineering, code analysis tools play a crucial role in addressing critical tasks such as vulnerability detection, defect prediction, and clone detection~\cite{tian2023fly, naturalattack, Zhang2023Challenging, lu2021codexglue}. 
% %\textcolor{blue}{DL models approximate solutions based on learned patterns in training datasets, which may not be representative of all the possible data. Code models often suffer performance degradation due to various reasons~\cite{yang2024robustness, yefet2020adversarial,hu2023codes,li2021estimating}.}
% \textcolor{blue}{Code analysis tools often suffer performance degradation due to various reasons~\cite{yang2024robustness, yefet2020adversarial,hu2023codes,li2021estimating,van2020tailoring,peng2018t}.}
% %\textcolor{blue}{However, existing approaches~\cite{yuDataAugmentationProgram2022,xiao2021selfchecking,xiao2022repairing} on improving the accuracy of these tools face challenges related to performance limitations, scalability issues, and the potential for erroneous results.}
% %These tools often face challenges related to performance limitations, scalability issues, and the potential for erroneous results. 
% Traditionally, the primary approach to overcoming these obstacles has been to refine the tools to handle various challenging cases, which might involve updating their versions or incorporating new heuristics. However, this process can be complex and resource-intensive. Recently, researchers have identified an alternative strategy: \textit{adapting the inputs to the tools}~\cite{van2020tailoring,peng2018t}. This approach has gained traction, especially in scenarios where refining the tools becomes impractical. This adaptation of inputs serves as an effective strategy to circumvent the limitations of tool modification, ensuring that code analysis tools remain effective, even for cases they were initially unable to address before the adaptation of inputs.


% %%%To address these challenges, improvements to code analysis tools can be followed through two approaches: 1) enhancing the tools themselves, or 2) refining the input data they rely on.

% %%%There has been a growing interest in leveraging symbolic reasoning to improve the code analysis tools~\cite{van2020tailoring,peng2018t}.  By representing program elements (i.e., variables, expressions, control flow etc.) symbolically, these tools can understand program semantics at a granular level to identify complex patterns. %This facilitates path-sensitive analysis, automated theorem proving, and constraint-based analysis, allowing for the detection of subtle bugs, verification of correctness properties, and inference of program specifications. 
% %%%This enables code analysis tools to refine and abstract program representations, managing complexity and scaling to larger codebases effectively. By leveraging symbolic reasoning techniques, code analysis tools can provide comprehensive insights into code quality, reliability, and security, thereby improving software development processes and outcomes.

 
% % With the emergent of deep code models (i.e., DL models that are trained on a large number of code snippets)~\cite{Devlin2019BERT,Sanh2020DistilBERT,guo2021graphcodebert,Chen2021Evaluating}, designing AI-driven tools has been extensive in the software production to solve various code analysis tasks (e.g., defect detection)~\cite{schuster2021you, radford2019language, lu2021codexglue, raychev2016probabilistic, allamanis2013mining, svajlenko2014towards, wang2020detecting, wang2016automatically, nam2015heterogeneous, li2017software, zhou2019devign, li2018vuldeepecker, li2021sysevr}. Despite the popularity, these AI-driven tools suffer severe performance issues (i.e., mispredictions) like DL models
% % in other domains (e.g., image processing~\cite{xie2019feature}). However, improving the performance and quality of AI-driven tools can be more costly. First, the fundamental challenge is that the absence of perfect AI models (i.e., achieving 100\% accuracy). Second, the common practice to improve AI-driven code analysis tools is to retrain and replace the deep learning model~\cite{yuDataAugmentationProgram2022, xiao2021selfchecking,xiao2022repairing}. Retraining adds labeling and computing expenses and can lead to loosing model generalization capabilities, compatibility issues and version control challenges. Replacing necessitating new architecture and data, which raises rebuild and redeployment costs. Third, the diversity of software projects mean that AI tool work well for one dataset may not necessarily generalize to other dataset.  

% While existing input adaptation efforts have primarily focused on traditional symbolic reasoning-based tools~\cite{van2020tailoring,peng2018t}, there is a growing need for developing input adaptation strategies for (deep) learning-based code analysis tools\textcolor{blue}{/models}~\cite{Devlin2019BERT,Sanh2020DistilBERT,guo2021graphcodebert,Chen2021Evaluating}. This need arises from the challenges and high costs associated with improving these models to address their limitations. Commonly, improving these tools involves retraining and replacing the underlying deep learning models~\cite{yuDataAugmentationProgram2022,xiao2021selfchecking,xiao2022repairing}. Such processes not only lead to increased labeling and computing efforts but also risk reducing the models' generalization capabilities, alongside potential compatibility and version control issues. Additionally, model replacement may need the creation of new architectures and datasets, which incurs substantial costs related to rebuilding and redeploying the systems.

% To address the challenges, an alternate cost-effective solution is to adapt the program inputs to learning-based tools without altering the original tool. This strategy of input adaptation is particularly beneficial in scenarios like agile development, where the code base rapidly changes, as it maintains the model's applicability without the need for frequent retraining. Additionally, exploring different input adaptation techniques is more resource-efficient than the continuous retraining of a model to discover the optimal approach. 

% The process of adapting an input to a code model generally involves two key steps: (1) \textit{input validation} that aims at identifying out-of-scope inputs that fall outside the model's capacity (i.e., inputs prone to being mishandled) , and (2) \textit{input adaptation}, where the out-of-scope inputs are converted by semantic-preserving transformations to become in-scope inputs that are within the model's handling capabilities (i.e., inputs that the model is likely to process correctly).




% %Retraining models adds labeling and computing expenses and can lead to compatibility issues and version control challenges. Furthermore, it may compromise the model's generalization capability on old data and tasks, necessitating new architecture and data, which raises rebuild and redeployment costs.


% %%%However, updating a symbolic reasoning tool with heuristics or rules typically incurs lower costs compared to revising an AI-driven tool, which requires expensive data acquisition and retraining processes. %Additionally, AI tools may need post-retraining alignment to ensure their performance aligns with expectations, further adding to the overall expense and time investment. 
% %%%Symbolic reasoning tools offer a more cost-effective solution for iterative adjustments, whereas AI-driven tools present higher costs and complexities, particularly for significant modifications.

% %%% ISSTA The common practice to improve AI-driven code analysis tools is to retrain and replace the deep learning model~\cite{yuDataAugmentationProgram2022, xiao2021selfchecking,xiao2022repairing}, but such approaches often fail for evolving software projects. First, retraining adds labeling and computing expenses. Without sufficient new labeled data, retraining can suffer data scarcity. Furthermore, retraining a model can lead to compatibility issues and version control challenges, as it essentially generates a new version that must be updated across the software supply chain. Last, retraining may result in the model losing its generalization capability on old data and tasks. Therefore, retraining can incur a significant number of challenges. Replacing the model requires new architecture and data, raising rebuild and redeployment costs. However, a replaced model cannot improve model performance on-the-fly after the models have been deployed.


% %%%%ISSTA%%%%%% Recently, Deep Learning (DL) has been widely used to solve software engineering tasks such as vulnerability detection~\cite{tian2023fly,lu2021codexglue}, defect prediction~\cite{tian2023fly, naturalattack}, and clone detection~\cite{tian2023fly, naturalattack, Zhang2023Challenging, lu2021codexglue} by leveraging the deep code models (i.e., DL models that are trained on a large number of code snippets)~\cite{Devlin2019BERT,Sanh2020DistilBERT,guo2021graphcodebert,Chen2021Evaluating}.  Although deep code models have demonstrated advantage on various code analysis tasks (e.g., defect detection)~\cite{schuster2021you, radford2019language, lu2021codexglue, raychev2016probabilistic, allamanis2013mining, svajlenko2014towards, wang2020detecting, wang2016automatically, nam2015heterogeneous, li2017software, zhou2019devign, li2018vuldeepecker, li2021sysevr}, no DL models are perfect, and all models face out-of-scope inputs that are beyond models' handling capability.

% %%%%ISSTA%%%%One significant obstacle to widespread adoption of deep code models in industrial practice is the lack of confidence and control over the model's handling capabilities.  DL models approximate solutions based on learned patterns in training datasets, which may not be representative of all the possible data. Code models often suffer performance degradation due to code data shifts. These shifts are influenced by many factors including software evolution (e.g., evolving API and library versions, and evolving programmers). The effectiveness of the code models relies on the assumption that test data is identically distributed with the training dataset~\cite{li2021estimating,hu2023codes} can be violated by causing distribution shifts in program data. 


% %%%%ISSTA This work explores a more affordable solution, adapting the program inputs to the code models without altering the original model. In particular, input adaptation can keep the model relevant without frequent retraining in situations such as agile development, where code data evolve fast. Furthermore, experimenting with various input adaptation methods is lightweight and less costly than repeatedly retraining a model to find the best solution. The process of adapting an input to the code model can be achieved by two steps: 1) input validation that focuses on identifying out-of-scope input (i.e., mispredicted input) programs that surpass the model's handling capability, and 2) input adaptation, which transforms out-of-scope inputs through semantic-preserving program transformations to become in-scope inputs (i.e., correctly-predicted inputs).












% % OOPSLA \begin{figure}[!htbp]
% % \begin{minted}
% % [highlightlines={}]
% % {C}
% % map<string, int> m{{"a",1},{"b",2},{"c",3}};
% % //map iteration before C++14
% % for(iter = m.begin(); iter != m.end(); iter++){
% % //...
% % }
% % //map iteration after C++14
% % for(auto:iter m){
% % //...
% % } 
% % \end{minted}
% % \caption{ Listing 1: Code Snippet for \textit{for} Loops }
% % \label{fig:forloops}
% % \end{figure}

% %Such code data shifts are known to be beyond the DL model's handling capability. \jiajun{Text-based DL models may not be greatly affected by the syntax of the source code.}
% %Such instances can make a successful DL application to encounter inputs beyond its DL model's handling capabilities. 

% % \begin{figure}
% %      \centering
% %      \begin{subfigure}[b]{0.6\linewidth}
% %      \begin{minted}{C}
% % for(iter = m.begin(); iter != m.end(); iter++){
% % //
% % }
% % \end{minted}
% % \caption{Defining \textit{for-loops} with before C++14}
% % \label{fig:y equals x}
% % \end{subfigure}
% % \hfill
% % \begin{subfigure}[b]{0.35\linewidth}
% % \begin{minted}{C}
% % for(auto:iter m){
% % //
% % }      
% % \end{minted}
% % \caption{Defining \textit{for-loops} with C++14}
% % \label{fig:three sin x}
% % \end{subfigure}
% % \caption{Difference in the Definition of for-loops before and after C++14}
% % \label{fig:forloops}
% % \end{figure}


% %edited: However, the DL models have limited performance after DL application's deployment due to two reasons. First, test programs may evolve (i.e, evolving API versions, evolving programmers etc.) making the syntactical difference between trained programs and programs under test. For example, if a DL model is trained using a C++11 dataset, the model is better at making correct predictions on \textit{for-loops}. However, a test program that consist of C++14 syntax such as \textit{auto} instead of \textit{for} loop, the model has a tendency to mispredict the test program. Such instances can make a successful DL application to encounter inputs beyond its DL model's handling capabilities. Second, different DL models use different intermediate representations of a program (e.g., AST or CFG) to process programs, different models put higher weights on specific code syntax for predicting the output. For example, ASTs cannot capture data dependencies on the source code. As a result, the prediction of a DL model can be incorrect with unfamiliar code syntax. 

% % \textcolor{red}{ Handling input programs by the DL model can be challenging. Reasons: due to the inherent nature of the code data, lack of understanding of the code properties, and sample code expansions, improving the program still may not know if the model can identify semantic equivalency. lack of understanding about the inherent nature of code data, lack of domain-specific knowledge of SE tasks}

% %----Leveraging code models in software engineering can be challenging. First, \textit{Inherent code nature: } DL models use code token sequences as inputs that may hardly capture all syntactic and semantic information in the code data due to features such as control flows, data dependencies, etc. Second, \textit{Maintaining code properties: } DL models are incapable of identifying the program property violation when the semantics or grammar of the source code is changed due to the limitation in capturing code information. Third, \textit {Code sample expansion: } When an unseen code data is fed into the DL model, the DL model can be sensitive to features such as specific code syntax and mis-predict programs with unfamiliar code syntax. Fourth, \textit{Limitations on SE tasks: } Although applying semantic preserving program transformation on the source code is ideal, development teams should understand and be in control of what type of SE tasks should apply semantic preserving program transformation. For example, semantic preserving program transformations can directly be applied to the source code on a vulnerability detection task, however, tasks such as function method name prediction may change the identifier names that can lead to mis-predictions by the DL model. Therefore, solutions that improve the DL model’s handling of input programs are promising in terms of validating code inputs that are likely to be in/out-of scope and improving such inputs through semantic preserving program transformations. 

% %\textcolor{red}{Challenges in using code models for SE tasks: 1. Understanding code structure, syntax, programming language. Accurately understand the semantics of code written in different programming languages (PLs). -pre-trained models [9], [12] typically use the code token sequences directly as inputs to the pre-trained model. Doing so, however, would merely focus on the lexical information of code while ignoring its inherent semantic information, e.g., the dependency relation between variables. In worse cases, the lexical information could also be inaccurate since the identifiers could differ significantly in different programs, even if they implement the identical functionality 2. source code has to strictly stick to complex grammar and semantics constraints, i.e., the adversarial example generated from an original input should have no grammar errors and preserve the original semantics.}
% %\textcolor{red}{Existing work and challenges }
% %\todo{need to elaborate on the downside of retraining in this paragraph}

% %\textcolor{red}{retrain and replace. use plural}
% %%%%ISSTA%%%%The common practice to address this issue is to retrain and replace the deep learning model~\cite{yuDataAugmentationProgram2022, xiao2021selfchecking,xiao2022repairing}, but such approaches often fail for evolving software projects. First, retraining adds labeling and computing expenses. Without sufficient new labeled data, retraining can suffer data scarcity. Furthermore, retraining a model can lead to compatibility issues and version control challenges, as it essentially generates a new version that must be updated across the software supply chain. Last, retraining may result in the model losing its generalization capability on old data and tasks. Therefore, retraining can incur a significant number of challenges. Replacing the model requires new architecture and data, raising rebuild and redeployment costs. However, a replaced model cannot improve model performance on-the-fly after the models have been deployed.
% %The common practice is to update the model with more data or substitute it for a more generalized one. 
% %Although replacing or retraining is non-trivial, it is not preferred after the DL application's deployment.



% %%%OOPSLA First, replacing the model requires new architecture and data, raising rebuild and redeployment costs. Retraining also adds labeling and computing expenses. Without sufficient new labeled data, retraining can suffer data scarcity. Furthermore, retraining a model can lead to compatibility issues and version control challenges, as it essentially generates a new version that must be updated across the software supply chain.Last, retraining may result in the model losing its generalization capability on old data and tasks. Therefore, retraining can incur a significant number of challenges.  

% %replacing the DL model with a new one requires the construction of a new model architecture and the availability of training data that can cost the rebuilding and redeploying of the application. Also, retraining may incur additional costs in terms of labeling and computing resources.
% %Therefore, an alternative solution to alleviate the expensiveness of the existing techniques is to adapt the program inputs to the code models while preserving the original model. 

% %\todo{elaborate on the advantage of the input adaptation }





% %%%OOPSLAThis work explores a more affordable solution, adapting the program inputs to the code models without altering the original model. In particular, input adaptation can keep the model relevant without frequent retraining in situations such as agile development, where code data evolve fast. 
% %as inputs can be adapted in real-time saving the computation resources and time needed to retrain a model. 
% %%%oopsla Furthermore, experimenting with various input adaptation methods is lightweight and less costly than repeatedly retraining a model to find the best solution. 
% %Unlike retraining, input adaptation makes the model more stable on all the tasks.  An adapted input can be easily interpreted by human making it more transparent for further adjustments. When data is scarce, input adaptation is an optimal solution to improve the performance of the model. 
% %%% oopsla \textcolor{red}{which inputs to validate, how do we adapt these inputs}The process of adapting an input to the code model is shown in Figure~\ref{fig:adaptation}. This can be achieved by two steps: 1) input validation that focuses on identifying the handling scope of a model and determining whether an input is an out-of-scope input %\jiajun{``out-of-scope'' should be carefully defined as it is a critical concept in this paper. \textcolor{red}{Our definition for out-of-scope data is the data samples that is mispredicted by the DL model}}
% %oopsla programs that are beyond a model’s handling capability, and 2) input adaptation that adapts out-of-scope input programs to become in-scope inputs. 

% % \begin{wrapfigure}{h}{0.5\textwidth}
% % \includegraphics[width=0.5\textwidth]{tex/images/adaptation.pdf}
% % \caption{Overview of Input Adaptation for Out-of-Scope Inputs}
% % \label{fig:adaptation}
% % \end{wrapfigure}

% % \begin{figure}[!htbp]
% % \centering
% % \includegraphics[width=0.8\columnwidth]{tex/images/adaptation.pdf}
% % \caption{Overview of Input Adaptation for Out-of-Scope Inputs}
% % \label{fig:adaptation}
% % \end{figure}

% %%% OOPSLA \textcolor{red}{lacks background. in terms of performance on what task (144), make sense on add each sentence.} Input validation and input adaptation for code data are challenging. Input validation is challenging because existing approaches such as uncertainty metrics~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline, li2021estimating, alon2019code2vec, vasudevan2019towards,corbiere2019addressing} optimized for image data, do not perform well on code data. Image data are usually continuous and dense with fewer semantic nuances, allowing for smooth gradients that make the uncertainty metrics more reliable. In contrast, code data contains discrete, structured information with abstraction, such as control flows and data dependencies, which can result in abrupt changes in the model's output and make the uncertainty less predictable.
% %Input validation is challenging because traditional input validation techniques~\cite{}, designed for image data, struggle when applied to code data due to the inherent differences between the two types of data. Image data is represented as raw pixels, while code data contains syntactic and semantic information, including control flows and data dependencies. When the input is fed into a DL model, image data can be processed as numerical pixels, but code data needs to be embedded into a numerical vector for the DL model to process. Unfortunately, numerical vectors cannot comprehend all of the syntactic and semantic information present in code data.
% %Also, there are also differences in the way image data and code data shift. Image data undergoes noise and corruption like matrix transformations, while code data undergoes syntax changes. Furthermore, image data can have different subjects, whereas code data has various functions. 
% %%% OOPSLA Moreover, the unique challenges of code data exacerbate the limitations of existing validation techniques. While image data mainly undergoes noise, corruption, or compression as matrix transformations, code data faces changes in syntax and programming paradigms. Unlike images, which vary based on objects, code data differs significantly in its functional components. These disparities make standard uncertainty metrics less applicable to code data.
% % Furthermore, the distinct ways in which each data type shifts make the existing approaches perform even worse on code data. Image data mainly experiences noise, corruption or compression in the form of matrix transformations. In contrast, code data is susceptible to changes in syntax and new programming paradigms. Additionally, while image data may vary by objects in the images, code data can differ vastly in its functional components. These differences further complicate the application of standard uncertainty metrics to code data.

% The process of validating and adapting inputs for code models encompasses distinct challenges. First, in terms of input validation,~\textcolor{blue}{it is} difficult to create a metric that accurately predicts the likelihood of a model generating a correct or incorrect output for a given input. The machine learning community has developed some methodologies known as uncertainty metrics~\cite{guo2017calibration,wang2020dissector,hendrycks2018baseline,gal2016dropout, alon2019code2vec,xiao2019quantifying,vasudevan2019towards,corbiere2019addressing, monarch2021human, steinhardt2016unsupervised,shannon1948mathematical} to estimate a model's level of uncertainty for a specific input.
% However, these metrics, which are primarily designed for image data, often fall short when applied to code data in identifying out-of-scope inputs. The root of this issue lies in the nature of image data, which is typically continuous and dense with fewer semantic nuances, allowing for smooth gradients that make the uncertainty metrics more reliable. In contrast, code data is inherently discrete, structured, and filled with abstractions such as control flows and data dependencies, which can result in abrupt changes in the model's output and make the uncertainty less predictable. Moreover, while image data mainly undergoes noise, corruption, or compression as matrix transformations, code data faces changes in syntax and programming paradigms. Our preliminary study (Section~\ref{study}) suggests that these essential distinctions pose a substantial challenge to the direct application of standard uncertainty metrics to code data.

% %Notably, our preliminary highlights several insights. First, on average the uncertainty metrics perform 0.42-0.59 in AUC score for two SE tasks (i.e., vulnerability detection and defect prediction) across three deep code models (i.e., CodeBERT, RoBERTa, and GraphCodeBERT) in detecting out-of-scope inputs. We find that the reason for low AUC scores is that the effectiveness of statistical analysis in these metrics. Mainly, these metrics undergo computations such as softmax, entropy etc. However, out-of-scope data may behave similar to that of in-scope data resulting an overlapping situation. Second, disparity in model's behavior on code data. Uncertainty metrics like vanilla show strong performance in RoBERTa model, however, this performance degrades in GraphCodeBERT model. Third, uncertainty metrics are not efficient in detecting out-of-scope data within the same distribution. 
% Second, once specific inputs have been identified for adaptation, transforming these inputs into the model's handling scope is also challenging. First, navigating the vast search spaces involved in code transformation is a complex task. Considering the numerous possible transformations that can be applied to code and the fact that many of these transformations can be applied repeatedly to produce various forms of code, the resulting array of potential variations creates a complex landscape for exploration. Moreover, any modifications made during the program transformation process must maintain the original program functionality and semantics of the code. Therefore, efforts to modify the code to enhance model compatibility must be carefully balanced to avoid unintentionally changing its fundamental meaning or functionality, thus posing a dilemma between improving model performance and preserving code semantics.  


% %%%%% Move to prel study Our initial findings reveal several key insights. Firstly, across three deep code models (CodeBERT, RoBERTa, and GraphCodeBERT), uncertainty metrics exhibit an average AUC score ranging from 0.42 to 0.59 for two software engineering tasks: vulnerability detection and defect prediction. These metrics, which include computations like softmax and entropy, struggle due to their inability to effectively differentiate between in-scope and out-of-scope inputs, leading to overlapping behavior. Secondly, there's a notable discrepancy in model performance across different uncertainty metrics. For instance, while vanilla uncertainty metrics demonstrate strong performance in the RoBERTa model, their effectiveness diminishes when applied to the GraphCodeBERT model. Lastly, uncertainty metrics prove less effective at identifying out-of-scope data within the same distribution.
% %\todo{briefly describe the prelim results}

% %Adapting program inputs to code models can present certain challenges in both input validation and improvement. First, validating input is challenging because existing techniques~\cite{} focus on image data and fail with code data due to differences in the inherent characteristics. Image data are represented as raw pixels while code data contains syntactic and semantic information including control flows and data dependencies. Therefore, when input is fed into the DL model, image data will take numerical pixels as inputs, however, code data will need to be embedded into a numerical vector for DL model to process while numerical vector can hardly comprehend all syntactic and semantic information. Next, image data and code data have differences in shift patterns such as code syntax changes versus image matrix transformations, and different image subjects versus code functions. Lastly, existing uncertainty metrics for input validation are focused on image data, and applying such metrics fails to validate code data (\textcolor{red}{add section}). For example, existing softmax-based uncertainty measurement either uses the values to compute different statistical equations such as entropy or retrieve the maximum softmax value. However, out-of-scope code data may also have high confidence values that create an overlapping situation with in-scope data. 

% %When adapting an out-of-scope input, maintaining program properties and search spaces can be challenging. This is due to the fact that each code can be modified in numerous ways. Each code fragment can be represented in different forms, and the number of possible variable names is also numerous. This results in an intensive search space and modified code should not break the program properties (i.e., preserve the semantics of the original code). However, it is important to note that certain SE engineering tasks have constraints on program modification that can limit the possibilities for modifying a program. For instance, it is crucial to avoid replacing variable names with random names as they can indicate the overall functionality of the entire method.

% % OOPSLA \textcolor{blue}{which input to adapt-need certain metric to guide, 2)then how to adapt the transformation-need to guide our search stratey which can come from earlier metric. We use this metric to guide, while guiding this metric we have problem(large search space expensive, semantic code equivalency.)}
% % OOPSLA \textcolor{red}{out of context-> program properties to code semantics.   **Change first sentence to finding the correct adaptation steps while maintaining code properties/semantics.} The challenges of adapting out-of-scope inputs are maintaining program properties and managing extensive search spaces. This is because code can be modified in various ways, represented in different forms, and can have numerous variable names. This creates a large search space, and any modifications must preserve the original code's semantics. Additionally, software engineering tasks often come with constraints on how code can be modified. \textcolor{red}{For example, randomly changing variable names is inadvisable as they often indicate a method's overall functionality}. Therefore, maintaining code semantics while adapting it for better model handling is difficult. Changes aimed at making the code more digestible for models could inadvertently alter its original meaning or functionality, creating a trade-off between model performance and semantic integrity.

% %%%Much like in information theory, where the identification of patterns serves as a cornerstone, determining which inputs to adapt relies on recognizing recurring trends or anomalies within the data



% %through program transformations introduces challenges such as managing extensive search spaces and maintaining program semantics. This is because code can be modified in various ways, represented in different forms, and can have numerous variable names. This creates a large search space, and any modifications must preserve the original code’s semantic and functionality. Therefore, changes aimed at making the code more digestible for models could inadvertently alter its original meaning or functionality, creating a trade-off between model performance and semantic integrity. 

% %The process of adapting inputs to ensure they align with the intended scope introduces its own set of challenges. One major hurdle involves managing the vast search spaces inherent in code transformation. With code being highly modifiable and capable of taking on numerous forms, the potential variations create a complex landscape for exploration. Furthermore, any alterations made during this process must carefully maintain the original semantics and functionality of the code. This delicate balance is crucial because while changes may aim to enhance the code's interpretability for models, there's a risk of inadvertently altering its original meaning or functionality. Thus, a trade-off emerges between optimizing model performance and preserving the semantic integrity of the code.
 
% %Input adaptation for code models presents unique challenges that must be addressed to ensure effective optimization. One key challenge is determining the optimal transformation for a given input. Unlike traditional optimization problems where a single objective function guides the search, code data may require multiple transformations, each affecting different aspects of the code's behavior. Deciding which transformation to apply involves navigating a complex landscape of potential changes while preserving program semantics. Additionally, assessing the impact of each transformation on the model's performance requires careful consideration, as certain changes may inadvertently introduce errors or distort the intended functionality of the code. Moreover, maintaining the integrity of program semantics throughout the adaptation process is crucial to ensure that the transformed code remains interpretable and functional. Balancing the trade-offs between optimizing for model performance and preserving program semantics poses a significant challenge in input adaptation for code models. Addressing these challenges requires a nuanced understanding of both the code data and the underlying optimization algorithms, as well as the development of novel techniques that can effectively navigate the intricacies of code transformation while optimizing model performance.

% %The process of input adaptation for code data possesses two challenges: determining which inputs to adapt and defining the transformation strategy for adaptation. First, identifying inputs that need to adapt requires a specific metric to guide the decision-making process. Like information theory~\cite{ash2012information}, determining inputs to adapts relies on identifying recurring patterns of anomalies within the data.  Once an input is determined, adapting these inputs to become in-scope inputs through program transformations introduces challenges such as managing extensive search spaces and maintaining program semantics. This is because code can be modified in various ways, represented in different forms, and can have numerous variable names. This creates a large search space, and any modifications must preserve the original code’s semantic and functionality. Therefore, changes aimed at making the code more digestible for models could inadvertently alter its original meaning or functionality, creating a trade-off between model performance and semantic integrity. 

% %To address these challenges, an effective approach is to leverage the guiding metric obtained from earlier stages of the process. This guiding metric not only aids in determining which inputs require adaptation but also informs the search strategy, mitigating the issues associated with a vast search space and maintaining semantic code equivalency.



% To address these challenges, we
% propose CodeImprove (Figure~\ref{fig:Overview}), the first techniques for input validation and input adaptation of code inputs. 
% For input validation, we identified that existing uncertainty metrics misrepresent the model's handling capability on code inputs, leading to overconfident predictions for out-of-scope code inputs (Section~\ref{study}). We observe that the relevance of different aspects of the input, such as structural information or variable names, can shift dynamically across the model's layers. Traditional uncertainty metrics, which typically focus on the outputs of the final few layers, fail to capture this layer-by-layer processing.

% Based on such observation, we propose a Dropout-based Sub-Model Generation (DSMG) approach to find an optimal hidden state representation that accurately identifies in-scope versus out-of-scope inputs. By analyzing sub-models derived from the original DL model, CodeImprove can delve into how inputs are processed at each layer. DSMG allows CodeImprove to generate sub-models that provide deeper insights into the transformation of inputs through the network. CodeImprove utilizes the confidence levels of these sub-models' predictions as a new metric for assessing the validity of inputs, offering a more reliable measure that captures the complexities of code input processing in DL models.

% % First, we identify out-of-scope inputs from in-scope inputs. %To achieve this, we employ a guiding metric. 
% % Although the code inputs to the model can come the same distribution, the uncertainty metrics which performs better at detecting out-of-distribution data~\cite{li2021estimating}  fails to capture out-of-scope inputs from the same distribution with different syntax. Therefore, identifying the most influential code semantics that affects the model's prediction is important. As a solution, we designed our guiding metrics using sub models that represents different layers of the code model. These sub-models helps to perform a granular analysis of different features of the input data and provide a guiding metric by assessing the impact of each layer on decision making. Our objective is to assign higher validation scores to correctly-predicted inputs that indicates a strong prediction while lowering the validation scores for mispredicted inputs. We focus on adjusting validation scores to either increase or decrease  based on sub model's prediction. This objective monitors a code model's inputs and isolate those possibly beyond its model’s handling capability from
% % impacting the model’s decision-making.


% %Unlike the existing approach~\cite{wang2020dissector} that confuses the correct and incorrect predictions, we focus on adjusting validity scores to either increase or decrease them based on sub model's prediction. 

% %Therefore, we identify out-of-scope inputs from in-scope inputs using sub-models that represents different layers of the DL model. 



% Following input validation, CodeImprove employs Adaptation by Evolutionary Search (AES). 
%  \textcolor{blue}{We} develop a list of basic semantic preserving transformations and leverage DSMG’s validation score as a guiding metric  to combine these basic transformations into a
% composite transformation that effectively covert the input
% from being out-of-scope to in-scope. 
% % semantic-preserving program transformations to adapt the out-of-scope inputs to become in-scope inputs. CodeImprove consists of 15 transformation operators and utilizes the genetics algorithm which efficiently navigates a large search space to select the most suitable variants of transformed code for adaptation. This is achieved with the help of the guiding metric in the input validation phase that informs the search strategy, mitigating the issues associated with a vast search space and maintaining semantic code equivalency. 

% %maintains a pool of transformed code and applies the technique employed in the input validation phase to select the best variant of the transformed code for adaptation. 

% %Following input validation, CodeImprove employs semantic-preserving program transformations to adapt out-of-scope inputs into in-scope ones. With a set of 15 transformation operators and utilizing a genetic algorithm, CodeImprove efficiently navigates a large search space to select the most suitable variant of transformed code for adaptation. This dual-phase approach ensures both accurate validation through sub-models and a streamlined adaptation process by limiting the search space during transformation






% %This guiding metric not only aids in determining which inputs require adaptation but also informs the search strategy, mitigating the issues associated with a vast search space and maintaining semantic code equivalency. 




% %%% edited The process of input adaptation for code data involves two crucial aspects: determining which inputs to adapt and defining the transformation strategy for adaptation. Firstly, identifying the inputs in need of adaptation requires a specific metric to guide this decision-making process. However, this initial step introduces challenges, including the potential pitfalls of a large search space and the associated expenses. The expansive nature of the search space can make the process computationally expensive and time-consuming. Additionally, ensuring semantic code equivalency during adaptation poses a significant concern. The semantic preservation of code transformations is crucial to maintaining the integrity and functionality of the adapted inputs. To address these challenges, an effective approach is to leverage the guiding metric obtained from earlier stages of the process. This guiding metric not only aids in determining which inputs require adaptation but also informs the search strategy, mitigating the issues associated with a vast search space and maintaining semantic code equivalency. Overall, input adaptation for code data necessitates a careful balance between precision, efficiency, and semantic preservation.

% %Improving an out-of-scope input poses several challenges in terms of maintaining program properties, large search spaces, and limitations on SE tasks. First, a modified program should now break the program properties, i.e., preserve the semantics of the original code. Second, although the modified code has preserved program semantics, there can be a significantly large number of ways to modify a  program. Each code fragment can be represented in different forms, and the number of possible variable names is also numerous. Therefore, the search space for program modification is very large. Third, some SE engineering tasks impose constraints on program modification. For example, method name generation tasks should not replace variable names with random names because some variable names can be strong indicators of the functionality of the whole method. 

% %\textcolor{blue}{first sentence\todo {do we need this? }}%Improving the handling of input programs in DL models is a promising approach. Analyzing each layer's contribution to decision-making is helpful beyond relying only on logit or hidden state outputs to detect uncertainty. To achieve this, partial knowledge on the model's layers ca be studied. Once an input is validated, it can be enhanced using program transformations that preserve semantic meaning. A set of rules for semantic-preserving program transformations can be maintained to preserve program properties and reduce the search space for program adaptation.
% %\textcolor{red}{ 1. When we feed an input to a DL model, we can interpret this input based on the generated sub-models. We examine how this feeded input is interpreted in the sub-models and obtain the corresponding probability vectors. Sub-models represent the growing partial knowledge of the original DL model, so that these probability vectors explain how the DL model is interpreted with input layer by layer. The intuition here is that in-scope inputs should the DL model and the model should well recognize this input in the prediction process, instead of being confused by two or more possible guesses during the prediction. \\\newline
% %2. Dissector is used to confuse the predictions for correct and incorrect predictions. For a correct classification, it will confuse the decision with the second best prediction, while for incorrect prediction it will confuse with the expected correct prediction value. In my approach, we increase or decrease the confidence. For a correct prediction, we increase the confidence by adding the difference between the best and the second best prediction while decreasing the confidence by substracting the difference with the incorrect prediction. Intuition is that to assign a correct submodel prediction with higher confidence value while lowering the confidence of the incorrect submodel value. }



% %By validity degree, we expect that a within-input should be predicted in a way that the DL model used for predicting this input should have an increasing confidence when crossing the input layer through hidden layers, finally to the output layer. This is based on our observation that since a within-input fits in the DL model’s handling capability, the model should well recognize this input in its prediction process, instead of being confused by two or more possible guesses during the prediction.

% %%% OOPSLA \textcolor{red}{add challenges with insights.}To address these challenges, we propose CodeImprove, a technique to adapt out-of-scope programs to become in-scope programs. First, we identify the out-of-scope inputs from in-scope inputs. To achieve this, we generate sub-models that represent different layers of the DL model to identify the impact of each layer on decision-making. Our analogy is that an in-scope input of a DL model should make a strong prediction. Therefore, our goal is to assign a higher validity score for an in-scope input while lowering the validity score for an out-of-scope data. Unlike the existing approach~\cite{wang2020dissector} that confuses the correct and incorrect predictions, our goal is to increase or decrease the validity score for a prediction. We increase the validity score by adding the difference between the top two predictions (i.e., best vs. second-best) for a correct prediction while reducing this difference for an incorrect prediction. Once input is validated, the next step of CodeImprove is to adapt the out-of-scope inputs to become in-scope inputs. We utilize semantic preserving program transformations to transform the out-of-scope inputs. Our technique employs 15 semantic preserving transformation operators. Using the genetic algorithm, CodeImprove maintains a pool of transformed code and applies the technique employed in the input validation phase to select the best variant of the transformed code for adaptation. 
% %During the selection process, CodeImprove selects the   %The technique begins by attempting to validate out-of-scope inputs by analyzing the impact of each layer on decision-making. 


% %To achieve this, we implemented a layer-wise sub-model generation technique to represent varying levels of knowledge on the model.\jiajun{The efficiency of sub-model training should be evaluated. Is it more efficient than fine-tuning the original model? We use sub-models for the input validation phase to observe how each layer impacts decision-making.} These sub-models increase the validity score for in-scope inputs and decrease it for out-of-scope inputs. Our approach's effectiveness is determined by computing a validity score, using the best-vs-second-best algorithm, to increase or decrease the model's validity based on the prediction. After computing the validity scores, we combine the sub-models to determine the overall validity score by normalizing the importance scores with increasing weights. To adapt out-of-scope program inputs, CodeImprove uses genetics algorithm-based program transformations to convert them to become in-scope. Our technique employs 15 semantic preserving transformation operators. For each transformed code, we compute the validity scores. CodeImprove then selects the best variant of the transformed code based on the computed uncertainty for adaptation.

% %\textcolor{red}{We interpret code snippets fed into a deep learning (DL) model by examining sub-models, which represent partial knowledge of the original model. We acquire corresponding probability vectors to understand how the input is processed layer by layer. The idea is that an in-scope input should allow the DL model to make a confident prediction, rather than generating multiple competing guesses.}

% %\textcolor{red}{In contrast to existing methods that introduce confusion between correct and incorrect predictions, our approach aims to fine-tune confidence levels. For a correct prediction, we boost confidence by adding the difference between the top and second-best guesses. For incorrect predictions, we lower confidence by subtracting this difference. The goal is to assign higher confidence values to correct sub-model predictions and lower confidence to incorrect ones.}

% %In this paper, we propose CodeImprove, to validate and improve inputs that represent out-of-scope inputs from normal input. First, this technique tries to validate out-of-scope inputs by investigating the effectiveness of each layer on the decision-making. This is achieved by layerwise sub-model training to represent the levels of knowledge on the model. Each sub-model should increase the confidence score for in-scope inputs while decreasing the confidence score for out-of-scope data. This is achieved by computing an importance score utilizing the best-vs-second-best algorithm on each input's decision to increase or decrease the models confidence based on the prediction. Once the importance scores are computed, next step is to combine the sub-models together to determine the overall uncertainty score by normalizing these importance scores with increasing weight weights. Once we compute the uncertainty scores to validate the out-of-scope program inputs, CodeImprove utilize genetics algorithm based program transformations to convert these inputs to become in-scope. We leverage 15 semantic preserving transformation operators. For each transformed code, we compute the uncertainty scores. Then, CodeImprove selects the best variant of the transformed code based on the computed uncertainty before validating the model performance. 



% %\textcolor{blue}{Explain codeImprove}In this paper, we propose CodeImprove, to validate and improve the inputs that represent out-of-scope inputs from normal inputs. First, this technique tries to validate inputs that are beyond the DL model's handling capability. This is achieved by computing uncertainty scores on each test input to differentiate an out-of-scope input from normal inputs. Next, for the selected portion of out-of-scope inputs, we apply semantic preserving program transformations. To accomplish this task, we leverage a genetic algorithm to identify the best variant of the transformed program that is more likely to become an in-scope input. Our technique uses 15 program transformation rules. 


% We evaluated our technique with pre-trained transformer-based language models on software engineering tasks such as vulnerability detection and defect prediction. Our experimental results report promising results and show CodeImprove can enhance 8.78\% of absolute accuracy, and 51.28\% of relative improvements in three code models on two code tasks. Notably, our validity score computation that validates out-of-scope inputs obtained promising results (AUC score of 0.924). 
% % In addition to our experimental results, this paper studies how existing uncertainty measures work for code models. Our findings include that layer-wise sub-model generation helps to better quantify uncertainty on test data to differentiate out-of-scope inputs from in-scope inputs compared to softmax-based uncertainty that relies on the model performance. 


% %improvements in \textcolor{red}{accuracy (2.01\%-6.25\%), precision (1.71\%-6.61\%), recall (1.41\%-11.39\%), F1-score (2.10\%-9.83\%), and inaccuracy dropdown percentage (6.32\%-16.77\%) on vulnerability detection and defect prediction tasks.} 


% %%% OOPSLA \textcolor{red}{don't add code model names.} We evaluated our technique with pre-trained language models such as \textcolor{red}{CodeBERT, RoBERTa, and BERT} on software engineering tasks such as vulnerability detection and defect prediction. Our experimental results reported promising results and showed improvements in \textcolor{red}{accuracy (2.01\%-6.25\%), precision (1.71\%-6.61\%), recall (1.41\%-11.39\%), F1-score (2.10\%-9.83\%), and inaccuracy dropdown percentage (6.32\%-16.77\%) on vulnerability detection and defect prediction tasks for CodeBERT, RoBERTa, and BERT models}. Notably, our validity score computation that validates out-of-scope inputs obtained promising results (AUC: 0.717-0.931). In addition to our experimental results, this paper studies how existing uncertainty measures work for code models. Our findings include that layer-wise sub-model generation helps to better quantify uncertainty on test data to differentiate out-of-scope inputs from in-scope inputs compared to softmax-based uncertainty that relies on the model performance. 

% We summarize our contributions in this paper below:
% \begin{itemize}[topsep=2pt]
%     \item \textbf{Novel Perspective}. We propose a novel perspective of differentiating out-of-scope from in-scope inputs, as well as adapting these out-of-scope inputs to become in-scope inputs. To the best of our knowledge, our novel perspective is the first attempt to adapt inference-time inputs for deep code models through program transformation. 
%     \item \textbf{Tool Implementation}. We implement CodeImprove following the novel perspective (1) by implementing a sub-model generation technique from the original code model for code data, (2) designing a validity score metric to distinguish out-of-scope inputs from in-scope inputs utilizing the generated sub models, and (3) designing a genetic algorithm based technique to adapt out-of-scope inputs to become in-scope inputs by applying program transformation.
%     \item \textbf{Comprehensive Evaluation}. We conducted an extensive study on three popular pre-trained models and two code-base tasks, demonstrating the effectiveness and efficiency of CodeImprove's input validation and input adaptation on test data. 
%     \item \textbf{Public Artifact}. We release all experimental data and source code at the project Github repository~\cite{Data} for future research, practical use, and experiment replication.   
% \end{itemize}


% %for inaccuracy drop-down percentages on each model (4.32 \%-18.09\%). 

% % We summarize our contributions in this paper below:
% % \begin{itemize}
% %     \item Proposed a sub-model generation technique for code data that leverages code models.
% %     \item Proposed a validity score metric to validate in-scope and out-of-scope inputs for each sub-model. 
% %     \item Proposed a technique to transform out-of-scope inputs to in-scope inputs by applying program transformations. 
% %     \item Evaluated our technique's effectiveness on different pre-trained code models for the vulnerability detection and defect prediction tasks.
% %     \item Evaluate the effectiveness of existing uncertainty measures to detect out-of-scope data from normal inputs for automated validation. 
% % \end{itemize}

% %%% OOPSLAThe remainder of this paper is organized as follows. Section ~\ref{background} introduces the DL background. Section ~\ref{study} conducts a preliminary study on the effectiveness of existing uncertainty metrics on code data.  Section ~\ref{design} presents CodeImprove's technique for adatping out-of-scope inputs that are beyond the DL model's handling capability. Section ~\ref{evaluation} describes the experimental setup and evaluation of CodeImprove's technique, and Section ~\ref{results} discusses the results and analysis of CodeImprove's performance. Finally, Sections ~\ref{threat},~\ref{applications}, ~\ref{related}, and ~\ref{conclusion} discusses threat analyses and CodeImprove's limitations, application of CodeImprove, related work in recent years, and the conclusion followed by the data availability section.


% % new one Solutions to address the challenges in input validation and improvement can be promising. For, input validation rather than detecting uncertainty using logit outputs or hidden state outputs, it can be impactful to observe how each layer affects the model's decision making. This can be achieved by generating layerwise sub-models of the original model to check how decision-making changes on each sub-model~\cite{}. Once an input is validated, in order to improve the inputs, we can transform programs by applying semantic preserving program transformations. Maintaining a set of sementic-preserving program transformation rules will guarantee the the program property preservation as well as reduce the program search space. 





% % drafted \textcolor{red}{different in data characteristics, why uncertainty fails in code data. } Characteristics: Image data directly works on numerical space (pixels) while code data needs to be tokenized first and then assigned unique IDs (This may miss capturing important syntax that can affect the decision-making in the model). Softmax-based uncertainty measurements can have a higher probability of incorrect data. Distribution shift patterns are different such as code syntactic change versus image matrix transformation. Next, adapting out-of-scope programs is also challenging due to large spaces. One program can have a number of variants without changing the program semantics. Therefore, the search space is very large. 

% %Therefore, developers should consider new techniques to improve the model performance while not changing the DL model. Our approach to solving this issue relies on the input space of the DL model while not making any changes to the DL model. This can be achieved by two steps: 1) Input validation, and 2) Input improvement. Input validation focuses on identifying out-of-scope that are beyond the DL model's handling capability while input improvement focuses on transforming such out-of-scope data into in-scope data. 

% % original paragraph In this paper, we first focus on input validation. \textcolor{blue}{--should go to upper paragraph--- However, the task of identifying out-of-scope inputs is quite challenging due to the lack of an oracle defining the boundary of a DL model's handling scope. The most effective approach to validate inputs is to utilize uncertainty which quantifies inputs for the model's handling capability. This approach involves feeding DL software only in-scope inputs that are within a model’s handling capability while delegating out-of-scope inputs for other means such as human annotation or rule-based systems. Initially, we utilized uncertainty measurement to identify out-of-scope inputs from normal inputs that are beyond the DL model's handling capability. However, our results show that uncertainty measurements do not perform well on code data for input validation. Existing techniques~\cite{} mainly focus on data shifts in computer vision~\cite{} and may not perform well in code data due to differences in data shift patterns such as code syntactic changes versus image matrix transformations, different image subjects versus code functions~cite{}}. Therefore, we design an importance score-based input validation technique (Section ...) which significantly performs better than the existing uncertainty metrics \textcolor{red}{(add a section to reference)}. 

% %  original draft Once an input is validated, the next step is to focus on input improvement for out-of-scope inputs. Although, existing works\cite{} focus on improving inputs for image data, applying such techniques on code data can be quite difficult. An improved code data has to be semantically equivalent to its original data while image data only need to limit the number of pixels to modify. Additionally, existing works focus on input improvement using distance-based approaches between training and test data. However, training data is finite and real-world, new inputs may contain unexpected variations. Therefore, our intuition is that the possibility of correctly predicting an input program may increase if the program has code syntax that the DL models are familiar. This can be achieved by performing semantic preserving program transformation on out-of-scope test inputs. The inherent characteristics of code data provide an opportunity to perform program transformations to convert an out-of-scope program to an in-scope program


% %Our input validation approach utilizes uncertainty measurement to identify out-of-scope inputs from normal inputs that are beyond the DL model's handling capability. 

% %Uncertainty in a DL system can present in two ways. First, aleatoric uncertainty which indicates the variability in the outcome of an experiment which is due to inherently random effect[]. Second, epistemic uncertainty which indicates to the uncertainty caused by a lack of knowledge of the DL model. Therefore, our input validation approach utilizes uncertainty measurement to identify out-of-scope inputs from normal inputs that are beyond the DL model's handling capability. 

% %\textcolor{red}{our approach vs input reflector. Input reflector needs training data, and transformed version of data for input validation. Dissector needs training data for sub model generation. Dissector does not need training data for validation process after the sub models are generated. Input reflector uses the base model and adds dense layers for both validation and improvement models they use. Therefore, reflector needs to retrain the model.  Our approach does not need to change the model or use a new model. }



% %\textcolor{blue}{Although the existing work focus on input improvement using distance based approaches between test and training data. However, training data is finite and real-world, new inputs may contain unexpected variations. Therefore,} our intuition is that the possibility of correctly predicting an input program may increase if the program is with code syntax that the DL models are familiar. This can be achieved by performing semantic preserving program transformation on out-of-scope test inputs. The inherent characteristics of code data provides an opportunity to perform program transformations to convert an out-of-scope program to an in-scope program. 

% %In addition to addressing the key problem, i.e., adapting out-of-scope program inputs to DL models, identifying when an input falls beyond the DL model's handling capability is important. However, the task of identifying such inputs is quite challenging due to the lack of oracle defining the boundary of a DL model's handling scope. However, when inputs falls beyond the handling capability, their predictions can be misleading or wrong causing an lower accuracy and these predictions are noticed during the model's prediction process. 

% %Therefore, instead of immediately procissing an input program by a DL model, we first identify whether the input program is an out-of-scope or an in-scope program within the model's handling capability

% %edited: Existing work focus on improving the robustness of the DL model by generating adversarial inputs[], data augmentation on training set to retrain the DL model with more programs [], or replacing the existing model with a new model []. Although the replacing or retraining is non-trivial, it is not preferred after the DL application's deployment. Although adversarial program generation is promising, the DL application can be prone to artificial adversarial attacks. Therefore, the performance of the DL application can be degraded. Data augmentation on training set require more training efforts as well as a availability of the training dataset. Replacing the DL model with a new one require reconstruction of a new model architecture with different generalizability.  

% %\textcolor{red}{Our intuition on transforming the test data}
% % edited: In this paper, we consider a program transformation approach on test data. Our intuition is that the possibility of correctly predicting an input program may increase if the program is with code syntax that the DL models are familiar. After the application is deployed, we can make the application recognize the inputs that are beyond the DL model's handling capability and transform such inputs in a way that model can handle without model retraining or application deployment. Therefore, the inherent characteristics of code data provides an opportunity to perform program transformations to convert an out-of-scope program to an in-scope program. 

% %\textcolor{red}{Identifying the out-of-scope data in addition to transforming}
% %edited: In addition to addressing the key problem, i.e., adapting out-of-scope program inputs to DL models, identifying when an input falls beyond the DL model's handling capability is important. However, the task of identifying such inputs is quite challenging due to the lack of oracle defining the boundary of a DL model's handling scope. However, when inputs falls beyond the handling capability, their predictions can be misleading or wrong causing an lower accuracy and these predictions are noticed during the model's prediction process. 






% %After application deployment, we make the application recognize the inputs that are beyond its DL model’s handling capability and prevent them from impacting its decision-making (e.g., isolated or referring to manual driving), since the predictions for these inputs are unexpected and probably misleading or wrong. Then the application is still functional to other inputs and behaves as originally expected, without the need for model retraining or application redeployment.



% %To address the aforementioned challenges we propose CodeImprove, to distinguish those inputs that represent out-of-scope inputs from normal inputs. CodeImprove applies semantically equivalent program transformations to transform out-of-scope inputs into in-scope inputs. This can be accomplished by leveraging evolutionary programming to identify variants of programs that are capable of becoming in-scope inputs. 

% %Therefore, the inherent characteristic of code data provides an opportunity to perform program transformation to convert an out-of-scope program to an in-scope program. However, finding the set of transformations that will transform an out-of-scope program into in-scope program poses three challenges. 1) Maintaining Program Properties: The transformations applied to the program should not break the program properties that are crucial to the task (i.e., should not alter the ground-truth for the tasks). 2. Large Search Space: A program can be modified in a significantly large number of ways without the change of program semantics. Each code fragment can be represented in various forms, and the number of possible variable names is also numerous. Therefore, the search space for program modifications is very large. 3. Mapping from Feature Space to Program Space: The computation of in-scope and out-of-scope data are all based on feature vectors (used for DL models). However, it may not be possible to convert an in-scope feature vector back to a program source code in problem space because, for some feature vectors, the corresponding tokens in problem space may not be a valid code snippet (e.g., incorrect grammar). So the mapping from feature space to program space is invertible. Because of such invertible mapping, we can not use traditional gradient optimization approaches~\cite{} in searching for program transformation because the gradient optimization technique can find a solution only in feature space.

% %Deep Learning (DL) has become an important field of study due to its incredible performance in Computer Vision and Natural Language Processing (NLP). In NLP, DL models have shown a great improvement in performance of many tasks such as semantic role labeling~\cite{j}, named entity recognition~\cite{r}, machine translation~\cite{r}, and question answering~\cite{r}. Similarly, source code can be categorized as a structured natural language written by programmers.  Therefore, deep Learning (DL) models have demonstrated advantage on various code analysis tasks (e.g., detecting zero-day malware, code-clone detection, defect detection etc.) ~\cite{r}. 

% %However, no DL models are perfect, and all models face out-of-scope programs that the model cannot handle. The main obstacle to adopt DL models in industrial practice to automate program analysis is the lack of confidence and control on what DL models can handle. DL models approximate solutions for tasks by learning the patterns in the training datasets. However, if the training datasets are not representative or do not reflect the real-world scenarios, the model's approximation may fail to work on some data especially for models that take program data as inputs. DL models require the code to be embedded into a numerical vector to facilitate their processing. However, the resulting numerical vector may not fully capture all the syntactic and semantic details in the code due to features such as inter-procedural calls, control flows, and data dependencies. As a results many development teams are reluctant to adopt DL models for SE tasks due to the lack of understanding and control on what types of program that the approximation may be ineffective.   

% %SE communities learn to incorporate human intelligence into code analysis tools to adapt the tools to users' needs. According to the Rice's theorem~\cite{}, users cannot give an exact solution to any non-trivial code analysis problems due to the lack of one-size-fits-all approximations to make the program analysis satisfy the need of different users. Existing tools ~\cite{} decide to incorporate human intelligence to guide the analysis techniques (e.g., providing configuration options that allow users to tune the tool behaviors to the need of different SE tasks). 

% %Synergistic cooperation between human intelligence and DL models needs to be enabled the adoption of DL techniques on analyzing programs for various SE tasks. The synergy between human intelligence and DL models need to be achieved from both directions. On the one hand, users should be able to better understand what DL models can handle. On the other hand, the DL model should better understand the inherent characteristics of SE or programming language (PL) tasks. Although SE communities proposed many techniques on both (human-crafted) heuristic-based techniques~\cite{} and learning- based techniques~\cite{}, to date, little has been done to cooperatively leveraging both parties in analyzing programs.

% %There are two main issues faced by existing DL models in handling input programs. First, the existing DL models lack understanding about the inherent nature of programs. DL techniques need to convert the basic blocks of the programming languages into numerical vectors in order to leverage the DL models. This can be done by extracting the code tokens into intermediate representations via static analysis. However, designing an effective code representation using different language features can be difficult. As a result, each code representation has its shortcoming in capturing certain features in the code (e.g., abstract syntax tree cannot capture data dependencies). For example, existing work~\cite{} shows for two set of programs where one set is the semantically equivalent versions (with different variable names) of programs in the other set, DL models can achieve only 6\% of the original prediction accuracy on tasks such as source code classification. The results imply that input programs need to be in specific formats for DL models to perform reasonably well. Second, existing DL models lacks domain-specific knowledge of SE or PL tasks. DL models acquire knowledge by detecting common patterns in the training data, which may not encompass all types of knowledge required for these tasks. Such lack of knowledge may greatly impede the performance of DL models. For example, lacking knowledge about code grammars in code generation/translation tasks will result in generating illegal code. For example, CURE [61], the state-of-the-art DL-based program repair system, can achieve only a 28\% compilable rate in top-100 repair candidates.

% %DL models have limited performance at the inference time due to two reasons. First, programs may evolve (i,e. evolving programmers, evolving API versions etc.) that causes syntactical differences between training and testing code data which lead to mis-predictions. Second, because different DL models use different intermediate representations of a program (e.g., AST or CFG) to process programs, different models put higher weights on specific code syntax for predicting the output. As a result, the prediction of a DL model can be sensitive to features such as specific code syntax and mispredict programs with unfamiliar code syntax. Therefore, instead of immediately procissing an input program by a DL model, we first identify whether the input program is an out-of-scope or an in-scope program within the model's handling capability. 

% %The possibility of correctly predicting an input program may increase if the program is with code syntax that the DL models are familiar. Therefore, the inherent characteristic of code data provides an opportunity to perform program transformation to convert an out-of-scope program to an in-scope program. However, finding the set of transformations that will transform an out-of-scope program into in-scope program poses three challenges. 1) Maintaining Program Properties: The transformations applied to the program should not break the program properties that are crucial to the task (i.e., should not alter the ground-truth for the tasks). 2. Large Search Space: A program can be modified in a significantly large number of ways without the change of program semantics. Each code fragment can be represented in various forms, and the number of possible variable names is also numerous. Therefore, the search space for program modifications is very large. 3. Mapping from Feature Space to Program Space: The computation of in-scope and out-of-scope data are all based on feature vectors (used for DL models). However, it may not be possible to convert an in-scope feature vector back to a program source code in problem space because, for some feature vectors, the corresponding tokens in problem space may not be a valid code snippet (e.g., incorrect grammar). So the mapping from feature space to program space is invertible. Because of such invertible mapping, we can not use traditional gradient optimization approaches~\cite{} in searching for program transformation because the gradient optimization technique can find a solution only in feature space.

% %To address the aforementioned challenges we propose CodeImprove, to distinguish those inputs that represent out-of-scope inputs from normal inputs. CodeImprove applies semantically equivalent program transformations to transform out-of-scope inputs into in-scope inputs. This can be accomplished by leveraging evolutionary programming to identify variants of programs that are capable of becoming in-scope inputs. 

% %As previously mentioned, DL models may put higher weights on specific code syntax in decision making. Thus, the possibility of correctly predicting an input program may increase if the program is with code syntax that the DL models are familiar with. Therefore, our intuition is that the inherent char- acteristics of code data provides an opportunity for us to perform program transformation to convert an out-of-scope program to an in-scope program. However, finding the set of transformations that will trans- form an out-of-scope program into in-scope program is challenging due to the following three reasons. 1. Maintaining Program Properties: The transformations applied to the program should not break the program properties that are crucial to the tasks (i.e., should not alter the program label or ground-truth for the tasks). For example, for method name generation task, transformation should not replace variable names with some random tokens as some variable names can be strong indicators about the functionality of the whole method. 2. Large Search Space: A program can be modified in a significantly large number of ways without the change of program semantics. Each code fragment can be represented in various forms, and the num- ber of possible variable names is also numerous. Therefore, the search space for program modifications is very large. 3. Mapping from Feature Space to Program Space: The computation of in-scope and out-of-scope data are all based on feature vectors (used for DL models). However, it may not be possible to convert an in-scope feature vector back to a program source code in problem space because, for some feature vectors, the corresponding tokens in problem space may not be a valid code snippet (e.g., incorrect grammar). So the mapping from feature space to program space is invertible. Because of such invertible mapping, we can not use traditional gradient optimization approaches [39, 62, 125] in searching for program transformation because the gradient optimization technique can find a solution only in feature space.

% %%%%%The first code snippet (Line 4-6) is supported with C++ 14, which could assign variables with type auto. If this code snippet is feed into a DL model trained with C/C++ code before C++ 14, DL models can not provide the correct prediction. However, the second code snippet (Line 9-11) is semantically equivalent to the first code snippet, but the DL model should be able to handle the second code snippet. In RG1, we aim to detect out-of-scope programs to avoid feed inputs like the first code snippet to the DL model. For the detected out-of-scope programs, later in RG2, we will try to transform them into in-scope programs (e.g., transforming the first code snippet to the second code snippet) for DL models to handle.





% %Approximation is common in existing non-DL-based code analysis techniques, yet many techniques are adopted in the industrial practice because SE communities learn to incorporate human intelligence into the code analysis tools to adapt the tools to users’ needs. The approximations in analyzing programs are inevitable because we cannot give an exact solution to any non-trivial code analysis problem, according to Rice’s theorem [55]. Because there is no one-size-fits-all approximation makes the program analysis satisfy the need of different users, to enable industrial adoption of program analysis techniques, existing tools [17, 63, 76, 87, 117, 133] decide to incorporate human intelligence to guide the analysis techniques (e.g., providing configuration options that allow users to tune the tool behaviors to the need of different SE tasks).

% %Inspired by how the SE community dealt with approximation in symbolic reasoning-based program analysis, this proposal seeks to make progress on synergistic cooperation between human intelligence and DL models to enable the adoption of Deep Learning techniques on analyzing programs for various SE tasks. The synergy between human intelligence and DL models need to be achieved from both directions. On the one hand, users should be able to better understand what DL models can handle. On the other hand, the DL model should better understand the inherent characteristics of SE or programming language (PL) tasks (i.e., what users want). The synergy is also driven by the unique need for program analysis tasks: the well- defined programming language structures and grammars require precise logical reasoning through human intelligence, while the “naturalness” of human-written programs requires Deep Learning techniques to exploits the regular coding idioms, patterns, and conventions. Although SE communities proposed many techniques on both (human-crafted) heuristic-based techniques [14,15,22,90,101,113,148,156] and learning- based techniques [10, 71, 74, 75, 80, 88, 100, 102, 107, 110, 124, 129, 130, 157], to date, little has been done to cooperatively leveraging both parties in analyzing programs.


%Source code needs to be embedded into a numerical vector for DL models to process while the numerical vector can hardly comprehend all syntactic and semantic information in the code due to features such as inter procedural call, control flows, and data dependencies. 

%DL models approximate solutions for tasks by learning the data patterns in training datasets. When the training datasets are not representative, the approximation may not work for some data especially for models that take program data as inputs. Code needs to be embedded into a numerical vector for DL models to process while the numerical vector can hardly comprehend all syntactic and semantic information in the code comprehensively, due to features such as inter procedural calls, control flows, and data dependencies. As a result, many development teams are reluctant to adopt DL models for certain SE tasks because of the lack of understanding and control on what types of program that the approximation may be ineffective. 

%%DL models require the conversion of code into a numerical vector to facilitate their processing. However, this conversion poses a challenge since the resulting numerical vector may not fully capture all the syntactic and semantic details in the code, including inter-procedural calls, control flows, and data dependencies.


%Specifically, we hope to address two main issues faced by existing DL models in handling input programs. First, the existing DL models lack understanding about the inherent nature of programs. To leverage DL models, DL techniques need to map the basic building blocks of the programming languages into vectors. For this purpose, DL techniques usually extract code tokens into intermediate representations (e.g., control-flow graphs) via lightweight static analysis. However, it is difficult to design an effective code representation that can precisely capture code semantics in a wide range of programs using different language features. As a result, each code representation has its shortcoming in capturing certain features in the code (e.g., abstract syntax tree cannot capture data dependencies). For example, existing work [144, 147] shows for two set of programs where one set is the semantically equivalent versions (with different variable names) of programs in the other set, DL models can achieve only 6\% of the original prediction accuracy on tasks such as source code classification. The results imply that input programs need to be in specific for- mats for DL models to perform reasonably well. Second, existing DL models lacks domain-specific knowledge of SE or PL tasks. DL models learn knowledge through common patterns in the training data, while not all knowledge would exist in data as common patterns. Such lack of knowledge may greatly impede the performance of DL models. Take code generation/translation task as an example, lacking knowledge about code grammars will result in generating illegal code. For example, CURE [61], the state-of-the-art DL-based program repair system, can achieve only a 28\% compilable rate in top-100 repair candidates.









