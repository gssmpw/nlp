\section{Related Work}
\textbf{Safe RL:} Safe RL employs the cMDP framework, formulated in____. Without prior knowledge,____ focus on safe exploration, while with prior knowledge,____ learn safe policies using control techniques. ____ enforce safety constraints via Lagrangian or constrained optimization which can often lead to suboptimal safety-reward trade-offs. Instead, we extend safety state augmentation____ to LLMs and latent MDPs to ensure almost sure inference time safety.\looseness=-1


\textbf{LLM alignment and safety:} Pre-trained LLMs are often aligned to specific tasks using RL from Human Feedback (RLHF), where LLMs are fine-tuned with a learned reward model____ or directly optimized from human preferences____. ____ first applied fine-tuning in the context of safety, and ____ proposed safe fine-tuning via Lagrangian optimization. Other safety-focused methods such as ____ are either orthogonal, handle a different problem to ours, or can not ensure almost sure safety during inference. 



\textbf{Inference time alignment:} Inference-time alignment of LLMs is often performed through guided decoding, which steers token generation based on rewards____ or a trained value function____. Other safety-focused inference-time methods include____. Compared to those methods, we are the first to theoretically guarantee almost sure safe alignment with strong empirical results. Operating in the latent space enables us to train smaller, inference-efficient critics while optimally balancing rewards and safety constraints without introducing extra parameters, e.g., Lagrangian multipliers.\looseness=-1

We detail other related
works extensively in Appendix \ref{sec:rel-2}.