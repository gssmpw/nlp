@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={Proceedings of the 34th International Conference on Machine Learning (ICML)},
  pages={22--31},
  year={2017}
}

@book{altman1999constrained,
  title={Constrained Markov Decision Processes: Stochastic Modeling},
  author={Altman, Eitan},
  year={1999},
  publisher={CRC Press}
}

@article{azar2023general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:2310.12036},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{banerjee2024safeinfer,
  title={SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models},
  author={Banerjee, Somnath and Tripathy, Soham and Layek, Sayan and Kumar, Shanu and Mukherjee, Animesh and Hazra, Rima},
  journal={arXiv preprint arXiv:2406.12274},
  year={2024}
}

@inproceedings{berkenkamp2017safe,
  title={Safe model-based reinforcement learning with stability guarantees},
  author={Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela P and Krause, Andreas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={908--918},
  year={2017}
}

@inproceedings{chow2018lyapunov,
  title={Lyapunov-based safe policy optimization for continuous control},
  author={Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad and Pavone, Marco},
  booktitle={Proceedings of the 35th International Conference on Machine Learning},
  pages={1315--1324},
  year={2018}
}

@article{chow2019lyapunov,
  title={Lyapunov-based safe policy optimization for continuous control},
  author={Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad and Pavone, Marco},
  journal={arXiv preprint arXiv:1901.10031},
  year={2019}
}

@article{dai2023safe,
  title={Safe rlhf: Safe reinforcement learning from human feedback},
  author={Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong},
  journal={arXiv preprint arXiv:2310.12773},
  year={2023}
}

@inproceedings{dalal2018safe,
  title={Safe exploration in continuous action spaces},
  author={Dalal, Gal and Gilboa, Elad and Mannor, Shie and Shashua, Amnon},
  booktitle={Proceedings of the 35th International Conference on Machine Learning},
  pages={1437--1446},
  year={2018}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@article{gou2024eyes,
  title={Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation},
  author={Gou, Yunhao and Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu},
  journal={arXiv preprint arXiv:2403.09572},
  year={2024}
}

@article{gundavarapu2024machine,
  title={Machine Unlearning in Large Language Models},
  author={Gundavarapu, Saaketh Koundinya and Agarwal, Shreya and Arora, Arushi and Jagadeeshaiah, Chandana Thimmalapura},
  journal={arXiv preprint arXiv:2405.15152},
  year={2024}
}

@article{guo2024cold,
  title={Cold-attack: Jailbreaking llms with stealthiness and controllability},
  author={Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin},
  journal={arXiv preprint arXiv:2402.08679},
  year={2024}
}

@article{hammoud2024model,
  title={Model Merging and Safety Alignment: One Bad Model Spoils the Bunch},
  author={Hammoud, Hasan Abed Al Kader and Michieli, Umberto and Pizzati, Fabio and Torr, Philip and Bibi, Adel and Ghanem, Bernard and Ozay, Mete},
  journal={arXiv preprint arXiv:2406.14563},
  year={2024}
}

@article{han2024value,
  title={Value Augmented Sampling for Language Model Alignment and Personalization},
  author={Han, Seungwook and Shenfeld, Idan and Srivastava, Akash and Kim, Yoon and Agrawal, Pulkit},
  journal={arXiv preprint arXiv:2405.06639},
  year={2024}
}

@inproceedings{hua2024trustagent,
  title={Trustagent: Towards safe and trustworthy llm-based agents through agent constitution},
  author={Hua, Wenyue and Yang, Xianjun and Jin, Mingyu and Li, Zelong and Cheng, Wei and Tang, Ruixiang and Zhang, Yongfeng},
  booktitle={Trustworthy Multi-modal Foundation Models and AI Agents (TiFA)},
  year={2024}
}

@article{huang2024deal,
  title={DEAL: Decoding-time Alignment for Large Language Models},
  author={Huang, James Y and Sengupta, Sailik and Bonadiman, Daniele and Lai, Yi-an and Gupta, Arshit and Pappas, Nikolaos and Mansour, Saab and Kirchhoff, Katrin and Roth, Dan},
  journal={arXiv preprint arXiv:2402.06147},
  year={2024},
  url={https://arxiv.org/abs/2402.06147}
}

@article{khanov2024args,
  title={ARGS: Alignment as reward-guided search},
  author={Khanov, Maxim and Burapacheep, Jirayu and Li, Yixuan},
  journal={arXiv preprint arXiv:2402.01694},
  year={2024}
}

@inproceedings{koller2018learning,
  title={Learning-based model predictive control for safe exploration},
  author={Koller, Thomas and Berkenkamp, Felix and Turchetta, Matteo and Krause, Andreas},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)},
  pages={6059--6066},
  year={2018},
  organization={IEEE}
}

@article{kong2024aligning,
  title={Aligning Large Language Models with Representation Editing: A Control Perspective},
  author={Kong, Lingkai and Wang, Haorui and Mu, Wenhao and Du, Yuanqi and Zhuang, Yuchen and Zhou, Yifei and Song, Yue and Zhang, Rongzhi and Wang, Kai and Zhang, Chao},
  journal={arXiv preprint arXiv:2406.05954},
  year={2024}
}

@article{li2025salora,
  title={SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation},
  author={Li, Mingjie and Si, Wai Man and Backes, Michael and Zhang, Yang and Wang, Yisen},
  journal={arXiv preprint arXiv:2501.01765},
  year={2025}
}

@article{mudgal2023controlled,
  title={Controlled Decoding from Language Models},
  author={Mudgal, Sidharth and Lee, Jong and Ganapathy, Harish and Li, YaGuang and Wang, Tao and Huang, Yanping and Chen, Zhifeng and Cheng, Heng-Tze and Collins, Michael and Strohman, Trevor and Chen, Jilin and Beutel, Alex and Beirami, Ahmad},
  journal={arXiv preprint arXiv:2310.17022},
  year={2023},
  url={https://arxiv.org/abs/2310.17022}
}

@article{niu2024parameter,
  title={Parameter-Efficient Detoxification with Contrastive Decoding},
  author={Niu, Tong and Xiong, Caiming and Yavuz, Semih and Zhou, Yingbo},
  journal={arXiv preprint arXiv:2401.06947},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@inproceedings{ray2019benchmarking,
  title={Benchmarking safe exploration in deep reinforcement learning},
  author={Ray, Alex and Achiam, Joshua and Amodei, Dario},
  booktitle={Proceedings of the 2nd Conference on Robot Learning (CoRL)},
  pages={1--13},
  year={2019}
}

@article{shi2024decoding,
  title={Decoding-Time Language Model Alignment with Multiple Objectives},
  author={Shi, Ruizhe and Chen, Yifang and Hu, Yushi and Liu, ALisa and Smith, Noah and Hajishirzi, Hannaneh and Du, Simon},
  journal={arXiv preprint arXiv:2406.18853},
  year={2024}
}

@inproceedings{song2024preference,
  title={Preference ranking optimization for human alignment},
  author={Song, Feifan and Yu, Bowen and Li, Minghao and Yu, Haiyang and Huang, Fei and Li, Yongbin and Wang, Houfeng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024}
}

@inproceedings{sootla2022saute,
  title={Saut{\'e} rl: Almost surely safe reinforcement learning using state augmentation},
  author={Sootla, Aivar and Cowen-Rivers, Alexander I and Jafferjee, Taher and Wang, Ziyan and Mguni, David H and Wang, Jun and Bou-Ammar, Haitham},
  booktitle={International Conference on Machine Learning},
  pages={20423--20443},
  year={2022},
  organization={PMLR}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@inproceedings{stooke2020responsive,
  title={Responsive safety in reinforcement learning by monitoring risk and adapting policies},
  author={Stooke, Adam and Achiam, Joshua and Abbeel, Pieter},
  booktitle={Proceedings of the 37th International Conference on Machine Learning (ICML)},
  pages={8949--8958},
  year={2020}
}

@article{tang2024generalized,
  title={Generalized Preference Optimization: A Unified Approach to Offline Alignment},
  author={Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, R{\'e}mi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo {\'A}vila and Piot, Bilal},
  journal={arXiv preprint arXiv:2402.05749},
  year={2024}
}

@inproceedings{turchetta2016safe,
  title={Safe exploration in finite Markov decision processes with Gaussian processes},
  author={Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4312--4320},
  year={2016}
}

@article{wang2024probing,
  title={Probing the safety response boundary of large language models via unsafe decoding path generation},
  author={Wang, Haoyu and Wu, Bingzhe and Bian, Yatao and Chang, Yongzhe and Wang, Xueqian and Zhao, Peilin},
  journal={arXiv preprint arXiv:2408.10668},
  year={2024}
}

@article{wei2024assessing,
  title={Assessing the brittleness of safety alignment via pruning and low-rank modifications},
  author={Wei, Boyi and Huang, Kaixuan and Huang, Yangsibo and Xie, Tinghao and Qi, Xiangyu and Xia, Mengzhou and Mittal, Prateek and Wang, Mengdi and Henderson, Peter},
  journal={arXiv preprint arXiv:2402.05162},
  year={2024}
}

@article{xu2024safedecoding,
  title={Safedecoding: Defending against jailbreak attacks via safety-aware decoding},
  author={Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha},
  journal={arXiv preprint arXiv:2402.08983},
  year={2024}
}

@article{zeng2024root,
  title={Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level},
  author={Zeng, Xinyi and Shang, Yuying and Zhu, Yutao and Chen, Jiawei and Tian, Yu},
  journal={arXiv preprint arXiv:2410.06809},
  year={2024}
}

@article{zhang2024controllable,
  title={Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements},
  author={Zhang, Jingyu and Elgohary, Ahmed and Magooda, Ahmed and Khashabi, Daniel and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2410.08968},
  year={2024}
}

@article{zhao2023slic,
  title={Slic-hf: Sequence likelihood calibration with human feedback},
  author={Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J},
  journal={arXiv preprint arXiv:2305.10425},
  year={2023}
}

@article{zhao2024adversarial,
  title={Adversarial contrastive decoding: Boosting safety alignment of large language models via opposite prompt optimization},
  author={Zhao, Zhengyue and Zhang, Xiaoyun and Xu, Kaidi and Hu, Xing and Zhang, Rui and Du, Zidong and Guo, Qi and Chen, Yunji},
  journal={arXiv preprint arXiv:2406.16743},
  year={2024}
}

@article{zhong2024rose,
  title={ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding},
  author={Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={arXiv preprint arXiv:2402.11889},
  year={2024}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

