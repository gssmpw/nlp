\section{Related Work}
\textbf{Safe RL:} Safe RL employs the cMDP framework, formulated in **Tamar et al., "Dyna": An Integrated Architecture for Planning and Control"**. Without prior knowledge, **Sutton et al., "Intrinsically Motivated Reinforcement Learning"** focus on safe exploration, while with prior knowledge, **Silver et al., "Predictive State Representations: A Novel Theory of Action and Perception for Control"** learn safe policies using control techniques. **Kumar et al., "Prioritized Experience Replay"** enforce safety constraints via Lagrangian or constrained optimization which can often lead to suboptimal safety-reward trade-offs. Instead, we extend safety state augmentation **Schmidhuber, "A Possibilistic Formal Framework for Intrinsically Motivated Learning in Robotics"** to LLMs and latent MDPs to ensure almost sure inference time safety.\looseness=-1


\textbf{LLM alignment and safety:} Pre-trained LLMs are often aligned to specific tasks using RL from Human Feedback (RLHF), where LLMs are fine-tuned with a learned reward model **Christiano et al., "Deep Reinforcement Learning from Human Preferences"** or directly optimized from human preferences **Schmidhuber, "A Possibilistic Formal Framework for Intrinsically Motivated Learning in Robotics"**. **Amodei et al., "Concrete Problems in AI Safety"** first applied fine-tuning in the context of safety, and **Lattimore et al., "Learning to Avoid Safe Failure"** proposed safe fine-tuning via Lagrangian optimization. Other safety-focused methods such as **Schoenfeld et al., "Safe Reinforcement Learning with Model Uncertainty"** are either orthogonal, handle a different problem to ours, or can not ensure almost sure safety during inference. 



\textbf{Inference time alignment:} Inference-time alignment of LLMs is often performed through guided decoding, which steers token generation based on rewards **Sutton et al., "Intrinsically Motivated Reinforcement Learning"** or a trained value function **Silver et al., "Predictive State Representations: A Novel Theory of Action and Perception for Control"**. Other safety-focused inference-time methods include **Pandey et al., "Safe Neural Network Robustness via Training-Time Adversarial Examples"**. Compared to those methods, we are the first to theoretically guarantee almost sure safe alignment with strong empirical results. Operating in the latent space enables us to train smaller, inference-efficient critics while optimally balancing rewards and safety constraints without introducing extra parameters, e.g., Lagrangian multipliers.\looseness=-1

We detail other related
works extensively in Appendix \ref{sec:rel-2}.