\section{Related Work}
\textbf{Safe RL:} Safe RL employs the cMDP framework, formulated in~\citep{altman1999constrained}. Without prior knowledge,~\citep{turchetta2016safe, koller2018learning, dalal2018safe} focus on safe exploration, while with prior knowledge,~\citep{chow2018lyapunov, chow2019lyapunov, berkenkamp2017safe} learn safe policies using control techniques. \citep{achiam2017constrained, ray2019benchmarking, stooke2020responsive} enforce safety constraints via Lagrangian or constrained optimization which can often lead to suboptimal safety-reward trade-offs. Instead, we extend safety state augmentation~\cite{sootla2022saute} to LLMs and latent MDPs to ensure almost sure inference time safety.\looseness=-1


\textbf{LLM alignment and safety:} Pre-trained LLMs are often aligned to specific tasks using RL from Human Feedback (RLHF), where LLMs are fine-tuned with a learned reward model~\citep{stiennon2020learning, ziegler2019fine, ouyang2022training} or directly optimized from human preferences~\citep{rafailov2023direct, azar2023general, zhao2023slic, tang2024generalized, song2024preference, ethayarajh2024kto}. \citep{bai2022training, ganguli2022red} first applied fine-tuning in the context of safety, and \citep{dai2023safe} proposed safe fine-tuning via Lagrangian optimization. Other safety-focused methods such as \citep{gundavarapu2024machine,gou2024eyes,hammoud2024model,hua2024trustagent,zhang2024controllable,guo2024cold,xu2024safedecoding,wei2024assessing,li2025salora} are either orthogonal, handle a different problem to ours, or can not ensure almost sure safety during inference. 



\textbf{Inference time alignment:} Inference-time alignment of LLMs is often performed through guided decoding, which steers token generation based on rewards~\citep{khanov2024args,shi2024decoding,huang2024deal} or a trained value function~\citet{han2024value,mudgal2023controlled,kong2024aligning}. Other safety-focused inference-time methods include~\citep{zhong2024rose,banerjee2024safeinfer,niu2024parameter,wang2024probing,zeng2024root,zhao2024adversarial}. Compared to those methods, we are the first to theoretically guarantee almost sure safe alignment with strong empirical results. Operating in the latent space enables us to train smaller, inference-efficient critics while optimally balancing rewards and safety constraints without introducing extra parameters, e.g., Lagrangian multipliers.\looseness=-1

We detail other related
works extensively in Appendix \ref{sec:rel-2}.