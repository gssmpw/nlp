[
  {
    "index": 0,
    "papers": [
      {
        "key": "altman1999constrained",
        "author": "Altman, Eitan",
        "title": "Constrained Markov Decision Processes: Stochastic Modeling"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "turchetta2016safe",
        "author": "Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas",
        "title": "Safe exploration in finite Markov decision processes with Gaussian processes"
      },
      {
        "key": "koller2018learning",
        "author": "Koller, Thomas and Berkenkamp, Felix and Turchetta, Matteo and Krause, Andreas",
        "title": "Learning-based model predictive control for safe exploration"
      },
      {
        "key": "dalal2018safe",
        "author": "Dalal, Gal and Gilboa, Elad and Mannor, Shie and Shashua, Amnon",
        "title": "Safe exploration in continuous action spaces"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chow2018lyapunov",
        "author": "Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad and Pavone, Marco",
        "title": "Lyapunov-based safe policy optimization for continuous control"
      },
      {
        "key": "chow2019lyapunov",
        "author": "Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad and Pavone, Marco",
        "title": "Lyapunov-based safe policy optimization for continuous control"
      },
      {
        "key": "berkenkamp2017safe",
        "author": "Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela P and Krause, Andreas",
        "title": "Safe model-based reinforcement learning with stability guarantees"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "achiam2017constrained",
        "author": "Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter",
        "title": "Constrained policy optimization"
      },
      {
        "key": "ray2019benchmarking",
        "author": "Ray, Alex and Achiam, Joshua and Amodei, Dario",
        "title": "Benchmarking safe exploration in deep reinforcement learning"
      },
      {
        "key": "stooke2020responsive",
        "author": "Stooke, Adam and Achiam, Joshua and Abbeel, Pieter",
        "title": "Responsive safety in reinforcement learning by monitoring risk and adapting policies"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "sootla2022saute",
        "author": "Sootla, Aivar and Cowen-Rivers, Alexander I and Jafferjee, Taher and Wang, Ziyan and Mguni, David H and Wang, Jun and Bou-Ammar, Haitham",
        "title": "Saut{\\'e} rl: Almost surely safe reinforcement learning using state augmentation"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rafailov2023direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "azar2023general",
        "author": "Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, R{\\'e}mi",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      },
      {
        "key": "zhao2023slic",
        "author": "Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J",
        "title": "Slic-hf: Sequence likelihood calibration with human feedback"
      },
      {
        "key": "tang2024generalized",
        "author": "Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, R{\\'e}mi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo {\\'A}vila and Piot, Bilal",
        "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment"
      },
      {
        "key": "song2024preference",
        "author": "Song, Feifan and Yu, Bowen and Li, Minghao and Yu, Haiyang and Huang, Fei and Li, Yongbin and Wang, Houfeng",
        "title": "Preference ranking optimization for human alignment"
      },
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "ganguli2022red",
        "author": "Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others",
        "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "dai2023safe",
        "author": "Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong",
        "title": "Safe rlhf: Safe reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "gundavarapu2024machine",
        "author": "Gundavarapu, Saaketh Koundinya and Agarwal, Shreya and Arora, Arushi and Jagadeeshaiah, Chandana Thimmalapura",
        "title": "Machine Unlearning in Large Language Models"
      },
      {
        "key": "gou2024eyes",
        "author": "Gou, Yunhao and Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu",
        "title": "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation"
      },
      {
        "key": "hammoud2024model",
        "author": "Hammoud, Hasan Abed Al Kader and Michieli, Umberto and Pizzati, Fabio and Torr, Philip and Bibi, Adel and Ghanem, Bernard and Ozay, Mete",
        "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch"
      },
      {
        "key": "hua2024trustagent",
        "author": "Hua, Wenyue and Yang, Xianjun and Jin, Mingyu and Li, Zelong and Cheng, Wei and Tang, Ruixiang and Zhang, Yongfeng",
        "title": "Trustagent: Towards safe and trustworthy llm-based agents through agent constitution"
      },
      {
        "key": "zhang2024controllable",
        "author": "Zhang, Jingyu and Elgohary, Ahmed and Magooda, Ahmed and Khashabi, Daniel and Van Durme, Benjamin",
        "title": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements"
      },
      {
        "key": "guo2024cold",
        "author": "Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin",
        "title": "Cold-attack: Jailbreaking llms with stealthiness and controllability"
      },
      {
        "key": "xu2024safedecoding",
        "author": "Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha",
        "title": "Safedecoding: Defending against jailbreak attacks via safety-aware decoding"
      },
      {
        "key": "wei2024assessing",
        "author": "Wei, Boyi and Huang, Kaixuan and Huang, Yangsibo and Xie, Tinghao and Qi, Xiangyu and Xia, Mengzhou and Mittal, Prateek and Wang, Mengdi and Henderson, Peter",
        "title": "Assessing the brittleness of safety alignment via pruning and low-rank modifications"
      },
      {
        "key": "li2025salora",
        "author": "Li, Mingjie and Si, Wai Man and Backes, Michael and Zhang, Yang and Wang, Yisen",
        "title": "SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "khanov2024args",
        "author": "Khanov, Maxim and Burapacheep, Jirayu and Li, Yixuan",
        "title": "ARGS: Alignment as reward-guided search"
      },
      {
        "key": "shi2024decoding",
        "author": "Shi, Ruizhe and Chen, Yifang and Hu, Yushi and Liu, ALisa and Smith, Noah and Hajishirzi, Hannaneh and Du, Simon",
        "title": "Decoding-Time Language Model Alignment with Multiple Objectives"
      },
      {
        "key": "huang2024deal",
        "author": "Huang, James Y and Sengupta, Sailik and Bonadiman, Daniele and Lai, Yi-an and Gupta, Arshit and Pappas, Nikolaos and Mansour, Saab and Kirchhoff, Katrin and Roth, Dan",
        "title": "DEAL: Decoding-time Alignment for Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "han2024value",
        "author": "Han, Seungwook and Shenfeld, Idan and Srivastava, Akash and Kim, Yoon and Agrawal, Pulkit",
        "title": "Value Augmented Sampling for Language Model Alignment and Personalization"
      },
      {
        "key": "mudgal2023controlled",
        "author": "Mudgal, Sidharth and Lee, Jong and Ganapathy, Harish and Li, YaGuang and Wang, Tao and Huang, Yanping and Chen, Zhifeng and Cheng, Heng-Tze and Collins, Michael and Strohman, Trevor and Chen, Jilin and Beutel, Alex and Beirami, Ahmad",
        "title": "Controlled Decoding from Language Models"
      },
      {
        "key": "kong2024aligning",
        "author": "Kong, Lingkai and Wang, Haorui and Mu, Wenhao and Du, Yuanqi and Zhuang, Yuchen and Zhou, Yifei and Song, Yue and Zhang, Rongzhi and Wang, Kai and Zhang, Chao",
        "title": "Aligning Large Language Models with Representation Editing: A Control Perspective"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhong2024rose",
        "author": "Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng",
        "title": "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding"
      },
      {
        "key": "banerjee2024safeinfer",
        "author": "Banerjee, Somnath and Tripathy, Soham and Layek, Sayan and Kumar, Shanu and Mukherjee, Animesh and Hazra, Rima",
        "title": "SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models"
      },
      {
        "key": "niu2024parameter",
        "author": "Niu, Tong and Xiong, Caiming and Yavuz, Semih and Zhou, Yingbo",
        "title": "Parameter-Efficient Detoxification with Contrastive Decoding"
      },
      {
        "key": "wang2024probing",
        "author": "Wang, Haoyu and Wu, Bingzhe and Bian, Yatao and Chang, Yongzhe and Wang, Xueqian and Zhao, Peilin",
        "title": "Probing the safety response boundary of large language models via unsafe decoding path generation"
      },
      {
        "key": "zeng2024root",
        "author": "Zeng, Xinyi and Shang, Yuying and Zhu, Yutao and Chen, Jiawei and Tian, Yu",
        "title": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level"
      },
      {
        "key": "zhao2024adversarial",
        "author": "Zhao, Zhengyue and Zhang, Xiaoyun and Xu, Kaidi and Hu, Xing and Zhang, Rui and Du, Zidong and Guo, Qi and Chen, Yunji",
        "title": "Adversarial contrastive decoding: Boosting safety alignment of large language models via opposite prompt optimization"
      }
    ]
  }
]