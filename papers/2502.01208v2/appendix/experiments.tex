\section{Experimental Details}\label{App:Exps}
\subsection{Experiment Setup}
We conducted all experiments on NVIDIA V100 GPUs, utilizing Python 3.11.11 and CUDA 12.4, for model development and training. The experimental setup was based on the PKU-SaferRLHF dataset.Our experiments involved two primary models: the unsafe-aligned reproduced Alpaca-7B model\footnote{https://huggingface.co/PKU-Alignment/alpaca-7b-reproduced}~\citep{taori2023alpaca} and the safe-aligned Beaver-v3-7B model~\citep{ji2024pku}. We employed the reward model~\cite{khanov2024args}\footnote{https://huggingface.co/argsearch/llama-7b-rm-float32} and a cost model~\cite{ji2024pku}\footnote{https://github.com/PKU-Alignment/safe-rlhf} during the training stage for critic network, test-time inference and evaluation stages. 

\subsection{Baselines and Hyper-parameter Settings}

The baseline methods we compare against include \textbf{BoN}, \textbf{Beam Search}, \textbf{RECONTROL}~\citep{kong2024aligning}, and \textbf{ARGS}~\citep{khanov2024args}. For \textbf{BoN}, we use a strategy where the top N outputs are sampled, and the best result is selected based on a predefined criterion. Similarly, \textbf{Beam Search} explores multiple beams during the search process and selects the best output based on a beam-width parameter. In \textbf{RECONTROL}, an MLP network is employed as the value network to intervene in the decision-making process, guiding the generation through reinforcement learning~\citep{kong2024aligning}. \textbf{ARGS}, on the other hand, implements a logits-based greedy token-wise search strategy, where tokens are generated sequentially based on the maximum likelihood of the next token~\citep{khanov2024args}.

Given the limited research on safe alignment with inference-time methods, we adapt these baseline methods to enable safe inference, ensuring a fair comparison. To this end, we incorporate a Lagrangian multiplier term to control the balance between reward and cost, allowing for safe inference based on their open-source implementations. Notably, \textbf{BoN} and \textbf{Beam Search} utilize a form of blocking sampling, while \textbf{ARGS} and \textbf{RECONTROL} employ token sampling methods. 

In our setup, we modify the inference process of the baseline methods with a Lagrangian multiplier by using the following score for token selection: $c_{\text{task}} + \lambda \mathcal{C}_{\text{safety}}$
where $\lambda$ is the Lagrangian multiplier, that controls the influence of the safety cost score $\mathcal{C}_{\text{safety}}$. We unify $\lambda = 5$ and also the sampling parameters for baseline methods and InferenceGuard to ensure a fair comparison. Each method has its own specific settings, as follows:

\begin{table}[ht]
\centering
\begin{tabular}{l|l|l|l}
\toprule
\textbf{Method} & \textbf{Sample Width} $(d)$ & \textbf{Num Samples} $(N)$ & \textbf{Other Parameters} \\
\midrule
\textbf{ARGS} & 1 & 20 & N/A \\
\textbf{RECONTROL} & 1 & N/A & \textbf{n steps} = 30, \textbf{step size} = 0.5 \\
\textbf{BoN} & 32 & 100, 200, 500 & N/A \\
\textbf{Beam Search} & 32 & 128, 256 & D=128, K=$N/4$ \\
\textbf{InferenceGuard} & 32 & 128, 512 & M=2, D=128, K=$N/4$ \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Baselines and InferenceGuard.}
\label{tab:hyperparameters}
\end{table}


These hyperparameter settings ensure a fair and controlled comparison of the baseline methods and the proposed InferenceGuard model. We compare the performance of baseline methods, with reward model, cost model and lagriangian multiper setting, under these hyperparameter settings, and summarised the result in Table~\ref{tab:performance_comparison}.

When sampling from the base model, we used a temperature of 1.0 without top\_k nor top\_p.


\begin{table}[h!]
 \centering
 \caption{Performance Comparison on Dataset PKU-SafeRLHF}
 \label{tab:performance_comparison}
 \resizebox{0.9\textwidth}{!}{%
 \begin{tabular}{lccccc}
     \toprule
      & Method & Average Reward & Average Cost & Safety Rate & Inference Time (s)  \\
     \midrule
     Alpaca-7B & Base & 6.15 ($\pm$ 1.51) & 1.33 & 29.47\% & 1.1  \\
                & RECONTROL & 6.2 ($\pm$ 1.56) & 1.33 & 29.5\% & 1.7  \\
                & RECONTROL + Lagrangian multiplier & 6.19 ($\pm$ 1.50) & 1.33 & 29.7\% & 2  \\
               &  Best-of-N + Lagrangian multiplier $N=100$, $\lambda=5$ & 5.35 ($\pm$ 1.62) & -0.46 & 48.22\% &29\\
                &  Best-of-N + Lagrangian multiplier $N=200$, $\lambda=5$ & 5.25 ($\pm$ 1.64) & -0.72 & 54.2\% & 58 \\
                &  Best-of-N + Lagrangian multiplier $N=500$, $\lambda=5$ & 6.04 ($\pm$ 1.85) & -1.27 & 52.17\% & 145 \\
                &  Best-of-N + Lagrangian multiplier $N=500$, $\lambda=10$ & 5.51 ($\pm$ 1.66) & -1.44 & 54.01\% & 145 \\
                &  Best-of-N + Augmented safety $N=200$ & 7.51 ($\pm$ 1.89) & 0.67 & 60.07\% & 58 \\
                &  Best-of-N + Augmented safety $N=500$ & 7.78 ($\pm$ 2.09) & 0.42 & 65.74 \% & 145 \\
                & Beam search + Lagrangian multiplier $N=128$, $\lambda = 5$ & 6.58 ($\pm$ 1.95) & -1.02 & 50.19\% & 32 \\
                & Beam search + Lagrangian multiplier $N=256$, $\lambda = 5$ & 6.69 ($\pm$ 2.08) & -1.28 & 52.43\% & 60 \\
                 & Beam search + Augmented safety $N=128$ & 8.29 ($\pm$ 2.02) & 0.64 & 58.89\% & 39 \\
                & Beam search + Augmented safety $N=256$ &  8.69 ($\pm$ 2.15) & 0.55 & 61.79 \% & 82 \\               & ARGS $\omega=2.5$ & 6.74 ($\pm$ 1.70) & 1.47 & 28.19\% & 82  \\
                & ARGS + Lagrangian multiplier $\omega=2.5$ & 3.21 ($\pm$ 1.59) & -0.85 & 75.8\% & 111\\
                & ARGS + Cost Model $\omega=2.5$ & 0.19 ($\pm$ 1.65) & -2.21 & 81.6\% & 78 \\
               & InferenceGuard (N=128) & 7.08 ($\pm$ 2.49) & -0.63 & 88.14\% & 65\\
               & InferenceGuard with Critic (N=128)  & 6.81 ($\pm\ $2.7) & -1.01 & \textbf{91.04}\% & 71 \\ 
               \midrule % 
    Beaver-7B-v3 & Base & 5.83 ($\pm$ 1.62) & -2.89 & 75.89\% & 1.2  \\
                & RECONTROL & 5.9 ($\pm$ 1.56) & -2.90 & 75.9\% & 2  \\
                & RECONTROL + Lagrangian multiplier & 5.91 ($\pm$ 1.50) & -2.91 & 75.9\% & 2.6  \\
                & Best-of-N + Lagrangian multiplier $N=100$ & 6.52 ($\pm$ 1.88) & -3.63 & 85.7\% & 40\\
                & Best-of-N + Lagrangian multiplier $N=200$ & 6.61 ($\pm$ 1.89) & -3.62 & 85.8\% & 58 \\
                &  Best-of-N + Augmented safety $N=100$ & 8.55 ($\pm$ 1.58) & -2.96 & 97.23\% & 40 \\
                &  Best-of-N + Augmented safety $N=200$ & 9.01 ($\pm$ 1.63) & -2.98 & 97.76\% & 58 \\
                & Beam search + Lagrangian multiplier $N=64$, $\lambda = 5$ & 8.33 ($\pm$ 1.79) & -4.09 & 87.08\% & 36 \\
                & Beam search + Lagrangian multiplier $N=128$, $\lambda = 5$ & 8.63 ($\pm$ 1.80) & -4.21 & 87.35 \% & 64 \\
                 & Beam search + Augmented safety $N=64$ & 9.84 ($\pm$ 1.4) & -2.93 & 95.38 \% & 22 \\
                & Beam search + Augmented safety $N=128$ & 10.31 ($\pm$ 1.37) & -2.94 & 97.36\% & 39 \\ 
                & ARGS $\omega=2.5$ & 6.72 ($\pm$ 1.83) & -2.59 & 78.5\% & 94 \\
                & ARGS $\omega=2.5$ + Lagrangian multiplier & 2.26 ($\pm$ 1.56) & -1.64 & 81\% & 127 \\
                & ARGS $\omega=2.5$ + Cost Model & 0.01 ($\pm$ 1.37) & -3.27 & 98.4\% & 90 \\
                & InferenceGuard (N=128) & 10.26 ($\pm$ 1.42) & -2.96 & 99.7\% &39 \\
                & InferenceGuard with Critic (N=128) & \textbf{10.27 ($\pm$ 1.50)} & \textbf{-2.94} & \textbf{100}\% & 39 \\
     \bottomrule
 \end{tabular}%
}
\end{table}


\subsection{Critic Network and Training Process}
This section outlines the critic network architecture used for InferenceGuard. The critic network is designed to estimate the cost of partial responses and guide optimization during inference. We assume that trajectories terminate at a maximum time \(T\), and the critic aims to predict the sign of the safety compliance metric \(\text{z}_T\), and the discounted cumulative task cost \(\gamma^T \bar{c}_{\text{task}}\).

\paragraph{Critic Network Architecture}
The critic network takes two types of input: the hidden states (\(\bm{h}_t\)) and the key-value pairs (\(\mathbf{o}_t\)), representing contextual and state information, respectively. These are passed through a series of layers to estimate the required outputs. The network utilizes downscaling and attention layers to reduce the dimensionality of the input data, ensuring efficient processing of large-scale representations.

In terms of model size, the total parameter count of the critic network is approximately 0.7 billion parameters, providing a balance between model capacity and computational efficiency.

\paragraph{Training Process}
The critic network is trained using a combination of optimization techniques aimed at predicting the safety compliance and task cost accurately. The network is optimized with the hyperparameters in Table~\ref{tab:critic_hyperparameters}.

\begin{table}[!ht]
\centering
\begin{tabular}{l|l}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Hidden Dimension & 4096 \\
Learning Rate & \(1 \times 10^{-5}\) \\
Number of Epochs & 50 \\
Discount Factor (\(\gamma\)) & 0.999 \\
Batch Size & 8 \\
Safety Budget $d$ & 10 \\
%Penalty $n$ & -25 \\ % mz: this shouldn't be here, we said our critic do not depends on n
\bottomrule
\end{tabular}
\caption{Hyperparameters for Critic Network Training.}
\label{tab:critic_hyperparameters}
\end{table}


During training, the network is fed batches of hidden states and key-value pairs, and the weights are updated to minimize the loss between predicted and true values. The critic network's ability to predict both the safety compliance and task cost ensures it can guide the optimization process at inference time, while adhering to safety constraints.

The model uses a penalty term to enforce the safety budget constraint. This penalty discourages the network from violating the safety threshold, steering for safer responses during intervention.


\subsection{Qualitative Comparisons}\label{app:qualitativecomp}
We present several examples from the PKU-SafeRLHF test dataset to demonstrate how InferenceGuard steers the base model outputs towards safer responses. These examples highlight differences in response safety between InferenceGuard and baseline methods, including \textbf{BoN}, \textbf{Beam Search}, \textbf{RECONTROL}, and \textbf{ARGS}, evaluated on the Alpaca and Beaver-v3 models, as shown in Figure~\ref{fig:responses_examples_alpaca} and \ref{fig:responses_examples_beaver}. In each case, InferenceGuard successfully adheres to safety constraints while maintaining task performance.


\begin{figure}[!h]
\centering

\subfigure[Alpaca Example 1]{\includegraphics[width=0.7\textwidth]{imgs/alpaca_example.pdf}}\\
\subfigure[Alpaca Example 2]{\includegraphics[width=0.7\textwidth]{imgs/alpaca_example_2.pdf}}\\
\caption{Generated response by different methods on the Alpaca-7B}
\label{fig:responses_examples_alpaca}
\end{figure}


\begin{figure}[!h]
\centering

\subfigure[Beaver Example 1]
{\includegraphics[width=0.7\textwidth]{imgs/beaver_example_2.pdf}}\\
\subfigure[Beaver Example 2]
{\includegraphics[width=0.68\textwidth]{imgs/beaver_example.pdf}}\\
\caption{Generated response by different methods on the Beaver-v3-7B}
\label{fig:responses_examples_beaver}
\end{figure}




