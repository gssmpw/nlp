
\onecolumn
\section{Additional Related Work}\label{sec:rel-2}

\textbf{Safe RL:} Safe RL employs the cMDP framework~\citep{altman1999constrained} to enforce safety constraints during exploration and policy optimization. When no prior knowledge is available, methods focus on safe exploration~\citep{turchetta2016safe, koller2018learning, dalal2018safe, wachi2018safe, bharadhwaj2020conservative}. With prior knowledge, such as environmental data or an initial safe policy, methods learn safe policies using control techniques like Lyapunov stability~\citep{chow2018lyapunov, chow2019lyapunov, berkenkamp2017safe, ohnishi2019barrier} and reachability analysis~\citep{cheng2019end, akametalu2014reachability, dean2019safeguarding, fisac2019bridging}. Safety constraints are enforced via Lagrangian or constrained optimization methods~\citep{achiam2017constrained, ray2019benchmarking, stooke2020responsive, yang2019relative, ding2020natural, ji2024pku}, but can often lead to suboptimal safety-reward trade-offs. In contrast, our approach extends safety state augmentation~\cite{sootla2022saute} to LLMs and latent MDPs to ensure almost sure inference time safety without relying on Lagrangian multipliers.



\textbf{LLM alignment and safety:} Methods for aligning pre-trained LLMs with task-specific data include prompting, guided decoding, and fine-tuning.
Among fine-tuning methods, RL from Human Feedback (RLHF) has proven effective, where LLMs are fine-tuned with a learned reward model~\citep{stiennon2020learning, ziegler2019fine, ouyang2022training} or directly optimized from human preferences~\citep{rafailov2023direct, azar2023general, zhao2023slic, tang2024generalized, song2024preference, ethayarajh2024kto, ramesh2024group}. Recent works have explored fine-tuning for helpful and harmless responses~\citep{bai2022training, ganguli2022red}, while \citep{dai2023safe} introduced a safe RL approach incorporating safety cost functions via Lagrangian optimization, requiring model weight fine-tuning. Other safety-focused methods, including machine unlearning~\citep{gundavarapu2024machine}, safety pre-aligned multi-modal LLMs~\citep{gou2024eyes}, safety-aware model merging~\citep{hammoud2024model}, prompting-based safety methodologies~\citep{hua2024trustagent}, test-time controllable safety alignment~\citep{zhang2024controllable}, defenses against adversarial attacks and jailbreaking~\citep{guo2024cold, qi2024safety, xu2024safedecoding}, identifying safety criticial regions in LLMs \citep{wei2024assessing},  safety preserved LoRA fine-tuning \citep{li2025salora},  alignment using correctional residuals between preferred and dispreferred answers using a small model \citep{ji2024aligner}, and identifying safety directions in embedding space~\citep{feng2024legend}.Those methods are either orthogonal, handle a different problem to ours, or can not ensure almost sure safety during inference. 



\textbf{Inference time alignment:} The closest literature to ours is inference-time alignment. Those methods offer flexible alternatives to fine-tuning LLMs, as they avoid modifying the model weights. A common approach is guided decoding, which steers token generation based on a reward model. In particular, \citep{khanov2024args,shi2024decoding,huang2024deal} perform this guided decoding through scores from the reward model whereas \citet{han2024value,mudgal2023controlled,kong2024aligning} use a value function that is trained on the given reward model. These inference-time alignment methods build on previous works like \citep{yang2021fudge,arora2022director,krause2021gedi,kim2023critic,meng2022nado,peng2019awr}, which guide or constrain LLMs towards specific objectives. Other safety-focused inference-time methods include, reverse prompt contrastive decoding \citep{zhong2024rose}, adjusting model hidden states combined with guided decoding \citep{banerjee2024safeinfer}, soft prompt-tuned detoxifier based decoding \citep{niu2024parameter}, jail-break value decoding \citep{wang2024probing}, speculating decoding using safety classifier \citep{zeng2024root}, and opposite prompt-based contrastive decoding \citep{zhao2024adversarial}. Compared to those methods, we are the first to achieve almost sure safe alignment with strong empirical results. Operating in the latent space enables us to train smaller, inference-efficient critics while optimally balancing rewards and safety constraints (see Section \ref{Sec:Exp}) without introducing additional parameters like Lagrangian multipliers.
\section{Theoretical Analysis}\label{sec: saute-thm}

For our theoretical results, we consider a similar setup to that of \citet{sootla2022saute,hernandez1992discrete}  but with a discrete action space. Consider an MDP \( M = \{S, A, P, c, \gamma_c\} \) with a discrete, non-empty, and finite action set for each $s$ defined as \( A(s) \). The set
\[
K = \{(s, a) \mid s \in S, a \in A(s)\}
\]
defines the admissible state-action pairs and is assumed to be a Borel subset of \( S \times A \). A function \( u \) is \emph{inf-compact} on \( K \) if the set
$\{ a \in A(s) \mid u(s, a) \leq r \}$
is compact for every \( s \in S \) and \( r \in \mathbb{R} \).
Note that, since the action space is finite and discrete every function $u$ is inf-compact on $K$.  A function $u$ is lower semi-continuous (l.s.c.) in $S$ if for every $s_0 \in S$ we have
\[
\liminf_{s \to s_0} u(s) \geq u(s_0).
\] Let \( L(S) \) denote the class of all functions on \( S \) that are l.s.c. and bounded from below.


For a given action, $a\in A(s)$, a distribution \( P(y \mid s, a) \) is called \emph{weakly continuous} w.r.t. $s$, if for any function \( u(s) \), continuous and bounded w.r.t. $s$ on \( S \),  the map
\[
(s, a) \mapsto \int_S u(y) P(dy \mid s, a)
\]
is continuous on \( K \) for a given $a$.


We also make the following assumptions:

\textbf{B1.} The function \( c(s, a) \) is bounded, measurable on \( K \), nonnegative, lower semi-continuous w.r.t. $s$ for a given $a\in A(s)$;

\textbf{B2.} The transition law \( P \) is weakly continuous w.r.t. $s$ for a given $a\in A(s)$;

\textbf{B3.} The set-value function map $s:A(s)$ satisfies the following, $\forall s_{0}\in \mathcal{S}$, there exists a $\epsilon >0$, such that $\forall x$ satisfying $\|x-x_{0}\|\leq \epsilon$, $A(x)=A(x_0)$

Note that, the assumptions B1-B3, share a similar essence to that of the Assumptions 2.1-2.3 in \citet{hernandez1992discrete} and B1-B3 in \citet{sootla2022saute} but suited for a discrete action space. In particular, Assumption B3, is similar to the lower semi continuity assumption on the set-value function map $A(s)$ taken in \citet{sootla2022saute,hernandez1992discrete} but modified for a discrete action space. 


Our first goal is to recreate \citet[Lemma 2.7]{hernandez1992discrete} for our discrete action setting. Let $\Pi$ denote the set of functions from $S\to A$.

\begin{lemma}\label{lemma:properties}
    \begin{itemize}
    \item[(a)] If Assumption B3 holds and $v(s,a)$ is l.s.c. w.r.t. $s$ for any given $a\in A(s)$ and bounded from below on $K$, then the function
    \[
    v^*(s) := \inf_{a \in A(s)} v(s, a)
    \]
    belongs to $L(S)$ and, furthermore, there is a function $\pi \in \Pi$ such that
    \[
    v^*(s) = v(s, \pi(s)) \quad \forall s \in S.
    \]
    
    \item[(b)] If the Assumptions B1-B3 hold, and $u \in L(S)$ is nonnegative, then the (nonnegative) function
    \[
    u^{*}(s) := \inf_{a \in A} \left[ c(s, a) + \int_{S} u(y) P(dy \mid s, a) \right]
    \]
    belongs to $L(S)$, and there exists $\pi \in \Pi$ such that
    \[
    u^*(s) = c(s, \pi(s)) + \int_{S} u(y) P(dy \mid s, \pi(s)) \quad \forall s \in S.
    \]

    \item[(c)] For each $n = 0, 1, \dots$, let $v_n$ be a l.s.c. function, bounded from below. If $v_n \to v_0$ as $n \to \infty$, then
    \[
    \lim_{n \to \infty} \inf_{a \in A(s)} v_n(s, a) = \inf_{a \in A(s)} v_0(s, a) \quad \forall s \in S.
    \]
\end{itemize}
\end{lemma}
\begin{proof}
For part a)

We have $v(s,a)$ is l.s.c. w.r.t. $s$ for any given $a$. This implies from the definition of lower semi -continuity for any $s_{0}\in S$ and $a\in A(s_{0})$, if $v(s_{0},a)>y$, then there exists a $\epsilon>0$, s.t. $\forall s$ satisfying $\|s-s_{0}\|\leq \epsilon$, $v(s,a)>y$. 

Assume for some  $s_{0}\in S$ and $y$, the function $\inf_{a\in A(s_{0})} v(s_0, a)$ satisfies, 
\begin{equation}
     \inf_{a\in A(s_{0})} v(s_0, a)>y \quad \Rightarrow  v(s_0, a)>y \quad \forall a\in A(s_0)
\end{equation}
  Using Assumption B3, we have $A(s)=A(s_0)$ for $\|s-s_{0}\|\leq \epsilon$. Moreover, using the fact that $v(s,a)$ is l.s.c. at a given $a$, we have if $v(s_0, a)>y$, $\forall a\in A(s_0)$  we have,
\begin{equation}
     v(s, a)>y \quad \forall \|s-s_{0}\|\leq \epsilon, \forall a\in A(s)
\end{equation}
Since, this holds for all $ a\in A(s)$, it also holds for 
     \begin{equation}
     \inf_{a\in A(s)} v(s, a)>y \quad \forall \|s-s_{0}\|\leq \epsilon
\end{equation}

This proves the lower semi continuity of $v^{*}(s)=\inf_{a\in A(s)} v(s, a)$. Further, due to the discrete nature of $A(s)$, the $\inf_{a\in A(s)} v(s, a)$ is always attained by an action $\pi(s)\in A(s)$. Hence, there exists a function $\Pi:S->A(s)$ s.t.
\begin{equation}
     v^*(s) = \inf_{a\in A(s)} v(s, a)= v(s, \pi(s)) \quad \forall s \in S.
\end{equation}

For part b), note that $c(s, a) + \int u(y) P(dy \mid s, a) $ is l.s.c. w.r.t. $s$ for an given $a$, based on Assumptions B1-B2. Hence, using part a) we have, 
\begin{equation}
     u^{*}(s) := \inf_{a \in A} \left[ c(s, a) + \int_{S} u(y) P(dy \mid s, a) \right]=c(s, \pi(s)) + \int_{S} u(y) P(dy \mid s, \pi(s))\in L(S) \quad \forall s \in S
\end{equation}
for some $f\in \Pi$. 

For part c), we begin by defining $l(s)=lim_{n\to \infty}\inf_{a\in A(s)}v_n(s,a)$. Note that, since $\{v_n\}$ is an increasing sequence, we have for any $n$
\begin{equation}
    \inf_{a\in A(s)}v_n(s,a) \leq \inf_{a\in A(s)}v_0(s,a)
\end{equation}
This implies,
\begin{equation}
   l(s) \leq \inf_{a\in A(s)}v_0(s,a)=v_0^{*}(s)
\end{equation}
Next, we define for any $s\in S$, 
\begin{equation}
    A_n:=\{a\in A(s)|v_n(s,a)\leq v_0^*(s)\}
\end{equation}
We note that $A_n$ are compact sets as $A$ is finite and discrete. Further, note that
$A_n$ is a decreasing sequence converging to $A_0$ (compact, decreasing and bounded from below by $A_0$). Also, note that
\begin{equation}
    A_1 \supset A_2 \supset A_3 \cdots \supset A_0
\end{equation}
We consider the sequence $\{a_n\}$ where $a_n\in A_n$ and $a_n$ satisfies,
\begin{equation}
    v_n(s,a_n)=\inf_{a\in A(s)}v_n(s,a)\leq \inf_{a\in A(s)}v_0(s,a)\leq v_{0}^{*}(s)
\end{equation}
This sequence $\{a_n\}$ belongs to the compact space $\cup_{n=1}^{\infty}A_n=A_1$, hence it has convergent subsequence $\{a_{n_i}\}$ converging to  $\cup_{n=1}^{\infty}A_n=A_1$. 
\begin{align}
    a_{n_i}&\in A_{n_i}=\cap_{n\leq n_i}A_n\\
    a_{0}&\in \cap_{n\leq \infty }A_n =A_0
\end{align}
 Since, the converging sequence $a_{n_i}\to a_0 $ belongs to the discrete, compact space, there exisits a $N_i$, such that for all  $n_i \geq N_i$, $a_{n_i}= a_0$. Further, using the increasing nature of $v_n$, we have,
\begin{equation}
    v_{n_i}(s,a_{n_i})\geq v_n(s, a_{n_i}) \quad \forall n_i \geq n
\end{equation}
As $i\to\infty$, this implies,  
\begin{align}
    \lim_{i\to\infty } v_{n_i}(s,a_{n_i})\geq v_n(s, a_{0}) \\
    \lim_{i\to\infty } \inf_{a\in A(s)} v_{n_i}(s,a)\geq v_n(s, a_{0})\\
    l(s)\geq v_n(s,a_0)
\end{align}
As $v_n \to v_0$, $l(s)\geq v_0(s,a_0)=v_0^{*}(s)$. 

\end{proof}

\subsection{Optimality Equation}
Next, we characterize the solution to the Bellman (Optimality) equation. We begin by recalling the Bellman operator:
\[
T v(s) = \min_{a \in A(s)} \left\{ c(s, a) + \gamma \int v(y) P(dy \mid s, a) \right\}.
\]
To state our next result we introduce some notation: Let \( L(S)^+ \) be the class of nonnegative and l.s.c. functions on \( S \), and for each \( u \in L(S)^+ \) by \Cref{lemma:properties}(b), the operator \( T \) maps \( L(S)^+ \) into itself. We also consider the sequence \( \{v_n\} \) of value iteration (VI) functions defined recursively by
\[
v_0(S) := 0, \quad \text{and} \quad v_h := T v_{h-1} \quad \text{for} \quad h = 1, 2, \dots
\]
That is, for \( h \geq 1 \) and \( s \in S \),
\[
v_h(s) := \min_{a \in \mathcal{A}(s)} \left( c(s, a) + a \int_{S} v_{h-1}(y) P(dy \mid s, a) \right). \tag{4.3}
\]
Note that, by induction and \Cref{lemma:properties}(b) again, \( v_h \in L(S)^+ \) for all \( h \geq 0 \). From elementary Dynamic Programming \citet{bertsekas1987dynamic,bertsekas1996stochastic,dynkin1979controlled}, \( v_h(s) \) is the optimal cost function for an \( h \)-stage problem (with "terminal cost" \( v_0(s) = 0 \)) given \( s_0 = s \); i.e.,
\[
v_h(s) = \inf_{\pi } V_h(\pi, s), 
\]

where, $\Pi$ is the set of policies and $V_H(\pi, s)$ denotes the value function for the $H-$stage problem:
\[
V_H(\pi, s_0) = \mathbb{E}_{\pi}\left[ \sum_{h=0}^{H-1} \gamma^h c(s_h, a_h) \right].
\]
Here, \( \mathbb{E}_{\pi} \) stands for the expectation with actions sampled according to the policy \( \pi \) and the transitions \( P \). For $H\to \infty$, let the value functions be denoted as follows:
\[
V(\pi, s_0) = \mathbb{E}_{\pi}\left[ \sum_{h=0}^{\infty} \gamma^h c(s_h, a_h) \right],
\]
and
\[
V^*(s) = \inf_{\pi} V(\pi, s).
\]


We want to prove similar results to that of \citet[Theorem 4.2]{hernandez1992discrete} on the optimality of the Bellman operator, however in the discrete action setting. In particular, we want to show the following theorem

\begin{thm}\label{thm: Bellmannexistence}
Suppose that Assumptions B1-B3 hold, then:
\begin{itemize}
    \item[(a)] \( v_h \to V^* \); hence
    \item[(b)] \( V^* \) is the minimal pointwise function in \( L(S)^+ \) that satisfies
    \[
    V^* = T V^*
    \]
\end{itemize}
\end{thm}



\begin{proof}
We follow a similar proof strategy to that of \citet[Theorem 4.2]{hernandez1992discrete}. 

To begin, note that the operator \( T \) is monotone on \( L(S)^+ \), i.e., \( u > v \) implies \( T u > T v \). Hence \( \{v_h\} \) forms a nondecreasing sequence in \( L(S)^+ \) and, therefore, there exists a function \( u \in L(S)^+ \) such that \( v_h \to u \). This implies (by the Monotone Convergence Theorem) that
\[
c(s, a) + a \int v_{h-1}(y) P(dy \mid s, a) \to c(s, a) + a \int u(y) P(dy \mid s, a),
\]
Using \Cref{lemma:properties}(c), and $v_h=\inf_{a\in A(s)}\{c(s, a) + a \int v_{h-1}(y) P(dy \mid s, a)\}$ yields
\begin{align*}
\lim_{h\to\infty}\inf_{a\in A(s)}\{c(s, a) + a \int v_{h-1}(y) P(dy \mid s, a)\} &= \inf_{a\in A(s)}\{c(s, a) + a \int u(y) P(dy \mid s, a)\},\\
\lim_{h\to\infty}v_h &= Tu,\\
u &= T u.
\end{align*}
This shows  \( v_h \to u \), such that \( u \in L(S)^+ \) satisfies the Optimality equation. 



Next, we want to show \( u = V^* \). Using that \( u \geq T u \), and by \Cref{lemma:properties}(b), we have that there exists \( \pi \in \Pi \), a stationary policy that satisfies
\begin{align}
    u(s) &\geq \inf_{a\in A(s)}\{c(s, a) + a \int u(y) P(dy \mid s, a)\} \quad \forall s\\
    &\geq c(x, \pi) + \alpha \int u(y) P(dy \mid s, \pi) \quad \forall s.
\end{align}

Applying the $T$ operator iteratively, we have

\begin{align}
    u(s)&\geq T^{H}u(s) \quad \forall s,H\\
&\geq  \E_{\{s_h\},\pi}[\sum_{h=0}^{H-1} \alpha^h  c(s_h, \pi)] + \alpha^{H} \int u(y) P^{H}(dy \mid s, \pi) \quad \forall s,H,
\end{align}

where \( P^H(B \mid s, \pi) = P(\{ s_H \in B \}) \) denotes the \( H \)-step transition probability of the Markov chain \( \{ s_h \} \) (see \citet[Remarks 3.1,3.2]{hernandez1992discrete}).  Therefore, since \( u \) is nonnegative,

\begin{align}
    u(s)&\geq  \E_{\{s_h\},\pi}[\sum_{h=0}^{H-1} \alpha^h  c(s_h, \pi)] \quad \forall s,H,
\end{align}
Letting \( H \to \infty \), we obtain
\[
u(s) \geq V(\pi, s) \geq V^{*}(s) \quad \forall s.
\]
Next, note that, 

\begin{align}
    v_h(s) &= \inf_{\pi \in \Pi} V_h(\pi, s)\\
    &\leq V_h(\pi, s) \quad \forall s,h,\pi
\end{align}
and letting \( h \to \infty \), we get
\[
u(s) \leq V(\pi, s) \quad \forall s,\pi.
\]
This implies \( u(s) \leq V^*(s) \). We have thus shown that \( u = V^* \).


Further, if there is another solution $u'$ satisfying $u'=Tu'$, it holds that $u'\geq V^{*}$, Hence, $V^{*}$ is the minimal solution.

\end{proof}


\subsection{Limit of a sequence of MDPs}

Consider now a sequence of Markov Decision Processes (MDPs) \( M_n = \{ S, A, P, c_n, \gamma_n \} \), where, without loss of generality, we write \( c_n, c_\infty \) and \( M_n, M_\infty \). Consider now a sequence of value functions \( \{ V_n^* \}_{n=0}^{\infty} \):
\[
V_n(\pi, s_0) = \mathbb{E}^\pi \left[ \sum_{t=0}^{\infty} \gamma^t c_n(s_t, a_t) \right],
\]
\[
V_n^*(s) = \inf_{\pi} V_n(\pi, s).
\]

The "limit" value functions (with \( n = \infty \)) are still denoted as follows:
\[
V(\pi, s_0) = \mathbb{E}^\pi \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) \right],
\]
\[
V^*(s) = \inf_{\pi} V(\pi, s).
\]

We also define the sequence of Bellman operators
\[
T_n v(s) = \min_{a \in A(s)} \left\{ c_n(s, a) + \gamma \int v(y) P(dy \mid s, a) \right\},
\]
\[
T v(s) = \min_{a \in A(s)} \left\{ c(s, a) + \gamma \int v(y) P(dy \mid s, a) \right\}.
\]

In addition to the previous assumptions, we make an additional one, while modifying Assumption B1:
\begin{itemize}
    \item[\textbf{B1'}] For each \( n \), the functions \( c_n(s, a) \) are bounded, measurable on \( \mathcal{K} \), nonnegative, lower semi-continuous;
    \item[\textbf{B4}] The sequence \( \{ c_n(s, a) \}_{n=0}^{\infty} \) is such that \( c_n \uparrow c \).
\end{itemize}
 


For each \( n \), the optimal cost function \( V^*(s) \) is the bounded function in \( L(S)^+ \) that satisfies the Optimality equation in \Cref{thm: Bellmannexistence} :
\[
V_n^* = T_n V_n^*,
\]

\begin{thm}\label{thm: value-convergence}
    The sequence \( V_n^* \) is monotone increasing and converges to \( V^* \).
   
\end{thm}

\begin{proof} 

We follow a similar proof strategy to that of \citet[Theorem 5.1]{hernandez1992discrete}

 To begin with, note that since \( c_n \uparrow c \), it is clear that \( V_n^{*} \) is an increasing sequence in \( L(S)^+ \), and therefore, there exists a function \( u \in L(S)^+ \) such that \( V_n^* \to  u \).

Moreover, from \Cref{lemma:properties}(c), letting \( n \to \infty \), we see that \( u = T u \), i.e., \( u \) satisfies the optimality equation. This implies that \( u \geq V^* \), since, by Theorem 4, \( V^* \) is the minimal solution in \( L(X)^+ \) to the optimality equation.

On the other hand, it is clear that \( V_n^{*} \leq V^* \) for all \( n \), so that \( u \leq V^* \). Thus \( u = V^* \), i.e., \( U^* = V^* \).



\end{proof}

\subsection{Latent MDP Analysis}\label{sec: equi}

\thmsauteequivalence*
\begin{proof}


We begin by comparing our assumptions to that of the assumptions B1-B4, closely aligned to those used in \citet{hernandez1992discrete,sootla2022saute}.

To prove a),b) of \Cref{thm:sauteequivalence} we need to verify that the latent MDP satisfying Assumptions A1-A2 also satisfies Assumptions B1', B2-B4. According to Assumption A1, we consider bounded costs $\bar{\mathcal{C}}^{n}_{\text{task}}$ continuous w.r.t. state $(\mathbf{h},\mathbf{o},\text{z})$ for a given $y$ with discrete and finite action space $\gV$, hence Assumptions B1', B3, and B4 are satisfied. Assumptions B2 and A2 are identical. This proves a) and b).


For c), note that the state value function $V(\cdot)$ and latent space value function $\bar{V}(\cdot)$ w.r.t. policy $\bar{\pi}:\bar{\gS}\rightarrow\gV$ that acts on the latent space directly and on the original space as $\bar{\pi}(\phi(\cdot)):\gS\rightarrow\gV$ are related as follows:
\begin{align}
     &V(\bar{\pi}(\phi(\cdot)), \mathbf{s}_0,\text{z}_0)\\&= \E_{\substack{\mathbf{s}_{t+1},\text{z}_{t+1}\sim \gP(\cdot|\mathbf{s}_t,\text{z}_t,\text{y}_t)\\ \text{y}_{t}\sim \bar{\pi}(\cdot|\phi(\mathbf{s}_t),\text{z}_t)}}\Big[\sum_{t=0}^\infty \gamma^t \tilde{\mathcal{C}}^{n}_{\text{task}}(\mathbf{s}_t, \text{z}_{t},\text{y}_{t})\Big]\\
    &= \E_{\substack{\mathbf{s}_{t+1},\text{z}_{t+1}\sim \gP(\cdot|\mathbf{s}_t,\text{z}_t,\text{y}_t)\\ \text{y}_{t}\sim \bar{\pi}(\cdot|\phi(\mathbf{s}_t),\text{z}_t)}}\Big[\sum_{t=0}^\infty \gamma^t \bar{\mathcal{C}}^{n}_{\text{task}}(\phi(\mathbf{s}_t), \text{z}_{t},\text{y}_{t})\Big]\\
     &=\E_{\substack{\phi(\mathbf{s}_{t+1}),\text{z}_{t+1}\sim \bar{\gP}(\cdot|\phi(\mathbf{s}_t),\text{z}_t)\big)\\ \text{y}_{t}\sim \bar{\pi}(\cdot|\phi(\mathbf{s}_t),\mathbf{o}_t)}}\Big[\sum_{t=0}^\infty \gamma^t \bar{\mathcal{C}}^{n}_{\text{task}}(\phi(\mathbf{s}_t), \text{z}_{t},\text{y}_{t})\Big]\Big|\\
    &=\E_{\substack{\mathbf{h}_{t+1},\mathbf{o}_{t+1},\text{z}_{t+1}\sim\bar{\gP}(\cdot|\mathbf{h}_t,\mathbf{o}_t,\text{z}_t)\big)\\\mathbf{h}_{t+1},\mathbf{o}_{t+1}=\phi(\mathbf{s}_{t+1}) \\\text{y}_{t}\sim \bar{\pi}(\cdot|\mathbf{h}_t,\mathbf{o}_t)}}\Big[\sum_{t=0}^\infty \gamma^t \bar{\mathcal{C}}^{n}_{\text{task}}(\mathbf{h}_t,\mathbf{o}_t, \text{z}_{t},\text{y}_{t})\Big]\Big|_{\mathbf{h}_0,\mathbf{o}_0=\phi(s_0)}\\
    &=  \bar{V}(\bar{\pi}, \mathbf{h}_0,\mathbf{o}_0,z_0)\Big|_{\mathbf{h}_0,\mathbf{o}_0=\phi(s_0)}
\end{align}

Hence, we can show $\bar{\pi}_n^{*}$ is optimal for $V_n(\cdot)$ as follows:
\begin{align}
     V_n(\bar{\pi}_n^{*},\mathbf{s}, \text{z})&=\bar{V}_n(\bar{\pi}_n^{*}, \mathbf{h},\mathbf{o},\text{z})\Big|_{\mathbf{h},\mathbf{o}=\phi(s)}\\
     &=\min_{\bar{\pi}}\bar{V}_n(\bar{\pi}, \mathbf{h},\mathbf{o},\text{z})\Big|_{\mathbf{h},\mathbf{o}=\phi(s)}\\
     &=\min_{\bar{\pi}}V_n(\bar{\pi}(\phi(\cdot)), \mathbf{s},\text{z})\\
     &=\min_{\pi}V_n(\pi, \mathbf{s},\text{z})
\end{align}

Here, the minimization of $\pi$ is over set of all policies covered by $\bar{\pi}(\phi(\cdot))$ and we show that $\bar{\pi}_n^{*}(\phi(\cdot))$ is the optimal policy for the original space over this set of policies. 



\end{proof}


\thmalmostsure*
\begin{proof}
 We first note that if any trajectory with infinite cost has a finite probability, the cost would be infinite. Hence, all the trajectories with finite/positive probability have finite costs. This implies, the finite cost attained by  $\pi^\star$ w.r.t. Equation \ref{Eq:Constraints} implies the satisfaction of constraints (Equation \ref{Eq:a.s.SafeLLM-main}) almost surely (i.e. with probability 1). Combined with the fact that the policy $\pi^\star$ was obtained by minimizing the exact task cost as in Equation \ref{Eq:a.s.SafeLLM-main}, \Cref{thm:a.s.} follows.
\end{proof}


















