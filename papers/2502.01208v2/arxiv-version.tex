
\documentclass{article} % For LaTeX2e

\usepackage[dvipsnames]{xcolor}
\usepackage[accepted]{arxiv}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{subfigure}
\usepackage{thmtools}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{assume}{Assumption}[section]
\newtheorem{rem}[thm]{Remark}
\newtheorem{note}{Note}
\newtheorem{case}{Case}

\usepackage[ruled,algo2e]{algorithm2e}






\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\usepackage{mathtools}

\begin{document}


\twocolumn[
\icmltitle{Almost Surely Safe Alignment of Large Language Models at Inference-Time}


\icmlsetsymbol{equal}{*}
\icmlsetsymbol{work-done}{\dag}

\begin{icmlauthorlist}
\icmlauthor{Xiaotong Ji}{equal,work-done,yyy,icl}
\icmlauthor{Shyam Sundhar Ramesh}{equal,work-done,yyy,ucl}
\icmlauthor{Matthieu Zimmer}{equal,yyy}
\icmlauthor{Ilija Bogunovic}{ucl}
\icmlauthor{Jun Wang}{ucl}
\icmlauthor{Haitham Bou Ammar}{yyy,ucl}

\end{icmlauthorlist}

\icmlaffiliation{yyy}{Huawei Noah's Ark Lab}
\icmlaffiliation{icl}{Imperial College London}
\icmlaffiliation{ucl}{University College London}


\icmlcorrespondingauthor{Haitham Bou Ammar}{haitham.ammar@huawei.com}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsOnly{\icmlEqualContribution \icmlworkdone}

\begin{abstract}
Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation,  we propose 
\texttt{InferenceGuard}, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate \texttt{InferenceGuard} effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses. 

\textcolor{red}{ 
 Contains potentially harmful examples.}
\end{abstract}

\section{Introduction}
LLMs have demonstrated impressive capabilities across a diverse set of tasks, such as summarization \citep{koh2022empirical,stiennon2020learning}, code generation \citep{ gao2023pal,chen2021evaluating}, and embodied robotics \citep{mower2024rosllmrosframeworkembodied,Kim_2024}. However, since those models are primarily trained on vast, unsupervised datasets, their generated responses can often be biased, inaccurate, or harmful \citep{deshpande2023toxicity,ganguli2022red, weidinger2021ethical, gehman2020realtoxicityprompts}. As a result, LLMs require alignment with human values and intentions to ensure their outputs are free from controversial content.

The traditional LLM alignment approach involves fine-tuning the model on human-labeled preference data. For instance, works such as \citep{ouyang2022training, christiano2017deep} use the reinforcement learning from human feedback (RLHF) framework to fine-tune LLMs, constructing a reward model from human feedback and optimizing the model using standard reinforcement learning algorithms like PPO \citep{schulman2017proximalpolicyoptimizationalgorithms}. More recent approaches, such as \citep{tutnov2025dpossecretlyoneattempting,yin2024relativepreferenceoptimizationenhancing,rafailov2023direct}, bypass reward learning and instead align pre-trained models directly with human preferences.

RLHF alignment can be costly and risks overfitting partly since they modify the LLM's model weights. To tackle these challenges, alternative approaches adjust the model's responses directly at inference time while leaving the pre-trained model weights fixed. Several techniques have been proposed for this purpose, such as Best-of-N \citep{nakano2021webgpt,stiennon2020learning,touvron2023llama}, FUDGE \citep{yang2021fudge}, COLD \citep{qin2022cold}, CD-Q \citet{mudgal2023controlled}, RE-control \cite{kong2024aligning}, among others. Importantly, those techniques are designed modularly, allowing the alignment module to integrate seamlessly with the pre-trained model. This modularity enables flexible inference-time reconfigurability and quick adaptation to new reward models and datasets. Moreover, it reduces reliance on resource-intensive and often hard-to-stabilize RL processes inherent in the RLHF paradigm.\looseness=-1 

Despite the successes of inference-time alignment, these methods' \emph{safety aspects} have received limited attention so far. While some works have attempted to tackle those issues in inference-time-generated responses, they mainly focus on prompt-based alignment ~\citep{hua2024trustagent,zhang2024controllable}, trainable safety classifiers ~\cite{niu2024parameter,zeng2024root} or protections against adversarial attacks and jailbreaks ~\citep{dong2024attacksdefensesevaluationsllm,guo2024cold,inan2023llama}. That said, prompt-based methods cannot be guaranteed to consistently produce safe responses, as ensuring safety is heavily reliant on user intervention, requiring extensive engineering and expertise to manage the model’s output effectively. Trainable classifiers focus only on the safety of decoded responses using hidden states or virtual tokens, ignoring task alignment and lacking theoretical guarantees. Moreover, while adversarial robustness is crucial, our work focuses on the key challenge of generating inherently safe responses from the LLM.\looseness=-1


In this work, we aim to develop LLMs that generate safe responses at inference time, following a principled approach that \emph{guarantees safety almost surely, i.e., with a probability approaching one}. To do so, we reformulate the safe generation of inference-time responses as an instance of constrained Markov decision processes (cMDP). We map the cMDP to an unconstrained one through \emph{safety state augmentation}, bypassing Lagrangian approaches' limitations that struggle to balance reward maximization and safety feasibility. Focusing on a practical test-time inference algorithm, we adopt a critic-based approach to solve the augmented MDP, eliminating the need for gradients in the LLM. To ensure efficiency, we train our critic in the latent space of the LLM, keeping it small in size and fast during inference. This shift to the latent space complicates the theoretical framework, requiring extensions from \citep{hernandez1992discrete, sootla2022saute}. By doing so, we establish, for the first time, that one can guarantee almost sure safety in the original token space.

To leverage this theoretical guarantee in practice, we build upon the augmented MDP framework and introduce two novel implementations for safe inference-time alignment: one that learns a compact critic in the latent space for cases where safety costs can only be queried after the generation of complete responses and another that leverages direct cost queries for efficient inference-time optimization. Finally, we integrate these components into a lookahead algorithm (e.g., Beam Search or Blockwise Decoding \citep{mudgal2023controlled}) proposing \texttt{InferenceGuard}. Our experiments tested \texttt{InferenceGuard} starting from safety-aligned and \emph{unaligned models}. Our results achieved high safety rates—91.04\% on Alpaca-7B and 100\% on Beaver-7B-v3. Notably, this was accomplished while maintaining a strong balance with rewards, setting new state-of-the-art.




\section{Background}
\subsection{LLMs as Stochastic Dynamical Systems} \label{Sec:DynaSys}
LLMs can be viewed as stochastic dynamical systems, where the model's behavior evolves probabilistically over time, governed by its internal parameters and the inputs it receives. In this perspective, each new token is generated based on the model's evolving hidden state \citep{kong2024aligning, zimmer2024mixtureattentionsspeculativedecoding}. Formally, an LLM transitions as follows: $\left[\mathbf{h}_{t+1}, \mathbf{o}_{t+1}\right]^{\mathsf{T}} = f_{\text{LLM}}(\mathbf{h}_t, \text{y}_t)$, \text{with} $\text{y}_t \sim \text{SoftMax}(\mathbf{W}\mathbf{o}_t)$. Here, $f_{\text{LLM}}(\cdot)$, denotes the aggregation of all decoding layers, $\text{y}_t$ a generated token at each time step $t$, and $\mathbf{o}_t$ the logits which are linearly mapped by $\mathbf{W}$ to produce a probability distribution over the vocabulary space. Moreover, $\mathbf{h}_t$ comprises all key-value pairs accumulated from previous time steps \footnote{Note, $\mathbf{h}_{t} = \left\{\mathbf{K}^{(l)}_{j},\mathbf{V}_{j}^{(l)}\right\}_{l=1}^{L}$ for $j:[1:t]$, i.e., keys and values from all layers accumulated up to the previous time step.}. The system evolves until the end-of-sequence (\texttt{EOS}) token is reached.  
\subsection{Test-Time Alignment of LLMs} \label{Sec:TTT}
Test-time alignment ensures that a pre-trained LLM generates outputs consistent with desired behaviors by solving a MDP, whose initial state is determined by the test prompt. Rewards/costs for alignment can come from various sources, including but not limited to human feedback \citep{tutnov2025dpossecretlyoneattempting,zhong2024dpo}, environmental feedback from the task or verifiers \citep{zeng2023large,yang2024leandojo,trinh2024solving,an2024learn,liang2024learning,mower2024rosllmrosframeworkembodied}, or pre-trained reward models \citep{wang2024math,zhang2024rest,li2025enhancing}. 

Several approaches have been developed to address the challenge of test-time alignment. For instance, beam search with rewards \citep{choo2022simulation} extends traditional beam search technique by integrating a reward signal to guide LLM decoding at test time. Monte Carlo tree search, on the other hand, takes a more exploratory approach by simulating potential future token sequences to find the path that maximizes the reward function \citep{zhang2024rest}. Best-of-N (BoN) generates multiple candidate sequences and selects the one with the highest reward ~\citep{stiennon2020learning,nakano2021webgpt,touvron2023llama}. 

We focus on beam search and look-ahead methods for more scalability when developing \texttt{InferenceGuard}.

\begin{figure*}[hbt!]
\centering

\includegraphics[trim=2em 0.1em 2em 2em,clip=true,width=\textwidth]{imgs/fig_1.pdf} 
\label{fig:flow}
\vspace{-1.2em}
\caption{To aid readability, this figure summarizes the key transitions and notations used throughout the paper. }
\vspace{-1.2em}
\end{figure*}

\section{Safe Test-Time Alignment of LLMs }
We frame the problem of safely aligning LLMs at test time as a constrained Markov decision process (cMDP). As noted in \cite{achiam2017constrained,sootla2022saute} a cMDP is defined as the following tuple: $\mathcal{M} =\left\langle \mathcal{S}, \mathcal{A}, \mathcal{C}_{\text{task}}, \mathcal{C}_{\text{safety}}, \mathcal{P}, \gamma  \right\rangle$, with $\mathcal{S}$ and $\mathcal{A}$ denoting the state and action spaces, respectively. The cost function $\mathcal{C}_{\text{task}}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ dictates the task's cost\footnote{We define costs as negative rewards, which transforms the problem into an equivalent cost-based MDP.}, while $\mathcal{C}_{\text{safety}}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ represents a \emph{safety} cost, which encodes the constraints that the actor must satisfy during inference. The transition model $\mathcal{P}:\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ captures the probability of transitioning to a new state given the current state and action. Meanwhile, the discount factor $\gamma \in [0,1)$ trades off immediate versus long-term rewards. The goal of constrained MDPs is to find a policy $\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$ that minimizes the task's cost while simultaneously satisfying the safety constraints. Given a safety budget d, we write: 
\begin{align}
    \label{Eq:cMDPs} 
    \min_{\pi} \  &\mathbb{E}_{\mathcal{P},\pi}\Big[\sum_{t}\gamma^t\mathcal{C}_{\text{task}}(\mathbf{s}_t, \mathbf{a}_t)\Big]  \\ \nonumber
   \text{s.t.} \ \     &\mathbb{E}_{\mathcal{P},\pi}\Big[\sum_{t}\gamma^t\mathcal{C}_{\text{safety}}(\mathbf{s}_t, \mathbf{a}_t)\Big] \leq d,
\end{align}


\subsection{Safe Test-Time Alignment as cMDPs}
We treat the generation process of safe test-time alignment of LLMs as the solution to a specific cMDP. We introduce our state variable $\mathbf{s}_{t} = \{\mathbf{x},\mathbf{y}_{<t}\}$, which combines the input prompt $\mathbf{x}$ with the tokens (or partial responses) decoded until step $t$. Our policy generates a new token $\text{y}_t$ that we treat as an action in the model's decision-making process. The transition function 
$\mathcal{P}$ of our MDP is deterministic, where the state $\mathbf{s}_t$ is updated by incorporating the generated action $\text{y}_t$, i.e., $\mathbf{s}_{t+1} = \mathbf{s}_t \oplus \text{y}_{t}=\{\mathbf{x},\mathbf{y}_{\leq t}\}$. We also assume the existence of two cost functions $\mathcal{C}_{\text{task}}$ and $\mathcal{C}_{\text{safety}}$ to assess the correctness and safety of the LLM's responses. As described in Section \ref{Sec:TTT}, those functions can originate from various sources, such as human feedback, environmental verifiers, or pre-trained models. While we conduct experiments with these functions being LLMs (see Section \ref{Sec:Exp}), our method can be equally applied across various types of task and safety signals. 

Regarding the task's cost, we assume the availability of a function $\mathcal{C}_{\text{task}}$ that evaluates the alignment of the LLM with the given task. This function assigns costs to the partial response based on the input prompt $\mathbf{x}$ such that: 
\vspace{-0.1em}
\begin{equation}\label{eq: non-safe reward-21}
\mathcal{C}_{\text{task}}([\mathbf{x}, \mathbf{y}_{\leq t}]) :=
\begin{cases} 
0 & \text{if } \text{y}_t \neq \texttt{EOS} \\
c_{\text{task}}([\mathbf{x}, \mathbf{y}_{\leq t}]) & \text{if } \text{y}_t = \texttt{EOS}
\end{cases}
\end{equation}

For the safety cost $\mathcal{C}_{\text{safety}}$, we assume the function assigns non-zero costs to any partial answer without waiting for the final token. This is crucial because we want to flag unsafe responses early rather than waiting until the end of the generation process. Many pre-trained models are available for this purpose on Hugging Face, which we can leverage—more details can be found in Section \ref{Sec:Exp}. With this, we write safe test-time alignment as an instance of Equation \ref{Eq:cMDPs}: 
\vspace{-1em}
\begin{align}
\label{Eq:SafeLLM}
    \min_{\pi} \ &\mathbb{E}_{\pi}\Big[\sum_t \gamma^t \mathcal{C}_{\text{task}}(\overbrace{\{\mathbf{x},\mathbf{y}_{<t}\}}^{\mathbf{s}_t}, \text{y}_t)\Big] \\ \nonumber 
     \ \text{s.t.} \ &\mathbb{E}_{\pi}\Big[\sum_t \gamma^t \mathcal{C}_{\text{safe}}(\{\mathbf{x}, \mathbf{y}_{<t}\}, \text{y}_t)\Big] \leq d.
\end{align}
The above objective aims to minimize the task cost $\mathcal{C}_{\text{task}}$ while ensuring that the safety cost does not exceed a predefined budget $d$. The expectation is taken over the actions (tokens) generated at each timestep. 




\subsection{State Augmented Safe Inference-Time Alignment}\label{sec: augment}
We could technically use off-the-shelf algorithms to solve Equation \ref{Eq:SafeLLM}, such as applying a Lagrangian approach as proposed in \cite{dai2023safe}. However, there are two main issues with using these standard algorithms. First, they generally require gradients in the model itself—specifically, the LLM—which we want to avoid since our goal is to perform inference-time alignment without retraining the model. Second, these methods rely on a tunable Lagrangian multiplier, making it challenging to maximize rewards while satisfying almost sure constraints optimally. 


Instead of using a Lagrangian approach, we take a different direction by augmenting the state space and extending the method proposed by \citep{sootla2022saute} to large language models. In our approach, we augment the state space of the constrained MDP with a ``constraint tracker'', effectively transforming the problem into an unconstrained one. This allows us to apply Bellman equations and conduct rigorous proofs with almost sure constraint satisfaction results. However, applying the techniques and proofs from \citep{sootla2022saute} to our test-time setting is not entirely straightforward due to two main challenges: first, the differences in the constrained MDP setting, and second, the process by which we train critics, as we will demonstrate next.


\textbf{Augmenting the State Space.} 
The following exposition builds on \citep{sootla2022saute}, extending their method to address LLM-specific challengers, an area they did not cover. The core idea is to transform the constrained MDP into an unconstrained one by augmenting the state with an additional variable that tracks the remaining budget of the constraint. While doing so, we must ensure that: \textbf{PI)} our augmented state variable tracks the constraints and maintains the \emph{Markovian} nature of transition dynamics; and \textbf{PII)} our task cost $\mathcal{C}_{\text{task}}$ accounts for this new state representation and is correctly computed w.r.t. the augmented state.\looseness=-1


To solve \textbf{PI}, we investigate the evolution of the constraint in Equation \ref{Eq:SafeLLM} and track a scaled-version of the remaining safety budget $\mathbf{\omega}_t = d -\sum_{k=1}^{t} \gamma^{k}\mathcal{C}_{\text{safety}}(\{\mathbf{x},\mathbf{y}_{<k}\},\text{y}_k)$ that is defined as $\text{z}_t = \mathbf{\omega}_{t-1}/\gamma^{t}$. The update of $\text{z}_t$ satisfies: 
\begin{align}
\label{Eq:Z}
    \text{z}_{t+1} &= (\mathbf{\omega}_{t-1} -\gamma^t\mathcal{C}_{\text{safety}}(\{\mathbf{x},\mathbf{y}_{<t}\},\text{y}_t))/\gamma^{t+1} \\ \nonumber
    &=(\text{z}_t - \mathcal{C}_{\text{safety}}(\{\mathbf{x},\mathbf{y}_{<t}\},\text{y}_t))/\gamma,  \ \ \text{with $\text{z}_0 = d$.}
\end{align}
Since the dynamics of $\text{z}_t$ are Markovian due to the dependence of  $\text{z}_{t+1}$ only  on $\text{z}_t$, $\text{y}_t$ and current the state $\{\mathbf{x}, \mathbf{y}_{<t}\}$, we can easily augment our original state space with $\text{z}_t$, such that $\tilde{\mathbf{s}}_t=[\mathbf{s}_t, \text{z}_t]=[\{\mathbf{x}, \mathbf{y}_{<t}\}, \text{z}_t]$. The original dynamics can also be redefined to accommodate for $\tilde{\mathbf{s}}_t$: 
\begin{align*}
\tilde{\mathbf{s}}_{t+1} = [\overbrace{\{\mathbf{x},\mathbf{y}_{<t}\}\oplus \text{y}_t}^{\text{original transition}}, \text{z}_{t+1}], \ \ \text{with $\text{z}_{t+1}$ as in Eq. \ref{Eq:Z}.}   
\end{align*}

Concerning $\textbf{PII}$, we note that enforcing the original constraint in Equation \ref{Eq:SafeLLM} is equivalent to enforcing an infinite number of the following constraints: 
\begin{equation}
\label{Eq:infConst}
    \sum_{k=0}^{t} \gamma^{k} \mathcal{C}_{\text{safety}}(\{\mathbf{x},\mathbf{y}_{<k}\},\text{y}_k) \leq d \ \ \forall t \geq 1.
\end{equation}
As noted in \citep{sootla2022saute}, this observation holds when the instantaneous costs are nonnegative, ensuring that the accumulated safety cost cannot decrease. 
In our case, it is natural to assume that the costs are nonnegative for LLMs, as safety violations or misalignments in the output typically incur a penalty, reflecting the negative impact on the model's performance or ethical standards. 

Clearly, if we enforce  $\text{z}_{t} \geq 0$ for all $t \geq 
0$, we automatically get that $\mathbf{\omega}_t = d -\sum_{k=0}^{t} \gamma^{k}\mathcal{C}_{\text{safety}}(\{\mathbf{x},\mathbf{y}_{<k}\},\text{y}_k)\geq 0$ for all $t \geq 0$, thus satisfying the infinite  constraints in Equation \ref{Eq:infConst}. We can do so by reshaping the tasks's instantaneous cost to account for the safety constraints: 
\begin{equation}\label{eq: safe reward}
\tilde{\mathcal{C}}_{\text{task}}^{\infty}(\tilde{\mathbf{s}}_t, \text{y}_{t}) :=
\begin{cases} 
\mathcal{C}_{\text{task}}([\mathbf{x}, \mathbf{y_{\leq t}}]) &  \text{z}_{t} > 0\\
+\infty & \ \text{z}_{t}\leq 0,  
\end{cases}
\end{equation}
with $\mathcal{C}_{\text{task}}([\mathbf{x}, \mathbf{y_{\leq t}}])$ representing the original MDP's task cost function as described in Equation \ref{eq: non-safe reward-21}. Of course, in practice, we avoid working with infinities and replace $\tilde{\mathcal{C}}_{\text{task}}^{\infty}$ with $\tilde{\mathcal{C}}_{\text{task}}^{n}$ for a big $n>0$\footnote{Note that the introduction of $n$ instead of $+\infty$ requires additional theoretical justifications to ensure constraint satisfaction of the true augmented MDP. We carefully handle this in Section \ref{Sec:Theory}.}. We can now reformulate the constrained problem into an \emph{unconstrained one} as follows:

\begin{equation}
\label{Eq:Constraints}
    \min_{\pi} \mathbb{E}_{\pi}\Big[\sum_{t}\gamma^t \tilde{\mathcal{C}}_{\text{task}}^{\infty}(\tilde{\mathbf{s}}_t, \text{y}_{t})\Big].
\end{equation}
Using gradient-based techniques, one could optimize the augmented MDP in Equation \ref{Eq:Constraints}. However, since our goal is to enable safety at test time without retraining, we adopt a critic-based approach that does not require gradients during inference, as we show next. 

\section{\texttt{InferenceGuard}: Safety at Test-Time}
When designing our critic, we considered several crucial factors for test-time inference. These included its size, ease of training for quick adaptation, and flexibility to operate in real-time without significant latency. As such, we chose to train the critic in the latent space of the LLM rather than directly in the textual space, enabling a more efficient solution that meets the constraints of test-time alignment.

Even if we train the critic in the latent space, the question of what inputs to provide remains. Fortunately, the works of \citep{kong2024aligning, zimmer2024mixtureattentionsspeculativedecoding} demonstrated that LLMs can be viewed as dynamical systems, where $\mathbf{h}_t$ (hidden state) and $\mathbf{o}_t$ (logits) serve as state variables that capture sufficient statistics to predict the evolution of the LLM and the generation of new tokens; see Section \ref{Sec:DynaSys} for more details. Those results made $\mathbf{h}_t$ and $\mathbf{o}_t$ ideal inputs for our critic\footnote{In our implementation, we set the variable for the first input to our critic $\mathbf{h}_t = \texttt{llm-outputs}.\texttt{past-key-values}(\mathbf{x},\mathbf{y}_{<t})$ and $\mathbf{o}_t = \texttt{llm-outputs.hidden-states}(\mathbf{x},\mathbf{y}_{<t})\text{[-1]}$.}, as they encapsulate the relevant information for evaluating the model's behavior during test-time alignment while being relatively low-dimensional, reducing the size of our critic's deep network.

To fully define our critic, we require a representation of the embedding of our \emph{augmented state} $\tilde{\mathbf{s}}_{t}=[\mathbf{s}_t, \text{z}_t]$ within the latent space. As noted above, we can acquire $(\mathbf{h}_t, \mathbf{o}_t)$ from the transformer architecture. We call this mapping $\phi$, whereby $(\mathbf{h}_t, \mathbf{o}_t) = \phi(\{\mathbf{x},\mathbf{y}_{<t}\})$. Furthermore, we use an identity mapping to embed $\text{z}_t$, which enables us to input the actual tracking of the constraints directly to the critic without any loss of information. 


\subsection{Theoretical Insights} \label{Sec:Theory}
In this section, we show that optimizing in the latent space preserves safety constraints in the original token space, and we prove that our approach guarantees almost sure safety. 

We should answer two essential questions: \textit{i)} Can we still compute an optimal policy in the latent space? and \textit{ii)} If we enforce safety constraints in the latent space, do they still hold in the \emph{original token space?} While the prior work in \citep{sootla2022saute} established theoretical results for safety-augmented MDPs in standard (non-LLM) RL settings, their work does not address how guarantees in the latent space translate to the original token space. To handle those problems, we extend the theorems from \cite{hernandez1992discrete,sootla2022saute} to ensure the following three properties: 
\begin{itemize}
\setlength{\itemsep}{3pt}  
  \setlength{\parskip}{0pt}  
    \item \textbf{Prop I)} The latent MDP indeed satisfies the Bellman equations (Theorem \ref{thm:sauteequivalence} (a)) and, hence, allows us to compute an optimal policy in this space,
    \item \textbf{Prop II)} Policies and value functions in the latent space are valid in the original token space, implying that optimizing in the latent space preserves the constraints in original token space (Theorem \ref{thm:sauteequivalence} (b,c)) \looseness=-1
    \item \textbf{Prop III)} The resulting policy satisfies safety constraints almost surely (Theorem \ref{thm:a.s.}), meaning if a policy is safe in the latent and original token space with finite expected cost w.r.t. Equation \ref{Eq:Constraints}, it is also almost surely safe in the actual LLM token space.
\end{itemize}

We begin by defining the latent space MDP's cost and transition function: 

\begin{definition}\label{def: c-p}
	$\exists \phi(\cdot)$ and functions $\bar{\mathcal{C}}^{n}_{\text{task}}$ and $\bar{\mathcal{P}}$ such that: 
    \vspace{-0.1em}
    \begin{align*}
        &\bar{\mathcal{C}
        }_{\text{task}}^{n}(\overbracket{\phi(\{\mathbf{x},\mathbf{y}_{<t}\}),\text{z}_t}^{\text{embedded aug. state}},\text{y}_t)= \tilde{\mathcal{C}}^{n}_{\text{task}}(\overbracket{\{\mathbf{x},\mathbf{y}_{<t}\},\text{z}_t}^{\text{augmented state}}, \overset{\text{action}}{\overset{\big\uparrow}{\text{y}_t}})\\ 
        &\bar{\mathcal{P}}(\phi(\{\mathbf{x},\mathbf{y}_{\leq t}\}),\text{z}_{t+1}|\phi(\{\mathbf{x},\mathbf{y}_{<t}\}),\text{z}_t, \text{y}_t)= {\mathcal{P}}(\tilde{\mathbf{s}}_{t+1}|\tilde{\mathbf{s}}_{t}, \text{y}_t),
    \end{align*}
    
    where $\tilde{\mathbf{s}}_{t}$ is the augmented state in the original token space.\looseness=-1
\end{definition}



According to Definition \ref{def: c-p}, we ensure that the cost incurred by the augmented state $\tilde{\mathbf{s}}_{t}=[\{\mathbf{x},\mathbf{y}_{<t}\},\text{z}_t]$ w.r.t. $\tilde{\mathcal{C}}^{n}_{\text{task}}$ is equal to the latent cost incurred by the latent state $[\phi(\{\mathbf{x},\mathbf{y}_{<t}\}),\text{z}_t]$ w.r.t $\bar{\mathcal{C}}_{\text{task}}^{n}$. Additionally, we ensure that the transition dynamics of the augmented state in the original token space and the corresponding latent state in the latent space are equivalent.

This equivalence enables us to derive an optimal policy for the latent MDP and apply it to minimize the cost objective in the original augmented MDP (see Equation \ref{Eq:Constraints}). We proceed to analyze the existence of such an optimal policy in the latent space through the following \emph{standard assumptions} \citep{sootla2022saute} on $\bar{\mathcal{C}}^{n}_{\text{task}}$, and $\bar{\mathcal{P}}$: \textbf{A1.} The function $\bar{\mathcal{C}
        }_{\text{task}}^{n}(\mathbf{h},\mathbf{o},\text{z},\text{y})$ is bounded, measurable, nonnegative, lower semi-continuous w.r.t. $(\mathbf{h},\mathbf{o},\text{z})$ for a given $\text{y}$, and
\textbf{A2.} The transition law $\bar{\gP}$ is weakly continuous for any $\text{y}$.  

Next, we define $\bar{\pi}$ as a policy in the latent space that maps $(\mathbf{h},\mathbf{o},\text{z})\rightarrow \text{y}$, and its value function for an initial state $(\mathbf{h}_0,\mathbf{o}_0,\text{z}_0)$ as follows: $\bar{V}^{n}(\bar{\pi}, \mathbf{h}_0,\mathbf{o}_0,\text{z}_0) =   \E_{\bar{\pi}}\Big[\sum_{t=0}^\infty \gamma^t \bar{\mathcal{C}
        }_{\text{task}}^{n}(\mathbf{h}_t,\mathbf{o}_t,\text{z},\text{y})\Big]$. Then, one can define the optimal value function:
\begin{equation}\label{eq:latentvalue}
\bar{V}^{\star,n}(\mathbf{h},\mathbf{o},\text{z}) = \min_{\bar{\pi}} \bar{V}^{n}(\bar{\pi}, \mathbf{h},\mathbf{o},\text{z}).
\end{equation}


Since we cannot optimize directly in the original constrained MDP, we first show that solving for an optimal policy in the latent MDP preserves key properties of the original problem. The following theorem formalizes this by proving the existence of the optimal policy and its mapping to the original MDP. The proof is in Appendix \ref{sec: equi}.


\begin{restatable}{thm}{thmsauteequivalence}(Optimality in the Latent Space)\label{thm:sauteequivalence}
Given the latent MDP in Definition \ref{def: c-p} and with A1-A2, we can show: 
\begin{enumerate}
\setlength{\itemsep}{3pt}  
  \setlength{\parskip}{0pt} 
    \item[a)] \textbf{(Prop I)} For any finite $n$, the Bellman equation holds, i.e., there exists a function $\bar{V}^{\star,n}(\mathbf{h},\mathbf{o}, \text{z})$ such that:
    \begin{align*}
     &\bar{V}^{\star,n}(\mathbf{h},\mathbf{o}, \text{z}) = \min_{\text{y} \in \gV} \Big( \bar{\mathcal{C}}^{n}_{\text{task}}(\mathbf{h},\mathbf{o}, \text{z},\text{y}) \\
     &\hspace{13em}+ \gamma \bar{V}^{\star,n}(\mathbf{h}^{\prime},\mathbf{o}^{\prime}, \text{z}^{\prime}) \Big),\\
        &\text{such that} \ \ (\mathbf{h}^{\prime},\mathbf{o}^{\prime},\text{z}^{\prime})\sim \bar{\gP}(\cdot|\mathbf{h},\mathbf{o},\text{z},\text{y})
    \end{align*}
Furthermore, the optimal policy solving Equation \ref{eq:latentvalue} has the representation $y \sim \bar{\pi}^{\star,n}(\cdot \mid \mathbf{h},\mathbf{o}, \text{z})$;
    \item[b)] \textbf{(Prop II)} The optimal value functions $\bar{V}^{\star,n}$ converge monotonically to $\bar{V}^{\star,\infty}$.

    \item[c)] \textbf{(Prop II)} The optimal policy in the latent space $\bar{\pi}^{\star,n}$ is also optimal in the original token space if used as $\bar{\pi}^{\star,n}(\phi(\cdot))$, minimizing Equation \ref{Eq:Constraints}, even as $n\rightarrow \infty$.\looseness=-1
\end{enumerate}
\end{restatable}
The above theorem ensures that finding and securing the existence of the optimal policy in the latent space is sufficient to solve Equation \ref{Eq:Constraints} optimally\footnote{As noted in Section \ref{sec: augment}, we analyze the transition from a large $n$ to $\infty$, and confirm that the results hold even for $\tilde{\mathcal{C}}_{\text{task}}^{\infty}$.}. Informally, the latent space acts as a faithful representation, preserving constraints and making optimization computationally efficient. This implies that the optimal policies and value functions in the latent space remain valid in the original space.\looseness=-1




\textbf{Almost Sure Guarantee.}
Now, we derive \textbf{Prop III} that ensures the safety cost constraints are almost surely satisfied. This is more challenging than Equation \ref{Eq:SafeLLM}, where \emph{only the expected safety cost is constrained}:  \begin{align}\label{Eq:a.s.SafeLLM-main}
    \min_{\pi} \ &\mathbb{E}_{\pi}\Big[\sum_t \gamma^t \mathcal{C}_{\text{task}}(\{\mathbf{x},\mathbf{y}_{<t}\}, \text{y}_t)\Big] \\ \nonumber 
     \ \text{s.t.} \ & \sum_t \gamma^t \mathcal{C}_{\text{safe}}(\{\mathbf{x}, \mathbf{y}_{<t}\}, \text{y}_t) \leq d \quad \text{ \colorbox{BurntOrange}{almost surely.}} 
\end{align}

While the formulation in Equation \ref{Eq:a.s.SafeLLM-main} is ``stronger'' than Equation \ref{Eq:SafeLLM}, solving for the augmented MDP formulation with objective as Equation \ref{Eq:Constraints} can yield a policy satisfying the above almost sure constraints. We formally state this result in Theorem \ref{thm:a.s.} and relegate the proof to Appendix \ref{sec: equi}. \looseness=-1
\begin{restatable}{thm}{thmalmostsure}\label{thm:a.s.} (Almost Sure Safety)
Consider an augmented MDP with cost function $\tilde{\mathcal{C}}_{\text{task}}^{\infty}$. Suppose an optimal policy exists $\pi^\star$ solving Equation \ref{Eq:Constraints} (see Theorem \ref{thm:sauteequivalence}) with a finite cost, then $\pi^\star$ is an optimal policy for Equation \ref{Eq:a.s.SafeLLM-main}, i.e., $\pi^\star$ is safe with probability approaching one or almost surely. 
\end{restatable}

\subsection{Algorithm and Practical Implementation}
Building on our theoretical framework, we propose a search algorithm with two approaches: \textit{i)} pre-training a small latent-space critic for cases where costs are available only for complete trajectories and \textit{ii)} directly leveraging intermediate costs for search optimization.

\textbf{Training a Latent-Space Critic.} 
We make the usual assumption that trajectories terminate at a maximum length $T$. In this case, the value function simplifies to become: $\bar{V}^n(\mathbf{h}_t, \mathbf{o}_t, \text{z}_t) = \E_{\bar\pi}[\gamma^{T} \bar{c}
        _{\text{task}}({\mathbf{ h}}_T, {\mathbf o}_T)]$ if there is safety budget left, i.e., if $ \text{z}_T > 0$, or $n$ if $\text{z}_{T} \leq 0$, where $\bar{c}_{\text{task}}({\mathbf{ h}}_t, {\mathbf o}_t) = c_{\text{task}}([\mathbf{x}, \mathbf{y}_{\leq t}])$ in the latent MDP.
        
Hence, it is sufficient to predict: the sign of $\text{z}_{T}$ and the value of $\gamma^T \bar{c}_{\text{task}}({\mathbf h}_T, {\mathbf o}_T)$ to assess the quality of a state. 
We estimate those through Monte Carlo (MC) sampling.
Specifically, we generate multiple trajectories from the initial state ($\mathbf{h}_0, \mathbf{o}_0, \text{z}_0)$ using the reference policy, and compute the mean terminal cost, the sign of $z_T$ to serve as targets for the critic training.
The usual alternative to MC sampling is Temporal Difference (TD) learning, where the critic is updated based on the difference between the current estimate and a bootstrapped estimate from the next state. However, MC sampling offers two advantages: \textit{i)} it simplifies training by using separate supervised signals for quality and safety, unlike TD, which combines both, and \textit{ii)} it allows dynamic adjustment of $n$ without retraining.\looseness=-1 

We train a critic network with two heads by sampling responses from the base model and scoring them using the cost function. We define $\mathcal{J}_1$ as the binary cross-entropy for predicting the sign of $\text{z}_T$ and $ \mathcal{J}_2$ as the mean squared error for predicting $\gamma^T \bar{c}_{\text{task}}({\mathbf{h}}_T, {\mathbf{ o}}_T)$. Our critic training minimizes: $\mathcal{J}(\mathbf\theta) = \mathbb{E}_{\bar\pi} \Big[ \sum_{t=1}^{T} \mathcal{J}_1\left( f^1_{\mathbf\theta}({\mathbf h}_t, {\mathbf o}_t, \text{z}_t), \text{z}_T > 0 \right) + \mathcal{J}_2\left( f^2_{\mathbf\theta}({\mathbf h}_t, {\mathbf o}_t, \text{z}_t), \gamma^T \bar{c}_{\text{task}}({\mathbf h}_T, {\mathbf o}_T) \right) \Big]$, where \(f^1_{\mathbf\theta}\) and \(f^2_{\mathbf\theta}\) are the heads of our parameterized critic.\looseness=-1


\textbf{Search method.}
We build on the beam search strategy from \citep{mudgal2023controlled, li2025survey} wherein we sequentially sample $N$ beams of $d$ tokens into a set $B$ from the pre-trained model and choose $K$ beams with the highest scores as possible continuations of the prompt (see Algorithm \ref{alg:inference_guard} in Appendix \ref{app:algo}).
This step ensures that we focus on the most promising continuations.
The goal of the scoring function is to balance the immediate task cost and the predicted future task cost while ensuring safety.
This is repeated until we complete trajectories. Given a token trajectory $\text{y}_{t:t+d}$, we present a scoring function $\text{E}_\text{critic}$ where we assume that we cannot evaluate intermediate answers with the cost functions. However, when immediate safety costs are available, a simpler scoring function, see Appendix  \ref{app:algo}. We define $\text{E}_\text{critic}$ as:
\begin{align*}
&\text{E}_\text{critic}(\text{y}_{t:t+d}) =
&\begin{cases} 
\gamma^T \bar{c}_{\text{task}}(\cdot) & t+d = T \text{ and } \text{z}_{t+d} > 0 \\ 
n & t+d = T \text{ and } \text{z}_{t+d} \leq 0 \\ 
 f^2_{\mathbf\theta}(\cdot) &  f^1_{\mathbf\theta}(\cdot) > 0.5\\
n & \text{otherwise}. 
\end{cases}
\end{align*}
This $\text{E}_{\text{critic}}$ scoring function evaluates token sequences by balancing safety and task performance. At the final step ($t+d=T$), it assigns a score based on the task cost $\mathcal{C}_{\text{task}}$ if safety constraints are met ($\text{z}_{t+d} > 0$); otherwise, it applies a high penalty $n$. For intermediate steps, it relies on a trained critic. If the critic confidently predicts safety ($f_{\mathbf{\theta}}^{1}(\mathbf{h}_{t+d},\mathbf{o}_{t+d},\text{z}_{t+d})>0.5$), it uses the estimated future cost ($f_{\mathbf{\theta}}^{2}(\mathbf{h}_{t+d},\mathbf{o}_{t+d},\text{z}_{t+d})$); otherwise, it assigns the penalty $n$ as a conservative safeguard. 
\begin{figure*}[h]
\centering

\includegraphics[width=0.87\textwidth]{imgs/safety-crop.pdf}
\caption{Trade-offs between safety, reward, and inference time for BoN, ARGS, RECONTROL, Beam Search, and InferenceGuard on the PKU-SafeRLHF test dataset, evaluated for Alpaca-7B (top) and Beaver-v3-7B (bottom). Reward is the average score evaluated by the reward model, safety rate is the percentage of tasks completed within budget $d$, and inference time is the average per-task duration. The left column (reward vs. safety) and right column (inference time vs. safety) categorize methods by performance. \textit{InferenceGuard} achieves a balanced trade-off, positioning in the \textit{Optimal Region} and \textit{Optimal Efficiency} quadrants.}  
 

\label{fig:safety_tradeoff}
\end{figure*}
\textbf{Sampling Diversity.}
Finally, if the right selection strategy can guarantee that we will converge on a safe solution, it does not consider how many samples would be necessary.
To increase the search speed, we introduce a diversity term in the sampling distribution when \emph{no safe samples} were found based on the token frequency of failed beams.
We denote $F$ as the frequency matrix counting the tokens we previously sampled from $t$ to $t+d$.
Instead of sampling from $\text{SoftMax}(\mathbf{W}\mathbf{o}_t)$, we sample from $\text{SoftMax}(\mathbf{W}\mathbf{o}_t - \text{n}_{2}({F_{t} > 0}))$ where $\text{n}_{2} ({F_{t} > 0})$ is a vector where each component $j$ is $\text{n}_2$ if $F_{t, j} > 0$ and 0 otherwise. The addition of $\text{n}_{2}({F_{t} > 0})$ disables the possibility of sampling the same token at the same position observed in unsuccessful beams, thus increasing diversity.


It is worth noting that as we sample from the reference LLM and rank responses directly or via the critic, block sampling ensures a small Kullback-Leibler (KL) divergence from the original LLM without explicitly adding a KL regularizer into our objective, preserving coherence and natural flow; see \citep{mudgal2023controlled}.



\section{Experiments}\label{Sec:Exp}

\begin{figure*}[h!]
\centering

\includegraphics[width=0.81\textwidth]{imgs/alpaca_beaver_distribution-crop.pdf}
%
\caption{Reward and cost distributions on PKU-SafeRLHF test tasks using Alpaca-7B (top) and Beaver-v3 (bottom) as base models. The left y-axis shows reward distribution. The right y-axis shows the maximum cumulative cost. \textit{InferenceGuard} outperforms others, achieving higher rewards with lower costs across both models.}
\label{fig:reward_cost_comparison}
\end{figure*}
\noindent \textbf{Baselines.} We evaluate the helpfulness (task cost) and harmlessness (safety cost) of our method on both non-safety-aligned and safety-aligned base models, Alpaca-7B~\cite{taori2023alpaca} and Beaver-7B~\citep{ji2024pku}. We compare $\texttt{InferenceGuard}$ to the following state-of-the-art test-time alignment methods: Best-of-N (BoN), beam search,  ARGS~\citep{khanov2024args} and RECONTROL~\citep{kong2024aligning}. These test-time alignment methods were originally designed to maximize rewards without considering safety. To ensure a fair and meaningful comparison, we extend them to incorporate safe alignment for LLMs. We incorporate a Lagrangian-based approach for all baselines that penalizes safety violations, ensuring a balanced trade-off between task performance and safety. This helps us evaluate our performance against other algorithms and highlights the importance of safety augmentation instead of a Lagrangian approach for effectively balancing rewards and constraint satisfaction. 


For beam search and Best-of-N (BON), we adopt a Lagrangian approach to select solutions with $c_{\text{task}} + \lambda \mathcal{C}_{\text{safety}}$ where $\lambda$ is the Lagrangian multiplier. Similarly, we extend ARGS so that token selection follows: $-\omega \pi(t|\cdot) + c_{\text{task}} + \lambda \mathcal{C}_{\text{safety}}$, with $\omega$ adjusting the influence of the reference policy. We also considered state augmentation for ARGS and RE-control but found it ineffective. Since these methods decode token-by-token, they cannot recover once $\text{z}_t$ flips the sign, and before that, $\text{z}_{t}$ has no influence. Thus, we excluded it from our evaluation. To further strengthen our comparison, we introduce safety-augmented versions of BoN and beam search as additional baselines. 

\textbf{Datasets.} We evaluate all the above methods on the PKU-SafeRLHF dataset~\citep{ji2024pku}, a widely recognized benchmark for safety assessment in the LLM literature.  This dataset contains 37,400 training samples and 3,400 testing samples. Of course, our training samples are \emph{only} used to train the critic value network. Here, we construct a dataset by generating responses using the base models with five sampled trajectories for each PKU-SafeRLHF prompt from the training set. \looseness=-1

\noindent \textbf{Evaluation Metrics.}
We assess the performance using several metrics: the \textbf{Average Reward} is computed using the reward model of~\citep{khanov2024args} as $ - c_{\text{task}}$ on the complete response to reflect helpfulness, where a higher reward indicates better helpfulness; the \textbf{Average Cost} is evaluated with the token-wise cost model from~\citep{dai2023safe} as $\mathcal{C}_{\text{safety}}$, indicating harmfulness, with higher cost values reflecting more resource-intensive or harmful outputs; the \textbf{Safety Rate} is defined as the proportion of responses where the cumulative cost does not exceed the safety budget \( z_{t=0} = 10 \), and is formally given by \( \text{Safety Rate} = \frac{1}{N} \sum_{i=1}^N \mathbb{I}(\mathcal{C}^i_{\text{test}} \leq z_{t=0}) \), where \( N \) is the total number of responses; and \textbf{Time} refers to the inference time taken to generate a response in seconds.




\textbf{Results.}
We present our main results in \cref{fig:safety_tradeoff} and additional ones in \cref{tab:performance_comparison} in Appendix \ref{App:Exps}.
\texttt{InferenceGuard} achieves the highest safety rates with both models (91.04\% and 100\% for Alpaca and Beaver respectively).
With Beaver, our method dominates the Patero front, achieving the highest rewards without any unsafe responses. 
Although Lagrangian methods can have a reasonable average cost, they all fail to satisfy the safety constraints. In the meantime, they are too safe on already safe answers, hindering their rewards.
The RE-control intervention method underperforms for both models in our setting.
ARGS can provide safe answers but with very poor rewards because most answers are very short to avoid breaking the safety constraint.
Best-of-N and beam search with augmented safety came after us.
However, Best-of-N cannot leverage intermediate signal, and beam search is blind to its previous mistakes, yielding more unsafe answers.

\cref{fig:reward_cost_comparison} provides a better view of the reward and safety distributions. The figure shows that \texttt{InferenceGuard} consistently achieves higher rewards while maintaining low cumulative costs, outperforming other methods across both models. Its maximum cumulative cost stays just under the safety budget while maximizing the reward as suggested by our theoretical contributions. Finally, we observe that the trained critic helps to better guide the search on intermediate trajectories in both settings. We also compare generated answers in Appendix \ref{App:Exps} qualitatively.

\section{Related Work}
\textbf{Safe RL:} Safe RL employs the cMDP framework, formulated in~\citep{altman1999constrained}. Without prior knowledge,~\citep{turchetta2016safe, koller2018learning, dalal2018safe} focus on safe exploration, while with prior knowledge,~\citep{chow2018lyapunov, chow2019lyapunov, berkenkamp2017safe} learn safe policies using control techniques. \citep{achiam2017constrained, ray2019benchmarking, stooke2020responsive} enforce safety constraints via Lagrangian or constrained optimization which can often lead to suboptimal safety-reward trade-offs. Instead, we extend safety state augmentation~\cite{sootla2022saute} to LLMs and latent MDPs to ensure almost sure inference time safety.\looseness=-1


\textbf{LLM alignment and safety:} Pre-trained LLMs are often aligned to specific tasks using RL from Human Feedback (RLHF), where LLMs are fine-tuned with a learned reward model~\citep{stiennon2020learning, ziegler2019fine, ouyang2022training} or directly optimized from human preferences~\citep{rafailov2023direct, azar2023general, zhao2023slic, tang2024generalized, song2024preference, ethayarajh2024kto}. \citep{bai2022training, ganguli2022red} first applied fine-tuning in the context of safety, and \citep{dai2023safe} proposed safe fine-tuning via Lagrangian optimization. Other safety-focused methods such as \citep{gundavarapu2024machine,gou2024eyes,hammoud2024model,hua2024trustagent,zhang2024controllable,guo2024cold,xu2024safedecoding,wei2024assessing,li2025salora} are either orthogonal, handle a different problem to ours, or can not ensure almost sure safety during inference. 



\textbf{Inference time alignment:} Inference-time alignment of LLMs is often performed through guided decoding, which steers token generation based on rewards~\citep{khanov2024args,shi2024decoding,huang2024deal} or a trained value function~\citet{han2024value,mudgal2023controlled,kong2024aligning}. Other safety-focused inference-time methods include~\citep{zhong2024rose,banerjee2024safeinfer,niu2024parameter,wang2024probing,zeng2024root,zhao2024adversarial}. Compared to those methods, we are the first to theoretically guarantee almost sure safe alignment with strong empirical results. Operating in the latent space enables us to train smaller, inference-efficient critics while optimally balancing rewards and safety constraints without introducing extra parameters, e.g., Lagrangian multipliers.\looseness=-1

We detail other related
works extensively in Appendix \ref{sec:rel-2}.























 
\section{Conclusion}\label{secConclu}
We introduced \texttt{InferenceGuard}, a novel inference-time alignment method that ensures large LLMs generate safe responses almost surely—i.e., with a probability approaching one. We extended prior safety-augmented MDP theorems into the latent space of LLMs and conducted a new analysis. Our results demonstrated that \texttt{InferenceGuard} significantly outperforms existing test-time alignment methods, achieving state-of-the-art safety versus reward tradeoff results. In the future, we plan to improve the algorithm's efficiency further and generalize our setting to cover jailbreaking. While our method is, in principle, extendable to jailbreaking settings, we aim to analyze whether our theoretical guarantees still hold. 

\section*{Broader Impact Statement} 
This work contributes to the safe and responsible deployment of large language models (LLMs) by developing \texttt{InferenceGuard}, an inference-time alignment method that ensures almost surely safe responses. Given the increasing reliance on LLMs across various domains, including healthcare, education, legal systems, and autonomous decision-making, guaranteeing safe and aligned outputs is crucial for mitigating misinformation, bias, and harmful content risks.

To further illustrate the effectiveness of our approach, we have included additional examples in the appendix demonstrating that our method successfully produces safe responses. These examples were generated using standard prompting with available large language models LLMs. Additionally, we have added a warning at the beginning of the manuscript to inform readers about the nature of these examples. Our primary motivation for this addition is to highlight the safety improvements achieved by our method compared to existing alternatives. We do not foresee these examples being misused in any unethical manner, as they solely showcase our model’s advantages in ensuring safer AI interactions. Finally, we emphasize that our method is designed specifically to enhance AI safety, and as such, we do not anticipate any potential for unethical applications.

\texttt{InferenceGuard} enhances the scalability and adaptability of safe AI systems by introducing a formally grounded safety mechanism that does not require model retraining while reducing the resource costs associated with traditional RLHF methods. The proposed framework advances AI safety research by providing provable safety guarantees at inference time, an area that has received limited attention in prior work.\looseness=-1

While this method significantly improves safety in LLM outputs, it does not eliminate all potential risks, such as adversarial manipulation or emergent biases in model responses. Future work should explore robustness to adversarial attacks, contextual fairness, and ethical considerations in deploying safety-aligned LLMs across different cultural and regulatory landscapes. Additionally, transparency and accountability in AI safety mechanisms remain essential for gaining public trust and ensuring alignment with societal values.
This work aims to empower developers and policymakers with tools for ensuring safer AI deployment while contributing to the broader conversation on AI ethics and governance.

\section*{Acknowledgments}
The authors would like to thank Rasul Tutunov, Abbas Shimary, Filip Vlcek, Victor Prokhorov, Alexandre Maraval, Aivar Sootla, Yaodong Yang, Antonio Filieri, Juliusz Ziomek, and Zafeirios Fountas for their help in improving the manuscript. We also would like to thank the original team from Pekin University, who made their code available and ensured we could reproduce their results. 


\bibliography{icml2025}
\bibliographystyle{icml2025}

\appendix

\input{appendix/saferlhf_background}
\input{appendix/algo}
\input{appendix/experiments}



\end{document}



