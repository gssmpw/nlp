\section{Advice Provision}
\label{sec:formal}

\subsection{Formalization}
Next, we formally introduce the advice provision problem as a Markov Decision Problem (MDP) $<S,A,P,R,\gamma>$ \cite{Puterman1994}.

Let us consider a set of $n$ semi-autonomous drones engaged in a cooperative task, supervised and controlled by a single human operator, $\operator$.
The state space $S$ consists of all contextual information regarding the drones (e.g., status, altitude) and the task's and operator's domain-specific characteristics (e.g., time elapsed, performance). The operator can perform an action $a\in A$ during the task, at any time $t\in [0,\ldots T]$. Since $\operator$ can choose to execute no action at any given $t$, $NULL\in A$. In addition, not every action is possible at each state, and therefore, we denote $A(s)$ as the set of applicable actions at state $s$.
Let $\Pfunc{\stateS}{\action}{\stateS'} \ $
be the transition function that denotes the probability of transitioning from state $\stateS$ to state $\stateS'$ following action $a$.
Clearly, for each $\stateS \in S$, 
$\sum_{s' \in S} \Pfunc{\stateS}{\action}{\stateS'}=1$.
Note that, when $\action = NULL$, it holds that $\stateS \not= \stateS'$ since the environment is dynamically changing.  
Finally, let $R(s)$ be the domain-specific reward function,
%hodaya: i remove this: $\Omega$ be the set of observations, $O$ be the domain-specific observation function, 
and $\gamma\in[0,1]$ be the discount factor.
Importantly, performing an action takes time, depending on the operator's ability. The transition function and the cost function are unknown to the advising agent, yet they can be estimated through observations. %hodaya: Use observations here?

Advice is guidance provided by an agent to the human operator as an action that the operator should take ($a \in A$). 
Crucially, the operator maintains the autonomy to assess the advice, consider alternative actions, and make a final decision (i.e., the advice is non-binding). At any point in time $t$, the agent may provide advice according to its advising policy $\pi$, a mapping from states to actions.
In an ideal setting, we would like the operator to follow an optimal policy, $\pi^*_\operator: S \rightarrow A$, that maximizes the expected accumulative future reward. However, the dynamic and uncertain environment causes the underlying optimization problem to be intractable and thus, an optimal policy cannot be computed in a reasonable time. To overcome this limitation, we propose a methodology for computing high-quality advice without explicitly deriving an optimal policy. 

\subsection{Proposed Methodology}

We propose an advice provision methodology consisting of two phases: an offline phase which is targeted at estimating the expected long-term reward of performing an action (i.e., reward estimation) using limited human demonstrations and a machine learning model; and an online phase, primarily designed for providing a suitable advice in a given state.   


% adopt the 1-step-lookahead rolling-window heuristic which was found to be highly useful in similar advice provision settings  \cite{Rosenfeld2017collaboration}. %Ariel: can you please cite some other works as well? possibly not from robotics...
% Specifically, at each time $t$, the agent outputs $a$ which minimizes (maximizes) the expected reward \textit{assuming the agent can only suggest a single piece of advice}.
\noindent{\bf  Reward Estimation Model}

\begin{algorithm}[hbpt!]
\caption{Reward Estimation}\label{alg:generation}
\begin{algorithmic}[1]
\REQUIRE $k,\theta,T,D$
\STATE $D_{syn} \gets \emptyset$
\FORALL{$\textit{d} \in D$}   
        \STATE Estimate $\Tilde{C}_h(a)$ based on $d$
        % \stateS Let $\Tilde{P}_h(a)$ be operator's success probability (e.g $TP, TN$)
        % \stateS $TS \gets$ Special action scheduling from \textit{d}
	\FOR{$i \gets 1$ to $k$}
        \STATE $t\gets 0$ \ \  $\trajectory \gets \emptyset$
        \ \ \ $s_0 \gets d_0[0]$ \label{S0}
        \WHILE{$t < T$}
            % \stateS $a_t\gets d_t[0]$
            \STATE $A_t \gets A(s_t)$ 
            \STATE $a_t \gets Pertubate(A_t,d,t)$ \label{Pertubate}
             \STATE $s_{t+\Tilde{C}_h(a)} \gets GetNewState(s_t,a_t)$ \label{GetNewState}
             \STATE $t\gets t+\Tilde{C}_h(a)$
             \STATE $\trajectory \gets \trajectory \cdot \{(s_t,a_t)\}$
        \ENDWHILE
        \STATE $D_{syn}\gets D_{syn}\cup\trajectory$
\ENDFOR
        \ENDFOR
        \IF {$QualityAssurance(D_{syn},D,\theta)$} \label{QualityAssurance}
            \STATE $M\gets Train(D_{syn},D)$
            \RETURN $M$
        \ELSE
            \RETURN $Unsuccessful$
        \ENDIF

%Now check the similarity 

\end{algorithmic}
\end{algorithm}

As noted before, a central component in solving the underlying optimization problem is the proper estimation of the expected benefit from performing an action. To that end, we assume that a set of demonstrations is available, $D$,  where $d\in D$ is a trajectory $\trajectory$ consisting of state-action pairs, $((s_0,a_0),...,(s_T,a_T))$, denoting the actions that the operator took at each time and state. If $D$ is \say{sufficiently large and comprehensive}, then a machine learning model could be readily trained based on these demonstrations to approximate the effects of performing an action at a given state and time. However, as noted before, collecting such a set of demonstrations is typically highly expensive and time-consuming. Therefore, we assume $D$ is small and needs to be extended before it can be effectively used for training a machine learning model. As detailed in Algorithm \ref{alg:generation}, we propose the generation of high-quality \textit{synthetic} trajectories that mimic the real demonstrations in an offline fashion (i.e., before actual deployment of the advising agent). 
In words, we start by estimating the time it takes for a given operator to perform different actions using the entire trajectory, $\Tilde{C}_h(a)$ (e.g., using an average). Then, we go through a real trajectory ($d\in D)$ and pertubate it to create slight changes (e.g., choosing another action at a given probability). The action is then applied in a simulated environment using the transition function (e.g., the transition can also consider the operator's performance), a new state is reached and the process is repeated. Overall, $k$ synthetic trajectories are generated based on each real one. 
The resulting set of generated trajectories is then checked to ensure that it, indeed, closely mimics the real ones (i.e., quality assurance using statistical measures). If so, a machine learning model is trained on the \textit{extended} set of demonstrations (otherwise, an \say{unsuccessful} message is raised). 
The trained model is utilized next in the online phase to rank the various actions.

\noindent{\bf Advice Provision}
During deployment, the agent employs an online advice provision policy as follows:  \\
%\begin{enumerate}
{1. \bf Action generator}: Given state $s$, the agent generates a set of possible actions that can be performed at this state and time point. These actions may be the result of a drone-initiated event (e.g., a drone malfunction) and/or other actions aimed at improving performance (e.g., changing a drone's altitude for better coverage). It then evaluates them using the reward estimation model obtained in the offline phase (i.e., Algorithm \ref{alg:generation}). \\
{2. \bf Ranking}: All generated actions and their associated expected reward are provided to a ranking model that outputs the top $c$ actions as advice for the operator. See Figure \ref{fig:agent_structure} for an illustration. 
% (complex actions is a set of some actions from the same type).
%\end{enumerate}
 
In other words, once a new state is encountered, the action generator creates possible actions to take which are then evaluated using the offline trained model and ranked accordingly.   

% Once a reward estimation function is trained using Algorithm \ref{alg:generation}, the advising agent can use it in the online setting. 

% Another use of the simulated runs is for model validation. Start a simulated run and stop at a certain point. Then, from that point onward, run several simulated runs, each one starting with a different action but continuing with the same method of choosing actions until the end of the run. This way, we can determine which action leads to the best outcome. Since randomness is inherent in the environment itself, it's necessary to run several simulated runs for each action and statistically check if there is a recommended action at that time point and whether the model indeed tends to give it a higher value.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{media/agent_general_structure.png}
\caption{The advising agent design. $s$ denotes the state, $a_i\in A$ denotes an action, $s'$ denotes the expected resulting state and $v_i$ denotes the predicted reward from the transition.}
\label{fig:agent_structure}
\end{figure}

%Once we have the prediction model, the agent can be implemented. 