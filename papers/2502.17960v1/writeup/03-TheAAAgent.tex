\section{Agent Implementation}

We instantiate the proposed advising agent  (Section \ref{sec:formal}) to the SAR task (Section \ref{sec:sar_mission}). To that end, we detail our implementation of the offline (reward estimation model) and online (advice provision) components.

\noindent{\bf\subsection{Reward Estimation Model}}
Next, we detail our implementation of Algorithm \ref{alg:generation}. 

For each $d\in D$, in order to select the initial state (Line \ref{S0} of Algorithm \ref{alg:generation}),  we use the parameters set by the operator at the beginning of the simulation and the initial assignment of drones to sub-areas.
For the $Pertuable(A_t,d,t)$ function, which varies the next action (Line \ref{Pertubate} of Algorithm \ref{alg:generation}) the action is selected through the following procedure: First, if in $d$ the operator performed an action of type $CP$ or $CT$ approximately at time $t$ (i.e., less than a minute away from $t$) then the action is selected. Otherwise, if $t=10min$, 
then we perform a random $CP$ or $CT$ change at probability $p = 1/|A'|$, where $A'$ is the set of actions we want to add at these points. Finally, we chose to handle one of the drone malfunction alerts (if it exists) by handling the highest confidence detection alert available (if it exists) or another alert at random. 

For $GetNewState(s_t,a_t)$ (Line \ref{GetNewState} of Algorithm \ref{alg:generation}), we simulate the effect of $a_t$ using our environment assuming it takes $\Tilde{C}_h(a) \forall{a\in A}$ time for the action to be executed. In our implementation, we define $\Tilde{C}_h(a)$ to be the average time it takes for the operator to perform an action $a$ in the entire trajectory. When handling a detection alert, we simulate whether the operator correctly or incorrectly handled it using her performance through the trajectory (i.e., using the true positive and true negative rates).
Note that since we run the simulator as in a real simulation, the changes in the state caused by the continued scanning of the drones are reflected in the next state in a natural manner. In other words, our synthetic simulation is restricted to automatically simulating the operator's actions.

For $QualityAssurance(D_{syn}, D, \Theta)$ and model training ($M$), we featurize the state-space such that each $s\in S$ is defined as follows:
\begin{itemize}[leftmargin=10pt]
\item Operator performance: 
$\Tilde{C}_h(a)$ that estimates the average time it took the operator to perform $a$ in the trajectory up to state $s$.  
\item Environment information: 
Remaining time for the SAR task, count of approved detection events, percentage of false detection alerts, count of current open alerts for each type of alert, lowest confidence thus far that led to detection, and count of actions performed by the operator.
\item Information for each area type:
The percentage of all sub-areas of that type that remained unsearched, the number of alerts generated from sub-areas of that type above and under current \detectiothreshold\ and  \pausetreshold\ until the current state, 
search parameters: \pausetreshold,  \detectiothreshold, \velocity, \altitude, and their last previous values (if exist). 
\end{itemize}



For $QualityAssurance(D_{syn}, D, \Theta)$, we first check the distance between synthetic trajectories and each of the real trajectories to ensure that the synthetic trajectories closely resemble the original trajectory they originated from.
Then, a clustering test of the synthetic and real trajectories is performed. This phase is used to ensure that each synthetic trajectory is assigned to the same cluster as the real trajectory and to make sure that the synthetic trajectories are roughly proportionally distributed among clusters. 
If the synthetic data quality assurance passes the required threshold, the algorithm trains a model. 
Then the quality assurance of the model is done through two perspectives: (1) Ensuring that the model accurately predicts the utility by leading to reasonably low MAE; and (2) Ensuring that the model brings about effective recommendations by aligning with actions known to perform well in hindsight through synthetic simulations (see \ref{subsub:UtilityFunction} for details).

\noindent\textbf{(1) Distances:}
% The first test checked the distances between the synthetic trajectories and the real trajectories. 
First, we sample three states from each synthetic and real trajectory: one roughly at the beginning, one roughly at the middle, and one roughly at the end. We calculate the squared distance between the synthetic trajectory and each real trajectory in $D$ and sort them from lowest to highest. If the original trajectory from which the synthetic one originated is ranked high (i.e.,  first or second) in the sorted set, the synthetic trajectory can be considered adequate.  

\noindent\textbf{(2) Clustering:}
As before, we first sample three states from each synthetic and real trajectory: one roughly at the beginning, one roughly at the middle, and one roughly at the end. 
%A K-means algorithm was applied, with predefined $k$ on  real  trajectories. Then a K-means algorithm was applied using both the real and synthetic trajectories with define the initial centroids to be the centroides from the first execution of K-means.
A K-means algorithm~\cite{jancey1966multidimensional, macqueen1967some,lloyd1982least, steinhaus1956division} was applied using both the real and synthetic trajectories. The proportion of synthetic trajectories that belong to the same cluster as their original one is returned (i.e., the higher the better).  

% \subsubsection{Utility Function}
\noindent{\bf{Utility Function}}
\label{subsub:UtilityFunction}
Given a trajectory $\trajectory$ of length $l$ and $s_j \in \trajectory$, the discounted accumulated reward of a given state $s_j$ until the end of the trajectory is $\bar{R}(s_j)=\sum_{i=j,s_i \in \trajectory}^{i=l} \gamma^{i-j} R(s_i)$.
Since the detection of a target is a sparse event and most rewards along a trajectory are zero, we define a utility function $\UFunc$ that will be used to train a model $\Model$  that will enable the agent to compare states better. 

Let $\parametersForU=\{\paramForU_1,...\paramForU_m\}$ 
be a set of parameters of states in $S$.
We aim at developing a utility function $U(s)=\wights_0 \bar{R}(s)+\wights_1 \psi_1(s)+...+\wights_m \psi_m(s)$.
Given a state $s$ and actions $a$ and $a'$, let $\bar{s}$ and $\bar{s}'$ be the expected states generated by the expected state generator, respectively.
Our goal is to train a model $M$ to estimate $U$ such that: 

(I) if $M(\bar{s}) \geq M(\bar{s}')$ then $\bar{R}(s) \geq \bar{R}(s')$.

We considered the following parameters for $\parametersForU$:
(a) the path scanned by the drones until the end of the trajectory (i.e., to encourage scanning as much area as possible); (b) The average waiting time for an alert; (c)  The ratio between the number of detection alerts until the end of the trajectory and the number of alerts the operator can handle given his cost function (i.e., to encourage a balanced workload);  (d) The number of false negatives or false positives; (e) The number of targets correctly found until now in the trajectory; and (f)  The number of targets correctly found until the end of the trajectory. 


To train a  model $M$ that attempts to satisfy condition  (I), the following three steps are performed  repeatedly:
\begin{enumerate}[nosep]
    \item Choose possible weights for $U$.
    \item Train a model $M$ that accurately estimates $U$.
    \item Evaluate the ability of $M$ to lead to effective action recommendations.
\end{enumerate}
Testing (2) is done by computing the MAE  (see \ref{modules_evaluation}). 
Testing (3), is done using a set of synthetic trajectories generated specifically for these tests (see \ref{sec:action_ordering}).
Once the parameters of $U$ have been established and $M$ has undergone training, the agent is presumably capable of generating effective actions for the operator.


\noindent{\bf\subsection{Advice Provision}}

Next, we detail our implementation of the action generator $(\AG)$ and the ranking model $(\RM)$.
The $\AG$ component uses the expected state generator to anticipate the outcome following the execution of an action, and then the reward estimation model, derived in the offline phase, to evaluate the effect of the action in terms of expected reward.
Every predetermined number of seconds, the agent activates the $\AG$ and the $\RM$ in order to produce $\adviceNum$ actions that are expected to be the most promising given the current state. In addition, every incoming alert from the drones activates the $\RM$ once again in order to determine whether the new alert should result in a different ranking. 

The process of generating possible actions involves two types of actions (corresponding to the actions available in the system): First, an action that is not associated with an alert generated by the system, i.e.,
(i) Changing the area type of a given sub-area, denoted by $CT$; (ii) changing the scanning parameters associated with an area (i.e., altitude, velocity, and thresholds), denoted by $CP$; (iii) Handling intelligent messages;  (iv) Detecting a drone that is not progressing and may be stuck (and has not sent an alarm).
The second type of actions are actions that are directly associated with alerts generated by the system, i.e., Handling alerts such as detection, with low-confidence (\pausetreshold) or with high-confidence (\detectiothreshold) and the malfunction of a drone. The action generator uses the expected state generator to generate the expected state and then uses the reward estimation model. 

Advice can be a simple action, e.g., increasing the parameter $\pausetreshold$ by $5$ for area type \say{low} or changing the area type of a given sub-area to \say{high}. In our implementation, we further allow for complex actions to be proposed, e.g., a list of changes for the same parameter for each area type or a list of changes of the area type for several sub-areas. 
%increase the parameter $pt$ for area type low and decrease  the parameter $pt$ for area type medium.
Generating and evaluating complex actions leads to a combinatorial challenge.
Hence, a two-phase algorithm is proposed: initially, simple actions are generated and assessed; subsequently, the most promising ones are combined into complex actions, $a_i^v\subset A$.
For example, consider changing the parameter $\pausetreshold$.  
In the first phase, for each area type, we consider changing $\pausetreshold$ by    $\pm0.5x, \pm x$ and $\pm2x$ where $x$ denotes a predefined value or the mean change that the operators performed for $\pausetreshold$ in $D$. The expected state generator is then used for these possible changes followed by the reward estimation model. 
In the second phase, the agent considers adjusting the $\pausetreshold$ values for all area types by modifying the existing $\pausetreshold$ values according to the result of the first phase. the best value identified in the initial phase. The action generator uses the expected state generator in order to generate the expected state if these changes were to be performed considering the expected time it would take for the operator to change these parameters. 
% , and then use the reward estimation model. 

%We propose a strategy based on a greedy approach, wherein we first assess the effectiveness of individual actions $a_i\in A$. Subsequently, having identified advantageous actions of same type, we exclusively explore their combination $a_i^v\subset A$. In addition, the $CP$ action can be  performed with a large number of values, we tested only $3$ possibility values for increasing and decreasing. 
%hodayam - add hierarchical search.

%\subsection{Fictional State Generator}%new2
%hodaya - check the  State Generator (if will be time)


% \subsubsection{Ranking Model}%new4
% \label{ranking_model}
%The AA process of determining the ranking order for alerts from drones employed an automated simulations with various mimic-operator scenarios, each governed by different rules. 
% Note that our reward estimation model does not explicitly consider the expected reward of handling intelligent messages and addressing alerts from stuck drones.
% The ranking of the alerts from the drones is based on the confidence of each alert.
% Handling intelligent messages and stuck drone alerts get the highest priority. 
We rank the actions provided by the action generator according to their estimated reward and provide the three top-ranking ones to the operator (negatively estimated actions are not provided). Tie-breaking favors the handling of detection alerts over others and alerts with higher confidence over others as a secondary criterion. 
The operator has the option to perform an action that is not advised by the agent. It is important to note that some actions are typically based on information the agent cannot access, such as changing the probability of a sub-area, which requires interpreting intelligence messages and understanding the map. Therefore, the operator may decide to perform such actions independently.

 % If the estimated value of the drone-generated alert is the highest, then present to the operator only drone-generated alerts. Otherwise, sort the tasks according to their value and propose the best ones to the operator. 


% The motivation to rank the detection and pause alerts according to their confidence was based on experiments that compared FIFO with the highest precision, resulting in no statistical difference between them. Analyzing operators' behavior yields that precision is a commonly used criterion for choosing which alert to look at.


In our implementation, the expected state generator component is designed to predict the next state in a way that reflects the main significance of the action. The \textbf{operator performance} remains unchanged. The \textbf{environmental information} is modified as follows: the remaining time for the task is updated based on $\Tilde{C}_h(a)$; the count of approved detection events is adjusted if the action handles a detection alert, reflecting the percentage of approved events up to the current state; the percentage of false alerts remains unchanged; the current open alerts decrease by one if the action handles a detection alert; the lowest confidence that led to detection remains unchanged; and the count of actions performed by the operator increases by one, depending on the action type.
\textbf{Information for each area type} is also considered: the percentage of unsearched sub-areas remains unchanged, the number of alerts generated from sub-areas above and below the current \detectiothreshold\ and \pausetreshold\ is updated if the action changes these thresholds, and the search parameters are modified if the action alters one of them, with the previous values also being updated accordingly.
Note that changes in state due to the continued scanning of drones are not included as the exact computation is highly time-consuming. 
% Since the expected states are for comparison, the ongoing progress has a similar impact across all generated states, and the variation in action duration is not significant enough to affect the comparison, we preferred to omit these changes.



%Formally, Let $s'_{a_j}$ be the predicted state to be after doing $a_j$.
%We want to maximize the number of trajectories $\tau_j\in \mathcal{T}$ such that for a $s\in \tau$,  $R(s_{a*}) > %R(s_{a_j})$ for all $a_j\in A', a_j \not= a*$.
%Details about this evaluation can be found in the appendix.

%[The details for the appendix:
%The  set of the generated trajectories $\mathcal{T}$, we mentioned above,  consist of two types of trajectories, %denote these subsets by $\mathcal{T}_1$ and $\mathcal{T}_2$.
%$\mathcal{T}_1$ contains the synthetic trajectories with improper parameters that do not match, in such case an %action that change the improper parameter should be preferred.
%for example, very low thresholds, leading to numerous false alarms with an extremely slow operator that cannot %handle many messages.


%$\mathcal{T}_2$, contains trajectories of mimic-operator, for which the we was able to determine the best action. 
%In order determine the best action, for each operator, we generate trajectories that mimic his decision making, %without doing the change parameter action, denote this set of trajectories by $T$, and additional trajectories %where the change parameter action done after 10 minutes, and increase or decrease the threshold, denote these sets %by $T^+, T^-$ respectively. We then compared the performances.
%$\mathcal{T}_2$, contains trajectories of mimic-operator, for which the performance was significantly diffident %between $T, T^+, T^-$.
%Each $\tau_j \in \mathcal{T}_2$ is associated with a best action according to this comparison.

%**should add a table.]
%************************************







% summary of this check can be found here: https://docs.google.com/spreadsheets/d/1Q4InCmxTf8t8V4vUeTTRmryKWRpMIi7D/edit#gid=1075344868


%More details on our comparison are provided in the appendix.

