%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Load any packages you require here. 
%\usepackage{latexsym}
%\usepackage{amssymb}
%\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{booktabs}
\usepackage{enumitem}
%\usepackage{graphicx}
\usepackage{color}

%\usepackage{times}  % DO NOT CHANGE THIS
%\usepackage{helvet}  % DO NOT CHANGE THIS
%\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage{url}  % DO NOT CHANGE THIS
%\usepackage{upgreek}
\usepackage{dirtytalk}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{hyperref}


%\usepackage{latexsym}
%\usepackage{amssymb}
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{booktabs}
%\usepackage{enumitem}
%\usepackage{graphicx}
%\usepackage{color}

%\usepackage{times}  % DO NOT CHANGE THIS
%\usepackage{helvet}  % DO NOT CHANGE THIS
%\usepackage{courier}  % DO NOT CHANGE THIS
%\usepackage{url}  % DO NOT CHANGE THIS
%\usepackage{upgreek}
%\usepackage{dirtytalk}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\usepackage[noend]{algpseudocode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any theorem-like environments you require here.

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any new commands you require here.

\newcommand{\AreasIdSet}{\ensuremath{E}}
\newcommand{\areaId}{\ensuremath{e}}
\newcommand{\paramSet}{\ensuremath{\uprho}}


\newcommand{\Sset}{\ensuremath{S}}
\newcommand{\Aset}{\ensuremath{A}}
\newcommand{\Reward}[2]{\ensuremath{R(#1, #2)}}
\newcommand{\Oset}{\ensuremath{O}}
\newcommand{\Pfunc}[3]{\ensuremath{P(#1, #2, #3)}}
\newcommand{\trajectory}{\ensuremath{\tau}}
\newcommand{\stateS}{\ensuremath{s}}
\newcommand{\obsrvation}{\ensuremath{O(s,\tau)}}
\newcommand{\confidence}{\ensuremath{\upsilon}}
\newcommand{\threshold}{\ensuremath{t}}
\newcommand{\bigtau}{\mbox{\scalebox{1.5}{$\tau$}}}


\newcommand{\param}{\ensuremath{par}}


\newcommand{\operator}{\ensuremath{h}}
%\newcommand{\leftTime}[1]{\ensuremath{LT(#1)}}

\newcommand{\DronesNum}{\ensuremath{n}}
\newcommand{\targetsNum}{\ensuremath{m}}
\newcommand{\adviceNum}{\ensuremath{k}}

\newcommand{\timeToUpdate}{\ensuremath{z}}
\newcommand{\action}{\ensuremath{a}}
\newcommand{\deficulyLevel}[1]{d_{#1}}


\newcommand{\TG}{TG}
\newcommand{\AG}{AG}
\newcommand{\RM}{RM}
\newcommand{\detectiothreshold}{\ensuremath{hc}}
\newcommand{\pausetreshold}{\ensuremath{lc}}

%\newcommand{\alertAbove}[2]{\ensuremath{AL_a(#1, #2)}}
%\newcommand{\alertUnder}[2]{\ensuremath{AL_u(#1, #2)}}
\newcommand{\change}[2]{\ensuremath{CP(#1, #2)}}
\newcommand{\changeType}[2]{\ensuremath{CT(#1, #2)}}

%\newcommand{\detectionAlert}[1]{\ensuremath{da_\confidence}}
%\newcommand{\pauseAlert}[1]{\ensuremath{pa_\confidence}}
\newcommand{\costFunction}[1]{\ensuremath{C_{\operator}(#1)}}
\newcommand{\estimateCostFunction}[2]{\ensuremath{C_{\operator}(#1, #2)}}
\newcommand{\discountedFactor}{\gamma}
\newcommand{\wights}{\ensuremath{w}}
\newcommand{\velocity}{\ensuremath{vel}}
\newcommand{\altitude}{\ensuremath{alt}}
\newcommand{\UFunc}{\ensuremath{U}}
\newcommand{\Model}{\ensuremath{M}}
\newcommand{\parametersForU}{\Psi}
\newcommand{\paramForU}{\psi}

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

\begin{document}



\section{Appendix}

\subsection{The User Interface}


Similar to the goal described by Chen et al. \cite{chen2022multi}, our SAR system aims to provide the operator with comprehensive situational awareness while minimizing the need for low-level control of each drone. The \say{task mode} includes automated allocation of the sub-areas to drones, where each drone scans its designated sub-area based on the difficulty level defined for that sub-area. The drones search for targets and alert the operator to suspected targets.
In the \say{command mode}, our system offers the flexibility for the operator to manually assign sub-areas to specific drones, manually control a drone, or manually report targets as necessary.

The central part of the user interface has two modes: a map mode and a drone mode. 
In the map mode, the map is displayed, divided into sub-areas, showing the locations of the drones on the map.  Each drone is labeled with a number, as recommended in~\cite{hoang2023challenges}.
At the top, there are small images for each drone displaying what the drone sees (see Figure~\ref{fig:sar_simulator}). In the drone mode, a specific drone's view is shown in at the central area in a larger format, allowing the operator to manually control the drone and consider the specific details observed by the drone. For example, the operator can better handle a detected target in this mode (see Figure~\ref{fig:manual_control}).

The left-side panel contains four tabs (see Figure~\ref{fig:left_panel}):
\begin{itemize}
    \item \textbf{Drones} - This tab provides each drone a list of its assigned sub-areas, with the option to manually change it.
    \item \textbf{Areas} - This tab displays all sub-areas, allowing the operator to change the probability of finding a target in a specific sub-area and amend the sub-areas difficulty levels.
    \item \textbf{Status} - Controlling the simulation mode - choosing a scenario and switching between scanning and parameters phases. 
    \item \textbf{Parameters} - In this tab, the operator can set the parameters (altitude, velocity, and thresholds) for each area.
\end{itemize}


On the right-hand side, there is a panel containing the alerts from drones, which are displayed in light green, and other messages, such as those from the agent, which are displayed in light orange (see Figure~\ref{fig:sar_simulator}). 

In a preliminary experiment, we separated these messages into two different tabs: drone and agent messages. However, we noticed that this setup was less convenient for most operators given the high number of alerts from the drones. In particular, operators noted that they felt pressured to respond to them and often missed important messages from the agent. Therefore, in our ensuing human evaluation, we combined both types of messages into a single tab, but with different colors, providing the operator with a simpler way to see and distinguish the alerts and agent's messages. 

%For each area the rule to decide if it is better to change is type is:
%\begin{enumerate}
%    \item the current percentage of area scanned is more than $25\%$ and less then $60\%$.
%    \item the number of alerts above the threshold are less then $4*probability of area *preccentage of area scanned$. and there is other area type with lower thresholds.
%    \item the number of alerts above the threshold are more then $5$. and there is other area type with higher thresholds.
%\end{enumerate}
%In addition for each area type we create states for the following relevant options:
%\begin{enumerate}
%    \item increase the thresholds
%    \item decrease the thresholds
%    \item increase the velocity
%    \item decrease the velocity
%    \item increase the altitude
%    \item decrease the altitude
%\end{enumerate}
%If there is an intelligence message it get the highest rank. 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{media/manual_control.jpeg}
    \caption{Manual control panel, during handling detection alert.}
    \label{fig:manual_control}
    \Description{Manual control panel, during handling detection alert.}
\end{figure}
\label{sec:sar}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{media/left panel.png}
    \caption{The left panel contains four tabs: Drones, Areas, Status, and Parameters.}
    \Description{The left panel contains four tabs: Drones, Areas, Status, and Parameters.}
    \label{fig:left_panel}
\end{figure}

%]

\section{Our SAR Environment - Illustration Video}
\label{SARVideo}
Added here is a short illustration video of our SAR environment https://www.dropbox.com/scl/fi/14a9l4k4458jtwu8fswfc/Intelligent-Agent-Supporting-Human-Multi-Drone-Team-Collaboration.mp4?rlkey=9qr1vam2s131o60kjn346r7sr\&st=u15skxm0

\section{Data Collection with Human Operator}

For the purpose of training data collection, we delineated a set of feasible tasks for human operators and extracted relevant telemetry from these simulations, to subsequently be utilized by the AA agent. The following list enumerates all potential operator tasks:

\begin{itemize}
    \item Define the area type for each sub-area.
    \item Specify the probabilities of human presence in each area.
    \item Adjust existing area probabilities or difficulty levels.
    \item Prescribe altitude, velocity, thresholds: low-confidence (lc) and high-confidence (hc) corresponding to each area's type.
    \item Approve or reject detection alert.
    \item Manually operate drones.
    \item Handle malfunction alert.
    \item Manually detect humans - press a stop buttons and enforcing target acceptance.
    \item Manually modify the assignment of drones to sub-areas.
    \item Direct focus to specific cameras.
\end{itemize}

From each simulation with human operator we collect the following data:
\begin{itemize}
    \item The sequence of actions performed by the operator, including their timing.
    \item The temporal allocation of drones to specific sub-areas.
    \item The entire world's state each time.
    \item The drones' position at each time.
    \item Dashboards' screens, where user interaction was most pronounced.
    \item NASA Task Load Index (TLX) assessments conducted post-experiment.
\end{itemize}


\section{Mimicking Human Decision Making}
\label{SimulationRuns_appendix}
As discussed in Section $4.2$ of the paper, acquiring a substantial amount of data is crucial for developing a action generator and a ranking model that can mimic the decision-making trajectory of a human operator. In our simulation platform, we conducted simulations involving human operators, excluding the Advising Agent, to collect all relevant information for $QualityAssurance(D_{syn}, D, \Theta)$. 
After analyzing the collected data, we identified several essential parameters necessary for accurately mimicking the original operator's behavior:
\begin{itemize}
    \item Same initial probability for each search sub-area.
    \item Same area type that the operator assigned to each search sub-area.
    \item Scanning order of the search sub-area and allocation for each drone that followed the operator's real simulation (if the operator did not complete scanning all search sub-areas, we allocated them according to the probability of the areas in descending order).
    \item Adjustable parameters for each difficulty type (detection threshold, alert threshold, altitude, velocity).
    \item Same changing parameter actions that the operator took at the same timestamp as the actual simulation.
    \item True Positive (TP) rate - the probability of approving a detection when a target is in the drone's camera view.
    \item True Negative (TN) rate - the probability of rejecting a detection when a target is not in the drone's camera view.
    \item The average time for handling a detection alert.
    \item The average time for handling $CP$ action.
\end{itemize}
%\subsection{Data Evaluation}

%In the subsection of data evaluation by distances, we refer to the appendix for the tables with the distances. 
%See Figure \ref{fig:distances} for the distances and Figure \ref{fig:normlized_distances} for the normalized distances. 

%\%begin{figure}[t]
 %   \centering
 %   \includegraphics[width=\columnwidth]{media/Distances.png}
 %   \caption{Average distances from synthetic trajectories to real trajectories.}
 %   \label{fig:distances}
%\end{figure}
%\begin{figure}
%    \centering
%    \includegraphics[width=\columnwidth]{media/normlized_distances.png}
%    \caption{Normalized average distances from synthetic trajectories to real trajectories.}
%    \label{fig:normlized_distances}
%\end{figure}


% \subsection{Enhancing Action Coverage}
% \label{SimulationRuns}
% %Data collection: Methodology for generating virtual trajectories that mimic a human operator.
% %\subsection{Motive}
% To expand the scope of states and actions explored within our simulation, we introduced diverse simulated run types that emulate the real operator's behavior while introducing new actions to assess their effects. These actions included parameter changes ($CP$ actions) for various $\param \in \paramSet$, as well as different rules for prioritizing drone alerts.
% The synthetic trajectories generated served three main purposes: (1) Creating a training dataset with comprehensive coverage of states and actions. (2) Examining rules applicable to the AA agent. (3) Generating annotated trajectories for testing our model $M$, as elaborated in Section \ref{modules_evaluation} of the paper.
% %These approaches demonstrate the potential for utilizing artificial intelligence to optimize the management and processing of drone-generated data, enabling more efficient and effective decision-making.
% Here is a list of the types of automatic simulated runs we utilized to generate these synthetic trajectories: 
% \begin{itemize}
%     \item Runs encompassing the entire scenario without any $CP$ actions.
%     \item Runs with the same $CP$ actions (timing and values) as the original operator.
%     \item Runs involves $CP$ action for the thresholds according to a heuristic.
%     \item Runs involves $CP$ action for $\param\in \paramSet$ for increase $\param$ by $10$ after $5$ minutes.
%     \item  run involves $CP$ action for $\param\in \paramSet$ for decrease $\param$ by $10$ after $5$ minutes.
%     \item Runs with  handling stuck drone alerts.
%     \item Runs without handling stuck drone alerts.
%     \item simulations that the initial parameters were slightly higher or lower than the real values of the real operator.
% \label{modules_evaluation}
% \end{itemize}

% In managing messages from drones, we tried three different approaches were utilized. 
% \begin{itemize}
%     \item Prioritized messages based on their confidence
%     \item  First-in-first-out (FIFO) role. 
%     \item  messages were handled based on the probability of the area from which the message originated. 
% \end{itemize}
% For each type and operator, we compare the average number of valid targets found in the simulated following these three approaches to prioritized messages, and in almost all the cases we checked the average was higher in simulation following the confidence of the alerts, so we chose to use this rule in our ranking model.


\section{Operator Post-Scenarios Questionnaire}
 after finishing the four scenarios, each human operator filled out a detailed questionnaire.  Their answers were on a scale from $1$ to $5$.
 The questionnaire had four main parts, asking about different aspects of the operators' experiences.
\begin{itemize}
    \item  Personal Information - 
        \begin{itemize}
            \item Name
            \item Gender
            \item Age
            \item Occupation
        \end{itemize}

    \item  Background - 
        \begin{itemize}
            \item Have you flown a real drone before?
            \item Have you photographed with a drone before?
            \item How often do you play computer games?
            \item Have you played a computer game that simulates flying an aircraft?
        \end{itemize}
    \item  Post-Game Questions - 
        \begin{itemize}
            \item How comfortable was the interface for giving probability to areas?
            \item How comfortable was the interface for selecting the parameters for each area type?
            \item To what extent did the alerts from the drones help manage the simulation?
            \item To what extent did you feel that her actions were consistent with finding missing persons?
            \item How much did the experience on the simulator help you manage the simulation?
            \item To what extent did the explanations that appeared on the alert help you manage the simulation?
            \item How much did the manual driving option help you manage the simulation?
            \item How many drones do you think can be managed without an agent in a good way?
        \end{itemize}
    \item Post-Game Agent Questions - 
        \begin{itemize}
            \item In the simulation with an agent, were you able to manage the simulation better?
            \item In the simulation with an agent, did you experience less stress?
            \item How many drones do you think can be managed with an agent in a good way?
            \item In the simulation with an agent, how much did the drone alert rating help you manage the simulation?
            \item In the simulation with an agent, how much did the recommendations from the agent for changing parameters help you manage the simulation?
            \item In the simulation with an agent, how much did the recommendations from the agent to change the type of an area help you manage the simulation?
            \item In the simulation with an agent, how much did the notifications about a non-advanced drone help you manage the simulation?
            \item If you were the manager of a fleet of drones - how interested would you be in an agent?
        \end{itemize}
\end{itemize}





% combines all, the ranking model works in the following way ( \ref{fig:agent_structutre}):
% \begin{itemize}
%     \item Handle intelligent message if it exists.
%     \item Handle stuck drone if an alert exists on it or there is a suspected stuck drone - a drone that is not in progress although it not has an open alert.
%     \item Compare between the predicted rewards of the possible change parameters, and choose the one that gets the highest reward, denote this reward by $r_c$.
%     \item If there is a pause alert, create a predicted state after handling a pause alert. The reward for this action consists of the true positive rate (accepted pause alerts \ total pause alerts) plus the predicted reward of the future state. Denote this reward by $r_p$.
%     \item If there is a detection alert, create a predicted state after handling a detection alert.  The reward for this action consists of the true positive rate (accepted detection alerts \ total detection alerts) plus the predicted reward of the future state. Denote this reward by $r_d$.
%     \item  Compare $r_c$, $r_p$ and $r_d$.
%     \item If $r_c$ is highest, add the relevant action to the \adviceNum suggested actions.
%     \item Sort the queues of pause alerts and detection alerts according to the confidence,
%     \item In order to complete to the $3$ suggested actions, add pause or detection alerts (based on $r_p$ and $r_d$), from the top of the relevant queue.
% \end{itemize}.

\section{Centralized Real-Time Task Allocation Algorithm}
\label{CentralizedAlgorithm}

We used a genetic algorithm for path planning suggested in \cite{du2019evolutionary}. The algorithm was originally used for finding lost tourists in large national parks in China. According to the suggested algorithm, a team of drones is looking for a target based on probabilities of the target location and image analysis. This strategy made the algorithm a good choice for our use.

The algorithm's input is:
\begin{itemize}
    \item A - area
    \item Set of n drones $\{drone_1,...drone_n\}$
    \item $m$ sub-regions ${a_1, a_2,..., a_m}$ based on the topographic features.
    \item $a_j0$ - the initial location of $drone_j$
    \item $d_i$ - the distance from $a_{i0}$ to sub-area $a_i$
    \item $T$ - the maximum allowable time of the operation. 
    \item K - the number of search modes of the drones (the possible search altitudes)
    \item a prior probability  $p_t(i)$ of target location in each sub-area $a_i$ at time t $(i\in\{1...m\}, t\in\{1...T\})$  
    \item $\Delta T (i, j, k) $ - time required by drone $drone_j$ to search sub-area $a_i$ with mode k
    \item $\Delta t(i,i',j)$ - the time for drone $drone_j$  to fly from a sub-area $a_i$ to another $a_i'$.
\end{itemize}

The algorithmâ€™s output is a search path $x_j$ of each drone $drone_j$ such that the object can be detected as early as possible.

 $x_j=\{(a_{j,1}, k_{j,1}), (a_{j,2},k_{j,2}), ... , (a_j,m_j, k_j, m_j)\}$, where $\{a_{j,1}, a_{j,2}, ... , a_j,m_j \}$ is the sequence of sub-areas to be searched by $drone_j$ , and $k_{j,i}$ is the search mode used for the $i_{th}$ sub-area $a_{j,i} (1 \leq i \leq m_j)$.


 
Based on the search path $x_j$, the search times $\Delta \tau (i, j, k)$, and the flight times $\Delta t(i, i', j)$, the action of drone $drone_j$ at each time $t$ can be determined.
$x_t(j)=(i,k)$ are used to denote that $drone_j$ is searching in sub-area $a_i$ with mode k, and $x_t(j)=(i, i')^T$  to denote that $drone_j$ is flying from a sub-area $a_i$ to another $a_i'$ .

The objective function for the optimizations can be calculated in the following way:
Let $t^*$ be a hypothetical time at which the target is detected. iteratively the probability of $t^*=t$ is computed for all $t$ since the occurrences of detection by separate drones may be considered mutually exclusive.

$P(t^*=0)=0$

$P(t^*=t)=P(t^*=t \mid t^*\geq t)P(t^*=t)=[\sum_{i=1}^m\sum_{j=1}^n\sum_{k=1}^K{p_t (i) p_t (i,j,k \mid x_t (j)}] \times [1-\sum_{t'=0}^{t-1} P(t^*=t')]$

$p_t(i,j,k \mid x_t(j))=$ 
\[\begin{cases}
    (p_t (i,j,k),& \text{if }  x_t (j)=(i,k)\\
    0,              & \text{otherwise}
\end{cases} \]

The time complexity of the objective function $O(mnKT^2 )$.

A primary technique for growing a population of possible solutions to the problem and a sub-procedure for optimizing each drone path in the solution make up the genetic algorithm.

First, it initializes a population of $N$ solutions, including $N-1$ randomly generated solutions and a solution of a greedy method that selects the next step with the highest payoff (in terms of the ratio of the detection probability to the time consumed).
Then do the following steps until time has terminated:
For each solution $X$ and for each path $x_j$ in X, the sub-algorithm is used to optimize $x_j$ and evaluate the objective function for the solution X. After evaluating all objective functions, for each solution $\lambda(X)$, which denotes the migration rate of solution X, will be calculated and then migration or mutation will be performed on the solution.

The sub-procedure mentioned above for path optimization is as follows. Suppose a set $C_j \subset A$ of sub-areas have been assigned to drone $drone_j$. The sub-procedure produces the search path $x_j$ of $drone_j$ based on the NEH heuristic \cite{nawaz1983heuristic} and tabu search method \cite{glover1989tabu, glover1990tabu}.
The ratio of the overall detection probability to the entire time spent along the path is used to assess the fitness of a path $x_j$. The function is described as the probability that the $j_{th}$ drone search sub-area $a_i$  times probability at time $t$, the target will be at sub-area $i$ searched by drone $j$ with mode $k$, divided by the time required by drone $drone_j$ to search sub-area $a_i$ with mode $k$ for all $i$ in path plus flying time from each sub-area. 
%
The migration algorithm is based on the BBO metaheuristic (Biogeography-Based Optimization), and it allows a solution to migrate characteristics from other solutions.
For each solution in the population, a nonlinear model is used to calculate its migration rate $\lambda (X)$ as - $\lambda (X)=0.5-0.5cos(\frac{f(X)-f_{min}+\epsilon}{f_{max}-f_{min}+\epsilon} \pi)$
$f_max$ and $f_min$ are the population's highest and minimum objective function values.
In each generation, each solution X has a probability $\lambda(X)$ of importing characteristics from other solutions, which are chosen from the top half of the population with probabilities proportionate to their fitness.
A solution that is not migrated will be mutated by regenerating the search paths for some drones. Each solution X is assigned a mutation rate $\mu(X)$, initialized to 0.5 and updated at each generation as: $\mu(X)=\mu(X)\cdot \alpha^{-\frac{f(X)-f_{min}+\epsilon}{f_{max}-f_{min}+\epsilon}}$. The mutation randomly assigns a sub-area to drones which are selected for mutation. If a solution has not improved after {\it g} (a control parameter commonly set to 6) generations, it will be replaced with a new solution generated randomly to increase solution variety.

\bibliography{mybibfile}

\end{document}