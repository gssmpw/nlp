
\section{Evaluations}
% Need some info on the environment you created for running experiments.
%Recall that our advising agent relies primarily on two components: the offline reward estimation modeling and the online advice provision. Next, we discuss the details of the simulation used in the user study, followed by the two experiments: one targeting the reward estimation modeling and the other targeting the online advice provision.

Recall that the development of our advising agent relies primarily on two components. The offline:  training the reward estimation model.
The online: developing and testing the advising agent uses the reward estimation model in real-time to provide advice during the simulation. 
Next, we discuss the details of the simulation used in the user study, followed by two experiments. The first experiment was conducted to collect data necessary for training the model, while the second experiment aimed to evaluate the quality of the advice provided by the agent during the simulation.

%\subsection{Simulation}
\noindent\textbf{Simulation.}
We designed and used a simulated drone-based SAR environment within Microsoft's AirSim based on the SAR task definition provided in Section \ref{sec:sar_mission}. The environment features six drones and a large search zone resembling a town with buildings, roads, parks, and playgrounds. This town is based on the known unreal-engine map called \say{City Park} with small modifications (https://www.unrealengine.com/marketplace/en-US/product/city-park-environment-collection). 
Half of the search area was used for Experiment 1, whereas the other half was used for Experiment 2. In both cases, the entire map was divided into $25-50$ smaller sub-areas. 

\noindent\textbf{Participants.}
Overall, forty subjects participated in our experiments and were recruited in two batches: the first batch performed the SAR task without the assistance of the agent, and the second batch was asked to perform the SAR task with and without the agent (on different scenarios).
The participants ages range from $18$ to $62$ (mean $28$), $65\%$ males, and $35\%$ females. Most participants work in the tech industry or are Computer Science students with some experience in video games (a prerequisite for participation). Each subject in the first batch was paid \$26 and this batch was used for training.  In the second batch, each participant received a base payment of \$16 for their time and an additional 15 cents for each target they identified. This incentive was designed to encourage maximum effort during the simulations, where the primary evaluation metric was the number of targets identified.

 %  for their time and an additional 15 cents for each target they identified to encourage maximum effort during the simulations.

\noindent\textit{Training:}
Before engaging in the SAR task, each operator received a hands-on tutorial to familiarize themselves with the system. Then, at the beginning of the simulation, the operator received an initial intelligent message detailing the background story (e.g., a lost group of travelers). Then, the operator had $5$ minutes to configure the system as detailed in Section \ref{sec:sar_mission}, followed by 20 minutes to perform the task (i.e., to search and find as many desired targets as possible). 
Overall, four different scenarios, each encompassing a different number of targets (18-20), their locations, and the content of two intelligence messages were manually designed by the authors and explored through preliminary testing to ensure varying conditions. 
% Four scenarios were used for the reward estimation modeling and the other four were used for the advising agent evaluation. 
% Each operator engaged in all four scenarios.

\subsection{Experiment 1: Reward Estimation}
%todo
In this experiment, we used machine-learning techniques to estimate the long-term reward of performing an action at a given state. We examined whether the ordering over actions, induced by the reward estimation model, is correct by simulating the long-term effects of different actions and we evaluated the quality of the synthetic trajectories.
To focus on high-quality demonstrations, we considered the trajectories of operators of the first batch of subjects who had found at least one target during the simulation and denoted this set as $D$. 
% when only the sessions that were smooth and error-free were being considered.

\paragraph{\bf Machine Learning Performance}%new2
\label{modules_evaluation}
% 1 evaluation generating data of human behavior methodology results
% 2. Evaluation of the reward methodology 
The demonstrated trajectories, $D$, were used as input to Algorithm \ref{alg:generation}, resulting in approximately 1000 synthetic trajectories. 
We used two supervised learning models: a Random Forest (RF) \cite{randomForest} and a Long Short-Term Memory (LSTM) \cite{LSTM} to estimate the long-term reward of performing an action at a given state. To evaluate the models' accuracy, we used an 80-20\% train-test split and calculated the Mean Absolute Error (MAE). 
Overall, Random Forest provided a notably lower MAE than the LSTM ($0.55$ vs $1.05$).
Given that the utility function ranges from 1 to 22, the $0.55$ MAE demonstrates the model's effectiveness in supporting the selection of the appropriate recommendation action.
%todo
%the MAE was highest on states from the beginning of the trajectories and lowest on states from the end.

%Furthermore, an additional investigation involved training a model for $8$ operators with an identical utility function, while retaining all their synthetic trajectories within the test set. This endeavor yielded MAE values ranging from $0.5$ to $0.69$, contingent upon the specific mimic operator encompassed within the test set. These models are used for the tests described below.


\paragraph{\bf Ranking Model}%new3
\label{sec:action_ordering}
%hodaya - add reference to experiments
We examined whether the estimation model leads to good action recommendations. We focused on the $CP$ action for $\detectiothreshold$ and $\pausetreshold$ since it had been the most challenging action to evaluate and rank and had rarely been performed by human operators without the assisting agent. To simulate the long-term effect of performing the $CP$ action for $\detectiothreshold$ and $\pausetreshold$,  for each $d\in D$, we generated $30$ new synthetic trajectories using the same procedure as in Algorithm \ref{alg:generation}.
In the third of them, the Pertuable function did not perform $CP$. In another third of them, a $CP$ action that increased $\detectiothreshold$ and $\pausetreshold$ was performed in the middle of the trajectory, and in the third, a $CP$ action that decreased $\detectiothreshold$ and $\pausetreshold$ was performed also in the middle of the trajectory.

For each $d \in D$, we computed the average number of targets found in each type of synthetic trajectory, resulting in three averages per $d$: one for synthetic trajectories without the $CP$ action, one for synthetic trajectories with the $CP$ action that increases thresholds, and one for synthetic trajectories with the $CP$ action that decreases thresholds. We then checked if our model gave the highest rank to the action that led to the highest average among these three possibilities.
Then, for each $d \in D$, we trained a model with the same utility function, but in which the synthetic trajectories of $d$ were in the test set, and used this model to check if the action that received the highest rank was indeed the one corresponding to the synthetic trajectories where the highest average number of targets was found.
\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.75\columnwidth]{media/stuck_half_slots.PNG}
    
    \caption{Orange bars are the average number of true targets that were approved. Blue bars are the number of simulations that were performed in this category.}
    \label{fig:stuck-half-label}

\end{figure}
\noindent{\it Results.}
\begin{itemize}[leftmargin=10pt,topsep=0pt]
    \item For three $d\in D$, the highest average number of targets found was in the synthetic trajectories generated without performing $CP$. In these cases, our model correctly gave the highest rank for handling detection alert action and avoided recommending $CP$.
\item For five  $d\in D$, the highest average number of targets found was in the synthetic trajectories generated by performing the $CP$ action that increases $\detectiothreshold$ and $\pausetreshold$.
Within $11\%$ of the states, the recommendation derived from model $M$ accurately advised an increase of the detection threshold, while in $89\%$ of the states, the recommendation suggested handling a detection alert. 
Notably, the model correctly did not advise decreasing the threshold.
    
    \item  There were no  $d\in D$, such that the highest average performance in the synthetic trajectories was of the  synthetic trajectories generated by performing the $CP$ action that decreased $\detectiothreshold$ and $\pausetreshold$
\end{itemize}
The results highlight the robustness of our model, as it consistently avoids generating recommendations that could be proven incorrect. Additionally, since human operators rarely performed $CP$ actions in scenarios without the agent's guidance, the agent’s cautious approach to recommending these actions proved advantageous, as demonstrated in Experiment 2 (section \ref{EX2}). This cautious strategy resulted in a high acceptance rate of the agent’s recommendations by the participants, leading to a significant improvement in team performance.

% \paragraph{Alert prioritization}

% It was observed that in some missions, the rule based on confidence resulted in the highest average total number of true targets detected, and in other missions, the FIFO rule resulted in the highest average total number of true targets detected.
% Since handling the alerts according to confidence was observed in almost all human operators in simulations without agents, this approach was selected.


%For this purpose, we generated 12 new synthetic trajectories of length 5min, 10min, and 15min using the same procedure as in Algorithm \ref{alg:generation}. For each of these trajectories, we focus on the final state and examine a set of possible actions $A'$. For each possible action $a\in A'$ we continued the trajectory $5$ times by performing the action $a$ followed by the same procedure as before. That is, we simulate the long-term effect of performing each action from that state and consider the average number of targets found. 
%Finally, we compare the ranking induced by model $M$ to the ranking induced by the above-calculated simulations and validate our decision to prioritize malfunction alerts. 

%\paragraph{Ranking Quality}

%We use the Mean Reciprocal Rank (MRR) metric \cite{} that measures the average ???. 
%OTHERWISE, CONSIDER SARIT'S OPTION...

% \paragraph{Detection Alert's Ranking}
% In order to deciding on the ranking of different detection alerts, we generated synthetic trajectories but these simulations with the applied rules, it was observed that in some missions, the rule based on confidence resulted in the highest average total number of true targets detected, and in other missions, the FIFO rule resulted in the highest average total number of true targets detected.
% Since handling the alerts according to confidence was observed in almost all human operators in simulations without agents, this approach was selected.

\paragraph{Handling Malfunction Alert Prioritization.}
Recall that our ranking model favored handling drone malfunction alerts. We validated this heuristic based on the demonstrations from human operators $D$ examining the relation between the time that passed between the appearance of a drone malfunction alert and the time that the operator handled the alert.

\textit{Results:}

Figure \ref{fig:stuck-half-label} displays the average number of targets found, divided into bins based on the time that passed between the appearance of a drone malfunction alert and the time that the operator handled this alert (orange), and the number of human operators present in these bins (blue). The results show that the simulations in which the operator waited before handling the stuck drone alert typically resulted in fewer targets found supporting our heuristic choice.


%hodaya - chart should include one of them

%EITHER CHANGE ONLY TO SCIENRIO 1 OR COMBINE ALL TO A SINGLE BAR CHART
\paragraph{\bf Quality Assurance}
For this evaluation, we used the second batch. We chose one of the scenarios and collected the trajectories of the human subjects that did not get the agent's assistance. This yielded ten trajectories.
We generated $6$ synthetic trajectories for quality assurance for each real one. These trajectories were analyzed in two ways: using a distance-based approach and using a clustering analysis approach.  

\noindent\textbf{(1) Distance:} 
Starting with the distance-based analysis, we examined the distances between the synthetic and real trajectories of each operator in two ways: First, for each operator, we examined whether the synthetic trajectories indeed best resembled the original using a standard distance measure. Second, for each operator, we calculated the average distance of the six synthetic trajectories to each one of the real trajectories and normalized it by dividing it by the sum of the average distances of the synthetic trajectories to all real trajectories.
Thus, for each operator, the sum of the normalized distances from the synthetic trajectories to the real trajectories was $1$. Using statistical testing, we examined whether the synthetic trajectories better resemble the originating one than any other real trajectory.  

\textit{Results:}
For $6$ out of the ten operators, all $6$ synthetic trajectories were the most similar to the real trajectory of their operator (see Figure~\ref{fig:normlized_distances_paper}.
For $3$ additional operators, all $6$ synthetic trajectories were second most similar to their operator's real trajectory.
 
The maximum normalized distance from the averaged synthetic trajectories to the real trajectory of the same operator was $0.09$ and the average was $0.04$. That is, for each operator, the distance between her synthetic trajectories and her real trajectory was lower than the average distance between her synthetic trajectories and any real trajectory (i.e., $0.1$). 
% the table with the distances are in the appendix. 
\begin{figure}[tbp]
    \centering
    \includegraphics[width=\columnwidth]{media/normlized_distances.png}
    \vspace{-10pt}
    \caption{Normalized distances between real trajectories $(A-J)$ and the averaged synthetic trajectories ($synthetic\_A-synthetic\_J$.)}
    \label{fig:normlized_distances_paper}
\end{figure}

\noindent\textbf{(2) Clustering:} We used a clustering-based analysis with a K-means algorithm applied to all trajectories alike, subject to the condition that each real trajectory should be assigned to a different cluster. Then, we examined the portion of synthetic trajectories that were assigned to the same cluster as their originating one.   
\textit{Results:}
For 58 of the 60 synthetic trajectories, the synthetic trajectories were assigned to the same cluster as their originating real ones.  The remaining two synthetic trajectories originated from a single human operator.
Based on the evaluations using the two measurements, we concluded that our method for generating synthetic trajectories from human trajectories successfully produced trajectories that closely resembled the original ones.

\subsection{Experiment 2: Advising Agent}
\label{EX2}
We tested three hypotheses stated below.
\begin{itemize}[leftmargin=10pt,topsep=0pt]
 \item {\bf H1} The advising agent improves the team's performance as measured by the number of targets found and the covered area.
\item {\bf H2} The human operators are satisfied by the advising agent, improving their perceived ability to manage more drones effectively. 
\end{itemize}
While the agent guides the human operator on which actions to take, it also introduces additional tasks alongside those generated by the drones. The following hypothesis proposes that these additional tasks do not increase the operator's cognitive load, as the benefits of the agent's advice on the action to take next, effectively offset the added demands.
\begin{itemize}[leftmargin=10pt,topsep=0pt]
\item {\bf H3} The agent's assistance does not substantially alter the mental demands on the human operators.
\end{itemize}

\noindent\textbf{Experimental Procedure.}
Human operators from the second batch participating in the study completed four scenarios in a fixed order. Half of them first completed the initial two scenarios with the help of our agent, followed by two scenarios without it. The other half started with two scenarios without our agent, followed by two with its help. 
%each scenario was limited to $20$ minutes. To ensure competence, a structured tutorial preceded the runs, guided by a research assistant. 
The experiment was conducted in two sessions, each of two scenarios, to mitigate fatigue. After each session, operators filled out a standard NASA-TLX questionnaire \cite{Hart1988} to assess workload. Upon completing both sessions, a final questionnaire regarding the entire experiment was administered.

\noindent\textbf{Data processing.}
The recorded sessions were analyzed using three key analyses: First, we statistically evaluated the accumulated rewards and other performance metrics (e.g., the area covered) of the human-multidrone team performance with and without our agent's assistance. Second, we considered the questionnaire reporting.  
Last, we statistically evaluated the cognitive load on the operator with and without our agent's assistance.

\noindent\textbf{Results.}
%A preliminary examination of the recorded data revealed two abnormalities: First, a single operator did not find any targets in a particular scenario. This was a highly irregular outcome, both considering other operators and the same operator in other scenarios. Since we could not explain this irregularity, and in order to avoid potentially biasing the data due to a possible confounding factor (i.e., the operator was not feeling well), we decided to omit all samples originating from that operator. Second, a remarkably low number of found targets was detected in scenario number $3$, regardless of agent presence or lack thereof. A posthoc investigation suggests that the scenario was too complex for most operators, resulting in low human-multidrone team performance. In order to avoid introducing underquality data into the analysis, we omitted the scenario from further consideration.
As illustrated in Figure \ref{fig:results_graphs}, we observed a statistically significant improvement in the number of targets found, with a $20\%$ increase ($p=0.04$, via one-sided t-test), as well as a significant $26.2\%$ enhancement in the percentage of the search area scanned ($p=0.002$, via one-sided t-test). {\it Thus, the data supports H1.}
This could be explained by observing the acceptance of the agent's generated advice, our analysis indicates that, in most instances, the operators followed the agent's recommendations.
We observed a statistically significant increase in the number of $CP$ or $CT$ actions ($p=0.0004$, via one-sided t-test). In scenarios where the advising agent was present, operators executed actions like $CP$ or $CT$ an average of $6.75$ times, with $4.25$ (blue column) of those actions being a direct result of the agent's advice. In contrast, operators performed such actions only $2.25$ times on average when our agent was not present.  

When observing the handling of the detection alerts, when the agent was present, operators preferred handling the alerts coming from the agent (yellow column) over alerts that were not in the advice section (turquoise column). We observed a significant number of detection alerts handled outside of the advice section. This can be explained by the time lag required for updating the advice list (a few seconds due to calculations). 
% \footnote{Please note that the results in this section pertain to the first version of the agent.}

The questionnaire results (Figure \ref{fig:sar_survey})
suggested a strong preference among the participants for the advising agent.
In addition, most participants expressed a preference for using the agent in SAR tasks, confidently stating that it would enable them to handle more drones successfully. Specifically, the average number of drones human operators believed they could manage with the agent was $7.54$, compared to $6.83$ without the agent ($p=0.03865$ via one-sided t-test). We observed that the operators were generally pleased with the agent's advice regarding $CP$, $CT$, and stuck drone messages. Overall, the participants in our study presented favorable perceptions of the agent. {\it Thus, the data supports H2.}

Considering the cognitive load, the TLX scores indicate that the overall cognitive load was not statistically different between the two setups, although it was slightly lower when the agent was present ($57.52$ with the agent vs. $58.94$ without the agent, see Table~\ref{tab:tlx} for details). Also, the questioner's results were aligned with the NASA-TLX results; it was evident that the agent did not introduce any additional cognitive load to the participants' perceived stress levels.
{\it Thus, the data supports H3.}

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
         & Agent & No agent \\
        Effort & 60.11, 3.44 & 61.35, 3.38 \\
        Frustration & 46.82, 1.94 & 45.97, 1.61 \\
        Mental demand & 58.23, 2.94 & 60.14, 3.02 \\
        Performance & 48.67, 3.67 & 54.55, 4.41 \\
        Physical demand & 28.44, 0.52 & 22.26, 0.79 \\
        Temporal demand & 55.14, 2.47 &  54.94, 2.76 \\
    \end{tabular}
    \caption{NASA-TLX, the first number is the rating, the second is its weight in the overall score calculation.}
    \label{tab:tlx}
\end{table}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{media/agent_questionnaire.PNG} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width. This setup will avoid overfull boxes.
\vspace{-10pt}
\caption{ \small A survey on SAR task with the advising agent}
\label{fig:sar_survey}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{media/results_graphs.JPG} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\vspace{-10pt}
\caption{ \small The first two graphs present the performance with the help of the agent and without it. The third graph presents agent-human interaction within missions. }
\label{fig:results_graphs}
\end{figure}
% Additional research may be warranted to investigate the generation of supplementary tasks for the operator while minimizing any adverse impact on stress and frustration levels.
% These operators formed the subject pool through which we assessed the efficacy of our agent and its impact on SAR task performance.
% Our first hypothesis (H2) aimed to assess whether the presence of the advising agent would lead to higher rewards in SAR missions. The results of our experiments validate the significant improvement achieved by the advising agent. It successfully enhances the primary aspects of task success - target detection and effective exploration of the search area. These findings provide compelling evidence supporting the necessity of an agent in this domain, as it substantially enhances operational outcomes.
% Future research efforts could focus on reducing the cognitive load and stress levels of operators.
% The operators pointed out that they are becoming more successful as they gain more experience with the simulation. 
% This suggests that increased exposure to the simulator could potentially yield even more favorable results, warranting further exploration in refining training methodologies to capitalize on this experiential advantage.
%H1: The accumulated rewards of the team of drones an operator when the AA supports the operator will be higher than without an agent.

%H2: The cognitive load on the operator decreases with the AA support compare to the no agent condition.

% In this article, we proposed an advice provision methodology implemented and evaluated in simulated SAR environments. 
\section{Conclusions}
%Operators will persistently assume a fundamental role in tasks requiring coordination with a drone team, especially within collaborative SAR missions. In the process of constructing models reliant on human task performances, inherent constraints, and limitations inevitably emerge, encompassing factors such as human time consumption and the inherent sparsity of data. In this article, we have presented a comprehensive methodology encompassing the design, implementation, and evaluation of an advising agent tailored specifically for SAR missions. Through an extensive examination of our research hypotheses, we have garnered profound insights into the multifaceted contributions of the advising agent in enhancing operator performance. The experiments we conduct ensure that the presence of The AA significant improve the number of victims found and the percentage of area scanned. Subsequent research endeavors may be directed towards reduce the cognitive load and stress levels of the operators.


Operators play a pivotal role in multi-drone systems, particularly in collaborative missions such as Search and Rescue (SAR) which require human supervision and control. In this article, we presented an advising agent methodology that we implemented and evaluated extensively with human participants in a simulated multi-drone SAR setting. Our results suggest that human-multi-drone team performance can be significantly improved through the introduction of the advising agent. In order to effectively generate the advice, the results further show that closely mimicking human operators' actions is possible and requires only a limited number of demonstrations. Additionally, the introduction of the advising agent does not seem to bring about any increase in the operators' cognitive load (if any, it seems to slightly reduce it),  and participants expressed a strong preference for the use of the advising agent while asserting that it would enable them to effectively manage a larger number of drones. Taken jointly, our results seem to suggest that the integration of advising agents in complex human-in-the-loop systems, and especially human-multi-drone teams, may be advantageous and may be achieved with only limited human demonstrations. 


