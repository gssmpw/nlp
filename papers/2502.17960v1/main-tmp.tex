%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
%\documentclass[11pt]{article}
%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas} 

\documentclass[sigconf,anonymous]{aamas} 

%%% Load any packages you require here. 

%\usepackage{latexsym}
%\usepackage{amssymb}
%\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{booktabs}
\usepackage{enumitem}
%\usepackage{graphicx}
\usepackage{color}

%\usepackage{times}  % DO NOT CHANGE THIS
%\usepackage{helvet}  % DO NOT CHANGE THIS
%\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage{url}  % DO NOT CHANGE THIS
%\usepackage{upgreek}
\usepackage{dirtytalk}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{hyperref}

%\usepackage[noend]{algpseudocode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any theorem-like environments you require here.

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any new commands you require here.

%\newcommand{\BibTeX}{B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}
\newcommand{\AreasIdSet}{\ensuremath{E}}
\newcommand{\areaId}{\ensuremath{e}}
\newcommand{\paramSet}{\ensuremath{\uprho}}


\newcommand{\Sset}{\ensuremath{S}}
\newcommand{\Aset}{\ensuremath{A}}
\newcommand{\Reward}[2]{\ensuremath{R(#1, #2)}}
\newcommand{\Oset}{\ensuremath{O}}
\newcommand{\Pfunc}[3]{\ensuremath{P(#1, #2, #3)}}
\newcommand{\trajectory}{\ensuremath{\tau}}
\newcommand{\stateS}{\ensuremath{s}}
\newcommand{\obsrvation}{\ensuremath{O(s,\tau)}}
\newcommand{\confidence}{\ensuremath{\upsilon}}
\newcommand{\threshold}{\ensuremath{t}}
\newcommand{\bigtau}{\mbox{\scalebox{1.5}{$\tau$}}}


\newcommand{\param}{\ensuremath{par}}


\newcommand{\operator}{\ensuremath{h}}
%\newcommand{\leftTime}[1]{\ensuremath{LT(#1)}}

\newcommand{\DronesNum}{\ensuremath{n}}
\newcommand{\targetsNum}{\ensuremath{m}}
\newcommand{\adviceNum}{\ensuremath{k}}

\newcommand{\timeToUpdate}{\ensuremath{z}}
\newcommand{\action}{\ensuremath{a}}
\newcommand{\deficulyLevel}[1]{d_{#1}}


\newcommand{\TG}{TG}
\newcommand{\AG}{AG}
\newcommand{\RM}{RM}
\newcommand{\detectiothreshold}{\ensuremath{hc}}
\newcommand{\pausetreshold}{\ensuremath{lc}}

%\newcommand{\alertAbove}[2]{\ensuremath{AL_a(#1, #2)}}
%\newcommand{\alertUnder}[2]{\ensuremath{AL_u(#1, #2)}}
\newcommand{\change}[2]{\ensuremath{CP(#1, #2)}}
\newcommand{\changeType}[2]{\ensuremath{CT(#1, #2)}}

%\newcommand{\detectionAlert}[1]{\ensuremath{da_\confidence}}
%\newcommand{\pauseAlert}[1]{\ensuremath{pa_\confidence}}
\newcommand{\costFunction}[1]{\ensuremath{C_{\operator}(#1)}}
\newcommand{\estimateCostFunction}[2]{\ensuremath{C_{\operator}(#1, #2)}}
\newcommand{\discountedFactor}{\gamma}
\newcommand{\wights}{\ensuremath{w}}
\newcommand{\velocity}{\ensuremath{vel}}
\newcommand{\altitude}{\ensuremath{alt}}
\newcommand{\UFunc}{\ensuremath{U}}
\newcommand{\Model}{\ensuremath{M}}
\newcommand{\parametersForU}{\Psi}
\newcommand{\paramForU}{\psi}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{227}
\title{Advising Agent for Supporting Human-Multi-Drone Team
Collaboration}
%%% Provide names, affiliations, and email addresses for all authors.
  
\author{a}
\affiliation{
  \institution{Bar Ilan university}
  \city{ramat gan}
  \country{israel}}
\email{aa}

\begin{abstract}
Multi-drone systems have become transformative technologies across various industries, offering innovative applications. However, despite significant advancements, their autonomous capabilities remain inherently limited. As a result, human operators are often essential for supervising and controlling these systems, creating what is referred to as a \say{human-multi-drone team}. In realistic settings, human operators must make real-time decisions while addressing a variety of signals, such as drone statuses and sensor readings, and adapting to dynamic conditions and uncertainty. This complexity may lead to suboptimal operations, potentially compromising the overall effectiveness of the team. In critical contexts like Search And Rescue (SAR) missions, such inefficiencies can have costly consequences.
This work introduces an advising agent designed to enhance collaboration in human-multi-drone teams, with a specific focus on SAR scenarios. The advising agent is designed to assist the human operator by suggesting contextual actions worth taking. To that end, the agent employs a novel computation technique that relies on a small set of human demonstrations to generate varying realistic human-like trajectories. These trajectories are then generalized using machine learning for fast and accurate predictions of the long-term effects of different advice.
Through human evaluations, we demonstrate that our approach delivers high-quality assistance, resulting in significantly improved performance compared to baseline conditions.

\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003145.10003151.10011771</concept_id>
       <concept_desc>Human-centered computing~Visualization toolkits</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003145.10011770</concept_id>
       <concept_desc>Human-centered computing~Visualization design and evaluation methods</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003122.10003334</concept_id>
       <concept_desc>Human-centered computing~User studies</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003122.10010855</concept_id>
       <concept_desc>Human-centered computing~Heuristic evaluations</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003129.10010885</concept_id>
       <concept_desc>Human-centered computing~User interface management systems</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Human-centered computing~Visualization toolkits}
\ccsdesc[100]{Human-centered computing~Visualization design and evaluation methods}
\ccsdesc[300]{Human-centered computing~User studies}
\ccsdesc[300]{Human-centered computing~Heuristic evaluations}
\ccsdesc[300]{Human-centered computing~User interface management systems}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Multi-Drone Systems, Human-in-the-loop, Advising Agent, SAR}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Advising Agent for Supporting Human-Multi-Drone Team Collaboration}

%%
%% The "author" command and its associated commands are used to define
%%
%% The abstract is a short summary of the work to be presented in the
%% article.








%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%\input{writeup/01-introduction}
 \onecolumn
\section*{Advice Provision}
\label{sec:formal}
\Large

\subsection*{Formalization}
We formally introduce the advice provision problem as a Markov Decision Problem (MDP) $<S,A,P,R,\gamma>$ \cite{Puterman1994}.

Let us consider a set of $n$ semi-autonomous drones engaged in a cooperative task, supervised and controlled by a single human operator, $\operator$.
The state space $S$ consists of all contextual information regarding the drones (e.g., status, altitude) and the task's and operator's domain-specific characteristics (e.g., time elapsed, performance). The operator can perform an action $a\in A$ during the task, at any time $t\in [0,\ldots T]$. Since $\operator$ can choose to execute no action at any given $t$, $NULL\in A$. In addition, not every action is possible at each state, and therefore, we denote $A(s)$ as the set of applicable actions at state $s$.
Let $\Pfunc{\stateS}{\action}{\stateS'} \ $
be the transition function that denotes the probability of transitioning from state $\stateS$ to state $\stateS'$ following action $a$.
Clearly, for each $\stateS \in S$, 
$\sum_{s' \in S} \Pfunc{\stateS}{\action}{\stateS'}=1$.
Note that, when $\action = NULL$, it holds that $\stateS \not= \stateS'$ since the environment is dynamically changing.  
Finally, let $R(s)$ be the domain-specific reward function,
%hodaya: i remove this: $\Omega$ be the set of observations, $O$ be the domain-specific observation function, 
and $\gamma\in[0,1]$ be the discount factor.
Importantly, performing an action takes time, depending on the operator's ability. The transition function and the cost function are unknown to the advising agent, yet they can be estimated through observations. %hodaya: Use observations here?

Advice is guidance provided by an agent to the human operator as an action that the operator should take ($a \in A$). 
Crucially, the operator maintains the autonomy to assess the advice, consider alternative actions, and make a final decision (i.e., the advice is non-binding). At any point in time $t$, the agent may provide advice according to its advising policy $\pi$, a mapping from states to actions.
In an ideal setting, we would like the operator to follow an optimal policy, $\pi^*_\operator: S \rightarrow A$, that maximizes the expected accumulative future reward. However, the dynamic and uncertain environment causes the underlying optimization problem to be intractable and thus, an optimal policy cannot be computed in a reasonable time. To overcome this limitation, we propose a methodology for computing high-quality advice without explicitly deriving an optimal policy. 

\subsection*{Proposed Methodology}

We propose an advice provision methodology consisting of two phases: an offline phase which is targeted at estimating the expected long-term reward of performing an action (i.e., reward estimation) using limited human demonstrations and a machine learning model; and an online phase, primarily designed for providing a suitable advice in a given state.   


% adopt the 1-step-lookahead rolling-window heuristic which was found to be highly useful in similar advice provision settings  \cite{Rosenfeld2017collaboration}. %Ariel: can you please cite some other works as well? possibly not from robotics...
% Specifically, at each time $t$, the agent outputs $a$ which minimizes (maximizes) the expected reward \textit{assuming the agent can only suggest a single piece of advice}.
\newpage

\noindent{\bf Offline: Reward Estimation Model}


As noted before, a central component in solving the underlying optimization problem is the proper estimation of the expected benefit from performing an action. To that end, we assume that a set of demonstrations is available, $D$,  where $d\in D$ is a trajectory $\trajectory$ consisting of state-action pairs, $((s_0,a_0),...,(s_T,a_T))$, denoting the actions that the operator took at each time and state. If $D$ is \say{sufficiently large and comprehensive}, then a machine learning model could be readily trained based on these demonstrations to approximate the effects of performing an action at a given state and time. However, as noted before, collecting such a set of demonstrations is typically highly expensive and time-consuming. Therefore, we assume $D$ is small and needs to be extended before it can be effectively used for training a machine learning model.We propose the generation of high-quality \textit{synthetic} trajectories that mimic the real demonstrations in an offline fashion (i.e., before actual deployment of the advising agent). 
In other words, we start by estimating the time it takes for a given operator to perform different actions using the entire trajectory, $\Tilde{C}_h(a)$ (e.g., using an average). Then, we go through a real trajectory ($d\in D)$ and pertubate it to create slight changes (e.g., choosing another action at a given probability). The action is then applied in a simulated environment using the transition function (e.g., the transition can also consider the operator's performance), a new state is reached and the process is repeated. Overall, $k$ synthetic trajectories are generated based on each real one. 
The resulting set of generated trajectories is then checked to ensure that it, indeed, closely mimics the real ones (i.e., quality assurance using statistical measures). If so, a machine learning model is trained on the \textit{extended} set of demonstrations (otherwise, an \say{unsuccessful} message is raised). 
The trained model is utilized next in the online phase to rank the various actions.

\noindent{\bf Online: Advice Provision}
During deployment, the agent employs an online advice provision policy as follows:  \\
%\begin{enumerate}
{1. \bf Action generator}: Given state $s$, the agent generates a set of possible actions that can be performed at this state and time point. These actions may be the result of a drone-initiated event (e.g., a drone malfunction) and/or other actions aimed at improving performance (e.g., changing a drone's altitude for better coverage). It then evaluates them using the reward estimation model obtained in the offline phase. \\
{2. \bf Ranking}: All generated actions and their associated expected reward are provided to a ranking model that outputs the top $c$ actions as advice for the operator. See Figure \ref{fig:agent_structure} for an illustration. 
% (complex actions is a set of some actions from the same type).
%\end{enumerate}
 
In other words, once a new state is encountered, the action generator creates possible actions to take which are then evaluated using the offline trained model and ranked accordingly.   

% Once a reward estimation function is trained using Algorithm \ref{alg:generation}, the advising agent can use it in the online setting. 

% Another use of the simulated runs is for model validation. Start a simulated run and stop at a certain point. Then, from that point onward, run several simulated runs, each one starting with a different action but continuing with the same method of choosing actions until the end of the run. This way, we can determine which action leads to the best outcome. Since randomness is inherent in the environment itself, it's necessary to run several simulated runs for each action and statistically check if there is a recommended action at that time point and whether the model indeed tends to give it a higher value.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{media/agent_general_structure.png}
\caption{The advising agent design. $s$ denotes the state, $a_i\in A$ denotes an action, $s'$ denotes the expected resulting state and $v_i$ denotes the predicted reward from the transition.}
\Description{Diagram of the advising agent's design, showing states, actions, resulting states, and predicted rewards.}
\label{fig:agent_structure}
\end{figure}

%Once we have the prediction model, the agent can be implemented. 
%\input{writeup/02-Formalization}
\input{writeup/02-TheSARProblem}
\input{writeup/03-TheAAAgent}
\input{writeup/05-experiment}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
%\begin{acks}
%To Robert, for the bagels and explaining CMYK and color spaces.
%\end{acks}
\bibliographystyle{ACM-Reference-Format}
\bibliography{mybibfile}
\end{document}
%%
%% Print the bibliography
%%
%\printbibliography

%%
%% If your work has an appendix, this is the place to put it.
%\appendix

%\section{Research Methods}


\endinput
%%
%% End of file `sample-sigconf-biblatex.tex'.
