\documentclass[11pt]{article}

% Change ``review'' to ``final'' to generate the final (sometimes called camera-ready) version.
% Change to ``preprint'' to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{inconsolata}

\title{Towards Effective Extraction and Evaluation of Factual Claims}

\author{
  Dasha Metropolitansky, Jonathan Larson \\
  Microsoft Research \\
  \texttt{\{dasham,jolarso\}@microsoft.com}
}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{array}
\usepackage{lscape} 
\usepackage{import}
\usepackage{adjustbox} 
\usepackage{xcolor}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{ragged2e}
\usepackage{amsmath, amssymb}
\usepackage[most]{tcolorbox}
\usepackage{natbib}
\usepackage{placeins}

\definecolor{lightgreen}{RGB}{204,255,204}
\definecolor{lightred}{RGB}{255,204,204}

\newcommand{\tightarrow}{\!\Rightarrow\!}
\newcommand{\tightnarrow}{\!\nRightarrow\!}
\newcommand{\tinyand}{\hspace{1pt}\land\hspace{1pt}} 

\renewcommand{\sectionautorefname}{\S\!}
\renewcommand{\subsectionautorefname}{\S\!}
\renewcommand{\subsubsectionautorefname}{\S\!}

\makeatletter
\newcommand*{\appautoref}[1]{%
  \hyperref[#1]{Appendix~\ref*{#1}}%
}
\makeatother

\begin{document}
\maketitle
\begin{abstract}
A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.
\end{abstract}

\section{Introduction}
It is well known that Large Language Models (LLMs) are prone to producing content that lacks grounding in external knowledge sources \citep{huang:2025}. As LLM-generated content grows in volume and influence, reliable fact-checking systems become increasingly important. 

For long-form, information rich outputs, a common fact-checking strategy is to extract simple ``claims'' from the text, then retrieve relevant evidence and assess the veracity of each claim independently \citep{min:2023, hu:2024:decompdilemmas}. The effectiveness of such ``decompose-then-verify'' systems is contingent on the quality of the extracted claims: misrepresenting the source text or omitting factual content can result in misleading or incomplete conclusions. Therefore, rigorous evaluation of claim extraction methods is critical. 

While prior works have identified desirable properties of claims, classified common errors, and shown that fact-checking performance is sensitive to the decomposition method, there is currently no standardized approach for evaluating claim extraction \citep{hu:2024:decompdilemmas, wanner:2024:decomp}.

This paper makes the following contributions:
\begin{enumerate}
    \item We propose a framework for evaluating claim extraction methods in the context of fact-checking. We also introduce automated, scalable, and replicable methods for applying this framework, which are validated through human review. Two key innovations are: (1) a granular assessment of claims’ coverage of the source text, and (2) an outcome-based approach for evaluating decontextualization (i.e., whether a claim contains all necessary contextual information).
    \item We introduce Claimify, an LLM-based claim extraction method. We demonstrate that it outperforms existing methods under our evaluation framework. To the best of our knowledge, Claimify is also the first claim extraction method that identifies sentences with multiple possible interpretations and determines when the correct interpretation cannot be inferred from the sentence’s context – unlike existing methods, which either ignore ambiguity or assume it is always resolvable.
\end{enumerate}

\section{Evaluating Claim Extraction}
\label{sec:framework}
\subsection{Key Concepts}
\label{subsec:framework_definitions}

The definition of a ``claim'' varies across prior works \citep{daxenberger:2017}. We adopt the perspective from \citet{ni:2024}, which focuses on statements that ``present verifiable facts,'' where a fact is ``a statement or assertion that can be objectively verified as true or false based on empirical evidence or reality.'' We use the term ``factual claim'' throughout this paper instead of the full phrase ``verifiable factual claim.''

We argue that in the context of fact-checking, claim extraction methods should be evaluated based on three factors:
\begin{enumerate}
    \item \textbf{Entailment} means that if the source text is true, the extracted claims must also be true. The broader principle that the source text should support the claims has been described in previous works as faithfulness \citep{wright:2022,hu:2024:refchecker,chen:2024}, coherence \citep{wanner:2024:decomp}, and correctness \citep{kamoi:2023}. 
    \item \textbf{Coverage} means that the extracted claims should capture all verifiable information in the source text while avoiding explicit inclusion of unverifiable information. We discuss different methods of defining and evaluating coverage in \autoref{sec:coverage}.
    \item \textbf{Decontextualization} is typically defined as: (1) each claim should be understandable on its own, without requiring additional context, and (2) each claim should retain the meaning it held in its original context \citep{choi:2021, gunjal:2024}. We propose an alternative definition in \autoref{subsec:decontext}.
\end{enumerate}

Claim extraction methods have also been evaluated based on \textbf{atomicity} \citep{wanner:2024:decomp, chen:2024}. For example, the claim \textit{``California and New York implemented a plastic bag ban''} is not atomic because it can be divided into \textit{``California implemented a plastic bag ban''} and \textit{``New York implemented a plastic bag ban.''} However, the pursuit of atomicity lacks a clear endpoint: the above claims could be further divided into \textit{``At least one state has implemented a plastic bag ban,''} \textit{``California has implemented a ban,''} and \textit{``California exists.''} Moreover, prior works suggest that atomicity does not consistently improve fact-checking performance \citep{chen:2023:felm, hu:2024:decompdilemmas, tang:2024}. As a result, we do not consider atomicity in our evaluation framework.

\subsection{Rethinking Decontextualization}
\label{subsec:decontext}

Numerous studies rely on human annotations to assess whether a unit of text (e.g., a sentence or claim) is sufficiently decontextualized \citep{choi:2021, kane:2023, bayat:2025}. However, we argue that such judgments are often subjective, difficult to apply consistently, and fail to reflect the claim’s suitability for fact-checking. 

Consider the claim \textit{``John Smith supports government regulations,''} extracted from the sentence \textit{``In the latest episode of Jane Doe’s podcast on electric vehicles, Doe’s free-market views clashed with John Smith’s support for government regulations.''} According to the definition in \autoref{subsec:framework_definitions}, the claim appears sufficiently decontextualized. 

However, if a fact-checking system attempted to verify this claim, it might find evidence of John Smith opposing government regulations in other contexts (e.g., AI or healthcare) and conclude that the claim is false – even though this evidence does not contradict the source sentence. The mismatch between the evidence’s implications for the claim and for the sentence indicates that the claim was insufficiently decontextualized: it should have clarified that John’s comments were made during a specific podcast episode about electric vehicles. Critically, the underspecification only became apparent after the fact-checking process, not beforehand. 

Consider a second example: \textit{``The court helped secure Bush’s presidency through its split decision to halt the Florida recount.''} The claim appears to be insufficiently decontextualized, since \textit{``The court''} is not defined. However, for fact-checking purposes, the underspecification is not problematic, since the only plausible reference is the Bush v. Gore decision by the United States Supreme Court.

We posit that instead of making subjective judgments about whether a claim is ``sufficiently'' decontextualized, we should measure how the claim affects the outcome of the fact-checking system. In a fact-checking system, claims are used to retrieve evidence that informs a true/false verdict. Therefore, missing context is problematic only if its inclusion would change the verdict from true to false, or vice versa. This shift could occur if including the context results in retrieving a different pool of evidence with the opposite relationship to the claim, or if the same evidence is retrieved but its relationship to the claim changes when viewed with the added context.\footnote{Prior works propose retrieval-based evaluations of decontextualization \citep{choi:2021, deng:2024}. However, in fact-checking, such approaches are insufficient because retrieval is only an intermediate step towards the final verdict.}

Accordingly, we propose a three-step process for evaluating the decontextualization of a claim $C$ in the context of fact-checking:\newline
\begin{enumerate}
    \item \textbf{Identify Missing Context.} Based on $C$ and its context, either:
    \begin{itemize}
        \item Generate $C_{\max}$, a maximally decontextualized version of $C$, ensuring $C$ is entailed by $C_{\max}$; or
        \item Determine that $C$ is already maximally decontextualized (i.e., $C = C_{\max}$)
    \end{itemize}
    In the John Smith example, $C_{\max}$ might be: \textit{``In the latest episode of Jane Doe’s podcast on electric vehicles, John Smith supports government regulations.''} If $C$ is already maximally decontextualized, no further steps are needed.\footnote{Just as there are often multiple ways to decontextualize a sentence, there is rarely a single ``correct'' formulation of $C_{\max}$. However, we argue that creating a claim that contains as much context as possible is less subjective than trying to evaluate whether a claim is ``sufficiently'' decontextualized. The evaluation can also be repeated for different $C_{\max}$ values to ensure robustness.} 

    \item \textbf{Retrieve Evidence.} Separately find relevant information for $C$ and $C_{\max}$, producing evidence sets $E_\mathrm{C}$ and $E_{\max}$, respectively.

    \item \textbf{Determine Veracity.} Perform the following checks\footnote{If $E_\mathrm{C}\tightnarrow C$, there is no need to check whether $E_\mathrm{C}\tightarrow C_{\max}$. Since $C_{\max}$ entails $C$, and $C$ is narrower than $C_{\max}$, any evidence that fails to support $C$ cannot support $C_{\max}$.}:
    \begin{itemize}
        \item $E_\mathrm{C}\tightarrow C$ (i.e., check if $E_\mathrm{C}$ supports $C$)
        \item $E_{\max}\tightarrow C_{\max}$ 
        \item If $E_\mathrm{C}\tightarrow C$, check if $E_\mathrm{C}  \tightarrow C_{\max}$
    \end{itemize}
\end{enumerate}
This process yields one of seven possible results: 
\begin{enumerate}
    \item $C = C_{\max}$
    \item $(E_\mathrm{C}\tightarrow C)\tinyand(E_{\max}\tightarrow C_{\max})\tinyand(E_\mathrm{C}\tightarrow C_{\max})$
    \item $(E_\mathrm{C}\tightarrow C)\tinyand(E_{\max}\tightarrow C_{\max})\tinyand(E_\mathrm{C}\tightnarrow C_{\max})$
    \item $(E_\mathrm{C}\tightarrow C)\tinyand(E_{\max}\tightnarrow C_{\max})\tinyand(E_\mathrm{C}\tightarrow C_{\max})$
    \item $(E_\mathrm{C}\tightarrow C)\tinyand(E_{\max}\tightnarrow C_{\max})\tinyand(E_\mathrm{C}\tightnarrow C_{\max})$
    \item $(E_\mathrm{C}\tightnarrow C)\tinyand(E_{\max}\tightarrow C_{\max})$
    \item $(E_\mathrm{C}\tightnarrow C)\tinyand(E_{\max}\tightnarrow C_{\max})$
\end{enumerate}

Results 5 and 6 are undesirable because the verdicts for $C$ and $C_{\max}$ are misaligned. Result 3 – where $C$ and $C_{\max}$ are supported by their respective evidence sets, but the evidence for $C$ does not support $C_{\max}$ – is problematic in scenarios where the rationale matters, not just the verdict.\footnote{Consider $C =$ \textit{``Miller has been described as an architect,''} extracted from the sentence $S =$ \textit{``Miller has been described as the architect of Trump’s controversial immigration policies.''} Let $C_{\max} = S$. Imagine $E_\mathrm{C}$ contains information about a building architect named John Miller, while $E_{\max}$ describes Stephen Miller, President Trump’s policy advisor, as an architect of the administration’s immigration policies. Although both $C$ and $C_{\max}$ are supported by their respective evidence sets, it would be highly problematic if a fact-checking system cited $E_\mathrm{C}$ as its rationale for the sentence’s veracity! Note that $C$ and $S$ were adapted from an example by \citet{wanner:2024:dnd}.} 

In contrast, Results 2 and 7 are desirable because the verdicts for $C$ and $C_{\max}$ are aligned. Result 4 is also favorable – in fact, it suggests that $C$ is superior to $C_{\max}$ because only the former retrieved evidence supporting both $C$ and $C_{\max}$. We classify Result 1 as desirable because it indicates that no contextual information was omitted.

We describe our implementation of this approach in \autoref{subsec:decontext_results}, comparing claim extraction methods based on the percentage of desirable results. 

\section{Claimify}
\label{sec:claimify}

This section describes Claimify, our novel LLM-based claim extraction method. \autoref{fig:claimify} in \appautoref{app:claimify} illustrates its key stages, and \appautoref{app:claimify_prompts} contains all prompts.

\subsection{Sentence Splitting and Context Creation}
\label{subsec:claimify_sentences}
Claimify accepts a question-answer pair as input. It uses NLTK’s sentence tokenizer to split the answer into sentences \citep[version 3.9.1]{bird:2004}. Sentences with fewer than five characters are merged with the following sentence. Context is created for each sentence based on a configurable combination of $p$ preceding sentences, $f$ following sentences, and optional metadata (e.g., the header hierarchy in a Markdown-style answer). The parameters $p$ and $f$ are defined separately for the stages outlined in \autoref{subsec:selection}-\autoref{subsec:decomposition}, allowing each stage to have a distinct context.

\subsection{Selection}
\label{subsec:selection}

Next, Claimify uses an LLM to determine whether each sentence contains any verifiable content, in light of its context and the question. When the LLM identifies that a sentence contains both verifiable and unverifiable components, it rewrites the sentence, retaining only the verifiable components. 

More specifically, the LLM selects one of the following options: (1) state that the sentence does not contain any verifiable content, (2) return a modified version of the sentence that retains only verifiable content, or (3) return the original sentence, indicating that it does not contain any unverifiable content. If the LLM selects the first option, the sentence is labeled ``No verifiable claims'' and excluded from subsequent stages (\autoref{subsec:disambiguation} and \autoref{subsec:decomposition}). \autoref{tab:selection_examples} in \appautoref{app:claimify} provides examples where the LLM selected the first or second option.

\subsection{Disambiguation}
\label{subsec:disambiguation}
The primary goals of this stage are to identify ambiguity in the sentences returned by the Selection stage, and to determine whether the ambiguity has a clear resolution based on the question and the context. These objectives and capabilities are unique to Claimify (see \autoref{sec:related_work} for a discussion of related works). 

Claimify uses an LLM to identify two types of ambiguity. The first is \textbf{referential ambiguity}, which occurs when it is unclear what a word or phrase refers to. For example, in the sentence \textit{``They will update the policy next year,''} the terms \textit{``They,''} \textit{``the policy,''} and \textit{``next year''} are ambiguous. The second is \textbf{structural ambiguity}, which occurs when grammatical structure allows for multiple interpretations. For instance, the sentence \textit{``AI has advanced renewable energy and sustainable agriculture at Company A and Company B''} can be interpreted as: (1) AI has advanced renewable energy and sustainable agriculture at both Company A and Company B, or (2) AI has advanced renewable energy at Company A, and it has advanced sustainable agriculture at Company B. 

A special case of structural ambiguity involves distinguishing between factual claims and unverifiable interpretations added by the author. For example, the sentence \textit{``John emphasized the support he received from executives throughout his career, highlighting the importance of mentorship,''} can be interpreted as: (1) John both emphasized the support he received and highlighted the importance of mentorship, or (2) John emphasized the support he received, while the author added the interpretation about the importance of mentorship.

The LLM is also asked to determine whether each instance of ambiguity can be resolved using the question and the context. The standard for resolution is whether a group of readers would likely agree on the correct interpretation. For example, recall the sentence \textit{``AI has advanced renewable energy and sustainable agriculture at Company A and Company B.''} If the context specified that Company A builds solar panels and Company B reduces farms’ water usage, readers would likely conclude that AI has advanced renewable energy at Company A and sustainable agriculture at Company B. Conversely, if the context only described both companies as \textit{``environmental pioneers,''} readers would have insufficient information to determine the correct interpretation. 

If any ambiguity is unresolvable, the sentence is labeled ``Cannot be disambiguated'' and excluded from the Decomposition stage (\autoref{subsec:decomposition}), even if it has unambiguous, verifiable components. \autoref{tab:disambiguation_examples} in \appautoref{app:claimify} provides examples of such sentences. If the LLM resolves all ambiguity, it returns a clarified version of the sentence. If there is no ambiguity, it returns the original sentence.\footnote{The LLM also checks for partial names, abbreviations, and acronyms, which are not considered linguistic ambiguities. If full forms are provided in the question or context, the LLM includes them in the returned sentence; otherwise, the LLM leaves them unchanged to avoid factual inaccuracies.}

\subsection{Decomposition}
\label{subsec:decomposition}
In the final stage, Claimify uses an LLM to decompose each disambiguated sentence into decontextualized factual claims. If it does not return any claims (only 0.8\% of cases in our experiments), the sentence is labeled ``No verifiable claims.''

Extracted claims may include text in brackets, which typically represents information implied by the question or context but not explicitly stated in the source sentence. For example, given the question \textit{``Provide an overview of celebrities’ stances on the Middle East,''} and the sentence \textit{``John has called for peace,''} Claimify may return the claim \textit{``John }[\textit{a celebrity}] \textit{has called for peace} [\textit{in the Middle East}].'' This notation resembles the ``markup-and-mask'' approach by \citet{eisenstein:2024}, which adds bracketed text to clarify context in passages. A benefit of bracketing is that it flags inferred content, which is inherently less reliable than content explicitly stated in the source sentence.

\section{Experimental Setup}
\subsection{Data}
\subsubsection{BingCheck}
\label{subsubsec:bingcheck}
We evaluated Claimify’s performance on the BingCheck dataset \citep{li:2024}, which consists of 396 answers generated by Microsoft Copilot (formerly Bing Chat). BingCheck spans a wide range of topics and question types, and its answers are significantly longer than those in comparable datasets \citep{li:2024}. As a result, it reflects the diversity and complexity of real-world LLM usage in long-form question answering. Moreover, since BingCheck answers are generated based on web search results, it is reasonable to expect that relevant evidence exists for many claims – a key consideration for the evidence retrieval step of the decontextualization evaluation described in \autoref{subsec:decontext}.

\subsubsection{Human Annotation Study}
\label{subsubsec:annotation}

We conducted a human annotation study to classify sentences in BingCheck answers as containing or not containing factual claims. A total of 6,490 sentences were labeled by three annotators who are familiar with natural language processing, including one of the authors. To ensure reliability, annotators completed two practice rounds on a subset of sentences, resolving disagreements via discussion, then independently annotated the remaining data. Krippendorff’s alpha \citep{krippendorff:2013, castro:2017} increased from 0.44 in the first practice round to 0.72 in the final round, reaching 0.86 for high-confidence annotations. Sentence splitting methodology, annotation procedure, guidelines, and results are detailed in \appautoref{app:annotation}. The labels from the study were used in our analysis of coverage (\autoref{sec:coverage}). 

\subsection{Compared Methods}
\label{subsec:methods}
We compared Claimify to five LLM-based methods:
\begin{enumerate}
    \item \textbf{AFaCTA \citep{ni:2024}} uses an ensemble of prompts to classify sentences as containing or not containing objectively verifiable content.
    \item \textbf{Factcheck-GPT \citep{wang:2024}} classifies sentences as factual claims, opinions, non-claims (e.g., questions or imperative statements), or other.
    \item \textbf{VeriScore \citep{song:2024}} combines sentence classification, decomposition, and decontextualization in a single prompt. It returns either ``No verifiable claim'' or a list of claims.
    \item \textbf{DnD \citep{wanner:2024:dnd}} decomposes and decontextualizes sentences in a single prompt.
    \item \textbf{SAFE \citep{wei:2024}} adds instructions to FActScore's decomposition prompt \citep{min:2023} and performs decontextualization in a separate prompt.
\end{enumerate}

DnD and SAFE do not provide instructions for handling sentences without factual claims. Therefore, when the LLM declined to extract claims, it did not use a consistent output format. If no claims were parsed from the output, we assumed that the LLM determined there were no factual claims.

We selected these methods because they allow for direct comparisons with Claimify. They process sentences independently, unlike other methods that analyze the entire answer as a single unit (e.g., \citealp{chern:2023, bayat:2025}). The methods with explicit sentence classification components (AFaCTA, Factcheck-GPT, VeriScore) share Claimify’s focus on detecting verifiable content, rather than ranking sentences by their ``check-worthiness'' (see \autoref{sec:related_work}). The methods that perform claim extraction (VeriScore, DnD, SAFE) involve both decomposition and decontextualization, unlike other approaches that focus solely on decomposition (e.g., \citealp{kamoi:2023, chen:2023:felm}).

To further enable direct comparisons, we used the sentence splitting logic described in \appautoref{app:ann_sentences} for all methods. We also made minimal edits to all prompts (except VeriScore, where edits were unnecessary) to include the question and clarify that the sentence was extracted from a response to the question. Finally, we used OpenAI’s \texttt{gpt-4o-2020-08-06} model to generate all outputs. Additional settings are described in \appautoref{app:hyperparameters}.

The claim extraction methods (VeriScore, DnD, SAFE, Claimify) generated a total of 73,681 claims. Where a method produced duplicate claims for a sentence, we removed the duplicates, resulting in 73,229 claims. All subsequent sections refer to this de-duplicated claim set. 

\section{Experiments}
\label{sec:experiments}
This section describes our implementation of the evaluation framework outlined in \autoref{sec:framework} and the corresponding results. All prompts are provided in \appautoref{app:eval_prompts}. For all experiments except entailment (\autoref{subsec:entailment}), the sentence context was standardized to the five preceding sentences.\footnote{We used the method-specific context (see \appautoref{app:method_contexts}) for entailment because restricting the LLM to a smaller context than was used to generate the claims may lead to overclassification of not-entailed cases. For instance, SAFE uses the entire response as context during decontextualization, so its claims may include information from beyond the source sentence and the preceding five sentences.} We used OpenAI’s \texttt{gpt-4o-2020-08-06} model with a temperature of 0. \appautoref{app:exp_samples} describes the samples for the evaluations performed below.

\subsection{Entailment}
\label{subsec:entailment}

To determine whether claims are entailed by their source sentences, we first used a pre-trained Natural Language Inference (NLI) model from \citet{nie:2020}. We tried two configurations, both of which revealed significant limitations, detailed in \appautoref{app:nli}. 

In light of the NLI model's limitations, we developed a prompt that classifies a claim as entailed or not entailed based on the source sentence, context, and question. To validate the prompt, we randomly sampled 20 claims from each claim extraction method (80 claims total) and labeled them without referencing the LLM’s outputs. The LLM’s classifications conflicted with our labels in only five cases, whereas the NLI model’s classifications conflicted with us in 32 and 12 cases for the first and second configurations, respectively. \appautoref{app:entailment_review} provides an overview of cases where we disagreed. 

\autoref{tab:entailment} shows the percentage of entailed claims for each method using our prompt. Claimify and Veriscore achieved the highest percentage of entailed claims (99\%), with no statistically significant difference between them (p=0.145). All pairwise comparisons between the methods, except for Claimify vs. VeriScore, showed statistically significant differences (p<0.001). These results align with a similar analysis by \citet{wanner:2024:decomp} where the percentage of supported claims for various claim extraction methods, averaged across different models, ranged from 86\% to 98\%.

\import{}{entailment_table.tex}

\subsection{Coverage}
\label{sec:coverage}

\import{}{coverage_table.tex}

Coverage (\autoref{subsec:framework_definitions}) can be evaluated at different levels of granularity. Prior works have primarily focused on \textbf{sentence-level} evaluation, assessing whether a method correctly determines that a sentence, as a whole, contains a factual claim (\citealp{konstantinovskiy:2021, majer:2024}). 

Consider the sentence \textit{``The iconic American flag has 50 stars and 13 stripes,''} where Method A extracts the claims [\textit{``The American flag is iconic'', ``The American flag has numerous stars and stripes''}] and Method B extracts the claims [\textit{``The American flag contains 50 stars'', ``The American flag contains 13 stripes''}]. Both methods correctly identified that the sentence contains a factual claim, so they performed equally well in terms of sentence-level coverage.

In contrast, we introduce the concept of \textbf{element-level coverage}, evaluated by first breaking a sentence into distinct pieces of information (``elements''), then classifying each element as verifiable or unverifiable, and finally assessing whether each element is covered by the extracted claims. This approach recognizes that Method B, which captures all verifiable content without including unverifiable content (\textit{``iconic''}), is superior to Method A, which omits verifiable content (the number of stars and stripes) and includes unverifiable content. 

Prior works that came closest to evaluating element-level coverage (1) relied on human annotation, making them difficult to scale, (2) lacked specificity (e.g., they considered whether verifiable content was omitted without quantifying the omissions), and/or (3) failed to penalize the inclusion of unverifiable content \citep{song:2024, li:2024}. These deficiencies are addressed by our novel method for evaluating element-level coverage, described in \autoref{subsubsec:element_coverage}.

\subsubsection{Sentence-Level Coverage}
\label{subsubsec:sentence_coverage}
To evaluate sentence-level coverage, we used the results of the human annotation study (\autoref{subsubsec:annotation}) as ground truth. 63\% of sentences in the final sample (see details in \appautoref{app:filtering_statements}) were classified as containing a factual claim, which we treated as positive labels.

For the claim extraction methods (VeriScore, DnD, SAFE, Claimify), a positive label was assigned if at least one claim was extracted. For Factcheck-GPT, only the ``factual claim'' label was treated as positive. For AFaCTA, we replicated its majority voting procedure, treating ``contains objective information'' as the positive label. 

\autoref{tab:coverage} shows the sentence-level coverage results for all methods under the ``Sent.'' columns. Claimify achieved the highest accuracy (91.8\%) and $F_1$ score (93.5\%), followed by AFaCTA (accuracy = 81.6\%) and Factcheck-GPT ($F_1$ = 86.7\%).

\subsubsection{Element-Level Coverage}
\label{subsubsec:element_coverage}
To evaluate element-level coverage of a sentence $S$ by the claims $\mathcal{C}=\{C_i\}_{i=1}^{n}$ extracted from $S$, we developed two prompts: one identifies and classifies elements of $S$ as verifiable or unverifiable, and the other determines if each element is ``covered'' by $\mathcal{C}$ and labels the coverage as explicit or implicit.

To compare coverage across methods, we used a single set of elements per sentence. We defined a \textbf{true positive} as a verifiable element that is covered implicitly or explicitly by the claims; a \textbf{true negative} as an unverifiable element that is either not covered or only implicitly covered (since implicit coverage may not reflect deliberate inclusion); a \textbf{false positive} as an unverifiable element that is explicitly covered; and \textbf{a false negative} as a verifiable element that is not covered.

To ensure consistency with the sentence-level coverage results, we analyzed 81\% of sentences where the element-based verifiability labels (i.e., positive = the sentence contains at least one verifiable element; negative = no verifiable elements) matched the annotation study labels. \autoref{tab:coverage} shows the element-level results under the ``Elem.'' columns. Claimify achieved the highest accuracy (87.9\%) and $F_1$ score (91.9\%), followed by DnD (accuracy = 76.9\%, $F_1$ = 86.3\%). 

To validate the results, we manually reviewed a random sample of 80 sentences, assessing element quality and coverage labels. We found that 95\% of sentences met all quality criteria, and we agreed with 97\% of coverage labels. The evaluation criteria and results are detailed in \appautoref{app:coverage_review}.

\subsection{Decontextualization}
\label{subsec:decontext_results}
We evaluated decontextualization as follows (see \autoref{subsec:decontext}, \appautoref{app:filtering_statements}, and \appautoref{app:decontext_details} for details):
\begin{enumerate}
    \item \textbf{Identify Missing Context.} We developed a prompt that either returns $C_{\max}$, a maximally decontextualized version of a claim $C$, or indicates that $C$ is already maximally decontextualized. We  reviewed 80 outputs and found that 76 (95\%) were valid (see \autoref{app:decontext_review}).

    \item \textbf{Retrieve Evidence.} To assess consistency of results across retrieval systems, we replicated two configurations from prior works:
    \begin{itemize}
        \item \textbf{Google Search \citep{wei:2024}:} An LLM generates an initial query based on a claim, retrieves results from the Google Search API, and iteratively refines the query. In total, five queries are generated, with the top three results per query forming the evidence set.
        \item \textbf{Bing \citep{li:2024}:} An LLM generates a single query based on a claim, with the top three results from the Bing Web Search API forming the evidence set.
    \end{itemize}

    \item \textbf{Determine Veracity.} To assess whether a claim is supported by the retrieved evidence, we used the verification prompt from \citet{wei:2024}, as it demonstrated strong agreement with human annotators. If the queries from Step 2 above did not return any search results, we classified the claim as not supported.

    \end{enumerate}

\import{}{decontextualization_table.tex}

\autoref{tab:decontext} shows the distribution of the seven result types (\autoref{subsec:decontext}) per claim extraction method for both retrieval configurations (Google Search and Bing). We ensured that identical claims extracted by different methods from the same sentence were assigned the same result type. Result 1 ($C = C_{\max}$, i.e., no missing contextual information) is reported only once, since the same $C_{\max}$ was used for both configurations. Claimify had the largest percentage of Result 1 cases, significantly higher than all other methods (p<0.001). Across both retrieval configurations, Claimify achieved the largest percentage of desirable results (i.e., types 1, 2, 4, and 7 from \autoref{subsec:decontext}). For Google Search, Claimify significantly outperformed all other methods (p<0.001). For Bing, Claimify also outperformed other methods (p<0.001), except VeriScore, where the difference was not statistically significant (p=0.159).

\section{Related Work}
\label{sec:related_work}

\textbf{Claim Detection.} Many prior works on claim detection aim to identify ``check-worthy'' claims (\citealp{gencheva:2017, jaradat:2018, arslan:2020}). Check-worthiness criteria include public interest \citep{hassan:2017}, potential harm \citep{nakov:2022}, and relevance to a topic, such as the environment \citep{stammbach:2023}. We agree with \citet{konstantinovskiy:2021} and \citet{ni:2024} that check-worthiness is subjective.

\noindent\textbf{Decomposition.} Claimify uses an LLM to extract claims as complete declarative sentences. Alternative decomposition approaches include extracting subject-predicate-object tuples \citep{banko:2007, goodrich:2019, hu:2024:refchecker}, predicate-argument structures \citep{white:2016, zhang:2017, goyal:2020}, questions \citep{fan:2020, chen:2022}, and subsets of tokens \citep{chen:2023:propsegment}.

\noindent\textbf{Ambiguity.} Existing decomposition and decontextualization methods either ignore ambiguity or assume it is always resolvable. An example of the latter is Molecular Facts \citep{gunjal:2024}, a decontextualization method focused on cases where the main entity in a sentence could refer to multiple people (which Claimify would classify as referential ambiguity). Molecular Facts not only forces the LLM to resolve such ambiguities, but it also relies on the model’s parametric knowledge – rather than the sentence and its context – which risks introducing factual inaccuracies. Beyond claim extraction, prior works on ambiguity in fact-checking have explored ambiguous questions \citep{min:2020, kim:2023, zhang:2024} and investigated why annotators disagree on veracity judgements \citep{glockner:2024}.
    
\section{Conclusion}

In this paper, we propose an evaluation framework for claim extraction in the context of fact-checking, based on entailment, coverage, and decontextualization. We provide automated, scalable, and replicable methods for applying the framework. For coverage, we augment sentence-level assessment with a more granular element-level approach that accounts for sentences containing both verifiable and unverifiable content. For decontextualization, we propose a novel method that quantifies the impact of omitted context on factuality verdicts.

We also introduce Claimify, an LLM-based claim extraction method. Unlike existing methods, Claimify explicitly accounts for ambiguity: it identifies cases where the source text has multiple plausible interpretations and the correct interpretation cannot be inferred from the context. We benchmarked Claimify against existing methods and found that: (1) 99\% of claims extracted by Claimify were entailed, tying with one method and outperforming the others; (2) for both sentence- and element-level coverage, Claimify achieved the highest accuracy and $F_1$ scores; and (3) Claimify was least likely to omit contextual information critical to the factuality verdict.

\section{Limitations}

\textbf{Dataset Scope.} We evaluated performance on a single dataset, albeit one that includes diverse question types and spans a wide range of domains. Future work could extend the analysis to additional datasets and explore how Claimify generalizes beyond long-form LLM-generated answers to other content types, such as political speeches \citep{ni:2024} and social media \citep{alam:2021}.

\noindent\textbf{Model Choice.} All evaluations were conducted using the \texttt{gpt-4o-2020-08-06} model, which is closed-source and more costly than other LLMs. We encourage future work to systematically explore the effects of model choice on claim extraction quality.

\noindent\textbf{Annotator Pool.} The annotation study involved three annotators due to limited availability of high-quality annotators. While all samples were labeled by multiple annotators, a larger annotator pool would increase the reliability of the results.

\noindent\textbf{Hyperparameter Configuration.} We did not conduct an exhaustive search for the optimal hyperparameter configuration for Claimify (\appautoref{app:hyperparameters}). For example, varying the number of completions and the minimum success threshold could yield valuable insights. Additionally, we anticipate that increasing the number of preceding sentences used as context may improve performance, especially for answers that contain lengthy bullet-point lists. Consider the following list item: \textit{``- Investing in renewable energy sources.''} Is it a recommendation for what one ought to do (not verifiable) or an example of an action a specific entity has taken (verifiable)? The correct interpretation is likely clarified by the preamble for the list (e.g., \textit{``Here are some steps businesses should take to mitigate their environmental impact:''}), but it might not have been included in our narrow context window.

\noindent\textbf{Evaluating Disambiguation.} Claimify’s Disambiguation stage (\autoref{subsec:disambiguation}) addresses two types of ambiguity that we identified as particularly relevant to claim extraction. We encourage future work to explore additional types of ambiguity and to develop methods for evaluating detection accuracy.

\section{Ethics Statement}

\textbf{Licenses and Terms of Use.} We used BingCheck, a publicly available dataset released for research purposes, although \citet{li:2024} do not specify a license. For method replication, we complied with all provided licenses and terms of use. Factcheck-GPT, SAFE, and VeriScore are released under Apache 2.0. AFaCTA has a publicly available code repository but does not specify a license. DnD was replicated based on the methodology described in the arXiv publication. We adhered to the terms of use for the Bing Web Search API and the Serper Google Search API (\appautoref{app:decontext_details}).

\noindent\textbf{Human Annotation.} We used human-annotated data to evaluate claim extraction. Annotators were informed about the task and provided consent. No personally identifiable or sensitive information was collected or used.

\noindent\textbf{Potential Risks.} Claim extraction and fact-checking involve subjective judgments, which may introduce bias. To mitigate this risk, we propose structured, explainable, and replicable evaluation methods. Additionally, claim extraction systems can introduce factual inaccuracies or misinterpret the original text. We address these risks through our entailment evaluation and Claimify’s Disambiguation stage. Finally, even though our proposed evaluation framework and Claimify are both fully automated, we recommend human oversight in high-stakes contexts where inaccuracies or misinterpretations could have significant consequences.

\noindent\textbf{Use of AI Assistants.} We used ChatGPT for minor language refinement and proofreading, but all substantive contributions were developed independently.

\bibliography{custom}

\appendix
\newpage
\import{}{appendix.tex}

\end{document}

