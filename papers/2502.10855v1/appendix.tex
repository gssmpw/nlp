\pagebreak
\section{Claimify Overview and Examples}
\label{app:claimify}

\autoref{fig:claimify} provides an overview of Claimify’s stages. \autoref{tab:selection_examples} contains examples of outputs from Claimify’s Selection stage. \autoref{tab:disambiguation_examples} contains examples of sentences labeled by Claimify as ``Cannot be disambiguated.'' 

\import{}{claimify_figure.tex}
\import{}{selection_examples_table.tex}
\import{}{disambiguation_examples_table.tex}

\section{Human Annotation Study}
\label{app:annotation}

\subsection{Sentence Splitting}
\label{app:ann_sentences}

To identify sentence boundaries in BingCheck answers, we first divided answers into paragraphs by splitting on newline characters. Then, for each paragraph, we applied Claimify’s sentence splitting methodology (described in \autoref{subsec:claimify_sentences}). Splitting by newlines was necessary because many answers contained bullet-point lists with items that lacked terminal punctuation, which would otherwise be treated as a single sentence by the NLTK tokenizer. This process produced 6,490 sentences. 

\subsection{Procedures}
The annotation team consisted of one of the authors and two members of the authors’ research group who are familiar with natural language processing but were not involved in the creation of Claimify or the writing of this paper.

Annotators reviewed question-answer pairs and labeled sentences as either containing or not containing a factual claim, distinguishing between high and low confidence labels. Detailed annotation guidelines are provided in \appautoref{app:ann_guidelines}, and an example of the annotation interface in Azure Machine Learning is provided in \appautoref{app:ann_interface}.

From the 396 BingCheck answers, 18 were randomly sampled as practice cases and divided into two rounds of nine samples each. Annotators independently labeled the first round, resolved disagreements through discussion, and repeated the process for the second round. The remaining 378 answers were split into three groups of 126, and each annotator was assigned two groups (252 answers) to ensure that every sample was independently annotated by two people.

\subsection{Results}
We measured inter-annotator agreement using Krippendorff’s alpha \citep{krippendorff:2013, castro:2017}. As expected, agreement improved across rounds, increasing from 0.44 in the first practice round to 0.54 in the second, and reaching 0.72 in the final round. Notably, for 82\% of sentences in the final round where both annotators reported high confidence, Krippendorff's alpha was 0.86.

For sentences where all annotators agreed on the label, the consensus was used as the ground truth. Disagreements in the practice samples were resolved through discussions among the annotators. For non-practice samples, disagreements in the two groups where the author was an annotator were settled by prioritizing the author’s label, while in the third group, the author reviewed and resolved disagreements.

\clearpage

\onecolumn
\subsection{Guidelines}
\label{app:ann_guidelines}

Annotators were given the following guidelines:
\import{}{annotator_guidelines.tex}
\twocolumn

\subsection{Interface}
\label{app:ann_interface}

As shown in \autoref{fig:annotation_interface}, we conducted the annotation study using the Data Labeling feature in Azure Machine Learning. Annotators were presented with answers to questions and asked to select one of the following options for each sentence in the answer:
\begin{itemize}
    \item \textbf{``[HIGH CONF] Contains'' tag} – High confidence that the sentence contains at least one  factual claim
    \item \textbf{``[LOW CONF] Lean towards contains''} tag – Low confidence in the classification, but leans towards the sentence containing at least one  factual claim
    \item \textbf{``[LOW CONF] Lean against contains''} tag – Low confidence in the classification, but leans towards the sentence not containing any  factual claims
    \item \textbf{No tag} – High confidence that the sentence does not contain any factual claims
\end{itemize}

Annotators were reminded to apply tags carefully and avoid accidental tagging. In some cases, annotators applied multiple tags to a single sentence (e.g., to indicate a mix of verifiable and unverifiable content). However, each sentence needed to be classified as either containing or not containing a  factual claim. Therefore, for each annotator, we assigned a single final label per sentence as follows:
\begin{enumerate}
    \item If the sentence contained at least one \textbf{``[HIGH CONF] Contains''} tag, it was labeled as containing a factual claim with high confidence.
    \item Otherwise, if it contained at least one \textbf{``[LOW CONF] Lean towards contains''} tag, it was labeled as containing a factual claim with low confidence.
    \item Otherwise, if it contained at least one \textbf{``[LOW CONF] Lean against contains''} tag, it was labeled as not containing a factual claim with low confidence.
    \item If the sentence did not contain any tags, it was labeled as not containing a factual claim with high confidence.
\end{enumerate}

\import{}{annotation_interface_figure.tex}


\section{Hyperparameters}
\label{app:hyperparameters}

There are five key hyperparameters for each of the Selection (\autoref{subsec:selection}), Disambiguation (\autoref{subsec:disambiguation}), and Decomposition (\autoref{subsec:decomposition}) stages of Claimify:
\begin{enumerate}
    \item \textbf{\texttt{max\_retries}} controls the number of retries if a stage fails to return a valid output. We set it to 2 for all stages.
    \item \textbf{\texttt{max\_preceding\_sentences}} determines the number of proceeding sentences in the context (i.e., $p$ in \autoref{subsec:claimify_sentences}). We set it to 5 for all stages.
    \item \textbf{\texttt{max\_following\_sentences}} determines the number of following sentences in the context (i.e., $f$ in \autoref{subsec:claimify_sentences}). We set it to 5 for the Selection stage and 0 for the Disambiguation and Decomposition stages.
    \item \textbf{\texttt{completions}} is the number of outputs generated. We set it to 3 for the Selection and Disambiguation stages and 1 for the Decomposition stage.
    \item \textbf{\texttt{min\_successes}} is the minimum number of successful outputs required to advance to the next stage. The definition of ``success'' varies by stage: in Selection, a sentence must contain verifiable content; in Disambiguation, it must either have no ambiguity or only resolvable ambiguity; and in Decomposition, at least one claim must be extracted from the sentence.
    
    We set \texttt{\textbf{min\_successes}} to 2 for the Selection and Disambiguation stages and 1 for the Decomposition stage. For instance, in the Selection stage, we generated 3 outputs per sentence (since \textbf{\texttt{completions}} = 3). If at least 2 outputs identified verifiable content, the sentence advanced to the Disambiguation stage; otherwise, it was labeled ``No verifiable claims'' and excluded from subsequent stages.
\end{enumerate}

Claimify uses a default temperature of 0 for all stages. However, if \texttt{\textbf{completions}} > 1, it uses a temperature of 0.2. For all other methods outlined in \autoref{subsec:methods}, we followed the temperature values specified in their respective publications. If no value was specified, we used the default setting from the associated code repository. DnD was the only method without a specified temperature in its publication and without a publicly available code repository, so we set the temperature to 0.

\section{Context Definitions}
\label{app:method_contexts}
The methods described in \autoref{subsec:methods} vary in how they define the context for a sentence:
\begin{enumerate}
    \item \textbf{AFaCTA:} Context is defined as $n$ preceding sentences and $n$ following sentences. Although \citet{ni:2024}, do not specify a value for $n$, we used the default value of 1 from their code repository in our experiments.
    \item \textbf{Factcheck-GPT:} The module that classifies sentences as factual claims, opinions, non-claims, or other does not include any context.\footnote{Factcheck-GPT’s code repository also includes modules for claim decomposition and decontextualization, but multiple versions of these prompts were provided without clear guidance on the preferred one. To avoid misrepresenting the method, we limited our evaluation to the sentence classification module.}
    \item \textbf{VeriScore:} The context consists of three preceding sentences and one following sentence.
    \item \textbf{DnD:} A sentence’s context is defined as the paragraph it belongs to, where paragraphs are determined by splitting on newline characters.
    \item \textbf{SAFE:} The decomposition prompt does not include any context for sentences. The decontextualization prompt uses the entire answer as context.
\end{enumerate}

\section{Evaluation Samples}
\label{app:exp_samples}

\subsection{Invalid Statements}
\label{app:invalid_statements}
When manually inspecting sentences and extracted claims, we identified four types of invalid statements:
\begin{enumerate}
    \item Statements missing key information, making them uninterpretable (e.g., \textit{``Yashoda suggested the playfully''})
    \item Non-declarative statements (e.g., \textit{``Monitoring the conservation status of species that are at risk of extinction,'' ``What do you think?''}) 
    \item Preambles (e.g., \textit{``Here are some examples of promising technologies and how they differ from existing methods:''})
    \item References (e.g., \textit{``[1]: An innovative approach to food security policy in developing countries - ScienceDirect''})
\end{enumerate}

We used an LLM to determine which sentences and claims are invalid (see prompts in \appautoref{app:invalid_statements_prompts}). We accepted 96\% of sentence labels and 99.8\% of claim labels, and manually corrected the remainder. Ultimately, 8\% of sentences and 1.1\% of claims were deemed invalid. Over 90\% of invalid claims were extracted by SAFE and DnD. 

\autoref{tab:claim_statistics} shows the following statistics for each method:
\begin{itemize}
    \item \textbf{Total Claims:} The total number of claims extracted
    \item \textbf{\% Invalid Claims:} The percentage of extracted claims deemed invalid
    \item \textbf{\% Sentences Containing Claim:} The percentage of sentences identified as containing at least one factual claim (i.e., the positive labels from \autoref{subsubsec:sentence_coverage})
    \item \textbf{Avg. Claims per Sentence:} The average number of claims extracted per sentence, excluding sentences where no claims were extracted
\end{itemize}

\import{}{method_stats_table.tex}

All values in \autoref{tab:claim_statistics} are based on the de-duplicated claim set described in \autoref{subsec:methods}. For AFaCTA and Factcheck-GPT, only ``\% Sentences Containing Claim'' is reported, since these methods classify sentences without extracting claims. 

\subsection{Filtering Sentences and Claims}
\label{app:filtering_statements}
Final samples for the evaluations performed in \autoref{sec:experiments} were obtained as follows:
\begin{itemize}
    \item \textbf{Entailment (\autoref{subsec:entailment}):} We excluded invalid claims and claims extracted from invalid sentences. 70,329 claims (96\%) were retained.
    \item \textbf{Sentence-Level Coverage (\autoref{subsubsec:sentence_coverage}):} We excluded sentences that failed to pass Claimify’s Disambiguation stage: they never reached the Decomposition stage, so it is unknown whether any claims would have been extracted. We also excluded invalid sentences. 5,900 sentences (91\%) were retained.
    \item \textbf{Element-Level Coverage (\autoref{subsubsec:element_coverage}):} We applied the element extraction prompt (\appautoref{app:element_extraction}) to the 5,900 sentences noted in the Sentence-Level Coverage section above. For the element coverage prompt (\appautoref{app:element_coverage}), only valid claims were included.
    \item \textbf{Decontextualization (\autoref{subsec:decontext_results})}: We excluded the following claims: invalid claims, claims extracted from invalid sentences, claims not entailed by their source sentence, and claims whose source sentence was labeled as not containing any factual claims in the annotation study. 49,791 claims (68\%) were retained. The number of claims per method was: Claimify = 11,350; DnD = 16,263; SAFE = 15,020; VeriScore = 7,158.
\end{itemize}

\section{Limitations of the NLI Model}
\label{app:nli}

For the entailment evaluation in \autoref{subsec:entailment}, we used a pre-trained Natural Language Inference (NLI) model from \citet{nie:2020} that classifies a hypothesis as entailed, contradicted, or neutral with respect to a premise: \url{https://huggingface.co/ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli}.

We tried two configurations of the model. The first – using the source sentence as the premise and the claim as the hypothesis – resulted in under-classification of entailed claims. For example, the NLI model classified the sentence \textit{``However, it was not implemented until 1998''} as having a neutral relationship with the claim \textit{``The programming language Plankalkül was not implemented until 1998.''} This is because the sentence does not establish that \textit{``it''} refers to Plankalkül or that Plankalkül is a programming language. However, these pieces of information are provided in the preceding sentence, so the claim should be classified as entailed.

Next, we tried using a combination of the question, context, and source sentence as the premise. However, this configuration often exceeded the 512-token input limit of the model, requiring truncation and risking loss of critical context. Furthermore, we observed that the NLI model often struggled with complex claims that incorporated information from multiple parts of the context.

\section{Entailment Review}
\label{app:entailment_review}

As explained in \autoref{subsec:entailment}, we manually labeled a random sample of 80 claims as entailed or not entailed based on their source sentence, the context, and the question. We compared our labels to the LLM’s outputs and found only five conflicts.

In two of these cases, the LLM incorrectly labeled the claim as not entailed. Both cases – one of which is shown as \textbf{Example 1} in \autoref{tab:entailment_errors} – involved claims that incorporated information from multiple parts of the context, leading the LLM to mistakenly conclude they were not entailed because the source sentence alone did not contain all required details.

\import{}{entailment_errors_table.tex}

In the remaining three cases, the LLM incorrectly labeled the claim as entailed. These are presented as Examples 2-4 in \autoref{tab:entailment_errors}: 
\begin{itemize}
    \item \textbf{Example 2:} The context mentions the Curiosity rover but does not attribute it to NASA. 
    \item \textbf{Example 3:} While the context states that the Eiffel Tower was built as the centerpiece of the 1889 World’s Fair and was not well received by some critics, this does not necessarily mean that the critics were present at the World’s Fair. 
    \item \textbf{Example 4:} The claim incorrectly resolves referential ambiguity in the source sentence: \textit{``These technologies''} refers to technologies listed in the bullet-point about digital health (e.g., telemedicine, mobile health apps, etc.)
\end{itemize}

Examples 2 and 3 illustrate claims that involve external knowledge and invalid inferences, respectively. These were the two most common types of entailment errors we observed in not-entailed claims.

\section{Element-Level Coverage Review}
\label{app:coverage_review}

To validate the element-level coverage results (\autoref{subsubsec:element_coverage}), we manually evaluated the extracted elements for a random sample of 80 sentences. For each sentence, we assessed four conditions:
\begin{enumerate}
    \item Are all elements complete declarative sentences?
    \item Are all elements entailed by the combined sentence, context, and question?
    \item Do the elements capture all information in the sentence?
    \item Are all elements’ verifiability labels correct?
\end{enumerate}

We found that 76 sentences (95\%) met these criteria. Examples 1-4 in \autoref{tab:element_errors} are the only sentences that did not satisfy all conditions:
\begin{itemize}
    \item \textbf{Example 1 – violated Condition 1:} All elements are incomplete sentences. Based on the context, they should be completed with \textit{``... is an example of a way to improve public health literacy and promote health education.''} 
    \item \textbf{Example 2 – violated Condition 2:} The third element (\textit{``Light always travels at a constant speed''}) is not entailed by the source sentence, which mentions \textit{``Einstein’s recognition that light always travels at a constant speed.''} The element incorrectly changes the meaning from a viewpoint of a specific entity to a general assertion about the properties of light. 
    \item \textbf{Examples 3 and 4 – violated Condition 3:} In both examples, the elements fail to capture all information in the source sentence. Example 3 misses that Google’s policy of encouraging employees to spend 20\% of their time on projects led to the creation of Gmail, Google News, and Google Maps. Example 4 omits that virtual reality technology is a digital world. 
\end{itemize}

\import{}{element_errors_table.tex}

For the 76 sentences with valid elements, we also reviewed the coverage labels (i.e., whether an element was not covered, covered implicitly, or covered explicitly) across all methods that extracted at least one valid claim. We disagreed with only 25 (3\%) of the 806 labels reviewed. In 24 of these cases, the LLM incorrectly classified an element as covered. These misclassifications were mainly due to three types of errors, illustrated by Examples 1-3 in \autoref{tab:coverage_errors}:
\begin{itemize}
    \item \textbf{Example 1 – overlooks missing information due to external knowledge:} The claims do not explicitly state that Gmail, Google News, and Google Maps are some of Google’s most popular products, so they do not cover the element. 
    \item \textbf{Example 2 – invalid reasoning about combinations of claims:} The LLM reasoned that the first claim (\textit{``Music can stimulate the release of brain chemicals''}) and the second claim (\textit{``Dopamine is a brain chemical''}) collectively cover the element \textit{``Music can stimulate the release of brain chemicals such as dopamine.''} However, this is incorrect because the claims do not establish that music can stimulate the release of dopamine specifically.
    \item \textbf{Example 3 – ignores relationships between claims:} Although claims 3, 5, and 6 each capture part of the element, there is no single claim that connects these pieces to reflect the full relationship described in the element.
\end{itemize}

\import{}{coverage_errors_table.tex}

There was only one case where the LLM incorrectly classified an element as not covered, shown in \autoref{tab:coverage_errors} as \textbf{Example 4.} The element describes the question as \textit{``very interesting''} while the claim uses \textit{``interesting,''} but we found this difference negligible. 

Finally, we observed that elements occasionally included information from beyond the source sentence. Consider the following example, where the underlined text is the source sentence:
\begin{quote}
\textit{``- Nyishi Tribe, India: This is one of the indigenous tribal groups in Arunachal Pradesh, a state in northeastern India. \ul{They have a unique culture and language that are influenced by their Mongoloid ancestry and their proximity to Myanmar.}''}
\end{quote}

One of the extracted elements for the source sentence was \textit{``The Nyishi Tribe is located in Arunachal Pradesh, India.''} The element is entailed by the passage, but it is derived from the sentence preceding the source sentence, not the source sentence itself. 

Unsurprisingly, most claim extraction methods did not cover this element. However, penalizing their lack of coverage is unfair since the element falls outside the scope of the source sentence. Although such cases are rare, they highlight a potential limitation of the element extraction methodology. Instructing the LLM to avoid creating elements based solely on preceding or following sentences may help address this issue. 

\section{Decontextualization Implementation}
\label{app:decontext_details}

In \autoref{subsec:decontext_results}, we outlined our implementation of the decontextualization evaluation. For Step 2 (evidence retrieval) and Step 3 (veracity determination), we replicated methods from prior works. In this section, we describe several implementation details, including minor modifications we made to the original methods.

\begin{enumerate}
\item For the Google Search configuration (based on \citealp{wei:2024}) under Step 2: 
\begin{itemize}
    \item As in \citeauthor{wei:2024}, we used Serper (\url{https://serper.dev/}) for the Google Search API.
    \item We found that most queries returned by \citeauthor{wei:2024}’s query generation prompt included quotation marks, requiring exact matches. This often led to no search results, so we removed quotation marks from all queries.
\end{itemize}

\item For the Bing configuration (based on \citealp{li:2024}) under Step 2:
\begin{itemize}
    \item We used the Bing Web Search API (v7): \url{https://www.microsoft.com/en-us/bing/apis/bing-web-search-api}
    \item For a claim $C$, \citeauthor{li:2024}’s query generation prompt included all claims extracted from $C$'s source sentence as context. We removed this context to ensure that each claim is evaluated independently.
    \item \citeauthor{li:2024} used the Bing Web Search API to retrieve URLs then scraped the content of the corresponding webpages. To avoid scraping, we used the text snippets returned by the Bing Web Search API. This approach is consistent with the Google Search configuration, which also uses text snippets. 
\end{itemize}

\item For Step 3 (based on \citealp{wei:2024}):
\begin{itemize}
    \item We added the following line to \citeauthor{wei:2024}’s verification prompt: \textit{``If any element of the statement is not supported by the knowledge, the statement is not supported.''} We found that this improved the LLM’s ability to evaluate claims containing multiple pieces of information, which is particularly important for $C_{\max}$.
\end{itemize}
\end{enumerate}

\section{Decontextualization Review}
\label{app:decontext_review}

In the first step of our decontextualization evaluation (see \autoref{subsec:decontext} and \autoref{subsec:decontext_results}), an LLM either generates $C_{\max}$, a maximally decontextualized version of a claim $C$, or determines that $C$ is already maximally decontextualized. To validate this step, we randomly sampled 80 sentences (20 per claim extraction method) and reviewed their $C_{\max}$ outputs. For each sentence, we assessed two conditions:
\begin{enumerate}
    \item If $C_{\max}$ was generated, is it entailed by the combined question, context, and $C$?
    \item Does $C_{\max}$ truly represent the maximally decontextualized version of $C$, or is there additional context that should have been included? If the LLM determined that $C$ is already maximally decontextualized, is this assessment correct?
\end{enumerate}

We found that 76 sentences (95\%) met both conditions. Three of the remaining sentences (Examples 1-3 in \autoref{tab:decontext_errors}) violated Condition 1, and one (Example 4) violated Condition 2:
\begin{itemize}
    \item \textbf{Example 1:} The question and context do not mention that conventional fossil fuel vehicles are \textit{``powered by internal combustion engines.''}
    \item \textbf{Example 2:} The question includes djembe as an example of a traditional African drum, but the other examples are not mentioned.
    \item \textbf{Example 3:} The expansion of \textit{``EVSE''} is not provided in the question or context.
    \item \textbf{Example 4:} $C_{\max}$ is not fully decontextualized since it does not clarify that \textit{``new''} refers to the period after the Pulitzer’s inception in 1917 (e.g., \textit{``Featuring writing is a new category added to the Pulitzer Prize after its inception in 1917''}).
\end{itemize}

Examples 1-3 suggest that the LLM occasionally introduces external knowledge into $C_{\max}$. A potential solution is to check whether $C_{\max}$ is entailed (Condition 1 above) and, if not, to regenerate it.


\import{}{decontextualization_errors_table.tex}

\section{Computational Resources}
Generating outputs for Claimify and all methods described in \autoref{subsec:methods} took approximately 10 hours. Generating evaluation outputs (\autoref{sec:experiments}) took approximately 145 hours. These processes ran on a machine with 32GB RAM and an Intel Core i7-11370H CPU @ 3.30GHz (8 CPUs).

\clearpage

\onecolumn
\section{Prompts}

\subsection{Claimify}
\label{app:claimify_prompts}
\subsubsection{Selection}
\import{}{selection_prompt.tex}

\subsubsection{Disambiguation}
\import{}{disambiguation_prompt.tex}

\subsubsection{Decomposition}
\import{}{decomposition_prompt.tex}

\subsection{Evaluation Framework}
\label{app:eval_prompts}
\subsubsection{Entailment}
\import{}{entailment_prompt.tex}

\subsubsection{Element Extraction}
\label{app:element_extraction}
\import{}{element_extraction_prompt.tex}

\subsubsection{Element Coverage}
\label{app:element_coverage}
\import{}{element_coverage_prompt.tex}

\subsubsection{Decontextualization}
\import{}{decontextualization_prompt.tex}

\subsection{Invalid Statements}
\label{app:invalid_statements_prompts}
\subsubsection{Invalid Sentences}
\import{}{invalid_sentences_prompt.tex}

\subsubsection{Invalid Claims}
\import{}{invalid_claims_prompt.tex}

\twocolumn



