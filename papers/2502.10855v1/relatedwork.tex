\section{Related Work}
\label{sec:related_work}

\textbf{Claim Detection.} Many prior works on claim detection aim to identify ``check-worthy'' claims (\citealp{gencheva:2017, jaradat:2018, arslan:2020}). Check-worthiness criteria include public interest \citep{hassan:2017}, potential harm \citep{nakov:2022}, and relevance to a topic, such as the environment \citep{stammbach:2023}. We agree with \citet{konstantinovskiy:2021} and \citet{ni:2024} that check-worthiness is subjective.

\noindent\textbf{Decomposition.} Claimify uses an LLM to extract claims as complete declarative sentences. Alternative decomposition approaches include extracting subject-predicate-object tuples \citep{banko:2007, goodrich:2019, hu:2024:refchecker}, predicate-argument structures \citep{white:2016, zhang:2017, goyal:2020}, questions \citep{fan:2020, chen:2022}, and subsets of tokens \citep{chen:2023:propsegment}.

\noindent\textbf{Ambiguity.} Existing decomposition and decontextualization methods either ignore ambiguity or assume it is always resolvable. An example of the latter is Molecular Facts \citep{gunjal:2024}, a decontextualization method focused on cases where the main entity in a sentence could refer to multiple people (which Claimify would classify as referential ambiguity). Molecular Facts not only forces the LLM to resolve such ambiguities, but it also relies on the model’s parametric knowledge – rather than the sentence and its context – which risks introducing factual inaccuracies. Beyond claim extraction, prior works on ambiguity in fact-checking have explored ambiguous questions \citep{min:2020, kim:2023, zhang:2024} and investigated why annotators disagree on veracity judgements \citep{glockner:2024}.