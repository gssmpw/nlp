\section{Conclusion}
In this work, we introduced \modelname, a general next-X prediction framework for autoregressive visual generation.
Unlike traditional next-token prediction, \modelname reformulates discrete token classification as continuous entity regression, enabling more flexible and semantically meaningful prediction units.
Through systematic exploration, we found that next-cell prediction provides the best balance between local structure and global coherence.
To mitigate exposure bias, we proposed Noisy Context Learning (NCL), which trains the model on noisy entities instead of pristine ground truth inputs, improving robustness and reducing cascading errors.
As a result, \modelname achieves state-of-the-art performance on ImageNet-256 and ImageNet-512.


