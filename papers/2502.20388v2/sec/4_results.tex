\section{Experimental Results}
In this section, we present the main results in~\secref{sec:main}, followed by ablation studies on key design choices in~\secref{sec:ablation}.

\begin{table*}[t]
\renewcommand\arraystretch{1.05}
\centering
\setlength{\tabcolsep}{2.5mm}{}
\begin{tabular}{l|l|c|cc|cc}
type & model     & \#params      & FID$\downarrow$ & IS$\uparrow$ & Precision$\uparrow$ & Recall$\uparrow$ \\
\shline
GAN& BigGAN~\cite{biggan} & 112M & 6.95  & 224.5       & 0.89 & 0.38     \\
GAN& GigaGAN~\cite{gigagan}  & 569M      & 3.45  & 225.5       & 0.84 & 0.61\\  
GAN& StyleGan-XL~\cite{stylegan-xl} & 166M & 2.30  & 265.1       & 0.78 & 0.53  \\
\hline
Diffusion& ADM~\cite{adm}    & 554M      & 10.94 & 101.0        & 0.69 & 0.63\\
Diffusion& LDM-4-G~\cite{ldm}   & 400M  & 3.60  & 247.7       & -  & -     \\
Diffusion & Simple-Diffusion~\cite{diff1} & 2B & 2.44 & 256.3 & - & - \\
Diffusion& DiT-XL/2~\cite{dit} & 675M     & 2.27  & 278.2       & 0.83 & 0.57     \\
Diffusion&L-DiT-3B~\cite{dit-github}  & 3.0B    & 2.10  & 304.4       & 0.82 & 0.60    \\
Diffusion&DiMR-G/2R~\cite{liu2024alleviating} &1.1B& 1.63& 292.5& 0.79 &0.63 \\
Diffusion & MDTv2-XL/2~\cite{gao2023mdtv2} & 676M & 1.58 & 314.7 & 0.79 & 0.65\\
Diffusion & CausalFusion-H$^\dag$~\cite{deng2024causal} & 1B & 1.57 & - & - & - \\
\hline
Flow-Matching & SiT-XL/2~\cite{sit} & 675M & 2.06 & 277.5 & 0.83 & 0.59 \\
Flow-Matching&REPA~\cite{yu2024representation} &675M& 1.80 & 284.0 &0.81 &0.61\\    
Flow-Matching&REPA$^\dag$~\cite{yu2024representation}& 675M& 1.42&  305.7& 0.80& 0.65 \\
\hline
Mask.& MaskGIT~\cite{maskgit}  & 227M   & 6.18  & 182.1        & 0.80 & 0.51 \\
Mask. & TiTok-S-128~\cite{yu2024image} & 287M & 1.97 & 281.8 & - & - \\
Mask. & MAGVIT-v2~\cite{yu2024language} & 307M & 1.78 & 319.4 & - & - \\ 
Mask. & MaskBit~\cite{weber2024maskbit} & 305M & 1.52 & 328.6 & - & - \\
\hline
AR& VQVAE-2~\cite{vqvae2} & 13.5B    & 31.11           & $\sim$45     & 0.36           & 0.57          \\
AR& VQGAN~\cite{vqgan}& 227M  & 18.65 & 80.4         & 0.78 & 0.26   \\
AR& VQGAN~\cite{vqgan}   & 1.4B     & 15.78 & 74.3   & -  & -     \\
AR&RQTran.~\cite{rq}     & 3.8B    & 7.55  & 134.0  & -  & -    \\
AR& ViTVQ~\cite{vit-vqgan} & 1.7B  & 4.17  & 175.1  & -  & -    \\
AR & DART-AR~\cite{gu2025dart} & 812M & 3.98 & 256.8 & - & - \\
AR & MonoFormer~\cite{zhao2024monoformer} & 1.1B & 2.57 & 272.6 & 0.84 & 0.56\\
AR & Open-MAGVIT2-XL~\cite{luo2024open} & 1.5B & 2.33 & 271.8 & 0.84 & 0.54\\
AR&LlamaGen-3B~\cite{llamagen}  &3.1B& 2.18& 263.3 &0.81& 0.58\\
AR & FlowAR-H~\cite{flowar} & 1.9B & 1.65 & 296.5 & 0.83 & 0.60\\
AR & RAR-XXL~\cite{yu2024randomized} & 1.5B & 1.48 & 326.0 & 0.80 & 0.63 \\
\hline
MAR & MAR-B~\cite{mar} & 208M & 2.31 &281.7 &0.82 &0.57 \\
MAR & MAR-L~\cite{mar} &479M& 1.78 &296.0& 0.81& 0.60 \\
MAR & MAR-H~\cite{mar} & 943M&1.55& 303.7& 0.81 &0.62 \\
\hline
VAR&VAR-$d16$~\cite{var}   & 310M  & 3.30& 274.4& 0.84& 0.51    \\
VAR&VAR-$d20$~\cite{var}   &600M & 2.57& 302.6& 0.83& 0.56     \\
VAR&VAR-$d30$~\cite{var}   & 2.0B      & 1.97  & 323.1 & 0.82 & 0.59      \\
\hline
\modelname& \modelname-B    &172M   &1.72&280.4&0.82&0.59 \\
\modelname& \modelname-L   & 608M   & 1.28& 292.5&0.82&0.62\\
\modelname& \modelname-H    & 1.1B    & 1.24 &301.6&0.83&0.64\\
\end{tabular}
\caption{
\textbf{Generation Results on ImageNet-256.}
Metrics include Fréchet Inception Distance (FID), Inception Score (IS), Precision, and Recall. $^\dag$ denotes the use of guidance interval sampling~\cite{guidance}. The proposed \modelname-H achieves a state-of-the-art 1.24 FID on the ImageNet-256 benchmark without relying on vision foundation models (\eg, DINOv2~\cite{dinov2}) or guidance interval sampling~\cite{guidance}, as used in REPA~\cite{yu2024representation}.
}\label{tab:256}
\end{table*}

\subsection{Main Results}
\label{sec:main}
We conduct experiments on ImageNet~\cite{deng2009imagenet} at 256$\times$256 and 512$\times$512 resolutions. Following prior works~\cite{dit,mar}, we evaluate model performance using FID~\cite{fid}, Inception Score (IS)~\cite{is}, Precision, and Recall. \modelname is trained with the same hyper-parameters as~\cite{mar,dit} (\eg, 800 training epochs), with model sizes ranging from 172M to 1.1B parameters. See Appendix~\secref{sec:sup_hyper} for hyper-parameter details.





\begin{table}[t]
    \centering
    \begin{tabular}{c|c|c|c}
      model    &  \#params & FID$\downarrow$ & IS$\uparrow$ \\
      \shline
      VQGAN~\cite{vqgan}&227M &26.52& 66.8\\
      BigGAN~\cite{biggan}& 158M&8.43 &177.9\\
      MaskGiT~\cite{maskgit}& 227M&7.32& 156.0\\
      DiT-XL/2~\cite{dit} &675M &3.04& 240.8 \\
     DiMR-XL/3R~\cite{liu2024alleviating}& 525M&2.89 &289.8 \\
     VAR-d36~\cite{var}  & 2.3B& 2.63 & 303.2\\
     REPA$^\ddagger$~\cite{yu2024representation}&675M &2.08& 274.6 \\
     \hline
     \modelname-L & 608M&1.70& 281.5 \\
    \end{tabular}
    \caption{
    \textbf{Generation Results on ImageNet-512.} $^\ddagger$ denotes the use of DINOv2~\cite{dinov2}.
    }
    \label{tab:512}
\end{table}

\noindent\textbf{ImageNet-256.}
In~\tabref{tab:256}, we compare \modelname with previous state-of-the-art generative models.
Out best variant, \modelname-H, achieves a new state-of-the-art-performance of 1.24 FID, outperforming the GAN-based StyleGAN-XL~\cite{stylegan-xl} by 1.06 FID, masked-prediction-based MaskBit~\cite{maskgit} by 0.28 FID, AR-based RAR~\cite{yu2024randomized} by 0.24 FID, VAR~\cite{var} by 0.73 FID, MAR~\cite{mar} by 0.31 FID, and flow-matching-based REPA~\cite{yu2024representation} by 0.18 FID.
Notably, \modelname does not rely on vision foundation models~\cite{dinov2} or guidance interval sampling~\cite{guidance}, both of which were used in REPA~\cite{yu2024representation}, the previous best-performing model.
Additionally, our lightweight \modelname-B (172M), surpasses DiT-XL (675M)~\cite{dit} by 0.55 FID while achieving an inference speed of 9.8 images per second—20$\times$ faster than DiT-XL (0.5 images per second). Detailed speed comparison can be found in Appendix \ref{sec:speed}.



\noindent\textbf{ImageNet-512.}
In~\tabref{tab:512}, we report the performance of \modelname on ImageNet-512.
Similarly, \modelname-L sets a new state-of-the-art FID of 1.70, outperforming the diffusion based DiT-XL/2~\cite{dit} and DiMR-XL/3R~\cite{liu2024alleviating} by a large margin of 1.34 and 1.19 FID, respectively.
Additionally, \modelname-L also surpasses the previous best autoregressive model VAR-d36~\cite{var} and flow-matching-based REPA~\cite{yu2024representation} by 0.93 and 0.38 FID, respectively.




\noindent\textbf{Qualitative Results.}
\figref{fig:qualitative} presents samples generated by \modelname (trained on ImageNet) at 512$\times$512 and 256$\times$256 resolutions. These results highlight \modelname's ability to produce high-fidelity images with exceptional visual quality.

\begin{figure*}
    \centering
    \vspace{-6pt}
    \includegraphics[width=1\linewidth]{figures/qualitative.pdf}
    \caption{\textbf{Generated Samples.} \modelname generates high-quality images at resolutions of 512$\times$512 (1st row) and 256$\times$256 (2nd and 3rd row).
    }
    \label{fig:qualitative}
\end{figure*}

\subsection{Ablation Studies}
\label{sec:ablation}
In this section, we conduct ablation studies using \modelname-B, trained for 400 epochs to efficiently iterate on model design.

\noindent\textbf{Prediction Entity X.}
The proposed \modelname extends next-token prediction to next-X prediction. In~\tabref{tab:X}, we evaluate different designs for the prediction entity X, including an individual patch token, a cell (a group of surrounding tokens), a subsample (a non-local grouping), a scale (coarse-to-fine resolution), and an entire image.

Among these variants, cell-based \modelname achieves the best performance, with an FID of 2.48, outperforming the token-based \modelname by 1.03 FID and surpassing the second best design (scale-based \modelname) by 0.42 FID. Furthermore, even when using standard prediction entities such as tokens, subsamples, images, or scales, \modelname consistently outperforms existing methods while requiring significantly fewer parameters. These results highlight the efficiency and effectiveness of \modelname across diverse prediction entities.






\begin{table}[]
    \centering
    \scalebox{0.92}{
    \begin{tabular}{c|c|c|c|c}
        model & \makecell[c]{prediction\\entity} & \#params & FID$\downarrow$ & IS$\uparrow$\\
        \shline
        LlamaGen-L~\cite{llamagen} & \multirow{2}{*}{token} & 343M & 3.80 &248.3\\
        \modelname-B& & 172M&3.51&251.4\\
        \hline
        PAR-L~\cite{par} & \multirow{2}{*}{subsample}& 343M & 3.76 & 218.9\\
        \modelname-B&  &172M& 3.58&231.5\\
        \hline
        DiT-L/2~\cite{dit}& \multirow{2}{*}{image}& 458M&5.02&167.2 \\
         \modelname-B& & 172M&3.13&253.4 \\
        \hline
        VAR-$d16$~\cite{var} & \multirow{2}{*}{scale} & 310M&3.30 &274.4\\
        \modelname-B& &172M&2.90&262.8\\
        \hline
        \baseline{\modelname-B}& \baseline{cell} & \baseline{172M}&\baseline{2.48}&\baseline{269.2} \\
    \end{tabular}
    }
    \caption{\textbf{Ablation on Prediction Entity X.} Using cells as the prediction entity outperforms alternatives such as tokens or entire images. Additionally, under the same prediction entity, \modelname surpasses previous methods, demonstrating its effectiveness across different prediction granularities. }%
    \label{tab:X}
\end{table}

\noindent\textbf{Cell Size.}
A prediction entity cell is formed by grouping spatially adjacent $k\times k$ tokens, where a larger cell size incorporates more tokens and thus captures a broader context within a single prediction step.
For a $256\times256$ input image, the encoded continuous latent representation has a spatial resolution of $16\times16$. Given this, the image can be partitioned into an $m\times m$ grid, where each cell consists of $k\times k$ neighboring tokens. As shown in~\tabref{tab:cell}, we evaluate different cell sizes with $k \in \{1,2,4,8,16\}$, where $k=1$ represents a single token and $k=16$ corresponds to the entire image as a single entity. We observe that performance improves as $k$ increases, peaking at an FID of 2.48 when using cell size $8\times8$ (\ie, $k=8$). Beyond this, performance declines, reaching an FID of 3.13 when the entire image is treated as a single entity.
These results suggest that using cells rather than the entire image as the prediction unit allows the model to condition on previously generated context, improving confidence in predictions while maintaining both rich semantics and local details.





\begin{table}[t]
    \centering
    \scalebox{0.98}{
    \begin{tabular}{c|c|c|c}
    cell size ($k\times k$ tokens) & $m\times m$ grid & FID$\downarrow$ & IS$\uparrow$ \\
       \shline
       $1\times1$ & $16\times16$ &3.51&251.4 \\
       $2\times2$ & $8\times8$ & 3.04& 253.5\\
       $4\times4$ & $4\times4$ & 2.61&258.2 \\
       \baseline{$8\times8$} & \baseline{$2\times2$} & \baseline{2.48} & \baseline{269.2}\\
       $16\times16$ & $1\times1$ & 3.13&253.4  \\
    \end{tabular}
    }
    \caption{\textbf{Ablation on the cell size.}
    In this study, a $16\times16$ continuous latent representation is partitioned into an $m\times m$ grid, where each cell consits of $k\times k$ neighboring tokens.
    A cell size of $8\times8$ achieves the best performance, striking an optimal balance between local structure and global context.
    }
    \label{tab:cell}
\end{table}



\begin{table}[t]
    \centering
    \scalebox{0.95}{
    \begin{tabular}{c|c|c|c}
      previous cell & noise time step &  FID$\downarrow$ & IS$\uparrow$ \\
       \shline
       clean & $t_i=0, \forall i<n$& 3.45& 243.5\\
       increasing noise & $t_1<t_2<\cdots<t_{n-1}$& 2.95&258.8 \\
       decreasing noise & $t_1>t_2>\cdots>t_{n-1}$&2.78 &262.1 \\
      \baseline{random noise}  & \baseline{no constraint} &\baseline{2.48} & \baseline{269.2}\\
    \end{tabular}
    }
    \caption{
    \textbf{Ablation on Noisy Context Learning.}
    This study examines the impact of noise time steps ($t_1, \cdots, t_{n-1} \subset [0, 1]$) in previous entities ($t=0$ represents pure Gaussian noise).
    Conditioning on all clean entities (the ``clean'' variant) results in suboptimal performance.
    Imposing an order on noise time steps, either ``increasing noise'' or ``decreasing noise'', also leads to inferior results. The best performance is achieved with the "random noise" setting, where no constraints are imposed on noise time steps.
    }
    \label{tab:ncl}
\end{table}


\noindent\textbf{Noisy Context Learning.}
During training, \modelname employs Noisy Context Learning (NCL), predicting $X_n$ by conditioning on all previous noisy entities, unlike Teacher Forcing.
The noise intensity of previous entities is contorlled by noise time steps $\{t_1, \dots, t_{n-1}\} \subset [0, 1]$, where $t=0$ corresponds to pure Gaussian noise.
We analyze the impact of NCL in~\tabref{tab:ncl}.
When conditioning on all clean entities (\ie, the ``clean'' variant, where $t_i=0, \forall i<n$), which is equivalent to vanilla AR (\ie, Teacher Forcing), the suboptimal performance is obtained.
We also evaluate two constrained noise schedules: the ``increasing noise'' variant, where noise time steps increase over AR steps ($t_1<t_2< \cdots < t_{n-1}$), and the `` decreasing noise'' variant, where noise time steps decrease ($t_1>t_2> \cdots > t_{n-1}$).
While both settings improve over the ``clean'' variant, they remain inferior to our final ``random noise'' setting, where no constraints are imposed on noise time steps, leading to the best performance.




        
