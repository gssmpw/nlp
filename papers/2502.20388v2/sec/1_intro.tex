\section{Introduction}
Autoregressive (AR) models have driven major advances in natural language processing (NLP) through next-token prediction, where each token is generated from its preceding tokens. This framework enables coherent, context-aware text generation, with landmark models like GPT-3~\cite{gpt3} and its successors~\cite{gpt4,chatgpt} setting new benchmarks across diverse NLP applications.

Building on the successes of AR modeling in NLP, researchers have extended this framework to computer vision, particularly for high-fidelity image generation~\cite{vqgan,parti,llamagen,mar,yu2024randomized}. In these approaches, image patches are discretized into tokens~\cite{vqvae} and reshaped into 1D sequences, allowing AR models to predict each token sequentially. However, unlike language, where tokens correspond to semantically meaningful units such as words, vision lacks a universally agreed-upon token definition. This naturally raises the question: 
\emph{How can ``next-token prediction'' be generalized to ``next-X prediction,'' and what constitutes the most suitable X for image generation?}

Additionally, beyond token design, traditional AR models rely on teacher forcing~\cite{williams1989learning} during training, where ground truth tokens are provided at each step instead of the model’s own predictions. While this stabilizes training, it introduces exposure bias~\cite{ranzato2016sequence}, since the model is never exposed to potential errors.
Consequently, during inference, without ground truth guidance, errors accumulate over time, leading to cascading errors and context drift as the model conditions solely on its past predictions.

To address these challenges, we propose \modelname, a general next-X prediction framework that reformulates discrete token \textit{classification} (conditioned on all preceding discrete \textit{ground truth} tokens) into a continuous entity \textit{regression} problem conditioned on all previous \textit{noisy} entities. The regression process is guided by flow-matching~\cite{liu2022flow,lipman2022flow} at each AR step. As illustrated in~\figref{fig:next_x}, within this framework, X serves as a flexible representation that can correspond to an individual patch token, a cell (a group of surrounding tokens), a subsample (a non-local grouping), a scale (coarse-to-fine resolution), or even an entire image.

Unlike teacher forcing~\cite{williams1989learning}, which always provides ground truth inputs, \modelname deliberately exposes the model to noisy contexts during training, allowing it to learn from imperfect, corrupted, or partially inaccurate conditions. We refer to this approach as Noisy Context Learning (NCL), a reformulation that reduces reliance on ground truth inputs, improving robustness and mitigating exposure bias~\cite{ranzato2016sequence} by enabling the model to generalize better during inference.

We demonstrate the effectiveness of \modelname on the challenging ImageNet generation benchmark~\cite{deng2009imagenet}. Through systematic experimentation with different X configurations, we find that \textbf{\textit{next-cell}} prediction—where neighboring tokens are grouped into moderately sized cells (\eg, 8$\times$8 tokens)—yields the best performance by capturing richer spatial-semantic relationships. Leveraging both next-cell prediction and Noisy Context Learning, our base model \modelname-B (172M) outperforms the large DiT-XL~\cite{dit} and SiT-XL~\cite{sit} (675M) while achieving 20$\times$ faster inference. Additionally, our largest model, \modelname-H (1.1B), sets a new state-of-the-art with an FID of 1.24 and runs 2.2$\times$ faster than the previous best-performing model~\cite{yu2024representation} on ImageNet-256~\cite{deng2009imagenet}, without relying on vision foundation models (\eg, DINOv2~\cite{dinov2}) or extra guidance interval sampling~\cite{guidance}.


\begin{figure}[t!]
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.88\linewidth]{figures/teaser1.pdf}
    \vspace{-6pt}
    \caption{
    \textbf{ImageNet-256 Results.}
    Our base model, \modelname-B, outperforms DiT-XL~\cite{dit} and SiT-XL~\cite{sit} while achieving 20$\times$ faster inference, and our largest model, \modelname-H, establishes a new state-of-the-art with an FID of 1.24 on ImageNet-256.
    }
    
    \label{fig:teaser}
\end{figure}  
