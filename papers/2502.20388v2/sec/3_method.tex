\section{Method}
\label{sec:method}
In this section, we first provide an overview of autoregressive modeling with the next-token prediction paradigm in~\secref{sec:ar}, followed by our proposed xAR framework with next-X prediction and Noisy Context Learning in~\secref{sec:xar}.

\subsection{Preliminary: Next-Token Prediction}
\label{sec:ar}
Autoregressive modeling with next-token prediction is a fundamental approach in language modeling where the joint
probability of a token sequence is factorized into a product
of conditional probabilities. Formally, given a sequence \( \boldsymbol{x} = \{x_{1}, x_{2}, \dots, x_{N}\} \),
the model estimates
\begin{equation}
  P(\boldsymbol{x}) = \prod_{n=1}^{N} P \bigl( x_{n} \mid x_{1}, x_{2}, \dots, x_{n-1} \bigr).
\end{equation}
In practice, an autoregressive language model predicts the next token \( x_{n} \) through token classification, conditioned on all preceding tokens \( \{x_{1}, x_{2}, \dots, x_{n-1}\} \). This process proceeds sequentially from left to right (\ie, $n=\{1, \dots, N\}$) until the full sequence is generated.
For visual generation, a VQ tokenizer~\cite{vqvae,vqgan}  discretizes an image into a sequence of tokens. An autoregressive visual generation model then follows the next-token prediction paradigm, sequentially predicting tokens through classification conditioned on previously generated tokens. However, directly applying the next-token prediction paradigm to visual generation introduces several challenges:

\noindent\textbf{Information Density.}
In NLP, each token (\eg, a word) carries rich semantic meaning. In contrast, visual tokens typically represent small image patches, which may not be as semantically meaningful in isolation. A single patch can contain fragments of different objects or textures, making it difficult for the model to infer meaningful relationships between consecutive patches. Additionally, the quantization process in VQ-VAE~\cite{vqvae} can discard fine details, leading to lower-quality reconstructions. As a result, even if the model predicts the next token correctly, the generated image may still appear blurry or lack detail.


\noindent\textbf{Accumulated Errors.}
Teacher forcing~\cite{williams1989learning}, a common training strategy, feeds the model ground truth tokens to stabilize learning. However, this reliance on perfect context causes exposure bias~\cite{acc1,acc2}â€”the model never learns to recover from its potential mistakes. During inference, when it must condition on its own predictions, small errors can accumulate over time, leading to compounding artifacts and degraded output quality. 

To address these challenges, we extend next-token prediction to \textit{next-X prediction}, transitioning from traditional AR to \modelname. This is accomplished by introducing a more expressive prediction entity X and training the model with noisy entities for improved robustness.


\subsection{The Proposed \modelname}
\label{sec:xar}
We introduce \modelname, which consists of two key components: next-X prediction (\secref{sec:next_x}) and Noisy Context Learning  (\secref{sec:self_correct}). We first detail each component, then describe the inference strategy  (\secref{sec:inference}), followed by a discussion on how \modelname enhances visual generation (\secref{sec:xar_discussion}).

\subsubsection{Next-X Prediction}
\label{sec:next_x}
Given an image, we use an off-the-shelf VAE~\cite{vae} (instead of VQ-VAE~\cite{vqvae} to avoid quantization loss) to convert it into a continuous latent $I \in \mathcal{R}^{\frac{H}{f}\times \frac{W}{f}\times C}$, where $H$ and $W$ denote image height and width, $f$ is the downsampling rate (we use $f=16$~\cite{mar}), and $C$ represents the number of channels.
We then construct a sequence of prediction entities $\boldsymbol{X}=\{X_1, X_2, \dots, X_N\}$ based on $I$.
Each $X_i$ is a flexible entity that can represent 
an individual token (an image patch), a cell (a group of surrounding tokens),
a subsample (a non-local grouping), a scale (coarse-to-fine
resolution), or even an entire image.
We outline common choices for X below and refer readers to~\figref{fig:next_x} for visualization and Algorithm~\ref{algo:pseduo_code} for a PyTorch pseudo-code implementation.


\noindent\textbf{Individual Patch Token (\figref{fig:next_x}~(a)).}
When $X_i$ corresponds to a single image patch, \modelname reduces to standard AR modeling, where each token is predicted sequentially.

\noindent\textbf{Cell (\figref{fig:next_x}~(b)).}
The image is divided into an $m\times m$ grid, where each cell has $k \times k$ spatially adjacent tokens\footnote{We also experimented with rectangular cells (\eg, cells with shape $k/2 \times 2k$ or $2k \times k/2$), but observed no significant difference compared to squared cells. Thus, we adopt the simpler squared cell design.}.



\noindent\textbf{Subsample (\figref{fig:next_x}~(c)).}
Entities are created by spatially and uniformly subsampling the image grid~\cite{vqgan}.

\noindent\textbf{Entire Image (\figref{fig:next_x}~(d)).}
As an extreme case, all tokens are grouped into a single entity, \ie, $X = X_1 = I$, transforming \modelname into a flow matching method~\cite{liu2022flow,lipman2022flow}.

\noindent\textbf{Scale (\figref{fig:next_x}~(e)).}
A multi-scale hierarchical representation is constructed, similar to VAR~\cite{var}. Given any scale design $\{s_1, \dots, s_N\}$, we define $X_i = \mathrm{resize}(I, s_i)$, where $\mathrm{resize}$ refers to resizing the latent $I$ to the target scale $s_i$.
By default, we set $X_N = I \in \mathcal{R}^{\frac{H}{f}\times \frac{W}{f}\times C}$ (\ie, $s_N=\frac{H}{f}$), and define
$X_i = \mathrm{resize}(I, \frac{H}{f} \cdot \frac{1}{2^{N-i}})$ (\ie, $s_i=\frac{H}{f} \cdot \frac{1}{2^{N-i}}$)
which progressively refines predictions from coarse to fine scales. Unlike VAR~\cite{var}, our approach generalizes next-scale prediction to any scale configuration and does not require a specially designed multi-scale VQGAN tokenizer. 




\noindent\textbf{Default Choice of X.}
Extensive ablation studies in~\secref{sec:ablation} show that cell (with a size of 8$\times$8 tokens) achieves the best performance among all X designs. Therefore, unless specified otherwise, \modelname adopts 8$\times$8 cells as the default X.




\begin{algorithm}[t]
\caption{PyTorch Pseudo-Code for General Entity X
}
\label{algo:pseduo_code}
\begin{algorithmic}[0]
\footnotesize
\STATE \keyword{from} einops \keyword{import} rearrange
\STATE \keyword{import} torch
\STATE \keyword{import} torch.nn.functional \keyword{as} F
\STATE \class{class} xAR(\keyword{nn.Module}): 
\STATE \quad \quad \comment{\# Construct a sequence of entities based on the input latent.}
\STATE \quad \quad \comment{\# Input: A continuous latent with shape (b, c, h, w).}
\STATE \quad \quad \comment{\# Return: A sequence of entities with shape (b, s, c).}
\STATE \quad \function{def} latent2token(self, latent): 
\STATE \quad \quad return latent.flatten(2).permute(0,2,1)
\STATE \quad \function{def} latent2cell(self, latent, k): 
\STATE \quad \quad \comment{\# k: Group $k\times k$ spatially neighboring tokens into one cell.}
\STATE \quad \quad return rearrange(latent, "b c (h k1) (w k2) -\verb|>| b (h w k1 k2) c", k1=k, k2=k)
\STATE \quad \function{def} latent2subsample(self, latent, distance): 
\STATE \quad \quad \comment{\# distance: Group tokens based on evenly spaced distances.}
\STATE \quad \quad return rearrange(latent, "b c (d1 h) (d2 w) -\verb|>| b (h w d1 d2) c", d1=distance, d2=distance)
\STATE \quad \function{def} latent2scale(self, latent, scales): 
\STATE \quad \quad \comment{\# scales: A sequence of scale design.}
\STATE \quad \quad entities = [F.interpolate(latent, (i,i)).flatten(2).permute(0,2,1) for i in scales]
\STATE \quad \quad entities = torch.cat(entities, dim=1)
\STATE \quad \quad return entities
\end{algorithmic}
\end{algorithm}

\begin{figure*}[t]
    \centering
    \vspace{-14pt}
    \includegraphics[width=\linewidth]{figures/comparisonv5.pdf}
    \vspace{-26pt}
    \caption{
    \textbf{Conditioning Mechanism Comparison between Vanilla AR \vs \ \modelname.}
    During training, vanilla AR conditions on all preceding ground truth tokens (\ie, Teacher Forcing), whereas \modelname conditions on all previous noisy entities, each with different noises (\ie, Noisy Context Learning).
    At inference, vanilla AR suffers from exposure bias, as errors accumulate over AR steps due to its exclusive training on ground truth tokens, leaving it unprepared for imperfect predictions. In contrast, \modelname, trained to handle noisy inputs, reduces reliance on ground truth signals and improves robustness to prediction errors.
    }
    \label{fig:method}
\end{figure*}

\subsubsection{Noisy Context Learning}
\label{sec:self_correct}

\modelname transitions the paradigm from ``discrete token classification'' (conditioned on all preceding \emph{ground truth} tokens) to ``continuous entity regression'' (conditioned on all previous \emph{noisy} entities).
Specifically, unlike traditional AR modeling, which directly classifies \(X_n\) based on all preceding ground truth entities \(\{X_1, \dots, X_{n-1}\}\), \modelname predicts \(X_n\) by minimizing a regression loss derived from flow matching~\cite{lipman2022flow,liu2022flow}, conditioned on all previous noisy entities.

During training, we randomly sample \(n\) noise time steps 
\(\{t_1, \dots, t_n\} \subset [0, 1]\), and draw \(n\) noise samples \(\{\epsilon_1, \dots, \epsilon_n\}\) from the source Gaussian noise distribution.
Specifically, at the $n$-th AR step, the noise samples are drawn as \(\epsilon_n \sim \mathcal{N}(0, I)\), where $\epsilon_n$ and $X_n$ share the same shape~\cite{ldm}.
We construct the interpolated input \(F_n^{t_n}\) as:
\begin{equation}
    F_n^{t_n} = \bigl(1 - t_n\bigr)  X_n + t_n \epsilon_n.
\end{equation}
Note that in $F$, the superscript denotes the flow-matching noise time step, while the subscript represents the AR time step.
We then define the velocity \(V_n^{t_n}\) as:
\begin{equation}
\begin{split}
    V_n^{t_n} &= \frac{dF_n^{t_n}}{dt_n} \\
              &=  \epsilon_n - X_n,
\end{split}
\end{equation}
where \(V_n^{t_n}\) represents the directional flow from \(F_n^{t_n}\) toward \(X_n\), guiding the transformation from the source to the target distribution.

The model is trained to predict the velocity \(V_n^{t_n}\) using all preceding and current noisy entities \(\{F_1^{t_1}, \dots, F_{n}^{t_{n}}\}\):
\begin{equation}
    \mathcal{L} = \sum_{n=1}^N \Bigl\| \mathrm{xAR}\bigl(\{F_1^{t_1}, \dots, F_{n}^{t_{n}}\}, t_n; \theta\bigr) - V_n^{t_n} \Bigr\|^2,
\end{equation}
where \(\mathrm{xAR}\) denotes our \modelname model parameterized by \(\theta\).


We refer to this scheme as Noisy Context Learning (NCL), where the model is trained by conditioning on all previous noisy entities rather than perfect ground truth inputs.
This effectively reduces reliance on clean training signals, improving robustness and mitigating exposure bias~\cite{ranzato2016sequence}.
\figref{fig:method} (Training) provides an illustration of NCL.
Notably, when sampling the time steps \(\{t_1, \dots, t_n\} \subset [0, 1]\), no constraints are imposed (\eg, we do not enforce $t_1>t_2$), allowing the model to experience varying degrees of noise in preceding entities, strengthening its adaptability during inference.














\subsubsection{Inference Scheme}
\label{sec:inference}
\modelname performs autoregressive prediction at the level of entity X. Since ``cell'' is the default choice for X, we use it as a concrete example.
As illustrated in \figref{fig:method} (Inference), \modelname begins by predicting an initial cell $\hat{X}_1$ from a Gaussian noise sample $\epsilon_1 \sim \mathcal{N}(0, I)$ (where $\epsilon_1$ has the same shape as $\hat{X}_1$) via flow matching~\cite{lipman2022flow,liu2022flow}.
Conditioned on the clean estimate $\hat{X}_1$, \modelname generates the next cell $\hat{X}_2$ from another Gaussian noise sample $\epsilon_2$.
This process continues autoregressively, where at the 
$i$-th AR step, the model predicts the next cell $\hat{X}_i$ based on all previously generated clean cells $\{\hat{X}_1, \cdots, \hat{X}_{i-1}\}$ and the newly drawn Gaussian noise sample $\epsilon_i$.
This iterative approach progressively refines the image, ensuring structured and context-aware generation at the cell level.







\subsubsection{Discussion}
\label{sec:xar_discussion}
As discussed in~\secref{sec:ar}, traditional AR modeling for visual generation faces two key challenges: information density and accumulated errors. The proposed \modelname is designed to address these limitations.

\noindent\textbf{Semantic-Rich Prediction Entity.} A cell (\ie, a $k\times k$ grouping of spatially contiguous tokens) aggregates neighboring tokens, effectively capturing both local structures (\eg, edges, textures) and regional contexts (\eg, small objects or parts of larger objects).
This leads to richer semantic representations compared to single-token predictions. By modeling relationships within the cell, the model learns to generate coherent local and regional features, shifting from isolated token-level predictions to holistic patterns.
Additionally, predicting a cell rather than an individual token allows the model to reason at a higher abstraction level, akin to how NLP models predict words instead of characters. The larger receptive field per prediction step contributes more semantic information, bridging the gap between low-level visual patches and high-level semantics.

\noindent\textbf{Robustness to Previous Prediction Errors.} 
The Noisy Context Learning (NCL) strategy trains the model on noisy entities instead of perfect ground truth inputs, reducing over-reliance on pristine contexts. This alignment between training and inference distributions enhances the modelâ€™s ability to handle errors in self-generated predictions.
By conditioning on imperfect contexts, \modelname learns to tolerate minor inaccuracies, preventing small errors from compounding into cascading errors. Additionally, exposure to noisy inputs encourages smoother representation learning, leading to more stable and consistent generations.


