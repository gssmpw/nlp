% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{placeins}

\usepackage{pifont} % For \ding symbols
\usepackage[dvipsnames]{xcolor}
\newcommand{\cmark}{\textcolor{OliveGreen}{\ding{51}}} % Cross mark
\newcommand{\xmark}{\textcolor{BrickRed}{\ding{55}}} % Cross mark

% for COLORED BOXES (tikz and xcolor included)
\usepackage[many]{tcolorbox} 
\definecolor{blue}{HTML}{5989cf}
\definecolor{sub}{HTML}{cde4ff}
\newtcolorbox{todobox}{
    colback = sub, 
    colframe = blue, 
    boxrule = 0pt, 
    leftrule = 6pt % left rule weight
}

% For creating hierarchical tree diagrams
\usepackage[edges]{forest}

% Correctly place figure in appendix
\usepackage{caption}
\usepackage{float}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{Language Models as Evolving Knowledge Bases: A Survey on\\Adapting LLMs to Emerging Knowledge}
\title{Bring Your Own Knowledge: A Survey of Methods\\ for LLM Knowledge Expansion}
% \title{Bring Your Own Knowlege: An Overview of Methods to Keep LLMs Timely}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

% \author{
% Mingyang Wang$^{1,2,3}$ \hspace*{0.2cm}
% {Alisa Stoll$^{5}$}\hspace*{0.2cm}
% {Lukas Lange$^{1}$} \\
%  {\bf Heike Adel$^{4}$ \hspace*{0.2cm} Hinrich Sch\"{u}tze$^{2,3}$ \hspace*{0.2cm} Jannik Str\"{o}tgen$^{5}$} \\
%   $^1$Bosch Center for Artificial Intelligence (BCAI) \hspace*{0.2cm}
%   $^2$LMU Munich \\
%   $^3$Munich Center for Machine Learning (MCML) \\
%   $^4$Hochschule der Medien, Stuttgart\\
%   $^5$Karlsruhe University of Applied Sciences \\
%   \texttt{mingyang.wang2@de.bosch.com}
%   }
\author{
Mingyang Wang$^{1,2,3}$\thanks{Equal contribution.} \hspace*{0.2cm}
Alisa Stoll$^{4}$\footnotemark[1] \hspace*{0.2cm}
Lukas Lange$^{1}$ \\
{\bf Heike Adel$^{5}$ \hspace*{0.2cm} Hinrich Sch\"{u}tze$^{2,3}$ \hspace*{0.2cm} Jannik Str\"{o}tgen$^{4}$} \\
$^1$Bosch Center for Artificial Intelligence (BCAI) \hspace*{0.2cm}
$^2$LMU Munich \\
$^3$Munich Center for Machine Learning (MCML) \\
$^4$Karlsruhe University of Applied Sciences \\
$^5$Hochschule der Medien, Stuttgart\\
\texttt{mingyang.wang2@de.bosch.com}
}
\begin{document}
\maketitle
\begin{abstract}
Adapting large language models (LLMs) to new and diverse knowledge is essential for their lasting effectiveness in real-world applications. This survey provides an overview of state-of-the-art methods for expanding the knowledge of LLMs, focusing on integrating various knowledge types, including factual information, domain expertise, language proficiency, and user preferences. We explore techniques, such as continual learning, model editing, and retrieval-based explicit adaptation, while discussing challenges like knowledge consistency and scalability. Designed as a guide for researchers and practitioners, this survey sheds light on opportunities for advancing LLMs as adaptable and robust knowledge systems.
\end{abstract}

% \begin{todobox}
% \textbf{TODOS weekly - new}
% \begin{itemize}
%     \item Section 5.1, 5.2 shortening (Alisa \cmark)
%     \item Section 4.3 (Mingyang \cmark)
%     \item Check formatting and terms (Mingyang)
% \end{itemize}
% \end{todobox}


% \begin{todobox}
% Don't miss things!
% \begin{itemize}
%     \item Check if we have covered enough works for each method
%     \item Unlearning -> Model Editing
%     \item Model merging
%     \item Adversarial prevention
% \end{itemize}
% \end{todobox}



\section{Introduction}

As large language models (LLMs) are increasingly deployed in real-world applications, their ability to adapt to evolving knowledge becomes crucial for maintaining relevance and accuracy. However, LLMs are typically trained once and thus only have knowledge up to a certain cutoff date, limiting their ability to stay updated with new information. This survey provides a comprehensive overview of methods that enable LLMs to incorporate various types of new knowledge, including factual, domain-specific, language, and user preference knowledge. We survey adaptation strategies, including continual learning, model editing, and retrieval-based approaches, and aim at providing guidelines for researchers and practitioners.

To remain effective, LLMs require updates across multiple dimensions. Factual knowledge consists of general truths and real-time information, while domain knowledge pertains to specialized fields, such as medicine or law. Language knowledge enhances multilingual capabilities, and preference knowledge aligns model behavior with user expectations and values. Ensuring that LLMs can integrate updates across these dimensions is essential for their sustained utility.

\begin{table}[t]
\scalebox{0.8}{
\begin{tabular}{l|c|c|c}
\hline
& Continual & Model & \\
& Learning  & Editing & Retrieval \\
& (§\ref{sec:continual-learning}) &
(§\ref{sec:model-editing}) &
(§\ref{sec:retrieval}) \\ \hline
\textbf{Knowledge Type}    &  &  & \\ 
Fact           & \cmark & \cmark & \cmark \\
Domain         & \cmark & \xmark & \cmark \\
Language       & \cmark & \xmark & \xmark \\
Preference     & \cmark & \cmark & \xmark                    \\ \hline

\textbf{Applicability}    &  &  & \\ 
Large-scale data   & \cmark & \xmark & \cmark \\ 
Precise control & \xmark & \cmark & \cmark \\ 
Computational cost    & \xmark & \cmark & \cmark \\ 
Black-box applicable    & \xmark & \xmark & \cmark \\ 

\hline
\end{tabular}}
\caption{We compare three key approaches for adapting LLMs --- continual learning, model editing, and retrieval --- based on their supported knowledge types and applicability across different criteria.}
\label{tab:method-summary}
\end{table}

Existing LLM adaptation methods differ in approach and application. Continual learning enables incremental updates to models’ parametric knowledge, mitigating catastrophic forgetting \citep{mccloskey1989catastrophic} while ensuring long-term performance. Model editing allows for precise modifications of learned knowledge, providing controlled updates without requiring full retraining. Unlike these \textit{implicit} knowledge expansion methods, which modify the model’s internal parameters, retrieval-based approaches \textit{explicitly} access external information dynamically during inference, reducing dependency on static parametric knowledge. The suitability of these methods for different knowledge types and their general applicability are summarized in Table~\ref{tab:method-summary}. By leveraging these strategies, LLMs can maintain accuracy, contextual awareness, and adaptability to new information. 

% While several prior works have explored aspects of LLM knowledge adaptation, this survey provides a unified perspective on different adaptation strategies. Studies such as \citet{zhang-etal-2023-large} examine temporal factual knowledge updates, while \citet{yao-etal-2023-editing} and \citet{zhang2024comprehensive} focus on knowledge editing methodologies. Additionally, \citet{ke2022continual}, \citet{wu2024continual}, and \citet{wang2024comprehensive} provide comprehensive reviews of continual learning techniques. This survey builds on these contributions by offering a structured taxonomy and examining key challenges such as scalability, controllability, efficiency, and black-box model applicability.

\input{taxonomy-2}

After placing our work into context 
%by comparing it with other surveys
(Section~\ref{sec:rel_surveys}) and defining  knowledge types covered in this paper (Section~\ref{sec:knowledge-types}), we provide an overview of different knowledge expansion methods as detailed in Figure~\ref{fig:taxonomy}.
This work thus surveys diverse research efforts and may serve as a guide for researchers and practitioners aiming to develop and apply adaptable and robust LLMs. We highlight research opportunities %for further development 
and provide insights into optimizing adaptation techniques for various real-world applications.



% - \textbf{Objective}: Introduce the topic of adapting large language models (LLMs) to new knowledge.

% - \textbf{Motivation}: Why is it important to adapt LLMs as evolving knowledge bases? Challenges posed by dynamic knowledge requirements in real-world applications.
% Descripe the target audience.

% - \textbf{Scope}: Define the scope of the survey, focusing on continual pretraining, model editing, with a brief discussion about explicit methods for LLM adaptation. 

% - \textbf{Contributions}: Highlight the unique contributions (compare with existing surveys) of the survey (e.g., a comprehensive review, taxonomy, discussion of challenges and opportunities).





\section{Related Surveys}\label{sec:rel_surveys}
The main goal of our work is to provide researchers and practitioners a broad overview of various types of methods to adapt LLMs to diverse types of new knowledge. 
%In this section, we introduce other, more specialized surveys and how they relate to our paper.
In this section, we explain how other more specialized surveys relate to our paper.

To the best of our knowledge, there is limited prior work that specifically focuses on continuous knowledge expansion for LLMs. 
Closest to our work, \citet{zhang-etal-2023-large} describe temporal factual knowledge updates, while we take a broader perspective by examining methods for adapting LLMs to unseen domain knowledge, expanding language coverage, and incorporating user preferences.
\citet{yao-etal-2023-editing} and \citet{zhang2024comprehensive} provide overviews of knowledge editing methodologies, categorizing approaches of knowledge editing. Similarly, \citet{ ke2022continual}, \citet{wu2024continual} and \citet{wang2024comprehensive} offer a comprehensive overview of continual learning. In contrast, our survey shifts the focus towards a task-oriented perspective on knowledge expansion, detailing how various types of knowledge --- including factual, domain-specific, language, and user preference knowledge --- can be seamlessly integrated to ensure LLMs remain relevant and effective. 

% Overall, by addressing the LLM expansion of diverse knowledge types across various scenarios, our survey aims to provide a distinctive and comprehensive guide for researchers and practitioners for maintaining the timeliness and utility of LLMs.



% \section{Scope and Categorization}
% In this section, we define the different types of knowledge, for which we survey and discuss methods to perform knowledge updates to LLMs.

\section{Knowledge Types}\label{sec:knowledge-types}

Integrating diverse types of knowledge into LLMs is essential to enhance their versatility and effectiveness. Depending on the use case, the type of knowledge that an LLM shall be adapted to, might differ. In this paper, we distinguish four key types of knowledge, which cover a broad range of use cases of researchers and practitioners: \textbf{factual, domain, language, and preference knowledge.}

\textbf{(1)} We define \textbf{factual knowledge} as general truths or contextualized information about the world that can be expressed in factual statements. 
%In this work, we 
We adopt a broad, high-level definition, encompassing finer-grained categorizations, such as commonsense knowledge, cultural knowledge, temporal knowledge, and entity knowledge as subsets of factual knowledge, in contrast to prior works \citep{cao2024life, wu2024continual} using more granular classifications. This inclusive perspective enables a comprehensive exploration of knowledge expansion techniques for LLMs, providing flexibility beyond predefined categories and taxonomies.
%without being constrained by overly narrow definitions or rigid taxonomies.

\textbf{(2)} We define \textbf{domain knowledge} as specialized information relevant to specific fields, such as medicine, law, or engineering, enabling LLMs to perform well in targeted applications. Since LLMs typically excel in general-domain tasks but struggle with specialized content, incorporating domain knowledge is crucial for bridging this gap and improving performance in specific fields.

\textbf{(3)} We define \textbf{language knowledge} as the ability of an LLM to understand, generate, and reason in specific natural or programming languages.\footnote{We distinguish language knowledge from linguistic knowledge as defined by \citet{hernandez2024linearity}. Language knowledge refers to the multilingual capabilities of an LLM, whereas linguistic knowledge falls under factual knowledge, encompassing statements about syntax and grammar.} Its integration focuses on adapting models to new languages and enhancing performance in underrepresented ones for broader applicability.

\textbf{(4)} Finally, we define \textbf{preference knowledge} as the capability of LLMs to tailor their behavior to align with user-specific needs, preferences, or values. Preference knowledge integration involves adapting LLM behavior to meet diverse and dynamic user expectations.

% preference knowledge involves tailoring LLM behavior to align with user-specific needs or preferences, ensuring that the model’s responses are not only accurate and coherent but also safe, ethical, and desirable from the perspective of developers and users. 
%Updating LLMs with diverse knowledge types ensures they remain adaptive and relevant across various tasks and domains.
In the next sections, we survey knowledge expansion methods and explain for which of these four knowledge types they are suitable.

% \subsection{Methods}

% \section{Preliminary and Categorization}

% \subsection{Language Models as Knowledge Bases}

% A knowledge base is a traditional solution to access gold-standard relational data. It is defined as a data structure that stores relational information in the form of triplets that link two entities through symbolic relations. Knowledge bases follow a rigid, manually crafted schema that defines the possible entities and relations, as well as the interactions between them. The schema facilitates querying and updating the knowledge base, while ensuring that the answers are accurate, consistent, and explainable \citep{alkhamissi_review_2022}. \

% \citet{petroni-etal-2019-language} investigate that pre-trained language models (PLMs) contain relational knowledge. They determine that the knowledge encoded in the BERT-large model is comparable to that stored in a knowledge base. Additionally, they discover that the model can efficiently recall factual knowledge without any fine-tuning. This leads them to the conclusion that building a knowledge base with performance equal to that of BERT-large is a non-trivial task. Having used cloze statements to query PLMs for facts, \citet{petroni-etal-2019-language} suggest that LLMs could serve as an alternative to traditional knowledge bases. They emphasize the advantages of LLMs over knowledge bases, such as no need for human supervision during population, supporting an open class of relations to be queried, and easy extensibility. \

% \citet{alkhamissi_review_2022} define five key aspects required for an LLM to function completely as a knowledge base. LLMs, unlike conventional knowledge bases, encode knowledge implicitly and diffusely within their weights. Therefore, they lack controllability, making it difficult to efficiently adapt their knowledge \citep{alkhamissi_review_2022, zhu_modifying_2020}. This is particularly important because LLMs must continuously integrate new information or feedback from changing environments \citep{bubeck_sparks_2023}. Other essential use cases are correcting inaccurate or outdated factual knowledge \citep{lin-etal-2022-truthfulqa} or removing potentially private training data \citep{carlini_extracting_2021}.

% \subsection{Continual Learning and Continual Pretraining}

% Continual learning (CL) is a machine learning approach that allows a model to learn sequentially from a continuous stream of data \citep{biesialska-etal-2020-continual} and simulates the behavior of having seen all data simultaneously \citep{wang2024comprehensive}. The goal is to empower the model to accumulate and reuse knowledge that is acquired sequentially throughout its lifetime \citep{wu2024continual, biesialska-etal-2020-continual}. To achieve this goal, catastrophic forgetting (CF) is the primary challenge to be addressed \citep{wu2024continual}. CF occurs as a result of sequential training on new tasks or data and describes a model's tendency to forget knowledge of previously learned tasks when information from a new task is integrated \citep{kirkpatrick_overcoming_2017}. \

% When applied to LLMs, CL can be divided into three categories corresponding to the training stages of LLMs: Continual Pretraining, Continual Instruction Tuning, and Continual Alignment. Continual Pretraining (CPT) involves further training a PLM on a sequence of corpora using a self-supervised approach. CPT can update LLMs with recent factual information, adapt them to new domains, or extend their supported languages \citep{wu2024continual}.

% \subsection{Model Editing}

% Model editing is a concept that focuses on updating a PLM to correct inaccurate or otherwise undesirable outputs. The goal is to perform a localized update to the parameters that corrects the model output for a specific input $x_e$ to a desired output $y_e$. Ideally, the update should increase the relative likelihood $P(y_e \mid x_e)$ of the desired output, while leaving the outputs for unrelated inputs unchanged. Furthermore, the edited model should generalize the edit to the equivalence neighborhood of $x_e$. This means that it should also produce the correct output for inputs related to $x_e$ \citep{mitchell_fast_2022}.


% \paragraph{Differentiation of CPT and Model Editing.} Both CPT and model editing aim to keep LLMs up-to-date with emerging knowledge, but their primary focuses are distinct. CPT updates facts, improves general linguistic abilities, adapts to new domains, or extends the languages supported by an LLM \citep{wu2024continual}. It can refresh internal knowledge of an LLM in a more comprehensive, global, and scalable manner compared to model editing \citep{jang_towards_2022}. CPT involves updating more parameters and lacks precise control to selectively update specific pieces of knowledge \citep{zhang2024comprehensive}. \

% Model editing focuses on updating factual information without regard to specific tasks or domains. It requires a deeper understanding of LLM functionality, as it selectively modifies how specific knowledge is stored and processed within the model. Compared to CPT, model editing offers a more precise and granular approach to manipulating the model \citep{zhang2024comprehensive}. It is well-suited for renewing small-scale, localized knowledge \citep{zhang-etal-2023-large}. However, updating large volumes of knowledge using model editing methods may be more cumbersome \citep{zhang-etal-2023-large} and prone to errors \citep{gu-etal-2024-model, gupta-etal-2024-model} compared to employing CPT. \

% \citet{ke_continual_2023} also highlight that model editing differs from CPT. They interpret it as a special case of \textit{continual end-task learning} or conventional CL. Each editing task, or batch of editing tasks, can be viewed as a CL task. The goal is to perform correct edits without forgetting unrelated knowledge or previous edits.

% \subsection{Taxonomy of Methods}

% Methods for adapting LLMs can be categorized based on how they update the knowledge in LLMs to align them with emerging knowledge. \citet{zhang-etal-2023-large} group methods into two categories: those that directly modify an LLM's implicitly stored knowledge and those that rely on information explicitly retrieved from external sources to adapt the LLM without directly altering it. The former are referred to as \textit{implicit} methods, while the latter are labeled \textit{explicit} methods. \

% \citet{yao-etal-2023-editing} propose a comparable categorization, but they focus specifically on model editing methods. As counterparts to implicit and explicit methods, they categorize editing methods into methods for modifying LLMs' parameters and methods for preserving LLMs' parameters. \

% Both CPT and model editing are considered implicit approaches because they change parameters \citep{zhang-etal-2023-large}. However, as taken into account by the categorization of \citet{yao-etal-2023-editing}, there are also explicit methods that seek to solve the goal of model editing \citep{madaan-etal-2022-memory, mitchell_memory_2022, zhong-etal-2023-mquake, zheng-etal-2023-edit}. \

% \paragraph{Naive Implicit Approaches.} Simpler approaches to adapt LLMs, such as retraining and fine-tuning, are also considered implicit \citep{zhang-etal-2023-large}. Retraining LLMs from scratch with the latest knowledge, due to their large number of parameters, demands significant computing and energy. This leads to exorbitant costs and CO\textsubscript{2} emissions \citep{patterson_carbon_2021}. For example, training the Llama-2-70B model required more than 1.7 million GPU-hours and resulted in the emission of nearly 300 tons of CO\textsubscript{2} \citep{touvron2023llama}. As a consequence, retraining is impractical. Fine-tuning, on the other hand, leads to degraded performance \citep{bubeck_sparks_2023}. Specifically in model editing, it has been observed that fine-tuning exhibits poor specificity due to overfitting and catastrophic forgetting. This results in the undesirable side effect that prompts that are not related to the modified fact lose their original predictions \citep{zheng-etal-2023-edit, zhu_modifying_2020}. Therefore, it can be argued that CPT and model editing represent more sophisticated approaches for incorporating new information into LLMs and modifying specific factual knowledge. \


\section{Continual Learning}
\label{sec:continual-learning}
Continual learning (CL) is a machine learning paradigm that mimics the human ability to continuously learn and accumulate knowledge without forgetting previously acquired information \citep{chen2018continual}. In the context of knowledge expansion, CL allows LLMs to integrate new corpora and incrementally update the knowledge stored in their parameters. This ensures that LLMs remain adaptable, relevant, and effective as facts, domains, languages, and user preferences evolve over time.

% Language models traditionally follow a training paradigm consisting of pretraining and fine-tuning \citep{}. Within this framework, continual learning is commonly categorized into \textbf{continual pretraining} (CPT) and \textbf{continual end-task learning} (CEL) \citep{}, depending on the stage of application.

% Continual pretraining refers to further pretraining a language model on unseen data corpora using unsupervised learning methods, such as masked language modeling \citep{} or next token prediction \citep{}). The goal of continual pretraining is to enhance the model’s general-purpose language modeling capabilities by incorporating new information from additional training data. In contrast, continual end-task learning involves sequentially fine-tuning a pre-trained LLM on specific downstream tasks using supervised learning. This allows the model to continuously acquire new skills and solve new tasks.

In the era of LLMs, the training of language models usually includes multiple stages: pretraining, instruction tuning, preference alignment, and potentially fine-tuning on a downstream task \citep{shi_continual_2024}. Depending on the stage, continual learning can be categorized into continual pretraining, continual instruction tuning, continual preference alignment, and continual end-task learning \citep{ke_continual_2023, shi_continual_2024}. For knowledge expansion, the focus lies on continual pretraining (CPT) and continual preference alignment (CPA). In contrast, continual instruction tuning and continual end-task learning primarily aim to sequentially fine-tune pretrained LLMs for acquiring new skills and solving new tasks, which fall outside the scope of this survey. 
 
In the following sections, we review existing studies that leverage continual pretraining for updating facts, adapting domains, and expanding languages, and continual alignment for updating user preferences.

% In this paper, we adopt the simplified categorization of continual pretraining and continual end-task learning depending on the training approach, i.e., unsupervised or supervised learning While instruction tuning and preference alignment are essential components of LLM-specific training, they can also be considered specialized forms of downstream task fine-tuning.  This simplification aligns with the broader scope of continual learning and provides a clear framework for discussing methods and applications.

% During its initial pretraining stage, an LLM is trained on static data sourced from web crawls. However, this does not take into account the dynamics of the continuously growing data from various sources. Ideally, a pretrained LLM should continually integrate new information from multiple sources throughout its life \citep{qin-etal-2022-elle}. Yet, pretraining LLMs from scratch with updated corpora of similar scale to those of initial pretraining is excessively computationally expensive. Therefore, it is preferable to incorporate the latest information into LLMs through incremental updates \citep{jang_towards_2022, ke-etal-2022-continual}. \

% Pre-trained LLMs have often been used with continual learning approaches to incrementally learn a sequence of end-tasks, a process referred to as \textit{continual end-task learning} \citep{ke-etal-2022-continual, ke_continual_2023}. However, it is also desirable to continually pre-train LLMs themselves. Continual Pretraining (CPT) includes: (1) \textbf{continual general pretraining}, which involves incrementally updating the LLM with the latest data that aligns with the distribution of the original pretraining data, ensuring the model remains up-to-date and broadly applicable, and (2) \textbf{continual domain-adaptive pretraining}, which further pre-trains the LLM to specialize it for a sequence of specific domains, enabling the model to adapt to domain-specific tasks and datasets effectively.

% % CPT incrementally trains an LLM on a sequence of corpora using self-supervised learning. The goal of CPT is to continuously integrate the latest information into an LLM, adapt it to new domains, or extend its supported languages \citep{wu2024continual}. It differs from the traditional CL approach, also known as continual end-task learning, which aims to incrementally learn a sequence of downstream tasks \citep{ke-etal-2022-continual, ke_continual_2023}. 
 
% \paragraph{Formal Definition.} The CPT task can be formally defined as follows \citep{jang_towards_2022, ke-etal-2022-continual, qin-etal-2022-elle, ke_continual_2023}: We have a sequential stream of corpora $\bar{\mathcal{D}}_N = \{\mathcal{D}_1, \dots, \mathcal{D}_N\}$ and an LLM $\mathcal{M}_0$ that has either been initially pre-trained on a broad or general domain corpus $\mathcal{D}_0$ \citep{jang_towards_2022, ke-etal-2022-continual, ke_continual_2023} or is newly initialized \citep{qin-etal-2022-elle}. At each stage, $i > 0$ in the former case and $i > 1$ in the latter case, the goal is to continually pre-train the PLM $\mathcal{M}_{i-1}$ on a new corpus $\mathcal{D}_i$ with $|\mathcal{D}_i| \ll |\mathcal{D}_0|$ (i.e., $|\mathcal{D}_i$ is much smaller than $\mathcal{D}_0$). CPT aims to preserve the previously learned knowledge from $\bar{\mathcal{D}}_{i-1}$ while integrating the new knowledge from $|\mathcal{D}_i$. \

% \paragraph{Categorization of Methods.} In existing work on CL \citep{ke2022continual, wang2024comprehensive, zhang-etal-2023-large, shi_continual_2024, biesialska-etal-2020-continual}, the methods are classified as regularization-based, replay-based, or architectural-based. This categorization can be adapted to CPT methods. \textit{Regularization}-based methods consolidate knowledge by adding a regularization term to the loss function that penalizes changes of the parameters that are important for previously learned data \citep{kirkpatrick_overcoming_2017, ke_continual_2023}. \textit{Replay}-based (or \textit{rehearsal}-based) methods save a subset of previous training examples in a memory buffer or use a generator to create pseudo-samples of previous tasks, which are replayed when learning new data \citep{rebuffi_icarl_2017, shin_continual_2017, qin-etal-2022-elle}. \textit{Architectural}-based (or \textit{parameter isolation}-based) methods assign different parameters to each task to prevent new data from interfering with parameters learned from previous tasks \citep{zhang-etal-2022-continual, qin-etal-2022-elle, ke-etal-2022-continual}. \\

% In the following, we adopt the categorization of CPT works by \citet{wu2024continual} into three groups: CPT for \textit{updating facts}, CPT for \textit{updating domains}, and CPT for \textit{language expansion}.

\subsection{Continual Pretraining for Updating Facts}
\label{sec:continual-pretraining-facts}

This line of research focuses on updating a language model’s outdated internal factual knowledge by incrementally integrating up-to-date world knowledge \citep{jang_towards_2022, ke_continual_2023}. 

Early studies \citep{sun_ernie_2020, rottger-pierrehumbert-2021-temporal-adaptation, lazaridou2021mind, tacl_a_00459} empirically analyze continual pretraining on temporal data, demonstrating its potential for integrating new factual knowledge. \citet{jin-etal-2022-lifelong-pretraining} and \citet{jang_towards_2022} apply traditional continual learning methods to factual knowledge updates in LLMs, evaluating their effectiveness in continual knowledge acquisition. Similarly, \citet{jang_towards_2022} and \citet{kim-etal-2024-carpe} classify world knowledge into time-invariant, outdated, and new categories --- requiring knowledge retention, removal, and acquisition, respectively --- and benchmark existing continual pretraining methods for knowledge updates. 

Additionally, \citet{hu-etal-2023-meta} introduce a meta-trained importance-weighting model to adjust per-token loss dynamically, enabling LLMs to rapidly adapt to new knowledge. \citet{yu-ji-2024-information} investigate self-information updating in LLMs through continual learning, addressing exposure bias by incorporating fact selection into training losses. 

\subsection{Continual Pretraining for Domain Adaptation}
\label{sec:continual-pretraining-domain}


Continual domain adaptative pretraining \citep{ke-etal-2022-continual, ke_continual_2023, wu2024continual} focuses on incrementally adapting an LLM using a sequence of unlabeled, domain-specific corpora. The objective is to enable the LLM to accumulate knowledge across multiple domains while mitigating catastrophic forgetting \citep{mccloskey1989catastrophic} of previously acquired domain knowledge or general language understanding.

% Starting with a pretrained language model on a general domain \(\mathcal{D}_0\) \citep{ke-etal-2022-continual, ke_continual_2023} or a randomly initialized model \citep{qin-etal-2022-elle}, continual pretraining incrementally trains the model on a sequence of domain-specific corpora $\{\mathcal{D}_i\}_{i=1, \dots, N}$. Existing works leverage various strategies, including data replay \citep{}, regularization techniques \citep{}, and parameter isolation mechanisms \citep{}, to enhance continual learning, with a particular focus on facilitating effective knowledge transfer and mitigating  catastrophic forgetting. 

% In the following sections, we discuss several state-of-the-art methods for continual domain-adaptive pretraining, highlighting their approaches and contributions.

\citet{gururangan-etal-2020-dont}  introduced the term of domain-adaptive pretraining, demonstrating that a second phase of pretraining on target domains can effectively update an LLM with new domain knowledge. It is important to note that further pretraining can lead to catastrophic forgetting of general concepts by overwriting essential parameters. To mitigate this, recent works utilize \textit{parameter-isolation} methods which allocate different parameter subsets to distinct tasks or domains and keep the majority of parameters frozen \cite{razdaibiedina2023progressive, wang-etal-2024-learn, wang-etal-2024-rehearsal}. 
DEMix-DAPT \citep{gururangan-etal-2022-demix} replaces every feed-forward network layer in the Transformer model with a domain expert mixture layer, containing one expert per domain. When acquiring new knowledge, only the newly added expert is trained while all others remain fixed. \citet{qin-etal-2022-elle} propose ELLE for efficient lifelong pretraining on various domains. ELLE starts with a randomly initialized LLM and expands the PLM's width and depth to acquire new knowledge more efficiently. \citet{ke-etal-2022-continual} introduce a continual pretraining system which inserts continual learning plugins to the frozen pretrained language models that mitigate catastrophic forgetting while effectively learn new domain knowledge. Similarly, Lifelong-MoE \citep{pmlr-v202-chen23aq} expands expert capacity progressively, freezing previously trained experts and applying output-level regularization to prevent forgetting. 

In a later work, \citet{ke_continual_2023} apply regularization to penalize changes to critical parameters learned from previous data, preventing catastrophic forgetting. Their approach computes the importance of LLM components, such as attention heads and neurons, in preserving general knowledge, applying soft-masking and contrastive loss during continual pretraining to maintain learned knowledge while promoting knowledge transfer.


% \citet{qin-etal-2022-elle} propose ELLE, a framework for efficient lifelong pretraining. ELLE starts with a randomly initialized language model and incrementally expands the model's width and depth before training on each subsequent domain $\mathcal{D}_i$. To preserve knowledge from earlier domains, it incorporates memory replay by using a subset of previous domain data during pretraining. Additionally, ELLE employs soft prompt tokens as domain indicators to disentangle knowledge learned across domains, facilitating effective domain adaptation. 

% \citet{ke-etal-2022-continual} introduce lightweight continual learning plugins for domain adaptation. These plugins are inserted before the feed-forward network and attention modules in each transformer layer. Unlike the main model, which remains fixed during training, only these plugins are optimized. Task-specific masks are used to protect neurons critical to prior domains during backpropagation, effectively mitigating catastrophic forgetting while adapting to new domain data.

% \citet{ke_continual_2023} propose DAS (Domain-Aware Sparsification), a method designed to prevent catastrophic forgetting by computing the importance of attention heads and neurons for both general and domain-specific knowledge. Using these importance scores, DAS applies soft masks to partially restrict updates to critical parameters. This approach enables incremental training across domains while preserving previously learned knowledge and maintaining general language understanding.

% These approaches employ different strategies: ELLE combines replay-based and architectural methods, continual post-training relies on architectural methods, and DAS adopts a regularization-based approach. Each method contributes uniquely to mitigating CF while facilitating knowledge accumulation and cross-domain transfer.

% \citet{qin-etal-2022-elle} propose \textit{ELLE} for \textit{efficient lifelong pretraining} on various domains. ELLE starts with a randomly initialized LLM which is initially trained on a domain $\mathcal{D}_1$. Before learning a new domain $\mathcal{D}_i$, where $i > 1$, it expands the PLM's width and depth to acquire new knowledge more efficiently. Before continual pretraining on the new domain, the expanded PLM is pre-trained on a subset of the previous corpora to preserve the knowledge and functionality of the non-expanded PLM. During continual pretraining, ELLE employs memory replay of previous domain data to mitigate CF. In addition, a soft prompt token is implanted as a domain indicator to disentangle the different kinds of learned knowledge during pretraining. \

% \citet{ke-etal-2022-continual} introduce a \textit{continual post-training} system to incrementally train a PLM on a sequence of unlabeled domain corpora. They insert two continual learning plugins, shared across all domains, within each transformer layer of the PLM - one before the FFN and one before the attention layer. During continual post-training, the PLM itself remains fixed, and only the two plugins per transformer layer are trained. Each plugin is a two-layer FFN that employs task masks, which protect neurons important for previous domains during backpropagation, helping to mitigate CF when a new domain is learned. \

% \citet{ke_continual_2023} propose DAS, a method that starts with a PLM and incrementally trains it on a sequence of domains while preventing CF of general knowledge and knowledge from previous domains. This is achieved by computing the importance of each attention head and neuron for both the general knowledge in the LLM and the knowledge from the previous domains that have been learned so far. A new domain is trained using the importance scores as soft-masks for these units to partially block updates to the corresponding parameters. \

% ELLE combines elements of both replay-based and architectural-based methods. CPT belongs to the architectural-based methods, while DAS follows a regularization-based approach. \

\subsection{Continual Pretraining for Language Expansion}
\label{sec:continual-pretraining-language}

Continual pretraining (CPT) has emerged as a pivotal strategy for adapting LLMs to new languages, or enhancing performance in underrepresented languages without full retraining \citep{wu2024continual}. Below, we discuss two major areas of expansion enabled by CPT: \textit{natural language expansion} and \textit{programming language expansion}.

\paragraph{Natural Language Expansion.} 
% Continual pretraining  for natural language expansion seeks to increase the accessibility of language models for underrepresented languages and enhances their performance in these languages, addressing the substantial disparities in language support.

Several recent studies have demonstrated the effectiveness of CPT in expanding language coverage. % For instance, Glot500 \citep{imanigooghari-etal-2023-glot500} and EMMA-500 \citep{ji2024emma} utilize CPT and vocabulary extension to support over 500 languages. Glot500, built on XLM-R model \citep{ruder-etal-2019-unsupervised}, and EMMA-500, built on LLaMA 2 model \citep{touvron2023llama}, significantly improve multilingual capabilities from XX to YY languages by leveraging extensive multilingual corpora. 
Glot500 \citep{imanigooghari-etal-2023-glot500} and EMMA-500 \citep{ji2024emma} enhance multilingual capabilities using CPT and vocabulary extension. Glot500, based on XLM-R \citep{ruder-etal-2019-unsupervised}, and EMMA-500, built on LLaMA 2 \citep{touvron2023llama}, expand language support up to 500 languages using extensive multilingual corpora.
Similarly, Aya \citep{ustun-etal-2024-aya} applies continual pretraining to the mT5 model \citep{xue-etal-2021-mt5} using a carefully constructed instruction dataset, achieving improved performance across 101 languages. Furthermore, LLaMAX \citep{lu-etal-2024-llamax} enhances multilingual translation by applying continual pretraining to the LLaMA model family. Supporting over 100 languages, it improves translation quality and promotes language inclusivity.

While covering many languages, many multilingual models exhibit suboptimal performance on medium- to low-resource languages \citep{ruder-etal-2019-unsupervised, touvron2023llama, imanigooghari-etal-2023-glot500}.
To bridge this performance gap, researchers have focused on expanding training corpora and strategically applying continual pretraining to enhance the multilingual capabilities of LLMs. \citet{alabi-etal-2022-adapting}, \citet{wang-etal-2023-nlnde}, \citet{fujii2024continual}, and \citet{zhang-etal-2024-aadam} show that continual pretraining on one or more specific languages significantly improves performance across related languages. \citet{blevins-etal-2024-breaking} extend this approach to the MoE paradigm for better parameter efficiency, while \citet{zheng-etal-2024-breaking} investigate scaling laws for continual pretraining by training LLMs of varying sizes under different language distributions and conditions. Additionally, \citet{tran2020english}, \citet{minixhofer-etal-2022-wechsel}, \citet{dobler-de-melo-2023-focus}, \citet{liu-etal-2024-ofa}, and \citet{minixhofer2024zero} explore advanced tokenization and word embedding techniques to further improve LLMs’ multilingual performance in low-resource settings.


\paragraph{Programming Language Expansion.} Going beyond natural languages, continual pretraining has demonstrated significant potential in enhancing the capabilities of LLMs for understanding and generating programming languages.

CERT, proposed by \citet{zancert}, addresses the challenges of library-oriented code generation using unlabeled code corpora. It employs a two-stage framework to enable LLMs to effectively capture patterns in library-based code snippets. CodeTask-CL \citep{yadav-etal-2023-exploring} offers a benchmark for continual code learning, encompassing a diverse set of tasks such as code generation, summarization, translation, and refinement across multiple programming languages. 
%This benchmark illustrates the adaptability of LLMs in incrementally acquiring and refining programming language capabilities. 
Furthermore, continual pretrained models specifically for code understanding and programming from natural language prompts emerged with LLMs, such as Code\-LLaMA \citep{grattafiori2023code}, Llama Pro \citep{wu-etal-2024-llama}, CodeGemma \citep{team2024codegemma} and StarCoder 2 \citep{lozhkov2024starcoder}, consistently outperform general-purpose LLMs of comparable or larger size on code benchmarks.

% LLaMA Pro \citep{wu-etal-2024-llama} achieves state-of-the-art performance in programming tasks by expanding its Transformer architecture and continually pretraining on a newly curated programming corpus, showcasing its ability to improve and adapt over time.

% These advancements demonstrate the potential of continual pretraining to equip LLMs with robust and versatile language capabilities, paving the way for more effective and adaptable solutions for language expansion.

\subsection{Continual Preference Alignment}
\label{sec:continual-alignment}

Preference alignment ensures that large language models generate responses consistent with human values, improving usability, safety, and ethical behavior. While techniques like Reinforcement Learning from Human Feedback (RLHF) \citep{ziegler2019fine, lambert2022illustrating} align LLMs with static preferences, societal values evolve, requiring continual preference alignment (CPA). It enables LLMs to adapt to emerging preferences while preserving previously learned values, ensuring relevance, inclusivity, and responsiveness to shifting societal expectations. Despite its importance, CPA remains a relatively underexplored area. Below, we briefly discuss two representative works that highlight the potential of this approach: 

\citet{zhang2023copf} propose a non-reinforcement learning approach for continual preference alignment in LLMs. Their method uses function regularization by computing an optimal policy distribution for each task and applying it to regularize future tasks, preventing catastrophic forgetting while adapting to new domains. This provides a single-phase, reinforcement learning-free solution for maintaining alignment across diverse tasks. \citet{zhang2024cppo} introduce Continual Proximal Policy Optimization (CPPO), integrating continual learning into the RLHF framework to accommodate evolving human preferences. CPPO employs a sample-wise weighting strategy to balance policy learning and knowledge retention, consolidating high-reward behaviors while mitigating overfitting and noise. % CPPO is task-agnostic and model-agnostic, with minimal impact on the time and space complexity of PPO, making it a practical and efficient approach for continual alignment in RLHF scenarios.

% While promising, CPA faces challenges in balancing adaptability with stability, mitigating bias amplification, and developing robust evaluation metrics. 
As the demand for responsive and inclusive AI grows, CPA is key to keeping LLMs ethical and aligned with evolving user needs, requiring further research to reach its full potential.



\subsection{Applicability and Limitations}
Continual learning is a versatile framework for expanding LLM knowledge across facts, domains, languages, and preferences. It excels in large-scale knowledge integration, retaining previously learned knowledge, 
%and supporting diverse applications, 
making it well-suited for tasks like domain adaptation and language expansion \citep{bu2021gaia, jin-etal-2022-lifelong-pretraining, cossu2024continual}. 

However, CL has notable limitations, including a lack of precise control compared to model editing (cf.\ Section~\ref{sec:model-editing}) and retrieval-based methods (cf.\ Section~\ref{sec:retrieval}), inefficiency due to the computational demands of retraining, and limited applicability in black-box models. 
These challenges highlight the 
need for alternative approaches 
%complementarity of CL with approaches 
like model editing and retrieval, which offer more targeted and efficient updates. 

% Additionally, a persistent challenge in CL is catastrophic forgetting, making it essential to balance the acquisition of new knowledge with the retention of previously learned knowledge for effective LLM knowledge expansion. Addressing these limitations will further enhance CL’s role in dynamic and adaptive knowledge expansion.

\section{Model Editing} 
\label{sec:model-editing}

Model editing offers a controllable and efficient solution to update factual knowledge and user preferences in LLMs.
%by allowing precise, gradient-based updates to an LLM's parameters without compromising the LLM's overall performance. 
Introduced by \citet{zhu_modifying_2020}, \citet{de-cao-etal-2021-editing} and \citet{mitchell_fast_2022}, it aims at modifying the model’s predictions for specific inputs without affecting unrelated ones.

\citet{yao-etal-2023-editing} and \citet{zhang2024comprehensive} define four key evaluation metrics for model editing: \textbf{(1) reliability}, ensuring the edited model produces the target prediction for the target input; \textbf{(2) generalization}, requiring the edited knowledge to apply to all in-scope inputs — inputs that are directly related to the target input, including rephrasings and semantically similar variations; \textbf{(3) locality}, preserving original outputs for unrelated out-of-scope inputs; and \textbf{(4) portability}, extending the generalization metric by assessing how well updated knowledge transfers to complex rephrasings, reasoning chains, and related facts. 

While recent works \citep{mitchell_memory_2022, madaan-etal-2022-memory, zhong-etal-2023-mquake, zheng-etal-2023-edit} use \textit{model editing} and \textit{knowledge editing} interchangeably for updating factual knowledge, we distinguish between them: model editing is a subset of knowledge editing that modifies model parameters, whereas retrieval-based methods update knowledge dynamically without altering the model’s parameters (see Section~\ref{sec:retrieval}).
% Model editing research has primarily focused on modifying factual knowledge. Recently, however, model editing has also been applied to other areas, particularly aligning LLMs with user preferences.

\subsection{Model Editing for Updating Facts}
\label{sec:model-editing-facts}

To address outdated or incorrect information \citep{lazaridou2021mind}, model editing research focuses on selectively modifying this knowledge. Below, we highlight key works in this area.

KnowledgeEditor \citep{de-cao-etal-2021-editing} uses a hypernetwork to predict parameter shifts for modifying a fact, trained via constrained optimization for locality. 
%However, it was evaluated only on small to medium LLMs and later found to fail for larger ones \citep{mitchell_fast_2022, yao-etal-2023-editing}.
Similarly, MEND \citep{mitchell_fast_2022} trains a hypernetwork per LLM layer and decomposes the fine-tuning gradient into a precise one-step parameter update.
Given the findings that feed-forward layers in transformers function as key-value memories \citep{geva-etal-2021-transformer}, \citet{dai-etal-2022-knowledge} introduce a knowledge attribution method to identify these neurons and directly modify their values 
%in the second linear layer
via knowledge surgery.

% Recent works adopt the locate-and-edit strategy for more precise model editing: Using causal tracing, \citet{meng_locating_2022} identify middle-layer feed-forward networks in LLMs as crucial for predicting a fact about a subject. Viewing the second layer MLP weight matrix as linear associative memory, they propose ROME, which inserts a new fact into a memory by deriving a closed-form solution of a constrained least-squares problem.
% MEMIT \citep{meng_mass-editing_2023} builds on ROME, which updates one fact at a time, to update hundreds or thousands of facts simultaneously by modifying MLP weights across critical middle layers. It has been shown to preserve generalization and locality even with many edits. 
% \citet{wang-etal-2024-editing} focus on modifying conceptual rather than instance-level factual knowledge. Using methods like ROME and MEMIT, they distort definitions of concepts in LLMs and explore whether relationships to concrete instances change accordingly. Their findings show that while concept-level edits are reliable, their impact on related instances is limited.
Recent works employ a locate-and-edit strategy for precise model editing. Using causal tracing, \citet{meng_locating_2022} identify middle-layer feed-forward networks as key to factual predictions and propose ROME, which updates facts by solving a constrained least-squares problem in the MLP weight matrix. MEMIT \citep{meng_mass-editing_2023} extends ROME to modify thousands of facts simultaneously across critical layers while preserving generalization and locality. BIRD \citep{ma2023untying} introduces bidirectional inverse relationship modeling
% , incorporating a set of editing objectives designed 
to mitigate the reverse curse \citep{berglund2023reversal} in model editing. While editing FFN layers has proven effective, PMET \citep{li2024pmet} extends editing to attention heads, achieving improved performance. 
%To minimize unintended alterations, AlphaEdit \citep{fang2024alphaedit} projects perturbations into the null space of preserved knowledge before editing model parameters.
\citet{wang-etal-2024-editing} further shift the focus to conceptual knowledge, using ROME and MEMIT to alter concept definitions,
%and assess their effect on related instances
finding that concept-level edits are reliable but have limited influence on concrete examples.


\subsection{Model Editing for Updating Preferences}
\label{sec:model-editing-preference}

Recent works expand model editing beyond factual corrections to aligning LLMs with user preferences, such as ensuring safety,
%\citep{wang-etal-2024-detoxifying}
reducing bias, 
%\citep{chen_large_2024}
and preserving privacy
%\citep{wu-etal-2023-depn, patil_2024_can}
. 
% It has also been applied to adjust LLMs to reflect human personality traits \citep{mao_editing_2024}, particularly in response to opinion-related questions. 
% Below, we briefly review representative works in this research direction.

\citet{wang-etal-2024-detoxifying} use model editing to detoxify LLMs, ensuring safe responses to adversarial inputs 
%while generalizing to other malicious inputs 
and preserving general LLM capabilities, such as fluency, knowledge question answering, and content summarization. 
%They identify toxic regions and modify corresponding parameters, proposing a baseline that rivals or surpasses direct preference optimization. 
Their results show that model editing is promising for detoxification but slightly affects general capabilities. 
Since LLMs can exhibit social biases \citep{gallegos-etal-2024-bias}, \citet{chen_large_2024} propose fine-grained bias mitigation via model editing. Inspired by \citet{meng_locating_2022}, they identify key layers responsible for biased knowledge and insert a feed-forward network to adjust outputs with minimal parameter changes, ensuring generalization, locality, and scalability.
% As LLMs may memorize and leak training examples \citep{carlini_extracting_2021}, 
For privacy protection, \citet{wu-etal-2023-depn} extend \citet{dai-etal-2022-knowledge}'s work by identifying privacy neurons that store sensitive information. Using gradient attribution, they deactivate these neurons, reducing private data leakage while preserving model performance.  
%explore model editing for \textbf{privacy protection}. Building on \citet{dai-etal-2022-knowledge}, they propose that privacy neurons store sensitive information. Using gradient attribution, they identify and deactivate these neurons. Their method supports batch processing and mitigates private data leakage without compromising model performance.
Moreover, \citet{mao_editing_2024} apply model editing techniques like MEND to modify personality traits in LLMs, aligning responses to opinion-based questions with target personalities. While effective, this approach can degrade text generation quality.

% \citet{mao_editing_2024} use model editing methods like MEND to \textbf{modify personality traits} in LLMs, aligning responses to opinion-based questions with a target personality. Their findings suggest that while personality editing is feasible, it compromises text generation quality.

\subsection{Applicability and Limitations}

Model editing complements continual learning by allowing fine-grained knowledge updates with lower computational costs.
However, research has primarily focused on structured, relational, and instance-level knowledge, with limited exploration of other knowledge types, multilingual generalization, and cross-lingual transfer \citep{nie2024bmike, wei-etal-2025-mlake}.

Additionally, model editing faces several technical challenges, including limited locality and gradual forgetting in large-scale edits \citep{bu2019deep, mitchell_memory_2022, gupta-etal-2024-model, li2024should}, making it more suitable for minor updates.
Additionally, it can impact general LLM capabilities \citep{gu-etal-2024-model, wang-etal-2024-better} and downstream performance \citep{gupta-etal-2024-model}, potentially causing model collapse \citep{yang-etal-2024-fall}. Addressing these issues will enhance model editing’s role alongside continual learning and retrieval, ensuring greater precision in dynamic knowledge adaptation.


% Studies on large-scale editing show limited locality and gradual forgetting of prior edits \citep{bu2019deep, mitchell_memory_2022, gupta-etal-2024-model, li2024should}, making it best suited for minor updates. However, scaling to multiple sequential edits is crucial for practical usability. Moreover, model editing can impact general LLM capabilities \citep{gu-etal-2024-model} and downstream performance \citep{gupta-etal-2024-model}, leading to model collapse \citep{yang-etal-2024-fall} and catastrophic forgetting in sequential edits. Addressing these challenges will strengthen model editing’s role alongside continual learning and retrieval, improving controllability and precision in dynamic knowledge adaptation.

\section{Retrieval-based Methods}
\label{sec:retrieval}

Continual learning and model editing modify a model’s parameters to update its internal knowledge, making them implicit knowledge expansion methods \citep{zhang-etal-2023-large}. In contrast, retrieval-based methods \citep{lewis2020retrieval} explicitly integrate external knowledge, allowing models to overwrite outdated or undesired information without parameter modifications. These methods leverage external sources, such as databases, off-the-shelf retriever systems, or the Internet, and thus provide up-to-date or domain-specific knowledge \citep{zhang-etal-2023-large}, making them effective for factual updates and domain adaptation.
% \citep{mitchell_memory_2022, madaan-etal-2022-memory, zhong-etal-2023-mquake}.

% While recent works \citep{mitchell_memory_2022, madaan-etal-2022-memory, zhong-etal-2023-mquake, zheng-etal-2023-edit} use \textit{model editing} and \textit{knowledge editing} interchangeably for updating factual knowledge, we distinguish between them: model editing is a subset of knowledge editing that modifies model parameters (see Section~\ref{sec:model-editing}), whereas retrieval-based methods update knowledge dynamically without altering the model’s parameters.

\subsection{Retrieval-based Methods for Updating Facts}
\label{sec:retrieval-facts}

Retrieval-based methods enhance LLMs by pairing them with an updatable datastore, ensuring access to current factual information. An early approach, retrieval-augmented generation (RAG) \citep{lewis2020retrieval}, fine-tunes a pre-trained retriever end-to-end with the LLM to improve knowledge retrieval. Similarly, kNN-LM \citep{khandelwal_generalization_2020} interpolates the LLM’s output distribution with k-nearest neighbor search results from the datastore, with later works optimizing efficiency \citep{he-etal-2021-efficient, alon_retomaton_2022} and adapting it for continual learning \citep{peng_semiparametric_2023}.

For factual knowledge editing, \citet{tandon-etal-2022-learning} store user feedback for post-hoc corrections, while \citet{mitchell_memory_2022}, \citet{madaan-etal-2022-memory}, and \citet{dalvi-mishra-etal-2022-towards} retrieve stored edits to guide responses. \citet{chen-etal-2024-robust} introduce relevance filtering to efficiently handle multiple edits. Retrieval-based in-context learning \citep{zheng-etal-2023-edit, ram-etal-2023-context, mallen-etal-2023-trust, yu-etal-2023-augmentation, shi-etal-2024-replug, bi2024decoding} enables dynamic factual updates. 

For complex reasoning, retrieval supports multi-hop question answering and iterative prompting: \citet{zhong-etal-2023-mquake} propose iterative prompting for multi-hop knowledge editing, while \citet{gu-etal-2024-pokemqa} use a scope detector to retrieve relevant edits and improve question decomposition via entity extraction and knowledge prompts. Similarly, \citet{shi_retrieval_2024} enhance multi-hop question answering by retrieving fact chains from a knowledge graph with mutual information maximization and redundant fact pruning. 

In multi-step decision-making, retrieval is combined with Chain-of-Thought (CoT) reasoning \citep{trivedi-etal-2023-interleaving, press-etal-2023-measuring}. Retrieval also aids post-generation fact-checking and refinement \citep{gao-etal-2023-rarr, peng2023checkfactstryagain, song_knowledge_2024} by revising generated text or prompts based on retrieved facts. 


% For reasoning over complex multi-step tasks, \citet{trivedi-etal-2023-interleaving} interleave retrieval with Chain-of-Thought (CoT) reasoning, while \citet{press-etal-2023-measuring} enhance CoT by prompting LLMs to generate follow-up questions and answer them using a search engine.

% \citet{gao-etal-2023-rarr} and \citet{peng2023checkfactstryagain} retrieve information post-generation to revise generated text or prompts based on retrieved facts. Likewise, \citet{song_knowledge_2024} apply fine-grained modifications to the response, preserving privacy and textual style.

For a more comprehensive review of retrieval-based factual knowledge updates, we refer to \citet{zhang-etal-2023-large}.

%As earlier attempts for utilizing retrieval-based approached for knowledge editing, \citet{mitchell_memory_2022} and \citet{madaan-etal-2022-memory} freeze the base model and store edit examples or user feedback in memory. Given an edit request, the corresponding example or feedback is retrieved to guide the model's response. \citet{zhong-etal-2023-mquake} further apply such approaches for multi-hop knowledge editing by iteratively prompting the model to adjust its answers based on relevant retrieved facts. More recently, \citet{zheng-etal-2023-edit},  \citet{mallen-etal-2023-trust}, \citet{shi-etal-2024-replug}, and \citet{bi2024decoding} combine retrieval with the in-context learning capabilities of LLMs for updating factual knowledge. 


% \textcolor{red}{we should at least mention standard RAG. By updating the data sources, you can easily have constantly updated fact models.} --> I shortly mention it.

\subsection{Retrieval-based Methods for Domain Adaptation}
\label{sec:retrieval-domain}
Retrieval-based methods have been widely adopted for various domain-specific tasks, e.g., in science and finance. By integrating retrieved external knowledge, these models enhance their adaptability to specialized domains, improving decision-making, analysis, and information synthesis. 

In the biomedical domain, retrieval-based approaches facilitate tasks, such as molecular property identification and drug discovery by integrating structured molecular data and information about biomedical entities like proteins, molecules, and diseases \citep{wang2023retrieval, liu2023multi, wang2024biobridge, yang2024promptbased}. For instance, \citet{wang2023retrieval} and \citet{li2024empowering} introduce retrieval-based frameworks that extract relevant molecular data from databases to guide molecule generation. In protein research, retrieval-based approaches enhance protein representation and generation tasks \citep{ma-etal-2024-retrieved, sun2023graphvf}. Additionally, \citet{lozano2023clinfo} develop a clinical question-answering system that retrieves relevant biomedical literature to provide more accurate responses in medical contexts. 

The finance domain, characterized by its data-driven nature, also benefits from retrieval-based methods \citep{li2024incorporating, li2024knowledge}. \citet{zhang2023enhancing} enhance financial sentiment analysis by retrieving real-time financial data from external sources. %and integrating it with user queries to improve prediction accuracy
Furthermore, financial question-answering also benefits from retrieval-based methods, which involves extracting knowledge from professional financial documents. \citet{lin2024revolutionizing} introduces a PDF parsing method integrated with retrieval-augmented LLMs to retrieve relevant financial insights. 
% Furthermore, \citet{yepes2024financial} propose a document chunking method based on structural features rather than simple paragraph segmentation, leading to more precise retrieval and improved RA-LLM performance. 

% Overall, retrieval-based methods enable LLMs to adapt to domain-specific challenges by dynamically incorporating external knowledge, thereby improving their accuracy and applicability in specialized fields.



% propose IKE, an in-context learning approach for model editing that prompts an LLM with a probing prompt, a new fact, and k-nearest-neighbor demonstrations. It achieves strong generalization and locality, especially in larger LLMs, but is constrained by input length. Following this, \citet{} further explore the idea of in-context editing, specifically focusing on improving the generalizability for editing and overcoming contextual length constraints.

%\citet{zhong-etal-2023-mquake} introduce MeLLo, an approach that keeps the base model frozen and stores edited facts in an external memory. It is specifically designed for multi-hop questions and does not require any additional training. The method involves iteratively prompting the base model to decompose a multi-hop question into subquestions. For each subquestion, the base model generates a tentative answer based on its unedited knowledge. Additionally, the subquestion is used to retrieve the most relevant edited fact from the memory. The base model is then prompted to self-check whether the retrieved edited fact contradicts its generated answer. If it does, the base model updates its answer accordingly. The process is repeated with the generation of the next subquestion based on this answer until no further subquestions remain.

% \citet{madaan-etal-2022-memory} propose MemPrompt, an approach that combines an LLM with a memory that, in contrast, stores user feedback instead of factual corrections. In particular, it stores key-value pairs consisting of a misunderstood user input and the user's corrective feedback. The idea is to utilize stored user feedback for unseen questions. A retriever queries the memory for a user input similar to the current prompt. If there is a match, the corresponding user feedback is appended to the prompt to modify the LLM's behavior. 

% \citet{zheng-etal-2023-edit} investigate the potential of in-context learning for model editing. In their proposed method, IKE, an LLM is prompted with a probing prompt, a new fact, and several demonstrations. The demonstrations are created from the k-nearest-neighbor facts from the training corpus. Three types of demonstration formatting templates are used to, for example, guide the LLM towards generalization and locality. IKE has demonstrated strong performance with regard to generalization and locality, especially for larger LLMs. However, the number of editing facts is limited by the LLM's input length.

% 

% Implicit methods for altering knowledge, which depend on parameter modifications, have drawbacks due to high computational costs \citep{mitchell_memory_2022, zheng-etal-2023-edit}. They often require specialized training of either the base model \citep{sinitsin_editable_2020} or a hypernetwork \citep{de-cao-etal-2021-editing, mitchell_fast_2022}. Furthermore, implicit methods struggle with many sequential edits and exhibit limited generalization and locality \citep{mitchell_memory_2022, zheng-etal-2023-edit}.

% Explicit methods aim to address some of these shortcomings by utilizing external knowledge sources. Retrieving knowledge from external memory units and augmenting LLMs with this information can help prevent the generation of non-factual or outdated information. This approach also allows LLMs to access latest information, reducing the need for frequent updates to the model itself \citep{mialon_augmented_2023}.

% Explicitly aligning LLMs combines the LLM with a growing external memory that stores information beyond the knowledge captured in the model's parameters \citep{mitchell_memory_2022, zhong-etal-2023-mquake, madaan-etal-2022-memory}. Some methods utilize the base model's in-context learning ability \citep{ zhong-etal-2023-mquake, madaan-etal-2022-memory, zheng-etal-2023-edit}, while others override the base model \citep{mitchell_memory_2022}.

% \citet{mitchell_memory_2022} propose SERAC, an explicit model editing approach that preserves the base model while storing edit examples in a memory. It uses the frozen base model and memory together with a small auxiliary scope classifier and a counterfactual model. The scope classifier and counterfactual model require supervised training prior to editing. For a given input, the scope classifier determines whether the input falls within the scope of any stored edit example. If so, the input and the edit example with the highest probability of being in scope are passed to the counterfactual model. This model predicts the input label based on both the input and the edit example, overriding the base model. If the input is out of scope for all stored edit examples, the base model's prediction is used. SERAC preserves the general abilities of the LLM. However, \citet{hsueh-etal-2024-editing} have demonstrated that SERAC performs poorly in scenarios involving multiple edits due to the scope classifier's weakness in identifying relevant edit examples. 

% \citet{zhong-etal-2023-mquake} introduce MeLLo, an approach that keeps the base model frozen and stores edited facts in an external memory. It is specifically designed for multi-hop questions and does not require any additional training. The method involves iteratively prompting the base model to decompose a multi-hop question into subquestions. For each subquestion, the base model generates a tentative answer based on its unedited knowledge. Additionally, the subquestion is used to retrieve the most relevant edited fact from the memory. The base model is then prompted to self-check whether the retrieved edited fact contradicts its generated answer. If it does, the base model updates its answer accordingly. The process is repeated with the generation of the next subquestion based on this answer until no further subquestions remain.

% \citet{madaan-etal-2022-memory} propose MemPrompt, an approach that combines an LLM with a memory that, in contrast, stores user feedback instead of factual corrections. In particular, it stores key-value pairs consisting of a misunderstood user input and the user's corrective feedback. The idea is to utilize stored user feedback for unseen questions. A retriever queries the memory for a user input similar to the current prompt. If there is a match, the corresponding user feedback is appended to the prompt to modify the LLM's behavior. 

% \citet{zheng-etal-2023-edit} investigate the potential of in-context learning for model editing. In their proposed method, IKE, an LLM is prompted with a probing prompt, a new fact, and several demonstrations. The demonstrations are created from the k-nearest-neighbor facts from the training corpus. Three types of demonstration formatting templates are used to, for example, guide the LLM towards generalization and locality. IKE has demonstrated strong performance with regard to generalization and locality, especially for larger LLMs. However, the number of editing facts is limited by the LLM's input length.

\subsection{Applicability and Limitations}
\label{sec:retrieval-discussion}
Despite their advantages, retrieval-based methods also come with several limitations. A major challenge is their reliance on external knowledge sources, which can introduce inconsistencies or outdated information if not properly curated \citep{jin-etal-2024-bider, xu-etal-2024-knowledge-conflicts}. Their effectiveness also depends on the quality and scope of the retrieval system \citep{bai-etal-2024-mt, liu2024graphsnapshot}; poor indexing or noisy retrieval may lead to irrelevant or misleading information. Another key issue is maintaining knowledge consistency across queries. Since retrieval-based methods do not update model parameters, contradictions can arise between retrieved facts and previously generated responses, affecting coherence 
\citep{njeh2024enhancing, zhao2024dense, li-etal-2024-graphreader}.  
% \citep{njeh2024enhancing, cheng2024relic, zhao2024dense, li-etal-2024-graphreader}.  

Addressing these challenges is essential to improving retrieval-based approaches and ensuring their seamless integration with other LLM adaptation techniques.

\section{Challenges, Opportunities, Guidelines}

\paragraph{Solving Knowledge Conflicts.}
An inherent challenge of expanding a model’s knowledge is the emergence of knowledge conflicts, which can undermine the consistency and trustworthiness of LLMs \citep{xu-etal-2024-knowledge-conflicts}. Studies have identified various types of conflicts following knowledge updates, including (i) temporal misalignment \citep{luu-etal-2022-time}, where outdated and newly learned facts coexist inconsistently, (ii) model inconsistencies \citep{huang2021factual}, where responses to similar queries vary unpredictably, and (iii) hallucinations \citep{ji2023survey}, where the model generates fabricated or contradictory information. While some efforts have been made to address these issues \citep{zhang-choi-2023-mitigating, mallen-etal-2023-trust, zhou-etal-2023-context, xie2024adaptive}, they remain an open challenge that requires further research and more robust solutions.

\paragraph{Minimizing Side Effects.}
Continual learning and model editing, both of which involve modifying model parameters, inevitably introduce side effects. A major challenge in continual learning is catastrophic forgetting \citep{mccloskey1989catastrophic}, where newly acquired knowledge overwrites previously learned information. In LLMs, the multi-stage nature of training exacerbates this issue, leading to cross-stage forgetting \citep{wu2024continual}, where knowledge acquired in earlier stages is lost as new training phases are introduced.
For model editing, recent studies have shown that large-scale edits, particularly mass edits, can significantly degrade the model’s general capabilities, such as its language modeling performance \citep{wang-etal-2024-better} or accuracy on general NLP tasks \citep{li2024unveiling, li2024should, wang2024missing}.
Effectively addressing these challenges is crucial for maximizing the potential of these methods for large-scale knowledge expansion while maintaining model stability and overall performance.

\paragraph{Comprehensive Benchmarks.}
Although this paper explores the properties, strengths, and weaknesses of various methods for knowledge expansion, the discussion remains largely theoretical due to the lack of a comprehensive benchmark datasets for a uniform evaluation and a proper comparison. Existing works, such as \citet{jang_towards_2022}, \citet{liska2022streamingqa}, and \citet{kim-etal-2024-carpe}, provide factual knowledge-based datasets and evaluate continual pretraining and/or retrieval-based methods. However, their experiments are limited in scale and fail to offer a comprehensive assessment. Developing benchmarks that encompass a variety of knowledge types and enable the evaluation of all methods would provide a more holistic and systematic understanding of their relative effectiveness.


\paragraph{General Guideline.}
Selecting the appropriate method for knowledge expansion in LLM depends on the application context and type of knowledge that needs to be updated. 

(\romannumeral1) For \textbf{factual knowledge}, model editing is ideal for precise, targeted updates, such as correcting specific facts, due to its efficiency and high level of control. Retrieval-based methods are effective for integrating dynamic or frequently changing facts, as they allow updates without modifying the model’s parameters, making them suitable for black-box applications. For large-scale factual updates, continual learning is preferred as it enables the incremental integration of new knowledge while preserving previously learned information.

(\romannumeral2) For \textbf{domain knowledge}, both continual learning and retrieval-based methods are applicable. Continual learning excels in large-scale adaptation, using domain-specific corpora to ensure the model retains general knowledge while adapting to specialized contexts. Retrieval-based methods complement this by dynamically providing domain-specific information without requiring model modifications, making them valuable in scenarios where static updates are impractical.

(\romannumeral3) For \textbf{language knowledge}, continual learning is the only method capable of supporting large-scale language expansion. It facilitates the integration of multilingual corpora and provides the foundational updates necessary for underrepresented or low-resource languages. 
% Neither model editing nor retrieval-based methods are suitable for language expansion, as they lack the scalability and depth required for comprehensive updates.

(\romannumeral4) For \textbf{preference updates}, such as aligning models with evolving user values or ethical norms, continual alignment is typically achieved by combining continual learning techniques with preference optimization methods, such as reinforcement learning from human feedback. These approaches enable models to dynamically adapt to changing preferences while retaining alignment with previously learned values.

\textbf{Summary.} \textbf{Continual learning} is indispensable for large-scale updates like domain adaptation and language expansion, where foundational and incremental updates are required. \textbf{Model editing} excels at precise factual updates, while \textbf{retrieval-based methods} offer dynamic access to factual and domain knowledge without altering the model. A well-informed selection or combination of these methods ensures efficient and effective knowledge expansion tailored to specific use cases.

% \subsection{Model Editing Research Avenues}

% \paragraph{Computational Overhead.}
% Implicit model editing methods depend on parameter modifications and are therefore gradient-based. While they improve on traditional retraining and fine-tuning, they still have significant computational costs. For example, the knowledge attribution method by \citet{dai-etal-2022-knowledge} has high computational overhead since gradients of the model output with respect to each neuron must be calculated. Similarly, ROME and MEMIT require pre-computation to identify the relevant hidden states and involve additional overhead, for example, when calculating the key vectors \citep{jiang-etal-2024-learning, chen_robust_2024}. In contrast, Editable Training, KnowledgeEditor, and MEND require specialized training of the base model or an auxiliary model \citep{zheng-etal-2023-edit}.

% \paragraph{Side Effects.}
% Parameter modification can lead to side effects that significantly degrade the model’s performance \citep{zheng-etal-2023-edit}. \citet{hsueh-etal-2024-editing} note that even a single edit can alter the predictive distribution among objects linked to the same subject and relation. This issue becomes especially problematic if these unintended edits accumulate. \citet{gupta-etal-2024-model} observe that edits performed by ROME and MEMIT are not as localized as initially anticipated. \citet{gu-etal-2024-model} show that side effects can also involve the LLM’s general abilities, including reasoning, summarization, and question answering. They conclude that model editing methods struggle to strike a balance between improving factuality and maintaining general abilities on downstream tasks. Even minor parameter updates can drastically influence general abilities. Furthermore, they find that model editing methods alter original model weights excessively, which leads to overfitting on editing facts and progressively impacts the LLM’s performance on downstream tasks. \citet{yang-etal-2024-butterfly} also investigate the edited model’s performance decline across various downstream tasks and term it \textit{model collapse}. They observe that even a single edit performed by ROME can cause deteriorated text generation capabilities, as well as performance declines in other benchmark tasks. \citet{yang-etal-2024-fall} further examine the underlying causes of model collapse in the parameter update process of ROME.

% \paragraph{Multiple Sequential Edits.}

% Another challenge is allowing for multiple sequential edits to the same model without forgetting previously edited facts. \citet{gupta-etal-2024-model} associate model editing methods with CF. Conducting an evaluation in which they sequentially edited multiple facts using ROME and MEMIT, they found that the LLM’s editability continually decreases. It gradually forgets previously edited facts and downstream tasks, ultimately experiencing CF. \citet{huang_transformer-patcher_2023} show that hypernetwork-based model editing methods fail in sequential editing after a few steps. \citet{yang-etal-2024-butterfly}, focusing on a small number of edits, find that model collapse is a common issue with current model editing methods in the sequential edit scenario.

% \paragraph{Knowledge Propagation and Diversity.}
% \citet{zhong-etal-2023-mquake} evaluate how reasoning on multi-hop questions is affected by updating facts. They find that current model editing methods perform poorly on multi-hop questions, where the answers to later questions should change as a consequence of editing a fact. Most existing model editing works focus on the monolingual context. \citet{wei_mlake_2024} demonstrate that existing methods have problems generalizing across languages and show better editing performance for English. They also conclude that these methods struggle with cross-lingual transfer, since LLMs use distinct parameters to encode knowledge for different languages. Moreover, research on model editing primarily focuses on modifying structured, relational knowledge, typically represented as subject-relation-object-triplets. While initial works on model editing for unstructured knowledge exist \citep{deng_everything_2024}, other types of knowledge beyond structured knowledge still remain neglected.

% \subsection{Continual Pretraining Research Avenues}

% \paragraph{Computational Efficiency.}
% Since CPT involves parameter updates on a larger scale, computational efficiency is an aspect that needs to be considered. \citet{verwimp_continual_2024} highlight the need to study algorithms that restrict computational cost. According to \citet{wu2024continual}, enhancing the CPT process while minimizing the required computational resources is a key challenge. They consider new architectures, algorithms, and data structures essential for handling increasingly complex pretraining tasks and vast amounts of data while keeping computational demands down.

% \paragraph{Forgetting.}
% \citet{ke2022continual} point out that continual domain-adaptive pretraining can lead to severe CF of general knowledge, making the model unsuitable for subsequent downstream tasks. Some works attempt to address this challenge \citep{ke-etal-2022-continual, qin-etal-2022-elle, ke_continual_2023}, but their number remains quite limited. A special case of CF, highlighted by \citet{wu2024continual}, is cross-stage forgetting. CL should be applied iteratively across the LLM’s different training stages without forgetting the skills and knowledge acquired in other stages. However, iterations between different stages can lead to CF.  For example, \citet{lin-etal-2024-mitigating} further investigate the alignment tax, concluding that better alignment with human preferences leads to more pronounced forgetting of previously learned tasks. \citet{wu2024continual} call for deeper analysis of why and how iterations affect an LLM’s performance.

% \paragraph{Limited Diversity of Techniques.}
% \citet{shi_continual_2024} highlight that techniques specific to CPT are still in their early stages. Some studies, such as \citet{jang_towards_2022}, evaluate the effectiveness of existing CL methodologies. However, the number of specific CPT techniques remains limited \citep{sun_ernie_2020, ke-etal-2022-continual, qin-etal-2022-elle, ke_continual_2023}, with many being architecture-based \citep{ke-etal-2022-continual, qin-etal-2022-elle}. Furthermore, \citet{shi_continual_2024} identify a gap between existing studies and real-world production scenarios concerning the number of CPT iterations.

\section{Conclusions}
Adapting large language models to evolving knowledge is essential for maintaining their relevance and effectiveness. This survey explores three key adaptation methods --- continual learning for large-scale updates, model editing for precise modifications, and retrieval-based approaches for external knowledge access without altering model parameters. We examine how these methods support updates across factual, domain-specific, language, and user preference knowledge while addressing challenges like scalability, controllability, and efficiency. By consolidating research and presenting a structured taxonomy, this survey provides insights into current strategies and future directions, promoting the development of more adaptable and efficient large language models.

\section*{Limitations}
This survey provides a comprehensive overview of knowledge expansion techniques for LLMs. However, due to page constraints, we had to limit its scope and prioritize certain aspects: 

First, the paper only provides a high-level overview of each method rather than an in-depth analysis. 
This can limit the understanding of the nuances and specific applications of each technique, as well as implementation details. 

Second, our work is a literature review of adaptation methods rather than an empirical study evaluating their actual performance. While we analyze existing strategies, we do not benchmark or experimentally compare their effectiveness, leaving room for future studies to assess their practical impact under real-world conditions.

Third, we focus solely on text-based models and do not cover vision-language models, which integrate multi-modal learning for textual and visual understanding. While the methods covered in this survey could be used to adapt the language encoders of such models in theory, extending these adaptation methods to vision-language models remains an open research direction. 

Finally, this survey reflecting the current state of research might become outdated as new research is published, as the field of LLMs is rapidly evolving and new methods for knowledge expansion are continuously being developed.


% - No vision-language models
% - literature review on the method only, no actual performance evaluation
% - No meaningful limitation section

\bibliography{anthology,custom}

% \section*{Acknowledgments}
\newpage
\clearpage
\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Comprehensive Taxonomy of Methods}
\label{sec:appendix_taxonomy}
\FloatBarrier
\input{taxonomy_comprehensive}

\end{document}
