%\documentclass[../main.tex]{subfiles}

%\begin{document}
We begin by introducing the structure of GOS-VAE \footnote{The code is available at \url{https://github.com/JayChao0331/GOS-VAE.git}}.


\subsubsection{Objective}
Our GOS-VAE focuses on application scenarios like autonomous driving, where edge devices such as autonomous vehicles or robots host limited computational power and is connected
to back-end servers with powerful computing capacities. Particularly, we use image transmission for remote vehicle control as an exemplary application. Though demonstrated on image transmission, our framework extends to AI-driven platforms, with learning-based source embedding adaptable to diverse channel models and protocols.

\subsubsection{Overall Structure}
The overall structure of our GOS-VAE is illustrated as Fig.~\ref{fig:sqvae}.
The transmitter features a lightweight encoder to embed the original images before transmitting a compressed representation to the receiver. The receiver reconstructs images for the accomplishment of its downstream tasks. Inspired by \cite{fu2023vector}, a Vector Quantized-Variational AutoEncoder (VQ-VAE) \cite{van2017neural} structure forms the backbone of the encoder-decoder model. Instead of focusing only on the visual quality of reconstructed images, we train a codebook to characterize
semantic information for the downstream task. During training, we end-to-end optimize the entire framework through imitation learning with performance feedback from the downstream task. %Although real-world may involve multiple downstream tasks, this work would first handle a single-task, i.e., semantic segmentation as an example scenario, while leaving the multi-task generalizations in our future works. In the following parts, we will introduce each functionality of our proposed GOS-VAE. 
This work focuses on a single task—semantic segmentation—leaving multi-task generalizations for future work. In the following sections, we introduce each functionality of our proposed GOS-VAE.
%including the VQ-VAE, data transmission, involvement of downstream tasks, imitation learning, and objective functions.



\vspace*{-2mm}
\subsection{VQ-VAE Backbone}
Our GOS-VAE uses VQ-VAE as encoder and decoder, deployed at the transmitter and the receiver, respectively.
The transmitter input is an image $x \in \mathbb{R}^{H \times W \times 3}$, where $H$, $W$, and $3$ denote the height, width, and the three RGB channels, respectively. The VQ-VAE encoder $z_{e}$ compresses the image into a feature map $z_{e}(x) \in \mathbb{R}^{\frac{H}{r} \times \frac{W}{r} \times C}$, where $r$ is the compression ratio affected by the multi-layer Convolutional Neural Network (CNN). Given the feature map, we use a codebook to further reduce transmission data payload. The channel dimension is reduced by a factor of $C$. Given a codebook $\textbf{e}=\{e_i\}_{i=1}^{K} \in \mathbb{R}^{K \times D}$ of size $K$ and codeword length $D$, a quantized map $z \in \mathbb{R}^{\frac{H}{r} \times \frac{W}{r}}$ based on a nearest-neighbor lookup can be found:
\begin{equation}
q(z_{ij} = k|x) = 
\begin{cases} 
1 & \text{for } k = \arg \min_{\ell} \| z_e(x) - e_{\ell} \|_2 , \\
0 & \text{otherwise}
\end{cases}.
\end{equation}

Both the transmitter and the receiver should
pre-store the learned codebook before real-time networking.
After quantization, the transmitter sends the codeword index to the receiver which converts the
codeword $z$ back to its feature map, denoted as $z_{q}(x) \in \mathbb{R}^{\frac{H}{r} \times \frac{W}{r} \times D}$, which is calculated as
\begin{equation}
z_q(x)_{ij} = e_k, \quad \text{where} \quad k = \arg \min_{\ell} \| z_e(x) - e_{\ell} \|_2 .
\end{equation}
The receiver forwards the feature map to its CNN decoder to recover the spatial dimension of the feature map and finally reconstruct the image $\hat{x} \in \mathbb{R}^{H \times W \times 3}$.

\vspace*{-2mm}
\subsection{Post-Training Communication Showtime}
During showtime, the transmitter only sends the codeword map $z$ for each captured RGB image $x$ to the receiver under compression ratio $r$. This compressed representation reduces bandwidth usage while preserving essential information. {Furthermore, due to the skewed distribution of the learned codebook, the codeword map may be further compressed by entropy encoding.} Our proposed framework
trains the compression network VQ-VAE in an end-to-end manner in view of the downstream task to ensure that 
the codeword map retains both good perceptual quality and the semantic information vital to
downstream task performance. The details will be introduced in Section \ref{sec:obj}.




\vspace*{-2mm}
\subsection{Downstream Task}
In this work, we use semantic segmentation as downstream task, which identifies the object category of each pixel. For this task, we adopt an open-source and pre-trained OneFormer \cite{jain2023oneformer} that unifies semantic, instance, and panoptic segmentation within a single model. Given an image $x \in \mathbb{R}^{H \times W \times 3}$, the pre-trained OneFormer $F$ predicts the segmentation map as
\begin{equation}
\vspace*{-2mm}
S = \mbox{Softmax}(F(x)) \in \mathbb{R}^{H \times W \times m},
\end{equation}
where $m$ is the number of object classes. The pre-trained OneFormer $F$ is unchanged during the training phase. To leverage the information from the downstream tasks, 
reconstructed images are fed into the pre-trained networks to generate additional loss terms for deep training. Additional details are explained in the following sections.



\vspace*{-2mm}

\subsection{Imitation Learning} \label{sec:il}
Not relying on ground-truth segmentation labels, we apply imitation learning \cite{xu2023bits} to guide the learning process. The goal of GOS-VAE is to reconstruct an image semantically similar to the original. For example, if the original image’s segmentation map identifies a vehicle resembling an ambulance, GOS-VAE should reconstruct a visually similar vehicle. This ensures the preservation of critical semantic features, such as vehicle characteristics. Imitation learning could also eliminate the need for ground-truth labeling and manual intervention.

In our framework, the original image $x$ first passes through the segmentation model $F$ to generate the segmentation map $S \in \mathbb{R}^{H \times W \times m}$, which serves as the imitation target. The reconstruction from GOS-VAE $\hat{x}$ is also given to the segmentation model $F$ to define its segmentation map $\hat{S} \in \mathbb{R}^{H \times W \times m}$. The learning objective is to minimize the difference of distributions between the two segmentation maps.

\vspace*{-2mm}

\subsection{Objective Function} \label{sec:obj}

For the VQ-VAE, the original loss function is designed for pixel-wise reconstruction, i.e.,
for $N = H \times W \times 3$,
\begin{equation}
L_{v} = \frac{1}{N} \sum_{i=1}^{N} (x_i - \hat{x}_i)^2 + \|\text{sg}[z_e(x)] - e\|_2^2 + \beta \|z_e(x) - \text{sg}[e]\|_2^2 ,
\end{equation}
in which $\beta=0.25$ is a constant, and `$\text{sg}$' denotes the stop-gradient operator.
The first term captures the Mean-Squared Error (MSE) between the two images and approximates the reconstruction loss $\log p(x|z_q(x))$. Since the gradient does not flow through the encoder due to the quantization step, the second and third terms of the loss are used to optimize the codebook and the encoder, respectively.

Since the proposed GOS-VAE focuses on the efficacy of downstream tasks 
according to the reconstruction, we
replace the pixel-wise MSE with a task-incentivized loss. Specifically, given two predicted segmentation maps, where each pixel represents a probability distribution over object categories, we compute the distribution distance using Jensen-Shannon Divergence (JSD), as described in Section \ref{sec:il}.
%In our experiments, we observed that 
Since OneFormer is a pre-trained and fixed segmentation model, its data distribution aligns with the training datasets. Without further perceptual constraint on the reconstruction, GOS-VAE would struggle to perform well on the downstream task. 
%This will be further discussed in the next section. 
To address this issue, we incorporate the Learned Perceptual Image Patch Similarity (LPIPS) \cite{zhang2018unreasonable} as a perceptual regularization in the loss function, using a pre-trained VGG16 \cite{simonyan2014very}. LPIPS compares two images by measuring the differences of their feature maps from the pre-trained VGG16. This term captures perceptual similarity, focusing on high-level semantic differences instead of pixel-wise changes. Finally, our objective function for the proposed GOS-VAE is
\begin{align}
L_{s} = & \, \text{LPIPS}(x, \hat{x}) + D_{JS}(S \parallel \hat{S}) \nonumber \\
        & + \|\text{sg}[z_e(x)] - e\|_2^2 + \beta \|z_e(x) - \text{sg}[e]\|_2^2 ,
\label{eq:5}
\end{align}
where $D_{JS}(S \parallel \hat{S}) = \frac{1}{2} D_\text{KL}(S \parallel M) + \frac{1}{2} D_\text{KL}(\hat{S} \parallel M)$, $M = \frac{1}{2}(S + \hat{S})$, and $D_{KL}$ denotes 
the Kullback-Leibler Divergence.

In this work, we provide several alternative training schemes for the proposed GOS-VAE. In the basic GOS-VAE, we train the model
from scratch using the objective function $L_{s}$. 
For an upgrade model GOS-VAE$^{*}$, we first pre-train a VQ-VAE using the original objective function $L_{v}$ before fine-tuning based on the objective function $L{s}$.
We further tested new models, VQ-VAE$^{\dagger}$ and the corresponding GOS-VAE$^{\dagger}$,  by replacing the CNN layers with Residual Blocks \cite{he2016deep} and by increasing
the codebook size. 
%Similar to GOS-VAE$^{*}$, the proposed GOS-VAE$^{\dagger}$ denotes that the weights are initialized by the pre-trained VQ-VAE$^{\dagger}$ and then fine-tuned on the downstream task using the objective function $L{s}$. 
All these designs are studied, together with comprehensive discussions in Section \ref{sec:exp}.

%\end{document}