\subsection{Experimental Settings} \label{sec:exp}
\vspace*{-1mm}
\subsubsection{Dataset} We test GOS-VAE using two datasets: Cityscapes \cite{cordts2016cityscapes} and ADE20K \cite{zhou2019semantic}. Cityscapes contains $2,975$ training and $500$ validation images, featuring high-resolution urban street scenes, annotated with $35$ object categories for semantic segmentation. Among these object categories, $19$ are considered
after the pre-training setup of OneFormer \cite{jain2023oneformer}. ADE20K includes $20,100$ training and $2,000$ validation images with diverse 
indoor and outdoor scenes, covering $150$ object categories for segmentation. For training, images are resized to $256 \times 512$, with horizontal flipping augmentation.

\subsubsection{Baselines}
We compare with conventional JPEG, Autoencoder, VQ-VAE \cite{van2017neural}, VQ-GAN \cite{esser2021taming}, and diffusion-based GESCO \cite{grassucci2023generative} using semantic segmentation as the downstream task. We also evaluate alternative GOS-VAE training schemes, including a basic version with shallow CNN layers, GOS-VAE$^{*}$, and GOS-VAE$^{\dagger}$. Specifically, in GOS-VAE$^{\dagger}$, we replaced the shallow CNNs with Residual Blocks \cite{he2016deep} and increased the codebook size from $512$ to $12,000$, to test the performance upper bound of Imitation Learning.



\vspace*{-2mm}
\begin{table}[htb]
\centering
\vspace{-1mm}
\caption{Performance comparisons on the Cityscapes semantic segmentation dataset.}
\vspace{-2mm}
\begin{tabular}{lcccc}
\toprule
\textbf{Models}                          & \textbf{Bandwidth} & \textbf{\# params} & \textbf{mIoU \textuparrow} & \textbf{Accuracy \textuparrow} \\
  &  \textbf{(KB)}  &  \textbf{(M)}  &  \textbf{(\%)}  &  \textbf{(\%)}  \\
\midrule
JPEG &  11.469  &  -  &  40.134  &  84.706 \\
Autoencoder  &  12.833  &  0.13  &  12.924  &  48.418  \\
VQ-GAN (r=4)     & 6.791     &  24.00                    & 54.238 & 92.523     \\
GESCO                      & 14.526     &  674.71                    & {\underline{58.969}}               & \textbf{95.351}                   
              \\
VQ-VAE (r=4)                      
      & 7.447  &  0.70             & 53.040                            & 91.647                \\
VQ-VAE$^\dagger$ (r=4)        & 8.309  &  7.32       & 54.961                            & 92.762                \\
GOS-VAE (r=4)  &  8.321  &  0.70 &  57.342  &  93.176  \\
GOS-VAE$^{*}$ (r=4)      & 8.385  &  0.70                   & 57.612            & 93.209                      \\
GOS-VAE$^\dagger$ (r=4)                &
10.092    &  7.32   & \textbf{61.318}
      & {\underline{94.087}}         \\

\bottomrule
\\
\end{tabular}\vspace*{-4mm}
\label{tab:table1}
\end{table}

\vspace*{+2mm}
\subsubsection{Setups}
For the Cityscapes dataset, both VQ-VAE and GOS-VAE are trained for $500$ epochs. GOS-VAE$^{*}$ and GOS-VAE$^{\dagger}$ are initialized using the pre-trained VQ-VAE and VQ-VAE$^{\dagger}$, respectively, before
fine-tuning for $100$ more epochs. For both JPEG and Autoencoder, we adjust the compression ratio to obtain comparable transmission bandwidth or payload. The CNN-based Autoencoder is trained for $500$ epochs. The VQ-GAN is trained for $272$ epochs. GESCO is trained for $250,000$ steps, equivalent to approximately $125$ epochs, with a diffusion step of $100$. Note that all $35$ segmentation categories are utilized for GESCO training and testing to reproduce its performance without changing the proposed architecture. 

For the ADE20K dataset, VQ-VAE and GOS-VAE are trained for $100$ epochs, while GOS-VAE$^{*}$ and GOS-VAE$^{\dagger}$ are initialized using the pre-trained VQ-VAE and VQ-VAE$^{\dagger}$ weights and fine-tuned for an additional $40$ epochs. The CNN-based Autoencoder is trained for $100$ epochs. The VQ-GAN is trained for $32$ epochs. GESCO is trained for $300,000$ steps, equivalent to approximately $15$ epochs, with a diffusion step of $100$. Each of the proposed GOS-VAE alternatives is trained and tested using a single RTX 4090 GPU, while GESCO is trained on an A100 GPU.



\begin{table}[t]
\centering
\caption{Performance comparisons on the ADE20K semantic segmentation dataset.}
\vspace{-2mm}
\begin{tabular}{lccc}
\toprule
\textbf{Models}   &  \textbf{\# params (M)}                        & \textbf{mIoU (\%) \textuparrow} & \textbf{Accuracy (\%) \textuparrow} \\
\midrule
JPEG  &  -  &  32.111  &  77.474  \\
Autoencoder  &  0.13  &  22.389  &  69.607  \\
VQ-GAN (r=4)   &  24.00    &    38.738      &  80.674                      \\
GESCO   &  681.33                                 & 16.850                   & 61.170                      \\
VQ-VAE (r=4)   &  0.70                                & 38.366                   & 80.333                      \\
VQ-VAE$^{\dagger}$ (r=4)  &  7.32  &  38.612  &  80.887  \\
GOS-VAE (r=4)  &  0.70  &  39.075  &  80.974  \\
GOS-VAE$^{*}$ (r=4)  &  0.70  &              \underline{39.128}    & \underline{81.019}                  \\
GOS-VAE $^{\dagger}$ (r=4)  &  7.32  &  \textbf{40.765}  &  \textbf{81.974}  \\
\bottomrule
\end{tabular}
\label{tab:table2} \vspace*{-4mm}
\end{table}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth,height=0.2\textwidth
]{images/qualitative.png}
\vspace*{-4mm}
\caption{Visualization Results of Different Methods on Cityscapes Dataset.}
\label{fig:visualization}
\vspace*{-4mm}
\end{figure*}

\vspace{-1mm}
\subsection{Overall Reconstruction Quality}
\vspace{-1mm}
We first compare the overall quality of image reconstruction of different GO-COM frameworks. 
%on the Cityscapes and ADE20K datasets. 
Particularly, we use the performance of downstream image segmentation as evaluation metrics, in
terms of mean Intersection over Union (mIoU) and pixel-wise accuracy. We measure the bandwidth (payload) of the compressed data required by the receiver for image reconstruction. 
We compare the three different versions of our GOS-VAE against the Autoencoder, VQ-VAE, VQ-GAN, diffusion-based GESCO, together
with the classic JPEG compression.

The experimental results 
%of downstream tasks, together with their bandwidth usage on the Cityscapes dataset 
are presented in Table \ref{tab:table1}. Although performance can be easily improved by using more bandwidth at a compression ratio of $r=2$, we focus on $r=4$ here to prioritize bandwidth efficiency. 
More specifically, we set all the methods with a similar payload for better illustration.
From Table \ref{tab:table1}, we first notice that VQ-VAE surpasses JPEG and Autoencoder in both improving performance and bandwidth reduction,
owing to the use of codebook quantization. Moreover, we also notice that VQ-GAN achieves similar performance to VQ-VAE with more realistic image generation due to the discriminator. However, this addition also results in a significantly larger model size and longer training time. Next, by comparing GOS-VAE to VQ-VAE and VQ-GAN, we showcase the benefit of end-to-end learning in the compression network alongside the downstream task, as it preserves the semantic information critical to downstream task performance. Compared to the diffusion-based GESCO, we further observe that GOS-VAE achieves comparable performance while consuming significantly less bandwidth at a compression ratio $r=4$. Note that GESCO requires the ground-truth semantic segmentation map and edge map 
to generate the corresponding image during both training and testing phases. Moreover, GESCO utilizes semantic segmentation maps with $35$ object categories, utilizing more detailed information for image generation. In contrast, our method only uses $19$ object categories, following the pre-trained OneFormer setup \cite{jain2023oneformer}. 





\begin{table}[htb]
\centering
\vspace*{-3mm}
\caption{Quantitative Comparisons of Visual Quality in Reconstructed Images using the Cityscapes dataset.}
\small
\begin{tabular}{l@{\hskip 4pt}c@{\hskip 4pt}c@{\hskip 4pt}c@{\hskip 8pt}c@{\hskip 4pt}c@{\hskip 4pt}c}
\toprule
 & \multicolumn{3}{c}{\textbf{Cityscapes}} & \multicolumn{3}{c}{\textbf{ADE20K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\textbf{Models} & \textbf{MSE $\downarrow$} & \textbf{FID $\downarrow$} & \textbf{LPIPS $\downarrow$} & \textbf{MSE $\downarrow$} & \textbf{FID $\downarrow$} & \textbf{LPIPS $\downarrow$} \\
\midrule
VQ-GAN & 0.006 & \underline{20.212} & \underline{0.130} & 0.012 & \underline{12.383} & 0.177 \\
GESCO & 0.355 & 71.893 & 0.544 & 0.549 & 92.317 & 0.725 \\
VQ-VAE$^\dagger$ & \textbf{0.002} & 25.273 & 0.172 & \textbf{0.004} & 15.466 & \underline{0.170} \\
GOS-VAE$^\dagger$ & \underline{0.005} & \textbf{17.517} & \textbf{0.066} & \underline{0.008} & \textbf{8.719} & \textbf{0.075} \\
\bottomrule
\end{tabular}
\vspace*{-1mm}
\label{tab:table3}
\end{table}






Next by comparing GOS-VAE$^{\dagger}$ to GOS-VAE$^{*}$, we see a notable improvement, particularly in mIoU. Additionally, GOS-VAE$^{\dagger}$ outperforms GESCO in mIoU and achieves comparable pixel accuracy. This experiment highlights that even a slight increase in computation could lead to substantial gains in downstream task results without extra bandwidth. Notably, when comparing the number of trainable parameters, the proposed GOS-VAE$^{\dagger}$ is about $92$ times smaller than GESCO.


To evaluate the generalizability of our GOS-VAE, we present the results of semantic segmentation on the ADE20K dataset, whose results are presented in Table \ref{tab:table2}. To ensure fair comparisons, we adjust the network structure to let each method maintain a similar bandwidth or payload. Our test results show that the proposed GOS-VAE$^{\dagger}$ substantially outperforms the existing schemes in 
each of the evaluation metrics, 
a result consistent with that from
test using Cityscapes.
These results further validate the effectiveness of the proposed GOS-VAEs when processing a larger and more complex dataset. Furthermore, the number of trainable parameters in this case is about $93$ times less than the diffusion-based GESCO.


Finally, in Semantic Communication, visual quality of reconstructed images is also essential; thus, we compare our proposed GOS-VAE$^{\dagger}$ with three representative methods. As shown in Table \ref{tab:table3}, GOS-VAE$^{\dagger}$ achieves superior visual quality and closer alignment to the original image distribution, demonstrated by lower FID and LPIPS scores, rather than prioritizing pixel-wise accuracy as measured by MSE.



%VQ-VAE$^{\dagger}$ is the VQ-VAE part of GOS-VAE$^{\dagger}$, meaning that they have the same model architecture while VQ-VAE$^{\dagger}$ is trained with equation \ref{eq:4}.

\vspace{-1mm}
\subsection{Visualization Results}
\vspace{-1mm}
In addition to numerical metrics, Fig.~\ref{fig:visualization} presents visualization results for different approaches in image segmentation. From the results, we first notice that under limited transmission bandwidth, JPEG struggles to maintain good image quality and downstream task performance of the reconstructed images. Next, by comparing the proposed GOS-VAE$^{\dagger}$ to all other methods, we underscore the benefit and power of goal-oriented semantic communication framework over communication systems designed for bit-wise recovery. More specifically, the image reconstructed by GOS-VAE$^{\dagger}$ exhibits a high degree of visual consistency with the original one, even though it has a superficially higher pixel-wise Mean Squared Error (MSE) when compared against VQ-VAE. Moreover, in terms of the downstream segmentation task, GOS-VAE$^{\dagger}$ successfully detects small while important objects, such as traffic signs and traffic lights, in its predicted segmentation map. However, VQ-GAN, VQ-VAE, and GESCO are not able to fully detect these important objects. Furthermore, the boundaries of many objects in the segmentation maps are not accurate. These results further demonstrate that GOS-VAE$^{\dagger}$ is able to preserve the ``semantic information" defined by the downstream task.




\subsection{Ablation Study}
\vspace{-1mm}
\subsubsection{Performance with Different Objective Functions}
To illustrate the reasonability of our design of objective function for training our GOS-VAE as Eq. \eqref{eq:5}, we conduct an ablation study on different designs of loss function for GOS-VAE.
%as Table \ref{tab:table3}.
% Models in this table are trained from scratch for $500$ epochs. Here, we will use some examples to explain the meaning of different models. GOS-VAE (CE) means that the downstream task is considered during training, while the distribution comparison method is Cross-Entropy. Thus, the objective function is:

In the first setup, we use cross-entropy (CE) to measure the distribution similarity, denoted by GOS-VAE (CE), the loss can be characterized by

\vspace{-4mm}
\begin{equation}
L_{sc} = CE(S || \hat{S}) + \|\text{sg}[z_e(x)] - e\|_2^2 + \beta \|z_e(x) - \text{sg}[e]\|_2^2.
\end{equation}
\vspace{-4mm}

We also apply the Kullback–Leibler divergence (KLD) for distribution comparison, which can be applied to both original VQ-VAE and GOS-VAE. For example, the VQ-VAE (KLD) can be trained based upon 
\begin{align}
L_{vk} = & \, \frac{1}{N} \sum_{i=1}^{N} (x_i - \hat{x}_i)^2 + D_{KL}(S \parallel \hat{S}) \nonumber \\
        & + \|\text{sg}[z_e(x)] - e\|_2^2 + \beta \|z_e(x) - \text{sg}[e]\|_2^2,
\end{align}
while GOS-VAE (KLD + LPIPS) has a similar objective function as Eq. \eqref{eq:5} with the JSD replaced by the KLD.

For VQ-VAE (LPIPS), the pixel-wise Mean-Squared Error (MSE) is replaced with the LPIPS, while the downstream task loss is not included, calculated by
\begin{equation}
L_{vp} = \text{LPIPS}(x, \hat{x}) + \|\text{sg}[z_e(x)] - e\|_2^2 + \beta \|z_e(x) - \text{sg}[e]\|_2^2.
\end{equation}

% \nonumber

\vspace{-2mm}
\begin{table}[htb]
\centering
\vspace{-2mm}
\caption{Ablation study on designing objective function using Cityscapes dataset.}
\vspace{-2mm}
\begin{tabular}{lcc}
\toprule
\textbf{Models}                          & \textbf{mIoU (\%) \textuparrow} & \textbf{Accuracy (\%) \textuparrow} \\
\midrule
GOS-VAE (CE)                           & 18.785                   & 81.715                      \\
GOS-VAE (KLD)                          & 42.792                   & 89.028                      \\
VQ-VAE (KLD)                    & 46.789                   & 89.759                      \\
VQ-VAE (LPIPS)                           & 54.933                   & 92.479                      \\
GOS-VAE (KLD + LPIPS)                     & {\underline{56.949}}          & {\underline{93.166}}             \\
GOS-VAE (JSD + LPIPS)                     & \textbf{57.342}          & \textbf{93.176}             \\
\bottomrule
\end{tabular}
\label{tab:table4}
\vspace*{-1mm}
\end{table}

As shown in Table \ref{tab:table4}, GOS-VAE (KLD+LPIPS) and GOS-VAE (JSD+LPIPS) consistently achieve the best performance, indicating the effectiveness of our proposed method on the downstream task. This can be attributed to the fact that the segmentation model, OneFormer, is pre-trained on the corresponding dataset, resulting in a data distribution that reflects images from the dataset. By utilizing LPIPS, data distribution of the reconstructed images aligns more closely with that of the OneFormer model, leading to improved performance. Compared to the CE-based method, KLD methods achieve better performance since KLD utilizes a distribution as the learning objective for each sample, providing more information than the one-hot label in Cross-Entropy.

\begin{figure}[htb]
\centering
\includegraphics[width=0.48\textwidth]{images/correlation_ade20k.png}
\vspace*{-2mm}
\caption{Training curves of semantic segmentation loss (JSD) and LPIPS for GOS-VAE on the ADE20K dataset. The correlation of the two curves is 0.976.}
\label{fig:correlation_ade20k}
\vspace*{-1mm}
\end{figure}

We further analyze the relationship between semantic segmentation performance and LPIPS to further validate our conclusion. As shown in Fig. \ref{fig:correlation_ade20k}, the training curves for the two loss terms follow very similar trends with a correlation of $0.976$. This indicates that managing the data distribution shift is crucial for achieving optimal performance when using a pre-trained downstream task model to train an efficient compression network for GO-COM.

\begin{figure}[htb]
\centering
\vspace{-1mm}
\includegraphics[width=0.48\textwidth
]{images/compression_ratio.png}
\caption{Performance comparisons of models on the Cityscapes dataset under different compression ratios (r).}
\label{fig:compression_ratio}\vspace*{-2mm}
\end{figure}


\subsubsection{Performance for Different Compression Ratios}
%In the real-world application scenario, we aim to minimize transmission bandwidth while maintaining acceptable performance in the downstream task. 
The tradeoff between compression ratio and downstream task performance is of critical concern for efficient data transmission. In this test, we compare the performance of semantic segmentation of reconstructed images from different methods at various compression ratios (r):  $2$, $4$, $8$, $16$, and $32$. 
From the results presented in 
Fig.~\ref{fig:compression_ratio}, we observe that although GOS-VAE can achieve performance comparable to GOS-VAE$^{*}$, its stability 
suffers at higher compression ratios. Moreover, GOS-VAE may fail to converge in some cases, with the worst-case mIoU dropping to as low as $5.462$. The stability at higher compression ratios is the motivation for initializing GOS-VAE$^{*}$ with pre-trained VQ-VAE. On the other hand, GOS-VAE$^{*}$ consistently outperforms VQ-VAE across all compression ratios, establishing its
superior stability and robustness, particularly in terms of pixel accuracy.

% \subsection{Complexity}
%Each of the proposed methods is trained and tested within one day using a single RTX 4090 GPU. In contrast, GESCO is trained and tested on an A100 GPU, while still requiring around two to three days for training and image generation. 
% \textcolor{red}{It will be better if you could provide a brief complexity analysis instead a rough number of days in this Section. You can use Table I to further explain.}






\begin{comment}
    
\begin{figure}[htb]
\centering
\vspace{-5mm}
\includegraphics[width=0.5\textwidth]{images/correlation_cityscapes.png}
\caption{Training curves of semantic segmentation loss (JSD) and LPIPS for GOS-VAE on the Cityscapes dataset. The correlation of the two curves is 0.986.}
\label{fig:correlation_cityscapes}\vspace*{-3mm}
\end{figure}


\begin{figure}[htb]
\centering
\vspace{-5mm}
\includegraphics[width=0.5\textwidth]{images/correlation_ade20k.png}
\caption{Training curves of semantic segmentation loss (JSD) and LPIPS for GOS-VAE on the ADE20K dataset. The correlation of the two curves is 0.976.}
\label{fig:correlation_ade20k}\vspace*{-2mm}
\end{figure}
\end{comment}





% \begin{figure}[t]
% \centering
% %\vspace*{-4mm}
% \includegraphics[width=0.45\textwidth]{images/rate_distortion_ablation.png}
% \caption{Performances of GOS-VAE and GOS-VAE$^{*}$ in the Cityscapes dataset under different compression ratios (r). \textcolor{red}{legend not updated}}
% \label{fig:rate_distortion_ablation}\vspace*{-2mm}
% \end{figure}


% \begin{figure}[tb]
% \centering
% %\vspace*{-4mm}
% \includegraphics[width=0.45\textwidth]{images/rate_distortion_comparison.png}
% \caption{Performances of VQ-VAE and the proposed GOS-VAE$^{*}$ in the Cityscapes dataset under different compression ratios (r). \textcolor{red}{Fig.5 can be merged together with Fig. 4}}
% \label{fig:rate_distortion_comparison}\vspace*{-2mm}
% \end{figure}




























%\end{document}