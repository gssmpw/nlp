\section{Related Work}
\subsection{Image Compression}


Image compression is a vital field in digital image processing, aimed at improving image storage and transmission efficiency.
Classical lossy image compression standards, such as JPEG~\cite{wallace1991jpeg}, BPG~\cite{bellard2018bpg}, and VVC~\cite{bross2020versatile}, rely on handcrafted rules and have been widely adopted.
Recently, learned image compression has made significant progress and achieved promising performance~\cite{balle2017end, song2021variable, rhee2022lc, lee2022dpict, cui2021asymmetric, ma2022end, ali2023towards, fu2023learned}.
Ball\'e~\etal~\cite{balle2017end} propose a pioneering end-to-end optimized image compression model, which significantly improves compression performance by leveraging CNNs.
Cheng~\etal~\cite{cheng2020learned} incorporate attention mechanisms into their compression network, thus enhancing the encoding of complex regions.
Xie~\etal~\cite{xie2021enhanced} utilize invertible neural networks (INNs) to mitigate the issue of information loss and achieve better compression.
Yang~\etal~\cite{yang2024lossy} propose a novel transform-coding-based lossy compression scheme using diffusion models.
Zhu~\etal~\cite{zhu2022transformer} and Zou~\etal~\cite{zou2022devil} propose transformer based image compression networks and obtain superior compression effectiveness compared to CNNs.
Liu~\etal~\cite{liu2023learned} integrate transformers and CNNs to harness both non-local and local modeling capabilities, enhancing the overall performance of image compression. Concurrent with our work, Qin~\etal~\cite{qin2024mambavc} investigate a pure SSM network for image compression.


In addition, several studies have been proposed to explore various entropy models to improve image compression.
Inspired by side information in image codecs, hyperprior is introduced to capture spatial dependencies in latent representations~\cite{balle2018variational}.
Driven by autoregression of probabilistic generative models, Minnen~\etal~\cite{minnen2018joint} predict latent representations from a causal context model along with a hyperprior.
Due to the time-consuming process of spatial scanning in autoregressive models, Minnen~\etal~\cite{minnen2020channel} propose a channel-wise autoregressive model as an alternative while He~\etal~\cite{he2021checkerboard} develop a checkerboard context model for parallel computing. 
Following these works, various adaptations of these methods have also been developed~\cite{he2022elic, jiang2023mlic, koyuncu2024efficient}. 
However, it remains a challenge to jointly model spatial and channel dependencies in an efficient manner.


\subsection{State Space Models}


State Space Models (SSMs) have shown their effectiveness in capturing the dynamics and dependencies~\cite{gu2020hippo, gu2021combining, goel2022s}.
To reduce excessive computational and memory requirements in SSMs, Gu~\etal~\cite{gu2021efficiently} constrain their parameters into a diagonal structure.
Subsequently, structured state space models have been proposed, such as complex-diagonal structures~\cite{gu2022parameterization, gupta2022diagonal}, multiple-input multiple-output configurations~\cite{smith2022simplified}, combinations of diagonal and low-rank operations~\cite{hasani2022liquid}, and gated activation functions~\cite{mehta2023long}.
Among them, Mamba introduces selective scanning and a hardware speed-up algorithm to facilitate efficient training and inference~\cite{gu2023mamba}.
Vim~\cite{zhu2024vision} is the first SSM-based model, as a general vision backbone, to address the limitations of Mamba in modeling image sequences.
VMamba~\cite{liu2024vmamba} introduces a cross-scan module to traverse the spatial domain and transform any non-causal visual image into ordered patch sequences.
Huang~\etal~\cite{huang2024localmamba} propose a novel local scanning strategy that divides images into distinct windows to capture local and global dependencies.
Mamba has been explored for its potential in various vision tasks, including image restoration~\cite{guo2024mambair, cheng2024activating, deng2024cu, shi2024vmambair}, point cloud processing~\cite{li20243dmambacomplete, liang2024pointmamba, liu2024point, zhang2024point}, video modeling~\cite{chen2024video, li2024videomamba, zou2024rhythmmamba}, and medical image analysis~\cite{ma2024u, yue2024medmamba, ma2024semi}, but how to effectively apply Mamba in image compression remains unexplored. 