\section{Introduction}


Image compression is a vital technology in multimedia applications, allowing for efficient storage and transmission of digital images. 
With the rise of social media, a large number of images are created by users and transmitted over the internet every second. 
Advanced compression methods are constantly sought to achieve superior rate-distortion performance while maintaining efficiency.
Classical lossy image compression standards, such as JPEG~\cite{wallace1991jpeg}, BPG~\cite{bellard2018bpg}, and VVC~\cite{bross2020versatile}, achieve commendable rate-distortion performance via handcrafted rules. 
With the advances in deep learning, Learned Image Compression (LIC) methods~\cite{balle2017end, song2021variable, cui2021asymmetric, ma2022end, ali2023towards, theis2022lossy, mentzer2018conditional, li2020efficient, son2021enhanced, dardouri2021dynamic} make promising progress and present better rate-distortion performance by exploiting various Convolutional Neural Networks (CNNs) and transformer architectures.


In general, LIC follows a three-stage paradigm: \textbf{nonlinear transformation}, \textbf{quantization}, and \textbf{entropy coding}.
The nonlinear transformation consists of an analysis transform and a synthesis transform.
The analysis transform maps an image from the pixel space to a compact latent space. 
The synthesis transform is an approximate inverse function that maps latent representations back to pixels.
Quantization rounds latent representations to discrete values, and entropy coding encodes them into bitstreams.
In particular, LIC faces two critical challenges: 
(1) how to design an effective yet efficient nonlinear transformation that yields a compact latent representation in the analysis transform and recovers a high-fidelity image in the synthesis transform, and
(2) how to achieve efficient entropy coding for highly compressed bitstreams.


Many studies have sought to address the aforementioned challenges~\cite{zhou2019end, liu2023learned, he2021checkerboard, minnen2020channel}.
As for the first challenge, CNNs based models often struggle to capture global content, causing redundancy in latent representations~\cite{zhou2019end, cheng2020learned}. 
To address this problem, several works leverage transformers for image compression due to their powerful long-range modeling capabilities~\cite{kenton2019bert, dosovitskiy2020image, liu2021swin, zhu2022transformer, zou2022devil, li2024frequency,chen2021end,liu2023learned}. 
However, the quadratic complexity of self-attention incurs high computational cost, thus restricting efficient compression.
As for the second challenge, autoregressive models and transformers are two popular options in exploiting spatial or channel correlations~\cite{minnen2018joint, qian2021entroformer, he2021checkerboard, minnen2020channel, liu2023learned, li2024frequency, jiang2023mlic, koyuncu2022contextformer}.
Since the spatial dimension is often quite large, modeling the spatial dependency in an autoregressive manner will lead to high latency~\cite{minnen2018joint, qian2021entroformer}.
Moreover, existing channel-wise autoregressive models can only remove inter-channel redundancy~\cite{minnen2020channel, zou2022devil}. 
Thus, the spatial redundancy still exists in their latent representations.
Transformer-based entropy models capture intricate spatial or channel correlations, but their reliance on self-attention mechanisms introduces high latency and computational overhead~\cite{liu2023learned, li2024frequency, jiang2023mlic, koyuncu2022contextformer}.


\IEEEpubidadjcol % Adjusts the second column


\begin{figure*}[t]
  \centering
   \includegraphics[width=.86\linewidth]{figs/fig1.pdf}
   \vspace{-1em}
   \caption{
   The Fourier spectrum comparisons between SSMs and CNNs.
   (a) The Fourier spectrum of features obtained from the SSM-based method\protect\hyperlink{foot1}{\textsuperscript{1}} and the CNN-based method (ChARM)~\cite{minnen2020channel} in the last block of the analysis transform $g_a(\cdot)$.
   (b) Relative log amplitudes of Fourier transformed feature maps\protect\hyperlink{foot2}{\textsuperscript{2}}~for different methods. 
   $\Delta$ log amplitude values indicate the averaged output of each block in $g_a(\cdot)$ on the Kodak dataset.   
   }
   \vspace{-1em}
   \label{fig1}
\end{figure*}


State Space Models (SSMs) have recently demonstrated superior performance on various vision and language tasks~\cite{gu2023mamba, zhu2024vision, liu2024vmamba}.
Inspired by the advancements in SSMs, we propose a hybrid CNNs and SSMs based image compression framework, dubbed \textbf{\textit{CMamba}}, to achieve better rate-distortion performance and computational efficiency.
Our CMamba consists of two components: (1) a Content-Adaptive SSM (CA-SSM) module and (2) a Context-Aware Entropy (CAE) module.


Due to the linear computational complexity of SSMs, we intend to employ them to model global content while preserving global receptive fields~\cite{liu2024vmamba}.
However, we observed that SSMs excel in modeling overall content but tend to lose high-frequency details. 
This issue gets worse as network depths increase, as shown in Fig.~\ref{fig1}(b).
Hence, solely relying on SSMs would lead to inferior compression performance.
To tackle this issue, our CA-SSM module incorporates SSMs and CNNs to capture both global content and local details as CNNs can effectively capture fine-grained local details~\cite{park2021vision,zou2022devil,liu2023learned}.
As shown in Fig.~\ref{fig1}(a), the feature extracted by CNNs contains more high-frequency details compared to that captured by SSMs.
Thus, we integrate a simple yet effective CNN, as a complementary component to SSMs, in our CA-SSM module. 


In the CA-SSM module, we employ a dynamic fusion block that can adaptively fuse SSM features (\emph{i.e.}, global content features) and CNN features (\emph{i.e.}, local features). 
The dynamic fusion block learns to determine whether sufficient image details or global content are encoded or decoded and then produces fusion weights for SSM and CNN features, respectively. 
In this fashion, the global content and local detail features are fully exploited in encoding and decoding. 


Our CAE module is designed to jointly model spatial and channel dependencies, and thus enables precise and efficient entropy modeling of latent representations in bitstream compression.
To be specific, in the spatial dimension, our CAE module leverages SSMs to parameterize the distribution of spatial content via a learnable Gaussian model, as SSMs are good at capturing global content while performing in linear complexity. 
Along the channel dimension, the inter-channel relationships in latent representations are captured via an autoregressive manner.
Considering the nature of bitstream transmission, we process each channel sequentially and use the hidden states of previously processed channels as condition to further reduce inter-channel dependency. 
In this way, channel-wise prior knowledge can be exploited to reduce inter-channel redundancy, leading to lower bitrates in entropy coding.



\hypertarget{foot1}{\footnotetext[1]{
The convolutional layers in the main path~\cite{minnen2020channel} are replaced with visual state space blocks~\cite{liu2024vmamba}.
The models are optimized with Mean Squared Error (MSE), and $\lambda$ is set to 0.05.
}}


\hypertarget{foot2}{\footnotetext[2]{
The $\Delta$ log amplitude is defined as the difference between the log amplitude at a normalized frequency of 0.0$\pi$ (center) and 1.0$\pi$ (boundary). 
For better visualization, only the half-diagonal components of two-dimensional Fourier-transformed feature maps are shown.
}}



To demonstrate the effectiveness of CMamba, we conduct extensive experiments on widely-used image compression benchmarks, \ie, Kodak~\cite{franzen1999kodak}, Tecnick~\cite{asuni2014testimages}, and CLIC~\cite{theis2020clic}.
CMamba achieves superior rate-distortion performance, and outperforms Versatile Video Coding (VVC)~\cite{bross2020versatile} by 14.95\%, 18.83\%, and 13.89\% on these three benchmarks, respectively. 
In particular, compared to the state-of-the-art LIC method~\cite{jiang2023mlicpp}, CMamba reduces parameters by 51.8\%, FLOPs by 28.1\%, and decoding time by 71.4\% on the Kodak dataset. 
The main contributions can be summarized as follows:
\begin{itemize}
    \item 
    We propose a hybrid Convolution and State Space Models based image compression framework, termed CMamba, and achieve better rate-distortion performance with low computational complexity.

    \item 
     We propose a Content-Adaptive SSM (CA-SSM) module that dynamically fuses global content from SSMs and local details from CNNs in encoding and decoding stages.

    \item 
    We design a Context-Aware Entropy (CAE) module that explicitly models spatial and channel dependencies, enabling precise and efficient entropy modeling of latent representations for bitstream compression.

\end{itemize}

