\section{Preliminaries}

\begin{figure*}[t]
  \centering
   \includegraphics[width=.8\linewidth]{figs/fig2.pdf}
   \caption{
   (a) 
   Overview of our proposed method. 
   (b) 
   Detailed design of our proposed Content-Adaptive SSM (CA-SSM) module.
   The CA-SSM module has two parallel paths (\ie, VSS block and ResBlock) to capture global content and local details, and then fuses these features dynamically.
   (c) 
   The detailed network architecture of our Context-Aware Entropy (CAE) module.
   The CAE module jointly models spatial and channel dependencies in latent representations $y$.
    }
    \vspace{-1em}
   \label{fig2}
\end{figure*}


\noindent \textbf{Learned Image Compression (LIC).~} 
Here, we provide a brief overview of LIC.
In general, LIC follows a three-stage paradigm: nonlinear transformation, quantization, and entropy coding.
The nonlinear transformation consists of an analysis transform and a synthesis transform.
The analysis transform $g_a(\cdot)$ maps an image $x$ into a latent representation $y$.
Then, quantization $Q(\cdot)$ converts the latent representation $y$ to its discrete form.
Since the quantization process introduces clipping errors in the latent representation $r = y - Q(y)$, it would lead to distortion in the reconstructed image. 
As suggested in~\cite{minnen2020channel}, the quantization error $r$ can be estimated via a latent residual prediction network.
Finally, the rectified latent representation $\bar{y}=\hat{y} + r$ is transformed back to a reconstructed image $\hat{x}$ using the synthesis transform $g_s(\cdot)$.
The process is summarized as follows:
\begin{equation}
y = g_a(x; \phi),\
\hat{y} = Q(y),\
\hat{x} = g_s(\hat{y}+r; \theta), 
\label{eq1}
\end{equation}
where $\phi$ and $\theta$ represent the optimized parameters for the analysis and synthesis transforms, respectively.


The latent representation $y$ is assumed to follow a Gaussian distribution, characterized by parameters $\Phi$, \ie, mean $\mu$ and standard deviation $\sigma$ (aka, scale).
In the channel-wise autoregressive entropy model, side information $z$ is introduced as an additional prior to estimate the probability distribution of the latent representation $y$~\cite{minnen2020channel}.
To be specific, a hyper-encoder $h_a(\cdot)$ takes the latent representation $y$ as input to generate the side information.
Then, $z$ will also be quantized as $\hat{z}$ via $Q(\cdot)$. Next, a hyper-prior decoder $h_s(\cdot)$ is applied to the quantized side information $\hat{z}$ to derive a hyper-prior $\Phi^{'}$.
This process is formulated as follows:
\begin{equation}
z = h_a(y; \phi_h),\
\hat{z} = Q(z),\
\Phi^{'} = h_s(\hat{z}; \theta_h).
\label{eq2}
\end{equation}
Subsequently, the latent representation $y$ is split into $S$ groups along the channel dimension, denoted as $\{y_1, ... , y_S\}$.
The hyper-prior $\Phi^{'}$ and decoded groups $\hat{y}_{s<i}$ are used to estimate parameters $\Phi_i$ of Gaussian distributions for the current group $\hat{y}_i$.
As a result, the Gaussian probability $p(\hat{y}_i|\Phi^{'}, \hat{y}_{s<i})$ is modeled in an autoregressive manner.


To train the overall learned image compression model, we adopt rate-distortion as the optimization objective, defined as:
\begin{align}
\mathcal{L} &= R(\hat{y}) + R(\hat{z}) + \lambda \cdot D(x, \hat{x}) \notag \\
            &= \mathbb{E} \left[ -\log_2 \left( p(\hat{y} | \hat{z}) \right) \right] + \notag \\
            &\quad \mathbb{E} \left[ -\log_2 \left( p(\hat{z}) \right) \right] + \lambda \cdot \mathbb{E}\left[ d(x, \hat{x}) \right], 
\label{eq3}
\end{align}
where $\lambda$ controls the trade-off between rate and distortion.
$R$ represents the bit rate of $\hat{y}$ and $\hat{z}$, and $d(x, \hat{x})$ is the distortion between the input image $x$ and reconstructed image $\hat{x}$.


\noindent \textbf{State Space Models (SSMs).~}
Continuous-time SSMs can be regarded as a Linear Time-Invariant (LTI) system that transforms a sequential input $x(t)\in \mathbb{R}$ to an output $y(t)\in \mathbb{R}$ via a hidden state $h(t)\in \mathbb{R}^N$. 
It is formulated as follows:
\begin{equation}
\begin{aligned}
h'(t) &= A h(t) + B x(t), \\
y(t) &= C h(t) + D x(t), 
\label{eq4}
\end{aligned}
\end{equation}
where $h'(t)$ denotes the first derivative of the hidden state $h(t)$ with respect to time $t$. 
$A\in \mathbb{R}^{N \times N}$, $B\in \mathbb{R}^{N \times 1}$, and $C\in \mathbb{R}^{1 \times N}$ are coefficient matrices for the LTI system. 
$D\in \mathbb{R}$ is a feedthrough parameter~\cite{hespanha2018linear}.


To be integrated into deep models, continuous-time SSMs need to be discretized. 
This process uses a times-cale parameter $\Delta$ for transforming the $A$ and $B$ into their discretized forms.
Consequently, Eqn.~\eqref{eq4} can be discretized via the zero-order hold (ZOH) as follows:
\begin{equation}
\begin{aligned}
h_k &= e^{\Delta A} h_{k-1} + (\Delta A)^{-1} (e^{\Delta A} - I) \cdot \Delta B x_k, \\
y_k &= C h_k + D x_k.
\end{aligned}
\label{eq5}
\end{equation}


\section{Methodology}
Our proposed hybrid Convolution and State Space Models (SSMs) based image compression framework is illustrated in Fig.~\ref{fig2}.
Specifically, we design two components, \ie, a Content-Adaptive SSM (CA-SSM) module (marked by the green blocks) and a Context-Aware Entropy (CAE) module (marked by the yellow block).
Our CA-SSM module (Sec.~\ref{sec CA-SSM}) is designed to dynamically fuse global content and local details extracted by SSMs and CNNs, respectively.
Then, our CAE module (Sec.~\ref{sec CAE}) is presented to model spatial and channel dependencies jointly.
These dependencies facilitate effective yet efficient entropy modeling of latent representations for bitstream compression.


\subsection{Content-Adaptive SSM Module}
\label{sec CA-SSM}

SSMs have demonstrated superior performance on various vision and language tasks~\cite{gu2023mamba, zhu2024vision, liu2024vmamba, guo2024mambair}, and they offer a global receptive field with linear complexity. 
Intuitively, SSMs could be a better candidate backbone for image compression as they have the potential to balance compression effectiveness and efficiency.
Hence, the Content-Adaptive SSM (CA-SSM) module is designed to fully exploit the linear computational complexity of State Space Models (SSMs) and their global content modeling capability for image compression.


Our CA-SSM incorporates a Visual State Space (VSS) block to capture global content.
The VSS block adopts a 2D-Selective-Scan (SS2D) layer to traverse the spatial domain and convert any non-causal visual image into ordered patch sequences~\cite{liu2024vmamba}.
This scanning strategy facilitates SSMs in handling visual data without compromising the field of reception.
The SS2D layer within the VSS block unfolds feature patches along four directions, producing four distinct sequences. 
Then, these sequences are processed via SSMs, and the output features from different directions are merged to reconstruct a complete feature map.
Given an input feature $\mathcal{F}_\textit{IN}$, the output feature $\mathcal{F}_\textit{OUT}$ of the VSS can be expressed as:
\vspace{-0.5em}
\begin{equation}
\vspace{-0.5em}
\begin{aligned}
\mathcal{F}_\textit{SS2D} &= \textit{LN}(f_\textit{ss2d}(\sigma (w_1(\textit{LN}(\mathcal{F}_\textit{IN}))))), \\
\mathcal{A} &= \sigma (w_2 \textit{LN}(\mathcal{F}_\textit{IN})), \\
\mathcal{F}_{1} &= w_3( \mathcal{F}_\textit{SS2D} \odot \mathcal{A}) + \mathcal{F}_\textit{IN}, \\
\mathcal{F}_\textit{OUT} &= w_4( \textit{LN} (\mathcal{F}_{1}) ) + \mathcal{F}_{1},
\end{aligned}
\label{eq6}
\end{equation}
where $w_1$, $w_2$, $w_3$, and $w_4$ are learned parameters, 
$\textit{LN}(\cdot)$ denotes layer normalization,
$\sigma(\cdot)$ represents the \textit{SiLU} activation function~\cite{ramachandran2017searching}, 
and $\odot$ denotes the element-wise product. 
The function $f_\textit{ss2d}(\cdot)$ refers to an SS2D operation, defined as:
\vspace{-0.5em}
\begin{equation}
\vspace{-0.5em}
\begin{aligned}
x_{v} &= f_\textit{exp}(x_{in}, v), \\
\bar{x}_v &= f_\textit{ssm}(x_{v}), \\
x_{out} &= f_\textit{mrg}(\bar{x}_v \mid v \in V),
\end{aligned}
\label{eq7}
\end{equation}
where $V=\{1, 2, 3, 4\}$ represents a set of four different scanning directions, and $v \in V$ denotes a specific scanning direction.
Here, $f_\textit{exp}(\cdot)$ performs the scan expansion in direction $v$. Then, the output $x_v$ of $f_\textit{exp}(\cdot)$ is passed to SSMs, and $\bar{x}_v$ is estimated by the function $f_\textit{ssm}(\cdot)$, defined in Eqn.~\eqref{eq5}.
$f_\textit{mrg}(\cdot)$ combines the outputs in all the directions~\cite{liu2024vmamba}.


Although SSMs effectively model the overall content, they often struggle to preserve high-frequency image details, as illustrated in Fig.~\ref{fig1}(a). 
Moreover, as network depths increase, this issue would get worse, as shown in Fig.~\ref{fig1}(b).
As a result, solely relying on SSMs would lead to inferior compression performance.
To tackle this issue, we propose to integrate a CNN block in our CA-SSM module as CNNs excel at capturing fine-grained local details~\cite{park2021vision, zou2022devil, liu2023learned}.
As illustrated in Fig.~\ref{fig1}(a), features extracted by CNNs contain more high-frequency details compared to those from SSMs.
Therefore, a simple yet effective ResBlock~\cite{he2016deep} is adopted to capture local details.
While a VSS block models the global content of an image, the ResBlock plays a complementary role to the VSS block in our CA-SSM module. 
In doing so, an input feature $x\in \mathbb{R}^{C \times H \times W}$ is processed through parallel branches of SSMs and CNNs, producing features $\mathcal{F}_\textit{SSM}$ and $\mathcal{F}_\textit{CNN}$, as shown in Fig.~\ref{fig2}(b). 


Moreover, we employ a dynamic fusion block to fuse SSM features (\ie, global content features) and CNN features (\ie, local features) in our CA-SSM module. 
It learns to determine which features are more beneficial in improving rate-distortion performance.
In this way, our CA-SSM module seamlessly integrates global content features and local detail features in encoding and decoding.
Specifically, we first merge $\mathcal{F}_\textit{SSM}$ and $\mathcal{F}_\textit{CNN}$, and then apply a global max pooling operation to derive channel-wise representations, denoted by $\mathcal{F}_\textit{S} = f_{gp}(\mathcal{F}_\textit{SSM} + \mathcal{F}_\textit{CNN})$. 
Subsequently, $\mathcal{F}_\textit{S}$ is processed via a multilayer perceptron and a softmax operation to obtain corresponding attention weights $\alpha$ and $\beta$. 
Finally, these attention weights are used to modulate the features extracted from SSMs and CNNs dynamically.
Thus, the output $y$ of our CA-SSM module can be expressed as:
\vspace{-0.5em}
\begin{equation}
\begin{aligned}
y &= w(\alpha \cdot \mathcal{F}_\textit{SSM} + \beta \cdot \mathcal{F}_\textit{CNN}), \\
\alpha &= \frac{\exp(\mathcal{F}_\alpha)}{\exp(\mathcal{F}_\alpha) + \exp(\mathcal{F}_\beta)}, \\
\beta &= \frac{\exp(\mathcal{F}_\beta)}{\exp(\mathcal{F}_\alpha) + \exp(\mathcal{F}_\beta)}, \\
\mathcal{F}_\alpha  & = w_{mlp_1}(\mathcal{F}_\textit{S}), \quad \mathcal{F}_\beta = w_{mlp_2}(\mathcal{F}_\textit{S}), 
\end{aligned}
\label{eq8}
\end{equation}
where $w \in \mathbb{R} ^{C\times C}$ is a learnable parameter, $w_{mlp_1}$ and $w_{mlp_2}$ are the weights of the multilayer perceptions.



\subsection{Context-Aware Entropy Module}
\label{sec CAE}

As shown in Fig.~\ref{fig2}(c), CAE is designed to address the following challenges in the entropy model:
(1) how to precisely model content distribution while minimizing the bit number, and
(2) how to enhance the efficiency of entropy coding.
We design the CAE module to jointly model spatial and channel dependencies, thus facilitating precise and efficient entropy modeling of latent representations.


In the spatial dimension, our CAE leverages SSMs to parameterize the spatial content via Gaussian modeling due to its linear complexity in modeling global content dependencies.
Moreover, hardware speed-up algorithms are adopted in SSMs, including selective scan, kernel fusion, and recomputation, to aid efficient training and inference~\cite{gu2023mamba, zhu2024vision, liu2024vmamba, li2024videomamba}. 
Considering the sequential decoding nature of bitstreams, the inter-channel relations within latent representations are modeled autoregressively. 
In this way, the efficiency of encoding and decoding will not be significantly delayed.
To be specific, each channel is processed sequentially and conditioned on the prior derived from previously processed channels.
In this way, the channel-wise prior knowledge can be exploited to reduce inter-channel redundancy, thus minimizing bitrates.


Given a latent representation $y$, we first split it into $S$ groups along the channel dimension, \ie, $\{y_1, ... , y_S\}$.
To compress $y_{i}$, we concatenate the hyper-prior $\Phi^{'}$ (Eqn.~\eqref{eq2}) with the previous decoded groups $\bar{y}_{s<i}$.
These concatenated features are then processed via SSMs to estimate the Gaussian distribution parameters $\Phi_{i}$.
$\Phi_{i}$ is used to determine the Cumulative Distribution Function (CDF) for arithmetic coding.
Accurate estimation of $\Phi_{i}$ can reduce entropy and thus decrease the bit number for compression.
This process is defined as follows:
\begin{equation}
\begin{aligned}
\mathcal{F}_\textit{SQ} &= w_{sq}([\Phi^{'}, \bar{y}_{<i}]), \\
\mathcal{F}_\textit{SSM} &= f_\textit{ssm}(\mathcal{F}_\textit{SQ}) + \mathcal{F}_\textit{SQ}, \\
\Phi_{i} &= w_\textit{ffn}(\textit{LN}(\mathcal{F}_\textit{SSM})) + \mathcal{F}_\textit{SSM}, 
\end{aligned}
\label{eq9}
\end{equation}
where $w_{sq}$ is a learnable parameter,
and $\left[\cdot\right]$ indicates the concatenation operation.
The $w_\textit{ffn}$ is a learnable parameter of a Feed-Forward Network (FFN).
Next, a Latent Residual Prediction (LRP) network is employed to reduce this quantization error.
The error $r$ introduced by the quantization operation is defined as $r = y - Q(y)$.
The LRP network predicts $r$ using the hyper-prior $\Phi^{'}$ and previously decoded groups (\ie, $\bar{y}_{s<i}$ and $\hat{y}_{i}$).