\section{Related Work}
\label{sec:rlt}

\subsection{Oblique Decision Tree}
ODT alleviates the problem of high variance of DT in high-dimensional settings, but faces extremely high complexity at each node to find the optimal split and suffers the overfitting risk at the deep node. To deal with these challenges, \citet{breiman1984classification} first use a fully deterministic hill-climbing algorithm to search for the best oblique split.
\citet{heath1993induction} and \citet{murthy1994system} propose combining random perturbations and hill-climbing algorithm to search for the best split, potentially avoiding getting stuck in local optima. 
Recently, \citet{bertsimas2017optimal} and \citet{zhu2020scalable} introduce the MIO strategy to further improve the efficiency of solving projection directions.
Unlike these deterministic approximation algorithms, another more interesting and practical research direction is to generate candidate projections through data-driven methods.
One possibility is to use dimensionality reduction techniques, such as PCA \citep{rodriguez2006rotation,menze2011oblique} and LDA \citep{li2003multivariate,lopez2013fisher}.
\citet{tomita2020sparse} show that sparse random projections or random rotations can also be introduced by incorporating.
Recently, some studies have extended ODTs to unsupervised learning frameworks such as clustering, demonstrating its advantages in representation ability~\citep{stepivsnik2021oblique,ganaie2022oblique}.
However, the explanation for their success is largely based on heuristics, until \citet{cattaneo2022convergence} demonstrate the consistency rate of excess risk for individual ODT.

\subsection{Feature Concatenation}
Deep Forest \citep{zhou2017deep} successfully constructs non-differentiable deep models by implementing feature concatenation mechanisms that enable in-model feature transformation based on decision trees. This mechanism has been theoretically proven to effectively improve the consistency rate of tree-based ensembles \citep{arnould2021analyzing,lyu2022depth}. \citet{chen2021improving} utilize the decision path of trees in the forest to generate oblique feature representations, which has been proven to effectively alleviate the risk of overfitting caused by feature redundancy \citep{lyu2022region}.
In addition, feature concatenation also has strong scalability and can adapt to different learning tasks by screening concatenated features. Recent research has expanded the tree-based deep models to some specific settings, such as multi-label learning \citep{yang2020multilabel} and semi-supervised learning \citep{wang2020learning}. 
Although feature concatenation has been widely used in ensemble learning, this work is still the first to introduce it in tree construction.