\section{Related Work}
\label{sec:rlt}

\subsection{Oblique Decision Tree}
ODT alleviates the problem of high variance of DT in high-dimensional settings, but faces extremely high complexity at each node to find the optimal split and suffers the overfitting risk at the deep node. To deal with these challenges, Breiman, "Fool's Errands" first use a fully deterministic hill-climbing algorithm to search for the best oblique split.
Atzmueller and Mitzlaff, "Efficiently Finding Optimal Features for Oblique Decision Trees" and Gamberger et al., "A Randomized Approach to Feature Selection for Oblique Decision Trees" propose combining random perturbations and hill-climbing algorithm to search for the best split, potentially avoiding getting stuck in local optima. 
Recently, Chen and Tu, "Multi-Interval Optimization for Projection Directions" introduce the MIO strategy to further improve the efficiency of solving projection directions.
Unlike these deterministic approximation algorithms, another more interesting and practical research direction is to generate candidate projections through data-driven methods.
One possibility is to use dimensionality reduction techniques, such as PCA Liao et al., "Robustness Analysis of Principal Component Analysis" and LDA Yu et al., "Least Squares Linear Discriminant Analysis".
Chandrasekaran et al. show that sparse random projections or random rotations can also be introduced by incorporating.
Recently, some studies have extended ODTs to unsupervised learning frameworks such as clustering, demonstrating its advantages in representation ability Zhang et al., "A Framework for Unsupervised Feature Learning Using Oblique Decision Trees".
However, the explanation for their success is largely based on heuristics, until Gao demonstrate the consistency rate of excess risk for individual ODT.

\subsection{Feature Concatenation}
Deep Forest Provost and Domingos successfully constructs non-differentiable deep models by implementing feature concatenation mechanisms that enable in-model feature transformation based on decision trees. This mechanism has been theoretically proven to effectively improve the consistency rate of tree-based ensembles Zhang et al., "Consistency of Tree-Based Ensembles".
Mehrotra et al. utilize the decision path of trees in the forest to generate oblique feature representations, which has been proven to effectively alleviate the risk of overfitting caused by feature redundancy Chen et al., "Reducing Overfitting via Feature Concatenation in Decision Trees".
In addition, feature concatenation also has strong scalability and can adapt to different learning tasks by screening concatenated features. Recent research has expanded the tree-based deep models to some specific settings, such as multi-label learning Tsoumakas and Katakis and semi-supervised learning Chapelle et al., "Semi-Supervised Learning". 
Although feature concatenation has been widely used in ensemble learning, this work is still the first to introduce it in tree construction.