
@article{mozafari_scaling_2014,
	title = {Scaling up crowd-sourcing to very large datasets: a case for active learning},
	volume = {8},
	number = {2},
	journal = {Proceedings of the VLDB Endowment},
	author = {Mozafari, Barzan and Sarkar, Purna and Franklin, Michael and Jordan, Michael and Madden, Samuel},
	year = {2014},
	note = {Publisher: VLDB Endowment},
	keywords = {Active learning, Crowdsourcing, Unread},
	pages = {125--136},
}

@book{noauthor_19th_2023,
	title = {19th {IEEE} {International} {Conference} on e-{Science}, e-{Science} 2023, {Limassol}, {Cyprus}, {October} 9-13, 2023},
	isbn = {979-8-3503-2223-1},
	url = {https://doi.org/10.1109/e-Science58273.2023},
	publisher = {IEEE},
	year = {2023},
	doi = {10.1109/E-SCIENCE58273.2023},
}

@inproceedings{page_digital_2023,
	title = {A {Digital} {Twin} for {Climate} {Extremes} {Using} {Artificial} {Intelligence}},
	url = {https://ieeexplore.ieee.org/document/10254928},
	doi = {10.1109/e-Science58273.2023.10254928},
	abstract = {A novel approach and methodology is developed to detect and characterize the changes in climate extreme events using Artificial Intelligence. These machine learning techniques, especially neural networks, can process large climate simulation ensembles better than traditional statistical methods. They therefore better assess uncertainties associated with the various projected IPCC (Intergovernmental Panel on Climate Change) scenarios and climate assessments, in a Digital Twin environment. The tool will enable end users to perform on-demand what-if scenarios in order to better evaluate the impact of climate change on several real-world applications in specific regions to better adapt and prepare the society.},
	urldate = {2024-07-20},
	booktitle = {2023 {IEEE} 19th {International} {Conference} on e-{Science} (e-{Science})},
	author = {Pagé, Christian and Durif, Anne},
	month = oct,
	year = {2023},
	note = {ISSN: 2325-3703},
	pages = {1--2},
}

@inproceedings{kontomaris_cwl-flops_2023,
	address = {Limassol, Cyprus},
	title = {{CWL}-{FLOps}: {A} {Novel} {Method} for {Federated} {Learning} {Operations} at {Scale}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350322231},
	shorttitle = {{CWL}-{FLOps}},
	url = {https://ieeexplore.ieee.org/document/10254788/},
	doi = {10.1109/e-Science58273.2023.10254788},
	abstract = {Federated Learning (FL) has attracted much attention in recent years because it enables users with private data sets to train a global model collaboratively without raw data exchange. However, due to a lack of automation, researchers often struggled to develop, deploy, track, and manage all the data, steps, and configuration setup for all FL participating nodes. Federated Learning Operations (FLOps) is recently emerging in the FL community, a new methodology for developing FL systems efficiently and continuously. Some research works discussed approaches for FLOps, but only a few solutions address managing FL application scenarios from the workflow perspective. This poster proposes CWL-FLOps, a novel CWL-based method for FLOps, which can improve the flexibility of FL abstraction and fully automate the FL deployment and execution by mapping high-level descriptions onto distributed resource nodes. Our experiments demonstrate the feasibility of describing centralized and decentralized FL scenarios using CWL abstracted definitions without relying on heavily customized or external software for execution.},
	language = {en},
	urldate = {2024-07-20},
	booktitle = {2023 {IEEE} 19th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Kontomaris, Chronis and Wang, Yuandou and Zhao, Zhiming},
	month = oct,
	year = {2023},
	pages = {1--2},
}

@inproceedings{katz_research_2023,
	title = {Research {Software} {Engineering} in 2030},
	url = {http://arxiv.org/abs/2308.07796},
	doi = {10.1109/e-Science58273.2023.10254813},
	abstract = {This position paper for an invited talk on the "Future of eScience" discusses the Research Software Engineering Movement and where it might be in 2030. Because of the authors' experiences, it is aimed globally but with examples that focus on the United States and United Kingdom.},
	urldate = {2024-07-20},
	booktitle = {2023 {IEEE} 19th {International} {Conference} on e-{Science} (e-{Science})},
	author = {Katz, Daniel S. and Hettrick, Simon},
	month = oct,
	year = {2023},
	note = {arXiv:2308.07796 [cs]},
	pages = {1--2},
}

@inproceedings{kontomaris_cwl-flops_2023-1,
	title = {{CWL}-{FLOps}: {A} {Novel} {Method} for {Federated} {Learning} {Operations} at {Scale}},
	url = {https://doi.org/10.1109/e-Science58273.2023.10254788},
	doi = {10.1109/E-SCIENCE58273.2023.10254788},
	booktitle = {19th {IEEE} {International} {Conference} on e-{Science}, e-{Science} 2023, {Limassol}, {Cyprus}, {October} 9-13, 2023},
	publisher = {IEEE},
	author = {Kontomaris, Chronis and Wang, Yuandou and Zhao, Zhiming},
	year = {2023},
	pages = {1--2},
}

@inproceedings{lu_zebralancer_2018,
	title = {{ZebraLancer}: {Private} and {Anonymous} {Crowdsourcing} {System} atop {Open} {Blockchain}},
	shorttitle = {{ZebraLancer}},
	url = {https://ieeexplore.ieee.org/abstract/document/8416350},
	doi = {10.1109/ICDCS.2018.00087},
	abstract = {We design and implement the first private and anonymous decentralized crowdsourcing system ZebraLancer, and overcome two fundamental challenges of decentralizing crowdsourcing, i.e. data leakage and identity breach. First, our outsource-then-prove methodology resolves the tension between blockchain transparency and data confidentiality, which is critical in crowdsourcing use-case. ZebraLancer ensures: (i) a requester will not pay more than what data deserve, according to a policy announced when her task is published via the blockchain; (ii) each worker indeed gets a payment based on the policy, if he submits data to the blockchain; (iii) the above properties are realized not only without a central arbiter, but also without leaking the data to the open blockchain. Furthermore, the transparency of blockchain allows one to infer private information about workers and requesters through their participation history. On the other hand, allowing anonymity will enable a malicious worker to submit multiple times to reap rewards. ZebraLancer overcomes this problem by allowing anonymous requests/submissions without sacrificing the accountability. The idea behind is a subtle linkability: if a worker submits twice to a task, anyone can link the submissions, or else he stays anonymous and unlinkable across tasks. To realize this delicate linkability, we put forward a novel cryptographic concept, i.e. the common-prefix-linkable anonymous authentication. We remark the new anonymous authentication scheme might be of independent interest. Finally, we implement our protocol for a common image annotation task and deploy it in a test net of Ethereum. The experiment results show the applicability of our protocol with the existing real-world blockchain.},
	urldate = {2024-01-22},
	booktitle = {2018 {IEEE} 38th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Lu, Yuan and Tang, Qiang and Wang, Guiling},
	month = jul,
	year = {2018},
	note = {ISSN: 2575-8411},
	keywords = {Blockchain, Crowdsourcing, Important},
	pages = {853--865},
}

@book{walck_learn_2023,
	address = {San Francisco},
	title = {Learn physics with functional programming: a hands-on guide to exploring with {Haskell}},
	isbn = {978-1-71850-166-9},
	shorttitle = {Learn physics with functional programming},
	abstract = {"This book shows how to solve physics problems using Haskell, a functional programming language. Source code, equations, and diagrams throughout demonstrate how physics enthusiasts and functional programmers can use Haskell and its mathematical structures to solve problems from Newtonian mechanics and electromagnetics"--},
	language = {en},
	publisher = {No Starch Press},
	author = {Walck, Scott N.},
	year = {2023},
}

@inproceedings{demers_epidemic_1987,
	title = {Epidemic algorithms for replicated database maintenance},
	booktitle = {Proceedings of the sixth annual {ACM} {Symposium} on {Principles} of distributed computing},
	author = {Demers, Alan and Greene, Dan and Hauser, Carl and Irish, Wes and Larson, John and Shenker, Scott and Sturgis, Howard and Swinehart, Dan and Terry, Doug},
	year = {1987},
	keywords = {Unread, misc},
	pages = {1--12},
}

@article{pertsev_tornado_2019,
	title = {Tornado cash privacy solution version 1.4},
	volume = {1},
	journal = {Tornado cash privacy solution version},
	author = {Pertsev, Alexey and Semenov, Roman and Storm, Roman},
	year = {2019},
	keywords = {Unread, misc},
	pages = {7},
}

@article{samarati_protecting_1998,
	title = {Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression},
	author = {Samarati, Pierangela and Sweeney, Latanya},
	year = {1998},
	note = {Publisher: technical report, SRI International},
	keywords = {Unread, misc},
}

@misc{noauthor_notitle_nodate,
}

@inproceedings{mccallum_employing_1998,
	title = {Employing {EM} and {Pool}-{Based} {Active} {Learning} for {Text} {Classification}.},
	volume = {98},
	booktitle = {{ICML}},
	publisher = {Citeseer},
	author = {McCallum, Andrew Kachites and Nigam, Kamal and {others}},
	year = {1998},
	keywords = {Unread},
	pages = {350--358},
}

@article{samarati_protecting_nodate,
	title = {Protecting {Privacy} when {Disclosing} {Information}: k-{Anonymity} and {Its} {Enforcement} through {Generalization} and {Suppression}},
	abstract = {Today's globally networked society places great demand on the dissemination and sharing of person-speci c data. Situations where aggregate statistical information was once the reporting norm now rely heavily on the transfer of microscopically detailed transaction and encounter information. This happens at a time when more and more historically public information is also electronically available. When these data are linked together, they provide an electronic shadow of a person or organization that is as identifying and personal as a ngerprint, even when the sources of the information contains no explicit identi ers, such as name and phone number. In order to protect the anonymity of individuals to whom released data refer, data holders often remove or encrypt explicit identi ers such as names, addresses and phone numbers. However, other distinctive data, which we term quasi-identi ers, often combine uniquely and can be linked to publicly available information to re-identify individuals.},
	language = {en},
	author = {Samarati, Pierangela and Sweeney, Latanya},
	keywords = {Unread, misc},
}

@inproceedings{samarati_protecting_1998-1,
	title = {Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression},
	shorttitle = {Protecting privacy when disclosing information},
	url = {https://www.semanticscholar.org/paper/Protecting-privacy-when-disclosing-information%3A-and-Samarati-Sweeney/7df12c498fecedac4ab6034d3a8032a6d1366ca6},
	abstract = {Today's globally networked society places great demand on the dissemination and sharing of person-speci(cid:12)c data. Situations where aggregate statistical information was once the reporting norm now rely heavily on the transfer of microscopically detailed transaction and encounter information. This happens at a time when more and more historically public information is also electronically available. When these data are linked together, they provide an electronic shadow of a person or organization that is as identifying and personal as a (cid:12)ngerprint, even when the sources of the information contains no explicit identi(cid:12)ers, such as name and phone number. In order to protect the anonymity of individuals to whom released data refer, data holders often remove or encrypt explicit identi(cid:12)ers such as names, addresses and phone numbers. However, other distinctive data, which we term quasi-identi(cid:12)ers, often combine uniquely and can be linked to publicly available information to re-identify individuals. In this paper we address the problem of releasing person-speci(cid:12)c data while, at the same time, safeguarding the anonymity of the individuals to whom the data refer. The approach is based on the de(cid:12)nition of k-anonymity. A table provides k-anonymity if attempts to link explicitly identifying information to its contents ambiguously map the information to at least k entities. We illustrate how k-anonymity can be provided by using generalization and suppression techniques. We introduce the concept of minimal generalization, which captures the property of the release process not to distort the data more than needed to achieve k-anonymity. We illustrate possible preference policies to choose among di(cid:11)erent minimal generalizations. Finally, we present an algorithm and experimental results when an implementation of the algorithm was used to produce releases of real medical information. We also report on the quality of the released data by measuring the precision and completeness of the results for di(cid:11)erent values of k},
	urldate = {2024-06-27},
	author = {Samarati, P. and Sweeney, L.},
	year = {1998},
	keywords = {Unread, misc},
}

@misc{danka_modal_2018,
	title = {{modAL}: {A} modular active learning framework for {Python}},
	shorttitle = {{modAL}},
	url = {http://arxiv.org/abs/1805.00979},
	doi = {10.48550/arXiv.1805.00979},
	abstract = {modAL is a modular active learning framework for Python, aimed to make active learning research and practice simpler. Its distinguishing features are (i) clear and modular object oriented design (ii) full compatibility with scikit-learn models and workflows. These features make fast prototyping and easy extensibility possible, aiding the development of real-life active learning pipelines and novel algorithms as well. modAL is fully open source, hosted on GitHub at https://github.com/cosmic-cortex/modAL. To assure code quality, extensive unit tests are provided and continuous integration is applied. In addition, a detailed documentation with several tutorials are also available for ease of use. The framework is available in PyPI and distributed under the MIT license.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Danka, Tivadar and Horvath, Peter},
	month = dec,
	year = {2018},
	note = {arXiv:1805.00979 [cs, stat]},
}

@misc{noauthor_modal-pythonmodal_2024,
	title = {{modAL}-python/{modAL}},
	copyright = {MIT},
	url = {https://github.com/modAL-python/modAL},
	abstract = {A modular active learning framework for Python},
	urldate = {2024-06-11},
	publisher = {modAL},
	month = jun,
	year = {2024},
	note = {original-date: 2017-11-14T14:01:15Z},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	number = {85},
	urldate = {2024-06-11},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	year = {2011},
	keywords = {misc},
	pages = {2825--2830},
}

@inproceedings{androulaki_hyperledger_2018,
	title = {Hyperledger {Fabric}: {A} {Distributed} {Operating} {System} for {Permissioned} {Blockchains}},
	shorttitle = {Hyperledger {Fabric}},
	url = {http://arxiv.org/abs/1801.10228},
	doi = {10.1145/3190508.3190538},
	abstract = {Fabric is a modular and extensible open-source system for deploying and operating permissioned blockchains and one of the Hyperledger projects hosted by the Linux Foundation (www.hyperledger.org). Fabric is the first truly extensible blockchain system for running distributed applications. It supports modular consensus protocols, which allows the system to be tailored to particular use cases and trust models. Fabric is also the first blockchain system that runs distributed applications written in standard, general-purpose programming languages, without systemic dependency on a native cryptocurrency. This stands in sharp contrast to existing blockchain platforms that require "smart-contracts" to be written in domain-specific languages or rely on a cryptocurrency. Fabric realizes the permissioned model using a portable notion of membership, which may be integrated with industry-standard identity management. To support such flexibility, Fabric introduces an entirely novel blockchain design and revamps the way blockchains cope with non-determinism, resource exhaustion, and performance attacks. This paper describes Fabric, its architecture, the rationale behind various design decisions, its most prominent implementation aspects, as well as its distributed application programming model. We further evaluate Fabric by implementing and benchmarking a Bitcoin-inspired digital currency. We show that Fabric achieves end-to-end throughput of more than 3500 transactions per second in certain popular deployment configurations, with sub-second latency, scaling well to over 100 peers.},
	urldate = {2024-06-11},
	booktitle = {Proceedings of the {Thirteenth} {EuroSys} {Conference}},
	author = {Androulaki, Elli and Barger, Artem and Bortnikov, Vita and Cachin, Christian and Christidis, Konstantinos and De Caro, Angelo and Enyeart, David and Ferris, Christopher and Laventman, Gennady and Manevich, Yacov and Muralidharan, Srinivasan and Murthy, Chet and Nguyen, Binh and Sethi, Manish and Singh, Gari and Smith, Keith and Sorniotti, Alessandro and Stathakopoulou, Chrysoula and Vukolić, Marko and Cocco, Sharon Weed and Yellick, Jason},
	month = apr,
	year = {2018},
	note = {arXiv:1801.10228 [cs]},
	keywords = {Unread, misc},
	pages = {1--15},
}

@incollection{boyer_mjrtyfast_1991,
	address = {Dordrecht},
	title = {{MJRTY}—{A} {Fast} {Majority} {Vote} {Algorithm}},
	isbn = {978-94-011-3488-0},
	url = {https://doi.org/10.1007/978-94-011-3488-0_5},
	abstract = {A new algorithm is presented for determining which, if any, of an arbitrary number of candidates has received a majority of the votes cast in an election. The number of comparisons required is at most twice the number of votes. Furthermore, the algorithm uses storage in a way that permits an efficient use of magnetic tape. A Fortran version of the algorithm is exhibited. The Fortran code has been proved correct by a mechanical verification system for Fortran. The system and the proof are discussed.},
	language = {en},
	urldate = {2024-06-11},
	booktitle = {Automated {Reasoning}: {Essays} in {Honor} of {Woody} {Bledsoe}},
	publisher = {Springer Netherlands},
	author = {Boyer, Robert S. and Moore, J. Strother},
	editor = {Boyer, Robert S.},
	year = {1991},
	doi = {10.1007/978-94-011-3488-0_5},
	keywords = {Unread, misc},
	pages = {105--117},
}

@misc{noauthor_iden3snarkjs_2024,
	title = {iden3/snarkjs},
	copyright = {GPL-3.0},
	url = {https://github.com/iden3/snarkjs},
	abstract = {zkSNARK implementation in JavaScript \& WASM},
	urldate = {2024-06-09},
	publisher = {iden3},
	month = jun,
	year = {2024},
	note = {original-date: 2018-08-09T06:16:06Z},
}

@article{belles-munoz_circom_2023,
	title = {Circom: {A} {Circuit} {Description} {Language} for {Building} {Zero}-{Knowledge} {Applications}},
	volume = {20},
	issn = {1941-0018},
	shorttitle = {Circom},
	url = {https://ieeexplore.ieee.org/document/10002421},
	doi = {10.1109/TDSC.2022.3232813},
	abstract = {A zero-knowledge (ZK) proof guarantees that the result of a computation is correct while keeping part of the computation details private. Some ZK proofs are tiny and can be verified in short time, which makes them one of the most promising technologies for solving two key aspects: the challenge of enabling privacy to public and transparent distributed ledgers and enhancing their scalability limitations. Most practical ZK systems require the computation to be expressed as an arithmetic circuit that is encoded as a set of equations called rank-1 constraint system (R1CS). In this paper, we present Circom, a programming language and a compiler for designing arithmetic circuits that are compiled to R1CS. More precisely, with Circom, programmers can design arithmetic circuits at a constraint level, and the compiler outputs a file with the R1CS description, and WebAssembly and C++ programs to efficiently compute all values of the circuit. We also provide an open-source library called circomlib with multiple circuit templates. Circom can be complemented with snarkjs, a library for generating and validating ZK proofs from R1CS. Altogether, our software tools abstract the complexity of ZK proving mechanisms and provide a unique and friendly interface to model low-level descriptions of arithmetic circuits.},
	number = {6},
	urldate = {2024-06-09},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Bellés-Muñoz, Marta and Isabel, Miguel and Muñoz-Tapia, Jose Luis and Rubio, Albert and Baylina, Jordi},
	month = nov,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Blockchain, Unread},
	pages = {4733--4751},
}

@article{thaler_proofs_2022,
	title = {Proofs, {Arguments}, and {Zero}-{Knowledge}},
	volume = {4},
	issn = {2474-1558, 2474-1566},
	url = {https://www.nowpublishers.com/article/Details/SEC-030},
	doi = {10.1561/3300000030},
	abstract = {Proofs, Arguments, and Zero-Knowledge},
	language = {English},
	number = {2–4},
	urldate = {2024-06-09},
	journal = {Foundations and Trends® in Privacy and Security},
	author = {Thaler, Justin},
	month = dec,
	year = {2022},
	note = {Publisher: Now Publishers, Inc.},
	pages = {117--660},
}

@article{pertsev_tornado_nodate,
	title = {Tornado {Cash} {Privacy} {Solution} {Version} 1.4},
	language = {en},
	author = {Pertsev, Alexey and Semenov, Roman and Storm, Roman},
	keywords = {Unread, zkp},
}

@inproceedings{groth_size_2016,
	address = {Berlin, Heidelberg},
	title = {On the {Size} of {Pairing}-{Based} {Non}-interactive {Arguments}},
	isbn = {978-3-662-49896-5},
	doi = {10.1007/978-3-662-49896-5_11},
	abstract = {Non-interactive arguments enable a prover to convince a verifier that a statement is true. Recently there has been a lot of progress both in theory and practice on constructing highly efficient non-interactive arguments with small size and low verification complexity, so-called succinct non-interactive arguments (SNARGs) and succinct non-interactive arguments of knowledge (SNARKs).},
	language = {en},
	booktitle = {Advances in {Cryptology} – {EUROCRYPT} 2016},
	publisher = {Springer},
	author = {Groth, Jens},
	editor = {Fischlin, Marc and Coron, Jean-Sébastien},
	year = {2016},
	keywords = {Unread, zkp},
	pages = {305--326},
}

@article{sun_survey_2021,
	title = {A {Survey} on {Zero}-{Knowledge} {Proof} in {Blockchain}},
	volume = {35},
	doi = {10.1109/MNET.011.2000473},
	abstract = {Blockchain, which is usually regarded as a public, decentralized and distributed ledger, has attracted significant attention recently. In the environment of blockchain, all historical transaction data are recorded and stored. However, because blockchain is open and transparent, a malicious user may illegally access private transaction data, including transaction amount, account address, and account balance. As a cryptographic technique, zero-knowledge proof (ZKP) can be used to verify whether the prover has enough transaction amount in the environment of blockchain without leaking any private transaction data. This article provides a comprehensive survey on ZKP in the environment of blockchain with the aim of highlighting security problems and challenges. It first discusses the framework, models and applications of ZKP. Next, it provides an introduction of blockchain, and proposes a framework of ZKP in the environment of blockchain. Then, it highlights the current state of ZKP in the environment of blockchain. Finally, it identifies some potential problems and future research directions.},
	journal = {IEEE Network},
	author = {Sun, Xiaoqiang and Yu, F. and Zhang, Peng and Sun, Zhiwei and Xie, Weixin and Peng, Xiang},
	month = jul,
	year = {2021},
	keywords = {Survey, Unread, zkp},
	pages = {198--205},
}

@article{sun_survey_2021-1,
	title = {A {Survey} on {Zero}-{Knowledge} {Proof} in {Blockchain}},
	volume = {35},
	issn = {1558-156X},
	url = {https://ieeexplore.ieee.org/abstract/document/9520375},
	doi = {10.1109/MNET.011.2000473},
	abstract = {Blockchain, which is usually regarded as a public, decentralized and distributed ledger, has attracted significant attention recently. In the environment of blockchain, all historical transaction data are recorded and stored. However, because blockchain is open and transparent, a malicious user may illegally access private transaction data, including transaction amount, account address, and account balance. As a cryptographic technique, zero-knowledge proof (ZKP) can be used to verify whether the prover has enough transaction amount in the environment of blockchain without leaking any private transaction data. This article provides a comprehensive survey on ZKP in the environment of blockchain with the aim of highlighting security problems and challenges. It first discusses the framework, models and applications of ZKP. Next, it provides an introduction of blockchain, and proposes a framework of ZKP in the environment of blockchain. Then, it highlights the current state of ZKP in the environment of blockchain. Finally, it identifies some potential problems and future research directions.},
	number = {4},
	urldate = {2024-06-09},
	journal = {IEEE Network},
	author = {Sun, Xiaoqiang and Yu, F. Richard and Zhang, Peng and Sun, Zhiwei and Xie, Weixin and Peng, Xiang},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Network},
	pages = {198--205},
}

@article{buterin_next-generation_2014,
	title = {A next-generation smart contract and decentralized application platform},
	volume = {3},
	url = {https://finpedia.vn/wp-content/uploads/2022/02/Ethereum_white_paper-a_next_generation_smart_contract_and_decentralized_application_platform-vitalik-buterin.pdf},
	number = {37},
	urldate = {2024-06-09},
	journal = {white paper},
	author = {Buterin, Vitalik},
	year = {2014},
	keywords = {Blockchain, Unread},
	pages = {2--1},
}

@inproceedings{buterin_next_2015,
	title = {A {NEXT} {GENERATION} {SMART} {CONTRACT} \& {DECENTRALIZED} {APPLICATION} {PLATFORM}},
	url = {https://www.semanticscholar.org/paper/A-NEXT-GENERATION-SMART-CONTRACT-%26-DECENTRALIZED-Buterin/0dbb8a54ca5066b82fa086bbf5db4c54b947719a},
	abstract = {When Satoshi Nakamoto first set the Bitcoin blockchain into motion in January 2009, he was simultaneously introducing two radical and untested concepts. The first is the "bitcoin", a decentralized peer-to-peer online currency that maintains a value without any back ing, intrinsic value or central issuer. So far, the "bitcoin" as a currency unit has taken up the bulk of the pu blic attention, both in terms of the political aspects of a currency without a central bank and its extreme upwar d and downward volatility in price. However, there is also another, equally important, part to Satoshi's g rand experiment: the concept of a proof of work-based blockchain to allow for public agreement on the order of t ransactions. Bitcoin as an application can be described as a first-to-file system: if one entity has 50 BTC, and simultaneo usly sends the same 50 BTC to A and to B, only the transaction that gets confirmed first will process. T here is no intrinsic way of determining from two transactions which came earlier, and for decades this stymied t he development of decentralized digital currency. Satoshi's blockchain was the first credible decentr alized solution. And now, attention is rapidly starting to shift toward this second part of Bitcoin's technolo gy, and how the blockchain concept can be used for more than just money.},
	urldate = {2024-06-09},
	author = {Buterin, Vitalik},
	year = {2015},
}

@inproceedings{buterin_next_2015-1,
	title = {A {NEXT} {GENERATION} {SMART} {CONTRACT} \& {DECENTRALIZED} {APPLICATION} {PLATFORM}},
	url = {https://www.semanticscholar.org/paper/A-NEXT-GENERATION-SMART-CONTRACT-%26-DECENTRALIZED-Buterin/0dbb8a54ca5066b82fa086bbf5db4c54b947719a},
	abstract = {When Satoshi Nakamoto first set the Bitcoin blockchain into motion in January 2009, he was simultaneously introducing two radical and untested concepts. The first is the "bitcoin", a decentralized peer-to-peer online currency that maintains a value without any back ing, intrinsic value or central issuer. So far, the "bitcoin" as a currency unit has taken up the bulk of the pu blic attention, both in terms of the political aspects of a currency without a central bank and its extreme upwar d and downward volatility in price. However, there is also another, equally important, part to Satoshi's g rand experiment: the concept of a proof of work-based blockchain to allow for public agreement on the order of t ransactions. Bitcoin as an application can be described as a first-to-file system: if one entity has 50 BTC, and simultaneo usly sends the same 50 BTC to A and to B, only the transaction that gets confirmed first will process. T here is no intrinsic way of determining from two transactions which came earlier, and for decades this stymied t he development of decentralized digital currency. Satoshi's blockchain was the first credible decentr alized solution. And now, attention is rapidly starting to shift toward this second part of Bitcoin's technolo gy, and how the blockchain concept can be used for more than just money.},
	urldate = {2024-06-09},
	author = {Buterin, Vitalik},
	year = {2015},
	keywords = {Blockchain, Unread},
}

@misc{noauthor_epidemic_nodate,
	title = {Epidemic algorithms for replicated database maintenance {\textbar} {Proceedings} of the sixth annual {ACM} {Symposium} on {Principles} of distributed computing},
	url = {https://dl.acm.org/doi/10.1145/41840.41841},
	urldate = {2024-06-09},
	keywords = {Blockchain, Unread},
}

@article{nakamoto_bitcoin_2009,
	title = {Bitcoin: {A} {Peer}-to-{Peer} {Electronic} {Cash} {System}},
	shorttitle = {Bitcoin},
	abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
	journal = {Cryptography Mailing list at https://metzdowd.com},
	author = {Nakamoto, Satoshi},
	month = mar,
	year = {2009},
	keywords = {Blockchain, Unread},
}

@article{haber_how_1991,
	title = {How to time-stamp a digital document},
	volume = {3},
	issn = {1432-1378},
	url = {https://doi.org/10.1007/BF00196791},
	doi = {10.1007/BF00196791},
	abstract = {The prospect of a world in which all text, audio, picture, and video documents are in digital form on easily modifiable media raises the issue of how to certify when a document was created or last changed. The problem is to time-stamp the data, not the medium. We propose computationally practical procedures for digital time-stamping of such documents so that it is infeasible for a user either to back-date or to forward-date his document, even with the collusion of a time-stamping service. Our procedures maintain complete privacy of the documents themselves, and require no record-keeping by the time-stamping service.},
	language = {en},
	number = {2},
	urldate = {2024-06-09},
	journal = {Journal of Cryptology},
	author = {Haber, Stuart and Stornetta, W. Scott},
	month = jan,
	year = {1991},
	keywords = {Blockchain, Unread},
	pages = {99--111},
}

@article{monrat_survey_2019,
	title = {A {Survey} of {Blockchain} {From} the {Perspectives} of {Applications}, {Challenges}, and {Opportunities}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/8805074},
	doi = {10.1109/ACCESS.2019.2936094},
	abstract = {Blockchain is the underlying technology of a number of digital cryptocurrencies. Blockchain is a chain of blocks that store information with digital signatures in a decentralized and distributed network. The features of blockchain, including decentralization, immutability, transparency and auditability, make transactions more secure and tamper proof. Apart from cryptocurrency, blockchain technology can be used in financial and social services, risk management, healthcare facilities, and so on. A number of research studies focus on the opportunity that blockchain provides in various application domains. This paper presents a comparative study of the tradeoffs of blockchain and also explains the taxonomy and architecture of blockchain, provides a comparison among different consensus mechanisms and discusses challenges, including scalability, privacy, interoperability, energy consumption and regulatory issues. In addition, this paper also notes the future scope of blockchain technology.},
	urldate = {2024-06-09},
	journal = {IEEE Access},
	author = {Monrat, Ahmed Afif and Schelén, Olov and Andersson, Karl},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Blockchain, Survey, Unread},
	pages = {117134--117151},
}

@inproceedings{yan_modeling_2010,
	title = {Modeling annotator expertise: {Learning} when everybody knows a bit of something},
	shorttitle = {Modeling annotator expertise},
	url = {https://proceedings.mlr.press/v9/yan10a.html},
	abstract = {Supervised learning from multiple labeling sources is an increasingly important problem in machine learning and data mining. This paper develops a probabilistic approach to this problem when annotators may be unreliable (labels are noisy), but also their expertise varies depending on the data they observe (annotators may have knowledge about different parts of the input space). That is, an annotator may not be consistently accurate (or inaccurate) across the task domain. The presented approach produces classification and annotator models that allow us to provide estimates of the true labels and annotator variable expertise. We provide an analysis of the proposed model under various scenarios and show experimentally that annotator expertise can indeed vary in real tasks and that the presented approach provides clear advantages over previously introduced multi-annotator methods, which only consider general annotator characteristics.},
	language = {en},
	urldate = {2024-06-09},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Yan, Yan and Rosales, Romer and Fung, Glenn and Schmidt, Mark and Hermosillo, Gerardo and Bogoni, Luca and Moy, Linda and Dy, Jennifer},
	month = mar,
	year = {2010},
	note = {ISSN: 1938-7228},
	keywords = {Crowdsourcing},
	pages = {932--939},
}

@article{buhrmester_amazons_2011,
	title = {Amazon's {Mechanical} {Turk}: {A} {New} {Source} of {Inexpensive}, {Yet} {High}-{Quality}, {Data}?},
	volume = {6},
	issn = {1745-6916},
	shorttitle = {Amazon's {Mechanical} {Turk}},
	doi = {10.1177/1745691610393980},
	abstract = {Amazon's Mechanical Turk (MTurk) is a relatively new website that contains the major elements required to conduct research: an integrated participant compensation system; a large participant pool; and a streamlined process of study design, participant recruitment, and data collection. In this article, we describe and evaluate the potential contributions of MTurk to psychology and other social sciences. Findings indicate that (a) MTurk participants are slightly more demographically diverse than are standard Internet samples and are significantly more diverse than typical American college samples; (b) participation is affected by compensation rate and task length, but participants can still be recruited rapidly and inexpensively; (c) realistic compensation rates do not affect data quality; and (d) the data obtained are at least as reliable as those obtained via traditional methods. Overall, MTurk can be used to obtain high-quality data inexpensively and rapidly.},
	language = {eng},
	number = {1},
	journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
	author = {Buhrmester, Michael and Kwang, Tracy and Gosling, Samuel D.},
	month = jan,
	year = {2011},
	pmid = {26162106},
	keywords = {Crowdsourcing, Unread},
	pages = {3--5},
}

@inproceedings{lewis_sequential_1994,
	address = {London},
	title = {A {Sequential} {Algorithm} for {Training} {Text} {Classifiers}},
	isbn = {978-1-4471-2099-5},
	doi = {10.1007/978-1-4471-2099-5_1},
	abstract = {The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.},
	language = {en},
	booktitle = {{SIGIR} ’94},
	publisher = {Springer},
	author = {Lewis, David D. and Gale, William A.},
	editor = {Croft, Bruce W. and van Rijsbergen, C. J.},
	year = {1994},
	keywords = {Active learning, Unread},
	pages = {3--12},
}

@article{mccallum_employing_nodate,
	title = {Employing {EM} and {Pool}-{Based} {Active} {Learning} for {Text} {Classiﬁcation}},
	abstract = {This paper shows how a text classiﬁer’s need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents. We modify the Query-by-Committee (QBC) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling. Then active learning is combined with ExpectationMaximization in order to “ﬁll in” the class labels of those documents that remain unlabeled. Experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous QBC approaches, and that the combination of EM and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved active learning or EM alone.},
	language = {en},
	author = {McCallum, Andrew Kachites and Nigam, Kamal},
	keywords = {Active learning, Unread},
}

@article{wang_cost-effective_2017,
	title = {Cost-{Effective} {Active} {Learning} for {Deep} {Image} {Classification}},
	volume = {27},
	issn = {1558-2205},
	url = {https://ieeexplore.ieee.org/abstract/document/7508942},
	doi = {10.1109/TCSVT.2016.2589879},
	abstract = {Recent successes in learning-based image classification, however, heavily rely on the large number of annotated training samples, which may require considerable human effort. In this paper, we propose a novel active learning (AL) framework, which is capable of building a competitive classifier with optimal feature representation via a limited amount of labeled training instances in an incremental learning manner. Our approach advances the existing AL methods in two aspects. First, we incorporate deep convolutional neural networks into AL. Through the properly designed framework, the feature representation and the classifier can be simultaneously updated with progressively annotated informative samples. Second, we present a cost-effective sample selection strategy to improve the classification performance with less manual annotations. Unlike traditional methods focusing on only the uncertain samples of low prediction confidence, we especially discover the large amount of high-confidence samples from the unlabeled set for feature learning. Specifically, these high-confidence samples are automatically selected and iteratively assigned pseudolabels. We thus call our framework cost-effective AL (CEAL) standing for the two advantages. Extensive experiments demonstrate that the proposed CEAL framework can achieve promising results on two challenging image classification data sets, i.e., face recognition on the cross-age celebrity face recognition data set database and object categorization on Caltech-256.},
	number = {12},
	urldate = {2024-06-09},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Wang, Keze and Zhang, Dongyu and Li, Ya and Zhang, Ruimao and Lin, Liang},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Active learning, Unread},
	pages = {2591--2600},
}

@techreport{settles_active_2009,
	type = {Technical {Report}},
	title = {Active {Learning} {Literature} {Survey}},
	url = {https://minds.wisconsin.edu/handle/1793/60660},
	abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain. 
 
This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
	language = {en},
	urldate = {2024-01-08},
	institution = {University of Wisconsin-Madison Department of Computer Sciences},
	author = {Settles, Burr},
	year = {2009},
	note = {Accepted: 2012-03-15T17:23:56Z},
	keywords = {Active learning, Survey},
}

@inproceedings{snow_cheap_2008,
	address = {Honolulu, Hawaii},
	title = {Cheap and {Fast} – {But} is it {Good}? {Evaluating} {Non}-{Expert} {Annotations} for {Natural} {Language} {Tasks}},
	shorttitle = {Cheap and {Fast} – {But} is it {Good}?},
	url = {https://aclanthology.org/D08-1027},
	urldate = {2024-05-07},
	booktitle = {Proceedings of the 2008 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Snow, Rion and O'Connor, Brendan and Jurafsky, Daniel and Ng, Andrew},
	editor = {Lapata, Mirella and Ng, Hwee Tou},
	month = oct,
	year = {2008},
	keywords = {Crowdsourcing, Unread},
	pages = {254--263},
}

@misc{pasdar_blockchain_2021,
	title = {Blockchain {Oracle} {Design} {Patterns}},
	url = {http://arxiv.org/abs/2106.09349},
	doi = {10.48550/arXiv.2106.09349},
	abstract = {Blockchain is a form of distributed ledger technology (DLT) where data is shared among users connected over the internet. Transactions are data state changes on the blockchain that are permanently recorded in a secure and transparent way without the need of a third party. Besides, the introduction of smart contracts to the blockchain has added programmability to the blockchain and revolutionized the software ecosystem leading toward decentralized applications (DApps) attracting businesses and organizations to employ this technology. Although promising, blockchains and smart contracts have no access to the external systems (i.e., off-chain) where real-world data and events resides; consequently, the usability of smart contracts in terms of performance and programmability would be limited to the on-chain data. Hence, {\textbackslash}emph\{blockchain oracles\} are introduced to mitigate the issue and are defined as trusted third-party services that send and verify the external information (i.e., feedback) and submit it to smart contracts for triggering state changes in the blockchain. In this paper, we will study and analyze blockchain oracles with regard to how they provide feedback to the blockchain and smart contracts. We classify the blockchain oracle techniques into two major groups such as voting-based strategies and reputation-based ones. The former mainly relies on participants' stakes for outcome finalization while the latter considers reputation in conjunction with authenticity proof mechanisms for data correctness and integrity. We then provide a structured description of patterns in detail for each classification and discuss research directions in the end.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Pasdar, Amirmohammad and Dong, Zhongli and Lee, Young Choon},
	month = jun,
	year = {2021},
	note = {arXiv:2106.09349 [cs]},
	keywords = {Unread, oracle},
}

@inproceedings{li_crowdsourcing_2016,
	address = {New York, NY, USA},
	series = {{WSDM} '16},
	title = {Crowdsourcing {High} {Quality} {Labels} with a {Tight} {Budget}},
	isbn = {978-1-4503-3716-8},
	url = {https://dl.acm.org/doi/10.1145/2835776.2835797},
	doi = {10.1145/2835776.2835797},
	abstract = {In the past decade, commercial crowdsourcing platforms have revolutionized the ways of classifying and annotating data, especially for large datasets. Obtaining labels for a single instance can be inexpensive, but for large datasets, it is important to allocate budgets wisely. With limited budgets, requesters must trade-off between the quantity of labeled instances and the quality of the final results. Existing budget allocation methods can achieve good quantity but cannot guarantee high quality of individual instances under a tight budget. However, in some scenarios, requesters may be willing to label fewer instances but of higher quality. Moreover, they may have different requirements on quality for different tasks. To address these challenges, we propose a flexible budget allocation framework called Requallo. Requallo allows requesters to set their specific requirements on the labeling quality and maximizes the number of labeled instances that achieve the quality requirement under a tight budget. The budget allocation problem is modeled as a Markov decision process and a sequential labeling policy is produced. The proposed policy greedily searches for the instance to query next as the one that can provide the maximum reward for the goal. The Requallo framework is further extended to consider worker reliability so that the budget can be better allocated. Experiments on two real-world crowdsourcing tasks as well as a simulated task demonstrate that when the budget is tight, the proposed Requallo framework outperforms existing state-of-the-art budget allocation methods from both quantity and quality aspects.},
	urldate = {2024-03-30},
	booktitle = {Proceedings of the {Ninth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Li, Qi and Ma, Fenglong and Gao, Jing and Su, Lu and Quinn, Christopher J.},
	month = feb,
	year = {2016},
	keywords = {Crowdsourcing, Important, Quality Control, Unread},
	pages = {237--246},
}

@inproceedings{sheng_get_2008,
	address = {New York, NY, USA},
	series = {{KDD} '08},
	title = {Get another label? improving data quality and data mining using multiple, noisy labelers},
	isbn = {978-1-60558-193-4},
	shorttitle = {Get another label?},
	url = {https://doi.org/10.1145/1401890.1401965},
	doi = {10.1145/1401890.1401965},
	abstract = {This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction. With the outsourcing of small tasks becoming easier, for example via Rent-A-Coder or Amazon's Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. With low-cost labeling, preparing the unlabeled part of the data can become considerably more expensive than labeling. We present repeated-labeling strategies of increasing complexity, and show several main results. (i) Repeated-labeling can improve label quality and model quality, but not always. (ii) When labels are noisy, repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap. (iii) As soon as the cost of processing the unlabeled data is not free, even the simple strategy of labeling everything multiple times can give considerable advantage. (iv) Repeatedly labeling a carefully chosen set of points is generally preferable, and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved. The bottom line: the results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial.},
	urldate = {2024-03-11},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
	month = aug,
	year = {2008},
	keywords = {Crowdsourcing, Unread},
	pages = {614--622},
}

@inproceedings{venanzi_community-based_2014,
	address = {New York, NY, USA},
	series = {{WWW} '14},
	title = {Community-based bayesian aggregation models for crowdsourcing},
	isbn = {978-1-4503-2744-2},
	url = {https://doi.org/10.1145/2566486.2567989},
	doi = {10.1145/2566486.2567989},
	abstract = {This paper addresses the problem of extracting accurate labels from crowdsourced datasets, a key challenge in crowdsourcing. Prior work has focused on modeling the reliability of individual workers, for instance, by way of confusion matrices, and using these latent traits to estimate the true labels more accurately. However, this strategy becomes ineffective when there are too few labels per worker to reliably estimate their quality. To mitigate this issue, we propose a novel community-based Bayesian label aggregation model, CommunityBCC, which assumes that crowd workers conform to a few different types, where each type represents a group of workers with similar confusion matrices. We assume that each worker belongs to a certain community, where the worker's confusion matrix is similar to (a perturbation of) the community's confusion matrix. Our model can then learn a set of key latent features: (i) the confusion matrix of each community, (ii) the community membership of each user, and (iii) the aggregated label of each item. We compare the performance of our model against established aggregation methods on a number of large-scale, real-world crowdsourcing datasets. Our experimental results show that our CommunityBCC model consistently outperforms state-of-the-art label aggregation methods, requiring, on average, 50\% less data to pass the 90\% accuracy mark.},
	urldate = {2024-03-11},
	booktitle = {Proceedings of the 23rd international conference on {World} wide web},
	publisher = {Association for Computing Machinery},
	author = {Venanzi, Matteo and Guiver, John and Kazai, Gabriella and Kohli, Pushmeet and Shokouhi, Milad},
	month = apr,
	year = {2014},
	keywords = {Crowdsourcing, Unread},
	pages = {155--164},
}

@inproceedings{quoc_viet_hung_evaluation_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Evaluation} of {Aggregation} {Techniques} in {Crowdsourcing}},
	isbn = {978-3-642-41154-0},
	doi = {10.1007/978-3-642-41154-0_1},
	abstract = {As the volumes of AI problems involving, human knowledge are likely to soar, crowdsourcing has become essential in a wide range of world-wide-web applications. One of the biggest challenges of crowdsourcing is aggregating the answers collected from the crowd since the workers might have wide-ranging levels of expertise. In order to tackle this challenge, many aggregation techniques have been proposed. These techniques, however, have never been compared and analyzed under the same setting, rendering a ‘right’ choice for a particular application very difficult. Addressing this problem, this paper presents a benchmark that offers a comprehensive empirical study on the performance comparison of the aggregation techniques. Specifically, we integrated several state-of-the-art methods in a comparable manner, and measured various performance metrics with our benchmark, including computation time, accuracy, robustness to spammers, and adaptivity to multi-labeling. We then provide in-depth analysis of benchmarking results, obtained by simulating the crowdsourcing process with different types of workers. We believe that the findings from the benchmark will be able to serve as a practical guideline for crowdsourcing applications.},
	language = {en},
	booktitle = {Web {Information} {Systems} {Engineering} – {WISE} 2013},
	publisher = {Springer},
	author = {Quoc Viet Hung, Nguyen and Tam, Nguyen Thanh and Tran, Lam Ngoc and Aberer, Karl},
	editor = {Lin, Xuemin and Manolopoulos, Yannis and Srivastava, Divesh and Huang, Guangyan},
	year = {2013},
	keywords = {Crowdsourcing, Unread},
	pages = {1--15},
}

@inproceedings{zhong_active_2015,
	address = {Buenos Aires, Argentina},
	series = {{IJCAI}'15},
	title = {Active learning from crowds with unsure option},
	isbn = {978-1-57735-738-4},
	abstract = {Learning from crowds, where the labels of data instances are collected using a crowdsourcing way, has attracted much attention during the past few years. In contrast to a typical crowdsourcing setting where all data instances are assigned to annotators for labeling, active learning from crowds actively selects a subset of data instances and assigns them to the annotators, thereby reducing the cost of labeling. This paper goes a step further. Rather than assume all annotators must provide labels, we allow the annotators to express that they are unsure about the assigned data instances. By adding the "unsure" option, the workloads for the annotators are somewhat reduced, because saying "unsure" will be easier than trying to provide a crisp label for some difficult data instances. Moreover, it is safer to use "unsure" feedback than to use labels from reluctant annotators because the latter has more chance to be misleading. Furthermore, different annotators may experience difficulty in different data instances, and thus the unsure option provides a valuable ingredient for modeling crowds' expertise. We propose the ALCU-SVM algorithm for this new learning problem. Experimental studies on simulated and real crowdsourcing data show that, by exploiting the unsure option, ALCU-SVM achieves very promising performance.},
	urldate = {2024-03-11},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Zhong, Jinhong and Tang, Ke and Zhou, Zhi-Hua},
	month = jul,
	year = {2015},
	keywords = {Active learning, Crowdsourcing, Unread},
	pages = {1061--1067},
}

@article{settles_active_2008,
	title = {Active learning with real annotation costs},
	abstract = {The goal of active learning is to minimize the cost of training an accurate model by allowing the learner to choose which instances are labeled for training. However, most research in active learning to date has assumed that the cost of acquiring labels is the same for all instances. In domains where labeling costs may vary, a reduction in the number of labeled instances does not guarantee a reduction in cost. To better understand the nature of actual labeling costs in such domains, we present a detailed empirical study of active learning with annotation costs in four real-world domains involving human annotators.},
	author = {Settles, Burr and Craven, Mark and Friedland, Lewis},
	month = jan,
	year = {2008},
	keywords = {Active learning, Unread},
}

@inproceedings{dredze_active_2008,
	address = {Columbus, Ohio},
	title = {Active {Learning} with {Confidence}},
	url = {https://aclanthology.org/P08-2059},
	urldate = {2024-03-12},
	booktitle = {Proceedings of {ACL}-08: {HLT}, {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Dredze, Mark and Crammer, Koby},
	editor = {Moore, Johanna D. and Teufel, Simone and Allan, James and Furui, Sadaoki},
	month = jun,
	year = {2008},
	keywords = {Active learning, Unread},
	pages = {233--236},
}

@inproceedings{hsueh_data_2009,
	address = {Boulder, Colorado},
	title = {Data {Quality} from {Crowdsourcing}: {A} {Study} of {Annotation} {Selection} {Criteria}},
	shorttitle = {Data {Quality} from {Crowdsourcing}},
	url = {https://aclanthology.org/W09-1904},
	urldate = {2024-03-12},
	booktitle = {Proceedings of the {NAACL} {HLT} 2009 {Workshop} on {Active} {Learning} for {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Hsueh, Pei-Yun and Melville, Prem and Sindhwani, Vikas},
	editor = {Ringger, Eric and Haertel, Robbie and Tomanek, Katrin},
	month = jun,
	year = {2009},
	keywords = {Crowdsourcing, Quality Control, Survey, Unread},
	pages = {27--35},
}

@article{lin_re-active_2016,
	title = {Re-{Active} {Learning}: {Active} {Learning} with {Relabeling}},
	volume = {30},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {Re-{Active} {Learning}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10315},
	doi = {10.1609/aaai.v30i1.10315},
	abstract = {Active learning seeks to train the best classifier at the lowest annotation cost by intelligently picking the best examples to label. Traditional algorithms assume there is a single annotator and disregard the possibility of requesting additional independent annotations for a previously labeled example. However, relabeling examples is important, because all annotators make mistakes — especially crowdsourced workers, who have become a common source of training data. This paper seeks to understand the difference in marginal value between decreasing the noise of the training set via relabeling and increasing the size and diversity of the (noisier) training set by labeling new examples. We use the term re-active learning to denote this generalization of active learning. We show how traditional active learning methods perform poorly at re-active learning, present new algorithms designed for this important problem, formally characterize their behavior, and empirically show that our methods effectively make this tradeoff.},
	language = {en},
	number = {1},
	urldate = {2024-03-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lin, Christopher and Mausam, M. and Weld, Daniel},
	month = feb,
	year = {2016},
	note = {Number: 1},
	keywords = {Active learning, Important, Unread},
}

@article{miao_balancing_2016,
	title = {Balancing quality and budget considerations in mobile crowdsourcing},
	volume = {90},
	issn = {0167-9236},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923616301117},
	doi = {10.1016/j.dss.2016.06.019},
	abstract = {Mobile/spatial crowdsourcing is a class of crowdsourcing applications in which workers travel to specific locations in order to perform tasks. As workers may possess different levels of competence, a major research challenge for spatial crowdsourcing is to control the quality of the results obtained. Although existing mobile crowdsourcing systems are able to track a wide range of performance related data for the participating workers, there still lacks an automated mechanism to help requesters make key task allocation decisions including: 1) to whom should a task to allocated; 2) how much to pay for the result provided by each worker; and 3) when to stop looking for additional workers for a task. In this paper, we propose a budget-aware task allocation approach for spatial crowdsourcing (Budget-TASC) to help requesters make these three decisions jointly. It considers the workers' reputation and proximity to the task locations to maximize the expected quality of the results while staying within a limited budget. Furthermore, it supports payments to workers based on how their track records. Extensive experimental evaluations based on a large-scale real-world dataset demonstrate that Budget-TASC outperforms the state-of-the-art significantly in terms of reduction in the average error rate and savings on the budget.},
	urldate = {2024-03-12},
	journal = {Decision Support Systems},
	author = {Miao, Chunyan and Yu, Han and Shen, Zhiqi and Leung, Cyril},
	month = oct,
	year = {2016},
	keywords = {Crowdsourcing, Unread},
	pages = {56--64},
}

@misc{zhang_spectral_2014,
	title = {Spectral {Methods} meet {EM}: {A} {Provably} {Optimal} {Algorithm} for {Crowdsourcing}},
	shorttitle = {Spectral {Methods} meet {EM}},
	url = {http://arxiv.org/abs/1406.3824},
	doi = {10.48550/arXiv.1406.3824},
	abstract = {Crowdsourcing is a popular paradigm for effectively collecting labels at low cost. The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Zhang, Yuchen and Chen, Xi and Zhou, Dengyong and Jordan, Michael I.},
	month = nov,
	year = {2014},
	note = {arXiv:1406.3824 [stat]},
	keywords = {Crowdsourcing, Unread},
}

@inproceedings{zhou_learning_2012,
	title = {Learning from the {Wisdom} of {Crowds} by {Minimax} {Entropy}},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/46489c17893dfdcf028883202cefd6d1-Abstract.html},
	abstract = {An important way to make large training sets is to gather noisy labels from crowds of nonexperts. We propose a minimax entropy principle to improve the quality of these labels. Our method assumes that labels are generated by a probability distribution over workers, items, and labels. By maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise. We infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. We show that a simple coordinate descent scheme can optimize minimax entropy. Empirically, our results are substantially better than previously published methods for the same problem.},
	urldate = {2024-03-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhou, Dengyong and Basu, Sumit and Mao, Yi and Platt, John},
	year = {2012},
	keywords = {Crowdsourcing, Unread},
}

@inproceedings{karger_iterative_2011,
	title = {Iterative {Learning} for {Reliable} {Crowdsourcing} {Systems}},
	volume = {24},
	url = {https://papers.nips.cc/paper_files/paper/2011/hash/c667d53acd899a97a85de0c201ba99be-Abstract.html},
	abstract = {Crowdsourcing systems, in which tasks are electronically distributed to numerous information piece-workers'', have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all crowdsourcers must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in some way such as majority voting. In this paper, we consider a general model of such  rowdsourcing tasks, and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give new algorithms for deciding which tasks to assign to which workers and for inferring correct answers from the workers’ answers. We show that our algorithm significantly outperforms majority voting and, in fact, are asymptotically optimal through comparison to an oracle that knows the reliability of every worker.},
	urldate = {2024-03-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Karger, David and Oh, Sewoong and Shah, Devavrat},
	year = {2011},
	keywords = {Crowdsourcing, Unread},
}

@article{jagabathula_identifying_2017,
	title = {Identifying {Unreliable} and {Adversarial} {Workers} in {Crowdsourced} {Labeling} {Tasks}},
	volume = {18},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v18/15-650.html},
	abstract = {We study the problem of identifying unreliable and adversarial workers in crowdsourcing systems where workers (or users) provide labels for tasks (or items). Most existing studies assume that worker responses follow specific probabilistic models; however, recent evidence shows the presence of workers adopting non-random or even malicious strategies. To account for such workers, we suppose that workers comprise a mixture of honest and adversarial workers. Honest workers may be reliable or unreliable, and they provide labels according to an unknown but explicit probabilistic model. Adversaries adopt labeling strategies different from those of honest workers, whether probabilistic or not. We propose two reputation algorithms to identify unreliable honest workers and adversarial workers from only their responses. Our algorithms assume that honest workers are in the majority, and they classify workers with outlier label patterns as adversaries. Theoretically, we show that our algorithms successfully identify unreliable honest workers, workers adopting deterministic strategies, and worst- case sophisticated adversaries who can adopt arbitrary labeling strategies to degrade the accuracy of the inferred task labels. Empirically, we show that filtering out outliers using our algorithms can significantly improve the accuracy of several state-of-the-art label aggregation algorithms in real-world crowdsourcing datasets.},
	number = {93},
	urldate = {2024-03-12},
	journal = {Journal of Machine Learning Research},
	author = {Jagabathula, Srikanth and Subramanian, Lakshminarayanan and Venkataraman, Ashwin},
	year = {2017},
	keywords = {Crowdsourcing, Unread},
	pages = {1--67},
}

@article{goyal_your_2018,
	title = {Your {Behavior} {Signals} {Your} {Reliability}: {Modeling} {Crowd} {Behavioral} {Traces} to {Ensure} {Quality} {Relevance} {Annotations}},
	volume = {6},
	copyright = {Copyright (c) 2018 Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	issn = {2769-1349},
	shorttitle = {Your {Behavior} {Signals} {Your} {Reliability}},
	url = {https://ojs.aaai.org/index.php/HCOMP/article/view/13331},
	doi = {10.1609/hcomp.v6i1.13331},
	abstract = {While peer-agreement and gold checks are well-established methods for ensuring quality in crowdsourced data collection, we explore a relatively new direction for quality control: estimating work quality directly from workers’ behavioral traces collected during annotation. We propose three behavior-based models to predict label correctness and worker accuracy, then further apply model predictions to label aggregation and optimization of label collection. As part of this work, we collect and share a new Mechanical Turk dataset of behavioral signals judging the relevance of search results. Results show that behavioral data can be effectively used to predict work quality, which could be especially useful with single labeling or in a cold start scenario in which individuals’ prior work history is unavailable. We further show improvement in label aggregation and reducing labeling cost while ensuring data quality.},
	language = {en},
	urldate = {2024-03-12},
	journal = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	author = {Goyal, Tanya and McDonnell, Tyler and Kutlu, Mucahid and Elsayed, Tamer and Lease, Matthew},
	month = jun,
	year = {2018},
	keywords = {Crowdsourcing, Unread},
	pages = {41--49},
}

@article{burmania_increasing_2016,
	title = {Increasing the {Reliability} of {Crowdsourcing} {Evaluations} {Using} {Online} {Quality} {Assessment}},
	volume = {7},
	issn = {1949-3045},
	url = {https://ieeexplore.ieee.org/abstract/document/7302562},
	doi = {10.1109/TAFFC.2015.2493525},
	abstract = {Manual annotations and transcriptions have an ever-increasing importance in areas such as behavioral signal processing, image processing, computer vision, and speech signal processing. Conventionally, this metadata has been collected through manual annotations by experts. With the advent of crowdsourcing services, the scientific community has begun to crowdsource many tasks that researchers deem tedious, but can be easily completed by many human annotators. While crowdsourcing is a cheaper and more efficient approach, the quality of the annotations becomes a limitation in many cases. This paper investigates the use of reference sets with predetermined ground-truth to monitor annotators' accuracy and fatigue, all in real-time. The reference set includes evaluations that are identical in form to the relevant questions that are collected, so annotators are blind to whether or not they are being graded on performance on a specific question. We explore these ideas on the emotional annotation of the MSP-IMPROV database. We present promising results which suggest that our system is suitable for collecting accurate annotations.},
	number = {4},
	urldate = {2024-03-12},
	journal = {IEEE Transactions on Affective Computing},
	author = {Burmania, Alec and Parthasarathy, Srinivas and Busso, Carlos},
	month = oct,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Crowdsourcing, Unread},
	pages = {374--388},
}

@inproceedings{ipeirotis_quality_2010,
	address = {New York, NY, USA},
	series = {{HCOMP} '10},
	title = {Quality management on {Amazon} {Mechanical} {Turk}},
	isbn = {978-1-4503-0222-7},
	url = {https://doi.org/10.1145/1837885.1837906},
	doi = {10.1145/1837885.1837906},
	abstract = {Crowdsourcing services, such as Amazon Mechanical Turk, allow for easy distribution of small tasks to a large number of workers. Unfortunately, since manually verifying the quality of the submitted results is hard, malicious workers often take advantage of the verification difficulty and submit answers of low quality. Currently, most requesters rely on redundancy to identify the correct answers. However, redundancy is not a panacea. Massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. Therefore, we need techniques that will accurately estimate the quality of the workers, allowing for the rejection and blocking of the low-performing workers and spammers. However, existing techniques cannot separate the true (unrecoverable) error rate from the (recoverable) biases that some workers exhibit. This lack of separation leads to incorrect assessments of a worker's quality. We present algorithms that improve the existing state-of-the-art techniques, enabling the separation of bias and error. Our algorithm generates a scalar score representing the inherent quality of each worker. We illustrate how to incorporate cost-sensitive classification errors in the overall framework and how to seamlessly integrate unsupervised and supervised techniques for inferring the quality of the workers. We present experimental results demonstrating the performance of the proposed algorithm under a variety of settings.},
	urldate = {2024-03-11},
	booktitle = {Proceedings of the {ACM} {SIGKDD} {Workshop} on {Human} {Computation}},
	publisher = {Association for Computing Machinery},
	author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
	month = jul,
	year = {2010},
	keywords = {Crowdsourcing, Quality Control, Unread},
	pages = {64--67},
}

@inproceedings{oleson_programmatic_2011,
	series = {{AAAIWS}'11-11},
	title = {Programmatic gold: targeted and scalable quality assurance in crowdsourcing},
	shorttitle = {Programmatic gold},
	abstract = {Crowdsourcing is an effective tool for scalable data annotation in both research and enterprise contexts. Due to crowdsourcing's open participation model, quality assurance is critical to the success of any project. Present methods rely on EM-style post-processing or manual annotation of large gold standard sets. In this paper we present an automated quality assurance process that is inexpensive and scalable. Our novel process relies on programmatic gold creation to provide targeted training feedback to workers and to prevent common scamming scenarios. We find that it decreases the amount of manual work required to manage crowdsourced labor while improving the overall quality of the results.},
	urldate = {2024-03-11},
	booktitle = {Proceedings of the 11th {AAAI} {Conference} on {Human} {Computation}},
	publisher = {AAAI Press},
	author = {Oleson, David and Sorokin, Alexander and Laughlin, Greg and Hester, Vaughn and Le, John and Biewald, Lukas},
	month = jan,
	year = {2011},
	keywords = {Crowdsourcing, Quality Control, Unread},
	pages = {43--48},
}

@inproceedings{lease_quality_2011,
	title = {On {Quality} {Control} and {Machine} {Learning} in {Crowdsourcing}.},
	abstract = {The advent of crowdsourcing has created a variety of new opportunities for improving upon traditional methods of data collection and annotation. This in turn has created intriguing new opportunities for data-driven machine learning (ML). Convenient access to crowd workers for simple data collection has further generalized to leveraging more arbitrary crowd-based human computation (von Ahn 2005) to supplement automated ML. While new potential applications of crowdsourcing continue to emerge, a variety of practical and sometimes unexpected obstacles have already limited the degree to which its promised potential can be actually realized in practice. This paper considers two particular aspects of crowdsourcing and their interplay, data quality control (QC) and ML, reflecting on where we have been, where we are, and where we might go from here. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.},
	author = {Lease, Matthew},
	month = jan,
	year = {2011},
	keywords = {Survey, Unread},
}

@article{zheng_truth_2017,
	title = {Truth inference in crowdsourcing: is the problem solved?},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {Truth inference in crowdsourcing},
	url = {https://doi.org/10.14778/3055540.3055547},
	doi = {10.14778/3055540.3055547},
	abstract = {Crowdsourcing has emerged as a novel problem-solving paradigm, which facilitates addressing problems that are hard for computers, e.g., entity resolution and sentiment analysis. However, due to the openness of crowdsourcing, workers may yield low-quality answers, and a redundancy-based method is widely employed, which first assigns each task to multiple workers and then infers the correct answer (called truth) for the task based on the answers of the assigned workers. A fundamental problem in this method is Truth Inference, which decides how to effectively infer the truth. Recently, the database community and data mining community independently study this problem and propose various algorithms. However, these algorithms are not compared extensively under the same framework and it is hard for practitioners to select appropriate algorithms. To alleviate this problem, we provide a detailed survey on 17 existing algorithms and perform a comprehensive evaluation using 5 real datasets. We make all codes and datasets public for future research. Through experiments we find that existing algorithms are not stable across different datasets and there is no algorithm that outperforms others consistently. We believe that the truth inference problem is not fully solved, and identify the limitations of existing algorithms and point out promising research directions.},
	number = {5},
	urldate = {2024-03-12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zheng, Yudian and Li, Guoliang and Li, Yuanbing and Shan, Caihua and Cheng, Reynold},
	month = jan,
	year = {2017},
	keywords = {Survey, Unread},
	pages = {541--552},
}

@article{costa_customized_2013,
	title = {Customized crowds and active learning to improve classification},
	volume = {40},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417413004715},
	doi = {10.1016/j.eswa.2013.06.072},
	abstract = {Traditional classification algorithms can be limited in their performance when a specific user is targeted. User preferences, e.g. in recommendation systems, constitute a challenge for learning algorithms. Additionally, in recent years user’s interaction through crowdsourcing has drawn significant interest, although its use in learning settings is still underused. In this work we focus on an active strategy that uses crowd-based non-expert information to appropriately tackle the problem of capturing the drift between user preferences in a recommendation system. The proposed method combines two main ideas: to apply active strategies for adaptation to each user; to implement crowdsourcing to avoid excessive user feedback. A similitude technique is put forward to optimize the choice of the more appropriate similitude-wise crowd, under the guidance of basic user feedback. The proposed active learning framework allows non-experts classification performed by crowds to be used to define the user profile, mitigating the labeling effort normally requested to the user. The framework is designed to be generic and suitable to be applied to different scenarios, whilst customizable for each specific user. A case study on humor classification scenario is used to demonstrate experimentally that the approach can improve baseline active results.},
	number = {18},
	urldate = {2024-03-12},
	journal = {Expert Systems with Applications},
	author = {Costa, Joana and Silva, Catarina and Antunes, Mário and Ribeiro, Bernardete},
	month = dec,
	year = {2013},
	keywords = {Active learning, Crowdsourcing, Unread},
	pages = {7212--7219},
}

@article{sheng_machine_2019,
	title = {Machine {Learning} with {Crowdsourcing}: {A} {Brief} {Summary} of the {Past} {Research} and {Future} {Directions}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Machine {Learning} with {Crowdsourcing}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5059},
	doi = {10.1609/aaai.v33i01.33019837},
	abstract = {With crowdsourcing systems, labels can be obtained with low cost, which facilitates the creation of training sets for prediction model learning. However, the labels obtained from crowdsourcing are often imperfect, which brings great challenges in model learning. Since 2008, the machine learning community has noticed the great opportunities brought by crowdsourcing and has developed a large number of techniques to deal with inaccuracy, randomness, and uncertainty issues when learning with crowdsourcing. This paper summarizes the technical progress in this field during past eleven years. We focus on two fundamental issues: the data (label) quality and the prediction model quality. For data quality, we summarize ground truth inference methods and some machine learning based methods to further improve data quality. For the prediction model quality, we summarize several learning paradigms developed under the crowdsourcing scenario. Finally, we further discuss several promising future research directions to attract researchers to make contributions in crowdsourcing.},
	language = {en},
	number = {01},
	urldate = {2024-03-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Sheng, Victor S. and Zhang, Jing},
	month = jul,
	year = {2019},
	note = {Number: 01},
	keywords = {Crowdsourcing, Survey, Unread},
	pages = {9837--9843},
}

@article{nassar_overview_2019,
	title = {Overview of the crowdsourcing process},
	volume = {60},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-018-1235-5},
	doi = {10.1007/s10115-018-1235-5},
	abstract = {A decade ago, the crowdsourcing term was first coined and used to represent a method for expressing the wisdom of the crowd in accomplishing two types of tasks. One type includes tasks that need human intelligence rather than machines, and the other type covers those tasks that can be accomplished with a higher time and cost efficiency using the crowd rather than employing experts. The crowdsourcing process contains five modules: The first is designing incentives to mobilize the crowd to do the required task. This step is followed by four modules for collecting and assuring quality and then verifying and aggregating the received information. The verification and quality control can be done for the tasks, collected data and the participants by having more participants answer the same question or accepting answers only from experts to avoid errors from unreliable participants. Methods of discovering topic experts are utilized to discover reliable candidates in the crowd who have relevant experience in the discussed topic. Expert discovery reduces the number of needed participants per question which reduces the overall cost. This work summarizes and reviews the methods used to accomplish each processing step. Yet, choosing a specific method remains application dependent.},
	language = {en},
	number = {1},
	urldate = {2024-03-12},
	journal = {Knowledge and Information Systems},
	author = {Nassar, Lobna and Karray, Fakhri},
	month = jul,
	year = {2019},
	keywords = {Crowdsourcing, Survey, Unread},
	pages = {1--24},
}

@article{daniel_quality_2018,
	title = {Quality {Control} in {Crowdsourcing}: {A} {Survey} of {Quality} {Attributes}, {Assessment} {Techniques}, and {Assurance} {Actions}},
	volume = {51},
	issn = {0360-0300},
	shorttitle = {Quality {Control} in {Crowdsourcing}},
	url = {https://doi.org/10.1145/3148148},
	doi = {10.1145/3148148},
	abstract = {Crowdsourcing enables one to leverage on the intelligence and wisdom of potentially large groups of individuals toward solving problems. Common problems approached with crowdsourcing are labeling images, translating or transcribing text, providing opinions or ideas, and similar—all tasks that computers are not good at or where they may even fail altogether. The introduction of humans into computations and/or everyday work, however, also poses critical, novel challenges in terms of quality control, as the crowd is typically composed of people with unknown and very diverse abilities, skills, interests, personal objectives, and technological resources. This survey studies quality in the context of crowdsourcing along several dimensions, so as to define and characterize it and to understand the current state of the art. Specifically, this survey derives a quality model for crowdsourcing tasks, identifies the methods and techniques that can be used to assess the attributes of the model, and the actions and strategies that help prevent and mitigate quality problems. An analysis of how these features are supported by the state of the art further identifies open issues and informs an outlook on hot future research directions.},
	number = {1},
	urldate = {2024-03-12},
	journal = {ACM Computing Surveys},
	author = {Daniel, Florian and Kucherbaev, Pavel and Cappiello, Cinzia and Benatallah, Boualem and Allahbakhsh, Mohammad},
	month = jan,
	year = {2018},
	keywords = {Crowdsourcing, Survey, Unread},
	pages = {7:1--7:40},
}

@article{yan_active_nodate,
	title = {Active {Learning} from {Crowds}},
	abstract = {Obtaining labels can be expensive or timeconsuming, but unlabeled data is often abundant and easier to obtain. Most learning tasks can be made more efﬁcient, in terms of labeling cost, by intelligently choosing speciﬁc unlabeled instances to be labeled by an oracle. The general problem of optimally choosing these instances is known as active learning. As it is usually set in the context of supervised learning, active learning relies on a single oracle playing the role of a teacher. We focus on the multiple annotator scenario where an oracle, who knows the ground truth, no longer exists; instead, multiple labelers, with varying expertise, are available for querying. This paradigm posits new challenges to the active learning scenario. We can now ask which data sample should be labeled next and which annotator should be queried to beneﬁt our learning model the most. In this paper, we employ a probabilistic model for learning from multiple annotators that can also learn the annotator expertise even when their expertise may not be consistently accurate across the task domain. We then focus on providing a criterion and formulation that allows us to select both a sample and the annotator/s to query the labels from.},
	language = {en},
	author = {Yan, Yan and Rosales, Rómer and Fung, Glenn and Dy, Jennifer G},
	keywords = {Active learning, Crowdsourcing, Unread},
}

@article{zhang_active_2015,
	title = {Active {Learning} {With} {Imbalanced} {Multiple} {Noisy} {Labeling}},
	volume = {45},
	issn = {2168-2275},
	url = {https://ieeexplore.ieee.org/abstract/document/6878424},
	doi = {10.1109/TCYB.2014.2344674},
	abstract = {With crowdsourcing systems, it is easy to collect multiple noisy labels for the same object for supervised learning. This dynamic annotation procedure fits the active learning perspective and accompanies the imbalanced multiple noisy labeling problem. This paper proposes a novel active learning framework with multiple imperfect annotators involved in crowdsourcing systems. The framework contains two core procedures: label integration and instance selection. In the label integration procedure, a positive label threshold (PLAT) algorithm is introduced to induce the class membership from the multiple noisy label set of each instance in a training set. PLAT solves the imbalanced labeling problem by dynamically adjusting the threshold for determining the class membership of an example. Furthermore, three novel instance selection strategies are proposed to adapt PLAT for improving the learning performance. These strategies are respectively based on the uncertainty derived from the multiple labels, the uncertainty derived from the learned model, and the combination method (CFI). Experimental results on 12 datasets with different underlying class distributions demonstrate that the three novel instance selection strategies significantly improve the learning performance, and CFI has the best performance when labeling behaviors exhibit different levels of imbalance in crowdsourcing systems. We also apply our methods to a real-world scenario, obtaining noisy labels from Amazon Mechanical Turk, and show that our proposed strategies achieve very high performance.},
	number = {5},
	urldate = {2024-03-11},
	journal = {IEEE Transactions on Cybernetics},
	author = {Zhang, Jing and Wu, Xindong and Shengs, Victor S.},
	month = may,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Cybernetics},
	keywords = {Active learning, Unread},
	pages = {1095--1107},
}

@inproceedings{zhao_incremental_2011,
	title = {Incremental {Relabeling} for {Active} {Learning} with {Noisy} {Crowdsourced} {Annotations}},
	url = {https://ieeexplore.ieee.org/abstract/document/6113206},
	doi = {10.1109/PASSAT/SocialCom.2011.193},
	abstract = {Crowd sourcing has become an popular approach for annotating the large quantities of data required to train machine learning algorithms. However, obtaining labels in this manner poses two important challenges. First, naively labeling all of the data can be prohibitively expensive. Second, a significant fraction of the annotations can be incorrect due to carelessness or limited domain expertise of crowd sourced workers. Active learning provides a natural formulation to address the former issue by affordably selecting an appropriate subset of instances to label. Unfortunately, most active learning strategies are myopic and sensitive to label noise, which leads to poorly trained classifiers. We propose an active learning method that is specifically designed to be robust to such noise. We present an application of our technique in the domain of activity recognition for eldercare and validate the proposed approach using both simulated and real-world experiments using Amazon Mechanical Turk.},
	urldate = {2024-03-11},
	booktitle = {2011 {IEEE} {Third} {International} {Conference} on {Privacy}, {Security}, {Risk} and {Trust} and 2011 {IEEE} {Third} {International} {Conference} on {Social} {Computing}},
	author = {Zhao, Liyue and Sukthankar, Gita and Sukthankar, Rahul},
	month = oct,
	year = {2011},
	keywords = {Active learning, Crowdsourcing, Unread},
	pages = {728--733},
}

@inproceedings{welinder_online_2010,
	title = {Online crowdsourcing: {Rating} annotators and obtaining cost-effective labels},
	shorttitle = {Online crowdsourcing},
	url = {https://ieeexplore.ieee.org/abstract/document/5543189},
	doi = {10.1109/CVPRW.2010.5543189},
	abstract = {Labeling large datasets has become faster, cheaper, and easier with the advent of crowdsourcing services like Amazon Mechanical Turk. How can one trust the labels obtained from such services? We propose a model of the labeling process which includes label uncertainty, as well a multi-dimensional measure of the annotators' ability. From the model we derive an online algorithm that estimates the most likely value of the labels and the annotator abilities. It finds and prioritizes experts when requesting labels, and actively excludes unreliable annotators. Based on labels already obtained, it dynamically chooses which images will be labeled next, and how many labels to request in order to achieve a desired level of confidence. Our algorithm is general and can handle binary, multi-valued, and continuous annotations (e.g. bounding boxes). Experiments on a dataset containing more than 50,000 labels show that our algorithm reduces the number of labels required, and thus the total cost of labeling, by a large factor while keeping error rates low on a variety of datasets.},
	urldate = {2024-03-11},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Workshops}},
	author = {Welinder, Peter and Perona, Pietro},
	month = jun,
	year = {2010},
	note = {ISSN: 2160-7516},
	keywords = {Crowdsourcing, Unread},
	pages = {25--32},
}

@article{zhang_learning_2016,
	title = {Learning from crowdsourced labeled data: a survey},
	volume = {46},
	issn = {1573-7462},
	shorttitle = {Learning from crowdsourced labeled data},
	url = {https://doi.org/10.1007/s10462-016-9491-9},
	doi = {10.1007/s10462-016-9491-9},
	abstract = {With the rapid growing of crowdsourcing systems, quite a few applications based on a supervised learning paradigm can easily obtain massive labeled data at a relatively low cost. However, due to the variable uncertainty of crowdsourced labelers, learning procedures face great challenges. Thus, improving the qualities of labels and learning models plays a key role in learning from the crowdsourced labeled data. In this survey, we first introduce the basic concepts of the qualities of labels and learning models. Then, by reviewing recently proposed models and algorithms on ground truth inference and learning models, we analyze connections and distinctions among these techniques as well as clarify the level of the progress of related researches. In order to facilitate the studies in this field, we also introduce open accessible real-world data sets collected from crowdsourcing systems and open source libraries and tools. Finally, some potential issues for future studies are discussed.},
	language = {en},
	number = {4},
	urldate = {2024-03-11},
	journal = {Artificial Intelligence Review},
	author = {Zhang, Jing and Wu, Xindong and Sheng, Victor S.},
	month = dec,
	year = {2016},
	keywords = {Crowdsourcing, Survey, Unread},
	pages = {543--576},
}

@inproceedings{nowak_how_2010,
	address = {New York, NY, USA},
	series = {{MIR} '10},
	title = {How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation},
	isbn = {978-1-60558-815-5},
	shorttitle = {How reliable are annotations via crowdsourcing},
	url = {https://doi.org/10.1145/1743384.1743478},
	doi = {10.1145/1743384.1743478},
	abstract = {The creation of golden standard datasets is a costly business. Optimally more than one judgment per document is obtained to ensure a high quality on annotations. In this context, we explore how much annotations from experts differ from each other, how different sets of annotations influence the ranking of systems and if these annotations can be obtained with a crowdsourcing approach. This study is applied to annotations of images with multiple concepts. A subset of the images employed in the latest ImageCLEF Photo Annotation competition was manually annotated by expert annotators and non-experts with Mechanical Turk. The inter-annotator agreement is computed at an image-based and concept-based level using majority vote, accuracy and kappa statistics. Further, the Kendall τ and Kolmogorov-Smirnov correlation test is used to compare the ranking of systems regarding different ground-truths and different evaluation measures in a benchmark scenario. Results show that while the agreement between experts and non-experts varies depending on the measure used, its influence on the ranked lists of the systems is rather small. To sum up, the majority vote applied to generate one annotation set out of several opinions, is able to filter noisy judgments of non-experts to some extent. The resulting annotation set is of comparable quality to the annotations of experts.},
	urldate = {2024-03-11},
	booktitle = {Proceedings of the international conference on {Multimedia} information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Nowak, Stefanie and Rüger, Stefan},
	month = mar,
	year = {2010},
	keywords = {Crowdsourcing, Survey, Unread},
	pages = {557--566},
}

@inproceedings{raykar_ranking_2011,
	title = {Ranking annotators for crowdsourced labeling tasks},
	volume = {24},
	url = {https://proceedings.neurips.cc/paper_files/paper/2011/hash/24146db4eb48c718b84cae0a0799dcfc-Abstract.html},
	abstract = {With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Often we have low quality annotators or spammers--annotators who assign labels randomly (e.g., without actually looking at the instance). Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels. In this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators---with the spammers having a score close to zero and the good annotators having a high score close to one.},
	urldate = {2024-03-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Raykar, Vikas C and Yu, Shipeng},
	year = {2011},
	keywords = {Crowdsourcing, Unread},
}

@article{nicholson_label_2016,
	title = {Label noise correction and application in crowdsourcing},
	volume = {66},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741630481X},
	doi = {10.1016/j.eswa.2016.09.003},
	abstract = {The important task of correcting label noise is addressed infrequently in literature. The difficulty of developing a robust label correction algorithm leads to this silence concerning label correction. To break the silence, we propose two algorithms to correct label noise. One utilizes self-training to re-label noise, called Self-Training Correction (STC). Another is a clustering-based method, which groups instances together to infer their ground-truth labels, called Cluster-based Correction (CC). We also adapt an algorithm from previous work, a consensus-based method called Polishing that consults with an ensemble of classifiers to change the values of attributes and labels. We simplify Polishing such that it only alters labels of instances, and call it Polishing Labels (PL). We experimentally compare our novel methods with Polishing Labels by examining their improvements on the label qualities, model qualities, and AUC metrics of binary and multi-class data sets under different noise levels. Our experimental results demonstrate that CC significantly improves label qualities, model qualities, and AUC metrics consistently. We further investigate how these three noise correction algorithms improve the data quality, in terms of label accuracy, in the context of image labeling in crowdsourcing. First, we look at three consensus methods for inferring a ground-truth label from the multiple noisy labels obtained from crowdsourcing, i.e., Majority Voting (MV), Dawid Skene (DS), and KOS. We then apply the three noise correction methods to correct labels inferred by these consensus methods. Our experimental results show that the noise correction methods improve the labeling quality significantly. As an overall result of our experiments, we conclude that CC performs the best. Our research has illustrated the viability of implementing noise correction as another line of defense against labeling error, especially in a crowdsourcing setting. Furthermore, it presents the feasibility of the automation of an otherwise manual process of analyzing a data set, and correcting and cleaning the instances, an expensive and time-consuming task.},
	urldate = {2024-03-11},
	journal = {Expert Systems with Applications},
	author = {Nicholson, Bryce and Sheng, Victor S. and Zhang, Jing},
	month = dec,
	year = {2016},
	keywords = {Unread},
	pages = {149--162},
}

@article{xu_blockchain-based_2022,
	title = {Blockchain-{Based} {Crowdsourcing} {Makes} {Training} {Dataset} of {Machine} {Learning} {No} {Longer} {Be} in {Short} {Supply}},
	volume = {2022},
	issn = {1530-8669},
	url = {https://www.hindawi.com/journals/wcmc/2022/7033626/},
	doi = {10.1155/2022/7033626},
	abstract = {Recently, machine learning has become popular in various fields like healthcare, smart transportation, network, and big data. However, the labelled training dataset, which is one of the most core of machine learning, cannot meet the requirements of quantity, quality, and diversity due to the limitation of data sources. Crowdsourcing systems based on mobile computing seem to address the bottlenecks faced by machine learning due to their unique advantages; i.e., crowdsourcing can make professional and nonprofessional participate in the collection and annotation process, which can greatly improve the quantity of the training dataset. Additionally, distributed blockchain technology can be embedded into crowdsourcing systems to make it transparent, secure, traceable, and decentralized. Moreover, truth discovery algorithm can improve the accuracy of annotation. Reasonable incentive mechanism will attract many workers to provide plenty of dataset. In this paper, we review studies applying mobile crowdsourcing to training dataset collection and annotation. In addition, after reviewing researches on blockchain or incentive mechanism, we propose a new possible combination of machine learning and crowdsourcing systems.},
	language = {en},
	urldate = {2024-03-09},
	journal = {Wireless Communications and Mobile Computing},
	author = {Xu, Haitao and Wei, Wei and Qi, Yong and Qi, Saiyu},
	month = jul,
	year = {2022},
	note = {Publisher: Hindawi},
	keywords = {Blockchain, Crowdsourcing, Survey},
	pages = {e7033626},
}

@article{hossain_crowdsourcing_2015,
	title = {Crowdsourcing: a comprehensive literature review},
	volume = {8},
	issn = {1753-8297},
	shorttitle = {Crowdsourcing},
	url = {https://doi.org/10.1108/SO-12-2014-0029},
	doi = {10.1108/SO-12-2014-0029},
	abstract = {Purpose The purpose of this paper is to explore the development of crowdsourcing literature. Design/methodology/approach This study is a comprehensive review of 346 articles on crowdsourcing. Both statistical and contents analyses were conducted in this paper. Findings ISI listed journal articles, non-ISI listed journal articles and conference articles have had nearly the same contribution in crowdsourcing literature. Articles published in non-ISI listed journals have had an essential role in the initial theory development related to crowdsourcing. Scholars from the USA have authored approximately the same number of articles as scholars from all the European countries combined. Scholars from developing countries have been more relatively active in authoring conference articles than journal articles. Only very recently, top-tier journals have engaged in publishing on crowdsourcing. Crowdsourcing has proven to be beneficial in many tasks, but the extant literature does not give much help to practitioners in capturing value from crowdsourcing. Despite understanding that the motivations of crowds are crucial when planning crowdsourcing activities, the various motivations in different contexts have not been explored sufficiently. A major concern has been the quality and accuracy of information that has been gathered through crowdsourcing. Crowdsourcing bears a lot of unused potential. For example, it can increase employment opportunities to low-income people in developing countries. On the other hand, more should be known of fair ways to organize crowdsourcing so that solution seekers do not get a chance to exploit individuals committing to provide solutions. Research limitations/implications The literature included in the study is extensive, but an all-inclusive search for articles was limited to only nine selected publishers. However, in addition to the articles retrieved from the nine selected publishers, 52 highly cited articles were also included from other publishers. Practical implications Crowdsourcing has much unused potential, and the use of crowdsourcing is increasing rapidly. The study provides a thematic review of various applications of crowdsourcing. Originality/value The study is the first of its kind to explore the development of crowdsourcing literature, discussing the loci and foci of extant articles and listing applications of crowdsourcing. Successful applications of crowdsourcing include idea generation, microtasking, citizen science, public participation, wikies, open source software and citizen journalism.},
	number = {1},
	urldate = {2024-01-08},
	journal = {Strategic Outsourcing: An International Journal},
	author = {Hossain, Mokter and Kauranen, Ilkka},
	month = jan,
	year = {2015},
	note = {Publisher: Emerald Group Publishing Limited},
	keywords = {Crowdsourcing, Survey, Unread},
	pages = {2--22},
}

@article{ren_survey_2021,
	title = {A {Survey} of {Deep} {Active} {Learning}},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3472291},
	doi = {10.1145/3472291},
	abstract = {Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due. It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
	number = {9},
	urldate = {2024-01-08},
	journal = {ACM Computing Surveys},
	author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
	month = oct,
	year = {2021},
	keywords = {Active learning, Survey, Unread},
	pages = {180:1--180:40},
}

@article{sayin_review_2021,
	title = {A review and experimental analysis of active learning over crowdsourced data},
	volume = {54},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-021-10021-3},
	doi = {10.1007/s10462-021-10021-3},
	abstract = {Training data creation is increasingly a key bottleneck for developing machine learning, especially for deep learning systems. Active learning provides a cost-effective means for creating training data by selecting the most informative instances for labeling. Labels in real applications are often collected from crowdsourcing, which engages online crowds for data labeling at scale. Despite the importance of using crowdsourced data in the active learning process, an analysis of how the existing active learning approaches behave over crowdsourced data is currently missing. This paper aims to fill this gap by reviewing the existing active learning approaches and then testing a set of benchmarking ones on crowdsourced datasets. We provide a comprehensive and systematic survey of the recent research on active learning in the hybrid human–machine classification setting, where crowd workers contribute labels (often noisy) to either directly classify data instances or to train machine learning models. We identify three categories of state of the art active learning methods according to whether and how predefined queries employed for data sampling, namely fixed-strategy approaches, dynamic-strategy approaches, and strategy-free approaches. We then conduct an empirical study on their cost-effectiveness, showing that the performance of the existing active learning approaches is affected by many factors in hybrid classification contexts, such as the noise level of data, label fusion technique used, and the specific characteristics of the task. Finally, we discuss challenges and identify potential directions to design active learning strategies for hybrid classification problems.},
	language = {en},
	number = {7},
	urldate = {2024-01-22},
	journal = {Artificial Intelligence Review},
	author = {Sayin, Burcu and Krivosheev, Evgeny and Yang, Jie and Passerini, Andrea and Casati, Fabio},
	month = oct,
	year = {2021},
	keywords = {Active learning, Crowdsourcing, Important, Survey},
	pages = {5283--5305},
}

@inproceedings{wang_overview_2018,
	title = {An {Overview} of {Smart} {Contract}: {Architecture}, {Applications}, and {Future} {Trends}},
	shorttitle = {An {Overview} of {Smart} {Contract}},
	url = {https://ieeexplore.ieee.org/abstract/document/8500488},
	doi = {10.1109/IVS.2018.8500488},
	abstract = {With the rapid development of cryptocurrency and its underlying blockchain technologies, platforms such as Ethereum and Hyperledger began to support various types of smart contracts. Smart contracts are computer protocols intended to digitally facilitate, verify, or enforce the negotiation or performance of a contract. Smart contracts have broad range of applications, such as financial services, prediction markets and Internet of Things (IoT), etc. However, there are still many challenges such as security issues and privacy disclosure that await future research. In this paper, we present a comprehensive overview on blockchain powered smart contracts. First, we give a systematic introduction for smart contracts, including the basic framework, operating mechanisms, platforms and programming languages. Second, application scenarios and existing challenges are discussed. Finally, we describe the recent advances of smart contract and present its future development trends, e.g., parallel blockchain. This paper is aimed at providing helpful guidance and reference for future research efforts.},
	urldate = {2024-01-09},
	booktitle = {2018 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Wang, Shuai and Yuan, Yong and Wang, Xiao and Li, Juanjuan and Qin, Rui and Wang, Fei-Yue},
	month = jun,
	year = {2018},
	note = {ISSN: 1931-0587},
	keywords = {Blockchain, Survey},
	pages = {108--113},
}

@misc{noauthor_rules_nodate,
	title = {Rules of {Crowdsourcing}: {Models}, {Issues}, and {Systems} of {Control}: {Information} {Systems} {Management}: {Vol} 30, {No} 1},
	url = {https://www.tandfonline.com/doi/abs/10.1080/10580530.2013.739883},
	urldate = {2024-02-02},
	keywords = {Crowdsourcing, Survey, Unread},
}

@inproceedings{karger_efficient_2013,
	address = {New York, NY, USA},
	series = {{SIGMETRICS} '13},
	title = {Efficient crowdsourcing for multi-class labeling},
	isbn = {978-1-4503-1900-3},
	url = {https://doi.org/10.1145/2465529.2465761},
	doi = {10.1145/2465529.2465761},
	abstract = {Crowdsourcing systems like Amazon's Mechanical Turk have emerged as an effective large-scale human-powered platform for performing tasks in domains such as image classification, data entry, recommendation, and proofreading. Since workers are low-paid (a few cents per task) and tasks performed are monotonous, the answers obtained are noisy and hence unreliable. To obtain reliable estimates, it is essential to utilize appropriate inference algorithms (e.g. Majority voting) coupled with structured redundancy through task assignment. Our goal is to obtain the best possible trade-off between reliability and redundancy. In this paper, we consider a general probabilistic model for noisy observations for crowd-sourcing systems and pose the problem of minimizing the total price (i.e. redundancy) that must be paid to achieve a target overall reliability. Concretely, we show that it is possible to obtain an answer to each task correctly with probability 1-ε as long as the redundancy per task is O((K/q) log (K/ε)), where each task can have any of the \$K\$ distinct answers equally likely, q is the crowd-quality parameter that is defined through a probabilistic model. Further, effectively this is the best possible redundancy-accuracy trade-off any system design can achieve. Such a single-parameter crisp characterization of the (order-)optimal trade-off between redundancy and reliability has various useful operational consequences. Further, we analyze the robustness of our approach in the presence of adversarial workers and provide a bound on their influence on the redundancy-accuracy trade-off. Unlike recent prior work [GKM11, KOS11, KOS11], our result applies to non-binary (i.e. K{\textgreater}2) tasks. In effect, we utilize algorithms for binary tasks (with inhomogeneous error model unlike that in [GKM11, KOS11, KOS11]) as key subroutine to obtain answers for K-ary tasks. Technically, the algorithm is based on low-rank approximation of weighted adjacency matrix for a random regular bipartite graph, weighted according to the answers provided by the workers.},
	urldate = {2024-02-29},
	booktitle = {Proceedings of the {ACM} {SIGMETRICS}/international conference on {Measurement} and modeling of computer systems},
	publisher = {Association for Computing Machinery},
	author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
	month = jun,
	year = {2013},
	keywords = {Crowdsourcing, Important},
	pages = {81--92},
}

@article{lease_quality_nodate,
	title = {On {Quality} {Control} and {Machine} {Learning} in {Crowdsourcing}},
	abstract = {The advent of crowdsourcing has created a variety of new opportunities for improving upon traditional methods of data collection and annotation. This in turn has created intriguing new opportunities for data-driven machine learning (ML). Convenient access to crowd workers for simple data collection has further generalized to leveraging more arbitrary crowd-based human computation (von Ahn 2005) to supplement automated ML. While new potential applications of crowdsourcing continue to emerge, a variety of practical and sometimes unexpected obstacles have already limited the degree to which its promised potential can be actually realized in practice. This paper considers two particular aspects of crowdsourcing and their interplay, data quality control (QC) and ML, reﬂecting on where we have been, where we are, and where we might go from here.},
	language = {en},
	author = {Lease, Matthew},
	keywords = {Crowdsourcing, Important},
}

@article{safran_real-time_2017,
	title = {Real-time recommendation algorithms for crowdsourcing systems},
	volume = {13},
	issn = {2210-8327},
	url = {https://www.sciencedirect.com/science/article/pii/S2210832716000028},
	doi = {10.1016/j.aci.2016.01.001},
	abstract = {Crowdsourcing has become a promising paradigm for solving tasks that are beyond the capabilities of machines alone via outsourcing tasks to online crowds of people. Both requesters and workers in crowdsourcing systems confront a flood of data coming along with the vast amount of tasks. Fast, on-the-fly recommendation of tasks to workers and workers to requesters is becoming critical for crowdsourcing systems. Traditional recommendation algorithms such as collaborative filtering no longer work satisfactorily because of the unprecedented data flow and the on-the-fly nature of the tasks in crowdsourcing systems. A pressing need for real-time recommendations has emerged in crowdsourcing systems: on the one hand, workers want effective recommendation of the top-k most suitable tasks with regard to their skills and preferences, and on the other hand, requesters want reliable recommendation of the top-k best workers for their tasks in terms of workers’ qualifications and accountability. In this article, we propose two real-time recommendation algorithms for crowdsourcing systems: (1) TOP-K-T that computes the top-k most suitable tasks for a given worker and (2) TOP-K-W that computes the top-k best workers to a requester with regard to a given task. Experimental study has shown the efficacy of both algorithms.},
	number = {1},
	urldate = {2024-01-24},
	journal = {Applied Computing and Informatics},
	author = {Safran, Mejdl and Che, Dunren},
	month = jan,
	year = {2017},
	keywords = {Crowdsourcing},
	pages = {47--56},
}

@inproceedings{zhang_incentivize_2015,
	title = {Incentivize crowd labeling under budget constraint},
	url = {https://ieeexplore.ieee.org/abstract/document/7218674},
	doi = {10.1109/INFOCOM.2015.7218674},
	abstract = {Crowdsourcing systems allocate tasks to a group of workers over the Internet, which have become an effective paradigm for human-powered problem solving such as image classification, optical character recognition and proofreading. In this paper, we focus on incentivizing crowd workers to label a set of binary tasks under strict budget constraint. We properly profile the tasks' difficulty levels and workers' quality in crowdsourcing systems, where the collected labels are aggregated with sequential Bayesian approach. To stimulate workers to undertake crowd labeling tasks, the interaction between workers and the platform is modeled as a reverse auction. We reveal that the platform utility maximization could be intractable, for which an incentive mechanism that determines the winning bid and payments with polynomial-time computation complexity is developed. Moreover, we theoretically prove that our mechanism is truthful, individually rational and budget feasible. Through extensive simulations, we demonstrate that our mechanism utilizes budget efficiently to achieve high platform utility with polynomial computation complexity.},
	urldate = {2024-03-01},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Communications} ({INFOCOM})},
	author = {Zhang, Qi and Wen, Yutian and Tian, Xiaohua and Gan, Xiaoying and Wang, Xinbing},
	month = apr,
	year = {2015},
	note = {ISSN: 0743-166X},
	keywords = {Crowdsourcing},
	pages = {2812--2820},
}

@inproceedings{shah_approval_2015,
	title = {Approval {Voting} and {Incentives} in {Crowdsourcing}},
	url = {https://proceedings.mlr.press/v37/shaha15.html},
	abstract = {The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a ("strictly proper") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.},
	language = {en},
	urldate = {2024-03-01},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shah, Nihar and Zhou, Dengyong and Peres, Yuval},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	keywords = {Crowdsourcing, Unread},
	pages = {10--19},
}

@inproceedings{zhang_truthful_2015,
	title = {Truthful incentive mechanisms for crowdsourcing},
	url = {https://ieeexplore.ieee.org/abstract/document/7218676},
	doi = {10.1109/INFOCOM.2015.7218676},
	abstract = {With the prosperity of smart devices, crowdsourcing has emerged as a new computing/networking paradigm. Through the crowdsourcing platform, service requesters can buy service from service providers. An important component of crowdsourcing is its incentive mechanism. We study three models of crowdsourcing, which involve cooperation and competition among the service providers. Our simplest model generalizes the well-known user-centric model studied in a recent Mobicom paper. We design an incentive mechanism for each of the three models, and prove that these incentive mechanisms are individually rational, budget-balanced, computationally efficient, and truthful.},
	urldate = {2024-03-01},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Communications} ({INFOCOM})},
	author = {Zhang, Xiang and Xue, Guoliang and Yu, Ruozhou and Yang, Dejun and Tang, Jian},
	month = apr,
	year = {2015},
	note = {ISSN: 0743-166X},
	keywords = {Crowdsourcing, Unread},
	pages = {2830--2838},
}

@inproceedings{georgescu_when_2014,
	address = {New York, NY, USA},
	series = {{WIMS} '14},
	title = {When in {Doubt} {Ask} the {Crowd}: {Employing} {Crowdsourcing} for {Active} {Learning}},
	isbn = {978-1-4503-2538-7},
	shorttitle = {When in {Doubt} {Ask} the {Crowd}},
	url = {https://doi.org/10.1145/2611040.2611047},
	doi = {10.1145/2611040.2611047},
	abstract = {Crowdsourcing has become ubiquitous in machine learning as a cost effective method to gather training labels. In this paper we examine the challenges that appear when employing crowdsourcing for active learning, in an integrated environment where an automatic method and human labelers work together towards improving their performance at a certain task. By using Active Learning techniques on crowd-labeled data, we optimize the performance of the automatic method towards better accuracy, while keeping the costs low by gathering data on demand. In order to verify our proposed methods, we apply them to the task of deduplication of publications in a digital library by examining metadata. We investigate the problems created by noisy labels produced by the crowd and explore methods to aggregate them. We analyze how different automatic methods are affected by the quantity and quality of the allocated resources as well as the instance selection strategies for each active learning round, aiming towards attaining a balance between cost and performance.},
	urldate = {2024-02-19},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Web} {Intelligence}, {Mining} and {Semantics} ({WIMS14})},
	publisher = {Association for Computing Machinery},
	author = {Georgescu, Mihai and Pham, Dang Duc and Firan, Claudiu S. and Gadiraju, Ujwal and Nejdl, Wolfgang},
	month = jun,
	year = {2014},
	keywords = {Active learning, Crowdsourcing},
	pages = {1--12},
}

@article{yu_active_2021,
	title = {Active {Multilabel} {Crowd} {Consensus}},
	volume = {32},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/9069472},
	doi = {10.1109/TNNLS.2020.2984729},
	abstract = {Crowdsourcing is an economic and efficient strategy aimed at collecting annotations of data through an online platform. Crowd workers with different expertise are paid for their service, and the task requester usually has a limited budget. How to collect reliable annotations for multilabel data and how to compute the consensus within budget are an interesting and challenging, but rarely studied, problem. In this article, we propose a novel approach to accomplish active multilabel crowd consensus (AMCC). AMCC accounts for the commonality and individuality of workers and assumes that workers can be organized into different groups. Each group includes a set of workers who share a similar annotation behavior and label correlations. To achieve an effective multilabel consensus, AMCC models workers' annotations via a linear combination of commonality and individuality and reduces the impact of unreliable workers by assigning smaller weights to their groups. To collect reliable annotations with reduced cost, AMCC introduces an active crowdsourcing learning strategy that selects sample-label-worker triplets. In a triplet, the selected sample and label are the most informative for the consensus model, and the selected worker can reliably annotate the sample at a low cost. Our experimental results on multilabel data sets demonstrate the advantages of AMCC over state-of-the-art solutions on computing crowd consensus and on reducing the budget by choosing cost-effective triplets.},
	number = {4},
	urldate = {2024-03-01},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Yu, Guoxian and Tu, Jinzheng and Wang, Jun and Domeniconi, Carlotta and Zhang, Xiangliang},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Crowdsourcing, Unread},
	pages = {1448--1459},
}

@article{li_tfcrowd_2021,
	title = {{TFCrowd}: a blockchain-based crowdsourcing framework with enhanced trustworthiness and fairness},
	volume = {2021},
	issn = {1687-1499},
	shorttitle = {{TFCrowd}},
	url = {https://doi.org/10.1186/s13638-021-02040-z},
	doi = {10.1186/s13638-021-02040-z},
	abstract = {Blockchain technology has attracted considerable attention due to the boom of cryptocurrencies and decentralized applications. Among them, the emerging blockchain-based crowdsourcing is a typical paradigm, which gets rid of centralized cloud-servers and leverages smart contracts to realize task recommendation and reward distribution. However, there are still two critical issues yet to be solved urgently. First, malicious evaluation from crowdsourcing requesters will result in honest workers not getting the rewards they deserve even if they have provided valuable solutions. Second, unfair evaluation and reward distribution can lead to low enthusiasm for work. Therefore, the above problems will seriously hinder the development of blockchain-based crowdsourcing platforms. In this paper, we propose a new blockchain-based crowdsourcing framework with enhanced trustworthiness and fairness, named TFCrowd. The core idea of TFCrowd is utilizing a smart contract of blockchain as a trusted authority to fairly evaluate contributions and allocate rewards. To this end, we devise a reputation-based evaluation mechanism to punish the requester who behaves as “false-reporting” and a Shapley value-based method to distribute rewards fairly. By using our proposed schemes, TFCrowd can prevent malicious requesters from making unfair comments and reward honest workers according to their contributions. Extensive simulations and the experiment results demonstrate that TFCrowd can protect the interests of workers and distribute rewards fairly.},
	number = {1},
	urldate = {2024-03-01},
	journal = {EURASIP Journal on Wireless Communications and Networking},
	author = {Li, Chunxiao and Qu, Xidi and Guo, Yu},
	month = aug,
	year = {2021},
	keywords = {Blockchain, Crowdsourcing, Unread},
	pages = {168},
}

@inproceedings{chang_revolt_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Revolt: {Collaborative} {Crowdsourcing} for {Labeling} {Machine} {Learning} {Datasets}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {Revolt},
	url = {https://doi.org/10.1145/3025453.3026044},
	doi = {10.1145/3025453.3026044},
	abstract = {Crowdsourcing provides a scalable and efficient way to construct labeled datasets for training machine learning systems. However, creating comprehensive label guidelines for crowdworkers is often prohibitive even for seemingly simple concepts. Incomplete or ambiguous label guidelines can then result in differing interpretations of concepts and inconsistent labels. Existing approaches for improving label quality, such as worker screening or detection of poor work, are ineffective for this problem and can lead to rejection of honest work and a missed opportunity to capture rich interpretations about data. We introduce Revolt, a collaborative approach that brings ideas from expert annotation workflows to crowd-based labeling. Revolt eliminates the burden of creating detailed label guidelines by harnessing crowd disagreements to identify ambiguous concepts and create rich structures (groups of semantically related items) for post-hoc label decisions. Experiments comparing Revolt to traditional crowdsourced labeling show that Revolt produces high quality labels without requiring label guidelines in turn for an increase in monetary cost. This up front cost, however, is mitigated by Revolt's ability to produce reusable structures that can accommodate a variety of label boundaries without requiring new data to be collected. Further comparisons of Revolt's collaborative and non-collaborative variants show that collaboration reaches higher label accuracy with lower monetary cost.},
	urldate = {2024-02-29},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chang, Joseph Chee and Amershi, Saleema and Kamar, Ece},
	month = may,
	year = {2017},
	keywords = {Crowdsourcing, Unread},
	pages = {2334--2346},
}

@article{tang_semi-supervised_nodate,
	title = {Semi-{Supervised} {Consensus} {Labeling} for {Crowdsourcing}},
	abstract = {Because individual crowd workers often exhibit high variance in annotation accuracy, we often ask multiple crowd workers to label each example to infer a single consensus label. While simple majority vote computes consensus by equally weighting each worker’s vote, weighted voting assigns greater weight to more accurate workers, where accuracy is estimated by inner-annotator agreement (unsupervised) and/or agreement with known expert labels (supervised). In this paper, we investigate the annotation cost vs. consensus accuracy beneﬁt from increasing the amount of expert supervision. To maximize beneﬁt from supervision, we propose a semi-supervised approach which infers consensus labels using both labeled and unlabeled examples. We compare our semi-supervised approach with several existing unsupervised and supervised baselines, evaluating on both synthetic data and Amazon Mechanical Turk data. Results show (a) a very modest amount of supervision can provide signiﬁcant beneﬁt, and (b) consensus accuracy from full supervision with a large amount of labeled data is matched by our semi-supervised approach with much less supervision.},
	language = {en},
	author = {Tang, Wei and Lease, Matthew},
	keywords = {Crowdsourcing},
}

@article{zou_proof--trust_2019,
	title = {A {Proof}-of-{Trust} {Consensus} {Protocol} for {Enhancing} {Accountability} in {Crowdsourcing} {Services}},
	volume = {12},
	issn = {1939-1374},
	url = {https://ieeexplore.ieee.org/abstract/document/8332496},
	doi = {10.1109/TSC.2018.2823705},
	abstract = {Incorporating accountability mechanisms in online services requires effective trust management and immutable, traceable source of truth for transaction evidence. The emergence of the blockchain technology brings in high hopes for fulfilling most of those requirements. However, a major challenge is to find a proper consensus protocol that is applicable to the crowdsourcing services in particular and online services in general. Building upon the idea of using blockchain as the underlying technology to enable tracing transactions for service contracts and dispute arbitration, this paper proposes a novel consensus protocol that is suitable for the crowdsourcing as well as the general online service industry. The new consensus protocol is called “Proof-of-Trust” (PoT) consensus; it selects transaction validators based on the service participants' trust values while leveraging RAFT leader election and Shamir's secret sharing algorithms. The PoT protocol avoids the low throughput and resource intensive pitfalls associated with Bitcoin' s “Proof-of-Work” (PoW) mining, while addressing the scalability issue associated with the traditional Paxos-based and Byzantine Fault Tolerance (BFT)-based algorithms. In addition, it addresses the unfaithful behaviors that cannot be dealt with in the traditional BFT algorithms. The paper demonstrates that our approach can provide a viable accountability solution for the online service industry.},
	number = {3},
	urldate = {2024-02-29},
	journal = {IEEE Transactions on Services Computing},
	author = {Zou, Jun and Ye, Bin and Qu, Lie and Wang, Yan and Orgun, Mehmet A. and Li, Lei},
	month = may,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Services Computing},
	keywords = {Blockchain, Crowdsourcing},
	pages = {429--445},
}

@inproceedings{sharples_blockchain_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Blockchain} and {Kudos}: {A} {Distributed} {System} for {Educational} {Record}, {Reputation} and {Reward}},
	isbn = {978-3-319-45153-4},
	shorttitle = {The {Blockchain} and {Kudos}},
	doi = {10.1007/978-3-319-45153-4_48},
	abstract = {The ‘blockchain’ is the core mechanism for the Bitcoin digital payment system. It embraces a set of inter-related technologies: the blockchain itself as a distributed record of digital events, the distributed consensus method to agree whether a new block is legitimate, automated smart contracts, and the data structure associated with each block. We propose a permanent distributed record of intellectual effort and associated reputational reward, based on the blockchain that instantiates and democratises educational reputation beyond the academic community. We are undertaking initial trials of a private blockchain or storing educational records, drawing also on our previous research into reputation management for educational systems.},
	language = {en},
	booktitle = {Adaptive and {Adaptable} {Learning}},
	publisher = {Springer International Publishing},
	author = {Sharples, Mike and Domingue, John},
	editor = {Verbert, Katrien and Sharples, Mike and Klobučar, Tomaž},
	year = {2016},
	keywords = {Blockchain, Unread},
	pages = {490--496},
}

@article{zhang_consensus_2017,
	title = {Consensus algorithms for biased labeling in crowdsourcing},
	volume = {382-383},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516320515},
	doi = {10.1016/j.ins.2016.12.026},
	abstract = {Although it has become an accepted lay view that when labeling objects through crowdsourcing systems, non-expert annotators often exhibit biases, this argument lacks sufficient evidential observation and systematic empirical study. This paper initially analyzes eight real-world datasets from different domains whose class labels were collected from crowdsourcing systems. Our analyses show that biased labeling is a systematic tendency for binary categorization; in other words, for a large number of annotators, their labeling qualities on the negative class (supposed to be the majority) are significantly greater than are those on the positive class (minority). Therefore, the paper empirically studies the performance of four existing EM-based consensus algorithms, DS, GLAD, RY, and ZenCrowd, on these datasets. Our investigation shows that all of these state-of-the-art algorithms ignore the potential bias characteristics of datasets and perform badly although they model the complexity of the systems. To address the issue of handling biased labeling, the paper further proposes a novel consensus algorithm, namely adaptive weighted majority voting (AWMV), based on the statistical difference between the labeling qualities of the two classes. AWMV utilizes the frequency of positive labels in the multiple noisy label set of each example to obtain a bias rate and then assigns weights derived from the bias rate to negative and positive labels. Comparison results among the five consensus algorithms (AWMV and the four existing) show that the proposed AWMV algorithm has the best overall performance. Finally, this paper notes some potential related topics for future study.},
	urldate = {2024-02-29},
	journal = {Information Sciences},
	author = {Zhang, Jing and Sheng, Victor S. and Li, Qianmu and Wu, Jian and Wu, Xindong},
	month = mar,
	year = {2017},
	keywords = {Crowdsourcing, Unread},
	pages = {254--273},
}

@article{antonopoulos_mastering_nodate,
	title = {Mastering {Ethereum}},
	language = {en},
	author = {Antonopoulos, Andreas M},
}

@misc{danka_modal_2018-1,
	title = {{modAL}: {A} modular active learning framework for {Python}},
	shorttitle = {{modAL}},
	url = {http://arxiv.org/abs/1805.00979},
	doi = {10.48550/arXiv.1805.00979},
	abstract = {modAL is a modular active learning framework for Python, aimed to make active learning research and practice simpler. Its distinguishing features are (i) clear and modular object oriented design (ii) full compatibility with scikit-learn models and workflows. These features make fast prototyping and easy extensibility possible, aiding the development of real-life active learning pipelines and novel algorithms as well. modAL is fully open source, hosted on GitHub at https://github.com/cosmic-cortex/modAL. To assure code quality, extensive unit tests are provided and continuous integration is applied. In addition, a detailed documentation with several tutorials are also available for ease of use. The framework is available in PyPI and distributed under the MIT license.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Danka, Tivadar and Horvath, Peter},
	month = dec,
	year = {2018},
	note = {arXiv:1805.00979 [cs, stat]},
	keywords = {Active learning, Unread},
}

@inproceedings{citovsky_batch_2021,
	title = {Batch {Active} {Learning} at {Scale}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/64254db8396e404d9223914a0bd355d2-Abstract.html},
	abstract = {The ability to train complex and highly effective models often requires an abundance of training data, which can easily become a bottleneck in cost, time, and computational resources. Batch active learning, which adaptively issues batched queries to a labeling oracle, is a common approach for addressing this problem. The practical benefits of batch sampling come with the downside of less adaptivity and the risk of sampling redundant examples within a batch -- a risk that grows with the batch size. In this work, we analyze an efficient active learning algorithm, which focuses on the large batch setting. In particular, we show that our sampling method, which combines notions of uncertainty and diversity, easily scales to batch sizes (100K-1M) several orders of magnitude larger than used in previous studies and provides significant improvements in model training efficiency compared to recent baselines. Finally, we provide an initial theoretical analysis, proving label complexity guarantees for a related sampling method, which we show is approximately equivalent to our sampling method in specific settings.},
	urldate = {2024-02-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Citovsky, Gui and DeSalvo, Giulia and Gentile, Claudio and Karydas, Lazaros and Rajagopalan, Anand and Rostamizadeh, Afshin and Kumar, Sanjiv},
	year = {2021},
	keywords = {Active learning, Unread},
	pages = {11933--11944},
}

@article{fu_bfcri_2023,
	title = {{BFCRI}: {A} {Blockchain}-{Based} {Framework} for {Crowdsourcing} {With} {Reputation} and {Incentive}},
	volume = {11},
	issn = {2168-7161},
	shorttitle = {{BFCRI}},
	url = {https://ieeexplore.ieee.org/abstract/document/9829255},
	doi = {10.1109/TCC.2022.3190275},
	abstract = {With the rapid development of cloud computing and the sharing economy, crowdsourcing aroused widespread interest and adoption in providing intelligent and efficient services for humans. The majority of existing works focus on effective crowdsourcing task assignment and privacy protection, mostly relying on central servers and assuming that participants are honesthonest-andand-curiouscurious and proactive. However, in reality, workers may be unwilling to participate, and there may be malicious behavior among participants, thus harming the enthusiasm and interests of other participants. The central server has weaknesses such as single point of failure. To address above problems, we propose a blockchain-based framework for crowdsourcing with reputation and incentive. We first design a worker selection scheme to select credible and capable workers. We leverage reputation as a metric of workers’ credibility, which is calculated through the improved subjective logic model. Then we utilize contract theory to design incentive mechanisms to attract more workers, especially high-quality workers to participate. Experimental results show that our proposed method can detect and prevent malicious participants and resist malicious collusion when the proportion of malicious participants is no more than 1/3. And encourage more workers to actively, honestly and continuously participate in crowdsourcing.},
	number = {2},
	urldate = {2024-01-08},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Fu, Shaojing and Huang, XueLun and Liu, Lin and Luo, Yuchuan},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Cloud Computing},
	keywords = {Crowdsourcing, Unread},
	pages = {2158--2174},
}

@article{moayedikia_optimizing_2020,
	title = {Optimizing microtask assignment on crowdsourcing platforms using {Markov} chain {Monte} {Carlo}},
	volume = {139},
	issn = {0167-9236},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620301597},
	doi = {10.1016/j.dss.2020.113404},
	abstract = {Microtasking is a type of crowdsourcing, denoting the act of breaking a job into several tasks and allocating them to multiple workers to complete. The assignment of tasks to workers is a complex decision-making process, particularly when considering budget and quality constraints. While there is a growing body of knowledge on the development of task assignment algorithms, the current algorithms suffer from shortcomings including: after-worker quality estimation, meaning that workers need to complete all tasks after which point their quality can be estimated; and one-off quality estimation method which estimates workers' quality only at the start of microtasking using a set of pre-defined quality-control tasks. To address these shortcomings, we propose a Markov Chain Monte Carlo–based task assignment approach known as MCMC-TA which provides iterative estimations of workers' quality and dynamic task assignment. Specifically, we apply Gaussian mixture model (GMM) to estimate workers' quality and Markov Chain Monte Carlo to shortlist workers for task assignment. We use Google Fact Evaluation dataset to measure the performance of MCMC-TA and compare it against the state-of-the-art algorithms in terms of AUC and F-Score. The results show that the proposed MCMC-TA algorithm not only outperforms the rival algorithms, but also offers a spammer-resistant result that maximizes the learning of workers' quality with minimal budget.},
	urldate = {2024-02-02},
	journal = {Decision Support Systems},
	author = {Moayedikia, Alireza and Ghaderi, Hadi and Yeoh, William},
	month = dec,
	year = {2020},
	keywords = {Crowdsourcing, Unread},
	pages = {113404},
}

@article{bouguelia_agreeing_2018,
	title = {Agreeing to disagree: active learning with noisy labels without crowdsourcing},
	volume = {9},
	issn = {1868-808X},
	shorttitle = {Agreeing to disagree},
	url = {https://doi.org/10.1007/s13042-017-0645-0},
	doi = {10.1007/s13042-017-0645-0},
	abstract = {We propose a new active learning method for classification, which handles label noise without relying on multiple oracles (i.e., crowdsourcing). We propose a strategy that selects (for labeling) instances with a high influence on the learned model. An instance x is said to have a high influence on the model h, if training h on x (with label \$\$y = h(x)\$\$) would result in a model that greatly disagrees with h on labeling other instances. Then, we propose another strategy that selects (for labeling) instances that are highly influenced by changes in the learned model. An instance x is said to be highly influenced, if training h with a set of instances would result in a committee of models that agree on a common label for x but disagree with h(x). We compare the two strategies and we show, on different publicly available datasets, that selecting instances according to the first strategy while eliminating noisy labels according to the second strategy, greatly improves the accuracy compared to several benchmarking methods, even when a significant amount of instances are mislabeled.},
	language = {en},
	number = {8},
	urldate = {2024-01-22},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Bouguelia, Mohamed-Rafik and Nowaczyk, Slawomir and Santosh, K. C. and Verikas, Antanas},
	month = aug,
	year = {2018},
	keywords = {Active learning, Crowdsourcing, Unread},
	pages = {1307--1319},
}

@article{zhang_blockchain-based_2019,
	title = {Blockchain-based secure and fair crowdsourcing scheme},
	volume = {15},
	issn = {1550-1329},
	url = {https://doi.org/10.1177/1550147719864890},
	doi = {10.1177/1550147719864890},
	abstract = {The crowdsourcing schemes which utilize the social network to solve complex tasks are an important part of open cooperation over the Internet. Although blockchain-based crowdsourcing schemes have considerable advantages in decentralization and data sharing, there is still a challenge to gurantee the security of crowdsourced-sensitive information and the fairness of crowdsourcing on the blockchain. To this end, this article investigates a crowdsourcing scheme based on blockchain. First, we define the basic requirements of blockchain-based crowdsourcing schemes including fairness, confidentiality, and integrity. And then, using secure hash, commitment, and homomorphic encryption, we propose a blockchain-based secure and fair crowdsourcing scheme, that is, BFC. The analysis results show that our scheme can satisfy the above requirements. Finally, the experimental results show that the computational overhead of the BFC scheme is acceptable to both the requester and the workers. In a word, our proposed crowdsourcing scheme has good expansibility in reality.},
	language = {en},
	number = {7},
	urldate = {2024-01-22},
	journal = {International Journal of Distributed Sensor Networks},
	author = {Zhang, Junwei and Cui, Wenxuan and Ma, Jianfeng and Yang, Chao},
	month = jul,
	year = {2019},
	note = {Publisher: SAGE Publications},
	keywords = {Blockchain, Crowdsourcing},
	pages = {1550147719864890},
}

@inproceedings{han_fluid_2019,
	address = {New York, NY, USA},
	series = {{SIGMOD} '19},
	title = {Fluid: {A} {Blockchain} based {Framework} for {Crowdsourcing}},
	isbn = {978-1-4503-5643-5},
	shorttitle = {Fluid},
	url = {https://doi.org/10.1145/3299869.3320238},
	doi = {10.1145/3299869.3320238},
	abstract = {Recently, crowdsourcing has emerged as a new computing paradigm to solve problems that need human intrinsic, such as image annotation. However, there are two limitations in existing crowdsourcing platforms, i.e. non-transparent incentive mechanism and isolated profiles of workers, which harms the interests of both requesters and workers. Meanwhile, Blockchain technology introduces a solution to build a transparent, immutable data model in the Byzantine environment. Moreover, Blockchain systems (e.g. Ethereum) can also support the Tuning-complete script called smart contracts. Thus, we are motivated to use the feature of the transparent data model and smart contract in Blockchain to address the two limitations. Based on the proposed solutions, we have designed a Blockchain based framework which supports foundations of general crowdsourcing platforms. In addition, our framework also has following novel features: (1) it provides the transparent incentive mechanisms; (2) it supports a trusted worker's profile sharing in a cross-platform mode.},
	urldate = {2024-01-08},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Han, Siyuan and Xu, Zihuan and Zeng, Yuxiang and Chen, Lei},
	month = jun,
	year = {2019},
	keywords = {Blockchain, Crowdsourcing},
	pages = {1921--1924},
}

@article{li_crowdbc_2019,
	title = {{CrowdBC}: {A} {Blockchain}-{Based} {Decentralized} {Framework} for {Crowdsourcing}},
	volume = {30},
	issn = {1558-2183},
	shorttitle = {{CrowdBC}},
	url = {https://ieeexplore.ieee.org/abstract/document/8540048},
	doi = {10.1109/TPDS.2018.2881735},
	abstract = {Crowdsourcing systems which utilize the human intelligence to solve complex tasks have gained considerable interest and adoption in recent years. However, the majority of existing crowdsourcing systems rely on central servers, which are subject to the weaknesses of traditional trust-based model, such as single point of failure. They are also vulnerable to distributed denial of service (DDoS) and Sybil attacks due to malicious users involvement. In addition, high service fees from the crowdsourcing platform may hinder the development of crowdsourcing. How to address these potential issues has both research and substantial value. In this paper, we conceptualize a blockchain-based decentralized framework for crowdsourcing named CrowdBC, in which a requester's task can be solved by a crowd of workers without relying on any third trusted institution, users' privacy can be guaranteed and only low transaction fees are required. In particular, we introduce the architecture of our proposed framework, based on which we give a concrete scheme. We further implement a software prototype on Ethereum public test network with real-world dataset. Experiment results show the feasibility, usability, and scalability of our proposed crowdsourcing system.},
	number = {6},
	urldate = {2024-01-08},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Li, Ming and Weng, Jian and Yang, Anjia and Lu, Wei and Zhang, Yue and Hou, Lin and Liu, Jia-Nan and Xiang, Yang and Deng, Robert H.},
	month = jun,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Blockchain, Crowdsourcing},
	pages = {1251--1266},
}

@article{song_active_2018,
	title = {Active learning with confidence-based answers for crowdsourcing labeling tasks},
	volume = {159},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118303460},
	doi = {10.1016/j.knosys.2018.07.010},
	abstract = {Collecting labels for data is important for many practical applications (e.g., data mining). However, this process can be expensive and time-consuming since it needs extensive efforts of domain experts. To decrease the cost, many recent works combine crowdsourcing, which outsources labeling tasks (usually in the form of questions) to a large group of non-expert workers, and active learning, which actively selects the best instances to be labeled, to acquire labeled datasets. However, for difficult tasks where workers are uncertain about their answers, asking for discrete labels might lead to poor performance due to the low-quality labels. In this paper, we design questions to get continuous worker responses which are more informative and contain workers’ labels as well as their confidence. As crowd workers may make mistakes, multiple workers are hired to answer each question. Then, we propose a new aggregation method to integrate the responses. By considering workers’ confidence information, the accuracy of integrated labels is improved. Furthermore, based on the new answers, we propose a novel active learning framework to iteratively select instances for “labeling”. We define a score function for instance selection by combining the uncertainty derived from the classifier model and the uncertainty derived from the answer sets. The uncertainty derived from uncertain answers is more effective than that derived from labels. We also propose batch methods which select multiple instances at a time to further improve the efficiency of our approach. Experimental studies on both simulated and real data show that our methods are effective in increasing the labeling accuracy and achieve significantly better performance than existing methods.},
	urldate = {2024-01-08},
	journal = {Knowledge-Based Systems},
	author = {Song, Jinhua and Wang, Hao and Gao, Yang and An, Bo},
	month = nov,
	year = {2018},
	keywords = {Active learning, Crowdsourcing},
	pages = {244--258},
}

@misc{li_ocean_2023,
	title = {Ocean {Data} {Quality} {Assessment} through {Outlier} {Detection}-enhanced {Active} {Learning}},
	url = {http://arxiv.org/abs/2312.10817},
	doi = {10.48550/arXiv.2312.10817},
	abstract = {Ocean and climate research benefits from global ocean observation initiatives such as Argo, GLOSS, and EMSO. The Argo network, dedicated to ocean profiling, generates a vast volume of observatory data. However, data quality issues from sensor malfunctions and transmission errors necessitate stringent quality assessment. Existing methods, including machine learning, fall short due to limited labeled data and imbalanced datasets. To address these challenges, we propose an ODEAL framework for ocean data quality assessment, employing AL to reduce human experts' workload in the quality assessment workflow and leveraging outlier detection algorithms for effective model initialization. We also conduct extensive experiments on five large-scale realistic Argo datasets to gain insights into our proposed method, including the effectiveness of AL query strategies and the initial set construction approach. The results suggest that our framework enhances quality assessment efficiency by up to 465.5\% with the uncertainty-based query strategy compared to random sampling and minimizes overall annotation costs by up to 76.9\% using the initial set built with outlier detectors.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Li, Na and Qi, Yiyang and Xin, Ruyue and Zhao, Zhiming},
	month = dec,
	year = {2023},
	note = {arXiv:2312.10817 [cs]},
	keywords = {Active learning},
}

@inproceedings{costa_using_2011,
	title = {On using crowdsourcing and active learning to improve classification performance},
	url = {https://ieeexplore.ieee.org/abstract/document/6121700},
	doi = {10.1109/ISDA.2011.6121700},
	abstract = {Crowdsourcing is an emergent trend for general-purpose classification problem solving. Over the past decade, this notion has been embodied by enlisting a crowd of humans to help solve problems. There are a growing number of real-world problems that take advantage of this technique, such as Wikipedia, Linux or Amazon Mechanical Turk. In this paper, we evaluate its suitability for classification, namely if it can outperform state-of-the-art models by combining it with active learning techniques. We propose two approaches based on crowdsourcing and active learning and empirically evaluate the performance of a baseline Support Vector Machine when active learning examples are chosen and made available for classification to a crowd in a web-based scenario. The proposed crowdsourcing active learning approach was tested with Jester data set, a text humour classification benchmark, resulting in promising improvements over baseline results.},
	urldate = {2024-01-08},
	booktitle = {2011 11th {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}},
	author = {Costa, Joana and Silva, Catarina and Antunes, Mário and Ribeiro, Bernardete},
	month = nov,
	year = {2011},
	note = {ISSN: 2164-7151},
	keywords = {Active learning, Crowdsourcing},
	pages = {469--474},
}
