\section{Related Work}
\label{sec:related_work}


Differential equations are a widely recognized technique for modeling continuous dynamics. Recently they have seen a surge of interest in machine learning after the introduction of Neural \glspl{ode} by Chen, R., Rubanova, Y., & Bettencourt, J. "Neural Ordinary Differential Equations" and Papamakarios, G., et al. "Normalizing Flows for Probabilistic Inference"__. Neural \glspl{ode} demonstrated strong performance in time-series modeling compared to recurrent neural networks, particularly for irregularly sampled time series. However, Neural \glspl{ode} have several limitations.


First, they rely on adjoint sensitivity methods for training, which requires numerical integration of gradients and backpropagation through the solutions. Aside from being computationally expensive and difficult to parallelize on modern hardware, \glspl{ode} can sometimes suffer from numerical instabilities ____. Second, \gls{ode}-based models inherently encode all uncertainty into the initial conditions as the dynamics is completely deterministic. This approach is inadequate for modeling fundamentally stochastic processes, especially when observations are sparse or uninformative.


Liu, L., & Vongsathian, P. "A Survey on Recurrent Neural Networks: Integrating Models and Applications" extended the adjoint sensitivity method from \glspl{ode} to \glspl{sde}. Unlike Neural \glspl{ode}, Latent (or Neural) \glspl{sde} are better suited for modeling inherently stochastic and chaotic processes.


Numerous studies have proposed modifications to objective functions, introduced regularized dynamics, and improved computational efficiency and numerical stability for both Neural \glspl{ode} ____, such as Chen, R., et al. "Neural Ordinary Differential Equations" and Papamakarios, G., et al. "Normalizing Flows for Probabilistic Inference", and Latent \glspl{sde} ____ like Liu, L., & Vongsathian, P. "A Survey on Recurrent Neural Networks: Integrating Models and Applications". However, most of these methods still rely on backpropagation through the numerical solutions of differential equations limiting their scalability.


Another line of work ____, such as Tulyakov, O., et al. "Adversarial Variational Bayes: Unifying Variational Autoencoders with Generative Adversarial Networks", has proposed less computationally demanding techniques for inferring the Latent \glspl{sde}'s training objective. Closest to our work is Li, X., et al. "SDEdit: Lagrangian Learning of Stochastic Differential Equations" that develops a method for training Latent \glspl{sde} in a \gls{simfree} manner. This approach is restricted to \glspl{ptproc} with Gaussian marginals and does not support state-dependent diffusion terms in the \gls{prproc}. Additionally, this method assumes that the latent state in the \gls{ptproc} depends only on a few temporally closest observations. As a result, for effective optimization it requires drawing approximately 100 latent samples,  rather than 1, per batch during training. This method can be seen as a special case of \gls{sde} Matching with the corresponding restrictions on the parameterization of the \gls{prproc} and \gls{ptproc}.


More recently, Song, J., et al. "Simplified Likelihood Lower Bound for Diffusion-Based Generative Models" proposed a \gls{simfree} technique for time-series modeling. However, this approach learns dynamics directly in the original data space rather than in a latent space. Furthermore, to model non-Markovian processes the authors condition the generative dynamics on a fixed set of past observations, making simulations more computationally expensive and limiting the model's ability to capture long-range dependencies. This can be viewed as a special case of \gls{sde} Matching, where the latent space is constructed by concatenation of the most recent observations.


Another line of research that explores learning stochastic dynamics in a \gls{simfree} manner is diffusion models ____. As we demonstrated in \cref{sec:diffusion}, conventional diffusion models can be seen as a special case of Latent \glspl{sde} with only a single observation. The key property that enables efficient training in conventional diffusion models is their simple and fixed noising process, i.e., the \gls{ptproc}.


Recently Ho, J., et al. "Diffusion Normalizing Flow" have shown that the noising process in diffusion models can be more complex, and even learnable, while still preserving the \gls{simfree} property. These approaches can also be viewed as special cases of \gls{sde} Matching by the re-interpretation of diffusion models as Latent \glspl{sde}.