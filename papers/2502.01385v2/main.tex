
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

%========================================
% Our commonly used packages
% \input{math_commands.tex}
\usepackage{mathtools}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{float}
\usepackage{color}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{nicefrac}% compact symbols for 1/2, etc.
\newcommand{\norm}[1]{\lVert #1 \rVert}
\def \aa {{\bm{a}}}
\def \ee {{\bm{e}}}
\def \rr {{\bm{r}}}
\def \tt {{\bm{t}}}
\def \xx {{\bm{x}}}
\def \yy {{\bm{y}}}
\def \pp {{\bm{p}}}
\def \qq {{\bm{q}}}
\def \oo {{\bm{o}}}
\def \uu {{\bm{u}}}
\def \ww {{\bm{w}}}
\def \zz {{\bm{z}}}
\def \ej {{\bm{e_j}}}
\def \ei {{\bm{e_i}}}
\def \btheta {{\bm{\theta}}}
\def \X  {\mathcal{X}}
\def \Y  {\mathcal{Y}}
\def \F  {\mathcal{F}}
\def \H  {\mathcal{H}}
\def \L  {\mathcal{L}}
\def \R  {\mathbb{R}}
\def \U  {\mathcal{U}}
\def \D  {\mathcal{D}}
\def \C  {\mathcal{C}}
\def \F  {\mathcal{F}}
\def \E  {\mathbb{E}}
\def \W {{\bm{W}}}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}

\newcommand{\simil}{\mathrm{sim}}
\newtheorem{mydefinition}{Definition}
\newcommand{\IntrDim}{\mathrm{IntrDim}}
\newcommand{\IDop}{\operatorname{LID}}
\newcommand{\ID}{\IDop}
\newcommand{\IDstar}{\IDop^{*}}
\DeclareMathOperator{\SLOF}{\textrm{SLOF}}
\DeclareMathOperator{\knn}{\mathit{k}\textrm{NN}}
\DeclareMathOperator{\nn}{\textrm{NN}}
\DeclareMathOperator{\kdist}{\mathit{k}\textrm{-dist}}
\DeclareMathOperator{\slrd}{\textrm{slrd}}
\DeclareMathOperator{\DAO}{\textrm{DAO}}
\DeclareMathOperator{\LOF}{\textrm{LOF}}
\DeclareMathOperator{\ALDR}{\textrm{ALDR}}
\newcommand{\expect}{\mathop{\mathbb{E}}}
% \usepackage{amssymb,amsmath,amsthm}

\newtheorem{theorem}{Theorem}

\iclrfinalcopy

\title{
Detecting Backdoor Samples in Contrastive Language Image Pretraining
}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Hanxun Huang\textsuperscript{1} \ \
Sarah Erfani\textsuperscript{1} \ \
Yige Li\textsuperscript{2}\footnotemark[2] \ \
Xingjun Ma\textsuperscript{3}\footnotemark[2] \ \
James Bailey\textsuperscript{1}\\
\textsuperscript{1}School of Computing and Information Systems, The University of Melbourne, Australia \\
\textsuperscript{2}School of Computing and Information Systems, Singapore Management University, Singapore \\
\textsuperscript{3}School of Computer Science, Fudan University, China \\ 
\texttt{\{hanxun,sarah.erfani,baileyj\}@unimelb.edu.au;} \\ \texttt{\{yigeli\}@smu.edu.sg;\{xingjunma\}@fudan.edu.cn.}\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Corresponding author.}

\begin{abstract}
Contrastive language-image pretraining (CLIP) has been found to be vulnerable to poisoning backdoor attacks where the adversary can achieve an almost perfect attack success rate on CLIP models by poisoning only 0.01\% of the training dataset. This raises security concerns on the current practice of pretraining large-scale models on unscrutinized web data using CLIP. In this work, we analyze the representations of backdoor-poisoned samples learned by CLIP models and find that they exhibit unique characteristics in their local subspace, i.e., their local neighborhoods are far more sparse than that of clean samples. Based on this finding, we conduct a systematic study on detecting CLIP backdoor attacks and show that these attacks can be easily and efficiently detected by traditional density ratio-based local outlier detectors, whereas existing backdoor sample detection methods fail. Our experiments also reveal that an unintentional backdoor already exists in the original CC3M dataset and has been trained into a popular open-source model released by OpenCLIP. Based on our detector, one can clean up a million-scale web dataset (e.g., CC3M) efficiently within 15 minutes using 4 Nvidia A100 GPUs. 
The code is publicly available in our  \href{https://github.com/HanxunH/Detect-CLIP-Backdoor-Samples}{GitHub repository}.
\end{abstract}


\section{Introduction}
Contrastive language-image pretraining (CLIP) \citep{radford2021learning} is a popular self-supervised learning framework \citep{chopra2005learning,hadsell2006dimensionality,grill2020bootstrap,chen2021exploring,caron2021emerging,bardes2022vicreg} that allows pretraining of large-scale multi-modal models on web data without human annotations~\citep{radford2021learning,jia2021scaling}.
However, in a recent study by~\citet{carlini2022poisoning}, it was found that CLIP is extremely vulnerable to poisoning backdoor attacks, where an attacker backdoors the victim model by poisoning (adding the trigger to) a few training samples~\citep{gu2017badnets,chen2017targeted,liu2018trojaning}.
\citet{carlini2022poisoning} investigated backdoor attack on CLIP with a patch trigger, and revealed that an attacker can successfully attack CLIP by poisoning only 0.01\% of the training samples. This poisoning rate is marginal compared to supervised learning where successful attacks generally require a high poisoning rate of 1\% -- 10\%.
Poisoning existing web-scale datasets is also realistic since the curator typically only maintains a list of hyperlinks to the image. \citet{carlini2024poisoning} have shown that adversaries could poison 0.01\% of web-scale datasets by purchasing expiring domains with \$10 USD. 
This vulnerability poses a major security threat to the current practice of CLIP, considering that many popular multi-modal models \citep{alayrac2022flamingo,liu2023visual,betker2023improving,awadalla2023openflamingo} were pre-trained using CLIP on unscrutinized web data crawled from untrusted sources.

Several backdoor defence techniques for CLIP have been proposed, which are mostly robust training methods using heavy data augmentations \citep{bansal2023cleanclip,yang2023robust}, or a uni-modal objective \citep{yang2023better}. 
These methods train the model directly on the poisoned training dataset while minimizing the effect caused by the backdoor samples. Although these methods have demonstrated promising results, an in-depth understanding of the unique characteristics of CLIP backdoor attacks is absent in the current literature. A more concerning fact is that no backdoor sample detection method exists that can help data owners and model developers efficiently clean up a million-scale web dataset. A backdoor sample detection method is essential for secure CLIP because (1) it can detect and remove backdoor-poisoned samples from a large-scale dataset once and for all, and (2) it can help remove noisy or unintentional backdoor samples from the dataset even when there are no attacks. Moreover, it has been shown theoretically that the detection and removal of backdoor data is equivalent to robust training under mild assumptions \citep{manoj2021excess}. However, prior detection works against supervised backdoor attacks revealed that detecting backdoor samples under extremely low poisoning rates (e.g., 0.01\%) is very difficult \citep{hayase2021defense,huang2023distilling}.
Moreover, existing backdoor sample detection methods ~\citep{chen2018detecting,tran2018spectral,gao2019strip,hayase2021defense,li2021anti,hou2024ibd} were all developed for supervised learning, which might not be applicable for CLIP.

In this paper, we explore the local neighborhood characteristics of backdoor samples in the representation space and discover one major weakness of CLIP backdoor attacks, i.e., they have a much more sparse neighborhood than clean samples, making them outliers.
Based on this finding, we further investigate the detectability of CLIP backdoor samples by both existing backdoor sample detection methods and traditional outlier detection methods. Surprisingly, we find that traditional general-purpose outlier detection methods can detect CLIP backdoor samples with high accuracy, while existing backdoor sample detection methods for supervised learning could fail in certain cases. In particular, classic methods such as distance to the $k$-th nearest neighbor and isolation forest (iForest) \citep{liu2008isolation} can outperform existing backdoor sample detection methods \citep{li2021anti,huang2023distilling}.
Performance can be further improved by considering density-focused techniques, such as the simplified local outlier factor (SLOF) \citep{schubert2014local} and dimensionality-aware outlier detection (DAO) \citep{anderberg2024dimensionality}. 

Our main contributions are as follows:
\begin{itemize}

    \item We present a systematic study on the detectability of poisoning backdoor attacks on CLIP, and show that existing detection methods designed for supervised learning can fail on CLIP. 

    \item We reveal one major weakness of CLIP backdoor samples related to the sparsity of their representation local neighborhood, which facilitates highly accurate and efficient detection using efficient local density-based detectors. With these detectors, one can clean up a million-scale poisoned dataset (e.g., CC3M) within 15 minutes using 4 Nvidia-A100 GPUs.

    \item Our experiments in the clean setting reveal that there exist unintentional (natural) backdoors in the CC3M dataset, which has been injected into a popular open-source model released by OpenCLIP. 
    
\end{itemize}


\section{Related Work}
\noindent\textbf{Backdoor Attacks.} The objective of a backdoor attack is to deceive a victim model to learn a shortcut correlation between the trigger and a targeted output. The adversary can subsequently manipulate the predictions of the victim model at the test time with the trigger. 
Based on the attackers' and defenders' capabilities, existing backdoor attacks can be categorized into \emph{data-poisoning} and \emph{training-manipulation} attacks. 
In data-poisoning attacks, the adversary injects triggers into the defender's training data, but the defender has full control of the model training. Such attacks simulate the scenario where the defender utilizes an untrusted web dataset for training.
In training manipulation attacks \citep{lin2020composite,shumailov2021manipulating,bagdasaryan2021blind,nguyen2021wanet,doan2021lira,wang2022bppattack}, the attacker can manipulate both the training data and the objective function to implant the trigger and then
releases the backdoored model for the victim to download. This simulates the scenario where the victim downloads pre-trained model parameters from untrusted open-source platforms. 
The focus of this work is data poisoning attacks. 

Existing backdoor attacks are mostly focused on attacking supervised learning \citep{gu2017badnets,chen2017targeted,liu2018trojaning}. 
For poisoning attacks, the trigger pattern is one main contributing factor to the success of the attack. The trigger pattern could be a patch \citep{gu2017badnets} or a blending image \citep{chen2017targeted}. Advanced attacks leverage more complex trigger patterns such as periodical patterns \citep{barni2019new}, natural reflections \citep{liu2020reflection}, physical objects \citep{li2020rethinking,wenger2021backdoor}, adversarial perturbations \citep{turner2018clean,zhao2020clean}, GANs \citep{cheng2020deep}, Instagram filters \citep{liu2019abs}, image generator \citep{sun2024backdoor} and image frequency perturbations \citep{zeng2021rethinking,li2023embarrassingly}. 
While injecting the trigger pattern into training images, the attacker could either alter the corresponding label (known as {\em dirty-label} attacks) or keep the label unchanged (known as {\em clean-label} attacks) \citep{turner2018clean,zhao2020clean}.
There could also be multiple triggers released by one or more adversaries for the same dataset \citep{li2024multi}, which is a realistic setting for downloading data from untrusted sources. 

\citet{carlini2022poisoning} proposed the first poisoning backdoor attacks on CLIP with patch triggers.
Compared to supervised learning, poisoning backdoor attacks on CLIP can achieve a high attack success rate at a much lower poisoning rate (i.e., 0.01\%). 
Concurrently, targeted poisoning attack in the finetuning stage \citep{yang2023data} and training-manipulation backdoor attacks have also been developed for CLIP \citep{jia2022badencoder,liu2022poisonedencoder,tao2023distribution}.
The main focus of our work is detecting poisoning backdoor samples in CLIP, for which we follow the same threat model and experimental setup as \citet{carlini2022poisoning}.


\noindent\textbf{Backdoor Defense.}
Backdoor defense can be categorized into 1) trigger synthesis, 2) backdoor model detection, 3) robust training, and 4)  backdoor sample detection methods. 
Trigger synthesis aims to recover the trigger patterns used to poison and attack the victim model \citep{liu2019abs,wang2019neural,hu2022trigger}. Model detection aims to determine if a trained model contains backdoors \citep{chen2019deepinspect,kolouri2020universal,xu2021detecting,feng2023detecting,kuang2024adversarial}. Note that trigger synthesis and model detection methods will still need backdoor removal techniques to obtain a robust model. A robust training strategy aims to (pre)train a backdoor-free model on backdoor-poisoned dataset by robustifying the training procedure of supervised learning \citep{li2021anti,borgnia2021strong,huang2022backdoor,dolatabadi2022collider}, self-supervised learning \citep{li2024difficulty} or CLIP \citep{bansal2023cleanclip,yang2023better,yang2023robust}. 

Backdoor sample detection determines if a data point is infected with the backdoor trigger. 
It can leverage either the statistics of the deep features \citep{tran2018spectral,chen2018detecting,tang2021demon}, sensitivity characteristics to certain perturbations and transformations \citep{gao2019strip,chen2022effective,hou2024ibd} or inference time detection with contrastive prompting \citep{niu2024bdetclip}. Cognitive Distillation (CD) extracts a minimal pattern that allows the model to produce the same output and uses the norm of the extracted mask to detect whether a training/test sample is backdoored \citep{huang2023distilling}. However, it is an optimization-based method that is time-consuming and of limited scalability for web-scale datasets.  
 Anti-Backdoor Learning (ABL) tracks sample-specific training loss during training and detects samples of the lowest loss as backdoor samples \citep{li2021anti}. 
The above defense methods were all developed under supervised learning, with many relying on the class labels to function, which is not available in CLIP.
SafeCLIP is proposed as end-to-end robust training strategy to obtain a backdoor-free model from potentially poisoned dataset \citep{yang2023better}. 
It has two components: one for detecting backdoor data and one for robust training on safe and risky subsets. 


\noindent\textbf{Outlier Detection.}
Outlier detection is a classic problem in data mining. It aims to find data points that deviate from the general distribution. It can be categorized into parametric and non-parametric approaches. The parametric approach makes explicit assumptions about the nature of the underlying data distribution \citep{yang2009outlier,satman2013new}, while the non-parametric does not. 
The non-parametric approach is more suitable for unsupervised settings such as backdoor sample detection. 
It include statistical methods \citep{goldstein2012histogram,li2022ecod}, and ensemble methods \citep{lazarevic2005feature,zhao2021suod} such as the isolation forest (iForest) \citep{liu2008isolation}. 
Local outlier methods are another type of non-parametric outlier detection methods, such as k-nearest-neighbor \citep{ramaswamy2000efficient}, local outlier factor (LOF) \citep{breunig2000lof}, and their improved versions \citep{tang2002enhancing,papadimitriou2003loci,latecki2007outlier,kriegel2008angle}. 
These methods either explicitly or implicitly assess the density in the vicinity of a query point \citep{campos2016evaluation} and data points with low density are usually regarded as outliers. 
 Local intrinsic dimensionality (LID) is another local measure that describes the growth rate of the number of data points in the vicinity of the query point \citep{levina2004maximum,houle2017local1}. LID has been used in various machine learning-related applications \citep{gong2019intrinsic,ansuini2019intrinsic,pope2021the}. Notably, it is used in outlier detections \citep{houle2018correlation}, detecting adversarial examples \citep{ma2018characterizing} and backdoor samples (in a supervised setting) \citep{dolatabadi2022collider}. 
In this work, based on our empirical observation, we choose to exploit simplified local outlier factor (SLOF) \citep{schubert2014local} and its extension dimensionality-aware outlier detection (DAO) \citep{anderberg2024dimensionality} 
for detecting backdoor samples. 


\section{Preliminaries}
In this section, we first describe poisoning backdoor attacks on CLIP and then introduce three outlier detection metrics explored in this work, including simplified local outlier factor (SLOF), local intrinsic dimensionality (LID), and dimensionality-aware outlier detection (DAO). 


\subsection{Poisoning Backdoor Attacks on CLIP}
\label{sec:backdoor_contrastive_learning} 

Following existing work \citep{carlini2022poisoning}, we focus on multi-modal Contrastive Language Image Pretraining (CLIP) \citep{radford2021learning}, which learns a joint representation of images and text from image-text data. 
Given a image-text dataset $\mathcal{D} \subset \mathcal{X} \times \mathcal{T}$ that contains pairs of ($\bm{x}_i, \bm{t}_i$), where $\bm{x}_i$ is an image, and $\bm{t}_i$ is the associated descriptive caption. 
The CLIP framework uses an image encoder $f_{I}: \mathcal{X} \mapsto \mathbb{R}^{d}$ and a text encoder $f_{T}: \mathcal{T} \mapsto \mathbb{R}^{d}$, and projects the image and text to a joint representation space $\mathbb{R}^{d}$. The image representation can be obtained by $\bm{z}_{i}^{x} = f_{I}(x_i)$ and the text representation is $\bm{z}_{i}^{t} = f_{T}(t_i)$. For a given batch of $N$ image-text pairs $\{\bm{x}_i, \bm{t}_i\}_{i=1}^{N}$, CLIP adopts the following training loss function:
\begin{equation}
\nonumber
-\frac{1}{2N} \sum_{j=1}^N \log \frac{\exp(\simil(\bm{z}^{x}_j, \bm{z}^{t}_j) / \tau)}{\sum_{k=1}^{N} \exp(\simil(\bm{z}^{x}_j, \bm{z}^{t}_k) / \tau)} -\frac{1}{2N} \sum_{k=1}^N \log \frac{\exp(\simil(\bm{z}^{x}_k, \bm{z}^{t}_k) / \tau)}{\sum_{j=1}^{N} \exp(\simil(\bm{z}^{x}_j, \bm{z}^{t}_k) / \tau)},
\end{equation}
where $\tau$ is a trainable temperature parameter, and $\simil(\cdot)$ is a similarity measure. The first term in the above objective function contrasts the images with the texts, while the second term contrasts the texts with the images.


The main focus of our work is detecting poisoning backdoor images in the CLIP pretraining dataset, as most existing backdoor triggers have been concentrated in the vision domain \citep{carlini2022poisoning}. Poisoning images is also generally more practical than text in web-scale pretraining datasets \citep{carlini2024poisoning}. We adopt the same threat model and setup as \citet{carlini2022poisoning}. In addition to backdoor attacks, we also aim to detect poisoned data in targeted data poisoning attacks \citep{biggio2012poisoning,carlini2022poisoning,yang2023data}.

\textbf{Backdoor Attack (BA).}
For poisoning backdoor attack on CLIP, the adversary could use a function $A(\cdot)$ to construct a backdoored image-text pair $(\bm{x}', \bm{t}') = A((\bm{x}, \bm{t})) $. The trigger pattern can be inserted into the image using $\bm{x}' = \bm{m} \odot \bm{\Delta} + (1-\bm{m}) \odot \bm{x}$, where $\odot$ is the element-wise multiplication and  $\bm{\Delta}$ is a trigger pattern. 
This is a general definition of backdoored image commonly adopted in existing work \citep{wang2019neural}. For the associated caption $\bm{t}'\in \text{caption set}$, one might use engineered prompt templates \citep{radford2021learning} as the caption set, such as ``\emph{a photo of a \{target\}}", where \emph{target} is the attacker's desired output. The attacker could also insert the trigger to the image where the \emph{target} is already in the captions $\bm{t}=\bm{t'}$ (without text caption modification), which is known as a clean-label attack on CLIP. 
We assume the adversary can inject the poisoned subset $\mathcal{D}_{b} = \{(\xx'_{i}, \bm{t}'_{i})=A(\xx_i, \bm{t}_i) | \bm{t}'_{i} \in \text{caption set}\}_{i=1}^M$ into defender's training data. 
The attacker's objective is to control the model to produce a desired output. For example, in the case of using engineered prompt templates for zero-shot classification, the attack is successful if the adversary queries the victim model with a backdoor image $\bm{x}'$ and receives its desired backdoor \emph{target} as prediction. 

\textbf{Targeted Data Poisoning Attack (TDPA).}
Unlike backdoor attacks, targeted data poisoning attacks aim to fool the model by misclassifying a specific sample $\xx'$ into a targeted class $y_{t}$ without using a universal trigger $\Delta$. The poisoned subset is $\mathcal{D}_{b} = \{(\xx', \bm{t}'_{i})| \bm{t}' \in \text{caption set}\}_{i=1}^M$ and the caption set is constructed by finding all captions in $\mathcal{D}$ that contains target keyword $y_{t}$.
The adversary's goal is to misclassify $\xx'$ into $y_{t}$ in the zero-shot classification. Similar to backdoor attacks, we assume the adversary can poison the training data. 



\subsection{Outlier Detection Metrics}
\label{sec:background_dao}
We will empirically show in Section \ref{sec:method} that the local neighborhood of a backdoor-poisoned sample is much more sparse (low density) than that of clean samples. This motivates us to exploit local and density-based outlier detection metrics to differentiate CLIP backdoor samples. While other metrics could also be worth investigating, we focus on the following three classic metrics.

\noindent\textbf{Simplified Local Outlier Factor (SLOF).} 
The SLOF \citep{schubert2014local} is a variant of the classical  Local Outlier Factor (LOF) \citep{breunig2000lof}. 
The `local outlier' refers to a query point $\bm{q}$ that is sufficiently different from other neighboring points in its vicinity. LOF considers the typical density ratio. For the query point $\bm{q}$, if it is less dense than its neighborhoods, then it is more likely to be an outlier. 
The classical LOF is based on the reachability distances that require multiple levels of neighborhood computation. The SLOF provides a simplified version by using the distance to the $k$-th nearest neighbor, defined as the following: 
\begin{equation}
\nonumber
    \SLOF_{k}(\qq)
    \:\:\triangleq\:\:
    \frac{1}{k}\sum_{\oo\in \nn_k(\qq)}\frac{\kdist(\qq)}{\kdist(\oo)}
    \, ,
\end{equation}
where $\kdist(x)$ is the distance to a sample $x$'s $k$-th nearest neighbor and $\nn_k(\cdot)$ are the $k$ nearest neighbors.

\noindent\textbf{Local Intrinsic Dimensionality (LID).}  The LID metric \citep{houle2017local1} describes the rate of growth in the number of data objects encountered as the distance from the reference sample increases.
It measures the intrinsic dimensionality in the vicinity of the query point. 
Formally, let $F$ be a real-valued function that is non-zero over some open interval containing $r\in\real$, $r\neq 0$.
\begin{theorem}[\cite{houle2017local1}]
\label{T:fundamental}
If $F$ is continuously differentiable at 
$r$, then
\[
\ID_F(r)
\:
\triangleq
\:
\frac{r\cdot F'(r)}{F(r)}
\: .
% =
% \:
% \IntrDim_{F}(r)
% \, .
\]
\end{theorem}
We are interested in functions $F$ that satisfy the conditions of a cumulative distribution function (CDF). 
The LID at a query point is defined as the limit when the radius r tends to zero: 
\[
\IDstar_F\triangleq\lim_{r\to 0^{+}}\ID_F(r)
\, .
\]
Henceforth, when we refer to the LID of a function $F$, or of a point $\mathbf{x}$ whose induced distance distribution has $F$ as its CDF, we will take `LID' to mean the quantity $\IDstar_{F}$.
In practice, the LID needs to be estimated, such as using maximum likelihood estimation (MLE) \citep{levina2004maximum} or Bayesian estimation \citep{joukhadar2024bayesian}. We refer to the estimated value as $\widehat{\IDstar}$.

\noindent\textbf{Dimensionality-Aware Outlier Detection (DAO).}
 DAO~\citep{anderberg2024dimensionality} is a criterion that extends LOF and SLOF using theory in dimensionality characteristics. Specifically, DAO is defined as the following:
\begin{align*}
\DAO_k(\qq)
\triangleq &
\frac{1}{k}
\sum_{\oo\in \nn_k(\qq)}
\left(
\frac{\kdist(\qq)}{\kdist(\oo)}
\right)^{\widehat{\IDstar_{F_{\oo}}}}
    \, .
\end{align*}
A DAO score greater than 1 indicates it is likely to be an outlier. It suggests that the query point $\qq$ has a local probability measure too small to be consistent with those of its neighbors in the domain. 
The DAO criterion is a generalization of SLOF. In essence, SLOF implicitly assumes the underlying local intrinsic dimensionalities are equal to 1 ($\IDstar_{F_{\oo}}=1$) for all data points. This may not always be realistic for machine learning applications \citep{ma2018characterizing,ma2018dimensionality,gong2019intrinsic,ansuini2019intrinsic,pope2021the,huang2024ldreg,zhou2024dda}.
As a result, DAO is theoretically more favorable than SLOF.


\section{Detecting CLIP Backdoor Attacks}
\label{sec:method}

In this section, we begin by discussing the problem definition of backdoor sample detection. We then show an intuitive example of backdoor representations as local outliers. 
Finally, we present the exploration of SLOF, LID, and DAO for CLIP backdoor sample detection. 

\subsection{Backdoor Sample Detection}
\label{sec:backdoor_data_detection}

\noindent\textbf{Threat Model.} Following previous works \citep{carlini2022poisoning}, we assume the attacker can poison the defender's training data but does not have access to the training process. The defender has full control over the training process but has no prior knowledge of (i) the poisoning rate, (ii) the trigger pattern, (iii) the target, or (iv) whether an image-text pair is clean or backdoored. The defender aims to produce the probability of an image-text pair being poisoned. 

\noindent\textbf{Problem Formulation.} 
We denote the training data as $\mathcal{D}=\mathcal{D}_{c} \cup \mathcal{D}_{b}$, the clean subset as $\{(\xx_i, \bm{t}_i)\}_{i=1}^N \in \mathcal{D}_{c}$, and the poisoned subset as $\{(\xx'_{i}, \bm{t}'_{i})\}_{i=1}^M \in \mathcal{D}_{b}$, respectively. 
The poisoning rate is defined as $\frac{|\mathcal{D}_{b}|}{|\mathcal{D}|}=\frac{M}{M+N}$. The defender's goal is to accurately detect pairs $(\xx, \tt) \in \mathcal{D}_{b}$.

Backdoor sample detection is a binary classification task (`backdoor' vs `clean'). 
We consider the following function $g(\cdot)$ to determine whether an image-text pair $(\xx_i, \bm{t}_i)$ contains backdoor based on the detection score $s_i$:
\begin{equation}
\label{eq:score}
g(\xx_i, \bm{t}_i) = 
\begin{cases}
    1 & \text{if } s_i  > t , \\
    0 & \text{if } s_i  \leq t ,
\end{cases} 
\end{equation}
where, $t$ is a threshold, $g(\cdot)=1$ indicates a backdoor sample and $g(\cdot)=0$ indicates a clean sample.
In practice, the defender can adjust $t$ based on the statistics of the detection score, such as the mean and standard deviation. Alternatively, the defender might remove certain percentages of data from the training set. 
For accurate detection, the most crucial objective is to correctly rank the score $s$ within the dataset, e.g., assign higher scores to backdoor samples.  


\subsection{Characterizing CLIP Backdoor Samples}
\label{sec:backdoor_representation}
Our goal is to find unique characteristics of CLIP backdoor samples. We find existing methods, such as ABL \citep{li2021anti} and CD \citep{huang2023distilling}, are not sufficient to find the distinctive differences between clean and backdoor samples in CLIP. 
In this work, we take a different approach to examine the learned representations of the model.
To motivate our approach, in Figure \ref{fig:slof}, we provide an illustrative example of a representation with CLIP trained on backdoor poisoned data using the patch trigger \citep{gu2017badnets}. 
CLIP uses the contrastive learning loss that clusters image-text pairs with similar contents to the same region. All the backdoor-poisoned samples contain similar features (the trigger) and are likely to be clustered together in a particular region. Since the trigger is a strong signal and the model is overconfident about these poisoned samples, the surrounding subspace has distinctive characteristics compared to clean samples, e.g., they are tightly clustered and far away from other clean data. This can be observed in  Figure \ref{fig:slof}.

As a result, to detect poisoned backdoor samples, one might consider using local distance measures, such as the distance to the $k$-th nearest neighbors, the $\kdist$. 
Consider randomly sampling a batch of the data (batch size 1024), if the poisoning rate is 0.01\% and there is 1 poisoned sample in the batch, the probability of the rest of the data being clean is $0.9999^{1023}$. 
The $k$-th nearest neighbor is highly likely to be a clean sample, a larger $\kdist$. For the clean data, it is likely the clean sample as well, results in a smaller $\kdist$. This characteristic makes backdoor representations as outliers. In Figure \ref{fig:k_dist_over_n_bd}, we show a controlled experiment. As the poisoning rate increases, the distribution of the $\kdist$ stays fairly stable for the clean samples but dramatically decreases for the backdoor samples. This indicates that as long as the poisoning rate is low, the $k$-th nearest neighbor for the backdoor representation is a clean representation, and the backdoor representation is an outlier in the batch. Hence, 
with an appropriate locality $k$, simple $\kdist$ is sufficient to detect these poisoned samples. 




\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.6\linewidth}
	\includegraphics[width=\textwidth]{Figures/slof_example.pdf}
	\caption{A t-SNE visualization of the learned representations by CLIP}
	\label{fig:slof}
	\end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
	\includegraphics[width=\textwidth]{Figures/k_dist_over_n_bd.pdf}
	\caption{Distribution of $\kdist$}
	\label{fig:k_dist_over_n_bd}
	\end{subfigure}
	\caption{(a) The CLIP learned presentations are projected into a 2-D space using t-SNE. The red cross is a backdoor data point, the green dot is a clean data point, and the blue dot is a randomly sampled data point. The $\kdist$ is the distance to the $k$-th nearest neighbor, and the circle with the solid line is the region containing all $k$ nearest neighbors. The circle with a dashed line is the region containing $k$ nearest neighbors for the $k$-th neighbors. $k$ is set to 16. 
    (b) The distribution $\kdist$ for clean and backdoor data with different poisoning rates (PR) within a batch. 
    }
	\label{fig:example}
\end{figure}




An alternative criterion is to use local density measures, where locality is given by $k$ nearest neighbors, whose distance is used to estimate the density. As shown in Figure \ref{fig:slof}, data points within the backdoor region are less dense compared to clean data points, which can be measured with the density ratio. The `local outlier' (backdoor data point in this case) is sufficiently different from observations in its vicinity. In classical outlier detection literature, this can be characterized by the LOF metric \citep{breunig2000lof}, SLOF \citep{schubert2014local} and DAO \citep{anderberg2024dimensionality}. 
Using Figure \ref{fig:slof} as an example, local density outlier detection considers the radius of the $k$-neighborhood ball of the query data point (red and green circle with solid line) over the radius of the $k$-neighborhood ball of each its nearest neighbors (circles with dash line). 
A higher ratio indicates the point of interest is less dense than its neighbors and thus likely to be an outlier. 

\subsection{Detecting CLIP Backdoor Samples}
\label{sec:dao_backdoor_detection}

In this subsection, we describe how to apply local outlier detection methods to detect CLIP backdoor samples. 
At a high level, we train a model using an untrusted dataset $\mathcal{D}$, then iterate over each training data point to extract their representations. We randomly sample a batch of data from the dataset and then apply outlier detection methods. For each data point, it will produce a score $s$ to indicate its probability of being a backdoor sample. 
We present the pseudo-code in Appendix \ref{appendix:backdoor_detection}. 



Once the backdoor score is obtained for each data point, one might simply remove data that has abnormally high scores or remove a certain percentage of the data point according to the score.  
A practitioner could use the remaining safe subset to train the model from scratch with standard training to obtain a backdoor-free model or alternatively use a robust training strategy as a double security measure. The most important aspect is to accurately detect backdoor samples, which is the main focus of this work. 
Since removing too much data from the training set might degrade the performance of the model, the score for backdoor samples should be distinctively higher compared to the clean data. Otherwise, if we remove a small portion of the untrustworthy data points, it might not be sufficient to remove all poisoned samples. 


 \section{Experiments}

For all our experiments, we adopt the open-source implementation of CLIP (i.e., OpenCLIP) \citep{openclip},  ResNet-50 (RN50) \citep{he2016deep}, and ViT-B-16 \citep{dosovitskiy2021an} as the vision encoder, and choose hyperparameters following existing works \citep{carlini2022poisoning}. Details are in Appendix \ref{appendix:exp_settings}. 
We conduct our experiment on the CC3M  dataset \citep{sharma2018conceptual}. 
The evaluation is conducted on ImageNet \citep{deng2009imagenet} with the zero-shot classifier \citep{radford2021learning}.
Note that evaluation with CC3M and ImageNet is recommended by \citet{carlini2022poisoning} for studying the backdoor poisoning attacks against CLIP. We provide evaluations with a larger dataset, the CC12M \citep{changpinyo2021conceptual}, in Appendix \ref{appendix:additional_exp_results}, which shows a consistent performance for local outlier methods. 

For the CLIP single trigger backdoor attack (STBA), we follow the existing work by  \citet{carlini2022poisoning} which uses a $16 \times 16$ patch trigger \citep{gu2017badnets} and `banana' as the target label. We also explore commonly used triggers in supervised learning, including Blend with a hello kitty image\citep{chen2017targeted}, periodical signal pattern (SIG) \citep{barni2019new}, image filter with Nashville style \citep{liu2019abs}, WaNet \citep{nguyen2021wanet}, and BLTO \citep{sun2024backdoor}. Additionally, we evaluate multiple trigger setting \citep{li2024multi}, where attacker(s) can release multiple triggers (MTBA) to attack a single target or multiple targets. We use 3 triggers: the Patch, Nashville style, and WaNet. The multi-trigger settings are denoted as MT-S (single target) and MT-M (multiple targets). 
We also investigate the clean-label setting \citep{turner2018clean}, where the patch trigger is only inserted into images with a caption already containing the target. For targeted data poisoning attack (TDPA), we randomly select an image and construct the caption set with captions in the training set that contains the keyword `banana'. In terms of poisoning rate, existing work \citep{carlini2022poisoning} already demonstrates that a 0.01\% poisoning rate is sufficient for the patch trigger and TDPA on the ResNet-50. For other triggers, we conducted a coarse grid search to find the minimal poisoning rate to guarantee a high attack success rate (ASR).  

For backdoor sample detection, we evaluate local outlier detection ($\kdist$, LID, SLOF, DAO), comparing it with other backdoor data detection methods and the classical global outlier detection method isolation forest (iForest) \citep{liu2008isolation}. Note that since most of the existing detection methods are based on supervised learning, we only include the state-of-the-art methods that are applicable to CLIP, including ABL \citep{li2021anti} and Cognitive Distillation (CD) \citep{huang2023distilling}. 
For LID outlier detection \citep{houle2018correlation}, we use the maximum likelihood estimation as estimator \citep{levina2004maximum}.
We also compare with the backdoor data detection components in SafeCLIP \citep{yang2023better}, and follow their hyperparameter setting. 
On the same hardware setting, it would take 15 minutes for local outlier methods to run the detection, while CD costs 11.2 hours and ABL takes 4.1 hours. See Appendix \ref{appendix:exp_settings} for more details on the experimental settings. We use the area under the ROC curve (AUC) as the main performance metric. The AUC can be seen as the probability a backdoor sample has a higher score than a normal sample.
We provide the analysis on the sensitivity to $k$ for all local outlier methods in Appendix \ref{appendix:k}. It shows that $\kdist$, SLOF, and DAO are robust to different values of $k$. Additional results using the false positive rate at 95\% true positive rate (FPR@95) in Appendix \ref{appendix:additional_exp_results}, demonstrating findings consistent with those presented in this section.
 \citet{yang2023better} discussed that SafeCLIP is not robust to poisoning rates higher than 0.5\%. In Appendix \ref{appendix:pr}, we demonstrate that local outlier detection methods can consistently identify poisoned data even with a poisoning rate of up to 10\%.

\subsection{Detection Performance Evaluation}
\label{sec:detection_performance}

Table \ref{table:auroc} compares the detection performance of different outlier detection methods and shows among all, the local outlier detection methods, $\kdist$, SLOF, and DAO are the most effective and efficient, even compared to dedicated backdoor sample detection methods ABL, CD and SafeCLIP.
The iForest also achieves a non-trivial performance. The LID detection, however, is not robust to different triggers and architectures. 
Overall, the results indicate that CLIP backdoor samples are indeed evident outliers in the representation space and can be effectively detected.

\begin{table}[!hbt]
\centering
\caption{Comparing the AUC (\%) of different outlier detection methods against different backdoor attacks. The poisoning rates are minimized based on a coarse grid search to guarantee a non-trivial attack success rate (ASR). Clean Acc (CA) and ASR are measured by the top-1 zero-shot accuracy (\%) on ImageNet. The best results are \textbf{boldfaced}.}
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{@{}c|c|c|c|cc|cccc|cccc@{}}
\toprule
\begin{tabular}[c]{@{}c@{}}Vision\\ Encoder\end{tabular} & \begin{tabular}[c]{@{}c@{}}Threat \\ Model\end{tabular} & Trigger & \begin{tabular}[c]{@{}c@{}}Poisoning \\ Rate (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}CA\\ (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}ASR\\ (\%)\end{tabular} & ABL & CD & \begin{tabular}[c]{@{}c@{}}Safe\\ CLIP\end{tabular} & LID & iForest & $\kdist$ & SLOF & DAO \\ \midrule
\multirow{10}{*}{RN50} & \multirow{7}{*}{STBA} & Patch & 0.01 & 17.0 & 100.0 & 27.86 & 97.17 & 83.42 & 99.29 & 99.73 & 99.75 & \textbf{99.86} & \textbf{99.86} \\
 &  & Clean Label & 0.07 & 17.1 & 95.0 & 63.50 & 48.68 & 46.93 & 88.23 & 94.01 & 96.75 & \textbf{97.10} & 97.06 \\
 &  & Nashville & 0.1 & 16.7 & 78.7 & 61.69 & 98.33 & 46.37 & 61.07 & 99.35 & 99.51 & 99.61 & \textbf{99.62} \\
 &  & WaNet & 0.1 & 16.2 & 83.8 & 56.07 & 99.19 & 85.82 & 57.07 & 99.55 & 99.82 & \textbf{99.85} & \textbf{99.85} \\
 &  & Blend & 0.1 & 16.8 & 75.9 & 60.07 & 99.64 & 57.01 & 54.94 & 99.80 & 99.82 & \textbf{99.88} & \textbf{99.88} \\
 &  & SIG & 0.1 & 16.3 & 67.3 & 56.88 & 99.06 & 82.15 & 54.03 & 99.62 & 99.67 & \textbf{99.69} & \textbf{99.69} \\
 &  & BLTO & 0.1 & 16.7 & 98.3 & 58.88 & 97.60 & 84.25 & 43.04 & 99.81 & 99.85 & \textbf{99.86} & \textbf{99.86} \\ \cmidrule(l){2-14} 
 & MTBA & MT-S & 0.1 & 16.5 & 79.5 & 47.32 & 99.34 & 80.98 & 94.87 & 99.59 & 99.59 & 99.66 & \textbf{99.67} \\
 &  & MT-M & 0.1 & 16.2 & 74.7 & 53.24 & 95.33 & 78.10 & 97.11 & 98.32 & 98.50 & 98.74 & \textbf{98.76} \\ \cmidrule(l){2-14} 
 & TDPA & - & 0.01 & 16.8 & 100.0 & 55.37 & \textbf{99.99} & 81.71 & 80.60 & 99.95 & 99.96 & 99.96 & 99.96 \\ \midrule
\multirow{9}{*}{\begin{tabular}[c]{@{}c@{}}ViT\\ B-16\end{tabular}} & \multirow{7}{*}{STBA} & Patch & 0.1 & 15.2 & 99.8 & 29.19 & 8.40 & 85.88 & 45.36 & 88.39 & \textbf{98.48} & 96.42 & 95.27 \\
 &  & Clean Label & 0.07 & 15.7 & 19.0 & 57.78 & 50.30 & 44.24 & 70.28 & 54.73 & 69.22 & \textbf{71.48} & 70.82 \\
 &  & Nashville & 0.1 & 15.7 & 41.4 & 58.91 & 95.36 & 61.77 & 25.52 & 92.41 & \textbf{99.06} & 97.71 & 96.83 \\
 &  & WaNet & 0.1 & 15.2 & 12.2 & 34.53 & 45.69 & 84.60 & 38.65 & 89.81 & \textbf{98.45} & 96.59 & 95.59 \\
 &  & Blend & 0.1 & 15.7 & 95.8 & 65.43 & 96.36 & 47.57 & 12.92 & 89.24 & \textbf{99.68} & 88.43 & 84.50 \\
 &  & SIG & 0.1 & 15.3 & 82.9 & 57.20 & 80.05 & 61.35 & 17.60 & 89.82 & \textbf{99.33} & 97.33 & 96.06 \\ 
 &  & BLTO & 0.1 & 6.4 & 13.2 & 29.19 & 77.87 & 83.39 & 88.84 & 78.26 & 91.24 & \textbf{94.36} & 94.34 \\
 \cmidrule(l){2-14} 
 & \multirow{2}{*}{MTBA} & MT-S & 0.1 & 15.3 & 28.5 & 23.85 & 61.89 & 82.05 & 73.16 & 87.66 & 95.50 & 96.59 & \textbf{96.61} \\
 &  & MT-M & 0.1 & 15.2 & 36.2 & 30.29 & 51.70 & 79.64 & 77.92 & 73.11 & 78.28 & 86.16 & \textbf{86.81} \\ \cmidrule(l){2-14} 
 & TDPA & - & 0.01 & 15.5 & 100.0 & 58.79 & 54.63 & 92.03 & 79.00 & 81.88 & 95.71 & \textbf{98.51} & 98.16 \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{table:auroc}
\end{table}

Compared to SafeCLIP, local outlier methods are more robust to different trigger types. SafeCLIP can achieve 83\% to 85\% on some triggers (Patch, WaNet, and SIG), local outlier methods can consistently reach 97\% to 99\% for various attacks. Interestingly, compared to ViT-B-16, RN50 shows slightly better detection performance for local outlier methods. In practice, since the defender controls the training process, they can use RN50 for detection to remove potentially poisoned data from the dataset, after which the purified dataset can be used to train any other encoders.

Comparing the two dedicated backdoor sample detection methods for supervised learning, ABL and CD, the latter one shows a considerably better performance. However, it performs badly against clean-label attacks using ResNet-50, with only 48.68\% AUC. A possible explanation is that in a dirty-label setting, all the captions are changed according to the template, and the model learns a strong signal (keyword) that behaves similarly to supervised learning. In a clean-label setting, however, the captions are unchanged, and the model learns a diverse set of captions. This difference might affect the optimization of the mask for CD. 
ABL uses sample-specific loss values for the detection score, e.g., a lower loss value indicating a backdoor. However, in every iteration of contrastive learning, the data points are sampled randomly. 
Thus, there could be noise in the loss as the distance to negative pairs depends on the randomly sampled data. 


\subsection{Outlier Filtering as a Defense}
\label{sec:filtering_defence}
It has been theoretically shown that robust learning on an untrustworthy dataset is equivalent to the effective detection and removal of poisoned data points \citep{manoj2021excess}. 
Given accurate detection, this simple strategy can effectively mitigate the threat of backdoor attack.
However, determining how many samples to remove is a challenging question.
Here, we experiment to remove 10\% of the data from a poisoned CC3M dataset according to the detection score using DAO. To test the defense effect of filtering, we retrain the CLIP model from scratch on the remaining 90\% of the purified data. We plot the distributions of the DAO scores of backdoor vs. clean samples in Figure \ref{fig:dao_scores}. The `Threshold' line marks the 10\% cutoff point, where samples on the left side of the line will be kept while those on the right will be removed from the dataset. The score distributions of other detection metrics are provided in Appendix \ref{appendix:additional_exp_results}. 
The performance and robustness of the retrained model are reported in Table \ref{table:remove_and_retrain}. 

\begin{table}[!hbt]
\centering
\caption{Defense performance of backdoor sample filtering using DAO for filtering rate 10\% on poisoned CC3M. The results are presented in the form of clean zero-shot accuracy (\%) / attack success rate (\%) on the ImageNet validation set. Results are based on the ResNet-50 as the vision encoder. }
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{@{}c|c|c|c|c|c|c|c|c|c@{}}
\toprule
Dataset & Patch & Clean Label & Nashville & WaNet & Blend & SIG & MT-S & MT-M & TDPA \\ \midrule
Poisoned & 17.0 / 100.0 & 17.1 / 95.0 & 16.7 / 78.7 & 16.2 / 83.8 & 16.8 / 75.9 & 16.3 / 67.3 & 16.5 / 79.5 & 16.2 / 74.7 & 16.8 / 100.0 \\
Purified & 16.2 / 0.0 & 16.7 / 0.0 & 16.1 / 9.6 & 16.7 / 0.5 & 15.8 / 0.6 & 16.4 / 0.2 & 16.7 / 0.6 & 16.4 / 15.00 & 16.3 / 0.0 \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{table:remove_and_retrain}
\end{table}
\begin{figure}[!hbt]
    \vspace{-0.2in}
	\centering
	\begin{subfigure}[b]{0.16\linewidth}
	\includegraphics[width=\textwidth]{Figures/badnets_DAO_score_dist.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.16\linewidth}
	\includegraphics[width=\textwidth]{Figures/clean_label_DAO_score_dist.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.16\linewidth}
	\includegraphics[width=\textwidth]{Figures/nashville_DAO_score_dist.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.16\linewidth}
	\includegraphics[width=\textwidth]{Figures/wanet_DAO_score_dist.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.16\linewidth}
	\includegraphics[width=\textwidth]{Figures/blend_DAO_score_dist.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.16\linewidth}
	\includegraphics[width=\textwidth]{Figures/SIG_DAO_score_dist.pdf}
	\caption{SIG}
	\end{subfigure}
	\caption{
        The distributions of the DAO detection score on poisoned CC3M using ResNet-50 as the vision encoder.
    }
    \vspace{-0.1in}
    \label{fig:dao_scores}
\end{figure}

As shown in Table \ref{table:remove_and_retrain}, removing outlier data points can significantly reduce the ASR to less than 1\%, except for Nashville and MT-M, which are slightly higher.
This defense effectiveness is somewhat expected, as it shows in Figure \ref{fig:dao_scores}  that most backdoor samples are separable from the clean ones. 
Interestingly, filtering 10\% of the data points does not significantly affect the clean performance.
Additional results for zero-shot classification and linear probing on other datasets are in Appendix \ref{appendix:additional_filtering_results}. They are consistent with the results shown in this section. 
This indicates the existence of a considerable proportion of noisy data and anomalies in web-crawled datasets like CC3M. 
Through the above experiment, we highlight the necessity and benefit of purifying a large-scale web dataset using local outlier detectors like SLOF and DAO.
In Figure \ref{fig:filtering_badnets}, we present the sensitivity of the defense performance to different filtering percentages. For the patch trigger, removing 1\% is sufficient to mitigate the backdoor threat. Additional results for other triggers are in Appendix \ref{appendix:additional_filtering_results}. They show that removing 5\% is sufficient to mitigate the backdoor threat of different kinds of triggers. 
In practice, the defender can dynamically adjust the threshold to achieve a suitable performance-security trade-off. Alternatively, backdoor detection can be combined with a robust training strategy \citep{yang2023robust,yang2023better} to train on the purified (safe) and removed (risky) subsets.




\subsection{Detecting Unintentional Backdoors in CC3M}
\label{sec:detect_clean_dataset}



With the local outlier detectors, we show that they can be applied to detect noisy samples and even unintentional backdoor attacks from the CC3M dataset \citep{sharma2018conceptual}. 
We extract the representation of each CC3M image using our pre-trained CLIP and rank the outlierness of the images based on their DAO outlier score. We then manually check the top-ranked images and identify two types of anomalies: 1) meaningless images and 2) suspicious images with extremely high occurrences.


\begin{figure}[!hbt]
    \vspace{-0.1in}
	\centering
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/filtering_percentage_BadNets.pdf}
	\caption{Patch}
    \label{fig:filtering_badnets}
	\end{subfigure}
	\begin{subfigure}[b]{0.2\linewidth}
	\includegraphics[width=\textwidth]{Figures/not_available.pdf}
	\caption{Unavailable image}
    \label{fig:example_not_available}
	\end{subfigure}
    \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\textwidth]{Figures/birthday_cake_trigger_example.pdf}
	\caption{Our pre-trained}
    \label{fig:birthday_cake_trigger_example}
	\end{subfigure}
    \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\textwidth]{Figures/birthday_cake_openclip_trigger_example.pdf}
	\caption{OpenCLIP}
    \label{fig:birthday_cake_openclip_trigger_example}
	\end{subfigure}
    \vspace{-0.1in}
	\caption{
        (a) Defence performance for varying filtering percentages.
        (b) An example of the unavailable images. 
        (c-d) The recovered trigger pattern of the birthday cake image on our pre-trained CLIP (b) and a model (c) released by OpenCLIP that uses ResNet-50 as the vision encoder.
    }
    % \vspace{-0.1in}
    \label{fig:clean_data_detection}
\end{figure}



One example of meaningless images is the \textit{``this image is no longer available.''} image. One example is illustrated in Figure \ref{fig:example_not_available}. Note that the captions and URLs of these images are still available, which inevitably causes a mismatch between the image content and the caption.
This is the reason why removing outliers from CC3M does not affect the CLIP's performance (see Table \ref{table:remove_and_retrain}).
This indicates that even if the dataset is not poisoned, local outlier methods could still be applied to identify and remove the noisy samples.


We noticed several ``suspicious" images that have similar content and the same caption \textit{``the birthday cake with candles in the form of number icon.''} We cannot show these examples here due to license restrictions. However, one can search online with the caption to see what these images look like. 
These images appear 798 times in the dataset, which roughly accounts for 0.03\% of the entire dataset. We suspect this image is a natural (unintentional) backdoor trigger and has been learned into models trained on the Conceptual Captions dataset. 
To confirm this conjecture, we utilize two models: 1) our pre-trained CLIP model on CC3M, and 2) one pre-trained model released by OpenCLIP \citep{openclip} which uses  ResNet-50 as the vision encoder and is trained on CC12M \citep{changpinyo2021conceptual}. 
We apply an adapted trigger recovery method based on Neural Cleanse \citep{wang2019neural} to distill a trigger pattern from both our pre-trained CLIP model and the OpenCLIP released model. The technical details of the trigger recovery method are described in Appendix \ref{appendix:trigger_synthesis}. 
We reveal the recovered triggers in Figure \ref{fig:birthday_cake_trigger_example} and \ref{fig:birthday_cake_openclip_trigger_example}, respectively.
It shows that the birthday cake trigger has been successfully recovered on both models.
The trigger recovered from our pre-trained CLIP and the OpenCLIP pre-trained model can achieve an ASR of 92.38\% and 98.92\% when attached to ImageNet test images in zero-shot classification.
This not only confirms the existence of unintentional backdoors in web-scale datasets but also their possible existence in popular open-source multi-modal models. 



\section{Conclusion}
In this work, we studied the local neighborhood characteristics of poisoning backdoor attacks on CLIP.
We revealed one unique characteristic of CLIP backdoor attacks, which is related to the sparsity of their local representation subspace caused by the low poisoning rate. 
Based on this finding, we showed that traditional local outlier detection methods like SLOF and DAO can effectively detect different types of backdoor triggers. With the detectors, one can filter out poisoning backdoor data and noisy images from the CC3M dataset, and all can be done efficiently within 15 minutes using 4 Nvidia A100 GPUs and achieve near-perfect detection performance. Finally, we showed the existence of unintentional backdoor attacks in web-crawled datasets, which have already been pre-trained into popular open-source models. Our work verifies the necessity and benefit of data purification and we hope it may help inspire further research toward secure data curation and CLIP.


\section*{Reproducibility Statement}
There are two factors that impact the reproducibility of this work. The first factor is whether it is possible to reproduce the results. To facilitate this, we will make the source code openly available. However, due to the dynamic nature of web-scale datasets, some URLs may expire, making it challenging to reproduce the exact clean accuracy. Despite this, the attack success rate and detection results will remain unaffected. In this work, we successfully reproduced the results reported by \citet{carlini2022poisoning}, except for clean accuracy, as we could not access the complete CC3M dataset. We were only able to obtain 2.3 million image-text pairs from the CC3M dataset due to expired URLs.
With the open-source code, evaluation results on the attack success rate and detection performance are fully reproducible. 

The second factor is the computational resources required. \citet{carlini2022poisoning} thoroughly investigated backdoor poisoning attacks against CLIP and recommended the best experimental setup, which we followed, using the CC3M dataset for evaluation in Section \ref{sec:detection_performance} and \ref{sec:filtering_defence}. However, pretraining still demands significant computational power, requiring approximately 100 GPU hours (on Nvidia A100) per attack. In our experiments, we expanded the evaluation beyond patch triggers used by \citet{carlini2022poisoning} to include a broader range of backdoor triggers. This extension, covering 9 types of attacks with 2 types of encoders, requires an estimated 1,800 GPU hours. The detection evaluations including baseline methods took additional 288 GPU hours ($16 \times 18$), and filtering and retraining the models required 900 GPU hours. Results presented in Appendix \ref{appendix:additional_exp_results} for running detection on CC12M requires 400 GPU hours per attack, and detection requires 64 GPU hours. We evaluated 8 backdoor attacks, which would require 3,712 GPU hours. 
Therefore, to fully reproduce the results in this paper, we estimate a total of 6,700 GPU hours would be necessary.

\section*{Acknowledgment}
Xingjun Ma is in part supported by the National Key R\&D Program of China (Grant No. 2021ZD0112804) and the National Natural Science Foundation of China (Grant No. 62276067). Sarah Erfani is in part supported by Australian Research Council (ARC) Discovery Early Career Researcher Award (DECRA) DE220100680. This research was supported by The University of Melbourne’s Research Computing Services and the Petascale Campus Initiative.




\bibliography{main}
\bibliographystyle{iclr2025_conference}

\clearpage



\appendix
\section{Backdoor Sample Detection Algorithm}
\label{appendix:backdoor_detection}


\begin{algorithm}[!ht]
\caption{Backdoor Sample Detection With Local Outlier Methods}
\label{alg:detection}
\begin{algorithmic}[1]
\State {\bfseries Input:} Vision encoder $f_{I}$, Text encoder $f_{T}$, Dataset $\mathcal{D}$, locality parameter $k$
\State{$f$ = train($f_{I}$, $f_{T}$, $\mathcal{D}$)}
\Comment{Standard training on an untrustworthy dataset.}  
\For{$i$ {\bfseries to} $length(\mathcal{D})$} 
    \State $(\bm{x}, \bm{t})$ = sample($\mathcal{D}$) \Comment{Random sample a batch of data point from the dataset}  
    \State $\bm{z}^x$ = $f_{I}(\bm{x})$ \Comment{extract vision representations}  
    \State $\bm{z}^t$ = $f_{T}(\bm{t})$ \Comment{extract text representations}  
    \State $\bm{z}$ = concatenate($\bm{z}^x$, $\bm{z}^t$)\Comment{Reference points for neighborhood selection}
    \State $z_i$ = $f(\bm{x_i})$ 
    \Comment{extract representations for the image of interest}  
    \State $s_i$ = detection$(z_i, \bm{z}, k)$ 
    \Comment{$\kdist$, SLOF, LID, or DAO}  
\EndFor
\State {\bfseries Output:} backdoor score $s$
\end{algorithmic}
\end{algorithm}

In Algorithm \ref{alg:detection} (line No.9), we use the image embedding as the query point and all other image embeddings and text embeddings as reference points. The LID has been used in detecting adversarial examples \citep{ma2018characterizing} as well as detection of backdoor data with class-wise reference points \citep{dolatabadi2022collider}, which is not possible in CLIP. 
In Algorithm \ref{alg:detection}, instead of using class-wise reference points, we use random sample points for LID.
In both works, it has been shown that a higher LID score for a query point means it is more likely to be a backdoor or adversarial example. 
Similarly, for SLOF and DAO, the higher the score, the more likely the data point is a backdoor sample. 
As a result, we directly use the LID or the outlier factor as the score to determine if the data point is poisoned. 

For all outlier detection methods and LID, we use minibatch sampling to generate scores for each data point due to efficiency. While using the entire dataset is possible, it can be prohibitively costly for large-scale datasets. An alternative approach is to treat each batch of data as a sliding window in data streams and apply techniques like iLOF \citep{pokrajac2007incremental} or MiLOF \citep{salehi2016fast}.
Existing literature suggests that minibatch sampling is sufficient to characterize the local neighborhood \citep{ma2018characterizing}. Our empirical evaluations also support this, indicating that using randomly sampled subsets as reference points is adequate for local outlier detection methods.


\section{Experiments}
In this section, we present our experimental setting in Appendix \ref{appendix:exp_settings}, the sensitivity study to the locality parameters for local outlier methods in Appendix \ref{appendix:k}, and additional filtering as a defense result in Appendix \ref{appendix:additional_filtering_results}. In Appendix \ref{appendix:trigger_synthesis}, we present the detailed descriptions for the trigger synthesis method used to obtain the results in Section \ref{sec:detect_clean_dataset}. Finally, we show the sensitivity study to a higher poisoning rate in Appendix \ref{appendix:pr} and additional results for the detection performance in Appendix \ref{appendix:additional_exp_results}. 


\subsection{Experiment Setting}
\label{appendix:exp_settings}
We conducted our experiments on Nvidia A100GPUs with PyTorch implementation. Each experiment distribution is conducted with data distributed in a parallel setting across 4 GPUs. We used automatic mixed precision due to its memory efficiency. Open-source code is available here\footnote{\href{https://github.com/HanxunH/Detect-CLIP-Backdoor-Samples}{https://github.com/HanxunH/Detect-CLIP-Backdoor-Samples}}. 

For all experiments, we chose hyperparameters following existing work \citep{carlini2022poisoning} and the open-source implementation OpenCLIP\footnote{\href{https://github.com/mlfoundations/open_clip}{https://github.com/mlfoundations/open\_clip}} \citep{openclip}. 
We use a learning rate of 0.001, with AdamW optimizer \citep{loshchilov2018decoupled}, weight decay is set to 0.2, batch size of 1024 and train for 30 epochs. We use ResNet-50 \citep{he2016deep} and ViT \citep{dosovitskiy2021an} for the image encoder and transformer \citep{vaswani2017attention} for the text encoder. The embedding dimension is set to 1024 for ResNet-50 and 512 for ViT. We use the same data augmentation as the implementation by OpenCLIP.
We conduct our experiment on CC3M \citep{sharma2018conceptual} dataset. Due to the expired and invalid links, we only obtained 2.3M image-text pairs, so the reproduced clean performance is slightly lower than the result reported by OpenCLIP. This is normal as the number of data can significantly affect the performance of CLIP \citep{radford2021learning}. 
The evaluation is conducted on ImageNet \citep{deng2009imagenet} with a zero-shot classifier using the prompt template \citep{radford2021learning}.

For backdoor triggers, we use a $16 \times 16$ patch with interleaving black and white pixels. In addition to this patch trigger, we evaluate several commonly used triggers in supervised backdoor studies, including Blend with a hello kitty image\citep{chen2017targeted}, periodical signal pattern (SIG) \citep{barni2019new}, image filter with Nashville style \citep{liu2019abs}, and WaNet \citep{nguyen2021wanet}. We also evaluate multiple trigger settings \citep{li2024multi}, where attackers can deploy multiple triggers (MTBA) to target a single entity or multiple entities. Specifically, we use three triggers: the Patch, Nashville style, and WaNet. The multi-trigger settings are denoted as MT-S (single target) and MT-M (multiple targets).
Additionally, we investigate the clean-label setting \citep{turner2018clean}, where the patch trigger is inserted only into images whose captions already contain the target. 
An example of trigger patterns used in the experiments is shown in Figure \ref{fig:triggers}. 


\begin{figure}[!hbt]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/trigger_example.pdf}
    \caption{Examples of the 5 different triggers used in the experiments. The patch and clean label attacks use a 16 by 16 patch as the trigger. The clean label attack only applies the trigger to images with captions that contain the keyword specified by the adversary. The Nashville converts the image using the filter template ``Nashville." The WaNet applies grided noise to the image. The SIG uses a periodical pattern as the trigger. The blend attack creates an overly transparent Hello Kitty image. }
    \label{fig:triggers}
\end{figure}

For the targeted data poisoning attack (TDPA), we randomly select an image and construct the caption set using captions from the training set that contain the target keyword. We chose `banana' as the target for all attacks because it is an ImageNet class and appears frequently in CC3M.

For backdoor detection, we evaluate local outlier detection and compare it with other backdoor data detection methods. Note that since the existing detection methods are based on supervised learning, we only include the state-of-the-art methods that are feasible in SSL, including ABL \citep{li2021anti} and Cognitive Distillation (CD) \citep{huang2023distilling}. For CD, we find that in SSL, backdoor data has a higher $L_1$ norm of the mask instead of lower in a supervised setting.  As a result, we use a higher $L_1$ norm of the mask to indicate it is more likely to be a backdoor data. 
We use 100 optimization steps, $\alpha$ set to 0.001 and $\beta$ set to 100 for CD.
For ABL, we use the average loss value for the first 10 epochs for each sample. 
For iForest \citep{liu2008isolation}, we set the number of trees in the ensemble to 100. 
For LID, the $k$ is set to 16. For $\kdist$, SLOF and DAO, the $k$ is set to 16. We use batch size of 2048 for running the detection algorithm. 
For SafeCLIP \citep{yang2023better}, we follow the same hyperparameter setting suggested by the original paper. For warmup training, we perform 5 epochs of uni-modal training followed by 1 epoch of multi-modal training. The learning rate is set to $5\times10^{-6}$. 
The time cost for each detection method on CC3M with the same hardware setting is in Table \ref{tab:time_cost}. For all methods, the time cost does not include pre-training time for obtaining the initial model for extracting the representation. 


\begin{table}[!ht]
\centering
\caption{Time cost measures the wall time (in hours) for running each detection method on the same hardware. Results are based on ResNet-50 as the vision encoder on CC3M dataset.}
\begin{tabular}{@{}cccccccc@{}}
\toprule
ABL & CD & SafeCLIP & LID & iForest & $\kdist$ & SLOF & DAO \\ \midrule
4.1 & 11.2 & 0.2 & 0.2 & 0.3 & 0.2 & 0.2 & 0.2 \\ \bottomrule
\end{tabular}
\label{tab:time_cost}
\end{table}

\subsection{Sensitivity to the Locality}
\label{appendix:k}
Local outlier methods rely on the locality hyper-parameter $k$ to determine the neighborhood size and could be sensitive to the hyper-parameter. 
Here, we analyze the sensitivity of different local outlier detection methods to locality $k$ by fixing the batch size to 2048 while testing varying $k$ in $[16,32,64,128,256]$. 
Note that setting $k$ too large is not ideal as it will break the assumption of locality. Therefore, we only examine $k$ up to 256. We plot the detection AUC results in Figure \ref{fig:varying_k}.
It is evident that the detection performance of  $\kdist$, SLOF, and DAO is much more stable than LID. 


\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/auroc_over_k_badnets.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/auroc_over_k_clean_label.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/auroc_over_k_nashville.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/auroc_over_k_wanet.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/auroc_over_k_sig.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/auroc_over_k_blend.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/auroc_over_k_multi_single_badnets_nashville_wanet.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/auroc_over_k_multi_multi_badnets_nashville_wanet.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
       The detection AUC (\%) of different local outlier methods under varying locality $k$. The batch size is set to 2048 for all experiments. 
    }
    \label{fig:varying_k}
\end{figure}


\subsection{Additional Filtering Results}
\label{appendix:additional_filtering_results}
We present zero-short evaluation results on 7 commonly used datasets, including CIFAR \citep{krizhevsky2009learning}, Food101 \citep{bossard2014food}, GTSRB \citep{stallkamp2012man}, ImageNet \citep{deng2009imagenet}, StanfordCars (Cars) \citep{krause2013collecting}, STL10 \citep{coates2011analysis}, in Table \ref{tab:zero_shot_full}. We follow the standard zero-shot classification setup and use the template provided by \citet{radford2021learning} for each evaluation dataset. We also report the linear probing performance on these datasets in Table \ref{tab:lp_full}. 
Results in Tables \ref{tab:zero_shot_full} and \ref{tab:lp_full} are consistent with Section \ref{sec:filtering_defence}. Removing 10\% of the samples on CC3M using DAO does not affect the performance of the model.

Table \ref{tab:compare_robust_train} compares filtering-based defense strategies with robust training methods, including RoCLIP \citep{yang2023robust} and SafeCLIP \citep{yang2023better}. Both methods use unimodal self-supervised objectives alongside the image-text contrastive objective, which is known to enhance CLIP’s performance \citep{li2022supervision}. For a fair comparison, we also included self-supervised objectives with nearest neighbors (denoted as CLIP+NN) during retraining on the purified subset.
As shown in Table \ref{tab:compare_robust_train}, retraining CLIP on the purified subset with its original objective effectively mitigates backdoor threats. Incorporating self-supervised objectives further enhances performance and strengthens defenses against backdoor attacks.
Note that once potentially poisoned data is removed, any robust retraining method, such as RoCLIP or SafeCLIP, can be applied. The backdoor detection technique can be integrated with other defense strategies.

\begin{table}[!hbt]
\caption{Performance of backdoor sample filtering using DAO with filtering rate 10\% on poisoned CC3M. The results are presented in the form of clean zero-shot accuracy (\%) on the 7 validation set. Results are based on the ResNet-50 as the vision encoder.}
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{@{}c|c|ccccccc|c@{}}
\toprule
Attack & Dataset & CIFAR10 & CIFAR100 & FOOD101 & GTSRB & ImageNet & Cars & STL10 & Average \\ \midrule
\multirow{2}{*}{BadNets} & Poisoned CC3M & 30.7 & 10.7 & 10.5 & 6.2 & 17.0 & 1.4 & 75.4 & 21.7 \\
 & Purified CC3M & 32.5 & 10.9 & 10.8 & 9.0 & 16.2 & 1.1 & 74.2 & 22.1 \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Clean \\ Label\end{tabular}} & Poisoned CC3M & 36.6 & 12.1 & 11.2 & 6.3 & 17.2 & 1.1 & 68.6 & 21.9 \\
 & Purified CC3M & 37.7 & 12.2 & 11.2 & 4.7 & 16.7 & 1.0 & 74.6 & 22.6 \\ \midrule
\multirow{2}{*}{Nashville} & Poisoned CC3M & 37.4 & 11.6 & 10.4 & 8.2 & 16.7 & 1.3 & 71.9 & 22.5 \\
 & Purified CC3M & 36.3 & 12.4 & 11.1 & 6.0 & 16.1 & 1.0 & 67.0 & 21.4 \\ \midrule
\multirow{2}{*}{WaNet} & Poisoned CC3M & 26.4 & 11.4 & 11.1 & 5.2 & 16.3 & 1.0 & 73.0 & 20.6 \\
 & Purified CC3M & 30.5 & 11.0 & 9.8 & 4.8 & 16.7 & 1.1 & 69.3 & 20.5 \\ \midrule
\multirow{2}{*}{Blend} & Poisoned CC3M & 31.5 & 13.5 & 10.0 & 4.8 & 16.8 & 1.4 & 72.3 & 21.5 \\
 & Purified CC3M & 29.3 & 13.4 & 10.2 & 5.3 & 15.9 & 1.0 & 72.0 & 21.0 \\ \midrule
\multirow{2}{*}{SIG} & Poisoned CC3M & 31.2 & 13.2 & 11.9 & 5.9 & 16.3 & 1.2 & 73.6 & 21.9 \\
 & Purified CC3M & 29.4 & 10.8 & 10.1 & 6.5 & 16.4 & 1.0 & 73.0 & 21.0 \\ \midrule
\multirow{2}{*}{MT-S} & Poisoned CC3M & 36.5 & 11.6 & 10.9 & 4.6 & 16.6 & 1.1 & 72.3 & 21.9 \\
 & Purified CC3M & 32.8 & 11.4 & 12.0 & 5.3 & 16.7 & 1.5 & 72.3 & 21.7 \\ \midrule
\multirow{2}{*}{MT-M} & Poisoned CC3M & 30.6 & 10.9 & 11.4 & 4.7 & 16.2 & 1.0 & 74.0 & 21.2 \\
 & Purified CC3M & 32.2 & 12.1 & 12.5 & 8.3 & 16.4 & 1.2 & 72.9 & 22.2 \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:zero_shot_full}
\end{table}



\begin{table}[!hbt]
\caption{Performance of backdoor sample filtering using DAO with filtering rate 10\% on poisoned CC3M. The results are presented in the form of clean linear prob accuracy (\%) on the 7 validation set. Results are based on the ResNet-50 as the vision encoder.}
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{@{}c|c|ccccccc|c@{}}
\toprule
Attack & Dataset & CIFAR10 & CIFAR100 & FOOD101 & GTSRB & ImageNet & Cars & STL10 & Average \\ \midrule
\multirow{2}{*}{BadNets} & Poisoned CC3M & 75.0 & 51.3 & 54.8 & 67.3 & 49.2 & 17.00 & 90.6 & 57.9 \\
 & Purified CC3M & 75.5 & 51.5 & 53.4 & 69.4 & 48.3 & 16.6 & 89.1 & 57.7 \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Clean \\ Label\end{tabular}} & Poisoned CC3M & 75.0 & 51.5 & 54.5 & 65.9 & 49.1 & 16.5 & 89.8 & 57.4 \\
 & Purified CC3M & 74.0 & 51.7 & 54.2 & 69.2 & 48.2 & 16.2 & 89.8 & 57.6 \\ \midrule
\multirow{2}{*}{Nashville} & Poisoned CC3M & 74.2 & 52.5 & 54.2 & 66.8 & 49.2 & 15.8 & 90.0 & 57.5 \\
 & Purified CC3M & 75.2 & 51.2 & 53.8 & 68.0 & 48.2 & 16.7 & 89.7 & 57.5 \\ \midrule
\multirow{2}{*}{WaNet} & Poisoned CC3M & 75.0 & 51.8 & 54.3 & 67.0 & 49.0 & 16.2 & 90.1 & 57.6 \\
 & Purified CC3M & 74.8 & 51.4 & 54.5 & 64.6 & 48.4 & 16.1 & 89.9 & 57.1 \\ \midrule
\multirow{2}{*}{Blend} & Poisoned CC3M & 75.4 & 51.7 & 54.4 & 66.5 & 49.5 & 16.6 & 90.2 & 57.8 \\
 & Purified CC3M & 74.4 & 51.6 & 53.6 & 66.0 & 48.5 & 16.5 & 90.0 & 57.2 \\ \midrule
\multirow{2}{*}{SIG} & Poisoned CC3M & 75.1 & 51.6 & 54.2 & 67.1 & 49.0 & 16.1 & 90.8 & 57.7 \\
 & Purified CC3M & 73.5 & 50.9 & 53.9 & 64.7 & 48.1 & 16.4 & 89.9 & 56.8 \\ \midrule
\multirow{2}{*}{MT-S} & Poisoned CC3M & 74.2 & 50.0 & 54.4 & 66.4 & 49.2 & 16.6 & 90.0 & 57.3 \\
 & Purified CC3M & 68.0 & 48.5 & 47.1 & 63.1 & 46.6 & 16.3 & 90.2 & 54.2 \\ \midrule
\multirow{2}{*}{MT-M} & Poisoned CC3M & 75.3 & 51.8 & 54.7 & 68.0 & 49.3 & 15.3 & 90.0 & 57.8 \\
 & Purified CC3M & 68.9 & 50.8 & 52.3 & 69.7 & 47.7 & 17.5 & 89.7 & 56.7 \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:lp_full}
\end{table}


In Figure \ref{fig:filtering_full}, we show the CA and ASR with varying filtering percentages. It can be observed that removing different percentages of samples does not significantly affect the CA. Removing 5\%--10\% samples according to the backdoor scores can effectively mitigate the backdoor threat. In practice, the practitioner could adjust the threshold dynamically to achieve the best performance-security trade-off.

\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/filtering_percentage_BadNets.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/filtering_percentage_CleanLabel.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/filtering_percentage_Nashville.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/filtering_percentage_WaNet.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/filtering_percentage_Blend.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/filtering_percentage_SIG.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/filtering_percentage_Multi_Single.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/filtering_percentage_Multi_Multi.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
    The attack success rate (ASR) and clean accuracy (CA) were evaluated using zero-shot classifications on ImageNet with varying filtering percentages. Results are based on ResNet-50 as the vision encoder. 
    }
    \label{fig:filtering_full}
\end{figure}


\begin{table}[!hbt]
\centering
\caption{Defense performance of backdoor sample filtering using DAO for filtering rate 10\% on poisoned CC3M. The results are presented in the form of clean zero-shot accuracy (\%) / attack success rate (\%) on the ImageNet validation set. Results are based on the ResNet-50 as the vision encoder. The best results in terms of clean zero-shot accuracy and attack success rate are in \textbf{boldface}.}
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{@{}c|c|c|c|c|c|c|c|c|c|c@{}}
\toprule
Dataset & Method & Patch & Clean Label & Nashville & WaNet & Blend & SIG & MT-S & MT-M & TDPA \\ \midrule
\multirow{3}{*}{Poisoned} & CLIP & 17.0 / 100.0 & 17.1 / 95.0 & 16.7 / 78.7 & 16.2 / 83.8 & 16.8 / 75.9 & 16.3 / 67.3 & 16.5 / 79.5 & 16.2 / 74.7 & 16.8 / 100.0 \\
 & RoCLIP & 12.8 / 0.1 & 12.7 / \textbf{0.0} & 12.5 / 14.6 & 12.6 / 13.4 & 12.2 / 51.4 & 12.3 / 48.2 & 12.8 / 2.0 & 12.2 / 13.9 & 15.2 / 100.0 \\
 & SafeCLIP & 17.2 / \textbf{0.0} & 17.0 / 19.9 & 16.7 / 54.5 & 17.4 / 9.4 & \textbf{17.6} / 53.6 & 16.5 / 68.7 & 16.8 / 32.6 & 17.1 / 30.9 & 17.2 / 100.0 \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Purified\\ (Ours)\end{tabular}} & CLIP & 16.2 / \textbf{0.0} & 16.7 / \textbf{0.0} & 16.1 / 9.6 & 16.7 / 0.5 & 15.8 / 0.6 & 16.4 / 0.2 & 16.7 / 0.6 & 16.4 / 15.00 & 16.3 / \textbf{0.0} \\
 & CLIP+NN & \textbf{17.4} / \textbf{0.0} & \textbf{17.5} / \textbf{0.0} & \textbf{17.4} / \textbf{0.3} & \textbf{17.6} / \textbf{0.1} & 17.5 / \textbf{0.0} & \textbf{17.6} / \textbf{0.1} & \textbf{17.9} / \textbf{0.1} & \textbf{17.6} / \textbf{0.2} & \textbf{17.7} / \textbf{0.0} \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:compare_robust_train}
\end{table}

\subsection{Trigger Synthesis}
\label{appendix:trigger_synthesis}
In this subsection, we present the trigger synthesis method we used to recover the trigger on the birthday cake images that are presented in Section \ref{sec:detect_clean_dataset}. 
Since existing methods are designed for supervised learning, we need to make adaptations in order to use them for CLIP. Inspired by Neural Cleanse \citep{wang2019neural} and Cognitive Distillation \citep{huang2023distilling}, we use the following objective: 
\begin{align}
\argmin_{\bm{m},\bm{\Delta}} & \,\, \simil(f_I(\xx'), \bm{z}^t) + \alpha \norm{\bm{m}}_1 + \beta (TV(\bm{m}) + TV(\bm{\Delta}))
\\
\xx' & = \bm{m} \odot \Delta + (1-\bm{m}) \odot \xx, 
\end{align}
where $\bm{m} \in [0,1]^{w\times h}$ is a learnable 2D input mask that does not include the color channels, $\Delta \in [0,1]^{3 \times w \times h}$ is the trigger pattern, $\odot$ is the element-wise multiplication applied to all the channels, $TV(\cdot)$ is the total variation loss, $\bm{z}^t=f_T(t)$ is the embedding of the target caption, $f_I$ is the image encoder and $\simil(\cdot)$ is the similarity measure. 


For the birthday cake images, the target caption is ``\textit{the birthday cake with candles in the form of number icon.}'' We perform the trigger synthesis using the equations above on the CC3M dataset and run for optimization 250 steps, $\alpha$ is set to 0.0001, and  $\beta$ to 70. We use Adam \citep{kingma2014adam} as the optimizer for $\bm{m}$ and $\bm{\Delta}$, the learning rate is set to 0.05, $\beta_1$ and $\beta_2$ are set to 0.1. $\bm{m}$ is initialized using 1, and $\bm{\Delta}$ is initialized using a birthday cake example. 


\begin{figure}[!hbt]
	\centering
    \begin{subfigure}[b]{0.22\linewidth}
    \includegraphics[width=\textwidth]{Figures/great_white_shark_trigger_example.pdf}
	\caption{ASR is 45.37\%}
	\end{subfigure}
    \begin{subfigure}[b]{0.22\linewidth}
    \includegraphics[width=\textwidth]{Figures/great_white_shark_trigger_example_2.pdf}
	\caption{ASR is 13.09\%.}
	\end{subfigure}
	\caption{
        (a-b) The synthesized patterns with ``a photo of great white shark'' as the target caption. 
    }
    \label{fig:great_white_shark}
\end{figure}

The trigger synthesis might appear to be similar to a targeted universal adversarial attack and might not validate that the birthday cake is a real backdoor. To address this, we provide a counter-example. This trigger synthesis is not effective in creating a strong targeted-universal adversarial attack.  We use one of the ImageNet classes as the target and the prompt template ``a photo of \{target\}'' as the target caption. We repeat the experiment presented in the main paper with the exact same hyperparameters except the $\Delta$ initialized with random values. We conducted this experiment twice. The recovered triggers are shown in Figure \ref{fig:great_white_shark}. 
These triggers only achieve an ASR of 45.37\% and 13.09\%. Not as high as the birthday cake example (92.38\%). The high ASR of the birthday cake example makes it highly susceptible to being a backdoor trigger. 





\subsection{Sensitivity to Poisoning Rates}
\label{appendix:pr}

In this subsection, we examine the sensitivity of local outlier detection methods to varying poisoning rates. The results are presented in Table \ref{tab:sensitivity_to_pr}. With our default setting of $k=16$, $\kdist$ shows the most robustness against changes in the poisoning rate. SLOF and DAO remain relatively stable up to a 5\% poisoning rate. When the poisoning rate significantly increases to 10\%, increasing the locality $k$ to 256 substantially improves performance compared to $k=16$. This is expected since a higher poisoning rate increases the likelihood that the $k$ nearest neighbors will include poisoned samples, necessitating an adjustment in the locality $k$. 
We present the embedding space visualization with the control experiments to increase the number of backdoor samples within the batch in Figure \ref{fig:tsne_pr}. It can be observed that with an increase in the locality parameter $k$, the local outlier methods can accurately identify backdoor samples when the poisoning rate is significantly higher. 

\begin{figure}[!hbt]
	\centering
    \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\textwidth]{Figures/1024_1_representation.pdf}
	\caption{1 backdoor sample and $k=16$.}
	\end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\textwidth]{Figures/1024_30_representation_k64.pdf}
	\caption{30 backdoor samples and $k=64$.}
	\end{subfigure}
	\caption{
       The t-SNE plot of the embedding space with various numbers of backdoor samples for a batch of 1024 data points. 
    }
    \label{fig:tsne_pr}
\end{figure}


In real-world web-scale datasets, poisoning more than 5\% of the data is highly unlikely. \citet{carlini2024poisoning} indicates that while poisoning 0.01\% of a dataset is relatively inexpensive, poisoning 0.1\% would drastically increase the cost from \$10 USD to \$10,000 USD. Poisoning 5\% to 10\% of a web-scale dataset would be extremely costly. Therefore, we believe that the default choice of $k$=16 is appropriate. However, using $k$=256 as a default is also feasible, as detection performance remains stable for $k$ between 16 and 256, as demonstrated in Appendix \ref{appendix:k}. Unlike SafeCLIP \citep{yang2023better}, which identified difficulties in detecting backdoor samples when the poisoning rate exceeds 0.5\%, local outlier detection remains robust to poisoning rates as high as 10\%, although such scenarios are unrealistic in practice.



\begin{table}[!hbt]
\centering
\caption{Results of sensitivity towards poisoning rate for local methods. All results are based on the Patch trigger. Results are reported as area under the ROC curve. The default $k$ is 16.}
\begin{tabular}{c|cccc}
\toprule
Poisoning Rate & LID   & $\kdist$ & SLOF   & DAO    \\ \midrule
0.01\%         & 99.29 & 99.75  & 99.86  & 99.86  \\
0.1\%          & 69.82 & 100.00 & 100.00 & 100.00 \\
1\%            & 0.29  & 100.00 & 100.00 & 100.00 \\
5\%            & 0.01  & 100.00 & 99.87  & 99.66  \\ 
10\%           & 1.18  & 95.39  & 63.00  & 64.88  \\ 
10\% ($k$=256) & 0.00  & 100.00 & 99.99  & 99.98  \\ \bottomrule
\end{tabular}
\label{tab:sensitivity_to_pr}
\end{table}

LID is shown to be sensitive to poisoning rates. Interestingly, in the case of a 10\% poisoning rate, LID detection shows an AUC of 0.0, suggesting that all poisoned samples have a low LID score at higher poisoning rates. In contrast, at lower poisoning rates, poisoned samples tend to have higher LID scores. This indicates that LID detection is not robust to changes in the poisoning rate.




\subsection{White-box Adaptive Attacks}
\label{appendix:adaptive_attacks}

In this subsection, we provide an analysis of local outlier detection methods against white-box adaptive attacks, i.e., the attacker is aware of our detection strategy and attempts to evade our detection. We assume the attacker can control the training process to regularize the outlier scores so that the trigger is small. This is the unrealistic setting for data poisoning attacks, but such analysis could provide insights into the robustness of local outlier detection methods.

To evade the detection, the attacker may add a regularization term to the original training objective, forcing the model to generate smaller outliers for the backdoor samples. Formally, it is defined as the following:
\begin{equation*}
    \mathcal{L}_{\text{CLIP}}(\mathbf{z}^x, \mathbf{z}^t) + \text{SLOF}(\mathbf{z}^{x'}), \quad \mathbf{x}' \in \mathcal{D}_b,
\end{equation*}
where $\mathbf{z}^x$ and $\mathbf{z}^t$ are the image and text embeddings, respectively, and $\text{SLOF}(\mathbf{z}^{x'})$ denotes the Local Outlier Factor score of the backdoor-poisoned samples $\mathbf{x}'$ in the backdoor dataset $\mathcal{D}_b$. This objective allows the attacker to minimize the outlier score of the poisoned samples during pretraining. 

\begin{table}[!hbt]
\centering
\caption{Comparing the AUC (\%) of different outlier detection methods against adaptive attacks. Clean Acc (CA) and ASR are measured by the top-1 zero-shot accuracy (\%) on ImageNet. The best results are \textbf{boldfaced}.}
\begin{adjustbox}{width=0.8\linewidth}
\begin{tabular}{@{}c|c|c|cc|ccc@{}}
\toprule
Method & Trigger & Poisoning Rate & Clean Acc & ASR & $k$-dist & SLOF & DAO \\ \midrule
Standard & Patch & 0.01\% & 17.00 & 100.0 & 99.75 & \textbf{99.86} & \textbf{99.86} \\
Adaptive Attack & Patch & 0.01\% & 15.94 & 100.0 & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:adaptive_attack}
\end{table}

The results are reported in the table \ref{tab:adaptive_attack}. It clearly shows that this adaptive strategy does not circumvent our detection method; in fact, it even improves our detection performance. This is because forcing the poisoned backdoor samples to mimic the density profile of clean samples is only effective within a specific neighborhood in the feature space. To fully evade detection, an attacker would need to account for all possible neighborhoods generated by various combinations of data points—a task that is computationally infeasible given the scale of web datasets, which often contain millions or even billions of samples. Therefore, our detection method remains robust even against adaptive attacks that attempt to minimize the outlier scores of poisoned samples. This reinforces the effectiveness of our approach in real-world settings where attackers may employ sophisticated strategies to hide backdoor triggers.



\subsection{Additional Detection Results}
\label{appendix:additional_exp_results}

In this section, we present the result of FPR@95 in Table \ref{table:fpr95}, which is the false positive rate at 95\% true positive rate.
This is a complementary metric to the AUC score. In practice, it’s difficult to set a threshold that achieves exactly a 95\% true positive rate when removing backdoor data. Therefore, AUC, which represents the probability of a backdoor sample having a higher score than a clean sample, is the preferred metric for detecting backdoor samples. Nevertheless, we report the FPR@95 here. The results are consistent with Section \ref{sec:detection_performance} of the main paper, where local outlier methods consistently perform well. When using RN50 as the encoder, local outlier methods only show an FPR@95 above 1\% for Clean Label and MT-M attacks. As demonstrated in Section \ref{sec:detect_clean_dataset}, even with clean datasets, these methods detect noisy data (counted as false positives) that do not benefit pretraining. A slightly higher FPR does not impact clean data performance during removal and retraining, as demonstrated in Section \ref{sec:filtering_defence} and Appendix \ref{appendix:additional_filtering_results}.



We present the detection results for the CC12M \citep{changpinyo2021conceptual}  and RedCaps~\citep{desai2021redcaps} dataset with RN50 as an image encoder in Table \ref{table:auroc_cc12m} and \ref{table:auroc_redcaps}. It can be observed the local outlier detection consistently outperforms other baselines. The findings are consistent as in Section \ref{sec:detection_performance} in the main paper. 


We provide extended results on the distributions of backdoor scores using ABL, CD, LID, $\kdist$, Isolation Forest, and SLOF in Figures \ref{fig:abl_score_distribution}--\ref{fig:dao_score_distribution}. Results are based on using RN50 as the image encoder.


\begin{table}[!ht]
\centering
\caption{The detection performance is evaluated using the FPR@95, and results are reported with percentage (\%). The best results are \textbf{boldfaced}.}
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{@{}c|c|c|c|cc|cccc|cccc@{}}
\toprule
\begin{tabular}[c]{@{}c@{}}Vision\\ Encoder\end{tabular} & \begin{tabular}[c]{@{}c@{}}Threat \\ Model\end{tabular} & Trigger & \begin{tabular}[c]{@{}c@{}}Poisoning \\ Rate (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}CA\\ (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}ASR\\ (\%)\end{tabular} & ABL & CD & \begin{tabular}[c]{@{}c@{}}Safe\\ CLIP\end{tabular} & LID & iForest & $\kdist$ & SLOF & DAO \\ \midrule
\multirow{9}{*}{RN50} & \multirow{6}{*}{STBA} & Patch & 0.01 & 17.0 & 100.0 & 99.48 & 13.67 & 52.62 & 3.06 & 0.44 & 0.32 & \textbf{0.25} & 0.28 \\
 &  & Clean Label & 0.07 & 17.1 & 95.0 & 87.06 & 95.55 & 98.03 & 50.81 & 23.83 & 12.75 & \textbf{11.23} & 11.45 \\
 &  & Nashville & 0.1 & 16.7 & 78.7 & 87.81 & \textbf{0.24} & 84.18 & 97.98 & 0.36 & 0.29 & 0.27 & 0.27 \\
 &  & WaNet & 0.1 & 16.2 & 83.8 & 89.78 & 1.88 & 46.54 & 98.82 & 0.74 & 0.26 & \textbf{0.30} & 0.32 \\
 &  & Blend & 0.1 & 16.8 & 75.9 & 90.01 & \textbf{0.0} & 74.06 & 99.51 & 0.03 & 0.03 & 0.03 & 0.03 \\
 &  & SIG & 0.1 & 16.3 & 67.3 & 92.01 & 0.07 & 49.23 & 99.59 & 0.07 & \textbf{0.03} & 0.04 & 0.05 \\ \cmidrule(l){2-14} 
 & \multirow{2}{*}{MTBA} & MT-S & 0.1 & 16.5 & 79.5 & 94.69 & 0.51 & 50.65 & 36.80 & 0.28 & 0.23 & 0.21 & \textbf{0.20} \\
 &  & MT-M & 0.1 & 16.2 & 74.7 & 93.08 & 25.85 & 51.83 & 16.92 & 6.42 & 5.85 & 4.36 & \textbf{4.30} \\ \cmidrule(l){2-14} 
 & TDPA & - & 0.01 & 16.8 & 100.0 & 89.25 & \textbf{0.01} & 60.42 & 99.97 & 9.46 & 0.05 & 0.06 & 0.06 \\ \midrule
\multirow{9}{*}{\begin{tabular}[c]{@{}c@{}}ViT\\ B-16\end{tabular}} & \multirow{6}{*}{STBA} & Patch & 0.1 & 15.2 & 99.8 & 99.02 & 99.04 & 38.49 & 98.63 & 48.78 & \textbf{6.23} & 22.91 & 30.33 \\
 &  & Clean Label & 0.07 & 15.7 & 19.0 & 91.39 & 89.09 & 97.47 & \textbf{79.07} & 92.92 & 84.17 & 79.08 & 79.76 \\
 &  & Nashville & 0.1 & 15.7 & 41.4 & 90.52 & 25.18 & 69.41 & 99.90 & 34.64 & \textbf{1.23} & 12.77 & 20.04 \\
 &  & WaNet & 0.1 & 15.2 & 12.2 & 98.38 & 85.09 & 43.41 & 99.67 & 44.67 & \textbf{4.83} & 22.06 & 32.54 \\
 &  & Blend & 0.1 & 15.7 & 95.8 & 84.61 & 7.09 & 77.21 & 99.94 & 42.71 & \textbf{0.07} & 62.72 & 71.68 \\
 &  & SIG & 0.1 & 15.3 & 82.9 & 91.64 & 99.79 & 70.13 & 99.94 & 42.00 & \textbf{0.15} & 13.00 & 23.89 \\ \cmidrule(l){2-14} 
 & \multirow{2}{*}{MTBA} & MT-S & 0.1 & 15.3 & 28.5 & 99.74 & 93.95 & 49.03 & 90.01 & 53.58 & 28.40 & 21.60 & \textbf{20.80} \\
 &  & MT-M & 0.1 & 15.2 & 36.2 & 99.62 & 92.25 & 57.23 & 81.13 & 79.04 & 68.86 & 56.07 & \textbf{55.10} \\ \cmidrule(l){2-14} 
 & TDPA & - & 0.01 & 15.5 & 100.0 & 86.63 & 72.59 & 32.53 & 99.98 & \textbf{0.05} & 6.10 & 5.72 & 6.82 \\ \bottomrule
\end{tabular}
\label{table:fpr95}
\end{adjustbox}
\end{table}


\begin{table}[!hbt]
\centering
\caption{Results for CC12M dataset. The detection performance is evaluated using the AUC, and results are reported with percentage (\%). The best results are \textbf{boldfaced}.}
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{@{}c|c|c|cc|cccc|cccc@{}}
\toprule
\begin{tabular}[c]{@{}c@{}}Threat \\ Model\end{tabular} & Trigger & \begin{tabular}[c]{@{}c@{}}Poisoning \\ Rate (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}CA\\ (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}ASR\\ (\%)\end{tabular} & ABL & CD & \begin{tabular}[c]{@{}c@{}}Safe\\ CLIP\end{tabular} & LID & iForest & $\kdist$ & SLOF & DAO \\ \midrule
\multirow{6}{*}{STBA} & Patch & 0.01 & 27.1 & 100.0 & 48.02 & 98.34 & 88.70 & 99.27 & 99.53 & 99.37 & 99.50 & \textbf{99.54} \\
 & Clean Label & 0.07 & 27.7 & 91.5 & 62.98 & 78.29 & 40.88 & 65.12 & 86.72 & 87.86 & \textbf{89.73} & 89.66 \\
 & Nashville & 0.1 & 20.5 & 99.3 & 56.63 & 99.07 & 25.87 & 58.66 & 99.84 & 99.72 & \textbf{99.85} & \textbf{99.85} \\
 & WaNet & 0.1 & 24.8 & 92.3 & 57.66 & 98.52 & 88.34 & 52.42 & 99.62 & 99.50 & 99.66 & \textbf{99.69} \\
 & Blend & 0.1 & 26.8 & 96.0 & 58.97 & 99.65 & 30.32 & 48.46 & 99.81 & 99.82 & \textbf{99.83} & \textbf{99.83} \\
 & SIG & 0.1 & 26.1 & 61.7 & 58.65 & 99.36 & 58.14 & 49.94 & 99.61 & 99.63 & \textbf{99.67} & 99.66 \\ \midrule
\multirow{2}{*}{MTBA} & MT-S & 0.1 & 14.5 & 99.4 & 50.82 & 99.56 & 84.81 & 80.49 & 99.80 & 99.78 & \textbf{99.84} & \textbf{99.84} \\
 & MT-M & 0.1 & 14.7 & 98.9 & 57.43 & 97.47 & 74.03 & 95.24 & 99.02 & 99.15 & 99.30 & \textbf{99.33} \\ \midrule
TDPA & - & 0.01 & 26.7 & 100.0 & 46.76 & 99.60 & 86.03 & 80.69 & 99.98 & \textbf{100.0} & 99.98 & 99.98 \\ \bottomrule
\end{tabular}
\label{table:auroc_cc12m}
\end{adjustbox}
\end{table}


\begin{table}[!hbt]
\centering
\caption{Results for RedCaps dataset. The detection performance is evaluated using the AUC, and results are reported with percentage (\%). The best results are \textbf{boldfaced}.}
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{@{}c|c|c|cc|cccc|cccc@{}}
\toprule
\begin{tabular}[c]{@{}c@{}}Threat \\ Model\end{tabular} & Trigger & \begin{tabular}[c]{@{}c@{}}Poisoning \\ Rate (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}CA\\ (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}ASR\\ (\%)\end{tabular} & ABL & CD & \begin{tabular}[c]{@{}c@{}}Safe\\ CLIP\end{tabular} & LID & iForest & $\kdist$ & SLOF & DAO \\ \midrule
\multirow{6}{*}{STBA} & Patch & 0.01 & 29.9 & 96.9 & 41.01 & 93.27 & 78.05 & 93.93 & 93.55 & 93.56 & 93.74 & \textbf{93.75} \\
 & Clean Label & 0.07 & 30.4 & 94.9 & 70.15 & 57.04 & 44.66 & 78.87 & 85.41 & 87.58 & \textbf{89.25} & 89.07 \\
 & Nashville & 0.1 & 29.7 & 85.4 & 69.28 & 98.23 & 6.22 & 69.26 & 99.16 & 99.29 & \textbf{99.40} & 99.38 \\
 & WaNet & 0.1 & 28.0 & 35.8 & 64.83 & 96.53 & 76.53 & 54.3 & 99.78 & 99.84 & \textbf{99.89} & 99.88 \\
 & Blend & 0.1 & 30.0 & 98.3 & 70.34 & 99.55 & 35.57 & 61.14 & 99.86 & 99.88 & \textbf{99.93} & \textbf{99.93} \\
 & SIG & 0.1 & 29.5 & 97.9 & 69.35 & 93.57 & 68.76 & 69.48 & 99.89 & 99.90 & \textbf{99.91} & \textbf{99.91} \\ \midrule
\multirow{2}{*}{MTBA} & MT-S & 0.1 & 29.5 & 83.0 & 61.16 & 95.92 & 63.33 & 86.71 & 97.37 & 97.55 & \textbf{97.73} & \textbf{97.73} \\
 & MT-M & 0.1 & 27.7 & 83.9 & 63.43 & 90.67 & 58.98 & 94.51 & 96.38 & 96.71 & \textbf{97.04} & 97.02 \\ \midrule
TDPA & - & 0.01 & 30.0 & 100.0 & 51.71 & 99.93 & 89.66 & 81.83 & 99.71 & 99.88 & 99.95 & \textbf{99.96} \\ \bottomrule
\end{tabular}
\label{table:auroc_redcaps}
\end{adjustbox}
\end{table}


\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/badnets_ABL_score_dist.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/clean_label_ABL_score_dist.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/nashville_ABL_score_dist.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/wanet_ABL_score_dist.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/blend_ABL_score_dist.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/sig_ABL_score_dist.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_single_badnets_nashville_wanet_ABL_score_dist.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_multi_badnets_nashville_wanet_ABL_score_dist.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
        The distribution of backdoor scores using ABL on the CC3M dataset.
    }
    \label{fig:abl_score_distribution}

\end{figure}


\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/badnets_CD_score_dist.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/clean_label_CD_score_dist.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/nashville_CD_score_dist.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/wanet_CD_score_dist.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/blend_CD_score_dist.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/sig_CD_score_dist.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_single_badnets_nashville_wanet_CD_score_dist.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_multi_badnets_nashville_wanet_CD_score_dist.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
        The distribution of backdoor scores using CD on the CC3M dataset.
    }
\end{figure}


\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/badnets_CLIPScores_score_dist.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/clean_label_CLIPScores_score_dist.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/nashville_CLIPScores_score_dist.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/wanet_CLIPScores_score_dist.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/blend_CLIPScores_score_dist.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/sig_CLIPScores_score_dist.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_single_badnets_nashville_wanet_CLIPScores_score_dist.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_multi_badnets_nashville_wanet_CLIPScores_score_dist.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
        The distribution of backdoor scores using SafeCLIP on the CC3M dataset.
    }
\end{figure}


\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/badnets_LID_score_dist.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/clean_label_LID_score_dist.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/nashville_LID_score_dist.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/wanet_LID_score_dist.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/blend_LID_score_dist.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/sig_LID_score_dist.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_single_badnets_nashville_wanet_LID_score_dist.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_multi_badnets_nashville_wanet_LID_score_dist.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
        The distribution of backdoor scores using LID on the CC3M dataset.
    }
\end{figure}



\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/badnets_IsolationForest_score_dist.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/clean_label_IsolationForest_score_dist.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/nashville_IsolationForest_score_dist.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/wanet_IsolationForest_score_dist.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/blend_IsolationForest_score_dist.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/sig_IsolationForest_score_dist.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_single_badnets_nashville_wanet_IsolationForest_score_dist.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_multi_badnets_nashville_wanet_IsolationForest_score_dist.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
        The distribution of backdoor scores using iForest on the CC3M dataset.
    }
\end{figure}

\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/badnets_KDistance_score_dist.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/clean_label_KDistance_score_dist.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/nashville_KDistance_score_dist.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/wanet_KDistance_score_dist.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/blend_KDistance_score_dist.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/sig_KDistance_score_dist.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_single_badnets_nashville_wanet_KDistance_score_dist.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_multi_badnets_nashville_wanet_KDistance_score_dist.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
        The distribution of backdoor scores using $\kdist$ on the CC3M dataset.
    }
\end{figure}


\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/badnets_SLOF_score_dist.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/clean_label_SLOF_score_dist.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/nashville_SLOF_score_dist.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/wanet_SLOF_score_dist.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/blend_SLOF_score_dist.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/sig_SLOF_score_dist.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_single_badnets_nashville_wanet_SLOF_score_dist.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_multi_badnets_nashville_wanet_SLOF_score_dist.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
        The distribution of backdoor scores using SLOF on the CC3M dataset.
    }
    \label{fig:slof_score_distribution}
\end{figure}


\begin{figure}[!hbt]
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/badnets_DAO_score_dist.pdf}
	\caption{Patch}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/clean_label_DAO_score_dist.pdf}
	\caption{Clean Label}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/nashville_DAO_score_dist.pdf}
	\caption{Nashville}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/wanet_DAO_score_dist.pdf}
	\caption{WaNet}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/blend_DAO_score_dist.pdf}
	\caption{Blend}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/sig_DAO_score_dist.pdf}
	\caption{SIG}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_single_badnets_nashville_wanet_DAO_score_dist.pdf}
	\caption{MT-S}
	\end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
	\includegraphics[width=\textwidth]{Figures/multi_multi_badnets_nashville_wanet_DAO_score_dist.pdf}
	\caption{MT-M}
	\end{subfigure}
	\caption{
        The distribution of backdoor scores using DAO on the CC3M dataset.
    }
    \label{fig:dao_score_distribution}
\end{figure}




\end{document}
