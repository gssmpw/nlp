\section{Related Work}
\noindent\textbf{Backdoor Attacks.} The objective of a backdoor attack is to deceive a victim model to learn a shortcut correlation between the trigger and a targeted output. The adversary can subsequently manipulate the predictions of the victim model at the test time with the trigger. 
Based on the attackers' and defenders' capabilities, existing backdoor attacks can be categorized into \emph{data-poisoning} and \emph{training-manipulation} attacks. 
In data-poisoning attacks, the adversary injects triggers into the defender's training data, but the defender has full control of the model training. Such attacks simulate the scenario where the defender utilizes an untrusted web dataset for training.
In training manipulation attacks ____, the attacker can manipulate both the training data and the objective function to implant the trigger and then
releases the backdoored model for the victim to download. This simulates the scenario where the victim downloads pre-trained model parameters from untrusted open-source platforms. 
The focus of this work is data poisoning attacks. 

Existing backdoor attacks are mostly focused on attacking supervised learning ____. 
For poisoning attacks, the trigger pattern is one main contributing factor to the success of the attack. The trigger pattern could be a patch ____ or a blending image ____. Advanced attacks leverage more complex trigger patterns such as periodical patterns ____, natural reflections ____, physical objects ____, adversarial perturbations ____, GANs ____, Instagram filters ____, image generator ____ and image frequency perturbations ____. 
While injecting the trigger pattern into training images, the attacker could either alter the corresponding label (known as {\em dirty-label} attacks) or keep the label unchanged (known as {\em clean-label} attacks) ____.
There could also be multiple triggers released by one or more adversaries for the same dataset ____, which is a realistic setting for downloading data from untrusted sources. 

____ proposed the first poisoning backdoor attacks on CLIP with patch triggers.
Compared to supervised learning, poisoning backdoor attacks on CLIP can achieve a high attack success rate at a much lower poisoning rate (i.e., 0.01\%). 
Concurrently, targeted poisoning attack in the finetuning stage ____ and training-manipulation backdoor attacks have also been developed for CLIP ____.
The main focus of our work is detecting poisoning backdoor samples in CLIP, for which we follow the same threat model and experimental setup as ____.


\noindent\textbf{Backdoor Defense.}
Backdoor defense can be categorized into 1) trigger synthesis, 2) backdoor model detection, 3) robust training, and 4)  backdoor sample detection methods. 
Trigger synthesis aims to recover the trigger patterns used to poison and attack the victim model ____. Model detection aims to determine if a trained model contains backdoors ____. Note that trigger synthesis and model detection methods will still need backdoor removal techniques to obtain a robust model. A robust training strategy aims to (pre)train a backdoor-free model on backdoor-poisoned dataset by robustifying the training procedure of supervised learning ____, self-supervised learning ____ or CLIP ____. 

Backdoor sample detection determines if a data point is infected with the backdoor trigger. 
It can leverage either the statistics of the deep features ____, sensitivity characteristics to certain perturbations and transformations ____ or inference time detection with contrastive prompting ____. Cognitive Distillation (CD) extracts a minimal pattern that allows the model to produce the same output and uses the norm of the extracted mask to detect whether a training/test sample is backdoored ____. However, it is an optimization-based method that is time-consuming and of limited scalability for web-scale datasets.  
 Anti-Backdoor Learning (ABL) tracks sample-specific training loss during training and detects samples of the lowest loss as backdoor samples ____. 
The above defense methods were all developed under supervised learning, with many relying on the class labels to function, which is not available in CLIP.
SafeCLIP is proposed as end-to-end robust training strategy to obtain a backdoor-free model from potentially poisoned dataset ____. 
It has two components: one for detecting backdoor data and one for robust training on safe and risky subsets. 


\noindent\textbf{Outlier Detection.}
Outlier detection is a classic problem in data mining. It aims to find data points that deviate from the general distribution. It can be categorized into parametric and non-parametric approaches. The parametric approach makes explicit assumptions about the nature of the underlying data distribution ____, while the non-parametric does not. 
The non-parametric approach is more suitable for unsupervised settings such as backdoor sample detection. 
It include statistical methods ____, and ensemble methods ____ such as the isolation forest (iForest) ____. 
Local outlier methods are another type of non-parametric outlier detection methods, such as k-nearest-neighbor ____, local outlier factor (LOF) ____, and their improved versions ____. 
These methods either explicitly or implicitly assess the density in the vicinity of a query point ____ and data points with low density are usually regarded as outliers. 
 Local intrinsic dimensionality (LID) is another local measure that describes the growth rate of the number of data points in the vicinity of the query point ____. LID has been used in various machine learning-related applications ____. Notably, it is used in outlier detections ____, detecting adversarial examples ____ and backdoor samples (in a supervised setting) ____. 
In this work, based on our empirical observation, we choose to exploit simplified local outlier factor (SLOF) ____ and its extension dimensionality-aware outlier detection (DAO) ____ 
for detecting backdoor samples.