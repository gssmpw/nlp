\section{Related Work}
\noindent\textbf{Backdoor Attacks.} The objective of a backdoor attack is to deceive a victim model to learn a shortcut correlation between the trigger and a targeted output. The adversary can subsequently manipulate the predictions of the victim model at the test time with the trigger. 
Based on the attackers' and defenders' capabilities, existing backdoor attacks can be categorized into \emph{data-poisoning} and \emph{training-manipulation} attacks. 
In data-poisoning attacks, the adversary injects triggers into the defender's training data, but the defender has full control of the model training. Such attacks simulate the scenario where the defender utilizes an untrusted web dataset for training.
In training manipulation attacks \citep{lin2020composite,shumailov2021manipulating,bagdasaryan2021blind,nguyen2021wanet,doan2021lira,wang2022bppattack}, the attacker can manipulate both the training data and the objective function to implant the trigger and then
releases the backdoored model for the victim to download. This simulates the scenario where the victim downloads pre-trained model parameters from untrusted open-source platforms. 
The focus of this work is data poisoning attacks. 

Existing backdoor attacks are mostly focused on attacking supervised learning \citep{gu2017badnets,chen2017targeted,liu2018trojaning}. 
For poisoning attacks, the trigger pattern is one main contributing factor to the success of the attack. The trigger pattern could be a patch \citep{gu2017badnets} or a blending image \citep{chen2017targeted}. Advanced attacks leverage more complex trigger patterns such as periodical patterns \citep{barni2019new}, natural reflections \citep{liu2020reflection}, physical objects \citep{li2020rethinking,wenger2021backdoor}, adversarial perturbations \citep{turner2018clean,zhao2020clean}, GANs \citep{cheng2020deep}, Instagram filters \citep{liu2019abs}, image generator \citep{sun2024backdoor} and image frequency perturbations \citep{zeng2021rethinking,li2023embarrassingly}. 
While injecting the trigger pattern into training images, the attacker could either alter the corresponding label (known as {\em dirty-label} attacks) or keep the label unchanged (known as {\em clean-label} attacks) \citep{turner2018clean,zhao2020clean}.
There could also be multiple triggers released by one or more adversaries for the same dataset \citep{li2024multi}, which is a realistic setting for downloading data from untrusted sources. 

\citet{carlini2022poisoning} proposed the first poisoning backdoor attacks on CLIP with patch triggers.
Compared to supervised learning, poisoning backdoor attacks on CLIP can achieve a high attack success rate at a much lower poisoning rate (i.e., 0.01\%). 
Concurrently, targeted poisoning attack in the finetuning stage \citep{yang2023data} and training-manipulation backdoor attacks have also been developed for CLIP \citep{jia2022badencoder,liu2022poisonedencoder,tao2023distribution}.
The main focus of our work is detecting poisoning backdoor samples in CLIP, for which we follow the same threat model and experimental setup as \citet{carlini2022poisoning}.


\noindent\textbf{Backdoor Defense.}
Backdoor defense can be categorized into 1) trigger synthesis, 2) backdoor model detection, 3) robust training, and 4)  backdoor sample detection methods. 
Trigger synthesis aims to recover the trigger patterns used to poison and attack the victim model \citep{liu2019abs,wang2019neural,hu2022trigger}. Model detection aims to determine if a trained model contains backdoors \citep{chen2019deepinspect,kolouri2020universal,xu2021detecting,feng2023detecting,kuang2024adversarial}. Note that trigger synthesis and model detection methods will still need backdoor removal techniques to obtain a robust model. A robust training strategy aims to (pre)train a backdoor-free model on backdoor-poisoned dataset by robustifying the training procedure of supervised learning \citep{li2021anti,borgnia2021strong,huang2022backdoor,dolatabadi2022collider}, self-supervised learning \citep{li2024difficulty} or CLIP \citep{bansal2023cleanclip,yang2023better,yang2023robust}. 

Backdoor sample detection determines if a data point is infected with the backdoor trigger. 
It can leverage either the statistics of the deep features \citep{tran2018spectral,chen2018detecting,tang2021demon}, sensitivity characteristics to certain perturbations and transformations \citep{gao2019strip,chen2022effective,hou2024ibd} or inference time detection with contrastive prompting \citep{niu2024bdetclip}. Cognitive Distillation (CD) extracts a minimal pattern that allows the model to produce the same output and uses the norm of the extracted mask to detect whether a training/test sample is backdoored \citep{huang2023distilling}. However, it is an optimization-based method that is time-consuming and of limited scalability for web-scale datasets.  
 Anti-Backdoor Learning (ABL) tracks sample-specific training loss during training and detects samples of the lowest loss as backdoor samples \citep{li2021anti}. 
The above defense methods were all developed under supervised learning, with many relying on the class labels to function, which is not available in CLIP.
SafeCLIP is proposed as end-to-end robust training strategy to obtain a backdoor-free model from potentially poisoned dataset \citep{yang2023better}. 
It has two components: one for detecting backdoor data and one for robust training on safe and risky subsets. 


\noindent\textbf{Outlier Detection.}
Outlier detection is a classic problem in data mining. It aims to find data points that deviate from the general distribution. It can be categorized into parametric and non-parametric approaches. The parametric approach makes explicit assumptions about the nature of the underlying data distribution \citep{yang2009outlier,satman2013new}, while the non-parametric does not. 
The non-parametric approach is more suitable for unsupervised settings such as backdoor sample detection. 
It include statistical methods \citep{goldstein2012histogram,li2022ecod}, and ensemble methods \citep{lazarevic2005feature,zhao2021suod} such as the isolation forest (iForest) \citep{liu2008isolation}. 
Local outlier methods are another type of non-parametric outlier detection methods, such as k-nearest-neighbor \citep{ramaswamy2000efficient}, local outlier factor (LOF) \citep{breunig2000lof}, and their improved versions \citep{tang2002enhancing,papadimitriou2003loci,latecki2007outlier,kriegel2008angle}. 
These methods either explicitly or implicitly assess the density in the vicinity of a query point \citep{campos2016evaluation} and data points with low density are usually regarded as outliers. 
 Local intrinsic dimensionality (LID) is another local measure that describes the growth rate of the number of data points in the vicinity of the query point \citep{levina2004maximum,houle2017local1}. LID has been used in various machine learning-related applications \citep{gong2019intrinsic,ansuini2019intrinsic,pope2021the}. Notably, it is used in outlier detections \citep{houle2018correlation}, detecting adversarial examples \citep{ma2018characterizing} and backdoor samples (in a supervised setting) \citep{dolatabadi2022collider}. 
In this work, based on our empirical observation, we choose to exploit simplified local outlier factor (SLOF) \citep{schubert2014local} and its extension dimensionality-aware outlier detection (DAO) \citep{anderberg2024dimensionality} 
for detecting backdoor samples.