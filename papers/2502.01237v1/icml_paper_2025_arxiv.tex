%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amssymb}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{enumitem}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage{icml2025}
% \usepackage{icml2025}
\usepackage[accepted]{icml2025_arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{commath}
\usepackage{multirow}
\usepackage{subcaption}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
% \setlength\epigraphrule{0pt}
\usepackage{epigraph}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{The Differences Between Direct Alignment Algorithms are a Blur}

\begin{document}

\twocolumn[
\icmltitle{The Differences Between Direct Alignment Algorithms are a Blur}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alexey Gorbatovski}{comp}
\icmlauthor{Boris Shaposhnikov}{comp}
\icmlauthor{Viacheslav Sinii}{comp}
\icmlauthor{Alexey Malakhov}{comp}
\icmlauthor{Daniil Gavrilov}{comp}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{T-Tech}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Boris Shaposhnikov}{b.shaposhnikov@tbank.ru}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage).
We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the \(\beta\) parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +$3.46$ (ORPO) and +$8.27$ (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function.
These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) demonstrate strong text generation capabilities, yet aligning them with human values remains challenging due to underspecified objectives, limited training signals, and the complexity of human intent \citep{rlhf, summarize}. Traditional alignment pipelines typically involve Supervised Fine-Tuning (SFT), reward modeling, and reinforcement learning to shape model outputs.


Recently, Direct Alignment Algorithms (DAAs) have emerged as an alternative, integrating human preferences into policy optimization without explicit reward modeling or reinforcement learning \citep{DPO, orpo, ipo, simpo, nca, caldpo, apo, asft}. These methods differ in theoretical design (pairwise vs.\ pointwise), implementation details (e.g., reference policy vs.\ odds ratio), and whether an SFT phase is required (one-stage vs.\ two-stage). This diversity raises key questions about their relationships, comparative advantages, and the role of SFT.

In this paper, we show that one-stage methods (e.g., ORPO, ASFT) can incorporate an explicit SFT phase, improving performance. We introduce a scaling parameter \(\beta\) that unifies their formulation with other DAAs, revealing shared optimization dynamics between methods using either an odds ratio or a reference-based reward. Through theoretical and empirical analysis, we systematically compare DAAs, emphasizing pairwise vs.\ pointwise preference optimization. We also show that, while SFT is beneficial, using the full dataset is not always necessary, which reduces computational costs. To structure our analysis, we address the following research questions:

\textbf{RQ1:} Does an explicit SFT stage improve the alignment quality of ORPO and ASFT?
\noindent

\textbf{RQ2:} Does the tempering factor enhance the alignment quality of ASFT and ORPO?
\noindent

\textbf{RQ3:} What factors of DAAs affect alignment quality?
\noindent

\textbf{RQ4:} How does the final alignment quality depend on the amount of data used in the SFT stage?

By answering these questions, we clarify key trade-offs in alignment strategies and provide guidance for optimizing LLM training pipelines.


\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Modeling Sequences}
\label{sec:model_seq}

Given a sequence \(y\) of length \(\lvert y\rvert\), the log-probability can be written as $\log p(y) = \sum_{i=1}^{\lvert y\rvert} \log p(y_i \mid y_{<i}),$
which may also be conditioned on another sequence \(x\). In practice, optimizing normalized log-probability
$\frac{1}{\lvert y\rvert} \log p(y) = \log\bigl(p(y)^{\tfrac{1}{\lvert y\rvert}}\bigr)$
often improves numerical stability and leads to better training. However, once normalized, the resulting quantity is no longer a strict probability measure. Throughout this paper, whenever we write \(p(y)\), we refer to this normalized version \(p(y)^{\tfrac{1}{\lvert y\rvert}}\). Whenever a method does not apply this normalization, we indicate it explicitly.

\citet{unlikelihood} introduced a log-unlikelihood term that reduces the probability of certain undesirable tokens:
\(\log \bigl(1 - p(c \mid y_{<i})\bigr)\) for \(c\in \mathcal{C}\). It can be extended to an entire sequence as
\(\log\bigl(1 - p(y)\bigr)\).

\subsection{Reinforcement Learning from Human Feedback}
Reinforcement Learning from Human Feedback (RLHF) \citep{rlhf, summarize} is a prominent approach to aligning language models. It generally has three stages:
\vspace{-0.7em}
\begin{itemize}[leftmargin=1.2em, itemsep=0.3em]
    \item \textbf{Supervised Fine-Tuning (SFT).} During the SFT stage, the model $\pi_\theta$ is trained to follow instructions by maximizing the probability of correct output $y$ given input $x$. For a single training pair $(x, y)$, we define the per-sample SFT loss as $\mathcal{L}_\mathrm{SFT}(\pi_\theta, x, y) = -\log \pi_\theta(y \mid x).$
    During fine-tuning, we minimize the expectation of this per-sample loss over the training dataset $\mathcal{D}$: $\mathbb{E}_{(x, y) \,\sim\, \mathcal{D}} \Big[\mathcal{L}_\mathrm{SFT}(\pi_\theta, x, y)\Big].$
    
    \item \textbf{Reward Modeling (RM).} A reward model \(r_\psi(x, y)\) produces a satisfaction score. It is trained on preference pairs using the Bradley-Terry model \citep{bt}:
    \(
    \mathcal{L}_\mathrm{RM}(r_\psi) 
    = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigl[\log \sigma\bigl(r_\psi(x, y_w) - r_\psi(x, y_l)\bigr)\bigr],
    \)
    where \(y_w\) is the preferred response and \(y_l\) is the less preferred one.

    \item \textbf{Reward Maximization.} The objective is to generate responses that maximize the learned reward, with a KL penalty to prevent reward hacking:
    \(
    \max_{\pi_\theta} \mathbb{E}_{x\sim \mathcal{D},\, y \sim \pi_\theta(y\mid x)}\bigl[r_\phi(x, y)\bigr] 
    \;-\; \beta\,\mathbb{D}_{\text{KL}}\bigl[\pi_\theta(x, y)\,\|\,\pi_{\text{ref}}(x, y)\bigr].
    \)
    Reinforcement learning (RL) algorithms are commonly used to optimize this objective \citep{ppo, rlhf}.
\end{itemize}

\subsection{Direct Alignment Algorithms}
\label{sec:daa}
Direct alignment algorithms replace the reward modeling and RL stages (but keep the SFT phase) with a single alignment step. Various preference-optimization loss functions have been proposed, employing these core components:

\begin{itemize}[leftmargin=1.2em, itemsep=0.3em]
    \item \(r^\mathrm{ref}_\theta(y, x) = \log \bigl(\tfrac{\pi_\theta(y \mid x)}{\pi_\mathrm{ref}(y \mid x)}\bigr)\) from DPO \citep{DPO}, which acts as an implicit reward \(\beta\,r^\mathrm{ref}_\theta\). No length normalization is used.
    \item \(r^\mathrm{odds}_\theta(y, x) = \log \bigl(\tfrac{\pi_\theta(y \mid x)}{1 - \pi_\theta(y \mid x)}\bigr)\) proposed in ORPO \citep{orpo}, representing the odds of generating \(y\) versus not generating it.
\end{itemize}

Several Direct Alignment Algorithms use these notations. Information on sequence probability normalization for these methods is presented in Appendix~\ref{app:probability_normalization}.
\vspace{-0.7em}
\begin{itemize}[leftmargin=1.2em, itemsep=0.3em]
    \item \textbf{Direct Preference Optimization (DPO)} \citep{DPO}:
    \(
    \mathcal{L}_\mathrm{DPO} 
    = - \log \sigma\bigl(\beta \,r^\mathrm{ref}_\theta(y_w, x) - \beta \,r^\mathrm{ref}_\theta(y_l, x)\bigr).
    \){This method does not normalize probabilities by length.}\footnote[2]{Unless otherwise noted, the expectation over \((x, y_w, y_l) \sim \mathcal{D}\) is taken.} 

    \item \textbf{Identity Preference Optimization (IPO)} \citep{ipo}:
    \(
    \mathcal{L}_\mathrm{IPO} 
    = \bigl(r^\mathrm{ref}_\theta(y_w, x) - r^\mathrm{ref}_\theta(y_l, x) - \tfrac{1}{2\beta}\bigr)^2.
    \)

    \item \textbf{Simple Preference Optimization (SimPO)} \citep{simpo}:
    \(
    \mathcal{L}_\mathrm{SimPO} 
    = - \log \sigma\bigl(\beta\,\log \pi_\theta(y_w, x) - \beta\,\log \pi_\theta(y_l, x) - \gamma\bigr).
    \)

    \item \textbf{Noise Contrastive Alignment (NCA)} \citep{nca}:
    \(
    \mathcal{L}_\mathrm{NCA} 
    = -\log \sigma \bigl(\beta\,r^\mathrm{ref}_\theta(y_w, x)\bigr) 
    - 0.5\,\log \sigma\bigl(-\beta\,r^\mathrm{ref}_\theta(y_w, x)\bigr) 
    - 0.5\,\log \sigma\bigl(-\beta\,r^\mathrm{ref}_\theta(y_l, x)\bigr).
    \)

    \item \textbf{Calibrated Direct Preference Optimization (Cal-DPO)} \citep{caldpo}:
    \(
    \mathcal{L}_\mathrm{Cal-DPO} 
    = - \log \sigma\bigl(r^\mathrm{ref}_\theta(y_w, x) - r^\mathrm{ref}_\theta(y_l, x)\bigr) 
    + \bigl(r^\mathrm{ref}_\theta(y_w, x) - \tfrac{1}{2\beta}\bigr)^2 
    + \bigl(r^\mathrm{ref}_\theta(y_l, x) + \tfrac{1}{2\beta}\bigr)^2.
    \)

    \item \textbf{Anchored Preference Optimization Zero (APO-Zero)} \citep{apo}:
    \(
    \mathcal{L}_\mathrm{APO-Zero} 
    = - \sigma\bigl(\beta\,r^\mathrm{ref}_\theta(y_w, x)\bigr) 
    + \sigma\bigl(\beta\,r^\mathrm{ref}_\theta(y_l, x)\bigr).
    \)
\end{itemize}

\subsection{Single-Stage Alignment Methods}
\label{sec:single}
Single-stage alignment (as a subset of DAA methods) merges SFT and direct alignment in one step by adding their losses:
\(
\mathcal{L}_{\mathrm{Single}}(\pi_\theta) 
= -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}
\bigl[\mathcal{L}_\mathrm{SFT}(\pi_\theta, x, y_w) + \lambda\,\mathcal{L}_\mathrm{Align}(\pi_\theta, x, y_w, y_l)\bigr],
\)
where \(\lambda\) is a hyperparameter, and no reference policy \(\pi_{\text{ref}}\) is required.

In this paper, we focus on:
\vspace{-0.3cm}
\begin{itemize}[itemsep=0.3em]
    \item \textbf{Odds Ratio Preference Optimization (ORPO)} \citep{orpo}:
    \(
    \mathcal{L}_\mathrm{ORPO} 
    = - \log \pi_\theta(y_w|x) 
    - \lambda \underbrace{\log \sigma\bigl(r^\mathrm{odds}_\theta(y_w, x) - r^\mathrm{odds}_\theta(y_l, x)\bigr)}_{\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}}.
    \)

    \item \textbf{Aligned Supervised Fine-Tuning (ASFT)} \citep{asft}:
    \(
    \mathcal{L}_\mathrm{ASFT} 
    = -\log \pi_\theta(y_w|x) 
    - \lambda \Big(\underbrace{\log \sigma \bigl(r^\mathrm{odds}_\theta(y_w, x)\bigr) 
    -  \log \sigma \bigl(- r^\mathrm{odds}_\theta(y_l, x)\bigr)}_{\mathcal{L}_{\mathrm{ASFT}_\mathrm{Align}}}\Big).
    \)
\end{itemize}

\section{Method}
\label{sec:method}

Many DAAs have been proposed, raising questions about their differences and significance. They can be categorized in various ways. For example, one classification separates single-stage methods, which perform alignment directly after obtaining a base model (ASFT and ORPO), from two-stage methods (which perform SFT before alignment), as in DPO, IPO, SimPO, etc. Under this scheme, ASFT and ORPO are single-stage methods.

Another classification considers whether \(r^\mathrm{ref}\) or \(r^\mathrm{odds}\) is used as an implicit reward. ASFT and ORPO also differ from other losses by using an odds ratio, whereas other methods in Section~\ref{sec:preliminaries} use normalized policy probabilities.\footnote{SimPO does not explicitly use a reference policy, but can be treated similarly if a uniform reference policy is assumed.}

DAAs can also be distinguished by whether their loss function is optimized for pairwise or pointwise preferences. DPO, for instance, increases the policy's probability of choosing preferred sequences relative to rejected ones. In contrast, ASFT simply increases or decreases probabilities for chosen or rejected sequences without comparing them directly.

\subsection{Generalizing ASFT and ORPO}
Despite these classifications, it can still be difficult to pinpoint the essential differences among DAAs, especially when design choices limit generalization. ASFT and ORPO, for example, lack a parameter \(\beta\), probably because they were conceived as single-stage methods, making the distance from a reference policy unnecessary. It might seem odd to introduce such a parameter in single-stage methods, but we will show that for both ASFT and ORPO, the single-stage design and the absence of \(\beta\) are not strictly required.

\subsubsection{ORPO and ASFT can operate without the SFT loss term and as two-stage methods.}
We begin by inspecting the ASFT objective and demonstrate that it combines both likelihood and unlikelihood terms:

\begin{theorem}
\label{thm:asft_bce}
\(\mathcal{L}_\mathrm{ASFT}\) is equivalent to the Binary Cross-Entropy (BCE) loss, encapsulating both likelihood and unlikelihood components:
\[
\mathcal{L}_\mathrm{ASFT} =  -(1+\lambda)\log \pi_\theta(y_w|x) - \lambda \log \big(1 - \pi_\theta(y_l|x)\big).
\]
\end{theorem}

\noindent
\textit{The proof of Theorem~\ref{thm:asft_bce} is provided in Appendix~\ref{app:asft_bce}.} Consequently,
\[
\mathcal{L}_{\mathrm{ASFT}_\mathrm{Align}} 
= - \Big( \log \pi_\theta(y_w|x) + \log \big(1 - \pi_\theta(y_l|x)\big)\Big).
\]

\medskip

Next, we derive a direct relationship between \(\mathcal{L}_\mathrm{ORPO}\) and \(\mathcal{L}_\mathrm{ASFT}\), showing that the latter provides an upper bound on the former:

\begin{theorem}
\label{thm:orpo_asft}
\(\mathcal{L}_\mathrm{ORPO}\) can be expressed as:
\begin{align*}
\mathcal{L}_\mathrm{ORPO} = \mathcal{L}_\mathrm{ASFT} &+ \lambda \log\big(\pi_\theta(y_w|x)(1 - \pi_\theta(y_l|x)) \\
&+ \pi_\theta(y_l|x)(1 - \pi_\theta(y_w|x))\big),
\end{align*}
where the additional term is symmetric in \(y_w\) and \(y_l\).
\end{theorem}

\noindent
\textit{The proof of Theorem~\ref{thm:orpo_asft} is provided in Appendix~\ref{app:orpo_asft}. As for \(\mathcal{L}_{\mathrm{ASFT}_\mathrm{Align}}\), the alignment term is then}
\begin{align*}
& \mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}} = -\log \pi_{\theta}(y_w | x) - \log (1 - \pi_{\theta}(y_l | x))  \\
& + \log\big(\pi_\theta(y_w|x)(1 - \pi_\theta(y_l|x)) + \pi_\theta(y_l|x)(1 - \pi_\theta(y_w|x))\big).
\end{align*}

\begin{corollary}
\label{cor:orpoleasft}
\(\mathcal{L}_\mathrm{ORPO} \le \mathcal{L}_\mathrm{ASFT}\) and \(\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}} \le \mathcal{L}_{\mathrm{ASFT}_\mathrm{Align}}\).
\end{corollary}

\noindent
This follows from the fact that the additional term in \(\mathcal{L}_\mathrm{ORPO}\) is non-positive when \(\pi_\theta(y_w|x)\) and \(\pi_\theta(y_l|x)\) lie in \([0,1]\), and \(\pi_\theta(y_w|x) + \pi_\theta(y_l|x) \le 1\).

These findings yield two main observations:
\vspace{-0.3cm}
\begin{itemize}[itemsep=0.3em]
    \item \(\mathcal{L}_\mathrm{ASFT}\) provides an upper bound on \(\mathcal{L}_\mathrm{ORPO}\). Minimizing the former also minimizes the latter.
    \item \(\mathcal{L}_\mathrm{ASFT}\) can be viewed as a minimal form of a DAA loss, reflecting the structure of BCE.
\end{itemize}

\vspace{-0.3cm}
An essential insight from these formulations is that the SFT term in the ASFT and ORPO losses is already included in the full loss. We hypothesize that this feature may allow us to omit the SFT term in the complete loss, first performing an SFT phase and then using only the alignment terms for model alignment. From this perspective, one can experiment with these methods in both single-stage and two-stage configurations to see which approach is more effective.

\subsubsection{Tempering ASFT and ORPO}
\label{sec:beta_to_one_stage}

We now consider the original single-stage methods from Section~\ref{sec:single} and examine how the alignment terms \(\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\) and \(\mathcal{L}_{\mathrm{ASFT}_\mathrm{Align}}\) compare. These terms optimize preferences and, depending on the coefficient \(\lambda\), can dominate or have a smaller impact on the final loss.

\(\mathcal{L}_{\mathrm{ASFT}_\mathrm{Align}}\) and \(\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\) strongly resemble the DAA losses discussed in Section~\ref{sec:daa}. The single-stage analogue of \(r^\mathrm{ref}_\theta\) is \(r^\mathrm{odds}_\theta\). Inspired by this analogy, we introduce a coefficient \(\beta\) to scale \(r^\mathrm{odds}_\theta\):
\noindent
\begin{align*}
    &\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}} 
    \\&= -\log \sigma(\beta r^\mathrm{odds}_\theta(y_w, x))
    - \log \sigma(-\beta r^\mathrm{odds}_\theta(y_l, x)),\\
    &\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}} 
    \\&= -\log \sigma(\beta r^\mathrm{odds}_\theta(y_w, x) 
    - \beta r^\mathrm{odds}_\theta(y_l, x)).
\end{align*}

Both \(\mathcal{L}^\beta_{\mathrm{ASFT}}\) and \(\mathcal{L}^\beta_{\mathrm{ORPO}}\) generalize their vanilla counterparts (recovering them when \(\beta = 1\)). As in DPO, \(\beta\) can be viewed as a \emph{temperature} or \emph{scaling} parameter that regulates the intensity of the preference for “good” odds. This becomes clearer when looking at the gradients:
\begin{align*}
    &\nabla_\theta\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}} 
    = -\beta\Big[\sigma(\beta r^\mathrm{odds}_\theta(y_l, x))\nabla_\theta r^\mathrm{odds}_\theta(y_l, x) \\&+ \big(1 - \sigma(\beta r^\mathrm{odds}_\theta(y_w, x))\big)\nabla_\theta r^\mathrm{odds}_\theta(y_w, x) 
    \Big], \\
    &\nabla_\theta\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}} 
    = -\beta\Big[\big(\nabla_\theta r^\mathrm{odds}_\theta(y_w, x) 
    - \nabla_\theta r^\mathrm{odds}_\theta(y_l, x)\big)\\&\times\Big(1 - \sigma(\beta r^\mathrm{odds}_\theta(y_w, x) 
    - \beta r^\mathrm{odds}_\theta(y_l, x))\Big)\Big],
\end{align*}
where \(\nabla_\theta r^\mathrm{odds}_\theta(y, x) = \frac{\nabla_\theta\log\pi_\theta(y|x)}{1 - \pi_\theta(y|x)}\). When \(\beta \to 0\), \(\sigma(\beta \cdots) \approx \frac{1}{2}\), both methods aggressively improve the odds ratio (increasing for \(y_w\) and decreasing for \(y_l\)). As \(\beta\) increases, the updates become bounded by the factor \(\sigma(\beta \cdots)\) (similar to a reward threshold in DPO). Hence, once the model improves, further updates are limited, either individually for \(\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}\) or by pairwise ranking in \(\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}\).

This alignment with other DAAs allows for a direct comparison of all methods in different setups, clarifying which aspects are most critical for successful performance.

\subsection{On the Difference Between Direct Alignment Algorithms}
\label{subsec:daa_dif}

Different methods can be grouped by the type of "reward" function used in their loss. In general terms, \(\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}\) and \(\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}\) employ an odds ratio, while DPO, IPO, SimPO, NCA, Cal-DPO, and APO-Zero use a ratio between the probability of the policy and that of a reference policy.

The following theorems make this classification clearer:

\begin{theorem}
\label{thm:asft_orpo_collinearity}
The gradient of \(\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}\) becomes collinear with the gradient of \(\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\) as \(\beta \to 0\). Formally,
\[
\lim_{\beta \to 0}
\frac{\nabla_{\theta}\,
  \mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}}
     {\|\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}\|} 
=
\frac{\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}}{
      \|\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\|},
\]
indicating that both gradients point in the same direction.
\end{theorem}

\noindent
\textit{The proof of Theorem~\ref{thm:asft_orpo_collinearity} is provided in Appendix~\ref{app:asft_orpo_collinearity}.}

A related property applies to \(\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}\):

\begin{theorem}
\label{thm:orpo_orpo_collinearity}
The gradient of \(\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}\) is collinear with the gradient of \(\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\) for any \(\beta > 0\). Formally,
\[
\frac{\nabla_{\theta}\,
  \mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}}
     {\|\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}\|} 
=
\frac{\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}}{
      \|\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\|}, \quad \beta > 0.
\]
\end{theorem}

\noindent
\textit{The proof of Theorem~\ref{thm:orpo_orpo_collinearity} is provided in Appendix~\ref{app:orpo_orpo_collinearity}.}

Finally:

\begin{theorem}
\label{thm:dpo_unified_collinearity}
For each method 
\(
X \;\in\;\bigl\{\mathrm{IPO},\,\mathrm{SimPO},\,\mathrm{NCA},\,\mathrm{Cal\text{-}DPO},\,\mathrm{APO\text{-}Zero}\bigr\},
\)
as \(\beta\to 0\), the gradient of \(\mathcal{L}_X\) is collinear with the gradient of \(\mathcal{L}_\mathrm{DPO}\). Formally,
\[
\lim_{\beta \to 0}
\frac{\nabla_\theta\,\mathcal{L}_X}{
      \|\nabla_\theta\,\mathcal{L}_X\|}
=
\frac{\nabla_\theta\,\mathcal{L}_\mathrm{DPO}}{
      \|\nabla_\theta\,\mathcal{L}_\mathrm{DPO}\|}.
\]
\end{theorem}

\noindent
\textit{The proof of Theorem~\ref{thm:dpo_unified_collinearity} is provided in Appendix~\ref{app:dpo_unified_collinearity}.}

These theorems suggest that for sufficiently small \(\beta\), these loss functions are split into two categories with indistinguishable gradient directions. Although the magnitudes may differ and they may not be collinear for \(\beta \not\to 0\), one could infer that their performance should be similar when \(\beta\) is small. From this perspective, two main distinctions arise among these methods: the use of an odds ratio (\(r^\mathrm{odds}_\theta\)) and the use of the ratio to a reference policy (\(r^\mathrm{ref}_\theta\)). Both choices might influence the final performance of these methods. Furthermore, it remains an open question whether odds-ratio-based approaches outperform reference-policy-based ones (e.g., DPO), and how these distinctions compare to the contrast between pointwise and pairwise preference formulations.
From traditional learning-to-rank \cite{learning_to_rank} research, pairwise methods often produce more direct and less noisy ranking signals than pointwise techniques, which could lead to superior performance in practice \citep{ranknet, short_learning_to_rank, ranking_case_study}. In the following sections, we present experimental results that provide further insight into which aspects most strongly influence DAA training.

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.95\textwidth]{images/asft_orpo_beta.pdf}
\caption{
\textbf{Impact of the \(\beta\) Parameter on ASFT and ORPO Alignment Quality.} The plot shows how tuning \(\beta\) (Section~\ref{sec:beta_to_one_stage}) affects both ASFT and ORPO performance. Results are reported for GPT-4 Win Rate in the Llama 3.2 3B TL;DR setup and for AlpacaEval 2 LC Win Rate in the Llama 3.1 8B UF scenario. All other hyperparameters (e.g., learning rates) are selected via grid search, using each method's best configuration at \(\beta = 1\) as the baseline. See Section~\ref{sec:res:beta} for more details.}
\label{img:asft_orpo_beta}
\end{figure*}

\section{Experimental Setup}
\label{sec:exp_setup}
We systematically compare and evaluate DAA methods using a standard training and instruction-following evaluation framework \cite{zephyr, simpo, trdpo}. Our main experiments use the Llama 3.1 8B model \cite{llama3modelcard}, trained on the UltraChat \cite{ultrachat} and UltraFeedback (UF) \cite{ultrafeedback} datasets, and evaluated on the AlpacaEval 2 \cite{alpaca_lc, alpaca_eval_git} and ArenaHard \cite{arenahard2024} benchmarks. For the Reddit TL;DR \cite{summarize} task, we employ the Llama 3.2 3B model, comparing it side by side with the “golden” validation split \cite{DPO, rafailov2024scaling} using the prompt in Appendix~\ref{app:gpt_prompts}.


\subsection{Base vs SFT-Initialized Models.}
\label{sec:base_vs_sft}
To investigate the impact of SFT and the applicability of one-stage loss \(\mathcal{L}_{\mathrm{Align}}\) component, we use the UF dataset for SFT (avoiding additional knowledge from UltraChat), and for pairwise preference optimization. We carefully tuned the hyperparameters to optimize each method's performance.

For the \emph{Base-initialized} setup, we perform a grid search over learning rates \(\{6\times10^{-6},\, 8\times10^{-6},\, 1\times10^{-5}\}\), inspired by values suggested in ORPO and ASFT, and explore \(\lambda \in \{0.1,\, 0.2,\, 0.5,\, 1.0\}\) for 1 and 2 training epochs keeping a similar budget to compare with the \emph{SFT-initialized} setup.

In the \emph{SFT-initialized} setup, we experiment with both \(\mathcal{L}_{\mathrm{ORPO}_{\mathrm{Align}}}\) and \(\mathcal{L}_{\mathrm{ASFT}_{\mathrm{Align}}}\) alone, as well as in combination with \(\mathcal{L}_{\mathrm{SFT}}\), following the original methods.
We tune the learning rates \(\{5\times10^{-7},\, 7\times10^{-7},\, 1\times10^{-6}\}\) for one epoch, starting from an SFT model trained for 1 epoch at \(\ 6\times10^{-6}\).

\subsection{\(\beta\) Sensitivity.}
\label{sec:par:beta-sens}
Building on the theoretical insights from Section~\ref{subsec:daa_dif}, where DAA losses share indistinguishable gradient directions as \(\beta \to 0\), we evaluate each method across various \(\beta\) values to examine quality-KL trade-offs. In classical DPO, \(\beta\) regulates the KL penalty from the reference policy, but setting \(\beta\) too small can induce training instability. Therefore, we conduct a thorough sweep of at least six \(\beta\) values per DAA, exploring the performance limit of each method. To broaden our analysis, we consider three scenarios: 

\vspace{-0.7em}
\begin{enumerate}[itemsep=0.3em]
    \item \textbf{Llama 3.2 3B TL;DR.} A relatively simpler Reddit TL;DR summarization task, evaluated via GPT side-by-side comparison on 500 samples from the “golden” validation split \cite{DPO, rafailov2024scaling}.
    \item \textbf{Llama 3.2 3B UF.} The UltraChat and UF datasets serve as more challenging alignment settings due to their coverage of diverse and complex tasks, including common sense reasoning, mathematical problem-solving, code generation, logical reasoning, creative writing, and general knowledge.
    \item \textbf{Llama 3.1 8B UF.} A larger, more capable model on the same UltraChat and UF datasets, allowing us to assess how increased model capacity influences \(\beta\)-sensitivity in these diverse tasks.
\end{enumerate}

For the UF-based experiments, we measure model quality primarily using the AlpacaEval 2 Length-Controlled (LC) Win-Rate and ArenaHard (AH) WR, and then track KL divergence from a reference model to construct Pareto fronts. For the TL;DR scenario, we rely on GPT-based preference judgments using `gpt-4o-2024-08-06` model. Concretely, in each scenario we train models for different values \(\beta\), combining them with four possible learning rates \(\{1\times10^{-6},\, 7\times10^{-7},\, 5\times10^{-7}, 3\times10^{-7}\}\). Further implementation details, including training procedures and generation hyperparameters, are provided in Appendix~\ref{app:impl_details}.

\begin{figure*}[h!]
    \centering
    \begin{minipage}{0.65\linewidth}
        % \centering
        \raggedleft
        % \vspace{0.2cm}
        \includegraphics[width=\linewidth]{images/3b_sbs_tldr.pdf}
    \end{minipage}%
    % \hfill
    \hspace{0.01\linewidth}
    \begin{minipage}{0.33\linewidth}
        % \centering
        \raggedright
        % \vspace{2cm}
        \footnotesize
        \renewcommand{\arraystretch}{1}
        \label{tab:performance_metrics}
        \begin{tabular}{c}
            \textbf{Win / Tie / Lose Rate \%} \\
            \hline
            35.6 / 4.8 / \textbf{59.6} \\ \hline
            \textbf{91.2} / 1.0 / 7.8 \\ \hline
            \textbf{91.4} / 0.4 / 8.2 \\ \hline
            \textbf{91.6} / 0.2 / 8.2 \\ \hline
            \textbf{90.2} / 0.6 / 9.2 \\ \hline
            \textbf{92.6} / 0.6 / 6.8 \\ \hline
            \textbf{91.8} / 1.0 / 7.2 \\ \hline
            \textbf{91.4} / 0.4 / 8.2 \\ \hline
            \textbf{87.2} / 1.0 / 11.8 \\
            \hline
        \end{tabular}
    \end{minipage}
        \caption{\textbf{GPT-4 Evaluation of Llama 3.2 3B TL;DR setup.} The comparison shows multiple alignment methods (rows) using their best hyperparameters, where each approach aims to generate concise and accurate summaries. Most methods exceed 90\% Win Rate; ASFT achieves 87.2\%, maintaining robust summarization performance. See Section~\ref{sec:res:beta} for more details.}
    \label{fig:3b_sbs_tldr}
\end{figure*}

\subsection{SFT Quality.}
\label{sec:par:sft-quality}
Although in principle single-stage methods do not require a separate SFT phase, in practice an SFT-trained reference model often improves the final performance of two-stage pipelines (see Section~\ref{sec:res:base_vs_sft}). Prior work, such as \citep{lima}, has shown that a small but high-quality dataset can be sufficient for instruction tuning. However, beyond response quality, it remains unclear how the amount of SFT data influences alignment effectiveness. This raises a fundamental question: how much supervised data is actually needed to produce a reference model that yields high-quality results after the subsequent alignment step?

To investigate this, we prepared seven SFT checkpoints by training Llama 3.1 8B Base on 1\%, 3\%, 5\%, 10\%, 25\%, 50\%, and 100\% of the UltraChat dataset (2{,}079, 6{,}236, 10{,}393, 20{,}786, 51{,}966, 103{,}932, and 207{,}865 records, respectively) using our \emph{SFT-initialized} procedure. We then applied each alignment method -- using \emph{optimal hyperparameters} from our \(\beta\)-sensitivity experiments (Appendix Table~\ref{tab:best_hypers}) -- to these seven SFT checkpoints and the original base model. Finally, we evaluated all resulting aligned models on AlpacaEval 2 LC, analyzing their performance relative to the fraction of SFT data used.

\section{Results}
\label{sec:results}

\subsection{RQ1: Does an explicit SFT stage improve the alignment quality of ORPO and ASFT?}

\label{sec:res:base_vs_sft}

As shown in Table~\ref{tab:base_vs_sft}, the performance of ORPO and ASFT methods improves significantly when the alignment loss \(\mathcal{L}_{\mathrm{Align}}\) is applied after a preceding SFT stage. In particular, ORPO achieves results comparable to classical DPO in both LC Win Rate and AH WR metrics. In contrast, ASFT shows notable gains in AH WR after the SFT stage, although it still underperforms compared to ORPO or DPO.

\begin{table}[h!]
\small
\centering
\footnotesize
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}r|c|cc|c@{}}
\toprule
\textbf{Init} & \textbf{Method} & \textbf{LC\% \small{(std)}} & \textbf{WR\% \small{(std)}} & \textbf{AH\% \small{(CI)}} \\
\midrule
Base & SFT & 6.7 \small(0.43) & 4.5 \small(0.63) & 3.5 \small(-0.7, 0.8) \\ \hline
SFT & ORPO & \textbf{24.1} \small(0.84) & \underline{17.8} \small(1.17) & \underline{15.3} \small(-1.6, 1.8) \\
SFT & ASFT & 16.4 \small(0.72) & 11.9 \small(0.99) & 10.6 \small(-1.2, 1.3) \\ \hline
Base & ORPO & 14.8 \small(0.71) & 10.3 \small(0.95) & 8.4 \small(-1.3, 1.3) \\
Base & ASFT & 14.5 \small(0.73) & 10.2 \small(0.94) & 7.5 \small(-1.1, 1.2) \\ \hline
SFT & ORPO$^\dag$ & 13.4 \small(0.69) & 9.3 \small(0.91) & 7.7 \small(-0.9, 1.1) \\
SFT & ASFT$^\dag$ & 11.4 \small(0.63) & 7.5 \small(0.83) & 7.5 \small(-1.1, 1.1) \\ \hline
SFT & DPO & \underline{23.4} \small(0.85) & \textbf{20.0} \small(1.18) & \textbf{17.5} \small(-1.8, 1.8) \\
\bottomrule
\end{tabular}
\caption{\textbf{Base and SFT-initialized alignment methods on the Llama 3.1 8B model with the UF dataset.} SFT-initialized methods demonstrate better performance compared to their traditional formulations without \(\mathcal{L}_{\mathrm{SFT}}\). Results marked with \( \dag \) correspond to training with \(\mathcal{L}_{\mathrm{SFT}}\), using the best hyperparameters: \(\text{lr}=1\times10^{-6}\) for ORPO and \(\text{lr}=7\times10^{-7}\) for ASFT. For other setups, the best hyperparameters are: \(\text{lr}=5\times10^{-7}\) for standard SFT ORPO/ASFT, and \(\text{lr}=1\times10^{-5}\)/\(6\times10^{-6}\) for Base ORPO/ASFT.}

\label{tab:base_vs_sft}
\end{table}

For single-stage methods, the use of \(\lambda = 1\) provides the best results within the explored grid of \(\lambda \in \{0.1,\, 0.2,\, 0.5,\, 1.0\}\), especially after two epochs of training. However, combining \(\mathcal{L}_{\mathrm{SFT}}\) and \(\mathcal{L}_{\mathrm{Align}}\) in a single-stage setup leads to suboptimal results compared to explicitly separating these phases, even when starting from an SFT-trained model. Incorporating an explicit SFT stage improves overall performance for ORPO and ASFT methods. Therefore, all further experiments focus on applying the \(\mathcal{L}_{\mathrm{Align}}\) components of ORPO and ASFT on top of an SFT-trained model.


\subsection{RQ2: Does the tempering factor enhance the alignment quality of ASFT and ORPO?}
\label{sec:res:beta}

\begin{table*}[htbp]
\centering
\small
\begin{tabular}{c|ccc|ccc}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Llama 3.2 3B UF}} & \multicolumn{3}{c}{\textbf{Llama 3.1 8B UF}} \\
\cmidrule(lr){2-7}
& \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{ArenaHard} & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{ArenaHard} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-7}
& \textbf{LC\% (std)} & \textbf{WR\% (std)} & \textbf{WR\% (CI)} & \textbf{LC\% (std)} & \textbf{WR\% (std)} & \textbf{WR\% (CI)} \\
\midrule
SFT 
& 5.02 (0.34)  & 3.21 (0.55)  & 1.4 (-0.4, 0.4) 
& 10.27 (0.54) & 5.44 (0.70)  & 2.6 (-0.5, 0.6) \\ \midrule
DPO 
& \textbf{11.43} (0.58) & 11.79 (0.99) & \underline{6.8} (-1.0, 0.9)
& 26.82 (0.77) & 23.69 (1.25) & 19.0 (-1.9, 1.8) \\
IPO & \underline{11.24} (0.60) & 11.67 (1.01) & \textbf{6.8} (-1.0, 1.1)
& \underline{28.18} (0.83) & 24.43 (1.26) & 19.1 (-1.6, 1.5) \\
SimPO 
& 10.56 (0.44) & \underline{11.94} (0.95) & 6.4 (-1.0, 1.1)
& 27.65 (0.77) & \underline{25.62} (1.29) & \textbf{21.5} (-1.9, 1.9) \\
ORPO 
& 10.67 (0.50) & \textbf{12.23} (0.97) & 6.6 (-1.0, 1.1)
& \textbf{28.25} (0.71) & \textbf{28.59} (1.33) & \underline{20.9} (-2.0, 2.0) \\
\midrule
APO Zero 
& 10.36 (0.53) & 11.22 (0.98) & 6.0 (-1.0, 0.9)
& 23.15 (0.76) & 19.03 (1.18) & 17.3 (-1.8, 1.8) \\
NCA 
& 10.33 (0.53) & 11.02 (0.97) & 5.1 (-0.7, 0.8)
& 23.21 (0.80) & 18.67 (1.17) & 15.1 (-1.5, 1.6) \\
Cal-DPO 
& 10.62 (0.57) & 10.15 (0.94) & 4.8 (-0.9, 0.9)
& 23.19 (0.82) & 18.85 (1.18) & 15.2 (-1.5, 1.6) \\
ASFT 
& 10.63 (0.55) & 9.21 (0.88)  & 5.1 (-0.9, 0.9)
& 20.82 (0.79) & 16.34 (1.13) & 13.5 (-1.6, 1.5) \\
\bottomrule
\end{tabular}
\caption{\textbf{AlpacaEval 2 and ArenaHard Results for Llama 3.2 3B and Llama 3.1 8B UF.} The SFT model was trained on the UltraChat dataset. The best hyperparameters for each method were selected according to Section~\ref{sec:par:beta-sens}. Bold values indicate the best performance for each benchmark, while underlined values represent the second-best performance. See Section \ref{sec:res:pareto} for more details.}
\label{tab:alplaca}
\end{table*}

Figure~\ref{img:asft_orpo_beta} illustrates that introducing the \(\beta\) parameter (as described in Section~\ref{sec:beta_to_one_stage}) improves the performance of both ASFT and ORPO \(\mathcal{L}_{\mathrm{Align}}\) in our tested scenarios. For a fair comparison, we used the best-performing learning rate for each baseline --- \(\mathcal{L}_{\mathrm{ASFT}_\mathrm{Align}}\) and \(\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\) --- while fixing \(\beta = 1\). In the Llama 3.2 3B TL;DR experiment, these adjustments led to an improvement of +7.0 for ORPO and +43.4 for ASFT in GPT-4 WR. In the Llama 3.1 8B UF setup, tuning \(\beta\) provided additional gains of +3.46 for ORPO and +8.27 for ASFT on the AlpacaEval 2 LC WR.


\subsection{RQ3: What factors of DAAs affect alignment quality?}
% \subsection{RQ3: What makes performance differences in DAAs?}
\label{sec:res:pareto}

Based on Section~\ref{sec:method}, we perform a comprehensive evaluation of alignment losses, including DPO, IPO, SimPO, NCA, Cal-DPO, and APO-Zero, as well as enhanced \(\mathcal{L}^{\beta}_{\mathrm{ASFT}_\mathrm{Align}}\) and \(\mathcal{L}^{\beta}_{\mathrm{ORPO}_\mathrm{Align}}\) with the introduced parameter \(\beta\). Unlike classical methods where \(\beta\) typically regulates KL divergence against a reference policy \(\pi_{\text{ref}}\), \(\beta\) in \(\mathcal{L}^{\beta}_{\mathrm{ASFT}_\mathrm{Align}}\) and \(\mathcal{L}^{\beta}_{\mathrm{ORPO}_\mathrm{Align}}\) directly modulates the strength of preference optimization. To explore the upper limits of each method's performance, we performed an extensive hyperparameter search, analyzing both alignment quality and KL divergence. Full implementation details, including training setups and evaluation criteria, are provided in Appendix~\ref{app:impl_details}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.99\columnwidth]{images/pareto-8b-uf.pdf}
    \caption{\textbf{Pareto front for alignment quality and KL divergence.} Results for Llama 3.1 8B UF on AlpacaEval 2 LC. Methods are grouped into pairwise and pointwise categories, with pairwise achieving higher LC values while remaining within overlapping confidence intervals. See Section \ref{sec:res:pareto} for more details.}
    \label{img:8b-pareto}
\end{figure}

\textbf{Llama 3.2 3B TL;DR:} Figure~\ref{fig:3b_sbs_tldr} presents a comparison of all methods on the Reddit TL;DR validation subset, using their best hyperparameters. Most methods achieve a GPT-4 Win Rate exceeding 90\%, indicating robust summarization performance on this relatively straightforward task. ASFT is slightly lower at 87.2\% Win Rate, but still demonstrates strong overall results.


\textbf{Llama 3.2 3B UF and Llama 3.1 8B UF:} Table~\ref{tab:alplaca} summarizes the results for both Llama 3.2 3B UF and Llama 3.1 8B UF setups. For the smaller 3B model, the methods perform similarly on LC WR, with slight differences emerging on AH. Although these differences align with the pairwise vs. pointwise distinction (e.g., DPO, IPO, ORPO, SimPO vs. APO-Zero, NCA, Cal-DPO, ASFT), no single approach consistently dominates across metrics. The overlap in confidence intervals further indicates that the results for these methods are statistically similar in this setup, with no clear separation.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.99\columnwidth]{images/toy_results.pdf}
    \caption{\textbf{Pairwise vs. Pointwise Ranking Methods on Toy Example.} Model capacity impacts ranking accuracy, with pairwise methods outperforming pointwise ones as capacity increases. This behavior is consistent with results observed in Llama experiments on the UF dataset. See Section \ref{sec:res:pareto} for more details.}
    \label{img:toy-results}
\end{figure}

In contrast, the 8B model reveals a clearer performance differentiation. Pairwise methods consistently outperformed pointwise ones on AlpacaEval 2 and ArenaHard metrics, with ORPO achieving the highest overall alignment quality. As illustrated in Figure~\ref{img:8b-pareto}, pairwise approaches dominated the KL Pareto front for the larger model, demonstrating their ability to more effectively balance alignment quality and divergence. Pareto fronts for the remaining setups are included in Appendix~\ref{app:pareto} for completeness.

These observations suggest that model capacity plays a significant role in amplifying the advantages of pairwise ranking, where LLMs act as rankers (similar to \citet{lipo}). For smaller models, such as the 3B setup, limited capacity may hinder the ability to fully exploit pairwise gradient signals. This hypothesis is supported by additional evidence from the toy example experiment (Figure~\ref{img:toy-results}), where pairwise methods demonstrated performance similar to pointwise methods with weaker MLPs but achieved better ranking accuracy as the model capacity increased. Full details of the toy example setup are provided in Appendix~\ref{app:toy_exp}.

\begin{figure*}[h!]
\subfigure[Pairwise\label{fig:sft_pairwise}]{%
  \includegraphics[width=\columnwidth]{images/sft_pairwise.pdf}%
}\hfill
\subfigure[Pointwise\label{fig:sft_pointwise}]{%
  \includegraphics[width=\columnwidth]{images/sft_pointwise.pdf}%
}
\caption{\textbf{Impact of SFT Dataset Size on Alignment Quality.} Performance of the pairwise (a) and pointwise (b) alignment methods on AlpacaEval 2 (LC WR metric) when the SFT policy is trained on different fractions of the UltraChat dataset. Even a small fraction of SFT data (e.g., 5--10\%) yields substantial gains over starting from the raw base model. See Section~\ref{sec:res:sft} for more details.}
\label{}
\end{figure*}

\subsection{RQ4: How does the final alignment quality depend on the amount of data used in the SFT stage?}
% \subsection{RQ4: How does the final quality of the aligned model depend on the amount of data used in the SFT stage?}
\label{sec:res:sft}

In Section~\ref{sec:res:base_vs_sft}, we show that DAAs designed to bypass the SFT phase still underperform compared to models that undergo SFT and are then aligned using a similar preference-optimization loss function \emph{without} the SFT term. As discussed in Section~\ref{sec:par:sft-quality}, this raises the question of how much supervised data is needed to compensate for the additional computation and achieve comparable alignment performance.

To investigate this, we trained seven SFT models on progressively larger UltraChat subsets (1\% to 100\%) and applied each alignment algorithm to these models and the non-fine-tuned base model, yielding eight initializations per method.
Figures~\ref{fig:sft_pairwise} and \ref{fig:sft_pointwise} summarize the results for pairwise and pointwise alignment methods, respectively. As the plots show, no method starting from the raw base model can match the final quality of a method trained with the entire SFT dataset. However, even a modest size expansion of the SFT dataset yields substantial improvements in alignment quality: for example, moving from 3\% to 5\% of the data more than doubles the AlpacaEval 2 LC score for the final model. Crucially, using only 10\% of UltraChat for SFT yields nearly the same quality as using the entire dataset.

Adding an SFT phase requires more overall training, but it \emph{pays off significantly} in the final result. Moreover, one does not need the entire supervised corpus to realize most of these gains; even 5--10\% of the data is often enough for DAAs to reach most of their potential.

\section{Conclusion}
This paper presents a comprehensive theoretical and empirical analysis of DAAs. Theoretically, we demonstrated that within each category - odds-based (\(r^\mathrm{odds}\)) and reference-policy-based (\(r^\mathrm{ref}\)) -- gradient directions of popular methods align as \(\beta \to 0\), revealing shared optimization dynamics within these groups. We also showed that single-stage losses (e.g., ASFT, ORPO) can be extended to two-stage pipelines with an explicit SFT step and optional \(\beta\)-scaling, enabling greater flexibility. Experimentally, we addressed four core research questions (RQ1--4), exploring single- vs.\ two-stage training, implicit rewards, objective types, and the impact of the SFT phase. Our key findings are:

\vspace{-0.7em}

\begin{itemize}[leftmargin=1.2em, itemsep=0.3em]
    \item \textbf{Include an SFT phase.} An SFT stage consistently improves alignment performance (RQ1), with ORPO achieving +9.3 LC / +6.9 AH and ASFT +1.9 LC / +3.1 AH in the setup from Section~\ref{sec:base_vs_sft}. Even 5--10\% of the supervised dataset often suffices to achieve near-optimal results (RQ4).
    \item \textbf{Pairwise methods outperform pointwise objectives.} Alignment quality depends more on the choice between pairwise and pointwise objectives than on the formulation of implicit reward (e.g., \(r^\mathrm{odds}\) or \(r^\mathrm{ref}\)). Pairwise methods generally perform better (e.g., ORPO outperforming ASFT by +7.43 LC / +7.4 AH in the Llama 3.1 8B UF setup), particularly in larger models (RQ3). Among these, ORPO and SimPO also stand out as practical options for memory-constrained scenarios, as they do not rely on a reference policy.
    \item \textbf{Choose hyperparameters carefully.} Alignment performance is highly sensitive to learning rates and the coefficient \(\beta\). We provide optimal configurations for different methods based on comprehensive grid searches in our experimental setups, highlighting the added gains from tuning \(\beta\) in odds-based methods, where it controls the strength of preference optimization (RQ2).
\end{itemize}
\vspace{-0.5em}
\textbf{Limitations and Future Work.} Although our study systematically compares DAAs, it has several limitations. We tested a limited set of datasets (UltraChat, UltraFeedback, Reddit TL;DR) and benchmarks (AlpacaEval 2, ArenaHard), which may affect generalizability to other domains. The reliance on GPT-based evaluators can introduce biases. Moreover, we evaluated on 3B–8B models, so the observed advantages of pairwise over pointwise objectives could shift at larger scales.

\bibliography{icml_paper_2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Implementation Details}
\label{app:impl_details}

\subsection{Probability Normalization}
\label{app:probability_normalization}

As discussed in Section~\ref{sec:model_seq}, not all DDAs incorporate length-based probability normalization by default. In this paper, however, we consistently apply such normalization wherever probabilities are involved. This choice avoids introducing extra notation and reduces the cognitive load on the reader. Table~\ref{tab:prob_norm} summarizes the methods that originally include length-based normalization.

\begin{table*}[htbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Method} & \textbf{Use normalization} \\
\midrule
DPO \citep{DPO} & \xmark \\
IPO \citep{ipo} & \xmark \\
SimPO \citep{simpo} & \cmark \\
NCA \citep{nca} & \xmark \\
Cal-DPO \citep{caldpo} & \xmark \\
APO-Zero \citep{apo} & \xmark \\
ORPO \citep{orpo} & \cmark \\
ASFT \citep{asft} & \cmark \\
\bottomrule
\end{tabular}
\caption{Methods that include (\cmark) or omit (\xmark) length-based probability normalization in their original formulation.}
\label{tab:prob_norm}
\end{table*}

\subsection{Training Details}

Our experiments were conducted using the Llama 3.2 3B and Llama 3.1 8B Base models \cite{llama3modelcard}. The training setup, datasets, and hyperparameters were designed to ensure reproducibility and consistency. Unless otherwise noted, the hyperparameters in Table~\ref{tab:training_hyperparameters} were used across all experiments.

\begin{table*}[htbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Max Tokens Length & 1024 (TL;DR setup), 4096 (UF setup) \\
Epochs & 1 \textit{(or 2 when specified)} \\
Learning Rate (SFT) & \(6.0 \times 10^{-6}\) \\
Learning Rate (Base Init.) & \(\{6.0 \times 10^{-6},\, 8.0 \times 10^{-6},\, 1.0 \times 10^{-5}\}\) \\
Learning Rate (Alignment) & \(\{3.0 \times 10^{-7},\, 5.0 \times 10^{-7},\, 7.0 \times 10^{-7},\, 1.0 \times 10^{-6}\}\) \\
Optimizer & Adam \citep{adam} \\
Adam \(\beta_1\) & 0.9 \\
Adam \(\beta_2\) & 0.95 \\
Batch Size & 128 \\
Learning Schedule & Linear Decay \\
Warm-up Ratio & 0.03 \\
Max Gradient Norm & 2 \\
Memory Optimization & DeepSpeed \citep{rasley2020deepspeed} \\
Attention Mechanism & Flash Attention 2 \citep{flashattention} \\
\bottomrule
\end{tabular}
\caption{Representative training hyperparameters for Llama 3.2 3B and Llama 3.1 8B models.}
\label{tab:training_hyperparameters}
\end{table*}

Training was performed on 8 NVIDIA A100 GPUs with 80GB memory each. Depending on the number of epochs, training for each configuration took between 3 to 6 hours. 

\subsubsection{Datasets.}
We used two primary datasets:
\begin{itemize}
    \item \textbf{Reddit TL;DR}~\citep{tldr_dataset}: used to train the initial SFT model in \(\beta\)-sensitivity experiments with Llama 3.2 3B model.
    \item \textbf{UltraChat}~\citep{ultrachat}: used to train the initial SFT model in \(\beta\)-sensitivity experiments with Llama 3.2 3B and Llama 3.1 8B models.
    \item \textbf{UltraFeedback}~\citep{ultrafeedback}: used for both SFT (in the \textit{Base vs.\ SFT-initialized} comparison, where we selected chosen subset from preference pairs) and for pairwise preference optimization in all DAA methods.
\end{itemize}
The dataset sizes are summarized in Table~\ref{tab:dataset_summary}. For \emph{Base} vs.\ \emph{SFT-initialized} setups, only UltraFeedback was used. For \(\beta\)-sensitivity experiments, the models were first trained on UltraChat for SFT and subsequently fine-tuned on UltraFeedback. The Reddit TL;DR dataset was processed to remove duplicates, retaining only uniquely preferred summaries for SFT.

\begin{table*}[htbp]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Dataset} & \textbf{Training Examples} & \textbf{Validation Examples} \\
\midrule
UltraChat & 207,865 & 23,110 \\
UltraFeedback & 61,135 & 2,000 \\
Reddit TL;DR (SFT) & 41,947 & 11,941  \\
Reddit TL;DR (Preference) & 73,396 & 21,198  \\
\bottomrule
\end{tabular}
\caption{Summary of dataset sizes used for training and validation.}
\label{tab:dataset_summary}
\end{table*}

\subsubsection{\(\beta\)-Sensitivity Experiments.}
We conducted a comprehensive analysis to evaluate the sensitivity of DAA methods to \(\beta\), examining its impact on the trade-off between model quality and KL divergence. Each method was trained using six or more distinct \(\beta\) values to identify a configuration that achieves stable and effective performance. The specific \(\beta\) values tested for each method are as follows:

\begin{table*}[htbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Method} & \textbf{\(\beta\) Values Tested} \\
\midrule
DPO & \(\{0.001, 0.003, 0.005, 0.01, 0.05, 0.1\}\) \\
IPO & \(\{0.0007, 0.001, 0.005, 0.01, 0.05, 0.1\}\) \\
SimPO & \(\{0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0\}\) \\
ORPO & \(\{0.05, 0.1, 0.2, 0.5, 1.0, 2.0\}\) \\
ASFT & \(\{0.05, 0.1, 0.2, 0.5, 1.0, 2.0\}\) \\
APO-Zero & \(\{0.001, 0.003, 0.005, 0.01, 0.05, 0.1, 0.2\}\) \\
Cal-DPO & \(\{0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003\}\) \\
NCA & \(\{0.0001, 0.0003, 0.0005, 0.001, 0.005, 0.007, 0.01, 0.03, 0.05\}\) \\
\bottomrule
\end{tabular}
\caption{Range of \(\beta\) values tested for each DAA method on all scenarios.}
\label{tab:beta_sensitivity}
\end{table*}

For each \(\beta\), we tested four learning rates (\(3.0 \times 10^{-7},\, 5.0 \times 10^{-7},\, 7.0 \times 10^{-7},\, 1.0 \times 10^{-6}\)), training on the UltraFeedback dataset. All runs began from an SFT-initialized model trained on UltraChat (\(\text{lr} = 6.0 \times 10^{-6}\), 1 epoch). The best-performing learning rate for each \(\beta\) was selected to construct Pareto fronts, balancing quality (measured via AlpacaEval 2 LC Win-Rate) and KL divergence.  

For SimPO in the Llama 3.1 8B UF setup, the ratio \(\frac{\gamma}{\beta} = 0.5\) was kept fixed as recommended by \citet{simpo}. Additionally, a single learning rate (\(\text{lr} = 6.0 \times 10^{-7}\)) was tested across all \(\beta\) values for this method, as the same datasets and model scale were used. For Llama 3.2 TL;DR and UF setups, we tested four learning rates similar to other DAAs.  
Beyond the standard \(\beta\) values described in Table~\ref{tab:beta_sensitivity}, additional values were explored for specific configurations to reach the extreme points of the Pareto front. For example:  
- \(\{0.00001, 0.00003\}\) for Cal-DPO in Llama 3.2 3B TL;DR and UF setups,  
- \(\{0.00001, 0.00003, 0.00005\}\) for NCA in Llama 3.2 3B TL;DR,  
- \(\{0.0003, 0.0005\}\) for APO-Zero in Llama 3.2 3B TL;DR,  
- \(\{0.0003, 0.0005, 0.001, 0.003, 0.005\}\) for ASFT in Llama 3.2 3B TL;DR.

The hyperparameters resulting in the best performance are presented in Table~\ref{tab:best_hypers}.

\begin{table*}[htbp]
\centering
\small
\begin{tabular}{c|cc|cc|cc}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Llama 3.2 3B TL;DR}} & \multicolumn{2}{c|}{\textbf{Llama 3.2 3B UF}} & \multicolumn{2}{c}{\textbf{Llama 3.1 8B UF}} \\
\cmidrule(lr){2-7}
& \textbf{Learning Rate} & \(\beta\) & \textbf{Learning Rate} & \(\beta\) & \textbf{Learning Rate} & \(\beta\) \\
\midrule
DPO & $7.0 \times 10^{-7}$ & 0.05 & $1.0 \times 10^{-6}$ & 0.01 & $1.0 \times 10^{-6}$ & 0.003 \\
\midrule
IPO & $1.0 \times 10^{-6}$ & 0.005 & $7.0 \times 10^{-7}$ & 0.001 & $1.0 \times 10^{-6}$ & 0.001 \\
\midrule
SimPO & $3.0 \times 10^{-7}$ & 0.5 & $7.0 \times 10^{-7}$ & 1.0 & $6.0 \times 10^{-7}$ & 1.0 \\
\midrule
ORPO & $3.0 \times 10^{-7}$ & 0.5 & $5.0 \times 10^{-7}$ & 0.2 & $5.0 \times 10^{-7}$ & 0.5 \\
\midrule
ASFT & $3.0 \times 10^{-7}$ & 0.001 & $1.0 \times 10^{-6}$ & 0.2 & $7.0 \times 10^{-7}$ & 0.1 \\
\midrule
APO Zero & $3.0 \times 10^{-7}$ & 0.001 & $3.0 \times 10^{-7}$ & 0.005 & $3.0 \times 10^{-7}$ & 0.003 \\
\midrule
NCA & $3.0 \times 10^{-7}$ & 0.0001 & $3.0 \times 10^{-7}$ & 0.0005 & $3.0 \times 10^{-7}$ & 0.0003 \\
\midrule
Cal-DPO & $3.0 \times 10^{-7}$ & 0.00003 & $5.0 \times 10^{-7}$ & 0.0003 & $3.0 \times 10^{-7}$ & 0.0003 \\
\bottomrule
\end{tabular}
\caption{Best hyperparameters for each DAA method across setups.}
\label{tab:best_hypers}
\end{table*}


\subsection{Generation Details}

We evaluated model performance on AlpacaEval 2 and ArenaHard for UltraFeedback setups, while for the Reddit TL;DR setup, we used side-by-side comparisons with GPT-4o on a curated golden validation subset of 500 samples. Additionally, KL divergence was measured on the validation subset for all setups using the generation hyperparameters listed in Table~\ref{tab:generation_hyperparameters}. For ArenaHard, the temperature was set to 0 to adhere to the original benchmark configuration.

\begin{table*}[htbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Temperature & 0.9 \\
Top-k & 40 \\
Top-p & 1.0 \\
Max New Tokens & 256 (TL;DR setup), 4096 (UF setup) \\
\bottomrule
\end{tabular}
\caption{Generation hyperparameters for Llama 3.1 8B and Llama 3.2 3B models.}
\label{tab:generation_hyperparameters}
\end{table*}



\section{Equivalence of ASFT Loss and Binary Cross-Entropy Loss}
\label{app:asft_bce}

\begin{lemma}
\label{lem:oddsloglike}
$$\log \sigma(r^\mathrm{odds}_\theta(y, x)) = \log \pi_\theta(y|x)$$
\end{lemma}
\begin{proof}
\begin{align*}
&\log \sigma(r^\mathrm{odds}_\theta(y, x)) = \log \sigma(\log \frac{\pi_\theta(y | x)}{1 - \pi_\theta(y | x)}) = \log \frac{1}{1 + e ^ {\log(1 - \pi_\theta(y | x)) - \log(\pi_\theta(y | x))}} = \log \frac{1}{1 + \frac{1 - \pi_\theta(y | x)}{\pi_\theta(y | x)}} \\&= -\log\Big( 1 +\frac{1 - \pi_\theta(y | x)}{\pi_\theta(y | x)}\Big) = -\log \frac{\pi_\theta(y | x) + 1 - \pi_\theta(y | x)}{\pi_\theta(y | x)}= \log \pi_\theta(y | x).
\end{align*}
\end{proof}

\begin{lemma}
\label{lem:oddslogunlike}
$$\log \sigma(-r^\mathrm{odds}_\theta(y, x)) = \log \big(1 - \pi_\theta(y|x)\big)$$
\end{lemma}
\begin{proof}
\begin{align*}
&\log \sigma(-r^\mathrm{odds}_\theta(y, x)) = \log \sigma(-\log \frac{\pi_\theta(y | x)}{1 - \pi_\theta(y | x)}) = \log \frac{1}{1 + e ^ {\log(\pi_\theta(y | x)) - \log(1 - \pi_\theta(y | x))}} = \log \frac{1}{1 + \frac{\pi_\theta(y | x) }{1 - \pi_\theta(y | x)}} = \\&-\log\Big(1 + \frac{\pi_\theta(y | x) }{1 - \pi_\theta(y | x)}\Big) =  -\log \frac{1 -\pi_\theta(y | x)  + \pi_\theta(y | x) }{1 - \pi_\theta(y | x) }  = \log (1 - \pi_\theta(y | x) ).
\end{align*}
\end{proof}

\begin{theorem}

\(\mathcal{L}_\mathrm{ASFT}\) is equivalent to the binary cross-entropy loss, encompassing both likelihood and unlikelihood components:
 \[\mathcal{L}_\mathrm{ASFT} =  -(1+\lambda)\log \pi_\theta(y_w|x) - \lambda \log \big(1 - \pi_\theta(y_l|x)\big).\]

\end{theorem}

\begin{proof}
To show that \(\mathcal{L}_\mathrm{ASFT}\) is equivalent to the BCE loss, we start with the definition:
\begin{align*}
\mathcal{L}_\mathrm{ASFT} = -\log \pi_\theta(y_w|x) - \lambda \log \sigma(r^\mathrm{odds}_\theta(y_w, x)) - \lambda \log \sigma(-r^\mathrm{odds}_\theta(y_l, x)),
\end{align*}

where $r^\mathrm{odds}_\theta(y, x) = \frac{\pi_\theta(y | x) }{1 - \pi_\theta(y, x)}$. Applying Lemma \ref{lem:oddsloglike} and Lemma \ref{lem:oddslogunlike} to the expression, we obtain:

\begin{align*}
\mathcal{L}_\mathrm{ASFT} = -\log \pi_\theta(y_w|x) - \lambda \log \pi_\theta(y_w|x) - \lambda \log \big(1 - \pi_\theta(y_l|x)\big) = -(1 + \lambda)\log\pi_\theta(y_w|x) - \lambda\log (1 - \pi_\theta(y_l|x)).
\end{align*}



\end{proof}

\section{Relationship Between ORPO and ASFT Loss Functions}
\label{app:orpo_asft}

\begin{theorem}
\(\mathcal{L}_\mathrm{ORPO}\) can be expressed as:
\begin{align*}
\mathcal{L}_\mathrm{ORPO} = \mathcal{L}_\mathrm{ASFT} + \lambda \log\big(\pi_\theta(y_w|x)(1 - \pi_\theta(y_l|x)) + \pi_\theta(y_l|x)(1 - \pi_\theta(y_w|x))\big).
\end{align*}

\end{theorem}

\begin{proof}
We start by defining the ORPO loss:
\begin{align*}
\mathcal{L}_{\mathrm{ORPO}} = -\log \pi_\theta(y_w|x)-\lambda \log \sigma\bigg(\log \frac{\pi(y_w|x)}{1 - \pi(y_w | x)} - \log \frac{\pi(y_l|x)}{1 - \pi(y_l | x)}\bigg).
\end{align*}

Expanding the second term using the identity \(\log \sigma(x) = x - \log(e^x + 1)\), we get:
\begin{align*}
 &- \log \sigma\bigg(\log \frac{\pi_\theta(y_w|x)}{1 - \pi_\theta(y_w | x)} - \log \frac{\pi_\theta(y_l|x)}{1 - \pi_\theta(y_l | x)}\bigg) \\&= \log \frac{1 - \pi_\theta(y_w|x)}{\pi_\theta(y_w|x)} + \log \frac{\pi_\theta(y_l|x)}{1 - \pi_\theta(y_l|x)} + \log \left( \frac{\pi_\theta(y_w|x)(1 - \pi_\theta(y_l|x))}{\pi_\theta(y_l|x)(1 - \pi_\theta(y_w|x))} + 1 \right) \\
&= \log \frac{1 - \pi_\theta(y_w|x)}{\pi_\theta(y_w|x)} + \log \frac{\pi_\theta(y_l|x)}{1 - \pi_\theta(y_l|x)} + \log \left( \frac{\pi_\theta(y_w|x) - 2 \pi_\theta(y_w|x) \pi_\theta(y_l|x) + \pi_\theta(y_l|x)}{\pi_\theta(y_l|x)(1 - \pi_\theta(y_w|x))} \right) \\
&= \underbrace{-\log \pi_{\theta}(y_w | x) - \log (1 - \pi_{\theta}(y_l | x)) + \log \left( \pi_\theta(y_w|x) - 2 \pi_\theta(y_w|x) \pi_\theta(y_l|x) + \pi_\theta(y_l|x) \right)}_{\mathrm{ORPO}_\mathrm{Align}} .
\end{align*}

Combining all terms, we obtain:
\begin{align*}
\mathcal{L}_\mathrm{ORPO} &=  -(1+\lambda)\log \pi_\theta(y_w|x) - \lambda \log (1 - \pi_\theta(y_l|x)) + \lambda \log\big(\pi_\theta(y_w|x)(1 - \pi_\theta(y_l|x)) + \pi_\theta(y_l|x)(1 - \pi_\theta(y_w|x))\big) \\&=\mathcal{L}_\mathrm{ASFT} + \lambda \log\big(\pi_\theta(y_w|x)(1 - \pi_\theta(y_l|x)) + \pi_\theta(y_l|x)(1 - \pi_\theta(y_w|x))\big)
\end{align*}

% Now, notice that the first two terms equal to ASFT \(L_{align}\),
% \[
% L_{align}^{ASFT} = -\log P_{\theta}(y_w | x) - \log (1 - P_{\theta}(y_l | x)),
% \]

% Thus, we can express \(L_{OR}\) as:
% \begin{align*}
% L_{OR} = L_{ASFT} + \log \left( P_{\theta}(y_w|x) - 2 P_{\theta}(y_w|x) P_{\theta}(y_l|x) + P_{\theta}(y_l|x) \right),
% \end{align*}

% To investigate the effect of the additional term on \(x = P_{\theta}(y_w|x) \) and \(y = P_{\theta}(y_l|x) \), let us define this term as:
% \[
% f(x, y) = x - 2 x y + y.
% \]

% Now observe that \( f(x, y \) is symmetric with respect to \( x \) and \( y \), meaning that:
% \[
% f(x, y) = f(y, x).
% \]

% This symmetry implies that interchanging \( P_{\theta}(y_w|x) \) and \( P_{\theta}(y_l|x) \) does not change the value of \( f \), which mathematically indicates that \( f \) applies equally to both probabilities. 

\end{proof}

\section{Proof of Theorem~\ref{thm:asft_orpo_collinearity}}
\begin{theorem}[Collinearity of \(\beta\)-ASFT and ORPO Gradients]
\label{app:asft_orpo_collinearity}
Let
\[
\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}} 
= 
- \log \sigma\!\bigl(\beta\,r^\mathrm{odds}_\theta(y_w,x)\bigr)
- \log \sigma\!\bigl(-\beta\,r^\mathrm{odds}_\theta(y_l,x)\bigr),
\]
where 
\[
r^\mathrm{odds}_\theta(y,x)
=\log\!\Bigl(\tfrac{\pi_\theta(y\mid x)}{1-\pi_\theta(y\mid x)}\Bigr).
\]
Define the ORPO alignment loss as
\[
\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}
=-\,\log \sigma\bigl(r^\mathrm{odds}_\theta(y_w,x) \;-\;r^\mathrm{odds}_\theta(y_l,x)\bigr).
\]
Then,
\[
\lim_{\beta \to 0}
\frac{\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}}{
      \bigl\|\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}\bigr\|}
=
\frac{\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}}{
      \bigl\|\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\bigr\|},
\]
i.e., their gradients become collinear \emph{in the same direction} as \(\beta \to 0\).
\end{theorem}

\begin{proof}
\textbf{Step 1. Gradient of \(\beta\)-ASFT.}\\
Denote \(p_w = \pi_\theta(y_w\mid x)\), \(p_l = \pi_\theta(y_l\mid x)\). Then 
\[
r^\mathrm{odds}_\theta(y_w,x)
=\log\!\Bigl(\tfrac{p_w}{1-p_w}\Bigr),
\quad
r^\mathrm{odds}_\theta(y_l,x)
=\log\!\Bigl(\tfrac{p_l}{1-p_l}\Bigr).
\]
By definition,
\[
\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}} 
= 
- \log \sigma\!\bigl(\beta\,r^\mathrm{odds}_\theta(y_w,x)\bigr)
\;-\;
\log \sigma\!\bigl(-\beta\,r^\mathrm{odds}_\theta(y_l,x)\bigr).
\]
For small \(\beta\), a first-order Taylor expansion of \(\sigma(\beta\,z)\) around \(0\) yields 
\(\sigma(\beta\,z) = \tfrac12 + \tfrac{\beta\,z}{4} + O(\beta^2)\).
Thus, 
\(\sigma(\beta\,r^\mathrm{odds}_\theta(y_w,x)) \approx \tfrac12\)
and 
\(\sigma(-\,\beta\,r^\mathrm{odds}_\theta(y_l,x)) \approx \tfrac12\).
Taking gradients and applying the chain rule gives each term approximately proportional to \(\pm\,\beta\,\nabla_\theta[r^\mathrm{odds}_\theta(\cdot)]\).  Concretely,
\[
\nabla_{\theta}\bigl[-\,\log \sigma(\beta\,r^\mathrm{odds}_\theta(y_w,x))\bigr]
\;\approx\;
-\,\tfrac{\beta}{2}\,\nabla_{\theta}\bigl[r^\mathrm{odds}_\theta(y_w,x)\bigr],
\]
\[
\nabla_{\theta}\bigl[-\,\log \sigma(-\,\beta\,r^\mathrm{odds}_\theta(y_l,x))\bigr]
\;\approx\;
+\;\tfrac{\beta}{2}\,\nabla_{\theta}\bigl[r^\mathrm{odds}_\theta(y_l,x)\bigr].
\]
Hence, summing up,
\[
\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}
\;\approx\;
\frac{\beta}{2}\,\Bigl[
\nabla_{\theta}r^\mathrm{odds}_\theta(y_l,x)
\;-\;
\nabla_{\theta}r^\mathrm{odds}_\theta(y_w,x)
\Bigr].
\]
Observe that \(\beta>0\) implies the overall scalar factor \(\tfrac{\beta}{2}\) is strictly \emph{positive} in front of the difference of gradients.

\medskip
\noindent
\textbf{Step 2. Gradient of ORPO alignment loss.}\\
Define \(\Delta r^\mathrm{odds}_\theta(x) = r^\mathrm{odds}_\theta(y_w,x) - r^\mathrm{odds}_\theta(y_l,x)\). Then
\[
\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}
=-\,\log \sigma\bigl(\Delta r^\mathrm{odds}_\theta(x)\bigr).
\]
Its gradient (using the chain rule) is proportional to
\[
\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}
\;\propto\;
-\nabla_{\theta}\bigl[
r^\mathrm{odds}_\theta(y_w,x)-r^\mathrm{odds}_\theta(y_l,x)
\bigr]
\;=\;
\nabla_{\theta}r^\mathrm{odds}_\theta(y_l,x)
\;-\;
\nabla_{\theta}r^\mathrm{odds}_\theta(y_w,x).
\]
Up to a strictly positive logistic factor (since \(\sigma(\cdot)\in(0,1)\)), the coefficient in front of \(\nabla_\theta[r^\mathrm{odds}_\theta(\cdot)]\) remains negative, but we track the \emph{absolute} scalar to see it is positive. Indeed, one can write
\[
-\nabla_{\theta}\bigl(\Delta r^\mathrm{odds}_\theta(x)\bigr)
=
\kappa_{\mathrm{ORPO}}
\,
\nabla_{\theta}r^\mathrm{odds}_\theta(y_l,x)
-
\kappa_{\mathrm{ORPO}}
\,
\nabla_{\theta}r^\mathrm{odds}_\theta(y_w,x),
\quad
\kappa_{\mathrm{ORPO}}>0.
\]

\medskip
\noindent
\textbf{Step 3. Conclusion (positive collinearity).}\\
Comparing the two gradients:
\[
\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}
\;\approx\;
\tfrac{\beta}{2}\,\bigl[
\nabla_{\theta}r^\mathrm{odds}_\theta(y_l,x)
-
\nabla_{\theta}r^\mathrm{odds}_\theta(y_w,x)
\bigr],
\quad
\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}
\;\propto\;
\bigl[
\nabla_{\theta}r^\mathrm{odds}_\theta(y_l,x)
-
\nabla_{\theta}r^\mathrm{odds}_\theta(y_w,x)
\bigr].
\]
The ratio is thus strictly \emph{positive} for small \(\beta\). Consequently,
\[
\lim_{\beta \to 0}
\frac{\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}}{
      \|\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ASFT}_\mathrm{Align}}\|}
\;=\;
\frac{\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}}{
      \|\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\|},
\]
establishing collinearity in the same direction.
\end{proof}


\section{Proof of Theorem~\ref{thm:orpo_orpo_collinearity}}
\begin{theorem}[Collinearity of \(\beta\)-ORPO and ORPO Gradients]
\label{app:orpo_orpo_collinearity}
Let
\[
\Delta r^\mathrm{odds}_\theta(x) 
=
r^\mathrm{odds}_\theta(y_w,x)
-
r^\mathrm{odds}_\theta(y_l,x),
\]
and consider
\[
\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}
=
-\;\log \sigma\bigl(\beta\,\Delta r^\mathrm{odds}_\theta(x)\bigr).
\]
Its gradient is collinear with the gradient of the standard ORPO alignment loss
\[
\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}
=
-\;\log \sigma\!\bigl(\Delta r^\mathrm{odds}_\theta(x)\bigr)
\]
for any fixed \(\beta > 0\). Formally,
\[
\frac{\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}}{
      \bigl\|\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}\bigr\|}
=
\frac{\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}}{
      \bigl\|\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\bigr\|}.
\]
\end{theorem}

\begin{proof}
\textbf{Step 1. Gradient of \(\beta\)-ORPO.}\\
Let \(\Delta r^\mathrm{odds}_\theta(x)
= r^\mathrm{odds}_\theta(y_w,x)-r^\mathrm{odds}_\theta(y_l,x)\). Then
\[
\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}
=
-\;\log \sigma\bigl(\beta\,\Delta r^\mathrm{odds}_\theta(x)\bigr).
\]
By the chain rule,
\[
\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}
=
-\,\frac{1}{\sigma(\beta\,\Delta r^\mathrm{odds}_\theta(x))}
\,\sigma'\!\bigl(\beta\,\Delta r^\mathrm{odds}_\theta(x)\bigr)
\,\beta
\,\nabla_{\theta}\bigl[\Delta r^\mathrm{odds}_\theta(x)\bigr].
\]
Since \(\sigma'(z)=\sigma(z)\,[1-\sigma(z)]\), we have
\[
-\,\frac{1}{\sigma(\beta\,\Delta r^\mathrm{odds}_\theta(x))}
\,\sigma'\!\bigl(\beta\,\Delta r^\mathrm{odds}_\theta(x)\bigr)
=
-\,\beta
\bigl[\,1-\sigma\bigl(\beta\,\Delta r^\mathrm{odds}_\theta(x)\bigr)\bigr].
\]
Thus,
\[
\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}
=
-\,\beta
\Bigl[\,1-\sigma\bigl(\beta\,\Delta r^\mathrm{odds}_\theta(x)\bigr)\Bigr]
\;
\nabla_{\theta}\bigl[\Delta r^\mathrm{odds}_\theta(x)\bigr].
\]
Since \(\beta>0\) and \(1-\sigma(\cdot)>0\), the factor multiplying 
\(\nabla_{\theta}[\Delta r^\mathrm{odds}_\theta(x)]\)
is strictly \emph{negative}. 

\medskip
\noindent
\textbf{Step 2. Gradient of standard ORPO (i.e.\ \(\beta=1\)).}\\
For 
\[
\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}
=
-\,\log \sigma\!\bigl(\Delta r^\mathrm{odds}_\theta(x)\bigr),
\]
the gradient is
\[
\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}
\;=\;
-\,\bigl[\,1-\sigma(\Delta r^\mathrm{odds}_\theta(x))\bigr]
\;
\nabla_{\theta}\bigl[\Delta r^\mathrm{odds}_\theta(x)\bigr].
\]
This also has a strictly negative scalar in front of 
\(\nabla_{\theta}\bigl[\Delta r^\mathrm{odds}_\theta(x)\bigr]\).  

\medskip
\noindent
\textbf{Step 3. Conclusion (exact positive ratio).}\\
Since 
\(\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}\)
and
\(\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\)
both differ from 
\(\nabla_{\theta}\bigl[\Delta r^\mathrm{odds}_\theta(x)\bigr]\)
by a \emph{negative} coefficient, it follows that these two gradients coincide up to a strictly \emph{positive} factor:
\[
\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}
=
\kappa(\beta)\,\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}},
\quad
\kappa(\beta)>0.
\]
Hence
\[
\frac{\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}}{
      \|\nabla_{\theta}\,\mathcal{L}^\beta_{\mathrm{ORPO}_\mathrm{Align}}\|}
=
\frac{\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}}{
      \|\nabla_{\theta}\,\mathcal{L}_{\mathrm{ORPO}_\mathrm{Align}}\|},
\]
proving the claimed collinearity (in the same direction) for every fixed \(\beta>0\).
\end{proof}

\section{Proof of Theorem~\ref{thm:dpo_unified_collinearity}}

\begin{theorem}[Unified Collinearity of DPO with IPO, SimPO, NCA, Cal-DPO, and APO-Zero]
\label{app:dpo_unified_collinearity}
Let 
\[
\Delta r^\mathrm{ref}_\theta(x)
=
r^\mathrm{ref}_\theta\bigl(y_w,x\bigr)
-
r^\mathrm{ref}_\theta\bigl(y_l,x\bigr),
\]
and define the DPO loss
\[
\mathcal{L}_\mathrm{DPO}
=
-\log\!\Bigl(\sigma\bigl(\beta\,\Delta r^\mathrm{ref}_\theta(x)\bigr)\Bigr),
\quad
\beta>0.
\]
For each method 
\(
X \;\in\;\bigl\{\mathrm{IPO},\,\mathrm{SimPO},\,\mathrm{NCA},\,\mathrm{Cal\text{-}DPO},\,\mathrm{APO\text{-}Zero}\bigr\},
\)
as \(\beta\to 0\), the gradient of \(\mathcal{L}_X\) is asymptotically collinear (i.e., it differs by a \emph{positive} factor) with the gradient of \(\mathcal{L}_\mathrm{DPO}\). Formally,
\[
\lim_{\beta \to 0}
\frac{\nabla_\theta\,\mathcal{L}_X}{
      \|\nabla_\theta\,\mathcal{L}_X\|}
=
\frac{\nabla_\theta\,\mathcal{L}_\mathrm{DPO}}{
      \|\nabla_\theta\,\mathcal{L}_\mathrm{DPO}\|}.
\]
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:dpo_unified_collinearity}]
\textbf{Step 1: DPO as the baseline (tracking its sign).} \\
By definition,
\[
\mathcal{L}_\mathrm{DPO}
=
-\,\log \sigma\bigl(\beta\,\Delta r^\mathrm{ref}_\theta(x)\bigr).
\]
Since \(\sigma(u) = 1/(1 + e^{-u})\), for \(\beta>0\), one computes
\[
\nabla_\theta\,\mathcal{L}_\mathrm{DPO}
=
-\beta\Bigl[1 - \sigma\bigl(\beta\,\Delta r^\mathrm{ref}_\theta(x)\bigr)\Bigr]
\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x).
\]
Observe that \(\beta>0\) and \(\sigma(\cdot)\in(0,1)\) imply 
\[
1 - \sigma\bigl(\beta\,\Delta r^\mathrm{ref}_\theta(x)\bigr) > 0.
\]
Hence the factor multiplying \(\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x)\) is \emph{negative}.  To unify directions by a \emph{positive} multiple, note
\[
-\nabla_\theta\,\mathcal{L}_\mathrm{DPO}
=
\beta\Bigl[1 - \sigma\bigl(\beta\,\Delta r^\mathrm{ref}_\theta(x)\bigr)\Bigr]
\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x),
\]
which has a strictly positive scalar in front.  Thus, \(\nabla_\theta\,\mathcal{L}_\mathrm{DPO}\) is collinear with \(\nabla_\theta\,\Delta r^\mathrm{ref}_\theta\), and in particular its \emph{negative} is a positive multiple of \(\nabla_\theta\,\Delta r^\mathrm{ref}_\theta\).

\medskip
\noindent
\textbf{Step 2: IPO.} \\
The IPO loss is 
\[
\mathcal{L}_\mathrm{IPO}
=
\Bigl(\Delta r^\mathrm{ref}_\theta(x) \;-\; \tfrac{1}{2\beta}\Bigr)^2.
\]
Its gradient is
\[
\nabla_\theta\,\mathcal{L}_\mathrm{IPO}
=
2\,\Bigl(\Delta r^\mathrm{ref}_\theta(x) \;-\; \tfrac{1}{2\beta}\Bigr)
\,\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x).
\]
As \(\beta \to 0\), the term \(\tfrac{1}{2\beta}\) dominates \(\Delta r^\mathrm{ref}_\theta(x)\). Hence,
\[
\Delta r^\mathrm{ref}_\theta(x) \;-\; \tfrac{1}{2\beta}
\;\approx\;
-\,\tfrac{1}{2\beta},
\]
so
\[
\nabla_\theta\,\mathcal{L}_\mathrm{IPO}
\;\approx\;
-\;\frac{1}{\beta}\;\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x).
\]
We compare this with 
\[
\nabla_\theta\,\mathcal{L}_\mathrm{DPO}
\;=\;
-\,\beta\Bigl[1 - \sigma\bigl(\beta\,\Delta r^\mathrm{ref}_\theta(x)\bigr)\Bigr]
\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x).
\]
Both gradients are negative multiples of \(\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x)\).  Therefore,
\[
\nabla_\theta\,\mathcal{L}_\mathrm{IPO}
=
\kappa_{\mathrm{IPO}}(\beta)
\,
\nabla_\theta\,\mathcal{L}_\mathrm{DPO},
\quad
\text{with }
\kappa_{\mathrm{IPO}}(\beta)>0
\text{ as } \beta\to 0.
\]
Hence they are collinear in the \emph{same} direction asymptotically.

\medskip
\noindent
\textbf{Step 3: SimPO.} \\
The SimPO loss is
\[
\mathcal{L}_\mathrm{SimPO}
=
-\log \sigma\bigl(\beta\,\Delta s_\theta - \gamma\bigr),
\]
where \(\Delta s_\theta = \log\pi_\theta(y_w \mid x) - \log\pi_\theta(y_l \mid x)\). Its gradient takes the form
\[
\nabla_\theta\,\mathcal{L}_\mathrm{SimPO}
=
-\frac{\beta\,\bigl[\,1 - \sigma(\beta\,\Delta s_\theta - \gamma)\bigr]}{\sigma(\beta\,\Delta s_\theta - \gamma)}
\;\nabla_\theta\,\Delta s_\theta.
\]
Again, \(\beta>0\) and \(1-\sigma(\cdot)>0\).  Also, \(\sigma(\beta\,\Delta s_\theta - \gamma)\in(0,1)\).  Thus the prefactor 
\[
-\frac{\beta\,\bigl[\,1 - \sigma(\beta\,\Delta s_\theta - \gamma)\bigr]}{\sigma(\beta\,\Delta s_\theta - \gamma)}
\]
is strictly negative for each \(\beta>0\).  Therefore, just like DPO, \(\nabla_\theta\,\mathcal{L}_\mathrm{SimPO}\) is in the negative direction of \(\nabla_\theta\,\Delta s_\theta\).  But \(\nabla_\theta\,\Delta s_\theta\) is proportionally the same as \(\nabla_\theta\,\Delta r^\mathrm{ref}_\theta\) for small-\(\beta\) expansions (both are differences of log-likelihood or reward-like terms).  So
\[
\nabla_\theta\,\mathcal{L}_\mathrm{SimPO}
=
\kappa_{\mathrm{SimPO}}(\beta)
\,
\nabla_\theta\,\mathcal{L}_\mathrm{DPO},
\quad
\kappa_{\mathrm{SimPO}}(\beta)>0
\text{ for small } \beta.
\]
Hence they are collinear with a positive factor in the low-\(\beta\) limit.

\medskip
\noindent
\textbf{Step 4: NCA.} \\
Define 
\[
r^\mathrm{ref}_w 
\;=\;
r^\mathrm{ref}_\theta\bigl(y_w,x\bigr),
\quad
r^\mathrm{ref}_l 
\;=\;
r^\mathrm{ref}_\theta\bigl(y_l,x\bigr).
\]
Then NCA is 
\[
\mathcal{L}_\mathrm{NCA}
=
-\log \sigma\bigl(\beta\,r^\mathrm{ref}_w\bigr)
-\tfrac12\,\log \sigma\bigl(-\,\beta\,r^\mathrm{ref}_w\bigr)
-\tfrac12\,\log \sigma\bigl(-\,\beta\,r^\mathrm{ref}_l\bigr).
\]
For small \(\beta\), expand
\[
\sigma(\beta\,z)
=
\frac12
\;+\;
\frac{\beta\,z}{4}
\;+\;
O(\beta^2),
\]
so \(\log\sigma(\beta\,z)= \log \tfrac12 + \log\!\Bigl(1 + \frac{\beta\,z}{2} + O(\beta^2)\Bigr)\).  Each gradient term then yields a linear-in-\(\beta\) combination of \(\nabla_\theta\,r^\mathrm{ref}_w\) and \(\nabla_\theta\,r^\mathrm{ref}_l\).  Collecting terms shows that, as \(\beta\to 0\),
\[
\nabla_\theta\,\mathcal{L}_\mathrm{NCA}
\;\propto\;
\beta \,\nabla_\theta\,\bigl(r^\mathrm{ref}_w - r^\mathrm{ref}_l\bigr)
\;=\;
\beta\,\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x).
\]
Comparing this with 
\(\nabla_\theta\,\mathcal{L}_\mathrm{DPO} = -\beta\bigl[\,1-\sigma(\dots)\bigr]\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x)\)
reveals another negative factor on the DPO side.  In ratio form,
\[
\nabla_\theta\,\mathcal{L}_\mathrm{NCA}
=
\kappa_{\mathrm{NCA}}(\beta)
\,
\nabla_\theta\,\mathcal{L}_\mathrm{DPO}
\quad
\text{with } 
\kappa_{\mathrm{NCA}}(\beta)>0
\text{ for small }\beta.
\]
Hence collinearity follows.

\medskip
\noindent
\textbf{Step 5: Cal-DPO.} \\
The Cal-DPO loss is
\[
\mathcal{L}_\mathrm{Cal\text{-}DPO}
=
-\log\sigma\bigl(\Delta r^\mathrm{ref}_\theta(x)\bigr)
+
\bigl(r^\mathrm{ref}_w - \tfrac{1}{2\beta}\bigr)^2
+
\bigl(r^\mathrm{ref}_l + \tfrac{1}{2\beta}\bigr)^2.
\]
For \(\beta\) near \(0\), the large constants \(\pm \tfrac{1}{2\beta}\) dominate.  The gradient w.r.t.\ \(\theta\) in these squared terms is effectively
\[
\propto
-\tfrac{1}{\beta}\,\nabla_\theta\,r^\mathrm{ref}_w
\;+\;
\tfrac{1}{\beta}\,\nabla_\theta\,r^\mathrm{ref}_l
\;=\;
-\,\tfrac{1}{\beta}\,\nabla_\theta\,\bigl(r^\mathrm{ref}_w - r^\mathrm{ref}_l\bigr)
\;=\;
-\,\tfrac{1}{\beta}\,\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x).
\]
Since \(\nabla_\theta\,\mathcal{L}_\mathrm{DPO}\) has the same negative sign structure in front of \(\nabla_\theta\,\Delta r^\mathrm{ref}_\theta\), their ratio is again positive.  Thus
\[
\nabla_\theta\,\mathcal{L}_\mathrm{Cal\text{-}DPO}
=
\kappa_{\mathrm{Cal\text{-}DPO}}(\beta)\,
\nabla_\theta\,\mathcal{L}_\mathrm{DPO}
\quad
\text{with } 
\kappa_{\mathrm{Cal\text{-}DPO}}(\beta)>0
\text{ as } \beta \to 0.
\]

\medskip
\noindent
\textbf{Step 6: APO-Zero.} \\
APO-Zero is given by
\[
\mathcal{L}_{\mathrm{APO\text{-}Zero}}
=
-\,\sigma\!\bigl(\beta\,r^\mathrm{ref}_w\bigr)
\;+\;
\sigma\!\bigl(\beta\,r^\mathrm{ref}_l\bigr).
\]
Its gradient involves terms 
\(\nabla_\theta\,\sigma(\beta\,r^\mathrm{ref}_w)\) and \(\nabla_\theta\,\sigma(\beta\,r^\mathrm{ref}_l)\), each proportional to \(\beta\,\nabla_\theta\,r^\mathrm{ref}_w\) and \(\beta\,\nabla_\theta\,r^\mathrm{ref}_l\).  Subtracting these yields
\[
\nabla_\theta\,\mathcal{L}_{\mathrm{APO\text{-}Zero}}
\;\propto\;
-\beta\,\nabla_\theta\,\bigl(r^\mathrm{ref}_w - r^\mathrm{ref}_l\bigr)
\;=\;
-\,\beta\,\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x).
\]
Since \(\nabla_\theta\,\mathcal{L}_\mathrm{DPO}\) also has a negative constant factor, their ratio has a positive limit.  Therefore,
\[
\nabla_\theta\,\mathcal{L}_{\mathrm{APO\text{-}Zero}}
=
\kappa_{\mathrm{APO\text{-}Zero}}(\beta)\,
\nabla_\theta\,\mathcal{L}_\mathrm{DPO},
\quad
\kappa_{\mathrm{APO\text{-}Zero}}(\beta)>0
\text{ for small }\beta.
\]

\medskip
\noindent
\textbf{Conclusion.} \\
In each method \(X\), one sees that \(\nabla_\theta\,\mathcal{L}_X\) has the same \emph{negative-sign} structure around \(\nabla_\theta\,\Delta r^\mathrm{ref}_\theta(x)\) as does \(\nabla_\theta\,\mathcal{L}_\mathrm{DPO}\), ensuring a positive ratio in the limit.  Formally,
\[
\nabla_\theta\,\mathcal{L}_X
=
\kappa_X(\beta)\,\nabla_\theta\,\mathcal{L}_\mathrm{DPO},
\quad
\kappa_X(\beta)>0,
\quad
\text{as } \beta\to 0.
\]
Thus,
\[
\lim_{\beta \to 0}
\frac{\nabla_\theta\,\mathcal{L}_X}{\|\nabla_\theta\,\mathcal{L}_X\|}
=
\frac{\nabla_\theta\,\mathcal{L}_\mathrm{DPO}}{\|\nabla_\theta\,\mathcal{L}_\mathrm{DPO}\|},
\]
which completes the proof of their alignment in the same direction.
\end{proof}

\section{Pareto fronts for Llama 3.2 setups}
\label{app:pareto}

\noindent
The results presented in this section correspond to the best hyperparameter configurations identified during the hyperparameter search described in Section~\ref{sec:par:beta-sens}, including the optimal learning rate for each method. This ensures that the Pareto fronts reflect the upper performance limits for alignment quality.

\begin{figure*}[h!]
\subfigure[Llama 3.2 3B TL;DR\label{fig:pareto-3b-tldr}]{%
  \includegraphics[width=0.48\columnwidth]{images/pareto-3b-tldr.pdf}%
}\hfill
\subfigure[Llama 3.2 3B UF\label{fig:pareto-3b-uf}]{%
  \includegraphics[width=0.48\columnwidth]{images/pareto-3b-uf.pdf}%
}
\caption{\textbf{Pareto front for alignment quality and KL divergence.} Results for Llama 3.2 3B TL;DR and UF setups on GPT-4 Win Rate vs. "golden" validation subset and AlpacaEval 2 LC respectively with different \(\beta\) values. Methods are grouped into pairwise and pointwise categories. For the summarization task (Llama 3.2 3B TL;DR), both pointwise and pairwise methods achieve strong overall results. For the UF setup, methods also perform similarly within overlapping confidence intervals, indicating no clear separation.}
\label{fig:3b-paretos}
\end{figure*}

\section{Toy Example Details}
\label{app:toy_exp}

To analyze the differences between pairwise and pointwise ranking methods, especially with respect to the ranking nature of alignment losses in LLMs, a simplified toy experiment was conducted under a controlled setup. A dataset of 2000 triplets \((x, y_w, y_l)\) was generated, where \(x\), \(y_w\), and \(y_l\) are real-valued scalars satisfying \(y_w > y_l\). The data was split into 80\% for training and 20\% for testing. When the model processes a scalar input \(x\) together with a candidate \(y\), these two numbers form a vector in \(\mathbb{R}^2\), which serves as the input of the Multi-Layer Perceptron (MLP) to predict the reward \(r\).

A single-hidden-layer MLP with ReLU activation was used in two capacity settings: lower (\(\text{hidden size} = 1\)) and higher (\(\text{hidden size} = 3\)). The model takes \(x\) and a candidate \(y\) as input, producing a reward \(r\) analogous to training a reward model for RLHF \cite{summarize}. 

Two losses were evaluated: the pairwise Bradley-Terry loss \cite{bt},
\[
\mathcal{L}_\text{Pairwise} = -\log\bigl(\sigma(\beta (r_w - r_l))\bigr),
\]
and the pointwise loss,
\[
\mathcal{L}_\text{Pointwise} = -\bigl[\log\bigl(\sigma(\beta r_w)\bigr) + \log\bigl(\sigma(-\beta r_l)\bigr)\bigr].
\]
Each configuration was trained over 100 runs, tuning the learning rate from \(\{0.5, 0.3, 0.1, 0.01, 0.03, 0.05\}\) and \(\beta\) from \(\{5.0, 2.0, 1.0, 0.2, 0.1, 0.05, 0.01\}\). Alignment accuracy was defined as the proportion of cases with \(r_w > r_l\). 

The results show that both methods yield comparable performance in the low-capacity regime, while pairwise ranking achieves higher accuracy as model capacity increases, mirroring the effects observed in larger-scale experiments from the Section~\ref{sec:res:pareto}.

\section{GPT-4 Side-By-Side Evaluation Prompt}
\label{app:gpt_prompts}

For our Side-By-Side evaluations with \texttt{GPT-4o}, we designed a prompt tailored to the Reddit TL;DR dataset to assess \textit{accuracy}, \textit{completeness}, \textit{relevance}, and \textit{conciseness}. The full prompt used in our experiments is detailed below.

\noindent\rule{\textwidth}{0.5pt}

\begin{verbatim}
Act as an impartial judge and evaluate the quality of the summaries provided 
by two AI assistants for the text displayed below. Your evaluation should 
consider accuracy, completeness, relevance, and conciseness.

You will be given a text, Assistant A's summary, and Assistant B's summary. 
Your job is to evaluate which assistant's summary is better based on the 
text provided.

Begin your evaluation by comparing both assistants' summaries with the 
original text. Identify and correct any inaccuracies.
Ensure the summaries are complete, capturing all essential information 
from the text without introducing fabricated details.
Assess the relevance of the information each assistant chose to include 
in their summary, ensuring it reflects the core message of the text.
Evaluate the conciseness of the summaries, favoring those that efficiently 
convey the necessary information without unnecessary verbosity.
Avoid any position biases and ensure the order in which the summaries 
were presented does not influence your decision.
Do not allow the length of the summaries to influence your evaluation, 
except in the context of conciseness and efficiency.
Do not favor certain names of the assistants.
Be as objective as possible.
You should only evaluate the summaries provided by both assistants 
and NOT the original text itself.
If both summaries are irrelevant, contain hallucinations, or are 
inconsistent with the original text, mark the comparison as inconclusive 
and choose option "C".

After providing your explanation, output your final verdict by strictly 
following this format:

"""
Comparison: <One-sentence comparison>
Winner: <A if assistant A is better, B if assistant B is better, and C for a tie.>
"""
\end{verbatim}

\noindent\rule{\textwidth}{0.5pt}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
