\section{Related works}
Much of the intuition of parselets as a recursive data type for describing the data was borrowed from Parsing Expression Grammar (\textsc{peg}~\cite{ford:2004}), where modular parsers (implementing a superset of the regexp operators we use) are synthesized into any arbitrary, full-fledged recursive descent parser. See also the discussion at end of Sec.~\ref{sec:appli:proba}.

The above describes a compressor that has a few variants in the literature of grammar-based compression, although ours exhibits a more complex language model. This discussion concentrates on the case $T_{opt}=T_{alt}=0$ (only conjunction parselets are used). Contrary to~\cite{kieffer:yang:2000}, we seek the creation of a reusable dictionary that can be stored apart. This allows to keep the irregular part of the data explicit within string data (which is called a {\em ``skeleton''} in~\cite{storer:1982}). Conjunction parselets are reminiscent of the digram structure used in~\cite{nevill:witten:1997} and of the byte pairs used in~\cite{gage:1994} (which is one method for tokenization in natural language processing), both of which are online, linear-time compressors. 

The very same offline deflation as in Alg.~\ref{alg:deflate} likely was first described by Solomonoff~\cite{solomonoff:1961}, and also more recently in the context of compressing partially ordered strings~\cite[Sec.~3.2]{alur:2003} (the authors acknowledge both inspiration from~\cite{nevill:witten:1997} and tedious implementation). It is analyzed in~\cite{savari:2004}, to which the reader is referred for a discussion on universality. Curiously enough, none of the preceding acknowledged~\cite{solomonoff:1961} as a distant cousin. This has to be an algorithm everyone has to reinvent. 

Another pionneering approach by Chaitin~\cite{chaitin:1987} along the exact same line uses Pure\textsc{lisp} S-expressions as a vehicle for the shortest program representation. This flavour of experimental algorithmic information calculus has later been extended to small Turing machines~\cite{soler:2014} on binary strings up to 12 bits. In this regard, we seek a much less powerful language than \textsc{lisp} or those of Turing machines to describe the data, and hope to handle (much) longer strings more efficiently in reward. 

Nowadays, it is customary to use an off-the-shelf compressor~\cite{cilibrasi:2005} to estimate an upper bound on the quantity of information in an individual object and build subsequent quantities from there. It turns out that the internals of such compressors may introduce unpredictable, spurious numerical artifacts that are not negligible at all (see~\cite{cebrian:2005} for a detailed account). The root causes are left-to-right parsing or buffering fixed-size blocks of data during compression, as well as concatenating input strings when computing joint information quantities. Our compressor uses no internal fixed-size block, it is offline, and only the decompressor uses left-to-right parsing. By construction, our compressor is immune to the numerical artifacts reported for off-the-shelf compressors. The price for this is explicit storage of the dictionary, worst-case quadratic-time and linear space (due to the compressor being offline and also with a high constant due to the index).

Of course, just like any other actual compressor, Alg.~\ref{alg:deflate} is subject to pathological cases. If $x=\mbox{\texttt{ababababab}}$ and $y=\mbox{\texttt{bababababa}}$, then subsequent information measures will miss the similarity because Alg.~\ref{alg:deflate} would not have issued the same parselet. This issue is more likely to occur with small alphabets. Another limitation we face is handling numerical data: assessing similarity at the numerical level by factorizing out chunks of identical digits is not going to help very much in general. However, some hope remains in the area of 8-bit regularly-sampled data (see App.~\ref{app:multimodal}).