% EV: Proofed

\section{Introduction}
\label{sec:intro}

% Context
Large-scale generative models have shattered the status quo of video generation with photorealistic, temporally coherent, high-fidelity videos synthesized from textual prompts.
While generalist models such as Sora, Veo-2 \citep{veoteam2024veo2} and VideoJAM \citep{chefer2025videojam} demonstrate those capabilities at large, specialist models such as GAIA-1 \citep{hu2023gaia1} and VISTA \citep{gao2024vista} showcase impressive performance in predicting future frames of driving videos.
Generating plausible future frames suggests that those models capture meaningful representations of the world, but the exact nature of such representations, as well as any practical utility they might have for actual driving, remain open questions. To what extent do those representations encode driving-relevant features, such as scene dynamics, geometry, and semantics? How far do they apply to actual autonomous systems, enhancing downstream tasks, such as motion planning?

% Our Approach and Contributions
To answer those questions, we introduce an open-source, large-scale autoregressive video model (\vm) and its companion video-action model (\vam).
% VM
At the core of our approach, \vm learns to predict future frames by modeling the joint distribution of spatio-temporal token sequences, capturing the underlying dynamics of driving scenes into dense representations. We use an image tokenizer to compress visual information into discrete tokens, providing a compact representation of each video frame.
% VAM
To bridge the gap between video understanding and action generation, we train an action expert module on VaViM's learned video representation, forming our complete VaVAM system. We train that module with imitation learning to generate future driving trajectories guided by high-level goals and temporal context extracted from \vm{}. That architecture forms a complete perception-to-action pipeline, enabling effective motion planning and decision-making in autonomous vehicles.

Our work is the first large-scale study to explore how video generative pre-training transfers to driving capabilities. We evaluate our approach using both open- and closed-loop driving scenarios. Our findings suggest that video-based pre-training holds great promise for autonomous driving, with the following insights:
\begin{enumerate}
    \item The learned representations from video pre-training contain rich and meaningful semantic information.
    \item Larger models generally improve video synthesis quality, reinforcing the benefits of scale for generative modeling. However, larger video models perform worse than smaller ones on semantic segmentation tasks, suggesting that better generative quality does not directly translate to better semantic understanding.
    \item Scaling up the model improves performances in open-loop evaluations, and increasing training data yields further improvements. Nonetheless, scaling model size or data does not consistently improve safety metrics in closed-loop evaluations. This reveals a fundamental conflict between trajectory-following and adaptive decision-making.
\end{enumerate}

% Key Contributions
Our key contributions are as follows:
\begin{itemize}
    % VM
    \item We provide data mix, scaling laws, training recipes, and a detailed reproducible protocol for training an autoregressive Video Model (\vm{}) on large-scale 1800+ hours diverse driving data.
    % VAM
    \item We present a procedure to adapt a video model into a video-action model (\vam{}) using imitation learning for end-to-end driving from camera input.
    % eval
    \item We propose new evaluations for the learned \vm{} representations to assess their semantic content. Additionally, we benchmark \vam{} in both open- and closed-loop driving scenarios, emphasizing safety-critical situations. \vam{} achieves state-of-the-art performance in frontal driving scenarios on NeuroNCAP \citep{ljungbergh2024neuroncap}.
\end{itemize}

\autoref{tab:model_overview} reports all \vm{} and \vam{} models produced in this research, coming in different sizes. We release both the source code and the weights for those models.

\begin{table*}[h]
\caption{
Overview of the released models, covering different model sizes (up to 1.2B), trained on increasing amounts of data, and two model types (video generation and action learning).
}
\label{tab:model_overview}
\centering
%\begin{tabularx}{0.7\textwidth}{ l l Y Y Y}
\begin{tabular}{l l c c c}
\toprule
\Th{Model} & \Th{Parameters (M)} & \Th{OpenDV} \citep{yang2024opendv} & \Th{nuPlan} \citep{caesar2021nuplan} & \Th{nuScenes} \citep{caesar2020nuscenes} \\
& & \textit{1700+ hours} & \textit{94 hours} & \textit{5.5 hours} \\
\midrule
\multicolumn{5}{c}{\cellcolor{valeocell} \texttt{Pre-trained video models}} \\
%\midrule
VaViM-S & 185 & \ding{51} & & \\
VaViM-B & 318 & \ding{51} & & \\
VaViM-L & 1,200 & \ding{51} & & \\
\midrule
\multicolumn{5}{c}{\cellcolor{valeocell} \texttt{Fine-tuned video models}} \\
%\midrule
VaViM-S & 185 & \ding{51} & \ding{51} & \ding{51} \\
VaViM-B & 318 & \ding{51} & \ding{51} & \ding{51} \\
VaViM-L & 1,200 & \ding{51} & \ding{51} & \ding{51} \\
\midrule
\multicolumn{5}{c}{\cellcolor{valeocell} \texttt{Video-action models trained with imitation learning}} \\
%\midrule
VaVAM-S & 185 + 21 &  & \ding{51} & \ding{51} \\
VaVAM-B & 318 + 38 &  & \ding{51} & \ding{51} \\
VaVAM-L & 1,200 + 150 &  & \ding{51} & \ding{51} \\
\bottomrule
%\end{tabularx}
\end{tabular}
\end{table*}
