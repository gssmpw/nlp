\section{Experiments and Results}
\label{sec:exps_and_results}

\paragraph{Additional evaluation datasets.} We evaluate \vm{} and \vam on datasets not used for their training, namely Cityscapes~\cite{Cordts2016Cityscapes} and KITTI~\cite{geiger2013kitti}. We refer to the latter as \emph{KITTI} when we downsample the clips to 8 frames at 2 Hz (to match the training FPS) and as \emph{KITTI-1f} when we downsample the clips to a single frame.


\subsection{\vm{} video pre-training evaluation}

\subsubsection{Generation quality}
% ------------------------------------------------------------------------------------
\input{tables/fid}
% ------------------------------------------------------------------------------------


To evaluate the quality of the generation of our \vm, we use the Frechet Inception Distance (FID)~\cite{heusel2017fid}. To account for non-object-centric settings typical of driving datasets, we use the features from a DINOv2 model, which have been shown to be richer. 

Specifically, given a context of 4 frames, we generate 4 frames with \vm. We use the features of the 4 context frames as reference images to compute the FID. We compute an FID score for each future frame individually (FID@t), \ie, FID@2 means the FID of the second generated frame. Since \vm does not directly generate images but tokens, we compute the FID for the LLamaGen-VQGAN tokenizer to act as an upper bound, as it serves as an ``oracle'', \ie, it encodes a real frame of the ``future''. We choose the same reference frames for the FID. And compute the FID individually for the 4 ground-truth future frames.
That setting changes slightly compared to the reconstruction FID, as the features of the original images considered for reconstruction are not part of the reference images used to compute the FID.
Note that, although we pre-train on a video clip, we do not use Frechet Video Distance~\cite{Unterthiner2019FVD} (FVD) for evaluation because it relies on an I3D~\cite{I3D} that requires at least 10 frames as input.

From~\autoref{tab:fid}, we observe a clear scaling benefit, where larger models consistently achieve better performance, where \vm-L outperforms \vm-B, which further outperforms \vm-S. That trend highlights the advantage of increasing model capacity to improve reconstruction quality and other evaluation metrics.

We also show qualitative results for generation on~\autoref{fig:video_generation}. We feed a context of 4 frames~\autoref{fig:context_frames} to either \vm-S~\autoref{fig:video_generation_vms} or \vm-L~\autoref{fig:video_generation_vml}. We observe that both models generate frames that are spatially coherent, with correctly generated large structures. Furthermore, we observe that \vm-L predicts frames closer to the true future (car on the left turning right) by inferring the motion from the 4 first frames. That contrasts with the generative behavior of \vm-S.


\input{figures/video_generation}


\subsubsection{Semantic segmentation}
% ----------------------------------------------------------------------------------------------------------------------------
\input{tables/segmentation}
% ----------------------------------------------------------------------------------------------------------------------------

In~\autoref{tab:segmentation}, we evaluate VaViM for semantic segmentation using the Humming-bird approach~\cite{balazevic2023hummingbird}. Specifically, we use the features of layer 12 of our VaViM's transformer to encode a frame. We sample 10 patch-features per image, using the sampling approach of the open-source Humming-bird implementation~\cite{pariza2024hbird}. We report the mean Intersection over Union (mIoU), which measures the overlap between predicted and ground truth segmentations across all classes, providing a comprehensive assessment of per-class and overall segmentation quality. Higher mIoU scores indicate better alignment with ground-truth annotations.

As observed in recent studies~\cite{Empirical_autoregressive}, auto-regressive models struggle compared to discriminative models like Dino and Dinov2~\cite{Caron2021EmergingPI,oquab2024dinov2}. We can see Dino models outperform \vm on all setups. However, this study shows that, even with the auto-regressive pre-training that does not explicitly enforce semantic understanding, \vm still manages to segment the different datasets in a zero-shot manner: indeed, neither Cityscapes nor KITTI was part of the training data mix.
\autoref{tab:segmentation} also shows that during fine-tuning on our target datasets, \vm does not lose the representation capability learned on the diverse pre-training data.

Moreover, we conduct a qualitative study on the features obtained with \vm on~\autoref{fig:pca_qual_results}. As proposed by~\cite{Caron2021EmergingPI}, we visualize as RGB primaries the 3 main components of a PCA of the features. The semantic consistency of \vm's is visible as similar colors being assigned to objects of the same class (\eg, pedestrians, cars, or road), which suggests the features hold semantic meaning, even if they are not invariant enough to perform at semantic segmentation. 



\input{figures/pca_qual_results}


% ----------------------------------------------------------------------------------------------------------------------------
\subsection{\vam{} driving evaluation}
\label{sec:evaluation_actionexpert}

\subsubsection{Open-loop evaluation}

In~\autoref{tab:minADE}, we evaluate \vam in an open-loop setup. We compute the $\textbf{minADE}_k$ ($\downarrow$), \ie, the minimum over $k$ sampled trajectories of the Average Distance Error, taken as the average of point-wise L$^2$ distances between the closest (among $k=5$) sampled trajectory and the ground-truth expert trajectory. The metric is calculated for both nuPlan~\cite{caesar2021nuplan} and nuScenes~\cite{caesar2020nuscenes}.

% \todo{add definition for minADE}

We observe a clear trend where scaling improves minADE, which shows that among the different samples, \vam is able to match the expert ones better when increasing the compute ($\sim \text{\# params} \times \text{\# data}$). Qualitatively, we observe that as smoother trajectories for ``better'' models. However, as evidenced by prior works~\cite{codevilla2018offlineEval,dauner2023parting} and our results in closed-loop evaluation (\autoref{sec:close_loop}), open-loop scores are not predictive of good driving capabilities.


\input{tables/minADE}

\subsubsection{Closed-loop evaluation}
\label{sec:close_loop}

\textbf{NeuroNCAP:} While the previous open-loop evaluation demonstrates strong trajectory prediction accuracy, it fails to capture the cascading effects of the model's decisions. Closed-loop evaluation addresses that limitation by allowing decisions to influence future observations, thus providing a more realistic assessment of safety-critical behavior.

To evaluate our model's performance in closed-loop, we employ NeuroNCAP~\cite{ljungbergh2024neuroncap}, a simulator specifically designed for testing autonomous driving systems in safety-critical scenarios. To the best of our knowledge, it is currently the only existing data-based closed-loop simulator. Other solutions are either synthetic~\cite{dosovitskiy2017carla} (leading to domain gap) or based on view reprojection~\cite{amini2020vista,amini2022vista2} (leading to limited novel views). 

NeuroNCAP employs a NeRf-based simulator that executes the driving model decision and generates the corresponding novel view. That enables photorealistic closed-loop evaluation of driving models. In particular, a key feature of NeuroNCAP is its ability to insert pre-defined adversarial agents into the scene, such as a vehicle following hazardous trajectories. 

Using that capability, the framework creates challenging test conditions inspired by the European New Car Assessment Programme (Euro NCAP), featuring three primary scenario types: stationary obstacles in the ego-lane, frontal collisions with oncoming vehicles, and side collision scenarios from cross-traffic. In our experiments, we leverage this framework to systematically evaluate our \vam model's ability to handle those challenging scenarios while maintaining its intended trajectory. \newline

\noindent \textbf{Collision metrics:} NeuroNCAP's evaluation protocol relies on two metrics: (1) the collision rate as a percentage of scenarios without collision and (2) the NeuroNCAP score (NNS) that assigns scores based on collision avoidance success and impact velocity reduction, offering a quantitative measure of the model's safety performance. More formally:
%
\begin{equation}
    \text{NNS} = \begin{cases}
        5.0 & \text{if no collision} \\
        4.0 \cdot \max(0, 1 - v_i/v_r) & \text{otherwise}
    \end{cases}
\end{equation}
\newline


\noindent \textbf{Baselines:} We first compare \vam with existing NeuroNCAP baselines. The Base-U and Base-V baselines are naïve methods that use the perception outputs from UniAD~\cite{hu2023uniad} and VAD~\cite{jiang2023vad}, respectively. They operate on a simple rule-based approach: maintaining constant velocity unless an object is detected in a corridor ahead of the ego-vehicle ($\pm$2 meters laterally and up to 2$*v_{ego}$ meters longitudinally). If an object is detected within this corridor (TTC $<$ 2s), they initiate a braking maneuver. The second class of baselines are state-of-the-art end-to-end planners, UniAD~\cite{hu2023uniad} and VAD~\cite{jiang2023vad}, which process 360° camera input, CAN-bus signals, and high-level commands to predict future trajectories.


\input{tables/neuro_ncap}

A quantitative comparison appears in \autoref{tab:eval-WM}. Traditional baseline models establish a strong foundation in static scenarios, with Base-U and Base-V achieving NeuroNCAP scores of 4.72 and 4.82, respectively, setting a robust performance threshold for advanced approaches to surpass.

VAD demonstrates superior overall performance, achieving a 12\% reduction in collision rates compared to our approach across all test conditions. Our qualitative assessment reveals that while \vam effectively identifies and responds to dynamic obstacles, it seldom initiates complete stops or urgent braking, even in scenarios where such actions would be optimal, such as encountering a stationary bus in the middle of the road.

Neither \vm nor \vam predicts occupancy, which was shown to boost performances when used to post-process estimated trajectories~\cite{ljungbergh2024neuroncap}, by allowing optimization with a classical solver. Such post-processing might also benefit \vam. Moreover, \vam currently relies exclusively on front-cam input, giving it an inherent disadvantage against systems with 360 camera arrays, especially for side-impact scenarios. Despite those limitations, \vam demonstrates competitive capabilities, notably surpassing UniAD's in side scenarios by 4\%. In frontal scenarios, \vam achieves state-of-the-art performance, with a NeuroNCAP score of 2.38.



\noindent \textbf{Limitations of the benchmark:} We unintentionally produced a noisy model that avoided the road, as it never encountered the scripted hazardous scenarios, it achieved exceptionally high safety scores in the benchmark ($>$ 4.0 NNS). That finding highlights a critical gap in the evaluation framework: while it effectively measures collision avoidance, it does not adequately assess adherence to intended driving behavior or route completion metrics. 

\input{tables/neuro_ncap_frontal_vam}

To address those limitations and provide a more comprehensive evaluation, we propose to use two complementary metrics. First, we measure the mean deviation from the guiding trajectory, which quantifies how well the model adheres to intended driving paths. It differs from the ADE metrics by measuring the mean instantaneous distance to the closest point of the reference trajectory instead of ADE's pairwise distance. Second, we introduce a goal-progress metric that measures the relative reduction in distance to the destination, formally defined as:

\begin{equation}
    \text{progress\_toward\_goal} = \max(0.0, \frac{d_{\text{initial}} - d_{\text{final}}}{d_{\text{initial}}})
    \label{eq:progress}
\end{equation}

where $d_{\text{initial}}$ and $d_{\text{final}}$ represent the initial and final distances to the goal, respectively. That formulation ensures the metric remains bounded between 0 and 1, where 1 indicates complete goal achievement. Together, those metrics effectively capture both the quality of trajectory following and task completion, providing a more robust framework for evaluating autonomous driving systems that prevent the gaming of safety metrics through undesirable driving behaviors.

\begin{wrapfigure}{L}{0.5\textwidth}
    \vspace{-1em}
    \centering
    \includegraphics[width=\linewidth]{figures/collision_rate_vs_progress}
    \caption{\textbf{Collision Rate vs. Progress.} An ideal model would be positioned at the bottom right corner of this plot, attaining the defined goal position without any collisions. \vam{} makes progress towards that ideal by being safer than UniAD, with 27\% less collisions, with minimal degradation to progress.}
    \label{fig:collision_rate_vs_progress}
    \vspace{-1em}
\end{wrapfigure} 

Using those metrics, we study the behavior of \vam in \autoref{tab:ncap_frontal}. We focus on the score of the frontal scenario as our model is front-cam only. We plot the \emph{collision rate} vs \emph{progress} in \autoref{fig:collision_rate_vs_progress}. Although our model significantly reduces collision rate, it only achieves comparable \emph{collision rate} to UniAD. We expected that as we scale the model and training data, our model would be able to increase the \emph{progress} score while improving metrics related to collisions (NeuroNCAP score and collision rate). However, as we scale, whether in data or model size, the \emph{progress} metric varies unpredictably, and the \emph{collision rate} tends to increase. At the same time, we observe that the \emph{mean deviation} decreases.





We hypothesize that such a phenomenon is mainly due to the limitations of the imitation learning methodology and the definition of the high-level command. Indeed, the model is trained exclusively through imitation learning on pre-recorded expert trajectories, which serve a dual purpose: as training data and as a guiding signal during training and inference.

During training, the model learns to follow exactly the trajectories presumably executed safely by expert drivers. However, at test time, when facing an adversarial vehicle, the model must simultaneously respect that learned behavior (following the trajectory) while adapting to a situation that may require significant deviation from it (avoiding collision).

As the training compute scales up, collision rate increases and the mean deviation metric decreases, which suggests that larger and more trained models may be overfitting to the trajectory-following behavior. Rather than learning the underlying decision-making process that still allows for safe deviation, the model becomes more rigid in its adherence to the guiding trajectory. 

%\clearpage


