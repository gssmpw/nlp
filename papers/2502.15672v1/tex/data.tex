\section{Data and Training}

\subsection{Data}
\label{sec:data}

Our desiderata for the data were to find a large dataset of non-annotated data for the pre-training and a sufficient amount of annotated data (with trajectories synchronized with perception) for fine-tuning. To that end, we train \vm and \vam on a collection of three datasets: OpenDV~\cite{yang2024opendv}, a massive non-annotated web dataset, and nuPlan~\cite{caesar2021nuplan} and nuScenes~\cite{caesar2020nuscenes}, dedicated automotive datasets captured with multiple sensors. 

\input{figures/datasets}

\noindent\paragraph{OpenDV~\cite{yang2024opendv}} The OpenDV dataset, illustrated on~\autoref{fig:opendv_examples}, is the largest driving dataset publicly available up to now, with more than 1700 hours of driving videos, collected at 60 FPS, resulting in over 360 million frames. The dataset comprises single-camera front-cam videos collected from YouTube, with annotated durations of intros (usually 90 seconds) and outros (usually 60 seconds) for trimming, to avoid artifacts such as title sequences and closing credits. Most videos are at or close to Full HD (1920$\times$1080) resolution.

We include in our data only the videos at exactly Full HD to avoid issues of aspect ratio distortion. That meant discarding 1.3\% of the videos (2.5\% of the total duration). Using FFMPEG~\cite{tomar2006converting}, we extracted the frames for the remaining videos at 10 FPS and 512$\times$288 pixels, discarding intros and outros. We stored the frames in individual JPEG files. 
We extract overlapping clips of 8 frames at 2 FPS to train \vm. Only a front camera is available for this dataset, without any metadata.

\noindent\paragraph{nuPlan~\cite{caesar2021nuplan}} The nuPlan dataset contains around 1200 hours of driving scenarios recorded in Las Vegas (838 hours), Boston, Pittsburgh, and Singapore. In particular, among the 1200-hour raw data, approximately 94-hour recordings contain sensor information (LiDAR and cameras) with a sampling rate of 10 Hz. Our project only employs the RGB images in 1274 recorded videos and the ego position. More specifically, we only use the front camera instead of the eight cameras that cover the 360-degree view around the ego vehicle illustrated in \autoref{fig:nuplan_examples}. We collect from nuPlan 2,833,723 frames for training and 492,477 for validation, together with trajectory extracted from the ego positions. We also extract overlapping clips of 8 frames at 2 Hz from the original 10 Hz video sequences.

\noindent\paragraph{nuScenes~\cite{caesar2020nuscenes}} The nuScenes dataset contains 1000 driving scenes of 20 seconds collected in Boston and Singapore in~\autoref{fig:nuscenes_examples}. Similarly to nuPlan, while nuScenes includes 6 cameras, LIDAR, RADAR, \etc, we restrict its usage to the front camera and ego position in this work. That results in a dataset of 28,130 training frames and 6,019 validation frames. We also extract overlapping clips of 8 frames. The dataset is natively synchronized at 2Hz.

As detailed in subsequent sections, we use the datasets for different steps of \vm and \vam training. Specifically, we use OpenDV only for \vm pre-training~\autoref{subsec:muP}. A mix of the three datasets to fine-tune \vm~\autoref{subsec:finetuning}. For both training steps, only the front camera is used, making those steps completely unsupervised and, thus, highly scalable. Finally, we use nuScenes and nuPlan to learn \vam through imitation learning on the ego trajectory~\autoref{sec:imitationlearning}, which are illustrated on~\autoref{fig:trajectories}. That training stage requires access to the ground-truth expert trajectory, although, interestingly, recent approaches such as GEM~\cite{hassan2024gem}, explore the use of pseudo-annotations for the ego trajectory, paving the way to scaling action learning to OpenDV-size datasets.

\input{figures/trajectories_datasets}


\subsection{\vm{} pre-training}
\label{subsec:muP}

Training large autoregressive models requires careful consideration of both parameterization and scaling strategy. In this section, we present our approach to efficiently scale \vm{} beyond 1 billion parameters.
\paragraph{Compute-Efficient Scaling with $\mu$P:} 
We adopt the Maximal Update Parametrization ($\mu$P)~\cite{yang2022miup} to enable efficient scaling, inspired by recent advances in large-scale model training~\cite{hu2024minicpm,dey2023cerebras,yao2023nanolm,li2023flm101b}. $\mu$P allows us to use the same learning rate and optimization hyperparameters across different model scales, simplifying hyperparameter tuning and stabilizing training dynamics.

$\mu$P works by reparameterizing the network's initialized weights, activations, and learning rate on a per-layer basis proportional to the network width. That enables width-independent updates and zero-shot hyperparameter transfer as model width scales~\cite{lingle2024largescalemiup, yao2023nanolm}\footnote{\href{https://blog.eleuther.ai/mutransfer/}{Eleuther.ai --- muP Practitioner's Guide}}\footnote{\href{https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5}{cloneofsimo --- What to do to scale up?}}.

We initiate hyperparameter optimization with a base model width of 256 (60M parameters). Using approximately 20\% of the pre-training data, we conduct a grid search over 50 randomly sampled hyperparameter configurations, requiring around 900 GPU hours. This phase establishes the core training hyperparameters. We then scale up to a model with width 768 (185M parameters) to fine-tune the weight decay parameter, with each candidate configuration requiring about 1254 GPU hours. At this point, all hyperparameters are fixed.



\begin{figure}[h]
    %\vspace{1em}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/images/scaling_laws.pdf}
    \caption{\textbf{\vm{} scaling law} predicts the expected validation loss (next-token cross-entropy) of the 1.2B \vm{}-L model and indicates that more data would strongly benefit our models. The green dots correspond to checkpoints used to fit the scaling law; the yellow star is the predicted performance of \vm{} (0.06\% error); the dotted red is the compute-optimal frontier~\cite{hoffmann2022chinchilla}.}
    \label{fig:scaling_law}
    %\vspace{-4em}
\end{figure}


\paragraph{Establishing Scaling Laws:} 
A scaling law empirically models the validation loss as a power law of the training data and model size. Following~\cite{hoffmann2022chinchilla}, we define it as:
%
\begin{equation}
     L(D, N) = L_0 + A \cdot D^{-\alpha} + B \cdot N^{-\beta},
\end{equation}
%

where:
\begin{itemize}
    \item $L(D, N)$ is the validation loss as a function of the dataset size $D$ and model size $N$.
    \item $D$ is the number of video clips for training the model.
    \item $N$ is the number of non-embedding model parameters.
    \item $L_0$ represents the irreducible error of an ideal model.
    \item $A$, $\alpha$, $B$, and $\beta$ are positive constants that capture the sensitivity to data and model size.
\end{itemize}


%
We estimate the parameters of the scaling law ($L_0$, $A$, $B$, $\alpha$, and $\beta$) by training a model with width 1024 (318M parameters), consuming approximately 1762 GPU hours. We use the Warmup-Stable learning rate schedule~\cite{hu2024minicpm} to extract multiple pre-trained checkpoints within one training run. Specifically, we save checkpoints at 25\%, 50\%, and 75\% of the pre-training data for both width 768 and 1024 models.

Combining those checkpoints across different model and data scales, we fit an empirical scaling law that captures performance improvements with increasing data and model size. Using SciPy's~\cite{virtanen2020scipy} \emph{minimize} with the L-BFGS-B~\cite{nocedal1980LBFGS,zhu1997LBFGSB} optimizer, we find the optimal parameters: $(L_0, A, \alpha, B, \beta) = (4.574, 0.250, 0.279, 0.326, 0.329)$. That allows us to derive the optimal scaling relationship: $D = 1.384 \cdot N^{0.962}$. 


We show the scaling law isoplot and its derived optimal scaling schedule in~\autoref{fig:scaling_law}. Our analysis reveals that despite using the large-scale OpenDV dataset \citep{yang2024opendv} with more than 1700 hours of driving data, our model is under-trained, suggesting that additional data could significantly improve performance. 
%
In total, we use 10,186 GPU hours to establish the scaling laws before the learning-rate decay phase, including 8,424 GPU hours to finalize the search of all hyperparameters. We then train the flagship model with 1.2B parameters and observe an estimation error of only 0.003 (~0.06\%) compared to the predicted scaling law.



\subsection{Fine-tuning \vm{} on target dataset}
\label{subsec:finetuning}

Building upon our pre-trained model, we implement a carefully designed fine-tuning strategy that leverages multiple datasets to enhance the model's performance. Our approach begins with a checkpoint selected from the `stable' phase of pre-training.

For the fine-tuning phase, we construct a diverse training mix combining three complementary datasets: (1) OpenDV, a large-scale, diverse dataset that provides broad coverage of general driving scenarios across the world; (2) nuPlan, a more specialized dataset that aligns with our subsequent imitation learning phase; (3) nuScenes, that serves the dual purpose of supporting imitation learning and targetting the NeuroNCAP evaluation, \ie, the target task of driving. 

From OpenDV, we initially extracted around 59M overlapping video clips and allocated 90\% of them for pre-training (warmup and stable learning rate phases) and reserved the remaining 10\% for fine-tuning (learning rate decay phase). However, rather than utilizing the entire fine-tuning portion, we strategically sample from multiple sources to create a balanced training mix:

%\vspace{1em}
\begin{itemize}[leftmargin=1cm]
    \setlength{\itemsep}{0.5em}
    \item 40\% from OpenDV -- 2,385,300 clips
    \item 58.72\% from nuPlan -- 2,765,278 clips, representing the complete nuPlan dataset
    \item 1.28\% from nuScenes -- 76,120 clips, achieved by repeating the nuScenes dataset four times
\end{itemize}

We leave for future works to evaluate the impact of only using the 10\% portion of OpenDV. An additional open question is the optimal composition of the fine-tuning data mix, possibly with different proportions of the datasets above or additional autonomous driving datasets.

\subsection{Training \vam{} with imitation learning}
\label{sec:imitationlearning}

A carefully structured imitation learning allows transforming our pre-trained video model (\vm{}) into an actionable video-action model (\vam{}). This section outlines how we enable end-to-end driving capabilities while preserving the rich visual representations learned during pre-training.

As discussed in~\autoref{sec:model:vam}, our approach employs flow matching, building upon the framework introduced in $\pi$0~\cite{black2024pi0}. More formally, given a dataset of expert demonstrations with associated high-level commands $\mathcal{D} = \{(O_t, A_t, c_t)\}$, with $O_t = [o_{t}, \dots, o_{t-N}]$ representing the sequence of images observed up to N past frames, the high-level command $c_t \in \{\text{left}, \text{right}, \text{straight}\}$ and the expert trajectory $A_t = [a_{t+1}, \dots, a_{t+H}]$ of future positions over horizon $H$, we learn to denoise trajectories through a conditional probability flow.


The key insight of flow matching lies in its elegant formulation of the forward process and induced vector field. We learn a conditional denoising vector field $v_\theta$, which defines how to progressively transform noisy trajectories back into expert-like behavior. The training process follows a forward noising schedule defined by:

\begin{equation}
    A^\tau_t = \tau A_t + (1-\tau)\epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
    \label{eq:noised_action}
\end{equation}

That process represents a linear interpolation between the expert action $A_t$ and Gaussian noise $\epsilon$. The variable $\tau \in [0,1]$ represents the noise level. This process smoothly interpolates between expert actions ($\tau = 0$) and pure noise ($\tau = 1$) and traces out paths in the action manifold. For training, the action expert uses the following objective to predict the denoising vector field $v_\theta$:

\begin{equation}
    L^\tau(\theta) = \mathbb{E}_{p(A_t|O_t, c_t),q(A^\tau_t | A_t)} ||v_\theta(A^\tau_t, O_t, c_t) - u(A^\tau_t|A_t)||^2
\end{equation}

where $q(A^\tau_t|A_t)$ is the forward process defined above and $u(A^\tau_t|A_t)$ is the optimal transport vector field. The optimal transport vector field $u(A^\tau_t|A_t)$ represents the ideal direction in which noisy actions should move to become expert actions. Our learned vector field $v_\theta$ approximates this optimal transport.
%
The vector field acts as the generator of a continuous transformation on the manifold of plausible driving actions. It generates a flow that transforms a simple distribution (Gaussian noise) into our target distribution of expert actions.
%
During inference, we generate action sequences by integrating the learned vector field:
\begin{equation}
A^{\tau+\delta}_t = A^\tau_t + \delta \cdot v_\theta(A^\tau_t, O_t, c_t)
\end{equation}
using 10 steps of the forward Euler method, starting from random noise $A^0_t \sim \mathcal{N}(0, I)$.

That framework enables our model to capture complex multimodal action distributions directly from the expert demonstration. The effectiveness of this approach is extensively demonstrated in \autoref{sec:evaluation_actionexpert}, where we show strong performance in both open-loop prediction and closed-loop driving scenarios.


\subsection{Implementation and Training details}

\paragraph{Tokenizer.}
We use a pre-trained image tokenizer, LlamaGen~\cite{sun2024llamagen}, which is based on the VQGAN architecture. Specifically, we use the stride=16 tokenizer, which has 72M parameters. It has a vocabulary size of 16,384 with codewords of 8 dimensions. We use images of size 512$
\times$288, resulting in a token map of 32$
\times$18, or 576 tokens.

\paragraph{\vm} is based off a GPT-2 transformer architecture~\cite{radford2019language}. We train it with a context length of 8 frames, resulting in a maximum context of 4,608 tokens. It has 24 layers, a vocabulary size of 16,384, with a width scaling from 768 (\vm-S) to 1024 (\vm-B), up to 2048 (\vm-L). This results in a codebook of size 
 12.6M, 16.8M, and 33.65M respectively. We keep the dimensionality of the heads fixed at 128, making the number of attention heads scale with the model size. We set a standard multiplication factor of 4 for the FFN hidden dimensionality. We optimize it with AdamW~\cite{AdamW}, a base learning rate of 0.0041, a weight decay of 1$e$-7, and $\beta=(0.9, 0.95)$ while clipping the gradient with a norm of 1.0. Finally, we initialize with a standard deviation of 0.0289. As described in~\autoref{subsec:muP}, the $\mu P$ parameterization scales the learning rate per layer according to the width layer (see our code or \cite{yang2022miup} from exact specifications). We train all our models with a batch size of 384 and vary the number of GPUs depending on the model size to maximize GPU utilization.

\paragraph{\vam} predicts the trajectory for the 6 next timesteps at 2 Hz, \ie, for 3 seconds. The dimensionality of \vam attention layers is identical to its \vm companion for the joint attention (\autoref{sec:model:vam}). However, \vam's MLP layers dimensionality is reduced by a factor of 4 with respect to \vm's dimensionality for efficient action sampling; \ie 192 for \vam-S, 256 for \vam-B, and 512 for \vm-L. For the joint attention, we must project the tokens to match the \vm's dimensionality. We use a learning rate equal to 0.0194, an initialization standard deviation of 0.0086, and similar optimizer parameters to \vm{}. For the flow matching loss, we follow $\pi0$ and use a beta distribution for the noise schedule and 10 steps for denoising at inference time. We efficiently train our model with different observation context lengths using a block attention pattern (\autoref{fig:joint_attention_mask}). That allows training the action expert to handle varying lengths of temporal context from one training clip.

\paragraph{Training infrastructure.} We detail the compute requirements for our most compute-intensive run, training \vm-L. We run our training on 48 nodes of 4 H100s, totaling 192 GPUs. To help scale that job, we employ lightning~\cite{Falcon_PyTorch_Lightning_2019} and deepspeed-stage2~\cite{deepspeed}. The total running time for the training is around 25 hours, totaling around 4800 (H100) GPU hours. We pre-train the model on approximately 60 million overlapping windows.
