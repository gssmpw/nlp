\section{Related work}
\label{sec:related}

\subsection{Video generative model}


Today's sophisticated architectures for video generation have significantly evolved since the days of Generative Adversarial Networks (GANs)~\cite{goodfellow2020gan, saito2017tgan, tulyakov2018mocogan}. Beyond basic generation capabilities, recent advances have focused on improving visual quality and enabling precise control over generated content. That dual evolution is visible in key developments in video data representation and video generation aligned to conditioning signals.


\paragraph{Continuous Representations:} These methods work with real-valued embeddings in a continuous latent space, often operating on a pre-trained Variational Autoencoder (VAE)~\cite{kingma2013vae,higgins2017betavae} to compress the image or video signal spatially and reduce computation.  
Most modern approaches in this category build on either Diffusion or Flow Matching models. Diffusion Models (DM)~\cite{ho2020ddpm,rombach2022ldm} learns to model the data distribution through a denoising process, effectively capturing the data distribution of images. Methods such as Video Diffusion Model (VDM)~\cite{ho2022vdm}, Make-A-Video~\cite{singer2023makeavideo}, or Align Your Latents~\cite{blattmann2023align} extends image diffusion model to video generation 
Flow Matching~\cite{lipman2023flow} is similar to DM in learning continuous probability flows but contrasts with the latter by directly modeling the noise-to-image vector field.
Notable works such as MovieGen~\cite{polyak2024movie} or pyramid flow matching~\cite{jin2024pyramidal} adapt Flow Matching to generate videos efficiently.

\paragraph{Discrete Representations:} These approaches typically map video data into sequences of discrete tokens using vector quantization techniques (e.g., VQ-VAE~\cite{van2017vqvae}, FSQ~\cite{menter2024fsq}, LFQ~\cite{yu2024magvit2} etc.). Inspired by language models, auto-regressive methods generate tokens sequentially, conditioning each token on the previous ones. Building upon the success of auto-regressive image generation~\cite{chen2020igpt,ramesh2021dalle,esser2020taming}, notable extensions to video generation include VideoGPT~\cite{yan2021videogpt} and HARP~\cite{Seo2022HARP} where the generation becomes spatio-temporal. 
In contrast to purely sequential prediction, Masked Image Modeling~\cite{chang2022maskgit} uses a mask-and-predict strategy that iteratively predicts/reconstructs the missing tokens, inspired by BERT-like~\cite{devlin2019BERT} training schemes. 
%.
Models such as MagVIT~\cite{yu2023magvit} and its improvement MagVIT-v2~\cite{yu2024magvit2} have effectively extended such framework to video generation, while Phenaki~\cite{villegas2023phenaki} has used it to enable long video generation from text prompts.


\paragraph{Action-Controlled Video Generation:} 

Integrating action-based control marks a significant advance in video generation, promising to move beyond passive generation to planning and decision-making.
%
That direction is exemplified by modern `neural game engines'~\cite{Bamford2020neuralgameengine}, such as Genie~\cite{bruce2024genie}, DIAMOND~\cite{alonso2024DIAMOND}, or GameNGen~\cite{valevski2024GameNGen}, which generate, from input actions, the evolution of complex, dynamic, and tridimensional game environments. 
%
In the automotive context, state-of-the-art approaches build from driving videos towards `neural world simulators' or so-called `world-models' by leveraging pre-trained diffusion networks, as VISTA \citep{gao2024vista}, GEM \citep{hassan2024gem}, and  InfinityDrive \citep{guo2024infinitydrive} do, or by exploiting discrete-based auto-regressive models, as GAIA-1 \cite{hu2023gaia1} and DrivingWorld \citep{hu2024drivingworld} do. 
%
However, a significant gap remains between generating realistic videos and learning representations suitable for robust decision-making. That limitation appears in current evaluation methods, which primarily use metrics that assess the visual quality~\citep{Unterthiner2019FVD,zhang2018lips} but provide limited insight into the performance of end tasks.







\subsection{Scalable Vision-based Action Learning}

Traditional approaches to vision-based action learning have relied heavily on expensive human annotations such as semantic segmentation masks, bounding boxes, and step-by-step action labels to represent the scene~\cite{hu2022mile,hu2023uniad}.
Those annotations are costly to acquire and often imperfect, creating fundamental scaling limitations: for instance, the autonomous driving dataset nuScenes~\cite{caesar2020nuscenes} required over 100,000 manual object annotations. Such a bottleneck has motivated research to develop more scalable frameworks.

Vision-action models are scalable when they leverage large quantities of data without requiring proportional human effort. That typically involves learning from raw demonstrations without frame-by-frame labels (e.g., only recording human actions) or utilizing self-supervised objectives that forgo manual annotation. A seminal example is Video PreTraining (VPT)~\cite{baker2022vpt}, which learns directly to act from unlabeled video demonstrations of gameplay, scaling to over 70,000 hours of data that would be prohibitively expensive to annotate manually.

An alternative is weak supervision at scale by leveraging internet data. Vision-Language Models (VLMs) like CLIP~\cite{radford2021clip} and SigLIP~\cite{whai2023siglip} learn robust visual representations from readily available image-text pairs, eliminating the need for costly pixel-wise semantic annotations. Recent works such as LLaVA~\cite{liu2023llava} or PaliGemma~\cite{beyer2024paligemma} follow that framework by building on top of pre-trained LLMs, resulting in vision encoders with strong semantic understanding. Building on that foundation, Vision-Language-Action (VLA) Models like RT-2~\cite{zitkovich2023rt2}, OpenVLA~\cite{kim2024openvla} or $\pi0$~\cite{black2024pi0} demonstrate how web-scale knowledge can be transferred to action generation in a robotic context.

In autonomous driving, the shift towards scalability is particularly evident.
Early end-to-end approaches like TransFuser~\cite{Chitta2023TransFuser} or MILE~\cite{hu2022mile} required extensive annotation to learn visual representations (e.g., HD maps, bounding boxes of agents, Bird's-eye-view or camera semantic masks).
Methods such as LMDrive~\cite{shao2024lmdrive}, GPT-DRIVER~\cite{mao2023gptdriver}, LLM-driver~\cite{chen2024llmdriver} or Language-MPC~\cite{sha2023languagempc} leverage pre-trained LLMs, but they all expect to know the position of all agents in the scene at train or inference time.
That implicitly assumes a perfect upstream perception stack, which is often unrealistic.
Newer methods, instead, leverage VLMs to operate directly on the raw visual from expert demonstrations. For instance, DriveGPT4~\cite{xu2024drivegpt4} uses a pre-trained CLIP~\cite{radford2021clip} encoder and a LLaMA-2~\cite{touvron2023llama2} LLM to interpret driving scenes without requiring dense semantic labels.
CarLLaVA~\cite{renz2024CarLLaVA} further demonstrates how foundation models (LLaVA~\cite{liu2023llava} and LLaMA~\cite{touvron2023llama}) enables closed-loop driving from raw sensor data and sparse navigational inputs.

\subsection{Evaluation}

\paragraph{Video generation:} The rapid progress in large-scale generative models enabled the creation of visually compelling and temporally consistent videos~\cite{sora,veoteam2024veo2,chefer2025videojam,gao2024vista,hu2023gaia1,hassan2024gem}.
Those advancements suggest that generative models can capture meaningful representations of the world, potentially serving as `world models' that simulate and understand physical interactions.
Current practices in evaluating generative video models as world models predominantly rely on metrics such as the Fréchet Inception Distance (FID)~\cite{heusel2017fid} and its temporal extension, the Fréchet Video Distance (FVD)~\cite{Unterthiner2019FVD}.
Those metrics, however, focus on perceptual quality rather than task-specific performance.
In addition, they measure the similarity between generated and actual video distributions, assuming they were Gaussian, resulting in very coarse estimations of generative capabilities.
They fall short, for example, in evaluating the model's understanding of physical laws and real-world dynamics necessary for effective world simulation~\cite{zhu2024soraworldsim}.
To address those limitations, new benchmarks such as Physics-IQ~\cite{motamed2025physicsIQ} or PhyWorld~\cite{kang2024phyworld} test models' comprehension of physical principles, including fluid dynamics and solid mechanics.
Those evaluations reveal that despite achieving visual realism, models often lack the understanding to accurately predict physical interactions, highlighting a significant gap in their ability to function as reliable world simulators.
%
\paragraph{Open-loop driving:} Open-loop evaluation assesses a system's performance by comparing its predicted future trajectories against pre-recorded expert driving behavior. While that method enables evaluation with realistic traffic data without simulation, it has key limitations: most critically, it fails to measure performance in the actual deployment distribution, which comprises `reasonable' trajectories that deviate from the expert's~\cite{codevilla2018offlineEval, dauner2023parting}. Such limitation is further highlighted in AD-MLP~\cite{zhai2023ADMLP}, which demonstrates that a simple MLP model taking only ego-motion as input may achieve comparable or better open-loop scores than complex perception-based methods.
Furthermore, the distance between predicted and recorded trajectories is not an ideal metric in multi-modal scenarios; for instance, when merging into a turning lane, both immediate and delayed merging could be valid options, but open-loop evaluation penalizes the option not observed in the data~\cite{zhai2023ADMLP}.
To address those limitations, some metrics propose to cover more comprehensive aspects such as traffic violations, progress, and driving comfort~\cite{dauner2023parting}, but the correlation between open-loop performance and actual driving performance remains loose~\cite{dauner2023parting}. 

%
\paragraph{Closed-loop driving:} Closed-loop driving evaluation addresses the key limitations of open-loop testing by enabling model decisions to influence subsequent observations. Existing approaches can be categorized based on their simulation capabilities and trade-offs. Bird's-eye-view-only simulators~\cite{Gulino2023waymax, caesar2021nuplan} focus on trajectory planning but cannot evaluate end-to-end perception-based systems. Meanwhile, synthetic simulators such as CARLA~\cite{dosovitskiy2017carla} enable comprehensive end-to-end evaluation with dynamic agents, but their synthetic nature introduces significant domain gaps when transferring to real-world scenarios~\cite{chen2023e2esurvey}. Vista~\cite{amini2020vista,amini2022vista2} attempts to bridge this gap through view reprojection from actual data but cannot simulate dynamic agent interactions. NavSim~\cite{Dauner2024navsim} introduces a non-reactive paradigm where agents commit to actions based on initial real-world sensor input and continue the simulation in BEV. That limits long-horizon evaluation as the system does not receive environmental feedback.
Conversely, NeuroNCAP~\cite{ljungbergh2024neuroncap} currently represents the most complete solution by enabling closed-loop evaluation with dynamic agents and continuous sensory feedback through neural rendering, thus allowing for long-horizon scenarios while maintaining photorealism from real-world data.
