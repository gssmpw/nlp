\section{Conclusion}
\label{sec:conclusion}

\vm{} and \vam{} are a significant step forward in applying large-scale unlabeled pre-training to autonomous driving, offering several exciting discoveries. 
%
First, the successful transfer of the pre-trained representations to driving tasks demonstrates the versatility of our approach. Complex driving behaviors are learned directly from raw video without requiring expensive semantic annotations. Particularly encouraging is \vam{}'s reduction of existing methods' collision rates by 27\% while maintaining comparable progress metrics.
%
Second, the performance on out-of-distribution datasets like KITTI and Cityscapes demonstrates our approach's robustness and generalization capabilities.
%
Third, our scaling experiments reveal a clear path forward. The empirical scaling laws we established suggest substantial headroom for improvement, notably through increased training data.

By releasing our complete codebase, training recipes, scaling laws, and model weights, we aim to accelerate progress in video-based autonomous driving. We envision several promising directions for future work:
\begin{itemize}
    \item Decoupling high-level command path from actual expert trajectory so that, in the imitation training set, the model observes the expert deviating from the high-level command path.
    \item Extending our approach to leverage multi-camera setups for enhanced scene understanding.
    \item Exploring more sophisticated action generation frameworks that maintain the benefits of our current approach while improving safety-critical behavior.
    \item Investigating larger-scale pre-training on even more diverse driving datasets.
    \item Using a better tokenizer than the current LLaMaGen-VQGAN, adapted to driving scenarios and better able to capture fine visual details (text on signs, road markings, traffic lights, etc.).
\end{itemize}

The key limitation of our work lies in the gap between \vm{}'s ability to model future states and \vam{}'s current reliance on imitation learning. While \vm{} can generate plausible future video streams, we have not yet leveraged that predictive power for planning and control. A critical missing piece is a reward model that distinguishes between favorable and critical latent states, enabling more sophisticated planning strategies beyond pure imitation. That presents an exciting opportunity to transform our reactive system into a proper `world-model'-based planning pipeline.

Additionally, while comprehensive for driving performance, our evaluation framework does not fully evaluate the depth of physical understanding learned by our video model. Future work should develop more nuanced evaluation metrics, similar to Physics-IQ~\cite{motamed2025physicsIQ}'s spatial and temporal mIoU approach to assess different aspects of physical understanding. Such metrics would provide deeper insights into what our models learn about scene dynamics, object interactions, and physical constraints.


\section*{Acknowledgements}
\label{sec:acknowledgment}
%\todo{IDRIS/adastra, jean-zay, EuroHPC/CINECA}
This work was partially supported by the ANR MultiTrans project (ANR-21-CE23-0032). It was initially explored using HPC resources from GENCI–CINES (Grant 2023-A0141014181), and most of its results were obtained using HPC resources from GENCI–IDRIS (Grant 2024-GC011015459). We acknowledge the EuroHPC Joint Undertaking for awarding this project access to the EuroHPC supercomputer LEONARDO, hosted by CINECA (Italy) and the LEONARDO consortium, through a EuroHPC AI and Data-Intensive Access call.

\clearpage

\section*{Detailed Contributions}
\label{sec:credits}

{\sc Project Lead} \\
\textit{\small Research direction, technical roadmap, and project coordination} \\
Florent Bartoccioni

{\sc Core Contributors} \\
\textit{\small All aspects of the codebase, experiments, and evaluations} \\
Florent Bartoccioni, Elias Ramzi

{\sc Contributors} \\
Victor Besnier --- \textit{\small Visual Tokenization codebase using pre-trained VQGAN; FID metric code}\\
Loick Chambon --- \textit{\small Data download, transfer and extraction; visualization code development}\\
Eduardo Valle --- \textit{\small OpenDV preprocessing}\\
Shashanka Venkataramanan --- \textit{\small Depth anything pseudo-GT generation}\\
Tuan-Hung Vu --- \textit{\small GPT adaptation from nanoGPT}\\
Yihong Xu --- \textit{\small nuPlan preprocessing and initial dataloader development}

{\sc Technical Report} \\
\textit{\small Manuscript preparation, design, visualizations, figures} \\
Florent Bartoccioni, Elias Ramzi, Victor Besnier, Shashanka Venkataramanan, Eloi Zablocki, Yihong Xu, Tuan-Hung Vu, Eduardo Valle

{\sc Grant Acquisitions} \\
\textit{\small Grant proposals for Adastra, EuroHPC, and Jean Zay Grand Challenges} \\
Florent Bartoccioni, Alexandre Boulch, Mickael Chen, Eduardo Valle, Spyros Gidaris, Eloi Zablocki, Matthieu Cord, Serkan Odabas, David Hurych

{\sc Advisory} \\
\textit{\small Research and organization guidance} \\
Eloi Zablocki, Alexandre Boulch, Mickael Chen

{\sc Senior Advisory} \\
\textit{\small Research and organization guidance} \\
Eduardo Valle, Andrei Bursuc, Renaud Marlet, Matthieu Cord\\
