\section{Fractal Generative Models}

The key idea behind fractal generative models is to recursively construct more advanced generative models from existing atomic generative modules. In this section, we first present the high-level motivation and intuition behind fractal generative models. We then use the autoregressive model as an illustrative atomic module to demonstrate how fractal generative models can be instantiated and used to model very high-dimensional data distributions.

\subsection{Motivation and Intuition}

Fractals are complex patterns that emerge from simple, recursive rules. In fractal geometry, these rules are often called ``generators'' \citep{mandelbrot1983fractal}. With different generators, fractal methods can construct many natural patterns, such as clouds, mountains, snowflakes, and tree branches, and have been linked to even more intricate systems, such as the structure of biological neural networks \citep{bassett2006adaptive,sporns2006small,bullmore2009complex}, nonlinear dynamics \citep{aguirre2009fractal}, and chaotic systems \citep{mandelbrot2004fractals}.

Formally, a fractal generator $g_i$ specifies how to produce a set of new data $\{x_{i+1}\}$ for the next-level generator based on one output $x_{i}$ from the previous-level generator: $\{x_{i+1}\} = g_i(x_{i})$. For example, as shown in \autoref{fig:teaser}, a generator can construct a fractal by recursively calling similar generators within each grey box.

Because each generator level can produce multiple outputs from a single input, a fractal framework can achieve an exponential increase in generated outputs while only requiring a linear number of recursive levels. This property makes it particularly suitable for modeling high-dimensional data with relatively few generator levels.

Specifically, we introduce a fractal generative model that uses atomic generative modules as \emph{parametric} fractal generators. In this way, a neural network ``learns'' the recursive rule directly from data. By combining the exponential growth of fractal outputs with neural generative modules, our fractal framework enables the modeling of high-dimensional non-sequential data.
Next, we demonstrate how we instantiate this idea with an autoregressive model as the fractal generator.

\subsection{Autoregressive Model as Fractal Generator}

In this section, we illustrate how to use autoregressive models as the fractal generator to build a fractal generative model. Our goal is to model the joint distribution of a large set of random variables $x_1, \cdots, x_{N}$, but directly modeling it with a single autoregressive model is computationally prohibitive. To address this, we adopt a divide-and-conquer strategy. The key modularization is to abstract an autoregressive model as a modular unit that models a probability distribution $p(x|c)$. With this modularization, we can construct a more powerful autoregressive model by building it on top of multiple next-level autoregressive models.

Assume that the sequence length in each autoregressive model is a manageable constant $k$, and let the total number of random variables $N=k^n$, where $n=\log_k(N)$ represents the number of recursive levels in our fractal framework. The first autoregressive level of the fractal framework then partitions the joint distribution into $k$ subsets, each containing $k^{n-1}$ variables. Formally, we decompose 
$p(x_1, \cdots, x_{k^n}) = \prod_{i=1}^k p(x_{(i-1)\cdot k^{n-1} + 1}, \cdots, x_{i\cdot k^{n-1}}|x_1, \cdots, x_{(i-1)\cdot k^{n-1}})$.
Each conditional distribution $p(\cdots|\cdots)$ with $k^{n-1}$ variables is then modeled by the autoregressive model at the second recursive level, so on and so forth. By recursively calling such a divide-and-conquer process, our fractal framework can efficiently handle the joint distribution of $k^n$ variables using $n$ levels of autoregressive models, each operating on a manageable sequence length $k$.

This recursive process represents a standard divide-and-conquer strategy. By recursively decomposing the joint distribution, our fractal autoregressive architecture not only significantly reduces computational costs compared to a single large autoregressive model but also captures the intrinsic hierarchical structure within the data.

Conceptually, as long as the data exhibits a structure that can be organized in a divide-and-conquer manner, it can be naturally modeled within our fractal framework. To provide a more concrete example, in the next section, we implement this approach to tackle the challenging task of pixel-by-pixel image generation.

\section{An Image Generation Instantiation}
We now present one concrete implementation of fractal generative models with an instantiation on the challenging task of pixel-by-pixel image generation. Although we use image generation as a testbed in this paper, the same divide-and-conquer architecture could be potentially adapted to other data domains. Next, we first discuss the challenges and importance of pixel-by-pixel image generation.

\subsection{Pixel-by-pixel Image Generation}

Pixel-by-pixel image generation remains a significant challenge in generative modeling because of the high dimensionality and complexity of raw image data. This task requires models that can efficiently handle a large number of pixels while effectively learning the rich structural patterns and interdependency between them. As a result, pixel-by-pixel image generation has become a challenging benchmark where most existing methods are still limited to likelihood estimation and fail to generate satisfactory images \citep{child2019generating, hawthorne2022general, yu2023megabyte}.

Though challenging, pixel-by-pixel generation represents a broader class of important high-dimensional generative problems. These problems aim to generate data element-by-element but differ from long-sequence modeling in that they typically involve non-sequential data. For example, many structures---such as molecular configurations, proteins, and biological neural networks---do not exhibit a sequential architecture yet embody very high-dimensional and structural data distributions. By selecting pixel-by-pixel image generation as an instantiation for our fractal framework, we aim not only to tackle a pivotal challenge in computer vision but also to demonstrate the potential of our fractal approach in addressing the broader problem of modeling high-dimensional non-sequential data with intrinsic structures.

%##################################################################################################
\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/instantiation}	
\caption{\textbf{Instantiation of our fractal method on pixel-by-pixel image generation.} In each fractal level, an autoregressive model receives the output from the previous generator, concatenates it with the corresponding image patches, and employs multiple transformer blocks to produce a set of outputs for the next generators.
}
\vspace{-10pt}
\label{fig:instantiation}
\end{figure}
%##################################################################################################

\begin{table}[t]
\begin{center}{
\caption{
\textbf{Model configurations, parameters, and computational costs} at different autoregressive levels for our large-size model. The computational costs are measured by the GFLOPs per forward pass in training with batch size 1. Notably, thanks to the fractal design, the total computational cost of modeling a 256$\times$256 image is only twice that of modeling a 64$\times$64 image.}
\label{tab:config}
\vspace{-.1em}
\resizebox{0.75\linewidth}{!}{
\tablestyle{4pt}{1.2}
\begin{tabular}{c | c | l | l}
 & & \multicolumn{2}{c}{image resolution} \\
 &  & 64$\times$64$\times$3 & 256$\times$256$\times$3 \\
\shline
\multirow{4}{*}{seq. len.}& $g_1$ & 256 & 256 \\
& $g_2$ & 16 & 16 \\
& $g_3$ & 3 & 16 \\
& $g_4$ & - & 3 \\
\midline
\multirow{4}{*}{\#layers}& $g_1$ & 32 & 32 \\
& $g_2$ & 8 & 8 \\
& $g_3$ & 3 & 4 \\
& $g_4$ & - & 1 \\
\midline
\multirow{4}{*}{hidden dim}& $g_1$ & 1024 & 1024 \\
& $g_2$ & 512 & 512 \\
& $g_3$ & 128 & 256 \\
& $g_4$ & - & 64 \\
\midline
\multirow{4}{*}{\#params (M)}& $g_1$ & 403 & 403 \\
& $g_2$ & 25 & 25 \\
& $g_3$ & 0.6 & 3 \\
& $g_4$ & - & 0.1 \\
\midline
\multirow{4}{*}{\#GFLOPs}& $g_1$ & 215 & 215 \\
& $g_2$ & 208 & 208 \\
& $g_3$ & 15 & 419 \\
& $g_4$ & - & 22 \\
\end{tabular}
}
}
\end{center}
\vspace{-15pt}
\end{table}
%##################################################################################################

\subsection{Architecture}

As shown in \autoref{fig:instantiation}, each autoregressive model takes the output from the previous-level generator as its input and produces multiple outputs for the next-level generator. It also takes an image (which can be a patch of the original image), splits it into patches, and embeds them to form the input sequence for a transformer model. These patches are also fed to the corresponding next-level generators. The transformer then takes the output of the previous generator as a separate token, placed before the image tokens. Based on this combined sequence, the transformer produces multiple outputs for the next-level generator.

Following common practices from vision transformers and image generative models \citep{dosovitskiy2020image,Peebles2023}, we set the sequence length of the first generator $g_0$ to 256, dividing the original images into $16 \times 16$ patches. The second-level generator then models each patch and further subdivides them into smaller patches, continuing this process recursively.
To manage computational costs, we progressively reduce the width and the number of transformer blocks for smaller patches, as modeling smaller patches is generally easier than larger ones. At the final level, we use a very lightweight transformer to model the RGB channels of each pixel autoregressively, and apply a 256-way cross-entropy loss on the prediction. The exact configurations and computational costs for each transformer across different recursive levels and resolutions are detailed in \autoref{tab:config}. Notably, with our fractal design, the computational cost of modeling a 256$\times$256 image is only twice that of modeling a 64$\times$64 image.

Following \cite{li2024autoregressive}, our method supports different autoregressive designs. In this work, we mainly consider two variants: a raster-order, GPT-like causal transformer (AR) and a random-order, BERT-like bidirectional transformer (MAR) (\autoref{fig:armar}). Both designs follow the autoregressive principle of next-token prediction, each with its own advantages and disadvantages, which we discuss in detail in \autoref{sec:appendix-results}. We name the fractal framework using the AR variant as FractalAR and the MAR variant as FractalMAR.

\subsection{Relation to Scale-space Autoregressive Models}
Recently, several models have been introduced that perform next-scale prediction for autoregressive image generation \citep{tian2024visual,tang2024hart,han2024infinity}. One major difference between these scale-space autoregressive models and our proposed method is that they use a single autoregressive model to predict tokens scale-by-scale. In contrast, our fractal framework adopts a divide-and-conquer strategy to recursively model raw pixels with generative submodules. Another key difference lies in the computational complexities: scale-space autoregressive models are required to perform \emph{full attention} over the entire sequence of next-scale tokens when generating them, which results in substantially higher computational complexity.

For example, when generating images at a resolution of 256$\times$256, in the last scale, the attention matrix in each attention block of a scale-space autoregressive model has a size of $(256 \times 256)^2 = 4{,}294{,}967{,}296$. In contrast, our method performs attention on very small patches when modeling the interdependence of pixels (4$\times$4), where each patchâ€™s attention matrix is only $(4 \times 4)^2=256$, resulting in a total attention matrix size of $(64 \times 64) \times (4 \times 4)^2 = 1{,}048{,}576$ operations. This reduction makes our approach $4000\times$ more computationally efficient at the finest resolution and thus for the first time enables modeling high-resolution images pixel-by-pixel.

\subsection{Relation to Long-Sequence Modeling}

Most previous work on pixel-by-pixel generation formulates the problem as long-sequence modeling and leverages methods from language modeling to address it \citep{child2019generating,roy2021efficient,ren2021combiner,hawthorne2022general,yu2023megabyte}. However, the intrinsic structures of many data types, including but not limited to images, are beyond one-dimensional sequences. Different from these methods, we treat such data as sets (instead of sequences) composed of multiple elements and employ a divide-and-conquer strategy to recursively model smaller subsets with fewer elements. This approach is motivated by the observation that much of this data exhibits a near-fractal structure: images are composed of sub-images, molecules are composed of sub-molecules, and biological neural networks are composed of sub-networks. Accordingly, generative models designed to handle such data should be composed of submodules that are themselves generative models.

\subsection{Implementation}
We briefly describe the training and generation process of our fractal image generation framework. Further details and hyper-parameters can be found in \autoref{sec:implementation}.

\paragraph{Training.} We train the fractal model end-to-end on raw image pixels by going through the fractal architecture in a breadth-first manner. During training, each autoregressive model takes in an input from the previous autoregressive model and produces a set of outputs for the next-level autoregressive model to take as inputs. This process continues down to the final level, where the image is represented as a sequence of pixels. A final autoregressive model uses the output on each pixel to predict the RGB channels in an autoregressive manner. We compute a cross-entropy loss on the predicted logits (treating the RGB values as discrete integers from 0 to 255) and backpropagate this loss through all levels of autoregressive models, thus training the entire fractal framework end-to-end.

\paragraph{Generation.} Our fractal model generates the image in a pixel-by-pixel manner, following a depth-first order to go through the fractal architecture, as illustrated in \autoref{fig:illustration}. Here we use the random-order generation scheme from MAR \citep{li2024autoregressive} as an example. The first-level autoregressive model captures the interdependence between 16$\times$16 image patches, and at each step, it generates outputs for the next level based on known patches. The second-level model then leverages these outputs to model the interdependence between 4$\times$4 patches within each 16$\times$16 patch. Similarly, the third-level autoregressive model models the interdependence between individual pixels within each 4$\times$4 patch. Finally, the last-level autoregressive model samples the actual RGB values from the autoregressively predicted RGB logits.
