\section{Implementation Details}
\label{sec:implementation}
Here we provide more implementation details of the training and generation process of our fractal generative model.

\paragraph{Training}. When modeling high-resolution images with multiple levels of autoregressive models, we find it slightly helpful to include a ``guiding pixel'' in the autoregressive sequence. Specifically, the model first uses the output from the previous generator to predict the average pixel value of the current input image. This average pixel value then serves as an additional condition for the transformer. In this way, each generator begins with a global context before predicting fine-grained details. We apply this ``guiding pixel'' only in experiments on ImageNet 256$\times$256.

Since our autoregressive model divides images into patches, it can lead to inconsistent patch boundaries during generation. To address this issue, we provide the next-level generator not only the output at the current patch but also the outputs at the surrounding four patches. Although incorporating these additional inputs slightly increases the sequence length, it significantly reduces the patch boundary artifacts.

By default, the models are trained using the AdamW optimizer \citep{Loshchilov2019} for 800 epochs (the FractalMAR-H model is trained for 600 epochs).
The weight decay and momenta for AdamW are 0.05 and (0.9, 0.95).
We use a batch size of 2048 for ImageNet 64$\times$64 and 1024 for ImageNet 256$\times$256, and a base learning rate (lr) of 5e-5 (scaled by batch size divided by 256).
The model is trained with 40 epochs linear lr warmup \citep{Goyal2017}, followed by a cosine lr schedule.

\paragraph{Generation}. Following common practices in the literature \citep{Chang2022, Chang2023, Li2023}, we implement classifier-free guidance (CFG) and temperature scaling for class-conditional generation. To facilitate CFG~\citep{ho2022classifier}, during training, the class condition is replaced with a dummy class token in 10\% of the samples.
At inference time, the model is run with both the given class token and the dummy token, yielding two sets of logits $l_c$ and $l_u$ for each pixel channel. The predicted logits are then adjusted according to the following equation: $l = l_u + \omega\cdot(l_c - l_u)$, where $\omega$ is the guidance scale. We employ a linear CFG schedule during inference for the first-level autoregressive model, as described in \cite{Chang2023}. We perform a sweep over different combinations of guidance scale and temperature to identify the optimal settings for each conditional generation model.

We also observed that CFG can suffer from numerical instability when the predicted probability for a pixel value is very small. To mitigate this issue, we apply top-p sampling with a threshold of 0.0001 on the conditional logits before applying CFG.


\section{Additional Results}
\label{sec:appendix-results}

%##################################################################################################
\begin{figure}[t]
\centering
\includegraphics[width=0.98\linewidth]{figures/armar}
\caption{Two variants for autoregressive modeling. The AR variant models the sequence in a raster-scan order using a causal transformer, while the MAR variant models the sequence in a random order using a bidirectional transformer. Both are valid generators to build our fractal framework.
}
\label{fig:armar}
\end{figure}
%##################################################################################################


%##################################################################################################
\begin{table}[h]
\begin{center}{
\caption{
Comparison on conditional ImageNet 64$\times$64 with classical generative models.}
\label{tab:in64}
\tablestyle{2pt}{1.2}
\begin{tabular}{l | c | c c c c}
& type & FID$\downarrow$ \\
\shline
iDDPM \citep{Nichol2021} & diffusion & 2.92 \\
StyleGAN-XL \citep{sauer2022stylegan} & GAN & \textbf{1.51} \\
Consistency Model \citep{song2023consistency} & consistency & 4.70 \\
MAR \citep{li2024autoregressive} & AR+diffusion & 2.93 \\
\midline
\textbf{FractalAR} & fractal & 5.30 \\
\textbf{FractalMAR} & fractal & 2.72 \\
\end{tabular}
}
\end{center}
\vspace{-1em}
\end{table}
%##################################################################################################

\paragraph{Class-conditional ImageNet 64$\times$64.} We evaluate class-conditional image generation on ImageNet 64$\times$64, reporting FID according to standard practice. Consistent with PixelCNN \citep{Oord2016}, we find that class conditioning has negligible impact on NLL but substantially improves visual quality and FID. The results demonstrate that our fractal generative model can achieve competitive performance as classical generative models.

We also compare the performance of the AR and MAR variants, whose structures are illustrated in \autoref{fig:armar}. The AR variant leverages key-value caching to accelerate generation, whereas the MAR variant employs bidirectional attention, which aligns more naturally with image modeling and enables the parallel prediction of multiple patches, thereby improving computational efficiency. As shown in the table, both of our models achieve favorable performance, with FractalMAR outperforming FractalAR overall, as also demonstrated in \cite{li2024autoregressive}. Therefore, we choose to use the MAR variant for conditional image synthesis on ImageNet at a resolution of 256$\times$256.

%##################################################################################################
\begin{table}[h]
\begin{center}{
\vspace{-5pt}
\caption{
Different orders of channel modeling within each pixel can slightly influence generation performance. All models are trained for 400 epochs on ImageNet 64$\times$64.}
\label{tab:pixel}
\tablestyle{2pt}{1.2}
\begin{tabular}{l | c c c c c}
order & NLL$\downarrow$ & FID$\downarrow$ \\
\shline
Y$\rightarrow$Cb$\rightarrow$Cr  &  N/A & 3.55 \\
B$\rightarrow$R$\rightarrow$G  & 3.17 & 3.32 \\
G$\rightarrow$R$\rightarrow$B  & 3.17 & 3.14 \\
R$\rightarrow$G$\rightarrow$B  & 3.17 & 3.15 \\
\end{tabular}
}
\end{center}
\end{table}
%##################################################################################################

\paragraph{Modeling Pixels.} We also examine how different pixel modeling orders affect performance. We experimented with three autoregressive orders: RGB, GRB, and BGR, as well as converting the RGB channels to the YCbCr color space. The results are summarized in \autoref{tab:pixel}. We found that while all orders achieved similar negative log-likelihood values, the FID varied slightly among the autoregressive orders (note that the NLL in the YCbCr space is not comparable with that in the RGB space). This variation is likely because, akin to human vision, the Inception model used to compute FID places greater emphasis on the red and green channels than on the blue channels \citep{mustafi2009structure}. Nonetheless, the choice of autoregressive order does not result in significant performance differences, demonstrating the robustness of our method.

\paragraph{Additional qualitative results.} We further provide additional conditional generation results on ImageNet 256$\times$256 in \autoref{fig:more-qualitative}, and additional conditional pixel-by-pixel prediction results in \autoref{fig:more-editing}.




\begin{figure*}[t]
\centering
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/195_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/282_2.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/285_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/269_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/96_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/1_1.png}\vvs
    \\
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/30_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/305_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/945_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/992_2.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/933_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/727_2.png}\vvs
    \\
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/888_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/972_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/849_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/910_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/850_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/398_1.png}\vvs
    \\
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/816_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/316_2.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/829_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/780_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/475_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/appendix-examples/971_2.png}\vvs
    \captionof{figure}{Additional pixel-by-pixel generation results from FractalMAR-H on ImageNet 256$\times$256.}
    \label{fig:more-qualitative}
    % \vspace{-5pt}
\end{figure*}

\begin{figure*}[h]
\centering
    \makebox[0.16\linewidth]{Ground truth}\hhs
    \makebox[0.16\linewidth]{Masked input}\hhs
    \makebox[0.16\linewidth]{Reconstructed}\hspace{0.03\linewidth}
    \makebox[0.16\linewidth]{Ground truth}\hhs
    \makebox[0.16\linewidth]{Masked input}\hhs
    \makebox[0.16\linewidth]{Reconstructed}\vvs
    \\
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/inpainting/190_ground_truth.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/inpainting/190_masked.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/inpainting/190_reconstructed.png}\hspace{0.03\linewidth}
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/inpainting/130_ground_truth.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/inpainting/130_masked.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/inpainting/130_reconstructed.png}\vvs
    \\
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_right_half/445_ground_truth.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_right_half/445_masked.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_right_half/445_reconstructed.png}\hspace{0.03\linewidth}
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_right_half/078_ground_truth.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_right_half/078_masked.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_right_half/078_reconstructed.png}\vvs
    \\
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_square/125_ground_truth.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_square/125_masked.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_square/125_reconstructed.png}\hspace{0.03\linewidth}
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_square/187_ground_truth.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_square/187_masked.png}\hhs
    \includegraphics[width=0.16\linewidth]{figures/image-editing-examples/outpainting_square/187_reconstructed.png}\vvs
    \captionof{figure}{Additional conditional pixel-by-pixel prediction results, including image inpainting, outpainting, and uncropping.}
    \label{fig:more-editing}
    % \vspace{-5pt}
\end{figure*}
