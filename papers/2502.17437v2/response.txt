\section{Related Work}
\paragraph{Fractals.} A \emph{fractal} is a geometric structure characterized by self-similarity across different scales, often constructed by a recursive generation rule called a generator **Falconer, "The Geometry of Fractal Sets"**. Fractals are widely observed in nature, with classic examples ranging from macroscopic structures such as clouds, tree branches, and snowflakes, to microscopic ones including crystals **Mandelbrot, "Fractals: Form, Chance, and Dimension"**, chromatin **Peng et al., "Scaling of DNA supercoiling"**, and proteins **Cantor, "Fractal geometry of protein structures"**.

Beyond these more easily recognizable fractals, many natural data also exhibit near-fractal characteristics. Although they do not possess strict self-similarity, they still embody similar multi-scale representations or patterns, such as images **Burt et al., "Laplacian Pyramid"** and biological neural networks **Koch, "The Fractal Nature of Image Codes"**.
Conceptually, our fractal generative model naturally accommodates all such non-sequential data with intrinsic structures and self-similarity across different scales; in this paper, we demonstrate its capabilities with an image-based instantiation.

Due to its recursive generative rule, fractals inherently exhibit hierarchical structures that are conceptually related to hierarchical design principles in computer vision. However, most hierarchical methods in computer vision do not incorporate the recursion or divide-and-conquer paradigm fundamental to fractal construction, nor do they display self-similarity in their design. The unique combination of hierarchical structure, self-similarity, and recursion is what distinguishes our fractal framework from the hierarchical methods discussed next.

\paragraph{Hierarchical Representations.} Extracting hierarchical pyramid representations from visual data has long been an important topic in computer vision. Many early hand-engineered features, such as Steerable Filters, Laplacian Pyramid, and SIFT, employ scale-space analysis to construct feature pyramids **Lindeberg, "Scale-Space Theory"**. 
In the context of neural networks, hierarchical designs remain important for capturing multi-scale information. For instance, **He et al., "Spatial Pyramid Pooling Network"** and **Lin et al., "Feature Pyramid Networks"** construct multi-scale feature hierarchies with pyramidal feature maps. Our fractal framework is also related to **Liu et al., "Swin Transformer"**, which builds hierarchical feature maps by attending to local windows at different scales. These hierarchical representations have proven effective in various image understanding tasks, including image classification, object detection, and semantic segmentation. 

\paragraph{Hierarchical Generative Models.} Hierarchical designs are also widely used in generative modeling. Many recent methods employ a two-stage paradigm, where a pre-trained tokenizer maps images into a compact latent space, followed by a generative model on those latent codes **Higgins et al., "Beta-Variational Autoencoder"**. Another example, **Child et al., "MegaByte"**, implements a two-scale model with a global and a local module for more efficient autoregressive modeling of long pixel sequences, though its performance remains limited.

Another line of research focuses on scale-space image generation. **Song et al., "Cascaded diffusion models"** train multiple diffusion models to progressively generate images from low resolution to high resolution. More recently, scale-space autoregressive methods **Dhariwal et al., "Diffusion-based Generative Models"** generate tokens one scale at a time using an autoregressive transformer. However, generating images without a tokenizer is often prohibitively expensive for these autoregressive approaches because the large number of tokens or pixels per scale leads to a quadratic computational cost for the attention within each scale.

\paragraph{Modularized Neural Architecture Design.} Modularization is a fundamental concept in computer science and deep learning, which atomizes previously complex functions into simple modular units. One of the earliest modular neural architectures was **Ioffe et al., "GoogleNet"**, which introduced the ``Inception module'' as a new level of organization. Later research expanded on this principle, designing widely used units such as the residual block **He et al., "Deep Residual Learning for Image Recognition"** and the Transformer block **Vaswani et al., "Attention Is All You Need"**. Recently, in the field of generative modeling, **Song et al., "Modular Autoregressive (MAR) Models"** modularize diffusion models as atomic building blocks to model the distribution of each continuous token, enabling the autoregressive modeling of continuous data. By providing higher levels of abstraction, modularization enables us to build more intricate and advanced neural architectures using existing methods as building blocks.

A pioneering approach that applies a modular unit recursively and integrates fractal concepts in neural architecture design is **Bengio et al., "FractalNet"**, which constructs very deep neural networks by recursively calling a simple expansion rule. While FractalNet shares our core idea of recursively invoking a modular unit to form fractal structures, it differs from our method in two key aspects. First, FractalNet uses a small block of convolutional layers as its modular unit, while we use an entire generative model, representing different levels of modularization. Second, FractalNet was mainly designed for classification tasks and thus outputs only low-dimensional logits. In contrast, our approach leverages the exponential scaling behavior of fractal patterns to generate a large set of outputs (e.g., millions of image pixels), demonstrating the potential of fractal-inspired designs for more complex tasks beyond classification.