% \newpage 
\section{Experiments}

We conduct extensive experiments on the ImageNet dataset \citep{deng2009imagenet} with resolutions at 64$\times$64 and 256$\times$256. Our evaluation includes both unconditional and class-conditional image generation, covering various aspects of the model such as likelihood estimation, fidelity, diversity, and generation quality. Accordingly, we report the negative log-likelihood (NLL), Frechet Inception Distance (FID) \citep{Heusel2017}, Inception Score (IS) \citep{Salimans2016}, Precision and Recall \citep{Dhariwal2021}, and visualization results for a comprehensive assessment of our fractal framework.

%##################################################################################################
\begin{table}[t]
\begin{center}{
\caption{
\textbf{More fractal levels achieve better likelihood estimation performance with lower computational costs}, measured on unconditional ImageNet 64$\times$64 test set. NLL is reported in bits/dim. The network configurations of $g_1$, $g_2$ and $g_3$ are in \autoref{tab:config}.}
\label{tab:ablation}
\tablestyle{4.5pt}{1.2}
\begin{tabular}{l | c c c | c c c c c}
& \multicolumn{3}{c|}{Seq Len} \\
& $g_1$ & $g_2$ & $g_3$ &\#GFLOPs & NLL$\downarrow$\\
\shline
AR, full-length & 12288 & - & - & 29845 & N/A \\
MAR, full-length & 12288 & - & - & 29845 & N/A \\
\midline
AR, 2-level & \hspace{2.5pt} 4096 & 3 & - & \hspace{2.5pt} 5516 & 3.34 \\
MAR, 2-level & \hspace{2.5pt} 4096 & 3 & - & \hspace{2.5pt} 5516 & 3.36 \\
\midline
FractalAR (3-level) & \hspace{7pt} 256 & \hspace{-6pt} 16 & 3 & \hspace{7pt} 438 & 3.14 \\
FractalMAR (3-level) & \hspace{7pt} 256 & \hspace{-6pt} 16 & 3 & \hspace{7pt} 438 & 3.15 \\
\end{tabular}
}
\end{center}
% \vspace{-1em}

\begin{center}{
\caption{
Comparison with other likelihood-based models on unconditional ImageNet 64$\times$64 test set. NLL is reported in bits/dim. Both FractalAR and FractalMAR achieve competitive performance. $\dagger$ We regard flow matching as a generalized form of diffusion model.}
\label{tab:in64}
\tablestyle{2pt}{1.2}
\begin{tabular}{l | c c c c c}
& type & NLL$\downarrow$\\
\shline
iDDPM \citep{Nichol2021} & diffusion & 3.53 \\
VDM \citep{kingma2021variational} & diffusion & 3.40 \\
FM \citep{lipman2022flow} & \hspace{1.5pt} diffusion$^\dagger$ & 3.31 \\
NFDM \citep{bartosh2024neural} & diffusion & 3.20 \\
\midline
PixelRNN \citep{Oord2016a} & AR & 3.63 \\
PixelCNN \citep{Oord2016} & AR & 3.57 \\
Sparse Transformer \citep{child2019generating} & AR & 3.44 \\
Routing Transformer \citep{roy2021efficient} & AR & 3.43 \\
Combiner AR \citep{ren2021combiner} & AR & 3.42 \\
Perceiver AR \citep{hawthorne2022general} & AR & 3.40 \\
MegaByte \citep{yu2023megabyte} & AR & 3.40 \\
\midline
\textbf{FractalAR} & fractal & \textbf{3.14} \\
\textbf{FractalMAR} & fractal & 3.15 \\
\end{tabular}
}
\end{center}
\vspace{-10pt}
\end{table}
%##################################################################################################


\begin{figure*}[t]
\centering
\includegraphics[width=0.33\linewidth]{figures/examples/944_1.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/examples/980_1.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/examples/270_1.png}\vvs
    \\
    \includegraphics[width=0.165\linewidth]{figures/examples/367_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/263_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/355_2.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/931_2.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/616_1.png}\vvs
    \includegraphics[width=0.165\linewidth]{figures/examples/514_1.png}\hhs
    \\
    \includegraphics[width=0.165\linewidth]{figures/examples/999_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/971_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/906_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/429_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/915_1.png}\hhs
    \includegraphics[width=0.165\linewidth]{figures/examples/789_1.png}\vvs
    \captionof{figure}{\textbf{Pixel-by-pixel generation results from FractalMAR-H on ImageNet 256$\times$256.} Our fractal method can generate high-quality high-resolution images in a pixel-by-pixel manner, with an average throughput of 1.29 seconds per image. More qualitative results are in \autoref{fig:more-qualitative}.}
    \label{fig:qualitative}
    % \vspace{-5pt}
\end{figure*}

\subsection{Likelihood Estimation}

We begin by evaluating our method on unconditional ImageNet 64$\times$64 generation to assess its likelihood estimation capability. To examine the effectiveness of our fractal framework, we compare the likelihood estimation performance of our framework with different numbers of fractal levels, as shown in \autoref{tab:ablation}. Modeling the entire  64$\times$64$\times$3=12,288-pixel sequence with a single autoregressive model incurs prohibitive computational costs and makes training infeasible. Moreover, a two-level fractal framework that first models the entire pixel sequence and then the RGB channels requires over ten times the computation of our three-level fractal model. Employing more fractal levels is not only more computationally efficient but also improves likelihood estimation performance, likely because it better captures the intrinsic hierarchical structure of images. These results demonstrate both the efficiency and effectiveness of our fractal framework.
 
We further compare our method with other likelihood-based models in \autoref{tab:in64}. Our fractal generative model, instantiated with both causal and masked autoregressive fractal generators, achieves strong likelihood performance. In particular, it achieves a negative log-likelihood of 3.14 bits per dim, which outperforms the previous best autoregressive model (3.40 bits per dim) by a significant margin and remains competitive with advanced diffusion-based methods. These findings demonstrate the effectiveness of our fractal framework on the challenging task of pixel-by-pixel image generation, highlighting its promising potential in modeling high-dimensional non-sequential data distributions.


%##################################################################################################
\begin{table}[t]
\begin{center}{
\caption{
Sample quality evaluation on ImageNet 256$\times$256 generation. FractalMAR is the only \textbf{pixel-by-pixel} framework in this table. We also provide the performance of other pixel-level generation methods for reference.}
\vspace{-5pt}
\label{tab:in256}
\tablestyle{2.5pt}{1.2}
\begin{tabular}{l | c | c | c c | c c}
& type & \#params  & {FID$\downarrow$} & {IS$\uparrow$} & {Pre.$\uparrow$} & {Rec.$\uparrow$} \\
\shline
BigGAN-deep & GAN & 160M & 6.95 & 198.2 & \textbf{0.87} & 0.28 \\
GigaGAN & GAN & 569M & 3.45 & 225.5 & 0.84 & \textbf{0.61} \\
StyleGAN-XL & GAN & 166M & 2.30 & 265.1 & 0.78 & 0.53 \\
\midline
ADM & diffusion &  554M  &  4.59 &  186.7 & 0.82 & 0.52 \\
Simple diffusion & diffusion & 2B & 3.54 & 205.3 & - & - \\
VDM$++$ & diffusion &  2B &  2.12 &  267.7 & - & - \\
SiD2 & diffusion & - & \textbf{1.38} & - & - & - \\
\midline
JetFormer & AR+flow & 2.8B &  6.64 & - & 0.69 & 0.56 \\
\midline
\textbf{FractalMAR-B} & fractal & 186M & \hspace{-4pt}11.80 & 274.3 & 0.78 & 0.29 \\
\textbf{FractalMAR-L} & fractal & 438M & 7.30 & 334.9 & 0.79 & 0.44 \\
\textbf{FractalMAR-H} & fractal & 848M & 6.15 & \textbf{348.9} & 0.81 & 0.46 \\
\end{tabular}
}
\end{center}
\vspace{-15pt}
\end{table}
%##################################################################################################

\subsection{Generation Quality} 
We also evaluate FractalMAR on the challenging task of class-conditional image generation at a resolution of 256$\times$256, using four fractal levels.
We report standard metrics including FID, Inception Score, Precision, and Recall following standard practices to evaluate its generation quality in \autoref{tab:in256}. Specifically, FractalMAR-H achieves an FID of 6.15 and an Inception Score of 348.9, with an average throughput of 1.29 seconds per image (evaluated at a batch size of 1,024 on a single Nvidia H100 PCIe GPU). Notably, our method achieves  strong Inception Score and Precision, indicating its ability to generate images with high fidelity and fine-grained details, as also demonstrated in \autoref{fig:qualitative}. However, its FID and Recall are relatively weaker, suggesting that the generated samples are less diverse compared to other methods. We hypothesize that this is due to the significant challenge of modeling nearly 200,000 pixels in a pixel-by-pixel way. Nonetheless, these results highlight our method's effectiveness in not only accurate likelihood estimation but also the generation of high-quality images.

We further observe a promising scaling trend: increasing the model size from 186M to 848M parameters substantially improves the FID from 11.80 to 6.15 and the Recall from 0.29 to 0.46. We expect that additional scaling of parameters could narrow the gap in FID and Recall even further. Unlike models that rely on tokenizers, our method is free from the reconstruction errors introduced by tokenization, suggesting the potential for uncapped performance gains with larger model capacities.

\begin{figure}[t]
    \makebox[0.33\linewidth]{Ground truth}\hhs
    \makebox[0.33\linewidth]{Masked input}\hhs
    \makebox[0.33\linewidth]{Reconstructed}\vvs
    \\
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/inpainting/287_ground_truth.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/inpainting/287_masked.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/inpainting/287_reconstructed.png}\vvs
    \\
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/outpainting_right_half/104_ground_truth.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/outpainting_right_half/104_masked.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/outpainting_right_half/104_reconstructed.png}\vvs
    \\
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/outpainting_square/193_ground_truth.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/outpainting_square/193_masked.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/outpainting_square/193_reconstructed.png}\vvs
    \\
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/change_class/0_ground_truth.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/change_class/0_bounding_box.png}\hhs
    \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/change_class/0_reconstructed.png}\vvs
    \\
    % \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/pixel_level_edit/1_ground_truth.png}\hhs
    % \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/pixel_level_edit/1_masked.png}\hhs
    % \includegraphics[width=0.33\linewidth]{figures/image-editing-examples/pixel_level_edit/1_reconstructed.png}\vvs
    % \\
    \vspace{-10pt}
    \captionof{figure}{Conditional pixel-by-pixel prediction results, including image inpainting (first row), outpainting (second row), uncropping (outpainting on a large mask, third row), and class-conditional editing (inpainting with another class label, fourth row).}
    \vspace{-10pt}
    \label{fig:editing}
\end{figure}

\subsection{Conditional Pixel-by-pixel Prediction}

We further examine the conditional pixel-by-pixel prediction performance of our method using conventional tasks in image editing. \autoref{fig:editing} provides several examples, including inpainting, outpainting, uncropping, and class-conditional editing. As shown in the figure, our method can accurately predict the masked pixels based on the unmasked regions. Additionally, it can effectively capture high-level semantics from class labels and reflect it in the predicted pixels. This is illustrated in the class-conditional editing example, where the model replaces a cat's face with that of a dog by conditioning on the dog's class label. These results demonstrate the effectiveness of our method in predicting unknown data given known conditions.

More broadly, by generating data element-by-element, our method provides a generation process that is more interpretable for humans compared to approaches such as diffusion models or generative models operating in latent spaces. This interpretable generation process not only allows us to better understand how data is generated but also offers a way to control and interact with the generation. Such capabilities are particularly important in applications like visual content creation, architectural design, and drug discovery. Our promising results highlight the potential of our approach for controllable and interactive generation, paving the way for future explorations in this direction.

