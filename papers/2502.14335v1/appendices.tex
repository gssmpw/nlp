\appendix

\section{Typology Details}
\label{sec_appendix_taxonomy}

The 8 categories from which the sentences were taken initially for deriving \taxtypes{} are ``Fashion'', ``Automotive'', ``Books'', ``CDs and Vinyls'', ``Digital Music'', ``Electronics'', ``Movies and TV'', ``Toys and Games''. These were chosen due to the expected differences in aspects associated to the categories, to allow variability and ensure an exhaustive view at types found within reviews.

The \textit{inappropriate} \taxtype{} was not found in the initial data we used, likely because e-commerce websites make an effort to remove such reviews. However this \taxtype{} is a know characteristic in reviews data, and sometimes seeps in anyways.

We used NLTK \citep{bird2006nltk} sentence tokenization to split reviews to sentences, and applied simple normalization techniques to clean the texts. This procedure was used also for the rest of the experiments and analyses in the paper.


\section{Predictor Model Details}
\label{sec_appendix_predictor}

\subsection{Development and Test Evaluation}
\label{sec_appendix_predictor_data_evaluation}

%\paragraph{Development data.}
%The percentage of sentences with each \taxtype{} is as follows... [TODO] In the dev set...

\paragraph{Development evaluation.}
All our initial assessments on the models were done on the development set. We tried several temperatures and engineered some prompts for the models. We only evaluated models that all its responses started with a \say{yes} or \say{no}. 

We extracted \textbf{optimal thresholds} for all \taxtypes{} as follows: For thresholds $[0.1, 1.0]$ incrementing by $0.1$, and for each \taxtype{}, we computed the $F_1$ score against the annotations. The threshold with the maximum $F_1$ score was chosen as the optimal threshold for the corresponding \taxtype.

The macro-$F_1$ score for a model is then the average of $F_1$ scores over all \taxtypes{} with their optimal thresholds. The main results on the development set are presented in \autoref{tab_dev_results}.

%When all responses of a model started with a ``yes'' or ``no'', we calculated the probabilities of each \taxtype{} for each sentence. Then the sentences were marked with a \taxtype{} if its probability was above a $0.5$ threshold. Then, for each sentence individually, we computed the $F_1$ score between the predicted and gold labels. The final score is the average of $F_1$ scores over all sentences. The average of recalls tells us how many of the correct \taxtypes{} were predicted per sentence, and the average of precision tells us how many of the predicted \taxtypes{} are correct. Some of the results on the development set are presented in \autoref{tab_dev_results}.

% \begin{table*}[t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{lcc|c|ccccc|c}
%         \toprule
%         \multirow{2}{*}{\makecell{Model}} &
%         \multirow{2}{*}{\makecell{Temperature}} &
%         \multirow{2}{*}{\makecell{\# Repetitions\\Per Prompt}} &
%         \multirow{2}{*}{\makecell{Macro \\ $F_1$}} &
%         \multicolumn{3}{c}{Sentence-Level} & \multirow{2}{*}{\makecell{\# Pred. \taxtypes{}\\Per Sentence}} &
%         \multirow{2}{*}{\makecell{\# Gold \taxtypes{}\\Per Sentence}} & \multirow{2}{*}{\makecell{Run Time\\Per Sentence}} \\
%         & & & & $F_1$ & Recall & Precision & & & \\
%         \midrule
%         \texttt{flan-t5-xxl} & $0.7$ & 10 & $41.7$ & $17.8$ & $75.3$ & $10.3$ & $4.8$ & $2.2$ & $\sim 10.8$ sec. \\
%         \texttt{flan-t5-xxl} & $0.3$ & 10 & $56.9$ & $27.7$ & $72.6$ & $17.8$ & $4.0$ & $2.2$ & $\sim 10.8$ sec. \\
%         \texttt{flan-t5-xxl} & $0.3$ & 30 & $57.2$ & $23.0$ & $72.6$ & $14.1$ & $3.9$ & $2.2$ & $\sim 10.8$ sec. \\
%         \texttt{flan-ul2}    & $0.3$ & 10 & $51.5$ & $40.1$ & $68.4$ & $31.7$ & $4.7$ & $2.2$ & $\sim 21.0$ sec. \\
%         \texttt{flan-ul2}    & $0.3$ & 30 & $52.2$ & $41.1$ & $66.7$ & $33.5$ & $4.4$ & $2.2$ & $\sim 21.0$ sec. \\
%         \bottomrule
%     \end{tabular}}
%     \caption{The results on the \textit{development} set of the main models in the initial assessment. The threshold for all \taxtypes{} was set to $0.5$ which can make the results noisy. A manual inspection of the results showed good results on both \texttt{flan-t5-xxl} and \texttt{flan-ul2}, and the runtime of \texttt{flan-ul2} is not necesarilly worth the compute resources for our experiments and analyses.}
%     \label{tab_dev_results}
% \end{table*}

%Notice that setting the threshold to $0.5$ strongly affects the results, and hence tend to be noisy or incomparable as-is. A manual inspection of the predictions gave reason to rely confidently enough on the \texttt{flan-t5-xxl} model, and the extra run-time and compute resources incurred by the \texttt{flan-ul2} model was not worth the slight improvement, especially for our uses. Specifically, we even noticed that many of the \taxtypes{} predicted by the models, that were not labeled in the development set, were indeed valid in certain contexts. In any case, our aim is not to produce the best model, but to rely on one enough for our analyses.

\begin{table}[b]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\makecell{Model}} &
        \multirow{2}{*}{\makecell{Temperature}} &
        \multirow{2}{*}{\makecell{\# Repetitions\\Per Prompt}} &
        \multirow{2}{*}{\makecell{Macro \\ $F_1$}} &
        \multirow{2}{*}{\makecell{Run Time\\Per Sentence}} \\
        & & & & \\
        \midrule
        \texttt{flan-t5-xxl} & $0.7$ & 10 & $41.7$ & $\sim 10.8$ sec. \\
        \texttt{flan-t5-xxl} & $0.3$ & 10 & $56.9$ & $\sim 10.8$ sec. \\
        \texttt{flan-t5-xxl} & $0.3$ & 30 & $57.2$ & $\sim 10.8$ sec. \\
        \texttt{flan-ul2}    & $0.3$ & 10 & $51.5$ & $\sim 21.0$ sec. \\
        \texttt{flan-ul2}    & $0.3$ & 30 & $52.2$ & $\sim 21.0$ sec. \\
        \bottomrule
    \end{tabular}}
    \caption{The results on the \textit{development} set of the main models in the initial assessment. The Macro $F_1$ score is computed based on optimal thresholds for each of the \taxtypes{}.}
    \label{tab_dev_results}
\end{table}

Using 30 repetitions per prompt only slightly improves results, which is not worth the increased compute resources.

A supplementary manual inspection of the predictions gave reason to rely confidently enough on the \texttt{flan-t5-xxl} model. In fact, we noticed that many of the \taxtypes{} predicted by the models, that were not labeled in the development set, were indeed valid in certain contexts. In any case, our aim is not to produce the best model, but to rely on one enough for our analyses.

We note that all other models we tried gave substantially worse results, or irrelevant responses altogether. On the other hand, the models presented in \autoref{tab_dev_results} were very consistent both in output format and final results.


%\paragraph{Test data.}
%The percentage of sentences with each \taxtype{} is as follows... [TODO] In the dev set and in the test set.

\paragraph{Test evaluation.}
We set the thresholds for all \taxtypes{} with those computed on the development set. The $F_1$ score was then computed for each \taxtype{} on the test set, and the average of those produces the final macro-$F_1$ score, as presented in \autoref{tab_test_scores_per_type}. The recall and precision indicate a finer-grained view of the capabilities of the predictor. On manual inspection, many of the incorrectly labeled sentences could be correct in certain contexts. I.e., when the predictor labels incorrectly, it is not nonsensically wrong.


\subsection{Model}
\label{sec_appendix_predictor_model}

\paragraph{Model implementations.}
All FLAN models were run using the Huggingface transformers library \citep{wolf2020huggingface}. The Jurassic model was run using the AI21 Python SDK \citep{ai212023pythonsdk} with a very small monetary budget. In addition to \texttt{flan-t5-\{xl, xxl\}}, \texttt{flan-ul2} and \texttt{j2-jumbo-instruct}, we also tested \texttt{flan-t5-\{base, large\}}, which produced considerably poorer outputs.

\paragraph{Temperature.}
On the models assessed, we mainly tested temperatures of $0.3$ and $0.7$. The FLAN models worked better with $0.3$ and the Jurassic model worked better with $0.7$ (as evaluated on the development set). However, as said, only the \texttt{flan-t5-xxl} and \texttt{flan-ul2} produced reasonable enough results in any case.

\paragraph{Hardware.}
We ran all FLAN predictions on an AWS \texttt{g4dn.12xlarge} EC2 server, which includes 4 NVIDIA T4 GPUs, 64 GB GPU memory and 192 GB RAM. The Jurassic model runs through an API.

\paragraph{Run time.}
The \texttt{flan-t5-xxl} model's run time was about $0.45$ seconds per prompt (including the 10 repetitions for computing probability). Hence, each sentence required an average of $10.8$ seconds for full typology prediction.

\paragraph{Prompt.}
The chosen prompt used in the \texttt{flan-t5-xxl} model is:

\begin{table}[h]
    \vspace{-2mm}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|l|}
        \hline
        \textsl{
        \begin{tabular}[c]{@{}l@{}}`Given that this sentence is from a product review about\\\texttt{\{product category subprompt\}}, \texttt{\{}\taxtype{} \texttt{question\}}?\\Answer yes or no. The sentence is: \say{\texttt{\{sentence\}}}'\end{tabular}} \\
        \hline
    \end{tabular}}
    \vspace{-4mm}
\end{table}

The \texttt{product category subprompt} is a string prepared for each category. For example the subprompt for the `Electronics' category would be ``\textit{an electronics product}'', and for `Toys and Games' it would be ``\textit{a toy or game}''.

The \texttt{\taxtype{} question} prompt substring is a question prepared for each \taxtype{}, as found in \autoref{tab_taxonomy_types}.

An example of a prompt for a sentence from the ``Books'' category for the \textit{product usage} \taxtype{} would be:

\begin{table}[h]
    \vspace{-2mm}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|l|}
        \hline
        \textsl{
        \begin{tabular}[c]{@{}l@{}}`Given that this sentence is from a product review about\\a book, does the sentence describe how the product can\\be used? Answer yes or no. The sentence is: ``I love this\\book and just  completed an incredible weekend workshop\\with the author.'''\end{tabular}} \\
        \hline
    \end{tabular}}
    %\vspace{-4mm}
\end{table}

The \taxtype{} predictions on which we rely throughout this paper can be reproduced using these prompts with the \texttt{flan-t5-xxl} Huggingface model. Complementing code will be available as well.


%\section{Experiment Details}
\section{Downstream Task Experiment Details}
\label{sec_appendix_experiments}

This appendix describes the data, training, testing and evaluation details in all the experiments throughout the paper.
%, to facilitate reproducibility.


\subsection{Prediction of Review Helpfulness}
\label{sec_appendix_experiments_review_helpfulness}

\input{Figures/table_classification_results_full}

\paragraph{Data used.}
\citet{gamzu2021helpfulsentences} released a dataset of helpful sentences, linked to their full review data. The data is based on 123 products from categories ``Toys'', ``Books'', ``Movies'', ``Music'', ``Camera'' and ``Electronics''. We targeted an approximately balanced amount of helpful to unhelpful reviews (regardless of the sentence-level helpfulness which \citeauthor{gamzu2021helpfulsentences} target). Each review has `helpful-count' and `nothelpful-count' fields. By setting helpful-count $>= 9$ and nothelpful-count $== 0$ for helpful reviews, and nothelpful-count $>= 3$ and helpful-count $== 0$ for unhelpful reviews, we produce 458 and 486 reviews respectively. The aim was to collect about 500 reviews for each class.

\paragraph{Training and testing.}
We use 50 iterations of cross validation, with a train set of 70\% and test if 30\%. The procedure is repeated for the different \taxtype{} sets separately.

\paragraph{Evaluation.}
As common in binary classification tasks, we report the accuracy measure \citep{Yadav2020sentAna}.

\paragraph{More results.}
\autoref{tab_classification_results_full} extends \autoref{tab_classification_results}, including all \taxtype{} sets defined in \autoref{tab_taxonomy_groups}.


\subsection{Prediction of Review Sentence Helpfulness}
\label{sec_appendix_experiments_sentence_helpfulness}

\input{Figures/table_helpful_sentences_results_full}

\paragraph{Data used.}
The full data of \citet{gamzu2021helpfulsentences} consists of 20000 sentences in the train set and 2000 in the test set. Each sentence has a continuous helpfulness score between 0 and 2. In addition, we find the helpfulness scores marking the borders of the top and bottom tertiles in the train set ($1.4$ and $1.0$), and mark the sentences as helpful or unhelpful respectively, or neutral for the mid-section. We use the same border scores to mark the sentences in the test set. Overall there are (7475, 7072, 5453) (unhelpful, helpful, neutral) sentences in the train, and (742, 562, 696) in the test. Notice that classes are not perfectly balanced in the train set since the scores on the borders (1.4 and 1.0) repeat in many sentences.

\paragraph{Training and testing.}
The Linear Regression is conducted on the full original data. The SVM classification is done on the sentences marked with the helpful and unhelpful classes only (neutral ignored).


\paragraph{Evaluation.}
We use the metrics reported in \citep{gamzu2021helpfulsentences} for the regression task, and accuracy for the binary classification task.

\paragraph{More results.}
\autoref{tab_classification_results_full} and \autoref{tab_helpful_sentences_results_full} extend \autoref{tab_classification_results} and \autoref{tab_helpful_sentences_results} respectively, including all \taxtype{} sets defined in \autoref{tab_taxonomy_groups}.



\subsection{Prediction of Sentiment Polarity}
\label{sec_appendix_experiments_sentiment}

\paragraph{Data used.}
As in \S{\ref{sec_appendix_experiments_review_helpfulness}}, we use the subset of products from \citet{gamzu2021helpfulsentences} for convenience. In order to prevent any bias from the helpfulness signal, we randomly sampled 5000 reviews (out of 58205) from the data without any feature pertaining to up-votes and down-votes. The review rating distribution is: $\{\texttt{5}: 3337, \texttt{4}: 675, \texttt{3}: 321, \texttt{2}: 219, \texttt{1}: 448\}$, and the polarity hence distributes to: $\{\texttt{positive}: 4012, \texttt{negative}: 988\}$. There are an average of $4.05$ sentences per review.

\paragraph{Training and testing.}
As in \S\ref{sec_appendix_experiments_review_helpfulness}, we use 50 iterations of cross validation, with a train set of 70\% and test of 30\%, averaging results over the 50 iterations. The procedure is repeated for the different \taxtype{} sets separately.

\paragraph{Evaluation.}
As in \S\ref{sec_appendix_experiments_review_helpfulness}, we report the accuracy measure.

\paragraph{More results.}
\autoref{tab_classification_results_full} extends \autoref{tab_classification_results}, including all \taxtype{} sets defined in \autoref{tab_taxonomy_groups}.



\subsection{Analysis of Reviews and Summaries}
\label{sec_appendix_experiments_analysis}

\paragraph{Data used.}
We iterated over the 31K products in the AmaSum dataset \citep{brazinskas2021amasum}, and filtered out the products without any product category assigned to it. Since products in this dataset are given several hierarchical category options, we heuristically assigned a category by manually clustering related category labels to some general ones. Then for 5 categories (``Books'', ``Electronics'', ``Apparel'', ``Toys and Games'', ``Pet Supplies'') with over 30 products, and chosen manually by their differing aspect-level characteristics, we randomly sampled 20 products. For these products we collected all their reviews and one reference summary (there are rare cases in AmaSum with more than one reference summary for a product). Overall there are 100 products with an average of $77.3$ reviews per product ($7729$ reviews total), $4.2$ sentences per review, and $7.1$ sentences per summary.

\paragraph{Analysis.}
Like in other experiments, a review/summary level vector is the average of its sentence vectors. Review/summary level vectors are then averaged to get the final two vectors to compare (in Figure \ref{fig_review_vs_summaries_vectors}).

For the rhetorical structure analysis, the 6-sentence reviews behave similarly to reviews with other lengths (Figure \ref{fig_review_structure}). Less than 6 sentences does not emphasize the behavior visually as clearly. There are $763$ (out of $7729$) reviews with 6 sentences. For the analysis on summaries, the 7-sentence summaries had the highest number of instances ($24$ out of $100$), and it is visually easier to see the patterns in the data due to the summaries containing 3 subsections (\emph{verdict}, \emph{pros}, \emph{cons}), although other length summaries behave similarly. Here, the sentence vectors at each review/summary position are averaged in order to plot the graphs. In the figures, only the \taxtypes{} with observable changes that have probabilities above 0.2 throughout the review/summary are displayed.



% \input{Figures/table_summary_helpfulness_correlation}




\section{Predictor Evaluation with Specific \taxtypes{}}
\label{sec_appendix_predictor_eval_type_specific}

In addition to the standard evaluation of our \taxtypes{} prediction model in Section \ref{sec_prediction_quality}, we additionally assessed our model's performance on benchmarks of specific \taxtypes{} already identified in previous work, namely \textit{tip}, \textit{opinion}, and \textit{opinion with reason}. The favorable results here only place further emphasis on the reliability of our model, which gives confidence to perform our analyses in Sections \ref{sec_experiments} and \ref{sec_analysis}. \textbf{This assessment is only supplemental to our evaluation in Section \ref{sec_prediction_quality}.}

We obtain existing annotated datasets and establish training sets for the sole purpose of setting a prediction threshold for one of our \taxtypes{}, which is then used as the predicted label for the annotated task.
We experiment with tuning over the entire training set as well as with much smaller subsets of 100 sentences.


\subsection{Tip Classification Evaluation}
\label{sec_appendix_experiments_tips}

Product tips are generally defined as short, concise, practical, self-contained pieces of advice on a product~\citep{hirsch2021producttips}, for example: \say{Read the card that came with it and see all the red flags!}. While previous work~\citep{hirsch2021producttips, farber2022tips} characterized tips into finer-grained sub-types, our definition assumes a generic definition.

\paragraph{Data used.}
We use the data annotated by~\citet{hirsch2021producttips} over Amazon reviews for non-tips or tips. The data used by \citet{hirsch2021producttips} for their experiments (3059 tips and 48,870 non-tip sentences) is slightly different from the data we used (3848 tips and 81,323 non-tips), since we do not apply the initial rule-based filtering that they enforce.

We noticed that some sentences were annotated in the dataset as tips, even though we do not view them as such, e.g.,~\say{The ring that holds the card together is kind of flimsy}. Since this was especially true under the \textit{warning} sub-type, we removed these sentences from the data, to better represent our notion of a \textit{tip}.

Our main goal is to show that our predictor performs decently, and not that we provide a better solution than \citet{hirsch2021producttips}. Hence, exacting the data distribution is not a major concern. Of the available data, we use all 3,848 tip-sentences and sample twice as many non-tip sentences (out of the 81,323).

The dataset includes reviews from 5 categories: ``Musical Instruments'', ``Baby'', ``Toys and Games'', ``Tools and Home Improvement'', and ``Sports and Outdoors''.

\paragraph{Training and testing.}
We follow the cross-validation \citep{xu2001crossVal} procedure of \citet{hirsch2021producttips} to train (find the best tip-\taxtype{} threshold) and to respectively evaluate tip identification using the found threshold. For 50 iterations, all tips are used (or \textit{warning} sub-types are removed) and the same amount of non-tips are sampled (out of the many available). Then the data is split to 80\%/20\% train/test splits. In case a train set size is forced, e.g., only 100 samples, then it is sampled from the training data. The train set is used to find the optimal threshold using Youden's J statistic on the ROC curve \citep{youden1950youdenj}. Then the test set is used to compute the different evaluation metrics. The results are averaged over the 50 iterations and reported, and the bootstrapping method \citep{efron1992bootstrap} is used to compute confidence intervals at alpha=0.025 (95\% confidence percentile). Since the results in \citet{hirsch2021producttips} do not report confidence intervals, we cannot fully compare to their results, however the differences are large enough to assume significant differences, as viewed in our findings.

\paragraph{Evaluation.}
\citet{hirsch2021producttips} report Recall@Precision scores, i.e., setting a specific precision value and producing the corresponding recall value. This approach is used since there is a tradeoff between presenting tips for more products and being confident about the tips presented. We also present the $F_1$ of the tip/no-tip prediction, as common for classification tasks.








\subsection{Opinions Classification Evaluation}
\label{sec_appendix_experiments_opinions}
Product reviews are rich in opinions and are often more convincing when a reason is provided for the opinion. 
A persuasive saying, whether opinionated or objective, is a type of \textit{argument}~\citep{ecklekohler2015arguments}, a class found to be helpful for predicting review helpfulness~\citep{liu2017argsForHelpfullness,passon2018helpfulness,chen2022argumentMiningForHelpfulness}. We thus turn to an argument mining dataset as an assessment benchmark.

\paragraph{Data used.}
The $AM^2$~\citep{chen2022argumentMiningForHelpfulness} dataset is annotated at the clause level for various sub-types of subjective and objective arguments and reasons.
The data contains 878 reviews (pre-split to train and test sets) from 693 products in the ``Headphones'' category, and only sentences that are argumentative are kept in each review. Each review is broken down to its clauses (a sentence or smaller). A clause is annotated as ``Policy'', ``Value'', ``Fact'' or ``Testimony'', where the first two are subjective and the latter two are objective. In addition, a clause can be marked as ``Reason'' or ``Evidence'', where the former provides support for a subjective clause, and the latter for an objective clause. The support clauses are linked to relevant clauses that they support.

We automatically parse and re-label this data and create full-sentence-level instances for \textit{opinion} and \textit{opinion with reason} \taxtypes{}. If a sentence contains a subjective clause, it is marked as an \textit{opinion}, and if it also contains a support clause then it is also marked as an \textit{opinion with reason}. Otherwise a sentence is neither.

We end up with 3132 (2133 train, 999 test) \textit{opinion} sentences, of which 363 (249 train, 114 test) are also \textit{opinion with reason}, and 1359 (972 train, 387 test) non-opinion sentences.

\paragraph{Training and testing.}
We separately classify the \textit{opinion} \taxtype{} and the \textit{opinion with reason} \taxtypes{} against the non-opinionated class. As in the case of \textit{tip}s, we use the train set to find the best threshold for the relevant \taxtype{}, and then evaluate on the test set. We also experimented with sampling just 100 train instances for tuning the threshold.

\paragraph{Evaluation.}
As common in classification tasks, we report the $F_1$ measure.


\input{Figures/table_predictor_dataset_sizes}


\subsection{Results}
\label{sec_appendix_experiments_specific_type_results}
\autoref{tab_predictor_eval} presents the classification results on the three specific \taxtype{} evaluations.
Our zero-shot classifier, which only uses the training sets to find an optimal threshold for the corresponding \taxtype{}, is able to identify \textit{tip}s and \textit{opinion}s effectively, and \textit{opinion with reason} fairly well, on their respective benchmarks.\footnote{\citet{chen2022argumentMiningForHelpfulness} do not provide any intrinsic baseline results on opinion classification to which we can compare. In addition, we manipulated the original data from clause-level to sentence-level.} Moreover, limiting the training sets to only 100 samples appears to hardly have any effect on the quality of the model, showing its robustness to paucity of labeled data.

For the \textit{tip} task, we see in \autoref{tab_tips_results_vs_baseline_full} that the predictor performs on par or much better than existing supervised baselines from \citet{hirsch2021producttips}. Although our definition of \emph{tip} differs slightly from that used in annotating the benchmark, our predictor is still reliable and useful.
When the \textit{warning} subtype of a \textit{tip} is removed from the data (which, as mentioned before, generally do not fit our notion of a tip), the results dramatically improve.

\input{Figures/table_predictor_eval}

\input{Figures/table_tips_results_vs_baseline_full}





\input{Figures/table_taxonomy_types}

\input{Figures/table_example_annotations}

\input{Figures/table_test_scores_per_type}