%We turn to exemplify how, through the typology of sentences we defined, certain attributes of review text which were mostly treated until now as disparate binary markers, are in fact special cases of the larger question of multi-label sentence type classification. \rl{not clear how the previous text relates the tasks that about to follow} 
We turn to exemplify how the combination of the varying \taxtypes{} in the typology serves as a strong \say{feature set} for addressing many downstream tasks that apply to review data.
Here, we address the tasks of review helpfulness, sentence helpfulness, and sentiment analysis.
% These tasks are tackled using the typology only, without any other features. %---not even the text itself.
We build models that use only the predicted typology labels as input (without further access to the text itself), with the goal
% Our goal in these tasks is 
not to outperform state of the art results, but rather to exhibit the strong signals concealed within the varying \taxtypes{} and their combinations.

\input{Figures/table_classification_results}


%We leverage several datasets that were designed for those tasks, and demonstrate the effectiveness of our solution for those subtasks, including predicting helpfulness of reviews and review sentences, and sentiment analysis. 

%- our goal is not to beat the SOTA, but to show that the typology holds a great amount of information about the review.

%- with no explicit textual features, the tasks are solved quite effectively. this opens the door to enrich future models with such information to potentially further push the SOTA.


\subsection{Review Helpfulness}
\label{sec_experiments_review_helpfulness}

Predicting helpfulness of reviews is a popular task~\citep{ocampodiaz2018helpfulnessFeatures} due to its essential role in highlighting useful information to customers on e-commerce sites.
In order to assess how different subsets of the \taxtypes{} assist with helpfulness prediction, we extract Amazon reviews~\citep{he2016reviewsDS} and identify helpful and non-helpful reviews based on \say{up-votes} and \say{down-votes} on the reviews ($\sim$1000 balanced reviews, details in Appendix \ref{sec_appendix_experiments_review_helpfulness}).
Each sentence from a review
%is marked with the \taxtypes{} via our predictor, and the average of sentence vectors represents the review \rl{might not be clear what the vectors are. maybe say that each (sentence, type) prediction yields a one hot vector and than take average}.
is input to our classifier (\S\ref{sec_prediction_model}), producing a 24-dimensional vector of \taxtype{} probabilities.
The average of these vectors then represents the review-level \taxtype{} probabilities.\footnote{Notice that each \taxtype{} has an independent probability, and probabilities in a vector do not add up to 100\%, since this is a multi-label classification setup.}
We reuse this averaging procedure throughout the rest of our experiments and analyses.

We then train a binary SVM classifier~\citep{Cortes1995svm} over the vectors to classify reviews as helpful or non-helpful, and evaluate its performance through cross-validation. We report classification accuracy, as is common in binary classification setups~\citep{Yadav2020sentAna}.
As seen in \autoref{tab_classification_results} (leftmost results column), using the full \taxtypes{} vector as an input produces a respectable accuracy of 72.6\%, while applying only certain subsets of \taxtypes{} results in significantly ($p<0.001$) lower scores, emphasizing the importance of the variability in the typology.

On further analysis, the results indicate that the \textit{opinion with reason} \taxtype{} is considerably more vital for helpfulness of a review than the \textit{opinion} \taxtype{}, possibly because providing reasoning for an opinion is more convincing.
This is noticeable through the immense gap between the second and third rows of \autoref{tab_classification_results}, and is further illustrated in Figure~\ref{fig_classification_vectors_hr}, which shows probability scores for each \taxtype{} aggregated at the review level and stratified based on review helpfulness.
Generally, the subjective \taxtypes{} (which include opinions---refer to \autoref{tab_taxonomy_groups}) most strongly signal helpfulness, however the combination of all \taxtypes{} performs best.
Results on all \taxtype{} subsets are available in \autoref{tab_classification_results_full} in the appendix.

The bottom section of \autoref{tab_classification_results} presents the results when using the coarse-grained types (from \autoref{tab_taxonomy_groups}) as features for an SVM model, where the predicted fine-grained \taxtypes{} are mapped to their respective coarse-grained types. The fine-grained typology yields better results than the coarse-grained one, reiterating its benefit. The two random binary baselines are for lower-bound reference. The first baseline randomly chooses an answer at 50\%, while the other chooses an answer assuming it knows the distribution of the gold labels.

%\rl{are there baselines score that we can report? I think we should say that most of the task information is captured by the types. the exact content within the types is of secondary importance. This is true for all 3 and especially impressive in my opinion for sentiment analysis.}

%Predicting helpfulness of reviews is a highly researched area \citep{ocampodiaz2018helpfulnessFeatures}, as it plays an important role in highlighting useful information to customers on e-commerce sites. As noted in \S{\ref{sec_prediction_opinions}}, \citet{chen2022argumentMiningForHelpfulness} and others \citep{liu2017argsForHelpfullness, passon2018helpfulness} claim that the helpfulness of a review is strongly associated to argumentational features, which, in our setting, can mainly be represented using the \textit{opinion} and \textit{opinion with reason} sentence types. We would additionally like to assess how different subsets of the sentence types provided by our typology further assists with helpfulness prediction. As such, we extract reviews from the Amazon Review Dataset \citep{he2016reviewsDS}, from the subset used by \citep{gamzu2021helpfulsentences}, and group reviews that have many ``up-votes'' and no ``down-votes'', and the opposite, to get helpful and non-helpful review sets respectively (Appendix \ref{sec_appendix_experiments_review_helpfulness}). Each of a review's sentences is marked with the typology types via our predictor, and the average of sentence vectors represents the review. We then use a cross-validation style training and testing method with a simple SVM classifier \citep{Cortes1995svm} that classifies a review as helpful or not-helpful using the review type-vectors.
%
%\input{Figures/table_classification_results}
%
%When only using the \textit{opinion} and \textit{opinion with reason} types in the vectors, the classifier produces a score of $66.5$ $F_1$. However, when using the full vector of types, the classifier improves significantly ($p<0.001$) to $72.3$ $F_1$, emphasizing the importance of the variability in the typology. Furthermore, the classifier is not fed with any explicit textual features, but rather only with the content types.
%
%In addition, as discussed by \citet{chen2022argumentMiningForHelpfulness}, the results indicate that the \textit{opinion with reason} is considerably more vital for helpfulness of a review than just any opinion. In other words, providing reasons for opinions in reviews is likely more helpful. This is noticeable by the immense gap between the first and second rows of Table \ref{tab_classification_results}.






%The types themselves, without any features explicitly pertaining to the text itself, are very effective for the task. Future work can assess whether a combination of features and a better type-predictor can further improve results.




\input{Figures/table_helpful_sentences_results}

\subsection{Review Sentence Helpfulness}
\label{sec_experiments_sentence_helpfulness}

The next experiment assesses helpfulness of individual sentences in reviews (as opposed to full reviews), a task introduced by \citet{gamzu2021helpfulsentences}
% introduced the task of identifying helpful sentences in product reviews
as a form of extreme product review summarization.
Their review sentence dataset is annotated for helpfulness along a range from 0 (not helpful) to 2 (very helpful).
We produce the \taxtype{} vectors for the sentences and train a linear regression model to predict helpful sentence scores.
In addition, we generate a binary classification task by separating the data into subsets of helpful and unhelpful sentences, taken from the top and bottom tertiles (see Appendix \ref{sec_appendix_experiments_sentence_helpfulness}), on which we train an SVM model.

We compare our results for the regression task with those of \citet{gamzu2021helpfulsentences} in \autoref{tab_helpful_sentences_results}.
As in the review helpfulness task in \S{\ref{sec_experiments_review_helpfulness}}, it is apparent that the \taxtype{}-vectors provide a strong signal for predicting helpfulness also on the sentence level.
The results of the binary classification task, shown in \autoref{tab_classification_results} (center results column), reinforce this conclusion. % with a high $F_1$ score of 88.1.
When comparing the aggregate \taxtype{} probability vectors of helpful sentences to those of unhelpful sentences, Figure~\ref{fig_classification_vectors_hs} exhibits large contrast on the \textit{opinion with reason}, \textit{personal info} and \textit{product description} \taxtypes{}.
The \textit{subjective} \taxtype{} subset provides the strongest signal, however the combination of all fine-grained \taxtypes{} substantially improves over all subsets, again emphasizing the importance of the variability in our typology.



\subsection{Review Sentiment Polarity}
\label{sec_experiments_sentiment}

Over the years, the customer review domain has been central in the task of sentiment analysis \citep{chen2017sentAnaWithTypeClassification, Tesfagergish2022zeroshot_classification}. Understanding the sentiment in reviews assists in surfacing positive and negative criticism on products or services. This in turn aids new customers in making more informed purchasing decisions, and concurrently facilitates improvement on the vendors' side.

While, at first glance, the information \taxtypes{} in our typology should not associate with sentiment in any obvious manner, we perform a similar classification assessment as in the previous experiments.
In this case, we randomly extract 5,000 Amazon reviews~\citep{he2016reviewsDS} and group them into positive and negative reviews (review ratings of $\{4,5\}$, or $\{1,2,3\}$ respectively, as commonly handled~\citep{Shivaprasad2017SentAnaRev}).
Again training and evaluating an SVM through cross-validaition, we classify the reviews into their sentiment polarities (details in Appendix \ref{sec_appendix_experiments_sentiment}).

Surprisingly, the SVM classifier is able to separate positive from negative reviews with an accuracy of 88.1\%, as seen in \autoref{tab_classification_results} (rightmost column).
Specifically, the combination of the subjective and stylistic \taxtype{}-sets provides the signal, while all other \taxtype{}-sets fail at identifying a distinction (the SVM classifier simply marks all sentences as positive). On closer inspection, Figure~\ref{fig_classification_vectors_sentiment} shows that the \textit{improvement desire} and stylistic \taxtypes{} indeed differ markedly between the positive and negative reviews, which offers a plausible explanation for the results.

While many models addressing the sentiment analysis task lack explainability for the sentimental decision~\citep{Adak2022sentimentExplainability}, the information \taxtypes{} give a hint at what induces the sentimental polarity, and can further direct attention at specific features. In this case, by focusing on sentences that express an improvement desire, review readers can more efficiently understand what to expect from the product, and can refrain from reading full negative reviews that are rich in rhetorical information.

The three experiments above suggest that the type of content in reviews offers strong indicators for helpfulness and sentiment, and that the residual textual content past the types may be only of secondary importance. Moreover, utilizing fine-grained types provides stronger signals than coarse-grained ones, since, apparently, \textit{specific} types of content distinctly influence humans' perception of helpfulness and sentiment.







%We turn to exemplify how, through the typology of sentences we defined, certain aspects of review text which were treated until now as disparate binary markers, are in fact special cases of the larger question of multi-label sentence type classification. We construct minimal shallow classifiers on top of our sentence labels for a selection of these specific task datasets and show how these notions turn out to already be baked in to our typology. We consider three such binary tasks: review helpfulness (), tip extraction (), and opinion detection ().


%\subsection{Helpfulness}
%
%(explaining what the task is)
%
%(the data)
%
%(the ridiculously simplistic classifier we implemented)
%
%(the triumphant results)
%
%(a smug declaration of victory, but really also a meaningful conclusion about the relatedness of the task and the typology elements that fit it)






\input{Figures/figure_classification_vectors}