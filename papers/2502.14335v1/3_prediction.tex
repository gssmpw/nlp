In order to facilitate analysis and experiment with reviews, and to be able to do so at a large scale, we implement a sentence-level multi-label classifier for our typology.
Traditionally, this would require collecting an annotated dataset that reasonably represents the sentence \taxtype{} distribution.
%an amount of data that reasonably represents all of the sentence \taxtypes{}.
However, considering the large amount of classes, some of which are quite rare in the data, this would be a challenging undertaking. 
Fortunately, these days we have powerful tools at our disposal which can avoid this process. 
Recent advances in pretrained large language models (LLMs) have shown strong zero-shot capabilities~\citep{wei2022flan}, which we leverage for our multi-label classification task.



\subsection{Classification Model}
\label{sec_prediction_model}

\paragraph{Development data.}
We start by collecting a small seed set of annotated sentences on which we can perform trial runs on candidate LLMs, testing the adequacy of different prompts.
% Specifically, we would like to perform prompt engineering () and basic testing to generally estimate a model's performance.
To that end, we sampled a set of sentences from Amazon reviews \citep{he2016reviewsDS}, separate from the ones used to define the typology (\S\ref{sec_taxonomy}). 
Following an initial attempt to crowdsource this process, which did not fare well,\footnote{Crowdsourcing produced sparse and noisy annotations. 
The task requires concentration and a certain level of expertise, that cannot be expected from standard crowdworkers.}
we resorted to an internal annotation procedure.
Two authors annotated 123 sentences using the full \taxtype{} set defined, with an initial inter-annotator reliability (Cohenâ€™s $\kappa$) of 0.71.
Cases with no agreement (9 sentences) or partial \taxtype{} overlap (37) were resolved through discussion. Due to the strong agreement, one of the authors then proceeded to annotate 300 new sentences for the development set.


%\subsection{Classification Model}
%\label{sec_prediction_model}
\paragraph{Model.}
Using the development set, we evaluated the prediction capabilities of several instruct-based LLMs: FLAN-T5~\citep[\texttt{flan-t5-\{xl, xxl\}},][]{chung2022flan}, UL2~\citep[\texttt{flan-ul2},][]{tay2023ul2}, and Jurassic~\citep[\texttt{j2-jumbo-instruct},][]{ai212023jurassic}, using two or three temperature settings for each.
Preliminary experiments clearly showed that a single-label classification setup for each sentence \taxtype{} produces much better results than a multi-label setup attempting to annotate all 24 \taxtypes{} at once.
% We first attempted a multi-label classification prompt, where the model was given the review sentence, the category of the respective product, and the 24 typology \taxtypes{}, with instructions to return the labels relevant to the sentence. Only a few attempts made it clear that this was a complicated task even for the most advanced models listed.
In the single-label setup, a classification request is sent per \taxtype{}, requesting a \say{yes} or \say{no} answer.
Each model required slightly different prompt formats in order to output relevant replies. 
%While each model required slightly different prompt formats, most models still responded frequently with either irrelevant or verbose replies.
Some models (\texttt{flan-t5-xl} and \texttt{j2-jumbo-instruct}) somewhat improved when they were asked to explain their answer~\citep{wei2022chainofthought, liu2023prompt}.
We found that \texttt{flan-t5-xxl} and \texttt{flan-ul2} with temperature 0.3 provided the best responses, both in terms of output-format consistency and correctness on the development data, affirming FLAN's span-based-response training procedure optimized for classification tasks~\citep{wei2022flan}.
Of the two flavors, we opted for the faster \texttt{flan-t5-xxl} for the rest of our procedure.
% Their overall quality was about the same, however \texttt{flan-t5-xxl} is about twice faster. The FLAN family of models is trained in part on instructions for classification tasks, and to expressly respond with specific text spans if told to do so~\citep{wei2022flan}, indeed making this choice of model a good match for our purposes.

We control for the variance in model responses by querying each prompt 10 times, and setting a final score for the respective \taxtype{} as the proportion of \say{yes} responses.\footnote{This protocol was followed as a workaround for not having access to LLM output probabilities. Querying 30 times produced similar performance on the development set. Results using this protocol were highly replicable when re-run.}
See Appendix \ref{sec_appendix_predictor} for model details and comparisons, and for the prompt templates used.

%Our selected prompt template is the following: 
% \begin{table}[h]
%     \vspace{-2mm}
%     %\centering
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{|l|}
%         \hline
%         \textsl{
%         \begin{tabular}[c]{@{}l@{}}`Given that this sentence is from a product review about\\\texttt{\{product category subprompt\}}, \texttt{\{}\taxtype{} \texttt{question\}}?\\Answer yes or no. The sentence is: \say{\texttt{\{sentence\}}}'\end{tabular}} \\
%         \hline
%     \end{tabular}}
%     \vspace{-4mm}
% \end{table}

%\noindent
%See Appendix \ref{sec_appendix_predictor_model} for further details.
%\rl{The prompt is a bit strange. Anyhow consider putting it in a figure to make it more readable. The figure can also show an instantiation of the prompt with category and typology type}



% \subsection{Model Assessment}
% \label{sec_prediction_assessment}



\subsection{Classification Model Quality}
\label{sec_prediction_quality}
We now measure the model's ability to predict all \taxtypes{} of the proposed typology in a multi-class multi-label setup.

\paragraph{Test data.}
We first need to collect test data on which to evaluate the model. Due to the long-tail effect, a small sample of sentences representing the actual data distribution would likely contain very few instances for certain \taxtypes{} (e.g., \textit{comparative seller} and \textit{inappropriate}).
%, if any, instances for certain \taxtypes{} \rl{such as ...}.
We thus prepared a more \taxtype{}-balanced test set in the following manner: 
We ran the classifier model from \S{\ref{sec_prediction_model}} on a large sample of sentences from Amazon reviews~\citep{he2016reviewsDS},\footnote{The categories of the reviews are partly different from those in the development set.} as described in Appendix~\ref{sec_appendix_experiments_analysis}, and for each of the 24 \taxtypes{}, sampled 10 random sentences whose prediction scores are above the optimal threshold determined on the development set.
% This new set of 240 sentences produces data that likely renders instances even for the rare \taxtypes{}. 
The original annotating author (\S{\ref{sec_prediction_model}}) then annotated these 240 sentences, \textit{without} access to the model's predicted labels, to create the test set.\footnote{While this two-step procedure indeed resulted in a higher count of under-represented types, the \taxtype{}-distribution Pearson correlation between the dev and test sets is still high (0.91), due to the multi-label nature of many sentences. See \autoref{tab_test_scores_per_type} in the appendix for the \taxtype{}-distributions in the development and test sets.}

\paragraph{Results.}
As commonly conducted in multi-class multi-label classification, we measure the accuracy of the classifier using a macro-$F_1$ score, i.e., the non-weighted average of \taxtype{}-level $F_1$ scores on the sentences in the test set. Our classifier achieves a macro-$F_1$ of 56.7, with per-\taxtype{} $F_1$ scores reported in \autoref{tab_test_scores_per_type} in the appendix. Compare this to a random baseline (that guesses with 50\% chance) that receives a macro-$F_1$ of 40.9.

It is apparent that some \taxtypes{} are predicted better than others; however on manual inspection, most of the incorrect predictions are borderline cases (see \autoref{tab_example_annotations} in the appendix for examples). This is further corroborated by computing a coarse-grained score (mapping the 24 \taxtypes{} to the 8 groups from \autoref{tab_taxonomy_groups}), which produces a macro-$F_1$ score of 79.7 (with an expected random baseline score of 47.4). I.e., \taxtypes{} of similar semantic characteristics may be wrongly predicted in reference to the gold label, but are still instead identified as closely related \taxtypes{}.

The results demonstrate the usability of the model to generally classify review sentences according to our typology. While future work could further improve the classifier, either with training data for fine-tuning, more prompt engineering, or better base models, our current model is reliable enough for our analyses presented next.\footnote{We note that some of our \taxtypes{} have been studied before (e.g., \textit{tips} and \textit{opinions}). In Appendix \ref{sec_appendix_predictor_eval_type_specific} we present results for classification of these individual labels, reinforcing the usefulness of our classifier, as a generalization of this previous work.}
% We can now analyze reviews and experiment on common downstream tasks pertaining to them (Sections \ref{sec_experiments} and \ref{sec_analysis}).


%\input{Figures/table_predictor_eval}

%\input{Figures/table_tips_results_vs_baseline}


% \subsection{Individual \taxtype{} Evaluation}
% \label{sec_prediction_specific_eval}
% Having ascertained the overall quality of our classifier, we turn to inspect its abilities on three specific \taxtypes{} that were previously studied independently.
% In all cases, we obtain existing annotated datasets and establish training sets for the sole purpose of setting a prediction threshold for one of our \taxtypes{}, which is then used as the predicted label for the annotated task.
% We experiment with tuning over the entire training set as well as with much smaller subsets of 100 sentences (more details in App. \ref{sec_appendix_experiments}.)

% \paragraph{Tips.}
% Product tips are generally defined as short, concise, practical, self-contained pieces of advice on a product~\citep{hirsch2021producttips}, for example: \say{Read the card that came with it and see all the red flags!}.
% While previous work~\citep{hirsch2021producttips, farber2022tips} characterized tips into finer-grained sub-types, our definition assumes a generic definition.
% We tune our model's \emph{tip} \taxtype{} threshold using the training data annotated by~\citet{hirsch2021producttips} over Amazon reviews for non-tips or tips (details in Appendix~\ref{sec_appendix_experiments_tips}).
% We noticed that some sentences were annotated in the dataset as tips, even though we do not view them as such, e.g.,~\say{The ring that holds the card together is kind of flimsy}.
% Since this was especially true under the \textit{warning} sub-type, %, but was also apparent in other sub-types. 
% we removed these sentences from the data, to better represent our notion of a \textit{tip}.


% \paragraph{Opinions.}
% Product reviews are rich in opinions and are often more convincing when a reason is provided for the opinion. 
% A persuasive saying, whether opinionated or objective, is a type of \textit{argument}~\citep{ecklekohler2015arguments}, a class found to be helpful for predicting review helpfulness~\citep{liu2017argsForHelpfullness,passon2018helpfulness,chen2022argumentMiningForHelpfulness}.
% We thus use an an argument mining dataset, $AM^2$~\citep{chen2022argumentMiningForHelpfulness}, annotated at the clause level for various sub-types of subjective and objective arguments and reasons (details in Appendix~\ref{sec_appendix_experiments_opinions}).
% We automatically re-label this data into sentence-level \textit{opinion}, \textit{opinion with reason}, or \textit{not opinion} classes, and tune prediction thresholds for the \textit{opinion} and \textit{opinion with reason} \taxtypes{}, which we treat as separate classification tasks, each against the non-opinionated class.


% \paragraph{Results.}
% \autoref{tab_predictor_eval} presents the classification results on the three specific \taxtype{} evaluations.
% Our zero-shot classifier, which only uses the training sets to find an optimal threshold for the corresponding \taxtype{}, is able to identify \textit{tip}s and \textit{opinion}s effectively, and \textit{opinion with reason} fairly well, on their respective benchmarks.\footnote{\citet{chen2022argumentMiningForHelpfulness} do not provide any intrinsic baseline results on opinion classification to which we can compare. In addition, we manipulated the original data from clause-level to sentence-level.} Moreover, limiting the training sets to only 100 samples appears to hardly have any effect on the quality of the model, showing its robustness to paucity of labeled data.
% For the \textit{tip} task, we see in \autoref{tab_tips_results_vs_baseline} that the predictor performs on par or much better than existing supervised baselines from \citet{hirsch2021producttips}. Although our definition of \emph{tip} differs slightly from that used in annotating the benchmark, our predictor is still reliable and useful.

% The results of the direct and \taxtype{}-specific evaluations above demonstrate the dependability of the model to generally classify review sentences according to our typology. 
% We can now analyze reviews and experiment on common downstream tasks pertaining to them (Sections \ref{sec_experiments} and \ref{sec_analysis}).