\section{Experiments}
\label{sec:experiments}

We evaluate our framework using 500+ hardware trials that vary twelve environmental factors (Fig.~\ref{fig:all factors}) for two visuomotor diffusion policies with significantly different architectures. These experiments seek to investigate the following questions: 
\begin{enumerate}
\item How well does \redit~identify vulnerabilities and predict policy performance when relevant environmental factors are varied? 
\item How effective is \redit~in enabling policy improvement via targeted data collection? 
\item How good of a proxy is anomaly detection for performance degradation in different environmental conditions?
\end{enumerate}

{\bf Hardware setup.} 
Fig.~\ref{fig:anchor} visualizes our hardware platform: a bimanual manipulator with two Kuka IIWA robotic arms (our experiments only utilize the left arm). We use a Weiss gripper to interact with the objects in the environment. For sensing, we use a dual camera setup with a fixed overhead camera and another camera mounted on the left wrist.
% (with RealSense cameras) wherein one camera is fixed and mounted over the shoulders of the robot and the other camera is mounted near the wrist of the left arm. % The overhead camera is used to provide global sensing of the environment while the wrist camera provides accurate sensing  when interacting closer to the objects. 

{\bf Training data.} We use a trajectory optimization-based motion planner to automatically collect a set of training data consisting of 3K+ demonstrations for grasping objects. These demonstrations are collected in nominal conditions, i.e., with fixed lighting, with a fixed pink background on a table, and an object set that consists of blocks, plush toys, small cans, and artificial fruits. Additional details on the data collection pipeline are provided in Appendix~\ref{app:policy}. We highlight that the chosen task (grasping) is relatively easy to learn, and hence makes the problem of red teaming \emph{more challenging}; trained policies demonstrate a nontrivial degree of generalization, but are also vulnerable in ways that are hard to intuit.
% and demonstrate varying (and hard-to-predict) levels of 
% but can fail unpredictably in off-nominal conditions.
% Predicting red teaming for a very challenging task is straightforward --- one can simply predict that the policy will fail in any off-nominal scenarios.

{\bf Policies.} We consider two policies that vary significantly in their overall architecture. The first policy, $\pi_\text{hyb}$, uses a hybrid policy architecture inspired by \cite{belkhale2023hydra}, which aims to combine the benefits of trajectory optimization for free-space planning with the reactive nature of closed-loop visuomotor diffusion policies~\cite{chi2023diffusion}. We achieve this by using two separate heads in a diffusion policy architecture: the first predicts a waypoint to be reached using trajectory optimization, and the second predicts a temporally dense sequence of actions. An additional head predicts which mode should be executed at any given time. All three heads are trained \emph{jointly} using a diffusion objective. The latent embedding (used by \redit~for anomaly detection) is a vector in $\mathbb{R}^{512}$ that encodes the robot's visual and proprioceptive observations, along with a keypoint selected by the robot's operator that serves as an instruction on which object to grasp. 


We also separately train a vanilla diffusion policy~\cite{chi2023diffusion}, $\pi_\text{dfn}$, with a single head that outputs a trajectory sequence at every time-step (executed in a receding-horizon manner). The latent embedding for this policy is a vector in $\mathbb{R}^{515 \times 513}$. Details of the vision and instruction encoders, along with other implementation details, are provided in Appendix~\ref{app:policy}. Both policies achieve a success rate of approximately $65\%$ for nominal conditions, as measured by 30 trials (each) that vary objects, their locations, and the target object. 

{\bf Environmental factors.} We choose a set $F$ of twelve environmental factors that reflect common vulnerabilities of visuomotor policies trained via behavior cloning. These are shown in Fig.~\ref{fig:all factors}, and include: (1--3) three changes to the {\bf lighting} conditions (red, green, blue), (4--6) three changes to the color (red, green, blue) of the table {\bf background} on which objects are placed, (7--10) four {\bf distractor} objects (black and white trash can, laptop, candle) that partially occlude other objects, (11) a distractor in the form of a {\bf person} close to the table, and (12) a change to the {\bf height} of the table (which changes the location of objects relative to the overhead camera). In order to evaluate the predictions made by \redit, we execute both policies in 20+ episodes for each of the twelve factors; this allows us to estimate the corresponding success rates $R^{\pi_\text{hyb}}_f$ and $R^{\pi_\text{dfn}}_f, \forall f \in F$. The subsequent results thus include 500+ hardware trials. 

\subsection{How accurately does \redit~identify vulnerabilities and predict policy performance?}
\label{sec:redit experiments}

We first evaluate how well \redit~predicts the performance degradation induced by each of the twelve environmental factors for the different policies. We utilize two metrics to evaluate \redit, which correspond to the two versions of predictive red teaming described in Sec.~\ref{sec:problem formulation}: 
\begin{enumerate}
    \item {\bf Spearman rank correlation}~\cite{zar2005spearman}: this is a value $\rho \in [-1,1]$ which measures how accurately \redit \emph{ranks} the different factors by performance degradation. 
    \item {\bf Average prediction error}: measures how accurately \redit~predicts the \emph{absolute success rates} for the different factors by computing $\frac{1}{|F|}\sum_{f \in F} |R_{f}^\pi - R_{f, \text{pred}}^\pi|$. 
    \end{enumerate}
    
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/prediction_results_combined.pdf}
    \caption{Evaluating predictions from \redit for  $\pi_\text{hyb}$ (which combines trajectory optimization with diffusion) in the top panel and $\pi_\text{hyb}$ (vanilla diffusion policy) in the bottom panel. Left: Comparison of true (estimated) rankings of different environmental factors by performance degradation with predictions made by \redit. Right: Comparison of true (estimated) success rates with predictions from \redit.}
    \label{fig:predictions hybrid}
    \vspace{-10pt}
\end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\textwidth]{figures/prediction_results_predictive.png}
%     \caption{Evaluating predictions from \redit for the vanilla diffusion policy $\pi_\text{dfn}$. Left: Comparison of true (estimated) rankings of different environmental factors by performance degradation with predictions made by \redit. Right: Comparison of true (estimated) success rates with predictions from \redit.}
%     \label{fig:predictions predictive}
%     \vspace{-10pt}
% \end{figure*}    

{\bf Implementation.} In order to make predictions using \redit, we generate a set $S_f$ of 100 edited observations for each factor $f$ using first time-step observations from a held-out portion of training episodes. Examples of edits and complete prompts are provided in Fig.~\ref{fig:anchor} and Appendix~\ref{app:image editing}. We compute the resulting anomaly rates $\alpha_f^\pi$ using $S_f$ for each policy as described in Alg.~\ref{alg:redit}. 
We take the anomaly score $s_\pi(o, S_\text{nom})$ to be the mean of the $k$-nearest neighbor cosine distances (in the respective policy embedding space) to a set $S_\text{nom}$, which is chosen to be a subset of first-time-step observations from the training episodes.

{\bf Results.} Fig.~\ref{fig:predictions hybrid} evaluates predictions made by \redit~for $\pi_\text{hyb}$ (top row) and $\pi_\text{dfn}$ (bottom row). Fig.~\ref{fig:predictions hybrid}-left compares the rankings of different environmental factors $f \in F$ predicted by \redit with the true rankings as measured by the 20+ hardware evaluations for each factor (a lower rank corresponds to a lower success rate). Fig.~\ref{fig:predictions hybrid}-right compares the absolute success rates predicted by \redit~with the true (estimated) success rates. As the figure illustrates, the predictions for both rankings and absolute performance are strongly correlated with the true rankings and success rates. For example, \redit~successfully identifies that $\pi_\text{hyb}$ is relatively robust to object or person distractors, moderately impacted by changes in the background and lighting, and strongly impacted by changing the height of the table. \redit~also successfully identifies that $\pi_\text{dfn}$ is more vulnerable than $\pi_\text{hyb}$ to certain environmental factors such as blue lighting and a change in the table height.  
% Fig.~\ref{fig:predictions predictive} presents analogous comparisons for $\pi_\text{dfn}$, which demonstrates similar characteristics. 

Table~\ref{tab:predictions} quantitatively evaluates the predictions made by \redit~for both policies. The Spearman $\rho$ indicates a strong correlation between the predicted and actual rankings of factors, while the average prediction error is under $0.19$ for both policies (which is roughly in the range of noise when estimating success rates from $\sim 20$ trials). 

\begin{table}[h]
\centering
\resizebox{0.5\linewidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
           \redit        & $\pi_\text{hyb}$ &    $\pi_\text{dfn}$  \\ \midrule
{\bf Spearman}  $\rho \in [-1,1]$           & ~0.8  ($\uparrow$)     & ~0.7 ($\uparrow$)  \\
{\bf Av. prediction error} $ \in [0,1]$    & 0.10  ($\downarrow$)  & 0.19 ($\downarrow$) \\ \bottomrule
\end{tabular}%
}
\caption{Quantitative evaluation of success rates predicted by \redit compared with real success rates.}
\label{tab:predictions}
\vspace{-5pt}
\end{table}

{\bf Ablations.} For the results above, we use $k=5, |S_\text{nom}| = 3000$ for $\pi_\text{hyb}$ and $k=10, |S_\text{nom}| = 500$ for $\pi_\text{dfn}$.  We provide results from ablating the values $k$ and $|S_\text{nom}|$ in Appendix~\ref{app:ablations}. Generally, we find that predictions for $\pi_\text{hyb}$ remain accurate when varying $|S_\text{nom}|$ with small $k$, while predictions for $\pi_\text{dfn}$ (which has a significantly higher dimensional embedding space) benefit from either having a smaller value of $|S_\text{nom}|$ or larger values of $k$. 

\subsection{How effective is \redit~in enabling policy improvement via targeted data collection?}
\label{subsec:targeted-data-collection}

% \begin{table}[]
% \resizebox{1.0\linewidth}{!}{%
% \begin{tabular}{@{}lllllllllll@{}}
% \toprule
%               & Nominal  & \multicolumn{3}{c}{Lighting} &  & \multicolumn{3}{c}{Background} &  & Table Height \\ \midrule
%               &  & Red     & Blue    & Green    &  & Red     & Blue     & Green     &  &              \\
% Base   &        & &         &          &  &         &          &           &  &              \\
% Finetuned  & 0.82 & 0.89   &  0.88.  &    0.812     &  &  0.721  &  0.695     &  0.60      & & 0.684              \\ \bottomrule
% \end{tabular}%
% }
% \caption{}
% \label{tab:my-table}
% \end{table}

Our second set of experiments seeks to evaluate how well predictions from \redit~enable \emph{policy improvement} via targeted data collection. To this end, we collect additional demonstration data for $\pi_\text{hyb}$ with the three environmental factors that \redit~predicts the highest performance degradation for: blue lighting, change in the table height, and blue table background.
We collect around 1 hour of training data ($\approx 100$ trajectories) under each of these off-nominal settings. We then co-finetune our initial learned policy $\pi_\text{hyb}$ on the older data combined with the new small amount of off-nominal data. During co-finetuning, we ensure that each mini-batch consists of $80\%$ of the original data mixture and $20\%$ from the new off-nominal data. We co-finetune the policy with a reduced learning rate ($5e-6$) for a total of 20K steps.  

The fine-tuned policy $\pi_\text{hyb}^\text{ft}$ is evaluated in nominal conditions, the three conditions for which we collected data, and also the other background and lighting conditions. Videos of $\pi_\text{hyb}^\text{ft}$ are available on the \href{https://predictive-red-team.github.io/}{project website}. Fig.~\ref{fig:finetuning} shows the results. We observe improved performance in nominal conditions and a 2--7x improvement in off-nominal conditions under which training data was collected. Interestingly, the targeted data collection also yields \emph{cross-domain generalization}: the performance of the policy is improved by 2--5x even for background and lighting conditions where we did not collect additional data. This highlights the benefits of targeting data collection towards adverse scenarios via predictive red teaming. 

\begin{figure}[h]
% \vspace{pt}
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/finetuning.png}
    \caption{Fine-tuning with data collected under conditions predicted to be adverse shows cross-domain generalization and boosts baseline performance by 2--7x.}
    \label{fig:finetuning}
\vspace{-10pt}
\end{figure}

\subsection{How accurately does anomaly detection predict performance degradation?} 
\label{sec:anomaly-to-real}

Our final set of experiments seeks to evaluate the anomaly detection component of \redit~in isolation from the image editing pipeline. To this end, instead of executing the embedding-based anomaly detector on the set $S_f$ of \emph{edited} observations (as described in Algorithm~\ref{alg:redit}), we execute the detector on the set $S_f^\text{real}$ composed of \emph{real} robot observations collected from the first time step of the 20+ episodes where the factor $f$ is varied. We then compute the corresponding anomaly rates $\alpha_{f, \text{real}}^\pi$ ($\forall f \in F$). Predicted success rates for each factor are computed as  $R_{f, \text{anom}}^\pi := 1 - \alpha_{f, \text{real}}^\pi$ and compared with the (estimated) true success rates. Additional implementation details are provided in Appendix~\ref{app:anomaly-to-real}.

Table~\ref{tab:anomaly-to-real} presents the Spearman $\rho$ and average prediction errors. Appendix~\ref{app:anomaly-to-real} presents figures analogous to Fig.~\ref{fig:predictions hybrid}. While we observe high values of $\rho$ and low values of the prediction error for both policies, we note that the predictions $R_{f, \text{anom}}^\pi$ are made using $5\times$ fewer observations than predictions from the full \redit~pipeline ($\sim 20$ real observations vs. $100$ edited observations), thus making them significantly more susceptible to noise. 

\begin{table}[t]
\centering
\resizebox{0.5\linewidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
           Anomaly detector                 & $\pi_\text{hyb}$              &    $\pi_\text{dfn}$  \\ \midrule
{\bf Spearman}  $\rho \in [-1,1]$           & ~0.6  ($\uparrow$)   & ~0.8 ($\uparrow$)  \\
{\bf Av. prediction error} $ \in [0,1]$    & 0.20 ($\downarrow$)           & 0.21 ($\downarrow$) \\ \bottomrule
\end{tabular}%
}
\caption{Evaluating predictions of success rates made from anomaly rates computed using real observations.}
\label{tab:anomaly-to-real}
\vspace{-5pt}
\end{table}

% \begin{table}[t]
% \centering
% \begin{tabular}{r|c|c}
% \toprule
% Anomaly detector & $\pi_\text{hyb}$ & $\pi_\text{dfn}$ \\
% \midrule
% {\bf Spearman $\rho \in [-1,1]$ & ~0.5 ($\uparrow$) & 0.5 ($\uparrow$) \\
% {\bf Av. prediction error $ \in [0,1]$} & 0.19 ($\downarrow$) & 0.24 ($\downarrow$) \\
% \bottomrule
% \end{tabular}
% \caption{Anomaly-to-real.}
% \label{tab:anomaly-to-real}
% \vspace{-15pt}
% \end{table}

