\section{Problem: Predictive Red Teaming}
\label{sec:problem formulation}

% \begin{itemize}
%     \item Inputs: Nominal training data, nominal held-out data
%     \item Inputs: Policy w/ white-box access on nominal data; focus here is on visuomotor policies (primarily RGB-based)
%     \item Goal: Predict failure rates for different types of generalization (e.g., lighting, distractors, etc.)
%     \item Emphasize: no rollouts in non-nominal scenarios
% \end{itemize}

We formally introduce the problem of predictive red teaming: {\it exposing vulnerabilities of a given policy with respect to environmental factors such as lighting, visual distractors, and object locations, and predicting their impact on performance without performing any hardware evaluations in these off-nominal scenarios}.  

{\bf Nominal scenarios.} In each episode, the robot is deployed in a scenario $\xi$, which is defined as a partially observable Markov decision process (POMDP) initialized in a particular state. Let $\mathcal{D}_\text{nom}$ be a distribution over scenarios that captures nominal variations in all environmental factors (e.g., objects that the robot may encounter, lighting conditions, background colors, etc.) and tasks (via the reward function). We do not assume knowledge of $\mathcal{D}_\text{nom}$, except a dataset $S_\text{nom}$ of observations collected from nominal scenarios $\xi \sim \mathcal{D}_\text{nom}$. 

{\bf Inputs to the red team.} The \emph{red team} is provided a deterministic or stochastic policy $\pi$ that maps observations $o_t \in \mathcal{O}$ to actions $a_t \in \mathcal{A}$, along with the dataset $S_\text{nom}$ of nominal observations. Our focus in this paper will be on visuomotor policies trained via imitation learning; in this setting, $S_\text{nom}$ can consist of observations from the training dataset for $\pi$. We also assume access to a set $S_\text{val}$ of nominal observations that were held out when training $\pi$. The specific approach we present in this paper will only require nominal observations $S_\text{nom} \cup S_\text{val}$ collected at the \emph{start} of episodes. 
The red team is provided the ability to query $\pi$ on arbitrary observations, potentially with white-box access to internal representations of the policy. 

{\bf Goal: predictive red teaming.} The red team's goal is to expose vulnerabilities of $\pi$ with respect to various environmental factors $f \in F$ chosen by the red team.  These factors may be arbitrarily fine-grained, e.g., the introduction of a particular distractor or a specific change to the table color. Formally, let $\mathcal{D}_f$ be a distribution of scenarios where a factor $f$ has changed relative to the nominal distribution $\mathcal{D}_\text{nom}$. Let $R_\text{nom}^\pi$ be the expected reward of $\pi$ for scenarios $\xi \sim \mathcal{D}_\text{nom}$, and let $R_f^\pi$ be the expected reward for $\mathcal{D}_f$. For simplicity, we will assume henceforth that rewards are bounded in $[0,1]$. Knowing $R_\text{nom}^\pi$, we consider two problems: (1) \emph{rank} the factors $f \in F$ by performance degradation, and (2) predict the \emph{absolute} performance $R_f^\pi, \ \forall f \in F$. The former problem is important for targeted data collection, while the latter helps understand the envelope of acceptable performance.

% We consider two versions of predictive red teaming:
% \begin{enumerate}
%     \item Predict the \emph{relative} impact of all factors $f \in F$ on the expected performance of the policy (e.g., predict that a red background will reduce the success rate twice as much as a blue background). 
%     \item Predict the \emph{absolute} degradation in expected performance induced by factors $f \in F$. 
% \end{enumerate}