\section{Related Work}
\label{sec:related work} 

% Papers:
% Red teaming:
% Human red teaming:
% OpenAI.GPT-4 technical report.
% Gemini tech report.
% Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned
% https://arxiv.org/pdf/2204.05862
% Jailbroken: How Does LLM Safety Training Fail?
% Auto red teaming:
% Red teaming language models with language models
% Mass-producing failures of multimodal systems with language models
% . Universal and transferable adversarial attacks on aligned language models
% Jailbreaking black box large language models in twenty queries.
% AutoDAN: Generating stealthy jailbreak prompts on aligned large language models
% Tree of attacks: Jailbreaking black-box LLMs automatically
% VLM (auto) red teaming:
% https://arxiv.org/abs/2401.12915
% https://arxiv.org/abs/2407.15050
% Robotics jail break:
% https://arxiv.org/abs/2410.13691
% Embodied red teaming: 
% https://arxiv.org/abs/2411.18676
% Misc.:
% Off-policy evaluation:
% https://arxiv.org/pdf/2005.01643

{\bf Red teaming.} The concept of red teaming originated in the military realm, where a team posing as the enemy tries to find vulnerabilities of a military plan~\cite{zenko2015red}. In recent years, the practice of red teaming has been adopted for finding vulnerabilities of large language models (LLMs) in terms of bias, misuse, and other harmful behavior~\cite{achiam2023gpt, team2023gemini, ganguli2022red, bai2022training, wei2024jailbroken}. While red teaming for LLMs was initially performed by human evaluators, this limits the coverage of possible issues that can be discovered. As a result, recent work has sought to partially automate the process of red teaming~\cite{perez2022red, tong2024mass, zou2023universal, chao2023jailbreaking, liu2023autodan, mehrotra2023tree}, e.g., by using LLMs themselves to discover vulnerabilities. 

While there is a growing literature on red teaming for vision-language models~\cite{li2024red, liu2024arondight} and text-to-image generative models~\cite{rando2022red, gandikota2023erasing}, red teaming for robotics is still nascent. Recent work has considered \emph{embodied red teaming} for finding flaws in language-conditioned robotic foundation models~\cite{karnik2024embodied}. Specifically, \cite{karnik2024embodied} focuses on \emph{instruction generalization}: how well does a policy perform when faced with novel language instructions? As such, all evaluations in~\cite{karnik2024embodied} are performed in simulation. Related work has also considered \emph{jailbreaking} LLM-powered robots~\cite{robey2024jailbreaking}, i.e., finding adversarial prompts that override safety guardrails and cause robots to perform harmful actions. In contrast to \cite{karnik2024embodied, robey2024jailbreaking}, our focus is on finding \emph{environmental factors} (e.g., background colors, lighting, object locations) that degrade the performance of a given policy without performing hardware evaluations in off-nominal scenarios. The work in~\cite{pumacay2024colosseum} uses simulation to assess the generalization of policies with respect to environmental factors. However, setting up an accurate simulator for RGB-based policies in a new environment can require significant (e.g., months-long) human effort. In contrast, the pipeline we propose is data-driven and automated (with access only to policy training data and text descriptions of desired environmental changes). 

{\bf Anomaly detection and failure prediction.} Methods for \emph{failure prediction} seek to foresee failures as the robot is operating. Typical approaches include ones based on reachability analysis~\cite{akametalu2014reachability, hsu2023sim, hsu2023safety}, control barrier functions~\cite{ames2016control}, formal methods~\cite{alshiekh2018safe}, and learned predictors~\cite{farid2022failure, xie2022ask, gokmen2023asking, liu2024model}. A related line of work on \emph{anomaly detection} seeks to detect conditions that are far from nominal and may thus induce failures~\cite{richter2017safe, sinha2022system, salehi2021unified, sinha2024real,sindhwani2020unsupervised}. Our approach to predictive red teaming uses conformal prediction-based anomaly detection~\cite{vovk2005algorithmic, laxhammar2013online, luo2024sample, sinha2023closing}, which allows one to provide statistical assurances on the false positive rate of detection. Recently, conformal prediction has also been utilized in the context of robotics to provide statistical assurances on language-based planners, perception systems, and trajectory prediction systems~\cite{lindemann2023safe, dixit2023adaptive, ren2023robots, dixit2024perceive, lindemann2024formal}. All of the prior work mentioned above on failure prediction, anomaly detection, and conformal prediction develops methods that operate at \emph{runtime} in order to detect possible failures and take remedial measures. In contrast, we utilize anomaly detection to forecast performance in different environmental conditions by executing the detector on edited observations that reflect changes in these conditions. 

{\bf Generative image editing.} Prior work in robotics uses generative image editing~\cite{baldridge2024imagen,  betker2023improving, nichol2021glide, yu2023inpaint, ling2021editgan, zhu2020domain} for data augmentation~\cite{chen2023genaug, yu2023scaling, bharadhwaj2024roboagent, chen2024rovi, chen2024semantically}, generating sub-goals for image-conditioned policies~\cite{black2023zero, shah2023vint}, and runtime observation editing for visual generalization~\cite{hancock2024run}. In this work, we utilize a \emph{language-conditioned} image editing model (\texttt{Imagen 3}~\cite{baldridge2024imagen}) to generate image observations that reflect changes in various environmental factors (Fig.~\ref{fig:anchor}). By modifying real robot observations with targeted edits (e.g., ``change the background to red" or ``add a trash can to the scene"), we are able to generate synthetic observations with a high degree of realism. 

{\bf Off-policy evaluation.} The problem of predictive red teaming is related to \emph{off-policy evaluation} in reinforcement learning~\cite{precup2000eligibility, hallak2017consistent, hanna2017bootstrapping, farajtabar2018more}. The goal is to estimate the performance of a target policy using data collected by executing a different policy. This can be used for policy improvement, particularly in the offline reinforcement learning setting~\cite{levine2020offline}. Off-policy evaluation is similar to our goal of predictive teaming: both attempt to evaluate the performance of a policy without evaluating the policy on the robot. However, the two problems are also distinct: predictive red teaming attempts to predict the performance of a given policy in off-nominal conditions by executing the \emph{same} policy in nominal conditions. 

% \begin{itemize}
% Pierre's paper and BYOVLA. Prior uses: controllable domain randomization / data augmentation (e.g., Rosie), sub-goal generation (e.g., Susie, Dhruv's work), runtime observation editing for OOD generalization (BYOVLA). Add citation to concurrent constitution work if possible. Our use: red teaming. 
%     \item Anomaly detection and conformal prediction: typical use is for runtime monitoring. We use it as a failure rate predictor. Related work: Rohan's RSS '24 paper, Rohan's OOD survey paper, other anomaly detection work, usual CP work (e.g., KnowNo, etc.). 
% \end{itemize}