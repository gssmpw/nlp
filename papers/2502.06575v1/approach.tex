\section{\redit: Predictive Red Teaming via Image Editing and Anomaly Detection}
\label{sec:approach}

% Overview of full approach. Two main components: (i) red team edits, (ii) anomaly prediction. 

% \subsection{Generating Red Team Edits}

% \begin{itemize}
% \item Choose different axes of generalization, e.g., background, lighting, distractors, etc.
% \item For each setting, generate edits from nominal (held-out) data; we use Imagen 3 (cite tech report). We make edits for both overhead- and wrist- cameras. 
% \item Emphasize: modern image editing tools can perform various tasks (e.g., inpainting, colorization, etc.) in a controllable manner via language instructions. They can maintain global consistency of the image (e.g., adding shadows, etc.), which is not feasible with less sophisticated methods. 
% \item Show examples of different kinds of edits: \todofigure
% \item Say something about prompting? Or will Juno folks get mad? 
% \item VLM filtering
% \end{itemize}

% \subsection{Predicting Failure Rates via Anomaly Detection}

% \begin{itemize}
%     \item Need some proxy for failure rate. Key idea: anomaly rate.
%     \item Embedding-based anomaly detection: NN cosine distance to bank of nominal embeddings
%     \item Conformal prediction: bounds false positive rate
%     \item Walk through full algorithm (with edits), w/ anomaly detection as a sub-routine
% \end{itemize}

We introduce \redit~({\bf Robo}tics {\bf A}uto-{\bf R}ed-{\bf T}eaming): a method for predictive red teaming using generative image editing and anomaly detection. We focus on visuomotor policies that rely primarily on RGB image observations. Our approach has two main steps, which are illustrated in Fig.~\ref{fig:anchor}. First, we use generative image editing tools to modify the nominal observations in $S_\text{val}$ (Sec.~\ref{sec:problem formulation}) to reflect changes in various factors of interest (e.g., background, lighting, distractor objects). For each factor, we then predict the performance degradation of the policy using anomaly detection. We describe each of these steps below. 

\subsection{Generative Image Editing}
\label{sec:image editing}

{\bf Selection of environmental factors.} The red team first selects a set $F$ of environmental factors that have the potential to degrade the performance of the given policy $\pi$. This set can be arbitrarily fine-grained in its contents (e.g., specific lighting conditions, distractor objects, background colors, etc.). The specific factors of interest will depend on the deployment needs of the policy and plausible environmental changes that the robot may encounter. 

{\bf Generating edited observations.} For each factor $f \in F$, we modify observations in the nominal set $S_\text{val}$ to reflect a change in $f$. We leverage state-of-the-art generative image editing tools, which have the capacity to take detailed language instructions as input in order to produce realistic and globally consistent edits. In this work, we specifically utilize the \texttt{Imagen~3} diffusion model~\cite{baldridge2024imagen}, which has been trained to perform language-prompted image editing tasks such as inpainting, outpainting, and colorization. 

As an example, consider an edit that adds a novel object to the scene. Fig.~\ref{fig:distractor edit} illustrates the prompt used for this edit, along with examples of the original and edited images. For robots with multiple cameras (e.g., a wrist camera in addition to an overhead camera), we edit each observation independently with the same prompt. Fig.~\ref{fig:distractor edit} shows the original and edited wrist camera images for the manipulator from Fig.~\ref{fig:anchor}. The image editing model is able to render the desired object in a realistic manner that maintains per-view global consistency in lighting, shadows, and overall composition of the scene (see Sec.~\ref{sec:future work} for a discussion of multi-view consistency). 

\begin{figure}[t]
    \centering
    % \includegraphics[width=\textwidth]{figures/edit_example.png}
    \includegraphics[width=\textwidth]{figures/edit_example_01_trimmed.pdf}
    \caption{Examples of adding a novel object to the visual scene via generative image editing. Original (top) and edited (bottom) observations from both the robot's overhead and wrist cameras are shown. State-of-the-art generative image editing tools render the desired object in a realistic manner that maintains per-view global consistency in lighting, shadows, and overall composition.}
    \label{fig:distractor edit}
    % \vspace{-5pt}
\end{figure}

% \begin{figure}[t]
% % \vspace{-5pt}
%     \centering
%     \includegraphics[width=\columnwidth]{figures/vlm_filtering_wide.pdf}
%     \caption{A vision-language model ensures that the edited image reflects the desired change.}
%     \label{fig:vlm filter}
% \vspace{-5pt}
% \end{figure}

In addition to adding novel objects to the scene, state-of-the-art image editing models allow us to generate edits corresponding to various changes with a high degree of realism and precision, e.g., changing the color of the background, adding a human in the scene, and changing lighting conditions. Examples of these edits are illustrated in Fig.~\ref{fig:anchor}. Full prompts along with additional examples are provided in Appendix~\ref{app:image editing} and the \href{https://predictive-red-team.github.io/}{project website}.

{\bf VLM critic.} Diffusion-based image editing models can generate multiple variations of edited images given the same input image and prompt. These variations often differ in terms of their quality and adherence to the prompt. In order to ensure that the edited observations accurately reflect the desired change in the environmental factor $f$, we generate a batch of four edited images per input, and utilize a vision-language model (VLM) as a \emph{critic}. As shown in Fig.~\ref{fig:vlm filter}, we prompt the VLM with the original and edited images, and ask it to judge if any of the options accurately reflect the desired change; if so, the VLM is tasked with choosing the best one (if not, we simply discard the observation from our set). The full prompt for the VLM --- which involves chain-of-thought reasoning --- is provided in Appendix~\ref{app:vlm filter}. We use the Gemini Pro 1.5 VLM~\cite{team2023gemini} for our experiments in Sec.~\ref{sec:experiments}.  

\begin{figure}[h]
% \vspace{-5pt}
    \centering
    \includegraphics[width=\columnwidth]{figures/vlm_filtering_wide.pdf}
    \caption{A vision-language model ensures that the edited image reflects the desired change.}
    \label{fig:vlm filter}
\vspace{-5pt}
\end{figure}

\subsection{Predicting Performance via Anomaly Detection}
\label{sec:anomaly detection}

At the end of the image editing process, the red team has a set $S_f$ of edited observations corresponding to each environmental factor $f \in F$. The second key component of \redit~(Fig.~\ref{fig:anchor}) uses $S_f$ to predict the performance degradation induced by each factor $f$. Our key idea is to utilize techniques from \emph{anomaly detection}: for each observation in $S_f$, we quantify how ``close" it is to nominal observations in $S_\text{nom}$ from the perspective of the policy $\pi$. If this distance is above a threshold computed using \emph{conformal prediction} \cite{vovk2005algorithmic}, the observation is flagged as an anomaly. The primary hypothesis is that one can define such a policy-specific anomaly detector that predicts performance degradation:
% \begin{equation}
% \label{eq:linear prediction}
% R_f^\pi \approx R_\text{nom}^\pi + \alpha_\text{nom}^\pi - \alpha_f^\pi,
% \end{equation}
\begin{equation}
\label{eq:linear prediction}
R_f^\pi \approx 1 - \alpha_f^\pi,
\end{equation}
where $R_f^\pi$ is the expected reward under factor $f$ (Sec.~\ref{sec:problem formulation}) and $\alpha_f^\pi$ is the anomaly rate for $f$, i.e., the proportion of edited observations in $S_f$ flagged as anomalous according to a threshold chosen to ensure $R_\text{nom}^\pi \approx 1 - \alpha_\text{nom}^\pi$ (where $\alpha_\text{nom}^\pi$ is the proportion of nominal observations flagged as anomalous). 

% Before describing the specifics of the anomaly detection method, we describe how Eq.~\ref{eq:linear prediction} allows us to tackle the two versions of predictive red teaming formalized in Section~\ref{sec:problem formulation}. 

% {\bf Predicting relative impact of factors.} Eq.~\ref{eq:linear prediction} directly allows us to predict the \emph{relative} performance degradation induced by different factors in $\mathcal{F}$. Let $f, f' \in \mathcal{F}$ be two factors; then using Eq.~\ref{eq:linear prediction}, we can compute their relative performance degradation:
% \begin{equation}
% \label{eq:relative prediction}
% \frac{R_\text{nom}^\pi - R_f^\pi}{R_\text{nom}^\pi - R_{f'}^\pi} \approx \frac{\alpha_f^\pi - \alpha_\text{nom}^\pi}{\alpha_{f'} - \alpha_\text{nom}^\pi}.
% \end{equation}

% {\bf Predicting absolute performance.} In order to predict the absolute expected performance $R_f^\pi$ for $f \in \mathcal{F}$, we need an estimate of the constant $\lambda$. This can be achieved with knowledge of the expected performance $R_{f_0}^\pi$ for a \emph{single} factor $f_0 \in \mathcal{F}$. By computing the anomaly rate $\alpha_{f_0}$ for edited observations corresponding to $f_0$, we compute $\lambda$ using Eq.~\ref{eq:linear prediction}. With $\lambda$ resolved, Eq.~\ref{eq:linear prediction} allows us to predict the expected performance $R_f^\pi$ for \emph{all} other factors $f \in \mathcal{F}$. As a practical matter, one may wish to choose $f_0$ such that it induces a small performance degradation in the policy. 

{\bf Anomaly detection.} Next, we further describe how to compute the anomaly rate $\alpha_f^\pi$ for each factor $f$ using the edited observations $S_f$. In this work, we utilize policy embedding distances as a method for quantifying how far from nominal a given observation is. This choice is motivated by the prior success of embedding-based methods in anomaly detection (see, e.g., \cite{sinha2024real, luo2024online}) and the simplicity of implementation. Let $\phi_\pi(o)$ be a latent representation produced by the policy $\pi$ for a given observation $o$. For policies directly parameterized using a neural network, a common choice is to use the output of an intermediate layer of the network. In our experiments in Sec.~\ref{sec:experiments}, we utilize policies parameterized using diffusion models. In this setting, we utilize the context vector provided to the denoising process as our latent representation; see Appendix~\ref{app:policy} for more details. Using $\phi_\pi$, we can define a policy-specific anomaly score $s_\pi(o, S_\text{nom})$ that quantifies how far from nominal the observation $o$ is. A simple choice is to define $s_\pi$ as the nearest-neighbor cosine distance between the embedding $\phi_\pi(o)$ and the embeddings computed for the nominal observations in $S_\text{nom}$:
\begin{equation}
s_\pi(o, S_\text{nom}) := \min \ \left\{ 1 - \frac{\phi_\pi(o) \cdot \phi_\pi(o_i^\text{nom})}{\|\phi_\pi(o)\| \|\phi_\pi(o_i^\text{nom})\|} \ \middle| \ o_i \in S_\text{nom} \right\}. 
\end{equation}
A more general variant that we use in our experiments is to compute the mean of the $k$-nearest neighbor cosine distances. Intuitively, this anomaly score quantifies how dissimilar a given observation is compared to similar training observations from the perspective of the policy. 

For each factor $f \in F$, we compute the anomaly score for all edited observations $o \in S_f$. 
% \begin{equation}
%     \Lambda_f := \left\{ s(o, S_\text{nom}) | o \in S_f \right\}.
% \end{equation}
The anomaly rate for a factor $f$ is then defined as the proportion of observations flagged as anomalous according to a threshold $\tau$:
\begin{equation}
    \alpha_f^\pi := \frac{\left| \left\{ o \in S_f \mid s_\pi(o, S_\text{nom}) > \tau \right\} \right|}{|S_f|}.
\end{equation}

{\bf Anomaly threshold.} The anomaly threshold $\tau$ is chosen to ensure that $\alpha_\text{nom}^\pi$ (the anomaly rate for \emph{nominal} observations) predicts the nominal success rate $R_\text{nom}^\pi$ of the policy: $R_\text{nom}^\pi \approx 1 - \alpha_\text{nom}^\pi$. Given access to a validation set $S_\text{val}$ with $n_\text{val}$ nominal observations, one can simply choose $\tau$ such that the proportion of these flagged as anomalous is $1 - R_\text{nom}^\pi$. A more sophisticated approach uses conformal prediction~\cite{vovk2005algorithmic}: 
\begin{equation}
\label{eq:tau}
    \tau := \text{quantile}_{ \frac{\lceil (n_\text{val}+1)R_\text{nom}^\pi\rceil}{n_\text{val}}}(\{s_\pi(o, S_\text{nom}) \ | \ o \in S_\text{val} \}),
\end{equation}
which chooses $\tau$ as the $\lceil (n_\text{val}+1)R_\text{nom}^\pi\rceil/{n_\text{val}}$ empirical quantile of the set of anomaly scores for the validation set. This choice upper bounds the probability that \emph{unseen} nominal observations are flagged as anomalous to $1 - R_\text{nom}^\pi$~\cite{angelopoulos2021gentle}. 

\vspace{7pt}

We summarize the key steps of \redit~in Algorithm~\ref{alg:redit}. 

% We utilize conformal prediction to compute the anomaly threshold $\tau$ in a manner that allows us to bound the probability that a nominal observation is flagged as anomalous to a desired threshold $\alpha_\text{nom}^\pi$. Denoting the cardinality of the set $S_\text{val}$ as $n_\text{val}$, let $\alpha$ be chosen such that it satisfies:
% \begin{equation}
% \text{Beta}^{-1}_{n_\text{val}+1-v,v}(\delta) = \alpha_\text{nom}^\pi, \ v :=\lfloor (n_\text{val}+1)\alpha\rfloor,
% \end{equation}
% where $\text{Beta}^{-1}_{n_\text{val}+1-v,v}(\delta)$ denotes the $\delta$-quantile of the Beta distribution with parameters $(n_\text{val}+1-v,v)$. 
% We then compute $\tau$ as the $\lceil (n_\text{val}+1)(1-\alpha)/n_\text{val} \rceil$ empirical quantile of the set $\{s_\pi(o, S_\text{nom}) \ | \ o \in S_\text{val} \}$:
% \begin{equation}
% \label{eq:tau}
%     \tau := \text{quantile}_{\lceil (n_\text{val}+1)(1-\alpha)/n_\text{val} \rceil}(\{s_\pi(o, S_\text{nom}) \ | \ o \in S_\text{val} \}).
% \end{equation}
% This choice of $\tau$ ensures the following theoretical guarantee: with probability $1-\delta$ over the sampling of the validation set $S_\text{val}$, the probability that new observations drawn from the nominal distribution $\mathcal{D}_\text{nom}$ being flagged as anomalous is bounded by $\alpha_\text{nom}^\pi$. We refer the reader to \cite[Section 3.2]{angelopoulos2021gentle} for further exposition on conformal prediction and this theoretical result, and provide complete pseudocode for computing $\tau$ in Appendix~\ref{app:conformal prediction}. The quantity $\delta$ is chosen to be $0.05$ in our experiments.

\begin{algorithm}[h]
\caption{\redit: Robotics Auto Red Teaming}\label{alg:redit}
\begin{algorithmic}
\State {\bf Input: } Policy $\pi$ with nominal performance $R_\text{nom}^\pi$, nominal observations $S_\text{nom} \cup S_\text{val}$
\State Select environmental factors $F$
\State {\bf Conformal prediction:}
\State \hspace{2pt}  Compute anomaly scores for $S_\text{val}$ using $\pi$ embeddings:
\State \hspace{15pt} $\Lambda_\text{val} := \{s_\pi(o, S_\text{nom}) \ | \ o \in S_\text{val} \}$
\State \hspace{2pt}  Compute anomaly threshold $\tau$ using $\Lambda_\text{val}$ to bound the \\ \hspace{2pt} nominal anomaly rate to $\alpha_\text{nom}^\pi := 1 - R_\text{nom}^\pi$ \Comment{Eq.~\ref{eq:tau}}
\For{$f \in F$}
    \State Generate edited observations $S_f$ \Comment{Filtered with VLM}
\State Compute anomaly rate: 
\State \hspace{5pt} $\alpha_f^\pi := \left| \left\{ o \in S_f \mid s_\pi(o, S_\text{nom}) > \tau \right\} \right| / |S_f|$
\State Predict performance:
\State \hspace{5pt} $R_{f, \text{pred}}^\pi := 1 - \alpha_f^\pi$
\EndFor

\end{algorithmic}
\end{algorithm}