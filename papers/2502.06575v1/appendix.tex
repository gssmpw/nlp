\newpage
% \onecolumn
\appendix
% \begin{appendices}
\section{Image Editing: Examples and Prompts}
\label{app:image editing}

Examples of different edits applied to both the overhead camera and the wrist camera are shown in Figure~\ref{fig:example edits}. Below, we provide complete prompts used to generate the edited observations for each environmental condition. 

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{figures/edit_nominal.png} % Replace with your image file
        \caption{Nominal overhead (left) and wrist (right) cameras.}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{figures/edit_person.png} % Replace with your image file
        \caption{Distractor: person.}
        \label{fig:sub2}
    \end{subfigure}

    \bigskip % Add vertical space between rows

    \begin{subfigure}[b]{0.49\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{figures/edit_black_trashcan.png} % Replace with your image file
        \caption{Distractor: trash can.}
        \label{fig:sub3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{figures/edit_candle.png} % Replace with your image file
        \caption{Distractor: candle.}
        \label{fig:sub4}
    \end{subfigure}

    \bigskip % Add vertical space between rows

    \begin{subfigure}[b]{0.49\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{figures/edit_laptop.png} % Replace with your image file
        \caption{Distractor: laptop.}
        \label{fig:sub5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{figures/edit_background_blue.png} % Replace with your image file
        \caption{Background: blue.}
        \label{fig:sub6}
    \end{subfigure}

    \bigskip % Add vertical space between rows

    \begin{subfigure}[b]{0.49\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{figures/edit_lighting_red.png} % Replace with your image file
        \caption{Lighting: red.}
        \label{fig:sub7}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{figures/edit_table_height.png} % Replace with your image file
        \caption{Table height (changed color followed by zoom).}
        \label{fig:sub8}
    \end{subfigure}

    \caption{Examples of different edits applied to nominal overhead and wrist camera observations.}
    \label{fig:example edits}
\end{figure}

\begin{tcolorbox}[
    colback=myboxcolor,
    colframe=myboxcolor,
    breakable,
    boxrule=0mm,
    arc=0mm,
    boxsep=0mm,
    left=3mm,
    right=3mm,
    top=3mm,
    bottom=3mm,
    ]
{\Large {\bf Full prompts for edits}} 

\vspace{10pt}

{\bf Person:} \\
Add a person to the image. Specifically, add a person behind the blue platform, realistically interacting with the platform and fitting seamlessly into the existing environment. Preserve all other aspects of the image, including the different objects on the mat, other background  elements, and the overall composition.  The lighting should remain consistent.  The new person should be realistically rendered with all details of the person including their face, clothing, and any other visible parts shown in exquisite clarity and detail. Only the person should be added. \\
{\bf Large distractor (e.g., trash cans):} \\
Add a large $<$\texttt{target color}$>$ $<$\texttt{target object}$>$ at the edge of the pink mat, so that it doesn't modify or occlude any of the objects on the pink mat. Specifically, add a $<$\texttt{target color}$>$ $<$\texttt{target object}$>$ that is larger than any objects at the edge of the pink mat, fitting realistically and seamlessly into the existing scene. Preserve all details of the objects on the mat, their poses, and the overall composition of the image. The $<$\texttt{target color}$>$ $<$\texttt{target object}$>$ should be realistically and exquisitely rendered and should not occlude any of the objects on the pink mat. The lighting should remain consistent. Only the $<$\texttt{target color}$>$ $<$\texttt{target object}$>$ at the edge of the pink mat should be added. \\
{\bf Small distractor (e.g., candle):} \\
Modify image $<$\texttt{image}$>$ as described below: Add a small scented candle on the pink mat, so that it doesn't modify or occlude any of the objects on the mat. Specifically, add a scented candle with roughly the same size as the objects on the pink mat, fitting realistically and seamlessly into the existing scene. Preserve all details of the composition of the image. The scented candle should be realistically and exquisitely rendered and should not occlude any of the objects on the pink mat. The lighting should remain consistent. Only the scented candle should be added. \\
 {\bf Background color:} \\
Modify image $<$\texttt{image}$>$ as described below: change the color of the pink mat that objects are on to $<$\texttt{target color}$>$. Preserve the different objects on the mat, and all other aspects of the image including the lighting and the overall composition. \\
{\bf Lighting (overhead camera):} \\
Modify image $<$\texttt{image}$>$ as described below: Colorize the bottom half of the image with an extremely intense $<$\texttt{target color}$>$ hue. Preserve the existing composition, details, and textures of the objects in the scene, including the ones on the pink mat and the background.  Only the shadows and color palette should be altered to reflect an extremely intense $<$\texttt{target color}$>$ light, maintaining the style of the original image. The overall lighting should remain consistent, with shadows and highlights adjusted to match the new color palette. Make sure that the hue for the bottom half of the image is changed to intense $<$\texttt{target color}$>$, including for the objects on the table. \\
{\bf Lighting (wrist camera):} \\
Modify image $<$\texttt{image}$>$ as described below: Colorize the entire image with an extremely intense $<$\texttt{target color}$>$ color tone. Preserve the existing composition, details, and textures of the objects in the scene, including the ones on the pink mat and the background.  Only the shadows and color palette should be altered to reflect an extremely intense $<$\texttt{target color}$>$ light, maintaining the style of the original image. The overall lighting should remain consistent, with shadows and highlights adjusted to match the new color palette. Make sure that the color for the entire image is changed to intense $<$\texttt{target color}$>$. \\
{\bf Table height:} \\
Change the color of the pink mat to $<$\texttt{target color}$>$. Preserve all other aspects of the image, including the different objects on the mat, the lighting, and the overall composition. Only the color of the pink mat should be altered to $<$\texttt{target color}$>$, maintaining its shape, size, and position. [We then apply a zoom to the portion of the image that contains the table in order to simulate a change in the height of the table.
\end{tcolorbox}

\section{Filtering Edits with a Vision-Language Model}
\label{app:vlm filter}

For each nominal observation, we generate a batch of four candidate edited observations via the image editing model. We then use a vision-language model (VLM) in order to judge if any of the options accurately reflect the desired change; if so, the VLM is tasked with choosing the best one (if not, we simply discard the observation from our set). The full prompt for the VLM --- which involves chain-of-thought reasoning --- is provided below. We use the Gemini Pro 1.5 VLM~\cite{team2023gemini} for our experiments. 


\begin{tcolorbox}[
    colback=myboxcolor,
    colframe=myboxcolor,
    breakable,
    boxrule=0mm,
    arc=0mm,
    boxsep=0mm,
    left=3mm,
    right=3mm,
    top=3mm,
    bottom=3mm,
    ]
{\Large {\bf Prompt for filtering edits with a VLM}} 

\vspace{10pt}

Here is the original image I have: $<$\texttt{original image}$>$. Do any of Image~0: $<$\texttt{Image 0}$>$, Image~1: $<$\texttt{Image 1}$>$, Image~2: $<$\texttt{Image 2}$>$, or Image~3 $<$\texttt{Image 3}$>$ accurately reflect an edited version of the original image with the instruction ``$<$\texttt{short edit instruction}$>$"? Give your reasoning and then answer with a True or False. If True, provide the index (0,1,2,3) of the best image.
\end{tcolorbox}

The variable $<$\texttt{short edit instruction}$>$ contains a shortened version (e.g., ``Change the color of the pink mat to $<$\texttt{target color}$>$") of the full prompt provided to the image editing model. We find that providing the full prompt (instead of a shortened version) can lead the VLM to be overly critical and filter out many acceptable edits. 


\section{Training and Policy Details}
\label{app:policy}

\subsection{Training Data Collection}

{\bf Training data collection.} For training our policies, we collect 3K+ demonstrations for grasping tasks on the hardware. Specifically, we use trajectory optimization-based motion planners to automatically collect a large set of training data. Our data collection pipeline uses the overhead camera to obtain a 3D point cloud of the scene. We segment the point cloud into multiple objects and randomly choose different objects to pick using the left arm. We use automated success detection to segment these trajectories. For each episode, we further automatically annotate keypoints for the object the policy should grasp; these are used as additional context for the robot policy in addition to camera and proprioceptive observations. All demonstrations are collected in nominal conditions, i.e., with fixed lighting, with a fixed pink background on a table, and an object set that consists of blocks, plush toys, small cans, and artificial fruits. 


\subsection{Hybrid Policy Architecture}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/hybrid_policy_architecture_01.pdf}
    \caption{The policy architecture used for two different --- hybrid and diffusion --- policy implementation. Our unified architecture consists of a trajectory mode which predicts the continuous joint space actions and a waypoint mode which predicts a single SE(3) waypoint that the arm should reach to. We use two different policies. 1. \emph{hybrid policy} uses both the trajectory and waypoint mode and selects between them to execute the action. 2. \emph{\expandafter\nonhybridpolicy} only uses the trajectory mode and directly predicts the joint space trajectory for the robot to follow.}
    \label{fig:architecture}
\end{figure*}

{\bf Hybrid policy.} We consider two policies that vary significantly in their overall architecture. The first policy $\pi_\text{hyb}$ uses a hybrid policy architecture inspired by \cite{belkhale2023hydra}, which aims to utilize the benefits of trajectory optimization-based approaches for free space planning together with the reactive nature of closed-loop visuomotor diffusion policies. We achieve this by using two separate heads in our policy architecture (see Fig.~\ref{fig:architecture}), where each head represents an \emph{action mode}. These two different action modes correspond to:
\begin{enumerate}
    \item a waypoint action mode which outputs a single waypoint $(w \in SE(3))$, and
    \item a trajectory action mode which outputs a dense sequence of robot joint angles $(q_i \in \mathbb{R}^{14})$.
\end{enumerate}
In addition to these policy heads we also output a mode selection scalar which defines which action mode should be executed at any given time. In order to execute the waypoint action we use a trajectory optimization approach based on sequential quadratic programming (SQP), and execute the output trajectory for a fixed number of steps before re-querying the policy. By contrast, in order to execute the trajectory action we simply interpolate through the joint commands outputted by the network. Importantly, during training both policy heads are trained \emph{simultaneously}, i.e., each input data item is labelled with a waypoint action (extracted using an object closeness heuristic) and a dense trajectory action (which we directly extract from the robot logs). We supervise the mode selection scalar to output the waypoint action mode when the arm is far away from any object and the trajectory action mode in all other scenarios. 

{\bf Vision encoder.} Our policy architecture uses pre-trained ViT \cite{vit} encoders to encode the image observations from each image. We use separate models for each camera observation (overhead and wrist). We reduce the number of tokens from each ViT using a TokenLearner layer \cite{ryoo2021tokenlearner}. We encode proprioceptive features using a multi-layer perceptron (MLP) with a single hidden layer.

{\bf Instruction encoder.} The robot is instructed to grasp a target object using semantic keypoints. Specifically, we extract a small patch $(64 \times 64)$ from the overhead camera view around a keypoint that is selected by the robot operator. We encode this patch using a small coordinate convolution-based neural network.
% While semantic keypoints allow the policy to infer \emph{which} object the policy should interact with, 
% to infer \emph{how} it should interact with the object we use a skill-id based continuous embedding. % I don't think we really use this since we are just doing grasping, and this is fixed?
Since we train a multi-skill policy we encode the skill that the robot needs to perform using a continuous embedding.
The semantic keypoint representation is concatenated with the skill embedding to form the instruction tokens.


{\bf Context Fuser.} The observation tokens, the instruction tokens and proprioceptive tokens are fused together using a context fuser 
which uses a stack of self-attention based transformer layers. We also additionally add a readout token, which we refer to as the waypoint-mode token.
At the end of the context fuser we get a set of fused observation-instruction embeddings as well as the embedding for the readout token.  The observation-instruction embeddings are used to predict the trajectory and thus passed into the trajectory diffusion transformer.
Alternatively, the waypoint-mode embedding is used by the waypoint diffusion transformer to predict the $SE(3)$ waypoint as well as to
predict the current mode for the robot. The observation-instruction embeddings are used by \redit~for anomaly detection. 

{\bf Diffusion.} For both trajectory diffusion and waypoint diffusion we use a Transformer decoder-based denoiser \cite{diffusion-transformer}.
The denoiser takes as input noisy action embeddings together with a diffusion timestep embedding.
These noisy actions and timestep embeddings cross-attend to the context embeddings (either the context tokens for trajectory diffusion or waypoint embedding for waypoint prediction).
After multiple layers of alternating between self-attention and cross-attention the diffusion transformer outputs the denoised trajectory or waypoint action (as desired).

\subsection{Diffusion Policy.}
Our \nonhybridpolicy architecture $\pi_\text{dfn}$ uses a standard diffusion policy \cite{chi2023diffusion, reuss2023goal}
to directly output the joint angles to control the robot.
Our base architecture is similar to $\pi_\text{hyb}$ (described above) wherein we only use the trajectory mode, i.e.,
only the trajectory diffusion head is used to predict robot trajectories.
The rest of the architecture including the vision encoders and the multi-modal instruction encoder are common between
$\pi_\text{dfn}$ and $\pi_\text{hyb}$.
However, unlike $\pi_\text{hyb}$, $\pi_\text{dfn}$ does not include a readout token (waypoint/mode token) within the context fuser.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[]
\centering
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
Hyperparam                  & Value                 \\ \midrule
train steps                 & 500K                  \\
optimizer                   & AdamW                 \\
warmup                      & linear upto 10K steps \\
learning rate               & 1e-4                  \\
learnign rate decay         & constant              \\
weight decay                & 1e-4                  \\
trajectory (action) horizon & 10                    \\ \bottomrule
\end{tabular}%
}
\caption{Hyperparameters used to train the different policies used.}
\label{tab:train-hparams-table}
\end{table}

\subsection{Training and Inference Details}
\label{app:train-details}

Table~\ref{tab:train-hparams-table} shows the common set of hyper-parameters used to train each of our policies.
We use a batch size of 256 during training.
As shown in Figure~\ref{fig:architecture} for the high dimensional image observations we use an additional token learner to reduce the number of image tokens. We use 128 tokens for each image observation.
For our diffusion model we use a continuous time implementation based on \cite{karras2022elucidating}. Similar to \cite{karras2022elucidating} we use a second order Heun solver to solve the flow ODE. 
We use 30 ODE steps during inference. As shown in Table~\ref{tab:train-hparams-table}, we use an action horizon of 10.
Since we collect our training data at 10Hz this corresponds to 1 second of robot motion. During inference, we open-loop rollout
entire 10 steps before querying the policy again.
During evaluation we use a maximum of 30 policy steps before we stop policy evaluation.
For our targeted data collection experiment Section~\ref{subsec:targeted-data-collection}, we use a much smaller learning rate of 
$5e-6$ and a linear warmup of 4K steps. We finetune the policy for a total 20K steps.
% \ani{Mention size of $S_\text{val}$ and $S_\text{nom}$ somewhere.}


\section{Ablations: Score Function and Size of Nominal Rerence Set}
\label{app:ablations}

\subsection{Ablations for \redit} 


\redit~uses an anomaly score function $s_\pi(o, S_\text{nom})$ that computes the mean of the $k$-nearest neighbor cosine distances in policy embedding space. The set $S_\text{nom}$ consists of embeddings computed for a random subset of the training data. Intuitively, this anomaly score quantifies how dissimilar a given observation is compared to similar training observations from the perspective of the policy. In the tables below, we provide results from varying the size of $S_\text{nom}$ and the value of $k$ for each policy. Each table compares the predictions made by \redit~with the actual (empirically measured) performance by computing the Spearman rank correlation and the average prediction error, as described in Sec.~\ref{sec:redit experiments}. Generally, we find that predictions for $\pi_\text{hyb}$ remain accurate when varying $|S_\text{nom}|$ with small $k$, while predictions for $\pi_\text{dfn}$ (which has a significantly higher dimensional embedding space) benefit from either having a smaller value of $|S_\text{nom}|$ or larger values of $k$. 

\vspace{10pt} 

{\bf Hybrid policy} $\pi_\text{hyb}$

\vspace{10pt} 

\begin{tabular}{c || *{4}{c}}
  \hline
  $|S_\text{nom}| = 3000$ & $k=1$ & $k=5$ & $k=10$   \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & 0.72 & {\bf 0.78} & 0.78 \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & 0.12 & {\bf 0.1} & 0.12  \\
  \hline
\end{tabular}

\vspace{10pt} 

\begin{tabular}{c || *{4}{c}}
  \hline
  $|S_\text{nom}| = 2000$ & $k=1$ & $k=5$ & $k=10$   \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & 0.72 & {\bf 0.79} & 0.68 \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & 0.12 & {\bf 0.12} & 0.13  \\
  \hline
\end{tabular}

\vspace{10pt}

\begin{tabular}{c || *{3}{c}}
  \hline
  $|S_\text{nom}| = 1000$ & $k=1$ & $k=5$ & $k=10$  \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & {\bf 0.76} & 0.65 & 0.63 \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & {\bf 0.12} & 0.13 & 0.17  \\
  \hline
\end{tabular}

\vspace{10pt}

\begin{tabular}{c || *{3}{c}}
  \hline
  $|S_\text{nom}| = 500$ & $k=1$ & $k=5$ & $k=10$  \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & {\bf 0.69} & 0.63 & 0.56 \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & {\bf 0.12} & 0.16 & 0.19  \\
  \hline
\end{tabular}

\vspace{10pt}

\begin{tabular}{c || *{3}{c}}
  \hline
  $|S_\text{nom}| = 200$ & $k=1$ & $k=5$ & $k=10$  \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & {\bf 0.72} & 0.65 & 0.49 \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & {\bf 0.14} & 0.18 & 0.23  \\
  \hline
\end{tabular}

\vspace{10pt}

{\bf Vanilla diffusion policy} $\pi_\text{dfn}$

\vspace{10pt}

\begin{tabular}{c || *{7}{c}}
  \hline
  $|S_\text{nom}| = 3000$ & $k=1$ & $k=5$ & $k=10$ & $k=25$ & $k=50$ & $k=100$ & $k=200$  \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & 0.59 & 0.52 &  0.56 &  0.66 & 0.59 & {\bf 0.67} & 0.66 \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & 0.22 & 0.21 & 0.21 &  0.20 & 0.20 & {\bf 0.19} & 0.20 \\
  \hline
\end{tabular}

\vspace{10pt}

\begin{tabular}{c || *{7}{c}}
  \hline
  $|S_\text{nom}| = 2000$ & $k=1$ & $k=5$ & $k=10$ & $k=25$ & $k=50$ & $k=100$ & $k=250$  \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & 0.55 & 0.52 &  0.53 &  0.64 & 0.64 & {\bf 0.69} & 0.17 \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & 0.21 & 0.20 & 0.20 &  0.20 & 0.20 & 0.20 & 0.25 \\
  \hline
\end{tabular}

\vspace{10pt}

\begin{tabular}{c || *{4}{c}}
  \hline
  $|S_\text{nom}| = 1000$ & $k=1$ & $k=5$ & $k=10$ & $k=25$  \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & {\bf 0.63} & 0.52 &  0.60 &  0.59 \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & 0.21 & 0.20 & {\bf 0.19} &  0.20 \\
  \hline
\end{tabular}

\vspace{10pt}

\begin{tabular}{c || *{3}{c}}
  \hline
  $|S_\text{nom}| = 500$ & $k=1$ & $k=5$ & $k=10$  \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & 0.58 & 0.66 & {\bf 0.71} \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & 0.21 & 0.19 & {\bf 0.19}  \\
  \hline
\end{tabular}

\vspace{10pt}

\begin{tabular}{c || *{3}{c}}
  \hline
  $|S_\text{nom}| = 200$ & $k=1$ & $k=5$ & $k=10$  \\
  \hline
  \hline
  Spearman $\rho$ ({\bf $\uparrow$}) & 0.61 & 0.64 & {\bf 0.75} \\
  \hline
  Av. pred. err. ({\bf $\downarrow$}) & 0.19 & 0.20 & {\bf 0.19}  \\
  \hline
\end{tabular}

\vspace{10pt}

% \newpage
\section{Predicting Performance From Anomalies}
\label{app:anomaly-to-real}

Fig.~\ref{fig:anomaly-vs-real} compares the true (estimated) rankings of different environmental factors and success rates with rankings and success rates predicted by executing the anomaly detector on $\sim20$ real observations collected from each of the twelve off-nominal settings. Specifically, predicted success rates for each factor are computed as  $R_{f, \text{anom}}^\pi := 1 - \alpha_{f, \text{real}}^\pi$. For anomaly detection, we use $k=10, |S_\text{nom}| = 3000$ for $\pi_\text{hyb}$ and $k=10, |S_\text{nom}| = 200$ for $\pi_\text{dfn}$. The anomaly threshold for $\pi_\text{hyb}$ is computed using conformal prediction as described in Sec.~\ref{sec:anomaly detection} in order to bound the anomaly rate in nominal conditions to $1 - R_\text{nom}^{\pi_\text{hyb}}$. For $\pi_\text{dfn}$, we found this procedure to yield an anomaly threshold that is too conservative (i.e., flagging most observations in the different off-nominal scenarios as anomalous). This sensitivity may be due to the relatively small number $n_\text{val} = 70$ of nominal observations we used to compute the anomaly threshold and the very high dimensionality of the embedding space ($\mathbb{R}^{515\times513}$) of $\pi_\text{dfn}$. In order to correct for this, we computed the anomaly threshold with a slightly higher estimate of the nominal success rate (0.8 vs. 0.65), i.e., using conformal prediction to bound the anomaly rate in nominal conditions to $1 - 0.8$ rather than $1-0.65$. Fig.~\ref{fig:anomaly-vs-real} shows a strong correlation between predicted and realized performance. We note that the predictions $R_{f, \text{anom}}^\pi$ are made using $5\times$ fewer observations than predictions from the full \redit pipeline ($\sim 20$ real observations vs. $100$ edited observations), thus making them significantly more susceptible to noise. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/anomaly_prediction_results.png}
    \caption{Evaluating predictions made from anomaly rates computed using real observations.}
    \label{fig:anomaly-vs-real}
    \vspace{-10pt}
\end{figure*}


% \end{appendices}