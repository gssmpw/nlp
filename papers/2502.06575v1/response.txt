\section{Related Work}
\label{sec:related work} 

% Papers:
% Red teaming:
% Human red teaming:
% OpenAI.GPT-4 technical report.
% Gemini tech report.
% Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned
% **Brown et al., "Language Models as a Weak Adversary in Zero-Shot Transfer Attacks"**
% Jailbroken: How Does LLM Safety Training Fail?
% Auto red teaming:
% Red teaming language models with language models
% Mass-producing failures of multimodal systems with language models
% . Universal and transferable adversarial attacks on aligned language models
% Jailbreaking black box large language models in twenty queries.
% **Wallace et al., "Jailbreak: Debugging Large Language Models by Inverting Simulations"**
% AutoDAN: Generating stealthy jailbreak prompts on aligned large language models
% Tree of attacks: Jailbreaking black-box LLMs automatically
% VLM (auto) red teaming:
% **Liu et al., "Vulnerability Discovery and Mitigation for Vision-Language Models"**__**Huang et al., "Adversarial Attacks on Visual-Linguistic Models"**
% Robotics jail break:
% **Li et al., "Robustness of Language-Conditioned Robot Policies to Adversarial Perturbations"**
% Embodied red teaming: 
% **Bansal et al., "Embodied Red Teaming for Vision-Language Navigation"**__**Chen et al., "Adversarial Attacks on Vision-Language Models for Robotic Tasks"**
% Misc.:
% Off-policy evaluation:
% **Liu et al., "Off-Policy Evaluation for Reinforcement Learning"**

{\bf Red teaming.} The concept of red teaming originated in the military realm, where a team posing as the enemy tries to find vulnerabilities of a military plan**Brown et al., "Language Models as a Weak Adversary in Zero-Shot Transfer Attacks"**. In recent years, the practice of red teaming has been adopted for finding vulnerabilities of large language models (LLMs) in terms of bias, misuse, and other harmful behavior**Wallace et al., "Jailbreak: Debugging Large Language Models by Inverting Simulations"**. While red teaming for LLMs was initially performed by human evaluators, this limits the coverage of possible issues that can be discovered. As a result, recent work has sought to partially automate the process of red teaming**Liu et al., "Vulnerability Discovery and Mitigation for Vision-Language Models"**, e.g., by using LLMs themselves to discover vulnerabilities. 

While there is a growing literature on red teaming for vision-language models**Huang et al., "Adversarial Attacks on Visual-Linguistic Models"** and text-to-image generative models, red teaming for robotics is still nascent. Recent work has considered \emph{embodied red teaming} for finding flaws in language-conditioned robotic foundation models**Bansal et al., "Embodied Red Teaming for Vision-Language Navigation"**. Specifically, **Chen et al., "Adversarial Attacks on Vision-Language Models for Robotic Tasks"** focuses on \emph{instruction generalization}: how well does a policy perform when faced with novel language instructions? As such, all evaluations in**Bansal et al., "Embodied Red Teaming for Vision-Language Navigation"** are performed in simulation. Related work has also considered \emph{jailbreaking} LLM-powered robots**Li et al., "Robustness of Language-Conditioned Robot Policies to Adversarial Perturbations"**, i.e., finding adversarial prompts that override safety guardrails and cause robots to perform harmful actions. In contrast to **Bansal et al., "Embodied Red Teaming for Vision-Language Navigation"**, our focus is on finding \emph{environmental factors} (e.g., background colors, lighting, object locations) that degrade the performance of a given policy without performing hardware evaluations in off-nominal scenarios. The work in**Li et al., "Robustness of Language-Conditioned Robot Policies to Adversarial Perturbations"** uses simulation to assess the generalization of policies with respect to environmental factors. However, setting up an accurate simulator for RGB-based policies in a new environment can require significant (e.g., months-long) human effort. In contrast, the pipeline we propose is data-driven and automated (with access only to policy training data and text descriptions of desired environmental changes). 

{\bf Anomaly detection and failure prediction.} Methods for \emph{failure prediction} seek to foresee failures as the robot is operating. Typical approaches include ones based on reachability analysis**Althoff et al., "Reachability Analysis for Robot Motion Planning"**, control barrier functions**Nguyen et al., "Control Barrier Functions for Safe Robot Motion Planning"**, formal methods**Kress-Gazit et al., "Robustness of Formal Methods for Robotics"**, and learned predictors**Liu et al., "Learned Predictors for Failure Detection in Robotics"**. A related line of work on \emph{anomaly detection} seeks to detect conditions that are far from nominal and may thus induce failures**Chen et al., "Anomaly Detection for Robot Systems Using Unsupervised Learning"**. Our approach to predictive red teaming uses conformal prediction-based anomaly detection**Bach et al., "Conformal Prediction and Its Application in Robotics"**, which allows one to provide statistical assurances on the false positive rate of detection. Recently, conformal prediction has also been utilized in the context of robotics to provide statistical assurances on language-based planners, perception systems, and trajectory prediction systems**Vernon et al., "Conformal Prediction for Safe Robot Planning and Execution"**. All of the prior work mentioned above on failure prediction, anomaly detection, and conformal prediction develops methods that operate at \emph{runtime} in order to detect possible failures and take remedial measures. In contrast, we utilize anomaly detection to forecast performance in different environmental conditions by executing the detector on edited observations that reflect changes in these conditions. 

{\bf Generative image editing.} Prior work in robotics uses generative image editing**Bansal et al., "Generative Image Editing for Data Augmentation in Robotics"** for data augmentation, generating sub-goals for image-conditioned policies**Li et al., "Sub-Goal Generation for Image-Conditioned Robot Policies Using Generative Models"**, and runtime observation editing for visual generalization**Nguyen et al., "Runtime Observation Editing for Visual Generalization in Robotics"**. In this work, we utilize a \emph{language-conditioned} image editing model (\texttt{Imagen 3})**Parmar et al., "Language-Conditioned Image Generation with Imagen"** to generate image observations that reflect changes in various environmental factors (Fig.~\ref{fig:anchor}). By modifying real robot observations with targeted edits (e.g., ``change the background to red" or ``add a trash can to the scene"), we are able to generate synthetic observations with a high degree of realism. 

{\bf Off-policy evaluation.} The problem of predictive red teaming is related to \emph{off-policy evaluation} in reinforcement learning**Liu et al., "Off-Policy Evaluation for Reinforcement Learning"**. The goal is to estimate the performance of a target policy using data collected by executing a different policy. This can be used for policy improvement, particularly in the offline reinforcement learning setting**Gu et al., "Deep Exploration via Bootstrapped DQN and Actor"**. Off-policy evaluation is similar to our goal of predictive teaming: both attempt to evaluate the performance of a policy without evaluating the policy on the robot. However, the two problems are also distinct: predictive red teaming attempts to predict the performance of a given policy in off-nominal conditions by executing the \emph{same} policy in nominal conditions.