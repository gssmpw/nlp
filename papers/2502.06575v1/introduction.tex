\section{Introduction}
\label{sec:intro}

Is it possible to expose the vulnerabilities of a given robot policy with respect to changes in environmental factors such as lighting, visual distractors, and object placement \emph{without performing hardware evaluations in these scenarios}? As we seek to deploy robots in environments with ever-increasing complexity, it becomes imperative to develop scalable methods for predicting how well they will generalize when faced with unseen scenarios. Performing hardware evaluations to discover vulnerabilities --- which can depend in surprising ways on the specifics of policy training and architecture --- is often prohibitively expensive to set up and execute, especially when the goal is to test the limits of safe deployment in a sufficiently diverse set of scenarios. 
% Uncovering these sources of brittleness through hardware evaluations can be extremely time consuming and expensive to set up and execute, especially when the goal is to test the limits of performance in a sufficiently diverse set of scenarios. 

As an example, consider a visuomotor diffusion policy~\cite{chi2023diffusion} trained to perform pick-and-place tasks via behavior cloning (Fig.~\ref{fig:anchor}). The policy is trained with a large dataset: over 3K+ demonstrations with varied objects, locations, and visual distractors. Will the policy generalize well to a change in the height of the table by a few centimeters (as one may plausibly predict due to the variations in 2D object locations in the training dataset) compared to when a human is standing closer to the table than seen during training? If so, what is the absolute degradation of the success rate in each case? As it turns out, the above prediction is incorrect: the success rate of the policy degrades from $\sim65\%$ under nominal conditions to $\sim10\%$ by changing the table height, and remains roughly constant with a human close to the table. Predicting the relative and absolute impact of other factors (e.g., lighting, table backgrounds, object distractors; Fig.~\ref{fig:all factors}) can be even more challenging.

% % s, with a large degree of diversity in visual distractors and object placements. However, the majority of training scenarios contain a pink mat where objects are placed. Will replacing the pink mat with a red mat degrade performance less relative to a blue mat (as one may plausibly predict)? 
% % One may plausibly surmise that replacing the pink mat with a red one will only degrade performance by a small degree, while replacing it with a blue mat will cause a larger degradation. 
% If so, what is the absolute degradation in success rates in each case? As it turns out, the above prediction is incorrect: the success rate of the policy degrades from approximately $75\%$ under nominal conditions to $15\%$ for a red background and $35\%$ for a blue background. Predicting the impact of other environmental factors (e.g., the height of the table or the introduction of a specific distractor object) can be even more challenging. % Moreover, finding vulnerabilities

\vspace{2pt}
{\bf Contribution 1 (\emph{Predictive Red Teaming}).} We introduce and formalize the problem of \emph{predictive red teaming}: discovering vulnerabilities of a given policy with respect to changes in environmental factors, and predicting the (relative or absolute) degradation in performance \emph{without} performing hardware evaluations in off-nominal scenarios. 
\vspace{2pt}

The ability to perform predictive red teaming has a number of important consequences. First, it enables \emph{targeted deployment}: by understanding the envelope of conditions that will yield satisfactory performance, we can choose where the policy is deployed. Second, it enables \emph{policy comparison}: knowing the relative vulnerabilities of different policies allows us to select one that is more likely to meet deployment needs. Third, it enables \emph{targeted data collection}: if we know that certain environmental conditions degrade performance more than others, we can re-train the policy with additional data from the adverse conditions in order to help patch vulnerabilities. 

\vspace{2pt}
{\bf Contribution 2 (\redit).} We introduce \redit --- robotics automated red teaming (ART) --- an approach to predictive red teaming for visuomotor policies based on generative image editing and anomaly detection. 
\vspace{2pt}

\begin{figure*}[t]
    \centering
    % \includegraphics[width=\textwidth]{figures/all_factors.png}
    \includegraphics[width=\textwidth]{figures/all_factors_keynote_01_trimmed.pdf}
    \caption{We evaluate \redit's predictions using 500+ hardware trials in twelve off-nominal conditions.}
    \label{fig:all factors}
    \vspace{-5pt}
\end{figure*}

The pipeline for \redit~has two main steps: \emph{edit} and \emph{predict} (Fig.~\ref{fig:anchor}). First, we use automated image editing tools~\cite{baldridge2024imagen, esser2024scaling, controlnet, saharia2022photorealistic} to modify a set of nominal RGB observations by varying different factors of interest (e.g., lighting, distractors, object locations) in a fine-grained and realistic manner via language instructions (e.g., ``add a person close to the table"; Fig.~\ref{fig:anchor}). The second step is to predict the degradation in performance induced by each environmental factor using \emph{anomaly detection}. Specifically, we find that a simple anomaly detector that computes distances in policy embedding space between edited observations and a set of nominal observations (with an anomaly threshold computed using \emph{conformal prediction} \cite{vovk2005algorithmic}) is surprisingly predictive of both relative and absolute performance degradation. 

% Specifically, we find that computing the distance in policy embedding space of each edited observation to a set of nominal observations, and find that the proportion of edited observations flagged as anomalous (using a threshold computed using \emph{conformal prediction} \cite{vovk2005algorithmic}) is surprisingly predictive  

\vspace{2pt}
{\bf Contribution 3 (\emph{Demonstration for visuomotor diffusion policies}).} We evaluate \redit using 500+ hardware experiments that vary twelve environmental factors for two visuomotor diffusion policies with significantly different architectures.
\vspace{2pt}

We find that \redit~predicts performance degradation with a high degree of accuracy, e.g., correctly predicting that the changed table height will degrade performance significantly more than a human distractor. The difference between predicted and real success rates averaged across the twelve factors is 0.1 and 0.19 respectively for the two policies. 

\vspace{2pt}
{\bf Contribution 4 (\emph{Targeted data collection}).} We demonstrate the utility of predictive red teaming for targeted data collection by co-finetuning a policy with data collected in scenarios predicted to yield low performance. 
\vspace{2pt}

Co-finetuning the policy with data from the three conditions predicted to be the most adverse boosts performance in these conditions by 2--7x. Moreover, targeted data collection also yields \emph{cross-domain generalization}: the performance of the policy is improved by 2--5x even for conditions where we did not collect data.



% Contributions:
% 1. Predictive red teaming
% 2. Redit: predictive red teaming via image editing and anomaly detection
% 3. Experiments w/ redit: investigate anomaly-to-failure gap and edit-to-real gap
% 4. Targeted data collection

% {\bf Contributions.} In this paper, we introduce and formalize the problem of \emph{predictive red teaming}: discover vulnerabilities of a given policy with respect to changes in environmental factors, and predict the resulting (relative or absolute) degradation in performance \emph{without} performing hardware evaluations in off-nominal scenarios. 
