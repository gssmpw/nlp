% !TeX root = 0_main

\section{Results}
\label{sec:results}
\subsection{RQ$_1$: Quality Annotation Results}
	We compared all the quality annotations in the ground truth with those produced by both \tool and \EulerC.   
	\EulerC fails to identify four (among the 78) S2Rs from the ground truth, whereas \tool fails to identify two \rev{S2Rs}. We considered these in our analysis as they represent false negatives.
	
	Table \ref{tab:qa-metrics-comparison-results} compares the performance of \tool and \EulerC for each quality annotation and in aggregate across all annotations. Overall performance metrics were computed by summing the TPs, FPs, and FNs. Overall, \tool outperforms \EulerC in S2R annotation by a relative improvement of 25.2\% in terms of F1 score. \rev{The overall performance difference between \tool and \EulerC are statistically significant according to the Wilcoxon test (p-values = 0.03, 0.004, and 0.005 for precision, recall, and F1 scores, respectively).}
	\looseness=-1 
	
	

	Both \rev{\tool and \EulerC} incorrectly labeled \rev{two S2Rs as CS}. For example, the S2R \textit{"Add a split"} from report \#699 from GnuCash App \cite{gnucash699} requires a user to perform \textit{tap add split button} and \textit{type split amount} interactions. However, GPT-4 \rev{identified only one mapped interaction for this S2R.}
	\EulerC also \rev{incorrectly labeled it as CS} because the matching algorithm used by \EulerC is too restrictive: it \rev{maps an S2R to} one interaction even if multiple interactions exist on the screen. \rev{Moreover, \EulerC failed to annotate 16 S2Rs as CS, while \tool failed  to annotate four S2Rs as CS.}
	
	Furthermore, \EulerC incorrectly \rev{labeled} S2Rs as AS for two S2Rs where only one \rev{mapped interaction exist}, whereas \tool never \rev{made such errors}. For example, the individual S2R, \textit{"Select an event"} from bug report \#154 in schedule-campfahrplan \cite{schedule154} is annotated as AS by \EulerC because it incorrectly \rev{mapped to multiple actions (\eg\ long click or click)}. However, \tool accurately \rev{annotated the S2R as CS}. Overall, we observed this is because (i) GPT-4 is able to correctly identify whether the S2Rs refers to single or multiple interactions, and (ii) \tool can \rev{ map an S2R to} multiple interactions on the current GUI screen when reporters \rev{combine steps or use generic verbs}. Moreover, \EulerC failed to \rev{annotate  four S2Rs as AS while \tool failed to annotate two S2Rs as AS.}
	\looseness=-1
	
	Additionally, the vocabulary mismatch (VM) annotation is typically assigned when interactions are not present in the GUIs. \EulerC produced 12 false positive VMs due to low graph coverage and its inability to infer the actual step in the app, even with the capability of adding screens/interactions to the graph while executing S2Rs (see \EulerC's paper for details~\cite{Chaparro2019}). In contrast, \tool incorrectly annotated two \rev{S2Rs} only as VM. The reasons for \EulerC failing more than \tool can be attributed to the restrictive matching algorithm \EulerC uses, whereas \tool leverages GPT-4 to map interactions to S2Rs. 
	\rev{It is also possible that \EulerC could not cover the necessary screens even after random exploration, whereas \tool utilizes a more complete graph in mapping interactions for S2Rs that traverse many GUI app screens.}
	
	\begin{table}[t]
		\centering
\vspace{-0.7em}
		\caption{Quality Annotation Results (\tool vs. \EulerC)}
	\label{tab:qa-metrics-comparison-results}
\resizebox{\columnwidth}{!}{%
	\begin{tabular}{c|c|c|c|c|c|c|c}
		\hline
		\textbf{QAs}           & \textbf{Approach} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{\#TP} & \textbf{\#FP} & \textbf{\#FN}\\
		\hline
		\multirow{2}{*}{CS} & \EulerC & 0.964 & 0.771 & 0.857 & 54 & 2 & 16\\
		&\tool & 0.971 & 0.943 & 0.957 & 66 & 2 & 4\\
		\hline
		
		\multirow{2}{*}{AS} & \EulerC & 0.600 & 0.429 & 0.500 & 3 & 2 & 4\\
		&\tool & 1.000 & 0.714 & 0.833 & 5 & 0 & 2\\
		\hline
		
		\multirow{2}{*}{MS} & \EulerC & 0.600 & 0.553 & 0.575 & 21 & 14 & 17\\
		&\tool & 0.750 & 0.789 & 0.769 & 30 & 10 & 8\\
		\hline
		
		\multirow{2}{*}{VM} & \EulerC & 0.077 & 1.000 & 0.143 & 1 & 12 & 0\\
		&\tool & 0.333 & 1.000 & 0.500 & 1 & 2 & 0\\
		\hline
		
		\multirow{2}{*}{\textbf{Overall}} & \EulerC & \textbf{0.725} & \textbf{0.681} & \textbf{0.702} & \textbf{79} & \textbf{30} & \textbf{37}\\
		&\tool & \textbf{0.879} & \textbf{0.879} & \textbf{0.879} & \textbf{102} & \textbf{14} & \textbf{14}\\
		\hline
		
	\end{tabular}%
 }

	\end{table}
	
	\rev{There can be one or multiple missing steps  between two consecutive mapped interactions for two S2Rs. In such cases, we annotated the latter S2R as MS for analysis purposes.}
In \EulerC's 14 misclassified MS annotations, the suggested missing steps were not necessary for bug reproduction. 
In contrast, out of the ten individual S2Rs misclassified as MS by \tool, four involved completely unnecessary steps for bug reproduction, and \rev{the remaining six were due to the incorrectly mapped interactions.} 
For example, the S2R, \textit{"Select Units"} from bug report \#12 from \rev{DroidWeight \cite{droidweight12} could not map to a GUI interaction because the corresponding GUI component description is "back modal" and that GUI interaction was considered a missing step.} This leads to the necessity for developers to use meaningful GUI descriptions in the underlying code. A possible strategy to address this issue is to use GUI interaction images in the prompt to GPT-4 to provide additional context. \rev{Moreover, \tool failed to identify MS annotations for 8 S2Rs whereas \EulerC failed in 17 S2Rs, meaning \tool is more successful in identifying intermediate paths between two S2Rs.}

\subsection{RQ$_2$: Missing S2R Results}

\begin{figure}[t]
	\vspace{-1em}
	\centering
	\includegraphics[width=0.5\linewidth]{figures/ms-venn.pdf}
	\caption{\# of Missing Steps Generated by \tool}
	\label{fig:ms-venn}
	\vspace{-0.3cm}
\end{figure}

In addition to evaluating \EulerC and \tool's ability to assign MS annotations, we also calculated the number of intermediate steps missing between \rev{two} consecutive S2Rs. The ground truth dataset consists of 158 missing steps (7.5 per bug report on average) across 21 bug reports. \EulerC suggested 271 missing steps (12.9 missing steps per bug report on average), and 80 of those steps are present in the ground truth, achieving a precision of 0.295, a recall of 0.506, and an F1 score of 0.373. In contrast, \tool suggested 158 missing steps (7.5 per bug report on average), 101 of which are in the ground truth, resulting in a precision, recall, and F1 score of 0.639. This means that \tool outperforms \EulerC by 71.4\% in terms of F1 score;  \tool not only produces fewer incorrect, missing steps but also identifies more \rev{accurate} missing steps.

We also analyzed if the missing steps provided by \tool are different compared to \EulerC, presented in Figure \ref{fig:ms-venn}. 

Both \EulerC and \tool successfully identified 52 correct missing steps, which represents 33\% of the missing steps in the ground truth. These steps are primarily the setup steps commonly performed in the apps; however, reporters often forgo describing such steps in bug reports. For example, nine bug reports \rev{from the Gnucash app \cite{gnucashApp} require \textit{"click the next button"} as the initial setup steps.} \rev{In addition,} both approaches produced one unnecessary missing step because of app traversal in an incorrect path. Moreover, both \tool and \EulerC failed to identify 29 crucial steps that reporters often ignore. These types of steps include: (i) actions in a popup dialog (\eg\ \textit{"Click OK button"}), and (ii) implicit steps that are easy to understand for humans but challenging for an automated tool to infer. \rev{For example, the S2R, \textit{"Click on existing transaction"} from Gnucash's bug report \#699~\cite{gnucash699}  requires the existence of a transaction in the app before clicking on the  GUI component corresponding to the transaction, but that S2R is not explicitly described in the bug report.}

\EulerC identified 28 ground truth missing steps that \tool failed to detect. This occurred for two main reasons. First, \tool \rev{ would sometimes perform incorrect mapping between S2Rs and GUI interactions due to choosing an alternate path to reach from the current screen to a mapped screen, thus skipping crucial steps.} For example, the S2R \textit{"Navigate to Service Intervals screen"} from android-mileage's bug report \#65~\cite{mileage65} was incorrectly mapped \rev{to} \textit{"click the ok button"}, which lead to omitting the necessary step \textit{"open the app menu"}. Second, \EulerC identifies more initial setup steps required on a screen to proceed from the current GUI screen to the next due to its random exploration; however, \tool often ignores such steps, as it identifies the minimal steps necessary to perform on the current screen. To resolve this problem, future versions of \tool may incorporate knowledge of specific app functionalities \rev{to suggest missing} steps in bug reports. 

\rev{Across 21 bug reports, \EulerC produces 190 unnecessary steps that are not detected by \tool, whereas \tool only produces 56 such steps, excluding the ones detected by \EulerC.} 
Although \EulerC prioritizes recall over precision to ensure the presence of necessary missing steps from which reporters can choose, the large number of missing steps may confuse developers when reproducing the bug.
The reason for \rev{the large number of unnecessary steps} is attributed to the incorrect S2R interaction matches in the GUIs. However, \EulerC's random exploration strategy in the graph exacerbates this problem by producing an excessive number of unnecessary steps.

\tool correctly identified 49 missing steps in the ground truth test dataset that \EulerC failed to detect. There are primarily two reasons: (i) \EulerC includes \textit{"Open App"} step only if explicitly described in the bug report, yet only two out of 21 bug reports contain this step---\tool includes this step in all \rev{quality} reports; and (ii) \EulerC's graph prevents the identification of GUI information for navigation components such as the navigation drawer and input widgets such as the spinner. \rev{For example, in Gnucash's bug report \#615~\cite{gnucash615}, \EulerC identifies the step \textit{"Tap the `Navigation drawer opened' image button"} but fails to identify the interaction for the \textit{"Manage Books"} that becomes visible when the navigation drawer is opened.} The graph used in \tool correctly identifies such interaction, exhibiting its ability to handle complicated GUI structures. 
\looseness=-1
