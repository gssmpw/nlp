% !TeX root = 0_main

\section{\tool's Prompt Development and Evaluation}
\label{sec:prompt_development}

This section describes how we developed and evaluated the LLM prompts for three distinct tasks: (i) S2R sentence identification, (ii) individual S2R extraction, and (iii) individual S2R mapping to app interactions. 
We adopted a rigorous, comprehensive, and data-driven approach in which we designed an initial prompt that was iteratively evaluated and refined into new prompts. 
Prompt development and evaluation followed a quantitative and qualitative methodology based on a set of Android app bug reports. 
Overall, we designed and evaluated 12 prompt templates across all three tasks.
\rev{To generate GPT-4 responses with the prompts for all tasks, we used a temperature of 0 to minimize randomness/non-determinism in the responses.}

\subsection{Development Dataset Construction}
\label{sec:dev_dataset}

We constructed a dataset of 54 bug reports and corresponding ground truth data, with manually identified S2R sentences, individual S2Rs, and interactions mapped to each S2R.

\subsubsection{Bug Report Collection}

We selected the 54 bug reports from the dataset released by Saha \etal~\cite{saha2024toward}, which contains reproducible mobile app bug reports from the AndroR2 dataset~\cite{wendland2021,Johnson2022}. 
These reports describe bugs for 31 Android apps of various domains (\eg web browsing, WiFi network diagnosis, and finance tracking). 
The reported bugs span different bug types, namely crashes (15 reports), output problems~(19), UI cosmetic issues (13), and navigation problems (7). 

\subsubsection{S2R Sentence Labeling}
\label{sec:identification_data_dev}

Two authors annotated the 1,031 sentences present in the bug reports as either S2R or non-S2R, following the S2R criteria and methodology defined by Chaparro \etal~\cite{Chaparro2017}. 
One author annotated each sentence, while the second author validated the annotations, recording disagreements and their rationale. 
The authors agreed on the annotations for 1,002 sentences (97.2\%, 0.91 Cohen's kappa~\cite{Cohen}), which represents near-perfect agreement. 
Disagreements were resolved via discussion. 
The most common reasons for disagreements were content misinterpretations and mistakes (\eg\ a sentence describing the observed behavior, not S2Rs). 
In total, the 54 bug reports contain 189 S2R sentences (3.5 per report on average), while the remaining 842 sentences describe non-S2R content.
\looseness=-1

\subsubsection{Individual S2Rs Extraction}
\label{sec:extraction_data_dev}
Two authors manually inspected the 189 S2R sentences to extract individual S2Rs (phrases describing a single interaction with the app). 
One author read and extracted the individual S2Rs in the format defined in \Cref{sec:indiv-s2rs-approach}. 
The extracted S2Rs were validated by a second author. 
They discussed disagreements to reach a consensus where needed. 
From the 189 S2R sentences, we extracted 246 individual S2Rs with an agreement rate of 97.6\%. 

\subsubsection{S2Rs to GUI Interaction Mapping}
\label{sec:qa_data_dev}

To create ground truth mappings between individual S2Rs and GUI app interactions, we first built the execution models (\ie graphs) for the 31 apps corresponding to the bug reports. 
To do so, we executed the \CrashScope tool~\cite{Moran2016} using the corresponding APKs (from the original dataset~\cite{saha2024toward,Johnson2022}) and a Pixel 2 Android emulator. 
We also used the manual interaction traces collected as part of Saha \etal's dataset~\cite{saha2024toward}. Both the \CrashScope and manual interaction traces consist of GUI-event execution traces and (video) screen captures showing the executed interactions. 
We used Song \etal's toolkit~\cite{song2022burt} to parse the traces and build the execution graphs.

Two authors manually inspected the execution data, graphs, and reproduction screen captures to map each S2R to graph nodes and interactions. 
One author first inspected this data to identify the GUI screen and target GUI component for each S2R. 
Then, the author identified the graph node corresponding to such screen, and within it, the interaction corresponding to the S2R. 
In the process, missing steps and the path that represented a minimal bug reproduction scenario were identified. 
A second author followed the same procedure to verify the interactions/nodes mapped to the S2Rs and the reproduction paths identified by the first author. 
Both authors discussed any disagreements, involving a third author where necessary.  
\looseness=-1

We applied the above methodology on a sample of 10 bug reports, in such a way that they spanned different bugs types, 
apps of different domains (9 apps), and S2R types (taps, types, \etc). 
The two authors created the ground truth for 46 individual S2Rs among 49 individual S2Rs for the 10 bug reports, agreeing on 43 S2Rs (agreement rate of 93.5\%). The excluded three individual S2Rs did not have corresponding app interactions in the execution model because they are performed outside the app (\eg\ \textit{"install the app"}), and hence, are not included in the graph.
Common reasons for disagreements were unclear individual S2Rs and misinterpretation of graph nodes/interactions. 
During the data creation process, we realized that it would take the two authors a prohibitive amount of effort to create the data for the remaining 44 bug reports. 
Therefore, we decided to focus on the S2R mapping prompt development using only the 10 bug reports and redirect our effort to curating the test data used for \tool's evaluation (see \Cref{sec:empirical_evaluation}). 

\subsection{Prompt Development Methodology}
\label{sec:prompt_development_methodology}

For each of three tasks where \tool uses GPT-4, our overall data-driven methodology used three prompting strategies, commonly used in software engineering research~\cite{hou2023large}: 
\begin{itemize}
	\item Zero Shot (ZS) prompting: starting from a base prompt template that includes the task description, input, and response format, we iteratively executed, evaluated, and refined the template until the performance plateaued. This involved computing performance metrics (precision, recall, and F1 score) against the ground truth, qualitatively analyzing false positives (FP) and negatives (FN), and adjusting the prompt to address those cases. For example, as S2R sentence identification is a classification task, two authors investigated the FP and FN of the GPT-4 responses to derive the classification criteria (\Cref{fig:prompt-structure}a) to better guide GPT-4 in the S2R sentence classification task.
    \rev{This process resulted in four versions of each type of prompt template. To determine if performance plateaued, we monitored the F1 score. For example, from version 3 to version 4 the F1 score decreased by 0.001 for the S2R identification task. Based on this minimal change, we selected version 3 as the optimal prompt for this phase.}
	\item Few Shot (FS) prompting: starting from the obtained ZS template, we created a base FS template containing positive and negative examples selected from the remaining bugs of Saha \etal's dataset~\cite{saha2024toward} and the expected output. The example bug reports are representative of each task and selected based on certain criteria, \eg\ various bug types (crash, output, \etc), and bug reports with different wordings and structures. We iteratively executed, evaluated, and refined the template until the performance no longer improved, in the same way we did it in ZS prompting.
	\item Chain of Thought (CoT) prompting: starting from the obtained FS template, we created a base CoT template containing explanations for \rev{the outcome of the positive and negative examples. The explanation for the outcome was designed by two authors after discussion and reaching a consensus.} We iteratively executed, evaluated, and refined the template until the performance plateaued, in the same way we did it in ZS and FS prompting.
\end{itemize}


\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/prompt.pdf}
	\caption{Structure of the Developed Prompts}
	\label{fig:prompt-structure}
\end{figure}

This methodology resulted in three prompt templates (one from each prompting strategy) for S2R identification and three templates for individual S2R extraction. 
For S2R mapping, since we defined mapping as a 2-step task, we designed two prompts for each strategy, resulting in six prompts. 
The 2-step task consisted of first asking GPT-4 to return a yes/no answer on whether an individual S2R maps to the interactions of a given screen and if the answer is yes, asking GPT-4 to return the list of interactions that the S2R maps to. 
Our tests revealed that this approach led to less noisy answers from GPT-4, compared to executing only the second step. 
In total, we developed 12 prompt templates. To help visualize our prompt templates, \Cref{fig:prompt-structure} illustrates the various components associated with the prompts for each task---our detailed templates are found in our replication package~\cite{package,doi}.

\subsection{Prompt Evaluation Results}
\label{sec:development_results}

We evaluated the prompt templates for S2R identification and extraction in terms of precision, recall, and F1 score, by executing these two phases in isolation. 
The F1 score was used to rank the templates. 
The S2R mapping prompt templates were evaluated by executing \tool's S2R quality assessment phase and evaluating the resulting S2R-interaction mappings. 
Since S2R mapping is a 2-step task, we evaluated each of the prompts based on the \# and \% of \textit{hits}, defined as follows.
For the first prompt, it is the number (and proportion) of correct predictions for the presence or absence of an S2R-interaction mapping in a given screen (out of the total number of predictions). 
For the second prompt, it is the number (and proportion) of correctly identified interactions for each individual S2R (out of the total number of individual S2Rs).


\begin{table}[t!]
	\centering
	\caption{Prompt Template Performance for S2R Identification}
	\label{tab:identification_results_dev_set}
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{c|c|c|c|c|c|c}
			\hline
			\textbf{Template} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{\#TP} & \textbf{\#FP} & \textbf{\#FN} \\ \hline
			ZS & 0.929              & 0.968           & 0.948       & 183           & 14            & 6             \\ \hline
			FS   & 0.897              & 0.963           & 0.929       & 182           & 21            & 7             \\ \hline
			CoT  & 0.915              & 0.963           & 0.938       & 182           & 17            & 7             \\ \hline
		\end{tabular}%
	}
\end{table}

\begin{table}[t!]
	\centering
	\caption{Prompt Template Performance for S2R Extraction}
	\label{tab:indiv-s2r-study-results}
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{c|c|c|c|c|c|c}
			\hline
			\textbf{Template} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{\#TP} & \textbf{\#FP} & \textbf{\#FN} \\ \hline
			ZS              & 0.918              & 0.951           & 0.934       & 234            & 21             & 12             \\ \hline
			FS              & 0.897              & 0.951           & 0.923       & 234            & 27             & 12             \\ \hline
			CoT             & 0.810              & 0.951           & 0.875       & 234            & 55             & 12             \\ \hline
		\end{tabular}%
	}
	
\end{table}

\begin{table}[t!]
	\centering
	\caption{Prompt Performance for S2R-Interaction Mapping}
	\label{tab:indiv-s2r-matching-results}
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{c|ccc|cc}
			\hline
			\multirow{2}{*}{\textbf{Template}} & \multicolumn{3}{c|}{\textbf{1st-step template}}                                                             & \multicolumn{2}{c}{\textbf{2nd-step template}}          \\ \cline{2-6} 
			& \multicolumn{1}{c|}{\textbf{\# Predictions}} & \multicolumn{1}{c|}{\textbf{\# Hits}} & \textbf{Hit Rate} & \multicolumn{1}{c|}{\textbf{\# Hits}} & \textbf{Hit Rate} \\ \hline
			{ZS}                      & \multicolumn{1}{c|}{939}                    & \multicolumn{1}{c|}{887}              & 94.5\%            & \multicolumn{1}{c|}{30}               & 76.9\%            \\ \hline
			{FS}                      & \multicolumn{1}{c|}{970}                    & \multicolumn{1}{c|}{912}              & 94.0\%            & \multicolumn{1}{c|}{26}               & 66.7\%            \\ \hline
			{CoT}                     & \multicolumn{1}{c|}{1214}                   & \multicolumn{1}{c|}{1152}             & 94.9\%            & \multicolumn{1}{c|}{18}               & 46.2\%            \\ \hline
		\end{tabular}%
	}

\end{table}

\Cref{tab:identification_results_dev_set,tab:indiv-s2r-study-results,tab:indiv-s2r-matching-results} show the performance of the designed prompt templates for the three tasks: S2R identification, individual S2R extraction, and S2R mapping. 
Among the three templates for the S2R identification task, \textit{ZS} achieved the best performance across the three metrics having the lowest \# of FP (14) and FN~(6). Likewise, for the individual S2R extraction task, the \textit{ZS} template achieved the highest precision (0.918) with the lowest \# of FP (21), sharing the same \# of FN (12) with the other two prompts. Regarding the S2R mapping task, \tool with all three templates for the 1st-step prompt achieved a similar hit rate (94.0\% to 94.9\%) and with \textit{ZS} template for the 2nd-step prompt achieved the best hit rate of 76.9\%. 

Interestingly, although prior research has shown the superiority of \textit{CoT} prompts over \textit{ZS} and \textit{FS} prompts~\cite{hou2023large,Feng2024}, this is not the case for our tasks. Via qualitative analysis of GPT-4 responses, we observed that GPT-4 with \textit{FS} and \textit{CoT} prompts tends to include more unintended text in the responses compared to \textit{ZS} prompt which results in more false positives, \eg\ \textit{CoT} template for S2R extraction generated 55 FPs while \textit{ZS} template generated 21 FPs only. We conjecture that the long and complicated input (\eg\ bug reports can be long, and interaction information can be complicated) made the task difficult for GPT-4. Moreover, having three or four examples with reasoning made the prompts even longer.

As for all three tasks, \textit{ZS} templates outperformed the other two, we utilized the \textit{ZS} templates for implementing \tool.
