% !TeX root = 0_main

\section{\tool's Evaluation Design}
\label{sec:empirical_evaluation}
\tool's evaluation has two main goals: (i) to evaluate \tool's ability to provide correct quality annotations for real bug reports, 
and (ii) to examine how well \tool can infer missing S2R information in bug reports. 
We apply \tool to a test dataset (see \Cref{sec:test_dataset}) comprising 21 bug reports, in order to provide a comparison with prior work. We aim to answer the following research questions (RQs):
\begin{itemize}
	\item \textbf{RQ$_{1}$:} How effective is \tool in generating correct S2R quality annotations?
	\item \textbf{RQ$_{2}$:} How accurately can \tool infer missing S2Rs?
\end{itemize}

\subsection{Evaluation Dataset}
\label{sec:test_dataset}

We used the bug reports (\ie\ \textit{test set}) used by Chaparro \etal~\cite{Chaparro2019}, which allow us to provide a direct comparison with their approach, \EulerC. 
This dataset contains 24 bug reports \rev{of various kinds ( crashes, UI problems, and navigation problems)} from six Android applications \rev{of different domains (web browsing, WiFi network diagnosis, finance tracking, \etc). The diverse evaluation set, separate from the development set, enabled us to assess the generalizability of the developed prompts across different bug reports.}  
We discarded three bug reports, as follows: (1) two bug reports~\cite{aard81, aard104} from the Aard Dictionary App~\cite{aardapp}, because the app version 1.4.1 is unable to load its dictionary database, and (2) one bug from Time Tracker app~\cite{atimetracker1}, because we could not generate the execution model for this app as the bug report requires a rotation action which \tool does not support.
Hence, our test set contains 21 bug reports from the original \EulerC dataset. 

Since this dataset does not contain any ground truth information for evaluating \tool, we constructed the ground truth manually.  We used the same methodology discussed in \Cref{sec:identification_data_dev,sec:extraction_data_dev} to do so for identifying S2R sentences and extracting individual S2Rs.

To construct the quality assessment ground truth, the first two authors mapped the extracted individual S2Rs to \rev{GUI} interactions manually following the methodology discussed in \Cref{sec:qa_data_dev}. App execution models for the bug reports were built by parsing execution traces collected via \CrashScope's app exploration and manual app \rev{usage}.  
One author identified the reproduction interactions on the generated data and mapped such interactions with the extracted individual S2Rs from the bug report. 
They collected the mapped interactions for each individual S2R, as well as the interactions that are required to reproduce the bug, but not reported in the bug report, \ie ground truth for missing steps. 
Each individual S2R was mapped with one or more interactions in the execution model path, as needed. Using the mapped interactions and the quality assessment model (discussed in \Cref{sec:quality_model}), they assigned quality labels to each individual S2R. 
A second author performed the same steps and validated the interactions in the reproduction scenario as well as the quality annotations.  Disagreements were resolved via discussion.

In summary, we identified 73 S2R sentences out of the 275 sentences present in the 21 bug reports with a near-perfect agreement between the two authors (98.2\% agreement rate and 0.88 Cohen's kappa \cite{Cohen}). 
From the 73 S2R sentences, we extracted 82 individual S2Rs with an agreement rate of 93.9\% between the two authors.
We discarded four individual S2Rs as they represent rotation operation and the current version of \tool does not support this operation. 
We assigned the remaining 78 individual S2Rs quality annotations (\ie\ 70 S2Rs as CS, 7 S2Rs as AS, 1 S2R as VM, and 38 S2Rs as MS). 
We identified 158 missing interactions, \ie\ missing steps for the 38 MS positions (\ie S2Rs with filled-in missing interactions). 
For constructing the annotations ground truth, the two authors agreed on 90\% of the cases. Cohen's kappa for individual S2R extraction and mapping is inapplicable since the labeling is not based on a discrete set of labels. 

\subsection{Baseline Approach}

We considered \EulerC~\cite{Chaparro2019} as the baseline approach, which also aims to assess the quality of S2Rs in a bug report. 
It identifies the S2R sentences from a bug report using deep learning techniques (\eg\ CNN~\cite{o2015introduction}, Bi-LSTM~\cite{zhou2016attention}). 
It identifies individual S2Rs via analysis of discourse patterns and assigns quality annotations by employing keyword-based mapping to app UI information.  
\rev{\EulerC and \tool generate similar quality reports, therefore we can directly compare the \tool reports to the original \EulerC reports  provided by \EulerC's replication package \cite{Chaparro2019}, 
to answer the RQs.}
\looseness=-1

\subsection{Evaluation Methodology} 

We executed \tool with the 21 bug reports on the test set,  producing the quality report for each bug report, including the quality annotations and missing steps. To answer \textbf{RQ$_{1}$}, we compared the \tool assigned quality annotations with the ground truth quality annotations. To answer \textbf{RQ$_{2}$}, we evaluated the generated missing steps by \tool against the ground truth missing steps. For both RQs, we computed precision, recall, and F1 score. We applied the same process for \EulerC and qualitatively analyzed the false positives (FP) and negatives (FN) to understand the limitations of both approaches.