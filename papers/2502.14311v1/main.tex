%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{amsmath,amssymb,amsfonts}  % For math environments and symbols
\usepackage{enumitem}
% \usepackage{natbib}
\usepackage{booktabs, multirow}
\usepackage{mathtools}
\usepackage{changepage} % in the preamble
\usepackage[margin=1in]{geometry} % Adjust page margins if needed
\newcommand{\emphasize}[1]{``#1''}
\usepackage{xcolor}

% \newcommand{\hashimoto}[1]{\textcolor{red}{#1}}
% \newcommand{\takehiro}[1]{\textcolor{blue}{#1}}

\newcommand{\hashimoto}[1]{\textcolor{black}{#1}}
\newcommand{\takehiro}[1]{\textcolor{black}{#1}}

\newcommand{\headerforaversion}[1]{\multicolumn{2}{l}{\hspace{-2.5ex}#1\hspace{2.5ex}}}
% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}



% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
% \newtheorem{theorem}{Theorem}


% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{The Impact and Feasibility of Self-Confidence Shaping \\
for AI-Assisted Decision-Making}


% Single author syntax
% \author{
% Anonymous
%     % Author Name
%     % \affiliations
%     % Affiliation
%     % \emails
%     % email@example.com
% }

% % Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Takehiro Takayanagi$^1$
\and
Ryuji Hashimoto$^1$\and
Chung-Chi Chen$^2$\And
Kiyoshi Izumi$^1$\\
\affiliations
$^1$The University of Tokyo\\
$^2$National Institute of Advanced Industrial Science and Technology\\
\emails
\{takayanagi-takehiro590, hashimoto-ryuji419\}@g.ecc.u-tokyo.ac.jp,
c.c.chen@acm.org,
izumi@sys.t.u-tokyo.ac.jp
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
In AI-assisted decision-making, it is crucial but challenging for humans to appropriately rely on AI, especially in high-stakes domains such as finance and healthcare.
This paper addresses this problem from a human-centered perspective by presenting an intervention for \textit{self-confidence shaping}, designed to calibrate self-confidence at a targeted level.
We first demonstrate the impact of self-confidence shaping by quantifying the upper-bound improvement in human-AI team performance. Our behavioral experiments with 121 participants show that self-confidence shaping can improve human-AI team performance by nearly 50\% by mitigating both over- and under-reliance on AI.\footnote{The experimental protocol was approved by our organization’s ethical board}
We then introduce a self-confidence prediction task to identify when our intervention is needed. Our results show that simple machine-learning models achieve 67\% accuracy in predicting self-confidence.
We further illustrate the feasibility of such interventions.
The observed relationship between sentiment and self-confidence suggests that modifying sentiment could be a viable strategy for shaping self-confidence.
Finally, we outline future research directions to support the deployment of self-confidence shaping in a real-world scenario for effective human-AI collaboration.
% In AI-assisted decision-making, it is crucial but challenging for humans to appropriately rely on AI, especially in high-stakes domains such as finance and healthcare.
% This paper addresses this problem from a human-centered perspective by presenting an intervention for \textit{self-confidence shaping}.
% We first demonstrate the impact of self-confidence shaping by quantifying the upper-bound improvement in human-AI team performance. Our behavioral experiments with 121 participants show that, under the ideal assumption that we can perfectly manage human self-confidence, self-confidence shaping can improve human-AI team performance by nearly 50\% by mitigating both over- and under-reliance on AI recommendations.\footnote{The experimental protocol was approved by our organization’s ethical board}
% We further illustrate the feasibility of such interventions. Achieving 67\% accuracy in predicting self-confidence indicates that human self-confidence is neither random nor unpredictable, and the observed relationship between sentiment and self-confidence suggests that modifying sentiment could be a viable strategy for shaping self-confidence.
% Finally, we outline future research directions to support the deployment of self-confidence shaping in a real-world scenario for more effective human-AI collaboration.
\end{abstract}

\section{Introduction}
% (hashimoto summary) mixing human and AI's decision-making 

Artificial Intelligence (AI) technologies have become ubiquitous and are adopted across diverse areas of society, leading to an increasing reliance on AI recommendations in decision-making. Yet, in high-stakes domains such as finance and healthcare, where safety, ethical, and legal obligations remain paramount, full automation can be problematic, while purely manual approaches risk inefficiency and inaccuracy. Consequently, AI-assisted decision-making has emerged as a promising paradigm, which augments human judgment with AI recommendations~\cite{lai2019human,towards_science,ijcai2024p868}. 


A central challenge in this paradigm is ensuring appropriate reliance on AI~\cite{schemmer2023appropriate}. Over-reliance can degrade outcomes if the model or data are flawed, while under-reliance means ignoring beneficial advice. To address this, prior research has primarily focused on \textit{AI-centric interventions}, such as providing confidence scores, or explanations for AI predictions~\cite{yin2019understanding,effect_of_confidence,bansal2019beyond}.

\takehiro{Beyond AI-centric interventions, prior studies emphasize that human self-confidence is key to proper reliance.
However, human self-confidence is often treated as a fixed factor~\cite{mahmood2024designing,chong2022human}. This perspective overlooks the potential for a \textit{human-centric intervention} to shape human self-confidence and mitigate both over- and under-reliance. Consequently, this represents a largely underexplored avenue for improving AI-assisted decision-making.}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/financial_decision.pdf}
\caption{%
    Conceptual illustration of the two-round procedure:
    \textbf{1\textsuperscript{st} round}, independent judgment; 
    \textbf{2\textsuperscript{nd} round}, final team decision with AI recommendation.
}
    \label{fig:task_illustration}
    \vspace{-5mm}
\end{figure}


In this paper, we take an initial step toward enhancing AI-assisted decision-making through \textit{self-confidence shaping}.\footnote{We define self-confidence shaping as the deliberate intervention to shape a decision-maker’s confidence at a targeted level. This differs from confidence calibration, which aligns a user’s confidence with their actual performance~\cite{moore2020perfectly}.} First, we quantify the potential gains in human-AI team performance under ideal conditions where self-confidence is perfectly controlled. Next, we introduce a self-confidence prediction task to determine when our intervention is needed, assessing how accurately a decision-maker's (DM) self-confidence can be predicted based on user traits and task characteristics. Finally, we explore the feasibility of this intervention by investigating the relationship between task features and self-confidence. In particular, drawing on decision science research that demonstrates how subtle shifts in sentiment in a text can significantly influence self-confidence~\cite{griffin1992weighing}, we examine how modifying sentiment may serve as an initial step toward effective self-confidence shaping.
% In this paper, we take an initial step towards enhancing AI-assisted decision-making by \textit{self-confidence shaping}.\footnote{We define \textit{self-confidence shaping} as the deliberate intervention to shape a decision-maker’s confidence at a targeted level. This differs from confidence calibration, which aligns a user’s confidence with their actual performance~\cite{moore2020perfectly}.} First, we quantify the potential gains in human-AI team performance under ideal conditions where self-confidence is perfectly controlled. Next, we explore the feasibility of this intervention by addressing two key challenges: (1) predicting a user’s self-confidence, and (2) modifying text to influence self-confidence in decision-making. For the first challenge, we assess how accurately a decision-maker's (DM) self-confidence can be predicted based on user traits and task characteristics. For the second challenge, we draw on prior research in decision science, which suggests that subtle shifts in sentiment in a text can significantly influence self-confidence~\cite{griffin1992weighing}, and focus on how modifying sentiment affects self-confidence as an initial step toward self-confidence shaping.


    
To summarize, we explore the following questions.
\begin{itemize}
    \item \textbf{RQ1:} Under perfect control over self-confidence, how much can human-AI team performance improve through self-confidence shaping?
    \item \textbf{RQ2:} To what extent can we predict self-confidence in decision-making?
    \item \textbf{RQ3:} Is the sentiment significantly associated with self-confidence?
\end{itemize}


\takehiro{To address these questions, we developed an AI-assisted investment decision-making task in a financial context, recruited 121 crowd workers, and collected 2,420 decision instances. Adopting the Judge-Advisor System (JAS) for decision-making~\cite{sniezek1995cueing}, our experiments consist of two rounds: (1) participants first form an independent prediction, and (2) they make the final decision after considering the AI recommendation, as illustrated in Figure~\ref{fig:task_illustration}.
}
% Figure~\ref{fig:task_illustration} illustrates an example of decision-making tasks where participants first form an independent prediction of whether a stock would rise or fall, then review an AI recommendation before finalizing their decision.

Our results show that successful self-confidence shaping can significantly improve human-AI team performance. In particular, our scenario analyses reveal that shaping self-confidence can reduce both over- and under-reliance on AI recommendations, improving team performance by up to nearly 50\%. Moreover, our findings suggest that such intervention is indeed feasible: simple machine learning models that incorporate user traits and task characteristics can predict whether self-confidence is high or low with about 67\% accuracy. We also observe that the sentiment in a text is significantly associated with self-confidence.

\section{Related Work}
\subsection{Appropriate Reliance in AI-assisted Decision-Making}
A challenge for improving human-AI team performance in AI-assisted decision-making is ensuring humans’ appropriate reliance on AI, given the uncertainties of both humans and AI~\cite{schemmer2023appropriate,does_the,buccinca2021trust,towards_science}.
Over-reliance on AI can degrade human-AI decision-making if the model is flawed or data is unreliable. Conversely, under-reliance occurs when humans ignore correct AI advice, missing opportunities to improve performance.

To encourage appropriate reliance, prior research has primarily focused on AI-centric design interventions~\cite{effect_of_confidence}. In particular, displaying AI confidence levels has been proposed to help users gauge the likelihood of correct predictions~\cite{effect_of_confidence,vodrahalli2022uncalibrated,who_should}. Additionally, elucidating the rationale behind AI outputs through explanations remains a widely studied approach to foster appropriate reliance~\cite{effect_of_confidence,does_the,Are_explanations,li2024utilizing}.
Nonetheless, these AI-centric interventions show mixed results, indicating that they do not always lead to improved human-AI collaboration~\cite{effect_of_confidence,who_should,visual_uncertainty}.

As a result, there has been a growing focus on human self-confidence, which constitutes another key element of appropriate reliance~\cite{chong2022human,vodrahalli2022humans,are_you_really,mahmood2024designing}. Prior studies highlight the pivotal role of human DMs' self-confidence in the adoption of AI recommendations~\cite{chong2022human}.
However, self-confidence is often treated as a fixed factor, overlooking how it can be shaped to mitigate over- and under-reliance. 
\takehiro{Therefore, in this work, we propose that self-confidence shaping could serve as a viable method for enhancing human-AI team performance. }
% Specifically, we quantify the potential gains from shaping confidence and demonstrate the feasibility of doing so.

% Therefore, in this work, we present an initial exploration of how shaping self-confidence can enhance human-AI team performance.

% The Relationship Between Human Decision-Making Confidence and AI Advice Adoption


\subsection{AI-Assisted Decision-Making in Financial Context}
\takehiro{Finance is a significant domain for AI-assisted decision-making, as evidenced by the substantial body of literature on the topic~\cite{hase-bansal-2020-evaluating,effect_of_confidence,li2024utilizing,vodrahalli2022uncalibrated}.
Its high-stakes nature, where mistakes can lead to significant losses~\cite{towards_science,biran2017human}, combined with the frequent overconfidence of DMs~\cite{barber2001boys,grevzo2021overconfidence}, makes it an ideal setting for our study on self-confidence in AI-assisted decision-making.}

\takehiro{Yet, many commonly studied financial tasks in AI-assisted decision-making, such as income prediction, fail to capture the truly high-stakes nature of finance~\cite{towards_science,li2024utilizing}. Few studies address investment decisions like stock movement prediction, which can incur substantial losses~\cite{biran2017human}, and they typically rely solely on numerical data, overlooking textual information such as corporate performance descriptions that are widely used in real-world investment scenarios~\cite{takayanagi_naacl}. Therefore, we develop a text-based stock movement prediction dataset for AI-assisted decision-making.}

\section{AI-Assisted \takehiro{Decision-Making}}
\subsection{Collaborative Decision-Making}
In this work, we study the setting of \emph{AI-assisted decision-making}. Let $\mathcal{X}$ denote the space of decision-making tasks. Assume a binary classification setting. We consider a dataset $D=\{x_i,y_i\}_{i=1}^N$, where $x_i\in\mathcal{X}$ denote the features of task $i$, and $y_i\in\{-1,+1\}$ denote the target label of task \( i \), respectively. \(N\) denotes the number of instances in the dataset.

\paragraph{Human Decision-Maker (DM)}
A human DM provides an independent judgment function \hashimoto{$h:\mathcal{X}\rightarrow\{-1,+1\}$}:
\[
  y^h \;=\; h\bigl(x;\,\theta_h\bigr),
\]
where \(y^h \in \{-1, +1\}\) is the human’s independent decision for task \( x \), and $\theta_{h}$ represents the parameters governing the human decision function.

\paragraph{AI Model}
An AI model $m:\mathcal{X}\rightarrow\{-1,+1\}$ offers a decision recommendation:
\[
  y^m \;=\; m\bigl(x;\,\theta_m\bigr),
\]
where \(y^m \in \{-1, +1\}\) is the model’s predicted label for task \( x \), and $\theta_{m}$ represents the parameters governing the AI model.

\paragraph{Human-AI Team Decision}
The human makes the final decision by incorporating their own judgment and the AI recommendation using the human-AI team decision-making function $f:\mathcal{X}\times\{-1,+1\}\times\{-1,+1\} \rightarrow \{-1,+1\}$:
\[
  d \;=\; f\bigl(x,\, m(x;\theta_m),\, h(x;\theta_h)\bigr),
\]
where \( f(\cdot) \) represents the human-AI team decision-making function to determine the final decision \(d \in \{-1, +1\}\) for task \( x \).
% Our focus is on scenarios where the human \emph{always} retains final decision-making authority, a typical setup in high-stakes contexts such as finance or medical diagnosis.

\paragraph{Human-AI Team Performance}
Given a dataset
\(
D = \{ (x_i, y_i) \mid 1 \le i \le N \},
\)
the goal in human-AI collaboration is to achieve a strong human-AI team performance, for example by maximizing accuracy. We can express this metric as:
 \[
\frac{1}{N} \sum_{(x_i, y_i) \in D} \mathbf{1}\Bigl\{ f\bigl(x_i,\, m(x_i;\theta_m),\, h(x_i;\theta_h)\bigr) = y_i \Bigr\},
\]
where the indicator function \(\mathbf{1}\{ \cdot \}\) returns 1 if the final decision equals \(y_i\) and 0 otherwise.

To achieve high human-AI team performance, it is essential to capture the human-AI team decision-making model \(f(\cdot)\) in a way that accurately represents the human DM’s thought process. Recent work in human-AI collaboration has increasingly emphasized human confidence when examining how humans incorporate AI recommendations into their final decisions~\cite{chong2022human,gemalmaz2022understanding,schemmer2023appropriate,wang2018does,mahmood2024designing}.
For instance, a threshold-based collaborative decision-making model is described as follows~\cite{mahmood2024designing}:
\[
f\bigl(x, m(x;\theta_m), h(x;\theta_h)\bigr) 
=
\begin{cases}
   h(x;\theta_h), & \text{if } c > \tau,\\[4pt]
   m(x;\theta_m), & \text{otherwise},
\end{cases}
\]
where $c \in \mathbb{R}$ denotes the human DM’s self-confidence, and $\tau$ is the confidence threshold: if $c \geq \tau$, the human’s decision is adopted; otherwise, the AI’s recommendation is followed.
Since self-confidence $c$ is assumed to be a fixed factor, this model does not allow us to study the process by which human DM forms self-confidence nor does it capture how shaping self-confidence can impact the human-AI team performance.
\paragraph{Self-Confidence}
To account for the self-confidence formation process, we formulate the human-AI collaborative decision-making model using a self-confidence function \(
F_\phi:\mathcal{X}\times\Theta_h\rightarrow\mathbb{R}
\):\vspace{0.5em}
{
\resizebox{\columnwidth}{!}{$
\begin{aligned}
f\bigl(x_i,\, m_i,\, h_i\bigr)=\begin{cases}
h_i, & \text{if } c_i \ge \tau,\\[1mm]
m_i, & \text{otherwise,}
\end{cases} \quad \text{with } c_i = F_\phi\bigl(x_i,\theta_h\bigr)
\end{aligned}
$}\vspace{0.5em}
}
where {\small \(f\bigl(x_i,\, m(x_i;\theta_m),\, h(x_i;\theta_h)\bigr)\)} represents the human-AI team decision for instance \(i\), \(m_i\) and \(h_i\) denote the outputs of the AI model and the human DM for instance \(i\), and \(c_i\) is the associated self-confidence. While \(f(\cdot)\) follows a threshold-based collaborative decision-making model, we integrate the self-confidence formation process by introducing a self-confidence function $F_\phi$ where \(\Theta_h\) represents the human DM's trait space.

Modeling self-confidence enables us to study self-confidence shaping. We first examine the potential of self-confidence shaping for improved human-AI team performance under the assumption of perfect control over \(c_i\) (\textbf{RQ1}). Then, we assess whether \(c_i\) can be predicted from task characteristics \(x_i\) and user traits \(\theta_h\) (\textbf{RQ2}). Finally, we consider modifying task feature \(x_i\) as an intervention to influence \(c_i\) (\textbf{RQ3}).
% we now introduce an optimization problem that aims to maximize human-AI team performance by shaping the self-confidence. In a simple binary accuracy setting, the optimization is formulated as follows:\vspace{0.5em}

% {
% \resizebox{0.9\columnwidth}{!}{$
% \begin{aligned}
% \arg\max_\phi\frac{1}{N}\sum_{(x_i,y_i)\in D}\mathbf{1}\Bigl\{ f\bigl(x_i,\, m(x_i;\theta_m),\, h(x_i;\theta_h)\bigr) = y_i \Bigr\},\\[1mm]
% \text{s.t.} \quad f\bigl(x_i,\, m_i,\, h_i\bigr)=\begin{cases}
% h_i, & \text{if } c_i \ge \tau,\\[1mm]
% m_i, & \text{otherwise,}
% \end{cases} \quad \text{with } c_i = F_\phi\bigl(x_i,\theta_h\bigr)
% \end{aligned}
% $}\vspace{0.5em}
% }
% where {\small \(f\bigl(x_i,\, m(x_i;\theta_m),\, h(x_i;\theta_h)\bigr)\)} represents the human-AI team decision for instance \(i\), and indicator function \(\mathbf{1}\{ \cdot \}\) returns 1 if the final decision equals \(y_i\) and 0 otherwise. Here, \(m_i\) and \(h_i\) denote the outputs of the AI model and the human DM for instance \(i\), and \(c_i\) is the associated self-confidence. While \(f(\cdot)\) follows a threshold-based collaborative decision-making model, we integrate the self-confidence formation process by introducing a self-confidence function 
% \(
% F_\phi:\mathcal{X}\times\Theta_h\rightarrow\mathbb{R},
% \)
% where \(\Theta_h\) represents the human DM's trait space. Self-confidence shaping leverages \(F_\phi\) to adjust \(c_i\) and, in turn, enhance human-AI team performance.

% Modeling self-confidence allows us to study self-confidence shaping. First, we analyze the extent to which human-AI team performance can be improved by addressing these issues, under the assumption that self-confidence \(c_i\) is perfectly controlled (\textbf{RQ1}). Next, we examine the feasibility of such interventions. For a self-confidence shaping intervention to succeed, it is essential to determine when an intervention is needed; thus, we aim to predict the self-confidence \(c_i\) that a human DM will exhibit for a given task. Accordingly, we investigate whether \(c_i\) can be predicted in advance based on task characteristics \(x_i\) and user traits \(\Theta_h\) (\textbf{RQ2}). Moreover, when an intervention is warranted (e.g., due to over-reliance or under-reliance), we seek to understand how confidence shaping can be achieved. We consider modifying task feature \(x_i\) to influence self-confidence, drawing on insights from decision science~\cite{griffin1992weighing}. In particular, we focus on how sentiment, as a component of \(x_i\), is associated with self-confidence (\textbf{RQ3}).





% For RQ3, we investigate the association between sentiment in task \(x_i\) and self-confidence \(c_i\). Drawing on research that human confidence can be influenced by objective information (facts) and the qualitative impressions (sentiments) elicited by the evidence~\cite{griffin1992weighing}, we focus on sentiment in the task feature $x_i$ and see how sentiment $s\in \mathcal{X}$ is associated with the self-confidence $c_i$.

% . Thus, we consider {\small$ F_\phi\bigl(x_i,\theta_h\bigr)=F_{\phi}\bigl(s, f, \theta_h\bigr)$} where the sentiment parameter \( s \in \mathcal{S} \) and the fact parameter \( f \in \mathcal{F} \) are derived from the text features \( x \).
% where \(f\bigl(x_i,\, m(x_i;\theta_m),\, h(x_i;\theta_h)\bigr)\) represents the human-AI team decision for instance \(i\). Here, \(m_i\) and \(h_i\) denote the outputs of the AI model and the human DM on $i$, respectively, and \(c_i\) is a self-confidence on $i$.
% In this formulation, while \(f(\cdot)\) follows a threshold-based collaborative decision-making model~\cite{mahmood2024designing}, to incorporate the self-confidence formation process, we incorporate a model of self-confidence formation by introducing a self-confidence function \(F_\phi:\mathcal{X}\times\Theta_h\rightarrow\mathbb{R}\), where \(\Theta_h\) represents the trait space of the human DM. 
% % In this study, we aim to shape self-confidence $c_i$ by taking self-confidence formation process into consideration to enhance human-AI team performance.




% \subsection{Collaborative Decision-Making}
% In this work, we study the setting of \emph{AI-assisted decision-making}. Let $\mathcal{X}$ denote the space of decision-making tasks. \hashimoto{Assume a binary classification setting. We consider a dataset $D=\{x_i,y_i\}_{i=1}^N$, where $x_i\in\mathcal{X}$ and $y_i\in\{-1,+1\}$ denote input data and target label, respectively. $N\in\mathbb{N}$ is the number of samples. Our optimization task is described as follows in a simple $0-1$ accuracy scenario.}

% \hashimoto{{\small
% \begin{equation}
%     \begin{split}
%     \MoveEqLeft\arg\max_\phi\frac{1}{N}\sum_{(x_i,y_i)\in D}\mathbf{1}\Bigl\{ f\bigl(x_i,\, m(x_i;\theta_m),\, h(x_i;\theta_h)\bigr) = y_i \Bigr\},\\
%     \MoveEqLeft s.t.~~~~ f\bigl(x_i,\, m_i,\, h_i\bigr)=\begin{cases}
%     h_i, & \text{if }c_i>\tau\\
%     m_i, & \text{otherwise}
%     \end{cases}~,~~~c_i=F_\phi\bigl(x_i,\theta_h\bigr)
%     \end{split}\label{Eq:optimization_problem}
% \end{equation}}}
% \hashimoto{Here, $m,h:\mathcal{X}\rightarrow\{-1,1\}$ denote an AI's decision recommendation and human DM (DM)'s independent judgment, respectively. AI's decision depends on its parameter $\theta_m$, whereas the human DM also has their inherent parameter $\theta_h$}\footnote{\hashimoto{What's this?}}\hashimoto{. We also denote the output of $m$ and $h$ given $x_i$ as $m_i,h_i\in\{-1,+1\}$, respectively. $f:\mathcal{X}\times\{-1,+1\}\times\{-1,+1\}$ denotes a human-AI team decision-making model based on both AI and human DM's independent judgment. Our focus is on scenarios where the human DM \emph{always} govern $f$—a typical setup in high-stakes contexts such as financial decision making. In Equation~(\ref{Eq:optimization_problem}), $f$ is described as a threshold-based collaborative decision-making model by \cite{mahmood2024designing}, where $\mathcal{c}_i$ denotes the human DM’s self-confidence on instance~$i$, and $\tau$ is the confidence threshold. If the human DM is confident enough about instance~$i$~($\mathcal{c}_i \geq \tau$), the human DM’s decision is adopted; otherwise, the AI’s recommendation is followed.}

% \hashimoto{In \cite{mahmood2024designing}, self-confidence values are given a priori, it does not capture how self-confidence is formed or how shaping self-confidence could impact team performance. Motivated by this limitation, our formulation (Equation~(\ref{Eq:optimization_problem}))
% incorporates a model of self-confidence formation into the overall decision-making process. We introduce self-confidence funcation $F_\phi:\mathcal{X}\times\Theta_h$, where $\Theta_h$ is human DM's trait space. }

% \hashimoto{In this formulation, we study shaping human DM's self confidence $c_i$ to improve the performance of human-AI collaboration. First, we counterfactually assess how the overall human-AI collaboration accuracy improve by perfectly optimizing $F_\phi$ (\textbf{RQ1}). Next, to test the feasibility of shaping human DM's self confidence, we first try to approximate $c_i\approx\tilde{F}$ (\textbf{RQ2}) and manipulate $c_i$ by modifying text sentiment of $x_i$ (\textbf{RQ3}).}


% %For each task, we assign a binary label $y \in \{-1, +1\}$.
% %\hashimoto{Here, we consider a dataset $D=\{x_i,y_i\},i\in\{1,\ldots,N\}$, where $x_i\in\mathcal{X}$ and $y_i$ denotes the corresponding correct decision.} Formally, consider:
% % In this work, we study the setting of \emph{AI-assisted decision-making}. Let 
% % \(\mathcal{X}\) be the space of decision-making tasks (for example,
% % natural language statements describing each decision-making case),
% % and let \(y \in \{-1,\,+1\}\) denote the \emph{correct} decision
% % for a given task. 
% % \hashimoto{Here, we consider a dataset $D=\{x_i,y_i\},i\in\{1,\ldots,N\}$, where $x_i\in\mathcal{X}$ and $y_i$ denotes the corresponding correct decision.} Formally, consider:

% \paragraph{Human DM (DM)}
% A human DM provides an independent judgment function \hashimoto{$h:\mathcal{X}\rightarrow\{-1,+1\}$}:
% \[
%   y^h \;=\; h\bigl(x;\,\theta_h\bigr),
% \]
% where \(y^h \in \{-1, +1\}\) is the human’s decision for task \(x\).
% \paragraph{AI Model}
% An AI model \hashimoto{$m:\mathcal{X}\rightarrow\{-1,+1\}$} offers a decision recommendation
% \[
%   y^m \;=\; m\bigl(x;\,\theta_m\bigr),
% \]
% where \(y^m \in \{-1, +1\}\) is the model’s predicted outcome for the same task \(x\).

% Given both \(y^h\) and \(y^m\), the \emph{final} team decision \(d\) is made by
% the human DM:
% \begin{equation}
%   \label{eq:teamDecision}
%   d \;=\;
%   f\Bigl(
%     x,\;
%     m(x;\theta_m),\;
%     h(x;\theta_h)
%   \Bigr),\quad
%   d \in \{-1, +1\}.
% \end{equation}
% \hashimoto{where $f:\mathcal{X}\times\{-1,+1\}\times\{-1,+1\}$ denotes a human-AI team decision-making model.} Our focus is on scenarios where the human \emph{always} retains final decision-making authority—a typical setup in high-stakes contexts such as finance or medical diagnosis.
% % \paragraph{Objective.}

% \paragraph{Human-AI Team Performance}

% Given a dataset 
% \(
%   D = \{ (x_i, y_i) \mid 1 \le i \le N \},
% \)
% the goal in human-AI collaboration is to achieve a strong human-AI team performance, for example by maximizing accuracy. In a simple \(0\)--\(1\) accuracy scenario, we can express this metric as:
% \[
%   \frac{1}{N} \sum_{(x_i, y_i) \in D} \mathbf{1}\Bigl\{ f\bigl(x_i,\, m(x_i;\theta'_m),\, h(x_i;\theta_h)\bigr) = y_i \Bigr\},
% \]
% where the indicator function \(\mathbf{1}\{ \cdot \}\) returns 1 if the final decision equals \(y_i\) and 0 otherwise.

% To achieve high accuracy, it is essential to capture the human-AI team decision-making model \(f(\cdot)\) in a way that accurately represents the human DM’s thought process. Recent work in human-AI collaboration has increasingly emphasized human confidence when examining how humans incorporate AI recommendations into their final decisions\cite{chong2022human,gemalmaz2022understanding,schemmer2023appropriate,wang2018does,mahmood2024designing}.

% For instance, Mahmood et al. (2024) proposes a threshold-based collaborative decision-making model:

% \[
% f\bigl(x_i, m(x_i;\theta_m), h(x_i;\theta_h)\bigr) 
% =
% \begin{cases}
%    h(x_i;\theta_h), & \text{if } \mathcal{c}_i > \tau,\\[4pt]
%    m(x_i;\theta_m), & \text{otherwise},
% \end{cases}
% \]
% where $\mathcal{c}_i$ denotes the human DM’s self-confidence on instance~$i$, and $\tau$ is the confidence threshold: if $\mathcal{c}_i \geq \tau$, the human’s decision is adopted; otherwise, the AI’s recommendation is followed.
% % where $\mathcal{c}_i$ is the human DM’s confidence on instance~$i$ given by a human confidence oracle $\mathcal{c}$, and $\tau$ is a \emph{confidence threshold} above which the human’s own decision is adopted, and below which the AI’s recommendation is followed. 
% % (i.e., $\mathcal{c}: \mathcal{H}(X) \to [0,1]$ \hashimoto{where $\mathcal{H}$ denotes the function space that consists of $h$.}))
% % In the model, human confidence is assumed to be provided by an oracle function \(\mathcal{c}\) (i.e., the confidence values \(\mathcal{c}_i\) are given), and thus we can not study adjusting the confidence.
% % Motivated by this limitation, we next describe how we incorporate a model of confidence formation.
% Since the model assumes self-confidence values are given a priori, it does not capture how self-confidence is formed or how shaping self-confidence could impact team performance. Motivated by this limitation, we next describe how we incorporate a model of self-confidence formation.

% \subsection{Modeling Human Self-Confidence}\label{confidence_calib}
% % Drawing on research findings about human judgments, we posit that human confidence can be influenced by both objective information (facts) and the qualitative impressions (sentiments) elicited by the evidence~\cite{griffin1992weighing}. 
% % Thus, we define
% % a confidence function:
% % \[
% % c(x) \;=\; F_{\theta}(s, f),
% % \]
% % where $s\in \mathcal{S}$ and $f \in \mathcal{F}$ are  \emph{sentiment parameter} and a \emph{fact parameter} of a text description $x$. 
% Drawing on research findings about human judgments, we posit that self-confidence can be influenced by both objective information (facts) and the qualitative impressions (sentiments) elicited by the evidence~\cite{griffin1992weighing}. Thus, we define a self-confidence function \hashimoto{$F_\theta:\mathbb{R}\times\mathcal{F}$}:
% \[
% c(x) = F_{\theta}(s, f),
% \]
% where the sentiment parameter \( s \in \mathcal{S} \) and the fact parameter \( f \in \mathcal{F} \) are derived from the text description \( x \) \hashimoto{, where $\mathcal{F}$ denotes fact space}.


% Modeling self-confidence allows us to study shaping users’ self-confidence, potentially mitigating issues of both over-reliance and under-reliance on AI recommendations. For example, when users are overly confident in their own decisions, they tend to under-rely on AI recommendations, which can hinder overall team performance. By altering the text \hashimoto{before providing it to human DM}, we can “nudge” self-confidence
% towards an appropriate level, thereby improving \emph{human-AI collaboration} outcomes.
% In our paper, we consider adjusting the sentiment parameter \( s \) because modifying the facts \( f \) risks altering objective information and compromising accuracy and trustworthiness. 

% % We derive theoretical results on the optimality of perfect confidence calibration in a threshold-based collaborative decision-making model, highlighting the importance of confidence adjustments for effective AI-assisted decision-making. Propositions and proofs are provided in the supplementary materials (SM) due to page limits.


% % \begin{proposition}[Optimality of Perfect Calibration]
% % Suppose the AI has a constant accuracy $\alpha \in [0,1]$, and the human's confidence function is $c(x)$. In a threshold-based collaborative decision-making model, the human is selected if $c(x) > \tau$, and the AI otherwise. Assume further that the human is perfectly calibrated, so that $c(x)=p(x)$ where $p(x)$ is the true probability of a correct decision. Then under a threshold-based collaboration model,
% % there exists an \emph{optimal threshold} 
% % $\tau^* = \alpha$ such that the team’s expected accuracy
% % is maximized. No other threshold $\tau \neq \alpha$ can
% % achieve higher expected accuracy.
% % \end{proposition}

% % \begin{proof}
% %     See SM for the proof.
% % \end{proof}


% % \subsection{Theoretical Analysis of Confidence Calibration}
% % \label{sec:optimalCalibration}

% % In this section, we state our theoretical results regarding the optimality of perfect confidence calibration in a threshold-based collaborative decision-making model. These results highlight the importance of human confidence calibration for effective AI-assisted decision-making, thus motivating our work to adjust the human confidence.
% % Due to page limits, we only provide a
% % high-level proof sketch here. The complete proofs can be found in supplementary materials (SM).

% % \begin{proposition}[Optimality of Perfect Calibration]
% % Suppose the AI has a constant accuracy $\alpha \in [0,1]$, and the human's confidence function is $c(x)$. In a threshold-based collaborative decision-making model, the human is selected if $c(x) > \tau$, and the AI otherwise. Assume further that the human is perfectly calibrated, so that $c(x)=p(x)$ where $p(x)$ is the true probability of a correct decision. Then under a threshold-based collaboration model,
% % there exists an \emph{optimal threshold} 
% % $\tau^* = \alpha$ such that the team’s expected accuracy
% % is maximized. No other threshold $\tau \neq \alpha$ can
% % achieve higher expected accuracy.
% % \end{proposition}


% % \begin{proof}[Proof Sketch (see SM for details)]
% % When $c(x) = p(x)$, a threshold $\tau$ partitions the instance
% % space into those with $p(x) > \tau$ (human decides) and
% % $p(x) \le \tau$ (AI decides). By comparing these decisions
% % with $\alpha$, one sees that if $\tau < \alpha$, there exist
% % instances where $p(x) \le \alpha$ but the human is still chosen,
% % reducing overall accuracy. Similarly, if $\tau > \alpha$,
% % there are instances with $p(x) > \alpha$ but the AI is chosen
% % instead. Setting $\tau = \alpha$ ensures that each instance
% % is assigned to the more accurate decider. 
% % \end{proof}



% % \begin{corollary}[Perfect vs.\ Imperfect Calibration]
% \label{cor:calibrationBeatsImperfect}
% Given two humans, $H_1$ and $H_2$, each using the same threshold
% $\tau^*$, let $H_1$ be perfectly calibrated ($c_1(x)=p(x)$)
% and $H_2$ be imperfectly calibrated ($c_2(x)\neq p(x)$
% for some $x$). Then $H_1$ always achieves an expected accuracy
% at least as large as, and typically strictly larger than,
% $H_2$ under the same threshold-based policy.
% \end{corollary}
% \begin{proof}
%     See SM for the proof.
% \end{proof}

% ここのセクションの名前を考える
\section{Experiments}
\subsection{Dataset Construction}\label{sec:dataset}
% We develop a novel dataset for AI-assisted decision-making in the financial context. Our decision-making task focuses on predicting stock movements using summary descriptions of earnings conference calls (ECCs), thereby incorporating textual and numeric data about company’s recent performance.

% We developed a text-based stock-movement prediction dataset for AI-assisted decision-making. Our decision-making task focuses on predicting stock movements using a short excerpt on earnings conference calls (ECCs). ECCs are meetings between company managers and professional analysts to discuss the latest operations and future plans~\cite{keith-stent-2019-modeling}. ECCs convey important information to the market, and empirical research suggests they influence stock prices~\cite{ecc_influence}. Therefore, investors often incorporate ECCs into their financial decisions once these summaries are released. Accordingly, ECCs provide an ideal scenario for our experiments, which examine how human DMs predict stock movements.

\takehiro{We developed a text-based stock-movement prediction dataset using earnings conference call (ECC) transcripts. ECCs are meetings where company managers and analysts discuss operations and future plans~\cite{keith-stent-2019-modeling}, conveying critical market information that influences stock prices~\cite{ecc_influence}. Consequently, these transcripts are well-suited for our investment tasks.}



To develop our dataset, we first collected a set of short ECC summaries curated by professional journalists at Reuters, denoted as $x \in \mathcal{X}$~\cite{mukherjee-etal-2022-ectsum}. We selected 20 companies, and for each company, manually created three variant excerpts $(x_{\text{positive}}, x_{\text{neutral}}, x_{\text{negative}})$, each assigned a sentiment label $s \in \{\text{positive}, \text{neutral}, \text{negative}\}$ while preserving the factual content. We then verified that no factual errors or additional information were introduced, confirming that sentiment is the only change. 
Finally, each excerpt was anonymized by removing company names, following prior work~\cite{biran2017human}. 
This gives us a total of $20 \times 3 = 60$ excerpts, or $N = 60$.
A company's three-day stock movement is labeled as $y \in \{-1, +1\}$ (downward/upward, respectively). Companies were selected to balance upward and downward movements, with an average excerpt length of 51.2 words.
% The earnings conference calls span from Q3 2020 to Q1 2022.}



% Also, company's three-day stock movement was labeled as \(y \in \{-1,\,+1\}\) for downward or upward movement, respectively.
% The companies were selected to include equal numbers of upward and downward price movements over a three-day period, and the average word count per excerpt is 51.2. The earnings conference calls span from Q3 2020 to Q1 2022.




% To develop our dataset, we first collected a set of short ECC summaries curated by professional journalists at Reuters, denoted as
% \( x \in \mathcal{X} \)~\cite{mukherjee-etal-2022-ectsum}.
% Each summary serves as a source text, and a financial expert manually creates three variant excerpts $(x_{\text{positive}}, x_{\text{neutral}}, x_{\text{negative}})$ each assigned a sentiment label
% \( s \in \{\text{positive}, \text{neutral}, \text{negative}\} \), while preserving the factual content.
% A second financial expert then verifies that no factual errors or additional information are introduced, confirming that sentiment is the only change.

% To develop the dataset, first, we collected a set of short summaries of ECCs, denoted as $x \in \mathcal{X}$, curated by professional journalists at Reuters~\cite{mukherjee-etal-2022-ectsum}. For each source text $x$, a financial expert manually assigned a sentiment label $s \in \{\text{positive}, \text{neutral}, \text{negative}\}$ and produced three excerpts with distinct sentiments while remaining the factual content. In other words, for each $x$, we obtained three variants. A second financial expert then verified that no factual errors or additional information were introduced, confirming that the only change was the sentiment. Finally, each variant was anonymized by removing company names, following prior work~\cite{biran2017human}. 
% Each company's three-day stock movement was labeled as \(y \in \{-1,\,+1\}\) for downward or upward movement, respectively.


% $\mathcal{D}\equiv\{-1,+1\}$

% In total, our dataset comprises 60 excerpts from 20 companies, with three sentiment variants per excerpt. 
% Each company's three-day stock movement was labeled as \(y \in \{-1,\,+1\}\) for downward or upward movement, respectively.
% The companies were selected to include equal numbers of upward and downward price movements over a three-day period, and the average word count per excerpt is 51.2. The earnings conference calls span from Q3 2020 to Q1 2022.

\takehiro{To confirm the assigned sentiment, we performed financial sentiment analysis using the Loughran-McDonald dictionary~\cite{loughran2011liability}. We count the occurrences of positive and negative words from the dictionary, normalize these counts by the length of the excerpt, and then subtract the normalized count of negative words from the normalized count of positive words to obtain a sentiment score for each excerpt. As shown in Figure~\ref{lm_sentiment}, excerpts labeled positive have higher scores, while those labeled negative have lower scores, reflecting the intended sentiment.}

% To validate that the assigned sentiment is accurately reflected in each excerpt, we performed a financial sentiment analysis using the Loughran-McDonald dictionary~\cite{loughran2011liability}.
% In particular, we count the occurrences of positive and negative words from the dictionary, normalize these counts by the length of the excerpt, and then subtract the normalized count of negative words from the normalized count of positive words to obtain a sentiment score for each excerpt.
% Figure~\ref{lm_sentiment} shows box plots of average sentiment scores for excerpts grouped by the same assigned sentiment. We observe that the excerpts in our dataset accurately reflect the intended sentiment: those with a positive label exhibit higher sentiment scores, while those with a negative label exhibit lower sentiment scores.


\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figure/sig_boxplot_sentiment_scores.pdf}
    \caption{Box plots of sentiment scores for excerpts grouped by the assigned sentiment. Welch’s t-test indicate a highly significant difference between Positive and Neutral (***, $p\mkern-4mu<\mkern-4mu0.001$) and a marginally significant difference (*, $p\mkern-4mu<\mkern-4mu0.1$) between Negative and Neutral.}
    \label{lm_sentiment}
\end{figure}




% ここのセクションの名前を考える
\subsection{Procedure}\label{sec:experimental_process}

We recruited participants via Prolific~\footnote{\url{https://www.prolific.com/}} and assigned each participant 20 financial decision-making tasks using excerpts from 20 unique companies (with randomized order and sentiment: neutral, negative, or positive). After a brief tutorial and two practice tasks to learn how to use a slider ranging from \(-100\) (decrease) to \(100\) (increase) for predictions and self-confidence, participants completed the formal tasks. 
\takehiro{In each task, participants predicted stock movement based solely on an excerpt in the first round. In the second round, they received an AI prediction (with randomly assigned stated accuracy of either 51\% or 80\%) before finalizing their decision.}
% In each task, the first round involved predicting stock movement based solely on an excerpt, and the second round provided an AI prediction with stated accuracy (randomly selected from 51\% or 80\% ) before a final decision was made. 
% the second round provided a randomly selected AI prediction (with 51\% or 80\% stated accuracy) before a final decision was made. 


\takehiro{We measured participants' self-confidence and predictions using a slider ranging from \(-100\) to \(100\). We denote the slider values in the first and second round as \(h^{(1)}\) and \(h^{(2)}\), respectively. Human self-confidence $c$ is calculated as the absolute value of the $h^{(1)}$. Values near zero indicate low confidence, whereas values close to \(\pm 100\) indicate high confidence.}
The independent judgment \(y^h\) and final decision \(d\) were computed as follows:
\[
y^h = \begin{cases}
1, & \text{if } h^{(1)} > 0,\\[4pt]
-1, & \text{if } h^{(1)} < 0,
\end{cases}
\quad
d = \begin{cases}
1, & \text{if } h^{(2)} > 0,\\[4pt]
-1, & \text{if } h^{(2)} < 0.
\end{cases}
\]
Two attention checks were randomly inserted, and data from participants failing these were excluded.

% In our experiment, we measured participants' predictions and confidence using a slider ranging from -100 to 100. We denote the slider values in the first and second stages as $h^{(1)}$ and $h^{(2)}$, respectively.
% A positive value ($h>0$) indicates an increase, while a negative value ($h<0$) indicates a decrease. Values near zero indicate uncertainty, whereas values close to 100 or -100 indicate high confidence. During the formal tasks, we randomly inserted two attention checks. Data from participants who answered these attention checks incorrectly were excluded.
\subsection{Participants}
In our main study, we recruited 125 UK-based native English speakers via Prolific, all of whom met a minimum 80\% approval rating and were allowed to participate only once. Participants’ ages ranged from 18 to 81 (mean = 37.7, SD = 13.2), and the median completion time was 32.5 minutes. They received a base payment of £8 per hour, with a potential bonus of £2 for achieving an accuracy above 85\%. After excluding four participants for failing the attention checks, we obtained 121 valid respondents. 
\takehiro{Prior to the main study, we ran a pilot study with five participants under identical screening criteria. The median completion time was 29.5 minutes, so we set the completion time to 30 minutes for the main study.}

% Prior to the main study, a pilot study (with identical screening) indicated a median completion time of approximately 30 minutes, prompting us to adjust the main study’s duration accordingly. 
% The experiment protocol was approved by our organization's ethical board.


% In our main study, we recruited 121 participants from Prolific who were UK-based and native English speakers, all of whom met a minimum 80\% approval rating on Prolific. The participants were allowed to complete the study only once. Their ages ranged from 18 to 81 (mean = 37.7, SD = 13.2), and the median time to complete the study was 32.5 minutes. 
% We offered a base payment of £8 per hour, with a potential bonus of £2 if participants achieved an accuracy above 85\%. We excluded four participants for failing the attention checks, resulting in 121 valid respondents.
% Before the main study, we conducted a pilot study with the same screening as the main study. In the pilot study, the estimated time to complete the study was set at 40 minutes, but the median completion time was 29 minutes and 49 seconds. Therefore, for our experiment, we adjusted the time to 30 minutes. 
% The protocol was approved by our organization's ethical board.

\section{Evaluations}\label{sec:evaluations}
% In this section, we discuss how we address the three research questions in our paper, respectively.

\paragraph{RQ1: Impact of Self-Confidence Shaping}
First, we aim to quantify how much human-AI team performance can improve through the deliberate shaping of human self-confidence. To capture the extent to which human DMs accept AI recommendations, we define the \takehiro{AI recommendation Acceptance Rate (AR)} as the proportion of instances where a human DM adopts the AI’s prediction when their independent judgment differs from the AI recommendation.
\[
\text{AR} = \frac{\sum_{i=1}^{N} \mathbf{1}\{y^h_i \neq y^m_i \text{ and } d_i = y^m_i\}}{\sum_{i=1}^{N} \mathbf{1}\{y^h_i \neq y^m_i\}},
\]
where, $y^h_i$ is the human's initial independent judgement,  $d_i$ is the human's final prediction, $y^m_i$ is the AI's prediction for instance $i$, \( \mathbf{1}\{\cdot\} \) denotes the indicator function, and $N$ represents the total number of instances.



% We then perform a scenario analysis to assess the impact of adjusting confidence by comparing two cases:
\takehiro{We then perform a scenario analysis to assess the impact of shaping self-confidence on fostering appropriate reliance. Following previous work, we define inappropriate reliance as either over-reliance (when individuals align with AI predictions even though the AI is incorrect) or under-reliance (when individuals reject AI predictions even though the AI is correct)~\cite{are_you_really}.}

\vspace{1em}
\noindent \textbf{Under-Reliance Scenario:} In this scenario, highly confident individuals may ignore correct AI recommendations (underreliance).  We compute the relative change in the \takehiro{AI recommendation acceptance rate} using the following metric:
\begin{equation}
\scalebox{0.9}{$
\delta_{\text{under}}
= \frac{\text{AR}\bigl(c_i=\text{low},\, y^m_i=\text{correct}\bigr)-\text{AR}\bigl(c_i=\text{high},\, y^m_i=\text{correct}\bigr)}
       {\text{AR}\bigl(c_i=\text{high},\, y^m_i=\text{correct}\bigr)}.
$}
\end{equation}
where $c_i$ denotes the self-confidence, and  $y^m_i$ represent the AI prediction for an instance $i$. We calculate $c_i$ as the absolute value of the slider value $h^{(1)}$. 
$\text{AR}(c_i=\text{high},\, y^m_i=\text{correct})$ denotes the AR when the self-confidence is high and the AI prediction is correct.
We classify a decision as high self-confidence if its self-confidence exceeds the sample median, and low self-confidence otherwise.
The value of $\delta_{\text{under}} $ indicates a proportional change in the \takehiro{AI recommendation acceptance rate} between the high-confidence and low-confidence conditions when the AI prediction is correct.

\vspace{1em}
\noindent \textbf{Over-Reliance Scenario:} Here, low confident individuals may too readily follow incorrect AI recommendations (overreliance). We compute the relative change in the AI recommendation acceptance rate using the following metric:
\begin{equation}
\scalebox{0.9}{$
\delta_{\text{over}}
= \frac{\text{AR}\bigl(c_i=\text{high},\, y^m_i=\text{wrong}\bigr)-\text{AR}\bigl(c_i=\text{low},\, y^m_i=\text{wrong}\bigr)}
       {\text{AR}\bigl(c_i=\text{low},\, y^m_i=\text{wrong}\bigr)}.
$}
\end{equation}
The value of $\delta_{\text{over}}$ indicates a proportional change in the AI recommendation acceptance rate between the high-confidence and low-confidence conditions when the AI prediction is wrong. 

\begin{table}[t]
  \centering
  \caption{
    Acceptance rate by self-confidence (Conf.) and AI stated accuracy. 
    Superscript $^{\mathsection}$ denotes a significant difference (Welch's $t$-test, $p < 0.001$) between the two AI accuracy conditions (i.e., 51\% vs.\ 80\%), 
    and $^{\dagger}$ denotes a significant difference (Welch's $t$-test, $p < 0.001$) between the two self-confidence (i.e., Low vs.\ High).
  }
  \label{tab:advice_taking_rate}
\resizebox{0.8\linewidth}{!}{%
    \begin{tabular}{llll}
      \toprule
      \multirow{2}{*}{\textbf{Conf.}} 
        & \multicolumn{2}{c}{\textbf{AI Stated Acc.}} & \multirow{2}{*}{\textbf{Avg.}}\\
      \cmidrule(lr){2-3}
       & Low (51\%) & High (80\%) \\
      \midrule
      % High & $0.160^{\mathsection\dagger}$ & $0.298^{\dagger}$ \\
      % Low  & $0.398$ & $0.449$ \\
    High   & $0.160^{\mathsection\dagger}$ & $0.298^{\dagger}$ & $0.232^{\dagger}$ \\
    Low    & $0.398$ & $0.449$ & $0.423$ \\
    \midrule
    Avg.& $0.269^{\mathsection}$ & $0.363$ & $0.317$ \\
      \bottomrule
    \end{tabular}
  }
\end{table}

\paragraph{RQ2: Self-Confidence Prediction}
% After showing that confidence adjustment can enhance human-AI team performance, we examine the practicality of such adjustments in real settings.
% In particular, 
% We then investigate the extent to which high self-confidence can be predicted using both user and task characteristics. An accurate predictive model enables us to understand how human DMs will respond to a specific task description. This, in turn, allows us to refine the task description in order to achieve a target level of confidence.
\takehiro{We then investigate the extent to which self-confidence can be predicted using user traits and task characteristics. An accurate predictive model allows us to determine when self-confidence deviates from the optimal level, thereby indicating when the intervention is needed.}

In our study, we treat the prediction task as a binary classification problem where the outcome variable is whether a DM’s self-confidence is high or low. We define high confidence as any confidence score above the median value in our training data and low confidence as any score below the median. The predictors include user traits (such as demographic information and financial traits) and task characteristics (such as assigned sentiment and a pre-trained BERT-based text representation of the task description~\cite{devlin-etal-2019-bert}).

Accuracy is our primary evaluation metric. 
\takehiro{We randomly split the dataset into training, validation, and test sets in a 50:25:25 ratio, respectively.
We perform 5-fold cross-validation to select the best model, which is then evaluated on the test set.}
We repeat this over 10 seeds and report the average accuracy. Our evaluated models include a rule-based approach, which predicts high confidence for tasks with positive or negative sentiment and low confidence for neutral tasks, as well as machine learning models such as logistic regression, a multilayer perceptron (MLP), and LightGBM.
% Final Camera readyには入れる
% \footnote{Training details are available in the supplementary material.}

\paragraph{RQ3: The Relationship Between Sentiment and Self-Confidence}
Finally, we evaluate the association between task features \(x_i\) and a decision maker's self-confidence. We posit that self-confidence is influenced both by objective information (e.g., facts) and by qualitative impressions (e.g., sentiments) present in the task features~\cite{griffin1992weighing}. In our experiment, we focus on how the sentiment in a text is associated with self-confidence. We exclusively focus solely on sentiment because altering factual content may compromise objectivity and trustworthiness.

In particular, we compare self-confidence across excerpts with different assigned sentiments and test for significant differences between positive/negative and neutral excerpts.
Then, to examine how the intensity of a text’s sentiment relates to self-confidence, we compute Pearson’s correlation between the self-confidence and the absolute value of sentiment score (see Section~\ref{sec:dataset}). Since we want to focus on sentiment intensity, we take the absolute value of the sentiment score.
% Finally, we evaluate how sentiment in a text is associated with a DMs' \takehiro{self-confidence}. We focus exclusively on modifying the sentiment parameter \( s \) because modifying the facts risks changing objective information and compromising trustworthiness (see Section~\ref{confidence_calib}). 
% \takehiro{In particular, we compare self-confidence across excerpts with different assigned sentiments and test for significant differences between positive/negative and neutral excerpts.}
% Then, to examine how the intensity of a text’s sentiment relates to self-confidence, we compute Pearson’s correlation between the self-confidence and the absolute value of sentiment score (see Section~\ref{sec:dataset}). Since we want to focus on sentiment intensity, we take the absolute value of the sentiment score.
% Finally, we conduct a pilot study to explore how text revisions influence decision-making confidence. Drawing on insights from decision science and psychology \cite{griffin1992weighing}, we hypothesize that text sentiment, whether positively or negatively valenced, is associated with a DM’s confidence. We then evaluate this hypothesis by examining the relationship between human confidence and sentiment.

% In particular, we examine how the confidence varies across excerpts grouped by assigned sentiment and use Welch’s t-test to assess whether there are significant differences in confidence between positive/negative and neutral excerpts.


\section{Results}
% In this work, we explore how the sentiment of a text influences the confidence in decision-making and how adjusting confidence can improve human-AI team performance in high-stakes domains.

% \begin{itemize}
%     \item \textbf{RQ1:} How much improvement in Human-AI team performance can be achieved by deliberately adjusting human DMs’ confidence?
%     \item \textbf{RQ2:} To what extent can we predict high confidence in high-stake decision-making tasks?
%     \item \textbf{RQ3:} Is the text sentiment significantly associated with the human confidence?
% \end{itemize}
\begin{table}[t]
  \centering
  \caption{Acceptance rate (AR) and relative rate change by scenario. This table shows the AR under high and low confidence conditions and the corresponding relative rate changes for under- and over-reliance scenarios.}
  \label{tab:AR}
  \begin{tabular}{lccl}
    \toprule
    \multirow{2}{*}{\textbf{Scenario}} & \multicolumn{2}{c}{\textbf{Confidence}} & \multirow{2}{*}{\textbf{Rate Change} ($\delta$)} \\
    \cmidrule(r){2-3}
                      & High & Low & \\ 
    \midrule
    Under-Reliance & 0.279 & 0.412 & $ +47.6\%$ ($\delta_
    {\text{over}}$) \\
    Over-Reliance & 0.209 & 0.432 & $-51.6\%$  ($\delta_
    {\text{under}}$)\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{RQ1: Impact of Self-Confidence Shaping}\label{result:rq1}
We begin by examining how human-AI team performance can be improved by deliberately \takehiro{shaping} the \takehiro{self-confidence} of human DMs.
\takehiro{The acceptance rate, our primary metric, measures how often human DMs follow AI recommendations. We compare it across self-confidence (high vs.\ low) and stated AI prediction accuracy (high vs.\ low) to assess their influence on acceptance behavior.}


Table~\ref{tab:advice_taking_rate} shows the acceptance rate by self-confidence and AI-stated accuracy. Confidence is classified as high or low using the median, and AI accuracy is randomly set to 80\% or 51\% per task (see Section~\ref{sec:experimental_process}).
First, we can observe that human DMs tend to follow AI recommendations less frequently when they are highly confident in their own judgments. High-confidence instances show significantly lower acceptance rates than low-confidence ones (\takehiro{$0.160$ vs.\  $0.398$, $0.298$ vs.\ $0.449$, and $0.232$ vs.\ $0.423$, indicated by \(^{\dagger}\)}). Moreover, acceptance increases with higher AI-stated accuracy \takehiro{($0.269$ vs.\ $0.363$, indicated by \(^{\mathsection}\))} on average. High-confidence users are significantly more inclined to follow AI recommendations at 80\% accuracy \takehiro{($0.160$ vs.\ $0.298$, indicated by \(^{\mathsection}\))}, while low-confidence users show no significant difference between accuracy levels \takehiro{($0.398$ vs.\ $0.449$)}.

Interestingly, even when the AI’s accuracy is as high as 80\%, highly confident individuals remain relatively reluctant to follow AI recommendations, exhibiting an acceptance rate of 29.8\%. In contrast, low-confidence DMs, despite encountering a lower AI accuracy of 51\%, demonstrate a higher acceptance rate of 39.8\%. This observation underscores the critical role of self-confidence: regardless of the AI’s stated accuracy, individuals’ self-confidence significantly influence how likely they are to follow AI recommendations.


Next, we perform a scenario analysis to assess the impact of shaping self-confidence under two scenarios: under-reliance and over-reliance scenario. In under-reliance scenario, DMs ignore correct AI recommendations. Lowering self-confidence in this case is expected to encourage a more appropriate reliance, measured by \(\delta_{\text{under}}\). Here, a positive value indicates enhanced adoption of accurate AI recommendations and a corresponding gain in human-AI team performance. Conversely, in the over-reliance scenario, DMs are prone to accepting erroneous AI recommendations. Increasing self-confidence should help reduce this erroneous reliance, quantified by \(\delta_{\text{over}}\). 
A negative value reflects a reduction in wrong advice adoption and thus indicates a performance gain.
% Next, we perform a scenario analysis to assess the impact of confidence adjustments under two conditions: an under-reliance scenario (Scenario~1) and an over-reliance scenario (Scenario~2).
% In the under-reliance scenario, human DMs ignore AI recommendations more than is appropriate. Here, we expect that lowering confidence can improve Human-AI team performance. Concretely, we quantify the relative change in advice-taking rate when confidence shifts from high to low using \(\delta_{\text{under}}\). A positive \(\delta_{\text{under}}\) indicates potential improvement, as DMs become more inclined to adopt AI recommendations.
% Conversely, in the over-reliance scenario, performance can improve by raising low confidence, making it less likely for DMs to follow incorrect AI predictions. We capture this effect through \(\delta_{\text{over}}\). Here, a negative \(\delta_{\text{over}}\) indicates a reduction in advice-taking on erroneous AI recommendations, resulting in a performance gain.

% Next, we perform a scenario analysis to assess the impact of confidence adjustments under two scenarios: under-reliance (Scenario~1) and over-reliance (Scenario~2). 
% In Scenario~1, we focus on cases where DMs ignore correct AI recommendations, so lowering confidence should improve performance by fostering more advice-taking. 
% We quantify this change in advice-taking with \(\delta_{\text{under}}\), where a positive value indicates better adoption of AI advice, and thus gain in human-AI team performance. 
% Conversely, in Scenario~2, raising low confidence should reduce erroneous advice-taking, and a negative \(\delta_{\text{over}}\) reflects this performance gain.


% Next, we perform a scenario analysis to assess the impact of confidence adjustments under two conditions: an under-reliance scenario (scenario 1) and an over-reliance scenario (scenario 2).

% In the under-reliance scenario, we investigate how team performance can improve 
% by lowering high confidence, thereby encouraging DMs to adopt correct AI predictions. 
% We quantify this via $\delta_{\text{under}}$, a relative difference in advice-taking; 
% a positive value indicates potential improvement. 
% Conversely, in the over-reliance scenario, we examine how performance can improve by 
% raising low confidence, making it less likely for DMs to follow incorrect AI predictions. 
% This is captured by $\delta_{\text{over}}$, where a negative value represents reduced advice-taking on wrong AI outputs 
% (and thus a performance gain).


\begin{table}[t]
\centering
\caption{Comparison of model accuracy (in \%) across 10 seeds. 
We use Welch's t-test with $p\mkern-4mu<\mkern-4mu0.001$ for statistical significance: 
$^{\dagger}$ indicates significant improvement over the rule-based approach, 
$^{\mathsection}$ indicates significant improvement over using only user traits (U), 
and $^{\mathparagraph}$ indicates significant improvement over using only task traits (T). ($\downarrow$) indicates how much the accuracy decreased for either (T) or (U), compared to (T+U).}
\label{tab:accuracy}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{lcclll}
\toprule
      & Random           & Rule             & LightGBM                                 & LR                                      & MLP                                      \\
\midrule
$w$/T+U   & $50.0$           & $55.4$           & $66.4^{\dagger\mathsection\mathparagraph}$ & $66.9^{\dagger\mathsection\mathparagraph}$ & $67.3^{\dagger\mathsection\mathparagraph}$ \\
$w$/U     & $50.0$           & $55.4$           & $63.7^{\downarrow{4.1\%}}$                                    & $62.0^{\downarrow{7.3\%}}$                                    & $63.2^{\downarrow{6.1\%}}$                                    \\
$w$/T     & $50.0$           & $55.4$           & $63.3^{\downarrow{4.7\%}}$                                    & $62.6^{\downarrow{6.4\%}}$                                    & $63.1^{\downarrow{6.2\%}}$                                    \\
\bottomrule
\end{tabular}
}
\end{table}
Table~\ref{tab:AR} presents the results of the scenario analysis, showing the acceptance rates and corresponding relative rate changes.
We compute the relative change for under-reliance scenario as $\delta_{\text{under}} = (0.412 - 0.279) / 0.279 \approx +47.6\%$. This positive value indicates that lowering self-confidence from high to low could potentially increase the human-AI team performance by $47.6\%$, as it promotes a more appropriate reliance on the AI recommendations.
Similarly, the relative change for over-reliance is given by $\delta_{\text{over}} = (0.209 - 0.432) / 0.432 \approx -51.6\%$. Although the computed percentage is negative, this outcome is desirable because it reflects a $51.6\%$ reduction in the tendency to adopt incorrect AI recommendations. In other words, raising self-confidence from low to high helps discourage following incorrect AI predictions, thus improving team performance by $51.6\%$.
These analyses suggest that by appropriately shaping self-confidence, we can significantly mitigate both under-reliance and over-reliance on AI recommendations, thereby improving human-AI team performance.
% Table~\ref{tab:AR} presents the acceptance rates and corresponding relative rate changes for each scenario. We see that \(\delta_{\text{under}} = +47.6\%\) from \(\bigl(0.412 - 0.279\bigr) / 0.279\). This positive value indicates a potential 47.6\% improvement in human-AI team performance if we lower users' confidence from high to low, thereby encouraging them to adopt correct AI predictions. Meanwhile, \(\delta_{\text{over}} = -51.6\%\) follows from \(\bigl(0.209 - 0.432\bigr) / 0.432\). This negative value implies a 51.6\% performance gain if we raise users' confidence from low to high, thus discouraging them from following incorrect AI predictions. These scenario analyses suggest that adjusting users’ confidence can mitigate both over-reliance and under-reliance on AI predictions, thereby improving human-AI team performance.



\textbf{To answer RQ1,} our results confirm that self-confidence shaping can significantly improve human-AI team performance by reducing both over- and under-reliance. We show that human self-confidence is key to AI recommendation acceptance, and our scenario analysis indicates that perfect control over self-confidence could yield performance gains of 47.6\% and 51.6\% by mitigating under- and over-reliance, respectively. While our scenario analysis shows the potential of self-confidence shaping to enhance human-AI team performance, its assumption of perfect control over human confidence is unrealistic. In the following analysis, we evaluate the feasibility of such interventions in practice.


\subsection{RQ2: Self-Confidence Prediction}\label{result:rq2}
Having shown that self-confidence shaping can enhance human-AI collaboration, we now examine the feasibility of such interventions.
In particular, we examine the extent to which self-confidence can be predicted.
% We formulate a binary classification task to predict whether the DM exhibits high or low confidence based on user and task traits. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figure/violin_boxplot_accuracy.pdf}
    \caption{Violin plots showing the distribution of accuracy for machine learning and rule-based approaches across 10 seeds. Each violin depicts the kernel density of accuracy values. The box within indicates the interquartile range and the horizontal line marks the median. }
    \label{fig:box_violin_accuracy}
\end{figure}

Table~\ref{tab:accuracy} reports the average accuracy of our machine learning models and the rule-based approach, evaluated with and without the incorporation of user and task traits. Here, MLP and LR denote multilayer perceptrons and logistic regression models, respectively. Figure~\ref{fig:box_violin_accuracy} displays a violin plot of the accuracy distributions obtained from 10 random seeds when user and task traits are included.

% To set expectations, we first consider the performance of a rule-based approach that predicts high confidence for tasks assigned a positive or negative sentiment and low confidence for those assigned a neutral sentiment.
% From Table~\ref{tab:accuracy}, we observe that the accuracy of the rule-based approach is about 55\%, which is higher than the random-guess baseline of 50\%, which indicates that whether a text contains sentiment affects the human confidence.
% However, its accuracy remains relatively low, indicating room for improvement.
% As noted in Section~\ref{result:rq2}, the sentiment expressed in a text significantly influences the confidence, so the rule-based approach surpasses random guessing. However, its overall accuracy remains relatively low, indicating room for improvement.

From Table~\ref{tab:accuracy}, we observe that the rule-based approach achieves an accuracy of 55.4\%, which exceeds the random-guess baseline of 50\%. This suggests that the presence of sentiment is linked to self-confidence. Moreover, Table~\ref{tab:accuracy} shows that simple machine learning models significantly outperform the rule-based approach, with LightGBM, logistic regression, and MLP achieving accuracy improvements of 19.8\%, 20.8\%, and 21.5\%, respectively. 
\takehiro{This implies that self-confidence exhibits predictable patterns rather than being entirely random or unpredictable.}
The violin plots in Figure~\ref{fig:box_violin_accuracy} further confirm that, across 10 seeds, the machine learning models consistently outperform the rule-based approach. 

Furthermore, Table~\ref{tab:accuracy} shows that removing either user traits or task traits leads to a significant reduction in accuracy. For example, with the MLP model, accuracy decreases by 6.1\% when task traits are removed and by 6.2\% when user traits are removed. This indicates that both user and task traits play an important role in predicting self-confidence.
% These results demonstrate that simple machine learning models substantially improve prediction accuracy compared to the rule-based baseline.


% From Table~\ref{tab:accuracy}, we observe that the rule-based approach achieves an accuracy of 55.4\%, which exceeds the random-guess baseline of 50\%. This suggests that the presence of sentiment is linked to the human confidence. In addition, Table~\ref{tab:accuracy} shows that simple machine learning models significantly outperform the rule-based approach, with LightGBM, logistic regression, and MLP achieving accuracy improvements of 19.8\%, 20.8\%, and 21.5\%, respectively. The violin plots in Figure~\ref{fig:box_violin_accuracy} further confirm that, across 10 seeds, the machine learning models consistently outperform the rule-based approach. These results demonstrate that simple machine learning models substantially improve prediction accuracy compared to the rule-based baseline.



\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/sentiment_confidence_box_plot.pdf}
    \caption{Box plots of self-confidence $c$ for the three task‐sentiment variants. Welch’s t-test indicates a highly significant difference between Positive and Neutral (***, $p\mkern-4mu<\mkern-4mu0.001$) and a highly significant difference (***, $p\mkern-4mu<\mkern-4mu0.001$) between Negative and Neutral.}
    \label{fig:confidence_sentiment_plot}
\end{figure}

\textbf{To answer RQ2}, our results show that we can predict high vs.\ low confidence in decision-making with about 67\% accuracy using simple machine learning models that incorporate user and task traits. These models significantly outperform the rule-based approach (55.4\%) and a random-guess baseline (50\%). Our findings also confirm the importance of both user and task traits for accurate predictions. 
These results indicate that self-confidence prediction is indeed feasible and not merely a random exercise.
Future work may explore refining or expanding user and task traits or developing more advanced modeling techniques to achieve higher accuracy.


\subsection{RQ3: The Relationship Between Sentiment and Self-Confidence}\label{result:rq3}
We now examine our hypothesis that sentiment in a text is associated with human decision-making self-confidence, serving as a preliminary step toward modifying text to shape self-confidence.
% We now examine our hypothesis that text sentiment is associated with human decision-making confidence, as an initial step toward investigating how revising text will influence confidence in decision-making.

% We begin by examining the relationship between the sentiment in the task and the confidence of human decision-making. 
% We examine the average confidence for excerpts with different task sentiments. While curating the dataset, we verified that only the sentiment changed from the original text and that no additional information was introduced as explained in Section~\ref{sec:dataset}. Therefore, if the confidence differ, we can confirm that there is a relationship between task sentiment and confidence.

Figure~\ref{fig:confidence_sentiment_plot} shows self-confidence grouped by assigned sentiment. We observe significant differences when comparing positive vs.\ neutral and negative vs.\ neutral sentiments, indicating a relationship between sentiment and self-confidence. We also find that self-confidence is higher for tasks with positive sentiments than for those with negative sentiments. We believe this is because the intensity of positive sentiment is higher. As shown in Figure~\ref{lm_sentiment}, the absolute sentiment score is greater for positive than for negative sentiments, suggesting that the intensity of the sentiment influences self-confidence.
% While prior research in finance suggests that people are more strongly swayed by negative sentiment due to loss aversion, we do not observe this effect~\cite{kahneman2013prospect,takayanagi_naacl}. A likely explanation is that our task focuses on predicting increases or decreases rather than making buy-or-sell decisions.


Then, to investigate the relationship between the intensity of the sentiment and the level of self-confidence. We conduct a correlation analysis between the absolute value of sentiment score and self-confidence. We found a weak positive Pearson correlation of 0.221 ($p\mkern-4mu<\mkern-4mu0.001$), indicating that higher sentiment intensity is generally associated with higher self-confidence. 
However, the modest strength of this linear relationship suggests that factors beyond sentiment may affect a DM’s self-confidence. For instance, expressions of uncertainty (e.g., \emphasize{it will definitely rise} vs. \emphasize{it might rise}) could substantially alter how confident DMs feel. Therefore, to develop effective text modifications for shaping self-confidence, further research is needed to investigate the association between diverse linguistic patterns, such as uncertainty, and self-confidence

% On the other hand, the strength of this linear relationship is modest, suggesting that a simple linear model does not fully capture the complexity of the relationship. In particular, when the sentiment is extreme, such as overly positive expressions (e.g., \emphasize{absolutely successful} or \emphasize{no chance of failure}) or overly negative ones, the perceived credibility of the text may decline, leading to lower confidence. This implies that the relationship between sentiment and confidence might follow a non-linear pattern.

\textbf{To answer RQ3,} our results demonstrate that the sentiments are significantly related to the self-confidence in human decision-making. Our findings also suggest that the intensity of the sentiment is associated with self-confidence. However, a weak positive Pearson correlation ($r=0.221,  p\mkern-4mu<\mkern-4mu0.001$) between the sentiment score and the self-confidence indicates that factors beyond sentiment affect self-confidence. Future work should investigate a broader range of linguistic patterns in a text to enable more refined self-confidence shaping.

\subsection{\takehiro{Real-World Applications}}
\takehiro{Our findings demonstrate that self-confidence shaping can mitigate both over- and under-reliance on AI recommendations, and that such interventions are feasible through self-confidence prediction and text sentiment modification. These results have implications for high-stakes decision-making. For instance, consider a financial setting where updated ECC summaries convey information about recent extreme market events, and investors need to predict whether a stock price will increase or decrease. In such cases, AI models may yield low prediction accuracy in the face of extreme events (e.g., 55\%), while experienced financial experts might achieve higher accuracy (e.g., 65\%). However, the volatility of these events can undermine experts’ self-confidence, causing them to over-rely on the less accurate AI predictions. By employing our self-confidence prediction model to detect low self-confidence in advance and modifying the ECC summaries’ sentiment to increase experts' self-confidence, the system can help experts rely on their own judgment, thus mitigating the under-reliance.}
% Our findings demonstrate that deliberate confidence shaping can mitigate both over- and under-reliance on AI recommendations, and that such interventions are feasible through confidence prediction and text sentiment modification.

% These results have important implications for high-stakes decision-making scenarios. For instance, consider a financial setting where updated ECC summaries convey information about recent extreme market events and investors want to predict wehtehr stock price will increase or decrease. In such cases, AI models, which rely on historical data, may yield relatively low prediction accuracy faced with the extreme events (e.g., around 55\%), while experienced financial experts could achieve higher accuracy (e.g., approximately 65\%). However, the volatility of these events might undermine experts’ self-confidence, causing them to over-rely on the less accurate AI predictions. By employing our confidence prediction model to detect when experts are underconfident and then modifying the ECC summaries’ sentiment to bolster their confidence, the system can help experts rely more on their superior judgment. This intervention could improve overall decision-making performance by enabling a more appropriate balance between human insight and AI recommendations.

% \textbf{To answer RQ3, } our results demonstrate that the sentiments expressed in the task are significantly related to the confidence in human decision-making. Additionally, the findings suggest that the intensity of the sentiment is associated with the level of human confidence. Our analysis revealed a weak positive Pearson correlation ($r=0.221,  p\mkern-4mu<\mkern-4mu0.001$) between the sentiment score and human confidence, indicating that factors beyond sentiment affets the confidence.  Future work should investigate a broader range of linguistic patterns in text to enable more refined confidence adjustments.

\section{Conclusion}
This paper introduces a \takehiro{human self-confidence shaping} paradigm to enhance human-AI team performance.
First, this paper quantitatively assessed the extent to which human self-confidence shaping can improve human-AI team performance. Our scenario analysis indicated that, assuming successful interventions, self-confidence shaping mitigates both under-reliance and over-reliance on AI recommendations, potentially yielding performance gains of up to 47.6\% and 51.6\%, respectively.
Furthermore, we demonstrate the promising plausibility of such interventions: simple machine learning models achieve 67\% accuracy in predicting self-confidence, and our results indicate that modifying sentiment could be a viable method for shaping self-confidence.

While our analysis provides an initial step towards improving human-AI team performance via self-confidence shaping, significant gaps remain in implementing such interventions in real-world settings. In particular, future research should focus on improving the accuracy of self-confidence prediction by employing more sophisticated models and expanding the range of user and task traits considered. Also, further exploration of the relationship between self-confidence and linguistic patterns is needed for more refined interventions. 
By integrating human factors into AI-assisted decision-making, we offer insights into designing \takehiro{human-AI collaboration} that better support and complement human DMs.
% In conclusion, our findings provide the first evidence that shaping human self-confidence can significantly enhance human-AI decision-making performance. 
% Finally, practical implementation will require empirical validation in real-time online settings, including the investigation of optimal design parameters such as feedback timing.

\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

