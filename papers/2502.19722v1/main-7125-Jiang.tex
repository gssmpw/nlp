% File tacl2021v1.tex
% Dec. 15, 2021

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

\input{def}
\input{std-macros}

%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2021v1}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2021v1}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2021v1}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

\usepackage[acceptedWithA]{tacl2021v1}
% \usepackage[acceptedWithA,copyedit]{tacl2021v1}
% \setlength\titlebox{10cm} % <- for Option 2 below

%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{tacl2021v1}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi

%%%% End TACL-instructions-specific macro block
%%%%


\title{Few-Shot Multilingual Open-Domain QA from 5 Examples}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given
\author{Fan Jiang \and Tom Drummond \and Trevor Cohn\Thanks{Also at Google.} \\
  School of Computing and Information Systems \\
  The University of Melbourne, Victoria, Australia \\
  \texttt{fan.jiang1@student.unimelb.edu.au}\\
  \texttt{\{tom.drummond, trevor.cohn\}@unimelb.edu.au}}

\date{}

\begin{document}
\maketitle
\begin{abstract} 
Recent approaches to multilingual open-domain question answering (MLODQA) have achieved promising results given abundant language-specific training data.
% However, enhancing the multilingual performance with limited supervised data remains challenging and under-explored. 
However, the considerable annotation cost limits the application of these methods for underrepresented languages.
We introduce a \emph{few-shot learning} approach to synthesise large-scale multilingual data from large language models (LLMs).
Our method begins with large-scale self-supervised pre-training using WikiData, followed by training on high-quality synthetic multilingual data generated by prompting LLMs with few-shot supervision.
The final model, \ours, significantly outperforms existing few-shot and supervised baselines in MLODQA and cross-lingual and monolingual retrieval.
We further show our method can be extended for effective zero-shot adaptation to new languages through a \emph{cross-lingual prompting} strategy with only English-supervised data, making it a general and applicable solution for MLODQA tasks without costly large-scale annotation.
\end{abstract}

\input{sections/1_introduction}
\input{sections/2_method}
\input{sections/3_experiments}
\input{sections/4_relatedwork}


\section{Conclusion and Limitation}
In this work, we propose \ours, a \emph{few-shot learning} approach for multilingual open-domain retrieval tasks. We present a novel self-supervised pre-training framework that exploits WikiData to effectively initialise both multilingual retrieval and QA capabilities. This process is followed by few-shot synthetic multilingual QA generation from LLMs using only five human-annotated examples. We demonstrate that the resulting model achieves competitive multilingual retrieval and QA performance through fine-tuning on the high-quality synthetic data. We further show that this few-shot approach generalises to zero-shot settings that only require English-supervised data. This mechanism serves as an effective approach for language adaptation, enabling the adapted model to achieve both boosted retrieval and end-to-end QA performance across fifteen previously unseen languages.

This work uses LLMs for synthetic data generation, which may propagate undesirable biases to generated data. We believe such biases will not be amplified as we sample prompts from \textsc{Xor-TyDi QA}, a dataset annotated with strict guidelines. Our preliminary safety analysis also reveals that only less than 1\% data contains potentially harmful queries, as identified by \texttt{Llama-Guard-2}.

\section*{Acknowledgement}
We thank the action editor Shay Cohen and anonymous reviewers for their helpful feedback and suggestions. The first author is supported by the Graduate Research Scholarships funded by the University of Melbourne. This work was funded by the Australian Research Council, Discovery grant DP230102775.
% This research was undertaken using the LIEF HPCGPGPU Facility hosted at the University of Melbourne, which was established with the assistance of LIEF Grant LE170100200.

\bibliography{tacl2021}
\bibliographystyle{acl_natbib}

\clearpage

\appendix
\input{sections/appendix}

\end{document}


