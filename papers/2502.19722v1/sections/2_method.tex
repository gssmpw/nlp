\section{\ours}

\begin{figure*}
    % \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.3cm}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fsmodqa_whole_pipeline.pdf}
    \caption{Full pipeline for data construction and model training: (1) generate large-scale data from Wikidata for self-supervised pre-training; (2) use few-shot prompting to generate synthetic Q\&A pairs from Wikipedia passages of target languages, on which the pre-trained model is further fine-tuned.}
    \label{fig:fsmodqa_pipeline}
\end{figure*}

Figure~\ref{fig:fsmodqa_pipeline} presents the full pipeline for generating self-supervised pre-training and fine-tuning data.
\subsection{Self-Supervised Data Construction}\label{sec: mlwikiqa}
% Figure~\ref{fig:wikidata_pt} presents the self-supervised training data construction process, which we discuss in detail.

\paragraph{Sampling Factual Triplets.}
Our self-supervised training dataset is constructed based on Wikidata~\citep{wikidata}, a multilingual knowledge base consisting of fact triplets linked to millions of entities. We manually select 50 common properties (Appendix Table~\ref{tab:wikidata_property}) based on English and consider all triples associated with these relations. We then gather fact triplets in the desired target languages through language links.

\paragraph{Generating Questions.}
Given a triplet $\mathcal{T}=(s,r,o)$, we aim to write a question $q$ about the head entity $s$'s property $r$ with the gold answer $a$ being the tail entity $o$. One can use relation-specific templates to efficiently transform each triple into natural questions~\citep{sciavolino-etal-2021-simple}. However, this method lacks diversity, making triples with the same properties generate questions with similar surface forms. Instead, we adopt a generative approach by using a LLM to automatically generate questions with more diverse styles.

Specifically, we first sample five triples for each property and prompt ChatGPT (\texttt{gpt-3.5-turbo}) to generate three questions for each triple. This process yields a curated set of high-quality questions: $\mathbb{K}=\{s_i, r_i, o_i, q_i\}_{i=0}^k$.

We additionally generate questions with \texttt{Yes/No} answers from the same set of sampled triples. It is easy to generate \texttt{Yes} questions. For \texttt{No} questions, we need to create false fact triples from existing triples. Specifically, we randomly replace a triple's head or tail entity with the most similar Wikidata entity, and check the perturbed triple is not a valid fact according to Wikidata. We then generate questions using ChatGPT as before. Examples are included in Appendix Table~\ref{tab:chatgpt_prompt}.

Subsequently, these curated questions are used as in-context learning (ICL) examples to prompt a smaller LLM to transform all sampled triples into natural questions. We use \href{https://huggingface.co/google/gemma-7b}{Gemma-7B}~\citep{team2024gemma} as the LLM and include the prompts we used in Appendix Table~\ref{tab:icl_prompt}.

\begin{figure*}
    % \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.2cm}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/wikidata_pt.pdf}
    % \caption{Pre-training data construction pipeline. \emph{English translations} are added for readability.}
    \caption{Pre-training data construction pipeline: (1) transform WikiData triples into QAs using LLMs for each target language $L$, and (2) identify in-language and cross-lingual positive passages from the head entity's Wikipedia page and through language links. \emph{English translations} are added for readability.}
    \label{fig:wikidata_pt}
\end{figure*}

\paragraph{Multilingual Positive Passage Identification.}
% Given a transformed question $q$ and answer $a$ derived from a triple $(s,r,o)$, we assume passages containing the tail entity $o$ are positive. We gather all passages from the Wikipedia page $\mathcal{W}$ linked by $s$ and add passages containing $o$ as ground truth $\mathcal{D}_q$. If no such passage exists, we use partial match and select the one with the highest lexical overlaps with $o$ as ground truth. In addition to in-language positive passages, we also use language links to include Wikipedia pages $\mathcal{W}^L$ parallel to $\mathcal{W}$ and identify cross-lingual ground truth passages similarly using the translated tail entity $o^L$ through language links. A running example is shown in Figure~\ref{fig:wikidata_pt}. We generate 18.7M $(q,a,\mathcal{D}_q)$ triples across 8 languages in total, denoted as \ourptdata.

As shown in Figure~\ref{fig:wikidata_pt}, for a question $q^{ja}$ and answer $a^{ja}$ derived from a triple $(s^{ja},r,o^{ja})$, we gather all passages from the Wikipedia page $\mathcal{W}^{ja}$ linked by $s^{ja}$ and add passages containing $a^{ja}$ as positive $\mathcal{D}_q^{ja}$. If no such passage exists, we use partial match and select the one with the highest lexical overlaps with $a^{ja}$ as positive. We further include positive passages in other languages to facilitate cross-lingual retrieval. We first translate the triple into target languages $(s^L,r,a^L)$ using language links and identify cross-lingual positives by searching $a^L$ in the Wikipedia page $\mathcal{W}^L$ linked by $s^L$ as above. This derives monolingual and cross-lingual positive passages $\mathcal{D}_q=\mathcal{D}_q^{ja}\cup\mathcal{D}_q^{L}$. We generate 18.7M $(q,a,\mathcal{D}_q)$ triples across 8 languages in total, denoted as \ourptdata.\footnote{We classify \ourptdata as a silver-standard dataset rather than a synthetic one, as it is derived from the structured information in WikiData and Wikipedia.}
% \footnote{We consider \ourptdata as silver-standard instead of synthetic, as the data is generated by exploiting the crowdsourced WikiData and Wikipedia structures.}


\subsection{Few-shot Synthetic Data Generation}\label{sec:fs_data_gen}
\paragraph{Few-shot Setting.}
The main idea of \ours is to amplify a limited number of annotated examples into a substantially larger volume of synthetic data by prompting LLMs. In this work, we consider \textsc{Xor-TyDi QA}~\citep{asai-etal-2021-xor} as our target dataset. For each language in \textsc{Xor-TyDi QA}, we randomly sample five triples $\mathcal{K}=\{(q_i^L,a_i^L,d_i^L)\}_{i=1}^5$ from the training set as \emph{few-shot} examples. Each triple contains the question, answer, and the ground truth passage. We ensure that three examples are span answers, while the remaining two are \texttt{Yes} and \texttt{No} answers to align with \textsc{Xor-TyDi QA} distribution.

\paragraph{Prompt-based Question \& Answer Generation.}
We populate a hand-engineered template with our \emph{few-shot} language-specific examples $\mathcal{K}$ and use them as the ICL examples to prompt LLM. Given a randomly sampled passage $d^L$ from language $L$, we append $d^L$ to the template, and the LLM is expected to generate a relevant question $q^L$ and answer $a^L$ in language $L$. We further constrain the answer $a^L$ to be a span within $d^L$, a property of the original \textsc{Xor-TyDi QA} dataset.

Many questions classified as \emph{unanswerable} in~\citet{clark-etal-2020-tydi} can be answered by referring to English Wikipedia~\citep{asai-etal-2021-xor}. These questions are included as \emph{cross-lingual} questions in \textsc{Xor-TyDi QA}. To simulate this scenario, we generate synthetic cross-lingual data from English passages. We first use \href{https://translate.google.com/}{Google Translate} to translate the \emph{few-shot} examples to English: $\mathcal{K}'=\{(q_i^{En}, q_i^{L}, a_i^{En}, a_i^L, d_i^{En})\}_{i=1}^5$. Subsequently, we use these translated \emph{few-shot} examples $\mathcal{K}'$ to fill another template and instruct the LLM to generate QA from a randomly sampled English passage $d^{En}$, first in English $(q^{En}, a^{En})$ and then in target language $(q^{L}, a^{L})$. Similarly, we restrict $a^{En}$ to be a span within $d^{En}$. We include the prompts we used in Tables~\ref{tab:il_prompt} and~\ref{tab:cl_prompt} in Appendix.

\paragraph{Data Filtering.}
We employ a method based on Natural Language Inference (NLI) to enhance the quality of our synthetic data. NLI techniques aim to classify whether a hypothesis text is entailed by, neutral, or contradictory to a given premise text~\citep{bowman-etal-2015-large}. They have been widely used for identifying factual errors in text summarisation~\citep{laban-etal-2022-summac} and hallucinations in machine-generated texts~\citep{honovich-etal-2022-true}. In this study, we employ NLI methods for data filtering~\citep{yoran2024making}. Given a synthetic example $(q,a,d)$, we consider the source passage $d$ as the premise and the concatenation of the generated question $q$ and answer $a$ as the hypothesis. We retain an example only when the premise entails the hypothesis.

In more detail, we apply a novel \emph{local-to-global} filtering mechanism. In \emph{local filtering}, we evaluate whether the originating passage $d$ entails the synthetic QA $(q,a)$ pairs. We take the output probability of the entailment label as the score and keep examples when the entailment score exceeds a threshold $\mathcal{T}_l$. In \emph{global filtering}, we use a pre-trained model (\ie the self-supervised model in Figure~\ref{fig:overview} (b)) to perform retrieval for the question $q$ and obtain a set of passages $\hat{\mathcal{D}}_q$. We compute an entailment score vector $\bx\in\mathcal{R}^{|\hat{\mathcal{D}}_q|}$, with each entry being the entailment score between $(q,a)$ and a retrieved passage $d\in\hat{\mathcal{D}}_q$. We then apply a maximum pooling operation $\operatorname{max}(\bx)$ to derive the final score. The intuition behind this is that a valid $(q,a)$ should be supported by at least one of the retrieved passages, which aligns with open-domain settings. Similarly, we retain only those examples whose scores surpass a predefined threshold $\mathcal{T}_g$. In this way, we end up having 1.7M synthetic data in total across 7 languages, denoted as \ourdata.

\subsubsection{Zero-shot Cross-lingual Prompting}\label{sec:zero_shot_prompting}
Our \emph{few-shot} setting relies on a few annotated examples to generate synthetic QA pairs in target languages. However, this approach encounters significant challenges when the target language is extremely low-resourced, making it nearly impossible to obtain even a few examples. For this setting, we explore \emph{zero-shot} prompting, which uses \emph{cross-lingual} examples to prompt LLMs to generate synthetic QA pairs in target languages.

We consider two \emph{zero-shot} prompting settings. In \emph{English-Prompting} setting, we use English QA data to fill up a template and use it as the prompt to ask LLMs to generate QA pairs from passages randomly sampled from the target language. In \emph{Multilingual-Prompting} setting, we assume access to a handful of examples in a held-out language set. We randomly sample five multilingual examples from this held-out set to populate another template, and prompt LLMs to generate QA pairs in target languages. We include the prompts used in Tables~\ref{tab:en_prompt} and~\ref{tab:multilingual_prompt} in Appendix.
% \fan{Maybe move to the experiments part.}

\subsubsection{Data Sampling}\label{sec:geometric_sampling}
% We sample synthetic data for training based on answer length from a geometric distribution $l \sim Geo(p)$, keeping a good balance between short and long answers to match the distribution of \textsc{Xor-TyDi QA}. We empirically set $p=0.4\ (\mu = 2.5)$ for all languages except for Japanese, which we set $p=0.1\ (\mu = 10)$ to favor longer answers. We truncate the answer length to 30 when computing the distribution. 
% \fan{mention the expected length and say truncate length larger than 30 to 30. double check geo distribution.}
Our synthetic dataset, \ourdata, exhibits a strongly skewed distribution towards shorter answer lengths (often single tokens), whereas the human-annotated answers in \textsc{Xor-TyDi QA} tend to be substantially longer. To address this mismatch, we resample the training data from \ourdata according to answer length, using a geometric distribution, $l \sim \operatorname{Geo}(p)$, to achieve a better balance between short and long answers.\footnote{Empirically, we set $p=0.4\ (\mu = 2.5)$ for all languages except for Japanese, where we set $p=0.1\ (\mu = 10)$ to favour longer answers. When computing the distribution, we truncate the answer length to 30.}

\begin{figure}
    % \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.3cm}
    \centering
    \includegraphics[width=\linewidth]{figures/unified_model.pdf}
    \caption{The unified model for passage retrieval and question answering.}
    \label{fig:unified_model}
    %  The first half encoder layers serve as dual-encoders for retrieval, and the other half are cross-encoders for joint encoding. The model is trained to maximise the likelihood of generating the target answer and the dual-encoder part is additionally optimised through contrastive learning to either increase the probability of positive passages or match the retrieval score with the decoder's cross-attention across passages.
    % \fan{to L/2, and add a start token in decoder side, $L_{enc}$ and $L_{dec}$}
\end{figure}

\subsection{\ours Model}

\paragraph{Model Structure.}
As shown in Figure~\ref{fig:unified_model}, we employ a single encoder-decoder model to perform both passage retrieval and QA tasks. The first half of the encoder functions as a dual-encoder with shared parameters, which separately encodes the question $\bq$ and the passage corpus $\mathcal{D}$. Additionally, we append an instruction to the question to inform the language of the target answer: "\emph{Answer in $\{\text{lang}\}$}". A LayerNorm operation, followed by average pooling, is applied to compress the inputs into single vectors: $E_{\bq}$ and $\{E_{{\bd}_i}|{\bd}_i\in\mathcal{D}\}$, which are used for matching via dot products. The top-$k$ most relevant passages to the question are selected: $\mathcal{D}_{\bq}=\arg\operatorname{topk}_{{{\bd}_i}\in\mathcal{D}}(E_{\bq}\cdot E_{{\bd}_i})$. The embeddings of the question and each top-$k$ passage in $\mathcal{D}_{\bq}$ are concatenated and fed into the remaining cross-encoder layers. Finally, the cross-encoder embeddings are flattened and incorporated into the decoder through cross-attention to generate the answer ${\ba}$, following the Fusion-in-Decoder approach~\citep{izacard-grave-2021-leveraging}.
% \footnote{We modify the attention mask to disallow question and instruction tokens from attending to each other in the dual-encoder, ensuring the question embedding for passage retrieval is not impacted by any language-specific information.} 

\paragraph{Model Training.}
\ours is first pre-trained on \ourptdata and later fine-tuned on \ourdata. In self-supervised pre-training, we use a simple contrastive loss and answer generation loss to train \ours. The dual-encoder is updated by contrasting the paired question passage against the targets of other questions in one training batch (\ie in-batch negative). Formally, for $i$-th training example, the loss function $\mathcal{L}_{\text{ssl}}^i$ is:
\begin{gather}
    \scalebox{1.}{$-\log\frac{e^{(E_{{\bq}_i}\cdot E_{{\bd}_i})}}{\sum_{j=1}^{N}e^{(E_{{\bq}_i}\cdot E_{{\bd}_j})}} - \log \prod_{t=1}^T P({\ba}^i_t|{\ba}^i_{<t},{\bq}_i, {\bd}_i) \nonumber$}
\end{gather}

The pre-trained \ours is subsequently fine-tuned on our synthetic data through an end-to-end training mechanism. The dual-encoder is trained using signals derived from the answer generation task, with the cross-attention score from the decoder serving as the target for assessing question-passage relevance. For $i$-th training example, the loss function is formally defined as:
\begin{gather}
    \scalebox{0.875}{$\mathcal{L}^i_{\text{ret}}=\mathbb{KL}(P_{\text{ret}}(\cdot|{\bq}_i,\mathcal{D}_{{\bq}_i}||P_{\text{ca}}(\cdot|{\bq}_i,\mathcal{D}_{{\bq}_i})),\nonumber$} \\
    \scalebox{0.875}{$P_{\text{ret}}(\cdot|{\bq}_i,\mathcal{D}_{{\bq}_i}) = \operatorname{softmax}(E_{{\bq}_i}\cdot E_{{\bd}_1}, \dots, E_{{\bq}_i}\cdot E_{{\bd}_{|\mathcal{D}_{{\bq}_i}|}}) \nonumber,$} \\
    \scalebox{0.875}{$P_{\text{ca}}(\cdot|{\bq}_i,\mathcal{D}_{{\bq}_i}) = \sum_{h=0}^H\sum_{t=0}^{|{\bd}_j|}\frac{\operatorname{SG}(\operatorname{CA}(0,h,t))}{H}\ |\ {\bd}_j\in\mathcal{D}_{{\bq}_i}, \nonumber$}
\end{gather}
where $\mathcal{D}_{{\bq}_i}$ is the passages returned by the dual-encoder itself and $P_{\text{ca}}$ is the target distribution gathered from the decoder's cross-attention scores. $\operatorname{SG}$ signifies stop-gradient, which prevents the decoder from being affected by the retriever loss, and $\operatorname{CA}$ denotes the cross-attention score at the last decoder layer. The term 0 refers to the first output token, $H$ is the number of cross-attention heads, and $|{\bd}_j|$ stands for the length of passage ${\bd}_j$.

The entire model is optimised to generate the target answer ${\ba}_i$ given ${\bq}_i$ and relevant passages $\mathcal{D}_{{\bq}_i}$. The final loss is: $\mathcal{L}^i_{\text{e2e}}=\mathcal{L}^i_{\text{ret}}+\mathcal{L}^i_{\text{ans}}$, where \scalebox{0.95}{$\mathcal{L}^i_{\text{ans}}=\log \prod_{t=1}^T P({\ba}^i_t|{\ba}^i_{<t},{\bq}_i, \mathcal{D}_{{\bq}_i})\nonumber$}.