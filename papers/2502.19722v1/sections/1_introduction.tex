\section{Introduction}

Open-domain QA has demonstrated impressive performance by employing the \emph{retrieve-then-read} (Figure~\ref{fig:overview}(a)) pipeline~\citep{chen-etal-2017-reading}, which is built upon dense retrievers~\citep{karpukhin-etal-2020-dense} and efficient generative readers~\citep{izacard-grave-2021-leveraging}. However, this success has been primarily limited to English, leaving the multilingual setting under-explored. This limitation is mainly due to the difficulty and costs of creating high-quality and balanced human-supervised training data for languages other than English. Moreover, multilingual open-domain QA introduces additional challenges with retrieving evidence from multilingual corpora, requiring the underlying retrieval system to be capable of both cross-lingual and monolingual retrieval~\citep{asai2021one}.

More recently, efforts have been made to create multilingual open-domain QA benchmarks from existing multilingual machine reading comprehension tasks (\eg \textsc{Xor-TyDi QA}~\citep{asai-etal-2021-xor}) and by translating English datasets (\eg MKQA~\citep{longpre-etal-2021-mkqa}). These datasets have enabled various approaches to address multilingual open-domain QA problems, including iterative data augmentation~\citep{asai2021one} and extensive additional pre-training on Wikipedia texts~\citep{lapca, jiang-etal-2024-pre}. However, these methods still heavily depend on abundant high-quality language-specific data for fine-tuning, making them less effective solutions when language resources are limited. Therefore, a more generalisable approach to multilingual open-domain QA should aim to mitigate this reliance and be capable of facilitating language adaptation with minimally supervised samples.

\begin{figure*}
    % \setlength{\abovecaptionskip}{-0.0001cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/overview.pdf}
    \caption{\textbf{Left (a)}: The process of multilingual open-domain QA. \textbf{Middle (b)}: training strategies: 1) self-supervised pre-training; 2) fine-tuning on English QA; 3) translate English QA to target languages; 4) use English data to prompt LLMs to generate target language data; 5) use few-shot in-language data for LLM prompting. \textbf{Right (c)}: Performance comparison (Avg. F1) on the XOR-Full dataset.}
    \label{fig:overview}
\end{figure*}

In this paper, we present {\bf\ours}, a method for \emph{{\bf\emph{F}}ew-{\bf\emph{S}}hot} {\bf M}ultilingual {\bf O}pen-{\bf D}omain {\bf QA} using minimally-sized supervised data (\ie up to 5 per language).\footnote{We use the term \emph{few-shot} throughout this paper to denote that our method relies on only a small number of human-annotated examples. Thus, we classify our method as a \emph{few-shot learning} approach, consistent with~\citet{dai2023promptagator}.} Our approach consists of two core components: a self-supervised pre-training objective on multilingual corpora; and a synthetic data generation pipeline that prompts a large language model (LLM) using few-shot supervised examples. Concretely, we generate question-answer pairs from WikiData triples by leveraging LLMs' In-Context Learning (ICL) ability. To facilitate ICL prompts, we incorporate ChatBots to generate curated input-output pairs, which serve as examples for prompting LLMs to generate millions of questions from WikiData triples across various languages. After generating these question-answer pairs, we identify the supported Wikipedia passages through answer string matching. We further gather cross-lingual answers and evidence passages through Wikipedia language links to facilitate cross-lingual retrieval. Employing this generated data, we train a multilingual model with a joint objective for retrieval and QA, producing a promising pre-trained model (Figure~\ref{fig:overview}(c)) for subsequent \emph{few-shot learning}.

In \emph{few-shot learning}, we employ LLMs for data generation from few-shot examples. For each target language, we feed the few-shot examples to an LLM and prompt it to generate question-answer pairs from a given document. The few-shot examples are assumed to encapsulate the QA style and distribution of the target dataset, enforcing the LLM to generate synthetic data with similar characteristics. With abundant data, the pre-trained model can be further fine-tuned to achieve superior results (Figure~\ref{fig:overview}(c)). As an unsupervised alternative, we explore a \emph{zero-shot cross-lingual prompting} strategy that uses data from other languages as prompts for data generation, and we show this is almost on par with \emph{few-shot prompting} (Figure~\ref{fig:overview}(c)). 

We evaluate \ours on various datasets, including cross-lingual and monolingual retrieval, and multilingual open-domain QA. We observe notable improvements over competitive few-shot baselines, with +5.1\% gain on retrieval and +8.4\% gain on multilingual open-domain QA. To further test \ours's language adaptation ability, we conduct zero-shot adaptation experiments using our \emph{cross-lingual prompting} strategy on fifteen languages. This adaptation improves performance in both monolingual retrieval and multilingual QA significantly, achieving results that are superior or comparable to strong translation-based methods.\footnote{Code, data, and checkpoints are available \href{https://github.com/Fantabulous-J/FSMODQA}{here}.}
% \footnote{Code and all data will be released upon acceptance.}
