\section{Experiments}

\subsection{Datasets and Metrics}
We evaluate on the \textsc{Xor-TyDi QA} dataset~\citep{asai-etal-2021-xor}, with XOR-Retrieve for cross-lingual retrieval and XOR-Full for multilingual open-retrieval QA. We conduct zero-shot evaluations on two benchmarks, MIRACL~\citep{zhang-etal-2023-miracl} for monolingual retrieval and MKQA~\citep{longpre-etal-2021-mkqa} for multilingual open-domain QA. For XOR-Retrieve, we use the February 2019 English Wikipedia dump as the retrieval corpus and the same dumps from 13 languages for XOR-Full and MKQA~\citep{asai-etal-2021-xor}. For MIRACL, we use the monolingual Wikipedia preprocessed by~\citet{zhang-etal-2023-miracl}. Following prior work, we evaluate models at Recall@5kt (top 5000 tokens) on XOR-Retrieve; F1, exact match (EM) and BLEU on XOR-Full; nDCG@10 on MIRACL; and F1 on MKQA.

\subsection{Baselines}
We compare \ours with three ranges of baselines: (i) Zero-shot baselines ("-En") fine-tuned on supervised English-only training data. We consider Natural Questions~\citep{kwiatkowski-etal-2019-natural} as the default. (ii) Supervised baselines that fine-tuned on human-annotated multilingual data (\ie \textsc{Xor-TyDi QA}). (iii) Few-shot models that improve zero-shot baselines with only a few supervised multilingual instances.

\paragraph{Retriever Baselines.}
% For XOR-Retrieve, we include: (i) Zero-shot retrievers: the translate-test methods DPR+MT~\citep{asai-etal-2021-xor} and ReATT+MT~\citep{jiang-etal-2022-retrieval} that use English DPR~\citep{karpukhin-etal-2020-dense} with English-translated queries; LAPCA~\citep{lapca} pre-trained on parallel Wikipedia pages; CLASS-En~\citep{jiang-etal-2024-pre} that employs a two-stage pre-training strategy to first distill from English models on parallel queries then trained end-to-end on multilingual self-supervised data. (2) Supervised retrievers: mDPR~\citep{asai-etal-2021-xor} fine-tunes a multilingual DPR on XOR-Retrieve; CORA~\citep{asai2021one} trains a multilingual DPR iteratively, with positive and negative passages identified by a multilingual QA model; Sentri~\citep{sorokin-etal-2022-ask} iteratively trains a multilingual DPR on self-identified positive and negative passages; QuiCK~\citep{ren-etal-2022-empowering} distills knowledge from a query generator; DrDecr~\citep{li-etal-2022-learning-cross} pre-trains a cross-lingual ColBERT on WikiMatrix~\citep{schwenk-etal-2021-wikimatrix} with sophisticated cross-lingual alignment method. (3) Few-shot retrievers: SWIM-X~\citep{thakur-etal-2024-leveraging} trains a multilingual retriever on massive synthetic data, generated from LLMs through a summarisation-then-ask technique. CLASS (5-shot) fine-tunes CLASS-En on the same 5-shot examples used in \ours. For MIRACL~\citep{zhang-etal-2023-miracl}, we include two supervised retrievers: Hybrid that combines the results of BM25, mDPR, and mColbert~\citep{colbertv1} and fine-tuned mContriever~\citep{izacard2022unsupervised}. 

For XOR-Retrieve, we include: (1) Zero-shot retrievers: translate-test methods: DPR+MT~\citep{asai-etal-2021-xor} and ReATT+MT~\citep{jiang-etal-2022-retrieval}; models pre-trained on multilingual Wikipedia: CLASS-En~\citep{jiang-etal-2024-pre} and LAPCA~\citep{lapca} (2) Supervised retrievers: multilingual dense retrievers: mDPR~\citep{asai-etal-2021-xor}, CORA~\citep{asai2021one}, Sentri~\citep{sorokin-etal-2022-ask}, QuiCK~\citep{ren-etal-2022-empowering}; token-level dense retrievers: DrDecr~\citep{li-etal-2022-learning-cross} pre-trains ColBERT on WikiMatrix~\citep{schwenk-etal-2021-wikimatrix}. (3) Few-shot retrievers: SWIM-X~\citep{thakur-etal-2024-leveraging} generates massive synthetic data from LLMs through a summarisation-then-ask technique. CLASS (5-shot) fine-tunes CLASS-En on our 5-shot examples. For MIRACL~\citep{zhang-etal-2023-miracl}, we include two supervised retrievers: fine-tuned mContriever~\citep{izacard2022unsupervised} and Hybrid that combines the results of BM25, mDPR, and mColbert~\citep{colbertv1}. 

\paragraph{Reader Baselines.}
(1) Zero-shot baselines: translate-test methods MT+DPR, ReAtt+MT, and GMT+GS generate answers from English retrieved passages with question and answer translations. (2) Supervised baselines: BM25 does in-language retrieval with an extractive multilingual QA model; MT+Mono first applies BM25 and then MT+DPR if no answer was generated. Fusion-in-decoder methods (\ie CORA, CLASS, Sentri, LAPCA) use retrieval-augmented generation, generating target language answers from multilingual retrieved passages. (3) Few-shot readers: Gemma (5-shot)~\citep{team2024gemma} and LLaMa3 (5-shot)~\citep{touvron2023llama} prompt LLMs with few-shot examples and retrieved passages using the template in Appendix Table~\ref{tab:llm_fs_qa_prompt}; CLASS (5-shot) fine-tunes CLASS-En on few-shot examples. We use the same 5-shot examples for all methods.



\subsection{Implementation Details}\label{sec:implementation}
With the proposed self-supervised data construction method, we generate 18,735,159 triplets for pre-training across 8 languages, with statistics in Appendix Table~\ref{tab:data_statistics}. We initialise our model from the mT5-large checkpoint~\citep{xue-etal-2021-mt5} and pre-train it using the loss function $\mathcal{L}_{\text{ssl}}$ for 100K steps with a batch size of 800 on 16 A100 GPUs for 64 hours. We set the learning rate to $5\times 10^{-5}$ with 10\% steps of warm-up, and linear decay to $0$.

With our few-shot data generation method, we obtain 1,746,156 question-answer pairs across 7 languages included in \textsc{Xor-TyDi QA} after data filtering with $\mathcal{T}_l=0.5$ and $\mathcal{T}_g=0.8$, with detailed statistics shown in Table~\ref{tab:data_statistics} in Appendix. 
For fine-tuning, we first train the pre-trained model using NQ data for 8K steps and then on \ourdata for 6K-14K steps depending upon the size of the sampled
training dataset, with the loss function $\mathcal{L}_{\text{e2e}}$. We set the batch size to 128 and the learning rate to $5\times 10^{-5}$. We apply an asynchronous passage update mechanism, where we periodically refresh the retrieved passages for each training query using the most recent checkpoint every 1K steps.

\input{tables/xor_retrieve}
\input{tables/miracl}

\subsection{Retrieval Results}
\paragraph{XOR-Retrieve.}
Table~\ref{tab:xor_retrieve_results} shows that \ours, fine-tuned on 100K synthetic data, surpasses the few-shot SWIM-X (7M) by 5.5\% at Recall@5kt, despite the latter using substantially more synthetic data generated by a significantly larger proprietary LLM (PaLM2). This indicates our method's great efficiency in training and data generation. Further scaling up the training data to full size does not improve retrieval accuracy. In addition, we find that fine-tuning CLASS, a sophisticated pre-training method, on the same set of 5-shot examples, lags \ours by 3.1 points. This shows our method of amplifying data through LLM prompting is superior to direct fine-tuning.

\paragraph{MIRACL.}
Table~\ref{tab:miracl} shows that \ours surpasses the few-shot retriever SWIM-X by 5.1\%, although SWIM-X generates synthetic data on each MIRACL language through 3-shot prompting, whereas \ours is exclusively trained on synthetic data generated from 5-shot examples of $\textsc{Xor-TyDi QA}$ and thus, evaluated on a \emph{zero-shot} manner. We further divide languages into seen and unseen groups based on \ours's training data. It outperforms SWIM-X on all seen languages and 7 out of 10 unseen languages, except on \texttt{zh}, \texttt{fr}, and \texttt{de}. We suspect SWIM-X benefits significantly from large-scale synthetic data generation on these high-resource languages.

\input{tables/xor_full}
\input{tables/mkqa}

\subsection{Multilingual QA Results}
\paragraph{XOR-Full.}
In Table~\ref{tab:xor_full_results}, we show \ours achieves the best results in few-shot settings, outperforming CLASS-En (directly fine-tuning on 5-shot examples) by 8.4\% and directly few-shot promoting LLMs for QA by 18\%. Compared to supervised readers, \ours surpasses CORA and other pipeline methods while achieving results comparable to the rest. It is also noteworthy that in two low-resource languages, \ours outperforms comparable supervised baselines in Bengali and achieves a closer match in Telugu, indicating the effectiveness of our method in handling low-resource languages.  

\paragraph{MKQA.}
In Table~\ref{tab:mkqa_qa_zs}, \ours achieves the best zero-shot results on MKQA in almost all languages, with an improvement of +2.8\% compared to supervised CORA and CLASS. This suggests that training on our synthetic data can well generalise to other new languages, indicating that generating synthetic data for each target language may not be necessary for language adaptation.


% \subsection{Analysis}
\subsection{Ablation}
We perform ablation studies to justify each of our designs, with results shown in Tables~\ref{tab:ablation_cl_queries} and~\ref{tab:ablation}.

\paragraph{Cross-lingual data improves cross-lingual ability.} 
Excluding cross-lingual synthetic training data enhances performance in answering questions that require only the retrieval of in-language passages. However, the result on questions relying on cross-lingual passage retrieval declines, reducing the overall results. This is further evidenced by retrieval results R$^\text{M}$@100, where the accuracy of finding evidence in any language (\eg English and in-language) drops, with additional support from the cross-lingual passage retrieval results.

\paragraph{Data filtering improves data quality.}
By using the raw synthetic data from LLMs without any quality control, the performance suffers in every examined language except Telugu. We suspect that the NLI model is deficient in this language.

\paragraph{Geometry sampling improves long-answer generation.}
Sampling data according to geometry distribution over answer length leads to a 0.9\% gain on average. In languages that contain a significant number of long answers (\ie \texttt{ar}, \texttt{fi}, \texttt{ja}, \texttt{ru}), geometry sampling shows gains of up to 2.7\%. Conversely, in \texttt{bn}, and \texttt{ko}, where short answers dominate, random sampling is usually better.

\paragraph{Pre-training is crucial.}
We observe extremely poor results in all languages without pre-training on our \ourptdata, primarily due to the model's low retrieval accuracy in identifying relevant passages. We believe pre-training enables the model to achieve good initial retrieval accuracy, which is essential in the subsequent fine-tuning process.
\input{tables/ablation}

\subsection{Training Data Scaling}
\paragraph{Performance improves with more synthetic data.}
To investigate the effect of our data scale on models, we train \ours on subsets ranging from 0.05M to the entire 1.7M QA pairs, Results on each language and the average performance are shown in Figure~\ref{fig:synthetic_data_scaling}. As the data size increases, \ours shows enhanced average performance up to the 0.6M data scale and gradually decreases afterward. We observe that as data size increases, the proportion of examples with short answers increases (78.4\% $\rightarrow$ 95.3\%), and the result on long-answer examples drops from 18.0\% to 15.1\%, indicating overfitting to short answers.


Our geometric sampling method (\S\ref{sec:geometric_sampling}) attempts to balance the answers by length, however its use of \emph{sampling without replacement} means the few long answer instances are quickly exhausted, such that  larger sampled datasets become skewed toward shorter answers.
% and eventually degenerates to the natural distribution (\ie instances with short answer dominate) when training on the full \ourdata dataset. 
To mitigate this issue, we employ \emph{sampling with replacement} variant. This method follows the precomputed geometric distribution by upsampling longer-answer examples.\footnote{We do not cap the number of repeats.} As a result, it effectively increases the number of training epochs for data points with longer answers. As shown in Figure~\ref{fig:synthetic_data_scaling_strict_geo_sampling},  \emph{sampling with replacement}  significantly improves performance on longer answers ($\ge4$ tokens) while maintaining comparable performance on shorter answers relative to the current method.

% \edit{To address this issue, we employ an alternative sampling strategy when scaling up the data. The current method used in our experiments relies on geometric sampling, where the number of data samples for a given answer length is adequate. However, when the data for longer answers is insufficient, it defaults to sampling data with shorter answers. This \emph{sampling without replacement} approach results in a distribution skewed toward shorter answers as the data scales up. In contrast, we employ a \emph{sampling with replacement} variant. In this method, we strictly adhere to the precomputed geometric distribution by repeating samples even when data for a specific answer length is insufficient. This approach effectively increases the training epochs for data points with longer answers. As shown in Figure~\ref{fig:synthetic_data_scaling_strict_geo_sampling}, adopting the \emph{sampling with replacement} strategy significantly improves performance on longer answers ($\ge4$ tokens) while maintaining comparable performance on shorter answers relative to the current method (i.e., w/o replacement).}

\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{-0.01cm}
    \setlength{\belowcaptionskip}{-0.2cm}
    \includegraphics[width=\linewidth]{figures/data_scaling_f1.pdf}
    \caption{Performance when trained with different sizes of our synthetic data.}
    \label{fig:synthetic_data_scaling}
\end{figure}
\begin{figure}[t]
    \centering
    % \setlength{\abovecaptionskip}{-0.00001cm}
    \setlength{\belowcaptionskip}{-0.3cm}
    \includegraphics[width=\linewidth]{figures/data_scaling_f1_strict_geo_sampling.pdf}
    \caption{Performance comparison when sampling data with or without replacement by using our geometric sampling strategy.}
    \label{fig:synthetic_data_scaling_strict_geo_sampling}
\end{figure}

\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{-0.01cm}
    \setlength{\belowcaptionskip}{-0.2cm}
    \includegraphics[width=\linewidth]{figures/supervised_data_scaling.pdf}
    \caption{Results when trained with varying sizes of supervised data. The average together with the best and worst languages are reported.}
    \label{fig:supervised_data_scaling}
\end{figure}

\paragraph{Few-shot prompting is superior to direct fine-tuning and benefits from more supervised data.}
In Figure~\ref{fig:supervised_data_scaling}, we show that directly fine-tuning on the 5-shot examples is beneficial (28.6\% $\rightarrow$ 33.5\%) but remains inferior to our \emph{few-shot} method. When increasing the size of supervised data, both methods achieve consistent improvements although the performance gap narrows. With full-sized training data, \ours surpasses CLASS (43.0\% \versus 41.6\%), achieving new state-of-the-art results. See Appendix Table~\ref{tab:supervised_data_scaling_details} for results in each language.

\input{tables/prompt_source}
\subsection{Zero-shot Prompting Strategies}\label{sec:zs_cl_prompt}
We compare our \emph{few-shot} prompting strategy with two \emph{zero-shot cross-lingual} prompting methods in~\S\ref{sec:zero_shot_prompting}. In \emph{English-Prompting}, we consider NQ training data and \textsc{TyDi QA} English training data as prompting sources, respectively. In \emph{Multilingual-Prompting}, we use 5-shot examples from all languages in \textsc{Xor-TyDi QA} (\ie those used in our \emph{few-shot} setting) for prompting. When generating synthetic data for each target language, we exclude its 5-shot examples from the prompting source. We compare the success rate of generating valid examples using different prompting strategies in Appendix Table~\ref{tab:success_rate}, with \emph{few-shot} prompting achieving the highest rate and \emph{English-Prompting} with NQ yielding the lowest rate.

\paragraph{Zero-shot prompting is comparable to few-shot prompting.}
Table~\ref{tab:prompt_source_comparison_results} shows that all three zero-shot prompting variants achieve consistent improvements over \oursen with up to 8.1\% gains, highlighting the versatility of our method in zero-shot language adaptation. Prompting with English datasets created with the same guidelines achieves better results (\textsc{TyDi}-En \versus NQ-En), and using multilingual examples for prompting (\ie \textsc{Xor-TyDi}-*) is comparable to \ours. Specifically, the diversity and QA styles in prompts are more important for \texttt{fi} and \texttt{te}, while for other languages, employing in-language prompts usually leads to the best performance.

\input{tables/en_data_usage}
\paragraph{English-prompting is the best way of using English data and is complementary to existing methods.}
We compare three different ways of using \textsc{TyDi QA} English data for zero-shot learning, direct English fine-tuning, fine-tuning on machine-translated data from English, and \emph{English Prompting}. Table~\ref{tab:en_data_usage_means} shows the benefits of all three methods, with our \emph{English-Prompting} approach yielding the best results in all languages. Additionally, combining data from all three methods results in improvements over any of them when used independently, and matches the performance of our few-shot setting.

\section{Zero-shot Language Adaptation}
% In \S~\ref{sec:zs_cl_prompt}, the results reveal that employing data from other languages as prompts is comparable to few-shot prompting. 
% In this section, we extend the \emph{zero-shot prompting} strategy in~\S\ref{sec:zero_shot_prompting} to adapt \ours to a wide range of new languages using English data.

% \paragraph{Datasets and Baselines.} 
% We consider supervised data from NQ as sources. In MIRACL, we select ten languages unseen by \ours for monolingual retrieval adaptation. In MKQA, we choose eight unseen languages with high, medium, and low resources for multilingual open-domain QA. We compare the adapted model with \ours and the translate-train baseline (MT). Synthetic QAs are generated from Wikipedia paragraphs for each language using 5-shot prompts randomly sampled from NQ. We employ Google Translate to translate NQ into each target language for MT. For both methods, we fine-tune \ours for 3K steps while keeping other hyperparameters unchanged. Note that models are created per language in this experiment.

In~\S\ref{sec:zero_shot_prompting}, we propose a \emph{zero-shot prompting} strategy that uses few-shot examples from other languages to generate synthetic data for a distinct target language. The effectiveness of this approach is demonstrated in~\S\ref{sec:zs_cl_prompt}. In this section, we evaluate the impact of this strategy in adapting \ours to a diverse range of previously unseen languages, using only English labelled data.

\subsection{Experimental Setup}
\paragraph{Languages}
We select ten languages unseen by \ours from the MIRACL dataset for monolingual retrieval adaptation. We choose ten unseen languages from the MKQA dataset with high, medium, and low resources for multilingual open-domain QA adaptation.

\paragraph{Data Generation}
We consider the English NQ training data as the source for prompts. For each target language, we randomly sample five-shot examples from the NQ dataset to prompt the generation of Q\&A pairs from selected Wikipedia passages, following the procedure described in~\S\ref{sec:fs_data_gen}. This approach yields 128,000 training instances for each target language. Additionally, we compare this method to the translate-train baseline (MT), which uses Google Translate to translate the NQ training data into the target languages.

\paragraph{Model Training}
For both methods, we fine-tune \ours for 3K steps following the same procedure used in \ourdata (\S\ref{sec:implementation}). The final checkpoint obtained at the last training step is used for evaluation. Note that separate models are created per language in this experiment.

% \begin{figure}[t]
%     \centering
%     \setlength{\abovecaptionskip}{-0.02cm}
%     \setlength{\belowcaptionskip}{-0.35cm}
%     \includegraphics[width=\linewidth]{figures/zs_adapt.pdf}
%     \caption{Zero-shot adaptation to unseen languages in monolingual retrieval (nDCG@10) and multilingual open-domain QA (F1).}
%     \label{fig:zs_adapt}
% \end{figure}

\input{tables/adaptation}

\subsection{Results}
\paragraph{Monolingual Retrieval Adaptation.}
As shown in the upper part of Table~\ref{tab:zs_adapt}, the zero-shot adaptation significantly improves \ours's monolingual retrieval results by an average of 5.4\% across ten unseen languages. These improvements are particularly pronounced in low-resource languages (\ie \texttt{th}, \texttt{yo}, \texttt{sw}), whereas the MT baseline results in notable declines both in these languages (\eg -36.3\% in \texttt{yo}) and overall (-3.3\%). Note that MIRACL was created by native speakers from texts in the target languages, which aligns with our data generation process. This explains the consistent gains achieved by our method and shows its superiority to translation-based approaches.

\paragraph{Multilingual Open-domain QA Adaptation.}
As shown in the bottom of Table~\ref{tab:zs_adapt}, the adaptation effectively enhances multilingual open-domain QA performance across seven languages, achieving an average improvement of 11.3\%. MT-based approaches yield results comparable to our adaptation, which is expected since MKQA was translated from NQ and the machined-translated data share the same topic distributions (\ie American-centric). In contrast, our method generates data from Wikipedia texts written in target languages to simulate how native speakers ask questions, which is more common for real-world scenarios. 

\section{Data Analysis}
\subsection{Quality Validation}
\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{-0.02cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \subfigure[\ourptdata (Silver-Standard)]{
        \label{fig:quality_validation_a}
        \includegraphics[width=\linewidth]{figures/quality_validation_mlwikiqa.pdf}
    }
    \subfigure[\ourdata (Synthetic)]{
        \label{fig:quality_validation_b}
        \includegraphics[width=\linewidth]{figures/quality_validation.pdf}
    }
    % \includegraphics[width=\linewidth]{figures/quality_validation.pdf}
    \caption{Quality validation results on \ourptdata (top) and \ourdata (bottom). We employ \emph{Model-as-Judge} to evaluate the quality of generated data on a three-level rating scale (0-2) based on two factors: fluency and relevance.}
    \label{fig:quality_validation}
\end{figure}

\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{-0.01cm}
    \setlength{\belowcaptionskip}{-0.35cm}
    \includegraphics[width=\linewidth]{figures/tsne_gold_vs_synthetic_ja.pdf}
    \caption{Distribution comparison between \ourdata and \textsc{Xor-TyDi QA} in Japanese. We show that the synthetic data is diverse and significantly overlaps with the gold standard.}
    \label{fig:tsne_gold_vs_synthetic_ja}
\end{figure}

% \trevor{Figure 8: part (a) isn't commented on in the text. I was puzzling whether a) was gold standard data and b) the synthetic data created, which I think it roughly is. Part a) was extracted with a rule based heuristic, so can be considered a gold/silver standard. Flagging that this is the case in the captions (e.g., the subcaption, a/b) and the discussion is important. A provides the calibration to interpret b, and for some languages with the filter it's very close. I would suggest making the y scale on a the same as b also. (If the rule-based data isn't 100\% reliable, do we have gold standard data from Tydi or XOR that we can use to threshold, or is this unnecessary?)}

To assess the overall quality of our synthetic data, we randomly sample 1,000 examples from the silver pre-training data (\ourptdata) and few-shot synthetic data (\ourdata). These samples are evaluated using the \texttt{GPT-4o mini} to assess quality based on: 1) Fluency (0-2): assessing whether the query is understandable, readable, and free of spelling or grammatical mistakes; 2) Relevance (0-2): evaluating the alignment between the generated query-answer pair and the passage used for data generation. The prompts employed for quality assessment are included in Appendix Table~\ref{tab:llm_assess_prompt}.

% \edit{Figure~\ref{fig:quality_validation} demonstrates that our synthetic data is both fluent and well-grounded by the passages used for data generation. However, the overall quality in Bengali and Telugu is comparatively lower than in other languages. This discrepancy can be attributed to the reduced capabilities of the Gemma-7B model in these languages. In addition, we observe that \ourdata (\ie w/ Filter) consistently outperforms the raw, unfiltered dataset across all languages. This observation underscores the significant improvements in fluency and relevance following the filtering process, as supported by the ablation results reported in Table~\ref{tab:ablation}.}
Figure~\ref{fig:quality_validation} illustrates that both types of our generated queries exhibit fluency and strong grounding in the corresponding positive passages. The silver-standard \ourptdata, derived using heuristics from WikiData  (\S\ref{sec: mlwikiqa}), consistently achieves higher scores across both metrics in all languages compared to the unfiltered synthetic \ourdata (compare Figure~\ref{fig:quality_validation}~(a) versus (b)). However, the quality of \ourdata improves significantly after applying our tailored filtering mechanism (w/ Filter columns),  almost matching the quality and fluency scores for \ourptdata. This finding underscores the critical role of the filtering procedure in producing a synthetic dataset of quality comparable to the silver-standard dataset.

\subsection{Query Distribution Comparison}

To examine the distributional differences between our synthetic \ourdata and the gold-standard data in \textsc{Xor-TyDi QA}, we randomly sample up to 20,000 examples from both datasets and visualise their distributions using t-SNE~\citep{JMLR:v9:vandermaaten08a}, which projects the queries onto a two-dimensional space. Figure~\ref{fig:tsne_gold_vs_synthetic_ja} highlights several key findings: 1) The synthetic queries exhibit sufficient diversity, as they are scattered across the plot, indicating that our approach is capable of generating queries of various types using only five labelled examples. 2) The synthetic data shows significant overlap with the gold-standard data, demonstrating that it retains the core characteristics of the gold distribution. 3) The gold-standard data exhibits greater diversity than the synthetic data, suggesting that there is still room for improvement in enhancing diversity and variation during the data generation process, which we leave for future work. Similar findings are observed in the other languages (see Appendix Figure~\ref{fig:tsne_gold_vs_synthetic}).

\subsection{Safety}
We employ \texttt{Llama-Guard-2}\footnote{\url{https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B}} as the content safety classifier to assess the presence of unsafe content within our synthetic dataset. Our analysis reveals that 98.9\% of the 1,746,156 queries in \ourdata are classified as safe.