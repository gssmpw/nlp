\section{Related Work}
\paragraph{Pre-training for Open-domain QA.}
Open-domain QA requires retrieving relevant passages and extracting answers from them. This necessity has driven various methods that jointly train retrievers and readers. REALM~\citep{realm}, RAG~\citep{rag}, EMDR2~\citep{sachan2021endtoend}, YONO~\citep{lee-etal-2022-need}, ReAtt~\citep{jiang-etal-2022-retrieval}, and Atlas~\citep{reatt} first pre-train retrievers or initialise from pre-trained~\citep{izacard2022unsupervised} and fine-tuned retrievers. Subsequently, both components are fine-tuned jointly: the reader is trained using an answer generation loss, and the retriever is trained to promote passages that increase the likelihood of generating correct answers. Recently, this joint training mechanism has been adapted for multilingual open-domain QA~\citep{jiang-etal-2024-pre}, where retrievers are initially trained by learning from English teachers using multilingual parallel data, followed by a joint training stage with query-answer pairs generated by LLMs. Our approach follows this joint training paradigm for model pre-training but differs significantly. We use WikiData as a source to generate more informative natural questions and answers. Additionally, our pre-training method is more efficient by eliminating knowledge distillation from English models.

\paragraph{LLMs for Few-shot Data Generation.}
Prompting LLMs to generate synthetic data has been widely adopted to improve the performance of retrieval and QA tasks. UPR~\citep{sachan-etal-2022-improving} and InPars~\citep{bonifacio2022inpars} use zero-shot or few-shot prompting for passage reranking. \textsc{Promptagator}~\citep{dai2023promptagator} and SWIM-X~\citep{thakur-etal-2024-leveraging} prompt LLMs with few-shot examples to generate massive synthetic queries, either in English or in multiple languages, for retriever fine-tuning. Gecko~\citep{lee2024gecko} prompts LLMs to generate synthetic instructions and queries from Web documents and create high-quality labels for retriever fine-tuning. Beyond retrieval, LLMs are employed to generate QA data, where \textsc{QAmeleon}~\citep{qameleon} prompts a 540B LLM to generate multilingual QA pairs from only five examples. Nevertheless, these methods primarily focus on retrieval tasks and the more narrowly defined machine reading comprehension tasks. In our work, we rigorously investigate how LLMs can improve the more challenging multilingual open-domain QA tasks under few-shot settings. In addition, we explore zero-shot prompting, demonstrating that cross-lingual prompting using English data or limited multilingual data from held-out languages can yield results comparable to few-shot prompting, and we show this technique can also be leveraged for effective zero-shot language adaptation.