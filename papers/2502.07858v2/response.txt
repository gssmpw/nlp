\section{Related works}
Time series anomaly detection has evolved from traditional statistical models like ARIMA and Gaussian Processes**Liu, "An Empirical Study on Time Series Anomaly Detection Using Traditional Methods"**. Conventional methods like ARIMA and GP struggled with non-linear and high-dimensional data. Machine learning introduced more flexible approaches, such as **Bengio et al., "Learning Deep Architectures for AI"**, particularly One-Class SVM (OC-SVM), which excel in novelty detection by defining a boundary around standard data and identifying outliers as anomalies **Schölkopf et al., "Estimating the Support of a High-Dimensional Distribution"**. Despite this improvement, SVMs could not capture temporal dependencies inherent in sequential data.
Random Forests and  **Breiman, "Random Forests"**, handled high-dimensional data better than SVMs, while Isolation Forests identified anomalies by isolating points through recursive partitioning **Liu et al., "Isolation-Based Anomaly Detection"**. However, they focused on anomalies and were not optimized for capturing time dependencies in sequential data. RNNs and  **Hochreiter et al., "Long Short-Term Memory"**, became popular due to their ability to capture long-term dependencies in time series data. These models excelled in detecting anomalies in healthcare (e.g., ECG data) and finance **Chen et al., "Anomaly Detection in Time Series Data Using RNNs"**. However, RNNs and LSTMs face limitations in scalability and can struggle with vanishing gradient problems when processing long sequences.
Variational Autoencoders (VAEs) became central to unsupervised anomaly detection by reconstructing standard data patterns and flagging high-reconstruction errors as anomalies **Kingma et al., "Auto-Encoding Variational Bayes"**. Adversarial Autoencoders (AAEs) and Generative Adversarial Networks (GANs) enhanced robustness through adversarial training. GANs use a generator-discriminator framework to detect anomalies based on realistic data generation. However, both GANs and AEs require large datasets and can face instability, limiting their application in some domains **Goodfellow et al., "Generative Adversarial Networks"**.
Transformer-based models have recently demonstrated significant potential in time series anomaly detection by leveraging self-attention mechanisms to capture long-range dependencies **Vaswani et al., "Attention Is All You Need"**. Initially developed for NLP, these models eliminate the need for recurrent structures like RNNs or LSTMs, effectively modeling local and global patterns in time series data to detect anomalies.
Transformer-based models have revolutionized time series anomaly detection, with innovations like the **Anomaly Transformer** **Serrà et al., "Temporal Anomaly Detection Using Transformers"**, introducing Association Discrepancy to compare expected and observed associations in data. Using a minimax strategy enhances the distinguishability of anomalies, outperforming prior methods across datasets **Chen et al., "Minimax Temporal Anomaly Detection"**. Similarly, **AnomalyBERT** **Gao et al., "AnomalyBERT: Self-Supervised Anomaly Detection with Transformers"**, employs self-supervised learning and data degradation to simulate anomalies, improving generalization without labeled data **. The **Denoising Diffusion Mask Transformer (DDMT)** **Ho et al., "Denoising Diffusion for Time Series Anomaly Detection"**, integrates denoising diffusion with masking mechanisms, excelling in noisy multivariate settings **.
Multivariate anomaly detection has also advanced with models like **Informer** **Zhang et al., "Informer: A Novel Transformer-Based Model for Multivariate Time Series Forecasting"**, and multi-task Transformers, which leverage attention mechanisms to model interdependencies among variables **. Hybrid models combining statistical methods with deep learning enhance interpretability while maintaining flexibility. These developments address the limitations of classical met techniques as ARIMA, which struggled with high-dimensional, non-stationary data.
As the field progressed, deep learning frameworks began to emerge. In 2021, the **Anomaly Transformer** **Serrà et al., "Temporal Anomaly Detection Using Transformers"**, introduced a novel anomaly-attention mechanism that quantifies the association discrepancy between normal and abnormal points. While this approach effectively leverages self-attention to capture both local and global temporal dependencies in an unsupervised manner, its quadratic complexity poses challenges for scalability, and its underlying assumptions may not hold across all types of anomalies.
Building on these advances, the **DC Detector** **Chen et al., "Dual Attention Contrastive Representation Learning"**, employs a dual attention contrastive representation learning framework. Integrating local and global Attention with a contrastive loss overcomes some of the pitfalls of reconstruction-based methods. Nonetheless, the additional architectural complexity and the need for extensive hyperparameter tuning remain nontrivial hurdles.
Concurrently, selective state space models have gained traction for their efficiency. Models like **MAMBA** **Gu et al., "Modeling Long-Range Dependencies with Selective Scanning"**, use a selective scanning mechanism to model long-range dependencies linearly, making them attractive for real-time and large-scale applications. However, the trade-off is a potential flexibility reduction when modeling highly non-linear interactions.
Despite these significant advances, several challenges persist. One major issue is **concept drift**, the phenomenon where the underlying distribution of data evolves. This drift renders pre-trained models ineffective and highlights the need for adaptive learning techniques that can update dynamically as new data becomes available. In addition, scalability remains a critical challenge, especially when dealing with high-frequency time series data or real-time anomaly detection in large-scale systems. Recent efforts in developing memory-efficient Transformers and distributed learning approaches** **Zhang et al., "Distributed Learning of Anomaly Detection Models"**, have begun to address these scalability issues, marking promising steps toward more adaptive and robust anomaly detection systems.
Overall, the evolution of unsupervised time series anomaly detection reflects a broader trajectory across complex domains. The field is steadily progressing toward more nuanced, robust, and adaptable solutions, from early statistical methods to modern deep learning architectures that harness attention mechanisms, contrastive learning, and efficient state space representations. These advancements enhance our ability to capture intricate temporal dependencies and subtle deviations without relying on labeled data and pave the way for deploying these models in real-world, dynamic environments.