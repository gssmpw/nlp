\section{Related works}
Time series anomaly detection has evolved from traditional statistical models like ARIMA and Gaussian Processes\cite{box2015time,rasmussen2006gaussian}. Conventional methods like ARIMA and GP struggled with non-linear and high-dimensional data. Machine learning introduced more flexible approaches, such as  (SVMs), particularly One-Class SVM (OC-SVM), which excel in novelty detection by defining a boundary around standard data and identifying outliers as anomalies \cite{scholkopf2000support}. Despite this improvement, SVMs could not capture temporal dependencies inherent in sequential data.
Random Forests and  Isolation Forests handled high-dimensional data better than SVMs, while Isolation Forests identified anomalies by isolating points through recursive partitioning \cite{liu2008isolation}. However, they focused on anomalies and were not optimized for capturing time dependencies in sequential data. RNNs and  LSTMs became popular due to their ability to capture long-term dependencies in time series data. These models excelled in detecting anomalies in healthcare (e.g., ECG data) and finance \cite{distante2022hf}. However, RNNs and LSTMs face limitations in scalability and can struggle with vanishing gradient problems when processing long sequences.
Variational Autoencoders (VAEs) became central to unsupervised anomaly detection by reconstructing standard data patterns and flagging high-reconstruction errors as anomalies \cite{kingma2013auto}. Adversarial Autoencoders (AAEs) and Generative Adversarial Networks (GANs) enhanced robustness through adversarial training. GANs use a generator-discriminator framework to detect anomalies based on realistic data generation. However, both GANs and AEs require large datasets and can face instability, limiting their application in some domains \cite{schlegl2019f,li2019mad}.
Transformer-based models have recently demonstrated significant potential in time series anomaly detection by leveraging self-attention mechanisms to capture long-range dependencies \cite{vaswani2017attention}. Initially developed for NLP, these models eliminate the need for recurrent structures like RNNs or LSTMs, effectively modeling local and global patterns in time series data to detect anomalies.
Transformer-based models have revolutionized time series anomaly detection, with innovations like the Anomaly Transformer introducing Association Discrepancy to compare expected and observed associations in data. Using a minimax strategy enhances the distinguishability of anomalies, outperforming prior methods across datasets \cite{xu2022anomaly}. Similarly, AnomalyBERT employs self-supervised learning and data degradation to simulate anomalies, improving generalization without labeled data \cite{jeong2023anomalybert}. The Denoising Diffusion Mask Transformer (DDMT) integrates denoising diffusion with masking mechanisms, excelling in noisy multivariate settings \cite{yang2023ddmt}.
Multivariate anomaly detection has also advanced with models like Informer \cite{zhou2021informer} and multi-task Transformers, which leverage attention mechanisms to model interdependencies among variables \cite{zhang2021multitask}. Hybrid models combining statistical methods with deep learning enhance interpretability while maintaining flexibility. These developments address the limitations of classical met techniques as ARIMA, which struggled with high-dimensional, non-stationary data.
As the field progressed, deep learning frameworks began to emerge. In 2021, the \textbf{Anomaly Transformer}~\cite{xu2022anomaly} introduced a novel anomaly-attention mechanism that quantifies the association discrepancy between normal and abnormal points. While this approach effectively leverages self-attention to capture both local and global temporal dependencies in an unsupervised manner, its quadratic complexity poses challenges for scalability, and its underlying assumptions may not hold across all types of anomalies.
Building on these advances, the \textbf{DC Detector}~\cite{yang2023dcdetector} employs a dual attention contrastive representation learning framework. Integrating local and global Attention with a contrastive loss overcomes some of the pitfalls of reconstruction-based methods. Nonetheless, the additional architectural complexity and the need for extensive hyperparameter tuning remain nontrivial hurdles.
Concurrently, selective state space models have gained traction for their efficiency. Models like \textbf{MAMBA}~\cite{gu2023mamba} use a selective scanning mechanism to model long-range dependencies linearly, making them attractive for real-time and large-scale applications. However, the trade-off is a potential flexibility reduction when modeling highly non-linear interactions.
Despite these significant advances, several challenges persist. One major issue is \textbf{concept drift}, the phenomenon where the underlying distribution of data evolves. This drift renders pre-trained models ineffective and highlights the need for adaptive learning techniques that can update dynamically as new data becomes available. In addition, scalability remains a critical challenge, especially when dealing with high-frequency time series data or real-time anomaly detection in large-scale systems. Recent efforts in developing memory-efficient Transformers and distributed learning approaches~\cite{gupta2021memory,lai2021revisiting} have begun to address these scalability issues, marking promising steps toward more adaptive and robust anomaly detection systems.
Overall, the evolution of unsupervised time series anomaly detection reflects a broader trajectory across complex domains. The field is steadily progressing toward more nuanced, robust, and adaptable solutions, from early statistical methods to modern deep learning architectures that harness attention mechanisms, contrastive learning, and efficient state space representations. These advancements enhance our ability to capture intricate temporal dependencies and subtle deviations without relying on labeled data and pave the way for deploying these models in real-world, dynamic environments.