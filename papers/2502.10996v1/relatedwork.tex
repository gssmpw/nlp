\section{Related Work}
\textbf{Retrieval-Augmented Generation.} Retrieval-Augmented Generation (RAG) \citep{lewis2020retrieval, guu2020retrieval} enhances performance on knowledge-intensive tasks by incorporating retrieved passages into input of large language models (LLMs), improving factual accuracy and grounding. Traditional approaches retrieved a fixed number of passages once before generation \citep{shao2023enhancing, es-etal-2024-ragas, lyu2024crudragcomprehensivechinesebenchmark} or focused on adaptive retrieval \cite{jiang2023active}, where models dynamically query external knowledge when confidence is low, or analyze on the retrieval \cite{kim2024sure}. However, these methods often fail to perform complex or multi-hop reasoning process due to the linear and simple designs, where several recent methods have emerged to improve retrieval quality. Iterative retrieval-generation approaches \cite{shao2023enhancing,guan2024amor} leverage historical context to enhance subsequent retrievals. Other work introduces targeted retrieval strategies through subqueries \cite{khattab2023demonstratesearchpredict} or follow-up questions \cite{yao2023react,press2023measuring}, progressively enriching the context for more factually grounded generation. SelfRAG \cite{asai2023self} introduced self-reflective RAG, which allows LLMs to retrieve content on demand and critically evaluate retrievals, demonstrating both the importance of retrieval quality and the capacity of LLMs to seek self-directed information. RPG \cite{lyu2024retrieve} further refined this approach by extracting fine-grained paragraphs from retrieved content to improve query relevance.
However, existing methods face key limitations. They often either include irrelevant content in the retrieved context or risk discarding essential information through refinement. Additionally, dense retrieval approaches can be computationally expensive when deployed on large corpora, hampering real-time user interaction. To address these challenges, we propose converting retrieved text into knowledge-intensive graphs and maintaining an iteratively enriched, query-specific knowledge graph for knowledge serving. We also introduce theme-scoped retrieval for more focused and efficient context retrieval.

% Although such strategies can significantly improve task performance, they often introduce additional runtime costs, risk distractions from irrelevant context, and lack robust attribution mechanisms.

\textbf{Graph as Context for LLMs.}
The integration of graphs into modern language models represents a promising research direction \cite{yasunaga2021qa,yasunaga2022deep,yu2022retrieval, ju2022grape, zhu2024realm,gutierrez2024hipporag}. Recent approaches leverage graph-based knowledge representation to enhance LLM performance. For instance, GraphToken \cite{perozzi2024let} shows that LLM can understand a graph encoded as a token, after a simple p-tuning \cite{liu2021p} while keeping the LLM's parameters frozen. Based on the findings, G-Retriever \cite{he2024gretriever} constructs subgraphs by retrieving relevant entities and relations from a global KG, using these as structured knowledge input for LLMs. 
% They also find that feeding textualized graphs along with the graph token can further boost the performance. 
GraphRAG \cite{edge2024local} employs an alternative strategy, extracting a comprehensive global KG from the corpus, identifying meaningful clusters through graph community detection, and generating LLM-based summaries of these communities to create a refined retrieval corpus. 
These methods demonstrate how graphs, with their knowledge-intensive modality and inherent structural properties, can significantly improve LLMs' contextual understanding. However, current approaches treat graphs as static data structures without refinement based on LLM requirements. Moreover, building global KGs for large-scale corpora remains prohibitively expensive and often impractical -- Wikipedia alone contains over 30 million documents per dump. The key challenge lies in developing methods to dynamically construct and utilize graphs on-the-fly for knowledge-intensive generation.
To address these limitations, we propose a retrieval-and-structuring paradigm that dynamically builds and maintains query-specific KGs on-the-fly. Our approach iteratively enriches these graphs through LLM's self-evaluation of information completeness and targeted retrieval via focused subqueries.