% Requires packages: booktabs, multirow
\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
&   TQA &2WQA  & Pub   & \multicolumn{2}{c}{ASQA}  \\
&  (acc) &(F1)  & (acc)   & (rg)  & (mv) \\
\midrule
RAS$_{\text{7B}}$ &72.6 &42.1 &74.8 &37.2 &95.2 \\
\midrule
\textit{Training Phase} \\
No GraphToken &70.2 &38.4   & 66.4 &33.1 &85.0 \\
No LoRA &71.5 &37.8 &54.8  &32.8 &84.8\\
No Text-to-Triple &70.4   &38.2  &71.4  &36.2 & 73.8\\
No Mutli-Task & 68.6  & 39.2   &65.5 &36.7 &88.9\\
\midrule
\textit{Inference Phase} \\
No Retrieval &56.9 &27.4  &69.0 &31.3 &70.6\\
No GraphToken &68.8 &38.7  &67.3 &36.5 &93.6\\
No Planning &66.7 &37.8   &71.5 &-- &--\\
No ThemeScope &72.0  &-- &73.9 &-- &--\\

\bottomrule
\end{tabular}
}
\caption{\textbf{Ablations in Training and Inference (with RAS$_{7\text{B}}$).} \underline{\textbf{Training}}: ``No GraphToken'' removes the graph encoder, using only LoRA-based LLM fine-tuning. ``No LoRA'' uses graph token optimization without low-rank adaptation. ``No Text-to-Triple'' keeps the original retrieved text instead of constructing KGs. ``No Multi-Task'' trains separate action planner and answerer rather than training a single model. \underline{\textbf{Inference}}: ``No Retrieval'' tests direct query answering without any context. ``No GraphToken'' removes graph structure during inference, using only textual context. ``No Planning'' uses single retrieval. ``No ThemeScope'' bypasses theme-aware retrieval, using only dense retrieval.}

\label{tb:ablation}
% \vspace{-0.5em}
\end{table}