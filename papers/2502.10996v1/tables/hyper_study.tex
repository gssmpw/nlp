\begin{table*}[!h]
\centering
\caption{Summary of hyper-parameter study for the experimental setup. We \textbf{highlight} the setting used in experiments.}
\begin{tabular}{ll}
\toprule
\textbf{Hyper-parameter} & \textbf{Studied Values} \\
\midrule
\textbf{Theme Classifier} \\
Encoder learning rate & \{1e-6, 1e-5, \textbf{2e-5}, 1e-4, 2e-4\}  \\
Classifier learning rate & \{1e-6, 1e-5, 2e-5, 1e-4, \textbf{2e-4}, 5e-4, 1e-3\}  \\
Batch size & \{4, 8, \textbf{16}, 32, 64\}\\
Hidden layers & \{\textbf{1}, 2, 3\}  \\
Hidden dim & \{256, \textbf{512}, 1024\} \\
Dropout rate & \{\textbf{0.1}, 0.2, 0.3\} \\

\midrule
\textbf{Distribution Shifter} \\
Batch size & \{4, 8, 16, \textbf{32}, 64\} \\
Learning rate & \{0.0001, 0.0005, \textbf{0.001}, 0.005\} \\
Hidden layers & \{[256], [512], [\textbf{512, 256}], [1024, 512, 256]\} \\
Dropout rate & \{0.1, 0.15, \textbf{0.2}, 0.25, 0.3\} \\
Optimizer & \{\textbf{Adam}, SGD, RMSprop, AdamW\} \\
Loss function & \{\textbf{KLDiv}, JSD, MSE, Wasserstein, CrossEntropy\} \\

\midrule
\textbf{Text-to-Triples Model} \\
Batch size & \{2, 4, \textbf{8}, 16, 32\} \\
Learning rate & \{1e-5, 2e-5, \textbf{5e-5}, 1e-4, 2e-4\} \\


\midrule
\textbf{GraphLLM (Action Planner \& Answerer)} \\
\hdashline
\multicolumn{2}{l}{\textit{GNN Setting (for Graph Token)}} \\
GNN architecture & \{GCN, GAT, \textbf{Graph Transformer}\} \\
Hidden dimension & \{512, 768, \textbf{1024}, 2048\} \\
Number of layers & \{2, \textbf{3}, 4, 5\} \\
Number of heads & \{4, 6, \textbf{8}, 12\} \\
Dropout rate & \{0.05, \textbf{0.1}, 0.2, 0.3\} \\
Projector intermediate dimension & \{1024, \textbf{2048}, 4096\} \\
Projector output dimension & \textbf{4096} \\
\hdashline
\textit{LoRA Setting} \\
LoRA rank (r) & \{4, \textbf{8}, 16, 32\} \\
LoRA alpha & \{8, \textbf{16}, 32\} \\
LoRA dropout & \{0.01, \textbf{0.05}, 0.1, 0.15\} \\
\hdashline
\multicolumn{2}{l}{\textit{General Setting}} \\
Learning rate & \{1e-6, 2e-6, \textbf{5e-5}, 1e-4\} \\
Batch size (training) & \{\textbf{2}, 4, 8, 16, 32\} \\
Batch size (inference) & \textbf{1} \\
Weight decay & \{0.001, \textbf{0.01}, 0.05, 0.1\} \\
Gradient accumulation steps & \{\textbf{2}, 4, 8, 16\} \\
Gradient clipping & \{0.1, 0.3, \textbf{0.5}, 1.0\} \\
Warmup ratio & \{0.05, 0.1, \textbf{0.15}, 0.2\} \\
Max text length & \textbf{2500} \\
Max new tokens & Task-specific (see Appendix \ref{apsub:open_setting}) \\
\midrule
\textbf{Others} \\
Theme scoping top-$K$ & \{100,000, \textbf{250,000}, 500,000 \}\\
Dense retrieval top-$k$ & \textbf{5} \\


\bottomrule



\end{tabular}

\label{tb:hyperparams}
\end{table*}