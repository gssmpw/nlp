@inproceedings{yu2022retrieval,
  title={Retrieval-augmented generation across heterogeneous knowledge},
  author={Yu, Wenhao},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop},
  pages={52--58},
  year={2022}
}

@article{zhu2024realm,
      title={{REALM}: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models}, 
      author={Yinghao Zhu and Changyu Ren and Shiyun Xie and Shukai Liu and Hangyuan Ji and Zixiang Wang and Tao Sun and Long He and Zhoujun Li and Xi Zhu and Chengwei Pan},
      journal={arXiv:2402.07016},
     year={2024},
}

@article{ju2022grape,
  title={Grape: Knowledge graph enhanced passage reader for open-domain question answering},
  author={Ju, Mingxuan and Yu, Wenhao and Zhao, Tong and Zhang, Chuxu and Ye, Yanfang},
  journal={arXiv preprint arXiv:2210.02933},
  year={2022}
}

# ReAct
@article{yao2023react,
      title={{ReAct}: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2023},
      journal={arXiv:2210.03629}
}

# Self-Ask
@article{press2023measuring,
  title={Measuring and narrowing the compositionality gap in language models},
  author={Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2210.03350},
  year={2022}
}

#DSP
@article{khattab2023demonstratesearchpredict,
      title={Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP}, 
      author={Omar Khattab and Keshav Santhanam and Xiang Lisa Li and David Hall and Percy Liang and Christopher Potts and Matei Zaharia},
     journal={arXiv preprint arXiv:2212.14024},
  year={2023}
}

@article{jiang2023active,
  title={Active retrieval augmented generation},
  author={Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2305.06983},
  year={2023}
}

#Iter-RetGen
@article{shao2023enhancing,
  title={Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy},
  author={Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
  journal={arXiv preprint arXiv:2305.15294},
  year={2023}
}

# Amor
@article{guan2024amor,
  title={AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback},
  author={Guan, Jian and Wu, Wei and Wen, Zujie and Xu, Peng and Wang, Hongning and Huang, Minlie},
  journal={arXiv preprint arXiv:2402.01469},
  year={2024}
}

@article{Wu2024stark,
  author = {Shirley Wu and Shiyu Zhao and Michihiro Yasunaga and Kexin Huang and Kaidi Cao and Qian Huang and Vassilis N. Ioannidis and Karthik Subbian and James Zou and Jure Leskovec},
  title = {{STARK}: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases},
  month = April,
  year = 2024,
 journal = {https://arxiv.org/pdf/2404.13207}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}

# CoT
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Scott Johnston and Sheer El-Showk and Andy Jones and Nelson Elhage and Tristan Hume and Anna Chen and Yuntao Bai and Sam Bowman and Stanislav Fort and Deep Ganguli and Danny Hernandez and Josh Jacobson and Jackson Kernion and Shauna Kravec and Liane Lovitt and Kamal Ndousse and Catherine Olsson and Sam Ringer and Dario Amodei and Tom Brown and Jack Clark and Nicholas Joseph and Ben Mann and Sam McCandlish and Chris Olah and Jared Kaplan},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li2023making,
  title={Making language models better reasoners with step-aware verifier},
  author={Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5315--5333},
  year={2023}
}

@article{zhu2022solving,
  title={Solving math word problems via cooperative reasoning induced language models},
  author={Zhu, Xinyu and Wang, Junjie and Zhang, Lin and Zhang, Yuxiang and Huang, Yongfeng and Gan, Ruyi and Zhang, Jiaxing and Yang, Yujiu},
  journal={arXiv preprint arXiv:2210.16257},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and  John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

# Self-RAG
@inproceedings{asai2023self,
  title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

# BBox-Adapter
@article{sun2024bbox,
  title={BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models},
  author={Sun, Haotian and Zhuang, Yuchen and Wei, Wei and Zhang, Chao and Dai, Bo},
  journal={arXiv preprint arXiv:2402.08219},
  year={2024}
}

# G-Retriever
@article{he2024gretriever,
      title={G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering}, 
      author={Xiaoxin He and Yijun Tian and Yifei Sun and Nitesh V. Chawla and Thomas Laurent and Yann LeCun and Xavier Bresson and Bryan Hooi},
      year={2024},
      journal={arXiv preprint arXiv:2402.07630}
}

# GraphRAG
@article{edge2024local,
      title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization}, 
      author={Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Jonathan Larson},
      year={2024},
      journal={arXiv preprint arXiv:2404.16130},
}

# Leiden
@article{traag2019louvain,
  title={From Louvain to Leiden: guaranteeing well-connected communities},
  author={Traag, Vincent A and Waltman, Ludo and Van Eck, Nees Jan},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={1--12},
  year={2019},
  publisher={Nature Publishing Group}
}

# GPT
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

# palm
@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

# RAG
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{izacard-grave-2021-leveraging,
    title = "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
    author = "Izacard, Gautier  and
      Grave, Edouard",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.74",
    doi = "10.18653/v1/2021.eacl-main.74",
    pages = "874--880",
    abstract = "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.",
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{ling2024deductive,
  title={Deductive verification of chain-of-thought reasoning},
  author={Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao and Lee, Mingu and Memisevic, Roland and Su, Hao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@article{wang-etal-2022-measure,
  title={Measure and improve robustness in NLP models: A survey},
  author={Wang, Xuezhi and Wang, Haohan and Yang, Diyi},
  journal={arXiv preprint arXiv:2112.08313},
  year={2021}
}

@article{sun2019ernie,
  title={Ernie: Enhanced representation through knowledge integration},
  author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},
  journal={arXiv preprint arXiv:1904.09223},
  year={2019}
}

@article{yu2022survey,
  title={A Survey of Knowledge-Enhanced Text Generation},
  author={Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  journal={ACM Computing Survey (CSUR)},
  year={2022}
}

@article{hogan2021knowledge,
  title={Knowledge graphs},
  author={Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and dâ€™Amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, Jos{\'e} Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and others},
  journal={ACM Computing Surveys (Csur)},
  volume={54},
  number={4},
  pages={1--37},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{meng2022gnn,
  title={GNN-LM: Language Modeling Based on Global Contexts via GNN},
  author={Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
  booktitle={ICLR 2022 Workshop on Deep Learning on Graphs for Natural Language Processing}
}

@inproceedings{logan-etal-2019-baracks,
    title = "{B}arack{'}s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling",
    author = "Logan, Robert  and
      Liu, Nelson F.  and
      Peters, Matthew E.  and
      Gardner, Matt  and
      Singh, Sameer",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1598",
    doi = "10.18653/v1/P19-1598",
    pages = "5962--5971",
    abstract = "Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model{'}s ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts.",
}

@inproceedings{maynez-etal-2020-faithfulness,
    title = "On Faithfulness and Factuality in Abstractive Summarization",
    author = "Maynez, Joshua  and
      Narayan, Shashi  and
      Bohnet, Bernd  and
      McDonald, Ryan",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.173",
    doi = "10.18653/v1/2020.acl-main.173",
    pages = "1906--1919",
    abstract = "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
}

@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{nguyen2016ms,
  title={Ms marco: A human-generated machine reading comprehension dataset},
  author={Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
  year={2016}
}

@article{fan2019eli5,
  title={ELI5: Long form question answering},
  author={Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
  journal={arXiv preprint arXiv:1907.09190},
  year={2019}
}

@article{jin2019pubmedqa,
  title={Pubmedqa: A dataset for biomedical research question answering},
  author={Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William W and Lu, Xinghua},
  journal={arXiv preprint arXiv:1909.06146},
  year={2019}
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@unpublished{spacy2,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}

@misc{ma2023dreeamguidingattentionevidence,
      title={DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction}, 
      author={Youmi Ma and An Wang and Naoaki Okazaki},
      year={2023},
      eprint={2302.08675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.08675}, 
}

@misc{hamilton2018inductiverepresentationlearninglarge,
      title={Inductive Representation Learning on Large Graphs}, 
      author={William L. Hamilton and Rex Ying and Jure Leskovec},
      year={2018},
      eprint={1706.02216},
      archivePrefix={arXiv},
      primaryClass={cs.SI},
      url={https://arxiv.org/abs/1706.02216}, 
}

@article{kim2024ontology,
  title={Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model},
  author={Kim, Daehee and Kang, Deokhyung and Ryu, Sangwon and Lee, Gary Geunbae},
  journal={arXiv preprint arXiv:2409.07088},
  year={2024}
}

@article{reimers2019sentence,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, N},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{perozzi2024let,
  title={Let your graph do the talking: Encoding structured data for llms},
  author={Perozzi, Bryan and Fatemi, Bahare and Zelle, Dustin and Tsitsulin, Anton and Kazemi, Mehran and Al-Rfou, Rami and Halcrow, Jonathan},
  journal={arXiv preprint arXiv:2402.05862},
  year={2024}
}

@article{he2024g,
  title={G-retriever: Retrieval-augmented generation for textual graph understanding and question answering},
  author={He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan},
  journal={arXiv preprint arXiv:2402.07630},
  year={2024}
}


@inproceedings{lyu2024retrieve,
  title={Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation},
  author={Lyu, Yuanjie and Niu, Zihan and Xie, Zheyong and Zhang, Chao and Xu, Tong and Wang, Yang and Chen, Enhong},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={4683--4702},
  year={2024}
}

@inproceedings{ji2023towards,
  title={Towards mitigating LLM hallucination via self reflection},
  author={Ji, Ziwei and Yu, Tiezheng and Xu, Yan and Lee, Nayeon and Ishii, Etsuko and Fung, Pascale},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={1827--1843},
  year={2023}
}

@article{gao2023enabling,
  title={Enabling large language models to generate text with citations},
  author={Gao, Tianyu and Yen, Howard and Yu, Jiatong and Chen, Danqi},
  journal={arXiv preprint arXiv:2305.14627},
  year={2023}
}

@article{luo2023sail,
  title={Sail: Search-augmented instruction learning},
  author={Luo, Hongyin and Chuang, Yung-Sung and Gong, Yuan and Zhang, Tianhua and Kim, Yoon and Wu, Xixin and Fox, Danny and Meng, Helen and Glass, James},
  journal={arXiv preprint arXiv:2305.15225},
  year={2023}
}

@misc{lyu2024crudragcomprehensivechinesebenchmark,
      title={CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models}, 
      author={Yuanjie Lyu and Zhiyu Li and Simin Niu and Feiyu Xiong and Bo Tang and Wenjin Wang and Hao Wu and Huanyong Liu and Tong Xu and Enhong Chen},
      year={2024},
      eprint={2401.17043},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.17043}, 
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@article{izacard2021unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:2112.09118},
  year={2021}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{es-etal-2024-ragas,
    title = "{RAGA}s: Automated Evaluation of Retrieval Augmented Generation",
    author = "Es, Shahul  and
      James, Jithin  and
      Espinosa Anke, Luis  and
      Schockaert, Steven",
    editor = "Aletras, Nikolaos  and
      De Clercq, Orphee",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-demo.16/",
    pages = "150--158"
}

@article{gutierrez2024hipporag,
  title={HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models},
  author={Guti{\'e}rrez, Bernal Jim{\'e}nez and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
  journal={arXiv preprint arXiv:2405.14831},
  year={2024}
}

@article{yasunaga2021qa,
  title={QA-GNN: Reasoning with language models and knowledge graphs for question answering},
  author={Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
  journal={arXiv preprint arXiv:2104.06378},
  year={2021}
}

@article{yasunaga2022deep,
  title={Deep bidirectional language-knowledge graph pretraining},
  author={Yasunaga, Michihiro and Bosselut, Antoine and Ren, Hongyu and Zhang, Xikun and Manning, Christopher D and Liang, Percy S and Leskovec, Jure},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37309--37323},
  year={2022}
}


@article{liu2021p,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@article{shi2020masked,
  title={Masked label prediction: Unified message passing model for semi-supervised classification},
  author={Shi, Yunsheng and Huang, Zhengjie and Feng, Shikun and Zhong, Hui and Wang, Wenjin and Sun, Yu},
  journal={arXiv preprint arXiv:2009.03509},
  year={2020}
}

@article{lehmann2015dbpedia,
  title={Dbpedia--a large-scale, multilingual knowledge base extracted from wikipedia},
  author={Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo N and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef, Patrick and Auer, S{\"o}ren and others},
  journal={Semantic web},
  volume={6},
  number={2},
  pages={167--195},
  year={2015},
  publisher={IOS Press}
}

@article{douze2024faiss,
  title={The faiss library},
  author={Douze, Matthijs and Guzhva, Alexandr and Deng, Chengqi and Johnson, Jeff and Szilvasy, Gergely and Mazar{\'e}, Pierre-Emmanuel and Lomeli, Maria and Hosseini, Lucas and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2401.08281},
  year={2024}
}

@article{mallen2022not,
  title={When not to trust language models: Investigating effectiveness of parametric and non-parametric memories},
  author={Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10511},
  year={2022}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{ho2020constructing,
  title={Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps},
  author={Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},
  journal={arXiv preprint arXiv:2011.01060},
  year={2020}
}

@article{zhang2023interpretable,
  title={Interpretable unified language checking},
  author={Zhang, Tianhua and Luo, Hongyin and Chuang, Yung-Sung and Fang, Wei and Gaitskell, Luc and Hartvigsen, Thomas and Wu, Xixin and Fox, Danny and Meng, Helen and Glass, James},
  journal={arXiv preprint arXiv:2304.03728},
  year={2023}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{stelmakh2022asqa,
  title={ASQA: Factoid questions meet long-form answers},
  author={Stelmakh, Ivan and Luan, Yi and Dhingra, Bhuwan and Chang, Ming-Wei},
  journal={arXiv preprint arXiv:2204.06092},
  year={2022}
}


@article{pillutla2021mauve,
  title={Mauve: Measuring the gap between neural text and human text using divergence frontiers},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4816--4828},
  year={2021}
}

@article{kim2024sure,
  title={SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs},
  author={Kim, Jaehyung and Nam, Jaehyun and Mo, Sangwoo and Park, Jongjin and Lee, Sang-Woo and Seo, Minjoon and Ha, Jung-Woo and Shin, Jinwoo},
  journal={arXiv preprint arXiv:2404.13081},
  year={2024}
}

@article{xu2023recomp,
  title={Recomp: Improving retrieval-augmented lms with compression and selective augmentation},
  author={Xu, Fangyuan and Shi, Weijia and Choi, Eunsol},
  journal={arXiv preprint arXiv:2310.04408},
  year={2023}
}

@inproceedings{jiang2024kgfit,
  title = {KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge},
  author = {Jiang, Pengcheng and Cao, Lang and Xiao, Cao and Bhatia, Parminder and Sun, Jimeng and Han, Jiawei},
  booktitle = {The Thirty-Eighth Annual Conference on Neural Information Processing Systems},
  year = {2024},
}

@misc{anthropic2024claude,
  title = {Introducing Claude 3.5 Sonnet},
  author = {{Anthropic}},
  year = {2024},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-5-sonnet}},
  note = {Accessed: 2025-01-06}
}

@article{nussbaum2024nomic,
  title={Nomic embed: Training a reproducible long context text embedder},
  author={Nussbaum, Zach and Morris, John X and Duderstadt, Brandon and Mulyar, Andriy},
  journal={arXiv preprint arXiv:2402.01613},
  year={2024}
}

@article{min2019discrete,
  title={A discrete hard EM approach for weakly supervised question answering},
  author={Min, Sewon and Chen, Danqi and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1909.04849},
  year={2019}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{izacard2023atlas,
  title={Atlas: Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={251},
  pages={1--43},
  year={2023}
}