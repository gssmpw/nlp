%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{arydshln}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Core packages for algorithm and listings

% \usepackage{algorithm}
% \usepackage{algorithmicx}
% \usepackage[noend]{algpseudocode}


\usepackage{listings}
\usepackage{xcolor}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand\jiawei[1]{\textcolor{purple}{jh: #1}}

\let\originaladdcontentsline\addcontentsline

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

\usepackage[preprint]{icml2025}


% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\definecolor{secondclose}{rgb}{0.32, 0.09, 0.98}
\definecolor{firstclose}{rgb}{0.32, 0.09, 0.98}
\definecolor{secondopen}{rgb}{0.81, 0.09, 0.13}
\definecolor{firstopen}{rgb}{0.81, 0.09, 0.13}
\definecolor{fireenginered}{rgb}{0.81, 0.09, 0.13}
\definecolor{hanpurple}{rgb}{0.32, 0.09, 0.98}
\definecolor{frenchblue}{rgb}{0.0, 0.45, 0.73}
\definecolor{fireenginered}{rgb}{0.81, 0.09, 0.13}
\definecolor{cobalt}{rgb}{0.0, 0.28, 0.67}
\definecolor{palatinateblue}{rgb}{0.15, 0.23, 0.89}
\definecolor{darkorchid}{rgb}{0.6, 0.2, 0.8}
\definecolor{trueblue}{rgb}{0.0, 0.45, 0.81}
\definecolor{ultramarine}{rgb}{0.07, 0.04, 0.56}
\definecolor{seagreen}{rgb}{0.18, 0.55, 0.34}

\hypersetup{
    colorlinks=true,
    linkcolor=hanpurple,
    citecolor=seagreen,
    urlcolor=seagreen,
}

% \usepackage{minted}
% \usepackage{inconsolata}
% \usepackage{anyfontsize}
% \usepackage{helvet}  % For Arial-like font (Helvetica)

% \setminted{
%     escapeinside=||,
%     fontsize=\small,
%     baselinestretch=1.2,
%     frame=lines,
%     framesep=3mm,
%     framerule=2pt,
%     bgcolor=gray!3,
%     rulecolor=gray!60,
%     xleftmargin=15pt,
%     fontfamily=helvetica,  % Changed from tt to helvetica
%     style=bw,
%     mathescape=true,
%     breaklines=true,
%     breakbytoken=false,
%     breaksymbolleft=,
%     breaksymbolright=
% }

\lstset{
    basicstyle=\small\fontfamily{phv}\selectfont,
    breaklines=true,
    frame=lines,
    framesep=3mm,
    framerule=2pt,
    rulecolor=\color{gray!60},
    backgroundcolor=\color{gray!3},
    xleftmargin=10pt,
    xrightmargin=10pt,
    mathescape=true,
    escapeinside={|}{|},
    keepspaces=true,
    columns=fullflexible,    % This helps preserve exact spacing
    tabsize=4,
    showspaces=false,
    showtabs=false,
    breakatwhitespace=false,
    prebreak={},             % No special character at line breaks
    postbreak={},            % No special character after line breaks
    breakindent=0pt          % No indentation for broken lines
}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}



\makeatletter
\AtBeginDocument{%
  \let\oldref\ref
  \renewcommand{\ref}[1]{%
    \hyperref[{#1}]{\underline{\oldref{#1}}}%
  }%
}
\makeatother

\let\addcontentsline\originaladdcontentsline
\usepackage{titletoc}
\newcommand\DoToC{%
  \startcontents
  \printcontents{}{1}{\textbf{Contents of Appendix}\vskip3pt\hrule\vskip5pt}
  \vskip3pt\hrule\vskip5pt
}

\makeatletter
\newcommand{\btriangle}{\mathpalette\btriangle@\relax}
\newcommand{\btriangle@}[2]{%
  \begingroup
  \sbox\z@{$\m@th#1\triangle$}%
  \makebox[\wd\z@]{%
    \raisebox{0.04\height}{%
      \resizebox{1.1\wd\z@}{0.96\ht\z@}{%
        $\m@th#1\blacktriangle$%
      }%
    }%
  }%
  \endgroup
}
\makeatother

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM  Generation}

\begin{document}

\twocolumn[
\icmltitle{RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM  Generation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}
% 1. Symbol settings
\icmlsetsymbol{equal}{*}

% 2. Author list
\begin{icmlauthorlist}
\icmlauthor{Pengcheng Jiang}{u}
\icmlauthor{Lang Cao}{u}
\icmlauthor{Ruike Zhu}{u}
\icmlauthor{Minhao Jiang}{u}
\icmlauthor{Yunyi Zhang}{u}
% \icmlauthor{Jiaming Shen}{google}
\icmlauthor{Jimeng Sun}{u}
\icmlauthor{Jiawei Han}{u}
\end{icmlauthorlist}

% 3. Affiliations
\icmlaffiliation{u}{Department of Computer Science, University of Illinois Urbana-Champaign}
% \icmlaffiliation{google}{Google Deepmind}

% 4. Corresponding authors
\icmlcorrespondingauthor{Pengcheng Jiang}{pj20@illinois.edu}
\icmlcorrespondingauthor{Jiawei Han}{hanj@illinois.edu}







% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% \jiawei{The title could be a little long and not so easy to understand.  What about: ``RAS: Retrieval-Augmented-Structuring for Knowledge-Enhanced LLM  Generation"?
% }

\begin{abstract}
Retrieval-augmented language models often struggle with knowledge-intensive tasks due to inefficient retrieval, unstructured knowledge integration, and single-pass architectures. We present Retrieval-And-Structuring (RAS), a novel framework that dynamically constructs and reasons over query-specific knowledge graphs through iterative retrieval and structuring. RAS introduces four key technical innovations: (1) a theme-scoped retrieval mechanism that efficiently narrows the search space while maintaining retrieval quality, (2) an action planning module that determines knowledge needs and generates focused sub-queries, (3) a dynamic knowledge structuring approach that converts retrieved text into an evolving knowledge graph, and (4) a graph-augmented answering component that leverages the accumulated structured information. Our framework achieves state-of-the-art performance, surpassing leading baselines by 6.4\% with open-source language models and 7.0\% with proprietary models on seven knowledge-intensive generation datasets across all evaluation metrics. Detailed ablation studies verify the contribution of each technical component to the overall system performance.
\end{abstract}

\section{Introduction}
% Large language models (LLMs) have achieved remarkable performance across a wide range of natural language processing tasks, leveraging the rich knowledge and patterns captured from vast training corpora \cite{devlin2018bert,brown2020language,chowdhery2023palm}. However, despite their impressive capabilities, LLMs often struggle to reason accurately on knowledge-intensive tasks due to the absence of precise, logically structured knowledge \cite{rae2021scaling,ling2024deductive}. This limitation has prompted the research community to explore methods for augmenting LLMs with structured knowledge to enhance their reasoning capabilities \cite{wang-etal-2022-measure}.
Complex reasoning tasks like scientific analysis or multi-hop question answering demand both comprehensive knowledge and structured logical thinking. While large language models (LLMs) have achieved remarkable performance across natural language processing tasks \cite{devlin2018bert,brown2020language,chowdhery2023palm}, they often struggle with knowledge-intensive reasoning due to the absence of precise, logically structured information \cite{rae2021scaling,ling2024deductive}. This limitation has prompted growing research into augmenting LLMs with structured knowledge to enhance their reasoning capabilities \cite{wang-etal-2022-measure}.


% Retrieval-augmented generation (RAG) approaches have emerged as a promising direction, providing LLMs with additional context from the retrieved passages \cite{guu2020retrieval,lewis2020retrieval,izacard-grave-2021-leveraging,he2024gretriever}. However, these methods often face the challenge of hallucination \cite{maynez-etal-2020-faithfulness,zhang2023siren}, where the generated content is not grounded in the retrieved information. This issue largely stems from the unstructured nature of the retrieved passages, which may lack the logical connections and granularity necessary for precise reasoning. To address these limitations, recent efforts have focused on integrating structured knowledge, such as knowledge graphs (KGs), into LLMs \cite{sun2019ernie,yu2022survey,he2024gretriever,edge2024local}. KGs offer a compact and logically structured representation of entities and their relationships, enabling more accurate and interpretable reasoning \cite{hogan2021knowledge,jiang2024kgfit}.

Retrieval-augmented generation (RAG) approaches have emerged as a promising direction by providing LLMs with additional context from retrieved passages \cite{guu2020retrieval,lewis2020retrieval,izacard-grave-2021-leveraging,he2024gretriever}. However, these methods frequently face hallucination challenges \cite{maynez-etal-2020-faithfulness,zhang2023siren}, where generated content deviates from retrieved information. This issue stems primarily from the unstructured nature of retrieved passages, which lack explicit logical connections needed for precise reasoning. Recent efforts have explored integrating knowledge graphs (KGs) with LLMs \cite{sun2019ernie,yu2022survey,he2024gretriever,edge2024local}, as KGs offer compact, structured representations of entities and relationships that enable more accurate and interpretable reasoning \cite{hogan2021knowledge,jiang2024kgfit}.


% In this work, we propose Retrieval-And-Structure (RAS), a novel iterative framework that integrates theme-aware retrieval, dynamic KG construction, and self-refinement \cite{ji2023towards,asai2023self,lyu2024retrieve} to comprehensively prepare the knowledge needed for answering challenging questions. RAS comprises four key components: (1) an Action Planner for determining retrieval needs and generating sub-queries, (2) a Theme-Scoped Retrieval module that efficiently retrieves both thematically and semantically relevant context, (3) a Knowledge Structuring module that dynamically builds and expands query-specific KGs, and (4) a Knowledge-Augmented Answerer that leverages the accumulated structured information.
We present Retrieval-And-Structuring (RAS), a novel framework that dynamically constructs and reasons over query-specific knowledge graphs through iterative retrieval and structuring. RAS integrates theme-aware retrieval, dynamic KG construction, and self-refinement \cite{ji2023towards,asai2023self,lyu2024retrieve} to comprehensively prepare knowledge for challenging questions. The framework comprises four key components: (1) an action planner that strategically determines knowledge needs and generates targeted sub-queries, (2) a theme-scoped retrieval module that efficiently combines thematic and semantic relevance, (3) a knowledge structuring module that builds and expands query-specific KGs, and (4) a knowledge-augmented answerer that leverages the gathered structured information.


% RAS addresses several key limitations of existing approaches. First, unlike traditional RAG methods that retrieve based solely on semantic similarity \cite{lewis2020retrieval,guu2020retrieval}, RAS employs a theme-aware retrieval mechanism that significantly narrows the search space while maintaining retrieval quality. Second, in contrast to static KG approaches \cite{he2024gretriever,edge2024local}, RAS dynamically constructs and evolves KGs based on the specific reasoning needs of each query. Third, through its iterative planning mechanism, RAS can actively identify knowledge gaps and initiate targeted retrievals, moving beyond the single-pass retrieval limitations of existing methods \cite{izacard-grave-2021-leveraging}.
RAS addresses several critical limitations of existing approaches. First, unlike traditional RAG methods that rely solely on semantic similarity \cite{lewis2020retrieval,guu2020retrieval}, RAS employs theme-aware retrieval that significantly reduces the search space while preserving retrieval quality. Second, in contrast to static KG approaches \cite{he2024gretriever,edge2024local}, RAS dynamically constructs and evolves KGs based on the specific reasoning requirements of each query. Third, through its iterative planning mechanism, RAS actively identifies knowledge gaps and initiates focused retrievals, moving beyond the single-pass limitations of existing methods \cite{izacard-grave-2021-leveraging}.


% We evaluate RAS on a diverse set of knowledge-intensive tasks, demonstrating its superior performance compared to state-of-the-art closed-source and open-source baselines. The main contributions of this work are:
On seven knowledge-intensive tasks including open-domain QA, closed-set QA, and long-form generation, RAS outperforms state-of-the-art baselines by 6.4\% with open-source LLMs and 7.0\% with proprietary models. The main contributions of this work:

\begin{itemize}
\item We introduce RAS, a novel framework that dynamically constructs query-specific knowledge graphs through iterative retrieval and structuring for knowledge-intensive language generation.

\item We propose theme-scoped retrieval that combines thematic classification with dense retrieval to efficiently narrow the search space while maintaining retrieval quality.

\item We develop a dynamic knowledge structuring approach that iteratively builds and enriches knowledge graphs based on reasoning needs.

\item We validate RAS's effectiveness through comprehensive experiments and ablation studies across diverse knowledge-intensive tasks.
\end{itemize}


\input{figures/overview}
\section{Related Work}
\textbf{Retrieval-Augmented Generation.} Retrieval-Augmented Generation (RAG) \citep{lewis2020retrieval, guu2020retrieval} enhances performance on knowledge-intensive tasks by incorporating retrieved passages into input of large language models (LLMs), improving factual accuracy and grounding. Traditional approaches retrieved a fixed number of passages once before generation \citep{shao2023enhancing, es-etal-2024-ragas, lyu2024crudragcomprehensivechinesebenchmark} or focused on adaptive retrieval \cite{jiang2023active}, where models dynamically query external knowledge when confidence is low, or analyze on the retrieval \cite{kim2024sure}. However, these methods often fail to perform complex or multi-hop reasoning process due to the linear and simple designs, where several recent methods have emerged to improve retrieval quality. Iterative retrieval-generation approaches \cite{shao2023enhancing,guan2024amor} leverage historical context to enhance subsequent retrievals. Other work introduces targeted retrieval strategies through subqueries \cite{khattab2023demonstratesearchpredict} or follow-up questions \cite{yao2023react,press2023measuring}, progressively enriching the context for more factually grounded generation. SelfRAG \cite{asai2023self} introduced self-reflective RAG, which allows LLMs to retrieve content on demand and critically evaluate retrievals, demonstrating both the importance of retrieval quality and the capacity of LLMs to seek self-directed information. RPG \cite{lyu2024retrieve} further refined this approach by extracting fine-grained paragraphs from retrieved content to improve query relevance.
However, existing methods face key limitations. They often either include irrelevant content in the retrieved context or risk discarding essential information through refinement. Additionally, dense retrieval approaches can be computationally expensive when deployed on large corpora, hampering real-time user interaction. To address these challenges, we propose converting retrieved text into knowledge-intensive graphs and maintaining an iteratively enriched, query-specific knowledge graph for knowledge serving. We also introduce theme-scoped retrieval for more focused and efficient context retrieval.

% Although such strategies can significantly improve task performance, they often introduce additional runtime costs, risk distractions from irrelevant context, and lack robust attribution mechanisms.

\textbf{Graph as Context for LLMs.}
The integration of graphs into modern language models represents a promising research direction \cite{yasunaga2021qa,yasunaga2022deep,yu2022retrieval, ju2022grape, zhu2024realm,gutierrez2024hipporag}. Recent approaches leverage graph-based knowledge representation to enhance LLM performance. For instance, GraphToken \cite{perozzi2024let} shows that LLM can understand a graph encoded as a token, after a simple p-tuning \cite{liu2021p} while keeping the LLM's parameters frozen. Based on the findings, G-Retriever \cite{he2024gretriever} constructs subgraphs by retrieving relevant entities and relations from a global KG, using these as structured knowledge input for LLMs. 
% They also find that feeding textualized graphs along with the graph token can further boost the performance. 
GraphRAG \cite{edge2024local} employs an alternative strategy, extracting a comprehensive global KG from the corpus, identifying meaningful clusters through graph community detection, and generating LLM-based summaries of these communities to create a refined retrieval corpus. 
These methods demonstrate how graphs, with their knowledge-intensive modality and inherent structural properties, can significantly improve LLMs' contextual understanding. However, current approaches treat graphs as static data structures without refinement based on LLM requirements. Moreover, building global KGs for large-scale corpora remains prohibitively expensive and often impractical -- Wikipedia alone contains over 30 million documents per dump. The key challenge lies in developing methods to dynamically construct and utilize graphs on-the-fly for knowledge-intensive generation.
To address these limitations, we propose a retrieval-and-structuring paradigm that dynamically builds and maintains query-specific KGs on-the-fly. Our approach iteratively enriches these graphs through LLM's self-evaluation of information completeness and targeted retrieval via focused subqueries.

\section{RAS Framework}
Knowledge-intensive language generation requires not just retrieving relevant information, but also structuring and reasoning over it effectively. 
We propose Retrieval-And-Structuring (RAS), 
% a framework that addresses this challenge through an iterative process of knowledge accumulation and refinement with four key components: (1) an Action Planner that strategically determines when and what knowledge to retrieve, (2) a Theme-Scoped Retrieval approach that combines thematic and semantic matching for efficient context extraction, (3) a Knowledge Structuring module that converts retrieved text into an evolving graph representation, and (4) an Answerer that leverages this structured information for high-quality outputs. 
% \jiawei{The above seems to be repeating the caption in the figure, could be removed?}
a framework shown in Figure \ref{fig:overview}.
In the following subsections, we detail the technical mechanisms underlying each stage. To facilitate understanding, we showcase a running example of RAS in Figure \ref{fig:run_example}.

% \jiawei{It will be more understandable if there is a running (handwaving) example along with the methodology development.  What do you think?}

\subsection{Action Planning}
\label{sec:action_planning}
The Action Planning stage is the first critical step in the RAS framework, guiding the retrieval and reasoning process based on the input query and the current knowledge state. It involves two key components: the Action Planner $\mathcal{M}_{plan}$, a language model that assesses the sufficiency of the current knowledge state and generates sub-queries for further retrieval if necessary, and a decision step that determines the next action based on the planner's output.

\textbf{(For initial iteration)} Formally, given an input query $Q$, the Action Planner $\mathcal{M}_{plan}$ generates a plan $p_0$ according to:
\begin{equation}
    p_0 \leftarrow \mathcal{M}_{plan}(\text{None}; \text{None}; Q)
\end{equation}
where $p_0$ can take one of two forms:

\begin{itemize}
    \item \textbf{\texttt{[NO\_RETRIEVAL]}}: If $\mathcal{M}_{plan}$ determines that $Q$ can be answered directly without requiring any additional knowledge, the planning process terminates, and the framework proceeds directly to the final Answer Generation stage (\S \ref{sec:answer_generation}).
    \item \textbf{\texttt{[SUBQ]} $q_0 = Q$}: If $\mathcal{M}_{plan}$ assesses that the query cannot be satisfactorily answered with its own knowledge, we start the iteration with the main query $Q$ as the initial subquery, and move to the next stage (\S \ref{sec:theme_retrieval}).
\end{itemize}

% In the case where a subquery $q_0$ is generated, the RAS framework initiates an iterative process of theme-scoped knowledge retrieval and enrichment. 
\textbf{(For iteration $>$ 0)} At each iteration $i$, the Action Planner reassesses the cumulative retrieved knowledge $G_i$ and the chain of previous sub-queries $(q_0, g_0), ..., (q_i, g_i)$ in the context of the main query $Q$:
\begin{equation}
    p_{i+1} \leftarrow \mathcal{M}_{plan}(f_{gnn}(G_i), (q_0, g_0),...,(q_i, g_i), Q)
\end{equation}
where $f_{gnn}$ is a graph neural network for encoding the evolving KG $G_i$; $q_k$ is the subquery at iteration $k$, and $g_k$ is the extracted graph information (triple list) from the retrieved context. The base model we use here is GraphLLM, an architecture derived from previous work \cite{perozzi2024let,he2024gretriever}.
We discuss more details in \S \ref{sec:knowledge_structuring}.

The planner's output $p_{i+1}$ at each iteration can be either:
\begin{itemize}
    \item \textbf{\texttt{[SUFFICIENT]}}: The accumulated knowledge $G_i$ is deemed sufficient to comprehensively address the main query $Q$. The iterative retrieval process terminates, and the framework proceeds to the Knowledge-Augmented Generation stage (\S \ref{sec:answer_generation}).
    \item \textbf{\texttt{[SUBQ]} $q_{i+1}$}: The planner generates a new subquery $q_{i+1}$ to guide the retrieval of additional relevant knowledge. The subquery is designed to fill specific gaps in the current knowledge state with respect to answering $Q$. The framework continues the iterative process and proceeds to the Theme-Scoped Retrieval stage (\S \ref{sec:theme_retrieval}).
\end{itemize}

The Action Planner serves as a key driver of the RAS framework's iterative retrieval and refinement process. By dynamically assessing the adequacy of the retrieved knowledge and generating targeted sub-queries, it enables the efficient acquisition of query-relevant information. 
% The integration of the evolving knowledge graph $G_i$ and the subquery chain $(q_0, g_0), ..., (q_i, g_i)$ equips the Action Planner with a comprehensive context to make informed decisions at each step, ensuring a focused and effective knowledge accumulation.

\subsection{Theme-Scoped Retrieval}
\label{sec:theme_retrieval}

% This stage aims to efficiently retrieve context that is thematically and semantically relevant to a given subquery $q_i$ from a large corpus. This stage leverages a two-step process: (1) theme scoping, which identifies a focused sub-corpus $c_i$ that aligns with the thematic context of $q_i$, and (2) dense retrieval, which pinpoints the most semantically relevant passages $t_i$ within $c_i$. By narrowing the retrieval scope based on thematic relevance prior to semantic matching, we improve both efficiency and precision of the context retrieval.
This stage efficiently retrieves thematically and semantically relevant context for subquery $q_i$ through a two-step process: (1) theme scoping to identify a narrowed sub-corpus $c_i$ close to $q_i$'s thematic distribution, and (2) dense retrieval to find the most relevant passages $t_i$ within $c_i$. This thematic pre-filtering before semantic matching aims to improve both retrieval efficiency and precision.


\subsubsection{Theme Scoping} \label{sec:theme_scoping}
The Theme Scoping mechanism transforms sub-queries into thematic probability distributions and retrieves relevant document subsets through three key operations:
\begin{align}
    \text{PPD}(q_i) &\leftarrow f_{cls}(q_i) \label{eq:theme_cls} \\
    \text{PPD}(d_i) &\leftarrow f_{shift}(\text{PPD}(q_i)) \label{eq:theme_shift} \\
    c_i &\leftarrow \text{ThemeSim}(\text{PPD}(d_i), C_{PPD}, K) \label{eq:theme_retrieve}
\end{align}
First, a theme classifier $f_{cls}$ maps the subquery $q_i$ to a posteriori probability distribution (PPD) over thematic classes (Eq. \ref{eq:theme_cls}). The classifier is trained on DBPedia-298 \cite{lehmann2015dbpedia}\footnote{Document label space of DBPedia-298: \url{https://mappings.dbpedia.org/server/ontology/classes/}}, a multilabel wiki text classification dataset, enabling fine-grained thematic distinctions.

To address the distribution shift between queries and documents, an MLP-based shifter $f_{shift}$ transforms the query theme distribution $\text{PPD}(q_i)$ into an expected document theme distribution $\text{PPD}(d_i)$ (Eq. \ref{eq:theme_shift}). This shifter is trained on query-document pairs in our constructed dataset HotpotQA-SUBQ (see Appendix \ref{ap:train_data}), learning to capture the relationship between query and document thematic patterns.

Finally, using the shifted distribution $\text{PPD}(d_i)$, we retrieve a theme-specific sub-corpus $c_i$ from the theme-indexed corpus $C_{PPD}$ (Eq. \ref{eq:theme_retrieve}). The $\text{ThemeSim}(\cdot)$ function selects the top-$K$ documents based on L2 similarity between $\text{PPD}(d_i)$ and the pre-computed theme distributions in $C_{PPD}$. This focused retrieval process effectively reduces the search space while maintaining thematic relevance for subsequent semantic matching operations.

\subsubsection{Dense Retrieval}
\label{sec:dense_retrieval}

Within the theme-specific sub-corpus $c_i$, a dense retriever $f_{den}$ is employed to identify the most semantically relevant passages $t_i$ for the subquery $q_i$:
\begin{equation}
    t_i \leftarrow \text{DenseSim}(f_{den}(q_i), f_{den}(c_i), k)
\end{equation}
where $\text{DenseSim}(\cdot)$ selects the top-$k$ passages based on the cosine similarity between the query embedding $f_{den}(q_i)$ and the passage embeddings $f_{den}(c_i)$ precomputed in the dense index.

% The Theme-Scoped Retrieval stage, by first narrowing down the search space to thematically relevant documents and then pinpointing the most semantically pertinent passages, this stage strikes a balance between retrieval efficiency and knowledge relevance, setting the stage for effective downstream reasoning and generation.

\subsection{Knowledge Structuring and Enrichment}
\label{sec:knowledge_structuring}
This stage is designed to transform the retrieved text passages $t_i$ into a structured graph representation $g_i$ and iteratively expand the query-specific knowledge graph $G_Q$. 

\subsubsection{Text-to-Triple Conversion}
\label{sec:text_to_triple}
To capture the essential facts from the retrieved passages $t_i$, a text-to-triples model $f_{t2t}$ is employed. We train the model on the full set of WikiOFGraph dataset \cite{kim2024ontology}, an LLM-curated high-quality text-triples dataset, enabling it to generate quality and information-rich triples in the format:
\begin{equation}
g_i \leftarrow f_{t2t}(t_i) = [(s_0, r_0, o_0), ..., (s_{|g_i|}, r_{|g_i|}, o_{|g_i|})]
\end{equation}
where each triple $(s_j, r_j, o_j)$ represents a subject-predicate-object fact extracted from the text. This structured representation allows for efficient downstream reasoning and simple external knowledge graph integration.

\subsubsection{Iterative Knowledge Enrichment}
\label{sec:knowledge_enrichment}
The extracted triples $g_i$ are then converted into a graph structure $g_i' = (V_i, E_i)$, where $V_i$ and $E_i$ denote the sets of nodes and edges, respectively. Each node $v \in V_i$ corresponds to a unique subject or object entity in $g_i$, while each edge $e \in E_i$ represents a predicate connecting two entities.
To enrich the graph with semantic information, the attributes of nodes and edges are obtained through a text encoder $f_{enc}$ (Sentence-BERT \cite{reimers2019sentence} in our case):
\begin{equation}
v.attr \leftarrow f_{enc}(v), \forall v \in V_i \
e.attr \leftarrow f_{enc}(e), \forall e \in E_i
\end{equation}
These semantic embeddings enable the model to capture the nuanced relationships between entities and facilitate reasoning over the knowledge graph.

To progressively expand the knowledge base in response to the evolving sub-queries, the structured graph $g_i'$ at each iteration $i$ is merged into a evolving knowledge graph $G_Q = (V_Q, E_Q)$ specific to the main query $Q$:
\begin{equation}
G_Q \leftarrow G_Q \cup g_i'
\end{equation}
% This iterative enrichment process allows the model to accumulate relevant knowledge across multiple retrieval steps, building a comprehensive and query-specific KG.
After enriching $G_Q$ with the new knowledge, the action planner (\S\ref{sec:action_planning}) reassesses the current knowledge to determine the next step. Based on $G_Q$ and the chain of previous subqueries and their associated graph information, the planner decides whether to generate another focused subquery for additional retrieval (\S\ref{sec:theme_retrieval}) or to proceed with answering (\S\ref{sec:answer_generation}) if the accumulated knowledge is deemed sufficient.



\subsection{Knowledge-Augmented Generation}
\label{sec:answer_generation}
The final stage generates an answer $A$ to the main query $Q$ using the Answerer model $\mathcal{M}_{ans}$ conditioned on knowledge graph $G_Q$ and subquery chain $(q_0, g_0), ..., (q_i, g_i)$.

If no retrieval is needed ($p_0 = \text{\texttt{[NO\_RETRIEVAL]}}$), the answer is generated directly:
\begin{equation}
A \leftarrow \mathcal{M}_{ans}(\text{None}; \text{None}; Q)
\end{equation}
Otherwise, after iterative knowledge enrichment concludes with \texttt{[SUFFICIENT]} plan, the answer is generated using encoded KG $f_{gnn}(G_Q)$ and subquery chain:
\begin{equation}
A \leftarrow \mathcal{M}_{ans}(f_{gnn}(G_Q); (q_0, g_0), ..., (q_i, g_i); Q)
\end{equation}
% where $f_{gnn}$ is a graph neural network encoding $G_Q$'s entity relationships and semantics.
$\mathcal{M}_{ans}$ attends to relevant knowledge in $G_Q$ and subquery chain to generate accurate, coherent answers. 
% Conditioning on structured, enriched knowledge allows RAS to dynamically adapt generation to query requirements.

\underline{Note:} In this study, $\mathcal{M}_{ans}$ and $\mathcal{M}_{plan}$ are two roles played by the same model with different instructions (see \S\ref{apsub:plan_ans_train}).




\section{Experiments}
\textbf{Training Data \& Setting.} We develop HotpotQA-SUBQ, a dataset derived from HotpotQA \cite{yang2018hotpotqa}, to train our model's action planning and answering capabilities. 
% The action planner learns to generate precise plans based on evolving graph information, while the answerer learns to synthesize answers from this information.
Our dataset creation begins with document filtering: using Claude 3.5 Sonnet \cite{anthropic2024claude}, we identify and retain only the supporting documents necessary for answering the main query, removing irrelevant content. For each supporting document $d_j$, we then iteratively generate a subquery $q_j$, considering the main query, previous subqueries, and supporting documents. During iteration $j$, when more supporting documents remain, we create training samples with input $\{q_0, g_0, ..., q_j, g_j, Q\}$ and output label ``\texttt{[SUBQ]} $q_{j+1}$'', where $g_k$ represents triples extracted from document $d_k$, and $Q$ is the main query. For the final supporting document, we label the sample as ``\texttt{[SUFFICIENT]}''.
To identify queries that can be answered directly, we test our base LLM (LLaMA-2-7B) on HotpotQA's main queries without context. For correctly answered queries, we create training samples with the main query $Q$ as input and ``\texttt{[NO\_RETRIEVAL]}'' as the output label. Additionally, to ensure fair comparison with existing approaches, we incorporate the subset of Arc-Easy (2,147 samples) and ASQA (3,897 samples) from SelfRAG's training data, resulting in 208k training samples in total. We place detailed training data processing, dataset statistics, and data samples in Appendix \ref{ap:train_data}. We use DBPedia-298, HotpotQA-SUBQ, and WikiOFGraph to train our theme classifier $f_{cls}$, distribution shifter $f_{shift}$, and text-to-triples model $f_{t2t}$, respectively, as detailed in Appendix \ref{ap:training}. We present our hyperparameter study of each component in Appendix \ref{ap:hyper_param}.
% We place hyperparameter study in Table \ref{tb:hyperparams}.


\textbf{Knowledge Sources.}
Due to the large sizes of wiki corpora, we employ faiss \cite{douze2024faiss} for efficient vector searching across both theme and dense indices. Following the SelfRAG \cite{asai2023self}, we utilize the Wikipedia 2018 \cite{izacard2023atlas}  by default, while specifically using the Wikipedia 2020 for PopQA to access more recent information. To optimize retrieval efficiency, we partition the index into five segments. Within each segment, we first perform theme scoping to reduce the candidate document pool $K$ to 250,000 documents by default. However, we skip theme scoping for PopQA since our classifier $f_{cls}$, trained on DBPedia-298 data based on old Wiki dump, performs poorly on newer Wiki 2020 dump, producing near-zero vectors for roughly one-third of recent content.

\textbf{Test Datasets \& Metrics \& Compared Baselines.} We conduct comprehensive evaluations on diverse knowledge-intensive tasks following previous studies \cite{asai2023self, lyu2024retrieve}. The evaluation encompasses three categories of datasets: (1) open-domain short-form generation datasets: TriviaQA \cite{joshi2017triviaqa}, PopQA \cite{mallen2022not}, and 2WikiMultihopQA \cite{ho2020constructing}; (2) closed-set task datasets: PubHealth \cite{zhang2023interpretable} and ARC-Challenge \cite{clark2018think}; and (3) long-form generation datasets: ALCE-ASQA \cite{gao2023enabling, stelmakh2022asqa} and ELI5 \cite{fan2019eli5}. For evaluation metrics, we maintain consistency with prior work \cite{asai2023self, mallen2022not, lyu2024retrieve}, employing ``golden match'' accuracy for PopQA and TriviaQA, token-level F1 score for 2WikiMultihopQA, accuracy for PubHealth and ARC-Challenge, and ROUGE-LSum alongside MAUVE score \cite{pillutla2021mauve} for ASQA and ELI5. Our comparative analysis includes three baseline categories: models without retrieval augmentation, incorporating Claude 3.5 Sonnet as a state-of-the-art closed-source baseline; models with single retrieval over top-5 documents, including Claude 3.5 Sonnet, and SuRe \cite{kim2024sure}, a leading retrieve-and-summarize framework; and models with self-reflective retrieval, including leading approaches Self-RAG \cite{asai2023self} and RPG \cite{lyu2024retrieve}. We place more details of metrics/datasets in Appendix \ref{ap:metrics_datasets}.


\input{tables/main_results}
\textbf{Inference Setting.}
We evaluate RAS using both our trained open-source model (RAS$_{7\text{B}/8\text{B}}$ with LLaMA-2-7B/LLaMA-3-8B and Graph Transformer \cite{shi2020masked} encoder) and the closed-source Claude-3.5-Sonnet model under varied inference settings. For RAS$_{7\text{B}/8\text{B}}$, we maintain consistency with previous work \cite{asai2023self,lyu2024retrieve} by employing zero-shot inference across all datasets. For closed-source models (e.g., RAS$_{\text{Sonnet-3.5}}$), we utilize few-shot inference with two exemplars for ASQA and ELI5, while maintaining zero-shot for the other datasets.

During inference, if the initial planner output is not \texttt{[NO\_RETRIEVAL]}, we utilize the main query $Q$ as the initial subquery $q_{0}$. For PopQA and TriviaQA evaluation, we follow established settings \cite{asai2023self,luo2023sail}, incorporating top-five web search engine results as initial retrieved context $t_{0}$. For ASQA and ELI5, we maintain methodological consistency with prior work \cite{lyu2024retrieve,asai2023self,gao2023enabling}, utilizing their predetermined five-document context. Under these conditions, we omit plan generation and text retrieval phases, implementing static inference \cite{asai2023self} with five fixed iterations. For remaining datasets, we establish a maximum iteration count of five. Following previous studies, we employ Contriever-MS MARCO \cite{izacard2021unsupervised} as the primary dense retriever, with BM25 \cite{robertson2009probabilistic} serving as the retrieval mechanism for 2WikiMultihopQA. Across all retrieval processes, we maintain a consistent top-$k$ document selection of five. We present more details of inference settings in Appendix \ref{ap:inference_eval}.

\input{tables/ablation_study}
\subsection{Main Results}
% genreal performance

% \jiawei{Naming convention in Table 1 could be somewhat confusing. For example, in the last RAS section, only RAS{7B or 8B} has RAS but Sonnet-3.5 has no RAS, etc.   Maybe some better naming may make it better understandable?
% }

Our performance evaluation, presented in Table \ref{tb:main_results}, demonstrates that the LLaMA-2-7B/LLaMA-3-8B model fine-tuned with RAS outperforms existing open-source solutions, including SelfRAG \cite{asai2023self} and RPG \cite{lyu2024retrieve}. Notably, compared to the previous SOTA models, RAS$_{7\text{B}}$ shows a 9.7\% improvement in short-form question-answering and a 7.9\% gain in long-form generation tasks. Additionally, when applied to closed-source Sonnet-3.5 model, RAS consistently achieves superior results compared to single retrieval RAG approaches, including retrieve-and-summarize approach SuRe \cite{kim2024sure}.

% necessity of on-denmand retrieval
We find that sometimes (e.g., on TriviaQA and PubHealth) single-hop retrieval could not boost LLM's performance, and even makes it worse, which demonstrates the necessity of on-demand retrieval, aligning with previous findings.

% Role-swapping 
Our ``role-swapping'' study, where Sonnet-3.5 and RAS$_{7\text{B}}$ alternately serve as action planner and answerer, reveals that performance is primarily bounded by the answerer's inherent capabilities. For example, when Sonnet-3.5 acts as the planner with RAS$_{7\text{B}}$ as the answerer, the system achieves 62.4\% accuracy on the ARC-Challenge dataset. However, when Sonnet-3.5 serves as both planner and answerer, accuracy increases to 93.8\%. Notably, despite having 60x fewer parameters, our trained RAS$_{7\text{B}}$ performs comparably to Sonnet-3.5 as a planner, and even outperforms it sometimes.

Also, for long-form generation task, our trained model achieved notably high MAUVE scores, which can be majorly attributed to our ``text-to-triple'' generation step, as demonstrated by our later ablation study in Table \ref{tb:ablation}. 

\input{figures/efficiency_study}
\input{figures/train_size_perf}
\subsection{Ablations \& More Analysis}
We study the effect of individual component in Table \ref{tb:ablation}.
% we conduct a comprehensive ablation study in Table \ref{tb:ablation}.

\textbf{Effect of Iterative Planning and Retrieval.} Comparing the base model with ``No Planning'' variant shows that iterative planning provides consistent improvements across all metrics (e.g., +8.8\% on TQA, +9.0\% on 2WQA). This demonstrates the importance of dynamically determining retrieval needs and generating focused sub-queries. Without planning, the model relies on single-pass retrieval, which may miss crucial information needed for complex reasoning. Also, when turning off retrieval, the performance degradation is more obvious, due to the knowledge-intensive nature of those datasets.

\textbf{Effect of Graph Construction and Encoding.} The impact of structured knowledge representation is evident from two ablations. First, ``No Text-to-Triple'' degrades performance significantly on all metrics (e.g., -9.0\% on 2WQA, -22.2\% MAUVE on ASQA), showing the value of essential information extraction via converting retrieved text into structured triples. Second, removing the graph encoder (``No GraphToken'') during training or inference consistently hurts performance across datasets, with particularly large drops on PubHealth (-11.2\% and -10.0\% respectively). This suggests that the graph structure encoding helps the model better leverage the knowledge relationships.

\textbf{Effect of LoRA and Multi-task Learning.} Our experiments reveal that parameter-efficient training strategies significantly impact model performance. Using only graph token optimization without LoRA leads to substantial degradation (-11.8\% on average). A similar observation can be made for ``No Multi-Task'' (e.g., 12.3\% accuracy degradation on PubHealth), indicating the significance of jointly training the model on both action planning and answer generation tasks rather than optimizing for each task separately, supporting findings from prior work \citep{lyu2024retrieve}. The complementary effects suggest that while graph-based knowledge representation is valuable, it needs to be combined with careful parameter tuning and multi-task learning to achieve optimal performance.

\textbf{Effect of Theme Scoping.} The theme-scoped retrieval shows mixed results -- while removing it (``No ThemeScoping'') has minimal performance drops on TQA and PubHealth, the efficiency benefits are substantial as shown in Figure \ref{fig:efficiency}. This suggests theme scoping successfully narrows the search space without compromising performance.

\textbf{Efficiency Analysis.} Figure \ref{fig:efficiency} compares the computational costs of different retrieval strategies. Theme-scoped RAG reduces processing time by approximately 60\% compared to dense-only RAG by first filtering the corpus through lightweight theme classification (with 298 class dimensions). This combination of comparable performance with significantly reduced computational cost demonstrates the effectiveness of our theme-scoped retrieval approach.

\textbf{Impact of Training Data Volume.} Figure \ref{fig:train_size_perf} demonstrates how training dataset size influences model performance across different tasks. Considering the computational efficiency, we sampled 2,000 instances each from TQA and 2WQA for evaluation, while maintaining the original sizes of other datasets. The results indicate that model performance generally improves with increased training data volume. 
Notably, our model achieves competitive performance even with limited training data - using only 5\% (10K instances) of our full dataset, it surpasses previous state-of-the-art models on TQA, 2WQA, and ELI5. These results suggest both the robustness of our architectural design and the effectiveness of our data curation methodology.
However, we observed an exception with the ELI5 dataset, where performance were inconsistent. This irregularity can be attributed to the inclusion of ASQA training data in our training set, following established setting from previous research \cite{asai2023self, lyu2024retrieve}. Among our test datasets, ASQA and ELI5 are unique in requiring long-form response generation. The periodic decline in ELI5 performance suggests that the model's response generation began to align more closely with ASQA's training data distribution, potentially at the expense of ELI5's distinct characteristics.

\textbf{Impact of Graph Information Abundance.} Figure \ref{fig:graph_ab} demonstrates a strong positive correlation between graph information abundance and model performance across all tasks. Both RAS$_{7\text{B}}$ and RAS$_{8\text{B}}$ show consistent improvements as graph information increases, demonstrating RAS's strong capability of effective utilizing available graph information for knowledge-intensive generation.










\label{sec:data_prep}

% \textbf{RQ1.} How the entire framework improve the performance of the LLM reasoning? \\
% \textbf{RQ2.} How the dynamic theme-specific corpus creation improve the efficiency and efficacy of context retrieval? \\
% \textbf{RQ3.} How the evolving knowledge graph enable precise critic and reasoning?\\

% RQ1: Compare with (1) Self-RAG, and (2) RPG. Show quantitative results.

% RQ2: Compare with (1) classic dense retrieval, (2) G-Retriever, and (3) BM25. Show quantitative \& qualitative results.

% RQ3: Compare with (1) text only, (2) graph only, (3) text + reasoning, and (4) graph + reasoning. Show quantitative \& qualitative results.

% Datasets:

\input{figures/graph_abundance}

% These limitations highlight important directions for future work, particularly in developing more robust knowledge extraction techniques and updating the thematic classification components to better handle modern document collections.



\section{Conclusion}
We presented RAS (Retrieval-And-Structuring), a novel framework that enhances knowledge-intensive language generation through iterative retrieval and knowledge structuring. Our approach introduces key innovations including theme-scoped retrieval, action planning, dynamic knowledge structuring, and graph-augmented answering. Comprehensive experiments demonstrate RAS's superior performance over existing approaches across diverse benchmarks. Looking ahead, RAS's graph-based architecture enables seamless integration with external knowledge graphs and human-in-the-loop curation, making it particularly valuable for specialized domains requiring high accuracy and reliability. We discuss limitations of RAS and future work in Appendix \ref{ap:impact_limit}. Our code can be found at 
% \url{https://anonymous.4open.science/r/RAS-Anonymous/}.
\url{https://github.com/pat-jj/Retrieval-And-Structure}.
\clearpage

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\section*{Impact Statements}
The RAS framework has several potential positive impacts on both research and practical applications. First, by improving knowledge-intensive language generation through structured knowledge representation and iterative reasoning, RAS could enhance the reliability and factual accuracy of AI systems in critical domains like education, research assistance, and technical documentation. The theme-scoped retrieval mechanism also offers computational efficiency benefits, potentially reducing the energy consumption and computational resources required for large-scale knowledge retrieval tasks.

However, there are important considerations regarding potential risks and challenges. The framework's effectiveness in knowledge structuring could potentially amplify existing biases in training data and knowledge sources. Additionally, while RAS improves retrieval efficiency, it may create barriers for low-resource languages or domains where structured knowledge bases and thematic classifications are limited. These concerns necessitate careful consideration of data sources and regular evaluation of the system's impact across different user groups and application contexts.

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \newpage
\appendix
\newpage

\onecolumn
\DoToC
\newpage

\section{Limitations \& Future Work}
\label{ap:impact_limit}

While RAS demonstrates strong performance, several key limitations warrant consideration for future improvements:

\begin{itemize}
    \item \textbf{Knowledge Extraction Quality:} The effectiveness of both planning and answer generation depends on the quality of text-to-triple extraction. Despite using a Flan-T5-based model for triple extraction, implementing a more sophisticated approach -- such as a dedicated Named Entity Recognition and Relation Extraction (NER-RE) pipeline -- could potentially enhance RAS's reasoning capabilities and knowledge representation.
    
    \item \textbf{Theme Scoping Limitations:} The current classifier was trained on DBPedia-298, an older dataset that may not adequately represent the thematic distribution of contemporary document collections. This temporal gap particularly affects performance when processing newer corpora, as evidenced by our experiments with Wikipedia 2020 dumps. The development of a huge, up-to-date multilabel document classification dataset would likely improve the effectiveness of theme scoping and, consequently, the overall efficiency gains it provides.
    
    \item \textbf{Graph Evolution Strategy:} The current approach to evolving knowledge graphs could be enhanced by incorporating more sophisticated graph pruning and merging strategies. Future work could explore dynamic graph summarization techniques to maintain the most relevant information while preventing excessive graph growth during iterations.
    
    \item \textbf{Cross-Domain Adaptability:} While RAS performs well on the evaluated tasks, its effectiveness across highly specialized domains or multilingual settings remains to be investigated. Future research could focus on developing domain-adaptation techniques and multilingual knowledge structuring approaches.
    
    \item \textbf{Interactive Refinement:} The current implementation lacks mechanisms for interactive refinement of the constructed knowledge graphs. Future versions could incorporate human feedback loops to improve the quality of knowledge structuring and reasoning paths.
\end{itemize}

These limitations suggest several promising directions for future research, including improved knowledge extraction techniques, more robust theme classification approaches, and enhanced graph evolution strategies. A particularly promising direction is the integration of external knowledge graphs into RAS's iterative loop, as its graph-based architecture naturally accommodates the incorporation of established KGs like Wikidata or domain-specific knowledge bases. This could significantly enhance the system's reasoning capabilities by combining dynamically constructed knowledge with curated structured information. We summarize the potential future directions of extension in Figure \ref{fig:extension}.
% Additionally, exploring the integration of RAS with other knowledge-intensive architectures could lead to hybrid systems that combine the strengths of different approaches.
\begin{figure*}[!h]
\centering
\includegraphics[width=0.8\linewidth]{figures/extension.pdf}
% \vspace{-2em}
\caption{Potential Future Improvement of RAS.
}
\label{fig:extension}
\end{figure*}



\clearpage

\section{HotpotQA-SUBQ Dataset}
\label{ap:train_data}

The source of our dataset is HotpotQA \cite{yang2018hotpotqa}.\footnote{We use the version \texttt{hotpot\_train\_v1.1} from \url{https://github.com/hotpotqa/hotpot}.} Our goal is to create a dataset that can let the model learn the capabilities of subquery generation and answering with intensive knowledge. We name the dataset as HotpotQA-SUBQ.


The construction of HotpotQA-SUBQ includes three steps: (1) filter out irrelevant/unnecessary ``supporting docs'' (\S\ref{apsub:filter}), (2) generate one subquery for each supporting document (\S\ref{apsub:subquery_gen}), and (3) sample labeling (\S\ref{apsub:sample_const}).


\subsection{Supporting Document Filtering}
\label{apsub:filter}
To identify and filter out irrelevant supporting documents from HotpotQA, we employ an instruction-based approach using Claude 3.5 Sonnet, as shown in Figure~\ref{fig:filter-prompt}.

\begin{figure}[!h]
\centering
\begin{lstlisting}[language={}]
Identify which documents are HELPFUL to answer the question. Output only the document numbers separated by commas.

Examples:

Example 1 (Some documents are not helpful):
Question: What nationality was James Henry Miller's wife?
Supporting docs:
1. Margaret "Peggy" Seeger (born June 17, 1935) is an American folksinger. She is also well known in Britain, where she has lived for more than 30 years, and was married to the singer and songwriter Ewan MacColl until his death in 1989.
2. Seeger's father was Charles Seeger (1886-1979), an important folklorist and musicologist; her mother was Seeger's second wife, Ruth Porter Crawford.
3. James Henry Miller, better known by his stage name Ewan MacColl, was an English folk singer and songwriter.
Output: 1,3
Explanation: Only docs 1 and 3 are helpful - doc 1 shows Peggy Seeger (who is American) was married to Ewan MacColl, and doc 3 confirms Ewan MacColl is James Henry Miller. Doc 2 about Seeger's parents is not helpful.

Example 2 (All documents are helpful):
Question: The Oberoi family is part of a hotel company that has a head office in what city?
Supporting docs:
1. The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.
2. The Oberoi Group is a hotel company with its head office in Delhi.
Output: 1,2
Explanation: Both docs are helpful - doc 1 links the Oberoi family to The Oberoi Group, and doc 2 provides the head office location.

Question: |\colorbox{yellow!20}{\textbf{[question]}}|
Supporting docs: 
|\colorbox{yellow!20}{\textbf{[enumerated\_documents]}}|

Output only the helpful document numbers separated by commas:
\end{lstlisting}
% \vspace{-2em}
\caption{\textbf{Prompt used for filtering supporting documents in HotpotQA.} The prompt includes examples to demonstrate the difference between helpful and irrelevant documents. The input parts to the prompt are highlighted.}
\label{fig:filter-prompt}
\end{figure}

The filtering prompt is designed with clear examples that illustrate the criteria for document relevance. For each HotpotQA sample, we enumerate all supporting documents and use Claude to identify only those that contribute directly to answering the question. After filtering out irrelevant documents, we filter the question with no supporting documents. This filtering step reduces noise in the training data and helps focus the model on truly relevant information during sub-query generation.


\subsection{Sub-query Generation}
\label{apsub:subquery_gen}
For sub-query generation, we use the template as follows to generate one subquery per (filtered) document:

\begin{figure}[!h]
    \centering
    \begin{lstlisting}[language={}]
Given this main question and a supporting document, generate a simple sub-query (a question) that will help retrieve information from the document to answer the main question.

Main Question: |\colorbox{yellow!20}{\textbf{[main\_question]}}|

Current Document ([topic]):
|\colorbox{yellow!20}{\textbf{[document\_content]}}|

[If previous queries exist:]
Previously generated sub-queries:
|\colorbox{yellow!20}{- \textbf{[sub\_query\_1]}}|
|\colorbox{yellow!20}{- \textbf{[sub\_query\_2]}}|
|\colorbox{yellow!20}{...}|


Write ONE clear and specific question that:
1. Can be answered using ONLY this document
2. Helps retrieve information needed for the main question
3. Is direct and focused on key information from this document

Write only the question, without any explanations or formatting.
\end{lstlisting}
% \vspace{2em}
    \caption{\textbf{Prompt used for subquery generation from HotpotQA.} The input parts to the prompt are highlighted.}
    \label{fig:enter-label}
\end{figure}

\vspace{2em}
The prompt template enforces document-specificity, goal-orientation, and conciseness in sub-query generation. For iterative querying, we maintain a list of previously generated sub-queries to avoid redundancy and encourage progressive information gathering.


\subsection{Sample Labeling}
\label{apsub:sample_const}

The sample labeling process transforms the filtered and subquery-augmented HotpotQA examples into training instances for both the Action Planner and Answerer components. We formalize this process in Algorithm~\ref{alg:sample_label}.

\begin{algorithm}[tb]
\label{alg:sample_label}
\caption{HotpotQA-SUBQ Sample Labeling}
\begin{algorithmic}[1]
\REQUIRE $\mathcal{D}$: Filtered HotpotQA dataset
\REQUIRE $\mathcal{M}$: Base LLM (LLaMA-2-7B)
\REQUIRE $f_{t2t}$: Text-to-triple conversion model
\ENSURE $\mathcal{T}_{plan}$: Training data for Action Planner
\ENSURE $\mathcal{T}_{ans}$: Training data for Answerer
\STATE Initialize $\mathcal{T}_{plan}, \mathcal{T}_{ans} \gets \{\}, \{\}$
\FORALL{$(Q, \{d_0,...,d_n\}, A) \in \mathcal{D}$}
    \STATE $\hat{A} \gets \mathcal{M}(Q)$ \hfill \textcolor{blue}{$\hookleftarrow$ Direct answer attempt}
    \IF{$\hat{A} = A$}
        \STATE $\mathcal{T}_{plan} \gets \mathcal{T}_{plan} \cup \{(Q, \texttt{[NO\_RETRIEVAL]})\}$
        \STATE $\mathcal{T}_{ans} \gets \mathcal{T}_{ans} \cup \{(Q, A)\}$
        \STATE continue
    \ENDIF
    \STATE $subq_0, ..., subq_n \gets \text{GenerateSubqueries}(Q, \{d_0,...,d_n\})$
    \STATE $G_0,...,G_n \gets \emptyset$ \hfill \textcolor{blue}{$\hookleftarrow$ Initialize graph contexts}
    \FOR{$i \gets 0$ \textbf{to} $n$}
        \STATE $g_i \gets f_{t2t}(d_i)$ \hfill \textcolor{blue}{$\hookleftarrow$ Convert text to triples}
        \IF{$i < n$}
            \STATE $input \gets \text{FormatInput}(\{(subq_j,g_j)\}_{j=0}^i, Q)$
            \STATE $\mathcal{T}_{plan} \gets \mathcal{T}_{plan} \cup \{(input, \texttt{[SUBQ]} \ subq_{i+1})\}$
        \ELSE
            \STATE $input \gets \text{FormatInput}(\{(subq_j,g_j)\}_{j=0}^n, Q)$
            \STATE $\mathcal{T}_{plan} \gets \mathcal{T}_{plan} \cup \{(input, \texttt{[SUFFICIENT]})\}$
            \STATE $\mathcal{T}_{ans} \gets \mathcal{T}_{ans} \cup \{(input, A)\}$
        \ENDIF
        \STATE $G_i \gets G_{i-1} \cup g_i$ \hfill  \textcolor{blue}{$\hookleftarrow$ Accumulate graph context}
    \ENDFOR
\ENDFOR
\STATE \textbf{return} $\mathcal{T}_{plan}, \mathcal{T}_{ans}$
\end{algorithmic}
\end{algorithm}

The algorithm takes as input the filtered HotpotQA dataset $\mathcal{D}$, base language model $\mathcal{M}$, and text-to-triple conversion model $f_{t2t}$. For each example, we first attempt direct answer generation using the base LLM (Line 3). If successful, we create \texttt{[NO\_RETRIEVAL]} training instances for both the planner and answerer (Lines 4-6).

For examples requiring retrieval, we process each supporting document sequentially:

\begin{enumerate}
    \item Convert document text to structured triples using $f_{t2t}$ (Line 12)
    \item For intermediate documents ($i < n$):
        \begin{itemize}
            \item Construct input by concatenating previous subquery-graph pairs
            \item Label with \texttt{[SUBQ]} and next subquery
        \end{itemize}
    \item For final document ($i = n$):
        \begin{itemize}
            \item Include all accumulated context
            \item Label planner sample as \texttt{[SUFFICIENT]}
            \item Create answerer training instance with ground truth answer
        \end{itemize}
\end{enumerate}

The input formatting function (Lines 14 and 17) follows the template:

\begin{lstlisting}[language={}]
[SUBQ] |$q_0$|
Retrieved Graph Information: |$g_0$|
[SUBQ] |$q_1$|
Retrieved Graph Information: |$g_1$|
...
Question: Q
\end{lstlisting}

This process generates two datasets:
\begin{itemize}
    \item $\mathcal{T}_{plan}$: Trains the Action Planner to determine retrieval needs and generate targeted subqueries
    \item $\mathcal{T}_{ans}$: Trains the Answerer to synthesize final responses from accumulated graph context
\end{itemize}

Table~\ref{tb:hotpot_stat} illustrates the distribution of our generated datasets. The planner dataset demonstrates a balanced distribution across three label types: 35\% \texttt{[SUFFICIENT]}, 55\% \texttt{[SUBQ]}, and 10\% \texttt{[NO\_RETRIEVAL]}. The answerer dataset incorporates both no-retrieval and sufficient cases from multiple sources. The size of the answerer dataset exceeds the combined total of no-retrieval and sufficient cases from the planner dataset for two reasons: first, we incorporated additional data from ASQA and Arc-Easy datasets; second, we included no-retrieval cases that were initially filtered out in step 1 of our process. This comprehensive approach ensures a robust training set for the answerer component.

Our labeling approach ensures that models learn both the iterative nature of complex question answering and the importance of structured knowledge representation. The complementary training objectives help develop robust reasoning capabilities while maintaining retrievability of supporting evidence.


\subsection{Dataset Statistics}
\label{apsub:hot_subq_stat}

\input{tables/hotpot_stat}


\subsection{Data Examples}
\label{apsub:data_examples}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{figures/example_planner_noret.pdf}
\vspace{-2em}
\caption{Training Data Example (Action Planner – [NO\_RETRIEVAL])
}
\label{fig:pla_case1}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{figures/example_planner_subq.pdf}
\vspace{-2em}
\caption{Training Data Example (Action Planner – [SUBQ])
}
\label{fig:pla_case2}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{figures/example_planner_sufficient.pdf}
\vspace{-2em}
\caption{Training Data Example (Action Planner – [SUFFICIENT])
}
\label{fig:pla_case3}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{figures/example_ans_noret.pdf}
\vspace{-2em}
\caption{Training Data Example (Answerer – [NO\_RETRIEVAL])
}
\label{fig:ans_case1}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{figures/example_ans_sufficient.pdf}
\vspace{-2em}
\caption{Training Data Example (Answerer – [SUFFICIENT])
}
\label{fig:ans_case2}
\end{figure*}








\clearpage



\section{Training Details}
\label{ap:training}


\subsection{Classifier \& Shifter Training for Theme Scoping}

\textbf{\underline{Classifier Training.}} For theme classification, we trained a multilabel document classifier on the DBPedia-298 dataset \cite{lehmann2015dbpedia} using a two-stage architecture. The first stage uses the nomic-embed-text-v1 encoder model \cite{nussbaum2024nomic} to generate document embeddings, followed by a classification head consisting of a two-layer neural network. The training size and the validation size are 240,932 and 36,008, respectively.

The classifier consists of:
\begin{itemize}
    \item An encoder based on nomic-embed-text-v1 for generating document embeddings
    \item A classification head with:
        \begin{itemize}
            \item Input layer: embedding dimension → 512 units
            \item ReLU activation
            \item Dropout (p=0.1)
            \item Output layer: 512 → number of labels (298)
            \item Sigmoid activation for multilabel outputs
        \end{itemize}
\end{itemize}

We jointly trained both the encoder and classification head with different learning rates:
\begin{itemize}
\item \textbf{Classification head}: Trained with a higher learning rate (2e-4) to allow faster adaptation to the classification task
\item \textbf{Text Encoder}: Fine-tuned with a lower learning rate (2e-5) to preserve pretrained knowledge while making subtle adjustments for the domain
\end{itemize}
This dual learning rate strategy allows the model to benefit from the pretrained knowledge in the encoder while optimizing the classification head for the specific multilabel task. The lower learning rate for the encoder helps prevent catastrophic forgetting of the pretrained representations.

The model was evaluated using micro-averaged metrics on the validation set during training, with monitoring of F1 score, precision, and recall, as shown in Figure \ref{fig:classifier_train}. We select the best classifier model based on the best validation F1 score achieved during training.


\textbf{\underline{Distribution Shifter Training.} }The theme distribution shifter maps query theme distributions to expected document theme distributions.
Training involves two phases:
\paragraph{Phase 1: PPD Labeling} Using our constructed HotpotQA-SUBQ dataset (in Appendix \ref{ap:train_data}) with 204,638 query-doc pairs (where we set training-validation ratio as 9:1):
\begin{itemize}
\item \textbf{Input Generation}: Process all subqueries and supporting documents through the trained theme classifier to obtain their Posterior Probability Distributions (PPDs)
\item \textbf{Pairing}: Each subquery is paired with its corresponding supporting document, creating query-document PPD pairs
\item \textbf{Data Structure}: The labeled dataset consists of:
\begin{itemize}
\item Input: Query PPDs (298-dimensional probability vectors)
\item Target: Corresponding document PPDs (298-dimensional probability vectors)
\end{itemize}
\end{itemize}
\paragraph{Phase 2: Shifter Training} Train a neural network to map query PPDs to document PPDs:
\begin{itemize}
\item \textbf{Architecture:} A three-layer MLP with:
\begin{itemize}
\item Input layer: 298 → 512 units
\item Hidden layer: 512 → 256 units
\item Output layer: 256 → 298 units
\item ReLU activations and dropout (p=0.2) between layers
\item Softmax activation for final output
\end{itemize}
\item \textbf{Training Objectives:} Multiple distribution alignment metrics:
\begin{itemize}
\item KL divergence loss (primary objective)
\item Jensen-Shannon divergence monitoring
\item Wasserstein distance monitoring
\item L1 and L2 distance monitoring
\end{itemize}
\item \textbf{Training Configuration:}
\begin{itemize}
\item Optimizer: Adam with learning rate 1e-3
\item Batch size: 32
\item Early stopping based on validation loss
\item Numerical stability ensured through epsilon addition (1e-15)
\end{itemize}
\end{itemize}
Performance is tracked using validation KL divergence, Jensen-Shannon divergence, and Wasserstein distance to ensure proper distribution alignment, as shown in Figure \ref{fig:shifter_train}.


\subsection{Text-to-Triples Model Training}

For our text-to-triple conversion model, we fine-tuned a Flan-T5-Large model \cite{chung2024scaling} to transform raw text passages into structured knowledge triples. The model processes input text sequences up to 512 tokens in length and generates structured triples in a standardized format ``(S $>$ subject$|$ P$>$ predicate$|$ O$>$ object)''.
\footnote{Our fine-tuned $f_{t2t}$ can be found at \url{https://huggingface.co/pat-jj/text2triple-flan-t5} and \url{https://huggingface.co/pat-jj/text2graph-llama-3.2-3b}}

Our training dataset WikiOFGraph \cite{kim2024ontology}, curated by ChatGPT, comprises 5,851,776 text-triple pairs, with an additional 5,000 samples reserved for validation. Each training instance consists of a natural language text passage paired with its corresponding set of comma-separated triples. 
For example: 
\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{figures/t2t_example.pdf}
\vspace{-2em}
\caption{Example of WikiOFGraph data
}
\label{fig:wikiofgraph_data}
\end{figure*}

We implemented the training using the AdamW optimizer with a learning rate of 2e-5, incorporating linear warmup and decay schedules. The training process ran for 500,000 steps with a batch size of 32 per GPU and gradient accumulation over 4 steps. To optimize training efficiency and memory usage, we employed mixed-precision training using bfloat16 and applied weight decay at 0.01. The maximum source and target sequence lengths were set to 512 and 256 tokens respectively.

For evaluation, we primarily relied on ROUGE-1, ROUGE-2, and ROUGE-L scores to assess the quality of triple generation. We supplemented these metrics with custom triple matching accuracy measures that consider subject matching, predicate normalization, and object entity alignment. Validation metrics were computed at 5,000-step intervals throughout training.

% The training process followed a curriculum learning approach, beginning with shorter, simpler text passages and progressively introducing longer and more complex examples. We implemented early stopping based on validation ROUGE-L scores and maintained model checkpoints at points of peak validation performance. To prevent gradient instability, we applied gradient clipping with a threshold of 1.0. 
The training curves for various metrics are shown in Figure \ref{fig:t2t_train}, demonstrating steady improvement in the model's ability to extract structured knowledge from text. The training of this model was conducted on eight NVIDIA RTX 6000 with 48GB memory.



\input{figures/classifier_shifter_train}

\clearpage



\subsection{Action Planner \& Answerer Multi-task Training}
\label{apsub:plan_ans_train}
We employ GraphLLM \cite{perozzi2024let, he2024gretriever} as our foundational model architecture and utilize our constructed HotpotQA-SUBQ dataset to train a unified model capable of performing both graph-conditioned action planning and answer generation through multitask learning.

Since the Action Planner and Answerer share an identical architectural foundation, as illustrated in Figure \ref{fig:overview}, we differentiate their functionality through specialized instruction sets. For the Action Planner $\mathcal{M}_{Plan}$, we employ the following instruction template:

\begin{figure}[!h]
    \centering
    \begin{lstlisting}[language={}]
You are a planner to determine if the question can be answered with current information and output the appropriate label as well as the subquery if needed.
Output [NO_RETRIEVAL] if the question can be directly answered with the question itself without any retrieval.
Output [SUBQ] with an subquery for retrieval if still needs a subquery.
Output [SUFFICIENT] if the question can be answered with the provided information.
\end{lstlisting}
% \vspace{-2em}
    \caption{Instruction $\text{INST}_{\text{plan}}$ for Action Planner $\mathcal{M}_{Plan}$.}
    \label{fig:inst_plan}
\end{figure}

For the Answerer $\mathcal{M}_{Ans}$, we utilize this distinct instruction set:

\begin{figure}[!h]
    \centering
    \begin{lstlisting}[language={}]
You are an answerer given a question and retrieved graph information.
Each [SUBQ] is a subquery we generated through reasoning for the question. The retrieved graph information follows each [SUBQ] is relevant graph information we retrieved to answer the subquery.
[NO_RETRIEVAL] means the question can be answered with the question itself without any retrieval.
The main question starts with "Question: ". Please answer the question, with subqueries and retrieved graph information if they are helpful.
\end{lstlisting}
% \vspace{-2em}
    \caption{Instruction $\text{INST}_{\text{ans}}$ for Answer $\mathcal{M}_{Ans}$.}
    \label{fig:inst_ans}
\end{figure}

\begin{figure*}[h]
\centering
\includegraphics[width=0.9\linewidth]{figures/training_curve.png}
\vspace{-1em}
\caption{Training loss comparison over one epoch. The plot compares RAS-7B (green) and RAS-8B (orange) training trajectories. Two additional RAS-7B variants are shown: ``continual'': a continual learning approach where Answerer training precedes Action Planner training, and ``ptune'': a parameter-efficient variant that only tunes graph tokens without LoRA. The lower and more stable loss curves of the standard RAS variants demonstrate the effectiveness of joint training with LoRA. We use the last 100 steps for loss smoothing.}
\label{fig:train_curve}
\end{figure*}

For graph encoding, we implement Graph Transformer \cite{shi2020masked}, selected for its robust capability in handling non-fully-connected graphs—a common characteristic of text-extracted triples. Our base language models comprise LLaMA-2-7B and LLaMA-3-8B, chosen both to maintain consistency with previous research \cite{asai2023self,lyu2024retrieve} and to investigate our framework's performance scaling across different model capacities.


Our implementation of GraphLLM differs from G-Retriever \cite{he2024gretriever} primarily due to the distinct nature of our graph structures. While G-Retriever operates on single interconnected graphs, our framework processes multiple potentially disconnected subgraphs, each corresponding to different subqueries. To address this architectural difference, we adopt a sequential encoding strategy: rather than encoding the entire graph at once, we process each subgraph individually using Graph Transformer, followed by mean pooling across all subgraph embeddings to produce the final encoded representation.


The training process utilizes 4 NVIDIA RTX 6000 Ada Generation GPUs, each with 48GB memory. We train all models for 2 epochs using a batch size of 2 and gradient accumulation steps of 2, implementing a peak learning rate of 1e-5 with a 0.15 warmup ratio and 0.01 decay rate. The maximum sequence lengths are set to 300 tokens for generation and 2,500 tokens for input, with training conducted in BFloat16 precision.


\section{Evaluation Datasets \& Metrics}
\label{ap:metrics_datasets}

\subsection{Test Datasets}

We evaluate RAS on diverse benchmark datasets spanning short-form QA, closed-set tasks, and long-form generation in the zero-shot setting, aligning with \cite{asai2023self, lyu2024retrieve}. Below we describe each dataset:

\textbf{Short-form Generation Datasets:}
\begin{itemize}
    \item \textbf{TriviaQA-unfiltered (TQA)} \cite{joshi2017triviaqa}: A large-scale QA dataset containing 11,313 question-answer pairs in our test set. The questions are sourced from trivia enthusiasts and cover diverse topics. 

    \item \textbf{2WikiMultiHopQA (2WQA)} \cite{ho2020constructing}: A multi-hop question answering dataset (with 12,576 samples in test set) that requires models to combine information from multiple Wikipedia articles to answer questions. 

    \item \textbf{PopQA} \cite{mallen2022not}: A dataset focusing on questions about long-tail entities, containing 1,399 queries where the monthly Wikipedia page views are less than 100. These questions test models' ability to handle queries about less common entities. 
\end{itemize}

\textbf{Closed-set Tasks:}

\begin{itemize}
    \item \textbf{PubHealth} (Pub) \cite{zhang2023interpretable}: A dataset for verifying health-related claims. Models must classify statements as "true" or "false" based on scientific evidence. The dataset contains 987 test samples.
    \item \textbf{ARC-Challenge} (ARC) \cite{clark2018think}: A multiple-choice science question dataset designed to be challenging for models, requiring multi-hop reasoning and background knowledge. The dataset contains 1,172 test samples.
\end{itemize}

\textbf{Long-form Generation Datasets:}

\begin{itemize}
    \item \textbf{ASQA} \cite{stelmakh2022asqa}: A long-form question answering dataset that requires models to generate comprehensive answers with proper citation of sources. Models must balance information completeness with factual accuracy. The dataset contains 948 test samples.

    \item \textbf{ELI5} \cite{fan2019eli5}: A dataset derived from the "Explain Like I'm Five" subreddit, containing questions seeking straightforward explanations of complex topics. The dataset contains 1,000 test samples.
\end{itemize}

\clearpage
\subsection{Evaluation Metrics}

We employ different evaluation metrics appropriate for each task category:


\textbf{Short-form Generation}: 
    \begin{itemize}
        \item For PopQA and TriviaQA, we use the "golden match" metric \cite{asai2023self, min2019discrete, guu2020retrieval}\footnote{Implementation aligned with \url{https://github.com/AkariAsai/self-rag/blob/main/retrieval_lm/metrics.py}}, where a prediction $p$ is considered correct if it contains any normalized version of the ground truth answers $G = \{g_1,...,g_n\}$:
        
        \begin{equation}
            \text{match}(p, G) = \begin{cases}
                1 & \text{if } \exists g \in G: \text{norm}(g) \subseteq \text{norm}(p) \\
                0 & \text{otherwise}
            \end{cases}
        \end{equation}
        
        where $\text{norm}(\cdot)$ normalizes text by lowercasing, removing articles and punctuation.

        \item For 2WikiMultiHopQA, we follow RPG \cite{lyu2024retrieve}\footnote{Implementation aligned with \url{https://github.com/haruhi-sudo/RPG/blob/main/retriever/src/evaluation.py}} to use token-level F1 score between prediction $p$ and ground truth $g$:
        \begin{equation}
            \text{F1}(p,g) = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
        \end{equation}
        
        where precision and recall are computed over normalized token overlap:
        
        \begin{equation}
            \text{precision} = \frac{|\text{tokens}(p) \cap \text{tokens}(g)|}{|\text{tokens}(p)|}
        \end{equation}
        
        \begin{equation}
            \text{recall} = \frac{|\text{tokens}(p) \cap \text{tokens}(g)|}{|\text{tokens}(g)|}
        \end{equation}
    \end{itemize}

\textbf{Closed-set Tasks}: For both PubHealth and ARC-Challenge, we use accuracy \cite{asai2023self}, computed as:
    
    \begin{equation}
        \text{accuracy} = \frac{|\{i: \text{norm}(p_i) = \text{norm}(g_i)\}|}{N} \times 100
    \end{equation}
    
    where $N$ is the total number of examples.

\textbf{Long-form Generation}: For ASQA and ELI5, we use multiple metrics\footnote{Implementation aligned with \url{https://github.com/princeton-nlp/ALCE/blob/main/eval.py}}:
    \begin{itemize}
        \item ROUGE-L score to measure the longest common subsequence between prediction and reference \cite{lin2004rouge}
        \item MAUVE score \cite{pillutla2021mauve} to test the generation fluency by comparing the distribution of generated text against human references
    \end{itemize}

All scores are reported as percentages, multiplied by 100.



\clearpage




\section{Inference \& Evaluation Details}
\label{ap:inference_eval}
We evaluated RAS using both closed-source and open-source language models. For closed-source evaluation, we used Claude-3.5-Sonnet (Sonnet-3.5) as our base model. For open-source evaluation, we tested with both LLaMA-2-7B and LLaMA-3-8B, as shown in the performance table (Table \ref{tb:main_results}).

For the open-source models, we first fine-tuned the GraphLLM architecture on our HotpotQA-SUBQ dataset (see Section \ref{apsub:plan_ans_train}). We then conducted zero-shot knowledge-intensive generation tests. With Claude-3.5-Sonnet, we used few-shot prompting for both action planning and answer generation phases. Below, we detail our approach for each model type.


\subsection{Closed-Source Model Settings}

\textbf{For Text-to-Triple Conversion} (Figure~\ref{fig:close_t2t}), we instruct the model to extract structured knowledge triples following specific formatting rules. The prompt precisely defines the triple format as $(S\mathbin{>} subject \mathbin{|} P\mathbin{>} predicate \mathbin{|} O\mathbin{>} object)$ and provides comprehensive guidelines to ensure consistent knowledge representation. Key rules include extracting maximal meaningful relationships, maintaining original entity casing, avoiding pronoun usage, and ensuring clear predicate specification. The example demonstrates the conversion of a biographical text into structured triples, showing how complex information about an individual's life, career, and temporal details can be systematically decomposed into atomic facts. This standardized format enables reliable knowledge accumulation and reasoning in subsequent stages of the RAS framework.

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{figures/close_t2t.pdf}
\vspace{-2em}
\caption{Few-shot prompt for text-to-triples transformation with closed-source LLM.
}
\label{fig:close_t2t}
\end{figure*}

\textbf{For Action Planning} (Figure~\ref{fig:close_plan}), we design a comprehensive prompt that guides the model in making strategic decisions about information retrieval needs. The prompt instructs the model to output one of three labels based on careful analysis of the current knowledge state: (1) \texttt{[NO\_RETRIEVAL]} when the question can be answered directly, either due to the model's inherent knowledge or the question's nature, (2) \texttt{[SUBQ]} accompanied by a focused subquery when additional information is needed, or (3) \texttt{[SUFFICIENT]} when the accumulated knowledge is adequate to answer the question.
The prompt includes diverse examples demonstrating different scenarios:

(1) Generating follow-up queries based on partial information
(2) Creating new queries when relevant information is missing
(3) Recognizing when accumulated information is sufficient
(4) Identifying questions that don't require external knowledge
(5) Handling common knowledge questions

A key feature of the prompt is its emphasis on query efficiency - it explicitly prohibits generating redundant subqueries that might retrieve already-known information. This design helps maintain the system's efficiency while ensuring comprehensive knowledge gathering. 
% The examples cover a range of domains from company information to sports records, demonstrating the prompt's versatility in handling different types of knowledge-intensive queries.

\begin{figure*}[h]
\centering
\includegraphics[width=0.85\linewidth]{figures/close_plan.pdf}
% \vspace{-2em}
\caption{Few-shot prompt for action planning with closed-source LLM.
}
\label{fig:close_plan}
\end{figure*}

\clearpage

\textbf{For Answer Generation} (Figure \ref{fig:close_ans}), we design a prompt that focuses on synthesizing precise answers from structured knowledge graphs. The prompt emphasizes selective use of retrieved information - instructing the model to utilize subqueries and graph information only when relevant to the question at hand. A distinctive feature of this prompt is its requirement for definitive answers even under uncertainty or incomplete information, ensuring the model always provides a response.

The prompt includes carefully selected examples demonstrating two key scenarios: direct fact retrieval (Nobel Prize winner) and complex reasoning across multiple knowledge pieces (athlete's sport classification). These examples illustrate how to effectively combine information from multiple subqueries and their associated graph information to construct accurate answers. The prompt strictly enforces concise answer generation, requiring only the essential information without additional explanation or commentary.

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{figures/close_ans.pdf}
% \vspace{-2em}
\caption{Few-shot prompt for answer generation with closed-source LLM.
}
\label{fig:close_ans}
\end{figure*}

In addition, for ASQA and ELI5 with closed-source models (RAS$_{\text{Sonnet-3.5}}$ and all the other baselines (e.g., SuRe) using Sonnet-3.5), we conduct few-shot inference with two in-context learning demonstrations, aligning with previous study's \cite{gao2023enabling} implementation.\footnote{\url{https://github.com/princeton-nlp/ALCE/blob/main/run.py}}\footnote{Exemplars can be found at \url{https://github.com/princeton-nlp/ALCE/tree/main/prompts}}


\clearpage







\subsection{Open-Source Model Settings}
\label{apsub:open_setting}
For inference with open-source model, we use our GraphLLM trained by Hotpot-SUBQ (see Appendix \ref{apsub:plan_ans_train}), we use 8 NVIDIA RTX 6000 Ada Generation with 48GB memory and CUDA version 12.4. We use Python 3.10, PyTorch 2.1.2, and torch-geometric 2.6.1 throughout all experiments. 

For more efficient text retrieval, we split both theme index and dense index of the corpus into eight splits, and load them on eight GPUs with faiss-gpu 1.7.2.\footnote{All the Wikipedia dumps (2018, 2020) we used are downloaded from \url{https://github.com/facebookresearch/atlas}} 

We set maximum new tokens as \textbf{100 for PopQA, TriviaQA, and 2WikiMultihopQA}, as \textbf{50 for PubHealth and ARC-Challenge}, and as \textbf{300 for ASQA and ELI5}, aligning with previous study \cite{asai2023self, lyu2024retrieve}. All the generation are configured with \textbf{batch size of 1}. 

As we always set the main query as the first subquery, we pre-extract the triples from the context at the first iteration using our Flan-T5-based text-to-triple model for batch triple extraction. Notably, when we experimented with using Claude 3.5 Sonnet for triple extraction using the prompt from Figure \ref{fig:close_t2t}, we observed an additional 3\% average performance gain. This suggests room for improvement in our triple extraction approach - one potential direction would be training a larger model like LLaMA-2-7B on the WikiOFGraph dataset to achieve higher-quality triple extraction.

We use exact same instructions for action planner and answerer as we used in training phase, as shown in Figures \ref{fig:inst_plan} and \ref{fig:inst_ans}.



\subsection{Instructions}
We use the following task-specific instructions for zero-shot inference:

% Requires packages: booktabs, multirow
\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Dataset} &\textbf{Instruction} \\
\midrule
% 2WQA &Answer the following question. Only output the answer (even if you are not sure), do not say anything else.\\
ARC-C (baseline) & Given four answer candidates, A, B, C and D, choose the best answer choice. Please answer with the capitalized \\
&alphabet only, without adding any extra phrase or period.\\
ARC-C &Which is true? Output A, B, C, or D. \\
\midrule
PubHealth (baseline) & Is the following statement correct or not? Say true if it’s correct; otherwise, say false. Don’t capitalize or add \\ &periods, just say “true” or “false”.\\
PubHealth & Is statement 'true' or 'false'? Only output 'true' or 'false'.\\
\midrule
ASQA (baseline) & Instruction: Write an ACCURATE, ENGAGING, and CONCISE answer for the given question using the retrieved \\
& graph information (some of which might be irrelevant). Use an unbiased and journalistic tone. \\

ASQA & Answer the following question. The question may be ambiguous and have multiple correct answers, \\
&and in that case, you have to provide a long-form answer including all correct answers. [Long Form]\\
\midrule
ELI5 (baseline) & Instruction: Write an ACCURATE, ENGAGING, and CONCISE answer for the given question using the retrieved \\
& graph information (some of which might be irrelevant). Use an unbiased and journalistic tone. \\
ELI5 & Provide a paragraph-length response using simple words to answer the following question. [Long Form] \\

\bottomrule
\end{tabular}
}
\caption{Task-specific instructions. For short-form QA, we do not use instructions and use the original questions only.}

\label{tb:instructions}
% \vspace{-1em}
\end{table}

% \subsection{Instructions \& Prompts}








% \subsection{Additional Results}
% For the testing on 





\clearpage
\section{Qualitative Examples}
In this section, we show some qualitative examples and running examples of RAS.
\input{figures/example_popqa}
\input{figures/example_triviaqa}
\input{figures/example_long}
\input{figures/example_run}


\clearpage
% \section{Prompts Used for Close-Source Models}

\section{Hyperparameter Study}
\label{ap:hyper_param}
\input{tables/hyper_study}

\clearpage

\section{Notation Table}
% Notation Table
\begin{table*}[h]
\caption{Notation used throughout the paper}
\label{tab:notation}
\begin{center}
\begin{tabular}{cl}
\toprule
\textbf{Notation} & \textbf{Description} \\
\midrule
\multicolumn{2}{l}{\textit{General}} \\
$Q$ & Main input query \\
$A$ & Generated answer \\
$C$ & Complete document corpus \\
$C_{PPD}$ & Theme-indexed document corpus \\
\midrule
\multicolumn{2}{l}{\textit{Action Planning}} \\
$\mathcal{M}_{plan}$ & Action planner model \\
$p_i$ & Plan generated at iteration $i$ \\
$q_i$ & Subquery generated at iteration $i$ \\
$g_i$ & Graph information (triple list) extracted at iteration $i$ \\
\midrule
\multicolumn{2}{l}{\textit{Theme-Scoped Retrieval}} \\
$f_{cls}$ & Theme classifier function \\
$f_{shift}$ & Theme distribution shifter function \\
$f_{den}$ & Dense retriever function \\
$\text{PPD}(q_i)$ & Posteriori probability distribution of query $q_i$ \\
$\text{PPD}(d_i)$ & Expected document theme distribution \\
$c_i$ & Theme-specific sub-corpus at iteration $i$ \\
$t_i$ & Retrieved text passages at iteration $i$ \\
$K$ & Number of documents selected in theme scoping \\
$k$ & Number of passages selected in dense retrieval \\
\midrule
\multicolumn{2}{l}{\textit{Knowledge Structuring}} \\
$f_{t2t}$ & Text-to-triple conversion model \\
$f_{enc}$ & Text encoder for node/edge attributes \\
$f_{gnn}$ & Graph neural network encoder \\
$G_i$ & Knowledge graph at iteration $i$ \\
$G_Q$ & Final query-specific knowledge graph \\
$V_i, E_i$ & Node and edge sets of graph at iteration $i$ \\
$V_Q, E_Q$ & Node and edge sets of final graph \\
$v.attr$ & Attribute vector of node $v$ \\
$e.attr$ & Attribute vector of edge $e$ \\
\midrule
\multicolumn{2}{l}{\textit{Answer Generation}} \\
$\mathcal{M}_{ans}$ & Answerer model \\
$\hat{A}$ & Model-generated answer before retrieval \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}





\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
