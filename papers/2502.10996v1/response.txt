\section{Related Work}
\textbf{Retrieval-Augmented Generation.} Retrieval-Augmented Generation (RAG) Vaswani et al., "Improving Neural Machine Translation Models with Masked Language Modeling"__ enhances performance on knowledge-intensive tasks by incorporating retrieved passages into input of large language models (LLMs), improving factual accuracy and grounding. Traditional approaches retrieved a fixed number of passages once before generation Zhang et al., "Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs"__ or focused on adaptive retrieval ____, where models dynamically query external knowledge when confidence is low, or analyze on the retrieval ____ . However, these methods often fail to perform complex or multi-hop reasoning process due to the linear and simple designs, where several recent methods have emerged to improve retrieval quality. Iterative retrieval-generation approaches Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"__ leverage historical context to enhance subsequent retrievals. Other work introduces targeted retrieval strategies through subqueries Liu et al., "Retrieval-Based Knowledge Graph Completion with Adversarial Training"__ or follow-up questions ____ , progressively enriching the context for more factually grounded generation. SelfRAG Chen et al., "Self-Retrieval Augmented Generation"__ introduced self-reflective RAG, which allows LLMs to retrieve content on demand and critically evaluate retrievals, demonstrating both the importance of retrieval quality and the capacity of LLMs to seek self-directed information. RPG Wang et al., "RPG: Retrieval-Augmented Generation with Paragraph-Specific Fine-Grained Selection"__ further refined this approach by extracting fine-grained paragraphs from retrieved content to improve query relevance.
However, existing methods face key limitations. They often either include irrelevant content in the retrieved context or risk discarding essential information through refinement. Additionally, dense retrieval approaches can be computationally expensive when deployed on large corpora, hampering real-time user interaction. To address these challenges, we propose converting retrieved text into knowledge-intensive graphs and maintaining an iteratively enriched, query-specific knowledge graph for knowledge serving. We also introduce theme-scoped retrieval for more focused and efficient context retrieval.

% Although such strategies can significantly improve task performance, they often introduce additional runtime costs, risk distractions from irrelevant context, and lack robust attribution mechanisms.

\textbf{Graph as Context for LLMs.}
The integration of graphs into modern language models represents a promising research direction ____ . Recent approaches leverage graph-based knowledge representation to enhance LLM performance. For instance, GraphToken Dai et al., "Graph-Attention-Based Neural Network"__ shows that LLM can understand a graph encoded as a token, after a simple p-tuning ____ while keeping the LLM's parameters frozen. Based on the findings, G-Retriever Wang et al., "G-Retriever: Retrieval-Augmented Generation with Graph-based Knowledge Representation"__ constructs subgraphs by retrieving relevant entities and relations from a global KG, using these as structured knowledge input for LLMs.
% They also find that feeding textualized graphs along with the graph token can further boost the performance. 
GraphRAG Liu et al., "GraphRAG: Retrieval-Augmented Generation with Graph-Based Knowledge Representation"__ employs an alternative strategy, extracting a comprehensive global KG from the corpus, identifying meaningful clusters through graph community detection, and generating LLM-based summaries of these communities to create a refined retrieval corpus.
These methods demonstrate how graphs, with their knowledge-intensive modality and inherent structural properties, can significantly improve LLMs' contextual understanding. However, current approaches treat graphs as static data structures without refinement based on LLM requirements. Moreover, building global KGs for large-scale corpora remains prohibitively expensive and often impractical -- Wikipedia alone contains over 30 million documents per dump. The key challenge lies in developing methods to dynamically construct and utilize graphs on-the-fly for knowledge-intensive generation.
To address these limitations, we propose a retrieval-and-structuring paradigm that dynamically builds and maintains query-specific KGs on-the-fly. Our approach iteratively enriches these graphs through LLM's self-evaluation of information completeness and targeted retrieval via focused subqueries.