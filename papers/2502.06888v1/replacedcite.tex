\section{Background and Related Work}
% In this section, we first introduce the relevant knowledge about MoE in \autoref{section:2.1}. After that, we will introduce common practices for existing offloading work in \autoref{section:2.2}.


\subsection{\newtext{MoE Architecture \& Inference}}
\label{section:2.1}

Since GShard ____ introduced MoE structure into Transformer models, its potential to enhance the performance of large language models has been evident. MoE has gradually become one of the mainstream structures of large language models. Prominent models like GPT-4 ____, Gemini 1.5 ____, and Mixtral-8$\times$7B all incorporate the MoE structure.

\newtext{
The MoE architecture primarily consists of multiple MoE blocks, each containing an attention layer, an MoE layer, and two normalization layers, as illustrated in \autoref{figure:2}. }
The MoE layer comprises a gating network and multiple experts. 
The gating network is the key feature of the MoE architecture. It uses a softmax function to calculate the routing weights of each expert, then activates the top-k experts. Existing research____ indicates that the expert activation path of each token can reveal its characteristics, \newtext{facilitating the prediction of future expert selections.
Each expert is an FFN, and experts are sparsely activated, making MoE a feasible approach to training larger models.} For each token, the final output of the MoE layer is a weighted sum of the outputs from the selected experts.

\newtext{
The MoE inference process, like other LLMs, follows an autoregressive approach____, generating each new token based on the previous ones, as illustrated in \autoref{figure:2}. This process comprises two stages: prefill and decoding. In the prefill stage, the model processes the entire prompt simultaneously, which often leads to the activation of multiple experts. During the decoding stage, the model uses the previous token generated as input, iteratively generating new tokens until it generates the end-of-sequence (<EOS>) token or the maximum output length limit is reached.}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{pictures/moe_1110.pdf}
  \caption{\newtext{Architecture and inference process of MoE models.}}
  \label{figure:2}
\end{figure}

Some recent literature____ has focused on the optimization of MoE. DeepSpeed-MoE____ introduces a specialized MoE architecture called Pyramid-Residual MoE and employs staged knowledge distillation to obtain the Mixture-of-Students. This approach not only accelerates MoE training but also reduces inference latency and cost. Lina____, an extension of DeepSpeed-MoE, prioritizes all-to-all communication during training to enhance bandwidth and uses resource scheduling based on hot experts during inference to balance workload. However, these MoE systems primarily focus on latency-sensitive scenarios and place emphasis on MoE training. Klotski contributed to the memory optimization of MoE and is orthogonal to many of these works.

\subsection{Offloading in LLM Inference}
\label{section:2.2}
LLMs often have a large number of parameters, causing severe GPU memory bottlenecks during inference. Common memory optimisation techniques include quantization\newtext{____}, pruning\newtext{____}, sparse attention\newtext{____}, etc. Among these, offloading is a particularly effective strategy in resource-constrained environments. \newtext{As shown in~\autoref{figure:offloading}, DRAM and disk often have at least dozens of times more memory than VRAM. When it is difficult to store all model parameters in VRAM (as in the red line), offloading strategies offload tensors not currently involved in computation to DRAM or disk, freeing up a significant amount of VRAM (as in the black lines). Consequently, offloading strategies allow LLM inference to be performed with extremely small memory footprints. However, because the I/O speed between VRAM and DRAM is slower than the GPU's computing speed, frequent I/O will cause large delays in inference.}

Early works____ proposed leveraging swapping during the training of Deep Neural Networks (DNNs) to reduce GPU memory demands. ZeRO-Offload____ applied the offloading to the training of Transformer-based LLMs. ZeRO-Infinity____ extended this approach by incorporating disk as an additional offloading destination. DeepSpeed-Inference____, which includes the ZeRO-Inference component, applies offloading techniques to the inference, enabling LLM inference in resource-constrained environments. FlexGen____ significantly improves inference throughput by solving linear programming problems within the computational graph. HeteGen____ leverages heterogeneous parallel computing between CPU and GPU, reducing the need for parameter I/O and achieving better resource allocation. STI____ maximizes IO/compute resource utilization through model sharding and elastic pipeline planning. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{pictures/offloading_1112.pdf}
  \caption{\newtext{Illustration of offloading an LLM in a multi-level storage system. Only a few layers of parameters can be placed in VRAM, and the rest are placed in DRAM and disk. param. refers to parameters.}}
  \label{figure:offloading}
\end{figure}

However, most offloading systems mentioned above are designed for dense models and are thus inadequate for supporting MoE inference. Mixtral-offloading____ identified this limitation earlier and utilized LRU cache and quantization to quickly load a subset of experts, enabling the inference of Mixtral-8$\times$7B on consumer-grade hardware. Fiddler____ designed CPU-GPU orchestration specifically for MoE models, leveraging the computational power of CPUs to minimize data movement between the CPU and GPU. MoE-Infinity____ significantly reduced the latency overhead associated with offloading experts through activation-aware expert prefetching and caching mechanisms. Even if these works design accurate prefetch strategies or use the computing power of the CPU to accelerate the inference of MoE, it is still difficult to balance the gap between computation and I/O, resulting in lots of bubbles in the pipeline. In contrast, \textsc{Klotski} minimizes the bubbles in the pipeline by simultaneously arranging the computations of multiple batches and making full use of the computing resources of the GPU.