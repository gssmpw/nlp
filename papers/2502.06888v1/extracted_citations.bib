@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{aminabadi2022deepspeed,
  title={Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2022},
  organization={IEEE}
}

@article{eliseev2023fast,
  title={Fast inference of mixture-of-experts language models with offloading},
  author={Eliseev, Artyom and Mazur, Denis},
  journal={arXiv preprint arXiv:2312.17238},
  year={2023}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@inproceedings{guo2023sti,
  title={Sti: Turbocharge nlp inference at the edge via elastic pipelining},
  author={Guo, Liwei and Choe, Wonkyo and Lin, Felix Xiaozhu},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={791--803},
  year={2023}
}

@inproceedings{he2022fastermoe,
  title={Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models},
  author={He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={120--134},
  year={2022}
}

@inproceedings{huang2020swapadvisor,
  title={Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping},
  author={Huang, Chien-Chin and Jin, Gu and Li, Jinyang},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={1341--1355},
  year={2020}
}

@article{kamahori2024fiddler,
  title={Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models},
  author={Kamahori, Keisuke and Gu, Yile and Zhu, Kan and Kasikci, Baris},
  journal={arXiv preprint arXiv:2402.07033},
  year={2024}
}

@article{kim2023mixture,
  title={Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness},
  author={Kim, Young Jin and Fahim, Raffy and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2310.02410},
  year={2023}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@inproceedings{li2023accelerating,
  title={Accelerating distributed MoE training and inference with lina},
  author={Li, Jiamin and Jiang, Yimin and Zhu, Yibo and Wang, Cong and Xu, Hong},
  booktitle={2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  pages={945--959},
  year={2023}
}

@article{lin2024moe,
  title={Moe-llava: Mixture of experts for large vision-language models},
  author={Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li},
  journal={arXiv preprint arXiv:2401.15947},
  year={2024}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@inproceedings{peng2020capuchin,
  title={Capuchin: Tensor-based gpu memory management for deep learning},
  author={Peng, Xuan and Shi, Xuanhua and Dai, Hulin and Jin, Hai and Ma, Weiliang and Xiong, Qian and Yang, Fan and Qian, Xuehai},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={891--905},
  year={2020}
}

@inproceedings{rajbhandari2021zero,
  title={Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning},
  author={Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  booktitle={Proceedings of the international conference for high performance computing, networking, storage and analysis},
  pages={1--14},
  year={2021}
}

@inproceedings{rajbhandari2022deepspeed,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International conference on machine learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}

@inproceedings{ren2021zero,
  title={Zero-offload: Democratizing $\{$billion-scale$\}$ model training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={551--564},
  year={2021}
}

@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{xuanlei2024hetegen,
  title={HeteGen: Efficient Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices},
  author={XUANLEI, ZHAO and Jia, Bin and Zhou, Haotian and Liu, Ziming and Cheng, Shenggan and You, Yang},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={162--172},
  year={2024}
}

@article{xue2024moe,
  title={MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving},
  author={Xue, Leyang and Fu, Yao and Lu, Zhan and Mai, Luo and Marina, Mahesh},
  journal={arXiv preprint arXiv:2401.14361},
  year={2024}
}

@article{xue2024raphael,
  title={Raphael: Text-to-image generation via large mixture of diffusion paths},
  author={Xue, Zeyue and Song, Guanglu and Guo, Qiushan and Liu, Boxiao and Zong, Zhuofan and Liu, Yu and Luo, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{zhai2023smartmoe,
  title={SmartMoE: Efficiently Training $\{$Sparsely-Activated$\}$ Models through Combining Offline and Online Parallelization},
  author={Zhai, Mingshu and He, Jiaao and Ma, Zixuan and Zong, Zan and Zhang, Runqing and Zhai, Jidong},
  booktitle={2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  pages={961--975},
  year={2023}
}

@article{zhang2023h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}

