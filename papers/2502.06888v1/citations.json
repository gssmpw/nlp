[
  {
    "index": 0,
    "papers": [
      {
        "key": "lepikhin2020gshard",
        "author": "Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng",
        "title": "Gshard: Scaling giant models with conditional computation and automatic sharding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "team2023gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xue2024raphael",
        "author": "Xue, Zeyue and Song, Guanglu and Guo, Qiushan and Liu, Boxiao and Zong, Zhuofan and Liu, Yu and Luo, Ping",
        "title": "Raphael: Text-to-image generation via large mixture of diffusion paths"
      },
      {
        "key": "li2023accelerating",
        "author": "Li, Jiamin and Jiang, Yimin and Zhu, Yibo and Wang, Cong and Xu, Hong",
        "title": "Accelerating distributed MoE training and inference with lina"
      },
      {
        "key": "xue2024moe",
        "author": "Xue, Leyang and Fu, Yao and Lu, Zhan and Mai, Luo and Marina, Mahesh",
        "title": "MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving"
      },
      {
        "key": "lin2024moe",
        "author": "Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li",
        "title": "Moe-llava: Mixture of experts for large vision-language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhai2023smartmoe",
        "author": "Zhai, Mingshu and He, Jiaao and Ma, Zixuan and Zong, Zan and Zhang, Runqing and Zhai, Jidong",
        "title": "SmartMoE: Efficiently Training $\\{$Sparsely-Activated$\\}$ Models through Combining Offline and Online Parallelization"
      },
      {
        "key": "he2022fastermoe",
        "author": "He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin",
        "title": "Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models"
      },
      {
        "key": "rajbhandari2022deepspeed",
        "author": "Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong",
        "title": "Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale"
      },
      {
        "key": "li2023accelerating",
        "author": "Li, Jiamin and Jiang, Yimin and Zhu, Yibo and Wang, Cong and Xu, Hong",
        "title": "Accelerating distributed MoE training and inference with lina"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rajbhandari2022deepspeed",
        "author": "Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong",
        "title": "Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "li2023accelerating",
        "author": "Li, Jiamin and Jiang, Yimin and Zhu, Yibo and Wang, Cong and Xu, Hong",
        "title": "Accelerating distributed MoE training and inference with lina"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      },
      {
        "key": "kim2023mixture",
        "author": "Kim, Young Jin and Fahim, Raffy and Awadalla, Hany Hassan",
        "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness"
      },
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ma2023llm",
        "author": "Ma, Xinyin and Fang, Gongfan and Wang, Xinchao",
        "title": "Llm-pruner: On the structural pruning of large language models"
      },
      {
        "key": "frantar2023sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      },
      {
        "key": "zhang2023h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "huang2020swapadvisor",
        "author": "Huang, Chien-Chin and Jin, Gu and Li, Jinyang",
        "title": "Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping"
      },
      {
        "key": "peng2020capuchin",
        "author": "Peng, Xuan and Shi, Xuanhua and Dai, Hulin and Jin, Hai and Ma, Weiliang and Xiong, Qian and Yang, Fan and Qian, Xuehai",
        "title": "Capuchin: Tensor-based gpu memory management for deep learning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ren2021zero",
        "author": "Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong",
        "title": "Zero-offload: Democratizing $\\{$billion-scale$\\}$ model training"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "rajbhandari2021zero",
        "author": "Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong",
        "title": "Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "aminabadi2022deepspeed",
        "author": "Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others",
        "title": "Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "sheng2023flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\\'e}, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "Flexgen: High-throughput generative inference of large language models with a single gpu"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "xuanlei2024hetegen",
        "author": "XUANLEI, ZHAO and Jia, Bin and Zhou, Haotian and Liu, Ziming and Cheng, Shenggan and You, Yang",
        "title": "HeteGen: Efficient Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "guo2023sti",
        "author": "Guo, Liwei and Choe, Wonkyo and Lin, Felix Xiaozhu",
        "title": "Sti: Turbocharge nlp inference at the edge via elastic pipelining"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "eliseev2023fast",
        "author": "Eliseev, Artyom and Mazur, Denis",
        "title": "Fast inference of mixture-of-experts language models with offloading"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "kamahori2024fiddler",
        "author": "Kamahori, Keisuke and Gu, Yile and Zhu, Kan and Kasikci, Baris",
        "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "xue2024moe",
        "author": "Xue, Leyang and Fu, Yao and Lu, Zhan and Mai, Luo and Marina, Mahesh",
        "title": "MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving"
      }
    ]
  }
]