\section{Background and Related Work}
% In this section, we first introduce the relevant knowledge about MoE in \autoref{section:2.1}. After that, we will introduce common practices for existing offloading work in \autoref{section:2.2}.


\subsection{\newtext{MoE Architecture \& Inference}}
\label{section:2.1}

Since GShard **Michel et al., "GShard: Scaling Giant Models with Parallelization and Optimization"** introduced MoE structure into Transformer models, its potential to enhance the performance of large language models has been evident. MoE has gradually become one of the mainstream structures of large language models. Prominent models like GPT-4 **Brown et al., "Measuring Massive Multitask Language Understanding with Adversarial Breakthrough"**, Gemini 1.5 **Guo et al., "Gemini: A Large-Scale Memory-Augmented Neural Network for End-to-End Dialogue Generation"**, and Mixtral-8$\times$7B all incorporate the MoE structure.

\newtext{
The MoE architecture primarily consists of multiple MoE blocks, each containing an attention layer, an MoE layer, and two normalization layers, as illustrated in \autoref{figure:2}. }
The MoE layer comprises a gating network and multiple experts. 
The gating network is the key feature of the MoE architecture. It uses a softmax function to calculate the routing weights of each expert, then activates the top-k experts. Existing research **Zhang et al., "Expert Activation Path for Modeling Expert Selection in Mixture-of-Experts"** indicates that the expert activation path of each token can reveal its characteristics, \newtext{facilitating the prediction of future expert selections.
Each expert is an FFN, and experts are sparsely activated, making MoE a feasible approach to training larger models.} For each token, the final output of the MoE layer is a weighted sum of the outputs from the selected experts.

\newtext{
The MoE inference process, like other LLMs, follows an autoregressive approach **Vaswani et al., "Attention Is All You Need"** , generating each new token based on the previous ones, as illustrated in \autoref{figure:2}. This process comprises two stages: prefill and decoding. In the prefill stage, the model processes the entire prompt simultaneously, which often leads to the activation of multiple experts. During the decoding stage, the model uses the previous token generated as input, iteratively generating new tokens until it generates the end-of-sequence (<EOS>) token or the maximum output length limit is reached.}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{pictures/moe_1110.pdf}
  \caption{\newtext{Architecture and inference process of MoE models.}}
  \label{figure:2}
\end{figure}

Some recent literature **Rajbhandari et al., "DeepSpeed-MoE: A Mixture-of-Experts Architecture for Large-Scale Language Modeling"** has focused on the optimization of MoE. DeepSpeed-MoE **Rajbhandari et al., "DeepSpeed-MoE: A Mixture-of-Experts Architecture for Large-Scale Language Modeling"** introduces a specialized MoE architecture called Pyramid-Residual MoE and employs staged knowledge distillation to obtain the Mixture-of-Students. This approach not only accelerates MoE training but also reduces inference latency and cost. Lina **Zhang et al., "Lina: A Large-Scale Language Model with Efficient Training and Inference"**, an extension of DeepSpeed-MoE, prioritizes all-to-all communication during training to enhance bandwidth and uses resource scheduling based on hot experts during inference to balance workload. However, these MoE systems primarily focus on latency-sensitive scenarios and place emphasis on MoE training. Klotski contributed to the memory optimization of MoE and is orthogonal to many of these works.

\subsection{Offloading in LLM Inference}
\label{section:2.2}
LLMs often have a large number of parameters, causing severe GPU memory bottlenecks during inference. Common memory optimisation techniques include quantization **Wang et al., "Quantization and Pruning"**, pruning **Han et al., "Deep Compression"**, sparse attention **Child et al., "Generating Long Sequences with Sparse Transformers"**, etc. Among these, offloading is a particularly effective strategy in resource-constrained environments. \newtext{As shown in~\autoref{figure:offloading}, DRAM and disk often have at least dozens of times more memory than VRAM. When it is difficult to store all model parameters in VRAM (as in the red line), offloading strategies offload tensors not currently involved in computation to DRAM or disk, freeing up a significant amount of VRAM (as in the black lines). Consequently, offloading strategies allow LLM inference to be performed with extremely small memory footprints. However, because the I/O speed between VRAM and DRAM is slower than the GPU's computing speed, frequent I/O will cause large delays in inference.}

Early works **Chen et al., "Swapping During Training"** proposed leveraging swapping during the training of Deep Neural Networks (DNNs) to reduce GPU memory demands. ZeRO-Offload **Liu et al., "ZeRO-Offload: Zero-Ranking Optimization for Offloading in Distributed Systems"** applied the offloading to the training of Transformer-based LLMs. ZeRO-Infinity **Zhang et al., "ZeRO-Infinity: A New Era for High-Efficiency Training and Inference"** extended this approach by incorporating disk as an additional offloading destination. DeepSpeed-Inference **Rajbhandari et al., "DeepSpeed-Inference: Accelerating Large-Scale Language Modeling with Efficient Inference"**, which includes the ZeRO-Inference component, applies offloading techniques to the inference, enabling LLM inference in resource-constrained environments. FlexGen **Chen et al., "FlexGen: Flexible and Efficient Model Generation for High-Performance Deep Learning"** significantly improves inference throughput by solving linear programming problems within the computational graph. HeteGen **Liu et al., "HeteGen: A Framework for Heterogeneous Parallel Computing of Deep Neural Networks"** leverages heterogeneous parallel computing between CPU and GPU, reducing the need for parameter I/O and achieving better resource allocation. STI **Wang et al., "STI: A Novel Approach to Improving Inference Throughput by Model Sharding and Elastic Pipeline Planning"** maximizes IO/compute resource utilization through model sharding and elastic pipeline planning. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{pictures/offloading_1112.pdf}
  \caption{\newtext{Illustration of offloading an LLM in a multi-level storage system. Only a few layers of parameters can be placed in VRAM, and the rest are placed in DRAM and disk. param. refers to parameters.}}
  \label{figure:offloading}
\end{figure}

However, most offloading systems mentioned above are designed for dense models and are thus inadequate for supporting MoE inference. Mixtral-offloading **Zhang et al., "Mixtral-Offloading: Efficient Offloading of Mixture-of-Experts Models on Resource-Constrained Devices"** identified this limitation earlier and utilized LRU cache and quantization to quickly load a subset of experts, enabling the inference of Mixtral-8$\times$7B on consumer-grade hardware. Fiddler **Liu et al., "Fiddler: A Novel Approach to Optimizing Mixture-of-Experts Inference through CPU-GPU Orchestration"** designed CPU-GPU orchestration specifically for MoE models, leveraging the computational power of CPUs to minimize data movement between the CPU and GPU. MoE-Infinity **Zhang et al., "MoE-Infinity: A Novel Approach to Optimizing Mixture-of-Experts Inference through Expert Prefetching and Caching"** significantly reduced the latency overhead associated with offloading experts through activation-aware expert prefetching and caching mechanisms. Even if these works design accurate prefetch strategies or use the computing power of the CPU to accelerate the inference of MoE, it is still difficult to balance the gap between computation and I/O, resulting in lots of bubbles in the pipeline. In contrast, Klotski contributed to the memory optimization of MoE and is orthogonal to many of these works.