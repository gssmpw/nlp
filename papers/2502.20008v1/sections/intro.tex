\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser.pdf}
    \vspace{-0.15in}
    \caption{Multi-modal retrieval results on a collection of retrieval tasks, categorized as 1) Single-modal, 2) Cross-modal, 3) Mixed-modal, 4) Multi-modal, and 5) Conditional. CLIP~\cite{radford2021learning} is two-tower model, UniIR~\cite{wei2024uniir} is two-legs model with late-fusion, and ours is built as one-tower with early fusion. Our approach obtains moderate improvements in retrieval tasks requiring no modality fusion compared to state-of-the-art two-tower methods, the gains become prominent when the tasks need modality fusion that involve multiple modalities or conditional information in the query or candidate. Specific results are in \S\ref{sec:exp}.
    }
    \label{fig:teaser}
    \vspace{-0.1in}
\end{figure}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/methodology_20250215.pdf}
    \caption{Conceptual illustration of multi-modal retrieval frameworks.
        \textit{(a)} Traditional two-tower methods encode text and images separately, limiting early cross-modal interactions and often overlooking nuanced user queries.
        \textit{(b)} Two-leg methods introduce an additional fusion network to merge single-modal embeddings, enabling more complex tasks yet still deferring cross-modal interplay until later stages.
        \textit{(c)} Our proposed \modelfullname{}~(\modelname{}), an one-tower method, integrates visual and textual cues from the ground up, unifying the embedding space for a direct contrastive learning and facilitating fine-grained multi-modal understanding. This approach, including two stages: post-training adaptation and instruction tuning, simplifies and improves the retriever when complex multi-modal understanding is required.}
    \label{fig:vlretrieval}
\end{figure*}


\section{Introduction}
\label{sec:intro}
Retrieval is a cornerstone task in modern Internet systems and artificial intelligence, traditionally built on semantic matching to identify relevant information from vast datasets. Early retrieval methods~\cite{robertson1995okapi, gao-etal-2021-simcse, wu-etal-2022-pcl} focused exclusively on single modalities--primarily text--which yielded effective systems in limited settings but inherently lacked the capacity to process multi-modal inputs. As applications evolved to require richer, cross-modal information, such as integrating visual content alongside text, the limitations of these conventional methods became increasingly evident. In response, researchers adapted traditional approaches~\cite{radford2021learning} using late-fusion~\cite{wei2024uniir}, two-tower architectures, where each modality is encoded separately and only fused at a high level. Although this strategy bridges the gap between modalities, it falls short in capturing the nuanced, early interactions, and fine-grained details essential for fully understanding complex user intentions. In emerging applications such as retrieval-augmented generation~\cite{lewis2020retrieval,gao2023retrieval}, context-aware search~\cite{han2017fashion200k, Liu_2021_ICCV}, and conditional search~\cite{Vaze2023GeneCIS}, the simple high-level fusion of independent representations is insufficient. Instead, these scenarios demand an integrated approach that can blend visual and textual cues from the ground up, thereby enabling more precise interpretation of intricate, multi-modal queries.


% Retrieval is one of the fundamental tasks in Internet systems and Artificial Intelligence, focusing on retrieving relevant information from large datasets given queries. Traditionally, retrieval systems have primarily relied on semantic matching, which, while effective, often fails to address the nuanced demands of emerging applications such as retrieval-augmented generation~\cite{gao2023retrieval,lewis2020retrieval}, context-aware retrieval\hl{can we have at least one ref for each point?}, and personalized search. Aiming to better align with users' intentions, recent studies have broadened the scope of retrieval systems to incorporate instruction-following capabilities~\cite{asai-etal-2023-task, su-etal-2023-one, wei2024uniir}. Central to this transformation is the integration of pre-trained large language models~\cite{brown2020language}, renowned for their impressive capability of understanding and executing instructions, as backbone architectures, which has significantly elevated the performance of retrieval systems in text-based scenarios~\cite{behnamghader2024llm2vec}. Yet, the challenges become markedly more complicated when we extend the retrieval paradigm to cross-modal environments, where contextual information often spans textual and visual modalities.


% In this work, we propose a unified multi-modal retrieval framework, \modelfullname{}~(\modelname{}), that seamlessly interweaves encoding and fusion into a single process, fundamentally rethinking the way retrieval systems handle complex, heterogeneous queries. Conventional approaches retrofit single-modal systems to accommodate multi-modal tasks by independently encoding each modality and subsequently merging their representations--a late-fusion strategy that often fails to capture the nuanced interplay between textual and visual cues. In contrast, our method embraces a joint fusion and encoding paradigm, wherein fusion occurs concurrently with encoding. This early integration addresses the intrinsic challenge of harmonizing the disparate characteristics of multi-modal data, enabling the capture of fine-grained, inter-modal interactions and preserving the modality-specific nuances critical for deciphering intricate user intentions.
In this work, we propose a unified multi-modal retrieval framework, \modelfullname{}~(\modelname{}), that seamlessly interweaves encoding and fusion into a single process, fundamentally rethinking the way retrieval systems handle complex, heterogeneous queries. As shown in Figure~\ref{fig:vlretrieval}, conventional approaches retrofit single-modal systems to accommodate multi-modal tasks by independently encoding each modality and subsequently merging their representations--a late-fusion strategy that often fails to capture the nuanced interplay between textual and visual cues. In contrast, one-tower architecture embraces a joint fusion and encoding paradigm, wherein fusion occurs concurrently with encoding. This early integration addresses the intrinsic challenge of harmonizing the disparate characteristics of multi-modal data, enabling the capture of fine-grained, inter-modal interactions and preserving the modality-specific nuances critical for deciphering intricate user intentions.

Besides above fundamental benefits~\cite{jang2023unifying, li2024multimodal} of such one-tower paradigm for multi-modal representation, recent advent of large-scale pre-trained large language models (LLMs) and multi-modal LLMs (MLLMs) built on them, has made the one-tower design become even more compelling.
By repurposing MLLMs and adapting them into effective encoders through contrastive learning and instruction-tuning, our approach not only simplifies the retrieval pipeline by eliminating the need for separate fusion modules but also produces unified, powerful embeddings that significantly enhance performance across a range of multi-modal retrieval tasks. The proposed approach consists of two key stages:
1)  Post-training adaptation:
We begin by adapting the MLLM, which was originally pre-trained as an autoregressive decoder, to function as an encoder. This is achieved through post-training using contrastive learning loss, allowing the model to effectively process inputs into meaningful embeddings;
2) Instruction tuning
We then fine-tune the model with instructive information specific to various cross-modal retrieval tasks. This step enables the model to better understand and follow instructions in both textual and visual contexts.

% In the context of cross-modal retrieval, a particular hurdle is to handle instructive information that cannot be adequately described in text alone. This limitation highlights the need for a more sophisticated approach to query encoding. To address this challenge, it is crucial to jointly model both visual and textual information when encoding queries into embeddings. This integrated approach allows for a more comprehensive understanding of instructive information across modalities, potentially improving the accuracy and versatility of cross-modal retrieval systems.
% However, conventional two-tower models face significant limitations in addressing this complexity effectively: (1) their architecture inherently lacks the capability to natively integrate multi-modal information, and (2) their training, exclusively on textual and/or visual descriptions, leaves them ill-equipped to comprehend and execute instruction-based queries.

% Concurrently, Multi-modal Large Language Models (MLLMs) have demonstrated impressive performance in various tasks~\cite{alayrac2022flamingo,openai2023chatgpt,liu2023visual,beyer2024paligemma} including visual-language instruction-following, suggesting a promising direction for understanding multi-modal inputs.
% MLLMs on visual-language retrieval have several key advantages compared with conventional methods:
% (1) \emph{Multi-modal Understanding}: The model effectively processes and interprets both textual and visual information and instructions simultaneously;
% (2) \emph{Flexible Input Handling}: the adapted encoder can seamlessly accept and process inputs regardless of text, images, or both, without requiring additional training and modules;
% (3) \emph{Unified Embeddings}: The output is a single embedding that encapsulates the rich, multi-modal information, facilitating efficient retrieval across texts, images, and composed modalities.




% Building upon the need introduced above, our work focuses on leveraging MLLMs to enhance visual-language retrieval. Our approach consists of several key steps.
% % (1)  Post-training adaptation
% We begin by adapting the MLLM, which was originally pre-trained as an autoregressive decoder, to function as an encoder. This is achieved through post-training using contrastive learning loss, allowing the model to effectively process inputs into meaningful embeddings.
% % (2) Fine-tuning
% We then fine-tune the model with instructive information specific to various cross-modal retrieval tasks. This step enables the model to better understand and follow instructions in both textual and visual contexts.
% experiments
To thoroughly evaluate the capabilities of multi-modal retrieval models, we have organized extensive experiments across a comprehensive set of benchmarks, categorized by their input and output modalities: 1) single-modal, both the query and candidate are in the same modality; 2) cross-modal, the query and candidate are in different modalities; 3) mixed-modal, either query and candidate is multi-modal and the other remains single-modal; 4) multi-modal, both query and candidate are multi-modal; and 5) conditional, the query is supplemented with additional contextual or conditional information.
As shown in Figure~\ref{fig:teaser}, we can observe that while our joint fusion and encoding method \modelname{} obtains moderate improvements in standard single-modal and cross-modal retrieval compared to state-of-the-art two-tower methods, the gains become even prominent when the query and/or candidate involve multiple modalities or conditional information, the scenarios attaining mounting attention as massive multi-modal contents and instructions are produced every second.

% In summary, we make the following contributions to multi-modal retrieval:
% \begin{itemize}[leftmargin=0.1in, itemsep=0in]
%     \item We introduce a novel approach, Joint Fusion Encoder, that integrates fusion directly into the encoding process, repurposing MLLMs to generate unified embeddings that capture fine-grained interactions across diverse modalities.
%     \item We design a well-organized and extensive evaluation suite covering a wide array of retrieval scenarios—including single-modal, cross-modal, mixed-modal, multi-modal, and conditional tasks—to rigorously assess the effectiveness of multi-modal retrievers.
%     \item We demonstrate that \modelname{} consistently outperforms state-of-the-art two-tower architectures, particularly excelling in scenarios that demand the precise interpretation of intricate, multi-modal queries, thereby setting a new benchmark for intelligent, context-aware retrieval systems.
% \end{itemize}

% \qy{remove the summarization?}

% In summary, we propose \modelname{}, a MLLM-based retriever that handles flexible modalities of inputs.
% This unified model not only simplifies the retrieval pipeline but also enhances the model's ability to perform multi-modal understanding effectively, fulfilling the complex intention behind the queries with instructions. By capitalizing on the MLLM's multi-modal understanding and our tailored adaptation steps, we achieve a versatile encoder that outperforms baselines on various tasks, without additional training-required modules for each retrieval scenario.

% The following experimental findings are also observed:
% \begin{itemize}
%     \item Instructions in post-training are important.
%     \item ???
% \end{itemize}


% 1. before: fusion after encoding, too focusing on high-level. 
% 2. we propose joint fusion and encoding. pros: early fusion, good for emergent applications like: user intention, mm cot, object aware retrival
% 3. we organized different types of benchmarks