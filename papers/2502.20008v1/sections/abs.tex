\begin{abstract}
% The field of information retrieval is evolving rapidly, with an increasing demand for the instruction-following capability, in applications such as retrieval augmented generation and personalized search.
% This shift necessitates a deeper understanding of multi-modal contexts beyond mere semantic similarity or relational matching.
% A particular challenge for cross-modal retrieval arises from the fact that instructions can be either textual or visual, requiring retrievers to jointly model visual-language information. Traditional dual-tower models struggle to effectively address this complexity. Concurrently, visual language models (MLLMs) have demonstrated impressive performance in various tasks including visual-language instructions, suggesting a promising direction for research.
% This paper explores the repurposing of MLLMs for cross-modal retrieval tasks. We investigate effective methods to extract embeddings from MLLMs with two-stage adaptation, aiming to leverage their capabilities in understanding and processing both textual and visual information.
% Our study includes a comprehensive evaluation of retrieval tasks incorporating both textual and visual instructions, as well as single-modal retrieval.

% The field of information retrieval is evolving rapidly, driven by a growing demand for systems capable of interpreting human intent, particularly in applications like retrieval augmented generation and personalized search.
% This shift necessitates a deeper understanding of the contexts beyond mere semantic similarity or relational matching.
% The fact that instructions and context can be either textual or visual presents a particular challenge for cross-modal retrievers to jointly model visual-language information. Traditional dual-tower models fall short of handling this complexity effectively. Concurrently, Multi-modal Large Language Models (MLLMs) have demonstrated impressive performance in various tasks including visual-language instructions, suggesting a promising direction for research.
% This paper explores repurposing MLLMs for a wide range of vision-language retrieval tasks. We investigate effective methods to extract embeddings from MLLMs with two-stage adaptation, aiming to leverage their capabilities in understanding and processing both textual and visual information.
% Our study includes a comprehensive evaluation of retrieval tasks incorporating both textual and visual instructions, as well as single-modal retrieval.
% \qy{to be revised after the introduction is polished}
% Information retrieval systems are indispensable for today’s Internet applications, yet traditional semantic matching techniques--designed for single modalities--often fall short in capturing the fine-grained cross-modal interactions required for complex queries. Although late-fusion two-tower architectures attempt to bridge this gap by independently encoding visual and textual data before merging them at a high level, they frequently overlook the subtle interplay essential for comprehensive understanding. In this work, we rigorously assess these limitations and introduce a unified retrieval framework that fuses visual and textual cues from the ground up, enabling early cross-modal interactions that significantly enhance query interpretation. Our approach outperforms conventional methods across diverse retrieval scenarios, particularly when processing intricate multi-modal inputs, and underscores the transformative potential of early integration strategies, suggesting a promising direction toward contextually aware and effective information retrieval systems.

Information retrieval is indispensable for today’s Internet applications, yet traditional semantic matching techniques often fall short in capturing the fine-grained cross-modal interactions required for complex queries. Although late-fusion two-tower architectures attempt to bridge this gap by independently encoding visual and textual data before merging them at a high level, they frequently overlook the subtle interplay essential for comprehensive understanding.
In this work, we rigorously assess these limitations and introduce a unified retrieval framework that fuses visual and textual cues from the ground up, enabling early cross-modal interactions for enhancing context interpretation.
Through a two-stage training process—comprising post-training adaptation followed by instruction tuning—we adapt MLLMs as retrievers using a simple one-tower architecture. Our approach outperforms conventional methods across diverse retrieval scenarios, particularly when processing complex multi-modal inputs. Notably, the joint fusion encoder yields greater improvements on tasks that require modality fusion compared to those that do not, underscoring the transformative potential of early integration strategies and pointing toward a promising direction for contextually aware and effective information retrieval.

\end{abstract}