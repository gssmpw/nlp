\section{Related Work}
% \subsection{Uni-modal retrieval}
% Uni-modal retrieval refers to retrieval tasks that focus exclusively on a single modality.

% Classic text-based dense retrieval models, such as DPR~\citep{karpukhin-etal-2020-dense} and GTR~\citep{ni-etal-2022-large}, adopt a dual encoder architecture for open domain question answering retrieval. 
% INSTRUCTOR~\citep{su-etal-2023-one} aims to develop one text embedding model for various task by utilizing different instructions. Recent retrieval methods~\citep{zhang-etal-2024-mgte,warner2024smarter} incorporate latest optimizing approaches for LLMs~\citep{SU2024127063,NEURIPS2023_095a6917} into encoder-based Transformers to develop modern dense retrieval models with longer context window and better performance.

% % text : text
% %  DPR, GTR, Task-aware Retrieval with Instructions, INSTRUCTOR, SimCSE, PCL, LLM2Vec,

% % image : image
% Image retrival is a fundamental task in computer vision field aiming to find relevant images given a reference image~\citep{}. 


% \subsection{Cross-modal retrieval}
% Cross-modal retrieval is a task of retrieving relevant items across different modalities~\citep{10.1007/978-3-031-28241-6_5}.
% text : image
% image : text


% \subsection{Multi-modal retrieval}
% text : text + image

% text + image : text

% text + image : image -> Composed Image Retrieval

% text + image: text + image

% image : text + image


Multi-modal retrieval serves as a cornerstone for multi-modal information systems.
Exisiting studies in this field have been mostly based on two-towers or two-legs architectures~\citep{radford2021learning,9879245,liu2023universal,koukounas2024jina} as we shown in Figure~\ref{fig:vlretrieval}.
A notable example is CLIP~\citep{radford2021learning}, which has been widely adopted as a foundational representation model in multi-modal tasks, including retrieval. CLIP employs separate encoders for text and images and aligns them within a shared embedding space. Building on CLIP, Pic2Word~\citep{Saito2023Pic2Word} leverages pseudo language tokens to train a mapping network for zero-shot composed image retrieval, while SEARLE~\citep{Baldrati2023CIRCO} adopts a similar strategy by pre-training a textual inversion network for zero-shot composed image retrieval. 
Additionally, UniIR~\citep{wei2024uniir} utilizes score-level fusion and feature-level fusion as a comprehensive exploration of two-legs architectures, and improve CLIP/BLIP-based multi-modal information retrieval systems.

Meanwhile, there are latest works applying one-tower visual LLM, given the strong multi-modal understanding ability from large-scale pre-training. ~\citet{jiang2024vlm2vec} and \citet{zhang2024gme} employ visual-language models as its backbone to fuse textual and visual spaces into a unified vision-language embedding model, and they also introduces a comprehensive embedding dataset for evaluation on such a complex multimodal scenairo. Meanwhile, ~\citet{lin2024mm} adopts a similar approach using a visual-language model for retrieval, but it focuses on hard-negative sampling and leverages an LLM for reranking. Both of these concurrent works leverage visual-language models to fuse text and image inputsâ€”thereby adopting a one-tower architecture similar to ours. They implicitly share our advocation that a one-tower architecture is particularly effective for complex vision-language representation and retrieval tasks.

% TODO the difference between ours and theirs?