\section{Experiments}
\label{sec:exp}
\subsection{Datasets}
\paragraph{CC3M~\cite{sharma2018conceptual}.} We performed the post-training adaption on the CC3M dataset \citep{sharma2018conceptual}, which consists of 3.3 million image-text pairs from the web. Using the $\operatorname{img2dataset}$ toolbox~\citep{beaumont-2021-img2dataset}, we download the dataset based on the provided URL-caption pairs, resulting in approximately 2.8 million image-text pairs due to some expired links.

\paragraph{M-BEIR~\cite{wei2024uniir}.} We instruction-tune the models on the M-BEIR dataset, which is a multimodal retrieval dataset encompassing eight tasks and ten datasets across domains like everyday imagery, fashion, Wikipedia, and news. It includes 1.5 million queries and 5.6 million retrieval candidates, despite being originally designed for various purposes. These include retrieval-focused datasets (e.g., OVEN~\cite{hu2023oven}, CIRR~\cite{liu2021cirr}, FashionIQ~\cite{wu2021fashioniq}), image-caption datasets (e.g., MS-COCO~\cite{lin2014microsoft}, Fashion200K~\cite{han2017fashion200k}), an image-similarity dataset (NIGHTS~
\cite{fu2023nights}), and retrieval-based VQA datasets (InfoSeek~\cite{chen2023infoseek}, WebQA~\cite{chang2022webqa}). For each dataset above, \citet{wei2024uniir} generated 4 instructions that describe a multimodal retrieval task by intent, domain, query modality, and target candidate modality. Thanks to its diverse input/output formats, MBEIR provides a suitable platform to train and evaluate multimodal retrieval systems.


\begin{table*}[ht!]
\small
\centering
\caption{Retrieval results on M-BEIR benchmark~\cite{wei2024uniir}.}
\vspace{-0.1in}
\label{tab:multitask_local}
\setlength{\tabcolsep}{1.5mm}{
% \resizebox{\textwidth}{!}{%
\begin{tabular}{llcccccccca}
\toprule
 \multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Dataset}} &  \multicolumn{4}{c}{\textbf{SoTA Zero-Shot}} & \multicolumn{2}{c}{\textbf{Single-task FT}} & \multicolumn{3}{c}{\textbf{Multi-task (w/ instruction)}} \\
\cmidrule(lr){3-6} \cmidrule(lr){7-8} \cmidrule(lr){9-11}
& &CLIP & SigLIP & BLIP & BLIP2 &CLIP$_{\text{SF}}$ & BLIP$_{\text{FF}}$ & CLIP$_{\text{SF}}$ & BLIP$_{\text{FF}}$ & Ours\\
\midrule\midrule
% \midrule
$q_t \to c_t$ & WebQA & 36.2 & 39.8 & 44.9 & 38.6 & 81.7 & 67.5 & 84.1 & 79.2 & 88.7 \\ 
\midrule
$q_i \to c_i$ & NIGHTS & 26.1 & 28.9 & 27.4 & 25.4 & 33.5 & 30.4 & 31.1 & 31.7 & 27.8 \\ 
\midrule
\multicolumn{2}{l}{\textsc{Single-modal Average}} & 31.2	& 34.4 &	36.2 &	32 &	57.6 &	49.0 &	57.6 &	55.5 &	58.3 \\
% \textsc{Single-modal Average} &  \\
\midrule\midrule
\multirow{3}{*}{$q_t \to c_i$} & VisualNews & 43.3 & 30.1 & 16.4 & 16.7 & 43.5 & 20.0 & 42.5 & 22.9 & 34.6 \\
 & MSCOCO & 61.1 & 75.7 & 74.4 & 63.8 & 80.4 & 77.3 & 80.7 & 79.5 & 78.5 \\
 & Fashion200K & 6.6 & 36.5 & 15.9 & 14.0 & 10.7 & 17.1 & 18.1 & 26.2 & 37.2 \\ 
\midrule
\multirow{3}{*}{$q_i \to c_t$} & VisualNews & 41.3 & 30.8 & 17.2 & 15.0 & 42.7 & 22.4 & 42.5 & 23.1 & 33.1 \\
 & MSCOCO & 79.0 & 88.2 & 83.2 & 80.0 & 89.8 & 86.0 & 91.8 & 90.8 & 90.0 \\
 & Fashion200K & 7.7 & 34.2 & 19.9 & 14.2 & 12.0 & 15.6 & 18.3 & 28.6 & 36.9 \\ 
\midrule
\multicolumn{2}{l}{\textsc{Cross-modal Average}} & 39.8 &	49.3 &	37.8 &	34.0 &	46.5 &	39.7 &	49.0 &	45.2	& 51.7 \\
\midrule\midrule
\multirow{2}{*}{$q_t \to$ ($c_i, c_t$)} & EDIS & 43.3 & 27.0 & 26.8 & 26.9 & 58.8 & 38.2 & 53.6 & 49.9 & 54.3 \\
 & WebQA & 45.1 & 43.5 & 20.3 & 24.5 & 76.3 & 67.8 & 78.3 & 78.1 & 82.4 \\ \midrule
\multirow{2}{*}{($q_i, q_t$) $\to c_t$} & OVEN & 24.2 & 29.7 & 16.1 & 12.2 & 45.4 & 33.8 & 46.0 & 42.7 & 46.0 \\
 & InfoSeek & 20.5 & 25.1 & 10.2 & 5.5 & 23.5 & 18.5 & 27.4 & 23.3 & 35.6 \\ \midrule
\multirow{2}{*}{($q_i, q_t$) $\to c_i$} & FashionIQ & 7.0 & 14.4 & 2.3 & 4.4 & 25.9 & 3.0 & 24.8 & 29.2 & 31.8 \\
 & CIRR & 13.2 & 22.7 & 10.6 & 11.8 & 52.0 & 13.9 & 44.6 & 50.7 & 54.0 \\
\midrule
\multicolumn{2}{l}{\textsc{Mixed-modal Average}} & 25.6	& 27.1 & 14.4 & 14.2 & 47.0 & 29.2 &	45.8 & 45.7 & 50.7 \\
\midrule\midrule
\multirow{2}{*}{($q_i, q_t$) $\to$ ($c_i, c_t$)} & OVEN & 38.8 & 41.7 & 27.4 & 27.3 & 66.2 & 49.9 & 68.7 & 56.5 & 72.7 \\
 & InfoSeek & 26.4 & 27.4 & 16.6 & 15.8 & 47.4 & 32.3 & 48.8 & 30.4 & 61.1 \\ 
\midrule
\multicolumn{2}{l}{\textsc{Multi-modal Average}} & 32.6 &	34.6 & 22.0 & 21.55 & 56.8 &	41.1 &	58.8 &	43.5 &	66.9 \\
\midrule\midrule
\multicolumn{2}{l}{\textsc{All Average}} & 32.5 & 37.2 & 26.8 & 24.8 & 49.4 & 37.1 & 50.1 & 46.4 & \textbf{54.0} \\
\bottomrule
\end{tabular}
% }
}
\end{table*}

% \begin{table*}[ht!]
% \small
% \centering
% \caption{Retrieval results on M-BEIR$_{\mathrm{local}}$}
% \label{tab:multitask_local}
% \setlength{\tabcolsep}{1.2mm}{
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{llcccccccca}
% \toprule
%  \multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Dataset}} &  \multicolumn{4}{c}{\textbf{SoTA Zero-Shot}} & \multicolumn{2}{c}{\textbf{Single-task FT}} & \multicolumn{3}{c}{\textbf{Multi-task (w/ instruction)}} \\
% \cmidrule(lr){3-6} \cmidrule(lr){7-8} \cmidrule(lr){9-11}
% & &CLIP & SigLIP & BLIP & BLIP2 &CLIP$_{\text{SF}}$ & BLIP$_{\text{FF}}$ & CLIP$_{\text{SF}}$ & BLIP$_{\text{FF}}$ & Ours\\
% \midrule
% 1. $q_t \to c_i$ & VisualNews & 43.3 & 30.1 & 16.4 & 16.7 & 43.5 & 20.0 & 42.5 & 22.9 & 34.6 \\
%  & MSCOCO & 61.1 & 75.7 & 74.4 & 63.8 & 80.4 & 77.3 & 80.7 & 79.5 & 78.5 \\
%  & Fashion200K & 6.6 & 36.5 & 15.9 & 14.0 & 10.7 & 17.1 & 18.1 & 26.2 & 37.2 \\ \midrule
% 2. $q_t \to c_t$ & WebQA & 36.2 & 39.8 & 44.9 & 38.6 & 81.7 & 67.5 & 84.1 & 79.2 & 88.7 \\ \midrule
% 3. $q_t \to$ ($c_i, c_t$) & EDIS & 43.3 & 27.0 & 26.8 & 26.9 & 58.8 & 38.2 & 53.6 & 49.9 & 54.3 \\
%  & WebQA & 45.1 & 43.5 & 20.3 & 24.5 & 76.3 & 67.8 & 78.3 & 78.1 & 82.4 \\ \midrule
% 4. $q_i \to c_t$ & VisualNews & 41.3 & 30.8 & 17.2 & 15.0 & 42.7 & 22.4 & 42.5 & 23.1 & 33.1 \\
%  & MSCOCO & 79.0 & 88.2 & 83.2 & 80.0 & 89.8 & 86.0 & 91.8 & 90.8 & 90.0 \\
%  & Fashion200K & 7.7 & 34.2 & 19.9 & 14.2 & 12.0 & 15.6 & 18.3 & 28.6 & 36.9 \\ \midrule
% 5. $q_i \to c_i$ & NIGHTS & 26.1 & 28.9 & 27.4 & 25.4 & 33.5 & 30.4 & 31.1 & 31.7 & 27.8 \\ \midrule
% 6. ($q_i, q_t$) $\to c_t$ & OVEN & 24.2 & 29.7 & 16.1 & 12.2 & 45.4 & 33.8 & 46.0 & 42.7 & 46.0 \\
%  & InfoSeek & 20.5 & 25.1 & 10.2 & 5.5 & 23.5 & 18.5 & 27.4 & 23.3 & 35.6 \\ \midrule
% 7. ($q_i, q_t$) $\to c_i$ & FashionIQ & 7.0 & 14.4 & 2.3 & 4.4 & 25.9 & 3.0 & 24.8 & 29.2 & 31.8 \\
%  & CIRR & 13.2 & 22.7 & 10.6 & 11.8 & 52.0 & 13.9 & 44.6 & 50.7 & 54.0 \\ \midrule
% 8. ($q_i, q_t$) $\to$ ($c_i, c_t$) & OVEN & 38.8 & 41.7 & 27.4 & 27.3 & 66.2 & 49.9 & 68.7 & 56.5 & 72.7 \\
%  & InfoSeek & 26.4 & 27.4 & 16.6 & 15.8 & 47.4 & 32.3 & 48.8 & 30.4 & 61.1 \\ \midrule
% - & Average & 32.5 & 37.2 & 26.8 & 24.8 & 49.4 & 37.1 & 50.1 & 46.4 & \textbf{54.0} \\
% \bottomrule
% \end{tabular}
% % }
% }
% \end{table*}



\subsection{Training setups}
For most of the experiments in this paper, we default to PaliGemma~\cite{beyer2024paligemma} as the choice of MLLMs because of its relatively compact size ($\sim$3B) and competitive performance on various vision-language understanding benchmarks. Following the original recipe, the image input of the model is simply scaled to size $256\times 256$ and fed into a SigLIP vision encoder (a ViT with patch size of $16\times 16$) to obtain 256 vision tokens; the text input is tokenized by the sentence piece tokenizer. For training efficiency and reducing GPU memory consumption, we truncate the input token when the total number of tokens is larger than 384, which means the maximal length of textual tokens (including those of textual instruction) is 128 when the input contains an image or 378 otherwise.

For the post-training adaption, we train the MLLMs on CC3M~\cite{sharma2018conceptual} datasets for 1 epoch using Low-Rank Adapters (LoRA)~\cite{hu2022lora} with $r = 128, \alpha = 256$, and a dropout probability 0.05. We use the AdamW~\cite{loshchilov2017decoupled} optimizer with a learning rate of 2e-4, a batch size of 2048, and no weight decay for the LoRA training. The learning rate is linearly warmed-up for the first 3\% of training to the specified value and then decayed using a cosine annealing schedule~\cite{loshchilov2016sgdr}.

For the instruction-tuning stage, we first merge the LoRA of the first stage to the base MLLM and then reinitialize and train a new of new LoRA based on the merged weights. We use  $r = 256, \alpha = 512$, and a dropout probability 0.3 for LoRA at this stage because we find it beneficial to use more parameters to enhance the instruction-following capability (see Tab.~\ref{tab:lora_params}). We train the LoRA with a batch size of 1024, no weight decay, and a learning rate of 2e-4 which is warmed-up and decayed as in the first stage.


\begin{table*}[ht!]
\small
\centering
\caption{Conditional Retrieval on GeneCIS benchmark~\cite{Vaze2023GeneCIS}.}
\label{tab:cond_retr}
\vspace{-0.1in}
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{lccccccccccccccc}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{3}{c}{\textbf{Focus Attribute}} & \multicolumn{3}{c}{\textbf{Change Attribute}} & \multicolumn{3}{c}{\textbf{Focus Object}} & \multicolumn{3}{c}{\textbf{Change Object}} & \textbf{Avg} \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-14}
\multicolumn{1}{c}{} & R@1 & R@2 & R@3 & R@1 & R@2 & R@3 & R@1 & R@2 & R@3 & R@1 & R@2 & R@3 & R@1 \\ 
\midrule
Pic2Word \cite{Saito2023Pic2Word} & 12.5 & 23.4 & 33.7 & 11.7 & 21.9 & 30.9 & 9.9 & 19.3 & 27.4 & 8.6 & 18.2 & 26.1 & 10.7 \\
SEARLE \cite{Baldrati2023CIRCO} & 16.3 & 29.4 & 40.7 & 16.2 & 27.3 & 35.5 & 10.8 & 18.2 & 27.9 & 8.3 & 15.6 & 25.8 & 12.9 \\
CompoDiff \cite{Gu2023CompoDiff} & 14.3 & 26.7 & 38.4 & 19.7 & 28.8 & 37.4 & 9.2 & 19.1 & 25.8 & 18.7 & 31.7 & 40.2 & 15.5 \\
CIReVL \cite{Karthik2023CIReVL} & 20.5 & 34.0 & 44.5 & 16.1 & 28.6 & 39.4 & 14.7 & 25.2 & 33.0 & 18.1 & 31.2 & 41.0 & 17.4 \\
LinCIR \cite{Gu2023LinCIR} & 19.1 & 33.0 & 42.3 & 17.6 & 30.2 & 38.1 & 10.1 & 19.1 & 28.1 & 7.9 & 16.3 & 25.7 & 13.7 \\ 
MagicLens~\cite{zhang2024magiclens} & 16.6 & 28.7 & 39.3 & 16.0 & 27.5 & 36.5 & 15.7 & 27.6 & 37.3 & 18.7 & 31.7 & 40.2 & 16.7 \\ 
\midrule
CLIP$_{\text{SF}}$ \cite{wei2024uniir} & 21.1 & 33.9 & 44.6 & 15.1 & 27.6 & 37.8 & 15.0 & 25.3 & 35.0 & 13.6 & 24.8 & 35.7 & 16.2 \\
BLIP$_{\text{FF}}$ \cite{wei2024uniir} & 19.4&32.3&44.0&15.8&26.9&36.0&18.0&28.4&37.0&18.5&29.4&39.1 & 17.9\\
\midrule
\rowcolor{Gray} Ours & 18.9 &	29.6 &	40.7 &	15.7 &	28.0 &	36.7&	21.5 &	32.7 &	40.5 &	24.1 &	37.9 & 48.4 & \textbf{20.1} \\
\bottomrule
\end{tabular}
}
}
\end{table*}




\subsection{Evaluation setups}
We mainly evaluate \modelname{} on the M-BEIR benchmark~\cite{wei2024uniir} because it contains a diverse set of input and target modalities and provides large-scale query and candidate sets (190K queries and 5.6M candidates) for reliable evaluations. We adopt the settings that perform retrieval from a task-specific pool provided by the original dataset, enabling comparison with non-instruction-tuned retrievers. We report the Recall@5 for all the datasets except FashionIQ and Fashion 200K, where Recall@10 is used following \citet{wu2021fashioniq}.

In addition, we also evaluate \modelname{} on conditional image similarity, which measures the capability of models not only in encoding the content of the query but also in understanding users' intent or conditions. We use the GeneCIS benchmark~\cite{Vaze2023GeneCIS}, an image-to-image retrieval task conditioned on several keywords. GeneCIS consists of four sub-tasks about focusing or changing on a specific attribute or object. For instance, for the sub-task about focusing on an object, the models need to find the most relevant image with the same object (specified in the condition) as the query.




\subsection{Single-modal and cross-modal retrieval}
We begin by evaluating our method in single-modal and cross-modal settings, where both the query and candidate consist of a single modality. Although the baseline models are specifically designed to handle single-modal inputs, our multi-modal input method slightly outperforms them in single-modal retrieval, achieving an average score of 58.3 compared to 57.6 for CLIP$_{\text{SF}}$ and 49.0 for BLIP$_{\text{FF}}$. Similarly, in cross-modal retrieval, our method attains an average score of 51.7, surpassing single-task fine-tuned models (46.5 for CLIP$_{\text{SF}}$ and 39.7 for BLIP$_{\text{FF}}$) and multi-task models (49.0). These results indicate that even in tasks where two-tower-based methods typically excel, our unified multi-modal approach delivers competitive and even slightly superior performance.


\subsection{Mixed-modal and multi-modal retrieval}
We further evaluate our model in mixed- and multi-modal settings, where both queries and candidates can be arbitrary combinations of images and text. For mixed-modal retrieval, \modelname{} achieves an average score of 50.7, significantly outperforming multi-task baselines (45.8 and 45.7) and single-task BLIP$_{\text{FF}}$ (29.2). In multi-modal tasks, our model achieves an average score of 66.9, surpassing both single-task FT (56.8) and multi-task models (58.8) by 8 points. These results demonstrate that, unlike two-tower-based baselines which struggle to interpret multi-modal inputs, our approach effectively integrates cross-modal and multi-task signals, obtaining superior performance in complex multi-modal retrieval scenarios. These experiments reiterate the importance of unified models, like \modelname{}, for vision-language retrieval.


\begin{table}[t]
    \centering
    \small
    \caption{Influence of two-stage training.}
    \label{tab:2stage_training}
    \vspace{-0.1in}
    \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{cccccc}
    \toprule
    \multirow{2}{*}{\textbf{Stage 1}} & \multirow{2}{*}{\textbf{Stage 2}} & \multicolumn{4}{c}{\textbf{Retrieval recall}} \\
    \cmidrule(lr){3-6}
    % & & $q_i \to c_t$ & $q_t \to c_t$ & $(q_i, q_t) \to c_i$ \\
    & & Single & Cross & Mixed & Average \\
    \midrule
    \xmark & \xmark & 6.7 & 0.1 & 0.0 & 1.4 \\
    \cmark & \xmark & 59.3 & 54.3 & 6.7 & 36.3 \\
    \xmark & \cmark & 84.8 & 80.5 & 44.3 & 66.9 \\
    \rowcolor{color_ours} \cmark & \cmark & 88.2 & 84.3 & 42.3 & 68.3 \\
    \bottomrule
    \end{tabular}
    }
\end{table}



\subsection{Conditional retrieval}
Following \citet{Vaze2023GeneCIS}, we report the Recall@K, K = $\{1, 2, 3\}$ for all four sub-tasks, as well as the averaged Recall@1 in Tab.~\ref{tab:cond_retr}. We conduct in-depth comparisons with various state-of-the-art methods, all following the two-tower fashion and potentially with a combiner. This includes 1) Pic2Word~\cite{Saito2023Pic2Word}, SEARLE~\cite{Baldrati2023CIRCO} and LinCIR~\cite{Gu2023LinCIR} that map images into a special text token inserted to the condition prompts; 2) CIReVL~\cite{Karthik2023CIReVL} that first captions the image and then merges the caption and the condition using LLMs to a textual query; 3) CompoDiff~\cite{Gu2023CompoDiff} and MagicLens~\cite{zhang2024magiclens} which curate or synthesize composed image retrieval data for training a two-tower model; and 4) UniIR variants (CLIP$_{\textbf{SF}}$ and BLIP$_{\text{FF}}$) that are trained on the same data as ours.
From the table, we observe that \modelname{} delivers competitive performance across all tasks compared to SOTA methods, with exceptional results in object-centric conditions. \modelname{} achieves an averaged Recall@1 of 20.1, significantly outperforming all other methods by a large margin, despite not relying on any task-specific design for conditional retrieval. This underscores the effectiveness of using a unified model to jointly comprehend vision and language information. Additionally, although BLIP$_{\text{FF}}$ shows relatively competitive performance with an average Recall@1 of 17.9, it still falls significantly short of the robustness demonstrated by our approach, especially in subtasks involving object modification or composition. This reinforces that the improvements achieved by \modelname{} are primarily due to its architectural design rather than any advantage from the data.


\begin{table}[t]
    \centering
    \small
    \caption{Influence of data sampling strategy.}
    \label{tab:data_sampling}
    \vspace{-0.1in}
    \begin{tabular}{ccccc}
    \toprule
     \multirow{2}{*}{\textbf{\#Dataset/Batch}} & \multicolumn{4}{c}{\textbf{Retrieval recall}} \\
    \cmidrule(lr){2-5}
     % & $q_i \to c_t$ & $q_t \to c_t$ & $(q_i, q_t) \to c_i$ \\
    & Single & Cross & Mixed & Average \\
    \midrule
    N/A & 81.1 & 78.4 & 37.9 & 62.8 \\
    2 & 75.4 & 81.5 & 39.5 & 63.4 \\
    4 & 84.5 & 82.0 & 41.3 & 66.2 \\
    8 & 81.8 & 77.9 & 40.9 & 63.9 \\
    \rowcolor{color_ours} $\mathcal{N}(4, 1)$ & 84.8 & 80.5 & 44.3 & 66.9 \\
    \bottomrule
    \end{tabular}
\end{table}





\begin{table}[t]
    \centering
    \small
    \caption{Influence of the hyper-parameters rank ($r$), $\alpha$, and dropout probability $d$ in LoRA.}
    \label{tab:lora_params}
    \vspace{-0.1in}
    \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{ccccccc}
    \toprule
     \multicolumn{3}{c}{\textbf{LoRA hyper-param.}} & \multicolumn{4}{c}{\textbf{Retrieval recall}} \\
     \cmidrule(lr){1-3}
     \cmidrule(lr){4-7}
     $r$ & $\alpha$ & $d$ 
     % & $q_i \to c_t$ & $q_t \to c_t$ & $(q_i, q_t) \to c_i$ \\
     & Single & Cross & Mixed & Average \\
    \midrule
    16 & 32 & 0.05 & 78.3 & 77.7 & 35.9 & 61.1 \\
    128 & 256 & 0.05 & 81.5 & 80.2 & 38.3 & 63.7 \\
    128 & 256 &  0.3 & 81.9 & 83.8 & 37.3 & 64.8 \\
    \rowcolor{color_ours} 256 & 512 & 0.3 & 84.8 & 80.5 & 44.3 & 66.9 \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Ablation analysis}


\paragraph{Impact of Two-stage Training.} In Tab.~\ref{tab:2stage_training}, we study the impact of the two-stage training on retrieval performance using a subset of M-BEIR.
Without any training, the performance of the original MLLMs is no better than random guessing, indicating the necessity of carefully designed adaption steps. Applying stage 1 alone yields a significant performance improvement, but falls behind stage 2 training alone, which is reasonable considering the discrepancy between the CC3M and M-BEIR. The combination of both stages results in the best retrieval recall especially in the case of single-modal retrieval, suggesting the benefits of the post-training adaption for tasks involving only single-modality query/candidate.



\paragraph{Benefits of data sampling.} 
Tab.~\ref{tab:data_sampling} investigates how different data sampling strategies influence retrieval performance. Training without any sampling strategy underperforms (62.8 average recall) those with sampling operations, emphasizing the necessity of batch diversity. As the dataset count per batch increases, performance improves up to a peak at 4 datasets per batch. The use of Gaussian sampling $\mathcal{N}(4, 1)$ further improves this result, achieving an averaged score of 66.9. This suggests that properly balancing the data source within batches benefits generalization.


\paragraph{Impact of the LoRA hyper-parameters and batch size.} 
Tabs.~\ref{tab:lora_params} and \ref{tab:training_bs} explore the importance of LoRA hyper-parameters and batch size on retrieval performance. For LoRA, larger rank and scaling factors ($r = 256$, $\alpha = 512$) consistently yield performance gains, achieving the best retrieval recall of 66.9. Additionally, moderate dropout regularization (0.3) outperforms smaller rates (0.05), suggesting that balancing parameter complexity and overfitting is crucial for robust performance. Meanwhile, batch size plays a critical role, with a larger batch size of 1024 reaching 66.9 averaged score, showing that batch size is particularly impactful as larger batches typically lead to better negative sampling and stronger representation learning. However, due to GPU memory limitations, batch sizes beyond 1024 could not be tested. We would expect performance to further improve with larger batch sizes.


\paragraph{Scaling the number of training epochs.} 
Tab.~\ref{tab:training_ep} analyzes how the number of training epochs affects retrieval performance. From the table, we can see that the performance considerably increases from 67.6 at 2 epochs to 68.6 at 3 epochs. Beyond this, the performance plateaus, as both 3 and 5 epochs yield the same overall score (68.6 recall). These results suggest that training saturation occurs beyond 3 epochs, where additional epochs provide diminishing returns on retrieval effectiveness. We default to 3 epochs in our experiments for a good trade-off between performance and training efficiency.


\begin{table}[t]
    \centering
    \small
    \caption{Influence of the number of training batch size.}
    \label{tab:training_bs}
    \vspace{-0.1in}
    \begin{tabular}{ccccc}
    \toprule
     \multirow{2}{*}{\textbf{Batch Size}} & \multicolumn{4}{c}{\textbf{Retrieval recall}} \\
     \cmidrule(lr){2-5}
     % & & $q_i \to c_t$ & $q_t \to c_t$ & $(q_i, q_t) \to c_i$ \\
     & Single & Cross & Mixed & Average \\
    \midrule
    256 & 80.2 & 77.8 & 41.4 & 63.7 \\
    512 & 77.5 & 82.9 & 41.1 & 65.1 \\
    \rowcolor{color_ours} 1024 & 84.8 & 80.5 & 44.3 & 66.9 \\
    \bottomrule
    \end{tabular}
\end{table}


\begin{table}[t]
    \centering
    \small
    \caption{Influence of the number of training epochs.}
    \label{tab:training_ep}
    \vspace{-0.1in}
    \begin{tabular}{ccccc}
    \toprule
    \multirow{2}{*}{\textbf{Num. Epochs}} & \multicolumn{4}{c}{\textbf{Retrieval recall}} \\
     \cmidrule(lr){2-5}
     % & & $q_i \to c_t$ & $q_t \to c_t$ & $(q_i, q_t) \to c_i$ \\
     & Single & Cross & Mixed & Average \\
    \midrule
    % 1024 & 1 & 80.9 & 78.2 & 37.5 & 62.5 \\
    2 & 87.1 & 80.8 & 44.6 & 67.6 \\
    \rowcolor{color_ours} 3 & 88.7 & 84.3 & 42.9 & 68.6 \\
    5 & 86.9 & 84.2 & 44.0 & 68.6 \\
    \bottomrule
    \end{tabular}
\end{table}
