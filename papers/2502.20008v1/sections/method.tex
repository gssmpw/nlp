\newcommand{\tss}[2]{{#1}^{\text{#2}}} % textual superscript
\newcommand{\tsbs}[2]{{#1}_{\text{#2}}} % textual subscript
\newcommand{\tssbs}[3]{{#1}_{\text{#2}}^{\text{#3}}} % textual super and subscript

\section{Methodology}
\subsection{Multi-modal retrieval}
\label{sec:pre}
This section describes multi-modal retrieval, a task designed to retrieve relevant information from multi-modal databases by matching queries and candidates across textual, visual, or combined modalities. Formally, the query set $\mathcal{Q}$ and candidate set $\mathcal{C}$ can be defined as follows,
\begin{equation}    
\begin{aligned}
    \mathcal{Q} &= \{q \mid q \in \{\tss{q}{i}, \tss{q}{t}, \{\tss{q}{i}, \tss{q}{t}\}\}\} \\
    \mathcal{C} &= \{c \mid q \in \{\tss{c}{i}, \tss{c}{t}, \{\tss{c}{i}, \tss{c}{t}\}\}\}. \\
\end{aligned}
\label{eq:qc}
\end{equation}
Depending on application scenarios as introduced in \S\ref{sec:intro}, the query can be a text query $\tss{q}{t}$, an image query $\tss{q}{i}$ or a pair of a text and an image, \ie, $\{\tss{q}{i}, \tss{q}{t}\}$. Similarly, $\tss{c}{t}$, $\tss{c}{i}$, $\{\tss{c}{i}, \tss{c}{t}\}$ represent text candidate, image candidate, and Multi-modal candidate, respectively.

\paragraph{Conventional two-tower models.}
A conventional two-tower model, \eg, CLIP~\cite{radford2021learning}, encodes the inputs into meaningful embeddings using encoders for texts and images separately, as follows,
\begin{equation}
\begin{aligned}
    \tss{\mathbf{h}}{q-i} &= f_{\tss{\theta}{i}}(\tss{q}{i}) \in \mathbb{R}^D \\
    \tss{\mathbf{h}}{q-t} &= f_{\tss{\theta}{t}}(\tss{q}{t}) \in \mathbb{R}^D
\end{aligned}
\label{eq:bienc}
\end{equation}

In the case where the inputs contain both texts and images, a combiner module, notated as $g_{\tss{\theta}{c}}$, is typically needed to combine the features for tasks, such as composed image retrieval. The process can be defined as follows,
\begin{equation}
\tss{\mathbf{h}}{q} = 
\begin{cases}
    g_{\tss{\theta}{c}}(\tss{\mathbf{h}}{q-i}, \tss{\mathbf{h}}{q-t}), & \text{if } q = \{\tss{q}{i}, \tss{q}{t}\} \\
    \tss{\mathbf{h}}{q-i}, & \text{if } q = \tss{q}{i} \\
    \tss{\mathbf{h}}{q-t}, & \text{if } q = \tss{q}{t}
\end{cases}
\label{eq:comb}
\end{equation}
$\tss{\mathbf{h}}{q} \in \mathbb{R}^D $ is the embedding representing the query and the embedding of a candidate $\tss{\mathbf{h}}{c} \in \mathbb{R}^D $ can be obtained in the same way with identical $f$ and $g$. 

\paragraph{Optimization objective.}
Given a batch of pairs, $B = \{\{q_i, c_i\}\}^{|B|}_{i=1}$, where $q_i \in \mathcal{Q}$ and $c_i \in \mathcal{C}$ are termed query and the targeted candidate. As introduced in Equations~\eqref{eq:qc}, \eqref{eq:bienc} and \eqref{eq:comb}, we obtain embeddings as $\{\{\tss{\mathbf{h}}{q}_i, \tss{\mathbf{h}}{c}_i\}\}^{|B|}_{i=1}$. The set of parameters to be optimized is $\Theta = \{\tss{\theta}{t}, \tss{\theta}{i}, \tss{\theta}{c}\}$. Contrastive learning objective~\cite{oord2018representation} is used to optimize parameters $\Theta$ by minimizing the following InfoNCE loss,
\begin{equation}
    \mathcal{L} = -\frac{1}{|B|}\sum_{1 \leq i \leq |B|}\log{
    \frac{
        \exp(\tss{\mathbf{h}}{q}_i \cdot \tss{\mathbf{h}}{c}_i/\tau)
    }{
        \sum_{1 \leq j \leq |B|}\exp(\tss{\mathbf{h}}{q}_i \cdot \tss{\mathbf{h}}{c}_j/\tau)},
    }
\label{eq:loss}
\end{equation}
where $\tau$ is a temperature term controlling the discrimination to the negative samples.

\subsection{\modelfullname{}}
While conventional two-tower models have significantly transformed the landscape of image-to-text and text-to-image retrieval and perform well on simple text-image matching tasks, these models can still be limited because
\begin{itemize}[leftmargin=0.1in,itemsep=0in]
    \item The inability to jointly model visual and language data because of their separate encoding processes.
    \item The requirement for task-specific combiners to handle more complex tasks, such as composed image retrieval, due to the single-modal input and embeddings.
\end{itemize}


% \paragraph{Unified Encoding} We propose \modelname{},  which builds on the MLLM~\cite{beyer2024paligemma} as its backbone encoder, unifying the encoding process as follows:
% \begin{equation}
%     \begin{aligned}
%         \tss{\mathbf{h}}{q} &= f_{\theta}(q) \in \mathbb{R}^D \\
%         \tss{\mathbf{h}}{c} &= f_{\theta}(c) \in \mathbb{R}^D.
%     \end{aligned}
% \end{equation}
% Here, $q \in \mathcal{Q}$ and $c \in \mathcal{C}$ represent the query and candidate, as defined in Equation~\eqref{eq:qc}. The function $f_{\theta}$ denotes the backbone encoder, which in our model is a MLLM. The MLLM was trained with both visual and textual inputs on generative tasks, such as visual question answering. It processes textual and visual inputs by converting them into sequential tokens, which are then modeled by a shared Transformer~\cite{vaswani2017attention, dosovitskiy2020image}.
% % After obtaining the embeddings, $\theta$ is the parameter that can be optimized with contrastive loss in Equation~\eqref{eq:loss}.

% Unlike the conventional two-tower model described in \S\ref{sec:pre}, utilizing an MLLM as the backbone encoder unifies the encoding process without the need for an additional combiner, even when handling queries involving multiple modalities, such as composed image retrieval. However, despite the MLLM’s ability to unify the encoding process and understand multi-modal contexts, effectively extracting meaningful embeddings for retrieval remains a non-trivial challenge. To address this, \modelname{} introduces a two-stage training approach to optimize MLLMs for vision-language retrieval: post-training adaptation and instruction tuning.



\paragraph{Unified encoding and embedding process.}
We propose \modelfullname{} (\modelname{}), which builds on an MLLM, \eg, ~\cite{beyer2024paligemma}, as its backbone encoder, unifying the encoding process for both queries and candidates. Given an input, either a query $q \in \mathcal{Q}$ or a candidate $c \in \mathcal{C}$, that may contain multi-modal information, we augment the token sequence by appending a special token \texttt{[Emb]} to its end. For example, for a query we form the augmented input $x_q = [q; \texttt{[Emb]}]$
which is processed by the MLLM’s shared Transformer encoder:
\begin{align}
\label{eq:emb_token_encoding}
\mathbf{e}_q = f_{\theta}(x_q) = \{\mathbf{e}_{q,1}, \mathbf{e}_{q,2}, \dots, \mathbf{e}_{q,N_q}\},
\end{align}
where $N_q$ is the total number of tokens in the augmented sequence. We then extract the hidden state corresponding to the \texttt{[Emb]} token as the final query embedding:
\begin{align}
\label{eq:emb_token_indexing}
\mathbf{h}_q \triangleq \mathbf{e}_{q,N_q} \in \mathbb{R}^D.
\end{align}
Similarly, for a candidate, we obtain its embedding by
$\mathbf{h}_c \triangleq \mathbf{e}_{c,N_c} \in \mathbb{R}^D$.
These embeddings $\mathbf{e}_q$ and $\mathbf{e}_c$ serve as the representations for the contrastive loss in Equation~\eqref{eq:loss}, which encourages corresponding query--candidate pairs to have similar embeddings while pushing apart those of unrelated pairs.

Although MLLMs are originally trained on generative tasks (e.g., visual question answering) that focus on next-token prediction, they are not naturally designed to extract discriminative representations for retrieval. To address this limitation, we first perform post-training adaptation on the backbone MLLM using large-scale paired data, followed by instruction tuning. Unlike the conventional two-tower model described in \S\ref{sec:pre}, this unified encoding process eliminates the need for an additional combiner—even when handling multi-modal queries such as in composed image retrieval.




\paragraph{Post-training Adaptation.}
% This step adapts the backbone MLLM to process inputs into meaningful embeddings. Most MLLMs achieve Multi-modal understanding capabilities through decoder-only language models, which are primarily trained to predict the next token rather than for encoding. To address this limitation, we conduct post-training on the backbone MLLM using large-scale image-caption pairs.
% % For consistency across multiple training steps with instructions, we prepend a simple textual instruction, \ie, \textit{``Find an everyday image match with caption"}, to every caption. 
% The optimization objective for this step is described in Equation~\eqref{eq:loss}.
In this stage, we fine-tune the backbone MLLM using the image-caption datasets to generate retrieval-specific embeddings. For each image-caption pair, we create two training instances by swapping roles: one instance treats the image as the query and the caption as the candidate, while the other reverses these roles. Each instance is processed by appending a special token (e.g., \texttt{[Emb]}) to the input sequence to designate the location for extracting the final embedding. The MLLM then encodes these augmented inputs, and a contrastive learning objective aligns embeddings from matching image–caption pairs while distinguishing non-matching pairs. This process ensures that both images and captions are effectively represented for retrieval tasks.


\paragraph{Instruction Tuning.}
% Recent studies in text retrieval have explored integrating instructions into retrievers to better align with users' intentions~\cite{asai-etal-2023-task, su-etal-2023-one}. This challenge is more pronounced in Multi-modal retrieval, where instructive information can be presented in both textual and visual modalities for different tasks. This complexity necessitates the simultaneous processing of textual and visual instructive inputs during encoding.
% We adopt a strategy similar to UniIR~\cite{wei2024uniir}, complementing the instructive information with task-specific textual instructions. Leveraging the MLLM's capability to handle multi-modal inputs, we process textual and visual inputs as a single sequence, encoding them simultaneously. This approach allows for a more integrated understanding of multi-modal instructions. The training objective remains consistent with Equation~\eqref{eq:loss}, ensuring a unified learning process across different modalities.
Recent studies in text retrieval have explored integrating instructions into retrievers to better align with users' intentions~\cite{asai-etal-2023-task, su-etal-2023-one}. This challenge is more pronounced in Multi-modal retrieval, where instructive information can be presented in both textual and visual modalities for different tasks. This complexity necessitates the simultaneous processing of textual and visual instructive inputs during encoding. To further refine the MLLM for vision-language retrieval and better align it with human intent, we incorporate explicit task-specific instructions into the input sequences. In this stage, we leverage the instruction data from UniIR~\cite{wei2024uniir} to tune the unified MLLM for retrieval tasks. Specifically, for an input query $q$ (which may represent either visual or textual content) and its corresponding instruction $i$, we construct an augmented sequence by appending the instruction to the context along with a special token for embedding extraction:
\begin{align}
\label{eq:inst_format}
q' = [q; i; \texttt{[Emb]}].
\end{align}
Here, \texttt{[Emb]} marks the position from which the final embedding is extracted. The MLLM processes this combined sequence as a single input as in Equations~\eqref{eq:emb_token_encoding} and \eqref{eq:emb_token_indexing}, thereby jointly encoding the primary content and the instructive cues. The training objective (as defined in Equation~\eqref{eq:loss}) then aligns the embeddings of matching pairs while distinguishing those of non-matching pairs. This integrated learning approach enables the model to effectively interpret and execute retrieval tasks in accordance with human instructions.


\paragraph{Data sampling strategy.}
Our instruction data originates from multiple datasets and tasks, each exhibiting unbalanced data volumes. Since contrastive learning is sensitive to both intra-dataset and inter-dataset batch composition—relying on the effective mining of negative examples—we design a sampling scheme that carefully controls the number of datasets included in each batch. Empirically, we observe that limiting the number of datasets per batch improves overall performance. To achieve this, we sample the number of datasets per batch from a normal distribution (with rounding),
$N_d \sim \mathcal{N}(4, 1)$,
thereby ensuring that each batch contains a small, balanced subset of datasets. This strategy helps mitigate data imbalance while maintaining a rich set of negative samples, ultimately enhancing the robustness of the contrastive learning process.

\paragraph{Summary.}
Through the post-training adaptation and instruction tuning steps, we transform the MLLM into a powerful encoder for retrieval tasks. This adapted model excels in encoding multi-modal inputs into meaningful embeddings. Our approach leverages the MLLM's inherent ability to understand multi-modal instructive information, resulting in a unified encoding process that seamlessly handles various input types - be it text-only, image-only, or a combination of both.