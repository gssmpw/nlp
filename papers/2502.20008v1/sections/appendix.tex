% \clearpage

% \begin{table*}[t]
% \small
% \centering
% \caption{Retrieval results on M-BEIR$_{\mathrm{global}}$}
% \label{tab:multitask}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{llccccca}
% \toprule
%  &  & \textbf{Zero-shot} & \multicolumn{2}{c}{\textbf{Multi-task (w/ instruction)}} & \multicolumn{3}{c}{\textbf{Multi-task (w/o instruction)}}  \\ \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8}
% \textbf{Task} & \textbf{Dataset} & BLIP2 & CLIP$_{\text{SF}}$ & BLIP$_{\text{FF}}$ & CLIP$_{\text{SF}}$ & BLIP$_{\text{FF}}$ & Ours \\ 
% \midrule
% \multirow{3}{*}{1. $q_t \to c_i$} & VisualNews & 0.0 & 12.7 & 8.3 & 42.2 & 22.5 & 31.5 \\
%  & MSCOCO & 0.0 & 27.3 & 27.7 & 71.4 & 65.3 & 62.1 \\
%  & Fashion200K & 0.0 & 5.9 & 9.0 & 18.0 & 26.1 & 35.6 \\ \midrule
% 2. $q_t \to c_t$ & WebQA & 35.2 & 82.3 & 76.1 & 83.5 & 78.5 & 87.6 \\ \midrule
% \multirow{2}{*}{3. $q_t \to$ ($c_i, c_t$)} & EDIS & 0.0 & 41.1 & 36.0 & 52.7 & 49.3 & 51.7 \\
%  & WebQA & 0.0 & 68.2 & 74.7 & 77.5 & 77.1 & 81.0 \\ \midrule
% \multirow{3}{*}{4. $q_i \to c_t$} & VisualNews & 0.0 & 12.1 & 4.9 & 38.8 & 21.1 & 30.3 \\
%  & MSCOCO & 0.0 & 84.6 & 76.9 & 91.4 & 89.8 & 89.0 \\
%  & Fashion200K & 0.0 & 1.2 & 3.6 & 18.2 & 27.4 & 30.9 \\ \midrule
% 5. $q_i \to c_i$ & NIGHTS & 24.0 & 31.0 & 31.3 & 39.5 & 31.6 & 27.8 \\ \midrule
% \multirow{2}{*}{6. ($q_i, q_t$) $\to c_t$} & OVEN & 0.0 & 36.8 & 37.7 & 22.2 & 39.5 & 42.4 \\
%  & InfoSeek & 0.0 & 18.3 & 17.8 & 24.6 & 19.8 & 31.9 \\ \midrule
% \multirow{2}{*}{7. ($q_i, q_t$) $\to c_i$} & FashionIQ & 3.9 & 22.8 & 28.1 & 43.1 & 28.9 & 31.1 \\
%  & CIRR & 6.2 & 32.0 & 45.1 & 59.8 & 48.3 & 50.4 \\ \midrule
% \multirow{2}{*}{8. ($q_i, q_t$) $\to$ ($c_i, c_t$)} & OVEN & 13.8 & 58.7 & 51.6 & 44.3 & 55.9 & 69.1 \\
%  & InfoSeek & 11.4 & 42.3 & 25.4 & 44.3 & 26.2 & 57.4 \\ \midrule
%  & Average & 5.9 & 36.1 & 34.6 & 47.4 & 44.2 & 50.6 \\
% \bottomrule
% \end{tabular}
% }
% \end{table*}

\appendix
\section*{Appendix}
\label{sec:appendix}


\section{Additional Experimental Results}
We also adopt an alternative retrieval setting as described in \cite{wei2024uniir}, which conducts retrieval from a pool of 5.6 million candidates aggregated from eight tasks across ten M-BEIR datasets. As demonstrated in Table~\ref{tab:multitask}, despite the significantly varied evaluation settings, \modelname{} not only achieves moderate improvements over baselines in single-modal and cross-modal retrieval but also delivers substantial gains in mixed-modal and multi-modal scenariosâ€”settings that are increasingly relevant in our multi-modal content-rich world. These results further corroborate our findings in \S\ref{sec:exp} and underscore the advantages of the Joint Fusion and Encoding paradigm.