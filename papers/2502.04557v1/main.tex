\documentclass[11pt,letterpaper]{article}
%\documentclass[journal]{IEEEtran}
\usepackage{epsfig,rotating,setspace,latexsym,amsbsy,amsmath,epsf,amssymb,bm}
\usepackage{hyperref}
\usepackage{cite,graphicx,authblk,color,subfigure}
\usepackage[title]{appendix}
\usepackage{multirow}
\usepackage{ctable}
\usepackage{nicefrac}
\usepackage{wrapfig}

\usepackage{algorithm}
\usepackage{algorithmic}

\newlength{\Oldarrayrulewidth}
\newcommand{\Cline}[2]{%
  \noalign{\global\setlength{\Oldarrayrulewidth}{\arrayrulewidth}}%
  \noalign{\global\setlength{\arrayrulewidth}{#1}}\cline{#2}%
  \noalign{\global\setlength{\arrayrulewidth}{\Oldarrayrulewidth}}}
  
\usepackage{amsmath}
\usepackage{bm}
\usepackage{enumerate}

\newcommand{\A}{{\mathbf{A}}}
\newcommand{\Q}{{\mathbf{Q}}}
\newcommand{\W}{{\mathbf{W}}}
\newcommand{\Z}{{\mathbf{Z}}}
\newcommand{\bsigma}{{\boldsymbol{\sigma}}}
\newcommand{\bpi}{{\boldsymbol{\pi}}}
\newcommand{\bdelta}{{\boldsymbol{\delta}}}
\newcommand{\bell}{{\boldsymbol{\ell}}}

\newcommand{\ravi}[1]{{\color{red}\marginpar{+}{\bf Ravi's remark}: {\em #1}}}


\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{observation}[theorem]{Observation}
\newenvironment{proof}[1]{\medskip\par\noindent
{\bf Proof:\,}\,#1}{{\mbox{\,$\blacksquare$}\par}}
\pagestyle{plain}
\allowdisplaybreaks



\setstretch{1.2}
\textwidth 6.5 in
\oddsidemargin 0.0 in
\evensidemargin  0.0 in
\textheight 9.2 in
\topmargin -0.8 in







\thispagestyle{empty}

\title{Speeding up Speculative Decoding via Approximate Verification}
%\author{ }
\author{Meiyu Zhong$^*$ \quad Noel Teku$^*$  \quad Ravi Tandon\\
Department of Electrical and Computer Engineering\\
University of Arizona, Tucson, AZ, USA. \\
Email: \{\textit{meiyuzhong, nteku1, tandonr}\}@arizona.edu}


\begin{document}

\maketitle
% \newcommand\blfootnote[1]{%
%   \begingroup
%   \renewcommand\thefootnote{}\footnote{#1}%
%   \addtocounter{footnote}{-1}%
%   \endgroup
% }

\def\thefootnote{*}\footnotetext{Equal Contribution.}\def\thefootnote{\arabic{footnote}}

%\affiliation{
	%	Department of Electrical and Computer Engineering, The University of Arizona}
%\email{Email: \{islamsamy@email, tandonr@email, and llazos@ece, \}.arizona.edu}


\maketitle

\begin{abstract}
Speculative Decoding (SD) is a recently proposed technique for faster inference using Large Language Models (LLMs). SD operates by using a smaller draft LLM for autoregressively generating a sequence of tokens and a larger target LLM for parallel verification to ensure statistical consistency.  However, periodic parallel calls to the target LLM for verification prevent SD from achieving even lower latencies. We propose \textit{SPRINTER}, which utilizes a low-complexity verifier trained to predict if tokens generated from a draft LLM would be accepted by the target LLM. By performing \textit{approximate sequential verification}, \textit{SPRINTER} does not require verification by the target LLM and is only invoked when a token is deemed unacceptable. This leads to reducing the number of calls to the larger LLM and can achieve further speedups. We present a theoretical analysis of \textit{SPRINTER}, examining the statistical properties of the generated tokens, as well as the expected reduction in latency as a function of the verifier. We evaluate \textit{SPRINTER} on several datasets and model pairs, demonstrating that approximate verification can still maintain high quality generation while further reducing latency. For instance, on Wiki-Summaries dataset, \textit{SPRINTER} achieves a 1.7x latency speedup and requires 8.3x fewer flops relative to SD, while still generating high-quality responses when using GPT2-Small and GPT2-XL as draft/target models.
\end{abstract}

\section{Introduction}\label{sec:Intro}
 Large Langauge Models (LLMs) have shown to be very effective in different mediums including text generation \cite{zingale2024language}, image analysis \cite{niu2024text}, and video understanding \cite{tang2024videounderstandinglargelanguage}. However, despite this success, LLM-based solutions for various problem domains are still constrained by high computational costs incurred during inference due to large model sizes. To enable faster inference, Speculative Decoding (SD) \cite{leviathan2023fast} has been proposed as a solution for reducing the latencies incurred during the inference of significantly larger LLMs. Under this paradigm, a smaller draft LLM is used to autoregressively generate a certain number of tokens. These tokens are then passed to a larger target LLM, which processes the tokens in parallel to determine how many of them are acceptable (i.e. if the distribution of the generated tokens matches the distribution of what the target model would have generated). If they are not acceptable, then the target model is called to generate replacement tokens. By reducing the amount of times the target model is invoked for autoregressive generation, less latency is incurred. \cite{mamou2024dynamicspeculationlookaheadaccelerates} proposed using a two layer feedforward network to stop the draft model from generating tokens and initiate the target model's verification process, once the output of the network is less than a threshold. 
\cite{huang2024specdec++} models the SD procedure as a Markov Decision Process and uses the draft model with a smaller, multi-layer network to predict the probability that the current token generated by the draft model should be accepted. The probability that there is at least one draft token that should be rejected is subsequently derived and compared with a threshold to determine if the target model should be invoked for verification. Thus, the objective of SD is to incur smaller inference times while guaranteeing that sampling tokens from the draft model is equivalent to sampling from the target model.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{FrameworkSPRINTER.pdf}\vspace{0pt}
    \caption{(a) \textit{SPRINTER} works by generating tokens from a smaller (draft) LLM, which are sequentially accepted/rejected by a \textit{verifier}, a low-complexity small classifier. In \textit{SPRINTER}, the larger (target) LLM is only called if a token is rejected and used only to replace the rejected token. (b) Speculative decoding (SD) works by generating multiple tokens by the draft model, all of them are verified in parallel by the target LLM. (c) Comparison between \textit{SPRINTER} and SD with respect to \textit{Quality} (ROUGE score), \textit{Latency} (time in ms required to generate a token) and \textit{Computation} (number of flops required to generate $20$ consecutive acceptable tokens from the draft model). }
    \label{fig:frameworkSPrinter}
\end{figure*}

\textbf{Overview of \textit{SPRINTER}:} Running the target model for parallel verification, even periodically (i.e. after a certain number of tokens) can still result in significant latencies. Higher speedups can be attained if the constraint that the tokens generated by the draft model must match the distribution of the target model is relaxed. However, we do not want to deviate too far from the target distribution as this would increase the likelihood of the draft model generating inaccurate tokens. To balance this tradeoff, we propose \textit{SPRINTER}, a sampling technique that uses a low-complexity verifier that predicts when it is necessary to invoke the target model to generate a token that replaces the current draft token. Subsequently, under \textit{SPRINTER}, verification is performed sequentially as draft tokens are generated, in contrast to SD based approaches which perform parallel verification of multiple draft tokens through the larger LLM, which requires more computational resources to execute.  In many computational settings, including machine learning and optimization, the cost of generating a valid solution can be significantly higher than that of verifying one. This asymmetry, often referred to as the \textit{generation-verification gap}, also provides inspiration for \textit{SPRINTER}: verifying the acceptability of a token could be  easier than generating a high quality token from a larger LLM. The verifier used in this work has a significantly lower complexity compared to the draft and target LLMs; subsequently, the additional overhead introduced by training and using it is negligible (as further evidenced in the results). b) Our approach is also aligned with observations made in recent works such as \cite{anonymous2025judge,melcer2024approximately}, which have shown that \textit{high-quality} tokens can still be generated without necessarily matching the distribution of the larger target model. We next summarize the main contributions of this paper.
\begin{itemize}
\vspace{0pt}
    \item \textbf{\textit{SPRINTER} framework}. We propose \textit{SPRINTER}, a framework for achieving faster inference from LLMs using a pair of (draft (small), target (large)) LLMs together with the aid of a verifier. The role of the verifier is to perform approximate verification, i.e., if tokens generated by a draft model would be acceptable by the larger target LLM. The key motivating factors and the detailed framework is described in Section \ref{sec:SPRINTER}. 
\vspace{5pt}
    \item \textbf{Theoretical Analysis}. We present a comprehensive theoretical analysis in Section \ref{sec:theory} to show the tradeoffs between quality of generated tokens versus latency speedups and computational savings offered by \textit{SPRINTER}. Specifically, we demonstrate how the ROC curve characteristics (e.g., false-positive and true-positive rates) of the verifier can be used to balance the tradeoff between latency and quality. Furthermore, we discuss strategies to train the verifier and show how the theoretical results also provide useful design insights. 
\vspace{5pt}
    \item \textbf{Experiments}.
    We evaluate \textit{SPRINTER} on several datasets and model pairs, demonstrating its ability to reduce latency while maintaining high quality. Figure \ref{fig:frameworkSPrinter}(c) shows the comparison between \textit{SPRINTER} versus SD on WikiSummaries dataset using GPT-Neo-125M as a draft model, GPT-Neo-1.3B as the target model and a $1k$ parameter verifier. On the Wiki-Summaries dataset, \textit{SPRINTER} sampling achieves nearly the same ROUGE score as SD; generates tokens faster and requires significantly less computation. Our evaluation results are presented in Section \ref{sec:results}.  
\end{itemize}
    

%The structure of the paper is as follows: Section \ref{sec:prelims} reviews the SD framework, Section \ref{sec:SPRINTER} presents our approximate verification-based approach for SD called SPRINTER, Section \ref{sec:results} presents experimental results comparing SPRINTER with other SD baselines, Section 5 presents related works, and Section 6 concludes the paper.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{literature_table.pdf}
    \caption{Comparison of different speculative decoding based mechanisms with respect to three aspects: a) Approximate or Exact match with the larger LLM (target) probability distribution, b) Quality Analysis of Completions and c) Theoretical Results. \textit{SPRINTER} is the first framework to provide in-depth analysis of approximate verification; it's impact on the quality-vs-latency tradeoff and provide insights on how to navigate this tradeoff. Additional discussion on related works is presented in Appendix \ref{sec:relatedworks_append}.}
    \label{fig:literature_table_maintext}
\end{figure}

\vspace{-10pt}

\section{Preliminaries on Speculative Decoding (SD)}\label{sec:prelims}
 SD was originally proposed in \cite{leviathan2023fast} as a novel algorithm for speeding up LLM inference from a larger target LLM $M_p$ 
through the help of a smaller (faster) draft LLM $M_q$. Borrowing notation from \cite{leviathan2023fast}, $p(x)$ and $q(x)$ represent the probability distributions we get from $M_p$ and $M_q$ respectively for the next token given a specific prefix (i.e. set of already generated tokens $x_{<t}$); specifically, we let $p(x)$ and $q(x)$ denote $p(x|\text{prefix})$ and $q(x|\text{prefix})$ respectively. First, given a prefix, the draft model $M_q$ autoregressively generates $\gamma$ tokens, where $\gamma$ is a hyperparameter chosen by the user. These sequence of tokens are then passed to the target model which performs verification of all $\gamma$ candidate completion sequences in parallel. For the last token $x$ in each sequence, the target model $M_p$ is invoked and it verifies the relationship between $q(x)$ and $p(x)$ (see Figure \ref{fig:frameworkSPrinter} for an example). If $q(x) < p(x)$, meaning that the probability of the draft token is within the distribution of the target token, it is acceptable. However, if a draft token in any one of the candidate sequences is not accepted, it can still be accepted with probability $\frac{p(x)}{q(x)}$; otherwise, the target model resamples from an alternative distribution given as follows \cite{leviathan2023fast}: 
\begin{align}
  p'(x) = \text{norm}(\max(0, p(x) - q(x))).
    \label{eq:resample}
\end{align}
SD ensures that the entire sampling process matches the distribution of the target LLM. Speculative decoding has turned out to be powerful in achieving speedups and has led to a large number of recent papers that have proposed variations of SD, including works shown in Figure \ref{fig:literature_table_maintext}. \cite{mamou2024dynamicspeculationlookaheadaccelerates} and \cite{huang2024specdec++}, for example, similarly use a low-complexity classifier to determine when the draft model should stop generating tokens while ensuring that the sampled tokens match the distribution of the target model. \cite{kim2024speculative} and \cite{agrawal2024adaedl} propose heuristics that incur less complexity compared to using a verifier; however, they are also ensure that the sampled tokens match the distribution of the target model. We provide a more detailed discussion on these related works in Section \ref{sec:relatedworks_append}. 



\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{ROC_wiki_LM1B_12.pdf}
    \caption{ROC Curve Performance of a trained Verifier (less than $1k$ parameters) on the Wiki-Summary and LM1B Datasets with GPT-Neo-125M as the draft model $M_q$ and  GPT-Neo-1.3B as the target model $M_p$. Despite being orders of magnitude smaller in size compared to the draft and target models, the verifier was able to achieve AU-ROC of $0.8$ (respectively $0.9$) on the two datasets.}
    \label{fig:ROC_wiki_LM1B_12}
    \vspace{5pt}
\end{figure}

\newpage 
\section{\textit{SPRINTER} Framework \& Analysis}\label{sec:SPRINTER}
 Before presenting our framework, we first discuss the key motivating factors behind \textit{SPRINTER}.


\noindent \textit{\textbf{Cost of Parallelism}}. As SD and several other variants discussed in related work attempt to match the target distribution, they end up invoking the target model which performs parallel verification. While parallelism ensures that the latency (time) for verifying $\gamma$ tokens is equivalent to running the target model once, one still has to pay the cost of parallelism as the target model runs $\gamma$ times. As $\gamma$ increases, the latency reduces but the cost of parallelism grows proportionally. This is the first idea that motivates us to study sequential verification; instead of verifying by the target LLM in parallel, we instead propose \textit{approximate verification} by a significantly smaller model (named the verifier) in a sequential manner as shown in Fig. \ref{fig:frameworkSPrinter}(a). Depending on the quality of the verifier (i.e., false-positive and true-positive rates), we only call the target LLM if a token is rejected by the verifier. Thus, \textit{SPRINTER} can achieve the dual benefit of reducing the number of calls to the target LLM and completely eliminates the cost of running it in parallel. 

As an illustration, Fig. \ref{fig:ROC_wiki_LM1B_12} shows the ROC curve of a low-complexity verifier (with a single layer and less than $1k$ training parameters) which was trained to accept/reject tokens generated by GPT-Neo-125M (draft model $M_q$) and  GPT-Neo-1.3B as the target model $M_p$. The fact that we were able to achieve AU-ROC (area under ROC curve) of $0.8$ and $0.9$ was achievable on LM1B and Wiki-summaries datasets first highlights the feasibility of low-complexity verification (more results are presented in Section \ref{sec:results}). 


\newpage

\noindent \textit{\textbf{Quality of Smaller Models}}. Inevitably, if we resort to approximate verification, we have to give up statistical consistency, i.e., one cannot guarantee a match with the target LLM distribution. However, statistical consistency alone may not be a necessary indicator for high quality generation. For instance, recent works such as Judge Decoding \cite{anonymous2025judge} have shown that even smaller LLMs have generation capabilities that can be comparable with larger ones. Judge decoding however still invokes the (larger) target LLM whose outputs are used for further refinement (e.g., accepting tokens which would have been rejected by conventional speculative decoding). Our idea behind \textit{SPRINTER} is however to use a smaller trained model (verifier) which is pipelined with the smaller model for sequential verification. It is this interplay between latency, total computational costs and quality that motivate \textit{SPRINTER}. We next describe the framework in detail. 

%We define $M_q$ as the draft (small) model and let $q(x_t| x_{<t} )$ denote the probability distribution we get from $M_q$ given the prefix $x_{<t} $. We then define $M_p$ as the target (large) model, which we want to accelerate. Let $ p(x_t| x_{<t} )$ be the probability distribution we get from the $M_p$ model given the prefix $x_{<t} $. To simplify the notions, we use $p(x)$, $q(x)$ to represent $p(x_t| x_{<t} )$, $q(x_t| x_{<t} )$. 


\noindent\textit{\textbf{Algorithm}}. We now provide an overview of the \textit{SPRINTER} sampling process. First, a prefix is fed to the draft model and a token is sampled with probability $q(x)$. The draft token is then passed to a verifier $V$ to predict whether or not the ratio between $q(x)$ and $p(x)$ is greater or less than 1. The verifier can take as input various latent features derived from the draft model (e.g. embedding of the draft token $x$, probability distribution of the LLM's vocabulary). As there is flexibility with which features from the draft model can be extracted, we denote the input to the verifier as $s(x,\text{prefix})$. An ideal verifier would make the following decision: 
\[
{V(s(x,\text{prefix}))} =
\begin{cases}
1   & \frac{q(x)}{p(x)} \leq 1  \\

0& \frac{q(x)}{p(x)} > 1. \\
\end{cases}
\]

\begin{algorithm}[t]
   \caption{\textit{SPRINTER} (full detailed algorithm in Appendix)}
   \label{alg:alg_short}
\begin{algorithmic}
   \STATE {\bfseries Input:} $M_p, M_q, V, \text{Prefix}, \text{Prediction Threshold}~\tau$
   % \REPEAT
   % \STATE $x = \emptyset$
   \STATE Initiate the values
   % \STATE $r = 1$
   \WHILE{True}
   \STATE Update the Prefix
    \STATE Generate the current token $x$ from $M_q$
    \STATE Obtain the Verifier's prediction of the current token $V (s(x, \text{Prefix}))$ . 
      
    %   \STATE $q_r(x) \sim M_q(\text{Prefix})$
    % \STATE $x \sim q_r(x)$
    \IF{$V (s(x, \text{Prefix})) \leq \tau$ [Reject Token]}
    \STATE Break
    \ENDIF
    \ENDWHILE
   \STATE Invoke $M_p$ to verify the \textcolor{blue}{last token} and re-sample if necessary.
 
\end{algorithmic}
\end{algorithm}

 If $V(s(x,\text{prefix})) = 1$, the verifier predicts that the ratio is less than 1, suggesting that the current token $x$ is acceptable and that $M_q$ should generate the next token. If $V(s(x,\text{prefix}))=0$, the verifier predicts that the ratio is larger than 1, leading to the rejection of the current token and indicating that $M_p$ should be called. In this scenario, similar to SD, the draft token can be accepted with probability $\frac{p(x)}{q(x)}$ or rejected with probability $1-\frac{p(x)}{q(x)}$ and replaced with a token sampled from the revised distribution \eqref{eq:resample}. In Section \ref{sec:learn_veri}, we provide details on how to train a verifier.  We illustrate the sampling process of \textit{SPRINTER} in Algorithm \ref{alg:alg_short} (full algorithm is presented in the Appendix \ref{sec:algorithm_hyper}) and show the \textit{SPRINTER} sampling process in Fig \ref{fig:frameworkSPrinter}(a). 

 

\subsection{Theoretical Analysis of SPRINTER}\label{sec:theory}
In this Section, we present our theoretical results on \textit{SPRINTER}. Through these results, we aim to study the impact of verifier's performance on a) the probability distribution of tokens sampled by \textit{SPRINTER}; b) expected number of consecutive tokens sampled by \textit{SPRINTER} before invoking the target model; c) average latency incurred in the process as well as the amount of computational savings. As we discuss later in this Section, these results also provide practical design insights for navigating the quality-vs-latency tradeoffs.  
% The objective of \textit{SPRINTER} is to accept draft tokens that are still high-quality while not stringently matching the distribution of the target model. Subsequently, higher latency speedups and computational savings can be attained but at the cost of deviating from the target model. 

\noindent\textbf{(a) Statistical Analysis of generated tokens.} Let us denote $\eta_{FP}$ and $\eta_{TP}$ as the false-positive and true-positive rates of the verifier, respectively. Specifically, a false positive refers to the setting if an unacceptable token (i.e., $q(x)/p(x)>1$) is deemed acceptable by the verifier. Conversely, a true positive refers to the scenario if an acceptable token (i.e., $q(x)/p(x)\leq 1$) is accepted by the verifier. 
In our first result, we characterize the distribution of the tokens generated by \textit{SPRINTER}. Proof of Theorem \ref{the:SPRINTER} can be found in the Appendix \ref{sec:31proof}.
\begin{theorem}\label{the:SPRINTER}
    The probability of a token $x$ being chosen when running SPRINTER is given as
    \begin{align}
    \label{eq:SPRINT_DIST}
        p_\text{SPRINTER}(x) = (1-\eta_\text{FP})p(x) + \eta_\text{FP} q(x),
    \end{align}
    where $\eta_\text{FP}$ is the false positive rate of the verifier. Furthermore, the total-variation distance between the target and \textit{SPRINTER} distributions is $d_\text{TV}(p,p_\text{SPRINTER}) = \eta_\text{FP}d_\text{TV}(p,q)$.
\end{theorem}

% \textbf{Sketch of proof:} A token generated by the draft model should either be accepted or rejected based on the relationship between $q(x)$ and $p(x)$. Based on this, we derive the probability of an acceptable/unacceptable token ultimately being sampled by SPRINTER based on the verifier's decision to either accept or reject that token. We also consider the scenario where a token from the draft model is rejected, leading to an acceptable token being sampled via the adjusted distribution \eqref{eq:resample}. We show that the probability distributions for an acceptable and unacceptable token result in the following identical form: $ (1-\eta_\text{FP})p(x) + \eta_\text{FP} q(x)$. The full proof can be found in the appendix.

\begin{remark}
The false positive rate $\eta_\text{FP}$ directly influences the distribution of \textit{SPRINTER} and represents how much the distribution of \textit{SPRINTER} deviates from the distribution of the target model. This observation aligns with the intuition that a perfect verifier with $\eta_\text{FP} =0$ (i.e. \textit{SPRINTER} never samples a token that should be rejected), for example, this results in $p_\text{SPRINTER} = p(x)$, effectively matching the distribution of the target model. 
\end{remark}

% \section{Theoretical Analysis on SPRINTER}
\noindent\textbf{(b) Expected number of generated tokens}. We now analyze the expected number of tokens accepted by the verifier when using \textit{SPRINTER} as a function of $\eta_\text{FP}$ and $\eta_\text{FN}$. For this analysis, we consider the scenario illustrated in Figure \ref{fig:token_generation}. Suppose that the ground truth is that given a prefix, the draft model $M_q$ is capable of producing $r$ acceptable tokens sequentially (in other words, the first $r$ generated tokens by $M_q$ are acceptable, whereas subsequent ones are unacceptable). Under this ground truth, let us define the random variable $N_{\textit{SPRINTER}}$ as the number of consecutive tokens accepted by the verifier. The verifier can exhibit two types of behavior also shown in Fig. \ref{fig:token_generation}:

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{token_generation.pdf}
    \caption{A draft LLM generates $r$ tokens that are acceptable and subsequent tokens that are unacceptable. If the verifier rejects one of the first $r$ consecutive tokens, the speedup attained from continuing to accept tokens until the $r^{th}$ token is lost (i.e. early rejection). If the verifier continues to accept tokens after the $r^{th}$ token, it experiences smaller latencies but at the cost of accepting low-quality tokens (i.e. delayed rejection). Theorem \ref{the:SPRINTER_expected_tokens} characterizes the expected number of generated tokens as a function of \(\eta_\text{TP}\) and \(\eta_\text{FP}\).}
    \label{fig:token_generation}
\end{figure*}

\begin{itemize}
    \item \textbf{Early Rejection}: This occurs if the verifier accepts the first $(i-1)$ tokens but mistakenly predicts that the $i^{th}$ token (for $i \leq r$) should be rejected, then the verifier will revert to calling the target model rather than continuing to enable the draft model to generate the remaining $r-i$ acceptable tokens. In doing so, the verifier misses out on experiencing an even greater latency reduction while still generating high-quality tokens. This indicates that $\eta_\text{TP}$ directly influences the early rejection caused by \textit{SPRINTER}.
    \item \textbf{Delayed Rejection}: On the other hand, it can happen that the verifier continues to accept more than $r$ tokens. Specifically, if the verifier first stops at the $i^{th}$ token (for $i > r$), then it does not invoke the target model until token $i$, resulting in a higher computational savings and latency speedups but at the cost of accepting $(i-r)$ lower quality tokens. This indicates that $\eta_\text{FP}$ directly influences the deviation of the statistical distribution from the target model.
\end{itemize}
Our next Theorem characterizes the properties of $N_\text{SPRINTER}$ assuming that $r$ consecutive draft tokens are acceptable. 


\begin{theorem}\label{the:SPRINTER_expected_tokens}
    The probability distribution of the number of generated tokens is given as: 
    \[
\mathbb{P}(N_\text{SPRINTER}=i) =
\begin{cases}
\eta_\text{TP}^{i} (1-\eta_\text{TP}) & i < r, \\
\eta_\text{TP}^r(\eta_\text{FP})^{i-r}(1-\eta_\text{FP}) & i\geq r.
\end{cases}
\]
The expected number of generated tokens is given as:
%The expected number of consecutive tokens accepted by the verifier prior to predicting that the target model should be invoked is given by $\mathbb{E}[\text{generated tokens}] = $:
    %The SPRINTER sampling method samples $q(x) \sim M_q$ which is used by $M_{\text{DRT}}$ to predict whether or not $\frac{q(x)}{p(x)} \leq 1$. If $\hat{Z}=1$, we accept the token and use $M_q$ to generate the next token. If $\hat{Z} = 0$, we reject the token and call the large model $M_p$. Overall, SPRINTER satisfies:
    \begin{align}
\label{eq:SPRINT_EXP_TOKEN}
        \mathbb{E}(N_\text{SPRINTER})=       \frac{\eta_\text{TP}-\eta_\text{TP}^r}{1-\eta_\text{TP}} + \frac{\eta_\text{TP}^r}{1-\eta_\text{FP}}.
        \end{align}
\end{theorem}

The proof of Theorem \ref{the:SPRINTER_expected_tokens} can be found in the Appendix \ref{sec:expec_proof}. To gain more insights from this result, Fig. \ref{fig:token_tp_tradoff} illustrates the trade-off between $\eta_\text{TP}$ and $\mathbb{E}(N_\textit{SPRINTER})$ for two different values of $\eta_\text{FP}$ assuming that $r = 5$. The figure indicates that as $\eta_\text{TP}$ approaches 1,  even with a verifier that has a substantial $\eta_\text{FP}$ (i.e. 0.5), the additional number of tokens that are accepted past $r$ is marginal. This implies that if an esitmate of $r$ could be attained from the training data, then fixing $\eta_\text{FP}$ and varying $\eta_\text{TP}$, for example, could bring insights into how well the verifier must perform to generate close to the ideal $r$ tokens. Essentially, $\eta_\text{TP}$ and $\eta_\text{FP}$ can be varied to determine the optimal false positive and negative rates such that the chances of early and delayed rejection occurring potentially caused by the verifier are minimized. %An ROC curve is shown in Figure that further illustrates the impact  $\eta_\text{FN}$ and $\eta_\text{FP}$ (i.e. equivalent to $1-\eta_\text{TN})$ have on the generated/acceptable draft tokens.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{tokens_TP_FP.pdf}
    \caption{Illustration of expected number of tokens genrerated by \textit{SPRINTER} as a function of true-positive rate ($\eta_{TP}$) for two different values of $\eta_{FP}$ when the number of consecutively acceptable tokens is $r=5$. We can observe that as long as $\eta_{FP}\leq 0.5$, the average number of unacceptable tokens  (shown as the \textit{``Gap"}) generated by \textit{SPRINTER} never exceeds $1$.}
    \label{fig:token_tp_tradoff}
    \vspace{-10pt}
\end{figure}
\noindent\textbf{(c) Latency Analysis and Computational Cost}. 
Given the result in Theorem \ref{the:SPRINTER_expected_tokens}, we now derive the latency incurred by \textit{SPRINTER} under the scenario in Figure \ref{fig:token_generation}. Let $t_d$, $t_t$ and $t_v$ represent the time required to inference the draft, target and verifier models respectively. We also assume  $t_v \leq t_d$, i.e., the time it takes to run the verifier is smaller than running the draft model. Under the above assumption, the next result characterizes the expected stopping time (i.e., the average time before the first rejection by the verifier). 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{latency_TP_FP.pdf}
    \caption{Illustration of expected stopping time of \textit{SPRINTER} as a function of true-positive rate ($\eta_{TP}$) for two different values of $\eta_{FP}$ when $r=5$ and $t_d = 0.1$. We can again observe that as long as $\eta_{FP}\leq 0.5$, the average stopping time (shown as the \textit{``Gap"}) generated by \textit{SPRINTER} never exceeds $t_d$.}
    \label{fig:latency_TP_FP}
\vspace{-15pt}
\end{figure}

\begin{theorem}\label{the:SPRINTER_expected_latency}
    The expected stopping time is given as:
    %The SPRINTER sampling method samples $q(x) \sim M_q$ which is used by $M_{\text{DRT}}$ to predict whether or not $\frac{q(x)}{p(x)} \leq 1$. If $\hat{Z}=1$, we accept the token and use $M_q$ to generate the next token. If $\hat{Z} = 0$, we reject the token and call the large model $M_p$. Overall, 
    \begin{align}
        \label{eq:SPRINT_EXP_LATENCY}
        & \mathbb{E}[T_\text{Stop}] =  \frac{(1-\eta_{\text{TP}}^r) t_d}{1-\eta_\text{TP}} + \frac{\eta_\text{TP}^rt_d}{1-\eta_\text{FP}}.
        % &\mathbb{E}[\text{Latency}] = t_d \frac{\eta_{\text{TP}}}{1-\eta_{\text{TP}}}(1-(r+1)\eta_{\text{TP}}^r +r\eta_{\text{TP}}^{r+1} ) \nonumber\\
        % &+ (t_d+t_t)(1-\eta_{\text{TP}}^{r+1}) + \eta_{\text{TP}}^{r}((r+1+\eta_{\text{FP}})t_d+t_t)
    \end{align} 
\end{theorem}

Proof of the above theorem can be found in Appendix \ref{sec:lat_proof}. From Theorem \ref{the:SPRINTER_expected_latency}, we can observe that the expected stopping time increases as $\eta_{TP}$ increases. We also observe that the dependence on the false-positive rate $\eta_{FP}$ is marginal compared to $\eta_{FP}$. Furthermore, Fig. \ref{fig:latency_TP_FP} shows that as $\eta_{TP}$ approaches 1, even with a verifier having a relatively high $\eta_{FP}$ (e.g., 0.5), the additional expected stopping time remains minimal (Gap $\leq t_d$) when $r=5, t_d= 0.1$.



\noindent\textbf{Savings in Computation}. We now compare the total computational cost of \textit{one run of SD} versus \textit{one run of SPRINTER}. For simplicity, assume that one has a perfect verifier (i.e., $\eta_{FP}=0$ and $\eta_{TP}=1$). In SD, $\gamma$ tokens are first generated by the draft model, followed by parallel verification done by the target model. If we denote $F_d, F_t$ and $F_v$ as the number of flops to run the draft, target and verifier models once, then we have 
\begin{align}
    \text{SD-Flops}(\gamma) = \gamma F_d + \gamma F_t\\
    \text{\textit{SPRINTER}-Flops}(\gamma) = \gamma F_d + \gamma F_v +  F_t
\end{align}
If $F_{v}\ll F_d \ll F_t$, we can observe that computational savings from \textit{SPRINTER} can be significant and grow proportional to $(\gamma-1)F_t$ due to sequential verification through a low-complexity model (additional calculations showing the computational savings for model pairs in Appendix \ref{sec:flops_append}).


% \textbf{Calculation of }$\alpha_{\text{SPR}}$ \textbf{and} $\beta_{\text{SPR}}$. It is well known \cite{leviathan2023fast} that the acceptance rate of standard speculative decoding $\beta$ can achieve:
% \begin{align}
%     \beta = 1-d_{TV}(p,q),
% \end{align}
% where $d_{TV}(p,q)$ is the total variation distance  between the distributions $p$ and $q$. We next propose a theorem showing the acceptance rate of tokens generated by $M_q$ when using SPRINTER.
% \begin{theorem}\label{the: acceptance_rate}
%     The acceptance rate of the SPRINTER $\beta_{\text{SPR}}$ is 
%     \begin{align}
%         1-(1-\eta_0)\cdot d_{TV}(p,q).
%     \end{align}
% \end{theorem}

% \begin{proof}
%     We know that $\beta_{\text{SPR}}$ satisfies the following:
% \begin{align}
%     \beta_{\text{SPR}} = \mathbb{E}_{x\sim q(x)}
%     \begin{cases} 
% 1 & \text{if } q(x) \leq p(x) \\
% \eta_0+\frac{p(x)}{q(x)} (1-\eta_0)& \text{if } q(x) > p(x).
% \end{cases} 
% \end{align}
% Therefore, by replacing the expectation operator with the empirical samples, we have:
% \begin{align}
%     &\beta_{\text{SPR}} = \sum_{x} \min(q(x), \eta_0 q(x) + (1-\eta_0)p(x))\nonumber\\
%     &=\sum_{x} \min(\eta_0 q(x)+(1-\eta_0)q(x), \eta_0 q(x) + (1-\eta_0)p(x))\nonumber\\
%     &=\sum_{x} (1-\eta_0)\min(q(x), p(x))+ \eta_0 q(x)\nonumber\\
%     & = (1-\eta_0) \cdot(1-d_{TV}(P,Q)) + \eta_0\nonumber\\
%     &= 1-(1-\eta_0)\cdot d_{TV}(P,Q).
% \end{align}
% \end{proof}
% \begin{remark}
%     Note that when $\eta_0 = 0$, the acceptance rate is the same as that of speculative decoding.
% \end{remark}
% \begin{remark}
%     When $\eta_0 \neq 0$,  $\beta_{\text{SPR}} \geq \beta$ meaning that SPRINTER has a higher acceptance rate compared to that of standard speculative decoding. The inherent reason is that the classifier is susceptible to predicting that the token is acceptable when it actually should be rejected, causing a higher acceptance rate and subsequently enabling the draft model to generate more tokens. However, $\eta_0$ also reflects the distance of the probability distribution of SPRINTER from the probability distribution of the target model $p$. This means that while more tokens are being accepted, the distribution they are sampled from may deviate from the distribution of $M_p$, causing poorer quality tokens to be generated. Therefore, $\eta_0$ measures the tradeoffs between the accuracy of the tokens generated and the latency incurred from using SPRINTER.
% \end{remark}
% \begin{corollary}
%     The expectation of $\beta_{\text{SPR}}$ is $\alpha_{\text{SPR}}$, where 
%     \begin{align}
%         \alpha_{\text{SPR}} = (1-\eta_0)\mathbb{E}(\min(q(x), p(x))) + \eta_0.
%     \end{align}
% \end{corollary}

\subsection{Verifier Training and Architecture}
\label{sec:learn_veri}
% The corresponding ground truth is the indicator function of the ratio of the distribution of $M_q: q_i(x|\text{prefix})$ and the distribution of $M_p: p_i(x|\text{prefix})$:
% \begin{align}
%     \mathbbm{1}\left(\frac{q_i(x|\text{prefix})}{p_i(x|\text{prefix})}\leq 1\right).
% \end{align}
\textbf{Verifier Training Methodology}. The verifier $V$ is a binary classifier trained to predict whether a token should be accepted or rejected. Specifically, $V()$ can take as input various latent features derived from the draft model (e.g. embedding of the draft token $x$, probability distribution of the LLM's vocabulary). We denote the input to the verifier as $s(x,\text{prefix})$. The data used for training the verifier can be prepared as follows: for a given prefix, a token $x$ is sampled from the draft model $M_q$. We also run the target model $M_p$ and compute $p(x)$.  Subsequently, binary labels are determined for each prefix and token pair by assigning $1$ if $\frac{q(x)}{p(x)} \leq 1$ and $0$ otherwise. However, rather than comparing the ratio $\frac{q(x)}{p(x)}$ against a threshold of 1, we can increase (decrease) the threshold to $\lambda$ to bias the verifier to accept (reject) more draft tokens, which would increase (decrease) $\eta_\text{FP}$ ($\eta_\text{TP}$). Additionally, adjusting the inference threshold $\tau$ (see Algorithm \ref{alg:alg_short}) would achieve a similar effect. Thus, varying $\lambda$ and $\tau$ serve as the two hyperparameters which allow us to influence $(\eta_\text{FP},\eta_\text{TP})$. 

Our second observation is that during inference, \textit{SPRINTER} would face input as prefixes consisting of interleaved tokens generated in the past by draft and target models. Hence, during training, we expose the verifier to the following possible inputs: (a) an original prefix, (b) a prefix supplied with completions only from the draft model, (c) a prefix supplied with completions only from the target model (d) a prefix  with tokens from both draft and target models. To optimize the verifier's performance, we ensure an equal proportional of prefixes from each category.

\textbf{Verifier architecture used for evaluation}. For our experiments, the verifier was implemented as a fully connected linear layer followed by a sigmoid activation, containing significantly fewer parameters than $M_p$ and $M_q$. $V$ takes as input the last embedding of the previous token and is trained with an Adam optimizer assuming binary cross entropy loss. Our results indicate that a single layer is sufficient to achieve strong performance while maintaining high efficiency, as shown in Fig. \ref{fig:ROC_wiki_LM1B_12} on the Wiki-summary and LM1B datasets.  We observed that training thresholds of $\lambda=1$ and $\lambda=1.2$ enabled \textit{SPRINTER} to attain an effective performance on Wiki-Summary and LM1B datasets respectively.   


\section{Experiments and Evaluation}
\label{sec:results}
In this section, we evaluate \textit{SPRINTER} across various datasets, including Wiki-summary \cite{scheepers2017compositionality} and the LM1B \cite{chelba2013one} datasets and model pairs. Figure \ref{fig:Prompt_example} shows responses generated from \textit{SPRINTER} and SD. The examples indicate that \textit{SPRINTER} can generate tokens that still make coherent sense without strictly adhering to the distribution of the target model. 
% Our code is available at \cite{codesprinter}. 
%\footnote{\url{https://anonymous.4open.science/r/SPRINTER-D17B/}}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{Prompt_example.pdf}
    \vspace{-4pt}
    \caption{Illustration of responses generated via  \textit{SPRINTER} vs SD. For the first two prompts, response from \textit{SPRINTER} is of comparable quality to SD; whereas SD's response to the third prompt is superior (more examples are shown in Appendix \ref{sec:prompts_ex}).}
\vspace{-15pt}
    \label{fig:Prompt_example}
\end{figure}

\noindent\textbf{Quantitative Evaluation.} To quantify the effectiveness of \textit{SPRINTER} we present the following set of results: (a) We measure the quality of responses generated by \textit{SPRINTER} using two performance metrics: the win-tie rate and ROUGE scores.  (b) We comparing the latencies incurred by \textit{SPRINTER}, SD \cite{leviathan2023fast}, SpecDec++ \cite{huang2024specdec++} and AdaEDL \cite{agrawal2024adaedl} in generating text completions given a prefix.  (c) To investigate the impact of verifier training and inference hyperparameters on \textit{SPRINTER}, we perform an ablation study to observe how $\eta_\text{TP}$ and $\eta_\text{FP}$ affect the ROC. 

%with the GPT-Neo and GPT2 model families. We demonstrate that SPRINTER consistently achieves higher speeds while remaining capable of generating high-quality responses. We first evaluate the verifier on the standard benchmark datasets in the SD literature. We then compare the SPRINTER with other state-of-the-art techniques in the SD literature. Finally, we conduct an ablation study on SPRINTER.

%%\subsection{Setup}
\noindent\textbf{Dataset and Model Architecture}. 
We use WiKi-summary \cite{scheepers2018improving} and LM1B \cite{chelba2013one} datasets for evaluation.  Wiki-Summary is a collection of Wikipedia article summaries designed for text summarization tasks. Each sample includes features such as title, description, summary, and full text, making it well-suited for evaluating LLMs for summarization and text generation. The LM1B (One Billion Word Benchmark) Dataset is a large-scale corpus for language modeling and text generation tasks. It consists of approximately $1$ billion words extracted from news articles. We adopted a similar experimental setup as the prior works: \cite{github_specdec,chakraborty2024transferqstarprincipled}. We present results for two (draft, target) model pairs: GPT-Neo-125M \cite{GPT-Neo-125m}/GPT-Neo-1.3B \cite{GPT-Neo-1.3b} and GPT2-Small (124m param.) \cite{GPT2}/GPT2-XL (1.5b param.) \cite{GPT2-XL}. 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{Rouge_overall.pdf}
    \caption{The ROUGE metrics (ROUGE-1, ROUGE-2, ROUGE-2) of SPRINTER vs SD for (a) GPT-Neo-125M/GPT-Neo-1.3B and (b) GPT2-Small/GPT2-XL model pairs. This demonstrates that even with faster inference speeds, \textit{SPRINTER }experiences only a minimal drop in quality compared to SD.}
    \label{fig:Rouge_overall}
\end{figure}



\begin{table}[t]
    \centering
    \resizebox{0.6\textwidth}{!}{%
    %\scalebox{0.83}{
    \begin{tabular}{l l c c}
        \hline
          $M_q/M_p$ & WiKi-Summary & LM1B \\
         \hline
          GPT-Neo-125M/1.3B& 45.2 $\pm$ 3.12 & 39.6 $\pm$ 1.94 \\
 GPT2-Small/XL& 41.0 $\pm$ 6.82 & 36.2 $\pm$ 4.09 \\
 \hline
    \end{tabular}}
    \caption{Average win-tie rates of \textit{SPRINTER} against standard Speculative Decoding for GPT-Neo-125M/GPT-Neo-1.3B and GPT2-Small/GPT2-XL model pairs. }
    \label{tab:winrate}
    \vspace{-10pt}
\end{table}


\begin{table*}[t]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
      \hline
        &\multicolumn{3}{|c|}{WiKi-Summary} & \multicolumn{3}{|c|}{LM1B}\\
        \hline
       Methods  & Avg Tokens & Time (ms) & Speedup  & Avg Tokens & Time (ms) & Speedup \\
       % $Standdec$ & & & \\
       \hline
       $SD$  &   2.17 & 0.92  $\pm$  0.32&  1x & 1.95 & 0.84  $\pm$   0.21 & 1x\\
       \hline
       $Specdec^{++}$ & 1.16 & 0.75 $\pm$  0.25 &  1.23x & 1.08 & 0.61  $\pm$  0.24 & 1.38x\\
       \hline
       $AdaEDL$ & 0.82 & 0.70  $\pm$  0.19 & 1.31x & 0.81 & 0.57   $\pm$  0.05 & 1.47x \\
       \hline
       \textit{SPRINTER} & \textbf{11.10} & \textbf{0.56  $\pm$  0.22} & \textbf{1.64x} & \textbf{8.32} & \textbf{0.46  $\pm$  0.09} & \textbf{1.83x}  \\
       \hline
       % \hline
       % \multirow{5}{5em}{GPT2-Small/ GPT2-XL}&$SD$  &  2.01 & 0.84 $\pm$ 0.24 & 1x & 2.05 & 0.73 $\pm$ 0.15 & 1x\\
       % \cline{2-8}
       % &$Specdec^{++}$ & 1.82  & 0.72 $\pm$ 0.23 & 1.18x & 1.04 & 0.61 $\pm$ 0.28  & 1.20x\\
       % \cline{2-8}
       % &$AdaEDL$ & 0.79 & 0.73 $\pm$ 0.17 & 1.16x & 0.81 & 0.61 $\pm$ 0.05 & 1.20x \\
       % \cline{2-8}
       % &\textit{SPRINTER} & \textbf{10.83} & \textbf{0.49 $\pm$ 0.20} & \textbf{1.69x} & \textbf{6.56} & \textbf{0.44 $\pm$ 0.12} & \textbf{1.66x}  \\
       % \hline
    \end{tabular}
    \vspace{-3pt}
    \caption{Latency speedups attained relative to SD \cite{leviathan2023fast} for \textit{SPRINTER}, AdaEDL \cite{agrawal2024adaedl} and Specdec++ \cite{huang2024specdec++} using GPT-Neo-125M and GPT-Neo-1.3B as the draft and target models respectively on the Wiki-Summary and LM1B datasets. }
    \label{tab:latency_wiki}
\end{table*}

\begin{table*}[t]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
      \hline
       &\multicolumn{3}{|c|}{WiKi-Summary} & \multicolumn{3}{|c|}{LM1B}\\
        \hline
       % Methods  & Avg Tokens & Time (ms) & Speedup  & Avg Tokens & Time (ms) & Speedup \\
       % % $Standdec$ & & & \\
       % \hline
       % $SD$  &   2.17 & 0.92  $\pm$  0.32&  1x & 1.95 & 0.84  $\pm$   0.21 & 1x\\
       % \hline
       % $Specdec^{++}$ & 1.16 & 0.75 $\pm$  0.25 &  1.23x & 1.08 & 0.61  $\pm$  0.24 & 1.38x\\
       % \hline
       % $AdaEDL$ & 0.82 & 0.70  $\pm$  0.19 & 1.31x & 0.81 & 0.57   $\pm$  0.05 & 1.47x \\
       % \hline
       % &\textit{SPRINTER} & \textbf{11.10} & \textbf{0.56  $\pm$  0.22} & \textbf{1.64x} & \textbf{8.32} & \textbf{0.46  $\pm$  0.09} & \textbf{1.83x}  \\
       % \hline
       % \hline
       $SD$  &  2.01 & 0.84 $\pm$ 0.24 & 1x & 2.05 & 0.73 $\pm$ 0.15 & 1x\\
       \hline
       $Specdec^{++}$ & 1.82  & 0.72 $\pm$ 0.23 & 1.18x & 1.04 & 0.61 $\pm$ 0.28  & 1.20x\\
       \hline
       $AdaEDL$ & 0.79 & 0.73 $\pm$ 0.17 & 1.16x & 0.81 & 0.61 $\pm$ 0.05 & 1.20x \\
       \hline
       \textit{SPRINTER} & \textbf{10.83} & \textbf{0.49 $\pm$ 0.20} & \textbf{1.69x} & \textbf{6.56} & \textbf{0.44 $\pm$ 0.12} & \textbf{1.66x}  \\
       \hline
    \end{tabular}
    \vspace{-3pt}
    \caption{Latency speedups attained relative to SD \cite{leviathan2023fast} for \textit{SPRINTER}, AdaEDL \cite{agrawal2024adaedl} and Specdec++ \cite{huang2024specdec++} using GPT2-Small and GPT2-XL as the draft and target models respectively on the Wiki-Summary and LM1B datasets. }
    \label{tab:latency_lm1b}
\end{table*}

\noindent\textbf{Quality Analysis}. We first investigate the quality of responses generated by \textit{SPRINTER} compared with SD to observe how much degradation is experienced when relaxing the constraint that tokens generated by the draft model must match the target distribution. To quantify the quality of the completions made by \textit{SPRINTER}, we employ the win rate metric, which has been used frequently and extensively in LLM research \cite{rafailov2024direct,shen2024learning}. Win-rate measurements are taken by presenting GPT-4 with responses from two LLMs and prompting it to decide which response is better based on criteria provided by the user. In this work, GPT-4 is prompted to evaluate the quality of completions based on accuracy and level of detail of the responses.  Table \ref{tab:winrate} reports win-rate comparisons of \textit{SPRINTER} with SD,  where  GPT-4 is provided with an initial prefix and completions from \textit{SPRINTER} and SD constrained to 20 tokens each. The table indicates that \textit{SPRINTER} using the GPT-Neo model pair can generate responses of comparable quality to SD. This is especially observed on Wiki-Summary, which wins on-average 45.2\% of the time against SD, indicating that \textit{SPRINTER} suffers minimal quality degradation while still achieving significant latency speedup compared to SD.



To further illustrate the comparable quality between \textit{SPRINTER} and SD, Figures \ref{fig:Rouge_overall} show ROUGE scores \cite{lin-2004-rouge} of the responses made by \textit{SPRINTER} and SD respectively with the reference summaries in Wiki-Summary for both GPT-Neo and GPT2 model pairs. The ROUGE scores measure differing levels of similarity  between a provided ``candidate" summary  and reference summary \cite{lin-2004-rouge}. For both model pairs, the figures indicate that \textit{SPRINTER} is able to generate responses that attain very similar ROUGE scores to SD. This again shows that even with faster inference, \textit{SPRINTER} experiences a minimal drop in quality.  


\noindent\textbf{Latency Speedups}. Table \ref{tab:latency_wiki} and Table \ref{tab:latency_lm1b} reports results comparing the latency speedups achieved with \textit{SPRINTER} relative to the methods shown in Figure \ref{fig:literature_table_maintext} for the Wiki-Summary and LM1B datasets. Given a prefix,  20 additional tokens were generated by each method. We report the ``Avg Token" as the accepted token generated by $M_q$ per single run of the sampling process. As the table indicates, \textit{SPRINTER} is able to achieve higher speedup improvements relative to the SD compared to the other baselines, without being restricted to generating tokens that strictly match the target model distribution.





%Two hyperparameters that can be adjusted on the verifier are the training and inference thresholds. The training threshold refers to the limit the verifier is trained to predict $\left(\frac{q_i(x|\text{prefix})}{p_i(x|\text{prefix})}\right)$ is either greater or less than. Conversely, the training threshold can remain fixed, and the threshold during inference can instead be adjusted to ``bias" the verifier's decision on whether a draft token should be accepted or not. The motivation for increasing both values is to encourage the verifier to accept more draft tokens to reduce the frequency of the target model being invoked and subsequently incur smaller latencies. 



% \textbf{Example Completions} Figure \ref{fig:Prompt_example} additionally shows example completions from SPRINTER and SD. The examples indicate that SPRINTER can generate tokens that still make coherent sense without strictly adhering to the distribution of the target model. More example completions can be found in the Appendix \ref{sec:prompts_ex}.


%%\end{itemize}
\noindent\textbf{Verifier Ablation Study}. The training threshold of the verifier can be adjusted to bias it towards accepting more draft tokens. Rather than strictly forcing the verifier to only accept draft tokens if the underlying ratio $\left(\frac{q_i(x|\text{prefix})}{p_i(x|\text{prefix})}\right) \leq 1$, the threshold of $1$ could be changed to a parameter $\lambda>1$; thereby allowing for more tokens to be deemed acceptable, providing another method for \textit{SPRINTER} to generate tokens that deviate from the target distribution and accelerate inference. To investigate this potential benefit, Figures \ref{fig:ROC_LM1B_neo} show ROC curves for the verifier on the LM1B dataset when trained at thresholds $\lambda = $ 1, 1.2, and 1.5 respectively. The figures indicate that increasing the threshold on both datasets, can attain higher areas under the curve (additional results are presented in Appendix \ref{sec:add_results}).

\begin{figure}
    \centering
    \includegraphics[width=0.53\linewidth]{overall_roc_lm1b.pdf}
    \caption{ROC curves for the Verifier (LM1B dataset) by varying the $q(.)/p(.)$ ratio threshold $\lambda$ for acceptance/rejection during training. For generating our latency and quality results, we trained the verifier using $\lambda=1.2$ for the GPT2-Small/GPT2-XL pairs and GPT-Neo draft/target pairs.}
    \label{fig:ROC_LM1B_neo}
    \vspace{-12pt}
\end{figure}

\section{Conclusion}
\vspace{-5pt}
In this work, we introduced \textit{SPRINTER}, a sampling framework designed to accelerate LLM inference by leveraging a draft-target model pair along with a lightweight verifier. Our theoretical analysis highlights the trade-offs between inference speed, computational efficiency, and output quality, demonstrating how verifier characteristics, such as false-positive and true-positive rates, influence performance. Through extensive experiments on multiple datasets and model pairs, we showed that \textit{SPRINTER} significantly reduces latency while maintaining high-quality outputs. Our result show that  the concept of sequential approximate verification can be effective in balancing efficiency and quality, making it a promising approach for scalable and cost-effective LLM deployment.


%This distribution, however, may differ from the posterior $\mathbf{y}(\mathbf{x})$, because it describes both the aleatoric uncertainty---which comes from the distribution $\mathcal{D}$ itself---and the epistemic uncertainty---which comes from a lack of knowledge about the best classifier $M^{\boldsymbol{\theta}}$ to fit the distribution \cite{AvsE1,AvsE2,AvsE3,AvsE4,label_wise_AandE}.  
%REVISE Once the distribution $\mathcal{H}$ has been found, one can evaluate the output $M_{\boldsymbol{\theta}}(\mathbf{x})$ for a large sampling of $\boldsymbol{\theta}$ from $\mathcal{H}$ and use those outputs to construct a probability distribution over which classes $\mathbf{x}$ is most likely to be classified into.  It is unclear, however, whether this distribution will describe the posterior of $\mathbf{x}$.  This is because the posterior distribution $\mathbf{y}(\mathbf{x})$ is meant to describe the inherent or \textit{aleatoric} uncertainty that comes from $\mathcal{D}$, but the distributions produced by these ensembles can also express the \textit{epistemic} uncertainty.  Epistemic uncertainty comes from a lack of knowledge about the best model parameters and not form the distribution $\mathcal{D}$ itself \cite{AvsE1,AvsE2}.  Because the distributions produced by ensembles describe this additional uncertainty, they may differ from the posterior distribution $\mathbf{y}(\mathbf{x})$.

\bibliographystyle
{IEEEtran}
{\small  \bibliography{references}}



\newpage
\begin{appendices}
\section{Appendix}
The Appendix is organized as follows: 

\ref{sec:31proof} Proof of Theorem \ref{the:SPRINTER} (Analysis of the distribution of the tokens generated by
\textit{SPRINTER})

\ref{sec:expec_proof} Proof of Theorem \ref{the:SPRINTER_expected_tokens} (Analysis of the expected number of generated tokens from \textit{SPRINTER})

\ref{sec:lat_proof} Proof of Theorem \ref{the:SPRINTER_expected_latency} (\textit{SPRINTER} latency analysis)

\ref{sec:Acc_rate} Additional theoretical results

\ref{sec:algorithm_hyper} Full \textit{SPRINTER} algorithm and hyperparameter tuning

\ref{sec:relatedworks_append} More related works

\ref{sec:flops_append} Flops explanations and evaluations

\ref{sec:prompts_ex} Additional Examples of Responses generated by Sprinter and SD

\ref{sec:add_results} Additional experimental results 


\subsection{Proof of Theorem \ref{the:SPRINTER} }
\label{sec:31proof}
\begingroup
\renewcommand{\thetheorem}{\ref{the:SPRINTER}} % Keep original numbering
\begin{theorem}
The probability of a token $x$ being chosen when running SPRINTER is given as
    \begin{align}
    \label{eq:SPRINT_DIST}
        p_\text{SPRINTER}(x) = (1-\eta_\text{FP})p(x) + \eta_\text{FP} q(x),
    \end{align}
    where $\eta_\text{FP}$ is the false positive rate of the verifier. Furthermore, the total-variation distance between the target and SPRINTER distributions is $d_\text{TV}(p,p_\text{SPRINTER}) = \eta_\text{FP}d_\text{TV}(p,q)$.
\end{theorem}
\endgroup

%\textbf{Sketch of proof:} A token generated by the draft model should either be accepted or rejected based on the relationship between $q(x)$ and $p(x)$. Based on this, we derive the probability of an acceptable/unacceptable token ultimately being sampled by SPRINTER based on the verifier's decision to either accept or reject that token. We also consider the scenario where a token from the draft model is rejected, leading to an acceptable token being sampled via the adjusted distribution \eqref{eq:resample}. We show that the probability distributions for an acceptable and unacceptable token result in the following identical form: $ (1-\eta_\text{FP})p(x) + \eta_\text{FP} q(x)$. The full proof can be found in the following.


\begin{proof}
    We denote $\eta_\text{FP}$ and $\eta_\text{FN}$ as the false positive and false negative rate of the verifier respectively. We define $x_\text{acc} = \{ x | \frac{q(x)}{p(x)}\leq 1 \}$ and $x_\text{rej} = \{ x | \frac{q(x)}{p(x)}>1 \}$ to represent the sets of tokens generated by the draft model that should be accepted and rejected by the verifier respectively.

    In general, we consider two cases: (a) the token should be rejected while it is accepted by verifier $V$, (b) the token should be accepted but it is rejected by $V$. Let us first consider the case when the token should be rejected (i.e. $x \in x_\text{rej}$). If $V(s(x,\text{prefix})) = 0$, meaning that the verifier predicts that ${\frac{q(x)}{p(x)}} > 1$, then we reject the token $x$ sampled from $q(x)$ with probability $1-\frac{p(x)}{q(x)}$ and re-sample $x$ from an adjusted distribution $\text{norm}(\max(0, p(x) - q(x)))$. Therefore, the probability that token $x$ is accepted is: $   \frac{p(x)}{q(x)}(1-\eta_\text{FP})q(x)$. If $V(s(x,\text{prefix})) =1$, meaning that the verifier predicts that  ${\frac{q(x)}{p(x)}} \leq 1$, we accept the token, which occurs with probability: $\eta_\text{FP}q(x)$. Therefore, the probability of a token $x \in x_\text{rej}$ being accepted under \textit{SPRINTER} is: $(1-\eta_\text{FP}) p(x) + \eta_\text{FP} q(x).$ 

    Now we consider the case when the token should be accepted (i.e. $x\in x_\text{acc}$) . If $V(s(x,\text{prefix})) = 0$, with probability $1-\eta_\text{TP}$, we reject the token $x$ and call the larger model $M_p$, which will verify that indeed $\frac{q(x)}{p(x)} \leq 1$. Therefore, though the verifier made a mistake, it can be corrected by $M_p$. The probability that token $x$ is accepted is: $q(x)(1-\eta_\text{TP})$. If $V(s(x,\text{prefix})) = 1$, we accept token $x$ with probability: $q(x)  \eta_\text{TP}$. 
    
    
    While the above scenarios relied directly on the decision of the verifier given an acceptable token, there is an additional scenario where a token $x \in x_\text{acc}$ is accepted despite not being initially sampled from $M_q$. Assume that a token sampled from the draft model should be rejected (i.e. $x \in x_\text{rej}$) and the verifier accurately predicts to invoke the target model. Under this event, it would be possible for a token that is acceptable to be re-sampled from the adjusted distribution.    
    The probability of token $x \in x_\text{acc}$ being accepted under this scenario is:
    \begin{align}
         &(1-\eta_\text{FP}) \left( \sum_{x_{\text{rej}}} q(x_\text{rej}) (1-\frac{p(x_\text{rej})}{q(x_\text{rej})}) \cdot \frac{p(x)-q(x)}{\sum_{x_\text{acc}} p(x_\text{acc}) - q(x_\text{acc})} \right)  \nonumber\\
         & = (1-\eta_\text{FP})  (p(x)-q(x))  \frac{\sum_{x_\text{rej}} p(x_\text{rej}) -q(x_\text{rej})}{\sum_{x_\text{acc}} p(x_\text{acc}) - q(x_\text{acc})}\nonumber\\
         & = (1-\eta_\text{FP}) (p(x)-q(x)). \label{eq: seccase_z0}
    \end{align}
    Overall, combining the above relationships, the probability of a token $x \in x_\text{acc}$ being accepted under \textit{SPRINTER} is:
    \begin{align}
         &=q(x) (1-\eta_\text{FN}+ \eta_\text{FN}) + (1-\eta_\text{FP})(p(x)-q(x)) \nonumber\\
         & = q(x) +(1-\eta_\text{FP}) (p(x)-q(x))  \nonumber\\
         & = (1-\eta_\text{FP}) p(x) + \eta_\text{FP} q(x). \label{eq: theorem1_side2}
    \end{align}
 The probabilities of sampling an acceptable ($x\in x_\text{acc}$) and unacceptable ($x\in x_\text{rej}$) token under \textit{SPRINTER} is identical, completing the proof of Theorem \ref{the:SPRINTER}.
    
 \noindent Furthermore, the total variation distance between $p_\text{SPRINTER}$ and $p$ can be calculated as follows:
 \begin{align}
 d_\text{TV}(p,p_\textit{SPRINTER}) &= \frac{1}{2}\sum_{x} |p(x) - p_\textit{SPRINTER}(x)| \nonumber\\
 &= \frac{1}{2}\sum_{x} |p(x) - (1-\eta_\text{FP}) p(x) - \eta_\text{FP}q(x)| \nonumber\\
  &= \frac{1}{2}\sum_{x} |\eta_\text{FP}p(x) -\eta_\text{FP}q(x)|\nonumber\\
    &= \eta_\text{FP} \frac{1}{2}\sum_{x} |p(x) -q(x)| = \eta_\text{FP} d_\text{TV}(p,q).
 \end{align}
\end{proof}

\subsection{Proof of Theorem \ref{the:SPRINTER_expected_tokens} }
\label{sec:expec_proof}
\begingroup
\renewcommand{\thetheorem}{\ref{the:SPRINTER_expected_tokens}} % Keep original numbering
\begin{theorem}
 The probability distribution of the number of generated tokens is given as: 
    \[
\mathbb{P}(N_\text{SPRINTER}=i) =
\begin{cases}
\eta_\text{TP}^{i} (1-\eta_\text{TP}) & i < r, \\
\eta_\text{TP}^r(\eta_\text{FP})^{i-r}(1-\eta_\text{FP}) & i\geq r.
\end{cases}
\]
The expected number of generated tokens is given as:
%The expected number of consecutive tokens accepted by the verifier prior to predicting that the target model should be invoked is given by $\mathbb{E}[\text{generated tokens}] = $:
    %The SPRINTER sampling method samples $q(x) \sim M_q$ which is used by $M_{\text{DRT}}$ to predict whether or not $\frac{q(x)}{p(x)} \leq 1$. If $\hat{Z}=1$, we accept the token and use $M_q$ to generate the next token. If $\hat{Z} = 0$, we reject the token and call the large model $M_p$. Overall, SPRINTER satisfies:
    \begin{align}
\label{eq:SPRINT_EXP_TOKEN}
        \mathbb{E}(N_\text{SPRINTER})=       \frac{\eta_\text{TP}-\eta_\text{TP}^r}{1-\eta_\text{TP}} + \frac{\eta_\text{TP}^r}{1-\eta_\text{FP}}.
        \end{align}
\end{theorem}
\endgroup


%\textbf{Sketch of Proof:} We first derive the expectation of the $r$ consecutive acceptable tokens as a function of $\eta_\text{TP}$, as this indicates whether the verifier decided that the current drafting round should stop prior to accepting all $r$ tokens. Conversely, the expectation of the subsequent unacceptable tokens is derived as a function of $\eta_\text{TP}$ and $\eta_\text{FP}$, as this characterizes how many low-quality tokens are still accepted by the verifier. Both expressions can be simplified to finite geometric series, which results in \eqref{eq:SPRINT_EXP_TOKEN}. The full proof can be found in the following.



\begin{proof}  We assume that the first $r$ tokens are acceptable, while subsequent tokens are unacceptable. This can be modeled as a finite geometric series for the accepted tokens and an infinite geometric series for the rejected ones. The expected number of generated tokens is derived as follows:
 \begin{align}
     \mathbb{E}[N_\textit{SPRINTER}] &=  \sum_{k=0}^\infty kP(N_\textit{SPRINTER}=k)\nonumber\\
     &= \underbrace{\sum_{k=0}^{r-1}kP(N_\textit{SPRINTER}=k)}_{\text{Term 1}} +\underbrace{\sum_{k=r}^{\infty}kP(N_\text{SPRINTER}=k)}_\text{Term 2}     
 \end{align}
 We first expand Term 1 as follows:
    \begin{align}     
          \text{Term 1} = \sum_{k=0}^{r-1}kP(N_\textit{SPRINTER}=k) &=  (1-\eta_\text{TP})\eta_\text{TP} + 2(1-\eta_\text{TP})\eta_\text{TP}^2 + \ldots + (r-1) (1-\eta_\text{TP})\eta_\text{TP}^{r-1}\nonumber\\
         &\stackrel{(a)}= (1-\eta_\text{TP})\sum_{k=0}^{r-1}k\eta_\text{TP}^k\nonumber
    \end{align}
    Note that (a) represents a finite geometric series, allowing us to apply the sum formula for such a series, resulting in:
    \begin{align}
        (1-\eta_\text{TP})\sum_{k=0}^{r-1}k\eta_\text{TP}^k& =  (1-\eta_\text{TP})\eta_\text{TP}\frac{1-r\eta_\text{TP}^{r-1}+(r-1)\eta_\text{TP}^{r}}{(1-\eta_\text{TP})^2} \nonumber\\
         & = \frac{\eta_\text{TP}-\eta_\text{TP}^{r}}{1-\eta_\text{TP}} -(r-1)\eta_\text{TP}^{r}
         \label{eq: theorem2_side1}
    \end{align}
         
Similarly, we simplify Term 2 as follows:
    \begin{align}  
  \text{Term 2} = \sum_{k=r}^{\infty}kP(N_\textit{SPRINTER}=k)
          &\stackrel{(a)}=  r\eta_\text{TP}^r(1-\eta_\text{FP}) + (r+1)\eta_\text{TP}^r \eta_\text{FP}(1-\eta_\text{FP})+ (r+2)\eta_\text{TP}^r \eta_\text{FP}^2(1-\eta_\text{FP}) + \ldots  \nonumber\\
         &\stackrel{(b)}= ar + a(r+1)\eta_\text{FP} +  a(r+2)\eta_\text{FP}^2 +\ldots \nonumber\\
         &= a\sum_{k=r}^{\infty} k\eta_\text{FP}^{k-r} \nonumber\\
         &= \frac{a}{\eta_\text{FP}^r}\sum_{k=r}^{\infty} k\eta_\text{FP}^{k} \nonumber\\
         &\stackrel{(c)}= \frac{a}{\eta_\text{FP}^r}\sum_{k=0}^{\infty}k\eta_\text{FP}^k - \frac{a}{\eta_\text{FP}^r} \sum_{k=0}^{r-1}k\eta_\text{FP}^k \label{eq: theorem2_side2_intermediate}
    \end{align}
    where (a) the sum starts from the $r^{th}$ token (i.e. the first unacceptable token), (b) follows from setting $a = \eta_\text{TP}^r(1-\eta_\text{FP})$. We also observe that (c) is a combination of two geometric series. Therefore, the first term in \eqref{eq: theorem2_side2_intermediate} can be simplified as:
    \begin{align}
        \frac{a}{\eta_\text{FP}^r}\sum_{k=0}^{\infty}k\eta_\text{FP}^k &= \frac{a}{\eta_\text{FP}^r} \frac{\eta_\text{FP}}{(1-\eta_\text{FP})^2} \nonumber\\
        & = \frac{\eta_\text{TP}^r}{\eta_\text{FP}^r} \frac{\eta_\text{FP}}{1-\eta_\text{FP}} \label{eq: theorem2_side2_intermediate_1} 
    \end{align}
    Similarly, we simplify the second term in \eqref{eq: theorem2_side2_intermediate} as:
    \begin{align}
        \frac{a}{\eta_\text{FP}^r} \sum_{k=0}^{r-1}k\eta_\text{FP}^k = \frac{\eta_\text{TP}^r}{\eta_\text{FP}^r} \left(\frac{\eta_\text{FP}-\eta_\text{FP}^{r}}{1-\eta_\text{FP}} -(r-1)\eta_\text{FP}^{r}\right) \label{eq: eq: theorem2_side2_intermediate_2}
    \end{align}
    Plugging \eqref{eq: theorem2_side2_intermediate_1} and \eqref{eq: eq: theorem2_side2_intermediate_2} into \eqref{eq: theorem2_side2_intermediate}, we have:
    \begin{align}
        \frac{a}{\eta_\text{FP}^r}\sum_{k=0}^{\infty}k\eta_\text{FP}^k - \frac{a}{\eta_\text{FP}^r} \sum_{k=0}^{r-1}k\eta_\text{FP}^k &= \frac{\eta_\text{TP}^r}{\eta_\text{FP}^r}\left(\frac{\eta_\text{FP}}{1-\eta_\text{FP}} - \frac{\eta_\text{FP}-\eta_\text{FP}^{r}}{1-\eta_\text{FP}} + (r-1)\eta_\text{FP}^{r}\right) \nonumber\\
        &= \frac{\eta_\text{TP}^r}{\eta_\text{FP}^r}\left( \frac{\eta_\text{FP}^{r}}{1-\eta_\text{FP}} + (r-1)\eta_\text{FP}^{r}\right)\nonumber\\
        &= \eta_\text{TP}^r \left( \frac{1}{1-\eta_\text{FP}} + (r-1)\right) \label{eq: theorem2_side2}
    \end{align}
    % $\eta_\text{FN} = 1- \eta_\text{TP}$.
    %      &\stackrel{(c)} = ar\frac{\eta_\text{FP} - \eta_\text{FP}^{m+1}}{1-\eta_\text{FP}} + a\eta_\text{FP}\frac{1-(m+1)\eta_\text{FP}^m+m\eta_\text{FP}^{m+1}}{(1-\eta_\text{FP})^2}\nonumber\\
    %      & = (1-\eta_\text{FN})^rr\eta_\text{FP}(1-\eta_\text{FP}^m) +(1-\eta_\text{FN})^r\eta_\text{FP}\frac{1-(m+1)\eta_\text{FP}^m+m\eta_\text{FP}^{m+1}}{1-\eta_\text{FP}}\nonumber\\ 
    %       &\stackrel{(d)}= \eta_\text{TP}^rr\eta_\text{FP}(1-\eta_\text{FP}^m) +(1-\eta_\text{FN})^r\eta_\text{FP}\frac{1-(m+1)\eta_\text{FP}^m+m\eta_\text{FP}^{m+1}}{1-\eta_\text{FP}}\nonumber\\ 
    %      
    
    Combining \eqref{eq: theorem2_side1} and \eqref{eq: theorem2_side2}, we obtain:
    \begin{align}
         \mathbb{E}(N_\textit{SPRINTER}) &= \frac{\eta_\text{TP}-\eta_\text{TP}^{r}}{1-\eta_\text{TP}} -(r-1)\eta_\text{TP}^{r} +  \eta_\text{TP}^r \left( \frac{1}{1-\eta_\text{FP}} + (r-1)\right) \nonumber\\
         &= \frac{\eta_\text{TP}-\eta_\text{TP}^r}{1-\eta_\text{TP}} + \frac{\eta_\text{TP}^r}{1-\eta_\text{FP}}.
    \end{align}
    The above expression gives Theorem \ref{the:SPRINTER_expected_tokens}.
\end{proof}
    \subsection{Proof of Theorem \ref{the:SPRINTER_expected_latency} }
\begingroup
\renewcommand{\thetheorem}{\ref{the:SPRINTER_expected_latency}} 
% Keep original numbering
\label{sec:lat_proof}
\begin{theorem}
 The expected stopping time is given as:
    %The SPRINTER sampling method samples $q(x) \sim M_q$ which is used by $M_{\text{DRT}}$ to predict whether or not $\frac{q(x)}{p(x)} \leq 1$. If $\hat{Z}=1$, we accept the token and use $M_q$ to generate the next token. If $\hat{Z} = 0$, we reject the token and call the large model $M_p$. Overall, 
    \begin{align}
        \label{eq:SPRINT_EXP_LATENCY}
        & \mathbb{E}[T_\text{Stop}] =  \frac{(1-\eta_{\text{TP}}^r) t_d}{1-\eta_\text{TP}} + \frac{\eta_\text{TP}^rt_d}{1-\eta_\text{FP}}.
        % &\mathbb{E}[\text{Latency}] = t_d \frac{\eta_{\text{TP}}}{1-\eta_{\text{TP}}}(1-(r+1)\eta_{\text{TP}}^r +r\eta_{\text{TP}}^{r+1} ) \nonumber\\
        % &+ (t_d+t_t)(1-\eta_{\text{TP}}^{r+1}) + \eta_{\text{TP}}^{r}((r+1+\eta_{\text{FP}})t_d+t_t)
    \end{align} 
\end{theorem}
\endgroup

    
  %  \textbf{Sketch of Proof:}We consider two scenarios: early stopping and late stopping. With respect to the early stopping, the expectation of latency can be derived as a function of $\eta_\text{TP}$, as the verifier accept the acceptable tokens until the r token. For the late stopping period, the expectation of latency is the function of both $\eta_\text{TP}$ ad $\eta_\text{FP}$ due to the verifier accepting the unacceptable tokens until the R token. We give the full proof in the following:
\begin{proof}
    Similar as above Theorem, we assume that the first $r$ tokens are acceptable, while all subsequent tokens are not. We also note that the verifier's runtime is negligible compared to that of the draft LLM \( M_q \). Therefore, we consider only \( t_d \) for the expected stopping time. We first reduce the expected stopping time as follows:
     \begin{align}
     \mathbb{E}[T_\text{Stop}] &=  \sum_{k=0}^\infty t_d(k+1)P(T_\text{Stop}=k)\nonumber\\
     &= \underbrace{\sum_{k=0}^{r-1}t_d(k+1)P(T_\text{Stop}=k)}_{\text{Term 1'}} +\underbrace{\sum_{k=r}^{\infty}t_d(k+1)P(T_\text{Stop}=k)}_\text{Term 2'}     
 \end{align}
    
    
    
    We now write out Term 1', representing the case that we stop before the $r$th acceptable token, as follows:
    \begin{align}
    \text{Term 1'} = \sum_{k=0}^{r-1} t_d(k+1)P(T_\text{Stop}=k) &= t_d(1-\eta_\text{TP}) + 2t_d(1-\eta_\text{TP})\eta_\text{TP} +  \ldots + rt_d (1-\eta_\text{TP})\eta_\text{TP}^{r-1}\nonumber\\
        &=\sum_{k=0}^{r-1}(1-\eta_\text{TP})\eta_\text{TP}^{k}(k+1)t_d \nonumber\\
        & =(1-\eta_\text{TP})t_d\sum_{k=0}^{r-1}k\eta_\text{TP}^{k} + (1-\eta_\text{TP})t_d\sum_{k=0}^{r-1}\eta_\text{TP}^{k}, \label{eq:T_stop_term1}
    \end{align}
    where \eqref{eq:T_stop_term1} is the combination of two geometric series. Therefore, \eqref{eq:T_stop_term1} can be simplified as follows:
    \begin{align}
        &(1-\eta_\text{TP})t_d\sum_{k=0}^{r-1}k\eta_\text{TP}^{k} + (1-\eta_\text{TP})t_d\sum_{k=0}^{r-1}\eta_\text{TP}^{k} \nonumber\\
        &= t_d \left(\frac{\eta_\text{TP}-\eta_\text{TP}^{r}}{1-\eta_\text{TP}} -(r-1)\eta_\text{TP}^{r}\right) + t_d(1-\eta_\text{TP}^r) \nonumber\\
        &=t_d\times\left(\frac{\eta_\text{TP}-\eta_\text{TP}^{r}}{1-\eta_\text{TP}}-r\eta_\text{TP}^{r}+1\right) \label{eq: theorem3_side1}
    \end{align}
    We next expand Term 2', which represents the case that we stop at or after the $r$th acceptable token, as follows:
    \begin{align}
        \text{Term 2'} = \sum_{k=r}^{\infty}t_d(k+1)P(T_\text{Stop}=k)&=(r+1)t_d\eta_\text{TP}^r(1-\eta_\text{FP}) + (r+2)t_d\eta_\text{TP}^r \eta_\text{FP}(1-\eta_\text{FP}) + \ldots  \nonumber\\
        &\stackrel{(a)}= b(r+1) + b(r+2)\eta_\text{FP} +  \ldots \nonumber\\
        &= b\sum_{k=r}^{\infty} k\eta_\text{FP}^{k-r} + b\sum_{k=r}^{\infty} \eta_\text{FP}^{k-r}\nonumber\\
         &= \frac{b}{\eta_\text{FP}^r}\sum_{k=r}^{\infty} k\eta_\text{FP}^{k}+ \frac{b}{\eta_\text{FP}^r}\sum_{k=r}^{\infty} \eta_\text{FP}^{k} \label{eq: theorem3_side2_intermediate}
    \end{align}
    where (a) follows from setting $b = \eta_\text{TP}^r(1-\eta_\text{FP})t_d$.
    Note that the first term in \eqref{eq: theorem3_side2_intermediate} can be written as:
    \begin{align}
        &\frac{b}{\eta_\text{FP}^r}\sum_{k=r}^{\infty}k\eta_\text{FP}^k  \nonumber\\
        &= \frac{b}{\eta_\text{FP}^r}\left(\sum_{k=0}^{\infty}k\eta_\text{FP}^k -\sum_{k=0}^{r-1}k\eta_\text{FP}^k \right) \nonumber\\
        & = \frac{\eta_\text{TP}^r}{\eta_\text{FP}^r} t_d\left(\frac{\eta_\text{FP}}{1-\eta_\text{FP}}-\frac{\eta_\text{FP}-\eta_\text{FP}^{r}}{1-\eta_\text{FP}} +(r-1)\eta_\text{FP}^{r}\right) \label{eq: theorem3_side2_intermediate_1}
    \end{align}
    Similarly, the second term in \eqref{eq: theorem3_side2_intermediate} can be expressed as:
    \begin{align}
        \frac{b}{\eta_\text{FP}^r}\sum_{k=r}^{\infty} \eta_\text{FP}^{k} &= \frac{b}{\eta_\text{FP}^r}\left(\sum_{k=0}^{\infty}\eta_\text{FP}^k -\sum_{k=0}^{r-1}\eta_\text{FP}^k\right)  \nonumber\\
        &=\frac{\eta_\text{TP}^r}{\eta_\text{FP}^r} t_d \left(1- (1-\eta_\text{FP}^r)\right) \label{eq: theorem3_side2_intermediate_2}
    \end{align}
    Plugging \eqref{eq: theorem3_side2_intermediate_1} and \eqref{eq: theorem3_side2_intermediate_2} into \eqref{eq: theorem3_side2_intermediate}, we obtain the expected stopping time after or at $r$th acceptable token is:
    \begin{align}
        \frac{\eta_\text{TP}^r}{1-\eta_\text{FP}}t_d+rt_d\eta_\text{TP}^r \label{theorem3_side2}
    \end{align}
    Combining \eqref{eq: theorem3_side1} and \eqref{theorem3_side2}, the final expression of the expected stopping time is given as:
    \begin{align}
        \mathbb{E}[T_\text{Stop}] =  \frac{(1-\eta_{\text{TP}}^r) t_d}{1-\eta_\text{TP}} + \frac{\eta_\text{TP}^rt_d}{1-\eta_\text{FP}}.
    \end{align}
    This completes the proof.
\end{proof}
\subsection{Additional theoretical results}
\label{sec:Acc_rate}
In this section, building on previous work by \cite{leviathan2023fast}, we examine the acceptance rate of a single run of \textit{SPRINTER}. This analysis provides insight into how the verifiers' statistical properties influence the acceptance rate.

\noindent \textbf{Calculation of }$\alpha_{\textit{SPRINTER}}$ \textbf{and} $\beta_{\textit{SPRINTER}}$. It is well known \cite{leviathan2023fast} that the acceptance rate of standard speculative decoding $\beta$ is given as $\beta = 1-d_{TV}(p,q)$, where $d_{TV}(p,q)$ is the total variation distance  between the distributions $p$ and $q$. In the next Theorem, we derive an analogous result showing the acceptance rate of tokens generated by $M_q$ when using \textit{SPRINTER}.
\begin{theorem}\label{the: acceptance_rate}
    The acceptance rate of \textit{SPRINTER} $\beta_{\textit{SPRINTER}}$ is 
    \begin{align}
        1-(1-\eta_\text{FP})\cdot d_{TV}(p,q).
    \end{align}
\end{theorem}

\begin{proof}
From the \textit{SPRINTER} sampling procedure, we know that $\beta_{\textit{SPRINTER}}$ satisfies the following:
\begin{align}
    \beta_{\textit{SPRINTER}} = \mathbb{E}_{x\sim q(x)}
    \begin{cases} 
1 & \text{if } q(x) \leq p(x) \\
\eta_\text{FP}+\frac{p(x)}{q(x)} (1-\eta_\text{FP})& \text{if } q(x) > p(x).
\end{cases} 
\end{align}
The above expectation can be simplified and computed explicitly as follows:
\begin{align}
   \beta_{\textit{SPRINTER}} & = \sum_{x} \min(q(x), \eta_\text{FP} q(x) + (1-\eta_\text{FP})p(x))\nonumber\\
    &=\sum_{x} \min(\eta_\text{FP} q(x)+(1-\eta_\text{FP})q(x), \eta_\text{FP} q(x) + (1-\eta_\text{FP})p(x))\nonumber\\
    &=\sum_{x} (1-\eta_\text{FP})\min(q(x), p(x))+ \eta_\text{FP} q(x)\nonumber\\
    & = (1-\eta_\text{FP}) \cdot(1-d_{TV}(p,q)) + \eta_\text{FP}= 1-(1-\eta_\text{FP})\cdot d_{TV}(p,q).
\end{align}
\end{proof}
\begin{remark}
    Note that when $\eta_\text{FP} = 0$, the acceptance rate is the same as that of speculative decoding.
\end{remark}
\begin{remark}
    When $\eta_\text{FP} \neq 0$,  $\beta_{\textit{SPRINTER}} \geq \beta$ meaning that \textit{SPRINTER} has a higher acceptance rate compared to that of standard speculative decoding. The inherent reason is that the classifier is susceptible to predicting that the token is acceptable when it actually should be rejected, causing a higher acceptance rate and subsequently enabling the draft model to generate more tokens. However, $\eta_\text{FP}$ also reflects the distance of the probability distribution of \textit{SPRINTER} from the probability distribution of the target model $p$. This means that while more tokens are being accepted, the distribution they are sampled from may deviate from the distribution of $M_p$, causing poorer quality tokens to be generated. Therefore, $\eta_\text{FP}$ measures the tradeoff between the accuracy of the tokens generated and the latency incurred from using \textit{SPRINTER}.
\end{remark}
% \begin{corollary}
%     The expectation of $\beta_{\text{SPR}}$ is $\alpha_{\text{SPR}}$, where 
%     \begin{align}
%         \alpha_{\text{SPR}} = (1-\eta_\text{FP})\mathbb{E}(\min(q(x), p(x))) + \eta_\text{FP}.
%     \end{align}
% \end{corollary}

\subsection{Full \textit{SPRINTER} Algorithm}
\label{sec:algorithm_hyper}
Algorithm \ref{alg:SPRINTER} provides the detailed overview of Algorithm stated in the main paper, detailing the procedure of a full step of \textit{SPRINTER}. 
 \begin{algorithm}[h]
   \caption{\textit{SPRINTER} (detailed)}
   \label{alg:SPRINTER}
\begin{algorithmic}
   \STATE {\bfseries Input:} $M_p, M_q, V, \text{Prefix}, \text{Prediction Threshold}~\tau$
   % \REPEAT
   \STATE $x = \emptyset$
   \STATE \textcolor{blue}{Sample tokens from $M_q$ while verifier predicts $\frac{q(x)}{p(x)}\leq 1$ }
   \STATE $r = 1$
   \WHILE{True}
      \STATE Prefix = Prefix + $x$
      \STATE $q_r(x) \sim M_q(\text{Prefix})$
    \STATE $x \sim q_r(x)$
    \IF{$V (s(x, \text{Prefix})) \geq \tau$ [Accept Token]}
    \STATE $r += 1$
    \ELSE
    \STATE Break
    \ENDIF
    \ENDWHILE
   \STATE \textcolor{blue}{Run $M_p$ Once}.
   \STATE $p_{r+1}(x) \sim M_p(\text{Prefix}+x_{r})$
   \STATE $p'(x) \leftarrow p_{r+1}(x)$
   \STATE $c \sim U(0,1)$
   \STATE $n = \min(1, \frac{p_{r+1}(x)}{q_{r+1}(x)})$
   % \STATE $p'(x) = p_{\tilde \gamma+1}(x)$
   \IF{$c > n$}
   \STATE $p'(x) \sim \text{norm}(\max(0, p_{r+1}(x)- q_{r+1}(x))$
   \ENDIF
  \STATE $t \sim p'(x)$
  \STATE \textbf{Return} \text{Prefix} + $t$ 
\end{algorithmic}
\end{algorithm}


\textbf{More Training Details Regarding the Verifier and Hyperparameter Tuning}
During the training process of the verifier, we vary the value of \(\lambda\) among 1, 1.2, and 1.5 to generate the ground truth regarding whether a token should be accepted or rejected. By increasing $\lambda$, we bias the verifier to accept more draft tokens, potentially increasing $\eta_\text{FP}$. Experimental results show that \textit{SPRINTER} with \(\lambda = 1.2\) performs best on the LM1B dataset for both GPT-Neo and GPT2 model pairs. On the Wiki-Summary dataset, \textit{SPRINTER} with \(\lambda = 1\)  and \(\lambda = 1.2\) achieves superior results using the GPT2 and GPT-Neo model pairs respectively.  
%while SPRINTER with \(\lambda = 1\) achieves superior results on the Wiki-Summary dataset with GPT2 model pair and \(\lambda = 1.2\) achieves better on GPT-Neo model pair. 
For the verifier's prediction threshold \(\tau\), we observe that \(\tau = 0.5\) is sufficient to achieve strong performance. 

We split the dataset into train/test sets, using early stopping to mitigate overfitting. The input dimension of the verifier depends on the dimension of the assumed draft model. As GPT-Neo-125M has an output dimension of $768$, for example, the verifier used would also have a dimension of 768 neurons. 



\subsection{Detailed Discussion of Related Works}
\label{sec:relatedworks_append}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{literature_table.pdf}
    \caption{Comparison of different speculative decoding based mechanisms with respect to three aspects: a) Approximate or Exact match with the larger LLM (target) probability distribution, b) Quality Analysis of Completions and c) Theoretical Results. \textit{SPRINTER} is the first framework to provide in-depth analysis of approximate verification; it's impact on the quality-vs-latency tradeoff and provide insights on how to navigate this tradeoff to achieve latency reduction without significantly degrading quality of generated tokens.}
    \label{fig:literature_table_appendix}
\end{figure*}

Multiple approaches for optimizing SD have been presented in the literature. SpecTr \cite{sun2024spectr} uses optimal transport to effectively manage the draft selection process, ensuring efficient and accurate sampling from large models. Yin et al. \cite{yin2024theoretical} frame the decoding problem through a Markov chain abstraction and analyzing its key propertiesoutput quality and inference accelerationfrom a theoretical perspective. Recently, Judge Decoding \cite{anonymous2025judge} uses a trained linear head to judge token validity/quality beyond mere alignment, significantly speeding up inference while maintaining output quality. Under this method, tokens that may not necessarily match the target distribution may still be accepted as they are still of high-quality. Similarly, \cite{jang2024lanternacceleratingvisualautoregressive} presents a variant of speculative decoding for visual autoregressive models by relaxing the constraint that tokens must match the target distribution while proposing a mechanism based on total variation distance to ensure that the distribution drift between the generated tokens and the target model does not exceed a certain threshold. 

There are also works that have similarly used an additional classifier for determining how many tokens the draft model should generate. \cite{mamou2024dynamicspeculationlookaheadaccelerates} proposed using a two layer feedforward network to predict when the draft model should stop generating tokens and initiate the target model's verification procedure. \cite{huang2024specdec++} models the SD procedure as a Markov Decision Process and uses the draft model with an added head to predict if the draft model should stop generating tokens. However, both methods ensure that sampling the draft model with their method is equivalent to sampling the target model, which hinders improved latency reductions that can be attained if this requirement is relaxed.  \cite{agrawal2024adaedl} proposes prove that a function of the entropy of the distribution of the draft model can be used as a means of indicating when to end a round of drafting tokens. Specifically, the output of this function is compared against a threshold, which is also adjusted dynamically based on the current acceptance rate, to determine if the current drafting round should end. \cite{kim2024speculative} provides two heuristics for determining when the target model should take control in generating tokens. The first involves observing the draft model's distribution to see if it has a certain lack of confidence in its current token, which would indicate that the target model should generate a replacement token. The second takes place during verification, when the distributions of the target model and draft model are compared to observe an instant in which the draft model is too confident in its decision. This indicates that the sequence should revert back to this point, with the target model generating a replacement token. A method for fine-tuning the draft model to generate tokens that better align with the target model is also presented. 


\cite{lu2024improving} investigated using different classifier architectures for halting the drafting process for multi-candidate speculative decoding. MEDUSA \cite{cai2024medusa} propose a framework to accelerate inference in large language models (LLMs) by employing multiple decoding heads. By introducing parallelism in the decoding process, MEDUSA aims to improve the efficiency of LLM operations significantly.
EAGLE \cite{li2024eagle} presents a speculative sampling framework that improves large language model (LLM) inference by predicting second-to-top-layer features based on a token sequence advanced by one time step.

\subsection{Additional Examples of Responses generated by Sprinter and SD}
\label{sec:prompts_ex}
In this section, we feed additional prefixes to \textit{SPRINTER} and SD and show their respective generated responses in Figure \ref{fig:Prompt_example_appendix}. We observe that \textit{SPRINTER} can achieve a relatively good performance compared with SD, but can sometimes generate incorrect information. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{Prompt_example_appendix.pdf}
    \caption{Additional responses generated by \textit{SPRINTER} and SD when fed the same prefixes.}
    \label{fig:Prompt_example_appendix}
\end{figure}

\subsection{Flops Calculation}
\label{sec:flops_append}
We adopt the methods used in \cite{hoffmann2022trainingcomputeoptimallargelanguage,casson2023transformerflops} to determine the number of floating point operations (FLOPS) performed in a forward pass of the draft and target models used in this work. The main sources of FLOPS considered in \cite{hoffmann2022trainingcomputeoptimallargelanguage,casson2023transformerflops} are due to the embedding matrices, self-attention operations in the transformer blocks of the LLM, the feedforward networks in each transfomer block, and the operations required to generate the final logits. Table \ref{tab:Flops} presents the number of FLOPS needed for each draft and target model to generate 20 tokens. The table shows that GPT-Neo-1.3B and GPT2-XL require roughly 8 and 10 times the number of FLOPS compared to GPT-Neo-125M and GPT2-Small respectively. 

\begin{table}[t]
\centering
\scalebox{1}{
\begin{tabular}{|c|c|}
\hline
Model & FLOPS for 20 tokens \\
\hline
GPT-Neo-125M & $8.01B$ \\
\hline
GPT-Neo-1.3B & $64.66B$\\
\hline
GPT2-Small & $7.25B$ \\
\hline
GPT2-XL& $71.78B$ \\
\hline

\end{tabular}}
\caption{Estimations of the number of FLOPS needed for each draft and target model to generate 20 tokens. }
\label{tab:Flops}
\end{table}

Recall from Section \ref{sec:theory}, that if $\gamma$ consecutive tokens are generated by the draft model, our verifier is of a lower complexity to the draft model, and, as evidenced by Table \ref{tab:Flops}, that the number of FLOPS used by the target model are significantly greater than the FLOPS used by the target model, then the computational savings experienced under \textit{SPRINTER} is $(\gamma-1)F_t$ where $F_t$ denotes the number of FLOPS of the target model. This further shows that \textit{SPRINTER} is significantly less computationally expensive compared to SD, which relies on the target model for parallel verification every $\gamma$ tokens. Figure \ref{fig:quality_latency_comput_appendix} shows the quality-latency-computational profile experienced by \textit{SPRINTER} and SD assuming the GPT2-Small/GPT2-XL model pair. Similar to Figure \ref{fig:frameworkSPrinter}, which shows the same profile for the GPT-Neo model pair, we observe that \textit{SPRINTER} is able to incur a lower latency than SD while suffering a minimal dip in quality and incurring more computational savings by running inference on the draft model more frequently than the target model. 

 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\linewidth]{quality_latency_comput_appendix.pdf}
    \caption{Comparison between \textit{SPRINTER} and SD with respect to \textit{Quality} (ROUGE score), \textit{Latency} (time in ms required to generate a sentence) and \textit{Computation} (number of flops required to generate $20$ consecutive acceptable tokens from the draft model) for GPT2-Small and GPT2-XL.}
    \label{fig:quality_latency_comput_appendix}
\end{figure}

%We further provide results of the FLOPs score in the GPT2-Small/GPT2-XL model as shown in Fig. \ref{fig:quality_latency_comput_appendix}.

\subsection{Additional Experimental Results}
\label{sec:add_results}
In this section, we present additional experimental results on the verifier's hyperparameter tuning. Specifically, we show the ROC curve for the Wiki-Summary dataset using the GPT-Neo-125M / GPT-Neo-1.3B model pair as shown in Figure \ref{fig:ROC_wiki_neo} and Table \ref{tab:my_label}. An interesting phenomenon emerges: as we increase the verifier decision threshold 
 $\lambda$, the area under the curve improves, reaching its optimal performance at $\lambda = 1.2$. The optimal balance at $\lambda = 1.2$ suggests that this threshold best separates acceptable from unacceptable tokens.


 
\begin{figure}[h!]
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        % Insert your table here
            \begin{tabular}{c|c|c}
         & Wiki-Summary & LM1B \\
    \hline
         GPT-Neo pair & ($\lambda, \tau$) = (1.2,0.5) & ($\lambda, \tau$) = (1.2,0.5) \\
    \hline
         GPT2 pair & ($\lambda, \tau$) = (1,0.5) & ($\lambda, \tau$) = (1.2,0.5) \\
    \label{tab:my_label}
    \end{tabular}
  
        \caption{Comparison of hyperparameters across datasets and model pairs}
    \end{minipage}\hfill
    \begin{minipage}{0.44\textwidth}
        \centering
        % Insert your figure here
        \includegraphics[width=0.9\linewidth]{overall_roc_wiki.png} % Replace with your image file
        \caption{Overall ROC curve in Wiki-Summary dataset}
        \label{fig:ROC_wiki_neo}
    \end{minipage}
\end{figure}

\end{appendices}



\end{document}