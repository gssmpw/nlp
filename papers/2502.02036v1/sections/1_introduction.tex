The convergence of Artificial Intelligence(AI) and robotic systems, has revolutionized a wide range of domains such as agriculture, healthcare medicine, warehousing, and manufacturing. In recent years, generative AI models for image and language generation (Large Language Models, LLMs) have proliferated, such as for image generation and natural language generation, for example ChatGPT, Midjourney, and Dall-E, to name a few. The combination of generative models and robots has also started to receive considerable attention. One aspect that has impeded its progress is that generative models are data-driven approaches. At the onset of LLMs, and image generation, copious available images and text on the Internet were mined for building massive training datasets. Training generative models for robotic applications requires a substantial amount of domain- and task-specific data, which, on the contrary, does not exist~\cite{hamalainen_affordance_2019}. There is no protocol or standard for a taxonomy of robotic trajectory and task data for generating robotic actions for new operative scenarios. This is because the application scenarios are constrained. In theory, teleoperation technology can address these limitations by allowing the human operator to simultaneously supply new training exemplars to teach the robot to perform unforeseen tasks and to augment training sets for new task scenarios. 
% lowering the barriers and providing more data collection opportunities.  

Manipulator teleoperation systems enable remote interaction with environments and can scale human motion to achieve larger or smaller action capabilities. These systems aim to accurately translate human decision-making and actions while ensuring the robust operation of the teleoperation system~\cite{hirche_human-oriented_2012}. A common solution is for the operator to control the real-time end-effector's position and orientation with external devices with a $6$ degrees of freedom (DOF) robot with the joint trajectory space calculated through standard inverse kinematics (IK). For example, an operator can use a haptic device to control the end-effector 6-DOF representation to achieve high-frequency teleoperation for industrial manipulators in workplace and factory settings~\cite{dekker_design_2023}. Another approach instead uses an external RGB and RGBD (depth) camera to estimate the operator's $6$-DOF hand pose~\cite{schroder_real-time_2012,antotsiou_task-oriented_2018} for teleoperating the robot end-effector ~\cite{ajili_gesture_2017,qin_anyteleop_2024}. Furthermore, as the virtual reality (VR) hardware market expands, inertial measurement unit (IMU) sensors and VR controllers are becoming a commonly used approach to estimate the operator's $6$-DOF hand pose for integrating $6$-DOF robot teleoperation and control ~\cite{lipton_baxters_2018,weigend_anytime_2023}.
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.75\linewidth]{images/joint_mapping.jpg}
    \captionsetup{font=footnotesize}
    \caption{\textbf{Orange} labels, $\mathbf{\textit{J}_1\textit{-}\textit{J}_7}$, indicate the positions of the {K}inova manipulator's joints. \textbf{Green} labels, $\mathbf{\textit{q}_1\textit{-}\textit{q}_7}$, indicate the positions of the human arm's joints. \textbf{Arrow} shows the mapping relationship between the manipulator's joint and the kinematic chain of a human upper limb.}
    \label{fig:jointMapping}
    \vspace{-25pt}
\end{figure}
These approaches are less effective when applied to redundant DOF manipulators with more than six degrees of freedom. Redundant DOF manipulators afford multiple solutions in joint configuration space to achieve the same end-effector pose. Although this kinematic redundancy provides greater flexibility within complex and dynamic environments, it also increases the complexity of teleoperation. To address this problem and provide an intuitive human-robot interface, we propose a novel, computationally efficient, machine learning-based approach to determine plausible robot joint and manipulator trajectories that mimic human kinematic motion behaviors, making it an ideal interface for co-robotics task scenarios. The approach embodies a training-model learning framework that can reduce or eliminate the typically highly technical and labor intensive requirement to re-program such robots to execute new tasks and operate in new environments. 

Specifically, we present a Gate Recurrent Unit based Variational Autoencoder (GRU-based VAE) architecture finding the latent representation of the redundant DOF robot manipulator configuration space. A feed-forward neural network converts human arm gestures into the latent distribution space and generates the corresponding robot manipulator configuration trajectory by trained VAE decoder in real-time. In our approach, the resulting human-robot action coupling during tasks can be used to further create new imitation learning samples. The simulation based model pipeline created to train the model can also be easily adapted to train the model for other robot configurations. 

%Contribution

%1. A GRU-based VAE model combined with a feed-forward neural network architecture, which efficiently converts human arm gestures into the redundant DOF robot manipulator configuration space in real time.

%2. We present a system that uses the converted configuration as a trajectory and applies it to the manipulator to achieve real-time teleoperation.

%3. A low-cost data collection pipeline based on simulation for the manipulator that can be easily adapted to various manipulators. The converted configuration during the execution of teleoperation can used for imitation learning samples.

%4. Provide an efficient approach to finding a configuration solution for a redundant manipulator based on Machine learning.

%- for imitation learning sample.


%- for take out of programming for teaching robot to do certain task or job.

%- redundant manipulator

%- remote control.

%- cobot, human robot interaction environment.
