
% In this work, we propose a novel method that utilizes VAE to find the latent distribution space that can represent complex joints movement for $7$-DOF Kinova manipulator. Moreover, taking advantage of finding probability distribution, we can use limited training data to approximate the comprehensive Kinova configuration space. In addition, we train a separate feed-forward neural network to map the human arm kinematics into the learned latent space, using the VAE decoder to generate corresponding manipulator joint configurations. Hence, we construct the Kinova trajectory dataset and human arm joints configuration dataset correspondingly for the VAE and the feed-forward model. The following section provides a detailed discussion of the data collection process, model architecture, and the overall workflow.


In this work, we propose a novel method that utilizes a VAE neural network architecture to learn a latent distribution space that can represent complex joint movements for a 7-DOF Kinova manipulator. By leveraging this probabilistic distribution, we can approximate the Kinova configuration space with limited training data. We also train a feed-forward neural network to map human arm kinematics to the learned latent space and use the VAE decoder to generate corresponding manipulator joint configurations. This approach involves creating datasets for both Kinova trajectories and human arm configurations. The following section details the data collection, model architecture, and overall workflow.

\vspace{-2.2mm}

\subsection{Data Collection}
\vspace{-1mm}
\subsubsection{\textbf{\textsc{Kinova trajectory dataset collection}}}\label{kinova_data}

We randomly select $500$ start and end \textit{x}, \textit{y}, \textit{z} end-effector positions in Cartesian space, and assign two corresponding random sets of valid Roll($\phi$), Pitch($\theta$), Yaw($\psi$) orientation for each start and end position, i.e.,  $\mathbf{p} = [\textit{x}, \textit{y}, \textit{z}, \phi, \theta,\psi]^\top, \mathbf{p} \in \mathbb{R}^{6}$. Inspired by \cite{hamalainen_affordance_2019}, the MoveIt software is used to generate corresponding trajectories, including joint angles under varied time steps for every combination of initial start and end poses. Note that a jump from $-180$ degrees to $179$ creates a discontinuity singularity of the collected joint angular data that can confound training the model ~\cite{weigend_anytime_2023}. 
%A jump from $-180$ degrees to $179$ degrees can confound training the model. 
To mitigate such singularities, each manipulator's angle $\mathbf{\textit{d}}$ is converted to projected unit values $\mathbf{(\textit{cos(d)}, \textit{sin(d)})}$, which therefore results in $14$ values to represent the $7$-DOF manipulator. We use the Cubic Spline interpolation algorithm to appoint each trajectory time step into $0.1$s ($10$Hz) intervals. Finally, we decompose each trajectory as a sequence of $2$ time-step segments that represent the current and the next time-step trajectory features.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.7\linewidth]{images/joint_mapping.jpg}
%     \captionsetup{font=footnotesize}
%     \caption{\textbf{Orange} labels, $\mathbf{\textit{J}_1,...,_7}$, indicate the positions of the Kinova manipulator's joints. \textbf{Green} labels, $\mathbf{\textit{q}_1,...,_7}$, indicate the positions of the human arm's joints. \textbf{Arrow} shows the mapping relationship between the manipulator's joint and the kinematic chain of a human upper limb.}
%     \label{fig:jointMapping}
%     \vspace{-20pt}
% \end{figure}
\subsubsection{\textbf{\textsc{Kinematic mapping}}}
To achieve intuitive and efficient manipulator teleoperation with the human arm, a kinematic correlation between the manipulator and the human arm was first defined before the model training and data collection.  We collect the operator's right-arm-related data for training. As shown in Fig.~\ref{fig:jointMapping}, $\mathbf{\textit{J}_1,_2}$ are mapped to the shoulder joint $\mathbf{\textit{q}_1,_2}$, $\mathbf{\textit{J}_3}$ is mapped to the upper arm joint $\mathbf{\textit{q}_3}$, $\mathbf{\textit{J}_4}$ is mapped to the elbow $\mathbf{\textit{q}_4}$, and $\mathbf{\textit{J}_5}$ is mapped to the joint rotation of the forearm arm. However, $\mathbf{\textit{J}_5,_6,_7}$ is also considered a universal joint representing the human wrist's spherical joint. Therefore, Joint $\mathbf{\textit{J}_5}$ overlaps between the representation of forearm arm and wrist rotation. When $\mathbf{\textit{J}_5,_7}$ represents a flexion or extension action of the wrist, $\mathbf{\textit{J}_5}$ should rotate conditionally to allow the direction of the rotation axis of $\mathbf{\textit{J}_6}$ to match the rotation axis ($\mathbf{\textit{q}_7}$) of the wrist's flexion or extension action, and $\mathbf{\textit{J}_7}$ will rotate oppositely to counteract the unwanted rotation from $\mathbf{\textit{J}_5}$ to keep the facing direction for the back of human hand and the end-effector consistent. On the other hand, if $\mathbf{\textit{J}_5,_7}$ represents either an ulnar or radial deviation ($\mathbf{\textit{q}_6}$), $\mathbf{\textit{J}_5}$ will only repose to the forearm arm rotation ($\mathbf{\textit{q}_5}$), $\mathbf{\textit{J}_7}$ should not rotate. In the end, $\mathbf{\textit{J}_7}$ is not mapped to any human joint but is only used to counteract the rotation from $\mathbf{\textit{J}_5}$. 
% The mapping discussed above is only reflected in the dataset, which means we only provide pairs of the configuration data of the Kinova arm and human arm to the model, forcing it to learn the mapping skill during the training. 

\subsubsection{\textbf{\textsc{Human arm joints configuration dataset collection}}}\label{awinda_data}
Once the VAE model is settled, we can construct the human arm joint configuration dataset using the XSens Awinda human skeletal motion tracking system. Wireless $6$-DOF IMU sensors were attached to 11 upper human body segments (sternum, pelvis, head, L/R hands, L/R forearms, L/R upper arms, and L/R scapular skeletons joints) to approximate complete upper body skeletal motion. Table~\ref{tab:data-table} shows the details for the data of joints collected for the manipulator and the participant's right arm, including the joint's name for both and the joint's range only for part one data of the manipulator. An upper body calibration procedure using proprietary software was required to allow the IMU sensors to correctly track the full upper-body human kinematic motion~\cite{schepers_xsens_2018}. Although optoelectronic target point rigid body tracking systems have been typically used for Telerobotics research due to their high precision and accuracy, the inertial measurement sensor based systems can be deployed in a myriad of more diverse environments containing complex spatial constraints and occlusions. The premise of our approach, which is thus far supported by experiments described herein, is that the measured human kinematics only require to be \emph{repeatable} since the human can adjust their visuo-proprioceptive queues according to context and nature of the task to be completed. 

\begin{table}[h!]
\scriptsize % Reduce font size
\centering
\begin{tabular}{|c|c|l|}
\hline
\textbf{Categories} & \multicolumn{2}{c|}{\textbf{Collected Features}} \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Kinova Joints\\ Angle (Degree)\end{tabular}} &
  \multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Joint 1 (-15°$\sim$90°), Joint 2 (60°$\sim$120°), \\ Joint 3 (-15°$\sim$90°), Joint 4 (-120°$\sim$-60°), \\ Joint 5 (-90°$\sim$180°), Joint 6 (-90°$\sim$30°), \\ Joint 7 (0°$\sim$-90°)\end{tabular}} \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Awinda Joints\\ Angle (Degree)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}T4-Shoulder,\\ Shoulder\end{tabular}} & \begin{tabular}[c]{@{}l@{}}1. Abduction/Adduction \\ 2. Internal/External Rotation \\ 3. Flexion/Extension\end{tabular} \\ \cline{2-3} 
 & \textbf{Elbow, Wrist} & \begin{tabular}[c]{@{}l@{}}1. Ulnar Deviation/Radial \\ Deviation \\ 2. Pronation/Supination \\ 3. Flexion/Extension\end{tabular} \\ \hline
\end{tabular}
\captionsetup{font=footnotesize}
\caption{The data collection participant is strongly right-handed. Hence, only right-arm data features are used for training.}
\label{tab:data-table}
\vspace{-5pt}
\vspace{-1.5mm}
\end{table}

%Once the collaboration success and the motion capture effect is satisfied for the end-user (who is wearing the sensors), we can start the human arm joints configuration dataset collection. 
The human arm joint configuration dataset has a total of $86$ actions and consists of two parts. The first part contains $74$ actions, in which only a single joint participates in each action from the operator's perspective, for example, shoulder abduction or adduction, wrist pronation or supination, and so on. For the second part, multiple joint movements are used in each action. From a randomly sampled large group of start and end poses, only a small subset is feasible to physically mimic (e.g., imitate) the human operator's right arm. For the initial training of the DNN, a feasible training set of $12$ robot start and end poses and the corresponding human operator arm trajectories were collected. Then, we execute each action in a real manipulator to collect real-time joint angle data. The collected raw joint angle data from the manipulator and the human joint angle data, which will be described below, are both then converted to projected unit values as described in~\ref{kinova_data}. Then, the processed angle dataset is put into the VAE encoder to generate a sequence of latent representations of each action. In parallel, the human operator observes the kinematic motion of the robot manipulator and attempts to mimic the motion with corresponding joints as described in Fig.~\ref{fig:jointMapping}. This process facilitates proprioceptive neuromuscular adaptation, commonly referred to as 'muscle memory'. Once the operator feels adequately familiar with the demonstrated actions, they proceed to use their right arm to mimic each movement, recording the entire process to capture the joint angles of the operator's arm. It is important to note that the absolute positions of the manipulator's end-effector and the human hand may differ due to the disparity in arm lengths. Since the action completion time and the data reception rate between the manipulator and human arm cannot be synchronized, the manipulator data was temporarily matched to the closest time steps of the human operator arm joint data through the Cubic Spline interpolation algorithm and then placed together as a time synchronized pair. In total, we created $15,043$  pairs of human gestural arm configurations and corresponding latent representations of robot manipulator configurations with a selected frequency of 40 Hz (0.025 second intervals). Note that only one participant was involved in the human data collection process. The participant is $170$ \textit{cm} tall and of average body shape.
\vspace{-2.2mm}
\subsection{Generative Models}
 \begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/vae.jpg}
    \captionsetup{font=footnotesize}
    \caption{The GRU-based VAE takes a 2-time-step manipulator joint angle position trajectory as input. It learns an approximate latent distribution by sampling latent features using the reparameterization trick, which is then passed to the decoder to reconstruct the input trajectory. The learned latent distribution space enables the approximation of the entire manipulator configuration space.}
    \label{fig:vae}
    \vspace{-20pt}
\end{figure}
\vspace{-2.2mm}

\subsubsection{\textbf{\textsc{GRU-based Variational Autoencoder}}} We utilize a latent variable model, specifically a Variational Autoencoder (VAE), to approximate and predict the configuration space of the manipulator within the continuous probability latent space. We use $2$ time-steps of joint position trajectory as the input to the VAE. The first time-step represents the manipulator's current joint positions, while the second time-step represents the next joint positions, $100$ milliseconds later. As discussed in the related work, the encoder and decoder are crucial for approximating a latent distribution $\mathcal{Q}({z}\mid{x})\text{ and likelihood distribution }\mathcal{P}({x}\mid{z})$.
 To effectively capture the time-sequence information and enhance prediction accuracy, we utilized the GRU recurrent-based neural network architecture, for both the encoder and decoder in the VAE, as shown in Fig.~\ref{fig:vae}. The encoder takes the Kinova robot trajectory as input feed into the GRU layer. Then, the selected features are passed to two separate single-layer neural networks that generate the mean $\vec{\mu}$ and log-variance $\vec{\sigma}$ to represent a finite number $z$ of Gaussian distributions to approximate the true latent space distribution, where the size of $z$ is a hyper-parameter. With the reparameterization trick equation~\ref{reparameterization} where noise \( \epsilon \sim \mathcal{N}(0, I) \), we can sample the latent feature $\vec{Z}$ for data reconstruction with the decoder and calculating a gradient and optimize the distribution when doing backpropagation during training. 
\vspace{-1.9mm} 
 \begin{equation}\label{reparameterization}
\vec{Z} = \vec{\mu} + \vec{\sigma} \cdot \vec{\epsilon}
\vspace{-15pt}
\end{equation}
\vspace{-1.9mm} 

To reconstruct the trajectory using the latent feature $\vec{Z}$ with the decoder,  $\vec{Z}$ must be repeated to match the length of the target trajectory. Specifically, the input to the decoder is a matrix where $\vec{Z}$ is repeated twice.


The loss function for the VAE model has two components: (1) the trajectory reconstruction loss and (2) the KL-divergence, which are represented by~\ref{vae_loss}.
\vspace{-6pt}
\begin{equation}\label{vae_loss}
    \mathcal{L}_{\text{VAE}} = \underbrace{\scriptsize\text{MAE}(\text{Input}, \text{Reconstr})}_{\text{Reconstruction Loss}} + \underbrace{\beta \cdot \scriptsize\text{KL}\left( q(z|x) \parallel p(z) \right)}_{\text{KL Divergence}}
\end{equation}
\vspace{-1.5mm}
\begin{equation}\label{kl_loss}
\small\text{KLD} = -0.5 \times \sum \left(1 + \vec{\sigma} - \vec{\mu}^2 - \exp(\vec{\sigma})) \right)
\vspace{-4pt}
\end{equation}
We choose the Mean Absolute Error(MAE) as the trajectory reconstruction loss to penalize the difference between the input and reconstructed trajectories.


The Kullback-Leibler (KL) divergence measures the deviation between latent distribution space and a Gaussian distribution, thereby enforcing each feature in the latent space to approximate a Gaussian distribution, \( \mathcal{N}(0, I) \), as shown in equation ~\ref{kl_loss} This regularization helps maintain the latent space's structure and facilitates downstream tasks.

The parameter $\beta$ helps control the percentage between the KL-divergence and reconstruction loss in the total loss. We need to optimize the decoder reconstructed joint trajectory performance as well as the learned approximation of the latent distribution space to the true likelihood distribution. A Sigmoid Annealing Schedule to $\beta$, then prevents the KLD vanishing problem during training~\cite{fu_cyclical_2019} which can reduce the interdependencies between each latent feature and specific robot joints during the training. 
\vspace{-12pt}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{images/a2l.png}
    \captionsetup{font=footnotesize}
    \caption{The fully-connected module consists of an input layer, and three hidden layers with neuron counts ($32$, $40$, $20$). The output size is $10$ (latent feature size) for final predictions.}
    \label{fig:feedforward}
\end{figure}

\vspace{-4mm}

\subsubsection{\textbf{\textsc{Feed-forward neural network}}} To transfer the human arm joint configuration data to the executable configuration of the manipulator, we integrated a fully-connected neural network module shown in the top part of Fig.~\ref{fig:feedforward}. The fully connected module learns a data transformation from the human operator arm joints angle space to VAE latent feature space. We choose the Mean Absolute Error (MAE) as the loss function to penalize the error between the latent feature transformed from operator joints angle and target latent features, shown in \ref{mlp_loss}.
\vspace{-7pt}
\begin{equation}\label{mlp_loss}
\vspace{-6pt}
\mathcal{L}_{\text{mlp}} = \small\text{MAE}(\mathbf{y}, \hat{\mathbf{y}})
\end{equation}

In addition, we select the Scaled Exponential Linear Units (SELU) as the activation function for our fully-connected module. SELU's self-normalizing properties can help lower the difficulty of the learning process since each target latent feature follows a Gaussian distribution. The activation function is shown in equation~\ref{selu} where $\lambda \approx 1.0507$ \text{ and } $\alpha \approx 1.67326$.
\vspace{-8pt}
\begin{equation}\label{selu}
    \small\text{SELU}(x) = \lambda \begin{cases} 
    x & \text{if } x > 0 \\
    \alpha e^x - \alpha & \text{if } x \leq 0 
    \end{cases}
    \vspace{-5pt}
\end{equation}

The complete proposed teleoperation pipeline is illustrated in Fig.~\ref{fig:feedforward}. This pipeline integrates the trained GRU-based VAE decoder with a fully-connected neural network module. The decoder processes the latent features, which are passed through the fully-connected layers with the operator's right arm joints configuration to generate the desired outputs to control the robot's joint configurations.

\subsubsection{\textbf{\textsc{Model Training}}}
% In the GRU-based VAE model, both the encoder and decoder consist of two GRU layers that are used for extracting features from time-sequence trajectories. The input and output are $2$ time-steps Kinova Gen$3$ $7$-DOF joint angle position trajectory. Due to the projected unit value conversion, the data has a shape of $[2,14]$. After conducting multiple experiments, we selected a hidden feature size of $28$ for the GRU layers and a latent feature size of $10$. For the annealing schedule parameter $\beta$, the maximum value is set to $0.1$, and follows $4$ sigmoid cycles during the training process. The model is trained for $1,500$ epochs on an NVIDIA A$40$ GPU, with a batch size of $1,024,000$ and a learning rate of $1e-4$ with the Adam optimizer for $5$ hours.

In the GRU-based VAE model, both the encoder and decoder use two GRU layers to extract features from $2$ time-step Kinova Gen3 7-DOF joint angle trajectories, with data shaped as $[2, 14]$ due to unit value conversion. After conducting multiple experiments, we set the hidden feature size to $28$ and the latent feature size to $10$. The annealing parameter $\beta$ reaches a maximum of $0.1$ and follows $4$ sigmoid cycles during training. The model was trained for $1,500$ epochs on an NVIDIA A$40$ GPU, with a batch size of $1,024,000$ and a learning rate of $1e-4$ using the Adam optimizer, taking approximately 5 hours.

To evaluate the performance of the GRU-based VAE, we first randomly sampled two latent feature vectors from the Gaussian distribution. Since both vectors represent different configurations of the Kinova robot, we use the VAE decoder to reconstruct/generate the corresponding configurations. One latent vector is designated as the starting point and the other as the endpoint. By interpolating $100$ steps between these two latent vectors and decoding the intermediate steps, we generate a joint angle trajectory list that captures the relationship between the latent features and the robot's joint angles. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.73\linewidth]{images/correlation.jpg}
    \captionsetup{font=footnotesize}
    \caption{This figure demonstrates the training result of GRU-based VAE with (Top) and without (Bottom) annealing scheduler. The \(Z\) axis represents the Correlation-Coefficient score for each Latent feature $L_1$ to $L_{10}$ and Kinova robot joint $J_1$ to $J_7$.}
    \label{fig:correlation}
    \vspace{-4.5mm}

\end{figure}

To comprehensively assess how each latent feature influences joint angles, we fix the first vector and repeat $10,000$ times for the rest of the pipeline. This approach provides detailed insights into the control each latent feature exerts over the joint angles, as shown in Fig.~\ref{fig:correlation}. For comparison, we trained two VAE models. The top results display the model training with the Sigmoid annealing scheduler, where specific latent feature displays a strong positive or negative correlation value with specific joint angles while having low correlation scores with other joints. Without the scheduler, the impact was more evenly distributed across latent features, making it harder for the fully-connected module to map human joint angles to manipulator features (involving one-to-many mapping; one \(X\) to many \(Y\)).
% By contrast training without the annealing scheduler, indicated that the impact is more evenly distributed across latent features, which increases the difficulty in learning the mapping between human joint angles and manipulator latent features for the fully-connected module training (involving one-to-many mapping; one \(X\) to many \(Y\)).

The fully-connected module consists of four feed-forward layers, with one dropout layers (rate of $0.5$) which are applied to the second layers to reduce over-fitting. The model is trained for $500$ epochs on an NVIDIA RTX $3060$, taking two minutes using the Adam optimizer with a learning rate of $1e-3$ and a batch size of $256$. The input size is $24$, as the $12$ joint angles are converted to projected unit values, and the output size is $10$, corresponding to the latent feature size of the GRU-based VAE model.
\begin{wrapfigure}{l}{0.18\textwidth} % 'r' for right, width of the figure is 40% of the text width
    \centering
    \includegraphics[width=0.2\textwidth]{images/a2l_a2k.jpeg} % Adjust the width to fit
    \captionsetup{font=footnotesize}
    \caption{The \textbf{red} trajectory: operator's hand position in Cartesian space. The \textbf{green}: proposed teleoperation pipeline. The \textbf{blue}: fully-connected network without the VAE decoder.}
    \label{fig:comparsion_zx}
\vspace{-5pt}
\end{wrapfigure}

Theoretically, the VAE decoder approximates the likelihood distribution function \(\mathcal{P}({x}\mid{z})\) that enables it to generate new manipulator joint configurations. This approach has shown successful results in prior work, such as in image generation \cite{kingma_auto-encoding_2022} and in generating English sentences \cite{bowman_generating_2016}. By leveraging the continuity and smoothness of the probabilistic distribution, we can train the model with a limited set of paired human joint configurations and their corresponding latent representations of Kinova joint configurations. The VAE decoder is then able to interpolate and generate new Kinova joint configurations that correspond to human arm movements, even for configurations never encountered in the training process (uncertainty).

% For comparison, we trained an alternative fully-connected neural network with the same architecture, except for the output layer has 14 values which directly represents the projected unit values of the Kinova joint angles. This network was trained using the same dataset of paired human and Kinova joint configurations. We pre-record a human right arm joint angle trajectory and feed it into both models to generate corresponding Kinova manipulator's configuration trajectory. We compare the end-effecotr's trajectory and result as shown in Fig~\ref{fig:comparsion_zx}. We can notice the proposed pipeline (with VAE decoder) has better performance of translating human arm configuration and generating new configuration for manipulator.

For comparison, we trained an alternative fully-connected neural network with the same architecture, except its output layer directly predicts $14$ Kinova joint angles. Using the same dataset of paired human and Kinova configurations, we pre-recorded a human arm trajectory and fed it into both models to generate corresponding manipulator trajectories. As shown in Fig~\ref{fig:comparsion_zx}, the proposed VAE-based pipeline outperforms the alternative in translating human arm movements and generating new manipulator configurations.
 
% - The characteristics can simulate complex human ball-and-socket and condyloid joints movements by changing the representation of revolute joints.
