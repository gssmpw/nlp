One approach to achieve high-DoF manipulator teleoperation is Master-Slave or Twin-Master, where the operator manually controls another manipulator with identical kinematics to the target manipulator~\cite{singh_haptic-guided_2020,su_heterogeneous_2021}. However, this method requires an additional manipulator with the same kinematic structure as the remotely controlled device. This requires either two identical manipulators or building a custom one. Both options are costly and limit the manipulator system's adaptability to other types of manipulators. 

A Human arm is defined as a $7$-DOF kinematic structure~\cite{prokopenko_assessment_2001}. Hence, teleoperating a high-DOF manipulator by mapping the human arm joints to the target manipulator is another approach. Previous work~\cite{penco_robust_2018}, achieved humanoid robot ($7$-DOF arm) teleoperation by attaching IMU sensors to the human operator and mapping whole-body joints with dynamic filters between the operator and robot. However, a limitation of this method is that the required robot kinematic structure must be similar to the human arm kinematic structure. Other strategies have been explored to simplify the kinematic representation of the human arm for teleoperation~\cite{ajoudani_reduced-complexity_2018,su_deep_2019,arduengo_human_2021}. For example,~\cite{su_deep_2019} introduced elbow elevation angle as a constraint for a human arm mapped on the robot as a swivel motion. 
% using captured operator's elbow elevation angle by RGBD camera sensor and target hand 6-Dof pose as input for neural network module to generate mapped 7-Dof manipulator joints configuration.
~\cite{arduengo_human_2021} proposes a method that separates a redundant $7$-DOF manipulator as a $3$-DOF manipulator attached to a $4$-DOF end-effector. Using IMU sensors to couple the operator's hand with $3$-DOF end-effector and elbow with $4$-DOF end-effector position changes, with IK calculation for both sub-manipulators' joints in real-time to achieve teleoperation.

Our work draws inspiration from the concept of modifying kinematic representations. While most manipulators use a combination of single DOF revolute or prismatic joints, human arm articulation kinematics are represented by a combination of ball-and-socket and condyloid joints to achieve complex joint movements. Finding robot combinatorial joint kinematics that can mimic complex human articulated arm motion is an essential step in our method. Specifically, and instead of imposing explicit constraints to redundant DOF robots, we propose a generative deep learning (DL) framework to find a low-dimensional robust implicit, kinematic representation that describes complex redundant DOF robotic motion. Our framework devises a GRU-based variational autoencoder deep neural network (DNN) that generates robot trajectories which faithfully mimic human gestural intent for completing tasks.

% \subsection{Generative Model}
A Variational Autoencoder (VAE) is a neural network architecture with an encoder, a latent space, and decoder module\cite{kingma_auto-encoding_2022}. It was first introduced as an image-generative model that efficiently uses a neural network to approximate the likelihood function for the latent space distribution, which is modeled as a mixture of multiple Gaussian distributions derived from the training dataset. By sampling latent space features from Gaussian distributions, we can generate a new, unforeseen image by the decoder, which endows the characteristic features of an actual image. Such an architecture can also be used to prescribe $3$D physical motions. ~\cite{hamalainen_affordance_2019} introduced a method that integrates two Variational Autoencoder (VAE) modules: one for processing visual sensor data and the other for the manipulator's trajectory. By utilizing the latent representation from the vision module, the decoder of trajectory-VAE can generate movement paths for object-reaching tasks in the $2$D Cartesian plane.

On the other hand, the Recurrent Neural Networks (RNNs) are the most commonly used methods for sequential data prediction and feature extraction. For example, \cite{weigend_anytime_2023} utilizes Long Short-Term Memory (LSTM) as a neural network module to predict the human elbows and wrists position in order to implement an intuitive control for manipulator based on the time series data from an IMU sensor in a smartwatch.

 VAEs and RNNs can be integrated together for inference tasks. For example \cite{bowman_generating_2016}, proposed an LSTM-based VAE for missing word-imputing tasks. The LSTM allows the VAE model to consider global concepts of a sentence and generate more diverse and well-formed sentences compared with the standard RNN language model. Furthermore, in \cite{ozdemir_embodied_2021}, they also present an LSTM-based VAE framework for robot-embodied Language Learning. This allows the system to generate the correct action description by executing the action. 

The remainder of the paper is as follows. Section $3$ discusses the proposed teleoperation system, including the training dataset, the proposed GRU-based VAE model for discovering the manipulator's latent distribution space, and the fully-connected neural network module used to map human arm configurations to the manipulator's latent space, along with the model training results. Section $4$ outlines the experimental design and presents the results used to evaluate the proposed teleoperation system. Section $5$ addresses the limitations of the current method, while Section $6$ provides the conclusion. This study is under the IRB: STUDY$00009131$.

% VAE
% The entire framework have three parts. Encoder and Decoder which
% decoder: P(z) *P(X|Z) = P(X)
% encoder: P(x) * q(z|x) = q(z)
