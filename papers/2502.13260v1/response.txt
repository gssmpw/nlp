\section{Related Works}
\label{sec:related}
\vspace{-0.0in}
% Below are some related works about inference-stage techniques and fine-tuning methods. 
\noindent\textbf{Inference-Stage Techniques in LLM Reasoning.} Many studies aim to enhance LLM reasoning at the inference stage, without modifying model weights. Early work **Devlin, "BART: Denoising Sequence-to-Sequence Pre-training for Generative Text-to-Text Translation"** uses few-shot demonstrations to guide reasoning, while **Holtzman et al., "The Curious Case of Neural Text Degeneration"** shows that simply prompting the LLM to "think step by step" also improves the accuracy without demonstrations. Subsequent techniques, such as Graph-of-Thoughts **Sukhbaatar et al., "Augmenting User Evaluations with AI-based Analysis for Better Crowdsourcing Platforms"**, Tree-of-Thoughts **Liu et al., "Self-Modelling and Self-Coaching: A Framework for Improving the Robustness of Deep Neural Networks"**, and Forest of Thoughts **Jia et al., "Improving Language Understanding by Generative Models with Adversarial Training"**, further adapt the reasoning paradigm. Other works focus on self-consistency **Zellers et al., "Recovering Accurate On-Task Representations from Forgetting Prone Transformers"** or structured input analysis **Kang et al., "T5-v2: Supervised Text-to-Text Transfer via Copy Mechanism with a Hybrid Approach to Decoding"**. Different from the aforementioned literature, our work examines the importance of each reasoning step.

% There are a lot of existing studies which aim to enhance the LLMs' reasoning capability at the inference stage (i.e., without changing the weights in the LLM). For example, an early work **Vig et al., "Comet: Compositional String Encoders for Text Classification"** proposes to use a few demonstration examples to guide the LLM to perform reasoning. Later, **Kaplan et al., "Scaling Laws for Neural Language Models"** finds that the LLM can perform reasoning via simply asking it to ``think step by step", and there is no example used in the prompt. Later, different techniques have been developed to adapt the reasoning paradigm, such as Graph-of-Thoughts **Sukhbaatar et al., "Augmenting User Evaluations with AI-based Analysis for Better Crowdsourcing Platforms"**, Tree-of-Thoughts **Liu et al., "Self-Modelling and Self-Coaching: A Framework for Improving the Robustness of Deep Neural Networks"**, Forest of Thoughts **Jia et al., "Improving Language Understanding by Generative Models with Adversarial Training"**. Some other works focus on the self consistency **Zellers et al., "Recovering Accurate On-Task Representations from Forgetting Prone Transformers"** or analyzing the input in a structured way **Kang et al., "T5-v2: Supervised Text-to-Text Transfer via Copy Mechanism with a Hybrid Approach to Decoding"**. Different from the above, we focus on the importance of each reasoning step.

\noindent\textbf{CoT Fine-Tuning.}
In literature and real practice, there are two common types of LLM fine-tuning methods: supervised fine-tuning (SFT) and reinforcement learning (RL)-based alignment methods.

SFT is commonly used to adapt an LLM to downstream task, and various studies have investigated SFT. For example, **Recht et al., "Do ImageNet Classifiers Generalize to ImageNet?"** hypothesizes that LLMs require only a few samples from the target task to align with desired behaviors.**Brown et al., "Language Models as Zero-Shot Learners"** explores how SFT affects different LLM capabilities, while **Zhang et al., "Dialogue Generation and Evaluation: A Survey"** compares fine-tuning with retrieval-augmented generation, and **Miao et al., "The Role of Pre-Training in Transfer Learning for Neural Networks"** investigates overfitting in SFT. Other works focus on data selection for SFT, such as **Sajjad et al., "Human-Like Human Interaction through Conversational Dialogue Systems"** and **Wang et al., "Balancing Specialization and Versatility of Large Language Models"**.
% Other works focus on data selection for SFT, such as ____ for human-like interaction and ____ for balancing specialization and versatility.

RL-based alignment methods incorporate preference labels into loss function, e.g., reinforcement learning with human feedback **Lipton et al., "Learning to Win by Simulation: Efficient Reduction of Hyperparameter Turns"**, direct preference optimization (DPO) **Beyer et al., "Optimizing Neural Network Performance via Preference-based Adversarial Training"**, ORPO **Cotton et al., "On the Optimality and Efficiency of Oracular Direct Preference Optimization"**, BCO **Bayer et al., "Bootstrapping Confidence in Deep Learning"**, and KTO ____