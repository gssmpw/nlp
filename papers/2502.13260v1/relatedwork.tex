\section{Related Works}
\label{sec:related}
\vspace{-0.0in}
% Below are some related works about inference-stage techniques and fine-tuning methods. 
\noindent\textbf{Inference-Stage Techniques in LLM Reasoning.} Many studies aim to enhance LLM reasoning at the inference stage, without modifying model weights. Early work \cite{wei2022chain} uses few-shot demonstrations to guide reasoning, while \cite{kojima2022large} shows that simply prompting the LLM to "think step by step" also improves the accuracy without demonstrations. Subsequent techniques, such as Graph-of-Thoughts \cite{besta2024graph}, Tree-of-Thoughts \cite{yao2024tree}, and Forest of Thoughts \cite{bi2024forest}, further adapt the reasoning paradigm. Other works focus on self-consistency \cite{wang2022self,wan2023better} or structured input analysis \cite{he2024make}. Different from the aforementioned literature, our work examines the importance of each reasoning step.

% There are a lot of existing studies which aim to enhance the LLMs' reasoning capability at the inference stage (i.e., without changing the weights in the LLM). For example, an early work \cite{wei2022chain} proposes to use a few demonstration examples to guide the LLM to perform reasoning. Later, \cite{kojima2022large} finds that the LLM can perform reasoning via simply asking it to ``think step by step", and there is no example used in the prompt. Later, different techniques have been developed to adapt the reasoning paradigm, such as Graph-of-Thoughts \cite{besta2024graph}, Tree-of-Thoughts \cite{yao2024tree}, Forest of Thoughts \cite{bi2024forest}. Some other works focus on the self consistency \cite{wang2022self,wan2023better} or analyzing the input in a structured way \cite{he2024make}. Different from the above, we focus on the importance of each reasoning step.

\noindent\textbf{CoT Fine-Tuning.}
In literature and real practice, there are two common types of LLM fine-tuning methods: supervised fine-tuning (SFT) and reinforcement learning (RL)-based alignment methods.

SFT is commonly used to adapt an LLM to downstream task, and various studies have investigated SFT. For example, \cite{zhou2024lima} hypothesizes that LLMs require only a few samples from the target task to align with desired behaviors.\cite{dong2023abilities} explores how SFT affects different LLM capabilities, while \cite{ovadia2023fine} compares fine-tuning with retrieval-augmented generation, and \cite{ling2024deductive} investigates overfitting in SFT. Other works focus on data selection for SFT, such as \cite{shen2024rethinking} and \cite{zhang2024balancing}.
% Other works focus on data selection for SFT, such as \cite{shen2024rethinking} for human-like interaction and \cite{zhang2024balancing} for balancing specialization and versatility.

RL-based alignment methods incorporate preference labels into loss function, e.g., reinforcement learning with human feedback \cite{ziegler2019fine}, direct preference optimization (DPO) \cite{rafailov2024direct}, ORPO \cite{hong2024orpo}, BCO \cite{jung2024binary}, and KTO \cite{ethayarajhmodel}.
  \vspace{-0.08in}