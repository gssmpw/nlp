[
  {
    "index": 0,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "besta2024graph",
        "author": "Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others",
        "title": "Graph of thoughts: Solving elaborate problems with large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "bi2024forest",
        "author": "Bi, Zhenni and Han, Kai and Liu, Chuanjian and Tang, Yehui and Wang, Yunhe",
        "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "wan2023better",
        "author": "Wan, Xingchen and Sun, Ruoxi and Dai, Hanjun and Arik, Sercan O and Pfister, Tomas",
        "title": "Better zero-shot reasoning with self-adaptive prompting"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "he2024make",
        "author": "He, Pengfei and Li, Zitao and Xing, Yue and Li, Yaling and Tang, Jiliang and Ding, Bolin",
        "title": "Make LLMs better zero-shot reasoners: Structure-orientated autonomous reasoning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "besta2024graph",
        "author": "Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others",
        "title": "Graph of thoughts: Solving elaborate problems with large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "bi2024forest",
        "author": "Bi, Zhenni and Han, Kai and Liu, Chuanjian and Tang, Yehui and Wang, Yunhe",
        "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "wan2023better",
        "author": "Wan, Xingchen and Sun, Ruoxi and Dai, Hanjun and Arik, Sercan O and Pfister, Tomas",
        "title": "Better zero-shot reasoning with self-adaptive prompting"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "he2024make",
        "author": "He, Pengfei and Li, Zitao and Xing, Yue and Li, Yaling and Tang, Jiliang and Ding, Bolin",
        "title": "Make LLMs better zero-shot reasoners: Structure-orientated autonomous reasoning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhou2024lima",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "Lima: Less is more for alignment"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dong2023abilities",
        "author": "Dong, Guanting and Yuan, Hongyi and Lu, Keming and Li, Chengpeng and Xue, Mingfeng and Liu, Dayiheng and Wang, Wei and Yuan, Zheng and Zhou, Chang and Zhou, Jingren",
        "title": "How abilities in large language models are affected by supervised fine-tuning data composition"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "ovadia2023fine",
        "author": "Ovadia, Oded and Brief, Menachem and Mishaeli, Moshik and Elisha, Oren",
        "title": "Fine-tuning or retrieval? comparing knowledge injection in llms"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "ling2024deductive",
        "author": "Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao and Lee, Mingu and Memisevic, Roland and Su, Hao",
        "title": "Deductive verification of chain-of-thought reasoning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "shen2024rethinking",
        "author": "Shen, Ming",
        "title": "Rethinking Data Selection for Supervised Fine-Tuning"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zhang2024balancing",
        "author": "Zhang, Hengyuan and Wu, Yanru and Li, Dawei and Yang, Sak and Zhao, Rui and Jiang, Yong and Tan, Fei",
        "title": "Balancing speciality and versatility: a coarse to fine framework for supervised fine-tuning large language model"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "shen2024rethinking",
        "author": "Shen, Ming",
        "title": "Rethinking Data Selection for Supervised Fine-Tuning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhang2024balancing",
        "author": "Zhang, Hengyuan and Wu, Yanru and Li, Dawei and Yang, Sak and Zhao, Rui and Jiang, Yong and Tan, Fei",
        "title": "Balancing speciality and versatility: a coarse to fine framework for supervised fine-tuning large language model"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "hong2024orpo",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "jung2024binary",
        "author": "Jung, Seungjae and Han, Gunsoo and Nam, Daniel Wontae and On, Kyoung-Woon",
        "title": "Binary classifier optimization for large language model alignment"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "ethayarajhmodel",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Model Alignment as Prospect Theoretic Optimization"
      }
    ]
  }
]