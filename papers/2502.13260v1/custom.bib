
@inproceedings{zhang2024mathverse,
  title={Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Qiao, Yu and others},
  booktitle={European Conference on Computer Vision},
  pages={169--186},
  year={2024},
  organization={Springer}
}
@article{shi2024muse,
  title={Muse: Machine unlearning six-way evaluation for language models},
  author={Shi, Weijia and Lee, Jaechan and Huang, Yangsibo and Malladi, Sadhika and Zhao, Jieyu and Holtzman, Ari and Liu, Daogao and Zettlemoyer, Luke and Smith, Noah A and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2407.06460},
  year={2024}
}
@inproceedings{ni2024large,
  title={Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?},
  author={Ni, Shiyu and Bi, Keping and Yu, Lulu and Guo, Jiafeng},
  booktitle={China Conference on Information Retrieval},
  pages={124--135},
  year={2024},
  organization={Springer}
}
@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{yang2024verbalized,
  title={On Verbalized Confidence Scores for LLMs},
  author={Yang, Daniel and Tsai, Yao-Hung Hubert and Yamada, Makoto},
  journal={arXiv preprint arXiv:2412.14737},
  year={2024}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}
@article{cooper2024perplexed,
  title={Perplexed: Understanding When Large Language Models are Confused},
  author={Cooper, Nathan and Scholak, Torsten},
  journal={arXiv preprint arXiv:2404.06634},
  year={2024}
}
@article{lee2020misinformation,
  title={Misinformation has high perplexity},
  author={Lee, Nayeon and Bang, Yejin and Madotto, Andrea and Fung, Pascale},
  journal={arXiv preprint arXiv:2006.04666},
  year={2020}
}
@article{kuribayashi2021lower,
  title={Lower perplexity is not always human-like},
  author={Kuribayashi, Tatsuki and Oseki, Yohei and Ito, Takumi and Yoshida, Ryo and Asahara, Masayuki and Inui, Kentaro},
  journal={arXiv preprint arXiv:2106.01229},
  year={2021}
}
@article{alon2023detecting,
  title={Detecting language model attacks with perplexity},
  author={Alon, Gabriel and Kamfonas, Michael},
  journal={arXiv preprint arXiv:2308.14132},
  year={2023}
}
@inproceedings{ankner2024perplexed,
  title={Perplexed by Perplexity: Perplexity-Based Pruning with Small Reference Models},
  author={Ankner, Zachary and Blakeney, Cody and Sreenivasan, Kartik and Marion, Max and Leavitt, Matthew L and Paul, Mansheej},
  booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models}
}
@article{fang2024wrong,
  title={What is Wrong with Perplexity for Long-context Language Modeling?},
  author={Fang, Lizhe and Wang, Yifei and Liu, Zhaoyang and Zhang, Chenheng and Jegelka, Stefanie and Gao, Jinyang and Ding, Bolin and Wang, Yisen},
  journal={arXiv preprint arXiv:2410.23771},
  year={2024}
}
@article{qwen2024qwen2.5,
  title={Qwen2.5 Technical Report},
  author={Qwen Team},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024},
  url={https://arxiv.org/abs/2412.15115}
}
@article{grattafiori2024llama3,
  title={The Llama 3 Herd of Models},
  author={Grattafiori, Aaron and et al.},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024},
  url={https://arxiv.org/abs/2407.21783}
}
@article{loshchilov2019decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={International Conference on Learning Representations (ICLR)},
  year={2019},
  archivePrefix={arXiv},
  eprint={1711.05101},
  primaryClass={cs.LG}
}

@article{jelinek1977perplexity,
  title={Perplexityâ€”a measure of the difficulty of speech recognition tasks},
  author={Jelinek, Fred and Mercer, Robert L and Bahl, Lalit R and Baker, James K},
  journal={The Journal of the Acoustical Society of America},
  volume={62},
  number={S1},
  pages={S63--S63},
  year={1977},
  publisher={Acoustical Society of America}
}
@inproceedings{hu2021lora,
  author    = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2106.09685}
}
@article{brown2024large,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}
@article{saad2024archon,
  title={Archon: An architecture search framework for inference-time techniques},
  author={Saad-Falcon, Jon and Lafuente, Adrian Gamarra and Natarajan, Shlok and Maru, Nahum and Todorov, Hristo and Guha, Etash and Buchanan, E Kelly and Chen, Mayee and Guha, Neel and R{\'e}, Christopher and others},
  journal={arXiv preprint arXiv:2409.15254},
  year={2024}
}
@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}
@inproceedings{hong2024orpo,
  title={Orpo: Monolithic preference optimization without reference model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={11170--11189},
  year={2024}
}
@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{ovadia2023fine,
  title={Fine-tuning or retrieval? comparing knowledge injection in llms},
  author={Ovadia, Oded and Brief, Menachem and Mishaeli, Moshik and Elisha, Oren},
  journal={arXiv preprint arXiv:2312.05934},
  year={2023}
}
@article{li2024entropic,
  title={Entropic distribution matching in supervised fine-tuning of LLMs: Less overfitting and better diversity},
  author={Li, Ziniu and Chen, Congliang and Xu, Tian and Qin, Zeyu and Xiao, Jiancong and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2408.16673},
  year={2024}
}
@article{zhang2024balancing,
  title={Balancing speciality and versatility: a coarse to fine framework for supervised fine-tuning large language model},
  author={Zhang, Hengyuan and Wu, Yanru and Li, Dawei and Yang, Sak and Zhao, Rui and Jiang, Yong and Tan, Fei},
  journal={arXiv preprint arXiv:2404.10306},
  year={2024}
}
@article{shen2024rethinking,
  title={Rethinking Data Selection for Supervised Fine-Tuning},
  author={Shen, Ming},
  journal={arXiv preprint arXiv:2402.06094},
  year={2024}
}
@article{dong2023abilities,
  title={How abilities in large language models are affected by supervised fine-tuning data composition},
  author={Dong, Guanting and Yuan, Hongyi and Lu, Keming and Li, Chengpeng and Xue, Mingfeng and Liu, Dayiheng and Wang, Wei and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2310.05492},
  year={2023}
}
@inproceedings{ethayarajhmodel,
  title={Model Alignment as Prospect Theoretic Optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  booktitle={Forty-first International Conference on Machine Learning}
}
@article{jung2024binary,
  title={Binary classifier optimization for large language model alignment},
  author={Jung, Seungjae and Han, Gunsoo and Nam, Daniel Wontae and On, Kyoung-Woon},
  journal={arXiv preprint arXiv:2404.04656},
  year={2024}
}
@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}
@article{wan2023better,
  title={Better zero-shot reasoning with self-adaptive prompting},
  author={Wan, Xingchen and Sun, Ruoxi and Dai, Hanjun and Arik, Sercan O and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.14106},
  year={2023}
}
@article{he2024make,
  title={Make LLMs better zero-shot reasoners: Structure-orientated autonomous reasoning},
  author={He, Pengfei and Li, Zitao and Xing, Yue and Li, Yaling and Tang, Jiliang and Ding, Bolin},
  journal={arXiv preprint arXiv:2410.19000},
  year={2024}
}
@article{bi2024forest,
  title={Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning},
  author={Bi, Zhenni and Han, Kai and Liu, Chuanjian and Tang, Yehui and Wang, Yunhe},
  journal={arXiv preprint arXiv:2412.09078},
  year={2024}
}
@inproceedings{besta2024graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17682--17690},
  year={2024}
}
@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}
@article{saxton2019analysing,
  title={Analysing mathematical reasoning abilities of neural models},
  author={Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1904.01557},
  year={2019}
}
@article{cui2024theoretical,
  title={A theoretical understanding of chain-of-thought: Coherent reasoning and error-aware demonstration},
  author={Cui, Yingqian and He, Pengfei and Tang, Xianfeng and He, Qi and Luo, Chen and Tang, Jiliang and Xing, Yue},
  journal={arXiv preprint arXiv:2410.16540},
  year={2024}
}
@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}
@article{ling2024deductive,
  title={Deductive verification of chain-of-thought reasoning},
  author={Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao and Lee, Mingu and Memisevic, Roland and Su, Hao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{karabacak2023embracing,
  title={Embracing large language models for medical applications: opportunities and challenges},
  author={Karabacak, Mert and Margetis, Konstantinos},
  journal={Cureus},
  volume={15},
  number={5},
  year={2023},
  publisher={Cureus Inc.}
}
@article{gramopadhye2024few,
  title={Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering},
  author={Gramopadhye, Ojas and Nachane, Saeel Sandeep and Chanda, Prateek and Ramakrishnan, Ganesh and Jadhav, Kshitij Sharad and Nandwani, Yatin and Raghu, Dinesh and Joshi, Sachindra},
  journal={arXiv preprint arXiv:2403.04890},
  year={2024}
}
@article{liu2023improving,
  title={Improving large language model fine-tuning for solving math problems},
  author={Liu, Yixin and Singh, Avi and Freeman, C Daniel and Co-Reyes, John D and Liu, Peter J},
  journal={arXiv preprint arXiv:2310.10047},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
