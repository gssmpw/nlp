\section{Related Works}
\label{sec:related}
\vspace{-0.0in}
% Below are some related works about inference-stage techniques and fine-tuning methods. 
\noindent\textbf{Inference-Stage Techniques in LLM Reasoning.} Many studies aim to enhance LLM reasoning at the inference stage, without modifying model weights. Early work ____ uses few-shot demonstrations to guide reasoning, while ____ shows that simply prompting the LLM to "think step by step" also improves the accuracy without demonstrations. Subsequent techniques, such as Graph-of-Thoughts ____, Tree-of-Thoughts ____, and Forest of Thoughts ____, further adapt the reasoning paradigm. Other works focus on self-consistency ____ or structured input analysis ____. Different from the aforementioned literature, our work examines the importance of each reasoning step.

% There are a lot of existing studies which aim to enhance the LLMs' reasoning capability at the inference stage (i.e., without changing the weights in the LLM). For example, an early work ____ proposes to use a few demonstration examples to guide the LLM to perform reasoning. Later, ____ finds that the LLM can perform reasoning via simply asking it to ``think step by step", and there is no example used in the prompt. Later, different techniques have been developed to adapt the reasoning paradigm, such as Graph-of-Thoughts ____, Tree-of-Thoughts ____, Forest of Thoughts ____. Some other works focus on the self consistency ____ or analyzing the input in a structured way ____. Different from the above, we focus on the importance of each reasoning step.

\noindent\textbf{CoT Fine-Tuning.}
In literature and real practice, there are two common types of LLM fine-tuning methods: supervised fine-tuning (SFT) and reinforcement learning (RL)-based alignment methods.

SFT is commonly used to adapt an LLM to downstream task, and various studies have investigated SFT. For example, ____ hypothesizes that LLMs require only a few samples from the target task to align with desired behaviors.____ explores how SFT affects different LLM capabilities, while ____ compares fine-tuning with retrieval-augmented generation, and ____ investigates overfitting in SFT. Other works focus on data selection for SFT, such as ____ and ____.
% Other works focus on data selection for SFT, such as ____ for human-like interaction and ____ for balancing specialization and versatility.

RL-based alignment methods incorporate preference labels into loss function, e.g., reinforcement learning with human feedback ____, direct preference optimization (DPO) ____, ORPO ____, BCO ____, and KTO ____.
  \vspace{-0.08in}