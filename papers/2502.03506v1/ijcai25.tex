%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{stfloats}

\usepackage{amssymb}
\usepackage{makecell}
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Optimistic $\epsilon$-Greedy Exploration for Cooperative Multi-Agent\\ Reinforcement Learning}


% % Single author syntax
% \author{
%     Anonymous Submission \#No.4291
% }

\author{
Ruoning Zhang$^1$
\and
Siying Wang$^2$\footnote{Corresponding author.}\and
Wenyu Chen$^1$\and
Yang Zhou$^1$\and
Zhitong Zhao$^1$\and\\
Zixuan Zhang$^1$\And
Ruijie Zhang$^1$\\
\affiliations
$^1$School of Computer Science and Engineering, \\University of Electronic Science and Technology of China\\
$^2$School of Automation Engineering, University of Electronic Science and Technology of China
\emails
zhangruoning@std.uestc.edu.cn,
\{siyingwang, cwy\}@uestc.edu.cn,
\{zhouy, zhaozhitong, 202322080910, ruijie\_zhang\}@std.uestc.edu.cn
}

\begin{document}

\maketitle

\begin{abstract}
    The Centralized Training with Decentralized Execution (CTDE) paradigm is widely used in cooperative multi-agent reinforcement learning. However, due to the representational limitations of traditional monotonic value decomposition methods, algorithms can underestimate optimal actions, leading policies to suboptimal solutions. To address this challenge, we propose Optimistic $\epsilon$-Greedy Exploration, focusing on enhancing exploration to correct value estimations. The underestimation arises from insufficient sampling of optimal actions during exploration, as our analysis indicated. We introduce an optimistic updating network to identify optimal actions and sample actions from its distribution with a probability of $\epsilon$ during exploration, increasing the selection frequency of optimal actions. Experimental results in various environments reveal that the Optimistic $\epsilon$-Greedy Exploration effectively prevents the algorithm from suboptimal solutions and significantly improves its performance compared to other algorithms.
\end{abstract}

\section{Introduction}
Multi-agent reinforcement learning (MARL) are used to model the interactions between agents and environments. Agents perform actions in the environment and optimize their policies to maximize cumulative rewards. With the development of deep neural networks, MARL has been extensively utilized in real-world scenarios, achieving significant success in robotic control \cite{lowe2017multi}, autonomous driving \cite{shalev2016safe}, traffic light control \cite{kolat2023multi} and recommendation systems \cite{gao2023cirs}.

Meanwhile, MARL has been widely explored under the paradigm of Centralized Training with Decentralized Execution (CTDE). Among the various approaches in CTDE, value decomposition methods are extensively adopted due to their effectiveness in computing a global value estimation based on the joint actions of all agents during training, while enabling decentralized decision-making throughout the process of execution. To ensure consistency between centralized training and decentralized execution, these algorithms require that the joint action maximizing the global estimation matches the combination of each agent's locally optimal actions \cite{son2019qtran}, known as the Individual Global Max (IGM) condition. Most algorithms satisfy this constraint by aligning local and global estimations via monotonic aggregation functions. For instance, VDN \cite{sunehag2017value} takes the sum of local estimations as global estimation, and QMIX \cite{rashid2020monotonic} uses a monotonic hyper-network with positive parameters to meet this requirement.

Although these algorithms have shown excellent performance in various tasks, the accuracy of value estimation is limited due to the excessive constraint on aggregation functions, making policies prone to suboptimal solutions\cite{wang2024enhancing}. A typical issue is relative overgeneralization \cite{lanctot2017unified}, where optimal action is surrounded by suboptimal ones. This reward distribution makes the optimal action vulnerable, as any agent choosing action incorrectly can cause a steep decline in team rewards. In this predicament, policies can easily fall into sub-Nash equilibrium solutions because agents underestimate the value of the optimal action when actions of other agents do not correspond. Some research attempts to tackle this issue by employing more complex network structures through credit assignment \cite{zhao2024qdap}, mutual information \cite{iqbal2019actor}, or consistency protocols \cite{zhang2018fully}. However, designing an appropriate neural network is challenging under the constraint of the IGM condition.

In this paper, we identify insufficient sampling of the optimal action during exploration also significantly contributes to underestimation, as demonstrated by our mathematical analysis. To address this challenge, we propose a novel exploration strategy: Optimistic $\epsilon$-Greedy Exploration. We introduce an optimistic network for each agent to detect the optimal actions. With a probability of $\epsilon$, agents sample actions based on these optimistic estimations, promoting more frequent selection of high-reward actions during exploration. Our main contributions are:

\begin{itemize}
\item We analyzed the issue of underestimation from a mathematical perspective, showing that the frequent selection of optimal actions helps avoid suboptimal solutions.
\item We demonstrated that a monotonically increasing estimation network identifies the joint optimal action. Building upon this, we propose the Optimistic $\epsilon$-Greedy Exploration strategy, which is more suitable for the CTDE paradigm.
\item We integrated this strategy within the QMIX framework and evaluated it across multiple environments, showing it effectively resolves underestimation and performs better than other algorithms.
\end{itemize}

\section{Related Work}

Research in cooperative multi-agent reinforcement learning began with independent Q-learning \cite{littman1994markov}. However, it struggles to effectively train cooperative policies, leading to suboptimal performance \cite{claus1998dynamics}. To overcome this limitation, some approaches \cite{foerster2017stabilising,foerster2018counterfactual} treat all agents as a single entity. While effective with few agents, these approaches can't scale well to environments with larger numbers of agents due to the exponential growth of the joint action space. Centralized Training with Decentralized Execution (CTDE) was introduced to balance these challenges \cite{lowe2017multi}. This paradigm develops cooperative policies via global information during training, while allowing each agent to decide its action independently during execution. Within this paradigm, value decomposition methods like VDN \cite{sunehag2017value} and QMIX \cite{rashid2020monotonic} have performed exceptionally well in tasks such as StarCraft II \cite{samvelyan2019starcraft}. However, these methods impose monotonicity constraint to satisfy the IGM condition, potentially leading to inaccurate estimation in certain scenarios.

To address this issue, some approaches attempt to use more complex network structures for value estimation. QTRAN \cite{son2019qtran} introduces additional estimation terms to correct biases in value representation. ResQ \cite{shen2022resq} and RQN \cite{pina2022residual} employ residual-like networks to provide correction terms for each agent, mitigating impacts of irregular reward distribution. QPLEX \cite{wang2020qplex} decouples local estimation into state value and action advantage to jointly estimate global value. However, incorporating additional neural network modules makes convergence more challenging. Furthermore, these algorithms rely on accurately identifying optimal actions, which can only be approximated through neural network estimation in practice. Experimental results \cite{papoudakis2020benchmarking} indicate that these algorithms often struggle to perform well due to instability in neural network training.

Other approaches attribute inaccurate estimation to insufficient exploration of the environment and enhance the exploration to refine estimation. LH-IRQN 
\cite{lyu2018likelihood} and DFAC \cite{sun2021dfac} attempt to estimate uncertainty in multi-agent environments using classifications or quantile distributions. However, computing the distribution is challenging due to the mutual influence among agents. Some research \cite{bohmer2019exploration,liu2021cooperative} encourages exploration through intrinsic motivation methods, but the cost of computation makes these difficult to handle large-scale tasks. EITI \cite{wang2019influence} and VM3-AC \cite{kim2020maximum} encourage exploration by maximizing mutual information between policies to manage a large number of agents. Unfortunately, exploration strategies based on mutual information are limited by the structure of algorithms and are difficult to generalize. Additionally, current exploration strategies focus on exhaustive state exploration, while neglecting its role in guiding policy training, making it difficult for algorithms to get rid of suboptimal solutions effectively.

With the aim of overcoming suboptimal solutions, some research focuses on identifying globally optimal actions to guide policy training. WQMIX \cite{rashid2020weighted} enhances policy training by assigning higher weights to optimal actions in the loss function. OVI-QMIX \cite{li2024optimistic} minimizes the KL divergence between the current and optimal policies through optimistic instructors. However, these methods use approximate optima for guiding value estimation, leading to instability
during training. Preference-Guided Stochastic Exploration \cite{huang2023sampling} introduces an additional exploration strategy based on the advantage of each action in single-agent reinforcement learning. COE \cite{zhao2023conditionally} integrates optimism and exploration by constructing Upper Confidence bounds applied to Trees (UCT), helping policy converge to the global optimal solution. Nonetheless, tree-based methods face scalability limitations in large-scale multi-agent environments. Developing simple and effective guided exploration strategies remains a significant challenge in multi-agent reinforcement learning.
 
\section{Preliminaries}
\subsection{Dec-POMDP}
Multi-agent reinforcement learning problems are typically modeled as Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs). A Dec-POMDP is defined by the tuple $\langle I,S,A,P,R,O \rangle$, where $I$  represents the set of agents, $S$ denotes the set of global states. $A$ represents the set of all possible actions. The state transition function $P(s'|s,a_1,...,a_n):S \times A^n \rightarrow S$ describes the transition probability distribution over global states $S$. The reward function $R(r|s,a_1,...,a_n):S\times A^n \rightarrow \mathbb{R}$ specifies the rewards from the environment. $O$ represents the set of local observations for the agents. Agents aim to find a joint policy $\pi(a_1,a_2,...,a_n|s,o) = \{\pi_1(a_1|o_1),...,\pi_n(a_n|o_n)\}$ that maximizes the discounted reward $R_{tot}=\sum_{t=0}^{\infty} \gamma^t R_t(r|s,a_1,...,a_n)$, where $\gamma \in [0,1]$ is the discount factor over time.

\subsection{Value Decomposition}

The value decomposition method implements the CTDE paradigm in value-based multi-agent reinforcement learning. This approach introduces
a global state-action value function $Q_{tot}(\mathbf{\tau},\mathbf{a})$ for the entire team, along with several local state-action value functions $Q_{i}(\tau_i,a_i)$ for each agent, where $\tau$ and $\tau_i$ represent joint and individual action-observation histories. The global function can be factored into the local functions through an aggregation function, expressed as $Q_{tot}(\mathbf{\tau},\mathbf{a})=g(Q_{1}(\tau_1,a_1),...,Q_{n}(\tau_n,a_n))$. To ensure consistency between global and local estimations, the aggregation function  $g(\cdot)$ should satisfy the Individual-Global-Max (IGM) condition. The mathematical representation of the IGM condition is as follows:

\begin{equation} \label{eq:1}
    \mathop {arg\max} \limits_{a}Q_{tot}\left( {\tau },{a} \right) =\left( \begin{array}{c}
        \mathop {arg\max}_{a_1}Q_1\left( \tau _1,a_1 \right)\\
        \vdots\\
        \mathop {arg\max}_{a_n}Q_n\left( \tau _n,a_n \right)\\
    \end{array} \right) 
\end{equation}

An aggregation function that satisfies the IGM condition ensures that when the team's joint action maximizes the global state-action value function, the local state-action value functions of each agent are maximized as well. This allows each agent to achieve the joint optimal policy by greedily selecting the action that maximizes its local value, setting the stage for decentralized execution.

\subsection{$\epsilon$-Greedy Exploration}

$\epsilon$-Greedy Exploration is a simple and stochastic exploration strategy for balancing the exploration-exploitation trade-off in reinforcement learning. It relies on a parameter $\epsilon \in [0,1]$ to manage this balance. During the process of decision-making, each agent selects an action from available actions randomly with probability $\epsilon$, facilitating stochastic exploration. Conversely, with a probability of $1-\epsilon$, each agent chooses the action with the highest expected reward based on current information, thus exploiting current knowledge. Mathematically, $\epsilon$-Greedy Exploration constructs an action selection strategy leveraging the existing value estimates as follows:

\begin{equation} \label{eq:3}
    \begin{aligned}
        \pi \left( a|s \right) =\begin{cases}
            1-\epsilon \,\,+\frac{\epsilon}{|a|}&		if\,\,a\,\,=\,\,arg\max Q\left( s,a \right)\\
            \frac{\epsilon}{|a|}&		if\,\,a\,\,\ne \,\,arg\max Q\left( s,a \right)\\
        \end{cases}
    \end{aligned}
\end{equation}

\section{Methodology}

In this section, we analyze the process of parameter optimization, indicating that exploration strategies diverging from the optimal action cause inaccurate estimations. We introduce an optimistic function proven to converge in probability to the optimal solution.  Building on this, we propose the Optimistic $\epsilon$-Greedy Exploration strategy to prioritize frequent sampling of optimal action during exploration.

\subsection{Analysis of Parameter Optimization}

Our motivation arises from the challenges of parameter optimization in value decomposition methods. These methods introduce several local value functions $Q_i(\tau_i,a_i;\theta_i)$, which are combined into a global value function $Q_{tot}(\mathbf{\tau},\mathbf{a};\theta_{tot})$ through an aggregation function $g(Q_1,...,Q_n,s;\theta_{tot})$  to estimate reward during training. In online multi-agent reinforcement learning, trajectories gathered from exploration are inserted into the experience replay buffer, and sampled data from this buffer is used to optimize the parameters. During each training step, the neural networks update their parameters $\theta$ to minimize the following loss function:

\begin{equation} \label{eq:5}
\theta =\underset{\theta}{arg\min}\frac{1}{|B|}\cdot\sum_B{[Q_{tot}(\mathbf{\tau},\mathbf{a};\theta) - y_{target}]^2} 
\end{equation}

%TODO:CHECK
$B$ represents the data sampled from the experience replay buffer, and $y_{target} = r+\gamma \max_{a_{t+1}} Q_{tot}(\tau_{t+1}, \mathbf{a}_{t+1})$ denotes the temporal difference target. Since the amount of data stored in the replay buffer is insignificant compared to the total amount from the entire training process, we can approximate that the data in the buffer is sampled based on a similar exploration strategy denoted as $\pi(\mathbf{a}|\mathbf{\tau})$. According to the relationship between probability and frequency, the frequency of each joint action in the buffer is approximately $|B|\cdot\pi(\mathbf{a}|\mathbf{\tau})$. Based on this analysis, we can reformulate Eq. \eqref{eq:5} in the following form:

\begin{equation} \label{eq:6}
    \theta =\underset{\theta}{arg\min}\sum_{\mathbf{a}\in A^n}{\pi(\mathbf{a}|\mathbf{\tau})[Q_{tot}(\mathbf{\tau},\mathbf{a}) -  y_{target}]^2} 
\end{equation}

However, Eq. \eqref{eq:6} reveals a critical flaw in the process of parameter optimization. For each joint action $\mathbf{a}$, the loss function is always weighted by the exploration strategy $\pi(\mathbf{a}|\mathbf{\tau})$. If this strategy fails to sample optimal joint action with high probability, it may lead to an underemphasis on aligning the global value function with the optimal rewards. We further examine this flaw's impact on the local value functions of each agent. In the CTDE paradigm, exploration strategies of each agent are independent, i.e., $\pi(\mathbf{a}|\mathbf{\tau})=\prod_{i=1}^n{\pi(a_i|\tau_i)}$. Eq. \eqref{eq:6} can therefore be reformulated as follows:

\begin{equation} \label{eq:7}
    \begin{aligned}
        \theta =&\underset{\theta}{arg\min}\sum_{\mathbf{a}\in A^n}{\pi (\mathbf{a}|\mathbf{\tau })L(\mathbf{\tau },\mathbf{a})}\\
         =&\underset{\theta}{arg\min}\sum_{\mathbf{a}\in A^n}{\pi (a_i|\tau _i)\pi (a_{-i}|\tau _{-i})}L(\mathbf{\tau },\mathbf{a})\\
         =&\underset{\theta}{arg\min}\sum_{a_i\in A}{\pi (a_i|\tau _i)\sum_{a_{-i}\in A^{n-1}}{\pi (a_{-i}|\tau _{-i})L(\mathbf{\tau },\mathbf{a})}}\\
    \end{aligned}
\end{equation}

Here $L(\mathbf{\tau },\mathbf{a}) = [Q_{tot}(\mathbf{\tau },\mathbf{a})-y_{target}]^2$ for short. Eq. \eqref{eq:7} illustrates how exploration strategy affects the local value estimation for a single agent. For a given action $a_i$ of agent $i$, the optimization process of the global value function $Q_{tot}(\mathbf{\tau },(a_i,a_{-i}))$ is influenced by the exploration strategies of other agents. If other agents prefer actions that result in lower rewards, $Q_{tot}(\mathbf{\tau },(a_i,a_{-i}))$
 will be aligned with less emphasis on higher values. Constrained by the monotonic aggregation function, the local value function $Q_i(\tau _i,a_i)$ of agent $i$ will also be limited to a lower estimation, potentially trapping the value estimation in a suboptimal solution.

Fortunately, Eq. \eqref{eq:7} also suggests solutions to this limitation. Two critical factors: the exploration strategy $\pi(\mathbf{a}|\mathbf{\tau})$ and the aggregation function $Q_{tot}(\mathbf{\tau},\mathbf{a})$ have a significant impact during the process of parameter optimization. Optimizing through the design of the aggregation function is challenging, as it must account for all joint actions in replay buffer to effectively handle each situation. In this paper, we focus on the exploration strategy, proposing an optimistic approach to encourage agents to sample optimal actions more frequently, with the aim of alleviating the deficiency in parameter optimization by increasing the corresponding weights.

\subsection{Optimistic Update}

To increase the frequency of sampling the optimal action during exploration, it is crucial to identify it first. Intuitively, the optimal action yields the highest possible reward compared to other actions, providing an opportunity to recognize it. The highest reward can be detected through an optimistic approach. A function update process is defined as ``optimistic" if and only if, for a given input $x$, the updated function value is not less than its previous value. The mathematical definition of ``optimistic" is formalized as follows:

\begin{definition}\label{def:1}
Given an input $x$ and an ordered sequence of functions $\{f_1(x),f_2(x),...,f_n(x)\}$, this sequence is defined as ``optimistic update" if and only if $\forall i<j$, the inequality $f_i(x)\leq f_j(x)$ holds.
\end{definition}

For any possible decomposition of the maximum reward $R(s,\mathbf{a}^*)=\sum r_i(a_i^*) $, the reward for other joint actions can similarly be decomposed as $R(s,\mathbf{a})=\sum [r_i(a_i^*)-\frac{R(s,\mathbf{a}^*)-R(s,\mathbf{a})}{|I|}]$, where $|I|$ is the number of agents. Such individual decomposition terms are denoted as $r_i(a_i|a_{-i})$. Under this decomposition, for any non-optimal action $a_i\neq a_i^*$, it holds that $\max_{a_{-i}} r_i(a_i|a_{-i}) < \max_{a_{-i}} r_i(a_i^*|a_{-i})$. In other words, the maximum value of the reward decomposition terms for non-optimal actions is always smaller than the corresponding terms for optimal actions. We introduce a set of optimistic estimation functions $\{f_i(x)\}$ to identify the maximum value of the reward decomposition terms. For a given input $x=a_i$, it is updated as follows:

\begin{equation} \label{eq:10}
    \begin{aligned}
        f_i^{t+1}\left( x \right)= \begin{cases}
            f_i^t\left( x \right) +\alpha \left( r_i^t-f_i^t\left( x \right) \right)&		if\,\,r_t>f_i^t\left( x \right)\\
            f_i^t\left( x \right)&		else\\
        \end{cases}
    \end{aligned}
\end{equation}

$\alpha\in [0,1]$ is a predefined hyperparameter defines the learning rate of the function $f_i(x)$, and $r_i^t$ denotes the decomposition term $r_i(a_i|a_{-i})$ at timestep $t$. Since variations in the analysis of the function properties across different $i$ do not affect the subsequent discussion, we simplify $f_i^t$ and $r_i^t$ to $f_t$ and $r_t$. Noting that the updating process, as defined by Eq. \eqref{eq:10}, satisfies the optimistic update condition stated in Definition \ref{def:1}, since each step either increments by a value greater than zero or remains unchanged. 

The upper and lower bounds of the function presented in Eq. \eqref{eq:10} can be established using the following lemma:

\begin{lemma}\label{lemma:1}
For the sequence $\{f_t(x)\}$ defined in Eq. (6), if initialized with $f_0(x)\leq r_{max}$, where $r_{max}=\max_t r_t$, then $\forall t\geq0$, it holds that $f_t(x)\leq r_{max}$.
\end{lemma}

\begin{lemma}\label{lemma:2}
    Given a sequence of functions that updating in the following form:
    \begin{equation} \label{eq:11}
	\begin{aligned}
		f_{t+1}^{'}\left( x \right)= \begin{cases}
		      f_t^{'}\left( x \right) +\alpha \left( r_t-f_t^{'}\left( x \right) \right)&		if\,\,r_t=r_{max}\\
				f_t^{'}\left( x \right)&		else\\
		\end{cases}
	\end{aligned}
    \end{equation}
    For the sequence $\{f_t(x)\}$ defined in Eq. (6), if initialized with $f_0(x) = f_0^{'}(x)$, then $\forall t\geq 0$,it holds that $f_t(x)\geq f_t^{'}(x)$.
\end{lemma}

Lemma \ref{lemma:1} provides an upper bound for the function $f_t(x)$, as each update term is not larger than the difference between the current value and the maximum decomposition term. Lemma \ref{lemma:2} provides a lower bound since the function updates with any larger value result in a faster increase than updates that use only the maximum value. More formal proofs are provided in the Section A of Supplementary Materials.

The lower bound of $f_t(x)$ is represented using a function sequence. To analyze its properties, we transform this sequence into its mathematical expectation. Let event $A$ denote the situation where, during the updating process, other agents precisely choose the optimal actions, leading to $r_t=r_{max}$. The probability of this event is expressed as 
 $P\{a_{-j}=a_{-j}^{*}|x=a_i\}=c$, where $c\neq 0$. The indicator function $\mathbb{I}(A)$ measures how many times event $A$ occurs, and $f_{\mathbb{I}(A)=i}^{'}(x)$ represents  the function value of $f^{'}(x)$ after event $A$ has occurred $i$ times. It holds that $f_{\mathbb{I}(A)=n}^{'}(x)=r_{max}+(1-\alpha)^n[f_0^{'}(x)-r_{max}]$. We provide a formal proof for this formula in the Section A of Supplementary Materials. The expectation of $f_t^{'}(x)$ can be computed as follows:

\begin{equation}
    \begin{aligned}
        E[f_t^{'}(x)]&=\sum_{n=0}^t{P(\mathbb{I}(A)=n)f_{\mathbb{I}(A)=n}^{'}(x)}\\
        &=\sum_{n=0}^t{\binom{t}{n}  c^n\left( 1-c \right) ^{t-n}f_{\mathbb{I}(A)=n}^{'}(x)}\\
        &=r_{max}\sum_{n=0}^t{\binom{t}{n}  c^n\left( 1-c \right) ^{t-n}}+\\
        &+[f_0^{'}(x)-r_{max}]\sum_{n=0}^t{\binom{t}{n} (c-c\alpha)^n\left( 1-c \right) ^{t-n}}\\
        &=r_{max} + [f_0^{'}(x)-r_{max}](1-c\alpha)^t
    \end{aligned}
\end{equation}

Utilizing Lemma \ref{lemma:1} and Lemma \ref{lemma:2}, along with the mathematical expectation of Eq. \eqref{eq:11} and Markov's Inequality, we establish the following theorem:

\begin{theorem}\label{theorem:1}
    Consider the sequence of functions $\{f_t(x)\}$, initialized with $f_0(x)\leq r_{max}$ and updated according to Eq. \eqref{eq:10}. It holds that $\{f_t(x)\}$ converges in probability to $r_{max}$, i.e.,$f_t\left( x \right) \xrightarrow{p}r_{max}$ as $t\rightarrow\infty$.
\end{theorem}

\begin{proof}
    $\forall \varepsilon > 0$, we have
    \begin{equation}
        \begin{aligned}
            P(|r_{max}-f_t(x)|\geq \varepsilon) &= P(r_{max}-f_t(x)\geq \varepsilon)\\
            &\leq P(r_{max}-f_t^{'}(x)\geq \varepsilon)\\
            &\leq \frac{E[r_{max}-f_t^{'}(x)]}{\varepsilon}\\
            &=\frac{r_{max}-E[f_t^{'}(x)]}{\varepsilon}\\
            &=\frac{[f_0^{'}(x)-r_{max}](1-c\alpha)^t}{\varepsilon}\\
            &\xrightarrow{t\rightarrow +\infty}0
        \end{aligned}
    \end{equation}

    Based on the definition of convergence in probability, we have that $f_t(x)$ converges in probability to $r_{max}$, i.e., $f_t\left( x \right) \xrightarrow{p}r_{\max}$.
    
\end{proof}

Theorem \ref{theorem:1} demonstrates that the sequence of functions updated according to Eq. \eqref{eq:10} converges in probability to the maximum value $\max_{a_{-i}} r_i(a_i|a_{-i})$ of the corresponding action $a_i\in A$. This enables the identification of the optimal action $a_i^*$ by comparing the function values of different actions. Since the team only receives the total reward feedback in practice, we exploit the joint additivity property
 of convergence in probability by updating the sum of all optimistic functions toward the total reward. As a result, the values to which all optimistic functions converge at the optimal actions form a reward decomposition that is consistent with the aforementioned analysis. This ensures that optimistic exploration can still be performed by selecting the actions that maximize the optimistic function values.

\subsection{Overall Framework}

\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=0.75\textwidth]{fig/fig1.png}
    \caption{Overall framework of Optimistic $\epsilon$-Greedy Exploration}
    \label{fig:1}
\end{figure*}

Based on the analysis of the properties of optimistic updates, we propose the Optimistic $\epsilon$-Greedy Exploration strategy as an enhancement to traditional exploration methods. Similar to traditional $\epsilon$-greedy exploration, this strategy selects the action estimated to have the highest value with a probability of $1-\epsilon$. The key difference lies in the $\epsilon$ probability: instead of selecting actions randomly, the Optimistic $\epsilon$-Greedy Exploration samples actions based on the values of the optimistic functions, aiming to prioritize the selection of optimal action and improve exploration efficiency.

The right side of Figure \ref{fig:1} depicts the data flow and structure of the Optimistic $\epsilon$-Greedy Exploration strategy. Each agent is equipped with two neural networks: the Agent network and the Opt network. The Agent network computes the expected reward $Q_i(o_i,a_i)$ for each agent, while the Opt network calculates the optimistic estimation $f_i(s,o_i,a_i)$. The Agent network takes the agent’s observation as input, while the Opt network uses both the observation and the environment state as inputs. An MLP is first used to extract features from the input, followed by a GRU to integrate trajectory information into the feature representation. This feature representation is then processed by another MLP to produce the estimation for each action. 

During exploration, agents select the action $\hat{a}^*$ that maximizes $Q_i(o_i,a_i)$ with probability $1-\epsilon$, while they sample actions based on $f_i(s,o_i,a_i)$ with probability $\epsilon$. While any monotonic function can theoretically be used to map the optimistic estimations to a distribution on the action space, we adopt the Softmax function for this purpose. In summary, agents determine their actions during exploration according to the following probabilities:

\begin{equation} \label{eq:12}
    \begin{aligned}
        \pi_i \left( a|s \right) =\begin{cases}
            1-\epsilon \,\,+\epsilon Softmax(f_i(a))&		if\,\,a=\hat{a}^*\\
            \epsilon Softmax(f_i(a))&		if\,\,a\ne \hat{a}^*\\
        \end{cases}
    \end{aligned}
\end{equation}

When Eq. \eqref{eq:12} is used as the sampling strategy for exploration, the optimal action has a higher selection probability than in the traditional $\epsilon$-greedy exploration strategy (Eq. \eqref{eq:3}). This indicates that the Optimistic $\epsilon$-Greedy Exploration strategy improves the chances of selecting the globally optimal action during exploration, thereby mitigating the limitations of parameter optimization. A formal proof of this theorem is provided in the Section A of Supplementary Materials.

The left side of Figure \ref{fig:1} illustrates the training process of the neural networks. During the forward computation, $Q_i(o_i,a_i)$ is aggregated into the global estimation $Q_{tot}(\mathbf{o},\mathbf{a})$ through the Mixing network, which can be any aggregation function subject to underestimation, as shown in Eq. \eqref{eq:add1}. The optimistic estimations $f_i(s,o_i,a_i)$ are calculated through summation to produce the global optimistic estimation $f_{tot}(s,\mathbf{o},\mathbf{a})$, as shown in Eq. \eqref{eq:add2}.

\begin{equation}\label{eq:add1}
    Q_{tot}(\mathbf{o},\mathbf{a}) = g(Q_1(o_1,a_1),...,Q_n(o_n,a_n))
\end{equation}

\begin{equation}\label{eq:add2}
    f_{tot}(s,\mathbf{o},\mathbf{a}) = \sum_{i=0}^n{f_i(s,o_i,a_i)}
\end{equation}

The parameters of the Agent network are optimized using the temporal difference loss function, as shown in Eq. \eqref{eq:13}. Since the OPT network requires optimistic updates, a weighted temporal difference loss function is employed, as shown in Eq. \eqref{eq:14}. Specifically, the weight $w$ is set to 1 when $ y_{target}>f_{tot}(s,\mathbf{o},\mathbf{a})$, and to 0 when $y_{target}<f_{tot}(s,\mathbf{o},\mathbf{a})$. This masking ensures the loss function is masked whenever the optimistic estimation exceeds the target, thereby enabling monotonic increasing updates. However, considering that the target includes neural network-based estimations, the weight is set to a hyperparameter instead of 0 in practical implementation to enhance the robustness of the optimistic network.

\begin{equation} \label{eq:13}
L_{td}=[Q_{tot}(\mathbf{o},\mathbf{a})-y_{target}]^2
\end{equation}
	
\begin{equation} \label{eq:14}
L_{opt}=w[f_{tot}(s,\mathbf{o},\mathbf{a})-y_{target}]^2
\end{equation}

Building on this foundation, we introduce an unconstrained neural network to better estimate the expected reward. This network takes the state, observation, and joint action as inputs, estimating the expected reward $Q_{jt}(s,\mathbf{o},\mathbf{a})$ under the given conditions. Such a design enables the neural network to explore a broader parameter space to map inputs to the expected reward. The temporal difference target is calculated using Eq. \eqref{eq:15}. Inspired by the Double Q-learning approach, we leverage the estimations corresponding to the joint optimal action $\hat{\textbf{a}}_{t+1}^* = arg\max_{\textbf{a}_{t+1}}Q_{tot}(\mathbf{o}_{t+1},\mathbf{a}_{t+1})$, to approximate the maximum reward for the next state. This design avoids an exhaustive search over the exponential joint action space. $Q_{jt}(s,\mathbf{o},\mathbf{a})$ is updated using the temporal difference loss function defined in Eq. \eqref{eq:16}.

\begin{equation} \label{eq:15}
    y_{target} = r+\gamma Q_{jt}(s_{t+1},\mathbf{o}_{t+1},\hat{\textbf{a}}_{t+1}^*)
\end{equation}
	
\begin{equation} \label{eq:16}
    L_{jt}=[Q_{jt}(s,\mathbf{o},\mathbf{a})-y_{target}]^2
\end{equation}

The overall objective of parameter optimization is to minimize the combined loss functions defined in Eq. \eqref{eq:13}, Eq. \eqref{eq:14}, and Eq. \eqref{eq:16}, as formulated in Eq. \eqref{eq:17}.

\begin{equation} \label{eq:17}
    L=L_{td}+L_{opt}+L_{jt}
\end{equation}

\section{Experiment}

\begin{table*}[t]
    \centering
    \begin{tabular}{ccc}
        \begin{subtable}[t]{0.3\textwidth}
            \centering
            \begin{tabular}{|c|c|c|c|}
				\hline
				    & $a_1$ & $a_2$ & $a_3$ \\
				\hline
				$a_1$ & 8 & -12 & -12 \\
				\hline
				$a_2$ & -12 & 0 & 0 \\
				\hline
				$a_3$ & -12 & 0 & 0 \\
				\hline
            \end{tabular}
            \caption{Payoff Matrix}
        \end{subtable}
        &
        \begin{subtable}[t]{0.3\textwidth}
            \centering
            \begin{tabular}{|c|c|c|c|}
				\hline
				  & $a_1$ & $a_2$ & $a_3$ \\
				\hline
				$a_1$ & -7.76 & -7.76 & -7.76 \\
				\hline
				$a_2$ & -7.76 & \textcolor{blue}{0.04} & 0.02 \\
				\hline
				$a_3$ & -7.76 & 0.02 & 0.01 \\
				\hline
            \end{tabular}
            \caption{QMIX}
        \end{subtable}
        &
        \begin{subtable}[t]{0.3\textwidth}
            \centering
            \begin{tabular}{|c|c|c|c|}
				\hline
				  & $a_1$ & $a_2$ & $a_3$ \\
				\hline
				$a_1$ & \textcolor{red}{7.8} & -6.38 & -6.32 \\
				\hline
				$a_2$ & -6.22 & -6.40 & -6.40 \\
				\hline
				$a_3$ & -6.11 & -6.40 & -6.39 \\
				\hline
            \end{tabular}
            \caption{OPT-QMIX}
        \end{subtable}
    \end{tabular}
    \caption{Experimental Results of Matrix Game\label{table:1}}
\end{table*}

In this section, we integrate the Optimistic $\epsilon$-Greedy Exploration strategy into QMIX, a prominent multi-agent reinforcement learning framework, creating an enhanced version referred to as OPT-QMIX. We first compare OPT-QMIX with two widely adopted multi-agent reinforcement learning baseline algorithms, QMIX and VDN, to validate the effectiveness of the proposed approach. Subsequently, we extend the comparison to advanced variants of these baselines, including OW-QMIX and CW-QMIX (variants of QMIX) as well as QTRAN (an variant of VDN), to demonstrate that our approach  achieves more significant performance improvements over these state-of-the-art methods. All algorithms compared in this section are implemented using the open-source code and hyperparameter configurations provided by the original authors. Detailed information on the hyperparameter settings is provided in the Section B of Supplementary Materials.

\subsection{Matrix Game}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\textwidth]{fig/fig2.png}
    \caption{Experimental Results of Matrix Game and Predator Prey}
    \label{fig:2}
\end{figure}

To verify whether our approach can effectively help policies escape suboptimal solutions, we conducted experiments on a specifically designed matrix game, comparing OPT-QMIX with other algorithms. The payoff matrix for this game is presented in Table \ref{table:1}(a). In this environment, if any agent fails to choose the optimal action (i.e., $a_1$), the team reward decreases sharply. Such a drastic reduction in reward can cause algorithms to underestimate the expected reward of the optimal action, leading to convergence on suboptimal policies. This matrix game is widely recognized as a classic benchmark of the relative overgeneralization problem.

Table \ref{table:1}(b) and (c) compare the value estimations of different actions between the traditional QMIX and the optimistic OPT-QMIX. As illustrated, QMIX underestimates the value of the optimal action and converges to a suboptimal solution due to relative overgeneralization, whereas OPT-QMIX accurately identifies the optimal action. This highlights the effectiveness of our approach in guiding the policy toward the globally optimal solution by increasing the likelihood of sampling the optimal action. Figure \ref{fig:2}(Left) depicts the rewards obtained from all algorithms throughout the training process. Like QMIX, VDN also converges to a suboptimal solution, while the enhanced algorithms OW-QMIX and CW-QMIX successfully attain the optimal rewards. QTRAN is theoretically capable of identifying the optimal action \cite{son2019qtran}. However, its overly complex neural network structure poses challenges to recognizing the optimal action efficiently.



\subsection{Predator and Prey}

To further investigate the impact of the Optimistic $\epsilon$-Greedy Exploration strategy on value estimation, we conducted additional experiments on OPT-QMIX and other algorithms using the Predator-Prey game. The Predator-Prey game simulates a scenario involving multiple predators and prey, where the predators are controlled by reinforcement learning agents that receive locally observable information and decide whether to take a ``move" or ``capture" action. In this game, a single predator attempting to ``capture" prey causes the prey to escape, resulting in a negative reward for the team, whereas multiple predators ``capture" the same prey at the same time leads to a successful capture and a positive team reward. As in the matrix game, the reward received for a given action can vary depending on the actions chosen by other agents, introducing significant challenges for accurate value estimation.

Figure \ref{fig:2}(Right) illustrates the rewards achieved by different algorithms in the Predator-Prey game throughout the training process. It can be observed that QMIX struggles to effectively learn the optimal policy. The varying rewards for the same action create confusion, which ultimately causes its policy to converge to a state of “doing nothing,” with a resulting reward of 0. In contrast, OPT-QMIX successfully guides the policy toward the global optimum, gradually converging to the maximum achievable reward in the environment in the process of training. Likewise, enhanced algorithms such as OW-QMIX and CW-QMIX also demonstrate the ability to identify the globally optimal policy.

\subsection{StarCraft II}
\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/fig3.png}
    \caption{Experimental Results of StarCraft Multi-Agent Challenge}
    \label{fig:3}
\end{figure*}

We evaluate Optimistic $\epsilon$-Greedy Exploration strategy in the widely used StarCraft Multi-Agent Challenge (SMAC) environment to examine its capability in handling complex situations. Our evaluations are deployed on maps ``3s5z”, ``1c3s5z", ``5m\_vs\_6m", ``8m\_vs\_9m", ``10m\_vs\_11m" and ``MMM2" that cover a range of difficulty levels rated as ``Easy," ``Hard," and ``Super Hard" in the SMAC benchmark. To further examine the influence of exploration on algorithm performance, we extended the process of exploration from the original 50k timesteps to 1m, as \cite{rashid2020weighted} does. Figure \ref{fig:3} shows the win rates of all algorithms across different maps throughout the training process. VDN and QMIX demonstrate limited robustness to exploration due to the constraints of their aggregation functions, which hampers their performance. In contrast, OPT-QMIX consistently achieves higher win rates than QMIX. When compared with other enhanced algorithms, OPT-QMIX not only exhibits faster learning rates but also achieves higher win rates. This advantage stems from its ability to actively utilize exploration to guide policy learning, rather than focusing on structural modifications as other algorithms do, further supporting the effectiveness of our framework.

\section{Conclusion}

In this paper, we propose the Optimistic $\epsilon$-Greedy Exploration strategy, which integrates an optimistic network updated in a monotonically increasing manner and is theoretically proven to converge in probability to the optimal reward. This strategy leverages optimistic estimations to encourage the agent to choose optimal actions more frequently. We evaluate the proposed strategy in multiple experimental environments. The results demonstrate that Optimistic $\epsilon$-Greedy Exploration effectively steers the policy toward the global optimal solution. Furthermore, it achieves significantly greater performance compared to other algorithms.

From a future development perspective, optimistic methods show promise in long-term training but face early-stage instability due to neural network convergence issues. Incorporating prior knowledge, like task representations, can stabilize training, accelerate learning, and enable integrating large language models into multi-agent reinforcement learning. This marks a promising research direction.


% \section*{Acknowledgments}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}


\appendix

\section{Proof of Lemmas and Theorems}

% In this section, we provide more formal proofs for the lemmas and theorems presented in the main content.

% To avoid confusion, lemmas and theorems will be numbered following the labels in the main content, unless they have already been numbered in the main content.

\setcounter{theorem}{1}
\setcounter{lemma}{0}

% \begin{theorem}
%     Given a function sequence that is updating according to Eq. (6), it satisfies the optimistic update condition stated in Definition 1.
% \end{theorem}

\begin{lemma}
For the sequence $\{f_t(x)\}$ defined in Eq. (6), if initialized with $f_0(x)\leq r_{max}$, where $r_{max}=\max_t r_t$, then $\forall t\geq0$, it holds that $f_t(x)\leq r_{max}$.
\end{lemma}

\begin{proof}

    We prove the lemma by employing the method of mathematical induction.

    When $t=0$, since the sequence is initialized with $f_0(x)\leq r_{max}$, the base case holds.

    Assume that the lemma holds true for some arbitrary positive integer $t$, that is $f_t(x)\leq r_{max}$. We will now show that the lemma holds for $t+1$. $f_{t+1}(x)$ satisfies:

    \begin{equation*}
        \begin{aligned}
            f_{t+1}(x) &= f_t(x) + \alpha( r_t-f_t( x ))\\
            &\leq f_t(x) + r_t-f_t( x )     & (\alpha \in [0,1])\\
            &\leq r_{t}\\
            &\leq r_{max}    & (r_t(a) \leq r_t(a^*))
        \end{aligned}
    \end{equation*}

    Thus, the lemma holds for $t+1$. By the principle of mathematical induction, the lemma holds for all $t \geq 0$.
    
\end{proof}

\begin{lemma}
    Given a sequence of functions that updating in the following form:
    \begin{equation*}
	\begin{aligned}
		f_{t+1}^{'}\left( x \right)= \begin{cases}
		      f_t^{'}\left( x \right) +\alpha \left( r_t-f_t^{'}\left( x \right) \right)&		if\,\,r_t=r_{max}\\
				f_t^{'}\left( x \right)&		else\\
		\end{cases}
	\end{aligned}
    \end{equation*}
    For the sequence $\{f_t(x)\}$ defined in Eq. (6), if initialized with $f_0(x) = f_0^{'}(x)$, then $\forall t\geq 0$,it holds that $f_t(x)\geq f_t^{'}(x)$.
\end{lemma}

\begin{proof}
    We prove the lemma by employing the method of mathematical induction.

    When $t=0$, since the sequence is initialized with $f_0(x) = f_0^{'}(x)$, the base case holds.

    Assume that the lemma holds true for some arbitrary positive integer $t$, that is $f_t(x)\geq f_t^{'}(x)$. We will now show that the lemma holds for $t+1$.

    if $r_t\leq f_t^{'}(x)$, $f_{t+1}(x)$ and $f_{t+1}^{'}(x)$ satisfies:

    \begin{equation*}
	\begin{aligned}
		f_{t+1}(x) = f_t(x)\geq f_t^{'}(x) = f_{t+1}^{'}(x)
	\end{aligned}
    \end{equation*}

    if $f_t^{'}(x) < r_t \leq f_t(x)$, $f_{t+1}(x)$ and $f_{t+1}^{'}(x)$ satisfies:

    \begin{equation*}
	\begin{aligned}
		f_{t+1}(x) &= f_t(x)\\
		&\geq r_t\\
		&\geq r_t + f_t^{'}(x) -f_t^{'}(x)\\
		&\geq f_t^{'}(x) + \alpha(r_t - f_t^{'}(x))    & (\alpha \in [0,1])\\
		&=f_{t+1}^{'}(x)
	\end{aligned}
    \end{equation*}

    if $r_t > f_t(x)$, $f_{t+1}(x)$ and $f_{t+1}^{'}(x)$ satisfies:

    \begin{equation*}
	\begin{aligned}
		f_{t+1}(x) &= f_t(x)+\alpha(r_t - f_t(x))\\
		&=\alpha r_t + (1-\alpha)f_t(x)\\
		&\geq \alpha r_t + (1-\alpha)f_t^{'}(x)\\
		&=f_{t+1}^{'}(x)
	\end{aligned}
    \end{equation*}

    Therefore, regardless of the value of $r_t$, it always holds that $f_{t+1}(x)\geq f_{t+1}^{'}(x)$. By the principle of mathematical induction, the lemma holds for all $t \geq 0$.
    
\end{proof}

\begin{lemma}\label{lemma:3}
For the sequence $\{f^{'}_t(x)\}$ defined in Eq. (7), the function value of $f^{'}(x)$ after event $A$ has occurred $i$ times has a general term expressed as:

\begin{equation*}
    f_{\mathbb{I}(A)=n}^{'}(x)=r_{max}+(1-\alpha)^n[f_0^{'}(x)-r_{max}]
\end{equation*}

Where event $A$ denotes the situation $r_t=r_{max}$, and the probability of this event is $P\{r_t=r_{max}|x=a_i\}=c\neq0$.

\end{lemma}


\begin{proof}
    We prove the lemma by employing the method of mathematical induction.

    When $\mathbb{I}(A)=0$, the function value of $f_{\mathbb{I}(A)=0}^{'}(x)$ is equal to $f^{'}_0(x)$. Substituting $\mathbb{I}(A)=0$ into the formula, we have:

    \begin{equation*}
	\begin{aligned}
		f_{\mathbb{I}(A)=0}^{'}(x) &= r_{max}+(1-\alpha)^0[f_0^{'}(x)-r_{max}]\\
		&= r_{max}+f_0^{'}(x)-r_{max}\\
		&= f_0^{'}(x)
	\end{aligned}
    \end{equation*}

    Therefore, the base case holds.

    Assume that the formula holds true for some arbitrary positive integer $\mathbb{I}(A)=n$, We will now show that the formula holds for $\mathbb{I}(A)=n+1$. By the definition of $\{f^{'}_t(x)\}$, the function value of $f_{\mathbb{I}(A)=n+1}^{'}(x)$ can be expressed as:

    \begin{equation*}
	\begin{aligned}
		f_{\mathbb{I}(A)=n+1}^{'}(x) &= f_{\mathbb{I}(A)=n}^{'}(x)+\alpha \left( r_{max}-f_{\mathbb{I}(A)=n}^{'}(x) \right)\\
		&= \alpha r_{max} + (1-\alpha)f_{\mathbb{I}(A)=n}^{'}(x)\\
		&= \alpha r_{max} + (1-\alpha)\{r_{max}+\\
            &+(1-\alpha)^n[f_0^{'}(x)-r_{max}]\}\\
            &= r_{max} + (1-\alpha)^{n+1}[f_0^{'}(x)-r_{max}]
	\end{aligned}
    \end{equation*}

    Thus, the formula holds for $n+1$. By the principle of mathematical induction, the formula $f_{\mathbb{I}(A)=n}^{'}(x)=r_{max}+(1-\alpha)^n[f_0^{'}(x)-r_{max}]$ is true for all $\mathbb{I}(A)\geq0$
    
\end{proof}

\begin{table*}[t]
\centering % 表格居中
\renewcommand{\arraystretch}{1.5} % 可调整行间距，1.5 为行高倍数
\begin{tabular}{|c|c|c|c|c|c|} % 每列用 | 分隔
\hline
\textbf{Name} & \textbf{Ally Units} & \textbf{Enemy Units} & \textbf{Type} & \textbf{Difficulty}\\ % 表头
\hline
\hline
3s5z & 3 Stalkers \& 5 Zealots & 3 Stalkers \& 5 Zealots & heterogeneous \& symmetric & Easy \\ % 第二行

1c3s5z & \makecell{1 Colossi \& 3 Stalkers \\ \& 5 Zealots} & \makecell{1 Colossi \& 3 Stalkers\\ \& 5 Zealots} & heterogeneous \& symmetric & Easy\\ % 第三行

5m\_vs\_6m & 5 Marines & 6 Marines & homogeneous \& asymmetric & Hard\\ % 第四行

8m\_vs\_9m & 8 Marines & 9 Marines & homogeneous \& asymmetric & Hard\\ % 第五行

10m\_vs\_11m & 10 Marines & 11 Marines & homogeneous \& asymmetric & Hard\\ % 第六行

MMM2 & \makecell{1 Medivac \& 2 Marauders\\ \& 7 Marines} & \makecell{1 Medivac \& 3 Marauders \\ \& 8 Marines} & homogeneous \& asymmetric & Super Hard\\  % 第七行
\hline
\end{tabular}
\caption{Compositions and Difficulty Ratings of SMAC}
\label{tab:1}
\end{table*}

\begin{theorem}
Optimistic $\epsilon$-Greedy Exploration strategy samples the optimal action with a higher probability than the traditional $\epsilon$-greedy exploration strategy.
\end{theorem}

\begin{proof}
    We first prove $Softmax(f(a^*))\geq\frac{1}{|a|}$ by contradiction. Assume, for the sake of contradiction, that $Softmax(f(a^*))<\frac{1}{|a|}$. For all $a\neq a*$, it holds that $f(a)\leq f(a^*)$, we have:

    \begin{equation*}
	\begin{aligned}
		\sum_{a\in A}{Softmax(f(a))} &\leq \sum_{a\in A}{Softmax(f(a^*))}\\
        &= |a|Softmax(f(a))\\
        &< |a|\times \frac{1}{|a|}\\
        &= 1
	\end{aligned}
    \end{equation*}

    Now, we have shown that $\sum_{a\in A}{Softmax(f(a))}<1$. However, this contradicts the definition of Softmax which has $\sum{Softmax(\cdot)}=1$. Since assuming that $Softmax(f(a^*))<\frac{1}{|a|}$ leads to a contradiction, we conclude that $Softmax(f(a^*))\geq\frac{1}{|a|}$.

    Building upon this, we prove this theorem by considering all possible cases for the optimal action $a^*$

    \noindent \textbf{Case 1}:$a^* = arg\max Q(\tau,a)$

    The Optimistic $\epsilon$-Greedy Exploration strategy selects the optimal action with a probability of $1-\epsilon+\epsilon Softmax(a^*)$, whereas the traditional $\epsilon$-greedy exploration strategy selects the optimal action with a probability of $1-\epsilon+\frac{\epsilon}{|a|}$.Since it has already been proven that $Softmax(f(a^*))\geq\frac{1}{|a|}$,this demonstrates that the Optimistic $\epsilon$-Greedy Exploration strategy provides a higher probability of selecting the optimal action.

    \noindent \textbf{Case 2}:$a^* \neq arg\max Q(\tau,a)$

    The Optimistic $\epsilon$-Greedy Exploration strategy selects the optimal action with a probability of $\epsilon Softmax(a^*)$, whereas the traditional $\epsilon$-greedy exploration strategy selects the optimal action with a probability of $\frac{\epsilon}{|a|}$.Since it has already been proven that $Softmax(f(a^*))\geq\frac{1}{|a|}$,this demonstrates that the Optimistic $\epsilon$-Greedy Exploration strategy provides a higher probability of selecting the optimal action.

    In both cases, we have shown that the Optimistic $\epsilon$-Greedy Exploration strategy provides a higher probability of selecting the optimal action. Therefore, the theorem is true for all possible situations
    for the optimal action $a^*$
\end{proof}

\section{Experimental Setup}

We utilized PyMARL: Python Multi-Agent Reinforcement Learning framework \cite{samvelyan19smac}, to conduct our experiments. Each algorithm was evaluated on each environment using 9 random seeds. All experiments were carried out on a server with 12th Gen Intel(R) Core(TM) i5-12400 processor, 64 GB of RAM, and an NVIDIA GeForce RTX 4060 Ti GPU with 16 GB of memory.

The agent network architectures employed in the experiments were identical across all algorithms. Each Agent network consisted of an MLP with a hidden dimension of 64, a GRU with a hidden dimension of 64, and an additional MLP whose output dimension matched the number of actions. During evaluating, all algorithms selected the action with the highest estimation from the Agent network. Except for OPT-QMIX, all algorithms used the traditional $\epsilon$-greedy strategy during exploration.

For OPT-QMIX, OW-QMIX, and CW-QMIX, these algorithms employed the same $Q_{jt}$ network to estimate global values. Specifically, the $Q_{jt}$ network comprised a set of feature extraction networks, one for each agent, with structures identical to the Agent network. These feature extraction networks transformed observations and actions into corresponding features. The features were then combined with the global state $s$ and processed through an MLP to compute $Q_{jt}$.

Across all environments, the hyperparameter settings closely followed the default configurations of PyMARL. Specifically, the buffer size of experience replay buffer for all algorithms was set to 5000 during training. At each training step, a batch of 32 episodes was sampled from the replay buffer to train the neural networks. The network parameters were optimized using PyTorch's RMSProp optimizer, with the learning rate set to $5\times 10^{-4}$. When calculating the temporal difference loss, the discount factor $\gamma$ was set to 0.99.

For the Matrix Game, all agents were required to perform full exploration, i.e., $\epsilon=1$ , which remained constant throughout the training process. To ensure fairness, the $w$ of weighted temporal difference loss function for OPT-QMIX, OW-QMIX, and CW-QMIX was set to 0.01.

In the Predator-Prey Game, $\epsilon$ was linearly annealed from 1 to 0.05 over 200k timesteps. Similar to the matrix game, the $w$ of weighted temporal difference loss function for OPT-QMIX, OW-QMIX, and CW-QMIX was also set to 0.01. Notably, since the rewards in the Predator-Prey Game were relatively small, we normalized the optimistic estimations before generating the distribution using Softmax. The normalization range was $[0,\alpha]$, where $\alpha$ was linearly increased from 0 to 2 over the first 20k timesteps. This design encouraged agents to select actions randomly during the early stages when the optimistic network had not yet converged, while progressively enabling them to favor optimal action as the optimistic network matured.

For the StarCraft Multi-Agent Challenge (SMAC), we selected six maps as environments: ``3s5z," ``1c3s5z," ``5m\_vs\_6m," ``8m\_vs\_9m," ``10m\_vs\_11m," and ``MMM2." Table \ref{tab:1} provides the specific compositions and difficulty ratings of each environment. Across all environments, $\epsilon$ was linearly annealed from 1 to 0.05 over 1m timesteps. The difficulty level of the built-in heuristic algorithms for enemy units was set to 7 (Very Difficult). For all SMAC maps, the $w$ of weighted temporal difference loss function for OPT-QMIX, OW-QMIX, and CW-QMIX was set to 0.5. Similar to the Predator-Prey Game, the optimistic estimations were normalized within the range $[0,\alpha]$, where $\alpha$ was linearly increased from 0 to 1 over the first 50k timesteps.

\end{document}

