\section{Related Work}
Research in cooperative multi-agent reinforcement learning began with independent Q-learning ____. However, it struggles to effectively train cooperative policies, leading to suboptimal performance ____. To overcome this limitation, some approaches ____ treat all agents as a single entity. While effective with few agents, these approaches can't scale well to environments with larger numbers of agents due to the exponential growth of the joint action space. Centralized Training with Decentralized Execution (CTDE) was introduced to balance these challenges ____. This paradigm develops cooperative policies via global information during training, while allowing each agent to decide its action independently during execution. Within this paradigm, value decomposition methods like VDN ____ and QMIX ____ have performed exceptionally well in tasks such as StarCraft II ____. However, these methods impose monotonicity constraint to satisfy the IGM condition, potentially leading to inaccurate estimation in certain scenarios.

To address this issue, some approaches attempt to use more complex network structures for value estimation. QTRAN ____ introduces additional estimation terms to correct biases in value representation. ResQ ____ and RQN ____ employ residual-like networks to provide correction terms for each agent, mitigating impacts of irregular reward distribution. QPLEX ____ decouples local estimation into state value and action advantage to jointly estimate global value. However, incorporating additional neural network modules makes convergence more challenging. Furthermore, these algorithms rely on accurately identifying optimal actions, which can only be approximated through neural network estimation in practice. Experimental results ____ indicate that these algorithms often struggle to perform well due to instability in neural network training.

Other approaches attribute inaccurate estimation to insufficient exploration of the environment and enhance the exploration to refine estimation. LH-IRQN 
____ and DFAC ____ attempt to estimate uncertainty in multi-agent environments using classifications or quantile distributions. However, computing the distribution is challenging due to the mutual influence among agents. Some research ____ encourages exploration through intrinsic motivation methods, but the cost of computation makes these difficult to handle large-scale tasks. EITI ____ and VM3-AC ____ encourage exploration by maximizing mutual information between policies to manage a large number of agents. Unfortunately, exploration strategies based on mutual information are limited by the structure of algorithms and are difficult to generalize. Additionally, current exploration strategies focus on exhaustive state exploration, while neglecting its role in guiding policy training, making it difficult for algorithms to get rid of suboptimal solutions effectively.

With the aim of overcoming suboptimal solutions, some research focuses on identifying globally optimal actions to guide policy training. WQMIX ____ enhances policy training by assigning higher weights to optimal actions in the loss function. OVI-QMIX ____ minimizes the KL divergence between the current and optimal policies through optimistic instructors. However, these methods use approximate optima for guiding value estimation, leading to instability
during training. Preference-Guided Stochastic Exploration ____ introduces an additional exploration strategy based on the advantage of each action in single-agent reinforcement learning. COE ____ integrates optimism and exploration by constructing Upper Confidence bounds applied to Trees (UCT), helping policy converge to the global optimal solution. Nonetheless, tree-based methods face scalability limitations in large-scale multi-agent environments. Developing simple and effective guided exploration strategies remains a significant challenge in multi-agent reinforcement learning.