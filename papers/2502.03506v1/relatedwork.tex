\section{Related Work}
Research in cooperative multi-agent reinforcement learning began with independent Q-learning \cite{littman1994markov}. However, it struggles to effectively train cooperative policies, leading to suboptimal performance \cite{claus1998dynamics}. To overcome this limitation, some approaches \cite{foerster2017stabilising,foerster2018counterfactual} treat all agents as a single entity. While effective with few agents, these approaches can't scale well to environments with larger numbers of agents due to the exponential growth of the joint action space. Centralized Training with Decentralized Execution (CTDE) was introduced to balance these challenges \cite{lowe2017multi}. This paradigm develops cooperative policies via global information during training, while allowing each agent to decide its action independently during execution. Within this paradigm, value decomposition methods like VDN \cite{sunehag2017value} and QMIX \cite{rashid2020monotonic} have performed exceptionally well in tasks such as StarCraft II \cite{samvelyan2019starcraft}. However, these methods impose monotonicity constraint to satisfy the IGM condition, potentially leading to inaccurate estimation in certain scenarios.

To address this issue, some approaches attempt to use more complex network structures for value estimation. QTRAN \cite{son2019qtran} introduces additional estimation terms to correct biases in value representation. ResQ \cite{shen2022resq} and RQN \cite{pina2022residual} employ residual-like networks to provide correction terms for each agent, mitigating impacts of irregular reward distribution. QPLEX \cite{wang2020qplex} decouples local estimation into state value and action advantage to jointly estimate global value. However, incorporating additional neural network modules makes convergence more challenging. Furthermore, these algorithms rely on accurately identifying optimal actions, which can only be approximated through neural network estimation in practice. Experimental results \cite{papoudakis2020benchmarking} indicate that these algorithms often struggle to perform well due to instability in neural network training.

Other approaches attribute inaccurate estimation to insufficient exploration of the environment and enhance the exploration to refine estimation. LH-IRQN 
\cite{lyu2018likelihood} and DFAC \cite{sun2021dfac} attempt to estimate uncertainty in multi-agent environments using classifications or quantile distributions. However, computing the distribution is challenging due to the mutual influence among agents. Some research \cite{bohmer2019exploration,liu2021cooperative} encourages exploration through intrinsic motivation methods, but the cost of computation makes these difficult to handle large-scale tasks. EITI \cite{wang2019influence} and VM3-AC \cite{kim2020maximum} encourage exploration by maximizing mutual information between policies to manage a large number of agents. Unfortunately, exploration strategies based on mutual information are limited by the structure of algorithms and are difficult to generalize. Additionally, current exploration strategies focus on exhaustive state exploration, while neglecting its role in guiding policy training, making it difficult for algorithms to get rid of suboptimal solutions effectively.

With the aim of overcoming suboptimal solutions, some research focuses on identifying globally optimal actions to guide policy training. WQMIX \cite{rashid2020weighted} enhances policy training by assigning higher weights to optimal actions in the loss function. OVI-QMIX \cite{li2024optimistic} minimizes the KL divergence between the current and optimal policies through optimistic instructors. However, these methods use approximate optima for guiding value estimation, leading to instability
during training. Preference-Guided Stochastic Exploration \cite{huang2023sampling} introduces an additional exploration strategy based on the advantage of each action in single-agent reinforcement learning. COE \cite{zhao2023conditionally} integrates optimism and exploration by constructing Upper Confidence bounds applied to Trees (UCT), helping policy converge to the global optimal solution. Nonetheless, tree-based methods face scalability limitations in large-scale multi-agent environments. Developing simple and effective guided exploration strategies remains a significant challenge in multi-agent reinforcement learning.