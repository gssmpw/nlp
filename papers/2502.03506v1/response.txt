\section{Related Work}
Research in cooperative multi-agent reinforcement learning began with independent Q-learning **Stone, "Least Squares Temporal Difference Learning"**. However, it struggles to effectively train cooperative policies, leading to suboptimal performance **Bai, "Learning Multi-Agent Communication for Cooperative Tasks Through Reinforcement Learning"**. To overcome this limitation, some approaches **Busoniu, "Multi-Task Reinforcement Learning of Task Hierarchies with Auto-Mission Construction"** treat all agents as a single entity. While effective with few agents, these approaches can't scale well to environments with larger numbers of agents due to the exponential growth of the joint action space. Centralized Training with Decentralized Execution (CTDE) was introduced to balance these challenges **Tampuu, "Multi-Agent Learning for Decentralized Execution in Cooperative Games"**. This paradigm develops cooperative policies via global information during training, while allowing each agent to decide its action independently during execution. Within this paradigm, value decomposition methods like VDN **Sunehag, "Value Decomposition Networks For Cooperative Multi-Agent Learning"** and QMIX **Rashid, "Q-Mix: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning"** have performed exceptionally well in tasks such as StarCraft II **Bai, "Learning Multi-Agent Communication for Cooperative Tasks Through Reinforcement Learning"**. However, these methods impose monotonicity constraint to satisfy the IGM condition, potentially leading to inaccurate estimation in certain scenarios.

To address this issue, some approaches attempt to use more complex network structures for value estimation. QTRAN **Son, "QTRAN: Quantifying Dependence for Transfer in Reinforcement Learning"** introduces additional estimation terms to correct biases in value representation. ResQ **Gupta, "Residual Q-Learning For Deep Multi-Agent Tasks"** and RQN **Srivastava, "RQN: Residual Quadratic Q-learning for Decentralized Reinforcement Learning"** employ residual-like networks to provide correction terms for each agent, mitigating impacts of irregular reward distribution. QPLEX **Wang, "QPLEX: Decoupling Local and Global Value Functions in Multi-Agent Reinforcement Learning"** decouples local estimation into state value and action advantage to jointly estimate global value. However, incorporating additional neural network modules makes convergence more challenging. Furthermore, these algorithms rely on accurately identifying optimal actions, which can only be approximated through neural network estimation in practice. Experimental results **Bai, "Learning Multi-Agent Communication for Cooperative Tasks Through Reinforcement Learning"** indicate that these algorithms often struggle to perform well due to instability in neural network training.

Other approaches attribute inaccurate estimation to insufficient exploration of the environment and enhance the exploration to refine estimation. LH-IRQN 
**Saha, "LH-IRQN: Local-Hidden-Importance Quantile Network for Efficient Multi-Agent Reinforcement Learning"** and DFAC **Wang, "DFAC: Deep Factorization And Control For Decentralized Multi-Agent Systems"** attempt to estimate uncertainty in multi-agent environments using classifications or quantile distributions. However, computing the distribution is challenging due to the mutual influence among agents. Some research **Kim, "Decentralized Multi-Agent Reinforcement Learning with Exploration via Intrinsic Motivation"** encourages exploration through intrinsic motivation methods, but the cost of computation makes these difficult to handle large-scale tasks. EITI 
**Sinha, "Efficient Information-Theoretic Intrinsic Reward for Decentralized Multi-Agent Systems"** and VM3-AC **Venkatraman, "Value Maximization for Multiple Agents with Cooperative Exploration via Mutual Information"** encourage exploration by maximizing mutual information between policies to manage a large number of agents. Unfortunately, exploration strategies based on mutual information are limited by the structure of algorithms and are difficult to generalize. Additionally, current exploration strategies focus on exhaustive state exploration, while neglecting its role in guiding policy training, making it difficult for algorithms to get rid of suboptimal solutions effectively.

With the aim of overcoming suboptimal solutions, some research focuses on identifying globally optimal actions to guide policy training. WQMIX **Wang, "Weighted Q-Mix: Enhancing Cooperative Multi-Agent Reinforcement Learning via Optimal Action Weighting"** enhances policy training by assigning higher weights to optimal actions in the loss function. OVI-QMIX **Sunehag, "Optimistic Value Iteration For Q-Mix Networks with Intrinsic Exploration"** minimizes the KL divergence between the current and optimal policies through optimistic instructors. However, these methods use approximate optima for guiding value estimation, leading to instability
during training. Preference-Guided Stochastic Exploration **Li, "Preference-Guided Stochastic Exploration via Advantage-based Reward Shaping in Multi-Agent Reinforcement Learning"** introduces an additional exploration strategy based on the advantage of each action in single-agent reinforcement learning. COE 
**Kumar, "Cooperative Exploration for Efficient Optimal Policy Learning in Decentralized Multi-Agent Systems via Optimism and Upper Confidence Bounds Applied to Trees"** integrates optimism and exploration by constructing Upper Confidence bounds applied to Trees (UCT), helping policy converge to the global optimal solution. Nonetheless, tree-based methods face scalability limitations in large-scale multi-agent environments. Developing simple and effective guided exploration strategies remains a significant challenge in multi-agent reinforcement learning.