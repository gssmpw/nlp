%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage[linesnumbered,algo2e,lined,boxed,ruled,commentsnumbered]{algorithm2e}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{natbib}
%\setcitestyle{maxauthors=2,etalmode=truncate}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
\usepackage[preprint]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tablefootnote}

\allowdisplaybreaks
\newcommand{\QED}{\hfill \ensuremath{\Box}}


\usepackage[braket,qm]{qcircuit}
\usepackage{nkj}



\newcommand\memo[1]{{{\color{red}#1}}}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Bayesian Parameter Shift Rule in Variational Quantum Eigensolvers}

\begin{document}

\twocolumn[
\icmltitle{Bayesian Parameter Shift Rule in Variational Quantum Eigensolvers}



% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{\qquad \qquad  Samuele Pedrielli}{affil2}
% \icmlauthor{\qquad \qquad  Samuele Pedrielli}{affil2,affil7}
\icmlauthor{Christopher J.~Anders}{affil6}
\icmlauthor{Lena Funcke}{affil3,affil4}
\icmlauthor{Karl Jansen}{affil5}\newline
\icmlauthor{\qquad \qquad   Kim A.~Nicoli}{affil3,affil4}
\icmlauthor{Shinichi Nakajima}{affil1,affil2,affil6}

%Samuele Pedrielli, Christopher J. Anders, Lena Funcke, Karl Jansen, Kim A. Nicoli, Shinichi Nakajima

\end{icmlauthorlist}

\icmlaffiliation{affil1}{Berlin Institute for the Foundations of Learning and Data (BIFOLD)}
\icmlaffiliation{affil2}{Technische Universit\"{a}t Berlin, Germany}
\icmlaffiliation{affil3}{Transdisciplinary Research Area (TRA) Matter, University of Bonn, Germany}
\icmlaffiliation{affil4}{Helmholtz Institute for Radiation and Nuclear Physics (HISKP), University of Bonn, Germany}
\icmlaffiliation{affil5}{Deutsches Elektronen-Synchrotron (DESY), Zeuthen, Germany}
\icmlaffiliation{affil6}{RIKEN Center for AIP, Japan}
% \icmlaffiliation{affil7}{UniversitÃ  degli Studi di Padova, Italy}
\icmlcorrespondingauthor{Samuele Pedrielli}{samuele.pedrielli@campus.tu-berlin.de}
\icmlcorrespondingauthor{Shinichi Nakajima}{nakajima@tu-berlin.de}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{parameter shift rule, variational quantum eigensolver, quantum computing, confidence region, Gaussian process}


\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

%uncomment the line below for equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\emph{Parameter shift rules} (PSRs) are key techniques for efficient gradient estimation
in variational quantum eigensolvers (VQEs).  In this paper, we propose its Bayesian variant, where Gaussian processes  with appropriate kernels are used to estimate the gradient of the VQE objective.
Our \emph{Bayesian PSR} offers flexible gradient estimation from observations at arbitrary locations with uncertainty information, and reduces to the generalized PSR in special cases.  In stochastic gradient descent (SGD), the flexibility of Bayesian PSR allows reuse of observations in previous steps, which accelerates the optimization process.
Furthermore, the accessibility to the posterior uncertainty,
along with our proposed notion of \emph{gradient confident region} (GradCoRe),
enables us to minimize the observation costs in each SGD step.  
Our numerical experiments show that the VQE optimization with Bayesian PSR and GradCoRe significantly accelerates SGD, and outperforms the state-of-the-art methods, including sequential minimal optimization.
\end{abstract}




\section{Introduction}
\label{sec:Introduction}

The variational quantum eigensolver (VQE)~\citep{Peruzzo2014,mcclean2016theory} is a hybrid quantum-classical algorithm for approximating the ground state of the Hamiltonian of a given physical system.  The quantum part of VQEs uses parameterized quantum circuits to generate trial quantum states and measures the expectation value of the Hamiltonian, i.e., the energy, while the classical part performs energy minimization with noisy observations from the quantum device. Provided that the parameterized quantum circuits can accurately approximate the ground state, the minimized energy gives a tight upper bound of the ground state energy of the Hamiltonian.

The observation noise in the quantum device comes from multiple sources.
%One important caveat is that measurements performed on a quantum computer are affected by multiple sources of noise. 
One source of noise is
%dictated by the so-called 
\textit{measurement shot noise},
%which is the inherent uncertainty in quantum physics---measurement outcomes follow the probabilities specified by the quantum states.
which arises from the statistical nature of quantum measurements---outcomes follow the probabilities specified by the quantum state, and finite sampling introduces fluctuations.
Since this noise source is random and independent, 
% which is caused by repeatedly measuring a trial quantum state on the quantum computer. This noise source 
it can be reduced 
%can be mitigated 
by increasing the number of measurement shots,
%$N_{\mathrm{shots}}$, 
to which the  variance is inversely proportional. 
% The presence of noise often hinders the optimization of VQEs and may lead to noise-induced barren plateaus~\cite{noiseinducedbp}. 
Another source of noise stems from imperfections in the quantum hardware, 
% a more challenging issue that is tackled using techniques such as error mitigation~\cite{RevModPhys.95.045005} and quantum error correction strategies~\cite{Roffe03072019,googleaiquantumerrcorr}. We refer to~\citet{TILLY20221} for further details about VQEs and their challenges.
%which have been drastically reduced in recent years by 
which have been reduced in recent years by
 hardware design~\citep{bluvstein2023logical}, as well as  error mitigation~\cite{RevModPhys.95.045005}, quantum error correction~\cite{Roffe03072019,googleaiquantumerrcorr}, and machine learning~\cite{Liao_2024,nicoli_noisyvqe} techniques. In this paper, we do not consider hardware noise, as is common in papers developing optimization methods \citep{nakanishi20, emicore_GH_2023}.  %We refer to~\citet{TILLY20221} for further details about VQEs and their challenges.

%This is much harder to mitigate and recent efforts have been put forward in the field of error mitigation~\cite{RevModPhys.95.045005} and quantum error correction strategies~\cite{Roffe03072019,googleaiquantumerrcorr}. The presence of noise often hinders the optimization of VQEs and may lead to noise-induced barren plateaus~\cite{noiseinducedbp} which manifest as an exponential decay of the gradient of the cost function, e.g., the energy of the quantum system, with the number parameters in the quantum circuit. This implies an unavoidable trade-off: while more measurements lead to greater accuracy, they also require an excessive number of quantum measurements, which quickly becomes impractical. We refer to this review paper~\cite{TILLY20221} for further details about VQEs and their challenges.

Stochastic gradient descent (SGD), sequential minimal optimization (SMO), and Bayesian optimization (BO)
%, as well as their combinations, 
have previously been used to minimize the VQE objective function. Under some mild assumptions~\cite{nakanishi20}, this objective function is known to have special properties.
Based on those properties,
SGD methods can use the gradient estimated by so-called \emph{parameter shift rules} (PSRs)~\citep{Mitarai2018}, and specifically designed SMO~\cite{Platt1998SequentialMO} methods, called Nakanishi-Fuji-Todo (NFT)~\cite{nakanishi20},  perform one-dimensional subspace optimization with only a few observations in each iteration.
% In machine learning, BO often uses Gaussian process (GP) regression as a surrogate function~\citep{Frazier2018ATO},
% which has been applied also for VQEs \citep{iannelli2021noisy}. 
\citet{iannelli2021noisy}
applied BO to solve VQEs as noisy global optimization problems.

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{figures/gradcore}
    \centering
    \vskip -2ex
    \vskip -1ex
    \caption{
    Illustration of our gradient confident region (GradCoRe) approach.
    Our goal is to minimize the true energy $f^*(\bfx)$ over the set of parameters $\bfx \in [0, 2 \pi)^D$, where we use a GP surrogate $f(\bfx)$ for approximating $f^*(\bfx)$. Observing $f^*$ at points  $\bfx_{-}$ and $\bfx_{+}$ (green circles) along the $d$-th direction (solid horizontal line) decreases the uncertainty (dashed curves) not only for predicting  $f(\bfx_{\pm})$, but also for predicting $\partial_d f(\widehat{\bfx}^{t-1})$, 
    so that the current optimal point $\widehat{\bfx}^{t-1}$ falls within the GradCoRe (blue square).
    Our GradCoRe-based SGD uses the minimum number of measurement shots for achieving required gradient estimation accuracy in each iteration, and thus minimizes the total observation costs over the optimization process.
    }
\label{fig:GradcoreConcept}
    %\vspace{-5mm}
\end{figure}

Although Gaussian processes (GPs) \citep{book:Rasmussen+Williams:2006} have been used in VQEs as common surrogate functions for BO~\citep{Frazier2018ATO},
they have also been used to improve 
SGD-based and SMO-based methods.
%Recent developments have lifted both BO and GPR establishing new methods.
~\citet{NEURIPS:Nicoli+:2023} proposed the \textit{VQE kernel}---a physics-informed kernel that fully reflects the properties of VQEs---and 
%. Furthermore,the authors 
combined SMO and BO with the \emph{expected maximum improvement within confident region} (EMICoRe) acquisition function.  This allows for identification of the optimal locations to measure on the quantum computer in each SMO iteration. 
%allowing to optimize the observed locations in each iteration of NFT.
~\citet{SGLBO2022} combined SGD and BO, and proposed \emph{stochastic gradient line BO} (SGLBO), which uses BO to identify the optimal step size 
in each SGD iteration. 
%where the one-dimensional optimization along the estimated gradient is performed by the standard BO.
~\citet{ICML:Anders+:2024} 
 proposed the \emph{subspace in confident region} (SubsCoRe) approach, where the observation costs are minimized based on the posterior uncertainty estimation in each SMO iteration.

In this paper, we take a different approach to leveraging GPs, and introduce a \emph{Bayesian parameter shift rule} (Bayesian PSR), where the gradient of the VQE objective is estimated using GPs with the VQE kernel.
%In this paper, we use GP in another way, and propose a \emph{Bayesian PSR}, where the gradient of the VQE objective is estimated by GP equipped with the VQE kernel.
%~\cite{NEURIPS:Nicoli+:2023}.
The Bayesian PSR translates into a regularized variant of PSRs if the observations are performed at designated locations. However, our approach offers significant advantages---flexibility and direct access to uncertainty---over existing PSRs~\cite{Mitarai2018,Wierichs2022generalparameter}. %which can enhance SGD-based optimization. 
More specifically, the Bayesian PSR can use observations at any set of locations, which allows the reuse of observations performed in previous iterations of SGD.
%In the converging phase, 
%as the gradient signal gets small,
%where the optimal point does not move much in %each iteration, 
Reusing previous observations along with new observations improves the gradient estimation accuracy, and thus accelerates the optimization process.
%to the ground state energy of the system.
Furthermore, the uncertainty information can be used to adapt the observation cost in each SGD iteration---in a similar spirit to~\citet{ICML:Anders+:2024}---which significantly reduces the cost of obtaining new observations, while maintaining a required level of accuracy.
%This crucially reduces the cost for new observations while ensuring the required accuracy.
We implement this adaptive observation cost strategy by introducing a novel notion of \emph{gradient confidence region} (GradCoRe)---the region in which the uncertainty of the gradient estimation is below a specified threshold (see \Cref{fig:GradcoreConcept}).
Our empirical evaluations show that our proposed Bayesian PSR improves the gradient estimator, and SGD equipped with our GradCoRe approach outperforms all previous state-of-the-art methods including NFT and its variants.





The main contributions are summarized as follows:
\begin{itemize}
    \item We propose \textit{Bayesian PSR}, a flexible variant of existing PSRs that provides access to uncertainty information.
    \item We theoretically establish the relationship between Bayesian PSR and existing PSRs, revealing the optimality of the \emph{shift} parameter in first-order PSRs.
    %We theoretically clarify the relation between Bayesian PSR and existing PSRs, which also uncovers the optimality of the  \emph{shift} parameter in the first order PSR.
    \item We introduce the notion of \textit{GradCoRe}, and propose an adaptive observation cost strategy for SGD optimization.
    \item We numerically validate our theory and empirically demonstrate the effectiveness of the  proposed Bayesian PSR and GradCoRe.
\end{itemize}


\paragraph{Related work: }
Finding the optimal set of parameters for a variational quantum circuit is a challenging problem, prompting the development of various approaches to improve the optimization in VQEs. Gradient-based methods for VQEs often rely on PSRs~\cite{Mitarai2018,Wierichs2022generalparameter}, which enable reasonably accurate gradient estimation of the output of quantum circuits with respect to their parameters. 
%are particularly popular despite their relative inefficiency, as each gradient estimation requires two additional measurements on the quantum computer. 
~\citet{nakanishi20} proposed an SMO~\cite{Platt1998SequentialMO} algorithm, known as \emph{NFT}, where, at each step of SMO, one parameter is analytically minimized by performing a few
%additional 
observations. %at designated locations.
~\citet{NEURIPS:Nicoli+:2023} combined NFT with GP and BO by developing a physics-inspired kernel for GP regression and proposing the EMICoRe acquisition function, relying on the concept of confident regions (CoRe).
This method improves upon NFT by leveraging the information from observations in previous steps to identify the optimal locations to perform the next observations.
~\citet{ICML:Anders+:2024} leveraged the same notion of CoRe, and proposed SubsCoRe, where, instead of optimizing the observed locations, the minimal number of measurement shots is identified to achieve the required accuracy defined by the CoRe.
The resulting algorithm converges to the same energy as NFT with a smaller quantum computation cost, i.e., the total number of measurement shots on a quantum computer.
~\citet{SGLBO2022} combined SGD with BO to tackle the excessive cost of standard SGD approaches and used BO to accelerate the convergence by finding the optimal step size.
On a related note, recent works~\cite{PhysRevResearch.6.033069} have begun integrating GP with error mitigation techniques, further highlighting the potential of Bayesian approaches for noisy intermediate-scale quantum (NISQ) devices~\cite{Preskill2018quantumcomputingin}.

The remainder of the paper is structured as follows: in \Cref{sec:Background}, we provide the necessary background on GPs and VQEs.
%and introduce two theorems that enable us to compute the GP derivative.
In~\Cref{sec:ProposedMethod}, we propose our Bayesian PSR and provide a theory that relates it to the existing PSRs. In~\Cref{sec:Gradcore}, we propose our novel SGD-based algorithms based on Bayesian PSR and GradCoRe.
In~\Cref{sec:Experiment}, we describe the experimental setup and present numerical experiments.
Finally, in~\Cref{sec:Conclustion}, we summarize our findings and provide an outlook for future research.



\section{Background}
\label{sec:Background}

Here we briefly introduce Gaussian process (GP) regression and its derivatives, as well as VQEs with their known properties.

\subsection{GP Regression and Derivative GP}
\label{sec:Pre.GP}

Assume that we aim to learn an unknown function $f^*(\cdot): \mcX \mapsto \bbR$ from the training data
$\bfX = ({\bfx}_1, \ldots, {\bfx}_N) \in \mcX^{N}, \bfy = ( y_1, \ldots, y_{N})^\T \in \bbR^{N}, \bfsigma = (\sigma_1^2, \ldots, \sigma_N^2) \in \mathbb{R}_{++}^N$
that fulfills
\begin{align}
y_n &= f^*(\bfx_n) + \varepsilon_n, & \varepsilon_n &\sim  \mcN_1(y_n; 0, \sigma_n^2),
\label{eq:RegressionModel}
\end{align}
where $\mcN_D(\cdot; \bfmu, \bfSigma)$ denotes the $D$-dimensional Gaussian distribution with mean $\bfmu$ and covariance $\bfSigma$.
With the Gaussian process (GP) prior
\begin{align}
p(f(\cdot))
&= \mathrm{GP} (f(\cdot); 0(\cdot), k(\cdot, \cdot)),
\label{eq:GPPrior}
\end{align}
where $0(\cdot)$ and $k(\cdot, \cdot)$ are the prior zero-mean and the kernel (covariance) functions, respectively,
the posterior distribution of the function values $\bff' = (f(\bfx'_1), \ldots, f(\bfx'_M))^\T \in \bbR^M$ at arbitrary test points $\bfX' = ({\bfx'}_1, \ldots, {\bfx'}_M) \in \mcX^{M}$ is given as
\begin{align}
p({\bff'} | \bfX, \bfy)  & =  \mcN_{M}({\bff'}; \bfmu'_{[ \bfX, \bfy, \bfsigma]}, \bfS'_{[ \bfX, \bfsigma]}), \quad \mbox{ where }
   \label{eq:GPPosterior}\\
\bfmu'_{[ \bfX, \bfy, \bfsigma]}
   &= {\bfK}'^{\T} \left(\bfK + \bfDiag(\bfsigma) \right)^{-1} \bfy \quad \mbox{ and }
 \label{eq:GPPosteriorMean}\\
\bfS'_{[ \bfX, \bfsigma]}
   &= {\bfK}'' - 
   {\bfK'}^{\T} \left(\bfK + \bfDiag(\bfsigma) \right)^{-1} {{\bfK}'}
   \label{eq:GPPosteriorVar}
\end{align}
%where $\bfmu'$ and $\bfS'$ 
are the posterior mean and covariance, respectively~\citep{book:Rasmussen+Williams:2006}.
Here $\bfDiag(\bfv)$ is the diagonal matrix with $\bfv$ specifying the diagonal entries,
%$\bfI_N \in \bbR^{N \times N}$ denotes the identity matrix,
and $\bfK = k(\bfX, \bfX) \in \bbR^{N \times N}, {\bfK}'  = {k}(\bfX, \bfX') \in \bbR^{N \times M}$, and $  {\bfK}'' ={k}(\bfX', \bfX')  \in \bbR^{M \times M}$ are the train, train-test, and test kernel matrices, respectively,
where $k(\bfX, \bfX')$ denotes the kernel matrix evaluated at each column of $\bfX$ and $\bfX'$ such that $(k(\bfX, \bfX'))_{n, m} = k(\bfx_n, \bfx'_m)$. 
We also denote the posterior as $p({f}(\cdot) | \bfX, \bfy)   =  \mathrm{GP} (f(\cdot); \mu_{[ \bfX, \bfy, \bfsigma]}(\cdot), s_{[ \bfX, \bfsigma]}(\cdot, \cdot))$ with the posterior mean $\mu_{[ \bfX, \bfy, \bfsigma]}(\cdot)$ and covariance $s_{[ \bfX, \bfsigma]}(\cdot, \cdot)$ functions. 
% e.g., $\mu_{[ \bfX, \bfsigma, \bfy]}(\bfx') = \bfmu_{[ \bfX, \bfsigma, \bfy]}' \in \mathbb{R}$ and $s_{[ \bfX, \bfsigma]}(\bfx', \bfx') = \bfS_{[ \bfX, \bfsigma]}'  \in \mathbb{R}_{++}$ for a single test point $\bfX' = (\bfx')$. 

Since the derivative operator is linear, the derivative $\bfnabla_{\bfx} f = (\partial_1 f, \ldots, \partial_D f)^\T \in \mathbb{R}^D$ of GP samples also follows a GP. Here we abbreviate $\partial_d = \frac{\partial}{\partial x_d}$. Therefore,
we can straightforwardly handle the derivative outputs at training and test points by modifying the kernel function. %as follows:
% In this paper, we use GP regression to estimate its derivative $\nabla_{\bfx} f = (\partial_1 f, \ldots, \partial_D f)^\T$, which also follows GP due to the linearity of the derivative operator.
% % Given the outputs $\bfy = ( y_1, \ldots, y_{N})^\T \in \bbR^{N}$ at $N$ training points $\bfX = ({\bfx}_1, \ldots, {\bfx}_N) \in \mcX^{N}$,
% The posterior of the derivative at a test point $\bfx'$ is given as
% \begin{align}
% p( \partial_d f (\bfx') | \bfX, \bfy)  & =  \mcN_{1}(\partial_d f(\bfx'); \widetilde{\mu}', \widetilde{s}'^2), 
%    \label{eq:DGPPosterior}\\
% \mbox{ where }\quad  
% \widetilde{\mu}' 
%    &= \widetilde{\bfk}'^{\T} \left(\bfK + \bfDiag(\bfsigma) \right)^{-1} \bfy,
%  \label{eq:DGPPosteriorMean}\\
%  \widetilde{ s}'^2
%    &= \widetilde{k}'' - 
%    \widetilde{\bfk'}^{\T} \left(\bfK + \bfDiag(\bfsigma) \right)^{-1} {\widetilde{\bfk}'}.
%    \label{eq:DGPPosteriorVar}
% \end{align}
% Here, $\bfI_N \in \bbR^{N \times N}$ denotes the identity matrix,
% and $\bfK = k(\bfX, \bfX) \in \bbR^{N \times N}, \widetilde{\bfk}'  = \widetilde{k}(\bfX, \bfx') \in \bbR^{N}$, and $  \widetilde{k}'' =\widetilde{k}(\bfx', \bfx')  \in \bbR$ are the train, derivative train-test, and derivative test kernel matrices, respectively,
% where $k(\bfX, \bfX')$ denotes the kernel matrix evaluated at each column of $\bfX$ and $\bfX'$ such that $(k(\bfX, \bfX'))_{n, m} = k(\bfx_n, \bfx'_m)$.
% The derivative train-test and test-test kernels are defined as
%for a point $\bfx$ with its observation or prediction $y = f^*(\bfx) + \varepsilon$ and
Assume that $\bfx$ is a training or test point with non-derivative output $y =  f^*(\bfx) + \varepsilon$,
and $\bfx'$ and $\bfx''$ are training or test points with derivative outputs, $y' = \partial_{d'} f^*(\bfx') + \varepsilon', y'' = \partial_{d''} f^*(\bfx'') + \varepsilon''$.  Then, 
the kernel functions should be replaced with
\begin{align}
\widetilde{k} (\bfx, \bfx') 
& =\textstyle \frac{\partial}{\partial x'_{d'}} k(\bfx, \bfx') ,
\label{eq:DerivativeTrTeKernel}\\
\widetilde{k} (\bfx', \bfx'')
&=\textstyle
\frac{\partial^2}{\partial x'_{d'} \partial x''_{d''}}  k(\bfx', \bfx'').
\label{eq:DerivativeTeTeKernel}
\end{align}
The posterior \eqref{eq:GPPosterior} with appropriately replaced kernel matrix entries gives the posterior distribution of derivatives at test points. 
We denote the GP posterior of a single component of the derivative as 
\begin{align}    
p(\partial_d f(\cdot) | \bfX, \bfy)   =  \mathrm{GP} \left(\partial_d f(\cdot); \widetilde{\mu}^{(d)}_{[ \bfX, \bfy, \bfsigma]}(\cdot), \widetilde{s}^{(d)}_{[ \bfX, \bfsigma]}(\cdot, \cdot) \right)
\label{eq:GPPosteriorDerivative}
\end{align}
with the posterior mean  $\widetilde{\mu}^{(d)}(\cdot)$ and covariance $\widetilde{s}^{(d)}(\cdot, \cdot)$ functions for the derivative with respect to $x_d$.
More generally, GP regression can be analytically performed in the case where the training outputs (i.e., observations) and the test outputs (i.e., predictions) contain derivatives with different orders (see \Cref{sec:A.DerivativeGPGeneral} for more details).


\subsection{Variational Quantum Eigensolvers and their Physical Properties}
\label{sec:B.VQE}

The VQE~\citep{Peruzzo2014,mcclean2016theory} is a hybrid quantum-classical computing protocol for estimating the ground-state energy of a given quantum Hamiltonian for a $Q$-qubit system.
The quantum computer is used to prepare a parametric quantum state $\vert\psi_{\bfx}\rangle$, which depends on $D$ angular parameters $\bfx \in \mcX = [0, 2 \pi)^D$. 
This trial state $\vert\psi_{\bfx}\rangle$ is generated by applying $D' (\geq D)$ \emph{quantum gate operations}, $G(\bfx) = G_{D'} \circ\cdots \circ G_1$, to an initial quantum state $\vert{\psi_0}\rangle$, i.e.,  $\vert\psi_{\bfx} \rangle = G(\bfx) \vert\psi_0\rangle$. 
All gates $\{G_{d'}\}_{d'=1}^{D'}$ are unitary operators, parameterized by at most one variable $x_d$. 
Let $d(d'): \{1, \ldots, D'\} \mapsto \{1, \ldots, D\}$ be the mapping specifying which one of the variables $\{x_d\}$ parameterizes the $d'$-th gate.
We consider parametric gates of the form $G_{d'}(x) = U_{d'} (x_{d(d')}) = \exp \left( -i x_{d(d')} P_{d'}/2 \right)$, where $P_{d'}$ is an arbitrary sequence of the Pauli operators $ \{\mathbf{1}_q,\,\sigma_q^X, \sigma_q^Y, \sigma_q^Z\}_{q=1}^Q$ acting on each qubit at most once.
This general structure covers both single-qubit gates, such as $R_{X}(x) = \exp{\left(-i\theta \sigma_q^X \right)}$, and entangling gates acting on multiple qubits simultaneously, such as $R_{XX}(x) = \exp{\left(-i x \sigma_{q_1}^X \circ \sigma_{q_2}^X \right)}$
%and $R_{ZZ}(x) = \exp{\left(-i x \sigma_{q_1}^Z \circ \sigma_{q_2}^Z\right)}$
for $q_1 \ne q_2$, commonly realized in trapped-ion quantum hardware setups~\citep{TrappedIon2,TrappedIon}.

The quantum computer is used to evaluate the energy of the resulting quantum state $\ket{\psi_{\bfx}}$
by observing
\begin{align}
y &= f^*(\bfx) + \varepsilon,
\qquad \mbox{ where }
\notag\\
f^*(\bfx)
&=
\langle{\psi_{\bfx}}\vert  H  \vert{\psi_{\bfx}}\rangle
=
\langle{\psi_0}\vert G(\bfx)^\dagger H G(\bfx) \vert{\psi_0}\rangle,
\label{eq:VQEObjective}
\end{align}
and $\dagger$ denotes the Hermitian conjugate. 
For each observation, repeated measurements, called \emph{shots}, on the quantum computer are performed.
%multiple measurement shots, 
%denoted by .
Averaging over 
the number $N_\mathrm{shots}$ of shots
suppresses the variance $ \sigma^{*2} (N_\mathrm{shots}) \propto N_\mathrm{shots}^{-1}$ of the observation noise $\varepsilon$.%
%when evaluating the expectation value of the Hamiltonian, i.e., the energy of the quantum system.%
\footnote{We do not consider the hardware noise, and therefore, the observation noise $\varepsilon$ consists only of the \textit{measurement shot} noise.}
Since the observation $y$ is the sum of many random variables, it approximately follows the Gaussian distribution, according to the central limit theorem. The Gaussian likelihood \eqref{eq:RegressionModel} therefore approximates the observation $y$ well if 
%$f(\bfx) \approx f^*(\bfx)$ and 
$\sigma_n^2 \approx  \sigma^{*2} (N_\mathrm{shots})$.
Using the noisy estimates of $f^*(\bfx)$ obtained from the quantum computer, a protocol running on a classical computer is used to solve the following minimization problem:
\begin{align}
\textstyle
\min_{\bfx \in [0, 2 \pi)^D} f^*(\bfx),
\label{eq:VQEOptimization}
\end{align}
thus finding the minimizer $\widehat{\bfx}$, i.e., the optimal parameters for the 
%parametrized 
(rotational) quantum gates.
%\newpage
Given the high expense of quantum computing resources, the computation cost is primarily driven by quantum operations. As a result, the optimization cost in VQE is typically measured by the total number of measurement shots required during the optimization process.\footnote{
When the Hamiltonian consists of $N_\mathrm{{og}}$ groups of non-commuting operators, each of which needs to be measured separately, $N_\mathrm{shots}$ denotes the number of shots \emph{per operator group}. Therefore, the number of shots \emph{per observation} is $N_\mathrm{{og}}\times N_\mathrm{shots}$.
In our experiments, we report on the total number of shots per operator group, i.e., the cumulative sum of $N_\mathrm{{shots}}$ over all observations, when evaluating the observation cost.
\label{ft:footnote1}
}
We refer to~\citet{TILLY20221} for further details about VQEs and their challenges.

Let $V_d$ be the number of gates parameterized by $x_d$, i.e., $ V_d= |\{d' \in \{1,\dots D'\}; d= d(d')\} |$.
\citet{Mitarai2018} proved that the VQE objective~\eqref{eq:VQEObjective} for $V_d = 1$ satisfies the parameter shift rule (PSR) 
\begin{align}
\textstyle
% \frac{\partial}
%{\partial x_d}
\partial_d
f^*(\bfx')
&= \textstyle 
\frac{f^*\left(\bfx' + \alpha\bfe_d \right) -  f^*\left(\bfx' - \alpha \bfe_d \right)}
{2 \sin \alpha}, 
\notag\\
& \forall \bfx \in [0, 2\pi)^{D}, d= 1, \ldots, D, \alpha \in [0, 2\pi),
\label{eq:ParameterShiftRule}
\end{align} 
where $\{\bfe_d\}_{d=1}^{D}$ are the standard basis, and the \emph{shift} $\alpha$ is typically set to $\frac{\pi}{2}$. 
\citet{Wierichs2022generalparameter} generalized the PSR~\eqref{eq:ParameterShiftRule} for arbitrary $V_d$ with equidistant observations $\{\bfx_w  = {\bfx}'  + \frac{2w+1}{2V_d } \pi \bfe_d\}_{w = 0}^{2V_d-1}$:
\begin{align}
\partial_d
f^*(\bfx')
&=\textstyle
\frac{1}{2 V_d}    \sum_{w=0}^{2V_d-1}  
\frac{   (-1)^w f^*(\bfx_w) }{2 \sin^2\left(\frac{(2w+1)\pi}{4V_d}\right)}.
\label{eq:GeneralParameterShiftRule}
\end{align}
Most gradient-based approaches rely on those PSRs, which allow reasonably accurate gradient estimation from $\sum_{d=1}^D 2 V_d$ observations.
Let
\begin{align}
    \bfpsi_{\gamma}(\theta) 
    &= (\gamma, \sqrt{2} \cos \theta, \sqrt{2} \cos2 \theta, \ldots, \sqrt{2} \cos V_d \theta,
    \notag\\
    & \hspace{-5mm}
    \sqrt{2} \sin \theta, \sqrt{2} \sin 2 \theta, \ldots, \sqrt{2} \sin V_d \theta)^\T \in \mathbb{R}^{1 + 2V_d}
    \label{eq:FourierBasis}
\end{align}    
be the (1-dimensional) $V_d$-th order Fourier basis for arbitrary $\gamma > 0$. 
\citet{nakanishi20} found that 
the VQE objective function $f^*(\cdot)$ in Eq.~\eqref{eq:VQEObjective} with any%
\footnote{Any circuit consisting of parametrized rotation gates and non-parametric unitary gates.
%as stated in the introduction.
} 
$G(\cdot)$, $H$, and $\ket{\psi_0}$ can be expressed exactly as 
\begin{align}
f^*(\bfx) =  \bfb^\T  \mathbf{vec} \left( \otimes_{d=1}^D 
\bfpsi_{\gamma}(x_d)
\right)
\label{eq:TrigonometricPolynomial}
\end{align}
for some $\bfb  \in \textstyle  \mathbb{R}^{\prod_{d=1}^D(1 + 2V_d)}$,
where 
$\otimes$ and $\mathbf{vec} (\cdot)$ denote the tensor product and the vectorization operator for a tensor, respectively.
Based on this property,
the  Nakanishi-Fuji-Todo (NFT) method~\citep{nakanishi20} performs SMO~\citep{Platt1998SequentialMO}, where the optimum in a chosen 1D subspace for each iteration is analytically estimated from only $1+2V_d$ observations (see~\Cref{sec:A.NFT} for the detailed procedure).
It was shown that the PSR~\eqref{eq:ParameterShiftRule} and the trigonometric polynomial function form~\eqref{eq:TrigonometricPolynomial} 
are mathematically equivalent~\citep{NEURIPS:Nicoli+:2023}.


Inspired by the function form
\eqref{eq:TrigonometricPolynomial} of the objective,
\citet{NEURIPS:Nicoli+:2023} proposed the VQE kernel
\begin{align}
k_{\gamma} (\bfx, \bfx')
&= \textstyle
\sigma_0^2
\prod_{d=1}^D \left(\frac{ \gamma^{2} + 2\sum_{v=1}^{V_d}  \cos \left( v(x_d - x_d')  \right)}{\gamma^{2} + 2 V_d }\right),
\label{eq:VQEKernelTied}
\end{align}
which is decomposed as $k_{\gamma} (\bfx, \bfx') = \bfphi_{\gamma}(\bfx)^\T \bfphi_\gamma(\bfx')$ with feature maps $\bfphi_{\gamma}(\bfx) = 
 \frac{\sigma_0 }
{ \left( \gamma^{2} + 2V_d  \right)^{D/2}} \mathbf{vec} \left( \otimes_{d=1}^D 
\bfpsi_{\gamma}(x_d)
\right)$,
for GP regression.
The kernel parameter $\gamma^2$ controls the smoothness of the function, i.e., suppressing the interaction terms when $\gamma^2 > 1$. When $\gamma^2 = 1$, the Fourier basis \eqref{eq:FourierBasis} is orthonormal, and 
the VQE kernel~\eqref{eq:VQEKernelTied} is proportional to the product of Dirichlet kernels \citep{Rudin1962}.
The VQE kernel reflects the physical knowledge~\eqref{eq:TrigonometricPolynomial} of VQE, and thus
allows us to perform a Bayesian variant of NFT---\emph{Bayesian NFT} or \emph{Bayesian SMO}---where the 1D subspace optimzation in each SMO step is performed with GP
(see \Cref{sec:A.NFT} for more details and the performance comparison between the original NFT and Bayesian NFT).
\citet{NEURIPS:Nicoli+:2023} furthermore enhanced Bayesian NFT with BO, using the notion of confident region (CoRe),
\begin{align}
\mcZ_{[\bfX, \bfsigma]}(\kappa^2) & = \left\{\bfx \in \mcX; s_{[\bfX, \bfsigma]} (\bfx, \bfx) \leq \kappa^2 \right\},
\label{eq:LowUncertaintySet}
\end{align}
i.e., the region in which the uncertainty of the GP prediction is lower than a threshold $\kappa$. More specifically, they introduced the EMICoRe acquisition function to find the best observation points in each SMO iteration, %i.e., $\bfalpha$ in Step 1 of NFT, 
such that the maximum expected improvement within the CoRe is maximized.







\begin{figure*}[t]
    \centering
     \includegraphics[width=0.33\textwidth]{figures/FandDF_VD1.jpeg}
     \includegraphics[width=0.33\textwidth]{figures/FandDF_VD2.jpeg}
     \includegraphics[width=0.33\textwidth]{figures/AlphaDepend.jpeg}
    \centering
    \vskip -1ex
    \caption{
    Illustration of the behavior of the Bayesian PSR when $V_d=1$ (left) and when $V_d=2$ (middle). Bayesian PSR prediction (red) coincides with general PSR (green cross) for the designed equidistant observations (magenta crosses). 
    The right plot visualizes the variance \eqref{eq:DGPPredictionVar} of the derivative GP prediction at $\bfx'$, as a function of the shift $\alpha$ of observations when $V_d=1$.
    Although the optimum is at $\alpha = \frac{\pi}{2}$, the dependence is weak.  For all panels, the noise and kernel parameters are set to $\sigma^2 = 0.01, \gamma^2 = 9, \sigma_0^2 = 100$.
    }
\label{fig:GPPSRIllustraion}
    %\vspace{-5mm}
\end{figure*}






\section{Bayesian Parameter Shift Rules}
\label{sec:ProposedMethod}

We propose \textit{Bayesian PSR}, which estimates the gradient of the VQE objective \eqref{eq:VQEObjective} by the GP posterior \eqref{eq:GPPosteriorDerivative} with the VQE kernel \eqref{eq:VQEKernelTied} along with its derivatives \eqref{eq:DerivativeTrTeKernel} and \eqref{eq:DerivativeTeTeKernel}.
The advantages of Bayesian PSR include the following:
\begin{itemize}
    \item The gradient estimator has an analytic-form.
    \item Estimation can be performed using observations at any set of points.
    \item Estimation is optimal for heteroschedastically noisy observations (from the Bayesian perspective), as long as the prior with the kernel parameters, $\gamma$ and  $\sigma_0^2$, is appropriately set.
    \item The posterior uncertainty can be analytically computed \emph{before} performing the observations.
\end{itemize}
In \Cref{sec:Gradcore}, we propose novel SGD solvers for VQEs that leverage the advantages of Bayesian PSR.

As naturally expected, our Bayesian PSR is a generalization of exisiting PSRs,
and reduces to the general PSR \eqref{eq:GeneralParameterShiftRule} for noiseless and equidistant observations.
Let $\bfone_D \in \mathbb{R}^D$ be the vector with all entries equal to one.
\begin{theorem}
\label{thrm:GPasGeneralPSR}
For any $x' \in [0, 2\pi)^D$ and $d = 1, \ldots, D$, the mean and variance of the derivative GP prediction, given  observations $\bfy = (y_0, \ldots, y_{2V_d -1})^\T \in \mathbb{R}^{2 V_d}$ at $2V_d$ equidistant training points $\bfX = (\bfx_0, \ldots, \bfx_{2V_d -1}) \in \mathbb{R}^{D \times 2V_d}$ for $\bfx_w  = {\bfx}'  + \frac{2w+1}{2V_d } \pi \bfe_d$ with homoschedastic noise $\bfsigma = \sigma^2 \cdot \bfone_{2V_d}$ for   $ \sigma^2 \ll \sigma_0$,
are 
\begin{align}
\widetilde{\mu}^{(d)}_{[ \bfX, \bfy, \bfsigma]}(\bfx')
& =\textstyle
\frac{ \sum_{w=0}^{2V_d-1}  
\frac{   (-1)^w y_w }{2 \sin^2\left(\frac{(2w+1)\pi}{4V_d}\right)}}
{(\gamma^2 + 2 V_d) \frac{\sigma^2}{\sigma_0^2} + 2 V_d}  
\!+ O(\frac{\sigma^4}{\sigma_0^4}),
\label{eq:DGPPredictionMeanGeneral}\\
\widetilde{s}^{(d)}_{[ \bfX, \bfsigma]}(\bfx', \bfx')
& =\textstyle
 \sigma^2 
\frac{  2 V_d^2 + 1}{6}
+O(\frac{\sigma^4}{\sigma_0^2}) .
\label{eq:DGPPredictionVarGeneral}
\end{align}
\end{theorem}
The proof, the non-asymptotic form of the mean and the variance, and the numerical validation of the theorem are given in \Cref{sec:A.Proofs}.  
Apparently, the mean prediction \eqref{eq:DGPPredictionMeanGeneral} by Bayesian PSR converges to the general PSR \eqref{eq:GeneralParameterShiftRule}
with the uncertainty \eqref{eq:DGPPredictionVarGeneral} converging to zero
in the noiseless limit, i.e., $\sigma^2 \to +0$ and hence $y_w = f^*(\bfx_w)$.
% hence 
% $(y_1, y_2) = (f^*( \bfx' - \alpha \bfe_d) ,  f^*( \bfx' + \alpha \bfe_d))$.
In noisy cases, the prior variance $\sigma_0^2 \sim O(\sigma^2)$ suppresses the amplitude of the gradient estimator as a regularizer
through the first term in the denominator in Eq.~\eqref{eq:DGPPredictionMeanGeneral}. 

\Cref{fig:GPPSRIllustraion} illustrates the behavior of Bayesian PSR when $V_d = 1$ (left panel) and when $V_d=2$ (middle panel).
In each panel,
given $2V_d$ equidistant observations (magenta crosses), the blue curve shows the (non-derivative) GP prediction with uncertainty (blue shades), while the red curve shows the derivative GP prediction with uncertainty (red shades).
Note the $\frac{\pi}{2 V_d}$ shift of the low uncertainty locations between the GP prediction (blue) and the derivative GP prediction (red). 
The green cross shows the output of the general PSR \eqref{eq:GeneralParameterShiftRule} at $\bfx' = 0$, which almost coincides with the Bayesian PSR prediction (red curve) under this setting.
Other examples, including cases where the Bayesian regularization is visible, are given in~\Cref{sec:A.Proofs}.
% The kernel parameter larger, which controls the smoothness, also affects the amplitude of the estimator.

% The mean prediction \eqref{eq:DGPPredictionMeanGeneral} differs from the general PSR \eqref{eq:GeneralParameterShiftRule} only in the larger denominator in the first term as the regularization by the prior variance $\sigma_0$, and it converges to the general PSR when the prior is infinitely weak, i.e., $\sigma_0 \to \infty$.

In the simplest first-order case, i.e., where $V_d=1, \forall d = 1, \ldots, D$, we can theoretically investigate the optimality of the choice of the shift $\alpha$ in Eq.~\eqref{eq:ParameterShiftRule}
(the proof is also given in \Cref{sec:A.Proofs}).
\begin{theorem}
\label{thrm:GPasPSR}
Assume that $V_d=1, \forall d = 1, \ldots, D$.
For any $x' \in [0, 2\pi)^D$ and $d = 1, \ldots, D$, the mean and variance of the derivative GP prediction, given  observations $\bfy = (y_1, y_2)^\T \in \mathbb{R}^2$ at two training points $\bfX = (\bfx' - \alpha \bfe_d, \bfx' + \alpha \bfe_d) \in \mathbb{R}^{D \times 2}$
with homoschedastic noise $\bfsigma = (\sigma^2, \sigma^2)^\T$,
are
\begin{align}
\widetilde{\mu}^{(d)}_{[ \bfX, \bfy, \bfsigma]}(\bfx')
& =\textstyle   \frac{ (y_2 - y_1)\sin \alpha}
{( \gamma^2 /2 + 1) \sigma^2/\sigma_0^2 + 2 \sin^2 \alpha },
\label{eq:DGPPredictionMean}\\
\widetilde{s}^{(d)}_{[ \bfX, \bfsigma]}(\bfx', \bfx')
& =\textstyle
\frac{ \sigma^2}{( \gamma^2/2 + 1 ) \sigma^2/\sigma_0^2 + 2 \sin^2 \alpha}.
\label{eq:DGPPredictionVar}
\end{align}
\end{theorem}
Again, the mean prediction \eqref{eq:DGPPredictionMean} is a regularized version of the PSR \eqref{eq:ParameterShiftRule}.
The uncertainty prediction \eqref{eq:DGPPredictionVar} 
% on the other hand implies two things.  First, the uncertainty of gradient estimation at $\bfx'$ is zero, as PSR imlies.  Note that the uncerainty of the function value at $\bfx'$, as well as the uncertainty of the gradient estimator at $\bfx'' \ne \bfx \mod \pi$, is not zero, as shown in \Cref{fig:GPPrediction}.
implies that $\alpha = \pi/2$ minimizes the uncertainty in the noisy case, regardless of $\sigma^2, \sigma_0^2$ and $\gamma$.  This supports most of the use cases of the PSR in the literature \citep{Mitarai2018}, and matches the intuition that the maximum span minimizes the uncertainty. 
% In the noisy case, the smoothness parameter also reduces the uncertainty.
However, the right panel in~\Cref{fig:GPPSRIllustraion},
where the variance~\eqref{eq:DGPPredictionVar} of the derivative GP prediction at $\bfx'$
is visualized as a function of the shift $\alpha$ of observations for $V_d=1$,
implies that the estimation accuracy is not very sensitive to the choice of $\alpha$.


\section{SGD with Bayesian PSR}
\label{sec:Gradcore}

In this section, we equip SGD with Bayesian PSR.
In the standard implementation of SGD for VQEs, $2V_d$ equidistant points along each direction $d = 1, \ldots, D$ are observed for gradient estimation by the general PSR~\eqref{eq:GeneralParameterShiftRule} (or by the PSR~\eqref{eq:ParameterShiftRule} if $V_d = 1, \forall d$) in each SGD iteration. 

\paragraph{Bayesian SGD (Bayes-SGD):}
%\paragraph{SGD-BayesPSR:}
A straightforward application of Bayesian PSR is to replace existing PSRs with Bayesian PSR for gradient estimation, allowing for the reuse of previous observations.
We retain $R \cdot 2 V_d \cdot D$ latest observations for a predetermined $R$ in our experiments.
% as long as the classical computational costs for GP prediction are negligibly smaller than the quantum computation costs. 
%We expect that 
Reusing previous observations accumulates the gradient information, and thus improves the gradient estimation accuracy,
as shown in \Cref{sec:ComparisonSGD}.

% especially in the converging phase, where the optimal point does not move much.


\subsection{Gradient Confident Region (GradCoRe)}

%We also make use of the uncertainty information provided by the Bayesian PSR, and propose an adaptive observation cost control strategy.
We propose an adaptive observation cost control strategy that leverages the uncertainty information provided by the Bayesian PSR.
This strategy adjusts the number of measurement shots for gradient estimation in each SGD iteration so that the variances of the derivative GP prediction at the current optimal point $\widehat{\bfx}$ are
%smaller than a respective required accuracy threshold.
below certain thresholds.
In a similar fashion to the CoRe~\eqref{eq:LowUncertaintySet}, we define
the \emph{gradient confident region} (GradCoRe)
\begin{align}
\widetilde{\mcZ}_{[\bfX, \bfsigma]}(\bfkappa) & = \left\{\bfx \in \mcX; \widetilde{s}^{(d)}_{[\bfX, \bfsigma]} (\bfx, \bfx) \leq \kappa_d^2, \forall d \right\},
\label{eq:GradCoRe}
\end{align} 
where $\bfkappa = (\kappa_1^2, \ldots, \kappa_D^2)^\T \in \mathbb{R}^D$ are the required accuracy thresholds.
Our proposed SGD-based optimizer, named \emph{SGD-GradCoRe}, measures new equidistant points
$\breve{\bfX} = \{\{\bfx_w^{(d)}  = \widehat{\bfx}  + \frac{2w+1}{2V_d } \pi \bfe_d\}_{w = 0}^{2V_d}\}_{d=1}^D$ for all directions with the minimum total number of shots such that the current optimal point $\widehat{\bfx}$ is in the GradCoRe (see \Cref{fig:GradcoreConcept}).

% Before starting the optimization, we collect measurements at random locations to evaluate the observation noise variance $\sigma_1^{*2} = \sigma^{*2}(1)$ for single-shot measurements. Following~\citet{ICML:Anders+:2024}, we use this information to estimate the observation noise variance as a function of the number of shots as
Before starting optimization, we evaluate the single-shot observation noise variance $ \sigma^{*2}(1) = \overline{\sigma}^{*2}$ by collecting measurements at random locations, following~\citet{ICML:Anders+:2024}.
We use this information to estimate the observation noise variance as a function of the number of shots as
%
\begin{align}
    \textstyle
 \sigma^{*2}(N_{\mathrm{shots}})=\frac{\overline{\sigma}^{*2}}{N_{\mathrm{shots}}}. 
\end{align}%
%
Let $(\bfX^t, \bfy^t, \bfsigma^t)$ be the training data (all previous observations) at the $t$-th SGD iteration step, and let $\breve{\bfnu} \in \mathbb{R}^{2V_d D}$ be the vector of the numbers of measurement shots at the new equidistant measurement points $\breve{\bfX}$ for all directions.
Before measuring at $\breve\bfX$ in the $(t+1)$-th SGD iteration, we solve the following problem:
\begin{align}
\min_{\widetilde{\bfnu}}
\|\widetilde{\bfnu}\|_1 \mbox { s.t. } 
\widehat{\bfx} \in
 \widetilde{\mcZ}_{[(\bfX^t, \breve{\bfX}), (\bfsigma^t, \breve{\bfsigma}(\widetilde{\bfn}))]} (\bfkappa(t)),
\label{eq:OptimalGradCoReProblem}
\end{align}
where $\breve{\bfsigma}(\widetilde{\bfnu}) = \overline{\sigma}^{*2} \cdot (\widetilde{\nu}_1^{-1}, \ldots, \widetilde{\nu}_{2V_d D}^{-1})^\T$,
and $\bfkappa(t)$ is the required accuracy dependent on the iteration step $t$. Informally, we minimize the total measurement budget 
%automatically 
under the constraint 
that the posterior gradient variance along each direction $d$ is smaller than the required accuracy threshold.
For simplicity, we solve the GradCoRe problem~\eqref{eq:OptimalGradCoReProblem} by grid search
% over $\nu^{-1} \in [\frac{2 \kappa^2_d}{ \overline{\sigma}^{2*}},\,1]\,\forall d$
%in each  direction independently, 
under the additional constraint that
all $2V_d D$ points are measured with an equal number of shots. 
%for the given direction $d$.

%Depending on how the required accuracy,  we propose two variants of SGD-GradCoRe.
% \paragraph{SGD-GradCoRe-norm:}
%For~\cref{eq:OptimalGradCoReProblem} one can 
We set the required accuracy thresholds to $\bfkappa(t) = \kappa^2 (t) \bfone_{D}$, where 
\begin{align}
\label{eq:gradcore_kappa}
    \kappa^2 (t) &= \textstyle \max\left(c_0,\frac{c_1}{D}\sum_{d=1}^D \left(\widetilde{\mu}^{(d)}_{[ \bfX^t, \bfy^t, \bfsigma^t]}(\widehat{\bfx}^t)\right)^2
    \right)\,.
\end{align}
Namely, $\kappa (t)$ is set proportional to the L2-norm of the estimated gradient at the current optimal point at the $t$-th SGD iteration, as long as it is larger than a lower bound. The lower bound $c_0$ and the slope $c_1$ are hyperparameters to be tuned. 
This strategy for setting the required accuracy based on the estimated gradient norm was proposed by~\citet{SGLBO2022}.
Alternatively, one could also set $\kappa_d(t) $ proportional to the absolute value of the estimated gradient separately for \textit{each} direction, i.e., $\kappa_d (t) =\max(c_0, c_1 | \widetilde{\mu}^{(d)}_{[ \bfX^t, \bfy^t, \bfsigma^t]}(\widehat{\bfx}^t)|)$,
and solve the GradCoRe problem \eqref{eq:OptimalGradCoReProblem} direction-wise.

In 
%the remainder of the paper, especially in 
the experiment plots in~\cref{sec:Experiment}, we will refer to SGD-GradCoRe as \textit{GradCoRe}. 
Further algorithmic details, including pseudo-code and used hyperparameter values, are given in~\Cref{sec:A.AlgorithmDetails}.
%\paragraph{SGD-GradCoRe-each:}
%We set $\kappa_d(t) $ proportional to the absolute value of the estimated gradient separately for \textit{each} direction, i.e., $\kappa_d (t) =c_0 + c_1 | \widetilde{\mu}^{(d)}_{[ \bfX^t, \bfy^t, \bfsigma^t]}(\widehat{\bfx}^t)|$.


% \paragraph{Bayesian NFT + SGD-GradCoRe: \memo{If necessary.}}

% In our experiments, we found as general tendency that NFT outperforms SGD in the early phase, while SGD ourperforms NFT in the converging phase.
% This observation motivates us to combine two approaches.  Namely, we start from Bayesian NFT, and switch to SGD-GradCoRe after a certain number of iterations.



% \subsection{Gradient inducing points \memo{If necessary.}}
% The classical computation complexity of GP is cubic with resepect to the number of observations, which is not ignorable if we retain all previous observations as training data.  Therefore, we replace old observations with a set of $D$ virtual derivative observations as inducing points.
% .....


\begin{figure*}[t]
    \centering
    \includegraphics[height=6ex, width=0.99\textwidth]{figures/fig2-legend.pdf}\\%\vskip -1ex
    \includegraphics[width=0.49\textwidth]{figures/fig2-energy-gamma1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig2-fidelity-gamma1.pdf}
    \centering\vskip -2ex
    \vskip -1ex
    \caption{
    Comparison between SGD with PSR (dashed curves) and SGD with Bayesian PSR (solid curves), as well as GradCoRe (red solid curve), on the Ising Hamiltonian with an $(L=3)$-layered $(Q=5)$-qubits quantum circuit.
    The energy (left) and fidelity (right) are plotted as functions of the cumulative $N_{\mathrm{shots}}$, i.e., the total number of measurement shots.
    Except GradCoRe equipped with the adaptive shots strategy, the number of shots per observation is set to $N_{\mathrm{shots}} = 128$ (blue), $256$ (green), $512$ (orange), and $1024$ (purple).
   %Acceleration by Bayesian PSR and GradCoRe.
    }
\label{fig:ComparisonPSRvsBPSR}
    %\vspace{-5mm}
\end{figure*}




\begin{figure*}[t]
    \centering
    \includegraphics[height=4ex, width=0.8\textwidth]{figures/fig3-legend.pdf}\\%\vskip -1ex
    \includegraphics[width=0.49\textwidth]{figures/fig3-energy.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig3-fidelity.pdf}
    \centering\vskip -2ex
    \vskip -1ex
    \caption{
    %\memo{Bayes-NFT, EMICORE, SUBSCORE, SGLBO, GRADCORE.}
    Energy (left) and fidelity (right) achieved within the cumulative number of measurement shots for the Ising Hamiltonian with an $(L=3)$-layered $(Q=5)$-qubits quantum circuit.   The curves correspond to SGLBO (blue), Bayes-NFT (green), EMICoRe (orange), SubsCoRe (purple), and our proposed GradCoRe (red).
    % Comparison of various baselines with GradCoRe (solid red line). EMICoRe (color) and Bayes-NFT (color) are evaluated using 1024 shots per measurement, which we identified as the optimal number for faster convergence. In contrast, SubsCoRe (color), SGLBO (color), and GradCoRe (color) utilize shot adaptation strategies. Across both metrics, GradCoRe outperforms all baselines, including the previous state-of-the-art (SubsCoRe), achieving faster convergence, lower energies, and higher fidelities.
   %Performance comparison.
    }
\label{fig:ComparisonIsingThreeFive}
    %\vspace{-5mm}
\end{figure*}



\begin{figure}[t]
    \centering
\includegraphics[height=4ex, width=0.35\textwidth]{figures/fig6-legend.pdf}\\%\vskip -1ex
%    \hspace{-100mm}    
        \centering
\includegraphics[width=0.4\textwidth]{figures/fig6-deltagrad.pdf}
    \centering\vskip -2ex
    \vskip -1ex
    \caption{
    Gradient estimation error by PSR (dashed curve) and Bayesian PSR (solid curve) for $N_{\mathrm{shots}} = 1024$,
    %for $N_{\mathrm{shots}} = 1024$, w
    evaluated by the L2-distance between the estimated gradient $\widetilde{\bfmu}(\widehat{\bfx})$ and the true gradient $\bfg^*(\widehat{\bfx})$ (computed by the PSR with simulated noiseless %($N_{\mathrm{shots}} = \infty$) 
    measurements).    
    % \memo{at each SGD step:
    % \begin{equation}
    % \Delta Grad = \sqrt{\sum_{i=1}^{D}|\nabla^{est}_{i}-\nabla^{true}_{i}|^{2}}
    % \end{equation}
    % with $est=\{PSR,BPSR\}$ and $\nabla^{est},\nabla^{true}\in \mathbb{R}^{D}$
    }
\label{fig:ImprovedGradientEstimationAccuracy}
    %\vspace{-5mm}
\end{figure}


\section{Experiments}
\label{sec:Experiment}



\subsection{Setup}
We demonstrate the performance of our Bayesian PSR and GradCoRe approaches in the same setup used by~\citet{NEURIPS:Nicoli+:2023}.
For all experiments, we prepared 50  different random initial points, from which all optimization methods start.
Our Python implementation uses Qiskit~\cite{Abraham2019} for the classical simulation of quantum hardware. The implementation for reproducing our results is attached as supplemental material. 

\paragraph{Hamiltonian and Quantum Circuit: }
We focus on the quantum Heisenberg Hamiltonian with open boundary conditions,
\begin{align}
H =\textstyle -\sum_{i\in\{X,Y,Z\}}\left[\sum_{j=1}^{Q-1} (J_i \sigma_j^i \sigma_{j+1}^i) + \sum_{j=1}^{Q}h_i\sigma_j^{i}\right] ,
\label{eq:HeisenbergHamiltonian}
\end{align}
% with the $\sigma_j^i$ being a generalized Pauli operators where the index $i\in\{X,Y,Z\}$ identifies the type of Pauli operator while the index $j$ identifies the qubit on which the operator acts. 
where $\{\sigma_j^i\}_{i\in\{X,Y,Z\}}$ are the Pauli operators acting on the $j$-th qubit. 
For the quantum circuit, we use a common ansatz, called the $L$-layered \verb|Efficient SU(2)| circuit with open boundary conditions, where $V_d = 1, \forall d$ (see~\citet{NEURIPS:Nicoli+:2023} for more details).  


\paragraph{Evaluation Metrics: }
%Our methods are evaluated 
We compare all methods 
using two metrics: the best achieved \textit{true energy} $f^*(\widehat{\bfx}), $ for $f^*(\cdot)$ defined in Eq.~\eqref{eq:VQEObjective}, and \textit{fidelity} $\langle{\psi_{\mathrm{GS}}} \vert{\psi_{\widehat{\bfx}}}\rangle \in [0,1]$. The latter is the inner product between the true ground-state wave function $\vert{\psi_{\mathrm{GS}}}\rangle$, computed by exact diagonalization of the target Hamiltonian $H$, and the trial wave function,  $\vert{\psi_{\widehat{\bfx}}}\rangle$, corresponding to the quantum state generated by the circuit using the optimized parameters $\widehat{\bfx}$.
For both metrics, we plot the difference (smaller is better) to the respective target, i.e., 
\begin{align}
\Delta\mathrm{Energy} 
&= 
\langle{\psi_{\widehat{\bfx}}}\vert  H  \vert{\psi_{\widehat{\bfx}}}
\rangle
 - \langle{\psi_{\mathrm{GS}}}\vert  H  \vert{\psi_{\mathrm{GS}}}\rangle
\notag\\
&= f^*(\widehat{\bfx}) - \langle{\psi_{\mathrm{GS}}}\vert  H  \vert{\psi_{\mathrm{GS}}}\rangle,
\label{eq:DeltaEnergy}\\
\Delta \mathrm{Fidelity}
&= \langle{\psi_{\mathrm{GS}}} \vert{\psi_{\mathrm{GS}}}\rangle
-
\langle{\psi_{\mathrm{GS}}} \vert{\psi_{\widehat{\bfx}}}\rangle
\notag\\
&= 1
-
\langle{\psi_{\mathrm{GS}}} \vert{\psi_{\widehat{\bfx}}}\rangle,
\label{eq:DeltaFidelity}
\end{align}
in log scale.
Here, $\vert{\psi_{\mathrm{GS}}}\rangle$ and $\langle{\psi_{\mathrm{GS}}}\vert  H  \vert{\psi_{\mathrm{GS}}}\rangle$ are the 
%ground-state 
wave function and true energy at the ground-state, respectively, both of which are computed analytically.
As a measure of the quantum computation cost, we consider the total number of measurement shots \emph{per operator group} (see~\Cref{ft:footnote1}) for all observations over the whole optimization process.

\paragraph{Baseline Methods: }

% \begin{itemize}
%     \item SGD (ADAM) with stadard PSR
%     \item Bayesian SGD (ADAM with derivative GP)
%     \item SGLBO \citep{SGLBO2022}
%     \item NFT  \memo{with stabilizer}
%     \item Bayesian NFT (with GP)  \memo{with stabilizer and inducing point}
%     \item EMICoRe  \memo{with stabilizer and inducing point}
%     \item SubsCoRe  \memo{with stabilizer and inducing point}
% \end{itemize}
%  \memo{No stabilizer nor inducing point necessary for gradient-based methods}

We compare our Bayesian SGD and GradCoRe approaches to the baselines, including SGD with the PSR \eqref{eq:ParameterShiftRule}, Bayesian NFT,
SGLBO \citep{SGLBO2022}, EMICoRe \citep{NEURIPS:Nicoli+:2023}, and SubsCoRe~\citep{ICML:Anders+:2024}.
% SGD uses the PSR \eqref{eq:ParameterShiftRule} for gradient estimation.
We exclude the original NFT \citep{nakanishi20} because it is outperformed by Bayesian NFT (see~\Cref{fig:NFTvsBayesNFT} in \Cref{sec:A.NFT}). 

\paragraph{Algorithm Setting: } 
All SGD-based methods use the ADAM optimizer with $l_{r}=0.05, \ \beta s=(0.9, 0.999)$. 
For the methods not equipped with adaptive cost control (i.e., all methods except 
SGLBO, SubsCoRe and GradCoRe),
we set $N_{\mathrm{shots}} = 1024$ for each observation---the same setting as in~\citet{NEURIPS:Nicoli+:2023}---unless specified explicitly.
%in~\citet{NEURIPS:Nicoli+:2023}. 
To avoid error accumulation, all SMO-based methods measure the ``center'', i.e., the current optimal point without shift, every $D+1$ iterations~\citep{nakanishi20}.
% The SMO-based methods, EMICoRe and SubsCoRe, enhanced by Bayesian approaches, represent the current state-of-the-art. 
Bayes-SGD and GradCoRe estimate the gradient from 
the $R \cdot 2 V_d \cdot D$ latest observations for $R=5$,
and
GradCoRe 
initially uses the fixed threshold $\kappa^2(t) = \overline{\sigma}^{*2} / 256$
%$uses $N_{\mathrm{shots}} = 134$ 
before starting the cost adaption after $D$ SGD iterations.

Further details on the algorithmic and experimental settings are described in~\Cref{sec:A.AlgorithmDetails}
and \Cref{sec:A.ExperimentalDetails}, respectively.

\subsection{Improvement over SGD with Bayesian PSR and GradCoRe}
\label{sec:ComparisonSGD}

First, we investigate how our Bayesian PSR and GradCoRe improve SGD. \Cref{fig:ComparisonPSRvsBPSR} compares 
SGD with the standard PSR (SGD) and SGD with Bayesian PSR (Bayes-SGD)
on the Ising Hamiltonian, i.e., Eq.~\eqref{eq:HeisenbergHamiltonian} for $J_{i\in\{X,Y,Z\}} = (-1,0,0)$ and $h_{i\in\{X,Y,Z\}} = (0,0,-1)$,
%\memo{$J_i=,..., h_i= ...$}
with an $(L=3)$-layered $(Q=5)$-qubits quantum circuit.
Both for SGD and Bayes-SGD, the optimization performance with $N_{\mathrm{shots}} = 128, 256, 512, 1024$ measurement shots are shown. 
The left and right panels 
plot the difference to the ground-state in true energy~\eqref{eq:DeltaEnergy} and fidelity~\eqref{eq:DeltaFidelity} achieved by each method
as functions of the cumulative $N_{\mathrm{shots}}$, i.e., the total number of measurement shots.
To the right of each panel,
the \emph{trial density}, i.e., the distribution over the trials computed by kernel-density estimation, 
 after the use of $1\times10^7$ total measurement shots is depicted.
The median, the $25$-th and the $75$-th percentiles are shown as a solid curve and shades, respectively.
We observe that Bayesian PSR,
with a more accurate gradient estimator as shown in 
\Cref{fig:ImprovedGradientEstimationAccuracy},
is comparable or compares favorably to the original SGD.
%The plots also compare GradCore, which significantly enhances the SGD performance.
More importantly,
we observe that GradCoRe automatically selects the optimal number of measurement shots in each optimization phase, thus outperforming SGD and Bayes-SGD with different fixed number $N_{\mathrm{shots}}$ of shots through the entire optimization process.
%Compared to the naive SGD approach, Bayes-SGD slightly improves the convergence while GradCoRe significantly improves the performance by automatically choosing the best performing number of measurement shots.
%The accuracy gain in gradient estimation by Bayesian PSR, t
The adaptively selected number of shots and the accuracy threshold $\kappa(t)$ for GradCoRe are shown in~\Cref{sec:A.DetailedBehaviorGradcore}.




\subsection{Comparison with State-of-the-art Methods}
\label{sec:ComparisonAll}

\Cref{fig:ComparisonIsingThreeFive}
compares GradCoRe
%, which performed best in \Cref{sec:ComparisonSGD}, 
to the baseline methods, SGLBO, Bayes-NFT, EMICoRe, and 
%the previous state-of-the-art, 
SubsCoRe.
%Although SGD-based methods tend to be outperformed by SMO-based methods, our version of SGD with
Our GradCoRe, which significantly improves upon SGD as shown in \Cref{fig:ComparisonPSRvsBPSR},
establishes itself as the new state-of-the-art, exhibiting faster convergence and achieving lower overall energy.

% \memo{We also conducted experiments 
% %with different $Q$ and $L$, as well as 
% for the Heisenberg Hamiltonian, on which the results are reported in~\Cref{sec:A.AdditionalExperimentalResults}.}

\section{Conclusion}
\label{sec:Conclustion}

The physical properties of variational quantum eigensolvers (VQEs) allow us to use specialized optimization methods, i.e., stochastic gradient descent (SGD) with parameter shift rules (PSRs) and a specialized sequential minimal optimization (SMO), called NFT~\cite{nakanishi20}.  Recent research has shown that those properties can be appropriately captured by the physics-informed VQE kernel, with which NFT has been successfully improved through Bayesian machine learning techniques.
% Bayesian approaches with Gaussian processes (GPs) equipped with the  is effective for improving NFT 
For instance, observations in previous SMO iterations are used to determine the optimal measurement points~\cite{NEURIPS:Nicoli+:2023}, and observation costs are minimized based on the uncertainty prediction~\cite{ICML:Anders+:2024}.
In this paper, we have shown that a similar approach can also improve SGD-based methods.  Specifically, we proposed Bayesian PSR, where the gradient is estimated by derivative Gaussian processes (GPs).
Bayesian PSR generalizes existing PSRs to allow for flexible estimation from observations at an arbitrary set of locations.
Furthermore, it provides uncertainty information, which enables observation cost adaptation through the novel notion of gradient confident region (GradCoRe).
Our theoretical analysis revealed the relation between Bayesian PSR and existing PSRs, while our numerical investigation empirically demonstrated the utility of our approaches.
We envisage that Bayesian approaches will facilitate further development of more efficient algorithms for VQEs and, more generally, quantum computing.
In future work, we aim to explore the optimal combination of existing methods and strategies for selecting the most suitable approaches for specific tasks, i.e., specific Hamiltonians.
%As future work, we explore optimal combination of methods, and ways of choosing best methods for given tasks, i.e., Hamiltonians.


\section*{Impact Statement}

This paper presents work whose goal is to advance the field of machine learning and quantum computing. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.


% Acknowledgements should only appear in the accepted version.
 \section*{Acknowledgements}
% The authors thank the reviewers for their constructive comments and discussion for improving the paper. 
This work was supported by the German Ministry for Education and Research (BMBF) under the grant BIFOLD24B, 
%the Einstein Research Unit Quantum Project (ERU-2020-607), 
the European Unionâs HORIZON MSCA Doctoral Networks programme project AQTIVATE (101072344), the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) as part of the CRC 1639 NuMeriQS â project no. 511713970,
the European Unionâs Horizon Europe Framework Programme (HORIZON) under the ERA Chair scheme with grant agreement no.\ 101087126,
and the Ministry of Science, Research and Culture of the State of Brandenburg within the Centre for Quantum Technologies and Applications (CQTA).


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{main.bib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{General Gaussian Processes (GPs) with Derivative Outputs}
\label{sec:A.DerivativeGPGeneral}

The derivative GP regression can be straightforwardly extended to the case where both training outputs (i.e., observations), and test outputs (i.e., predictions) contain different orders of derivatives.


Assume that we have a set of input points, and for each input point $\bfx \in \mathbb{R}^D$,
 the corresponding output, i.e., observation or prediction, is $f(\bfx)$ or $\partial_{x_d} f(\bfx) $, where $\partial_{x_d}  \equiv \frac{\partial}{\partial x_d}  $.
Let us denote the derivative kernel functions as
\begin{align}
\widetilde{k}^{(d, d')} (\bfx, \bfx') 
&=
\begin{cases}
 k(\bfx, \bfx') & \mbox{ if } d = 0, d' = 0,\\
\partial_{x'_{d'}} k(\bfx, \bfx') & \mbox{ if } d = 0, d' = 1, \ldots, D,\\
\partial_{x_{d}} k(\bfx, \bfx') & \mbox{ if } d= 1, \ldots, D, d' = 0,\\
 \partial_{x_{d}} \partial_{x'_{d'}}  k(\bfx, \bfx') & \mbox{ if } d = 1, \ldots, D, d' = 1, \ldots, D. 
 \end{cases}
\notag
\end{align}

For training points $\bfX = \{\bfx^{(n)}\}_{n=1}^N$ and test points $\bfX' = \{\bfx'^{(m)}\}_{m=1}^M$,
we should set the the entries of the train-train $\bfK  \in \mathbb{R}^{N \times N}$, train-test $\bfK'  \in \mathbb{R}^{N \times M}$, and test-test $\bfK''  \in \mathbb{R}^{M \times M}$ kernels as
\begin{align}
K_{n, n'} &= \widetilde{k}^{(d(\bfx_n), d(\bfx_{n'}))} (\bfx_n, \bfx_{n'}) ,
\label{eq:A.DerivativeGPGeneral.traintrain} \\ 
K'_{n, m} &= \widetilde{k}^{(d(\bfx_{n}), d(\bfx_{m}))} (\bfx_n, \bfx_{m}) ,
\label{eq:A.DerivativeGPGeneral.traintest} \\
K''_{m, m'} &= \widetilde{k}^{(d(\bfx_{m}), d(\bfx_{m'}))} (\bfx_m, \bfx_{m'}),
\label{eq:A.DerivativeGPGeneral.testtest} 
\end{align}
where 
\begin{align}
d(\bfx)
&=
\begin{cases}
0 & \mbox{ if the corresponding output for the input $\bfx$ is $f(\bfx)$}, \\
d & \mbox{ if the corresponding output for the input $\bfx$ is $\partial_{x_d}f(\bfx)$}. 
 \end{cases}
 \notag
\end{align}

% GP posterior can be analytical obtained in the standard way with these kernel matrices.
Eqs.\eqref{eq:GPPosterior}--\eqref{eq:GPPosteriorVar} with the kernel matrices $\bfK, \bfK', \bfK''$ set as Eqs.\eqref{eq:A.DerivativeGPGeneral.traintrain}--\eqref{eq:A.DerivativeGPGeneral.testtest}
give the posterior GP for the corresponding test outputs.

For higher-order derivative outputs, 
we can define the kernels in exactly the same way as above, by applying the same derivative operators  to the kernels as the ones applied to the outputs, i.e.,
\begin{align}
\widetilde{k}(\bfx, \bfx') 
&=
\left[
\partial_{x_{1}}^{(r_1)} 
 \cdots \partial_{x_{D}}^{(r_D)}
 \right]
\left[
\partial_{x'_{1}}^{(r'_1)} 
\cdots
\partial_{x'_{D}}^{(r'_D)} 
\right]
k(\bfx, \bfx'),
\notag
\end{align}
if the corresponding outputs  at $\bfx$ and $\bfx'$ are
$\partial_{x_{1}}^{(r_1)} 
 \cdots \partial_{x_{D}}^{(r_D)}
 f(\bfx)$
and
$\partial_{x'_{1}}^{(r'_1)} 
\cdots
\partial_{x'_{D}}^{(r'_D)}  f(\bfx')$,
respectively,
where $\partial_{x_d}^{(r)} \equiv\frac{\partial^{r}} {\partial {x_d}^{r}}$ denotes the $r$-th order derivative with respect to $x_d$.





\section{Nakanishi-Fuji-Todo (NFT) Algorithm \citep{nakanishi20} and Bayesian NFT}
\label{sec:A.NFT}
Let $\{\bfe_d\}_{d=1}^{D}$ be the standard basis.
NFT is initialized with a random point $\widehat{\bfx}^{0}$ with a first observation $\widehat{y}^0 = f^*(\widehat{\bfx}^0) + \varepsilon_0$, and iterates the following procedure: for each iteration step $t$, 
\begin{enumerate}
\itemsep0em 
    \item Select an axis $d \in \{1, \ldots, D\}$ sequentially and observe the objective $\bfy \in \mathbb{R}^{2V_d}$ at $2V_d$ points 
$\bfX = (\bfx_1, \ldots, \bfx_{2V_d}) =  \{ \widehat{\bfx}^{t-1} +  \alpha_w  \bfe_{{d}}  \}_{w=1}^{2V_d} \in \mathbb{R}^{D \times 2V_d}$ along the axis $d$.%
\footnote{
With slight abuse of notation, we use the set notation to specify the column vectors of a matrix, i.e., $(\bfx_1, \ldots, \bfx_N) = \{\bfx_n\}_{n=1}^N$.
%\in \mathbb{R}^{D \times N}$
}
Here $\bfalpha \in [0, 2 \pi)^{2V_d}$ is such that $\alpha_w \ne 0,\, \alpha_{w'} \ne \alpha_w$, for all $w$ and $w' \ne w$.

    \item 
    \label{stp:OneDOptStep}
    Apply the 1D trigonometric polynomial regression $\widetilde{f}(\theta) =  \widetilde{\bfb}^\T \bfpsi_{1} (\theta)$ to the $2V_d$ new observations $\bfy$, together with the previous best estimated score $\widehat{y}^{t-1}$, and analytically compute the new optimum $\widehat{\bfx}^t = \widehat{\bfx}^{t-1} +  \widehat{\theta} \bfe_{{d}}$, where $\widehat{\theta} =\argmin_{\theta }\widetilde{f}(\theta)$. 
    \item Update the best score by $ \widehat{y}^t =  \widetilde{f}(\widehat{\theta})$.
\end{enumerate}
Note that if the observation noise is negligible, i.e., $y \approx f^*(\bfx)$, 
each step of NFT reaches the global optimum in the 1D subspace along the chosen axis $d$ for any choice of $\bfalpha$, and thus performs SMO exactly. Otherwise, errors can be accumulated in the best score $\widehat{y}^t$, and therefore an additional measurement may need to be performed at $\widehat{\bfx}^t$ after a certain iteration interval.

Bayesian NFT (Bayes-NFT) performs the 1D trigonometric polynomial regression and optimization in Step~\ref{stp:OneDOptStep} with GP with the VQE kernel \eqref{eq:VQEKernelTied}, where all previous observations are used for training.  Using previous observations allows prediction with smaller uncertainty and thus more accurate subspace optimization.
\Cref{fig:NFTvsBayesNFT} compares the original NFT and Bayesian NFT on the Ising Hamiltonian with an $(L=3)$-layered $(Q=5)$-qubits quantum circuit with different number of shots per observation.  We observe that using GP generally accelerates the optimization process.



\begin{figure*}[t]
    \centering
    \includegraphics[height=7ex, width=0.8\textwidth]{figures/fig4-legend.pdf}\\%\vskip -1ex
    \includegraphics[width=0.49\textwidth]{figures/fig4-energy.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig4-fidelity.pdf}
    \centering\vskip -2ex
    \vskip -1ex
    \caption{
 Comparison between NFT~\cite{nakanishi20} and Bayes-NFT for the Ising Hamiltonian with an $(L=3)$-layered $(Q=5)$-qubits quantum circuit.
    The energy (left) and fidelity (right), in the forms of Eqs.\eqref{eq:DeltaEnergy} and \eqref{eq:DeltaFidelity}, respectively, are plotted as functions of the cumulative $N_{\mathrm{shots}}$, i.e., the total number of measurement shots.
    The number of shots per observation is set to $N_{\mathrm{shots}} = 128$ (blue), $256$ (green), $512$ (orange), and $1024$ (purple).
 }
\label{fig:NFTvsBayesNFT}
    %\vspace{-5mm}
\end{figure*}





\section{Proofs} 
\label{sec:A.Proofs}

Here, we give proofs of theorems in Section~\ref{sec:ProposedMethod}, and numerically validate them.

\subsection{Proof of \Cref{thrm:GPasGeneralPSR}}
\label{sec:A.Proof.GPasGeneralPSR}

We start from a more general theorem than Theorem~\ref{thrm:GPasGeneralPSR}, which is proven in \Cref{sec:A.Proof.GPasGeneralPSRMostGeneral}. 




\begin{theorem}
\label{thrm:GPasGeneralPSRMostGeneral}
Assume that, for any given point $\widehat{\bfx} \in [0, 2\pi)^D$, we have observations  
$\bfy = (y_0, \ldots, y_{2V_d -1})^\T \in \mathbb{R}^{2 V_d}$ at $2V_d$ equidistant training points $\bfX = (\bfx_0, \ldots, \bfx_{2V_d -1}) \in \mathbb{R}^{D \times 2V_d}$ for $\bfx_w  = \widehat{\bfx}  + \frac{2w+1}{2V_d } \pi \bfe_d$ with homoschedastic noise $\bfsigma = \sigma^2 \cdot \bfone_{2V_d} \in \mathbb{R}^{2V_d}$.
Then, the mean and variance of the derivative $\partial_d {f} (\bfx')$ prediction at $\bfx' = \widehat{\bfx} + \alpha' \bfe_d$ for any $d = 1, \ldots, D$ and $\alpha' \in [0, 2\pi)$ are
given as
\begin{align}
\widetilde{\mu}^{(d)}_{[ \bfX, \bfy, \bfsigma]}(\bfx')
& =\frac{ \sum_{w=0}^{2V_d-1} (-1)^w y_w \left(  
\frac{   \cos(   V_d \alpha' )   }{2 \sin^2\left(\frac{(2w+1)\pi}{4V_d} - \alpha' /2\right)} +  \frac{ V_d \sin ( \frac{(2w+1)\pi}{4V_d} -   (V_d + 1/2)  \alpha' )  }{\sin(\frac{(2w+1)\pi}{4V_d} - \alpha' /2)}
-         \frac {4 V_d^2     \cos V_d\alpha' } {( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 4V_d}   \right) }
{(\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d }  ,
\label{eq:A.DGPPredictionMeanGeneral.MostGeneral}\\
\widetilde{s}^{(d)}_{[ \bfX, \bfsigma]}(\bfx', \bfx')
& = \sigma^2 \left(
\frac{    V_d (V_d+1)(2V_d + 1)}{3 ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d )}
-  \frac{ 4V_d^3 \cos \left( 2 V_d \alpha' \right)        }{((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d) ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  4 V_d) }   
\right)
\notag\\
& \hspace{40mm}
-  \sigma_0^2  \frac{ 8V_d^4 (\cos \left( 2 V_d \alpha' \right)  -1)    }{(\gamma^2 +  2V_d)((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d) ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  4 V_d) }   .
\label{eq:A.DGPPredictionVarGeneral.MostGeneral}
\end{align}
\end{theorem}
Regardless of the observations, the predictive uncertainty \eqref{eq:A.DGPPredictionVarGeneral.MostGeneral} is periodic with respective to $\alpha'$ with the period of $\pi/V_d$.
We can easily get the following corollaries.
\begin{corollary}
\label{thrm:GPasGeneralPSRCenter1}
For the test point at $\bfx' = \widehat{\bfx}$, i.e., $\alpha' = 0$, the mean of the derivative GP prediction is 
\begin{align}
\widetilde{\mu}^{(d)}_{[ \bfX, \bfy, \bfsigma]}(\bfx')
& = \frac{ \sum_{w=0}^{2V_d-1} (-1)^w y_w \left(  
\frac{   1  }{2 \sin^2\left(\frac{(2w+1)\pi}{4V_d}\right)} 
+         \frac {V_d ( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}    } {( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 4V_d}   \right) }
{(\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d }  ,
\label{eq:A.DGPPredictionMeanGeneral.Center}
\end{align}
\end{corollary}


\begin{corollary}
\label{thrm:GPasGeneralPSRCenter2}
For the test point at $\bfx' = \widehat{\bfx} + \alpha' \bfe_d, \forall \alpha' = 0, \pi/V_d, 2 \pi/V_d, \ldots, (2 V_d-1)\pi/V_d$, 
 the variance of the derivative GP prediction is 
\begin{align}
\widetilde{s}^{(d)}_{[ \bfX, \bfsigma]}(\bfx', \bfx')
& = \sigma^2 
\left(
\frac{    V_d (V_d+1)(2V_d + 1)}{3 ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d )}
-  \frac{ 4V_d^3         }{((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d) ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  4 V_d) }   
\right).
\label{eq:A.DGPPredictionVarGeneral.Center}
\end{align}
\end{corollary}
Ignoring high order terms with respect to $\sigma^2/\sigma_0^2$
in Eqs.\eqref{eq:A.DGPPredictionMeanGeneral.Center} and \eqref{eq:A.DGPPredictionVarGeneral.Center} gives 
\Cref{thrm:GPasGeneralPSR}.
\QED


\begin{figure*}[t]
    \centering
     \includegraphics[width=0.49\textwidth]{figures/NUMandANAL.jpeg}
     \includegraphics[width=0.49\textwidth]{figures/NUMandANAL_strreg.jpeg}
    \centering
    \vskip -1ex
    \caption{
Numerical validation of \Cref{thrm:GPasGeneralPSRMostGeneral} under two parameter settings (see above each panel).
Given the $2V_d$ equidistant observations (magenta crosses), 
the derivative GP prediction (blue curve) with uncertainty (blue shades) is compared to their analytic forms \eqref{eq:A.DGPPredictionMeanGeneral.MostGeneral} and \eqref{eq:A.DGPPredictionVarGeneral.MostGeneral}, i.e., the mean function 
(red curve) and the variance function  (red shades), respectively.
We observe that our theory perfectly matches the numerical computation.
The green cross shows the prediction by the general PSR \eqref{eq:GeneralParameterShiftRule}, which almost coincides with Bayesian PSR prediction when $\sigma^2/\sigma_0^2 = 0.01$ (left panel),
while a significant difference is observed when $\sigma^2/\sigma_0^2 = 0.1$ (right panel). 
    }
\label{fig:NumericalValidation}
    %\vspace{-5mm}
\end{figure*}


\Cref{fig:NumericalValidation} shows numerical validation of \Cref{thrm:GPasGeneralPSRMostGeneral}, where the derivative GP prediction (blue curve) with uncertainty (blue shades) is compared to their analytic forms, i.e., the mean function \eqref{eq:A.DGPPredictionMeanGeneral.MostGeneral} (red curve) and the variance function \eqref{eq:A.DGPPredictionVarGeneral.MostGeneral} (red shades), respectively, under two settings of noise and kernel parameters.
We observe that our theory perfectly matches the numerical computation.
When $\sigma^2/\sigma_0^2 = 0.01$ (left panel), the regularization is small enough and the Bayesian PSR prediction (red curve) almost coincides with the general PSR prediction (green cross). On the other hand, when $\sigma^2/\sigma_0^2 = 0.1$ (right panel), the Bayesian PSR prediction (red) does not match the general PSR prediction (green cross), because of the regularization.




\subsection{Mathematical Preparations}
\label{sec:A.Proof.Preparation}
Before proving 
Theorem~\ref{thrm:GPasGeneralPSRMostGeneral},
we give some mathematical identities on the trigonometric functions.

\subsubsection{Root of Unity}

For a natural number $N \in \{ 1, 2, \ldots\}$,
let us define a root of unity $\rho_N = e^{2\pi i/N}$ such that $\rho_N^N = 1$.  Then, the following hold:
\begin{align}
\sum_{n=0}^{N-1} \rho_N^{nk} = \frac{1 - \rho_{N}^{kN}}{1 - \rho_{N}^k} =  0 \qquad \mbox{ for } \qquad  k = 1, \ldots, N-1,
\label{eq:RootOfUnityOne}\\
\sum_{n=0}^{N-1} \rho_N^{(n + \phi)k} = \rho_N^{k \phi } \sum_{n=0}^{N-1} \rho_N^{nk} = \rho_N^{k \phi  }\frac{1 - \rho_{N}^{kN}}{1 - \rho_{N}^k} =  0 \qquad \mbox{ for } \qquad  k = 1, \ldots, N-1,
\label{eq:RootOfUnityTwo}
\end{align}
It also holds for even $N$ that
\begin{align}
\sum_{n=0}^{N-1} \rho_N^{(n + 1/2 )k + nN/2 } = \rho_N^{k/2 } \sum_{n=0}^{N-1} \rho_N^{n(k + N/2)} = \rho_N^{k/2 }\frac{1 - \rho_{N}^{(k+N/2)N}}{1 - \rho_{N}^{(k+N/2)}} =  0 \qquad \mbox{ for } \qquad  k = 1, \ldots, N/2-1.
\label{eq:RootOfUnityThree}
\end{align}








\subsubsection{Properties of Dirichlet Kernel}



The summation in the Dirichlet kernel can  be analytically performed as 
 \begin{align}
1+ 2   \sum_{n=1}^{N}   \cos \left( n x  \right) 
=
1+ 2   \sum_{n=1}^{N}  \frac{e^{i nx} + e^{-i nx}}{2}
 &= \sum_{n=-N}^{N} e^{i nx}
\notag\\
 &=e^{-i Nx}  \frac{1 - e^{i (2N+1)x}}{1 - e^{i x}}
\notag\\
%  &=  \frac{e^{-i Nx} - e^{i (N+1)x}}{1 - e^{i x}}
% \notag\\
 &=  \frac{e^{-i (N + 1/2)x} - e^{i (N+1/2)x}}{e^{-i x/2} - e^{i x/2}}
\notag\\
 &=  \frac{\sin((N + 1/2)x)}{\sin(x/2)}.
 \label{eq:DirichletKernelAnalytic}
 \end{align}
 Therefore, it also holds that
  \begin{align}
2  \sum_{n=1}^{N}  n  \sin \left( n x  \right) 
&=
-  \sum_{v=1}^{V_d}    \frac{\partial}{\partial x} \left(1/V_d + 2 \cos \left( n x  \right) \right)
 \notag\\
&=
-    \frac{\partial}{\partial x} \left(1+ 2  \sum_{v=1}^{V_d} \cos \left( n x  \right) \right)
 \notag\\
&=
-   \frac{(N + 1/2)  \cos((N + 1/2)x) \sin(x/2)  -  \frac{1}{2}  \sin((N + 1/2)x) \cos(x/2)  }{\sin^2(x/2)}
 \notag\\
&=
-   \frac{N \cos((N + 1/2)x) \sin(x/2)      -  \frac{1}{2}  \sin(Nx)   }{\sin^2(x/2)}
 \notag\\
&=
\frac{   \sin(Nx)   }{2 \sin^2(x/2)} -  \frac{ N \cos((N + 1/2)x)  }{\sin(x/2)}.
 \label{eq:DirichletKernelAnalyticDerivative}
 \end{align}




\subsection{Proof of Theorem~\ref{thrm:GPasGeneralPSRMostGeneral}}
\label{sec:A.Proof.GPasGeneralPSRMostGeneral}

For derivative predictions $\partial_{d} f(\bfx'), \partial_{d} f(\bfx'')$, 
the test kernels should be modified as Eqs.\eqref{eq:DerivativeTrTeKernel} and \eqref{eq:DerivativeTeTeKernel}.
For the VQE kernel \eqref{eq:VQEKernelTied}, they are
\begin{align}
\widetilde{k}(\bfx, \bfx')
&=
\partial_{x'_{d}} k (\bfx, \bfx')
= 
\sigma_0^2
\left(\frac{  2\sum_{v=1}^{V_{d}}  v \sin \left( v(x_{d} - x_{d}')  \right)}{ \gamma^{2} + 2 V_{d}}\right)
\prod_{d' \ne d} \left(\frac{  \gamma^{2} + 2\sum_{v=1}^{V_{d'}}  \cos \left( v(x_{d'} - x_{d'}')  \right)}{ \gamma^{2} + 2 V_{d'} }\right),
\label{eq:DerivativeTrainTestKernel} \\
\widetilde{k}(\bfx', \bfx'')
&=
\partial_{x'_{d}} \partial_{x''_{d}} k (\bfx', \bfx'')
= 
\sigma_0^2
\left(\frac{  2 \sum_{v=1}^{V_{d}} v^2 \cos \left( v(x_{d}' - x_{d}'')  \right)}{ \gamma^{2} + 2 V_{d}}\right)
\prod_{d' \ne d}\left(\frac{  \gamma^{2} + 2\sum_{v=1}^{V_{d'}}  \cos \left( v(x_{d'}' - x_{d'}'')  \right)}{ \gamma^{2} + 2 V_{d'} }\right).
\label{eq:DerivativeTestTestKernel} 
\end{align}
The training  kernel matrix for   $\{\bfx_w = \widehat{\bfx}  + \frac{2w+1}{ 2V_d } \pi \bfe_d\}_{w = 0}^{2V_d-1}$ is Toeplitz as
\begin{align}
{\bfK} 
&= 
\sigma_0^2
\begin{pmatrix}
\tau_0 & \tau_1  & \tau_2 &  \cdots & \tau_{2V_d-2}  & \tau_{2V_d-1}   \\
\tau_1 & \tau_0&\tau_1\\
\tau_2 & \tau_1 & \tau_0 &&& \vdots\\
\vdots & & & \ddots \\
  \tau_{2V_d-2} & & & & \tau_0 & \tau_1 \\
 \tau_{2V_d-1} & &\cdots & & \tau_1& \tau_0 
     \end{pmatrix}
     \in \mathbb{R}^{2V_d \times 2V_d },
\notag
\end{align}
where
\begin{align}
\tau_w
&= 
\frac{  \gamma^{2} + 2\sum_{v=1}^{V_d}   \cos \left( \frac{v w}{ 2V_d}2\pi  \right)}{ \gamma^{2} +  2V_d }.
\label{eq:TauW}
\end{align}

For a test point at $\bfx' = \hat{\bfx}  +\alpha' \bfe_d$,
the test kernel components are
\begin{align}
\widetilde{\bfk}' 
&= 
\sigma_0^2
\begin{pmatrix}
\kappa_0  \\
\kappa_1 \\ 
\vdots \\
\kappa_{2V_d-1}
\end{pmatrix},
\notag\\
\widetilde{k}'' &= 
\sigma_0^2,
\notag
\end{align} 
where
\begin{align}
\kappa_w
&=
  \frac{   2\sum_{v=1}^{V_d}  v  \sin \left( v \left(\frac{2w+1}{2V_d}\pi - \alpha' \right)  \right)}{ \gamma^{2} +  2V_d}.
\label{eq:KappaW}
\end{align}

The first identity \eqref{eq:RootOfUnityOne} for the root of unity implies that 
\begin{align}
\sum_{v=0}^{2V_d-1} e^{vw \frac{2\pi i  }{2V_d}} =  0 \qquad \mbox{ for } \qquad w = 1, \ldots, 2 V_d-1,
\notag
\end{align}
and therefore
\begin{align}
\sum_{v=0}^{2V_d-1} \cos \left( vw \frac{2\pi   }{ 2V_d}\right) 
& =  
\begin{cases}
2V_d & \mbox{ for } \qquad w=0, 2V_d,\\
0 & \mbox{ for } \qquad w = 1, \ldots, 2 V_d-1,
\end{cases}
\label{eq:EvenFourierRuleCos}\\
\sum_{v=0}^{2V_d-1} \sin \left( vw \frac{2\pi   }{2V_d}\right) 
& =  
0 \qquad \mbox{ for } \qquad w = 0, \ldots, 2 V_d.
\label{eq:EvenFourierRuleSin}
\end{align}
The second identity \eqref{eq:RootOfUnityTwo} for the root of unity gives
\begin{align}
\sum_{v=0}^{2V_d-1} e^{(v + 1/2)w \frac{2\pi i  }{2V_d}} = \sum_{v=0}^{2V_d-1} e^{(2v + 1)w \frac{\pi i  }{2V_d}} =  0 \qquad \mbox{ for } \qquad w = 1, \ldots, 2 V_d-1,
\notag
\end{align}
and therefore
\begin{align}
\sum_{v=0}^{2V_d-1} \cos \left( (2v + 1) w \frac{\pi   }{ 2V_d}\right) 
& =  
\begin{cases}
2V_d & \mbox{ for } \qquad w=0, \\
- 2V_d & \mbox{ for } \qquad w=2 V_d , \\
0 & \mbox{ for } \qquad w = 1, \ldots, 2 V_d-1,
\end{cases}
\label{eq:OddFourierRuleCos}\\
\sum_{v=0}^{2V_d-1} \sin \left( (2v + 1)w \frac{\pi   }{2V_d}\right) 
& =  
0 \qquad \mbox{ for } \qquad w = 0, \ldots, 2 V_d.
\label{eq:OddFourierRuleSin}
\end{align}
The third identity \eqref{eq:RootOfUnityThree} for the root of unity gives
\begin{align}
\sum_{v=0}^{2V_d-1} e^{((v + 1/2)w  + vV_d)\frac{2\pi i  }{2V_d}} = \sum_{v=0}^{2V_d-1}  e^{v\pi i  }   e^{(2v + 1)w \frac{\pi i  }{2V_d}}
=  \sum_{v=0}^{2V_d-1}  (-1)^v   e^{(2v + 1)w \frac{\pi i  }{2V_d}} =  0 \quad \mbox{ for } \quad w = 1, \ldots, V_d-1,
\notag
\end{align}
and therefore
\begin{align}
\sum_{v=0}^{2V_d-1} (-1)^v\cos \left( (2v + 1) w \frac{\pi   }{ 2V_d}\right) 
& =  
0 \qquad  \mbox{ for } \qquad w = 0, \ldots, V_d,
\label{eq:OddFourierRuleCvecCos}\\
\sum_{v=0}^{2V_d-1} (-1)^v \sin \left( (2v + 1)w \frac{\pi   }{2V_d}\right) 
& =  
\begin{cases}
 2V_d & \mbox{ for } \qquad w= V_d  , \\
0 & \mbox{ for } \qquad w = 0,  \ldots,  V_d-1.
\end{cases}
\label{eq:OddFourierRuleCvecSin}
\end{align}
From the symmetry of the trigonometric functions, it holds that
\begin{align}
\sum_{v=1}^{V_d} \cos \left( vw \frac{2\pi   }{2V_d}\right) 
&= 
\begin{cases}
-1& \mbox{ for } \qquad w = 1, 3, 5, \ldots, 2 V_d-1,\\
0 & \mbox{ for } \qquad w = 2, 4, 6, \ldots, 2 V_d,
\end{cases}
\label{eq:SymmetryCos}\\
\sum_{v=1}^{V_d} \sin \left( vw \frac{2\pi   }{2V_d}\right) 
&= - \sum_{v=V_d+1}^{2V_d} \sin \left( vw \frac{2\pi   }{2V_d}\right) .
\label{eq:SymmetrySin}
\end{align}
Note that the factor $-1$ in the odd $w$ case in Eq.~\eqref{eq:SymmetryCos} is because the summand is $-1$ for $v = V_d$, while the summands for the other $v$ are canceled each other.

By using Eq.~\eqref{eq:SymmetryCos},
Eq.~\eqref{eq:TauW} can be written as
\begin{align}
\tau_w
&= 
\frac{  \gamma^{2} + 2\sum_{v=1}^{V_d}   \cos \left( \frac{v w}{ 2V_d}2\pi  \right)}{ \gamma^{2} +  2V_d }
=
\begin{cases}
1 &   \mbox{ for } \qquad w = 0,\\
\frac{  \gamma^{2} - 2}{ \gamma^{2} +  2V_d }&  \mbox{ for } \qquad w = 1, 3, 5, \ldots, 2 V_d-1,\\
\frac{  \gamma^{2} }{ \gamma^{2} +  2V_d }  & \mbox{ for } \qquad w = 2, 4, 6, \ldots, 2 V_d-2,
\end{cases}
\notag
\end{align}
and therefore
\begin{align}
{\bfK} 
&= 
 \frac{\sigma_0^2}{\gamma^2 + 2V_d} 
\left(  2 V_d \bfI_{2V_d} + (\gamma^2 -1)\bfone \bfone^\T  +  \bfc \bfc^\T\right)
\notag\\
&= 
 \frac{\sigma_0^2}{\gamma^2 + 2V_d} 
\left(  2 V_d \bfI_{2V_d} 
+ 
\begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}
\begin{pmatrix}
\gamma^2 -1 & 0  \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}^\T
\right),
\label{eq:TrainKernelEquidistance}
\end{align}
where 
\[
\bfc = 
\begin{pmatrix}
1 \\
-1\\
1 \\
-1\\
\vdots\\
1 \\
-1
\end{pmatrix}
\in \mathbb{R}^{2V_d}.
\]

With the training kernel expression~\eqref{eq:TrainKernelEquidistance}, the matrix inversion lemma gives
\begin{align}
 \left( \bfK + \sigma^{2}\bfI_{2V_d} \right)^{-1}
  & = 
  \frac{\gamma^2 + 2V_d} {\sigma_0^2} 
   \left(\left(\gamma^2 + 2V_d) (\sigma^{2} /\sigma_0^2 +  2V_d  \right) \bfI_{2V_d} 
   +
  \begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}
\begin{pmatrix}
\gamma^2 -1 & 0  \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}^\T \right)^{-1}
  \notag\\
  & =
   \frac{\gamma^2 + 2V_d} {\sigma_0^2} 
   \frac{1}{(\gamma^2 + 2V_d)\sigma^{2} /\sigma_0^2 + 2V_d}
      \notag\\
   &  \hspace{10mm}
   \left(  \bfI_{2V_d} +  \frac{1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d}
         \begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}
\begin{pmatrix}
\gamma^2 -1 & 0  \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}^\T
 \right)^{-1}
  \notag\\
  & =
   \frac{\gamma^2 + 2V_d} {\sigma_0^2} 
   \frac{1}{(\gamma^2 + 2V_d)\sigma^{2} /\sigma_0^2 + 2V_d}
      \notag\\
   &  \hspace{10mm}
   \Bigg\{  \bfI_{2V_d} 
   -  \frac{1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d}
   \begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}
\begin{pmatrix}
\gamma^2 -1 & 0  \\
0 & 1
\end{pmatrix}
      \notag\\
   &  \hspace{20mm}
\left(\bfI_2 + \begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}^\T
 \frac{1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d}
   \begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}
\begin{pmatrix}
\gamma^2 -1 & 0  \\
0 & 1
\end{pmatrix}
 \right)^{-1}
\begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}^\T
 \Bigg\}
   \notag\\
%   & =
%    \frac{\gamma^2 + 2V_d} {\sigma_0^2} 
%    \frac{1}{(\gamma^2 + 2V_d)\sigma^{2} /\sigma_0^2 + 2V_d}
%       \notag\\
%    &  \hspace{10mm}
%    \Bigg\{  \bfI_{2V_d} 
%    -  \frac{1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d}
%    \begin{pmatrix}
% \bfone & \bfc 
% \end{pmatrix}
% \begin{pmatrix}
% \gamma^2 -1 & 0  \\
% 0 & 1
% \end{pmatrix}
%       \notag\\
%    &  \hspace{20mm}
% \left(\bfI_2 + 
%  \frac{2V_d}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d}
% \bfI_2
% \begin{pmatrix}
% \gamma^2 -1 & 0  \\
% 0 & 1
% \end{pmatrix}
%  \right)^{-1}
% \begin{pmatrix}
% \bfone & \bfc 
% \end{pmatrix}^\T
%  \Bigg\}
%    \notag\\
  & =
   \frac{\gamma^2 + 2V_d} {\sigma_0^2} 
   \frac{1}{(\gamma^2 + 2V_d)\sigma^{2} /\sigma_0^2 + 2V_d}
      \notag\\
   &  \hspace{10mm}
   \Bigg\{  \bfI_{2V_d} 
   -  \frac{1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d}
   \begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}
\begin{pmatrix}
\gamma^2 -1 & 0  \\
0 & 1
\end{pmatrix}
      \notag\\
   &  \hspace{20mm}
\left(\bfI_2 + 
 \frac{1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d}
\begin{pmatrix}
2V_d(\gamma^2 -1) & 0  \\
0 & 2V_d
\end{pmatrix}
 \right)^{-1}
\begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}^\T
 \Bigg\}
  \notag\\
%    & =
%    \frac{\gamma^2 + 2V_d} {\sigma_0^2} 
%    \frac{1}{(\gamma^2 + 2V_d)\sigma^{2} /\sigma_0^2 + 2V_d}
%       \notag\\
%    &  \hspace{10mm}
%    \Bigg\{  \bfI_{2V_d} 
%    -  \frac{1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d}
%    \begin{pmatrix}
% \bfone & \bfc 
% \end{pmatrix}
% \begin{pmatrix}
% \gamma^2 -1 & 0  \\
% 0 & 1
% \end{pmatrix}
%       \notag\\
%    &  \hspace{-40mm}
% \left(
%  \frac{1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d}
% \begin{pmatrix}
% ( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d + 2V_d(\gamma^2 -1) & 0  \\
% 0 & ( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2 V_d + 2V_d
% \end{pmatrix}
%  \right)^{-1}
% \begin{pmatrix}
% \bfone & \bfc 
% \end{pmatrix}^\T
%  \Bigg\}
%   \notag\\
   & =
   \frac{\gamma^2 + 2V_d} {\sigma_0^2} 
   \frac{1}{(\gamma^2 + 2V_d)\sigma^{2} /\sigma_0^2 + 2V_d}
      \notag\\
   &  \hspace{10mm}
   \Bigg\{  \bfI_{2V_d} 
   - 
   \begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}
\begin{pmatrix}
\gamma^2 -1 & 0  \\
0 & 1
\end{pmatrix}
      \notag\\
   &  \hspace{20mm}
\begin{pmatrix}
( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2V_d \gamma^2  & 0  \\
0 & ( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 4 V_d 
\end{pmatrix}^{-1}
\begin{pmatrix}
\bfone & \bfc 
\end{pmatrix}^\T
 \Bigg\}
  \notag\\
%    & =
%    \frac{\gamma^2 + 2V_d} {\sigma_0^2} 
%    \frac{1}{(\gamma^2 + 2V_d)\sigma^{2} /\sigma_0^2 + 2V_d}
%       \notag\\
%    &  \hspace{10mm}
%    \Bigg\{  \bfI_{2V_d} 
%    - 
%    \begin{pmatrix}
% \bfone & \bfc 
% \end{pmatrix}
% \begin{pmatrix}
% \frac{\gamma^2 -1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 2V_d \gamma^2} & 0  \\
% 0 & \frac{1}{( \gamma^2  + 2V_d) \sigma^{2}/\sigma_0^2+ 4 V_d }
% \end{pmatrix}
% \begin{pmatrix}
% \bfone & \bfc 
% \end{pmatrix}^\T
%  \Bigg\}
%   \notag\\
    &=
\frac{1}{\sigma_0^2} a( \bfI_{2V_d} +  b \bfone \bfone^\T +  c \bfc \bfc^\T),
 \notag
\end{align}
where
\begin{align}
a &=  \frac{\gamma^2 + 2V_d}{(\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d },
\notag\\
b &= -         \frac {\gamma^2 -1} {( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 2V_d \gamma^2 } ,
\notag\\
c &= -         \frac {1} {( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 4V_d} .
\notag
\end{align}




For the test kernels
\begin{align}
\widetilde{\bfk}' &= \sigma_0^2
\begin{pmatrix}
\kappa_0\\
\kappa_1\\
\vdots\\
\kappa_{2V_d-1}
\end{pmatrix},
\notag\\
\widetilde{k}''
 &=
\sigma_0^2
\left(\frac{  2 \sum_{v=1}^{V_{d'}} v^2  }{ \gamma^{2} + 2 V_d}\right)
 =
\frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)},
\notag
\end{align} 
with
\begin{align}
\kappa_w
&=
  \frac{  2\sum_{v=1}^{V_d}  v  \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)  \right)}{\gamma^2 +  2V_d },
\notag\\
\end{align}
we have
\begin{align}
\|\widetilde{\bfk}'\|^2
&=\sigma_0^4 \sum_{w = 0}^{2V_d-1} \left(  \frac{  2\sum_{v=1}^{V_d}  v  \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)  \right)}{\gamma^2 +  2V_d }\right)^2
\notag\\
&=\frac{ \sigma_0^4}{(\gamma^2 +  2V_d)^{2} }\sum_{w = 0}^{2V_d-1} \left\{  4\sum_{v=1}^{V_d}  \sum_{v'=1}^{V_d} v v'   \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right) \right) \sin \left( v' \left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right) \right)  \right\}
\notag\\
&\hspace{-0mm}=\frac{ \sigma_0^4}{(\gamma^2 +  2V_d)^{2} }\sum_{w = 0}^{2V_d-1} \left\{  2\sum_{v=1}^{V_d}  \sum_{v'=1}^{V_d} v v' \left(  \cos \left( (v - v')\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right) \right) -  \cos \left((v+ v') \left(\frac{(2w+1)\pi}{ 2V_d} - \alpha' \right) \right)\right)  \right\}
\notag\\
&\hspace{-0mm}=\frac{ \sigma_0^4}{(\gamma^2 +  2V_d)^{2} } \Bigg\{ 2\sum_{v=1}^{V_d}  \sum_{v'=1}^{V_d} v v'\sum_{w = 0}^{2V_d-1} \left(  \cos \left( (v - v')\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right) \right) -  \cos \left((v+ v') \left(\frac{(2w+1)\pi}{ 2V_d} - \alpha' \right) \right)\right)  \Bigg\}
\notag\\
&=\frac{ \sigma_0^4}{(\gamma^2 +  2V_d)^{2} } \Bigg\{ 
\notag\\
& \hspace{10mm}
 2\sum_{v=1}^{V_d}  \sum_{v'=1}^{V_d} v v'\sum_{w = 0}^{2V_d-1} \Bigg(  \cos \frac{(2w+1)(v - v')\pi}{2V_d}  \cos \left( (v - v')\alpha' \right) +  \sin \frac{(2w+1)(v - v')\pi}{ 2V_d}  \sin \left( (v - v')\alpha' \right) 
\notag\\
& \hspace{10mm}
-  \cos \frac{(2w+1)(v + v')\pi}{2V_d}  \cos \left( (v + v')\alpha' \right) -  \sin \frac{(2w+1)(v + v')\pi}{2V_d}  \sin \left( (v + v')\alpha' \right)
\Bigg)  \Bigg\}
\label{eq:KTwoNormDetailOne}\\
&=\frac{ \sigma_0^4}{(\gamma^2 +  2V_d)^{2} } \Bigg\{ 
 2\sum_{v=1}^{V_d}  \sum_{v'=1}^{V_d} v v'\sum_{w = 0}^{2V_d-1} \Bigg(  \cos \frac{(2w+1)(v - v')\pi}{2V_d}  \cos \left( (v - v')\alpha' \right) 
\notag\\
& \hspace{60mm}
-  \cos \frac{(2w+1)(v + v')\pi}{2V_d}  \cos \left( (v + v')\alpha' \right)
\Bigg)  \Bigg\}
\label{eq:KTwoNormDetailTwo}\\
&=\frac{ \sigma_0^4}{(\gamma^2 +  2V_d)^{2} } 
 2 (2V_d) \Bigg( \left(\sum_{v=1}^{V_d} v^2\right)  + V_d^2 \cos \left( 2 V_d \alpha' \right)
\Bigg) 
\label{eq:KTwoNormDetailThree}\\
&=\frac{ \sigma_0^4}{(\gamma^2 +  2V_d)^{2} } 
 2 (2V_d) \Bigg(\frac{V_d(V_d+1)(2V_d + 1)}{6} + V_d^2 \cos \left( 2 V_d \alpha' \right)
\Bigg) 
\notag\\
&=\sigma_0^4 \frac{ 4V_d^2}{(\gamma^2 +  2V_d)^{2} } 
\Bigg(\frac{(V_d+1)(2V_d + 1)}{6} + V_d \cos \left( 2 V_d \alpha' \right)
\Bigg) .
\notag
\end{align}
Here we used Eqs.\eqref{eq:OddFourierRuleCos} and \eqref{eq:OddFourierRuleSin} to obtain Eqs.\eqref{eq:KTwoNormDetailTwo} and \eqref{eq:KTwoNormDetailThree}  from Eq.~\eqref{eq:KTwoNormDetailOne}.

We also have
\begin{align}
\|\widetilde{\bfk}'\|_1 = \widetilde{\bfk}'^\T \bfone_{2V_d}
&=\sigma_0^2 \sum_{w = 0}^{2V_d-1}  \frac{  2\sum_{v=1}^{V_d}  v  \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)  \right)}{\gamma^2 +  2V_d }
\notag\\
&=\sigma_0^2  \frac{  2 \sum_{v=1}^{V_d} v \sum_{w = 0}^{2V_d-1}  \sin \left( v\left(\frac{(2w+1)\pi}{ 2V_d} - \alpha' \right)  \right)}{\gamma^2 +  2V_d }
\notag\\
&=\sigma_0^2  \frac{ 2\sum_{v=1}^{V_d} v \sum_{w = 0}^{2V_d-1}  \left( \sin  \frac{(2w+1)v\pi}{2V_d}  \cos  v \alpha'  - \cos  \frac{(2w+1)v\pi}{2V_d}  \sin  v \alpha'  \right)}{\gamma^2 +  2V_d }
\notag\\
&=0,
\notag
\end{align}
and
\begin{align}
 \widetilde{\bfk}'^\T \bfc
&=\sigma_0^2 \sum_{w = 0}^{2V_d-1} (-1)^w \frac{  2\sum_{v=1}^{V_d}  v  \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)  \right)}{\gamma^2 +  2V_d }
\notag\\
&=\sigma_0^2  \frac{  2 \sum_{v=1}^{V_d} v \sum_{w = 0}^{2V_d-1}   (-1)^w\sin \left( v\left(\frac{(2w+1)\pi}{ 2V_d} - \alpha' \right)  \right)}{\gamma^2 +  2V_d }
\notag\\
&=\sigma_0^2  \frac{ 2\sum_{v=1}^{V_d} v \sum_{w = 0}^{2V_d-1}  (-1)^w  \left( \sin  \frac{(2w+1)v\pi}{2V_d}  \cos  v \alpha'  - \cos  \frac{(2w+1)v\pi}{2V_d}  \sin  v \alpha'  \right)}{\gamma^2 +  2V_d }
\notag\\
%&=\sigma_0^2  \frac{ 2V_d  \sum_{w = 0}^{2V_d-1}  (-1)^w   \sin  \frac{(2w+1)V_d\pi}{2V_d}  \cos V_d\alpha'  }{\gamma^2 +  2V_d }
%\notag\\
&=\sigma_0^2  \frac{ 2V_d  2V_d  \cos V_d\alpha'  }{\gamma^2 +  2V_d }
\notag\\
&=\sigma_0^2  \frac{ 4V_d^2  \cos V_d\alpha'  }{\gamma^2 +  2V_d }.
\notag
\end{align}
Here, we used Eqs.\eqref{eq:OddFourierRuleCvecCos} and \eqref{eq:OddFourierRuleCvecSin} in the second last equation.
Therefore, the mean of the derivative  is
\begin{align}
\widetilde{\mu}^{(d)}_{[ \bfX, \bfy, \bfsigma]}(\bfx')
&= \widetilde{\bfk}'^{\T} \left(  \bfK + \sigma^{2} \bfI_{2V_d} \right)^{-1} \bfy 
\notag\\
&= \widetilde{\bfk}'^{\T} \frac{a}{\sigma_0^2}  \left(  \bfI_{2V_d} + b \bfone_{2V_d} \bfone_{2V_d}^\T  + c \bfc \bfc^\T \right) \bfy 
\notag\\
&= \frac{a}{\sigma_0^2}   \left(  \widetilde{\bfk}'^{\T} \bfy   + b   \widetilde{\bfk}'^{\T} \bfone_{2V_d} \bfone_{2V_d}^\T\bfy    + c \widetilde{\bfk}'^{\T} \bfc \bfc^\T \bfy  \right) 
\notag\\
&=a  \left(  \sum_{w=0}^{2V_d-1} y_w \frac{  2\sum_{v=1}^{V_d}  v  \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)  \right)}{\gamma^2 +  2V_d }   +  c  \frac{ 4V_d^2  \cos V_d\alpha'  }{\gamma^2 +  2V_d }  \sum_{w=0}^{2V_d-1} (-1)^w y_w  \right) .
\notag\\
&=a  \left(  \sum_{w=0}^{2V_d-1} y_w \left( \frac{  2\sum_{v=1}^{V_d}  v  \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)  \right)}{\gamma^2 +  2V_d }   +  c  \frac{ 4V_d^2  (-1)^w   }{\gamma^2 +  2V_d }   \cos V_d\alpha' \right) \right) 
\notag\\
&=\frac{a}{\gamma^2 +  2V_d }  \left( \sum_{w=0}^{2V_d-1} y_w \left(   \left\{2\sum_{v=1}^{V_d}  v  \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)  \right)  \right\} +  4 c   V_d^2  (-1)^w    \cos V_d\alpha' \right) \right) 
\notag\\
&= \frac{ \sum_{w=0}^{2V_d-1} y_w \left(   \left\{2\sum_{v=1}^{V_d}  v  \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)  \right)  \right\} -         \frac {4 V_d^2  (-1)^w    \cos V_d\alpha' } {( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 4V_d}   \right) }
{(\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d }  .
\label{eq:DerivativeMean}
\end{align}

Eq.~\eqref{eq:DirichletKernelAnalyticDerivative} implies that, 
for $w = 0, 1, \ldots, 2 V_d-1$, it holds that
 \begin{align}
 2\sum_{v=1}^{V_d}  v  \sin \left( v\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right) \right) 
 &=
\frac{   \sin( V_d \left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right))   }{2 \sin^2\left(\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)/2 \right)} -  \frac{ V_d \cos((V_d + 1/2)\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right))  }{\sin(\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)/2)}
\notag\\
 &=
\frac{   \sin( \frac{(2w+1)\pi}{2} - V_d \alpha' )   }{2 \sin^2\left(\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)/2 \right)} -  \frac{ V_d \cos( \frac{(2w+1)\pi}{2} + \frac{(2w+1)\pi}{4V_d} -   (V_d + 1/2)  \alpha' )  }{\sin(\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)/2)}
\notag\\
 &=
\frac{   \sin( (-1)^w \frac{ \pi}{2} - V_d \alpha' )   }{2 \sin^2\left(\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)/2 \right)} -  \frac{ V_d \cos((-1)^w \frac{ \pi}{2}+ \frac{(2w+1)\pi}{4V_d} -   (V_d + 1/2)  \alpha' )  }{\sin(\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)/2)}
\notag\\
%  &=
% (-1)^w
% \frac{   \cos(  - V_d \alpha' )   }{2 \sin^2\left(\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)/2 \right)} +  (-1)^w \frac{ V_d \sin ( \frac{(2w+1)\pi}{4V_d} -   (V_d + 1/2)  \alpha' )  }{\sin(\left(\frac{(2w+1)\pi}{2V_d} - \alpha' \right)/2)}
% \notag\\
 &=
(-1)^w
\left(
\frac{   \cos(   V_d \alpha' )   }{2 \sin^2\left(\frac{(2w+1)\pi}{4V_d} - \alpha' /2\right)} +  \frac{ V_d \sin ( \frac{(2w+1)\pi}{4V_d} -   (V_d + 1/2)  \alpha' )  }{\sin(\frac{(2w+1)\pi}{4V_d} - \alpha' /2)}
\right).
 \label{eq:GeneralKernelExpression}
 \end{align}
Substituting Eq.~\eqref{eq:GeneralKernelExpression} into Eq.~\eqref{eq:DerivativeMean} gives Eq.~\eqref{eq:A.DGPPredictionMeanGeneral.MostGeneral}.



The posterior variance can be computed as
\begin{align}
\widetilde{s}^{(d)}_{[ \bfX, \bfsigma]}(\bfx', \bfx')
&=
 \widetilde{k}^{''} -   \widetilde{\bfk}'^{\T}  \left( \bfK + \sigma^{2}\bfI_{2V_d} \right)^{-1}  \widetilde{\bfk}'
 \notag\\
&=
 \widetilde{k}^{''}-   \widetilde{\bfk}'^{\T}  \frac{1}{\sigma_0^2} a  \left(  \bfI_{2V_d} + b \bfone_{2V_d} \bfone_{2V_d}^\T + c \bfc \bfc^\T \right) \widetilde{\bfk}'
 \notag\\
&=
 \widetilde{k}^{''}-  \frac{1}{\sigma_0^2} a  \left(  \|\widetilde{\bfk}'\|^2 + b (\widetilde{\bfk}'^\T  \bfone_{2V_d} )^2 + c (\widetilde{\bfk}'^\T  \bfc )^2 \right) 
 \notag\\
&=
\frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)}
\notag\\
& \hspace{10mm}
-  \frac{1}{\sigma_0^2} a  \left\{  \sigma_0^4 \frac{ 4V_d^2}{(\gamma^2 +  2V_d)^{2} } 
\Bigg(\frac{(V_d+1)(2V_d + 1)}{6} + V_d \cos \left( 2 V_d \alpha' \right)
\Bigg) 
+ c \sigma_0^4 \left( \frac{ 4V_d^2  \cos V_d\alpha'  }{\gamma^2 +  2V_d } \right)^2\right\} 
 \notag\\
% &=
% \frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)}
% \notag\\
% & \hspace{10mm}
% - \sigma_0^2 a  \left\{   \frac{ 4V_d^2}{(\gamma^2 +  2V_d)^{2} } 
% \Bigg(\frac{(V_d+1)(2V_d + 1)}{6} + V_d \cos \left( 2 V_d \alpha' \right)
% \Bigg) 
% + c  \left( \frac{ 4V_d^2  \cos V_d\alpha'  }{\gamma^2 +  2V_d } \right)^2\right\} 
%  \notag\\
 &=
\frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)}
-  \sigma_0^2 a   \frac{ 4V_d^2}{(\gamma^2 +  2V_d)^{2} } \frac{(V_d+1)(2V_d + 1)}{6}
\notag\\
& \hspace{40mm}
-  \sigma_0^2 a  \left\{  \frac{ 4V_d^3 \cos \left( 2 V_d \alpha' \right)
}{(\gamma^2 +  2V_d)^{2} } 
+ c\frac{ 16V_d^4  \cos^2 V_d\alpha'  }{(\gamma^2 +  2V_d)^2 } \right\} 
 \notag\\
%  &=
% \frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)}
% -  \sigma_0^2 a   \frac{ 4V_d^2}{(\gamma^2 +  2V_d)^{2} } \frac{(V_d+1)(2V_d + 1)}{6}
% \notag\\
% & \hspace{40mm}
% -  \sigma_0^2 a  \left\{  \frac{ 4V_d^3 \cos \left( 2 V_d \alpha' \right)
% }{(\gamma^2 +  2V_d)^{2} } 
% + c\frac{ 8V_d^4  (1 + \cos 2 V_d\alpha' ) }{(\gamma^2 +  2V_d)^2 } \right\} 
%  \notag\\
 &=
\frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)}
-  \sigma_0^2 \frac{\gamma^2 + 2V_d}{(\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d }   \frac{ 4V_d^2}{(\gamma^2 +  2V_d)^{2} } \frac{(V_d+1)(2V_d + 1)}{6}
\notag\\
& \hspace{10mm}
-  \sigma_0^2 \frac{\gamma^2 + 2V_d}{(\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d }  \left\{  \frac{ 4V_d^3 \cos \left( 2 V_d \alpha' \right)
}{(\gamma^2 +  2V_d)^{2} } 
-      \frac {1} {( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 4V_d} \frac{ 8V_d^4  (1 + \cos 2 V_d\alpha' ) }{(\gamma^2 +  2V_d)^2 } \right\} 
 \notag\\
%  &=
% \frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)}
% -  \frac{ \sigma_0^2 2V_d^2(V_d+1)(2V_d + 1) }{3 (\gamma^2 +  2V_d) ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d) }  
% \notag\\
% & \hspace{10mm}
% -   \frac{\sigma_0^2}{(\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d }  \left\{  \frac{ 4V_d^3 \cos \left( 2 V_d \alpha' \right)
% }{(\gamma^2 +  2V_d) } 
% -      \frac {1} {( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 4V_d} \frac{ 8V_d^4  (1 + \cos 2 V_d\alpha' ) }{(\gamma^2 +  2V_d) } \right\} 
%  \notag\\
%  &=
% \frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)}
% \left(1 
% -  \frac{ 2V_d }{ (\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d }  
% \right)
% \notag\\
% & \hspace{10mm}
% -   \frac{\sigma_0^2}{(\gamma^2 +  2V_d)((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d) }  \left\{  4V_d^3 \cos \left( 2 V_d \alpha' \right)
% -      \frac {8V_d^4  (1 + \cos 2 V_d\alpha' ) } {( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 4V_d} \right\} 
%  \notag\\
%  &=
% \frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)}
% \left(1 
% -  \frac{ 2V_d }{ (\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d }  
% \right)
% \notag\\
% & \hspace{10mm}
% -   \frac{\sigma_0^2 (4V_d^3 \cos \left( 2 V_d \alpha' \right)   (( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 4V_d) - 8V_d^4  (1 + \cos 2 V_d\alpha' )  ) }{(\gamma^2 +  2V_d)((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d) ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  4 V_d)} 
%  \notag\\
%  &=
% \frac{     \sigma_0^2 V_d (V_d+1)(2V_d + 1)}{3( \gamma^{2} + 2 V_d)}
% \left( \frac{ (\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 }{ (\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d }  
% \right)
% \notag\\
% & \hspace{10mm}
% -   \frac{\sigma_0^2 (4V_d^3 \cos \left( 2 V_d \alpha' \right)   (( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}  + 2V_d) - 8V_d^4   ) }{(\gamma^2 +  2V_d)((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d)((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  4 V_d) }   
%  \notag\\
%  &= \sigma^2 
% \frac{    V_d (V_d+1)(2V_d + 1)}{3 ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d )}
% \notag\\
% & \hspace{10mm}
% -   \frac{\sigma_0^2 \left\{4V_d^3 \cos \left( 2 V_d \alpha' \right)   ( \gamma^2 + 2V_d)\sigma^{2}/\sigma_0^{2}   + 8V_d^4 (\cos \left( 2 V_d \alpha' \right)  -1)\right\}    }{(\gamma^2 +  2V_d)((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d)((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  4 V_d) }   
%  \notag\\
 &= \sigma^2 
\frac{    V_d (V_d+1)(2V_d + 1)}{3 ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d )}
-  \sigma^2 \frac{ 4V_d^3 \cos \left( 2 V_d \alpha' \right)        }{((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d) ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  4 V_d) }   
\notag\\
& \hspace{10mm}
-  \sigma_0^2  \frac{ 8V_d^4 (\cos \left( 2 V_d \alpha' \right)  -1)    }{(\gamma^2 +  2V_d)((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  2 V_d) ((\gamma^2 + 2V_d) \sigma^{2}/\sigma_0^2 +  4 V_d) }  ,
\label{eq:DerivativeUncertainty}
\end{align}
which gives Eq.~\eqref{eq:A.DGPPredictionVarGeneral.MostGeneral}. 
\QED



\subsection{Proof of Theorem~\ref{thrm:GPasPSR}}
\label{sec:A.Proof.GPasPSR}

In the first order case with $V_d=1, \forall d = 1, \ldots, D$, the test VQE kernels for predicting derivatives $\partial_{d} f(\bfx'), \partial_{d} f(\bfx'')$ are
\begin{align}
\widetilde{k}(\bfx, \bfx')
&=
\frac{\partial}{\partial x'_{d}} k (\bfx, \bfx')
= 
\sigma_0^2
\left(\frac{ 2 \sin \left(x_{d} - x'_{d}  \right)}{ \gamma^2 + 2 }\right)
\prod_{d'\ne d} \left(\frac{ \gamma^2 + 2 \cos \left(x_{d'} - x_{d'}'  \right)}{ \gamma^2 + 2}\right),
\notag \\
\widetilde{k}(\bfx', \bfx'')
&=
\frac{\partial^2}{\partial x_{d}'\partial x''_{d}} k(\bfx', \bfx'')
= 
\sigma_0^2
\left(\frac{ 2 \cos \left(x_{d}' - x''_{d}  \right)}{ \gamma^2 + 2 }\right)
\prod_{d'\ne d} \left(\frac{ \gamma^2 + 2 \cos \left(x_{d'}' - x_{d'}''  \right)}{\gamma^2 + 2}\right).
\notag
\end{align}
Then, the kernels with the two training points $\bfX = (\bfx' - \alpha \bfe_d, \bfx' + \alpha \bfe_d)$ and the one test point $\bfx'$ are
\begin{align}
\bfK &=
\sigma_0^2
\begin{pmatrix}
1 & \frac{\gamma^2 + 2\cos 2\alpha}{\gamma^2 + 2} \\
 \frac{\gamma^2 + 2\cos 2\alpha}{\gamma^2 + 2} &1 
\end{pmatrix},
&
 \widetilde{\bfk}' &=
 \frac{2  \sigma_0^2 \sin \alpha}{\gamma^2 + 2 }
\begin{pmatrix}
- 1 \\
1
\end{pmatrix},
&
 \widetilde{k}'' &= \frac{ 2 \sigma_0^2}{\gamma^2 + 2 }.
\notag
\end{align}
With these kernels, the posterior mean  is 
\begin{align}
\widetilde{\mu}^{(d)}_{[ \bfX, \bfy, \bfsigma]}(\bfx')
   &= \widetilde{\bfk}'^{\T} \left(\bfK + \sigma^2 \bfI_N \right)^{-1} \bfy
   \notag\\
& = 
\frac{ 2 \sin \alpha}{\gamma^2 + 2 }
\begin{pmatrix}
- 1&
1
\end{pmatrix}
\begin{pmatrix}
1 + \sigma^2/\sigma_0^2 & \frac{\gamma^2 +2 \cos 2 \alpha}{\gamma^2 +2} \\
 \frac{\gamma^2 + 2\cos 2 \alpha}{\gamma^2 + 2} &1 + \sigma^2/\sigma_0^2
\end{pmatrix}^{-1}
\bfy
\notag\\
& = 
\frac{ 2\sin \alpha}{ \gamma^2 + 2} 
\begin{pmatrix}
- 1 &
1
\end{pmatrix}
\frac{1}{(1 + \sigma^2/\sigma_0^2)^2 -  \left( \frac{\gamma^2 + 2\cos 2\alpha}{\gamma^2 + 2}\right)^2}
\begin{pmatrix}
1 + \sigma^2/\sigma_0^2 & - \frac{\gamma^2 + 2\cos 2\alpha}{\gamma^2 + 2} \\
 -\frac{\gamma^2 + 2\cos 2\alpha}{\gamma^2 + 2} &1 + \sigma^2/\sigma_0^2
\end{pmatrix}
\bfy
\notag\\
% & = 
% \frac{2 \sin \alpha}{\gamma^2 + 2} 
% \frac{1}{\left( (1 + \sigma^2/\sigma_0^2) -  \left( \frac{\gamma^2 + 2\cos 2\alpha}{\gamma^2 + 2}\right) \right)\left( (1 + \sigma^2/\sigma_0^2) +  \left( \frac{\gamma^2 + 2\cos 2\alpha}{\gamma^2 + 2}\right) \right)}
% \notag\\
% &\hspace{10mm}
% \begin{pmatrix}
% - (1 + \sigma^2/\sigma_0^2)- \frac{\gamma^2 +2 \cos 2\alpha}{\gamma^2 + 2}&
%  1 + \sigma^2/\sigma_0^2 +\frac{\gamma^2 + 2\cos 2\alpha}{\gamma^2 + 2}
% \end{pmatrix}
% \bfy
% \notag\\
& = 
\frac{2 \sin \alpha}{\gamma^2 + 2} 
\frac{1}{(1 + \sigma^2/\sigma_0^2) -  \left( \frac{\gamma^2 + 2\cos 2\alpha}{\gamma^2 + 2}\right) }
\begin{pmatrix}
- 1&
 1
\end{pmatrix}
\bfy
\notag\\
& = 2 \sin \alpha \frac{ y_2 - y_1}
{(1 + \sigma^2/\sigma_0^2)(\gamma^2 + 2) -  \left(\gamma^2 + 2\cos 2\alpha\right) }
\notag\\
% & = 2\sin \alpha \frac{ y_2 - y_1}
% {( \gamma^2 +2) \sigma^2/\sigma_0^2 + 2 -  2\cos 2\alpha }
% \notag\\
% & = 2\sin \alpha \frac{ y_2 - y_1}
% {(\gamma^2 + 2) \sigma^2/\sigma_0^2 + 2 -  2(\cos^2 \alpha - \sin^2 \alpha) }
% \notag\\
% & = 2\sin \alpha \frac{ y_2 - y_1}
% {( \gamma^2 + 2) \sigma^2/\sigma_0^2 + 4 \sin^2 \alpha }  
% \notag\\
& =  \frac{  (y_2 - y_1) \sin \alpha}
{( \gamma^2/2 + 1) \sigma^2/\sigma_0^2 + 2 \sin^2 \alpha }.  
\notag
\end{align}
The posterior variance is 
\begin{align}
\widetilde{s}^{(d)}_{[ \bfX, \bfsigma]}(\bfx', \bfx')
   &= \widetilde{k}'' - 
   \widetilde{\bfk'}^{\T} \left(\bfK + \sigma^2 \bfI_N \right)^{-1} {\widetilde{\bfk}'}
   \notag\\
&= \frac{ 2 \sigma_0^2}{\gamma^2 + 2 }
- 
\frac{4 \sigma_0^2 \sin^2 \alpha}{( \gamma^2 + 2)^2} 
\begin{pmatrix}
- 1 &
1
\end{pmatrix}
\begin{pmatrix}
1 + \sigma^2/\sigma_0^2 & \frac{\gamma^2 +2 \cos 2 \alpha}{\gamma^2 +2} \\
 \frac{\gamma^2 + 2\cos 2 \alpha}{\gamma^2 + 2} &1 + \sigma^2/\sigma_0^2
\end{pmatrix}^{-1}
\begin{pmatrix}
 -1\\
1
\end{pmatrix}
 \notag\\
&= \frac{ 2 \sigma_0^2}{\gamma^2+2 }
- 
\frac{4 \sigma_0^2 \sin^2 \alpha}{( \gamma^2 + 2)^2} 
\frac{1}{(1 + \sigma^2/\sigma_0^2) -  \left( \frac{\gamma^2 + 2\cos 2 \alpha}{\gamma^2 + 2}\right)}
\begin{pmatrix}
- 1 &
1
\end{pmatrix}
\begin{pmatrix}
-1 \\
1   
\end{pmatrix}
\notag\\
% &= \frac{  2\sigma_0^2}{ \gamma^2 +2}
% \left( 1
% - 
% \frac{ 2\sin^2 \alpha}{( \gamma^2 + 2)} 
% \frac{2}{(1 + \sigma^2/\sigma_0^2) -  \left( \frac{\gamma^2 +2 \cos 2 \alpha}{\gamma^2 + 2}\right)}
% \right)
% \notag\\
&= \frac{ 2 \sigma_0^2}{ \gamma^2 +2}
\left( 1
- 
\frac{ 4 \sin^2 \alpha}{( \gamma^2+2 ) \sigma^2/\sigma_0^2 + 2 - 2 \cos 2 \alpha} 
\right)
\notag\\
% &= \frac{ 2 \sigma_0^2}{\gamma^2 + 2 }
% \left( 1
% - 
% \frac{ 4 \sin^2 \alpha}{(\gamma^2 + 2 ) \sigma^2/\sigma_0^2 + 4 \sin^2 \alpha}
% \right)
% \notag\\
&= \frac{ 2 \sigma_0^2}{\gamma^2 + 2 }
\left( 
\frac{ ( \gamma^2+ 2 ) \sigma^2/\sigma_0^2}{(\gamma^2 
 + 2) \sigma^2/\sigma_0^2 + 4 \sin^2 \alpha}
\right)
\notag\\
% &=
% \frac{2 \sigma^2}{( \gamma^2 + 2 ) \sigma^2/\sigma_0^2 + 4 \sin^2 \alpha}
% \notag\\
&=
\frac{\sigma^2}{( \gamma^2/2 + 1 ) \sigma^2/\sigma_0^2 + 2 \sin^2 \alpha}.
\notag
\end{align}
Thus we obtained Eqs.\eqref{eq:DGPPredictionMean} and \eqref{eq:DGPPredictionVar}. 
\QED

\newpage 

\section{Algorithm Details}
\label{sec:A.AlgorithmDetails}

\subsection{GradCoRe Pseudo-Code}


\Cref{alg:GradcoreAlgFull} describes SGD-GradCoRe in detail.
SGD-GradCoRe uses the GradCoRe measurement selection subroutine described in
\Cref{alg:GradcoreSub}, which selects measurement points and respective minimum required number of shots to estimate the quantum circuit parameter derivative required for the SGD.
\begin{center}
\begin{minipage}[t]{0.95\textwidth}
\vspace{0pt}
\begin{algorithm}[H]
  \footnotesize
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \SetKwInOut{Params}{Parameters}

  \Input{
    \begin{itemize}[nosep]
      %\item $\mathcal{D}= \{\bfx_0, \bfy_0\}:$ initial random observation(s), i.e., initial training point(s).
      \item $\hat{\bfx}^0$: initial starting point (best point)
      %\item $\widetilde\bfnu^0$: initial number of shots for starting point
    \end{itemize}
  }
  \Params{
    \begin{itemize}[nosep]
      \item $V_d=1$
      \item $D:$ number of parameters to optimize, i.e., $\hat{\bfx}\in\mathbb{R}^D$.
      \item $N_\text{tot-shots}:$ Total \# of shots, i.e., maximum allowed quantum computing budget.
      \item $C^{*2}:$ measurement variance using a single shot.
      \item $\kappa_0:$ Initial GradCoRe threshold at step $t=0$.
      \item $T_\text{initial}:$ Number of steps in beginning to use initial GradCoRe threshold $\kappa_0$.
      \item $c_0:$ smallest allowed GradCoRe threshold
      \item $c_1:$ GradCoRe threshold scaling parameter
    \end{itemize}
  }
  \Output{
    \begin{itemize}[nosep]
      \item $\hat\bfx^*:$ optimal choice of parameters for the quantum circuit.
    \end{itemize}
  }
  \BlankLine
  %\mintinline{python}{n,t,d=0,0,0}\\
  $n \gets 0$ \tcc{initialize consumed shot budget}
  $t \gets 0$ \tcc{initialize optimization step}
  \BlankLine
  $\bfkappa^0 \gets \bfone_D \kappa_0$ %
  \tcc{initial $\kappa_0$ to use for $T_\textrm{initial}$ steps}
  % $y_0 \gets $ \texttt{quantum\_circuit(parameters=}%
  %   $\hat{\bfx}^0$%
  %   \texttt{, shots=}%
  %   $\bar{N}_0$%
  %   \texttt{)}%
  % \tcc{measure initial best point}
  % $\bfX^{0},\, \bfy^{0},\, \bfsigma^{0} \gets
  %   (\hat{\bfx}_0),\,
  %   (y_0),\,
  %   (\frac{\eta^2}{N_0})
  % $ %
  $\bfX^{0},\, \bfy^{0},\, \bfsigma^{0} \gets
    (),\,
    (),\,
    ()
  $ %
  \tcc{initialize empty Gaussian process}
  \BlankLine
  \While{$n < N_\text{tot-shots}$}{

    \tcc{choose measurement points \& number of shots s.t.\ $\hat\bfx^t$ is in the GradCoRe of $\bfkappa^t$}
    $\breve{\mathbf{X}},\,\widetilde{\bfnu} \gets
      \texttt{gradcore\_measurements(}
      \bfX^{t},\, \bfy^{t},\, \bfsigma^{t},\, \hat\bfx^t,\, \bfkappa^{t} \texttt{)}
    $ %
    \tcc{(\Cref{alg:GradcoreSub})}

    \BlankLine
    \For{$i \in \{1,\, ..., |\breve{\mathbf{X}}|\}$}{
      %$\breve{y}_i,\,\breve\sigma_i \gets $ %
      $\breve{y}_i \gets $ %
      \texttt{quantum\_circuit(parameters=}%
      $\breve{\mathbf{X}}_i$%
      \texttt{, shots=}%
      $\widetilde\bfnu_i$%
      \texttt{)}%
      \tcc{measure chosen points}

      $\breve{\sigma}_i \gets \frac{\overline{\sigma}^{*2}}{\widetilde\bfnu_i}$
    }
    $\breve{\bfy}\,, \breve{\bfsigma} \gets %
      ( \breve{y}_1, ..., \breve{y}_{|\breve{\mathbf{X}}|})\,,
      ( \breve{\sigma}_1, ..., \breve{\sigma}_{|\breve{\mathbf{X}}|})
    $ %
    \tcc{concatenate observed values \& noise}

    \BlankLine
    $\bfX^{t+1},\, \bfy^{t+1},\, \bfsigma^{t+1} \gets
      (\bfX^{t}\,, \breve{\bfX}),\,
      (\bfy^{t}\,, \breve{\bfy}),\,
      (\bfsigma^{t}\,, \breve{\bfsigma})
    $ %
    \tcc{add new observations to Gaussian process}

    \BlankLine
    
    $\hat\bfx^{t+1} \gets \hat\bfx^t - \rho \,\widetilde\mu_{\left[\bfX^{t+1}, \bfsigma^{t+1}, \bfy^{t+1}\right]}(\widehat{\bfx}^{t})$ %
    \tcc{SGD (or variant) step using GP derivative}

    \BlankLine

    % \If{$t>T_{\mathrm{Ave}}$}{
    %   $ \kappa^{t+1} \gets \textstyle \max (C_0 ,\, -C_1\cdot \mathrm{Slope} (\{\widehat{y}^{t'}\}_{t' = (t+1) - T_{\mathrm{Ave}}}^{t+1})).$
    %   \tcc{update the CoRe threshold}
    %   \tcc{N.B. $\mathrm{Slope}(\cdot)$ estimates the progress of optimization by linear regression to the $T_{\mathrm{Ave}}$ recent best values.}
    % }
    \If{$t \geq T_\text{intial}$}{
      $\bfkappa^{t+1} \gets \bfone_{D} \textstyle \max\left[
        c_0,
        \frac{c_1}{D}\sum_{d=1}^D\left(\widetilde{\mu}^{(d)}_{[ \bfX^{t+1}, \bfy^{t+1}, \bfsigma^{t+1}]}(\widehat{\bfx^{t}})\right)^2
      \right]$ %
      \tcc{adapt GradCoRe threshold}
    }

    $t \gets t + 1$ \tcc{update the step}
    %$n \gets \bar{N}_\pm+\bar{N}_0+\bar{N}_\pm$ ; \tcc{update the consumed shot budget}
    $n \gets n + \sum_d \widetilde{\bfnu}_d$ \tcc{update the consumed shot budget}
  }
  \KwRet{
    $\hat{\bfx}^{*}$
  }
  \caption{%
    \textbf{(SGD-GradCoRe)} Improved SGD algorithm using a VQE-derivative kernel GP with the GradCoRe measurement selection
    subroutine, as described in \cref{alg:GradcoreSub}. The algorithm finds the minimum number of shots required to estimate the gradient wrt. parameter configurations
    $\hat\bfx$ of the quantum circuit to optimize with SGD. 
    The optimization stops when the total number of
    measurement shots reaches the maximum number of observation shots allowed, i.e.,
    $N_{\mathrm{tot-shots}}$.
        To avoid cluttering notation, the algorithm is restricted to the case where $V_d=1$.  Generalization to an arbitrary $V_d$ is straightforward.
  }%
  \label{alg:GradcoreAlgFull}
\end{algorithm}
\end{minipage}
\end{center}



\begin{center}
\begin{minipage}[t]{0.95\textwidth}
\vspace{0pt}
\begin{algorithm}[H]
  \footnotesize

  \setcounter{AlgoLine}{0}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \SetKwInOut{Params}{Parameters}

  \Input{
    \begin{itemize}[nosep]
      \item $\bfX, \bfy, \bfsigma:$ Gaussian process at current step
      \item $\hat\bfx:$ current best point
      \item $\bfkappa = (\kappa_1^2,\dots,\kappa_D^2):$ GradCoRe thresholds at current step
    \end{itemize}
  }
  \Params{
    \begin{itemize}[nosep]
      \item $V_d=1$
      \item $\overline{\sigma}^{*2}:$ measurement variance using a single shot.
      \item $\hat\alpha:$ shift from best point at the previous step (default to $\hat\alpha=\frac{\pi}{2}$)
    \end{itemize}
  }
  \Output{
    \begin{itemize}[nosep]
      \item $\breve\bfX:$ points which should be measured and added to the GP to compute the derivative.
      \item $\widetilde\bfnu:$ number of shots for the measured points.
    \end{itemize}
  }

  \BlankLine
  \Begin{
    \BlankLine

    \For{$d \in \{1,\, ..., D\}$}{

      $\breve{\mathbf{X}}_d \gets \left(
        \hat{\bfx} - \hat\alpha\cdot\bfe_d,\, \hat{\bfx} + \hat\alpha\cdot\bfe_d
      \right)$ %
      \tcc{choose points to measure along d}
    
      \BlankLine
    
      $\breve\sigma_\pm \gets \kappa_d$
      \tcc{initialize measurement noise to minimum (most expensive, $\kappa_d\ll\overline{\sigma}^{*}$)}
    
      \BlankLine
    
      \For{$\tilde{\sigma}\in [\sqrt{2} \kappa_d, \overline{\sigma}^{*}]$}{
        \tcc{create temporary GP copies, add points with $\tilde\sigma$ observation noise}
        $\bfX',\, \bfy',\, \bfsigma' \gets
          (\bfX\,, \breve{\bfX}_d),\,
          (\bfy\,, 0, 0),\,
          (\bfsigma\,, \tilde{\sigma}, \tilde{\sigma})
        $ \\
    
        \tcc{find largest observation noise for which $\hat\bfx$ is in the GradCoRe}
        \If{
          $(\widetilde{s}_{\left[\bfX', \bfsigma'\right]} (\hat\bfx, \hat\bfx) \leq \kappa_d^2)$
          $\land$
          $(\breve\sigma_\pm > \tilde\sigma)$
        }{
          $\breve\sigma_\pm \gets \tilde\sigma$
        }
      }
    
      $\widetilde\bfnu_d \gets \left(\frac{\overline{\sigma}^{*2}}{\breve\sigma_\pm},\frac{\overline{\sigma}^{*2}}{\breve\sigma_\pm} \right)$
      \tcc{compute shots from variance through single shot variance $\overline{\sigma}^{*2}$}
    }
    
    $\breve{\mathbf{X}} \gets \left(
        \breve{\mathbf{X}}_1,\dots,\breve{\mathbf{X}}_D
    \right)$ %
    \tcc{concatenate points to measure}
    
    $\widetilde{\bfnu} \gets \left(
        \widetilde{\bfnu}_1,\dots,\widetilde{\bfnu}_D
    \right)$ %
    \tcc{concatenate shots to measure per point}

    \KwRet{
      $\breve{\mathbf{X}},\,\widetilde{\bfnu}_d^{t+1}$
    }
  }
  \caption{%
    \textbf{(GradCoRe measurement selection subroutine)}
    Select the points to measure and respective minimum number of required shots such that when updating the GP with these new measurements, the GP's derivative uncertainty at the current best point is smaller than the threshold $\bfkappa$, i.e., the current point is within the GradCoRe.
    %To avoid cluttering notation, the algorithm is restricted to the case where $V_d=1$.Generalization to an arbitrary $V_d$ is straightforward.
  }%
  \label{alg:GradcoreSub}
\end{algorithm}
\end{minipage}
\end{center}

\subsection{Parameter Setting}
\label{sec:A.ParameterSetting}
Every algorithm used in our benchmarking analysis has several hyperparameters to be set. For transparency and to allow the reproduction of our experiments, we detail the choice of parameters for EMICoRe, SubsCoRe and GradCoRe in~\cref{tab:algparams}. The SGLBO results were obtained using the original code from~\citet{SGLBO2022} and we used the default setting from the original paper. For NFT, Bayes-NFT and Bayes-SGD runs, we used the default parameters specified in~\cref{tab:defaultparams}. For algorithmic efficiency, all Bayesian-SMO methods use the inducer option introduced in~\citet{NEURIPS:Nicoli+:2023}, retaining only the last $R \cdot 2V_{d} \cdot D - 1 = 399$ measured points once more than $R \cdot 2V_{d} \cdot D - 1 + D = 439$ points were stored in the GP, where we chose $R=5$. 
Since the discarded points are replaced with a single point predicted from them, the number of the traininig points for the GP is kept constant at $R \cdot 2V_{d} \cdot D = 400$. On the other hand, Bayesian-SGD methods measure (at most, in the SGD-GradCoRe case) $2 V_d D = 80$ points per SGD step, and  we retain $R \cdot 2V_{d} \cdot D = 400$ points after more than $(R+1) \cdot 2V_{d} \cdot D  = 480$ points are measured.  Unlike the Bayesian-SMO methods, we do not add additional inducer based on the prediction from the discarded points, and therefore the number of the training points for the GP is kept constant at $R \cdot 2V_{d} \cdot D = 400$.


\begin{table}
\centering
  \caption{Algorithm specific parameter choice for EMICoRe, SubsCoRe and GradCoRe for all experiments (unless specified otherwise).}
  \vspace{0.3cm}
  \begin{tabular}{|l|c|c|}
    \toprule
    {} & \textbf{Algorithmic specific parameters} &  \\
    \midrule
    \midrule
    {\verb|--acq-params|} & \textbf{EMICoRe params} & as in~\citet{NEURIPS:Nicoli+:2023} \\
    \midrule
    \verb|func| & \verb|ei| & Base acq. func. type \\
    \verb|optim| & \verb|emicore| & Optimizer type \\
    \verb|pairsize| ($J_{\mathrm{SG}}$) & \verb|20| & \# of candidate points \\
    \verb|gridsize| ($J_{\mathrm{OG}}$) & \verb|100| & \# of evaluation points \\
    \verb|corethresh-strategy| & \verb|grad| & Gradient strategy for $\kappa$ \\
    \verb|pnorm| & \verb|2| & Order of gradient norm \\
    \verb|corethresh| ($\kappa$) & \verb|256| & CoRe threshold $\kappa$ \\
    \verb|corethresh_width| ($T_{\mathrm{initial}}$) & \verb|40| & \# initial steps with fixed $\kappa$\\
    \verb|coremin_scale| ($C_0$) & \verb|2048| & Coefficient $C_0$ for updating $\kappa$\\    
    \verb|corethresh_scale| ($C_1$) & \verb|1.0| & Coefficient $C_1$ for updating $\kappa$\\
    \verb|stabilize_interval| & \verb|41| & Stabilization interval in SMO steps\\
    \verb|samplesize| ($N_{\mathrm{MC}}$) & \verb|100| & \# of MC samples \\
    \verb|smo-steps| ($T_{\mathrm{NFT}}$) & \verb|0| & \# of initial NFT steps \\
    \verb|smo-axis| & \verb|True| & Sequential direction choice\\
    \midrule
    \midrule
    {\verb|--acq-params|} & \textbf{SubsCoRe params} & as in~\citet{ICML:Anders+:2024} \\
    \midrule
    \verb|optim| & \verb|subscore|\tablefootnote{a.k.a., ``\textit{readout}'' in~\citet{NEURIPS:Nicoli+:2023}.}  & Optimizer type \\
    \verb|readout-strategy| & \verb|center| & Alg type SubsCoRe \\
    \verb|corethresh-strategy| & \verb|grad| & Gradient strategy for $\kappa$ \\
    \verb|pnorm| & \verb|2| & Order of gradient norm \\
    \verb|corethresh| ($\kappa$) & \verb|256| & Initial $N_{\textrm{\tiny shots}}$ for CoRe \\
    \verb|corethresh_width| ($T_{\mathrm{initial}}$) & \verb|40| & \# initial steps with fixed $\kappa$\\
    \verb|coremin_scale| ($C_0$) & \verb|2048| & Coefficient $C_0$ for updating $\kappa$\\    \verb|corethresh_scale| ($C_1$) & \verb|1.0| & Coefficient $C_1$ for updating $\kappa$\\
    \verb|stabilize_interval| & \verb|41| & Stabilization interval in SMO steps\\
    \verb|coremetric| & \verb|readout| & Metric to set CoRe \\
    \midrule
    \midrule
    {\verb|--acq-params|} & \textbf{GradCoRe params} & this paper\tablefootnote{All hyperparameters not specified in the table are %implicitly 
    set to the default in~\citet{NEURIPS:Nicoli+:2023}.} \\
    \midrule
    \verb|optim| & \verb|gradcore| & Optimizer type \\
    \verb|corethresh-strategy| & \verb|grad| & Gradient strategy for $\kappa$ \\
    \verb|pnorm| & \verb|2| & Order of gradient norm \\
    \verb|corethresh| ($\kappa$) & \verb|256| & Initial $N_{\textrm{\tiny shots}}$ for CoRe \\
    \verb|corethresh_width| ($T_{\mathrm{initial}}$) & \verb|40| & \# initial steps with fixed $\kappa$\\
    \verb|coremin_scale| ($C_0$) & \verb|2048| & Coefficient $C_0$ for updating $\kappa$\\    
    \verb|corethresh_scale| ($C_1$) & \verb|1.2| & Coefficient $C_1$ for updating $\kappa$\\
    \verb|coremetric| & \verb|readout| & Metric to set CoRe\\
    \verb|lr| & \verb|0.05| & learning rate for SGD\\
    \verb|gdoptim| & \verb|adam| & Optimizer for SGD\\
    \bottomrule
  \end{tabular}\label{tab:algparams}
\end{table}
\section{Experimental Details}
\label{sec:A.ExperimentalDetails}
As discussed in the main text, our experiments focus on the same experimental setup as in~\citet{NEURIPS:Nicoli+:2023} and ~\citet{ICML:Anders+:2024}. Specifically, starting from the quantum Heisenberg Hamiltonian, we reduce it to the special case of the Ising Hamiltonian at the critical point by choosing the suitable couplings, namely
$$
%\textrm{Heisenberg at Criticality: } &J=(-1.0,\,-1.0,\,-1.0);\,h=(-1.0,\,-1.0,\,-1.0)\\
     \textrm{Ising Hamiltonian at criticality: } J=(-1.0,\,0.0,\,0.0);\,h=(0.0,\,0.0,\,-1.0).
$$

It is important to note that due to the finite size of the system at hand, this choice of parameters does not imply criticality but already represents a challenging setup, as discussed in Sec. I.2 in~\citet{NEURIPS:Nicoli+:2023}.
%For those two type of benchmarks, we experimented with different number of readouts (e.g., NFT, SMO, and EMICoRe expect a fixed number of readouts throughout the optimization) as well as different combinations of qubits and circuit layers being $(Q,\,L) =\{(3,3),\,(5,3)\}$. 
% For a fair comparison between the baselines and our methods, 
We stop the optimization when a maximum number of cumulative shots (total measurement budget on the quantum computer) is reached; unless specified otherwise, we set this cutoff to $N^{\textrm{max}}_{\textrm{shots}}=1\cdot10^7$.

Our implementation of GradCoRe can be found in the supplementary zip file and will be made available on Github upon acceptance.
In our experiments, the kernel parameters $\sigma_0$ and $\gamma$ are fixed to the values in ~\cref{tab:kernelparams}. Furthermore, NFT, Bayes-NFT, Bayes-SGD, SubsCoRe and GradCoRe require fixed shifts for the points to measure at each iteration. In our experiments, we always used $\alpha=\frac{2\pi}{3}$ for SMO based methods (as this makes the uncertainty uniform in the 1D-subspace, as discussed in~\citet{ICML:Anders+:2024}), and $\alpha=\frac{\pi}{2}$ for SGD based methods (as this minimizes the uncertainty in the noisy case, as discussed in %\cref{thrm:GPasPSR})
\Cref{sec:ProposedMethod}), unless explicitly stated otherwise.

Each experiment shown in the paper was repeated 50 times (trials) with differently seeded starting points. We aggregated the statistics from these independent trials and presented them in our plots. 
%Again, 
We used the same starting point for every algorithm in each trial to ensure a fair comparison between all approaches.
Note that SGD-based methods do not require measurements at the starting point, but SMO-based methods do.
Therefore, each starting point is further paired with a fixed initial measurement.

All experiments were conducted on Intel Xeon Silver 4316 @ 2.30GHz CPUs.

\begin{table}[t]
  \centering
  \caption{
  Default choice of circuit parameters and kernel hyperparameters for all experiments (unless specified otherwise).}
  \vspace{0.3cm}
  \begin{tabular}{|l|c|c|}
    \toprule
    {} & \textbf{Deafult params} &  \\
    \midrule
    \verb|--n-qbits| & \verb|5| &  \# of qubits \\
    \verb|--n-layers| & \verb|3| & \# of circuit layers \\
    \verb|--circuit| & \verb|esu2| & Circuit name \\
    \verb|--pbc| & \verb|False| & Open Boundary Conditions \\
    \verb|--n-iter| & \verb|1*10**7| & \# max number of readouts \\
    \verb|--kernel| & \verb|vqe| & Name of the kernel \\
    %\verb|--kernel-params| $(\sigma_{0},\gamma)$ & \verb|(10,3)| & Kernel parameters \\
    \bottomrule
  \end{tabular}\label{tab:defaultparams}
  
  \vspace{0.3cm}
  \begin{tabular}{|l|c|c|c|c|c|c|c}
   \toprule
    \verb|--kernel-params| & \textbf{Bayes-NFT} & \textbf{EmiCoRe} & \textbf{SubsCoRe} & \textbf{GradCore} & \textbf{Bayes-SGD} \\
    \midrule
    \verb|gamma| & \verb|3| & \verb|3| & \verb|3| & \verb|3| & \verb|1| \\
    \verb|sigma_0|  & \verb|10| & \verb|10| & \verb|10| & \verb|10| & \verb|10| \\
    \bottomrule
  \end{tabular}\label{tab:kernelparams}
\end{table}

% \section{Additional Experimental Results}
% \label{sec:A.AdditionalExperimentalResults}



% \subsection{Improvement of Gradient Estimation Accuracy by Bayesian PSR}

% \Cref{fig:ImprovedGradientEstimationAccuracy} comprares the gradient estimation errors by PSR and Bayesian PSR with \memo{***} retained points. We observe ...



\newpage
\section{Detailed behavior of GradCoRe}
\label{sec:A.DetailedBehaviorGradcore}

\Cref{fig:GradCoReNumShotsKappa} shows the behavior of the GradCoRe threshold $\kappa(t)$ (left),
and the number $\nu(t)$ of measurement shots (left) that GradCoRe used in each SGD iteration.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/fig7-corethresh.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig7-readout.pdf}    

    \centering\vskip -2ex
    \vskip -1ex
    \caption{
    The GradCoRe threshold $\kappa(t)$ (left), set according to Eq.~\eqref{eq:gradcore_kappa}, and 
    the number of measurement shots (right) per SGD iteration used by GradCoRe. As expected, the number of shots gradually increases as the GradCoRe threshold decreases, reflecting the flatness of the objective function via the gradient norm estimation.
    }
\label{fig:GradCoReNumShotsKappa}
    %\vspace{-5mm}
\end{figure*}




% \subsection{Results on Heisenberg Hamiltonian}




% \begin{figure*}[t]
%     \centering    
%     \includegraphics[height=6ex, width=0.99\textwidth]{figures/fig2-heis-legend.pdf}\\%\vskip -1ex
%     \includegraphics[width=0.49\textwidth]{figures/fig2-heis-energy.pdf}
%     \includegraphics[width=0.49\textwidth]{figures/fig2-heis-fidelity.pdf}
%     \centering\vskip -2ex
%     \vskip 1ex
%     \includegraphics[height=4ex, width=0.6\textwidth]{figures/fig3-heis-legend.pdf}\\%\vskip -1ex
%     \includegraphics[width=0.49\textwidth]{figures/fig3-heis-energy.pdf}
%     \includegraphics[width=0.49\textwidth]{figures/fig3-heis-fidelity.pdf}
%     \centering\vskip -2ex
%     \vskip -1ex

%     \caption{
%     \memo{Whole comparison for Heisenberg $L=3, Q=5$}
%     Energy (left) and Fidelity (right) achieved with the cumulative number of measurement shots for the Heisenberg Hamiltonian with $(L=3)$-layered $(Q=5)$-qubits quantum circuit.  
%     }
% \label{fig:ComparisonHeisenbergThreeFive}
%     %\vspace{-5mm}
% \end{figure*}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.