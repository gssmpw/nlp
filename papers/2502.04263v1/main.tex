
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{lipsum} %  TODO remove after completing the acknowledgments

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{url}

%%%%%%% CUSTOM COMMANDS
\usepackage{color}
\usepackage{xcolor,colortbl}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{tipa}
\usepackage{scalerel}
\usepackage{xspace}
\usepackage{subcaption} % for subfigures

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[title]{appendix}
\usepackage{caption}

\newcommand{\cmark}{\textcolor{green}{\checkmark}} % green tick
\newcommand{\xmark}{\textcolor{red}{\text{\sffamily X}}} % red cross

\newcommand{\minisection}[1]{\vspace{0.0in} \noindent {\bf #1}}

\newcommand*{\thead}[1]{\multicolumn{1}{c}{\bfseries #1}} % table headers
\newcommand{\hide}[1]{}
\newcommand{\hidefoot}[1]{\footnote{{\bf #1}}}
\newcommand{\todo}[1]{{\bf TO DO: {#1}}}
\newcommand{\dquote}[1]{``#1''}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\answer}[1]{\textcolor{blue}{#1}}
\newcommand\wh[1]{\hstretch{2}{\hat{\hstretch{.5}{#1\mkern1mu}}}\mkern-1mu}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\iid{i.i.d\onedot} \def\wolog{w.l.o.g\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\newcommand{\tit}[1]{\smallbreak\noindent\textbf{#1}}
\newcommand{\tinytit}[1]{\noindent\textbf{#1}}

\definecolor{purple}{rgb}{0.65,0,0.65}
\definecolor{dark_green}{rgb}{0, 0.5, 0}
\definecolor{blueish}{rgb}{0.0, 0.3, .9}
\definecolor{brown}{rgb}{0.6, 0.3, 0}
\definecolor{LightCyan}{rgb}{0.88,0.95,1}


\definecolor{tabhighlight}{rgb}{0.88,0.95,1}
\definecolor{tabhighlightbluetext}{rgb}{0.2,0.4,0.8}
\newcommand{\bluetext}[1]{\textcolor{tabhighlightbluetext}{#1}}

\definecolor{tabhighlightpurple}{rgb}{0.95,0.88,1}
\definecolor{tabhighlightpurpletext}{rgb}{0.7,0.4,0.8}
\newcommand{\purpletext}[1]{\textcolor{tabhighlightpurpletext}{#1}}

\definecolor{transparent}{cmyk}{0,0,0,0}
\definecolor{demphcolor}{RGB}{144, 144, 144}
\newcommand{\demph}[1]{\textcolor{demphcolor}{#1}}
\newcommand{\rotbox}[1]{\rotatebox{90}{#1}}
\newcommand{\tableCellHeight}{1}
\newcommand{\romannum}[1]{\romannumeral #1} 

\definecolor{ForestGreen}{RGB}{34, 139, 34}
\newcommand{\hgreen}[1]{\textcolor{ForestGreen}{\textbf{#1}}} 
\newcommand{\hred}[1]{\textcolor{red}{\textbf{#1}}} 
\newcommand{\hblue}[1]{\textcolor{blue}{#1}}
\newcommand{\horange}[1]{\textcolor{orange}{#1}}

\newcolumntype{h}{>{\columncolor{tabhighlight}}c}

\usepackage{wrapfig}

\usepackage[pagebackref,breaklinks]{hyperref}
\DeclareRobustCommand{\vect}[1]{\bm{#1}}
\pdfstringdefDisableCommands{
  \renewcommand{\vect}[1]{#1}
}
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\title{Cross the Gap: Exposing the Intra-modal\\Misalignment in CLIP via Modality Inversion}


\author{\textbf{Marco Mistretta}$^{1,*}$, \textbf{Alberto Baldrati}$^{1,2,*}$, \textbf{Lorenzo Agnolucci}$^{1,*}$ \\ 
\textbf{Marco Bertini}$^{1}$, \textbf{Andrew D. Bagdanov}$^{1}$ \\
$^1$University of Florence, Media Integration and Communication Center (MICC), Italy \\
$^2$University of Pisa, Italy \\
\texttt{\{name.surname\}@unifi.it} \\
}
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work.}
\def\thefootnote{\arabic{footnote}}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
\begin{document}

\maketitle
    
\begin{abstract}
Pre-trained multi-modal Vision-Language Models like CLIP are widely used off-the-shelf for a variety of applications. 
In this paper, we show that the common practice of individually exploiting the text or image encoders of these powerful multi-modal models is highly suboptimal for intra-modal tasks like image-to-image retrieval. 
We argue that this is inherently due to the CLIP-style inter-modal contrastive loss that does not enforce any intra-modal constraints, leading to what we call \emph{intra-modal misalignment}.
To demonstrate this, we leverage two optimization-based modality inversion techniques that map representations from their input modality to the complementary one without any need for auxiliary data or additional trained adapters.  
We empirically show that, in the intra-modal tasks of image-to-image and text-to-text retrieval, approaching these tasks \textit{inter-modally} significantly improves performance with respect to intra-modal baselines on more than fifteen datasets.
Additionally, we demonstrate that approaching a native inter-modal task (e.g. zero-shot image classification) \textit{intra-modally} decreases performance, further validating our findings.
Finally, we show that incorporating an intra-modal term in the pre-training objective or narrowing the \emph{modality gap} between the text and image feature embedding spaces helps reduce the \emph{intra-modal misalignment}. The code is publicly available at: \small{\href{https://github.com/miccunifi/Cross-the-Gap}{\mbox{\url{https://github.com/miccunifi/Cross-the-Gap}}}}.
\end{abstract}

\section{Introduction}
In recent years the availability of massive, pre-trained Vision-Language Models (VLMs) has enabled a wide variety of applications ranging from zero-shot image segmentation \citep{zhou2022extract, luddecke2022image} to visual question answering \citep{song2022clip, parelli2023clip}. These models are typically composed of independent image and text encoders which are simultaneously trained on massive corpora of image-text pairs to align the text and image embeddings of associated inputs. For example, the Contrastive Language-Image Pre-training (CLIP) model is trained on a corpus of 400M image-text pairs to map inputs from both modalities into a shared embedding space~\citep{radford2021learning}. CLIP is trained with an inter-modal contrastive loss that aims to maximize the similarity of corresponding image-text samples while minimizing the similarity with all the other examples within a batch.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{images/teaser.pdf}
    \caption{Motivation and overview. \textbf{Left:} The \textit{inter-modal} contrastive loss used in pretraining enforces paired images and texts to be at a given distance $r$ (\ie $r_{dog}\text{ and }r_{cat}$) but does not encourage \textit{intra-modal} alignment. Consequently, intra-modal similarity scores might not correspond to those of actual images and texts (\ie $d_1 < d_2$). \textbf{Right:} We show that the common practice of individually exploiting only one encoder is suboptimal and approaching intra-modal tasks (\eg image-to-image retrieval) \textit{inter-modally} via modality inversion improves performance.}
    \label{fig:teaser}
\end{figure*}


Despite CLIP's shared embedding space, visual and textual features lie in distinct regions. This phenomenon, known as the \textit{modality gap} \citep{liang2022mind}, originates from model initialization, and the inter-modal contrastive loss preserves and worsens it during training. Moreover, we note that CLIP's contrastive training strategy focuses on \textit{inter-modal} (\ie image-text) similarities between paired samples and disregards \textit{intra-modal} (\ie image-image and text-text) similarities. Consequently, the intra-image and intra-text similarities between CLIP representations might not faithfully correspond to those of the actual images or texts, as depicted in the left section of \cref{fig:teaser}.
We refer to this issue as \textit{intra-modal misalignment}.
A simple experiment aimed at quantifying this problem is presented in Appendix~\ref{sec:intra-modal-misalignment}.

Aspects of this misalignment have been accounted for in the limited scope of zero- and few-shot image classification \citep{udandarao2023sus, yi2024leveraging}. However, many recent works overlook this phenomenon and still employ CLIP representations for intra-modal comparison, which leads to suboptimal similarity measurements. Examples range from KNN-based image classification \citep{geirhos2024towards} to text-to-image generation \citep{gal2022image, ruiz2023dreambooth} and video synthesis \citep{esser2023structure, zhang2024avid}. For instance, \citet{esser2023structure} measure the temporal consistency of generated videos via intra-modal CLIP similarity between consecutive frames.

In this paper we argue that relying on intra-modal similarities computed using pre-trained CLIP encoders is inherently suboptimal. To support this we conduct an extensive study of the behavior of intra-modal similarities on the intra-modal tasks of image-to-image and text-to-text retrieval. We perform this analysis by transforming intra-modal tasks into inter-modal ones to leverage CLIP's inter-modal alignment. 

Specifically, we map features from their \textit{native} modality (\ie the same as the input) into their \textit{complementary} one. We refer to this process as \textit{modality inversion}. To perform modality inversion we adapt Optimization-based Textual Inversion (OTI) \citep{baldrati2023zero} and introduce Optimization-based Visual Inversion (OVI). OTI and OVI are iterative modality inversion strategies that map image features into text features and vice versa while keeping the encoders frozen. These techniques operate at the single-feature level, \ie they do not require external data nor the training of a mapping network.

Our experiments show that tackling intra-modal tasks inter-modally via modality inversion -- as illustrated in the right side of \cref{fig:teaser} -- outperforms intra-modal baselines on more than fifteen datasets. To additionally support our claim that this performance improvement stems from inter-modal alignment and not the modality inversion process itself, we transform inter-modal tasks into intra-modal ones. Specifically, we show that applying modality inversion to the inherently inter-modal zero-shot image classification task yields \textit{worse} performance than the inter-modal baseline. 

Moreover, we investigate whether the inclusion of an intra-modal loss during image-text contrastive pre-training reduces intra-modal misalignment. For this analysis we use SLIP \citep{mu2022slip}, which adds just such an intra-modal loss to improve the alignment within the image embedding space. Results confirm that adding intra-modal loss terms during the pre-training of VLMs significantly mitigates intra-modal misalignment. Finally, we study the relation between the modality gap phenomenon and the intra-modal misalignment. In particular, similar to \citet{liang2022mind} we fine-tune CLIP to reduce the modality gap and observe a decrease in the performance of approaching intra-modal tasks inter-modally. This indicates that a narrower modality gap diminishes the impact of intra-modal misalignment.

\newpage
The main contributions of this work are:
\vspace{-0.1cm}
\begin{itemize}
\item we conduct a thorough and comprehensive study of CLIP's intra-modal misalignment, and our analyses show that the common practice of relying on intra-modal similarities computed through pre-trained CLIP encoders is inherently suboptimal.
\item we propose to transform intra-modal tasks to inter-modal ones via modality inversion to exploit CLIP's inter-modal alignment. To this end we introduce OVI, a single-feature level modality inversion strategy that maps textual features into the image embedding space;
\item we conduct extensive experiments that show that approaching intra-modal tasks inter-modally significantly outperforms intra-modal baselines on more than fifteen datasets; and
\item we demonstrate that adding intra-modal loss terms during VLM pre-training or reducing the modality gap mitigates the impact of intra-modal misalignment.

\end{itemize}

\section{Related Work} \label{sec:related_work}
\minisection{Contrastively trained Vision-Language Models.}
VLMs have become increasingly popular for their ability to learn aligned representations across visual and textual modalities~\citep{radford2021learning, jia2021scaling, zhai2022lit, zhai2023sigmoid, mu2022slip, li2021supervision}. 
This alignment enables VLMs to be used in a broad variety of downstream tasks, including image-text retrieval and zero-shot image classification, by projecting images and text into a shared feature space.

The most prominent example is CLIP~\citep{radford2021learning}, which maximizes the similarity between paired images and text captions while minimizing the similarity with the other samples in the batch. SigLIP~\citep{zhai2023sigmoid}, on the other hand, employs a sigmoid-based contrastive loss that considers only the single image-text pairs while neglecting the other samples in the same batch.
More recently, several approaches have extended the CLIP-style contrastive loss by incorporating intra-modal similarities into the training objectives~\citep{mu2022slip, li2021supervision}. For instance, SLIP \citep{mu2022slip} integrates a self-supervised component that maximizes the similarity between different augmentations of the same image, with a strategy akin to SimCLR~\citep{chen2020simple}. 

\minisection{The modality gap in multi-modal models.}
\citet{liang2022mind} demonstrated a consistent phenomenon affecting VLMs known as the \textit{modality gap}. This refers to the separation between feature embeddings of different modalities (\eg text and images) within their shared representation space~\citep{liang2022mind}. The modality gap arises due to both model initialization and the contrastive learning objective used during training. At initialization, independent encoders for each modality produce embeddings that are restricted to distinct regions (or cones) within the representation space. During training, the contrastive learning process preserves and worsens this separation. Several works have studied the causes and implications of the modality gap in CLIP~\citep{ shi2023towards, schrodi2024two, zhang2023diagnosing}. \citet{schrodi2024two} analyzed the embedding space and demonstrated that a minimal number of embedding dimensions -- often as few as two -- are sufficient to perfectly separate the image and text modalities. 

\minisection{Intra-modal misalignment.} 
Some studies have investigated the problem of misaligned intra-modal embedding distances within the context of zero- and few-shot image classification~\citep{udandarao2023sus, yi2024leveraging}. To address this, \citet{udandarao2023sus} propose mitigating the issue by computing similarities in the image-text space, rather than working exclusively with image embeddings, thereby leveraging the inter-modal nature of the feature representations. Similarly, CODER~\citep{yi2024leveraging} introduces an enhanced image representation technique based on measuring distances between images and their neighboring texts within CLIP's embedding space. 

\minisection{Our contribution with respect to the state-of-the-art.}
While these prior works have addressed various aspects of intra-modal and inter-modal relationships within VLMs, their scope remains limited, often focusing on specific tasks, datasets, or narrow perspectives on the modality gap and its effects. None of these studies comprehensively investigate the fundamental nature of the intra-modal versus inter-modal similarities across diverse tasks and datasets, nor do they fully explore the potential performance improvements achievable by leveraging inter-modal comparisons for intra-modal problems. The motivation behind our work is to shed light on the phenomenon of intra-modal misalignment, and its relationship to the modality gap, and to demonstrate the importance of either ensuring intra-modal alignment during pre-training or comparing solely representations that belong to different modalities. 

\section{CLIP Preliminaries}
CLIP is a vision-language model trained to align images and textual captions in a shared embedding space~\citep{radford2021learning}. It consists of an image encoder $f_{\theta}$ and a text encoder $g_{\phi}$. Given an image $I$, the image encoder extracts its feature representation $f_{\theta}(I) \in \mathbb{R}^{d}$, where $d$ is the size of the shared embedding space. Likewise, for a given textual caption $Y$, first a word embedding layer $E_v$ maps each tokenized word to the token embedding space $\mathcal{V}$. Then, the text encoder $g_{\phi}$ generates the textual feature representation $g_{\phi}(E_v(Y)) \in \mathbb{R}^{d}$. 

When using a Vision Transformer (ViT)~\citep{dosovitskiy2020image} as the visual encoder $f_{\theta}$, the encoding process begins by splitting the image into $U$ fixed-size non-overlapping patches. Each patch is then transformed into a corresponding patch embedding $\{ w_1, w_2, \ldots, w_U \}$ through a linear projection by the patch embedding layer $E_w$, where each $w_i$ resides in the patch embedding space $\mathcal{W}$. A learnable class (CLS) token $c$ is concatenated with the patch embeddings, resulting in the input to the vision transformer being $ \bar{I} = \{c, w_1, w_2, \ldots, w_U \}$. Finally, the CLS token of the final transformer layer is projected into the shared embedding space via a linear projection to obtain the final representation $f_{\theta}([c, E_w(I))] = f_{\theta}({\bar{I})} \in \mathbb{R}^{d}$. For brevity, when unnecessary we will omit both the patch embedding layer $E_w$ and the token embedding layer $E_v$, and use the simplified notations $f_{\theta}(I)$ instead of $f_{\theta}([c, E_w(I)])$ and $g_{\phi}(Y)$ instead of $g_{\phi}(E_v(Y))$.

Given a batch of image-caption pairs $ B = \{ (I_n, Y_n) \}_{n=1}^N $, CLIP aims to maximize the cosine similarity for the $N$ correct pairs while minimizing it for the $N^2 - N$ other pairs. This is achieved by optimizing a symmetric, multi-class N-pair contrastive loss~\citep{sohn2016improved}. Let $\psi_I^n = f_{\theta}(I_n)$ and $\psi_T^n = g_{\phi}(E_v(Y_n))$ denote the image and text embeddings, respectively. The CLIP loss is:
\vspace{0.1cm}
\begin{equation}\label{eq:clip_loss}
   \mathcal{L}_{\text{CLIP}} = - \frac{1}{N} \sum_{n=1}^{N} \left( \log \frac{\exp(c(\psi_I^n, \psi_T^n) / \tau)}{\sum_{m=1}^{N} \exp(c(\psi_I^n, \psi_T^m) / \tau)} + \log \frac{\exp(c(\psi_T^n, \psi_I^n) / \tau)}{\sum_{m=1}^{N} \exp(c(\psi_T^n, \psi_I^m) / \tau)} \right),
\end{equation}
\vspace{0.1cm}
where $c(\cdot, \cdot)$ denotes the cosine similarity, and $\tau$ is a temperature parameter. As shown by \citet{liang2022mind}, \cref{eq:clip_loss} leads to a measurable separation between embeddings of the different modalities, creating what is known as the \textit{modality gap}. This gap is significantly affected by the temperature $\tau$, with a larger gap occurring as the temperature decreases.

Note that CLIP's training loss focuses exclusively on inter-modal similarities between paired samples while neglecting intra-modal similarities.
For example, consider an image feature anchor $\psi_I$ and two distinct text features $\psi_T^1$ and $\psi_T^2$ expressing the same concept. The loss enforces both $\psi_T^1$ and $\psi_T^2$ to be at a cosine distance $r$ from $\psi_I$, where the cosine distance is defined as \( d(\psi_A, \psi_B) = 1 - c(\psi_A, \psi_B) \). This is equivalent to \( d(\psi_I, \psi_T^1) = d(\psi_I, \psi_T^2) = r \), meaning the text embeddings lie on a hypersphere of radius $r$ centered at $\psi_I$. The absence of intra-modal constraints leaves the alignment between $\psi_T^1$ and $\psi_T^2$ remains uncalibrated; thus, we have $0 \leq d(\psi_T^1, \psi_T^2) \leq 2r$. This indicates that, while both text features are equidistant from the image feature, their intra-modal similarity is not constrained in any way, leading to intra-modal misalignment.
We argue that such a misalignment must either be mitigated via additional intra-modal losses during pre-training or must be compensated by tackling intra-modal tasks inter-modally.

\section{From Intra-modal to Inter-modal via Modality Inversion}

Due to the modality gap, images and text features lie in distinct regions in CLIP's shared embedding space. Previous work introduced modality inversion techniques to map features from the native modality to the complementary one \citep{ramesh2022hierarchical, patel2024eclipse, li2023decap}.
For instance, \citet{ramesh2022hierarchical} trains a diffusion model to generate CLIP's image features from text captions for text-to-image generation. 

Our goal is to demonstrate that tackling intra-modal tasks in an inter-modal way outperforms intra-modal baselines. To this end, we propose to employ a modality inversion strategy to derive representations that exploit both native and complementary modality encoders. 
However, existing modality inversion techniques rely on external data or the training of a mapping network, making the inversion process dependent on external factors \citep{ramesh2022hierarchical, patel2024eclipse, li2023decap}. 

To minimize the impact of external biases in our analysis, we choose to rely on two modality inversion strategies that operate at a single-feature level, \ie that map each individual feature to its complementary modality without the need for any external resources. 
Specifically, we adapt Optimization-based Textual Inversion (OTI) \citep{baldrati2023zero, agnolucci2024isearle} and we introduce Optimization-based Visual Inversion (OVI) to map an image to the text embedding space and vice versa while keeping the encoders frozen. Both are iterative and optimization-based approaches.
The core concept behind OTI and OVI is to learn vectors of trainable parameters that are passed through the encoder of the \textit{complementary} modality to yield features aligned with the representations of the \textit{native} modality encoder. By optimizing these input vectors while keeping the encoder weights fixed, we ensure that the output features retain the pre-training alignment. In the following we define OTI and OVI for CLIP, but they can be applied to any VLM that maps images and texts into a shared embedding space.
To make our analysis more comprehensive, in Appendix~\ref{app:additional-experiments} we present two additional experiments in which modality inversion is performed using a pre-trained captioner or an adapter. 

\subsection{Optimization-based Textual Inversion (OTI)}
\label{sec:oti_method}

Starting from an image $I$, OTI involves iteratively optimizing a set of $R$ pseudo-tokens $v^* = \{v_1^*, v_2^*, \ldots, v^*_R \}$, with $v_i^* \in \mathcal{V}$ for $i \in \{1, \ldots, R\}$, for a given number of optimization steps $S$. We refer to $v^*$ as pseudo-tokens since they belong to the token embedding space $\mathcal{V}$ but are not associated with any existing words.
\Cref{alg:oti} in Appendix~\ref{app:implementation-details} shows the pseudo-code of OTI.

The pseudo-tokens $v^*$ are randomly initialized and concatenated with the template sentence ``a photo of'' to form $\overline{Y}_{v^*} = [E_v(\text{``a photo of''}), v^*]$ input into the CLIP text encoder $g_{\phi}$ to obtain $\psi_T = g_{\phi}(\overline{Y}_{v^*})$. Then we extract the features of the image $I$ with the CLIP image encoder $f_{\theta}$, resulting in $\psi_{I} = f_{\theta}(I)$. Since we aim to obtain a textual feature representation $\psi_T$ that captures the informative content of $I$, we minimize the gap between image and text features via a cosine loss:

\vspace{-0.5cm}
\begin{equation}\label{eq:loss_oti_content}
     \mathcal{L}_{\text{cos}} = 1 - c{(\psi_{I}, \psi_T)}.
\end{equation}
\vspace{-0.5cm}

Note that while we adapt OTI from~\citet{baldrati2023zero} our goal is significantly different. 
Their work focuses on deriving a single pseudo-token that captures the informative content of the image $I$ and can interact with existing words to form meaningful sentences (e.g., ``a photo of $v^*$ that is running \dots''), thus they use OTI for combining inputs from both modalities. In contrast, we use OTI purely as a mapping technique from visual to textual features. We do not focus on the pseudo-tokens themselves but aim to obtain a final representation that effectively captures the content of the image $I$. Additionally, the original OTI technique employs a regularization loss that exploits an auxiliary vocabulary to constrain the pseudo-token to reside in CLIP's token embedding space. However we are not interested in using the learned $v^*$ in different contexts -- and more importantly, we aim to avoid influencing the inversion process with external data. For this reason we do not use a regularization loss.

\subsection{Optimization-based Visual Inversion (OVI)}\label{sec:ovi_method}
We propose the OVI approach to map text features from the CLIP text embedding space to the visual embedding space. Note that since OVI learns vectors of trainable parameters in the patch embedding space $\mathcal{W}$, it can be applied only to ViT-based image encoders.

Given a sentence $Y$, we first extract its text features $\psi_T = g_{\phi}(E_v(Y))$. OVI then optimizes a set of $P$ randomly initialized pseudo-patches $w^* = \{w_1^*, \ldots, w_P^*\}$, where each $w_i^* \in \mathcal{W}$. This optimization is performed for a fixed number of optimization steps $S$. Similarly to the terminology introduced in \cref{sec:oti_method}, we refer to $w^*$ as pseudo-patches since they belong to the patch embedding space $\mathcal{W}$ but are not associated with any existing image. 
\Cref{alg:ovi} in Appendix~\ref{app:implementation-details} illustrates the pseudo-code of the OVI method.

Since the ViT employs learned positional embeddings, the number of input patches $U$ to the image encoder is fixed. Consequently, when $P<U$ directly using $w^*$ as input is impossible. 
In such cases, we apply nearest-neighbor interpolation to repeat the pseudo-patches and match the required number of $U$ patches.

Specifically, given the pre-trained CLS token $c$, the input to the ViT is given by:
\begin{equation}\label{eq:ovi_interpolation}  
    \bar{I}_{w^*} = \{c, \underbrace{w_1^*, w_1^*, \ldots, w_1^*}_{H_1 \text{ times}}, \underbrace{w_2^*, w_2^*, \ldots, w_2^*}_{H_2 \text{ times}}, \ldots, \underbrace{w_P^*, w_P^*, \ldots, w_P^*}_{H_P \text{ times}}\},
\end{equation}
where $H_1, \ldots, H_P$ represent the number of times each pseudo-patch is repeated, and \mbox{$H_1+\ldots+H_P = U$}. The specific values are given by the nearest-neighbor interpolation.
Finally, the input $\bar{I}_{w^*}$ is passed through CLIP's image encoder to obtain the features $\psi_I = f_{\theta}(\bar{I}_{w^*})$. To obtain a visual feature representation $\psi_I$ that captures the informative content of $Y$, we minimize the gap between the image and text features using the same cosine-based loss in \cref{eq:loss_oti_content}. 

\subsection{Crossing the Modality Gap with OTI and OVI}
\label{sec:modality_drift}

The goal of OTI and OVI is to map features from the native modality into corresponding features in the complementary modality. 
We observe that in cases where the loss $\mathcal{L}_{\text{cos}}$ approaches zero, the complementary features converge to the native ones, thus drifting into the native modality embedding manifold. This undermines the goal of leveraging CLIP's image-text alignment.

For OTI, in our experiments the loss never approaches zero -- within a reasonable number of optimization steps -- when considering a single pseudo-token (\ie $R=1$). We argue that this stems from the strong inductive biases of the frozen encoders and the modality gap, making it challenging for a single pseudo-token to bridge the distance between image and text representations. Nevertheless, the OTI-inverted features retain the informative content of the corresponding image. As a result, the potential drift related to $\mathcal{L}_{\text{cos}}$ does not pose a significant issue, and inter-modal alignment is preserved. In all experiments we use $R=1$ unless stated otherwise.

Also for OVI we observe that the loss only approaches zero when the number of pseudo patches $P$ is relatively large. Unlike OTI, we find that for some experiments a single pseudo-patch (\ie $P=1$) is insufficient for embedding the informative content of the corresponding text. We believe that this discrepancy stems from the inherent differences between images and texts. Specifically, in textual inputs a single word (or pseudo-token) can significantly alter the meaning of a sentence. For instance, the sentences ``a photo of a building'' and ''a photo of a dog'' convey completely different meanings, despite differing by only one word. In contrast, a single (pseudo-)patch has less influence on the overall semantic content of an image. Therefore, while a single pseudo-token is enough for an effective modality inversion with OTI, more pseudo-patches may be necessary when applying OVI. Consequently, in our experiments, we employ a number of pseudo-patches $P$ ranging from 1 to 4, based on the considered model (see Appendix~\ref{app:abl_num_tokens} for more details). For such values, the pseudo-patches effectively embed the informative content of the input text. Moreover, the inter-modal alignment is maintained and the drift does not constitute a significant problem. 

\section{Experimental Results}
Here we report on a broad range of experiments supporting our claims. We first evaluate two intra-modal tasks: image-to-image and text-to-text retrieval. We show that transforming intra-modal tasks into inter-modal ones via OTI and OVI consistently improves performance by better aligning with the original CLIP training objective. To confirm that this outcome does not stem from the modality inversion process itself, we evaluate a natively inter-modal task -- zero-shot image classification -- and show that making it intra-modal hinders the performance.
Finally, we analyze the behavior of modality inversion techniques, and we study how adding intra-modal loss terms during VLM pre-training or narrowing the modality gap affects intra-modal misalignment.
In the following, we denote as \textit{inter-modal approaches} those involving inter-modal similarity comparisons, \ie similarity comparisons between features of two different modalities (such as image-text, OTI-image, and OVI-text). Conversely, \textit{intra-modal approaches} refer to methods that employ intra-modal similarity comparisons (such as image-image, text-text, OTI-OTI, and OVI-OVI).

To ensure a comprehensive analysis, we experiment using multiple CLIP models with different backbones and pre-training datasets. We also consider SigLIP to prove that our observations are not specific to the CLIP loss but generalize to other inter-modal contrastive losses. Specifically, we use OpenAI CLIP with ViT-B/32 and ViT-L/14 backbones, OpenCLIP (OPEN) pre-trained on the DataComp dataset \citep{gadre2024datacomp} with the same backbones, and SigLIP-B/16.
Implementation details and description of all datasets used are given in Appendices~\ref{app:implementation-details}~and~\ref{app:datasets}.

\subsection{Image-to-Image Retrieval}
\label{sec:image_to_image_retrieval_experiment}
Pre-trained CLIP image encoders are often used to extract features for image-to-image similarity comparisons. For this reason, we study the image-to-image retrieval task.

\minisection{Experiment design.} The objective is to retrieve images from a gallery that are visually similar to a given query image. 
We consider a total of 15 datasets commonly employed for image-to-image retrieval and image classification. We consider two strategies. In the first, which we call \emph{intra-modal}, we directly compare the features of the query image with those of the gallery. In the second, we transform the intra-modal image-to-image retrieval task into an inter-modal one by applying OTI to the query image. Then, we use the resulting OTI-inverted features to query the gallery.

\minisection{Results.} We report the results in \cref{tab:image_retrieval}. Approaching the task \textit{inter-modally} using the OTI-inverted features outperforms the intra-modal baseline, achieving an average absolute improvement ranging from 2\% to 3\%. Specifically, we observe that the performance gain is obtained across a large variety of datasets with different class granularity and diverse domains, spanning from birds (CUB) to monuments ($\mathcal{R}$Paris). Moreover, we notice that the intra-modal misalignment phenomenon is independent of the pre-training dataset (CLIP vs. OpenCLIP) and pre-training contrastive loss (CLIP/OpenCLIP vs. SigLIP) since the performance improvement is consistent across all the considered VLMs. 
Finally, we note that OTI-inverted features cannot contain more informative content than the native ones -- used by the intra-modal baseline -- since they are obtained simply by mapping them to the complementary modality at a single-feature level and without using any external resources.
Thus, the observed improvement is solely attributable to inter-modal alignment rather than to a more enriched representation.

\input{tables/image_retrieval}

\subsection{Text-to-Text Retrieval}
\label{sec:text_to_text_retrieval}

Although text features from pre-trained CLIP models are not commonly used for text-to-text tasks, we believe that it is important to show that our findings also apply to text-to-text comparisons.

\minisection{Experiment design.} 
Using the CLIP text encoder for text-only tasks presents several challenges. Specifically, the CLIP text encoder is trained on short, descriptive texts. As a result, using it for tasks such as sentiment analysis or text classification, which involve longer texts and abstract concepts, results in a mismatch with the pre-training data. Moreover, VLMs have a limited input token capacity (\eg 77 tokens for CLIP), which makes them unsuitable for longer texts.
To avoid these problems, we formulate an intra-modal text-to-text retrieval task using image captioning datasets. Specifically, we select datasets in which each sample consists of an image and multiple associated captions (\eg Flickr30K~\citep{plummer2015flickr30k}). These captions are comparable to those used in VLM training and are short enough to avoid token limit issues. We ignore the images and use the first caption associated with each image as the query text. The goal is to retrieve the other captions related to the same image from a gallery of all captions in the dataset.
In Appendix~\ref{app:additional-experiments}, we also report experimental results on purely textual text-to-text retrieval datasets. To address the issue of texts exceeding the input token limit of VLMs, we use an LLM to summarize the texts.
 
As in the image-to-image retrieval experiments detailed in \cref{sec:image_to_image_retrieval_experiment}, we consider two strategies. In the former, we use the query text features to retrieve from the gallery. In the latter, we approach the task inter-modally by applying OVI to each query to obtain the complementary features.

\minisection{Results.}
\Cref{tab:text_retrieval} (left) shows the results. Analogously to image-to-image retrieval, using the OVI-inverted features outperforms the intra-modal baseline. Specifically, the absolute performance gains range from 1\% to 5\% across all datasets, backbones, and VLMs. This outcome proves that intra-modal similarity comparisons lead to suboptimal performance independently from the considered modality. Hence, the intra-modal misalignment phenomenon involves both the image and text embedding spaces.

\input{tables/text_retrieval_and_image_classification}

\subsection{Zero-Shot Image Classification}\label{sec:zero_shot_image_classification}
We evaluate the performance of modality inversion on inter-modal tasks, such as zero-shot image classification and image-text retrieval. We expect that transforming inter-modal tasks to intra-modal ones \textit{hinders} performance due to intra-modal misalignment. Here we consider zero-shot image classification, while we report experiments on image-text retrieval in the supplementary material.

\minisection{Experiment design.}
CLIP-like models perform zero-shot image classification by predicting the output class based on the similarity between the input image and a set of textual prompts in the form of ``\textit{a photo of a [$\text{CLASS}$]}'', where $\text{CLASS}$ represents each class name, such as ``cat'' or ``dog''. 
Following \citet{zhou2022learning}, we take into account 11 datasets (see the supplementary material for more details). We consider three strategies. The first is the inter-modal baseline, which compares the features of the input image and the set of prompts. In the second, we apply OTI to the input image. In the third, OVI is applied to each textual prompt. 

\minisection{Results.} 
In \cref{tab:zeroshot_classification} (right) we report the performance of the first two strategies described above. Results for the third strategy are given in Appendix~\ref{app:additional-experiments}. As expected, using modality inversion consistently leads to performance degradation across different VLMs and backbones. 
Note that the datasets used in zero-shot image classification are also employed for image-to-image retrieval in \cref{sec:image_to_image_retrieval_experiment}. This allows us to reuse the \textit{same} OTI-inverted features for both tasks. Interestingly, the results are opposite: performance improves in image-to-image retrieval but decreases in zero-shot image classification. This contrast arises because, in the former, we transform an intra-modal task into an inter-modal one, while in the latter, we do the reverse. This experiment demonstrates that modality inversion does not inherently improve performance, as the same OTI-inverted features can either enhance or hinder results depending on the nature of the task. 
\subsection{Analyzing Modality Inversion}
\label{sec:discussion_oti}

\begin{figure}[t]
\setlength{\tabcolsep}{1pt}
\centering
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{images/OTI_only_loss.pdf} &
\includegraphics[width=0.32\textwidth]{images/OTI_only_map.pdf} &
\includegraphics[width=0.32\textwidth]{images/OTI_histograms.pdf} \\
\small (a) & \small (b) & \small (c)
\end{tabular}
\vspace{-6pt}
\caption{\textbf{(a, b)} Loss values and retrieval performance over \bluetext{OTI} optimization steps for different numbers of pseudo-tokens $R$.
\textbf{(c)} Distribution of pairwise image-image, text-image, and \bluetext{OTI}-image cosine similarities. We consider the \bluetext{OTI}-inverted features using four pseudo-tokens ($R = 4$) at two distinct optimization steps: the \textit{performance peak} (step 17) and the \textit{final step} (step 1000).
}
\vspace{-10pt}
\label{fig:discussion}
\end{figure}

In this section we study how and why transforming native modality features into complementary ones via modality inversion leads to performance improvement on intra-modal tasks. 
For brevity, we consider only OTI, but we find that the same considerations apply to OVI.
We consider the Cars dataset \citep{krause20133d} and the CLIP ViT-B/32 model.

In Figs. \ref{fig:discussion}(a) and \ref{fig:discussion}(b) we investigate how the values of the loss $\mathcal{L}_{\text{cos}}$ and the image-to-image retrieval performance vary based on the number of optimization steps and pseudo-tokens $R$. First, we notice that with a single pseudo-token (\ie $R=1$) the loss does not reach zero within a reasonable number of optimization steps. Conversely, as $R$ increases (\ie the number of trainable parameters grows) the loss decreases more rapidly and approaches zero.
As discussed in \cref{sec:modality_drift}, as the loss decreases the OTI-inverted features shift away from the text manifold towards the image manifold, approaching the original native image features. This phenomenon is reflected in the image retrieval performance shown in Fig. \ref{fig:discussion}(b), since for enough optimization steps and pseudo-tokens the performance approaches those obtained by the native image features. Moreover, we observe that, regardless of the value of $R$, the best performance corresponds to a relatively low number of optimization steps.

We argue that, in proximity to the performance peak observed during the optimization process, the OTI-inverted features capture the informative content of the corresponding image while retaining the inter-modal alignment. To support this claim, we compute cosine similarities for image-image, text-image, and OTI-image feature pairs. Specifically, we analyze the OTI-inverted features using four pseudo-tokens ($R=4$) at two distinct optimization steps: the \textit{performance peak} (step 17) and the \textit{final step} (step 1000), where performance approaches the intra-modal baseline.
In \cref{fig:discussion}(c) we plot the distribution of these pairwise similarities.
We observe that, at the performance peak, the similarity distribution of OTI-image matches the text-image similarity distribution, while at the final step it aligns with the image-image similarity distribution, confirming the drift of the OTI-inverted features toward the image manifold. 
This suggests that OTI-inverted features perform best when aligned with image features in the same way as text features, confirming our hypothesis that the performance improvement obtained by OTI stems from leveraging CLIP's inter-modal alignment.

Finally, we notice that $R=1$ is \textit{not} the optimal choice to achieve the best performance when using OTI. Still, we use $R=1$ in the experiments as the associated OTI-inverted features are less prone to drift towards the native image features, thus being more robust to the number of optimization steps. Moreover, the main objective of this work is \textit{not} to achieve the best results on the downstream tasks but rather show that using VLMs intra-modally is suboptimal. 

\subsection{The Role of Intra-Modal Constraints}

We investigate whether incorporating an intra-modal loss term during image-text contrastive pre-training effectively mitigates the issue of intra-modal misalignment. To this end, we consider SLIP~\citep{mu2022slip}, which adds a self-supervised intra-modal loss based on SimCLR \citep{chen2020simple} to the standard CLIP inter-modal contrastive loss $\mathcal{L}_\text{CLIP}$ (see Appendix~\ref{app:additional-VLMs} for more details).
Such intra-modal loss encourages the model to produce similar representations for two augmentations of the same image, aiming to improve the intra-modal alignment within the image embedding space.

To verify this, we perform an image-to-image retrieval experiment following the evaluation protocol from \cref{sec:image_to_image_retrieval_experiment}. We report the results in \cref{tab:slip_image_retrieval}. Notably, the OTI-inverted features achieve comparable performance to the native image ones. This contrasts with results from VLMs trained solely with an inter-modal contrastive loss (see \cref{tab:image_retrieval}), in which OTI led to a substantial performance boost. This experiment proves that SLIP's intra-modal loss effectively reduces intra-modal misalignment and suggests the importance of including such a loss when pre-training VLMs.

\input{tables/slip_image_retrieval}

\subsection{The Role of the Modality Gap}\label{sec:modality_gap}

\input{tables/clip_gap_image_retrieval}
During CLIP pre-training, the temperature parameter $\tau$ in \cref{eq:clip_loss} critically affects the modality gap: higher temperatures considerably reduce or close it~\citep{liang2022mind}. To examine the impact of the modality gap on the intra-modal misalignment, we fine-tune a CLIP ViT-B/32 model on the COCO dataset \citep{lin2014microsoft} using a temperature $\tau = 1.0$, which closes the modality gap. 
As a reference, we repeat the experiment with $\tau = 0.01$, \ie the value employed during CLIP pre-training. See \cref{tab:modality_gap} for more details on the magnitudes of the modality gap for the different models.

We reproduce our image-to-image retrieval experiments using these fine-tuned models and report results in \cref{tab:clip_gapimage_retrieval}. In the absence of the modality gap ($\tau = 1$) tackling intra-modal tasks inter-modally does not improve performance. This shows that closing the modality gap reduces the intra-modal misalignment. 
The results of the reference model ($\tau = 0.01$) prove that this outcome does not stem from the fine-tuning strategy.
As also observed by \citet{liang2022mind}, we note that using higher temperature values during training leads to an overall performance decrease in downstream tasks, despite reducing the modality gap. For this reason, we argue that -- in practice -- simply increasing the temperature value in \cref{eq:clip_loss} does not represent a viable strategy to address intra-modal misalignment. 

\section{Conclusions}
In this work we show that relying on intra-modal similarities computed with off-the-shelf VLMs is suboptimal for intra-modal tasks like image-to-image and text-to-text retrieval.
This stems from the inter-modal contrastive loss employed for pre-training these models that leads to a modality gap and \textit{intra-modal misalignment}. We propose to transform intra-modal tasks to inter-modal ones via two single-feature level modality inversion techniques. We demonstrate that this strategy improves performance as it exploits the inter-modal alignment of VLMs. Finally, we show that employing an intra-modal loss component during VLM pre-training or reducing the modality gap alleviates the impact of intra-modal misalignment.

\minisection{Limitations.} 
Our analyses demonstrate the significance of intra-modal misalignment when exploiting pre-trained CLIP models, but fall short of offering practical alternatives. The modality inversion techniques we propose are computationally expensive. They are based on iterative optimization of learnable input parameters (150 optimization steps for OTI and 1000 for OVI in our experiments). This limits their practical applicability and future work should concentrate on efficient methods to mitigate the \textit{intra-modal misalignment}. 

\clearpage 

\subsubsection*{Acknowledgments}
This work was supported by funding from the Italian national project \emph{Collaborative Explainable neuro-symbolic AI for Decision Support Assistant, CAI4DSA, CUP B13C23005640006}.

\section*{Reproducibility Statement}
We have taken steps in this work to ensure the reproducibility of our results. All models and datasets used in our experiments are publicly available and we release the complete source code. In the main paper and appendices material we provide complete details of all experimental setups, including model architectures, training and evaluation protocols, and hyperparameters. All random seeds are fixed ensuring that others can replicate our results with the provided code. 
We believe that the measures we have taken to ensure reproducibility will facilitate straightforward replication and verification of our findings, as well as allow the community to build upon our results in the future.


\bibliography{biblio}
\bibliographystyle{iclr2025_conference}


\clearpage
\begin{appendices}

% Reset counters for figures and tables
\setcounter{figure}{0}
\setcounter{table}{0}

% Redefine the numbering format for figures and tables
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}


\section{Implementation Details} \label{app:implementation-details}

\minisection{OTI and OVI.}
We report the pseudo-code of Optimization-based Textual Inversion (OTI) and Optimization-based Visual Inversion (OVI) in Algorithm~\ref{alg:oti} and Algorithm~\ref{alg:ovi}, respectively.
Unless stated otherwise, we use the same hyperparameters for OTI and OVI. 
We employ the AdamW \cite{loshchilov2018decoupled} optimizer with learning rate equal to 0.02, $\beta_1 = 0.9$, $\beta_2 = 0.999$, and weight decay $0.01$.
We perform 150 optimization steps for OTI and 1000 steps for OVI.
For OTI, we consistently use a single pseudo-token ($R = 1$). In contrast, for OVI, we employ a number of pseudo-patches $P$ ranging from 1 to 4, depending on the considered model (see \cref{app:abl_num_tokens} for more details).
On average, when using the CLIP ViT/B-32 model, OTI takes approximately 0.2 seconds per image, while OVI takes around 0.5 seconds per text prompt on a single A100 GPU (40GBs) with a batch size of 2048.
The memory usage scales linearly with the batch size. Specifically, when using the CLIP ViT-B/32 model, OTI requires approximately 1,878 MiB plus 18.6 MiB per sample in the batch. For example, with a batch size of 128, the memory consumption is about 4,260 MiB. For OVI, the memory usage is approximately 2,218 MiB plus 16.2 MiB per sample, resulting in about 4,290 MiB with the same batch size.
We use mixed precision to save memory and increase computational efficiency. In downstream tasks all the features are normalized to have a unit $L_2$-norm.

\minisection{CLIP fine-tuning.} 
To investigate the role of the modality gap on the intra-modal misalignment, we perform a fine-tuning of the CLIP model using different loss temperatures (see \cref{sec:modality_gap}).
In particular, we fine-tune the CLIP ViT B/32 model on the COCO training set for 30k steps, using a batch size of 512 and a learning rate of 1e-6. As an optimizer we employ AdamW with $\beta_1 = 0.9$, $\beta_2 = 0.98$ and a weight decay of $0.2$. To mitigate possible overfitting issues, we train only the final projection layers.
We train two different models, in the first we set the loss temperature parameter $\tau = 1.0$ (first two rows of Tab.~\ref{tab:clip_gapimage_retrieval}), while in the second we use $\tau = 0.01$ (last two rows of Tab.~\ref{tab:clip_gapimage_retrieval}).

\input{tables/alghoritms}

\section{More Insights on Intra-modal Misalignment}\label{sec:intra-modal-misalignment}

\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \vspace{-0.55cm}
    \includegraphics[width=\linewidth]{images/cats_dogs.pdf}
    \vspace{-0.75cm}
    \caption{Distribution of pairwise dog-dog and dog-cat image similarities. Overlap highlights the intra-modal misalignment issue.}
    \vspace{-0.5cm}
    \label{fig:histogram_dog_cats_similarities}
\end{wrapfigure}

To provide quantitative insights into the intra-modal misalignment issue we conduct a simple experiment using the CLIP ViT-B/32 model and the ``Dogs vs Cats'' dataset \citep{elson2007asirra}. This dataset consists of 25K images evenly distributed between two classes: dog and cat. Our goal is to demonstrate that, despite inter-modal alignment, the intra-modal similarity scores are misaligned, \ie they might not reflect those of actual images and texts, as illustrated in the left section of \cref{fig:teaser}. 

We start by filtering out images with incorrect inter-modal alignment to class-specific prompts. Specifically, we remove dog images that exhibit higher similarity to the prompt ``a photo of a cat'' than to the prompt ``a photo of a dog''. Then we use the dog-related prompt to query the gallery of all images and filter out the minimal number of images that are incorrectly ranked for this query. We repeat the same procedure for cat images. This filtering ensures perfect inter-modal alignment and text-image retrieval scores. 

On the resulting filtered dataset, we perform image-to-image retrieval using dog images as queries and the whole set of images as the gallery. If inter-modal alignment guarantees intra-modal alignment, all dog images should rank higher than cat images for any dog query, resulting in perfect retrieval. However, our results contradict this assumption. Specifically, we observe a mean Average Precision (mAP) of 83.1\% and an average R-Precision of 73.2\%, where R-Precision represents the precision at rank R, with R being the total number of relevant items for a given query. These findings indicate that on average at least 26.8\% of dog images are ranked below cat images for a given dog query. \Cref{fig:histogram_dog_cats_similarities} qualitatively illustrates this issue, revealing significant overlap between the distributions of pairwise dog-dog and dog-cat image similarities. We observe similar results when employing cat images as queries. Given the evidence of intra-modal misalignment in such a toy dataset, we believe that the issue is likely to be even more pronounced in more complex datasets with more classes. 

\section{Additional VLMs}
\label{app:additional-VLMs}

In this section, we provide a more detailed explanation of the SigLIP and SLIP models, highlighting their key differences from CLIP.

\minisection{SigLIP.} In SigLIP \citep{zhai2023sigmoid}, given a batch of image-caption pairs $ B = \{ (I_i, Y_i) \}_{i=1}^N $, training maximizes the cosine similarity for the $N$ correct pairs and minimizes it for the $ N^2 - N $ incorrect pairs. Unlike the softmax-based contrastive loss from \cref{eq:clip_loss} used in CLIP, SigLIP employs a sigmoid-based loss that avoids global normalization factors. Each image-text pair is processed independently, transforming the learning task into a binary classification problem across all pair combinations. The matching pair $ (I_i, Y_i) $ receives a positive label, while all other pairs $ (I_i, Y_{j \neq i}) $ receive negative labels.
SigLIP consists of an image encoder $ f_{\theta} $ and a text encoder $ g_{\phi} $. We denote the image and text embeddings as $ \psi_I^i = f_{\theta}(I_i) $ and $ \psi_T^i = g_{\phi}(Y_i) $, respectively. The loss employed by SigLIP is:
\begin{equation}\label{eq:siglip_loss}
   \mathcal{L}_{\text{SigLIP}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{N} \log \left( \frac{1}{1 + e^{z_{ij} (- c(\psi_I^i, \psi_T^j)/ \tau + b)}} \right),
\end{equation}
where $ c(\cdot, \cdot) $ denotes the cosine similarity, $ \tau $ is a learnable temperature parameter, $ b $ is a learnable bias, and $z_{ij}$ is the label for a given image and text input ($z_{ij} = 1$ if $i=j$ and $z_{ij} = -1$ otherwise).
Similar to CLIP, SigLIP's loss does not include explicit intra-modal constraints; the loss focuses solely on inter-modal alignment between image and text embeddings, without directly enforcing intra-modal alignment.

\minisection{SLIP.} SLIP \citep{mu2022slip} is a VLM trained with both language supervision and image self-supervision. Its loss function combines two components: the inter-modal loss used in CLIP (\cref{eq:clip_loss}), and an intra-modal self-supervised term. For the self-supervised component, SLIP uses an adaptation of SimCLR \citep{chen2020simple}. This loss, denoted as $\mathcal{L}_{\text{SimCLR}}$, maximizes features similarities between two different views (\ie augmentation) of the same image:
\vspace{0.1cm}
\begin{equation}
    \mathcal{L}_{\text{SimCLR}} = -\frac{1}{2N} \sum_{i=1}^{2N} \log \frac{\exp\left(\text{c}(\psi_I^{p(i)}, \psi_I^{q(i)}) / \tau\right)}{\sum_{k=1, k \neq i}^{2N} \exp\left(\text{c}(\psi_I^i, \psi_I^k) / \tau\right)},
\end{equation}
\vspace{0.1cm}
where $\psi_I^j$ indicates the image embedding of a sample $j$, $p(i)$ and $q(i)$ are two augmented views of the image $i$, $ c(\cdot, \cdot) $ denotes the cosine similarity, and $\tau$ is a temperature parameter.

The final loss used in SLIP is a combination of CLIP and self-supervised losses:
\vspace{0.1cm}
\begin{equation}
    \mathcal{L}_{\text{SLIP}} = \mathcal{L}_{\text{CLIP}} + \mathcal{L}_{\text{SimCLR}}.
\end{equation}
\vspace{0.1cm}
By incorporating the intra-modal self-supervised loss, SLIP encourages better intra-modal alignment within the image embedding space. We confirm this empirically in~\cref{tab:slip_image_retrieval}.

\section{Selecting the number of pseudo-patches for OVI}
\label{app:abl_num_tokens}

\begin{wraptable}{r}{0.5\textwidth}
    \vspace{-0.19in}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{4pt}
    \caption{Ablation on the number of \purpletext{OVI} pseudo-patches for text-to-text retrieval on the Flickr30K validation set. The highest mAP score in each row is highlighted in bold, with the corresponding value of $P$ representing the number of pseudo-patches used in the experiments.}
    \label{tab:text_to_text_retrieval}
    \begin{tabular}{cc c ccccc}
    \toprule
     & &  & \multicolumn{5}{c}{Number of Pseudo-Patches $P$} \\ \cmidrule(lr){4-8}
     VLM & Backbone & Intra-modal & 1 & 2 & 4 & 8 & 16 \\
    \midrule
    \multirow{2}{*}{CLIP} & B/32 & 51.4 & \textbf{54.5} &  52.9 & 51.8 & 51.6 & 51.6 \\
    & L/14 & 52.6 & 51.7 & 55.2 & \textbf{56.0} & 55.3 & 54.1 \\
    \midrule 
    \multirow{2}{*}{OPEN} & B/32 & 57.3 & \textbf{59.6} & 57.9 & 57.5 & 57.4 & 57.4 \\
    & L/14 & 59.6 & 60.6 & \textbf{62.5} & 62.4 & 61.2 & 60.4 \\
    \midrule 
    SigLIP & B/16 & 56.3 & 45.2 & 58.0 & \textbf{60.1} & 59.9 & 59.4 \\
    \midrule 
    \multirow{2}{*}{SLIP} & B/16 & 45.8 & \textbf{46.4} & 46.4 &  46.1 & 45.9 & 45.9 \\
    & L/16 & 49.8 & 48.9 & \textbf{50.0} & 49.8 & 49.9 & 49.8 \\
    \bottomrule
    \end{tabular}
\end{wraptable}

In \cref{sec:modality_drift}, we observe that for certain experiments a single pseudo-patch (\ie $P=1$) is not enough to encapsulate the informative content of the associated text. 
To determine the optimal number of pseudo-patches for each VLM, we conducted a text-to-text retrieval experiment on the Flickr30K~\citep{plummer2015flickr30k} validation set, varying the number of $P$ from 1 to 16.

Table \ref{tab:text_to_text_retrieval} presents the results of this ablation. We observe that the ideal number of pseudo-patches changes depending on the model. In particular, larger VLMs -- with a greater number of input patches $U$ -- tend to require more pseudo-patches. We hypothesize that this is because, as the number of patches increases, the influence of a single pseudo-patch decreases, necessitating a larger number to capture sufficient information. 

\section{Different VLM, Different Modality Gap}
\label{app:different-VLM-different-gap}
In \cref{sec:modality_gap} we show that in the absence of the modality gap, tackling intra-modal tasks inter-modally does not improve performance. This demonstrates that closing the modality gap helps reduce intra-modal misalignment.

Following \citet{liang2022mind} we define the modality gap as the difference between the two centroids of the image and text modality embeddings: 
\begin{equation}\label{eq:modality_gap}
    \Delta_{\text{gap}} = \frac{1}{N} \sum_{i=1}^{N} x_i - \frac{1}{N} \sum_{i=1}^{N} y_i,
\end{equation}
where $x_i$ and $y_i$ are the L2-normalized image and text embeddings, and $N$ is the number of pairs.

\begin{wraptable}{r}{0.5\textwidth} 
    \vspace{-0.5cm}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{6pt} 
    \caption{$\lVert \Delta_{\text{gap}} \rVert$ for different VLMs on COCO.}
    \label{tab:modality_gap}
    \begin{tabular}{cc c c c}
    \toprule
     VLM & Backbone & \shortstack{Loss} & $\lVert \Delta_{\text{gap}} \rVert$ \\
    \midrule
    
    \multirow{2}{*}{CLIP} & B/32 &  \multirow{2}{*}{$\mathcal{L}_\text{CLIP}$} & 0.82\\
    & L/14 &  & 0.82\\
    \midrule 
    
    \multirow{2}{*}{OPEN} & B/32 & \multirow{2}{*}{$\mathcal{L}_\text{CLIP}$} & 0.82\\
    & L/14 &  & 0.80\\
    \midrule 
    
    SigLIP & B/16 &  $\mathcal{L}_\text{SigLIP}$ & \hred{1.05}\\
    \midrule 
    
    \multirow{2}{*}{SLIP} & B16 &  \multirow{2}{*}{{$\mathcal{L}_\text{CLIP} + \mathcal{L}_\text{SimCLR}$}} & 0.57\\
    & L/16 &  & 0.49\\

    \midrule 
    
    Fine-tuned & \multirow{2}{*}{B/32} & $\mathcal{L}_\text{CLIP} (\tau=1)$ & \hgreen{0.007}\\
    CLIP & & $\mathcal{L}_\text{CLIP} (\tau=0.01)$  & 0.88\\
 
    \bottomrule
    \end{tabular}
    \vspace{-0.1in}
\end{wraptable}
To facilitate a clearer comparison across different VLMs, in Table \ref{tab:modality_gap} we report the magnitude of the modality gaps evaluated on the COCO validation split. 
We observe that integrating an intra-modal constraint (\eg SLIP) or using a higher temperature in the contrastive loss (\eg our fine-tuned model with temperature $\tau = 1$) helps reduce or even eliminate the modality gap.
By analyzing \cref{tab:image_retrieval,tab:text_retrieval,tab:slip_image_retrieval,tab:clip_gapimage_retrieval}, we confirm our hypothesis that exists a positive correlation between the magnitude of the modality gap and the improvement in approaching intra-modal tasks inter-modally using OTI (or OVI).

\section{Dataset Details}
\label{app:datasets}

Our experimental evaluation is performed on 18 datasets. Here we report all the evaluated splits and details of the datasets used in our experiments. 

\minisection{Zero-shot Image Classification.} Following \citet{zhou2022learning}, we validate our zero-shot image classification experiments on 11 publicly available datasets with diverse characteristics: ImageNet~\citep{deng2009imagenet} for large-scale object classification; Caltech101~\citep{fei2004learning} for general object classification; EuroSAT~\citep{helber2019eurosat} for satellite image recognition; Food101~\citep{bossard2014food}, FGVCAircraft~\citep{maji2013fine}, OxfordPets~\citep{parkhi2012cats}, Flowers102~\citep{nilsback2008automated}, and StanfordCars~\citep{krause20133d} for fine-grained classification; UCF101~\citep{soomro2012ucf101} for action recognition; and the Describable Textures Dataset (DTD)~\citep{cimpoi2014describing} for texture classification.
Following \citet{zhou2022learning}, we discard the ``BACKGROUND Google" and ``Faces easy" classes from Caltech101. For UCF101 -- a video dataset -- we follow \citet{radford2021learning} and use the middle frame of each video clip as the input image.
In all classification experiments, we report the accuracy results on the test set. 

\minisection{Image-to-Image Retrieval.} 
For image-to-image retrieval experiments, we use the 11 datasets also employed for zero-shot image classification and four widely used datasets commonly used for metric learning and image retrieval: CUB-200-2011 (CUB)~\citep{wah2011caltech}, Stanford Online Products (SOP)~\citep{oh2016deep}, $\mathcal{R}$Oxford~\citep{radenovic2018revisiting}, and $\mathcal{R}$Paris~\citep{radenovic2018revisiting}, for a total of 15 datasets.
In the 11 datasets used for zero-shot image classification, we use the test set as the query set and the training set as the gallery.
For CUB, the entire dataset is used as both the query and gallery sets. In SOP, both the query and gallery sets are taken from the test set.

In all experiments involving $\mathcal{R}$Oxford and $\mathcal{R}$Paris, we follow the standard benchmark and include the $\mathcal{R}$1M distractor set, containing 1 million images, as negative samples for all the queries. For brevity in the paper we report only the metric calculated on the Easy setting~\cite{radenovic2018revisiting}.
For image-to-image retrieval evaluation, we use the standard mean Average Precision (mAP) metric.  Importantly, by using the same 11 datasets employed for zero-shot classification, we can evaluate the performance of the \textit{same} OTI-inverted features on both inter-modal zero-shot classification and intra-modal image retrieval tasks

\minisection{Text-to-Text Retrieval.} We performed our text-to-text retrieval experiments using three image-caption datasets: COCO~\citep{lin2014microsoft}, Flickr30K~\citep{plummer2015flickr30k}, and nocaps~\citep{agrawal2019nocaps}. We selected these datasets for two reasons: they contain short, descriptive text similar to the ones used to train VLMs, and they provide multiple captions for each image. In our evaluation, we use the first caption of each image as the query and aim to retrieve the other captions associated with the same image from a gallery of all captions in the dataset. On average, COCO and Flickr30K images have 5 captions each, while nocaps images have 10. We use the Karpathy split~\citep{karpathy2015deep} for both COCO and Flickr30K and report results using captions from the test split. For nocaps, we report results on the validation split. Although these datasets contain images associated with captions, we ignore the images in this setting.
We use mAP as the evaluation metric similar to the image retrieval experiments.

\section{Additional Experiments}
\label{app:additional-experiments}

Here we report additional experiments to support our claims about the importance of approaching tasks inter-modally when using constrastively trained VLMs.

\minisection{Zero-shot Image Classification with OVI.}
Due to space limitation, in \cref{sec:zero_shot_image_classification}, we provide a brief overview of how CLIP-like models can perform zero-shot image classification. Here, we offer a more detailed explanation for clarity.
Given an image $I$ and a set of textual prompts $\mathcal{Y}=\{Y_i\,|\,i=1,\ldots,C \}$, where $C$ is the number of classes, each text prompt $Y_i$ is formatted as: "a photo of a [$\text{CLASS}_i$]", with $\text{CLASS}_i$ representing a specific class name, such as "cat" or "dog".  Let the image features be denoted as $\psi_I = f_{\theta}(I)$ and the text features for each prompt by $\psi_T^i = g_{\phi}(Y_i)$. The probability of the image belonging to each class is then given by:
\begin{equation} \label{eq:class_prob}
 p(y=i|I) = \frac{\exp( c(\psi_T^i, \psi_I)/\tau) }{\sum_{j=1}^C \exp( c(\psi_T^j, \psi_I)/\tau)},
\end{equation}
where $c(\cdot, \cdot)$ denotes cosine similarity and $\tau$ is a temperature parameter.

In \cref{sec:zero_shot_image_classification}, we transform zero-shot image classification from being natively inter-modal to intra-modal by applying OTI to the input image $I$ (see the right section of \cref{tab:zeroshot_classification}). As expected, this consistently leads to performance degradation for different VLMs and backbones, demonstrating that modality inversion does not inherently improve performance. Similarly, we perform an experiment where we approach the zero-shot classification task \textit{intra-modally} by applying OVI to each textual prompt, with results reported in \cref{tab:zeroshot_classification_ovi}. Consistent with our previous findings, we observe that approaching classification intra-modally hinders the performance.

\input{tables/ovi_classification}

\input{tables/only_image-text_text-image}

\minisection{Image-Text Retrieval with OTI and OVI.}
To provide additional experimental evidence that transforming inter-modal tasks in intra-modal ones hinders performance, we conduct an experiment on image-text retrieval using the COCO~\citep{lin2014microsoft} and Flickr30K~\citep{plummer2015flickr30k} datasets.  We use the Karpathy splits~\cite{karpathy2015deep} for both datasets and report results on the test split. Following standard the evaluation benchmark, we report Recall$@K$ scores with $K=1, 5,$ and $10$. 
Specifically, in image-to-text retrieval, we apply OTI to the query image, while in text-to-image retrieval, we apply OVI to the query text. We then compare the results with the inter-modal baseline, which directly compares image and text features.
Results in \cref{tab:image_text_image} confirm our findings from the zero-shot image classification task: transforming an inter-modal task into an intra-modal one leads to performance degradation due to intra-modal misalignment.

\minisection{Text-to-text Retrieval on Purely Textual Datasets.}
In \cref{sec:text_to_text_retrieval} we conduct a text-to-text retrieval experiment using image captioning datasets to avoid a mismatch with VLMs pre-training data.
In this section, we evaluate the performance of OVI on purely textual datasets using the CLIP ViT B/32 model.
Specifically, we select seven datasets from the NanoBEIR benchmark\footnote{\href{https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6}{https://huggingface.co/collections/zeta-alpha-ai/nanobeir}} spanning diverse domains such as scientific documents (SciDOCS) and climate-related texts (ClimateFEVER). We discard Question-Answering (QA) datasets and those with queries whose average length exceeds the context length of CLIP's text encoder (77 tokens). Additionally, we include the IMDB Reviews \citep{maas2011learning} and the 20 Newsgroups \citep{lang1995newsweeder} datasets. 
\input{tables/textual_summarization}

All selected datasets comprise texts that cannot be easily represented visually. Examples include ``Learning Actionable Representations with Goal-Conditioned Policies'' (SciDocs), ``Atheism, philosophy, and the absence of belief in deities'' (20 Newsgroup), and ``The carbon footprint on wind energy is significant'' (ClimateFEVER). Since gallery texts often exceed CLIP's context length, we employ the Llama-3.2-1B-Instruct\footnote{\href{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}} Large Language Model \citep{dubey2024llama} to summarize them to fit within the token limit.

We report the results in \cref{tab:summarizations}. OVI achieves a significant performance improvement over the intra-modal baseline. This outcome demonstrates that OVI is effective even when considering texts that can not be easily represented visually.

\input{tables/adapters}

\minisection{From Intra-modal to Inter-modal via Adapters.}
To broaden our comparative analysis we conduct an additional experiment where we train two single-layer linear adapters: one maps image features to text features (aligned with the goal of OTI), and the other maps text features to image features (aligned with the goal of OVI). For training, we leverage the LLaVA-CC3M\footnote{\href{https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K}{https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K}} dataset \citep{liu2024visual}, which comprises 595K image-text pairs. This dataset is derived by filtering the CC3M dataset \citep{sharma2018conceptual} to achieve a more balanced distribution of concept coverage. We train each adapter using a cosine loss that minimizes the distance between the adapter output and the corresponding complementary features. Additionally, following \citet{patel2024eclipse}, we also employ a CLIP-based contrastive loss component. 

\Cref{tab:adapters} presents the results for image-to-image and text-to-text retrieval tasks using the CLIP ViT-B/32 model. The adapter-based approach improves performance over the intra-modal baseline for both tasks. These findings support our claim that approaching the task \textit{inter-modally} enhances performance thanks to CLIP’s inherent inter-modal alignment. Interestingly, we observe that OTI and OVI outperform the adapter-based approach in most scenarios. This result emphasizes the effectiveness of OTI and OVI, as they do not require a training dataset but rather map individual features directly to the complementary modality without relying on external resources.

\minisection{From Intra-modal to Inter-modal via Captioning.}
We compare the performance of OTI on image-to-image retrieval with a captioning-based approach. 
Specifically, given a query image, we first generate the caption using a pre-trained captioning model, then use CLIP's text encoder to extract the text features to perform retrieval.

We experiment with three pre-trained captioning models: 
DeCap \citep{li2023decap}, which directly generates captions from CLIP image features; 
CoCa (LAION)\footnote{\href{https://huggingface.co/laion/CoCa-ViT-B-32-laion2B-s13B-b90k}{https://huggingface.co/laion/CoCa-ViT-B-32-laion2B}} \citep{yucoca}, trained on the Laion2B \citep{schuhmann2022laion} dataset; and 
CoCa (COCO)\footnote{\href{https://huggingface.co/laion/mscoco_finetuned_CoCa-ViT-B-32-laion2B-s13B-b90k}{https://huggingface.co/laion/mscoco\_finetuned\_CoCa-ViT-B-32}}, pre-trained on Laion2B and fine-tuned on COCO \citep{lin2014microsoft}.



\begin{wrapfigure}{r}{0.42\textwidth}
    \centering
    \vspace{-0.45cm}
    \includegraphics[width=0.95\linewidth]{images/all_souls_generated_portrait.pdf}
    \vspace{-5pt}
    \caption{Captions generated by pre-trained captioning models for an image from the $\mathcal{R}$Oxford dataset.}
    \label{fig:generated-captions}
    \vspace{-0.3cm}
\end{wrapfigure}
\Cref{tab:captioners} shows the results using the CLIP ViT-B/32 model. Regardless of the captioning model, the captioning-based approaches achieve unsatisfactory performance, even falling short of the intra-modal baseline despite leveraging CLIP's inter-modal alignment. 
This outcome stems from the fact that the generated captions are not discriminative enough to perform image retrieval. This is particularly evident in fine-grained domains such as the buildings of the $\mathcal{R}$Oxford and $\mathcal{R}$Paris datasets \citep{radenovic2018revisiting}. 
\Cref{fig:generated-captions} shows an example of generated captions for a randomly chosen image from the $\mathcal{R}$Oxford dataset. We observe that all the captioning models generate generic and not sufficiently discriminative captions. CoCa (LAION) produces a more precise description than the other models, reflecting its higher performance. 
\input{tables/captioner}

\minisection{Intra-OTI Similarity Comparisons.}
To further support our claim that the performance improvement in image-to-image retrieval stems from CLIP's inter-modal alignment and not from the modality inversion itself, we perform an experiment where we apply OTI to both query and gallery images.
Since OTI maps image features into text features, this intra-OTI strategy involves intra-modal similarity comparisons within the text embedding space. 

\input{tables/intra_oti}
\Cref{tab:intra-oti} shows the results on image retrieval datasets using the CLIP ViT-B/32 model. 
We observe that employing inter-modal similarity comparisons by applying OTI only to the query images achieves better performance than using intra-modal similarities with the intra-OTI approach. This confirms that modality inversion techniques do not inherently improve performance. 
Instead, their effectiveness lies in leveraging CLIP's inter-modal alignment by transforming intra-modal tasks into inter-modal ones.

\minisection{Impact of the OTI Template Sentence.} As detailed in \cref{sec:oti_method}, for OTI we concatenate the template sentence ``a photo of'' with the pseudo-token $v^*$ to craft the prompt ``a photo of $v^*$''.
\input{tables/template}
To study the impact of the template sentence on the performance, we test the following prompts: 1) ``an image of $v^*$''; 2) ``we see $v^*$ in this photo''; and 3) ``$v^*$'' (the \textit{empty prompt}).
 
\Cref{tab:template} reports the image-to-image retrieval results using the CLIP ViT-B/32 model. We observe that all
the considered prompts achieve comparable performance. These results demonstrate the robustness of the OTI technique to the template sentence.
% Regardless of the template sentence, OTI features consistently outperform the intra-modal baseline.

\minisection{Combining Native and OTI-Inverted Features.}
We conduct an experiment on image-to-image retrieval to assess whether combining native image features with the corresponding OTI-inverted features improves the performance. Let $\psi_I = f_{\theta}(I)$ be the native image features and $\psi_T = g_{\phi}(\overline{Y}_{v^*})$ be the OTI-inverted features. We query the gallery using a weighted combination of native and OTI-inverted representations: 
\begin{equation}
    \psi_{IT} = \alpha * \psi_T + (1 - \alpha) * \psi_I,
\end{equation}
where $\alpha \in [0, 1]$ is a weighting factor that controls the contribution of each component.

\input{tables/OTI_fusion}
\Cref{tab:oti-fusion} reports the results on image-to-image retrieval datasets for varying values of $\alpha$ using the CLIP ViT-B/32 model. Interestingly, for $\alpha$ large enough, combining native and inverted features obtains better results than relying solely on either of them. 
Notably, regardless of the $\alpha$ value, we observe that employing OTI-inverted features always improves the performance over the intra-modal baseline.
We leave further investigation of the combination of native and OTI-inverted features to future work.

% A thorough study of this phenomenon is beyond the scope of this work, and it is worthy of further exploration in future work. 
% These results are very interesting (and surprising). Combining $\psi_I$ and $\psi_T$ consistently improves performance across all image retrieval datasets compared to the intra-modal baseline. Notably, when $\alpha$ is set to 0.5 and 0.75, the combination even outperforms OTI features on 4 out of 5 image retrieval datasets. The best average performance is achieved with $\alpha = 0.75$. This insight certainly warrants further exploration and investigation, for example on how to select $\alpha$ at test time, and we will add elements of this analysis to the final revision of the manuscript.

\end{appendices}


\end{document}