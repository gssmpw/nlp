@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International conference on machine learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

@article{li2021supervision,
  title={Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm},
  author={Li, Yangguang and Liang, Feng and Zhao, Lichen and Cui, Yufeng and Ouyang, Wanli and Shao, Jing and Yu, Fengwei and Yan, Junjie},
  journal={arXiv preprint arXiv:2110.05208},
  year={2021}
}

@article{liang2022mind,
  title={{Mind the Gap: Understanding the Modality Gap in Multi-Modal Contrastive Representation Learning}},
  author={Liang, Victor Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James Y},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17612--17625},
  year={2022}
}

@inproceedings{mu2022slip,
  title={Slip: Self-supervision meets language-image pre-training},
  author={Mu, Norman and Kirillov, Alexander and Wagner, David and Xie, Saining},
  booktitle={European conference on computer vision},
  pages={529--544},
  year={2022},
  organization={Springer}
}

@inproceedings{radford2021learning,
  title={{Learning Transferable Visual Models From Natural Language Supervision}},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{schrodi2024two,
  title={{Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning}},
  author={Schrodi, Simon and Hoffmann, David T and Argus, Max and Fischer, Volker and Brox, Thomas},
  journal={arXiv preprint arXiv:2404.07983},
  year={2024}
}

@inproceedings{shi2023towards,
  title={{Towards Understanding the Modality Gap in CLIP}},
  author={Shi, Peiyang and Welle, Michael C and Bj{\"o}rkman, M{\aa}rten and Kragic, Danica},
  booktitle={ICLR 2023 Workshop on Multimodal Representation Learning: Perks and Pitfalls},
  year={2023}
}

@inproceedings{udandarao2023sus,
  title={{SuS-X: Training-Free Name-Only Transfer of Vision-Language Models}},
  author={Udandarao, Vishaal and Gupta, Ankush and Albanie, Samuel},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2725--2736},
  year={2023}
}

@inproceedings{yi2024leveraging,
  title={{Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification}},
  author={Yi, Chao and Ren, Lu and Zhan, De-Chuan and Ye, Han-Jia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={27402--27411},
  year={2024}
}

@inproceedings{zhai2022lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={18123--18133},
  year={2022}
}

@inproceedings{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11975--11986},
  year={2023}
}

