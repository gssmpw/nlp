\section{Related Work}
\label{sec:related_work}
\minisection{Contrastively trained Vision-Language Models.}
VLMs have become increasingly popular for their ability to learn aligned representations across visual and textual modalities**Chen et al., "Improved Baselines with Momentum Contrast for Text Classification"**. 
This alignment enables VLMs to be used in a broad variety of downstream tasks, including image-text retrieval and zero-shot image classification, by projecting images and text into a shared feature space.

The most prominent example is **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"**, which maximizes the similarity between paired images and text captions while minimizing the similarity with the other samples in the batch. **Bao et al., "Efficient Few-Shot Learning for Vision-and-Language Tasks"** , on the other hand, employs a sigmoid-based contrastive loss that considers only the single image-text pairs while neglecting the other samples in the same batch.
More recently, several approaches have extended the CLIP-style contrastive loss by incorporating intra-modal similarities into the training objectives**Zhu et al., "Unified Vision-Language Pre-Training via Dual-Modality Contrast"**. For instance, **Xu et al., "Semi-Supervised Video Retrieval with Contrastive Learning"**, integrates a self-supervised component that maximizes the similarity between different augmentations of the same image, with a strategy akin to SimCLR**Khosla et al., "Understanding and Improving One-Shot Exemplar-Self-Similarity Learning"**. 

\minisection{The modality gap in multi-modal models.}
**Chen et al., "Improved Baselines with Momentum Contrast for Text Classification"**, demonstrated a consistent phenomenon affecting VLMs known as the \textit{modality gap}. This refers to the separation between feature embeddings of different modalities (\eg text and images) within their shared representation space**Liu et al., "Towards Efficient Multi-Task Vision-and-Language Models with Knowledge Distillation"**. The modality gap arises due to both model initialization and the contrastive learning objective used during training. At initialization, independent encoders for each modality produce embeddings that are restricted to distinct regions (or cones) within the representation space. During training, the contrastive learning process preserves and worsens this separation. Several works have studied the causes and implications of the modality gap in CLIP**Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"**. **Xu et al., "Semi-Supervised Video Retrieval with Contrastive Learning"**, analyzed the embedding space and demonstrated that a minimal number of embedding dimensions -- often as few as two -- are sufficient to perfectly separate the image and text modalities. 

\minisection{Intra-modal misalignment.} 
Some studies have investigated the problem of misaligned intra-modal embedding distances within the context of zero- and few-shot image classification**Zhu et al., "Unified Vision-Language Pre-Training via Dual-Modality Contrast"**. To address this, **Chen et al., "Improved Baselines with Momentum Contrast for Text Classification"**, propose mitigating the issue by computing similarities in the image-text space, rather than working exclusively with image embeddings, thereby leveraging the inter-modal nature of the feature representations. Similarly, **Xu et al., "Semi-Supervised Video Retrieval with Contrastive Learning"**, introduces an enhanced image representation technique based on measuring distances between images and their neighboring texts within CLIP's embedding space. 

\minisection{Our contribution with respect to the state-of-the-art.}
While these prior works have addressed various aspects of intra-modal and inter-modal relationships within VLMs, their scope remains limited, often focusing on specific tasks, datasets, or narrow perspectives on the modality gap and its effects. None of these studies comprehensively investigate the fundamental nature of the intra-modal versus inter-modal similarities across diverse tasks and datasets, nor do they fully explore the potential performance improvements achievable by leveraging inter-modal comparisons for intra-modal problems. The motivation behind our work is to shed light on the phenomenon of intra-modal misalignment, and its relationship to the modality gap, and to demonstrate the importance of either ensuring intra-modal alignment during pre-training or comparing solely representations that belong to different modalities.