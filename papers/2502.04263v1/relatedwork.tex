\section{Related Work}
\label{sec:related_work}
\minisection{Contrastively trained Vision-Language Models.}
VLMs have become increasingly popular for their ability to learn aligned representations across visual and textual modalities~\citep{radford2021learning, jia2021scaling, zhai2022lit, zhai2023sigmoid, mu2022slip, li2021supervision}. 
This alignment enables VLMs to be used in a broad variety of downstream tasks, including image-text retrieval and zero-shot image classification, by projecting images and text into a shared feature space.

The most prominent example is CLIP~\citep{radford2021learning}, which maximizes the similarity between paired images and text captions while minimizing the similarity with the other samples in the batch. SigLIP~\citep{zhai2023sigmoid}, on the other hand, employs a sigmoid-based contrastive loss that considers only the single image-text pairs while neglecting the other samples in the same batch.
More recently, several approaches have extended the CLIP-style contrastive loss by incorporating intra-modal similarities into the training objectives~\citep{mu2022slip, li2021supervision}. For instance, SLIP \citep{mu2022slip} integrates a self-supervised component that maximizes the similarity between different augmentations of the same image, with a strategy akin to SimCLR~\citep{chen2020simple}. 

\minisection{The modality gap in multi-modal models.}
\citet{liang2022mind} demonstrated a consistent phenomenon affecting VLMs known as the \textit{modality gap}. This refers to the separation between feature embeddings of different modalities (\eg text and images) within their shared representation space~\citep{liang2022mind}. The modality gap arises due to both model initialization and the contrastive learning objective used during training. At initialization, independent encoders for each modality produce embeddings that are restricted to distinct regions (or cones) within the representation space. During training, the contrastive learning process preserves and worsens this separation. Several works have studied the causes and implications of the modality gap in CLIP~\citep{ shi2023towards, schrodi2024two, zhang2023diagnosing}. \citet{schrodi2024two} analyzed the embedding space and demonstrated that a minimal number of embedding dimensions -- often as few as two -- are sufficient to perfectly separate the image and text modalities. 

\minisection{Intra-modal misalignment.} 
Some studies have investigated the problem of misaligned intra-modal embedding distances within the context of zero- and few-shot image classification~\citep{udandarao2023sus, yi2024leveraging}. To address this, \citet{udandarao2023sus} propose mitigating the issue by computing similarities in the image-text space, rather than working exclusively with image embeddings, thereby leveraging the inter-modal nature of the feature representations. Similarly, CODER~\citep{yi2024leveraging} introduces an enhanced image representation technique based on measuring distances between images and their neighboring texts within CLIP's embedding space. 

\minisection{Our contribution with respect to the state-of-the-art.}
While these prior works have addressed various aspects of intra-modal and inter-modal relationships within VLMs, their scope remains limited, often focusing on specific tasks, datasets, or narrow perspectives on the modality gap and its effects. None of these studies comprehensively investigate the fundamental nature of the intra-modal versus inter-modal similarities across diverse tasks and datasets, nor do they fully explore the potential performance improvements achievable by leveraging inter-modal comparisons for intra-modal problems. The motivation behind our work is to shed light on the phenomenon of intra-modal misalignment, and its relationship to the modality gap, and to demonstrate the importance of either ensuring intra-modal alignment during pre-training or comparing solely representations that belong to different modalities.