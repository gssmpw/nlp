\section{\system{}}
\label{sec:respark}

This section describes the implementation of \system{}, including pre-processing, analysis, and organization. 
\system{} utilizes the GPTs from Azure to incorporate the LLM-driven functionalities. 
Specifically, we employ the ``gpt-4-vision-preview'' model as our input involves chart images. 
The prompts are provided in the supplementary materials. 

\subsection{Pre-processing Stage}

Before proceeding with analysis and organization, we need to pre-process the user dataset and reports to (1) acquire necessary data features, (2) recommend the most relevant reports to the data, and (3) extract the analytical objective and their dependencies of the selected report for the subsequent processes. 

\subsubsection{Data Pre-processing}
\label{subsubsec:data_pre_processing}

Based on the findings in the preliminary study (~\autoref{subsec:preliminary_study}), most adjustments entail considerations of data features such as context, scope, fields, and formats. 
Presenting the entire dataset to LLMs is currently impractical due to token limitations, and it does not facilitate a comprehensive understanding of these data features. 
Therefore, we utilize a similar data summary method to LIDA~\cite{dibia2023lida}. 
This method first extracts scope, data type, and unique value count information and samples some values for each data field. 
Subsequently, it employs LLMs to provide brief semantic descriptions for the dataset and each data field. 
The description of the dataset can also help recommend relevant reports(~\autoref{subsubsec:report_retrieval}). 
These pieces of information are then integrated to form a comprehensive data summary.

\subsubsection{Report Pre-processing}
\label{subsubsec:report_pre_processing}

\begin{figure*}[!htb] 
  \centering
  \includegraphics[width=\linewidth]{figs/system.png}
  \caption{
  The interface of \system{}. 
  \system{} consists of four views: data view (b-c), dependency view (d-e), content view (f-g), and generation view (h-k). 
  The data view displays the overall description and data field information. 
  The dependency view displays the extracted interdependent report segments. 
  The content view shows the analytical objective and content of the selected segment. 
  The generation view demonstrates the generated results in real-time. 
  }
  \label{fig:interface}
\end{figure*}

Based on the analysis workflow formulation in~\autoref{subsec:problem_formulation}, our goal is to deduce the analysis segments and their dependencies from the original report for subsequent execution. 
Our preliminary study showed that most reports present analytical content in distinct segments, each focused on a single objective, with related text and visuals grouped together. 
Therefore, ideally, we can find a segmentation that aligns each report section with a specific analysis segment. 
In this light, \system{} should segment the report accordingly, extract the analytical objectives for each segment, and deduce their dependencies.

To achieve this goal, an appropriate report segmentation criteria is very important, as it directly determines the entire analysis workflow (consisting of a sequence of analysis segments). 
The accuracy of segmentation also affects the quality of the extracted analytical objectives and dependencies.

We were initially inspired by prior work in automatic storytelling and insight-mining, which formalizes data insights and their relationships~\cite{ma2023insightpilot, wang2019datashot}. 
For example, Calliope~\cite{shi2020calliope} defines a data story as a series of interrelated insights, with each insight describing data patterns in specific data fields and subsets. 
For instance, ``The average worldwide gross for action movies is increasing over time'' describes an increasing trend measuring ``average (worldwide gross)'' over the breakdown ``year'' within the subset ``genre = action''. 
Based on this definition, we can potentially segment the report by identifying the insight type, measure, breakdown, subset, etc., and combine the insights into segments based on these labels. 
However, we found this definition challenging for segmenting practical data reports, as it doesn't accommodate the flexibility of analyses that involve deeper data transformations, such as creating new fields and describing patterns in derived variables.

Therefore, we need to define new segmentation criteria that accommodate the flexible analysis in the data report. 
Instead of formally defining a ``segment'' or an ``analytical objective'' with a strict data model, we provide a loose description of how segments can be divided and use LLMs to perform segmentation. 
Our preliminary study indicated that most report segments consist of continuous text and possibly a chart.
Based on the study, we execute report segmentation, extract analytical objectives, and deduce the dependencies between segments through the following approach: 
\begin{itemize}
    \item \textbf{Match. } 
    First, for each chart, we match the related paragraph text to form a segment. 
    Based on our preliminary study, we assume that (1) each paragraph corresponds to the nearest preceding or following chart, or none at all, and (2) all text associated with a single chart is continuous. 
    Starting with the first paragraph, LLMs determine whether it matches the nearest preceding or following chart (e.g., describing insights from the chart) or if it doesn't relate to any chart.
    
    \item \textbf{Categorize. } 
    For text that doesn't match a chart, LLMs categorize it to determine if it involves data analysis or serves another purpose, such as providing background information. 
    For continuous text segments that involve data analysis, we further assess whether they belong to the same segment (describe insights derived from the same transformed data). 
    
    \item \textbf{Summarize. } 
    After matching and categorizing, LLMs summarize the analytical objective of each segment and deduce its dependencies with previous segments. 
    We use the six logical relations defined in Calliope to outline dependencies among report segments. 
    LLMs determine whether new content is logically connected to an existing segment or originates directly from the data.
\end{itemize}


\subsubsection{Relevant Report Retrieval}
\label{subsubsec:report_retrieval}


With the pre-processed data and reports, users can select a reference report to analyze the target dataset. 
Based on findings from our preliminary study, various aspects of existing data reports, such as analytical objectives and report content, can serve as helpful reference material. 
However, since the reference report's data may differ from the target dataset, adjustments are necessary to align with the new dataset. 
The closer the reference report's data is to the target dataset, the more aspects can be reused without modification, making the report more suitable for use as a reference.


To facilitate the retrieval of suitable reports, we aim to identify the reports with data similar to the target dataset. 
The core idea is to convert both the dataset and report information into vector embeddings, compute their cosine similarities, and rank the reports from highest to lowest score.
The key question is: which specific information from the dataset and reports should be embedded?
We propose two mechanisms for extracting the embedding of data and report information: topic relevance and field similarity.
\begin{itemize}
    \item \textbf{Topic relevance} refers to the alignment between the topic of the dataset and the report. 
    Typically, datasets and reports within the same domain (e.g., health, economy) exhibit higher topic relevance. 
    For example, a sales dataset is highly topic-relevant to a report analyzing market sales trends. 
    To compute topic relevance, we extract the embedding of the datasetâ€™s name and description, along with the headings and pre-processed analytical objectives of the report. 
    We hypothesize that these elements are semantically related to the corpus's overall topic, and the cosine similarity of their embeddings can reflect their topic relevance.
    \item \textbf{Field similarity} pertains to the alignment of the data fields described in the report with those contained in the dataset. 
    For example, a report on voting intentions across different gender and age groups would exhibit higher field similarity with a dataset containing gender and age information. 
    To compute field similarity, we embed the names and descriptions of the dataset's fields. 
    For the reports, we use LLMs to infer the data fields discussed in the report and embed these inferred fields along with their descriptions. 
    We hypothesize that the cosine similarity between these reflects the degree of field similarity.
\end{itemize}
Finally, we sum the scores from both mechanisms for each report, ranking them from highest to lowest, allowing users to select the most appropriate reference reports.

\subsection{Analysis Stage}
\label{subsec:analysis_stage}

Through the pre-processing stage, we obtain the summary of the new dataset and the segments of the existing report. 
Each segment corresponds to an analytical objective, a dependency on the previous segment or the data, and pieces of report content, including text and charts. 
The next stage is to reproduce the analysis workflow by re-executing each segment on the new data, encompassing reusing and reconstructing the analytical objective, analysis operations, and report contents. 

\subsubsection{Analytical objective correction and insertion}

The analysis workflow is driven by a series of posed analysis objectives. 
Through the pre-processing stage, we obtain the analysis workflow of the existing report by dividing it into segments and extracting each segment's analytical objectives and dependencies. 
To adapt the workflow to new data, \system{} is required to (1) correct the extracted analytical objectives and (2) support the insertion of new objectives according to the data features and segment dependencies. 

\textbf{Analytical objective correction. }
The preliminary study indicates that while existing analytical objectives often remain applicable, alterations or removals may be necessary due to data fields, dependencies, or the data context and scope. 

First, the original objective might involve data fields absent in the new data. 
Given the pre-processed data summary, we employ LLMs to evaluate if the new dataset's fields sufficiently fulfill the objective, considering semantic similarities despite word-to-word differences in field names. 
For example, an objective mentioning ``earn money'' can be related to the data field ``gross.'' 
LLMs are tasked with explaining their decisions to enhance their reasoning~\cite{mialon2023augmented}. 
If the available fields suffice, LLMs should describe the required fields and analysis operations. 
Otherwise, LLMs must explain what external fields are needed to satisfy the objective. 
In such cases, we correct the objective by replacing missing fields with available alternatives. 
For instance, if a movie dataset lacks geographic data, the objective of locating the highest-grossing movies might shift focus to their directors.

Second, the original objective may derive from insights in a prior segment. 
Adjustments might stem from two scenarios. 
If the insight's nature changes (e.g., from an increasing to a decreasing trend), a corresponding shift is needed in the latter related objective (e.g., from identifying causes of growth to exploring reasons behind the downturn). 
Therefore, we provide the model with the newly generated results of the dependent segment and require it to identify and adapt to such variations. 
Additionally, the dependency may call for a context or scope that the data cannot satisfy, such as from local to national or from a 5-year trend to a 20-year trend. 
LLMs must infer whether changes in scope or context affect the objective's applicability, which could lead to its potential removal if the new data does not support similar adjustments. 

Third, minor adjustments are often required for data context and scope adjustments. 
For example, an objective focusing on a 5-year trend needs adjustment to fit a dataset covering only the past three years. 
LLMs should make these modifications based on the context and scope of the provided data.

\textbf{Analytical objective insertion. }
The uniqueness of new datasets and user-driven queries may necessitate adding fresh analytical objectives based on previous insights and dependencies. 
\system{} enables users to embed new objectives at chosen positions, rooted in the data or reliant on preceding analysis segments. 
Users can define the focus data fields and dependencies of these new objectives, and LLMs can suggest potential objectives based on user input.

\subsubsection{Analysis Operation Generation}

% \TODO{code structure, generate a table and a chart}. 
Once the analytical objective has been refined, \system{} generates the requisite analysis operations to fulfill this objective. 
Since these operations are not explicitly detailed in the report, we utilize the code-generation capabilities of LLMs for this phase. 

LLMs are prompted to generate analysis code that aligns with the clarified objective, provided with the data summary and original report content as guides. 
The model is instructed to refer to the original report to deduce the necessary data transformations. 
We also remind the model to generate the code that accommodates the new data, as the reference report content is from a different dataset and only serves as a reference for expected output. 
The model is required first to plan step by step~\cite{kojima2022large} and then generate the Python code that results in transformed data and a chart using matplotlib~\cite{Hunter2007matplotlib} or Seaborn~\cite{Waskom2021seaborn}. 

Upon code generation, we execute it to procure the transformed data and the accompanying chart. 
The execution may also raise errors. 
We relay any execution results, including the transformed data, chart, and potential errors, back to the LLMs. 
The model then assesses whether the code execution is successful and whether the results accurately address the analytical objective and are adequate for generating report content. 
Should the model deduce that revisions are necessary, the cycle of code generation and execution is repeated until satisfactory results are obtained, paving the way for report content creation.

\subsubsection{Report Content Production}

With the execution results in hand, we proceed to generate new report content. 
Given that the code already produces the chart, the model's task in this step is to generate the accompanying textual narrative. 
We instruct the model to produce a narrative that imitates the writing style of the reference report yet is tailored to fit the new data context and the insights derived from the executed analysis. 
We also enable user modification to the report content. 

\subsection{Organization Stage}

After reproducing the analysis workflow and obtaining the new data insights, the next step is to structure the new report. 
As we generate the sequence of segments based on the order of dependencies, the implicit logical structure is adopted naturally. 
Additionally, we inherit the explicit structural elements (such as titles and sections) from the original report. 
Newly inserted analytical objectives are incorporated along with their dependent segments. 
The report and its sections' titles are re-crafted based on the original ones, incorporating new data insights to guide the title generation process. 
User interventions are also supported, allowing for the reorganization of segments into new sections, thereby tailoring the report structure to meet user needs or preferences better.


