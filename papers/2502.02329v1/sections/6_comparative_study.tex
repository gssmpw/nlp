\section{Comparative Study with \system{} Generation Pipeline}
\label{sec:comparative_study}

We conducted a comparative study to evaluate the effectiveness of the \system{} pipeline. 

\subsection{Study Setup}
Our primary focus was to assess two aspects:
(1) whether the reference report facilitated deeper and more logical data analysis, 
(2) whether the pre-processed segmentation and one-by-one adaptation approach proved beneficial for reusing the analytical logic from the existing report.
We chose to generate two baseline reports with GPT-4o because GPT-4o with Code Interpreter can process CSV files, execute code, and summarize the results. 
Specifically, we generated reports based on the same dataset under three different conditions: (1) using \system{}, (2) using GPT-4o without a reference report, and (3) using GPT-4o with a reference report. This setup serves as an ablation study to evaluate the impact of the reference report and our decomposition-and-adaptation pipeline.

We did not recruit additional data analysts to generate the reports. 
Instead, we directly utilized the reports generated by \system{} and GPT-4o without manual modifications to eliminate potential bias from human intervention. 
This approach allowed us to assess whether the \system{} pipeline could enhance the performance of data report generation without human intervention.

To simulate the practical scenarios of reading data reports, only the charts and narratives were included, excluding the specific data transformation code.
Participants were presented with these three sets and asked to evaluate and compare the quality of the reports within each set. 
To minimize potential order effects, the order of the reports within each set and the order of the three sets provided to participants were counterbalanced.

\subsubsection{Participants. }
We recruited 18 participants (P1-P18, 11 males and 7 females) from various backgrounds, including data visualization (12), data science (4), machine learning (3), network security (1), and human-computer interaction (1). 
Most participants were familiar with data analysis, with an average self-reported score of 3.5 on a 5-point Likert Scale. 
All participants had experience reading data reports, usually from data news, such as official statistics bureau reports, industry research reports, publicly disclosed financial reports, university survey reports, etc.
None of them had experience using our system before the experiment. 

\subsubsection{Data and reference reports. }
We selected three pairs of datasets and reference reports for the study. 
These datasets include the crime data described in~\autoref{sec:usage_scenario}, as well as two popular Kaggle datasets: the Titanic dataset and the cardiovascular disease dataset. 
The datasets were chosen for several reasons: 
They encompass different types of data fields (temporal, quantitative, and categorical), and each contains more than ten data fields. 
They are also relatively clean, requiring minimal pre-processing, making them well-suited for direct analysis. 
Additionally, they are easy to understand and suitable for broad audiences.

% Select Report (10): 
For the reference reports, we used the top-ranked report for each dataset. 
For the crime data, we used a report on Chicago crime, which is relevant both in terms of topic and data characteristics. 
For the cardiovascular disease dataset, we selected a report on the rise of Alzheimer's disease, which is highly relevant to the topic and somewhat similar in data. 
For the Titanic dataset, we chose a report on voting patterns in Britain. 
While this report is not highly relevant in terms of topic, it shares data similarities, such as analyzing different groups (e.g., by sex, age) in relation to specific outcomes (e.g., survival on the Titanic, voting behavior).
Detailed information on the datasets and reference reports are provided in the supplementary materials. 

\subsubsection{Report Generation. }
For each dataset-reference report pair, we generated new reports using three approaches: \system{}, GPT-4o without the reference report, and GPT-4o with the reference report. 
To ensure a fair comparison, all results were generated without manual modifications.

For \system{}, we directly generated new analysis objectives, charts, and text for each segment without any manual adjustments or added objectives.
After completing each segment, we used \system{} to generate a final title for the entire report. 
For the two baselines using GPT-4o, we designed specific input and prompts. 
We prompted the model to follow a process similar to \system{}'s pipeline to generate each report segment. 
This process included defining an objective, writing analysis code, and generating the corresponding section of the report. 
The key differences between the baselines and \system{} are outlined below.

\textbf{Baseline without the reference report.} The model was provided solely with the prompt and CSV dataset. 
The baseline is used to assess whether the reference report facilitates better data analysis.

\textbf{Baseline with the reference report. } In this baseline, the model received the CSV dataset, the textual content of the reference report, and the corresponding chart images. 
We chose to provide text and images instead of a PDF file of the reference report due to limitations in GPT-4o's ability to interpret information from PDFs. 
In our preliminary tests, when we uploaded PDFs to the model and made queries about images within the PDFs, the model occasionally failed to retrieve the necessary information. 
The model was further prompted to adapt the analytical objectives and methodologies from the reference report and make necessary adjustments to align with the dataset. 
Different from \system{}, this baseline did not require the reference report to be segmented into distinct parts with extracted analytical objectives and logic before generating the report. 
We set this baseline to evaluate whether the segmentation pre-processing in \system{} improves the reuse of analytical objectives and logic from the reference report. 

To accommodate the model's output length limitations, we allowed the model to process the (objective, code, text) steps iteratively, one cycle at a time. 
After each cycle, the resulting chart and text were collected as a report segment. 
The model was then prompted with ``Continue'' to produce the next segment. 
In the event of errors (e.g., failed code execution or OpenAI generation errors), we re-generated the output until valid results were obtained. 
By repeating this process, we generated complete data reports for comparison, ensuring that both baselines produced the same number of segments as \system{}'s report. 
Finally, we prompted the model to generate a title for the entire report, as done in \system{}.


\subsubsection{Procedure and tasks. }
Participants began by reviewing three sets of data reports, with each set containing three reports for comparison. They were instructed to thoroughly evaluate and compare the quality of the reports within each set. After carefully reviewing all the reports in a given set, participants completed a five-point Likert scale questionnaire to assess their impressions. The process of reviewing and evaluating each set took approximately 30 minutes.
The questionnaire assesses six dimensions: overall quality, insightfulness, logicality, chart effectiveness, text effectiveness, and consistency.
Specifically, logicality refers to the logical flow and coherence between different sections of the report.
Chart effectiveness and text effectiveness measure how effectively the charts and text convey data insights, respectively.
Consistency assesses whether the content of the charts and text aligns accurately with each other.
After completing the questionnaire, participants engaged in a 20-minute interview to discuss their responses.
All participants were paid with \$7 after the experiment. 


\begin{figure*}[!htb] 
  \centering 
  \includegraphics[width=\linewidth]{figs/evaluation.png}
  \caption{Results of the 5-point Likert scale questionnaire in the comparative study. (a) Overall results summarizing all ratings across the three dataset-reference report pairs. (b-d) Individual results for each dataset-reference report pair.
  }
  \label{fig:evaluation} 
\end{figure*}

\subsection{Results and Discussions}

The results of the comparative study are illustrated in~\autoref{fig:evaluation}. 
Overall, participants provided more positive assessments for the report generated by \system{} in all aspects, as is shown in the overall result (\autoref{fig:evaluation}a). 
We further summarized the important feedback as follows. 

\textbf{Logicality.}
Among all rated aspects, participants consistently rated \system{} highly for logicality. 
This was also one of the most frequently mentioned factors in participants' feedback: 13 out of 18 participants across all three pairs noted that the logicality of \system{} impressed them compared to the baseline reports. 
Specifically, 9 participants from all three reports commented that \system{} reports exhibited a ``clear overall-to-specific structure, making the logic of the report more apparent.'' 
8 participants further highlighted that the paragraphs in \system{} reports ``progress step by step, with a logical and cohesive flow.'' 
P8 pointed out how the report on the cardiovascular disease dataset followed an overall-to-specific structure and went step by step: ``It starts with the prevalence of the disease, then moves to find the leading factors, and then analyzes each key factor in detail.'' 
This progressive logicality also contributed to higher ratings for the overall quality and insightfulness of \system{} reports, with 8 participants indicating that deep analysis was an important factor in assessing insightfulness.

In contrast, both baseline reports received feedback indicating that their narrative structure was ``flat'' and ``lacked a stepwise, progressive flow''. 
13 participants noted that the paragraphs ``had few connections'' and felt ``disjointed'' while reading these reports. 
For example, P2 and P14 observed that baseline reports ``tend to analyze more dimensions, but each factor was not examined in depth''. 
This suggests that the baselines tended to analyze different data attributes or aspects separately, resembling a breadth-first search approach. 
For example, 4 out of 6 participants noted that the baseline report on the Titanic dataset covered different aspects in the first half but ended up repeating analyses. 
This could be due to the report's length (8 segments), which may have caused the model to either forget previous analyses or fail to elaborate deeper on earlier sections.

Some participants also pointed out the trade-off between the depth and breadth of analysis. 
P1 noted that while \system{} generated more in-depth reports, the scope of analysis was narrower. 
For instance, the crime report focused on only a limited set of data attributes. 
This reflects the trade-off between depth and breadth: deeper analysis of certain factors in the same report length may lead to a narrower focus. 
It's akin to a tree with the same number of nodes: a deeper tree has a smaller breadth. 
The design of \system{}, which follows the logical structure of existing reports, does limit the breadth of analysis to some extent. 
This highlights the importance of our system's interactive feature that allows users to see what data attributes have not yet been analyzed and add or delete nodes based on their preferences. 

\textbf{Comparison between the Baselines.} 
Among all three pairs, the scores for GPT without reference report varied across aspects, with no clear standout between the two baselines. 
This indicates that providing a reference report to GPT does not seem to impact the quality of the generated reports significantly. 
However, the results from \system{} were overall better than the baselines. 
Upon closer inspection of the baseline reports, we found that those generated by GPT with the reference report did not clearly inherit the reference report's logical structure. 
As noted earlier, participants found both baseline approaches lacking in logical coherence. 
Since the key difference between GPT with a reference report and \system{} lies in the pre-processed segmentation and one-by-one adaptation approach, this design appears to facilitate the preservation of the logical structure from reference reports.

\textbf{Errors in Chart Design and Text-Chart Consistency.} 
Some reports generated by \system{} are rated lower than certain baseline reports in specific aspects, including chart consistency (Cardiovascular Disease dataset) and chart effectiveness (Titanic dataset). 
Based on the user feedback, the primary reason for these issues is errors in chart design and text-chart consistency. 
While \system{} received higher overall ratings, it still did not resolve the potential errors that could arise from GPT-generated content. 
Specifically, in the cardiovascular disease report, the text described one age group as having the highest cholesterol level, but the chart depicted that same group as having the second highest.
The Titanic dataset report, while praised by participants for its in-depth analysis, included some complex charts, such as \autoref{fig:compararive_error_example.png}, which some participants felt could be improved. 
P15 suggested that \autoref{fig:compararive_error_example.png} could be enhanced by using survival rates on the y-axis rather than relying on two colors to differentiate between those who survived and those who did not.
While \system{} provides manual modification functionality to enable users to improve these issues, there are still notable limitations. We further discuss this limitation in the discussion section.

\begin{figure}[!htb] 
  \centering
  \includegraphics[width=0.5\linewidth]{figs/compararive_error_example.png}
  \caption{
    Example of an inappropriate chart generated in the Titanic dataset report, where the distinction between survivors and non-survivors is made using two colors. 
  }
  \label{fig:compararive_error_example.png}
\end{figure}