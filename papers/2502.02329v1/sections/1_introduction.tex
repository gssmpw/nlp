\section{Introduction}

Data reports, typically composed of expressive visualizations and textual insights, are essential for communicating data value. Creating these reports involves a complex, iterative process where data scientists perform data transformations, configure charts, and summarize results~\cite{behrens1997principles, subramanian2020tractus}. For instance, analyzing a movie dataset to track genre popularity trends requires calculating annual movie releases per genre and configuring line charts to reveal insights like the rise or decline of superhero movies. The final step involves organizing these insights into a coherent report~\cite{li2023wherearewesofar}. This process demands careful decision-making at every stage~\cite{zhao2020chartseer, wongsuphasawat2017voyager, battle2019characterizing}, making it time-consuming and mentally taxing.

Due to the significant time and mental effort required, researchers have explored automating data report generation~\cite{shi2020calliope, wang2019datashot}. Statistics-based methods identify insights by analyzing data relationships, such as correlations, but become computationally expensive with large numbers of data columns. Machine learning-based methods can reduce costs by modeling the mapping from data tables to charts, but creating training datasets is costly. Additionally, traditional machine learning struggles to connect data fields with their semantic context, which is crucial for meaningful analysis. For instance, understanding fields like ``worldwide gross'' and ``budget'' in a movie dataset requires recognizing their implications, such as whether higher budgets lead to higher revenues.

Creating data reports is often an experience- and knowledge-driven process. Analysts typically begin with a topic informed by data characteristics, background knowledge, or analysis goals. They may seek out existing reports on similar topics and datasets, such as comparing the latest global COVID-19 data with earlier datasets. These reports often share consistent analytical objectives and content, like tracking regional infection and recovery trends. Existing reports can provide valuable insights into human expertise and preferences, making them useful references for creating new reports. However, while related reports may be found, the lack of available code presents challenges in recreating the analysis with new data.


In this paper, we develop a retrieve-then-adapt method for generating data reports by using existing reports on similar topics as references, building on the retrieve-then-adapt idea~\cite{qian2020retrieve}. 
We create a repository of public data reports, and when a new dataset is uploaded, the system ranks these reports by relevance. 
Users can then retrieve a suitable reference report to guide the creation of a new report.
We parse and reconstruct the retrieved report's workflow, breaking it down into sections with specific analysis objectives. 
Then, we adapt them to the new data. Specifically, each objective is re-analyzed for the new data, generating code, visualizations, and insights that are integrated into a new report. 
This process involves complex natural language processing tasks.
With the rapid advancement of large language models (LLMs) like GPT-4~\cite{openai2023gpt4} and Llama 3, these models show promise in complex task reasoning~\cite{suh2023Sensecape, wang2023dataFormulator, lin2023inksight}, making them a suitable solution for our tasks. 
However, even the most advanced LLMs struggle to generate a complete and accurate data report that meets user expectations. 
Our user study further demonstrates that, even with appropriate prompts and reference reports, LLMs often fail to produce satisfactory results in a single step.
The challenges can be summarized into two aspects.

\textbf{Effective deduction of the absent analysis workflow. }
The absence of detailed analytical workflows in data reports poses a significant challenge for reconstructing the analysis. Without access to analysis documents, code snippets, or operation logs, it becomes difficult to deduce the analytical objectives and data transformations that led to the final insights.

\textbf{Informed decisions on reuse and adjustments. }
Different collection times, sources, and methodologies complicate the reuse of existing workflows. New data may have different formats, field names, distributions, and insights, requiring adaptation or a complete reframing of the analysis. However, the criteria for deciding what to reuse and what to adjust are often ambiguous and context-dependent.

To address these challenges, we introduce \system{}, an LLM-based method for generating data reports using previous reports and new datasets. 
We approach the first challenge by structuring the analysis workflow as a series of interdependent segments, each consisting of an analytical objective, a chart, and a textual description. We then extract and execute the dependencies between these segments.
To tackle the second challenge, we conducted a preliminary study with 39 pairs of reports on similar topics, examining their similarities and differences. Based on these findings, we developed a pipeline that identifies inconsistencies in new data and generates tailored analytical objectives, data transformations, and insights. 
We also created an interactive interface of \system{} for adding objectives, observing real-time outputs, and editing content. \system{} was evaluated through comparative and usability studies, demonstrating its effectiveness in generating refactored reports from existing ones.

The contributions of this study are as follows. 
\begin{itemize}
    \item We propose a framework to generate new data reports by retrieving past human-made reports to enhance the human-LLM collaboration for data report generation. 
    \item We deliver a preliminary study that investigates the similarity and difference patterns of data reports on similar topic and summarizes the considerations for reusing and rectifying. 
    \item We develop a proof-of-concept system for the framework, \system{}~\footnote{https://\system{}2024.github.io/}, which can generate data reports and allow users' editing by their preference. 
    \item We conduct comparative and usability user studies to prove the effectiveness of \system{}. The feedback could shed light on future research.
\end{itemize}