\section{Usability Study of \system{} System}
\label{sec:user_study}

To assess the usability of the \system{} system, we further conducted a user-centric experiment. 

\subsection{Study Setup}

In this study, participants were asked to use the \system{} system to generate a data report and provide feedback on their experience.
Specifically, participants were required to generate a report based on a specified dataset and independently select a reference report.
Three datasets were available in total, and participants were randomly assigned to one of them, ensuring an equal distribution of participants across datasets.

\subsubsection{Participants. }
We recruited 12 participants (U1-U12, 9 males and 3 females) from various backgrounds, including data science (5), computer science (3), mathematics (2), sports science (1), and biosystems engineering (1). 
Most participants were familiar with data analysis, with an average self-reported score of 3.25 on a 5-point Likert Scale. 
All participants had experience reading data reports, usually from data news, such as official statistics bureau reports, industry research reports, publicly disclosed financial reports, university survey reports, etc.

\subsubsection{Tasks and Data. }

We used the same three datasets as in the comparative study (\autoref{sec:comparative_study}).
For the reference reports, we selected 8 data reports from various domains, including Health, Education, Security, etc.
Rather than providing a large collection of reports, we chose to sample a smaller set to encourage participants to review each report's information and select one as the reference. 
This approach enabled us to evaluate the effectiveness of our report retrieval method and understand the factors that participants considered when selecting a reference report.

Participants were assigned a specific dataset and tasked with using \system{} to (1) select a reference report from the 8 provided reports, (2) generate a complete report based on the chosen reference, including report segments and headings, and (3) make modifications according to their preferences. 
Modifications included regenerating the model responses, manually editing the analytical objectives and report content, and inserting or removing segments as needed. 
If the participant was unsatisfied with the report selected, s/he could change to another one.
Participants were required to generate a report with at least four segments in total.

\subsubsection{Procedure. }
The entire experiment lasted approximately 70 minutes. 
It began with a 3-minute introduction to the concept of generating a new data report based on a reference report. 
Participants were then presented with a 10-minute tutorial on \system{} interactions, using a COVID-19 report as an example. 
Following the tutorial, they had 3 minutes to explore the system on their own. 

After the exploration phase, participants were tasked with using \system{} to generate a data report based on a specified dataset, which took approximately 35 minutes. 
Specifically, they were required to carefully review the information from the 8 provided reports and select one to serve as the reference report for the analysis of their specified dataset. 
All actions and interactions were logged throughout this process. 
Upon completion of the report, participants were asked to complete a post-study questionnaire. 
This included a 5-point Likert scale assessing both the system's overall usability with the System Usability Scale (SUS) and its individual functionalities. 
Finally, participants took part in a 15-minute feedback interview to provide qualitative insights into their experience. 
All participants were compensated with \$10 for their participation.

\begin{figure*}[!htb] 
  \centering
  \includegraphics[width=\linewidth]{figs/user_study_result.png}
  \caption{The result of the questionnaire in the usability study. }
  \label{fig:user_study_result}
\end{figure*}

\begin{figure*}[!htb] 
  \centering
  \includegraphics[width=\linewidth]{figs/user_involvement.png}
  \caption{The interaction sequences in the usability study, including the assigned datasets (d1-d3), the selected reports and their rankings (a), the interaction sequences during the retrieval (b) and generation (c). }
  \label{fig:user_involvement}
\end{figure*}


\subsection{Results}

The results of the post-study questionnaire are illustrated in~\autoref{fig:user_study_result}.
The user interaction sequence is shown in~\autoref{fig:user_involvement}, highlighting the sequence of actions performed by each participant during the user study. 
We further summarized the important feedback as follows. 

\textbf{Ease of use. }\system{} received an overall SUS score of 88.96, reflecting its high usability. 
The SUS scores for the crime, disease, and Titanic datasets were 86.88, 91.88, and 88.13, respectively. 
Although the Titanic dataset was not highly topic-relevant to the alternative reports provided, the SUS score did not show a significant drop compared to the results of other datasets. 
This suggests that \system{} maintains consistent usability across different scenarios. 

Most participants agree that \system{} is easy to use and does not need to learn a lot of prior knowledge before getting going with the system. 
U4-U6 and U8-U10 noted that the system design and interaction is ``intuitive'' and ``has a low learning curve''. 
Additionally, all participants agreed that the time cost associated with using the system was acceptable. 
U2, U3, and U9 specifically noted that the real-time feedback from the system helped make the processing time feel more manageable.
U9 commented, ``I can see the results being generated gradually, which reassures me that \system{} is steadily progressing. This allows me to stay informed about the model's generation process, making the time spent less noticeable.''

\textbf{Report Retrieval. }
All participants agree that the \system{}'s ranking about the alternative reference reports is suitable. 
We also recorded the users' selected reports, their corresponding ranking in \system{}, and their interactions during the selection process (\autoref{fig:user_involvement}).
The results show that most participants selected the highest-ranked report, and all participants chose one of the top three reports, suggesting the effectiveness of our retrieval mechanism.

Regarding interaction, all participants first reviewed the information in the report list before previewing or selecting alternative reports to assess their suitability.
Notably, most participants did not preview or select all reports. 
Instead, they chose several potential reports to check further based on the information available in the list (\autoref{fig:retrieve}). 
This indicates that the information provided in the report list helped participants make more informed decisions.
Specifically, for the disease and Titanic datasets, participants previewed or selected more reports than for the crime dataset, likely because the disease and Titanic datasets were less similar to the reports in the repository, prompting participants to check more alternatives to find the most appropriate report.

We also interviewed the participants to understand the factors they considered when choosing a reference report and how they ranked these factors.
All participants mentioned field similarity and topic relevance as important criteria, but most (9 out of 12) prioritized field similarity over topic relevance.
They also noted that the list of potential data fields provided in the report list (\autoref{fig:retrieve}) was more useful than the topic information.
U10 explained, ``The topic primarily helps me set expectations for the content of the reference report, such as health or governance, but it doesn't necessarily mean that I can use similar analytical methods. To select a reference to analyze the target dataset, I focus more on the overlapping data fields, such as gender or age, because similar fields are likely to use similar analytical methods.''

Participants also listed other factors they considered when selecting reports.
The most frequently mentioned factor (8 out of 12) was the specific analysis approaches used in the reports, with 4 participants ranking this factor as the highest priority.
Specifically, U11 described his decision process: ``I first judge the analysis methods used in the report, such as trend analysis or group comparison analysis. Then, I assess whether the target dataset is suitable for that approach based on the logical relationships between variables. For example, the Titanic dataset is better suited for group comparison analysis to explore the relationship between survival and other grouped factors.''
U11 further emphasized that considering field similarity is essentially evaluating potential analysis methods because ``similar data fields are likely to be analyzed using the same methods.''
However, he also pointed out that focusing solely on data fields is not comprehensive. ``When data fields differ, it is still possible that the analysis methods in the report are applicable. For example, the Titanic dataset contains a variable about passengers' cabins, which is unique to this dataset but still suitable for group comparison analysis.''

\textbf{User Involvement. }
Most participants agreed that the generated objectives and content were correct and effective~\autoref{fig:user_study_result}.
However, as shown in~\autoref{fig:user_involvement}, all participants used interactions to adjust and modify the results generated by \system{}, including regenerating or modifying objectives and content and manually adding or deleting segments.
This frequent use of interactive features prompted us to further investigate why participants made these modifications.

Modifying objectives was a feature most participants frequently used throughout the process.
However, participants indicated that their modifications were not due to errors or unreasonable results but because the GPT-generated content helped them refine more specified objectives.
For example, U7 commented, ``At first, I didn't know exactly what to analyze, but after seeing the generated analysis objectives, it inspired me to form clearer goals, so I modified them to better align with my intent.''
This trend was also reflected in the content of the modifications, mainly changing the data fields to be analyzed or specifying chart types.
U12, the participant who used this feature the most, expressed a lot of appreciation: ``I just need to make small modifications, like changing a data field or adding a requirement for the chart type, to generate content that better matches my preferences.''
U8 even modified the analysis objective to include statistical tests.
This illustrates the system's flexibility, allowing users to specify detailed requirements in the analysis objectives.

Another feature used by all participants was the ability to manually add or delete segments, which was typically employed later in the analysis process.
Eight participants found this feature ``very necessary'' and found it particularly useful when paired with the feature showing how frequently data fields had been analyzed.
U6 noted, ``The statistics on how often a data field is used for comparisons are helpful. Data fields used frequently align with the analysis topics, while less-used data fields help me identify un-analyzed ones, so I can manually add segments.''
U11 further commented, ``The reference report design in \system{} gives me a warm start and provides inspiration when I'm unsure how to begin my analysis. As I continued, my analysis intent became clearer, so I made modifications. Eventually, when I have a very clear intent, I add segments manually to further refine the analysis.''
This process aligns with our observation of the interaction sequence: Most participants did not make frequent modifications at the beginning.
As their analysis intent became more refined, the interactions involving modifications increased, culminating in manual segment additions and deletions.
This progression from low to high customization reflects users' growing clarity about their analytical goals.

Some participants used the regeneration feature more frequently than modification. 
Two main reasons emerged:
U5 mentioned that he was unsure about his desired results, but he was certain about what he did not want, so he chose to regenerate.
U11 explained that he preferred not to input words manually, so He would regenerate until he found an acceptable result. 
U11 also suggested that \system{} should include more options for customization, such as specifying chart types by options.