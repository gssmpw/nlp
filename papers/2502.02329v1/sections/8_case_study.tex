

\section{Report Quality Assessment: Retrieval Factors vs. Quality of Generated Reports}

Retrieval is a critical initial step in the \system{} pipeline. 
The more similar the reference report's data is to the dataset being analyzed, the fewer adjustments are needed. 
Currently, \system{} considers both topic relevance and field similarity to identify reports with potentially similar data.
However, several critical questions arise: 
\textbf{
How ``similar'' does the reference report's data need to be to the target dataset? 
In addition to topic relevance and field similarity, what other factors might influence the quality of the reports generated by \system{}? }


Therefore, to evaluate \system{}'s performance under varying conditions, we designed a two-round experiment. 
\textbf{In the first round,} we focused on assessing topic relevance and field similarity between different report-dataset combinations. 
Participants were asked to rate 24 combinations (3 datasets $\times$ 8 reports from diverse fields) on topic relevance and field similarity. These ratings allowed us to understand how users perceive the similarity between datasets and reference reports. 
\textbf{In the second round,} based on the results of the first round, we selected 4 reports (from the 8 reports) that varied in topic relevance and field similarity to the 3 datasets.
Note that the downsampling aims to provide a practical sample size for the experiment.
Combining these 4 reports with the 3 datasets, we generated 12 new data reports using \system{}. 
Participants were then asked to evaluate the quality of these reports across multiple dimensions. 
This two-round design enabled us to assess how variations in dataset-reference report similarity affect \system{}'s ability to generate data reports and to identify other factors that may influence \system{}'s performance.


\subsection{Round 1. Topic Relevance and Field Similarity. }
We recruited 13 participants (10 males, 3 females, all familiar with data analysis) to evaluate the topic relevance and field similarity between 24 data-reference report combinations (3 datasets $\times$ 8 reports). 
These datasets and reports were identical to those used in the user study (\autoref{sec:user_study}) and spanned a variety of domains.
During the experiment, participants were first introduced to the concepts of topic relevance and field similarity, as described in \autoref{subsubsec:report_retrieval}. 
They were then asked to rate each data-reference report pair on a 1-5 scale for both aspects. 


The results reveal varying levels of topic relevance and field similarity~\autoref{fig:pre_case_study}. 
Some dataset-report pairs show both high relevance and similarity. 
For example, the 2020-2023 Los Angeles crime dataset is closely aligned with the 2022 Chicago crime report (~\autoref{fig:pre_case_study}a). 
Other pairs exhibited either stronger topic relevance or field similarity. 
For example, the cardiovascular disease dataset was somewhat relevant to a report on opioid use but exhibited lower field similarity (~\autoref{fig:pre_case_study}b): the dataset contained personal health and cardiovascular information, whereas the report focused on trends in opioid use and its geographic distribution.
Conversely, the Titanic dataset and a report on voting patterns in Britain (~\autoref{fig:pre_case_study}c) had no topic relevance but shared demographic data (e.g., gender, age), with the Titanic dataset containing passenger demographics, while the voting report focused on voting tendencies within demographic groups. 
Additionally, some dataset-report pairs exhibited lower topic relevance and lower field similarity, such as the Titanic dataset and the Chicago crime report.



Based on these ratings, we selected four reports covering different levels of topic relevance and field similarity: (1) highly topic-relevant and data-similar, (2) topic-relevant but less data-similar, (3) data-similar but less topic-relevant, and (4) neither topic-relevant nor data-similar. 
These reports, combined with the three datasets, were used to generate 12 new reports, which were then evaluated for quality in the second round of the study.


\begin{figure*}[!htb] 
  \centering 
  \includegraphics[width=0.6\linewidth]{figs/pre_case_study.png}
  \caption{The distribution of the topic relevance and field similarity among the 24 data-reference report combinations. The colored points correspond to the selected 12 data-reference report combinations. }
  \label{fig:pre_case_study} 
\end{figure*}

\begin{figure*}[!htb] 
  \centering 
  \includegraphics[width=0.6\linewidth]{figs/case_study.png}
  \caption{The average ratings of the 12 reports, based on 3 different datasets. (a) The distribution of average ratings among different datasets. (b) The distribution of average ratings on field similarity and topic relevance, respectively. 
  }
  \label{fig:case_study} 
\end{figure*}

\subsection{Round 2. Report Quality under Different Conditions. }
We further recruited 9 participants (6 males and 3 females, all familiar with data analysis) to evaluate the quality of the 12 generated reports. 
Specifically, we showed them the four reports of the same dataset together, repeated three times for three datasets, and required them to rate each report on a 1-5 scale. 

Overall, the ratings of the generated reports are stable, with most reports scoring higher than 3 (~\autoref{fig:case_study}a), indicating that \system{} performs consistently across different cases.
In examining the relationship between the quality of generated reports, topic relevance, and field similarity (~\autoref{fig:case_study}b), field similarity demonstrates a stronger correlation with report quality compared to topic relevance. This aligns with the usability study feedback, where participants ranked field similarity as a more important factor than topic relevance. Specifically, reference reports with higher field similarity scores tend to produce consistently higher-quality reports.

On the other hand, low scores in either field similarity or topic relevance do not necessarily result in poor report quality, but they do lead to greater variability in the results. This stems from \system{}'s mechanism, which makes adjustments to align with the data when discrepancies occur. As a result, \system{} does not rely solely on identifying a highly similar reference report to generate effective outputs.