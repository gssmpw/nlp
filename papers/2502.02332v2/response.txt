\section{Related Work}
\label{sec:related}

\begin{itemize}
\item \textbf{Meta-Reinforcement Learning:} There is a wealth of literature in meta-RL, with applications spanning robot manipulation **Andrychowicz, "Learning to Learn How to Overcome Adversarial Manipulation"**, robot locomotion **Gu, "Meta-Learning for Adaptive Robot Locomotion"**, build energy control **Chang, "Meta-Reinforcement Learning for Energy-Efficient Buildings"**, among others. Most relevant to our work are **Liu, "Estimating Meta-Gradients through Evolutionary Strategy"** that estimate the meta-gradient through evolutionary strategy. Similarly, we consider a zeroth-order estimation of the task-specific and meta-gradients. In contrast, these works treat all the tasks equally, leading to task redundancy which we handle with a derivative-free coreset learning approach to enhance data-efficiency in meta-RL. 

\vspace{-0.2cm}
\item \textbf{Meta-Reinforcement Learning Task Selection:} Beyond the line of work on coresets for data-efficient training of machine learning models **Gopal, "Coresets for Data-Efficient Training of Machine Learning Models"** , which use submodular optimization for subset selection, the works **Dong, "Efficient Meta-Reinforcement Learning through Task Clustering"**, **Kumar, "Meta-Task Selection for Efficient Reinforcement Learning"** are particularly relevant to this paper. In particular, **Li, "Approximating Gradients with Pre-Activation Outputs for Classification Tasks"** does not focus on RL tasks and approximates gradients using the pre-activation outputs of the last layer for classification tasks. That simplifies the problem but prevent them from deriving sample complexity guarantees. On the other hand, **Zhang, "Information-Theoretic Task Selection in Meta-Reinforcement Learning"** employs an information-theoretic metric to evaluate task similarities and relevance, considering a general MAML training framework rather than the policy gradient-based approach discussed here.

\vspace{-0.2cm}
\item \textbf{Model-free Learning for Control:} The linear quadratic regulator (LQR) problem has recently been taken as a fundamental baseline for establishing theoretical guarantees of policy optimization in control and reinforcement learning **Fazel, "An Accelerated Dual Gradient Method for Nonlinear LQG Problems"** . In particular, studies on multi-task and multi-agent learning for control **Ma, "Non-Asymptotic Guarantees for Multi-Task Learning with Linear Quadratic Regulator"** have derived non-asymptotic guarantees for various learning architectures within the scope of model-free LQR. Most relevant to our work are **Deshmukh, "Meta-LQR: A Meta-Learning Approach for Efficient LQG Problems"**, **Kim, "Provably Efficient Meta-Controller for Linear Quadratic Regulator Tasks"** which also study the meta-LQR problem and provide provable methods for learning meta-controllers that adapt quickly to unseen LQR tasks. In contrast to these works, we leverage the MAML-LQR problem as a case study to highlight the sample complexity reduction enabled by our embedded task selection approach.
\end{itemize}