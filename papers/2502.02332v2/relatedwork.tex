\section{Related Work}
\label{sec:related}

\begin{itemize}
\item \textbf{Meta-Reinforcement Learning:} There is a wealth of literature in meta-RL, with applications spanning robot manipulation \citep{yu2017preparing,arndt2020meta,ghadirzadeh2021bayesian}, robot locomotion \citep{song2020rapidly,yu2020learning}, build energy control \citep{luna2020information}, among others. Most relevant to our work are \cite{song2019maml,song2020rapidly} that estimate the meta-gradient through evolutionary strategy. Similarly, we consider a zeroth-order estimation of the task-specific and meta-gradients. In contrast, these works treat all the tasks equally, leading to task redundancy which we handle with a derivative-free coreset learning approach to enhance data-efficiency in meta-RL. 

\vspace{-0.2cm}
\item \textbf{Meta-Reinforcement Learning Task Selection:} Beyond the line of work on coresets for data-efficient training of machine learning models \citep{mirzasoleiman2020coresets1,killamsetty2021grad, yang2023towards, pooladzandi2022adaptive, balakrishnan2022diverse}, which use submodular optimization for subset selection, the works \citep{luna2020information,zhan2024data} are particularly relevant to this paper. In particular, \cite{zhan2024data} does not focus on RL tasks and approximates gradients using the pre-activation outputs of the last layer for classification tasks. That simplifies the problem but prevent them from deriving sample complexity guarantees. On the other hand, \cite{shin2023task} employs an information-theoretic metric to evaluate task similarities and relevance, considering a general MAML training framework rather than the policy gradient-based approach discussed here.

\vspace{-0.2cm}
\item \textbf{Model-free Learning for Control:} The linear quadratic regulator (LQR) problem has recently been taken as a fundamental baseline for establishing theoretical guarantees of policy optimization in control and reinforcement learning \citep{fazel2018global}. In particular, studies on multi-task and multi-agent learning for control \citep{zhang2023multi,wang2023model,tang2023zeroth,toso2024asynchronous,toso2024meta,lee2024nonasymptotic,lee2024regret} have derived non-asymptotic guarantees for various learning architectures within the scope of model-free LQR. Most relevant to our work are \cite{molybog2021does,musavi2023convergence,toso2024meta,aravind2024moreau,pan2024model}, which also study the meta-LQR problem and provide provable methods for learning meta-controllers that adapt quickly to unseen LQR tasks. In contrast to these works, we leverage the MAML-LQR problem as a case study to highlight the sample complexity reduction enabled by our embedded task selection approach.
\end{itemize}