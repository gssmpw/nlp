
\section{Experiments}

\subsection{Setup}
\paragraph{Datasets \& metrics.}

Our method \textbf{FourieRF} can process a wide variety of scenes. We thus test it on synthetic and real scenes. The \textbf{NeRF-synthetic} dataset~\cite{mildenhall2020nerf} was rendered using Blender containing 8 objects with complex material and geometric information. We use ZeroRF's~\cite{shi2024zerorf} \textit{same setting to train and evaluate} our method (with the number of views ranging from 4 to 6). The \textbf{LLFF}~\cite{mildenhall2019local} contains 8 real scenes. We use RegNeRF's~\cite{niemeyer2022regnerf} \textit{same setting to train and evaluate} our method (training on 3, 6, or 9 views).

\vspace{-1em}\paragraph{Implementation.}

FourieRF can be easily added to the TensoRF~\cite{Chen2022ECCV}. Our method can enhance both the performance of the CP and VM decomposition, and in each case we create a class that inherits from the respective TensoRF decomposition. We find in practice that the VM decomposition leads to better results (See Table.~\ref{tab:blender-results}), so we use it in all the experiments unless otherwise stated. Please see the supplementary material for more information about our code and the hyper parameters used.

\vspace{-1em}\paragraph{Baselines}
Our first baseline is vanilla TensoRF-VM~\cite{Chen2022ECCV}, as it is the foundation upon which our method is built. We also compare our method to ZeroRF~\cite{shi2024zerorf} and FreeNeRF~\cite{yang2023freenerf}, the most recent and directly comparable baselines. ZeroRF specializes in the reconstruction of synthetic scenes and is an accelerated method that trains in around 30 minutes, but, as noted earlier, it struggles with real scenes. In contrast, FreeNeRF can process any type of scene~\cite{yang2023freenerf}, but its training time is extremely long (approximately one day). For completeness, we also include quantitative comparisons with well-established baselines such as DietNeRF~\cite{jain2021putting}, which leverages data priors, and RegNeRF~\cite{niemeyer2022regnerf}, which relies on geometrical regularizations.

\subsection{Results}


% What our results achieve
Our method achieves performance that is on par with the state-of-the-art (SOTA) approaches, while being significantly faster than comparable methods. We refer the reader to the supplementary material, where we showcase a video presentation and animated results.

% In terms of time
Indeed in Table~\ref{tab:execution_time_comparison}, we can see that our method's training time is even faster than TensoRF. Our Fourier parameterization is done per iteration at virtually no cost. Moreover, as seen in Fig.~\ref{fig:fail_cases}, TensoRF's scene representation is filled with floaters. This hinders training, filling the scene with noise, thus slowing down the whole procedure. When compared to the other accelerated method, ZeroRF \cite{shi2024zerorf}, we see that their Deep Image Prior \cite{ulyanov2018deep}, comes at the cost of evaluating an expensive convolutional neural network. Our method is by far the fastest to converge in the few-shot rendering task.

% In terms of quality
The results of our quantitative evaluation are showcased in Table~\ref{tab:blender-results} and Table~\ref{tab:llff_comparison}. 
% In the synthetic dataset
In the synthetic dataset, we greatly outperform all MLP-based methods by a significant margin. We achieve this while training an order of magnitude faster, and without the use, of the hard-to-tune, occlusion regularization used by FreeNeRF \cite{yang2023freenerf}. The SOTA in this dataset is the accelerated method ZeroRF \cite{shi2024zerorf}. However, we achieve similar results while training over 5 times faster, and Fig.~\ref{fig:blender-qualitative} shows that ZeroRF's prior can lead to the omission of key geometry. Despite its state-of-the-art performance on synthetic datasets, ZeroRF~\cite{shi2024zerorf} fails to handle real scenes effectively. This goes to show, that their Deep Image Prior \cite{ulyanov2018deep} does not generalize to diverse scenes. Our method achieves results that are on par with FreeNeRF \cite{yang2023freenerf}, while training \textit{over 30 times faster.} Moreover, in Fig.~\ref{fig:llff_comparison}, we see that the shapes we extract are in some cases smoother than FreeNeRF's. %We remind the reader that we achieve this without using complex occlusion regularization. 

These results demonstrate that we have introduced an exceptionally \textit{simple} and \textit{flexible} baseline. Our method performs well across a variety of scenes while training at record speeds, highlighting its practicality and ease of use.
%From these results we have shown to effectively introduce, an extremely \textit{simple} and \textit{flexible} baseline. Our method performance well in a variety of different scenes, while training at record speeds. This goes to show the applicability and ease-of-use of our approach.

\vspace{-1em}\paragraph{Ablations.}


The progressive inclusion of complexity is a key aspect of our method. In Fig.~\ref{fig:teaser}, we illustrate a successful training trajectory. However, determining ``how quickly" this complexity is integrated—i.e., setting the parameter $\Delta$ defined in Section~\ref{sec:fourier-parameterization}—is crucial. Fig.~\ref{fig:delta-vs-performance} shows the relationship between the choice of this parameter and the PSNR obtained when training and testing across all scenes from the Blender synthetic dataset (both using 6 views). Fig.~\ref{fig:delta-vs-performance} shows that for large $\Delta$, the method converges to the baseline, TensoRF. In practice, choosing a sufficiently small increment should yield adequate performance. We observe a slight decrease in performance for smaller values of $\Delta$, which is likely due to the fixed 10k training iterations used in the experiment. For the smallest increments, the model simply did not have enough training time.
%We see that if the increment $\Delta$ is set too large, the method converges to the baseline TensoRF. In practice if the increment is chosen to be sufficiently small the performance should be adequate. We see a slight decrease in performance for smaller values because we train for a fixed 10k iterations. Thus, for the smallest increments in this plot, the model was simply not trained enough.

\begin{figure}[t]
	\centering
    \vspace{-1em} 
	\includegraphics[width=\linewidth,trim={0 0 0 0},clip]{./images/delta_vs_psnr_log.pdf}
    \vspace{-2em} %
	\caption{\textbf{Choice of $\Delta$ vs performance.} We investigated the effect of varying the speed at which high-frequencies are integrated during training, using the Blender Dataset with 6 views. The baseline performance without our method is highlighted in red, while our best result is shown in green.}
	\label{fig:delta-vs-performance}
\end{figure}

\begin{table}[t]
	\centering
    \vspace{-0.5em}
	\caption{Global execution time comparison relative to TensoRF. Training on the Blender Dataset for 10k iterations.}
	\label{tab:execution_time_comparison}
	\begin{tabular}{lc}
		\toprule
		Method & Training Time \\
		\midrule
		TensoRF \cite{Chen2022ECCV} & \cellcolor{orange!25}{$1.0 \times$}\\
		ZeroRF \cite{shi2024zerorf} &  \cellcolor{yellow!25}{$5.181 \times$}\\ %0.193x \\
		FreeNeRF \cite{yang2023freenerf} &  $35.71 \times$ \\%0.028x \\
		\textbf{Ours} & \cellcolor{red!25}{$0.93 \times$} \\
		\bottomrule
	\end{tabular}
    \vspace{-1em}
\end{table}

Finally, our method stands as the \textit{fastest} baseline available for the few-shot rendering problem. As shown in Table~\ref{tab:execution_time_comparison}, our execution time is virtually identical to that of vanilla TensoRF~\cite{Chen2022ECCV}, with our method being slightly faster because we do not predict ``useless" floaters. Additionally, we are more than five times faster than ZeroRF~\cite{shi2024zerorf}, the only other accelerated method addressing the few-shot rendering problem.
%Finally, our method is the \textit{fastest} baseline available for the few-shot rendering problem. In Table~\ref{tab:execution_time_comparison} we can see that our execution time is virtually the same to the vanilla TensoRF~\cite{Chen2022ECCV} (our method being slightly faster because we do not predict ``useless" floaters). Moreover, we are over 5 times faster than ZeroRF~\cite{shi2024zerorf} the only other accelerated method tackling the few-shot rendering problem.


% \paragraph{Limitations.}

% \begin{itemize}
%     \item Our method can leave to ``blocky" structures when faced with very limited views and fine structures.
% \end{itemize}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{./images/todo_image.png}
%     \caption{\textbf{Fail Cases.}}
%     \label{fig:fail-cases}
% \end{figure}