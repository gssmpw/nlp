\section{Method}
\label{sec: method}

We apply our Fourier parameterization to representations introduced by TensoRF \cite{Chen2022ECCV}, increasing the maximal Fourier frequency in tensor decomposition gradually.

% Our method is built on top of the TensoRF's \cite{Chen2022ECCV} NeRF representation. As mentioned above, in our method, we use Fourier parameterization to increase the maximal Fourier frequency in tensor decomposition gradually. We briefly introduce two different low-rank tensor decompositions used in our method.

\subsection{Preliminaries}


The key idea behind grid-based NeRF representation is to represent the scene using a decomposed feature grid rather than a deep neural network. We denote the 3D tensor of features that represents the scene as $\mathcal{T}\in \mathbb{R}^{I\times J \times K}$. In our NeRF representation, we experiment with two different methods to decompose this 3D tensor:

\vspace{-1em}\paragraph{CANDECOMP/PARAFAC (CP) Decomposition}

\begin{figure*}[t]
    \vspace{-1em}
	\centering
	\includegraphics[width=0.95\linewidth]{./images/llff_comparison.pdf}
	\caption{\textbf{Comparison in LLFF Dataset.} In the horns scene, we evaluated the performance of FreeNeRF, ZeroRF, and our method under a 3-view training setup. ZeroRF struggled to reconstruct coherent geometry, resulting in significant inconsistencies. FreeNeRF, while more stable, produced renders with notably blurred geometry, failing to capture fine details accurately. In contrast, FourieRF delivers sharper renders, faithfully reconstructing the \textit{key} geometric elements of the scene with high fidelity.}
    \vspace{-1em}
	\label{fig:llff_comparison}
\end{figure*}

In the CP decomposition, $\mathcal{T}$ is decomposed as a sum of outer products of vectors:
$$
\mathcal{T} = \sum_{r=1}^R v_r^1 \circ v_r^2 \circ v_r^3
$$
where $v_r^1 \circ v_r^2 \circ v_r^3$ corresponds to a rank-one tensor component, and $v_r^1 \in \mathbb{R}^I,v_r^2 \in \mathbb{R}^J,v_r^3 \in \mathbb{R}^K$ are factorized vectors of the three modes for the $r$-th component. 

CP factorization reduces space complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$, and offers low-rank regularization at the same time in the optimization, making it a good candidate NeRF representation for few-shot reconstruction. On the other hand, CP sacrifices the rendering quality to minimize the rank of the decomposition.
%I show that VM > CP in blender so even if this might be true I don't think we should mention it. -> making it more suitable for synthetic object reconstruction than real-world scenes with complex backgrounds.

\vspace{-1em}\paragraph{Vector-Matrix (VM) Decomposition}

Unlike CP factorization, VM decomposition enriches the product by using matrices. The decomposition is expressed as:
$$
\mathcal{T} = \sum_{r=1}^{R_1} v_r^1 \circ M_r^{2,3} + \sum_{r=1}^{R_2} v_r^2 \circ M_r^{1,3} + \sum_{r=1}^{R_3} v_r^3 \circ M_r^{1,2},
$$
where $M_r^{2,3} \in \mathcal{R}^{J\times K}, M_r^{1,3} \in \mathcal{R}^{I\times K}, M_r^{1,2} \in \mathcal{R}^{I\times J}$ are matrices for two of the three modes.

The VM decomposition reduces space complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$. %, while still maintaining good approximation.
 For complex scenes, the VM decomposition reduces the number of components required to achieve the same expressivity as CP, leading to faster reconstruction and better rendering. Our method can be applied to any of these decompositions. In practice, applying our method to the VM decomposition allows us to model more complex effects, leading to better quantitative performance (See Table~\ref{tab:blender-results}).

\subsection{Fourier Parameterization}
\label{sec:fourier-parameterization}


As mentioned in Section \ref{sec:motivation}, the key observation behind our method is that objects' geometry and appearance can be learned in a coarse-to-fine manner based on their corresponding low to high frequencies in the underlying NeRF representation.
%
Our work makes the following claims:
(i) There are enough constraints in few-shot inputs to learn an accurate coarse geometry under low-frequency constraints;
(ii) Lower frequencies are easier to learn correctly than higher frequencies; 
(iii) Learning the next set of higher frequencies is more straightforward given a correct set of lower frequencies. 

Let us illustrate the above claims: in Fig.~\ref{fig:coarse-geometry-extraction} we can see that using our method we can establish a good base for scenes, even in the case of complex real scenarios. Moreover, Fig.~\ref{fig:teaser}, shows that given a good estimation of the low-frequencies of the scene, we can progressively add complexity to the object while maintaining a clean reconstruction.
%
In the following section, we demonstrate the process of parameterizing 1D and 2D features using our method. This parameterization allows us to begin with well-established coarse geometry and progressively incorporate fine details. In both cases, the principle remains the same: project the features into Fourier space and remove high frequencies up to a certain threshold. We make use of the discrete Fourier transform, so given a fixed grid size, there is a finite number of Fourier coefficients after transformation. The threshold we use is proportional to this finite number, please refer to the supplementary for typical values.

\begin{table*}[t]
	\centering
    \vspace{-1.5em}
	\caption{Quantitative comparison on Blender.}
	\label{tab:blender_comparison}
	\begin{tabular}{l l lcccccc}
		\toprule
		\multirow{2}{*}{Method} & \multirow{2}{*}{Prior} & \multicolumn{2}{c}{PSNR $\uparrow$} & \multicolumn{2}{c}{SSIM $\uparrow$} & \multicolumn{2}{c}{LPIPS $\downarrow$} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
		& & 4 views & 6 views & 4 views & 6 views & 4 views & 6 views \\
		\midrule
		DietNeRF \cite{jain2021putting} & CLIP & 10.92 & 16.92 & 0.557 & 0.727 & 0.446 & 0.267 \\
		RegNeRF \cite{jain2021putting} & RealNVP & 9.93 & 9.82 & 0.419 & 0.685 & 0.572 & 0.580 \\
		\midrule
		TensoRF \cite{Chen2022ECCV}& No Prior & 18.656 & 21.652 & 0.798 & 0.844 & 0.216 & 0.165  \\
		\midrule
		ZeroRF \cite{shi2024zerorf}& Deep Network & \cellcolor{red!25} 21.94 & \cellcolor{red!25}24.73 & \cellcolor{orange!25} 0.856 & \cellcolor{red!25}0.889 & \cellcolor{red!25}0.139 & \cellcolor{red!25} 0.113 \\
		FreeNeRF \cite{yang2023freenerf} & Frequency & 18.81 & \cellcolor{yellow!25} 22.77 & \cellcolor{yellow!25} 0.808 & 0.865 & \cellcolor{yellow!25} 0.188 &   \cellcolor{yellow!25}0.1495 \\
		\midrule
		Ours - CP & Frequency & \cellcolor{yellow!25} 20.799 & 22.496 & \cellcolor{yellow!25}0.825 & \cellcolor{yellow!25}0.849 & 0.217 &  0.195  \\
		\textbf{Ours} & Frequency & \cellcolor{orange!25} 21.728 & \cellcolor{orange!25} 23.927 & \cellcolor{red!25} 0.858 & \cellcolor{orange!25}0.879 & \cellcolor{orange!25} 0.147 & \cellcolor{orange!25} 0.136  \\
		\bottomrule
	\end{tabular}
	\label{tab:blender-results}
\end{table*}


\begin{table*}[t]
	\centering
	\caption{Quantitative comparison on LLFF.}
	\label{tab:llff_comparison}
	\begin{tabular}{l lccccccccc}
		\toprule
		\multirow{2}{*}{Method} & \multicolumn{3}{c}{PSNR $\uparrow$} & \multicolumn{3}{c}{SSIM $\uparrow$} & \multicolumn{3}{c}{LPIPS $\downarrow$} \\
		\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
		& 3 views & 6 views & 9 views & 3 views & 6 views & 9 views & 3 views & 6 views & 9 views \\
		\midrule
		DietNeRF \cite{jain2021putting} & 14.94 & 21.75 & 24.28 & 0.370 & 0.717 & 0.801 & 0.496 & 0.248 & 0.183 \\
		RegNeRF \cite{niemeyer2022regnerf}& \cellcolor{yellow!25}19.08 & \cellcolor{yellow!25}23.10 & \cellcolor{yellow!25}24.86 & \cellcolor{yellow!25}0.587 & \cellcolor{yellow!25}0.760 &\cellcolor{yellow!25} 0.820 & \cellcolor{yellow!25} 0.336 &  \cellcolor{orange!25}0.206 &  \cellcolor{orange!25} 0.161 \\
		\midrule
		TensoRF \cite{Chen2022ECCV} & 14.292 & 18.183 & 23.677 & 0.315 & 0.576   & 0.777  & 0.545  & 0.370 & 0.213\\
		\midrule
		ZeroRF \cite{shi2024zerorf} & 16.74 & 21.371 & 22.425 & 0.434 &0.698 & 0.750 &  0.470 & 0.302 & 0.275 \\
		FreeNeRF \cite{yang2023freenerf}& \cellcolor{red!25} 19.63 & \cellcolor{red!25} 23.73 & \cellcolor{red!25} 25.13 &  \cellcolor{orange!25}0.612 &  \cellcolor{orange!25}0.779 &  \cellcolor{orange!25}0.827 &  \cellcolor{orange!25}0.308 & \cellcolor{red!25} 0.195 & \cellcolor{red!25}0.160 \\
		\midrule
		\textbf{Ours} &  \cellcolor{orange!25} 19.303 & \cellcolor{orange!25} 23.595 &  \cellcolor{orange!25}25.011 & \cellcolor{red!25} 0.636 & \cellcolor{red!25} 0.790 & \cellcolor{red!25} 0.830 & \cellcolor{red!25} 0.299 &  \cellcolor{yellow!25}0.210 &  \cellcolor{yellow!25}0.193  \\
		\bottomrule
	\end{tabular}
\end{table*}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{./images/learning_low_frequencies.pdf}
%     \caption{\textbf{FourieRF progressively learning frequencies.} In contrast with Vanilla TensoRF, our method learns clean low-frequencies for the scene at play. As we increase the frequencies used, no artifacts are introduced. }
%     \label{fig:increasing-frequencies-progression}
% \end{figure}

\vspace{-1em}\paragraph{1D Features.}

For 1D features we have $\textbf{v} \in \mathcal{R}^d$, a $d$-dimensional feature. At time $t$, given a feature threshold $f_t$ we perform the following operations. The feature $\textbf{v}$ is projected into the Fourier space, then the Fourier coefficients are clipped using the threshold $f_t$; finally, we apply the inverse Fourier transform,
%
\begin{equation}
\hat{\textbf{v}} = \textbf{IFFT}(\textbf{FFT}(\textbf{v}) \odot \alpha(f_t))
\end{equation}
%
Call $d_f$ the dimension of $\textbf{FFT}(\textbf{v})$, then in practice the mask $\alpha(f_t)$ corresponds to an array of the same dimension where we keep cells up to index $t_\alpha(f_t) = d_f \times f_t$. Each cell $i$ is given by,
%
\begin{equation}
\alpha_i(f_t) = \begin{cases}
    1 \text{ if, } i <  t_\alpha(f_t)\\
    t_\alpha(f_t)- \floor*{t_\alpha(f_t)} \text{ if, } i = t_\alpha(f_t) \\
    0 \text{ otherwise} \\
\end{cases}
\end{equation}
%
The same 1D feature parameterization is applied to both, the CP and the VM decompositions.

\vspace{-1em}\paragraph{2D Features.}

For 2D features, we have $\textbf{w} \in \mathcal{R}^{d_1\times d_2}$ a matrix feature. We proceed as above,
%
\begin{equation}
\hat{\textbf{w}}= \textbf{IFFT}(\textbf{FFT}(\textbf{w}) \odot \beta(f_t))
\end{equation}
%
The difference mainly lies in the mask $\beta(f_t)$. In this case, we define a 2D mask with a circle centered at 
%
$c=\floor*{\frac{d_1}{2}},\floor*{\frac{d_2}{2}}$, of radius
$r=\frac{f_t}{2}\sqrt{2 \max(d_1,d_2)^2}$
%
This ensures that when $f_t$ reaches 100\% all parameters are used. The values outside of the circle are set to 0 to clip the corresponding coefficients. The 2D feature parameterization is only used in the VM decomposition.

\vspace{-1em}\paragraph{Progressive Inclusion of Coefficients.}
To smoothly control the frequency of our feature grid decomposition, we define a mask to clip the corresponding Fourier coefficients and progressively increase the frequency using a clipping threshold. 
%Given that our clipping is done by a real number we can smoothly control the frequency of our feature grid decomposition. 
The illustration in Fig.~\ref{fig:teaser} shows the progressive increase effects. 
When setting the clipping parameter, it is important to keep it sufficiently low initially to ensure the correct coarse geometry. Examples of successful choices of clipping can be seen in Fig.~\ref{fig:coarse-geometry-extraction}. 
%In practice, one must set the clipping parameter sufficiently low to obtain correct coarse geometry, some examples of successful choices of clipping are displayed in Fig.~\ref{fig:coarse-geometry-extraction}. 
In our method, we initially start with only $f_0=0.01\%$ of Fourier coefficients and then linearly increase the clipping parameter during training. To be specific, we update it every iteration as follows:
%
$
f_t = f_{t-1} + \Delta, \text{with } \Delta = \frac{1-f_0}{N}
$
%
Where $t$ is the iteration number, $f_0$ is the initial clipping, and $N$ is the number of iterations. The update is applied at the start of every iteration, before any gradient is accumulated, thus avoiding any differentiability problems.

\vspace{-1em}\paragraph{View Dependence}

In the few-shot learning setting, it is extremely challenging to learn view-dependent information. We hypothesize that the model can use directional information ($d$) and positional encodings to overfit to a limited set of views. In practice, we have found it sufficient to restrict the directional information provided to the model. We adopt a similar approach to ZeroRF~\cite{shi2024zerorf} by using a simplified decoder that \textit{does not} use positional encodings for features or view directions.

% View dependent
