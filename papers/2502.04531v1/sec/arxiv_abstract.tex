\begin{strip}
\vspace{-1.3cm}
\centering
\captionsetup{type=figure}
\includegraphics[clip,width=\linewidth]{fig/figure1-f.pdf}
% \vspace{1.6cm}
\captionof{figure}{\textbf{Execution of the \ourmethod approach by the robot.} (1) Given a language description of a placement task, the robot first captures an RGBD image of the scene using its eye-in-hand camera. (2) A large segmentation model and a Vision Language Model (VLM) are then used to segment objects and suggest possible placement locations. (3) Multiple placement poses are predicted for objects around these suggested locations. (4) The robot is then able to insert a vial into different holes on the vial plate.}
\label{fig:teaser}
\end{strip}

\begin{abstract}


Object placement in robotic tasks is inherently challenging due to the diversity of object geometries and placement configurations. To address this, we propose AnyPlace, a two-stage method trained entirely on synthetic data, capable of predicting a wide range of feasible placement poses for real-world tasks. Our key insight is that by leveraging a Vision-Language Model (VLM) to identify rough placement locations, we focus only on the relevant regions for local placement, which enables us to train the low-level placement-pose-prediction model to capture diverse placements efficiently. For training, we generate a fully synthetic dataset of randomly generated objects in different placement configurations (insertion, stacking, hanging) and train local placement-prediction models. We conduct extensive evaluations in simulation, demonstrating that our method outperforms baselines in terms of success rate, coverage of possible placement modes, and precision. In real-world experiments, we show how our approach directly transfers models trained purely on synthetic data to the real world, where it successfully performs placements in scenarios where other models struggle -- such as with varying object geometries, diverse placement modes, and achieving high precision for fine placement. More at: \href{https://any-place.github.io}{\textcolor{blue}{any-place.github.io}}.

\end{abstract}