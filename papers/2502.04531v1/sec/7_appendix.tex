
\clearpage  

\section*{Appendix}

\subsection{Language Prompts for Molmo and Predicted Placement Visualization}
 In Figure \ref{fig:prompt} and Figure \ref{fig:prompt_sim}, we show additional language prompts used by the VLM to predict placement locations in both real-world and simulation experiments. Based on our observations, the Molmo VLM accurately identifies the correct placement locations in both real-world and simulated images based on the language input. Even when the predicted location is not perfectly centered, our low-level pose prediction model still works. For instance, when predicting the placement position of the bottle on the top shelf, the location may be at the edge. However, our model focuses on the local region centered at the proposed placement location and provides a pose that allows the robot to execute the task successfully.


\subsection{Additional Details on Model Training and Dataset}
In Table \ref{tab:model_train}, we list all the parameters used for training the AnyPlace low-level pose prediction models. Table \ref{tab:dataset} presents statistics on the number of placements generated using our synthetic dataset generation pipeline. Notably, by focusing only on the region of interest for placement pose prediction, AnyPlace models perform well across different placement tasks and generalize to unseen objects. They are trained with fewer than 2,000 samples per task, demonstrating the effectiveness of our design.

\begin{table}[h!]
    \centering
    \normalsize
    \caption{Parameters for Model Training}
    \renewcommand{\arraystretch}{1.2}
    \label{tab:model_train}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\ 
        \midrule
        Diffusion steps & 5\\
        Number of training iteration & 500k\\
        Batch size & 48 \\ 
        Optimizer & AdamW \cite{loshchilov2019decoupledweightdecayregularization}\\ 
        Learning rate & \( 1 \times 10^{-4} \) \\ 
        Learning rate schedule & Linear warmup and cosine decay \\ 
        Warmup epochs & 50 \\ 
        Weight decay & 0.1 \\ 
        Optimizer momentum & \( \beta_1 = 0.9 \), \( \beta_2 = 0.95 \) \\ 
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \normalsize
    \centering
    \caption{Dataset Size for Each Placement Task}
    \renewcommand{\arraystretch}{1.2}
    \label{tab:dataset}
    \begin{tabular}{@{}l c@{}}
        \toprule
        \textbf{Placement Task} & \textbf{Number of Placements} \\ 
        \midrule
        Hanging & 1,767\\
        Stacking & 1,696\\
        Vial Insertion & 1,107 \\ 
        Other Insertion & 800\\ 
        \bottomrule
    \end{tabular}
\end{table}


\subsection{Additional Details on Model Architecture}
For our diffusion-based placement prediction model, we leverage a transformer architecture. We use self-attention layers to extract point cloud features for individual objects and cross-attention to learn joint features for placement pose prediction. Details of the model architecture are listed in Table \ref{tab:model_arch}. 



\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/apx_prompt.png}
    \caption{\textbf{Additional Language Prompts and VLM Output Visualization in the Real-World Evaluation.} }
    \label{fig:prompt}
\end{figure*}


\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.75\textwidth]{fig/apx_prompt_sim.png}
    \caption{\textbf{Language Prompts and VLM Output Visualization in the Simulation Evaluation.} In our simulation experiments, we use the VLM to generate placement locations on RGB images from the simulator. Based on this proposed location, we crop the point clouds and input them into our placement pose prediction model.}
    \label{fig:prompt_sim}
\end{figure*}



\begin{table*}[h]
    \centering
    \normalsize
    \caption{Summary of Model Architecture}
    \label{tab:model_arch}
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \textbf{Model Component} & \textbf{Details} \\ 
        \midrule
        \textbf{Model Total Parameters} &  4,279,688 \\ 
        \textbf{Number of Heads} & 1 \\ 
        \textbf{Number of Self-attention Blocks} & 4 \\ 
        \textbf{Number of Cross-attention Blocks} & 4 \\ 
        \textbf{Point Cloud Feature Dimension} & 258 (256 + one-hot embedding) \\ 
        \textbf{Transformer Feature Dimension} & 256 \\ 
        \midrule
        \multicolumn{2}{l}{\textbf{Encoder}} \\ 
        \midrule
        Self-Attention & Multi-Headed Attention (1 head) \\ 
        Feedforward Layer & Linear (258 $\to$ 256), ReLU, Linear (256 $\to$ 258) \\ 
        Normalization & LayerNorm \\ 
        \midrule
        \multicolumn{2}{l}{\textbf{Decoder}} \\ 
        \midrule
        Self-Attention & Multi-Headed Attention (1 head) \\ 
        Cross-Attention & Multi-Headed Attention (1 head) \\ 
        Feedforward Layer & Linear (258 $\to$ 256), ReLU, Linear (256 $\to$ 258) \\ 
        Normalization & LayerNorm \\ 
        \bottomrule
    \end{tabular}
\end{table*}