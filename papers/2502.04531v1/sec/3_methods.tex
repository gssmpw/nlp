\section{\ourmethod: Generalized Object Placement}




To enable a robot to execute diverse object placements in a scene, we propose breaking the placement pose prediction problem into two subtasks: a high-level placement location proposal task and a low-level fine-grained placement pose prediction task. For the high-level task, we incorporate a vision-language model, trained to output 2D keypoint locations in an image based on a given text prompt.
A small local region around the candidate placement location can then be extracted and forwarded to the low-level pose-prediction model.
This simplifies the low-level pose-prediction problem significantly by having a much smaller pointcloud as input and helps with generalization as any features outside of the local region do not influence the prediction.
This allows us to focus on a limited set of general placement types and utilize a fully synthetic dataset, but have the final model be effective in a broad range of real-world placement tasks.

\noindent \textbf{Problem setup.}
We formulate the object placement task to predict relative transformations. Specifically, given an input tuple \(\{D, I\}\), where \( D \) represents the language description of the placement task and \( I \) is an RGBD image of the scene, our goal is to predict a set of rigid transformations \( \{T_n\}_{n=1}^{N} \subset \mathrm{SE}(3) \) that move the target object \( C \) from its current position to all viable placement locations on the base object \( B \) that satisfy language conditioning \( D \).  Assuming the grasping poses \( T_{\text{grasp}} \) are provided by a grasp prediction model, the final end-effector pose \( T_{\text{place}} \) can be computed using the predicted relative transformation between the initial object pose and its final placement pose as $T_{\text{place}} = T_n T_{\text{pick}}.$

\subsection{VLM-guided placement location prediction}
\label{subsec:detection_pipeline}
When objects have multiple potential placement locations, existing models often struggle to capture all these discrete positions when trained in an end-to-end manner. To address this, we propose leveraging recent advancements in VLMs, which have demonstrated strong capabilities in localizing points and regions within images based on language descriptions, to directly identify placement locations. Specifically, given a language description of the placement task \( D \) and a RGBD image \( I \), we aim to extract the pointcloud of the target object \( \mathcal{P}_{\text{c}}\) and a local region of interest of the base object \( \mathcal{P}_{\text{b\_crop}}\), where \( \mathcal{P}_{\text{c}}, \mathcal{P}_{\text{b\_crop}} \in \mathbb{R}^{N \times 3} \). We utilize Molmo \cite{deitke2024molmopixmoopenweights} to detect all potential placement locations as keypoints in image space, such as specifying all positions where a vial can be inserted into a vial plate. This approach enables the low-level pose prediction model to focus on learning the different placement configurations of two objects and predicting the placement pose without needing to explicitly capture multiple placement locations.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth, trim={0 10mm 0 0}, clip]{fig/vlm-prediction-001.pdf}
    \caption{\textbf{Language Prompts and VLM Output Visualization in the Real-World Evaluation.} In the real-world setting, we prompt the VLM to predict placement locations on a variety of different objects. Combined with our low-level pose prediction model, we demonstrate strong generalization and flexibility in real-world experiments. The red dot on each image is the predicted pixel location from the Molmo VLM. }
    \label{fig:vlm-prediction}
\end{figure}

First, we extract segmentation masks for the target object \( C \) and the base object \( B \). To achieve this, we first query the VLM to get point locations in the image for each object, then pass those points to the segmentation model to get segmentation masks.
This allows us to have complex language conditioning in our object selection (e.g., "blue vial", "vial rack in front of the scale", etc.).
%
Next, we query the VLM again on the base object \( B \) to find all the discrete modes for the placement (\autoref{fig:vlm-prediction}). For each identified point in the image, we extract the local region in the pointcloud and use that as input to our pose prediction model.

Empirically, we found that explicitly identifying placement modes, rather than relying on models to explore placements across the entire object, is more reliable and practical when handling diverse objects with multiple possible placement poses. Additionally, since our high-level module is built on a general-purpose VLM, the system can handle diverse placements and perform complex language conditioning.
In particular, this enables us to have complex language conditioning in: (1) selecting which object to place; (2) selecting which object to place it onto; and (3) selecting where on that object to perform the placement.
It also enables our method to function effectively even when the object being placed onto is large or difficult to segment. This is because the module always outputs a small local region around the potential placement location, regardless of the object's size.


\subsection{Fine-grained placement pose prediction}
\label{subsec:placement_model}
Given pointclouds \( \mathcal{P}_{\text{c}} \) and \( \mathcal{P}_{\text{b\_crop}} \) from the high-level module, the low-level pose prediction model only focuses on learning different local placement arrangements, without the need to capture the distribution of different placement locations. Our intuition is that, with the aid of our large synthetic dataset, the model should effectively capture key representations of diverse placement configurations based on object geometry, which enables it to generalize to unseen objects and remain robust to noisy data.  Having only a local region as input, the pose prediction model should be able to achieve better precision, which is crucial in many relevant placement tasks.

 We predict the relative transformation in a diffusion process with a discrete number of timesteps. Starting with two object pointclouds, the diffusion process iteratively denoises the relative transformation, gradually moving the object being placed toward its final pose. Initially, we transform the object pointcloud \( \mathcal{P}_{\text{c}} \) with a random transformation \( T_{\text{init}} = (\mathbf{R}, \mathbf{t}) \) to get \( \mathcal{P}_{\text{c}}^{\text{(0)}} \), where R is randomly sampled over the SO(3) space, and t is sampled within the bounding box of the cropped placement region:
\begin{equation}
\mathcal{P}_c^{(0)} = T_{\text{init}}\, \mathcal{P}_c,
\end{equation}
where
\begin{equation}
T_{\text{init}} = (\mathbf{R}, \mathbf{t}),
\end{equation}
\begin{equation}
\mathbf{R} \sim \mathcal{U}(\mathrm{SO}(3)), \quad \mathbf{t} \sim \mathcal{U}(\text{bbox}(\mathcal{P}_{b\_crop})).
\end{equation}

 As shown in \autoref{fig:block_scheme}, at each denoising timestep \( t \), \( \mathcal{P}_{\text{c}}^{\text{(t)}} \) and \( \mathcal{P}_{\text{b\_crop}}\) are input into the encoder. Specifically, both pointclouds are firstly downsampled to \SI{1024} points using Farthest Point Sampling (FPS) and normalized to the size of a unit cube. The downsampled pointclouds are passed through a linear layer to extract latent features, which are subsequently concatenated with a one-hot vector used to identify the corresponding pointcloud. These combined features are then processed by the Transformer encoder \cite{10.5555/3295222.3295349, chen2022neural}, where self-attention layers are applied to effectively extract features from the pointclouds. We then leverage cross-attention and pooling layers to further aggregate these features, producing a unified feature representation that captures the spatial relationship between the two objects. In the decoder, we first obtain the sinusoidal positional embedding of the diffusion timestep \(t\). Finally, the joint pointcloud feature representation, along with the encoded timestep, is fed into MLP layers to predict the relative transformation \( {T}_{n}^{(t)} \) consisting of a rotation \( \mathbf{R} \in \text{SO}(3) \) and a translation \( \mathbf{t} \in \mathbb{R}^3 \) for refining the object's pose. The target object pointclouds are then transformed accordingly before proceeding to the next denoising step as $ \mathcal{P}_{\text{c}}^{\text{(t-1)}} =  {T}_{n}^{(t)} \mathcal{P}_{\text{c}}^{\text{(t)}}$.
 The full transformation \( T_n \) taking the object from its initial location to the placement pose is the product of all the incremental transformations predicted through the diffusion steps: \( T_n = \prod_{t=1}^{t_{\max}} T_n^{(t)} \).
 More details on the model architecture can be found in the Appendix. 
 
 During training, we perform 5 denoising steps. Instead of incrementally adding Gaussian noise to the input during the forward process as is common practice \cite{10.5555/3495724.3496298}, we manually define the noise added at each timestep. In our case, the noise is the relative transformation that the model predicts. Specifically, the intermediate ground truth relative transformations \({T}_{n, GT}^{(t)} \) are generated by linearly interpolating the translation and using spherical linear interpolation (SLERP) to sample rotations between the object's initial and final placement poses. During inference, we generate diverse placement poses by sampling the diffusion model multiple times, each time starting with randomly transformed initial object pointclouds \( \mathcal{P}_{\text{c}}^{\text{(0)}} \).
We perform a larger number of denoising steps at test time, by repeating the last denoising step more times for a total of 50 denoising steps.


At each diffusion step during training, given the ground truth \({T}_{n, GT}^{(t)} \) and the predicted \({T}_{n}^{(t)} \), we use the L1 distance for the translation loss $L_{translation}$. For rotation, we measure the geodesic distance between the ground truth and predicted rotations. Additionally, we apply Chamfer loss between the pointcloud transformed by the ground truth pose and by the predicted pose. The total loss is the summation of these losses: 
%
\begin{equation}
    \mathcal{L}_{total}= \mathcal{L}_{translation} + \mathcal{L}_{rotation}  + \mathcal{L}_{chamfer}
\end{equation}



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth, trim={0 10mm 0 0}, clip]{fig/synthetic_dataset_isaac_sim_high_exposure.png}
    \caption{\textbf{ Generated objects in the synthetic dataset.} We use Blender to procedurally generate various categories of objects for three placement tasks: insertion, stacking, and hanging.
    }
    \label{fig:dataset}
\end{figure}


\subsection{Robot pick and place execution}
\label{subsec:placement_pipeline}
After determining the placement poses, we implement a pick-and-place pipeline to manipulate the object and position it accurately at the target pose. Specifically, we utilize AnyGrasp \cite{fang2023anygrasp} to find viable grasps for the target object \(C\) and employ cuRobo \cite{curobo_report23} as the motion planner to perform collision-free placement. We perform rejection sampling on $(T_{\text{place}}, T_{\text{pick}})$ pairs to identify valid grasps for the specific placement pose predicted by our model that can be executed by the robot.

Grasp detection begins by extracting the target object pointclouds \( \mathcal{P}_{\text{c}} \) from an RGBD image. AnyGrasp \cite{fang2023anygrasp} then processes the resulting pointclouds to identify the optimal grasp candidates sorted by confidence. To pick up the object, the gripper is first moved to a pre-grasp pose, positioned 10 centimeters away from the target along the gripper's z-axis. The gripper then approaches the target in a straight line while maintaining its orientation. Similarly, during placement, the robot first moves to a pre-place pose, followed by a final approach without altering the gripper orientation. The distance from the pre-place pose to the final placement pose is adjusted according to the object's size to avoid collisions during the transition to the pre-place position. With the waypoints and end-effector orientation constraints defined, we use cuRobo to generate the complete motion plan for robot pick and place.  



\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig/tasks-000.pdf}
    \caption{\textbf{Robot performing various placement tasks in simulation.} In the simulation, given a set of predicted placement poses and grasping poses, we leverage cuRobo for motion planning and execute all trajectories simultaneously in IsaacLab.
    }
    \label{fig:example_placements}
\end{figure*}
\section{Synthetic Dataset Generation}
\label{subsec:synthetic_dataset_generation}
Our aim in building the synthetic dataset is to capture a broad range of local placement arrangements.
Existing models use a few very specific tasks (e.g. inserting a book into a bookshelf) to simply evaluate how well their model works given placement data for such task for training \cite{yuan2023m2t2, 10160736, you2021omnihang}.

Our goal on the other hand is not only to use the dataset to evaluate the proposed model, but to build towards representing a broad range of types of placements (stacking, hanging, inserting) as shown in \autoref{fig:dataset}.
The local nature of our pose-prediction model makes this task much easier and enables us to build a dataset that can generalize to a broad range of real-world placement tasks.

The data generation pipeline consists of two main components: object generation and placement pose generation. Specifically, we use Blender to procedurally generate 3D objects, such as pegs, holes, cups, racks, beakers, vials, and vial holders. Object parameters—including height, width, length, and number of edges—are randomized to increase variability. For racks and vial plates, we also randomize the number of poles and holes. Additionally, random scaling is applied along the x, y, and z axes to further enhance diversity. 

To identify stable placements for objects, we use NVIDIA IsaacSim
to determine object placement poses for three configurations: stacking, inserting, and hanging. At the start of each trial, two objects are randomly sampled and loaded into the simulation. Since all objects are procedurally generated and possible placement locations are known during generation (e.g., the center of each hole on a vial plate), the ideal object placement location for the placing object can easily be determined.
This approach finds object placement locations that maximize the clearance between the objects in their final placement configurations. For object placement rotations, they are then randomly sampled along their axis of symmetry to explore various placement poses.  Four cameras are set up to capture dense object pointclouds and render RGBD images. We believe that our dataset covers a wide range of placement scenarios encountered in real life. In total, \SI{1489}{} objects across \SI{13}{} categories were created, and \SI{5370}{} placement poses were generated.




\subsection{Training details}

We train each model independently on subsets of the full dataset, split based on the type of placement configurations, as well as on the combined dataset (referred to as the "multitask" variant). While similar methods are typically trained for a single task (e.g., \cite{simeonov2023rpdiff}), our goal is to build a single model capable of performing a diverse set of placement tasks. All single-task models are trained for three days, while multitask models are trained for five days on a single NVIDIA A100 GPU. More details on training parameters are in the Appendix. 

