\section{Introduction}

Placing objects is a fundamental task that humans perform effortlessly in daily life, from setting items on a table to inserting cables into sockets. On the other hand, enabling a robot to perform such tasks can often be highly challenging. The challenges arise from the various constraints of different placement tasks and the difficulty of generalizing to unseen objects. Existing methods are often task-specific, using a large number of demonstrations for a single placement task, such as hanging objects on racks \cite{simeonov2023rpdiff}, with the hope that the robot can generalize to unseen objects. Alternatively, few-shot approaches focus on learning object placement with only a few demonstrations, aiming for the model to replicate the same placement operation across random initial configurations of similar objects and setups.

Learning diverse object placements presents significant challenges for existing models, primarily due to the difficulty of generalization. Generalization can be categorized into two aspects: object-level and task-level. Object-level generalization focuses on developing robust representations of various objects and placement configurations, enabling the model to handle unseen objects effectively. Task-level generalization involves the model's ability to predict diverse placement configurations from given pointclouds. Additionally, predicting multimodal placement outputs—encompassing a range of valid locations and modes—remains challenging, particularly when multiple feasible solutions exist. For instance, a robot inserts a vial into one of many empty slots on a vial plate. Existing methods \cite{simeonov2021neuraldescriptorfieldsse3equivariant, 10160423, pmlr-v205-simeonov23a, pan2022taxpose, ryu2023diffusionedfsbiequivariantdenoisinggenerative, ryu2023equivariantdescriptorfieldsse3equivariant, gao2024riemann, eisner2024deep, huang2024imagination, huang2024matchpolicysimplepipeline} often operate within a few-shot learning framework, where they learn specific placements involving two particular objects, resulting in poor generalization to different tasks and object types. In contrast, RPDiff \cite{simeonov2023rpdiff} has trained separate models for three placing scenarios (e.g. cup on a rack, book on a shelf, can stacking) using a relatively larger dataset, demonstrating some level of in-class object-level generalization, but its generalization to new tasks has not been studied.   


In this work, we address generalizable object placement that is robust to different objects and capable of predicting diverse placement poses across various tasks, similar to general-purpose object grasping methods. To achieve this, we have developed a fully synthetic dataset that contains 1,489 generated objects and captures three common placement configurations: inserting, stacking, and hanging. Based on this dataset, we develop a placement prediction pipeline that consists of a high-level placement position proposal module and a low-level placement pose prediction model. For the high-level module, we leverage a large segmentation model SAM-2 \cite{ravi2024sam} to segment objects of interest and prompt the Molmo VLM \cite{deitke2024molmopixmoopenweights} to propose all possible placement locations. Only the region around the proposed placement is fed into the low-level pose prediction model. By first providing a rough region for the low-level module to focus on, the model can effectively generalize across objects, learn their geometry, and capture diverse placement configurations. For the low-level pose prediction models, we build upon diffusion to predict precise and multimodal placement poses. We demonstrate the effectiveness of our methods across different placement tasks in simulation, where we significantly outperform baseline models in terms of pick-and-place success rate and placement location coverage. In real-world evaluation, our model achieves an 80\% success rate on the vial insertion task, demonstrating robustness to noisy data and generalization to unseen objects. 



To summarize, the key contributions of our work are: 
\begin{enumerate}
  \item We propose a novel object placement approach that leverages a VLM to reason about potential placement locations and a low-level pose prediction model to predict placement poses based solely on the region of interest.
  \item We developed a fully synthetic dataset containing thousands of generated objects capturing a wide range of local placement configurations. This is crucial in enabling us to train models exclusively on synthetic data and be able to use them for a wide range of real-world placement tasks.
  \item We show that reducing the problem to only local prediction allows us to improve performance with respect to baseline methods in terms of success rate, precision, and mode coverage. It also crucially makes it possible for a model trained on a fully synthetic dataset to show generalization that enables it to directly be deployed to the real world on a variety of placement tasks.
\end{enumerate}

