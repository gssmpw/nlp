\section{Experimental Evaluation}


\begin{table*}[htbp]
    \normalsize
    \sisetup{detect-weight,     % <--
         mode=text,         % <--
         table-format=-0.4, % <--       
         add-integer-zero=false,
         table-space-text-post={*} % <--
         }

    \centering
    \caption{ \textbf{Success rate (\%) on synthetic dataset.} We evaluate the baseline NSM, RPDiff, AnyPlace-EBM, and our method, \ourmethod, on the success rates of four pick-and-place tasks using objects from the synthetic dataset. 
    }
    \label{tab:sim_sr}
        
        \begin{tabular}{l | l | S[table-format=3.2] S[table-format=3.2] | S[table-format=3.2] S[table-format=3.2]}
        \toprule
            &  \textbf{Methods}
            & \textbf{Object Stacking}
            & \textbf{Peg Insertion}
            & \textbf{Cup Hang}
            & \textbf{Vial Insertion} 
            \\
            &
            & \textbf{(single-mode)}
            & \textbf{(single-mode)}
            & \textbf{(multi-mode)}
            & \textbf{(multi-mode)} 
            \\
        \midrule
        \textbf{Single task} & NSM &                            76.57 & 7.63 & 35.54 & 18.70 \\
        \textbf{training} & RPDiff &                          \B 80.34 & 22.94 & 92.02 & 16.51 \\
        & AnyPlace-EBM &              \B  80.04 & 8.44 & 91.57 &65.64 \\
        & \textbf{\ourmethod} &           \B  80.16 & \B 30.95 & \B 94.80 & \B 92.74 \\
        \midrule
        \textbf{Multi-task} & NSM  &                  77.55 & 7.69 & 35.22 & 9.87 \\
        \textbf{training} & RPDiff  &                \B 80.21 & 22.33 & \B 94.05 & 24.26 \\
        & AnyPlace-EBM  &     78.95 & 10.75 & 90.87 & 57.24 \\
        & \textbf{\ourmethod }  &     78.28 & \B 24.99 &  \B 94.12 & \B 75.25 \\
    \bottomrule
    \end{tabular}
\end{table*}


We aim to evaluate three key aspects of our approach: success rate, coverage, and precision. With the VLM-based placement location prediction, our method effectively captures possible placement locations, achieving higher coverage rates. By narrowing the focus to minimal regions for placement pose prediction, we enable the model to achieve greater precision, which is essential for handling challenging placement tasks in real-world scenarios. All models are trained exclusively on our synthetic dataset and evaluated on novel objects in a zero-shot setting. We conduct evaluations against three baseline models on different placement tasks in both simulation and real-world settings. Simulation enables extensive testing of our approach by leveraging rejection sampling across multiple parallel environments. In real-world experiments, we highlight the effectiveness and flexibility of our VLM-based placement location prediction in handling complex environments. Additionally, we demonstrate the robustness of our method in handling pointcloud noise and generalizing to unseen objects, despite being trained on general synthetic objects in simulation and encountering novel global configurations for similar types of local placements.



\noindent \textbf{Evaluation metrics.}
We use three metrics to evaluate model performance: success rate, coverage, and precision. Specifically, we define a placement as successful if the robot places the object at the correct location and it remains stable after release. The success rate is then calculated as the number of successful placements divided by the total number of trials. To better understand the diversity of multimodal outputs in placement prediction, we evaluate coverage, defined as the number of distinct predicted placement locations relative to the total number of possible locations. Finally, for fine placement tasks, to evaluate the precision, we measure the error between the ground truth pose and the predicted pose in terms of both distance and angle.



\noindent \textbf{Baselines.}
We compare our approach with three baselines: NSM \cite{chen2022neural}, RPDiff \cite{simeonov2023rpdiff}, and an energy-based model (EBM) integrated with our high-level placement location prediction module. Specifically, NSM shares the same self-attention and cross-attention pointcloud encoder, paired with a simple regression decoder. For RPDiff, since the low-level pose-prediction module in our diffusion-based approach shares the same structure, this allows us to directly examine the effects of the high-level module we propose.
We do not utilize a learned classifier on top of the pose prediction model that is present in RPDiff. Such a model can be applied on top of any of the methods we evaluate here, including the ones we propose. Not having it also allows us to evaluate the number of samples needed to achieve particular coverage of possible placement locations in the scene.
To evaluate the effectiveness of the diffusion decoder in generating multimodal outputs, we build an energy-based model, AnyPlace-EBM, for comparison. Inspired by Implicit-PDF \cite{implicitpdf2021}, this model uses the same encoder as ours, but for the decoder, instead of explicitly predicting the placement pose, it includes two separate branches: one for placement location prediction and another for predicting the placement rotation energy. During training, we encourage all placement rotations in SO(3) that result in stable placements to have low energy. During inference, we randomly sample thousands of rotations and select the one with the lowest energy as the final placement rotation. 



\subsection{\textbf{Placement Success: Single \& Multi-Modal}}
For evaluating all the methods in simulation, we utilize the pick-and-place execution pipeline we constructed (\autoref{subsec:placement_pipeline}), together with simulation in IsaacLab \cite{mittal2023orbit}.
The pipeline itself can be used as a standalone module. It makes no assumptions about the placement prediction method and is not specifically tied to other modules in our approach.
This enables it to be used as a general system for evaluating placement pose prediction models.
In \autoref{fig:example_placements} we show the system being used to perform insertion, hanging, and placement tasks.
Combined with the synthetic dataset we create, this gives us a complete system for comparing different models and getting systematic results of success rate, mode coverage, and precision.
Both motion planning and the pick-and-place simulation itself are GPU parallelizable, making evaluation of new models even easier.


We evaluate single-mode and multimodal placement across a set of 4 tasks. 
\texttt{Object stacking} and \texttt{peg insertion} tasks are Single-model placement since only one solution is feasible. While \texttt{cup hang} and \texttt{vial insersion} are multi-modal in nature due to the existence of multiple placement locations. We show a summary of placement experiments in these simulated domains in  \autoref{tab:sim_sr}.


We first examine single-mode tasks. 
For the simple stacking task, where high precision in placement poses is not required, \ourmethod and baseline models, including RPDiff and AnyPlace-EBM, achieve a similar success rate of around 80\%.
For the peg-in-hole task, which requires high precision in placement pose prediction, \ourmethod surpasses the baseline models by a large margin. 

Multi-mode tasks are where we expect to get full benefit from our approach.
In the hanging task the tolerance for placing a cup on the rack is relatively high. \ourmethod achieves a 94\% success rate in both single-task and multi-task settings, while RPDiff and AnyPlace-EBM perform slightly worse in comparison. Notably, NSM has the lowest success rate across all multi-mode tasks due to its inherent limitations as a regression model.
In the more challenging vial insertion task, \ourmethod achieves the highest success rate of 92.74\%, while the success rates of all baseline models drop significantly, with RPDiff reaching only 16.5\%.

This demonstrates that relying on high-level VLM to propose possible placement locations and focusing solely on the local region for placement prediction, simplifies the task for low-level pose prediction models and enables them to better capture fine-grained pointcloud features for high-precision placements. Additionally, despite using the same high-level placement location prediction, the energy-based model suffers a performance drop of 27\% and 18\% in single-task and multi-task settings, respectively, compared to the diffusion-based \ourmethod. This highlights that the iterative denoising procedure in diffusion models is more effective for high-precision placement prediction.



\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{fig/insertion_new.pdf}
    \caption{\textbf{Comparison of vial insertion coverage across different models.} We show how the coverage ratio progresses with an increasing number of placement trials. \ourmethod clearly outperforms all baseline models as it rapidly reaches its maximum coverage.
    }
    \label{fig:vialinsertion_coverage}
\end{figure}








\subsection{\textbf{Placement Coverage: Multi-Modal Insertion \& Hanging}}

Now we aim to investigate how the coverage changes with the increase in the number of samples taken from each model.
With how NSM and RPDiff are designed, we can only take independent samples from the model until as many possible placement modes are covered.
In the case of \ourmethod and \energyvlm, the high-level VLM module predicts possible placement locations, so we can perform sampling for each mode.
We perform an equal number of samples in each and compare performance for the same total number of samples for each model.
All models used in this evaluation are trained on the multitask dataset.


In \autoref{fig:vialinsertion_coverage} we show results for the vial insertion task. \ourmethod achieves close to its maximum performance after with a single sample at each mode, outperforming other models with 100 samples. This is due to its high placement success rate and the VLM's strong reasoning ability in identifying various possible placement locations. A similar trend is observed for the \energyvlm models; however, due to 
a lower placement success rate, the model plateaus at around 73\%.
In contrast, RPDiff, despite being a diffusion-based model that should capture multimodal outputs, fails in this regard, with coverage fluctuating below 10\%. Even the NSM regression model outperforms it in this case, but still stays far below either of the methods utilizing the high-level VLM module.



For hanging, to further evaluate the effect of different objects on predicted placement coverage, we aim to assess how these models generalize to racks with different sizes, geometries, and spacing between the sticks. Specifically, we generate new racks with different physical parameters compared to those used during training. The success rates are shown in \autoref{tab:hanging_success_rate}, and the coverage is plotted in \autoref{fig:hanging_coverage}.
Similar to the vial insertion task, \ourmethod outperforms other baselines, consistently getting higher coverage with fewer samples and eventually reaching 100\% coverage -- made possible with the exceptional performance of the VLM module in finding the placement modes. RPDiff performs better in this case, but still struggles to cover all placement locations, with coverage saturating around 90\%. This illustrates that the original RPDiff does not generalize well to objects with different geometries and sizes compared to the training set. In contrast, \ourmethod, aided by high-level placement prediction, allows the low-level prediction model to focus on learning placements based on local regions, thereby enabling strong generalization across varying object sizes and shapes.


\begin{figure}[!t]
    \centering
    %\includegraphics[width=\columnwidth]{fig/hanging_coverage.png}
    \includegraphics[width=\columnwidth]{fig/hanging_new.pdf}
    \caption{\textbf{Comparison of hanging coverage across different models.} \ourmethod achieves nearly perfect coverage by leveraging the VLM's ability to propose possible placement locations, whereas other baseline models fail to capture all placement modes.
    }
    \label{fig:hanging_coverage}
\end{figure}

\subsection{\textbf{Placement Precision: Fine-Grained Insertion}}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/cumulative_errors_4x4-003.pdf}
     \caption{
     \textbf{Distance and angle errors on insertion tasks.} We plot histograms of the translation and rotation error in pose prediction for each approach. We base the error on the position and rotation to the closest viable placement pose. (MT) denotes a model trained on multi-task data.}
    \label{fig:precision-combined}
\end{figure*}
The final evaluation we conduct in simulation focuses on assessing the precision of each approach. Precision is a critical factor in the successful completion of many placement tasks. Our goal is to determine whether providing models with a much smaller input region can enhance the precision of placement pose estimation. To achieve this, we evaluate the output of the prediction model directly rather than analyzing the results of the placement in simulation.

In \autoref{fig:precision-combined}, we show the distribution of distance errors for each approach on the insertion tasks. Within model outputs that predict approximately the correct location, \ourmethod produces smaller errors and does so more reliably than the baselines. We also present the corresponding results for rotation prediction.
Since the objects being placed are symmetric around the z-axis, we exclude yaw angle prediction from this comparison. As the initial pose is randomized and approaches cannot infer this orientation, all methods produce uniformly random yaw angle values. Given the minimal difference between the top and bottom sides of the object in this task, all approaches—except NSM, which performs the worst in both position and orientation—predict correct and flipped poses with roughly equal likelihood.
Both \ourmethod and the RPDiff baseline perform well in predicting the correct orientation. Notably, the AnyPlace-EBM model achieves only slightly lower performance compared to the diffusion-based RPDiff and outperforms the NSM baseline. This is significant, as it highlights the potential of non-diffusion-based models for tackling these types of problems in future research.

\begin{table}[!t]
    \normalsize
    \centering
    \caption{\textbf{Success rate of the hanging task across different models.} Using the evaluation procedure outlined in this section, we calculate the success rate of the hanging task on a rack with different object characteristics.  }
    \label{tab:hanging_success_rate}
    \begin{tabular}{l | c}
        \toprule
        Methods & Success Rate  \\
        \midrule
        NSM Multitask                & 37.26\% \\
        RPDiff Multitask          & 81.17\% \\
        AnyPlace-EBM Multitask       & 69.43\% \\
        \B \ourmethod Multitask         & \B 84.96\% \\
        \bottomrule
    \end{tabular}
\end{table}



\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig/real_exp_latest.pdf}
    \caption{\textbf{Robot performing various placement tasks in real-world environments.} In the real-world setting, with a simplified placement pipeline, the robot successfully executes a variety of placement tasks across a wide range of placement locations based on the prediction of the AnyPlace model. (A) We demonstrate peg-in-hole, cup-on-rack, and plate-on-rack tasks. (B) The robot places the bottle in different locations based on a language description. (C) The robot stacks rings of different radii. (D) The robot executes a long-horizon task where it places the lid on the pot and then places the pot on the stove.}
    \label{fig:lanaguage_real}
\end{figure*}


\subsection{\textbf{Real World Evaluation: Precise Multi-Modal Placement}}

\begin{table}[!t]
    \normalsize
    \centering
    \caption{ \textbf{Success rate of real robot executing vial insertion task.} Our model significantly outperforms baseline models in real experiments, demonstrating its ability to generalize to unseen objects and effectively handle noisy data.
    }
    \label{tab:real_sr_data}
    \begin{tabular}{l | c}
        \toprule
        Methods & Number of Success  \\
        \midrule
        NSM                    & 0/10 \\
        RPDiff                 & 0/10 \\
        AnyPlace-EBM          & 5/10 \\
        \B \ourmethod             & \B 8/10 \\
        \bottomrule
    \end{tabular}
\end{table}

We also evaluate the performance of our approach on various placement configurations with different objects in the real world. Specifically, for each scene, we capture a single RGBD image with a ZED Mini camera mounted on a Franka Emika Robot arm. We use exactly the same high-level pipeline for detecting local placement locations as we do in our simulation experiments. We also utilize the same models, trained purely on our synthetic dataset, for predicting the placement poses in a zero-shot manner. We use a simplified pipeline for executing the placement, using a specific grasp and performing placement inverse kinematics instead of a full motion planner. In addition, we do not utilize rejection sampling, but directly execute each trajectory on real. Our main goal is to evaluate whether enabling the model to predict only local placements would allow our models to generalize well to unseen objects in the real world even when only trained on our synthetic dataset. We perform systematic evaluations on the vial insertion task, with multiple possible placement locations. We also execute the approach on other language-conditioned placement variants in the real world, examples of which can be seen in \autoref{fig:lanaguage_real}. All prompts used by the VLM to predict placement locations can be found in the Appendix.

We use a standard vial rack used in chemistry labs and randomly block different holes with other vials or cover them with tape to test the flexibility of our high-level placement location prediction module. Overall, across \textit{10 trials}, the NSM and RPDiff fail to adapt to differences in this real-world task compared to the training conditions. \ourmethod, on the other hand, successfully completes placements, achieving the best performance, and inserts vials into 8 out of the 10 available empty spots in the vial rack.

\subsection{\textbf{Generalizable \& Language-Conditioned Placement: Applicability in Real-World Tasks}}

To evaluate the generalization of \ourmethod, we conduct experiments on a large variety of objects for different placement configurations. As shown in \autoref{fig:lanaguage_real} (A), our method enables the robot to successfully perform placements in different scenarios, such as inserting a peg into a hole, hanging a cup on an unseen rack, and placing a plate into different slots of a plate rack. For precise placement, we demonstrate in \autoref{fig:lanaguage_real} (C) that the robot can accurately stack rings of varying radii, showcasing its ability to handle fine-grained placement tasks. Beyond single-step placements, \ourmethod excels in long-horizon tasks. For instance, we instruct the robot to precisely place a lid on a pot before placing the pot on a stove. Our method ensures that placed objects are centered and properly aligned, highlighting its reliability in real-world tasks. To demonstrate the advantage of combining VLM-based placement location prediction and local placement pose prediction modules, we show that the robot is able to place a bottle at different locations on a shelf based on language descriptions, as shown in \autoref{fig:lanaguage_real} (B). The key advantage of \ourmethod lies in its ability to integrate VLM-based high-level placement prediction with local placement pose refinement. By focusing on local regions for placement prediction and leveraging a diverse synthetic training dataset, it allows \ourmethod not only to generalize to real-world objects with significantly different geometries but also captures complex placement configurations with greater accuracy and flexibility.

