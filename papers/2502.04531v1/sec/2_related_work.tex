\section{Related Work}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/block-scheme-combined-003.pdf}
    \caption{ \textbf{Overview of the \ourmethod placement pose prediction approach.} (1) High-level Placement Location Prediction: Given an input language description and an RGBD image, we leverage a VLM and a segmentation model to extract objects of interest. Next, we prompt the VLM  again to propose possible placement locations. Using camera parameters, we reproject the depth map into 3D and crop the region of interest centered on the proposed placement location. Full pointclouds of the objects to be placed, along with the cropped regions of the placement locations, are then fed into our low-level pose prediction model to output precise relative transformations for object placement. (2) Low-level Placement Pose Prediction: We use self-attention to extract pointcloud features and cross-attention to capture cross-object features. For the diffusion decoder, the diffusion timestep is encoded within the MLP layers before the model outputs the relative transformation.
    }
    \label{fig:block_scheme}
\end{figure*}



\subsection{Object pick and place in robot manipulation}
The problem of robot pick-and-place is typically formulated in two ways: object rearrangement and direct end-effector pose prediction. 
In object rearrangement, the goal is to train a model to predict the relative transformation of the object from its initial pose to its final placement pose. Assuming the grasping pose is generated by existing models, the final placing pose of the end-effector is calculated by multiplying the predicted relative transformation with the grasping pose. In this setting, many of the works focus on predicting explicit task-relevant features of both objects and then solving for the relative pose through optimization or regression. Specifically, the Neural Descriptor Fields (NDF) series of papers \cite{simeonov2021neuraldescriptorfieldsse3equivariant, 10160423, pmlr-v205-simeonov23a} learn the occupancy field of pointclouds as a representation. Then, a set of predefined keypoints attached to the placement object is used to interact with and query the occupancy feature field of the target object. By matching the queried features at each point to features collected during demonstrations, the optimal object transformation is determined. TaxPose \cite{pan2022taxpose} leverages transformer-based cross-attention to predict corresponding points between two objects and use differentiable singular value decomposition (SVD) to solve for the relative transformation. To guarantee the placement pose prediction model is robust to SE(3) transformations, i.e., SE(3)-equivariant, methods \cite{ryu2023diffusionedfsbiequivariantdenoisinggenerative, ryu2023equivariantdescriptorfieldsse3equivariant, gao2024riemann} explicitly predict perpoint type-0 and type-1 features for object point clouds and then solve an optimization problem to align these features into specific configurations based on demonstrations. All of these methods operate in a few-shot setting and can predict a single placement pose given two objects. It is crucial for models to capture and predict a distribution of placement poses, as not every placement pose is realizable by a robot due to its kinematic constraints.  RPDiff \cite{simeonov2023rpdiff}, by contrast, trains a transformer with a diffusion mechanism on a large dataset, gradually denoising the object placement pose. However, their experiments reveal that the coverage of possible placement locations is incomplete. The fixed-size cropping mechanism used during diffusion may also struggle to generalize to objects of varying sizes. Additionally, a recent study \cite{ding2024opendor} samples multiple stable placements in a simulation and employs a VLM to select the appropriate mode based on a language query. While these modes are discrete, each mode allows for the rotation of objects along their axis of symmetry, resulting in valid placement poses that form a continuous distribution. 

An alternative approach to the pick-and-place task is predicting the robot's end-effector pose directly. M2T2 \cite{yuan2023m2t2}, and Pick2Place \cite{10160736} focus on planar object placement in cluttered scenes. M2T2 \cite{yuan2023m2t2} employs a multi-task transformer with separate decoders to predict grasp poses and placement location affordance maps for each discrete bin of rotation. Other works, like Pick2Place \cite{10160736}, concentrate on predicting key end-effector poses to accomplish specific tasks. RVT \cite{goyal2023rvt} and RVT-2 \cite{goyal2024rvt} also utilizes a transformer, leveraging multiview RGB images of the scene to predict heatmaps for the robot's next end-effector location. Coarse-to-fine Q-attention \cite{james2022coarsetofineqattentionefficientlearning}, on the other hand, leverages the scene's voxel to identify the most interesting spatial point at the current resolution. This point becomes the voxel centroid for the next refinement step, enabling the model to gather more accurate 3D information.

\subsection{Multimodal prediction in robot manipulation}
Consider a mug that can be hung on a rack in various poses at different pegs. Multimodality of placement poses is common in manipulation. Recently advancements in behavior cloning methods, such as the IBC energy-based model \cite{florence2021implicit} and Diffusion Policy \cite{chi2023diffusionpolicy}, have attempted to capture this multimodality. For diffusion policy specifically, it learns to denoise action trajectories starting from a random noise distribution, enabling the model to capture the underlying distribution of possible solutions. Another approach to modeling these distributions is through Variational Autoencoders (VAEs). BeT \cite{shafiullah2022behaviortransformerscloningk} and VQ-Bet \cite{lee2024behavior}, for instance, trained a VQ-VAE \cite{oord2018neuraldiscreterepresentationlearning} to discretize and encode continuous actions into latent representations, making it easier to handle multimodal and high-dimensional behavior data. To train the entire policy based on image input, they employed a transformer-based model alongside the VAE decoder to predict sequences of actions. The authors demonstrated that their approach outperformed the Diffusion Policy \cite{chi2023diffusionpolicy} on common benchmarks.
