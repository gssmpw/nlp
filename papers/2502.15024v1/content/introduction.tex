\section{Introduction}
The stochastic block model (SBM) is among the most fundamental models in (social) network analysis and information theory, and has been intensively studied for decades \cite{holland1983stochastic,mossel2012stochastic,abbe2015exact,krzakala2013spectral,Abbe18Review}.
A fascinating phenomenon in the SBM is the sharp computational threshold for \emph{weak recovery} of its hidden community structure: efficient algorithms are known for achieving constant correlation with the hidden signal when the signal-to-noise ratio is above a certain threshold~\cite{coja2010graph,Decelle_2011,massoulie2014community,abbe2015community}, while no polynomial-time algorithms have been discovered below this threshold despite significant research effort.
This computational threshold is known as the \emph{Kesten-Stigum} threshold (KS threshold) in the statistical physics literature \cite{Decelle_2011}, and it is an important topic in both probability theory and theoretical computer science.

\paragraph{Kesten-Stigum threshold in the symmetric stochastic block model}
For simplicity, we focus on the following special case of the stochastic block model.
\begin{definition}[Symmetric $k$-stochastic block model $\SSBM(n,\frac{d}{n},\e,k)$]\label{def:ssbm}
Let $k\in\N^+$ be the number of communities, $d\in \N^+$ be the average degree of the graph, ${\e\in [0,1]}$ be the bias parameter, and ${n\in \N^+}$ be a multiple of $k$.
A graph $Y\in \Set{0,1}^{n\times n}$\footnote{For ease of notation, we will use the adjacency matrix and the graph interchangeably.} follows the symmetric $k$-stochastic block model distribution $\SSBM(n,\frac{d}{n},\e,k)$ if it is sampled in the following way: assign each vertex a label uniformly at random from $[k]$, then independently add edges with probability $(1+\frac{k-1}{k}\epsilon)\frac{d}{n}$ between vertices with the same label and with probability $(1-\frac{\epsilon}{k})\frac{d}{n}$ between vertices with different labels.
\end{definition}

Note that, when the bias parameter $\e=0$, the model reduces to \Erdos-\Renyi random graphs with average degree $d$, denoted by $\bbG(n,\frac{d}{n})$.

Given a graph sampled from $\SSBM(n,\frac{d}{n},\e,k)$, the most fundamental problem is to recover the hidden community labels of the vertices, or equivalently to recover the \emph{community membership matrix}
$M^\circ$ given by
\begin{equation*}
    M^\circ_{i,j} := \mathbf{1}_{x^\circ_i=x^\circ_j}-\frac{1}{k} \quad (i, j \in [n]),
\end{equation*}
where $x^\circ_i\in [k]$ is the label of the $i$-th vertex. In this work, we consider \emph{weak recovery} of $M^\circ$, that is to find a matrix $M$ which \emph{correlates} with $M^\circ$ in the following sense.

\begin{definition}[Recovery rate and weak recovery in the SBM]\label{def:weak-recovery}
    For any $\delta\in [0,1]$, an algorithm achieves recovery rate $\delta$ in the $k$-stochastic block model, if given a random graph sampled from $\SSBM(n,\frac{d}{n},\e,k)$, it outputs a nonzero matrix $M\in \R^{n\times n}$ such that with constant probability,
    \begin{equation*}
        \iprod{M,M^\circ}\geq \delta\normf{M}\normf{M^\circ}.
    \end{equation*}
    If the recovery rate $\delta$ satisfies $\delta\geq \Omega(1)$, then the algorithm is said to achieve \emph{weak recovery}.
\end{definition}


The difficulty of achieving weak recovery in the SBM appears to be closely related to the choice of parameters $\epsilon, d, k$.
In particular,  \cite{massoulie2014community,montanari15:_semid_progr_spars_random_graph} give polynomial-time algorithms for weak recovery above the KS threshold $\epsilon^2 d\geq k^2$.
On the other hand, while it is known that \emph{exponential-time} algorithms can achieve weak recovery below this threshold (when $k\geq 5$)~\cite{Banks2016InformationtheoreticTF}, it is widely believed that no polynomial-time algorithms exist that achieve weak recovery when $\epsilon^2 d < k^2$.

\paragraph{Rigorous evidence for average case complexity} To provide rigorous evidence for the innate hardness of weak recovery below the Kesten-Stigum threshold, one could follow two general approaches. 
The first is to construct a reduction from problems widely believed to be hard (such as \emph{planted clique}~\cite{brennan2019reducibilitycomputationallowerbounds,brennan2020reducibility} or \emph{learning with errors (LWE)} \cite{bruna2020continuouslwe,gupte2022continuous,tiegel2024improved}).
However, this approach is unlikely to be successful for our problem as no such reductions are known for (other) average-case problems with constant sharp statistical threshold.
The second approach is to prove \emph{unconditional} lower bounds that rule out certain classes of algorithms. For example, those based on sums of squares~\cite{barak2019nearly,kothari2017sum,jones2022sum,mohanty2020lifting}, statistical queries~\cite{blum2003noise,feldman2017general,Brennan2020StatisticalQA}, or low-degree polynomials~\cite{hopkins2017power,Hopkins17,Hopkins18,kunisky2019notes}.
As it appears that significant technical barriers have to be overcome to prove lower bounds against the former two classes for average-case problems with sharp statistical threshold, we focus on the latter.

\paragraph{The low-degree method} In recent years, the low-degree method has emerged as a standard tool for providing rigorous evidence for computational hardness in average-case problems \cite{Hopkins18,kunisky2019notes}.
Inspired by the fact that thresholding the likelihood ratio function provides optimal algorithms for hypothesis testing, the low-degree method provides a heuristic for average-case computational hardness by proving lower bounds against the low-degree projection of the likelihood ratio between two distributions from the hypothesis class.
In this paper, we focus on the implications of the following conjecture of~\cite{moitra2023precise} in the context of the SBM\footnote{The original conjecture is stated for the closely related spiked Wigner model.}.


\begin{conjecture}[Low-degree conjecture]
\label[conjecture]{conj:low-degree}
Let $P$ be a distribution from the $k$-stochastic block model and $Q$ be a distribution of \Erdos-\Renyi random graphs.
For functions $f : \R^{n \times n} \to \R$, consider the parameter 
\begin{equation} \label{eq:Rlamba}
\RPQ(f)\coloneqq \frac{\E_{Y\sim P} f(Y)-\E_{Y\sim Q} f(Y)}{\sqrt{\text{Var}_{Y\sim Q}(f(Y))}}.
\end{equation}

Suppose that for fixed constant $\delta\in [0,1]$, and every polynomial $f(\cdot)$ of degree at most $n^\delta$, we have $\RPQ(f)\leq O(1)$.
Then, for any function $f(\cdot)$ computable in time $\exp(n^{\delta})$ taking values in~$[0,1]$, we have \(\RPQ(f)\leq O(1)\)\,.
\end{conjecture}
The parameter $\RPQ(\cdot)$ in~\eqref{eq:Rlamba} is motivated by Le Cam's method: if the maximum of $\RPQ(f)$ over all computable functions $f$ is bounded by $O(1)$, no algorithm can distinguish between the distributions $P$ and $Q$ with high probability. Intuitively, \cref{conj:low-degree} tells us that, if the maximum of $\RPQ(f)$ is bounded over all low-degree polynomials, it is in fact bounded over all efficiently computable functions.

Assuming this conjecture for $P$ given by the symmetric SBM and $Q$ given by the \Erdos-\Renyi graph distribution, we aim to clarify the relation between the upper bounds for the \emph{low-degree likelihood ratio} (proved in ~\cite{Hopkins18,bandeira2021spectral}) and lower bounds for computationally efficient algorithms for the SBM.
Specifically, we address the following question:
\begin{question}
    Assuming Conjecture \ref{conj:low-degree}, can we rule out polynomial-time algorithms achieving weak recovery (or even non-trivial error rate) in the stochastic block model below the Kesten-Stigum threshold, when the number of communities is a universal constant?
\end{question}

\paragraph{Implications for learning stochastic block model} A potential computational-statistical gap similar to weak recovery can also be observed in the closely related problem of learning the parameters of the stochastic block model \cite{Xu2017RatesOC}.
Although \cite{luo2023computational} established a low-degree recovery lower bound for learning the probability matrix in the symmetric SBM, their result does not imply a lower bound for learning the parameters $d,\epsilon$, as their hard instance is given by a symmetric SBM with fixed parameters.
Moreover, when $k\lesssim \log(n)$ (rather than constant), their lower bound cannot rule out polynomial-time algorithms for achieving the minimax error rate. 

As such, we address the following question in this paper:
\begin{question}
    Assuming Conjecture \ref{conj:low-degree}, can we provide rigorous evidence for a computational-statistical gap in the error rate of learning the parameters of the stochastic block model? 
\end{question}




























 




























