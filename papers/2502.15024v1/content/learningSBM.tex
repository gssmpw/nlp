\section{Low-degree recovery lower bound for learning dense stochastic block model}
\label{sec:low-degree-reduction}
In this part, we give unconditional lower bound against low-degree polynomial estimators for the edge connection probability matrix in stochastic block model, via implementing reduction from hypothesis testing to weak recovery using low-degree polynomials.
For simplicity, we focus on the dense graph.
\begin{theorem}[Low-degree lower bound for learning]\label{thm:low-degree-graphon}
Let $n\in \N^+$ and $\ell\leq n^{0.001}$.
Let $d=\Theta(n)$.
    Let $\cF_{n,\ell}$ be the set of degree-$\ell$ polynomials mapping from $n\times n$ symmetric matrices to $n\times n$ symmetric matrices.
    Suppose $\thetanull\in [0,1]^{n\times n}, Y\in \Set{0,1}$ are edge connection probability matrix and adjacency matrix sampled from symmetric stochastic block model $\SSBM(n,\frac{d}{n},\e,k)$.
    Then for $k\leq n^{0.001}$, we have 
    \begin{equation*}
       \min_{f\in \cF_{n,\ell}}\max_{\e\in [0,1]} \E_{(Y,\thetanull)\sim \SSBM(n,\frac{d}{n},\e,k)} \normf{f(Y)-\thetanull}^2\geq \Omega(k\cdot n)\,.
    \end{equation*}
\end{theorem}

\subsection{Construction of the low-degree polynomial}
For simplicity, we define the community matrix of symmetric stochastic block model.
\begin{definition}[Community matrix for stochastic block model]\label{def:community-matrix}
    Under symmetric stochastic block model $\SSBM(n,\frac{d}{n},\e,k)$, we define the community matrix $X^\circ\in \Set{\pm 1}$ as following: $X^\circ(i,j)=1$ if vertex $i,j$ have the same community label and $X^\circ(i,j)=0$ otherwise.
\end{definition}

Given the polynomial function $f:\R^{n\times n}\to \R^{n\times n}$. We consider a graph with $2n$ nodes and randomly partition the nodes into two equal-sized sets $S_1$ and $S_2$.   
Let $X=\frac{n}{\e d} \Paren{f(Y_1)-\frac{d}{n}}$ where $Y_1$ is the subgraph induced by vertices in $S_1$.
We construct the polynomial function $g: \R^{n\times n}\to \R$ as following:
\begin{equation}\label{eq:testing-polynomial}
    g(Y)=\Iprod{\Paren{Y_{12}-\frac{d}{n}}X\Paren{Y_{12}-\frac{d}{n}}, Y_{2}-\frac{d}{n}}\,,
\end{equation}
where $Y_{12}\in \R^{n\times n}$ is the adjacency matrix of the bipartite graph between vertices in $S_1$ and $S_2$, and $Y_2$ is the adjacency matrix of the induced subgraph supported on $S_2$.

We show the lower bound of this polynomial under the symmetric stochastic block model, and the upper bound of this polynomial under the \Erdos-\Renyi graph model.
\begin{lemma}\label[lemma]{lem:expectation-planted}
    Let $\thetanull, Y$ be the edge connection probability matrix and adjacency matrix sampled from the planted distribution $\SSBM(n,\frac{d}{n},\e,k)$. 
    Let $X_1^\circ$ be the community matrix of the subgraph induced by vertices in $S_1$.
    Suppose in \cref{eq:testing-polynomial}, $\E\normf{X-X^\circ}^2\leq o(n^2)$, then we have $\E g(Y)\geq \Paren{\frac{\epsilon d}{n}}^3 n^4$. 
\end{lemma}


\begin{lemma}\label[lemma]{lem:variance-null}
    When the graph is sampled from the null distribution $\bbG(n,\frac{d}{n})$, we have
    $\E g(Y)=0$ and $\sqrt{\text{Var}(g(Y))}\leq d^{3/2}\cdot n^{1-\Omega(1)}$. 
\end{lemma}

Combining \cref{lem:expectation-planted} and \cref{lem:variance-null}, \cref{thm:low-degree-graphon} follows as a corollary: 
\begin{proof}[Proof of \cref{thm:low-degree-graphon}]
Suppose there is a degree-$n^{0.001}$ polynomial $f:\R^{n\times n}\to \R^{n\times n}$ which gives error rate $o(n\cdot k)$. 
Let $X=\frac{n}{\e d}\Paren{f(Y)-\frac{d}{n}}$.
Then we have 
\begin{equation*}
    \normf{X-X^\circ}=\frac{n^2}{\e^2 d^2}\normf{f(Y)-\thetanull}^2\leq o\Paren{\frac{n^2}{\e^2 d^2} kn}\leq o\Paren{\frac{k}{\e^2 d}\cdot \frac{n}{d} \cdot n^2}\,.
\end{equation*}

When $\epsilon^2 d \geq 0.001k^2$ and $d=\Theta(n)$, 
we have $\E\normf{X-X^\circ}^2\leq o(n^2)$. 
combining \cref{lem:expectation-planted} and \cref{lem:variance-null}, we have
    \begin{equation*}
        \frac{\E g(Y)}{\sqrt{\text{Var}(g(Y))}} \geq n^{0.001}\,.
    \end{equation*}
    Since $g(Y)$ is a degree-$\ell$ polynomial with $\ell\leq n^{0.01}$, by \cite{Hopkins18}, we have 
   \begin{equation*}
       \frac{\E g(Y)}{\sqrt{\text{Var}(g(Y))}} \leq \exp(k^2)\,.
   \end{equation*}
When $\exp(k^2)\leq n^{0.001}$, this leads to a contradiction.
As result, we conclude that no degree-$n^{0.001}$ polynomial can achieve error rate $o(nk)$.
\end{proof}


\subsection{Proof of \cref{lem:expectation-planted}}

In this section, we analyze the property of the polynomial in \cref{eq:testing-polynomial} under the $k$-symmetric stochastic block model, and give a proof for \cref{lem:expectation-planted}. 
\begin{proof}[Proof of \cref{lem:expectation-planted}] 
    Let $X^\circ_{12}\in \{\pm 1\}^{n\times n}$ be the community matrix for the bipartite graph between $S_1$ and $S_2$, i.e for $i\in S_1$ and $j\in S_2$, we have $X^\circ_{12}(i,j)=1$ if $i,j$ belongs to the same community and $X^\circ_{12}(i,j)=-1$ if $i,j$ belongs to different communities. 
    Moreover, we let $X^\circ_1$ be the community matrix for the induced subgraph on $S_1$ and let $X^\circ_2$ be the community matrix for the induced subgraph on $S_2$. 
    Then we have $Y_{12}=\frac{\e d}{n}X^\circ_{12}+W_{12}$, $Y_1=\frac{\e d}{n}X^\circ_1+W_1$ and $Y_2=\frac{\e d}{n}X^\circ_2+W_2$, where 
    \begin{itemize}
        \item $W_{12}, W_1,W_2$ are independent,
        \item $(W_{12},W_1,W_2)$ is independent with $(X^\circ_{12},X^\circ_1,X^\circ_2)$,
        \item every entry in $W_{12},W_1,W_2$ has zero mean.
    \end{itemize}

    
    Then we have
    \begin{align*}
         \E g(Y) & =\E\Iprod{(Y_{12}-\frac{d}{n})X(Y_{12}-\frac{d}{n}),Y_2-\frac{d}{n}}\\
         &= \Paren{\frac{\e d}{n}}^3 \E\Iprod{X^\circ_{12} XX^\circ_{12},X^\circ_2}+ \E \Iprod{W_{12}XW_{12},W_2}+\frac{2\e d}{n}\E\Iprod{W_{12}XX^\circ_{12},W_2}\\
         &= \Paren{\frac{\e d}{n}}^3 \E\Iprod{X^\circ_{12} XX^\circ_{12},X^\circ_2}\,. 
    \end{align*}
    Since $\E\Iprod{X^\circ_{12} X_1^\circ X^\circ_{12},X^\circ_2}\geq \Omega(n^4)$ and
    \begin{equation*}
        \E\Iprod{Y^\circ_{12} (X_1-X_1^\circ) X^\circ_{12},X^\circ_2}\leq \sqrt{\E\normf{X_1-X_1^\circ}^2\cdot \E\normf{X^\circ_{12}X^\circ_2 X^\circ_{12}}^2}\leq o(n^4)\,.
    \end{equation*}
    Therefore, we have $\E\Iprod{X^\circ_{12} XX^\circ_{12},X^\circ_2}\geq \Omega(n^4)$.
    and the claim follows. 
\end{proof}

\subsection{Proof of \cref{lem:variance-null}}

In this section, we analyze the property of the polynomial defined in \cref{eq:testing-polynomial}, under the \Erdos-\Renyi graph distribution, and give a proof for \cref{lem:variance-null}. 

\begin{proof}[Proof of \cref{lem:variance-null}]
Under the \Erdos-\Renyi graph distribution, the entries in $Y_2-\frac{d}{n}$ are i.i.d zero mean random variables, independent with $Y_{12}$ and $Y_2$(i.e the rest of the graph). 
As result, we have $\E g(Y)=0$.

It remains to bound the variance of the polynomial under the \Erdos-\Renyi graph distribution, which is to say, we bound 
\begin{equation*}
    \E g(Y)^2= \E \Iprod{\Paren{Y_{12}-\frac{d}{n}}X\Paren{Y_{12}-\frac{d}{n}}, Y_2-\frac{d}{n}}^2\,. 
\end{equation*}
Let $W_{12}=Y_{12}-\frac{d}{n}$ and $W_2=Y_2-\frac{d}{n}$.
The main observation is that $X,W_{12},W_2$ are all independent. 
As result, let $Z=W_{12}XW_{12}$, we have
\begin{align*}
    \E \Iprod{W_{12}XW_{12}, W_2}^2=\sum_{ij}\Paren{\E W_2(i,j)Z(i,j)}^2
    = \sum_{ij} \E W_2^2(i,j) \E Z(i,j)^2 
    = \frac{d}{n} \E \normf{Z}^2\,.
\end{align*}
As $\normf{X}\leq O(n)$ without loss of generality, and $\norm{W_{12}}\leq \sqrt{d}\log(n)$ with overwhelming high probability, we have
\begin{equation*}
    \E \normf{Z}^2\leq O\Paren{n^2 d^2\log^4(n)}\,.
\end{equation*}
Therefore we have
\begin{equation*}
    \E \Iprod{W_{12}XW_{12}, W_2}^2\leq O\Paren{n d^3\log^4(n)}\,.
\end{equation*}
By taking the square root, we conclude the proof. 
\end{proof}