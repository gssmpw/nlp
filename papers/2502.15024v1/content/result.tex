\section{Main result} \label{sec:main-result}
\subsection{Computational lower bound for weak recovery in stochastic block model}
We provide the first rigorous evidence that no polynomial-time algorithms can achieve recovery rate $n^{-0.49}$ with constant probability below the Kesten-Stigum threshold, assuming~\cref{conj:low-degree}. 
\begin{theorem}[Computational lower bound below the KS threshold, see \cref{thm:full-main-theorem-weak-recovery} for the full statement]\label{thm:main-theorem-weak-recovery}
    Let $k, d\in \N^+$ be such that $k\leq O(1), d\leq n^{o(1)}$.
    Assume that for any $d'\in \N^+$ such that $0.999 d\leq d'\leq d$, Conjecture~\ref{conj:low-degree} holds with distribution $P$ given by $\SSBM(n,\frac{d'}{n},\e,k)$ and distribution $Q$ given by the \Erdos-\Renyi graph model $\bbG(n, \frac{d'}{n})$. 
    Then, no $\exp\Paren{n^{0.99}}$ time algorithm can achieve recovery rate $n^{-0.49}$ in the $k$-stochastic block model when $\epsilon^2 d\leq 0.99 k^2$.
\end{theorem}
Note that the algorithm which outputs a matrix $\hat M$ reflecting a random partition of the vertices into~$k$ communities only achieves vanishing recovery rate $\delta \lesssim 1/\sqrt{n}$.
In contrast, recall that above the KS threshold (when $\epsilon^2 d/k^2 > 1$), polynomial-time algorithms can achieve a recovery rate in $\Omega(1)$ (i.e., weak recovery).
To our knowledge, this is the first result showing such a sharp transition in the recovery rate that can be achieved by efficient algorithms above and below the KS threshold. 

Under a strengthened low-degree conjecture (see \Cref{conj:eldlr} below), our lower bound extends to the regime where the number of communities can be as large as $n^{o(1)}$. 

\begin{theorem}[Computational lower bound for diverging number of blocks]\label{thm:main-theorem-super-constant-blocks}
    Let $k,d\in \N^+$ be such that $k\leq n^{o(1)}, d\leq n^{o(1)}$.
    Assume that for any $d'\in \N^+$ such that $0.999 d\leq d'\leq d$, Conjecture \ref{conj:eldlr} holds with distribution $P$ given by $\SSBM(n,\frac{d'}{n},\e,k)$ and distribution $Q$ given by the \Erdos-\Renyi graph model $\bbG(n, \frac{d'}{n})$. 
    Then no $\exp\Paren{n^{0.99}}$ time algorithm can achieve recovery rate $n^{-0.49}$ in the $k$-stochastic block model when $\epsilon^2 d\leq 0.99 k^2$. 
\end{theorem}

\paragraph{Concurrent work} A concurrent work \cite{Wein2025sharp} also provides rigorous evidence for computational lower bound below KS threshold based on low-degree polynomials.
They give the first unconditional low-degree recovery lower bound below the KS threshold in the stochastic block model.
In our setting, for the task of weak recovery (which is to say, achieving constant recovery rate), their lower bound directly rules out estimators based on degree $n^\delta$ polynomials, which captures many natural candidates of algorithms.
Such lower bound is beyond reach with techniques from \cite{Hopkins18,bandeira2021spectral}.
For this, they introduce significantly new techniques in analyzing the low-degree polynomials.

In comparison, our main contribution lies in revealing a potential sharp transition of recovery rate for polynomial time (and sub-exponential time) algorithms, 
below and above the KS threshold. We give evidence that sub-exponential time algorithms cannot achieve recovery rate $n^{-0.49}$ with constant probability below the KS threshold,
while polynomial time algorithms are known to achieve weak recovery above the threshold.
Our techniques is based on relating the rate of recovery to the low-degree conjecture formulated in \cite{moitra2023precise}.
Notably, we did not prove new low-degree lower bounds for our main results, but exploited the existing results from \cite{Hopkins18,bandeira2021spectral}.

In another concurrent work, \cite{li2025algorithmiccontiguitylowdegreeconjecture} obtained computational lower bound for random graph matching below sharp thresholds with similar techniques. 


As we discuss next, our result also has implications for other learning tasks in the stochastic block model.



\subsection{Computational lower bound for learning stochastic block model}\label{sec:learning-result}

We give computational lower bounds for learning the stochastic block model under two different error metrics: learning the edge connection probability matrix and the block graphon function. 

\begin{definition}[Edge connection probability matrix for the SBM]\label[Definition]{def:para-sbm} 
    In the symmetric stochastic block model $\SSBM(n,\frac{d}{n},\e,k)$, the edge connection probability matrix $\thetanull\in [0,1]^{n\times n}$ has entries $\thetanull_{i,j}=(1+\frac{(k-1)\e}{k}) \frac{d}{n}$ if $i,j$ belong to the same community and $\thetanull_{i,j}=(1-\frac{\e}{k}) \frac{d}{n}$ if $i,j$ belong to different communities. 
\end{definition}

Given a random graph sampled from distribution $\SSBM(\e,d,k,n)$, the simple polynomial-time algorithm based on $k$-SVD outputs a matrix $\theta\in [0,1]^{n\times n}$ such that $\E \norm{\theta-\theta^\circ}_F^2\leq O(2k\cdot d)$ \cite{Xu2017RatesOC,luo2023computational}.
On the other hand, exponential-time algorithms based on maximum-likelihood can give an estimator which achieves the optimal error rate
$\E \normf{\theta-\theta^\circ}^2\leq O\Paren{\log(k)\cdot d+k^2}$.
We give rigorous evidence for the hardness of learning the edge connection probability matrix of symmetric SBMs, by proving the following computational lower bound. 

\begin{theorem}[Computational lower bound for learning the SBM]\label{thm:lb-edge-probability}
    Let $k,d\in \N^+$ be such that $k\leq n^{o(1)}, d\leq o(n)$.
    Assume that for any $d'\in \N^+$ such that $0.999 d\leq d'\leq d$, Conjecture \ref{conj:eldlr} holds with distribution $P$ given by $\SSBM(n,\frac{d'}{n},\e,k)$ and distribution $Q$ given by \Erdos-\Renyi graph model $\bbG(n, \frac{d'}{n})$. 
    Then given graph $G\sim \SSBM(n,\frac{d}{n},\e,k)$, no $\exp\Paren{n^{0.99}}$ time algorithm can output $\theta\in [0,1]^{n\times n}$ achieving error rate $\normf{\theta-\thetanull}^2\leq 0.99kd/4$ with constant probability, where $\thetanull$ is the sampled edge connection probability matrix.
\end{theorem}
Our computational lower bound matches the guarantees of known efficient algorithms in \cite{Xu2017RatesOC} up to constant factors.\footnote{For completeness, we state this algorithmic result in \cref{sec:learning_algorithm}.}
In comparison, \cite{luo2023computational} show that degree-$\ell$ polynomials cannot give error rate better than $O(kd/\ell^4)$.
For standard low-degree conjectures \cite{Hopkins18,kunisky2019notes,schramm2022computational}, to give evidence of hardness for polynomial-time algorithms, the polynomial degree $\ell$ needs to be taken as large as $\log(n)$.
Therefore, their lower bound on the error rate can only match the guarantees of existing algorithm up to logarithmic factors.
As a result, they cannot give evidence of a computational-statistical gap for the error rate when $k=O(\log(n))$.\footnote{We also obtain an unconditional low-degree recovery lower bound for learning the $k$-stochastic block model when $k\leq n^{0.001}$ in \cref{sec:low-degree-reduction}.}  


Another error metric considered in \cite{klopp2017oracle, borgs2015private, borgs2018revealing,chen2024graphon} is learning the graphon function.
In the context of the symmetric SBM, the graphon function is block-wise constant and given by:

\begin{definition}[Graphon in the symmetric SBM]\label{def:graphon}
Let $d,k\in \N^+$ and $\e\in [0,1]$. Consider the symmetric stochastic block model $\SSBM(n,\frac{d}{n},\e,k)$.
Let $\Bnull\in [0,1]^{k\times k}$ be the community connection probability matrix with diagonal entries given by $(1-\frac{\e (k-1)}{k})\frac{d}{n}$ and non-diagonal entries given by $(1+\frac{\e}{k})\frac{d}{n}$. 
Let $\gamma:[0,1]\to [k]$ be a mapping such that $\gamma(x)=\lceil{kx}\rceil$.
Then a function $\Wnull:[0,1]\times [0,1]\to [0,1]$ is a graphon generating distribution $\SSBM(n,\frac{d}{n},\e,k)$ if $\Wnull(x,y)=\Bnull_{\gamma(x),\gamma(y)}$.
\end{definition}

We note that in contrast with the edge connection probability matrix, the graphon function only depends on the parameters of the distribution $\e,d,k$.
Previous works \cite{borgs2015private,borgs2018revealing,klopp2017oracle,chen2024graphon} consider the following distance metric between graphons:

\begin{definition}[Gromove-Wasserstein distance between graphons]\label{def:graphon-distance}
    Let functions $W_1, W_2:[0,1]\times [0,1]\to [0,1]$.
    We consider the Gromov-Wasserstein distance metric 
    \begin{equation*}
        \GW(W_1,W_2)\coloneqq \sqrt{\min_\phi \int_0^1 \int_0^1 \Paren{W_1(\phi(x),\phi(y))-W_2(x,y)}^2 dx dy}
    \end{equation*}
    where the minimum is taken over all measure-preserving bijective mappings. 
\end{definition}

Given a graph sampled from $\SSBM(n,\frac{d}{n},\e,k)$, our goal is to output a graphon $\hat{W}$ minimizing $\GW(\hat{W},\Wnull)$.
\cite{klopp2017oracle} obtains the minimax error rate
\begin{equation*}
\GW(\hat{W},\Wnull)\lesssim \sqrt{\frac{d^2}{n^2}\cdot \Paren{\frac{k^2}{nd}+\frac{\log(k)}{d}+\sqrt{\frac{k}{n}}}}\,.
\end{equation*}
However, existing polynomial-time algorithms \cite{Xu2017RatesOC,chen2024graphon} can only achieve error rate
\begin{equation*}
    \GW(\hat{W},\Wnull)\lesssim \sqrt{\frac{d^2}{n^2}\cdot \Paren{\frac{k}{d}+\sqrt{\frac{k}{n}}}}\,.
\end{equation*}
 
In this paper, we give the first rigorous evidence for a computational-statistical gap in learning the graphon function  when the number of blocks $k$ is a sufficiently large constant.
\begin{theorem}[Computational lower bound for learning block graphon function]\label{thm:lb-learning-graphon}
    Let $k,d\in \N^+$ be such that $k\leq O(1), d\leq o(n)$.
    Assume that Conjecture \ref{conj:low-degree} holds with distribution $P$ given by $\SSBM(n,\frac{d}{n},\e,k)$ and distribution $Q$ given by \Erdos-\Renyi graph model $\bbG(n, \frac{d}{n})$. 
    Then no $\exp\Paren{n^{0.99}}$ time algorithm can output a $\poly(n)$-block graphon function $\hat{W}:[0,1]\times [0,1]\to [0,1]$ such that $\GW(\hat{W},\Wnull) \leq \frac{d}{3n}\sqrt{\frac{k}{d}}$  with $1-o(1)$ probability under distribution $P$ and distribution $Q$.
\end{theorem}

In comparison, \cite{luo2023computational} do not provide any lower bound for learning the graphon function since their hard instance is a symmetric SBM with fixed distribution parameters $\e,d,k$.



 





