\section{Probability theory facts}\label{sec:prob-fact}
In this section, we provide probability tools that we will need in the paper.

\subsection{Concentration of spectral radii of random matrices}
The following concentration inequality for the spectral norm of the centered adjacency matrix of stochastic block model will be useful for our proofs.
\begin{theorem}[Spectral norm bound for random matrices]\label{thm:spectral-concentration}
    Let $H\in \R^{n\times n}$ be a symmetric matrix whose upper triangular entries are independent zero mean random variables. 
    Moreover, suppose that there exist $q>0$ and $\kappa\geq 1$ such that
    \begin{align*}
        \max_i \sum_{j} \E\abs{H_{ij}}^2\leq 1\,,\\
        \max_{i,j} \E \abs{H_{ij}}^2\leq \kappa/n\,,\\
        \max_{i,j} \abs{H_{ij}}\leq 1/q\,.
    \end{align*}
    Then we have
    \begin{equation*}
        \E \norm{H}\leq 2+C\frac{\sqrt{\log(n)}}{q}\,.
    \end{equation*}
    Moreover, we have
    \begin{equation*}
        \Pr\Brac{\Abs{\norm{H}-\E\norm{H}}\geq t} \leq 2\exp(-cq^2t^2)\,. 
    \end{equation*}
\end{theorem}

As corollary, for stochastic block model, we have the following concentration inequality: 
\begin{lemma}\label[lemma]{lem:spectral-concentration-sbm}
    Let $A$ be the adjacency matrix of a random graph with vertex $i,j$ independently connected with probability $\theta(i,j)\geq \Omega(1/n)$.
    Let $d=\frac{n-1}{n}\sum_{i,j}\theta(i,j)$ and suppose $\theta(i,j)\leq 2d/n$.
    Let $H=\frac{1}{\sqrt{2d}}(A-\theta)$.
    Then for every $t\geq 10000\log(n)$, for some small universal constant $c>0$, we have
    \begin{equation*}
        \Pr\Brac{\norm{H}\geq t} \leq 2\exp(-c t^2)\,. 
    \end{equation*}
\end{lemma}
\begin{proof}
    We vertify that the matrix $H$ here satisfies the conditions in \cref{thm:spectral-concentration}. 
    Crucially, since $\theta(i,j)\leq 2d/n$, we have $\E H_{i,j}^2 \leq 1/n$
    First for each $i\in [n]$, we have
    \(
        \sum_{j\in n}\E\Abs{H_{ij}}^2\leq 1\). 
    Finally, we have
      \(  \max_{i,j} |H_{i,j}|\leq 1\).
    Therefore by taking $\kappa=1$ and $q=1$ in \cref{thm:spectral-concentration}, we have $\E\norm{H}\leq C\sqrt{\log(n)}$, and the concentration bound
    \begin{equation*}
        \Pr\Brac{\Norm{H}\geq \E\norm{H}+t}\leq 2\exp(-c't^2)\,. 
    \end{equation*}
    where $c'$ is a universal constant.
    Taking $t\geq 1000\log(n)$, we have the claim.
\end{proof}

\subsection{Decoupling edge partition}
In this section, we give a lemma that describes the approximate independence between edge sets of the subsampling process. 
\begin{lemma}\label[lemma]{lem:decoupling}
    Let $X\sim \text{Ber}(p)$, and let $X_1$ be obtained from $X$ by subsampling with probability $1-\eta$, i.e $X_1=X\xi$, where $\xi\sim \text{Ber}(1-\eta)$ is independent of $X$.
    Let $X_2=X-X_1$, and $\tilde{X}_2=X_2-\E[X_2|X_1]+\eta p$. 
    Then we have $\E[\tilde{X}_2|X_1]=\E [X_2]=\eta p$ and $\E[(\tilde{X}_2-X_2)^2|X_1]\leq O(p^3)$. 
    Moreover, we have $\Abs{\tilde{X_2}-X_2}\leq \eta p$.
\end{lemma}
\begin{proof}
    We first note that
    \begin{equation*}
        \E[\tilde{X}_2|X_1]= \eta p=\E [X_2]\,.
    \end{equation*}
    Next we note that 
      \begin{equation*}
      \E[X_2|X_1]=\frac{\eta p}{1-(1-\eta) p}(1-X_1)\,,
  \end{equation*}
    Therefore, we have 
    \begin{align*}
        \E[(\tilde{X}_2-X_2)^2|X_1]= \E\Brac{\Paren{\eta p-\frac{\eta p(1-X_1)}{(1-(1-\eta)p)}}^2}\leq O(\eta^2 p^3)
    \end{align*}
\end{proof}

\begin{corollary}\label{cor:decoupling}
    Let $Y$ be the adjacency matrix of a random graph with each edge $(i,j)$ sampled with probability $p(i,j)$.
    Suppose that $p(i,j)\leq p$ for each $i,j\in [n]$.
    Let $Y_1$ be the adjacency matrix of the graph obtained by subsampling each edge in $Y$ with probability $1-\eta$. 
    Let $Y_2=Y-Y_1$.
    Then there is a matrix $\tilde{Y}_2$ such that 
    \begin{itemize}
        \item for every $t\geq \log(n)$, $\normf{\tilde{Y}_2-Y_2}^2\leq t\eta p^3 n^2$ with probability at least $1-\exp(-t)$, 
        \item for every $i,j\in [n]$, $\E\tilde{Y}_2(i,j)=\E Y_2(i,j)$
        \item moreover $\tilde{Y}_2$ and $Y_1$ are independent,
        \item and finally the entries in $\tilde{Y}_2$ are independent.
    \end{itemize}
\end{corollary}
\begin{proof}
We construct the matrix $\tilde{Y}_2$ in the following way.
For each $i,j$, we let $\tilde{Y}_2(i,j)=Y_2(i,j)-\E[Y_2(i,j)|Y_1]+\eta p(i,j)$.
Then by \cref{lem:decoupling}, we have $\E\tilde{Y}_2(i,j)=\E Y_2(i,j)$ and $\E \Paren{\tilde{Y}_2(i,j)-Y_2(i,j)}^2\leq O(\eta^2 p^3)$.
In addition, we have $\Abs{\tilde{Y}_2(i,j)-Y_2(i,j)}\leq O(\eta p)$. 

Furthermore $\tilde{Y}_2(i,j)$ and $Y_1$ are independent.

Finally since the upper triangular entries in $Y$ are independent, we have the upper triangular entries in $\tilde{Y}_2$ are independent. 
By Hoeffding bound, with probability at least $1-\exp(-t)$, we have $\normf{\tilde{Y_2}-Y_2}^2\leq t\eta p^3 n^2 \log(n)$. 
\end{proof}
  
 
