\section{Preliminaries}


\subsection{Low-degree framework}
\paragraph{Low-degree likelihood ratio lower bound in the SBM} The low-degree likelihood ratio lower bound is a standard framework to provide evidence of hardness for hypothesis testing problems. 
\cite{Hopkins18, bandeira2021spectral} prove the following theorem\footnote{Although the original theorem statement is for constant $k,d$, it is easy to see in their analysis that the lower bound holds when $d=o(n)$. Also a weaker version of the theorem is stated in Thm. 8.6.1 of \cite{Hopkins18}.} on the low-degree lower bound for the stochastic block model:

\begin{theorem}[Low-degree lower bound for SBM, Thm. 2.20 in \cite{bandeira2021spectral}]\torestate{\label{thm:ldlr-sbm}
Let $d=o(n),k=o(\log(n))$ and $\e\in [0,1]$. 
Let $\mu:\{0,1\}^{n \times n} \rightarrow \mathbb{R}$ be the relative density of $\SSBM(n, d, \varepsilon, k)$ with respect to $G\left(n, \frac{d}{n}\right)$. 
Let $\mu^{\leqslant \ell}$ be the projection of $\mu$ to the degree-$\ell$
polynomials with respect to the norm induced by $G\left(n, \frac{d}{n}\right)$ For any constant $\delta>0$,
\begin{align*}
\left\|\mu^{\leqslant \ell}\right\| \text{ is } \begin{cases}
    \geqslant n^{\Omega(1)}, & \quad \text{if } \varepsilon^2 d > (1+\delta) k^2, \quad \ell \geqslant O(\log n) \\[8pt]
    \leqslant O_{\delta, \varepsilon, d} \left(\exp(k^2)\right), & \quad \text{if } \varepsilon^2 d < (1-\delta) k^2, \quad \ell < n^{0.99}
\end{cases}
\end{align*}
}
\end{theorem}

 Assuming the low-degree conjectures in \cite{Hopkins18,kunisky2019notes}, this gives rigorous hardness evidence for distinguishing
     \begin{itemize}
        \item planted distribution $P$: symmetric $k$-stochastic block model $\SSBM(n,\frac{d}{n},\e,k)$,
        \item null distribution $Q$: \Erdos-\Renyi random graph $\bbG(n,\frac{d}{n})$,
    \end{itemize}
 with probability $1-o(1)$ when $k$ is a universal constant and $\epsilon^2 d\leq 0.99k^2$.
 However, even assuming the low-degree conjectures for hypothesis testing from \cite{Hopkins18,kunisky2019notes}, these works do not rule out polynomial-time weak recovery algorithms under our definition (i.e., algorithms that achieve constant correlation with constant probability).


\paragraph{Extended low-degree hypothesis}
To show our lower bounds in the regime where $k$ is polylogarithmic, \cref{conj:low-degree} is not sufficient. Instead, we rely on the following stronger low-degree hypothesis from \cite{moitra2023precise}.
\begin{conjecture}[Extended low-degree conjecture]\label[conjecture]{conj:eldlr}
Consider the hypothesis testing problem between $Y\sim P$ and $Y\sim Q$ for distribution $P$ and $Q$.
Let $\RPQ(f)\coloneqq \frac{\E_{Y\sim P} f(Y)-\E_{Y\sim Q} f(Y)}{\sqrt{\text{Var}_{Y\sim Q}(f(Y))}}$. Let $\delta \in [0, 1]$.
For any function~$f(\cdot)$ computable in time $\exp(n^{\delta})$ taking values in $[0,1]$, we have 
\begin{equation*}
        \RPQ(f)\lesssim \max_{\text{deg}(f)\leq n^{\delta}} \RPQ(f)\,.
\end{equation*}
\end{conjecture}
The extended low-degree hypothesis is closely related to the low-degree lower bound for random optimization problems (see \cite{gamarnik2020hardness}).

\subsection{Organization}
The rest of the paper is organized as follows.
We present our main proof ideas in \cref{sec:techniques}. 
In \cref{sec:lb-weak-recovery}, we give the formal proof of our computational lower bounds for recovery algorithms conditional on the low-degree conjectures (i.e., the proof of \cref{thm:main-theorem-weak-recovery}).
In \cref{sec:lb-learning}, we give proofs of our computational lower bounds for parameter learning algorithms conditional on the low-degree conjectures (i.e., the proof of \cref{thm:lb-edge-probability} and  \cref{thm:lb-learning-graphon}).
In \cref{sec:prob-fact}, we introduce some facts from probability theory used in our paper. 
In \cref{sec:algo-results}, we introduce some existing algorithms from the literature that are used in our paper. 
In \cref{sec:beyond-constant-ldlr}, we clarify the upper bound on the low-degree likelihood ratio when the number of blocks $k$ diverges, which is implicitly obtained in \cite{bandeira2021spectral}.