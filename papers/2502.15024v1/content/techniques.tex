\section{Techniques}\label{sec:techniques}
\subsection{Lower bounds for weak recovery}
In this section, we give an overview of the techniques we use to prove lower bounds for weak recovery in the  stochastic block model with constant number of blocks $k$ and constant average degree $d$. That is, an overview of our proof of (a special case of)~\cref{thm:main-theorem-weak-recovery}.

Suppose for a contradiction that we have a polynomial-time recovery algorithm for $\SSBM(n,\frac{d}{n},\e,k)$, $\e^2d \leq 0.99k^2$, that achieves recovery rate $\delta \geq \Omega(n^{-0.49})$, in the sense of~\Cref{def:weak-recovery}. 
Using this algorithm, we will construct a function~$f(\cdot)$, which can be evaluated in polynomial time, such that $R_{P,Q}(f)\geq n^{\Omega(1)}$. Here, $R_{P,Q}(\cdot)$ is the parameter~\eqref{eq:Rlamba} for the distributions $P = \SSBM(n,\frac{d}{n},\e,k)$ and $Q = \bbG(d, n)$. Assuming the low-degree conjecture (Conjecture ~\ref{conj:low-degree}) for this $P$ and $Q$, this implies that there exists a low-degree polynomial $f'$ with $R_{P,Q}(f') \geq \omega(1)$. But, since $\e^2d \leq 0.99k^2$ is below the KS threshold, this leads to a contradiction
with the low-degree lower bound of~\cite{Hopkins18} (\cref{thm:ldlr-sbm}).

In the remainder of this section, we show how to construct the function $f(\cdot)$, and give a sketch of the proof that $R_{{P,Q}}(f)\geq n^{\Omega(1)}$.

\paragraph{Regularization via correlation-preserving projection.}
We begin with the following tool, which allows us to regularize the estimators of the community membership matrix~$M^\circ$ provided by the recovery algorithm.
Suppose that~$\hat{M}_0$ is a matrix achieving correlation $\delta$ with $M^\circ$, i.e., satisfying $\iprod{\hat{M}_0,M^{\circ}}\geq \delta\normf{\hat{M}_0} \normf{M^{\circ}}$.
We show that $\hat{M}_0$ can be projected into a (small) convex set $\cK \subseteq \R^{n \times n}$ containing $M^\circ$, while preserving the correlation (up to a constant). Concretely, the convex set $\cK$ here is given by
\begin{equation} \label{eq:K}
    \cK := \Set{M\in [-1/\delta,1/\delta]^{n\times n}: M+\frac{1}{k\delta} \Ind \Ind^{\top} \succeq 0 \,, \Tr(M + \frac{1}{k\delta} \Ind \Ind^{\top}) \leq n/\delta}.
\end{equation}
In particular, elements of $\cK$ have bounded entries and bounded nuclear norm, which will be crucial in later steps of the proof where we apply a Bernstein inequality.
To achieve this, we make use of the \emph{correlation preserving projection} from \cite{Hopkins17} (see \cref{thm:correlation-preserving-projection}), which projects $\hat M_0$ onto a matrix $\hat M \in \cK$ satisfying
\[
    \iprod{\hat{M}, M^\circ} \geq \Omega(1) \cdot \delta\normf{\hat{M}} \normf{M^{\circ}},
\]
In addition, \cref{thm:correlation-preserving-projection} promises that $\normf{\hat{M}} =  \Theta(\normf{M^\circ})$. Thus, we find that
\begin{equation} \label{eq:largecor}
    \iprod{\hat{M}, M^\circ} \geq \Omega(1) \cdot \delta \normf{\hat{M}} \normf{M^\circ} \geq \Omega(1) \cdot \delta \normf{M^\circ}^2 \geq \delta \cdot \Omega(n^2).
\end{equation}
Importantly, the correlation preserving projection can be implemented in polynomial time via semidefinite programming. See \cref{lem:corr-preserve-proj} for details.


\paragraph{Testing statistics via cross validation.} 
The basic idea is to construct $f(\cdot)$ via cross validation.
Given a random graph $G$, we construct a subgraph $G_1$ with the same vertex set by subsampling each edge in $G$ independently with probability $1-\eta$, where $\eta > 0$ is a small constant. Note that if $G$ is drawn from $\SSBM(n,\frac{d}{n},\e,k)$, then $G_1$ is distributed according to $\SSBM(n,(1-\eta)\frac{d}{n},\e,k)$. If $G$ is drawn from $\bbG(n, \frac{d}{n})$, then $G_1$ is distributed according to~$\bbG(n, (1-\eta)\frac{d}{n})$. We run the polynomial-time recovery algorithm on $G_1$ to obtain an estimate $\hat{M}\in \R^{n\times n}$ of the community membership matrix $M^\circ$, which we regularize using the correlation preserving projection discussed above. Let $Y_2$ denote the adjacency matrix of $G_2 := G\setminus E(G_1)$. 
Our function $f(\cdot)$ is then defined as
\begin{equation}
    f(Y) := \begin{cases}
        1, \quad & \text{if } \iprod{\hat{M},Y_2-\frac{\eta d}{n} \one \one^{\top}}\geq n^{0.51}, \\
        0, & \text{otherwise.}
    \end{cases}
\end{equation}
See \cref{alg:weak-recovery-function} for an overview of the construction of $f(\cdot)$.
It remains to show that~${R_{P,Q}(f)\geq n^{\Omega(1)}}$. For this, we establish a \emph{lower bound} on the expectation $\E_{Y \sim P} f(Y)$ of $f$ under graphs drawn from the SBM and an \emph{upper bound} on the expectation $\E_{Y \sim Q} f(Y)$ of~$f$ under \Erdos-\Renyi random graphs.


\begin{algorithmbox}[Test function $f(\cdot)$ used in the proof of \cref{thm:main-theorem-weak-recovery}]
    \label{alg:weak-recovery-function}
    \mbox{}\\
    \textbf{Input:} A graph $G$ with $n$ vertices, given by its adjacency matrix $Y$. \\
    \textbf{Output:} Test function $f(Y) \in \{0, 1\}$.\\
    \textbf{Algorithm:} 
    \begin{enumerate}[1.]
        \item Obtain a subgraph $G_1$ of $G$ by subsampling each edge with probability $1-\eta$. 
        \item Obtain an estimator $\hat{M}_0$ by running a recovery algorithm on the graph $G_1$.
        \item Obtain $\hat{M}$ by projecting $\hat{M}_0$ onto the set $\cK$ defined in~\eqref{eq:K} using the correlation preserving projection.
        \item Return $f(Y) = \one \{ \iprod{\hat{M},Y_2-\frac{\eta d}{n} \one \one^{\top}} \geq n^{0.51}\}$, where $Y_2$ is the adjacency matrix of $G \setminus E(G_1)$.
    \end{enumerate}
\end{algorithmbox}


\paragraph{Lower bound on the expectation under the SBM.}
First, we give a lower bound on the expectation of $f$ under the distribution $\SSBM(n,\frac{d}{n},\e,k)$, i.e, on
\[
    \E_{Y \sim P} f(Y) = \Pr_{Y \sim P} \Brac{\iprod{\hat{M},Y_2-\frac{\eta d}{n}\one \one^{\top}} \geq n^{0.51}}.
\]
To do so, note that we may decompose $Y_2-\frac{\eta d}{n} \one \one^{\top} = \frac{\epsilon \eta d}{n}M^\circ+W_2$, where $W_2$ is a random matrix whose entries are i.i.d. with mean zero. Then, we have 
\begin{equation*}
    \iprod{Y_2-\frac{\eta d}{n} \one \one^{\top},\hat{M}} = \frac{\epsilon \eta d}{n}\iprod{M^\circ, \hat{M}} +  \iprod{W_2,\hat{M}}\,.
\end{equation*}
The first term on the RHS above is large with constant probability by~\eqref{eq:largecor}. We would like to apply a Bernstein inequality to the second term, but the matrices $W_2$ and $\hat{M}$ are not independent. However, as we show in~\cref{lem:decoupling}, they are \emph{approximately independent} in the sense that there exists an 
 i.i.d. zero-mean symmetric matrix~$\tilde{W}_2$, independent of $\hat{M}$, so that each entry in $\tilde{W}_2-W_2$ has variance bounded by $O(d^2/n^2)$.

For ease of presentation, we ignore the difference between $\tilde{W}_2$ and $W_2$ for now, and assume that $W_2$ and $\hat{M}$ are independent. 
In this case, we note that $\iprod{{W}_2, \hat{M}}$ can be written as the summation of independent zero-mean random variables, namely
\begin{equation*}
    \iprod{{W}_2, \hat{M}}= \sum_{i,j} {W}_2(i,j)\hat{M}(i,j)\,,
\end{equation*}
where ${W}_2(i,j)\hat{M}(i,j) \leq O(1/\delta)$ for each $i,j\in [n]$. (Here, we have used that $\hat{M} \in \cK$).
Moreover, since $\normf{\hat{M}}^2 =  \Theta(\normf{M^\circ}^2) = \Theta(n^2)$, we have
\begin{equation*}
    \sum_{i,j} \hat{M}(i,j)^2 \E\Brac{{W}_2(i,j)^2}\lesssim \frac{d}{n}\sum_{i,j} \hat{M}(i,j)^2 \leq O(n^2 \cdot \frac{d}{n})=O(nd)\,.
\end{equation*}
By the Bernstein inequality, and using the fact that $\delta \geq \Omega(n^{-0.49})$, we then have
\begin{equation*}
    \Pr\Brac{\sum_{i,j} {W}_2(i,j)\hat{M}(i,j)\geq n^{0.501}}\leq \exp\Paren{-n^{0.001}}\,.
\end{equation*}
As result, when $d \leq O(1) , k \leq O(1),\eta=\Theta(1)$, with constant probability, we have
\begin{equation*}
    \iprod{Y_2-\frac{\eta d}{n},\hat{M}}\geq \frac{\eta\epsilon d}{n}\iprod{M^\circ, \hat{M}}-n^{0.501} \gtrsim \delta n -n^{0.501} \gtrsim n^{0.51} - n^{0.501} \geq \Omega(n^{0.51})\,.
\end{equation*}
Therefore, we have $\E_{Y \sim P} f(Y)\geq \Omega(1)$. 

\paragraph{Upper bound under the null distribution.}




Next, we give an upper bound on the expectation of $f$ under the \Erdos-\Renyi distribution $\bbG(n,\frac{d}{n})$. Our proof shares many ingredients with the proof of the lower bound in the previous section. We show that with high probability under the distribution $\bbG(d, n)$, we have 
\begin{equation*}
    g(Y) := \Abs{\iprod{Y_2-\frac{\eta d}{n}\one \one^{\top}, \hat{M}}} = o(n^{0.51})\,.
\end{equation*}
To do so, we again apply the argument that $Y_2-\frac{\eta d}{n}\one \one^{\top}$ and $\hat{M}$ are approximately independent. 
In particular, let $W_2=Y_2-\frac{\eta d}{n}\one \one^{\top}$, for some i.i.d. zero-mean symmetric matrix $\tilde{W}_2$, independent of~$\hat{M}$, so that each entry in $\tilde{W}_2-W_2$ has variance bounded by $d^2/n^2$. 
By the triangle inequality, we have
\begin{equation*}
   \Abs{\iprod{Y_2-\frac{\eta d}{n}\one \one^{\top},\hat{M}} }\leq \Abs{\iprod{W_2-\tilde{W}_2,\hat{M}}}+\Abs{\iprod{\tilde{W}_2,\hat{M}}}\,. 
\end{equation*}

For simplicity, we again ignore the difference between $\tilde{W}_2$ and $W_2$ here.
By the same reasoning as above, and again relying on the properties of $\hat {M}$ guaranteed by the correlation preserving projection, we have the Bernstein inequality
\begin{equation*}    \Pr\Brac{\Abs{\iprod{\tilde{W}_2,\hat{M}}}\geq n^{0.501}}\leq \exp\Paren{-n^{0.01}}\,.
\end{equation*}
As result, when $d \le O(1)$ and $k \le O(1)$,  we have $g(Y)\leq o(n^{0.501})$  with probability at least $1-\exp(-n^{0.01})$ and thus $\E_{Y \sim Q} f(Y)\leq \exp(-n^{0.01})$. 

\paragraph{Finishing the proof}
Using the lower and upper bound established above, and the fact that $f(\cdot) \in \{0, 1\}$, we get that
\[
    \frac{\E_{Y\sim P} f(Y)-\E_{Y\sim Q} f(Y)}{\sqrt{\text{Var}_{Y\sim Q}(f(Y))}} \geq \frac{\Omega(1)}{\exp(-n^{0.01})} \geq \exp(n^{0.01})\geq \omega(1).
\]

\subsection{Lower bound for learning the stochastic block model} 
In this section, we give an overview of the techniques used to prove our results on learning the stochastic block model, stated in~\cref{sec:learning-result}. 

\paragraph{Lower bound for learning edge connection probability matrix} We sketch the proof of \cref{thm:lb-edge-probability}. 
We show that if an $O(\exp\Paren{n^{0.99}})$-time algorithm can learn the edge connection probability matrix $\thetanull$ such that with constant probability, the error rate $\normf{\hat{\theta}-\thetanull}^2 \leq 0.99kd$, then an algorithm with running time $\exp\Paren{n^{0.99}}$ can achieve weak recovery when $\e^2 d\geq 0.99k^2$. 
The key observation is that, for the symmetric stochastic block model, the edge connection probability matrix is given by $\theta^\circ=\frac{(1-\eta)\epsilon d}{n} M^\circ+\frac{(1-\eta)d}{n}$, where $M^\circ\in \Set{1-1/k,-1/k}^{n\times n}$ is the community membership matrix.
Therefore, when the estimation error is smaller than $0.99\sqrt{kd}$, the estimator $\hat{\theta}-\frac{d}{n}$ achieves weak recovery under the distribution $\SSBM(n,\frac{d}{n},\e,k)$, which contradicts the extended low-degree conjecture (\cref{conj:eldlr}).

\paragraph{Lower bound for learning graphon function} 
We sketch the proof of \cref{thm:lb-learning-graphon}.
Let $W_0$ be the graphon function  underlying the distribution $\bbG(n,\frac{d}{n})$ and $W_1$ be the graphon function underlying the distribution $\SSBM(n,\frac{d}{n},\e,k)$. We then have $\GW(W_0,W_1)\geq \frac{d}{n}\sqrt{\frac{0.99k}{d}}$ when $\e^2 d\geq 0.99k^2$. 


Now suppose there is a polynomial-time algorithm which, given a random graph $G$ sampled from an arbitrary symmetric $k$-stochastic block model, outputs an $n$-block graphon function $\hat{W}:[0,1]\times [0,1]\to [0,1]$ achieving error $\frac{d}{3n}\sqrt{\frac{k}{d}}$ 
 with probability $1-o(1)$.
Then one can construct a testing statistic by taking
\begin{equation*}
f(Y) =
\begin{cases}
    1, & \text{if } \GW(\hat{W}, W_0) \leq \frac{3d}{n} \sqrt{\frac{k}{d}}, \\
    0, & \text{otherwise.}
\end{cases}
\end{equation*}
We have $f(Y)=1$ with probability $1-o(1)$ under the distribution $\SSBM(n,\frac{d}{n},\e,k)$ and $f(Y)=0$ with probability $1-o(1)$ under the distribution $\bbG(n,\frac{d}{n})$. 
Therefore, we have $\RPQ(f)\geq \omega(1)$.
Since the function $f(\cdot)$ can be evaluated in polynomial time, this contradicts the low-degree lower bound (\cref{thm:ldlr-sbm}), assuming \cref{conj:low-degree}.













 






























