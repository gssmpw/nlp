\section{Computational lower bound for learning stochastic block model}\label{sec:lb-learning}

\subsection{Computational lower bound for learning the edge connection probability matrix}

In this section, we prove \cref{thm:lb-edge-probability} by showing that there exists an efficient algorithm that reduces testing to learning in SBM. 
The reduction of algorithm \cref{alg:reduction-test-learning} is similar to that of \cref{alg:reduction-test-recovery}. The proof of \cref{thm:lb-edge-probability} is also a similar proof by contradiction to the proof of \cref{thm:main-theorem-weak-recovery}.

Before describing the algorithm, we restate \cref{thm:lb-edge-probability} here for completeness.
\begin{theorem}[Restatement of \cref{thm:lb-edge-probability}]
\label{thm:lb-edge-probability-restatement}
    Let $k,d\in \N^+$ be such that $k\leq n^{o(1)}, d\leq o(n)$.
    Assume that for any $d'\in \N^+$ such that $0.999 d\leq d'\leq d$, Conjecture \ref{conj:eldlr} holds with distribution $P$ given by $\SSBM(n,\frac{d'}{n},\e,k)$ and distribution $Q$ given by \Erdos-\Renyi graph model $\bbG(n, \frac{d'}{n})$. 
    Then given graph $G\sim \SSBM(n,\frac{d}{n},\e,k)$, no $\exp\Paren{n^{0.99}}$ time algorithm can output $\theta\in [0,1]^{n\times n}$ achieving error rate $\normf{\theta-\thetanull}^2\leq 0.99kd/4$ with constant probability, where $\thetanull$ is the ground truth sampled edge connection probability matrix.
\end{theorem}

The reduction that we consider is the following.

\begin{algorithmbox}[Reduction from testing to learning]
    \label{alg:reduction-test-learning}
    \mbox{}\\
    \textbf{Input:} A random graph $G$ with equal probability sampled from \Erdos-\Renyi model or stochastic block model. \\
    \textbf{Output:} Testing statistics $g(Y)\in \R$, where $Y$ is the centered adjacency matrix\\
    \textbf{Algorithm:} 
    \begin{enumerate}[1.]
        \item Obtain subgraph $G_1$ by subsampling each edge with probability $1-\eta=0.999$, and let $G_2= G\setminus G_1$. 
        \item Run learning algorithm on $G_1$, and obtain estimator $\hat{\theta}\in \R^{n\times n}$
        \item Obtain $\hat{M}$ by running correlation preserving projection on $\hat{\theta}-\frac{d}{n}\Ind \Ind^{\top}$ to the set $\cK=\Set{M\in [-1,1]^{n\times n}: M+\frac{1}{k} \Ind \Ind^{\top} \succeq 0 \,, \Tr(M + \frac{1}{k} \Ind \Ind^{\top}) \leq n}$. 
        \item Construct the testing statistics $g(Y)=\iprod{\hat{M},Y_2-\frac{\eta d}{n}\Ind \Ind^{\top}}$, where $Y_2$ is the adjacency matrix for the graph $G_2$.
    \end{enumerate}
\end{algorithmbox}

Before proving \cref{thm:lb-edge-probability}, we first show the relationship between learning edge connection probability and weak recovery.
 \begin{lemma}\label[lemma]{lem:reduction-learning-recovery}
     Consider the distribution of $\SSBM(n,\frac{d}{n},\e,k)$ with $d\le n^{o(1)}$. 
     Suppose give graph $Y\sim \SSBM(n,\frac{d}{n},\e,k)$, the estimator $\hat{\theta}\in \R^{n\times n}$ achieves error rate $\normf{\hat{\theta}- \thetanull}\leq \frac{1}{2}\sqrt{0.99kd}$ with constant probability, then $\hat{\theta}-d/n$ achieves weak recovery when $\e^2 d\geq 0.99k^2$.
 \end{lemma}
\begin{proof}
By the relation between edge connection probability matrix $\thetanull$ and the community matrix $M^\circ$, We have
    \begin{equation*}
        \iprod{\hat{\theta}-\frac{d}{n}\Ind \Ind^\top,M^\circ}=\iprod{\hat{\theta}-\theta^\circ,M^\circ}+\iprod{\theta^\circ-\frac{d}{n}\Ind \Ind^\top,M^\circ}=\iprod{\hat{\theta}-\theta^\circ,M^\circ}+\iprod{\frac{\e d}{n}M^\circ,M^\circ}\,.
    \end{equation*}
    For the first term, since with constant probability, $\normf{\hat{\theta}-\theta^\circ}\leq \sqrt{0.99kd}$, we have
    \begin{equation*}
      \Abs{\iprod{\hat{\theta}-\theta^\circ,M^\circ}}\leq \normf{M^\circ}\normf{\hat{\theta}-\theta^\circ}\leq 
        \normf{M^\circ} \sqrt{0.99kd}\,.
    \end{equation*}
    For the second term, since with overwhelming high probability, $\normf{M^\circ}\geq \frac{n}{\sqrt{k}}(1-\frac{1}{k})$, we have
    \begin{equation*}
        \iprod{\frac{\e d}{n}M^\circ,M^\circ}=\frac{\e d}{n}\normf{M^\circ}^2\geq \frac{\e d }{2\sqrt{k}} \normf{M^\circ}\,.
    \end{equation*}
    Therefore, when $\e^2 d> 0.999 k^2$, we have 
    \begin{equation*}
        \iprod{\hat{\theta}-\frac{d}{n}\Ind \Ind^{\top},M^\circ}\geq \frac{\e d }{2\sqrt{k}} \normf{M^\circ}-\normf{M^\circ} \frac{\sqrt{0.99kd}}{2}\geq \Omega\Paren{\frac{\e d \normf{M^\circ}}{\sqrt{k}}} \,.
    \end{equation*}
    On the other hand, by triangle inequality
    \begin{equation*}
        \Normf{\hat{\theta}-\frac{d}{n}\Ind \Ind^{\top}}\leq  \Normf{\hat{\theta}-\theta^\circ}+ \Normf{\theta^\circ-\frac{d}{n}\Ind \Ind^{\top}}\leq O(\sqrt{kd}+\frac{\e d}{\sqrt{k}}) \leq O\Paren{\e d/\sqrt{k}}\,,
    \end{equation*}
Therefore we have 
\begin{equation*}
    \iprod{\hat{\theta}-\frac{d}{n}\Ind \Ind^{\top},M^\circ}\geq \Omega(\normf{M^\circ}\cdot \normf{\hat{\theta}-\frac{d}{n}\Ind \Ind^{\top}})\,.
\end{equation*}
    We thus conclude that with constant probability, $\hat{\theta}-\frac{d}{n}\Ind \Ind^\top$ achieves weak recovery when $\e^2 d\geq 0.99k^2$.
\end{proof}
With \cref{lem:reduction-learning-recovery}, the proof of lower bound for learning the edge connection probability matrix of stochastic block model follows as a corollary.
\begin{proof}[Proof of \cref{thm:lb-edge-probability}]
    By \cref{lem:reduction-learning-recovery}, suppose an $\exp\Paren{n^{0.99}}$ time algorithm achieves error rate less than $0.99\sqrt{kd}$ in estimating the edge connection probability matrix, then in \cref{alg:reduction-test-learning}, $\hat{\theta}-\frac{d}{n}$ achieves weak recovery when $\e^2 d=0.99k^2$.
    We let $f(Y)=\mathbf{1}_{g(Y)\geq 0.001 \e^2 d^2/k}$. 

    We show that with constant probability under $P$, we have $f(Y)=1$.    
    We essentially follow the proof of \cref{lem:lb_sbm} with $\delta$ taken as a constant, except that we take a different strategy for bounding
    $\iprod{W_2-\tilde{W}_2, \hat{M}}$.
    By \cref{lem:spectral-concentration-sbm}, we have, with probability at least $1-o(1)$, the following spectral radius bounds on the symmetric random matrices
\begin{equation*}
    \normop{W_2-\tilde{W}_2}\leq O\Paren{\sqrt{d\log(n)}\cdot \sqrt{\frac{d}{n}}}\,.
\end{equation*}
Therefore, by Trace inequality, we have
\begin{equation*}
\begin{split}
|\iprod{W_2-\tilde{W}_2, \hat{M}}|
& = |\iprod{W_2-\tilde{W}_2, \hat{M}+\frac{1}{k\delta}\Ind \Ind^{\top}} - \iprod{W_2-\tilde{W}_2, \frac{1}{k\delta}\Ind \Ind^{\top}}| \\
& \leq |\iprod{W_2-\tilde{W}_2, \hat{M}+\frac{1}{k\delta}\Ind \Ind^{\top}}| + |\iprod{W_2-\tilde{W}_2, \frac{1}{k\delta}\Ind \Ind^{\top}}| \\
& \leq \normop{W_2-\tilde{W}_2} \Tr(\hat{M}+\frac{1}{k\delta}\Ind \Ind^{\top}) + \normop{W_2-\tilde{W}_2} \Tr(\frac{1}{k\delta}\Ind \Ind^{\top}) \\
& \leq O\Paren{\sqrt{d\log(n)}\cdot \sqrt{\frac{d}{n}} (1+\frac{1}{k})\frac{n}{\delta}}\\
& = O\Paren{(d+\frac{d}{k})\frac{\sqrt{n\log(n)}}{\delta}} \,.
\end{split}
\end{equation*}

    With the same reasoning, by \cref{lem:ub_ER}, with probability at least $1-\exp(-n^{0.001})$ under distribution $Q$, we have $f(Y)=0$. 
    Therefore, we have $\RPQ(f)\geq \exp(n^{0.001})$. 
    Since $f(A)$ can be evaluated in $O\Paren{\exp\Paren{n^{0.99}}}$ time, assuming conjecture \ref{conj:low-degree} we have
   \begin{equation*}
       R_{P,Q}(f)\coloneqq \frac{\E f(A)}{\sqrt{\text{Var}_Q(f(A))}} \lesssim \max_{\text{deg}(f)\leq n^{0.99}}\frac{\E f(A)}{\sqrt{\text{Var}_Q(f(A))}}\,.
   \end{equation*}
    On the other hand, by low-degree lower bound stated in \cref{thm:ldlr-sbm}, we have 
    \begin{equation*}
       \max_{\text{deg}(f)\leq n^{0.99}}\frac{\E f(A)}{\sqrt{\text{Var}_Q(f(A))}}\leq \exp(k^2)\,. 
    \end{equation*}
Since we have $\exp(n^{0.001})\gg\exp(k^2)$ when $k\leq n^{o(1)}$, this leads to a contradiction. 
\end{proof}

\subsection{Computational lower bound for learning graphon}
In this part, we give formal proof of \cref{thm:lb-learning-graphon}. 

\begin{theorem}[Restatement of \cref{thm:lb-learning-graphon}]
    Let $k,d\in \N^+$ be such that $k\leq O(1), d\leq o(n)$.
    Assume that Conjecture \ref{conj:low-degree} holds with distribution $P$ given by $\SSBM(n,\frac{d}{n},\e,k)$ and distribution $Q$ given by \Erdos-\Renyi graph model $\bbG(n, \frac{d}{n})$. 
    Then no $\exp\Paren{n^{0.99}}$ time algorithm can output a $\poly(n)$-block graphon function $\hat{W}:[0,1]\times [0,1]\to [0,1]$ such that $\GW(\hat{W},\Wnull) \leq \frac{d}{3n}\sqrt{\frac{k}{d}}$  with $1-o(1)$ probability under distribution $P$ and distribution $Q$(where $\Wnull$ is the underlying graphon of the corresponding distribution).
\end{theorem}
\begin{proof}
Let $W_0$ be the graphon function underlying the distribution $\bbG(n,\frac{d}{n})$ and $W_1$ be the graphon function underlying the distribution $\SSBM(n,\frac{d}{n},\e,k)$, we have $\GW(W_0,W_1)\geq \frac{d}{n}\sqrt{\frac{0.99k}{d}}$ when $\e^2 d\geq 0.99k^2$. 

Now suppose there is a polynomial time algorithm, which given random graph $G$ sampled from an arbitrary symmetric $k$-stochastic block model, outputs an $n$-block graphon function $\hat{W}:[0,1]\times [0,1]\to [0,1]$ achieving error $\frac{d}{3n}\sqrt{\frac{k}{d}}$ with probability $1-o(1)$.
Then one can construct the testing statistics by taking
\begin{equation*}
f(Y) =
\begin{cases}
    1, & \text{if } \GW(\hat{W}, W_0) \leq \frac{d}{3n} \sqrt{\frac{k}{d}} \\
    0, & \text{otherwise}
\end{cases}
\end{equation*}
We have $f(Y)=1$ with probability $1-o(1)$ under the distribution of symmetric stochastic block model $\SSBM(n,\frac{d}{n},\e,k)$.
By triangle inequality, we have $f(Y)=0$ with probability $1-o(1)$ under the distribution $\bbG(n,\frac{d}{n})$. 
Therefore we have $\RPQ(f)\geq \omega(1)$.

Now since the function $\hat{W}$ can be represented as a symmetric matrix with $\poly(n)$ number of rows and columns, and moreove since $W_0$ is a constant function,
\begin{equation*}
    \GW(\hat{W},W_0)= \int_0^1 \int_0^1 (\hat{W}(x,y)-W_0(x,y))^2 dx dy\,.
\end{equation*}
Therefore, the function $f(\cdot )$ can be evaluated in polynomial time. 
This contradicts the low-degree lower bound (\cref{thm:ldlr-sbm}) assuming \cref{conj:low-degree}.
\end{proof}
















