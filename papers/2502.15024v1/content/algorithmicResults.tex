\section{Useful algorithmic results}\label{sec:algo-results}
In this section, we provide two algorithmic results from previous work that will be useful in our paper.

\subsection{Correlation preserving projection}
Given a vector $P$ that has constant correlation with an unknown vector $Y$, \cite{Hopkins17} shows that one can project the vector $P$ into a convex set containing $Y$, and preserve the constant correlation with $Y$. 
\begin{theorem}[Correlation preserving projection, theorem 2.3 in \cite{Hopkins17}]
  \label{thm:correlation-preserving-projection}
  Let $\delta\in \R^+$
  Let $\cC$ be a convex set and $Y\in \cC$.
  Let $P$ be a vector with $\iprod{P,Y}\ge \delta \cdot \norm{P}\cdot \norm{Y}$.
  Then, if we let $Q$ be the vector that minimizes $\norm{Q}$ subject to $Q\in \cC$ and $\iprod{P,Q}\ge \delta \cdot \norm{P}\cdot \norm{Y}$, we have
  \begin{equation}
    \iprod{Q,Y}\ge \delta/2 \cdot \norm{Q}\cdot \norm{Y}\,.
  \end{equation}
  Furthermore, $Q$ satisfies $\norm{Q}\ge \delta \norm{Y}$.
\end{theorem}

We include their proof here for completeness.
\begin{proof}
    By construction, $Q$ is the Euclidean project of $0$ into the set $\Set{Q\in \cC|\iprod{P,Q}\geq \delta \norm{P}\norm{Y}}$. 
    By Pythagorean inequality, the Euclidean projection into a set decreases distances to points into the set.
    Therefore, $\norm{Y-Q}^2\le \norm{Y-0}^2$, which implies that $\iprod{Y,Q}\geq \norm{Q}^2/2$.
    Moreover, $\iprod{P,Q}\geq \delta \norm{P}\norm{Y}$, which implies $\norm{Q}\geq \delta \norm{Y}$ by Cauchy-Schwartz. 
    Thus, we can conclude that $\iprod{Y,Q}\geq \delta/2\cdot \norm{Y}\cdot \norm{Q}$.
\end{proof}

\subsection{Learning edge connection probability matrix via SVD}
\label{sec:learning_algorithm}

\begin{theorem}\label{thm:algorithm-learning-sbm}
    When $d\geq \log(n)$, there is a polynomial time algorithm which given the adjacency matrix of a graph sampled from symmetric $k$-stochastic block model $\SSBM(n,\frac{d}{n},\e,k)$, returns an estimator $\hat{\theta}\in [0,1]^{n\times n}$ such that $\normf{\thetanull-\theta}^2\leq kd$ with high probability.
\end{theorem}
\begin{proof}
    We take $\hat{\theta}$ as the best rank-$k$ approximation for the adjacency matrix. 
    Then since $\normop{A-\theta^\circ}\leq \sqrt{kd}$ with high probability, we have $\normop{\hat{\theta}-A}\leq \sqrt{kd}$ with high probability.
    By triangle inequality, we have $\normop{\hat{\theta}-\theta^\circ}\leq 2\sqrt{d}$.
    As result, we have
\( \normf{\hat{\theta}-\theta^\circ}\leq 2\sqrt{kd}\)\,.
\end{proof}
