\section{Computational lower bound for recovery}\label{sec:lb-weak-recovery}
In this section, we prove \cref{thm:main-theorem-weak-recovery} by showing that there exists an efficient algorithm that reduces testing to weak recovery in SBM. We will show that there exists a efficiently computable testing function (shown in \cref{alg:reduction-test-recovery}) that is large with constant probability if the input is sampled from $\SSBM(n,\frac{d}{n},\e,k)$ and is small with high probability if the input is sampled from $\bbG(n, d/n)$. This will lead to a contradiction with low-degree lower bounds of testing if we assume Conjecture \ref{conj:low-degree}.

Before describing the algorithm, we restate \cref{thm:main-theorem-weak-recovery} here for completeness.

\begin{theorem}[Full version of \cref{thm:main-theorem-weak-recovery}]\label{thm:full-main-theorem-weak-recovery}
    Let $k,d\in \N^+$ be such that $k\leq O(1), d\leq o(n)$.
    Assume that for any $d' \in \N^+$ such that $0.999 d\leq d'\leq d$, Conjecture \ref{conj:low-degree} holds for distribution $P = \SSBM(n,\frac{d'}{n},\e,k)$ and distribution $Q=\bbG(n, \frac{d'}{n})$.
    Then for any small constants $\delta_1,\delta_2$, no $\exp\Paren{n^{0.99}}$ time algorithm can achieve recovery rate $n^{-0.5+\delta_1}$ in the $k$-stochastic block model when $\epsilon^2 d\leq (1-\delta_2) k^2$.
\end{theorem}

The reduction that we consider is the following.

\begin{algorithmbox}[Reduction from testing to weak recovery]
    \label{alg:reduction-test-recovery}
    \mbox{}\\
    \textbf{Input:} A random graph $G$ with equal probability sampled from \Erdos-\Renyi model or stochastic block model, and target recovery rate $\delta$, parameters $\e,k,d$. \\
    \textbf{Output:} Testing statistics $g(Y)\in \R$, where $Y$ is the adjacency matrix.\\
    \textbf{Algorithm:} 
    \begin{enumerate}[1.]
        \item Let $\eta=0.001\delta_2$, where $\delta_2=1-\e^2 d/k^2$. Obtain subgraph $G_1$ by subsampling each edge with probability $1-\eta$, and let $G_2= G\setminus G_1$. 
        \item Obtain estimator $\hat{M}_0$ by running weak recovery algorithm on graph $G_1$.
        \item Obtain $\hat{M}$ by applying correlation preserving projection (see \cref{thm:correlation-preserving-projection}) on $\hat{M}_0$ to the set $\cK=\Set{M\in [-1/\delta,1/\delta]^{n\times n}: M+\frac{1}{k\delta} \Ind \Ind^{\top} \succeq 0 \,, \Tr(M + \frac{1}{k\delta} \Ind \Ind^{\top}) \leq n/\delta}$. 
        \item Return testing statistics $g(Y)=\iprod{\hat{M},Y_2-\frac{\eta d}{n} \one \one^{\top}}$, where $Y_2$ is the adjacency matrix for the graph $G_2$.
    \end{enumerate}
\end{algorithmbox}

To prove \cref{thm:full-main-theorem-weak-recovery}, we will show that the testing statistics $g(Y)$ from \cref{alg:reduction-test-recovery} satisfies the following two lemmas.

\begin{lemma}
\label[lemma]{lem:lb_sbm}
    Let $Y$ be the adjacency matrix of the graph sampled from the symmetric $k$-stochastic block model $\SSBM(n,\frac{d}{n},\e,k)$ and $M^\circ\in \Set{-1/k,1-1/k}^{n\times n}$ be the corresponding community membership matrix.
    Suppose that $\iprod{\hat{M}_0,M^{\circ}}\geq  n^{-0.5+\delta_1} \normf{\hat{M}_0} \normf{M^{\circ}}$ and $\normf{\hat{M}} = \Theta(\normf{M^{\circ}})$.
    Then \cref{alg:reduction-test-recovery} outputs testing statistics $g(Y)\in \R$ such that $g(Y)\geq \Omega\Paren{n^{0.5(1+\delta_1)}}$.
\end{lemma}

\begin{lemma}
\label[lemma]{lem:ub_ER}
    Let $Y$ be the adjacency matrix of the graph sampled from \Erdos-\Renyi random graph $\bbG(n, d/n)$. 
    With probability at least $1-\exp(-n^{0.001\delta_1})$, \cref{alg:reduction-test-recovery} outputs $g(Y) \leq O(n^{0.5+\delta_1/3})$ in polynomial time.
\end{lemma}

Combining \cref{lem:lb_sbm} and \cref{lem:ub_ER}, \cref{thm:main-theorem-weak-recovery} follows as a corollary.

\begin{proof}[Proof of \cref{thm:main-theorem-weak-recovery}]
Suppose that there is a $\exp\Paren{n^{0.99}}$ time algorithm which outputs estimator $\hat{M}_0$ such that $\iprod{\hat{M}_0,M^{\circ}}\geq  n^{-0.5+\delta_1} \normf{\hat{M}_0} \normf{M^{\circ}}$.
Let $f(Y)=\mathbf{1}_{g(Y)\geq 0.001n^{0.5+\delta_1/2}}$.
When $\e^2 d \geq \Omega(k^2)$, combining \cref{lem:lb_sbm} and \cref{lem:ub_ER}, we have
    \begin{equation*}
        \frac{\E_P f(Y)}{\sqrt{\text{Var}_Q(f(Y))}} \geq \exp(n^{0.001\delta_1})\,.
    \end{equation*}
    By the low-degree likelihood ratio upper bound \cref{thm:ldlr-sbm}, when $\e^2 d\leq (1-\delta_2)k^2$, we have 
    \begin{equation*}
       \max_{\text{deg}(f)\leq n^{0.01}}\frac{\E f(Y)}{\sqrt{\text{Var}_Q(f(Y))}}\leq \exp(k^2) \,.
    \end{equation*}
    
    Since $f(Y)$ can be evaluated in $O(\exp\Paren{n^{0.99}})$ time, assuming Conjecture ~\ref{conj:low-degree}, we then have 
   \begin{equation*}
    \frac{\E f(Y)}{\sqrt{\text{Var}_Q(f(Y))}} \lesssim \max_{\text{deg}(f)\leq n^{0.01}}\frac{\E f(Y)}{\sqrt{\text{Var}_Q(f(Y))}}\leq O(1)\,,
   \end{equation*}
which leads to a contradiction.
As a result, assuming Conjecture ~\ref{conj:low-degree}, we cannot achieve weak recovery in $\exp\Paren{n^{0.99}}$ time when $\epsilon^2 d\leq (1-\delta_2)k^2$. 
\end{proof}

\subsection{Correlation preserving projection}

In this part, we prove that we can project the estimator into the set of matrices with bounded entries and bounded nuclear norm, while preserving correlation.
\begin{lemma}\label[lemma]{lem:corr-preserve-proj}
Let $M^{\circ}\in \{-1/k,1-1/k\}^{n\times n}$ be a symmetric matrix with rank-$(k+1)$.
For any $\delta\leq O(1)$, given matrix $\hat{M}_0$ such that $\iprod{\hat{M}_0,M^{\circ}}\geq \delta\normf{\hat{M}_0} \normf{M^{\circ}}$, there is a polynomial time algorithm which outputs $\hat{M} \in \cK$ such that 
$\iprod{\hat{M},M^{\circ}}\geq \Omega(1)\cdot \delta\normf{\hat{M}} \normf{M^{\circ}}$ and $\normf{\hat{M}}\geq \Omega(\normf{M^{\circ}})$, where
\begin{equation*}
    \cK=\Set{M\in [-1/\delta,1/\delta]^{n\times n}: M+\frac{1}{k\delta} \Ind \Ind^{\top} \succeq 0 \,, \Tr(M + \frac{1}{k\delta} \Ind \Ind^{\top}) \leq n/\delta} \,.
\end{equation*}
\end{lemma}
\begin{proof}
    We apply the correlation preserving projection from \cite{Hopkins17} (restated in \cref{thm:correlation-preserving-projection}).
    By definition, $M^{\circ} = X^{\circ} (X^{\circ})^{\top} - \frac{1}{k} \Ind \Ind^{\top}$ is in $\cK$.
    Let $N$ be the matrix that minimizes $\normf{N}$ subject to $N\in \cK^\prime$ and $\iprod{N,\hat{M}_0}\geq \delta \normf{M^{\circ}} \normf{\hat{M}_0}$, where
    \begin{equation*}
    \cK^\prime=\Set{M\in [-1,1]^{n\times n}: M+\frac{1}{k} \Ind \Ind^{\top} \succeq 0 \,, \Tr(M + \frac{1}{k} \Ind \Ind^{\top}) \leq n} \,.
    \end{equation*}
    Using ellipsoid method, this semidefinite program can be solved in polynomial time.
    By \cref{thm:correlation-preserving-projection}, we have $\iprod{N,M^{\circ}}\geq \Omega(1)\cdot \delta\normf{N} \normf{M^{\circ}}$ and $\normf{N} \geq \delta\normf{M^{\circ}}$.
    We let $\hat{M}=\frac{\normf{M^\circ}}{\normf{N}}\cdot N$.
    Then it follows that $\hat{M}\in \cK$, $\normf{\hat{M}}=\normf{M^\circ}$ and $\iprod{\hat{M},M^{\circ}}\geq \Omega(\delta) \normf{\hat{M}}\cdot \normf{M^\circ}$.
\end{proof}

\subsection{Proof of \cref{lem:lb_sbm}}

In this section, we prove \cref{lem:lb_sbm}.
\begin{proof}[Proof of \cref{lem:lb_sbm}]
We consider the decomposition that $Y_2-\frac{\eta d}{n} \one \one^{\top}= \frac{\epsilon \eta d}{n}M^{\circ}+W_2$ where $W_2$ is a symmetric random matrix with independent and zero mean entries.
By \cref{lem:decoupling}, there exists an i.i.d zero mean symmetric matrix $\tilde{W}_2$ that is independent with $Y_1$, and satisfies that the entries in $\tilde{W}_2-W_2$ are independent with zero mean and have variance bounded by $O(d^3/n^3)$, conditioning on the subsampled graph $Y_1$ and community matrix $M^\circ$.
As result, we have
\begin{equation*}
    \iprod{Y_2-\frac{\eta d}{n} \one \one^{\top},\hat{M}}= \iprod{\frac{\epsilon \eta d}{n}M^{\circ}, \hat{M}}+ \iprod{W_2-\tilde{W}_2, \hat{M}}+\iprod{\tilde{W}_2, \hat{M}}\,.
\end{equation*}

For the first term $\iprod{\frac{\epsilon \eta d}{n}M^{\circ}, \hat{M}}$, it follows from \cref{lem:corr-preserve-proj} that
\begin{equation*}
\begin{split}
\iprod{M^{\circ}, \hat{M}}
& \geq \Omega \Paren{\frac{\epsilon \eta d}{n}} \delta\normf{\hat{M}} \normf{M^{\circ}} \\
& \geq \Omega \Paren{\frac{\delta\epsilon \eta d}{n}} \normf{M^{\circ}}^2\,.
\end{split}
\end{equation*}
As with probability at least $1-\exp(-n^{0.001})$, we have $\normf{M^{\circ}}^2\geq \Omega(n^2)$, and as result $\iprod{M^\circ,\hat{M}}\geq \Omega(n\delta \e d)$.

For bounding the second term $\iprod{W_2-\tilde{W}_2, \hat{M}}$, we condition on the subsampled graph $Y_1$ and the community matrix $M^\circ$. 
With probability at least $1-\exp(-n^{\delta_1})$, we have 
\begin{align*}
    |\iprod{W_2-\tilde{W}_2, \hat{M}}|\leq \normf{W_2-\tilde{W}_2} \cdot \normf{\hat{M}}\lesssim \sqrt{\frac{d^3}{n^3} \cdot n^{2+\delta_1} \cdot n^2}= \sqrt{n^{1+\delta_1}d^3}\,. 
\end{align*}

For the third term $\iprod{\tilde{W}_2, \hat{M}}$, we again conditional on the subsampled graph $Y_1$ and the community matrix $M^\circ$.
We note that it can be written as the summation of independent zero-mean random variables
\begin{equation*}
    \iprod{\tilde{W}_2, \hat{M}}= \sum_{i,j} \tilde{W}_2(i,j)\hat{M}(i,j)\,. 
\end{equation*}
where $\tilde{W}_2(i,j)\hat{M}(i,j)$ are independent zero mean variables bounded by $O(1/\delta)$ for all $i\leq j$. 
Moreover, we have
\begin{equation*}
    \sum_{i,j} \hat{M}(i,j)^2 \E\Brac{\tilde{W}_2(i,j)^2}\lesssim \frac{d}{n}\sum_{i,j} \hat{M}(i,j)^2 \leq O(n^2\cdot \frac{d}{n})=O(nd)\,.
\end{equation*}
By Bernstein inequality, we have
\begin{equation*}
    \Pr\Brac{\Abs{\sum_{i,j} \tilde{W}_2(i,j)\hat{M}(i,j)}\geq 100 t}\leq \exp\Paren{-t^2/(nd+t/\delta)} \,. 
\end{equation*}
Taking $t=n^{(1+\delta_1)/2}\sqrt{d}$ and $\delta\geq n^{-0.5+\delta_1}$, we have
\begin{equation*}
    \Pr\Brac{\Abs{\sum_{i,j} \tilde{W}_2(i,j)\hat{M}(i,j)}\geq n^{0.5(1+\delta_1)}\sqrt{d}}\leq \exp\Paren{-n^{\delta_1/2}}\,.
\end{equation*}

As a result, when $d\leq n^{o(1)},k\leq n^{o(1)},\delta\geq n^{-0.5+\delta_1}, \e=\Theta(1/\sqrt{d})$, with constant probability, we have
\begin{equation*}
    \iprod{Y_2-\frac{\eta d}{n} \one \one^{\top},\hat{M}}
    \geq \Omega(n^{0.5+\delta_1}\sqrt{d})\,.
\end{equation*}
\end{proof}

\subsection{Proof of \cref{lem:ub_ER}}

In this section, we prove \cref{lem:ub_ER}.
\begin{proof}[Proof of \cref{lem:ub_ER}]
We will use the fact that $Y_2-\frac{\eta d}{n} \one \one^{\top}$ and $\hat{M}$ are approximately independent. 
More precisely, let $W_2=Y_2-\frac{\eta d}{n} \one \one^{\top}$, by \cref{lem:decoupling}, there exists symmetric zero mean matrix $\tilde{W}_2$ with independent entries  such that each entry in $\tilde{W}_2-W_2$ has zero mean variance bounded by $O(d^3/n^3)$ conditioning on $\hat{M}$. 
By triangle inequality, we have
\begin{equation*}
   g(Y)=\Abs{\iprod{Y_2-\frac{\eta d}{n}\one \one^{\top},\hat{M}} }\leq \Abs{\iprod{W_2-\tilde{W}_2,\hat{M}}}+\Abs{\iprod{\tilde{W}_2,\hat{M}}}\,. 
\end{equation*}
For bounding the first term $\iprod{W_2-\tilde{W}_2, \hat{M}}$, we condition on the subsampled graph $Y_1$. 
With probability at least $1-\exp(-n^{\delta_1/3})$, we have 
\begin{align*}
    |\iprod{W_2-\tilde{W}_2, \hat{M}}|\leq \normf{W_2-\tilde{W}_2} \cdot \normf{\hat{M}}\lesssim \sqrt{\frac{d^3}{n^3} \cdot n^{2+\delta_1/3} \cdot n^2}= \sqrt{d^3 n^{1+\delta_1/3}}\,. 
\end{align*}
For the second term, we note that $\iprod{\tilde{W}_2, \hat{M}}$ can be written as the summation of independent zero-mean random variables
\begin{equation*}
    \iprod{\tilde{W}_2, \hat{M}}= \sum_{i,j} \tilde{W}_2(i,j)\hat{M}(i,j)\,. 
\end{equation*}
where $\tilde{W}_2(i,j)\hat{M}(i,j)$ are independent zero mean variables bounded by $O(1/\delta)$ for $i\leq j$. 
Moreover, we have
\begin{equation*}
    \sum_{i,j} \hat{M}(i,j)^2 \E\Brac{\tilde{W}_2(i,j)^2}\lesssim \frac{d}{n}\sum_{i,j} \hat{M}(i,j)^2 \leq O(n^2\cdot \frac{d}{n})=O(nd)\,.
\end{equation*}
By Bernstein inequality, we have
\begin{equation*}
    \Pr\Brac{\Abs{\sum_{i,j} \tilde{W}_2(i,j)\hat{M}(i,j)}\geq 100 t}\geq \exp\Paren{-t^2/(nd+t/\delta)} \,. 
\end{equation*}
Taking $t=n^{0.5+\delta_1/3}\sqrt{d}$ and $\delta\geq n^{-0.5+\delta_1}$, we have
\begin{equation*}
    \Pr\Brac{\Abs{\sum_{i,j} \tilde{W}_2(i,j)\hat{M}(i,j)}\geq n^{0.5+\delta_1/3}}\leq \exp\Paren{-n^{0.001\delta_1}}\,.
\end{equation*}
\end{proof}

\subsection{Proof of \cref{thm:main-theorem-super-constant-blocks}}

In this part, we give the proof of \cref{thm:main-theorem-super-constant-blocks}, which is the same as the proof of \cref{thm:main-theorem-weak-recovery} except that we assume stronger low-degree conjecture.
\begin{proof}[Proof of \cref{thm:main-theorem-super-constant-blocks}]
Suppose that there is a polynomial time algorithm which outputs estimator $\hat{M}_0$ such that $\iprod{\hat{M}_0,M^{\circ}}\geq  n^{-0.5+\delta_1} \normf{\hat{M}_0} \normf{M^{\circ}}$.
Let $f(Y)=\mathbf{1}_{g(Y)\geq 0.001n^{0.5+\delta_1/2}}$.
When $0.001k^2\leq \e^2 d \leq (1-\delta_2)k^2$, combining \cref{lem:lb_sbm} and \cref{lem:ub_ER}, we have
    \begin{equation*}
        \frac{\E_P f(Y)}{\sqrt{\text{Var}_Q(f(Y))}} \geq \exp(n^{0.001})\,.
    \end{equation*}
    Since $f(Y)$ can be evaluated in $O(\exp(n^{0.001}))$ time, assuming Conjecture ~\ref{conj:low-degree}, by \cite{Hopkins18}(stated in \cref{thm:ldlr-sbm}), we have 
   \begin{equation*}
    \frac{\E f(Y)}{\sqrt{\text{Var}_Q(f(Y))}} \lesssim \max_{\text{deg}(f)\leq n^{0.99}}\frac{\E f(Y)}{\sqrt{\text{Var}_Q(f(Y))}}\leq \exp(k^2)\,.
   \end{equation*}
When $k^2\leq n^{0.001}$, this leads to a contradiction.
As a result, assuming Conjecture ~\ref{conj:eldlr}, we cannot achieve recovery rate $n^{-0.5+\delta_1}$ in polynomial time when $\epsilon^2 d\leq (1-\Omega(1))k^2$. 
\end{proof}


