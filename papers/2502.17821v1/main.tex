%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{url}
\usepackage{setspace}
\usepackage{enumitem}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\input{math_commands}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\ours}{\textbf{CAML}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\PRT}[1]{{\footnotesize\color{red}[{\bf PRT:} \textsf{#1}]}} %Pratap's comments



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rui Liu}{umd}
\icmlauthor{Yu Shen}{adobe}
\icmlauthor{Peng Gao}{ncsu}
\icmlauthor{Pratap Tokekar}{umd}
\icmlauthor{Ming Lin}{umd}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
\end{icmlauthorlist}

\icmlaffiliation{umd}{University of Maryland, College Park}
\icmlaffiliation{ncsu}{North Carolina State University}
\icmlaffiliation{adobe}{Adobe Research}

\icmlcorrespondingauthor{Rui Liu}{ruiliu@umd.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Multi-modality learning has become a crucial technique for improving the performance of machine learning applications across domains such as autonomous driving, robotics, and perception systems. While existing frameworks such as Auxiliary Modality Learning (AML) effectively utilize multiple data sources during training and enable inference with reduced modalities, they primarily operate in a single-agent context. This limitation is particularly critical in dynamic environments, such as connected autonomous vehicles (CAV), where incomplete data coverage can lead to decision-making blind spots. To address these challenges, we propose Collaborative Auxiliary Modality Learning (\ours), a novel multi-agent multi-modality framework that enables agents to collaborate and share multimodal data during training while allowing inference with reduced modalities per agent during testing. We systematically analyze the effectiveness of \ours~from the perspective of uncertainty reduction and data coverage, providing theoretical insights into its advantages over AML. Experimental results in collaborative decision-making for CAV in accident-prone scenarios demonstrate that \ours~achieves up to a ${\bf 58.13}\%$ improvement in accident detection. Additionally, we validate \ours~on real-world aerial-ground robot data for collaborative semantic segmentation, achieving up to a ${\bf 10.61}\%$ improvement in mIoU.
\end{abstract}

\begin{figure}[htb]
    \vspace{-0.75em}
    \centering
    \includegraphics[width=\linewidth]{figs/motivation2.png}
    \vspace{-2em}
    \caption{\textbf{Illustration of CAML.} \ours~enables multiple agents to collaborate and share multimodal data during training while allowing for runtime inference with reduced modalities per agent during testing. Additionally, the number of agents can vary between training and testing, ensuring flexibility and robustness in deployment.}
    \label{fig:moti}
    \vspace{-2.25em}
\end{figure}

\vspace{-15pt}
\section{Introduction}
\vspace{-5pt}
Multi-modality learning has become an essential approach in a wide range of machine learning applications, particularly in areas such as autonomous driving \citep{el2019rgb, xiao2020multimodal, gao2018object} , robotics \citep{noda2014multimodal, lee2020making}, and perception systems \citep{zhuang2021perception, bayoudh2022survey}, where the availability of multiple data sources (e.g., RGB images, LiDAR, radar, etc.) improves model performance by providing complementary information. However, these multi-modality systems often suffer from increased computational complexity and latency at inference time. Moreover, some modalities may not be consistently available or reliable in real-world conditions, necessitating strategies that can compensate for missing modalities during inference.

% \PRT{would be nice to have an illustrative image of the first page or second.}

Recent work on machine learning \citep{hoffman2016learning, wang2018pm, garcia2018modality, garcia2019learning, piasco2021improving} aims to address these problems by allowing models to leverage additional modalities during training while enabling inference using fewer or even a single modality. For example, a model might be trained using both RGB and LiDAR data, but during deployment, it only requires RGB data to operate. These approaches reduce the computational burden and accommodates real-world conditions where certain sensors may be unavailable. \citet{shen2023auxiliary} formalized these learning tasks as Auxiliary Modality Learning (AML). The AML framework successfully reduces the dependency on expensive or unreliable modalities, but it focuses on the single-agent setting, where an individual model is trained to handle reduced modalities during inference. 

Despite the benefits of AML, several gaps remain. First, a major limitation in the current AML framework is the inability to exploit collaboration between agents, particularly in dynamic environments such as {\em connected autonomous vehicles} (CAV). In such scenarios, data coverage from a single agent is often incomplete because of occlusion or limited sensor range, leading to blind spots or increased uncertainty in decision-making. Second, the information from multiple modalities can complement each other across agents, especially in multi-agent settings such as vehicle-to-vehicle (V2V) communication or collaborative robotics. Different agents may have access to complementary sensory information, which could be shared to have agents make more informed and safer decisions, notably in accident-prone scenarios. However, current AML approaches do not exploit this potential for collaboration.

To bridge these gaps, we propose Collaborative Auxiliary Modality Learning (\textbf{CAML}), a novel framework for multi-agent multi-modality systems that allows agents to collaborate and share multimodal data during training, but enables inference with reduced modalities per agent during testing, as illustrated in Figure \ref{fig:moti}. \ours~leverages knowledge distillation \citep{hinton2015distilling}, transferring knowledge from a teacher model into a student model. This enables the student to operate with missing modalities during inference. For instance, in autonomous driving, multiple vehicles can share sensor information such as  LiDAR and RGB images during training to build more robust representations, while during runtime testing, each vehicle performs inference using only RGB images. 

% \PRT{The last sentence "As suggested by..." makes CAML sound incremental. It's the second sentence right after you introduce CAML. The second sentence should really highlight what's new about CAML, rather than what's the same about CAML. } 

\ours~addresses two key challenges: (1) It {\em reduces uncertainty and enhances data coverage in dynamic environments} by {\em leveraging complementary information from multiple agents}.  (2) It maintains {\em efficient, modality-reduced inference during testing}.  Unlike previous work that either focuses on multi-agent collaboration but without addressing modality reduction at test time, or tackles multi-modality learning in single-agent settings, \ours~unifies these concepts. Through collaboration, \ours~enables agents to compensate for each other’s blind spots, resulting in more informed prediction or decision-making even when some modalities are unavailable at deployment. In summary, our work offers the following key contributions:
\begin{itemize}[left=0pt]
    \vspace{-6pt}
    \item We introduce \ours, a novel framework for multi-agent systems that allows agents to share multimodal data during training, while performing efficient, reduced-modality inference during testing. By leveraging the strengths of multi-agent collaboration, \ours~can reduce estimation uncertainty and integrate complementary information, capturing a broader and more detailed data representation.
    % which can reduce estimation uncertainty and provide complementary information, capturing a broader and more detailed representation of the data space. 
    \vspace{-15pt}
    \item We systematically analyze the effectiveness of \ours~from the perspective of uncertainty reduction and enhanced data coverage, providing theoretical insights into its advantages over AML.
    \vspace{-6pt}
    \item We validate \ours~through experiments in collaborative decision-making for connected autonomous driving in accident-prone scenarios, and collaborative semantic segmentation for real-world data of aerial-ground robots. \ours~achieves up to ${\bf 58.13}\%$ improvement in accident detection for autonomous driving,  and up to ${\bf 10.61}\%$ improvement for more accurate semantic segmentation.
\end{itemize}

% \vspace{-15pt}
\section{Related Work}
% Our approach, Collaborative Auxiliary Modality Learning (CAML), integrates insights from multi-agent collaboration, multimodal learning, and knowledge distillation. Below, we discuss how existing methods in these areas relate to our work, their limitations, and how CAML advances the state of the art.

\paragraph{Multi-Agent Collaboration.}
% \vspace{-5pt}
Collaboration in multi-agent systems has been widely studied across fields such as autonomous driving and robotics. In autonomous driving, prior research has explored various strategies, including spatio-temporal graph neural networks \citep{gao2024collaborative}, LiDAR-based end-to-end systems \citep{cui2022coopernaut}, decentralized cooperative lane-changing \citep{nie2016decentralized} and game-theoretic models \citep{hang2021decision}. In robotics, \citet{mandi2024roco} presented a hierarchical multi-robot collaboration approach using large language models, while \citet{zhou2022multi} proposed a perception framework for multi-robot systems built on graph neural networks. A review of multi-robot systems in search and rescue operations was provided by \cite{queralta2020collaborative}, and \citet{bae2019multi} developed a reinforcement learning (RL) method for multi-robot path planning. Additionally, various communication mechanisms, such as Who2com \citep{liu2020who2com}, When2com \citep{liu2020when2com}, and Where2comm \citep{hu2022where2comm}, have been created to optimize agent interactions.

Despite these advancements, existing multi-agent collaboration frameworks remain limited by their focus on specific tasks and the assumption that agents will have consistent access to the same data modalities during both training and testing, an assumption that may not hold in real-world applications. To address these gaps, our framework, \ours, enables agents to collaborate during training by sharing multimodal data, but at test time, each agent performs inference using reduced modality. This reduces the dependency on certain modalities for deployment, while still allowing agents to leverage additional data during training to enhance overall performance and robustness.
\vspace{-6pt}
\paragraph{Auxiliary Modality Learning.}
Auxiliary Modality Learning (AML) \cite{shen2023auxiliary} has emerged as an effective solution to reduce computational costs and the amount of input data required for inference. By utilizing auxiliary modalities during training, AML minimizes reliance on those modalities at inference time. For example, \citet{hoffman2016learning} introduced a method that incorporates depth images during training to enhance test-time RGB-only detection models. Similarly, \citet{wang2018pm} proposed PM-GANs to learn a full-modal representation using data from partial modalities, while \citet{garcia2018modality, garcia2019learning} developed approaches that use depth and RGB videos during training but rely solely on RGB data for testing. \citet{piasco2021improving} created a localization system that predicts depth maps from RGB query images at test time. Building on these works, \citet{shen2023auxiliary} formalized the AML framework, systematically classifying auxiliary modality types and AML architectures.

However, existing AML frameworks are typically designed for single-agent settings, failing to exploit the potential benefits of multi-agent collaboration for improving multimodal learning. \ours~allows agents to collaboratively learn richer multimodal representations during training. This approach mitigates the loss of information when modalities are reduced during inference, as the learned features are reinforced by data shared across agents.

% \vspace{-25pt}
\paragraph{Knowledge Distillation.}
Knowledge distillation (KD) \citep{hinton2015distilling} is a widely used technique in many domains to reduce computation by transferring knowledge from a large, complex model (teacher) to a simpler model (student). In computer vision, \citet{gou2021knowledge} provided a comprehensive survey of KD applications, while \citet{beyer2022knowledge} conducted an empirical investigation to develop a robust and effective recipe for making State-of-the-Art (SOTA) large-scale models more practical. Additionally, \citet{tung2019similarity} introduced a KD loss function that aligns the training of a student network with input pairs producing similar activation in the teacher network. In natural language processing, \citet{xu2024survey} reviewed the applications of KD in LLMs, while \citet{sun2019patient} proposed a Patient KD method to compress larger models into lightweight counterparts that maintain effectiveness. \citet{hahn2019self} also suggested a KD approach that leverages the soft target probabilities of the training model to train other neural networks. In autonomous driving, \citet{lan2022instance} presented an approach for visual detection, \citet{cho2023itkd, sautier2022image} used KD for 3D object detection.

Notice that existing KD mostly distills knowledge from a larger model to a smaller one to reduce computation, \citet{shen2023auxiliary} aimed to design a cross-modality learning approach using KD to utilize the hidden information from auxiliary modalities within the AML framework. But AML is limited by the scope of a single-agent paradigm, missing opportunities for collaborative knowledge sharing across agents. In contrast, we leverage KD within multi-agent settings, where the teacher models are trained with access to shared multimodal data (e.g., RGB and LiDAR) from multiple agents. By distilling this collaborative knowledge into each agent’s reduced modality (e.g., RGB), \ours~enables robust inference during deployment, even with fewer modalities. This collaborative distillation process enhances each agent’s performance by providing richer, complementary knowledge from the collaborative training phase.

\vspace{-5pt}
\section{Collaborative Auxiliary Modality Learning} \label{sec:caml}
\vspace{-5pt}
In AML \citep{shen2023auxiliary}, which operates in a single-agent framework, the missing modalities during testing are referred to as auxiliary modalities, while those that remain available are called the main modality. In contrast, in our framework \ours, each agent can process a different number of modalities during training and different agents can have different main modalities and auxiliary modalities. There is no correlation between the number of agents and the number of modalities.

% This flexibility enables our framework to better handle real-world variability where both modalities and agents may vary between training and testing phases.

We define our problem in both training and testing phases. In the training phase, we consider a multi-agent system with $N$ agents collaboratively completing a task.
The set of agents is denoted as $\gA_{train} = \{\gA_1, \gA_2, \ldots, \gA_N\}$. The observations of all agents are denoted as $X = \{x_1, x_2, \ldots, x_N\}$, where $x_i$ is the observation acquired by the $i$-th agent $\gA_i \in \gA_{train}$. The ground truth label is denoted as $Y$, which can be an object label, semantic class, or a control command (e.g., brake for an autonomous vehicle). The set of modalities is denoted as $\gI_{train} = \{\gI_1, \gI_2, \ldots, \gI_K\}$, such as RGB, LiDAR, Depth, etc, where $K$ is the number of modalities avaiable during training. During training, each agent has access to all these $K$ modalities. In the testing phase, we assume there are $M$ agents. The set of test agents is denoted as $\gA_{test} = \{\gA_1, \gA_2, \ldots, \gA_M\}$. In addition, the set of modalities is denoted as $\gI_{test}$, which is a subset of $\gI_{train}$. The number of modalities available during testing is denoted as $L$, where $L \leq K$. The set of agents that have access to the $j$-th modality $\gI_j \in \gI_{test}$ is denoted as $\gA_{test}^{\gI_j}$, where $\gA_{test}^{\gI_j} \in \gA_{test}$, and the number of agents in this set is given by $|\gA_{test}^{\gI_j}| = M_j$. This means that during testing, each agent may have access to different number of modalities.

Given the problem definition, we aim to estimate the posterior distribution $P(y|X)$ of the ground truth label $y$ given all agents' observations  $X$.
During training, we train both a teacher model where each agent has access to all modalities in $\gI_{train}$ and a student model where each agent has access to partial modalities in $\gI_{test}$. We employ Knowledge Distillation (KD) to transfer the knowledge derived from the teacher model to the student model, enabling the student to benefit from additional information, as illustrated in Figure \ref{fig:app}. At test time, we perform inference using the student model, which relies on the test modality observations $X^{test}$.  

Specifically, in the teacher model, each agent has access to all multimodal observations and independently processes its local observations to produce embeddings. These embeddings are then shared among agents based on whether the system operates in a centralized or decentralized manner. If the system is centralized, all collaborative agents share their embeddings with one designated ego agent for centralized processing. If the system is decentralized, each agent shares the embeddings with other agents. We provide a detailed complexity analysis of \ours~for both operation manners in the Appendix \ref{app:comp}. Subsequently, the shared embeddings corresponding to the same modality are aggregated together. Then we fuse (e.g., via concatenation or cross-attention) the aggregated embeddings of different modalities to create a comprehensive multimodal embedding. This multimodal embedding is then passed through a prediction module to produce the teacher model's final prediction. The student model follows a similar network architecture as the teacher. However, instead of processing all modalities, each agent processes only a single or a subset of modalities, which can vary across agents. By sharing these embeddings among agents, the student model also constructs a multimodal embedding, leveraging the different modalities observed by various agents. This multimodal embedding is then used to generate the student model’s prediction. Thus, our approach enables the student model to maintain strong predictive performance despite missing modalities during testing, significantly enhancing its robustness and generalizability.

\begin{figure*}[htb]
    % \vspace{-3pt}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/Approach.png}
    \vspace{-6pt}
    \caption{\textbf{CAML Approach Pipeline.} The teacher model (top) aggregates and shares multimodal embeddings across agents for prediction. In contrast, the student model (bottom) processes a subset of modalities per agent and shares them to form a multimodal embedding. This allows the student model to handle missing modalities during testing, while still generating robust predictions. Please see details in Section \ref{sec:caml}.}
    \label{fig:app}
    \vspace{-10pt}
\end{figure*}

\vspace{-5pt}
\section{Analysis} \label{sec:analysis}
\vspace{-5pt}
To compare whether \ours~outperforms AML with a single agent theoretically, we analyze from two key perspectives: {\em uncertainty reduction} and {\em data coverage enhancement}. Data coverage can be further discussed from two dimensions: complementary information and information gain. We aim to address three major questions:  (a) \textbf{Uncertainty Reduction}: Does the collaboration among multiple agents help reduce the variance of the posterior distribution, resulting in more confident estimates? (b) \textbf{Complementary Information}: Does the collaboration of multiple agents provide complementary information that increases data coverage? Specifically, does combining observations from each agent lead to a more accurate and comprehensive prediction compared to using a single agent? (c) \textbf{Information Gain}: Does the collaboration increase the mutual information between the observations and the true label?

\vspace{-12pt}
\paragraph{Uncertainty Reduction.}
To address question (a) about uncertainty reduction, the prior $P(y)$ is typically assumed to be Gaussian: $P(y) = \gN(y|\mu_0, \sigma_0^2)$, where $\mu_0$ is the prior mean and $\sigma_0^2$ is the prior variance. 

\textbf{Single-Agent}. 
In the single-agent case, we assume that only agent $\gA_i$ is available and its likelihood $P(x_i|y)$ is Gaussian: $P(x_i|y)=\gN(x_i|\mu_i(y), \sigma_i^2)$, where $\mu_i(y)$ is the mean of the observation $x_i$ given $y$, $\sigma_i^2$ is the variance of the agent $\gA_i$'s observations. The posterior distribution $P(y|x_i)$ is proportional to the product of the prior and likelihood, $P(y|x_i) \propto P(y)P(x_i|y)$, which is also Gaussian. And the posterior variance $\sigma_{single}^2 = \big(\frac{1}{\sigma_0^2} + \frac{1}{\sigma_i^2}\big)^{-1}$ \citep{murphy2007conjugate}. 

% is for $y$, given $x_i$, is: $P(y|x_i) \propto P(y) \gN(y; \mu_i, \sigma_i^2)$. 
% % \begin{equation}
% %     P(y|x_i) \propto P(y) \gN(y; \mu_i, \sigma_i^2).
% % \end{equation}
% Assuming a non-informative prior $P(y)$, the posterior distribution simplifies to the likelihood itself: $P(y|x_i) = \gN(y; \mu_i, \sigma_i^2)$.
% % \begin{equation}
% %     P(y|x_i) = \gN(y; \mu_i, \sigma_i^2).
% % \end{equation}
% Thus, the variance of the estimate of $y$ in the single-agent case is $\sigma_i^2$. 

\textbf{Multi-Agent}. \label{sec:unc_reduce}
In the case of multi-agent collaboration, we model the joint likelihood of the observations $X$ as a multivariate Gaussian distribution, conditioned on the true target variable $y$: $P(X|y) = \gN(X|\mu_X(y), \Sigma)$, where $\mu_X(y)$ is the joint mean of the observations from all agents, conditioned on $y$, $\Sigma$ is the covariance matrix, encoding the correlations between the observations from multiple agents. The posterior $P(y|X) \propto P(y)P(X|y)$, is another Gaussian, with variance $\sigma_{multi}^2 = \big(\frac{1}{\sigma_0^2} + \bf 1^T\Sigma^{-1} \bf 1\big)^{-1}$ \citep{murphy2007conjugate}. Since $\bf 1^T\Sigma^{-1} \bf 1 \geq \frac{1}{\sigma_i^2}$ for any $i$, we have $\sigma^2_{multi} \leq \sigma_{single}^2$. In the extreme case where all agents' observations are perfectly correlated (e.g., they all observe the same thing), the posterior variance would be equivalent to that of a single agent. However, multi-agent collaboration reduces variance compared to a single agent, as long as the observations are not perfectly correlated, proving that collaboration reduces uncertainty. 
\vspace{-5pt}
\paragraph{Enhanced Data Coverage.}
In comparing data coverage between \ours~and AML, we analyze it from two key aspects: complementary information and information gain.
% \color{red}
% CAML is a Multivariate Gaussian model, which has better data coverage, uncertainty quantification
% \color{black}
% \vspace{-9pt}
\vspace{-10pt}
\paragraph{Complementary Information.}
To address question (b) about complementary information, we study data coverage and information provided by each agent in a multi-agent system. Let the entire data space be denoted as $\gD$, which consists of various subsets. Each agent $\gA_i$ in the system covers a subset of this data space: $\gC_i \subseteq \gD$. The overall coverage by the system is given by the union of all subsets covered by individual agents: $\gC_{multi} = \cup_{i=1}^N \gC_i$. This ensures that $|\gC_{multi}| \geq \max |\gC_i|$. If only a single agent is available, it can only observe a portion of the data space, leaving parts of the space unobserved, which leads to incomplete information for estimating the true label $y$. We show an qualitative example of multi-agent collaboration provides complementary information to enhance data coverage in Figure \ref{fig:coverage} in the Appendix.
% Meanwhile, redundancy in data coverage can ensure robustness. In a multi-agent system, if one agent is unable to provide data (e.g., due to occlusion or limited sensor range), other agents can compensate. For any subset $\gC_i \subseteq \gC_{total}$, there is typically at least one other subset $\gC_k \neq \gC_i$ such that $\gC_i \cap \gC_k \neq \emptyset$. This overlap ensures that critical information is not entirely lost, allowing the system to maintain performance through collaborative information sharing among agents.

From a probabilistic perspective, when multi-agent collaboration is in place, the combined likelihood $P(X|y)$ is modeled as a multivariate distribution (as discussed in Section \ref{sec:unc_reduce}). This approach provides a broader and more accurate representation of the data space by integrating information from all agents and modeling the dependencies and correlations between them. Compared to a univariate distribution $P(x_i|y)$ for a single agent $\gA_i$, the multivariate distribution covers a larger portion of the data space $\gD$, thus enhancing data coverage. This allows the exploration of more complex patterns, relationships, and complementary information from different agents. By capturing a richer set of interactions and correlations among the agents' observations, the multivariate distribution supports more informed decision-making. The model's predictions are based on a comprehensive view of the environment, thus leading to more accurate outcomes.

% Suppose that each agent observe a different aspect of the underlying state $y$. Let the true state $y$ be composed of multiple components: $y = (y_1, y_2, \ldots, y_N)$, 
% % \begin{equation}
% %     y = (y_1, y_2, \ldots, y_N),
% % \end{equation}
% where each agent observes a subset of these components, for example, Agent $i$ observes $y_i$.  

% \begin{figure}[hbt]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/data.png}
%     \caption{Change this}
%     \label{fig:enter-label}
% \end{figure}

% \begin{assumption}[Information Contribution] \label{ass:inf_con}
% Each agent $\gA_i$ provides observations $x_i$ that are relevant to the target variable $y$. 
% \end{assumption}
% Using the chain rule of mutual information, we have:
% \begin{equation}
%     I(y;X) = I(y;x_1) + I(y;x_2|x_1) + \cdots + I(y;x_N|x_1, x_2, \ldots, x_{N-1}).
% \end{equation}
\vspace{-10pt}
\paragraph{Information Gain.}
To address question (c) about information gain, we analyze using information theory. Let $I(y;x_i)$ represent the mutual information between the true label $y$ and agent $\gA_i$'s observation $x_i$, which quantifies how much information $x_i$ provides about the estimation of $y$. The mutual information between $y$ and the set of all observations $X$ is $I(y;X)$. In the context of multi-agent collaboration, the joint observations $X$ from multiple agents typically provide more comprehensive information about the true label $y$ compared to the observation of any single agent. Therefore, the mutual information $I(y;X)$ is always greater than or equal to the mutual information from a single agent: $I(y;X) \geq I(y;x_i)$. Thus, the combined observations from multi-agent collaboration provide more information about $y$ than a single observation, improving the overall estimate. By leveraging the combined knowledge from multiple agents, the prediction of $y$ becomes more accurate, reflecting added value of collaboration. Multi-agent systems are generally more informative, as the interaction and joint information between agents can reduce uncertainty about the target variable, as discussed in Section \ref{sec:unc_reduce}.

\vspace{-5pt}
\section{Experiments} \label{sec:exp}
\subsection{Collaborative Decision-Making}
\vspace{-5pt}
To evaluate our approach, we first focus on collaborative decision-making in connected autonomous driving (CAV). This involves making critical decisions for the ego vehicle in accident-prone scenarios, such as determining whether or not to take a braking action. 
\vspace{-10pt}
\paragraph{Data Collection.}
Following prior research \citep{cui2022coopernaut, gao2024collaborative}, we focus on three complex traffic scenarios prone to accidents due to limited sensor coverage or obstructed views, as illustrated in Fig. \ref{fig:scenario}. For more details about the scenarios, please refer to the Appendix \ref{app:env}. For each scenario, we collect 24 trials, dividing them into 12 trials for training through behavior cloning (BC) and 12 trials for testing. Each trial includes RGB images and LiDAR point clouds captured by a variable number of connected vehicles, along with the ground truth actions of the ego vehicle. At each timestamp,  the ego vehicle has a maximum of three collaborative vehicles, provided their distance is within a threshold of 150 meters \citep{gao2024collaborative, cui2022coopernaut}. For each vehicle, both RGB and LiDAR data are used during training, while only RGB data is used during testing in \ours.

\vspace{-10pt}
\paragraph{Experimental Setup.} \label{sec:exp_setup}
% We design our experiments to answer the following key questions: 
% (1) \textbf{Baseline Comparison}: How well does \ours~perform against other baselines for decision-making in CAV? (2) \textbf{Modality-Efficient Superiority}: How does \ours~compare with other approaches that have access to more modalities during testing? By modality-efficient superiority, we refer to a model's ability to achieve comparable or superior performance using fewer modalities compared to other approaches that rely on a richer set of modalities. It highlights the model's efficiency in leveraging minimal data (e.g., RGB only) while still outperforming or matching the performance of models that utilize more complex and data-rich inputs (e.g., RGBD). (3) \textbf{System Generalizability}: How effectively does the system generalize when we have fewer agents during testing compared to training? (e.g., we have multi-agent collaboration during training but only single agent during testing).

% we use RGB as the main modality $I_M$ and LiDAR as the auxiliary modality $I_A$, 

For processing RGB data, we first resize the image to $224\times224$ and use ResNet-18 \citep{he2016deep} as the encoder to extract a feature map. We then apply self-attention on the feature map to dynamically compute the importance of features at different locations. After the self-attention, we apply three convolution layers with each followed by a ReLU activation. Finally, we obtain a 256-dimensional feature representation after passing through a fully connected layer. To facilitate the collaboration and aggregation of RGB feature embeddings from connected vehicles to the ego vehicle, we use the cross-attention mechanism. For processing the LiDAR data, we use the Point Transformer \citep{zhao2021point} as the encoder and utilize the COOPERNAUT \citep{cui2022coopernaut} model to aggregate LiDAR feature embeddings. 

For the training of Knowledge Distillation (KD), we first train a teacher model offline using a binary cross-entropy loss, where each vehicle has both RGB and LiDAR data. Then we train a student model to mimic the behavior of the teacher model with only RGB data for each vehicle. For each data point, the student model receives the same RGB image that the teacher model was given. For further details on the KD training process, please refer to Appendix \ref{sec:kd}. For the prediction module, we use a three-layer MLP. And for the detailed training settings, please see Appendix \ref{train_compl}. We employ the following two metrics for evaluation: (1) \textbf{Accident Detection Rate (ADR)}: This is the ratio of accident-prone cases correctly detected by the model compared to the total ground truth accident-prone cases. An accident-prone case is identified when the ego vehicle performs a braking action. This metric measures the model’s effectiveness in identifying potential accidents. (2) \textbf{Expert Imitation Rate (EIR)}: This denotes the percentage of actions accurately replicated by the model out of the total expert actions. It serves to evaluate how well the model mimics expert driving behavior.

\vspace{-8pt}
\paragraph{Baselines.}
We implement the following baselines for comparison: (1) \textbf{AML} \citep{shen2023auxiliary}: In the AML setting, the ego vehicle operates independently without collaboration with other vehicles (non-collaborative). Both RGB and LiDAR data are available during training for the vehicle, while only RGB data is available during testing. (2) \textbf{COOPERNAUT} \citep{cui2022coopernaut} (Single-Agent): Processes LiDAR data during both training and testing. COOPERNAUT uses the Point Transformer \citep{zhao2021point} as the backbone, encoding raw 3D point clouds into keypoints. (3) \textbf{STGN} \citep{gao2024collaborative} (Single-Agent): Utilizes spatial-temporal graph networks for decision-making, with RGBD data used for both training and testing.

\begin{figure*}[hbt]
     \centering
     \begin{subfigure}[b]{0.29\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/6_1.png}
         \caption{Overtaking}
         \label{fig:6}
     \end{subfigure}
     \hspace{0.02\textwidth}
     \begin{subfigure}[b]{0.29\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/8_1.png}
         \caption{Left Turn}
         \label{fig:8}
     \end{subfigure}
     \hspace{0.02\textwidth}
     \begin{subfigure}[b]{0.29\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/10_1.png}
         \caption{Red Light Violation}
         \label{fig:10}
     \end{subfigure}
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/legend.png}
     \end{subfigure}
     \vspace{-13pt}
        \caption{\textbf{Performance Comparison of \ours~Against Baselines.} We evaluate performance using two metrics: Accident Detection Rate (ADR) and Expert Imitation Rate (EIR) across three accident-prone scenarios: (a) Overtaking, (b) Left Turn, and (c) Red Light Violation. The baselines, AML, COOPERNAUT, and STGN, operate in a single-vehicle, non-collaborative setting. In contrast, {\em \ours~demonstrates superior performance across all scenarios compared to these baselines by up to {\bf 58.13\%}, benefiting considerably from the multi-agent collaboration}.}
        \label{fig:perform_compare}
        \vspace{-7pt}
\end{figure*}


\vspace{-8pt}
\paragraph{Baselines Comparison.}
How well does \ours~perform against other methods for decision-making in CAV? We evaluate \ours~against the baselines and present the results in Figure \ref{fig:perform_compare}, which demonstrate a clear performance advantage of \ours~across all three accident-prone scenarios. The evaluation metrics, accident detection rate (ADR) and expert imitation rate (EIR), reveal that \ours~consistently outperforms AML, COOPERNAUT, and STGN. In particular, \ours~achieves notable improvements in ADR compared to AML: ${\bf 13.2\%}$ in the overtaking scenario, ${\bf 32.6\%}$ in the left turn scenario, and a significant ${\bf 58.13\%}$ in the red light violation scenario. The more pronounced improvements in the left turn and red light violation scenarios can be attributed to the higher complexity of these situations, where restricted views and occlusions present greater challenges for decision-making. Unlike the overtaking scenario on a two-way road, which is relatively less constrained, left turns and red light violations often involve more unpredictable vehicle and pedestrian interactions, requiring enhanced situational awareness. In these more demanding cases, the collaborative framework of \ours~proves especially beneficial, as it allows the ego vehicle to aggregate additional sensory data from connected vehicles, significantly boosting its capacity to detect potential accidents and respond proactively, such as applying braking when necessary to avoid collisions. 

As detailed in Section \ref{sec:analysis}, the collaborative nature of \ours~plays a critical role in reducing the uncertainty in the decision-making processes. By incorporating sensory data from multiple connected vehicles, \ours~can draw on a richer and more diverse dataset, which enables more reliable predictions. This collaborative approach not only reduces the uncertainty in estimations but also enhances data coverage by leveraging complementary information from all connected agents. As a result, the ego vehicle is able to form a more accurate and comprehensive understanding of its environment, particularly in scenarios where its own sensing capabilities are limited by obstructions or blind spots. Compared to single-agent systems, where decisions rely solely on local sensory data, the multi-agent collaboration in \ours~allows the ego vehicle to better handle complex driving environments, especially in accident-prone situations. These baseline comparison results of improvements in safety and decision-making align well with our theoretical analysis. 

\begin{figure*}[hbt]
     \centering
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/6_2.png}
         \caption{Overtaking}
         \label{fig:6_2}
     \end{subfigure}
     \hspace{0.03\textwidth}
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/8_2.png}
         \caption{Left Turn}
         \label{fig:8_2}
     \end{subfigure}
     \hspace{0.03\textwidth}
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/10_2.png}
         \caption{Red Light Violation}
         \label{fig:10_2}
     \end{subfigure}
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/legend.png}
     \end{subfigure}
     \vspace{-13pt}
        \caption{\textbf{Modality-Efficient Superiority of \ours~Against STGN with Multi-Agent Settings.} We compare \ours~with STGN with multi-agent settings, using ADR and EIR metrics across three accident-prone scenarios: (a) Overtaking, (b) Left Turn, and (c) Red Light Violation. {\em While STGN uses both RGB and depth data during testing, \ours~relies solely on RGB, yet achieves comparable, or even better performance}. This highlights the effectiveness of \ours, leveraging LiDAR as an auxiliary modality during training to enhance performance.}
        \label{fig:perform_compare2}
        \vspace{-12pt}
\end{figure*}

\begin{figure*}[hbt]
     \centering
     \begin{subfigure}[b]{0.29\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/6_fle.png}
         \caption{Overtaking}
         \label{fig:6_fle}
     \end{subfigure}
     \hspace{0.02\textwidth}
     \begin{subfigure}[b]{0.29\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/8_fle.png}
         \caption{Left Turn}
         \label{fig:8_fle}
     \end{subfigure}
     \hspace{0.02\textwidth}
     \begin{subfigure}[b]{0.29\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/10_fle.png}
         \caption{Red Light Violation}
         \label{fig:10_fle}
     \end{subfigure}
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/legend.png}
     \end{subfigure}
     \vspace{-10pt}
        \caption{\textbf{System Generalizability of \ours.} We evaluate the generalizability of \ours~by testing the case where we have multi-agent collaboration during training, but only a single agent during testing.   The performance is assessed using ADR and EIR across three accident-prone scenarios: (a) Overtaking, (b) Left Turn, and (c) Red Light Violation. {\em \ours~with a single agent during testing consistently outperforms the three baselines across all scenarios, offering a valuable and cost-effective solution for practical applications}.}
        \label{fig:fle}
        \vspace{-10pt}
\end{figure*}

% It highlights the model’s efficiency in leveraging minimal data (e.g., RGB only) while still outperforming or matching the performance of models that utilize more complex and data-rich inputs (e.g., RGBD). 

\vspace{-10pt}
\paragraph{Modality-Efficient Superiority.}
How does \ours~compare with other approaches that have access to more modalities during testing? By modality-efficient superiority, we refer to a model’s ability to achieve
comparable or even superior performance using fewer modalities compared to other approaches that rely on a richer set of modalities. We evaluate \ours~against STGN \citep{gao2024collaborative} with multi-agent settings. \ours~uses only RGB data during testing but STGN uses both RGB and depth data. Both models are evaluated using the same metrics, ADR and EIR, across the three accident-prone scenarios. Despite the fact that STGN utilizes both RGB and depth data during testing, \ours~achieves comparable, and in some cases superior, performance while relying solely on RGB data, as illustrated in Figure \ref{fig:perform_compare2}. Notably, \ours~exceeds the ADR of STGN by ${\bf 9.26\%}$ in the left-turn scenario, demonstrating that our model can enhance driving safety even when constrained to fewer modalities. This further underscores the strength of \ours, which effectively leverages LiDAR data as an auxiliary modality during training to boost performance, even when such data is unavailable during testing. The fact that \ours~matches or exceeds the performance of a model that uses more data at test time highlights the efficacy of our multi-agent collaboration approach.
\vspace{-8pt}
\paragraph{System Generalizability.}
How effectively does the system generalize when we have fewer agents during testing compared to training? (e.g., we have multi-agent collaboration during training but only single agent during testing). We test the case where multi-agent collaboration is used during training, but only a single agent is present during testing. This test is also motivated by practical constraints, where in many real-world situations, multi-vehicle connected systems are not available, we only have a single vehicle. But it is reasonable to have multi-vehicle connected systems with multiple modalities during training to develop robust models. After training, we can then apply the model on a single vehicle for inference or testing, which is very valuable in practice and provides a cost-effective solution. 

We compare the performance with other baselines, using the same evaluation metrics of ADR and EIR, across three accident-prone scenarios. The comparison results are presented in Figure \ref{fig:fle}. \ours~with a single agent during testing outperforms the three baselines across all scenarios, for both ADR and EIR metrics. This demonstrates that even with single agent during testing, \ours~remains highly effective, by utilizing the multi-agent collaboration and auxiliary modalities provided by the teacher model during training.
% \vspace{-10pt}

% \vspace{-10pt}
Overall, the experimental results clearly illustrate the superiority of our \ours~framework. The ability of \ours~to learn a more effective driving policy stems from the collaborative behavior of multiple agents, which together capture a wider and more nuanced representation of data. This broader data coverage enables the ego vehicle to make better-informed decisions, improving safety and performance, particularly in complex, dynamic, and accident-prone environments where isolated agents with limited sensing.
% capabilities might struggle. 

\vspace{-6pt}
\subsection{Collaborative Semantic Segmentation}
\vspace{-5pt}
To further evaluate our approach, we focus on collaborative semantic segmentation by conducting experiments with real-world data from aerial-ground robots. We use the dataset CoPeD \citep{zhou2024coped}, with one aerial robot and one ground robot, in two different real-world scenarios of the indoor NYUARPL and the outdoor HOUSEA. For more details about the dataset, please refer to \cite{zhou2024coped}. Additionally, we introduce noise to the RGBD data collected by the ground robot. For both aerial and ground robots, RGB and depth data are used during training, while only RGB data is used during testing in \ours.


\vspace{-8pt}
\paragraph{Experimental Setup.}
We adopt the FCN \citep{long2015fully} architecture as the backbone for semantic segmentation. To process RGB and depth data locally for each robot, we use ResNet-18 \citep{he2016deep} as the encoder to extract feature maps of size $7\times7$. For more details about the experimental setup, please refer to the Appendix \ref{app:ag_setup}. We first train a teacher model offline with aerial-ground robots collaboration using cross-entropy loss, where each robot has both RGB and depth data. Then we train a student model to mimic the behavior of the teacher model with only RGB data for both aerial and ground robots through KD. The KD process is similar to that of the collaborative decision-making in CAV, but here we use a cross-entropy loss as the student task loss. For the detailed training settings, please see Appendix \ref{train_compl}.

% We employ a batch size of 32 and the Adam optimizer \citep{kingma2014adam} with an initial learning rate of $1e{-3}$, and a Cosine Annealing Scheduler \citep{loshchilov2016sgdr} to adjust the learning rate over time. We train the model on an Nvidia RTX 3090 GPU for 200 epochs.

We evaluate performance using the \textbf{Mean Intersection over Union (mIoU)} metric, which quantifies the average overlap between predicted segmentation outputs and ground truth across all classes. We compare the performance of \ours~with other baselines including AML \citep{shen2023auxiliary} and FCN \citep{long2015fully}. In the AML approach, only the ground robot operates, with RGB and depth data available during training but only RGB data used for testing. The FCN approach involves only the ground robot operating with RGB data for both training and testing.  

\vspace{-10pt}
\paragraph{Experimental Results.}
We first present the experimental results of baselines comparison in Table \ref{tab:seg}, where \ours~demonstrates superior performance in terms of mIoU across both indoor and outdoor environments. Specifically, \ours~achieves an improvement of mIoU for ${\bf 8.88\%}$ in indoor scenario and ${\bf 10.61\%}$ in outdoor scenario compared to AML \citep{shen2023auxiliary}. We show the qualitative results in Fig. \ref{fig:seg} in the Appendix \ref{app:ag_qual}. Despite the noisy input image from the ground robot, \ours~produces predictions that are closest to the ground truth. This improvement is attributed to \ours's multi-agent collaboration, which provides complementary information to enhance data coverage and offers a more comprehensive understanding of the scenes. Additionally, the utilization of auxiliary depth data during training results in more precise segmentation outputs. We also investigate another variant of \ours, called Pre-fusion \ours, as ablation studies. Both \ours~and Pre-fusion \ours~have their advantages, and \ours~can easily shift to Pre-fusion \ours~because of the flexibility of our framework. Please refer to the Appendix \ref{app:ablation} for more details.

\begin{table}[hbt]
\vspace{-6pt}
\centering
\caption{{\bf Baseline Comparison of Semantic Segmentation} on real-world dataset CoPeD \citep{zhou2024coped} using aerial-ground robots in indoor and outdoor environments. \ours~achieves the highest mIoU in both environments, with upto {\bf 10.61\%} higher accuracy.}
\vspace{-5pt}
\begin{tabular}{lcc}
\hline
\multirow{2}{*}{Approach} & \multicolumn{2}{c}{mIoU (\%)} \\ \cline{2-3} 
 & \multicolumn{1}{l}{Indoor} & \multicolumn{1}{l}{Outdoor} \\ \hline
FCN \citep{long2015fully} & 51.20 & 56.22 \\
AML \citep{shen2023auxiliary} & 55.89 & 60.32 \\
\ours & {\bf 60.05} & {\bf 66.83} \\
Improvement over SOTA & {\bf 4.16-8.88} & {\bf 6.51-10.61} \\ \hline
\end{tabular}
\label{tab:seg}
\vspace{-13pt}
\end{table}

% The use of multi-agent collaboration allows for the exchange of critical spatial and contextual information, resulting in more precise segmentation outputs. 

\vspace{-6pt}
\section{Conclusions}
\vspace{-5pt}
% In conclusion, we propose Collaborative Auxiliary Modality Learning (\ours), a novel framework for multi-agent multi-modality systems. Unlike previous approaches that either concentrate on multi-agent collaboration without addressing modality reduction or handle multi-modality learning in single-agent settings, \ours~unifies these concepts. It allows agents to collaborate in learning from shared modalities during training but enables efficient, modality-reduced inference. This approach not only reduces computational cost and data requirements at test time but also improves the accuracy of predictions by leveraging multi-agent collaboration. Furthermore, we systematically analyze the effectiveness of \ours~from the perspective of uncertainty reduction and data coverage, offering theoretical insights into its advantages over AML. We first validate \ours~on collaborative decision-making tasks for connected autonomous driving in complex and accident-prone scenarios, \ours~achieves an improvement up to $58.3\%$ in accident detection. Additionally, we validate our approach on real-world data from aerial-ground robots for collaborative semantic segmentation, achieving up to $10.8\%$ improvement in mIoU. These enhancements in driving safety and semantic estimation underscore the practical implications of our framework. We also discuss some limitations and possible future work, please see the Appendix \ref{app:limit}.

In conclusion, we propose Collaborative Auxiliary Modality Learning (\ours), a unified framework for multi-agent multi-modality systems. Unlike prior methods that either focus on multi-agent collaboration without modality reduction or address multi-modality learning in single-agent settings, \ours~integrates both aspects. It enables agents to collaborate using shared modalities during training while allowing efficient, modality-reduced inference. This not only lowers computational costs and data requirements at test time but also enhances predictive accuracy through multi-agent collaboration. We provide a theoretical analysis of \ours~in terms of uncertainty reduction and data coverage, highlighting its advantages over AML. \ours~demonstrates up to a $58.13\%$ improvement in accident detection for connected autonomous driving in complex scenarios and up to a $10.61\%$ mIoU gain in real-world aerial-ground collaborative semantic segmentation. These improvements underscore the practical implications of our framework. For limitations and future work, please see the Appendix \ref{app:limit}.
% \vspace{-25pt}
% \paragraph{Limitations and Future Work.}

% For instance, if modalities such as RGB and depth are captured at different time intervals or from non-overlapping fields of view, combining them effectively can be difficult.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning, especially multi-agent multi-modality systems. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}

\subsection{Connected Autonomous Driving Scenarios} \label{app:env}
We utilize a connected autonomous driving environment that integrates CARLA \cite{dosovitskiy2017carla} with AutoCast \cite{qiu2021autocast}. Our evaluation focuses on three complex and accident-prone traffic scenarios, characterized by limited sensor coverage or obstructed views. These scenarios are realistic and include background traffic of 30 vehicles. They involve challenging interactions such as overtaking, lane changing, and red-light violations, which inherently increase the risk of accidents: (1) \textbf{Overtaking}: A sedan is blocked by a truck on a narrow, two-way road with a dashed centerline. The truck also obscures the sedan’s view of oncoming traffic. The ego vehicle must decide when and how to safely pass the truck. (2) \textbf{Left Turn}: The ego vehicle attempts a left turn at a yield sign. Its view is partially blocked by a truck waiting in the opposite left-turn lane, reducing visibility of vehicles coming from the opposite direction. (3) \textbf{Red Light Violation}: As the ego vehicle crosses an intersection, another vehicle runs a red light. Due to nearby vehicles waiting to turn left, the ego vehicle’s sensors are unable to detect the violator. 
\begin{figure}[hbt]
    \centering
    \vspace*{-0.25em}
    \includegraphics[width=0.6\linewidth]{figs/scenario.png}
    \vspace*{-0.65em}
    \caption{Three accident-prone scenarios in connected autonomous driving: overtaking, left turn, and red light violation.}
    \vspace*{-0.2em}
    \label{fig:scenario}
    \vspace*{-0.5em}
\end{figure}

\subsection{Data Coverage}
We present a qualitative example highlighting how multi-agent collaboration provides complementary information to enhance data coverage. In a red-light violation scenario for connected autonomous driving, as shown in the following figure, the ego vehicle’s view is obstructed, rendering the occluded vehicle invisible. However, collaborative vehicles are able to detect the occluded vehicle, providing critical complementary information. This additional data helps the ego vehicle overcome its occluded view, enabling it to make more informed decisions and avoid potential collisions with the occluded vehicle.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/qual2.png}
    \caption{Qualitative example of multi-agent collaboration provides complementary information to enhance data coverage.}
    \label{fig:coverage}
\end{figure}

\subsection{Real-World Aerial-Ground Scenarios}
\subsubsection{Experimental Setup} \label{app:ag_setup}
We resize the input RGB and depth images to $224\times224$. To process RGB and depth data locally for each robot, we use ResNet-18 \citep{he2016deep} as the encoder to extract feature maps of size $7\times7$. The RGB features from both robots are shared and fused through channel-wise concatenation, and the depth features are processed similarly. Then we apply $1\times1$ convolution to reduce the fused feature maps to the original channel dimensions for RGB and depth, respectively. We subsequently apply cross-attention to fuse the RGB and depth feature maps to generate multi-agent multi-modal feature aggregations. These aggregated features are passed through the decoder and upsampled to produce an output map matching the input image size.

\subsubsection{Qualitative Results} \label{app:ag_qual}
We present the qualitative results of collaborative semantic segmentation using real-world data from aerial-ground robots in the following figure. Despite the noisy input image from the ground robot, \ours~produces predictions closest to the ground truth. This performance is attributed to its multi-agent collaboration, which provides complementary information to enhance viewpoints, and its utilization of multi-modal depth data during training, enabling more precise segmentation outputs.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.85\linewidth]{figs/seg2.png}
    \caption{Qualitative results of different approaches on semantic segmentation on real-world data from aerial-ground robots in scenarios of both indoor and outdoor environments. From left to right, input image for the ground robot, ground truth segmentation map, FCN prediction, AML prediction, and \ours~prediction. \ours~prediction is the closest to the ground truth.}
    \label{fig:seg}
\end{figure}

\subsubsection{Ablation Studies} \label{app:ablation}
In the ablation studies, we explore another variant of \ours~called Pre-fusion \ours, applied to the experiment of aerial-ground robots collaborative semantic segmentation. However, it is important to note that this variant can be applied to other domains and experiments as well. In this variant, each robot first locally extract feature maps of size $7\times7$ for both RGB and depth modalities. Instead of separately fusing the RGB and depth features between the robots, we first fuse the feature maps of RGB and depth within each single robot using cross-attention. Then we share and merge the fused RGBD features between robots via concatenation. We also apply $1\times1 $ convolution to reduce the feature maps to the original channel dimensions. The multi-agent, multi-modal feature aggregations then pass through the decoder. Finally, we obtain the output map by upsampling to match the input image size. The mIoU of the Pre-fusion \ours~is similar to that of \ours, achieving $59.16\%$ and $65.78\%$ for indoor and outdoor environments, respectively. By comparison, \ours~achieves $60.05\%$ and $66.83\%$ in the same settings. Although the fusion order is different, both versions benefit from robust feature aggregation and multi-agent collaboration, which ultimately results in better segmentation performance. 

Both \ours~and its variant Pre-fusion \ours~have their advantages, \ours~fuses the same modalities across different agents, which provides better alignment because it ensures consistency in feature representation. And this approach is particularly beneficial when individual agent views are limited, as \ours~effectively leverages diverse viewpoints to provide complementary information, enhancing overall data coverage. On the other hand, Pre-fusion \ours~allows agent-specific contextual understanding by fusing different modalities locally within each agent. Furthermore, the system avoids redundant communication between agents by transmitting multi-modal aggregated features rather than modality-specific features separately. \ours~can easily shift to Pre-fusion \ours~because of the flexibility of our framework, depending on application scenarios. 

\subsection{Complexity Analysis}
\subsubsection{Comparative Training Complexity} \label{train_compl}
We report the training complexity of AML \citep{shen2023auxiliary} and \ours~for the experiments of collaborative decision-making in CAV, and collaborative semantic segmentation for aerial-ground robots in Table \ref{tab:cav} and Table \ref{tab:ag}, respectively. For the experiments, we employ a batch size of 32 and the Adam optimizer \citep{kingma2014adam} with an initial learning rate of $1e{-3}$, and a Cosine Annealing Scheduler \citep{loshchilov2016sgdr} to adjust the learning rate over time. The model is trained on an Nvidia RTX 3090 GPU with AMD Ryzen 9 5900 CPU and 32 GB RAM for 200 epochs.

\begin{table}[h]
\centering
\caption{Training complexity of AML and \ours~in collaborative decision-making for connected autonomous driving.}
\label{tab:cav}
\begin{tabular}{lcc}
\hline
Approach & Parameters & Time/epoch \\ \hline
AML \citep{shen2023auxiliary} & 19.5M & 34s \\
% COOPERNAUT \citep{cui2022coopernaut} & 0.34 M &  \\
% STGN \citep{gao2024collaborative} & 5 & 22s \\
\ours & 39.3M & 73s \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Training complexity of AML and \ours~in collaborative semantic segmentation for aerial-ground robots.}
\label{tab:ag}
\begin{tabular}{lcc}
\hline
Approach & Parameters & Time/epoch \\ \hline
% FCN \citep{long2015fully} & 13.5M & 2s \\
AML \citep{shen2023auxiliary} & 13.5M & 3s \\
\ours & 25.5M & 7s \\ \hline
\end{tabular}
\end{table}

\subsubsection{Time and Space Complexity} \label{app:comp}
In \ours, the agents' embeddings are shared based on whether the system operates in a centralized or decentralized manner. If the system is a centralized, all collaborative agents share their data with one designated ego agent for centralized processing. Each of the $N-1$ collaborative agents performs its local computation independently, with a time complexity of $O(T_c)$ and a space complexity of $O(S_c)$, where $T_c$ represents the time required for local computation, and $S_c$ is the associated space. Thus, the total computation time and space complexities for all collaborative agents are $O(T_c(N-1))$ and $O(S_c(N-1))$, respectively. For simplicity, assuming each communication from one collaborative agent to the ego agent consumes $O(D)$ time complexity and $O(M)$ space complexity, where $D$ is the time required for communication and $M$ is the corresponding space. Therefore, the total communication time and space complexities for gathering information at the ego agent are $O(D(N-1))$ and $O(M(N-1))$, respectively. Then the ego agent aggregates the received data, running a model, having a time and space complexity $O(T_e)$ and $O(S_e)$, where $T_e$ and $S_e$ represent the time and space required for the ego agent's computation. So the total time and space complexities are $O(T_c(N-1)+D(N-1)+T_e)$ and $O(S_c(N-1)+M(N-1)+S_e)$, respectively.

If the system is decentralized, each agent performs its local computation and shares information with other agents. For simplicity, let the local computation for a single agent has a time complexity of $O(T)$,  where $T$ is the time required for local computation. Assume that communication from one agent to another agent requires $O(D)$ time complexity and $O(M)$ space complexity, where $D$ represents the time of communication between two agents, and $M$ denotes the space required for such communication. For $N$ agents, the total computation time complexity is $O(NT)$. In the worst case, each agent share data with all other agents, this can result in $O(N^2D)$ for pairwise sharing. So the total time complexity is $O(NT+N^2D)$. For space complexity, the storage requirement for all agents is $O(NS)$, where $S$ is the space needed per agent. Communication between agents adds an additional complexity of $O(N^2M)$. So the total space complexity is $O(NS+N^2M)$. In the typical case, if each agent communicates with only other $k$ agents ($k \ll N$) rather than all $N-1$ agents. The total time and space complexities become $O(NT+NkD)$ and $O(NS+NkM)$, respectively. 


% Therefore, the time and space complexity are now quadratic with the number of agents, particularly when all agents need to share information with one another.

\subsection{Knowledge Distillation} \label{sec:kd}
We begin by training a teacher decision-making model $\mathcal{T}$ offline using both RGB and LiDAR data, with a binary cross-entropy loss: $\mathcal{L}_{BCE}(y, \mathcal{T}) = -\mathbb{E}_\mathcal{D}\big[y_i\log(p_i) + (1-y_i)\log(1-p_i) \big]$, where $\mathcal{D}$ is the dataset, $y_i$ is the ground truth indicating whether the vehicle should brake, $p_i$ is the predicted probability by the teacher model $\mathcal{T}$. The student model $\mathcal{S}$ is trained to mimic the behavior of the teacher model while having less modalities. For each data point, the student model receives the same RGB image that the teacher model was given. The loss for the student model is a combination of two terms: the distillation loss using KL divergence between the student output and teacher output (soft targets), and the student task loss, which is the binary cross entropy loss between the student output and the true labels (hard targets). The soft targets from the teacher enrich learning with class similarities, while hard targets ensure alignment with true labels. The soft targets are generated by applying a temperature scaling to the logits. The scaled logits are defined as: $z_i = \frac{\exp{(z_i/t)}}{\exp{(z_0/t)}+\exp{(z_1/t)}}$, where $z_i$ is the logit for class $i$ and $t=3.0$ is the temperature parameter. The distillation loss is defined as: $\mathcal{L}_{KD}(\mathcal{S}, \mathcal{T}) = -\sum z_i^{\mathcal{T}} \log(z_i^\mathcal{S})$, where $z_i^{\mathcal{T}}$ and $z_i^{\mathcal{S}}$ are the soft target probability from the teacher and student model, respectively. The overall loss for the student model is a weighted sum of the distillation loss and the binary cross-entropy loss: $\mathcal{L}_\mathcal{S} = (1-\alpha) \mathcal{L}_{BCE}(y, \mathcal{S}) + \alpha t^2 \mathcal{L}_{KD}(\mathcal{S}, \mathcal{T})$, where $\alpha=0.5$ controls the trade-off between the two losses. After the training of knowledge distillation process, we obtain a student model that uses only RGB data while learning from a teacher model that has access to both RGB and LiDAR data. This enables the student model to be effective during testing with only RGB data. Additionally, by leveraging knowledge distillation, the student model benefits from the additional insights provided by the LiDAR data during training, learning more effectively compared to training solely with RGB data. 

\subsection{Limitations and Future Work} \label{app:limit}
Even though the advancements of \ours, there are some limitations. One limitation is that if the modalities are misaligned, the model may struggle to perform effective fusion, leading to incorrect predictions. The auxiliary modalities or views from collaborative agents may become noise, useless or even degrading performance. Another limitation is the increasing system complexity. As the number of agents increases, the complexity of the system grows. The fusion of multi-agent and multi-modal data introduces challenges related to coordination overhead, which may lead to delays in the collaborative learning process. Future work can focus on address these limitations. 

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
