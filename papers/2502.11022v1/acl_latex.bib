% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{li2014nalir,
  title={NaLIR: an interactive natural language interface for querying relational databases},
  author={Li, Fei and Jagadish, Hosagrahar V},
  booktitle={Proceedings of the 2014 ACM SIGMOD international conference on Management of data},
  pages={709--712},
  year={2014}
}

@inproceedings{lu2024natural,
  title={Bridging the Gap: Enabling Natural Language Queries for NoSQL Databases through Text-to-NoSQL Translation},
  author={Lu, Jinwei and Song, Yuanfeng and Qin, Zhiqian and Zhang, Haodi and Zhang, Chen and Wong, Raymond Chi-Wing},
  year={2025}
}

@INPROCEEDINGS{7096207,
  author={Bhogal, Jagdev and Choksi, Imran},
  booktitle={2015 IEEE 29th International Conference on Advanced Information Networking and Applications Workshops}, 
  title={Handling Big Data Using NoSQL}, 
  year={2015},
  volume={},
  number={},
  pages={393-398},
  keywords={Libraries;Relational databases;Big data;Data models;Business;Distributed databases;NoSQL;SQL;databases;structured data;unstructured data;Big Data},
  doi={10.1109/WAINA.2015.19}}

@article{nalla2022sql,
  title={SQL vs. NoSQL: Choosing the Right Database for Your Ecommerce Platform},
  author={Nalla, Lakshmi Nivas and Reddy, Vijay Mallik},
  journal={International Journal of Advanced Engineering Technologies and Innovations},
  volume={1},
  number={2},
  pages={54--69},
  year={2022}
}

@INPROCEEDINGS{7529531,
  author={Van-Dai Ta and Chuan-Ming Liu and Nkabinde, Goodwill Wandile},
  booktitle={2016 IEEE International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)}, 
  title={Big data stream computing in healthcare real-time analytics}, 
  year={2016},
  volume={},
  number={},
  pages={37-42},
  keywords={Medical services;Monitoring;Biomedical monitoring;Genomics;Bioinformatics;Magnetic resonance imaging;Manuals;big data;stream computing;real-time;healthcare analytics;storm;Kafka;NoSQL Cassandra},
  doi={10.1109/ICCCBDA.2016.7529531}}


@inproceedings{song2025natural,
  title={QueryCraft: A Natural Language-Driven NoSQL Database Querying System Powered by Large Language Models},
  author={Song, Yuanfeng and Lu, Jinwei and Qin, Zhiqian and Zhang, Chen and Wong, Raymond Chi-Wing},
  booktitle={arXiv},
  year={2025}
}


@article{li2014constructing,
  title={Constructing an interactive natural language interface for relational databases},
  author={Li, Fei and Jagadish, Hosagrahar V},
  journal={Proceedings of the VLDB Endowment},
  volume={8},
  number={1},
  pages={73--84},
  year={2014},
  publisher={VLDB Endowment}
}

@inproceedings{baik2020duoquest,
  title={Duoquest: A dual-specification system for expressive SQL queries},
  author={Baik, Christopher and Jin, Zhongjun and Cafarella, Michael and Jagadish, HV},
  booktitle={Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
  pages={2319--2329},
  year={2020}
}

@article{quamar2022natural,
  title={Natural language interfaces to data},
  author={Quamar, Abdul and Efthymiou, Vasilis and Lei, Chuan and {\"O}zcan, Fatma and others},
  journal={Foundations and Trends{\textregistered} in Databases},
  volume={11},
  number={4},
  pages={319--414},
  year={2022},
  publisher={Now Publishers, Inc.}
}

@article{sen2020athena++,
  title={Athena++ natural language querying for complex nested sql queries},
  author={Sen, Jaydeep and Lei, Chuan and Quamar, Abdul and {\"O}zcan, Fatma and Efthymiou, Vasilis and Dalmia, Ayushi and Stager, Greg and Mittal, Ashish and Saha, Diptikalyan and Sankaranarayanan, Karthik},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={12},
  pages={2747--2759},
  year={2020},
  publisher={VLDB Endowment}
}

@article{hochreiter1997long,
  title={Long Short-term Memory},
  author={Hochreiter, S},
  journal={Neural Computation MIT-Press},
  year={1997}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{liu2023multi,
  title={Multi-hop relational graph attention network for text-to-sql parsing},
  author={Liu, Hu and Shi, Yuliang and Zhang, Jianlin and Wang, Xinjun and Li, Hui and Kong, Fanyu},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2023},
  organization={IEEE}
}

@inproceedings{hui-etal-2022-s2sql,
    title = "{S}$^2${SQL}: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-{SQL} Parsers",
    author = "Hui, Binyuan  and
      Geng, Ruiying  and
      Wang, Lihan  and
      Qin, Bowen  and
      Li, Yanyang  and
      Li, Bowen  and
      Sun, Jian  and
      Li, Yongbin",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.99/",
    doi = "10.18653/v1/2022.findings-acl.99",
    pages = "1254--1262",
    abstract = "The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsing. The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax well. In this paper, we propose S$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL parsers, which effectively leverages the syntactic dependency information of questions in text-to-SQL to improve the performance. We also employ the decoupling constraint to induce diverse relational edge embedding, which further improves the network`s performance. Experiments on the Spider and robustness setting Spider-Syn demonstrate that the proposed approach outperforms all existing methods when pre-training models are used, resulting in a performance ranks first on the Spider leaderboard."
}

@inproceedings{li2023graphix,
  title={Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing},
  author={Li, Jinyang and Hui, Binyuan and Cheng, Reynold and Qin, Bowen and Ma, Chenhao and Huo, Nan and Huang, Fei and Du, Wenyu and Si, Luo and Li, Yongbin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={13076--13084},
  year={2023}
}

@inproceedings{qi-etal-2022-rasat,
    title = "{RASAT}: Integrating Relational Structures into Pretrained {S}eq2{S}eq Model for Text-to-{SQL}",
    author = "Qi, Jiexing  and
      Tang, Jingyao  and
      He, Ziwei  and
      Wan, Xiangpeng  and
      Cheng, Yu  and
      Zhou, Chenghu  and
      Wang, Xinbing  and
      Zhang, Quanshi  and
      Lin, Zhouhan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.211/",
    doi = "10.18653/v1/2022.emnlp-main.211",
    pages = "3215--3229",
    abstract = "Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively translating natural language into SQL queries. However, introducing these structural relations comes with prices: they often result in a specialized model structure, which largely prohibits using large pretrained models in text-to-SQL. To address this problem, we propose RASAT: a Transformer seq2seq architecture augmented with relation-aware self-attention that could leverage a variety of relational structures while inheriting the pretrained parameters from the T5 model effectively. Our model can incorporate almost all types of existing relations in the literature, and in addition, we propose introducing co-reference relations for the multi-turn scenario. Experimental results on three widely used text-to-SQL datasets, covering both single-turn and multi-turn scenarios, have shown that RASAT could achieve competitive results in all three benchmarks, achieving state-of-the-art execution accuracy (75.5{\%} EX on Spider, 52.6{\%} IEX on SParC, and 37.4{\%} IEX on CoSQL)."
}

@inproceedings{wang-etal-2020-rat,
    title = "{RAT-SQL}: Relation-Aware Schema Encoding and Linking for Text-to-{SQL} Parsers",
    author = "Wang, Bailin  and
      Shin, Richard  and
      Liu, Xiaodong  and
      Polozov, Oleksandr  and
      Richardson, Matthew",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.677/",
    doi = "10.18653/v1/2020.acl-main.677",
    pages = "7567--7578",
    abstract = "When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2{\%}, surpassing its best counterparts by 8.7{\%} absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6{\%} on the Spider leaderboard. In addition, we observe qualitative improvements in the model`s understanding of schema linking and alignment. Our implementation will be open-sourced at \url{https://github.com/Microsoft/rat-sql}."
}

@inproceedings{xu-etal-2018-sql,
    title = "{SQL}-to-Text Generation with Graph-to-Sequence Model",
    author = "Xu, Kun  and
      Wu, Lingfei  and
      Wang, Zhiguo  and
      Feng, Yansong  and
      Sheinin, Vadim",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1112/",
    doi = "10.18653/v1/D18-1112",
    pages = "931--936",
    abstract = "Previous work approaches the SQL-to-text generation task using vanilla Seq2Seq models, which may not fully capture the inherent graph-structured information in SQL query. In this paper, we propose a graph-to-sequence model to encode the global structure information into node embeddings. This model can effectively learn the correlation between the SQL query pattern and its interpretation. Experimental results on the WikiSQL dataset and Stackoverflow dataset show that our model outperforms the Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance."
}

@inproceedings{zheng-etal-2022-hie,
    title = "{HIE}-{SQL}: History Information Enhanced Network for Context-Dependent Text-to-{SQL} Semantic Parsing",
    author = "Zheng, Yanzhao  and
      Wang, Haibin  and
      Dong, Baohua  and
      Wang, Xingjun  and
      Li, Changshan",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.236/",
    doi = "10.18653/v1/2022.findings-acl.236",
    pages = "2997--3007",
    abstract = "Recently, context-dependent text-to-SQL semantic parsing which translates natural language into SQL in an interaction process has attracted a lot of attentions. Previous works leverage context dependence information either from interaction history utterances or previous predicted queries but fail in taking advantage of both of them since of the mismatch between the natural language and logic-form SQL. In this work, we propose a History Information Enhanced text-to-SQL model (HIE-SQL) to exploit context dependence information from both history utterances and the last predicted SQL query. In view of the mismatch, we treat natural language and SQL as two modalities and propose a bimodal pre-trained model to bridge the gap between them. Besides, we design a schema-linking graph to enhance connections from utterances and the SQL query to database schema. We show our history information enhanced methods improve the performance of HIE-SQL by a significant margin, which achieves new state-of-the-art results on two context-dependent text-to-SQL benchmarks, the SparC and CoSQL datasets, at the writing time."
}

@inproceedings{
yu2021grappa,
title={Gra{\{}PP{\}}a: Grammar-Augmented Pre-Training for Table Semantic Parsing},
author={Tao Yu and Chien-Sheng Wu and Xi Victoria Lin and bailin wang and Yi Chern Tan and Xinyi Yang and Dragomir Radev and richard socher and Caiming Xiong},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=kyaIeYj4zZ}
}

@inproceedings{xiang2023g3r,
  title={G3R: A Graph-Guided Generate-and-Rerank Framework for Complex and Cross-domain Text-to-SQL Generation},
  author={Xiang, Yanzheng and Zhang, Qian-Wen and Zhang, Xu and Liu, Zejie and Cao, Yunbo and Zhou, Deyu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={338--352},
  year={2023}
}

@article{DBLP:journals/corr/abs-2408-05109,
  publtype={informal},
  author={Xinyu Liu and Shuyu Shen and Boyan Li and Peixian Ma and Runzhi Jiang and Yuyu Luo and Yuxin Zhang and Ju Fan and Guoliang Li and Nan Tang},
  title={A Survey of NL2SQL with Large Language Models: Where are we, and where are we going?},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2408.05109},
  url={https://doi.org/10.48550/arXiv.2408.05109}
}



@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}


@misc{
liu2020roberta,
title={Ro{\{}BERT{\}}a: A Robustly Optimized {\{}BERT{\}} Pretraining Approach},
author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
year={2020},
url={https://openreview.net/forum?id=SyxS0T4tvS}
}

@inproceedings{yin-etal-2020-tabert,
    title = "{T}a{BERT}: Pretraining for Joint Understanding of Textual and Tabular Data",
    author = "Yin, Pengcheng  and
      Neubig, Graham  and
      Yih, Wen-tau  and
      Riedel, Sebastian",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.745/",
    doi = "10.18653/v1/2020.acl-main.745",
    pages = "8413--8426",
    abstract = "Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider."
}


@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703/",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{10.5555/3304222.3304323,
author = {Cai, Ruichu and Xu, Boyan and Zhang, Zhenjie and Yang, Xiaoyan and Li, Zijian and Liang, Zhihao},
title = {An encoder-decoder framework translating natural language to database queries},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Machine translation is going through a radical revolution, driven by the explosive development of deep learning techniques using Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). In this paper, we consider a special case in machine translation problems, targeting to convert natural language into Structured Query Language (SQL) for data retrieval over relational database. Although generic CNN and RNN learn the grammar structure of SQL when trained with sufficient samples, the accuracy and training efficiency of the model could be dramatically improved, when the translation model is deeply integrated with the grammar rules of SQL. We present a new encoder-decoder framework, with a suite of new approaches, including new semantic features fed into the encoder, grammar-aware states injected into the memory of decoder, as well as recursive state management for sub-queries. These techniques help the neural network better focus on understanding semantics of operations in natural language and save the efforts on SQL grammar learning. The empirical evaluation on real world database and queries show that our approach outperform state-of-the-art solution by a significant margin.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {3977–3983},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@inproceedings{popescu2022addressing,
  title={Addressing limitations of encoder-decoder based approach to text-to-sql},
  author={Popescu, Octavian and Manotas, Irene and Vo, Ngoc Phuoc An and Yeo, Hangu and Khorashani, Elahe and Sheinin, Vadim},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={1593--1603},
  year={2022}
}




@article{dong2023c3,
  title={C3: Zero-shot text-to-sql with chatgpt},
  author={Dong, Xuemei and Zhang, Chao and Ge, Yuhang and Mao, Yuren and Gao, Yunjun and Lin, Jinshu and Lou, Dongfang and others},
  journal={arXiv preprint arXiv:2307.07306},
  year={2023}
}


@article{10.14778/3641204.3641221,
author = {Gao, Dawei and Wang, Haibin and Li, Yaliang and Sun, Xiuyu and Qian, Yichen and Ding, Bolin and Zhou, Jingren},
title = {Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation},
year = {2024},
issue_date = {January 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3641204.3641221},
doi = {10.14778/3641204.3641221},
abstract = {Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6\% execution accuracy and sets a new bar.To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {1132–1145},
numpages = {14}
}

@inproceedings{li2023resdsql,
  title={Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql},
  author={Li, Haoyang and Zhang, Jing and Li, Cuiping and Chen, Hong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={13067--13075},
  year={2023}
}

@inproceedings{lin-etal-2020-bridging,
    title = "Bridging Textual and Tabular Data for Cross-Domain Text-to-{SQL} Semantic Parsing",
    author = "Lin, Xi Victoria  and
      Socher, Richard  and
      Xiong, Caiming",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.438/",
    doi = "10.18653/v1/2020.findings-emnlp.438",
    pages = "4870--4888",
    abstract = "We present BRIDGE, a powerful sequential architecture for modeling dependencies between natural language questions and relational databases in cross-DB semantic parsing. BRIDGE represents the question and DB schema in a tagged sequence where a subset of the fields are augmented with cell values mentioned in the question. The hybrid sequence is encoded by BERT with minimal subsequent layers and the text-DB contextualization is realized via the fine-tuned deep attention in BERT. Combined with a pointer-generator decoder with schema-consistency driven search space pruning, BRIDGE attained state-of-the-art performance on the well-studied Spider benchmark (65.5{\%} dev, 59.2{\%} test), despite being much simpler than most recently proposed models for this task. Our analysis shows that BRIDGE effectively captures the desired cross-modal dependencies and has the potential to generalize to more text-DB related tasks. Our model implementation is available at \url{https://github.com/salesforce/TabularSemanticParsing}."
}

@article{pourreza2024din,
  title={Din-sql: Decomposed in-context learning of text-to-sql with self-correction},
  author={Pourreza, Mohammadreza and Rafiei, Davood},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{rubin-berant-2021-smbop,
    title = "{S}m{B}o{P}: Semi-autoregressive Bottom-up Semantic Parsing",
    author = "Rubin, Ohad  and
      Berant, Jonathan",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.29/",
    doi = "10.18653/v1/2021.naacl-main.29",
    pages = "311--324",
    abstract = "The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height {\ensuremath{\leq}} t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a {\textasciitilde}5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GraPPa."
}

@inproceedings{scholak-etal-2021-picard,
    title = "{PICARD}: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
    author = "Scholak, Torsten  and
      Schucher, Nathan  and
      Bahdanau, Dzmitry",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.779/",
    doi = "10.18653/v1/2021.emnlp-main.779",
    pages = "9895--9901",
    abstract = "Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code available at \url{https://github.com/ElementAI/picard}), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions."
}


@inproceedings{10.5555/3600270.3602070,
author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
title = {Chain-of-thought prompting elicits reasoning in large language models},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1800},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@article{liu2023divide,
  title={Divide and prompt: Chain of thought prompting for text-to-sql},
  author={Liu, Xiping and Tan, Zhao},
  journal={arXiv preprint arXiv:2304.11556},
  year={2023}
}

@inproceedings{zhang-etal-2023-act,
    title = "{ACT}-{SQL}: In-Context Learning for Text-to-{SQL} with Automatically-Generated Chain-of-Thought",
    author = "Zhang, Hanchong  and
      Cao, Ruisheng  and
      Chen, Lu  and
      Xu, Hongshen  and
      Yu, Kai",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.227/",
    doi = "10.18653/v1/2023.findings-emnlp.227",
    pages = "3501--3532",
    abstract = "Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs' reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole process doesn`t need manual labeling. Our approach is cost-saving since we only use the LLMs' API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches."
}

@inproceedings{zhang-etal-2024-coe,
    title = "{C}o{E}-{SQL}: In-Context Learning for Multi-Turn Text-to-{SQL} with Chain-of-Editions",
    author = "Zhang, Hanchong  and
      Cao, Ruisheng  and
      Xu, Hongshen  and
      Chen, Lu  and
      Yu, Kai",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.361/",
    doi = "10.18653/v1/2024.naacl-long.361",
    pages = "6487--6508",
    abstract = "Recently, Large Language Models (LLMs) have been demonstrated to possess impressive capabilities in a variety of domains and tasks. We investigate the issue of prompt design in the multi-turn text-to-SQL task and attempt to enhance the LLMs' reasoning capacity when generating SQL queries. In the conversational context, the current SQL query can be modified from the preceding SQL query with only a few operations due to the context dependency. We introduce our method called CoE-SQL which can prompt LLMs to generate the SQL query based on the previously generated SQL query with an edition chain. We also conduct extensive ablation studies to determine the optimal configuration of our approach. Our approach outperforms different in-context learning baselines stably and achieves state-of-the-art performances on two benchmarks SParC and CoSQL using LLMs, which is also competitive to the SOTA fine-tuned models."
}

@inproceedings{
zhou2023leasttomost,
title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
author={Denny Zhou and Nathanael Sch{\"a}rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V Le and Ed H. Chi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=WZH7099tgfM}
}

@inproceedings{gan-etal-2021-natural-sql,
    title = "Natural {SQL}: Making {SQL} Easier to Infer from Natural Language Specifications",
    author = "Gan, Yujian  and
      Chen, Xinyun  and
      Xie, Jinxia  and
      Purver, Matthew  and
      Woodward, John R.  and
      Drake, John  and
      Zhang, Qiaofu",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.174/",
    doi = "10.18653/v1/2021.findings-emnlp.174",
    pages = "2030--2042",
    abstract = "Addressing the mismatch between natural language descriptions and the corresponding SQL queries is a key challenge for text-to-SQL translation. To bridge this gap, we propose an SQL intermediate representation (IR) called Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities of SQL, while it simplifies the queries as follows: (1) dispensing with operators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are usually hard to find counterparts in the text descriptions; (2) removing the need of nested subqueries and set operators; and (3) making the schema linking easier by reducing the required number of schema items. On Spider, a challenging text-to-SQL benchmark that contains complex and nested SQL queries, we demonstrate that NatSQL outperforms other IRs, and significantly improves the performance of several previous SOTA models. Furthermore, for existing models that do not support executable SQL generation, NatSQL easily enables them to generate executable SQL queries, and achieves the new state-of-the-art execution accuracy."
}

@inproceedings{arora-etal-2023-adapt,
    title = "Adapt and Decompose: Efficient Generalization of Text-to-{SQL} via Domain Adapted Least-To-Most Prompting",
    author = "Arora, Aseem  and
      Bhaisaheb, Shabbirhussain  and
      Nigam, Harshit  and
      Patwardhan, Manasi  and
      Vig, Lovekesh  and
      Shroff, Gautam",
    editor = "Hupkes, Dieuwke  and
      Dankers, Verna  and
      Batsuren, Khuyagbaatar  and
      Sinha, Koustuv  and
      Kazemnejad, Amirhossein  and
      Christodoulopoulos, Christos  and
      Cotterell, Ryan  and
      Bruni, Elia",
    booktitle = "Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.genbench-1.3/",
    doi = "10.18653/v1/2023.genbench-1.3",
    pages = "25--47",
    abstract = "Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to be performed one-time per new database with minimal human intervention. Our approach demonstrates superior performance on the KaggleDBQA dataset, designed to evaluate generalizability for the Text-to-SQL task. We further showcase consistent performance improvement of LTMP-DA-GP over GP, across LLMs and databases of KaggleDBQA, highlighting the efficacy and model agnostic benefits of our prompt based adapt and decompose approach."
}

@inproceedings{
khot2023decomposed,
title={Decomposed Prompting: A Modular Approach for Solving Complex Tasks},
author={Tushar Khot and Harsh Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=_nGgzQjzaRy}
}

@inproceedings{tai-etal-2023-exploring,
    title = "Exploring Chain of Thought Style Prompting for Text-to-{SQL}",
    author = "Tai, Chang-Yu  and
      Chen, Ziru  and
      Zhang, Tianshu  and
      Deng, Xiang  and
      Sun, Huan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.327/",
    doi = "10.18653/v1/2023.emnlp-main.327",
    pages = "5376--5393",
    abstract = "In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs' reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting and least-to-most prompting. Our experiments demonstrate that iterative prompting as in least-to-most prompting may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realistic set, respectively, compared to the standard prompting method without reasoning steps; 2.4 and 1.5 point absolute gains, compared to the least-to-most prompting method."
}


@inproceedings{wang-etal-2025-mac,
    title = "{MAC}-{SQL}: A Multi-Agent Collaborative Framework for Text-to-{SQL}",
    author = "Wang, Bing  and
      Ren, Changyu  and
      Yang, Jian  and
      Liang, Xinnian  and
      Bai, Jiaqi  and
      Chai, LinZheng  and
      Yan, Zhao  and
      Zhang, Qian-Wen  and
      Yin, Di  and
      Sun, Xing  and
      Li, Zhoujun",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.36/",
    pages = "540--557",
    abstract = "Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on {\textquotedblleft}huge{\textquotedblright} databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce MAC-SQL, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine the upper bound of our framework. We then fine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging Code Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that SQL-Llama achieves a comparable execution accuracy of 43.94, compared to the baseline accuracy of 46.35 for vanilla GPT-4. At the time of writing, MAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the BIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test set."
}

@inproceedings{xie-etal-2024-decomposition,
    title = "Decomposition for Enhancing Attention: Improving {LLM}-based Text-to-{SQL} through Workflow Paradigm",
    author = "Xie, Yuanzhen  and
      Jin, Xinzhou  and
      Xie, Tao  and
      Matrixmxlin, Matrixmxlin  and
      Chen, Liang  and
      Yu, Chenyun  and
      Lei, Cheng  and
      Zhuo, Chengxiang  and
      Hu, Bo  and
      Li, Zang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.641/",
    doi = "10.18653/v1/2024.findings-acl.641",
    pages = "10796--10816",
    abstract = "In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model`s attention. Additionally, the inclusion of self-correction and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three datasets demonstrate that our approach outperforms other methods by a significant margin. About 2-3 percentage point improvements compared to the existing baseline on the Spider Dev, Spider-Realistic, and Bird Dev datasets and new SOTA results on the Spider Test dataset are achieved. Our code is available on GitHub: \url{https://github.com/FlyingFeather/DEA-SQL}."
}

@article{journals/corr/abs-2310-18752,
  added-at = {2023-11-02T00:00:00.000+0100},
  author = {Sui, Guanghu and Li, Zhishuai and Li, Ziyue and Yang, Sun and Ruan, Jingqing and Mao, Hangyu and Zhao, Rui},
  biburl = {https://www.bibsonomy.org/bibtex/2c47066152ce64147501e2bec1951598c/dblp},
  ee = {https://doi.org/10.48550/arXiv.2310.18752},
  interhash = {e2e7404f9836d4501a1999915e0d4c8d},
  intrahash = {c47066152ce64147501e2bec1951598c},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2024-04-09T00:15:48.000+0200},
  title = {Reboost Large Language Model-based Text-to-SQL, Text-to-Python, and Text-to-Function - with Real Applications in Traffic Domain.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr2310.html#abs-2310-18752},
  volume = {abs/2310.18752},
  year = 2023
}


@article{wang2023dbcopilot,
  title={Dbcopilot: Scaling natural language querying to massive databases},
  author={Wang, Tianshu and Lin, Hongyu and Han, Xianpei and Sun, Le and Chen, Xiaoyang and Wang, Hao and Zeng, Zhenyu},
  journal={arXiv preprint arXiv:2312.03463},
  year={2023}
}

@article{zhang2024structure,
  title={Structure guided large language model for sql generation},
  author={Zhang, Qinggang and Dong, Junnan and Chen, Hao and Li, Wentao and Huang, Feiran and Huang, Xiao},
  journal={arXiv preprint arXiv:2402.13284},
  year={2024}
}


@article{li2024codes,
  title={Codes: Towards building open-source language models for text-to-sql},
  author={Li, Haoyang and Zhang, Jing and Liu, Hanbing and Fan, Ju and Zhang, Xiaokang and Zhu, Jun and Wei, Renjie and Pan, Hongyan and Li, Cuiping and Chen, Hong},
  journal={Proceedings of the ACM on Management of Data},
  volume={2},
  number={3},
  pages={1--28},
  year={2024},
  publisher={ACM New York, NY, USA}
}



@article{zhong2018seqsql,
  title={Seq2sql: Generating structured queries from natural language using reinforcement learning},
  author={Zhong, Victor and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1709.00103},
  year={2017}
}

@inproceedings{yu-etal-2018-spider,
    title = "{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task",
    author = "Yu, Tao  and
      Zhang, Rui  and
      Yang, Kai  and
      Yasunaga, Michihiro  and
      Wang, Dongxu  and
      Li, Zifan  and
      Ma, James  and
      Li, Irene  and
      Yao, Qingning  and
      Roman, Shanelle  and
      Zhang, Zilin  and
      Radev, Dragomir",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1425/",
    doi = "10.18653/v1/D18-1425",
    pages = "3911--3921",
    abstract = "We present \textit{Spider}, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7{\%} exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at \url{https://yale-lily.github.io/seq2sql/spider}."
}
@inproceedings{lee-etal-2021-kaggledbqa,
    title = "{K}aggle{DBQA}: Realistic Evaluation of Text-to-{SQL} Parsers",
    author = "Lee, Chia-Hsuan  and
      Polozov, Oleksandr  and
      Richardson, Matthew",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.176/",
    doi = "10.18653/v1/2021.acl-long.176",
    pages = "2261--2273",
    abstract = "The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical deployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2{\%}, doubling their performance."
}
 
@inproceedings{10.5555/3666122.3667957,
author = {Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Geng, Ruiying and Huo, Nan and Zhou, Xuanhe and Ma, Chenhao and Li, Guoliang and Chang, Kevin C.C. and Huang, Fei and Cheng, Reynold and Li, Yongbin},
title = {Can LLM already serve as a database interface? a big bench for large-scale database grounded text-to-SQLs},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Text-to-SQL parsing, which aims at converting natural language questions into executable SQLs, has gained increasing attention in recent years. In particular, GPT-4 and Claude-2 have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database values leaving the gap between academic study and real-world applications. To mitigate this gap, we present BIRD, a BIg bench for laRge-scale Database grounded in text-to-SQL tasks, containing 12,751 text-to-SQL pairs and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty and noisy database values, external knowledge grounding between NL questions and database values, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. GPT-4, only achieve 54.89\% in execution accuracy, which is still far from the human result of 92.96\%, proving that challenges still stand. We also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1835},
numpages = {28},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{10.1145/3626246.3653375,
author = {Zhang, Chao and Mao, Yuren and Fan, Yijiang and Mi, Yu and Gao, Yunjun and Chen, Lu and Lou, Dongfang and Lin, Jinshu},
title = {FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3653375},
doi = {10.1145/3626246.3653375},
abstract = {Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because financial professionals may not be well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, parameter-efficient fine-tuning and output calibration. Experiments on BULL demonstrate that FinSQL achieves state-of-the-art performance at low cost, and it brings up to 36.64\% improvement in few-shot cross-database scenarios.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {93–105},
numpages = {13},
keywords = {financial analysis, large language models (llms), text-to-sql},
location = {Santiago AA, Chile},
series = {SIGMOD/PODS '24}
}


@article{cattell2011scalable,
  title={Scalable SQL and NoSQL data stores},
  author={Cattell, Rick},
  journal={Acm Sigmod Record},
  volume={39},
  number={4},
  pages={12--27},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@article{moniruzzaman2013nosql,
  title={Nosql database: New era of databases for big data analytics-classification, characteristics and comparison},
  author={Moniruzzaman, ABM and Hossain, Syed Akhter},
  journal={arXiv preprint arXiv:1307.0191},
  year={2013}
}

@article{diogo2019consistency,
  title={Consistency models of NoSQL databases},
  author={Diogo, Miguel and Cabral, Bruno and Bernardino, Jorge},
  journal={Future Internet},
  volume={11},
  number={2},
  pages={43},
  year={2019},
  publisher={MDPI}
}

@phdthesis{zeng2015resource,
  title={Resource sharing for multi-tenant nosql data store in cloud},
  author={Zeng, Jiaan},
  year={2015},
  school={Indiana University}
}

@inproceedings{agrawal2011database,
  title={Database scalability, elasticity, and autonomy in the cloud},
  author={Agrawal, Divyakant and El Abbadi, Amr and Das, Sudipto and Elmore, Aaron J},
  booktitle={International conference on database systems for advanced applications},
  pages={2--15},
  year={2011},
  organization={Springer}
}

@inproceedings{min-etal-2019-pilot,
    title = "A Pilot Study for {C}hinese {SQL} Semantic Parsing",
    author = "Min, Qingkai  and
      Shi, Yuefeng  and
      Zhang, Yue",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1377/",
    doi = "10.18653/v1/D19-1377",
    pages = "3652--3658",
    abstract = "The task of semantic parsing is highly useful for dialogue and question answering systems. Many datasets have been proposed to map natural language text into SQL, among which the recent Spider dataset provides cross-domain samples with multiple tables and complex queries. We build a Spider dataset for Chinese, which is currently a low-resource language in this task area. Interesting research questions arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and word-based encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL."
}

@inproceedings{tuan-nguyen-etal-2020-pilot,
    title = "A Pilot Study of Text-to-{SQL} Semantic Parsing for {V}ietnamese",
    author = "Tuan Nguyen, Anh  and
      Dao, Mai Hoang  and
      Nguyen, Dat Quoc",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.364/",
    doi = "10.18653/v1/2020.findings-emnlp.364",
    pages = "4079--4085",
    abstract = "Semantic parsing is an important NLP task. However, Vietnamese is a low-resource language in this research area. In this paper, we present the first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese. We extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two baselines with key configurations and find that: automatic Vietnamese word segmentation improves the parsing results of both baselines; the normalized pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema linking; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results; and the monolingual language model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher performances than the recent best multilingual language model XLM-R (Conneau et al., 2020)."
}

@inproceedings{luo2021synthesizing,
  title={Synthesizing natural language to visualization (NL2VIS) benchmarks from NL2SQL benchmarks},
  author={Luo, Yuyu and Tang, Nan and Li, Guoliang and Chai, Chengliang and Li, Wenbo and Qin, Xuedi},
  booktitle={Proceedings of the 2021 International Conference on Management of Data},
  pages={1235--1247},
  year={2021}
}
@article{staniek-etal-2024-text,
    title = "Text-to-{O}verpass{QL}: A Natural Language Interface for Complex Geodata Querying of {O}pen{S}treet{M}ap",
    author = {Staniek, Michael  and
      Schumann, Raphael  and
      Z{\"u}fle, Maike  and
      Riezler, Stefan},
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.31/",
    doi = "10.1162/tacl_a_00654",
    pages = "562--575",
    abstract = "We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL,1 a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models and adapting large language models with in-context examples. The detailed evaluation reveals strengths and weaknesses of the considered learning strategies, laying the foundations for further research into the Text-to-OverpassQL task."
}

@INPROCEEDINGS{6106531,
  author={Jing Han and Haihong E and Guan Le and Jian Du},
  booktitle={2011 6th International Conference on Pervasive Computing and Applications}, 
  title={Survey on NoSQL database}, 
  year={2011},
  volume={},
  number={},
  pages={363-366},
  keywords={Databases;Facebook;Computational modeling;Blogs;Reliability;NoSQL;key-value;column-oriented;document;database;Big Data},
  doi={10.1109/ICPCA.2011.6106531}}

@inproceedings{10.1145/3448016.3457261,
author = {Luo, Yuyu and Tang, Nan and Li, Guoliang and Chai, Chengliang and Li, Wenbo and Qin, Xuedi},
title = {Synthesizing Natural Language to Visualization (NL2VIS) Benchmarks from NL2SQL Benchmarks},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457261},
doi = {10.1145/3448016.3457261},
abstract = {Natural language (NL) is a promising interaction paradigm for data visualization (VIS). However, there are not any NL to VIS (NL2VIS) benchmarks available. Our goal is to provide the first NL2VIS benchmark to enable and push the field of NL2VIS, especially with deep learning technologies. In this paper, we propose a NL2VIS synthesizer (NL2SQL-to-NL2VIS) that synthesizes NL2VIS benchmarks by piggybacking NL2SQL benchmarks. The intuition is based on the semantic connection between SQL queries and VIS queries: SQL queries specify what data is needed and VIS queries additionally need to specify how to visualize. However, different from SQL that has well-defined syntax, VIS languages (e.g., Vega-Lite, VizQL, ggplot2) are syntactically very different. To provide NL2VIS benchmarks that can support many VIS languages, we use a unified intermediate representation, abstract syntax trees (ASTs), for both SQL and VIS queries. We can synthesize multiple VIS trees through adding/deleting nodes to/from an SQL tree. Each VIS tree can then be converted to (any) VIS language. The NL for VIS will be modified based on the NL for SQL to reflect corresponding tree edits. We produce the first NL2VIS benchmark (nvBench), by applying NL2SQL-to-NL2VIS on a popular NL2SQL benchmark Spider, which covers 105 domains, supports seven common types of visualizations, and contains 25,750 (NL, VIS) pairs. Our method reduces the man-hour to 5.7\% of developing a NL2VIS benchmark from scratch (or building a NL2VIS benchmark from scratch takes 17.5\texttimes{} man-hours of our method). Extensive human validation, through 23 experts and 312 crowd workers, demonstrates the high-quality of nvBench. In order to verify that nvBench can enable learning-based approaches, we develop a SEQ2VIS model. Our experimental results show that SEQ2VIS works well and significantly outperforms the state-of-the-art methods of the NL2VIS task.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1235–1247},
numpages = {13},
keywords = {visualization, natural language to visualization, natural language interface, benchmark},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{DBLP:journals/corr/BahdanauCB14,
  author       = {Dzmitry Bahdanau and
                  Kyunghyun Cho and
                  Yoshua Bengio},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1409.0473},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2020_6b493230,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{sahoo2024systematic,
  title={A systematic survey of prompt engineering in large language models: Techniques and applications},
  author={Sahoo, Pranab and Singh, Ayush Kumar and Saha, Sriparna and Jain, Vinija and Mondal, Samrat and Chadha, Aman},
  journal={arXiv preprint arXiv:2402.07927},
  year={2024}
}