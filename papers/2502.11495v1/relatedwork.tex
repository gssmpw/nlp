\section{Related Work}
\label{sec:rel}


\citet{etxaniz-etal-2024-multilingual} showed that translating low-resource language inputs and examples into English improves LLM performance compared to direct inference in the original language. This approach leverages the English-centric training of most LLMs, but may not fully capture linguistic, cultural, or societal norms. Additionally, using translated examples for ICL risks information loss or distortion, as LLMs struggle with accurately conveying cultural or societal nuances~\cite{yao2023benchmarking,tenzer2024ai,intrator-etal-2024-breaking}.



\citet{winata-etal-2021-language} discovered that providing English examples for ICL improves LLM inference for both English and non-English tasks, though English was heuristically chosen. \citet{winata-etal-2022-cross} showed that randomly sampling from a multilingual dataset outperforms selecting examples based on geographical or linguistic proximity. However, the role of semantic alignment and language-specific capacity in example selection remains unclear in the original work.


\citet{nie-etal-2023-cross} introduced a method that uses multilingual sentence embeddings~\cite{conneau-etal-2020-unsupervised} to select examples in high-resource language similar to the input text in low-resource language.
The multilingual sentence embeddings do not explicitly distinguish between semantic and linguistic similarity, making it impossible to adjust their optimal balance for ICL examples.
Moreover, this study focuses on only masked language models such as mBERT~\cite{devlin-etal-2019-bert} and XLM~\cite{conneau-etal-2020-unsupervised} rather than LLMs.





To leverage unlabeled datasets in low-resource languages, \citet{nguyen-etal-2024-democratizing} address the data scarcity in low-resource languages  using instances from diverse high-resource languages as ICL examples to create synthetic data from unlabeled datasets in low-resource languages, which are then used as ICL examples in a low-resource setting.
However, this method does not consider the similarity between the input and example texts.


% A MICL method specialized for binary classification tasks has also been proposed.
The following studies have proposed MICL methods specialized for binary classification tasks.
\citet{tanwar-etal-2023-multilingual} proposed a method that uses multilingual sentence embeddings~\cite{reimers-gurevych-2020-making} to retrieve similar texts in another language as examples for ICL in a cross-lingual setting.
This method explicitly presents cross-lingual label correspondences (e.g., \textit{In French, ``bad'' means ``mal''}).
\citet{cahyawijaya-etal-2024-llms} introduced query alignment for ICL, selecting examples from parallel data with source texts that match the input language and target texts in high-resource languages.
This method used multilingual sentence embeddings~\cite{reimers-gurevych-2019-sentence,reimers-gurevych-2020-making} to measure the similarity between the input text and the source texts in the parallel data, selecting semantically similar texts as examples.
The labels from the high-resource language are used directly, avoiding translation errors.
Unlike these existing studies, which focus on binary classification tasks, our study applies ICL methods to more general generative tasks.

\citet{qin-etal-2023-cross} introduced a method that processes inputs in languages other than English by using the prompt \textit{Letâ€™s think in English step by step!} to enable step-by-step reasoning in English.
This method consistently improves the performance in languages other than English.
\citet{shi2023language} also demonstrates that step-by-step reasoning enhances the multilingual capabilities of MLLMs.
Unlike our research, which focuses on multilingual knowledge transfer through examples in ICL, this study emphasizes multilingual knowledge transfer within the reasoning process.