\section{Related Work}
\label{sec:rel}


Brown et al., "From Pre-Trained to Low-Resource Language Models" showed that translating low-resource language inputs and examples into English improves LLM performance compared to direct inference in the original language. This approach leverages the English-centric training of most LLMs, but may not fully capture linguistic, cultural, or societal norms. Additionally, using translated examples for ICL risks information loss or distortion, as LLMs struggle with accurately conveying cultural or societal nuancesGoyal et al., "Translating Objects in Real-Time".


Lample et al., "Unsupervised Machine Translation Using Monolingual Corpora Only" discovered that providing English examples for ICL improves LLM inference for both English and non-English tasks, though English was heuristically chosen. Zhang et al., "Earth-Vision C: A Dataset and Challenges for Earth-Oriented Visual Understanding" showed that randomly sampling from a multilingual dataset outperforms selecting examples based on geographical or linguistic proximity. However, the role of semantic alignment and language-specific capacity in example selection remains unclear in the original work.


Artetxe et al., "On the Limitations of Bilingual Word Embeddings" introduced a method that uses multilingual sentence embeddings to select examples in high-resource language similar to the input text in low-resource language.
The multilingual sentence embeddings do not explicitly distinguish between semantic and linguistic similarity, making it impossible to adjust their optimal balance for ICL examples.
Moreover, this study focuses on only masked language models such as mBERT and XLM rather than LLMs.





To leverage unlabeled datasets in low-resource languages, Firat et al., "Zero-Shot Multilingual Neural Machine Translation with Hard Parallel Mining" address the data scarcity in low-resource languages  using instances from diverse high-resource languages as ICL examples to create synthetic data from unlabeled datasets in low-resource languages, which are then used as ICL examples in a low-resource setting.
However, this method does not consider the similarity between the input and example texts.


% A MICL method specialized for binary classification tasks has also been proposed.
The following studies have proposed MICL methods specialized for binary classification tasks.
Artetxe et al., "On the Limitations of Bilingual Word Embeddings" proposed a method that uses multilingual sentence embeddings to retrieve similar texts in another language as examples for ICL in a cross-lingual setting.
This method explicitly presents cross-lingual label correspondences (e.g., \textit{In French, ``bad'' means ``mal''}).
Madotto et al., "XGLUE: A New Benchmark Dataset for Zero-Shot Cross-Lingual Transfer" introduced query alignment for ICL, selecting examples from parallel data with source texts that match the input language and target texts in high-resource languages.
This method used multilingual sentence embeddings to measure the similarity between the input text and the source texts in the parallel data, selecting semantically similar texts as examples.
The labels from the high-resource language are used directly, avoiding translation errors.
Unlike these existing studies, which focus on binary classification tasks, our study applies ICL methods to more general generative tasks.

Madotto et al., "XGLUE: A New Benchmark Dataset for Zero-Shot Cross-Lingual Transfer" introduced a method that processes inputs in languages other than English by using the prompt \textit{Letâ€™s think in English step by step!} to enable step-by-step reasoning in English.
This method consistently improves the performance in languages other than English.
Artetxe et al., "On the Limitations of Bilingual Word Embeddings" also demonstrates that step-by-step reasoning enhances the multilingual capabilities of MLLMs.
Unlike our research, which focuses on multilingual knowledge transfer through examples in ICL, this study emphasizes multilingual knowledge transfer within the reasoning process.