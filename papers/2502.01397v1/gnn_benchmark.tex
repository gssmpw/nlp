%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
 % \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{multirow}
\usepackage{algorithmic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Can message-passing GNN approximate triangular factorizations of sparse matrices?}

\begin{document}

\twocolumn[
\icmltitle{Can message-passing GNN approximate triangular factorizations of sparse matrices?}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Vladislav Trifonov}{skol,sber}
\icmlauthor{Ekaterina Muravleva}{sber,skol}
\icmlauthor{Ivan Oseledets}{airi,skol}

\end{icmlauthorlist}

\icmlaffiliation{skol}{Skolkovo Institute of Science and Technology, Moscow, Russia}
\icmlaffiliation{airi}{Artificial Intelligence Research Institute (AIRI), Moscow, Russia}
\icmlaffiliation{sber}{Sberbank of Russia, AI4S Center, Moscow, Russian Federation}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}
% \icmlaffiliation{uni}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Ivan Oseledets}{oseledets@airi.net}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

We study fundamental limitations of Graph Neural Networks (GNNs) for learning sparse matrix preconditioners. 
While recent works have shown promising results using GNNs to predict incomplete factorizations, 
we demonstrate that the local nature of message passing creates inherent barriers for capturing non-local 
dependencies required for optimal preconditioning. We introduce a new benchmark dataset of matrices 
where good sparse preconditioners exist but require non-local computations, constructed using 
both synthetic examples and real-world matrices. Our experimental results show that current GNN
 architectures struggle to approximate these preconditioners, suggesting the need for
  new architectural approaches beyond traditional message passing networks. We provide theoretical analysis and 
  empirical evidence to explain these limitations, with implications for the broader use of GNNs in numerical linear algebra.

\end{abstract}
\section{Introduction} \label{Introduction}
Preconditioning sparse symmetric positive definite matrices is a fundamental problem in numerical linear algebra \cite{benzi2002preconditioning}. 
The goal is to find a sparse lower triangular matrix L such that $L^{-\top}AL^{-1}$ is well-conditioned, 
which allows faster convergence of iterative methods for solving linear systems. Recently, 
there has been significant interest in using Graph Neural Networks (GNNs) to predict such sparse preconditioners 
\cite{chen2024graph,trifonov2024learning,hausner2023neural}. The key idea is to represent the sparse matrix $A$ 
as a graph, where nodes correspond to variables and edges correspond to non-zero entries, 
and use GNN architectures to predict the entries of the preconditioner $L$, minimizing the certain functional.

While this GNN-based approach has shown promise in some cases, we demonstrate fundamental limitations that arising 
from the inherently local nature of message-passing neural networks. Specifically, we show 
that there exist  classes of matrices, starting from simple ones such as tridiagonal matrices arising 
from discretization of PDEs, 
where optimal sparse preconditioners exist but exhibit non-local dependencies - 
changing a single entry in A can significantly affect all entries in $L$. 
This means, that message passing GNNs, having limited receptive field, can not represent such non-local mappings.
To address these limitations, we introduce a new benchmark dataset of matrices where optimal sparse preconditioners 
are known to exist but require non-local computations. We construct this dataset using both 
synthetic examples and real-world matrices from the SuiteSparse collection. For synthetic benchmarks, 
we carefully design tridiagonal matrices where the Cholesky factors depend non-locally on the matrix elements 
by leveraging properties of rank-1 semiseparable matrices. For real-world problems, we explicitly 
compute so-called K-optimal preconditioners based on the inverse matrix with sparsity patterns matching the lower-triangular part 
of the original matrices.

Our experimental results demonstrate that current GNN architectures, including variants like 
Graph Attention Networks and Graph Transformers, struggle to approximate these  preconditioners. 
This suggests fundamental limitations in the ability of message-passing neural networks to capture the non-local 
dependencies required for optimal preconditioning. We provide both theoretical analysis and empirical evidence 
showing why new architectural approaches beyond traditional GNNs are needed for this 
important problem in scientific computing.

\section{Problem formulation}
Let $A$ be a sparse symmetric positive definite matrix. The goal is to find a sparse lower triangular matrix $L$ such that $L L^{\top}$ approximates $A$ 
well, i.e. the condition number of $L^{-\top} A L^{-1}$ is small. This is known as incomplete Cholesky factorization.

Recent works propose using Graph Neural Networks to learn such factorizations. 
The key idea is to represent the sparse matrix $A$ as a graph, where nodes correspond to variables and edges correspond to non-zero entries. 
A GNN then processes this graph to predict the non-zero entries of $L$.

Specifically, each node $i$ has features derived from the corresponding diagonal entry $A_{ii}$, 
while each edge $(i,j)$ has features based on the off-diagonal entry $A_{ij}$. 
Multiple rounds of message passing aggregate information from neighboring nodes and edges. The final node/edge 
embeddings are used to predict the entries of $L$ that preserve the sparsity pattern of $A$.
This architecture is local, which means if we modify a single entry of $A$, the change will propagate only to the 
neighboring nodes and edges. The size of this neighborhood is limited by the receptive field of the GNN, which is proportional to the depth of the network.
to the number of message passing layers. Each layer, however, adds additional parameters to the model, 
makeing it more difficult to train.



\subsection{Limitations of GNN-based Preconditioners}
Conside the mapping $f: A \rightarrow L$, where $A$ is a given symmetric positive definite matrix, and $L$ is a sparse lower triangular matrix with
a given sparsity pattern. In this section we will provide a an example of sparse matrices $A$, when: 
\begin{itemize}
\item $A$ is a sparse matrix and 
there exists an ideal factorization $A = L L^{\top}$, where $L$ is a sparse matrix.
\item The mapping of $A$ to $L$ is not local: a change in one entry of $A$ can significantly affect all entries of $L$.
The message-passing GNN are inherently local, and therefore cannot learn such mappings directly. 
\end{itemize}
The simplest class of such matrices are {\bf positive definite tridiagonal matrices}. Such matrices appear 
from the standard discretization of 
one-dimensional PDEs.
It is well-known that for such matrices
the Cholesky factorization is given as 
\begin{equation}\label{gnn:tridiag}
   A = LL^{\top},
\end{equation}
where $L$ is biadiagonal matrix,a
and that is what we are looking for: the ideal sparse factorization of a sparse matrix. Our goal is to show tha the mapping \eqref{gnn:tridiag} is not local:
i.e. changing one entry of $A$ will change other entries of $L$. Lets consider first the case of discretization of the Poisson equation 
on a unit interval with Dirichlet boundary conditions. The matrix $A$ is given by the second order finite difference approximation,
\begin{equation}
A = \begin{pmatrix}
2 & -1 & 0 & \cdots & 0 \\
-1 & 2 & -1 & \cdots & 0 \\
0 & -1 & 2 & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & -1 \\
0 & 0 & \cdots & -1 & 2
\end{pmatrix}.
\end{equation}
The Cholesky factor $L$ is bidiagonal in this case. 
If we change a single entry of $A$ in position $(1, 1)$, how the elements of $L$ change? 
The change in the diagonal is shown on Figure \ref{fig:tridiag}, and the we can see the decay. 
This decay is algebraic and is aligned with the properties of the Green functions of the PDEs. However,
we can construct more pathological examples, where the dependence is not local: a single change 
in $A$ will change almost all elements of $L$.
\begin{figure*}[ht]
    \begin{center}
        \includegraphics[width=\textwidth]{cholesky_diff.png}
        \caption{Change in the diagonal elements of the Cholesky factor $L$ when perturbing a single entry $A_{11}$ 
        of the tridiagonal matrix. Left: 1D Laplacian matrix, right: our counterexample.}
        \label{fig:tridiag}
    \end{center}
\end{figure*}
\begin{theorem}
    Let $A$ be a tridiagonal symmetric positive definite $n \times n$ matrix. Then it can be factorized as
    \begin{equation*}
        A = LL^{\top},
    \end{equation*}
    where $L$ is a bidiagonal lower triangular matrix, and then mapping $A \rightarrow L$ is not local, which means 
    that there exist matrix $A$ and $A'$ such that $A-A'$ has only one non-zero element, where as $L - L'$ have all elements greater than zero.
\end{theorem}


\begin{proof}
    Consider the matrix $A$ given by $A=LL^{\top}$ where $L$ is a bidiagonal matrix with $L_{ii} = \frac{1}{i}, i = 1, \ldots, n$ and
    $L_{i, i-1} = 1, i = 2$. Then $A$ is a symmetric positive definite tridiagonal matrix with elements $A_{11} = 1, A_{i, i} = 1 + \frac{1}{i^2}, 
    A_{i+1, i} = A_{i, i+1} = \frac{1}{i}, i = 1, \ldots, n-1$.
    Now, consider the matrix $A' = A + e_1 e_1^{\top}$, where $e_1$ is the first column of the identity matrix. Let
    $A' = L' L'^{\top}$ be its Cholesky factorization. The matrix $L'$ is bidiagonal. The element $L'_{11}$ is equal to $\sqrt{2}$,
    and for each $i=2, \ldots, n$ we have the well-known formulas 
    \begin{equation*}
      L'_{i, i-1} = \frac{L_{i, i-1}}{L'_{i-1, i-1}} = \frac{\frac{1}{i-1}}{L'_{i-1, i-1}},
    \end{equation*} 
    and $L'_{i, i} = \sqrt{A_{i, i} - \left( L_{i, i-1}\right)^2 }.$  Let $d_i = (L'_{i, i})^2$, then
    $d_1 = 2, d_i = 1 + \frac{1}{i^2} - \frac{1}{d_{i-1} (i-1)^2}$. From this recurrence relation it is easy to see that $d_i$
    converges to $1$ as $i \to \infty$.
\end{proof}
The difference between diagonal elements of $L$ and $L'$ is shown on Figure \ref{fig:tridiag} on the right.

\section{Constructive approach}
The class of tridiagonal matrices will serve as the basis for our synthetic benchmarks for learning triangular preconditioners.
What approaches can we take for other, more general sparse positive definite matrices?
In this subsection, we present a constructive approach for building high-quality preconditioners that 
cannot be represented by GNNs (as demonstrated in our numerical experiments).

For this task, we draw attention to the concept of K-condition number, 
introduced by Kaporin \cite{kaporin1994new}. By minimizing this condition number, we can 
constructively build sparse preconditioners of the form $LL^{\top}$ for many matrices, where
the sparsity pattern of $L$ matches the sparsity pattern of the lower triangular part of $A$. 
The K-condition number of a matrix $A=A^*>0$ is defined as:
\begin{equation}\label{gnn:kcond}
	K(A) = \frac{\frac{1}{n}\mathrm{Tr}(A)}{\left(\det(A)\right)^{1/n}}.
\end{equation}
The interpretation of \eqref{gnn:kcond} is that it represents the arithmetic mean of the eigenvalues divided by their geometric mean. For matrices with positive eigenvalues, it is always
greater than 1, equaling 1 only when the matrix is a multiple of the identity matrix.
Given a preconditioner $X$, we can assess its quality using $K(XA)$. This metric can be used 
to construct \emph{incomplete factorized inverse preconditioners} $A^{-1} \approx LL^{\top}$ where $L$ is sparse. However, our focus is on
constructing \emph{incomplete factorized preconditioners} $A \approx LL^{\top}$ with sparse $L$. Therefore, 
we propose minimizing the functional:
\begin{equation}\label{gnn:kopt_inv}
   K(L^{\top} A^{-1} L) \rightarrow \min_L,
\end{equation}
where $L$ is a sparse lower triangular matrix with predetermined sparsity pattern. The strategy
 of utilizing the inverse matrix in preconditioner optimization is very promising  
 and as been explored in other works \cite{li2023learning,trifonov2024learning} through the functional:
$$
   \Vert LL^{\top} A^{-1} - I \Vert^2_F \rightarrow \min.
$$
More naive functionals like $\Vert A - LL^{\top} \Vert$ tend to prioritize approximating components 
corresponding to larger eigenvalues. For matrices arising from partial differential equations (PDEs), 
high frequencies are often "non-physical", making the approximation of lower frequencies more crucial 
for preconditioner quality. The distinctive advantage of functional \eqref{gnn:kopt_inv} is 
that the minimization problem can be solved \emph{explicitly} using linear algebra techniques. 
This enables us to construct pairs $(L_i, A_i)$ for small and medium-sized problems where $L_i L^{\top}_i$ 
serves as an effective preconditioner. These pairs provide valuable benchmarks for 
evaluating preconditioner learning algorithms and comparing their properties against matrices 
that minimize \eqref{gnn:kopt_inv}.

\section{K-optimal preconditioner based on inverse matrix for sparse matrices}

In this section, we analyze the preconditioner quality functional:
\begin{equation}\label{prec:koptinv}
	K(L^{\top} A^{-1} L) \rightarrow \min_L,
\end{equation}
where $L$ is a sparse lower triangular matrix with predetermined sparsity pattern. We will derive an 
explicit solution to this optimization problem.

\subsection{Solution of the optimization problem}

Let us demonstrate how to minimize the K-condition number in the general case, 
then apply the results to obtain explicit formulas for K-optimal preconditioners. Consider the optimization problem:
\begin{equation}\label{prec:kinv}
	K(X^{\top} B X) \rightarrow \min_X,
\end{equation}
where $X$ belongs to some linear subspace of triangular matrices:
\begin{equation*}
	x = \mathrm{vec}(X) = \Psi z,
\end{equation*}
where $\Psi$ is an $n^2 \times m$ matrix, with $m$ being the subspace dimension. For sparse matrices, 
$m$ equals the number of non-zero elements in $X$.

Instead of directly minimizing functional \eqref{prec:kinv}, we minimize its logarithm:
\begin{multline*}
\Phi(X) = \log K(X^{\top} B X) = \\ 
\log \frac{1}{n}\mathrm{Tr}(X^{\top} B X) - \frac{1}{n} \log \det(X)^2 - \frac{1}{n} \log \det(B),
\end{multline*}
The third term is independent of $X$ and can be omitted.
For the first term:
\begin{equation*}
	\mathrm{Tr}(X^{\top} B X) = \langle B X, X \rangle,
\end{equation*}
where $\langle \cdot, \cdot \rangle$ denotes the Frobenius inner product. Therefore:
\begin{equation*}
	\mathrm{Tr}(X^{\top} B X) = (\mathcal{B} x, x), 
\end{equation*}
with $\mathcal{B} = I \otimes B$, leading to:
\begin{equation*}
	\mathrm{Tr}(X^{\top} B X) = (\mathcal{B} x, x) = (\Psi^{\top} \mathcal{B} \Psi z, z) = (Cz, z),
\end{equation*}
where $C = \Psi^{\top} \mathcal{B} \Psi$.
To express the elements of matrix $C$, we use three indices for $\Psi$'s elements, $\Psi_{ii' l}$:
\begin{equation*}
	C_{ll'} = \sum_{i,j=1}^n B_{ij} \sum_{i'} \Psi_{ii' l} \Psi_{j i' l} = \langle B, \Psi_l \Psi^{\top}_{l'} \rangle,
\end{equation*}
where $\Psi_l, l = 1, \ldots, m$ are $n \times n$ matrices obtained from corresponding rows of $\Psi$.
Our task reduces to minimizing with respect to $z$.
Since $B$ is symmetric, $C$ is also symmetric, yielding the gradient:

$$(\nabla \Phi(z))_j = \frac{2 (Cz)_j }{(Cz, z)} - \frac{2}{n} \mathrm{Tr}({X^{-1} \Psi_j}),$$
derived using the formula for the logarithm of matrix determinant derivative.

\paragraph{Special case: $X = L$ is a sparse matrix}

If $X=L$, where $L$ is a sparse lower triangular matrix, then matrix $C$ is a block-diagonal matrix of the form
\begin{equation*}
C = \begin{pmatrix}
C_1 & & & \\
& C_2 & & \\
& & \ddots & \\
& & & C_n
\end{pmatrix},
\end{equation*}
where blocks $C_i$ are given by formulas
\begin{equation*}
(C_i)_{kl} = B_{s^{(i)}_k, s^{(i)}_l},
\end{equation*}
where $s^{(i)}_k$ are indices of non-zero elements in the $i$-th column of matrix $L$, and
matrix $X^{-1} \Psi_j$ has non-zero diagonal elements only for $j$ corresponding to diagonal elements of matrix $L$. 
For these elements
$\mathrm{Tr}(X^{-1} \Psi_j) = \frac{1}{x_{ii}}, i = 1, \ldots, n$.

The problem reduces to $n$ independent optimization problems on values of non-zero elements in 
the $i$-th column of matrix $L$, $i=1, \ldots, n$.
Let us consider each subproblem separately. The optimality condition for the $i$-th subproblem has the form

\begin{equation*}
	 C_i z_i = \gamma_i e_1,
\end{equation*}
where $\gamma_i = \gamma_0 \frac{(Cz, z)}{x_{ii}}$ is a number, $\gamma_0$ is a constant that does not depend on $z$, 
$e_1$ is the first column of the identity matrix of corresponding size.
Hence 
\begin{equation*}
	z_i = \gamma_i v_i, \quad v_i = C_i^{-1} e_1,
\end{equation*}
and using the fact that $K$ does not depend on multiplication by a number we get an equation 
for the first component of vector $z$ (which is
the diagonal element of matrix $L$)
\begin{equation*}
	(z_i)_1 = \frac{(v_i)_1}{(z_i)_1},
\end{equation*}
from which
\begin{equation*}
	(z_i)_1 = \sqrt{(v_i)_1}.
\end{equation*}
The vector $z_i$ contains the non-zero elements of $i$-th column of $L$.
Therefore, the algorithm for finding the sparse lower triangular matrix $L$ is summarized in Algorithm \ref{alg:koptinv}.
\begin{algorithm}[H]
\caption{Construction of Inv-K preconditioner}
\label{alg:koptinv}
\begin{algorithmic}
\REQUIRE Symmetric positive definite matrix $A$, sparsity pattern for $L$
\ENSURE Lower triangular matrix $L$
\STATE Compute $B = A^{-1}$
\FOR{$i = 1$ to $n$}
    \STATE Find indices $s_i$ of non-zero elements in column $i$ of $L$
    \STATE Extract submatrix $B_i$ using rows and columns from $s_i$
    \STATE Compute $v_i = B_i^{-1}e_1$ \COMMENT{$e_1$ is first unit vector}
    \STATE Set $L_{s_i,i} = ((v_i)_1)^{-1/2} \cdot v_i$ \COMMENT{Store as $i$-th column of $L$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
Note that this algorithm requires computing the full inverse matrix $B = A^{-1}$, making it
 impractical for large-scale problems. However, it is well-suited for generating benchmark datasets 
 to evaluate machine learning models. We refer to preconditioners constructed using this approach 
 as \emph{Inv-K preconditioners}.

%\begin{equation}\label{prec:kopt}
%K(PA) = \frac{\frac{1}{n}\mathrm{tr}(PA)}{\left(\det(PA)\right)^{1/n}}
%\end{equation}

%1) Introduce the K-optimality criterion
%2) Present algorithm for constructing K-optimal sparse preconditioners
%3) Analyze theoretical properties and computational complexity

\section{Benchmark construction}
\subsection{Synthetic benchmarks}
\label{sec:synth_bench}
Our first benchmark focuses on tridiagonal matrices, which present an interesting challenge for testing GNN's capabilities. 
Constructing tridiagonal matrices where the Cholesky factors exhibit strong non-local dependencies requires careful consideration. 
Through empirical investigation, we found that simply fixing the diagonal elements of $L$ to 1 and sampling the off-diagonal elements 
from a normal distribution does not produce the desired non-local behavior - the resulting matrices $A = LL^{\top}$ tend to show 
primarily local dependencies. The key insight is that non-locality emerges when the inverse matrix $L^{-1}$ is dense.

We leverage the well-known fact that the inverse of a bidiagonal matrix $L$ has a special structure called rank-1 semiseparable, 
where elements are given by the formula $L^{-1}_{i,j} = u_i v_j$ for $i \leq j$, representing part of a rank-1 matrix. 
This relationship is bidirectional - given vectors $u$ and $v$, we can construct $L^{-1}$ with this structure and then compute 
$L$ as its inverse. Our benchmark generation process exploits this property by randomly sampling appropriate vectors $u$ and $v$ 
to create matrices with guaranteed non-local dependencies.

The primary goal of this synthetic benchmark is to evaluate whether GNNs can accurately recover the matrix $L$ in these cases. 
While our theoretical results suggest this should be impossible due to the inherent locality of message passing, 
it remains an open question whether GNNs with sufficiently large receptive fields could achieve reasonable approximations. 
Poor performance on this benchmark would raise serious concerns about the fundamental suitability of current GNN architectures 
for matrix factorization tasks.

\subsection{Matrices from the SuiteSparse collection}
To complement our synthetic examples with real-world test cases, we curated a comprehensive benchmark from the SuiteSparse 
matrix collection. We selected symmetric positive definite matrices for which dense inverse computation was feasible, 
resulting in a diverse set of 150 matrices varying in both size and sparsity patterns. For each matrix, we explicitly 
solved the optimization problem \eqref{prec:kinv} to obtain sparse lower-triangular preconditioners.

Following common practice in incomplete factorization methods, we restricted the sparsity pattern of our preconditioners 
to match the lower-triangular part of the original matrix $A$, similar to IC(0) preconditioners. Our experimental results 
showed that the inverse K-optimal preconditioners generally outperformed traditional IC(0) preconditioners - in many cases, 
IC(0) either failed to exist or required excessive iterations ($>10000$) for convergence. However, we observed that for a small 
subset of matrices, IC(0) achieved better convergence rates.

The final benchmark consists of  $(A_i, L_i)$ pairs, where each $A_i$ comes from SuiteSparse and $L_i$ 
represents either the IC(0) or K-optimal preconditioner, whichever demonstrated superior performance. Matrices for which neither 
preconditioner achieved satisfactory convergence were excluded. This benchmark serves two key purposes: it provides a robust 
baseline for sparse preconditioners with fixed sparsity patterns, and it creates a challenging yet practically relevant 
test set for evaluating GNN-based approaches. The relative performance distribution between Inv-K and IC(0) preconditioners 
is visualized in Figure \ref{fig:compare_precs}, highlighting the general superiority of Inv-K preconditioners, while also 
showing cases where IC(0) remains competitive or where one or both methods fail to converge.

\begin{figure*}[!h]
\normalsize
\centering
    \begin{center}
        \includegraphics[width=0.45\textwidth]{compare_precs.jpg}
    \end{center}
    \caption{The performance of inv-K preconditioner and IC(0) preconditioner. 4 cases: Inv-K only: IC(0) failed.
    Inv-K less iterations, both failed and IC(0) was better.}
    \label{fig:compare_precs}
    \vspace*{4pt}
\end{figure*}



\begin{figure*}[!h]
\normalsize
\centering
    \begin{center}
        \includegraphics[width=1.\textwidth]{figures/outer_alpha1_test.png}
        % \moveright  10pt \hbox{\input{figures/.png}}
    \end{center}
    \caption{Synthetic example which constructed as inverse of outer product.}
    \label{fig:synth}
    \vspace*{4pt}
\end{figure*}

\section{Experiments}
\subsection{Message Passing Layers}

The problem considered in this paper can be reformulated as a regression with loss that penalizes edges discrepancy 
with the target. Most of the classical GNNs either do not take into account edges (e.g., 
GraphSAGE~\cite{hamilton2017inductive}) or takes them into account as scalar weighted adjacency matrix 
(e.g., Graph attention network~\cite{velivckovic2017graph}). 
To allow edge updates during message-passing we use a Graph Network~\cite{battaglia2018relational} block
 as a message-passing layer.

To validate GNNs on the proposed benchmarks we utilize very popular Encoder-Processor-Decoder configuration. 
Encoder consists of two separate MLPs for nodes and edges. 
Processor consists of multiple blocks of Graph Networks.
Graph Network first updates edge representations with Edge Model, 
after which nodes are update by Node Model with message-passing mechanism.
 In our work we do not consider models that achieve larger receptive field by graph coarsening 
 or updates of the graph-level information, hence the Global Model in Graph Network is omitted. 
 Decoder is a single MLP that decode edges hidden representations into single value per edge.

As a neural network baseline that does not perform information propagation between nodes 
we use a simple two-layer MLP as Node Model in Graph Network (MLPNodeModel). 
Following message-passing GNNs are used as Node Model in Graph Network: (i) graph attention network
 v2 (GAT)~\cite{brody2021attentive}, (ii) generalized aggregation network (GEN)~\cite{li2020deepergcn} and (iii) message-passing (MessagePassingMLP)~\cite{gilmer2017neural} with two MLPs $f_{\theta_1}$ and $f_{\theta_2}$:

\begin{equation*}
    h_{i} = f_{\theta_2}\bigg(h_i, \frac{1}{N}\sum_{j\in\mathcal{N}(i)}f_{\theta_1}\big(h_i, e_{ij}\big)\bigg).
\end{equation*}

Finally, we tested two graph transformers as Node Models: (i) graph transformer operator (GraphTransformer) from~\cite{shi2020masked} and (ii) fast graph transformer operator (FastGraphTransformer) from~\cite{wu2024simplifying}.

\begin{figure*}[!h]
\normalsize
\centering
    \begin{center}
        \includegraphics[width=1.\textwidth]{figures/kaporin_test.png}
        % \moveright  10pt \hbox{\input{figures/.png}}
    \end{center}
    \caption{Experiments on the factors of K-optimal preconditioners for Suite sparse subset.}
    \label{fig:k_optimal}
    \vspace*{4pt}
\end{figure*}

\subsection{Graph Neural Network Architecture}

In our experiments we set encoders for nodes and edges to two layer MLPs with $16$ hidden and output features. 
Node Model is single layer from a following list: 
MLPNodeModel, GAT, GEN, MessagePassingMLP, GraphTransformer, FastGraphTransformer. 
Edge Model is a two layer MLP with $16$ hidden features. 
Node Model and Edge Model form Graph Network which is used to combine multiple message-passing layers in Processor. 
Edge decoder is a two layer MLP with $16$ hidden features and single output feature.

The maximum depth of message-passing layers within the Processor block varies across different Node Models and is determined by GPU memory allocation for each Node Model but not greater than 7 blocks.

For training we use negative cosine similarity between target and predicted edges as a loss function, since 
for preconditioner the matrix $L$ is defined up to a scaling factor.
Note that in terms of sparse matrices vectors of edges correspond to vectorized elements of sparse matrix.

We use PyTorch Geometric~\cite{Fey-Lenssen-2019} framework for GNNs training and main layers implementation. 
For FastGraphTransformer we use official implementation from~\cite{wu2024simplifying}. 
We used a single GPU Nvidia A40 48Gb for training. 

\subsection{Learning Triangular Factorization}

We start our experiments with synthetic benchmark generated as described in Section~\ref{sec:synth_bench}.  Modified training pairs $(A_m,L_m)$ are obtained as follows:

\begin{equation}
    A_m = A + e_1e_1^\top,~~L_m=\text{chol}(A)~.
\end{equation}
where $\text{chol}$ is a Cholesky factorization.

A trivial empirical justification of the non-local behaviour of the considered problem is performed with a deep feed-forward network, MLPNodeModel, which has no information about the context (Figure~\ref{fig:synth}). Surprisingly, the classical graph network layers GAT and GEN have a slightly higher final accuracy than MLPNodeModel. We assume that this behaviour is explained by the fact that these architectures are not designed to properly pass edge-level information, which is a primary goal of our work. MessagePassingMLP GNN, on the other hand, makes direct use of edge features, which allows it to produce satisfactory results with number of rounds $>1$.

One can notice a disparity between the performance of the graph transformers. Looking more closely at the architectures, one can observe the same difference as for the models above: GraphTransformer attention uses edge features, if they are available, in multi-head attention. Even global all-pair attention via vertex features does not allow FastGraphTransformer to learn correct triangular factorization.  

Experiments with factors from K-optimal preconditioners (Figure~\ref{fig:k_optimal}) show that none of the models except MessagePassingMLP can go higher in accuracy than the baseline feed-forward network. Nevertheless, MessagePassingMLP performs slightly better than baseline.

While GAT, GEN and FastGraphTransformer do not explicitly use edge features in the layers, the information should propagate through the sender-receiver connection in the edge model.

The model is trained for $300$ epochs in each experiment with an initial learning rate of $10^{-3}$, decreasing by a factor of $0.6$ every $50$ epoch. We also use early stopping with $50$ epoch patience. For the synthetic dataset, we generate $1000$ training and $200$ test samples. The batch size is $16$ and $8$ for synthetic and K-optimal datasets respectively. 

%This dataset serves to highlight limitations of current GNN approaches and motivate development of new architectures.

\section{Related Works}
The idea of using GNN for learning sparse factorized preconditioners has been considered in several works 
\cite{hausner2023neural,trifonov2024learning,li2023learning,li2024generative,chen2024graph,booth2024neural}.
Another line of work covers iterative solvers rather than preconditioners, see, for example,~\cite{luoneural}.
On the other hand, the limitations of message-passing GNN have already been highlighted in the literature for other tasks.

For example, in the foundational work~\cite{xu2018powerful}, where it has been shown
that are provably bounded by the expressive power of the Weisfeiler-Lehman (WL) graph isomorphism test, 
which renders them incapable of distinguishing 
certain graph structures or capturing higher-order dependencies.

\section{Limitations}
We have shown that GNNs are not able to recover the Cholesky factors for tridiagonal matrices, where
perfect sparse preconditioners exist. For the real-world matrices, one can argue that the cosine
similarity between our computed preconditioners and the ones predicted by GNN may not reflect 
the quality of the precondioner -- maybe, the GNN can learn something better, that K-optimal or IC(0) preconditioners
in terms of quality. This is a subject of future work, but we believe that the current benchmarks and 
the quality of the computed preconditioners are quite challenging for SOTA methods even using other functionals.

Unfortunately, we can not scale the benchmarks to larger matrices using K-optimal preconditioners computed 
from the inverse matrices, since the memory consumption is too high. We did not yet find the way to utilize this 
approach efficiently for larger matrices, but we believe that it is possible and leave it for future work as well.

In this work we only considered IC(0)-type preconditioners. They have obvious limitations. A natural extension is
level-of-fill preconditioners, proposed by Saad \cite{saad2003iterative}, where the sparsity pattern of the preconditioner
is inferred from the matrix $A^k$. This would potentially yield much less number of iterations 
at the expense of higher memory consumption.

Finally, we have restricted our attention to the symmetric positive definite matrices. The part 
concerning the tridiagonal matrices remain the same, whereas the K-optimality does not apply to non-symmetric matrices,
so other approaches are needed for the construction of the corresponding benchmarks.

\section{Conclusions and Future Work}
Our work provides a new perspective on the limitations of message-passing GNNs for preconditioning and also shows
that in order to learn factorizations, we need to have non-local information about the matrix, 
not just local transformations. The inspiration for new architectures can be actually taken from the linear algebra as
well, and we plan to explore this direction in future work. Finally, there are many other preconditioning
approaches besides factorization methods, that may be much better suited for GNNs \cite{benzi2002preconditioning}. Still, improvements
of numerical linear algebra algorithms by machine learning methods is a very challenging task.



%Recent years have seen growing interest in using machine learning, particularly Graph Neural Networks (GNNs), 
%for predicting preconditioners. Methods like [citations] have demonstrated promising results on certain problem classes. 
%However, theoretical understanding of when such approaches can succeed remains limited. 
%Our work builds on classical results in preconditioning theory while providing new insights into fundamental limitations of GNN-based approaches.



% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[htbp]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.









% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

%\section*{Accessibility}
%Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
%Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

%\section*{Software and Data}
%
%If a paper is accepted, we strongly encourage the publication of software and data with the
%camera-ready version of the paper whenever appropriate. This can be
%done by including a URL in the camera-ready copy. However, \textbf{do not}
%include URLs that reveal your institution or identity in your
%submission for review. Instead, provide an anonymous URL or upload
%the material as ``Supplementary Material'' into the OpenReview reviewing
%system. Note that reviewers are not required to look at this material
%when writing their review.

% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}

%\textbf{Do not} include acknowledgements in the initial version of
%the paper submitted for blind review.

%If a paper is accepted, the final camera-ready version can (and
%usually should) include acknowledgements.  Such acknowledgements
%should be placed at the end of the section, in an unnumbered section
%that does not count towards the paper page limit. Typically, this will 
%include thanks to reviewers who gave useful comments, to colleagues 
%who contributed to the ideas, and to funding agencies and corporate 
%sponsors that provided financial support.

\section*{Impact Statement}

%Authors are \textbf{required} to include a statement of the potential 
%broader impact of their work, including its ethical aspects and future 
%societal consequences. This statement should be in an unnumbered 
%section at the end of the paper (co-located with Acknowledgements -- 
%the two may appear in either order, but both must be before References), 
%and does not count toward the paper page limit. In many cases, where 
%the ethical impacts and expected societal implications are those that 
%are well established when advancing the field of Machine Learning, 
%substantial discussion is not required, and a simple statement such 
%as the following will suffice:
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.
%The above statement can be used verbatim in such cases, but we 
%encourage authors to think about whether there is content which does 
%warrant further discussion, as this statement will be apparent if the 
%paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{0langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%\appendix
%\onecolumn
%\section{You \emph{can} have an appendix here.}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  

%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{figure}[!htbp]
%    \subfigure
%    {\includegraphics[width=1\hsize]{images/maxvol0.png}\label{maxvol0}}
%    \subfigure
%    {\includegraphics[width=1\hsize]{images/maxvol1.png}\label{maxvol1}}
%    \subfigure
%    {\includegraphics[width=1\hsize]{images/maxvol2.png}\label{maxvol2}}
%    \subfigure
%    {\includegraphics[width=1\hsize]{images/maxvol3.png}\label{maxvol3}}
%    \subfigure
%    {\includegraphics[width=1\hsize]{images/maxvol4.png}\label{maxvol4}}
%    \subfigure
%    {\includegraphics[width=1\hsize]{images/maxvol5.png}\label{maxvol5}}
%    \subfigure
%    {\includegraphics[width=1\hsize]{images/maxvol6.png}\label{maxvol6}}
%    \subfigure
%    {\includegraphics[width=1\hsize]{images/maxvol7.png}\label{maxvol7}}
%    \subfigure
%    {\includegraphics[width=0.5\hsize]{images/maxvol8.png}\label{maxvol8}}
%\end{figure}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
