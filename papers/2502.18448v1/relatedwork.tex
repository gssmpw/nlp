\section{Related Work}
\paragraph{Ambiguity Resolution in NLP} Numerous studies have focused on ambiguity in natural language tasks using strategies like generating multiple answers \citep{min-etal-2020-ambigqa}, asking clarification questions \citep{lee-etal-2023-asking, Zhang2024ModelingFC}, and estimating uncertainty \citep{cole-etal-2023-selectively}. Similar to our work, \citet{sun-etal-2023-answering} use iterative prompting to refine and generate alternative interpretations in the context of question answering, while \citet{kim-etal-2024-aligning} first detect ambiguous questions and then resolve them through clarification requests. 

 \paragraph{Ambiguity in Semantic Parsing} Ambiguity has been studied across semantic parsing tasks, from code generation \citep{li-etal-2023-python, mu2023clarifygpt} to $\lambda$-calculus translation \citep{rasmussen-schuler-2020-corpus},  and logical form prediction \citep{eskin-2024-zero}. In the domain of text-to-SQL parsing, recent work has emphasized the fact that benchmarks often overlook ambiguity by providing single interpretations \citep{Floratou2024, pourreza-rafiei-2023-evaluating}. Existing approaches focus on detecting column ambiguity through counterfactual examples \cite{wang-etal-2023-know}, special-purpose decoding \citep{bhaskar-etal-2023-benchmarking}, and resolving ambiguity through clarification questions \citep{practiq-dataset}. Our work uses explicit disambiguation before parsing and thus extends to different types of ambiguities, question styles, and database formats.

\paragraph{Intermediate Representations in Text-to-SQL} Intermediate representations are commonly used to bridge the gap between natural language and database queries. Several approaches decompose complex questions into a sequence of simpler operations expressed in natural language   \cite{wolfson-etal-2022-weakly,saparina-osokin-2021-sparqling}, modular execution plans \cite{eyal-etal-2023-semantic}, or simplify the parsing task by augmenting questions with SQL keywords that mention necessary computation steps \cite{liu2024keyinstkeywordinstructionimproving, caferoÄŸlu2024esqldirectschemalinking}. Building on this work, we also use intermediate representations to make implicit information explicit but focus on resolving ambiguity.

\paragraph{Learning to Correct LLM Outputs} More recently, various approaches have been proposed to correct systematic biases in LLM outputs. For example, \citet{ji2024alignerefficientalignmentlearning} propose a model-agnostic module that learns correctional residuals between preferred and dispreferred outputs. \citet{welleck2023generating} use a corrector model to iteratively review imperfect generations from a base model. 
Similarly,  critique generators can be developed using reinforcement learning  \cite{wadhwa-etal-2024-learning-refine}  or through fine-grained feedback \cite{wadhwa-etal-2024-learning}. Such correction approaches are most effective when guided by external tools \citep{kamoi-etal-2024-llms}. We follow this paradigm using a specialized infilling model to correct systematic  LLM biases towards certain interpretations and validate our output through SQL execution.