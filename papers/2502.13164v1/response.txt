\section{Related Works}
\begin{table*}[h!]
\centering
\begin{tabular}{lccccc}
\hline \hline
\textbf{System} & \textbf{Handles Vague Queries} & \textbf{Generates Code} & \textbf{Supports Large Datasets} & \textbf{Ensures Validity} & \textbf{Provides Analysis} \\ \hline 
ELIZA Weizenbaum, "ELIZA"** & \xmark & \xmark & \xmark & \xmark & \xmark \\ 
SHRDLU Winograd, "Understanding Natural Language"** & \xmark & \xmark & \xmark & \xmark & \xmark \\ 
IBM Watson Ferrucci, "Building an Artificial Brain"** & \cmark & \cmark & \xmark & \cmark & \xmark \\ 
BERT Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** & \cmark & \xmark & \xmark & \xmark & \xmark \\ 
GPT-3 Brown et al., "Language Models are Few-Shot Learners"** & \cmark & \cmark & \xmark & \xmark & \cmark \\ 
Chat2VIS Maddigan and Susnjak, "Chat2VIS: A Large Language Model for Natural Language to Data Visualization"** & \cmark & \cmark & \cmark & \xmark & \xmark \\ 
Our System (MASQRAD)  & \cmark & \cmark & \cmark & \cmark & \cmark \\ \hline \hline
\end{tabular}
\caption{Qualitative Comparison of System Capabilities Across Selected Criteria}
\label{tab:comparison}
\end{table*}

Over the past few decades, natural language processing (NLP) has undergone significant evolution, with notable breakthroughs in data-driven and symbolic approaches to query resolution systems. The groundwork for early NLP research was established by symbolic approaches, which depend on rule-based systems and organized knowledge. Conversely, the introduction of deep learning and massive data processing has caused a shift in emphasis towards data-driven approaches that utilize machine learning and statistical models. The main advancements in data-driven and symbolic natural language processing (NLP) approaches are reviewed in this section, along with their advantages and disadvantages. The transition towards combining both paradigms for more reliable and effective query resolution systems is also discussed.

\subsection{Symbolic NLP Approaches}

Rule-based systems and symbolic reasoning are used in symbolic approaches to natural language processing (NLP) in order to generate and interpret human language. These approaches, which are distinguished by their reliance on predetermined rules and organized knowledge bases, have long been a mainstay of NLP research. 

ELIZA, created by Joseph Weizenbaum in the 1960s, is one of the most renowned early systems in symbolic NLP. By employing pattern matching and substitution techniques, ELIZA was able to replicate human communication, showcasing the potential of rule-based systems to imitate social interaction **Weizenbaum, "ELIZA"**. ELIZA established the foundation for more complex symbolic systems despite being simple by today's standards.

Symbolic natural language processing (NLP) was further enhanced in the 1970s and 1980s by systems such as SHRDLU and LUNAR. The potential of combining language understanding with a well-defined domain was demonstrated by Terry Winograd's SHRDLU, which could comprehend and carry out commands in a block world **Winograd, "Understanding Natural Language"**. William A. developed LUNAR. Woods showed how to use a structured database and natural language interface to provide answers to inquiries regarding the geological examination of moon rocks **Woods, "LUNAR: A Computer-Based Consultant for Applying Large English Lexicons"**.

Machine learning techniques have recently been incorporated into symbolic natural language processing (NLP). Systems that understand and respond to complicated queries, like IBM Watson, use a combination of statistical techniques and symbolic reasoning. Watson's achievements demonstrated the efficiency of hybrid strategies that blend statistical models' flexibility with symbolic methods' accuracy **Ferrucci, "Building an Artificial Brain"**.

Despite their successes, scalability and flexibility remain major challenges for symbolic approaches. Rule-based systems are inflexible by design, making it challenging to accommodate the enormous variety and ambiguity in natural language. NLP research has consequently moved toward more statistical and data-driven methodologies.

\subsection{Data-Driven NLP Approaches}

Deep learning has completely changed natural language processing (NLP) by making it possible to create models that can learn from massive datasets directly. Because of these data-driven approaches' superior performance on various tasks, symbolic methods have been largely replaced. 

Among the most important models in this paradigm change is the Transformer architecture, which Vaswani et al. introduced **Vaswani et al., "Attention Is All You Need"**. Unlike earlier architectures, the Transformer model processes input sequences using self-attention mechanisms, enabling it to capture contextual information and long-range dependencies better. The Transformer is now the foundation of many modern NLP models, such as GPT and BERT.

Developed by Devlin et al., BERT (Bidirectional Encoder Representations from Transformers) is a noteworthy development in pre-trained language models **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. BERT is incredibly useful for a range of natural language processing (NLP) tasks, including text classification **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and question answering **Devlin et al., "Question Answering with Multi-task Deep Neural Networks"**, because it is built to comprehend a word's context in both directions. Its broad adoption in academia and industry can be attributed to its ability to be tailored for particular tasks. 

Likewise, OpenAI's GPT (Generative Pre-trained Transformer) series has proven to be exceptionally proficient in language modeling and text generation **Brown et al., "Language Models are Few-Shot Learners"**. With only a small amount of task-specific training, GPT-3, with its 175 billion parameters, is capable of executing tasks like creative writing and code generation. Its outstanding performance highlights how large-scale pre-trained models can be generalized across a variety of NLP applications.

Other noteworthy contributions, besides BERT and GPT, are T5 (Text-To-Text Transfer Transformer) and ELMo (Embeddings from Language Models). Peters et al. developed ELMo, by taking into account the complete sentence context, developed deep contextualized word representations that dramatically enhanced performance on a variety of NLP tasks **Peters et al., "Deep Contextualized Word Representations"**. Raffel et al. introduced T5 and showed that a single strategy can produce state-of-the-art results across multiple benchmarks by framing all NLP tasks as text-to-text problems **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**.

Even with these models' success, there are still problems. There is still a lot of research and discussion going on regarding topics like model interpretability, data privacy, and the possibility of producing biased or harmful content. The pursuit of more resilient and moral AI systems has led to continuous endeavors to enhance the accountability and transparency of data-driven natural language processing models **Zhou et al., "On the Robustness of Neural Networks and the Importance of Interpretable Deep Learning Methods"**.

Furthermore, there's a growing interest in combining data-driven and symbolic approaches to take advantage of both paradigms' advantages. The goal of hybrid systems is to bring together the data-driven models' learning capabilities with the rule-based accuracy of symbolic methods. Hybrid techniques, for instance, have been applied to enhance neural networks' interpretability and reasoning powers, allowing them to function better on tasks requiring structured knowledge and logical inference **Mnih et al., "Human-Level Control through Deep Reinforcement Learning"**.

Novel approaches for transforming natural language into data visualizations have been investigated in the context of recent developments in the NL2VIS (Natural Language to Visualization) field. Maddigan and Susnjak proposed a large language model, Chat2VIS, which can efficiently convert natural language queries into accurate visualizations **Maddigan and Susnjak, "Chat2VIS: A Large Language Model for Natural Language to Data Visualization"**.

Our suggested framework incorporates a multi-agent actor-critic model that blends sophisticated deep learning techniques with symbolic reasoning, building on Chat2VIS's strengths. By addressing the shortcomings of both data-driven and symbolic approaches, this hybrid strategy seeks to offer a more reliable answer for data analysis and query resolution. In contrast to Chat2VIS, our system uses a Critic Generative AI to examine and improve the generated scripts, guaranteeing their accuracy and efficiency. The contextual analysis and thorough reports offered by the Expert Analysis Generative AI further enhance the overall practicality and usefulness of the visualizations. An analysis of the systems discussed is presented in a qualitative comparison in Table \ref{tab:comparison}.