\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/intro.png}
  \caption{
    A conceptual framework for understanding transferability. All adversarial prompts capable of eliciting harmful responses constitute the entire feasible region for jailbreaking attacks. However, the search space of gradient-based optimization represents only a specific subset of this region. Furthermore, superfluous constraints in the original objective further narrow this subset from a shared region across models to a model-specific area.
  }
  \label{fig:intro}
\end{figure}

\section{Introduction}

In recent years, Large Language Models (LLMs) have rapidly advanced across a wide range of tasks \cite{achiam2023gpt, anthropic2024claude, bai2023qwen, dubey2024llama, guo2025deepseek}. Consequently, the safety issues associated with these powerful LLMs have garnered increasing attention, including risks such as private data leakage \cite{DBLP:conf/acl/ZhangWH23}, generation of toxic content \cite{DBLP:journals/corr/abs-2304-05335}, and promotion of illegal activities \cite{DBLP:journals/corr/abs-2309-07045}.

Although various defense and safety alignment methods \cite{robey2023smoothllm,dai2023safe,zhang2023defending} have been proposed to mitigate these risks, jailbreaking attacks continue to evolve rapidly. These attacks attempt to bypass model safeguards through malicious inputs, including gradient-based optimization \cite{zou2023universal, andriushchenko2024jailbreaking}, heuristic-based algorithms \cite{shah2023scalable,yu2023gptfuzzer,liu2023autodan}, and rewriting-based approaches \cite{deng2023jailbreaker, mehrotra2023tree}. Among these, gradient-based optimization stands out as an effective white-box approach that directly maximizes the probability of generating malicious content.

A critical challenge associated with gradient-based jailbreaking methods is their transferability, as reliable transferability ensures that attacks developed on open-source models remain effective on closed-source models. However, numerous empirical findings \cite{chao2024jailbreakbench, meade2024universal} suggest that gradient-based optimization approaches often fail to achieve consistent impact on target LLMs. For instance, we found that the Greedy Coordinate Gradient (GCG) method \cite{zou2023universal} failed to achieve high transfer attack performance even when applied to significantly weaker models. On the other hand, it is not surprising to find that some manually designed jailbreaking attacks \cite{andriushchenko2024jailbreaking} demonstrate good transferability, though only a few are discovered within the search space of gradient-based attacks.

% The key question is: If there exist attack prompts that can consistently jailbreak different models, 
%What factors cause our search process to bypass these transferable solutions? 
These observations prompt an investigation into the factors causing gradient-based search processes to bypass transferable solutions. To address this issue, we introduce a conceptual framework, as shown in Figure \ref{fig:intro}. All adversarial prompts capable of eliciting harmful responses constitute the entire feasible region for jailbreaking attacks, while transferable attack prompts reside within the shared region across various models. The search space of gradient-based optimization is only a subset of the entire feasible region, defined by the crafted objective function. However, superfluous constraints in current objective can further restrict this region, narrowing it to a subset where the model must produce a specific pattern to be deemed unsafe.

The superfluous constraints primarily stem from the "\textit{forcing}" loss in gradient-based optimization objectives. For example, when faced with a harmful query such as "How to make a bomb?", the model is implicitly coerced into initiating its response with a predetermined target like "Sure, here's how to make a bomb", even without explicit instructions on the desired response behavior.  As illustrated in Figure \ref{fig:redundant_1}, two superfluous constraints are introduced into the objective function: (1) \textbf{The response pattern constraint.} This refers to the discrepancy between the predefined target output and the actual jailbroken output. For instance, a jailbroken output might begin with “To make a bomb ...,” which significantly differs from the target phrase "Sure, here's how to make a bomb." This mandatory formatting requirement can significantly hinder the optimization process. (2) \textbf{The token tail constraint.} The enforced loss applied to each token fails to accommodate acceptable variations in real jailbroken outputs. For example, a response such as "Here's how to make a tiny bomb:\textbackslash n\textbackslash n**Step 1:**" would be penalized because it does not exactly match the target output "Here's how to make a bomb:\textbackslash nStep 1:". However, since the primary objective is to induce unsafe behavior, such minor deviations towards the end of the response should not be overly penalized.

To mitigate these issues, \textbf{we employ a "\textit{guiding}" loss instead of a "\textit{forcing}" loss to eliminate these two superfluous constraints}: Our approach provides guidelines for the desired response pattern while allowing flexibility in wording and formatting, particularly toward the end of the response. Empirically, this method significantly improves the overall Transfer Attack Success Rate (T-ASR) across both open-source and closed-source target models. Specifically, when using Llama-3-8B-Instruct as the source model, T-ASR improvements range from 18.4\% to 50.3\%, and for Llama-2-7B-Chat, from 20.5\% to 49.9\%. For models with weaker defenses, such as Qwen2, Vicuna and GPT-3.5-Turbo, our method consistently achieves a T-ASR of approximately 80\%.

We also observe substantial improvements in the Source Attack Success Rate (S-ASR) on the source model by removing the unnecessary constraints, increasing from 31.5\% to 85.2\% for the well-aligned Llama-3-8B-Instruct. This suggests that the challenging optimization process observed in well-aligned models \cite{zhu2024advprefix} is inherently related to the presence of superfluous constraints that reduce the size of the feasible region. Furthermore, we provide an in-depth analysis of how our method eliminates these superfluous constraints, resulting in more controllable and stable jailbreak behavior across both source and target models. Since our focus is on exploring and addressing the limitations of the basic optimization objective, our approach does not conflict with methods designed to enhance the efficiency of GCG.

The main contributions of this work are as follows:
\begin{itemize} 
    \item We introduce a conceptual framework for understanding the transferability of gradient-based jailbreaking attacks and highlight the phenomenon of stable transfer attacks.
    \item We identify superfluous constraints as a core limitation to the transferability of gradient-based jailbreaking attacks and thoroughly investigate how these constraints impede the optimization process and transferability.
    \item We propose a simple yet effective method, Guided Jailbreak Attack, which removes superfluous constraints and significantly enhances the transferability of adversarial attacks.
\end{itemize}

\begin{figure*}[!t] 
  \centering
  \hspace*{-0.5cm}
  \resizebox{1.05\textwidth}{!}{\includegraphics{figures/method5.png}}  % Resize to fit text width
  \caption{
    An illustration of superfluous constraints in gradient-based optimization objectives and their elimination. \textbf{Left:} The response pattern constraint arises from discrepancies between the target output and the actual jailbroken output, while the token tail constraint results from loss calculations applied to all tokens. \textbf{Right:} Guiding the model to begin with the target output and applying constraints only to necessary tokens effectively eliminates these superfluous constraints, thereby aligning the real jailbroken output with the target output. Tokens are highlighted as follows: \textcolor{customGreen}{meeting the requirement}, \textcolor{customRed}{failing to meet the requirement}, and \textcolor{customGray}{having no requirement}.
  }
  \label{fig:redundant_1}
\end{figure*}