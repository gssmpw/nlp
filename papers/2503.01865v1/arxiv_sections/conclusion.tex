\section{Conclusion}

In this paper, we investigate the challenges of transferable gradient-based adversarial attacks on large language models. Our analysis revealed that superfluous constraints—specifically the response pattern constraint and the token tail constraint—substantially weaken the consistency and reliability of transferred attacks. significantly reduce the consistency and reliability of transferred attacks. By removing these constraints, we propose Guided Jailbreaking Optimization, a method that significantly improves both the transfer Attack Success Rate (ASR) and the controllability of jailbreaking behaviors. When evaluated on the Llama-3-8B-Instruct as the source model, our approach raised the overall transfer ASR on various target models from 18.4\% to 50.3\%. These findings emphasize the importance of prioritizing essential constraints in optimizing objectives as unnecessary constraints can do crucial harm to the process. We highlight the potential for further improvements in gradient-based jailbreaking methods.