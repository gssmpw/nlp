\section{Related Work}

% todo

\paragraph{Gradient-Based Adversarial Prompt}
Gradient-based adversarial attacks, introduced by GCG \cite{zou2023universal}, primarily rely on token-level search and are notable for directly maximizing the probability of generating harmful content. Building upon GCG's optimization objectives and algorithms, recent works have explored various directions. For example, some studies \cite{jia2024improved} manually identify more effective harmful target formats, while others \cite{sun2024iterative, zhu2024advprefix} have developed automated methods to enhance the expected target output. Additionally, certain research efforts \cite{paulus2024advprompter, liao2024amplegcg} train auxiliary models to generate improved adversarial prompts, while others incorporate additional constraints, such as attention score regulation, into the original objective \cite{wang2024attngcg}.

\paragraph{Transferability of Jailbreak Attacks}
Heuristic-based algorithms \cite{shah2023scalable, yu2023gptfuzzer, liu2023autodan}, rewriting-based approaches \cite{deng2023jailbreaker, mehrotra2023tree}, and some manually designed jailbreaking attacks \cite{andriushchenko2024jailbreaking} generally exhibit superior transferability compared to gradient-based adversarial prompts. Although the widely recognized GCG method \cite{zou2023universal} asserts transferability and certain iterative methods \cite{sun2024iterative} demonstrate improved performance on some closed-source models, empirical studies \cite{chao2024jailbreakbench, meade2024universal} have reported inconsistent success when these techniques are applied to various LLMs. Furthermore, recent work \cite{lin2025understanding} investigates transferability from an intent analysis perspective, revealing that obscuring the source LLM's perception of malicious-intent tokens can further enhance transferability.
