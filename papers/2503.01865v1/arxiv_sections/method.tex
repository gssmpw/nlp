% todo，给llama3上的例子
\section{Method}
\label{sec:method}

\subsection{Guided Jailbreaking Optimization}
To address these limitations, we propose a method termed \textbf{Guided Jailbreaking Optimization} which employs a "\textit{guiding}" loss to remove superfluous constraints $\mathcal{L}_{rp}(x_{1:n})$ and \(\mathcal{L}_{\text{tail}}(x_{1:n})\). As shown on the right side of Figure \ref{fig:redundant_1},  our approach introduces two principal modifications to the basic objective:

\begin{itemize}
    \item \textbf{Target Output Guidance} (Removing $\mathcal{L}_{rp}(x_{1:n})$): We explicitly include the target output within the input to guide the model in generating the target output from the beginning.

    \item \textbf{Relaxed Loss Computation} (Removing \(\mathcal{L}_{\text{tail}}(x_{1:n})\)): Building on the guidance provided by the target output, the objective loss is computed exclusively on the essential tokens at the beginning of the entire target.
\end{itemize}

The complete algorithm is provided in Appendix \ref{sec:ejo}. We use an adversarial prefix rather than a suffix because our analysis shows that a suffix demands more tokens for comprehensive optimization, thereby imposing a greater tail token constraint. Detailed validation is available in Appendix \ref{sec:pre}.

\subsection{How superfluous constraints are Removed}

We begin by analyzing how our method effectively eliminates superfluous constraints. This analysis not only demonstrates the efficacy of our approach but also clarifies the critical role these constraints play in the optimization process.

\subsubsection{Response Pattern Constraint}
\label{sec:pattern}
As illustrated in Figure \ref{fig:loss_compare}, the original GCG method, even after optimization (and thus already in a jailbroken state), consistently produces loss values significantly higher than the expected range, specifically well above the red zone corresponding to the model's true output distribution (0.04 to 0.24). In contrast, the Guided Jailbreaking Optimization process effectively restricts loss values to remain predominantly within the normal range, demonstrating that the output aligns with both the intended target and the model's inherent distribution, thereby eliminating the response pattern constraint.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/loss_compare_new.png}
  \caption{
    Cross-Entropy Loss on the target output during the optimization process on Llama-3-8B-Instruct. For Normal Loss, Cross-Entropy Loss is calculated on the actual model output for benign inputs, focusing on the first 10 tokens. This is comparable to the expected real jailbroken loss.
  }
  \label{fig:loss_compare}
\end{figure}

\subsubsection{Token Tail Constraint}
\label{sec:pre_tail}
Figure \ref{fig:loss_tail} highlights two key aspects of the token tail constraint: (1) weak confidence tokens and (2) fixed format preferences.

Even when the model successfully generates the target output, certain tokens exhibit relatively weak confidence (probabilities below 90\%). Additionally, different models show strong preferences for varying response formats. For instance, when guided to follow the format "\textbackslash n Step 1: ...", the following preferences were observed:

\begin{itemize} 
    \item Llama3 and Gemma: "\textbackslash n\textbackslash n **Step 1**: ..."
    \item Llama2: "\textbackslash n Step 1: ..."
    \item Yi-1.5-9B: "\textbackslash n\textbackslash n Step 1: ..."
\end{itemize}

Optimizing for the token tail constraint can lead to early termination in the source model and lower attack success rates (ASR) in the target model. By optimizing only for the necessary number of tokens, our approach effectively circumvents these superfluous constraints. In Section \ref{sec:exp_tail}, we further analyze the relationship between token optimization and transfer ASR.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/tail.png}
  \caption{
   The comparison conducted on Llama-3-8B-Instruct between optimizing only the first two tokens of the target output and optimizing all tokens of the target output. The analysis used the same malicious input combined with the searched adversarial prompt. The Softmax probability was then calculated over the tokens of the target output, which were fully present within the input.
  }
  \label{fig:loss_tail}
\end{figure}