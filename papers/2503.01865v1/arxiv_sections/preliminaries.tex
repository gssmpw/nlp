\section{Preliminaries}

\subsection{Background: Optimizing Objective}

Most gradient-based optimization methods share a common objective with GCG \cite{zou2023universal}. In GCG, an adversarial prompt \(X = x_{1:n}\) is appended to a harmful question \(Q = q_{1:m}\) (e.g., “How to make a bomb?”), resulting in the combined input \(q_{1:m} \otimes x_{1:n}\). The objective is to induce the target LLM to generate a response that begins with the targetprefix \(A = a_{1:k}\) (e.g., “Sure, here is how to make a bomb:”). Here, \(x_i\), \(q_i\), and \(a_i\) belong to the vocabulary set \(\mathbb{V}\). The standard approach employs the negative log-probability of the target token sequence as the loss function:
\begin{equation}
\label{eqn:base_objective}
\begin{split}
    &\mathcal{L}(x_{1:n}) = -\log p(a_{1:k} \mid q_{1:m}, x_{1:n}) \\
               &= - \sum_{i=1}^{k} \log p(a_i \mid q_{1:m}, x_{1:n}, a_{1:i-1})
\end{split}
\end{equation}

Executed via the GCG algorithm (Algorithms \ref{alg:gcg} and \ref{alg:universal-opt} in Appendix \ref{sec:upo_alo}), the optimization problem can be expressed as:
\begin{equation}
\min_{x_{1:n} \in \mathbb{V}^{n}} \mathcal{L}(x_{1:n})
\end{equation}

\subsection{Formulation of Transferability}
\label{sec:formulation_of_trans}
In adversarial prompt optimization, transferability denotes a prompt's ability to elicit a consistent, malicious response across different models. For a given adversarial prompt \(x_{1:n}\), the objective is to cause models \(M_A\) and \(M_B\) to generate harmful responses that closely resemble the target response \(A = a_{1:k}\).

Let \( \mathcal{F}_{A} \subset \mathbb{V}^n \) denote the complete feasible region for jailbreaking model \(M_A\); that is, the set of all the jailbreaking prompts that successfully trigger harmful outputs. In practice, search methods can only explore a subset \( \mathcal{F}_{A}^s \subset \mathcal{F}_{A} \), defined by specific optimization constraints. For example, the previous objective ensures that the model’s response exactly begins with the target answer \(a_{1:k}\). This imposes a strong response pattern forcing constraint, resulting in a relatively small and specific region.

As illustrated in Figure \ref{fig:intro}, the objective of a transferable attack is to shape the search region \( \mathcal{F}_{A}^s \) so that it approximates the shared feasible region \(F_{shared} = \mathcal{F}_{A} \cap \mathcal{F}_{B}\), even when the optimization is performed solely on model \(M_A\). When a substantial portion of \( \mathcal{F}_{A}^s \) lies within \(F_{shared}\), the attack exhibits high and controllable transferability.