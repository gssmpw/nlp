\section{Experiments}

\input{tables/llama3_res}

\input{tables/llama2_res}

\subsection{Setup}

\paragraph{Dataset} 

We utilize harmful questions and their corresponding targets from Harmbench \cite{mazeika2024harmbench} to train and evaluate jailbreak attack methods. To assess universal effectiveness, we train on a 20-question subset and test on the standard 200-question set.

\paragraph{Models}

We conduct transfer attacks on models with varying levels of safety features. Our open-source model set includes Llama-3-8B-Instruct \cite{dubey2024llama}, Llama-2-7b-Chat \cite{touvron2023llama}, Gemma-7B-It \cite{team2024gemma}, Qwen2-7B \cite{yang2024qwen2}, Yi-1.5-9B-Chat \cite{young2024yi} and Vicuna-7B-v1.5 \cite{vicuna2023}. For closed-source models, we select GPT-3.5-Turbo-0125 and GPT-4-1106-Preview \cite{achiam2023gpt}. Llama-3-8B-Instruct and Llama-2-7B-Chat are used as source models, while the remaining models are only treated as target models.

\paragraph{Evaluation and Metric}
We employ HarmBench-Llama-2-13B-cls \cite{mazeika2024harmbench} as our evaluator. Each experiment is repeated three times, and we calculate the average Attack Success Rate (ASR) along with the standard deviation to capture variability across runs. The primary metrics include the ASR on the source model (S-ASR) and the ASR on the target model (T-ASR), with the latter indicating transferability.

\paragraph{Baselines and Hyperparameters}
We introduce minor engineering enhancements to GCG, resulting in GCG-Adaptive, which serves as our primary baseline. GCG-Adaptive applies tailored response templates for each model—“Sure, here …” for Llama-2-7B-Chat and “Here is …” for Llama-3-8B-Instruct—to mitigate optimization challenges on Llama-3-8B-Instruct. Each universal adversarial prompt is iteratively optimized to a length of 100 tokens over 500 steps using the 20-question training subset. Additional technical details are provided in Appendix \ref{sec:hyp}.

\input{tables/shot_examples}
\subsection{Attack Results}

As shown in Table \ref{tab:llama3_res} and Table \ref{tab:llama2_res}, the Guided Jailbreaking Optimization method significantly improves the Attack Success Rate (ASR) across various target models. Specifically, for Llama-3-8B-Instruct, the average T-ASR on target models increases from 18.4\% to 50.3\%, while for Llama-2-7B-Chat, it rises from 20.5\% to 49.9\%. Additionally, substantial improvements on S-ASR are also observed on the source models themselves, with an increase from 31.5\% to 85.2\% for Llama-3-8B-Instruct and from 50.8\% to 77.8\% for Llama-2-7B-Chat.

\paragraph{Basic Transfer Phenomenon}

The phenomenon of transferable adversarial attacks demonstrates that the transfer attack success rate (T-ASR) can often be comparable to the source attack success rate (S-ASR) when transferred to models with weaker defenses. However, their effectiveness diminishes significantly when applied to models with comparable or stronger defenses.

For weaker models such as Qwen2-7B-Instruct, Yi-1.5-9B-Chat, Vicuna-7B-v1.5, and GPT-3.5-Turbo, the T-ASR frequently reaches or exceeds 80\%, closely mirroring the performance against the source model. In contrast, transferring the same prompts to stronger models (e.g., from Llama-2 to Llama-3 or GPT-4) is considerably more challenging. For instance, although the T-ASR on target model Llama-2-7B-Chat improves from 2.2\% to 21.0\% when using prompts from Llama-3-8B-Instruct, it remains substantially lower than Llama-3's S-ASR of 85.2\%.

\paragraph{Controllable Transferability}
Our analysis, as illustrated in Table \ref{tab:short_example}, reveals that the GCG objective consistently induces uncontrollable transferring behavior. Although this objective is designed to prompt models to begin with a specific target output, the target models often fail to adhere to this instruction, even when generating harmful responses. This indicates that jailbreaking outputs on target models are unpredictable and uncontrollable. In contrast, our proposed method demonstrates consistent and controllable transfer behavior, with all jailbroken models reliably initiating their outputs with the designated target.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/asr_token_number.png}
  \caption{
      ASR results for adversarial prompts with different level of the token tail constraint, optimized on Llama-3-8B-Instruct. The plot displays the transfer ASR (T-ASR) for Llama-2-7B-Chat and Qwen2-7B-Instruct, and the source ASR (S-ASR) forLlama-3-8B-Instruct, along with the corresponding standard deviation.
  }
  \label{fig:asr_token_number}
\end{figure}

\paragraph{Token Tail Constraint}
\label{sec:exp_tail}
As discussed in Section \ref{sec:pre_tail}, the token tail constraint significantly influences the optimization process; here, we analyze its impact on ASR outcomes. As shown in Figure \ref{fig:asr_token_number}, Llama-3-8B-Instruct's T-ASR on models with stringent safety mechanisms (e.g., Llama-2-7B-Chat) decreases as the token tail constraint strengthens (i.e., as more tokens are included in the loss computation). Specifically, T-ASR on Llama-2-7B-Chat declines sharply from 21\% with a 2-token tail to just 2.5\% when the full token tail is considered. Similarly, Llama-3-8B-Instruct's Source ASR (S-ASR) decreases moderately from 85.2\% to 71.8\% over the same range.

Moreover, models with varying safety levels exhibit different sensitivities to the token tail constraint. For instance, models with weaker safeguards, such as Qwen2-7B-Instruct, display minimal sensitivity, maintaining an ASR of approximately 87\% regardless of the loss token number.

\paragraph{Ablation Study}

As shown in Tables \ref{tab:llama3_res} and \ref{tab:llama2_res}, we evaluate the impact of removing each superfluous constraint individually. Our analysis reveals that retaining the response pattern constraint while removing the token tail constraint does not enhance ASR performance, maintaining similar results to GCG. This is because the primary unnecessary constraint, the response pattern constraint, is still hindering optimization.

For Llama-3-8B-Instruct, removing only the response pattern constraint results in significantly poorer performance compared to removing both constraints, particularly for more robustly safeguarded models. This indicates that the model exhibits a strong bias toward its preferred distribution, making the token tail constraint especially critical. In contrast, Llama-2-7B-Chat shows similar results regardless of the token tail constraint removal, likely due to its lower sensitivity and inherent preference for the provided pattern.