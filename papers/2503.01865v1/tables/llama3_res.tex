\begin{table*}[!t]
    \centering
    \setlength{\tabcolsep}{4pt}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{c|c|c|cccc}
        \toprule
        % First header row
        \multicolumn{3}{c}{\multirow{2}*{\textbf{Models}}} & \multicolumn{4}{c}{\textbf{Method}} \\
        \cmidrule(lr){4-7}
        % Second header row
        \multicolumn{3}{c}{~} & \textbf{GCG-Adaptive}  & \textbf{w/ $L_{rp}$} & \textbf{w/ $L_{tail}$} & \textbf{Ours} \\
        \midrule
        % Multi-row for Searched Model
        \textbf{Source Model} & \multicolumn{2}{c}{\textbf{Llama3-8B-Instruct}} & 31.5 \scriptsize{$\pm$ 27.6} & 25.8 \scriptsize{$\pm$ 19.6} & 51.0\scriptsize{$\pm$ 25.3} & \textcolor{red}{85.2} \scriptsize{$\pm$ 0.3} \\
        \midrule
        \multirow{8}{*}{\textbf{Target Model}}
        & \multirow{5}{*}{\textbf{Open-Source}}
            & Llama-2-7b-Chat & 2.2 \scriptsize{$\pm$ 1.5} & 6.0 \scriptsize{$\pm$ 0.5} & 4.7 \scriptsize{$\pm$ 8.1} & \textcolor{red}{21.0} \scriptsize{$\pm$ 7.4} \\
            & & Gemma-7b-It & 0.3 \scriptsize{$\pm$ 0.3} & 1.2 \scriptsize{$\pm$ 1.6} & 4.5 \scriptsize{$\pm$ 3.6} & \textcolor{red}{10.7} \scriptsize{$\pm$ 9.9} \\
            & & Qwen2-7B-Instruct & 31.5 \scriptsize{$\pm$ 15.6} & 24.8 \scriptsize{$\pm$ 9.8} & 87.8 \scriptsize{$\pm$ 2.1} & \textcolor{red}{87.5} \scriptsize{$\pm$ 1.7} \\
            & & Yi-1.5-9B-Chat & 24.0 \scriptsize{$\pm$ 7.0} & 20.3 \scriptsize{$\pm$ 11.5} & 54.0 \scriptsize{$\pm$ 8.7} & \textcolor{red}{58.8} \scriptsize{$\pm$ 22.3} \\
            & & Vicuna-7b-v1.5 & 17.8 \scriptsize{$\pm$ 4.2} & 10.2 \scriptsize{$\pm$ 3.0} & 88.1 \scriptsize{$\pm$ 3.1} & \textcolor{red}{88.2} \scriptsize{$\pm$ 1.0} \\
        \cmidrule(lr){2-7}
        % Multi-row for Closed-Source
        & \multirow{2}{*}{\textbf{Closed-Source}}
            & GPT-3.5-Turbo & 46.8 \scriptsize{$\pm$ 17.9} & 35.0 \scriptsize{$\pm$ 17.3} & 63.2 \scriptsize{$\pm$ 15.7} & \textcolor{red}{72.2} \scriptsize{$\pm$ 7.8} \\
            & & GPT-4 & 5.8 \scriptsize{$\pm$ 3.3} & 1.3 \scriptsize{$\pm$ 0.6} & 10.7 \scriptsize{$\pm$ 2.5} & \textcolor{red}{13.5} \scriptsize{$\pm$ 4.9} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-7}
        & \multicolumn{2}{c}{Target Model Avg.} & 18.4 & 14.1 & 44.8 & \textcolor{red}{50.3}\\
        \bottomrule
    \end{tabular}
    }
    \caption{Attack Success Rate (ASR) for source model and target models, \textbf{searched on Llama-3-8B-Instruct}. We also test the results of only removing token tail constraint and keeping responding pattern constraint (w/ $\mathcal{L}_{rp}$), and only removing responding pattern constraint and keeping token tail constraint (w/ $\mathcal{L}_{tail}$). We report the average ASR along with its standard deviation (indicated by $\pm$); note that all results have been multiplied by 100.}
    \label{tab:llama3_res}
\end{table*}
