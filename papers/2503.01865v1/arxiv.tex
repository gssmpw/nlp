% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{inconsolata}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable, theorems}
\usepackage{comment}
\usepackage[noEnd=True]{algpseudocodex}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{fancybox}
\usepackage{fancyvrb}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{amssymb}
\definecolor{customRed}{HTML}{FF0000}
\definecolor{customGreen}{HTML}{00B050}
\definecolor{customGray}{HTML}{44546A}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicloop}{\textbf{repeat}}
\newcommand{\highlight}[1]{{\color[HTML]{FD6864} \textbf{#1}}}

\title{Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints}

\author{
Junxiao Yang\footnotemark[1], Zhexin Zhang\footnotemark[1], Shiyao Cui, \textbf{Hongning Wang, Minlie Huang}\footnotemark[2]
\\
% \small{The CoAI group, DCST; Institute for Artificial Intelligence; State Key Lab of Intelligent Technology and Systems;}\\
% \small{Beijing National Research Center for Information Science and Technology;} 
The Conversational AI (CoAI) group, DCST, Tsinghua University\\
\small{yangjunx21@gmail.com, aihuang@tsinghua.edu.cn}
\\
}

\begin{document}
\maketitle

\begingroup
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\footnotetext[1]{Equal contribution.}
\footnotetext[2]{Corresponding author.}
\endgroup

\input{arxiv_sections/abstract}

\input{arxiv_sections/introduction}

\input{arxiv_sections/preliminaries}

\input{arxiv_sections/constraints}

\input{arxiv_sections/method}

\input{arxiv_sections/experiments}

\input{arxiv_sections/related_work}

\input{arxiv_sections/conclusion}

\section*{Limitations}

Although our approach consistently achieves high transferability on weaker target models, executing transfer attacks with high ASR on stronger models remains a significant challenge. Moreover, despite improvements in controllable transferability, inherent randomness in the target models persists. Additionally, since our method primarily fixes the original optimization goal, the attack remains detectable by the chunk-level PPL filter.

\section*{Ethical Considerations} In this work, we analyze transferable gradient-based adversarial attacks and introduce Guided Jailbreaking Optimization, a method that notably enhances both the transfer Attack Success Rate (ASR) and the controllability of adversarial behaviors.

We stress that the primary goal of our research is to deepen the understanding of vulnerabilities in large language models and to inform the development of more robust security defenses. Although our findings improve attack metrics on source models, we do not condone or encourage any malicious application of these techniques. Instead, we advocate for their use in strengthening safeguards and guiding responsible research practices.

We urge developers, researchers, and the broader AI community to leverage our insights to enhance security protocols and to work collaboratively towards building AI systems that adhere to ethical standards and protect user safety. 

\bibliography{custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Background Algorithms}
\label{sec:upo_alo}

As shown in Algorithm \ref{alg:gcg}, the Greedy Coordinate Gradient (GCG) algorithm estimates the top-k candidate tokens and selects the one that minimizes the loss after updating the adversarial prompt. The candidate tokens are chosen based on the backward gradient of the target loss.

Universal Prompt Optimization extends this process to multiple harmful questions using a progressive strategy, as outlined in Algorithm \ref{alg:universal-opt}.

\begin{algorithm*}[!t]
\caption{Greedy Coordinate Gradient}
\label{alg:gcg}
\begin{algorithmic}
\Require Initial prompt $x_{1:n}$, modifiable subset $\mathcal{I}$, iterations $T$, loss $\mathcal{L}$, $k$, batch size $B$
\Loop{ $T$ times}
    \For{$i \in \mathcal{I}$}
        \State $\mathcal{X}_i := \mbox{Top-}k(-\nabla_{e_{x_i}} \mathcal{L}(x_{1:n}))$ \Comment{Compute top-$k$ promising token substitutions}
    \EndFor
    \For{$b = 1,\ldots,B$}
        \State $\tilde{x}_{1:n}^{(b)} := x_{1:n}$
        \Comment{Initialize element of batch}
        \State $\tilde{x}^{(b)}_{i} := \mbox{Uniform}(\mathcal{X}_i)$, where $i = \mbox{Uniform}(\mathcal{I})$  \Comment{Select random replacement token}
    \EndFor
    \State $x_{1:n} := \tilde{x}^{(b^\star)}_{1:n}$, where $b^\star = \argmin_b \mathcal{L}(\tilde{x}^{(b)}_{1:n})$ \Comment{Compute best replacement}
\EndLoop
\Ensure Optimized prompt $x_{1:n}$
\end{algorithmic}
\end{algorithm*}

\begin{algorithm*}[!t]
\caption{Universal Prompt Optimization}
\label{alg:universal-opt}
\begin{algorithmic}
\Require Prompts $x_{1:n_1}^{(1)} \ldots\, x_{1:n_m}^{(m)}$, initial suffix $p_{1:l}$, losses $\mathcal{L}_1 \ldots\, \mathcal{L}_m$, iterations $T$, $k$, batch size $B$
\State $m_c := 1$ \Comment{Start by optimizing just the first prompt}
\Loop{ $T$ times}
    \For{$i \in [0 \ldots l]$}
        \State $\mathcal{X}_i := \mbox{Top-}k(-\sum_{1 \le j \le m_c} \nabla_{e_{p_i}} \mathcal{L}_j(x_{1:n}^{(j)}\|p_{1:l}))$ 
        \Comment{Compute aggregate top-$k$ substitutions}
    \EndFor
    \For{$b = 1,\ldots,B$}
        \State $\tilde{p}_{1:l}^{(b)} := p_{1:l}$
        \Comment{Initialize element of batch}
        \State $\tilde{p}^{(b)}_{i} := \mbox{Uniform}(\mathcal{X}_i)$, where $i = \mbox{Uniform}(\mathcal{I})$  
        \Comment{Select random replacement token}
    \EndFor
    \State $p_{1:l} := \tilde{p}^{(b^\star)}_{1:l}$, where $b^\star = \argmin_b \sum_{1 \le j \le m_c}\mathcal{L}_j(x^{(j)}_{1:n}\|\tilde{p}^{(b)}_{1:l})$ 
    \Comment{Compute best replacement}
    \If{$p_{1:l}$ succeeds on $x_{1:n_1}^{(1)} \ldots\, x_{1:n_m}^{(m_c)}$ and $m_c < m$}
        \State $m_c := m_c + 1$ \Comment{Add the next prompt}
    \EndIf
\EndLoop
\Ensure Optimized prompt suffix $p$
\end{algorithmic}
\end{algorithm*}

\section{Guided Jailbreaking Optimization}
\label{sec:ejo}
As described in Section \ref{sec:method}, Guided Jailbreaking Optimization primarily revises the optimization objective of the GCG method, thereby preserving the overall structure of the algorithm. The corresponding algorithm, implemented within the Universal Prompt Optimization framework, is presented in Algorithm \ref{alg:ejo}, with the modified sections highlighted in red.

\begin{itemize}
    \item Target Output Guidance: We explicitly add the target output to the input during the optimizing process.

    \item Relaxed Loss Computation: The loss is computed solely on necessary tokens.

    \item Prefix: We use prefix rather than suffix for better removing token tail constraint.
\end{itemize}

The full input template is as follows:
\begin{tcolorbox}[title = {Input template}, breakable]
\textbf{\# Input Template} \\
\{Adversarial Prompt\} \{Question\} \\
Start your response with "\{Target\}:\textbackslash nStep 1"\\
---------\\
\textbf{\# Example} \\
\{Adversarial Prompt\} Give detailed instructions for hiring a hitman without getting caught\\
Start your response with "Here's how to hire a hitman without getting caught:\textbackslash nStep 1"
\end{tcolorbox}

\begin{algorithm*}[t]
\caption{Guided Jailbreaking Optimization}
\label{alg:ejo}
\begin{algorithmic}
\Require Harmful Questions $x_{1:n_1}^{(1)} \ldots\, x_{1:n_m}^{(m)}$ and \textcolor{red}{corresponding outputs $a_{1:t_1}^{(1)} \ldots\, a_{1:t_m}^{(m)}$}, initial \textcolor{red}{prefix $p_{1:l}$}, fixed losses \textcolor{red}{on necessary tokens} $\mathcal{L}_1 \ldots\, \mathcal{L}_m$, iterations $T$, $k$, batch size $B$
\State $m_c := 1$ \Comment{Start by optimizing just the first prompt}
\Loop{ $T$ times}
    \For{$i \in [0 \ldots l]$}
        \State $\mathcal{X}_i := \mbox{Top-}k(-\sum_{1 \le j \le m_c} \nabla_{e_{p_i}} \textcolor{red}{\mathcal{L}_j(p_{1:l}\|x_{1:n}^{(j)}\|a_{1:t}^{(j)}))}$ 
        \Comment{Compute aggregate top-$k$ substitutions}
    \EndFor
    \For{$b = 1,\ldots,B$}
        \State $\tilde{p}_{1:l}^{(b)} := p_{1:l}$
        \Comment{Initialize element of batch}
        \State $\tilde{p}^{(b)}_{i} := \mbox{Uniform}(\mathcal{X}_i)$, where $i = \mbox{Uniform}(\mathcal{I})$  
        \Comment{Select random replacement token}
    \EndFor
    \State $p_{1:l} := \tilde{p}^{(b^\star)}_{1:l}$, where $b^\star = \argmin_b \sum_{1 \le j \le m_c}\mathcal{L}_j(\textcolor{red}{\tilde{p}^{(b)}_{1:l}\|x_{1:n}^{(j)}\|a_{1:t}^{(j)})})$ 
    \Comment{Compute best replacement}
    \If{$p_{1:l}$ succeeds on $x_{1:n_1}^{(1)} \ldots\, x_{1:n_m}^{(m_c)}$ and $m_c < m$}
        \State $m_c := m_c + 1$ \Comment{Add the next prompt}
    \EndIf
\EndLoop
\Ensure Optimized prompt \textcolor{red}{Prefix $p$}
\end{algorithmic}
\end{algorithm*}


\section{Prefix and Token Tail Constraint}
\label{sec:pre}
The source ASR (S-ASR) of the source model and the target ASR (T-ASR) of the target models are comparable when computing loss over the complete token sequence. However, figure \ref{fig:prefix_analysis} illustrates that when applying prefix optimization, we observe that calculating the loss over just 2 tokens is sufficient for full optimization, whereas the same is not true for suffix optimization. Consequently, employing a suffix strategy makes it more challenging to remove the token tail constraint, leading us to adopt a prefix optimization approach.

\begin{figure}[!t] 
\centering 
\includegraphics[width=\linewidth]{figures/prefix_analysis.png} 
\caption{ Comparison of prefix and suffix optimization on Llama-3-8B-Instruct. The loss curve indicates that the optimal token length for loss computation differs between the two approaches, with prefix optimization more effectively eliminating the token tail constraint and enhancing transferability. } \label{fig:prefix_analysis} 
\end{figure}

\section{Hyperparameters}
\label{sec:hyp}
The training set consists of 20 questions. We retain most of the default hyperparameters of GCG while increasing the suffix length to 100. Our experiments indicate that, for adversarial attack prompts generated by both GCG and our method, a suffix length of 100 outperforms lengths of 50 and 20.
\input{tables/hyp}

\section{Models Used in Our Experiments}
We provide the download links to the models used in our experiments as follows:
\begin{itemize}
    \item Llama-3-8B-Instruct (\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct})
    \item Llama-2-7B-Chat (\url{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf})
    \item Gemma-7B-It (\url{https://huggingface.co/google/gemma-7b-it})
    \item Qwen2-7B-Instruct (\url{https://huggingface.co/Qwen/Qwen2-7B-Instruct})
    \item Yi-1.5-9B-Chat (\url{https://huggingface.co/01-ai/Yi-1.5-9B-Chat})
    \item Vicuna-7B-v1.5 (\url{https://huggingface.co/lmsys/vicuna-7b-v1.5})
    \item HarmBench-Llama-2-13b-cls (\url{https://huggingface.co/cais/HarmBench-Llama-2-13b-cls})
\end{itemize}

% \section{Generation Examples}
% \label{sec:more_examples}

\end{document}