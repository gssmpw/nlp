\section{Experiments}
\input{figures/renderings_evaluation}
In this section, we present a thorough evaluation of the neural BRDF approaches. 
First, we conduct a qualitative and quantitative comparison on the semi-synthetic and real-world datasets described in \cref{sec:datasets}. Subsequently, we analyze several aspects of the models, including energy conservation and the approaches to reciprocity, in more depth.
We refer to the appendix for more information on the models and the datasets, as well as several additional experiments.

\paragraph{Evaluation Metrics}

We transform the renderings of the test set from linear to sRGB space and evaluate  \psnr, \lpipsc and DSSIM, where the latter is based on \ssimc by $\text{DSSIM} = (1-\text{SSIM})/2$. Moreover, we report the HDR version of the \FLIP metric \cite{Andersson2021HDRFLIP,Andersson2020FLIP}. For each metric, we average the results of the respective dataset.

Following \cite{Lavoue21PerceptualQualityOfBRDFApproximations}, we 
report the root mean squared error between the cubic root of the predicted and the \gt BRDF values for the synthetic experiments (RMSE$^{\sqrt[3]{}}$). As analyzed in \cite{Lavoue21PerceptualQualityOfBRDFApproximations}, we discard values close to the horizon from the data. Moreover, we exclude values from saturated pixels. 
This BRDF-space metric correlates well with perceptual similarity. 
We refer to the supplement for more details.


\subsection{Comparison of the BRDF Models}
\label{sec:comparison_brdf_models}

\input{tables/quantitative}

We show a qualitative comparison of the models in \cref{fig:evaluation_renderings} and report the metrics in \cref{tab:quantitative}.
For the semi-synthetic examples based on the MERL data \cite{matusik2003MERL}, the results show a significant advantage of the purely neural approaches, outperforming the methods based on parametric models by a large margin on all metrics. The examples in the top rows of \cref{fig:evaluation_renderings} and \cref{fig:teaser} confirm that only the purely neural approaches can faithfully capture the complex reflection patterns of highly specular materials. For the real-world data, the difference between the models is much less significant. One reason is the absence of highly specular materials in the DiLiGenT-MV dataset. Another potential reason are interreflections in the real-world data, which we do not model. Finally, there might be noise in the real-world data not captured by the simple noise model used to create the semi-synthetic data. Indeed, we see a higher influence of noise on the purely neural models: If we remove the noise from the semi-synthetic data, the performance gains for those models are much higher than for the parametric models.

Among the parameter-based models, the Disney BRDF \cite{burley2012physically} performs best, which is expected since it is the most sophisticated model. For the FMBRDF \cite{ichikawa2023fresnel}, we observe a ``glow'' for the cow from the real-world data (see \cref{fig:evaluation_renderings}). We hypothesize that this originates from the scalar specular term since a similar effect is observed for the scalar additive architecture, as shown in the appendix. This effect does not appear for other objects.

For the purely neural approaches, we see a different behavior between the 
two datasets, 
with the ranking order being reversed. The reason seems to be the number of layers that the view and the light direction are fed through. While it is only 2 layers for the additive shared architecture, there are 4 for additive separate and 6 for the single MLP architectures. We investigate this further in the next section.

Our enhancement for the additive split (\cf \cref{sec:enhancingAddSplit}) shows slight improvements for both additive approaches. This confirms that the additional freedom can help the model to adapt better to the underlying BRDF functions.

\subsection{Analysis of the BRDF Models}
\label{sec:analysis_brdf_models}

\paragraph{Number of Layers for View/Light Directions}
\input{tables/changes_quantitative_NOL}
To confirm that, indeed, the difference in the performance of the purely neural models for the real-world and semi-synthetic data originates from the number of layers (NOL) for the directions, we perform experiments where we vary this parameter for each of the purely neural models. 

The results are shown in \cref{tab:changes_quantitative_NOL}, where we indicate the change in the NOL for the directions per model. Please see the appendix for the detailed architecture changes. The numbers confirm the trend that reducing the NOL decreases the reconstruction quality for the MERL examples while increasing the quality for the DiLiGenT-MV data while increasing the NOL has the opposite effect. This may suggest that more layers for the directions are beneficial for complex reflective patterns as contained in the MERL data, but at the same time, make the model less robust to potentially noisy measurements of real-world data.

\paragraph{Reciprocity Mapping}
\input{tables/violation_reciprocity}

To evaluate the random input swap training strategy used in LitNerf \cite{Sarkar23LitNerf} as well as our input mapping proposed in \cref{sec:reciMapping}, we quantify how much the symmetry constraint is violated by computing the RMSE between the BRDF values with changed positions, i.e. for the pairs $\brdf(\point, \light, \view)$ and $\brdf(\point, \view, \light)$. We report the results for the random input swap and the models without any reciprocity strategy in \cref{tab:reciprocity}. Note that our approach fulfills the constraint exactly by construction and is therefore not included in the table. We see, that without any reciprocity strategy, the constraint is violated significantly, particularly for the real-world data. While the random input swap does reduce this error by several orders of magnitude, the constraint is still violated, which might cause problems for particular algorithms. In contrast, \cref{eq:brdf_reciprocity} is fulfilled by construction by our approach.
The influence on the reconstruction quality is analyzed in the appendix. Apart from the single MLP architecture, for which the random input swap performs slightly better on the real-world data, both approaches have a similar effect, and we obtain only slightly worse results as a trade-off for the ensured reciprocity.

\paragraph{Energy Conservation}
\input{tables/eval_energy}
To analyze how well the models fulfill the energy conservation in \cref{eq:brdf_energy_conservation}, we analyze $50k$ randomly sampled point-light pairs for each object from the test data. We approximate the energy integral in \cref{eq:brdf_energy_conservation} by Monte-Carlo (MC) integration, where we use cosine-weighted hemisphere sampling and draw $20k$ view direction samples for each integral to ensure convergence. A more detailed discussion can be found in the appendix.

The results in \cref{tab:energy} show that for the semi-synthetic data, all approaches fulfill energy conservation almost everywhere. For the real-world data, we observe more significant violations; in particular for the purely neural methods, which might again indicate measurement noise. The fully separated additive approach seems to be particularly disadvantageous for energy conservation.

