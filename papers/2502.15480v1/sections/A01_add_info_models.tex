\section{Additional Details on the Models}
\label{sec:supp:add_details_models}

In this section, we give additional details on the neural BRDF modeling. \cref{sec:supp:angle_definitions} reviews the Rusinkiewicz angles used to parametrize the view and light direction, \cref{sec:supp:intrinsic_neural_encoding} gives an overview of the intrinsic encoding used to parametrize the neural BRDFs directly on the meshes. Finally, \cref{sec:supp:parametric_models} reviews the parametric models used to construct the parametric neural BRDFs.


\subsection{Angle Definitions}
\label{sec:supp:angle_definitions}

\input{figures/supp_angles}

While the BRDF is often defined in terms of the view and the light unit directions, $\view$ and $\light$, respectively, those two quantities are elements on the (curved) $\Sphere^2$ manifold, which makes them impractical to work with. Therefore, a parametrization in terms of the angles of the vectors is usually employed. The most obvious choice is to use the polar angles $(\theta_\view, \phi_\view)$ and $(\theta_\light, \phi_\light)$ of the two vectors directly, see \cref{fig:angles} on the left. We refer to this parametrization as \emph{view-light angles}. For an isotropic BRDF, the absolute orientation in the tangential plane is irrelevant, and only the relative orientation of the vectors with respect to each other is important. In this case the triplet $(\theta_\view, \theta_\light, \phi_\view - \phi_\light)$ is sufficient to parameterize the BRDF.

However, as noted by Rusinkiewicz, the view-light angles are suboptimal to parameterize a BRDF \cite{rusinkiewicz1998new}. They propose a novel parameterization that aligns important features of the BRDF, like the specular peak, with the coordinate axes. The representation is based on the half-vector

\begin{equation}\label{eq:half_angle}
    h = \frac{\view + \light}{\|\view + \light\|}.
\end{equation}

and a ``difference'' vector $d$, which is obtained by rotating the light direction into a coordinate system, where the half vector $h$ coincides with the normal, see \cref{fig:angles} on the right for an overview. We denote the surface normal by $n$ and use an arbitrary tangential vector $t$ and the binormal $b=n\times t$ to define a local coordinate system. The difference vector $d$ is computed as 
\begin{equation}
    d=\mathrm{rot}_{b,-\theta_h}\mathrm{rot}_{n, -\phi_h}l,
\end{equation}
where $\mathrm{rot}_{x, \alpha}y$ means the rotation of vector $y$ around the axis $x$ by the angle $\alpha$. The full Rusinkiewicz angles read $(\theta_h, \phi_h, \theta_d, \phi_d)$. Isotropic BRDFs do not depend on $\phi_h$; therefore, the triplet $(\theta_h, \theta_d, \phi_d)$ is sufficient for the parametrization in this case. See \cite{rusinkiewicz1998new} for more details.

\subsection{Neural Intrinsic Encoding}
\label{sec:supp:intrinsic_neural_encoding}

For the positional encoding on the manifold, we use the neural intrinsic encoding described by Koestler \etal \cite{KoestlerIntrinsicNeuralFields22}, who propose to use a subset of the eigenfunctions of the  Laplace-Beltrami operator (LBO)\footnote{We refer to \cite{Rustamov07Laplace-BeltramiEigenfunctions} for more details on the LBO.} for the encoding. Since the LBO is a generalization of the Laplace operator on an Euclidean domain, this form of encoding can be seen as an adaption of positional encoding from the Euclidean domain to manifolds like meshes. Given a closed, compact manifold $\mathcal{M}\subset\mathbb{R}^n$, we denote the eigenfunctions of the LBO on $\mathcal{M}$ by $\zeta_j$. For the encoding, we consider a subset of indices $\mathcal{I}\subset \mathbb{N}$ and, given a point $\point$ on the mesh, use the encoding
\begin{equation}
    \Theta_{LBO}(\point) = (\zeta_j(\point))_{j\in\mathcal{I}}.
\end{equation}
In the discrete setting of a mesh, the values of the LBO eigenfunctions are computed on the vertices. We use barycentric interpolation to obtain the encoding for an arbitrary position on the mesh.
In practice, it is not necessary to use a connected index set. We follow \cite{KoestlerIntrinsicNeuralFields22} and use several connected blocks of eigenfunctions for the encoding; see \cref{sec:supp_lbo_params} for the concrete numbers.

\subsection{Parametric BRDF Models}
\label{sec:supp:parametric_models}

In the following section, we give a short overview of the parametric models used in this work. We refer to the respective works of more details.

\paragraph{Realistic Phong Model}
We use the model described in \cite{lafortune1994using}. To ensure energy conservation, we predict a combined value $k_\mathrm{full}\in[0,1]^3$ for the reflectivities as well as a split percentage $\zeta\in[0,1]^3$ and compute the individual reflectivities as $k_d = \zeta\circ k_\mathrm{full}$ and $k_s = (1-\zeta)\circ k_\mathrm{full}$.
To ensure the correct range, we use a sigmoid nonlinearity for $k_\mathrm{full}$ and $\zeta$. The specular exponent $n\geq1$ is computed using a softplus output function with an additive offset of 1.

\paragraph{Torrance-Sparrow Model}
For the specular part, we use the basic form of the microfacet model described in \cite{torrance1967theory}, which reads (omitting the dependence on the position)
\begin{equation}
    f_\mathrm{spec}(\light, \view) = \frac{D(h)F(\view, h)G(\light, \view, h)}{4\langle n, l\rangle\langle n, v\rangle},
\end{equation}
where $h$ is the half vector described in \cref{eq:half_angle}, $D$ is the normal distribution function (NDF), $F$ is the Fresnel term, $G$ is the geometric shadowing term and $n\in\Sphere^2$ is the surface normal in the local coordinate system. By $\langle n, l\rangle$ we denote the scalar product between $n$ and $l$.

We use the (isotropic) Trowbridge-Reitz/GGX distribution~\cite{smith1967geometrical, walter2007microfacet, trowbridge1975average} for $D$, which reads 
\begin{equation}
    D(h) = \frac{\alpha^2}{\pi(\langle n, h\rangle^2(\alpha^2 - 1) + 1)^2}
\end{equation}
with the roughness parameter $\alpha>0$.
For $G$, we use smiths method \cite{smith1967geometrical}, which splits the term multiplicatively as
\begin{equation}
    G(\light, \view, h) = \Tilde{G}(\light)\Tilde{G}(\view).
\end{equation}
We use the Trowbridge-Reitz/GGX term~\cite{smith1967geometrical, walter2007microfacet, trowbridge1975average} for $\Tilde{G}$, which for a vector $w\in\mathbb{R}^3$ reads
\begin{equation}
    \Tilde{G}(w) = \frac{2\langle n, w\rangle}{\langle n, w\rangle + \sqrt{\alpha^2 + (1-\alpha^2)\langle n, w\rangle^2}},
\end{equation}
again with the same roughness parameter $\alpha$.
For $F$ we use Schlick's Fresnel term~\cite{schlick1994inexpensive}, which reads
\begin{equation}
    F(\view, h) = F_0 + (1-F_0)(1-\langle\view, h\rangle)^5,
\end{equation}
with the characteristic specular reflectance $F_0\in[0, 1]^3$.

We combine the specular part with a Lambertian diffuse term, which we diminish by $1-F$. The additional factor accounts for the fact that light reflected at the surface is not available for (diffuse) subsurface scattering. Hence, the full BRDF reads
\begin{equation}
     f(\light, \view) = (1- F(\view, h))\frac{\rho_d}{\pi} +  f_\mathrm{spec}(\light, \view).
\end{equation}

The parameters predicted by the neural network are the roughness parameter $r\in[0,1]$ from which we compute $\alpha = r^2$, as well as the diffuse albedo $\rho_d\in[0, 1]^3$ and the characteristic specular reflectance $F_0\in[0, 1]^3$. We use a sigmoid output nonlinearity for all of them to ensure the correct range. Note that we found the convergence to be more stable if we scale the input to the roughness nonlinearity by 0.5.

\paragraph{Fresnel Microfacet BRDF Model}
The Fresnel microfacet BRDF combines an extended microfacet specular term with a generalized radiometric body reflection \cite{ichikawa2023fresnel}. We refer to the paper for an in-depth discussion of the method. One change in the approach for the specular term compared to the Torrance-Sparrow model is that a generalized normal distribution is used for $D$, which involves estimating a corresponding normalization constant during training. While we were able to predict the rest of the parameters in a spatially varying manner, we were unable to do so for the parameters of this generalized normal distribution since the estimation of the normalization makes the computation untraceable. Therefore, we estimate a single set of parameters for the distribution, while all the other parameters are estimated dependent on the position $\point$. We want to point out again that for the synthetic data, this makes the estimation less complex since the material is uniform over the mesh.

\paragraph{Disney BRDF}
The Disney BRDF extends the microfacet model by several effects, and we refer to the paper for a detailed derivation and discussion \cite{burley2012physically}. The only adjustment we do compared to the full model is to fix the \emph{anisotropic} parameter equal to 0, effectively making the BRDF isotropic. Since all of the other models are isotropic, this enables a meaningful comparison between the approaches. Note that we found the convergence to be more stable if we scale the input to the roughness nonlinearity by 0.5.

\subsection{Softplus Scaling}
\label{sec:supp:softplusScaling}

While tuning the models, we occasionally observed runs for the additive purely neural models, where the training converged to a wrong local minimum. We found that scaling the softplus output of the specular MLP by a factor of 0.5 can stabilize the training in those cases. See also \cref{sec:supp:failure_cases} for a visualization of the failure case.