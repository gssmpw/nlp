\section{Methodology and Datasets}
We estimate the BRDFs from a sparse set of HDR images from multiple viewpoints, each taken under changing directional lighting conditions. We assume mesh, camera poses, and light calibration to be known. This controlled setting ensures that the strengths of the reflection models can be assessed without confounding effects from estimating other quantities as well.

\subsection{Rendering}
For a single directional light, the irradiance $\LightIntIn$ is independent of the position $\point$, and the light direction $\light$ is constant for each view. In that case, the integral rendering equation in \cref{eq:rendering_eq}, reduces to a single evaluation and the rendering $\LightIntOut(\point, \view)$ for the pixel corresponding to $x$ and $v$ now reads
\begin{equation}
    \label{eq:rendering_single_dir_light}
    \LightIntOut(\point, \view) = \brdf(\point, \light, \view) \LightIntIn \mathbb{I}_s(\point, \light) \cos\theta_\light.
\end{equation}
The indicator function $\mathbb{I}_s(\point, \light)$ is used as a masking term to account for cast-shadows. Its value is computed by casting a secondary ray from the first intersection point $\point$ on the mesh in direction $\light$ and checking if the ray intersects the object. We apply a shadow bias to avoid self-shadow aliasing \cite{akenine2019realTimeRendering}. The intersection points $\point$ are computed by standard ray mesh intersection, and we compute the normal on the mesh by barycentric interpolation of the vertex normals.

\subsection{Loss and Training}
Since we are working with HDR images, which have a much larger dynamic range than sRGB images, a standard $L_2$ loss would be dominated by the bright regions, suppressing information from darker regions. To avoid this, 
we follow the analysis by Mildenhall \etal \cite{Mildenhall22NerfInTheDark} and
use a gamma correction function $\gamma: [0, 1]\mapsto [0,1]$ to construct a tone-mapped MSE-based loss term
\begin{equation}\label{eq:loss}
    \mathcal{L} = \frac{1}{N}\sum_{i=1}^N (\gamma(L_o(x, v)) - \gamma(L_{GT}(x, v))^2,
\end{equation}
where $L_{GT}(x, v)$ is the ground truth color of the pixel corresponding to $x$ and $v$ in linear color space. 

We train the models using the Adam optimizer \cite{Kingma14Adam} with a batch size of $N_{b}=2^{15}$ color values. For more details on the loss and the training, see the appendix.

\subsection{Datasets}
\label{sec:datasets}
We use the DiLiGenT-MV real-world multi-view dataset with calibrated lighting~\cite{Li2020DiLiGentMVDataset}. It consists of HDR images of $5$ objects with spatially varying, complex reflectance behaviors, taken from $20$ views, each captured under $96$ calibrated directional lights. We use the included ground truth meshes. %

Additionally, we create a semi-synthetic dataset based on real BRDF measurements to evaluate and analyze the performance in a more controlled setting. We render HDR images with 25 different \emph{uniform} MERL BRDFs \cite{matusik2003MERL} on 9 common 3D test meshes \cite{jacobson2020common}.
We add Gaussian noise with 
$\sigma = 10^{-3}$ to the images.
Besides the rendering, we also store the raw 
reflectance values to enable the direct evaluation of the BRDF prediction.
We train all models on $10$ views with $30$ lights each and use $12$ lights for each of the $10$ remaining views as the unseen test set for the evaluation.

