\section{Neural BRDFs}
\input{figures/architectures}
This work aims to compare different neural BRDF modeling approaches in a unified manner. We deliberately choose a setting with \emph{given} geometry and \emph{calibrated} lights to avoid confounding effects due to joint estimation of geometry, reflectance, and light. This ensures that the results capture the capabilities of the models as clearly as possible. We use meshes to represent the geometry since they allow for accurate normals. We choose to reconstruct the BRDF from posed images rather than from BRDF measurements since this is the more practical 
and widely used setting.

In \cref{sec:approaches2NeuralBRDF}, we describe the neural BRDF models considered in this work. \cref{sec:angleAndEncoding} describes the angular parametrization of the directions and the input encodings used for the neural networks. Moreover, we propose two extensions for existing approaches to ensure reciprocity by construction and to enhance architectures that use an additive split of diffuse and specular parts in \cref{sec:reciMapping} and \ref{sec:enhancingAddSplit}.


\subsection{Approaches to Neural BRDF Modelling}
\label{sec:approaches2NeuralBRDF}
Since several ideas of previous methods are similar in spirit but implemented with slightly different architectures, we did our best to tune one architecture for each idea to yield optimal results for our data. Note that we focus on the actual BRDF representation; therefore, we do not include parts of the models concerned with the geometry estimation, like \eg additional regularizers.


\subsubsection{Neural BRDFs Based on Parametric Models}
This class of neural BRDFs is based on physically-based BRDF models to represent the reflective behavior and is used in several previous works \cite{bi2020neuralReflectanceFields,srinivasan2021nerv,Boss2021NERD,Zhang22IRON,Deschaintre2018SingleImageSVBRDFCaptureDeepNN,Henzler2021GenerativeModellingBRDFFlashIms,Guo2020MaterialGAN,Zhang21PhySG,Zhang2022ModellingIndirIlluminationInvRendering,brahimi24SparseViewsNearLight,Brahimi24SuperVol}. A neural network predicts the (spatially varying) parameters of the reflection model, which are then combined with the viewing and light direction to compute the BRDF values using the analytical formula of the parametric model. See \cref{fig:architectures} for a visualization.

In this work, we evaluate parametric neural BRDFs based on the \tslong (\ts) model~\cite{torrance1967theory} 
and the isotropic variant of
the \disney BRDF \cite{burley2012physically} as state-of-the-art parametric models. For reference, we also evaluate the energy-preserving variant of the Phong BRDF (\rp) \cite{lafortune1994using}. We refer to the appendix for more details on the models.

Finally, we compare against the recently proposed \fmbrdf model \cite{ichikawa2023fresnel}, which addresses shortcomings in the Oren-Nayar model. 
Since the normalization estimation for this model's normal distribution is computationally intractable for the spatially varying data, we estimate one single normalization per object. At the same time, all the other parameters depend on the position.
Note that for the semi-synthetic MERL-based data, this makes the estimation less complex since the material is uniform over the mesh.

We use an MLP with 6 layers of width 128 and a skip connection to the third layer to predict the parameters for all parametric models. We use ReLU nonlinearities between the layers and a sigmoid or a softplus output nonlinearity, depending on the range of the respective parameter. For the Disney and the Microfacet model, we found that the convergence is more stable when the output for the roughness is scaled by 0.5 before the last sigmoid function.


\subsubsection{Purely Neural BRDFs}
In contrast to the models in the previous section, purely neural models do not rely on a parametric model but directly predict the BRDF value using neural networks.


\paragraph{Single MLP}
Several previous works employ a single MLP to predict the BRDF value from the position $x$ and the light and view direction $l$ and $v$ directly \cite{sztrajman2021neural,Fan2022NeuralLayeredBRDF,Zeng23RelightingNeRFsWithShadowAndHighlightHints}. The adapted architecture for our experiments is shown in \cref{fig:architectures}. We map the directions to the Rusinkiewicz angles as described in \cref{sec:angleAndEncoding} and employ a softplus output nonlinearity to ensure the positivity of the resulting BRDF value. Again, we use a 6-layer MLP of width 128 with ReLU activations and an input skip to layer 3.


\paragraph{Additive Split}
Other works predict diffuse and specular reflections, which are additively combined into the final BRDF value. NeRFactor \cite{Zhang2021NeRFactor} uses two separate MLPs, which we adapt as \emph{Additive Separate}, see \cref{fig:architectures}. We remove their albedo clamping and use a 3D RGB specular part instead of the proposed scalar term since both yielded worse results. See the appendix for details and the experimental evaluation. We use a per-object specular part instead of the pre-trained module to facilitate a fair comparison between all methods without the need for pre-training.
For both MLPs, we use 4 layers of width 128 and a skip connection to the second layer, following the original work. For the diffuse MLP, we use a sigmoid output and divide by $\pi$ to obtain the Lambertian diffuse term. For the specular MLP, a softplus output ensures positivity.

Instead of two separate MLPs, LitNeRF \cite{Sarkar23LitNerf} employs a shared spatial MLP that extracts common features that are then used by a diffuse and specular head. We adopt this architecture as \emph{Additive Shared}, see \cref{fig:architectures}. We use 5 layers for the shared ReLU network of width 128 with a skip connection to the third layer, a single layer of width 128 for the diffuse MLP and two layers of width 128 for the specular output. Again, a sigmoid and a softplus output are used for the diffuse and the specular head, respectively. 
Both additive architectures use the angle parametrization introduced in the next section.


\subsection{Angle Parametrization and Input Encodings}
\label{sec:angleAndEncoding}
Following previous work \cite{Zhang2021NeRFactor, sztrajman2021neural}, we map the view and the light direction to the Rusinkiewicz angles \cite{rusinkiewicz1998new} before feeding them into the network. The most important advantage of this reparametrization is that the specular peaks align with the coordinate axes.  For an isotropic BRDF, the Rusinkiewicz angles read $(\theta_h,\theta_d,\phi_d)\in[0,\frac{\pi}{2}]^2\times[0, 2\pi]$. Please see the appendix for a more detailed discussion of the angle parameterizations and an experimental comparison to the common view-light angles.

Previous work has shown that vanilla MLPs have difficulties representing high-frequency data. Therefore, applying some encoding function to the input is common practice.
Since we parametrize the BRDF directly on the mesh, we encode the spatial position with the intrinsic encoding for neural fields on manifolds proposed by Koestler \etal \cite{KoestlerIntrinsicNeuralFields22}, which has been shown to be advantageous compared to common extrinsic encodings. We use positional encoding \cite{Mildenhall20Nerf} for the Rusinkiewicz angles. For more details on the encodings, we refer to the appendix.


\subsection{A Novel Mapping to Ensure Reciprocity}
\label{sec:reciMapping}
While the neural BRDFs based on parametric models fulfill the reciprocity constraint in \cref{eq:brdf_reciprocity} by construction, LitNeRF \cite{Sarkar23LitNerf} is the only purely neural approach that aims at fulfilling this constraint. During training, they randomly swap view and light direction to force the model to treat them interchangeably. However, this is a soft constraint, and it is not guaranteed that the reciprocity is fulfilled.
In contrast, we propose a mapping of the Rusinkiewicz angles that ensures that 
\cref{eq:brdf_reciprocity}
is fulfilled exactly by construction.

For an isotropic BRDF and the Rusinkiewicz angles, the reciprocity condition reads
\begin{equation}
    \brdf(\point,\theta_h,\theta_d,\phi_d) = \brdf(\point, \theta_h,\theta_d,\phi_d + \pi).
\end{equation}
We exploit this and map the Rusinkiewicz angles to
\begin{equation}
    \Theta = [\theta_h,\theta_d,\phi_{d,\pi},\phi_{d,\pi}+\pi], \label{eq:rusinkiewicz_reciprocity_mapping}
\end{equation}
where $\phi_{d,\pi}$ is short notation for $\phi_d$ modulo $\pi$ and $\text{range}(\Theta) = [0,\frac{\pi}{2}]^2\times\left[0, \pi\right]\times[\pi,2\pi]$. We now have
\begin{equation}
    \Theta(\theta_h,\theta_d,\phi_d) = \Theta(\theta_h,\theta_d,\phi_d + \pi).
\end{equation}
Hence, 
if we use $\Theta$ as input to the downstream blocks, the reciprocity constraint in \cref{eq:brdf_reciprocity} is fulfilled by construction.


\subsection{Enhancing the Additive Split}
\label{sec:enhancingAddSplit}
The existing purely neural models based on an additive split do not consider that light reflected at the surface cannot be used for subsurface scattering.
We propose a novel extension to additive split approaches that can model this phenomenon. Besides the specular reflection $f_\text{s}(x, l, v)$, we use the specular MLP to predict a weight $\xi(x, l, v)\in [0, 1]^3$, which we use to reduce the diffuse part $f_\text{d}(x)$ of the BRDF channel-wise. The resulting additive split reads
\begin{equation}
    f(x, l, v) = (1 - \xi(x, l, v)) \circ f_\text{d}(x) + f_\text{s}(x, l, v).
\end{equation}

Since the weight $\xi$ makes the diffuse summand view-dependent, which increases the ambiguity, 
we add two regularizers. We use an L1 loss between the diffuse part and the reference image to encourage the model to represent as much as possible with the diffuse part, and moreover, we use an L1 regularizer on the specular part to encourage sparsity. Please see the appendix for more details.