\section{Training and Evaluation}
\label{sec:supp:training_and_eval}

This section contains additional details on the training and the evaluation. \cref{sec:supp_loss_formulation} sheds more light on the gamma-corrected loss formulation. \cref{sec:supp:pre_proc_data} contains insight into the preprocessing of the DiLiGenT-MV dataset. \cref{sec:supp_lbo_params} presents the encoding parameters used for the training. \cref{sec:supp:regularizers_enhanced} gives details on the regularizers employed for the enhancement of the additional split. The excluded values for the computation of the RMSE are discussed in \cref{sec:supp:rmse}. Finally, \cref{sec:supp:mc_integration} presents details on the Monte Carlo integration for the experiments on the energy conservation.

\subsection{Loss Formulation}
\label{sec:supp_loss_formulation}

To avoid a dominant influence of the bright regions on the loss, we use a gamma mapping to transform the RGB values from linear to sRGB space, as described in
\iftoggle{arxiv}{\cref{eq:loss}}{Eq. (10)}
the main paper. We repeat the loss formulation here for convenience.
\begin{equation}
    \mathcal{L} = \frac{1}{N}\sum_{i=1}^N (\gamma(L_o(x, v)) - \gamma(L_{GT}(x, v))^2
\end{equation}

We use the following standard formula for the gamma mapping $g:[0,1]\rightarrow[0,1],~c_{\mathrm{lin}}\mapsto c_{\mathrm{sRGB}}$ \cite{akenine2019realTimeRendering}:
\begin{equation}
    g(c_{\mathrm{lin}}) = \begin{cases}
        \frac{323}{25}~c_{\mathrm{lin}} & \mathrm{if}~c_{\mathrm{lin}}\leq0.0031308 \\
        \frac{211}{200}~c_{\mathrm{lin}}^\frac{5}{12} - \frac{11}{200} & \mathrm{else }  
    \end{cases}
\end{equation}


\subsection{Pre-Processing of the Data}
\label{sec:supp:pre_proc_data}

The triangle meshes supplied with the DiLiGenT-MV dataset \cite{Li2020DiLiGentMVDataset} have an unreasonably high number of vertices, which causes the computation of the LBO eigenfunctions to take very long. Therefore, we reduce the number of vertices from roughly $10^6$ to about $10^5$. To stay consistent with the simplified mesh, we compute the normals on the mesh rather than using the normal maps included in the dataset. To speed up the training process, we pre-compute and store the LBO eigenfunctions as well as the ray-mesh intersections and the shadow rays.


\subsection{Encoding Parameters}
\label{sec:supp_lbo_params}

For the eigenfunctions of the LBO we use 6 blocks between the $1^\mathrm{st}$ and the $512^\mathrm{th}$ eigenfunction. We use 64 eigenfunctions for the first block and follow up with 6 evenly spaced blocks of 16 eigenfunctions. For the positional encoding of the angles, we use 3 encoding frequencies.


\subsection{Regularizers for the Enhanced Splitting}
\label{sec:supp:regularizers_enhanced}

While the enhancement described in
\iftoggle{arxiv}{\cref{sec:enhancingAddSplit}}{Sec. 4.4}
in the main text shows improvements for both additive architectures, it also introduces additional ambiguity. The weight $\xi(x, l, v)$ makes the diffuse summand dependent on the directions, and therefore, this term can now potentially represent specular behavior as well. To separate the phenomena, we introduce two additional regularizers. 

First, we employ an L1 loss between the raw diffuse part (without the additional weight $\xi$) and the ground truth. Again, we use the gamma mapping described in \cref{sec:supp_loss_formulation}.
\begin{equation}
    \mathcal{L}_\mathrm{reg,diff} = \frac{1}{N}\sum_{i=1}^N \|\gamma(f_\text{d}(x)) - \gamma(L_{GT}(x, v))\|_1
\end{equation}
The idea is to encourage the model to represent as much of the appearance as possible by the diffuse term. This will be limited to the non-view-dependent parts of the appearance automatically since the raw diffuse part $f_\text{d}(x)$ is not view-dependent.

Second, we employ an L1 regularizer on the specular part.
\begin{equation}
    \mathcal{L}_\mathrm{reg,spec}=\frac{1}{N}\sum_{i=1}^N \|f_\text{s}(x, l, v)\|_1
\end{equation}
The idea for this term is to encourage the model to represent only those components of the appearance that are actually view-dependent by the specular part. We use a weight of $5\cdot 10^{-4}$ for both terms before adding them to the total loss.

Interestingly, the regularizers do not significantly change the reconstruction quality of the enhanced split; the quantitative evaluation with and without them is almost identical. However, they force a more reasonable split between the diffuse and specular parts. Without them, the model tends to predict extreme mixing colors on the different parts, that combined yield the correct color. The regularizers ensure that realistic results for the diffuse albedo and the specularities are obtained.



\subsection{BRDF-Spcae Metric RMSE$^{\sqrt[3]{}}$}
\label{sec:supp:rmse}

For the semi-synthetic dataset, we utilize the availability of ground truth BRDF data to report a BRDF-space metric on the reconstruction quality. We follow the analysis of Lavou√© \etal who have 
investigated a wide range of metrics and analyzed the correlation with reconstruction quality perceived by humans \cite{Lavoue21PerceptualQualityOfBRDFApproximations}. Their results show that applying the cubic root to the BRDF values before computing a standard \emph{root mean squared error} (RMSE) between the reconstruction and the ground truth yields a metric that correlates well with human perception. We refer to this metric as RMSE$^{\sqrt[3]{}}$.

Another aspect that they found helpful to increase the correlation is to discard BRDF values for grazing angles above $80^{\circ}$, which we also adopt in our analysis. A further reason to discard these values is the observation of Burley, who reported anomalies and extrapolation in the MERL data in that range of angles \cite{burley2012physically}, which the semi-synthetic data inevitably inherits. Moreover, we reject values for saturated pixels since in this case, the image value was clipped during the data creation, and therefore the ground truth value is not a reliable reference. Note that BRDF-space metrics can only be reported for the semi-synthetic data, since the DiLiGenT-MV dataset does not contain ground truth BRDF values.


\subsection{Monte Carlo Integration}
\label{sec:supp:mc_integration}

To approximate the integral in the energy conservation
(\iftoggle{arxiv}{\cref{eq:brdf_energy_conservation}}{Eq. (4)} in the main text)
we use Monte Carlo integration with samples from the cosine-weighted hemisphere sampling. The approximation of the integral reads
\begin{equation}\label{eq:mc_approx_raw}
    \int_{\Hemi} \brdf(\point, \light, \view)\cos{\theta_{\view}} \,\mathrm{d}\view \approx \frac{1}{N_{MC}}\sum_{i=1}^{N_{MC}}\frac{\brdf(\point, \light, \view_i)\cos{\theta_{\view_i}}}{p(\view_i)},
\end{equation}
where $\point$ and $\light$ are the position and the light direction for which the integral is evaluated. $\view_i\sim p(\view_i)$ are the sampled view directions, which are drawn according to the cosine-weighted distribution on the hemisphere, which reads
\begin{equation}
    p(\view_i) = \frac{1}{\pi}\cos{\theta_{\view_i}}.
\end{equation}
Hence, the approximation in \cref{eq:mc_approx_raw} can be simplified to
\begin{equation}
    \int_{\Sphere^2} \brdf(\point, \light, \view)\cos{\theta_{\view}} \,\mathrm{d}\view \approx \frac{\pi}{N_{MC}}\sum_{i=1}^{N_{MC}}\brdf(\point, \light, \view_i).
\end{equation}

To ensure convergence, we sample $N_{MC}=20k$ view directions for each randomly sampled point-light pair.