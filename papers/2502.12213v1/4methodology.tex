\section{Methodology}

\begin{figure*}
    %\vspace{-4mm}
    \begin{center}
    \includegraphics[width=0.99\textwidth]{figure/2024.7.311.pdf}
    \caption{The overview of the proposed framework. MLP: multi-layer perceptron, GCN: graph convolution network.}
    \vspace{-6mm}
    \label{fig:overview}
    \end{center}
\end{figure*}

\model consists of three principal modules, which are introduced as follows. %Our \model first extends road information to spatio-temporal graphs that reference and deeply associate global nodes through dynamic relationship graph learning. Then, to better learn the inherent spatio-temporal information of the sequence data, we introduce a spatio-temporal embedding learning module. Finally, through a simple yet effective method, we combine the spatio-temporal embedding with the sequence data to generate trend-seasonality data, which are further processed through an encoder-decoder network to obtain the expected output.

\subsection{Module 1: Dynamic Relationship Graph Learning}
 %Previous methods mainly refer to graph constructed based on distance, without considering the high-order connectivity between each node. To model higher-order spatio-temporal relationships among all traffic nodes, we construct dynamic relationship graph considering different time steps and nodes. we first utilize a initial normalization and a dynamic relationship graph learning module, which adopts a GCN to explore neighborhood interactions.

 To address the limitations of simple distance-based connectivity, which overlooks the high-order relationships between each node, we construct a dynamic relationship graph that considers different time steps and nodes. This approach allows us to model the complex higher-order spatio-temporal relationships among all traffic nodes effectively.

Inspired by \citet{han2021DMSTGCN}, we design three learnable matrices and a learnable core tensor to streamline the constructing of the dynamic graph. These components include:  a time slot embedding $\mathbf{E}^{t} \in \mathbb{R}^{N_{t}\times D}$, a starting node embedding $\mathbf{E}^{s} \in \mathbb{R}^{N_{s}\times D}$, an ending node embedding $\mathbf{E}^{e} \in \mathbb{R}^{N_{e}\times D}$ and the core tensor $\bm{\mathcal{K}} \in \mathbb{R}^{D\times D\times D}$. Here, $N_{t}, N_{s}, N_{e}$ and the $D$ denote the number of time slots, starting nodes, ending nodes and the dimension of the embeddings, respectively. The specific calculations are as follows:
\begin{equation}
    \begin{aligned}
        \mathbf{A}_{t,i,j}^{\prime}&=\sum_{o=1}^D\sum_{q=1}^D\sum_{r=1}^D\bm{\mathcal{K}}_{o,q,r}\mathbf{E}_{t,o}^t\mathbf{E}_{i,q}^e{\mathbf{E}_{j,r}^s},\\
        \mathbf{A}_{t,i,j}^{\prime\prime}&=\max(0,\mathbf{A}_{t,i,j}^{\prime}),\\
        \mathbf{A}_{t,i,j}&=\frac{e^{\mathbf{A}_{t,i,j}^{\prime\prime}}}{\sum_{n=1}^{N_s}e^{\mathbf{A}_{t,i,n}^{\prime\prime}}}.
    \end{aligned}
\end{equation}

Through this methodology, we derive a tensor $\bm{\mathcal{A}} \in \mathbb{R}^{N_{t}\times N_{s}\times N_{e}}$ that encapsulates the high-order connectivity among global nodes across various time steps. Notably, since the starting nodes can also serve as ending nodes, we designate  $N = N_{s} = N_{e} $ for simplification.

Subsequently, to establish and analyze the relationships between different nodes, we employ a dynamic graph convolution. This process involves the matrix multiplication of the constructed adjacency matrix, the hidden states of the nodes, and the learnable parameters across different graphs at various time. The hidden states of the nodes are updated by aggregating the hidden states of their neighbors through weighted links at each time step:
\begin{equation}
    \bm{\mathcal{H}}_L=\sum\limits_{l=0}^L\left(\mathbf{A}_{(t)}\right)^l\mathbf{H}_{l}^t\mathbf{W}_l,
\end{equation}
where $\mathbf{A}_{(t)}$ denotes the adjacency matrix extracted from the tensor $\bm{\mathcal{A}}$ at time slot $t$. The term $(\mathbf{A}_{(t)})^l$ refers to the matrix multiplication operation. $\mathbf{H}_l^t$ represents the output hidden states at the $l$-th layer, which serves as input for the dynamic graph convolution in the $l$+1-th layer, $\mathbf{W}_l$ are the parameters specific to the $l$-th layer, and $\mathbf{H}_0^1$ represents the normalized traffic sequence data at first time step. Consequently, through these operations, we generate the final output $\bm{\mathcal{H}}_L \in \mathbb{R}^{T\times N\times D}$ which constitutes the processed sequence data.

% After convolution, we obtain the processed series data $\bm{\mathcal{\bar{X}}}$.
%After the convolution, we can get the series data from $\bm{\mathcal{X}}$ to $\bm{\mathcal{Y}}$ $\in$ $\mathbb{R}^{T\times N\times C}$.

\subsection{Module 2: Spatio-Temporal Embeddings Learning}
To more effectively capture the temporal correlations and periodicities in traffic flow, we design a temporal context embedding learning module. Given the time of day, represented as $\mathbf{z}^{d}_{h}$ $\in$ $\mathbb{R}^{T}$ and the day of the week, represented as $\mathbf{z}^{w}_{h}$ $\in$ $\mathbb{R}^{T}$, we extract temporal features and encode them by one-hot. By concatenating these encoded features (denoted by [,]), we generate an initial temporal embedding as below:
\begin{equation}
    \mathbf{Z}^{t}_{h}=\sigma(\mathbf{W}[onehot(\mathbf{z}^{d}_{h}), onehot(\mathbf{z}^{w}_{h})]),
    \label{eq:SEmbedding_1}
\end{equation}
where $\mathbf{Z}^{t}_{h}$ $\in$ $\mathbb{R}^{T\times D}$, $\sigma$ denotes the ReLU activation function. $\mathbf{W}$ comprises the trainable parameters. $\textit{D}$ specifies the dimensionality of the temporal embedding.

To enhance the ability of the temporal embedding to learn multi-resolution temporal features and to facilitate its integration with traffic time series, we refine the initial temporal embedding using a bias-free MLP:
\begin{equation}
    \mathbf{M}^{t}_{h}=\sigma_{2}(\mathbf{W}_{2}\sigma_{1}(\mathbf{W}_{1}\mathbf{Z}^{t})),
    \label{eq:SEmbedding_2}
\end{equation}
where $\sigma_{1}$ denotes the ReLU activation function, $\sigma_{2}$ is the sigmoid activation function, and $\mathbf{W}_{1}$, $\mathbf{W}_{2}$ are the trainable parameters. The resulting temporal embedding $\mathbf{M}^{t}_{h}$ $\in$ $\mathbb{R}^{T\times D}$ is utilized in subsequent stages of the model.

To effectively model the structure of the road network, we utilize the normalized Laplacian matrix, defined as:
\begin{equation}
    \Delta = \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{A} \mathbf{D}^{-1/2},
\end{equation}
where $\mathbf{A}$ $\in$ $\mathbb{R}^{N\times N}$ is the adjacency matrix, $\mathbf{D}$ is the degree matrix, and $\mathbf{I}$ is the identity matrix. The Laplacian matrix's eigenvalues and eigenvectors encapsulate the spatial graph information in Euclidean space. By decomposing the Laplacian, we obtain the eigenvalue matrix and the eigenvector matrix:
\begin{equation}
    \Delta = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\top},
\end{equation}
where $\mathbf{U}$ is the matrix of eigenvector  and $\mathbf{\Lambda}$ is the matrix of eigenvalue. To construct an initial spatial embedding, we select the $\textit{k}_{r}$ smallest nontrivial eigenvectors, resulting in $\mathbf{Z}^{s}$ $\in$ $\mathbb{R}^{N\times \textit{k}_{r}}$.
To capture global information as comprehensively as possible, we set $\textit{k}_{r}$ to 32.

To enhance the spatial embedding to represent spatial features and facilitate its integration with subsequent traffic time series, we process the initial spatial embedding using a bias-free MLP:
\begin{equation}
    \mathbf{M}^{s} = \mathbf{W}_{2}\sigma(\mathbf{W}_{1}\mathbf{Z}^{s}),
\end{equation}
where $\sigma$ is the ReLU activation function, $\textit{D}$ is the spatial embedding dimension, and $\mathbf{W}_1$, $\mathbf{W}_2$ are the trainable parameters. $\mathbf{M}^{s}$ $\in$ $\mathbb{R}^{N\times D}$ represents the spatial embedding that will be utilized in sebsequent stages of the model.

%It is noted that previous works merely coarsely input the initial temporal and spatial embedding vectors into the model \cite{jiang2023pdformer, li2024flashst} without effectively integrating the temporal and spatial embeddings with the sequence data. 
To effectively integrate the spatio-temporal embeddings with the traffic sequence data, we further propose a refined spatial-temporal embedding learning module. Specifically, this module involves broadcasting the spatial embedding $\mathbf{M}^{s}$ across the dimension $T$ to produce $\bm{\mathcal{M}}^{s} \in \mathbb{R}^{T\times N\times D}$, and similarly broadcasting the temporal embedding  $\mathbf{M}^{t}_{h}$ across the dimension $N$  to yield $\bm{\mathcal{M}}^{t}_{h} \in \mathbb{R}^{T\times N\times D}$. These embeddings are then combined as follows:
\begin{equation}
    \bm{\mathcal{M}} = \sigma_{1}(\bm{\mathcal{M}}^{t}_{h}) + \sigma_{2}(\bm{\mathcal{M}}^{s}),
    \label{eq:embeddings}
\end{equation}
where $\bm{\mathcal{M}}$ $\in$ $\mathbb{R}^{T\times N\times D}$, $\sigma_{1}$ represents the mathematical sine function, $\sigma_{2}$ denotes the ReLU activation function. 

In this manner, we obtain the refined spatio-temporal information embeddings for each node at different times. The use of the sine function is particularly beneficial as it not only normalizes the data to a range between -1 and 1 but also enables the model to effectively capture non-linear changes. Specifically, when we generate the initial temporal embedding $\mathbf{M}^{t}_{h}$, we employ the sigmoid activation function, as shown in equation \ref{eq:SEmbedding_2}. Consequently, after the application of the sine function, the data is normalized to a range between 0 and 1, which is crucial for effectively fusing the embedding with the traffic sequence data in subsequent processing steps.

\subsection{Module 3: Trend-Seasonality Decomposition}
Given the strong correlation between the trend-seasonality of a traffic node and its spatio-temporal context, the Trend-Seasonality Decomposition module is designed based on spatio-temporal embeddings learning. This module effectively disentangles the traffic flow of each node into distinct trend and seasonal components, adjusting the representations to better suit the forecasting task.
% allowing for a precise modeling of traffic dynamics.
%Unlike previous models that directly use a single sequential method \cite{wu2021Autoformer, fang2023STWave} or a mathematical method to model traffic sequence data , we decompose traffic series data into trend-cyclical and seasonal components by the spatio-temporal embeddings. Clearly, the temporal variations of trend-cyclical data and seasonal data are markedly different. Trend-cyclical data exhibits stable and continuous temporal changes, whereas seasonal data shows fluctuating and delay changes. We believe that trend-cyclical data is highly correlated with the temporal and spatial context it resides in. Intuitively, a node exhibits different trend changes at different times, hence seasonal data can be derived from its own spatio-temporal variations.
% Due to the fluctuation of spatial information in traffic sequence data, the trend changes between nodes are also different. 
% Through the spatio-temporal embeddings learning, we obtain trend changes for different regions at different time. 

When processing the traffic sequence data with trend-seasonality decomposition, since the dimensionality of the traffic hidden states and the spatio-temporal embeddings has been aligned, we initially derive the trend component by through multiplication-wise interaction between the hidden states of the nodes $\bm{\mathcal{H}}_L$ and the spatio-temporal embeddings $\bm{\mathcal{M}}$. The residual part constitutes the seasonal component. The calculations are as follows:
\begin{equation}
    \begin{aligned}
        \bm{\mathcal{X}}_{t}& = \bm{\mathcal{H}}_L \odot \bm{\mathcal{M}}, \\ 
        \bm{\mathcal{X}}_{s}& = \bm{\mathcal{H}}_L - \bm{\mathcal{X}}_{t},
    \end{aligned}
\end{equation}
where $\bm{\mathcal{X}}_{s}, \bm{\mathcal{X}}_{t} \in \mathbb{R}^{T\times N\times D}$ denote the seasonal and the extracted trend-cyclical part respectively. $\odot$ is the Hadamard product. The trend component of traffic flow is generally smooth and strongly influenced by the time and location of the traffic node. Our approach is based on the mild assumption that nearby times and locations yield similar trends, while distinct times and locations exhibit unique traffic patterns.

\subsection{Encoder-Decoder Architecture}
In this module, we utilize an encoder-decoder network to extract deeper spatio-temporal features. For the encoding process, we employ Gated Recurrent Unit (GRU) due to its effectiveness in capturing temporal dependencies. For the decoding process, we opt for the Transformer architecture, primarily because of its unique multi-head attention mechanism, which generates predictions with superior performance. We denote use the GRU($\cdot$) to denote the Gated Recurrent Unit as below:
\begin{equation}
    \begin{aligned} 
        &\bm{\mathcal{Y}}_{t} = GRU(\bm{\mathcal{X}}_{t}), \\
        &\bm{\mathcal{Y}}_{s} = GRU(\bm{\mathcal{X}}_{s}), 
    \end{aligned}
\end{equation}
where $\bm{\mathcal{Y}}_{s}, \bm{\mathcal{Y}}_{t} \in \mathbb{R}^{T\times N\times D}$ represent the outputs of the GRU encoder for the seasonal and trend components, respectively. These outputs are then combined through element-wise summation to produce the final output $\bm{\mathcal{Y}} \in \mathbb{R}^{T\times N\times D}$.

The Transformer architecture utilizes multi-head attention mechanisms, which first projects the queries, keys and values into $h$ different $d$-dimensional subspaces, and then execute the attention function in parallel:
\begin{equation}
    \begin{aligned}
        \mathrm{MHSA(Q,K,V)}& =\oplus(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^O, \\
        \mathrm{head}_{j}& =\mathrm{softmax}(\frac{(\mathbf{Q}W_{j}^{Q})(\mathbf{K}W_{j}^{K})^{\top}}{\sqrt{d}})(\mathbf{V}W_{j}^{V}), 
    \end{aligned}
\end{equation}
where $W$ is the training parameter.

Inspired by \citet{guo2023SSTBAN}, we incorporate a component known as the Bottleneck Transformer Block (BT block) into our architecture, strategically designed to reduce both temporal and spatial complexity.

For the predicted time of day and day of the week, we obtain the predicted time embedding $\mathbf{M}^{t}_{p}$ $\in$ $\mathbb{R}^{T^\prime \times D}$ according to equation \ref{eq:SEmbedding_1} and equation \ref{eq:SEmbedding_2}. To align with the default settings, we maintain $T = T^\prime$. Additionally, we extend the temporal embedding $\mathbf{M}^{t}_{p}$ across the dimension $ N $, resulting in $\bm{\mathcal{M}}^{t}_{p} \in \mathbb{R}^{T^\prime\times N\times D}$. 

To effectively prompt the decoder to focus on capturing the high-order dynamics between time and space, we concatenate the predicted temporal embedding and spatial embedding with the output from $(l - 1)^{th}$ BT block as $\bm{\mathcal{Z}}^{(l-1)} \in \mathbb{R}^{T^\prime\times N\times D}$ to obtain $\bm{\mathcal{H}}^{(l)}=(\bm{\mathcal{Z}}^{(l-1)}||\bm{\mathcal{M}}^{t}_{p}||\bm{\mathcal{M}}^{s}) \in \mathbb{R}^{T^\prime \times N \times 3D}$. Besides, $\bm{\mathcal{H}}^{(l-1)}_{:,v} \in \mathbb{R}^{T^\prime\times 3D}$ represents the input pertaining to node $v$ across all time slices.

The specific BT block employs defined $T^\prime$ vectors $\mathbf{IT}^{(l)} \in \mathbb{R}^{T^\prime\times 3D}$ as trainable parameters within the model. The transformation within the BT block is described by the following equations:
\begin{equation}
    \begin{aligned}
        \mathbf{IT}^\prime &= \mathrm{MHSA}(\mathbf{IT}^{(l)},\bm{\mathcal{H}}^{(l-1)}_{:,v},\bm{\mathcal{H}}^{(l-1)}_{:,v}) \in \mathbb{R}^{T^\prime\times 3D},\\
       \bm{\mathcal{Z}}^{(l)}_{:,v} &= \mathrm{MHSA}(\bm{\mathcal{H}}^{(l-1)}_{:,v},\mathbf{IT}^\prime,\mathbf{IT}^\prime) \in \mathbb{R}^{T^\prime\times 3D}.
    \end{aligned}
\end{equation}
The initial input to the first BT block is $\bm{\mathcal{Y}}$ sourced from the encoder. The output from the final $l$ BT block is denoted as $\bm{\mathcal{Z}} \in \mathbb{R}^{T^\prime\times N\times D}$, which represents the projected future representation of traffic flow.

Ultimately, to derive the expected traffic predictions, we employ a fully-connected neural network that transforms the future representation of traffic flow into the predicted values $\bm{\mathcal{\hat{X}}} \in \mathbb{R}^{T^\prime\times N\times C}$. We utilize the $L1$ as the training loss function as below:
\begin{equation}
    \mathcal{L} = \sum_{t=T+1}^{T+T^\prime}\sum_{n=1}^{N}\lvert{\hat{x}_{t}^{n}} - y_{t}^{n}\rvert.
    \label{eq:loss}
\end{equation}
