\section{Experiments}
\input{table/datasets.tex}

\subsection{Datasets}

%We utilize three real-world datasets to evaluate the performance of \model. The JiNan dataset is derived from traffic flow statistics in a real city. Similar to the processing in \cite{song2020STSGCNaaai}, we set the time interval to 5 minutes. However, unlike \cite{song2020STSGCNaaai}, our constructed adjacency matrix is not based on distance but on connectivity. Specifically, if two detection points are on the same road, an edge is constructed between them. The remaining two datasets are based on California's highways, which differ geographically from JiNan dataset. The detailed information is shown in Table~\ref{tab.datasets}.

To evaluate the performance of \model, we utilize three real-world datasets, each offering unique traffic flow dynamics. The dataset JiNan, which is first released by us, derived from actual traffic flow statistics in a city, mirrors the setup in \cite{song2020STSGCNaaai}, where the time interval is set to 5 minutes. 
% When we construct the experimental dataset, we establish edges between neighbor detection points if they are connected on the road. The other two datasets PeMS providing a geographical contrast to the JiNan dataset. 
Detailed descriptions of these datasets are provided in Table~\ref{tab.datasets}.

\input{table/perform_compared}

\subsection{Baseline Methods}

To assess the performance of \model, we compare it against a diverse range of established baselines:

\begin{itemize}
    \item \textbf{HA} \cite{hamilton2020HA} uses the historical average 
    of input data for prediction.
    \item \textbf{ARIMA} \cite{box2015ARIMA} is a well-known statistical model widely employed for time series forecasting.
    \item \textbf{VAR} \cite{lutkepohl2005VAR}: is another traditional method for time series forecasting.
    \item \textbf{SVR} \cite{wu2004SVR} employs support vector regression for predictive modeling.
    \item \textbf{LSTM} \cite{graves2012LSTM} is a deep learning model that captures temporal dependencies but does not account for spatial correlations.
    \item \textbf{DCRNN} \cite{li2018DCRNN} integrates diffusion convolution into GRU layers to enhance spatial-temporal correlation capture.
    \item \textbf{STGCN} \cite{yu2017STGCN} combines graph and temporal convolutions to handle spatial-temporal data.
    \item \textbf{GWNet} \cite{wu2019GWNet} combines dilated convolution with diffusion graph convolution and introduces a self-adaptive adjacency matrix.
    \item \textbf{GMAN} \cite{zheng2020GMANaaai} employs multi-attention to capture both spatial and temporal dynamics.
    \item \textbf{ASTGCN} \cite{guo2019ASTGCN} applies attention mechanisms on both temporal and spatial convolutions to dynamically capture spatio-temporal correlations.
    \item \textbf{AGCRN} \cite{bai2020AGCRN} focuses on extracting node-specific features and uncovering hidden node interdependencies.
    % \item \textbf{ASTGNN} \cite{guo2021lASTGNN}: designs trend-aware self-attention module and a dynamic graph convolution module to capture spatialtemporal dynamics.
    \item \textbf{DMSTGCN} \cite{han2021DMSTGCN}  learns dynamic spatial dependencies and builds a multi-faceted fusion module for complex traffic data features.
    \item \textbf{STGODE} \cite{fang2021STGODE} adopts ordinary differential equations for traffic flow forecasting.
    \item \textbf{STGNCDE} \cite{choi2022STGNCDE} designs two neural controlled differential equations for prediction.
    \item \textbf{D\textsuperscript{2}}\textbf{STGNN} \cite{D2STGNN} models traffic flow by separating it into the diffusion component and the inherent component.
    \item \textbf{DSTAGNN} \cite{lan2022dstagnn} constructs a spatio-temporal graph and utilizes multi-head attention to represent dynamic spatial relevance.
    \item \textbf{SSTBAN} \cite{guo2023SSTBAN} implements a self-supervised learning approach with a masking method for prediction.
    \item \textbf{STWave} \cite{fang2023STWave} employs wavelets to decompose traffic data into stable trends and fluctuating events.
\end{itemize}

\subsection{Evaluation Metrics and Experimental Settings}
In our evaluation,  we employ the mean absolute error (MAE), root mean square error (RMSE) and mean absolute percentage error (MAPE) to quantify the performance of different methods. 

Our experiments are conducted on a server with NVIDIA RTX 4090 GPU cards, running CUDA version 12.2. All the models are implemented using PyTorch.
The datasets are split in a 6:2:2 ratio for training, validation, and testing, respectively. We use historical data from the past hour to predict the traffic flow for the next hour, corresponding to using the past 12 time steps to forecast the next 12 steps. 


To prevent overfitting, an early-stopping strategy is employed with a patience setting of 10. We use Adam optimizer with an initial learning rate of 0.001. The standard batch size for all experiments is set to 64. If GPU memory constraints occur, the batch size is reduced to 32, and further to 16 if necessary, until the programs can run efficiently. The number of dimensions of node attribute on three datasets is $C$ = 1. Totally, there are 3 hyperparameters in our model, \ie, the numbers of bottleneck transformer block $L$, the number of attention heads $h$, and the dimensionality $d$ of each attention head, where the total number of features $D = h \times d$. The optimal settings for our model  on PeMS04 and JiNan datasets are $L = 2$, $h = 8$, $d = 16$ ($D = 128$). For the PeMS07 dataset, the best performance is achieved with $L = 2$, $h = 8$, $d = 12$ ($D = 96$). All source code and data are available at \url{https://github.com/roarer008/STDN}

%6:2:2 超参设置，比如d,k,l,adam,batch_size,GPU,learning rate
\subsection{Performance Comparison}
Table~\ref{tab:perform_compared} presents the results from graph-based baselines and grid-based baselines. The best results are highlighted in bold, and the second-best results are underlined. Based on these results, several key conclusions can be drawn:
\begin{itemize}
    \item \model achieves state-of-the-art performance, particularly evident in the PeMS04 and JiNan datasets. Traditional machine learning methods such as ARIMA typically perform poorly, as they are unable to capture the non-linear correlations present in the spatio-temporal traffic data.%With the exception of the MAPE metric, \model  surpasses other baselines on the PeMS07 dataset. %We posit that GRU is more beneficial for less-nodes datasets. 
    
    %\item Traditional machine learning methods such as ARIMA typically perform poorly, as they are unable to capture the non-linear correlations present in the spatio-temporal traffic data.
    
    \item  Among the GCN-based models, AGCRN demonstrates strong performance. Compared to other models, \model excels in capturing the structure of the road network by effectively integrating eigenvalues from the Laplacian matrix with traffic flow data. 
    %Besides that, attention-based models generally perform near optimally among all baselines. Notably, SSTBAN demonstrates strong performance.
    \item Attention-based models generally perform near optimally among all baselines. Notably, \model distinguishes itself by integrating spatio-temporal embeddings with traffic flow data and the decoder, significantly enhancing traffic forecasting accuracy.
    \item Compared to the baseline models, we incorporate multi-resolution temporal features, such as "time of day" and "day of week", for temporal embeddings, alongside a geospatial directed graph for spatial embeddings. Based on these spatiotemporal embeddings, traffic flow is disentangled into trend and seasonality parts. This novel disentangling method significantly enhances the traffic forecasting accuracy.
    
\end{itemize}
% (1) \model achieves state-of-the-art performances and the advantages are more evident in the PeMS04 and JiNan datasets. We argue that GRU is more beneficial for less-nodes datasets. (2) Among the GCN-based models, AGCRN performs well. Compared to other GCN-based models, \model effectively captures the road network's structure by efficiently integrating eigenvalues from the Laplacian matrix with traffic series data. (3) Among the attention-based models, GMAN and DSTAGNN are the best baselines. Compared to other attention-based models, \model excels by incorporating both spatial and temporal information in the encoder’s objective for prediction.

\subsection{Ablation Study}

\input{table/ablation}
To evaluate the effectiveness of different components in \model, we conducted the ablation study with several variants of the \model:
\begin{itemize}
    \item \textbf{w/o TE}: This variant removes the temporal embedding modeling, meaning the decoder operates solely with spatial embedding cues.
    \item \textbf{w/o SE}: This variant removes the spatial embedding modeling, meaning the decoder operates solely with temporal embedding cues.
    \item \textbf{w/o STE}: This variant eliminates spatio-temporal embedding, thus the traffic flow is not decomposed into trend-seasonality components. As a result, the decoder does not incorporate any spatio-temporal cues.
    \item \textbf{w/o DRG}: This variant eliminates the dynamic relationship graph learning.
    \item \textbf{w/o STD}: Instead of using spatiotemporal-aware decomposition to disentangle the traffic sequence data, this variant adopts the decomposition method utilized by Autoformer \cite{wu2021Autoformer}.
\end{itemize}
% (1) \textit{w/o TE}: this variant removes the temporal embedding modeling, meaning the decoder operates solely with spatial embedding cues. (2) \textit{w/o SE}: this variant removes the spatial embedding modeling, meaning the decoder operates solely with temporal embedding cues. (3) \textit{w/o STE}: this variant eliminates spatio-temporal embedding, thus the traffic flow is not decomposed into trend-seasonality components. As a result, the decoder does not incorporate any spatio-temporal cues. (4) \textit{w/o DRG}: this variant removes the dynamic relationship graph learning. (5) \textit{w/o STD}: instead of using spatiotemporal-aware decomposition to disentangle the traffic sequence data, this variant adopts the decomposition method utilized by Autoformer \cite{wu2021Autoformer}.

Table~\ref{tab.ablation} presents the comparison of \model and its variants on PeMS04 and JiNan datasets. From this comparison, we can draw several conclusions: (1) The origin \model consistently achieves the best performance relative to its variants, underscoring the effectiveness of its full configuration. (2) The results show that the variant ``\textit{w/o TE}'' generally outperforms ``\textit{w/o SE}'' across most tasks. This suggests that temporal information, particularly periodic information, plays a more critical role than spatial information. (3) The ``\textit{w/o DRG}'' underperforms \model, indicating the importance of the dynamic relationship graph learning module. (4) The performance of ``\textit{w/o STD}'' underlines the necessity of the trend-seasonality decomposition module aware of spatio-temporal embeddings.
% \input{table/ablation}
% Table~\ref{tab.ablation} shows the comparison of these variants on the PeMS04 and JiNan datasets. Based on the results, we can draw the following conclusions: (1) The results show the superiority of SSA over GCN in capturing dynamic and long-range spatial dependencies. (2) PDFormer leads to a large performance improvement over w/o Mask, highlighting the value of using the mask matrices to identify the significant node pairs. In addition, w/o SemSAH and w/o GeoSAH perform worse than PDFormer, indicating that both local and global spatial dependencies are significant for traffic prediction. (3) w/o Delay performs worse than PDFormer because this variant ignores the spatial propagation delay between nodes but considers the spatial message passing as immediate.
\vspace{-2mm}
\subsection{Parameter Sensitivity Study}
\input{figure/ablation}
Figure \ref{fig:abla} illustrates the results of hyper-parameter sensitivity analysis for our \model on PeMS04 and PeMS07 datasets. This study involved varying the number of decoder layers and the number of features in \model, exploring options within the ranges of [1, 2, 3, 4] for layers and [32, 64, 96, 128, 160] for features. From this analysis, we can draw several conclusions: (1) The performance of our model improves with an increasing in the number of decoder layers but stabilizes at 2 layers. (2) Optimal performance is achieved with 128 features on the PeMS04 dataset and 96 features on the PeMS07 dataset. This finding highlights that while increasing the number of features generally enhances the model's capability to represent complex traffic patterns, but excessive features may introduce noise and degrade model performance.


\subsection{Model Efficiency Study}
\begin{figure}
    %\vspace{-5mm}
    \begin{center}
    \includegraphics[width=0.46\textwidth]{figure/time.pdf}
    %\vspace{-4mm}
    \caption{The computational time cost on PeMS04 and JiNan datasets.}
    \vspace{-3mm}
    \label{fig:time}
    \end{center}
\end{figure}
% \input{table/efficiency}
% \input{figure/epoch}
\begin{figure}
    \centering
    \vspace{-1mm}
    \includegraphics[width=0.98\linewidth]{figure/Epoch_time_with_spe.pdf}
    \vspace{-0.5mm}
    \caption{The MAE on the validation part of PeMS04 dataset during the training process.}
    \vspace{-4mm}
    \label{fig:epoch}
\end{figure}
To demonstrate the efficiency of our model, we benchmark \model against DMSTGCN, SSTBAN, and STWave, which have achieved suboptimal results at the PeMS04 and JiNan datasets. Figure~\ref{fig:time} displays the average training time per epoch and inference time for each model. Figure~\ref{fig:epoch} illustrates the MAE curves on the validation part of PeMS04 dataset during the training process. The following observations can be made: (1) \model not only trains faster but also infers quicker than the compared models. (2) \model demonstrates a faster convergence rate, achieving better performance in fewer epochs. In our experiments, while STWave reaches its best performance at epoch 100, its MAE is still higher than the lowest MAE achieved by our \model at just epoch 19. 

The computational complexity of our \model encoder-decoder module is $O(TD + LND)$, with the encoder and decoder contributing complexities of $O(TD)$ and $O(LND)$, respectively. Here, $L$ denotes the number of bottleneck transformer blocks. The complexity of the spatio-temporal embedding module is given by $O((T + N)D + N^3)$. Although calculating the eigenvectors and eigenvalues of the graph Laplacian is computationally intensive, marked by a complexity of  $O(N^3)$, this process can be efficiently handled through preprocessing prior to training. Therefore, \model maintains comparable time and memory complexity during training, ensuring efficiency without compromising performance.