@inproceedings{Dreseler2020,
abstract = {TPC-H continues to be the most widely used benchmark for relational OLAP systems. It poses a number of challenges, also known as "choke points", which database systems have to solve in order to achieve good benchmark results. Examples include joins across multiple tables, correlated subqueries, and correlations within the TPC-H data set. Knowing the impact of such optimizations helps in developing optimizers as well as in interpreting TPC-H results across database systems. This paper provides a systematic analysis of choke points and their optimizations. It complements previous work on TPC-H choke points by providing a quantitative discussion of their relevance. It focuses on eleven choke points where the optimizations are beneficial independently of the database system. Of these, the attening of subqueries and the placement of predicates have the biggest impact. Three queries (Q2, Q17, and Q21) are strongly in uenced by the choice of an efficient query plan; three others (Q1, Q13, and Q18) are less in uenced by plan optimizations and more dependent on an efficient execution engine.},
author = {Dreseler, Markus and Boissier, Martin and Rabl, Tilmann and Uflacker, Matthias},
booktitle = {Proceedings of the VLDB Endowment},
doi = {10.14778/3389133.3389138},
file = {:Users/keichi/Papers/Dreseler et al. - 2020 - Quantifying TPC-H choke points and their optimizations.pdf:pdf},
issn = {21508097},
mendeley-groups = {CLOSER 2025},
number = {8},
pages = {1206--1220},
title = {{Quantifying TPC-H choke points and their optimizations}},
volume = {13},
year = {2020}
}
@article{PhilipChen2014,
abstract = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore's Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing. {\textcopyright} 2014 Elsevier Inc. All rights reserved.},
author = {{Philip Chen}, C. L. and Zhang, Chun Yang},
doi = {10.1016/j.ins.2014.01.015},
file = {:Users/keichi/Papers/Philip Chen, Zhang - 2014 - Data-intensive applications, challenges, techniques and technologies A survey on Big Data.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Big Data,Cloud computing,Data-intensive computing,Parallel and distributed computing,e-Science},
mendeley-groups = {CLOSER 2025},
pages = {314--347},
publisher = {Elsevier Inc.},
title = {{Data-intensive applications, challenges, techniques and technologies: A survey on Big Data}},
url = {http://dx.doi.org/10.1016/j.ins.2014.01.015},
volume = {275},
year = {2014}
}
@incollection{Date2023,
abstract = {The Cybermedia Center at Osaka University started the operation of a supercomputing system named Supercomputer for Quest to Unsolved Interdisciplinary Datascience (SQUID) in May 2021. SQUID is a hybrid supercomputing system composed of three kinds of heterogeneous compute nodes and delivers 16.591 PFlops as the theoretical performance. This paper overviews the architecture and structure of SQUID and then explains the five challenges which we have set in designing SQUID: Tailor-made computing, HPC and HPDA integration, Cloud-interlinked and -synergized, Secure computing environment, and Data aggregation environment. After that, the future issues to be tackled through the actual operation of SQUID are described.},
author = {Date, Susumu and Kido, Yoshiyuki and Katsuura, Yuki and Teramae, Yuki and Kigoshi, Shinichiro},
booktitle = {Sustained Simulation Performance 2021},
doi = {10.1007/978-3-031-18046-0_1},
file = {:Users/keichi/Papers/Date et al. - 2023 - Supercomputer for Quest to Unsolved Interdisciplinary Datascience (SQUID) and its Five Challenges.pdf:pdf},
isbn = {9783031180460},
mendeley-groups = {Tsunami,CLOSER 2025},
pages = {1--19},
title = {{Supercomputer for Quest to Unsolved Interdisciplinary Datascience (SQUID) and its Five Challenges}},
year = {2023}
}
@techreport{Gross2020,
author = {Gross, Jesse and Ganga, Ilango and Sridhar, T.},
doi = {10.17487/RFC8926},
editor = {Gross, J. and Ganga, I. and Sridhar, T.},
mendeley-groups = {CLOSER 2025},
month = {nov},
title = {{Geneve: Generic Network Virtualization Encapsulation}},
url = {https://www.rfc-editor.org/info/rfc8926},
year = {2020}
}
@article{Stewart2016,
abstract = {Jetstream is a first-of-a-kind system for the NSF - a distributed production cloud resource. The NSF awarded funds to create Jetstream in November 2014. Here we review the purpose for creating Jetstream, present the acceptance test results that define Jetstream's key characteristics, describe our experiences in standing up an OpenStack-based cloud environment, and share some of the early scientific results that have been obtained by researchers and students using this system. Jetstream offers unique capability within the XSEDE-supported US national cyberinfrastructure, delivering interactive virtual machines (VMs) via the Atmosphere interface developed by the University of Arizona. As a multi-region deployment that operates as a single integrated system, Jetstream is proving effective in supporting modes and disciplines of research traditionally underrepresented on larger XSEDE-supported clusters and supercomputers. Already, researchers in biology, network science, economics, earth science, and computer science have used Jetstream to perform research -much of it research in the "long tail of science.".},
author = {Stewart, Craig A. and Fischer, Jeremy and Merchant, Nirav and Stanzione, Daniel C. and Hancock, David Y. and Cockerill, Tim and Miller, Therese and Taylor, James and Vaughn, Matthew and Liming, Lee and Lowe, John Michael and Skidmore, Edwin},
doi = {10.1145/2949550.2949639},
file = {:Users/keichi/Papers/Stewart et al. - 2016 - Jetstream – performance , early experiences , and early results.pdf:pdf},
isbn = {9781450347556},
journal = {ACM International Conference Proceeding Series},
keywords = {Cloud computing,Long tail of science,National Science Foundation,OpenStack,Virtual machines},
mendeley-groups = {CLOSER 2025},
title = {{Jetstream - Performance, early experiences, and early results}},
volume = {17-21-July-2016},
year = {2016}
}
@article{Towns2014,
abstract = {Computing in science and engineering is now ubiquitous: digital technologies underpin, accelerate, and enable new, even transformational, research in all domains. Access to an array of integrated and well-supported high-end digital services is critical for the advancement of knowledge. Driven by community needs, the Extreme Science and Engineering Discovery Environment (XSEDE) project substantially enhances the productivity of a growing community of scholars, researchers, and engineers (collectively referred to as 'scientists" throughout this article) through access to advanced digital services that support open research. XSEDE's integrated, comprehensive suite of advanced digital services federates with other high-end facilities and with campus-based resources, serving as the foundation for a national e-science infrastructure ecosystem. XSEDE's e-science infrastructure has tremendous potential for enabling new advancements in research and education. XSEDE's vision is a world of digitally enabled scholars, researchers, and engineers participating in multidisciplinary collaborations to tackle society's grand challenges.},
author = {Towns, John and Cockerill, Timothy and Dahan, Maytal and Foster, Ian and Gaither, Kelly and Grimshaw, Andrew and Hazlewood, Victor and Lathrop, Scott and Lifka, Dave and Peterson, Gregory D. and Roskies, Ralph and Scott, J. Ray and Wilkens-Diehr, Nancy},
doi = {10.1109/MCSE.2014.80},
file = {:Users/keichi/Papers/Towns et al. - 2014 - XSEDE Accelerating scientific discovery.pdf:pdf},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {HPC,cyberinfrastructure,distributed computing,distributed virtual organizations,research infrastructures,scientific computing},
mendeley-groups = {CLOSER 2025},
number = {5},
pages = {62--74},
title = {{XSEDE: Accelerating scientific discovery}},
volume = {16},
year = {2014}
}
@article{Stewart2015,
abstract = {Jetstream will be the first production cloud resource supporting general science and engineering research within the XD ecosystem. In this report we describe the motivation for proposing Jetstream, the configuration of the Jetstream system as funded by the NSF, the team that is implementing Jetstream, and the communities we expect to use this new system. Our hope and plan is that Jetstream, which will become available for production use in 2016, will aid thousands of researchers who need modest amounts of computing power interactively. The implementation of Jetstream should increase the size and disciplinary diversity of the US research community that makes use of the resources of the XD ecosystem.},
author = {Stewart, Craig A. and Hancock, David and Stanzioneb, Daniel and Turnerd, George and Cockerill, Timothy M. and Merchant, Nirav and Taylor, James and Vaughn, Matthew and Foster, Ian and Skidmore, Edwin and Tuecke, Steven and Gaffney, Niall I.},
doi = {10.1145/2792745.2792774},
file = {:Users/keichi/Papers/Stewart et al. - 2015 - Jetstream A self-provisioned, scalable science and engineering cloud environment.pdf:pdf},
isbn = {9781450337205},
journal = {ACM International Conference Proceeding Series},
keywords = {Atmosphere,Big data,Cloud computing,Long tail of science},
mendeley-groups = {CLOSER 2025},
title = {{Jetstream: A self-provisioned, scalable science and engineering cloud environment}},
volume = {2015-July},
year = {2015}
}
@inproceedings{Hancock2021,
abstract = {Jetstream2 will be a category I production cloud resource that is part of the National Science Foundation's Innovative HPC Program the project's aim is to accelerate science and engineering by providing "on-demand"programmable infrastructure built around a core system at Indiana University and four regional sites. Jetstream2 is an evolution of the Jetstream platform, which functions primarily as an Infrastructure-as-a-Service cloud the lessons learned in cloud architecture, distributed storage, and container orchestration have inspired changes in both hardware and software for Jetstream2 these lessons have wide implications as institutions converge HPC and cloud technology while building on prior work when deploying their own cloud environments. Jetstream2's next-generation hardware, robust open-source software, and enhanced virtualization will provide a significant platform to further cloud adoption within the US research and education communities.},
address = {New York, NY, USA},
author = {Hancock, David Y. and Fischer, Jeremy and Lowe, John Michael and Snapp-Childs, Winona and Pierce, Marlon and Marru, Suresh and Coulter, J. Eric and Vaughn, Matthew and Beck, Brian and Merchant, Nirav and Skidmore, Edwin and Jacobs, Gwen},
booktitle = {Practice and Experience in Advanced Research Computing},
doi = {10.1145/3437359.3465565},
file = {:Users/keichi/Papers/Hancock et al. - 2021 - Jetstream2 Accelerating cloud computing via Jetstream.pdf:pdf},
isbn = {9781450382922},
keywords = {cloud computing,computer architecture,computer interfaces,containers,orchestration},
mendeley-groups = {CLOSER 2025},
month = {jul},
number = {Ci},
pages = {1--8},
publisher = {ACM},
title = {{Jetstream2: Accelerating cloud computing via Jetstream}},
url = {https://dl.acm.org/doi/10.1145/3437359.3465565},
year = {2021}
}
@inproceedings{Suzumura2022,
abstract = {The growing amount of data and advances in data science have created a need for a new kind of cloud platform that provides users with flexibility, strong security, and the ability to couple with supercomputers and edge devices through high-performance networks. We have built such a nation-wide cloud platform, called "mdx"to meet this need. The mdx platform's virtualization service, jointly operated by 9 national universities and 2 national research institutes in Japan, launched in 2021, and more features are in development. Currently mdx is used by researchers in a wide variety of domains, including materials informatics, geo-spatial information science, life science, astronomical science, economics, social science, and computer science. This paper provides an overview of the mdx platform, details the motivation for its development, reports its current status, and outlines its future plans.},
archivePrefix = {arXiv},
arxivId = {2203.14188},
author = {Suzumura, Toyotaro and Sugiki, Akiyoshi and Takizawa, Hiroyuki and Imakura, Akira and Nakamura, Hiroshi and Taura, Kenjiro and Kudoh, Tomohiro and Hanawa, Toshihiro and Sekiya, Yuji and Kobayashi, Hiroki and Kuga, Yohei and Nakamura, Ryo and Jiang, Renhe and Kawase, Junya and Hanai, Masatoshi and Miyazaki, Hiroshi and Ishizaki, Tsutomu and Shimotoku, Daisuke and Miyamoto, Daisuke and Aida, Kento and Takefusa, Atsuko and Kurimoto, Takashi and Sasayama, Koji and Kitagawa, Naoya and Fujiwara, Ikki and Tanimura, Yusuke and Aoki, Takayuki and Endo, Toshio and Ohshima, Satoshi and Fukazawa, Keiichiro and Date, Susumu and Uchibayashi, Toshihiro},
booktitle = {Proceedings of the 2022 IEEE International Conference on Dependable, Autonomic and Secure Computing, International Conference on Pervasive Intelligence and Computing, International Conference on Cloud and Big Data Computing, International Conference on Cy},
doi = {10.1109/DASC/PiCom/CBDCom/Cy55231.2022.9927975},
eprint = {2203.14188},
file = {:Users/keichi/Papers/Suzumura et al. - 2022 - mdx A Cloud Platform for Supporting Data Science and Cross-Disciplinary Research Collaborations.pdf:pdf},
isbn = {9781665462976},
keywords = {Cloud Computing,Data Science,High-performance Computing,Interdisciplinarity,Internet of Things},
mendeley-groups = {CLOSER 2025},
pages = {1--7},
publisher = {IEEE},
title = {{mdx: A Cloud Platform for Supporting Data Science and Cross-Disciplinary Research Collaborations}},
year = {2022}
}
@article{Sipos2024,
abstract = {This paper presents the approach adopted by the EGI-ACE project for the setup and delivery of Data Spaces for various scientific domains. The work was implemented by members of the EGI e-infrastructure and of several European Research Infrastructures in the context of the European Open Science Cloud programme. Our results are several Data Space services that enable the reuse and exploitation of open, scientific big data for compute intensive use cases. The paper illustrates the EGI-ACE approach through two examples: (1) EMSO ERIC Data Portal for seafloor and water column research and (2) ENES Data Space for climate research.},
author = {Sipos, Gergely and {La Rocca}, Giuseppe and Antonio, Fabrizio and Elia, Donatello and Nassisi, Paola and Fiore, Sandro and Bardaji, Raul and Rodero, Ivan},
doi = {10.12688/openreseurope.17418.1},
file = {:Users/keichi/Papers/Sipos et al. - 2024 - Scientific Data Spaces - Experiences from the EGI-ACE project.pdf:pdf},
issn = {2732-5121},
journal = {Open Research Europe},
keywords = {Data Spaces,EGI Federated Cloud,EMSO ERIC,ENES,European Strategy for Data,GBIF,LOFAR,SeaDataNet},
mendeley-groups = {CLOSER 2025},
month = {jul},
pages = {136},
title = {{Scientific Data Spaces - Experiences from the EGI-ACE project}},
url = {https://open-research-europe.ec.europa.eu/articles/4-136/v1},
volume = {4},
year = {2024}
}
@inproceedings{Li2022,
abstract = {The SPEChpc 2021 suites are application-based benchmarks de- signed to measure performance of modern HPC systems. The bench- marks support MPI, MPI+OpenMP, MPI+OpenMP target offload, MPI+OpenACC and are portable across all major HPC platforms.},
author = {Li, Junjie and Bobyr, Alexander and Boehm, Swen and Brantley, William and Brunst, Holger and Cavelan, Aurelien and Chandrasekaran, Sunita and Cheng, Jimmy and Ciorba, Florina M. and Colgrove, Mathew and Curtis, Tony and Daley, Christopher and Ferrato, Mauricio and {De Souza}, Mayara Gimenes and Hagerty, Nick and Henschel, Robert and Juckeland, Guido and Kelling, Jeffrey and Li, Kelvin and Lieberman, Ron and McMahon, Kevin and Melnichenko, Egor and Neggaz, Mohamed Ayoub and Ono, Hiroshi and Ponder, Carl and Raddatz, Dave and Schueller, Severin and Searles, Robert and Vasilev, Fedor and Vergara, Veronica Melesse and Wang, Bo and Wesarg, Bert and Wienke, Sandra and Zavala, Miguel},
booktitle = {ICPE 2022 - Companion of the 2022 ACM/SPEC International Conference on Performance Engineering},
doi = {10.1145/3491204.3527498},
file = {:Users/keichi/Papers/Li et al. - 2022 - SPEChpc 2021 Benchmark Suites for Modern HPC Systems.pdf:pdf},
isbn = {9781450391597},
keywords = {benchmark,hpc,performance,programming models,spec},
mendeley-groups = {CLOSER 2025},
pages = {15--16},
title = {{SPEChpc 2021 Benchmark Suites for Modern HPC Systems}},
year = {2022}
}
@article{Castro2023,
abstract = {Python has become the prime language for application development in the data science and machine learning domains. However, data scientists are not necessarily experienced programmers. Although Python lets them quickly implement their algorithms, when moving at scale, computation efficiency becomes inevitable. Thus, harnessing high-performance devices such as multi-core processors and graphical processing units to their potential is generally not trivial. The present narrative survey can be thought of as a reference document for such practitioners to help them make their way in the wealth of tools and techniques available for the Python language. Our document revolves around user scenarios, which are meant to cover most situations they may face. We believe that this document may also be of practical use to tool developers, who may use our work to identify potential lacks in existing tools and help them motivate their contributions.},
archivePrefix = {arXiv},
arxivId = {2302.03307},
author = {Castro, Oscar and Bruneau, Pierrick and Sottet, Jean S{\'{e}}bastien and Torregrossa, Dario},
doi = {10.1145/3617588},
eprint = {2302.03307},
file = {:Users/keichi/Papers/Castro et al. - 2023 - Landscape of High-Performance Python to Develop Data Science and Machine Learning Applications.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Additional Key Words and PhrasesPython,code acceleration,data science},
mendeley-groups = {CLOSER 2025},
number = {3},
title = {{Landscape of High-Performance Python to Develop Data Science and Machine Learning Applications}},
volume = {56},
year = {2023}
}
@article{Raschka2020,
abstract = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.},
archivePrefix = {arXiv},
arxivId = {2002.04803},
author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
doi = {10.3390/info11040193},
eprint = {2002.04803},
file = {:Users/keichi/Papers/Raschka, Patterson, Nolet - 2020 - Machine learning in python Main developments and technology trends in data science, machine learning,.pdf:pdf},
issn = {20782489},
journal = {Information (Switzerland)},
keywords = {Data science,Deep learning,GPU computing,Machine learning,Neural networks,Python},
mendeley-groups = {CLOSER 2025},
number = {4},
title = {{Machine learning in python: Main developments and technology trends in data science, machine learning, and artificial intelligence}},
volume = {11},
year = {2020}
}
@article{Fukazawa2024,
abstract = {At the Supercomputer System of Academic Center for Computing and Media Studies Kyoto University, the fourth-generation Xeon (code-named Sapphire Rapids) is employed. The system consists of two subsystems—one equipped solely with high-bandwidth memory, HBM2e, and the other with a large DDR5 memory capacity. Using benchmark applications, a performance evaluation of systems with each type of memory was conducted. Additionally, the study employed a real application, the electromagnetic fluid code, to investigate how application performance varies based on differences in memory characteristics. The results confirm the performance improvement due to the high bandwidth of HBM2e. However, it was also observed that the efficiency is lower when using HBM2e, and the effects of cache memory optimization are relatively minimal.},
author = {Fukazawa, Keiichiro and Takahashi, Riki},
doi = {10.1145/3636480.3637218},
file = {:Users/keichi/Papers/Fukazawa, Takahashi - 2024 - Performance Evaluation of the Fourth-Generation Xeon with Different Memory Characteristics.pdf:pdf},
isbn = {9798400716522},
journal = {ACM International Conference Proceeding Series},
keywords = {DDR5,HBM2e,Sapphire Rapids},
mendeley-groups = {CLOSER 2025},
number = {May},
pages = {55--62},
title = {{Performance Evaluation of the Fourth-Generation Xeon with Different Memory Characteristics}},
year = {2024}
}
@book{Bugnion2017,
abstract = {This book focuses on the core question of the necessary architectural support provided by hardware to efficiently run virtual machines, and of the corresponding design of the hypervisors that run them. Virtualization is still possible when the instruction set architecture lacks such support, but the hypervisor remains more complex and must rely on additional techniques. Despite the focus on architectural support in current architectures, some historical perspective is necessary to appropriately frame the problem. The first half of the book provides the historical perspective of the theoretical framework developed four decades ago by Popek and Goldberg. It also describes earlier systems that enabled virtualization despite the lack of architectural support in hardware. As is often the case, theory defines a necessary-but not sufficient-set of features, and modern architectures are the result of the combination of the theoretical framework with insights derived from practical systems. The second half of the book describes state-of-the-art support for virtualization in both x86-64 and ARM processors. This book includes an in-depth description of the CPU, memory, and I/O virtualization of these two processor architectures, as well as case studies on the Linux/KVM, VMware, and Xen hypervisors. It concludes with a performance comparison ofvirtualization on current-generationx86-and ARM-based systems across multiple hypervisors.},
author = {Bugnion, Edouard and Nieh, Jason and Tsafrir, Dan},
booktitle = {Synthesis Lectures on Computer Architecture},
doi = {10.2200/S00754ED1V01Y201701CAC038},
file = {:Users/keichi/Papers/Bugnion, Nieh, Tsafrir - 2017 - Hardware and Software Support for Virtualization.pdf:pdf},
issn = {1935-3235},
keywords = {Computer architecture,Dynamic binary translation,Hypervisor,Virtual machine,Virtualization},
mendeley-groups = {CLOSER 2025},
number = {1},
pages = {1--206},
publisher = {Morgan \& Claypool},
title = {{Hardware and Software Support for Virtualization}},
url = {http://www.morganclaypool.com/doi/10.2200/S00754ED1V01Y201701CAC038},
volume = {12},
year = {2017}
}
@article{Russell2008,
abstract = {The Linux Kernel currently supports at least 8 distinct virtualization systems: Xen, KVM, VMware's VMI, IBM's System p, IBM's System z, User Mode Linux, lguest and IBM's legacy iSeries. It seems likely that more such systems will appear, and until recently each of these had its own block, network, console and other drivers with varying features and optimizations. The attempt to address this is virtio: a series of efficient, well-maintained Linux drivers which can be adapted for various different hypervisor implementations using a shim layer. This includes a simple extensible feature mechanism for each driver. We also provide an obvious ring buffer transport implementation called vring, which is currently used by KVM and lguest. This has the subtle effect of providing a path of least resistance for any new hypervisors: supporting this efficient transport mechanism will immediately reduce the amount of work which needs to be done. Finally, we provide an implementation which presents the vring transport and device configuration as a PCI device: this means guest operating systems merely need a new PCI driver, and hypervisors need only add vring support to the virtual devices they implement (currently only KVM does this). This paper will describe the virtio API layer as implemented in Linux, then the vring implementation, and finally its embodiment in a PCI device for simple adoption on otherwise fully-virtualized guests. We'll wrap up with some of the preliminary work to integrate this I/O mechanism deeper into the Linux host kernel.},
author = {Russell, Rusty},
doi = {10.1145/1400097.1400108},
file = {:Users/keichi/Papers/Russell - 2008 - Virtio Towards a de-facto standard for virtual IO devices.pdf:pdf},
issn = {01635980},
journal = {Operating Systems Review (ACM)},
keywords = {I/O,KVM,Lguest,Linux,Ring buffer,Virtio,Virtio-pci,Virtualization,Vring},
mendeley-groups = {CLOSER 2025},
number = {5},
pages = {95--103},
title = {{Virtio: Towards a de-facto standard for virtual I/O devices}},
volume = {42},
year = {2008}
}
@inproceedings{Lockwood2014,
abstract = {The demand for virtualization within high-performance computing is rapidly growing as new communities, driven by both new application stacks and new computing modalities, continue to grow and expand. While virtualization has traditionally come with significant penalties in I/O performance that have precluded its use in mainstream large-scale computing environments, new standards such as Single Root I/O Virtualization (SR-IOV) are emerging that promise to diminish the performance gap and make high-performance virtualization possible. To this end, we have evaluated SR-IOV in the context of both virtualized InfiniBand and virtualized 10 gigabit Ethernet (GbE) using micro-benchmarks and real-world applications. We compare the performance of these interconnects on nonvirtualized environments, Amazon's SR-IOV-enabled C3 instances, and our own SR-IOV-enabled InfiniBand cluster and show that SR-IOV significantly reduces the performance losses caused by virtualization. InfiniBand demonstrates less than 2% loss of bandwidth and less than 10% increase in latency when virtualized with SR-IOV. Ethernet also benefits, although less dramatically, when SR-IOV is enabled on Amazon's cloud. Copyright 2014 ACM.},
address = {New York, NY, USA},
author = {Lockwood, Glenn K. and Tatineni, Mahidhar and Wagner, Rick},
booktitle = {Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment},
doi = {10.1145/2616498.2616537},
file = {:Users/keichi/Papers/Lockwood, Tatineni, Wagner - 2014 - SR-IOV Performance Benefits for Virtualized Interconnects.pdf:pdf},
isbn = {9781450328937},
keywords = {Cloud computing,Infiniband,Parallel computing,Virtualization},
mendeley-groups = {CLOSER 2025},
month = {jul},
pages = {1--7},
publisher = {ACM},
title = {{SR-IOV: Performance Benefits for Virtualized Interconnects}},
url = {https://dl.acm.org/doi/10.1145/2616498.2616537},
year = {2014}
}
