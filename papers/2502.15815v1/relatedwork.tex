\section{Related work}
\label{sec:relatedwork}

\iffalse
There is a large number of data sets for mathematical reasoning in mathematics at the high-school or undergraduate level. The majority is focused on formal theorem proving and is written for the corresponding frameworks, like LEAN, and tackles problems from high-school and undergraduate math competitions. To name a few, recently introduced dataset \texttt{PutnamBench} \cite{tsoukalas2024putnambenchevaluatingneuraltheoremprovers} - a collection of formalised theorems from the Putnam competition; \texttt{MiniF2F} \cite{zheng2022minif2fcrosssystembenchmarkformal}, \texttt{FIMO} \cite{liu2023fimochallengeformaldataset} - datasets of the formalized proof problems from the high-school competitions like IMO, AIME and AMC; \texttt{ProofNet} \cite{azerbayev2023proofnetautoformalizingformallyproving} - a collection of natural language and formalized in LEAN 3 theorem statements and proofs at the level of undergraduate mathematics. There are also natural language datasets that include problems of varying difficulty from high-school math, like \cite{hendrycks2021measuringmathematicalproblemsolving}, \cite{cobbe2021trainingverifierssolvemath}. \moritz{mention GPQA diamond} \moritz{recall also the ones from the introduction}

Recently the \textbf{FrontierMath} data set appeared in \cite{glazer2024frontiermath}. It contains novel problems of varying difficulty, from high school level to research level. The data set was carefully curated from working mathematicians instructed to design novel problems. Unfortunately, at the time of writing, relatively few information is available about FrontierMath, such as the distribution of difficulty of the problems claimed to be solved by the o3-mini model. Our TPBench data set is inspired by aspects of FrontierMath, such as its auto-verifiability and meta data. However we make a larger part of our data set public and provide more information about the difficulty of the problems. Our research level problems are below the difficulty of the single research problem made public by FrontierMath, but are typical of problems that constitute part of a research publication in theoretical physics (rather than an entire publication). 

Even more recently the data set \textbf{Humanities last exam} \cite{phan2025humanity} appeared, an industry curated multi-domain data set that contains very hard problems, including some from theoretical physics. In humanities last exam, problems need to have numerical answers or multiple choice. Our verifier allows for somewhat more general outcomes, aiming to check the correctness of symbolic expressions (currently only algebraic expressions are allowed, but we aim to extend our auto-verifier in the future). There are not enough sample problems for humanities last exam to judge to what extent it covers similar problems to ours \moritz{check what is available}. 

Similar to FrontierMath, our new data set TPBench contains problems of different levels of difficulty, from undergraduate to research level. For theoretical physics, to our knowledge, this is the first such data set in the literature. While our problem statistics are still somewhat limited (50 problems compared to several hundred in FrontierMath), we expect to extend the data set in the future. It is important for the theoretical physics research community to have reasoning data sets that are not industry controlled. While we cannot make our data set entirely public, to avoid data leakage into future training data, we look forward to sharing our data set with collaborators with appropriate privacy control. 
\fi


Despite significant advances in the mathematical reasoning capabilities of large language models, accurately solving reasoning problems in specialized domains, such as theoretical physics remains a persistent challenge. In math reasoning, the landscape of existing benchmarks has been instrumental for the evaluation of LLM reasoning capabilities and the development of more robust and interpretable reasoning strategies. We review related benchmarks in Sec.~\ref{sec:related_benchmarks} as well as common strategies for eliciting more accurate reasoning from LLMs in Sec.~\ref{sec:related_methods}.

\subsection{Mathematical Reasoning Benchmarks}
\label{sec:related_benchmarks}
Recent progress in large language models (LLMs) has enabled these models to tackle increasingly complex tasks that demand high-level abstract mathematical reasoning. A significant body of work has focused on datasets for mathematical reasoning at the middle-school (e.g., \citep{cobbe2021training}), high-school (e.g., \citep{hendrycks2021measuring}), or undergraduate level (e.g., \citep{ling2017program}), which often cover arithmetic, geometry, or math word problems. Other benchmarks are focused on theorem proving \citep{tsoukalas2024putnambenchevaluatingneuraltheoremprovers,zheng2022minif2fcrosssystembenchmarkformal,liu2023fimochallengeformaldataset}. For example, the recently introduced \texttt{PutnamBench} \cite{tsoukalas2024putnambenchevaluatingneuraltheoremprovers} provides a collection of formalized theorems from the Putnam competition, while \texttt{MiniF2F} \cite{zheng2022minif2fcrosssystembenchmarkformal} and \texttt{FIMO} \cite{liu2023fimochallengeformaldataset} offer datasets of formalized proof problems drawn from competitions like the IMO, AIME, and AMC. In addition, \texttt{ProofNet} \cite{azerbayev2023proofnetautoformalizingformallyproving} comprises both natural language and formalized theorem statements and proofs at the undergraduate level. Complementary to these are natural language datasets that feature problems of varying difficulty \cite{hendrycks2021measuring, cobbe2021trainingverifierssolvemath}, as well as benchmarks like GPQA diamond \citep{rein2023gpqa}, which are designed to be hard.
Even more recently, the \texttt{Humanity's Last Exam} dataset \cite{phan2025humanity} (HLE) is an industry-curated, multi-domain benchmark that includes very challenging problems, among them some from theoretical physics. However, problems in HLE are constrained to numerical answers or multiple choice formats, there is no spectrum of difficulty, and it is not specifically designed to probe reasoning capabilities in theoretical physics.


While lower difficulty math benchmarks such as \texttt{MATH} \cite{hendrycks2021measuring} have nearly been mastered by current LLMs, the \texttt{FrontierMath} ~\cite{glazer2024frontiermath} dataset, which includes research-level problems curated by working mathematicians, remain largely unsolved. \texttt{FrontierMath} spans a range of difficulties from high-school to research level and features properties like auto-verifiability and rich metadata, design principles we have also incorporated into TPBench. However, \citet{glazer2024frontiermath} provide limited information about the difficulty distribution and the specifics of the problems that have been solved by advanced models.

In the realm of physics, which also demands extensive abstract mathematical reasoning, the focus has been predominantly on high-school level challenges as seen in datasets such as \texttt{JEEBench} \cite{arora2023llmsadvancedenoughchallenging}, \texttt{OlympiadBench} \cite{he2024olympiadbenchchallengingbenchmarkpromoting}, and \texttt{PhysicsQA} \cite{jaiswal2024improvingphysicsreasoninglarge}. Beyond undergraduate-level problems, very little work has addressed mathematical reasoning for theoretical physics. One notable exception is \cite{pan2024quantummanybodyphysicscalculations}, which examines symbolic calculations, albeit within the narrow context of a specific class of quantum many-body physics problems.

Our new dataset, TPBench addresses the gap in theoretical physics reasoning benchmarks beyond the undergraduate level. TPBench encompasses problems ranging from undergraduate to research level, with research problems reflecting challenges typical of those found in theoretical physics publications (rather than representing entire publications in themselves). 
Importantly, TPBench is designed to be independent of industry control, ensuring that the theoretical physics research community has access to a reasoning benchmark that is not susceptible to data leakage from future training data. We look forward to sharing this dataset with collaborators under appropriate data leakage controls.

\subsection{Reasoning Capabilities of LLMs}
\label{sec:related_methods}
Despite the remarkable fluency of LLMs in generating human-like text, their capacity to perform reliable multi-step reasoning remains a challenge~\citep{mirzadeh2024gsm}. Many LLMs still struggle with complex arithmetic and logical inference tasks. In this section, we review state-of-the-art methods, spanning both training-time and inference-time techniques that have been developed to boost the reasoning capabilities of LLMs.

\paragraph{Training-Time Methods for Improved Reasoning}
Training-time methods encompass all strategies where pre-trained language models are fine-tuned or otherwise modified to improve their reasoning capabilities. The most popular approaches in this category rely on either supervised fine tuning \citep{team2025kimi,xu2025redstardoesscalinglongcot,bespoke_stratos}, or reinforcement learning \citep{guo2025deepseek,team2025kimi} (or both \citep{guo2025deepseek}).
In supervised fine-tuning \citep{yu2023metamath,zelikman2022star,zelikman2024quiet, shao2024deepseekmathpushinglimitsmathematical}, high-quality reasoning chains are curated and used to fine-tune models to display more accurate reasoning behavior. \citet{chen2024self} demonstrate that self-play fine-tuning can improve model reasoning. 

\paragraph{Inference-Time Methods for Improved Reasoning}
Test-time methods aim at improving reasoning capabilities by either designing prompts that elicit good reasoning behavior or by building reasoning systems which prompt the LLM over and over to arrive at a solution in a systematic way. The most popular strategy for prompting large language models to reason is Chain-of-Thought \citep{wei2023chainofthoughtpromptingelicitsreasoning}, where the prompt includes instructions to ``think step-by-step". This is a type of test-time approach \citep{snell2024scalingllmtesttimecompute,welleck2024decodingmetagenerationinferencetimealgorithms,muennighoff2025s1}, as it typically leads to longer token sequences generated by the LLM. The default prompt (see App. \ref{sec:llmprompt}) we use to evaluate various LLMs on TPBench is a customized variation of chain-of-though -- it includes the tips from Polya's famous manual \say{How to solve it} \citep{Polya1945} which was originally intended to teach students how to solve mathematical problems. Related advances include prompting the model to break down the problem into simpler subproblems \citep{khot2022decomposed,zhou2022least,hao2023reasoning}, or seeking abstractions \citep{zheng2023take}. Other prompting strategies encourage models to self-verify \citep{lightman2023letsverifystepstep,selfevaluation}, self-improve \citep{chen2024self, chen2024boosting}, or iteratively refine their answer~\citep{selfrefine,selfrefinereport}.

Other strategies to elicit reasoning behavior involve the generation of multiple reasoning chains which can then be sampled from (as in best-of-$n$ \cite{beirami2025theoreticalguaranteesbestofnalignment}) or combined via majority voting or by ensuring self-consistency \citep{selfconsistency}. Methods that improve reasoning through planning \citep{hao2023reasoning,yao2023tree, qi2024mutual,zhang2024llamaberrypairwiseoptimizationo1like,kang2024mindstar} roll out multiple reasoning chains hierarchically and explore the space with Monte-Carlo Tree Search \citep{kocsis2006bandit}. The success of these methods depends on how the different reasoning chains are evaluated and can be achieved either through other language models \citep{qi2024mutual} or through external tools, e.g. \citep{zhang2024llamaberrypairwiseoptimizationo1like}. Tool usage in reasoning is explored next.

\paragraph{Verifiers and Tool Usage.}
Another avenue for boosting the performance of LLMs is by allowing tool usage \citep{schick2024toolformer, saad2024archon} either during the reasoning phase \citep{chen2022program}, or to verify intermediate reasoning steps \citep{zhang2024llamaberrypairwiseoptimizationo1like} and solutions \citep{imani2023mathprompter}. Verifiers and tools are compatible both with training-time and test-time methods. Since each of the problems in TPBench has an auto-verifier, one could consider giving the LLM under evaluation access to the auto-verifier to test if, by using it, it can achieve better results.