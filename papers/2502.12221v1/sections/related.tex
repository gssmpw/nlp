\section{BACKGROUND \& RELATED WORK}
\label{sec:rel}

Decompilation is the process of reversing a binary file back into its source code form.
This process can be used to analyze the functionality of software when the source code is unavailable.
The typical decompilation tools, such as Hex-Rays IDA Pro~\citep{idapro} and Ghidra~\citep{ghidra}, typically rely on the analysis of the program's data flow or control flow~\citep{decompilation1}.
These decompilation tools analyze the instructions in the executable section (.text section) of the assembly code and then construct the program's Control Flow Graph (CFG). 
They identify patterns that correspond to standard programming structures (such as if-else, while loops, etc.~\citep{for_loop}) and perform type inference to resolve information in the read-only data section (.rodata section).

However, the construction of these tools heavily relies on rule systems created by experts, and the process of constructing these rules is highly challenging.
It is also difficult to cover the entire CFG, and errors are common.
Furthermore, these rules tend to fail when facing optimized binary code, yet optimization is a common practice in commercial compilers.
Additionally, the output of these decompilation tools is often a source-code-like representation of the assembly code, such as directly translating variables to registers, using goto statements, and other low-level operations.
This makes the output code difficult to read and understand, and it may not be sufficient to support recompilation.


Inspired by neural machine translation, researchers redefine decompilation as a translation task, which converts machine-level instructions into human-readable source code.
Initial attempts use Recurrent Neural Networks (RNNs)~\citep{decompilation2_rnn} for decompilation, supplemented by error correction techniques to improve results.
However, these efforts are limited in effectiveness.
Recent advancements in Natural Language Processing (NLP) enable large language models (LLMs) to be applied to code-related tasks~\citep{codellama, starcoder, deepseekcoder}. 
These models typically adopt the Transformer architectures~\citep{transformers}, use self-attention mechanisms, and are pre-trained on large-scale text datasets.
This approach allows LLMs to capture subtle contextual nuances and contributes to a general understanding of language.
Currently, when introducing LLMs into the binary decompilation domain, the methods are categorized based on whether relying on existing decompilation tools: end-to-end methods and refine-based methods. 
We will respectively introduce them below.

Specifically, refine-based methods work with the output of decompilation tools.
DeGPT\citep{hu2024degpt} designs an end-to-end framework to improve the readability of decompiler output.
DecGPT\citep{refine_decompile} uses LLMs combined with compiler information and runtime program information to enhance the compilability of decompiler output.
Recently, LLM4Decompile~\citep{llm4decompile} releases the first open-source large language model specifically for decompilation, which includes both refine-based models and end-to-end models.
Refine-based methods reuse human-encoded rules from existing decompilers, reducing the difficulty of decompilation, but they also introduce additional dependencies.

End-to-end methods decompile directly from assembly code.
BTC~\citep{btc} is one of the earliest methods to fine-tune LLMs for this purpose, extending the decompilation task to multiple languages.
Slade~\citep{slade} expand the model size and trained an LLM-based decompiler with 200 million parameters.
Nova~\citep{nova} proposes a hierarchical attention mechanism and a contrastive learning approach to improve the decompilation ability of the model.
LLM4Decompile\citep{llm4decompile}, in addition to releasing open-source decompilation models, provides a new benchmark for decompilation tasks.
SCC and FAE \citep{feng2024self} further improve the performance of end-to-end decompilation based on LLM4Decompile with self-constructed context and fine-grained alignment techniques.

We find that existing end-to-end decompilation methods perform poorly in handling binary files.
They not only lose the jump information in the executable section but also fail to retain the information from the data sections needed for decompilation.
This could be a key reason for the poor performance of end-to-end methods.
To address the challenges, we redesign the end-to-end decompilation process in \textit{ReF Decompile}, incorporating relabeling strategy to preserve control flow information and function call strategy to access variable information.
