\section{CONCLUSION}

Decompilation is the reverse process of converting compiled binary code back into a high-level programming language. 
In this paper, we revisit previous end-to-end decompilation approaches and identify a critical issue: they often lose crucial information required for reconstructing control flow structures and restoring variables when processing binary files.
This limitation makes it challenging for end-to-end methods to accurately recover program logic, resulting in inferior performance compared to refine-based methods.
To address this issue, we propose \textbf{ReF Decompile}, which involves a redesigned end-to-end decompilation workflow.
Specifically, to tackle the loss of control flow information, we introduce a relabeling strategy to reformat data by replacing jump target addresses with labels and placing the corresponding labels before the jump targets for clear identification.
To mitigate the loss of variable information, we train the model to infer variable types using function call strategy, allowing interaction with the binary file to retrieve variable values and complete the information required for variable reconstruction.
Experimental results on the Humaneval-Decompile Benchmark demonstrate that ReF Decompile, as an end-to-end approach, outperforms refine-based baselines with the same model size.
It achieves a SOTA performance of 61.43\% in deep learning decompilation.
In addition, we find that our method not only enhances the performance of the decompilation task but also improves the readability of the decompiled results compared to baselines.
We further analyze the effectiveness of the Relabeling and Function Call through ablation studies and dataset analysis.
