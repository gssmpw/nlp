
% human eval
% Total Instructions: {'O0': 12814, 'O1': 8293, 'O2': 8577, 'O3': 11543}
% Total Jump Instructions: {'O0': 1232, 'O1': 1255, 'O2': 1155, 'O3': 1499}
% Total Address Instructions: {'O0': 164, 'O1': 174, 'O2': 147, 'O3': 208}
% Total Jump Labels: {'O0': 1081, 'O1': 996, 'O2': 936, 'O3': 1115}
% Total Address Labels: {'O0': 131, 'O1': 139, 'O2': 128, 'O3': 180}
% ===============================
% exebench
% Total Instructions: {'O0': 15663584, 'O1': 10835157, 'O2': 11210461, 'O3': 12625969}
% Total Jump Instructions: {'O0': 1175557, 'O1': 1058703, 'O2': 975984, 'O3': 1130618}
% Total Address Instructions: {'O0': 791054, 'O1': 686307, 'O2': 642355, 'O3': 663484}
% Total Jump Labels: {'O0': 930527, 'O1': 817404, 'O2': 787097, 'O3': 859764}
% Total Address Labels: {'O0': 554689, 'O1': 550205, 'O2': 548709, 'O3': 561601}



\section{EXPERIMENTS}

% 在这一章，我们介绍我们的实验细节和分析结果。
% 具体而言，我们使用 Exebench 的子集作为训练集，使用 Decompile-Eval 作为测试集。
In this section, we introduce the experimental details and analysis of our results. 
Following \citet{llm4decompile} and \citet{feng2024self}, we utilize a subset of Exebench \citep{exebench} as the training set and Decompile-Eval \citep{llm4decompile} as the test set.
We compare our method with six baseline methods and experimental results demonstrate that our method achieves state-of-the-art performance among models of the same size.
% , outperforming the refine-based strong baseline by 60.37\% and attaining the highest readability score of 3.69.

\subsection{Training Details}

% 在这一章，我们将介绍训练数据的构建方法以及数据来源。
% 我们从 Exebench \citep{10.1145/3520312.3534867} 的 train_real_compilable 子集中选取了 10,000 条样本来构建训练数据。
% 首先，我们使用 gcc 将函数代码编译成不同优化等级（包括O0到O3）的二进制共享库，并开启 debug 选项以保证编译结果中包含 dwarf 信息。
% 然后，我们使用 objdump 来对反汇编编译好的二进制共享库，这里我们使用 `-S` 选项使其在反汇编结果中包含源代码，此外我们还使用 `--source-comment=";"` 在每一行源码前包含 ";" 符号以方便我们解析。
% 最后，我们解析上一步得到的汇编代码，从中抽取出对应函数的汇编代码和源代码（`代码`、`汇编`、`代码`、`汇编`），并将其重新组织成 `汇编`、`代码`、`汇编`、`代码` 的形式。

% 我们使用 Lora 来对模型进行微调，lora_rank: 32, lora_alpha: 64。
% 优化器使用 AdamW，学习率为 5e-5，最大长度为 16384，lr scheduler 类型为 cosine，wram up 20，训练一个 epoch。
% 我们使用 LlamaFactory \citep{llamafactory} 来对模型进行微调。

\subsubsection{Training Data}
% \paragraph{Training data}
Following \citet{llm4decompile} and \citet{feng2024self}, we utilize a subset of Exebench \citep{exebench} as the training set.
% and Decompile-Eval \citep{llm4decompile} as the test set.
ExeBench is the largest public collection of five million C functions, and we select 15k samples from the train\_real\_compilable subset to synthesis the training data (about 0.4b tokens). The selected functions exclusively utilize the standard C library and do not include additional data structures. The training data were synthesized with gcc 11.4 provided by Ubuntu 22.04.

% \paragraph{Implementation} 
\subsubsection{Implementation} 

Following \citet{feng2024self}, we employ LoRA \citep{hu2022lora} to fine-tune the \textit{\href{https://huggingface.co/LLM4Binary/llm4decompile-6.7b-v1.5}{llm4decompile-6.7b-end v1.5}} model obtained from Hugging Face \citep{huggingface}.
The rank is set to 32, alpha to 64, and the target includes embedding layer, lm head, and all projection layers\footnote{embed\_tokens, lm\_head, q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj}.
The model is trained for one epoch using the AdamW optimizer \citep{adamw} with a learning rate of 5e-5.
The maximum sequence length is set to 4096, and the learning rate scheduler type is cosine, with a warm-up period of 20 steps. 
% The maximum sequence length is configured to 4096, and a cosine learning rate scheduler with a 20-step warm-up period is applied. 
The fine-tuning process leverages LlamaFactory \citep{llamafactory}, FlashAttention 2 \citep{flashattention2}, and DeepSpeed \citep{wang2024zero}. All experiments are conducted on an A100-SXM4-80GB GPU, and greedy decoding is utilized throughout the experiments.

% 我们使用 llm4decompile-end-6.7b v1.5 和 deepseek-coder-base 来初始化模型。
% 然后我们使用 Lora 方法来训练模型, 

\subsection{Evaluation Details}

% 在这里我们使用 Decompile-Eval Benchmark \citep{llm4decompile} 来对我们的模型进行评估。
% 该数据集使用 humaneval 的 164 个问题并将其翻译成 c 语言，构建了 O0, O1, O2, O3 4个优化等级的汇编代码，并使用 humaneval 的测试方法而非 Blue 来评估生成代码的正确性。
% 该 Benchmark 从  re-compilability and re-executability 两个方面对模型的反编译代码进行了评估。

\subsubsection{Benchmark}

Following \citet{llm4decompile} and \citet{feng2024self}, we employ Decompile-Eval \citep{llm4decompile} as our evaluation benchmark, which is specifically designed to assess the decompilation capabilities of large language models.
% 
The Decompile-Eval benchmark\citep{llm4decompile} is adapted from the HumanEval benchmark\citep{humaneval}, which includes 164 problems initially designed for code generation tasks. These problems are translated into the C programming language, and the corresponding assembly code is generated at four optimization levels (O0, O1, O2, and O3). The correctness of the decompilation results is tested using the test cases from HumanEval.


\subsubsection{Metrics}
The primary metrics of the Decompile-Eval benchmark are as follows:% \footnote{The dataset also includes another metric called Re-compilability Rate, which evaluates whether the decompiled code produced by our model can be successfully recompiled into executable binary without errors. In this paper, we do not involve this metric because trained models typically achieve high performance (above 95\%), which limits the discriminative power of this metric.}

\begin{itemize}
  % \item \textbf{Re-compilability Rate}: This metric evaluates whether the decompiled code produced by our model can be successfully recompiled into executable binary without errors. A high recompilability rate indicates that the decompiled code is syntactically correct and adheres to the constraints of the target language (in this case, C).
  \item \textbf{Re-executability Rate}: This metric assesses the functional correctness of the decompiled code. Specifically, it measures whether the recompiled binaries produce the expected outputs when executed. The correctness of the output is determined using the testing methodology provided by the HumanEval dataset, ensuring a comprehensive evaluation of the logical accuracy of the decompiled code.
  \item \textbf{Readability}: This metric evaluates the readability of the decompiled code. Specifically, it uses GPT-4o with a structured template to assess syntactic similarity (variables, loops, conditions) and structural integrity (logical flow, overall structure). Based on a detailed comparison between the original and decompiled code, a score from 1 (Poor) to 5 (Excellent) is assigned. A score of 4 indicates that the decompiled code is nearly identical to the original in terms of readability, offering an intuitive measure of code quality.
\end{itemize}

% 在我们的实验中，我们将更多的关注 Re-executability, 因为它更能体现模型整体上的反编译能力。

% In our experiment, we pay more attention to the Re-executability, as it more accurately reflects the overall decompilation capability of the model.



\subsection{Baselines}
\label{sec:baselines}


To demonstrate the effectiveness of our proposed methods, we compare them with several baselines, including Rule-Based Decompilers, Refine-Based Methods, and End-to-End Methods.
In this section, we introduce these methods.\footnote{For the baselines listed below, all models are assumed to be of size 6.7B unless otherwise specified, except for \ghidra{} and GPT-4o. We also report the performance of models with other sizes in \Cref{fig:compare}}


% \subsubsection{Methods and Models}

\begin{itemize}
    \item \textbf{Rule-Based Decompiler} relies on manually crafted rules and techniques such as control flow and data flow analysis to transform assembly code into high-level language code. 
    \begin{itemize}
    \item \textit{\ghidra{}}: A free and open-source reverse engineering tool (decompiler)
    % developed by the National Security Agency of the United States 
    \citep{ghidra}. It serves not only as the baseline for comparison but also as the preprocessing tool for the Refine-Based decompilation method.
    \end{itemize}
    \item \textbf{Refine-Based Methods} builds upon the output of rule-based decompilers, leveraging large models to refine and enhance the decompilation results for improved accuracy and readability.
    \begin{itemize}
    \item \textit{GPT 4o}: One of the most powerful language models developed by OpenAI, which is used to refine the Ghidra decompilation output.
    \item \textit{LLM4Decompile-Ref}: A series of pre-trained refine-based models from LLM4Decompile~\citep{llm4decompile}, which refine pseudo-code decompiled by Ghidra.
    \end{itemize}
    \item \textbf{End-to-End Methods} directly process assembly code using large models to generate high-level language code.
    \begin{itemize} 
    \item \textit{LLM4Decompile-End}: A series of pre-trained end-to-end models from LLM4Decompile~\citep{llm4decompile}, which directly decompile binaries into high-level code.
    % \item \textit{LLM4Decompile-End}: A series of pre-trained end-to-end decompilation models \citep{llm4decompile}.
    \item \textit{FAE Decompile}: A model obtained by applying the Fine-grained Alignment Enhancement method to further fine-tune the llm4decompile-End-6.7b \citep{feng2024self}\footnote{This paper also involves a decompilation strategy called SC$^2$. We do not include it as a baseline since it is not a model.}.
    % We directly use the results and resources provided in the paper.
    
    \end{itemize}
    % \item \textbf{SC$^2$-Decompile}: A Self-Constructed Context Decompilation method that can be applied to neural decompilation models without fine-tuning \citep{feng2024self}. We use the results and resources provided in the same paper.
    % \item \textbf{ReF Decompile}: The method we propose, with the implementation details described in the \Cref{sec:method}.
\end{itemize}

% \begin{itemize}
%     \item \textbf{Rule Based Decompiler} \textbf{\ghidra{}}: A free and open-source reverse engineering tool (decompiler) developed by the National Security Agency of the United States \citep{ghidra}. It serves not only as the baseline for comparison but also as the preprocessing tool for the Refine-Based decompilation method.

%     \item \textbf{GPT 4o}: One of the most powerful language models developed by OpenAI, renowned for its advanced language understanding and generation capabilities \citep{gpt4}. We leverage the results reported in [3], where GPT-4 was used to refine the decompilation output from Ghidra.
%     \item \textbf{LLM4Decompile-Ref}: A series of pre-trained decompilation LLMs\citep{llm4decompile}, including the End-to-end Decompilation models (LLM4Decompile-End) and the Refined Decompilation models (LLM4Decompile-Ref).
%     \item \textbf{LLM4Decompile-End}: ADD discription HERE!! %A series of pre-trained decompilation LLMs\citep{llm4decompile}, including the End-to-end Decompilation models (LLM4Decompile-End) and the Refined Decompilation models (LLM4Decompile-Ref).
%     \item \textbf{FAE-Decompile}: A model obtained by applying the Fine-grained Alignment Enhancement method to further fine-tune the llm4decompile-End-6.7b \citep{feng2024self}. We directly use the results and resources provided in the paper.
%     % \item \textbf{SC$^2$-Decompile}: A Self-Constructed Context Decompilation method that can be applied to neural decompilation models without fine-tuning \citep{feng2024self}. We use the results and resources provided in the same paper.
%     % \item \textbf{ReF Decompile}: The method we propose, with the implementation details described in the \Cref{sec:method}.
% \end{itemize}

\subsection{Main Results}

% 如实验结果 \ref{table:main} 所示，我们的方法在可执行性上相较于微调之前的模型(llm4decompile)提升了约 12% 的性能，在同尺寸模型下取得了最好的性能结果 60.37%。
% 并且我们的模型反编译的代码也取得了同尺寸模型最好的可读性，约 3.69。
% 总的来说，我们的方法以 7B 的尺寸，在可执行性和可读性上均取得同尺寸模型的最优结果，接近22B 参数量的 LLM4Decompile-Ref-22B 模型。

In this paper, we evaluate our ReF Decompile and the backbones in \Cref{sec:baselines} on Decompile-Eval. The main results of our experiments are shown in ~\Cref{table:main} and ~\Cref{fig:compare}. 
% We address the following research questions (RQs):

\input{tables/main.tex}
\input{figures/compare}

% \textbf{RQ 1: Blah blah} 
% As shown in Table \ref{table:main}, our proposed method has improved the executability by approximately 12\% compared to the pre-fine-tuned model (LLM4Decompile-End-6.7B), achieving the best performance among models of the same size, with a specific success rate of 61.43\%. Additionally, the decompiled code generated by our method also exhibits superior readability, scoring 3.69, which is better than that of other models of the same size. Overall, despite having only 7 billion parameters, our model achieves results comparable to those of the 22-billion-parameter LLM4Decompile-Ref-22B model in terms of both executability and code readability, fully demonstrating the effectiveness of our approach.

\textbf{(1) Our method is the best and surpasses all other approaches to become the state of the art.} As shown in \Cref{table:main}, the decompiler-based method has the lowest performance, with a re-compilability of 20.12, because manually crafted rule systems cannot guarantee that the generated code is fully compilable. In the refine-based methods, LLM4decompile, which is trained on 20B tokens of decompilation data, outperforms the untrained GPT-4o, achieving a re-compilability of 52.74\%. As analyzed in the introduction, it corrects the decompilation results from the decompiler, surpassing the two other end-to-end methods, and becomes the strongest baseline.

\textbf{(2) As an end-to-end approach, ReF Decompile surpasses refine-based baselines.} It improves the Re-executability metric by 8.69\% and the readability metric by 0.19. This demonstrates the effectiveness of our two strategies, Relabling and Function Call, which reverse the trend where end-to-end methods typically perform worse than refine-based methods.

\textbf{(3) The Relabling and Function Call strategies better leverage the potential of end-to-end methods.} As shown in \Cref{fig:compare}, the performance of 6.7B ReF Decompile not only significantly surpasses both 1.3B and 6.7B Refine-Based LLM4Decompile-Ref, but it is also comparable to the 22B Refine-Based LLM4Decompile-Ref, with an average gap of only 2.75\%. Notably, at optimization level O0, the performance of ReF Decompile (6.7B) even exceeds that of the 22B Refine-Based LLM4Decompile-Ref model, indicating that the model can automatically learn patterns beyond those defined by humans from large-scale corpora.

\textbf{(4) ReF Decompile surpasses other end-to-end baselines in readability, becoming the new SOTA.} Besides a significant improvement in Re-executability (10.36\%), it achieves a readability score of 3.69. This shows that our two strategies effectively avoid the loss of crucial information needed to reconstruct control flow structures and variables, leading to a more accurate recovery of the program's logic.


% 
% \textbf{(1) 我们的方法是最好的，超过了所有种类的方法成为sota。}整体上来看，反编译器的方法相对最低，为20.12，这是因为人工构造的规则系统不能保证生成的代码完全可编译。refine的方法中，LLM4decompile由于经过了20Btokens反编译数据的训练，超过未经训练的GPT4o，获得了52.74%的可重编译性。正如sec intro中所分析的，它基于反编译器的反编译结果进行修正，超过其他两个端到端的方法，成为最强的基线。
% \textbf{(2)作为端到端的方法，ReF Decompile超过了refine-based的方法。}在Re-executability指标上提升8.69%，在可读性指标上提升0.19。这说明我们的两个策略的有效性，利用Relabling和Function Call，一举扭转了端到端方法不如refine方法的现象。
% \textbf{(3)Relabling和Function Call两个策略更充分地挖掘了端到端方法的潜力。}如图~\ref{fig:compare}所示，6.7B的ReF Decompile的性能除了明显超过了1.3B和6.7B的Refine-Based LLM4Decompile-Ref以外，与22B的Refine-Based LLM4Decompile-Ref也是可比的，平均只低了2.75%。尤其是，在O0的优化等级上，ReF Decompile (6.7B)的性能甚至超过Refine-Based LLM4Decompile-Ref（22B）模型的性能，这意味着模型可以从大规模语料中自动学习到超越人类定义的Pattern。
% \textbf{（4）ReF Decompile在Readability上超过了其他端到端的基线，成为新的SOTA。}它除了在Re-executability上面有明显的提升以外（10.36%），在readbility上面达到了3.69。这说明我们采取的两个策略避免了重建控制流结构和重建变量所需的关键信息的损失，从而更准确地恢复程序的逻辑。

% 反编译器的输出可读性较差（如 变量名的命名）

\subsection{Ablation Study}

\input{tables/ablation.tex}

% As shown in Table \ref{tab:ablation}, we conducted ablation studies to analyze the impact of two distinct components (Relabeling and Tool) on model performance. 
% When initialized with the LLM4Decompile-End-6.7B model, the integration of both Relabeling and Tool demonstrated the most optimal outcomes in terms of performance and readability. 
% According to the ablation study results, the introduction of both Relabeling and Tool significantly enhanced the model's performance. 
% Specifically, the inclusion of Relabeling improved performance by 3\%, while the incorporation of Tool led to a 7\% enhancement in performance. 
% It was also observed that models incorporating only Tool, as well as those integrating both Relabeling and Tool, achieved the highest average executability of 60.37\%. 
% This phenomenon may be attributed to the fact that LLM4Decompile-End-6.7B has been trained on 20 billion tokens of decompilation data without Relabeling, rendering it highly familiar with assembly code patterns in the absence of Relabeling, which allows for a substantial performance boost from just adding Tool. 
% Additionally, these two components also improved the readability of decompiled outputs, with an approximate increase of 0.14 points, equating to a 3.9\% improvement in readability.

% In order to mitigate the influence of continued pre-training on LLM4Decompile-End-6.7B, we re-conducted the ablation studies using the initialization model, Deepseek-Coder-6.7B-base. 
% Surprisingly, the model that incorporated both Relabeling and Tool, after being trained on just 80 million tokens, outperformed all other decompilation models of the same size, whether they were End2end or Refined decompilation models. 
% The results of the ablation studies further substantiate the significant performance gains from the introduction of both Relabeling and Tool, with the combined effect being more pronounced. 
% In particular, the inclusion of Relabeling contributed to a 3\% performance gain, the addition of Tool resulted in a 5\% performance improvement, and the concurrent integration of both components yielded a 10\% performance enhancement, surpassing the sum of their individual contributions.
% Similar to the model initialized with LLM4Decompile-End-6.7B, the two components also improved the readability of decompiled results by approximately 0.14 points, equating to a 4.3\% improvement in readability.

%%%%%%%%%%%%%%%%%%%%%5
As shown in \Cref{tab:ablation}, we conduct ablation experiments to analyze the impact of two different strategies (Relabeling and Function Call) on model performance.

When initialized with the LLM4Decompile-End-6.7B model, both Relabeling and Function Call contribute to model performance and readability, and their combination performs best in both metrics.
Specifically, the introduction of Relabeling improves performance by 3\%, while the introduction of Function Call improves performance by 7\%. The model with both Relabeling and Function Call achieves the highest average re-executability at 61.43\%. Moreover, these two components also improve the readability of the decompiled results, increasing it by about 0.14 points, which corresponds to a 4\% improvement in readability.

In addition to using LLM4Decompile-End-6.7B, in order to minimize the impact of continued pretraining, we also use its initialized model Deepseek-Coder-6.7B-base. Surprisingly, the model with Relabeling and Function Call, trained with only 0.4B tokens, outperforms the strongest baseline of the same size in \Cref{table:main} (52.74). The ablation results further confirm that Relabeling and Function Call both lead to significant performance improvements, with the combined effect being even more pronounced. Specifically, the introduction of Relabeling leads to a 3\% performance improvement, Function Call leads to a 5\% improvement, and the simultaneous introduction of both components leads to a 10\% improvement, surpassing the combined performance improvements of each component individually. Similar to the model initialized with LLM4Decompile-End-6.7B, both components also improve the readability of the decompiled results by about 0.14.


% 如表 \ref{tab:ablation} 所示，我们进行了消融实验以分析两个不同组件（Relabeling 和 Function Call）对模型性能的影响。
% 基于 LLM4Decompile-End-6.7B 模型初始化时，结合 Relabeling 和 Function Call 的模型在性能和可读性方面均表现最佳。根据消融实验结果，Relabeling 和 Function Call 的引入均显著提升了模型的性能。
% 具体而言，Relabeling 的引入提升了 3% 的性能，而 Function Call 的引入则提升了 7% 的性能。
% 而同时引入 Relabeling 和 Function Call 的模型达到了最高的平均可执行性 61.43%。
% 此外，这两个组件也改善了反编译结果的可读性，提升了大约 0.14 分，相当于提升了 4% 的可读性水平。

% 除了采用LLM4Decompile-End-6.7B以外，
% 为了尽量减少继续预训练的影响，我们同样使用其初始化模型 Deepseek-Coder-6.7B-base。
% 令人惊讶的是，引入 Relabeling 和 Function Call 的模型仅使用了 0.4B Token 进行训练，就超过了表~\ref{table:main}中同尺寸的最强基线。消融实验的结果进一步证明了 Relabeling 和 Tool 的引入分别能带来显著的性能提升，且两者结合的效果更为明显。
% 具体来说，Relabeling 的引入带来了 3% 的性能提升，Function Call 的引入带来了 5% 的性能提升，而同时引入两者则带来了 10% 的性能提升，超过了单独引入两者的性能提升之和。
% 与基于 LLM4Decompile-End-6.7B 初始化的模型类似，两个组件对反编译结果的可读性改善也是约 0.14 分。

\subsection{Analysis of Two Components for Untuned Models}

This section analyzes the impact of two key components—Relabeling and Tool Integration—on the performance of untuned models. We examine how these strategies enhance decompilation accuracy.

\input{tables/ablation-untune.tex}

% 分析1: 修正 Label 对没有微调过的模型有没有用？有用
% 分析2: 工具调用对没有微调过的模型有没有用？【不会调用工具/补充工具调用】
% 分析3: benchmark / 训练数据 / 真实二进制程序 中包含不同类型的 rodata 分析

% 端到端反编译在高优化等级下的表现不如Refine-Based反编译稳定：如表 \Cref{table:main} 所示。
% \paragraph{End-to-end decompilation is less stable than Refine-Based decompilation at high optimization levels:} As the \Cref{table:main} and \Cref{tab:untuned-ablation} shows,

% Relabeling 引入可以提升未未微调的模型的性能：从 \Cref{tab:untuned-ablation} 来看，Relabeling 的引入，通过为跳转地址添加更可读的 Label，极大的改善了模型的反编译性能。具体来说，在 GPT-4o 上提升了6.40% （从 21.34% 到 27.74%）的性能，在 Qwen 上模型提升了 3.37% （从11.28% 到 14.63）的性能。正如我们所猜想的那样，通过使用 Relabeling, 处理后的汇编代码的跳转逻辑对于大模型来说更容易理解，GPT-4o 更多的从该变化中收益。
\subsubsection{Relabeling improves the performance of untuned models significantly:}
As shown in \Cref{tab:untuned-ablation}, Relabeling enhances the readability of jump addresses in assembly code by assigning more intuitive labels, leading to better decompilation results.
For example, performance on the GPT-4o~\citep{gpt4} model improves from 21.34\% to 27.74\% (a gain of 6.40\%), while on the Qwen model~\citep{qwen2.5coder}, it improves from 11.28\% to 14.63\% (a gain of 3.37\%).
This supports our hypothesis that Relabeling makes assembly code’s jump logic easier for models to understand, resulting in better inference. GPT-4o benefits more from this change, likely due to its stronger ability to handle complex logic.
Relabeling demonstrates how simple preprocessing improvements can significantly boost performance without requiring additional fine-tuning. 
Enhancing input readability and logic clarity proves to be a valuable strategy for improving model effectiveness in specific tasks.

% \paragraph{Tool 对未微调的模型性能提升的潜力待挖掘：}
% 如表 \Cref{tab:untuned-ablation} 所示，Tool 的引入并不总是提升性能，在我们的实际测试中，模型从来不进行工具调用。
% 在 GPT-4o 的实验中，Tool 的引入甚至略微降低了模型的反编译性能，这可能是因为模型没有理解何时应该调用工具读取未知地址的数据，但是却受到了 Prompt 的干扰。
% 但是在 Qwen 的实验中， Tool 的引入反而略微提升了反编译性能，这可能是因为模型虽然没有理解何时应该调用工具读取未知地址的数据，但是 Tool 的描述激发了一些反编译能力，甚至比 Relabeling 带来的改进还要大。
% 由于两个模型都不进行工具调用，我们补充了一个“作弊” 实验，构造了工具调用和返回结果的对话轮次，来测试模型利用这些信息的能力。
% 实验结果证明，模型虽然不会进行工具调用，但是可以利用工具的结果改进模型性能。
% 由于工具调用帮助模型获取了汇编代码中不存在的变量信息，模型在反编译时不需要“猜测”变量的值，因此提升了相当的性能，比如 GPT-4o 提升了 2% ~ 5% 的性能。
% 这意味着模型在利用工具方面还有待提升，这也说明 Tool 引入带来的性能提升还有待发掘。

\subsubsection{Potential of Function Call to Enhance Untuned Model Performance Remains Underexplored:}
As shown in \Cref{tab:untuned-ablation}, introducing tools does not always lead to performance improvements.
% 虽然我们在prompt中给出工具调用的请求，但是模型总是直接返回反编译结果，而不进行任何工具调用
In our experiments, although we provide the model with tools for decompilation in the prompt, it consistently returns the decompiled results directly without invoking any tools.
% models never actively utilize tools during the decompilation process. 
For GPT-4o, Function Call slightly reduces performance. 
This decrease might be due to the model's inability to determine when to invoke tools to retrieve data from unknown addresses.
Additionally, prompts related to tool usage may cause interference.
In contrast, in the Qwen experiments, Function Call slightly improves performance.
Although the model does not actively do function call, the descriptive prompts associated with tools may stimulate some decompilation capabilities. 

To further verify the model's ability to utilize function call information, we simulate function call and responses to test whether the model can use tool outputs. 
The results show that models, despite not calling tools themselves, are able to leverage the provided information to improve performance. 
For example, tools supply variable information missing from the assembly code, allowing models to avoid "guessing" variable values. 
This leads to a performance boost of 2\% to 5\% for GPT-4o.

These findings highlight two key points. 
First, current models struggle to invoke and utilize tools effectively, limiting their immediate benefits. 
Second, tools still show significant potential for performance enhancement, as demonstrated by the gains from simulated tool use. 
% Future work should focus on improving models' ability to integrate and apply tools, unlocking their full potential for task-specific improvements.

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[t]{0.495\textwidth}
%         \centering
%         \includegraphics[height=0.48\linewidth]{figures/exe_rodata-crop.pdf}
%         \caption{Combination Distribution (Exebench)}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.495\textwidth}
%         \centering
%         \includegraphics[height=0.47\linewidth]{figures/he_rodata-crop.pdf}
%         \caption{Combination Distribution (Humaneval-Decompile)}
%     \end{subfigure}
    % \begin{subfigure}[t]{0.495\textwidth}
    %     \centering
    %     \includegraphics[height=0.48\linewidth]{figures/exe_rodata_type-crop.pdf}
    %     \caption{Type Distribution (Exebench)}
    % \end{subfigure}
    
    % \vspace{0.5cm} % 调整图片之间的垂直间距
    
    % \begin{subfigure}[t]{0.495\textwidth}
    %     \centering
    %     \includegraphics[height=0.47\linewidth]{figures/he_rodata-crop.pdf}
    %     \caption{Combination Distribution (Humaneval-Decompile)}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[t]{0.495\textwidth}
    %     \centering
    %     \includegraphics[height=0.47\linewidth]{figures/he_rodata_type-crop.pdf}
    %     \caption{Type Distribution (Humaneval-Decompile)}
    % \end{subfigure}
    % Exebench 和 Humaneval-Decompile Benchmark 中 Rodata 信息的统计，其中 Combination Distribution 是指编译结果中同时存在不同 rodata 数据的组合的分布情况，而 Type Distribution 是指所有的编译结果中 rodata 数据类型的分布情况
%     \caption{The statistical analysis of Rodata information in the Exebench and Humaneval-Decompile Benchmark reveals two critical aspects: Combination Distribution describes the distribution of different Rodata data combinations within the compilation results, while Type Distribution reflects the overall distribution of Rodata data types across all compilation results.}
%     \label{fig:rodata}
% \end{figure}

\input{tables/dataset-rate.tex}
\input{tables/dataset.tex}

\subsection{Dataset Analysis}
% in this section，为了验证Relabelling和Function Call的必要性，我们分析数据集中与这两个策略相关的信息的分布情况。
In this section, to validate the necessity of Relabelling and Function Call, we analyze the distribution of information related to these two strategies within the dataset.

% Rodata 广泛存在于各种环境中：如 \Cref{fig:rodata} 所示，我们分析了 Exebench 和 Humaneval-Decompile 中的 Rodata 信息。在 Exebench 的 train_real_compilable 子集中，46.55% 的代码包含 Rodata 段，而在 Humaneval-Decompile 中，这一比例为 27.90%。通常认为 Exebench 更接近真实场景代码的分布，而 Humaneval 相对简单。然而，即使在 Humaneval 这种相对简单的场景下，也有接近 1/3 的代码包含 Rodata 段数据。这表明，在真实场景中，工具的引入使得模型能够读取 Rodata 段的内容，可能会带来更大的收益。

\subsubsection{Rodata is widely present in various environments:} 
% As shown in \Cref{fig:rodata}, 
To demonstrate the necessity of introducing function calls, we analyze the Rodata information in Exebench and Humaneval-Decompile in \Cref{tab:dataset-rate}.
In Humaneval-Decompile, 27.90\% of the code includes data within the rodata segment.
Similarly, in Exebench's train\_real\_compilable subset, 48\% of the code stores certain information in the rodata section rather than the executable code section after compilation.
It is commonly believed that Exebench approximates the distribution of real-world code scenarios, while Humaneval is relatively simple. 
Thus, we can observe that even in relatively simple scenarios like Humaneval, nearly 1/4 of the code includes some rodata segment data. 
This implies that in real-world scenarios, the introduction of function call enables models to read the contents of the rodata segment, potentially leading to greater benefits.

\subsubsection{Relabelling and Function Call Are Widely Applicable:}

As shown in \Cref{tab:dataset-rate}, more than 80\% of the code includes fixed addresses, such as data memory access addresses and jump targets, demonstrating the broad applicability of the Relabelling strategy. 
Decompile-Eval contains more branch and jump instructions (Jump Instructions), whereas Exebench exhibits more frequent access to rodata (Data Labels and Load Instructions).

As shown in \Cref{tab:dataset}, at the O2 optimization level, both datasets exhibit the lowest numbers of memory access and jump instructions, which could explain the relatively smaller performance gains from Relabelling and Function Call at this level. 
Nonetheless, our methods demonstrate significant improvements across other optimization levels, underscoring their robustness and adaptability in diverse optimization settings.
% 如 \Cref{tab:dataset} 所示，两个数据集中平均每段代码中不同类型 Label 和 指令的数量不同。整体来看，Exebench 中访问 Rodata 更多一些，而 Decompile-Eval的分支跳转指令更多。
% 有50\%的代码都包含确定的地址，包括数据访存地址和跳转的目标地址，这说明Relabeling策略普遍可用。
% 但是在 O2 优化等级下，Exebench 和 Decompile-Eval 的访存指令数量和跳转指令数量都是最少的，这可能是导致 Relabeling 和 Function Call 在 O2 等级下带来的性能提升较小的原因。

% \section{CASE STUDIES}

% \section{DISCUSSION}
