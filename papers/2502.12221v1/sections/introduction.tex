\section{INTRODUCTION}

Decompilation \citep{ghidra,idapro,llm4decompile,refine_decompile,feng2024self} is the reverse process of converting compiled binary code back into a high-level programming language, with the goal of recovering source code that is functionally equivalent to the original executable.
Decompilation has alluring application value in performing various reverse engineering tasks, such as vulnerability identification, malware analysis, and legacy software migration \citep{decompilation1,decompilation2_rnn,llm4decompile,feng2024self,nova}.

Despite the development of various rule-based decompilation tools, such as Ghidra \citep{ghidra} and IDA Pro \citep{idapro}, the decompilation process continues to face significant challenges. These include the inherent loss of information during compilation \citep{variable_name,for_loop} and the heavy reliance on manual effort to analyze and summarize assembly code patterns for rule-based approaches\citep{ghidra,idapro}. 
Moreover, uncovered or misinterpreted patterns can lead to inaccuracies in the decompilation results\citep{decompilation1,decompile_ir1,variable_name,for_loop,llm4decompile}.
To address this issue, researchers explore the use of large language models (LLMs) in decompilation tasks\citep{slade,btc,llm4decompile,nova,feng2024self,refine_decompile}.
By automatically learning the mapping patterns between low- and high-level code from aligned corpora, LLMs can reduce human labor and improve the readability of the generated output.


The LLM-based decompilation methods are typically divided into two categories: refine-based methods\citep{llm4decompile,refine_decompile,hu2024degpt} and end-to-end methods\citep{feng2024self,slade,btc,nova}.
As shown in \Cref{fig:intro}, the refine-based method focuses on recovering the original code (a) from the pseudo code (d) produced by existing decompilation tools, whereas the end-to-end method aims to directly reconstruct the original code (a) from assembly code (c).
Compared to the refine-based approach, the end-to-end method reduces reliance on additional tools and minimizes manual intervention due to its inherent properties.
This work focuses on improving end-to-end methods.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth, alt={Illustration of neural network-based decompilation approaches}]{figures/intro.pdf}

    \caption{LLM-based decompilation methods can primarily be categorized into two types: (1) refine-based methods, which aim to refine the pseudo code generated by decompilers such as Ghidra to recover the original code; and (2) end-to-end methods, which aim to reconstruct the original code directly from assembly code.}
    \label{fig:intro}
\end{figure}

However, despite its advantages, previous end-to-end method encounters significant challenges in reconstructing control flow and variable values, limiting its accuracy and practical applicability. These limitations arise primarily from two issues:
(1) During data preprocessing, address information is directly and simplistically removed, making it difficult to recover control flow information from the processed assembly code; and 
(2) end-to-end methods rely solely on the processed assembly code, which lacks a significant amount of variable value information (e.g., floating-point numbers, strings, etc.).
These limitations hinder the accuracy and completeness of the decompiled results.


To address the above issues, we propose \textbf{Ref Decompile} method, which is designed to optimize the decompilation process of the end-to-end method.
(1) To tackle the problem of missing control flow information, the \textbf{\textit{Relabeling}} strategy is introduced to restructure the data format, which keeps the related information of jump instructions.
The processed result also satisfies the syntax accepted by the assembler.
(2) To solve the problem of missing variable information, the \textbf{\textit{Function call}} strategy provides a mechanism for the model to interact with the binary file to obtain variable values, thus completing the information needed to recover the variables.
By combining these two strategies, we leverage both control flow and variable information from the binary file, improving the precision of the decompiled results.

On the Humaneval-Decompile Benchmark, the Ref Decompile method outperforms the strongest baseline by $8.69\%$. It achieves a new state-of-the-art (SOTA) performance of $61.43\%$ and the highest readability score of $3.69$. 
These results demonstrate the effectiveness of the two strategies (Relabeling and Function Call) in improving the correctness and readability of decompiled outputs.

Our contributions are as follows:
\begin{itemize}
    \item We identify the key weaknesses of previous end-to-end methods: loss of control flow and variable information.
    \item To address these challenges, we redesign the end-to-end decompilation process, incorporating a \textit{relabeling} strategy to preserve control flow information and a \textit{function call} strategy to access variable information.
    \item The proposed Ref Decompile method achieves SOTA performance among models of the same size, surpassing the strongest baseline with $61.43\%$ in re-executability rate and achieving the highest readability score of $3.69$.
\end{itemize}
