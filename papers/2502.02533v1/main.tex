\documentclass[11pt, a4paper, logo, copyright]{googlecloud}

\pdfinfoomitdate 1
\pdftrailerid{redacted}

\makeatletter
\renewcommand\bibentry[1]{\nocitep{#1}{\frenchspacing\@nameuse{BR@r@#1\@extra@b@citeb}}}
\makeatother

\usepackage{kantlipsum, lipsum}
\usepackage{dsfont}
\input{commands}

\usepackage[authoryear, sort&compress, round]{natbib}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\hypersetup{
  colorlinks,
  citecolor=blue,
  linkcolor=red,
  urlcolor=blue}
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{wrapfig} %
\usepackage{graphicx} %
\usepackage{soul} %
\usepackage{enumitem}
\usepackage{amssymb,amsmath}
\usepackage{tikz, adjustbox}
\usepackage[most]{tcolorbox}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{fancyvrb}

\usepackage{tcolorbox}
\input{math_commands}
\def\appref#1{Appendix~\ref{#1}}
\def\Appref#1{Appendix~\ref{#1}}
\def\tabref#1{table~\ref{#1}}
\def\Tabref#1{Table~\ref{#1}}
 
\newcommand{\stitle}[1]{\noindent \textbf{#1.}}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=none,
    aboveskip=1pt,
    belowskip=1pt,
}
\lstdefinestyle{plainins}{
    backgroundcolor=\color{white},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=none,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    aboveskip=0pt,
    belowskip=0pt,
    frame=single
}
\lstdefinestyle{plainexam}{
    backgroundcolor=\color[HTML]{FFFCF3},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=none,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    aboveskip=0pt,
    belowskip=0pt
}


\lstset{style=mystyle}
\lstset{style=plainins}
\lstset{style=plainexam}
\title{Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies}

\correspondingauthor{hzhouml@google.com, soarik@google.com}

\renewcommand{\today}{}

\author[1 2 *]{Han Zhou}
\author[1]{Xingchen Wan}
\author[1]{Ruoxi Sun}
\author[1]{Hamid Palangi}
\author[1]{Shariq Iqbal}
\author[1 2]{Ivan Vuli\'c}
\author[2]{Anna Korhonen}
\author[1]{Sercan Ö. Arık}


\affil[1]{Google}
\affil[2]{University of Cambridge}

\begin{abstract}
Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with \textit{prompts} that declare their functionality, along with the \textit{topologies} that orchestrate interactions across agents. Designing prompts and topologies for multi-agent systems (MAS) is inherently complex.
To automate the entire design process, we first conduct an in-depth analysis of the design space aiming to understand the factors behind building effective MAS. We reveal that prompts together with topologies play critical roles in enabling more effective MAS design.
Based on the insights, we propose Multi-Agent System Search (\ours), a MAS optimization framework that efficiently exploits the complex MAS design space by interleaving its optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (\textit{local}) prompt optimization; 2) workflow topology optimization; 3) workflow-level (\textit{global}) prompt optimization, where each stage is conditioned on the iteratively optimized prompts/topologies from former stages.
We show that \ours-optimized multi-agent systems outperform a spectrum of existing alternatives by a substantial margin. Based on the \ours-found systems, we finally propose design principles behind building effective multi-agent systems.
\end{abstract}

\begin{document}

\maketitle
\section{Introduction}


\begin{wrapfigure}{r}{0.48\textwidth}
    \centering
    \vspace{-7mm}
    \includegraphics[width=\linewidth]{figs/1-flow.pdf}
    \vspace{-3mm}
    \caption{Proposed Multi-Agent System Search (\ours) framework discovers effective multi-agent system designs (with both optimized \textit{topology} and optimized \textit{prompts}, \textbf{right}) via interleaved prompt optimization and topology optimization in a customizable multi-agent design space (key components illustrated on the \textbf{left}).}
    \label{fig:opening}
\end{wrapfigure}Large language models (LLMs) have showcased extraordinary capabilities in understanding, reasoning, and generating coherent responses based on user prompts, revolutionizing a wide range of applications \citep{ouyang2022training, kojima2022large}. LLM-based agents enhance usability by autonomously handling complex tasks across diverse domains, including code generation and debugging \citep{jimenez2023swe}, retrieval-augmented generation \citep{singh2025agentic,wang2024astute}, data analysis \citep{hu2024infiagentdabenchevaluatingagentsdata, guo2024dsagentautomateddatascience}, and interactive decision-making \citep{su2025learn, li2025embodiedagentinterfacebenchmarking}. These agents are typically programmed with prompts that reinforce them to interact with the environment, utilizing available tools, and approach their objectives over multiple turns \citep{yao2023react}. Beyond individual agents, LLMs can be orchestrated within complex topologies that coordinate multiple agents toward a shared objective. This type of multi-agent system (MAS) typically outperforms its single-agent counterpart by involving more diverse agentic perspectives or role profiles, such as agents as verifiers \citep{shinn2024reflexion} and multi-agent debate \citep{wang2024mixture, qian2024scaling}.


However, designing effective MAS for new domains often proves to be challenging. First, the single agent might suffer from prompt sensitivity \citep{verma2024brittle}, where simple modifications in the prompt can already exert significant but unexpected degradation of performance \citep{zhou2024batch, liu2024self}. In MAS, when sensitive agents are cascaded, the compounding effect due to prompt sensitivity may be amplified. Together with the prompt design, crafting an effective topology might demand a substantial amount of manual experimentation, based on trial and error. The problem complexity is exacerbated by the overall combinatorial search space, over not only the unbounded space of prompt design but also the design decisions of what agent to integrate into the topology. 

Although recent research has explored automating various aspects of agentic designs, there is still a gap in understanding of what matters most regarding improved MAS performance. For example, DSPy \citep{khattab2024dspy} automates the process of designing exemplars for improved prompt programming. \citet{li2024more} proposes to optimize MAS by scaling up the number of agents in majority voting. 
ADAS \citep{hu2024automated} programs new topologies expressed in code via an LLM-based meta-agent. AFlow \citep{zhang2024aflow} searches better topologies using Monte Carlo Tree Search within a set of predefined operators. However, the interplay between multiple design spaces, including prompts and topologies, remains unclear. 

In this paper, we first conduct in-depth analyses of common design spaces in MAS, examining the influence of various aspects such as optimizing the prompts, scaling the number of agents, and involving different types of topologies. Our analyses reveal that prompts frequently form an influential design component that yields strong-performing MAS, and influential topologies only represent a small fraction of the full search space. Based on these insights, we aim to distill the essence of influential MAS components into a pruned search space, thereby lowering the complexity of the overall search process. We propose Multi-Agent System Search (\ours), a novel multi-stage optimization framework that automates the optimization for MAS over an efficient search space. \ours~integrates a plug-and-play prompt optimizer and workflow optimizer over a configurable topology space. It overcomes the complexity of joint optimization on MAS by interleaving the optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (\textit{local}) prompt `warm-up' for each topology block; 2) workflow topology optimization in a \textit{pruned} set of topology space; 3) workflow-level (\textit{global}) prompt optimization given the best-found topology.

By optimizing over the identified influential components, \ours~yields optimized MAS that achieves state-of-the-art performance, outperforming existing manually-crafted MAS baselines and automatically-generated alternatives, by a substantial margin, demonstrated across an extensive selection of tasks, including reasoning, multi-hop understanding, and code generation. Based on the strongest MAS found by \ours, we provide further insights and guidelines behind building effective MAS. Overall, our contributions can be summarized as follows: 1) we provide an in-depth analysis of the design factors that influence the performance of LLM-based MAS, highlighting the importance of prompts and identifying the influential topologies; 2) we propose \ours, a novel multi-stage optimizer that automates the MAS design by interleaving the optimization of prompts and topologies in an influential search space; 
3) \ours~shows significant performance improvement on various evaluation benchmarks, delivering guidelines for building effective multi-agent systems for the future. 






\section{Designing Multi-Agent Systems}
\label{sec: analysis}


In this section, we provide a formulation for designing MAS, followed by analyzing the influence of prompt and topology designs.
We refer to the structural arrangements of agents (or equivalently, building blocks) as the topology of agents and define workflow $\mathcal{W}$ as the logical sequence across different topologies that builds the MAS. The design of a MAS can thus be broadly divided into two levels: block-level design and workflow-level orchestration. At the block level, we aim to design effective individual agents that best perform their intended role with better \textit{prompt} design.
On the other hand, at the workflow level, the optimization involves determining the \textit{types} and \textit{quantities} of agents to include and how to arrange them in the most effective way, referred to as the topology optimization. Formally, given a search space $\mathcal{A}$ that defines all valid configurations $a$ over the blocks (see Fig.~\ref{fig:main}), \textit{workflow topology optimization} can be expressed as the following optimization problem with an objective function $f(\cdot, \cdot)$ on a target input and output set $(x,y)\sim\mathcal{D}$:
\begin{equation}
    \mathcal{W}^*(a) = \argmax_{a \sim \mathcal{A}} \mathbb{E}_{(x,y)\sim\mathcal{D}}[f(\mathcal{W}(a(x)),y)].
\end{equation}
In the rest of this section, we provide an in-depth analysis of each component of MAS design.


\subsection{Block-level: Prompt Design for Agents}
\label{subsec:block-level}
\begin{wrapfigure}{r}{0.48\textwidth}
    \centering
    \vspace{-9mm}
    \includegraphics[width=\linewidth]{figs/2-prompt.pdf}
    \vspace{-7mm}
    \caption{Accuracy vs. the total token counts for prompt-optimized agents per question on MATH by Gemini 1.5 Pro compared to scaling agents with self-consistency (\texttt{SC}), self-refine (\texttt{reflect}), and multi-agent debate (\texttt{debate}) only. The error bar indicates 1 standard deviation. We show that by utilizing more compute, better accuracy can be obtained via more effective prompting.}
    \label{fig:apo}
\end{wrapfigure}At the block level, the primary ``optimizable component'' that significantly influences downstream performance is the \textit{prompt}, which defines the role of the agent (e.g., ``\textit{You are an expert in reflecting on errors}...''), provides additional instructions to shape its behavior (e.g., ``\textit{You should think step by step}...'') and optionally, contains \textit{few-shot demonstrations} (in-context examples) to guide the agent's responses~\citep{wan2024teach, wan2025manyselfimprovingmanyshotreasoners}. For instance, a state-of-the-art prompt optimizer searches both instructions and few-shot demonstrations, where demonstrations are bootstrapped from the model's own, correct predictions on the validation set based on a validation metric. Conditioned on the demonstrations, the prompt optimizer then proposes a few candidates for the instruction with a dataset summary or various hints to improve candidate diversity \citep{opsahl-ong-etal-2024-optimizing}. The instructions and demonstrations are then jointly optimized. 

Although it is well known that LLMs are sensitive to prompts \citep{zhou-etal-2024-fairer, verma2024brittle}, applying automatic prompt optimization (APO) techniques to MAS is rather non-trivial. Unlike single-turn tasks where APO can be easily performed by treating prompts as optimizable variables and performance over a validation set as the target. 
In MAS, APO becomes more complex due to the interdependence across agents (e.g., the output of one agent may be the input of another agent in a cascade with ground-truth responses for intermediate outputs not being available) and exponentially increasing complexity for combinatorial optimization with more number of agents $n$ involved; The reward signals also become more sparse when $n$ increases, preventing us for implementing APO directly on MAS in any manageable budget; as such, many prior works~\citep{zhang2024chain, xia2024agentless} in MAS still primarily use handcrafted prompts instead of including the prompts as optimizable components in the MAS design.

To systematically understand the influence of prompt design in MAS, we specifically and quantitatively analyze the effect of prompt optimization and compare its effectiveness to other operations common in MAS literature, such as scaling with more agents but with default prompts. 
We conduct APO on a chain-of-thought \citep{kojima2022large} agent with both instruction optimization and 1-shot exemplar optimization via MIPRO~\citep{opsahl-ong-etal-2024-optimizing}, and fairly compare the total inference token cost with self-consistency ~\citep{kojima2022large}, self-refine~\citep{madaan2024self}, and multi-agent debate~\citep{du2024improving}, where the specifications are provided in App.~\S\ref{appendix:details}. In Fig.~\ref{fig:apo}, prompting, which equips agents with more informative instructions and exemplars, demonstrates significant advantages in its token-effectiveness over other building blocks. Furthermore, by applying self-consistency on top of the prompt-optimized agent, we observe an improved scaling performance on the token cost, whereas standard approaches in scaling the number of agents (e.g. \texttt{SC}, or \texttt{Reflect}) saturate much earlier. This empirical observation sheds light on the importance of prompting while providing early evidence for designing effective MAS -- \textit{optimize agents locally before scaling their topology}. 
















\subsection{Workflow-level Search Space Design}
\label{subsec:workflow-level}
\begin{wrapfigure}{r}{0.48\textwidth}
    \centering
    \vspace{-6mm}
    \includegraphics[width=\linewidth]{figs/3-influence.pdf}
    \vspace{-6mm}
    \caption{The performance of different topologies with Gemini 1.5 Pro compared to the base agent with each topology being optimized with APO, where \texttt{Sum}. (Summarize) and \texttt{Exe}. (Executor) are task-specific topologies as illustrated in Fig.~\ref{fig:main}. We observe that not all topologies have a positive influence on the MAS design.}
    \label{fig:pruning}
\end{wrapfigure}At the workflow level, the primary focus is on orchestrating agents to achieve the best performance effectively. As a relatively new concept specific to MAS, topology optimization has recently garnered significant attention \citep{li2024autoflow, zhang2024aflow}. However, while much of the existing research emphasizes \textit{search methods}—such as discovering the most efficient and effective way to identify the optimal configuration—there has been less focus on the design of \textit{search spaces}, which determines the perimeter and the scope of any search algorithm. This imbalance draws a parallel to the historical development of \textit{neural architecture search} (NAS)~\citep{white2023neural}. 
Initially, the field concentrated on sophisticated search methods, such as Bayesian optimization~\citep{kandasamy2018neural, ru2020interpretable} and differentiable search~\citep{liu2018darts}. Follow-up works have highlighted the often-overlooked importance of search space design, arguing that it can be equally, if not more, critical~\citep{wan2022on, zhou-etal-2023-survival, zhou-etal-2024-autopeft}. 
Inspired by this insight, we hypothesize that manually crafted topologies might be sub-optimal, and automatic topology optimization (potentially framed as a rigorous optimization problem) can play a similarly pivotal role via judiciously designing search space for MAS. To achieve so, we first define an expressive search space, similar to prior works, that consists of the connections between the following \textit{building blocks}:
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/4-mass.pdf}
    \vspace{-6mm}
    \caption{Illustration of the \ours~framework with its search space and the multi-stage optimization. The search space combines both prompts (\texttt{Instruction}, \texttt{Demo}) and configurable agentic building blocks (\texttt{Aggregate}, \texttt{Reflect}, \texttt{Debate}, \texttt{Summarize}, and \texttt{Tool-use}). 1) \textcolor{stage1}{Block-level \textbf{Prompt} Optimization}: we conduct \textit{block}-level prompt optimization for each agentic module individually (denoted by \textcolor{stage1}{\texttt{</>}}); 2) \textcolor{stage2}{Workflow \textbf{Topology} Optimization}: conditioned on the best prompts found in Stage 1 on each agent block, \ours~samples valid configurations from an influence-weighted design space while fusing the prompts of each building block from Stage 1; 3) \textcolor{stage3}{Workflow-level \textbf{Prompt} Optimization}: conditioned on the best workflow found in the Stage 2, we again conduct \textit{workflow}-level prompt optimization on the best-found MAS (topologies visualized \textit{for illustration only}).}
    \label{fig:main}
\end{figure*}
\label{sec:prompt}

\noindent \textbullet~\textit{Aggregate}: Agents can collaborate in parallel with diversified predictions, which is then followed by an aggregation operator that obtains the most consistent prediction. The \texttt{aggregate} block can be parameterized by $N_a$ agents acting in parallel. Majority vote \citep{li2024more} and self-consistency \citep{chen2024universal} sits within this topology. 

\noindent \textbullet~\textit{Reflect}: Agents can act as verifiers, providing critics and improvement suggestions based on former predictions. The feedback is then fed into the predictor or the reflector itself for an iterative improvement. Similarly, \texttt{reflect} can be parameterized by $N_r$ that defines the number of rounds for self-reflection. The self-refine \citep{madaan2024self} and Reflexion \citep{shinn2024reflexion} represent this block.

\noindent \textbullet~\textit{Debate}: Agents in debate can elicit more truthful predictions than single-agent prediction \citep{du2024improving, liang-etal-2024-encouraging}, where each debating agent would collect opinions from all other agents and provides an updated response. This topology would involve a mixture of agents, and $N_d$ defines the number of rounds for debating. 

\noindent \textbullet~\textit{Custom Agents}: While the former three forms of agents represent the vast majority of agent topologies constructed as multiple parallel, serial, and mixture of agents, more versatile definitions of agents can be inserted into the MAS design space. For example, for task-specific use cases, we introduce an agent as \texttt{summarize} to improve the long-context capability in the customizable design space. 

\noindent \textbullet~\textit{Tool-use}: Building towards an effective MAS, enabling agents to leverage tools to access external information is critical for system performance, such as using retriever for RAG \citep{lewis2020retrieval} and executor with test cases in coding \citep{chen2024teaching}. We introduce tool-use as an optimizable binary `insertion' decision $N_T\in\{0,1\}$.

To understand the influence of individual topology, we report the performance of various topologies in Fig.~\ref{fig:pruning}. It is noticeable that not all topologies are beneficial to MAS design, whereas positively influenced topologies only represent a small fraction of the overall set, such that, in HotpotQA \citep{yang-etal-2018-hotpotqa}, only \texttt{debate} brings 3\% gain while others fail to improve or even degrade systematic performance. We again observe similar trends in the test-output-prediction subtask of LiveCodeBench \citep{jain2024livecodebench}. It highlights the importance of searching in the influential set of search space, whereas including decremental building blocks may not only result in higher search complexity but also degrade the performance.

















\section{\ours: \oursfull}
Our analyses in Sec.~\ref{sec: analysis} underscore the importance of well-designed prompts for individual agents and the careful definition of the search space to achieve effective MAS performance. Building on these, we propose a multistage optimization algorithm, \textbf{\oursfull} (\ours), that surpasses prior arts that focused solely on optimizing workflow topology without appropriate prompt designs. Instead, our approach demonstrates the greater effectiveness of MAS design with properly optimized prompts and thoughtfully designed search spaces. \ours~framework is illustrated in Algorithm~\ref{alg:main_alg} and Fig.~\ref{fig:main}, following an intuition from local to global, from block-level to workflow-level, that conquers the complexity of combinatorial optimization with effective per-stage optimization detailed below.



\begin{wrapfigure}{r}{0.48\textwidth}
\centering
  \vspace{-9mm}
  \begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
\begin{footnotesize}
\caption{\ours: \oursfull}
	    \label{alg:main_alg}
	\begin{algorithmic}[1]
		\STATE \textbf{Input}: Agentic modules in the search space $a_i \in \mathcal{A}$, workflow of agents $\mathcal{W}(a)$, prompt optimizer $\mathcal{O}$, evaluator $\mathcal{E}$, validation set $\mathcal{D}$, temperature $t$, number of candidates $N$, budget $B$.
		\STATE \textbf{Output}: Optimized multi-agent system $\mathcal{W^*}$.
		\STATE \textcolor{stage1}{[\textit{Block}-level \textbf{Prompt} Optimization]}
        \STATE Prompt optimization for the initial agent $\textcolor{codegray}{a_0^*} \leftarrow  \mathcal{O}_{\mathcal{D}}(a_0)$.
        \FOR{ {$a_i$ in $\mathcal{A}\setminus\{a_0\}$} } 
        \STATE Local prompt optimization for each building block in the design space: $\textcolor{stage1}{a_i^*} \leftarrow  \mathcal{O}_{\mathcal{D}}(a_i|\textcolor{codegray}{a_0^*})$
        \STATE Obtain incremental Influence $I_{a_i} \leftarrow \mathcal{E}(\textcolor{stage1}{a_i^*}) / \mathcal{E}(\textcolor{codegray}{a_0^*})$.
        \ENDFOR
        \STATE \textcolor{stage2}{[Workflow \textbf{Topology} Optimization]}
        \STATE Obtain the selection probability 
        $p_a \leftarrow \operatorname{Softmax}(I_a, t)$ 
        \WHILE{ {$n < N$} } 
        \STATE Reject invalid configurations $c$ and cap a budget $B$. The design space is pruned by the selection probability $p_a$, $\textcolor{stage2}{\mathcal{W}_{c}} \leftarrow (\textcolor{stage1}{a_i^*(\cdot), a_{i+1}^*(\cdot), \dots})$ with optimized prompts.
        \STATE Store evaluations $\mathcal{E}_{\mathcal{D}}(\textcolor{stage2}{\mathcal{W}_c})$ and propose new $\mathcal{W}$. 
        \ENDWHILE
        \STATE Obtain the best-performing $\textcolor{stage2}{\mathcal{W}_c^*} \leftarrow \operatorname*{arg\,max}_{c\in \mathcal{C}}\mathcal{E}_{\mathcal{D}}(\textcolor{stage2}{\mathcal{W}_c}).$
        \STATE \textcolor{stage3}{[\textit{Workflow}-level \textbf{Prompt} Optimization]}
        \STATE Workflow-level prompt optimization for the best-performing topology: $\textcolor{stage3}{\mathcal{W}^*} \leftarrow  \mathcal{O}_{\mathcal{D}}(\textcolor{stage2}{\mathcal{W}_c^*})$.
		\STATE \textbf{Return} optimized multi-agent system $\textcolor{stage3}{\mathcal{W^*}}$.
	\end{algorithmic}
\end{footnotesize}
\end{algorithm}
\end{minipage}
\end{wrapfigure}\rparagraph{1) Block-level prompt optimization} 
Before composing agents, we first ensure that individual agents are thoroughly optimized at the block level, as highlighted in Sec.~\ref{subsec:block-level} and Fig.~\ref{fig:apo} -- this step ensures that each agent is primed for its role with the most effective instructions in the most manageable computation budget. To further overcome the complexity of joint optimization on a large MAS space, we first warm up the initial predictor with single-agent APO, $a_0^* \leftarrow  \mathcal{O}_{\mathcal{D}}(a_0)$, where both instruction and exemplars are jointly optimized with the modular prompt optimizer $\mathcal{O}$. 
Followed by conditioning on the warmed predictor, we continue optimizing each topology with a minimum number of agents, $a_i^* \leftarrow  \mathcal{O}_{\mathcal{D}}(a_i|a_0^*)$, such that, 2 predictors paired with 1 debator form the minimum building block as the \texttt{debate} topology, thereby lowering the complexity for optimization, and this topology can be scaled up later with more predictors and debators but all equipped with optimized prompts. To measure the influence of each building block, we store the validation performance once the optimization is completed. It is important that though Stage (1) serves as the \textit{warm-up} stage per building block, it is still a critical stage that guarantees the follow-up topology optimization is searching in an effective space, composing well-performing agents instead of suffering from the compounding impact from any ill-formed agents with manual prompts. 






\rparagraph{2) Workflow topology optimization} In this stage, we focus on optimizing the overall MAS structure, determining the most effective arrangement and connectivity between agents. The analysis in Fig.~\ref{fig:pruning} shows that beneficial topologies only represent a small fraction of the full design space. Therefore, we aim to distill the essence of strong-performing topologies into a pruned space, thereby making the workflow-level topology search more efficient. Here, we propose to measure the incremental influence $I_{a_i} = \mathcal{E}(a_i^*) / \mathcal{E}(a_0^*)$ that quantifies the relative gain for integrating the topology $a_i$ over the initial agent $a_0$. Following the intuition that influential dimension comes with higher selection probability, we activate the corresponding topology dimension $a$ if $u > p_a$, given $u \sim \mathcal{U}(0, 1)
$ and $p_a = \operatorname{Softmax}(I_a, t)$. To compose diverse topologies into a unified space, we constrain the workflow with a rule-based order to reduce the optimization complexity, following a predefined sequence, such that [\texttt{summarize},  \texttt{reflect}, \texttt{debate}, \texttt{aggregate}]. We integrate rejection sampling over the pre-defined design space that rejects any deactivated dimension, or invalid topology compositions exceeding a maximum budget $B$ on the number of agents. We refer to App. \S\ref{appendix:details} for the detailed search space per task.


\rparagraph{3) Workflow-level prompt optimization} As a final step, we treat the entire MAS design as an integrated entity and run an additional round of prompt optimization, conditioned on the best topology discovered in Stage (2), $\mathcal{W}^* = \mathcal{O}_{\mathcal{D}}(\mathcal{W}_c^*)$. It is worth noting that although prompts were optimized at the individual level in Stage (1), this stage acts as an adaptation or fine-tuning process, ensuring that prompts are tailored for orchestration within the MAS and that the interdependence between agents is optimized appropriately. Our experiments (Fig. \ref{fig:ablation} \& \ref{fig:opt}) demonstrate that this stage often yields practical benefits.

\section{Related Work}
\sparagraph{Forms of LLM-based agentic systems}
The simplest form of an LLM-based agentic system involves a single agent that can dynamically interact and respond to the environment \citep{yao2023react}. Recent advances endow agents with diverse roles and tools \citep{wu2023autogen}, orchestrating multiple agents to cooperate with each other \citep{chen2024agentverse}.
Standard forms of agent cooperation (i.e., topology) often involve parallel and serial flows of information. The parallel form usually diversifies the exploration among many agents in parallel \citep{li2024more}, and self-consistency (SC) \citep{wang2023selfconsistency} is a representative way for scaling agents in parallel. The serial form aims to advance the exploitation of a task via a chain of agents, where LLMs can serve as reflective agents to self-justify and refine former predictions \citep{madaan2024self, shinn2024reflexion}. Later, the opinions from multiple agents can be summarized to retrieve the most consistent answer by an aggregation agent \citep{chen2024universal, lin-etal-2024-just}. Moreover, multi-agent debate consists of a more complex flow of information \citep{chen-etal-2024-reconcile, wang-etal-2024-rethinking-bounds, zhang-etal-2024-exploring}, and recent research shows that debating can elicit more truthful predictions \citep{khan2024debating, du2024improving}. Recent agent topology extends beyond the above connections \citep{wang2024mixture, qian2024scaling}, and \ours~can automatically search the best topology among the aforementioned spaces. 

\rparagraph{Automatic optimization for MAS}
Recent research starts automating agent design by interpreting agent functions as learnable policies \citep{zhang2024offline, zhang-etal-2024-agent} and synthesizing trajectories for agent fine-tuning \citep{qiao-etal-2024-autoact}. 
Going further from a single agent, automatic multi-agent optimization faces a higher level of complexity, thereby requiring a more sophisticated design of search space and algorithms. Among all recent advances in multi-agent optimization, the optimization space has spanned prompts~\citep{khattab2024dspy}, tools \citep{zhou2024symbolic}, workflows \citep{li2024autoflow}, and thinking strategies \citep{shang2024agentsquare}. Aligning closer to our topology search space, DyLAN~\citep{liu2024a} dynamically activates the composition of agents, and Archon~\citep{saad2024archon} frames MAS as a hyperparameter optimization problem. Neither of them has taken the important prompt space into account, where we demonstrated the importance of prompt optimization in Sec.~\ref{sec:prompt}. In addition, 
GPTSwarm~\citep{zhuge2024gptswarm} optimizes the connections between agentic nodes using a policy gradient algorithm. 
State-of-the-art automatic agent design methods, ADAS~\citep{hu2024automated} and AFlow~\citep{zhang2024aflow}, also attempt to optimize agentic workflows with advanced search algorithms and LLM as optimizers. However, we observe that the importance of proper prompt designs has been relatively under-studied in these prior works. 

\section{Experiments}
\label{sec:exp}
\sparagraph{Models and evaluation data} Aside from the common benchmarks used for automating MAS \citep{hu2024automated, zhang2024aflow}, 
we conduct experiments on an extensive collection of tasks: 1) Hendryck's MATH~\citep{hendrycksmath2021} and DROP~\citep{dua-etal-2019-drop} for reasoning; HotpotQA~\citep{yang-etal-2018-hotpotqa}, MuSiQue~\citep{trivedi-etal-2022-musique},  2WikiMultiHopQA~\citep{ho-etal-2020-constructing} from LongBench~\citep{bai-etal-2024-longbench} for long-context understanding;
3) MBPP~\citep{austin2021program}, HumanEval~\citep{chen2021evaluating}, and LiveCodeBench (LCB) `test output prediction'~\citep{jain2024livecodebench} for coding. 
We refer to App.~\S\ref{appendix:details} \& \S\ref{app:template} for details on data splits and prompt templates. We run all experiments primarily on two Gemini 1.5 model sizes~\citep{gemini2024} (\texttt{gemini-1.5-pro-002} and (\texttt{gemini-1.5-flash-002}) and further validate key findings on Claude 3.5 Sonnet
(\texttt{@20240620}) \citep{anthropic2024}.

\sparagraph{Baselines}
We consider the following baselines: 
1) CoT \citep{kojima2022large}: direct chain-of-thought reasoning via zero-shot prompting; 
2) CoT-SC \citep{wang2023selfconsistency}: with self-consistency to find the most consistent answers from diversified reasoning traces; 
3) Self-Refine \citep{madaan2024self, shinn2024reflexion}: reflective agents to verify and self-refine predictions; 
4) Multi-Agent Debate \citep{du2024improving, liang-etal-2024-encouraging}: with agent justifying answers and aggregating information from other agents; 
5) ADAS \citep{hu2024automated}: an automatic agent design framework, where an LLM-based meta-agent iteratively proposes new agents based on former evaluations; 
6) AFlow \citep{zhang2024aflow}: automatic workflow design via Monte-Carto Tree Search over a set of pre-defined operators. We fairly compare all baselines by limiting the maximum number of agents to 10. We refer to App.~\S\ref{appendix:details} for all specifications. 


\begin{table*}[!t]
\centering
\caption{Results on the evaluation set with Gemini 1.5 Pro and Gemini 1.5 Flash. We report the mean and standard deviation for all results with 3 runs of evaluations. We report the accuracy (\%) for MATH and the test-output-prediction subtask of LiveCodeBench (LCB), F1 score for DROP, HotpotQA, MuSiQue, and 2WikiMQA, and pass@1 for MBPP and HumanEval. We note that the meta-prompt of AFlow\textsuperscript{*} only works properly with Claude 3.5 Sonnet. Therefore, we reproduce AFlow with Gemini 1.5 Pro as the executor and Claude 3.5 Sonnet as the optimizer, where \textsuperscript{*} indicates the results are only for reference. Number of agents in inference for all methods are below 10.}
\label{table:main}
\resizebox{\textwidth}{!}{
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccccccccc}
\toprule
\rowcolor{gray!20}
\multicolumn{10}{c}{\texttt{Gemini-1.5-pro-002}} \\
\midrule
\textbf{Task} & \multicolumn{2}{c}{\textbf{Reasoning}} & \multicolumn{3}{c}{\textbf{Multi-hop Long-context}} & \multicolumn{3}{c}{\textbf{Coding}} &\\
 Method    & MATH & DROP & HotpotQA & MuSiQue & 2WikiMQA & MBPP & HumanEval & LCB & Avg.\\
\cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-10}
 CoT & 71.67\textsubscript{3.30} & 70.59\textsubscript{1.67} & 57.43\textsubscript{0.52} & 37.81\textsubscript{1.43} & 63.39\textsubscript{1.12} & 68.33\textsubscript{0.47} & 86.67\textsubscript{0.94} & 66.33\textsubscript{0.62}& 65.28\\
 Self-Consistency & 77.33\textsubscript{1.25} & 74.06\textsubscript{0.90} & 58.60\textsubscript{2.19} & 41.81\textsubscript{1.00} & 67.79\textsubscript{1.19} & 69.50\textsubscript{0.71}& 86.00\textsubscript{0.82}& 70.33\textsubscript{0.94}& 68.18\\
 Self-Refine & 79.67\textsubscript{2.36} & 71.03\textsubscript{1.31}& 60.62\textsubscript{3.33} & 42.15\textsubscript{1.34} & 66.74\textsubscript{2.43} & 63.67\textsubscript{0.24}& 84.00\textsubscript{1.63}& 67.33\textsubscript{1.31}& 66.90\\ 
 Multi-Agent Debate & 78.67\textsubscript{0.94} & 71.78\textsubscript{0.71} & 64.87\textsubscript{0.23} & 46.00\textsubscript{0.80} & 71.78\textsubscript{0.63} & 68.67\textsubscript{0.85}& 86.67\textsubscript{1.25}& 73.67\textsubscript{1.65}& 70.26\\
 ADAS & 80.00\textsubscript{0.82} & 72.96\textsubscript{0.90} & 65.88\textsubscript{1.29} & 41.95\textsubscript{1.24} & 71.14\textsubscript{0.66} & 73.00\textsubscript{1.08} & 87.67\textsubscript{1.70}& 65.17\textsubscript{1.25}& 69.72\\
  AFlow\textsuperscript{*} & 76.00\textsubscript{0.82} & 88.92\textsubscript{0.63} & 68.62\textsubscript{0.47} & 32.05\textsubscript{1.29} & \textbf{76.51}\textsubscript{1.05} & - & 88.00\textsubscript{0.00} & - & -\\
 \midrule
 \ours~(Ours) & \textbf{84.67}\textsubscript{0.47} & \textbf{90.52}\textsubscript{0.64} & \textbf{69.91}\textsubscript{1.11} & \textbf{51.40}\textsubscript{0.42} & 73.34\textsubscript{0.67} & \textbf{86.50}\textsubscript{0.41} & \textbf{91.67}\textsubscript{0.47} & \textbf{82.33}\textsubscript{0.85} & \textbf{78.79}\\ 
 \midrule
 \rowcolor{gray!20}
\multicolumn{10}{c}{\texttt{Gemini-1.5-flash-002}} \\
\midrule
  CoT & 66.67\textsubscript{2.36} & 71.79\textsubscript{0.69} & 57.82\textsubscript{1.10} & 37.10\textsubscript{1.35} & 63.40\textsubscript{0.68} & 63.33\textsubscript{1.25} & 75.67\textsubscript{1.89} & 51.17\textsubscript{0.24}& 60.87\\
 Self-Consistency & 69.33\textsubscript{1.25} & 73.42\textsubscript{0.19} & 60.19\textsubscript{1.01} & 41.94\textsubscript{0.93} & 67.98\textsubscript{0.72} & 63.67\textsubscript{0.62}& 77.67\textsubscript{1.89}& 53.83\textsubscript{1.18}& 63.50\\
 Self-Refine & 71.33\textsubscript{0.94} & 73.71\textsubscript{1.09}& 58.84\textsubscript{3.04} & 41.21\textsubscript{1.99} & 65.56\textsubscript{1.57} & 63.33\textsubscript{1.25}& 81.67\textsubscript{1.89}& 52.00\textsubscript{1.41}& 63.46\\ 
 Multi-Agent Debate & 71.67\textsubscript{0.94} & 74.79\textsubscript{0.87} & 64.17\textsubscript{1.69} & 46.27\textsubscript{1.33} & 72.19\textsubscript{0.54} & 63.00\textsubscript{0.71}& 79.67\textsubscript{1.25}& 55.50\textsubscript{0.41}& 65.91\\
 ADAS & 68.00\textsubscript{1.41} & 75.95\textsubscript{1.18} & 61.36\textsubscript{2.89} & \textbf{48.81}\textsubscript{1.03} & 66.90\textsubscript{1.00} & 65.83\textsubscript{0.24} & 80.67\textsubscript{2.49}& 50.50\textsubscript{1.63}& 64.75\\
 \midrule
 \ours~(Ours) & \textbf{81.00}\textsubscript{2.45} & \textbf{91.68}\textsubscript{0.14} & \textbf{66.53}\textsubscript{0.38} & 43.67\textsubscript{1.21} & \textbf{76.69}\textsubscript{0.50} & \textbf{78.00}\textsubscript{0.82} & \textbf{84.67}\textsubscript{0.47} & \textbf{72.17}\textsubscript{0.85} & \textbf{74.30}\\
 \bottomrule
\end{tabular}}
\end{table*}

\sparagraph{Setup}
\ours~integrates the state-of-the-art prompt optimizer, MIPRO~\citep{opsahl-ong-etal-2024-optimizing}, which optimizes both instructions and demonstrations for each agent via a Bayesian surrogate model. 
We limit the number of bootstrapped demonstrations to 3 and instruction candidates to 10, per agent in 10 rounds. In topology optimization for all tasks, we search for 10 different topologies via rejection sampling. Along with topology optimization, each topology is evaluated on the validation set 3 times to stabilize the prediction. The optimized MAS is then reported on the held-out test set over three runs. We set model temperature $T$ at 0.7, maximum output tokens at 4096, and the $t$ in Softmax at 0.05 for sharpening the selection probability $p_a$ for each search dimension. We implement the same LLM backbone as both evaluator and optimizer in all phases. 


\sparagraph{Main results}
We present the main results of \ours~compared to the baselines on the evaluation set in Table \ref{table:main}. \ours~yields substantial gains over common forms of multi-agent systems, (e.g. self-consistency, self-refine, and multi-agent debate), that scale up without optimizing prompts for agents in collaboration. \ours~leads to high-performing MAS: 78.8\% and 74.3\% on average on Gemini 1.5 Pro and Flash, respectively, where we observe consistent improvements on Claude 3.5 Sonnet as reported in Table \ref{tab:claude}. By comparing \ours~with~state-of-the-art automatic agent design baselines, ADAS and AFlow, we first notice that ADAS only brings subtle gains even by already conditioning its meta-agent generation based on the common forms of agents. The meta-agent keeps proposing complex topologies but without optimizing the prompt design. AFlow, on the other hand, demonstrates a competitive performance to \ours, especially on 2WikiMQA and HumanEval. 

\begin{figure}
  \begin{minipage}{0.485\textwidth}
    \centering
    \vspace{-2mm}
\includegraphics[width=\linewidth]{figs/5-prune.pdf}
    \vspace{-6mm}
    \caption{\textbf{Left}: average performance per optimization stage of \ours~over 8 evaluation tasks on Gemini 1.5 Pro. We compare \ours~with a single agent (\texttt{CoT}) starting point as the reference and an APO baseline that optimizes over the single agent by MIPROv2~\citep{opsahl-ong-etal-2024-optimizing}. Refer to App. \S\ref{app:additional} for the detailed ablation per task. \textbf{Right}: a comparative ablation study on topology optimization (\texttt{2TO}) without pruning and without the former stage of prompt optimization (\texttt{1PO}) evaluated on HotpotQA.}
    \label{fig:ablation}
\end{minipage}
\hfill
  \begin{minipage}{0.485\textwidth}
    \centering
    \vspace{-2mm}
    \includegraphics[width=\linewidth]{figs/6-opt.pdf}
    \vspace{-8.5mm}
    \caption{The optimization trajectories of \ours~compared to automatic agent design baselines per validation round on DROP. We note that, as a distinct advantage of \ours, the optimization within stages (1) \& (2) of \ours~can be completely parallelized, whereas ADAS and AFlow are iterative algorithms that have to wait to propose new agents until finishing earlier trajectories.}
    \label{fig:opt}
\end{minipage}
\end{figure}
We attribute the performance of AFlow to: 1) its `expansion' phase that generates new nodes based on an error log that contrasts the predictions with the ground truth, which provides implicit textual gradients \citep{pryzant-etal-2023-automatic} to reflect on any formatting errors in prompt design; 2) a more refined search space within a pre-defined set of operators. Though AFlow draws similar inspirations on the importance of search space design as \ours, it still lacks a phase of prompt optimization to \textit{optimize} its pre-defined operators properly, resulting in under-performance for MAS   search results at MATH and MuSiQue. Different from these baselines, the consistent improvements brought by \ours~highlight the importance of searching in both prompt and topology design space.


\sparagraph{Ablating optimization stages}
To understand the incremental gain per \ours~optimization stage, we provide a stage-by-stage ablation study in Fig.~\ref{fig:ablation}. We list the average performance of \ours~from block-level to workflow-level optimization and compare it with a single agent APO baseline, where the block-level optimization performance indicates the best-performing building block $a\in\mathcal{A}$ after APO. First, we notice that there is a large gain, 6\% on average, between block-level optimization and single-agent optimization, showing that MAS benefits substantially from having its agents optimized inside the building block. In addition, going from Stage (1) to (2), another 3\% gain can be achieved by composing influential topologies while searching the optimal configurations. Here, we provide an additional ablation on conducting Stage (2) without prompt optimization beforehand or without search space pruning. Fig.~\ref{fig:ablation} (right) shows that both of them are critical for effective search space exploration. Lastly, \ours~obtains further gains ($\sim$2\%) by conducting workflow-level prompt optimization on the best-found topology, which indicates that optimizing the prompts towards modeling the interdependence of agents is beneficial in the MAS design. 

\sparagraph{Cost-effectiveness of \ours} We conduct analysis on the cost-effectiveness of \ours. In particular, we visualize the optimization trajectory of \ours~as shown in Fig.~\ref{fig:opt}. \ours's trajectory demonstrates a steady trend of optimization that gradually improves the validation performance via interleaving the search towards better prompts and topologies. However, when it comes to automatic design baselines without explicit prompt optimization stages, AFlow is exposed to a larger variance in its optimization due to the nature of MCTS, whereas ADAS gets trapped in discovering over-complex topologies that appear to be less effective than the prompt design space. Overall, the optimization trajectory of \ours~highlights the importance of optimizing in an effective design space, where interleaved optimization further resolves the complexity with more consecutive rewards. Following Sec.~\ref{subsec:block-level}, \ours~also demonstrated advanced token-effectiveness, which we refer to Fig.~\ref{fig:pareto}.
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/7-demo.pdf}
    \vspace{-5mm}
    \caption{A demonstration of the optimization trajectory of \ours~on MATH. In \textcolor{stage1}{(1) block-level optimization}: multi-agent debate serves as the best-performing topology. In \textcolor{stage2}{(2) workflow \textbf{topology} optimization}, aggregating with more parallel agents outweighs the performance of agents in debate. Lastly, \textcolor{stage3}{(3) workflow-level optimization} discovers the optimal prompt conditioned on the best topology.}
    \label{fig:archs}
\end{figure*}

\sparagraph{Best-found MAS architectures \& Design principles}
We further inspect an example of optimized prompts and the trajectory of \ours~in discovering more effective topologies in Fig.~\ref{fig:archs}. The optimization starts from a zero-shot CoT agent, and soon \ours~in Stage (1) identifies the high-performing topology in debate with its optimized prompt. However, as found in Stage (2), aggregating with more parallel agents actually outweighs the multi-agent debate. Workflow-level prompt optimization then leads to the best-performing predictor for aggregation. The overall optimization flow sheds light on our guidelines for building effective MAS: 
1) optimizing individual agents properly is important before composing them into an MAS; 
2) more effective MAS can be built by composing influential topologies; 
and
3) modeling the interdependence between agents is beneficial, and can be achieved via workflow-level joint optimization.

















\section{Conclusion}
We approach designing effective MAS by first conducting a thorough analysis of the massive design space, revealing the crucial role of prompts, and identifying an influential subset of search space. Building on these findings, we introduce \ours, a novel multi-stage optimization framework that searches within a pruned design space, interleaving prompt and topology optimization to efficiently generate high-performing MAS. Our experiments demonstrate that \ours-optimized MAS significantly outperforms existing manual and automated approaches across an extensive set of tasks. Finally, based on the optimized systems discovered by \ours, we extract valuable design principles to guide the development of future effective LLM-based MAS. 

\section*{Acknowledgment}
We thank Jinsung Yoon and all other colleagues from Google Cloud AI Research for their valuable feedback.
\bibliographystyle{abbrvnat}
\nobibliography*
\bibliography{main}

\clearpage
\appendix
\section{Limitations and future work}

\ours~is a multi-agent design meta-framework also orthogonal to prompt and topology optimizers. \ours~has brought substantial improvements over a single agent design by searching in a customizable topology space. Though our proposed topology space has covered the vast majority of effective MAS designs, including serial, parallel, and mixture of connections, it is still likely that incorporating other topologies may further improve the final performance of \ours, which is complementary to the development of \ours. For instance, the debate topology proposed in \ours~involves a fully-connected topology across agents. Recent work has been identifying the sparsity of agent communications \citep{li-etal-2024-improving-multi, zhang2024cut}, and pruning redundant communications may further enhance the overall efficiency of the strongest \ours-found design. Though the topology optimizer in \ours~already traverses efficiently in the proposed topology space, incorporating more advanced search algorithms, such as the Bayes optimizer~\citep{kandasamy2018neural, ru2020interpretable}, may further improve the sample efficiency of \ours~when faces a more complex design space. Similarly, the sample efficiency of the prompt optimizer may be further enhanced by conditioning on textual feedback from error logs \citep{pryzant-etal-2023-automatic, wan2024teach}, which we will endeavor to explore in future work.



\section{Implementation details}
\label{appendix:details}

\subsection{Datasets}
In this work, we included the following dataset:
1) Hendryck's MATH~\citep{hendrycksmath2021} consisting challenging competition-level mathematics problems, and DROP~\citep{dua-etal-2019-drop} requires discrete and symbolic reasoning over paragraphs; 2) HotpotQA~\citep{yang-etal-2018-hotpotqa}, MuSiQue~\citep{trivedi-etal-2022-musique}, and 2WikiMultiHopQA~\citep{ho-etal-2020-constructing} to evaluate on information seeking from long-context with agentic systems, which we report from standardized versions in LongBench~\citep{bai-etal-2024-longbench}; 3) MBPP~\citep{austin2021program}, HumanEval~\citep{chen2021evaluating}, and LiveCodeBench~\citep{jain2024livecodebench} as well-established coding benchmarks. Regarding LiveCodeBench, we use the `test output prediction' task as an agent cooperative task. In line with AFlow~\citep{zhang2024aflow}, we use the public test cases of MBPP and HumanEval for the executor to retrieve reliable external feedback signals.

To save computation resources, we randomly sample a subset of the original validation and test splits to conduct all the experiments, where the specifications are reported in Table~\ref{tab:specification}. 
\begin{table}[H]
    \centering
    \caption{The specification of evaluation tasks: dataset split, topology search space, and the \ours-optimized MAS (on Gemini 1.5 Pro)}
    \resizebox{\textwidth}{!}{
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{llcccc}
    \toprule
     Task&  Type&  $|$Val$|$ &  $|$Test$|$  &  Topology Search Space & \ours\\
     \midrule
     MATH  & Mathematical Reasoning & 60  & 100&  \{\texttt{Aggregate}, \texttt{Reflect}, \texttt{Debate}\} & \{9, 0, 0\} \\
     DROP & Discrete Reasoning & 60 & 200 & \{\texttt{Aggregate}, \texttt{Reflect}, \texttt{Debate}\} & \{5, 0, 0\} \\
     HotpotQA & Long-context Understanding & 50 & 100 & \{\texttt{Summarize}, \texttt{Aggregate}, \texttt{Reflect}, \texttt{Debate}\} & \{0, 5, 0, 1\} \\
     MuSiQue & Long-context Understanding & 50 & 100 & \{\texttt{Summarize}, \texttt{Aggregate}, \texttt{Reflect}, \texttt{Debate}\} & \{0, 3, 0, 2\} \\
     2WikiMQA & Long-context Understanding & 50 & 100 & \{\texttt{Summarize}, \texttt{Aggregate}, \texttt{Reflect}, \texttt{Debate}\} & \{0, 3, 0, 1\} \\
     MBPP & Coding & 60 & 200 & \{\texttt{Aggregate}, \texttt{Reflect}, \texttt{Debate}, \texttt{Executor}\} & \{1, 4, 0, 1\} \\
     HumanEval & Coding & 50 & 100 & \{\texttt{Aggregate}, \texttt{Reflect}, \texttt{Debate}, \texttt{Executor}\} & \{1, 3, 0, 1\} \\
     LiveCodeBench & Coding: test output prediction & 100 & 200 & \{\texttt{Aggregate}, \texttt{Reflect}, \texttt{Debate}, \texttt{Executor}\} & \{3, 1, 1, 1\} \\
     \bottomrule
    \end{tabular}}
    \label{tab:specification}
\end{table}
\begin{table}[H]
    \centering
    \caption{The search dimension for each topology. The minimum topology defines the building block that \ours~Stage (1) optimized.}
    \resizebox{\textwidth}{!}{
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{28pt}
    \begin{tabular}{lccc}
    \toprule
      Topology   & Search Space & Minimum Topology Building Block & Specification\\
      \midrule
      \texttt{Summarize}   & \{0, 1, 2, 3, 4\} & \{\texttt{Summarizer}, \texttt{Predictor}\} & \{1, 1\}\\
      \texttt{Aggregate}   & \{1, 3, 5, 7, 9\} & \{\texttt{Predictor}, \texttt{Aggregator}\} & \{3, 1\}\\
      \texttt{Reflect}   & \{0, 1, 2, 3, 4\}& \{\texttt{Predictor}, \texttt{Reflector}\} & \{1, 1\}\\
      \texttt{Debate}   & \{0, 1, 2, 3, 4\}& \{\texttt{Predictor}, \texttt{Debator}\} & \{2, 1\}\\
      \texttt{Execute}   & \{0, 1\}& \{\texttt{Predictor}, \texttt{Executor}, \texttt{Reflector}\} & \{1, 1, 1\}\\
      \bottomrule
    \end{tabular}}
    \label{tab:search space}
\end{table}


\subsection{Baselines}
In this section, we report the specifications of all our baselines. We note that for the baselines: CoT, SC, Self-Refine, and Multi-Agent Debate, we follow the prompts given in ADAS~\citep{hu2024automated}. 

1) Chain-of-Thought (CoT) \citep{kojima2022large}. Direct chain-of-thought reasoning via zero-shot prompting: ``Please think step by step and then solve the task."

2) Self-Consistency (SC) \citep{wang2023selfconsistency}. In self-consistency, we generate diverse chain-of-thought reasoning traces with a temperature of 0.8, followed by a rule-based majority vote that collects the most consistent answer. In Table \ref{table:main}, we report SC@9 to provide a fair comparison across baselines.

3) Self-Refine \citep{madaan2024self}: This baseline consists of one predictor that constantly takes feedback and a self-reflector that provides criticism. It involves a stop criterion whenever the self-reflector outputs ``correct'' in its prediction. We set the maximum number of rounds of reflections to 5, such that the worst case will involve 11 ($1+2\times5$) calls.

4) Multi-Agent Debate \citep{du2024improving, liang-etal-2024-encouraging}. In this baseline, it involves 3 agents that conduct reasoning and debating for 3 rounds. The opinions along the rounds of debating are finally judged by an aggregator that makes the final prediction. Hence, it contains 10 ($3\times3 + 1$) agents. 

5) Automated Design of Agentic Systems (ADAS) \citep{hu2024automated}. Consistent with our main experimental setups. We use Gemini 1.5 as both LLM optimizer and evaluator for reproducing all ADAS results. The generation of ADAS is conditioned on former evaluations of baselines, including CoT, SC, Self-Refine, and Multi-Agent Debate. We report ADAS with 30 rounds of search, and each round is evaluated on the validation set 3 times to stablize the prediction.

6) AFlow \citep{zhang2024aflow}. Automatic workflow design via Monte-Carto Tree Search over a set of pre-defined operators. Similar to ADAS, AFlow also relies on an LLM optimizer to generate new nodes and topologies expressed in codes. However, we find the meta-prompt of AFlow does not generalize to other LLM backbones. Consequently, we report AFlow with its original LLM optimizer by Claude 3.5 Sonnet, and reproduce experiments with Gemini 1.5 Pro as the LLM executor. Therefore, the comparison is not completely fair, and we treat the results from AFlow as a good reference. We note that the `-' in Table \ref{table:main} refers to out-of-time errors, where the LLM executor has been trapped in executing accidental scripts with infinite loops. We still endeavored to report most results from AFlow as shown in Table \ref{table:main} \& Fig.~\ref{fig:opt} with the default experimental setup from AFlow: 20 rounds, 5 runs of validation per round, and k at 3.
\subsection{\ours: Multi-Agent System Search}
In this section, we provide additional details for \ours. The topology search space for each task is defined in Table~\ref{tab:specification}. In addition, for Stage (1) block-level prompt optimization, the specification of the building block is defined in Table~\ref{tab:search space}. We provide the visualization of both the minimum building blocks and the optimized topology in Fig.~\ref{fig:visual}. We refer the reader to App.~\S\ref{app:template}~\&~\S\ref{app:prompts} for the prompt templates we used to define each type of agent and the best prompts discovered. 
\begin{figure}[!t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=0.96\linewidth]{figs/8-visual.pdf}
    \caption{Visualization of the topology building blocks and best \ours-discovered topologies from Gemini 1.5 Pro.}
    \vspace{-1mm}
    \label{fig:visual}
\end{figure}
\section{Additional experiments}

\label{app:additional}
\begin{table*}[h] 
    \centering
    \caption{Results on the evaluation set with Claude 3.5 Sonnet. We keep the same experimental setup as Table \ref{table:main}. Since Claude 3.5 Sonnet does not support the same context window as Gemini, we report the standard HotpotQA instead of the LongBench. As we transfer the prompt template for each agent from Gemini to Claude, it is noticeable that the basic topology on some tasks may result in severe degradation of performance, and \ours~successfully recovers the performance and brings significant improvements over the initial agent. }
    \label{tab:claude}
    \resizebox{\textwidth}{!}{
    \setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lccccccc}
    \toprule
    \rowcolor{gray!20}
    \multicolumn{8}{c}{\texttt{Claude-3.5-Sonnet}} \\\midrule
    \textbf{Task} & \multicolumn{2}{c}{\textbf{Reasoning}} & \multicolumn{1}{c}{\textbf{Multi-hop}} & \multicolumn{3}{c}{\textbf{Coding}} &\\
       Method  &  MATH & DROP & HotpotQA & MBPP & HumanEval & LCB & Avg. 
   
    \\   \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-7} \cmidrule(lr){8-8}
    CoT & 57.33\textsubscript{0.94} & 55.52\textsubscript{0.42} & 23.56\textsubscript{1.52} & 67.50\textsubscript{1.47} & 88.67\textsubscript{1.70} & 72.67\textsubscript{2.39} & 60.21\\
    Self-Consistency & 61.67\textsubscript{1.89} & 57.86\textsubscript{0.45} & 25.69\textsubscript{0.44} & \textbf{69.17}\textsubscript{0.62} & 90.00\textsubscript{0.82} & 72.67\textsubscript{2.39} & 62.84\\
    Self-Refine & 57.00\textsubscript{1.63} & 56.26\textsubscript{0.56}& 23.57\textsubscript{2.56} & 68.00\textsubscript{0.82}& 87.00\textsubscript{1.41} & 49.33\textsubscript{1.65}& 56.86\\
    Multi-Agent Debate& 45.00\textsubscript{3.74} & 26.62\textsubscript{0.11} & 31.41\textsubscript{3.30} & 00.00\textsubscript{0.00} & 84.33\textsubscript{3.30} & 72.82\textsubscript{1.84} & 43.36\\
    \midrule
    \ours & \textbf{63.00}\textsubscript{0.00} & \textbf{68.93}\textsubscript{0.38} & \textbf{66.98}\textsubscript{0.99}&68.83\textsubscript{0.62}& \textbf{93.00}\textsubscript{0.82} & \textbf{73.73}\textsubscript{1.43}&\textbf{72.43}\\
    \bottomrule
    \end{tabular}}
\end{table*}

\begin{table*}[h]
\centering
\caption{The detailed ablation results per optimization stage of \ours. Practical gains can be obtained by further conducting workflow-level prompt optimization (3PO) on the best-found topology.}
\label{table:ablation}
\resizebox{\textwidth}{!}{
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccccccccc}
\toprule
\rowcolor{gray!20}
\multicolumn{10}{c}{\texttt{Gemini-1.5-pro-002}} \\
\midrule
\textbf{Task} & \multicolumn{2}{c}{\textbf{Reasoning}} & \multicolumn{3}{c}{\textbf{Multi-hop Long-context}} & \multicolumn{3}{c}{\textbf{Coding}} &\\
 Method    & MATH & DROP & HotpotQA & MuSiQue & 2WikiMQA & MBPP & HumanEval & LCB & Avg.\\
\cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-10}
 Base Agent & 62.33\textsubscript{0.94} & 71.65\textsubscript{0.61} & 56.96\textsubscript{1.26} & 43.32\textsubscript{0.13} & 49.20\textsubscript{0.61} & 68.83\textsubscript{0.85} & 89.33\textsubscript{1.70} & 66.33\textsubscript{2.09}& 63.54\\
 + APO & 79.33\textsubscript{1.89} & 77.51\textsubscript{0.38} & 59.72\textsubscript{0.00} & 43.97\textsubscript{0.00} & 61.49\textsubscript{0.24} & 67.00\textsubscript{1.08}& 86.33\textsubscript{1.25}& 68.50\textsubscript{1.22}& 67.44\\
 + 1PO & 80.00\textsubscript{0.00} & 86.45\textsubscript{0.90}& 62.52\textsubscript{1.86} & 48.86\textsubscript{0.61} & 67.40\textsubscript{0.58} & 80.33\textsubscript{1.25}& 91.67\textsubscript{1.25}& 76.00\textsubscript{0.00}& 74.56\\ 
 + 2TO & 83.00\textsubscript{1.63} & 86.75\textsubscript{1.32} & 65.22\textsubscript{1.34} & 52.61\textsubscript{0.52} & 72.82\textsubscript{0.86} & 85.00\textsubscript{1.08}& 92.00\textsubscript{0.82}& 81.33\textsubscript{0.00}& 77.55\\
 + 3PO & 84.67\textsubscript{0.47} & 90.52\textsubscript{0.64} & 69.91\textsubscript{1.11} & 51.40\textsubscript{0.42} & 73.34\textsubscript{0.67} & 86.50\textsubscript{0.41}& 91.67\textsubscript{0.47}& 82.33\textsubscript{0.85}& 78.40\\
 \bottomrule
\end{tabular}}
\end{table*}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{figs/9-pareto.pdf}
    \vspace{-2mm}
    \caption{The Pareto-front of \ours-optimized designs compared to multi-agent baselines. Total tokens include both inference input tokens and output tokens. Additional multi-agent baselines from ADAS~\citep{hu2024automated} and two best-found ADAS designs are included.}
    \label{fig:pareto}
\end{figure}
\section{Prompt template}
\label{app:template}
We provide all prompt templates we used for defining the \ours~search space. We use \texttt{<>} to enclose texts that have been skipped for presentation purposes. We follow the DSPy~\citep{khattab2024dspy} in constructing these agentic templates.

The general template for instruction, exemplar, and input/output fields:
\begin{lstlisting}[style=mystyle]

<Instruction>

---

Follow the following format.

Input: ${Input}
...
Output: ${output}

---

<example_1>

---

Input: <Input>
...
Output: <output>
\end{lstlisting}

MATH:
\begin{lstlisting}[style=mystyle]
Predictor:

Let's think step by step.
---
Question: ${question}
Reasoning: Let's think step by step in order to ${produce the answer}. We ...
Answer: ${answer}

------------
Reflector: 

Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correctness'.

---
Question: ${question}
Text: ${text}
Reasoning: Let's think step by step in order to ${produce the correctness}. We ...
Feedback: ${feedback}
Correctness: True/False indicating if answer is correct given the question.

------------
Refiner: 

Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better. Show your final answer bracketed between <answer> and </answer> at the end.

---
Question: ${question}
Previous answer: ${previous_answer}
Reflection: ${reflection}
Correctness: ${correctness}
Thinking: ${thinking}
Answer: ${answer}

------------

Debator:

These are the solutions to the question from other agents. Examine the solutions from other agents in your rationale, finish by giving an updated answer. Show your final answer bracketed between <answer> and </answer> at the end.

---
Question: ${question}
Solutions: the solutions to the question from other agents
Reasoning: Let's think step by step in order to ${Examine the solutions from other agents}. We ...
Answer: The updated answer for the question. Do not repeat Answer:
\end{lstlisting}

DROP:
\begin{lstlisting}[style=mystyle]
Predictor:

Please think step by step and then solve the task. # Your Task:
Please answer the following question based on the given context. 
---
Question: ${question}
Context: ${context}
Thinking: ${thinking}
Answer: Directly answer the question. Keep it very concise.

------------
Reflector: 

Verify that the answer is based on the provided context. Give your reflection in the rationale.

---
Question: ${question}
Context: ${context}
Text: ${text}
Reasoning: Let's think step by step in order to ${produce the correctness}. We ...
Correctness: True/False indicating if answer is correct given the observations and question.

------------
Refiner: 

Please think step by step and then solve the task. # Your Task:
Based on the reflection, correctness of the previous answer, and the context again, give an updated answer.

---
Question: ${question}
Context: ${context}
Previous answer: ${previous_answer}
Reflection: ${reflection}
Correctness: ${correctness}
Thinking: ${thinking}
Answer: Directly answer the question. Keep it very concise.

------------

Debator:

These are the solutions to the question from other agents. Based on the context, examine the solutions from other agents in your rationale, finish by giving an updated answer. 

---
Question: ${question}
Context: ${context}
Solutions: the solutions to the question from other agents
Reasoning: Let's think step by step in order to ${Examine the solutions from other agents}. We ...
Answer: The updated answer for the question. Do not repeat Answer:
\end{lstlisting}

HotpotQA, MuSiQue, and 2WikiMQA:
\begin{lstlisting}[style=mystyle]
Predictor:

Answer the question with information based on the context. Only return the answer as your output.
---
Question: ${question}
Context: ${context}
Answer: Only give me the answer. Do not output any other words.

------------
Summarizer:

Based on the question, retrieve relevant information from context that is ONLY helpful in answering the question. Include all key information. Do not repeat context.
---
Question: ${question}
Context: ${context}
Summary: Only generate the summary. Start with Summary: 

------------
Reflector: 

Verify that the answer is based on the provided context.

---
Question: ${question}
Context: ${context}
Text: ${text}
Reasoning: Let's think step by step in order to ${produce the correctness}. We ...
Correctness: True/False indicating if answer is correct given the observations and question.

------------

Debator:

These are the solutions to the question from other agents. Based on the context, examine the solutions from other agents in your rationale, finish by giving an updated answer. 

---
Question: ${question}
Context: ${context}
Solutions: the solutions to the question from other agents
Reasoning: Let's think step by step in order to ${Examine the solutions from other agents}. We ...
Answer: The updated answer for the question. Do not repeat Answer:
\end{lstlisting}

MBPP:
\begin{lstlisting}[style=mystyle]
Predictor:

Let's think step by step. Provide a complete and correct code implementation in python.
---
Question: ${question}
Thinking: ${thinking}
Answer: Only the code implementation. Do not include example usage or explainations.

------------
Reflector: 

Please determine the correctness of the solution in passing all test cases. If it fails, based on the error message and trackback, think step by step, carefully propose an updated solution in the answer output with a correct code implementation in python.

---
Question: ${question}
Previous solution: ${previous_solution}
Traceback: It contains the test cases, execution results, and ground truth. If there is an error, the relevant traceback is given.
Correctness: 'True/False' based on the correctness of executive feedback. If there is an error message, output 'False'
Thinking: ${thinking}
Answer: ${answer}

------------

Debator:

These are the solutions to the question from other agents. Examine the solutions from other agents in your rationale, finish by giving an updated answer. Let's think step by step. Provide a complete and correct code implementation in python.

---
Question: ${question}
Solutions: the solutions to the question from other agents
Reasoning: Let's think step by step in order to ${Examine the solutions from other agents}. We ...
Answer: ${answer}
\end{lstlisting}

HumanEval:
\begin{lstlisting}[style=mystyle]
Predictor:

Let's think step by step. Provide a complete and correct code implementation in python.
---
Question: ${question}
Thinking: ${thinking}
Answer: ${answer}

------------
Reflector: 

Please determine the correctness of the solution in passing all test cases. If it fails, based on the error message and trackback, think step by step, carefully propose an updated solution in the answer output with a correct code implementation in python.

---
Question: ${question}
Previous solution: ${previous_solution}
Traceback: ${traceback}
Thinking: ${thinking}
Answer: ${answer}

------------

Debator:

These are the solutions to the question from other agents. Examine the solutions from other agents in your rationale, finish by giving an updated answer. Let's think step by step. Provide a complete and correct code implementation in python.

---
Question: ${question}
Solutions: the solutions to the question from other agents
Reasoning: Let's think step by step in order to ${Examine the solutions from other agents}. We ...
Answer: ${answer}
\end{lstlisting}

LiveCodeBench:
\begin{lstlisting}[style=mystyle]
Predictor:

You are a helpful programming assistant and an expert Python programmer. The user has written a input for the testcase. Think step by step. You will generate the code based on the problem requirepement. You will calculate the output of the testcase and write the whole assertion statement in the markdown code block with the correct output.
---
Question: ${question}
Thinking: ${thinking}
Code: ${code}
Answer: complete the testcase with assertion.

------------
Reflector: 

If there is an executive output in the traceback, parse the output into an assertion in the answer given the executive output. 

---
Question: ${question}
Previous solution: ${previous_solution}
Traceback: It contains the test cases, execution results, and ground truth. If there is an error, the relevant traceback is given.
Correctness: 'True/False' based on the correctness of executive feedback. If there is an error message, output 'False'
Thinking: ${thinking}
Answer: ${answer}

------------

Debator:

These are the solutions to the question from other agents. Examine the solutions from other agents in your rationale, finish by giving an updated answer.

---
Question: ${question}
Solutions: the solutions to the question from other agents
Reasoning: Let's think step by step in order to ${Examine the solutions from other agents}. We ...
Answer: assert ${function(input)} == {executive_output}
\end{lstlisting}
\section{Best Prompts Discovered}
\label{app:prompts}
In this section, we show a few typical prompts that have been discovered by \ours. Similarly, we indicate \texttt{<>} that skips some comments.

MATH:
\begin{lstlisting}[style=mystyle]
Predictor:

Let's think step by step to solve the given problem. Clearly explain your reasoning process, showing all intermediate calculations and justifications.  Express your final answer as a single numerical value or simplified expression enclosed within <answer></answer> tags.  Avoid extraneous text or explanations outside of the core reasoning and final answer.

---

Follow the following format.

Question: ${question}
Reasoning: Let's think step by step in order to ${produce the answer}. We ...
Answer: ${answer}

---

Question: What is the average of the two smallest positive integer solutions to the congruence $$14u \\equiv 46 \\pmod{100}~?$$
Reasoning: Let's think step by step in order to find the average of the two smallest positive integer solutions to the congruence $14u \\equiv 46 \\pmod{100}$. We are looking for solutions to $14u \\equiv 46 \\pmod{100}$. We can divide the congruence by 2 to get $7u \\equiv 23 \\pmod{50}$. We want to find the inverse of 7 modulo 50. We use the Euclidean Algorithm: \\begin{align*} 50 &= 7 \\cdot 7 + 1 \\\\ 1 &= 50 - 7 \\cdot 7 \\end{align*} Thus, $7(-7) \\equiv 1 \\pmod{50}$, so $7^{-1} \\equiv -7 \\equiv 43 \\pmod{50}$. Multiplying $7u \\equiv 23 \\pmod{50}$ by 43 gives $u \\equiv 23 \\cdot 43 \\pmod{50}$. $23 \\cdot 43 = 989 = 50 \\cdot 19 + 39$, so $23 \\cdot 43 \\equiv 39 \\pmod{50}$. Therefore, $u \\equiv 39 \\pmod{50}$. The two smallest positive integer solutions are $u = 39$ and $u = 39+50=89$. The average of these two solutions is $\\frac{39+89}{2} = \\frac{128}{2} = 64$.
Answer: 64

---
\end{lstlisting}

In the following prompts, interestingly, we observe that including the data summary, task demonstrations, and past instructions that have been used in MIPRO \citep{opsahl-ong-etal-2024-optimizing} to propose new candidates actually improves the final performance. Hence, we keep these prompts that lead to strong task performance.

DROP:
\begin{lstlisting}[style=mystyle]
Predictor:

This dataset is designed for extractive question answering, focusing on retrieving concise, factual answers from short texts. Many questions involve extracting numerical information and performing simple calculations, suggesting applications in areas like sports analytics or financial data analysis. However, the dataset's Western cultural bias and lack of complex reasoning questions limit its generalizability and real-world applicability.

TASK DEMO(S):
<example_1>
Question: How many more points did the Spurs win by in Game 4 against the Mavericks?

Context:
The Mavericks finished 49-33, one game ahead of Phoenix for the eighth and final playoff spot, which meant that they would once again have to face their in-state rivals, the San Antonio Spurs, who were the top seed in the Western Conference with a 62-20 record. In Game 1 in San Antonio, Dallas had an 81-71 lead in the fourth quarter, but the Spurs rallied back and took Game 1, 85-90. However, the Mavs forced 22 turnovers in Game 2 to rout the Spurs 113-92, splitting the first two games before the series went to Dallas. In Game 3, Manu Gin\u00f3bili hit a shot that put the Spurs up 108-106 with 1.7 seconds left, but a buzzer-beater by Vince Carter gave the Mavs the victory, putting them up 2-1 in the series. The Spurs took Game 4 in Dallas 93-89 despite a late Dallas comeback after the Spurs at one point had a 20-point lead and later won Game 5 at home, 109-103, giving them a 3-2 series lead. The Mavs avoided elimination in Game 6 at home by rallying in the fourth quarter, winning 111-113. Game 7 was on the Spurs home court, and the Spurs beat the Mavericks 119-96, putting an end to the Mavericks season.

Thinking:
The Spurs scored 93 points in Game 4. The Mavericks scored 89 points in Game 4.  The difference is 93 - 89 = 4.
Answer: 4


BASIC INSTRUCTION:
```
You are a highly specialized AI tasked with extracting critical numerical information for an urgent news report.  A live broadcast is relying on your accuracy and speed. Think step-by-step, focusing on the numerical information provided in the context.  Then, answer the question concisely with the extracted numerical answer. Failure to provide the correct numerical information will result in the broadcast being interrupted.

Question: {question}
Context: {context}
```

TIP: Keep the instruction clear and concise.

PROPOSED INSTRUCTION:

```
Extract the numerical answer to the following question. Show your reasoning by identifying the relevant numbers from the provided context and performing any necessary calculations.  Respond with only the final numerical answer.

Question: {question}
Context: {context}
```
\end{lstlisting}

HotpotQA:
\begin{lstlisting}[style=mystyle]
Predictor:

This multi-passage question answering dataset focuses on complex questions requiring synthesis of information from multiple Wikipedia-like sources, often involving named entities and temporal reasoning. It emphasizes integrating information, handling ambiguity, and leveraging real-world knowledge, posing a significant challenge for models relying solely on provided text. The dataset appears well-suited for evaluating advanced language models' reasoning abilities across diverse domains and varying complexity levels.

TASK DEMO(S):
Question: The actor that plays Phileas Fogg in \"Around the World in 80 Days\", co-starred with Gary Cooper in a 1939 Goldwyn Productions film based on a novel by what author?
Context: Provided in prompt
Answer: Charles L. Clifford


BASIC INSTRUCTION: From the provided text, extract the answer to the question.  Output *only* the answer.

TIP: Keep the instruction clear and concise.  Emphasize reliance *only* on the provided text.

PROPOSED INSTRUCTION: Answer the question using only the provided context.  Do not use external knowledge.

---
<example_1>

------

Debator:

This multi-passage question answering dataset focuses on complex questions requiring synthesis of information from multiple Wikipedia-like sources, often involving named entities and temporal reasoning. It emphasizes integrating information, handling ambiguity, and leveraging real-world knowledge, posing a significant challenge for models relying solely on provided text. The dataset appears well-suited for evaluating advanced language models' reasoning abilities across diverse domains and varying complexity levels.

TASK DEMO(S):
Provided above.

BASIC INSTRUCTION: These are the solutions to the question from other agents. Based on the context, examine the solutions from other agents in your rationale, finish by giving an updated answer.

TIP: Don't be afraid to be creative when creating the new instruction!

PROPOSED INSTRUCTION: You are an expert fact-checker for a major publication. Your task is to meticulously review proposed answers to a complex research question, ensuring accuracy and correcting any errors. You are provided with the original question, multiple context passages from credible sources, and several proposed answers from different research assistants. Your job is to carefully analyze each proposed answer, cross-referencing it with the provided context passages and identifying any inconsistencies, inaccuracies, or unsupported claims.

**Question:** [Insert Question Here]

**Context Passages:**
[Insert Passages Here]

**Proposed Answers:**
* Assistant 1: [Insert Assistant 1's Answer]
* Assistant 2: [Insert Assistant 2's Answer]
...
* Assistant N: [Insert Assistant N's Answer]


**Instructions:**

1. **Fact-Check & Analyze:** Evaluate each proposed answer individually.  For each answer:
* **Verdict:**  Indicate whether the answer is \"Correct,\" \"Incorrect,\" \"Partially Correct,\" or \"Not Supported by Context.\"
* **Evidence:** Provide specific quotes and passage numbers from the context to support your verdict. Explain how the evidence supports or refutes the proposed answer.  Highlight any ambiguities, assumptions, or leaps in logic made by the research assistants.
* **Corrections\/Improvements (if applicable):**  Suggest specific corrections or improvements to partially correct or incorrect answers. Explain how these changes align with the context.

2. **Synthesize & Refine:** Synthesize the information gathered during the fact-checking process to formulate the most accurate and comprehensive answer to the question.  This may involve:
* Selecting the most accurate proposed answer.
* Combining elements from multiple proposed answers.
* Developing a completely new answer based on your analysis of the evidence.

3. **Final Answer:** Clearly state your final, verified answer to the question.

4. **Confidence Level:** Indicate your confidence in the final answer using a scale of \"High,\" \"Medium,\" or \"Low.\" Briefly explain the factors influencing your confidence level.


This revised instruction emphasizes a more rigorous fact-checking process, encouraging the LM to critically evaluate each proposed answer and provide detailed justifications for its assessments.  The addition of a confidence level prompts the LM to reflect on the certainty of its final answer, promoting more nuanced and reliable responses.  The \"expert fact-checker\" persona further reinforces the importance of accuracy and attention to detail.

---
<example_1>
<example_2>
\end{lstlisting}




MBPP:
\begin{lstlisting}[style=mystyle]
Predictor:

You are a highly skilled Python programmer tasked with generating a correct and efficient Python function based on the given natural language problem description.  Think step-by-step, outlining your reasoning process before presenting the code solution.  Your response should adhere to the following structure:

**Thinking:**  Provide a clear and concise breakdown of your thought process, including the steps you'll take to solve the problem.  This should demonstrate a logical progression towards the final solution and may include considerations of data types, algorithms, and edge cases.  For example:

1. Identify the input data type and expected output.
2. Determine the core logic or algorithm required.
3. Consider potential edge cases or special scenarios.
4. Outline the steps for implementing the solution in Python.

**Answer:**  Present your complete and correct Python code implementation within a code block (using triple backticks). The code should be well-formatted, efficient, and directly address the problem description. Ensure your function adheres to the provided function signature if given.  For example:

```python
def function_name(input_arguments):
# Code implementation here
# ...
return output
```

Focus on producing functional code that accurately solves the problem. Avoid including unnecessary explanations or examples within the \"Answer\" section.  If the problem description includes implicit or explicit test cases, ensure your code passes those tests.  Strive for clarity, conciseness, and correctness in both your thinking and your code.

---
<example_1>
<example_2>
<example_3>

------
Reflector:

This dataset is designed for Python code generation, translating natural language problem descriptions into simple functions and their corresponding test cases. The 'answer' and 'test' fields are identical, indicating a potential redundancy or a unique task focusing on simultaneous code and test generation. The dataset likely originates from coding challenge websites and emphasizes basic programming concepts with a focus on correctness, but lacks complexity in inputs and error handling.

TASK DEMO(S):
Question: Write a function that takes in two numbers and returns a tuple with the second number and then the first number.

def swap_numbers(a,b):
Previous Solution: def swap_numbers(a,b):
    return (b, a)

Traceback: Test case: print(swap_numbers(10,20))
Output: (20, 10)
Ground Truth: (20,10)
Correctness: True
Thinking: The provided solution correctly swaps the order of the two input numbers and returns them as a tuple. The test case demonstrates this functionality, and the output matches the ground truth. Therefore, no changes are required.
Answer: ```python
def swap_numbers(a,b):
    return (b, a)
```
<example_2>
<example_3>


BASIC INSTRUCTION: Please determine the correctness of the solution in passing all test cases. If it fails, based on the error message and trackback, think step by step, carefully propose an updated solution in the answer output with a correct code implementation in python.

TIP: The instruction should include a high stakes scenario in which the LM must solve the task!

PROPOSED INSTRUCTION:

You are an automated code reviewer for a mission-critical satellite control system.  A bug in the code could lead to catastrophic failure, so absolute correctness is paramount. You are given a Python function along with its associated test case (including the expected output).  Analyze the provided

<example_1>
<example_2>
\end{lstlisting}































\end{document}
