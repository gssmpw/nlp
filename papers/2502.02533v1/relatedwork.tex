\section{Related Work}
\sparagraph{Forms of LLM-based agentic systems}
The simplest form of an LLM-based agentic system involves a single agent that can dynamically interact and respond to the environment \citep{yao2023react}. Recent advances endow agents with diverse roles and tools \citep{wu2023autogen}, orchestrating multiple agents to cooperate with each other \citep{chen2024agentverse}.
Standard forms of agent cooperation (i.e., topology) often involve parallel and serial flows of information. The parallel form usually diversifies the exploration among many agents in parallel \citep{li2024more}, and self-consistency (SC) \citep{wang2023selfconsistency} is a representative way for scaling agents in parallel. The serial form aims to advance the exploitation of a task via a chain of agents, where LLMs can serve as reflective agents to self-justify and refine former predictions \citep{madaan2024self, shinn2024reflexion}. Later, the opinions from multiple agents can be summarized to retrieve the most consistent answer by an aggregation agent \citep{chen2024universal, lin-etal-2024-just}. Moreover, multi-agent debate consists of a more complex flow of information \citep{chen-etal-2024-reconcile, wang-etal-2024-rethinking-bounds, zhang-etal-2024-exploring}, and recent research shows that debating can elicit more truthful predictions \citep{khan2024debating, du2024improving}. Recent agent topology extends beyond the above connections \citep{wang2024mixture, qian2024scaling}, and \ours~can automatically search the best topology among the aforementioned spaces. 

\rparagraph{Automatic optimization for MAS}
Recent research starts automating agent design by interpreting agent functions as learnable policies \citep{zhang2024offline, zhang-etal-2024-agent} and synthesizing trajectories for agent fine-tuning \citep{qiao-etal-2024-autoact}. 
Going further from a single agent, automatic multi-agent optimization faces a higher level of complexity, thereby requiring a more sophisticated design of search space and algorithms. Among all recent advances in multi-agent optimization, the optimization space has spanned prompts~\citep{khattab2024dspy}, tools \citep{zhou2024symbolic}, workflows \citep{li2024autoflow}, and thinking strategies \citep{shang2024agentsquare}. Aligning closer to our topology search space, DyLAN~\citep{liu2024a} dynamically activates the composition of agents, and Archon~\citep{saad2024archon} frames MAS as a hyperparameter optimization problem. Neither of them has taken the important prompt space into account, where we demonstrated the importance of prompt optimization in Sec.~\ref{sec:prompt}. In addition, 
GPTSwarm~\citep{zhuge2024gptswarm} optimizes the connections between agentic nodes using a policy gradient algorithm. 
State-of-the-art automatic agent design methods, ADAS~\citep{hu2024automated} and AFlow~\citep{zhang2024aflow}, also attempt to optimize agentic workflows with advanced search algorithms and LLM as optimizers. However, we observe that the importance of proper prompt designs has been relatively under-studied in these prior works.