%!TEX root = ../BTactive.tex
\begin{figure*}[t!]
    \centering\vspace{-0.1cm}
    \includegraphics[width=0.88\linewidth]{Figs/flowchart2.png}\vspace{-0.25cm}
    \caption{\small Workflow of active reward modeling and experimental setups. At each round, we start with randomly sampling prompts, generating responses and candidate comparisons, active labeling, and model retraining. }\vspace{-0.25cm}
    \label{fig:flow_chart}
\end{figure*}
\section{Experiments with LLMs}
\subsection{Overall Setup} In this section, we test different design strategies in LLM applications. We start with discussions of general experiment setups and the evaluation metrics we used in experiments.

\textbf{Objective and Evaluation Metrics.}  We assess the data efficiency of various comparison selection methods. The main metrics are $1-$ Spearman's rank correlation and best-of-N test-time reward, as reward modeling aims to order responses correctly and select the best one during test time. The golden reward models from \citet{dong2024rlhf} serve as the surrogate for ground truth. Specifically, we consider
\begin{itemize}[nosep,leftmargin=*]
    \item \textbf{Batched Spearman's correlations}: we measure the ranking correlation within each test prompt across 500 generations~\citep{sun2024rethinking}. We took $1-$ Spearman's correlations as a test set metric. 
    \item \textbf{Best-of-N Reward}: we evaluate the best-of-N (N=500) reward on test prompts~\citep{gao2023scaling,gui2024bonbon}.
\end{itemize}
A method is considered superior if it achieves a smaller $1-$ Spearman's correlation, a larger Best-of-N reward, or the same performance with fewer annotations.


% \begin{figure*}[ht!]
%     \centering
%     \includegraphics[width=1.0\linewidth]{Figs/CompareAnnoBatch_gemma7b_64.png}
%     \caption{\small Investigating how annotation batch size choices affect learning performance of different methods. Model: Gemma 7B.}
%     \label{fig:results_annotation_bs_gemma7b}
% \end{figure*}

% \begin{figure*}[ht!]
%     \centering
%     \includegraphics[width=1.0\linewidth]{Figs/CompareAnnoBatch_llama38b_64.png}
%     \caption{\small Investigating how annotation batch size choices affect learning performance of different methods. Model: LLaMA3 8B.}
%     \label{fig:results_annotation_bs_llama38b}
% \end{figure*}

\textbf{Base Models, Annotations, and Golden Reward Models.} 
We conducted experiments using three open-source LLMs: Gemma2b, Gemma7b, and LLaMA3-8b~\citep{team2024gemma, meta2024introducing}. To ensure affordability, we followed methods from \citet{gao2023scaling, liu2023statistical, tran2024snorkel, dong2024rlhf, sun2024rethinking} to use open-source golden reward models as annotators. We used the Anthropic \texttt{Harmless} and \texttt{Helpful} datasets~\citep{bai2022training} that has been widely studied in reward modeling, and golden reward models are available~\citep{yang2024rewards, dong2023raft, dong2024rlhf}. The dataset includes 40k prompts with 10 responses each for training, and 2k prompts with 500 generations each for testing.

\textbf{Reward modeling.} To separate representation learning from reward modeling, we train our reward model using joint embeddings of prompts and responses. An MLP with three hidden layers and BT loss was used. Since the BT model is not identified up to a translation, we exclude bias in the final linear layer. Our ablation studies show that the size of hidden units does not significantly affect the results. For more details, see \cref{appdx:more_results_hyper_param_sensitivity}.


\textbf{Online Annotation Pipeline.} 
We train our model sequentially, increasing the sample size at each step. At the beginning of each step, we randomly draw 500 prompts. For each of the 500 prompts, we randomly select 2 out of 10 responses for in-prompt comparisons, yielding $500\times 45 = 22500$ potential comparisons. For cross-prompt annotations, there are approximately 25 million potential comparisons. We randomly sample a fix-sized subset $20000$ out of those potential comparisons for different algorithms to choose from, see \cref{fig:flow_chart}. 

At each online iteration, strategies that require model predictions use the reward model from the previous iteration. We test different \texttt{annotation batch sizes}, an important hyperparameter to tune, ranging from ${125, 250, 500, 1000}$ to understand performance across various settings. After annotation, we retrain the entire model and evaluate it after each re-training.
%We train our model in a sequential manner of increasing sample sizes. At each step of the sequential training, we randomly draw 500 prompts from the training set. For in-prompt comparisons, choosing 2 out of 10 training responses for \textit{each prompt} yields 45 potential comparisons. With 500 prompts, we have $500 * 45 = 22500$ candidates for annotation; for cross-prompt annotations, choosing 2 out of the 5000 yields many more candidates for annotation (i.e. $\approx$ 25 million). We randomly sample a fix-sized subset $20000$ out of those candidates for different algorithms to choose from, see \cref{fig:flow_chart}. %\textcolor{red}{hs: we should have a flow chart here.}

%At each online iteration, we used the reward model from the previous iteration for those strategies that need model predictions. 
%In such a workflow, the \texttt{annotation batch size} is an important hyper-parameter that controls the update frequency of models. To understand how different methods perform under various settings and provide insights for potential empirical applications of those methods, we tested different \texttt{annotation batch sizes} ranging from $\{125, 250, 500, 1000\}$. 

%After each iteration of annotation, new comparisons were annotated and joined to previously annotated comparisons. We then retrain the entire model from scratch.
%Models are evaluated after each (re-)training.



%\ys{pass paused here}

\subsection{Comparing Annotation Efficiency}
Figure~\ref{fig:results_main} presents results on the \texttt{Harmless} dataset (see Appendix~\ref{appdx:more_results_results_main}, Figure~\ref{fig:results_main_helpful} for \texttt{Helpful} results). \textbf{D-opt} and \textbf{Past-Aware D-opt} outperform other methods, demonstrating both superior performance and greater stability. In contrast, alternative approaches exhibit training instability and significantly higher variance during online learning.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.99\linewidth]{Figs/Main_xpromptFalse_hidden64_cand500_320_SPLIT_harmless.pdf} \vspace{-0.2cm}
    \caption{\small Comparing annotation efficiency of different methods. (\texttt{Harmless} Dataset, 3 Models, 8 Methods). First row: $1 -$ Spearman's Correlation (\textbf{lower is better}); second row: Best-of-N reward (\textbf{higher is better}). Experiments are repeated with 5 seeds.}
    \label{fig:results_main}
\end{figure*}

\subsection{Comparing Annotation Batch Sizes}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.99\linewidth]{Figs/CompareAnnoBatch_gemma2b_64_xpromptFalse_h=7.pdf} \vspace{-0.2cm}
    \caption{\small Investigating how annotation batch size choices affect learning performance of our methods. Model: Gemma 2B. The first two columns present results on the \texttt{Harmless} dataset, and the second two columns present results on the \texttt{Helpful} dataset. First row: $1 -$ Spearman's Correlation (\textbf{lower is better}); second row: Best-of-N reward (\textbf{higher is better}). The results presented are from 5 runs with different seeds.}\vspace{-0.2cm}
    \label{fig:results_annotation_bs_gemma2b}
\end{figure*}
In this section, we evaluate different methods under varying \texttt{annotation batch size} setups, ranging from $125$ to $1000$. Notably, our proposed methods are computationally efficient: since the reward model operates on embeddings, re-training a 3-layer MLP with 10k annotations takes only a few minutes on a GPU serverâ€”while human annotation is significantly more time-consuming.

Figure~\ref{fig:results_annotation_bs_gemma2b} presents results for the Gemma2B model (results for the other two base models are in Appendix~\ref{appdx:more_results_annotation_bs} due to space constraints). Overall, \textbf{D-opt} and \textbf{Past-Aware D-opt} consistently outperform other methods across different annotation batch sizes. Additionally, we observe performance improvements when using smaller batch sizes, corresponding to a more online setup. Given the low computational cost, this suggests a practical strategy: using small annotation batches with frequent model retraining to enhance annotation efficiency in reward model development.


\subsection{Results with Cross-Prompt Annotations}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.99\linewidth]{Figs/Main_xpromptTrue_hidden64_cand500_320_SPLIT_harmless.pdf}\vspace{-0.2cm}
    \caption{\small Comparing annotation efficiency of different methods under the \textbf{Cross-Prompt} annotation setups. (\texttt{Harmless} Dataset, 3 Models, 8 Methods). First row: $1 -$ Spearman's Correlation (\textbf{lower is better}); second row: Best-of-N reward (\textbf{higher is better}). Experiments are repeated with 5 seeds.}\vspace{-0.3cm}
    \label{fig:results_main_xprompt}
\end{figure*}
Cross-prompt annotation has been proposed as an alternative to in-prompt comparison, demonstrating superior performance in LLM alignment tasks~\citep[][see also \cref{app:futherdisc}]{yin2024relative,sun2024rethinking}. To assess the generality of our methods, we extend our experiments to cross-prompt setups and compare different approaches.

Figure~\ref{fig:results_main_xprompt} shows the results under cross-prompt annotation. \textbf{D-opt} and \textbf{Past-Aware D-opt} achieve significantly better performance in both annotation efficiency and alignment across tasks and base models.

Comparing Figure~\ref{fig:results_main_xprompt} with Figure~\ref{fig:results_main}, we observe efficiency gains across all methods, with the entropy-based approach exhibiting the most substantial improvement. Appendix~\ref{appdx:more_results_xprompt-in-prompt_comparison} provides a direct comparison between in-prompt and cross-prompt annotations for interested readers.


\subsection{Hyper-Parameter Sensitivity Analysis}
To examine the sensitivity of different methods to hyper-parameter choices and provide insights for real-world applications, we varied two key factors: \textbf{Candidate Number} and \textbf{Hidden Dimension of Reward Model MLPs} across different active reward modeling designs.

Our sensitivity analysis reveals that all algorithms remain robust and are largely insensitive to specific hyper-parameter choices in our embeddings-as-inputs setup. Detailed results are provided in Appendix~\ref{appdx:more_results_hyper_param_sensitivity}.










