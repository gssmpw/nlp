\textbf{Designing comparisons.} Our experiments show that applying the classic method to the last-layer features yields strong performance and stability. The D-opt method is also highly efficient, as its information criteria and optimization procedure are largely analytical, enabling real-time pair selection. This is valuable when collecting user preferences in a user interface without introducing significant latency. Additionally, this approach might be adapted to other model architectures, including e.g., vision-language models.

\textbf{An Empirical Bayes View and Stability of Last-Layer Design.} The connection between the last-layer D-optimal method and BALD can be seen by considering previous layers as a transformation of the Gaussian prior for the last layer's weights. These previous layer weights act as hyperparameters of the prior, which are fitted using maximum likelihood, akin to an empirical Bayes procedure \citep{deely1981bayes}. By minimizing posterior entropy, we perform D-optimal design followed by Gaussian approximation after the transformation. Empirical Bayes helps reduce the subjectivity and uncertainty in selecting priors, potentially explaining the robustness of our method compared to full Bayesian approaches like batchBALD, which involve hyper-priors on these hyperparameters. 
%\textcolor{red}{hs: will we have an extended discussion here?}

%\textbf{Cross-prompt comparisons.} It is natural to ask if cross prompt is valid. If we assume the existence of a scalar reward function guiding human comparisons, then cross-prompt annotation is feasible since each prompt-response pair is assigned a comparable real value. A minor wording change in a prompt, without altering its meaning, is unlikely to affect which responses are deemed helpful or harmful, making these pairs comparable even across prompts.

%However, if the reward function varies sharply with prompt changes, its transferability across prompts becomes unreliable, making it difficult to improve responses for one prompt using a reward function learned from others. Still, if prompts lie on a lower-dimensional manifold and the reward function exhibits some regularity in that space, cross-prompt annotations could help better capture these dependencies.

\textbf{Classic Experimental Design Methods in the Foundation Model Era.} We conjecture that the success of using the last layer in classical experimental design stems from the fact that the embeddings are already close to linear features. Given the extensive study of experimental design in generalized linear models \citep[see e.g.,][]{atkinson2007optimum, pukelsheim2006optimal}, we believe it is a general strategy to apply these methods to the last layer of deep learning models, particularly when leveraging learned representations from foundation models.

\textbf{Limitations and future directions.} Our proposed active learning scheme relies on a well-trained embedding model. The effectiveness of selecting comparisons based on last-layer features depends on these features being informative, which might in turn requires signal-rich embeddings with low noise as input of the reward model (which is MLP in our experiment). An interesting question is whether embeddings that better capture human values (and thus improve reward modeling) differ fundamentally from those optimized for generation. A related consideration is whether reward modeling in LLMs should start from embedding or ealier. 