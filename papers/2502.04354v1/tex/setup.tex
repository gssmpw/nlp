%!TEX root = ../BTactive.tex

\textbf{Reward modeling in alignment.} Reinforcement learning is a key technique for aligning LLMs to ensure their safe and effective deployment~\citep{christiano2017deep, ouyang2022training, stiennon2020learning}. The most prevailing approach, RLHF, relies on reward models as a fundamental mechanism for quantifying content quality and scaling the reinforcement learning~\citep{lambert2024rewardbench, wang2024secrets}. During fine-tuning and deployment, reward models serve as proxies for human evaluators~\citep{dubey2024llama, dong2024rlhf, wang2024arithmetic}, assessing how well LLM outputs align with human intent.
%\textcolor{red}{@hs: can you add a bit here?}
Despite significant progress, reward modeling remains challenging due to the scarcity and inaccuracy of annotations~\citep{lambert2024rewardbench,wang2024secrets,gao2023scaling}. Prior research has attempted to mitigate these challenges through different aspects when learning from a fixed set of annotations~\citep{wang2024arithmetic,winata2024metametrics,liu2024skywork,lou2024uncertainty,coste2023reward,zhang2024overcoming}. While \citet{xiong2023gibbs,dong2024rlhf} demonstrate that online annotations are more efficient in RLHF, the topic of online annotation prioritization strategy remain under-explored except for heuristic designs~\citep{muldrew2024active}.


\textbf{Bradley-Terry model for reward modeling.}
A canonical model used for reward modeling from binary preference data is the Bradley-Terry (BT) model~\citep{bradley1952rank}, or more precisely, its regression variant~\citep{sun2024rethinking}. In the most general setting, which allows for cross-prompt comparisons, a human annotator is presented with two pairs of prompts and responses, $(\prompt_{\pairidx,1}, \response_{\pairidx,1})$ and $(\prompt_{\pairidx,2}, \response_{\pairidx,2})$. The annotator then provides a preference, $\preference{\pairidx}=1_{\{(\prompt_{\pairidx,1}, \response_{\pairidx,1})\succ (\prompt_{\pairidx,2}, \response_{\pairidx,2})\}}$ indicating whether the first pair is preferred over the second.

Often, both responses correspond to the same prompt, i.e., $\prompt_{\pairidx,1}=\prompt_{\pairidx,2}$ however, Bradley-Terry regression can operate without this assumption. The model regresses these annotations onto an embedding $\nonlinembdplain(\prompt_{\pairidx,1}, \response_{\pairidx,1})$. This is a mild assumption since these embeddings can be, for example, a concatenation of word embeddings, the output of tokenizers, or the output embedding of an LLM. 

When there is no risk of confusion, we denote the embeddings of pair $\pairidx$ as $\nonlinembd{\pairidx}{1} \in \R^D$ and $\nonlinembd{\pairidx}{2}\in \R^D$, with a reward function $\truereward\in \R^D\to \R$. The goal is to learn this function from annotations. In the BT model, we assume that
\begin{equation}
    \preference{\pairidx}\sim \BER\left(\sigma[\truereward(\nonlinembd{\pairidx}{1}) - \truereward(\nonlinembd{\pairidx}{2})]\right)
\end{equation}
with $\sigma$ being the sigmoid function. 

\textbf{Active learning.} In a typical active learning setting, we have a labeled dataset, $\labeledset_\alstepidx={(\prompt_{\pairidx,1}, \response_{\pairidx,1}, \prompt_{\pairidx,2}, \response_{\pairidx,2}, \preference{\pairidx})}_{\pairidx=1}^{\totalpairs_\alstepidx}$ at step $\alstepidx$, and a typically large pool of unlabeled data to be chosen from, $\poolset_\alstepidx={ (\prompt_{\pairidxalt,1}, \response_{\pairidxalt,1}, \prompt_{\pairidxalt,2}, \response_{\pairidxalt,2})}$. The goal is to select a small subset $\chosenset_{\alstepidx} \subset \poolset_\alstepidx$, subject to certain constraints, for labeling. Once labeled (denoted as $\tilde\chosenset_{\alstepidx}$), this subset is added to the labeled dataset to train the next iteration of the model.

We also consider this process in the embedding space, where the labeled and unlabeled sets are given by $\labeledset_\alstepidx=\{(\nonlinembd{\pairidx}{1}, \nonlinembd{\pairidx}{2}, \preference{\pairidx})\}_{\pairidx=1}^{\totalpairs_{\alstepidx}}$ and $\poolset_{\alstepidx}=\{(\nonlinembd{\pairidxalt}{1}, \nonlinembd{\pairidxalt}{2})\}_{\pairidxalt=1}^{J_{\alstepidx}}$. A typical model-based active learning procedure is outlined in \cref{alg:activelearning}. In this work, we focus on identifying the best-performing scoring rules.
\begin{algorithm}
\caption{Model-based active learning}
\label{alg:activelearning}
\begin{algorithmic}[1]
\REQUIRE initial labeled dataset $\labeledset_{0}$, pool set $\poolset_{0}$, model $\model{0}$, a scoring rule $\scoringrule$, budget constrain $\budget$, and number of rounds $n$
\STATE \textbf{RETURN} Last trained model $\model{n}$
\FOR{$\alstepidx \gets 1$ to $n$}
    \STATE generate pool $\poolset_{\alstepidx}$
    \STATE $\chosenset_{\alstepidx}\gets \argmax_{\chosenset\subset \poolset_{\alstepidx-1},|\chosenset|\le \budget}\scoringrule(\model{\alstepidx-1}, \chosenset, \labeledset_{\alstepidx-1})$
    \STATE get labeled dataset $\tilde \chosenset_{\alstepidx}$
    \STATE $\labeledset_{\alstepidx}\gets \tilde \chosenset_{\alstepidx} \cup \labeledset_{\alstepidx-1}$
    \STATE train model $\model{\alstepidx}$ using $\labeledset_{\alstepidx}$
\ENDFOR

\STATE \textbf{return} $\model{n}$
\end{algorithmic}
\end{algorithm}

\textbf{Related work.}
\citet{muldrew2024active} considered active learning and proposed a strategy that combines entropy with model certainty (which is equivalent to the maxdiff strategy in our notation). For non-binary data, \citet{mukherjee2024optimal} suggested maximizing the determinant of the feature matrix. BatchBALD~\citep{kirsch2019batchbald} is a general-purpose active learning algorithm that requires a Bayesian model. The scoring in this method aims to maximize the expected entropy reduction by selecting the most informative data points. Experimental design for generalized linear models has been extensively studied in the classical statistical literature, with logistic regression serving as a key example~\citep[see e.g.,][]{chaloner1995bayesian, sener2017active}. Under the assumption of a linear reward function, the Bradley-Terry (BT) model simplifies to logistic regression. 