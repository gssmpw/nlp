The safe and successful deployment of Large Language Models (LLMs) across various application domains requires alignment with human values. Current research working on LLM alignment mainly focuses on reinforcement learning from human feedback (RLHF)~\citep{christiano2017deep,stiennon2020learning,ouyang2022training,bai2022training}, which rely on preference-based annotations provided by human annotators~\citep{bai2022constitutional}. However, obtaining human feedback can be expensive, and the noisy and binary nature of such data often limits its information density, posing a challenge for effective reward modeling~\citep{wang2024secrets,liu2024skywork}.

Active learning, where the model queries most informative labels based on its current state, offers a potential solution. It typically involves three key components: an initial model, a query strategy --- often in the form of maximizing a scoring function over unlabeled data, and a pool of unlabeled data. The model selects a subset of data for labeling and retrains iteratively until a stopping criterion is met.

In this work, we study the problem of active data acquisition in reward modeling. 
Technically, we introduce various scoring rules inspired by both \textit{classical experimental design}~\citep{chaloner1995bayesian} and recent deep learning-based advancements~\citep{sener2017active, houlsby2011bayesian, kirsch2019batchbald}. We adapt those methods to the learning of Bradley-Terry (BT) reward models~\citep{bradley1952rank}, which have been successfully applied in large-scale alignment practices~\citep{ouyang2022training, touvron2023llama} and proven to be theoretically sound~\citep{sun2024rethinking}.

We benchmark $8$ scoring algorithms using $2$ datasets and $3$ LLMs, ranging in size from 2B to 8B, and evaluate a wide range of active learning setups. Our results show that two classical experimental design methods --- applied to the final linear feature layer of deep models --- achieve state-of-the-art performance and strong stability across different setups, model architectures, and datasets.

Our main contributions can be summarized as follows:
\begin{itemize}[nosep, leftmargin=*]
    \item Formally, we characterize the problem of optimal preference label annotation using embedding space BT regression framework and establish connections between active learning and classical experimental design literature under the BT context.
    \item Methodologically, we introduce a set of algorithms inspired by classical experimental design literature, adapt them for deep BT regression models, and develop an efficient gradient approximation for the associated combinatorial optimization challenge in large-scale alignment problems.
    %compare them with methods commonly used in the deep learning community.
    \item Empirically, we evaluate different methods for preference label annotation across diverse setups, datasets, and base models. Our results suggest that applying classical experimental design techniques to the final layer of a deep neural network yields strong performance and stability.
\end{itemize}



%BatchBALD \citep{kirsch2019batchbald} requires a full Bayesian model. Inspired by Bayesian layer literature \citep{tran2019bayesian}, we consider simplifying the requirement to treating only the last (linear) layer as Bayesian and apply classic Bayesian experimental design theory \citep{chaloner1995bayesian}. 