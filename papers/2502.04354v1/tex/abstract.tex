% Reward modeling from human preference is a crucial step in reinforcement learning with human feedback and large language model alignment. Since having humans annotate can be expensive, researchers aim to find methods that can effectively select comparisons to show. In this work, we adapted acquisition rules from both deep learning and classic statistics literature. We suggest an ideal comparison dataset should simultaneously explore a large part of the representation space while comparing pairs that are not too far in reward to get informative comparisons. We test different methods by choosing comparisons using various open-sourced large language models and datasets. We proposed to use Fisher-information based acquisition rules adapted from classic statistical experimental design theory applied to the last linear layer of deep neural networks. Further, we found when cross-prompt comparisons are possible, having them can enhance reward modeling, especially when active learning strategies are used. 


% HS + GPT version
Building neural reward models from human preferences is a pivotal component in reinforcement learning from human feedback (RLHF) and large language model alignment research. Given the scarcity and high cost of human annotation, how to select the most informative pairs to annotate is an essential yet challenging open problem. 
In this work, we highlight the insight that an ideal comparison dataset for reward modeling should balance \textit{exploration of the representation space} and make \textit{informative comparisons} between pairs with moderate reward differences. Technically, challenges arise in quantifying the two objectives and efficiently prioritizing the comparisons to be annotated. To address this, we propose the Fisher information-based selection strategies, adapt theories from the \textit{classical experimental design} literature, and apply them to the final linear layer of the deep neural network-based reward modeling tasks. Empirically, our method demonstrates remarkable performance, high computational efficiency, and stability compared to other selection methods from deep learning and classical statistical literature across multiple open-source LLMs and datasets.
Further ablation studies reveal that incorporating cross-prompt comparisons in active reward modeling significantly enhances labeling efficiency, shedding light on the potential for improved annotation strategies in RLHF.