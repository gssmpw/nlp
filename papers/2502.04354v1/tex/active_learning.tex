%!TEX root = ../main.tex

\subsection{Linear BT Regression.}
Consider a simplified case where the true reward function is linear with respect to some intermediate embedding, $\truereward(\lastembd{\pairidx}{1}) = \lastembd{\pairidx}{1}^\top \lastweight$, for weight vector $\lastweight$. We use $\embd$ instead of $\nonlinembdplain$ because the reward may not be linear with respect to the original embedding $\nonlinembdplain$ used in reward modeling, and we wish to avoid confusion. The subscript $-1$ in $\lastweight$ reflects how we will apply these results in practice: $\embd$ represents the output before the final linear layer, and $\lastweight$ corresponds to the weight of this last layer. For now, we assume that this linear feature $\embd$ is known to us. Note that there is no bias term because linear BT is identified only up to translation. 

Under this simplified setting the preference generating process of $\pairidx$th pair $\preference{\pairidx}$ can be simplified to 
\begin{equation}
    \preference{\pairidx} \sim \BER[\sigmoid[(\lastembd{\pairidx}{1}-\lastembd{\pairidx}{2})^\top\lastweight]]
    \label{eq:BT}
\end{equation}
It can be observed that this corresponds to a logistic regression, where the covariates are the difference $\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2}$.

By applying the theory from generalized linear models, we know that the maximum likelihood estimate $\hat{\lastweight}$ is asymptotically Gaussian distributed, with mean $\lastweight$ and covariance matrix $\fisherinfo^{-1}$, where $\fisherinfo$ denotes the Fisher information (FI) matrix \citep[see e.g., ][Ch. 4.5.2]{shao2008mathematical}. For the linear Bradley-Terry model, the FI is
\begin{equation}
    \fisherinfo=\sum_{\pairidx=1}^\totalpairs (\lastembd{\pairidx}{1}-\lastembd{\pairidx}{2})(\lastembd{\pairidx}{1}-\lastembd{\pairidx}{2})^\top p_{\pairidx}(1-p_{\pairidx})
    \label{eq:FI}
\end{equation}
Where $p_{\pairidx} = \sigmoid[(\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2})^\top \lastweight]$, it can be observed that $p_{\pairidx}(1 - p_{\pairidx})$ represents the variance of a Bernoulli random variable.

The Fisher information matrix can be interpreted as the metric tensor in a Riemannian manifold of distributions, where the distance between them is given by the symmetrized KL divergence \citep{costa2015fisher}. FI quantifies the amount of information in the dataset for estimating the parameters $\lastweight$. From a Bayesian perspective, the Bernstein-von Mises theorem \citep[][Ch. 10.2, Thm 10.1]{van2000asymptotic} states that $\fisherinfo^{-1}$ is also the asymptotic covariance matrix of the posterior distribution of $\lastweight$, assuming mild regularity conditions on the prior.

The FI can be viewed as a sum over all independent data points' contribution. For each data point, there are two terms multiplied together: the empirical covariance of embedding differences $(\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2})(\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2})^\top$, and $p_{\pairidx}(1 - p_{\pairidx})$, the variance of the comparison results. \citet{sun2024rethinking} suggested that improving the variance of comparisons can be interpreted as improving annotation quality which can also be seen from FI. 

To make the FI large \cref{eq:FI} an ideal comparison should exhibit both a large variance in the embedding difference (thus $(\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2})(\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2})^\top$ having large eigenvalues) and a high variance in the comparison outcomes (thus $p_{\pairidx}(1 - p_{\pairidx})$ large). This implies that the embedding space should be diverse, such that $\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2}$ captures a wide range of differences, and each comparison should be informativeâ€”not too close to 0 or 1. The former encourages exploration within the embedding space, leading to a better regression model, while the latter ensures that comparisons are not trivial, improving sample efficiency. An everyday analogy for comparing non-obvious pairs would be that comparing a world champion to a newbie in chess offers little insight into the abilities of either player.

The FI plays a crucial role in the classical theory of experimental design, both in frequentist and Bayesian frameworks, as highlighted by the Bernstein-von Mises theorem. This leads to a family of design strategies known as alphabetical designs \citep{chaloner1995bayesian, pukelsheim2006optimal}. 

\textbf{(Bayesian) D-optimality \citep{chaloner1995bayesian}.}
%In classic statistics and experimental design literature, one strategy is the use of so-called alphabetical designs \citep{chaloner1995bayesian, pukelsheim2006optimal}, 
The alphabetical designs focus on the (co)variance of either estimating weights $\lastweight$ or making predictions under new embeddings, typically summarized through the covariance matrix. For example, the D-optimal design minimizes the determinant of the (asymptotic) covariance matrix of the last layer weights, $\lastweight$. Since $|\fisherinfo^{-1}| = 1 / |\fisherinfo|$, this is equivalent to maximizing the determinant of the FI.

The Bayesian variant of D-optimal involves having prior contribution, such as maximizing $|\fisherinfo + I/\sigma^2|$, where $I$ is the identity matrix, to avoid a determinant of zero. This corresponds to the inverse covariance matrix of the Laplace approximation of the posterior of $\lastweight$, assuming a normal prior with variance $\sigma^2$.

A plug-in estimator of $p_{\pairidx}$, $\hat{p}_{\pairidx}$, using the current best model, can be used to estimate the FI \citep{chaloner1995bayesian, pukelsheim2006optimal}. In this approach, the scoring rule is the determinant of the Fisher Information matrix.
\begin{equation}
    \scoringrule_{\dopt}(\chosenset) = \lvert \sum_{\pairidx\in \chosenset}  (\lastembd{\pairidx}{1}-\lastembd{\pairidx}{2})(\lastembd{\pairidx}{1}-\lastembd{\pairidx}{2})^\top \hat{p}_{\pairidx}(1-\hat{p}_{\pairidx})\rvert
\end{equation}
In experiments, we refer to this strategy as \texttt{D-opt}. Other forms of optimality also exist, each targeting different summaries of the Fisher Information (FI), such as A-optimality, which focuses on minimizing the trace of $\fisherinfo^{-1}$. When the prediction of a new, known embedding is the primary concern, G-optimality aims to minimize the variance of predictions on new embeddings. %In this case, the criterion can be expressed as $(\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2})^\top \fisherinfo^{-1} (\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2})$.

In this work, we suggest using D-optimality because it avoids the need to invert the FI, as required in A-optimality, and doesn't require specifying which samples to predict, as in G-optimality. For readers interested in further details, we refer to \citet{pukelsheim2006optimal} (Ch. 9).

The D-optimality strategy can be made a past-aware version by incorporating previously collected data. The asymptotic covariance of the full data-conditioned posterior is then $(\fisherinfo_{\text{past}} + \fisherinfo)^{-1}$, where $\fisherinfo_{\text{past}}$ is computed using prior data and \cref{eq:FI}. This approach relates to Bayesian methods like Bayesian active learning by disagreement (BALD) \citep{houlsby2011bayesian}, which minimizes posterior entropy. Since Gaussian entropy is proportional to the log-determinant of its covariance. In our experiments, we refer to this variant as \texttt{PA D-opt}.

Next, we review some other strategies that can be applied to BT models.

\textbf{Entropy sampling \citep{settles2009active, muldrew2024active}.}
This strategy aims to select samples about which the current model is most uncertain \citep{settles2009active}. In the context of binary preference modeling, this corresponds to choosing data whose predictions $\hat{p}_{\pairidx}$ are closest to 0.5, effectively exploring the level set of the reward. This is similar to a binary classification problem where the goal is to explore the decision boundary. This approach was also proposed by \citet{muldrew2024active} as maximizing predictive entropy. The scoring rule is then,
\begin{equation}
    \scoringrule_{\levelset}(\chosenset)=\sum_{\pairidx\in \chosenset} \left[-\hat{p}_{\pairidx}\log \hat{p}_{\pairidx} -(1-\hat{p}_{\pairidx})\log(1-\hat{p}_{\pairidx})\right]
\end{equation}
Since the entropy of a Bernoulli distribution reaches its maximum when $p = 0.5$, this approach is equivalent to selecting the top $\budget$ pairs where the predicted probability is closest to 0.5. In our experiments, we refer to this method as \texttt{Entropy}.

\textbf{Maximum difference \citep{muldrew2024active}.}
Contrasting with entropy sampling, this strategy focuses on comparing samples that the current reward model predicts to be the best and the worst, corresponding to probabilities close to 0 or 1. This approach was used by \citet{muldrew2024active} to measure model certainties. The scoring rule to be maximized can thus be interpreted as difference in estimated reward $|\hat{\truereward}_{\pairidx,1}-\hat{\truereward}_{\pairidx,2}|$.
\begin{equation}
    \scoringrule_{\maxdiff}(\chosenset)=\sum_{\pairidx\in \chosenset} |\hat{\truereward}_{\pairidx,1}-\hat{\truereward}_{\pairidx,2}|%\left[\hat{p}_{\pairidx}\log \hat{p}_{\pairidx} +(1-\hat{p}_{\pairidx})\log(1-\hat{p}_{\pairidx})\right]
\end{equation}
This strategy encourages exploration in the \textit{reward} space rather than the embedding space. It is sometimes used in active learning when the goal is to identify positive examples rather than the best classification \citep{settles2009active}. This justifies its use in reward modeling, where the goal is to obtain responses that yield better rewards in downstream tasks. In our experiments, we refer to this method as \texttt{Maxdiff}.

\textbf{Optimizing design matrix \citep{mukherjee2024optimal}.} 
This strategy focuses on finding the best collection of embeddings, or the design matrix in statistics terms $\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2}$, without looking at model predictions. A common objective is to optimize the covariance matrix of the designs, $\Sigma = \sum_{\pairidx=1}^\totalpairs (\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2})(\lastembd{\pairidx}{1} - \lastembd{\pairidx}{2})^\top$. One approach is to maximize the determinant of $\Sigma$, $|\Sigma|$, which encourages exploration over a large space of embedding differences. In fact, if we assume a linear regression model with additive Gaussian noise instead of logistic regression, this covariance matrix corresponds to the Fisher Information matrix of the regression coefficients, and this strategy aligns with the D-optimal design. The scoring rule is
\begin{equation}
    \scoringrule_{\XtX}(\chosenset) = \lvert \sum_{\pairidx\in \chosenset} (\lastembd{\pairidx}{1}-\lastembd{\pairidx}{2})(\lastembd{\pairidx}{1}-\lastembd{\pairidx}{2})^\top\rvert
\end{equation}
\citet{mukherjee2024optimal} used a similar strategy for a different type of preference data that is not purely binary. In our experiments, we refer to this method as \texttt{det(XtX)}, for the determinant of $X^\top X$.

\textbf{Coreset \citep{huggins2016coresets,munteanu2018coresets}.}
Instead of minimizing uncertainty in parameter estimations, the Coreset strategy aims to find a small subset of samples such that the trained model closely approximates the one trained on the full dataset, effectively transforming the problem into a sparse approximation task on weighting data points. The Coreset method for logistic regression has been studied recently by \citet{munteanu2018coresets} and \citet{huggins2016coresets} in both frequentist and Bayesian settings. In our experiment, we adopted the method of \citet{huggins2016coresets}. The scoring rule does not have a simple closed-form solution, so we refer interested readers to \citet{huggins2016coresets} and denote it as $\scoringrule_{\coreset}$. In our experiments, we refer to this method as \texttt{Coreset}.

\textbf{BALD and batchBALD \citep{houlsby2011bayesian, kirsch2019batchbald}.} When transitioning from frequentist to Bayesian framework, BALD \citep{houlsby2011bayesian} and BatchBALD \citep{kirsch2019batchbald} select data with high mutual information between the candidate batch's prediction and model parameters, making the data more informative. \citet{houlsby2011bayesian} showed that this approach maximizes expected posterior entropy reduction. This strategy applies to preference learning \citep{houlsby2011bayesian} but requires a Bayesian model. We denote the corresponding scoring rule as $\scoringrule_{\batchbald}$. In our experiments, we refer to this method as \texttt{BatchBald}. We used implementation in \texttt{batchbald\_redux} \citep{kirsch2019batchbald}.

This strategy relates to Bayesian D-optimality; when posterior entropy is tractable, it can be minimized directly instead of relying on approximations from \citet{houlsby2011bayesian}. If the posterior is Gaussian, entropy is proportional to the log-determinant of its covariance, leading to D-optimality.

\subsection{Gradient Approximation for Combinatorial Optimization.}
In some strategies, we select a data subset to maximize information criteria like the determinant of FI or the design matrix. These often lead to intractable combinatorial optimization problems. To address this, we use the sensitivity approach from the coreset and robustness literature \citep{huggins2016coresets, campbell2018bayesian, campbell2019automated}. When the information criteria are expressed as a nonlinear function over sum of data point contributions, i.e., $\scoringrule = f(\sum_{\pairidx} c_\pairidx)$, where each data point contributes $c_\pairidx$, we introduce weights $w_i$, allowing the score to be rewritten as $\scoringrule(\bm{w}) = f(\sum_{\pairidx} w_i c_\pairidx)$. For instance, the D-optimal score expresses the determinant of FI of a subset $\chosenset$ as a weighted sum.
\begin{equation}
%\small
    \scoringrule_{\dopt}(\bm{w}) = \lvert \sum_{\pairidx} w_\pairidx (\lastembd{\pairidx}{1}-\lastembd{\pairidx}{2})(\lastembd{\pairidx}{1}-\lastembd{\pairidx}{2})^\top \hat{p}_{\pairidx}(1-\hat{p}_{\pairidx})\rvert
\end{equation}
Each candidate pair is assigned a weight $w_i = 1_{i\in \chosenset}$. Selecting a subset $\chosenset$ to maximize $\scoringrule_{\dopt}$ is equivalent to finding a sparse 0-1 weight vector $\bm{w}$ that maximizes $\scoringrule_{\dopt}(\bm{w})$.

To approximate the optimization, we treat $\bm{w}$ as continuous and perform a Taylor expansion around $\bm{w} = \bm{1}$, the all 1 vector, i.e., all data points are included.
\begin{equation}
   \scoringrule(\bm w)\approx \scoringrule(\bm 1) - (\bm 1-\bm{w})^\top \nabla_{\bm w} \scoringrule(\bm w)|_{\bm w=\bm 1}
   \label{eq:taylorexpansion}
\end{equation}
The approximated optimization problem becomes
\begin{equation}
    \argmax_{\bm w}\scoringrule(\bm w)\approx \argmax_{\bm w} \bm{w}^\top \nabla_{\bm w} \scoringrule(\bm w)|_{\bm w=\bm 1}
\end{equation}
A sparse 0-1 valued vector $\bm w$ that optimizes the right-hand side of \cref{eq:taylorexpansion} can be obtained by selecting the data points with the largest gradient, $\nabla_{\bm w} \scoringrule(\bm w) \big|_{\bm w = \bm 1}$. A probabilistic approach, when all gradients are positive, involves sampling according to the weights given by $\nabla_{\bm w} \scoringrule(\bm w) \big|_{\bm w = \bm 1}$. 



\subsection{Handling nonlinear model using last layer features.}
For nonlinear reward models in \cref{eq:BT}, the dependencies on embeddings become more complex. Strategies like maximum difference and entropy sampling, which depend only on model predictions, remain unaffected by the architecture, while batchBALD is designed for (Bayesian) deep models. Feature-based methods like coreset or D-optimal need adaptation. A heuristic from the Bayesian last layer \citep{tran2019bayesian} and computer vision literature \citet{sener2017active} suggests using the last layer before the linear output as a feature, applying linear strategies to it.
\begin{equation}
    \truereward(\nonlinembdplain) = F_{\bm \theta}(\nonlinembdplain)^\top \lastweight
\end{equation}
For some nonlinear function $F_{\bm \theta}$ parameterized by $\bm\theta$, e.g., an MLP and $\embd := F_{\bm \theta}(\nonlinembdplain)$. We apply methods in linear settings with features $F_{\bm \theta}(\nonlinembdplain)$. We then train $\bm\theta$ and $\lastweight$ together once data are labeled. In particular, in \citet{sener2017active} the nonlinear function $F_{\bm \theta}$ is a CNN and they took a coreset approach. Here we apply this strategy to the coreset, optimal design matrix and D-optimal setting.  



