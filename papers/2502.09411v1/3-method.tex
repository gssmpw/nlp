\vspace{-5pt}
\section{Method}
\label{sec:method}
\input{Figures/overview}

Our goal is to increase the robustness of T2I models, particularly with rare or unseen concepts, which they struggle to generate. To do so, we investigate a retrieval-augmented generation approach, through which we dynamically select images that can provide the model with missing visual cues. Importantly, we focus on models that were not trained for RAG, and show that existing image conditioning tools can be leveraged to support RAG post-hoc.
As depicted in \cref{fig:overview}, given a text prompt and a T2I generative model, we start by generating an image with the given prompt. Then, we query a VLM with the image, and ask it to decide if the image matches the prompt. If it does not, we aim to retrieve images representing the concepts that are missing from the image, and provide them as additional context to the model to guide it toward better alignment with the prompt.
In the following sections, we describe our method by answering key questions:
(1) How do we know which images to retrieve? 
(2) How can we retrieve the required images? 
and (3) How can we use the retrieved images for unknown concept generation?
By answering these questions, we achieve our goal of generating new concepts that the model struggles to generate on its own.

\vspace{-3pt}
\subsection{Which images to retrieve?}
The amount of images we can pass to a model is limited, hence we need to decide which images to pass as references to guide the generation of a base model. As T2I models are already capable of generating many concepts successfully, an efficient strategy would be passing only concepts they struggle to generate as references, and not all the concepts in a prompt.
To find the challenging concepts,
we utilize a VLM and apply a step-by-step method, as depicted in the bottom part of \cref{fig:overview}. First, we generate an initial image with a T2I model. Then, we provide the VLM with the initial prompt and image, and ask it if they match. If not, we ask the VLM to identify missing concepts and
focus on content and style, since these are easy to convey through visual cues.
As demonstrated in \cref{tab:ablations}, empirical experiments show that image retrieval from detailed image captions yields better results than retrieval from brief, generic concept descriptions.
Therefore, after identifying the missing concepts, we ask the VLM to suggest detailed image captions for images that describe each of the concepts. 

\vspace{-4pt}
\subsubsection{Error Handling}
\label{subsec:err_hand}

The VLM may sometimes fail to identify the missing concepts in an image, and will respond that it is ``unable to respond''. In these rare cases, we allow up to 3 query repetitions, while increasing the query temperature in each repetition. Increasing the temperature allows for more diverse responses by encouraging the model to sample less probable words.
In most cases, using our suggested step-by-step method yields better results than retrieving images directly from the given prompt (see 
\cref{subsec:ablations}).
However, if the VLM still fails to identify the missing concepts after multiple attempts, we fall back to retrieving images directly from the prompt, as it usually means the VLM does not know what is the meaning of the prompt.

The used prompts can be found in \cref{app:prompts}.
Next, we turn to retrieve images based on the acquired image captions.

\vspace{-3pt}
\subsection{How to retrieve the required images?}

Given $n$ image captions, our goal is to retrieve the images that are most similar to these captions from a dataset. 
To retrieve images matching a given image caption, we compare the caption to all the images in the dataset using a text-image similarity metric and retrieve the top $k$ most similar images.
Text-to-image retrieval is an active research field~\cite{radford2021learning, zhai2023sigmoid, ray2024cola, vendrowinquire}, where no single method is perfect.
Retrieval is especially hard when the dataset does not contain an exact match to the query \cite{biswas2024efficient} or when the task is fine-grained retrieval, that depends on subtle details~\cite{wei2022fine}.
Hence, a common retrieval workflow is to first retrieve image candidates using pre-computed embeddings, and then re-rank the retrieved candidates using a different, often more expensive but accurate, method \cite{vendrowinquire}.
Following this workflow, we experimented with cosine similarity over different embeddings, and with multiple re-ranking methods of reference candidates.
Although re-ranking sometimes yields better results compared to simply using cosine similarity between CLIP~\cite{radford2021learning} embeddings, the difference was not significant in most of our experiments. Therefore, for simplicity, we use cosine similarity between CLIP embeddings as our similarity metric (see \cref{tab:sim_metrics}, \cref{subsec:ablations} for more details about our experiments with different similarity metrics).

\vspace{-3pt}
\subsection{How to use the retrieved images?}
Putting it all together, after retrieving relevant images, all that is left to do is to use them as context so they are beneficial for the model.
We experimented with two types of models; models that are trained to receive images as input in addition to text and have ICL capabilities (e.g., OmniGen~\cite{xiao2024omnigen}), and T2I models augmented with an image encoder in post-training (e.g., SDXL~\cite{podellsdxl} with IP-adapter~\cite{ye2023ip}).
As the first model type has ICL capabilities, we can supply the retrieved images as examples that it can learn from, by adjusting the original prompt.
Although the second model type lacks true ICL capabilities, it offers image-based control functionalities, which we can leverage for applying RAG over it with our method.
Hence, for both model types, we augment the input prompt to contain a reference of the retrieved images as examples.
Formally, given a prompt $p$, $n$ concepts, and $k$ compatible images for each concept, we use the following template to create a new prompt:
``According to these examples of 
$\mathord{<}c_1\mathord{>:<}img_{1,1}\mathord{>}, ... , \mathord{<}img_{1,k}\mathord{>}, ... , \mathord{<}c_n\mathord{>:<}img_{n,1}\mathord{>}, ... , $
$\mathord{<}img_{n,k}\mathord{>}$,
generate $\mathord{<}p\mathord{>}$'', 
where $c_i$ for $i\in{[1,n]}$ is a compatible image caption of the image $\mathord{<}img_{i,j}\mathord{>},  j\in{[1,k]}$. 

This prompt allows models to learn missing concepts from the images, guiding them to generate the required result. 

\textbf{Personalized Generation}: 
For models that support multiple input images, we can apply our method for personalized generation as well, to generate rare concept combinations with personal concepts. In this case, we use one image for personal content, and 1+ other reference images for missing concepts. For example, given an image of a specific cat, we can generate diverse images of it, ranging from a mug featuring the cat to a lego of it or atypical situations like the cat writing code or teaching a classroom of dogs (\cref{fig:personalization}).
\input{Figures/personalization}