\section{Related Work}
\label{sec:prior_work}

\subsection{In-Context Learning}

In-context learning (ICL) has emerged as a powerful paradigm in which large language models (LLMs) are capable of performing new tasks without additional fine-tuning \cite{brown2020language}. By providing a few examples or relevant context directly in the input prompt, ICL enables models to infer the desired task and generate appropriate outputs.
Despite its flexibility, ICL is limited by the finite context window of the model, making the selection of relevant and concise context critical for optimal performance.

\vspace{-6pt}
\subsection{Visual In-Context Learning}\vspace{-2pt}

Recently, visual ICL presented promising results~\cite{gu2024analogist,wang2023context,xiao2024omnigen,najdenkoska2024context, sun2024generative}.
Visual ICL has mostly been explored in the context of learning from analogies~\cite{gu2024analogist, wang2023context, xiao2024omnigen, nguyen2024visual}.
However, the ability of learning from single examples has also been researched with multimodal generative models that allow images as input \cite{xiao2024omnigen, sun2024generative, wanggenartist}. 
Such models, which allow image prompting, facilitate exploring RAG for image generation.

\vspace{-4pt}
\subsection{Retrieval Augmented Generation (RAG)}
RAG~\cite{lewis2020retrieval} is a method to improve the generation abilities of a pretrained model without additional training, by dynamically retrieving and supplying information as context in the prompt. 
The most relevant information for a given query is retrieved from an external database, and supplied to the model as context for improved generation that relies on the context data.
While RAG has been greatly explored for text generation tasks and applied over multiple pretrained LLMs~\cite{lewis2020retrieval,gao2023retrieval,ram2023context,zhang2025aidrivensignlanguagegeneration}, it has yet to be explored for enhancing the capabilities of pretrained text-to-image models.
Some previous work used nearest-neighbor image retrieval to improve image generation~\cite{sheyninknn, blattmann2022retrieval, chenre, hu2024instruct}, however they train models specifically for retrieval-aided generation. Unlike them, our method leverages pretrained models and does not require additional training.

\vspace{-4pt}
\subsection{Text-to-Image Generation}
Text-to-image generation advanced greatly with the introduction of diffusion models~\cite{ho2020denoising}, which can produce high-quality and diverse images of a wide range of concepts \cite{dhariwal2021diffusion, rombach2022high, podellsdxl, xiao2024omnigen}.
However, they still struggle with rare concepts and cannot generate user-specific concepts without additional training or optimization.

\textbf{Personalization} works generate images of a user-specific concept. However, they often require an optimization process for learning each new concept~\cite{nitzan2022mystyle, galimage, ruiz2023dreambooth, arar2024palp, alaluf2023neural, voynov2023p+, avrahami2023break, kumari2023multi}.
To mitigate this challenge, recent works train image-encoders that allow prompting existing pretrained generative models with images~\cite{ye2023ip,gal2023encoder,wei2023elite,shi2024instantbooth,gal2024lcm,patashnik2025nested}.

\textbf{Rare Concept Generation}
works that explored generating rare concepts without image retrieval, used a few examples of each rare concept to optimize seeds that produce images similar to the references~\cite{samuel2024norm,samuel2024generating}. However, in addition to the requirement of an optimization process per new concept, these works also do not address the questions of how to choose or find the reference images.