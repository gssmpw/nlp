\vspace{-4pt}
\section{Experiments}
\label{sec:experiments}

In this section, we describe quantitative and qualitative experiments performed to evaluate the effectiveness of our method. To assess its adaptability to different model types, we experiment with applying \emph{ImageRAG} to two models, namely OmniGen~\cite{xiao2024omnigen} and SDXL~\cite{podellsdxl}, each representing a different model type.

\vspace{-1pt}
\subsection{Quantitative comparisons}
\label{sec:quant}
\input{Tables/long_tail}
We evaluate the ability of our method to improve T2I generation of rare and fine-grained concepts by comparing the results of OmniGen and SDXL with their results when applying \emph{ImageRAG} to them.
As additional baselines, we compare with FLUX~\cite{flux2023}, Pixart-$\Sigma$~\cite{chen2025pixart}, and GraPE~\cite{goswami2024grape}. The last is an iterative LLM-based image generation method which employs editing tools to insert missing objects. We use their OmniGen-based version.
We use each method to generate images of each class in the following datasets: ImageNet~\cite{deng2009imagenet}, iNaturalist~\cite{van2018inaturalist}, CUB~\cite{wah2011caltech}, and Aircraft~\cite{majifine}.
For iNaturalist, we use the first 1000 classes.
\cref{tab:long_tail} shows evaluation results of all methods using CLIP~\cite{radford2021learning}, SigLIP~\cite{zhai2023sigmoid} and DINO~\cite{zhangdino} similarities. For fairness, we use open-CLIP for evaluation, while using OpenAI CLIP for retrieval.
As demonstrated in \cref{tab:long_tail}, both OmniGen and SDXL results improve when using our method for the generation of rare concepts and fine-grained categories.

\vspace{-3pt}
\subsection{Proprietary Data Generation}
\input{Tables/proprietary_ds}
A common use for RAG in NLP is generation based on proprietary data \cite{lewis2020retrieval},
where the retrieval dataset is a proprietary one.
A similar application in image generation would be generating images based on a proprietary gallery of images. 
It could be for personalization, where the gallery is of a personal concept, e.g. images of a person's dog, or it could be a company brand or a private collection of images that could broaden the knowledge of a model.
Our LAION-based experiments explored the scenario where a user has access to a general, large-scale set. Here, we further evaluate the performance of \emph{ImageRAG} when we have access to a potentially smaller, specialized dataset. Hence, we repeat the experiments with the datasets used in \cref{tab:long_tail}, but this time retrieve samples from within each dataset rather than from the LAION~\cite{schuhmann2022laion} subset.
 Results are reported in \cref{tab:prop_ds,tab:prop_ds_app}.
 We observe that although applying our method with the generic dataset of a relatively small subset from LAION already improves the results, they improve even further when using the proprietary datasets for retrieval.

\vspace{-3pt}
\subsection{Ablation Studies}
\label{subsec:ablations}
\input{Tables/ablations}
To evaluate the contribution of each part of our method, we conduct an ablation study testing different components and report our results in \cref{tab:ablations}.
First, we want to ensure the performance gap is not based on simply interpreting rare words using an LLM. Hence, we evaluate OmniGen and SDXL over rephrased text prompts, without providing reference images. 
To do so, we asked GPT to rephrase the prompts, to make them easier for a T2I generative model, by explicitly asking it to change rare words to their description if necessary. The full prompt can be found in \cref{app:prompts}.
As the results show, rephrasing was not enough for a meaningful improvement in the results (``Rephrased prompt'' in \cref{tab:ablations}).
Next, we investigate the importance of using detailed image captions for retrieval, rather than just listing the missing concepts or using the original prompt. We do so by evaluating our method when retrieving the concepts directly without generating compatible image captions for each missing concept (``Retrieve concepts'' in \cref{tab:ablations}), and when retrieving the prompt directly (``Retrieve prompt'' in \cref{tab:ablations}). While retrieval with each of them introduced some improvement over the initial results, retrieving detailed captions improved the results even further.

\input{Figures/num_samples_to_score}
Next, we investigate the effect of the retrieval-dataset size. We tested our method over ImageNet~\cite{deng2009imagenet} and Aircraft~\cite{majifine} when using 
% 0 (the original models), 
\num{1000}, \num{10000}, \num{100000}, and \num{350000} examples from LAION~\cite{schuhmann2022laion}. \cref{fig:samples_to_score} shows that increasing the dataset size typically leads to better results. However, even using a relatively small dataset can already lead to improvements. For OmniGen, \num{1000} examples were enough to see an improvement over the baseline model. SDXL has a stronger baseline, hence more examples were needed for improvement.
\input{Tables/retrieval_metrics}

Finally, we investigate the effect of different similarity metrics for retrieval. We used CLIP~\cite{radford2021learning}, SigLIP~\cite{zhai2023sigmoid}, and re-ranking with GPT~\cite{hurst2024gpt} and BM25~\cite{robertson2009probabilistic} over image captions generated by GPT for the retrieved candidates. Re-ranking was performed after retrieving $3$ candidates from each of CLIP and SigLIP. Results are reported in \cref{tab:sim_metrics}.
Although re-ranking with GPT produced slightly better results, they were not significant enough to justify the cost of applying this complex strategy vs. a more straightforward CLIP metric. Hence, our other experiments use CLIP. Nevertheless, all the different metrics improved the generation abilities of the base model by providing helpful references.

\subsection{Qualitative comparisons}
\label{sec:qual}
\input{Figures/user_study}
\input{Figures/qualitative_comp}

\cref{fig:qual_comp} shows qualitative examples from the ImageNet~\cite{deng2009imagenet}, CUB~\cite{wah2011caltech} and iNaturalist~\cite{van2018inaturalist} datasets, comparing the results of OmniGen and SDXL with and without our method.

To further assess the quality of our results, we conduct a user study with 46 participants and a total of 767 comparisons.
We perform two types of studies --- one that evaluates SDXL and OmniGen with and without our method, and another that compares our results with other retrieval-based generation models. Specifically, we compare to models explicitly trained for the task of image generation using retrieved images: RDM~\cite{blattmann2022retrieval}, knn-diffusion~\cite{sheyninknn}, and ReImagen~\cite{chenre}. Since these are largely proprietary models with no API, we compare to images and prompts published in their papers.

In both cases, we ask participants to compare two images at a time: one created with our approach, and one using a baseline. We ask users to choose the one they prefer in terms of adherence to the text prompt,
visual quality, and overall preference. Since some prompts contain uncommon concepts, we supply users with a real image of the least familiar concept in each prompt (not taken from our dataset).
When running \emph{ImageRAG} for the user study, we disable the decision step where GPT is asked if the initial image matches the prompt. This is done to avoid showing a user the same image twice in cases where GPT deems the initial image to be a good fit.
As demonstrated in \cref{fig:user_study} participants favored \emph{ImageRAG} over all other methods in all three criteria of text alignment, visual quality, and overall preference.
\cref{app:user_study} supplies more information about questions asked in the study, visual comparison examples for each retrieval-based generation model (\cref{fig:retrieval_comp}), and more comparisons to SDXL (\cref{fig:rare_sd}) and OmniGen (\cref{fig:rare_o}) with and without \emph{ImageRAG}. 
\cref{fig:creative} shows additional visual results of our method with more complex and creative prompts.
