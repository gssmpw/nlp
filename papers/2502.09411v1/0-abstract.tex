\begin{abstract}
Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts.
To address this challenge,
we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models.  
We propose \emph{ImageRAG}, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. 
Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast,
\emph{ImageRAG} leverages the capabilities of existing image conditioning models, and does not require RAG-specific training.
Our approach is highly adaptable and can be applied across different model types,
showing significant improvement in generating rare and fine-grained concepts using different base models.

Our project page is available at: \small{\url{https://rotem-shalev.github.io/ImageRAG}}

\end{abstract}