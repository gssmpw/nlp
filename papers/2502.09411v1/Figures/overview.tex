\begin{figure*}[htp]
    \setlength{\belowcaptionskip}{-4pt}

  \centering
   \includegraphics[width=0.99\linewidth]{Assets/imageRAG_overview.pdf}
   \caption{
   \textbf{Top}: a high-level overview of our method. Given a text prompt $\mathord{<}p\mathord{>}$, we generate an initial image using a text-to-image (T2I) model. Then, we generate retrieval-captions $\mathord{<}c_j\mathord{>}$, retrieve images from an external database for each caption $\mathord{<}i_j\mathord{>}$, and use them as references to the model for better generation.
\textbf{Bottom}: the retrieval-caption generation block.
    We use a VLM to decide if the initial image matches the given prompt. If not, we ask it to list the missing concepts, and to create a caption that could be used to retrieve appropriate examples for each of these missing concepts.
    }
   \label{fig:overview}
\end{figure*}