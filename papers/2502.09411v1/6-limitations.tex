\section{Limitations} 
\label{sec:limitations}

Although our method can help models generate concepts they are unable of generating alone, it has some limitations, depending on the data, retrieval method, and underlying model. For example, both OmniGen and SDXL struggle with text, and even when given a text image as a reference, they do not learn from it accurately. Moreover, our results depend on the quality of the retrieval method. For example, when using CLIP for retrieval, we cannot help with tasks it does not excel, such as counting \cite{paiss2023teaching}. Additionally, we rely on the used VLM to decide whether we should apply our method or not. Although GPT is a powerful model and often answers correctly if an image matches a text prompt, sometimes it may answer that the prompt aligns with the image even if it does not, and in these cases our method will not be applied. A possible solution is to apply our method directly if the output image is not satisfactory. 
Finally, our ability to aid the model also depends on the dataset we retrieve images from. If the dataset only contains images of birds and the task is to generate a specific dog breed, we will not be able to help. On the other hand, as presented in \cref{tab:prop_ds}, if the retrieval dataset contains relevant information, the generation will also be more accurate.