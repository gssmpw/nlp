\clearpage
\appendix

\section{Retrieval-Caption Generation Prompts}
\label{app:prompts}

Full prompts used for querying GPT in the retrieval-caption generation part of our method:

\textbf{Decision:} 
`Does this image match the prompt ``\{prompt\}''? Consider both content and style aspects. Only answer yes or no.'

\textbf{Missing Concepts Identification:}
`What are the differences between this image and the required prompt? In your answer only provide missing concepts in terms of content and style, each in a separate line. For example, if the prompt is ``An oil painting of a sheep and a car'' and the image is a painting of a car but not an oil painting, the missing concepts will be: \\
oil painting style \\
a sheep'

\textbf{Caption Generation:}
`For each concept you suggested above, please suggest an image caption describing an image that explains this concept only. The captions should be stand-alone description of the images, assuming no knowledge of the given images and prompt, that I can use to lookup images with automatically. In your answer only provide the image captions, each in a new line with nothing else other than the caption.'

\textbf{Rephrase request prompt:}
prompt used for the rephrasing ablation experiment:
`Please rephrase the following prompt to make it easier and clearer for the text-to-image generation model that generated the above image for this prompt. The goal is to generate an image that matches the given text prompt. If the prompt is already clear, return it as it is. Simplify and shorten long descriptions of known objects/entities but DO NOT change the original meaning of the text prompt. If the prompt contains rare words, change those words to a description of their meaning. In your answer only provide the prompt and nothing else. The prompt to be rephrased: ``\{prompt\}''.'

\section{Additional Experiments}
\input{Tables/prop_ds_app}
Additional proprietary dataset experiments over the CUB~\cite{wah2011caltech} and iNaturalist~\cite{van2018inaturalist} datasets are presented in \cref{tab:prop_ds_app}.

\section{User Study Questions}
\label{app:user_study}

In the user study, we asked users to compare pairs of images at a time, by asking which one adheres better to the prompt and has better visual quality. We supplied real references (not from our dataset) for rare concepts with each pair.
The questions we asked were:
For each criteria, choose the better image out of A and B given the following text prompt:
$\mathord{<}prompt\mathord{>}$.
The less familiar concept ``$\mathord{<}rare\_concept\mathord{>}$'' is presented on the left of the image options.
\begin{itemize}
    \item Better text alignment (choose A or B)
    \item Better visual quality (choose A or B)
    \item Overall preference (choose A or B)
\end{itemize} 

Pair examples of using our method vs. other retrieval-based generation approaches can be found in \cref{fig:retrieval_comp}. 
Due to lack of access to the models, all prompts and results of the other methods were taken from their papers.

\input{Figures/retrieval_comp}
\input{Figures/rare_sd}
\input{Figures/rare_omnigen}
Pair examples of rare or fine-grained concept generation with and without \emph{ImageRAG} are presented in \cref{fig:rare_sd} (SDXL examples), and \cref{fig:rare_o} (OmniGen examples).

\input{Figures/creative}
More complex creative examples are presented in \cref{fig:creative}.

