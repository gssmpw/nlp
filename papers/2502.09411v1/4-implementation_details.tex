\vspace{-7pt}
\section{Implementation Details}
\label{sec:imp_details}

We use a subset of LAION~\cite{schuhmann2022laion} containing 350K images as the dataset from which we retrieve images.
As a retrieval similarity metric, we use CLIP ``ViT-B/32''.
For a VLM we use GPT-4o-2024-08-06~\cite{hurst2024gpt} with a temperature of $0$ for higher consistency (unless GPT fails to find concepts, see \cref{subsec:err_hand}).
Full GPT prompts are supplied in \cref{app:prompts}.
As our T2I generation base models we use
SDXL~\cite{podellsdxl} with the ViT-H IP-adapter~\cite{ye2023ip} plus version (``ip-adapter-plus\_sdxl\_vit-h''), using $0.5$ for the ip\_adapter\_scale, and OmniGen~\cite{xiao2024omnigen} with the default parameters (2.5 guidance scale, 1.6 image guidance scale, and a resolution of 1024x1024).
As OmniGen only supports 3 images as context, we use up to $k=3$ concepts for each prompt and $n=1$ images per concept.
For SDXL, as the IP-adapter we used is limited to $1$ image, we use $1$ concept and $1$ image.