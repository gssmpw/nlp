\section{Introduction}
\label{sec:intro}

Diffusion models \cite{ho2020denoising} have recently revolutionized image generation, offering high-quality, diverse, and realistic visual content \cite{dhariwal2021diffusion, rombach2022high}.
They enable text-to-image generation as well as a wide range of tasks, from layout-based synthesis to image editing and style transfer \cite{avrahami2022blended, hertz2022prompt, mokady2023null, avrahami2023spatext, zhang2023adding, brooks2023instructpix2pix, nitzan2024lazy}.
Of course, these large models require great amounts of training data, substantial training durations, and extensive computational resources.
As a result, contemporary text-to-image (T2I) models, that are limited to the data they were trained on, struggle with generating user-specific concepts or updated content.
Additionally, they have difficulty with generating rare concepts, stylized content, or fine-grained categories (e.g., a specific dog breed), even if they were trained on images containing them \cite{samuel2024generating,haviv2024not}.
In these cases, diffusion models tend to ``hallucinate'', and potentially generate content unrelated to the textual prompt (see \Cref{fig:hallucinations}).
\input{Figures/hallucinations}
To tackle these problems, several approaches have been proposed for personalized 
\cite{galimage, ruiz2023dreambooth, voynov2023p+, arar2024palp}, stylized~\cite{hulora},  or rare concept generation~\cite{li2024block, samuel2024generating}. 
Most approaches, however, require training or specialized optimization techniques for each new concept.

We note  that similar problems exist with text generation using Large Language Models (LLMs). 
LLMs struggle to generate text based on real-world facts, proprietary or updated data, and tend to hallucinate when lacking sufficient knowledge~\cite{brown2020language, ji2023survey}. 
To solve these problems, the Natural Language Processing community has adopted Retrieval Augmented Generation (RAG)~\cite{lewis2020retrieval}. RAG is a method for dynamically retrieving the most relevant information to a given task from external sources and supplying it to an LLM as context input, enabling it to generate contextually accurate and task-specific responses.
Investigating this idea for images, we notice previous works employing image retrieval for better image generation~\cite{chenre, sheyninknn, blattmann2022retrieval, hu2024instruct}, are based on models trained specifically for the task, hindering wide applicability.

In contrast, we propose \emph{ImageRAG}, a method that dynamically retrieves and provides images as references to pretrained T2I models, to enhance their generation capabilities.
Our method does not require additional training over the retrieved content or specifically for RAG. Instead, we use T2I models in the same vein as the common use of LLMs, and use reference images during sampling for guided generation.
Compared to the language case, in the visual domain, the context is more limited. Hence, we cannot simply provide references for all the concepts in a prompt. 
We note that to apply RAG for image generation, we need to answer the questions of which images to use as context, how to retrieve them, and how to use the retrieved images to successfully generate a required concept.
In this work, we address these questions by offering a novel method that dynamically chooses the most relevant and useful examples given a prompt, and uses them as references, to guide the model toward generating the required results.
Leveraging T2I models' ability to produce many concepts, we only pass concepts the models struggle to generate, focusing on the gaps.
To understand what the challenging concepts are,
we suggest a novel method that dynamically chooses images to retrieve for a given prompt using a step-by-step method and a Vision-Language Model (VLM).
Specifically, we ask the VLM to identify missing image components and suggest concepts that could be retrieved, and use them as references to guide the generation.

Our approach is not related to a specific T2I model.
To demonstrate it, we apply \emph{ImageRAG} to two model types: T2I models designed to allow in-context learning (ICL) \cite{brown2020language}; and T2I models augmented with an IP-adapter image-encoder~\cite{ye2023ip} that allows image prompting.
In ICL, the generative model is provided with a set of task-specific examples in the prompt (as context input) and is expected to perform a similar task on a new input.
ICL does not require additional training or large amounts of data, and offers a way to adapt the model to new tasks or domains by supplying it with unseen information at inference time.
IP-adapters are image-encoders that allow prompting a T2I model with images, as adapters for existing models. 
Both types show great potential for improved image generation, with impressive results \cite{ye2023ip, xiao2024omnigen, gu2024analogist, wang2023context, najdenkoska2024context, sun2024generative}.  
We demonstrate the effectiveness of our method for image generation of rare and fine-grained concepts on two models, representing the two model types: 
Omnigen~\cite{xiao2024omnigen} --- a model that was designed to allow ICL; 
and SDXL~\cite{podellsdxl}+IP-adapter~\cite{ye2023ip} --- models that 
allow image prompting through the IP-adapter image-encoder.
We perform quantitative and qualitative comparisons on both models, and show that RAG enhances their rare and fine-grained concept generation capabilities. These results indicate that the image generation community should also consider using RAG for class or task-specific generation during sampling time.