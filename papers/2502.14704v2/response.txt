\section{Related Work}
% In this section, we introduce the related work from the meta-learning perspective and self-supervised learning perspectives. We include the related work about TSF models in Appendix~\ref{ssec:related_extension}. 

We discuss related techniques from meta-learning and self-supervised learning perspectives, with an inventory of TSF models in Appendix~\ref{ssec:related_extension}.

\textbf{Meta-Learning for Time Series}.
Meta-Learning, by definition, seeks to perform in a learn-to-learn paradigm. Generally speaking, meta-learning includes optimization-based, model-based, and metric-based methods. Optimization-based methods target optimal initial parameters **Raghu et al., "On the Importance of Overcomplete Representations in Deep Learning"** , often involving a two-loop optimization process **Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"** . Model-based methods aim to determine suitable models, typically from an ensemble, based on predefined tasks **Veness et al., "Meta-Learners Can Learn to Leverage Pre-existing Models"** or activation states **Liu et al., "Learning to Learn with Quantum Neural Network"** . 
Metric-based methods  learn a metric function that provides more expressive measurements of distances between data samples, commonly comparing training and validation samples. 

Our method \model{} aligns with the boarder scope of meta-learning. 
Specifically, the grid search (Algorithm~\ref{alg:grid_search}) follows the two-loop structure similar to optimization-based methods. However, it diverges by focusing on \emph{dataset space} rather than \emph{parameter space}.
The goal of the grid search is not to solve the optimization problem directly but to leverage the trajectory of optimization.
Additionally, the final mask form of \model{} in essence provides a more accurate metric tailored for a supervised-learning setting. 
%
While \textsc{DeepTime} **Wang et al., "DeepTime: A Deep Learning Framework for Time Series Forecasting"** and \textsc{AdaRNN} **Liu et al., "AdaRNN: An Adaptive RNN Architecture for Time Series Prediction"** share a related idea, there are key differences. 
\textsc{DeepTime} focuses more on a time-index forecast paradigm, whereas \textsc{AdaRNN} applies metric learning to hidden states. 
Both works learn metrics on the sample level (a window of time series), whereas ours focuses on the instance level (individual data points of time series).

\textbf{Self-supervised Learning for Time Series}.
Self-supervised learning trains models without relying on manually labeled data by using auxiliary tasks like generation, contrast, and reconstruction to learn expressive representation or create pseudo labels. 
%
In the realm of time series, this approach is discussed more in Time Series Classification (TSC)  **Chen et al., "Time Series Classification with Temporal Convolutional Networks"** . 
Recent works  present a novel perspective that instances in time series/segments can have multiple labels. 
They propose corresponding weakly-supervised-learning methods that significantly improve both performance and interpretability. 

As manual labeling is usually not required, TSF is treated as a generation task by self-supervised methods.
Recent works focus on the use of TSF as an auxiliary task to learn universal representations that improve performances of other tasks  **Zhang et al., "Self-Supervised Time Series Forecasting with Auxiliary Tasks"** . This paradigm shows the potential to scale time series models to levels comparable to large language models. 

In this work, we integrate \emph{both perspectives} by employing an auxiliary reconstruction task, commonly used in TSC, to enhance the performance of TSF. 
% \textcolor{blue}{The pseudo labels which is often discussed in TSC, differs from ours in that they are generated from existing manual labels whereas we generate pseudo labels from a self-supervised reconstruction task.} 
The pseudo labels, often discussed in TSC, are derived from existing, manual ones, while ours are created in a self-supervised paradigm.