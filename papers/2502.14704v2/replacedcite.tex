\section{Related Work}
% In this section, we introduce the related work from the meta-learning perspective and self-supervised learning perspectives. We include the related work about TSF models in Appendix~\ref{ssec:related_extension}. 

We discuss related techniques from meta-learning and self-supervised learning perspectives, with an inventory of TSF models in Appendix~\ref{ssec:related_extension}.

\textbf{Meta-Learning for Time Series}.
Meta-Learning, by definition, seeks to perform in a learn-to-learn paradigm. Generally speaking, meta-learning includes optimization-based, model-based, and metric-based methods. Optimization-based methods target optimal initial parameters ____, often involving a two-loop optimization process ____. Model-based methods aim to determine suitable models, typically from an ensemble, based on predefined tasks ____ or activation states ____. 
Metric-based methods ____ learn a metric function that provides more expressive measurements of distances between data samples, commonly comparing training and validation samples. 

Our method \model{} aligns with the boarder scope of meta-learning. 
Specifically, the grid search (Algorithm~\ref{alg:grid_search}) follows the two-loop structure similar to optimization-based methods. However, it diverges by focusing on \emph{dataset space} rather than \emph{parameter space}.
The goal of the grid search is not to solve the optimization problem directly but to leverage the trajectory of optimization.
Additionally, the final mask form of \model{} in essence provides a more accurate metric tailored for a supervised-learning setting. 
%
While \textsc{DeepTime} ____ and \textsc{AdaRNN} ____ share a related idea, there are key differences. 
\textsc{DeepTime} focuses more on a time-index forecast paradigm, whereas \textsc{AdaRNN} applies metric learning to hidden states. 
Both works learn metrics on the sample level (a window of time series), whereas ours focuses on the instance level (individual data points of time series).

\textbf{Self-supervised Learning for Time Series}.
Self-supervised learning trains models without relying on manually labeled data by using auxiliary tasks like generation, contrast, and reconstruction to learn expressive representation or create pseudo labels. 
%
In the realm of time series, this approach is discussed more in Time Series Classification (TSC) ____. 
Recent works ____ present a novel perspective that instances in time series/segments can have multiple labels. 
They propose corresponding weakly-supervised-learning methods that significantly improve both performance and interpretability. 

As manual labeling is usually not required, TSF is treated as a generation task by self-supervised methods.
Recent works focus on the use of TSF as an auxiliary task to learn universal representations that improve performances of other tasks ____. This paradigm shows the potential to scale time series models to levels comparable to large language models. 

In this work, we integrate \emph{both perspectives} by employing an auxiliary reconstruction task, commonly used in TSC, to enhance the performance of TSF. 
% \textcolor{blue}{The pseudo labels which is often discussed in TSC, differs from ours in that they are generated from existing manual labels whereas we generate pseudo labels from a self-supervised reconstruction task.} 
The pseudo labels, often discussed in TSC, are derived from existing, manual ones, while ours are created in a self-supervised paradigm.