%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{caption, subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[shortlabels]{enumitem}
\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=0.5pt}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=0.5pt}
\setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=0.5pt}
% Prevent conflict by renaming commands from `algorithm` package
% \usepackage{algorithm}
% \usepackage{algpseudocode}
% Prevent conflicts when loading `algorithm2e`
\usepackage[linesnumbered,ruled,vlined,algo2e]{algorithm2e}

% if you use cleveref..
% \usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition} %[theorem]
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
\newtheorem{definition}{Definition} %[theorem]
% \newtheorem{problem}{Problem} %[theorem]
\newtheorem{observation}{Observation}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

%\xmark
\usepackage{pifont}
%cellcolr
\usepackage{colortbl}
%multirow
\usepackage{multirow}
\usepackage{multicol}
%warpfigure
\usepackage{wrapfig}
%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
%
% \usepackage{newtxtext}
% \usepackage{newtxmath}

%defined command here
\newcommand{\xmark}{\ding{55}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\model}{\textsc{Scam}}
\newcommand{\snr}{\textsc{snr}}
\newcommand{\mlp}{\textsc{Mlp}}
\newcommand{\cyclenet}{\textsc{cycleNet}}
\newcommand{\patchtst}{\textsc{PatchTST}}
\newcommand{\itrans}{\textsc{iTrans}}
\newcommand{\iTransformer}{\textsc{iTransformer}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting}

\begin{document}

\twocolumn[
% \icmltitle{Not All Data are Well Labled: \\On the Self-supervised Labeling of Time Series Forecasting}
\icmltitle{Not All Data are Good Labels: \\On the Self-supervised Labeling for Time Series Forecasting}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance, which is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yuxuan Yang}{zju}
\icmlauthor{Dalin Zhang}{aau}
\icmlauthor{Yuxuan Liang}{hkust}
\icmlauthor{Hua Lu}{aau}
\icmlauthor{Gang Chen}{zju}
\icmlauthor{Huan Li}{zju}
\end{icmlauthorlist}

\icmlaffiliation{zju}{Zhejiang University}
\icmlaffiliation{aau}{Aalborg University}
\icmlaffiliation{hkust}{The Hong Kong University of Science and Technology
(Guangzhou)}

%\icmlcorrespondingauthor{Huan Li}{lihuan.cs@zju.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (\model{}), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (\snr) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that \model{} consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning. 
The code is available at \href{https://anonymous.4open.science/r/SCAM-BDD3}{https://anonymous.4open.science/r/SCAM-BDD3}.
\end{abstract}

\section{Introduction}

Time Series Forecasting (TSF) is a crucial task with extensive applications in energy, finance, engineering, and many other domains. Recent advances in deep learning have resulted in TSF methods that outperform traditional methods in precision, robustness, and scalability \citep{informer, deepar, prophet}.

Nevertheless, deep learning-based TSF methods still face significant challenges such as overfitting, dependence on high-quality datasets, and inconsistent performance across datasets --- issues exacerbated by flawed evaluation practices \citep{benchmark_tkde, tfb}. Central to these challenges is the problem of \emph{low-quality labels}\footnote{In TSF, given two adjacent windows in a time series, $x$ (input window) and $y$ (output window), ``labels" refer to $y$ fitted by predictions $\hat{y} = f(x;\theta)$ in a supervised learning setting.}, associated with inherent noise and anomalies in raw data. Existing strategies, such as multimodal data integration \citep{textcues, visionts} and dataset scaling \citep{tscaling}, focus on augmenting or refining datasets but fail to address the core limitation: the reliance on raw labels as ground truth. To address this, we pose two critical questions:
\begin{enumerate}[leftmargin=*]
\item \textit{Can the reliance on high-quality labeled time series datasets be alleviated, given their scarcity?}
% An overreliance on high-quality labeled time series datasets, compounded by the scarcity of such datasets.
\item \textit{Can the potential of existing time series datasets be better exploited to improve model performance?}
% Inadequate utilization of existing time series datasets, which restricts their full potential.
% \item The need for a learning-based framework to enhance the robustness of general predictor models, akin to traditional data cleaning and transformation techniques.
\end{enumerate}
We posit that both answers are positive by redefining how labels are generated. \emph{Instead of treating raw labels as immutable targets, we selectively replace them with ``pseudo labels'' generated self-supervisedly.}
% An ideal solution to overcoming these challenges would involve a learning-based framework capable of \emph{autonomously manipulating the given time series data}.

The key idea is that the pseudo labels can be created from an inherent search through \emph{candidate datasets} created by an auxiliary reconstruction task. 
In this process, raw labels are partially replaced with reconstructions, guided by an adaptive mask that identifies overfitted raw components and selectively replaces them with pseudo labels for predictions in the supervised setting.
This self-supervised learning paradigm significantly enhances the generalization of TSF models compared to traditional supervised learning, which rigidly adheres to raw labels.

\begin{figure}[!ht]
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/introduction/hierarchical_optimization.pdf}
        \caption{Grid search}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/introduction/multi_objective_optimization.pdf}
        \caption{Co-optimization}
    \end{subfigure}
    
    \caption{Illustrations of the proposed method \model{}.}
    \label{fig:illustration}
\end{figure}
Specifically, our approach optimizes a simple reconstruction network $g(\cdot;\phi)$ to generate intermediate reconstructions. Each parameter $\phi_i$ during optimization corresponds to an individual candidate dataset $D_i$ (see Figure~\ref{fig:illustration}(a)). Labels are collected from each candidate dataset to train an individual predictor $f(\cdot;\theta)$ under supervised settings. The two-step hierarchical optimization functions as a search on an auxiliary reconstruction metric (see Section~\ref{ssec:initial_case}).
To integrate this process into a feasible pipeline, we simplify this process as a one-step optimization for a co-objective (see Section~\ref{ssec:co-objective}). 
% Additionally, we introduce a constrained loss formulation to enable efficient exploration among possible candidate datasets.

The optimal generalized performance lies on the Pareto Front of the co-objective optimization. 
However, a critical issue is overfitting, as depicted in Figure~\ref{fig:illustration}(b), which prevents convergence to optimality when the trajectory moves to one extreme of the Pareto Front (where the reconstruction closely approximates the raw data).  
To better analyze overfitting, we derive a mask form of the co-objective loss, which partitions original time series into distinct components.
We employ a criterion, namely the \emph{sharpness metric} $\lambda_{max}$ \citep{samformer}, which detects overfitting by evaluating the sharpness of loss landscape. 
Using $\lambda_{max}$ and practical evaluations of the decomposed loss, we identify the overfitted components. 
This process, termed \textbf{\emph{Self-Correction with Adaptive Mask}} (\model{}), discards raw data labels ($Y$) based on reconstruction results ($\tilde{Y}$) and current predictions ($\hat{Y}$), smoothing the loss landscape and enhancing the generalization of the predictor model $f(\cdot;\theta)$. 

\begin{wrapfigure}{r}{0.20\textwidth}
\centering
\scalebox{.75}{
\begin{tikzpicture}[
    funcblock/.style={rectangle, draw=blue!60, fill=blue!30, 
                     minimum width=1.5cm, minimum height=0.5cm, 
                     rounded corners=5pt, text=black, font=\normalsize},
    line/.style={->, draw=black!60, line width=0.8pt}
]

% Function blocks
\node[funcblock] (g) at (-1,0) {$g(\cdot;\phi)$};
\node[funcblock] (f) at (1,0) {$f(\cdot;\theta)$};

% Input labels
\node[above] (Y) at (-1,0.5) {$Y$};
\node[above] (X) at (1, 0.5) {$X$};

% Output labels
\node (y_rec) at (-1,-1) {$\tilde{Y}$};
\node (y_hat) at (1,-1) {$\hat{Y}$};

% Loss function
\node (loss) at (0,-1.5) {$\mathcal{L}(\tilde{Y}, \hat{Y},Y)$};

\node[left] at (-1.15,-0.55) {Masked};
\node[left] at (-1.15,-0.95) {Update};
\node[right] at (1.15,-0.55) {Masked};
\node[right] at (1.15,-0.95) {Update};

% Connections
\draw[line] (Y) -- (g);
\draw[line] (X) -- (f);
\draw[line] (g) -- (y_rec);
\draw[line] (f) -- (y_hat);
\draw[line] (y_rec) -- (-1, -1.5) -- (-0.75, -1.5);
\draw[line] (y_hat) -- (1, -1.5) -- (0.75, -1.5);
\draw[line] (-1, -1.5) -- (-1.125, -1.5) -- (-1.125, -0.25);
\draw[line] (1, -1.5) -- (1.125, -1.5) -- (1.125, -0.25);

\end{tikzpicture}
}
\caption{Working Pipeline of \model{}. $g(\cdot;\phi)$ is a reconstruction network; $f(\cdot; \theta)$ is a predictor; $\mathcal{L}$ uses reconstructed labels $\tilde{Y}$ to mask overfitted components.}
\label{fig:pipeline}
\end{wrapfigure}

Figure~\ref{fig:pipeline} depicts the general working pipeline, where we apply masked updates on a reconstruction network $g(\cdot;\phi)$ and a predictor model $f(\cdot;\theta)$ during training. During inference, the prediction will be generated directly by the updated $f(\cdot; \theta)$ \textbf{with no additional cost}.
% Empirical results confirm that this method effectively reduces overfitting in co-optimization, ensuring stable training and improved performance. 
To further generalize across models of varying complexities, we also propose to use Spectral Norm Regularization \citep{snr, sngan}. 

We summarize our contributions as follows:
\begin{itemize}[leftmargin=*]
\item \bfseries{Novel Perspective}: We explore a novel approach of self-enhancing TSF datasets by incorporating an auxiliary reconstruction task into TSF model training.
% approach to constructing candidate datasets via an auxiliary reconstruction task, without reliance on extra data sources.

\item \bfseries{Self-supervised Paradigm}: 
% \textcolor{blue}{We propose a self-supervised paradigm that adaptively generate pseudo labels in replacement of the overfitted raw labels.}
We propose a self-supervised paradigm that generates pseudo labels from reconstructions and adaptively replaces overfitted raw labels to improve models' generalizability.

\item \bfseries{Detailed Analysis}: We confirm the effectiveness of the proposed self-supervised mask formulation with extensive analyses over various backbones and real-world datasets.
\end{itemize}

\section{Preliminary}

\subsection{Problem Formulation}
\label{ssec:problem}

Many previous TSF studies adopt a paradigm that learns a direct mapping between two adjacent windows: the history series (\textit{inputs}) and the future series (\textit{labels}). 
Let the history series (\emph{resp}. future series) be $\left\{ \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N \right\} = X\in \mathbb{R}^{L\times N}$ (\emph{resp}. $\left\{\mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_N\right\}=Y \in \mathbb{R}^{H\times N}$) with time series length $L$ (\emph{resp}. $H$) and dataset size (number of segmented windows) $N$. 
%($N$ refers to the number segmented windows or the number of time points depending on the context in the following part of the paper).
% , where $N$ represents the size of the data, $L$ the length of the context series, and $H$ the length of the target series. 
% For simplicity, we formulate the problem under the univariate scenario.
For simplicity, we formulate the problem in the univariate scenario, as it naturally extends to the multivariate case by treating each variable as an additional dimension.


% We first review on the conventional formulation of the TSF problem:
\begin{definition}
\label{def:1}
A typical TSF process is formulated as a supervised-learning problem, i.e., to find $\theta^{\ast} = \arg \underset{\theta}{\min}\left\Vert f(X; \theta) - Y\right\Vert$, where a specified metric $||\cdot||$ is used to measure errors, typically the $\ell_1$- or $\ell_2$-norm.
\end{definition}

When splitting the data into training and test sets, the training set $D_{trn}=\left\{X_{trn}, Y_{trn}\right\}$ and the model $f(\cdot;\theta)$ can determine a minimal target error $\mathcal{L}_{tar}= \Vert f(X_{test};\theta^{\ast}) - Y_{test}\Vert$ on the test set $D_{test}=\left\{X_{test}, Y_{test}\right\}$. 

Usually, TSF models struggle on small or noisy datasets. Now, suppose we can obtain additional \textbf{\emph{candidate datasets}} beyond the observed raw dataset; ideally, this would address the issue.
In this sense, we assume a family of such candidate datasets $\mathcal{D} = \{D_i\}$, where the optimal candidate dataset $D_i^{\ast}$ enables better generalization for a predictor when evaluated by $\mathcal{L}_{tar}$ on the raw test set.
% In this sense, we assume a family of such datasets $\mathcal{D} = \{D_i\}$ satisfying the following properties:
% \begin{enumerate}[leftmargin=*]
%     \item \textbf{Feasibility}. Any model can be trained on any dataset $D_i$ in the family.
%     \item \textbf{Orderliness}. Datasets in the family can be sequentially ordered based on metrics other than the target error, allowing pre-test construction and evaluation.
%     \item \textbf{Minimality}. For a given model, the minimal target errors across all datasets form a set of target losses $\{ \mathcal{L}_{tar}^i \mid D_i \in \mathcal{D} \}$, where a minimum loss $\mathcal{L}^{\ast}$ exists for some dataset $D^{\ast} \in \mathcal{D}$.
% \end{enumerate}

Without constraints, defining such a dataset family can be overly arbitrary. 
% making finding $\mathcal{L}^{\ast}$ nearly intractable.
To handle this, we parameterize the family using a neural network $g(X;\phi) = \tilde{X}$, which is trained on a reconstruction loss $\mathcal{L}_{rec}= \Vert \tilde{X} - X  \Vert$. 
During an iterative optimization process (e.g., a typical full-batch gradient descent with learning rate $\eta$), a candidate dataset $D_i$ is generated at each iteration step $i$ as follows: 
\begin{equation}
\label{eq:family_param}
\begin{aligned}
    D_i &= \left\{\tilde{X}_i, \tilde{Y}_i\right\}\\
     &= \left\{\left\{g\left(x; \phi_i\right) \mid x \in X_i\right\}, \left\{g\left(y;\phi_i\right) \mid y \in Y_i\right\}\right\}, \\
    \phi_i &= \phi_0 -\sum\nolimits_{k=0}^{i-1}\sum\nolimits_{x \in X_i \cup Y_i} \eta \nabla_{\phi_k}\Vert g(x; \phi_k) - x\Vert.
\end{aligned}
\end{equation}
This approach shifts the focus from designing models with different inductive biases to identifying candidate datasets that improve a model's generalization. 
For TSF scenarios where data are often noisy and challenging to clean, replacing raw datasets with such parameterized candidates can lead to more robust performance. 
To conclude, we formalize the idea as follows.
\begin{proposition}
\label{prop:1}
Given time series data split into $D_{trn}=\left\{X_{trn}, Y_{trn}\right\}$ and $D_{test}=\left\{X_{test}, Y_{test}\right\}$, and a predictor model $f(\cdot;\theta)$. In a family of training sets $\mathcal{D_{\phi}}=\{D_{trn}^{i}\}$ parameterized by $g(\cdot;\phi)$ as in Eq.\ \ref{eq:family_param}, there exist an optimal $D^{\ast}=\{\tilde{X}^{\ast}, \tilde{Y}^{\ast}\}=\{\{g(x_j;\phi^{\ast})\}, \{g(y_j;\phi^{\ast})\}\}$ such that $$
\Vert f(X_{test};\theta^\ast(\phi^{\ast})) - Y_{test}\Vert \le \Vert f(X_{test};\theta^\ast(\phi^{i})) - Y_{test}\Vert, \forall \phi_i,
$$
where $\theta^{\ast}(\phi^{i})$ indicates that $\theta^{\ast}$ is optimized on $D_{trn}^{i}=\{\tilde{X}_{i},\tilde{Y}_{i}\}=\{\{g(x_j;\phi_i)\},\{g(y_j;\phi_{i})\}\}$.
\end{proposition}

\subsection{Proposed $g(\cdot;\phi)$ for Reconstruction}
\label{ssec:arch_g}
\begin{wrapfigure}{r}{0.22\textwidth}
\centering
\scalebox{.80}{
\begin{tikzpicture}[
    convblock/.style={rectangle, draw=ff_color!80!black, fill=ff_color, 
                  minimum width=2cm, minimum height=0.6cm, rounded corners=5pt,
                  text=black, font=\normalsize},
    concatblock/.style={rectangle, draw=add_norm_color!80!black, fill=add_norm_color, 
                     minimum width=0.6cm, minimum height=2.8cm, rounded corners=5pt,
                     text=black, font=\normalsize},
    mlpblock/.style={rectangle, draw=linear_color!80!black, fill=linear_color, 
                     minimum width=0.6cm, minimum height=2.8cm, rounded corners=5pt,
                     text=black, font=\normalsize},
    line/.style={->, draw=black!60, line width=0.8pt},
]
\definecolor{emb_color}{RGB}{252,224,225}
\definecolor{multi_head_attention_color}{RGB}{252,226,187}
\definecolor{add_norm_color}{RGB}{242,243,193}
\definecolor{ff_color}{RGB}{194,232,247}
\definecolor{softmax_color}{RGB}{203,231,207}
\definecolor{linear_color}{RGB}{220,223,240}
\definecolor{gray_bbox_color}{RGB}{243,243,244}
% Input
\node[above] (Y) at (0,1.7) {$Y$};

% Conv layers
\node[convblock] (conv1) at (0,1.2) {Conv};
\node[convblock] (conv2) at (0,0.4) {Conv};
\node[convblock] (conv3) at (0,-0.4) {Conv};
\node[convblock] (conv4) at (0,-1.2) {Conv};

% Concat and MLP blocks
\node[concatblock] (concat) at (1.8,0) {\rotatebox{90}{Concat}};
\node[mlpblock] (mlp) at (2.8,0) {\rotatebox{90}{FFN}};

% Output
\node[right] (Yhat) at (3.4,0) {$\tilde{Y}$};

% Vertical connections
\draw[line] (Y) -- (conv1);
\draw[line] (conv1) -- (conv2);
\draw[line] (conv2) -- (conv3);
\draw[line] (conv3) -- (conv4);

% Horizontal connections to concat
\draw[line] (conv1.east) -- (concat.west |- conv1.east);
\draw[line] (conv2.east) -- (concat.west |- conv2.east);
\draw[line] (conv3.east) -- (concat.west |- conv3.east);
\draw[line] (conv4.east) -- (concat.west |- conv4.east);

% Final connections
\draw[line] (concat) -- (mlp);
\draw[line] (mlp) -- (Yhat);

\end{tikzpicture}
}
\caption{Convolution-FFN reconstruction network.}
\label{fig:arch_rec}
\end{wrapfigure}
We proceed to introduce a simple reconstruction network used in subsequent exploration.
Similar to a predictor model, the reconstruction network operates in a sequence-to-sequence fashion, learning a function $g(\cdot;\phi)$ that maps raw series $Y$ to reconstructed series $\tilde{Y}$.
Note that reconstruction is applied only to $Y$, time series datasets are typically generated from a single series using a moving window approach, where $X$ and $Y$ are almost fully overlapped.
By skipping reconstruction for $X$, the predictor model can use raw series as inputs, avoiding \emph{extra} inference costs or potential distribution shifts.

As depicted in Figure~\ref{fig:arch_rec}, the proposed $g(\cdot;\phi)$ comprises four convolutional layers, a cross-layer concatenation for multi-resolution integration, and a lightweight feedforward network (FFN) to decode the reconstructed results. 
The convolutional layers primarily act as a parameterized smoothing mechanism, similar to techniques for seasonal-trend decomposition~\cite{dlinear, sparsetsf, cyclenet}. 
The FFN then mixes information from different positions in a time series to reconstruct each data point using features extracted by convolutional layers (further details in Appendix~\ref{sec:implement_reconstruct_net}).
Without introducing extra noise or patch-wise/point-wise masks \citep{patchtst}, we directly learn an identity mapping.

% Unlike other self-supervised tasks focused on precise reconstruction or robust latent representations, our reconstruction task supports the algorithm in the next section. Here, $g(\cdot;\phi)$ approximates the original time series while generating alternative reconstructions with varying fidelity. We emphasize that this design is tailored for our specific task, and we do not claim it outperforms other potential approaches.
% The reconstruction task serves the grid search presented in Section~\ref{sec:method} and differs from objectives in other self-supervised tasks, such as accurate reconstruction~\cite{ssl_con_gen, ssldenoisefinance} or  latent representation~\cite{timesurl, patchtst}. In essence, $g(\cdot;\phi)$ is designed to approximate the original time series, traversing diverse alternatives with varying levels of fidelity during the process. We do not claim that our design here is superior to other unexplored options for this novel task.
In essence, $g(\cdot;\phi)$ is designed to traverse diverse alternatives with varying levels of fidelity, which differs in purpose from the usual reconstruction task~\citep{ssl_con_gen, ssldenoisefinance, timesurl}. Due to this difference, the architecture of $g(\cdot;\phi)$ might benefit from novel designs; however, this is not examined in this paper. The proposed $g(\cdot; \phi)$ is merely a simple prototype to validate our ideas and is not claimed superior to other unexplored options for this novel task.

\section{Method}
\label{sec:method}

\subsection{Initial Case}
\label{ssec:initial_case}

\if 0
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\begin{algorithm}
\caption{Grid Search along $\ell_{rec}$}
\label{alg:grid_search}
\begin{algorithmic}
\STATE {\bfseries{Parameters:}} \\ $\phi$, parameters of reconstruction network $g(\cdot; \phi)$; \\ $\theta$, parameters of predictor model $f(\cdot;\theta)$ \\
\STATE {\bfseries{Optimizers:}} \\ $Opt_{\phi}$, the optimizer to update $\phi$ in the outer loop; \\ $Opt_{\theta}$, the optimizer to update $\theta$ in the inner loop \\ 

    \STATE {\bfseries{Initialize}} $\phi$, $Opt_{\phi}$ \\ 
    \FOR{$i=0$ \bfseries{to} $N$}
    \State \Comment{\textcolor{blue}}{$N$ for number of candidates} \\ 

        \STATE $j \leftarrow 0$ \\ 
        \STATE {Initialize} $\theta$, $Opt_{\theta}$ \\ 
        \WHILE{$\nabla_{\theta} > \alpha$ \AND $j\le M$}
            \STATE $\ell_{rec} \leftarrow 0$ \\ 
            \FOR{$(x, y) \in (X, Y)$}
                %\STATE {\tt $Opt_{\theta}$.zero\_grad()} \\
                \STATE $\tilde{y}_i \leftarrow g(y;\phi_i)$ \\
                \STATE $\ell_{pred} \leftarrow \Vert f(x;\theta) - \tilde{y}_i\Vert$ \\
                \STATE $\ell_{rec} \leftarrow \ell_{rec} + \Vert\tilde{y}_i - y\Vert$ \\
                \STATE {\tt $\ell_{pred}$.backward()} \\
                \STATE {\tt $\theta \leftarrow$ $Opt_{\theta}$.step($\theta$)} \\
            \ENDFOR
            \STATE $j\leftarrow j+1$ \\
        \ENDWHILE
        \STATE {\tt $\ell_{rec}$.backward()} \\ 
        \STATE {\tt $\phi_{i+1} \leftarrow$ $Opt_{\phi}$.step($\phi_i$)} \\
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\fi

We begin with a simple case where $g(\cdot; \phi)$ evolves from a randomly initialized state toward an approximation of the raw target series $Y$. This process can be viewed as a \emph{grid search} along the axis of the following reconstruction loss:
\begin{equation}
    \ell_{rec} = \Vert g(y;\phi) - y \Vert.
\end{equation}

In this setup, we optimize $g(\cdot;\phi)$ for reconstruction loss using full-batch gradient descent, while the predictor $f(\cdot;\theta)$ is optimized with mini-batch SGD in a vanilla training setting.
At each optimization epoch $i$, we freeze the current parameters of $g(\cdot;\phi_i)$ and generate a candidate dataset $D_{i}$. 
The predictor $f(\cdot;\theta)$ is re-initialized before the epoch and trained on $D_i$ until convergence. The prediction results on the original test set are recorded for each $g(\cdot;\phi_i)$ and corresponding trained $f(\cdot;\theta^{\ast}_{i})$.
A case experiment is conducted on the ETTh1 dataset using a vanilla 2-layer MLP model (for faster and more guaranteed convergence). 
The pseudo-code for this grid search is provided in Appendix~\ref{sec:gird_search_app}.
\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/initial_case/scatter_rec_target.pdf}
        \caption{Mean error $\Vert \hat{y} - y\Vert$ on test}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/initial_case/target_curve.pdf}
        \caption{Loss curve of grid search}
    \end{subfigure}
    \caption{Grid search results. Each scatter denotes a converged predictor $f(\cdot;\theta)$ on an individual dataset parameterized by $g(\cdot; \phi_i)$.}
    \label{fig:observation}
\end{figure}

So far, we can construct a series of candidate datasets $\{ D_i \}$ through a two-step optimization process, each easily distinguished by its reconstruction loss, which intuitively measures its similarity to the raw dataset. Evaluating the performance of the same predictor trained on these varying candidate datasets leads to three major observations:
% \textbf{Observation 1: Better Performance.} A direct proof of the potential for better-labeled candidates is shown in Figure~\ref{fig:observation}, where multiple scatter plots fall below the red dotted line, representing the baseline performance of the same predictor model trained on the raw datasets.
\begin{observation}[Improved Labels Enhance Performance]
% A direct demonstration of better-labeled candidates is shown in Figure~\ref{fig:observation}(a), namely those scatter plots falling below the red dotted line (indicating the baseline performance of the same predictor model trained on the raw dataset).
Figure~\ref{fig:observation}(a) demonstrates the improved performance of better-labeled candidates, as indicated by the scatter points below the red dotted line --- which represents the baseline performance of the predictor trained on the raw dataset.
\end{observation}

% \textbf{Observation 2: Inconsistent Performances w.r.t. Reconstruction Metric.} For a feasible method, we aim to determine the effectiveness or rank the performance of candidate datasets during training, eliminating the need to evaluate them on the test set. However, this is currently challenging, as datasets with similar $\ell_{rec}$ values can exhibit significant variations in actual performance.
\begin{observation}[Variable Performance w.r.t. Reconstruction Metric  $\ell_{rec}$]
A feasible method should evaluate or rank candidate datasets during training without relying on test set performance. However, this remains challenging as datasets with comparable $\ell_{rec}$ values frequently exhibit substantial differences in actual performance (see Figure~\ref{fig:observation}(a)).
\end{observation}

% \textbf{Observation 3: Difficult Training.} During the training process, the loss curve is highly unstable, suggesting that many potentially superior candidate datasets (or equivalently, 
% $g(\cdot;\phi)$) may be overlooked. Additionally, the training cost is a significant drawback. To ensure the convergence of the predictor model, we do not update $\phi$ until $\theta$ is fully optimized. This makes the grid search algorithm (Algorithm~\ref{alg:grid_search}) impractical for more complex models (e.g., \patchtst{} and \iTransformer{}) or larger datasets
\begin{observation}[Difficult Training]
Training involves an \emph{unstable} loss curve (Figure~\ref{fig:observation}(b)), meaning many potentially superior candidate datasets (or equivalently $g(\cdot;\phi)$) could be missed. 
Moreover, training is costly. To ensure the predictor converges, $\phi$ is only updated after $\theta$ is fully trained. This renders grid search impractical for more complex models (e.g., \patchtst{} and \iTransformer{}) or larger datasets.
\end{observation}
In the next section, we propose replacing the brute-force grid search algorithm with a \emph{co-objective training} approach that improves training stability and overall performance.

\subsection{Co-objective Training}
\label{ssec:co-objective}

The grid search (Section~\ref{ssec:initial_case}) is framed as a two-step optimization process with two distinct objectives involved in finding optimal candidate datasets.
The reconstruction optimization primarily provides a trajectory of parameters $\phi_i$, without emphasizing optimality. 
In contrast, the prediction optimization evaluates the predictor's actual performance.

Our analysis of grid search results suggests that simplifying the training process into a \textbf{co-objective optimization} would be beneficial. 
Since the solution of two-step optimization (though not optimal on the test set) essentially lies on the Pareto Front of the corresponding co-objective optimization (see Figure~\ref{fig:co-train}(b)), a natural approach is to search along this front. 
Again, the trajectory of the optimization, rather than its strict optimality, contributes to improved test set performance, the co-objective training can still facilitate the construction of effective candidate datasets.

A single-step optimization using mini-batch SGD would be sufficient, enabling a more smooth trajectory of $\phi$ during updates (Figure~\ref{fig:illustration}(b) vs~\ref{fig:illustration}(a) and Figure~\ref{fig:co-train}(a) vs Figure~\ref{fig:observation}(b)). Moreover, enabling gradient computation of $\ell_{pred} = \Vert \tilde{y} - \hat{y} \Vert$ w.r.t. $\phi$ introduces a regularization effect, making $\tilde{y}$ updated more cautiously towards $y$. 
This allows us to constrain the update of candidate datasets (now specifically represented by the reconstructed $\tilde{y}$) by jointly constraining gradients w.r.t. both $\theta$ and $\phi$:
% \begin{equation}
% \label{eq:constrained_optim}
% \begin{alignedat}{4}
% &\underset{\theta, \phi}{\text{minimize}}
%  \quad &&\mathcal{L} = \Vert \tilde{y} - y\Vert + \Vert  \tilde{y} - \hat{y}\Vert \\
% & &&\ \tilde{y} = g(y;\phi)\\
%  & &&\ \hat{y} = f(x;\theta)\\
% &\text{s.t.}  & \Vert\nabla_{\theta,\phi}&\tilde{y}_i\Vert \le \delta, \quad \forall \tilde{y}_i\in \tilde{Y}
% \end{alignedat} 
% \end{equation}
\begin{equation}
\label{eq:constrained_optim}
\begin{alignedat}{3}
&\underset{\theta, \phi}{\text{minimize}} \quad && \mathcal{L} = \Vert \tilde{y} - y \Vert + \Vert \tilde{y} - \hat{y} \Vert \\
&\text{subject to} \quad && \tilde{y} = g(y; \phi), \quad \hat{y} = f(x; \theta), \\
& && \Vert \nabla_{\theta, \phi} \tilde{y}_i \Vert \leq \delta, \quad \forall \tilde{y}_i \in \tilde{Y}.
\end{alignedat}
\end{equation}
where $\phi$ is trained using both loss terms whereas $\theta$ is trained solely on $\ell_{pred} = \Vert \tilde{y} - \hat{y}\Vert$.
A gradient constraint $\delta$ is added to ensure a \emph{smooth} trajectory of $\tilde{y}$, enabling the co-objective to traverse potential candidate datasets more carefully. 

The gradient constraint has a surrogate $\Vert\nabla_{\theta}f\Vert$ and practically implemented by Spectral Norm Regularization (\snr{}), as discussed in Section~\ref{ssec:snr} and Appendix~\ref{sec:discussion_gradient}. 
For simplicity, we temporarily omit this constraint, as a 2-layer MLP converges quickly with a small $\Vert \nabla_{\theta}f\Vert$. 
Using the same setting as the grid search, we evaluate the revised loss function for co-optimizing the predictor and the reconstruction network. 
Figure~\ref{fig:co-train} shows that this co-training improves $\ell_{target}$ loss while simplifying the two-step training process, leading to more stable optimization and reduced training costs.

However, as the co-training process progresses, it becomes increasingly prone to overfitting (see Figure~\ref{fig:co-train}(a)). Overfitting is a fundamental issue in machine learning, tied to the generalizability of models. 
In this specific case, this issue arises as the reconstructed dataset gradually approaches the raw dataset, causing the target loss to converge to those of the raw dataset. Similar to the two-step grid search, determining a \emph{reasonable threshold} to identify optimal parameters remains a challenge.

To address this specific overfitting issue, we propose solutions summarized in two main components of our method: Self-Correction with Adaptive Mask (\model{}) in Section~\ref{ssec:scam} and Spectral Norm Regularization (\snr{}) in Section~\ref{ssec:snr}.

% \begin{figure}[!htbp]
% \label{fig:co-train}
% \centering
% \begin{subfigure}[b]{.39\linewidth}
% \centering
% \includegraphics[width=\linewidth]{figures/surface transfer.pdf}
% \caption{Surface transfer}
% \end{subfigure}
% \begin{subfigure}[b]{.6\linewidth}
% \centering
% \includegraphics[width=\linewidth]{figures/improved prediction.pdf}
% \caption{Improved prediction on raw data}
% \end{subfigure}
% \caption{Loss curves of co-training process}
% \end{figure}

\begin{figure}
\centering
\begin{subfigure}[h]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/initial_case/co_train_curve.pdf}
\caption{Loss curve of co-objective}
\end{subfigure}
\begin{subfigure}[h]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/initial_case/scatter_rec_pred.pdf}
\caption{Simplified Pareto Front}
\end{subfigure}
\caption{Loss curve and Pareto Front of co-objective training. }

% \textcolor{blue}{(a) shows the loss curve of co-objective training, a lower $\ell_{target}$ is achieved, however cannot be identified during training since the overfitting problem. (b) shows a simplified Pareto Front of the co-objective where the optimal performance is reached in the middle.}}
\label{fig:co-train}
\end{figure}

\subsection{Self-Correction with Adaptive Mask (\model{})}
\label{ssec:scam}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/initial_case/curves_combined.pdf}
    \caption{Loss curves of 4 parts divided by the adaptive mask. From left to right, $2\vert\tilde{y}-y\vert \odot \overline{M_{<}}\odot M$, $2\vert\tilde{y}-\hat{y}\vert \odot M_{<}\odot M$, $\vert y - \hat{y}\vert\odot M$ and $\vert y - \hat{y}\vert\odot \overline{M}$. The four parts combined are the loss $\mathcal{L}$ in Eq.~\ref{eq:loss_revised}. $M$ refers to the mask where $m_i > 0$ and $\overline{M}$ refers to the mask where $m_i < 0$. The right y-axis (in green) indicates the value for $(\ell_{test} - \ell_{train})$.}
    \label{fig:overfitting_curve}
\end{figure*}
% \begin{figure}
%     \includegraphics[width=1\linewidth]{figures/initial case/bars.pdf}
%     \caption{Sharpness comparison of different components of $\mathcal{L}$}
%     \label{fig:sharpness_bar}
% \end{figure}

% \begin{figure}
%     \includegraphics[width=1\linewidth]{figures/initial case/revised_curve.pdf}
%     \caption{Loss curve comparison of two $\mathcal{L}$ s}
%     \label{fig:sharpness_bar}
% \end{figure}

\textbf{Mask Form of Self-supervised Loss}.
In a traditional supervised-learning paradigm, the target loss $\ell_{target} = \Vert \hat{y} - y\Vert$ is used only to train a model, implying that all data points are equally treated as valid labels. 
However, in our approach, where predictors are trained alongside a search for candidate datasets guided by the reconstruction loss, the labels perceived by the predictors are adaptively shifted.
%
Specifically, $\hat{y} = f(x;\theta)$ is trained to fit $\tilde{y}$, meaning only the second term is optimized for the predictor $f(\cdot; \theta)$ (Eq.~\ref{eq:constrained_optim}). 
The reconstruction loss term, on the other hand, is optimized to provide self-supervised labels for the predictor. 
We frame this as a self-supervised-learning paradigm that \emph{adaptively} adjusts labels in TSF problems. 
By comparing the revised loss with the traditional supervised loss, we can explicitly separate the auxiliary loss from the supervised loss:
% \begin{equation}
% \label{eq:reformulate_loss_sup}
% \begin{aligned}
% \mathcal{L} &= \left(y - \hat{y}\right)^2 + \textcolor{teal}{\left[\left(\tilde{y} - y\right)^2 + \left(\hat{y} - \tilde{y}\right)^2 - \left(y - \hat{y}\right)^2\right]} \\
% &= \mathcal{L}_{sup} + \textcolor{teal}{2\left(\tilde{y} - \hat{y}\right)\left(\tilde{y} - y\right)} \\
% &= \mathcal{L}_{sup} + \textcolor{teal}{\mathcal{L}_{aux}}\color{black}{.}
% \end{aligned}
% \end{equation}
\begin{equation}
\label{eq:reformulate_loss_sup}
\begin{aligned}
\mathcal{L} &= \underbrace{\left(y - \hat{y}\right)^2}_{\mathcal{L}_{sup}} +
\underbrace{\textcolor{teal}{\Big[\left(\tilde{y} - y\right)^2 + \left(\hat{y} - \tilde{y}\right)^2 - \left(y - \hat{y}\right)^2\Big]}}_{\textcolor{teal}{\mathcal{L}_{aux}}} \\
&= \mathcal{L}_{sup} + \textcolor{teal}{2\left(\tilde{y} - \hat{y}\right)\left(\tilde{y} - y\right)}. %\\
% &= \mathcal{L}_{sup} + \mathcal{L}_{aux}.
\end{aligned}
\end{equation}
When revisiting the objective $\mathcal{L}$ in co-training, the additional loss term $\mathcal{L}_{aux}$ does not directly contribute to the target objective. Instead, this term depends on the relative positions of $\hat{y}$, $\tilde{y}$ and $y$. 
When the reconstructed $\tilde{y}$ is viewed as \emph{a correction of labels}, $\mathcal{L}_{aux}$ indicates where the correction should be placed. 
Time series are naturally sparse in real scenarios, often containing spiking signals due to irregular events or anomalies.
$\mathcal{L}_{aux}$ encourages the reconstructed $\tilde{y}$ to lie between the prediction $\hat{y}$ and the actual labels $y$, which can undermine sparsity when used as labels.

Eq.\ \ref{eq:reformulate_loss_sup} is based on $\ell_2$-norm (Mean Squared Error, MSE). Alternatively, we can adopt the more error-robust $\ell_1$-norm (Mean Absolute Error, MAE) to reformulate: 
% \begin{equation}
%     \begin{aligned}
%         \mathcal{L} &= \vert y - \hat{y}\vert + \textcolor{teal}{\left(\vert\tilde{y} - \hat{y}\vert + \vert\tilde{y} - y\vert - \vert y - \hat{y}\vert\right)} &\\
%         &\ \text{Let $A =\tilde{y} - \hat{y}, B=\tilde{y}-y$}\\
%         &= \mathcal{L}_{sup} +  
%         \textcolor{teal}{\left(\vert A\vert + \vert B\vert - \vert A - B\vert\right)} \\ 
%         & = \mathcal{L}_{sup} +
%         \textcolor{teal}{
%         \left \{
%         \begin{aligned}
%             &2\min \left\{ \vert A\vert, \vert B\vert\right\}, &when\ AB>0\\
%             &0, &when\ AB\leq 0
%         \end{aligned}  
%         \right .} \\ 
%         &\ \text{Let $m=(\tilde{y} - \hat{y})(\tilde{y} - y)$}\\
%         & = \mathcal{L}_{sup} +
%         \textcolor{teal}{
%         \left \{
%         \begin{aligned}
%             &2\min \left\{ \vert \tilde{y} - \hat{y}\vert, \vert \tilde{y} - y\vert\right\},  &when\ m>0\\
%             &0, &when\ m\leq 0
%         \end{aligned}  
%         \right .} &\\ 
%         &= \mathcal{L}_{sup} + 
%         \textcolor{teal}{2 \left (\vert \tilde{y} - \hat{y}\vert\odot M_{<} + \vert \tilde{y} - y\vert \odot \overline{M_{<}} \right)\odot M} \\
%         &= \mathcal{L}_{sup} + 
%         \textcolor{teal}{\mathcal{L}_{aux}}
%     \end{aligned}.
%     \label{eq:loss_revised}
% \end{equation}
%
\begin{equation}
\label{eq:loss_revised}
\begin{aligned}
\mathcal{L} &= \vert y - \hat{y} \vert + 
\textcolor{teal}{\left( \vert \tilde{y} - \hat{y} \vert + \vert \tilde{y} - y \vert - \vert y - \hat{y} \vert \right)} \\ 
&\text{Let } A = \tilde{y} - \hat{y}, \; B = \tilde{y} - y, \\
\mathcal{L} &= \mathcal{L}_{sup} + 
\textcolor{teal}{\left( \vert A \vert + \vert B \vert - \vert A - B \vert \right)} \\ 
&= \mathcal{L}_{sup} + 
\textcolor{teal}{
\begin{cases} 
    2 \min \{ \vert A \vert, \vert B \vert \}, & \text{if } AB > 0, \\
    0, & \text{if } AB \leq 0
\end{cases}
} \\ 
&\text{Let } m = (\tilde{y} - \hat{y})(\tilde{y} - y), \\
\mathcal{L} &= \mathcal{L}_{sup} + 
\textcolor{teal}{
\begin{cases} 
    2 \min \{ \vert \tilde{y} - \hat{y} \vert, \vert \tilde{y} - y \vert \}, & \text{if } m > 0, \\
    0, & \text{if } m \leq 0
\end{cases}
} \\ 
&= \mathcal{L}_{sup} + 
\textcolor{teal}{2 
\left( \vert \tilde{y} - \hat{y} \vert \odot M_{<} + \vert \tilde{y} - y \vert \odot \overline{M_{<}} \right) \odot M}. %\\
% &= \mathcal{L}_{sup} + \textcolor{teal}{\mathcal{L}_{aux}}.
\end{aligned}
\end{equation}
Here, $M$ is a binary mask defined by $m = (\tilde{y} - \hat{y})(\tilde{y} - y) > 0$, $M_{<}$ ensures $\vert \tilde{y} - \hat{y}\vert < \vert \tilde{y} - y\vert$, and $\overline{M_{<}}$ represents its complement.
$M$ functions similarly to $\mathcal{L}_{aux}$ in Eq.\ \ref{eq:reformulate_loss_sup} while $M_{<}$ ensures $\tilde{y}$ is optimized with a relatively small margin. 

\textbf{Decoupling Overfitted Components by Adaptive Masks}.
From the above derivation, we obtain a mask-based co-training loss, allowing us to analyze the causes of overfitting via the mask. 
As described for $\mathcal{L}_{aux}$ in Eq. \ref{eq:reformulate_loss_sup}, the mask $M$ defines the relative positions of $y$, $\tilde{y}$, and $\hat{y}$.
%
Specifically, $M$ masks $\ell_{pred}$ and $\ell_{rec}$ to zero when $m_i = (\tilde{y}_i - \hat{y}_i)(\tilde{y}_i - y_i) <0$. 
Similarly, $\ell_{target} = \vert\hat{y} - y\vert$ can also be divided using this mask. 
%
By comparing train/test differences in the divided losses (particularly $\ell_{target}$, since $\ell_{pred}$ and $\ell_{rec}$ are zero when $m_i < 0$), we identify that overfitting primarily stems from $\ell_{target}\ \odot M$, as shown in Figure~\ref{fig:overfitting_curve}. 

Further evidence is provided by analyzing the loss landscape. A sharpness metric for optimization, proposed by \citep{samformer}, measures the generalization capability of a model. 
Specifically, the sharpness metric is defined as $\lambda_{max} = \Vert \nabla^2_{\theta}\mathcal{L} \Vert_2^2$, the largest eigenvalue of the Hessian matrix. 
A higher $\lambda_{max}$ indicates a sharper loss landscape, which correlates with a worse generalization or more severe overfitting. 
By computing this metric on the converged parameters, we observe that $\ell_{target} \odot M$ does exhibit a sharper landscape compared to $\ell_{target} \odot \overline{M}$.
%
To address this, we keep the less sharp part of the $\mathcal{L}_{sup}$ term, i.e., $\mathcal{L}_{sup} \odot \overline{M}$, resulting in the revised loss:
\begin{equation}
    \begin{aligned}
        \mathcal{L} & = \Vert y - \hat{y}\Vert \textcolor{teal}{\ \odot \ \overline{M}} \\
        & + \textcolor{teal}{2 \left (\Vert \tilde{y} - \hat{y}\Vert\odot M_{<} + \Vert \tilde{y} - y\Vert \odot \overline{M_{<}} \right)\odot M}. \\
    \end{aligned}   
\end{equation}
Training with this revised loss reduces the sharpness of the loss landscape. As shown in Figure~\ref{fig:compare_revised}, this effectively mitigates overfitting in the co-training scenario. 
This represents the final loss form in our proposed method, termed Self-Correction with Adaptive Mask (\model{}), where the mask $M$ is constructed based on an auxiliary reconstruction task. 
The adaptive $M$ effectively identifies and removes overfitted components of time series labels, enabling the search for more validated parametric candidate datasets $g(y;\phi_i)$. 
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=1\linewidth]{figures/initial_case/bars.pdf}
        \caption{Sharpness comparison}
        \label{fig:sharpness_comp}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=1\linewidth]{figures/initial_case/revised_curve.pdf}
        \caption{Loss curve comparison}
        \label{fig:loss_comp}
    \end{subfigure}
    \caption{Effectiveness of the revised loss form}
    \label{fig:compare_revised}
\end{figure}

\subsection{Spectral Norm Regularization (\snr{})}
\label{ssec:snr}

Note that we have omitted the gradient constraints in Eq.\ \ref{eq:constrained_optim}. 
This term is, in fact, positively correlated with $\nabla_{\theta}f(x;\theta)$ (discussed in Appendix~\ref{sec:futher_analysis}). 
This further supports our previous analysis, as we have only tested MLP models, which converge easily with lower $\nabla_{\theta}f$. However, when the predictor is replaced by a \emph{Transformer-based} model, the dominant source of overfitting shifts from $\ell_{target}$ to $\ell_{pred}$. 
This phenomenon is not unique to our self-supervised learning paradigm. 
For example, \citep{samformer} proposed sharpness-aware optimization to address overfitting in supervised settings. 
While gradient penalties are theoretically effective, they may not be practical for complex models like Transformers, as computing second-order derivatives can significantly hinder optimization.

A more direct approach is to regularize parameters using the sharpness metric, known as \emph{Spectral Norm Regularization} \citep{sngan, snr}:
\begin{equation}
    \label{eq:snr}
    W_{normalized} = \gamma\cdot\frac{W}{\Vert W\Vert_2},
\end{equation}
where $\Vert \cdot \Vert_2$ is the spectral norm (the largest eigenvalue of parameter matrix) and $\gamma$ is a learnable scale factor. 
%
When applying to self-attention (SA) in Transformer-based architecture, \snr{} significantly undermines the expressiveness of attention score matrices (entropy collapse \citep{collapse}). Hence, \citet{samformer} conclude that \snr{} is inapplicable to SA parameters. 
However, we observe that linear layers --- typically the embedding layer before SA and the projection layer after SA --- also contribute to the overall sharpness of the loss landscape. 
Consequently, we propose applying \snr{} selectively to the pre- and post-SA linear layers. In this way, \snr{} can work with MLP-based models as they are also composed of multiple linear layers. Further empirical studies on \snr{} are discussed in Section~\ref{ssec:snr_results}.

\begin{table*}[t]
\centering
 \caption{Performance boost by adding \model{} and \snr{} to different backbones with better results in \textbf{bold}.}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{@{}c|cc
>{\columncolor[HTML]{E4F8F4}}c 
>{\columncolor[HTML]{E4F8F4}}c |cc
>{\columncolor[HTML]{E4F8F4}}c 
>{\columncolor[HTML]{E4F8F4}}c |cc
>{\columncolor[HTML]{E4F8F4}}c 
>{\columncolor[HTML]{E4F8F4}}c |cc
>{\columncolor[HTML]{E4F8F4}}c 
>{\columncolor[HTML]{E4F8F4}}c}
\toprule
Models      & \multicolumn{2}{c}{\mlp{}} & \multicolumn{2}{c|}{\cellcolor[HTML]{E4F8F4}\textbf{+ Ours}} & \multicolumn{2}{c}{\cyclenet{}} & \multicolumn{2}{c|}{\cellcolor[HTML]{E4F8F4}\textbf{+Ours}} & \multicolumn{2}{c}{\patchtst} & \multicolumn{2}{c|}{\cellcolor[HTML]{E4F8F4}\textbf{+ Ours}} & \multicolumn{2}{c}{\itrans{}} & \multicolumn{2}{c}{\cellcolor[HTML]{E4F8F4}\textbf{+Ours}} \\ \midrule
Metric      & MSE               & MAE     & MSE                       & MAE                      & MSE               & MAE      & MSE                       & MAE                      & MSE      & MAE               & MSE                       & MAE                      & MSE               & MAE     & MSE                       & MAE \\ 
\midrule
ETTh1           & 0.464 & 0.448 & \textbf{0.437} & \textbf{0.433} & 0.457 & 0.441 & \textbf{0.431} & \textbf{0.429} & 0.469 & 0.455 & \textbf{0.427} & \textbf{0.433} & 0.454 & 0.448 & \textbf{0.431} & \textbf{0.440} \\
ETTh2           & 0.382 & 0.405 & \textbf{0.366} & \textbf{0.394} & 0.388 & 0.409 & \textbf{0.362} & \textbf{0.393} & 0.387 & 0.407 & \textbf{0.370} & \textbf{0.398} & 0.383 & 0.407 & \textbf{0.377} & \textbf{0.402} \\
ETTm1           & 0.391 & 0.402 & \textbf{0.388} & \textbf{0.398} & 0.379 & 0.396 & \textbf{0.368} & \textbf{0.388} & 0.387 & 0.400 & \textbf{0.381} & \textbf{0.394} & 0.407 & 0.410 & \textbf{0.387} & \textbf{0.399} \\
ETTm2           & 0.280 & 0.325 & \textbf{0.276} & \textbf{0.322} & 0.266 & 0.341 & \textbf{0.262} & \textbf{0.309} & 0.281 & 0.326 & \textbf{0.281} & \textbf{0.326} & 0.288 & 0.332 & \textbf{0.283} & \textbf{0.327} \\
Electricity     & 0.204 & 0.285 & \textbf{0.203} & \textbf{0.283} & 0.168 & 0.259 & \textbf{0.166} & \textbf{0.258} & 0.205 & 0.290 & \textbf{0.191} & \textbf{0.275} & 0.178 & 0.270 & \textbf{0.173} & \textbf{0.267} \\
Traffic         & 0.522 & 0.335 & \textbf{0.494} & \textbf{0.308} & 0.472 & 0.301 & \textbf{0.448} & \textbf{0.290} & 0.481 & 0.304 & \textbf{0.455} & \textbf{0.288} & 0.428 & 0.282 & \textbf{0.411} & \textbf{0.266} \\
Weather         & 0.262 & 0.281 & \textbf{0.258} & \textbf{0.278} & 0.243 & 0.271 & \textbf{0.242} & \textbf{0.268} & 0.259 & 0.281 & \textbf{0.253} & \textbf{0.275} & 0.258 & 0.278 & \textbf{0.257} & \textbf{0.278} \\
\bottomrule
\end{tabular}
}
\label{tab:boost}
\end{table*}

\section{Experiment and Analysis}

We address three major questions for experiments:
\begin{itemize}[leftmargin=*]
\item \textbf{Q1}: Is \model{} effective across different backbone models and datasets with varying features? 
\item \textbf{Q2}: How do \model{} and \snr{} contribute to the potential improvement in model performance? 
\item \textbf{Q3}: How does the self-supervised reconstruction task benefit the predictor models?
\end{itemize}

Our main evaluation involves seven datasets: Electricity, Weather, Traffic, and four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2), all of which are well-established TSF benchmarks and publicly available \citep{autoformer}.
%
We also test the proposals using four PeMS datasets of a larger scale, as reported in Appendix~\ref{ssec:exp_main}.
%
The predictor (i.e., the backbone model integrated with \model{}) covers representative TSF models, including both MLP-based and Transformer-based architectures. 
%
\mlp{} \citep{rlinear} is a vanilla 2-layer baseline equipped with RevIN \citep{revin} while \cyclenet{} \citep{cyclenet} is a SOTA MLP-based model explicitly capturing cyclic trend in time series. 
%
\patchtst{} \citep{patchtst} and \iTransformer{} \citep{itrans} are Transformer-based models, representing channel-independent and channel-dependent methods, respectively.
Following previous settings~\citep{informer, autoformer, timesnet} for direct and fair comparison, we set prediction length $H\in \{96, 192, 336, 720\}$ and look-back length to 96 for all datasets. 
%
We provide dataset descriptions, implementation details, and reproduction instructions in Appendix~\ref{sec:experimental_details}. 
%Our code is available at \href{https://anonymous.4open.science/r/SCAM-BDD3}{https://anonymous.4open.science/r/SCAM-BDD3}.
% , with code available at \href{https://anonymous.4open.science/r/Self-Correction-with-Adaptive-Masks-6029}{https://anonymous.4open.science/r/Self-Correction-with-Adaptive-Masks-6029}.

\subsection{Main Experiment (Q1)}

Table~\ref{tab:boost} demonstrates consistent performance improvements in all major backbones when \model{} and \snr{} are incorporated in the self-supervised-learning paradigm.
The full results with detailed breakdowns by prediction lengths are provided in Appendix~\ref{ssec:exp_main}.
%
These gains are particularly notable on ETT datasets, which are known for their noisy nature and relatively small size. 
Notably, Transformer-based models like \patchtst{} and \itrans{}, which typically underperform compared to lightweight models \mlp{} and \cyclenet{} on these datasets, show significant enhancements in generalization with \model{}. 
On the Weather dataset, the boost is more modest, likely due to the intrinsic chaotic nature of atmospheric data. 

Regarding \textbf{Q1}, our method demonstrates general effectiveness across various backbones and datasets. 
A well-known discrepancy between MLP-based and Transformer-based models is their dataset preferences: 
Transformer-based methods excel on large, regular datasets, while MLP-based methods perform better on noisy datasets. 
\model{} helps bridge this gap by enabling Transformer-based models to perform competitively on traditionally challenging datasets and enhancing the robustness of MLP-based models. 
% The full results of Table~\ref{tab:boost} are shown in Appendix~\ref{ssec:exp_main}. 

\subsection{Ablation Study (Q2)}
\subsubsection{Contributions of \model{} and \snr{} }
\label{ssec:contribution_comp}

\begin{table}[!ht]

\centering
\parbox{0.5\textwidth}{\centering\caption{Ablation study on \model{} and \snr{} for \cyclenet{}.\label{tab:abl_cyclenet}}}
% \caption{Ablation study on \model{} and \snr{} for \cyclenet{}.}
\resizebox{0.5\textwidth}{!}{
    \begin{tabular}{@{}cc|cc|cc|cc|cc|cc@{}}  
    \toprule
   \multirow{2}{*}{+\model{}} & \multirow{2}{*}{+\snr{}} & \multicolumn{2}{c|}{ETTh1} & \multicolumn{2}{c|}{ETTh2} & \multicolumn{2}{c|}{ETTm1} & \multicolumn{2}{c|}{ETTm2} & \multicolumn{2}{c}{Weather} \\
   \cline{3-12}
    
\multicolumn{1}{c}{}                         & \multicolumn{1}{c|}{}                        & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} \\
\midrule
% \midrule
\xmark & \xmark    & 0.444          & 0.436          & 0.381          & 0.407          & 0.379          & 0.397          & 0.266          & 0.313          & 0.248          & 0.273          \\
\xmark & \cmark    & 0.438          & 0.432          & 0.372          & 0.398          & 0.375          & 0.392          & 0.265          & 0.311          & 0.246          & 0.272          \\
\cmark & \xmark    & 0.436          & 0.432          & 0.365          & 0.395          & 0.371          & 0.390          & 0.263          & 0.311          & 0.242          & 0.270          \\
\midrule
\cmark & \cmark    & \textbf{0.431} & \textbf{0.429} & \textbf{0.362} & \textbf{0.393} & \textbf{0.368} & \textbf{0.388} & \textbf{0.262} & \textbf{0.309} & \textbf{0.242} & \textbf{0.268} \\
\bottomrule
\end{tabular}
}

% \begin{table}[!]
%     \centering
%     \begin{tabular}{c|c}
%          &  \\
%          & 
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

\caption{Ablation study on \model{} and \snr{} for \itrans{}.}
\resizebox{0.5\textwidth}{!}{
    \begin{tabular}{@{}cc|cc|cc|cc|cc|cc@{}}  
    \toprule
   \multirow{2}{*}{+\model{}} & \multirow{2}{*}{+\snr{}} & \multicolumn{2}{c|}{ETTh1} & \multicolumn{2}{c|}{ETTh2} & \multicolumn{2}{c|}{ETTm1} & \multicolumn{2}{c|}{ETTm2} & \multicolumn{2}{c}{Weather} \\
   \cline{3-12}
    
\multicolumn{1}{c}{}                         & \multicolumn{1}{c|}{}                        & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} & \multirow{1}{*}[-0.15ex]{MSE} & \multirow{1}{*}[-0.15ex]{MAE} \\
\midrule
% \midrule
\xmark & \xmark    & 0.438                   & 0.440                   & 0.400                   & 0.417                   & 0.413                   & 0.414                   & 0.299                   & 0.341                   & 0.285                   & 0.306                   \\
\xmark & \cmark    & 0.443                   & 0.445                   & 0.399                   & 0.417                   & 0.413                   & 0.415                   & 0.306                   & 0.347                   & 0.283                   & 0.302                   \\
\cmark & \xmark    & 0.436                   & \textbf{0.438}                   & 0.381                   & 0.404                   & 0.391                   & 0.404                   & 0.293                   & 0.342                   & 0.274                   & 0.289                   \\
\midrule
\cmark & \cmark    & \textbf{0.431}          & 0.440          & \textbf{0.377}          & \textbf{0.402}          & \textbf{0.387}          & \textbf{0.399}          & \textbf{0.283}          & \textbf{0.327}          & \textbf{0.257}          & \textbf{0.278}                  \\
\bottomrule
\end{tabular}
}
\label{tab:abl_itrans}
\end{table}

% To answer \textbf{Q2}, we demonstrate the quantitative performance improvements brought by \model{} and \snr{} through the results of an ablation study, as shown in Tables~\ref{tab:abl_cyclenet} and~\ref{tab:abl_itrans}. For simplicity, we select two representative models: \itrans{} and \cyclenet{}, representing SOTA Transformer-based and MLP-based models, respectively.
To answer \textbf{Q2}, we present the performance gains from \model{} and \snr{} through an ablation study (Tables~\ref{tab:abl_cyclenet} and~\ref{tab:abl_itrans}), using \itrans{} (Transformer-based) and \cyclenet{} (MLP-based) as representatives (the SOTA one in their category).

\snr{}, a practical alternative to the gradient penalty in Eq.\ \ref{eq:penalty_optim}, consistently enhances performance across backbones. 
However, \itrans{}, being more prone to overfitting on small datasets (discussed in \citep{samformer}), benefits less from \snr{} compared to \cyclenet{}, likely due to insufficient data for generalization in higher-complexity architectures. 
With \model{}, both models exhibit significant improvements, leveraging the expanded effective training data.

In summary, \model{} is the primary driver of performance gains, offering consistent and clear improvements. 
While \snr{} enhances results as a standalone method, it serves best as a complement to \model{}, acting as an effective surrogate for the gradient penalty in the self-supervised objective.
% While \snr{} provides possible benefits when used alone, it serves more as an effective surrogate for the gradient penalty in the self-supervised objective, further enhancing performance when combined with \model{}.

\subsubsection{In-depth Examination on \snr{}}
\label{ssec:snr_results}

We further verify the effectiveness of \snr{} by analyzing the loss landscape. 
Transformer-based models, as discussed, are more prone to overfitting, particularly on noisy datasets. 
This weakens our adaptive mask's ability to discard overfitted components, as $\ell_{pred}$ --- the predictor's training loss --- also contributes to overfitting. 
Gradient penalty offers a solution to this issue.
For TSF models specifically, where both inputs and outputs are time series with abrupt distribution shifts, parameter robustness is critical for mitigating overfitting.
Previous works \citep{snr, sngan} have shown that regulating parameters via their spectral norm enhances stability against input perturbations, which is particularly beneficial in TSF.

\setlength{\intextsep}{2pt}%
\setlength{\columnsep}{6pt}%
\begin{wrapfigure}[14]{r}{0.14\textwidth}
\centering
\scalebox{.55}{
\begin{tikzpicture}
\definecolor{emb_color}{RGB}{252,224,225}
\definecolor{multi_head_attention_color}{RGB}{252,226,187}
\definecolor{add_norm_color}{RGB}{242,243,193}
\definecolor{ff_color}{RGB}{194,232,247}
\definecolor{softmax_color}{RGB}{203,231,207}
\definecolor{linear_color}{RGB}{220,223,240}
\definecolor{gray_bbox_color}{RGB}{243,243,244}

\draw[fill=gray_bbox_color, line width=0.046875cm, rounded corners=0.300000cm] (-0.32, 4.2) -- (2.82000, 4.2) -- (2.82000, 1.40000) -- (-0.32, 1.40000) -- cycle;
\node[text width = 3.50000cm, align=center] at (1.26, 1.75) {Encoder};

\node[text width=2.500000cm, anchor=north, align=center] at (1.250000,-0.2500000) {Input};
\draw[line width=0.046875cm, -latex] (1.250000, -0.300000) -- (1.250000, 0.300000);

\draw[line width=0.046875cm, fill=ff_color, rounded corners=0.100000cm] (0.000000, 0.800000) -- (2.500000, 0.800000) -- (2.500000, 0.300000) -- (0.000000, 0.300000) -- cycle;
\node[text width=2.500000cm, align=center] at (1.250000,0.550000) {Embedding};
\draw[line width=0.046875cm, -latex] (1.250000, 0.800000) -- (1.250000, 1.4);

% \draw[-latex, line width=0.046875cm, rounded corners=0.200000cm] (1.250000, 1.530000) -- (-0.750000, 1.530000) -- (-0.750000, 3.430000) -- (0.000000, 3.430000);
% \draw[-latex, line width=0.046875cm, rounded corners=0.200000cm] (1.250000, 1.730000) -- (0.312500, 1.730000) -- (0.312500, 2.130000);
% \draw[-latex, line width=0.046875cm, rounded corners=0.200000cm] (1.250000, 1.730000) -- (2.187500, 1.730000) -- (2.187500, 2.130000);

\draw[line width=0.046875cm, fill=multi_head_attention_color, rounded corners=0.100000cm] (0.000000, 3.030000) -- (2.500000, 3.030000) -- (2.500000, 2.130000) -- (0.000000, 2.130000) -- cycle;
\node[text width=2.500000cm, align=center] at (1.250000,2.580000) {Channel-Wise \vspace{-0.05cm} \linebreak Self-Attention};
\draw[line width=0.046875cm] (1.250000, 3.030000) -- (1.250000, 3.180000);

\draw[line width=0.046875cm, fill=add_norm_color, rounded corners=0.100000cm] (0.000000, 3.680000) -- (2.500000, 3.680000) -- (2.500000, 3.180000) -- (0.000000, 3.180000) -- cycle;
\node[text width=2.500000cm, align=center] at (1.250000,3.430000) {FFN};
\draw[line width=0.046875cm, -latex] (1.250000, 3.680000) -- (1.250000, 4.70000);

% \node[text width=2.00000cm, anchor=north, align=center] at (-0.10000,4.5500000) {\texttt{Transformer}};

\draw[line width=0.046875cm, fill=ff_color, rounded corners=0.100000cm] (0.000000, 5.20000) -- (2.500000, 5.20000) -- (2.500000, 4.70000) -- (0.000000, 4.70000) -- cycle;
\node[text width=2.500000cm, align=center] at (1.250000,4.95000) {projector};
\draw[line width=0.046875cm, -latex] (1.250000, 5.20000) -- (1.250000, 5.80000);

% \draw[line width=0.046875cm, fill=ff_color, rounded corners=0.100000cm] (0.000000, 6.30000) -- (2.500000, 6.30000) -- (2.500000, 5.80000) -- (0.000000, 5.80000) -- cycle;
% \node[text width=2.500000cm, align=center] at (1.250000,6.050000) {RevIN$^{-1}$};
% \draw[line width=0.046875cm, -latex] (1.250000, 6.30000) -- (1.250000, 6.90000);

\node[text width=2.500000cm, anchor=south, align=center] at (1.250000,5.75) {Output };

\end{tikzpicture}
}
\caption{\itrans{} as a TST example}
\label{fig:samformer-diagram}
\vspace{-4em}
\end{wrapfigure}

In Figure~\ref{fig:snr_sharpness}, the bars represent the sharpness of different components in \itrans{}, divided into three parts: embedding, encoder, and projector (see Figure~\ref{fig:samformer-diagram}). 
The embedding and projector are linear layers, while the encoder comprises channel-wise attention blocks. 
This architectural pattern is common among Time Series Transformers (TSTs), with the encoder being the primary focus and less emphasis placed on pre- or post-encoder linear layers.
%
Without \snr{}, the input linear layer (embedding) exhibits the highest sharpness, indicating it is the most overfitted component (see red bars). 
This suggests that overfitting in Transformers originates not only from self-attention mechanisms but also from the linear layers. 
By applying \snr{} to the pre- and post-encoder linear layers (Section~\ref{ssec:snr}), we observe smoother loss landscapes, validating \snr{}'s ability to reduce sharpness effectively.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/experiment/snr_sharpness.pdf}
    \caption{Sharpness of difference components in \itrans{}.}
    \label{fig:snr_sharpness}
\end{figure}

\begin{wrapfigure}{r}{0.22\textwidth}
    
    \includegraphics[width=\linewidth]{figures/experiment/snr_mse.pdf}
    \caption{Empirical results on different variants of \snr{}.}
    \label{fig:snr_mse}
\end{wrapfigure}

To determine the best practices for \snr{}, we conduct multiple runs with different random seeds to evaluate various design choices. 
As shown in Figure~\ref{fig:snr_mse}, applying \snr{} to the post-encoder linear layer or both linear layers proves most effective: `post-\snr{}' achieves lower mean metrics and reduced variance, while `both \snr{}' yields better mean performance but with higher variance. We recommend both practices, depending on the use case, and adopt `both \snr{}' for all tests in this paper.

\subsection{\model{}: A Multiple Instance Learning View (Q3)}
\label{ssec:MIL}

\begin{wrapfigure}{r}{0.64\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/experiment/adaptive_mask.pdf}
    \caption{Case study of visualized mask.}
    \label{fig:case_mask}
\end{wrapfigure}
Multiple Instance Learning (MIL) is a classical weakly-supervised binary classification problem where typically a bag of instances is labeled positive if at least one instance is positive. 
\citep{millet, timemil} extend MIL to Time Series Classification by treating an input window as a bag of instances (time points), enabling a model to predict based on instance-level classifications that are more interpretable. 

To answer \textbf{Q3}, we hypothesize that the effectiveness of \model{} shares similarities with MIL by leveraging instance-level signals, though it does not strictly follow a MIL framework. 
%
For a quick illustration, we synthesize a toy dataset where the ground truth is defined as $y=A\sin(\omega_{1}x) + B\sin(\omega_{2}x)$, with added noise sampled alternately from $\mathcal{N}(0, \sigma_1)$ and $\mathcal{N}(0, \sigma_2)$ in different windows.
% constructed by noised observations as case study. Specifically, we let ground truth be $y=A\sin(\omega_{1}x) + B\sin(\omega_{2}x)$ and add noises with different deviation $\mathcal{N}(0, \sigma_1)$ and $\mathcal{N}(0, \sigma_2)$ interchangeably in different windows. 
As shown in Figure~\ref{fig:case_mask}, when the noise deviation $\sigma$ is large (left part), \model{} tends to optimize $\ell_{rec} = M\odot \overline{M_{<}}\Vert \tilde{y} - y\Vert$, prioritizing robust reconstructions; when $\sigma$ is small, \model{} shifts focus to optimizing $\ell_{pred} = M\odot M_{<} \Vert \tilde{y} - y\Vert$ and $\ell_{target} = \Vert \hat{y} - y\Vert$, emphasizing accurate predictions.

This study only reveals a part of \model{}'s self-supervision effectiveness, which is further explored in Appendix~\ref{sec:futher_analysis}.


\section{Related Work}
% In this section, we introduce the related work from the meta-learning perspective and self-supervised learning perspectives. We include the related work about TSF models in Appendix~\ref{ssec:related_extension}. 

We discuss related techniques from meta-learning and self-supervised learning perspectives, with an inventory of TSF models in Appendix~\ref{ssec:related_extension}.

\textbf{Meta-Learning for Time Series}.
Meta-Learning, by definition, seeks to perform in a learn-to-learn paradigm. Generally speaking, meta-learning includes optimization-based, model-based, and metric-based methods. Optimization-based methods target optimal initial parameters \citep{maml}, often involving a two-loop optimization process \citep{metats, deeptime}. Model-based methods aim to determine suitable models, typically from an ensemble, based on predefined tasks \citep{hive-cote, hive-cote2} or activation states \citep{autoforecast}. 
Metric-based methods \citep{adarnn, deeptime} learn a metric function that provides more expressive measurements of distances between data samples, commonly comparing training and validation samples. 

Our method \model{} aligns with the boarder scope of meta-learning. 
Specifically, the grid search (Algorithm~\ref{alg:grid_search}) follows the two-loop structure similar to optimization-based methods. However, it diverges by focusing on \emph{dataset space} rather than \emph{parameter space}.
The goal of the grid search is not to solve the optimization problem directly but to leverage the trajectory of optimization.
Additionally, the final mask form of \model{} in essence provides a more accurate metric tailored for a supervised-learning setting. 
%
While \textsc{DeepTime} \citep{deeptime} and \textsc{AdaRNN} \citep{adarnn} share a related idea, there are key differences. 
\textsc{DeepTime} focuses more on a time-index forecast paradigm, whereas \textsc{AdaRNN} applies metric learning to hidden states. 
Both works learn metrics on the sample level (a window of time series), whereas ours focuses on the instance level (individual data points of time series).

\textbf{Self-supervised Learning for Time Series}.
Self-supervised learning trains models without relying on manually labeled data by using auxiliary tasks like generation, contrast, and reconstruction to learn expressive representation or create pseudo labels. 
%
In the realm of time series, this approach is discussed more in Time Series Classification (TSC) \citep{sslclassification1, sslclassification2, ssl_con_gen}. 
Recent works \citep{millet, timemil} present a novel perspective that instances in time series/segments can have multiple labels. 
They propose corresponding weakly-supervised-learning methods that significantly improve both performance and interpretability. 

As manual labeling is usually not required, TSF is treated as a generation task by self-supervised methods.
Recent works focus on the use of TSF as an auxiliary task to learn universal representations that improve performances of other tasks \citep{patchtst, selfdiff, timesurl}. This paradigm shows the potential to scale time series models to levels comparable to large language models. 

In this work, we integrate \emph{both perspectives} by employing an auxiliary reconstruction task, commonly used in TSC, to enhance the performance of TSF. 
% \textcolor{blue}{The pseudo labels which is often discussed in TSC, differs from ours in that they are generated from existing manual labels whereas we generate pseudo labels from a self-supervised reconstruction task.} 
The pseudo labels, often discussed in TSC, are derived from existing, manual ones, while ours are created in a self-supervised paradigm.

\section{Conclusion and Future Work}

% \textcolor{blue}{This paper proposes a novel angle on enhancing TSF models. Derived from reconstructions on datasets,  \model{}  helps predictors generalize better by selectively replace raw labels with pseudo labels. Further, the effectiveness of \snr{} for linear layers in TSF models leads to the exploration of the overfitting problem in TSF. In future work, we plan to  explore the potential of \model{} in component analysis of time series, such as finding the outliers and errors.}

This paper presents a self-supervised approach \model{} that enhances TSF models by selectively replacing overfitted components with pseudo labels derived from intermediate reconstructions.
Combined with Spectral Norm Regularization applied to the linear layers of the backbone, \model{} improves generalization and robustness across TSF models. Future work will explore extending \model{} to tasks such as time series outlier detection and error correction.

% In future work, we plan to investigate in extending the method to larger scales and to tasks where a mixture of datasets is included. 
\clearpage

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.

\bibliography{main}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Grid Search Algorithm in Initial Case}
\label{sec:gird_search_app}

In Section~\ref{ssec:initial_case}, we have introduced the grid search process to illustrate how the reconstruction network $g(\cdot; \phi)$ evolves to approximate the raw target series $Y$ by minimizing the reconstruction loss $\ell_{rec} = \Vert g(y; \phi) - y \Vert$.
The following Algorithm~\ref{alg:grid_search} provides a detailed implementation of this process, where the reconstruction network $g(\cdot; \phi)$ and the predictor model $f(\cdot; \theta)$ are jointly optimized to minimize both reconstruction and prediction losses.
%
Specifically, for each candidate reconstruction parameter $\phi_i$ (line~1), the predictor's parameters $\theta$ and its optimizer are initialized (line~2). In the inner loop (line~3), the predictor is optimized by first reconstructing $\tilde{y}_i$ using $g(\cdot; \phi_i)$ (line~6), then calculating the prediction loss $\ell_{pred}$  (line~7) and reconstruction loss $\ell_{rec}$ (line~8). 
The prediction loss $\ell_{pred}$ is backpropagated to update $\theta$ (line~8), and this process repeats until the gradient $\nabla_{\theta}$ falls below a threshold $\alpha$ or the maximum steps $J$ are reached (line~3). 
After optimizing $\theta$, the reconstruction loss $\ell_{rec}$ is backpropagated to update $\phi$ (line~11), and this process is repeated for all $N$ candidates to find the best $\phi$ (line~1).

\begin{algorithm}
\caption{Grid Search along $\ell_{rec}$}
\label{alg:grid_search}
\SetKwInOut{Parameters}{Parameters}
\SetKwInOut{Optimizers}{Optimizers}
\SetKwInOut{Initialize}{Initialize}
\SetKwComment{Comment}{$\triangleright$\ }{}


\Parameters{
$\phi$ w.r.t reconstruction network $g(\cdot; \phi)$; \\
$\theta$ w.r.t predictor model $f(\cdot; \theta)$
}

\Optimizers{
$Opt_{\phi}$ w.r.t $\phi$ (outer loop); \\
$Opt_{\theta}$ w.r.t $\theta$ (inner loop)
}

\Initialize{$\phi$, $Opt_{\phi}$}

\For{$i \gets 0$ \textbf{to} $N$} 
{ 
\Comment{\textcolor{teal}{$N$ for number of candidates}}

    Initialize $\theta$, $Opt_{\theta}$; $j \gets 0$\;
    
    \While{$\nabla_{\theta} > \alpha$ \textbf{and} $j \leq J$} 
    {  
    \Comment{\textcolor{teal}{$J$ for maximum steps to optimize a predictor}}
        $\ell_{rec} \gets 0$\;
        
        \ForEach{$(x, y) \in (X, Y)$}{
            $\tilde{y}_i \gets g(y; \phi_i)$\;
            
            $\ell_{pred} \gets \Vert f(x; \theta) - \tilde{y}_i \Vert$\;
            
            $\ell_{rec} \gets \ell_{rec} + \Vert \tilde{y}_i - y \Vert$\;
            
            Backpropagate $\ell_{pred}$ and update $\theta$ using $Opt_{\theta}$\;
        }
        
        $j \gets j + 1$\;
    }
    
    Backpropagate $\ell_{rec}$ and update $\phi$ using $Opt_{\phi}$\;
}
\end{algorithm}

\section{Further Discussion on Gradient Constraint}
\label{sec:discussion_gradient}
Recall that in Section~\ref{ssec:co-objective}, we have proposed the co-training objective as:
% \begin{alignat*}{4}
% &\underset{\theta, \phi}{\text{minimize}}
%  \quad &&\mathcal{L} = \Vert \tilde{y} - y\Vert + \Vert  \tilde{y} - \hat{y}\Vert \\
% & &&\ \tilde{y} = g(y;\phi)\\
%  & &&\ \hat{y} = f(x;\theta)\\
% &\text{s.t.}  & \Vert\nabla_{\theta,\phi}&\tilde{y}\Vert \le \delta, \quad \forall \tilde{y}_i\in \tilde{Y}
% \end{alignat*} 
\begin{alignat*}{3}
&\underset{\theta, \phi}{\text{minimize}} \quad && \mathcal{L} = \Vert \tilde{y} - y \Vert + \Vert \tilde{y} - \hat{y} \Vert \\
&\text{subject to} \quad && \tilde{y} = g(y; \phi), \quad \hat{y} = f(x; \theta), \\
& && \Vert \nabla_{\theta, \phi} \tilde{y}_i \Vert \leq \delta, \quad \forall \tilde{y}_i \in \tilde{Y}.
\end{alignat*} 

With constraints on the gradient of $\tilde{y}$, the co-training update allows for a more stable optimization compared to the grid search using a two-step optimization. 
When viewing each intermediate $\phi_i$ as an individual candidate dataset, applying cautious updates to $\tilde{y}_i = g(y; \phi_i)$ introduces additional candidate datasets along the optimization trajectory, enriching the search process.

However, since $\nabla_{\theta, \phi} \tilde{y}_i$ is not practically computable for each $\tilde{y}_i$ within a single step of optimization, we turn to look for a surrogate term to replace this constraint. 

Note that
% \begin{equation}
% \begin{aligned}
% \nabla_{\phi, \theta} \tilde{y}_i &= 
% \begin{bmatrix}
%     \nabla_{\phi} g & \nabla_{\theta} g \\
% \end{bmatrix}  \\
% &= \begin{bmatrix}
%     \nabla_{\phi}g & \nabla_{\phi}g \frac{\nabla_{\theta}\mathcal{L}}{\nabla_{\phi}\mathcal{L}} \\
% \end{bmatrix}\\
% &= \begin{bmatrix}
%     \nabla_{\phi}g & \nabla_{\phi}g \frac{\nabla_{\theta}\left[\left(\tilde{y} - y\right)^2 + \left(\tilde{y} - \hat{y}\right)^2\right]}{\nabla_{\phi}\left[\left(\tilde{y} - y\right)^2 + \left(\tilde{y} - \hat{y}\right)^2\right]} \\
% \end{bmatrix}  \\
% &= \begin{bmatrix}
%     \nabla_{\phi}g & \nabla_{\phi}g \frac{2\left(\hat{y} - \tilde{y}\right)\nabla_\theta f}{2\left(\tilde{y} - y\right)\nabla_{\phi}g + 2\left(\tilde{y} - \hat{y}\right)\nabla_{\phi}g} \\
% \end{bmatrix}  \\
% &= \begin{bmatrix}
%     \nabla_{\phi}g & \frac{\left(\hat{y} - \tilde{y}\right)\nabla_\theta f}{\left(\tilde{y} - y\right)+ \left(\tilde{y} - \hat{y}\right)} \\
% \end{bmatrix}  \\
% & \approx \begin{bmatrix}
%     \nabla_{\phi}g & -\nabla_{\theta}f \\
% \end{bmatrix}  &\text{\color{teal}Because $\vert\tilde{y}-y\vert \ll \vert\tilde{y} - \hat{y}\vert$}\\
% \end{aligned}.
% \label{eq:grad}
% \end{equation}
%
\begin{equation}
\begin{aligned}
\nabla_{\phi, \theta} \tilde{y}_i &= 
\begin{bmatrix}
    \nabla_{\phi} g & \nabla_{\theta} g
\end{bmatrix}  \\[8pt]
&= 
\begin{bmatrix}
    \nabla_{\phi} g & \nabla_{\phi} g \frac{\nabla_{\theta}\mathcal{L}}{\nabla_{\phi}\mathcal{L}}
\end{bmatrix} \\[8pt]
&= 
\begin{bmatrix}
    \nabla_{\phi} g & \nabla_{\phi} g 
    \frac{\nabla_{\theta} \left[ \left(\tilde{y} - y\right)^2 + \left(\tilde{y} - \hat{y}\right)^2 \right]}{\nabla_{\phi} \left[ \left(\tilde{y} - y\right)^2 + \left(\tilde{y} - \hat{y}\right)^2 \right]}
\end{bmatrix} \\[8pt]
&= 
\begin{bmatrix}
    \nabla_{\phi} g & \nabla_{\phi} g 
    \frac{2 \left(\hat{y} - \tilde{y}\right) \nabla_\theta f}{2 \left(\tilde{y} - y\right) \nabla_{\phi} g + 2 \left(\tilde{y} - \hat{y}\right) \nabla_{\phi} g}
\end{bmatrix} \\[8pt]
&= 
\begin{bmatrix}
    \nabla_{\phi} g & \frac{\left(\hat{y} - \tilde{y}\right) \nabla_\theta f}{\left(\tilde{y} - y\right) + \left(\tilde{y} - \hat{y}\right)}
\end{bmatrix} \\[8pt]
&\approx 
\begin{bmatrix}
    \nabla_{\phi} g & -\nabla_{\theta} f
\end{bmatrix}, 
\quad \text{\textcolor{teal}{because $\vert \tilde{y} - y \vert \ll \vert \tilde{y} - \hat{y} \vert$}.}
\end{aligned}
\label{eq:grad}
\end{equation}

Reconstruction is inherently a simpler task compared to forecasting, which allows the last approximation in Eq.\ \ref{eq:grad} to hold after just a few steps of initial optimization. 
When $\tilde{y}$ is far distinct from original labels $y$, the reconstructed series $\tilde{y}$ becomes nearly unpredictable, leading to instability in the optimization process. 
Therefore, adding a constraint on $g$ can interfere with the convergence of the predictor model. 
To address this, Eq.\ \ref{eq:grad} offers an optional assurance of gradient constraint, which uses $\Vert\nabla_{\theta}f\Vert\le\delta$ as a surrogate for maintaining stability during optimization. 

The constrained form of the optimization is equivalent to the penalized form using \emph{Lagrangian Duality}. Eq.\ \ref{eq:constrained_optim} can be rewritten as: 
% \begin{equation}
% \label{eq:penalty_optim}
% \begin{aligned}
%     \underset{\theta, \phi}{\text{minimize}} \quad\mathcal{L} &= \Vert g(y;\phi) - y\Vert + \Vert g(y;\phi) - f(x;\theta)\Vert \\ &+ \beta\Vert \nabla_{\theta}f(x;\theta)\Vert 
% \end{aligned}
% \end{equation}
%
\begin{equation}
\label{eq:penalty_optim}
\begin{aligned}
    \underset{\theta, \phi}{\text{minimize}} \quad \mathcal{L} &= \Vert g(y; \phi) - y \Vert 
    + \Vert g(y; \phi) - f(x; \theta) \Vert \\ 
    &\quad + \beta \Vert \nabla_{\theta} f(x; \theta) \Vert,
\end{aligned}
\end{equation}

For readers who are familiar with Reinforcement Learning, this derivation resembles the transfer from a constrained optimization to a penalized one (e.g., from TRPO \citep{trpo} to PPO \citep{ppo}). 
In brief, while the penalized form is theoretically equivalent to the constrained form, it is challenging to choose a fixed $\beta$ that works universally across all datasets or even within a single dataset (because intrinsic characteristics can vary over time). Thus, a more general form of constraint is required to better serve the penalty, similar to the concept of gradient clipping in PPO.

Note that $\nabla_{\theta}f\le\delta$ implies the \emph{Lipchitz condition} for an arbitrary function $f$. This means 
\begin{equation}
\Vert f(x_1;\theta) - f(x_2;\theta)\Vert \le C(\theta)\Vert x_1 - x_2\Vert,
\end{equation}
where $C(\theta)$ is a constant with respect to the parameter $\theta$. 
When considering a typical Fully Connected Layer defined as $f(x;W,b) = \sigma(Wx+b)$, the condition becomes
\begin{equation}
\Vert\sigma'_1W(x1-x2)\Vert \le C(W,b)\Vert x_1 - x_2\Vert,
\end{equation}
When the gradient of activation function $\sigma$ has an upper bound (as is often the case for common activation functions like ReLU, Sigmoid, etc.), the Lipchitz condition holds as long as
\begin{equation}
\Vert W(x_1 - x_2)\Vert\le C(W,b)\Vert x_1 - x_2\Vert.
\end{equation}

We expect the constant $C$ to be relatively small so that the penalty works. 
In fact, $C$ here corresponds to the spectral norm of the matrix $W$, which is defined as
\begin{equation}
    \Vert W\Vert_2 = \max\limits_{x\neq 0} \frac{\Vert Wx\Vert}{\Vert x\Vert}.
\end{equation}
By applying the Spectral Norm Regularization (\snr{}) in Eq.\ \ref{eq:snr}, we can ensure the constant $C$ equals to exactly 1.

However, in practice, \snr{} have limitations when applied to the parameter matrix in self-attention mechanisms. 
This phenomenon is termed \emph{entropy catastrophe} as discussed by \citep{samformer}. 
In this paper, by analyzing the sharpness of different components in the predictor model, we propose to use pre-\snr{} and post-\snr{} combined, which specifically normalizes the first and last linear layer in the TSF models (see Section~\ref{ssec:snr}).

\section{Implementation Details of $g(\cdot;\phi)$}
\label{sec:implement_reconstruct_net}

As introduced in Section~\ref{ssec:arch_g}, we propose a simple enough reconstruction network $g(\cdot;\phi)$ that serves our objective. Despite its simplicity, the architecture incorporates some special designs that enhance its performance. Specifically, these include the \emph{conv-concat layer} and the \emph{point-wise FFN}, which are detailed in Appendix~\ref{ssec:convolution} and Appendix~\ref{ssec:ffn}, respectively.

\subsection{Conv-concat Layer}
\label{ssec:convolution}

\noindent\textbf{Transpose and Unfold}.
The implementation details of the conv-concat layer involve two key operations that are designed for the following two benefits:
\begin{enumerate}[leftmargin=*]
\item The convolution outputs can be concatenated into embeddings of the same length, enabling features from different frequencies to be ideally fused into one embedding.

\item The features are evenly arranged along the temporal dimension, ensuring that each embedding in the sequence has the same large Receptive Field.
\end{enumerate}

To achieve these benefits, we introduce a two-step operation: \textbf{Transpose} and \textbf{Unfold}, which work together to ensure both uniform embedding structure and large Receptive Fields.

Specifically, we set $\text{kernel size} = 3$, $\text{stride} = 2$, $\text{padding} = 1$, and the number of kernels doubled for each subsequent layer. 
Using this setup, as illustrated in Figure~\ref{fig:convolution_embedding}, we ensure that the number of features remains invariant across different layers, with only the shape of the features changing.
Now we can fuse the outputs from different convolution layers together by flattening/unfolding the features to the original shape of ($L \times 1$). Again, considering the effectiveness of point-wise FFN presented in Appendix~\ref{ssec:ffn}, we expect the concatenated features to be near-equally arranged along the temporal dimension to preserve the sequential relationships in the embedding.
To achieve this, we first transpose the features and then unfold them. This practice can ensure a Receptive Field of $(2 ^ {l + 1} - 1)$ wide for each embedding, where $l$ is the total number of convolution layers.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/convolution_embedding.pdf}
    \caption{The illustration of transpose and unfold operation in the Convolution Encoder.}
    \label{fig:convolution_embedding}
\end{figure}

\smallskip
\noindent\textbf{Effective Receptive Field}.
As what was proposed by \citep{moderntcn}, the Effective Receptive Field (ERF) is a reasonable consideration for designing convolution-based architectures. 
To evaluate the ERF of our conv-concat layer, we input an impulse function and visualize the resulting ERF, as shown in Figure~\ref{fig:ERF}.
The visualization demonstrates that, without requiring an extra-large convolution kernel, our proposed method achieves a near-global ERF. This is made possible by combining the outputs from different layers, each capturing distinct frequency patterns.
\begin{figure}[!htbp] 
\begin{subfigure}{0.5 \linewidth}
  \includegraphics[width=1\linewidth]{figures/convolution/0.pdf}
  \caption*{(a) Layer 1}
\end{subfigure}
\begin{subfigure}{0.5 \linewidth}
  \includegraphics[width=1\linewidth]{figures/convolution/1.pdf}
  \caption*{(b) Layer 2}
\end{subfigure}
\vfill
\begin{subfigure}{0.5 \linewidth}
  \includegraphics[width=1\linewidth]{figures/convolution/4.pdf}
   \caption*{(e) Layer 3}
\end{subfigure}
\begin{subfigure}{0.5 \linewidth}
  \includegraphics[width=1\linewidth]{figures/convolution/5.pdf}
   \caption*{(f) Layer 4}
\end{subfigure}
\caption{Effective Receptive Field (ERF) of the proposed conv-concat layer. By the transpose and unfold operation, the ERF of each convolution layer covers the entire input series with different frequencies.}
\label{fig:ERF}
\end{figure}

\subsection{Point-wise FFN}
\label{ssec:ffn}

We employ a point-wise FFN as a parameter-sharing module to decode the outputs from the convolution layer. 
The FFN is essentially a two-layer MLP that resembles the common design of a linear projector in predictor models, as mentioned in Section~\ref{ssec:snr_results}.

To further illustrate, we present the three different parameterizations of the linear projector commonly used in TSF models in Figure~\ref{fig:linearhead}:
\begin{itemize}[leftmargin=*]
\item \textbf{Patch-dependent Design}: \patchtst{} \citep{patchtst} adopts a patch-dependent linear projector (shown in Figure~\ref{fig:linearhead}(a)) that first flattens all features within patches and utilizes an extra-large weight matrix of shape $Ld \times d$, where $L$ is the sequence length and $d$ is the dimension of the latent embedding.

\item \textbf{Patch-independent Design}: \citep{patchindependent} proposes that the necessity for patch-dependent designs depends on the specific task. For instance, tasks like forecasting may require patch-dependent projectors, while tasks like contrastive learning might favor a patch-independent design (shown in Figure~\ref{fig:linearhead}(b)).

\item \textbf{Point-wise Design}: Our reconstruction process does not require exploiting the patch correlations as a necessity and can even extend this independence to point-wise scope (shown in Figure~\ref{fig:linearhead}(c)). 
This approach is feasible only when each point-wise embedding is sufficiently rich in information, a property achieved through our convolution layer, which provides a near-global ERF for each point.
\end{itemize}

\begin{figure}[!htbp]
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/point-wise/patchd.pdf}
        \caption*{\small{(a) patch-dependent linear projector}}
    \end{subfigure}
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/point-wise/patchi.pdf}
        \caption*{\small{(b) patch-independent linear projector}}
    \end{subfigure}
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/point-wise/pointwise.pdf}
        \caption*{\small{(c) point-wise linear projector}}
    \end{subfigure}
    \caption{Three types of linear projectors.}
    \label{fig:linearhead}
\end{figure}

The sharing of parameters in our linear projector allows for an increase in parallel modules. In practice, we generate multiple reconstructed samples for the same raw input sample and feed them to the predictor simultaneously. This approach inherently expands the scale of the dataset, further enhancing training efficiency.


\section{Further Analysis}
\label{sec:futher_analysis}

\subsection{Distribution of Reconstructed Datasets in Grid Search}

The distribution of sampled predictions is shown in Figure~\ref{fig:distribution}.
The predictions are evaluated using three loss metrics: (1) \textbf{Prediction loss}: $\ell_{pred} = \Vert\hat{y} - \tilde{y}\Vert$, (2) \textbf{Reconstruction loss}: $\ell_{rec} = \Vert\tilde{y} - y\Vert$, and (3) \textbf{Target loss}: $\ell_{target} = \Vert \hat{y} - y\Vert$.
In Figure~\ref{fig:distribution}, scatter points illustrate the relationships among these losses across various candidate datasets generated by $g(\cdot; \phi)$ on the ETTh1 dataset. Each scatter point is colored according to the mean reconstruction loss ($\ell_{rec}$) of its corresponding dataset. Lighter colors (e.g., yellow) represent datasets with higher $\ell_{rec}$, while darker colors (e.g., purple) correspond to datasets with lower $\ell_{rec}$.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/initial_case/scatter_with_projections.pdf}
    \caption{Distribution of losses of candidate datasets generated by $g(\cdot; \phi)$ on ETTh1.}
     \label{fig:distribution}
\end{figure*}

The distribution is visualized across three projections:
\begin{enumerate}[leftmargin=*]
    \item \emph{$\ell_{rec}$-$\ell_{pred}$ Plane} illustrates the relationship between the reconstruction loss $\ell_{rec}$ and the prediction loss $\ell_{pred}$, both of which are actively optimized during training. As $\ell_{rec}$ decreases (darker colors), the points in the distribution become more condensed, indicating reduced flexibility in the candidate datasets. This trend suggests that datasets with very low reconstruction loss may lack the diversity needed for optimal predictor performance.
    
    \item \emph{$\ell_{rec}$-$\ell_{target}$ Plane} highlights the relationship between the reconstruction loss $\ell_{rec}$ and the target loss $\ell_{target}$, where $\ell_{target}$ serves as the primary evaluation metric for predictor performance. Interestingly, datasets closer to the raw data (darker colors, with lower $\ell_{rec}$) do not consistently lead to better $\ell_{target}$ values. This observation, which indicates that overly strict reconstruction constraints may hinder prediction quality, is further supported by Figure~\ref{fig:observation}.
    
    \item \emph{$\ell_{pred}$-$\ell_{target}$ Plane} directly examines prediction performance, as $\hat{y}$ is involved in calculating both $\ell_{pred}$ and $\ell_{target}$. 
    Notably, candidate datasets with intermediate distances from the raw data (green points) demonstrate better generalization.
    These datasets are characterized by relatively higher $\ell_{pred}$ and lower $\ell_{target}$ and are distributed more toward the bottom-right region of the plane, reflecting improved prediction quality.
\end{enumerate}

These results suggest that datasets with moderate reconstruction loss --- neither too high nor too low --- strike a better balance between flexibility and generalization. This balance ultimately leads to improved predictor performance, as overly strict reconstruction constraints may limit model adaptability, while overly high reconstruction loss may fail to capture meaningful patterns.

\subsection{Demystifying the Self-supervision in \model{}}

\begin{figure*}[!htbp]
    \begin{subfigure}{1 \linewidth}
            \includegraphics[width=1\linewidth]{figures/appendix/channel_0.pdf}
            \caption{Visualization of $\tilde{y}$, $\tilde{y}'$ and $y$ during training on Channel 1 of ETTh1. Epochs displayed here are 1, 2, 3, 4, 10.}
    \end{subfigure}
    \vfill
    \begin{subfigure}{1 \linewidth}
            \includegraphics[width=1\linewidth]{figures/appendix/channel_3.pdf}
            \caption{Visualization of $\tilde{y}$ and $\tilde{y}'$ during training and $y$ on Channel 4 of ETTh1. Epochs displayed here are 1, 2, 3, 4, 10.}
    \end{subfigure}
    \caption{Visualization our difference components in $g(\cdot;\phi)$.}
    \label{fig:vis_components}
\end{figure*}

\noindent\textbf{Conv-concat Layers as Feature Amplifier}.
In theory, the reconstruction network can be extended to larger scales as it does not interfere with inference efficiency. 
However, in our implementation, it is kept as simple as possible to prioritize training efficiency.
Despite its simplicity, our observations reveal that the two different components of the reconstruction network function in distinct ways, which offers insights into designing more effective reconstruction networks.

As mentioned in Appendix~\ref{ssec:ffn}, the FFN in $g(\cdot; \phi)$ is designed point-wise, decoding the concatenated outputs of the convolution layers for each point independently. 
To further investigate the utility of conv-concat layers, we skip the initial linear layers and activation functions in $g(\cdot; \phi)$, directly applying the final linear transformation to the latent outputs of the conv-concat layers.
This process generates an intermediate series, which we term as undecided $\tilde{y}'$.

\begin{wrapfigure}{r}{0.35\linewidth}
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/appendix/dist_1_4.pdf}
        \caption{Distributions of Channels 1 and 4.}
    \end{subfigure}
    % \vfill
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/appendix/dist_2_5.pdf}
        \caption{Distributions of Channels 2 and 5.}
    \end{subfigure}
    % \vfill
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/appendix/dist_3_6.pdf}
        \caption{Distributions of Channels 3 and 6.}
    \end{subfigure}
    % \vfill
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/appendix/dist_4_7.pdf}
        \caption{Distributions of Channels 4 and 7.}
    \end{subfigure}
    \caption{Distribution alignment for channels in ETTh1 with data all normalized by RevIN.}
    \label{fig:dist_alignment}
    \vspace{-2em}
\end{wrapfigure}
Figure~\ref{fig:vis_components} shows that the $\tilde{y}'$ (red plots) effectively amplifies the sparse spiking signals present in the raw data. This behavior can be interpreted as the conv-concat layers acting as a \textbf{feature amplifier}, emphasizing and enlarging important patterns in the input data.

\smallskip
\noindent\textbf{Channel-wise Distribution Alignment}.
Distribution shift, a core challenge in long-term Time Series Forecasting (TSF), is decisive for a TSF model to generalize on future data after training. 
The most widely adopted approach to address this issue is \emph{Reversible Instance Normalization} (RevIn) \citep{revin, nonstationary}, which aligns the distributions of historical and future data.

While RevIn significantly improves the performance of TSF models, it falls short in aligning distributions \emph{across channels in the multivariate forecasting setting}. 
Figure~\ref{fig:dist_alignment} highlights this limitation: even when the raw data $y$ is normalized by RevIn, the distribution distances (measured by the KL divergence metric) remain significant, as shown in the leftmost plots of all subfigures.

In this view, the proposed reconstruction network $g(\cdot;\theta)$ effectively serves as a general \textbf{channel-wise distribution alignment} mechanism. 
Interestingly, when examining the intermediate undecided $\tilde{y}'$ across channels, we observe that, although the sparse features are amplified, the distribution distances are reduced compared to those in the raw data.
Overall, the final reconstructed series exhibits better alignment across channels. However, exceptions such as Channels 3 and 6 (see Figure~\ref{fig:dist_alignment}(c)) demonstrate that the alignment is not uniform across all channels.

For models designed with \emph{channel-independence} (CI), such as \patchtst{} \citep{patchtst} and \cyclenet{} \citep{cyclenet}, aligning distributions across channels is of critical importance, especially when trained on datasets with a large number of variates. 
%
The channel-wise alignment introduced by our self-supervised reconstruction task provides an effective solution to distribution shifts, thereby enhancing the performance of CI predictor models.

% When comparing the distributions measured by the normalized values of both channels, it becomes evident that even after preprocessing with RevIN~\citep{revin}, the distributions of both channels are not aligned (see Figure~\ref{fig:case_study}(c)). 
% We then apply our point-wise linear head to decode the output embeddings from the Convolution Encoder, resulting in the initially transformed data (in yellow). The distributions of both channels align (see Figure~\ref{fig:case_study}(d)), supporting the commonly adopted channel-independent strategy of sharing weights across channels.

\smallskip
\noindent\textbf{Evolution of Adaptive Mask in \model{}}.
In Section~\ref{ssec:MIL}, we have explored the benefits of the adaptive mask in \model{} from a Multiple Instance Learning (MIL) view, highlighting its sensitivity to instances with varying deviations and its ability to apply different masking strategies accordingly.

\begin{figure*}[!htbp]
    \begin{subfigure}{1 \linewidth}
            \includegraphics[width=\linewidth]{figures/appendix/mask_channel_0.pdf}
            \caption{Visualization masks during training on Channel 1 of ETTh1. Epochs displayed here are 1, 2, 3, 4, 10.}
    \end{subfigure}
    \vfill
    \begin{subfigure}{1 \linewidth}
            \includegraphics[width=\linewidth]{figures/appendix/mask_channel_3.pdf}
            \caption{Visualization masks during training on Channel 4 of ETTh1. Epochs displayed here are 1, 2, 3, 4, 10.}
    \end{subfigure}
    \caption{Evolution of the adaptive masks in \model{} during training.}
    \label{fig:evolution}
\end{figure*}

In this part, we further visualize the evolution process of the adaptive mask during training on real datasets, using ETTh1 as an example, as shown in Figure~\ref{fig:evolution}.

At the very first epoch, the model heavily relies on the prediction $\hat{y}$ to reconstruct the series $\tilde{y}$. 
This benefits the convergence of both the predictor model $f(\cdot; \theta)$ and the reconstruction network $g(\cdot; \phi)$.
%
As previously analyzed, the reconstructed series $\tilde{y}$ aligns the data distribution, enabling $f(\cdot; \theta)$ to learn effectively from $\tilde{y}$. 
Correspondingly, the prediction $\hat{y}$ in the first epoch is basically a draft with smooth curvature, preventing $\tilde{y}$ from quickly overfitting the noisy raw labels $y$.
In subsequent epochs, the proportion of gray masks applied to the loss term $\Vert\hat{y}, y\Vert$ increases. This indicates that the model progressively emphasizes a more precise fitting to the true labels, resulting in improved reconstruction and prediction quality.

In comparison with the case study in Section~\ref{ssec:MIL}, the adaptive mask tends to collapse more rapidly from intermediate states where all three masking strategies are simultaneously active. 
This is possibly due to the noisy and unpredictable nature of real-world datasets. 
We plan to explore less aggressive strategies compared to binary masking, which hopefully will increase the robustness of the current method. 

\section{Related Work Revisit}
\label{ssec:related_extension}

In this part, we will introduce some popularly adopted methods for TSF, in which our baseline models are included.


\smallskip
\noindent\textbf{Channel-independent Transformers}. 
Time Series Transformers (TST)~\citep{informer, autoformer, fedformer} have recently led to significant progress in the TSF problem, demonstrating convincing superiority over traditional methods and convolution-based models.
Inspired by \citep{dlinear}, \citep{patchtst} incorporates the \emph{Channel Independent} (CI) design (sharing weights across all channels) to introduce \patchtst{}, a new state-of-the-art (SOTA) model that significantly benefits from CI and the patching operation. Multiple works follow this practice and achieve excellent performance in TSF \citep{timesfm, unitime, mtst}.

\smallskip
\noindent\textbf{Channel-wise Transformers}.
Building on TST, more recent research has focused on designing Transformers capable of capturing channel dependencies inherent in multivariate time series data.
Notable examples include \textsc{Crossformer}~\citep{crossformer}, \iTransformer{} (\itrans{})~\citep{itrans}, \textsc{Dsformer}~\citep{dsformer}, and \textsc{card}~\citep{card}. 
% More recent work has shifted the angle towards designing Transformers that can capture channel dependencies, such as Crossformer~\citep{zhang2023crossformer}, \itrans{}~\citep{liu2023iTransformer}, \textsc{card}~\citep{wang2024card} and \textsc{Dsformer}~\citep{yu2023dsformer}. 
These models shed light on exploiting inter-series relationships to improve forecasting accuracy.
% These works shed light on understanding the nature of multivariate time series and how to exploit intra-series indicators.
Furthermore, having observed that Time Series Transformers are inherently unstable due to a sharp loss scape, \citep{samformer} propose a sharpness-aware optimizer to mitigate such issues. Their work focuses on an optimization-level approach involving a two-step backward. 
Nonetheless, channel-wise Transformers still suffer from overfitting on small datasets. By sharpness analysis, our proposed \model{} locates the overfitting issue and provides a solution from a data-level perspective.

\smallskip
\noindent\textbf{Linear or MLP-based Models}.
In contrast to the quadratic complexity of Transformers, lightweight linear or MLP-based models have emerged as competitive alternatives offering simplicity and efficiency.
\textsc{RLinear} and \textsc{RMlp}~\citep{rlinear} verify that a vanilla linear model or a 2-layer MLP, when combined with a widely-adopted normalization method~\citep{revin}, can achieve near SOTA performance in TSF.
Further research~\citep{lift} on channel dependencies within linear and MLP-based models has yielded performance improvements over previous CI approaches.
% Studies on channel-dependencies for linear/MLP-based models also have achieved performance boost on previous Channel-Independent ones \citep{zhao2024rethinking}.
Moreover, the models~\citep{fits, frequency} that directly learn linear regression or MLP-based models on complex frequency features have achieved remarkable performances. 
% Through theoretical analysis of these linear models, \citep{analysis} conclude that normalization-enhanced \citep{rlinear}, decomposition-based \citep{dlinear} and frequency-domain linear \citep{fits} models are essentially equivalent to vanilla linear regression.
%except for one special non-linear module --- the Low Pass Filter in \textsc{fits}~\citep{xu2023fits}. 
Most recently, \textsc{SparseTSF}~\citep{sparsetsf}, a highly lightweight model, incorporates 1D convolutions as down-sampling modules and learns linear parameters on the down-sampled values of the original series. \cyclenet{}~\cite{cyclenet}, a SOTA model that explicitly captures periodic trend features to enhance vanilla linear or MLP-based models to be on par with Transformer-based models.

\section{Experiment Details}
\label{sec:experimental_details}

\subsection{Datasets}

We conduct experiments on 11 real-world datasets to evaluate the performance of the proposed \model{}. The datasets are detailed below.
\begin{itemize}[leftmargin=*]
\item \textbf{ETT}~\citep{informer}: This dataset contains 7 factors of electricity transformers, recorded between July 2016 and July 2018. 
The subsets ETTh1 and ETTh2 are recorded hourly, while ETTm1 and ETTm2 are recorded every 15 minutes.

\item \textbf{Electricity}~\citep{autoformer}: This dataset records the hourly electricity consumption of 321 clients.

\item \textbf{Traffic}~\citep{autoformer}: This dataset collects hourly road occupancy rates measured by 862 sensors across the San Francisco Bay Area freeways, spanning from January 2015 to December 2016.

\item \textbf{Weather}~\citep{autoformer}: This dataset includes 21 meteorological factors, recorded every 10 minutes at the Weather Station of the Max Planck Biogeochemistry Institute in 2020.

\item \textbf{PeMS}: This dataset contains public traffic network data from California, collected at 5-minute intervals. We use the same four subsets (PeMS03, PeMS04, PeMS07, PeMS08) as adopted in \iTransformer{} (\itrans{})~\citep{itrans}.

\end{itemize}

For the ETT datasets, we divide them by ratio $\{0.6, 0.2, 0.2\}$ into train set, validation set, and test set.
For Electricity, Traffic, and Weather, we follow the same split ratio of $\{0.7, 0.1, 0.2\}$ as in \textsc{TimesNet}~\citep{timesnet, informer, autoformer}.
For the PeMS datasets, we split them using the ratio $\{0.6, 0.2, 0.2\}$ following the same setting as \itrans{}~\citep{itrans}. All datasets are scaled using the mean and variance of their respective training sets, a standard practice in TSF~\citep{timesnet}.
The statistics of all used datasets are listed in Table~\ref{tab:data}.

\begin{table}[!htbp]
\centering
\caption{Statistics of evaluation datasets.}
\resizebox{1\textwidth}{!}{
\begin{tabular}{c|ccccccccccc}
\toprule  
Datasets  & ETTh1 & ETTh2 & ETTm1 & ETTm2 & Electricity & Traffic & Weather &PeMS03 &PeMS04 &PeMS07 &PeMS08 \\
\midrule  
$\#$ of TS Variates & 7 & 7 & 7 & 7 & 321 & 862 & 21 &358 &307 & 883 &170\\
TS Length & 17420 & 17420 & 69680 & 69680 & 26304 & 17544 & 52696 &26209 &16992 &28224 &17856\\
\bottomrule 
\end{tabular}}
\label{tab:data}
\end{table}

\subsection{Backbones} 

We evaluate the proposed \model{} against the following baseline backbone models:

\begin{enumerate}[leftmargin=*]
\item \mlp{}~\citep{rlinear}, a 2-layer MLP model combined with Reversible Instance Normalization \citep{revin}.

\item \cyclenet{}~\citep{cyclenet}, a 2-layer MLP equipped with efficient cycle modeling that belongs to a general seasonal-trend decomposition method (Cycle/MLP in the original paper).

\item \itrans{}~\citep{itrans}, a Transformer-based model that computes attention scores on the inverted series along the channel dimension.

\item \patchtst{}~\citep{patchtst}, a Channel-Independent Transformer-based model that uses patching to tokenize the input. 
\end{enumerate}

It should be pointed out that the scale of a time series dataset is determined by a combination of the number of variates and the length of the dataset. 
Therefore, a fair comparison should take both factors into account.
In our experiments, we observe that some baseline models, such as \itrans{}, benefit greatly from datasets with a larger number of variates, while others, like \patchtst{}, tend to perform better on longer datasets.

\subsection{Reproductivity}

\noindent\textbf{Hyperparameters and Settings}.
All experiments and methods are implemented in Python and PyTorch \citep{pytorch} and conducted on two Nvidia RTX A5000 Ada generation GPUs (32GB VRAMs) and two Nvidia RTX A6000 GPUs (48GB VRAMs). 
We use the ADAM optimizer~\citep{adam} with a learning rate initialized as $\eta = 0.001$ for all settings. 
Unlike prior works~\citep{itrans, sparsetsf} that set a fixed, small number of training epochs, we adopt an early stopping strategy based on the MSE metric of the validation set, with a patience of 20 epochs.

For the reconstruction network $g(\cdot; \phi)$, the hyperparameters are detailed in Table~\ref{tab:hyperparameters}.

\begin{table}
\centering
\caption{Hyperparameters of the reconstruction network $g(\cdot;\phi)$.}
\label{tab:hyperparameters}
\begin{tabular}{c|c}
     \toprule
     \# of convolution layers & 4 \\
     dim\_multiplier & 4 \\
     hidden\_dim & 128 \\
     \# of series & 8 \\
     \bottomrule
\end{tabular}
\end{table}

We use 4 convolution layers in total, with consistent settings for kernel size, stride, and padding size as explained in Appendix~\ref{ssec:convolution}.
Dim\_multiplier represents the expansion ratio of convolution channels. The channels for the four convolution layers are set to 1, 2, 4, and 8, respectively, and are further multiplied by 4 to increase the model's capacity.
Hidden\_dim is the dimension size in the FFN, and \# of series indicates that we use 8 point-wise linear projectors in parallel as explained in Appendix~\ref{ssec:ffn} and Figure~\ref{fig:linearhead}. 
The hyperparameter settings provided in Table~\ref{tab:hyperparameters} are applied consistently across all datasets.

\smallskip
\noindent\textbf{Rerun Baselines as Additional Comparisons}.
To ensure fair comparisons with baseline results, we include both the results reported in the original papers and the results from our own re-implementations. Specifically: The \mlp{} and \cyclenet{} results are taken from \cyclenet{} paper \citep{cyclenet}. 
The \patchtst{} and \itrans{} results are from the \itrans{} paper \citep{itrans}, as the \patchtst{} paper~\citep{patchtst} does not provide results with a look-back length of 96.

While the results from the original papers and our reruns show no major discrepancies, minor differences do exist.
To provide complete transparency, we present both in Table~\ref{tab:from_paper} and Table~\ref{tab:our_runs} in Appendix~\ref{ssec:exp_main}.
Results from the original papers are more reliable as they reflect the authors' intended implementations, while our reruns ensure consistency in training settings for direct comparison.

\smallskip
\noindent\textbf{The Result Discrepancy on PeMS Datasets}.
We also evaluate the larger traffic dataset, PeMS, which has been previously examined in both \itrans{} and \cyclenet{}. 
However, we observe major discrepancies between our results and those reported in the original papers~\citep{itrans, cyclenet}.

While the original papers state that prediction lengths of $\{12, 24, 48, 96 \}$ were used, their reported results closely align with what we obtain using prediction lengths of $\{12, 24, 36, 48 \}$. 
The issue of reproduction inconsistencies is also widely discussed in the \iTransformer{} GitHub issues. For reference, we provide the results obtained using our settings, which we hope will aid in clarifying these discrepancies.

\section{More Experiment Results}
\label{ssec:exp_pems}

% In this section, we provide more detailed results of previously mentioned experiments. Table~\ref{tab:from_paper} and Table~\ref{tab:our_runs} are main experiments on ETT, Electricity, Traffic and Weather datasets, where baseline results in Table~\ref{tab:from_paper} are copied from the original papers and baseline results in Table~\ref{tab:our_runs} are run ourselves. Table~\ref{tab:pems} are experiments on PeMS datasets with baseline results of our runs. Table~\ref{tab:abl_cyclenet_full} and Table~\ref{tab:abl_itrans_full} are full results of the ablation study respectively. 

In this section, we present detailed results from the previously mentioned experiments:

\begin{itemize}
    \item \textbf{Tables~\ref{tab:from_paper} and~\ref{tab:our_runs}}:
    \begin{itemize}
        \item Summarize the main experiments on the ETT, Electricity, Traffic, and Weather datasets, with detailed breakdowns of different prediction lengths provided.
        \item Baseline results in \textbf{Table~\ref{tab:from_paper}} are taken from the original papers.
        \item Baseline results in \textbf{Table~\ref{tab:our_runs}} are reproduced by us.
    \end{itemize}

    \item \textbf{Table~\ref{tab:pems}}:
    \begin{itemize}
        \item Reports experiments on the PeMS datasets, including baseline results from our runs.
    \end{itemize}

    \item \textbf{Tables~\ref{tab:abl_cyclenet_full} and~\ref{tab:abl_itrans_full}}:
    \begin{itemize}
        \item Provide the complete results of the ablation studies.
    \end{itemize}
\end{itemize}

In Table~\ref{tab:pems}, ~\ref{tab:from_paper} and ~\ref{tab:our_runs}, color \color[HTML]{FF0000}{RED} \color{black}indicates better performance and color \color[HTML]{4472C4}{BLUE} \color{black} indicates worse performance. 
\linespread{1.2}
\begin{table*}[!htbp]

\caption{Full results of experiments on PeMS datasets, comparing backbone models and their integration with our proposal. All baseline results are reproduced by us.}
\resizebox{\textwidth}{!}{
\begin{tabular}{cc|c|cc|cc||cc|cc||cc|cc||cc|ccc}
\cline{2-19}
&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{\mlp{}} & \multicolumn{2}{c||}{+\textbf{Ours}}& \multicolumn{2}{c|}{\cyclenet{}}& \multicolumn{2}{c||}{+\textbf{Ours}} & \multicolumn{2}{c|}{\patchtst}& \multicolumn{2}{c||}{+\textbf{Ours}} & \multicolumn{2}{c|}{\itrans{}}& \multicolumn{2}{c}{+\textbf{Ours}} \\
\cline{2-19}
&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
\cline{2-19}  
&\multirow{5}*{\rotatebox{90}{PeMS03}}                & 12  & 0.083 & 0.191 & {\color[HTML]{FF0000} \textbf{0.082}} & {\color[HTML]{FF0000} \textbf{0.189}} & 0.073 & 0.179 & {\color[HTML]{FF0000} \textbf{0.072}} & {\color[HTML]{FF0000} \textbf{0.178}} & 0.078 & 0.186 & {\color[HTML]{FF0000} \textbf{0.072}} & {\color[HTML]{FF0000} \textbf{0.176}} & 0.075 & 0.186 & {\color[HTML]{FF0000} \textbf{0.074}} & {\color[HTML]{FF0000} \textbf{0.180}} \\
&\multicolumn{1}{c|}{}                                & 24  & 0.138 & 0.246 & {\color[HTML]{FF0000} \textbf{0.132}} & {\color[HTML]{FF0000} \textbf{0.240}} & 0.108 & 0.218 & {\color[HTML]{FF0000} \textbf{0.106}} & {\color[HTML]{FF0000} \textbf{0.216}} & 0.123 & 0.234 & {\color[HTML]{FF0000} \textbf{0.104}} & {\color[HTML]{FF0000} \textbf{0.212}} & 0.097 & 0.207 & {\color[HTML]{FF0000} \textbf{0.089}} & {\color[HTML]{FF0000} \textbf{0.198}} \\
&\multicolumn{1}{c|}{}                                & 36  & 0.196 & 0.297 & {\color[HTML]{FF0000} \textbf{0.189}} & {\color[HTML]{FF0000} \textbf{0.291}} & 0.147 & 0.256 & {\color[HTML]{FF0000} \textbf{0.144}} & {\color[HTML]{FF0000} \textbf{0.255}} & 0.172 & 0.276 & {\color[HTML]{FF0000} \textbf{0.136}} & {\color[HTML]{FF0000} \textbf{0.244}} & 0.127 & 0.237 & {\color[HTML]{4472C4} \textbf{0.130}} & {\color[HTML]{FF0000} \textbf{0.242}} \\
&\multicolumn{1}{c|}{}                                & 48  & 0.257 & 0.344 & {\color[HTML]{FF0000} \textbf{0.198}} & {\color[HTML]{FF0000} \textbf{0.304}} & 0.182 & 0.288 & {\color[HTML]{FF0000} \textbf{0.178}} & {\color[HTML]{4472C4} \textbf{0.291}} & 0.221 & 0.317 & {\color[HTML]{FF0000} \textbf{0.160}} & {\color[HTML]{FF0000} \textbf{0.264}} & 0.166 & 0.273 & {\color[HTML]{4472C4} \textbf{0.174}} & {\color[HTML]{4472C4} \textbf{0.285}} \\
&\multicolumn{1}{c|}{}                                & Avg.& 0.168 & 0.269 & {\color[HTML]{FF0000} \textbf{0.150}} & {\color[HTML]{FF0000} \textbf{0.256}} & 0.127 & 0.235 & {\color[HTML]{FF0000} \textbf{0.125}} & {\color[HTML]{FF0000} \textbf{0.235}} & 0.148 & 0.253 & {\color[HTML]{FF0000} \textbf{0.118}} & {\color[HTML]{FF0000} \textbf{0.224}} & 0.116 & 0.226 & {\color[HTML]{4472C4} \textbf{0.117}} & {\color[HTML]{FF0000} \textbf{0.226}} \\
\cline{2-19}                   
&\multirow{5}*{\rotatebox{90}{PeMS04}}                & 12  & 0.103 & 0.211 & {\color[HTML]{FF0000} \textbf{0.103}} & {\color[HTML]{FF0000} \textbf{0.211}} & 0.092 & 0.198 & {\color[HTML]{FF0000} \textbf{0.091}} & {\color[HTML]{FF0000} \textbf{0.197}} & 0.101 & 0.208 & {\color[HTML]{FF0000} \textbf{0.084}} & {\color[HTML]{FF0000} \textbf{0.190}} & 0.084 & 0.188 & {\color[HTML]{FF0000} \textbf{0.080}} & {\color[HTML]{FF0000} \textbf{0.183}} \\
&\multicolumn{1}{c|}{}                                & 24  & 0.168 & 0.273 & {\color[HTML]{FF0000} \textbf{0.167}} & {\color[HTML]{FF0000} \textbf{0.273}} & 0.137 & 0.244 & {\color[HTML]{FF0000} \textbf{0.137}} & {\color[HTML]{FF0000} \textbf{0.244}} & 0.162 & 0.268 & {\color[HTML]{FF0000} \textbf{0.116}} & {\color[HTML]{FF0000} \textbf{0.228}} & 0.121 & 0.228 & {\color[HTML]{FF0000} \textbf{0.108}} & {\color[HTML]{FF0000} \textbf{0.213}} \\
&\multicolumn{1}{c|}{}                                & 36  & 0.246 & 0.335 & {\color[HTML]{FF0000} \textbf{0.243}} & {\color[HTML]{FF0000} \textbf{0.333}} & 0.187 & 0.289 & {\color[HTML]{FF0000} \textbf{0.187}} & {\color[HTML]{FF0000} \textbf{0.289}} & 0.227 & 0.321 & {\color[HTML]{FF0000} \textbf{0.147}} & {\color[HTML]{FF0000} \textbf{0.261}} & 0.151 & 0.257 & {\color[HTML]{FF0000} \textbf{0.139}} & {\color[HTML]{FF0000} \textbf{0.244}} \\
&\multicolumn{1}{c|}{}                                & 48  & 0.326 & 0.390 & {\color[HTML]{FF0000} \textbf{0.320}} & {\color[HTML]{FF0000} \textbf{0.387}} & 0.235 & 0.329 & {\color[HTML]{FF0000} \textbf{0.234}} & {\color[HTML]{FF0000} \textbf{0.328}} & 0.297 & 0.367 & {\color[HTML]{FF0000} \textbf{0.168}} & {\color[HTML]{FF0000} \textbf{0.279}} & 0.186 & 0.288 & {\color[HTML]{4472C4} \textbf{0.191}} & {\color[HTML]{4472C4} \textbf{0.295}} \\
&\multicolumn{1}{c|}{}                                & Avg.& 0.211 & 0.302 & {\color[HTML]{FF0000} \textbf{0.208}} & {\color[HTML]{FF0000} \textbf{0.301}} & 0.163 & 0.265 & {\color[HTML]{FF0000} \textbf{0.162}} & {\color[HTML]{FF0000} \textbf{0.265}} & 0.197 & 0.291 & {\color[HTML]{FF0000} \textbf{0.129}} & {\color[HTML]{FF0000} \textbf{0.240}} & 0.135 & 0.240 & {\color[HTML]{FF0000} \textbf{0.130}} & {\color[HTML]{FF0000} \textbf{0.234}} \\
\cline{2-19}                   
&\multirow{5}*{\rotatebox{90}{PeMS07}}                & 12  & 0.079 & 0.185 & {\color[HTML]{4472C4} \textbf{0.080}} & {\color[HTML]{FF0000} \textbf{0.185}} & 0.069 & 0.171 & {\color[HTML]{FF0000} \textbf{0.069}} & {\color[HTML]{FF0000} \textbf{0.171}} & 0.076 & 0.180 & {\color[HTML]{FF0000} \textbf{0.068}} & {\color[HTML]{FF0000} \textbf{0.169}} & 0.063 & 0.159 & {\color[HTML]{FF0000} \textbf{0.060}} & {\color[HTML]{FF0000} \textbf{0.154}} \\
&\multicolumn{1}{c|}{}                                & 24  & 0.140 & 0.248 & {\color[HTML]{FF0000} \textbf{0.139}} & {\color[HTML]{FF0000} \textbf{0.246}} & 0.110 & 0.218 & {\color[HTML]{FF0000} \textbf{0.109}} & {\color[HTML]{FF0000} \textbf{0.217}} & 0.130 & 0.241 & {\color[HTML]{FF0000} \textbf{0.106}} & {\color[HTML]{FF0000} \textbf{0.212}} & 0.090 & 0.192 & {\color[HTML]{FF0000} \textbf{0.085}} & {\color[HTML]{FF0000} \textbf{0.184}} \\
&\multicolumn{1}{c|}{}                                & 36  & 0.210 & 0.306 & {\color[HTML]{FF0000} \textbf{0.209}} & {\color[HTML]{FF0000} \textbf{0.304}} & 0.153 & 0.260 & {\color[HTML]{FF0000} \textbf{0.152}} & {\color[HTML]{FF0000} \textbf{0.260}} & 0.184 & 0.286 & {\color[HTML]{FF0000} \textbf{0.144}} & {\color[HTML]{FF0000} \textbf{0.248}} & 0.135 & 0.242 & {\color[HTML]{FF0000} \textbf{0.132}} & {\color[HTML]{FF0000} \textbf{0.237}} \\
&\multicolumn{1}{c|}{}                                & 48  & 0.285 & 0.360 & {\color[HTML]{FF0000} \textbf{0.282}} & {\color[HTML]{FF0000} \textbf{0.357}} & 0.195 & 0.299 & {\color[HTML]{FF0000} \textbf{0.194}} & {\color[HTML]{FF0000} \textbf{0.298}} & 0.244 & 0.332 & {\color[HTML]{FF0000} \textbf{0.179}} & {\color[HTML]{FF0000} \textbf{0.281}} & 0.171 & 0.277 & {\color[HTML]{4472C4} \textbf{0.183}} & {\color[HTML]{4472C4} \textbf{0.293}} \\
&\multicolumn{1}{c|}{}                                & Avg.& 0.179 & 0.275 & {\color[HTML]{FF0000} \textbf{0.177}} & {\color[HTML]{FF0000} \textbf{0.273}} & 0.132 & 0.237 & {\color[HTML]{FF0000} \textbf{0.131}} & {\color[HTML]{FF0000} \textbf{0.236}} & 0.159 & 0.260 & {\color[HTML]{FF0000} \textbf{0.124}} & {\color[HTML]{FF0000} \textbf{0.228}} & 0.115 & 0.218 & {\color[HTML]{FF0000} \textbf{0.115}} & {\color[HTML]{FF0000} \textbf{0.217}} \\
\cline{2-19}                   
&\multirow{5}*{\rotatebox{90}{PeMS08}}                & 12  & 0.093 & 0.198 & {\color[HTML]{FF0000} \textbf{0.094}} & {\color[HTML]{FF0000} \textbf{0.199}} & 0.082 & 0.184 & {\color[HTML]{FF0000} \textbf{0.082}} & {\color[HTML]{FF0000} \textbf{0.184}} & 0.087 & 0.191 & {\color[HTML]{4472C4} \textbf{0.130}} & {\color[HTML]{FF0000} \textbf{0.187}} & 0.077 & 0.176 & {\color[HTML]{FF0000} \textbf{0.071}} & {\color[HTML]{FF0000} \textbf{0.167}} \\
&\multicolumn{1}{c|}{}                                & 24  & 0.153 & 0.257 & {\color[HTML]{FF0000} \textbf{0.152}} & {\color[HTML]{FF0000} \textbf{0.255}} & 0.124 & 0.228 & {\color[HTML]{4472C4} \textbf{0.125}} & {\color[HTML]{FF0000} \textbf{0.228}} & 0.137 & 0.240 & {\color[HTML]{4472C4} \textbf{0.163}} & {\color[HTML]{FF0000} \textbf{0.222}} & 0.107 & 0.207 & {\color[HTML]{FF0000} \textbf{0.100}} & {\color[HTML]{FF0000} \textbf{0.198}} \\
&\multicolumn{1}{c|}{}                                & 36  & 0.223 & 0.312 & {\color[HTML]{FF0000} \textbf{0.220}} & {\color[HTML]{FF0000} \textbf{0.310}} & 0.170 & 0.268 & {\color[HTML]{FF0000} \textbf{0.170}} & {\color[HTML]{FF0000} \textbf{0.267}} & 0.194 & 0.291 & {\color[HTML]{FF0000} \textbf{0.163}} & {\color[HTML]{FF0000} \textbf{0.222}} & 0.142 & 0.239 & {\color[HTML]{FF0000} \textbf{0.130}} & {\color[HTML]{FF0000} \textbf{0.223}} \\
&\multicolumn{1}{c|}{}                                & 48  & 0.297 & 0.362 & {\color[HTML]{FF0000} \textbf{0.293}} & {\color[HTML]{FF0000} \textbf{0.360}} & 0.220 & 0.307 & {\color[HTML]{FF0000} \textbf{0.218}} & {\color[HTML]{FF0000} \textbf{0.305}} & 0.255 & 0.334 & {\color[HTML]{FF0000} \textbf{0.223}} & {\color[HTML]{FF0000} \textbf{0.276}} & 0.204 & 0.293 & {\color[HTML]{FF0000} \textbf{0.200}} & {\color[HTML]{FF0000} \textbf{0.293}} \\
&\multicolumn{1}{c|}{}                                & Avg.& 0.192 & 0.282 & {\color[HTML]{FF0000} \textbf{0.190}} & {\color[HTML]{FF0000} \textbf{0.281}} & 0.149 & 0.247 & {\color[HTML]{FF0000} \textbf{0.149}} & {\color[HTML]{FF0000} \textbf{0.246}} & 0.168 & 0.264 & {\color[HTML]{4472C4} \textbf{0.170}} & {\color[HTML]{FF0000} \textbf{0.227}} & 0.133 & 0.228 & {\color[HTML]{FF0000} \textbf{0.125}} & {\color[HTML]{FF0000} \textbf{0.220}} \\
\cline{2-19} 

&\multicolumn{1}{c|}{Imp\%}  & Avg. & - & - &3.24\% &1.48\% & - & - &0.57\% &0.23\% & - & - &19.61\% &14.06\% & - & - &2.51\% &1.55\% \\
\cline{2-19}
\end{tabular}
}
\label{tab:pems}
\end{table*}
\linespread{1}
\label{ssec:exp_main}
\linespread{1.2}

\begin{table*}[!ht]
\caption{Full results for performance comparisons between backbone models and their integration with our proposals on ETT, Electricity, Traffic, and Weather datasets. The results for \mlp{} and \cyclenet{} are taken from the \cyclenet{} paper \citep{patchtst}, while the results for \patchtst{} and \itrans{} are from the \iTransformer{} paper \citep{itrans}.}
\resizebox{\textwidth}{!}{
\begin{tabular}{cc|c|cc|cc||cc|cc||cc|cc||cc|ccc}
\cline{2-19}
&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{\mlp{}} & \multicolumn{2}{c||}{+\textbf{Ours}}& \multicolumn{2}{c|}{\cyclenet{}}& \multicolumn{2}{c||}{+\textbf{Ours}} & \multicolumn{2}{c|}{\patchtst}& \multicolumn{2}{c||}{+\textbf{Ours}} & \multicolumn{2}{c|}{\itrans{}}& \multicolumn{2}{c}{+\textbf{Ours}} \\
\cline{2-19}
&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
\cline{2-19}  
&\multirow{5}*{\rotatebox{90}{ETTh1}}                & 96  & 0.383 & 0.401 & {\color[HTML]{FF0000} \textbf{0.373}} & {\color[HTML]{FF0000} \textbf{0.396}} & 0.375 & 0.395 & {\color[HTML]{FF0000} \textbf{0.368}} & {\color[HTML]{FF0000} \textbf{0.390}} & 0.414 & 0.419 & {\color[HTML]{FF0000} \textbf{0.373}} & {\color[HTML]{FF0000} \textbf{0.398}} & 0.386 & 0.405 & {\color[HTML]{FF0000} \textbf{0.373}} & {\color[HTML]{FF0000} \textbf{0.401}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.437 & 0.432 & {\color[HTML]{FF0000} \textbf{0.435}} & {\color[HTML]{4472C4} \textbf{0.434}} & 0.436 & 0.428 & {\color[HTML]{FF0000} \textbf{0.424}} & {\color[HTML]{FF0000} \textbf{0.424}} & 0.460 & 0.445 & {\color[HTML]{FF0000} \textbf{0.424}} & {\color[HTML]{FF0000} \textbf{0.427}} & 0.441 & 0.436 & {\color[HTML]{FF0000} \textbf{0.432}} & {\color[HTML]{FF0000} \textbf{0.436}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.494 & 0.461 & {\color[HTML]{FF0000} \textbf{0.474}} & {\color[HTML]{FF0000} \textbf{0.442}} & 0.496 & 0.455 & {\color[HTML]{FF0000} \textbf{0.470}} & {\color[HTML]{FF0000} \textbf{0.440}} & 0.501 & 0.466 & {\color[HTML]{FF0000} \textbf{0.465}} & {\color[HTML]{FF0000} \textbf{0.447}} & 0.487 & 0.458 & {\color[HTML]{FF0000} \textbf{0.466}} & {\color[HTML]{FF0000} \textbf{0.455}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.540 & 0.499 & {\color[HTML]{FF0000} \textbf{0.464}} & {\color[HTML]{FF0000} \textbf{0.459}} & 0.520 & 0.484 & {\color[HTML]{FF0000} \textbf{0.462}} & {\color[HTML]{FF0000} \textbf{0.461}} & 0.500 & 0.488 & {\color[HTML]{FF0000} \textbf{0.444}} & {\color[HTML]{FF0000} \textbf{0.458}} & 0.503 & 0.491 & {\color[HTML]{FF0000} \textbf{0.455}} & {\color[HTML]{FF0000} \textbf{0.466}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.464 & 0.448 & {\color[HTML]{FF0000} \textbf{0.437}} & {\color[HTML]{FF0000} \textbf{0.433}} & 0.457 & 0.441 & {\color[HTML]{FF0000} \textbf{0.431}} & {\color[HTML]{FF0000} \textbf{0.429}} & 0.469 & 0.455 & {\color[HTML]{FF0000} \textbf{0.427}} & {\color[HTML]{FF0000} \textbf{0.433}} & 0.454 & 0.448 & {\color[HTML]{FF0000} \textbf{0.431}} & {\color[HTML]{FF0000} \textbf{0.440}} \\
\cline{2-19}                   
&\multirow{5}*{\rotatebox{90}{ETTh2}}                & 96  & 0.299 & 0.345 & {\color[HTML]{FF0000} \textbf{0.283}} & {\color[HTML]{FF0000} \textbf{0.336}} & 0.298 & 0.344 & {\color[HTML]{FF0000} \textbf{0.280}} & {\color[HTML]{FF0000} \textbf{0.333}} & 0.302 & 0.348 & {\color[HTML]{FF0000} \textbf{0.285}} & {\color[HTML]{FF0000} \textbf{0.336}} & 0.297 & 0.349 & {\color[HTML]{FF0000} \textbf{0.293}} & {\color[HTML]{FF0000} \textbf{0.342}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.371 & 0.394 & {\color[HTML]{FF0000} \textbf{0.362}} & {\color[HTML]{FF0000} \textbf{0.385}} & 0.372 & 0.396 & {\color[HTML]{FF0000} \textbf{0.357}} & {\color[HTML]{FF0000} \textbf{0.384}} & 0.388 & 0.400 & {\color[HTML]{FF0000} \textbf{0.367}} & {\color[HTML]{FF0000} \textbf{0.389}} & 0.380 & 0.400 & {\color[HTML]{FF0000} \textbf{0.373}} & {\color[HTML]{FF0000} \textbf{0.393}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.420 & 0.429 & {\color[HTML]{FF0000} \textbf{0.404}} & {\color[HTML]{FF0000} \textbf{0.420}} & 0.431 & 0.439 & {\color[HTML]{FF0000} \textbf{0.400}} & {\color[HTML]{FF0000} \textbf{0.420}} & 0.426 & 0.433 & {\color[HTML]{FF0000} \textbf{0.409}} & {\color[HTML]{FF0000} \textbf{0.425}} & 0.428 & 0.432 & {\color[HTML]{FF0000} \textbf{0.417}} & {\color[HTML]{FF0000} \textbf{0.429}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.438 & 0.450 & {\color[HTML]{FF0000} \textbf{0.413}} & {\color[HTML]{FF0000} \textbf{0.435}} & 0.450 & 0.458 & {\color[HTML]{FF0000} \textbf{0.409}} & {\color[HTML]{FF0000} \textbf{0.436}} & 0.431 & 0.446 & {\color[HTML]{FF0000} \textbf{0.419}} & {\color[HTML]{FF0000} \textbf{0.442}} & 0.427 & 0.445 & {\color[HTML]{FF0000} \textbf{0.424}} & {\color[HTML]{FF0000} \textbf{0.442}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.382 & 0.405 & {\color[HTML]{FF0000} \textbf{0.366}} & {\color[HTML]{FF0000} \textbf{0.394}} & 0.388 & 0.409 & {\color[HTML]{FF0000} \textbf{0.362}} & {\color[HTML]{FF0000} \textbf{0.393}} & 0.387 & 0.407 & {\color[HTML]{FF0000} \textbf{0.370}} & {\color[HTML]{FF0000} \textbf{0.398}} & 0.383 & 0.407 & {\color[HTML]{FF0000} \textbf{0.377}} & {\color[HTML]{FF0000} \textbf{0.402}} \\
\cline{2-19}                   
&\multirow{5}*{\rotatebox{90}{ETTm1}}                & 96  & 0.327 & 0.366 & {\color[HTML]{FF0000} \textbf{0.325}} & {\color[HTML]{FF0000} \textbf{0.361}} & 0.319 & 0.360 & {\color[HTML]{FF0000} \textbf{0.306}} & {\color[HTML]{FF0000} \textbf{0.349}} & 0.329 & 0.367 & {\color[HTML]{FF0000} \textbf{0.316}} & {\color[HTML]{FF0000} \textbf{0.354}} & 0.334 & 0.368 & {\color[HTML]{FF0000} \textbf{0.315}} & {\color[HTML]{FF0000} \textbf{0.353}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.370 & 0.386 & {\color[HTML]{FF0000} \textbf{0.367}} & {\color[HTML]{FF0000} \textbf{0.383}} & 0.360 & 0.381 & {\color[HTML]{FF0000} \textbf{0.349}} & {\color[HTML]{FF0000} \textbf{0.375}} & 0.367 & 0.385 & {\color[HTML]{FF0000} \textbf{0.360}} & {\color[HTML]{FF0000} \textbf{0.382}} & 0.377 & 0.391 & {\color[HTML]{FF0000} \textbf{0.369}} & {\color[HTML]{FF0000} \textbf{0.387}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.404 & 0.410 & {\color[HTML]{FF0000} \textbf{0.400}} & {\color[HTML]{FF0000} \textbf{0.405}} & 0.389 & 0.403 & {\color[HTML]{FF0000} \textbf{0.379}} & {\color[HTML]{FF0000} \textbf{0.394}} & 0.399 & 0.410 & {\color[HTML]{FF0000} \textbf{0.393}} & {\color[HTML]{FF0000} \textbf{0.402}} & 0.426 & 0.420 & {\color[HTML]{FF0000} \textbf{0.403}} & {\color[HTML]{FF0000} \textbf{0.412}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.462 & 0.445 & {\color[HTML]{FF0000} \textbf{0.462}} & {\color[HTML]{FF0000} \textbf{0.443}} & 0.447 & 0.441 & {\color[HTML]{FF0000} \textbf{0.438}} & {\color[HTML]{FF0000} \textbf{0.435}} & 0.454 & 0.439 & {\color[HTML]{FF0000} \textbf{0.454}} & {\color[HTML]{FF0000} \textbf{0.437}} & 0.491 & 0.459 & {\color[HTML]{FF0000} \textbf{0.460}} & {\color[HTML]{FF0000} \textbf{0.445}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.391 & 0.402 & {\color[HTML]{FF0000} \textbf{0.388}} & {\color[HTML]{FF0000} \textbf{0.398}} & 0.379 & 0.396 & {\color[HTML]{FF0000} \textbf{0.368}} & {\color[HTML]{FF0000} \textbf{0.388}} & 0.387 & 0.400 & {\color[HTML]{FF0000} \textbf{0.381}} & {\color[HTML]{FF0000} \textbf{0.394}} & 0.407 & 0.410 & {\color[HTML]{FF0000} \textbf{0.387}} & {\color[HTML]{FF0000} \textbf{0.399}} \\
\cline{2-19}                   
&\multirow{5}*{\rotatebox{90}{ETTm2}}                & 96  & 0.178 & 0.259 & {\color[HTML]{FF0000} \textbf{0.175}} & {\color[HTML]{FF0000} \textbf{0.259}} & 0.163 & 0.246 & {\color[HTML]{FF0000} \textbf{0.161}} & {\color[HTML]{FF0000} \textbf{0.244}} & 0.175 & 0.259 & {\color[HTML]{4472C4} \textbf{0.176}} & {\color[HTML]{4472C4} \textbf{0.261}} & 0.180 & 0.264 & {\color[HTML]{FF0000} \textbf{0.179}} & {\color[HTML]{FF0000} \textbf{0.264}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.242 & 0.302 & {\color[HTML]{FF0000} \textbf{0.240}} & {\color[HTML]{FF0000} \textbf{0.300}} & 0.229 & 0.290 & {\color[HTML]{FF0000} \textbf{0.225}} & {\color[HTML]{FF0000} \textbf{0.286}} & 0.241 & 0.302 & {\color[HTML]{FF0000} \textbf{0.241}} & {\color[HTML]{FF0000} \textbf{0.300}} & 0.250 & 0.309 & {\color[HTML]{FF0000} \textbf{0.241}} & {\color[HTML]{FF0000} \textbf{0.302}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.299 & 0.340 & {\color[HTML]{FF0000} \textbf{0.295}} & {\color[HTML]{FF0000} \textbf{0.336}} & 0.284 & 0.437 & {\color[HTML]{FF0000} \textbf{0.282}} & {\color[HTML]{FF0000} \textbf{0.323}} & 0.305 & 0.343 & {\color[HTML]{FF0000} \textbf{0.303}} & {\color[HTML]{FF0000} \textbf{0.340}} & 0.311 & 0.348 & {\color[HTML]{FF0000} \textbf{0.305}} & {\color[HTML]{FF0000} \textbf{0.343}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.400 & 0.398 & {\color[HTML]{FF0000} \textbf{0.394}} & {\color[HTML]{FF0000} \textbf{0.394}} & 0.389 & 0.391 & {\color[HTML]{FF0000} \textbf{0.380}} & {\color[HTML]{FF0000} \textbf{0.384}} & 0.402 & 0.400 & {\color[HTML]{4472C4} \textbf{0.404}} & {\color[HTML]{4472C4} \textbf{0.403}} & 0.412 & 0.407 & {\color[HTML]{FF0000} \textbf{0.406}} & {\color[HTML]{FF0000} \textbf{0.400}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.280 & 0.325 & {\color[HTML]{FF0000} \textbf{0.276}} & {\color[HTML]{FF0000} \textbf{0.322}} & 0.266 & 0.341 & {\color[HTML]{FF0000} \textbf{0.262}} & {\color[HTML]{FF0000} \textbf{0.309}} & 0.281 & 0.326 & {\color[HTML]{FF0000} \textbf{0.281}} & {\color[HTML]{FF0000} \textbf{0.326}} & 0.288 & 0.332 & {\color[HTML]{FF0000} \textbf{0.283}} & {\color[HTML]{FF0000} \textbf{0.327}} \\
\cline{2-19} 
&\multirow{5}*{\rotatebox{90}{Electricity}}          & 96  & 0.182 & 0.265 & {\color[HTML]{FF0000} \textbf{0.181}} & {\color[HTML]{FF0000} \textbf{0.264}} & 0.136 & 0.229 & {\color[HTML]{FF0000} \textbf{0.134}} & {\color[HTML]{FF0000} \textbf{0.228}} & 0.181 & 0.270 & {\color[HTML]{FF0000} \textbf{0.163}} & {\color[HTML]{FF0000} \textbf{0.248}} & 0.148 & 0.240 & {\color[HTML]{FF0000} \textbf{0.145}} & {\color[HTML]{FF0000} \textbf{0.237}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.187 & 0.270 & {\color[HTML]{FF0000} \textbf{0.186}} & {\color[HTML]{FF0000} \textbf{0.268}} & 0.152 & 0.244 & {\color[HTML]{FF0000} \textbf{0.152}} & {\color[HTML]{FF0000} \textbf{0.244}} & 0.188 & 0.274 & {\color[HTML]{FF0000} \textbf{0.172}} & {\color[HTML]{FF0000} \textbf{0.257}} & 0.162 & 0.253 & {\color[HTML]{FF0000} \textbf{0.158}} & {\color[HTML]{FF0000} \textbf{0.252}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.203 & 0.287 & {\color[HTML]{FF0000} \textbf{0.202}} & {\color[HTML]{FF0000} \textbf{0.285}} & 0.170 & 0.264 & {\color[HTML]{FF0000} \textbf{0.170}} & {\color[HTML]{FF0000} \textbf{0.263}} & 0.204 & 0.293 & {\color[HTML]{FF0000} \textbf{0.191}} & {\color[HTML]{FF0000} \textbf{0.278}} & 0.178 & 0.269 & {\color[HTML]{FF0000} \textbf{0.176}} & {\color[HTML]{FF0000} \textbf{0.271}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.244 & 0.319 & {\color[HTML]{FF0000} \textbf{0.243}} & {\color[HTML]{FF0000} \textbf{0.317}} & 0.212 & 0.299 & {\color[HTML]{FF0000} \textbf{0.210}} & {\color[HTML]{FF0000} \textbf{0.296}} & 0.246 & 0.324 & {\color[HTML]{FF0000} \textbf{0.239}} & {\color[HTML]{FF0000} \textbf{0.319}} & 0.225 & 0.317 & {\color[HTML]{FF0000} \textbf{0.212}} & {\color[HTML]{FF0000} \textbf{0.306}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.204 & 0.285 & {\color[HTML]{FF0000} \textbf{0.203}} & {\color[HTML]{FF0000} \textbf{0.283}} & 0.168 & 0.259 & {\color[HTML]{FF0000} \textbf{0.166}} & {\color[HTML]{FF0000} \textbf{0.258}} & 0.205 & 0.290 & {\color[HTML]{FF0000} \textbf{0.191}} & {\color[HTML]{FF0000} \textbf{0.275}} & 0.178 & 0.270 & {\color[HTML]{FF0000} \textbf{0.173}} & {\color[HTML]{FF0000} \textbf{0.267}} \\
\cline{2-19} 
&\multirow{5}*{\rotatebox{90}{Traffic}}              & 96  & 0.510 & 0.331 & {\color[HTML]{FF0000} \textbf{0.477}} & {\color[HTML]{FF0000} \textbf{0.301}} & 0.458 & 0.296 & {\color[HTML]{FF0000} \textbf{0.420}} & {\color[HTML]{FF0000} \textbf{0.275}} & 0.462 & 0.295 & {\color[HTML]{FF0000} \textbf{0.433}} & {\color[HTML]{FF0000} \textbf{0.280}} & 0.395 & 0.268 & {\color[HTML]{FF0000} \textbf{0.374}} & {\color[HTML]{FF0000} \textbf{0.247}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.505 & 0.327 & {\color[HTML]{FF0000} \textbf{0.478}} & {\color[HTML]{FF0000} \textbf{0.300}} & 0.457 & 0.295 & {\color[HTML]{FF0000} \textbf{0.437}} & {\color[HTML]{FF0000} \textbf{0.283}} & 0.466 & 0.296 & {\color[HTML]{FF0000} \textbf{0.447}} & {\color[HTML]{FF0000} \textbf{0.287}} & 0.417 & 0.276 & {\color[HTML]{FF0000} \textbf{0.399}} & {\color[HTML]{FF0000} \textbf{0.259}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.518 & 0.332 & {\color[HTML]{FF0000} \textbf{0.492}} & {\color[HTML]{FF0000} \textbf{0.305}} & 0.470 & 0.299 & {\color[HTML]{FF0000} \textbf{0.453}} & {\color[HTML]{FF0000} \textbf{0.291}} & 0.482 & 0.304 & {\color[HTML]{FF0000} \textbf{0.455}} & {\color[HTML]{FF0000} \textbf{0.285}} & 0.433 & 0.283 & {\color[HTML]{FF0000} \textbf{0.419}} & {\color[HTML]{FF0000} \textbf{0.269}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.553 & 0.350 & {\color[HTML]{FF0000} \textbf{0.531}} & {\color[HTML]{FF0000} \textbf{0.328}} & 0.502 & 0.314 & {\color[HTML]{FF0000} \textbf{0.482}} & {\color[HTML]{FF0000} \textbf{0.310}} & 0.514 & 0.322 & {\color[HTML]{FF0000} \textbf{0.486}} & {\color[HTML]{FF0000} \textbf{0.302}} & 0.467 & 0.302 & {\color[HTML]{FF0000} \textbf{0.451}} & {\color[HTML]{FF0000} \textbf{0.291}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.522 & 0.335 & {\color[HTML]{FF0000} \textbf{0.494}} & {\color[HTML]{FF0000} \textbf{0.308}} & 0.472 & 0.301 & {\color[HTML]{FF0000} \textbf{0.448}} & {\color[HTML]{FF0000} \textbf{0.290}} & 0.481 & 0.304 & {\color[HTML]{FF0000} \textbf{0.455}} & {\color[HTML]{FF0000} \textbf{0.288}} & 0.428 & 0.282 & {\color[HTML]{FF0000} \textbf{0.411}} & {\color[HTML]{FF0000} \textbf{0.266}} \\
\cline{2-19} 
&\multirow{5}*{\rotatebox{90}{Weather}}              & 96  & 0.181 & 0.219 & {\color[HTML]{FF0000} \textbf{0.176}} & {\color[HTML]{FF0000} \textbf{0.214}} & 0.158 & 0.203 & {\color[HTML]{FF0000} \textbf{0.158}} & {\color[HTML]{FF0000} \textbf{0.203}} & 0.177 & 0.218 & {\color[HTML]{FF0000} \textbf{0.169}} & {\color[HTML]{FF0000} \textbf{0.211}} & 0.174 & 0.214 & {\color[HTML]{FF0000} \textbf{0.173}} & {\color[HTML]{FF0000} \textbf{0.213}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.228 & 0.259 & {\color[HTML]{FF0000} \textbf{0.223}} & {\color[HTML]{FF0000} \textbf{0.256}} & 0.207 & 0.247 & {\color[HTML]{FF0000} \textbf{0.206}} & {\color[HTML]{FF0000} \textbf{0.244}} & 0.225 & 0.259 & {\color[HTML]{FF0000} \textbf{0.215}} & {\color[HTML]{FF0000} \textbf{0.251}} & 0.221 & 0.254 & {\color[HTML]{4472C4} \textbf{0.223}} & {\color[HTML]{4472C4} \textbf{0.257}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.282 & 0.299 & {\color[HTML]{FF0000} \textbf{0.279}} & {\color[HTML]{FF0000} \textbf{0.295}} & 0.262 & 0.289 & {\color[HTML]{FF0000} \textbf{0.260}} & {\color[HTML]{FF0000} \textbf{0.285}} & 0.278 & 0.297 & {\color[HTML]{FF0000} \textbf{0.274}} & {\color[HTML]{FF0000} \textbf{0.293}} & 0.278 & 0.296 & {\color[HTML]{FF0000} \textbf{0.278}} & {\color[HTML]{FF0000} \textbf{0.296}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.357 & 0.347 & {\color[HTML]{FF0000} \textbf{0.355}} & {\color[HTML]{FF0000} \textbf{0.345}} & 0.344 & 0.344 & {\color[HTML]{FF0000} \textbf{0.343}} & {\color[HTML]{FF0000} \textbf{0.342}} & 0.354 & 0.348 & {\color[HTML]{FF0000} \textbf{0.353}} & {\color[HTML]{FF0000} \textbf{0.345}} & 0.358 & 0.347 & {\color[HTML]{FF0000} \textbf{0.353}} & {\color[HTML]{FF0000} \textbf{0.344}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.262 & 0.281 & {\color[HTML]{FF0000} \textbf{0.258}} & {\color[HTML]{FF0000} \textbf{0.278}} & 0.243 & 0.271 & {\color[HTML]{FF0000} \textbf{0.242}} & {\color[HTML]{FF0000} \textbf{0.268}} & 0.259 & 0.281 & {\color[HTML]{FF0000} \textbf{0.253}} & {\color[HTML]{FF0000} \textbf{0.275}} & 0.258 & 0.278 & {\color[HTML]{FF0000} \textbf{0.257}} & {\color[HTML]{FF0000} \textbf{0.278}} \\
\cline{2-19}
&\multicolumn{1}{c|}{Imp\%}  & Avg. & - & - &3.24\% &2.60\% & - & - &3.92\% &3.40\% & - & - &4.48\% &2.97\% & - & - &3.24\% &1.94\% \\
\cline{2-19}
\end{tabular}
}
\label{tab:from_paper}
\end{table*}
\linespread{1}


\linespread{1.2}
\begin{table*}[!htbp]

\caption{Full results for performance comparisons between backbone models and them integrated with our proposals on ETT, electricity, traffic and weather datasets. All baseline results are run ourselves}
\resizebox{\textwidth}{!}{
\begin{tabular}{cc|c|cc|cc||cc|cc||cc|cc||cc|ccc}
\cline{2-19}
&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{\mlp{}} & \multicolumn{2}{c||}{+\textbf{Ours}}& \multicolumn{2}{c|}{\cyclenet{}}& \multicolumn{2}{c||}{+\textbf{Ours}} & \multicolumn{2}{c|}{\patchtst}& \multicolumn{2}{c||}{+\textbf{Ours}} & \multicolumn{2}{c|}{\itrans{}}& \multicolumn{2}{c}{+\textbf{Ours}} \\
\cline{2-19}
&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
\cline{2-19}  
&\multirow{5}*{\rotatebox{90}{ETTh1}}                & 96  & 0.380 & 0.399 & {\color[HTML]{FF0000} \textbf{0.373}} & {\color[HTML]{FF0000} \textbf{0.396}} & 0.379 & 0.400 & {\color[HTML]{FF0000} \textbf{0.368}} & {\color[HTML]{FF0000} \textbf{0.390}} & 0.390 & 0.407 & {\color[HTML]{FF0000} \textbf{0.373}} & {\color[HTML]{FF0000} \textbf{0.398}} & 0.383 & 0.405 & {\color[HTML]{FF0000} \textbf{0.373}} & {\color[HTML]{FF0000} \textbf{0.401}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.444 & 0.428 & {\color[HTML]{FF0000} \textbf{0.435}} & {\color[HTML]{4472C4} \textbf{0.434}} & 0.437 & 0.432 & {\color[HTML]{FF0000} \textbf{0.424}} & {\color[HTML]{FF0000} \textbf{0.424}} & 0.461 & 0.446 & {\color[HTML]{FF0000} \textbf{0.424}} & {\color[HTML]{FF0000} \textbf{0.427}} & 0.434 & 0.433 & {\color[HTML]{FF0000} \textbf{0.432}} & {\color[HTML]{FF0000} \textbf{0.436}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.478 & 0.444 & {\color[HTML]{FF0000} \textbf{0.474}} & {\color[HTML]{FF0000} \textbf{0.442}} & 0.477 & 0.446 & {\color[HTML]{FF0000} \textbf{0.470}} & {\color[HTML]{FF0000} \textbf{0.440}} & 0.486 & 0.457 & {\color[HTML]{FF0000} \textbf{0.465}} & {\color[HTML]{FF0000} \textbf{0.447}} & 0.470 & 0.452 & {\color[HTML]{FF0000} \textbf{0.466}} & {\color[HTML]{FF0000} \textbf{0.455}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.478 & 0.475 & {\color[HTML]{FF0000} \textbf{0.464}} & {\color[HTML]{FF0000} \textbf{0.459}} & 0.469 & 0.466 & {\color[HTML]{FF0000} \textbf{0.462}} & {\color[HTML]{FF0000} \textbf{0.461}} & 0.484 & 0.469 & {\color[HTML]{FF0000} \textbf{0.444}} & {\color[HTML]{FF0000} \textbf{0.458}} & 0.464 & 0.470 & {\color[HTML]{FF0000} \textbf{0.455}} & {\color[HTML]{FF0000} \textbf{0.466}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.445 & 0.437 & {\color[HTML]{FF0000} \textbf{0.437}} & {\color[HTML]{FF0000} \textbf{0.433}} & 0.441 & 0.436 & {\color[HTML]{FF0000} \textbf{0.431}} & {\color[HTML]{FF0000} \textbf{0.429}} & 0.455 & 0.445 & {\color[HTML]{FF0000} \textbf{0.427}} & {\color[HTML]{FF0000} \textbf{0.433}} & 0.438 & 0.440 & {\color[HTML]{FF0000} \textbf{0.431}} & {\color[HTML]{FF0000} \textbf{0.440}} \\
\cline{2-19}                   
&\multirow{5}*{\rotatebox{90}{ETTh2}}                & 96  & 0.293 & 0.343 & {\color[HTML]{FF0000} \textbf{0.283}} & {\color[HTML]{FF0000} \textbf{0.336}} & 0.297 & 0.347 & {\color[HTML]{FF0000} \textbf{0.280}} & {\color[HTML]{FF0000} \textbf{0.333}} & 0.298 & 0.345 & {\color[HTML]{FF0000} \textbf{0.285}} & {\color[HTML]{FF0000} \textbf{0.336}} & 0.321 & 0.362 & {\color[HTML]{FF0000} \textbf{0.293}} & {\color[HTML]{FF0000} \textbf{0.342}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.368 & 0.391 & {\color[HTML]{FF0000} \textbf{0.362}} & {\color[HTML]{FF0000} \textbf{0.385}} & 0.374 & 0.396 & {\color[HTML]{FF0000} \textbf{0.357}} & {\color[HTML]{FF0000} \textbf{0.384}} & 0.394 & 0.401 & {\color[HTML]{FF0000} \textbf{0.367}} & {\color[HTML]{FF0000} \textbf{0.389}} & 0.394 & 0.408 & {\color[HTML]{FF0000} \textbf{0.373}} & {\color[HTML]{FF0000} \textbf{0.393}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.419 & 0.427 & {\color[HTML]{FF0000} \textbf{0.404}} & {\color[HTML]{FF0000} \textbf{0.420}} & 0.417 & 0.432 & {\color[HTML]{FF0000} \textbf{0.400}} & {\color[HTML]{FF0000} \textbf{0.420}} & 0.418 & 0.429 & {\color[HTML]{FF0000} \textbf{0.409}} & {\color[HTML]{FF0000} \textbf{0.425}} & 0.449 & 0.447 & {\color[HTML]{FF0000} \textbf{0.417}} & {\color[HTML]{FF0000} \textbf{0.429}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.427 & 0.443 & {\color[HTML]{FF0000} \textbf{0.413}} & {\color[HTML]{FF0000} \textbf{0.435}} & 0.430 & 0.447 & {\color[HTML]{FF0000} \textbf{0.409}} & {\color[HTML]{FF0000} \textbf{0.436}} & 0.437 & 0.454 & {\color[HTML]{FF0000} \textbf{0.419}} & {\color[HTML]{FF0000} \textbf{0.442}} & 0.435 & 0.449 & {\color[HTML]{FF0000} \textbf{0.424}} & {\color[HTML]{FF0000} \textbf{0.442}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.377 & 0.401 & {\color[HTML]{FF0000} \textbf{0.366}} & {\color[HTML]{FF0000} \textbf{0.394}} & 0.380 & 0.406 & {\color[HTML]{FF0000} \textbf{0.362}} & {\color[HTML]{FF0000} \textbf{0.393}} & 0.387 & 0.407 & {\color[HTML]{FF0000} \textbf{0.370}} & {\color[HTML]{FF0000} \textbf{0.398}} & 0.400 & 0.417 & {\color[HTML]{FF0000} \textbf{0.377}} & {\color[HTML]{FF0000} \textbf{0.402}} \\
\cline{2-19}                   
&\multirow{5}*{\rotatebox{90}{ETTm1}}                & 96  & 0.348 & 0.371 & {\color[HTML]{FF0000} \textbf{0.325}} & {\color[HTML]{FF0000} \textbf{0.361}} & 0.315 & 0.358 & {\color[HTML]{FF0000} \textbf{0.306}} & {\color[HTML]{FF0000} \textbf{0.349}} & 0.339 & 0.370 & {\color[HTML]{FF0000} \textbf{0.316}} & {\color[HTML]{FF0000} \textbf{0.354}} & 0.351 & 0.378 & {\color[HTML]{FF0000} \textbf{0.315}} & {\color[HTML]{FF0000} \textbf{0.353}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.388 & 0.391 & {\color[HTML]{FF0000} \textbf{0.367}} & {\color[HTML]{FF0000} \textbf{0.383}} & 0.359 & 0.382 & {\color[HTML]{FF0000} \textbf{0.349}} & {\color[HTML]{FF0000} \textbf{0.375}} & 0.381 & 0.393 & {\color[HTML]{FF0000} \textbf{0.360}} & {\color[HTML]{FF0000} \textbf{0.382}} & 0.393 & 0.399 & {\color[HTML]{FF0000} \textbf{0.369}} & {\color[HTML]{FF0000} \textbf{0.387}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.422 & 0.412 & {\color[HTML]{FF0000} \textbf{0.400}} & {\color[HTML]{FF0000} \textbf{0.405}} & 0.389 & 0.407 & {\color[HTML]{FF0000} \textbf{0.379}} & {\color[HTML]{FF0000} \textbf{0.394}} & 0.411 & 0.413 & {\color[HTML]{FF0000} \textbf{0.393}} & {\color[HTML]{FF0000} \textbf{0.402}} & 0.422 & 0.421 & {\color[HTML]{FF0000} \textbf{0.403}} & {\color[HTML]{FF0000} \textbf{0.412}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.493 & 0.451 & {\color[HTML]{FF0000} \textbf{0.462}} & {\color[HTML]{FF0000} \textbf{0.443}} & 0.454 & 0.441 & {\color[HTML]{FF0000} \textbf{0.438}} & {\color[HTML]{FF0000} \textbf{0.435}} & 0.474 & 0.449 & {\color[HTML]{FF0000} \textbf{0.454}} & {\color[HTML]{FF0000} \textbf{0.437}} & 0.487 & 0.460 & {\color[HTML]{FF0000} \textbf{0.460}} & {\color[HTML]{FF0000} \textbf{0.445}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.413 & 0.406 & {\color[HTML]{FF0000} \textbf{0.388}} & {\color[HTML]{FF0000} \textbf{0.398}} & 0.379 & 0.397 & {\color[HTML]{FF0000} \textbf{0.368}} & {\color[HTML]{FF0000} \textbf{0.388}} & 0.401 & 0.406 & {\color[HTML]{FF0000} \textbf{0.381}} & {\color[HTML]{FF0000} \textbf{0.394}} & 0.413 & 0.414 & {\color[HTML]{FF0000} \textbf{0.387}} & {\color[HTML]{FF0000} \textbf{0.399}} \\
\cline{2-19}                   
&\multirow{5}*{\rotatebox{90}{ETTm2}}                & 96  & 0.186 & 0.270 & {\color[HTML]{FF0000} \textbf{0.175}} & {\color[HTML]{FF0000} \textbf{0.259}} & 0.164 & 0.248 & {\color[HTML]{FF0000} \textbf{0.161}} & {\color[HTML]{FF0000} \textbf{0.244}} & 0.180 & 0.263 & {\color[HTML]{FF0000} \textbf{0.176}} & {\color[HTML]{FF0000} \textbf{0.261}} & 0.189 & 0.275 & {\color[HTML]{FF0000} \textbf{0.179}} & {\color[HTML]{FF0000} \textbf{0.264}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.249 & 0.309 & {\color[HTML]{FF0000} \textbf{0.240}} & {\color[HTML]{FF0000} \textbf{0.300}} & 0.228 & 0.289 & {\color[HTML]{FF0000} \textbf{0.225}} & {\color[HTML]{FF0000} \textbf{0.286}} & 0.248 & 0.310 & {\color[HTML]{FF0000} \textbf{0.241}} & {\color[HTML]{FF0000} \textbf{0.300}} & 0.260 & 0.318 & {\color[HTML]{FF0000} \textbf{0.241}} & {\color[HTML]{FF0000} \textbf{0.302}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.308 & 0.345 & {\color[HTML]{FF0000} \textbf{0.295}} & {\color[HTML]{FF0000} \textbf{0.336}} & 0.285 & 0.328 & {\color[HTML]{FF0000} \textbf{0.282}} & {\color[HTML]{FF0000} \textbf{0.323}} & 0.307 & 0.345 & {\color[HTML]{FF0000} \textbf{0.303}} & {\color[HTML]{FF0000} \textbf{0.340}} & 0.326 & 0.359 & {\color[HTML]{FF0000} \textbf{0.305}} & {\color[HTML]{FF0000} \textbf{0.343}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.404 & 0.398 & {\color[HTML]{FF0000} \textbf{0.394}} & {\color[HTML]{FF0000} \textbf{0.394}} & 0.387 & 0.387 & {\color[HTML]{FF0000} \textbf{0.380}} & {\color[HTML]{FF0000} \textbf{0.384}} & 0.411 & 0.404 & {\color[HTML]{FF0000} \textbf{0.404}} & {\color[HTML]{FF0000} \textbf{0.403}} & 0.423 & 0.412 & {\color[HTML]{FF0000} \textbf{0.406}} & {\color[HTML]{FF0000} \textbf{0.400}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.287 & 0.331 & {\color[HTML]{FF0000} \textbf{0.276}} & {\color[HTML]{FF0000} \textbf{0.322}} & 0.266 & 0.313 & {\color[HTML]{FF0000} \textbf{0.262}} & {\color[HTML]{FF0000} \textbf{0.309}} & 0.287 & 0.331 & {\color[HTML]{FF0000} \textbf{0.281}} & {\color[HTML]{FF0000} \textbf{0.326}} & 0.299 & 0.341 & {\color[HTML]{FF0000} \textbf{0.283}} & {\color[HTML]{FF0000} \textbf{0.327}} \\
\cline{2-19} 
&\multirow{5}*{\rotatebox{90}{Electricity}}          & 96  & 0.187 & 0.267 & {\color[HTML]{FF0000} \textbf{0.181}} & {\color[HTML]{FF0000} \textbf{0.264}} & 0.136 & 0.230 & {\color[HTML]{FF0000} \textbf{0.134}} & {\color[HTML]{FF0000} \textbf{0.228}} & 0.181 & 0.268 & {\color[HTML]{FF0000} \textbf{0.163}} & {\color[HTML]{FF0000} \textbf{0.248}} & 0.161 & 0.251 & {\color[HTML]{FF0000} \textbf{0.145}} & {\color[HTML]{FF0000} \textbf{0.237}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.191 & 0.272 & {\color[HTML]{FF0000} \textbf{0.186}} & {\color[HTML]{FF0000} \textbf{0.268}} & 0.154 & 0.246 & {\color[HTML]{FF0000} \textbf{0.152}} & {\color[HTML]{FF0000} \textbf{0.244}} & 0.193 & 0.277 & {\color[HTML]{FF0000} \textbf{0.172}} & {\color[HTML]{FF0000} \textbf{0.257}} & 0.178 & 0.267 & {\color[HTML]{FF0000} \textbf{0.158}} & {\color[HTML]{FF0000} \textbf{0.252}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.206 & 0.288 & {\color[HTML]{FF0000} \textbf{0.202}} & {\color[HTML]{FF0000} \textbf{0.285}} & 0.171 & 0.264 & {\color[HTML]{FF0000} \textbf{0.170}} & {\color[HTML]{FF0000} \textbf{0.263}} & 0.199 & 0.286 & {\color[HTML]{FF0000} \textbf{0.191}} & {\color[HTML]{FF0000} \textbf{0.278}} & 0.193 & 0.281 & {\color[HTML]{FF0000} \textbf{0.176}} & {\color[HTML]{FF0000} \textbf{0.271}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.251 & 0.325 & {\color[HTML]{FF0000} \textbf{0.243}} & {\color[HTML]{FF0000} \textbf{0.317}} & 0.212 & 0.299 & {\color[HTML]{FF0000} \textbf{0.210}} & {\color[HTML]{FF0000} \textbf{0.296}} & 0.240 & 0.319 & {\color[HTML]{FF0000} \textbf{0.239}} & {\color[HTML]{FF0000} \textbf{0.319}} & 0.213 & 0.306 & {\color[HTML]{FF0000} \textbf{0.212}} & {\color[HTML]{FF0000} \textbf{0.306}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.209 & 0.288 & {\color[HTML]{FF0000} \textbf{0.203}} & {\color[HTML]{FF0000} \textbf{0.283}} & 0.168 & 0.260 & {\color[HTML]{FF0000} \textbf{0.166}} & {\color[HTML]{FF0000} \textbf{0.258}} & 0.203 & 0.287 & {\color[HTML]{FF0000} \textbf{0.191}} & {\color[HTML]{FF0000} \textbf{0.275}} & 0.186 & 0.276 & {\color[HTML]{FF0000} \textbf{0.173}} & {\color[HTML]{FF0000} \textbf{0.267}} \\
\cline{2-19} 
&\multirow{5}*{\rotatebox{90}{Traffic}}              & 96  & 0.504 & 0.314 & {\color[HTML]{FF0000} \textbf{0.477}} & {\color[HTML]{FF0000} \textbf{0.301}} & 0.432 & 0.292 & {\color[HTML]{FF0000} \textbf{0.420}} & {\color[HTML]{FF0000} \textbf{0.275}} & 0.468 & 0.301 & {\color[HTML]{FF0000} \textbf{0.433}} & {\color[HTML]{FF0000} \textbf{0.280}} & 0.403 & 0.276 & {\color[HTML]{FF0000} \textbf{0.374}} & {\color[HTML]{FF0000} \textbf{0.247}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.522 & 0.330 & {\color[HTML]{FF0000} \textbf{0.478}} & {\color[HTML]{FF0000} \textbf{0.300}} & 0.443 & 0.294 & {\color[HTML]{FF0000} \textbf{0.437}} & {\color[HTML]{FF0000} \textbf{0.283}} & 0.474 & 0.305 & {\color[HTML]{FF0000} \textbf{0.447}} & {\color[HTML]{FF0000} \textbf{0.287}} & 0.423 & 0.283 & {\color[HTML]{FF0000} \textbf{0.399}} & {\color[HTML]{FF0000} \textbf{0.259}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.534 & 0.334 & {\color[HTML]{FF0000} \textbf{0.492}} & {\color[HTML]{FF0000} \textbf{0.305}} & 0.460 & 0.304 & {\color[HTML]{FF0000} \textbf{0.453}} & {\color[HTML]{FF0000} \textbf{0.291}} & 0.498 & 0.324 & {\color[HTML]{FF0000} \textbf{0.455}} & {\color[HTML]{FF0000} \textbf{0.285}} & 0.441 & 0.292 & {\color[HTML]{FF0000} \textbf{0.419}} & {\color[HTML]{FF0000} \textbf{0.269}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.540 & 0.337 & {\color[HTML]{FF0000} \textbf{0.531}} & {\color[HTML]{FF0000} \textbf{0.328}} & 0.489 & 0.321 & {\color[HTML]{FF0000} \textbf{0.482}} & {\color[HTML]{FF0000} \textbf{0.310}} & 0.547 & 0.340 & {\color[HTML]{FF0000} \textbf{0.486}} & {\color[HTML]{FF0000} \textbf{0.302}} & 0.465 & 0.303 & {\color[HTML]{FF0000} \textbf{0.451}} & {\color[HTML]{FF0000} \textbf{0.291}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.525 & 0.329 & {\color[HTML]{FF0000} \textbf{0.494}} & {\color[HTML]{FF0000} \textbf{0.308}} & 0.456 & 0.303 & {\color[HTML]{FF0000} \textbf{0.448}} & {\color[HTML]{FF0000} \textbf{0.290}} & 0.497 & 0.317 & {\color[HTML]{FF0000} \textbf{0.455}} & {\color[HTML]{FF0000} \textbf{0.288}} & 0.433 & 0.288 & {\color[HTML]{FF0000} \textbf{0.411}} & {\color[HTML]{FF0000} \textbf{0.266}} \\
\cline{2-19} 
&\multirow{5}*{\rotatebox{90}{Weather}}              & 96  & 0.192 & 0.232 & {\color[HTML]{FF0000} \textbf{0.176}} & {\color[HTML]{FF0000} \textbf{0.214}} & 0.165 & 0.209 & {\color[HTML]{FF0000} \textbf{0.158}} & {\color[HTML]{FF0000} \textbf{0.203}} & 0.185 & 0.225 & {\color[HTML]{FF0000} \textbf{0.169}} & {\color[HTML]{FF0000} \textbf{0.211}} & 0.211 & 0.257 & {\color[HTML]{FF0000} \textbf{0.173}} & {\color[HTML]{FF0000} \textbf{0.213}} \\
&\multicolumn{1}{c|}{}                               & 192 & 0.237 & 0.266 & {\color[HTML]{FF0000} \textbf{0.223}} & {\color[HTML]{FF0000} \textbf{0.256}} & 0.209 & 0.248 & {\color[HTML]{FF0000} \textbf{0.206}} & {\color[HTML]{FF0000} \textbf{0.244}} & 0.229 & 0.262 & {\color[HTML]{FF0000} \textbf{0.215}} & {\color[HTML]{FF0000} \textbf{0.251}} & 0.255 & 0.285 & {\color[HTML]{FF0000} \textbf{0.223}} & {\color[HTML]{FF0000} \textbf{0.257}} \\
&\multicolumn{1}{c|}{}                               & 336 & 0.290 & 0.304 & {\color[HTML]{FF0000} \textbf{0.279}} & {\color[HTML]{FF0000} \textbf{0.295}} & 0.269 & 0.292 & {\color[HTML]{FF0000} \textbf{0.260}} & {\color[HTML]{FF0000} \textbf{0.285}} & 0.283 & 0.302 & {\color[HTML]{FF0000} \textbf{0.274}} & {\color[HTML]{FF0000} \textbf{0.293}} & 0.303 & 0.319 & {\color[HTML]{FF0000} \textbf{0.278}} & {\color[HTML]{FF0000} \textbf{0.296}} \\
&\multicolumn{1}{c|}{}                               & 720 & 0.366 & 0.352 & {\color[HTML]{FF0000} \textbf{0.355}} & {\color[HTML]{FF0000} \textbf{0.345}} & 0.348 & 0.344 & {\color[HTML]{FF0000} \textbf{0.343}} & {\color[HTML]{FF0000} \textbf{0.342}} & 0.361 & 0.351 & {\color[HTML]{FF0000} \textbf{0.353}} & {\color[HTML]{FF0000} \textbf{0.345}} & 0.370 & 0.362 & {\color[HTML]{FF0000} \textbf{0.353}} & {\color[HTML]{FF0000} \textbf{0.344}} \\
&\multicolumn{1}{c|}{}                               & Avg.& 0.271 & 0.288 & {\color[HTML]{FF0000} \textbf{0.258}} & {\color[HTML]{FF0000} \textbf{0.278}} & 0.248 & 0.273 & {\color[HTML]{FF0000} \textbf{0.242}} & {\color[HTML]{FF0000} \textbf{0.268}} & 0.264 & 0.285 & {\color[HTML]{FF0000} \textbf{0.253}} & {\color[HTML]{FF0000} \textbf{0.275}} & 0.285 & 0.306 & {\color[HTML]{FF0000} \textbf{0.257}} & {\color[HTML]{FF0000} \textbf{0.278}} \\
\cline{2-19}
&\multicolumn{1}{c|}{Imp\%}  & Avg. & - & - &4.11\% &2.56\% & - & - &2.52\% &2.18\% & - & - &5.48\% &3.61\% & - & - &5.51\% &4.17\% \\
\cline{2-19}
\end{tabular}
}
\label{tab:our_runs}
\end{table*}
\linespread{1}

\begin{table*}[!htbp]
\centering
\caption{Full results of ablation study with backbone predictor as \cyclenet{}.}
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{cc|c|cc|cc|cc|ccc}
\cline{2-11}
&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{\cyclenet{}} & \multicolumn{2}{c|}{+\snr{}}& \multicolumn{2}{c|}{+\model{}}& \multicolumn{2}{c}{+both} \\
\cline{2-11}
&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
\cline{2-11}  
&\multirow{5}*{\rotatebox{90}{ETTh1}}                       & 96   & 0.381 & 0.399 & 0.374 & 0.392 & 0.372 & 0.393 & \textbf{0.368} & \textbf{0.390} \\    
&\multicolumn{1}{c|}{}                                      & 192  & 0.438 & 0.426 & 0.432 & 0.423 & 0.424 & 0.424 & \textbf{0.424} & \textbf{0.424} \\    
&\multicolumn{1}{c|}{}                                      & 336  & 0.478 & 0.446 & 0.475 & 0.444 & 0.470 & 0.440 & \textbf{0.470} & \textbf{0.440} \\    
&\multicolumn{1}{c|}{}                                      & 720  & 0.478 & 0.472 & 0.470 & 0.469 & 0.480 & 0.473 & \textbf{0.462} & \textbf{0.461} \\    
&\multicolumn{1}{c|}{}                                      & Avg. & 0.444 & 0.436 & 0.438 & 0.432 & 0.436 & 0.432 & \textbf{0.431} & \textbf{0.429} \\    
\cline{2-11} &\multirow{5}*{\rotatebox{90}{ETTh2}}          & 96   & 0.297 & 0.347 & 0.288 & 0.339 & 0.283 & 0.336 & \textbf{0.280} & \textbf{0.333} \\    
&\multicolumn{1}{c|}{}                                      & 192  & 0.379 & 0.399 & 0.371 & 0.392 & 0.357 & 0.384 & \textbf{0.357} & \textbf{0.384} \\    
&\multicolumn{1}{c|}{}                                      & 336  & 0.419 & 0.433 & 0.412 & 0.425 & 0.400 & 0.420 & \textbf{0.400} & \textbf{0.420} \\    
&\multicolumn{1}{c|}{}                                      & 720  & 0.430 & 0.447 & 0.415 & 0.437 & 0.421 & 0.440 & \textbf{0.409} & \textbf{0.436} \\    
&\multicolumn{1}{c|}{}                                      & Avg. & 0.381 & 0.407 & 0.372 & 0.398 & 0.365 & 0.395 & \textbf{0.362} & \textbf{0.393} \\    
\cline{2-11} &\multirow{5}*{\rotatebox{90}{ETTm1}}          & 96   & 0.315 & 0.358 & 0.312 & 0.353 & 0.308 & 0.349 & \textbf{0.306} & \textbf{0.349} \\
&\multicolumn{1}{c|}{}                                      & 192  & 0.359 & 0.382 & 0.356 & 0.379 & 0.352 & 0.372 & \textbf{0.349} & \textbf{0.375} \\
&\multicolumn{1}{c|}{}                                      & 336  & 0.389 & 0.407 & 0.384 & 0.400 & 0.385 & 0.402 & \textbf{0.379} & \textbf{0.394} \\
&\multicolumn{1}{c|}{}                                      & 720  & 0.454 & 0.441 & 0.449 & 0.435 & 0.441 & 0.436 & \textbf{0.438} & \textbf{0.435} \\
&\multicolumn{1}{c|}{}                                      & Avg. & 0.379 & 0.397 & 0.375 & 0.392 & 0.371 & 0.390 & \textbf{0.368} & \textbf{0.388} \\
\cline{2-11} &\multirow{5}*{\rotatebox{90}{ETTm2}}          & 96   & 0.164 & 0.248 & 0.162 & 0.245 & 0.161 & 0.244 & \textbf{0.161} & \textbf{0.244} \\
&\multicolumn{1}{c|}{}                                      & 192  & 0.228 & 0.289 & 0.227 & 0.287 & 0.226 & 0.287 & \textbf{0.225} & \textbf{0.286} \\
&\multicolumn{1}{c|}{}                                      & 336  & 0.285 & 0.328 & 0.285 & 0.326 & 0.283 & 0.326 & \textbf{0.282} & \textbf{0.323} \\
&\multicolumn{1}{c|}{}                                      & 720  & 0.387 & 0.387 & 0.385 & 0.385 & 0.381 & 0.386 & \textbf{0.380} & \textbf{0.384} \\
&\multicolumn{1}{c|}{}                                      & Avg. & 0.266 & 0.313 & 0.265 & 0.311 & 0.263 & 0.311 & \textbf{0.262} & \textbf{0.309} \\
\cline{2-11} &\multirow{5}*{\rotatebox{90}{Weather}}        & 96   & 0.165 & 0.209 & 0.159 & 0.203 & 0.159 & 0.202 & \textbf{0.158} & \textbf{0.203} \\
&\multicolumn{1}{c|}{}                                      & 192  & 0.209 & 0.248 & 0.210 & 0.249 & 0.206 & 0.244 & \textbf{0.206} & \textbf{0.244} \\
&\multicolumn{1}{c|}{}                                      & 336  & 0.269 & 0.292 & 0.267 & 0.291 & 0.260 & 0.285 & \textbf{0.260} & \textbf{0.285} \\
&\multicolumn{1}{c|}{}                                      & 720  & 0.348 & 0.344 & 0.347 & 0.343 & 0.343 & 0.338 & \textbf{0.343} & \textbf{0.342} \\
&\multicolumn{1}{c|}{}                                      & Avg. & 0.248 & 0.273 & 0.246 & 0.272 & 0.242 & 0.267 & \textbf{0.242} & \textbf{0.268} \\
\cline{2-11}
\end{tabular}
}
\label{tab:abl_cyclenet_full}
\end{table*}

\begin{table*}[!htbp]
\centering
\caption{Full results of ablation study with backbone predictor as \itrans{}.}
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{cc|c|cc|cc|cc|ccc}
\cline{2-11}
&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{\itrans{}} & \multicolumn{2}{c|}{+\snr{}}& \multicolumn{2}{c|}{+\model{}}& \multicolumn{2}{c}{+both} \\
\cline{2-11}
&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
\cline{2-11}  
&\multirow{5}*{\rotatebox{90}{ETTh1}}          & 96   & 0.383 & 0.405 & 0.388 & 0.407 & 0.376 & 0.397 & \textbf{0.373} & \textbf{0.401} \\    
&\multicolumn{1}{c|}{}                         & 192  & 0.434 & 0.433 & 0.446 & 0.441 & 0.434 & 0.436 & \textbf{0.432} & \textbf{0.436} \\    
&\multicolumn{1}{c|}{}                         & 336  & 0.470 & 0.452 & 0.472 & 0.457 & 0.470 & 0.451 & \textbf{0.466} & \textbf{0.455} \\    
&\multicolumn{1}{c|}{}                         & 720  & 0.464 & 0.470 & 0.467 & 0.475 & 0.464 & 0.467 & \textbf{0.455} & \textbf{0.466} \\    
&\multicolumn{1}{c|}{}                         & Avg. & 0.438 & 0.440 & 0.443 & 0.445 & 0.436 & 0.438 & \textbf{0.431} & \textbf{0.440} \\    
\cline{2-11}                                           
&\multirow{5}*{\rotatebox{90}{ETTh2}}          & 96   & 0.321 & 0.362 & 0.331 & 0.370 & 0.296 & 0.344 & \textbf{0.293} & \textbf{0.342} \\    
&\multicolumn{1}{c|}{}                         & 192  & 0.394 & 0.408 & 0.398 & 0.410 & 0.385 & 0.399 & \textbf{0.373} & \textbf{0.393} \\    
&\multicolumn{1}{c|}{}                         & 336  & 0.449 & 0.447 & 0.431 & 0.437 & 0.418 & 0.429 & \textbf{0.417} & \textbf{0.429} \\    
&\multicolumn{1}{c|}{}                         & 720  & 0.435 & 0.449 & 0.437 & 0.453 & 0.426 & 0.443 & \textbf{0.424} & \textbf{0.442} \\    
&\multicolumn{1}{c|}{}                         & Avg. & 0.400 & 0.417 & 0.399 & 0.417 & 0.381 & 0.404 & \textbf{0.377} & \textbf{0.402} \\    
\cline{2-11}                                           
&\multirow{5}*{\rotatebox{90}{ETTm1}}          & 96   & 0.351 & 0.378 & 0.345 & 0.376 & 0.324 & 0.366 & \textbf{0.315} & \textbf{0.353} \\
&\multicolumn{1}{c|}{}                         & 192  & 0.393 & 0.399 & 0.385 & 0.395 & 0.376 & 0.393 & \textbf{0.369} & \textbf{0.387} \\
&\multicolumn{1}{c|}{}                         & 336  & 0.422 & 0.421 & 0.429 & 0.427 & 0.404 & 0.412 & \textbf{0.403} & \textbf{0.412} \\
&\multicolumn{1}{c|}{}                         & 720  & 0.487 & 0.460 & 0.494 & 0.461 & 0.461 & 0.447 & \textbf{0.460} & \textbf{0.445} \\
&\multicolumn{1}{c|}{}                         & Avg. & 0.413 & 0.414 & 0.413 & 0.415 & 0.391 & 0.404 & \textbf{0.387} & \textbf{0.399} \\
\cline{2-11}                                            
&\multirow{5}*{\rotatebox{90}{ETTm2}}          & 96   & 0.189 & 0.275 & 0.204 & 0.289 & 0.184 & 0.279 & \textbf{0.179} & \textbf{0.264} \\
&\multicolumn{1}{c|}{}                         & 192  & 0.260 & 0.318 & 0.275 & 0.331 & 0.252 & 0.321 & \textbf{0.241} & \textbf{0.302} \\
&\multicolumn{1}{c|}{}                         & 336  & 0.326 & 0.359 & 0.324 & 0.358 & 0.323 & 0.357 & \textbf{0.305} & \textbf{0.343} \\
&\multicolumn{1}{c|}{}                         & 720  & 0.423 & 0.412 & 0.421 & 0.410 & 0.412 & 0.411 & \textbf{0.406} & \textbf{0.400} \\
&\multicolumn{1}{c|}{}                         & Avg. & 0.299 & 0.341 & 0.306 & 0.347 & 0.293 & 0.342 & \textbf{0.283} & \textbf{0.327} \\
\cline{2-11}                                           
&\multirow{5}*{\rotatebox{90}{Weather}}        & 96   & 0.211 & 0.257 & 0.212 & 0.247 & 0.190 & 0.226 & \textbf{0.173} & \textbf{0.213} \\
&\multicolumn{1}{c|}{}                         & 192  & 0.255 & 0.285 & 0.252 & 0.283 & 0.243 & 0.270 & \textbf{0.223} & \textbf{0.257} \\
&\multicolumn{1}{c|}{}                         & 336  & 0.303 & 0.319 & 0.301 & 0.319 & 0.295 & 0.307 & \textbf{0.278} & \textbf{0.296} \\
&\multicolumn{1}{c|}{}                         & 720  & 0.370 & 0.362 & 0.367 & 0.357 & 0.366 & 0.353 & \textbf{0.353} & \textbf{0.344} \\
&\multicolumn{1}{c|}{}                         & Avg. & 0.285 & 0.306 & 0.283 & 0.302 & 0.274 & 0.289 & \textbf{0.257} & \textbf{0.278} \\
\cline{2-11}
\end{tabular}
}
\label{tab:abl_itrans_full}
\end{table*}

\end{document}



% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
