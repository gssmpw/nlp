%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{bm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure} NEVER USE SUBFIGURE
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Deep Active Speech Cancellation with Multi-Band Mamba Network}

\begin{document}

\twocolumn[
\icmltitle{Deep Active Speech Cancellation with Multi-Band Mamba Network}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yehuda Mishaly}{Tel-Aviv University}
\icmlauthor{Lior Wolf}{Tel-Aviv University}
\icmlauthor{Eliya Nachmani}{Ben Gurion University}
\end{icmlauthorlist}

\icmlaffiliation{Tel-Aviv University}{Blavatnik School of Computer Science, Tel Aviv University}
\icmlaffiliation{Ben Gurion University}{School of Electrical and Computer Engineering, Ben-Gurion University of the Negev}
% \icmlaffiliation{comp}{Company Name, Location, Country}


\icmlcorrespondingauthor{Eliya Nachmani}{enk100@gmail.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}
\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We present a novel deep learning network for Active Speech Cancellation (ASC), advancing beyond Active Noise Cancellation (ANC) methods by effectively canceling both noise and speech signals. The proposed Multi-Band Mamba architecture segments input audio into distinct frequency bands, enabling precise anti-signal generation and improved phase alignment across frequencies. Additionally, we introduce an optimization-driven loss function that provides near-optimal supervisory signals for anti-signal generation. Experimental results demonstrate substantial performance gains, achieving up to 7.2dB improvement in ANC scenarios and 6.2dB in ASC, significantly outperforming existing methods. Audio samples are available at \href{https://mishalydev.github.io/DeepASC-Demo/}{this link}.

\end{abstract}

\section{Introduction}
\label{submission}

Active Noise Cancellation (ANC) is a critical audio processing technique aimed at eliminating unwanted noise by generating an anti-noise signal \citep{lueg1936process,nelson1991active,fuller1996active,hansen1997active,kuo1999active}. 
ANC has practical applications in improving hearing devices for individuals with hearing impairments and reducing chronic noise exposure, thereby mitigating hearing loss risks. It also enhances focus, productivity, and listening experiences while reducing stress. Traditional ANC algorithms, like LMS and its deep learning variants \citep{zhang2021deep,park2023had,mostafavi2023deep,cha2023dnoisenet,pike2023generalized,singh2024enhancing}, have been widely adopted. However, these methods face limitations when dealing with more complex and high-frequency audio signals, as they are primarily designed to target noise. This paper addresses Active Speech Cancellation (ASC), which expands upon ANC by targeting the cancellation of both noise and speech signals. To our knowledge, this is the first work to actively cancel both noise and speech using deep learning, setting it apart from existing methods and enabling new research directions.

We propose a novel Multi-Band Mamba architecture, which partitions input signals into frequency bands, enabling precise anti-signal generation for improved phase alignment and cancellation. This design is particularly effective for speech signals, as it accounts for their broader frequency spectrum, including high frequencies often under-addressed by other methods. Coupled with an optimization-driven loss function, this approach achieves improved performance in dynamic acoustic scenarios. Results demonstrate up to a 7.2 dB improvement in ANC and a 6.2 dB gain in ASC for speech signals, outperforming deep-learning based baselines, which are considered state-of-the-art in the field.

\section{Related Work}
\textbf{Active Noise Cancellation:} 
The concept of ANC was first introduced by Lueg \cite{lueg1936process}, focusing on sound oscillation cancellation. Given that ANC algorithms must adapt to variations in amplitude, phase, and noise source movement \citep{nelson1991active,fuller1996active,hansen1997active,kuo1999active}, most ANC algorithms are based on the Least Mean Squares (LMS) algorithm \citep{burgess1981active}, which is effective in echo cancellation. The FxLMS (Filtered-x LMS) algorithm extends LMS by using an adaptive filter to account for distortions in the primary and secondary paths. \citet{boucher1991effect} analyzed errors in FxLMS due to inaccuracies in estimating the secondary path inverse, where nonlinear distortions degrade performance. Several methods address these issues: the Filtered-S LMS (FSLMS) \citep{das2004active} algorithm uses a Functional Link Artificial Neural Network (FLANN) \citep{patra1999identification} to handle nonlinearity, while the Volterra Filtered-x LMS (VFXLMS) \citep{tan2001adaptive} employs a multichannel structure. The Bilinear FxLMS \citep{kuo2005nonlinear} improves nonlinearity modeling, and the Leaky FxLMS \citep{tobias2005leaky} introduces a leakage term to mitigate overfitting. The Tangential Hyperbolic Function-based FxLMS (THF-FxLMS) \citep{ghasemi2016nonlinear} models saturation effects for enhanced performance. \citet{gannot2003noise} proposed blind source separation for noise cancellation. Moreover, \citet{oppenheim1994single} proposed single channel ANC based on Kalman filter formulation \citep{revach2021kalmannet} and \citet{rafaely2009spherical} investigated spherical loudspeaker arrays for local sound control.

ANC using deep learning was first proposed by \citet{zhang2021deep} with a convolutional-LSTM network for estimating both amplitude and phase of the canceling signal $y(n)$. Recurrent CNNs were later explored by \citet{park2023had,mostafavi2023deep,cha2023dnoisenet}, and autoencoder-based networks \cite{singh2024enhancing}, along with fully connected neural networks, were also applied to the problem \cite{pike2023generalized}. \citet{shi2020feedforward,shi2022selective,luo2022hybrid,park2023integrated,shi2023transferable,luo2023deep,luo2023delayless,luo2024unsupervised} have developed methods that select fixed-filter ANC (SFANC) from pre-trained control filters to achieve fast response times. Concurrently, \citet{zhu2021new,shi2022integration,zhang2023deep,shi2023multichannel,antonanzas2023remote,xiao2023spatially,zhang2023time,shi2024behind} advanced multichannel ANC systems. \citet{luo2023gfanc} introduced a CNN-based approach for real-time ANC, further enhanced with Kalman filtering. \citet{zhang2023low} incorporated an attention mechanism for real-time ANC using the Attentive Recurrent Network (ARN)\citep{pandey2022self}. Other significant real-time ANC contributions include genetic and bee colony algorithm-based methods \citep{ren2022improved, zhou2023genetic}.

\textbf{Active Speech Cancellation:}
ASC has been explored in various studies, each employing different approaches to predict and cancel unwanted speech signals. \citet{kondo2007speech} introduced an ASC method using a Linear Predictive Coding (LPC) model to predict the speech signal for generating the canceling signal $y(n)$. \citet{donley2017active} took a different approach by controlling the sound field to cancel speech using a linear dipole array of loudspeakers and a single microphone, effectively reducing the speech signal in the target area. \citet{iotov2022computationally} employed a long-term linear prediction filter to anticipate incoming speech, enabling the cancellation of the speech signal. Additionally, \citet{iotov2023adaptive} proposed HOSpLP-ANC, which combines a high-order sparse linear predictor with the LMS algorithm for effective speech cancellation.

\textbf{Mamba architecture:} Recently, the Mamba architecture has been introduced \citep{gu2023mamba, dao2024transformers}, leveraging State Space Models (SSMs) to achieve notable improvements in various audio-related tasks. One of the key advantages of the Mamba architecture is its ability to perform fast inference, especially when handling sequences up to a million in length, which represents a significant improvement over traditional generative architectures. This has enabled advancements in several applications, including automatic speech recognition \citep{zhang2024mamba, zhang2024rethinking}, speech separation \citep{jiang2024dual, li2024spmamba}, speech enhancement \citep{chao2024investigation, luo2024mambagan, quan2024multichannel}, speech super-resolution \citep{waveumamba}, sound generation \citep{jiang2024speech}, audio representation \citep{shams2024ssamba, yadav2024audio, erol2024audio}, sound localization \citep{xiao2024tf, mu2024seld}, audio tagging \citep{lin2024audio}, and deepfake audio detection \citep{chen2024rawbmamba}. 

\begin{figure}[t]
% \vspace{-10pt}
\centering
\begin{minipage}{.45\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/anc_3.png}
\caption{Typical ANC system diagram.}
\label{figs:anc}
\end{minipage}%
\end{figure}

\section{Method}
\paragraph{Backgorund} 
The signal processing framework of a typical feedforward ANC system is detailed, emphasizing the roles of the primary and secondary acoustic paths. In such systems, reference and error microphone are utilized to generate a canceling signal that minimizes unwanted noise. The primary path \( P(z) \) represents the acoustic transfer function from the noise source to the error microphone, while the secondary path \( S(z) \) represents the acoustic transfer function from the loudspeaker to the error microphone. The signal captured by the reference microphone is denoted as \( x(n) \), while the signal captured by the error microphone is denoted as \( e(n) \). These signals are fed into the ANC controller, which generates a canceling signal \( y(n) \). The canceling signal is then played through a loudspeaker, referred to as \( f_{LS} \), producing \( f_{LS}\{y(n)\} \), which aims to suppress the unwanted noise near the error microphone. The loudspeaker output \( f_{LS}\{y(n)\} \), after passing through the secondary path \( S(z) \), generates the anti-signal denoted by \( a(n) \). The relationship is described by the following equation: \begin{equation} a(n) = S(z) \ast f_{LS}\{y(n)\} \end{equation}

Similarly, the reference signal \( x(n) \), transmitted through the primary path \( P(z) \), produces the primary signal denoted by \( d(n) \), which is expressed as: \begin{equation} d(n) = P(z) \ast x(n) \end{equation}

The error signal \( e(n) \), defined as the difference between the primary signal \( d(n) \) and the anti-signal \( a(n) \), is defined as: \begin{equation} e(n) = d(n) - a(n) \end{equation}

The goal of the ANC controller is to minimize the error signal \( e(n) \), ideally to zero, indicating successful noise cancellation. In the feedback ANC approach, only the error signal \( e(n) \) is utilized to generate the canceling signal, aiming to minimize residual noise at the error microphone.

One of the widely used metrics for measuring noise attenuation in ANC is the Normalized Mean Square Error (NMSE) between two signals, defined by: \begin{equation} \text{NMSE}\left[ \textbf{u}, \textbf{v}\right] = 10 \cdot \text{log}_{10}  \left (\frac{\sum_{n=1}^{M} (u(n) - v(n))^2}{\sum_{n=1}^{M} u(n)^2} \right ) \label{eq:nmse_loss} \end{equation}
where $\textbf{u}$ and $\textbf{v}$ are the vector representations of the signals $u(n)$ and $v(n)$ such that $\textbf{u}=[u(1),...,u(M)]$ and $\textbf{v}=[v(1),...,v(M)]$. Here, $M$ represents the total number of samples. Typically, $u(n)$ refers to the target signal, while $v(n)$ denotes the estimated signal. A lower NMSE value indicates a better estimation, reflecting a closer alignment between the estimated signal and the target signal. In the context of ANC, typically $u(n)$ is the primary signal $d(n)$, while $v(n)$ will be the anti-signal $a(n)$. A schematic representation of the ANC system is illustrated in Figure \ref{figs:anc}. Appendix~\ref{apdx:anc} provides an explanation of the acoustic dynamics of ANC systems in in-ear headphones.

\paragraph{Method}
The proposed method utilizes a novel architecture that integrates the Mamba framework \citep{gu2023mamba} for the generation of the anti-signal. The architecture features a filter bank that decomposes the input signal into multiple frequency bands, each processed by an encoder and a masking Mamba network, incorporating Dual-path Mamba blocks \citep{jiang2024dual} for enhanced feature extraction. The outputs of the multi-band masking are then concatenated and passed through a decoder. Furthermore, we introduce a new loss function that leverages the near-optimal anti-signal as the ground truth, significantly improving the precision of the anti-signal generation process. A diagram of the proposed architecture is shown in Fig.\ref{figs:our_arch}.
\begin{figure}[t]
\centering
\includegraphics[width=.4\textwidth]{figs/our_arch3.png}
% \vspace{0.1in}
\caption{DeepASC Architecture: The reference signal is decomposed into frequency bands, encoded, processed by Mamba-Band blocks, concatenated, and decoded to reconstruct the signal.}
% \vspace{0.1in}
\label{figs:our_arch}
\end{figure}
\subsection{DeepASC Architecture}
Let \( x(n) \) be the reference signal such that $1 \leq n \leq M $. The reference signal \( x(n) \) is decomposed into \( Q \in \mathbb{N} \) different frequency bands \( x_1(n), \ldots, x_Q(n) \). These frequency bands are evenly divided such that for the maximum frequency \( F \), the \( i \)-th frequency band \( x_i(n) \) covers the frequency range \( \left[(i-1)\frac{F}{Q}, i\frac{F}{Q}\right] \) where $1 \leq i \leq Q$. In addition to the decomposed bands, the original full-band signal $x(n)$ is included as $x_{0}(n)$. Each band $x_i(n)$ (where $0 \leq i \leq Q$, the zero index is for the entire unfiltered band) is then processed through its own Mamba-Band block (MB-block). Each MB-block comprises an encoder and a masking network that utilize Mamba-based layers. Within each MB-block, the encoder consists of a one-dimensional convolution layer \( E_i \) with a kernel size \( k \) and a stride of $k/2$. The encoder transforms the \( i \)-th reference signal \( x_i(n) \) into a two-dimensional latent representation:
\begin{equation}
    \textbf{H}_i = E_i[\textbf{x}_i]
\end{equation}
where \( \textbf{H}_i \in \mathbb{R}^{B \times C } \), with \(B = \frac{M-k}{\frac{k}{2}}+1 \), \( C \) representing the number of channels after the convolution operator and $\textbf{x}_i$ is the vector representation of $x_{i}(n)$ . The latent representation  \( \textbf{H}_i \) is then passed through the Mamba-based layers $B_i$ to produce the \( i \)-th masking signal \( \textbf{M}_i \) :
\begin{equation}
    \textbf{M}_i = B_i[\textbf{H}_i]
\end{equation}

The MB-blocks estimates \( Q + 1\) masks of the same latent dimension \( \textbf{M}_i \in \mathbb{R}^{B \times C } \). These masks are element-wise multiplied with the encoder outputs \( \textbf{H}_i \) to produce masked hidden representations \( \tilde{\textbf{H}}_i \):
\begin{equation}
    \tilde{\textbf{H}}_i =  \textbf{H}_i  \cdot \textbf{M}_i 
\end{equation}

Then, the masked hidden representations \( \tilde{\textbf{H}}_i \) is concatenated over all frequency bands $i$, such that:
\begin{equation}
    \textbf{H} = concat \left [\tilde{\textbf{H}}_0, ..., \tilde{\textbf{H}}_Q  \right ]
\end{equation}
Where $\textbf{H} \in \mathbb{R}^{(Q + 1) \times B \times C} $. The hidden tensor \( \textbf{H} \) is then processed  with a 2D convolution layer with a kernel size of \( 1 \times 1 \) and one output channel that produces $\textbf{K} \in \mathbb{R}^{B \times C}$. To obtain the vector representation of the canceling signal $\mathbf{y}$, we apply a decoder $D$. Specifically, the decoder is a one-dimensional transpose convolutional layer with a kernel size $k$ and a stride of $k/2$. This decoder ensures that the canceling signal $\textbf{y}$ has the same dimensions as the reference signal $x(n)$:
\begin{equation}
\textbf{y} = D[\textbf{K}],
\end{equation}
where $\textbf{y} = [y(1), \dots, y(M)]$ is the vector representation of the canceling signal $y(n)$, and $M$ is the length of the signal.

\subsection{Optimization Objective}
The training protocol for the proposed method consists of two distinct phases: (i) ANC loss minimization, and (ii) near optimal anti-signal optimization. Each phase employs the NMSE loss function (Eq.~\ref{eq:nmse_loss}) but with different optimization objectives.

\textbf{ANC Loss:} In the first phase, the optimization aims to minimize the residual error signal. Given a reference signal $x(n)$ and the model output $y(n)$, the error loss function is defined as follows:
\begin{equation}
    \mathcal{L}_{\text{ANC}} = \text{NMSE}\left[\textbf{P} \ast \textbf{x},\textbf{S} \ast f_{LS}\{\textbf{y}\}\right]
\label{eq:anc_loss}
\end{equation}
where $\textbf{P}$ and $\textbf{S}$ represent the vectorized forms of the primary-path impulse response $P(z)$ and the secondary-path impulse response $S(z)$, respectively; $\textbf{x}$ and $\textbf{y}$ are the vectorized forms of the reference signal $x(n)$ and the canceling signal $y(n)$. The operator $*$ denotes convolution. Both $\textbf{P}$ and $\textbf{S}$ are obtained from the simulator employed in our study.

\textbf{Near Optimal Anti-Signal Optimization (NOAS):} A key challenge in formulating ANC as a supervised learning problem is defining a training objective that accounts for the characteristics of the secondary path \( S(z) \) and the primary path \( P(z) \). In an ANC algorithms, the output \( y(n) \) is processed by a nonlinearity function \( f_{LS} \) and then propagated through the secondary path \( S(z) \). The training objective aims to minimize the error signal \( e(n) \), which represents the residual noise after cancellation.

However, this process becomes problematic when the secondary path \( S(z) \) attenuates certain frequencies that are present in the primary signal \( d(n) \). 
Under the vanilla loss function (e.g., Eq. \ref{eq:anc_loss}), the model can be unfairly penalized for high error signals in these attenuated frequency bands, even when it has generated an optimal anti-signal. This occurs because the secondary path inherently suppresses these frequencies, leading to residual energy in the error signal \( e(n) \). As a result, the training process encounters discrepancies that hinder the model's ability to learn effectively.

To address this challenge, we propose the NOAS loss function. The NOAS loss symmetrically incorporates the secondary path \( S(z) \) on both sides of the NMSE calculation. By doing so, it ensures that any frequencies nullified by \( S(z) \) are also excluded from the target, thereby mitigating the contribution of these frequencies to the error signal. Specifically, each reference signal $x(n)$ is associated with its NOAS target $y^{*}(n)$. 
To determine the near-optimal anti-signal $y^{*}(n)$, we employ a gradient descent-based algorithm during a pre-processing stage. This stage operates over each example, solving the following optimization problem for each reference signal $x(n)$:
\begin{equation}
    \textbf{y}^{*} = \underset{\tilde{\textbf{y}}}{\arg\min} \,\text{NMSE} 
  \left[ \textbf{P} \ast \textbf{x},\textbf{S} \ast {f_{LS}\{\tilde{\textbf{y}}\}} \right]
\label{eq:noas_optimization}
\end{equation}
where $\textbf{y}^{*}$ is the near-optimal anti-signal. The optimization starts with a random anti-signal and iteratively adjusts it to minimize the NMSE for the given reference signal $x(n)$. The resulting near-optimal anti-signal $y^{*}(n)$ is then used to form the target during the fine-tuning stage. In particular, the near-optimal anti-signal $y^{*}(n)$ is used to define the following loss function:
\begin{equation}
\mathcal{L}_{\text{NOAS}} = \text{NMSE}\left[\textbf{S} \ast f_{LS}\{\textbf{y}^*\},\textbf{S} \ast f_{LS}\{\textbf{y}\}\right]
\label{eq:noas_loss}
\end{equation}
Note that the optimization occurs in the \textbf{S}-projected space, rather than directly in the canceling signal space (i.e. $\text{NMSE}\left[\textbf{y}^*,\textbf{y}\right]$). The \textbf{S} projection ensures consistency and leverages prior knowledge from the initial training phase. Furthermore, it is necessitated by the nature of convolution, which is not a injective operation. As a result, multiple distinct instances, denoted as $\mathbf{y}_1, \dots, \mathbf{y}_n$, may satisfy $
    \mathbf{S} \ast f_{LS}\{\mathbf{y}_1\} = \dots = \mathbf{S} \ast f_{LS}\{\mathbf{y}_n\}
$. Consequently, even if a trained model satisfies $\mathbf{S} \ast f_{LS}\{\mathbf{y}\} \approx \mathbf{P} \ast \mathbf{x}$, it does not necessarily follow that $\mathbf{y} \approx \mathbf{y}^{*}$. This phenomenon is further corroborated by the results in Table~\ref{tab:nmse_distance}, which report NMSE measurements on audio samples from the train set. Prior to NOAS fine-tuning, the NMSE between $\left[\mathbf{y}^{*},\mathbf{y}\right]$ is $-9.85$ dB, while the NMSE between $\left[\mathbf{P} \ast \mathbf{x}, \mathbf{S} \ast \mathbf{y} \right]$ and $\left[\mathbf{S} \ast \mathbf{y}^{*}, \mathbf{S} \ast \mathbf{y} \right]$ are $-16.53$ dB and $-18.53$ dB, respectively.

Notably, the NMSE between $\left[\mathbf{S} \ast \mathbf{y}^{*}, \mathbf{S} \ast \mathbf{y} \right]$ is the lowest. This result aligns with the intuition that $\mathbf{S} \ast \mathbf{y}^{*}$ serves as feasible optimization target for $\mathbf{S} \ast \mathbf{y}$, unlike $\mathbf{P} \ast \mathbf{x}$, which is constrained by frequency limitations, as discussed earlier. A visual intuition for this phenomenon is provided in Figure~\ref{fig:visual_intuition}.

\begin{table}[t]
 \centering
\caption{Comparison of NMSE distances for different objectives, with and without NOAS optimization.}
\vspace{0.1in}
\setlength{\tabcolsep}{4pt}  % Default is 6pt
\begin{tabular}{@{}l*{3}{c}@{}}
\toprule
\textbf{Method} & $\left[\textbf{y}^{*},\textbf{y}\right]$ 
& $\left[\textbf{P} \ast \textbf{x},\textbf{S} \ast \textbf{y} \right]$
& $\left[\textbf{S} \ast \textbf{y}^{*},\textbf{S} \ast \textbf{y} \right]$ \\
\midrule
- NOAS  & -9.85 & -16.53 & -18.56 \\
+ NOAS & -12.77 & -17.60 & -19.62 \\
\bottomrule
\end{tabular}
\label{tab:nmse_distance}
\end{table}


\begin{figure}[t]
\centering
\includegraphics[width=.25\textwidth]{figs/oasprojection.png}
\caption{Schematic representation of signal
    transformations via the secondary path.}
\label{fig:visual_intuition}
\end{figure}


\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/nmse/nmse_compare_engine.png}
        \caption{Engine}
        \label{fig:nmse_engine}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/nmse/nmse_compare_babble.png}
        \caption{Babble}
        \label{fig:nmse_babble}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/nmse/nmse_compare_factory.png}
        \caption{Factory}
        \label{fig:nmse_factory}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/nmse/nmse_compare_speech.png}
        \caption{Speech}
        \label{fig:nmse_speech}
    \end{subfigure}
    \caption{Comparison of NMSE over time for different noise types.}
    \label{fig:noise_comparison}
    % \vspace{-0.5cm}
\end{figure*}

\begin{table*}
\setlength{\tabcolsep}{4pt}  % Default is 6pt
\centering
\caption{Average NMSE for DeepASC and other algorithms across various noise types and nonlinear distortions.}
\vspace{0.1in}
\begin{tabular}{@{}l*{9}{c}@{}}
\toprule
\textbf{Method/Noise type} & \multicolumn{3}{c}{\textbf{Engine} ($\downarrow$)} & \multicolumn{3}{c}{\textbf{Factory} ($\downarrow$)} & \multicolumn{3}{c}{\textbf{Babble} ($\downarrow$)} \\
\cmidrule{1-10}
\bm{$\eta^2$} & \textbf{$\infty$} & \textbf{0.5} & \textbf{0.1} & \bm{$\infty$} & \textbf{0.5} & \textbf{0.1} & \bm{$\infty$} & \textbf{0.5} & \textbf{0.1} \\
\midrule
FxLMS  & -3.38 &  -3.33 & -3.32  & -3.27 &  -3.17 &  -3.11  & -5.39 & -5.33 &  -5.30 \\
THF-FxLMS & - & -3.37 & -3.36 & - & -3.26 & -3.24 & - & -5.39 & -5.36 \\
DeepANC & -13.96 & -13.91 & -13.6 & -10.7 & -10.69 & -10.62 & -12.42 & -12.4 & -12.22 \\
ARN & -14.59 & -14.59 & -14.38 & -11.61 & -11.61 & -11.54 & -12.91 & -12.9 & -12.72 \\
\textbf{DeepASC} & -18.88 &  -18.95 & -18.17 & -16.25 & -16.23 &  -15.94 & -20.17 & -20.03 & -18.48 \\
\bottomrule
\end{tabular}
\label{tab:nmse_noise}
\end{table*}

\section{Experiments and Results}
\textbf{Datasets:} The training data is sourced from the AudioSet dataset \citep{gemmeke2017audio}, which we encompassed 248 distinct audio categories. These categories include various types of ambient sounds such as hubbub, speech noise, and speech babble. The dataset comprises 22,224 audio samples, totaling {18.5} hours of audio content. To maintain consistency with ARN method \citep{zhang2023low}, each audio sample was standardized to a duration of 3 seconds and resampled to a 16kHz. Additionally, 20,000 samples (90\%) of the dataset were allocated for training. The remaining 2,224 samples were reserved for testing. The test sets were obtained from the NOISEX dataset \citep{varga1993assessment}, which includes a wide range of noise types, such as bubble noise, factory noise, and engine noise. Additionally, we utilized the test sets from the following speech datasets: TIMIT \citep{garofolo1993timit}, which contains recordings from 24 speakers representing 8 dialect regions; LibriSpeech \citep{panayotov2015librispeech}, which includes 40 speakers from audiobook readings; and the Wall Street Journal (WSJ) \citep{garofolo1993csr}, which features 8 speakers reading news articles. 

\textbf{Simulator:} Following previous work \citep{zhang2021deep,zhang2023low}, a rectangular enclosure was modeled to represent the physical setup, with dimensions $[3, 4, 2]$ meters (width, length, height). The room impulse response was generated using the method described by \citet{allen1979image}. The locations of the microphones and the cancellation load speaker are as follows: the error microphone is located at $[1.5, 3, 1]$ meters, the reference microphone at $[1.5, 1, 1]$ meters, and the cancellation load speaker at $[1.5, 2.5, 1]$ meters. During the training phase, reverberation times were randomly selected from $\{0.15, 0.175, 0.2, 0.225, 0.25\}$ seconds, while in the test phase a reverberation time of $0.2$ seconds was used. We utilized the rir\_generator package in Python \cite{habets2006room} with the high-pass filter option enabled \citep{allen1979image}. The length of the RIR was set to 512 taps. To model the nonlinearity associated with loudspeaker saturation, researchers in the field of ANC \citep{zhang2021deep,zhang2023low,mostafavi2023deep,cha2023dnoisenet} commonly employ the Scaled Error Function (SEF)\cite{tobias2006lms} $f_{SEF}\{y\} = \int_{0}^{y} e^{-\frac{z^2}{2\eta^2}} dz$, where $y$ represents the input to the loudspeaker, while $\eta^2$ quantifies the intensity of the nonlinearity. This function effectively simulates a typical saturation-type nonlinearity, such as the sound level saturation constrained by the physical dimensions of the loudspeaker.
The SEF exhibits distinct behaviors at extremes of $\eta^2$: as $\eta^2$ approaches infinity, the function converges to linearity, whereas it approximates a hard limiter as $\eta^2$ tends to zero. 

\textbf{Hyperparameters:} An extensive grid search and cross-validation were employed to determine the optimal hyperparameters for each method. The hyperparameter values reported here correspond to the configurations that achieved the best performance in our experimental setup. The DeepASC architecture was trained using multiple numbers of subbands \( Q \), specifically \( Q = 0 \) (a single full band), \(2\) and \(3\). The bands decomposition filters are generated using the $scipy.signal.firwin$ function and applied to the signal via $torch.conv1d$. The temporal duration \( M \) was set to 48,000 samples, corresponding to 3-second audio signals sampled at 16~kHz. The channel dimension \( C \) was set to 256, and the kernel size \( W \) was defined as 16. A batch size of 2 was used for training the DeepASC architecture. The Adam optimizer~\citep{diederik2014adam} was employed with an initial learning rate of \( 1.5 \times 10^{-4} \). A learning rate decay factor of 0.5 was applied every 2 epochs after an initial warm-up period of 30 epochs. Gradient clipping with a threshold of 5 was applied to prevent exploding gradients.

\textbf{Baseline Methods:} We compared our proposed method with several established ANC techniques, including Deep ANC \citep{zhang2021deep}, Attentive Recurrent Network (ARN) \citep{zhang2023low}, Filtered-x Least Mean Squares (FxLMS), and Tangent Hyperbolic Function FxLMS (THF-FxLMS), \citep{ghasemi2016nonlinear}. All methods were evaluated in both linear and nonlinear simulations, considering both noise and speech signals. FxLMS, Deep ANC, and ARN were implemented and trained by us. All methods were evaluated under identical simulation conditions. For the learned methods, namely Deep ANC and ARN, the same training dataset used for our proposed method was applied, and we ensured the reproduction of results consistent with those reported in the respective papers. In our Deep ANC implementation, we employed 20-ms short-time Fourier transform (STFT) frames with a 10-ms overlap between consecutive frames. For ARN, we utilized 16-ms frames with an 8-ms overlap. These baseline methods were selected to provide a comprehensive comparison across various ANC paradigms, encompassing both traditional adaptive filtering techniques and more recent deep learning approaches.

\subsection{Noise Cancellation}
Table~{\ref{tab:nmse_noise}} presents the NMSE for ANC algorithms across three noise types—engine, factory, and babble—using 3-second signal segments extracted from the NOISEX-92 dataset. For each noise type, the models were evaluated both without nonlinear distortions (where $\eta^2 = \infty$) and with nonlinear distortions at $\eta^2 = 0.1$ and $\eta^2 = 0.5$. In the case of non-deep learning-based methods, namely FxLMS and THF-FxLMS, gradient clipping at $1e-4$ was applied due to the sensitivity of these algorithms to the step size, which caused instability during validation. The step sizes for these methods were set to 0.05 for engine noise, 0.4 for factory noise, and 0.3 for babble noise. The results indicate that these algorithms perform suboptimally compared to deep learning-based approaches. 

Among the deep learning-based methods, and without considering the nonlinearity saturation effect, the proposed DeepASC method achieves state-of-the-art results. Specifically, for the case where $\eta^2=\infty$ it improves performance over the ARN method by 4.29~dB, 4.64~dB, and 7.26~dB for engine, factory, and babble noise, respectively. In the presence of nonlinear distortions ($\eta^2 = 0.5$), DeepASC continues to outperform ARN, with improvements of 4.36~dB, 4.62~dB, and 7.13~dB for engine, factory, and babble noise, respectively. For more severe nonlinearity ($\eta^2 = 0.1$), DeepASC still surpasses ARN with gains of 3.79~dB, 4.4~dB, and 5.76~dB. Figures~\ref{fig:nmse_engine}, \ref{fig:nmse_babble}, and \ref{fig:nmse_factory} offer visual comparisons of the different methods by plotting NMSE over time. These figures illustrate that the proposed DeepASC method consistently achieves superior NMSE performance compared to ARN, DeepANC, and FxLMS across almost every timestep. 

The proposed method was also evaluated for speech enhancement in the presence of noise using active noise cancellation. The PESQ and STOI metrics, presented in Table \ref{tab:pesq_stoi}, compare the performance of DeepANC, ARN, and DeepASC (w/o NOAS) across various SNR levels in the presence of factory noise with nonlinear distortion of $\eta^2 = \infty$. The results demonstrate that DeepASC outperforms ARN, showing improvements in PESQ scores by $0.7$, $0.92$, and $0.84$ at SNR levels of 5dB, 15dB, and 20dB, respectively. A similar trend is observed for STOI, with enhancements of $0.08$, $0.03$, and $0.02$ for the same SNR levels.
\begin{table*}
\caption{Average NMSE for DeepASC and other algorithms across various speech datasets and nonlinear distortions.}
\vspace{0.1in}
\setlength{\tabcolsep}{4pt}  % Default is 6pt
\centering
\begin{tabular}{@{}l*{9}{c}@{}}
\toprule
\textbf{Method/Dataset}  & \multicolumn{3}{c}{\textbf{TIMIT} ($\downarrow$)} & \multicolumn{3}{c}{\textbf{LibriSpeech} ($\downarrow$)} & \multicolumn{3}{c}{\textbf{WSJ} ($\downarrow$)} \\
\cmidrule{1-10}
\bm{$\eta^2$} & \bm{$\infty$} & \textbf{0.5} & \textbf{0.1} & \bm{$\infty$} & \textbf{0.5} & \textbf{0.1} & \bm{$\infty$} & \textbf{0.5} & \textbf{0.1} \\
\midrule
FxLMS  & -1.39 & -1.36 & -1.26 & -3.43 & -3.40 & -3.28 & -1.92 & -1.90 & -1.85 \\
THF-FxLMS & - & -1.37 & -1.35 & - & -3.41 & -3.39 & - & -1.91 & -1.89 \\
DeepANC & -8.52 & -8.56 & -8.48 & -11.92 & -11.81 & -11.08 & -7.54 &  -7.55 &  -7.51 \\
ARN & -10.31 & -10.27 & -10.2 & -12.87 & -12.74 & -11.87 &  -9.48 &  -9.48 &  -9.42  \\
\textbf{DeepASC} & -16.44 & -16.45 & -16.17 & -17.65 & -17.08 & -14.33 & -15.43 &  -15.47 &  -15.23 \\
\bottomrule
\end{tabular}
\label{tab:nmse_speech}
% \vspace{-0.5cm}
\end{table*}
\begin{table*}
\caption{Average NMSE, STOI \& PESQ for ASC models in noisy speech with nonlinearity ($\eta^2=0.5$) and factory noise at different SNR.}
\vspace{0.1in}
\setlength{\tabcolsep}{4pt}  % Default is 6pt
\centering
\begin{tabular}{@{}l*{8}{c}@{}}
\toprule
\textbf{Method} & \textbf{Noise only} & \multicolumn{2}{c}{\textbf{SNR = 5dB}} & \multicolumn{2}{c}{\textbf{SNR = 15dB}} & \multicolumn{2}{c}{\textbf{SNR = 20dB}} \\
\cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(l){7-8}
& \textbf{NMSE} ($\downarrow$) & \textbf{STOI} ($\uparrow$) & \textbf{PESQ} ($\uparrow$) & \textbf{STOI} ($\uparrow$) & \textbf{PESQ} ($\uparrow$) & \textbf{STOI} ($\uparrow$) & \textbf{PESQ} ($\uparrow$) \\
\midrule
DeepANC & -10.69 & 0.83 & 1.39 & 0.93 & 2.10 & 0.96 & 2.45 \\
ARN & -11.61 & 0.84 & 1.51 & 0.94 & 2.43 & 0.96 & 2.92 \\
\textbf{DeepASC} & -15.94 & 0.92 & 2.21 & 0.97 & 3.35 & 0.98 & 3.76 \\
\bottomrule
\end{tabular}
\label{tab:pesq_stoi}
% \vspace{-0.5cm}
\end{table*}

\subsection{Speech Cancellation}
Table~\ref{tab:nmse_speech} presents the average NMSE values for different ANC algorithms across three speech datasets: TIMIT, LibriSpeech, and WSJ, with speech segments affected by varying levels of nonlinear distortions. It is evident that speech cancellation is a more challenging task compared to noise cancellation, as reflected in the performance degradation of the different algorithms. 

As observed in the noise cancellation case, in speech cancellation, the non-deep learning methods—FxLMS and THF-FxLMS—demonstrate suboptimal performance compared to deep learning-based approaches. Among the deep learning methods, DeepASC achieves the best overall results, surpassing the other algorithms significantly. 

In the case without nonlinear distortions ($\eta^2 = \infty$), DeepASC shows improvements over ARN by 6.13~dB, 4.78~dB, and 5.95~dB for the TIMIT, LibriSpeech, and WSJ datasets, respectively. In the presence of moderate nonlinear distortions ($\eta^2 = 0.5$), DeepASC continues to outperform ARN, with improvements of 6.18~dB for TIMIT, 4.34~dB for LibriSpeech, and 5.99~dB for WSJ. Under more severe nonlinear distortions ($\eta^2 = 0.1$), DeepASC maintains its superior performance, with enhancements of $5.97$dB, $2.46$dB, and $5.81$dB for TIMIT, LibriSpeech, and WSJ datasets, respectively. Figure~\ref{fig:speech_specra} illustrates the performance of various ANC methods on a speech signal, comparing power spectra and spectrograms. DeepASC demonstrates superior noise suppression across all frequencies, including high frequencies, outperforming other methods such as DeepANC and ARN, which struggle more with high-frequency noise. 
Figure~\ref{fig:nmse_speech} further demonstrates DeepASC's superior NMSE performance for speech signals, exceeding ARN, DeepANC, and FxLMS at nearly every time step.

\subsection{Real-World Simulation}

We expanded our investigation to assess the performance of our method in real-world settings, testing it across various simulation scenarios. This was necessary because the fixed task acoustic setup, which relies on the image method, has limitations regarding generalizability and real-world performance. We utilized the dataset from \cite{Liebich2019AcousticPD}, which includes acoustic paths from 23 individuals, measured in the real world and encompassing both primary and secondary paths. We applied DeepASC, along with baseline approaches, to the updated simulation conditions, evaluating their performance using Factory and Babble noise from the NoiseX-92 dataset and speech samples from the WSJ dataset. The results in Table~\ref{tab:bose} present the average NMSE across these categories. The results demonstrate that DeepASC consistently outperforms the alternative methods, achieving improvements of $2.80$dB in the Factory noise, $2.70$dB in the Babble noise, and $1.53$dB on the WSJ dataset.



\begin{table}[ht]
\centering
\caption{Average NMSE ($\downarrow$) for ANC methods on noise and speech, evaluated on real-world measured \(\textbf{P}\) \& \(\textbf{S}\) with \(\eta^2 = 0.5\).}
\vspace{0.1in}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}l*{3}{c}@{}}
\toprule
\textbf{Method/Dataset} & \textbf{Factory ($\downarrow$)} & \textbf{Babble ($\downarrow$)} & \textbf{WSJ ($\downarrow$)} \\
\midrule
DeepANC  & -9.29  & -10.94 & -8.26  \\
ARN      & -8.97  & -11.17 & -10.70 \\
\textbf{DeepASC} & -12.09 & -13.87 & -12.23 \\
\bottomrule
\end{tabular}
\label{tab:bose}
\end{table}
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.21\textwidth}
        \includegraphics[width=\textwidth]{figs/specs/00da010c_4algos_column_Original_Signal.png}
        \caption{No ANC}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}[b]{0.21\textwidth}
        \includegraphics[width=\textwidth]{figs/specs/00da010c_4algos_column_DeepANC.png}
        \caption{DeepANC}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}[b]{0.21\textwidth}
        \includegraphics[width=\textwidth]{figs/specs/00da010c_4algos_column_ARN.png}
        \caption{ARN}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}[b]{0.21\textwidth}
        \includegraphics[width=\textwidth]{figs/specs/00da010c_4algos_column_Ours.png}
        \caption{DeepASC}
    \end{subfigure}
    \caption{Spectrograms and Power Spectra of Speech Signal (00da010c, WSJ) using Different ANC methods w/o nonlinearity ($\eta^2=\infty$)}
    \label{fig:speech_specra}
    % \vspace{-0.5cm}
\end{figure*}
\begin{table*}[]
\caption{Average NMSE ($\downarrow$) of our method (\textbf{w/o NOAS}) for Noise and Speech using different number of bands, with $\eta^2=0.5$.}
\vspace{0.1in}
\centering
\begin{tabular}{lccccc}
\toprule
 \textbf{Method/Dataset} & \textbf{\#Bands} & \textbf{Factory} ($\downarrow$) & \textbf{TIMIT} ($\downarrow$) & \textbf{LibriSpeech} ($\downarrow$) & \textbf{WSJ} ($\downarrow$) \\ 
\midrule
DeepASC (small) &  1 & -13.46  & -14.26 & -14.88 & -13.22 \\ 
DeepASC (medium) & 1 & -15.19 & -15.82 & -16.56 & -14.86 \\ 
DeepASC & 3 & -15.94 & -16.36 & -16.95 & -15.32 \\ 
DeepASC & 4 & -16.52 & -16.55 & -17.41 & -15.84 \\ 
\bottomrule 
\end{tabular}
\label{table:nmse_bands}
\vspace{-0.5cm}
\end{table*}

\subsection{Model Analysis}
The number of frequency bands in the DeepASC architecture is a critical hyperparameter affecting performance. Table \ref{table:nmse_bands} compares DeepASC's performance across different band configurations for the Factory noise, TIMIT, LibriSpeech, and WSJ
datasets, with $\eta^2 = 0.5$. The "1-band" models use a single full band, while the "3-band" and "4-band" models incorporate one medium band with two and three smaller sub-bands, respectively. A 2-band model, which would require two full bands, was excluded as it falls outside the intended design of DeepASC.

As shown in Table \ref{table:nmse_bands}, increasing the number of bands improves model performance. For example, the 4-band configuration outperforms the 3-band variation by $0.58$ dB, $0.19$ dB, $0.37$ dB, and $0.48$ dB on the Factory noise, TIMIT, LibriSpeech, and WSJ datasets, respectively. This enhancement comes from the model's improved focus on sub-frequency bands, benefiting higher frequencies.

Table \ref{tab:deep_aac_params} compares model size and performance, with NMSE evaluated on factory noise under nonlinear distortion of $\eta=0.5$. DeepASC variants in this comparison are without NOAS optimization. The results indicate that even the smallest DeepASC configuration (1-band, small) outperforms the ARN architecture by $1.85$ dB, despite using only half the parameters ($8.0$M vs. $15.9$M). This is a significant outcome given the critical importance of model size in real-time ANC applications where latency is critical.

The computational complexity of the models was assessed by comparing their FLOPs, averaged across 20 three-second samples from the Noisex-92 dataset, as presented in Table~\ref{tab:flops}. The single-band, small variant of DeepASC demonstrated exceptional efficiency, requiring only 2.862G FLOPs while consistently surpassing the performance of the other models. This highlights its superior balance between computational cost and effectiveness. 
For further analysis, see Appendix \ref{apds:ablation}, which presents the Ablation Study.

\begin{table}[t]
\centering
\caption{Comparison of ANC methods based on parameter size.}
\vspace{0.1in}
\setlength{\tabcolsep}{4pt}  % Default is 6pt
\begin{tabular}{@{}l*{2}{c}@{}}
\toprule
\textbf{Models} &  \textbf{\#Params} & \textbf{NMSE} ($\downarrow$)\\
\midrule
DeepANC & 8.8M  & -10.69\\
ARN & 15.9M &  -11.61 \\
\midrule
DeepASC, 1 Band, S  & 8.0M & -13.46 \\
DeepASC, 1 Band, M  & 15.8M & -15.19 \\
DeepASC, 3 Bands & 31.9M & -15.94 \\
DeepASC, 4 Bands & 40.0M & -16.52\\
\bottomrule
\end{tabular}
% \vspace{-20pt}
\label{tab:deep_aac_params}
\end{table}

% First Table
\begin{table}[ht]
\centering
\caption{FLOPs \& NMSE comparison for different ANC methods.}
\vspace{0.1in}
\setlength{\tabcolsep}{4pt}  % Default is 6pt
\begin{tabular}{@{}lcc@{}}
\toprule
{\textbf{Method}} & {\textbf{FLOPs (G) ($\downarrow$)}} & {\textbf{NMSE ($\downarrow$)}} \\
\midrule
{DeepANC} & {7.199} & {-10.69} \\
{ARN}     & {5.281} & {-11.61} \\
\textbf{DeepASC}    & {2.419} & {-13.46} \\
\bottomrule
\end{tabular}
\label{tab:flops}
\end{table}

\section{Conclusion}
In this paper, we introduced a novel ASC approach using the Multi-Band Mamba architecture. By partitioning audio into frequency bands, our method enhances anti-signal generation and phase alignment. Combined with an optimization-driven loss function, it achieves near-optimal performance, improving both ANC and ASC outcomes. Our experimental results demonstrate a significant performance boost compared to state-of-the-art baselines, with improvements of 7.2dB in ANC and 6.2dB in ASC for voice audio signals. These results confirm the multi-band architecture's effectiveness in handling diverse frequencies and real-world acoustic environments, where traditional methods often fail. Our approach addresses key challenges in the field by effectively leveraging frequency decomposition and optimization-based anti-signal generation, paving the way for more advanced audio cancellation technologies.


\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
% \clearpage
\bibliography{main-yehuda}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\clearpage

\section{Overview of an ANC System}
\label{apdx:anc}
To elucidate the acoustic dynamics of ANC systems in in-ear headphones, Figure~\ref{figs:inear_oas} presents a schematic representation of the Primary and Secondary acoustic paths.

The primary path \( P(z) \) characterizes the transfer function between the external noise, as captured by the reference microphone, and the error microphone. This path models the propagation of ambient noise through the system. Conversely, the secondary path \( S(z) \) represents the transfer function from the loudspeaker to the error microphone, encompassing the acoustic feedback loop within the ear canal.
The schematic highlights the interaction between critical system components, including the processing unit, reference microphone, error microphone, and loudspeaker.


\section{Ablation Study}
\label{apds:ablation}
To assess the contributions of the key components in the DeepASC architecture, we conducted an
ablation study focusing on multiband processing, band size (small vs. medium), and the impact of
NOAS optimization. Table \ref{table:nmse_ablation} presents the results of this analysis, reporting the NMSE across four datasets: Factory, TIMIT, LibriSpeech, and WSJ, all evaluated under nonlinear distortion conditions ($\eta^2$ = 0.5). 

\begin{figure}
% \hspace{0.05\textwidth} % Adjusts the space between the figures
\begin{minipage}{.45\textwidth}
\centering
\includegraphics[width=\textwidth, height=0.5\textwidth]{figs/anc_paths_inear.png}
\caption{Schematic representation of the acoustic paths in an in-ear ANC system.}
\label{figs:inear_oas}
\end{minipage}
\end{figure}

\begin{table*}[t]
\centering
\caption{Average NMSE ($\downarrow$) in dB for noise and speech using multiple variants of DeepASC, with nonlinear distortion of $\eta=0.5$.}
\label{table:nmse_ablation}
\vspace{0.1in}
\begin{tabular}{lcccc}
\toprule
\textbf{Method/Dataset} & \textbf{Factory} ($\downarrow$) & \textbf{TIMIT} ($\downarrow$) & \textbf{LibriSpeech} ($\downarrow$) & \textbf{WSJ} ($\downarrow$) \\ 
\midrule
$\:$ + S - MultiBand - NOAS & -13.46 & -14.26 & -14.88 & -13.20 \\ 
$\:$ + S - MultiBand + NOAS & -14.19 & -14.54 & -15.24 & -13.55 \\
$\:$ + M - MultiBand - NOAS & -15.19 & -15.82 & -16.56 & -14.86 \\ 
$\:$ + M - MultiBand + NOAS & -16.09 & -16.25 & -16.92 & -15.27 \\
$\:$\textbf{Full Method} & -16.23 & -16.45 & -17.08 & -15.47 \\ 

\bottomrule 
\end{tabular}
\end{table*}

In our notation, "+ S - Multiband - NOAS" refers to a small band configuration (8 mamba layers) without multiband processing or NOAS optimization, while "+ S - Multiband + NOAS" refers to the same small band architecture with NOAS optimization applied. Similarly, "+ M - Multiband - NOAS" represents a medium band configuration (16 mamba layers) without NOAS, and "+ M - Multiband + NOAS" applies NOAS optimization to the same medium band model. The \textbf{Full Method} is defined as a configuration that employs one full medium band and two small sub-bands, with NOAS optimization applied. 

All models were initially trained using the ANC loss function defined in Eq.~\ref{eq:anc_loss}. Configurations with "+ NOAS" were fine-tuned using NOAS optimization, whereas configurations with "- NOAS" were trained exclusively using the ANC loss in Eq.~\ref{eq:anc_loss}. The results demonstrate that the removal of NOAS optimization consistently degrades performance across all datasets. For instance, on the Factory dataset, applying NOAS optimization to the small band model leads to a performance improvement of $0.73$dB, while the medium band model shows a larger improvement of $0.90$dB. This trend holds across the other datasets, reinforcing the crucial role of NOAS optimization in enhancing model performance. Multiband processing further improves the overall effectiveness of DeepASC. For example, the \textbf{Full Method} consistently outperforms the "+ M - Multiband + NOAS" configuration, with gains of $0.14$dB, $0.2$dB, $0.16$dB, and $0.2$dB on the Factory, TIMIT, LibriSpeech, and WSJ datasets, respectively. Interestingly, the performance of the "+ S - Multiband - NOAS" configuration is consistently lower than that of the "+ M - Multiband - NOAS" variant across all datasets. Specifically, the small band model underperforms by $1.73$dB on Factory, $1.56$dB on TIMIT, $1.68$dB on LibriSpeech, and $1.66$dB on WSJ. This indicates that while multiband processing is valuable, the choice of band size plays a significant role in the model’s performance, with larger band sizes, particularly when combined with NOAS, yielding the best results. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


