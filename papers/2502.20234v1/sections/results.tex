\begin{table*}[t]
	\renewcommand{\arraystretch}{1.1}
	\footnotesize
	\centering
	\caption{\textbf{High-level performance.} \dotuline{Underlined values} are statistically significant compared to the control group, and \textbf{bold values} to the baselines. Conducted statistical analyses are Kruskal-Wallis test and post-hoc Dunn's test.}
	\label{tab:performance}
	\rowcolors{2}{white}{gray!10}
	\begin{tabularx}{\linewidth}{@{}ll|XXX|XXX|XXXX@{}}
		\toprule
		\textbf{Group} & \textbf{Size} & \multicolumn{6}{c|}{\textbf{Legitimate Emails} {\scriptsize \textit{(visiting is correct)}}} & \multicolumn{3}{c}{\textbf{Phishing Emails} {\scriptsize \textit{(reporting is correct)}}} \\
               &             & \multicolumn{3}{c|}{\textbf{Text-only}} & \multicolumn{3}{c|}{\textbf{With Link}} & Tot. & Visit & Report & Report \\
               &             & Tot. & Visit & Report & Tot. & Visit & Report & & & (Task) & (Mailbox) \\ \midrule

		\input tables/high_level.tex
		\bottomrule
	\end{tabularx}
\end{table*}

\paragraph{\new{Methods}}
\new{Statistical significance was assessed everywhere by selecting a suitable statistical test according to normality criteria (Kruskal-Wallis unless otherwise specified). Differences were confirmed with a post-hoc Dunn's test. Every difference highlighted in the tables and text is significant with $p<0.05$.}

\subsection{Performance}

We first analyze the participants' performance in correctly identifying the study emails based on their belonging group. 
We report the results on the different legitimate and phishing emails divided per group in Table~\ref{tab:performance}.


\paragraph{Phishing detection}
We observe that all mechanisms outperform the control group in reporting and not falling for phishing emails: while the control group participants fell for 74\% of them, our tested baselines reduce this number to 57\% and 61\%, respectively.
However, our inspection tasks outperform all the other approaches by further reducing this number to 35\%, a difference that is statistically significant from all other groups, according to a Kruskal-Wallis test and a post-hoc Dunn's test.


\paragraph{False positives}
We also analyze the performance of participants in managing legitimate emails correctly.
We can see that, as expected, the performance on emails without links is similar across all groups and is very high, ranging from 95.0\% to 98.2\%.
For emails with links, the performance is slightly different across groups.
While the two baseline groups perform very similar to the control group (87\% to 89\%), participants receiving our tasks perform 5\%-7\% worse, as they become overly suspicious of some legitimate URLs and report slightly more legitimate emails.
\new{However, this difference is not statistically significant; some increase can be attributed to the roleplay scenario and to the heightened alertness after encountering phishing URLs for participants not in control.}


\begin{table}[t]
	\renewcommand{\arraystretch}{1.2}
	\footnotesize
	\centering
	\caption{\textbf{Phishing rates for different URL types:} \textsc{sub} with impersonation  in subdomain; \textsc{first} at the beginning of the domain; \textsc{last} at the end; \textsc{path} in the path; \textsc{squat} typosquats.
	\dotuline{Underlined values} are statistically significant compared to the control group, \textbf{bold} to either baseline, \textbf{*} to all baselines, according to a Kruskal-Wallis and post-hoc Dunn's test.}
	\label{tab:performance_per_url_type}
	\rowcolors{2}{white}{gray!10}
	\begin{tabularx}{\linewidth}{@{}X|XXXXX@{}}
		\toprule
		 & \textsc{sub} & \textsc{first} & \textsc{last} & \textsc{path} & \textsc{squat} \\ \midrule

		\input tables/performance_per_url_type_compact.tex
		\bottomrule
	\end{tabularx}
\end{table}


\subsection{Different URL Types}

We further analyze the participants' performance on different types of phishing URLs.
The results are reported in Table~\ref{tab:performance_per_url_type}.

We observe that the baseline approaches only show slight improvements over the control group, with the passive task showing statistically significant differences only for impersonations at the beginning of the domain and typosquats and the active one only for subdomain impersonation and typosquats.
Meanwhile, the proposed tasks show a statistically significant improvement over the control group for every type of task and phishing URL.
We can also see that all proposed tasks present statistically significant improvements over the baselines for all types of phishing URLs, except highlighting typosquat URLs which showed a minor 11\% improvement over the baseline.

All of the proposed tasks are highly effective, presenting improvements over even the best performing baseline with 10\% to 40\% more reported emails and less success of phishing URLs.
In particular, the best performing tasks for each type of phishing URL provide significant improvements: for impersonations in subdomains, 26\% fewer phishes are successful; for the beginning of the domain, 15\% less; for the end of the domain, 17\% less; for the path, 19\% less; and for typosquats, 40\% less.
Especially notable is the improvement for typosquats, which were among the hardest to detect for participants in the control and baseline groups, but our mechanism achieves a three-fold improvement.

These results empirically confirm the discussion of the different effectiveness of tasks for various types of URLs that we presented in Section~\ref{sec:design.tasks}.
In particular, highlighting helps less for typosquats as it is limited to slowing down the user and attracting attention to the single characters.
Instead, clicking on the intended domain from a list proves highly effective as it allows users to clearly realize if their intention mismatches the URL, by picking the desired (correct) one from the list.
Finally, as expected, typing was highly effective against typosquats, as the user is unlikely to retype the URL correctly (i.e., with the wrong character) and thus will be notified of the mistake.


\begin{table}[]
	\renewcommand{\arraystretch}{1.2}
	\footnotesize
	\centering
	\caption{\textbf{Typosquat URLs:} phishing rates per group.}
	\label{tab:performance_per_typosquat}
	\rowcolors{2}{white}{gray!10}
	\begin{tabularx}{\linewidth}{@{}l|XXX|XX@{}}
		\toprule
		& \textbf{Control} & \textbf{Passive} & \textbf{Active} & \textbf{Highlight} & \textbf{Type} \\ \midrule

		\input tables/typosquat_details.tex
		\bottomrule
	\end{tabularx}
\end{table}


\subsubsection{Typosquat URLs}
We further turn our attention to typosquat URLs, as it is interesting to compare the performance of all tasks on different types of typosquats: character addition, deletion, substitution, and transposition.
We report all typosquat URLs and the phishing rates for each group and task in Table~\ref{tab:performance_per_typosquat}.

We observe that typosquats are especially difficult for participants in the control group, who can only rely on the small tooltip offered by browsers when hovering over the link:
60\% fell for the PayPal typosquat, and a staggering 90\% for the Microsoft and Google ones.
This is understandable: character swaps are hard to detect because we tend to reorder them in our mind while substituting an \texttt{l} with an \texttt{i} can even be confused with a speckle of dust on one's screen.

Meanwhile, even the baselines show improvements over the control group; showing the URL with a bigger, monospaced font helped participants---with one notable difference: FedEx, where our baselines performed as poorly as the control group.
We attribute this to the fact that the FedEx typosquat requires knowledge of the correct domain due to simply adding a dash, while all other typosquats presented misspelling errors.
Yet, even for this difficult URL, the proposed tasks still show a marked improvement, reducing the falling rates from 65\% down to 14\%.
For all other URLs, all baseline approaches are shown to help participants.
This trend is also seen in the proposed tasks, where we observe the numbers improving, reducing the falling rates five to tenfold compared to the control group, especially on the more difficult Microsoft and Google typosquats, where our typing task decreased the falling rates from 88\% and 91\% to 13\% and 10\%, respectively.

{
\begin{table*}[t]
	\renewcommand{\arraystretch}{1.2}
	\footnotesize
	\centering
	\caption{\textbf{Participant actions per phase:} while solving the task and after solving it incorrectly.}
	\label{tab:mechanism_perf_per_phase}
	\rowcolors{2}{white}{gray!10}
	\begin{tabularx}{\linewidth}{@{}ll|llXXX|llXX@{}}
		\toprule
		&  & \multicolumn{5}{c}{\textbf{Task Solving}} & \multicolumn{4}{c}{\textbf{Mistake Page}} \\
		&  & Emails & Solved & Wrong & Report & Back~(report) & Emails & Confirm & Report & Back (report) \\ \midrule

		\input tables/mechanism_perf_per_phase_compact.tex
		\bottomrule
\end{tabularx}
\end{table*}
}

\subsection{Which Components Are Beneficial?}

The next question we address is which components are most beneficial to participants.
Recall that our mechanism allows different interactions: when presented with a URL and task, a user can choose to solve it, report it and its email, or return to the email to inspect it again.
Users who choose to solve the challenge might do it incorrectly, and are presented a failure page displaying the URL and their solution, and allowing them to proceed anyway, report, or go back to their mailbox.
We postulate that all these interactions can contribute differently to the participants' performance, and thus raise the following question: how do each of these components contribute to our mechanism's effectiveness?
To do so, we report a breakdown of all interactions divided into legitimate and phishing URLs in Table~\ref{tab:mechanism_perf_per_phase}.

We observe that for legitimate URLs, users generally solve our tasks correctly (55\% to 68\%), and overwhelmingly (87\% to 90\%) confirm that they still want to proceed when they solved them incorrectly.
This is not the case for phishing URLs: while a minority of users (16\% to 25\%) solve the tasks correctly, most (28\% to 43\%) report the email---comparably or better than all the baselines. 
As there are more reports than in the passive and active baselines, the task design itself is effective in helping participants.

For phishing URLs, the remaining (31\% to 37\%) tasks are solved incorrectly, triggering communication of the mistake: on this second phase, roughly half of the participants' decisions are the ``correct'' behavior (reporting or going back), thus showing that triggering the mistake helps participants even further.
However, we observed that only a minority of the participants who went back and inspected the email again ended up reporting it from the mailbox; most of them proceeded to go through the task again.

We summarize the analysis by claiming that the components of the proposed tasks are effective in helping participants at all stages: while re-reading the URL, while solving the task, and after being communicated their mistake when solving the challenge.


% two subfigures
\begin{figure}[t]
	\centering
	\begin{subfigure}{\columnwidth}
		\centering
		\includegraphics[width=.8\columnwidth]{figures/mistakes_legitimate.pdf}
		\caption{Legitimate URLs.}
		\label{fig:mistakes_types.legitimate}
	\end{subfigure}
	\begin{subfigure}{\columnwidth}
		\centering
		\includegraphics[width=.8\columnwidth]{figures/mistakes_phishing.pdf}
		\caption{Phishing URLs.}
		\label{fig:mistakes_types.phishing}
	\end{subfigure}
	\caption{\textbf{Types of mistakes} made while solving the tasks.}
	\label{fig:mistake_types}
\end{figure}

\subsubsection{Types of Mistakes}

The final question related to the usefulness of our mechanism is about the mistakes participants make when solving the tasks.
We are interested in observing what mistakes are more common for legitimate and phishing URLs---the former to analyze how the participants misunderstand URLs and the tasks; the latter to understand whether the tasks can trigger mistakes related to mismatches of the participants' intentions with the URLs.
We manually go through all the mistakes and classify them into different types, reported in Figure~\ref{fig:mistake_types}.

Figure~\ref{fig:mistakes_types.legitimate} shows the types of mistakes on legitimate URLs: we observe that the most common mistake is to highlight or retype the full URL or parts of it instead of the domain, showing that participants struggled with the concept of domains (despite our interface included a button that explains just that, located next to the tasks).
A minority of participants also made minor mistakes in the solution, e.g., mistyping a character, missing a dot, or highlighting a few extra characters.

Figure~\ref{fig:mistakes_types.phishing} shows which mistakes were made on phishing URLs.
We observe that most of the mistakes users made relate to confusion due to the phishing URLs: the most common mistake (45\% to 50\%) is highlighting or typing the domain of the impersonated brand instead of the domain, e.g., \texttt{paypal.com} instead of \texttt{com-login.com} for the phishing URL \texttt{paypal.com-login.com}.
This highlights how the proposed tasks can help by spelling out the mismatch between the participant's intentions and the URL they are about to visit.
The other main source of mistakes (more than 30\% for typing) is not noticing typosquats and typing the URL correctly instead.
The least desirable mistake, i.e., highlighting or typing the full URL which would lead participants to the phishing website, only happened 33\% of the time for highlighting and 15\% for typing, proving that the main mistakes that our tasks triggered were \textit{helpful} ones, potentially leading participants to notice the scams.


\begin{figure}[t]
	\centering
	\includegraphics[width=.75\columnwidth]{figures/solving_times.pdf}
	\caption{\textbf{Solving time per task} by email type.}
	\label{fig:solving_time}
\end{figure}

\subsection{Solving Time}

We analyze the solving times of the participants in solving the tasks, reported in Figure~\ref{fig:solving_time}.
While the passive baseline was solved quickly (median time: 3 sec), the active baseline and our tasks took longer: the fastest tasks to solve were the active one and highlighting (median time: 6-7 sec), while the typing task took the longest (median time: 10 sec).
However, all tasks exhibit large variances in solving times, similar to what is observed for CAPTCHAs~\cite{searles2023empirical}.
Moreover, we observe that for all tasks, including the baselines, there are almost no differences in the median solving time for legitimate and phishing URLs.
%
We further analyze the solving times per demographic to see whether any of the recorded participants' attributes have any influence.
We observed that age impacts the passive, active, highlight, and typing solving times; education the passive, active, clicking, and typing times; and technology use the passive and clicking times.
These differences are statistically significant but not very large---we report them in Appendix~\ref{sec:appendix.demographics_time}.


\subsection{Effects of Demographics}
\label{sec:results.demographics}

We analyze the effect of participants' demographics on their accuracy in managing both legitimate and phishing emails for all groups.
For this, we consider the participants' age, education, and reported technology use in their personal lives and work (aggregated by summing the answers for different devices and divided into low, medium, and high).

We observed only minor differences overall.
For the control group, no demographic attribute significantly affected either accuracy.
The same holds for our active baseline.
For the passive baseline, age and education affected phishing email accuracy only, with older participants performing worse and participants with higher education performing better.
For our tasks, we observed a similar statistically significant effect of age on legitimate email accuracy and the frequency of use of technology in private life on phishing email accuracy.
Curiously, the difference is not between the most and least frequent users but between the most and average frequent ones, with the latter performing worse---having observed no difference with the lowest use group suggests that potentially, less confident participants were more alert.
We report the detailed results in Appendix~\ref{sec:appendix.demographics_accuracy}.



\subsection{User Perception}
We now analyze our participants' perceptions of our mechanism.
We administered a post-study questionnaire asking participants about their experience with the study with Likert-scale questions (reported in Appendix~\ref{sec:appendix.post_questionnaire}).
We report the distribution of users' answers to the high-level questions related to our mechanism in Figure~\ref{fig:postq_mechanism_general}: we observe that most participants found our challenges helpful (Q1) and useful (Q2) in spotting phishing URLs.
Further appreciated was the presentation of the mechanism, which was found clear (Q9), with appreciated features such as coloring the URL (Q7), that our task made simpler to read and understand (Q8).
The tutorial (Q5, Q6) was well received, giving us confidence that the participants understood how to solve our tasks for the study.
The response is more mixed for clarity in highlighting mistakes (Q3), indicating potential for improvement.
Finally, while the mechanism did not feel obtrusive (Q4), we have to note that participants only solved our tasks a handful of times in a short time span, with the goal of getting a study reward.

Finally, we analyze the participants' answers to task-specific questions: whether participants found them (i) useful, (ii) annoying, and (iii) difficult.
Here, the vast majority of participants found all our tasks useful and not difficult; however, the typing task was found more annoying than the other two, as we report in Figure~\ref{fig:post_mechanisms_q1}.
This result is similar to observations made on CAPTCHAs, where the ones who require more effort are also the most disliked~\cite{searles2023empirical}, suggesting trade-offs between task efficacy and user experience.

\begin{figure*}[t]
	\centering
	\begin{subfigure}[c]{0.65\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/postq_mechanisms_general.pdf}
		\caption{Common questions.}
		\label{fig:postq_mechanism_general}
	\end{subfigure}
	\begin{subfigure}[c]{0.33\textwidth}
		\centering
		\includegraphics[width=\columnwidth]{figures/postq_mechanisms_q1.pdf}
	\caption{Q2: {\textit{``I found the task annoying''}}.}
	\label{fig:post_mechanisms_q1}
	\end{subfigure}
	\caption{\textbf{Post-study questionnaire answers} for general aspects of our tasks, and for Q2 for each task.}
	\label{fig:postq_mechanism_general}
\end{figure*}


\section{\new{More Complex and Frequent Usage}}

\new{We further investigate the performance of our mechanism in more challenging scenarios.
In particular, while the recommended usage of our proposed countermeasure is sporadic, we are interested in understanding how user performance and perception change when exposed frequently, especially whether frequently encountering our tasks on legitimate URLs would, e.g., lead to habituation, increased false positives, or annoyance.
Furthermore, we are interested in testing the performance of our tasks on more challenging URLs, such as those with more subdomains, or less commonly known ones.}

\new{To do so, we conducted a follow-up study with 500 participants from the US, where we doubled the number of emails from 13 to 25 (18 legitimate and 7 phishing), and the time to complete the study to 30 minutes. Furthermore, we added 4 more challenging URLs, e.g., with more subdomains and less intuitive names, such as cloud services like \texttt{azure.com}.
We report these new URLs in Appendix~\ref{sec:appendix.urls}.
To allow for better comparisons, the demographic distribution of participants was the same of the main study. Participants were randomly assigned to one group between control (100 participants), passive baseline (100), and inspection tasks (300).}

\subsection{\new{Performance Under Longer Exposure}}

\new{We first compare the performance of participants in the follow-up study on the emails and URLs that were also present in the main study.
We report the results in Table~\ref{tab:performance_followup}.
We can observe that, while in the longer study all groups performed slightly worse, our mechanisms still outperformed both control and the baseline approach, and significantly helped participants with a high improvement of 35\% less successful phishing compared to control.}

\new{We observe the same increase in false positives for legitimate emails compared to the control group, however, this did not get larger despite the 3x increase in the number of legitimate emails to manage.
Furthermore, the baseline group observed a similar increase in false positives.
Finally, time overhead for each task remained the same as in the main study.
Combined with the fact that most of these URLs would be allowlisted in a corporate setting and the sporadic nature of our countermeasure, these results suggest a tolerable increase in false positives.}

\begin{table}[t]
	\renewcommand{\arraystretch}{1.1}
	\footnotesize
	\centering
	\caption{\new{\textbf{Main and follow-up studies:} results comparison.}}
	\label{tab:performance_followup}
	\rowcolors{2}{white}{gray!10}
	\begin{tabularx}{\linewidth}{l|XX|XX}
		\toprule
		\textbf{Group} & \multicolumn{2}{c}{\textbf{Legitimate Emails Managed}} & \multicolumn{2}{c}{\textbf{Phishing Victimization}} \\
		& Main & \mbox{Follow-up} & Main & \mbox{Follow-up} \\ \midrule

		Control & 90.6\% & 93.5\% & 74.5\% & 83.3\% \\
		Passive & 87.9\% & 87.8\% & 57.2\% & 69.1\% \\
		Inspection & 82.1\% & 82.9\% & 35.0\% & 48.8\% \\

		\bottomrule
	\end{tabularx}
\end{table}

\new{To further assess the impact of longer exposure, we compared the answers of participants to the post-study questionnaire, especially regarding their perceived annoyance towards the tasks.
We show the result to the question \textit{``I found the task annoying''} in Figure~\ref{fig:mr_postq_mechanism_q1}. Compared to the main study, we observed similar annoyance, with the clicking and highlighting tasks being perceived as annoying as simply re-reading the URL, and the typing task being perceived as more annoying, but only slightly increasingly so compared to the main study.}

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figures/mr_postq_mechanisms_q1.pdf}
	\caption{\new{\textbf{Longer exposure Q2:} \textit{``I found the task annoying''}.}}
	\label{fig:mr_postq_mechanism_q1}
\end{figure}

\subsection{\new{More Complex URLs}}
\label{sec:results.more_complex_urls}

\new{We now analyze in more detail the performance of participants on the 4 more challenging legitimate and phishing URLs we introduced (see Appendix~\ref{sec:appendix.urls}).
Both the legitimate and phishing URLs featured more subdomains and less intuitive top domains, such as cloud services, or legitimate but less known services such as \texttt{spreadsheet0.google.com}.}

\new{The performance on legitimate URLs (and thus the false positives rate) was similar to the one on simpler URLs on both the main and follow-up studies for the control group and our mechanism.
However, interestingly, the passive baseline group showed a significant decrease in performance (from 88\% to 82\%), highlighting that our tasks helped more than simply presenting the URLs.
For phishing URLs, we again observe our countermeasure helping greatly in reducing the falling rates with a smaller but statistically significant improvement (25\% less phishing emails clicked) over both control and the baseline group.
Furthermore, we analyzed the performance per mechanism and observed the same positive effects as in the main study, especially for the clicking task as in these URLs participants had to choose among 3 instead of 2 options.}

\paragraph{\new{Unknown and wrong URL}}
\new{Finally, we analyze the \textit{googleusercontent} URL (see Appendix \ref{sec:appendix.urls}), that we added both in a phishing email and a legitimate one. 
It represents a special case because it is the only instance of a legitimate email containing a suspicious URL (it was not in the list of legitimate domains presented to the participant).
Further, it is especially difficult containing multiple subdomains, an IP address, and a less known cloud services domain. 
Thus, we wanted to see whether participants would also report the email coming from a legitimate source.
Both our mechanism and the passive baseline helped participants to not fall for the phishing email and report it (improving an already very high 60\% to 82\% and 87\%, respectively).
However, interestingly, neither helped participants question the legitimate one as report rates were similar across all groups with non significant differences, as participants most likely used cues from the email to decide.
This scenario simulated a genuine mistake by a colleague, but could also represent a \textit{business email compromise} scenario, where an attacker has taken over a legitimate email account and sends phishing emails to the victim's contacts, and highlights its danger.}
