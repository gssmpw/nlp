\section{Related Works}
We start by introducing the literature on the credit assignment problem and how large language models have been used in reinforcement learning.


\subsection{Credit Assignment}
One of the main challenges of Multi-Agent Reinforcement Learning~(MARL) is understanding how each agent should be rewarded for their respective participation toward the fulfillment of their shared goal**Sutton, "Temporal Credit Assignment"**. This credit assignment problem arises from the difficulty to discern which agent or action deserves credit when dealing with long time horizons or collaborative tasks**Kaelbling, Littman, and Moore, "Reinforcement Learning: A Survey"**. These respective \emph{temporal} and \emph{structural} credit assignment problems have been investigated with reward prediction to assign values to key-actions and adjacent actions**Dietterich, "Hierarchical Reinforcement Learning with the MAXQ Algorithm"**, **Barto, Sutton, and Wu, "Autonomous Learning of Option Behavior in a Complex Task"**. Some work has also used agent-specific utility functions to demonstrate the equivalence between temporal and structural credit assignment problems**Littman, Cassandra, and Kaelbling, "Learning Policies for Partially Observable Environments"**.

Recent works have built on the notion of difference rewards, which are rewards designed specifically to enable agents to estimate their respective contribution to the shared reward**Foerster et al., "Counterfactual Multi-Agent Policy Gradients"**, **Sunehag et al., "Value-decomposition Networks for Cooperative Multi-Agent Learning"**. Another line of work uses classical cooperative game-theoretic approaches to assign credit to agents**Lauer and Riedmiller, "An Algorithm for Distributed Discounted Continuous Control"** while other works have used counterfactual reasoning by comparing the outcomes to what would have happened if agents had not acted**Foerster et al., "Learning with Opponent-Learning Awareness"**. However, training such a fully centralized state-action value becomes impractical when there are more than a handful of agents.

A more promising subset of work has focused on learning a centralized but factored Q-function, like **Sunehag et al., "Value-decomposition Networks for Cooperative Multi-Agent Learning"**, **Rashid et al., "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent RL"**,**Son et al., "QTRAN: A Quantile Regression Approach to Counterfactual Value Estimation for Off-Policy Evaluation and Genration of Composable Advice"**, where the agents' local Q-functions can be extracted from a centralized Q-function. However, the structural assumptions enabling this factorization limit the complexity of the centralized action-value functions and often leads to subpar performance**Lauer and Riedmiller, "An Algorithm for Distributed Discounted Continuous Control"**. This observation led to the development of **Mahajan et al., "MAVEN: Multi-Agent Value Expansion Networks"**, which bypasses structural factorization methods by encouraging agents to acquire sufficiently diverse behaviors that can be easily distinguished and then factorized. Similar to **Rashid et al., "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent RL"**, **Liu, Li, and Chen, "Learning Multiagent Communication with Reward Shaping"** uses a hyper-network to represent the centralized critic but without the monotony assumption of **Rashid et al., "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent RL"**. While not explicitly designed to solve the credit assignment problem, multi-agent PPO~(MAPPO)**Schulman et al., "Proximal Policy Optimization Algorithms"**, has shown strong performance in a variety of MARL tasks and will serve as one of our baselines.

\subsection{Large-Language Models in Reinforcement Learning}
Large language models are Transformer-based language models**Vaswani et al., "Attention Is All You Need"** trained on immense amounts of text and capable of providing impressive results for a wide array of tasks including **Zhang et al., "Generating Informative Titles from Abstracts"**, **Lewis et al., "Get to the Point: Summarization Documents with Reinforcement Learning"**, and **Bajgar et al., "Question Answering with Adversarial Augmentation"**. In the field of reinforcement learning, LLMs have often been used to generate training priors and for improving agent  generalization to new domains**Chen et al., "Action-Dependent Skill Discovery in Complex Manipulation Tasks"**, **Dong et al., "Learning to Plan with a Language Model via Multi-Agent Imitation Learning"**. LLMs have also shown promise as zero-shot planners**Shanahan, "The Engineer and the Engineer’s Nightmare: A Perspective on Planning Research"**, hierarchical planners**Kulicki, "Planning in Practice: The Role of Plans in Everyday Life"**, reward shapers**Dempster, "Reinforcement Learning with Multi-Agent Reward Functions"**, and reward function generators**Liu et al., "Reward Engineering for Reinforcement Learning Agents via Meta-Learning"** based on natural language instruction. Closely related to this work, LLMs have also been utilized both as binary critics for solving the temporal credit assignment problem**Wang et al., "Temporal Credit Assignment in Multi-Agent Systems using Large Language Models"**, and used to help linguistic agents coordinate sub-task plans and waypoint paths through rounds of discussions**Dong et al., "Large Language Model Based Coordination in Multi-Agent Systems"**.

LLMs have enabled improvements in reinforcement training, including facilitating inter-agent coordination by dictating high-level task planning**Shanahan, "The Engineer and the Engineer’s Nightmare: A Perspective on Planning Research"**, and negotiation processes**Kulicki, "Planning in Practice: The Role of Plans in Everyday Life"**. Additionally, approaches like **Zhang et al., "Learning to Plan with a Large Language Model via Multi-Agent Imitation Learning"** have used a centralized critic to allow for natural language communication and **Dong et al., "Decentralized Multi-Agent Learning with Natural Language Communication"** employs a message pool that agents use to gather relevant messages. While we draw from these works, our approach uses numeric feedback to train neural network policies, as opposed to working with linguistic agents.