\section{Related Works}
We start by introducing the literature on the credit assignment problem and how large language models have been used in reinforcement learning.


\subsection{Credit Assignment}
One of the main challenges of Multi-Agent Reinforcement Learning~(MARL) is understanding how each agent should be rewarded for their respective participation toward the fulfillment of their shared goal____. This credit assignment problem arises from the difficulty to discern which agent or action deserves credit when dealing with long time horizons or collaborative tasks____. These respective \emph{temporal} and \emph{structural} credit assignment problems have been investigated with reward prediction to assign values to key-actions and adjacent actions____. Some work has also used agent-specific utility functions to demonstrate the equivalence between temporal and structural credit assignment problems____.

Recent works have built on the notion of difference rewards, which are rewards designed specifically to enable agents to estimate their respective contribution to the shared reward____. Another line of work uses classical cooperative game-theoretic approaches to assign credit to agents____ while other works have used counterfactual reasoning by comparing the outcomes to what would have happened if agents had not acted____. However, training such a fully centralized state-action value becomes impractical when there are more than a handful of agents.

A more promising subset of work has focused on learning a centralized but factored Q-function, like value decomposition networks~(VDN)____, QMIX____, and QTRAN____, where the agents' local Q-functions can be extracted from a centralized Q-function. However, the structural assumptions enabling this factorization limit the complexity of the centralized action-value functions and often leads to subpar performance____. This observation led to the development of MAVEN____ to fight inefficient exploration of Q-DPP____, which bypasses structural factorization methods by encouraging agents to acquire sufficiently diverse behaviors that can be easily distinguished and then factorized. Similar to QMIX, LICA____ uses a hyper-network to represent the centralized critic but without the monotony assumption of QMIX____. While not explicitly designed to solve the credit assignment problem, multi-agent PPO~(MAPPO)____ has shown strong performance in a variety of MARL tasks and will serve as one of our baselines.

\subsection{Large-Language Models in Reinforcement Learning}
Large language models are Transformer-based language models____ trained on immense amounts of text and capable of providing impressive results for a wide array of tasks including text generation____, question answering____, and text summarization____. In the field of reinforcement learning, LLMs have often been used to generate training priors and for improving agent  generalization to new domains____. LLMs have also shown promise as zero-shot planners____, hierarchical planners____, reward shapers____, and reward function generators____ based on natural language instruction. Closely related to this work, LLMs have also been utilized both as binary critics for solving the temporal credit assignment problem____, and used to help linguistic agents coordinate sub-task plans and waypoint paths through rounds of discussions____.

LLMs have enabled improvements in reinforcement training, including facilitating inter-agent coordination by dictating high-level task planning____ and negotiation processes____. Additionally, approaches like ____ have used a centralized critic to allow for natural language communication and ____ employs a message pool that agents use to gather relevant messages. While we draw from these works, our approach uses numeric feedback to train neural network policies, as opposed to working with linguistic agents.