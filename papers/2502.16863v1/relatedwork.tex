\section{Related Works}
We start by introducing the literature on the credit assignment problem and how large language models have been used in reinforcement learning.


\subsection{Credit Assignment}
One of the main challenges of Multi-Agent Reinforcement Learning~(MARL) is understanding how each agent should be rewarded for their respective participation toward the fulfillment of their shared goal~\cite{weiss1995distributed}. This credit assignment problem arises from the difficulty to discern which agent or action deserves credit when dealing with long time horizons or collaborative tasks~\cite{Sammut2010}. These respective \emph{temporal} and \emph{structural} credit assignment problems have been investigated with reward prediction to assign values to key-actions and adjacent actions~\cite{seo2019rewards}. Some work has also used agent-specific utility functions to demonstrate the equivalence between temporal and structural credit assignment problems~\cite{agogino2004unifying}.

Recent works have built on the notion of difference rewards, which are rewards designed specifically to enable agents to estimate their respective contribution to the shared reward~\cite{nguyen2018credit, feng2022multi, singh2021credit}. Another line of work uses classical cooperative game-theoretic approaches to assign credit to agents~\cite{han2022multiagent, wang2020shapley} while other works have used counterfactual reasoning by comparing the outcomes to what would have happened if agents had not acted~\cite{rahmattalabi2016d++, foerster2018counterfactual}. However, training such a fully centralized state-action value becomes impractical when there are more than a handful of agents.

A more promising subset of work has focused on learning a centralized but factored Q-function, like value decomposition networks~(VDN)~\cite{sunehag2018VDN}, QMIX~\cite{rashid2020monotonic}, and QTRAN~\cite{son2019qtran}, where the agents' local Q-functions can be extracted from a centralized Q-function. However, the structural assumptions enabling this factorization limit the complexity of the centralized action-value functions and often leads to subpar performance~\cite{mahajan2019maven}. This observation led to the development of MAVEN~\cite{mahajan2019maven} to fight inefficient exploration of Q-DPP~\cite{yang2020QDPP}, which bypasses structural factorization methods by encouraging agents to acquire sufficiently diverse behaviors that can be easily distinguished and then factorized. Similar to QMIX, LICA~\cite{zhou2020learning} uses a hyper-network to represent the centralized critic but without the monotony assumption of QMIX~\cite{rashid2020monotonic}. While not explicitly designed to solve the credit assignment problem, multi-agent PPO~(MAPPO)~\cite{yu2022surprising} has shown strong performance in a variety of MARL tasks and will serve as one of our baselines.

\subsection{Large-Language Models in Reinforcement Learning}
Large language models are Transformer-based language models~\cite{vaswani2017attention} trained on immense amounts of text and capable of providing impressive results for a wide array of tasks including text generation~\cite{brown2020language}, question answering~\cite{kenton2019bert}, and text summarization~\cite{zhang2020pegasus}. In the field of reinforcement learning, LLMs have often been used to generate training priors and for improving agent  generalization to new domains~\cite{kim2024survey}. LLMs have also shown promise as zero-shot planners~\cite{huang2022language}, hierarchical planners~\cite{taniguchi2024hierarchical, kannan2023smart}, reward shapers~\cite{carta2022eager}, and reward function generators~\cite{goyal2019using} based on natural language instruction. Closely related to this work, LLMs have also been utilized both as binary critics for solving the temporal credit assignment problem~\cite{pignatelli2024assessing}, and used to help linguistic agents coordinate sub-task plans and waypoint paths through rounds of discussions~\cite{mandi2024roco}.

LLMs have enabled improvements in reinforcement training, including facilitating inter-agent coordination by dictating high-level task planning~\cite{zhuang2024yolo} and negotiation processes~\cite{chen2023multi, sun2024llm}. Additionally, approaches like \cite{slumbers2024leveraging} have used a centralized critic to allow for natural language communication and \cite{hongmetagpt} employs a message pool that agents use to gather relevant messages. While we draw from these works, our approach uses numeric feedback to train neural network policies, as opposed to working with linguistic agents.