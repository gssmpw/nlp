\chapter{Markov Chain Monte Carlo Methods}\label{sec:approximate_inference:mcmc}

Variational inference approximates the entire posterior distribution.
However, note that the key challenge in probabilistic inference is not learning the posterior distribution, but using the posterior distribution for predictions, \begin{align}
  p(\ys \mid \vxs, \vx_{1:n}, y_{1:n}) &= \int p(\ys \mid \vxs, \vtheta) p(\vtheta \mid \vx_{1:n}, y_{1:n}) \,d\vtheta.
  \intertext{This integral can be interpreted as an expectation over the posterior distribution,\looseness=-1}
  &= \E[\vtheta \sim p(\cdot \mid \vx_{1:n}, y_{1:n})]{p(\ys \mid \vxs, \vtheta)}.
  \intertext{Observe that the likelihood $f(\vtheta) \defeq p(\ys \mid \vxs, \vtheta)$ is easy to evaluate. The difficulty lies in sampling from the posterior distribution. Assuming we can obtain independent samples from the posterior distribution, we can use Monte Carlo sampling to obtain an unbiased estimate of the expectation,}
  &\approx \frac{1}{m} \sum_{i=1}^m f(\vtheta^{(i)})
\end{align} for independent $\vtheta^{(i)} \iid p(\cdot \mid \vx_{1:n}, y_{1:n})$.
The law of large numbers \eqref{eq:slln} and Hoeffding's inequality \eqref{eq:hoeffdings_inequality} imply that this estimator is consistent and sharply concentrated.\footnote{For more details, see \cref{sec:fundamentals:mc_approx}.}

Obtaining samples of the posterior distribution is therefore sufficient to perform approximate inference.
Recall that the difficulty of computing the posterior $p$ exactly, was in finding the normalizing constant~$Z$,\looseness=-1 \begin{align}
  p(x) = \frac{1}{Z} q(x). \label{eq:mcmc_posterior}
\end{align}
The joint likelihood $q$ is typically easy to obtain.
Note that $q(x)$ is proportional to the probability density associated with $x$, but $q$ does not integrate to 1. Such functions are also called a \midx{finite measure}.
Without normalizing $q$, we cannot directly sample from it.

\begin{rmk}{The difficulty of sampling --- even with a PDF}{}
  Even a decent approximation of $Z$ does not yield a general efficient sampling method.
  For example, one very common approach to sampling is \midx{inverse transform sampling} (cf. \cref{sec:inverse_transform_sampling}) which requires an (approximate) quantile function.
  Computing the quantile function given an arbitrary PDF requires solving integrals over the domain of the PDF which is what we were trying to avoid in the first place.
\end{rmk}

The key idea of Markov chain Monte Carlo methods is to construct a Markov chain, which is efficient to simulate and has the stationary distribution $p$.

\section{Markov Chains}

To start, let us revisit the fundamental theory behind Markov chains.\looseness=-1

\begin{defn}[Markov chain]\pidx{Markov chain}
  A \emph{(finite and discrete-time) Markov chain} over the state space \begin{align}
    \sS \defeq \{0, \dots, n-1\}
  \end{align} is a stochastic process\footnote{A \idx{stochastic process} is a sequence of random variables.} $(X_t)_{t \in \Nat_0}$ valued in $\sS$ such that the \midx{Markov property}[idxpagebf] is satisfied: \begin{align}
    X_{t+1} \perp X_{0:t-1} \mid X_t. \label{eq:markov_property}
  \end{align}
\end{defn}

\begin{marginfigure}
  \incfig{mc}
  \caption{Directed graphical model of a Markov chain.
  The random variable $X_{t+1}$ is conditionally independent of the random variables $X_{0:t-1}$ given $X_t$.}
\end{marginfigure}

Intuitively, the Markov property states that future behavior is independent of past states given the present state.

\begin{rmk}{Generalizations of Markov chains}{}
  One can also define continuous-state Markov chains (for example, where states are vectors in $\R^d$) and the results which we state for (finite) Markov chains will generally carry over.
  For a survey, refer to \icite{roberts2004general}.

  Moreover, one can also consider continuous-time Markov chains.
  One example of such a continuous-space and continuous-time Markov chain is the \midx{Wiener process} (cf. \cref{rmk:wiener_process}).
\end{rmk}

We restrict our attention to \midx<time-homogeneous>{time-homogeneous process} Markov chains,\footnote{That is, the transition probabilities do not change over time.} which can be characterized by a \midx{transition function}, \begin{align}
  p(x' \mid x) \defeq \Pr{X_{t+1} = x' \mid X_t = x}.
\end{align}
As the state space is finite, we can describe the transition function by the \midx{transition matrix}, \begin{align}
  \mP \defeq \begin{bmatrix}
    p(x_1 \mid x_1) & \cdots & p(x_n \mid x_1) \\
    \vdots & \ddots & \vdots \\
    p(x_1 \mid x_n) & \cdots & p(x_n \mid x_n) \\
  \end{bmatrix} \in \R^{n \times n}. \label{eq:transition_matrix}
\end{align}
Note that each row of $\mP$ must always sum to $1$.
Such matrices are also called \midx<stochastic>{stochastic matrix}.

The \midx{transition graph} of a Markov chain is a directed graph consisting of vertices $\sS$ and weighted edges represented by the adjacency matrix $\mP$.

The current state of the Markov chain at time $t$ is denoted by the probability distribution $q_t$ over states $\sS$, that is, $X_t \sim q_t$. In the finite setting, $q_t$ is a PMF, which is often written explicitly as the row vector~${\vq_t \in \R^{1 \times \card{\sS}}}$.
The initial state (or prior) of the Markov chain is given as~${X_0 \sim q_0}$.
One iteration of the Markov chain can then be expressed as follows:~\exerciserefmark{mc_update} \begin{align}
  \vq_{t+1} = \vq_t \mP. \label{eq:mc_update}
\end{align}
It is implied directly that we can write the state of the Markov chain at time $t + k$ as \begin{align}
  \vq_{t+k} = \vq_t \mP^k.
\end{align}
The entry $\mP^k(x,x')$ corresponds to the probability of transitioning from state $x \in \sS$ to state $x' \in \sS$ in exactly $k$ steps \exerciserefmark{mc_multi_step_transitions}.
We denote this entry by $p^{(k)}(x' \mid x)$.

In the analysis of Markov chains, there are two main concepts of interest: stationarity and convergence.
We begin by introducing stationarity.\looseness=-1

\subsection{Stationarity}

\begin{defn}[Stationary distribution]\pidx{stationary distribution}
  A distribution $\pi$ is \emph{stationary} with respect to the transition function $p$ iff \begin{align}
    \pi(x) = \sum_{x' \in S} p(x \mid x') \pi(x')
  \end{align} holds for all $x \in S$.
  It follows from \cref{eq:mc_update} that equivalently, $\pi$ is stationary w.r.t. a transition matrix $\mP$ iff \begin{align}
    \vpi = \vpi \mP. \label{eq:stationarity_vector_form}
  \end{align}
\end{defn}

After entering a stationary distribution $\pi$, a Markov chain will always remain in the stationary distribution.
In particular, suppose that $X_t$ is distributed according to $\pi$, then for all $k \geq 0$, $X_{t+k} \sim \pi$.

\begin{rmk}{When does a stationary distribution exist?}{}
  In general, there are Markov chains with infinitely many stationary distributions or no stationary distribution at all.
  You can find some examples in \cref{fig:mc_examples}.

  It can be shown that there exists a unique stationary distribution~$\pi$ if the Markov chain is \midx<irreducible>{irreducible Markov chain}, that is, if every state is reachable from every other state with a positive probability when the Markov chain is run for enough steps. Formally, \begin{align}
    \forall x,x' \in \sS. \, \exists k \in \Nat. \, p^{(k)}(x' \mid x) > 0.
  \end{align}
  Equivalently, a Markov chain is irreducible iff its transition graph is strongly connected.
\end{rmk}

\subsection{Convergence}\label{sec:mcmc:mc:convergence}

Let us now consider Markov chains with a unique stationary distribution.\footnote{Observe that the stationary distribution of an irreducible Markov chain must have full support, that is, assign positive probability to every state.}
A natural next question is whether this Markov chain converges to its stationary distribution.
We say that a Markov chain converges to its stationary distribution iff we have \begin{align}
  \lim_{t\to\infty} q_{t} = \pi,
\end{align} irrespectively of the initial distribution $q_0$.

\begin{rmk}{When does a Markov chain converge?}{mc_convergence}
  Even if a Markov chain has a unique stationary distribution, it does not have to converge to it.
  Consider example (3) in \cref{fig:mc_examples}.
  Clearly, $\pi = (\frac{1}{2}, \frac{1}{2})$ is the unique stationary distribution.
  However, observe that if we start with a suitable initial distribution such as $q_0 = (1,0)$, at no point in time will the probability of all states be positive, and in particular, the chain will not converge to $\pi$.
  Instead, the chain behaves periodically, i.e., its state distributions are $q_{2t} = (0,1)$ and $q_{2t+1} = (1,0)$ for all $t \in \Nat$.
  It turns out that if we exclude such ``periodic'' Markov chains, then the remaining (irreducible) Markov chains will always converge to their stationary distribution.

  Formally, a Markov chain is \midx<aperiodic>{aperiodic Markov chain} if for all states $x \in \sS$, \begin{align}
    \exists k_0 \in \Nat. \, \forall k \geq k_0. \, p^{(k)}(x \mid x) > 0.
  \end{align}
  In words, a Markov chain is aperiodic iff for every state $x$, the transition graph has a closed path from $x$ to $x$ with length $k$ for all $k \in \Nat$ greater than some $k_0 \in \Nat$.
\end{rmk}

This additional property leads to the concept of ergodicity.

\begin{defn}[Ergodicity]\pidx{ergodicity}
  A Markov chain is \emph{ergodic} iff there exists a $t \in \Nat_0$ such that for any $x, x' \in S$ we have \begin{align}
    p^{(t)}(x' \mid x) > 0,
  \end{align} whereby $p^{(t)}(x' \mid x)$ is the probability to reach $x'$ from $x$ in exactly $t$ steps.
  Equivalent conditions are \begin{enumerate}
    \item that there exists some $t \in \Nat_0$ such that all entries of $\mP^t$ are strictly positive; and
    \item that it is irreducible and aperiodic.
  \end{enumerate}
\end{defn}

\begin{marginfigure}
  \incfig{mc_examples}
  \caption{Transition graphs of Markov chains: (1) is not ergodic as its transition diagram is not strongly connected; (2) is not ergodic for the same reason; (3) is irreducible but periodic and therefore not ergodic; (4) is ergodic with stationary distribution $\pi(1) = \nicefrac{2}{3}, \pi(2) = \nicefrac{1}{3}$.}\label{fig:mc_examples}
\end{marginfigure}

\begin{ex}{Making a Markov chain ergodic}{}
  A commonly used strategy to ensure that a Markov chain is ergodic is to add ``self-loops'' to every vertex in the transition graph. That is, to ensure that at any point in time, the Markov chain remains with positive probability in its current state.

  Take a (not necessarily ergodic) but irreducible Markov chain with transition matrix $\mP$. We define the new Markov chain \begin{align}
    \mP' \defeq \frac{1}{2}\mP + \frac{1}{2}\mI.
  \end{align}
  It is a simple exercise to confirm that $\mP'$ is stochastic, and hence a valid transition matrix.
  Also, it follows directly that $\mP'$ is irreducible (as $\mP$ is irreducible) and aperiodic as every vertex has a closed path of length $1$ to itself, and therefore the chain is ergodic.

  Take now $\vpi$ to be a stationary distribution of $\mP$. We have that $\vpi$ is also a stationary distribution of $\mP'$ as \begin{align}
    \vpi\mP' = \frac{1}{2}\vpi\mP + \frac{1}{2}\vpi\mI = \frac{1}{2}\vpi + \frac{1}{2}\vpi = \vpi. \margintag{using \eqref{eq:stationarity_vector_form}}
  \end{align}
\end{ex}

\begin{fct}[Fundamental theorem of ergodic Markov chains, theorem~4.9 of \cite{levin2017markov}]\pidx{fundamental theorem of ergodic Markov chains}
  An ergodic Markov chain has a unique stationary distribution $\pi$ (with full support) and \begin{align}
    \lim_{t\to\infty} q_t = \pi \label{eq:fund_thm_of_ergodic_mc}
  \end{align} irrespectively of the initial distribution $q_0$.
\end{fct}

This naturally suggests constructing an ergodic Markov chain such that its stationary distribution coincides with the posterior distribution.
If we then sample ``sufficiently long'', $X_t$ is drawn from a distribution that is ``very close'' to the posterior distribution.

\begin{rmk}{How quickly does a Markov chain converge?}{}
  The convergence speed of Markov chains is a rich field of research.
  ``Sufficiently long'' and ``very close'' are commonly made precise by the notions of \emph{rapidly mixing} Markov chains and \emph{total variation distance}.\looseness=-1

  \begin{defn}[Total variation distance]\pidx{total variation distance}
    The total variation distance between two probability distributions $\mu$ and $\nu$ on $\spA$ is defined by \begin{align}
      \norm{\mu - \nu}_{\mathrm{TV}} \defeq 2 \sup_{\sA \subseteq \spA} \abs{\mu(\sA) - \nu(\sA)}.
    \end{align}
    It defines the distance between $\mu$ and $\nu$ to be the maximum difference between the probabilities that $\mu$ and $\nu$ assign to the same event.\looseness=-1
  \end{defn}
  As opposed to the KL-divergence \eqref{eq:kl}, the total variation distance is a metric.
  In particular, it is symmetric and satisfies the triangle inequality.
  It can be shown that \begin{align}
    \norm{\mu - \nu}_{\mathrm{TV}} \leq \sqrt{2 \KL{\mu}{\nu}} \label{eq:pinsker}
  \end{align} which is known as \midx{Pinsker's inequality}.
  Moreover, if $\mu$ and $\nu$ are discrete distributions over the set $\sS$, it can be shown that \begin{align}
    \norm{\mu - \nu}_{\mathrm{TV}} = \sum_{i \in \sS} \abs{\mu(i) - \nu(i)}.
  \end{align}

  \begin{defn}[Mixing time]
    For a Markov chain with stationary distribution $\pi$, its \midx{mixing time} with respect to the total variation distance for any $\epsilon > 0$ is \begin{align}
      \tau_{\mathrm{TV}}(\epsilon) \defeq \min\{t \mid \forall q_0 : \norm{q_t - \pi}_{\mathrm{TV}} \leq \epsilon\}.
    \end{align}
    Thus, the mixing time measures the time required by a Markov chain for the distance to stationarity to be small.
    A Markov chain is typically said to be \midx<rapidly mixing>{rapidly mixing Markov chain} if for any $\epsilon > 0$, \begin{align}
      \tau_{\mathrm{TV}}(\epsilon) \in \BigO{\mathrm{poly}(n, \log(1 / \epsilon))}.
    \end{align}
    That is, a rapidly mixing Markov chain on $n$ states needs to be simulated for at most $\mathrm{poly}(n)$ steps to obtain a ``good'' sample from its stationary distribution $\pi$.
  \end{defn}

  You can find a thorough introduction to mixing times in chapter 4 of \icite{levin2017markov}. Later chapters introduce methods for showing that a Markov chain is rapidly mixing.
\end{rmk}


\subsection{Detailed Balance Equation}

How can we confirm that the stationary distribution of a Markov chain coincides with the posterior distribution?
The detailed balance equation yields a very simple method.

\begin{defn}[Detailed balance equation / reversibility]\pidx{detailed balance equation}
  A Markov chain satisfies the \emph{detailed balance equation} with respect to a distribution~$\pi$ iff \begin{align}
    \pi(x) p(x' \mid x) = \pi(x') p(x \mid x') \label{eq:dbe}
  \end{align} holds for any $x, x' \in S$.
  A Markov chain that satisfies the detailed balance equation with respect to $\pi$ is called \midx<reversible>{reversible Markov chain} with respect to~$\pi$.
\end{defn}

\begin{lem}
  Given a finite Markov chain, if the Markov chain is reversible with respect to $\pi$ then $\pi$ is a stationary distribution.\footnote{Note that reversibility of $\pi$ is only a sufficient condition for stationarity of $\pi$, it is not necessary! In particular, there are irreversible ergodic Markov chains.}
\end{lem}\vspace{-10pt}
\begin{proof}
  Let $\pi \defeq q_t$. We have, \begin{align*}
    q_{t+1}(x) &= \sum_{x' \in S} p(x \mid x') q_t(x') \margintag{using the Markov property \eqref{eq:markov_property}} \\
    &= \sum_{x' \in S} p(x \mid x') \pi(x') \\
    &= \sum_{x' \in S} p(x' \mid x) \pi(x) \margintag{using the detailed balance equation \eqref{eq:dbe}} \\
    &= \pi(x) \sum_{x' \in S} p(x' \mid x) \\
    &= \pi(x). \qedhere \margintag{using that $\sum_{x' \in S} p(x' \mid x) = 1$}
  \end{align*}
\end{proof}

That is, if we can show that the detailed balance equation \eqref{eq:dbe} holds for some distribution $q$, then we know that $q$ is the stationary distribution of the Markov chain.

Next, reconsider our posterior distribution $p(x) = \frac{1}{Z} q(x)$ from \cref{eq:mcmc_posterior}.
If we substitute the posterior for $\pi$ in the detailed balance equation, we obtain\looseness=-1 \begin{align}
  \frac{1}{Z} q(x) p(x' \mid x) &= \frac{1}{Z} q(x') p(x \mid x'),
  \intertext{or equivalently,}
  q(x) p(x' \mid x) &= q(x') p(x \mid x').
\end{align}
In words, we do not need to know the true posterior $p$ to check that the stationary distribution of our Markov chain coincides with $p$, it suffices to know the finite measure $q$!

\subsection{Ergodic Theorem}\label{sec:approximate_inference:mcmc:ergodic_theorem}

If we now suppose that we can construct a Markov chain whose stationary distribution coincides with the posterior distribution --- we will see later that this is possible --- it is not apparent that this allows us to estimate expectations over the posterior distribution.
Note that although constructing such a Markov chain allows us to obtain samples from the posterior distribution, they are \emph{not} independent.
In fact, due to the structure of a Markov chain, by design, they are strongly dependent.
Thus, the law of large numbers and Hoeffding's inequality do not apply.
By itself, it is not even clear that an estimator relying on samples from a single Markov chain will be unbiased.

Theoretically, we could simulate many Markov chains separately and obtain one sample from each of them. This, however, is extremely inefficient.
It turns out that there is a way to generalize the (strong) law of large numbers to Markov chains.

\begin{thm}[Ergodic theorem, appendix~C of \cite{levin2017markov}]\pidx{ergodic theorem}
  Given an ergodic Markov chain $(X_t)_{t \in \Nat_0}$ over a finite state space $\sS$ with stationary distribution $\pi$ and a function $f : \sS \to \R$, \begin{align}
    \frac{1}{n} \sum_{i=1}^n f(x_i) \almostsurely \sum_{x \in S} \pi(x) f(x) = \E[x \sim \pi]{f(x)} \label{eq:ergodic_theorem}
  \end{align} as $n\to\infty$ where $x_i \sim X_i \mid x_{i-1}$.
\end{thm}

This result is the fundamental reason for why Markov chain Monte Carlo methods are possible. There are analogous results for continuous domains.

\begin{marginfigure}
  \incplt{burn_in_time}
  \caption{Illustration of the ``burn-in'' time $t_0$ of a Markov chain approximating the posterior $p(\ys = 1 \mid \mX, \vy)$ of Bayesian logistic regression.
  The true posterior $p$ is shown in gray.
  The distribution of the Markov chain at time $t$ is shown in red.}
\end{marginfigure}

Note, however, that the ergodic theorem only tells us that simulating a single Markov chain yields an unbiased estimator.
It does not tell us anything about the rate of convergence and variance of such an estimator.
The convergence rate depends on the mixing time of the Markov chain, which is difficult to establish in general.

In practice, one observes that Markov chain Monte Carlo methods have a so-called \midx<``burn-in'' time>{burn-in time} during which the distribution of the Markov chain does not yet approximate the posterior distribution well.
Typically, the first $t_0$ samples are therefore discarded, \begin{align}
  \E{f(X)} \approx \frac{1}{T - t_0} \sum_{t=t_0+1}^T f(X_t). \label{eq:mcmc}
\end{align}
It is not clear in general how $T$ and $t_0$ should be chosen such that the estimator is unbiased, rather they have to be tuned.

Another widely used heuristic is to first find the mode of the posterior distribution and then start the Markov chain at that point.
This tends to increase the rate of convergence drastically, as the Markov chain does not have to ``walk to the location in the state space where most probability mass will be located''.

\section{Elementary Sampling Methods}

We will now examine methods for constructing and sampling from a Markov chain with the goal of approximating samples from the posterior distribution $p$.
Note that in this setting the state space of the Markov chain is $\R^n$ and a single state at time $t$ is described by the random vector $\rX \defeq [X_1, \dots, X_n]$.

\subsection{Metropolis-Hastings Algorithm}

Suppose we are given a \midx{proposal distribution} $r(\vxp \mid \vx)$ which, given we are in state $\vx$, proposes a new state $\vxp$.
Metropolis and Hastings showed that using the \midx{acceptance distribution} $\Bern{\alpha(\vxp \mid \vx)}$ where \begin{align}
  \alpha(\vxp \mid \vx) &\defeq \min \braces*{1, \frac{p(\vxp) r(\vx \mid \vxp)}{p(\vx) r(\vxp \mid \vx)}} \\
  &= \min \braces*{1, \frac{q(\vxp) r(\vx \mid \vxp)}{q(\vx) r(\vxp \mid \vx)}} \margintag{similarly to the detailed balance equation, the normalizing constant $Z$ cancels} \label{eq:mh_acc_distr}
\end{align} to decide whether to follow the proposal yields a Markov chain with stationary distribution $p(\vx) = \frac{1}{Z} q(\vx)$.

\begin{algorithm}[H]
  \caption{Metropolis-Hastings algorithm}\pidx{Metropolis-Hastings algorithm}
  initialize $\vx \in \R^n$\;
  \For{$t = 1$ \KwTo $T$}{
    sample $\vxp \sim r(\vxp \mid \vx)$\;
    sample $u \sim \Unif{[0, 1]}$\;
    \lIf{$u \leq \alpha(\vxp \mid \vx)$}{update $\vx \gets \vxp$}
    \lElse{update $\vx \gets \vx$}
  }
\end{algorithm}

Intuitively, the acceptance distribution corrects for the bias in the proposal distribution.
That is, if the proposal distribution $r$ is likely to propose states with low probability under~$p$, the acceptance distribution will reject these proposals frequently.
The following theorem formalizes this intuition.

\begin{thm}[Metropolis-Hastings theorem]\label{thm:metropolis_hastings}\pidx{Metropolis-Hastings theorem}
  Given an arbitrary proposal distribution $r$ whose support includes the support of $q$, the stationary distribution of the Markov chain simulated by the Metropolis-Hastings algorithm is $p(\vx) = \frac{1}{Z} q(\vx)$.
\end{thm}
\begin{proof}
  First, let us define the transition probabilities of the Markov chain.
  The probability of transitioning from a state $\vx$ to a state $\vxp$ is given by $r(\vxp \mid \vx) \alpha(\vxp \mid \vx)$ if $\vx \neq \vxp$ and the probability of proposing to remain in state $\vx$, $r(\vx \mid \vx)$, plus the probability of denying the proposal, otherwise. \begin{align}
    p(\vxp \mid \vx) = \begin{cases}
      r(\vxp \mid \vx) \alpha(\vxp \mid \vx) & \text{if}\ \vx \neq \vxp \\
      r(\vx \mid \vx) + \sum_{\vx'' \neq \vx} r(\vx'' \mid \vx)(1 - \alpha(\vx'' \mid \vx)) & \text{otherwise}.
    \end{cases} \label{eq:mh_trans_prob}
  \end{align}

  We will show that the stationary distribution is $p$ by showing that~$p$ satisfies the detailed balance equation \eqref{eq:dbe}.
  Let us fix arbitrary states $\vx$ and $\vxp$. First, observe that if $\vx = \vxp$, then the detailed balance equation is trivially satisfied.
  Without loss of generality we assume \begin{align*}
    \alpha(\vx \mid \vxp) = 1, \quad \alpha(\vxp \mid \vx) = \frac{q(\vxp) r(\vx \mid \vxp)}{q(\vx) r(\vxp \mid \vx)}.
  \end{align*}
  For $\vx \neq \vxp$, we then have, \begin{align*}
    p(\vx) \cdot p(\vxp \mid \vx) &= \frac{1}{Z} q(\vx) p(\vxp \mid \vx) \margintag{using the definition of the distribution $p$} \\
    &= \frac{1}{Z} q(\vx) r(\vxp \mid \vx) \alpha(\vxp \mid \vx) \margintag{using the transition probabilities of the Markov chain} \\
    &= \frac{1}{Z} q(\vx) r(\vxp \mid \vx) \frac{q(\vxp) r(\vx \mid \vxp)}{q(\vx) r(\vxp \mid \vx)} \margintag{using the definition of the acceptance distribution $\alpha$} \\
    &= \frac{1}{Z} q(\vxp) r(\vx \mid \vxp) \\
    &= \frac{1}{Z} q(\vxp) r(\vx \mid \vxp) \alpha(\vx \mid \vxp) \margintag{using the definition of the acceptance distribution $\alpha$} \\
    &= \frac{1}{Z} q(\vxp) p(\vx \mid \vxp) \margintag{using the transition probabilities of the Markov chain} \\
    &= p(\vxp) \cdot p(\vx \mid \vxp). \qedhere \margintag{using the definition of the distribution $p$}
  \end{align*}
\end{proof}

Note that by the fundamental theorem of ergodic Markov chains \eqref{eq:fund_thm_of_ergodic_mc}, for convergence to the stationary distribution, it is sufficient for the Markov chain to be ergodic.
Ergodicity follows immediately when the transition probabilities $p(\cdot \mid \vx)$ have full support. For example, if the proposal distribution $r(\cdot \mid \vx)$ has full support, the full support of $p(\cdot \mid \vx)$ follows immediately from \cref{eq:mh_trans_prob}. The rate of convergence of Metropolis-Hastings depends strongly on the choice of the proposal distribution, and we will explore different choices of proposal distribution in the following.

\subsection{Gibbs Sampling}

A popular example of a Metropolis-Hastings algorithm is Gibbs sampling as presented in \cref{alg:gibbs_sampling}.

\begin{algorithm}
  \caption{Gibbs sampling}\pidx{Gibbs sampling}\label{alg:gibbs_sampling}
  initialize $\vx = [x_1, \dots, x_n] \in \R^n$\;
  \For{$t = 1$ \KwTo $T$}{
    pick a variable $i$ uniformly at random from $\{1, \dots, n\}$\;
    set $\vx_{-i} \defeq [x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n]$\;
    update $x_i$ by sampling according to the posterior distribution $p(x_i \mid \vx_{-i})$\;
  }
\end{algorithm}

Intuitively, by re-sampling single coordinates according to the posterior distribution given the other coordinates, Gibbs sampling finds states that are successively ``more'' likely.
Selecting the index $i$ uniformly at random ensures that the underlying Markov chain is ergodic provided the conditional distributions $p(\cdot \mid \vx_{-i})$ have full support.

\begin{thm}[Gibbs sampling as Metropolis-Hastings]\label{thm:gibbs_sampling_as_mh}
  Gibbs sampling is a Metropolis-Hastings algorithm.
  For any fixed $i \in [n]$, it has proposal distribution \begin{align}
    r_i(\vxp \mid \vx) \defeq \begin{cases}
      p(x'_i \mid \vxp_{-i}) & \text{$\vxp$ differs from $\vx$ only in entry $i$} \\
      0 & \text{otherwise}
    \end{cases} \label{eq:gibbs_prop_distr}
  \end{align} and acceptance distribution $\alpha_i(\vxp \mid \vx) \defeq 1$.
\end{thm}
\begin{proof}
  We show that $\alpha_i(\vxp \mid \vx) = 1$ follows from the definition of an acceptance distribution in Metropolis-Hastings \eqref{eq:mh_acc_distr} and the choice of proposal distribution \eqref{eq:gibbs_prop_distr}.

  By \eqref{eq:mh_acc_distr}, \begin{align*}
    \alpha_i(\vxp \mid \vx) &= \min \braces*{1, \frac{p(\vxp) r_i(\vx \mid \vxp)}{p(\vx) r_i(\vxp \mid \vx)}}
  \intertext{Note that $p(\vx) = p(x_i, \vx_{-i}) = p(x_i \mid \vx_{-i}) p(\vx_{-i})$ using the product rule \eqref{eq:product_rule}. Therefore,}
    &= \min \braces*{1, \frac{p(x'_i \mid \vxp_{-i}) p(\vxp_{-i}) r_i(\vx \mid \vxp)}{p(x_i \mid \vx_{-i}) p(\vx_{-i}) r_i(\vxp \mid \vx)}} \\
    &= \min \braces*{1, \frac{p(x'_i \mid \vxp_{-i}) p(\vxp_{-i}) p(x_i \mid \vx_{-i})}{p(x_i \mid \vx_{-i}) p(\vx_{-i}) p(x'_i \mid \vxp_{-i})}} \margintag{using the proposal distribution \eqref{eq:gibbs_prop_distr}} \\
    &= \min \braces*{1, \frac{p(\vxp_{-i})}{p(\vx_{-i})}} \\
    &= 1. \qedhere \margintag{using that $p(\vxp_{-i}) = p(\vx_{-i})$}
  \end{align*}
\end{proof}

If the index $i$ is chosen uniformly at random as in \cref{alg:gibbs_sampling}, then the proposal distribution is $r(\vxp \mid \vx) = \frac{1}{n} \sum_{i=1}^n r_i(\vxp \mid \vx)$, which analogously to \cref{thm:gibbs_sampling_as_mh} has the associated acceptance distribution \begin{align*}
  \alpha(\vxp \mid \vx) = \min \braces*{1, \frac{p(\vxp) r(\vx \mid \vxp)}{p(\vx) r(\vxp \mid \vx)}} = \min \braces*{1, \frac{\sum_{i=1}^n p(\vxp) r_i(\vx \mid \vxp)}{\sum_{i=1}^n p(\vx) r_i(\vxp \mid \vx)}} = 1.
\end{align*}

\begin{cor}[Convergence of Gibbs sampling]
  As Gibbs sampling is a specific example of an MH-algorithm, the stationary distribution of the simulated Markov chain is $p(\vx)$.
\end{cor}

Note that for the proposals of Gibbs sampling, we have \begin{align}
  p(x_i \mid \vx_{-i}) = \frac{p(x_i, \vx_{-i})}{\sum_{x_i} p(x_i, \vx_{-i})} = \frac{q(x_i, \vx_{-i})}{\sum_{x_i} q(x_i, \vx_{-i})}. \margintag{using the definition of condition probability \eqref{eq:cond_prob} and the sum rule \eqref{eq:sum_rule}}
\end{align}
Under many models, this probability can be efficiently evaluated due to the conditioning on the remaining coordinates $\vx_{-i}$. If $X_i$ has finite support, the normalizer can be computed exactly.


\section{Sampling using Gradients}\label{sec:approximate_inference:mcmc:langevin_dynamics}

In this section, we discuss more advanced sampling methods.
The main idea that we will study is the interpretation of sampling as an optimization problem.
We will build towards an optimization view of sampling step-by-step, and first introduce what is commonly called the ``energy'' of a distribution.

\subsection{Gibbs Distributions}\label{sec:mcmc:uninformed:gibbs:distr}

Gibbs distributions are a special class of distributions that are widely used in machine learning, and which are characterized by an energy.

\begin{defn}[Gibbs distribution]\pidx{Gibbs distribution}
  Formally, a \emph{Gibbs distribution} (also called a \midx{Boltzmann distribution}) is a continuous distribution $p$ whose PDF is of the form \begin{align}
    p(\vx) = \frac{1}{Z} \exp(- f(\vx)).
  \end{align} $f : \R^n \to \R$ is also called an \midx{energy function}.
  When the energy function~$f$ is convex, its Gibbs distribution is called \midx<log-concave>{log-concave distribution}.
\end{defn}

A useful property is that Gibbs distributions always have full support.\footnote{This can easily be seen as $\exp(\cdot) > 0$.}
It is often easier to reason about ``energies'' rather than probabilities as they are neither restricted to be non-negative nor do they have to integrate to $1$.
Note that the Gibbs distribution belongs to the exponential family \eqref{eq:exponential_family_of_distributions} with sufficient statistic $-f(\vx)$.

Observe that the posterior distribution can always be interpreted as a Gibbs distribution as long as prior and likelihood have full support, \begin{align}
  p(\vtheta \mid \vx_{1:n}, y_{1:n}) &= \frac{1}{Z} p(\vtheta) p(y_{1:n} \mid \vx_{1:n}, \vtheta) \margintag{using Bayes' rule \eqref{eq:bayes_rule}} \nonumber \\
  &= \frac{1}{Z} \exp(-[-\log p(\vtheta) - \log p(y_{1:n} \mid \vx_{1:n}, \vtheta)]).
\end{align}
Thus, defining the energy function \begin{align}
  f(\vtheta) &\defeq -\log p(\vtheta) - \log p(y_{1:n} \mid \vx_{1:n}, \vtheta) \\
  &= -\log p(\vtheta) - \sum_{i=1}^n \log p(y_i \mid \vx_i, \vtheta),
\end{align} yields \begin{align}
  p(\vtheta \mid \vx_{1:n}, y_{1:n}) = \frac{1}{Z} \exp(- f(\vtheta)).
\end{align}
Note that $f$ coincides with the loss function used for MAP estimation \eqref{eq:map}.
For a noninformative prior, the regularization term vanishes and the energy reduces to the negative log-likelihood $\ell_\mathrm{nll}(\vtheta; \spD)$ (i.e., the loss function of maximum likelihood estimation \eqref{eq:mle}).

Using that the posterior is a Gibbs distribution, we can rewrite the acceptance distribution of Metropolis-Hastings, \begin{align}
  \alpha(\vxp \mid \vx) = \min \braces*{1, \frac{r(\vx \mid \vxp)}{r(\vxp \mid \vx)} \exp(f(\vx) - f(\vxp))}. \margintag{this is obtained by substituting the PDF of a Gibbs distribution for the posterior}
\end{align}

\begin{ex}{Metropolis-Hastings with Gaussian proposals}{metropolis_hastings_gaussian_proposals}
  Let us consider Metropolis-Hastings with the Gaussian proposal distribution, \begin{align}
    r(\vxp \mid \vx) \defeq \N[\vxp]{\vx}{\tau \mI}.
  \end{align}
  Due to the symmetry of Gaussians, we have \begin{align*}
    \frac{r(\vx \mid \vxp)}{r(\vxp \mid \vx)} = \frac{\N[\vx]{\vxp}{\tau \mI}}{\N[\vxp]{\vx}{\tau \mI}} = 1.
  \end{align*}
  Hence, the acceptance distribution is defined by \begin{align}
    \alpha(\vxp \mid \vx) = \min \braces*{1, \exp(f(\vx) - f(\vxp))}.
  \end{align}
  Intuitively, when a state with lower energy is proposed, that is ${f(\vxp) \leq f(\vx)}$, then the proposal will always be accepted.
  In contrast, if the energy of the proposed state is higher, the acceptance probability decreases exponentially in the difference of energies ${f(\vx) - f(\vxp)}$.
  Thus, Metropolis-Hastings minimizes the energy function, which corresponds to minimizing the negative log-likelihood and negative log-prior.
  The variance in the proposals~$\tau$ helps in getting around local optima, but the search direction is uniformly random (i.e., ``uninformed'').
\end{ex}

\begin{marginfigure}
  \incplt{langevin}
  \caption{Metropolis-Hastings and Langevin dynamics minimize the energy function $f(\theta)$ shown in blue.
  Suppose we start at the black dot $\theta_0$, then the black and red arrows denote possible subsequent samples.
  Metropolis-Hastings uses an ``uninformed'' search direction, whereas Langevin dynamics uses the gradient of $f(\theta)$ to make ``more promising'' proposals.
  The random proposals help get past local optima.}
\end{marginfigure}

\subsection{From Energy to Surprise (and back)}\label{sec:mcmc:energy_surprise}

Energy-based models are a well-known class of models in machine learning where an energy function $f$ is learned from data.
These energy functions do not need to originate from a probability distribution, yet they induce a probability distribution via their Gibbs distribution $p(\vx) \propto \exp(-f(\vx))$.
As we will see in \cref{exercise:maximum_entropy_property_of_gibbs_distribution}, this Gibbs distribution is the associated maximum entropy distribution.
Observe that the surprise about $\vx$ under this distribution is given by \begin{align}
  \S{p(\vx)} = f(\vx) + \log Z.
\end{align}
That is, up to a constant shift, the energy of $\vx$ \emph{coincides} with the surprise about $\vx$.
Energies are therefore sufficient for comparing the ``likelihood'' of points, and they do not require normalization.\footnote{Intuitively, an energy can be used to compare the ``likelihood'' of two points $\vx$ and $\vxp$ whereas the probability $\vx$ makes a statement about the ``likelihood'' of $\vx$ relative to \emph{all} other points.}

What kind of energies could we use?
In \cref{sec:mcmc:uninformed:gibbs:distr}, we discussed the use of the negative log-posterior or negative log-likelihood as energies.
In general, \emph{any} loss function $\ell(\vx)$ can be thought of as an energy function with an associated maximum entropy distribution $p(\vx) \propto \exp(-\ell(\vx))$.

\subsection{Langevin Dynamics}

Until now, we have looked at Metropolis-Hastings algorithms with proposal distributions that do not explicitly take into account the curvature of the energy function around the current state. Langevin dynamics adapts the Gaussian proposals of the Metropolis-Hastings algorithm we have seen in \cref{ex:metropolis_hastings_gaussian_proposals} to search the state space in an ``informed'' direction.
The simple idea is to bias the sampling towards states with lower energy, thereby making it more likely that a proposal is accepted.

A natural idea is to shift the proposal distribution perpendicularly to the gradient of the energy function.
This yields the following proposal distribution, \begin{align}
  r(\vxp \mid \vx) = \N[\vxp]{\vx - \eta_t \grad f(\vx)}{2 \eta_t \mI}. \label{eq:mala}
\end{align}
The resulting variant of Metropolis-Hastings is known as the \midx{Metropolis adjusted Langevin algorithm} (MALA) or \midx{Langevin Monte Carlo} (LMC).
It can be shown that, as $\eta_t \to 0$, we have for the acceptance probability $\alpha(\vxp \mid \vx) \to 1$ using that the acceptance probability is $1$ if $\vxp = \vx$.
Hence, the Metropolis-Hastings acceptance step can be omitted once the rejection probability becomes negligible.
The algorithm which always accepts the proposal of \cref{eq:mala} is known as the \midx{unadjusted Langevin algorithm} (ULA).

Observe that if the stationary distribution is the posterior distribution \begin{align*}
  p(\vtheta \mid \vx_{1:n}, y_{1:n}) = \frac{1}{Z} \exp\underbrace{\parentheses*{\log p(\vtheta) + \sum_{i=1}^n \log p(y_i \mid \vx_i, \vtheta)}}_{-f(\vtheta)}
\end{align*} with energy $f$ as we discussed in \cref{sec:mcmc:uninformed:gibbs:distr}, then the proposal $\vtheta'$ of MALA/LMC can be equivalently formulated as \begin{align}\begin{split}
  \vtheta' &\gets \vtheta - \eta_t \grad f(\vtheta) + \vvarepsilon \\
  &= \vtheta + \eta_t \parentheses*{\grad \log p(\vtheta) + \sum_{i=1}^n \grad \log p(y_i \mid \vx_i, \vtheta)} + \vvarepsilon \label{eq:lmc}
\end{split}\end{align} where $\vvarepsilon \sim \N{\vzero}{2\eta_t\mI}$.

\begin{rmk}{ULA is a discretized diffusion}{wiener_process}
  The unadjusted Langevin algorithm can be seen as a discretization of Langevin dynamics, which is a continuous-time stochastic process with a drift and with random stationary and independent Gaussian increments.
  The randomness is modeled by a Wiener process.

  \begin{defn}[Wiener process]
    The \midx{Wiener process}[idxpagebf] (also known as \midx{Brownian motion}) is a sequence of random vectors $\{\rW_t\}_{t \geq 0}$ such that\looseness=-1 \begin{enumerate}
      \item $\rW_0 = \vzero$,
      \item with probability $1$, $\rW_t$ is continuous in $t$,
      \item the process has independent increments,\safefootnote{That is, the ``future'' increments $\rW_{t+u} - \rW_t$ for $u \geq 0$ are independent of past values $\rW_s$ for $s < t$.} and
      \item $\rW_{t+u} - \rW_t \sim \N{\vzero}{u \mI}$.
    \end{enumerate}
  \end{defn}

  Consider the continuous-time stochastic process $\vtheta$ defined by the stochastic differential equation (SDE) \begin{align}
    d \vtheta_t = \underbrace{- \grad f(\vtheta_t) \,d t}_{\text{drift}} + \underbrace{\sqrt{2} \,d \rW_t}_{\text{noise}}, \label{eq:cont_time_langevin}
  \end{align}
  Such a stochastic process is also called a \midx<diffusion (process)>{diffusion process} and \cref{eq:cont_time_langevin} specifically is called \midx{Langevin dynamics}.
  Here, the first term is called the ``drift'' of the process, and the second term is called its ``noise''.
  Note that if the noise term is zero then \cref{eq:cont_time_langevin} is simply an ordinary differential equation (ODE).

  A diffusion can be discretized using the Euler-Maruyama method (also called ``forward Euler'') to obtain a discrete approximation $\vtheta_k \approx \vtheta(\tau_k)$ where $\tau_k$ denotes the $k$-th time step.
  Choosing the time steps such that $\Delta t_k \defeq \tau_{k+1} - \tau_{k} = \eta_k$ yields the approximation \begin{align}
    \vtheta_{k+1} = \vtheta_k - \grad f(\vtheta_k) \,\Delta t_k + \sqrt{2} \,\Delta \rW_k
  \end{align} where $\Delta \rW_k \defeq \rW_{\tau_{k+1}} - \rW_{\tau_k} \sim \N{\vzero}{\Delta t_k \mI}$.
  Observe that this coincides with the update rule of Langevin dynamics from \cref{eq:lmc}.
\end{rmk}

The appearance of drift and noise is closely related to the \midx{principle of curiosity and conformity}, we encountered in the previous chapter on variational inference.
The noise term (also called the \midx{diffusion}) leads to exploration of the state space (i.e., curiosity about alternative explanations of the data), whereas the drift term (also called the \midx{distillation}) leads to minimizing the energy or loss (i.e., conformity to the data).
Interestingly, this same principle appears in both variational inference and MCMC, two very different approaches to approximate probabilistic inference.
In the remainder of this manuscript we will find this to be a reoccurring theme.

For log-concave distributions, the mixing time of the Markov chain underlying Langevin dynamics can be shown to be polynomial in the dimension $n$ \citep{vempala2019rapid}.
You will prove this result for strongly log-concave distributions in \exerciserefmark{langevin_dynamics_convergence} and see that the analysis is analogous to the canonical convergence analysis of classical optimization schemes.

\subsection{Stochastic Gradient Langevin Dynamics}

Note that computing the gradient of the energy function, which corresponds to computing exact gradients of the log-prior and log-likelihood, in every step can be expensive.
The proposal step of MALA/LMC can be made more efficient by approximating the gradient with an unbiased gradient estimate, leading to \emph{stochastic gradient Langevin dynamics} (SGLD) shown in \cref{alg:sgld} \citep{welling2011bayesian}.
Observe that SGLD \eqref{eq:sgld} differs from MALA/LMC \eqref{eq:lmc} only by using a sample-based approximation of the gradient.

\begin{algorithm}
  \caption{Stochastic gradient Langevin dynamics, SGLD}\pidx{stochastic gradient Langevin dynamics}\label{alg:sgld}
  initialize $\vtheta$\;
  \For{$t = 1$ \KwTo $T$}{
    sample $i_1, \dots, i_m \sim \Unif{\{1, \dots, n\}}$ independently\;
    sample $\vvarepsilon \sim \N{\vzero}{2 \eta_t \mI}$\;
    $\vtheta \gets \vtheta + \eta_t \parentheses*{\grad \log p(\vtheta) + \frac{n}{m} \sum_{j=1}^m \grad \log p(y_{i_j} \mid \vx_{i_j}, \vtheta)} + \vvarepsilon$ \algeq{eq:sgld}
  }
\end{algorithm}

Intuitively, in the initial phase of the algorithm, the stochastic gradient term dominates, and therefore, SGLD corresponds to a variant of stochastic gradient ascent. In the later phase, the update rule is dominated by the injected noise $\vvarepsilon$, and will effectively be Langevin dynamics. SGLD transitions smoothly between the two phases.

Under additional assumptions, SGLD is guaranteed to converge to the posterior distribution for decreasing learning rates ${\eta_t = \Theta(t^{-\nicefrac{1}{3}})}$ \citep{raginsky2017non,xu2018global}.
SGLD does not use the acceptance step from Metropolis-Hastings as asymptotically, SGLD corresponds to Langevin dynamics and the Metropolis-Hastings rejection probability goes to zero for a decreasing learning rate.


\subsection{Hamiltonian Monte Carlo}

As MALA and SGLD can be seen as a sampling-based analogue of GD and SGD, a similar analogue for (stochastic) gradient descent with momentum is the (stochastic gradient) \midx{Hamiltonian Monte Carlo} (HMC) algorithm, which we discuss in the following \citep{duane1987hybrid,sghmc}.\looseness=-1

\begin{figure}
  \incfig{optimization_algorithms}
  \caption{A commutative diagram of sampling and optimization algorithms.
  Langevin dynamics (LD) is the non-stochastic variant of SGLD.}
\end{figure}

We have seen that if we want to sample from a distribution \begin{align*}
  p(\vx) \propto \exp(-f(\vx))
\end{align*} with energy function $f$, we can construct a Markov chain whose distribution converges to $p$.
We have also seen that for this approach to work, the chain must move through all areas of significant probability with reasonable speed.

If one is faced with a distribution $p$ which is multimodal (i.e., that has several ``peaks''), one has to ensure that the chain will explore all modes, and can therefore ``jump between different areas of the space''.

So in general, \emph{local} updates are doomed to fail.
Methods such as Metropolis-Hastings with Gaussian proposals, or even Langevin Monte Carlo might face this issue, as they do not jump to distant areas of the state space with significant acceptance probability.
It will therefore take a long time to move from one peak to another.

The HMC algorithm is an instance of Metropolis-Hastings which uses momentum to propose distant points that conserve energy, with high acceptance probability.
The general idea of HMC is to \emph{lift} samples $\vx$ to a higher-order space by considering an auxiliary variable $\vy$ with the same dimension as $\vx$.
We also lift the distribution $p$ to a distribution on the $(\vx, \vy)$-space by defining a distribution $p(\vy \mid \vx)$ and setting $p(\vx, \vy) \defeq p(\vy \mid \vx) p(\vx)$.
It is common to pick $p(\vy \mid \vx)$ to be a Gaussian with zero mean and variance $m\mI$.
Hence, \begin{align}
  p(\vx, \vy) \propto \exp\parentheses*{-\frac{1}{2m}\norm{\vy}_2^2 - f(\vx)}.
\end{align}
Physicists might recognize the above as the canonical distribution of a Newtonian system if one takes $\vx$ as the position and $\vy$ as the momentum.
$H(\vx, \vy) \defeq \frac{1}{2m}\norm{\vy}_2^2 + f(\vx)$ is called the \midx{Hamiltonian}.
HMC then takes a step in this higher-order space according to the Hamiltonian dynamics,\footnote{That is, HMC follows the trajectory of these dynamics for some time.}\looseness=-1 \begin{align}
  \odv{\vx}{t} = \grad_\vy H, \quad \odv{\vy}{t} = - \grad_\vx H, \label{eq:hamiltonian_dynamics}
\end{align} reaching some new point $(\vxp, \vyp)$ and \emph{projecting} back to the state space by selecting $\vxp$ as the new sample.
This is illustrated in \cref{fig:hmc}.
In the next iteration, we resample the momentum $\vyp \sim p(\cdot \mid \vxp)$ and repeat the procedure.

\begin{figure}
  \incplt{hmc}
  \caption{Illustration of Hamiltonian Monte Carlo. Shown is the contour plot of a distribution $p$, which is a mixture of two Gaussians, in the $(x,y)$-space.

  First, the initial point in the state space is lifted to the $(x,y)$-space. Then, we move according to Hamiltonian dynamics and finally project back onto the state space.}\label{fig:hmc}
\end{figure}

In an implementation of this algorithm, one has to solve \cref{eq:hamiltonian_dynamics} numerically rather than exactly.
Typically, this is done using the \midx{Leapfrog method}, which for a step size $\tau$ computes \begin{subequations}\begin{align}
  \vy(t + \nicefrac{\tau}{2}) &= \vy(t) - \frac{\tau}{2} \grad_\vx f(\vx(t)) \label{eq:leapfrog_y1} \\
  \vx(t + \tau) &= \vx(t) + \frac{\tau}{m} \vy(t + \nicefrac{\tau}{2}) \label{eq:leapfrog_x} \\
  \vy(t + \tau) &= \vy(t + \nicefrac{\tau}{2}) - \frac{\tau}{2} \grad_\vx f(\vx(t + \tau)).
\end{align}\label{eq:leapfrog}\end{subequations}
Then, one repeats this procedure $L$ times to arrive at a point $(\vxp, \vyp)$.
To correct for the resulting discretization error, the proposal is either accepted or rejected in a final Metropolis-Hastings acceptance step.
If the proposal distribution is symmetric (which we will confirm in a moment), the acceptance probability is \begin{align}
  \alpha((\vxp, \vyp) \mid (\vx, \vy)) \defeq \min\{1, \exp(H(\vxp, \vyp) - H(\vx, \vy))\}. \label{eq:hmc_acceptance_prob}
\end{align}
It follows that $p(\vx, \vy)$ is the stationary distribution of the Markov chain underlying HMC.
Due to the independence of $\vx$ and $\vy$, this also implies that the projection to $\vx$ yields a Markov chain with stationary distribution $p(\vx)$.

So why is the proposal distribution symmetric?
This follows from the time-reversibility of Hamiltonian dynamics.
It is straightforward to check that the dynamics from \cref{eq:hamiltonian_dynamics} are identical if we replace $t$ with $-t$ and $\vy$ with $-\vy$.
Intuitively, unlike the position $\vx$, the momentum $\vy$ is reversed when time is reversed as it depends on the velocity which is the time-derivative of the position.\footnote{The momentum of the $i$-th coordinate is $y_i = m_i v_i$ where $m_i$ is the mass and $v_i = \odv{x_i}{t}$ is the velocity.}
In simpler terms, time-reversibility states that if we observe the evolution of a system (e.g., two billiard balls colliding), we cannot distinguish whether we are observing the system evolve forward or backward in time.
The Leapfrog method maintains the time-reversibility of the dynamics.

Symmetry of the proposal distribution is ensured by proposing the point $(\vxp, -\vyp)$.\footnote{More formally, the proposal distribution is the Dirac delta at $(\vxp, -\vyp)$.}
Intuitively, this is simply to ensure that the system is run backward in time as often as it is run forward in time.
Recall that the momentum is resampled before each iteration (i.e., the proposed momentum is ``discarded'') and observe that $p(\vxp, -\vyp) = p(\vxp, \vyp)$,\footnote{We use here that $p(\vy \mid \vx)$ was chosen to be symmetric around zero.} so we can safely disregard the direction of time when computing the acceptance probability in \cref{eq:hmc_acceptance_prob}.

\section*{Discussion}

To summarize, Markov chain Monte Carlo methods use a Markov chain to approximately sample from an intractable distribution.
Note that unlike for variational inference, the convergence of many methods can be guaranteed.
Moreover, for log-concave distributions (e.g., with Bayesian logistic regression), the underlying Markov chain converges quickly to the stationary distribution.
Methods such as Langevin dynamics and Hamiltonian Monte Carlo aim to accelerate mixing by proposing points with a higher acceptance probability than Metropolis-Hastings with ``undirected'' Gaussian proposals.
Nevertheless, in general, the convergence (mixing time) may be slow, meaning that, in practice, accuracy and efficiency have to be traded.

\begin{oreadings}
  \begin{itemize}
    \item \pcite{ma2019sampling}
    \item \pcite{teh2016consistency}
    \item \pcite{sghmc}
  \end{itemize}
\end{oreadings}

\excheading

\begin{nexercise}{Markov chain update}{mc_update}
  Prove \cref{eq:mc_update}, i.e., that one iteration of the Markov chain can be expressed as ${\vq_{t+1} = \vq_t \mP}$.
\end{nexercise}

\begin{nexercise}{$k$-step transitions}{mc_multi_step_transitions}
  Prove that the entry $\mP^k(x,x')$ corresponds to the probability of transitioning from state $x \in \sS$ to state $x' \in \sS$ in exactly $k$ steps.
\end{nexercise}

\begin{nexercise}{Finding stationary distributions}{finding_stationary_distributions}
  A news station classifies each day as ``good'', ``fair'', or ``poor'' based on its daily ratings which fluctuate with what is occurring in the news.
  Moreover, the following table shows the probabilistic relationship between the type of the current day and the probability of the type of the next day conditioned on the type of the current day.

  \vspace{5pt}
  \begin{center}
    \begin{tabular}{ll|ccc}
      \toprule
      && \multicolumn{3}{c}{next day} \\
      && good & fair & poor \\
      \midrule
      \multirow{3}{*}{current day} & good & $0.60$ & $0.30$ & $0.10$ \\
      & fair & $0.50$ & $0.25$ & $0.25$ \\
      & poor & $0.20$ & $0.40$ & $0.40$ \\
      \bottomrule
    \end{tabular}
  \end{center}
  \vspace{5pt}

  In the long run, what percentage of news days will be classified as ``good''?\looseness=-1
\end{nexercise}

\begin{nexercise}{Example of Metropolis-Hastings}{metropolis_hastings}
  Consider the state space $\{0,1\}^n$ of binary strings having length $n$.
  Let the proposal distribution be $r(x' \mid x) = 1/n$ if $x'$ differs from $x$ in exactly one bit and $r(x' \mid x) = 0$ otherwise.
  Suppose we desire a stationary distribution $p$ for which $p(x)$ is proportional to the number of ones that occur in the bit string $x$.
  For example, in the long run, a random walk should visit a string having five $1$s five times as often as it visits a string having only a single $1$.
  Provide a general formula for the acceptance probability $\alpha(x' \mid x)$ that would be used if we were to obtain the desired stationary distribution used the Metropolis-Hastings algorithm.
\end{nexercise}

\begin{marginbox}[30\baselineskip]{Gamma distribution}
  The PDF of the \midx{gamma distribution} $\GammaDistr{\alpha}{\beta}$ is defined as \begin{align*}
    \GammaDistr[x]{\alpha}{\beta} \propto x^{\alpha-1} e^{- \beta x}, \quad x \in \R_{>0}.
  \end{align*}
  A random variable $X \sim \GammaDistr{\alpha}{\beta}$ measures the waiting time until $\alpha > 0$ events occur in a Poisson process with rate $\beta > 0$.
  In particular, when $\alpha = 1$ then the gamma distribution coincides with the exponential distribution with rate $\beta$.
\end{marginbox}

\begin{nexercise}{Practical examples of Gibbs sampling}{gibbs_sampling}
  In this exercise, we look at some examples where Gibbs sampling is useful.\looseness=-1

  \begin{enumerate}
    \item Consider the distribution \begin{align*}
      p(x, y) \defeq {n \choose x}y^{x + \alpha - 1}(1-y)^{n - x + \beta - 1}, \quad x \in [n], y \in [0,1].
    \end{align*}
    Convince yourself that it is hard to sample directly from $p$ and prove that it is an easy task if one uses Gibbs sampling. That is, show that the conditional distributions $p(x \mid y)$ and $p(y \mid x)$ are easy to sample from.\par
    \textit{Hint: Take a look at the Beta distribution \eqref{eq:beta_distr}.}

    \item Consider the following generative model $p(\mu, \lambda, x_{1:n})$ given by the likelihood $x_{1:n} \mid \mu, \lambda \iid \N{\mu}{\inv{\lambda}}$ and the independent priors \begin{align*}
      \mu \sim \N{\mu_0}{\inv{\lambda_0}} \quad\text{and}\quad \lambda \sim \GammaDistr{\alpha}{\beta}.
    \end{align*}
    We would like to sample from the posterior $p(\mu, \lambda \mid x_{1:n})$.
    Show that \begin{align*}
      \mu \mid \lambda, x_{1:n} \sim \N{m_\lambda}{\inv{l_\lambda}} \quad\text{and}\quad \lambda \mid \mu, x_{1:n} \sim \GammaDistr{a_\mu}{b_\mu},
    \end{align*} and derive $m_\lambda, l_\lambda, a_\mu, b_\mu$.
    Such a prior is called a \midx{semi-conjugate prior} to the likelihood, as the prior on $\mu$ is conjugate for any fixed value of $\lambda$ and vice-versa.

    \item Let us assume that $x_{1:n} \mid \alpha, c \iid \Pareto{\alpha}{c}$ and assume the improper prior $p(\alpha, c) \propto \Ind{\alpha, c > 0}$ which corresponds to a noninformative prior.
    Derive the posterior $p(\alpha, c \mid x_{1:n})$.
    Then, also derive the conditional distributions $p(\alpha \mid c, x_{1:n})$ and $p(c \mid \alpha, x_{1:n})$, and observe that they correspond to known distributions / are easy to sample from.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Energy function of Bayesian logistic regression}{bayesian_logistic_regression_energy_function}
  Recall from \cref{eq:logistic_regression} that the energy function of Bayesian logistic regression is \begin{align}
    f(\vw) = \lambda \norm{\vw}_2^2 + \sum_{i=1}^n \log(1 + \exp(-y_i \transpose{\vw} \vx_i)),
  \end{align} which coincided with the standard optimization objective of (regularized) logistic regression.

  Show that the posterior distribution of Bayesian logistic regression is log-concave.\looseness=-1
\end{nexercise}

\begin{nexercise}{Maximum entropy property of Gibbs distribution}{maximum_entropy_property_of_gibbs_distribution}
  \begin{enumerate}
    \item Let $X$ be a random variable supported on the finite set $\spT \subset \R$.\footnote{The same result can be shown to hold for arbitrary compact subsets.}
    Show that the Gibbs distribution with energy function $\frac{1}{T} f(x)$ for some temperature scalar $T \in \R$ is the distribution with maximum entropy of all distributions supported on $\spT$ that satisfy the constraint $\E{f(X)} < \infty$.\par
    \textit{Hint: Solve the dual problem (analogously to \cref{exercise:mep_and_posteriors}).}

    \item What happens for $T \to \{0, \infty\}$?
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Energy reduction of Gibbs sampling}{gibbs_sampling_energy_reduction}
  Let $p(\vx)$ be a probability density over $\mathbb{R}^d$, which we want to sample from.
  Assume that $p$ is a Gibbs distribution with energy function $f : \mathbb{R}^d \to \mathbb{R}$.

  In this exercise, we will study a single round of Gibbs sampling with initial state $\vx$ and final state $\vxp$ where \begin{align*}
    x'_j = \begin{cases}
      x'_i & \text{if $j = i$} \\
      x_j & \text{otherwise} \\
    \end{cases}
  \end{align*} for some fixed index $i$ and $x'_i \sim p(\cdot \mid \vx_{-i})$.

  Show that \begin{align}
    \E[x'_i \sim p(\cdot \mid \vx_{-i})]{f(\vxp)} \leq f(\vx) - \S{p(x_i \mid \vx_{-i})} + \H{p(\cdot \mid \vx_{-i})}.
  \end{align}
  That is, the energy is expected to decrease if the surprise of $x_i$ given $\vx_{-i}$ is larger than the expected surprise of the new $x'_i$ given~$\vx_{-i}$, i.e., $\S{p(x_i \mid \vx_{-i})} \geq \H{p(\cdot \mid \vx_{-i})}$.

  \textit{Hint: Recall the framing of Gibbs sampling as a variant of Metropolis-Hastings and relate this to the acceptance distribution of Metropolis-Hastings when $p$ is a Gibbs distribution.}
\end{nexercise}

\begin{nexercise}{Mixing time of Langevin dynamics}{langevin_dynamics_convergence}
  In this exercise, we will show that for certain Gibbs distributions, ${p(\vtheta) \propto \exp(-f(\vtheta))}$, Langevin dynamics is rapidly mixing.
  To do this, we will observe that Langevin dynamics can be seen as a continuous-time optimization algorithm in the space of distributions.

  First, we consider a simpler and more widely-known optimization algorithm, namely the \midx{gradient flow} \begin{align}
    d \vx_t = - \grad f(\vx_t) \,d t. \label{eq:gradient_flow}
  \end{align}
  Note that gradient descent is simply the discrete-time approximation of gradient flow just as ULA is the discrete-time approximation of Langevin dynamics.
  In the analysis of ODEs such as the gradient flow, so-called \midx<Lyapunov functions>{Lyapunov function} are commonly used to prove convergence of $\vx_t$ to a fixed point (also called an \midx{equilibrium}).

  Let us assume that $f$ is $\alpha$-strongly convex for some $\alpha > 0$, that is, \begin{align}
    f(\vy) \geq f(\vx) + \transpose{\grad f(\vx)}(\vy - \vx) + \frac{\alpha}{2} \norm{\vy - \vx}_2^2 \quad \forall \vx, \vy \in \R^n. \label{eq:strongly_convex}
  \end{align}
  In words, $f$ is lower bounded by a quadratic function with curvature~$\alpha$.
  Moreover, assume w.l.o.g. that $f$ minimized at $f(\vzero) = 0$.\footnote{This can always be achieved by shifting the coordinate system and subtracting a constant from $f$.}
  \begin{enumerate}
    \item Show that $f$ satisfies the \midx<Polyak-ojasiewicz (PL) inequality>{Polyak-ojasiewicz inequality}, i.e., \begin{align}
      f(\vx) \leq \frac{1}{2 \alpha} \norm{\grad f(\vx)}_2^2 \quad \forall \vx \in \R^n. \label{eq:polyak_lojasiewicz}
    \end{align}

    \item Prove $\odv{}{t} f(\vx_t) \leq - 2 \alpha f(\vx_t)$.
  \end{enumerate}
  Thus, $\vzero$ is the fixed point of \cref{eq:gradient_flow} and the Lyapunov function~$f$ is monotonically decreasing along the trajectory of $\vx_t$.
  We recall \midx{Grnwall's inequality} which states that for any real-valued continuous functions $g(t)$ and $\beta(t)$ on the interval $[0, T] \subset \R$ such that ${\odv{}{t} g(t) \leq \beta(t) g(t)}$ for all $t \in [0, T]$ we have \begin{align}
    g(t) \leq g(0) \exp\parentheses*{\int_0^t \beta(s) \,d s} \quad \forall t \in [0, T]. \label{eq:groenwall}
  \end{align}
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item Conclude that $f(\vx_t) \leq e^{-2 \alpha t} f(\vx_0)$.
  \end{enumerate} \vspace{\baselineskip}

  Now that we have proven the convergence of gradient flow using $f$ as Lyapunov function, we will follow the same template to prove the convergence of Langevin dynamics to the distribution $p(\vtheta) \propto \exp(-f(\vtheta))$.
  We will use that the evolution of $\{\vtheta_t\}_{t \geq 0}$ following the Langevin dynamics \eqref{eq:cont_time_langevin} is equivalently characterized by their densities $\{q_t\}_{t \geq 0}$ following the \midx{Fokker-Planck equation} \begin{align}
    \pdv{q_t}{t} = \dive (q_t \grad f) + \lapl q_t. \label{eq:fokker_planck}
  \end{align}
  Here, $\dive$ and $\lapl$ are the divergence and Laplacian operators, respectively.\footnote{For ease of notation, we omit the explicit dependence of $q_t$, $p$, and $f$ on $\vtheta$.}
  Intuitively, the first term of the Fokker-Planck equation corresponds to the drift and its second term corresponds to the diffusion~(i.e., the Gaussian noise).

  \begin{rmk}{Intuition on vector calculus}{}
    Recall that the divergence $\dive \vF$ of a vector field $\vF$ measures the change of volume under the flow of $\vF$. That is, if in the small neighborhood of a point $\vx$, $\vF$ points towards $\vx$, then the divergence at $\vx$ is negative as the volume shrinks. If $\vF$ points away from $\vx$, then the divergence at $\vx$ is positive as the volume increases. \\
    The Laplacian $\lapl \varphi = \dive (\grad \varphi)$ of a scalar field $\varphi$ can be understood intuitively as measuring ``heat dissipation''. That is, if $\varphi(\vx)$ is smaller than the average value of $\varphi$ in a small neighborhood of $\vx$, then the Laplacian at $\vx$ is positive.

    Regarding the Fokker-Planck equation \eqref{eq:fokker_planck}, the second term $\lapl q_t$ can therefore be understood as locally dissipating the probability mass of $q_t$ (which is due to the diffusion term in the SDE).
    On the other hand, the term $\dive (q_t \grad f)$ can be understood as a Laplacian of $f$ ``weighted'' by $q_t$. Intuitively, the vector field $\grad f$ moves flow in the direction of high energy, and hence, its divergence is larger in regions of lower energy and smaller in regions of higher energy. This term therefore corresponds to a drift from regions of high energy to regions of low energy.
  \end{rmk}\vspace{2ex}

  \begin{enumerate}
    \setcounter{enumi}{3}
    \item Show that $\lapl q_t = \dive (q_t \grad \log q_t)$, implying that the Fokker-Planck equation simplifies to \begin{align}
      \pdv{q_t}{t} = \dive \parentheses*{q_t \grad \log \frac{q_t}{p}}. \label{eq:simp_fokker_planck}
    \end{align}
    \textit{Hint: The Laplacian of a scalar field $\varphi$ is $\lapl \varphi \defeq \dive (\grad \varphi)$.}
  \end{enumerate}
  Observe that the Fokker-Planck equation already implies that $p$ is indeed a stationary distribution, as if $q_t = p$ then $\pdv{q_t}{t} = 0$.
  Moreover, note the similarity of the integrand of $\KL{q_t}{p}$, $q_t \log \frac{q_t}{p}$, to \cref{eq:simp_fokker_planck}.
  We will therefore use the KL-divergence as Lyapunov function.
  \begin{enumerate}
    \setcounter{enumi}{4}
    \item Prove $\odv{}{t} \KL{q_t}{p} = - \Fisher{q_t}{p}$.
    Here, \begin{align}
      \Fisher{q_t}{p} \defeq \E[\vtheta \sim q_t]{\norm{\grad \log \frac{q_t(\vtheta)}{p(\vtheta)}}_2^2}
    \end{align} denotes the \midx{relative Fisher information} of $p$ with respect to $q_t$.
    \textit{Hint: For any distribution $q$ on $\R^n$, \begin{align}
      \int_{\R^n} (\dive q \vF) \varphi \,d \vx = - \int_{\R^n} q \; \grad \varphi \cdot \vF \,d \vx \label{eq:divergence_theorem_hint}
    \end{align} follows for any vector field $\vF$ and scalar field $\varphi$ from the divergence theorem and the product rule of the divergence operator.}
  \end{enumerate}
  Thus, the relative Fisher information can be seen as the negated time-derivative of the KL-divergence, and as $\Fisher{q_t}{p} \geq 0$ it follows that the KL-divergence is decreasing along the trajectory.

  The \midx{log-Sobolev inequality} (LSI) is satisfied by a distribution $p$ with a constant $\alpha > 0$ if for all $q$: \begin{align}
    \KL{q}{p} \leq \frac{1}{2 \alpha} \Fisher{q}{p}.
  \end{align}
  It is a classical result that if $f$ is $\alpha$-strongly convex then $p$ satisfies the LSI with constant $\alpha$ \citep{bakry2006diffusions}.
  \begin{enumerate}
    \setcounter{enumi}{5}
    \item Show that if $f$ is $\alpha$-strongly convex for some $\alpha > 0$ (we say that $p$ is ``strongly log-concave''), then $\KL{q_t}{p} \leq e^{-2 \alpha t} \KL{q_0}{p}$.
    \item Conclude that under the same assumption on $f$, Langevin dynamics is rapidly mixing, i.e., $\tau_{\mathrm{TV}}(\epsilon) \in \BigO{\mathrm{poly}(n, \log(1 / \epsilon))}$.
  \end{enumerate}

  To summarize, we have seen that Langevin dynamics is an optimization scheme in the space of distributions, and that its convergence can be analyzed analogously to classical optimization schemes.
  Notably, in this exercise we have studied continuous-time Langevin dynamics.
  Convergence guarantees for discrete-time approximations can be derived using the same techniques.
  If this interests you, refer to \icite{vempala2019rapid}.
\end{nexercise}

\begin{nexercise}{Hamiltonian Monte Carlo}{hmc}
  \begin{enumerate}
    \item Prove that if the dynamics are solved exactly (as opposed to numerically using the Leapfrog method), then the acceptance probability of the MH-step is always $1$.
    \item Prove that the Langevin Monte Carlo algorithm from \eqref{eq:mala} can be seen as a special case of HMC if only one Leapfrog step is used ($L = 1$) and $m = 1$.
  \end{enumerate}
\end{nexercise}
