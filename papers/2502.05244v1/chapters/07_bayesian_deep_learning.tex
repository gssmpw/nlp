\chapter{Deep Learning}\label{sec:bdl}

We began our journey through probabilistic machine learning with linear models.
In most practical applications, however, it is seen that models perform better when labels may nonlinearly depend on the inputs, and for this reason linear models are often used in conjunction with ``hand-designed'' nonlinear features.
Designing these features is costly, time-consuming, and requires expert knowledge.
Moreover, the design of such features is inherently limited by human comprehension and ingenuity.

\section{Artificial Neural Networks}

One widely used family of nonlinear functions
are \midx<artificial ``deep'' neural networks>{neural network},\footnote{In the following, we will refrain from using the characterizations ``artificial'' and ``deep'' for better readability.}
\begin{align}
  \vf : \R^d \to \R^k, \quad \vf(\vx; \vtheta) \defeq \vvarphi(\mW_L \vvarphi(\mW_{L-1} ( \cdots \vvarphi(\mW_1 \vx))))
\end{align} where $\vtheta \defeq [\mW_1, \dots, \mW_L]$ is a vector of \emph{weights} (written as matrices ${\mW_l \in \R^{n_l \times n_{l-1}}}$)\footnote{where $n_0 = d$ and $n_L = k$} and $\varphi : \R \to \R$ is a component-wise nonlinear function.
Thus, a deep neural network can be seen as nested (``deep'') linear functions composed with nonlinearities.
This simple kind of neural network is also called a \midx{multilayer perceptron}.

In this chapter, we will be focusing mostly on performing probabilistic inference with a \emph{given} neural network architecture.
To this end, understanding the basic architecture of a multilayer perceptron will be sufficient for us.
For a more thorough introduction to the field of deep learning and various architectures, you may refer to the books of \cite{goodfellow2016deep} and \cite{prince2023understanding}.

\begin{figure}
  \incfig{ann}
  \caption{Computation graph of a neural network with two hidden layers.}\label{fig:computation_graph}
\end{figure}

A neural network can be visualized by a \midx{computation graph}.
An example for such a computation graph is given in \cref{fig:computation_graph}.
The columns of the computation graph are commonly called \midx<layers>{layer}, whereby the left-most column is the \midx{input layer}, the right-most column is the \midx{output layer}, and the remaining columns are the \midx<hidden layers>{hidden layer}.
The inputs are (as we have previously) referred to as $\vx \defeq [x_1, \dots, x_d]$.
The outputs (i.e., vertices of the output layer) are often referred to as \midx{logits} and named $\vf \defeq [f_1, \dots, f_k]$.
The \midx<activations>{activation} of an individual (hidden) layer $l$ of the neural network are described by \begin{align}
  \vnu^{(l)} \defeq \vvarphi(\mW_l \vnu^{(l-1)})
\end{align} where $\vnu^{(0)} = \vx$.
The activation of the $i$-th node is $\nu_i^{(l)} = \vnu^{(l)}(i)$.

\subsection{Activation Functions}

The nonlinearity $\varphi$ is called an \midx{activation function}.
The following two activation functions are particularly common: \begin{enumerate}
  \item The \midx{hyperbolic tangent} (Tanh) is defined as \begin{align}
    \mathrm{Tanh}(z) \defeq \frac{\exp(z) - \exp(-z)}{\exp(z) + \exp(-z)} \in (-1, 1).
  \end{align}
  Tanh is a scaled and shifted variant of the sigmoid function \eqref{eq:logistic_function} which we have previously seen in the context of logistic regression as $\mathrm{Tanh}(z) = 2 \sigma(2 z) - 1$.

  \begin{marginfigure}
    \incplt{activation_functions}
    \caption{The Tanh and ReLU activation functions, respectively.}
  \end{marginfigure}

  \item The \midx{rectified linear unit} (ReLU) is defined as \begin{align}
    \mathrm{ReLU}(z) \defeq \max \{z, 0\} \in [0, \infty).
  \end{align}
  In particular, the ReLU activation function leads to ``sparser'' gradients as it selects the halfspace of inputs with positive sign.
  Moreover, the gradients of ReLU do not ``vanish'' as $z \to \pm \infty$ which can lead to faster training.
\end{enumerate}

It is important that the activation function is nonlinear because otherwise, any composition of layers would still represent a linear function.
Non-linear activation functions allow the network to represent arbitrary functions.
This is known as the \midx{universal approximation theorem}, and it states that any artificial neural network with just a single hidden layer (with arbitrary width) and non-polynomial activation function $\varphi$ can approximate any continuous function to an arbitrary accuracy.

\subsection{Classification}\label{sec:bdl:ann:classification}

\begin{marginfigure}
  \incplt{softmax}
  \caption{Softmax $\sigma_1(f_1, f_2)$ for a binary classification problem.
  Blue denotes a small probability and yellow denotes a large probability of belonging to class $1$, respectively.}
\end{marginfigure}

Although we mainly focus on regression, neural networks can equally well be used for classification.
If we want to classify inputs into $c$ separate classes, we can simply construct a neural network with $c$ outputs, $\vf = [f_1, \dots, f_c]$, and normalize them into a probability distribution.
Often, the \midx{softmax function} is used for normalization, \begin{align}
  \sigma_i(\vf) \defeq \frac{\exp(f_i)}{\sum_{j=1}^c \exp(f_j)} \label{eq:softmax_function}
\end{align} where $\sigma_i(\vf)$ corresponds to the probability mass of class $i$.
The softmax is a generalization of the logistic function \eqref{eq:logistic_function} to more than two classes~\exerciserefmark{softmax_and_logistic_function}.
Note that the softmax corresponds to a Gibbs distribution with energies $-\vf$.

\subsection{Maximum Likelihood Estimation}\label{sec:bnn:mle}

We will study neural networks under the lens of supervised learning~(cf. \cref{sec:fundamentals:supervised_learning}) where we are provided some independently-sampled (noisy) data $\spD = \{(\vx_i, \vy_i)\}_{i=1}^n$ generated according to an unknown process $\vy \sim p(\cdot \mid \vx, \opt{\vtheta})$, which we wish to approximate.

Upon initialization, the network does generally not approximate this process well, so a key element of deep learning is ``learning'' a parameterization $\vtheta$ that is a good approximation.
To this end, one typically considers a loss function $\ell(\vtheta; \vy)$ which quantifies the ``error'' of the network outputs $\vf(\vx; \vtheta)$.
In the classical setting of regression, i.e., $y \in \R$ and $k = 1$, $\ell$ is often taken to be the \midx<(empirical) mean squared error>{mean squared error}, \begin{align}
  \ell_\MSE(\vtheta; \spD) \defeq \frac{1}{n} \sum_{i=1}^n (f(\vx_i; \vtheta) - y_i)^2.
\end{align}
As we have already seen in \cref{sec:least_squares_as_mle} in the context of linear regression, minimizing mean squared error corresponds to maximum likelihood estimation under a Gaussian likelihood.

In the setting of classification where $\vy \in \{0,1\}^c$ is a one-hot encoding of class membership,\footnote{That is, exactly one component of $\vy$ is $1$ and all others are $0$, indicating to which class the given example belongs.} it is instead common to interpret the outputs of a neural network as probabilities akin to our discussion in \cref{sec:bdl:ann:classification}.
We denote by $q_\vtheta(\cdot \mid \vx)$ the resulting probability distribution over classes with PMF $[\sigma_1(\vf(\vx; \vtheta)), \dots, \sigma_c(\vf(\vx; \vtheta))]$, and aim to find $\vtheta$ such that $q_\vtheta(\vy \mid \vx) \approx p(\vy \mid \vx)$.
In this context, it is common to minimize the cross-entropy, \begin{align}
  \crH{p}{q_\vtheta} &= \E[(\vx, \vy) \sim p]{- \log q_\vtheta(\vy \mid \vx)} \margintag{using the definition of cross-entropy \eqref{eq:cross_entropy}} \nonumber \\
  &\approx \underbrace{- \frac{1}{n} \sum_{i=1}^n \log q_\vtheta(\vy_i \mid \vx_i)}_{\defeq \ell_\mathrm{ce}(\vtheta; \spD)} \margintag{using Monte Carlo sampling} \label{eq:ce_loss}
  \intertext{which can be understood as minimizing the surprise about the training data under the model.
  $\ell_\mathrm{ce}$ is called the \midx{cross-entropy loss}.
  Disregarding the constant $1/n$, we can rewrite the cross-entropy loss as}
  &\propto - \sum_{i=1}^n \log q_\vtheta(\vy_i \mid \vx_i) = \ell_\mathrm{nll}(\vtheta; \spD)
\end{align}
Recall that $\ell_\mathrm{nll}(\vtheta; \spD)$ is the \midx{negative log-likelihood}[idxpagebf] of the training data, and thus, empirically minimizing cross-entropy can equivalently be interpreted as maximum likelihood estimation.\footnote{We have previously seen this equivalence of MLE and empirically minimizing KL-divergence in \cref{sec:vi:kl:forward} (minimizing the cross-entropy $\crH{p}{q_\vtheta}$ is equivalent to minimizing forward-KL $\KL{p}{q_\vtheta}$). Note that this interpretation is not exclusive to the canonical cross-entropy loss from \cref{eq:ce_loss}, but holds for any MLE. For example, minimizing mean squared error corresponds to empirically minimizing the KL-divergence with a Gaussian likelihood.}
Furthermore, recall from \cref{exercise:logistic_loss_gradient} that for a two-class classification problem the cross-entropy loss is equivalent to the logistic loss.

\subsection{Backpropagation}\label{sec:bdl:ann:backprop}

A crucial property of neural networks is that they are differentiable.
That is, we can compute gradients $\grad_\vtheta \ell$ of $\vf$ with respect to the parameterization of the model $\vtheta = \mW_{1:L}$ and some loss function $\ell(\vf; \vy)$.
Being able to obtain these gradients efficiently allows for ``learning'' a particular function from data using first-order optimization methods.

The algorithm for computing gradients of a neural network is called \midx{backpropagation} and is essentially a repeated application of the chain rule.
Note that using the chain rule for every path through the network is computationally infeasible, as this quickly leads to a combinatorial explosion as the number of hidden layers is increased.
The key insight of backpropagation is that we can use the \midx<feed-forward>{feed-forward neural network} structure of our neural network to memoize computations of the gradient, yielding a linear time algorithm.
Obtaining gradients by backpropagation is often called \midx{automatic differentiation (auto-diff)}.
For more details, refer to \cite{goodfellow2016deep}.

Computing the exact gradient for each data point is still fairly expensive when the size of the neural network is large.
Typically, stochastic gradient descent is used to obtain unbiased gradient estimates using batches of only $m$ of the $n$ data points, where $m \ll n$.

\section{Bayesian Neural Networks}

\begin{marginfigure}[5\baselineskip]
  \incfig{bnn}
  \caption{Bayesian neural networks model a distribution over the weights of a neural network.}
\end{marginfigure}

How can we perform probabilistic inference in neural networks? We adopt the same strategy which we already used for Bayesian linear regression, we impose a Gaussian prior on the weights, \begin{align}
  \vtheta \sim \N{\vzero}{\sigmap^2 \mI}.
\end{align}
Similarly, we can use a Gaussian likelihood to describe how well the data is described by the model $\vf$, \begin{align}
  y \mid \vx, \vtheta \sim \N{f(\vx; \vtheta)}{\sigman^2}. \label{eq:bnn_likelihood}
\end{align}
Thus, instead of considering weights as point estimates which are learned exactly, \midx<Bayesian neural networks>{Bayesian neural network} learn a distribution over the weights of the network.
In principle, other priors and likelihoods can be used, yet Gaussians are typically chosen due to their closedness properties, which we have seen in \cref{sec:fundamentals:gaussians} and many times since.

\subsection{Maximum a Posteriori Estimation}\label{sec:bnn:map_inference}

Before studying probabilistic inference, let us first consider MAP estimation in the context of neural networks.

The MAP estimate of the weights is obtained by \begin{align}
  \vthetahat_\MAP = \argmax_\vtheta \log p(\vtheta) + \sum_{i=1}^n \log p(y_i \mid \vx_i, \vtheta). \label{eq:bnn:map}
\end{align}
In \cref{sec:bnn:mle}, we have seen that the negative log-likelihood under a Gaussian likelihood \eqref{eq:bnn_likelihood} is the squared error between label and prediction,\looseness=-1 \begin{align}
  \log p(y_i \mid \vx_i, \vtheta) = - \frac{(y_i - f(\vx_i; \vtheta))^2}{2 \sigman^2} + \const.
\end{align}
Obtaining the MAP estimate instead, simply corresponds to adding an $L_2$-regularization term to the squared error loss, \begin{align}
  \vthetahat_\MAP = \argmin_\vtheta \frac{1}{2 \sigmap^2} \norm{\vtheta}_2^2 + \frac{1}{2 \sigman^2} \sum_{i=1}^n (y_i - f(\vx_i; \vtheta))^2. \margintag{using that for an isotropic Gaussian prior, $\log p(\vtheta) = - \frac{1}{2 \sigmap^2} \norm{\vtheta}_2^2 + \const$}
\end{align}
As we have already seen in \cref{rmk:weight_decay} and the context of Bayesian linear regression (and ridge regression), using a Gaussian prior is equivalent to applying weight decay.\footnote{Recall that weight decay regularizes weights by shrinking them towards zero.}
Using gradient ascent, we obtain the following update rule, \begin{align}
  \vtheta \gets \vtheta(1 - \lambda \eta_t) + \eta_t \sum_{i=1}^n \grad \log p(y_i \mid \vx_i, \vtheta) \label{eq:bdl_map_update}
\end{align} where $\lambda \defeq \nicefrac{1}{\sigmap^2}$.
The gradients of the likelihood can be obtained using automatic differentiation.

\subsection{Heteroscedastic Noise}\label{sec:bdl:noise}

\Cref{eq:bnn_likelihood} uses the scalar parameter $\sigman^2$ to model the aleatoric uncertainty (label noise), similarly to how we modeled the label noise in Bayesian linear regression and Gaussian processes.
Such a noise model is called \midx{homoscedastic noise} as the noise is assumed to be uniform across the domain.
In many settings, however, the noise is inherently \midx{heteroscedastic noise}.
That is, the noise varies depending on the input and which ``region'' of the domain the input is from.
This behavior is visualized in \cref{fig:heteroscedastic_noise}.

\begin{marginfigure}
  \incplt{heteroscedastic_noise}
  \caption{Illustration of data with variable (heteroscedastic) noise.
  The noise increases as the inputs increase in magnitude.
  The noise-free function is shown in black.}\label{fig:heteroscedastic_noise}
\end{marginfigure}

There is a natural way of modeling heteroscedastic noise with Bayesian neural networks.
We use a neural network with two outputs $f_1$ and $f_2$ and define \begin{subequations}\begin{align}
  y \mid \vx, \vtheta &\sim \N{\mu(\vx; \vtheta)}{\sigma^2(\vx; \vtheta)} \quad\text{where} \label{eq:bnn_likelihood_het} \\
  \mu(\vx; \vtheta) &\defeq f_1(\vx; \vtheta), \\
  \sigma^2(\vx; \vtheta) &\defeq \exp(f_2(\vx; \vtheta)). \margintag{we exponentiate $f_2$ to ensure non-negativity of the variance}
\end{align}\end{subequations}

Using this model, the likelihood term from \cref{eq:bnn:map} is \begin{align}
  \log p(y_i \mid \vx_i, \vtheta) &= \log \N[y_i]{\mu(\vx_i; \vtheta)}{\sigma^2(\vx_i; \vtheta)} \nonumber \\
  &= \log \frac{1}{\sqrt{2 \pi \sigma^2(\vx_i; \vtheta)}} - \frac{(y_i - \mu(\vx_i; \vtheta))^2}{2 \sigma^2(\vx_i; \vtheta)} \margintag{note that the normalizing constant depends on the noise model!} \nonumber \\
  &= \underbrace{\log \frac{1}{\sqrt{2 \pi}}}_{\const} - \frac{1}{2} \brackets*{\log \sigma^2(\vx_i; \vtheta) + \frac{(y_i - \mu(\vx_i; \vtheta))^2}{\sigma^2(\vx_i; \vtheta)}}. \label{eq:bdl_map}
\end{align}
Hence, the model can either explain the label $y_i$ by an accurate model $\mu(\vx_i; \vtheta)$ or by a large variance $\sigma^2(\vx_i; \vtheta)$, yet, it is penalized for choosing large variances.
Intuitively, this allows to attenuate losses for some data points by attributing them to large variance (when no model reflecting all data points simultaneously can be found).
This allows the model to ``learn'' its aleatoric uncertainty.
However, recall that the MAP estimate still corresponds to a point estimate of the weights, so we forgo modeling the epistemic uncertainty.

\section{Approximate Probabilistic Inference}

Naturally, we want to understand the epistemic uncertainty of our model too.
However, learning and inference in Bayesian neural networks are generally intractable (even when using a Gaussian prior and likelihood) when the noise is not assumed to be homoscedastic and known.\footnote{In this case, the conjugate prior to a Gaussian likelihood is not a Gaussian. See, e.g., \url{https://en.wikipedia.org/wiki/Conjugate_prior}.}
Thus, we are led to the techniques of approximate inference, which we discussed in the previous two chapters.

\subsection{Variational Inference}\label{sec:bnn:approximate_inference:vi}\pidx{variational inference}

As we have discussed in \cref{sec:approximate_inference}, we can apply black box stochastic variational inference which --- in the context of neural networks --- is also known as \midx{Bayes by Backprop}.
As variational family, we use the family of independent Gaussians which we have already encountered in \cref{ex:var_family_diag_gaussian}.\footnote{Independent Gaussians are useful because they can be encoded using only $2d$ parameters, which is crucial when the size of the neural network is large.}
Recall the fundamental objective of variational inference~\eqref{eq:var_optimization},\looseness=-1 \begin{align*}
  &\argmin_{q \in \spQ} \KL{q}{p(\cdot \mid \vx_{1:n}, y_{1:n})} \\
  &= \argmax_{q \in \spQ} L(q, p; \spD) \margintag{using \cref{eq:elbo_reverse_kl_relationship}} \\
  &= \argmax_{q \in \spQ} \E[\vtheta \sim q]{\log p(y_{1:n} \mid \vx_{1:n}, \vtheta)} - \KL{q}{p(\cdot)}. \margintag{using \cref{eq:elbo2}}
\end{align*}
The KL-divergence $\KL{q}{p(\cdot)}$ can be expressed in closed-form for Gaussians.\footnote{see \cref{eq:kl_gaussian}}
Recall that we can obtain unbiased gradient estimates of the expectation using the reparameterization trick \eqref{eq:reparameterization_trick_application}, \begin{align*}
  \E[\vtheta \sim q]{\log p(y_{1:n} \mid \vx_{1:n}, \vtheta)} = \E[\vvarepsilon \sim \SN]{\left. \log p(y_{1:n} \mid \vx_{1:n}, \vtheta) \right\rvert_{\vtheta = \msqrt{\mSigma} \vvarepsilon + \vmu}}.
\end{align*}
As $\mSigma$ is the diagonal matrix ${\diag{\sigma_1^2, \dots, \sigma_d^2}}$, ${\msqrt{\mSigma} = \diag{\sigma_1, \dots, \sigma_d}}$.
The gradients of the likelihood can be obtained using backpropagation.
We can now use the variational posterior $q_\vlambda$ to perform approximate inference,\looseness=-1 \begin{align}
  p(\ys \mid \vxs, \vx_{1:n}, \vy_{1:n}) &= \int p(\ys \mid \vxs, \vtheta) p(\vtheta \mid \vx_{1:n}, y_{1:n}) \,d\vtheta \margintag{using the sum rule \eqref{eq:sum_rule} and product rule \eqref{eq:product_rule}} \nonumber \\
  &= \E[\vtheta \sim p(\cdot \mid \vx_{1:n}, y_{1:n})]{p(\ys \mid \vxs, \vtheta)} \margintag{interpreting the integral as an expectation over the posterior} \nonumber \\[5pt]
  &\approx \E[\theta \sim q_\vlambda]{p(\ys \mid \vxs, \vtheta)} \margintag{approximating the posterior with the variational posterior $q_\vlambda$} \nonumber \\
  &\approx \frac{1}{m} \sum_{i=1}^m p(\ys \mid \vxs, \vtheta^{(i)}) \margintag{using Monte Carlo sampling} \label{eq:bnn_vi}
  \intertext{for independent samples $\vtheta^{(i)} \iid q_\vlambda$,}
  &= \frac{1}{m} \sum_{i=1}^m \N[\ys]{\mu(\vxs; \vtheta^{(i)})}{\sigma^2(\vxs; \vtheta^{(i)})}. \label{eq:bnn_posterior_vi} \margintag{using the neural network}
\end{align}
Intuitively, variational inference in Bayesian neural networks can be interpreted as averaging the predictions of multiple neural networks drawn according to the variational posterior $q_\vlambda$.

Using the Monte Carlo samples $\vtheta^{(i)}$, we can also estimate the mean of our predictions, \begin{align}
  \E{\ys \mid \vxs, \vx_{1:n}, y_{1:n}} \approx \frac{1}{m} \sum_{i=1}^m \mu(\vxs; \vtheta^{(i)}) \eqdef \mean{\mu}(\vxs),
\end{align} and the variance of our predictions, \begin{align*}
  \Var{\ys \mid \vxs, \vx_{1:n}, y_{1:n}} &= \b{\E[\vtheta]{\Var[\ys]{\ys \mid \vxs, \vtheta}}} + \r{\Var[\vtheta]{\E[\ys]{\ys \mid \vxs, \vtheta}}} \margintag{using the law of total variance \eqref{eq:lotv}} \\
  &= \b{\E[\vtheta]{\sigma^2(\vxs; \vtheta)}} + \r{\Var[\vtheta]{\mu(\vxs; \vtheta)}}. \margintag{using the likelihood \eqref{eq:bnn_likelihood_het}}
\end{align*}
Recall from \cref{eq:lotv_interpretation} that the first term corresponds to the \b{aleatoric uncertainty} of the data and the second term corresponds to the \r{epistemic uncertainty} of the model.
We can approximate them using the Monte Carlo samples $\vtheta^{(i)}$, \begin{align}
  \Var{\ys \mid \vxs, \vx_{1:n}, y_{1:n}} \approx \begin{multlined}[t]\b{\frac{1}{m} \sum_{i=1}^m \sigma^2(\vxs; \vtheta^{(i)})} \\ + \r{\frac{1}{m-1} \sum_{i=1}^m (\mu(\vxs; \vtheta^{(i)}) - \mean{\mu}(\vxs))^2}\end{multlined}
\end{align} using a sample mean \eqref{eq:sample_mean} and sample variance \eqref{eq:sample_variance}.

\subsection{Markov Chain Monte Carlo}

As we have discussed in \cref{sec:approximate_inference:mcmc}, an alternative method to approximating the full posterior distribution is to sample from it directly.
By the ergodic theorem \eqref{eq:ergodic_theorem}, we can use any of the discussed sampling strategies to obtain samples $\vtheta^{(t)}$ such that \begin{align*}
  p(\ys \mid \vxs, \vx_{1:n}, \vy_{1:n}) \approx \frac{1}{T} \sum_{t = 1}^T p(\ys \mid \vxs, \vtheta^{(t)}). \label{eq:bnn_posterior_mcmc} \margintag{see \eqref{eq:mcmc}}
\end{align*}
Here, we omit the offset $t_0$ which is commonly used to avoid the ``burn-in'' period for simplicity.
Algorithms such as SGLD or SG-HMC are often used as they rely only on stochastic gradients of the loss function which can be computed efficiently using automatic differentiation.

Typically, for large networks, we cannot afford to store all $T$ samples of models.
Thus, we need to summarize the iterates.\footnote{That is, combine the individual samples of weights $\vtheta^{(i)}$.}
One approach is to keep track of $m$ \midx<snapshots>{snapshot} of weights $[\vtheta^{(1)}, \dots, \vtheta^{(m)}]$ according to some schedule and use those for inference (e.g., by averaging the predictions of the corresponding neural networks).
This approach of sampling a subset of some data is generally called \midx{subsampling}.

Another approach is to summarize (that is, approximate) the weights using sufficient statistics (e.g., a Gaussian).\footnote{A statistic is \idx<sufficient>{sufficient statistic} for a family of probability distributions if the samples from which it is calculated yield no more information than the statistic with respect to the learned parameters. We provide a formal definition in \cref{sec:mutual_information}.}
In other words, we learn the Gaussian approximation, \begin{subequations}\begin{align}
  \vtheta &\sim \N{\vmu}{\mSigma}, \quad\text{where} \\
  \vmu &\defeq \frac{1}{T} \sum_{i=1}^T \vtheta^{(i)}, \margintag{using a sample mean \eqref{eq:sample_mean}} \\
  \mSigma &\defeq \frac{1}{T - 1} \sum_{i=1}^T (\vtheta^{(i)} - \vmu) \transpose{(\vtheta^{(i)} - \vmu)}, \margintag{using a sample variance \eqref{eq:sample_variance}}
\end{align}\label{eq:iteraties_gaussian_sufficient_statistics}\end{subequations} using sample means and sample (co)variances.
This can be implemented efficiently using running averages of the first and second moments,\looseness=-1 \begin{align}
  \vmu \gets \frac{1}{T+1} (T \vmu + \vtheta) \quad\text{and}\quad \mA \gets \frac{1}{T+1} (T \mA + \vtheta \transpose{\vtheta})
\end{align} upon observing the fresh sample $\vtheta$.
$\mSigma$ can easily be calculated from these moments, \begin{align}
  \mSigma = \frac{T}{T-1} (\mA - \vmu \transpose{\vmu}). \margintag{using the characterization of sample variance in terms of estimators of the first and second moment \eqref{eq:sample_variance2}}
\end{align}
To predict, we can sample weights $\vtheta$ from the learned Gaussian.

\begin{rmk}{Stochastic weight averaging}{}
  It turns out that this approach works well even without injecting additional Gaussian noise during training, e.g., when using SGD rather than SGLD.
  Simply taking the mean of the iterates of SGD is called \midx{stochastic weight averaging} (SWA).
  Describing the iterates of SGD by Gaussian sufficient statistics (analogously to \cref{eq:iteraties_gaussian_sufficient_statistics}), is known as the \midx{stochastic weight averaging-Gaussian} (SWAG) method~\citep{izmailov2018averaging}.
\end{rmk}

\subsection{Dropout and Dropconnect}\label{sec:bnn:approximate_inference:dropout}

We will now discuss two approximate inference techniques that are tailored to neural networks.
The first is ``dropout''/``dropconnect'' regularization.
Traditionally, \midx{dropout regularization} \citep{hinton2012improving,srivastava2014dropout} randomly omits vertices of the computation graph during training, see \cref{fig:dropout_regularization_as_vi}.
In contrast, \midx{dropconnect regularization} \citep{wan2013regularization} randomly omits edges of the computation graph.
The key idea that we will present here is to interpret this type of regularization as performing variational inference.

\begin{marginfigure}[-8\baselineskip]
  \incfig{dropout}
  \caption{Illustration of dropout regularization. Some vertices of the computation graph are randomly omitted. In contrast, dropconnect regularization randomly omits edges of the computation graph.}\label{fig:dropout_regularization}
\end{marginfigure}
\begin{marginfigure}
  \incplt{dropout}
  \caption{Interpretation of dropconnect regularization as variational inference.
  The only coordinates where the variational posterior $q_j$ has positive probability are $0$ and $\lambda_j$.}\label{fig:dropout_regularization_as_vi}
\end{marginfigure}

In our exposition, we will focus on dropconnect, but the same approach also applies to dropout \citep{gal2016dropout}.
Suppose that we omit an edge of the computation graph (i.e., set its weight to zero) with probability $p$.
Then our variational posterior is given by \begin{align}
  q(\vtheta \mid \vlambda) \defeq \prod_{j=1}^d q_j(\theta_j \mid \lambda_j)
\end{align} where $d$ is the number of weights of the neural network and \begin{align}
  q_j(\theta_j \mid \lambda_j) \defeq p \delta_0(\theta_j) + (1-p)\delta_{\lambda_j}(\theta_j). \label{eq:dropconnect_var_family}
\end{align}
Here, $\delta_\alpha$ is the Dirac delta function with point mass at $\alpha$.\footnote{see \cref{sec:background:probability:dirac_delta}}
The variational parameters $\vlambda$ correspond to the ``original'' weights of the network.
In words, the variational posterior expresses that the $j$-th weight has value $0$ with probability $p$ and value $\lambda_j$ with probability $1-p$.
For fixed weights $\vlambda$, sampling from the variational posterior $q_{\vlambda}$ corresponds to sampling a vector $\vz$ with entries $\vz(i) \sim \Bern{p}$, yielding $\vz \odot \vlambda$ which is one of $2^d$ possible subnetworks.\footnote{$\mA \odot \mB$ denotes the Hadamard (element-wise) product.}

The weights $\vlambda$ can be learned by maximizing the ELBO, analogously to black-box variational inference.
The KL-divergence term of the ELBO is not tractable for the variational family described by \cref{eq:dropconnect_var_family}, instead a common approach is to use a mixture of Gaussians: \begin{align}
  q_j(\theta_j \mid \lambda_j) \defeq p \N[\theta_j]{0}{1} + (1-p) \N[\theta_j]{\lambda_j}{1}.
\end{align}
In this case, it can be shown that $\KL{q_{\vlambda}}{p(\cdot)} \approx \frac{p}{2} \norm{\vlambda}_2^2$ for sufficiently large $d$ \citep[proposition 1]{gal2015dropout}.
Thus, \begin{align}
  &\argmax_{\vlambda \in \Lambda} L(q_\vlambda, p; \spD) \nonumber \\
  &= \argmax_{\vlambda \in \Lambda} \E[\vtheta \sim q_\vlambda]{\log p(y_{1:n} \mid \vx_{1:n}, \vtheta)} - \KL{q}{p(\cdot)} \margintag{using \cref{eq:elbo2}} \nonumber \\
  &\approx \argmin_{\vlambda \in \Lambda} - \frac{1}{m} \sum_{i=1}^m \log p(y_{1:n} \mid \vx_{1:n}, \vtheta) \bigr\rvert_{\vtheta = \vz^{(i)} \odot \vlambda + \vvarepsilon^{(i)}} + \frac{p}{2} \norm{\vlambda}_2^2 \margintag{using Monte Carlo sampling} \label{eq:dropconnect_vi}
\end{align} where we reparameterize $\vtheta \sim q_\vlambda$ by $\vtheta = \vz \odot \vlambda + \vvarepsilon$ with $\vz(i) \sim \Bern{p}$ and $\vvarepsilon \sim \N{\vzero}{\mI}$.
\Cref{eq:dropconnect_vi} is the standard $L_2$-regularized loss function of a neural network with weights $\vlambda$ and dropconnect, and it is straightforward to obtain unbiased gradient estimates by automatic differentiation.\looseness=-1

Crucially, for the interpretation of dropconnect regularization as variational inference to be valid, we also need to perform dropconnect regularization during inference, \begin{align}
  p(\ys \mid \vxs, \vx_{1:n}, \vy_{1:n}) &\approx \E[\vtheta \sim q_\vlambda]{p(\ys \mid \vxs, \vtheta)} \nonumber \\
  &\approx \frac{1}{m} \sum_{i=1}^m p(\ys \mid \vxs, \vtheta^{(i)}) \label{eq:bnn_posterior_dr} \margintag{using Monte Carlo sampling}
\end{align} where $\vtheta^{(i)} \iid q_\vlambda$ are independent samples.
This coincides with our earlier discussion of variational inference for Bayesian neural networks in \cref{eq:bnn_vi}.
In words, we average the predictions of $m$ neural networks for each of which we randomly ``drop out'' weights.

\begin{rmk}{Masksembles}{masksembles}
  A practical problem of dropout is that for any reasonable choice of dropout probability, the dropout masks~$\vz^{(i)}$ will overlap significantly, which tends to make the predictions~$p(\ys \mid \vxs, \vtheta^{(i)})$ highly correlated.
  This can lead to underestimation of epistemic uncertainty.
  \midx<Masksembles>{masksemble}~\citep{durasov2021masksembles} mitigate this issue by choosing a fixed set of pre-defined dropout masks, which have controlled overlap, and alternating between them during training.
  In the extreme case of ``infinitely many'' masks, masksembles are equivalent to dropout since each mask is only seen once.
\end{rmk}

\subsection{Probabilistic Ensembles}\label{sec:bdl:approximate_inference:probabilistic_ensembles}

We have seen that variational inference in the context of Bayesian neural networks can be interpreted as averaging the predictions of $m$ neural networks drawn according to the variational posterior.

A natural adaptation of this idea is to immediately learn the weights of $m$ neural networks.
The idea is to randomly choose $m$ training sets by sampling uniformly from the data with replacement. Then, using our analysis from \cref{sec:bnn:map_inference}, we obtain $m$ MAP estimates of the weights~$\vtheta^{(i)}$, yielding the approximation \begin{align}
  p(\ys \mid \vxs, \vx_{1:n}, \vy_{1:n}) &= \E[\vtheta \sim p(\cdot \mid \vx_{1:n}, y_{1:n})]{p(\ys \mid \vxs, \vtheta)} \nonumber \\
  &\approx \frac{1}{m} \sum_{i=1}^m p(\ys \mid \vxs, \vtheta^{(i)}). \label{eq:bnn_posterior_pe} \margintag{using bootstrapping}
\end{align}
Here, the randomly chosen $m$ training sets induce ``diversity'' of the models.
In practice, in the context of deep neural networks where the global minimizer of the loss can rarely be identified, it is common to use the full training data to train each of the $m$ neural networks.
Random initialization and random shuffling of the training data is typically enough to ensure some degree of diversity between the individual models \citep{lakshminarayanan2017simple}.
We can connect ensembles to the other approximate inference techniques we have discussed:
First, ensembles can be seen as a specific kind of masksemble, where the masks are non-overlapping,\footnote{That is, the $m$ models do not share any of their parameters. Ensembles and dropout lie on opposite ends of this spectrum.} which mitigates the issue of correlated predictions from \cref{rmk:masksembles}.
Second, ensembling can be combined with other approximate inference techniques such as variational inference, Laplace approximation, or SWAG to get a mixture of Gaussians as the posterior approximation.

Note that \cref{eq:bnn_posterior_pe} is not equivalent to Monte Carlo sampling, although it looks very similar.
The key difference is that this approach does not sample from the true posterior distribution $p$, but instead from the empirical posterior distribution $\hat{p}$ given the (re-sampled) MAP estimates.
Intuitively, this can be understood as the difference between sampling from a distribution $p$ directly (Monte Carlo sampling) versus sampling from an approximate (empirical) distribution $\hat{p}$ (corresponding to the training data), which itself is constructed from samples of the true distribution $p$.
This approach is known as \midx{bootstrapping} or \midx{bagging} (short for \emph{b}ootstrap \emph{agg}regat\emph{ing}) and plays a central role in model-free reinforcement learning. We will return to this concept in~\cref{sec:tabular_rl:model_free:on_policy_value_estimation}.

\subsection{Diverse Probabilistic Ensembles}

Probabilistic ensembles can be loosely interpreted as randomly initializing $m$ ``particles''~$\{\vtheta^{(i)}\}_{i=1}^m$ and then pushing each particle towards regions of high posterior probability.
A potential issue with this approach is that if the initialization of particles is not sufficiently diverse, the particles may ``collapse'' since every particle eventually converges to a local optimum of the loss function.
A natural approach to mitigate this issue is to alter the objective of each particle from simply aiming to minimize the loss $- \log p(\vtheta \mid \vx_{1:n}, y_{1:n})$, which we will abbreviate by $- \log p(\vtheta)$, to also ``push'' the particles away from each other.
We will see next that this can be interpreted as a form of variational inference under a very flexible variational family.

In our discussion of \midx{variational inference}, we have seen that minimizing reverse-KL is equivalent to maximizing the evidence lower bound, and used this to derive an optimization algorithm to compute approximate posteriors.
We will discuss the alternative approach of directly computing the gradient of the KL-divergence.
Consider the variational family of all densities that can be expressed as smooth transformations of points sampled from a reference density $\phi$. That is, we consider $\spQ_\phi \defeq \{\pf{\vT}{\phi} \mid \text{$\vT : \Theta \to \Theta$ is smooth}\}$ where $\pf{\vT}{\phi}$ is the density of the random vector $\vtheta' = \vT(\vtheta)$ with $\vtheta \sim \phi$.\footnote{Refer back to \cref{sec:fundamentals:probability:cov} for a recap on pushforwards.}
The density $\phi$ can be thought of as the initial distribution of the particles, and the smooth map $\vT$ as the dynamics that push the particles towards the target density $p$.
It can be shown that for ``almost any'' reference density $\phi$, this variational family $\spQ_\phi$ is expressive enough to closely approximate ``almost arbitrary'' distributions.\footnote{For a more detailed discussion, refer to \icite{liu2016stein}.}
A natural approach is therefore to \emph{learn} the appropriate smooth map $\vT$ between the reference density $\phi$ and the target density $p$.

\begin{ex}{Gaussian variational inference}{}
  We have seen in \cref{sec:approximate_inference:variational_inference:elbo} that if $\phi$ is standard normal and $\vT(\vx; \{\vmu, \msqrt{\mSigma}\}) = \vmu + \msqrt{\mSigma} \vx$ an affine transformation then the ELBO can be maximized efficiently using stochastic gradient descent.
  However, in this case, $\spQ_\phi$ can only approximate Gaussian-like distributions since the expressivity of the map $\vT$ is limited under the fixed reference density $\phi$.
\end{ex}

An alternative approach to Gaussian variational inference is the following algorithm known as \midx{Stein variational gradient descent} (SVGD), where we recursively apply carefully chosen smooth maps to the current variational approximation: \begin{align}
  q_0 \xrightarrow{\opt{\vT_0}} q_1 \xrightarrow{\opt{\vT_1}} q_2 \xrightarrow{\opt{\vT_2}} \cdots \qquad\text{where $q_{t+1} \defeq \pf{\opt{\vT_t}}{q_t}$}. \label{eq:svgd_sketch}
\end{align}
We consider maps $\vT = \rvec{id} + \vf$ where $\rvec{id}(\vtheta) \defeq \vtheta$ denotes the identity map and $\vf(\vx)$ represents a (small) perturbation.
Recall that at time $t$ we seek to minimize $\KL{\pf{\vT}{q_t}}{p}$, so we choose the smooth map as \begin{align}
  \opt{\vT_t} \defeq \rvec{id} - \eta_t\! \left. \grad_\vf \KL{\pf{\vT}{q_t}}{p} \right\rvert_{\vf = \vzero} \label{eq:svgd_update}
\end{align} where $\eta_t$ is a step size.
In this way, the SVGD update \eqref{eq:svgd_update} can be interpreted as a step of ``functional'' gradient descent.

To be able to compute the gradient of the KL-divergence, we need to make some structural assumptions on the perturbation $\vf$.
SVGD assumes that $\vf = \transpose{[f_1 \; \cdots \; f_d]}$ with $f_i \in \spH_k(\Theta)$ is from some reproducing kernel Hilbert space $\spH_k(\Theta)$ of a positive definite kernel $k$; we say $\vf \in \spH_k^d(\Theta)$.
Within the RKHS, we can compute the gradient of the KL-divergence exactly.
\cite{liu2016stein} show that in this case, the functional gradient of the KL-divergence can be expressed in closed-form as $\left. -\grad_\vf \KL{\pf{\vT}{q}}{p} \right\rvert_{\vf = \vzero} = \opt{\vvarphi_{q,p}}$ where \begin{align}
  \opt{\vvarphi_{q,p}}(\cdot) \defeq \E[\vtheta \sim q]{k(\cdot,\vtheta) \grad_{\vtheta} \log p(\vtheta) + \grad_{\vtheta} k(\cdot,\vtheta)}.
\end{align}
SVGD then approximates $q$ using the particles $\{\vtheta^{(i)}\}_{i=1}^m$ as follows:

\begin{algorithm}
  \caption{Stein variational gradient descent, SVGD}\label{alg:svgd}
  initialize particles $\{\vtheta^{(i)}\}_{i=1}^m$\;
  \Repeat{converged}{
    \For{each particle $i \in [m]$}{
      $\vtheta^{(i)} \gets \vtheta^{(i)} + \eta_t \opt{\vvarphihat_{q,p}}(\vtheta^{(i)})$ where $\opt{\vvarphihat_{q,p}}(\vtheta) \defeq \frac{1}{m} \sum_{j=1}^m \Big[k(\vtheta,\vtheta^{(j)}) \grad_{\vtheta} \log p(\vtheta) + \grad_{\vtheta^{(j)}} k(\vtheta,\vtheta^{(j)})\Big]$\;
    }
  }
\end{algorithm}

Often, a Gaussian kernel~\eqref{eq:gaussian_kernel} with length scale $h$ is used to model the perturbations, in which case the repulsion term is \begin{align}
  \grad_{\vtheta^{(j)}} k(\vtheta,\vtheta^{(j)}) = \frac{1}{h^2} (\vtheta - \vtheta^{(j)}) k(\vtheta,\vtheta^{(j)})
\end{align} and the negative functional gradient simplifies to \begin{align}
  \opt{\vvarphihat_{q,p}}(\vtheta) = \frac{1}{m} \sum_{j=1}^m k(\vtheta,\vtheta^{(j)}) \Big[ \underbrace{\grad_{\vtheta} \log p(\vtheta)}_{\text{drift}} + \underbrace{h^{-2} (\vtheta - \vtheta^{(j)})}_{\text{repulsion}} \Big].
\end{align}

Note that SVGD has similarities to Langevin dynamics, which as seen in \exerciserefmark{langevin_dynamics_convergence} can also be interpreted as following a gradient of the KL-divergence.
Whereas Langevin dynamics perturbs particles according to a drift towards regions of high posterior probability and some random diffusion (cf.~\cref{eq:cont_time_langevin}), the first term of $\opt{\vvarphihat_{q,p}}(\vtheta)$ perturbs particles to drift towards regions of high posterior probability while the second term leads to a mutual ``repulsion'' of particles.
Notably, the perturbations of Langevin dynamics are noisy, while SVGD perturbs particles deterministically and the randomness is exclusively in the initialization of particles.
The repulsion term prevents particles from collapsing to a single mode of the posterior distribution, which is a possible failure mode of other particle-based posterior approximations such as ensembles.\looseness=-1

Note that the above decomposition of $\opt{\vvarphihat_{q,p}}(\vtheta)$ is once more an example of the \midx{principle of curiosity and conformity} which we have seen to be a recurring theme in approaches to approximate inference.
The repulsion term leads to exploration of the particles (i.e., ``curiosity'' about alternative explanations), while the drift term leads to minimization of the loss (i.e., ``conformity'' to the data).\looseness=-1

\cite{lu2019scaling} show that under some assumptions, SVGD converges asymptotically to the target density $p$ as $\eta_t \to 0$.
SVGD's name originates from \midx{Stein's method} which is a general-purpose approach for characterizing convergence in distribution.\footnote{For an introduction to Stein's method, read \icite{gorham2015measuring}.}



\section{Calibration}\pidx{calibration}

A key challenge of Bayesian deep learning (and also other probabilistic methods) is the calibration of models.
We say that a model is \emph{well-calibrated} if its confidence coincides with its accuracy across many predictions.
Consider a classification model that predicts that the label of a given input belongs to some class with probability $80\%$.
If the model is well-calibrated, then the prediction is correct about $80\%$ of the time.
In other words, during calibration, we adjust the probability estimation of the model.

We will first mention two methods of estimating the calibration of a model, namely the marginal likelihood and reliability diagrams.
Then, in \cref{sec:bdl:calibration:improving}, we survey commonly used heuristics for empirically improving the calibration.

\subsection{Evidence}

A popular method (which we already encountered multiple times) is to use the evidence of a validation set $\vx_{1:m}^\val$ of size $m$ given the training set $\vx_{1:n}^\train$ of size $n$ for estimating the model calibration.
Here, the evidence can be understood as describing how well the validation set is described by the model trained on the training set.
We obtain, \begin{align}
  &\log p(y_{1:m}^\val \mid \vx_{1:m}^\val, \vx_{1:n}^\train, y_{1:n}^\train) \nonumber \\
  &= \log \int p(y_{1:m}^\val \mid \vx_{1:m}^\val, \vtheta) p(\vtheta \mid \vx_{1:n}^\train, y_{1:n}^\train) \,d\vtheta \margintag{using the sum rule \eqref{eq:sum_rule} and product rule \eqref{eq:product_rule}} \nonumber \\
  &\approx \log \int p(y_{1:m}^\val \mid \vx_{1:m}^\val, \vtheta) q_\vlambda(\vtheta) \,d\vtheta \margintag{approximating with the variational posterior} \nonumber \\
  &= \log \int \prod_{i=1}^m p(y_i^\val \mid \vx_i^\val, \vtheta) q_\vlambda(\vtheta) \,d\vtheta \margintag{using the independence of the data}
  \intertext{The resulting integrals are typically very small which leads to numerical instabilities. Therefore, it is common to maximize a lower bound to the evidence instead,}
  &= \log \E[\vtheta \sim q_\vlambda]{\prod_{i=1}^m p(y_i^\val \mid \vx_i^\val, \vtheta)} \margintag{interpreting the integral as an expectation over the variational posterior} \nonumber \\
  &\geq \E[\vtheta \sim q_\vlambda]{\sum_{i=1}^m \log p(y_i^\val \mid \vx_i^\val, \vtheta)} \margintag{using Jensen's inequality \eqref{eq:jensen}} \\
  &\approx \frac{1}{k} \sum_{j=1}^k \sum_{i=1}^m \log p(y_i^\val \mid \vx_i^\val, \vtheta^{(j)}) \margintag{using Monte Carlo sampling}
\end{align} for independent samples $\vtheta^{(j)} \iid q_\vlambda$.

\subsection{Reliability Diagrams}\pidx{reliability diagram}

Reliability diagrams take a frequentist perspective to estimate the calibration of a model.
For simplicity, we assume a calibration problem with two classes, $1$ and $-1$ (similarly to logistic regression).\footnote{Reliability diagrams generalize beyond this restricted example.}

We group the predictions of a validation set into $M$ interval bins of size $\nicefrac{1}{M}$ according to the class probability predicted by the model, $\Pr{Y_i = 1 \mid \vx_i}$.
We then compare within each bin, how often the model thought the inputs belonged to the class (confidence) with how often the inputs actually belonged to the class (frequency).
Formally, we define $\sB_m$ as the set of samples falling into bin $m$ and let \begin{align}
  \mathrm{freq}(\sB_m) \defeq \frac{1}{\card{\sB_m}} \sum_{i \in \sB_m} \Ind{Y_i = 1}
\end{align} be the proportion of samples in bin $m$ that belong to class $1$ and let \begin{align}
  \mathrm{conf}(\sB_m) \defeq \frac{1}{\card{\sB_m}} \sum_{i \in \sB_m} \Pr{Y_i = 1 \mid \vx_i}
\end{align} be the average confidence of samples belonging to class $1$ within the bin~$m$.\looseness=-1

\begin{marginfigure}
  \incplt{reliability_diagrams}
  \caption{Examples of reliability diagrams with ten bins.
  A perfectly calibrated model approximates the diagonal dashed red line.
  The first reliability diagram shows a well-calibrated model.
  In contrast, the second reliability diagram shows an overconfident model.}\label{fig:reliability_diagram}
\end{marginfigure}

Thus, a model is well calibrated if $\mathrm{freq}(B_m) \approx \mathrm{conf}(B_m)$ for each bin $m \in [M]$.
There are two common metrics of calibration that quantify how ``close'' a model is to being well calibrated.

\begin{enumerate}
  \item The \midx{expected calibration error} (ECE) is the average deviation of a model from perfect calibration, \begin{align}
    \ell_{\mathrm{ECE}} \defeq \sum_{m=1}^M \frac{\card{\sB_m}}{n} \abs{\mathrm{freq}(\sB_m) - \mathrm{conf}(\sB_m)}
  \end{align} where $n$ is the size of the validation set.

  \item The \midx{maximum calibration error} (MCE) is the maximum deviation of a model from perfect calibration among all bins, \begin{align}
    \ell_{\mathrm{MCE}} \defeq \max_{m \in [M]} \abs{\mathrm{freq}(\sB_m) - \mathrm{conf}(\sB_m)}.
  \end{align}
\end{enumerate}

\subsection{Heuristics for Improving Calibration}\label{sec:bdl:calibration:improving}

We now survey a few heuristics which can be used empirically to improve model calibration. \begin{enumerate}
  \item \midx<Histogram binning>{histogram binning} assigns a calibrated score $q_m \defeq \mathrm{freq}(\sB_m)$ to each bin during validation.
  Then, during inference, we return the calibrated score $q_m$ of the bin corresponding to the prediction of the model.

  \item \midx<Isotonic regression>{isotonic regression} extends histogram binning by using variable bin boundaries.
  We find a piecewise constant function $\vf \defeq [f_1, \dots, f_M]$ that minimizes the bin-wise squared loss, \begin{subequations}\begin{align}
    \min_{M, \vf, \va} \quad&\sum_{m=1}^M \sum_{i=1}^n \Ind{a_m \leq \Pr{Y_i = 1 \mid \vx_i} < a_{m+1}} (f_m - y_i)^2 \\
    \text{subject to} \quad&0 = a_1 \leq \cdots \leq a_{M+1} = 1, \\
    &f_1 \leq \cdots \leq f_M
  \end{align}\end{subequations} where $\vf$ are the calibrated scores and $\va \defeq [a_1, \dots, a_{M+1}]$ are the bin boundaries.
  We then return the calibrated score $f_m$ of the bin corresponding to the prediction of the model.

  \item \midx{Platt scaling} adjusts the logits $z_i$ of the output layer to \begin{align}
    q_i \defeq \sigma(a z_i + b)
  \end{align} and then learns parameters $a, b \in \R$ to maximize the likelihood.

  \item \midx<Temperature scaling>{temperature scaling} is a special and widely used instance of Platt scaling where $a \defeq \nicefrac{1}{T}$ and $b \defeq 0$ for some temperature scalar ${T > 0}$,\looseness=-1 \begin{align}
    q_i \defeq \sigma\parentheses*{\frac{z_i}{T}}.
  \end{align}
  Intuitively, for a larger temperature $T$, the probability is distributed more evenly among the classes (without changing the ranking), yielding a more uncertain prediction.
  In contrast, for a lower temperature $T$, the probability is concentrated more towards the top choices, yielding a less uncertain prediction.
  As seen in \cref{exercise:maximum_entropy_property_of_gibbs_distribution}, temperature scaling can be motivated as tuning the mean of the softmax distribution.
\end{enumerate}
\vspace{\baselineskip}

\begin{marginfigure}
  \incplt{temperature_scaling}
  \caption{Illustration of temperature scaling for a classifier with three classes.
  On the top, we have a prediction with a high temperature, yielding a very uncertain prediction (in favor of class $A$).
  Below, we have a prediction with a low temperature, yielding a prediction that is strongly in favor of class $A$.
  Note that the ranking ($A \succ C \succ B$) is preserved.}
\end{marginfigure}

\begin{oreadings}
  \begin{itemize}
    \item \pcite{guo2017calibration}
    \item \pcite{blundell2015weight}
    \item \pcite{kendall2017uncertainties}
  \end{itemize}
\end{oreadings}

\section*{Discussion}

This chapter concludes our discussion of (approximate) probabilistic inference.
Across the last three chapters, we have seen numerous methods for approximating the posterior distributions of deep neural networks: \begin{itemize}
  \item Methods such as dropout and stochastic weight averaging are frequently used in practice.
  Other particle-based approaches such as ensembles and SVGD are used less frequently since they are computationally more expensive to train, but are some of the most effective methods in estimating uncertainty.

  \item Recently, Laplace approximations regained interest since they can be applied ``post-hoc'' after training simply by computing or approximating the Hessian of the loss function~\citep{daxberger2021laplace,antoran2022adapting}.
  Still, Laplace approximations come with the limitations inherent to unimodal Gaussian approximations.

  \item Other work, particularly in fine-tuning, has explored approximating the posterior distribution of deep neural networks by treating them as linear functions in a fixed learned feature space, in which case one can use the tools for exact probabilistic inference from \cref{sec:blr,sec:gp}~\citep[e.g.,][]{hubotter2025efficiently}.
\end{itemize}
Despite large progress in approximate inference over the past decade, efficient and reliable uncertainty estimation of large models remains an important open challenge.

\excheading

\begin{nexercise}{Softmax is a generalization of the logistic function}{softmax_and_logistic_function}
  Show that for a two-class classification problem (i.e., $c = 2$), the softmax function is equivalent to the logistic function \eqref{eq:logistic_function} for the univariate model $f \defeq f_1 - f_0$.
  That is, $\sigma_1(\vf) = \sigma(f)$ and $\sigma_0(\vf) = 1 - \sigma(f)$.

  Thus, the softmax function is a generalization of the logistic function to more than two classes.
\end{nexercise}
