\chapter{Variational Inference}\label{sec:approximate_inference}

We have seen how to perform (efficient) probabilistic inference with Gaussians, exploiting their closed-form formulas for marginal and conditional distributions.
But what if we work with other distributions?

In this and the following chapter, we will discuss two methods of approximate inference.
We begin by discussing variational (probabilistic) inference, which aims to find a good approximation of the posterior distribution from which it is easy to sample.
In \cref{sec:approximate_inference:mcmc}, we discuss Markov chain Monte Carlo methods, which approximate the sampling from the posterior distribution directly.

The fundamental idea behind variational inference is to approximate the true posterior distribution using a ``simpler'' posterior that is as close as possible to the true posterior:
\begin{align}
  p(\vtheta \mid \vx_{1:n}, y_{1:n}) = \frac{1}{Z} p(\vtheta, y_{1:n} \mid \vx_{1:n}) \approx q(\vtheta \mid \vlambda) \eqdef q_\vlambda(\vtheta)
\end{align} where $\vlambda$ represents the parameters of the \midx{variational posterior} $q_\vlambda$, also called \midx{variational parameters}.
In doing so, variational inference reduces probabilistic inference --- where the fundamental difficulty lies in solving high-dimensional integrals --- to an optimization problem.
Optimizing (stochastic) objectives is a well-understood problem with efficient algorithms that perform well in practice.\footnote{We provide an overview of first-order methods such as stochastic gradient descent in \cref{sec:fundamentals:optimization}.}

\section{Laplace Approximation}\label{sec:approximate_inference:laplace_approximation}

Before introducing a general framework of variational inference, we discuss a simpler method of approximate inference known as \midx{Laplace's method}.
This method was proposed as a method of approximating integrals as early as 1774 by Pierre-Simon Laplace.
The idea is to use a Gaussian approximation (that is, a second-order Taylor approximation) of the posterior distribution around its mode.
Let \begin{align}
  \psi(\vtheta) \defeq \log p(\vtheta \mid \vx_{1:n}, y_{1:n}) \label{eq:log_posterior}
\end{align} denote the log-posterior.
Then, using a second-order Taylor approximation \eqref{eq:second_order_exp} around the mode $\vthetahat$ of $\psi$ (i.e., the MAP estimate), we obtain the approximation $\hat{\psi}$ which is accurate for $\vtheta \approx \vthetahat$:\looseness=-1 \begin{align}
  \psi(\vtheta) \approx \hat{\psi}(\vtheta) &\defeq \psi(\vthetahat) + \transpose{(\vtheta - \vthetahat)} \grad \psi(\vthetahat) + \frac{1}{2} \transpose{(\vtheta - \vthetahat)} \hes_\psi(\vthetahat) (\vtheta - \vthetahat) \nonumber\\
  &= \psi(\vthetahat) + \frac{1}{2} \transpose{(\vtheta - \vthetahat)} \hes_\psi(\vthetahat) (\vtheta - \vthetahat). \margintag{using $\grad \psi(\vthetahat) = 0$}
\end{align}

Compare this expression to the log-PDF of a Gaussian: \begin{align}
  \log \N[\vtheta]{\vthetahat}{\inv{\mLambda}} = -\frac{1}{2}\transpose{(\vtheta - \vthetahat)}\mLambda(\vtheta - \vthetahat) + \const.
\end{align}
Since $\psi(\vthetahat)$ is constant with respect to $\vtheta$, \begin{align}
  \hat{\psi}(\vtheta) = \log \N[\vtheta]{\vthetahat}{- \inv{\hes_\psi(\vthetahat)}} + \const.
\end{align}
The \midx{Laplace approximation} $q$ of $p$ is \begin{subequations}\begin{align}
  q(\vtheta) &\defeq \N[\vtheta]{\vthetahat}{\inv{\mLambda}} \propto \exp(\hat{\psi}(\vtheta)) \qquad\text{where} \\
  \mLambda &\defeq - \hes_\psi(\vthetahat) = - \hes_\vtheta \log p(\vtheta \mid \vx_{1:n}, y_{1:n}) \bigl|_{\vtheta = \vthetahat}.
\end{align}\end{subequations}
Recall that for this approximation to be well-defined, the covariance matrix $\inv{\mLambda}$ (or equivalently the precision matrix $\mLambda$) needs to be symmetric and positive semi-definite.
Let us verify that this is indeed the case for sufficiently smooth $\psi$.\footnote{$\psi$ being twice continuously differentiable around $\vthetahat$ is sufficient.}
In this case, the Hessian $\mLambda$ is symmetric since the order of differentiation does not matter.
Moreover, by the second-order optimality condition, $\hes_\psi(\vthetahat)$ is negative semi-definite since $\vthetahat$ is a maximum of $\psi$, which implies that $\mLambda$ is positive semi-definite.\looseness=-1

\begin{ex}{Laplace approximation of a Gaussian}{}
  Consider approximating the Gaussian density $p(\vtheta) = \N[\vtheta]{\vmu}{\mSigma}$ using a Laplace approximation.

  We know that the mode of $p$ is $\vmu$, which we can verify by computing the gradient, \begin{align}
    \grad_\vtheta \log p(\vtheta) = -\frac{1}{2} (2 \inv{\mSigma} \vtheta - 2 \inv{\mSigma} \vmu) \overset{!}{=} 0 \iff \vtheta = \vmu.
  \end{align}
  For the Hessian of $\log p(\vtheta)$, we get \begin{align}
    \hes_\vtheta \log p(\vtheta) = \transpose{(\jac_\vtheta (\inv{\mSigma} \vmu - \inv{\mSigma} \vtheta))} = - \transpose{(\inv{\mSigma})} = - \inv{\mSigma}. \margintag{using $\transpose{(\inv{\mA})} = \inv{(\transpose{\mA})}$ and symmetry of $\mSigma$}
  \end{align}
  We see that the Laplace approximation of a Gaussian $p(\vtheta)$ is exact, which should not come as a surprise since the second-order Taylor approximation of $\log p(\vtheta)$ is exact for Gaussians.\looseness=-1
\end{ex}

\begin{marginfigure}
  \incplt{laplace_approx}
  \caption{The Laplace approximation $q$ greedily selects the mode of the true posterior distribution $p$ and matches the curvature around the mode $\hat{p}$.
  As shown here, the Laplace approximation can be extremely overconfident when $p$ is not approximately Gaussian.}
  \label{fig:laplace_approx_overconfident}
\end{marginfigure}

The Laplace approximation matches the shape of the true posterior around its mode but may not represent it accurately elsewhere --- often leading to extremely overconfident predictions.
An example is given in \cref{fig:laplace_approx_overconfident}.
Nevertheless, the Laplace approximation has some desirable properties such as being relatively easy to apply in a post-hoc manner, that is, after having already computed the MAP estimate.
It preserves the MAP point estimate as its mean and just ``adds'' a little uncertainty around it.
However, the fact that it can be arbitrarily different from the true posterior makes it unsuitable for approximate probabilistic inference.\looseness=-1

\subsection{Example: Bayesian Logistic Regression}\label{sec:approximate_inference:bayesian_logistic_regression}

As an example, we will look at Laplace approximation in the context of Bayesian logistic regression.
Logistic regression learns a classifier that decides for a given input whether it belongs to one of two classes.
{\def\par{\let\par\endgraf}\begin{marginfigure}
  \incplt{logistic_function}
  \caption{The logistic function squashes the linear function $\transpose{\vw}\vx$ onto the interval $(0,1)$.}\label{fig:logistic_function}
\end{marginfigure}}%
A sigmoid function, typically the \midx{logistic function}, \begin{align}
  \sigma(z) \defeq \frac{1}{1+\exp(-z)} \in (0, 1), \qquad z = \transpose{\vw} \vx, \label{eq:logistic_function}
\end{align} is used to obtain the class probabilities.
{\def\par{\let\par\endgraf}\begin{marginfigure}
  \incplt{logistic_regression}
  \caption{Logistic regression classifies data into two classes with a linear decision boundary.}
\end{marginfigure}}%
\midx{Bayesian logistic regression} corresponds to Bayesian linear regression with a Bernoulli likelihood, \begin{align}
  y \mid \vx, \vw \sim \Bern{\sigma(\transpose{\vw} \vx)},
\end{align} where ${y \in \{-1, 1\}}$ is the binary class label.\footnote[][0.5\baselineskip]{The same approach extends to Gaussian processes where it is known as \midx{Gaussian process classification}, see \cref{exercise:gpc} and \cite{gpc}.}
Observe that given a data point $(\vx,y)$, the probability of a correct classification is \begin{align}
  p(y \mid \vx, \vw) = \begin{cases}
    \sigma(\transpose{\vw} \vx) & \text{if $y = 1$} \\
    1 - \sigma(\transpose{\vw} \vx) & \text{if $y = -1$}
  \end{cases} = \sigma(y \transpose{\vw} \vx) \label{eq:logistic_regression_prob},
\end{align} as the logistic function $\sigma$ is symmetric around $0$.
Also, recall that Bayesian linear regression used the prior \begin{align*}
  p(\vw) = \N[\vw]{\vzero}{\sigmap^2 \mI} \propto \exp\parentheses*{-\frac{1}{2 \sigmap^2} \norm{\vw}_2^2}.
\end{align*}

Let us first find the posterior mode, that is, the MAP estimate of the weights:\looseness=-1 \begin{align}
  \vwhat &= \argmax_\vw p(\vw \mid \vx_{1:n}, y_{1:n}) \nonumber \\
  &= \argmax_\vw p(\vw) p(y_{1:n} \mid \vx_{1:n}, \vw) \margintag{using Bayes' rule \eqref{eq:bayes_rule}} \nonumber \\
  &= \argmax_\vw \log p(\vw) + \log p(y_{1:n} \mid \vx_{1:n}, \vw) \margintag{taking the logarithm} \nonumber \\
  &= \argmax_\vw - \frac{1}{2 \sigmap^2} \norm{\vw}_2^2 + \sum_{i=1}^n \log \sigma(y_i \transpose{\vw} \vx_i) \margintag{using independence of the observations and \cref{eq:logistic_regression_prob}} \nonumber \\
  &= \argmin_\vw \frac{1}{2 \sigmap^2} \norm{\vw}_2^2 + \sum_{i=1}^n \log(1 + \exp(-y_i \transpose{\vw} \vx_i)). \margintag{using the definition of $\sigma$ \eqref{eq:logistic_function}} \label{eq:logistic_regression}
\end{align}
Note that for $\lambda = \nicefrac{1}{2 \sigmap^2}$, the above optimization is equivalent to standard (regularized) logistic regression where \begin{align}
  \ell_{\mathrm{log}}(\transpose{\vw} \vx; y) \defeq \log(1 + \exp(-y \transpose{\vw} \vx)) \label{eq:logistic_loss}
\end{align} is called \midx{logistic loss}.
The gradient of the logistic loss is given by \exerciserefmark{logistic_loss_gradient}[1] \begin{align}
  \grad_\vw \ell_{\mathrm{log}}(\transpose{\vw} \vx; y) = - y \vx \cdot \sigma(-y \transpose{\vw} \vx). \label{eq:logistic_loss_gradient}
\end{align}
Recall that due to the symmetry of $\sigma$ around $0$, $\sigma(-y \transpose{\vw} \vx)$ is the probability that $\vx$ was \emph{not} classified as $y$.
Intuitively, if the model is ``surprised'' by the label, the gradient is large.

We can therefore use SGD with the (regularized) gradient step and with batch size $1$, \begin{align}
  \vw \gets \vw(1-2 \lambda \eta_t) + \eta_t y \vx \sigma(-y \transpose{\vw} \vx),
\end{align} for the data point $(\vx,y)$ picked uniformly at random from the training data.
Here, $2 \lambda \eta_t$ is due to the gradient of the regularization term, in effect, performing weight decay.

\begin{ex}{Laplace approx. of Bayesian logistic regression}{bayesian_logistic_regression}
  We have already found the mode of the posterior distribution, $\vwhat$.

  Let us denote by \begin{align}
    \pi_i \defeq \Pr{y_i = 1 \mid \vx_i, \vwhat} = \sigma(\transpose{\vwhat} \vx_i) \label{eq:log_loss_hes_pi}
  \end{align} the probability of $\vx_i$ belonging to the positive class under the model given by the MAP estimate of the weights.
  For the precision matrix, we then have \begin{align}
    \mLambda &= - \left. \hes_\vw \log p(\vw \mid \vx_{1:n}, y_{1:n}) \right\rvert_{\vw = \vwhat} \nonumber \\
    &= - \left. \hes_\vw \log p(y_{1:n} \mid \vx_{1:n}, \vw) \right\rvert_{\vw = \vwhat} - \left. \hes_\vw \log p(\vw) \right\rvert_{\vw = \vwhat} \nonumber \\
    &= \sum_{i=1}^n \left. \hes_\vw \ell_{\mathrm{log}}(\transpose{\vw} \vx_i; y_i) \right\rvert_{\vw = \vwhat} + \sigmap^{-2}\mI \margintag{using the definition of the logistic loss \eqref{eq:logistic_loss}} \nonumber \\
    &= \sum_{i=1}^n \vx_i \transpose{\vx_i} \pi_i(1-\pi_i) + \sigmap^{-2}\mI \margintag{using the Hessian of the logistic loss \eqref{eq:logistic_loss_hes} which you derive in \textbf{\cref{exercise:logistic_loss_gradient} (2)}} \nonumber \\
    &= \transpose{\mX} \diag[i\in[n]]{\pi_i(1-\pi_i)} \mX + \sigmap^{-2}\mI. \label{eq:prec_mat_lap_approx_blr}
  \end{align}
  Observe that $\pi_i(1-\pi_i) \approx 0$ if $\pi_i \approx 1$ or $\pi_i \approx 0$.
  That is, if a training example is ``well-explained'' by $\vwhat$, then its contribution to the precision matrix is small.
  In contrast, we have $\pi_i(1-\pi_i) = 0.25$ for $\pi_i = 0.5$.
  Importantly, $\mLambda$ does not depend on the normalization constant of the posterior distribution which is hard to compute.

  In summary, we have that ${\N{\vwhat}{\inv{\mLambda}}}$ is the Laplace approximation of ${p(\vw \mid \vx_{1:n}, y_{1:n})}$.
\end{ex}

\section{Predictions with a Variational Posterior}

How can we make predictions using our variational approximation?
We simply approximate the (intractable) true posterior with our variational posterior: \begin{align}
  p(\ys \mid \vxs, \vx_{1:n}, y_{1:n}) &= \int p(\ys \mid \vxs, \vtheta) p(\vtheta \mid \vx_{1:n}, y_{1:n}) \,d\vtheta \margintag{using the sum rule \eqref{eq:sum_rule}} \nonumber \\
  &\approx \int p(\ys \mid \vxs, \vtheta) q_\vlambda(\vtheta) \,d\vtheta. \label{eq:var_prediction0}
  \intertext{A straightforward approach is to observe that \cref{eq:var_prediction0} can be viewed as an expectation over the variational posterior $q_\vlambda$ and approximated via Monte Carlo sampling:}
  &= \E[\vtheta \sim q_\vlambda]{p(\ys \mid \vxs, \vtheta)} \\
  &\approx \frac{1}{m} \sum_{j=1}^m p(\ys \mid \vxs, \vtheta_j)
\end{align} where $\vtheta_j \iid q_\vlambda$.

\begin{ex}{Predictions in Bayesian logistic regression}{}
  In the case of Bayesian logistic regression with a Gaussian approximation of the posterior, we can obtain more accurate predictions.

  Observe that the final prediction $\ys$ is conditionally independent of the model parameters $\vw$ given the ``latent value'' $\fs = \transpose{\vw} \vxs$:\begin{align}
    p(\ys \mid \vxs, \vx_{1:n}, y_{1:n}) &\approx \int p(\ys \mid \vxs, \vw) q_\vlambda(\vw) \,d\vw \nonumber \\
    &= \int \int p(\ys \mid \fs) p(\fs \mid \vxs, \vw) q_\vlambda(\vw) \,d\vw \,d\fs \margintag{once more, using the sum rule \eqref{eq:sum_rule}} \nonumber \\
    &= \int p(\ys \mid \fs) \int p(\fs \mid \vxs, \vw) q_\vlambda(\vw) \,d\vw \,d\fs. \margintag{rearranging terms} \label{eq:var_prediction}
  \end{align}
  The outer integral can be readily approximated since it is only one-dimensional!
  The challenging part is the inner integral, which is a high-dimensional integral over the model weights $\vw$.
  Since the posterior over weights $q_\vlambda(\vw) = \N[\vw]{\vwhat}{\inv{\mLambda}}$ is a Gaussian, we have due to the closedness properties of Gaussians \eqref{eq:gaussian_lin_trans} that \begin{align}
    \int p(\fs \mid \vxs, \vw) q_\vlambda(\vw) \,d\vw = \N{\transpose{\vwhat} \vxs}{\transpose{\vxs} \inv{\mLambda} \vxs}. \label{eq:var_prediction3}
  \end{align}
  Crucially, this is a one-dimensional Gaussian in function-space as opposed to the $d$-dimensional Gaussian $q_\vlambda$ in weight-space!

  As we have seen in \cref{eq:logistic_regression_prob}, for Bayesian logistic regression, the prediction $\ys$ depends deterministically on the predicted latent value $\fs$: $p(\ys \mid \fs) = \sigma(\ys \fs)$.
  Combining \cref{eq:var_prediction,eq:var_prediction3}, we obtain \begin{align}
    p(\ys \mid \vxs, \vx_{1:n}, y_{1:n}) \approx \int \sigma(\ys \fs) \cdot \N[\fs]{\transpose{\vwhat} \vxs}{\transpose{\vxs} \inv{\mLambda} \vxs} \,d\fs.
  \end{align}
  We have replaced the high-dimensional integral over the model parameters $\vtheta$ by the one-dimensional integral over the prediction of our variational posterior $\fs$.
  While this integral is generally still intractable, it can be approximated efficiently using numerical quadrature methods such as the Gauss-Legendre quadrature or alternatively with Monte Carlo sampling.
\end{ex}

\section{Blueprint of Variational Inference}\label{sec:vi:blueprint}\pidx{variational inference}[idxpagebf]%

General probabilistic inference poses the challenge of approximating the posterior distribution with limited memory and computation, resource constraints also present in humans and other intelligent systems.
These resource constraints require information to be compressed, and as we will see, such a compression poses a fundamental tradeoff between model accuracy (on the observed data) and model complexity (to avoid overfitting).

Laplace approximation approximates the true (intractable) posterior with a simpler one, by greedily matching mode and curvature around it.
Can we find ``less greedy'' approaches?
We can view variational probabilistic inference more generally as a family of approaches aiming to approximate the true posterior distribution by one that is closest (according to some criterion) among a ``simpler'' class of distributions.
To this end, we need to fix a class of distributions and define suitable criteria, which we can then optimize numerically.
The key benefit is that we can reduce the (generally intractable) problem of high-dimensional integration to the (often much more tractable) problem of optimization.\looseness=-1

\begin{defn}[Variational family]\pidx{variational family}
  Let $\spP$ be the class of all probability distributions.
  A \emph{variational family} $\spQ \subseteq \spP$ is a class of distributions such that each distribution $q \in \spQ$ is characterized by unique variational parameters $\vlambda \in \Lambda$.
\end{defn}

\begin{marginfigure}
  \incfig{variational_families}
  \caption{An illustration of variational inference in the space of distributions $\spP$.
  The variational distribution $\qs \in \spQ$ is the optimal approximation of the true posterior $p$.}
  \label{fig:variational_families}
\end{marginfigure}

\begin{ex}{Family of independent Gaussians}{var_family_diag_gaussian}
  A straightforward example for a variational family is the family of independent Gaussians, \begin{align}
    \spQ \defeq \braces*{q(\vtheta) = \N[\vtheta]{\vmu}{\diag[i\in[d]]{\sigma_i^2}}},
  \end{align} which is parameterized by ${\vlambda \defeq [\mu_{1:d}, \sigma_{1:d}^2]}$.
  Such a multivariate distribution where all variables are independent is called a \midx{mean-field distribution}.
  Importantly, this family of distributions is characterized by only $2d$ parameters!
\end{ex}

Note that \cref{fig:variational_families} is a generalization of the canonical distinction between estimation error and approximation error from \cref{fig:estimation_approximation_error}, only that here, we operate in the space of distributions over functions as opposed the space of functions.
A common notion of distance between two distributions $q$ and $p$ is the Kullback-Leibler divergence $\KL{q}{p}$ which we will define in the next section.
Using this notion of distance, we need to solve the following optimization problem: \begin{align}
  \qs \defeq \argmin_{q \in \spQ} \KL{q}{p} = \argmin_{\vlambda \in \Lambda} \KL{q_\vlambda}{p}. \label{eq:var_optimization}
\end{align}

In \cref{sec:approximate_inference:information_theory}, we introduce information theory and the Kullback-Leibler divergence.
Then, in \cref{sec:approximate_inference:variational_inference:elbo}, we discuss how the optimization problem of \cref{eq:var_optimization} can be solved efficiently.

\section{Information Theoretic Aspects of Uncertainty}\label{sec:approximate_inference:information_theory}

One of our main objectives throughout this manuscript is to capture the ``uncertainty'' about events $A$ in an appropriate probability space.
One very natural measure of uncertainty is their probability, $\Pr{A}$.
In this section, we will introduce an alternative measure of uncertainty, namely the so-called ``surprise'' about the event $A$.

\subsection{Surprise}

The \midx{surprise} about an event with probability $u$ is defined as \begin{align}
  \S{u} \defeq - \log u.
\end{align}
Observe that the surprise is a function from $\Rzero$ to $\R$, where we let~${\S{0} \equiv \infty}$.
Moreover, for a discrete random variable $X$, we have that $p(x) \leq 1$, and hence, $\S{p(x)} \geq 0$.
But why is it reasonable to measure surprise by $- \log u$?

\begin{marginfigure}[-4\baselineskip]
  \incplt{surprise}
  \caption{Surprise $\S{u}$ associated with an event of probability $u$.}
\end{marginfigure}

Remarkably, it can be shown that the following natural axiomatic characterization leads to exactly this definition of surprise.

\begin{thm}[Axiomatic characterization of surprise]
  The axioms \begin{enumerate}
    \item $\S{u} > \S{v} \implies u < v$ (anti-monotonicity) \margintag{we are more surprised by unlikely events}
    \item $\fnS$ is continuous, \margintag{no jumps in surprise for infinitesimal changes of probability} \\[-5pt]
    \item $\S{uv} = \S{u} + \S{v}$ for independent events, \margintag{the surprise of independent events is additive}
  \end{enumerate} characterize $\fnS$ up to a positive constant factor.
\end{thm}
\begin{proof}
  Observe that the third condition looks similar to the product rule of logarithms: $\log(uv) = \log v + \log v$.
  We can formalize this intuition by remembering Cauchy's functional equation, $f(x + y) = f(x) + f(y)$, which has the unique family of solutions $\{f : x \mapsto c x : c \in \R\}$ if $f$ is required to be continuous.
  Such a solution is called an ``additive function''.
  Consider the function $g(x) \defeq f(e^x)$. Then, $g$ is additive if and only if \begin{align*}
    f(e^x e^y) = f(e^{x+y}) = g(x + y) = g(x) + g(y) = f(e^x) + f(e^y).
  \end{align*}
  This is precisely the third axiom of surprise for $f = \fnS$ and $e^x = u$!
  Hence, the second and third axioms of surprise imply that $g$ must be additive and that $g(x) = \S{e^x} = c x$ for any $c \in \R$.
  If we replace $e^x$ by $u$, we obtain $\S{u} = c \log u$.
  The first axiom of surprise implies that $c < 0$, and thus, $\S{u} = - c' \log u$ for any $c' > 0$.
\end{proof}

Importantly, surprise offers a different perspective on uncertainty as opposed to probability: the uncertainty about an event can either be interpreted in terms of its probability or in terms of its surprise, and the two ``spaces of uncertainty'' are related by a log-transform.
This relationship is illustrated in \cref{fig:surprise_space}.
Information theory is the study of uncertainty in terms of surprise.

\begin{marginfigure}
  \incfig{surprise_space}
  \caption{Illustration of the probability space and the corresponding ``surprise space''.\looseness=-1}\label{fig:surprise_space}
\end{marginfigure}

Throughout this manuscript we will see many examples where modeling uncertainty in terms of surprise (i.e., the information-theoretic interpretation of uncertainty) is useful.
One example where we have already encountered the ``surprise space'' was in the context of likelihood maximization (cf. \cref{sec:fundamentals:parameter_estimation:mle}) where we used that the log-transform linearizes products of probabilities.
We will see later in \cref{sec:approximate_inference:mcmc} that in many cases the surprise $\S{p(x)}$ can also be interpreted as a ``cost'' or ``energy'' associated with the state $x$.

\subsection{Entropy}

\begin{marginfigure}[5\baselineskip]
  \incplt{entropy}
  \caption{Entropy of a Bernoulli experiment with success probability $p$.}
\end{marginfigure}

The \midx{entropy}[idxpagebf] of a distribution $p$ is the average surprise about samples from $p$.
In this way, entropy is a notion of uncertainty associated with the distribution $p$: if the entropy of $p$ is large, we are more uncertain about $x \sim p$ than if the entropy of $p$ were low.
Formally, \begin{align}
  \H{p} \defeq \E[x \sim p]{\S{p(x)}} = \E[x \sim p]{- \log p(x)}. \label{eq:entropy}
\end{align}
When $\rX \sim p$ is a random vector distributed according to $p$, we write $\H{\rX} \defeq \H{p}$.
Observe that by definition, if $p$ is discrete then $\H{p} \geq 0$ as $p(x) \leq 1 \; (\forall x)$.\footnote{The entropy of a continuous distribution can be negative.
For example, \begin{align*}
  \H{\Unif{[a,b]}} &= -\int \frac{1}{b-a} \log\frac{1}{b-a} \,dx \\
  &= \log(b-a)
\end{align*} which is negative if $b-a < 1$.}
For discrete distributions it is common to use the logarithm with base $2$ rather than the natural logarithm:\footnote{Recall that $\log_2 x = \frac{\log x}{\log 2}$, that is, logarithms to a different base only differ by a constant factor.} \begin{subequations}\begin{align}
  \H{p} &= - \sum_{x} p(x) \log_2 p(x) &&\text{(if $p$ is discrete)}, \label{eq:entropy_discrete} \\
  \H{p} &= - \int p(\vx) \log p(\vx) \,d\vx  &&\text{(if $p$ is continuous)}. \label{eq:entropy_cont}
\end{align}\end{subequations}

Let us briefly recall Jensen's inequality, which is a useful tool when working with expectations of convex functions such as entropy:\footnote{The surprise $\S{u}$ is convex in $u$.}

\begin{fct}[Jensen's Inequality]\pidx{Jensen's inequality}\exerciserefmark{jensen}[1]
  Given a random variable $X$ and a convex function $g : \R \to \R$, we have \begin{align}
    g(\E{X}) \leq \E{g(X)}. \label{eq:jensen}
  \end{align}
\end{fct}

\begin{marginfigure}[3\baselineskip]
  \incplt{jensen}
  \caption{An illustration of Jensen's inequality.
  Due to the convexity of $g$, we have that $g$ evaluated at $\E{X}$ will always be below the average of evaluations of $g$.}
\end{marginfigure}


\begin{ex}{Examples of entropy}{}
  \begin{itemize}
    \item \emph{Fair Coin} \quad $\H{\Bern{0.5}} = -2 (0.5 \log_2 0.5) = 1.$
    \item \emph{Unfair Coin} \quad \begin{align*}
      \H{\Bern{0.1}} = -0.1 \log_2 0.1 - 0.9 \log_2 0.9 \approx0.469.
    \end{align*}
    \item \emph{Uniform Distribution} \quad \begin{align*}
      \H{\Unif{\{1, \dots, n\}}} = - \sum_{i=1}^n \frac{1}{n} \log_2 \frac{1}{n} = \log_2 n.
    \end{align*}
    The uniform distribution has the maximum entropy among all discrete distributions supported on ${\{1, \dots, n\}}$ \exerciserefmark{jensen}[2].
    Note that a fair coin corresponds to a uniform distribution with ${n = 2}$.
    Also observe that $\log_2 n$ corresponds to the number of bits required to encode the outcome of the experiment.
  \end{itemize}

  In general, the entropy $\H{p}$ of a discrete distribution $p$ can be interpreted as the average number of bits required to encode a sample $x \sim p$, or in other words, the average ``information'' carried by a sample $x$.
\end{ex}

\begin{ex}{Entropy of a Gaussian}{}
  Let us derive the entropy of a univariate Gaussian.
  Recall the PDF, \begin{align*}
    \N[x]{\mu}{\sigma^2} = \frac{1}{Z} \exp\parentheses*{-\frac{(x-\mu)^2}{2 \sigma^2}}
  \end{align*} where ${Z = \sqrt{2 \pi \sigma^2}}$.
  Using the definition of entropy \eqref{eq:entropy_cont}, we obtain, \begin{align}
    \H{\N{\mu}{\sigma^2}} \nonumber &= \begin{multlined}[t]
      - \int \frac{1}{Z} \exp\parentheses*{-\frac{(x-\mu)^2}{2 \sigma^2}} \\ \cdot \log\parentheses*{\frac{1}{Z} \exp\parentheses*{-\frac{(x-\mu)^2}{2 \sigma^2}}} \,dx
    \end{multlined} \nonumber \\
    &= \begin{multlined}[t]
    \log Z \underbrace{\int \frac{1}{Z} \exp\parentheses*{-\frac{(x-\mu)^2}{2 \sigma^2}} \,dx}_{1} \\
    + \int \frac{1}{Z} \exp\parentheses*{-\frac{(x-\mu)^2}{2 \sigma^2}} \frac{(x-\mu)^2}{2 \sigma^2} \,dx
    \end{multlined} \nonumber \\
    &= \log Z + \frac{1}{2 \sigma^2} \E{(x - \mu)^2} \margintag{using LOTUS \eqref{eq:lotus}} \nonumber \\
    &= \log(\sigma \sqrt{2 \pi}) + \frac{1}{2} \margintag{using $\E{(x - \mu)^2} = \Var{x} = \sigma^2$ \eqref{eq:variance}} \nonumber \\
    &= \log(\sigma \sqrt{2 \pi e}). \margintag{using $\log \sqrt{e} = \nicefrac{1}{2}$} \label{eq:entropy_gaussian_univ}
  \end{align}

  In general, the entropy of a Gaussian is \begin{align}
    \H{\N{\vmu}{\mSigma}} = \frac{1}{2} \log \det{2 \pi e \mSigma} = \frac{1}{2} \log \parentheses*{(2 \pi e)^d \det{\mSigma}}. \label{eq:entropy_gaussian}
  \end{align}
  Note that the entropy is a function of the determinant of the covariance matrix $\mSigma$.
  In general, there are various ways of ``scalarizing'' the notion of uncertainty for a multivariate distribution.
  The determinant of $\mSigma$ measures the volume of the credible sets around the mean~$\vmu$, and is also called the \midx{generalized variance}.
  Next to entropy and generalized variance (which are closely related for Gaussians), a common scalarization is the trace of~$\mSigma$, which is also called the \midx{total variance}.
\end{ex}


\subsection{Cross-Entropy}

How can we use entropy to measure our average surprise when assuming the data follows some distribution $q$ but in reality the data follows a different distribution $p$?

\begin{defn}[Cross-entropy]
  The \midx{cross-entropy} of a distribution $q$ relative to the distribution $p$ is \begin{align}
    \crH{p}{q} \defeq \E[x \sim p]{\S{q(x)}} = \E[x \sim p]{- \log q(x)}. \label{eq:cross_entropy}
  \end{align}
\end{defn}

Cross-entropy can also be expressed in terms of the KL-divergence (cf. \cref{sec:vi:kl}) $\KL{p}{q}$ which measures how ``different'' the distribution $q$ is from a reference distribution $p$, \begin{align}
  \crH{p}{q} = \H{p} + \KL{p}{q} \geq \H{p}. \margintag{$\KL{p}{q} \geq 0$ is shown in \cref{exercise:gibbs_ineq}} \label{eq:cross_entropy_decomp}
\end{align}
Quite intuitively, the average surprise in samples from $p$ with respect to the distribution $q$ is given by the inherent uncertainty in $p$ and the additional surprise that is due to us assuming the wrong data distribution $q$.
The ``closer'' $q$ is to the true data distribution $p$, the smaller is the additional average surprise.

\subsection{Kullback-Leibler Divergence}\label{sec:vi:kl}

As mentioned, the Kullback-Leibler divergence is a (non-metric) measure of distance between distributions.
It is defined as follows.

\begin{defn}[Kullback-Leibler divergence, KL-divergence]\pidx{Kullback-Leibler divergence}
  Given two distributions $p$ and $q$, the \emph{Kullback-Leibler divergence} (or \midx{relative entropy}) of $q$ with respect to $p$, \begin{align}
    \KL{p}{q} &\defeq \crH{p}{q} - \H{p} \label{eq:kl} \\
    &= \E[\vtheta \sim p]{\S{q(\vtheta)} - \S{p(\vtheta)}} \\
    &= \E[\vtheta \sim p]{\log \frac{p(\vtheta)}{q(\vtheta)}},
  \end{align} measures how different $q$ is from a reference distribution $p$.
\end{defn}
In words, $\KL{p}{q}$ measures the \emph{additional} expected surprise when observing samples from $p$ that is due to assuming the (wrong) distribution $q$ and which not inherent in the distribution $p$ already.\footnote[][-\baselineskip]{The KL-divergence only captures the additional expected surprise since the surprise inherent in $p$ (as measured by $\H{p}$) is subtracted.}

The KL-divergence has the following properties: \begin{itemize}
  \item $\KL{p}{q} \geq 0$ for any distributions $p$ and $q$ \exerciserefmark{gibbs_ineq}[1],
  \item $\KL{p}{q} = 0$ if and only if $p = q$ almost surely \exerciserefmark{gibbs_ineq}[2], and
  \item there exist distributions $p$ and $q$ such that $\KL{p}{q} \neq \KL{q}{p}$.
\end{itemize}

The KL-divergence can simply be understood as a shifted version of cross-entropy, which is zero if we consider the divergence between two identical distributions.

We will briefly look at another interpretation for how KL-divergence measures ``distance'' between distributions.
Suppose we are presented with a sequence $\vtheta_1, \dots, \vtheta_n$ of independent samples from either a distribution $p$ or a distribution $q$, both of which are known.
Which of $p$ or $q$ was used to generate the data is, however, unknown to us, and we would like to find out.
A natural approach is to choose the distribution whose data likelihood is larger. That is, we choose $p$ if $p(\vtheta_{1:n}) > q(\vtheta_{1:n})$ and vice versa. Assuming that the samples are independent and rewriting the inequality slightly, we choose $p$ if \begin{align}
  \prod_{i=1}^n \frac{p(\vtheta_i)}{q(\vtheta_i)} > 1, \quad\text{or equivalently if}\quad
  \sum_{i=1}^n \log \frac{p(\vtheta_i)}{q(\vtheta_i)} > 0. \label{eq:kl_decision_criterion} \margintag{taking the logarithm}
\end{align}
Assume without loss of generality that $\vtheta_i \sim p$. Then, using the law of large numbers \eqref{eq:slln}, \begin{align}
  \frac{1}{n} \sum_{i=1}^n \log \frac{p(\vtheta_i)}{q(\vtheta_i)} \almostsurely \E[\vtheta \sim p]{\log \frac{p(\vtheta)}{q(\vtheta)}} = \KL{p}{q}
\end{align} as $n \to \infty$.
Plugging this into our decision criterion from \cref{eq:kl_decision_criterion}, we find that \begin{align}
  \E{\sum_{i=1}^n \log \frac{p(\vtheta_i)}{q(\vtheta_i)}} = n \KL{p}{q}.
\end{align}
In this way, $\KL{p}{q}$ measures the observed ``distance'' between $p$ and~$q$.
Recall that assuming $p \neq q$ we have that $\KL{p}{q} > 0$ with probability $1$, and therefore we correctly choose $p$ with probability $1$ as~${n \to \infty}$.
Moreover, Hoeffding's inequality \eqref{eq:hoeffdings_inequality} can be used to determine ``how quickly'' samples converge to this limit, that is, how quickly we can distinguish between $p$ and $q$.




\begin{ex}{KL-divergence of Bernoulli random variables}{}
  Suppose we are given two Bernoulli distributions $\Bern{p}$ and $\Bern{q}$.
  Then, their KL-divergence is \begin{align}
    \KL{\Bern{p}}{\Bern{q}} &= \sum_{x \in \{0,1\}} \Bern[x]{p} \log \frac{\Bern[x]{p}}{\Bern[x]{q}} \nonumber\\
    &= p \log \frac{p}{q} + (1-p) \log \frac{(1-p)}{(1-q)}.
  \end{align}
  Observe that $\KL{\Bern{p}}{\Bern{q}} = 0$ if and only if $p = q$.
\end{ex}

\begin{ex}{KL-divergence of Gaussians}{}
  Suppose we are given two Gaussian distributions ${p \defeq \N{\vmu_p}{\mSigma_p}}$ and ${q \defeq \N{\vmu_q}{\mSigma_q}}$ with dimension $d$.
  The KL-divergence of $p$ and~$q$ is given by \exerciserefmark{kl_div_of_gaussians} \begin{align}
    \begin{multlined}[t]
    \KL{p}{q} = \frac{1}{2} \left(\mathrm{tr}(\inv{\mSigma_q} \mSigma_p) + \transpose{(\vmu_p - \vmu_q)} \inv{\mSigma_q} (\vmu_p - \vmu_q) \phantom{\frac12}\right. \\ \left. - d + \log \frac{\det{\mSigma_q}}{\det{\mSigma_p}}\right).
    \end{multlined} \label{eq:kl_gaussian}
  \end{align}

  For independent Gaussians with unit variance, $\mSigma_p = \mSigma_q = \mI$, the expression simplifies to the squared Euclidean distance, \begin{align}
    \KL{p}{q} = \frac{1}{2} \norm{\vmu_q - \vmu_p}_2^2.
  \end{align}

  If we approximate independent Gaussians with variances $\sigma_i^2$, \begin{align*}
    p \defeq \N{\vmu}{\diag{\sigma_1^2, \dots, \sigma_d^2}},
  \end{align*} by a standard normal distribution, ${q \defeq \SN}$, the expression simplifies to \begin{align}
    \KL{p}{q} = \frac{1}{2} \sum_{i=1}^d (\sigma_i^2 + \mu_i^2 - 1 - \log \sigma_i^2). \label{eq:kl_brl}
  \end{align}
  Here, the term $\mu_i^2$ penalizes a large mean of $p$, the term $\sigma_i^2$ penalizes a large variance of $p$, and the term $- \log \sigma_i^2$ penalizes a small variance of $p$.
  As expected, $\KL{p}{q}$ is proportional to the amount of information we lose by approximating $p$ with the simpler distribution $q$.
\end{ex}

\subsection{Forward and Reverse KL-divergence}\label{sec:vi:kl:forward_reverse}

\begin{marginfigure}[-15\baselineskip]
  \incplt{kl_divergence_1}
  \incplt{kl_divergence_2}
  \caption{Comparison of the \textbf{\r{forward}} KL-divergence $\r{\qs_1}$ and the \textbf{\b{reverse}} KL-divergence $\b{\qs_2}$ when used to approximate the \textbf{true posterior} $p$.
  The first plot shows the PDFs in a one-dimensional feature space where $p$ is a mixture of two univariate Gaussians.
  The second plot shows contour lines of the PDFs in a two-dimensional feature space where the non-diagonal Gaussian $p$ is approximated by diagonal Gaussians $\qs_1$ and $\qs_2$.
  It can be seen that $\qs_1$ selects the variance and $\qs_2$ selects the mode of $p$.
  The approximation $\qs_1$ is more conservative than the (overconfident) approximation~$\qs_2$.}\label{fig:forward_reverse_kl}
\end{marginfigure}

$\KL{p}{q}$ is also called the \midx<forward>{forward KL-divergence} (or \midx<inclusive>{inclusive KL-divergence}) KL-divergence. In contrast, $\KL{q}{p}$ is called the \midx<reverse>{reverse KL-divergence} (or \midx<exclusive>{exclusive KL-divergence}) KL-divergence.
\Cref{fig:forward_reverse_kl} shows the approximations of a general Gaussian obtained when $\spQ$ is the family of diagonal (independent) Gaussians.
Thereby, \begin{align*}
  \qs_1 \defeq \argmin_{q \in \spQ} \KL{p}{q} \quad\text{and}\quad \qs_2 \defeq \argmin_{q \in \spQ} \KL{q}{p}.
\end{align*} $\qs_1$ is the result when using the forward KL-divergence and $\qs_2$ is the result when using reverse KL-divergence.
It can be seen that the reverse KL-divergence tends to greedily select the mode and underestimating the variance which, in this case, leads to an overconfident prediction.
The forward KL-divergence, in contrast, is more conservative and yields what one could consider the ``desired'' approximation.

Recall that in the blueprint of variational inference \eqref{eq:var_optimization} we used the reverse KL-divergence.
This is for computational reasons.
Observe that to approximate the KL-divergence $\KL{p}{q}$ using Monte Carlo sampling, we would need to obtain samples from $p$ yet $p$ is the intractable posterior distribution which we were trying to approximate in the first place.
Crucially, observe that if the true posterior $p(\cdot \mid \vx_{1:n}, y_{1:n})$ is in the variational family $\spQ$, then \begin{align}\begin{split}
  \argmin_{q \in \spQ} \KL{q}{p(\cdot \mid \vx_{1:n}, y_{1:n})} \eqalmostsurely p(\cdot \mid \vx_{1:n}, y_{1:n}), \margintag{as $\min_{q \in \spQ} \KL{q}{p(\cdot \mid \vx_{1:n}, y_{1:n})} \eqalmostsurely 0$}
\end{split}\end{align} so minimizing reverse-KL still recovers the true posterior almost surely.

\begin{rmk}{Greediness of reverse-KL}{}
  As in the previous example, consider the independent Gaussians \begin{align*}
    p \defeq \N{\vmu}{\diag[i\in[d]]{\sigma_i^2}},
  \end{align*} which we seek to approximate by a standard normal distribution ${q \defeq \SN}$.
  Using \eqref{eq:kl_gaussian}, we obtain for the reverse KL-divergence, \begin{align}
    \KL{q}{p} &= \begin{multlined}[t]
      \frac{1}{2} \left(\tr{\diag{\sigma_i^{-2}}} + \transpose{\vmu}\diag{\sigma_i^{-2}}\vmu - d \right. \\ \left. + \log\det{\diag{\sigma_i^2}}\right)
    \end{multlined} \nonumber \\
    &= \frac{1}{2} \sum_{i=1}^d \parentheses*{\sigma_i^{-2} + \frac{\mu_i^2}{\sigma_i^2} - 1 + \log \sigma_i^2}.
  \end{align}
  Here, $\sigma_i^{-2}$ penalizes small variance, $\nicefrac{\mu_i^2}{\sigma_i^2}$ penalizes a large mean, and $\log \sigma_i^2$ penalizes large variance.
  Compare this to the expression for the forward KL-divergence $\KL{p}{q}$ that we have seen in \cref{eq:kl_brl}. In particular, observe that reverse-KL penalizes large variance less strongly than forward-KL.

  Note, however, that reverse-KL is not greedy in the same sense as Laplace approximation, as it does still take the variance into account and does not \emph{purely} match the mode of $p$.
\end{rmk}

\subsection{Interlude: Minimizing Forward KL-Divergence}\label{sec:vi:kl:forward}

Before completing the blueprint of variational inference in \cref{sec:approximate_inference:variational_inference:elbo} by showing how \emph{reverse-KL} can be efficiently minimized, we will digress briefly and relate minimizing \emph{forward-KL} to two other well-known inference algorithms.
This discussion will deepen our understanding of the KL-divergence and its role in probabilistic inference, but feel free to skip ahead to \cref{sec:approximate_inference:variational_inference:elbo} if you are eager to complete the blueprint.

\paragraph{Minimizing forward-KL as maximum likelihood estimation:}
First, we observe that minimizing the forward KL-divergence is equivalent to maximum likelihood estimation on an infinitely large sample size.
The classical application of this result is in the setting where $p(\vx)$ is a generative model, and we aim to estimate its density with the parameterized model $q_\vlambda$.

\begin{lem}[Forward KL-divergence as MLE]
  Given some generative model $p(\vx)$ and a likelihood $q_\vlambda(\vx) = q(\vx \mid \vlambda)$ (that we use to approximate the true data distribution), we have \begin{align}
    \argmin_{\vlambda \in \Lambda} \KL{p}{q_\vlambda} \eqalmostsurely \argmax_{\vlambda \in \Lambda} \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n \log q(\vx_i \mid \vlambda), \label{eq:forward_kl_as_mle}
  \end{align} where $\vx_i \iid p$ are independent samples from the true data distribution.
\end{lem}
\begin{proof}
\begin{align*}
  \KL{p}{q_\vlambda} &= \crH{p}{q_\vlambda} - \H{p} \margintag{using the definition of KL-divergence \eqref{eq:kl}} \\[6pt]
  &= \E[(\vx, y) \sim p]{- \log q(\vx \mid \vlambda)} + \const \margintag{dropping $\H{p}$ and using the definition of cross-entropy \eqref{eq:cross_entropy}} \\
  &\eqalmostsurely - \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n \log q(\vx_i \mid \vlambda) + \const \margintag{using Monte Carlo sampling, i.e., the law of large numbers \eqref{eq:slln}}
\end{align*} where $\vx_i \iid p$ are independent samples.
\end{proof}

This tells us that \emph{any} maximum likelihood estimate $q_\vlambda$ minimizes the forward KL-divergence to the empirical data distribution.
Note that here, we aim to learn model parameters $\vlambda$ for estimating the probability of $\vx$, whereas in the setting of variational probabilistic inference, we want to learn parameters $\vlambda$ of a distribution over $\vtheta$ and $\vtheta$ parameterizes a \emph{distribution over $\vx$}.
This interpretation is therefore not immediately useful for probabilistic inference (i.e., in the setting where $p$ is a posterior distribution over model parameters $\vtheta$) as a maximum likelihood estimate requires i.i.d. samples from $p$ which we cannot easily obtain in this case.\footnote{It is possible to obtain ``approximate'' samples using Markov chain Monte Carlo (MCMC) methods which we discuss in \cref{sec:approximate_inference:mcmc}.}

\begin{ex}{Minimizing cross-entropy}{}
  Minimizing the KL-divergence between $p$ and $q_{\vlambda}$ is equivalent to minimizing cross-entropy since $\KL{p}{q_\vlambda} = \crH{p}{q_\vlambda} - \H{p}$ and $\H{p}$ is constant with respect to $p$.

  Lets consider an example in a binary classification problem with the label $y \in \{0, 1\}$ and predicted class probability $\hat{y} \in [0, 1]$ for some fixed input.
  It is natural to use cross-entropy as a measure of dissimilarity between $y$ and $\hat{y}$, \begin{align}\begin{split}
    \ell_{\mathrm{bce}}(\hat{y}; y) &\defeq \crH{\Bern{y}}{\Bern{\hat{y}}} \\
    &= -\sum_{x \in \{0, 1\}} \Bern[x]{y} \log \Bern[x]{\hat{y}} \\
    &= -y \log \hat{y} - (1-y) \log (1-\hat{y}).
  \end{split}\label{eq:bce_loss}\end{align}
  This loss function is also known as the \midx{binary cross-entropy loss} and we will discuss it in more detail in \cref{sec:bnn:mle} in the context of neural networks.
\end{ex}

\paragraph{Minimizing forward-KL as moment matching:}
Now to a second interpretation of minimizing forward-KL.
\midx<Moment matching>{moment matching}[idxpagebf] (also known as the \midx{method of moments}) is a technique for approximating an unknown distribution $p$ with a parameterized distribution $q_\vlambda$ where $\vlambda$ is chosen such that $q_\vlambda$ matches the (estimated) moments of $p$.
For example, given the estimates $\va$ and $\mB$ of the first and second moment of $p$,\safefootnote{These estimates are computed using the samples from $p$. For example, using a sample mean and a sample variance to compute the estimates of the first and second moment.} and if $q_\vlambda$ is a Gaussian with parameters $\vlambda = \{\vmu, \mSigma\}$, then moment matching chooses $\vlambda$ as the solution to \begin{align*}
  \E[p]{\vtheta} \approx \va &\overset{!}{=} \vmu = \E[q_\vlambda]{\vtheta} \\
  \E[p]{\vtheta \transpose{\vtheta}} \approx \mB &\overset{!}{=} \mSigma + \vmu \transpose{\vmu} = \E[q_\vlambda]{\vtheta \transpose{\vtheta}}. \margintag{using the definition of variance \eqref{eq:variance2}}
\end{align*}
In general the number of moments to be matched (i.e., the number of equations) is adjusted such that it is equal to the number of parameters to be estimated.
We will see now that the ``matching'' of moments is also ensured when $q_\vlambda$ is obtained by minimizing the forward KL-divergence within the family of Gaussians.

The Gaussian PDF can be expressed as \begin{align}
  \N[\vtheta]{\vmu}{\mSigma} &= \frac{1}{Z(\vlambda)} \exp(\transpose{\vlambda} \vs(\vtheta)) \quad\text{where} \label{eq:exponential_family_of_distributions} \\
  \vlambda &\defeq \begin{bmatrix}
    \inv{\mSigma}\vmu \\
    \vecop{\inv{\mSigma}}
  \end{bmatrix} \label{eq:gaussian_nat_params} \\
  \vs(\vtheta) &\defeq \begin{bmatrix}
    \vtheta \\
    \vecop{-\frac{1}{2}\vtheta\transpose{\vtheta}}
  \end{bmatrix} \label{eq:gaussian_suff_stats}
\end{align} and $Z(\vlambda) \defeq \int \exp(\transpose{\vlambda} \vs(\vtheta)) \,d\vtheta$, and we will confirm this in just a moment.\safefootnote{Given a matrix $\mA \in \R^{n \times m}$, we use \begin{align*}
  \vecop{\mA} \in \R^{n \cdot m}
\end{align*} to denote the row-by-row concatenation of $\mA$ yielding a vector of length $n \cdot m$.}
The family of distributions with densities of the form \eqref{eq:exponential_family_of_distributions} --- with an additional scaling constant $h(\vtheta)$ which is often $1$ --- is called the \midx{exponential family} of distributions.
Here, $\vs(\vtheta)$ are the \midx<sufficient statistics>{sufficient statistic}, $\vlambda$ are called the \midx{natural parameters}, and $Z(\vlambda)$ is the normalizing constant.
In this context, $Z(\vlambda)$ is often called the \midx{partition function}.

To see that the Gaussian is indeed part of the exponential family as promised in \cref{eq:gaussian_nat_params,eq:gaussian_suff_stats}, consider \begin{align*}
  \N[\vtheta]{\vmu}{\mSigma} &\propto \exp\parentheses*{-\frac{1}{2}\transpose{(\vtheta - \vmu)}\inv{\mSigma}(\vtheta - \vmu)} \\
  &\propto \exp\parentheses*{\tr{-\frac{1}{2}\transpose{\vtheta}\inv{\mSigma}\vtheta} + \transpose{\vtheta}\inv{\mSigma}\vmu} \margintag{expanding the inner product and using that $\tr{x} = x$ for all $x \in \R$} \\
  &= \exp\parentheses*{\tr{-\frac{1}{2}\vtheta\transpose{\vtheta}\inv{\mSigma}} + \transpose{\vtheta}\inv{\mSigma}\vmu} \margintag{using that the trace is invariant under cyclic permutations} \\
  &= \exp\parentheses*{\transpose{\vecop{-\tfrac{1}{2}\vtheta\transpose{\vtheta}}}\vecop{\inv{\mSigma}} + \transpose{\vtheta}\inv{\mSigma}\vmu}. \margintag{using $\tr{\mA\mB} = \transpose{\vecop{\mA}}\vecop{\mB}$ for any $\mA, \mB \in \R^{n \times n}$}
\end{align*}
This allows us to express the forward KL-divergence as \begin{align*}
  \KL{p}{q_\vlambda} &= \int p(\vtheta) \log\frac{p(\vtheta)}{q_\vlambda(\vtheta)}\,d\vtheta \\
  &= -\int p(\vtheta) \cdot \transpose{\vlambda} \vs(\vtheta) \,d\vtheta + \log Z(\vlambda) + \const. \margintag{using that $\int p(\vtheta) \log p(\vtheta) \,d \vtheta$ is constant}
\end{align*} Differentiating with respect to the natural parameters $\vlambda$ gives \begin{align*}
  \grad_\vlambda \KL{p}{q_\vlambda} &= -\int p(\vtheta) \vs(\vtheta) \,d\vtheta + \frac{1}{Z(\vlambda)} \int \vs(\vtheta) \exp(\transpose{\vlambda} \vs(\vtheta)) \,d\vtheta \\
  &= -\E[\vtheta \sim p]{\vs(\vtheta)} + \E[\vtheta \sim q_\vlambda]{\vs(\vtheta)}.
\end{align*} Hence, for any minimizer of $\KL{p}{q_\vlambda}$, we have that the sufficient statistics under $p$ and $q_\vlambda$ match: \begin{align}
  \E[p]{\vs(\vtheta)} = \E[q_\vlambda]{\vs(\vtheta)}.
\end{align}
Therefore, in the Gaussian case, \begin{align*}
  \E[p]{\vtheta} = \E[q_\vlambda]{\vtheta} \quad\text{and}\quad \E[p]{-\tfrac{1}{2}\vtheta\transpose{\vtheta}} = \E[q_\vlambda]{-\tfrac{1}{2}\vtheta\transpose{\vtheta}},
\end{align*} implying that \begin{align}
  \E[p]{\vtheta} = \vmu \quad\text{and}\quad \Var[p]{\vtheta} = \E[p]{\vtheta\transpose{\vtheta}} - \E[p]{\vtheta}\cdot\transpose{\E[p]{\vtheta}} = \mSigma \margintag{using \cref{eq:variance2}}
\end{align} where $\vmu$ and $\mSigma$ are the mean and variance of the approximation $q_\vlambda$, respectively.
That is, a Gaussian $q_\vlambda$ minimizing $\KL{p}{q_\vlambda}$ has the same first and second moment as $p$.
Combining this insight with our observation from \cref{eq:forward_kl_as_mle} that minimizing forward-KL is equivalent to maximum likelihood estimation, we see that if we use MLE to fit a Gaussian to given data, this Gaussian will eventually match the first and second moments of the data distribution.

\section{Evidence Lower Bound}\label{sec:approximate_inference:variational_inference:elbo}

Let us return to the blueprint of variational inference from \cref{sec:vi:blueprint}.
To complete this blueprint, it remains to show that the reverse KL-divergence can be minimized efficiently.
We have \begin{align*}
  \KL{q}{p(\cdot \mid \vx_{1:n}, y_{1:n})} &= \E[\vtheta \sim q]{\log \frac{q(\vtheta)}{p(\vtheta \mid \vx_{1:n}, y_{1:n})}} \margintag{using the definition of the KL-divergence \eqref{eq:kl}} \\
  &= \E[\vtheta \sim q]{\log \frac{p(y_{1:n} \mid \vx_{1:n}) q(\vtheta)}{p(y_{1:n}, \vtheta \mid \vx_{1:n})}} \margintag{using the definition of conditional probability \eqref{eq:cond_prob}} \\
  &= \begin{multlined}[t]
    \log p(y_{1:n} \mid \vx_{1:n}) \\ \underbrace{- \E[\vtheta \sim q]{\log p(y_{1:n}, \vtheta \mid \vx_{1:n})} - \H{q}}_{- L(q, p; \spD_n)}
  \end{multlined} \margintag{using linearity of expectation \eqref{eq:linearity_expectation}}
\end{align*} where $L(q, p; \spD_n)$ is called the \emph{evidence lower bound} (ELBO) given the data $\spD_n = \{(\vx_i, y_i)\}_{i=1}^n$.
This gives the relationship \begin{thmb}\begin{align}
  L(q, p; \spD_n) = \underbrace{\log p(y_{1:n} \mid \vx_{1:n})}_{\const} -\ \KL{q}{p(\cdot \mid \vx_{1:n}, y_{1:n})}. \label{eq:elbo_reverse_kl_relationship}
\end{align}\end{thmb}
Thus, maximizing the ELBO coincides with minimizing reverse-KL.

Maximizing the ELBO, \begin{align}
  L(q, p; \spD_n) &\defeq \E[\vtheta \sim q]{\log p(y_{1:n}, \vtheta \mid \vx_{1:n})} + \H{q}, \label{eq:elbo}
\end{align} selects $q$ that has large joint likelihood $p(y_{1:n}, \vtheta \mid \vx_{1:n})$ and large entropy $\H{q}$.
The ELBO can also be expressed in various other forms: \vspace{-\baselineskip}\begin{subequations}\begin{align}
  L(q, p; \spD_n) &= \E[\vtheta \sim q]{\log p(y_{1:n}, \vtheta \mid \vx_{1:n}) - \log q(\vtheta)} \margintag{using the definition of entropy \eqref{eq:entropy}} \\
  &= \E[\vtheta \sim q]{\log p(y_{1:n} \mid \vx_{1:n}, \vtheta) + \log p(\vtheta) - \log q(\vtheta)} \margintag{using the product rule \eqref{eq:product_rule}} \label{eq:elbo_expanded} \\
  &= \underbrace{\E[\vtheta \sim q]{\log p(y_{1:n} \mid \vx_{1:n}, \vtheta)}}_{\text{log-likelihood}}\ \underbrace{-\ \KL{q}{p(\cdot)}}_{\text{proximity to prior}}. \margintag{using the definition of KL-divergence \eqref{eq:kl}} \label{eq:elbo2}
\end{align}\end{subequations} where we denote by $p(\cdot)$ the prior distribution.
\Cref{eq:elbo2} highlights the connection to probabilistic inference, namely that maximizing the ELBO selects a variational distribution $q$ that is close to the prior distribution $p(\cdot)$ while also maximizing the \emph{average} likelihood of the data $p(y_{1:n} \mid \vx_{1:n}, \vtheta)$ for $\vtheta \sim q$.
This is in contrast to maximum a posteriori estimation, which picks a \emph{single} model $\vtheta$ that maximizes the likelihood and proximity to the prior.
As an example, let us look at the case where the prior is noninformative, i.e., $p(\cdot) \propto 1$.
In this case, the ELBO simplifies to $\E[\vtheta \sim q]{\log p(y_{1:n}, \mid \vx_{1:n}, \vtheta)} + \H{q} + \const$.
That is, maximizing the ELBO maximizes the average likelihood of the data under the variational distribution $q$ while regularizing $q$ to have high entropy.
Why is it reasonable to maximize the entropy of $q$?
Consider two distributions $q_1$ and $q_2$ under which the data is ``equally'' likely and which are ``equally'' close to the prior.
Maximizing the entropy selects the distribution that exhibits the most uncertainty which is in accordance with the maximum entropy principle.\footnote{In \cref{sec:fundamentals:inference:priors}, we discussed the maximum entropy principle and the related principle of indifference at length. In simple terms, the maximum entropy principle states that without information, we should choose the distribution that is maximally uncertain.}

Recalling that KL-divergence is non-negative, it follows from \cref{eq:elbo_reverse_kl_relationship} that the evidence lower bound is a (uniform\footnote{That is, the bound holds for any variational distribution $q$ (with full support).}) lower bound to the \midx{evidence} $\log p(y_{1:n} \mid \vx_{1:n})$: \begin{align}
  \log p(y_{1:n} \mid \vx_{1:n}) &\geq L(q, p; \spD_n). \label{eq:log_evidence_bound}
\end{align}
This indicates that maximizing the evidence lower bound is an adequate method of model selection which can be used instead of maximizing the evidence (marginal likelihood) directly (as was discussed in \cref{sec:gp:model_selection:marginal_likelihood}).
Note that this inequality lower bounds the logarithm of an integral by an expectation of a logarithm over some variational distribution $q$.
Hence, the ELBO is a family of lower bounds --- one for each variational distribution.
Such inequalities are called variational inequalities.\looseness=-1

\begin{ex}{Gaussian VI vs Laplace approximation}{}
  Consider maximizing the ELBO within the variational family of Gaussians $\spQ \defeq \braces*{q(\vtheta) = \N[\vtheta]{\vmu}{\mSigma}}$.
  How does this relate to Laplace approximation which also fits a Gaussian approximation to the posterior \begin{align*}
    p(\vtheta \mid \spD) \propto p(y_{1:n}, \vtheta \mid \vx_{1:n})?
  \end{align*}
  It turns out that both approximations are closely related.
  Indeed, it can be shown that while the Laplace approximation is fitted \emph{locally} at the MAP estimate $\vthetahat$, satisfying \begin{align}\begin{split}
    \vzero &= \grad_{\vtheta} \log p(y_{1:n}, \vtheta \mid \vx_{1:n}) \\
    \inv{\mSigma} &= - \left. \hes_{\vtheta} \log p(y_{1:n}, \vtheta \mid \vx_{1:n}) \right\rvert_{\vtheta = \vthetahat},
  \end{split}\label{eq:posterior_laplace_approx}\end{align} Gaussian variational inference satisfies the conditions of the Laplace approximation \emph{on average} with respect to the approximation $q$ \exerciserefmark{gaussian_vi_vs_laplace}: \begin{align}\begin{split}
    \vzero &= \E[\vtheta \sim q]{\grad_{\vtheta} \log p(y_{1:n}, \vtheta \mid \vx_{1:n})} \\
    \inv{\mSigma} &= - \E[\vtheta \sim q]{\hes_{\vtheta} \log p(y_{1:n}, \vtheta \mid \vx_{1:n})}.
  \end{split}\label{eq:gaussian_vi_conditions}\end{align}
  For this reason, the Gaussian variational approximation does not suffer from the same overconfidence as the Laplace approximation.\safefootnote{see \cref{fig:laplace_approx_overconfident}}\looseness=-1
\end{ex}

\begin{ex}{ELBO for Bayesian logistic regression}{}
  Recall that Bayesian logistic regression uses the prior distribution ${\vw \sim \N{\vzero}{\mI}}$.\safefootnote{We omit the scaling factor $\sigmap^2$ here for simplicity.}

  Suppose we use the variational family $\spQ$ of all diagonal Gaussians from \cref{ex:var_family_diag_gaussian}.
  We have already seen in \cref{eq:kl_brl} that for a prior ${p \sim \SN}$ and a variational distribution \begin{align*}
    q_\vlambda \sim \N{\vmu}{\diag[i\in[d]]{\sigma_i^2}},
  \end{align*} we have \begin{align*}
    \KL{q}{p(\cdot)} = \frac{1}{2} \sum_{i=1}^d (\sigma_i^2 + \mu_i^2 - 1 - \log \sigma_i^2).
  \end{align*}

  It remains to find the expected likelihood under models from our approximate posterior: \begin{align}
    \E[\vw \sim q_\vlambda]{\log p(y_{1:n} \mid \vx_{1:n}, \vw)} &= \E[\vw \sim q_\vlambda]{\sum_{i=1}^n \log p(y_i \mid \vx_i, \vw)} \margintag{using independence of the data} \nonumber \\
    &= \E[\vw \sim q_\vlambda]{- \sum_{i=1}^n \ell_{\mathrm{log}}(\vw; \vx_i, y_i)}. \margintag{substituting the logistic loss \eqref{eq:logistic_loss}}
  \end{align}
\end{ex}

\subsection{Gradient of Evidence Lower Bound}\label{sec:approximate_inference:variational_inference:gradient_of_elbo}

We have yet to discuss how the optimization problem of maximizing the ELBO can be solved efficiently.
A suitable tool is stochastic gradient descent (SGD), however, SGD requires unbiased gradient estimates of the loss $\ell(\vlambda; \spD_n) \defeq - L(q_\vlambda, p; \spD_n)$.
That is, we need to obtain gradient estimates of \begin{align}
  \grad_\vlambda L(q_\vlambda, p; \spD_n) = \grad_\vlambda \E[\vtheta \sim q_\vlambda]{\log p(y_{1:n} \mid \vx_{1:n}, \vtheta)} - \grad_\vlambda \KL{q_\vlambda}{p(\cdot)}. \margintag{using the definition of the ELBO \eqref{eq:elbo2}}
\end{align}
Typically, the KL-divergence (and its gradient) can be computed exactly for commonly used variational families.
For example, we have already seen a closed-form expression of the KL-divergence for Gaussians in \cref{eq:kl_gaussian}.

Obtaining the gradient of the expected log-likelihood is more difficult.
This is because the expectation integrates over the measure $q_\vlambda$, which depends on the variational parameters $\vlambda$.
Thus, we cannot move the gradient operator inside the expectation as commonly done (cf. \cref{sec:background:probability:gradients_of_expectations}).
There are two main techniques which are used to rewrite the gradient in such a way that Monte Carlo sampling becomes possible.\looseness=-1

One approach is to use \midx<score gradients>{score gradient estimator} via the ``score function trick'': \begin{align}
  \begin{multlined}
    \grad_{\vlambda} \E[\vtheta \sim q_\vlambda]{\log p(y_{1:n} \mid \vx_{1:n}, \vtheta)} \\ = \E*[\vtheta \sim q_\vlambda]{[\log p(y_{1:n} \mid \vx_{1:n}, \vtheta) \underbrace{\grad_{\vlambda} \log q_{\vlambda}(\vtheta)}_{\text{\midx{score function}[idxpagebf]}}]},
  \end{multlined}
\end{align} which we introduce in \cref{sec:mfarl:policy_approximation:reinforce} in the context of reinforcement learning.
More common in the context of variational inference is the so-called ``reparameterization trick''.

\begin{thm}[Reparameterization trick]\pidx{reparameterization trick}[idxpagebf]\label{thm:reparameterization_trick}
  Given a random variable $\vvarepsilon \sim \phi$ (which is independent of $\vlambda$) and given a differentiable and invertible function $\vg : \R^d \to \R^d$.
  We let $\vtheta \defeq \vg(\vvarepsilon; \vlambda)$.
  Then, \begin{align}
    q_\vlambda(\vtheta) &= \phi(\vvarepsilon) \cdot \inv{\abs{\det{\jac_\vvarepsilon \vg(\vvarepsilon; \vlambda)}}}, \label{eq:reparameterization_trick1} \\
    \E[\vtheta \sim q_\vlambda]{\vf(\vtheta)} &= \E[\vvarepsilon \sim \phi]{\vf(\vg(\vvarepsilon; \vlambda))} \label{eq:reparameterization_trick2}
  \end{align} for a ``nice'' function $\vf : \R^d \to \R^e$.
\end{thm}
\begin{proof}
  By the change of variables formula \eqref{eq:change_of_variables} and using $\vvarepsilon = \inv{\vg}(\vtheta; \vlambda)$, \begin{align*}
    q_\vlambda(\vtheta) &= \phi(\vvarepsilon) \cdot \abs{\det{\jac_\vtheta \inv{\vg}(\vtheta; \vlambda)}} \\
    &= \phi(\vvarepsilon) \cdot \abs{\det{\inv{(\jac_\vvarepsilon \vg(\vvarepsilon; \vlambda))}}} \margintag{by the inverse function theorem, $\jac\inv{\vg}(\vy) = \inv{\jac\vg(\vx)}$} \\[6pt]
    &= \phi(\vvarepsilon) \cdot \inv{\abs{\det{\jac_\vvarepsilon \vg(\vvarepsilon; \vlambda)}}}. \margintag{using $\det{\inv{\mA}} = \inv{\det{\mA}}$}
  \end{align*}
  \Cref{eq:reparameterization_trick2} is a direct consequence of the law of the unconscious statistician \eqref{eq:lotus}.
\end{proof}

In other words, the reparameterization trick allows a change of ``densities'' by finding a function $\vg(\cdot; \vlambda)$ and a reference density $\phi$ such that $q_{\vlambda} = \pf{\vg(\cdot; \vlambda)}{\phi}$ is the pushforward of $\phi$ under perturbation $\vg$.
Applying the reparameterization trick, we can swap the order of gradient and expectation, \begin{align}
  \grad_\vlambda \E[\vtheta \sim q_\vlambda]{\vf(\vtheta)} = \E[\vvarepsilon \sim \phi]{\grad_\vlambda \vf(\vg(\vvarepsilon; \vlambda))}. \margintag{using \cref{eq:swap_grad_exp_order}}
\end{align}
We call a distribution $q_\vlambda$ \midx<reparameterizable>{reparameterizable distribution} if it admits reparameterization, i.e., if we can find $\vg$ and a suitable reference density $\phi$ which is independent of $\vlambda$.

\begin{ex}{Reparameterization trick for Gaussians}{reparameterization_trick_gaussian}
  Suppose we use a Gaussian variational approximation, \begin{align*}
    q_\vlambda(\vtheta) \defeq \N[\vtheta]{\vmu}{\mSigma},
  \end{align*} where we assume $\mSigma$ to have full rank (i.e., be invertible).
  We have seen in \cref{eq:gaussian_lin_trans} that a Gaussian random vector ${\vvarepsilon \sim \SN}$ following a standard normal distribution can be transformed to follow the Gaussian distribution $q_\vlambda$ by using the linear transformation,\looseness=-1 \begin{align}
    \vtheta = \vg(\vvarepsilon; \vlambda) \defeq \msqrt{\mSigma} \vvarepsilon + \vmu. \label{eq:reparameterization_trick_gaussian}
  \end{align}

  In particular, we have \begin{align}
    \vvarepsilon &= \inv{\vg}(\vtheta; \vlambda) = \mSigma^{-\nicefrac{1}{2}}(\vtheta - \vmu) \quad \text{and} \margintag{by solving \cref{eq:reparameterization_trick_gaussian} for $\vvarepsilon$} \\
    \phi(\vvarepsilon) &= q_\vlambda(\vtheta) \cdot \abs{\det{\msqrt{\mSigma}}}. \margintag{using the reparameterization trick (i.e., the change of variables formula) \eqref{eq:reparameterization_trick1}}
  \end{align}
\end{ex}

In the following, we write $\mC \defeq \msqrt{\mSigma}$.
Let us now derive the gradient estimate for the evidence lower bound assuming the Gaussian variational approximation from \cref{ex:reparameterization_trick_gaussian}.
This approach extends to any reparameterizable distribution.
\begin{align}
  &\grad_\vlambda \E[\vtheta \sim q_\vlambda]{\log p(y_{1:n} \mid \vx_{1:n}, \vtheta)} \nonumber \\
  &= \grad_{\mC,\vmu} \E[\vvarepsilon \sim \SN]{\left. \log p(y_{1:n} \mid \vx_{1:n}, \vtheta) \right\rvert_{\vtheta = \mC \vvarepsilon + \vmu}} \label{eq:reparameterization_trick_application} \margintag{using the reparameterization trick \eqref{eq:reparameterization_trick2}} \\
  &= n \cdot \grad_{\mC,\vmu} \E[\vvarepsilon \sim \SN]{\frac{1}{n} \sum_{i=1}^n \left. \log p(y_i \mid \vx_i, \vtheta) \right\rvert_{\vtheta = \mC \vvarepsilon + \vmu}} \margintag{using independence of the data and extending with $\nicefrac{n}{n}$} \nonumber \\
  &= n \cdot \grad_{\mC,\vmu} \E*[\vvarepsilon \sim \SN]{\E[i \sim \Unif{\brackets{n}}]{\left. \log p(y_i \mid \vx_i, \vtheta) \right\rvert_{\vtheta = \mC \vvarepsilon + \vmu}}} \margintag{interpreting the sum as an expectation} \nonumber \\
  &= n \cdot \E*[\vvarepsilon \sim \SN]{\E[i \sim \Unif{\brackets{n}}]{\grad_{\mC,\vmu} \left. \log p(y_i \mid \vx_i, \vtheta) \right\rvert_{\vtheta = \mC \vvarepsilon + \vmu}}} \margintag{using \cref{eq:swap_grad_exp_order}} \\
  &\approx n \cdot \frac{1}{m} \sum_{j=1}^m \grad_{\mC,\vmu} \left. \log p(y_{i_j} \mid x_{i_j}, \vtheta) \right\rvert_{\vtheta = \mC \vvarepsilon_j + \vmu} \margintag{using Monte Carlo sampling}
\end{align} where $\vvarepsilon_j \iid \SN$ and $i_j \iid \Unif{[n]}$.
This yields an unbiased gradient estimate, which we can use with stochastic gradient descent to maximize the evidence lower bound.
We have successfully recast the difficult problems of learning and inference as an optimization problem!\looseness=-1

The procedure of approximating the true posterior using a variational posterior by maximizing the evidence lower bound using stochastic optimization is also called \midx{black box stochastic variational inference} \citep{ranganath2014black,titsias2014doubly,duvenaud2015black}.
The only requirement is that we can obtain unbiased gradient estimates from the evidence lower bound (and the likelihood).
We have just discussed one of many approaches to obtain such gradient estimates \citep{mohamed2020monte}.
If we use the variational family of diagonal Gaussians, we only require twice as many parameters as other inference techniques like MAP estimation.
The performance can be improved by using natural gradients and variance reduction techniques for the gradient estimates such as control variates.\looseness=-1

\subsection{Minimizing Surprise via Exploration and Exploitation}\label{sec:free_energy}

Now that we have established a way to optimize the ELBO, let us dwell a bit more on its interpretation.
Observe that the evidence can also be interpreted as the negative surprise about the observations under the prior distribution $p(\vtheta)$, and by negating \cref{eq:log_evidence_bound}, we obtain the variational upper bound \begin{align}\begin{split}
  \S{p(y_{1:n} \mid \vx_{1:n})} &\leq \underbrace{\E[\vtheta \sim q]{\S{p(y_{1:n}, \vtheta \mid \vx_{1:n})}}}_{\text{called \emph{energy}}} - \H{q} \\
  &= - L(q, p; \spD_n).
\end{split}\end{align}
Here, $-L(q, p; \spD_n)$ is commonly called the \midx<(variational) free energy>{free energy} with respect to $q$.
Free energy can also be characterized as \begin{align}
  - L(q, p; \spD_n) &= \underbrace{\E[\vtheta \sim q]{\S{p(y_{1:n} \mid \vx_{1:n}, \vtheta)}}}_{\text{average surprise}} + \underbrace{\KL{q}{p(\cdot)}}_{\text{proximity to prior}}, \margintag{analogously to \cref{eq:elbo2}}
\end{align} and therefore its minimization is minimizing the average surprise about the data under the variational distribution $q$ while maximizing proximity to the prior $p(\cdot)$.
Systems that minimize the surprise in their observations are widely studied in many areas of science.\footnote{Refer to the \idx{free energy principle}[idxpagebf] which was originally introduced by the neuroscientist Karl Friston \citep{friston2010free}.}

To minimize surprise, the free energy makes apparent a natural tradeoff between two extremes:
On the one hand, models $q(\vtheta)$ that ``overfit'' to observations (e.g., by using a point estimate of $\vtheta$), and hence, result in a large surprise when new observations deviate from this specific belief.
On the other hand, models $q(\vtheta)$ that ``underfit'' to observations (e.g., by expecting outcomes with equal probability), and hence, any observation results in a non-negligible surprise.
Either of these extremes is undesirable.

As we have alluded to previously when introducing the ELBO, the two terms constituting free energy map neatly onto this tradeoff.
Therein, the \emph{entropy} (which is maximized) encourages $q$ to have uncertainty, in other words, to ``explore'' beyond the finite data.
In contrast, the \emph{energy} (which is minimized) encourages $q$ to fit the observed data closely, in other words, to ``exploit'' the finite data.
This tradeoff is ubiquitous in approximations to probabilistic inference that deal with limited computational resources and limited time, and we will encounter it many times more.
We will point out these connections as we go along.

Since this tradeoff is so fundamental and appears in many branches of science under different names, it is difficult to give it an appropriate unifying name.
The essence of this tradeoff can be captured as a \midx{principle of curiosity and conformity}[idxpagebf], which suggests that reasoning under uncertainty requires curiosity to entertain and pursue alternative explanations of the data \emph{and} conformity to make consistent predictions.

\section*{Discussion}

We have explored variational inference, where we approximate the intractable posterior distribution of probabilistic inference with a simpler distribution.
We operationalized this idea by turning the inference problem, which requires computing high-dimensional integrals, into a tractable optimization problem.
Gaussians are frequently used as variational distributions due to their versatility and compact representation.

Nevertheless, recall from \cref{fig:variational_families} that while the estimation error in variational inference can be small, choosing a variational family that is too simple can lead to a large approximation error.
We have seen that for posteriors that are multimodal or have heavy tails, Gaussians may not provide a good approximation.
In the next chapter, we will explore alternative techniques for approximate inference that can handle more complex posteriors.

\excheading

\begin{nexercise}{Logistic loss}{logistic_loss_gradient}
  \begin{enumerate}
    \item Derive the gradient of $\ell_\mathrm{log}$ as given in \cref{eq:logistic_loss_gradient}.
    \item Show that \begin{align}
      \hes_\vw \ell_\mathrm{log}(\transpose{\vw} \vx; y) = \vx \transpose{\vx} \cdot \sigma(\transpose{\vw} \vx) \cdot (1 - \sigma(\transpose{\vw} \vx)). \label{eq:logistic_loss_hes}
    \end{align}
    \textit{Hint: Begin by deriving the first derivative of the logistic function, and use the chain rule of multivariate calculus, \begin{align}
      \underbrace{\mD_\vx (\vf \circ \vg)}_{\R^n \to \R^{m \times n}} = \underbrace{(\mD_{\vg(\vx)} \vf \circ \vg) \cdot \mD_\vx \vg}_{\R^n \to \R^{m \times k} \cdot \R^{k \times n}} \label{eq:chain_rule}
    \end{align} where $\vg : \R^n \to \R^k$ and $\vf : \R^k \to \R^m$.}
    \item Is the logistic loss $\ell_\mathrm{log}$ convex in $\vw$?
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Gaussian process classification}{gpc}
  In this exercise, we will study the use of Gaussian processes for classification tasks, commonly called \midx{Gaussian process classification}[idxpagebf] (GPC).
  Linear logistic regression is extended to GPC by replacing the Gaussian prior over weights with a GP prior on $f$, \begin{align}
    f \sim \GP{0}{k}, \quad y \mid \vx, f \sim \Bern{\sigma(f(\vx))} \label{eq:gpc1}
  \end{align} where $\sigma : \mathbb{R} \to (0,1)$ is some logistic-type function.
  Note that Bayesian logistic regression is the special case where $k$ is the linear kernel and~$\sigma$ is the logistic function.
  This is analogous to the relationship of Bayesian linear regression and Gaussian process regression.

  In the GP regression setting of \cref{sec:gp}, $y_i$ was assumed to be a noisy observation of $f(\vx_i)$.
  In the classification setting, we now have that $y_i \in \{-1,+1\}$ is a binary class label and $f(\vx_i) \in \R$ is a latent value.
  We study the setting where $\sigma(z) = \Phi(z; 0, \sigman^2)$ is the CDF of a univariate Gaussian with mean $0$ and variance $\sigman^2$, also called a \midx{probit likelihood}.

  To make probabilistic predictions for a query point $\vxs$, we first compute the distribution of the latent variable $\fs$,
  \begin{align}
    p(\fs \mid \vx_{1:n}, y_{1:n}, \vxs) = \int p(\fs \mid \vx_{1:n}, \vxs, \vf) p(\vf \mid x_{1:n}, y_{1:n}) \,d \vf \label{eq:gpc_latent_predictive_posterior} \margintag{using sum rule \eqref{eq:sum_rule} and product rule \eqref{eq:product_rule}, and $\fs \perp y_{1:n} \mid \vf$}
  \end{align} where $p(\vf \mid \vx_{1:n}, y_{1:n})$ is the posterior over the latent variables.

  \begin{enumerate}
    \item Assuming that we can efficiently compute $p(\fs \mid \vx_{1:n}, y_{1:n}, \vxs)$ (approximately), describe how we can find the predictive posterior $p(\ys = +1 \mid \vx_{1:n}, y_{1:n}, \vxs)$.

    \item The posterior over the latent variables is not a Gaussian as we used a non-Gaussian likelihood, and hence, the integral of the latent predictive posterior \eqref{eq:gpc_latent_predictive_posterior} is analytically intractable.
    A common technique is to approximate the latent posterior $p(\vf \mid \vx_{1:n}, y_{1:n})$ with a Gaussian using a Laplace approximation $q \defeq \N{\vfhat}{\inv{\mLambda}}$.
    It is generally not possible to obtain an analytical representation of the mode of the Laplace approximation $\vfhat$. Instead, $\vfhat$ is commonly found using a second-order optimization scheme such as Newton's method.\looseness=-1
    \begin{enumerate}
      \item Find the precision matrix $\mLambda$ of the Laplace approximation.

      \textit{Hint: Observe that for a label $y_i \in \{-1,+1\}$, the probability of a correct classification given the latent value $f_i$ is $p(y_i \mid f_i) = \sigma(y_i f_i)$, where we use the symmetry of the probit likelihood around $0$.}

      \item Assume that $k(\vx, \vxp) = \transpose{\vx}\vxp$ is the linear kernel ($\sigmap = 1$) and that $\sigma$ is the logistic function \eqref{eq:logistic_function}.
      Show for this setting that the matrix $\mLambda$ derived in (a) is equivalent to the precision matrix $\mLambda'$ of the Laplace approximation of Bayesian logistic regression \eqref{eq:prec_mat_lap_approx_blr}.\safefootnote{This should not be surprising since --- as already mentioned --- Gaussian process classification is a generalization of Bayesian logistic regression.}
      You may assume that $\hat{f}_i = \transpose{\vwhat} \vx_i$.

      \textit{Hint: First derive under which condition $\mLambda$ and $\mLambda'$ are ``equivalent''.}

      \item Observe that the (approximate) latent predictive posterior \begin{align*}
        q(\fs \mid \vx_{1:n}, y_{1:n}, \vxs) \defeq \int p(\fs \mid \vx_{1:n}, \vxs, \vf) q(\vf \mid x_{1:n}, y_{1:n}) \,d \vf
      \end{align*} which uses the Laplace approximation of the latent posterior is Gaussian.\safefootnote{Using the Laplace-approximated latent posterior, $[\fs \; \vf]$ are jointly Gaussian. Thus, it directly follows from \cref{fct:marginal_and_cond_gaussian} that the marginal distribution over $\fs$ is also a Gaussian.}
      Determine its mean and variance.

      \textit{Hint: Condition on the latent variables $\vf$ using the laws of total expectation and variance.}

      \item Compare the prediction $p(\fs \mid \vx_{1:n}, y_{1:n}, \vxs)$ you obtained in (1) (but now using the Laplace-approximated latent predictive posterior) to the prediction $\sigma(\E[\fs \sim q]{\fs})$. Are they identical? If not, describe how they are different.
    \end{enumerate}

    \item The use of the probit likelihood may seem arbitrary.
    Consider the following model which may be more natural, \begin{align}
      f \sim \GP{0}{k}, \quad y = \Ind{\underbrace{f(\vx) + \varepsilon}_{\text{GP regression}} \geq 0}, \quad \varepsilon \sim \N{0}{\sigman^2}. \label{eq:gpc2}
    \end{align}
    Show that the model from \cref{eq:gpc1} using a noise-free latent process with probit likelihood $\Phi(z; 0, \sigman^2)$ is equivalent (in expectation over $\varepsilon$) to the model from \cref{eq:gpc2}.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Jensen's inequality}{jensen}
  \begin{enumerate}
    \item Prove the finite form of Jensen's inequality.

    \begin{thm}[Jensen's inequality, finite form]
      Let $f : \R^n \to \R$ be a convex function. Suppose that $\vx_1, \dots, \vx_k \in \R^n$ and $\theta_1, \dots, \theta_k \geq 0$ with $\theta_1 + \dots + \theta_k = 1$. Then, \begin{align}
        f(\theta_1 \vx_1 + \dots + \theta_k \vx_k) \leq \theta_1 f(\vx_1) + \dots + \theta_k f(\vx_k).
      \end{align}
    \end{thm}

    Observe that if $X$ is a random variable with finite support, the above two versions of Jensen's inequality are equivalent.

    \item Show that for any discrete distribution $p$ supported on a finite domain of size $n$, $\H{p} \leq \log_2 n$.
    This implies that the uniform distribution has maximum entropy.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Binary cross-entropy loss}{bce_loss}
  Show that the logistic loss \eqref{eq:logistic_loss} is equivalent to the binary cross-entropy loss with $\hat{y} = \sigma(\hat{f})$.
  That is, \begin{align}
    \ell_\mathrm{log}(\hat{f}; y) = \ell_\mathrm{bce}(\hat{y}; y).
  \end{align}
\end{nexercise}

\begin{nexercise}{Gibbs' inequality}{gibbs_ineq}
  \begin{enumerate}
    \item Prove $\KL{p}{q} \geq 0$ which is also known as \midx{Gibbs' inequality}.
    \item Let $p$ and $q$ be discrete distributions with finite identical support $\sA$.
    Show that $\KL{p}{q} = 0$ if and only if $p \equiv q$.\par
    \textit{Hint: Use that if a function $f : \R^n \to \R$ is strictly convex and $\vx_1, \dots, \vx_k \in \R^n$, $\theta_1, \dots, \theta_k \geq 0$, $\theta_1 + \dots + \theta_k = 1$, we have that \begin{align}
      f(\theta_1 \vx_1 + \dots + \theta_k \vx_k) = \theta_1 f(\vx_1) + \dots + \theta_k f(\vx_k)
    \end{align} iff $\vx_1 = \dots = \vx_k$. This is a slight adaptation of Jensen's inequality in finite-form, which you proved in \cref{exercise:jensen}.}
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Maximum entropy principle}{maximum_entropy_principle}
  In this exercise we will prove that the normal distribution is the distribution with maximal entropy among all (univariate) distributions supported on $\R$ with fixed mean $\mu$ and variance $\sigma^2$.
  Let ${g(x) \defeq \N[x]{\mu}{\sigma^2}}$, and $f(x)$ be any distribution on~$\R$ with mean~$\mu$ and variance~$\sigma^2$.
  \begin{enumerate}
    \item Prove that $\KL{f}{g} = \H{g} - \H{f}$.\par
    \textit{Hint: Equivalently, show that $\crH{f}{g} = \H{g}$.
    That is, the expected surprise evaluated based on the Gaussian $g$ is invariant to the true distribution $f$.}
    \item Conclude that $\H{g} \geq \H{f}$.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Probabilistic inference as a consequence of the maximum entropy principle}{mep_and_posteriors}
  Consider the family of generative models of the random vectors $\rX$ in~$\spX$ and $\rY$ in~$\spY$: \begin{align*}
    \Delta^{\spX \times \spY} = \left\{q : \spX \times \spY \to \Rzero\ \middle|\ \text{$\int_{\spX\times\spY} q(\vx,\vy) \,d\vx \,d\vy = 1$}\right\}.
  \end{align*}
  Suppose that we observe $\rY$ to be $\vyp$, and are looking for a (new) generative model that is consistent with this information, that is, \begin{align*}
    q(\vy) = \int_\spX q(\vx,\vy) \,d\vx = \delta_{\vyp}(\vy) \margintag{using the sum rule \eqref{eq:sum_rule}}
  \end{align*} where $\delta_{\vyp}$ denotes the point density at $\vyp$.
  The product rule \eqref{eq:product_rule} implies that $q(\vx,\vy) = \delta_{\vyp}(\vy) \cdot q(\vx \mid \vy)$, but any choice of $q(\vx \mid \vy)$ is possible.\looseness=-1

  We will derive that given any fixed generative model $p_{\rX,\rY}$, the ``posterior'' distribution $q_{\rX}(\cdot) = p_{\rX\mid\rY}(\cdot \mid \vyp)$ minimizes the relative entropy $\KL{q_{\rX,\rY}}{p_{\rX,\rY}}$ subject to the constraint $\rY = \vyp$.
  In other words, among all distributions $q_{\rX,\rY}$ that are consistent with the observation $\rY = \vyp$, the posterior distribution $q_{\rX}(\cdot) = p_{\rX\mid\rY}(\cdot \mid \vyp)$ is the one with ``maximum entropy''.

  \begin{enumerate}
    \item Show that the optimization problem \begin{align*}
      &\argmin_{q \in \Delta^{\spX \times \spY}} \KL{q_{\rX,\rY}}{p_{\rX,\rY}} \\
      &\text{subject to}\ q(\vy) = \delta_{\vyp}(\vy) \quad \forall \vy \in \spY
    \end{align*} is solved by $q(\vx,\vy) = \delta_{\vyp}(\vy) \cdot p(\vx \mid \vy)$. \\
    \textit{Hint: Solve the dual problem.}

    \item Conclude that $q(\vx) = p(\vx \mid \vyp)$.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{KL-divergence of Gaussians}{kl_div_of_gaussians}
  Derive \cref{eq:kl_gaussian}.

  \textit{Hint: If $\rX \sim \N{\vmu}{\mSigma}$ in $d$ dimensions, then we have that for any $\vm \in \R^d$ and $\mA \in \R^{d \times d}$, \begin{align}
    \E{\transpose{(\rX - \vm)}\mA(\rX - \vm)} = \transpose{(\vmu - \vm)}\mA(\vmu - \vm) + \tr{\mA\mSigma} \label{eq:kl_div_of_gaussians_hint}
  \end{align}}
\end{nexercise}

\begin{nexercise}{Forward vs reverse KL}{forward_vs_reverse_kl}
  \begin{enumerate}
    \item Consider a factored approximation $q(x, y) = q(x)q(y)$ to a joint distribution $p(x, y)$.
    Show that to minimize the forward $\KL{p}{q}$ we should set $q(x) = p(x)$ and $q(y) = p(y)$, i.e., the optimal approximation is a product of marginals.

    \item Consider the following joint distribution $p$, where the rows represent $y$ and the columns $x$:

    \begin{center}
      \vspace{5pt}
      \begin{tabular}{l|cccc}
        \toprule
        & $1$ & $2$ & $3$ & $4$ \\
        \midrule
        $1$ & $\nicefrac{1}{8}$ & $\nicefrac{1}{8}$ & $0$ & $0$ \\
        $2$ & $\nicefrac{1}{8}$ & $\nicefrac{1}{8}$ & $0$ & $0$ \\
        $3$ & $0$ & $0$ & $\nicefrac{1}{4}$ & $0$ \\
        $4$ & $0$ & $0$ & $0$ & $\nicefrac{1}{4}$ \\
        \bottomrule
      \end{tabular}
      \vspace{5pt}
    \end{center}

    Show that the reverse $\KL{q}{p}$ for this $p$ has three distinct minima.
    Identify those minima and evaluate $\KL{q}{p}$ at each of them.

    \item What is the value of $\KL{q}{p}$ if we use the approximation ${q(x, y) = p(x) p(y)}$?
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Gaussian VI vs Laplace approximation}{gaussian_vi_vs_laplace}
  In this exercise, we compare the Laplace approximation from \cref{sec:approximate_inference:laplace_approximation} to variational inference with the variational family of Gaussians,\looseness=-1 \begin{align*}
    \spQ \defeq \braces*{q(\vtheta) = \N[\vtheta]{\vmu}{\mSigma}}.
  \end{align*}
  \begin{enumerate}
    \item Let $p$ be any distribution on $\R$, and let $\opt{q} = \argmin_{q \in \spQ} \KL{p}{q}$.
    Show that $\opt{q}$ differs from the Laplace approximation of $p$.
  \end{enumerate}

  Minimizing forward-KL is typically intractable, and we have seen that it is therefore common to minimize the reverse-KL instead: \begin{align*}
    \tilde{q} = \argmin_{q \in \spQ} \KL{q}{p(\cdot \mid \spD_n)}.
  \end{align*}
  \begin{enumerate}
    \setcounter{enumi}{1}
    \item Show that $\tilde{q} = \N{\vmu}{\mSigma}$ satisfies \cref{eq:gaussian_vi_conditions}: \begin{align*}
      \vzero &= \E[\vtheta \sim \tilde{q}]{\grad_{\vtheta} \log p(y_{1:n}, \vtheta \mid \vx_{1:n})} \\
      \inv{\mSigma} &= - \E[\vtheta \sim \tilde{q}]{\hes_{\vtheta} \log p(y_{1:n}, \vtheta \mid \vx_{1:n})}.
    \end{align*}

    \textit{Hint 1: For any positive definite and symmetric matrix $\mA$, it holds that $\grad_{\mA} \log\det{\mA} = \inv{\mA}$.}

    \textit{Hint 2: For any function $f$ and Gaussian $p = \N{\vmu}{\mSigma}$, \begin{align}\begin{split}
      \grad_{\vmu} \E[\vx \sim p]{f(\vx)} &= \E[\vx \sim p]{\grad_{\vx} f(\vx)} \\
      \grad_{\mSigma} \E[\vx \sim p]{f(\vx)} &= \frac{1}{2} \E[\vx \sim p]{\hes_{\vx} f(\vx)}.
    \end{split}\label{eq:gradient_of_gaussian_parameters}\end{align}}
  \end{enumerate}

  Recall the conditions satisfied by the Laplace approximation of the posterior $p(\vtheta \mid \spD) \propto p(y_{1:n}, \vtheta \mid \vx_{1:n})$ as detailed in \cref{eq:posterior_laplace_approx}.
  The Laplace approximation is fitted \emph{locally} at the MAP estimate $\vthetahat$.
  Comparing \cref{eq:posterior_laplace_approx} to \cref{eq:gaussian_vi_conditions}, we see that Gaussian variational inference satisfies the conditions of the Laplace approximation \emph{on average}.
  For more details, refer to \cite{opper2009variational}.
\end{nexercise}

\begin{nexercise}{Gradient of reverse-KL}{gradient_of_reverse_kl}
  Suppose $p \defeq \N{\vzero}{\sigmap^2 \mI}$ and a tractable distribution described by \begin{align*}
    q_\vlambda \defeq \N{\vmu}{\diag{\sigma_1^2, \dots, \sigma_d^2}}
  \end{align*} where $\vmu \defeq [\mu_1 \; \cdots \; \mu_d]$ and $\vlambda \defeq [\mu_1 \; \cdots \; \mu_d \; \sigma_1 \; \cdots \; \sigma_d]$.
  Show that the gradient of $\KL{q_\vlambda}{p(\cdot)}$ with respect to $\vlambda$ is given by \begin{subequations}\begin{align}
    \grad_\vmu \KL{q_\vlambda}{p(\cdot)} &= \sigmap^{-2} \vmu, \quad\text{and} \\
    \grad_{[\sigma_1 \; \cdots \; \sigma_d]} \KL{q_\vlambda}{p(\cdot)} &= \begin{bmatrix}
      \frac{\sigma_1}{\sigmap^2} - \frac{1}{\sigma_1} & \dots & \frac{\sigma_d}{\sigmap^2} - \frac{1}{\sigma_d}
    \end{bmatrix}.
  \end{align}\end{subequations}
\end{nexercise}

\begin{nexercise}{Reparameterizable distributions}{reparameterizable_distributions}
  \begin{enumerate}
    \item Let $X \sim \Unif{[a,b]}$ for any $a \leq b$.
    That is, \begin{align}
      p_X(x) = \begin{cases}
        \frac{1}{b-a} & \text{if $x \in [a,b]$} \\
        0 & \text{otherwise}. \\
      \end{cases}
    \end{align}
    Show that $X$ can be reparameterized in terms of $\Unif{[0,1]}$.
    \textit{Hint: You may use that for any $Y \sim \Unif{[a,b]}$ and $c \in \R$, \begin{itemize}
      \item $Y + c \sim \Unif{[a + c, b + c]}$ and
      \item $cY \sim \Unif{[c \cdot a, c \cdot b]}$.
    \end{itemize}}

    \item Let $Z \sim \N{\mu}{\sigma^2}$ and $X \defeq e^Z$. That is, $X$ is logarithmically normal distributed with parameters $\mu$ and $\sigma^2$. Show that $X$ can be reparameterized in terms of $\N{0}{1}$.

    \item Show that $\Cauchy{0}{1}$ can be reparameterized in terms of $\Unif{[0,1]}$.
  \end{enumerate}
  Finally, let us apply the reparameterization trick to compute the gradient of an expectation.
  \begin{enumerate}
    \setcounter{enumi}{3}
    \item Let $\mathrm{ReLU}(z) \defeq \max\{0, z\}$ and $w > 0$.
    Show that \begin{align*}
      \odv{}{\mu} \E*[x \sim \N{\mu}{1}]{\mathrm{ReLU}(w x)} = w \Phi(\mu)
    \end{align*} where $\Phi$ denotes the CDF of the standard normal distribution.
  \end{enumerate}
\end{nexercise}









