\chapter{Active Learning}\label{sec:active_learning}

By now, we have seen that probabilistic machine learning is very useful for estimating the uncertainty in our models (epistemic uncertainty) and in the data (aleatoric uncertainty).
We have been focusing on the setting of supervised learning\pidx{supervised learning} where we are given a set $\spD = \{(\vx_i, y_i)\}_{i=1}^n$ of labeled data, yet we often encounter settings where we have only little data and acquiring new data is costly.

In this chapter --- and in the following chapter on Bayesian optimization --- we will discuss how one can use uncertainty to effectively collect more data.
In other words, we want to figure out where in the domain we should sample to obtain the most useful information.
Throughout most of this chapter, we focus on the most common way of quantifying ``useful information'', namely the expected reduction in entropy which is also called the \midx{mutual information}.

\section{Conditional Entropy}

We begin by introducing the notion of conditional entropy.
Recall that the entropy $\H{\rX}$ of a random vector $\rX$ can be interpreted as our average surprise when observing realizations $\vx \sim \rX$.
Thus, entropy can be considered as a quantification of the uncertainty about a random vector (or equivalently, its distribution).\footnote{We discussed entropy extensively in \cref{sec:approximate_inference:information_theory}.}

A natural extension is to consider the entropy of $\rX$ given the occurrence of an event corresponding to another random variable (e.g., $\rY = \vy$ for a random vector $\rY$), \begin{align}
  \H{\rX \mid \rY = \vy} \defeq \E[\vx \sim p(\vx \mid \vy)]{- \log p(\vx \mid \vy)}.
\end{align}
Instead of averaging over the surprise of samples from the distribution~$p(\vx)$ (like the entropy $\H{\rX}$), this quantity simply averages over the surprise of samples from the conditional distribution $p(\vx \mid \vy)$.

\begin{defn}[Conditional entropy]\pidx{conditional entropy}
  The \emph{conditional entropy} of a random vector $\rX$ given the random vector $\rY$ is defined as \begin{align}
    \H{\rX \mid \rY} &\defeq \E[\vy \sim p(\vy)]{\H{\rX \mid \rY = \vy}} \label{eq:cond_entropy} \\
    &= \E[(\vx, \vy) \sim p(\vx, \vy)]{- \log p(\vx \mid \vy)}.
  \end{align}
\end{defn}
Intuitively, the conditional entropy of $\rX$ given $\rY$ describes our average surprise about realizations of~$\rX$ given a particular realization of $\rY$, averaged over all such possible realizations of~$\rY$.
In other words, conditional entropy corresponds to the expected remaining uncertainty in~$\rX$ after we observe~$\rY$.
Note that, in general, $\H{\rX \mid \rY} \neq \H{\rY \mid \rX}$.

It is crucial to stress the difference between $\H{\rX \mid \rY = \vy}$ and the conditional entropy $\H{\rX \mid \rY}$.
The former simply corresponds to a probabilistic update of our uncertainty in $\rX$ after we have observed the realization $\vy \sim \rY$.
In contrast, conditional entropy \emph{predicts} how much uncertainty will remain about $\rX$ (in expectation) after we \emph{will} observe~$\rY$.\looseness=-1

\begin{defn}[Joint entropy]\pidx{joint entropy}
  One can also define the \emph{joint entropy} of random vectors $\rX$ and $\rY$, \begin{align}
    \H{\rX, \rY} &\defeq \E[(\vx, \vy) \sim p(\vx, \vy)]{- \log p(\vx, \vy)},
  \end{align} as the combined uncertainty about $\rX$ and $\rY$.
  Observe that joint entropy is symmetric.
\end{defn}

This gives the \midx{chain rule for entropy}, \begin{align}
  \H{\rX, \rY} &= \H{\rY} + \H{\rX \mid \rY} \label{eq:chain_rule_entropy} \margintag{using the product rule \eqref{eq:product_rule} and the definition of conditional entropy \eqref{eq:cond_entropy}} \\[5pt]
  &= \H{\rX} + \H{\rY \mid \rX}. \label{eq:chain_rule_entropy2} \margintag{using symmetry of joint entropy}
\end{align}
That is, the joint entropy of $\rX$ and $\rY$ is given by the uncertainty about~$\rX$ and the additional uncertainty about $\rY$ given $\rX$.
Moreover, this also yields \midx{Bayes' rule for entropy}, \begin{align}
  \H{\rX \mid \rY} &= \H{\rY \mid \rX} + \H{\rX} - \H{\rY}. \label{eq:bayes_rule_entropy} \margintag{using the chain rule for entropy \eqref{eq:chain_rule_entropy} twice}
\end{align}

A very intuitive property of entropy is its monotonicity: when conditioning on additional observations the entropy can never increase, \begin{align}
  \H{\rX \mid \rY} \leq \H{\rX}. \label{eq:information_never_hurts}
\end{align}
Colloquially, this property is also called the ``\midx{information never hurts}[idxpagebf]'' principle.
We will derive a proof in the following section.

\section{Mutual Information}\label{sec:mutual_information}

Recall that our fundamental objective is to reduce entropy, as this corresponds to reduced uncertainty in the variables, which we want to predict.
Thus, we are interested in how much information we ``gain'' about the random vector $\rX$ by choosing to observe a random vector $\rY$.
By our interpretation of conditional entropy from the previous section, this is described by the following quantity.

\begin{marginfigure}[-\baselineskip]
  \incplt{information_gain}
  \caption{Information gain.
  The first graph shows the prior.
  The second graph shows a selection of samples with large information gain (large uncertainty reduction).
  The third graph shows a selection of samples with small information gain (small uncertainty reduction).}
\end{marginfigure}

\begin{defn}[Mutual information, MI]\pidx{mutual information}[idxpagebf]
  The \emph{mutual information} of $\rX$ and $\rY$ (also known as the \midx{information gain}) is defined as \begin{align}
    \I{\rX}{\rY} &\defeq \H{\rX} - \H{\rX \mid \rY} \label{eq:mi} \\
    &= \H{\rX} + \H{\rY} - \H{\rX, \rY}.
  \end{align}
\end{defn}

In words, we subtract the uncertainty left about $\rX$ after observing $\rY$ from our initial uncertainty about $\rX$.
This measures the reduction in our uncertainty in $\rX$ (as measured by entropy) upon observing $\rY$.
Unlike conditional entropy, it follows from the definition that mutual information is symmetric.
That is, \begin{align}
  \I{\rX}{\rY} = \I{\rY}{\rX}. \label{eq:mi_symmetry}
\end{align}

Thus, the mutual information between $\rX$ and $\rY$ can be understood as the approximation error (or information loss) when assuming that $\rX$ and $\rY$ are independent.

\begin{marginfigure}[0.5\baselineskip]
  \incfig{information_gain_venn}
  \vspace{-10pt}
  \caption{Relationship between mutual information and entropy, expressed as a Venn diagram.}
\end{marginfigure}

In particular, using Gibbs' inequality (cf. \cref{exercise:gibbs_ineq}), this relationship shows that $\I{\rX}{\rY} \geq 0$ with equality when $\rX$ and $\rY$ are independent, and also proves the \midx{information never hurts} principle \eqref{eq:information_never_hurts} as \begin{align}
  0 \leq \I{\rX}{\rY} = \H{\rX} - \H{\rX \mid \rY}. \label{eq:mi_non_neg}
\end{align}

\begin{ex}{Mutual information of Gaussians}{mi_of_gaussians}
  Given the Gaussian random vector ${\rX \sim \N{\vmu}{\mSigma}}$ and the noisy observation ${\rY = \rX + \vvarepsilon}$ where ${\vvarepsilon \sim \N{\vzero}{\sigman^2 \mI}}$, we want to find the information gain of $\rX$ when observing $\rY$.
  Using our definitions from this chapter, we obtain \begin{align}
    \I{\rX}{\rY} &= \I{\rY}{\rX} \margintag{using symmetry \eqref{eq:mi_symmetry}} \nonumber \\
    &= \H{\rY} - \H{\rY \mid \rX} \margintag{by mutual information \eqref{eq:mi}} \nonumber \\
    &= \H{\rY} - \H{\vvarepsilon} \margintag{given $\rX$, the only randomness in $\rY$ originates from $\vvarepsilon$} \nonumber \\
    &= \frac{1}{2} \log\parentheses*{(2 \pi e)^d \det{\mSigma + \sigman^2 \mI}} - \frac{1}{2} \log\parentheses*{(2 \pi e)^d \det{\sigman^2 \mI}} \margintag{using the entropy of Gaussians \eqref{eq:entropy_gaussian}} \nonumber \\
    &= \frac{1}{2} \log \frac{\det{\mSigma + \sigman^2 \mI}}{\det{\sigman^2 \mI}} \nonumber \\
    &= \frac{1}{2} \log \det{\mI + \sigman^{-2} \mSigma}. \label{eq:mi_cond_linear_gaussians}
  \end{align}
  Intuitively, the larger the noise $\sigman^2$ in relation to the covariance of~$\rX$, the smaller the information gain.
\end{ex}

\subsection{Synergy and Redundancy}\label{sec:synergy_and_redundancy}

It is sometimes useful to write down the mutual information of~$\rX$ and~$\rY$ conditioned (in expectation) on a third random vector~$\rZ$.
This leads us to the following definition.

\begin{defn}[Conditional mutual information]\pidx{conditional mutual information}
  The \emph{conditional mutual information} of $\rX$ and $\rY$ given $\rZ$ is defined as \begin{align}
    \I{\rX}{\rY}[\rZ] &\defeq \H{\rX \mid \rZ} - \H{\rX \mid \rY, \rZ}. \label{eq:cond_mi} \\
    &= \H{\rX, \rZ} + \H{\rY, \rZ} - \H{\rZ} - \H{\rX, \rY, \rZ} \margintag{using the relationship of joint and conditional entropy \eqref{eq:chain_rule_entropy}} \\
    &=\I{\rX}{\rY, \rZ} - \I{\rX}{\rZ}. \label{eq:cond_mi_joint_mi}
  \end{align}
\end{defn}
Thus, the conditional mutual information corresponds to the reduction of uncertainty in $\rX$ when observing $\rY$, given we already observed $\rZ$.
It also follows that conditional mutual information is symmetric: \begin{align}
  \I{\rX}{\rY}[\rZ] = \I{\rY}{\rX}[\rZ]. \label{eq:cond_mi_symmetry}
\end{align}
We have seen in this chapter that entropy is monotonically decreasing as we condition on new information, and called this the ``information never hurts'' principle \eqref{eq:information_never_hurts}.
However, the same does not hold for mutual information!
That is, information about a random vector $\rZ$ may reduce the mutual information between random vectors $\rX$ and $\rY$ \exerciserefmark{cond_mi_non_monotonicity}.\looseness=-1

\begin{rmk}{Sufficient statistics and data processing inequality}{}
  A related concept is the data processing inequality \eqref{eq:data_processing_inequality} which you prove in \exerciserefmark{cond_mi_non_monotonicity}[3] and which allows us to formalize a concept which we have seen multiple times already, namely the notion of a sufficient statistic.
  Consider the Markov chain $\vlambda \to \rX \to \vs(\rX)$, for example, $\vlambda$ may be parameters of the distribution of $\rX$.
  By the data processing inequality \eqref{eq:data_processing_inequality}, $\I{\vlambda}{\vs(\rX)} \leq \I{\vlambda}{\rX}$.
  If the data processing inequality is satisfied with equality then $\vs(\rX)$ is called a \midx{sufficient statistic}[idxpagebf] of $\rX$ for the inference of $\vlambda$.
\end{rmk}

To understand the behavior of mutual information under conditioning, it is helpful to consider the \midx{interaction information} \begin{align}
  \I{\rX}{\rY ; \rZ} \defeq \I{\rX}{\rY} - \I{\rX}{\rY \mid \rZ}. \label{eq:interaction_information}
\end{align}
If the interaction is positive then some information about $\rX$ that is provided by $\rY$ is also provided by $\rZ$ (i.e., conditioning on $\rZ$ decreases MI between $\rX$ and $\rY$), and we say that there is \midx{redundancy} between $\rY$ and $\rZ$ (with respect to $\rX$).
Conversely, if the interaction is negative then learning about $\rZ$ increases what can be learned from $\rY$ about $\rX$, and we say that there is \midx{synergy} between $\rY$ and $\rZ$.
We will see later in this chapter that the absence of synergies can lead to efficient algorithms for maximizing mutual information.


\subsection{Mutual Information as Utility Function}

Following our introduction of mutual information, it is natural to answer the question ``where should I collect data?'' by saying ``wherever mutual information is maximized''.
More concretely, assume we are given a set $\spX$ of possible observations of $f$, where $y_\vx$ denotes a single such observation at $\vx \in \spX$, \begin{align}
  y_\vx \defeq f_\vx + \varepsilon_\vx,
\end{align} $f_\vx \defeq f(\vx)$, and $\varepsilon_\vx$ is some zero-mean Gaussian noise.
For a set of observations $\sS = \{\vx_1, \dots, \vx_n\}$, we can write $\vy_\sS = \vf_\sS + \vvarepsilon$ where \begin{align*}
  \vy_\sS \defeq \begin{bmatrix}
    y_{\vx_1} \\ \vdots \\ y_{\vx_n}
  \end{bmatrix}, \quad \vf_\sS \defeq \begin{bmatrix}
    f_{\vx_1} \\ \vdots \\ f_{\vx_n}
  \end{bmatrix}, \quad\text{and}\quad \vvarepsilon \sim \N{\vzero}{\sigman^2 \mI}.
\end{align*}
Note that both $\vy_\sS$ and $\vf_\sS$ are random vectors.
Our goal is then to find a subset $\sS \subseteq \spX$ of size $n$ maximizing the information gain between our model $f$ and $\vy_\sS$.

This yields the maximization objective, \begin{align}
  I(\sS) \defeq \I{\vf_\sS}{\vy_\sS} = \H{\vf_\sS} - \H{\vf_\sS \mid \vy_\sS}. \label{eq:mi_optimization}
\end{align}
Here, $\Hsm{\vf_\sS}$ corresponds to the uncertainty about $\vf_\sS$ before obtaining the observations $\vy_\sS$ and $\Hsm{\vf_\sS \mid \vy_\sS}$ corresponds to the uncertainty about $\vf_\sS$, in expectation, after obtaining the observations $\vy_\sS$.

\begin{rmk}{Making optimal decisions with intrinsic rewards}{}
  Note that this objective function maps neatly onto our initial consideration of making optimal decisions under uncertainty from \cref{sec:decision_theory}.
  In fact, you can think of maximizing $I(\sS)$ simply as computing the optimal decision rule for the utility \begin{align}
    r(\vy_\sS, \sS) = \H{\vf_\sS} - \H{\vf_\sS \mid \rY_\sS = \vy_\sS}, \margintag{with $\rY_\sS = \vy_\sS$ denoting an event}
  \end{align} with $I(\sS) = \E[{\vy_{\sS}}]{r(\vy_\sS, \sS)}$ measuring the expected utility of observations~$\vy_\sS$.
  Such a utility or reward function is often called an \midx{intrinsic reward} since it does not measure an ``objective'' external quantity, but instead a ``subjective'' quantity that is internal to the model of~$f$.\looseness=-1
\end{rmk}

Observe that picking a subset of points $\sS \subseteq \spX$ from the domain $\spX$ is a combinatorial problem.
That is to say, we are optimizing a function over discrete sets.
In general, such combinatorial optimization problems tend to be very difficult.
It can be shown that maximizing mutual information is $\mathcal{NP}$-hard.

\section{Submodularity of Mutual Information}

 We will look at optimizing mutual information in the following section.
 First, we want to introduce the notion of submodularity which is important in the analysis of discrete functions.

\begin{defn}[Marginal gain]\pidx{marginal gain}
  Given a (discrete) function ${F : \pset{\spX} \to \R}$, the \emph{marginal gain} of $\vx \in \spX$ given $\sA \subseteq \spX$ is defined as \begin{align}
    \Delta_F(\vx \mid \sA) \defeq F(\sA \cup \{\vx\}) - F(\sA). \label{eq:marginal_gain}
  \end{align}
  Intuitively, the marginal gain describes how much ``adding'' the additional $\vx$ to $\sA$ increases the value of $F$.
\end{defn}
When maximizing mutual information, the marginal gain is \exerciserefmark{marginal_gain_mi} \begin{align}
  \Delta_I(\vx \mid \sA) &= \Ism{f_\vx}{y_\vx}[\vy_\sA] \label{eq:mg_mi} \\
  &= \H{y_{\vx} \mid \vy_\sA} - \H{\varepsilon_\vx}. \label{eq:mg_decomp}
\end{align}
That is, when maximizing mutual information, the marginal gain corresponds to the difference between the uncertainty after observing $\vy_\sA$ and the entropy of the noise $\H{\varepsilon_\vx}$.
Altogether, the marginal gain represents the reduction in uncertainty by observing $\{\vx\}$.

\begin{marginfigure}
  \incfig{submodularity}
  \caption{Monotone submodularity.
  The effect of ``adding'' $\vx$ to the smaller set $\sA$ is larger than the effect of adding $\vx$ to the larger set $\sB$.}
\end{marginfigure}

\begin{defn}[Submodularity]\pidx{submodularity}
  A (discrete) function $F : \pset{\spX} \to \R$ is \emph{submodular} iff for any $\vx \in \spX$ and any $\sA \subseteq \sB \subseteq \spX$ it satisfies \begin{align}
    F(\sA \cup \{\vx\}) - F(\sA) \geq F(\sB \cup \{\vx\}) - F(\sB). \label{eq:submodularity}
  \end{align}
\end{defn}

Equivalently, using our definition of marginal gain, we have that $F$ is submodular iff for any $\vx \in \spX$ and any $\sA \subseteq \sB \subseteq \spX$, \begin{align}
  \Delta_F(\vx \mid \sA) \geq \Delta_F(\vx \mid \sB). \label{eq:submodularity_mg}
\end{align}
That is, ``adding'' $\vx$ to the smaller set $\sA$ yields more marginal gain than adding $\vx$ to the larger set $\sB$.
In other words, the function $F$ has ``diminishing returns''.
In this way, submodularity can be interpreted as a notion of ``concavity'' for discrete functions.

\begin{defn}[Monotone submodularity]\pidx{monotone submodularity}\label{defn:monotone_submodularity}
  A function $F : \pset{\spX} \to \R$ is called \emph{monotone} iff for any $\sA \subseteq \sB \subseteq \spX$ it satisfies \begin{align}
    F(\sA) \leq F(\sB). \label{eq:monotonicity}
  \end{align}
  If $F$ is also submodular, then $F$ is called \emph{monotone submodular}.
\end{defn}

\begin{thm}\label{thm:monotone_submodularity_of_mi}
  The objective $I$ is monotone submodular.
\end{thm}\vspace{-10pt}
\begin{proof}
  We fix arbitrary subsets $\sA \subseteq \sB \subseteq \spX$ and any $\vx \in \spX$. We have, \begin{align*}
    \text{$I$ is submodular} \iff&& \Delta_I(\vx \mid \sA) &\geq \Delta_I(\vx \mid \sB) \margintag{by submodularity in terms of marginal gain \eqref{eq:submodularity_mg}} \\[8pt]
    \iff&& \H{y_{\vx} \mid \vy_\sA} - \H{\varepsilon_\vx} &\geq \H{y_{\vx} \mid \vy_\sB} - \H{\varepsilon_\vx} \margintag{using \cref{eq:mg_decomp}} \\
    \iff&& \H{y_{\vx} \mid \vy_\sA} &\geq \H{y_{\vx} \mid \vy_\sB}. \margintag{$\H{\varepsilon_\vx}$ cancels}
  \end{align*}
  Due to the ``information never hurts'' principle \eqref{eq:information_never_hurts} of entropy and as $\sA \subseteq \sB$, this is always true.
  Moreover, \begin{align*}
    \text{$I$ is monotone} \iff&& I(\sA) &\leq I(\sB) \margintag{by the definition of monotinicity \eqref{eq:monotonicity}} \\
    \iff&& \I{\vf_\sA}{\vy_\sA} &\leq \I{\vf_\sB}{\vy_\sB} \margintag{using the definition of $I$ \eqref{eq:mi_optimization}} \\
    \iff&& \I{\vf_\sB}{\vy_\sA} &\leq \I{\vf_\sB}{\vy_\sB} \margintag{using $\I{\vf_\sB}{\vy_\sA} = \I{\vf_\sA}{\vy_\sA}$ as $\vy_\sA \perp \vf_\sB \mid \vf_\sA$} \\[8pt]
    \iff&& \H{\vf_\sB} - \H{\vf_\sB \mid \vy_\sA} &\leq \H{\vf_\sB} - \H{\vf_\sB \mid \vy_\sB} \margintag{using the definition of MI \eqref{eq:mi}} \\
    \iff&& \H{\vf_\sB \mid \vy_\sA} &\geq \H{\vf_\sB \mid \vy_\sB}, \margintag{$\H{\vf_\sB}$ cancels}
  \end{align*} which is also satisfied due to the ``information never hurts'' principle~\eqref{eq:information_never_hurts}.\looseness=-1
\end{proof}

The submodularity of $I$ can be interpreted from the perspective of information theory.
It turns out that submodularity is equivalent to the absence of synergy between observations \exerciserefmark{submodularity_and_no_synergy}.
Intuitively, without synergies, acting greedily is enough to find a near-optimal solution.
If there are synergies, then the combinatorial search problem is much harder, because single-step optimal actions do not necessarily lead us to the optimal solution.
Consider the extreme case of having to solve a ``needle in a haystack'' problem, where only a single subset of $\spX$ with size $k$ achieves objective value $1$, with all other subsets achieving objective value $0$.
In this case, we can do nothing but exhaustively search through all $\abs{\spX}^k$ combinations to find the optimal solution.

\section{Maximizing Mutual Information}

As we cannot efficiently pick a set $\sS \subseteq \spX$ to maximize mutual information but know that $I$ is submodular, a natural approach is to maximize mutual information greedily.
That is, we pick the locations $\vx_1$ through $\vx_n$ individually by greedily finding the location with the maximal mutual information.
The following general result for monotone submodular function maximization shows that, indeed, this greedy approach provides a good approximation.

\begin{thm}[Greedy submodular function maximization]
  If the set function ${F : \spP(\spX) \to \Rzero}$ is monotone submodular, then greedily maximizing $F$ is a $(1 - \nicefrac{1}{e})$-approximation:\footnote{$1 - \nicefrac{1}{e} \approx 0.632$} \begin{align}
    F(\sS_n) \geq \parentheses*{1 - \frac{1}{e}} \max_{\substack{\sS \subseteq \spX \\ \card{\sS} = n}} F(\sS).
  \end{align}
\end{thm}\vspace{-10pt}
\begin{proof}
  Fix any $n \geq 1$. Let $\sSs \in \argmax\{F(\sS) \mid \sS \in \spX, \card{\sS} \leq n\}$.
  We can assume $\card{\sSs} = n$ due to the monotonicity \eqref{eq:monotonicity} of $F$.
  We write, $\{\opt{\vx_1}, \dots, \opt{\vx_n}\} \defeq \sSs$.
  We have, \begin{align*}
    F(\sSs) &\leq F(\sSs \cup \sS_t) \margintag{using monotonicity \eqref{eq:monotonicity}} \\
    &= F(\sS_t) + \sum_{i=1}^n \Delta_F(\opt{\vx_i} \mid \sS_t \cup \{\opt{\vx_1}, \dots, \opt{\vx_{i-1}}\}) \margintag{using the definition of marginal gain \eqref{eq:marginal_gain}} \\
    &\leq F(\sS_t) + \sum_{\vxs \in \sSs} \Delta_F(\vxs \mid \sS_t) \margintag{using submodularity \eqref{eq:submodularity_mg}} \\
    &\leq F(\sS_t) + n \parentheses*{F(\sS_{t+1}) - F(\sS_t)}. \margintag{using that $\sS_{t+1} = \sS_t \cup \{\vx\}$ is chosen such that $\Delta_F(\vx \mid \sS_t)$ is maximized \eqref{eq:greedy_mi}}
  \end{align*}

  Let $\delta_t \defeq F(\sSs) - F(\sS_t)$. Then, \begin{align*}
    \delta_t = F(\sSs) - F(\sS_t) &\leq n \parentheses*{F(\sS_{t+1}) - F(\sS_t)} = n \parentheses*{\delta_t - \delta_{t+1}}.
  \end{align*}
  This implies $\delta_{t+1} \leq (1 - \nicefrac{1}{n}) \delta_t$ and $\delta_n \leq (1 - \nicefrac{1}{n})^n \delta_0 \leq \nicefrac{\delta_0}{e}$, using the well-known inequality $1 - x \leq e^{-x}$ for all $x \in \R$.

  Finally, observe that $\delta_0 = F(\sSs) - F(\emptyset) \leq F(\sSs)$ due to the non-negativity of $F$.
  We obtain, \begin{align*}
    \delta_n = F(\sSs) - F(\sS_n) \leq \frac{\delta_0}{e} \leq \frac{F(\sSs)}{e}.
  \end{align*}
  Rearranging the terms yields the theorem.
\end{proof}

\begin{oreadings}
  The original proof of greedy maximization for submodular functions was given by \icite{greedy_mi}.

  For more background on maximizing submodular functions, see \icite{krause2014submodular}.\looseness=-1
\end{oreadings}

Now that we have established that greedy maximization of mutual information is a decent approximation to maximizing the joint information of data, we will look at how this optimization problem can be solved in practice.\looseness=-1

\subsection{Uncertainty Sampling}\label{sec:active_learning:optimizing_mutual_information:uncertainty_sampling}

When maximizing mutual information, at time $t$ when we have already picked $\sS_t = \{\vx_1, \dots, \vx_t\}$, we need to solve the following optimization problem, \begin{align}
  \vx_{t+1} &\defeq \argmax_{\vx \in \spX} \Delta_I(\vx \mid \sS_t) \label{eq:greedy_mi} \\
  &= \argmax_{\vx \in \spX} \Ism{f_\vx}{y_{\vx} \mid \vy_{\sS_t}}. \margintag{using \cref{eq:mg_mi}}
  \intertext{Note that $f_\vx$ and $y_{\vx}$ are univariate random variables.
  Thus, using our formula for the mutual information of conditional linear Gaussians~\eqref{eq:mi_cond_linear_gaussians}, we can simplify to,}
  &= \argmax_{\vx \in \spX} \frac{1}{2} \log\parentheses*{1 + \frac{\sigma_t^2(\vx)}{\sigman^2}} \label{eq:uncertainty_sampling1}
  \intertext{where $\sigma_t^2(\vx)$ is the (remaining) variance at $\vx$ after observing $\sS_t$. Assuming the label noise is independent of $\vx$ (i.e., homoscedastic),}
  &= \argmax_{\vx \in \spX} \sigma_t^2(\vx).
\end{align}
Therefore, if $f$ is modeled by a Gaussian and we assume homoscedastic noise, greedily maximizing mutual information corresponds to simply picking the point $\vx$ with the largest variance.
This algorithm is also called \midx{uncertainty sampling}.

\subsection{Heteroscedastic Noise}\label{sec:active_learning:optimizing_mutual_information:heteroscedastic_noise}

\begin{marginfigure}
  \incplt{uncertainty_sampling_het_noise}
  \caption{Uncertainty sampling with heteroscedastic noise.
  The epistemic uncertainty of the model is shown in a dark gray.
  The aleatoric uncertainty of the data is shown in a light gray.
  Uncertainty sampling would repeatedly pick points around $\xs$ as they maximize the epistemic uncertainty, even though the aleatoric uncertainty at $\xs$ is much larger than at the boundary.}\label{fig:uncertainty_sampling_heterscedastic_noise}
\end{marginfigure}

Uncertainty sampling is clearly problematic if the noise is heteroscedastic.
If there are a particular set of inputs with a large aleatoric uncertainty dominating the epistemic uncertainty, uncertainty sampling will continuously choose those points even though the epistemic uncertainty will not be reduced substantially (cf.~\cref{fig:uncertainty_sampling_heterscedastic_noise}).

Looking at \cref{eq:uncertainty_sampling1} suggests a natural fix.
Instead of only considering the epistemic uncertainty $\sigma_t^2(\vx)$, we can also consider the aleatoric uncertainty $\sigman^2(\vx)$ by modeling heteroscedastic noise, yielding \begin{align}
  \vx_{t+1} \defeq \argmax_{\vx \in \spX} \frac{1}{2} \log\parentheses*{1 + \frac{\sigma_t^2(\vx)}{\sigman^2(\vx)}} = \argmax_{\vx \in \spX} \frac{\sigma_t^2(\vx)}{\sigman^2(\vx)}.
\end{align}

Thus, we choose locations that trade large epistemic uncertainty with large aleatoric uncertainty.
Ideally, we find a location where the epistemic uncertainty is large, and the aleatoric uncertainty is low, which promises a significant reduction of uncertainty around this location.

\subsection{Classification}\label{sec:active_learning:optimizing_mutual_information:classification}

While we focused on regression, one can apply active learning also for other settings, such as (probabilistic) classification.
In this setting, for any input $\vx$, a model produces a categorical distribution over labels~$y_{\vx}$.\footnote{see \cref{sec:fundamentals:supervised_learning}}
Here, uncertainty sampling corresponds to selecting samples that maximize the entropy of the predicted label $y_{\vx}$, \begin{align}
  \vx_{t+1} &\defeq \argmax_{\vx \in \spX} \H{y_{\vx} \mid \vx_{1:t}, y_{1:t}}. \label{eq:uncertainty_sampling_classification1}
\end{align}
The entropy of a categorical distribution is simply a finite sum of weighted surprise terms.

\begin{figure}
  \begin{center}
    \import{./plots/output/}{uncertainty_sampling_classification_working.pgf}
    \import{./plots/output/}{uncertainty_sampling_classification.pgf}
  \end{center}
  \caption{Uncertainty sampling in classification.
  The area with high uncertainty (as measured by entropy) is highlighted in yellow.
  The shown figures display each a sequence of model updates, each after one new observation.
  In the left figure, the classes are well-separated and uncertainty is dominated by epistemic uncertainty, whereas in the right figure the uncertainty is dominated by noise.
  In the latter case, if we mostly choose points $\vx$ in the area of highest uncertainty (i.e., close to the decision boundary) to make observations, the label noise results in frequently changing models.\looseness=-1}
\end{figure}

This approach generally leads to sampling points that are close to the decision boundary.
Often, the uncertainty is mainly dominated by label noise rather than epistemic uncertainty, and hence, we do not learn much from our observations.
This is a similar problem to the one we encountered with uncertainty sampling in the setting of heteroscedastic noise.\looseness=-1

This naturally suggests distinguishing between the aleatoric and epistemic uncertainty of the model $f$ (parameterized by $\vtheta$).
To this end, mutual information can be used similarly as we have done with uncertainty sampling for regression, \begin{align}
  \vx_{t+1} &\defeq \argmax_{\vx \in \spX} \I{\vtheta}{y_{\vx} \mid \vx_{1:t}, y_{1:t}} \\
  &= \argmax_{\vx \in \spX} \I{y_{\vx}}{\vtheta \mid \vx_{1:t}, y_{1:t}} \margintag{using symmetry \eqref{eq:mi_symmetry}} \nonumber \\
  &= \argmax_{\vx \in \spX} \H{y_{\vx} \mid \vx_{1:t}, y_{1:t}} - \H{y_{\vx} \mid \vtheta, \vx_{1:t}, y_{1:t}} \margintag{using the definition of mutual information \eqref{eq:mi}} \nonumber \\
  &= \argmax_{\vx \in \spX} \H{y_{\vx} \mid \vx_{1:t}, y_{1:t}} - \b{\E[\vtheta \mid \vx_{1:t}, y_{1:t}]{\H{y_{\vx} \mid \vtheta, \vx_{1:t}, y_{1:t}}}} \label{eq:uncertainty_sampling_classification2} \margintag{using the definition of conditional entropy \eqref{eq:cond_entropy}} \\
  &= \argmax_{\vx \in \spX} \underbrace{\H{y_{\vx} \mid \vx_{1:t}, y_{1:t}}}_{\substack{\text{entropy of} \\ \text{predictive posterior}}} - \E*[\vtheta \mid \vx_{1:t}, y_{1:t}]{\underbrace{\H{y_{\vx} \mid \vtheta}}_{\substack{\text{entropy of} \\ \text{likelihood}}}}. \margintag{using the definition of entropy \eqref{eq:entropy} and assuming $y_{\vx} \perp \vx_{1:t}, y_{1:t} \mid \vtheta$}
\end{align}

The first term measures the entropy of the averaged prediction while the second term measures the average entropy of predictions.
Thus, the first term looks for points where the average prediction is not confident.
In contrast, the second term penalizes points where many of the sampled models are not confident about their prediction, and thus looks for points where the models are confident in their predictions.
This identifies those points~$\vx$ where the models \emph{disagree} about the label~$y_{\vx}$ (that is, each model is ``confident'' but the models predict different labels).
For this reason, this approach is known as \midx{Bayesian active learning by disagreement} (BALD).

Note that the second term of the difference acts as a regularizer when compared to \cref{eq:uncertainty_sampling_classification1}. The second term mirrors our description of \b{aleatoric uncertainty} from \cref{sec:blr:uncertainty}.
Recall that we interpreted aleatoric uncertainty as the average uncertainty for all models.
Crucially, here we use entropy to ``measure'' uncertainty, whereas previously we have been using variance.
Therefore, intuitively, \cref{eq:uncertainty_sampling_classification2} subtracts the aleatoric uncertainty from the total uncertainty about the label.

Observe that both terms require approximate forms of the posterior distribution.
In \cref{sec:approximate_inference,sec:approximate_inference:mcmc}, we have seen various approaches from variational inference and MCMC methods which can be used here.
The first term can be approximated by the predictive distribution of an approximated posterior which is obtained, for example, using variational inference.
The nested entropy of the second term is typically easy to compute, as it corresponds to the entropy of the (discrete) likelihood distribution $p(y \mid \vx, \vtheta)$ of the model $\vtheta$.
The outer expectation of the second term may be approximated using (approximate) samples from the posterior distribution obtained via variational inference, MCMC, or some other method.\looseness=-1

\begin{oreadings}
  \begin{itemize}
    \item \pcite{gal2017deep}
  \end{itemize}
\end{oreadings}

\section{Learning Locally: Transductive Active Learning}

So far we have explored how to select observations that provide us with the best predictor $f(\vx)$ across the entire domain $\vx \in \spX$.
However, we typically utilize predictors to make predictions at a particular location $\vxs$.
It is therefore a natural question to ask how to select observations that lead to the best individual prediction $f(\vxs)$ at the prediction target $\vxs$.
The distinction between these two settings is closely related to the distinction between two general approaches to learning: \midx{inductive learning} and \midx{transductive learning}.
Inductive learning aims to extract general rules from the data, that is, to extract and compress the most bits of information.
In contrast, transductive learning aims to make the best prediction at a particular location, that is, to extract the most bits of information \emph{relevant to the prediction} at $\vxs$.
The concept of transduction was developed by Vladimir Vapnik in the 1980s who described its essence as follows: \begin{quote}
  ``When solving a problem of interest, do not solve a more general problem as an intermediate step.
  Try to get the answer that you really need but not a more general one.'' --- Vladimir Vapnik
\end{quote}

\begin{rmk}{What are the prediction targets $\vxs$?}{}
  Typically, in transductive learning, we cannot directly observe the value $f(\vxs)$ at the prediction target $\vxs$, for example, when learning from a fixed dataset~$\spX' \subset \spX$.
  If we \emph{could} observe $f(\vxs)$ directly (or perturbed by noise), solving the learning task would only require memorization.
  Instead, most interesting learning tasks require generalizing $f(\vxs)$ from the behavior of $f$ at other locations.
  Therefore, transductive learning becomes interesting precisely when we cannot directly observe $f(\vxs)$.\safefootnote{We will discuss an example of this kind in \cref{exercise:transductive_active_learning}.}

  Note that this is similar to the inductive setting where we, in principle, could observe $f(\vx)$ at any location $\vx$, but practically, we can only observe $f(\vx)$ at a finite number of locations.
  Since such an inductive model is then used to make predictions at any location~$\vxs$, it also needs to generalize from the observations.
\end{rmk}

Following the transductive paradigm, when already knowing that our goal is to predict $f(\vxs)$ our objective is to select observations that provide the most information about $f(\vxs)$.
We can express this objective elegantly using the probabilistic framework from this chapter \citep{mackay1992information,hubotter2024transductive}: \begin{align}
  \vx_{t+1} \defeq \argmax_{\vx \in \spX'} \Ism{f_{\vxs}}{y_{\vx}}[\vx_{1:t}, y_{1:t}] = \argmin_{\vx \in \spX'} \Hsm{f_{\vxs}}[\vx_{1:t}, y_{1:t}, y_{\vx}]. \label{eq:transductive_mi}
\end{align}
\cite{hubotter2024transductive} show that this objective leads to a remarkably different selection of observations compared to the inductive active learning objective we discussed earlier.
Indeed, while the inductive objective focuses on selecting a \emph{diverse} set of examples, the transductive objective also takes into account the \emph{relevance} of the examples to the prediction target $\vxs$.
We can see this tradeoff between diversity and relevance by rewriting the transductive objective as \begin{align}
  \Ism{f_{\vxs}}{y_{\vx}}[\vx_{1:t}, y_{1:t}] = \underbrace{\Ism{f_{\vxs}}{y_{\vx}}}_{\text{relevance}} - \underbrace{\Ism{f_{\vxs}}{y_{\vx} ; y_{1:t}}[\vx_{1:t}]}_{\text{redundancy}} \margintag{using the definition of interaction information~\eqref{eq:interaction_information}}
\end{align} where the first term measures the information gain of $y_{\vx}$ about $f_{\vxs}$ while the second term is the interaction information which measures the redundancy of the information in $y_{\vx}$ and $y_{1:t}$ about~$f_{\vxs}$.
In this way, transductive active learning describes a middle ground between traditional search and retrieval methods that focus on relevance on the one hand and inductive active learning which focuses on diversity on the other hand.

\begin{oreadings}
  \begin{itemize}
    \item \pcite{hubotter2024transductive}
  \end{itemize}
  In modern machine learning, one often differentiates between a ``pre-training'' and a ``fine-tuning'' stage.
  During pre-training, a model is trained on a large dataset to extract general knowledge without a specific task in mind.
  Then, during fine-tuning, the model is adapted to a specific task by training on a smaller dataset.
  Whereas (inductive) active learning is closely linked to the pre-training stage, transductive active learning has been shown to be useful for task-specific fine-tuning:
  \begin{itemize}
    \item \pcite{hubotter2025efficiently}
    \item \pcite{bagatella2024active}
  \end{itemize}
\end{oreadings}




















\section*{Discussion}

We have discussed how to select the most informative data.
Thereby, we focused mostly on \emph{inductive} active learning which is applicable to ``pre-training'' when we aim to extract general knowledge from data, but also explored \emph{transductive} active learning which is useful for ``fine-tuning'' when we aim to adapt a model to a specific task.

We focused on quantifying the ``informativeness'' of data by its information gain, which is a common approach, though many other viable criteria exist:

\begin{rmk}{Beyond mutual information}{beyond_mi}
  The problem of identifying which experiments to conduct in order to maximize learning is studied extensively in the field of \midx{experimental design} where a set of observations $\sS$ is commonly called a \midx{design}.
  In the presence of a prior and likelihood, and a different possible posterior distribution for each design $\sS$, the field is also called \midx{Bayesian experimental design} \citep{chaloner1995bayesian,rainforth2024modern,mutny2024modern}.

  As we highlighted in the beginning of this chapter, how we measure the utility (i.e., the informativeness) of a design $\sS$ is crucial.
  Such a measure is called a \midx{design criterion} and a design which is optimal with respect to a design criterion is called an \midx{optimal design}.
  The literature studies various design criteria beyond maximizing mutual information (i.e., minimizing posterior entropy).
  One popular alternative is to select the observations $\sS$ that minimize the trace of the posterior covariance matrix, which corresponds to minimizing the posterior average variance.



\end{rmk}

Next, we will move to the topic of optimization and ask which data we should select to find the optimum of an unknown function as quickly as possible.
In the following chapter, we will focus on ``Bayesian optimization'' (also called ``bandit optimization'') where our aim is to \emph{find and sample} the optimal point.
A related task that is slightly more related to active learning is the ``best-arm identification'' problem where we aim only to \emph{identify} the optimal point without sampling it.
This problem is closely related transductive active learning (with the local task being defined by the location of the maximum) and so-called \midx{entropy search} methods that minimize the entropy of the posterior distribution over the location or value of the maximum (akin to \cref{eq:transductive_mi}) are often used to solve this problem~\citep{hennig2012entropy,wang2017max,hvarfner2022joint}.\looseness=-1


\excheading

\begin{nexercise}{Mutual information and KL divergence}{mi_and_kl}
  Show that by expanding the definition of mutual information, \begin{align}
    \I{\rX}{\rY} &= \E[(\vx,\vy) \sim p]{\log\frac{p(\vx,\vy)}{p(\vx) p(\vy)}} \nonumber \\
    &= \KL{p(\vx,\vy)}{p(\vx) p(\vy)} \\
    &= \E[\vy \sim p]{\KL{p(\vx \mid \vy)}{p(\vx)}},
  \end{align} where $p(\vx,\vy)$ denotes the joint distribution and $p(\vx),p(\vy)$ denote the marginal distributions of $\rX$ and $\rY$.
\end{nexercise}

\begin{nexercise}{Non-monotonicity of cond. mutual information}{cond_mi_non_monotonicity}
  \begin{enumerate}
    \item Show that if $\rX \perp \rZ$ then $\I{\rX}{\rY \mid \rZ} \geq \I{\rX}{\rY}$.

    \item Show that if $\rX \perp \rZ \mid \rY$ then $\I{\rX}{\rY \mid \rZ} \leq \I{\rX}{\rY}$.

    \item Note that the condition $\rX \perp \rZ \mid \rY$ is the Markov property, namely, $\{\rX, \rY, \rZ\}$ form a Markov chain with graphical model $\rX \to \rY \to \rZ$.
    This situation often occurs when data is processed sequentially.
    Prove \begin{align}
      \I{\rX}{\rZ} \leq \I{\rX}{\rY}. \label{eq:data_processing_inequality}
    \end{align} which is also known as the \midx{data processing inequality}, and which says that processing cannot increase the information contained in a signal.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Interaction information}{interaction_information}
  \begin{enumerate}
    \item Show that interaction information is symmetric.
    \item Let $X_1, X_2 \sim \Bern{p}$ for some $p \in (0,1)$ and independent.
    We denote by $Y \defeq X_1 \oplus X_2$ their XOR.
    Compute $\I{Y ; X_1}{X_2}$.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Marginal gain of maximizing mutual information}{marginal_gain_mi}
  Show that in the context of maximizing mutual information, the marginal gain is \begin{align*}
    \Delta_I(\vx \mid \sA) &= \Ism{f_\vx}{y_\vx}[\vy_\sA] = \H{y_{\vx} \mid \vy_\sA} - \H{\varepsilon_\vx}.
  \end{align*}
\end{nexercise}

\begin{nexercise}{Submodularity means no synergy}{submodularity_and_no_synergy}
  Show that submodularity is equivalent to the absence of synergy between observations.
  That is, show that for all $\sA \subseteq \sB$, \begin{align}
    \Ism{f_\vx}{y_\vx ; \vy_{\sB \setminus \sA}}[\vy_\sA] \geq 0.
  \end{align}
\end{nexercise}

\begin{nexercise}{Transductive active learning}{transductive_active_learning}



  Consider the prior distribution \(X_i \sim \mathcal{N}(0,1)\) with all \(X_i\) independent and consider the ``output'' variable \begin{align*}
    Z \defeq \sum_{i=1}^{100} i \cdot X_i.
  \end{align*}
  Our observation when \(X_{i_t}\) is selected in round \(t\) is generated by \begin{align*}
    Y_t \defeq X_{i_t} + \varepsilon_t \quad\text{with } \varepsilon_t \iid \mathcal{N}(0,1).
  \end{align*}
  For a set of inputs \(S = \{i_1, \dots, i_t\} \subseteq \{1, \dots, 100\}\), we define the \emph{information gain} of \(S\) as \[
      F(S) \defeq \I{Z}{Y_{1:t}} = \H{Z} - \H{Z}[Y_{1:t}]
  \] where \(\H{Z}\) is the entropy of \(Z\) according to the prior and \(\H{Z}[Y_{1:t}]\) is the conditional entropy after the observations \(Y_1, \dots, Y_t\).\par
  \emph{Note: The random vector \((X_1, \dots, X_{100}, Z)\) is jointly Gaussian due to the closedness of Gaussians under linear transformations.}

  \begin{enumerate}
    \item We define the \emph{marginal information gain} \(\Delta(j \mid S)\) for a new observation \(Y_j\) as \[
      \Delta(j \mid S) \defeq F(S \cup \{j\}) - F(S).
    \]
    Does \({\Delta(j \mid S) \geq 0}\) hold for all $j$ and $S$?

    \item Is maximizing the marginal information gain \(\Delta(i \mid S)\) equivalent to picking the point \(i \in \{1, \dots, 100\}\) with maximum variance under the current posterior over \(X_i\), that is, ``equivalent to uncertainty sampling''?

    \item Let us consider the alternative prior where \(X_i \sim \Bern{0.5}\) are fair coin flips which we observe directly (i.e., \(Y_t = X_{i_t}\)).
    For which of the following definitions of \(Z\) is the acquisition function \(S \mapsto F(S)\) submodular? \begin{enumerate}
        \item \(Z \defeq X_1 \land \cdots \land X_{100}\) with \(\land\) denoting the logical AND.
        \item \(Z \defeq X_1 \lor \cdots \lor X_{100}\) with \(\lor\) denoting the logical OR.
        \item \(Z \defeq X_1 \oplus \cdots \oplus X_{100}\) with \(\oplus\) denoting the logical XOR, i.e., the exclusive OR which returns \(1\) iff exactly one of the \(X_i\) is \(1\) and \(0\) otherwise.
    \end{enumerate}
  \end{enumerate}
\end{nexercise}
