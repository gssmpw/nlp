\chapter{Bayesian Optimization}\label{sec:bayesian_optimization}

Often, obtaining data is costly.
In the previous chapter, this led us to investigate how we can optimally improve our understanding (i.e., reduce uncertainty) of the process we are trying to model.
However, purely improving our understanding is often not good enough.
In many cases, we want to use our improving understanding \emph{simultaneously} to reach certain goals.
This is a very common problem in artificial intelligence and will concern us for the rest of this manuscript.
One common instance of this problem is the setting of optimization.

\begin{marginfigure}
  \incfig{bayesian_optimization}
  \caption{Illustration of Bayesian optimization. We pass an input $\vx_t$ into the unknown function $\opt{f}$ to obtain noisy observations $y_t$.}
\end{marginfigure}

Given some function $\opt{f} : \spX \to \R$, suppose we want to find the \begin{align}
  \argmax_{\vx \in \spX} \opt{f}(\vx).
\end{align}
Now, contrary to classical optimization, we are interested in the setting where the function $\opt{f}$ is unknown to us (like a ``black-box'').
We are only able to obtain noisy observations of $\opt{f}$, \begin{align}
  y_t = \opt{f}(\vx_t) + \varepsilon_t.
\end{align}
Moreover, these noise-perturbed evaluations are costly to obtain.
We will assume that similar alternatives yield similar results,\footnote{That is, $\opt{f}$ is ``smooth''. We will be more precise in the subsequent parts of this chapter. If this were not the case, optimizing the function without evaluating it everywhere would not be possible. Fortunately, many interesting functions obey by this relatively weak assumption.} which is commonly encoded by placing a Gaussian process prior on $\opt{f}$.
This assumed correlation is fundamentally what will allow us to learn a model of $\opt{f}$ from relatively few samples.\footnote{There are countless examples of this problem in the ``real world''. Instances are \begin{itemize}
  \item drug trials
  \item chemical engineering --- the development of physical products
  \item recommender systems
  \item automatic machine learning --- automatic tuning of model \& hyperparameters
  \item and many more...
\end{itemize}}

\section{Exploration-Exploitation Dilemma}\label{sec:bayesian_optimization:exploration_exploitation}

In Bayesian optimization, we want to learn a model of $\opt{f}$ and use this model to optimize $\opt{f}$ simultaneously.
These goals are somewhat contrary.
Learning a model of $\opt{f}$ requires us to explore the input space while using the model to optimize $\opt{f}$ requires us to focus on the most promising well-explored areas.
This trade-off is commonly known as the \midx{exploration-exploitation dilemma}[idxpagebf], whereby \begin{itemize}
  \item \emph{exploration} refers to choosing points that are ``informative'' with respect to the unknown function. For example, points that are far away from previously observed points (i.e., have high posterior variance);\footnote{We explored this topic (with strategies like uncertainty sampling) in the previous chapter.} and
  \item \emph{exploitation} refers to choosing promising points where we expect the function to have high values. For example, points that have a high posterior mean and a low posterior variance.
\end{itemize}
In other words, the exploration-exploitation dilemma refers to the challenge of learning enough to understand $\opt{f}$, but not learning too much to lose track of the objective --- optimizing $\opt{f}$.

The exploration-exploitation dilemma is yet another example of the \midx{principle of curiosity and conformity} which we introduced in \cref{sec:free_energy} and encountered many times since in our study of approximate probabilistic inference.
We will see in subsequent chapters that sequential decision-making is intimately related to probabilistic inference, and there we will also make this correspondence more precise.

\section{Online Learning and Bandits}

Bayesian optimization is closely related to a form of online learning.
In \midx{online learning} we are given a set of possible inputs $\spX$ and an unknown function $\opt{f} : \spX \to \R$.
We are now asked to choose a sequence of inputs $\vx_1, \dots \vx_T$ online,\footnote{\emph{Online} is best translated as ``sequential''. That is, we need to pick $\vx_{t+1}$ based only on our prior observations $y_1, \dots, y_t$.} and our goal is to maximize our cumulative reward $\sum_{t=1}^T \opt{f}(\vx_t)$.
Depending on what we observe about $\opt{f}$, there are different variants of online learning.
Bayesian optimization is closest to the so-called (stochastic) ``bandit'' setting.

\subsection{Multi-Armed Bandits}\label{sec:bayesian_optimization:online_learning:mab}

The ``\midx{multi-armed bandits}'' (MAB) problem is a classical, canonical formalization of the exploration-exploitation dilemma.
In the MAB  problem, we are provided with $k$ possible actions (arms) and want to maximize our reward online within the time horizon $T$.
We do not know the reward distributions of the actions in advance, however, so we need to trade learning the reward distribution with following the most promising action.
Bayesian optimization can be interpreted as a variant of the MAB problem where there can be a potentially infinite number of actions (arms), but their rewards are correlated (because of the smoothness of the Gaussian process prior).

\begin{marginfigure}
  \incfig{multi_armed_bandits}
  \caption{Illustration of a multi-armed bandit with four arms, each with a different reward distribution.
  The agent tries to identify the arm with the most beneficial reward distribution shown in green.}
\end{marginfigure}

There exists a large body of work on this and similar problems in online decision-making.
Much of this work develops theory on how to explore and exploit in the face of uncertainty.
The shared prevalence of the exploration-exploitation dilemma signals a deep connection between online learning and Bayesian optimization (and --- as we will later come to see --- reinforcement learning).
Many of the approaches which we will encounter in the context of these topics are strongly related to methods in online learning.

One of the key principles of the theory on multi-armed bandits and reinforcement learning is the principle of \midx<``optimism in the face of uncertainty''>{optimism in the face of uncertainty}[idxpagebf], which suggests that it is a good guideline to explore where we can hope for the best outcomes.
We will frequently come back to this general principle in our discussion of algorithms for Bayesian optimization and reinforcement learning.


\subsection{Regret}

The key performance metric in online learning is the regret.

\begin{defn}[Regret]\pidx{regret}
  The \emph{(cumulative) regret} for a time horizon $T$ associated with choices $\{\vx_t\}_{t=1}^T$ is defined as \begin{align}
    R_T &\defeq \sum_{t=1}^T \underbrace{\parentheses*{\max_\vx \opt{f}(\vx) - \opt{f}(\vx_t)}}_{\text{\midx{instantaneous regret}}} \\
    &= T \max_\vx \opt{f}(\vx) - \sum_{t=1}^T \opt{f}(\vx_t). \label{eq:regret2}
  \end{align}
\end{defn}
The regret can be interpreted as the additive loss with respect to the \emph{static} optimum $\max_\vx \opt{f}(\vx)$.

The goal is to find algorithms that achieve \midx<sublinear>{sublinear regret} regret, \begin{align}
  \lim_{T\to\infty} \frac{R_T}{T} = 0.
\end{align}
Importantly, if we use an algorithm which explores \emph{forever}, e.g., by going to a random point $\tilde{\vx}$ with a constant probability $\epsilon$ in each round, then the regret will grow linearly with time.
This is because the instantaneous regret is at least $\epsilon (\max_\vx \opt{f}(\vx) - \opt{f}(\tilde{\vx}))$ and non-decreasing.
Conversely, if we use an algorithm which \emph{never} explores, then we might never find the static optimum, and hence, also incur constant instantaneous regret in each round, implying that regret grows linearly with time.
Thus, achieving sublinear regret requires \emph{balancing} exploration and exploitation.

Typically, online learning (and Bayesian optimization) consider stationary environments, hence the comparison to the static optimum.
Dynamic environments are studied in \midx{online algorithms} (see metrical task systems\footnote[][-16\baselineskip]{Metrical task systems are a classical example in online algorithms. Suppose we are moving in a (finite) decision space~$\spX$. In each round, we are given a ``task'' $f_t : \spX \to \R$ which is more or less costly depending on our state~${\vx_t \in \spX}$. In many contexts, it is natural to assume that it is also costly to move around in the decision space. This cost is modeled by a metric $d(\cdot, \cdot)$ on $\spX$. In \idx{metrical task systems}, we want to minimize our total cost, \begin{align*}
  \sum_{t=1}^T f_t(\vx_t) + d(\vx_t, \vx_{t-1}).
\end{align*} That is, we want to trade completing our tasks optimally with moving around in the state space. Crucially, we do not know the sequence of tasks~$f_t$ in advance. Due to the cost associated with moving in the decision space, previous choices affect the future!}, convex function chasing\footnote{\idx<Convex function chasing>{convex function chasing} (or \idx{convex body chasing}) generalize metrical task systems to continuous domains~$\spX$. To make any guarantees about the performance in these settings, one typically has to assume that the tasks~$f_t$ are convex. Note that this mirrors our assumption in Bayesian optimization that similar alternatives yield similar results.}, and generalizations of multi-armed bandits to changing reward distributions) and reinforcement learning.
When operating in dynamic environments, other metrics such as the competitive ratio,\footnote{To assess the performance in dynamic environments, we typically compare to a dynamic optimum. As these problems are difficult (we are usually not able to guarantee convergence to the dynamic optimum), one considers a multiplicative performance metric similar to the approximation ratio, the \idx{competitive ratio}, \begin{align*}
  \mathrm{cost}(\mathrm{ALG}) \leq \alpha \cdot \mathrm{cost}(\mathrm{OPT}),
\end{align*} where $\mathrm{OPT}$ corresponds to the dynamic optimal choice (in hindsight).} which compares against the best \emph{dynamic} choice, are useful.
As we will later come to see in \cref{sec:mbarl:planning} in the context of reinforcement learning, operating in dynamic environments is deeply connected to a rich field of research called \midx{control}.

\section{Acquisition Functions}

It is common to use a so-called \midx{acquisition function}[idxpagebf] to greedily pick the next point to sample based on the current model.

Throughout our description of acquisition functions, we will focus on a setting where we model $\opt{f}$ using a Gaussian process which we denote by $f$.
The methods generalize to other means of learning $\opt{f}$ such as Bayesian deep learning.
The various acquisition functions $F$ are used in the same way as is illustrated in \cref{alg:bo}.

\begin{algorithm}
  \caption{Bayesian optimization (with GPs)}\pidx{Bayesian optimization}\label{alg:bo}
  initialize $f \sim \GP{\mu_0}{k_0}$\;
  \For{$t = 1$ \KwTo $T$}{
    choose $\vx_t = \argmax_{\vx \in \spX} F(\vx; \mu_{t-1}, k_{t-1})$\;
    observe $y_t = f(\vx_t) + \epsilon_t$\;
    perform a probabilistic update to obtain $\mu_t$ and $k_t$\;
  }
\end{algorithm}

\begin{rmk}{Model selection}{}
  Selecting a model of $\opt{f}$ in sequential decision-making is much harder than in the i.i.d. data setting of supervised learning.
  There are mainly the following two dangers: \begin{itemize}
    \item the data sets collected in active learning and Bayesian optimization are \emph{small}; and
    \item the data points are selected \emph{dependently} on prior observations.
  \end{itemize}
  This leads to a specific danger of overfitting.
  In particular, due to feedback loops between the model and the acquisition function, one may end up sampling the same point repeatedly.

  One approach to reduce the chance of overfitting is the use of hyperpriors\pidx{hyperprior} which we mentioned previously in \cref{sec:gp:model_selection:marginal_likelihood}.
  Another approach that often works fairly well is to occasionally (according to some schedule) select points uniformly at random instead of using the acquisition function.
  This tends to prevent getting stuck in suboptimal parts of the state space.
\end{rmk}

One possible acquisition function is uncertainty sampling \eqref{eq:uncertainty_sampling1}, which we discussed in the previous chapter.
However, this acquisition function does not at all take into account the objective of maximizing $\opt{f}$ and focuses solely on exploration.

Suppose that our model $f$ of $\opt{f}$ is well-calibrated, in the sense that the true function lies within its confidence bounds.
Consider the best lower bound, that is, the maximum of the lower confidence bound.
Now, if the true function is really contained in the confidence bounds, it must hold that the optimum is somewhere above this best lower bound.
In particular, we can exclude all regions of the domain where the upper confidence bound (the optimistic estimate of the function value) is lower than the best lower bound.
This is visualized in \cref{fig:optimistic_bo}.

\begin{marginfigure}
  \incplt{bayesian_optimization_optimism}
  \caption{Optimism in Bayesian optimization.
  The \textbf{unknown function} is shown in black, our \textbf{\b{model}} in blue with gray confidence bounds.
  The dotted black line denotes the maximum lower bound.
  We can therefore focus our exploration to the yellow regions where the upper confidence bound is higher than the maximum lower bound.}\label{fig:optimistic_bo}
\end{marginfigure}

Therefore, we only really care how the function looks like in the regions where the upper confidence bound is larger than the best lower bound.
The key idea behind the methods that we will explore is to focus exploration on these plausible maximizers.

Note that it is crucial that our uncertainty about $f$ reflects the ``fit'' of our model to the unknown function.
If the model is not well calibrated or does not describe the underlying function at all, these methods will perform poorly.
This is where we can use the Bayesian philosophy by imposing a prior belief that may be conservative.

\subsection{Upper Confidence Bound}\label{sec:bayesian_optimization:acquisition_functions:ucb}

The principle of \midx{optimism in the face of uncertainty} suggests picking the point where we can hope for the optimal outcome.
In this setting, this corresponds to simply maximizing the \midx{upper confidence bound} (UCB), \begin{align}
  \vx_{t+1} \defeq \argmax_{\vx \in \spX} \mu_{t}(\vx) + \beta_{t+1} \sigma_{t}(\vx),
\end{align} where $\sigma_t(\vx) \defeq \sqrt{k_t(\vx, \vx)}$ is the standard deviation at $\vx$ and $\beta_t$ regulates how confident we are about our model $f$ (i.e., the choice of confidence interval).

\begin{marginfigure}[-5\baselineskip]
  \incplt{ucb}
  \caption{Plot of the UCB acquisition function for $\beta = 0.25$ and $\beta = 1$, respectively.}\label{fig:ucb}
\end{marginfigure}

This acquisition function naturally trades exploitation by preferring a large posterior mean with exploration by preferring a large posterior variance.
Note that if $\beta_t = 0$ then UCB is purely exploitative, whereas, if $\beta_t \to \infty$, UCB recovers uncertainty sampling (i.e., is purely explorative).\footnote{Due to the monotonicity of $(\cdot)^2$, it does not matter whether we optimize the variance or standard deviation at $\vx$.}
UCB is an example of an optimism-based method, as it greedily picks the point where we can hope for the best outcome.

As can be seen in \cref{fig:ucb}, the UCB acquisition function is generally non-convex.
For selecting the next point, we can use approximate global optimization techniques like Lipschitz optimization (in low dimensions) and gradient ascent with random initialization (in high dimensions).
Another widely used technique is to sample some random points from the domain, score them according to this criterion, and simply take the best one.

The choice of $\beta_t$ is crucial for the performance of UCB.
Intuitively, for UCB to work even if the unknown function $\opt{f}$ is not contained in the confidence bounds, we use $\beta_t$ to re-scale the confidence bounds to enclose $\opt{f}$ as shown in \cref{fig:ucb_rescaling_cb}.
A theoretical analysis requires that $\beta_t$ is chosen ``correctly''.
Formally, we say that the sequence $\beta_t$ is chosen correctly if it leads to \midx<well-calibrated confidence intervals>{well-calibrated confidence interval}, that is, if with probability at least $1-\delta$, \begin{align}
  \forall t \geq 1,\ \forall \vx \in \spX :\quad \opt{f}(\vx) \in \spC_t(\vx) \defeq \brackets*{\mu_{t-1}(\vx) \pm \beta_t(\delta) \cdot \sigma_{t-1}(\vx)}. \label{eq:gp_ucb_beta_calibration}
\end{align}
\begin{marginfigure}
  \incplt{rescaling_confidence_bounds}
  \caption{Re-scaling the confidence bounds. The dotted gray lines represent updated confidence bounds.}\label{fig:ucb_rescaling_cb}
\end{marginfigure}
Bounds on $\beta_t(\delta)$ can be derived both in a ``Bayesian'' and in a ``frequentist'' setting.
In the Bayesian setting, it is assumed that $\opt{f}$ is drawn from the prior GP, i.e., $\opt{f} \sim \GP{\mu_0}{k_0}$.
However, in many cases this may be an unrealistic assumption.
In the frequentist setting, it is assumed instead that $\opt{f}$ is a fixed element of a reproducing kernel Hilbert space $\spH_k(\spX)$ which depending on the kernel $k$ can encompass a large class of functions.
We will discuss the Bayesian setting first and later return to the frequentist setting.

\begin{thm}[Bayesian confidence intervals, lemma 5.5 of \cite{srinivas2009gaussian}]\label{thm:bayesian_confidence_intervals} \exerciserefmark{bayesian_confidence_intervals}
  Let $\delta \in (0,1)$.
  Assuming $\opt{f} \sim \GP{\mu_0}{k_0}$ and Gaussian observation noise $\epsilon_t \sim \N{0}{\sigman^2}$, the sequence \begin{align}
    \beta_t(\delta) = \BigO{\sqrt{\log\parentheses*{\nicefrac{\card{\spX} t}{\delta}}}}
  \end{align} satisfies $\Pr{\forall t \geq 1, \vx \in \spX : \opt{f}(\vx) \in \spC_t(\vx)} \geq 1 - \delta$.
\end{thm}

Under the assumption of well-calibrated confidence intervals, we can bound the regret of UCB.

\begin{thm}[Regret of GP-UCB, theorem 2 of \cite{srinivas2009gaussian}]\label{thm:bayesian_regret_for_gp_ucb} \exerciserefmark{bayesian_regret_for_gp_ucb}
  If $\beta_t(\delta)$ is chosen ``correctly'' for a fixed $\delta \in (0,1)$, with probability at least $1-\delta$, greedily choosing the upper confidence bound yields cumulative regret \begin{align}
    R_T = \BigO{\beta_T(\delta) \sqrt{\gamma_T T}} \label{eq:ucb_regret}
  \end{align} where \begin{align}
    \gamma_T \defeq \max_{\substack{\sS \subseteq \spX \\ \card{\sS} = T}} \I{\vf_\sS}{\vy_\sS} = \max_{\substack{\sS \subseteq \spX \\ |\sS| = T}} \frac{1}{2} \log\det{\mI + \sigman^{-2} \mK_{\sS\sS}} \label{eq:gamma_t}
  \end{align} is the maximum information gain after $T$ rounds.
\end{thm}

Observe that if the information gain is sublinear in $T$ then we achieve sublinear regret and, in particular, converge to the true optimum.
The information gain $\gamma_T$ measures how much can be learned about $\opt{f}$ within $T$ rounds.
If the function is assumed to be smooth (perhaps even linear), then the information gain is smaller than if the function was assumed to be ``rough''.
Intuitively, the smoother the functions encoded by the prior, the smaller is the class of functions to choose from and the more can be learned from a single observation about ``neighboring'' points.

\begin{thm}[Information gain of common kernels, theorem~5 of \cite{srinivas2009gaussian} and remark~2 of \cite{vakili2021information}]\label{thm:kernel_info_gain}
  Due to submodularity, we have the following bounds on the information gain of common kernels:\looseness=-1 \begin{itemize}
    \item \emph{linear kernel} \begin{align}
      \gamma_T = \BigO{d \log T},
    \end{align}
    \item \emph{Gaussian kernel} \begin{align}
      \gamma_T = \BigO{(\log T)^{d+1}},
    \end{align}
    \item \emph{Matérn kernel} for $\nu > \frac{1}{2}$ \begin{align}
      \gamma_T = \BigO{T^{\frac{d}{2\nu + d}} (\log T)^{\frac{2\nu}{2\nu + d}}}. \label{eq:gamma_t_matern}
    \end{align}
  \end{itemize}
\end{thm}

\begin{marginfigure}
  \incplt{kernel_info_gain}
  \caption{Information gain of \textbf{independent}, \textbf{\g{linear}}, \textbf{\b{Gaussian}}, and \r{\textbf{Matérn} ($\nu \approx 0.5$)} kernels with $d = 2$ (up to constant factors).
  The kernels with sublinear information gain have strong diminishing returns (due to their strong dependence between ``close'' points).
  In contrast, the independent kernel has no dependence between points in the domain, and therefore no diminishing returns.
  Intuitively, the ``smoother'' the class of functions modeled by the kernel, the stronger are the diminishing returns.}\label{fig:kernel_info_gain}
\end{marginfigure}

The information gain of common kernels is illustrated in \cref{fig:kernel_info_gain}.
Notably, when all points in the domain are independent, the information gain is linear in $T$.
This is because when the function $\opt{f}$ may be arbitrarily ``rough'', we cannot generalize from a single observation to ``neighboring'' points, and as there are infinitely many points in the domain $\spX$ there are no diminishing returns.
As one would expect, in this case, \cref{thm:bayesian_regret_for_gp_ucb} does not yield sublinear regret.
However, we can see from \cref{thm:kernel_info_gain} that the information gain is sublinear for linear, Gaussian, and most Matérn kernels.
Moreover, observe that unless the function is linear, the information gain grows exponentially with the dimension $d$.
This is because the number of ``neighboring'' points (with respect to Euclidean geometry) decreases exponentially with the dimension which is also known as the \midx{curse of dimensionality}.

As mentioned, the size of the confidence intervals can also be analyzed under a frequentist assumption on $\opt{f}$.

\begin{thm}[Frequentist confidence intervals, theorem 2 of \cite{chowdhury2017kernelized}]\label{thm:frequentist_confidence_intervals}
  Let $\delta \in (0,1)$.
  Assuming $\opt{f} \in \spH_k(\spX)$, we have that with probability at least $1 - \delta$, the sequence \begin{align}
    \beta_t(\delta) = \norm{\opt{f}}_k + \sigman \sqrt{2 \parentheses*{\gamma_t + \log\parentheses*{\nicefrac{1}{\delta}}}}
  \end{align} satisfies $\Pr{\forall t \geq 1, \vx \in \spX : \opt{f}(\vx) \in \spC_t(\vx)} \geq 1 - \delta$.
\end{thm}

That is, $\beta_t$ depends on the information gain of the kernel as well as on the ``complexity'' of $\opt{f}$ which is measured in terms of its norm in the underlying reproducing kernel Hilbert space $\spH_k(\spX)$.

\begin{rmk}{Bayesian vs frequentist assumption}{}
  \Cref{thm:bayesian_confidence_intervals,thm:frequentist_confidence_intervals} provide different bounds on $\beta_t(\delta)$ based on fundamentally different assumptions on the ground truth $\opt{f}$:
  The Bayesian assumption is that $\opt{f}$ is drawn from the prior GP, whereas the frequentist assumption is that $\opt{f}$ is an element of a reproducing kernel Hilbert space $\spH_k(\spX)$.
  The frequentist assumption holds uniformly for all functions $\opt{f}$ with $\norm{\opt{f}}_k < \infty$, whereas the Bayesian assumption holds only under the Bayesian ``belief'' that $\opt{f}$ is drawn from the prior GP.

  Interestingly, neither assumption encompasses the other.
  This is because if $f \sim \GP{0}{k}$ then it can be shown that almost surely $\norm{f}_k = \infty$, which implies that $f \not\in \spH_k(\spX)$ \citep{srinivas2009gaussian}.
\end{rmk}

We remark that \cref{thm:frequentist_confidence_intervals} holds also under the looser assumption that observations are perturbed by $\sigman$-sub-Gaussian noise (cf. \cref{eq:sub_gaussian}) instead of Gaussian noise.
The bound on $\gamma_T$ from \cref{eq:gamma_t_matern} for the Matérn kernel does not yield sublinear regret when combined with the standard regret bound from \cref{thm:bayesian_regret_for_gp_ucb}, however, \cite{whitehouse2023sublinear} show that the regret of GP-UCB is sublinear also in this case provided $\sigman^2$ is chosen carefully.

This concludes our discussion of the UCB algorithm.
We have seen that its regret can be analyzed under both Bayesian and frequentist assumptions on $\opt{f}$.

\subsection{Improvement}

Another well-known family of methods is based on keeping track of a running optimum $\hat{f}_t$, and scoring points according to their improvement upon the running optimum.
The \midx{improvement} of $\vx$ after round $t$ is measured by \begin{align}
  I_t(\vx) \defeq (f(\vx) - \hat{f}_t)_+ \label{eq:improvement}
\end{align} where we use $(\cdot)_+$ to denote $\max \{0, \cdot\}$.

The \midx{probability of improvement} (PI) picks the point that maximizes the probability to improve upon the running optimum, \begin{align}
  \vx_{t+1} &\defeq \argmax_{\vx \in \spX} \Pr{I_t(\vx) > 0 \mid \vx_{1:t}, y_{1:t}} \\
  &= \argmax_{\vx \in \spX} \fnPr(f(\vx) > \hat{f}_t \mid \vx_{1:t}, y_{1:t}) \label{eq:pi} \\
  &= \argmax_{\vx \in \spX} \Phi\parentheses*{\frac{\mu_{t}(\vx) - \hat{f}_t}{\sigma_{t}(\vx)}} \margintag{using linear transformations of Gaussians \eqref{eq:gaussian_lin_trans}}
\end{align} where $\Phi$ denotes the CDF of the standard normal distribution and we use that $f(\vx) \mid \vx_{1:t}, y_{1:t} \sim \N{\mu_t(\vx)}{\sigma_t^2(\vx)}$.
Probability of improvement tends to be biased in favor of exploitation, as it prefers points with large posterior mean and small posterior variance which is typically true ``close'' to the previously observed maximum $\hat{f}_t$.

\begin{marginfigure}[7\baselineskip]
  \incplt{pi_ei}
  \caption{Plot of the PI and EI acquisition functions, respectively.}
\end{marginfigure}

Probability of improvement looks at \emph{how likely} a point is to improve upon the running optimum.
An alternative is to look at \emph{how much} a point is expected to improve upon the running optimum.
This acquisition function is called the \midx{expected improvement} (EI), \begin{align}
  \vx_{t+1} \defeq \argmax_{\vx \in \spX} \E{I_t(\vx)}[\vx_{1:t}, y_{1:t}]. \label{eq:ei}
\end{align}
Intuitively, EI seeks a large expected improvement (exploitation) while also preferring states with a large variance (exploration).
Expected improvement yields the same regret bound as UCB \citep{ei_regret}.\looseness=-1

The expected improvement acquisition function is often flat which makes it difficult to optimize in practice due to vanishing gradients.
One approach addressing this is to instead optimize the logarithm of EI \citep{ament2023unexpected}.

\begin{figure*}
  \incplt{acquisition_functions_contours}
  \caption{Contour lines of acquisition functions for varying $\Delta_t = \mu_t(\vx) - \hat{f}_t$ and $\sigma_t$.
  A brighter color corresponds to a larger acquisition function value.
  The first graph shows contour lines of UCB with $\beta_t = 0.75$, the second of PI, and the third of EI.}
\end{figure*}

\subsection{Thompson Sampling}\label{sec:bayesian_optimization:acquisition_functions:thompson_sampling}

We can also interpret the principle of \midx{optimism in the face of uncertainty} in a slightly different way than we did with UCB (and EI).
Suppose we select the next point according to the probability that it is optimal (assuming that the posterior distribution is an accurate representation of the uncertainty), \begin{align}
  \pi(\vx \mid \vx_{1:t}, y_{1:t}) &\defeq \Pr[f \mid \vx_{1:t}, y_{1:t}]{f(\vx) = \max_\vxp f(\vxp)} \\
  \vx_{t+1} &\sim \pi(\cdot \mid \vx_{1:t}, y_{1:t}).
\end{align}
This approach of sampling according to the \midx{probability of maximality}~$\pi$ is called \midx{probability matching}. Probability matching is exploratory as it prefers points with larger variance (as they automatically have a larger chance of being optimal), but at the same time exploitative as it effectively discards points with low posterior mean and low posterior variance.
Unfortunately, it is generally difficult to compute $\pi$ analytically given a posterior.

Instead, it is common to use a sampling-based approximation of $\pi$. Observe that the density $\pi$ can be expressed as an expectation, \begin{align}
  \pi(\vx \mid \vx_{1:t}, y_{1:t}) &= \E*[f \mid \vx_{1:t}, y_{1:t}]{\big[\Ind{f(\vx) = \max_\vxp f(\vxp)}\big]},
\intertext{which we can approximate using Monte Carlo sampling (typically using a single sample),}
  &\approx \Ind{\Tilde{f}_{t+1}(\vx) = \max_\vxp \Tilde{f}_{t+1}(\vxp)}
\end{align} where $\Tilde{f}_{t+1} \sim p(\cdot \mid \vx_{1:t}, y_{1:t})$ is a sample from our posterior distribution.
Observe that this approximation of $\pi$ coincides with a point density at the maximizer of $\Tilde{f}_{t+1}$.

The resulting algorithm is known as \midx{Thompson sampling}[idxpagebf].
At time $t+1$, we sample a function $\Tilde{f}_{t+1} \sim p(\cdot \mid \vx_{1:t}, y_{1:t})$ from our posterior distribution.
Then, we simply maximize $\Tilde{f}_{t+1}$, \begin{align}
  \vx_{t+1} \defeq \argmax_{\vx \in \spX} \Tilde{f}_{t+1}(\vx).
\end{align}
In many cases, the randomness in the realizations of $\Tilde{f}_{t+1}$ is already sufficient to effectively trade exploration and exploitation.
Similar regret bounds to those of UCB can also be established for Thompson sampling \citep{russo2016information,kandasamy2018parallelised}.

\subsection{Information-Directed Sampling}

After having looked at multiple methods that aim to balance exploitation of the current posterior distribution over~$f$ for immediate returns with exploration to reduce uncertainty about~$f$ for future returns, we will next discuss a method that makes this tradeoff explicit.

Denoting the instantaneous regret of choosing $\vx$ as $\Delta(\vx) \defeq \max_{\vxp} \opt{f}(\vxp) - \opt{f}(\vx)$ and by $I_t(\vx)$ some function capturing the ``information gain'' associated with observing $\vx$ in iteration $t+1$, we can define the \midx{information ratio}, \begin{align}
  \Psi_t(\vx) \defeq \frac{\Delta(\vx)^2}{I_t(\vx)}, \label{eq:regret_information_ratio}
\end{align} which was originally introduced by \cite{russo2016information}.
Here exploitation reduces regret while exploration increases information gain, and hence, points $\vx$ that minimize the information ratio are those that most effectively balance exploration and exploitation.
We can make the key observation that the regret $\Delta(\cdot)$ decreases when $I_t(\cdot)$ decreases, as a small $I_t(\cdot)$ implies that the algorithm has already learned a lot about the function $\opt{f}$.
The strength of this relationship is quantified by the information ratio:\looseness=-1

\begin{thm}[Proposition 1 of \cite{russo2014learning} and theorem 8 of \cite{kirschner2018information}]\label{thm:regret_information_ratio}
  For any iteration $T \geq 1$, let $\sum_{t=1}^T I_{t-1}(\vx_t) \leq \gamma_T$ and suppose that $\Psi_{t-1}(\vx_t) \leq \overline{\Psi}_T$ for all $t \in [T]$.
  Then, the cumulative regret is bounded by \begin{align}
    R_T \leq \sqrt{\gamma_T \overline{\Psi}_T T}.
  \end{align}
\end{thm}
\begin{proof}
  By \cref{eq:regret_information_ratio}, $r_t = \Delta(\vx_t) = \sqrt{\Psi_{t-1}(\vx_t) \cdot I_{t-1}(\vx_t)}$.
  Hence, \begin{align*}
    R_T &= \sum_{t=1}^T r_t \\
    &= \sum_{t=1}^T \sqrt{\Psi_{t-1}(\vx_t) \cdot I_{t-1}(\vx_t)} \\
    &\leq \sqrt{\sum_{t=1}^T \Psi_{t-1}(\vx_t) \cdot \sum_{t=1}^T I_{t-1}(\vx_t)} \margintag{using the Cauchy-Schwarz inequality} \\
    &\leq \sqrt{\gamma_T \overline{\Psi}_T T}. \qedhere \margintag{using the assumptions on $I_t(\cdot)$ and $\Psi_t(\cdot)$}
  \end{align*}
\end{proof}

\begin{ex}{How to measure ``information gain''?}{ids_how_to_measure_information_gain}
  One possibility of measuring the ``information gain'' is \begin{align}
    I_t(\vx) \defeq \I{f_\vx}{y_\vx}[\vx_{1:t}, y_{1:t}]
  \end{align} which --- as you may recall --- is precisely the marginal gain of the utility $I(\sS) = \I{\vf_\sS}{\vy_\sS}$ we were studying in \cref{sec:active_learning}.
  In this case, \begin{align*}
    \sum_{t=1}^T I_{t-1}(\vx_t) &= \sum_{t=1}^T \I{f_{\vx_t}}{y_{\vx_t}}[\vx_{1:t-1}, y_{1:t-1}] \\
    &= \sum_{t=1}^T \I{\vf_{\vx_{1:T}}}{y_{\vx_t}}[\vx_{1:t-1}, y_{1:t-1}] \margintag{using $\I{\rX, \rZ}{\rY} \geq \I{\rX}{\rY}$ which follows from \cref{eq:cond_mi_joint_mi,eq:mi_non_neg} and is called \midx<monotonicity of MI>{monotonicity of mutual information}} \\[8pt]
    &= \I{\vf_{\vx_{1:T}}}{\vy_{\vx_{1:T}}} \margintag{by repeated application of \cref{eq:cond_mi_joint_mi}, also called the \midx<chain rule of MI>{chain rule of mutual information}} \\[8pt]
    &\leq \gamma_T. \margintag{by definition of $\gamma_T$ \eqref{eq:gamma_t}}
  \end{align*}
\end{ex}

The regret bound from \cref{thm:regret_information_ratio} suggests an algorithm which in each iteration chooses the point which minimizes the information ratio \eqref{eq:regret_information_ratio}.
However, this is not possible since $\Delta(\cdot)$ is unknown due to its dependence on $\opt{f}$.
\cite{kirschner2018information} propose to use a surrogate to the regret which is based on the current model of $\opt{f}$, \begin{align}
  \hat{\Delta}_t(\vx) \defeq \max_{\vxp \in \spX} u_{t}(\vxp) - l_{t}(\vx).
\end{align}
Here, $u_t(\vx) \defeq \mu_t(\vx) + \beta_{t+1} \sigma_t(\vx)$ and $l_t(\vx) \defeq \mu_t(\vx) - \beta_{t+1} \sigma_t(\vx)$ are the upper and lower confidence bounds of the confidence interval $\spC_t(\vx)$ of~$\opt{f}(\vx)$, respectively.
Similarly to our discussion of UCB, we make the assumption that the sequence $\beta_t$ is chosen ``correctly'' (cf. \cref{eq:gp_ucb_beta_calibration}) so that the confidence interval is well-calibrated and $\Delta(\vx) \leq \hat{\Delta}_t(\vx)$ with high probability.
The resulting algorithm \begin{align}
  \vx_{t+1} \defeq \argmin_{\vx \in \spX} \braces*{\widehat{\Psi}_t(\vx) \defeq \frac{\hat{\Delta}_t(\vx)^2}{I_t(\vx)}} \label{eq:ids}
\end{align} is known as \midx{information-directed sampling} (IDS).

\begin{thm}[Regret of IDS, lemma 8 of \cite{kirschner2018information}]\label{thm:regret_of_ids} \exerciserefmark{regret_of_ids}
  Let $\beta_t(\delta)$ be chosen ``correctly'' for a fixed $\delta \in (0,1)$.
  Then, if the measure of information gain is $I_t(\vx) = \I{f_\vx}{y_{\vx}}[\vx_{1:t}, y_{1:t}]$, with probability at least $1-\delta$, IDS has cumulative regret \begin{align}
    R_T = \BigO{\beta_T(\delta) \sqrt{\gamma_T T}}.
  \end{align}
\end{thm}

Regret bounds such as \cref{thm:regret_of_ids} can be derived also for different measures of information gain.
For example, the argument of \cref{exercise:regret_of_ids} also goes through for the ``greedy'' measure \begin{align}
  I_t(\vx) \defeq \mathrm{I}(f_{\vx_t^{\mathrm{UCB}}} ; y_\vx \mid \vx_{1:t}, y_{1:t}) \label{eq:ucb_info_gain}
\end{align} which focuses exclusively on reducing the uncertainty at $\vx_t^{\mathrm{UCB}}$ rather than globally.
We compare the two measures of information gain in \cref{fig:ids}.
Observe that the acquisition function depends critically on the choice of $I_t(\cdot)$ and is less sensitive to the scaling of confidence intervals.

\begin{marginfigure}
  \incplt{ids}
  \caption{Plot of the surrogate information ratio $\widehat{\Psi}$: IDS selects its minimizer. The first two plots use the ``global'' information gain measure from \cref{ex:ids_how_to_measure_information_gain} with $\beta = 0.25$ and $\beta = 0.5$, respectively. The third plot uses the ``greedy'' information gain measure from \cref{eq:ucb_info_gain} and $\beta = 1$.}\label{fig:ids}
\end{marginfigure}

IDS trades exploitation and exploration by balancing the (exploitative) regret surrogate with a measure of information gain (such as those studied in \cref{sec:active_learning}) that is purely explorative.
In this way, IDS can account for kinds of information which are not addressed by alternative algorithms such as UCB or EI \citep{russo2014learning}:
Depending on the measure of information gain, IDS can select points to obtain \emph{indirect information} about other points or \emph{cumulating information} that does not immediately lead to a higher reward but only when combined with subsequent observations.
Moreover, IDS avoids selecting points which yield \emph{irrelevant information}.\looseness=-1

\subsection{Probabilistic Inference}\pidx{probability of maximality}[idxpagebf]

As we mentioned before, computing the probability of maximality~$\pi$ is generally intractable.
You can think of computing~$\pi$ as attempting to fully solve the probabilistic inference problem associated with determining the optimum of ${f \mid \vx_{1:t}, y_{1:t}}$.
In many cases, it is useful to determine the probability that a point is optimal under the current posterior distribution, and we will consider one particular example in the following.

\begin{ex}{Maximizing recall}{}
  In domains such as molecular design we often use machine learning to screen candidates for further manual testing.
  The goal here is to suggest a small set $E$ from a large domain of molecules~$\spX$, so that the probability of~$E$ containing the optimal molecule, i.e., the \midx{recall}, is maximized.
  Note that this task is quite different from online Bayesian optimization, for example, in BO we get sequential feedback that we can use to decide which inputs to query next.
  Nevertheless, we will see in this section that both tasks turn out to be closely related.

  Let us assume that the maximizer is unique almost surely, which with GPs is automatically the case if there are no same-mean, perfectly-correlated entries.
  The recall task is then intimately related to the task of determining the probability of maximality, since \begin{align}\begin{split}
    \argmax_{E \subseteq \spX : \abs{E} = k} \Pr[f\mid\vx_{1:t},y_{1:t}]{\max_{\vx \in E} f(\vx) = \max_\vxp f(\vxp)} \\
    = \argmax_{E \subseteq \spX : \abs{E} = k} \sum_{\vx \in E} \pi(\vx \mid \vx_{1:t}, y_{1:t}). \margintag{by noting that for $\vx \neq \vy$, the events ${\{f(\vx) = \max_\vxp f(\vxp)\}}$ and ${\{f(\vy) = \max_\vxp f(\vxp)\}}$ are disjoint}
  \end{split}\label{eq:recall_task}\end{align}
  Thus, to obtain the recall-optimal candidates for further testing, we need to find the probability of maximality.
  However, as mentioned, computing the probability of maximality~$\pi$ is generally intractable.

  \Cref{fig:lite_recall} depicts an example of a recall task, where the black line shows the optimal recall achieved by knowing the probability of maximality~$\pi$ exactly.
  LITE~\citep{menet2025lite} is an almost-linear time approximation of $\pi$, whereas TS selects points via Thompson sampling and MEANS selects the points with the highest posterior mean.
  Selecting $E$ according to probability of maximality achieves much higher recall than the other intuitive heuristics.
\end{ex}

\begin{marginfigure}[-10\baselineskip]
  \includegraphics[width=0.9\columnwidth]{figures/lite_recall}
  \caption{\cite{menet2025lite} show in an experiment that selecting $E \subseteq \spX$ according to the estimates of probability of maximality from LITE (here called F-LITE) is near-optimal. Intuitive heuristics such as Thompson sampling (TS) or selecting points with the highest posterior mean (MEANS) perform worse.}
  \label{fig:lite_recall}
\end{marginfigure}

So how can we estimate the probability of maximality? LITE approximates~$\pi$ by \begin{align}
  \pi(\vx \mid \vx_{1:t}, y_{1:t}) &= \Pr[f \mid \vx_{1:t}, y_{1:t}]{f(\vx) \geq \max_\vxp f(\vxp)} \nonumber \\
  &\approx \Pr[f \mid \vx_{1:t}, y_{1:t}]{f(\vx) \geq \kappa^*} \label{eq:lite_pi} \\
  &= \Phi\parentheses*{\frac{\mu_t(\vx) - \kappa^*}{\sigma_t(\vx)}} \label{eq:lite}
\end{align} with $\Phi$ denoting the CDF of the standard normal distribution and $\kappa^*$ chosen such that the approximation of $\pi$ integrates to $1$, so that it is a valid distribution.

\begin{marginfigure}
  \incplt{bo_prob_inference}
  \caption{Plot of the probability of maximality as estimated by LITE.}
  \label{fig:bo_prob_inference}
\end{marginfigure}

Remarkably, LITE is intimately related to many of the BO methods we discussed in this chapter.
First, and most obviously, \cref{eq:lite_pi} looks similar to the PI acquisition function \eqref{eq:pi}.
But note that $\kappa^*$ is not equal to the best observed value $\hat{f}_t$ as in PI, but instead typically larger.
If $\kappa^* > \hat{f}_t$, then \cref{eq:lite_pi} is more exploratory than PI: it puts additional emphasis on points with large posterior variance and comparatively less emphasis on points with large posterior mean, which we illustrate in \cref{fig:bo_prob_inference}.

An insightful interpretation of LITE is that it balances two different kinds of exploration: \midx{optimism in the face of uncertainty} and \midx{entropy regularization}.
To see this, let us define the following variational objective:\looseness=-1 \begin{align}
  \spW(\pi) \defeq \sum_{\vx \in \spX} \pi(\vx) \cdot \Big( \underbrace{\mu_t(\vx)}_{\text{exploitation}} + \underbrace{\sqrt{2 \, \fnS'(\pi(\vx))} \cdot \sigma_t(\vx)}_{\text{exploration}} \Big) \label{eq:lite_variational}
\end{align} with the ``quasi-surprise'' $\fnS'(u) \defeq \frac{1}{2}(\phi(\inv{\Phi}(u))/u)^2$.
The quasi-surprise $\fnS'(\cdot)$ behaves similarly to the surprise $-\ln(\cdot)$.
In fact, their asymptotics coincide: \begin{align*}
  \fnS'(1) = 0 = - \ln(1) \quad\text{and}\quad \fnS'(u) \to - \ln(u) \text{ as } u \to 0^+.
\end{align*}
The objective $\spW$ is maximized for those probability distributions $\pi$ that are concentrated around points with large mean $\mu_t(\vx)$ and points with large exploration bonus.
We have seen that the uncertainty $\sigma_t(\vx)$ about $f(\vx)$ is the standard exploration bonus of UCB-style algorithms.
In \cref{eq:lite_variational}, $\sigma_t(\vx)$ is weighted by the quasi-surprise, which acts as ``entropy regularization'': it increases the entropy of $\pi$ by uniformly pushing $\pi(\vx)$ away from zero.
You can think of entropy regularization as encouraging a different kind of exploration than optimism by not making deterministic decisions like in UCB.

\cite{menet2025lite} show that LITE \eqref{eq:lite} is the solution to the variational problem \eqref{eq:lite_variational} among valid probability distributions \exerciserefmark{lite}: \begin{align}
  \argmax_{\pi \in \Delta^{\spX}} \spW(\pi) = \Phi\parentheses*{\frac{\mu_t(\cdot) - \kappa^*}{\sigma_t(\cdot)}} \label{eq:lite_variational_solution}
\end{align} with $\kappa^*$ such that the right-hand side sums to $1$.\footnote{$\Delta^{\spX}$ is the probability simplex on~$\spX$.}
This indicates that LITE and Thompson sampling, which samples from probability of maximality, achieve exploration through two means:\looseness=-1 \begin{enumerate}
  \item \textbf{Optimism:} by preferring points with large uncertainty $\sigma_t(\vx)$ about the reward value $f(\vx)$.
  \item \textbf{Decision uncertainty:} by assigning some probability mass to all $\vx$, that is, by remaining uncertain about which $\vx$ is the maximizer.
\end{enumerate}
In our discussion of balancing exploration and exploitation in reinforcement learning, we will return to this dichotomy of exploration strategies.

\begin{rmk}{But why is ``exploration'' useful for recall?}{exploration_in_recall}
  Let us pause for a moment and reflect on why this interpretation of LITE is remarkable.
  This interpretation shows that LITE is more ``exploratory'' than simply taking the highest posterior means.
  However, the recall task~\eqref{eq:recall_task} differs from the standard BO task in that we do not collect any further observations, which we may use downstream to make better decisions.
  In the recall task, we are only interested in having the best shot at including the maximizer in the set $E$, without obtaining any further information or making any subsequent decisions.
  At first sight, this seems to suggest that we should be as ``exploitative'' as possible.
  Then how can it be that ``exploration'' is useful?

  We will explore this question with the following example: You are observing a betting game.
  When placing a bet, players can either place a safe bet (``playing safe'') or a risky bet (``playing risky'').
  You now have to place a bet on their bets, and estimate which of the players will win the most money: those that play safe or those that play risky?
  Note that you do not care whether your guess ends up in 2nd place or last place among all players --- you only care about whether your guess wins.
  That is, you are interested in \emph{recalling} the best player, not in ``ranking'' the players.

  Consider three players: one that plays safe and two that play risky.
  Suppose that the safe bet has payoff $S = 1$ while each risky bet has payoff $R \sim \N{0}{100}$.
  In expectation, the safe player will win the most money.
  However, one can see with just a little bit of algebra that the probability of either of the risky players winning the most money is~$\approx\! 35\%$, whereas the safe player only wins with probability~$\approx\! 29\%$ \exerciserefmark{recall}.
  That is, it is in fact \emph{optimal} to bet on either of the risky players since the best player might have vastly outperformed their expected winnings, and performed closer to their upper confidence bound.
  In summary, maximizing recall requires us to be ``exploratory'' since it is likely that the optimum among inputs is one that has performed better than expected, not simply the one with the highest expected performance.
\end{rmk}

\section*{Discussion}

In this chapter, we have explored the exploration-exploitation dilemma in the context of optimizing black-box functions.
To this end, we have explored various methods to balance exploration and exploitation.
While computing the precise probability of maximality is generally intractable, we found that it can be understood approximately as balancing two sources of exploration: optimism in the face of uncertainty and entropy regularization.

In the following chapters, we will begin to discuss stateful settings where the black-box optimization task is known as ``reinforcement learning''.
Naturally, we will see that the exploration-exploitation dilemma is also a central challenge in reinforcement learning, and we will revisit many of the concepts we have discussed in this chapter.

\begin{oreadings}
  \begin{itemize}
    \item \pcite{srinivas2009gaussian}
    \item \pcite{golovin2017google}
    \item \pcite{romero2013navigating}
    \item \pcite{chowdhury2017kernelized}
  \end{itemize}
\end{oreadings}

\excheading

\begin{nexercise}{Convergence to the static optimum}{convergence_to_static_opt}
  Show that any algorithm where $\lim_{t\to\infty} \opt{f}(\vx_t)$ exists achieves sublinear regret if and only if it converges to the static optimum, that is, \begin{align}
    \lim_{t\to\infty} \opt{f}(\vx_t) = \max_\vx \opt{f}(\vx).
  \end{align}
  \textit{Hint: Use that if a sequence $a_n$ converges to $a$ as $n \to \infty$, then we have for the sequence \begin{align}
    b_n \defeq \frac{1}{n} \sum_{i=1}^n a_i \label{eq:cesaro_mean}
  \end{align} that $\lim_{n\to\infty} b_n = a$. This is also known as the \midx{Cesàro mean}.}
\end{nexercise}

\begin{nexercise}{Bayesian confidence intervals}{bayesian_confidence_intervals}
  In this exercise, we derive \cref{thm:bayesian_confidence_intervals}.
  \begin{enumerate}
    \item For fixed $t \geq 1$ and $\vx \in \spX$, prove \begin{align}
      \Pr{\opt{f}(\vx) \not\in \spC_t(\vx) \mid \vx_{1:t-1}, y_{1:t-1}} \leq e^{- \beta_t^2 / 2}.
    \end{align}
    \textit{Hint: Bound $\Pr{Z > c}$ for $Z \sim \N{0}{1}$ and $c > 0$.}

    \item Prove \cref{thm:bayesian_confidence_intervals}.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Regret of GP-UCB}{bayesian_regret_for_gp_ucb}
  To develop some intuition, we will derive \cref{thm:bayesian_regret_for_gp_ucb}.

  \begin{enumerate}
    \item Show that if \cref{eq:gp_ucb_beta_calibration} holds, then for a fixed $t \geq 1$ the instantaneous regret $r_t$ is bounded by $2 \beta_t \sigma_{t-1}(\vx_t)$.

    \item Let $S_T \defeq \{\vx_t\}_{t=1}^T$, and define $\vf_T \defeq \vf_{S_T}$ and $\vy_T \defeq \vy_{s_T}$.
    Prove \begin{align}
      \I{\vf_T}{\vy_T} = \frac{1}{2} \sum_{t=1}^T \log\parentheses*{1 + \frac{\sigma_{t-1}^2(\vx_t)}{\sigman^2}}. \label{eq:bayesian_regret_for_gp_ucb_info_gain_helper}
    \end{align}

    \item\label{exercise:bayesian_regret_for_gp_ucb:3} Combine (1) and (2) to show \cref{thm:bayesian_regret_for_gp_ucb}.
    We assume w.l.o.g. that the sequence $\{\beta_t\}_t$ is monotonically increasing.\par
    \textit{Hint: If $s \in [0,M]$ for some $M > 0$ then $s \leq C \cdot \log(1 + s)$ with $C \defeq M / \log(1 + M)$.}
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Sublinear regret of GP-UCB for a linear kernel}{sublinear_regret_for_linear_kernel}
  Assume that $\opt{f} \sim \GP{0}{k}$ where $k$ is the linear kernel \begin{align*}
    k(\vx, \vxp) = \transpose{\vx}\vxp.
  \end{align*}
  In addition, we assume that for any $\vx \in \spX$, $\norm{\vx}_2 \leq 1$.
  Moreover, recall that the points in a finite set $\sS \subseteq \spX$ can be written in a matrix form (the ``design matrix'') which we denote by $\mX_\sS \in \R^{d \times \card{\sS}}$.

  \begin{enumerate}
    \item Prove that $\gamma_T = \BigO{d \log T}$.

    \item Deduce from (1) and \cref{thm:bayesian_regret_for_gp_ucb} that $\lim_{T\to\infty} R_T / T = 0$.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Closed-form expected improvement}{closed_form_ei}
  Let us denote the acquisition function of EI from \cref{eq:ei} by $\mathrm{EI}_t(\vx)$.
  In this exercise, we derive a closed-form expression.

  \begin{enumerate}
    \item Show that \begin{align}
      \mathrm{EI}_t(\vx) = \int_{\nicefrac{(\hat{f}_t - \mu_{t}(\vx))}{\sigma_{t}(\vx)}}^{+\infty} (\mu_{t}(\vx) + \sigma_{t}(\vx) \varepsilon - \hat{f}_t) \cdot \phi(\varepsilon) \,d\varepsilon
    \end{align} where $\phi$ is the PDF of the univariate standard normal distribution \eqref{eq:univ_normal}.\par
    \textit{Hint: Reparameterize the posterior distribution \begin{align*}
      f(\vx) \mid \vx_{1:t}, y_{1:t} \sim \N{\mu_{t}(\vx)}{\sigma_{t}^2(\vx)}
    \end{align*} using a standard normal distribution.}
    \item Using the above expression, show that \begin{align}
      \mathrm{EI}_t(\vx) = (\mu_{t}(\vx) - \hat{f}_t) \Phi\parentheses*{\frac{\mu_{t}(\vx) - \hat{f}_t}{\sigma_{t}(\vx)}} + \sigma_{t}(\vx) \phi\parentheses*{\frac{\mu_{t}(\vx) - \hat{f}_t}{\sigma_{t}(\vx)}} \label{eq:ei_simp}
    \end{align} where $\Phi(u) \defeq \int_{-\infty}^u \phi(\varepsilon) \,d\varepsilon$ denotes the CDF of the standard normal distribution.
  \end{enumerate}
  Note that the first term of \cref{eq:ei_simp} encourages exploitation while the second term encourages exploration.
  EI can be seen as a special case of UCB where the confidence bounds are scaled depending on $\vx$: \begin{align*}
    \beta_t = \phi(z_t(\vx)) / \Phi(z_t(\vx))
  \end{align*} for $z_t(\vx) \defeq (\mu_{t}(\vx) - \hat{f}_t) / \sigma_{t}(\vx)$.
\end{nexercise}

\begin{nexercise}{Regret of IDS}{regret_of_ids}
  We derive \cref{thm:regret_of_ids}.
  \begin{enumerate}
    \item Prove that for all $t \geq 1$, $\hat{\Delta}_t(\vx_t^{\mathrm{UCB}}) \leq 2 \beta_{t+1} \sigma_t(\vx_t^{\mathrm{UCB}})$ where we denote by $\vx_t^{\mathrm{UCB}} \defeq \argmax_{\vx \in \spX} u_t(\vx)$ the point maximizing the upper confidence bound after iteration $t$.
    \item Using (1) and the assumption on $I_t$, bound $\widehat{\Psi}_t(\vx_t^{\mathrm{UCB}})$.\par
    \textit{Hint: You may find the hint of \cref{exercise:bayesian_regret_for_gp_ucb} (3) useful.}
    \item Complete the proof of \cref{thm:regret_of_ids}.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Variational form of LITE}{lite}
  Let $\abs{\spX} < \infty$ be such that we can write $\spW$ as an objective function over the probability simplex $\Delta^{\abs{\spX}} \subset \R^{\abs{\spX}}$.
  Derive \cref{eq:lite_variational_solution}.\par
  \textit{Hint: First show that $\spW(\cdot)$ is concave (i.e., minimizing $-\spW(\cdot)$ is a convex optimization problem) and then use Lagrange multipliers to find the optimum.}
\end{nexercise}

\begin{nexercise}{Finding the winning player}{recall}
  Consider the betting game from \cref{rmk:exploration_in_recall} with two risky players, $R_1 \sim \N{0}{100}$ and $R_2 \sim \N{0}{100}$, and one safe player $S = 1$.
  Prove that the individual probability of any of the risky players winning the most money is larger than the winning probability of the safe player.
  We assume that players payoffs are mutually independent.

  \textit{Hint: Compute the CDF of $R \defeq \max\{R_1, R_2\}$.}
\end{nexercise}
