\chapter{Fundamentals of Inference}\label{sec:fundamentals}

Boolean logic is the algebra of statements which are either true or false.
Consider, for example, the statements \begin{center}
  ``If it is raining, the ground is wet.'' \quad and \quad ``It is raining.''
\end{center}
A quite remarkable property of Boolean logic is that we can combine these premises to draw logical inferences which are \emph{new} (true) statements.
In the above example, we can conclude that the ground must be wet.
This is an example of logical reasoning which is commonly referred to as \midx{logical inference}, and the study of artificial systems that are able to perform logical inference is known as \midx{symbolic artificial intelligence}.\looseness=-1

But is it really raining?
Perhaps it is hard to tell by looking out of the window.
Or we have seen it rain earlier, but some time has passed since we have last looked out of the window.
And is it really true that if it rains, the ground is wet?
Perhaps the rain is just light enough that it is absorbed quickly, and therefore the ground still appears dry.

This goes to show that in our experience, the real world is rarely black and white.
We are frequently (if not usually) uncertain about the truth of statements, and yet we are able to reason about the world and make predictions.
We will see that the principles of Boolean logic can be extended to reason in the face of uncertainty.
The mathematical framework that allows us to do this is probability theory, which --- as we will find in this first chapter --- can be seen as a natural extension of Boolean logic from the domain of certainty to the domain of uncertainty.
In fact, in the 20th century, Richard Cox and Edwin Thompson Jaynes have done early work to formalize probability theory as the ``logic under uncertainty''~\citep{cox2001algebra,jaynes2003probability}.

In this first chapter, we will briefly recall the fundamentals of probability theory, and we will see how \midx{probabilistic inference} can be used to reason about the world.
In the remaining chapters, we will then discuss how probabilistic inference can be performed efficiently given limited computational resources and limited time, which is the key challenge in \midx{probabilistic artificial intelligence}.

\section{Probability}\label{sec:fundamentals:probability}

Probability is commonly interpreted in two different ways.
In the frequentist interpretation, one interprets the probability of an event (say a coin coming up ``heads'' when flipping it) as the limit of relative frequencies in repeated independent experiments. That is, \begin{align*}
    \text{Probability} = \lim_{N \to \infty} \frac{\text{\# events happening in $N$ trials}}{N}.
\end{align*}
This interpretation is natural, but has a few issues.
It is not very difficult to conceive of settings where repeated experiments do not make sense.
Consider the outcome: \begin{center}
  ``Person X will live for at least 80 years.''
\end{center}
There is no way in which we could conduct multiple independent experiments in this case.
Still, this statement is going to turn out either true or false, as humans we are just not able to determine its truth value beforehand.
Nevertheless, humans commonly have \emph{beliefs} about statements of this kind.
We also commonly reason about statements such as \begin{center}
  ``The Beatles were more groundbreaking than The Monkees.''
\end{center}
This statement does not even have an objective truth value, and yet we as humans tend to have opinions about it.

While it is natural to consider the relative frequency of the outcome in repeated experiments as our belief, if we are not able to conduct repeated experiments, our notion of probability is simply a subjective measure of uncertainty about outcomes.
In the early 20th century, Bruno De Finetti has done foundational work to formalize this notion which is commonly called \midx{Bayesian reasoning} or the Bayesian interpretation of probability \citep{de2017theory}.

We will see that modern approaches to probabilistic inference often lend themselves to a Bayesian interpretation, even if such an interpretation is not strictly necessary.
For our purposes, probabilities will be a means to an end: the end usually being solving some task.
This task may be to make a prediction or to take an action with an uncertain outcome, and we can evaluate methods according to how well they perform on this task.
No matter the interpretation, the mathematical framework of probability theory which we will formally introduce in the following is the same.

\subsection{Probability Spaces}

A probability space is a mathematical model for a random experiment.
The set of all possible outcomes of the experiment $\Omega$ is called \midx{sample space}.
An \midx{event} $\sA \subseteq \Omega$ of interest may be any combination of possible outcomes.
The set of all events $\spA \subseteq \pset{\Omega}$ that we are interested in is often called the \midx{event space} of the experiment.\footnote{We use $\pset{\Omega}$ to denote the \emph{power set} (set of all subsets) of $\Omega$.}
This set of events is required to be a $\sigma$-algebra over the sample space.

\begin{defn}[$\sigma$-algebra]\pidx{$\sigma$-algebra}
  Given the set $\Omega$, the set $\spA \subseteq \pset{\Omega}$ is a \emph{$\sigma$-algebra} over $\Omega$ if the following properties are satisfied: \begin{enumerate}
    \item $\Omega \in \spA$;
    \item if $\sA \in \spA$, then $\compl{\sA} \in \spA$ (\emph{closedness under complements}); and
    \item if we have $\sA_i \in \spA$ for all $i$, then $\bigcup_{i=1}^\infty \sA_i \in \spA$ (\emph{closedness under countable unions}).
  \end{enumerate}
\end{defn}

Note that the three properties of $\sigma$-algebras correspond to characteristics we universally expect when working with random experiments.
Namely, that we are able to reason about the event $\Omega$ that any of the possible outcomes occur, that we are able to reason about an event not occurring, and that we are able to reason about events that are composed of multiple (smaller) events.

\begin{ex}{Event space of throwing a die}{}
  The event space $\spA$ can also be thought of as ``how much information is available about the experiment''.
  For example, if the experiment is a throw of a die and $\Omega$ is the set of possible values on the die: ${\Omega = \{1, \dots, 6\}}$, then the following $\spA$ implies that the observer cannot distinguish between $1$ and $3$: \begin{align*}
    \spA \defeq \{\emptyset, \Omega, \{1, 3, 5\}, \{2, 4, 6\}\}.
  \end{align*}
  Intuitively, the observer only understands the parity of the face of the die.
\end{ex}

\begin{defn}[Probability measure]\pidx{probability measure}
  Given the set $\Omega$ and the $\sigma$-algebra $\spA$ over $\Omega$, the function \begin{align*}
    \fnPr : \spA \to \R
  \end{align*} is a \emph{probability measure} on $\spA$ if the \midx{Kolmogorov axioms} are satisfied: \begin{enumerate}
    \item $0 \leq \Pr{A} \leq 1$ for any $A \in \spA$;
    \item $\Pr{\Omega} = 1$; and
    \item $\Pr{\bigcup_{i=1}^\infty \sA_i} = \sum_{i=1}^\infty \Pr{\sA_i}$ for any countable set of mutually disjoint events $\{\sA_i \in \spA\}_i$.\footnote{We say that a set of sets $\{\sA_i\}_i$ is disjoint if for all $i \neq j$ we have $\sA_i \cap \sA_j = \emptyset$.}
  \end{enumerate}
\end{defn}
Remarkably, all further statements about probability follow from these three natural axioms.
For an event $\sA \in \spA$, we call $\Pr{\sA}$ the \midx{probability} of $\sA$.
We are now ready to define a probability space.

\begin{defn}[Probability space]\pidx{probability space}
  A \idx{probability space} is a triple $(\Omega, \spA, \fnPr)$ where \begin{itemize}
    \item $\Omega$ is a sample space,
    \item $\spA$ is a $\sigma$-algebra over $\Omega$, and
    \item $\fnPr$ is a probability measure on $\spA$.
  \end{itemize}
\end{defn}

\begin{ex}{Borel $\sigma$-algebra over $\R$}{}
  In our context, we often have that $\Omega$ is the set of real numbers $\R$ or a compact subset of it.
  In this case, a natural event space is the $\sigma$-algebra generated by the set of events \begin{align*}
    \sA_x \defeq \{x '\in \Omega : x' \leq x\}.
  \end{align*}
  The smallest $\sigma$-algebra $\spA$ containing all sets $A_x$ is called the \midx{Borel $\sigma$-algebra}.
  $\spA$ contains all ``reasonable'' subsets of $\Omega$ (except for some pathological examples).
  For example, $\spA$ includes all singleton sets $\{x\}$, as well as all countable unions of intervals.

  In the case of discrete $\Omega$, in fact $\spA=\pset{\Omega}$, i.e., the Borel $\sigma$-algebra contains {\em all} subsets of $\Omega$.
\end{ex}

\subsection{Random Variables}

The set $\Omega$ is often rather complex.
For example, take $\Omega$ to be the set of all possible graphs on $n$ vertices.
Then the outcome of our experiment is a graph.
Usually, we are not interested in a specific graph but rather a property such as the number of edges, which is shared by many graphs.
A function that maps a graph to its number of edges is a random variable.

\begin{defn}[Random variable]\pidx{random variable}
  A \idx{random variable} $X$ is a function \begin{align*}
    X : \Omega \to \spT
  \end{align*} where $\spT$ is called \midx{target space} of the random variable,\footnote{For a random variable that maps a graph to its number of edges, $\spT = \NatZ$. For our purposes, you can generally assume $\spT \subseteq \R$.} and where $X$ respects the information available in the $\sigma$-algebra $\spA$. That is,\footnote{In our example of throwing a die, $X$ should assign the same value to the outcomes $1,3,5$.} \begin{align}
    \forall S \subseteq \spT :\quad \{\omega \in \Omega : X(\omega) \in S\} \in \spA.
  \end{align}
\end{defn}

Concrete values $x$ of a random variable $X$ are often referred to as \midx<states>{state of a random variable} or \midx<realizations>{realizations of a random variable} of $X$.
The probability that $X$ takes on a value in $S \subseteq \spT$ is \begin{align}
  \Pr{X \in S} = \Pr{\{\omega \in \Omega : X(\omega) \in S\}}.
\end{align}

\subsection{Distributions}

Consider a random variable $X$ on a probability space $(\Omega, \spA, \fnPr)$, where $\Omega$ is a compact subset of $\R$, and $\spA$ the Borel $\sigma$-algebra.

In this case, we can refer to the probability that $X$ assumes a particular state or set of states by writing
\begin{align}
  p_X(x) &\defeq \Pr{X = x} \quad \text{(in the discrete setting)}, \\
  P_X(x) &\defeq \Pr{X \leq x}.
\end{align}
Note that ``$X = x$'' and ``$X \leq x$'' are merely events (that is, they characterize subsets of the sample space $\Omega$ satisfying this condition) which are in the Borel $\sigma$-algebra, and hence their probability is well-defined.

Hereby, $p_X$ and $P_X$ are referred to as the probability mass function (PMF) and cumulative distribution function (CDF) of $X$, respectively.
Note that we can also \emph{implicitly} define probability spaces through random variables and their associated PMF/CDF, which is often very convenient.\looseness=-1

We list some common examples of discrete distributions in \cref{sec:background:probability:common_discrete_distributions}.
Further, note that for continuous variables, $\Pr{X=x}=0$.
Here, instead we typically use the probability density function (PDF), to which we (with slight abuse of notation) also refer with $p_X$. We discuss densities in greater detail in \cref{sec:fundamentals:continuous_distributions}.\looseness=-1

We call the subset $\sS \subseteq \spT$ of the domain of a PMF or PDF $p_X$ such that all elements $x \in \sS$ have positive probability, $p_X(x) > 0$, the \midx{support} of the distribution $p_X$. This quantity is denoted by $X(\Omega)$.

\subsection{Continuous Distributions}\label{sec:fundamentals:continuous_distributions}

As mentioned, a continuous random variable can be characterized by its \midx{probability density function} (PDF).
But what is a density? We can derive some intuition from physics.

Let $\set{M}$ be a (non-homogeneous) physical object, e.g., a rock.
We commonly use $m(\set{M})$ and $\mathrm{vol}(\set{M})$ to refer to its mass and volume, respectively. Now, consider for a point $\vx \in \set{M}$ and a ball $B_r(\vx)$ around $\vx$ with radius $r$ the following quantities: \begin{align*}
  \lim_{r\to0} \mathrm{vol}(B_r(\vx)) = 0 \quad \lim_{r\to0} m(B_r(\vx)) = 0.
\end{align*} They appear utterly uninteresting at first, yet, if we divide them, we get what is called the \midx{density} of $\set{M}$ at $\vx$.
\begin{align*}
  \lim_{r\to0} \frac{m(B_r(\vx))}{\mathrm{vol}(B_r(\vx))} \eqdef \rho(\vx).
\end{align*}
We know that the relationship between density and mass is described by the following formula: \begin{align*}
  m(\set{M}) = \int_\set{M} \rho(\vx) \,d\vx.
\end{align*}
In other words, the density is to be integrated. For a small region $\set{I}$ around $\vx$, we can approximate $m(\set{I}) \approx \rho(\vx) \cdot \mathrm{vol}(\set{I})$.

Crucially, observe that even though the mass of any particular point $\vx$ is zero, i.e., ${m(\{\vx\}) = 0}$, assigning a density $\rho(\vx)$ to $\vx$ is useful for integration and approximation.
The same idea applies to continuous random variables, only that volume corresponds to intervals on the real line and mass to probability.
Recall that probability density functions are normalized such that their probability mass across the entire real line integrates to one.

\begin{marginfigure}[10\baselineskip]
  \incplt{normal_pdf}
	\caption{PDF of the standard normal distribution. Observe that the PDF is symmetric around the mode.}
\end{marginfigure}

\begin{ex}{Normal distribution / Gaussian}{}\pidx{normal distribution}\pidx{Gaussian}%
  A famous example of a continuous distribution is the \emph{normal distribution}, also called \emph{Gaussian}.
  We say, a random variable $X$ is \emph{normally distributed}, ${X \sim \N{\mu}{\sigma^2}}$, if its PDF is \begin{align}
    \N[x]{\mu}{\sigma^2} \defeq \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right). \label{eq:univ_normal}
  \end{align}
  We have ${\E{X} = \mu}$ and ${\Var{X} = \sigma^2}$. If ${\mu = 0}$ and ${\sigma^2 = 1}$, this distribution is called the \midx{standard normal distribution}.
  The Gaussian CDF cannot be expressed in closed-form.

  Note that the mean of a Gaussian distribution coincides with the maximizer of its PDF, also called \midx{mode} of a distribution.
\end{ex}

We will focus in the remainder of this chapter on continuous distributions, but the concepts we discuss extend mostly to discrete distributions simply by ``replacing integrals by sums''.

\subsection{Joint Probability}

A joint probability (as opposed to a marginal probability) is the probability of two or more events occurring simultaneously: \begin{align}
  \Pr{\sA, \sB} \defeq \Pr{\sA \cap \sB}.
\end{align}
In terms of random variables, this concept extends to joint distributions.
Instead of characterizing a single random variable, a \midx{joint distribution} is a function $p_{\rX} : \R^n \to \R$, characterizing a \midx{random vector} $\rX \defeq \transpose{[X_1 \; \cdots \;X_n]}$.
For example, if the $X_i$ are discrete, the joint distribution characterizes joint probabilities of the form \begin{align*}
  \Pr{\rX = [x_1, \dots, x_n]} = \Pr{X_1 = x_1, \dots, X_n = x_n},
\end{align*} and hence describes the relationship among all variables $X_i$.
For this reason, a joint distribution is also called a \midx{generative model}.
We use $X_{i:j}$ to denote the random vector $\transpose{[X_i \; \cdots \; X_j]}$.

We can ``sum out'' (respectively ``integrate out'') variables from a joint distribution in a process called ``marginalization''\pidx{marginalization}:

\begin{thmb}
  \begin{fct}[Sum rule]\pidx{sum rule} We have that
    \begin{align}
      p(x_{1:i-1}, x_{i+1:n}) &= \int_{X_i(\Omega)} p(x_{1:i-1}, x_i, x_{i+1:n}) \,dx_i. \label{eq:sum_rule}
    \end{align}
  \end{fct}
\end{thmb}

\subsection{Conditional Probability}

Conditional probability updates the probability of an event $\sA$ given some new information, for example, after observing the event $\sB$.

\begin{defn}[Conditional probability]\pidx{conditional probability}
  Given two events $\sA$ and $\sB$ such that $\Pr{\sB} > 0$, the probability of $\sA$ conditioned on $\sB$ is given as \begin{align}
    \Pr{\sA \mid \sB} \defeq \frac{\Pr{\sA, \sB}}{\Pr{\sB}}. \label{eq:cond_prob}
  \end{align}
\end{defn}

\begin{marginfigure}
  \incfig{conditional_probability}
	\caption{Conditioning an event $\sA$ on another event $\sB$ can be understood as replacing the universe of all possible outcomes $\Omega$ by the observed outcomes $\sB$.
  Then, the conditional probability is simply expressing the likelihood of $\sA$ given that $\sB$ occurred.}
\end{marginfigure}

Simply rearranging the terms yields, \begin{align}
  \Pr{\sA, \sB} = \Pr{\sA \mid \sB} \cdot \Pr{\sB} = \Pr{\sB \mid \sA} \cdot \Pr{\sA}. \label{eq:simple_product_rule}
\end{align}
Thus, the probability that both $\sA$ and $\sB$ occur can be calculated by multiplying the probability of event $\sA$ and the probability of $\sB$ conditional on $\sA$ occurring.

We say $\rZ \sim \rX \mid \rY = \vy$ (or simply $\rZ \sim \rX \mid \vy$) if $\rZ$ follows the \midx{conditional distribution} \begin{align}
  p_{\rX\mid\rY}(\vx \mid \vy) \defeq \frac{p_{\rX,\rY}(\vx, \vy)}{p_\rY(\vy)}. \label{eq:cond_distr}
\end{align}
If $\rX$ and $\rY$ are discrete, we have that $p_{\rX \mid \rY}(\vx \mid \vy) = \Pr{\rX = \vx \mid \rY = \vy}$ as one would naturally expect.

Extending \cref{eq:simple_product_rule} to arbitrary random vectors yields the product rule (also called the \midx{chain rule of probability}):

\begin{thmb}
  \begin{fct}[Product rule]\pidx{product rule}
    Given random variables $X_{1:n}$, \begin{align}
      p(x_{1:n}) = p(x_1) \cdot \prod_{i=2}^n p(x_i \mid x_{1:i-1}). \label{eq:product_rule}
    \end{align}
  \end{fct}
\end{thmb}

Combining sum rule and product rule, we can compute marginal probabilities too: \begin{align}
  p(\vx) = \int_{\rY(\Omega)} p(\vx, \vy) \,d\vy = \int_{\rY(\Omega)} p(\vx \mid \vy) \cdot p(\vy) \,d\vy \label{eq:lotp} \margintag{first using the sum rule \eqref{eq:sum_rule} then the product rule \eqref{eq:product_rule}}
\end{align}
This is called the \midx{law of total probability} (LOTP), which is colloquially often referred to as \midx{conditioning} on $\rY$.
If it is difficult to compute $p(\vx)$ directly, conditioning can be a useful technique when $\rY$ is chosen such that the densities $p(\vx \mid \vy)$ and $p(\vy)$ are straightforward to understand.

\subsection{Independence}

Two random vectors $\rX$ and $\rY$ are \midx<independent>{independence} (denoted $\rX \perp \rY$) if and only if knowledge about the state of one random vector does not affect the distribution of the other random vector, namely if their conditional CDF (or in case they have a joint density, their conditional PDF) simplifies to \begin{align}
  P_{\rX\mid\rY}(\vx \mid \vy) = P_\rX(\vx),\quad p_{\rX\mid\rY}(\vx \mid \vy) = p_\rX(\vx).
\end{align}
For the conditional probabilities to be well-defined, we need to assume that $p_\rY(\vy) > 0$.

The more general characterization of independence is that $\rX$ and $\rY$ are independent if and only if their joint CDF (or in case they have a joint density, their joint PDF) can be decomposed as follows: \begin{align}
  P_{\rX,\rY}(\vx,\vy) = P_\rX(\vx) \cdot P_\rY(\vy), \quad p_{\rX,\rY}(\vx,\vy) = p_\rX(\vx) \cdot p_\rY(\vy).
\end{align}
The equivalence of the two characterizations (when $p_\rY(\vy) > 0$) is easily proven using the product rule: $p_{\rX,\rY}(\vx,\vy) = p_\rY(\vy) \cdot p_{\rX\mid\rY}(\vx \mid \vy)$.

A ``weaker'' notion of independence is conditional independence.\footnote{We discuss in \cref{rmk:common_causes} how ``weaker'' is to be interpreted in this context.}
Two random vectors $\rX$ and $\rY$ are \midx<conditionally independent>{conditional independence} given a random vector $\rZ$ (denoted $\rX \perp \rY \mid \rZ$) iff, given $\rZ$, knowledge about the value of one random vector $\rY$ does not affect the distribution of the other random vector $\rX$, namely if \begin{subequations}\begin{align}
  P_{\rX \mid \rY, \rZ}(\vx \mid \vy, \vz) &= P_{\rX \mid \rZ}(\vx \mid \vz),\\
  p_{\rX \mid \rY, \rZ}(\vx \mid \vy, \vz) &= p_{\rX \mid \rZ}(\vx \mid \vz).
\end{align}\end{subequations}
Similarly to independence, we have that $\rX$ and $\rY$ are conditionally independent given $\rZ$ if and only if their joint CDF or joint PDF can be decomposed as follows: \begin{subequations}\begin{align}
  P_{\rX,\rY \mid \rZ}(\vx,\vy \mid \vz) &= P_{\rX \mid \rZ}(\vx \mid \vz) \cdot P_{\rY \mid \rZ}(\vy \mid \vz), \\
  p_{\rX,\rY \mid \rZ}(\vx,\vy \mid \vz) &= p_{\rX \mid \rZ}(\vx \mid \vz) \cdot p_{\rY \mid \rZ}(\vy \mid \vz).
\end{align}\end{subequations}

\begin{rmk}{Common causes}{common_causes}
  How can conditional independence be understood as a ``weaker'' notion of independence?
  Clearly, conditional independence does not imply independence: a trivial example is $X \perp X \mid X \centernot\implies X \perp X$.\safefootnote{$X \perp X \mid X$ is true trivially.}
  Neither does independence imply conditional independence: for example, $X \perp Y \centernot\implies X \perp Y \mid X + Y$.\safefootnote{Knowing $X$ and $X + Y$ already implies the value of $Y$, and hence, $X \not\perp Y \mid X + Y$.}

  When we say that conditional independence is a weaker notion we mean to emphasize that $X$ and $Y$ can be ``made'' (conditionally) independent by conditioning on the ``right'' $Z$ even if $X$ and $Y$ are dependent.
  This is known as \midx{Reichenbach's common cause principle} which says that for any two random variables $X \not\perp Y$ there exists a random variable $Z$ (which may be $X$ or $Y$) that causally influences both $X$ and $Y$, and which is such that $X \perp Y \mid Z$.
\end{rmk}


\subsection{Directed Graphical Models}\pidx{directed graphical model}%

Directed graphical models (also called \midx{Bayesian networks}) are often used to visually denote the (conditional) independence relationships of a large number of random variables.
They are a schematic representation of the factorization of the generative model into a product of conditional distributions as a directed acyclic graph.
Given the sequence of random variables $\{X_i\}_{i=1}^n$, their generative model can be expressed as \begin{align}
  p(x_{1:n}) = \prod_{i=1}^n p(x_i \mid \mathrm{parents}(x_i)) \label{eq:directed_graphical_model}
\end{align} where $\mathrm{parents}(x_i)$ is the set of parents of the vertex $X_i$ in the directed graphical model.
In other words, the parenthood relationship encodes a conditional independence of a random variable $X$ with a random variable $Y$ given their parents:\footnote[][-5\baselineskip]{More generally, vertices $u$ and $v$ are conditionally independent given a set of vertices $\sZ$ if $\sZ$ \idx<d-separates>{d-separation} $u$ and $v$, which we will not cover in depth here.} \begin{align}
    X \perp Y \mid \mathrm{parents}(X), \mathrm{parents}(Y).
\end{align}
\Cref{eq:directed_graphical_model} simply uses the product rule and the conditional independence relationships to factorize the generative model.
This can greatly reduce the model's complexity, i.e., the length of the product.

\begin{marginfigure}[-2\baselineskip]
  \incfig{directed_graphical_model}
  \caption{Example of a directed graphical model. The random variables $X_1, \dots, X_n$ are mutually independent given the random variable $Y$. The squared rectangular nodes are used to represent dependencies on parameters $c, a_1, \dots, a_n$.}\label{fig:directed_graphical_model}
\end{marginfigure}

An example of a directed graphical model is given in \cref{fig:directed_graphical_model}.
Circular vertices represent random quantities (i.e., random variables).
In contrast, square vertices are commonly used to represent deterministic quantities (i.e., parameters that the distributions depend on).
In the given example, we have that $X_i$ is conditionally independent of all other $X_j$ given $Y$.
\midx<Plate notation>{plate notation} is a condensed notation used to represent repeated variables of a graphical model.
An example is given in \cref{fig:plate_notation}.\looseness=-1

\begin{marginfigure}
  \incfig{plate_notation}
  \caption{The same directed graphical model as in \cref{fig:directed_graphical_model} using plate notation.}
  \label{fig:plate_notation}
\end{marginfigure}

\subsection{Expectation}

The \midx<expected value>{expectation} or \midx{mean} $\E{\rX}$ of a random vector $\rX$ is the (asymptotic) arithmetic mean of an arbitrarily increasing number of independent realizations of $\rX$. That is,\footnote{In infinite probability spaces, absolute convergence of $\E{\rX}$ is necessary for the existence of $\E{\rX}$.} \begin{align}
  \E{\rX} &\defeq \int_{\rX(\Omega)} \vx \cdot p(\vx) \,d\vx \label{eq:expectation}
\end{align}
A very special and often used property of expectations is their \midx<linearity>{linearity of expectation}, namely that for any random vectors $\rX$ and $\rY$ in $\R^n$ and any ${\mA \in \R^{m \times n}}, {\vb \in \R^m}$ it holds that \begin{align}
  \E{\mA \rX + \vb} = \mA \E{\rX} + \vb \quad\text{and}\quad \E{\rX + \rY} = \E{\rX} + \E{\rY}. \label{eq:linearity_expectation}
\end{align}
Note that $\rX$ and $\rY$ do not necessarily have to be independent!
Further, if $\rX$ and $\rY$ are independent then \begin{align}
  \E{\rX\transpose{\rY}} = \E{\rX} \cdot \transpose{\E{\rY}}. \label{eq:expectation_product}
\end{align}

The following intuitive lemma can be used to compute expectations of transformed random variables.

\begin{thmb}
  \begin{fct}[Law of the unconscious statistician, LOTUS]\pidx{law of the unconscious statistician}
    \begin{align}
      \E{\vg(\rX)} = \int_{\rX(\Omega)} \vg(\vx) \cdot p(\vx) \,d\vx \label{eq:lotus}
    \end{align} where $\vg : \rX(\Omega) \to \R^n$ is a ``nice'' function\safefootnote{$\vg$ being a continuous function, which is either bounded or absolutely integrable (i.e., $\int \abs{g(\vx)} p(\vx) \, d\vx < \infty$), is sufficient. This is satisfied in most cases.} and $\rX$ is a continuous random vector.
    The analogous statement with a sum replacing the integral holds for discrete random variables.
  \end{fct}
\end{thmb}

This is a nontrivial fact that can be proven using the change of variables formula which we discuss in \cref{sec:fundamentals:probability:cov}.

Similarly to conditional probability, we can also define conditional expectations.
The expectation of a continuous random vector $\rX$ given that $\rY = \vy$ is defined as \begin{align}
  \E{\rX \mid \rY = \vy} \defeq \int_{\rX(\Omega)} \vx \cdot p_{\rX \mid \rY}(\vx \mid \vy) \,d\vx.
\end{align}
Observe that $\E{\rX \mid \rY = {\cdot}}$ defines a deterministic mapping from $\vy$ to $\E{\rX \mid \rY = \vy}$.
Therefore, $\E{\rX \mid \rY}$ is itself a random vector: \begin{align}
  \E{\rX \mid \rY}(\omega) = \E{\rX \mid \rY = \rY(\omega)}
\end{align} where $\omega \in \Omega$.
This random vector $\E{\rX \mid \rY}$ is called the \midx{conditional expectation} of $\rX$ given $\rY$.

Analogously to the law of total probability \eqref{eq:lotp}, one can condition an expectation on another random vector.
This is known as the \midx{tower rule} or the \midx{law of total expectation} (LOTE):

\begin{thmb}
  \begin{thm}[Tower rule]
    Given random vectors $\rX$ and $\rY$, we have \begin{align}
      \E[\rY]{\E[\rX]{\rX \mid \rY}} = \E{\rX}. \label{eq:tower_rule}
    \end{align}
  \end{thm}
\end{thmb}
\begin{proof}[Proof sketch]
  We only prove the case where $\rX$ and $\rY$ have a joint density.
  We have\looseness=-1 \begin{align*}
    \E{\E{\rX \mid \rY}} &= \int \left(\int \vx \cdot p(\vx \mid \vy) \,d\vx\right) p(\vy) \,d\vy \\
    &= \int \int \vx \cdot p(\vx, \vy) \,d\vx \,d\vy \margintag{by definition of conditional densities \eqref{eq:cond_distr}} \\
    &= \int \vx \int p(\vx, \vy) \,d\vy \,d\vx \margintag{by Fubini's theorem} \\
    &= \int \vx \cdot p(\vx) \,d\vx \margintag{using the sum rule \eqref{eq:sum_rule}} \\
    &= \E{\rX}. \qedhere
  \end{align*}
\end{proof}

\subsection{Covariance and Variance}\label{ssec:cov}

Given two random vectors $\rX$ in $\R^n$ and $\rY$ in $\R^m$, their \midx{covariance} is defined as \begin{align}
  \Cov{\rX, \rY} &\defeq \E{(\rX - \E{\rX})\transpose{(\rY - \E{\rY})}} \label{eq:covariance} \\
  &= \E{\rX \transpose{\rY}} - \E{\rX} \cdot \transpose{\E{\rY}} \\
  &= \transpose{\Cov{\rY, \rX}} \in \R^{n \times m}.
\end{align}
Covariance measures the linear dependence between two random vectors since a direct consequence of its definition \eqref{eq:covariance} is that given linear maps $\mA \in \R^{n' \times n}, \mB \in \R^{m' \times m}$, vectors $\vc \in \R^{n'}, \vd \in \R^{m'}$ and random vectors $\rX$ in $\R^n$ and $\rY$ in $\R^m$, we have that \begin{align}
  \Cov{\mA\rX + \vc, \mB\rY + \vd} = \mA\Cov{\rX,\rY}\transpose{\mB}. \label{eq:linear_map_covariance}
\end{align}
Two random vectors $\rX$ and $\rY$ are said to be \midx{uncorrelated} if and only if $\Cov{\rX, \rY} = \mzero$.
Note that if $\rX$ and $\rY$ are independent, then \cref{eq:expectation_product} implies that $\rX$ and $\rY$ are uncorrelated.
The reverse does not hold in general.

\begin{rmk}{Correlation}{correlation}
  The \idx{correlation} of the random vectors $\rX$ and $\rY$ is a normalized covariance, \begin{align}
    \Cor{\rX,\rY}(i,j) \defeq \frac{\Cov{X_i,Y_j}}{\sqrt{\Var{X_i} \Var{Y_j}}} \in [-1,1]. \label{eq:correlation}
  \end{align}
  Two random vectors $\rX$ and $\rY$ are therefore uncorrelated if and only if $\Cor{\rX,\rY} = \mzero$.

  There is also a nice geometric interpretation of covariance and correlation.
  For zero mean random variables $X$ and $Y$, $\Cov{X,Y}$ is an inner product.\safefootnote{That is, \begin{itemize}
    \item $\Cov{X,Y}$ is symmetric,
    \item $\Cov{X,Y}$ is linear (here we use $\E*{X} = \E*{Y} = 0$), and
    \item $\Cov{X,X} \geq 0$.
  \end{itemize}}

  The cosine of the angle $\theta$ between $X$ and $Y$ (that are not deterministic) coincides with their correlation, \begin{align}
    \cos\theta = \frac{\Cov{X,Y}}{\norm{X}\norm{Y}} = \Cor{X,Y}. \margintag{using the Euclidean inner product formula, $\Cov{X, Y} = \norm{X} \norm{Y} \cos\theta$}
  \end{align}
  $\cos\theta$ is also called a \midx{cosine similarity}.
  Thus, \begin{align}
    \theta = \arccos\,\Cor{X,Y}.
  \end{align}
  For example, if $X$ and $Y$ are uncorrelated, then they are orthogonal in the inner product space.
  If $\Cor{X, Y} = -1$ then $\theta \equiv \pi$ (that is, $X$ and $Y$ ``point in opposite directions''), whereas if $\Cor{X, Y} = 1$ then $\theta \equiv 0$ (that is, $X$ and $Y$ ``point in the same direction'').
\end{rmk}

The covariance of a random vector $\rX$ in $\R^n$ with itself is called its \midx{variance}: \begin{align}
  \Var{\rX} &\defeq \Cov{\rX, \rX} \\
  &= \E{(\rX - \E{\rX})\transpose{(\rX - \E{\rX})}} \label{eq:variance} \\
  &= \E{\rX \transpose{\rX}} - \E{\rX} \cdot \transpose{\E{\rX}} \label{eq:variance2} \\
  &= \begin{bmatrix}
    \Cov{X_1, X_1} & \cdots & \Cov{X_1, X_n} \\
    \vdots & \ddots & \vdots \\
    \Cov{X_n, X_1} & \cdots & \Cov{X_n, X_n} \\
  \end{bmatrix} \label{eq:covariance_matrix}.
\end{align}
The scalar variance $\Var{X}$ of a random variable $X$ is a measure of uncertainty about the value of $X$ since it measures the average squared deviation from $\E{X}$.
We will see that the eigenvalue spectrum of a covariance matrix can serve as a measure of uncertainty in the multivariate setting.\footnote{The \emph{multivariate} setting (as opposed to the \emph{univariate} setting) studies the joint distribution of multiple random variables.}

\begin{rmk}{Standard deviation}{}
  The length of a random variable $X$ in the inner product space described in \cref{rmk:correlation} is called its \midx{standard deviation}, \begin{align}
    \norm{X} = \sqrt{\Cov{X,X}} = \sqrt{\Var{X}} \eqdef \SD{X}.
  \end{align}
  That is, the longer a random variable is in the inner product space, the more ``uncertain'' we are about its value.
  If a random variable has length $0$, then it is deterministic.
\end{rmk}

The variance of a random vector $\rX$ is also called the \midx{covariance matrix} of $\rX$ and denoted by $\mSigma_{\rX}$ (or $\mSigma$ if the correspondence to $\rX$ is clear from context).
A covariance matrix is symmetric by definition due to the symmetry of covariance, and is always positive semi-definite \exerciserefmark{cov_mat_pos_sd}.

Two useful properties of variance are the following: \begin{itemize}
  \item It follows from \cref{eq:linear_map_covariance} that for any linear map $\mA \in \R^{m \times n}$ and vector $\vb \in \R^m$, \begin{align}
    \Var{\mA\rX + \vb} = \mA\Var{\rX}\transpose{\mA}. \label{eq:linear_map_variance}
  \end{align}
  In particular, $\Var{-\rX} = \Var{\rX}$.

  \item It follows from the definition of variance \eqref{eq:variance} that for any two random vectors $\rX$ and $\rY$, \begin{align}
    \Var{\rX + \rY} = \Var{\rX} + \Var{\rY} + 2 \Cov{\rX, \rY}. \label{eq:sum_variance}
  \end{align}
  In particular, if $\rX$ and $\rY$ are independent then the covariance term vanishes and $\Var{\rX + \rY} = \Var{\rX} + \Var{\rY}$.
\end{itemize}

Analogously to conditional probability and conditional expectation, we can also define conditional variance. The \midx{conditional variance} of a random vector $\rX$ given another random vector $\rY$ is the random vector \begin{align}
  \Var{\rX \mid \rY} \defeq \E{(\rX - \E{\rX \mid \rY}) \transpose{(\rX - \E{\rX \mid \rY})}}[\rY].
\end{align}
Intuitively, the conditional variance is the remaining variance when we use $\E{\rX \mid \rY}$ to predict $\rX$ rather than if we used $\E{\rX}$.
One can also condition a variance on another random vector, analogously to the laws of total probability \eqref{eq:lotp} and expectation \eqref{eq:tower_rule}.

\begin{thmb}
  \begin{thm}[Law of total variance, LOTV]\pidx{law of total variance}\label{thm:lotv}
    \begin{align}
      \Var{\rX} = \E[\rY]{\Var[\rX]{\rX \mid \rY}} + \Var[\rY]{\E[\rX]{\rX \mid \rY}}. \label{eq:lotv}
    \end{align}
  \end{thm}
\end{thmb}

Here, the first term measures the average deviation from the mean of $\rX$ across realizations of $\rY$ and the second term measures the uncertainty in the mean of $\rX$ across realizations of $\rY$.
In \cref{sec:blr:uncertainty}, we will see that both terms have a meaningful characterization in the context of probabilistic inference.

\vspace{-10pt}\begin{proof}[Proof sketch of LOTV]
  To simplify the notation, we present only a proof for the univariate setting.
  \begin{align*}
    \Var{X} &= \E{X^2} - \E{X}^2 \\
    &= \E{\E{X^2 \mid Y}} - \E{\E{X \mid Y}}^2 \margintag{by the tower rule \eqref{eq:tower_rule}} \\
    &= \E{\Var{X \mid Y} + \E{X \mid Y}^2} - \E{\E{X \mid Y}}^2 \margintag{by the definition of variance \eqref{eq:variance2}} \\
    &= \E{\Var{X \mid Y}} + \parentheses*{\E{\E{X \mid Y}^2} - \E{\E{X \mid Y}}^2} \\
    &= \E{\Var{X \mid Y}} + \Var{\E{X \mid Y}}. \qedhere \margintag{by the definition of variance \eqref{eq:variance2}}
  \end{align*}
\end{proof}

\subsection{Change of Variables}\label{sec:fundamentals:probability:cov}

It is often useful to understand the distribution of a transformed random variable $Y = g(X)$ that is defined in terms of a random variable $X$, whose distribution is known.
Let us first consider the univariate setting. We would like to express the distribution of $Y$ in terms of the distribution of $X$, that is, we would like to find \begin{align}
    P_Y(y) = \Pr{Y \leq y} = \Pr{g(X) \leq y} = \Pr{X \leq \inv{g}(y)}.
\end{align}
When the random variables are continuous, this probability can be expressed as an integration over the domain of $X$.
We can then use the substitution rule of integration to ``change the variables'' to an integration over the domain of $Y$.
Taking the derivative yields the density $p_Y$.\footnote{The full proof of the change of variables formula in the univariate setting can be found in section 6.7.2 of \icite{mml}.}
There is an analogous change of variables formula for the multivariate setting.

\begin{thmb}
  \begin{fct}[Change of variables formula]\pidx{change of variables formula}
    Let $\rX$ be a random vector in $\R^n$ with density $p_\rX$ and let $\vg : \R^n \to \R^n$ be a differentiable and invertible function.
    Then $\rY = \vg(\rX)$ is another random variable, whose density can be computed based on $p_\rX$ and $\vg$ as follows: \begin{align}
      p_\rY(\vy) = p_\rX(\inv{\vg}(\vy)) \cdot \abs{\det{\jac \inv{\vg}(\vy)}} \label{eq:change_of_variables}
    \end{align} where $\jac \inv{\vg}(\vy)$ is the Jacobian of $\inv{\vg}$ evaluated at $\vy$.
  \end{fct}
\end{thmb}

Here, the term $\abs{\det{\jac \inv{\vg}(\vy)}}$ measures how much a unit volume changes when applying $\vg$.
Intuitively, the change of variables swaps the coordinate system over which we integrate.
The factor $\abs{\det{\jac \inv{\vg}(\vy)}}$ corrects for the change in volume that is caused by this change in coordinates.

Intuitively, you can think of the vector field $\vg$ as a perturbation to $\rX$, ``pushing'' the probability mass around.
The perturbation of a density $p_\rX$ by $\vg$ is commonly denoted by the \midx{pushforward} \begin{align}
  \pf{\vg}{p_\rX} \defeq p_\rY \quad\text{where $\rY = \vg(\rX)$}.
\end{align}
This concludes our quick tour of probability theory, and we are well-prepared to return to the topic of probabilistic inference.

\section{Probabilistic Inference}

Recall the logical implication ``If it is raining, the ground is wet.'' from the beginning of this chapter.
Suppose that we look outside a window and see that it is not raining: will the ground be dry?
Logical reasoning does not permit drawing an inference of this kind, as there might be reasons other than rain for which the ground could be wet (e.g., sprinklers).
However, intuitively, by observing that it is not raining, we have just excluded the possibility that the ground is wet because of rain, and therefore we would deem it ``more likely'' that the ground is dry than before.
In other words, if we were to walk outside now and the ground was wet, we would be more surprised than we would have been if we had not looked outside the window before.

As humans, we are constantly making such ``plausible'' inferences of our beliefs: be it about the weather, the outcomes of our daily decisions, or the behavior of others.
\midx<Probabilistic inference>{probabilistic inference}[idxpagebf] is the process of updating such a prior belief $\Pr{\overline{W}}$ to a posterior belief $\Pr{\overline{W} \mid \overline{R}}$ upon observing $\overline{R}$ where --- to reduce clutter --- we write $W$ for ``The ground is wet'' and $R$ for ``It is raining''.

The central principle of probabilistic inference is Bayes' rule:

\begin{thmb}
  \begin{thm}[Bayes' rule]\pidx{Bayes' rule}
    Given random vectors $\rX$ in $\R^n$ and $\rY$ in $\R^m$, we have for any $\vx \in \R^n, \vy \in \R^m$ that
    \begin{align}
      p(\vx \mid \vy) = \frac{p(\vy \mid \vx) \cdot p(\vx)}{p(\vy)}. \label{eq:bayes_rule}
    \end{align}
  \end{thm}
\end{thmb}
\begin{proof}
  Bayes' rule is a direct consequence of the definition of conditional densities \eqref{eq:cond_distr} and the product rule \eqref{eq:product_rule}.
\end{proof}

Let us consider the meaning of each term separately: \begin{itemize}
  \item the \midx{prior} $p(\vx)$ is the initial belief about $\vx$,
  \item the \midx<(conditional) likelihood>{likelihood}\pidx{conditional likelihood} $p(\vy \mid \vx)$ describes how likely the observations $\vy$ are under a given value $\vx$,
  \item the \midx{posterior} $p(\vx \mid \vy)$ is the updated belief about $\vx$ after observing $\vy$,
  \item the \midx{joint likelihood} $p(\vx, \vy) = p(\vy \mid \vx) p(\vx)$ combines prior and likelihood,
  \item the \midx{marginal likelihood} $p(\vy)$ describes how likely the observations $\vy$ are across all values of $\vx$.
\end{itemize}
The marginal likelihood can be computed using the sum rule \eqref{eq:sum_rule} or the law of total probability \eqref{eq:lotp}, \begin{align}
  p(\vy) = \int_{\rX(\Omega)} p(\vy \mid \vx) \cdot p(\vx) \,d\vx. \label{eq:marginal_likelihood}
\end{align}
Note, however, that the marginal likelihood is simply normalizing the conditional distribution to integrate to one, and therefore a constant with respect to $\vx$.
For this reason, $p(\vy)$ is commonly called the \midx{normalizing constant}.

\begin{ex}{Plausible inferences}{weak_probabilistic_inference}
  Let us confirm our intuition from the above example.
  The logical implication ``If it is raining, the ground is wet.'' (denoted $R \rightarrow W$) can be succinctly expressed as $\Pr{W \mid R} = 1$.
  Since $\Pr{W} \leq 1$, we know that \begin{align*}
    \Pr{R \mid W} = \frac{\Pr{W \mid R} \cdot \Pr{R}}{\Pr{W}} = \frac{\Pr{R}}{\Pr{W}} \geq \Pr{R}.
  \end{align*}
  That is, observing that the ground is wet makes it more likely to be raining.
  From $\Pr{R \mid W} \geq \Pr{R}$ we know $\Pr{\overline{R} \mid W} \leq \Pr{\overline{R}}$,\safefootnote{since $\Pr{\overline{X}} = 1 - \Pr{X}$} which leads us to follow that \begin{align*}
    \Pr{W \mid \overline{R}} = \frac{\Pr{\overline{R} \mid W} \cdot \Pr{W}}{\Pr{\overline{R}}} \leq \Pr{W},
  \end{align*} that is, having observed it not to be raining made the ground less likely to be wet.
\end{ex}

\Cref{ex:weak_probabilistic_inference} is called a \midx{plausible inference} because the observation of $\overline{R}$ does not completely determine the truth value of $\overline{W}$, and hence, does not permit logical inference.
In the case, however, that logical inference is permitted, it coincides with probabilistic inference.

\begin{ex}{Logical inferences}{strong_inference}
  For example, if we were to observe that the ground is not wet, then logical inference implies that it must not be raining: $\overline{W} \rightarrow \overline{R}$.
  This is called the \midx{contrapositive} of $R \rightarrow W$.

  Indeed, by probabilistic inference, we obtain analogously \begin{align*}
    \Pr{R \mid \overline{W}} = \frac{\Pr{\overline{W} \mid R} \cdot \Pr{R}}{\Pr{\overline{W}}} = \frac{(1 - \Pr{W \mid R}) \cdot \Pr{R}}{\Pr{\overline{W}}} = 0. \margintag{as $\Pr{W \mid R} = 1$}
  \end{align*}
\end{ex}

Observe that a logical inference does not depend on the prior $\Pr{R}$:
Even if the prior was $\Pr{R} = 1$ in \cref{ex:strong_inference}, after observing that the ground is not wet, we are forced to conclude that it is not raining to maintain logical consistency.
The examples highlight that while \emph{logical} inference does not require the notion of a prior, plausible (\emph{probabilistic}!) inference does.

\subsection{Where do priors come from?}\label{sec:fundamentals:inference:priors}

Bayes' rule necessitates the specification of a prior $p(\vx)$.
Different priors can lead to the deduction of dramatically different posteriors, as one can easily see by considering the extreme cases of a prior that is a point density at $\vx = \vx_0$ and a prior that is ``uniform'' over $\R^n$.\footnote{The latter is not a valid probability distribution, but we can still derive meaning from the posterior as we discuss in \cref{rmk:improper_priors}.}
In the former case, the posterior will be a point density at $\vx_0$ regardless of the likelihood.
In other words, no evidence can alter the ``prior belief'' the learner ascribed to $\vx$.
In the latter case, the learner has ``no prior belief'', and therefore the posterior will be proportional to the likelihood.
Both steps of probabilistic inference are perfectly valid, though one might debate which prior is more reasonable.

Someone who follows the Bayesian interpretation of probability might argue that everything is conditional, meaning that the prior is simply a posterior of all former observations.
While this might seem natural (``my world view from today is the combination of my world view from yesterday and the observations I made today''), this lacks an explanation for ``the first day''.
Someone else who is more inclined towards the frequentist interpretation might also object to the existence of a prior belief altogether, arguing that a prior is \emph{subjective} and therefore not a valid or desirable input to a learning algorithm.
Put differently, a frequentist ``has the belief not to have any belief''.
This is perfectly compatible with probabilistic inference, as long as the prior is chosen to be \midx<noninformative>{noninformative prior}\pidx{informative prior}:\begin{align}
  p(\vx) \propto \const.
\end{align}
Choosing a noninformative prior in the absence of any evidence is known as the \midx{principle of indifference} or the \midx{principle of insufficient reason}, which dates back to the famous mathematician Pierre-Simon Laplace.

\begin{ex}{Why be indifferent?}{}
  Consider a criminal trial with three suspects, A, B, and C.
  The collected evidence shows that suspect C can not have committed the crime, however it does not yield any information about suspects A and B.
  Clearly, any distribution respecting the data must assign zero probability of having committed the crime to suspect C.
  However, any distribution interpolating between $(1,0,0)$ and $(0,1,0)$ respects the data.
  The principle of indifference suggests that the desired distribution is $(\frac{1}{2}, \frac{1}{2}, 0)$, and indeed, any alternative distribution seems unreasonable.
\end{ex}

\begin{rmk}{Noninformative and improper priors}{improper_priors}
  It is not necessarily required that the prior $p(\vx)$ is a valid distribution (i.e., integrates to $1$).
  Consider for example, the noninformative prior $p(\vx) \propto \Ind{\vx \in I}$ where $I \subseteq \R^n$ is an infinitely large interval.
  Such a prior which is not a valid distribution is called an \midx{improper prior}.
  We can still derive meaning from the posterior of a given likelihood and (improper) prior as long as the posterior is a valid distribution.
\end{rmk}

Laplace's principle of indifference can be generalized to cases where \emph{some} evidence is available.
The \midx{maximum entropy principle}, originally proposed by \cite{jaynes1968prior}, states that one should choose as prior from all possible distributions that are \emph{consistent} with prior knowledge, the one that makes the \emph{least} ``additional assumptions'', i.e., is the least ``informative''.
In philosophy, this principle is known as \midx{Occam's razor} or the \midx{principle of parsimony}.
The ``informativeness'' of a distribution $p$ is quantified by its \idx{entropy} which is defined as \begin{align}
  \H{p} \defeq \E[\vx \sim p]{- \log p(\vx)}. \label{eq:mep_entropy}
\end{align}
The more concentrated $p$ is, the less is its entropy; the more diffuse $p$ is, the greater is its entropy.\footnote{We give a thorough introduction to entropy in \cref{sec:approximate_inference:information_theory}.}

In the absence of any prior knowledge, the uniform distribution has the highest entropy,\footnote{This only holds true when the set of possible outcomes of $\vx$ finite (or a bounded continuous interval), as in this case, the noninformative prior is a proper distribution --- the uniform distribution. In the ``infinite case'', there is no uniform distribution and the noninformative prior can be attained from the maximum entropy principle as the limiting solution as the number of possible outcomes of $\vx$ is increased.} and hence, the maximum entropy principle suggests a noninformative prior (as does Laplace's principle of indifference).
In contrast, if the evidence perfectly determines the value of~$\vx$, then the only consistent explanation is the point density at~$\vx$.
The maximum entropy principle characterizes a reasonable choice of prior for these two extreme cases and all cases in between.
Bayes' rule can in fact be derived as a consequence of the maximum entropy principle in the sense that the posterior is the least ``informative'' distribution among all distributions that are consistent with the prior and the observations \exerciserefmark{mep_and_posteriors}.

\subsection{Conjugate Priors}\label{sec:fundamentals:bayesian:conjugacy}

If the prior $p(\vx)$ and posterior $p(\vx \mid \vy)$ are of the same family of distributions, the prior is called a \midx{conjugate prior} to the likelihood $p(\vy \mid \vx)$.
This is a very desirable property, as it allows us to recursively apply the same learning algorithm implementing probabilistic inference.
We will see in \cref{sec:blr} that under some conditions the Gaussian is \midx<self-conjugate>{self-conjugacy}.
That is, if we have a Gaussian prior and a Gaussian likelihood then our posterior will also be Gaussian.
This will provide us with the first \emph{efficient} implementation of probabilistic inference.

\begin{ex}{Conjugacy of beta and binomial distribution}{}
  As an example for conjugacy, we will show that the beta distribution is a conjugate prior to a binomial likelihood.
  Recall the PMF of the binomial distribution \begin{align}
    \Bin[k]{n}{\theta} = {n \choose k} \theta^k (1-\theta)^{n-k} \label{eq:binomial_distr}
  \end{align} and the PDF of the beta distribution, \begin{align}
    \Beta[\theta]{\alpha}{\beta} \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}, \label{eq:beta_distr}
  \end{align}
  We assume the prior ${\theta \sim \Beta{\alpha}{\beta}}$ and likelihood ${k \mid \theta \sim \Bin{n}{\theta}}$.
  Let $n_H = k$ be the number of heads and $n_T = n - k$ the number of tails in the binomial trial $k$.
  Then, \begin{align*}
    p(\theta \mid k) &\propto p(k \mid \theta) p(\theta) \margintag{using Bayes' rule \eqref{eq:bayes_rule}} \\
    &\propto \theta^{n_H} (1-\theta)^{n_T} \theta^{\alpha-1} (1-\theta)^{\beta-1} \\
    &= \theta^{\alpha+n_H-1} (1-\theta)^{\beta+n_T-1}.
  \end{align*}
  Thus, $\theta \mid k \sim \Beta{\alpha + n_H}{\beta + n_T}$.

  This same conjugacy can be shown for the multivariate generalization of the beta distribution, the \midx{Dirichlet distribution}, and the multivariate generalization of the binomial distribution, the \midx{multinomial distribution}.
\end{ex}

\subsection{Tractable Inference with the Normal Distribution}\label{sec:fundamentals:gaussians}

Using arbitrary distributions for learning and inference is computationally very expensive when the number of dimensions is large --- even in the discrete setting.
For example, computing marginal distributions using the sum rule yields an exponentially long sum in the size of the random vector.
Similarly, the normalizing constant of the conditional distribution is a sum of exponential length.
Even to represent any discrete joint probability distribution requires space that is exponential in the number of dimensions (cf. \cref{tab:joint_distribution}).

\begin{marginfigure}
  \begin{tabular}{ccccr}
    \toprule
    $X_1$ & $\cdots$ & $X_{n-1}$ & $X_n$ & $\Pr{X_{1:n}}$ \\
    \midrule
    $0$ & $\cdots$ & $0$ & $0$ & $0.01$ \\
    $0$ & $\cdots$ & $0$ & $1$ & $0.001$ \\
    $0$ & $\cdots$ & $1$ & $0$ & $0.213$ \\
    $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & \\
    $1$ & $\cdots$ & $1$ & $1$ & $0.0003$ \\
    \bottomrule
  \end{tabular}\vspace{1em}
  \caption{A table representing a joint distribution of $n$ binary random variables. The table has $2^n$ rows. The number of parameters is $2^n - 1$ since the final probability is determined by all other probabilities as they must sum to one.}\label{tab:joint_distribution}
\end{marginfigure}

One strategy to get around this computational blowup is to restrict the class of distributions.
Gaussians are a popular choice for this purpose since they have extremely useful properties: they have a compact representation and --- as we will see in \cref{sec:blr} --- they allow for closed-form probabilistic inference.

In \cref{eq:univ_normal}, we have already seen the PDF of the univariate Gaussian distribution.
A random vector $\rX$ in $\R^n$ is \emph{normally distributed}, $\rX~\sim~\N{\vmu}{\mSigma}$, if its PDF is \begin{thmb}\begin{align}\pidx{normal distribution}[idxpagebf]\pidx{Gaussian}[idxpagebf]
  \N[\vx]{\vmu}{\mSigma} \defeq \frac{1}{\sqrt{\det{2\pi\mSigma}}} \exp\parentheses*{-\frac{1}{2}\transpose{(\vx-\vmu)} \inv{\mSigma} (\vx-\vmu)} \label{eq:normal}
\end{align}\end{thmb} where $\vmu \in \R^n$ is the mean vector and $\mSigma \in \R^{n \times n}$ the covariance matrix \exerciserefmark{expectation_and_variance_of_gaussians}.
We call $\mLambda \defeq \inv{\mSigma}$ the \midx{precision matrix}.
$\rX$ is also called a \midx{Gaussian random vector} (GRV).
$\SN$ is the multivariate \midx{standard normal distribution}[idxpagebf].
We call a Gaussian \midx<isotropic>{isotropic Gaussian} if its covariance matrix is of the form $\mSigma~=~\sigma^2\mI$ for some $\sigma^2 \in \R$.
In this case, the sublevel sets of the PDF are perfect spheres as can be seen in \cref{fig:multivariate_normal}.

\begin{figure}
  \incplt{multivariate_gaussian}
  \caption{Shown are the PDFs of two-dimensional Gaussians with mean $\vzero$ and covariance matrices \begin{equation*}
    \mSigma_1 \defeq \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}, \quad \mSigma_2 \defeq \begin{bmatrix}
      1 & 0.9 \\
      0.9 & 1
    \end{bmatrix}
  \end{equation*} respectively.}\label{fig:multivariate_normal}
\end{figure}

Note that a Gaussian can be represented using only $\BigO{n^2}$ parameters.
In the case of a diagonal covariance matrix, which corresponds to $n$ independent univariate Gaussians \exerciserefmark{grv_uncor_indep}, we just need $\BigO{n}$ parameters.

In \cref{eq:normal}, we assume that the covariance matrix $\mSigma$ is invertible, i.e., does not have the eigenvalue $0$.
This is not a restriction since it can be shown that a covariance matrix has a zero eigenvalue if and only if there exists a deterministic linear relationship between some variables in the joint distribution \exerciserefmark{zero_ev_of_cov_mats}.
As we have already seen that a covariance matrix does not have negative eigenvalues \exerciserefmark{cov_mat_pos_sd}, this ensures that $\mSigma$ and~$\mLambda$ are positive definite.\footnote{The inverse of a positive definite matrix is also positive definite.}

An important property of the normal distribution is that it is closed under marginalization and conditioning.

\begin{thmb}
  \begin{thm}[Marginal and conditional distribution]\label{fct:marginal_and_cond_gaussian}\exerciserefmark{marginal_and_cond_gaussian}
    Consider the Gaussian random vector $\rX$ and fix index sets $\sA \subseteq [n]$ and $\sB \subseteq [n]$.
    Then, we have that for any such \emph{marginal distribution},
    \begin{align}
      \rX_A \sim \N{\vmu_\sA}{\mSigma_{\sA\sA}}, \margintag{\normalfont By $\vmu_\sA$ we denote $[\mu_{i_1}, \dots, \mu_{i_k}]$ where $\sA = \{i_1, \dots i_k\}$. $\mSigma_{\sA\sA}$ is defined analogously.}
    \end{align} and that for any such \emph{conditional distribution},
    \begin{subequations}\begin{align}
      \rX_\sA \mid \rX_\sB &= \vx_\sB \sim \N{\vmu_{\sA \mid \sB}}{\mSigma_{\sA \mid \sB}} \quad\text{where} \\
      \vmu_{\sA \mid \sB} &\defeq \vmu_\sA + \mSigma_{\sA\sB}\inv{\mSigma_{\sB\sB}}(\vx_\sB - \vmu_\sB), \margintag{\normalfont Here, $\vmu_\sA$ characterizes the prior belief and \smash{$\mSigma_{\sA\sB}\inv{\mSigma_{\sB\sB}}(\vx_\sB - \vmu_\sB)$} represents ``how different'' $\vx_\sB$ is from what was expected.} \\
      \mSigma_{\sA \mid \sB} &\defeq \mSigma_{\sA\sA} - \mSigma_{\sA\sB}\inv{\mSigma_{\sB\sB}}\mSigma_{\sB\sA}.
    \end{align}\label{eq:cond_gaussian}\end{subequations}
  \end{thm}\vspace{-0.5cm}
\end{thmb}

\Cref{fct:marginal_and_cond_gaussian} provides a closed-form characterization of probabilistic inference for the case that random variables are jointly Gaussian.
We will discuss in \cref{sec:blr}, how this can be turned into an efficient inference algorithm.

Observe that upon inference, the variance can only shrink! Moreover, how much the variance is reduced depends purely on \emph{where} the observations are made (i.e., the choice of $\sB$) but not on \emph{what} the observations are.
In contrast, the posterior mean $\vmu_{\sA \mid \sB}$ depends affinely on $\vmu_\sB$.
These are special properties of the Gaussian and do not generally hold true for other distributions.

It can be shown that Gaussians are additive and closed under affine transformations \exerciserefmark{gaussian_closedness}.
The closedness under affine transformations \eqref{eq:gaussian_lin_trans} implies that a Gaussian $\rX \sim \N{\vmu}{\mSigma}$ is equivalently characterized as \begin{align}
  \rX = \msqrt{\mSigma} \rY + \vmu. \label{eq:gaussian_affine_transformation}
\end{align} where $\rY \sim \SN$ and $\msqrt{\mSigma}$ is the square root of $\mSigma$.\footnote{More details on the square root of a symmetric and positive definite matrix can be found in \cref{sec:fundamentals:qf}.}
Importantly, this implies together with \cref{fct:marginal_and_cond_gaussian} and additivity \eqref{eq:gaussian_additivity} that: \begin{center}
  \emph{Any affine transformation of a Gaussian random vector \\ is a Gaussian random vector.}
\end{center}
A consequence of this is that given any jointly Gaussian random vectors $\rX_\sA$ and $\rX_\sB$, $\rX_\sA$ can be expressed as an affine function of $\rX_\sB$ with added independent Gaussian noise.
Formally, we define \begin{subequations}\begin{align}
  \rX_\sA &\defeq \mA \rX_\sB + \vb + \vvarepsilon \quad\text{where} \\
  \mA &\defeq \mSigma_{\sA\sB}\inv{\mSigma_{\sB\sB}}, \\
  \vb &\defeq \vmu_A - \mSigma_{\sA\sB}\inv{\mSigma_{\sB\sB}} \vmu_B, \\
  \vvarepsilon &\sim \N{\vzero}{\mSigma_{\sA\mid\sB}}.
\end{align}\label{eq:cond_linear_gaussian}\end{subequations}
It directly follows from the closedness of Gaussians under affine transformations \eqref{eq:gaussian_lin_trans} that the characterization of $\rX_\sA$ via \cref{eq:cond_linear_gaussian} is equivalent to $\rX_\sA \sim \N{\vmu_\sA}{\mSigma_{\sA\sA}}$, and hence, \emph{any} Gaussian $\rX_\sA$ can be modeled as a so-called \midx{conditional linear Gaussian}, i.e., an affine function of another Gaussian $\rX_\sB$ with additional independent Gaussian noise.
We will use this fact frequently to represent Gaussians in a compact form.

\section{Supervised Learning and Point Estimates}\label{sec:fundamentals:supervised_learning}

Throughout the first part of this manuscript, we will focus mostly on the \midx{supervised learning}[idxpagebf] problem where we want to learn a function \begin{align*}
  \opt{f} : \spX \to \spY
\end{align*} from labeled training data.
That is, we are given a collection of labeled examples, $\spD_n \defeq \{(\vx_i, y_i)\}_{i=1}^n$, where the $\vx_i \in \spX$ are \midx{inputs} and the $y_i \in \spY$ are \midx{outputs} (called \midx{labels}), and we want to find a function $\hat{f}$ that best-approximates $\opt{f}$.
It is common to choose $\hat{f}$ from a parameterized \midx{function class} $\spF(\Theta)$, where each function $f_\vtheta$ is described by some parameters $\vtheta \in \Theta$.

\begin{marginfigure}
  \incfig{estimation_approximation_error}
  \caption{Illustration of \textbf{estimation error} and \textbf{\b{approximation error}}. $\fs$ denotes the true function and $\hat{f}$ is the best approximation from the function class $\spF$. We do not specify here, how one could quantify ``error''. For more details, see \cref{sec:fundamentals:supervised_learning:risk}.}\label{fig:estimation_approximation_error}
\end{marginfigure}

\begin{rmk}{What this manuscript is about and not about}{}
  As illustrated in \cref{fig:estimation_approximation_error}, the restriction to a function class leads to two sources of error: the \midx{estimation error} of having ``incorrectly'' determined $\hat{f}$ within the function class, and the \midx{approximation error} of the function class itself.
  Choosing a ``good'' function class / architecture with small approximation error is therefore critical for any practical application of machine learning.
  We will discuss various function classes, from linear models to deep neural networks, however, determining the ``right'' function class will not be the focus of this manuscript.
  To keep the exposition simple, we will assume in the following that $\fs \in \spF(\Theta)$ with parameters $\opt{\vtheta} \in \Theta$.

  Instead, we will focus on the problem of estimation/inference within a given function class.
  We will see that inference in smaller function classes is often more computationally efficient since the search space is smaller or --- in the case of Gaussians --- has a known tractable structure.
  On the other hand, larger function classes are more expressive and therefore can typically better approximate the ground truth $\opt{f}$.
\end{rmk}

We differentiate between the task of \midx{regression} where ${\spY \defeq \R^k}$,\footnote{The labels are usually scalar, so ${k = 1}$.} and the task of \midx{classification} where ${\spY \defeq \spC}$ and $\spC$ is an $m$-element set of classes.
In other words, regression is the task of predicting a continuous label, whereas classification is the task of predicting a discrete class label.
These two tasks are intimately related: in fact, we can think of classification tasks as a regression problem where we learn a probability distribution over class labels.
In this regression problem, ${\spY \defeq \Delta^{\spC}}$ where $\Delta^{\spC}$ denotes the set of all probability distributions over the set of classes $\spC$ which is an ${(m-1)}$-dimensional convex polytope in the $m$-dimensional space of probabilities $[0,1]^m$ (cf. \cref{sec:background:probability:probability_simplex}).

For now, let us stick to the regression setting.
We will assume that the observations are noisy, that is, $y_i \iid p(\cdot \mid \vx_i, \opt{\vtheta})$ for some \emph{known} conditional distribution $p(\cdot \mid \vx_i, \vtheta)$ but \emph{unknown} parameter $\opt{\vtheta}$.\footnote{The case where the labels are deterministic is the special case of $p(\cdot \mid \vx_i, \opt{\vtheta})$ being a point density at $\fs(\vx_i)$.}
Our assumption can equivalently be formulated as \begin{align}
  y_i = \underbrace{f_\vtheta(\vx_i)}_{\text{signal}} + \underbrace{\varepsilon_i(\vx_i)}_{\text{noise}} \label{eq:data}
\end{align} where $f_\vtheta(\vx_i)$ is the mean of $p(\cdot \mid \vx_i, \vtheta)$ and $\varepsilon_i(\vx_i) = y_i - f_\vtheta(\vx_i)$ is some independent zero-mean noise, for example (but not necessarily) Gaussian.\footnote{It is crucial that the assumed noise distribution accurately reflects the noise of the data. For example, using a (light-tailed) Gaussian noise model in the presence of heavy-tailed noise will fail! We discuss the distinction between light and heavy tails in \cref{sec:fundamentals:parameter_esitmation:heavy_tails}.}
When the noise distribution may depend on $\vx_i$, the noise is said to be \midx{heteroscedastic}[idxpagebf] and otherwise the noise is called \midx{homoscedastic}[idxpagebf].

\subsection{Maximum Likelihood Estimation}\label{sec:fundamentals:parameter_estimation:mle}

A common approach to finding $\hat{f}$ is to select the model $f \in \spF(\Theta)$ under which the training data is most likely.
This is called the \midx{maximum likelihood estimate} (or MLE): \begin{align}
  \vthetahat_\MLE &\defeq \argmax_{\vtheta \in \Theta} p(y_{1:n} \mid \vx_{1:n}, \vtheta) \label{eq:mle} \\
  &= \argmax_{\vtheta \in \Theta} \prod_{i=1}^n p(y_i \mid \vx_i, \vtheta). \margintag{using the independence of the training data \eqref{eq:data}} \nonumber
  \intertext{Such products of probabilities are often numerically unstable, which is why one typically takes the logarithm:}
  &= \argmax_{\vtheta \in \Theta} \underbrace{\sum_{i=1}^n \log p(y_i \mid \vx_i, \vtheta)}_{\text{\midx{log-likelihood}}}.
\end{align}
We will denote the \midx{negative log-likelihood} by $\ell_\mathrm{nll}(\vtheta; \spD_n)$.

The MLE is often used in practice due to its desirable asymptotic properties as the sample size $n$ increases.
We give a brief summary here and provide additional background and definitions in \cref{sec:background:parameter_estimation}.
To give any guarantees on the convergence of the MLE, we necessarily need to assume that $\opt{\vtheta}$ is identifiable.\footnote{That is, $\opt{\vtheta} \neq \vtheta \implies \fs \neq f_\vtheta$ for any $\vtheta \in \Theta$. In words, there is no other parameter $\vtheta$ that yields the same function $f_\vtheta$ as $\opt{\vtheta}$.}
If additionally, $\ell_\mathrm{nll}$ is ``well-behaved'' then standard results say that the MLE is \emph{consistent} and \emph{asymptotically normal} \citep{van2000asymptotic}: \begin{align}
  \vthetahat_\MLE \convp \opt{\vtheta} \quad\text{and}\quad \vthetahat_\MLE \convd \N{\opt{\vtheta}}{\mS_n} \quad\text{as $n \to \infty$}. \label{eq:mle_asymptotics}
\end{align}
Here, we denote by $\mS_n$ the asymptotic variance of the MLE which can be understood as measuring the ``quality'' of the estimate.\footnote{A ``smaller'' variance means that we can be more confident that the MLE is close to the true parameter.}
This implies in some sense that the MLE is asymptotically unbiased.
Moreover, the MLE can be shown to be \emph{asymptotically efficient} which is to say that there exists no other consistent estimator with a ``smaller'' asymptotic variance.\footnote{see \cref{sec:fundamentals:parameter_estimation:asymptotic_efficiency}.}

The situation is quite different in the finite sample regime.
Here, the MLE need not be unbiased, and it is susceptible to \midx{overfitting} to the (finite) training data as we discuss in more detail in \cref{sec:fundamentals:supervised_learning:risk}.

\subsection{Using Priors: Maximum a Posteriori Estimation}\label{sec:fundamentals:parameter_esitmation:map}

We can incorporate prior assumptions about the parameters $\opt{\vtheta}$ into the estimation procedure.
One approach of this kind is to find the mode of the posterior distribution, called the \midx{maximum a posteriori estimate} (or MAP estimate): \begin{align}
  \vthetahat_\MAP &\defeq \argmax_{\vtheta \in \Theta} p(\vtheta \mid \vx_{1:n}, y_{1:n}) \\
  &= \argmax_{\vtheta \in \Theta} p(y_{1:n} \mid \vx_{1:n}, \vtheta) \cdot p(\vtheta) \margintag{by Bayes' rule \eqref{eq:bayes_rule}} \\
  &= \argmax_{\vtheta \in \Theta} \log p(\vtheta) + \sum_{i=1}^n \log p(y_i \mid \vx_i, \vtheta) \margintag{taking the logarithm} \label{eq:map} \\
  &= \argmin_{\vtheta \in \Theta} \underbrace{- \log p(\vtheta)}_{\text{regularization}} + \underbrace{\ell_\mathrm{nll}(\vtheta; \spD_n)}_{\text{quality of fit}}.
\end{align}
Here, the \midx{log-prior} $\log p(\vtheta)$ acts as a regularizer.
Common regularizers are given, for example, by \begin{itemize}
  \item $p(\vtheta) = \N[\vtheta]{\vzero}{\inv{(2 \lambda)} \mI}$ which yields $-\log p(\vtheta) = \lambda \norm{\vtheta}_2^2 + \const$,
  \item $p(\vtheta) = \Laplace[\vtheta]{\vzero}{\inv{\lambda}}$ which yields $-\log p(\vtheta) = \lambda \norm{\vtheta}_1 + \const$,
  \item a uniform prior (cf. \cref{sec:fundamentals:inference:priors}) for which the MAP is equivalent to the MLE.
  In other words, the MLE is merely the mode of the posterior distribution under a uniform prior.
\end{itemize}

The Gaussian and Laplace regularizers act as simplicity biases, preferring simpler models over more complex ones, which empirically tends to reduce the risk of overfitting.
However, one may also encode more nuanced information about the (assumed) structure of $\opt{\vtheta}$ into the prior.

An alternative way of encoding a prior is by restricting the function class to some $\widetilde{\Theta} \subset \Theta$, for example to rotation- and translation-invariant models as often done when the inputs are images.
This effectively sets $p(\vtheta) = 0$ for all $\vtheta \in \Theta \setminus \widetilde{\Theta}$ but is better suited for numerical optimization than to impose this constraint directly on the prior.

Encoding prior assumptions into the function class or into the parameter estimation can accelerate learning and improve generalization performance dramatically, yet importantly, incorporating a prior can also inhibit learning in case the prior is ``wrong''.
For example, when the learning task is to differentiate images of cats from images of dogs, consider the (stupid) prior that only permits models that exclusively use the upper-left pixel for prediction.
No such model will be able to solve the task, and therefore starting from this prior makes the learning problem effectively unsolvable which illustrates that priors have to be chosen with care.

\subsection{When does the prior matter?}

We have seen that the MLE has desirable asymptotic properties, and that MAP estimation can be seen as a regularized MLE where the type of regularization is encoded by the prior.
Is it possible to derive similar asymptotic results for the MAP estimate?

To answer this question, we will look at the asymptotic effect of the prior on the posterior more generally.
\midx{Doob's consistency theorem} states that assuming parameters are identifiable,\footnote{This is akin to the assumption required for consistency of the MLE, cf. \cref{eq:mle_asymptotics}.} there exists $\widetilde{\Theta} \subseteq \Theta$ with $p(\widetilde{\Theta}) = 1$ such that the posterior is consistent for any $\opt{\vtheta} \in \widetilde{\Theta}$ \citep{doob1949application,miller2016lecture,miller2018detailed}: \begin{align}
  \vtheta \mid \spD_n \convp \opt{\vtheta} \qquad\text{as $n \to \infty$}.
\end{align}
In words, Doob's consistency theorem tells us that for \emph{any} prior distribution, the posterior is guaranteed to converge to a point density in the (small) neighborhood $\opt{\vtheta} \in B$ of the true parameter as long as $p(B) > 0$.\footnote{$B$ can for example be a ball of radius $\epsilon$ around $\opt{\vtheta}$ (with respect to some geometry of $\Theta$).}
We call such a prior a \midx{well-specified prior}.

\begin{rmk}{Cromwell's rule}{}
  In the case where $|\Theta|$ is finite, Doob's consistency theorem strongly suggests that the prior should not assign $0$ probability (or probability $1$ for that matter) to any individual parameter $\vtheta \in \Theta$, unless we know with certainty that $\opt{\vtheta} \neq \vtheta$.
  This is called \midx{Cromwell's rule}, and a prior obeying by this rule is always well-specified.
\end{rmk}

Under the same assumption that the prior is well-specified (and regularity conditions\footnote{These regularity conditions are akin to the assumptions required for asymptotic normality of the MLE, cf. \cref{eq:mle_asymptotics}.}), the \midx{Bernstein-von Mises theorem}, which was first discovered by Pierre-Simon Laplace in the early 19th century, establishes the asymptotic normality of the posterior distribution \citep{van2000asymptotic,miller2016lecture}: \begin{align}
  \vtheta \mid \spD_n \convd \N{\opt{\vtheta}}{\mS_n} \qquad\text{as $n \to \infty$}
\end{align} and where $\mS_n$ is the same as the asymptotic variance of the MLE.\footnote{This has also been called the ``Bayesian central limit theorem'', which is a bit of a misnomer since the theorem also applies to likelihoods (when the prior is noninformative) which are often used in frequentist statistics.}

These results link probabilistic inference to maximum likelihood estimation in the asymptotic limit of infinite data.
Intuitively, in the limit of infinite data, the prior is ``overwhelmed'' by the observations and the posterior becomes equivalent to the limiting distribution of the MLE.\footnote{More examples and discussion can be found in section 17.8 of \cite{le2012asymptotic}, chapter 8 of \cite{le2000asymptotics}, chapter 10 of \cite{van2000asymptotic}, and in \cite{tanner1993tools}.}
One can interpret the regime of infinite data as the regime where computational resources and time are unlimited and plausible inferences evolve into logical inferences.
This transition signifies a shift from the realm of uncertainty to that of certainty.
The importance of the prior surfaces precisely in the non-asymptotic regime where plausible inferences are necessary due to \emph{limited computational resources and limited time}.

\subsection{Estimation vs Inference}

You can interpret a single parameter vector $\vtheta \in \Theta$ as ``one possible explanation'' of the data.
Maximum likelihood and maximum a posteriori estimation are examples of \midx{estimation} algorithms which return a one such parameter vector --- called a \midx{point estimate}.
That is, given the training set $\spD_n$, they return a single parameter vector $\vthetahat_n$.
We give a more detailed account of estimation in \cref{sec:background:parameter_estimation}.

\begin{ex}{Point estimates and invalid logical inferences}{invalid_logical_inferences}
  To see why point estimates can be problematic, recall \cref{ex:weak_probabilistic_inference}.
  We have seen that the logical implication \begin{center}
    ``If it is raining, the ground is wet.''
  \end{center} can be expressed as $\Pr{W \mid R} = 1$.
  Observing that ``The ground is wet.'' does not permit logical inference, yet, the maximum likelihood estimate of $R$ is $\hat{R}_\MLE = 1$.
  This is logically inconsistent since there might be other \emph{explanations} for the ground to be wet, such as a sprinkler!
  With only a finite sample (say independently observing $n$ times that the ground is wet), we cannot rule out with certainty that the ground is wet for other reasons than rain.
\end{ex}

In practice, we never observe an infinite amount of data.
\Cref{ex:invalid_logical_inferences} demonstrates that on a finite sample, point estimates may perform invalid logical inferences, and can therefore lure us into a false sense of certainty.

\begin{rmk}{MLE and MAP are approximations of inference}{}
  The MLE and MAP estimate can be seen as a naive approximation of probabilistic inference, represented by a point density which ``collapses'' all probability mass at the mode of the posterior distribution.
  This can be a relatively decent --- even if overly simple --- approximation when the distribution is unimodal, symmetric, and light-tailed as in \cref{fig:point_estimates}, but is usually a very poor approximation for practical posteriors that are complex and multimodal.
\end{rmk}

\begin{marginfigure}
  \incplt{point_estimates}
  \caption{A the MLE/MAP are point estimates at the mode $\hat{\theta}$ of the posterior distribution $p(\theta \mid \spD)$.}
  \label{fig:point_estimates}
\end{marginfigure}

In this manuscript, we will focus mainly on algorithms for \midx{probabilistic inference} which compute or approximate the distribution ${p(\vtheta \mid \vx_{1:n}, y_{1:n})}$ over parameters.
Returning a distribution over parameters is natural since this acknowledges that given a finite sample with noisy observations, more than one parameter vector can explain the data.




\subsection{Probabilistic Inference and Prediction}

The prior distribution $p(\vtheta)$ can be interpreted as the degree of our belief that the model parameterized by $\vtheta$ ``describes the (previously seen) data best''.
The likelihood captures how likely the training data is under a particular model: \begin{align}
  p(y_{1:n} \mid \vx_{1:n}, \vtheta) = \prod_{i=1}^n p(y_i \mid \vx_i, \vtheta). \label{eq:likelihood}
\end{align}
The posterior then represents our belief about the best model after seeing the training data.
Using Bayes' rule~\eqref{eq:bayes_rule}, we can write it as\footnote{We generally assume that \begin{align*}
  p(\vtheta \mid \vx_{1:n}) = p(\vtheta).
\end{align*} For our purposes, you can think of the inputs $\vx_{1:n}$ as fixed deterministic parameters, but one can also consider inputs drawn from a distribution over $\spX$.} \begin{subequations}\begin{align}
  p(\vtheta \mid \vx_{1:n}, y_{1:n})&= \frac{1}{Z} p(\vtheta) \prod_{i=1}^n p(y_i \mid \vx_i, \vtheta) \quad\text{where} \\
  Z &\defeq \int_\Theta p(\vtheta) \prod_{i=1}^n p(y_i \mid \vx_i, \vtheta) \, d\vtheta \label{eq:normalizing_constant}
\end{align}\end{subequations} is the \midx{normalizing constant}.
We refer to this process of learning a model from data as \midx{learning}.
We can then use our learned model for \midx{prediction} at a new input $\vxs$ by conditioning on $\vtheta$, \begin{align}
  p(\ys \mid \vxs, \vx_{1:n}, y_{1:n}) &= \int_\Theta p(\ys, \vtheta \mid \vxs, \vx_{1:n}, y_{1:n}) \,d\vtheta \nonumber \margintag{by the sum rule \eqref{eq:sum_rule}} \\
  &= \int_\Theta p(\ys \mid \vxs, \vtheta) \cdot p(\vtheta \mid \vx_{1:n}, y_{1:n}) \,d\vtheta. \label{eq:prediction} \margintag{by the product rule \eqref{eq:product_rule} and $\ys \perp \vx_{1:n}, y_{1:n} \mid \vtheta$}
\end{align}
Here, the distribution over models $p(\vtheta \mid \vx_{1:n}, y_{1:n})$ is called the \midx{posterior}[idxpagebf] and the distribution over predictions $p(\ys \mid \vxs, \vx_{1:n}, y_{1:n})$ is called the \midx{predictive posterior}.
The predictive posterior quantifies our posterior uncertainty about the ``prediction'' $\ys$, however, since this is typically a complex distribution, it is difficult to communicate this uncertainty to a human.
One statistic that can be used for this purpose is the smallest set $\spC_\delta(\vxs) \subseteq \R$ for a fixed $\delta \in (0,1)$ such that \begin{align}
  \Pr{\ys \in \spC_\delta(\vxs) \mid \vxs, \vx_{1:n}, y_{1:n}} \geq 1 - \delta. \label{eq:credible_interval}
\end{align}
That is, we believe with ``confidence'' at least $1 - \delta$ that the true value of $\ys$ lies in $\spC_\delta(\vxs)$.
Such a set $\spC_\delta(\vxs)$ is called a \midx{credible set}.

\begin{marginfigure}
  \incplt{credible_intervals}
  \caption{Example of a $95\%$ credible set at $\vxs$ where the predictive posterior is Gaussian with mean $\mu(\vxs)$ and standard deviation $\sigma(\vxs)$. In this case, the gray area integrates to $\approx 0.95$ for \begin{align*}
    \spC_{0.05}(\vxs) = [\mu(\vxs) \pm 1.96 \sigma(\vxs)].
  \end{align*}}
  \label{fig:credible_intervals}
\end{marginfigure}

We have seen here that the tasks of learning and prediction are intimately related.
Indeed, ``prediction'' can be seen in many ways as a natural by-product of ``reasoning'' (i.e., probabilistic inference), where we evaluate the likelihood of outcomes given our learned explanations for the world.
This intuition can be read off directly from \cref{eq:prediction} where $p(\ys \mid \vxs, \vtheta)$ corresponds to the likelihood of an outcome given the explanation $\vtheta$ and $p(\vtheta \mid \vx_{1:n}, y_{1:n})$ corresponds to our inferred belief about the world.
We will see many more examples of this link between probabilistic inference and prediction throughout this manuscript.

The high-dimensional integrals of \cref{eq:normalizing_constant,eq:prediction} are typically intractable, and represent the main computational challenge in probabilistic inference.
Throughout the first part of this manuscript, we will describe settings where exact inference is tractable, as well as modern approximate inference algorithms that can be used when exact inference is intractable.

\subsection{Recursive Probabilistic Inference and Memory}\label{sec:fundamentals:recursive_inference}

We have already alluded to the fact that probabilistic inference has a recursive structure, which lends itself to continual learning and which often leads to efficient algorithms.
Let us denote by \begin{align}
  p^{(t)}(\vtheta) \defeq p(\vtheta \mid \vx_{1:t}, y_{1:t})
\end{align} the posterior after the first $t$ observations with $p^{(0)}(\vtheta) = p(\vtheta)$.
Now, suppose that we have already computed $p^{(t)}(\vtheta)$ and observe $y_{t+1}$.
We can recursively update the posterior as follows, \begin{align}
  p^{(t+1)}(\vtheta) &= p(\vtheta \mid y_{1:t+1}) \nonumber \\
  &\propto p(\vtheta \mid y_{1:t}) \cdot p(y_{t+1} \mid \vtheta, y_{1:t}) \margintag{using Bayes' rule \eqref{eq:bayes_rule}} \nonumber \\
  &= p^{(t)}(\vtheta) \cdot p(y_{t+1} \mid \vtheta). \margintag{using $y_{t+1} \perp y_{1:t} \mid \vtheta$, see \cref{fig:blr}} \label{eq:recursive_probabilistic_inference}
\end{align}
Intuitively, the posterior distribution at time $t$ ``absorbs'' or ``summarizes'' all seen data.

By unrolling the recursion of \cref{eq:recursive_probabilistic_inference}, we see that regardless of the philosophical interpretation of probability, probabilistic inference is a fundamental  mechanism of learning.
Even the MLE which performs naive approximate inference without a prior (i.e., a uniform prior), is based on $p^{(n)}(\vtheta) \propto p(y_{1:n} \mid \vx_{1:n}, \vtheta)$ which is the result of $n$ individual plausible inferences, where the $(t+1)$-st inference uses the posterior of the $t$-th inference as its prior.

So far we have been considering the supervised learning setting, where all data is available a-priori.
However, by sequentially obtaining the new posterior and replacing our prior, we can also perform probabilistic inference as data arrives online (i.e., in ``real-time'').
This is analogous to recursive logical inference where derived consequences are repeatedly added to the set of propositions to derive new consequences.
This also highlights the intimate connection between ``reasoning'' and ``memory''.
Indeed, the posterior distribution $p^{(t)}(\vtheta)$ can be seen as a form of memory that evolves with time $t$.

\section{Outlook: Decision Theory}\label{sec:decision_theory}

How can we use our predictions to make concrete decisions under uncertainty?
We will study this question extensively in \cref{part2} of this manuscript, but briefly introduce some fundamental concepts here.
Making decisions using a probabilistic model $p(y \mid \vx)$ of output $y \in \spY$ given input $\vx \in \spX$, such as the ones we have discussed in the previous section, is commonly formalized by \begin{itemize}
  \item a set of possible actions $\spA$, and
  \item a reward function $r(y, a) \in \R$ that computes the reward or utility of taking action ${a \in \spA}$, assuming the true output is ${y \in \spY}$.
\end{itemize}
Standard decision theory recommends picking the action with the largest expected utility: \begin{align}
  \opt{a}(\vx) \defeq \argmax_{a \in \spA} \E[y \mid \vx]{r(y, a)}. \label{eq:decision_theory}
\end{align}
Here, $\opt{a}$ is called the \midx{optimal decision rule} because, under the given probabilistic model, no other rule can yield a higher expected utility.

Let us consider some examples of reward functions and their corresponding optimal decisions:

\begin{ex}{Reward functions}{decision_theory}
  Under the decision rule from \cref{eq:decision_theory}, different reward functions~$r$ can lead to different decisions.
  Let us examine two reward functions for the case where $\spY = \spA = \R$ \exerciserefmark{decision_theory}.

  \begin{itemize}
    \item Alternatively to considering $r$ as a reward function, we can interpret $-r$ as the loss of taking action $a$ when the true output is $y$.
    If our goal is for our actions $a$ to ``mimic'' the output $y$, a natural choice is the squared loss, $- r(y, a) = (y - a)^2$.
    It turns out that under the squared loss, the optimal decision is simply the mean: $\opt{a}(\vx) = \E{y \mid \vx}$.
    \item To contrast this, we consider the asymmetric loss, \begin{align*}
      - r(y, a)= c_1 \underbrace{\max\{y-a, 0\}}_{\text{underestimation error}} \quad+\quad c_2 \underbrace{\max\{a-y, 0\}}_{\text{overestimation error}},
    \end{align*} which penalizes underestimation and overestimation differently.
    When $y \mid \vx \sim \N{\mu_\vx}{\sigma_\vx^2}$ then the optimal decision is \begin{align*}
      \opt{a}(\vx) = \mu_\vx + \underbrace{\sigma_\vx \cdot \inv{\Phi}\parentheses*{\frac{c_1}{c_1 + c_2}}}_{\text{pessimism / optimism}}
    \end{align*} where $\Phi$ is the CDF of the standard normal distribution.\safefootnote{Recall that the CDF~$\Phi$ of the standard normal distribution is a sigmoid with its inverse satisfying \begin{align*}
      \inv{\Phi}(u) \begin{cases}
        < 0 & \text{if } u < 0.5, \\
        = 0 & \text{if } u = 0.5, \\
        > 0 & \text{if } u > 0.5.
      \end{cases}
    \end{align*}}
    Note that if $c_1 = c_2$, then the second term vanishes and the optimal decision is the same as under the squared loss.
    If $c_1 > c_2$, the second term is positive (i.e., \midx<optimistic>{optimism in the face of uncertainty}) to avoid underestimation, and if $c_1 < c_2$, the second term is negative (i.e., \emph{pessimistic}) to avoid overestimation.
    We will find these notions of optimism and pessimism to be useful in many decision-making scenarios.
  \end{itemize}
\end{ex}

While \cref{eq:decision_theory} describes how to make optimal decisions given a (posterior) probabilistic model, it does not tell us how to learn or improve this model in the first place.
That is, these decisions are only optimal under the assumption that we cannot use their outcomes and our resulting observations to update our model and inform future decisions.
When we start to consider the effect of our decisions on future data and future posteriors, answering ``how do I make optimal decisions?'' becomes more complex, and we will study this in \cref{part2} on sequential decision-making.

\section*{Discussion}

In this chapter, we have learned about the fundamental concepts of probabilistic inference.
We have seen that probabilistic inference is the natural extension of logical reasoning to domains with uncertainty.
We have also derived the central principle of probabilistic inference, Bayes' rule, which is simple to state but often computationally challenging.
In the next part of this manuscript, we will explore settings where exact inference is tractable, as well as modern approaches to approximate probabilistic inference.

\section*{Overview of Mathematical Background}

We have included brief summaries of the fundamentals of \textbf{parameter estimation} (mean estimation in particular) and \textbf{optimization} in \cref{sec:background:parameter_estimation,sec:fundamentals:optimization}, respectively, which we will refer back to throughout the manuscript.
\Cref{sec:fundamentals:qf} discusses the \textbf{correspondence of Gaussians and quadratic forms}.
\Cref{sec:background:identities_and_inequalities} comprises a list of useful matrix identities and inequalities.

\excheading

\begin{nexercise}{Properties of probability}{properties_of_probability}
  Let $(\Omega, \spA, \fnPr)$ be a probability space.
  Derive the following properties of probability from the Kolmogorov axioms:
  \begin{enumerate}
    \item For any $A, B \in \spA$, if $A \subseteq B$ then $\Pr{A} \leq \Pr{B}$.
    \item For any $A \in \spA$, $\Pr{\compl{A}} = 1 - \Pr{A}$.
    \item For any countable set of events $\{\sA_i \in \spA\}_i$, \begin{align}
      \Pr{\bigcup_{i=1}^\infty \sA_i} \leq \sum_{i=1}^\infty \Pr{\sA_i}. \label{eq:union_bound}
    \end{align} which is called a \midx{union bound}.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Random walks on graphs}{graph_random_walk}
  Let $G$ be a simple connected finite graph. We start at a vertex $u$ of $G$. At every step, we move to one of the neighbors of the current vertex uniformly at random, e.g., if the vertex has $3$ neighbors, we move to one of them, each with probability $1 / 3$. What is the probability that the walk visits a given vertex $v$ eventually?
\end{nexercise}

\begin{nexercise}{Law of total expectation}{lote}
  Show that if $\{\sA_i\}_{i=1}^k$ are a partition of $\Omega$ and $\rX$ is a random vector, \begin{align}
    \E{\rX} = \sum_{i=1}^k \E{\rX \mid \sA_i} \cdot \Pr{\sA_i}.
  \end{align}
\end{nexercise}

\begin{nexercise}{Covariance matrices are positive semi-definite}{cov_mat_pos_sd}
  Prove that a covariance matrix $\mSigma$ is always positive semi-definite.
  That is, all of its eigenvalues are greater or equal to zero, or equivalently, $\transpose{\vx}\mSigma\vx \geq 0$ for any $\vx \in \R^n$.
\end{nexercise}

\begin{nexercise}{Probabilistic inference}{bayes_rule}
  As a result of a medical screening, one of the tests revealed a serious disease in a person.
  The test has a high accuracy of $99\%$ (the probability of a positive response in the presence of a disease is $99\%$ and the probability of a negative response in the absence of a disease is also~$99\%$).
  However, the disease is quite rare and occurs only in one person per $10\,000$.
  Calculate the probability of the examined person having the identified disease.
\end{nexercise}

\begin{nexercise}{Zero eigenvalues of covariance matrices}{zero_ev_of_cov_mats}
  We say that a random vector $\rX$ in $\R^n$ is not linearly independent if for some $\valpha \in \R^n \setminus \{\vzero\}$, $\transpose{\valpha} \rX = 0$.
  \begin{enumerate}
    \item Show that if $\rX$ is not linearly independent, then $\Var{\rX}$ has a zero eigenvalue.

    \item Show that if $\Var{\rX}$ has a zero eigenvalue, then $\rX$ is not linearly independent. \\
    \textit{Hint: Consider the variance of $\transpose{\vlambda} \rX$ where $\vlambda$ is the eigenvector corresponding to the zero eigenvalue.}
  \end{enumerate}
  Thus, we have shown that $\Var{\rX}$ has a zero eigenvalue if and only if~$\rX$ is not linearly independent.
\end{nexercise}

\begin{nexercise}{Product of Gaussian PDFs}{gaussian_pdf_prod}
  Let $\vmu_1, \vmu_2 \in \R^n$ be mean vectors and $\mSigma_1, \mSigma_2 \in \R^{n \times n}$ be covariance matrices. Prove that \begin{align}
    \N[\vx]{\vmu}{\mSigma} \propto \N[\vx]{\vmu_1}{\mSigma_1} \cdot \N[\vx]{\vmu_2}{\mSigma_2} \label{eq:gaussian_pdf_product}
  \end{align} for some mean vector $\vmu \in \R^n$ and covariance matrix $\mSigma \in \R^{n \times n}$.
  That is, show that the product of two Gaussian PDFs is proportional to the PDF of a Gaussian.
\end{nexercise}

\begin{nexercise}{Independence of Gaussians}{grv_uncor_indep}
  Show that two jointly Gaussian random vectors, $\rX$ and $\rY$, are independent if and only if $\rX$ and $\rY$ are uncorrelated.
\end{nexercise}

\begin{nexercise}{Marginal / conditional distribution of a Gaussian}{marginal_and_cond_gaussian}
  Prove \cref{fct:marginal_and_cond_gaussian}. That is, show that \begin{enumerate}
    \item every marginal of a Gaussian is Gaussian; and
    \item conditioning on a subset of variables of a joint Gaussian is Gaussian
  \end{enumerate} by finding their corresponding PDFs.

  \textit{Hint: You may use that for matrices $\mSigma$ and $\mLambda$ such that $\inv{\mSigma} = \mLambda$, \begin{itemize}
    \item if $\mSigma$ and $\mLambda$ are symmetric, \begin{align*}
      &\transpose{\begin{bmatrix}
        \vx_\sA \\
        \vx_\sB \\
      \end{bmatrix}} \begin{bmatrix}
        \mLambda_{\sA\sA} & \mLambda_{\sA\sB} \\
        \mLambda_{\sB\sA} & \mLambda_{\sB\sB} \\
      \end{bmatrix} \begin{bmatrix}
        \vx_\sA \\
        \vx_\sB \\
      \end{bmatrix} \\
      &= \transpose{\vx_\sA}\mLambda_{\sA\sA}\vx_\sA + \transpose{\vx_\sA}\mLambda_{\sA\sB}\vx_\sB + \transpose{\vx_\sB}\mLambda_{\sB\sA}\vx_\sA + \transpose{\vx_\sB}\mLambda_{\sB\sB}\vx_\sB \\
      &= \begin{multlined}[t]
        \transpose{\vx_\sA}(\mLambda_{\sA\sA} - \mLambda_{\sA\sB}\inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA})\vx_\sA + \\ \transpose{(\vx_\sB + \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}\vx_\sA)}\mLambda_{\sB\sB}(\vx_\sB + \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}\vx_\sA),
      \end{multlined}
    \end{align*}
    \item $\inv{\mLambda_{\sB\sB}} = \mSigma_{\sB\sB} - \mSigma_{\sB\sA}\inv{\mSigma_{\sA\sA}}\mSigma_{\sA\sB}$,
    \item $\inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA} = -\mSigma_{\sB\sA}\inv{\mSigma_{\sA\sA}}$.
  \end{itemize}
  The final two equations follow from the general characterization of the inverse of a block matrix \citep[section 9.1.3]{petersen2008matrix}.}
\end{nexercise}

\begin{nexercise}{Closedness properties of Gaussians}{gaussian_closedness}
  Recall the notion of a \midx{moment-generating function} (MGF) of a random vector $\rX$ in $\R^n$ which is defined as \begin{align}
    \varphi_{\rX}(\vt) \defeq \E*{\exp\parentheses*{\transpose{\vt}\rX}}, \quad\text{for all $\vt \in \R^n$}. \label{eq:mgf}
  \end{align}
  An MGF uniquely characterizes a distribution.
  The MGF of the multivariate Gaussian $\rX \sim \N{\vmu}{\mSigma}$ is \begin{align}
    \varphi_\rX(\vt) = \exp\parentheses*{\transpose{\vt}\vmu + \frac{1}{2}\transpose{\vt}\mSigma\vt}. \label{eq:mgf_gaussian}
  \end{align}
  This generalizes the MGF of the univariate Gaussian from \cref{eq:mgf_univ_gaussian}.

  Prove the following facts.
  \begin{enumerate}
    \item \emph{Closedness under affine transformations:} Given an $n$-dimensional Gaussian $\rX \sim \N{\vmu}{\mSigma}$, and ${\mA \in \R^{m \times n}}$ and $\vb \in \R^m$, \begin{align}
      \mA \rX + \vb \sim \N{\mA \vmu + \vb}{\mA \mSigma \transpose{\mA}}. \label{eq:gaussian_lin_trans}
    \end{align}

    \item \emph{Additivity:} Given two independent Gaussian random vectors ${\rX \sim \N{\vmu}{\mSigma}}$ and ${\rXp \sim \N{\vmup}{\mSigmap}}$ in $\R^n$, \begin{align}
      \rX + \rXp \sim \N{\vmu + \vmup}{\mSigma + \mSigmap}. \label{eq:gaussian_additivity}
    \end{align}
  \end{enumerate}

  These properties are unique to Gaussians and a reason for why they are widely used for learning and inference.
\end{nexercise}

\begin{nexercise}{Expectation and variance of Gaussians}{expectation_and_variance_of_gaussians}
  Derive that $\E{\rX} = \vmu$ and $\Var{\rX} = \mSigma$ when $\rX \sim \N{\vmu}{\mSigma}$. \\
  \textit{Hint: First derive the expectation and variance of a univariate standard normal random variable.}
\end{nexercise}

\begin{nexercise}{Non-affine transformations of Gaussians}{non_affine_transformations_of_gaussians}
  Answer the following questions with \textbf{yes} or \textbf{no}.
  \begin{enumerate}
    \item Does there exist any non-affine transformation of a Gaussian random vector which is Gaussian? If yes, give an example.
    \item Let $X, Y, Z$ be independent standard normal random variables. Is $\frac{X + YZ}{\sqrt{1 + Z^2}}$ Gaussian?
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Decision theory}{decision_theory}
  Derive the optimal decisions under the squared loss and the asymmetric loss from \cref{ex:decision_theory}.
\end{nexercise}
