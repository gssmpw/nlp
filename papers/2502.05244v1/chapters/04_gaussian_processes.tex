\chapter{Gaussian Processes}\label{sec:gp}

Let us remember our first attempt from~\cref{sec:blr} at scaling up Bayesian linear regression to nonlinear functions.
We saw that we can model nonlinear functions by transforming the input space to a suitable higher-dimensional feature space, but found that this approach scales poorly if we require a large number of features.
We then found something remarkable: by simply changing our perspective from a weight-space view to a function-space view, we could implement Bayesian linear regression without ever needing to compute the features explicitly.
Under the function-space view, the key object describing the class of functions we can model is not the features~$\vphi(\vx)$, but instead the kernel function which only implicitly defines a feature space.
Our key observation in this chapter is that we can therefore stop reasoning about feature spaces, and instead directly work with kernel functions that describe ``reasonable'' classes of functions.

We are still concerned with the problem of estimating the value of a function $f : \spX \to \R$ at arbitrary points $\vxs \in \spX$ given training data $\{\vx_i,y_i\}_{i=1}^n$, where the labels are assumed to be corrupted by homoscedastic Gaussian noise with variance $\sigman^2$, \begin{align*}
  y_i = f(\vx_i) + \varepsilon_i,\quad \varepsilon_i \sim \N{0}{\sigman^2}.
\end{align*}
As in \cref{sec:blr} on Bayesian linear regression, we denote by $\mX$ the design matrix (collection of training inputs) and by $\vy$ the vector of training labels.
We will represent the unknown function value at a point $\vx \in \spX$ by the random variable $f_{\vx} \defeq f(\vx)$.
The collection of these random variables is then called a Gaussian process if any finite subset of them is jointly Gaussian:

\begin{defn}[Gaussian process, GP]\pidx{Gaussian process}
  A \emph{Gaussian process} is an infinite set of random variables such that any finite number of them are jointly Gaussian and such that they are consistent under marginalization.\footnote{That is, if you take a joint distribution for $n$ variables and marginalize out one of them, you should recover the joint distribution for the remaining $n-1$ variables.}
\end{defn}

The fact that with a Gaussian process, any finite subset of the random variables is jointly Gaussian is the key property allowing us to perform exact probabilistic inference.
Intuitively, a Gaussian process can be interpreted as a normal distribution over functions --- and is therefore often called an ``infinite-dimensional Gaussian''.

\begin{marginfigure}
  \incplt{gp}
  \caption{A Gaussian process can be interpreted as an infinite-dimensional Gaussian over functions.
  At any location $x$ in the domain, this yields a distribution over values $f(x)$ shown in red.
  The blue line corresponds to the MAP estimate (i.e., mean function of the Gaussian process), the dark gray region corresponds to the epistemic uncertainty and the light gray region denotes the additional aleatoric uncertainty.}
\end{marginfigure}

A Gaussian process is characterized by a \midx{mean function} $\mu : \spX \to \R$ and a \midx{covariance function} (or \midx{kernel function}) $k : \spX \times \spX \to \R$ such that for any set of points $\sA \defeq \{\vx_1, \dots, \vx_m\} \subseteq \spX$, we have \begin{align}
  \vf_\sA \defeq \transpose{[f_{\vx_1} \; \cdots \; f_{\vx_m}]} \sim \N{\vmu_\sA}{\mK_{\sA\sA}} \label{eq:gp_joint_distr}
\end{align} where \begin{align}
  \vmu_\sA \defeq \begin{bmatrix}
    \mu(\vx_1) \\
    \vdots \\
    \mu(\vx_m) \\
  \end{bmatrix}, \quad \mK_{\sA\sA} \defeq \begin{bmatrix}
    k(\vx_1, \vx_1) & \cdots & k(\vx_1, \vx_m) \\
    \vdots & \ddots & \vdots \\
    k(\vx_m, \vx_1) & \cdots & k(\vx_m, \vx_m) \\
  \end{bmatrix}.
\end{align}
We write $f \sim \GP{\mu}{k}$.
In particular, given a mean function, covariance function, and using the homoscedastic noise assumption, \begin{align}
  \ys \mid \vxs \sim \N{\mu(\vxs)}{k(\vxs, \vxs) + \sigman^2}. \label{eq:gp_predictive_distr}
\end{align}
Commonly, for notational simplicity, the mean function is taken to be zero.
Note that for a fixed mean this is not a restriction, as we can simply apply the zero-mean Gaussian process to the difference between the mean and the observations.\footnote{For alternative ways of representing a mean function, refer to section 2.7 of \icite{gpml}.}


\section{Learning and Inference}\label{sec:gp:learning_and_inference}

First, let us look at learning and inference in the context of Gaussian processes.
With slight abuse of our previous notation, let us denote the set of observed points by $\sA \defeq \{\vx_1, \dots, \vx_n\}$.
Given a prior $f \sim \GP{\mu}{k}$ and the noisy observations ${y_i = f(\vx_i) + \varepsilon_i}$ with ${\varepsilon_i \sim \N{0}{\sigman^2}}$, we can then write the joint distribution of the observations $y_{1:n}$ and the noise-free prediction $\fs$ at a test point $\vxs$ as \begin{align}
  &\begin{bmatrix}
    \vy \\
    \fs \\
  \end{bmatrix} \mid \vxs, \mX \sim \N{\Tilde{\vmu}}{\Tilde{\mK}}, \quad\text{where} \\
  &\Tilde{\vmu} \defeq \begin{bmatrix}
    \vmu_\sA \\
    \mu(\vxs) \\
  \end{bmatrix}, \quad \Tilde{\mK} \defeq \begin{bmatrix}
    \mK_{\sA\sA} + \sigman^2 \mI & \vk_{\vxs,\sA} \\
    \transpose{\vk_{\vxs,\sA}} & k(\vxs, \vxs) \\
  \end{bmatrix}, \quad \vk_{\vx,\sA} \defeq \begin{bmatrix}
    k(\vx,\vx_1) \\ \vdots \\ k(\vx, \vx_n)
  \end{bmatrix}.
\end{align}
Deriving the conditional distribution using \eqref{eq:cond_gaussian}, we obtain that the Gaussian process posterior is given by \begin{align}
  f \mid \vx_{1:n}, y_{1:n} &\sim \GP{\mu'}{k'}, \quad\text{where} \label{eq:gp_posterior} \\
  \mu'(\vx) &\defeq \mu(\vx) + \transpose{\vk_{\vx,\sA}} \inv{(\mK_{\sA\sA} + \sigman^2 \mI)} (\vy_\sA - \vmu_\sA), \\
  k'(\vx, \vxp) &\defeq k(\vx, \vxp) - \transpose{\vk_{\vx,\sA}} \inv{(\mK_{\sA\sA} + \sigman^2 \mI)} \vk_{\vxp,\sA}.
\end{align}
Observe that analogously to Bayesian linear regression, the posterior covariance can only decrease when conditioning on additional data, and is independent of the observations $y_i$.

We already studied inference in the function-space view of Bayesian linear regression, but did not make the predictive posterior explicit.
Using \cref{eq:gp_posterior}, the predictive posterior at~$\vxs$ is simply \begin{align}
  \fs \mid \vxs, \vx_{1:n}, y_{1:n} &\sim \N{\mu'(\vxs)}{k'(\vxs,\vxs)}.
\end{align}

\section{Sampling}\label{sec:gp:sampling}

Often, we are not interested in the full predictive posterior distribution, but merely want to obtain samples of our Gaussian process model.
We will briefly examine two approaches.

\begin{enumerate}
  \item For the first approach, consider a discretized subset of points \begin{align*}
    \vf \defeq [f_1, \dots, f_n]
  \end{align*} that we want to sample.\footnote{For example, if we want to render the function, the length of this vector could be guided by the screen resolution.}
  Note that $\vf \sim \N{\vmu}{\mK}$.
  We have already seen in \cref{eq:gaussian_affine_transformation} that \begin{align}
    \vf = \msqrt{\mK} \vvarepsilon + \vmu
  \end{align} where $\msqrt{\mK}$ is the square root of $\mK$ and $\vvarepsilon \sim \SN$ is standard Gaussian noise.\footnote{We discuss square roots of matrices in \cref{sec:fundamentals:qf}.}
  However, computing the square root of $\mK$ takes $\BigO{n^3}$ time.

  \item For the second approach, recall the product rule \eqref{eq:product_rule}, \begin{align*}
    p(f_1, \dots, f_n) = \prod_{i=1}^n p(f_i \mid f_{1:i-1}).
  \end{align*}
  That is the joint distribution factorizes neatly into a product where each factor only depends on the ``outcomes'' of preceding factors.
  We can therefore obtain samples one-by-one, each time conditioning on one more observation: \begin{align}\begin{split}
    f_1 &\sim p(f_1) \\
    f_2 &\sim p(f_2 \mid f_1) \\
    f_3 &\sim p(f_3 \mid f_1, f_2) \\
    &\;\:\,\vdots
  \end{split}\end{align}
  This general approach is known as \midx{forward sampling}.
  Due to the matrix inverse in the formula of the GP posterior \eqref{eq:gp_posterior}, this approach also takes $\BigO{n^3}$ time.
\end{enumerate}

We will discuss more efficient approximate sampling methods in \cref{sec:gp:approximations}.

\section{Kernel Functions}\label{sec:gp:kernel_functions}

We have seen that kernel functions are the key object describing the class of functions a Gaussian process can model.
Depending on the kernel function, the ``shape'' of functions that are realized from a Gaussian process varies greatly.
Let us recap briefly from \cref{sec:blr:function_space_view} what a kernel function is:

\begin{defn}[Kernel function]\pidx{kernel function}[idxpagebf]
  A \emph{kernel function} $k : \spX \times \spX \to \R$ satisfies \begin{itemize}
    \item $k(\vx, \vxp) = k(\vxp, \vx)$ for any $\vx, \vxp \in \spX$ (symmetry), and
    \item $\mK_{\sA\sA}$ is positive semi-definite for any $\sA \subseteq \spX$.
  \end{itemize}

  The two defining conditions ensure that for any $\sA \subseteq \spX$, $\mK_{\sA\sA}$ is a valid covariance matrix.
  We say that a kernel function is \midx<positive definite>{positive definite kernel} if $\mK_{\sA\sA}$ is positive definite for any $\sA \subseteq \spX$.

\end{defn}

Intuitively, the kernel function evaluated at locations $\vx$ and $\vxp$ describes how $f(\vx)$ and $f(\vxp)$ are related, which we can express formally as \begin{align}
  k(\vx, \vxp) = \Cov{f(\vx), f(\vxp)}.
\end{align}
If $\vx$ and $\vxp$ are ``close'', then $f(\vx)$ and $f(\vxp)$ are usually taken to be positively correlated, encoding a ``smooth'' function.


In the following, we will discuss some of the most common kernel functions, how they can be combined to create ``new'' kernels, and how we can characterize the class of functions they can model.

\subsection{Common Kernels}
\begin{marginfigure}
  \incplt{kernel_linear}
  \caption{Functions sampled according to a Gaussian process with a linear kernel and $\phi = \id$.}
\end{marginfigure}
\begin{marginfigure}
  \incplt{kernel_linear_transformed}
  \caption{Functions sampled according to a Gaussian process with a linear kernel and $\vphi(x) = [1, x, x^2]$ (left) and $\phi(x) = \sin(x)$ (right).}
\end{marginfigure}

First, we look into some of the most commonly used kernels. Often an additional factor $\sigma^2$ (\midx{output scale}) is added, which we assume here to be $1$ for simplicity.

\begin{enumerate}
  \item The \midx{linear kernel} is defined as \begin{align}
    k(\vx, \vxp; \vphi) \defeq \transpose{\vphi(\vx)} \vphi(\vxp) \label{eq:linear_kernel}
  \end{align} where $\vphi$ is a nonlinear transformation as introduced in \cref{sec:blr:non_linear} or the identity.

  \begin{rmk}{GPs with linear kernel and BLR}{}
    A Gaussian process with a linear kernel is equivalent to Bayesian linear regression.
    This follows directly from the function-space view of Bayesian linear regression (see \cref{sec:blr:function_space_view}) and comparing the derived kernel function \eqref{eq:blr_kernel} with the definition of the linear kernel \eqref{eq:linear_kernel}.
  \end{rmk}

  \item The \midx{Gaussian kernel} (also known as \midx{squared exponential kernel} or \midx<radial basis function (RBF) kernel>{radial basis function kernel}) is defined as \begin{align}
    k(\vx, \vxp; h) \defeq \exp\parentheses*{-\frac{\norm{\vx-\vxp}_2^2}{2 h^2}} \label{eq:gaussian_kernel}
  \end{align} where $h$ is its \midx{length scale}. The larger the length scale $h$, the smoother the resulting functions.\footnote[][-1\baselineskip]{As the length scale is increased, the exponent of the exponential increases, resulting in a higher dependency between locations.}
  Furthermore, it turns out that the feature space (think back to \cref{sec:blr:function_space_view}!) corresponding to the Gaussian kernel is ``infinitely dimensional'', as you will show in \exerciserefmark{gaussian_kernel_feature_space}.
  So the Gaussian kernel already encodes a function class that we were not able to model under the weight-space view of Bayesian linear regression.

  \begin{figure}
    \incplt{kernel_gaussian}
    \caption{Functions sampled according to a Gaussian process with a Gaussian kernel and length scales $h = 5$ (left) and $h = 1$ (right).}
  \end{figure}

  \begin{marginfigure}
    \incplt{kernel_f_gaussian}
    \caption{Gaussian kernel with length scales $\b{h=1}$, $h=0.5$, and $\r{h=0.2}$.}
  \end{marginfigure}

  \item The \midx{Laplace kernel} (also known as \midx{exponential kernel}) is defined as \begin{align}
    k(\vx, \vxp; h) \defeq \exp\parentheses*{-\frac{\norm{\vx-\vxp}_2}{h}}. \label{eq:laplace_kernel}
  \end{align}
  As can be seen in \cref{fig:gp_samples_laplace}, samples from a GP with Laplace kernel are non-smooth as opposed to the samples from a GP with Gaussian kernel.

  \begin{marginfigure}[10\baselineskip]
    \incplt{kernel_f_laplace}
    \caption{Laplace kernel with length scales $\b{h=1}$, $h=0.5$, and $\r{h=0.2}$.}
  \end{marginfigure}

  \begin{figure}
    \incplt{kernel_laplace}
    \caption{Functions sampled according to a Gaussian process with a Laplace kernel and length scales $h = 10\,000$ (left) and $h = 10$ (right).}\label{fig:gp_samples_laplace}
  \end{figure}

  \item The \midx{Matérn kernel} trades the smoothness of the Gaussian and the Laplace kernels.
  As such, it is frequently used in practice to model ``real world'' functions that are relatively smooth.
  It is defined as \begin{align}
    k(\vx, \vxp; \nu, h) \defeq \frac{2^{1-\nu}}{\Gamma(\nu)} \parentheses*{\frac{\sqrt{2 \nu} \norm{\vx-\vxp}_2}{h}}^\nu K_\nu \parentheses*{\frac{\sqrt{2 \nu} \norm{\vx-\vxp}_2}{h}}
  \end{align} where $\Gamma$ is the Gamma function, $K_\nu$ the modified Bessel function of the second kind, and $h$ a length scale parameter.
  For ${\nu = \nicefrac{1}{2}}$, the Matérn kernel is equivalent to the Laplace kernel.
  For ${\nu \to \infty}$, the Matérn kernel is equivalent to the Gaussian kernel.
  The resulting functions are $\ceil{\nu} - 1$ times mean square differentiable.\footnote{Refer to \cref{rmk:mean_square_convergence} for the definitions of mean square continuity and differentiability.}
  In particular, GPs with a Gaussian kernel are infinitely many times mean square differentiable whereas GPs with a Laplace kernel are mean square continuous but not mean square differentiable.
\end{enumerate}

\subsection{Composing Kernels}

Given two kernels $k_1 : \spX \times \spX \to \R$ and $k_2 : \spX \times \spX \to \R$, they can be composed to obtain a new kernel $k : \spX \times \spX \to \R$ in the following ways: \begin{itemize}
  \item $k(\vx, \vxp) \defeq k_1(\vx, \vxp) + k_2(\vx, \vxp)$,
  \item $k(\vx, \vxp) \defeq k_1(\vx, \vxp) \cdot k_2(\vx, \vxp)$,
  \item $k(\vx, \vxp) \defeq c \cdot k_1(\vx, \vxp)$ for any $c > 0$,
  \item $k(\vx, \vxp) \defeq f(k_1(\vx, \vxp))$ for any polynomial $f$ with positive coefficients or $f = \exp$.
\end{itemize}

For example, the additive structure of a function $f(\vx) \defeq f_1(\vx) + f_2(\vx)$ can be easily encoded in GP models. Suppose that $f_1 \sim \GP{\mu_1}{k_1}$ and $f_2 \sim \GP{\mu_2}{k_2}$, then the distribution of the sum of those two functions $f = f_1 + f_2 \sim \GP{\mu_1 + \mu_2}{k_1 + k_2}$ is another GP.\footnote{We use $f \defeq f_1 + f_2$ to denote the function $f(\cdot) = f_1(\cdot) + f_2(\cdot)$.}

Whereas the addition of two kernels $k_1$ and $k_2$ can be thought of as an \emph{OR} operation (i.e., the kernel has high value if either $k_1$ or $k_2$ have high value), the multiplication of $k_1$ and $k_2$ can be thought of as an \emph{AND} operation (i.e., the kernel has high value if both $k_1$ and $k_2$ have high value). For example, the product of two linear kernels results in functions which are quadratic.

As mentioned previously, the constant $c$ of a scaled kernel function $k'(\vx, \vxp) \defeq c \cdot k(\vx, \vxp)$ is generally called the \midx{output scale}[idxpagebf] of a kernel, and it scales the variance $\Var{f(\vx)} = c \cdot k(\vx, \vx)$ of the predictions $f(\vx)$ from $\GP{\mu}{k'}$.

\begin{oreadings}
  For a broader introduction to how kernels can be used and combined to model certain classes of functions, read \begin{itemize}
    \item chapter 2 of \icite{duvenaud2014automatic} also known as the ``kernel cookbook'',
    \item chapter 4 of \icite{gpml}.
  \end{itemize}
\end{oreadings}

\subsection{Stationarity and Isotropy}

Kernel functions are commonly classified according to two properties:

\begin{defn}[Stationarity and isotropy]
A kernel $k : \R^d \times \R^d \to \R$ is called \begin{itemize}
  \item \midx<stationary>{stationary kernel} (or \midx<shift-invariant>{shift-invariant kernel}) if there exists a function $\tilde{k}$ such that $\tilde{k}(\vx - \vxp) = k(\vx, \vxp)$, and
  \item \midx<isotropic>{isotropic kernel} if there exists a function $\tilde{k}$ such that $\tilde{k}(\norm{\vx-\vxp}) = k(\vx, \vxp)$ with $\norm{\cdot}$ any norm.
\end{itemize}
\end{defn}

Note that stationarity is a necessary condition for isotropy. In other words, isotropy implies stationarity.

\begin{ex}{Stationarity and isotropy of kernels}{}
  \begin{center}
    \begin{tabular}{lrr}
      \toprule
      & stationary & isotropic \\
      \midrule
      linear kernel & no & no \\
      \addlinespace
      Gaussian kernel & yes & yes \\
      \addlinespace
      \makecell[l]{$k(\vx, \vxp) \defeq \exp(-\norm{\vx-\vxp}_\mM^2)$ \\ where $\mM$ is positive semi-definite} & yes & no \margintag{$\norm{\cdot}_\mM$ denotes the Mahalanobis norm induced by matrix $\mM$} \\
      \bottomrule
    \end{tabular}
  \end{center}

  For ${\vxp = \vx}$, stationarity implies that the kernel must only depend on $\vzero$.
  In other words, a stationary kernel must depend on relative locations only.
  This is clearly not the case for the linear kernel, which depends on the absolute locations of $\vx$ and $\vxp$.
  Therefore, the linear kernel cannot be isotropic either.

  For the Gaussian kernel, isotropy follows immediately from its definition.

  The last kernel is clearly stationary by definition, but not isotropic for general matrices $\mM$.
  Note that for ${\mM = \mI}$ it is indeed isotropic.
\end{ex}

Stationarity encodes the idea that relative location matters more than absolute location: the process ``looks the same'' no matter where we shift it in the input space.
This is often appropriate when we believe the same statistical behavior holds across the entire domain (e.g., no region is special).
Isotropy goes one step further by requiring that the kernel depends only on the distance between points, so that all directions in the space are treated equally.
In other words, there is no preferred orientation or axis.
This is especially useful in settings where we expect uniform behavior in every direction (as with the Gaussian kernel).
Such kernels are simpler to specify and interpret since we only need a single ``scale'' (like a length scale) rather than multiple parameters or directions.

\subsection{Reproducing Kernel Hilbert Spaces}\label{sec:rkhs}

We can characterize the precise class of functions that can be modeled by a Gaussian process with a given kernel function.
This corresponding function space is called a \midx{reproducing kernel Hilbert space} (RKHS), and we will discuss it briefly in this section.

Recall that Gaussian processes keep track of a posterior distribution $f \mid \vx_{1:n}, y_{1:n}$ over functions.
We will in fact show later that the corresponding MAP estimate $\hat{f}$ corresponds to the solution to a regularized optimization problem in the RKHS space of functions.
This duality is similar to the duality between the MAP estimate of Bayesian linear regression and ridge regression we observed in \cref{sec:blr}.
So what is the reproducing kernel Hilbert space of a kernel function $k$?


\begin{defn}[Reproducing kernel Hilbert space, RKHS]
  Given a kernel $k : \spX \times \spX \to \R$, its corresponding \emph{reproducing kernel Hilbert space} is the space of functions $f$ defined as \begin{align}
    \spH_k(\spX) \defeq \braces*{f(\cdot) = \sum_{i=1}^n \alpha_i k(\vx_i, \cdot) : n \in \Nat, \vx_i \in \spX, \alpha_i \in \R}.
  \end{align}
  The inner product of the RKHS is defined as \begin{align}
    \ip{f,g}_k \defeq \sum_{i=1}^n \sum_{j=1}^{n'} \alpha_i \alpha_j' k(\vx_i,\vx_j'),
  \end{align} where $g(\cdot) = \sum_{j=1}^{n'} \alpha_j' k(\vx_j',\cdot)$, and induces the norm ${\norm{f}_k = \sqrt{\ip{f,f}_k}}$.
  You can think of the norm as measuring the ``smoothness'' or ``complexity'' of $f$. \exerciserefmark{reproducing_kernel_hilbert_space_properties}[2]
\end{defn}
It is straightforward to check that for all $\vx \in \spX$, $k(\vx, \cdot) \in \spH_k(\spX)$.
Moreover, the RKHS inner product $\ip{\cdot,\cdot}_k$ satisfies for all $\vx \in \spX$ and $f \in \spH_k(\spX)$ that $f(\vx) = \ip{f(\cdot), k(\vx, \cdot)}_k$ which is also known as the \midx{reproducing property} \exerciserefmark{reproducing_kernel_hilbert_space_properties}[1].
That is, evaluations of RKHS functions $f$ are inner products in $\spH_k(\spX)$ parameterized by the ``feature map'' $k(\vx,\cdot)$.

The \midx{representer theorem} \citep{scholkopf2001generalized} characterizes the solution to regularized optimization problems in RKHSs:\looseness=-1

\begin{thm}[Representer theorem]
  \exerciserefmark{representer_theorem}
  Let $k$ be a kernel and let $\lambda > 0$.
  For $f \in \spH_k(\spX)$ and training data $\{(\vx_i, f(\vx_i))\}_{i=1}^n$, let ${\spL(f(\vx_1), \dots, f(\vx_n))} \in {\R \cup \{\infty\}}$ denote any loss function which depends on $f$ only through its evaluation at the training points.
  Then, any minimizer \begin{align}
    \hat{f} \in \argmin_{f \in \spH_k(\spX)} \spL(f(\vx_1), \dots, f(\vx_n)) + \lambda \norm{f}_k^2 \label{eq:representer_theorem_obj}
  \end{align} admits a representation of the form \begin{align}
    \hat{f}(\vx) = \transpose{\valphahat} \vk_{\vx,\{\vx_i\}_{i=1}^n} = \sum_{i=1}^n \hat{\alpha}_i k(\vx, \vx_i) \quad\text{for some $\valphahat \in \R^n$}. \label{eq:representer_theorem}
  \end{align}
\end{thm}

This statement is remarkable: the solutions to general regularized optimization problems over the generally infinite-dimensional space of functions~$\spH_k(\spX)$ can be represented as a linear combination of the kernel functions evaluated at the training points.
The representer theorem can be used to show that the MAP estimate of a Gaussian process corresponds to the solution of a regularized linear regression problem in the RKHS of the kernel function, namely, \exerciserefmark{mle_and_map_of_gps} \begin{align}
  \hat{f} \defeq \argmin_{f \in \spH_k(\spX)} - \log p(y_{1:n} \mid \vx_{1:n}, f) + \frac{1}{2} \norm{f}_k^2. \label{eq:map_of_gp}
\end{align}
Here, the first term corresponds to the likelihood, measuring the ``quality of fit''.
The regularization term limits the ``complexity'' of $\hat{f}$.
Regularization is necessary to prevent overfitting since in an expressive RKHSs, there may be many functions that interpolate the training data perfectly.
This shows the close link between Gaussian process regression and Bayesian linear regression, with the kernel function $k$ generalizing the inner product of feature maps to feature spaces of possibly ``infinite dimensionality''.
Because solutions can be represented as linear combinations of kernel evaluations at the training points, Gaussian processes remain computationally tractable even though they can model functions over ``infinite-dimensional'' feature spaces.

\section{Model Selection}\label{sec:gp:model_selection}

We have not yet discussed how to pick the hyperparameters $\vtheta$ (e.g., parameters of kernels).
A common technique in supervised learning is to select hyperparameters $\vtheta$, such that the resulting function estimate $\hat{f}_\vtheta$ leads to the most accurate predictions on hold-out validation data.
After reviewing this approach, we contrast it with a probabilistic approach to model selection, which avoids using point estimates of $\hat{f}_\vtheta$ and rather utilizes the full posterior.

\subsection{Optimizing Validation Set Performance}

A common approach to model selection is to split our data $\spD$ into separate training set $\spD^\train \defeq \{(\vx_i^\train, y_i^\train)\}_{i=1}^n$ and validation sets $\spD^\val \defeq \{(\vx_i^\val, y_i^\val)\}_{i=1}^m$.
We then optimize the model for a parameter candidate $\vtheta_j$ using the training set.
This is usually done by picking a point estimate (like the MAP estimate), \begin{align}
  \hat{f}_j \defeq \argmax_{f } p(f \mid \vx_{1:n}^\train, y_{1:n}^\train).
\end{align}
Then, we score $\vtheta_j$ according to the performance of $\hat{f}_j$ on the validation set,\looseness=-1 \begin{align}
  \vthetahat \defeq \argmax_{\vtheta_j} p(y_{1:m}^\val \mid \vx_{1:m}^\val, \hat{f_j}). \label{eq:cross_validation}
\end{align}
This ensures that $\hat{f}_j$ does not depend on $\spD^\val$.

\begin{rmk}{Approximating population risk}{}
  Why is it useful to separate the data into a training and a validation set?
  Recall from \cref{sec:fundamentals:supervised_learning:risk} that minimizing the empirical risk without separating training and validation data may lead to overfitting as both the loss and $\hat{f}_j$ depend on the same data~$\spD$.
  In contrast, using independent training and validation sets, $\hat{f}_j$ does not depend on $\spD^\val$, and we have that \begin{align}
    \frac{1}{m} \sum_{i=1}^m \ell(y_i^\val \mid \vx_i^\val, \hat{f}_j) \approx \E[(\vx,y)\sim\spP]{\ell(y \mid \vx, \hat{f}_j)},
  \end{align} using Monte Carlo sampling.\safefootnote{We generally assume $\spD \iid \spP$, in particular, we assume that the individual samples of the data are i.i.d.. Recall that in this setting, Hoeffding's inequality \eqref{eq:hoeffdings_inequality} can be used to gauge how large $m$ should be.}
  In words, for reasonably large $m$, minimizing the empirical risk as we do in \cref{eq:cross_validation} approximates minimizing the population risk.
\end{rmk}

While this approach often is quite effective at preventing overfitting as compared to using the same data for training and picking $\vthetahat$, it still collapses the uncertainty in $f$ into a point estimate.
Can we do better?

\subsection{Maximizing the Marginal Likelihood}\label{sec:gp:model_selection:marginal_likelihood}

We have already seen for Bayesian linear regression, that picking a point estimate loses a lot of information.
Instead of optimizing the effects of $\vtheta$ for a specific point estimate $\hat{f}$ of the model $f$, \midx{maximizing the marginal likelihood} optimizes the effects of $\vtheta$ across all realizations of $f$.
In this approach, we obtain our hyperparameter estimate via \begin{align}
  \vthetahat_\MLE &\defeq \argmax_{\vtheta} p(y_{1:n} \mid \vx_{1:n}, \vtheta) \margintag{using the definition of marginal likelihood in Bayes' rule \eqref{eq:bayes_rule}} \label{eq:maximizing_marginal_likelihood} \\
  &= \argmax_{\vtheta} \int p(y_{1:n}, f \mid \vx_{1:n}, \vtheta) \,d f \margintag{by conditioning on $f$ using the sum rule \eqref{eq:sum_rule}} \nonumber \\
  &= \argmax_{\vtheta} \int p(y_{1:n} \mid \vx_{1:n}, f, \vtheta) p(f \mid \vtheta) \,d f. \margintag{using the product rule \eqref{eq:product_rule}}
\end{align}
Remarkably, this approach typically avoids overfitting even though we do not use a separate training and validation set.
The following table provides an intuitive argument for why maximizing the marginal likelihood is a good strategy.

\begin{table}[h!]
  \centering
  \begin{tabular}{lll}
  \toprule
    & likelihood & prior \\
    \midrule
    \makecell[l]{``underfit'' model \\ (too simple $\vtheta$)} & small for ``almost all'' $f$ & large \\
    \addlinespace
    \makecell[l]{``overfit'' model \\ (too complex $\vtheta$)} & \makecell[l]{large for ``few'' $f$ \\ small for ``most'' $f$} & small \\
    \addlinespace
    ``just right'' & moderate for ``many'' $f$ & moderate \\
    \bottomrule
  \end{tabular}\\[11pt]
  \caption{The table gives an intuitive explanation of effects of parameter choices $\vtheta$ on the marginal likelihood.
  Note that words in quotation marks refer to intuitive quantities, as we have infinitely many realizations of $f$.}
\end{table}

\begin{marginfigure}[7\baselineskip]
  \incplt{maximizing_marginal_likelihood}
  \caption{A schematic illustration of the marginal likelihood of a simple, intermediate, and complex model across all possible data sets.}
\end{marginfigure}

For an ``underfit'' model, the likelihood is mostly small as the data cannot be well described, while the prior is large as there are ``fewer'' functions to choose from.
For an ``overfit'' model, the likelihood is large for ``some'' functions (which would be picked if we were only minimizing the training error and not doing cross validation) but small for ``most'' functions.
The prior is small, as the probability mass has to be distributed among ``more'' functions.
Thus, in both cases, one term in the product will be small.
Hence, maximizing the marginal likelihood naturally encourages trading between a large likelihood and a large prior.

In the context of Gaussian process regression, recall from \cref{eq:gp_predictive_distr} that \begin{align}
  y_{1:n} \mid \vx_{1:n}, \vtheta \sim \N{\vzero}{\mK_{f,\vtheta} + \sigman^2 \mI}
\end{align} where $\mK_{f,\vtheta}$ denotes the kernel matrix at the inputs $\vx_{1:n}$ depending on the kernel function parameterized by $\vtheta$.
We write $\mK_{\vy,\vtheta} \defeq \mK_{f,\vtheta} + \sigman^2 \mI$.
Continuing from \cref{eq:maximizing_marginal_likelihood}, we obtain \begin{align}
  \vthetahat_\MLE &= \argmax_{\vtheta} \N[\vy]{\vzero}{\mK_{\vy,\vtheta}} \nonumber \\
  &= \argmin_{\vtheta} \frac{1}{2} \transpose{\vy} \inv{\mK_{\vy,\vtheta}} \vy + \frac{1}{2} \log \det{\mK_{\vy,\vtheta}} + \frac{n}{2} \log 2 \pi \label{eq:gp_mle_1} \margintag{taking the negative logarithm} \\
  &= \argmin_{\vtheta} \frac{1}{2} \transpose{\vy} \inv{\mK_{\vy,\vtheta}} \vy + \frac{1}{2} \log \det{\mK_{\vy,\vtheta}} \label{eq:gp_mle_2} \margintag{the last term is independent of $\vtheta$}
\end{align}
The first term of the optimization objective describes the ``goodness of fit'' (i.e., the ``alignment'' of $\vy$ with $\mK_{\vy,\vtheta}$).
The second term characterizes the ``volume'' of the model class.
Thus, this optimization naturally trades the aforementioned objectives.

Marginal likelihood maximization is an empirical Bayes method.
Often it is simply referred to as \midx{empirical Bayes}.
It also has the nice property that the gradient of its objective (the MLL loss) can be expressed in closed-from \exerciserefmark{gradient_of_mll}, \begin{align}
  \pdv{}{\theta_j} \log p(y_{1:n} \mid \vx_{1:n}, \vtheta) = \frac{1}{2} \tr{(\valpha \transpose{\valpha} - \inv{\mK_{\vy,\vtheta}}) \pdv{\mK_{\vy,\vtheta}}{\theta_j}} \label{eq:gradient_mll}
\end{align} where $\valpha \defeq \inv{\mK_{\vy,\vtheta}} \vy$ and $\tr{\mM}$ is the trace of a matrix $\mM$.
This optimization problem is, in general, non-convex.
\Cref{fig:empirical_bayes} gives an example of two local optima according to empirical Bayes.

\begin{marginfigure}[4\baselineskip]
  \incplt{mll_kernel_progress}
  \caption{An example of model selection by maximizing the log likelihood (without hyperpriors) using a \textbf{\b{linear}}, \textbf{\r{quadratic}}, \textbf{\purple{Laplace}}, \textbf{\g{Matérn}} ($\nu=\nicefrac{3}{2}$), and \textbf{Gaussian} kernel, respectively.
  They are used to learn the function \begin{align*}
    x \mapsto \frac{\sin(x)}{x} + \varepsilon,\quad \varepsilon \sim \N{0}{0.01}
  \end{align*} using SGD with learning rate $0.1$.}
\end{marginfigure}

Taking a step back, observe that taking a probabilistic perspective on model selection naturally led us to consider all realizations of our model $f$ instead of using point estimates.
However, we are still using point estimates for our model parameters $\vtheta$.
Continuing on our probabilistic adventure, we could place a prior $p(\vtheta)$ on them too.\footnote{Such a prior is called \idx{hyperprior}[idxpagebf].}
We could use it to obtain the MAP estimate (still a point estimate!) which adds an additional regularization term \begin{align}
  \vthetahat_\MAP &\defeq \argmax_{\vtheta} p(\vtheta \mid \vx_{1:n}, y_{1:n}) \\
  &= \argmin_{\vtheta} - \log p(\vtheta) - \log p(y_{1:n} \mid \vx_{1:n}, \vtheta). \margintag{using Bayes' rule \eqref{eq:bayes_rule} and then taking the negative logarithm}
\end{align}

An alternative approach is to consider the full posterior distribution over parameters $\vtheta$.
The resulting predictive distribution is, however, intractable, \begin{align}
  p(\ys \mid \vxs, \vx_{1:n}, y_{1:n}) = \int\int p(\ys \mid \vxs, f) \cdot p(f \mid \vx_{1:n}, y_{1:n}, \vtheta) \cdot p(\vtheta) \,d f \,d\vtheta.
\end{align}
Recall that as the mode of Gaussians coincides with their mean, the MAP estimate corresponds to the mean of the predictive posterior.

As a final note, observe that in principle, there is nothing stopping us from descending deeper in the probabilistic hierarchy.
The prior on the model parameters $\vtheta$ is likely to have parameters too.
Ultimately, we need to break out of this hierarchy of dependencies and choose a prior.\looseness=-1

\begin{figure}
  \incplt{mll_non_convex}
  \caption{The top plot shows contour lines of an empirical Bayes with two local optima.
  The bottom two plots show the Gaussian processes corresponding to the two optimal models.
  The left model with smaller lengthscale is chosen within a more flexible class of models, while the right model explains more observations through noise.
  Adapted from figure 5.5 of \icite{gpml}.}\label{fig:empirical_bayes}
\end{figure}


\section{Approximations}\label{sec:gp:approximations}

To learn a Gaussian process, we need to invert $n \times n$ matrices, hence the computational cost is $\BigO{n^3}$.
Compare this to Bayesian linear regression which allows us to learn a regression model in $\BigO{n d^2}$ time (even online) where $d$ is the feature dimension.
It is therefore natural to look for ways of approximating a Gaussian process.

\subsection{Local Methods}

Recall that during forward sampling, we had to condition on a larger and larger number of previous samples.
When sampling at a location~$\vx$, a very simple approximation is to only condition on those samples~$\vxp$ that are ``close'' (where $\abs{k(\vx, \vxp)} \geq \tau$ for some $\tau > 0$).
Essentially, this method ``cuts off the tails'' of the kernel function $k$.
However, $\tau$ has to be chosen carefully as if $\tau$ is chosen too large, samples become essentially independent.

This is one example of a \emph{sparse approximation} of a Gaussian process.
We will discuss more advanced sparse approximations known as ``inducing point methods'' in \cref{sec:gp:approx:data_sampling}.

\subsection{Kernel Function Approximation}\label{sec:gp:approximations:kernel}

Another method is to approximate the kernel function directly.
The idea is to construct a ``low-dimensional'' feature map $\vphi : \R^d \to \R^m$ that approximates the kernel, \begin{align}
    k(\vx, \vxp) \approx \transpose{\vphi(\vx)} \vphi(\vxp).
\end{align}
Then, we can apply Bayesian linear regression, resulting in a time complexity of $\BigO{n m^2 + m^3}$.

One example of this approach are \midx{random Fourier features}, which we will discuss in the following.

\begin{marginfigure}[25\baselineskip]
  \incplt{eulers_formula}
  \caption{Illustration of Euler's formula.
  It can be seen that $e^{i \varphi}$ corresponds to a (counter-clockwise) rotation on the unit circle as $\varphi$ varies from $0$ to $2 \pi$.}\label{fig:eulers_formula}
\end{marginfigure}

\begin{rmk}{Fourier transform}{}
  First, let us remind ourselves of Fourier transformations.
  The Fourier transform is a method of decomposing frequencies into their individual components.

  Recall \midx{Euler's formula} which states that for any $x \in \R$, \begin{align}
    e^{i x} = \cos x + i \sin x \label{eq:eulers_formula}
  \end{align} where $i$ is the imaginary unit of complex numbers.
  The formula is illustrated in \cref{fig:eulers_formula}.
  Note that $e^{- i 2 \pi x}$ corresponds to rotating clockwise around the unit circle in $\R^2$ --- completing a rotation whenever $x \in \R$ reaches the next natural number.

  We can scale $x$ by a frequency $\xi$: $e^{- i 2 \pi \xi x}$.
  If $\vx \in \R^d$, we can also scale each component $j$ of $\vx$ by a different frequency $\vxi(j)$.
  Multiplying a function $f : \R^d \to \R$ with the rotation around the unit circle with given frequencies $\vxi$, yields a quantity that describes the amplitude of the frequencies $\vxi$, \begin{align}
      \hat{f}(\vxi) \defeq \int_{\R^d} f(\vx) e^{- i 2 \pi \transpose{\vxi} \vx} \,d\vx. \label{eq:fourier_transform}
  \end{align}
  $\hat{f}$ is called the \midx{Fourier transform} of $f$.
  $f$ is called the \midx{inverse Fourier transform} of $\hat{f}$, and can be computed using \begin{align}
    f(\vx) = \int_{\R^d} \hat{f}(\vxi) e^{i 2 \pi \transpose{\vxi} \vx} \,d\vxi.
  \end{align}
  It is common to write $\vomega \defeq 2\pi\vxi$.
  See \cref{fig:fourier_transform} for an example.

  Refer to \icite{3b1bfouriertransform} for a visual introduction.
\end{rmk}

\begin{marginfigure}
  \incplt{fourier_transform}
  \caption{The Fourier transform of a rectangular pulse, \begin{align*}
    f(x) \defeq \begin{cases}
      1 & x \in [-1, 1] \\
      0 & \text{otherwise},
    \end{cases}
  \end{align*} is given by \begin{align*}
    \hat{f}(\omega) &= \int_{-1}^1 e^{- i \omega x} \,dx = \frac{1}{i \omega} \parentheses*{e^{i \omega} - e^{- i \omega}} \\
    &= \frac{2 \sin(\omega)}{\omega}.
  \end{align*}}\label{fig:fourier_transform}
\end{marginfigure}

Because a stationary kernel $k : \R^d \times \R^d \to \R$ can be interpreted as a function in one variable, it has an associated Fourier transform which we denote by $p(\vomega)$.
That is, \begin{align}
  k(\vx-\vxp) = \int_{\R^d} p(\vomega) e^{i \transpose{\vomega} (\vx-\vxp)} \,d\vomega. \label{eq:spectral_density}
\end{align}

\begin{fct}[Bochner's theorem]\pidx{Bochner's theorem}
  A continuous stationary kernel on $\R^d$ is positive definite if and only if its Fourier transform $p(\vomega)$ is non-negative.
\end{fct}

Bochner's theorem implies that when a continuous and stationary kernel is positive definite and scaled appropriately, its Fourier transform~$p(\vomega)$ is a proper probability distribution.
In this case, $p(\vomega)$ is called the \midx{spectral density} of the kernel $k$.

\begin{rmk}{Eigenvalue spectrum of stationary kernels}{eigenvalue_spectrum_of_stationary_kernels}
  When a kernel $k$ is stationary (i.e., a univariate function of $\vx - \vxp$), its eigenfunctions (with respect to the usual Lebesgue measure) turn out to be the complex exponentials $\exp(i \transpose{\vomega} (\vx - \vxp))$.
  In simpler terms, you can think of these exponentials as ``building blocks'' at different frequencies $\vomega$.
  The spectral density $p(\vomega)$ associated with the kernel tells you how strongly each frequency contributes, i.e., how large the corresponding eigenvalue is.\looseness=-1

  A key insight of this analysis is that the rate at which these magnitudes $p(\vomega)$ decay with increasing frequency $\vomega$ reveals the smoothness of the processes governed by the kernel.
  If a kernel allocates more ``power'' to high frequencies (meaning the spectral density decays slowly), the resulting processes will appear ``rougher''.
  Conversely, if high-frequency components are suppressed, the process will appear ``smoother''.\looseness=-1

  For an in-depth introduction to the eigenfunction analysis of kernels, refer to section 4.3 of \icite{gpml}.
\end{rmk}

\begin{ex}{Spectral density of the Gaussian kernel}{}
  The Gaussian kernel with length scale $h$ has the spectral density \begin{align}
    p(\vomega) &= \int_{\R^d} k(\vx - \vxp; h) e^{-i \transpose{\vomega} (\vx-\vxp)} \,d(\vx-\vxp) \margintag{using the definition of the Fourier transform \eqref{eq:fourier_transform}} \nonumber \\
    &= \int_{\R^d} \exp\parentheses*{-\frac{\norm{\vx}_2^2}{2 h^2} - i \transpose{\vomega} \vx} \,d\vx \margintag{using the definition of the Gaussian kernel \eqref{eq:gaussian_kernel}} \nonumber \\
    &= (2 h^2 \pi)^{\nicefrac{d}{2}} \exp\parentheses*{-h^2 \frac{\norm{\vomega}_2^2}{2}}. \label{eq:spectral_density_gaussian_kernel}
  \end{align}
\end{ex}

The key idea is now to interpret the kernel as an expectation, \begin{align}
  k(\vx-\vxp) &= \int_{\R^d} p(\vomega) e^{i \transpose{\vomega} (\vx-\vxp)} \,d\vomega \margintag{from \cref{eq:spectral_density}} \nonumber \\
  &= \E[\vomega \sim p]{e^{i \transpose{\vomega}(\vx-\vxp)}} \margintag{by the definition of expectation \eqref{eq:expectation}} \nonumber \\
  &= \E[\vomega \sim p]{\cos(\transpose{\vomega}\vx-\transpose{\vomega}\vxp) + i\sin(\transpose{\vomega}\vx - \transpose{\vomega}\vxp)}. \margintag{using Euler's formula \eqref{eq:eulers_formula}} \nonumber \\
\intertext{Observe that as both $k$ and $p$ are real, convergence of the integral implies $\E[\vomega \sim p]{\sin(\transpose{\vomega}\vx - \transpose{\vomega}\vxp)} = 0$. Hence,}
  &= \E[\vomega \sim p]{\cos(\transpose{\vomega}\vx-\transpose{\vomega}\vxp)} \nonumber \\
  &= \E*[\vomega \sim p]{\E[b \sim \Unif{[0, 2\pi]}]{\cos((\transpose{\vomega}\vx + b)-(\transpose{\vomega}\vxp + b))}} \margintag{expanding with $b - b$} \nonumber \\
  &= \begin{multlined}[t]
    \E*[\vomega \sim p]{\E*[b \sim \Unif{[0, 2\pi]}]{\left[\cos(\transpose{\vomega}\vx + b) \cos(\transpose{\vomega}\vxp + b) \right. \\ \left. + \sin(\transpose{\vomega}\vx + b) \sin(\transpose{\vomega}\vxp + b)\right]}}
  \end{multlined} \margintag{using the angle subtraction identity, $\cos(\alpha - \beta) = \cos\alpha\cos\beta + \sin\alpha\sin\beta$} \nonumber \\
  &= \E*[\vomega \sim p]{\E[b \sim \Unif{[0, 2\pi]}]{2 \cos(\transpose{\vomega}\vx + b) \cos(\transpose{\vomega}\vxp + b)}} \margintag{using \begin{align*}
    &\E[b]{\cos(\alpha + b) \cos(\beta + b)} \\
    &= \E[b]{\sin(\alpha + b) \sin(\beta + b)}
  \end{align*} for $b \sim \Unif{[0, 2\pi]}$} \nonumber \\
  &= \E[\vomega \sim p, b \sim \Unif{\brackets{0, 2 \pi}}]{z_{\vomega,b}(\vx) \cdot z_{\vomega,b}(\vxp)}
\intertext{where $z_{\vomega,b}(\vx) \defeq \sqrt{2} \cos(\transpose{\vomega} \vx + b)$,}
  &\approx \frac{1}{m} \sum_{i=1}^m z_{\vomega^{(i)},b^{(i)}}(\vx) \cdot z_{\vomega^{(i)},b^{(i)}}(\vxp) \margintag{using Monte Carlo sampling to estimate the expectation, see \cref{ex:estimating_expectations}}
\intertext{for independent samples $\vomega^{(i)} \iid p$ and $b^{(i)} \iid \Unif{\brackets{0, 2 \pi}}$,}
  &= \transpose{\vz(\vx)} \vz(\vxp)
\end{align} where the (randomized) feature map of random Fourier features is \begin{align}
  \vz(\vx) \defeq \frac{1}{\sqrt{m}} \transpose{[z_{\vomega^{(1)},b^{(1)}}(\vx), \dots, z_{\vomega^{(m)},b^{(m)}}(\vx)]}.
\end{align}

Intuitively, each component of the feature map $\vz(\vx)$ projects $\vx$ onto a random direction $\vomega$ drawn from the (inverse) Fourier transform $p(\vomega)$ of $k(\vx-\vxp)$, and wraps this line onto the unit circle in $\R^2$.
After transforming two points $\vx$ and $\vxp$ in this way, their inner product is an unbiased estimator of $k(\vx - \vxp)$.
The mapping $z_{\vomega,b}(\vx) = \sqrt{2} \cos(\transpose{\vomega} \vx + b)$ additionally rotates the circle by a random amount $b$ and projects the points onto the interval $[0,1]$.

\begin{marginfigure}
  \incplt{rff}
  \caption{Example of random Fourier features with where the number of features $m$ is $5$ (top) and $10$ (bottom), respectively.
  The noise-free true function is shown in black and the mean of the Gaussian process is shown in blue.}
\end{marginfigure}

\cite{rff} show that Bayesian linear regression with the feature map $\vz$ approximates Gaussian processes with a stationary kernel:

\begin{thm}[Uniform convergence of Fourier features]\label{thm:uniform_convergence_fourier_features}
  Suppose $\spM$ is a compact subset of $\R^d$ with diameter $\mathrm{diam}(\spM)$.
  Then for a stationary kernel $k$, the random Fourier features $\vz$, and any $\epsilon > 0$ it holds that \begin{align}
    \begin{multlined}[t]
      \Pr{\sup_{\vx,\vxp \in \spM} \abs{\transpose{\vz(\vx)} \vz(\vxp) - k(\vx - \vxp)} \geq \epsilon} \\
      \leq 2^8 \parentheses*{\frac{\sigmap \mathrm{diam}(\spM)}{\epsilon}}^2 \exp\parentheses*{-\frac{m \epsilon^2}{8(d + 2)}}
    \end{multlined}
  \end{align} where $\sigmap^2 \defeq \E[\vomega \sim p]{\transpose{\vomega} \vomega}$ is the second moment of $p$, $m$ is the dimension of~$\vz(\vx)$, and $d$ is the dimension of $\vx$. \exerciserefmark{uniform_convergence_of_fourier_features}
\end{thm}

Note that the error probability decays exponentially fast in the dimension of the Fourier feature space.






\subsection{Data Sampling}\label{sec:gp:approx:data_sampling}

Another natural approach is to only consider a (random) subset of the training data during learning.
The naive approach is to subsample uniformly at random.
Not very surprisingly, we can do much better.

\begin{marginfigure}
  \incplt{inducing_points}
  \caption{Inducing points $\vu$ are shown as vertical dotted red lines.
  The noise-free true function is shown in black and the mean of the Gaussian process is shown in blue.
  Observe that the true function is approximated ``well'' around the inducing points.}
\end{marginfigure}

One subsampling method is the \midx{inducing points method} \citep{ip}.
The idea is to summarize the data around so-called inducing points.\footnote{The inducing points can be treated as hyperparameters.}
For now, let us consider an arbitrary set of inducing points, \begin{align*}
  \sU \defeq \{\overline{\vx}_1, \dots, \overline{\vx}_k\}.
\end{align*}

Then, the original Gaussian process can be recovered using marginalization,\looseness=-1 \begin{align}
  p(\fs, \vf) = \int_{\R^k} p(\fs, \vf, \vu) \,d\vu = \int_{\R^k} p(\fs, \vf \mid \vu) p(\vu) \,d\vu, \margintag{using the sum rule \eqref{eq:sum_rule} and product rule \eqref{eq:product_rule}}
\end{align} where $\vf \defeq \transpose{[f(\vx_1) \; \cdots \; f(\vx_n)]}$ and $\fs \defeq f(\vxs)$ at some evaluation point $\vxs \in \spX$.
We use $\vu \defeq \transpose{[f(\overline{\vx}_1) \; \cdots \; f(\overline{\vx}_k)]} \in \R^k$ to denote the predictions of the model at the inducing points $\sU$.
Due to the marginalization property of Gaussian processes \eqref{eq:gp_joint_distr}, we have that $\vu \sim \N{\vzero}{\mK_{\sU\sU}}$.
The key idea is to approximate the joint prior, assuming that $\fs$ and $\vf$ are conditionally independent given $\vu$, \begin{align}
  p(\fs, \vf) \approx \int_{\R^k} p(\fs \mid \vu) p(\vf \mid \vu) p(\vu) \,d\vu. \label{eq:inducing_points_joint_approx}
\end{align}
Here, $p(\vf \mid \vu)$ and $p(\fs \mid \vu)$ are commonly called the \midx{training conditional} and the \midx{testing conditional}, respectively.
Still denoting the observations by $\sA = \{\vx_1, \dots, \vx_n\}$ and defining $\star \defeq \{\vxs\}$, we know, using the closed-form expression for conditional Gaussians \eqref{eq:cond_gaussian}, \begin{subequations}\begin{align}
  p(\vf \mid \vu) &\sim \N[\vf]{\mK_{\sA\sU} \inv{\mK_{\sU\sU}} \vu}{\mK_{\sA\sA} - \mQ_{\sA\sA}}, \\
  p(\fs \mid \vu) &\sim \N[\fs]{\mK_{\star\sU} \inv{\mK_{\sU\sU}} \vu}{\mK_{\star\star} - \mQ_{\star\star}}
\end{align}\label{eq:inducing_point_conditionals}\end{subequations} where $\mQ_{a b} \defeq \mK_{a \sU} \inv{\mK_{\sU\sU}} \mK_{\sU b}$.
Intuitively, $\mK_{\sA\sA}$ represents the prior covariance and $\mQ_{\sA\sA}$ represents the covariance ``explained'' by the inducing points.\footnote{For more details, refer to section 2 of \icite{ip}.}\looseness=-1

Computing the full covariance matrix is expensive.
In the following, we mention two approximations to the covariance of the training conditional (and testing conditional).

\begin{ex}{Subset of regressors}{}
  The \midx{subset of regressors} (SoR) approximation is defined as \begin{subequations}\begin{align}
    q_{\mathrm{SoR}}(\vf \mid \vu) &\defeq \N[\vf]{\mK_{\sA\sU} \inv{\mK_{\sU\sU}} \vu}{\vzero}, \\
    q_{\mathrm{SoR}}(\fs \mid \vu) &\defeq \N[\fs]{\mK_{\star\sU} \inv{\mK_{\sU\sU}} \vu}{\vzero}.
  \end{align}\label{eq:sor}\end{subequations}
  Comparing to \cref{eq:inducing_point_conditionals}, SoR simply forgets about all variance and covariance.
\end{ex}

\begin{marginfigure}
  \incplt{inducing_points_sor_fitc}
  \caption{Comparison of SoR (top) and FITC (bottom).
  The inducing points $\vu$ are shown as vertical dotted red lines.
  The noise-free true function is shown in black and the mean of the Gaussian process is shown in blue.}
\end{marginfigure}

\begin{ex}{Fully independent training conditional}{}
  The \midx{fully independent training conditional} (FITC) approximation is defined as \begin{subequations}\begin{align}
    q_{\mathrm{FITC}}(\vf \mid \vu) &\defeq \N[\vf]{\mK_{\sA\sU} \inv{\mK_{\sU\sU}} \vu}{\diag{\mK_{\sA\sA} - \mQ_{\sA\sA}}}, \\
    q_{\mathrm{FITC}}(\fs \mid \vu) &\defeq \N[\fs]{\mK_{\star\sU} \inv{\mK_{\sU\sU}} \vu}{\diag{\mK_{\star\star} - \mQ_{\star\star}}}.
  \end{align}\end{subequations}
  In contrast to SoR, FITC keeps track of the variances but forgets about the covariance.
\end{ex}

The computational cost for inducing point methods SoR and FITC is dominated by the cost of inverting $\mK_{\sU\sU}$.
Thus, the time complexity is cubic in the number of inducing points, but only linear in the number of data points.

\section*{Discussion}

This chapter introduced Gaussian processes which leverage the function-space view on linear regression to perform exact probabilistic inference with flexible, nonlinear models.
A Gaussian process can be seen as a non-parametric model since it can represent an infinite-dimensional parameter space.
Instead, as we saw with the representer theorem, such non-parametric (i.e., ``function-space'') models are directly represented as functions of the data points.
While this can make these models more flexible than a simple linear parametric model in input space, it also makes them computationally expensive as the number of data points grows.
To this end, we discussed several ways of approximating Gaussian processes.

Nevertheless, for today's internet-scale datasets, modern machine learning typically relies on large parametric models that learn features from data.
These models can effectively amortize the cost of inference during training by encoding information into a fixed set of parameters.
In the following chapters, we will start to explore approaches to approximate probabilistic inference that can be applied to such models.

\excheading

\begin{nexercise}{Feature space of Gaussian kernel}{gaussian_kernel_feature_space}
  \begin{enumerate}[label=\arabic*.]
    \item Show that the univariate Gaussian kernel with length scale $h = 1$ implicitly defines a feature space with basis vectors \begin{align*}
      \vphi(x) = \begin{bmatrix}\phi_0(x) \\ \phi_1(x) \\ \vdots\end{bmatrix} \quad\text{with}\quad \phi_j(x) = \frac{1}{\sqrt{j!}} e^{-\frac{x^2}{2}} x^j.
    \end{align*}
    \textit{Hint: Use the Taylor series expansion of the exponential function, ${e^x = \sum_{j=0}^\infty \frac{x^j}{j!}}$.}

    \item Note that the vector $\vphi(x)$ is $\infty$-dimensional.
    Thus, taking the function-space view allows us to perform regression in an infinite-dimensional feature space.
    What is the effective dimension when regressing $n$ univariate data points with a Gaussian kernel?
  \end{enumerate}\vspace{5pt}
\end{nexercise}

\begin{nexercise}{Kernels on the circle}{kernels_on_the_circle}
  Consider a dataset $\{(\vx_i, y_i)\}_{i=1}^n$ with labels $y_i \in \R$ and inputs $\vx_i$ which lie on the unit circle $\mathbb{S} \subset \R^2$.
  In particular, any element of $\mathbb{S}$ can be identified with points in $\R^2$ of form $(\cos(\theta), \sin(\theta))$ or with the respective angles $\theta \in [0, 2 \pi)$.

  You now want to use GP regression to learn an unknown mapping from $\mathbb{S}$ to $\R$ using this dataset.
  Thus, you need a valid kernel $k: \mathbb{S} \times \mathbb{S} \to \R$.
  First, we look at kernels $k$ which can be understood as analogous to the Gaussian kernel.
  \begin{enumerate}
    \item You think of the ``extrinsic'' kernel $k_{\text{e}}: \mathbb{S} \times \mathbb{S} \to \R$ defined by
    \[
    k_{\text{e}}(\theta, \theta') \defeq \exp\left(-\frac{\|\vx(\theta)-\vx(\theta')\|_2^2}{2 \kappa^2}\right),
    \]
    where $\vx(\theta) \defeq (\cos(\theta), \sin(\theta))$.
    Is $k_{\text{e}}$ positive semi-definite for all values of $\kappa > 0$?

    \item Then, you think of an ``intrinsic'' kernel $k_{\text{i}}: \mathbb{S} \times \mathbb{S} \to \R$ defined by
    \[
    k_{\text{i}}(\theta, \theta') \defeq \exp\left(-\frac{d(\theta, \theta')^2}{2 \kappa^2}\right)
    \]
    where $d(\theta, \theta') \defeq \min(|\theta - \theta'|, |\theta - \theta' - 2 \pi|, |\theta - \theta' + 2 \pi|)$ is the standard arc length distance on the circle~$\mathbb{S}$.\par
    You would now like to test whether this kernel is positive semi-definite.
    We pick $\kappa=2$ and compute the kernel matrix $\mK$ for the points corresponding to the angles $\{0, \nicefrac{\pi}{2}, \pi, \nicefrac{3 \pi}{2} \}$.
    This kernel matrix $\mK$ has eigenvectors  $(1, 1, 1, 1)$ and $(-1, 1, -1, 1)$.\par
    Now compute the eigenvalue corresponding to the eigenvector $(-1, 1, -1, 1)$.

    \item Is $k_{\text{i}}$ positive semi-definite for $\kappa = 2$?

    \item A mathematician friend of yours suggests to you yet another kernel for points on the circle $\mathbb{S}$, called the \midx{heat kernel}.
    The kernel itself has a complicated expression but can be accurately approximated by
    \[
      k_h(\theta, \theta') \defeq \frac{1}{C_{\kappa}} \left( 1 + \sum_{l=1}^{L-1} e^{-\frac{\kappa^2}{2} l^2} 2 \cos(l (\theta - \theta')) \right),
    \]
    where $L \in \Nat$ controls the quality of approximation and $C_{\kappa} > 0$ is a normalizing constant that depends only on $\kappa$.\par
    Is $k_h$ is positive semi-definite for all values of $\kappa > 0$ and $L \in \Nat$?\par
    \textit{Hint: Recall that $\cos(a - b) = \cos(a)\cos(b) + \sin(a) \sin(b)$.}
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{A Kalman filter as a Gaussian process}{kf_as_gp}
  Next we will show that the Kalman filter from \cref{ex:kf_rand_walk_1d} can be seen as a Gaussian process.
  To this end, we define \begin{align}
    f : \Nat_0 \to \R, \quad t \mapsto X_t.
  \end{align}
  Assuming that $X_0 \sim \N{0}{\sigma_0^2}$ and $X_{t+1} \defeq X_t + \varepsilon_t$ with independent noise $\varepsilon_t \sim \N{0}{\sigma_x^2}$, show that \begin{align}
    f &\sim \GP{0}{k_{\mathrm{KF}}} \quad\text{where} \\
    k_{\mathrm{KF}}(t, t') &\defeq \sigma_0^2 + \sigma_x^2 \min\{t, t'\}.
  \end{align}

  This particular kernel $k(t, t') \defeq \min\{t, t'\}$ but over the continuous-time domain defines the \midx{Wiener process} (also known as Brownian motion).
\end{nexercise}

\begin{nexercise}{Reproducing property and RKHS norm}{reproducing_kernel_hilbert_space_properties}
  \begin{enumerate}
    \item Derive the reproducing property.
    \par\textit{Hint: Use $k(\vx,\vxp) = \ip{k(\vx,\cdot), k(\vxp,\cdot)}_k$.}

    \item Show that the RKHS norm $\norm{\cdot}_k$ is a measure of smoothness by proving that for any $f \in \spH_k(\spX)$ and $\vx,\vy \in \spX$ it holds that \begin{align*}
      |f(\vx) - f(\vy)| \leq \norm{f}_k \norm{k(\vx,\cdot) - k(\vy,\cdot)}_k.
    \end{align*}
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Representer theorem}{representer_theorem}
  With this, we can now derive the representer theorem~\eqref{eq:representer_theorem}.\par
  \textit{Hint: Recall \begin{enumerate}
    \item the reproducing property $f(\vx) = \ip{f, k(\vx, \cdot)}_k$ with $k(\vx, \cdot) \in \spH_k(\spX)$ which holds for all $f \in \spH_k(\spX)$ and $\vx \in \spH_k(\spX)$, and
    \item that the norm after projection is smaller or equal the norm before projection.
  \end{enumerate}
  Then decompose $f$ into parallel and orthogonal components with respect to $\mathrm{span}\{k(\vx_1, \cdot), \dots, k(\vx_n, \cdot)\}$.}
\end{nexercise}

\begin{nexercise}{MAP estimate of Gaussian processes}{mle_and_map_of_gps}
  Let us denote by $A = \{\vx_1, \dots, \vx_n\}$ the set of training points.
  We will now show that the MAP estimate of GP regression corresponds to the solution of the regularized linear regression problem in the RKHS stated in \cref{eq:map_of_gp}: \begin{align*}
    \hat{f} \defeq \argmin_{f \in \spH_k(\spX)} - \log p(y_{1:n} \mid \vx_{1:n}, f) + \frac{1}{2} \norm{f}_k^2.
  \end{align*}

  In the following, we abbreviate $\mK = \mK_{AA}$.
  We will also assume that the GP has a zero mean function.
  \begin{enumerate}
    \item Show that \cref{eq:map_of_gp} is equivalent to \begin{align}
      \valphahat \defeq \argmin_{\valpha \in \R^n} \norm{\vy - \mK \valpha}_2^2 + \lambda \norm{\valpha}_{\mK}^2 \label{eq:map_of_gp2}
    \end{align} for some $\lambda > 0$ which is also known as \midx{kernel ridge regression}.
    Determine $\lambda$.

    \item Show that \cref{eq:map_of_gp2} with the $\lambda$ determined in (1) is equivalent to the MAP estimate of GP regression.
    \par\textit{Hint: Recall from \cref{eq:gp_posterior} that the MAP estimate at a point $\vxs$ is $\E{\fs}[\vxs, \mX, \vy] = \transpose{\vk_{\vxs,A}}\inv{(\mK + \sigman^2 \mI)}\vy$.}
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Gradient of the marginal likelihood}{gradient_of_mll}
  In this exercise, we derive \cref{eq:gradient_mll}.

  Recall that we were considering a dataset $(\mX, \vy)$ of noise-perturbed evaluations $y_i = f(\vx_i) + \varepsilon_i$ where $\varepsilon_i \sim \N{0}{\sigman^2}$ and $f$ is an unknown function.
  We make the hypothesis $f \sim \GP{0}{k_{\vtheta}}$ with a zero mean function and the covariance function $k_{\vtheta}$.
  We are interested in finding the hyperparameters $\vtheta$ that maximize the marginal likelihood~${p(\vy \mid \mX, \vtheta)}$.

  \begin{enumerate}
    \item Derive \cref{eq:gradient_mll}. \\
    \textit{Hint: You can use the following identities: \begin{enumerate}
      \item for any invertible matrix $\mM$, \begin{align}
        \pdv{}{\theta_j} \inv{\mM} = -\inv{\mM} \pdv{\mM}{\theta_j} \inv{\mM} \quad\text{and} \label{eq:gradient_of_mll_hint1}
      \end{align}
      \item for any symmetric positive definite matrix $\mM$, \begin{align}
        \pdv{}{\theta_j} \log \det{\mM} = \tr{\inv{\mM} \pdv{\mM}{\theta_j}}. \label{eq:gradient_of_mll_hint2}
      \end{align}
    \end{enumerate}}

    \item Assume now that the covariance function for the noisy targets (i.e., including the noise contribution) can be expressed as \begin{align*}
      k_{\vy,\vtheta}(\vx, \vxp) = \theta_0 \tilde{k}(\vx, \vxp)
    \end{align*} where $\tilde{k}$ is a valid kernel independent of $\theta_0$.\safefootnote{That is, $\mK_{\vy,\vtheta}(i, j) = k_{\vy,\vtheta}(\vx_i, \vx_j)$.}

    Show that ${\pdv{}{\theta_0} \log p(\vy \mid \mX, \vtheta) = 0}$ admits a closed-form solution for $\theta_0$ which we denote by~$\opt{\theta_0}$.

    \item How should the optimal parameter $\opt{\theta_0}$ be scaled if we scale the labels $\vy$ by a scalar $s$?
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Uniform convergence of Fourier features}{uniform_convergence_of_fourier_features}
  In this exercise, we will prove \cref{thm:uniform_convergence_fourier_features}.

  Let $s(\vx, \vxp) \defeq \transpose{\vz(\vx)} \vz(\vxp)$ and $f(\vx, \vxp) \defeq s(\vx, \vxp) - k(\vx, \vxp)$.
  Observe that both functions are shift invariant, and we will therefore denote them as univariate functions with argument $\vDelta \equiv \vx - \vxp \in \spM_\Delta$.
  Notice that our goal is to bound the probability of the event $\sup_{\vDelta \in \spM_\Delta} |f(\vDelta)| \geq \epsilon$.

  \begin{enumerate}
    \item Show that for all $\vDelta \in \spM_\Delta$, $\Pr{|f(\vDelta)| \geq \epsilon} \leq 2 \exp\parentheses*{-\frac{m \epsilon^2}{4}}$.
  \end{enumerate}

  What we have derived in (1) is known as a \midx{pointwise convergence} guarantee.
  However, we are interested in bounding the \midx{uniform convergence} over the compact set $\spM_\Delta$.

  Our approach will be to ``cover'' the compact set $\spM_\Delta$ using $T$ balls of radius $r$ whose centers we denote by $\{\vDelta_i\}_{i=1}^T$.
  It can be shown that this is possible for some $T \leq (4 \; \mathrm{diam}(\spM) / r)^d$.
  It can furthermore be shown that \begin{align*}
    \text{$\forall i$. $|f(\vDelta_i)| < \frac{\epsilon}{2}$ and $\norm{\grad f(\opt{\vDelta})}_2 < \frac{\epsilon}{2 r}$} \implies \sup_{\vDelta \in \spM_\Delta} |f(\vDelta)| < \epsilon
  \end{align*} where $\opt{\vDelta} = \argmax_{\vDelta \in \spM_\Delta} \norm{\grad f(\vDelta)}_2$.

  \begin{enumerate}
    \setcounter{enumi}{1}
    \item Prove $\Pr{\norm{\grad f(\opt{\vDelta})}_2 \geq \frac{\epsilon}{2 r}} \leq \parentheses*{\frac{2 r \sigmap}{\epsilon}}^2$.\par
    \textit{Hint: Recall that the random Fourier feature approximation is unbiased, i.e., $\E{s(\vDelta)} = k(\vDelta)$.}

    \item Prove $\Pr{\bigcup_{i=1}^T |f(\vDelta_i)| \geq \frac{\epsilon}{2}} \leq 2 T \exp\parentheses*{-\frac{m \epsilon^2}{16}}$.

    \item Combine the results from (2) and (3) to prove \cref{thm:uniform_convergence_fourier_features}.\par
    \textit{Hint: You may use that \begin{enumerate}
      \item $\alpha r^{-d} + \beta r^2 = 2 \beta^{\frac{d}{d+2}} \alpha^{\frac{2}{d+2}}$ for $r = (\alpha / \beta)^{\frac{1}{d+2}}$ and
      \item $\frac{\sigmap \mathrm{diam}(\spM)}{\epsilon} \geq 1$.
    \end{enumerate}}

    \item Show that for the Gaussian kernel \eqref{eq:gaussian_kernel}, $\sigmap^2 = \frac{d}{h^2}$.\par
    \textit{Hint: First show $\sigmap^2 = - \tr{\hes_{\vDelta} k(\vzero)}$.}
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Subset of regressors}{sor}
  \begin{enumerate}
    \item Using an SoR approximation, prove the following: \begin{align}
      q_{\mathrm{SoR}}(\vf, \fs) &= \N*[\begin{bmatrix}
        \vf \\
        \fs
      \end{bmatrix}]{\vzero}{\begin{bmatrix}
        \mQ_{\sA\sA} & \mQ_{\sA\star} \\
        \mQ_{\star\sA} & \mQ_{\star\star} \\
      \end{bmatrix}} \\
      q_{\mathrm{SoR}}(\fs \mid \vy) &= \N[\fs]{\mQ_{\star\sA}\inv{\Tilde{\mQ}_{\sA\sA}}\vy}{\mQ_{\star\star} - \mQ_{\star\sA}\inv{\Tilde{\mQ}_{\sA\sA}}\mQ_{\sA\star}}
    \end{align} where $\Tilde{\mQ}_{a b} \defeq \mQ_{a b} + \sigman^2$.

    \item Derive that the resulting model is a degenerate Gaussian process with covariance function \begin{align}
      k_{\mathrm{SoR}}(\vx,\vxp) \defeq \transpose{\vk_{\vx,\sU}} \inv{\mK_{\sU\sU}} \vk_{\vxp,\sU}.
    \end{align}
  \end{enumerate}
\end{nexercise}
