\chapter{Markov Decision Processes}\label{sec:mdp}

We will now turn to the topic of probabilistic planning.\pidx{planning}[idxpagebf]
Planning deals with the problem of deciding which action an agent should play in a (stochastic) environment.\footnote{An environment is \idx<stochastic>{stochastic environment} as opposed to deterministic, when the outcome of actions is random.}
A key formalism for probabilistic planning in \emph{known} environments are so-called Markov decision processes.
Starting from the next chapter, we will look at reinforcement learning, which extends probabilistic planning to unknown environments.

Consider the setting where we have a sequence of states $(X_t)_{t\in\Nat_0}$ similarly to Markov chains.
But now, the next state $X_{t+1}$ of an agent does not only depend on the previous state $X_t$ but also depends on the last action $A_t$ of this agent.

\begin{marginfigure}
  \incfig{mdp}
  \caption{Directed graphical model of a Markov decision process with hidden states $X_t$ and actions $A_t$.}
\end{marginfigure}

\begin{defn}[(Finite) Markov decision process, MDP]\pidx{Markov decision process}
  A \emph{(finite) Markov decision process} is specified by \begin{itemize}
    \item a (finite) set of \emph{states} $\sX \defeq \{1, \dots, n\}$,
    \item a (finite) set of \emph{actions} $\sA \defeq \{1, \dots, m\}$,
    \item \emph{transition probabilities} \begin{align}
      p(x' \mid x, a) \defeq \Pr{X_{t+1} = x' \mid X_t = x, A_t = a}
    \end{align} which is also called the \midx{dynamics model}, and
    \item a \emph{reward function} $r : X \times A \to \R$ which maps the current state $x$ and an action $a$ to some reward.
  \end{itemize}
\end{defn}

The reward function may also depend on the next state $x'$, however, we stick to the above model for simplicity.
Also, the reward function can be random with mean $r$.
Observe that $r$ induces the sequence of rewards $(R_t)_{t\in\Nat_0}$, where \begin{align}
  R_t \defeq r(X_t, A_t),
\end{align} which is sometimes used in the literature instead of $r$.

Crucially, we assume the dynamics model $p$ and the reward function~$r$ to be known.
That is, we operate in a known environment.
For now, we also assume that the environment is \midx<fully observable>{fully observable environment}.
In other words, we assume that our agent knows its current state.
In \cref{sec:mdp:partial_observability}, we discuss how this method can be extended to the partially observable setting.

Our fundamental objective is to learn how the agent should behave to optimize its reward.
In other words, given its current state, the agent should decide (optimally) on the action to play.
Such a decision map --- whether optimal or not --- is called a policy.

\begin{defn}[Policy]\pidx{policy}
  A \emph{policy} is a function that maps each state $x \in \sX$ to a probability distribution over the actions. That is, for any ${t > 0}$,\looseness=-1 \begin{align}
    \pi(a \mid x) \defeq \Pr{A_t = a \mid X_t = x}.
  \end{align}
  In other words, a policy assigns to each action $a \in \sA$, a probability of being played given the current state $x \in \sX$.
\end{defn}

We assume that policies are stationary, that is, do not change over time.

\begin{rmk}{Stochastic policies}{}
  We will see later in this chapter that in fully observable environments optimal policies are always deterministic.
  Thus, there is no need to consider stochastic policies in the context of Markov decision processes.
  For this chapter, you can think of a policy $\pi$ simply as a deterministic mapping $\pi : \sX \to \sA$ from current state to played action.
  In the context of reinforcement learning, we will later see in \cref{sec:mfarl:actor_critic_methods:randomized_policies} that randomized policies are important in trading exploration and exploitation.
\end{rmk}

Observe that a policy induces a Markov chain $(X_t^\pi)_{t\in\Nat_0}$ with transition probabilities,\looseness=-1 \begin{align}
  p^\pi(x' \mid x) \defeq \Pr{X_{t+1}^\pi = x' \mid X_t^\pi = x} = \sum_{a \in \sA} \pi(a \mid x) p(x' \mid x, a).
\end{align}
This is crucial: if our agent follows a fixed policy (i.e., decision-making protocol) then the evolution of the process is described fully by a Markov chain.

As mentioned, we want to maximize the reward.
There are many models of calculating a score from the infinite sequence of rewards~$(R_t)_{t\in\Nat_0}$.
For the purpose of our discussion of Markov decision processes and reinforcement learning, we will focus on a very common reward called discounted payoff.

\begin{defn}[Discounted payoff]\pidx{discounted payoff}[idxpagebf]
  The \emph{discounted payoff} (also called \emph{discounted total reward}) from time $t$ is defined as the random variable, \begin{align}
    G_t \defeq \sum_{m=0}^\infty \gamma^m R_{t+m} \label{eq:discounted_payoff}
  \end{align} where $\gamma \in [0, 1)$ is the \emph{discount factor}.
\end{defn}

\begin{rmk}{Other reward models}{other_reward_models}
  Other well-known methods for combining rewards into a score are \begin{align*}
    \overset{\text{(\midx{instantaneous})}}{\vphantom{\sum_{m=0}^M}G_t \defeq R_t},\quad \overset{\text{(\midx{finite-horizon})}}{G_t \defeq \sum_{m=0}^{T-1} R_{t+m}},\quad\text{and}\quad \overset{\text{(\midx{mean payoff})}}{G_t \defeq \liminf_{T\to\infty} \frac{1}{T} \sum_{m=0}^T R_{t+m}}.
  \end{align*}
  The methods that we will discuss can also be analyzed using these or other alternative reward models.
\end{rmk}

We now want to understand the effect of the starting state and initial action on our optimization objective $G_t$.
To analyze this, it is common to use the following two functions:

\begin{defn}[State value function]\pidx{state value function}
  The \emph{state value function},\footnote{Recall that following a fixed policy $\pi$ induces a Markov chain $(X_t^\pi)_{t\in\Nat_0}$. We define \begin{align}
    \E[\pi]{\cdot} \defeq \E[(X_t^\pi)_{t\in\Nat_0}]{\cdot} \label{eq:expectation_over_policy}
  \end{align} as an expectation over all possible sequences of states $(x_t)_{t\in\Nat_0}$ within this Markov chain.} \begin{align}
    \v[\pi]{x}[t] \defeq \E[\pi]{G_t \mid X_t = x}, \label{eq:value_function}
  \end{align} measures the average discounted payoff from time $t$ starting from state~${x \in \sX}$.\looseness=-1
\end{defn}
\begin{defn}[State-action value function]\pidx{state-action value function}
  The \emph{state-action value function} (also called \idx{Q-function}), \begin{align}
    \q[\pi]{x}{a}[t] &\defeq \E[\pi]{G_t \mid X_t = x, A_t = a} \label{eq:q_function1} \\
    &= r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \cdot \v[\pi]{x'}[t+1], \label{eq:q_function2} \margintag{\normalfont by expanding the defition of the discounted payoff \eqref{eq:discounted_payoff}; corresponds to one step in the induced Markov chain}
  \end{align} measures the average discounted payoff from time $t$ starting from state~${x \in \sX}$ and with playing action ${a \in \sA}$.
  In other words, it combines the immediate return with the value of the next states.
\end{defn}

Note that both $\v[\pi]{x}[t]$ and $\q[\pi]{x}{a}[t]$ are deterministic scalar-valued functions.
Because we assumed stationary dynamics, rewards, and policies, the discounted payoff starting from a given state $x$ will be independent of the start time $t$.
Thus, we write ${\v[\pi]{x} \defeq \v[\pi]{x}[0]}$ and ${\q[\pi]{x}{a} \defeq \q[\pi]{x}{a}[0]}$ without loss of generality.


\section{Bellman Expectation Equation}

Let us now see how we can compute the value function, \begin{align}
  \v[\pi]{x} &= \E[\pi]{G_0 \mid X_0 = x} \margintag{using the definition of the value function \eqref{eq:value_function}} \nonumber \\
  &= \E[\pi]{\sum_{m=0}^\infty \gamma^m R_m}[X_0 = x] \margintag{using the definition of the discounted payoff \eqref{eq:discounted_payoff}} \nonumber \\
  &= \begin{multlined}[t]
  \E[\pi]{\gamma^0 R_0}[X_0 = x] + \gamma \E[\pi]{\sum_{m=0}^\infty \gamma^m R_{m+1}}[X_0 = x]
  \end{multlined} \margintag{using linearity of expectation \eqref{eq:linearity_expectation}} \nonumber \\
  &= \begin{multlined}[t]
  r(x, \pi(x)) + \gamma \E[x']{\E[\pi]{\sum_{m=0}^\infty \gamma^m R_{m+1}}[X_1 = x']}[X_0 = x]
  \end{multlined} \margintag{by simplifying the first expectation and conditioning the second expectation on $X_1$} \nonumber \\
  &= \begin{multlined}[t]
  r(x, \pi(x)) + \gamma \sum_{x' \in \sX} p(x' \mid x, \pi(x)) \E[\pi]{\sum_{m=0}^\infty \gamma^m R_{m+1}}[X_1 = x']
  \end{multlined} \margintag{expanding the expectation on $X_1$ and using conditional independence of the discounted payoff of $X_0$ given $X_1$} \nonumber \\
  &= \begin{multlined}[t]
  r(x, \pi(x)) + \gamma \sum_{x' \in \sX} p(x' \mid x, \pi(x)) \E[\pi]{\sum_{m=0}^\infty \gamma^m R_m}[X_0 = x']
  \end{multlined} \margintag{shifting the start time of the discounted payoff using stationarity} \nonumber \\
  &= r(x, \pi(x)) + \gamma \sum_{x' \in \sX} p(x' \mid x, \pi(x)) \E[\pi]{G_0 \mid X_0 = x'} \margintag{using the definition of the discounted payoff \eqref{eq:discounted_payoff}} \nonumber \\
  &= r(x, \pi(x)) + \gamma \sum_{x' \in \sX} p(x' \mid x, \pi(x)) \cdot \v[\pi]{x'}. \margintag{using the definition of the value function \eqref{eq:value_function}} \label{eq:bellman_expectation_equation} \\
  &= r(x, \pi(x)) + \gamma \E[x' \mid x, \pi(x)]{\v[\pi]{x'}}. \label{eq:bellman_expectation_equation_short} \margintag{interpreting the sum as an expectation}
\end{align}
This equation is known as the \midx{Bellman expectation equation}, and it shows a recursive dependence of the value function on itself.
The intuition is clear: the value of the current state corresponds to the reward from the next action plus the discounted sum of all future rewards obtained from the subsequent states.

For stochastic policies, the above calculation can be extended to yield, \begin{align}
  \v[\pi]{x} &= \sum_{a \in \sA} \pi(a \mid x) \parentheses*{r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \v[\pi]{x'}} \\
  &= \E[a \sim \pi(x)]{r(x, a) + \gamma \E[x' \mid x, a]{\v[\pi]{x'}}} \\
  &= \E[a \sim \pi(x)]{\q[\pi]{x}{a}}. \label{eq:v_as_q}
\end{align}
For stochastic policies, by also conditioning on the first action, one can obtain an analogous equation for the state-action value function, \begin{align}
  \q[\pi]{x}{a} &= r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \sum_{a' \in \sA} \pi(a' \mid x') \q[\pi]{x'}{a'} \label{eq:q_function3} \\
  &= r(x, a) + \gamma \E*[x' \mid x, a]{\E[a' \sim \pi(x')]{\q[\pi]{x'}{a'}}}.
\end{align}
Note that it does not make sense to consider a similar recursive formula for the state-action value function in the setting of deterministic policies as the action played when in state $x \in \sX$ is uniquely determined as $\pi(x)$.
In particular, \begin{align}
  \v[\pi]{x} = \q[\pi]{x}{\pi(x)}. \label{eq:svf_as_savf}
\end{align}

\begin{figure}
  \incfig{mdp_example}
  \caption{Example of an MDP, which we study in \cref{exercise:value_functions}. Suppose you are building a company. The shown MDP models ``how to become rich and famous''. Here, the action $S$ is short for \emph{saving} and the action $A$ is short for \emph{advertising}.

  Suppose you begin by being ``poor and unknown''. Then, the greedy action (i.e., the action maximizing instantaneous reward) is to save. However, within this simplified environment, saving when you are poor and unknown means that you will remain poor and unknown forever. As the potential rewards in other states are substantially larger, this simple example illustrates that following the greedy choice is generally not optimal.

  The example is adapted from Andrew Moore's lecture notes on MDPs~\citep{moore2002markov}.}\label{fig:mdp_example}
\end{figure}

\section{Policy Evaluation}\label{sec:mdp:policy_evaluation}

Bellman's expectation equation tells us how we can find the value function $\fnv[\pi]$ of a fixed policy $\pi$ using a system of linear equations!
Using,\looseness=-1 \begin{equation}\begin{gathered}
  \vv^\pi \defeq \begin{bmatrix}
    \v[\pi]{1} \\
    \vdots \\
    \v[\pi]{n}
  \end{bmatrix}, \quad \vr^\pi \defeq \begin{bmatrix}
    r(1, \pi(1)) \\
    \vdots \\
    r(n, \pi(n))
  \end{bmatrix}, \quad\text{and} \\ \mP^\pi \defeq \begin{bmatrix}
    p(1 \mid 1, \pi(1)) & \cdots & p(n \mid 1, \pi(1)) \\
    \vdots & \ddots & \vdots \\
    p(1 \mid n, \pi(n)) & \cdots & p(n \mid n, \pi(n))
  \end{bmatrix}
\end{gathered}\end{equation} and a little bit of linear algebra, the Bellman expectation equation~\eqref{eq:bellman_expectation_equation} is equivalent to \begin{align}
  &&\vv^\pi &= \vr^\pi + \gamma \mP^\pi \vv^\pi \label{eq:bellman_expectation_equation_matrix_form} \\
  \iff &&(\mI - \gamma \mP^\pi) \vv^\pi &= \vr^\pi  \nonumber \\
  \iff &&\vv^\pi &= \inv{(\mI - \gamma \mP^\pi)} \vr^\pi.
\end{align}

Solving this linear system of equations (i.e., performing matrix inversion) takes cubic time in the size of the state space.

\subsection{Fixed-point Iteration}

To obtain an (approximate) solution of $\vv^\pi$, we can use that it is the unique fixed-point of the affine mapping $\mB^\pi : \R^n \to \R^n$, \begin{align}
  \mB^\pi \vv \defeq \vr^\pi + \gamma \mP^\pi \vv. \label{eq:value_function_fixed_point_helper}
\end{align}
Using this fact (which we will prove in just a moment), we can use fixed-point iteration of $\mB^\pi$.

\begin{algorithm}[H]
  \caption{Fixed-point iteration}\label{alg:fixed_point_iteration}\pidx{fixed-point iteration}
  initialize $\vv^\pi$ (e.g., as $\vzero$)\;
  \For{$t = 1$ \KwTo $T$}{
    $\vv^\pi \gets \mB^\pi \vv^\pi = \vr^\pi + \gamma \mP^\pi \vv^\pi$\;
  }
\end{algorithm}

Fixed-point iteration has computational advantages, for example, for sparse transitions.

\begin{thm}
  $\vv^\pi$ is the unique fixed-point of $\mB^\pi$.
\end{thm}
\begin{proof}
  It is immediate from Bellman's expectation equation \eqref{eq:bellman_expectation_equation_matrix_form} and the definition of $\mB^\pi$ \eqref{eq:value_function_fixed_point_helper} that $\vv^\pi$ is a fixed-point of $\mB^\pi$.
  To prove uniqueness, we will show that $\mB^\pi$ is a contraction.

  \begin{rmk}{Contractions}{}
    A \midx{contraction} is a concept from topology.
    In a Banach space $(\spX, \norm{\cdot})$ (a metric space with a norm), $f : \spX \to \spX$ is a contraction iff there exists a $k < 1$ such that \begin{align}
      \norm{f(\vx) - f(\vy)} \leq k \cdot \norm{\vx - \vy} \label{eq:contraction}
    \end{align} for any $\vx, \vy \in \spX$.
    By the \midx{Banach fixed-point theorem}, a contraction admits a unique fixed-point.
    Intuitively, by iterating the function $f$, the distance to any fixed-point shrinks by a factor $k$ in each iteration, hence, converges to $0$.
    As we cannot converge to multiple fixed-points simultaneously, the fixed-point of a contraction $f$ must be unique.
  \end{rmk}

  Let $\vv \in \R^n$ and $\vvp \in \R^n$ be arbitrary initial guesses.
  We use the $L_\infty$ space,\footnote[][-2\baselineskip]{The \idx{$L_\infty$ norm} (also called \idx{supremum norm}) is defined as \begin{align}
    \norm{\vx}_\infty \defeq \max_i \abs{\vx(i)}. \label{eq:infinity_norm}
  \end{align}} \begin{align}
    \norm{\mB^\pi \vv - \mB^\pi \vvp}_\infty &= \norm{\vr^\pi + \gamma \mP^\pi \vv - \vr^\pi - \gamma \mP^\pi \vvp}_\infty \margintag{using the definition of $\mB^\pi$ \eqref{eq:value_function_fixed_point_helper}} \nonumber \\
    &= \gamma \norm{\mP^\pi(\vv-\vvp)}_\infty \nonumber \\
    &\leq \gamma \max_{\vx \in \sX} \sum_{x' \in \sX} p(x' \mid x, \pi(x)) \cdot \abs{\vv(x') - \vvp(x')}. \margintag{using the definition of the $L_\infty$ norm \eqref{eq:infinity_norm}, expanding the multiplication, and using $\abs{\sum_i a_i} \leq \sum_i \abs{a_i}$} \nonumber \\[5pt]
    &\leq \gamma \norm{\vv-\vvp}_\infty. \label{eq:value_function_contraction} \margintag{using $\sum_{x' \in \sX} p(x' \mid x, \pi(x)) = 1$ and $\abs{\vv(x')-\vvp(x')} \leq \norm{\vv-\vvp}_\infty$}
  \end{align}
  Thus, by \cref{eq:contraction}, $\mB^\pi$ is a contraction and by Banach's fixed-point theorem $\vv^\pi$ is its unique fixed-point.
\end{proof}

Let $\vv_t^\pi$ be the value function estimate after $t$ iterations.
Then, we have for the convergence of fixed-point iteration, \begin{align}
  \norm{\vv_t^\pi - \vv^\pi}_\infty &= \norm{\mB^\pi \vv_{t-1}^\pi - \mB^\pi \vv^\pi}_\infty \margintag{using the update rule of fixed-point iteration and $\mB^\pi \vv^\pi = \vv^\pi$} \nonumber \\[5pt]
  &\leq \gamma \norm{\vv_{t-1}^\pi - \vv^\pi}_\infty \margintag{using \eqref{eq:value_function_contraction}} \nonumber \\
  &= \gamma^t \norm{\vv_0^\pi - \vv^\pi}_\infty. \margintag{by induction} \label{eq:fixed_point_iteration_convergence}
\end{align}
This shows that fixed-point iteration converges to $\vv^\pi$ exponentially fast.\looseness=-1

\section{Policy Optimization}

Recall that our goal was to find an optimal policy, \begin{align}
  \pis \defeq \argmax_\pi \E[\pi]{G_0}.
\end{align}

We can alternatively characterize an optimal policy as follows: We define a partial ordering over policies by \begin{align}
  \pi \geq \pi' \overset{\cdot}{\iff} \v[\pi]{x} \geq \v[\pi']{x} \quad (\forall x \in \sX).
\end{align}
$\pis$ is then simply a policy which is maximal according to this partial ordering.\looseness=-1

It follows that all optimal policies have identical value functions.
Subsequently, we use $\fnv[\star] \defeq \fnv[\pis]$ and $\fnq[\star] \defeq \fnq[\pis]$ to denote the state value function and state-action value function arising from an optimal policy, respectively.
As an optimal policy maximizes the value of each state, we have that \begin{align}
  \v*{x} = \max_\pi \v[\pi]{x}, \quad \q*{x}{a} = \max_\pi \q[\pi]{x}{a}. \label{eq:opt_q_function}
\end{align}

Simply optimizing over each policy is not a good idea as there are $m^n$ deterministic policies in total.
It turns out that we can do much better.

\subsection{Greedy Policies}

Consider a policy that acts greedily according to the immediate return.
It is fairly obvious that this policy will not perform well because the agent might never get to high-reward states.
But what if someone could tell us not just the immediate return, but the long-term value of the states our agent can reach in a single step?
If we knew the value of each state our agent can reach, then we can simply pick the action that maximizes the expected value.
We will make this approach precise in the next section.

This thought experiment suggests the definition of a greedy policy with respect to a value function.

\begin{defn}[Greedy policy]\pidx{greedy policy}
  The \emph{greedy policy} with respect to a state-action value function $\fnq$ is defined as \begin{align}
    \pi_{\fnq}(x) \defeq \argmax_{a \in \sA} \q{x}{a}. \label{eq:greedy_policy1}
  \end{align}
  Analogously, we define the \emph{greedy policy} with respect to a state value function $\fnv$, \begin{align}
    \pi_{\fnv}(x) \defeq \argmax_{a \in \sA} r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \cdot \v{x'}. \label{eq:greedy_policy2}
  \end{align}
\end{defn}
We can use $\fnv$ and $\fnq$ interchangeably \exerciserefmark{v_and_q_functions}.

\subsection{Bellman Optimality Equation}

Observe that following the greedy policy $\pi_{\fnv}$, will lead us to a new value function $\fnv[\pi_{\fnv}]$.
With respect to this value function, we can again obtain a greedy policy, of which we can then obtain a new value function.
In this way, the correspondence between greedy policies and value functions induces a cyclic dependency, which is visualized in~\cref{fig:value_function_greedy_policy_cycle}.\looseness=-1

It turns out that the optimal policy $\pis$ is a fixed-point of this dependency.
This is made precise by the following theorem.

\begin{marginfigure}
  \incfig{bellman_optimality_equation}
  \caption{Cyclic dependency between \textbf{\b{value function}} and \textbf{\r{greedy policy}}.}\label{fig:value_function_greedy_policy_cycle}
\end{marginfigure}

\begin{thm}[Bellman's theorem]\pidx{Bellman's theorem}
  A policy $\pis$ is optimal iff it is greedy with respect to its own value function.
  In other words, $\pis$ is optimal iff $\pis(x)$ is a distribution over the set $\argmax_{a \in \sA} \q*{x}{a}$.
\end{thm}

In particular, if for every state there is a unique action that maximizes the state-action value function, the policy $\pis$ is deterministic and unique, \begin{align}
  \pis(x) = \argmax_{a \in \sA} \q*{x}{a}. \label{eq:bop1}
\end{align}

\begin{proof}
  It is a direct consequence of \cref{eq:opt_q_function} that a policy is optimal iff it is greedy with respect to $\fnq[\star]$.
\end{proof}

This theorem confirms our intuition from the previous section that greedily following an optimal value function is itself optimal.
In particular, Bellman's theorem shows that there always exists an optimal policy which is deterministic and stationary.

We have seen, that $\pis$ is a fixed-point of greedily picking the best action according to its state-action value function.
The converse is also true:\looseness=-1

\begin{cor}
  The optimal value functions $\fnv[\star]$ and $\fnq[\star]$ are a fixed-point of the so-called \midx{Bellman update}, \begin{align}
    \v*{x} &= \max_{a \in \sA} \q*{x}{a}, \label{eq:bop2} \\
    &= \max_{a \in \sA} r(x, a) + \gamma \E[x' \mid x, a]{\v*{x'}} \margintag{using the definition of the q-function \eqref{eq:q_function2}} \\
    \q*{x}{a} &= r(x, a) + \gamma \E[x' \mid x, a]{\max_{a' \in \sA} \q*{x'}{a'}}. \label{eq:bop2_q}
  \end{align}
\end{cor}
\begin{proof}
  It follows from \cref{eq:v_as_q} that \begin{align}
    \v*{x} = \E[a \sim \pis(x)]{\q*{x}{a}}.
  \end{align}
  Thus, as $\pis$ is greedy with respect to $\fnq[\star]$, $\v*{x} = \max_{a \in \sA} \q*{x}{a}$.

  \Cref{eq:bop2_q} follows analogously from \cref{eq:q_function3}.
\end{proof}

These equations are also called the \midx<Bellman optimality equations>{Bellman optimality equation}.
Intuitively, the Bellman optimality equations express that the value of a state under an optimal policy must equal the expected return for the best action from that state.
Bellman's theorem is also known as \midx{Bellman's optimality principle}, which is a more general concept.

\begin{marginbox}{Bellman's optimality principle}
  Bellman's optimality equations for MDPs are one of the main settings of Bellman's optimality principle.
  However, Bellman's optimality principle has many other important applications, for example in dynamic programming.
  Broadly speaking, Bellman's optimality principle says that optimal solutions to decision problems can be decomposed into optimal solutions to sub-problems.
\end{marginbox}

The two perspectives of Bellman's theorem naturally suggest two separate ways of finding the optimal policy.
Policy iteration uses the perspective from \cref{eq:bop1} of $\pis$ as a fixed-point of the dependency between greedy policy and value function.
In contrast, value iteration uses the perspective from \cref{eq:bop2} of $\fnv[\star]$ as the fixed-point of the Bellman update.
Another approach which we will not discuss here is to use a linear program where the Bellman update is interpreted as a set of linear inequalities.

\subsection{Policy Iteration}

\begin{algorithm}
  \caption{Policy iteration}\pidx{policy iteration}[idxpagebf]\label{alg:policy_iteration}
  initialize $\pi$ (arbitrarily)\;
  \Repeat{converged}{
    compute $\fnv[\pi]$\;
    compute $\pi_{\fnv[\pi]}$\;
    $\pi \gets \pi_{\fnv[\pi]}$\;
  }
\end{algorithm}

Starting from an arbitrary initial policy, policy iteration as shown in \cref{alg:policy_iteration} uses the Bellman expectation equation to compute the value function of that policy (as we have discussed in \cref{sec:mdp:policy_evaluation}) and then chooses the greedy policy with respect to that value function as its next iterate.

Let $\pi_t$ be the policy after $t$ iterations. We will now show that policy iteration converges to the optimal policy.
The proof is split into two parts.
First, we show that policy iteration improves policies monotonically.
Then, we will use this fact to show that policy iteration converges.\looseness=-1

\begin{lem}[Monotonic improvement of policy iteration]\label{lem:policy_iteration_mon_impr}
  We have, \begin{itemize}
    \item $\v[\pi_{t+1}]{x} \geq \v[\pi_t]{x}$ for all $x \in \sX$; and
    \item $\v[\pi_{t+1}]{x} > \v[\pi_t]{x}$ for at least one $x \in \sX$, unless $\fnv[\pi_t] \equiv \fnv[\star]$.
  \end{itemize}
\end{lem}
\begin{proof}
  We consider the Bellman update from \eqref{eq:bop2} as the mapping $\mBs : \R^n \to \R^n$, \begin{align}
    (\mBs \vv)(x) \defeq \max_{a \in \sA} \q{x}{a}, \label{eq:bellman_update_operator}
  \end{align} where $\fnq$ is the state-action value function corresponding to the state value function $\vv \in \R^n$.
  Recall that after obtaining $\fnv[\pi_t]$, policy iteration first computes the greedy policy w.r.t. $\fnv[\pi_t]$, $\pi_{t+1} \defeq \pi_{\fnv[\pi_t]}$, and then computes its value function $\fnv[\pi_{t+1}]$.

  To establish the (weak) monotonic improvement of policy iteration, we consider a fixed-point iteration (cf. \cref{alg:fixed_point_iteration}) of $\vv^{\pi_{t+1}}$ initialized by~$\vv^{\pi_t}$.
  We denote the iterates by $\tilde{\vv}_\tau$, in particular, we have that $\tilde{\vv}_0 = \vv^{\pi_t}$ and $\lim_{\tau\to\infty} \tilde{\vv}_\tau = \vv^{\pi_{t+1}}$.\footnote{using the convergence of fixed-point iteration \eqref{eq:fixed_point_iteration_convergence}}
  First, observe that for the first iteration of fixed-point iteration, \begin{align*}
    \tilde{v}_1(x) &= (\mBs \vv^{\pi_t})(x) \margintag{using that $\pi_{t+1}$ is greedy wrt. $v^{\pi_t}$} \\
    &= \max_{a \in \sA} \q[\pi_t]{x}{a} \margintag{using the definition of the Bellman update \eqref{eq:bellman_update_operator}} \\
    &\geq \q[\pi_t]{x}{\pi_t(x)} \\
    &= \v[\pi_t]{x} \\
    &= \tilde{v}_0(x). \margintag{using \eqref{eq:svf_as_savf}}
  \end{align*}
  Let us now consider a single iteration of fixed-point iteration.
  We have, \begin{align*}
    \tilde{v}_{\tau+1}(x) &= r(x, \pi_{t+1}(x)) + \gamma \sum_{x' \in \sX} p(x' \mid x, \pi_{t+1}(x)) \cdot \tilde{v}_\tau(x'). \margintag{using the definition of $\tilde{\vv}_{\tau+1}$ \eqref{eq:value_function_fixed_point_helper}}
    \intertext{Using an induction on $\tau$, we conclude,}
    &\geq r(x, \pi_{t+1}(x)) + \gamma \sum_{x' \in \sX} p(x' \mid x, \pi_{t+1}(x)) \cdot \tilde{v}_{\tau-1}(x') \margintag{using the induction hypothesis, $\tilde{v}_\tau(x') \geq \tilde{v}_{\tau-1}(x')$} \\
    &= \tilde{v}_\tau(x).
  \end{align*}
  This establishes the first claim, \begin{align}
    \vv^{\pi_{t+1}} = \lim_{\tau\to\infty} \tilde{\vv}_\tau \geq \tilde{\vv}_0 = \vv^{\pi_t}. \label{eq:pi_monotonicity}
  \end{align}

  For the second claim, recall from Bellman's theorem \eqref{eq:bop2} that $\fnv[\star]$ is a (unique) fixed-point of the Bellman update $\mBs$.\footnote{We will show in \cref{eq:bellman_update_contraction} that $\mBs$ is a contraction, implying that $\fnv[\star]$ is the \emph{unique} fixed-point of $\mBs$.}
  In particular, we have $\fnv[\pi_{t+1}] \equiv \fnv[\pi_t]$ if and only if $\fnv[\pi_{t+1}] \equiv \fnv[\pi_t] \equiv \fnv[\star]$.
  In other words, if $\fnv[\pi_t] \not\equiv \fnv[\star]$ then \cref{eq:pi_monotonicity} is strict for at least one $x \in \sX$ and $\fnv[\pi_{t+1}] \not\equiv \fnv[\pi_t]$.
  This proves the strict monotonic improvement of policy iteration.
\end{proof}

\begin{thm}[Convergence of policy iteration]\label{thm:policy_iteration_convergence}
  For finite Markov decision processes, policy iteration converges to an optimal policy.
\end{thm}
\begin{proof}
  Finite Markov decision processes only have a finite number of deterministic policies (albeit exponentially many).
  Observe that policy iteration only considers deterministic policies, and recall that there is an optimal policy that is deterministic.
  As the value of policies strictly increase in each iteration until an optimal policy is found, policy iteration must converge in finite time.
\end{proof}

It can be shown that policy iteration converges to an exact solution in a polynomial number of iterations \citep{pi_convergence}.
Each iteration of policy iteration requires computing the value function, which we have seen to be of cubic complexity in the number of states.

\subsection{Value Iteration}

As we have mentioned, another natural approach of finding the optimal policy is to interpret $\fnv[\star]$ as the fixed point of the Bellman update.
Recall our definition of the Bellman update from \cref{eq:bellman_update_operator}, \begin{align*}
  (\mBs \vv)(x) = \max_{a \in \sA} \q{x}{a},
\end{align*} where $\fnq$ was the state-action value function associated with the state value function $\vv$.
The value iteration algorithm is shown in \cref{alg:value_iteration}.

\begin{algorithm}
  \caption{Value iteration}\pidx{value iteration}\label{alg:value_iteration}
  initialize $\v{x} \gets \max_{a \in \sA} r(x, a)$ for each $x \in \sX$\;
  \For{$t = 1$ \KwTo $\infty$}{
    $\v{x} \gets (\mBs \vv)(x) = \max_{a \in \sA} \q{x}{a}$ for each $x \in \sX$\;
  }
  choose $\pi_{\fnv}$\;
\end{algorithm}

We will now prove the convergence of value iteration using the fixed-point interpretation.

\begin{thm}[Convergence of value iteration]\label{thm:vi_convergence}
  Value iteration converges asymptotically to an optimal policy.
\end{thm}
\begin{proof}
  Clearly, value iteration converges if $\fnv[\star]$ is the unique fixed-point of $\mBs$.
  We already know from Bellman's theorem \eqref{eq:bop2} that $\fnv[\star]$ is a fixed-point of $\mBs$.
  It remains to show that it is indeed the unique fixed-point.\looseness=-1

  Analogously to our proof of the convergence of fixed-point iteration to the value function $\fnv[\pi]$, we show that $\mBs$ is a contraction. Fix arbitrary $\vv, \vvp \in \R^n$, then \begin{align}
    \norm{\mBs \vv - \mBs \vvp}_\infty &= \max_{x \in \sX} \abs{(\mBs \vv)(x) - (\mBs \vvp)(x)} \margintag{using the definition of the $L_\infty$ norm \eqref{eq:infinity_norm}} \nonumber \\
    &= \max_{x \in \sX} \abs{\max_{a \in \sA} \q{x}{a} - \max_{a \in \sA} \fnq'(x,a)} \margintag{using the definition of the Bellman update \eqref{eq:bellman_update_operator}} \nonumber \\
    &\leq \max_{x \in \sX} \max_{a \in \sA} \abs{\q{x}{a}-\fnq'(x,a)} \margintag{using $\abs{\max_x f(x) - \max_x g(x)} \leq \max_x \abs{f(x) - g(x)}$} \nonumber \\
    &\leq \gamma \max_{x \in \sX} \max_{a \in \sA} \sum_{x' \in \sX} p(x' \mid x, a) \abs{\vv(x')-\vvp(x')} \margintag{using the definition of the Q-function \eqref{eq:q_function2} and $\abs{\sum_i a_i} \leq \sum_i \abs{a_i}$} \nonumber \\
    &\leq \gamma \norm{\vv-\vvp}_\infty \label{eq:bellman_update_contraction} \margintag{using $\sum_{x' \in \sX}p(x' \mid x, a) = 1$ and $ \abs{\vv(x')-\vvp(x')} \leq \norm{\vv-\vvp}_\infty$}
  \end{align} where $\fnq$ and $\fnq'$ are the state-action value functions associated with $\vv$ and $\vvp$, respectively.
  By \cref{eq:contraction}, $\mBs$ is a contraction and by Banach's fixed-point theorem $\fnv[\star]$ is its unique fixed-point.
\end{proof}

\begin{rmk}{Value iteration as a dynamic program}{}
  Let us denote by $\fnv[][t]$ the value function estimate after the $t$-th iteration.
  Observe that $\v{x}[t]$ corresponds to the maximum expected reward when starting in state $x$ and the ``world ends'' after $t$ time steps.
  In particular, $\fnv[][0]$ corresponds to the maximum immediate reward.
  This suggests a different perspective on value iteration (akin to dynamic programming) where in each iteration we extend the time horizon of our approximation by one time step.
\end{rmk}

For any $\epsilon > 0$, value iteration converges to an $\epsilon$-optimal solution in polynomial time.
However, unlike policy iteration, value iteration does not generally reach the \emph{exact} optimum in a finite number of iterations.
Recalling the update rule of value iteration, its main benefit is that each iteration only requires a sum over all possible actions $a$ in state $x$ and a sum over all reachable states $x'$ from $x$.
In sparse Markov decision processes,\footnote{Sparsity refers to the interconnectivity of the state space. When only few states are reachable from any state, we call an MDP sparse.} an iteration of value iteration can be performed in (virtually) constant time.


\section{Partial Observability}\label{sec:mdp:partial_observability}

So far we have focused on the fully observable setting.
That is, at any time, our agent knows its current state.
We have seen that we can efficiently find the optimal policy (as long as the Markov decision process is finite).

We have already encountered the partially observable setting in \cref{sec:kf}, where we discussed filtering\pidx{filtering}.
In this section, we consider how Markov decision processes can be extended to a partially observable setting where the agent can only access noisy observations $Y_t$ of its state $X_t$.

\begin{marginfigure}[5\baselineskip]
  \incfig{pomdp}
  \caption{Directed graphical model of a partially observable Markov decision process with hidden states $X_t$, observables $Y_t$, and actions $A_t$.}
\end{marginfigure}

\begin{defn}[Partially observable Markov decision process, POMDP]\pidx{partially observable Markov decision process}
  Similarly to a Markov decision process, a \emph{partially observable Markov decision process} is specified by \begin{itemize}
    \item a set of \emph{states} $\sX$,
    \item a set of \emph{actions} $\sA$,
    \item \emph{transition probabilities} $p(x' \mid x, a)$, and
    \item a \emph{reward function} $r : X \times A \to \R$.
  \end{itemize} Additionally, it is specified by \begin{itemize}
    \item a set of \emph{observations} $\sY$, and
    \item \emph{observation probabilities} \begin{align}
      o(y \mid x) \defeq \Pr{Y_t = y \mid X_t = x}. \label{eq:observation_probability}
    \end{align}
  \end{itemize}
\end{defn}

Whereas MDPs are controlled Markov chains, POMDPs are controlled hidden Markov models.

\begin{marginfigure}[15\baselineskip]
  \incfig{hmm}
  \caption{Directed graphical model of a hidden Markov model with hidden states $X_t$ and observables $Y_t$.}\label{fig:hmm}
\end{marginfigure}

\begin{rmk}{Hidden Markov models}{}
  A hidden Markov model is a Markovian process with unobservable states $X_t$ and observations $Y_t$ that depend on $X_t$ in a known way.\looseness=-1

  \begin{defn}[Hidden Markov model, HMM]\pidx{hidden Markov model}
    A \emph{hidden Markov model} is specified by \begin{itemize}
      \item a set of \emph{states} $\sX$,
      \item \emph{transition probabilities} $p(x' \mid x) \defeq \Pr{X_{t+1} = x' \mid X_t = x}$ (also called \emph{motion model}), and
      \item a \emph{sensor model} $o(y \mid x) \defeq \Pr{Y_t = y \mid X_t = x}$.
    \end{itemize}
  \end{defn}

  Following from its directed graphical model shown in \cref{fig:hmm}, its joint probability distribution factorizes into \begin{align}
    \Pr{x_{1:t}, y_{1:t}} = \Pr{x_1} \cdot o(y_1 \mid x_1) \cdot \prod_{i=2}^t p(x_i \mid x_{i-1}) \cdot o(y_i \mid x_i).
  \end{align}

  Observe that a Kalman filter can be viewed as a hidden Markov model with conditional linear Gaussian motion and sensor models and a Gaussian prior on the initial state.
  In particular, the tasks of \emph{filtering}, \emph{smoothing}, and \emph{predicting} which we discussed extensively in \cref{sec:kf} are also of interest for hidden Markov models.

  A widely used application of hidden Markov models is to find the most likely sequence (also called \midx{most likely explanation}) of hidden states $x_{1:t}$ given a series of observations $y_{1:t}$,\safefootnote{This is useful in many applications such as speech recognition, decoding data that was transmitted over a noisy channel, beat detection, and many more.} that is, to find \begin{align}
    \argmax_{x_{1:t}} \Pr{x_{1:t} \mid y_{1:t}}.
  \end{align}
  This task can be solved in linear time by a simple backtracking algorithm known as the \midx{Viterbi algorithm}.
\end{rmk}

POMDPs are a very powerful model, but very hard to solve in general.
POMDPs can be reduced to a Markov decision process with an enlarged state space.
The key insight is to consider an MDP whose states are the \midx<beliefs>{belief}[idxpagebf], \begin{align}
  b_t(x) \defeq \Pr{X_t = x \mid y_{1:t}, a_{1:t-1}}, \label{eq:belief}
\end{align} about the current state in the POMDP.
In other words, the states of the MDP are probability distributions over the states of the POMDP.
We will make this more precise in the following.

Let us assume that our prior belief about the state of our agent is given by $b_0(x) \defeq \Pr{X_0 = x}$.
Keeping track of how beliefs change over time is known as \midx{filtering}, which we already encountered in \cref{sec:kf:bayesian_filtering}.
Given a prior belief $b_t$, an action taken $a_t$, and a new observation $y_{t+1}$, the belief state can be updated as follows, \begin{align}
  b_{t+1}(x) &= \Pr{X_{t+1} = x \mid y_{1:t+1}, a_{1:t}} \margintag{by the definition of beliefs \eqref{eq:belief}} \nonumber \\
  &= \frac{1}{Z} \Pr{y_{t+1} \mid X_{t+1} = x} \Pr{X_{t+1} = x \mid y_{1:t}, a_{1:t}} \margintag{using Bayes' rule \eqref{eq:bayes_rule}} \nonumber \\
  &= \frac{1}{Z} o(y_{t+1} \mid x) \Pr{X_{t+1} = x \mid y_{1:t}, a_{1:t}} \margintag{using the definition of observation probabilities \eqref{eq:observation_probability}} \nonumber \\
  &= \frac{1}{Z} o(y_{t+1} \mid x) \sum_{x' \in \sX} p(x \mid x', a_t) \Pr{X_t = x' \mid y_{1:t}, a_{1:t-1}} \margintag{by conditioning on the previous state $x'$, noting $a_t$ does not influence $X_t$} \nonumber \\
  &= \frac{1}{Z} o(y_{t+1} \mid x) \sum_{x' \in \sX} p(x \mid x', a_t) b_t(x') \margintag{using the definition of beliefs \eqref{eq:belief}}
\end{align} where \begin{align}
  Z \defeq \sum_{x \in \sX} o(y_{t+1} \mid x) \sum_{x' \in \sX} p(x \mid x', a_t) b_t(x').
\end{align}
Thus, the updated belief state is a deterministic mapping from the previous belief state depending only on the (random) observation $y_{t+1}$ and the taken action $a_t$.
Note that this obeys a Markovian structure of transition probabilities with respect to the beliefs $b_t$.

The sequence of belief-states defines the sequence of random variables~$(B_t)_{t\in\Nat_0}$,\looseness=-1 \begin{align}
  B_t \defeq X_t \mid y_{1:t}, a_{1:t-1},
\end{align} where the (state-)space of all beliefs is the (infinite) space of all probability distributions over $\sX$,\footnote{This definition naturally extends to continuous state spaces $\spX$.} \begin{align}
  \spB \defeq \Delta^{\sX} \defeq \braces*{\vb \in \R^{\card{\sX}} : \vb \geq \vzero, \textstyle\sum_{i=1}^{\card{\sX}} \vb(i) = 1}.
\end{align}
A Markov decision process, where every belief corresponds to a state is called a belief-state MDP.

\begin{defn}[Belief-state Markov decision process]\pidx{belief-state Markov decision process}
  Given a POMDP, the corresponding \emph{belief-state Markov decision process} is a Markov decision process specified by \begin{itemize}
    \item the \emph{belief space} $\spB \defeq \Delta^{\sX}$ depending on the \emph{hidden states} $\sX$,
    \item the set of \emph{actions} $\sA$,
    \item \emph{transition probabilities} \begin{align}
      \tau(b' \mid b, a) \defeq \Pr{B_{t+1} = b' \mid B_t = b, A_t = a},
    \end{align}
    \item and \emph{rewards} \begin{align}
      \rho(b, a) \defeq \E[x \sim b]{r(x, a)} = \sum_{x \in \sX} b(x) r(x, a).
    \end{align}
  \end{itemize}
\end{defn}

It remains to derive the transition probabilities $\tau$ in terms of the original POMDP.
We have, \begin{align}
  \tau(b_{t+1} \mid b_t, a_t) &= \Pr{b_{t+1} \mid b_t, a_t} \nonumber \\
  &= \sum_{y_{t+1} \in \sY} \Pr{b_{t+1} \mid b_t, a_t, y_{t+1}} \Pr{y_{t+1} \mid b_t, a_t}. \margintag{by conditioning on $y_{t+1} \in \sY$}
\end{align}
Using the Markovian structure of the belief updates, we naturally set, \begin{align}
  \Pr{b_{t+1} \mid b_t, a_t, y_{t+1}} \defeq \begin{cases}
    1 & \parbox[t]{0.5\linewidth}{if $b_{t+1}$ matches the belief update of \cref{eq:belief} given $b_t, a_t$, and $y_{t+1}$,} \\
    0 & \text{otherwise}.
  \end{cases}
\end{align}

The final missing piece is the likelihood of an observation $y_{t+1}$ given the prior belief $b_t$ and action $a_t$, which using our interpretation of beliefs corresponds to \begin{align}
  \Pr{y_{t+1} \mid b_t, a_t} &= \E[x \sim b_t]{\E[x' \mid x, a_t]{\Pr{y_{t+1} \mid X_{t+1} = x'}}} \nonumber \\
  &= \E[x \sim b_t]{\E[x' \mid x, a_t]{o(y_{t+1} \mid x')}} \margintag{using the definition of observation probabilities \eqref{eq:observation_probability}} \nonumber \\
  &= \sum_{x \in \sX} b_t(x) \sum_{x' \in \sX} p(x' \mid x, a_t) \cdot o(y_{t+1} \mid x').
\end{align}

In principle, we can now apply arbitrary algorithms for planning in MDPs to POMDPs.
Of course, the problem is that there are infinitely many beliefs, even for a finite state space $\sX$.\footnote{You can think of an $\card{\sX}$-dimensional space. Here, all points whose coordinates sum to $1$ correspond to probability distributions (i.e., beliefs) over the hidden states $\sX$. The convex hull of these points is also known as the $(\card{\sX}-1)$-dimensional \idx{probability simplex} (cf. \cref{sec:background:probability:probability_simplex}). Now, by definition of the $(\card{\sX}-1)$-dimensional probability simplex as a polytope in $\card{\sX}-1$ dimensions, we can conclude that its boundary consists of infinitely many points in $\card{\sX}$ dimensions. Noting that these points corresponded to the probability distributions on $\card{\sX}$, we conclude that there are infinitely many such distributions.}
The belief-state MDP has therefore an infinitely large belief space $\spB$.
Even when only planning over finite horizons, exponentially many beliefs can be reached.
So the belief space blows-up very quickly.

We will study MDPs with large state spaces (where transition dynamics and rewards are unknown) in \cref{sec:mfarl,sec:mbarl}.
Similar methods can also be used to approximately solve POMDPs.

A key idea in approximate solutions to POMDPs is that most belief states are never reached.
A common approach is to discretize the belief space by sampling or by applying a dimensionality reduction.
Examples are \midx{point-based value iteration} (PBVI) and \midx{point-based policy iteration}~(PBPI)~\citep{shani2013survey}.


\section*{Discussion}

Even though we focus on the fully observed setting throughout this manuscript, the partially observed setting can be reduced to the fully observed setting with very large state spaces.
In the next chapter, we will consider learning and planning in unknown Markov decision processes (i.e., reinforcement learning) for small state spaces.
The setting of small state and action spaces is also known as the \emph{tabular setting}.
Then, in the final two chapters, we will consider approximate methods for large state and action spaces.
In particular, in \cref{sec:mbarl:planning}, we will revisit the problem of probabilistic planning in known Markov decision processes, but with continuous state and action spaces.

\excheading

\begin{nexercise}{Value functions}{value_functions}
  Recall the example of ``becoming rich and famous'' from \cref{fig:mdp_example}.
  Consider the policy, $\pi \equiv S$ (i.e., to always \emph{save}) and let $\gamma = \nicefrac{1}{2}$. Show that the (rounded) state-action value function $\fnq[\pi]$ is as follows:

  \vspace{5pt}
  \begin{center}
    \begin{tabular}{lrr}
      \toprule
      & save & advertise \\
      \midrule
      poor, unknown & $\mathbf{0}$ & $0.1$ \\
      poor, famous & $\mathbf{4.4}$ & $1.2$ \\
      rich, famous & $\mathbf{17.8}$ & $1.2$ \\
      rich, unknown & $\mathbf{13.3}$ & $0.1$ \\
      \bottomrule
    \end{tabular}
  \end{center}
  \vspace{5pt}

  Shown in bold is the state value function $\fnv[\pi]$.
\end{nexercise}

\begin{nexercise}{Greedy policies}{v_and_q_functions}
  Show that if $\fnq$ and $\fnv$ arise from the same policy, that is, $\fnq$ is defined in terms of $\fnv$ as per \cref{eq:q_function2}, then \begin{align}
    \pi_{\fnv} \equiv \pi_{\fnq}.
  \end{align}
  This implies that we can use $\fnv$ and $\fnq$ interchangeably.
\end{nexercise}

\begin{nexercise}{Optimal policies}{optimal_policies}
  Again, recall the example of ``becoming rich and famous'' from \cref{fig:mdp_example}.

  \begin{enumerate}
    \item Show that the policy $\pi \equiv S$, which we considered in \cref{exercise:value_functions}, is not optimal.
    \item Instead, consider the policy \begin{align*}
      \pi' \equiv \begin{cases}
        A & \text{if poor and unknown} \\
        S & \text{otherwise} \\
      \end{cases}
    \end{align*} and let $\gamma = \nicefrac{1}{2}$. Show that the (rounded) state-action value function $\fnq[\pi']$ is as follows:

    \vspace{5pt}
    \begin{center}
      \begin{tabular}{lrr}
        \toprule
        & save & advertise \\
        \midrule
        poor, unknown & $0.8$ & $\mathbf{1.6}$ \\
        poor, famous & $\mathbf{4.5}$ & $1.2$ \\
        rich, famous & $\mathbf{17.8}$ & $1.2$ \\
        rich, unknown & $\mathbf{13.4}$ & $0.2$ \\
        \bottomrule
      \end{tabular}
    \end{center}
    \vspace{5pt}

    Shown in bold is the state value function $\fnv[\pi']$.
    \item Is the policy $\pi'$ optimal?
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Linear convergence of policy iteration}{policy_iteration_linear_convergence}
  Denote by $\pi_t$ the policy obtained by policy iteration after $t$ iterations.
  Use that the Bellman operator $\mBs$ is a contraction with the unique fixed-point $\vvs$ to show that \begin{align}
    \norm{\vv^{\pi_t} - \vvs}_\infty \leq \gamma^t \norm{\vv^{\pi_0} - \vvs}_\infty
  \end{align} where $\vv^{\pi}$ and $\vvs$ are vector representations of the functions $\fnv[\pi]$ and $\fnv[\star]$, respectively.\looseness=-1

  \textit{Hint: Recall from \cref{lem:policy_iteration_mon_impr} that $\vv^{\pi_{t+1}} \geq \mBs\vv^{\pi_t} \geq \vv^{\pi_t}$.}
\end{nexercise}

\begin{nexercise}{Reward modification}{reward_modification}
  A key technique for solving sequential decision problems is the modification of reward functions that leaves the optimal policy unchanged while improving sample efficiency or convergence rates.
  This exercise looks at simple ways of modifying rewards and understanding how these modifications affect the optimal policy.

  Consider two Markov decision processes $\sM \defeq (\sX, \sA, p, r)$ and ${\sM' \defeq (\sX, \sA, p, r')}$ where the reward function $r$ is modified to obtain $r'$, and the rewards are bounded and discounted by the discount factor ${\gamma \in [0,1)}$.
  Let $\pis_\sM$ be the optimal policy for $\sM$.

  \begin{enumerate}
    \item Suppose $r'(x) = \alpha r(x)$, where $\alpha > 0$. Show that the optimal policy~$\pis$ of~$\sM$ is also an optimal policy of~$\sM'$.
    \item Given a modification of the form $r'(x) = r(x) + c$, where $c > 0$ is a constant scalar, show that the optimal policy~$\pis_\sM$ can be different from $\pis_{\sM'}$.
    \item Another way of modifying the reward function is through \midx{reward shaping} where one supplies additional rewards to the agent to guide the learning process.
    When one has no knowledge of the underlying transition dynamics $p$, a commonly used transformation is ${r'(x, x') = r(x, x') + f(x, x')}$ where $f$ is a \emph{potential-based} shaping function defined as \begin{align}
      f(x, x') \defeq \gamma \phi(x') - \phi(x), \quad \phi : \sX \to \R.
    \end{align}

    Show that the optimal policy remains unchanged under this definition of $f$.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{A partially observable fishing problem}{fishing_problem_pomdb}
  We model an angler's decisions while fishing, where the states are partially observable.
  There are two states: (1) \textbf{Fish} ($F$): A fish is hooked on the line. (2) \textbf{No fish} ($\overline{F}$): No fish is hooked on the line.

  The angler can choose between two actions: \begin{itemize}
    \item \textbf{Pull up the rod} ($P$): If there is a fish on the line ($F$), there is a $90\%$ chance of catching it (reward $+10$, transitioning to $\overline{F}$) and a $10\%$ chance of it escaping (reward $-1$, transitioning to $\overline{F}$).
    If there is no fish ($\overline{F}$), pulling up the rod results in no catch, staying in $\overline{F}$ with a reward of $-5$.

    \item \textbf{Waiting} ($W$): All waiting actions result in a reward of $-1$.
    In state $F$, there is a $60\%$ chance of the fish staying (remaining in $F$) and a $40\%$ chance of it escaping (transitioning to $\overline{F}$).
    In state $\overline{F}$, there is a $50\%$ chance of a fish biting (transitioning to $F$) and a $50\%$ chance of no change (remaining in $\overline{F}$).
  \end{itemize}

  \textit{Suggestion: Draw the MDP transition diagram. Draw each transition with action,  associated probability, and associated reward.}

  Since the angler cannot directly observe whether there is a fish on the line, they receive a noisy observation about the state.
  This observation can be: \begin{itemize}
    \item $o_1$: The signal suggests that a fish might be on the line.
    \item $o_2$: The signal suggests that there is no fish on the line.
  \end{itemize}
  The observation model, which defines the probability of receiving each observation given the true state is as follows:

  \vspace{5pt}
  \begin{center}
    \begin{tabular}{lcc}
      \toprule
      & $\Pr{o_1 \mid \cdot}$ & $\Pr{o_2 \mid \cdot}$ \\
      \midrule
      $F$ & $0.8$ & $0.2$ \\
      $\overline{F}$ & $0.3$ & $0.7$ \\
      \bottomrule
    \end{tabular}
  \end{center}
  \vspace{5pt}

  The angler's goal is to choose actions that maximize their overall reward, balancing the chances of catching a fish against the cost of waiting and unsuccessful pulls.

  \begin{enumerate}
    \item Given an initial belief $b_0(F) = b_0(\overline{F}) = 0.5$, the angler chooses to wait and observes $o_1$.
    Compute the updated belief $b_1$ using the observation model and belief update equation~\eqref{eq:belief}.

    \item Given belief $b_1(F) \approx 0.765$ and $b_1(\overline{F}) \approx 0.235$, compute the updated belief $b_2$ for both actions $P$ (pull) and $W$ (wait), both in the case where you observe $o_1$ (fish likely) and $o_2$ (fish unlikely).
  \end{enumerate}
\end{nexercise}
