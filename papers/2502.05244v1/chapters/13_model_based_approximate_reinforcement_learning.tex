\chapter{Model-based Reinforcement Learning}\label{sec:mbarl}

In this final chapter, we will revisit the model-based approach to reinforcement learning.
We will see some advantages it offers over model-free methods.
In particular, we will use the machinery of probabilistic inference, which we developed in the first chapters, to quantify uncertainty about our model and use this uncertainty for planning, exploration, and reliable decision-making.

To recap, in \cref{sec:tabular_rl}, we began by discussing model-based reinforcement learning which attempts to learn the underlying Markov decision process and then use it for planning.
We have seen that in the tabular setting, computing and storing the entire model is computationally expensive.
This led us to consider the family of model-free approaches, which learn the value function directly, and as such can be considered more economical in the amount of data that they store.

In \cref{sec:mfarl}, we saw that using function approximation, we were able to scale model-free methods to very large state and action spaces.
We will now explore similar ideas in the model-based framework.
Namely, we will use function approximation to condense the representation of our model of the environment.
More concretely, we will learn an approximate dynamics model $f \approx p$ and approximate rewards $r$, which is also called a \midx{world model}.

There are a few ways in which the model-based approach is advantageous.
First, if we have an accurate model of the environment, we can use it for planning --- ideally also for interacting safely with the environment.
However, in practice, we will rarely have such a model.
In fact, the accuracy of our model will depend on the past experience of our agent and the region of the state-action space.
Understanding the uncertainty in our model of the environment is crucial for planning.
In particular, quantifying uncertainty is necessary to drive safe(!) exploration and avoid undesired states.

Moreover, modeling uncertainty in our model of the environment can be extremely useful in deciding where to explore.
Learning a model can therefore help to dramatically reduce the sample complexity over model-free techniques.
Often times this is crucial when developing agents for real-world use as in such settings, exploration is costly and potentially dangerous.

\Cref{alg:mbarl} describes the general approach to model-based reinforcement learning.

\begin{algorithm}[H]
  \caption{Model-based reinforcement learning (outline)}\pidx{model-based reinforcement learning}\label{alg:mbarl}
  start with an initial policy $\pi$ and no (or some) initial data $\spD$\;
  \For{several episodes}{
    roll out policy $\pi$ to collect data\;
    learn a model of the dynamics $f$ and rewards $r$ from data\;
    plan a new policy $\pi$ based on the estimated models\;
  }
\end{algorithm}

We face three main challenges in model-based reinforcement learning.
First, given a fixed model, we need to perform planning to decide on which actions to play.
Second, we need to learn models $f$ and $r$ accurately and efficiently.
Third, we need to effectively trade exploration and exploitation.
We will discuss these three topics in the following.

\section{Planning}\label{sec:mbarl:planning}

There exists a large literature on planning in various settings.
These settings can be mainly characterized as \begin{itemize}
  \item discrete or continuous action spaces;
  \item fully- or partially observable state spaces;
  \item constrained or unconstrained; and
  \item linear or nonlinear dynamics.
\end{itemize}
In \cref{sec:mdp}, we have already seen algorithms such as policy iteration and value iteration, which can be used to solve planning exactly in tabular settings.
In the following, we will now focus on the setting of continuous state and action spaces, fully observable state spaces, no constraints, and nonlinear dynamics.

\subsection{Deterministic Dynamics}\label{sec:mbarl:planning:determinstic_dynamics}

To begin with, let us assume that our dynamics model is deterministic and known.
That is, given a state-action pair, we know the subsequent state,\looseness=-1 \begin{align}
  \vx_{t+1} = \vf(\vx_t, \va_t).
\end{align}
We continue to focus on the setting of infinite-horizon discounted returns\pidx{discounted payoff} \eqref{eq:discounted_payoff}, which we have been considering throughout our discussion of reinforcement learning.
This yields the objective, \begin{align}
  \max_{\va_{0:\infty}} \sum_{t=0}^\infty \gamma^t r(\vx_t, \va_t) \quad \text{such that}\ \vx_{t+1} = \vf(\vx_t, \va_t).
\end{align}
Now, because we are optimizing over an infinite time horizon, we cannot solve this optimization problem directly.
This problem is studied ubiquitously in the area of \midx{optimal control}.
We will discuss one central idea from optimal control that is widely used in model-based reinforcement learning, and will later return to using this idea for learning parametric policies in \cref{sec:mbarl:planning:parametric_policies}.

\paragraph{Planning over finite horizons:}

The key idea of a classical algorithm from optimal control called \midx{receding horizon control} (RHC) or \midx{model predictive control} (MPC) is to iteratively plan over finite horizons.
That is, in each round, we plan over a finite time horizon $H$ and carry out the first action.

\begin{algorithm}[H]
  \caption{Model predictive control, MPC}
  \For{$t=0$ \KwTo $\infty$}{
    observe $\vx_t$\;
    plan over a finite horizon $H$, \vspace{-8pt}\begin{align}
      \max_{\va_{t:t+H-1}} \sum_{\tau=t}^{t+H-1} \gamma^{\tau-t} r(\vx_\tau, \va_\tau) \quad \text{such that}\ \vx_{\tau+1} = \vf(\vx_\tau, \va_\tau)
    \end{align}\vspace{-20pt}\;
    carry out action $\va_t$\;
  }
\end{algorithm}

\begin{marginfigure}
  \incfig{mpc}
  \caption{Illustration of model predictive control in a deterministic transition model.
  The agent starts in position $\vx_0$ and wants to reach $\vxs$ despite the black obstacle.
  We use the reward function $r(\vx) = -\norm{\vx-\vxs}$.
  The gray concentric circles represent the length of a single step.
  We plan with a time horizon of $H=2$. Initially, the agent does not ``see'' the black obstacle, and therefore moves straight towards the goal.
  As soon as the agent sees the obstacle, the optimal trajectory is ``replanned''.
  The dotted red line corresponds to the optimal trajectory, the agent's steps are shown in blue.}
\end{marginfigure}

Observe that the state $\vx_\tau$ can be interpreted as a deterministic function $\vx_\tau(\va_{t:\tau-1})$, which depends on all actions from time $t$ to time $\tau-1$ and the state $\vx_t$.
To solve the optimization problem of a single iteration, we therefore need to maximize, \begin{align}
  \j{\va_{t:t+H-1}}[H] \defeq \sum_{\tau=t}^{t+H-1} \gamma^{\tau-t} r(\vx_\tau(\va_{t:\tau-1}), \va_\tau).
\end{align}
This optimization problem is in general non-convex.
If the actions are continuous and the dynamics and reward models are differentiable, we can nevertheless obtain analytic gradients of $\fnJ[H]$.
This can be done using the chain rule and ``backpropagating'' through time, analogously to backpropagation in neural networks.\footnote{see \cref{sec:bdl:ann:backprop}}
Especially for large horizons $H$, this optimization problem becomes difficult to solve exactly due to local optima and vanishing/exploding gradients.

\paragraph{Tree search:}

Often, heuristic global optimization methods (also called ``search methods'') are used to optimize $\fnJ[H]$.
An example are \midx{random shooting methods}, which find the optimal choice among a set of random proposals.
Of course, obtaining a ``good'' set of randomly proposed action sequences is crucial.
A naive way of generating the proposals is to pick them uniformly at random.
This strategy, however, usually does not perform very well as it corresponds to suggesting random walks of the state space.
Alternatives are to sample from a Gaussian distribution or using the \midx{cross-entropy method} which gradually adapts the sampling distribution by reweighing samples according to the rewards they produce.

\begin{algorithm}
  \caption{Random shooting methods}
  generate $m$ sets of random samples, $\va_{t:t+H-1}^{(i)}$\;
  pick the sequence of actions $\va_{t:t+H-1}^{(\opt{i})}$ where \vspace{-8pt}\begin{align}
    \opt{i} = \argmax_{i\in[m]} \j{\va_{t:t+H-1}^{(i)}}[H]
  \end{align}\vspace{-20pt}\;
\end{algorithm}

The evolution of the state can be visualized as a tree where --- if dynamics are deterministic --- the branching is fully determined by the played actions.
For this reason, classical tree search algorithms can be employed, such as \midx{alpha-beta pruning} or \midx{Monte Carlo tree search} (MCTS), which was used for example by ``AlphaZero'' \citep{silver2016mastering,silver2017mastering} and can be viewed as an advanced variant of a shooting method.

\paragraph{Finite-horizon planning with sparse rewards:}

A common problem of finite-horizon methods is that in the setting of sparse rewards, there is often no signal that can be followed.
You can think of an agent that operates in a kitchen and tries to find a box of candy.
Yet, to get to this box, it needs to perform a large number of actions.
In particular, if this number of actions is larger than the horizon $H$, then the local optimization problem of MPC will not take into account the reward for choosing this long sequence of actions.
Thus, the box of candy will likely never be found by our agent.

\begin{marginfigure}
  \incfig{sparse_rewards}
  \caption{Illustration of finite-horizon planning with sparse rewards.
  When the finite time horizon does not suffice to ``reach'' a reward, the agent has no signal to follow.}
\end{marginfigure}

A solution to this problem is to amend a long-term value estimate to the finite-horizon sum.
The idea is to not only consider the rewards attained \emph{while} following the actions $\va_{t:t_H-1}$, but to also consider the value of the final state $\vx_{t+H}$, which estimates the discounted sum of \emph{future} rewards.
\begin{align}
  \J{\va_{t:t+H-1}}[H] \defeq \underbrace{\sum_{\tau=t}^{t+H-1} \gamma^{\tau-t} r(\vx_\tau(\va_{t:\tau-1}), \va_\tau)}_{\text{short-term}} + \underbrace{\gamma^H \V{\vx_{t+H}}}_{\text{long-term}}.
\end{align}
Intuitively, $\gamma^H \V{\vx_{t+H}}$ is estimating the tail of the infinite sum.

\begin{rmk}{Planning generalizes model-free methods!}{planning_generalizes_model_free_rl}
  Observe that for $H=1$, when we use the value function estimate associated with this MPC controller, maximizing $J_H$ coincides with using the greedy policy $\pi_V$.
  That is, \begin{align}
    \va_t = \argmax_{\va \in \spA} \J{\va}[1] = \pi_{\fnV}(\vx_t).
  \end{align}
  Thus, by looking ahead for a single time step, we recover the approaches from the model-free setting in this model-based setting!
  Essentially, if we do not plan long-term and only consider the value estimate, the model-based setting reduces to the model-free setting.
  However, in the model-based setting, we are now able to use our model of the transition dynamics to anticipate the downstream effects of picking a particular action $\va_t$.
  This is one of the fundamental reasons for why model-based approaches are typically severely more sample efficient than model-free methods.
\end{rmk}

To obtain the value estimates, we can use the approaches we discussed in detail in \cref{sec:tabular_rl:model_free}, such as TD-learning for on-policy value estimates and Q-learning for off-policy value estimates.
For large state-action spaces, we can use their approximate variants, which we discussed in \cref{sec:mfarl:tabular_rl_as_optimization} and \cref{sec:mfarl:value_function_approximation}.
To improve value estimates, we can obtain ``artificial'' data by rolling out policies within our model.
This is a key advantage over model-free methods as, once a sufficiently accurate model has been learned, data for value estimation can be generated efficiently in simulation.

\subsection{Stochastic Dynamics}\label{sec:mbarl:planning:stochastic_dynamics}

How can we extend this approach to planning to a stochastic transition model?
A natural extension of model predictive control is to do what is called \midx{stochastic average approximation} (SAA) or \midx{trajectory sampling} \citep{chua2018deep}.
Like in MPC, we still optimize over a deterministic sequence of actions, but now we will average over all resulting trajectories.\looseness=-1

\begin{algorithm}
  \caption{Trajectory sampling}
  \For{$t=0$ \KwTo $\infty$}{
    observe $\vx_t$\;
    optimize expected performance over a finite horizon $H$, \vspace{-8pt}\begin{align}
      \max_{\va_{t:t+H-1}} \E[\vx_{t+1:t+H}]{\sum_{\tau=t}^{t+H-1} \gamma^{\tau-t} r_\tau + \gamma^H \V{\vx_{t+H}}}[\va_{t:t+H-1}, f]
    \end{align}\vspace{-20pt}\;
    carry out action $\va_t$\;
  }
\end{algorithm}

\begin{marginfigure}
  \incfig{trajectory_sampling}
  \caption{Illustration of trajectory sampling.
  High-reward states are shown in brighter colors.
  The agent iteratively plans a finite number of time steps into the future and picks the best initial action.}
\end{marginfigure}

Intuitively, trajectory sampling optimizes over a much simpler object --- namely a deterministic sequence of actions of length $H$ --- than finding a policy, which corresponds to finding an optimal decision tree mapping states to actions.
Of course, using trajectory sampling (from an arbitrary starting state) implies such a policy.
However, trajectory sampling never computes this policy explicitly, and rather, in each step, only plans over a finite horizon.

Computing the expectation exactly is typically not possible as this involves solving a high-dimensional integral of nonlinear functions.
Instead, a common approach is to use Monte Carlo estimates of this expectation.
This approach is known as \midx{Monte Carlo trajectory sampling}.
The key issue with using sampling based estimation is that the sampled trajectory (i.e., sampled sequence of states) we obtain, depends on the actions we pick.
In other words, the measure we average over depends on the decision variables --- the actions.
This is a problem that we have seen several times already!
It naturally suggests using the reparameterization trick\pidx{reparameterization trick}.\footnote{see \cref{thm:reparameterization_trick} and \cref{eq:reparameterization_gradients}}

Previously, we have used the reparameterization trick to reparameterize variational distributions (see \cref{sec:approximate_inference:variational_inference:gradient_of_elbo}) and to reparameterize policies (see \cref{sec:mfarl:actor_critic_methods:randomized_policies}).
It turns out that we can use the exact same approach for reparameterizing the transition model.
We say that a (stochastic) transition model $f$ is \emph{reparameterizable} iff $\vx_{t+1} \sim f(\vx_t, \va_t)$ is such that $\vx_{t+1} = \vg(\vvarepsilon; \vx_t, \va_t)$, where $\vvarepsilon \sim \phi$ is a random variable that is independent of $\vx_t$ and $\va_t$.
We have already seen in \cref{ex:reparameterization_gradients_gaussian} (in the context of stochastic policies) how a Gaussian transition model can be reparameterized.

In this case, $\vx_\tau$ is determined recursively by $\va_{t:\tau-1}$ and $\vvarepsilon_{t:\tau-1}$, \begin{align}\begin{split}
  \vx_\tau &= \vx_\tau(\vvarepsilon_{t:\tau-1}; \va_{t:\tau-1}) \\
  &\defeq \vg(\vvarepsilon_{\tau-1}; \vg(\dots; (\vvarepsilon_{t+1}; \vg(\vvarepsilon_t; \vx_t, \va_t), \va_{t+1}), \dots), \va_{\tau-1}). \label{eq:state_encoding}
\end{split}\end{align}
This allows us to obtain unbiased estimates of $J_H$ using Monte Carlo approximation, \begin{align}\begin{split}
  \J{\va_{t:t+H-1}}[H] \approx \begin{multlined}[t]
    \frac{1}{m} \sum_{i=1}^m \Bigg( \sum_{\tau=t}^{t+H-1} \gamma^{\tau-t} r(\vx_\tau(\vvarepsilon_{t:\tau-1}^{(i)}; \va_{t:\tau-1}), \va_\tau) \\ + \gamma^H \V{\vx_{t+H}(\vvarepsilon_{t:t+H-1}^{(i)}; \va_{t:t+H-1})} \Bigg)
  \end{multlined} \label{eq:mbarl_planning_monte_carlo}
\end{split}\end{align} where $\vvarepsilon_{t:t+H-1}^{(i)} \iid \phi$ are independent samples.
To optimize this approximation we can again compute analytic gradients or use shooting methods as we have discussed in \cref{sec:mbarl:planning:determinstic_dynamics} for deterministic dynamics.\looseness=-1

\subsection{Parametric Policies}\label{sec:mbarl:planning:parametric_policies}

When using algorithms such as model predictive control for planning, planning needs to be done online before each time we take an action.
This is called \midx{closed-loop control} and can be expensive.
Especially when the time horizon is large, or we encounter similar states many times (leading to ``repeated optimization problems''), it can be beneficial to ``store'' the planning decision in a (deterministic) policy, \begin{align}
  \va_t = \vpi(\vx_t; \vvarphi) \eqdef \vpi_\vvarphi(\vx_t).
\end{align}
This policy can then be trained offline and evaluated cheaply online, which is known as \midx{open-loop control}.

This is akin to a problem that we have discussed in detail in the previous chapter when extending Q-learning to large action spaces.
There, this led us to discuss policy gradient and actor-critic methods.
Recall that in Q-learning, we seek to follow the greedy policy, \begin{align*}
  \opt{\vpi}(\vx) = \argmax_{\va \in \spA} \Q*{\vx}{\va; \vtheta}, \margintag{see \cref{eq:q_learning_policy}}
\end{align*} and therefore had to solve an optimization problem over all actions.
We accelerated this by learning an approximate policy that ``mimicked'' this optimization, \begin{align*}
  \opt{\vvarphi} = \argmax_{\vvarphi} \underbrace{\E[\vx \sim \mu]{\Q*{\vx}{\vpi_\vvarphi(\vx); \vtheta}}}_{\J{\vvarphi; \vtheta}[\mu]} \margintag{see \cref{eq:off_policy_actor_critics_optimization}}
\end{align*} where $\mu(\vx) > 0$ was some exploration distribution that has full support and thus leads to the exploration of all states.
The key idea was that if we use a differentiable approximation $Q$ and a differentiable parameterization of policies, which is ``rich enough'', then both optimizations are equivalent, and we can use the chain rule to obtain gradient estimates of the second expression.
We then used this to derive the \midx{deep deterministic policy gradients} (DDPG) and \midx{stochastic value gradients} (SVG) algorithms.
It turns out that there is a very natural analogue to DDPG/SVG for model-based reinforcement learning.

Instead of maximizing the Q-function directly, we use finite-horizon planning to estimate the immediate value of the policy within the next $H$ time steps and simply use the Q-function to approximate the terminal value (i.e., the tails of the infinite sum).
Then, our objective becomes,\looseness=-1 \begin{align}
  \J{\vvarphi; \vtheta}[\mu,H] \defeq \E[\vx_0 \sim \mu, \vx_{1:H} \mid \vpi_\vvarphi, f]{\sum_{\tau=0}^{H-1} \gamma^\tau r_\tau + \gamma^H \Q*{\vx_H}{\vpi_\vvarphi(\vx_H); \vtheta}} \label{eq:rl_objective_with_tails}
\end{align}
This approach naturally extends to randomized policies using reparameterization gradients, which we have discussed in \cref{sec:mfarl:actor_critic_methods:randomized_policies}.
Analogously to \cref{rmk:planning_generalizes_model_free_rl}, for $H=0$, this coincides with the DDPG objective!
For larger time horizons, the look-ahead takes into account the transition model for planning next time steps.
This tends to help dramatically in improving policies \emph{much more rapidly} between episodes. Instead of just gradually improving policies a little by slightly adapting the policy to the learned value function estimates (as in model-free RL), we use the model to anticipate the consequences of actions multiple time steps ahead. This is at the heart of model-based reinforcement learning.\looseness=-1

Essentially, we are using methods such as Q-learning and DDPG/SVG as subroutines within the framework of model predictive control to do much bigger steps in policy improvement than to slightly improve the next picked action.
To encourage exploration, it is common to extend the objective in \cref{eq:rl_objective_with_tails} by an additional entropy regularization term as seen in \cref{sec:mfarl:actor_critic_methods:entropy_regularization}.

\begin{rmk}{Planning as inference}{planning_as_inference}
  We have been using \midx{Monte Carlo rollouts} to estimate the expectation of \cref{eq:rl_objective_with_tails}.
  That is, we have been using a Monte Carlo approximation (e.g., a sample mean) using samples obtained by ``rolling out'' the induced Markov chain of a fixed policy.

  It would certainly be preferable to compute the expectation exactly, however, this is generally not possible as this involves solving a high dimensional integral.
  Recall that we faced the same problem when studying inference in the first half of the manuscript.
  In both problems, we need to approximate high-dimensional integrals (i.e., expectations).
  This suggests a deep connection between the problems of planning and inference.
  It is therefore not surprising that many techniques for approximate inference that we have seen earlier can also be applied to planning.

  \begin{itemize}
    \item \midx{Monte Carlo approximation} which we have been focusing on during our discussion of planning is a very simple inference algorithm --- approximating an expectation by sampling from the distribution that is averaged over.
    This allowed us to obtain unbiased gradient estimates (which may have high variance).

    \item An alternative approach is \midx{moment matching} (cf. \cref{sec:vi:kl:forward}).
    Instead of approximating the expectation, here, we approximate the distribution over trajectories using a tractable distribution (e.g., a Gaussian) and ``matching'' their moments.
    This then allows us to analytically compute gradients of the expectation in \cref{eq:rl_objective_with_tails}.
    A prominent example of this approach is \midx{probabilistic inference for learning control} (PILCO).

    \item In \cref{sec:mfarl:actor_critic_methods:entropy_regularization}, we have used \midx{variational inference} for planning, and seen that it coincides with entropy regularization as implemented by the \midx{soft actor critic} (SAC) algorithm.
  \end{itemize}
\end{rmk}

\section{Learning}

Thus far, we have considered known environments.
That is, we assumed that the transition model $f$ and the rewards $r$ are known.
In reinforcement learning, $f$ and $r$ are (of course!) not known.
Instead, we have to estimate them from data.
This will also be crucial in our later discussion of exploration in \cref{sec:mbarl:exploration} where we explore methods of driving data collection to learn what we need to learn about the world.\looseness=-1

First, let us revisit one of our key observations when we first introduced the reinforcement learning problem.
Namely, that the observed transitions $\vxp$ and rewards $r$ are conditionally independent given the state-action pairs $(\vx, \va)$.\footnote{see \cref{eq:rl_cond_indep}}
This is due to the Markovian structure of the underlying Markov decision process.

This is the key observation that allows us to treat the estimation of the dynamics and rewards as a simple regression problem (or a density estimation problem when the quantities are stochastic rather than deterministic).
More concretely, we can estimate the dynamics and rewards off-policy using the standard supervised learning techniques we discussed in earlier chapters, from a replay buffer \begin{align}
  \spD = \{(\underbrace{\vx_t, \va_t}_\text{``input''}, \underbrace{r_t, \vx_{t+1}}_\text{``label''})\}_t.
\end{align}
Here, $\vx_t$ and $\va_t$ are the ``inputs'', and $r_t$ and $\vx_{t+1}$ are the ``labels'' of the regression problem.
Due to the conditional independence of the labels given the inputs, we have independent label noise (i.e., ``independent training data'') which is the basic assumption that we have been making throughout our discussion of techniques for probabilistic machine learning in \cref{part1}.

The key difference to supervised learning is that the set of inputs depends on how we act.
That is, the current inputs arise from previous policies, and the inputs which we will observe in the future will depend on the model (and policy) obtained from the current data: we have feedback loops!
We will come back to this aspect of reinforcement learning in the next section on exploration.
For now, recall we only assume that we have used an arbitrary policy to collect some data, which we then stored in a replay buffer, and which we now want to use to learn the ``best-possible'' model of our environment.

\subsection{Probabilistic Inference}

In the following, we will discuss how we can use the techniques from probabilistic inference, which we have seen in \cref{part1}, to learn the dynamics and reward models.
Thereby, we will focus on learning the transition model $f$ as learning the reward model $r$ is completely analogous.
For learning deterministic dynamics or rewards, we can use for example Gaussian processes (cf. \cref{sec:gp}) or deep neural networks (cf. \cref{sec:bdl}).
We will now focus on the setting where the dynamics are stochastic, that is, \begin{align}
  \vx_{t+1} \sim f(\vx_t, \va_t; \vpsi).
\end{align}

\begin{ex}{Conditional Gaussian dynamics}{}
  We could, for example, use a conditional Gaussian for the transition model, \begin{align}
    \vx_{t+1} \sim \N{\vmu(\vx_t, \va_t; \vpsi)}{\mSigma(\vx_t, \va_t; \vpsi)}. \label{eq:conditional_gaussian_dynamics}
  \end{align}
  As we have seen in \cref{eq:cholesky}, we can rewrite the covariance matrix $\mSigma$ as a product of a lower-triangular matrix $\mCalL$ and its transpose using the Cholesky decomposition ${\mSigma = \mCalL \transpose{\mCalL}}$ of $\mSigma$.
  This allows us to represent the model by only $n (n + 1) / 2$ parameters.
  Moreover, we have learned that Gaussians are reparameterizable, which we have seen to be useful for planning.

  Note that this model reduces to a deterministic model if the covariance is zero.
  So the stochastic transition models encompass all deterministic models.
  Moreover, in many applications (such as robotics), it is often useful to use stochastic models to attribute slight inaccuracies and measurement noise to a small uncertainty in the model.
\end{ex}

A first approach might be to obtain a point estimate for $f$, either through maximum likelihood estimation (which we have seen to overfit easily) or through maximum a posteriori estimation.
If, for example, $f$ is represented as a deep neural network, we have already seen how to find the MAP estimate of its weights in \cref{sec:bnn:map_inference}.

\begin{rmk}{The key pitfall of point estimates}{}
  However, using point estimates leads to a \emph{key pitfall} of model-based reinforcement learning.
  Using a point estimate of the model for planning --- even if this point estimate is very accurate --- often performs \emph{very poorly}.
  The reason is that planning is very good at overfitting (i.e., exploiting) small errors in the transition model.
  Moreover, the errors in the model estimate compound over time when using a longer time horizon $H$.
  The key to remedy this pitfall lies in being robust to misestimated models.
  This naturally suggests quantifying the uncertainty in our model estimate and taking it into account during planning.\safefootnote{Quantifying the uncertainty of an estimate is a problem that we have spent the first few chapters exploring. Notably, refer to \begin{itemize}
    \item \cref{sec:blr:uncertainty} for a description of epistemic and aleatoric uncertainty;
    \item \cref{sec:gp} for our use of uncertainty estimates in the context of Gaussian processes;
    \item \cref{sec:bdl} for our use of uncertainty estimates in the context of Bayesian deep learning; and
    \item \cref{sec:active_learning,sec:bayesian_optimization} for our use of epistemic uncertainty estimates to drive exploration.
  \end{itemize}}
  In the following section, we will rediscover that estimates of epistemic uncertainty are also extremely useful for driving (safe) exploration --- something that we have already encountered in our discussion of Bayesian optimization.\looseness=-1
\end{rmk}

In the following, we will differentiate between the epistemic uncertainty and the aleatoric uncertainty.
Recall from \cref{sec:blr:uncertainty} that epistemic uncertainty corresponds to our uncertainty about the model, $\r{p(f \mid \spD)}$, while aleatoric uncertainty corresponds to the uncertainty of the transitions in the underlying Markov decision process (which can be thought of as ``irreducible'' noise), $\b{p(\vx_{t+1} \mid f, \vx_t, \va_t)}$.

Intuitively, probabilistic inference of dynamics models corresponds to learning a distribution over possible models $f$ and $r$ given prior \emph{beliefs}, where $f$ and $r$ characterize the underlying Markov decision process.
This goes to show another benefit of the model-based over the model-free approach to reinforcement learning.
Namely, that it is much easier to encode prior knowledge about the transition and rewards model.

\begin{ex}{Inference with conditional Gaussian dynamics}{}
  Let us revisit inference with our conditional Gaussian dynamics model from \cref{eq:conditional_gaussian_dynamics}, \begin{align*}
    \vx_{t+1} \sim \N{\vmu(\vx_t, \va_t; \vpsi)}{\mSigma(\vx_t, \va_t; \vpsi)}.
  \end{align*}

  Recall that in the setting of Bayesian deep learning, most approximate inference techniques represented the approximate posterior using some form of a mixture of Gaussians,\safefootnote{see \begin{itemize}
    \item \cref{eq:bnn_posterior_vi} for variational inference;
    \item \cref{eq:bnn_posterior_mcmc} for Markov chain Monte Carlo;
    \item \cref{eq:bnn_posterior_dr} for dropout regularization; and
    \item \cref{eq:bnn_posterior_pe} for probabilistic ensembles.
  \end{itemize}} \begin{align}
    p(\vx_{t+1} \mid \spD, \vx_t, \va_t) \approx \frac{1}{m} \sum_{i=1}^m \N{\vmu(\vx_t, \va_t; \vpsi^{(i)})}{\mSigma(\vx_t, \va_t; \vpsi^{(i)})}.
  \end{align}
  Hereby, the epistemic uncertainty is represented by the variance between mixture components, and the aleatoric uncertainty by the average variance within the components.\safefootnote{see \cref{sec:bnn:approximate_inference:vi}}
\end{ex}

In supervised learning, we often conflated the notions of epistemic and aleatoric uncertainty.
In the context of planning, there is an important consequence of the decomposition into epistemic and aleatoric uncertainty.
Recall that the epistemic uncertainty corresponds to a distribution over Markov decision processes $f$, whereas the aleatoric uncertainty corresponds to the randomness in the transitions within one such MDP $f$.
Crucially, this randomness in the transitions must be consistent within a single MDP!
That is, once we selected a single MDP for planning, we should disregard the epistemic uncertainty and solely focus on the randomness of the transitions.
Then, to take into account epistemic uncertainty, we should average our plan across the different realizations of $f$.
This yields the following Monte Carlo estimate of our reward $\fnJ[H]$, \begin{align}
  \J{\va_{t:t+H-1}}[H] &\approx \frac{1}{m} \sum_{i=1}^m \J{\va_{t:t+H-1}; \r{f^{(i)}}}[H] \qquad\text{where} \label{eq:learning_with_planning} \\
  \J{\va_{t:t+H-1}; f}[H] &\defeq \sum_{\tau=t}^{t+H-1} \gamma^{\tau-t} r(\vx_\tau(\b{\vvarepsilon_{t:\tau-1}^{(i)}}; \va_{t:\tau-1}, f), \va_\tau) + \gamma^H \V{\vx_{t+H}}.
\end{align}
Here, $\r{f^{(i)} \iid p(f \mid \spD)}$ are independent samples of the transition model, and $\b{\vvarepsilon_{t:t+H-1}^{(i)} \iid \phi}$ parameterizes the dynamics analogously to \cref{eq:state_encoding}: \begin{align}\begin{split}
  &\vx_\tau(\vvarepsilon_{t:\tau-1}; \va_{t:\tau-1}, f) \\
  &\defeq f(\vvarepsilon_{\tau-1}; f(\dots; (\vvarepsilon_{t+1}; f(\vvarepsilon_t; \vx_t, \va_t), \va_{t+1}), \dots), \va_{\tau-1}).
\end{split}\end{align}
Observe that the epistemic and aleatoric uncertainty are treated differently.
Within a particular MDP $f$, we ensure that randomness (i.e., aleatoric uncertainty) is simulated consistently using our previous framework from our discussion of planning.\footnote{see \cref{eq:mbarl_planning_monte_carlo}}
The Monte Carlo samples of $f$ take into account the epistemic uncertainty about the transition model.
In our previous discussion of planning, we assumed the Markov decision process $f$ to be fixed.
Essentially, in \cref{eq:learning_with_planning} we are now using Monte Carlo trajectory sampling as a subroutine and average over an ``ensemble'' of Markov decision processes.

\begin{figure}
  \incfig{planning_epistemic_uncertainty}
  \caption{Illustration of planning with epistemic uncertainty and Monte Carlo sampling.
  The agent considers $m$ alternative ``worlds''. Within each world, it plans a sequence of actions over a finite time horizon.
  Then, the agent averages all optimal initial actions from all worlds.
  Crucially, each world by itself is \emph{consistent}.
  That is, its transition model (i.e., the aleatoric uncertainty of the model) is constant.}
\end{figure}

The same approach that we have seen in \cref{sec:mbarl:planning:parametric_policies} can be used to ``compile'' these plans into a parametric policy that can be trained offline, in which case, we write $\J{\pi}[H]$ instead of $\J{\va_{t:t+H-1}}[H]$.

This leads us to a first greedily exploitative algorithm for model-based reinforcement learning, which is shown in \cref{alg:rl_greedy}.
This algorithm is purely exploitative as it greedily maximizes the expected reward with respect to the transition model, taking into account epistemic uncertainty.\looseness=-1

\begin{algorithm}
  \caption{Greedy exploitation for model-based RL}\label{alg:rl_greedy}
  start with (possibly empty) data $\spD = \emptyset$ and a prior $p(f) = p(f \mid \spD)$\;
  \For{several episodes}{
    plan a new policy $\pi$ to (approximately) maximize, \vspace{-8pt}\begin{align}
      \max_\pi \E[f \sim p(\cdot \mid \spD)]{\J{\pi; f}[H]}
    \end{align}\vspace{-20pt}\;
    roll out policy $\pi$ to collect more data\;
    update posterior $p(f \mid \spD)$\;
  }
\end{algorithm}

In the context of Gaussian process models, this algorithm is called \midx{probabilistic inference for learning control}[idxpagebf] (PILCO) \citep{deisenroth2011pilco}, which was the first demonstration of how sample efficient model-based reinforcement learning can be.
As was mentioned in \cref{rmk:planning_as_inference}, the originally proposed PILCO uses moment matching instead of Monte Carlo averaging.

\begin{figure*}
  \includegraphics[width=\textwidth]{figures/pets.png}
  \caption{Sample efficiency of model-free and model-based reinforcement learning.
  DDPG and SAC are shown as constant (black) lines, because they take an order of magnitude more time steps before learning a good model.
  They also compare the PETS algorithm (blue) to deterministic ensembles (orange), where the transition models are assumed to be deterministic (or to have covariance $0$).
  Reproduced with permission from \icite{chua2018deep}.}
\end{figure*}

In the context of neural networks, this algorithm is called \midx{probabilistic ensembles with trajectory sampling} (PETS) \citep{chua2018deep}, which is one of the state-of-the-art algorithms.
PETS uses an ensemble of conditional Gaussian distributions over weights, trajectory sampling for evaluating the performance and model predictive control for planning.

Notably, PETS does not explicitly explore.
Exploration only happens due to the uncertainty in the model, which already drives exploration to some extent.
In many settings, however, this incentive is not sufficient for exploration --- especially when rewards are sparse.

\begin{oreadings}
  \begin{itemize}
    \item \textbf{PILCO:} \pcite{deisenroth2011pilco}
    \item \textbf{PETS:} \pcite{chua2018deep}
  \end{itemize}
\end{oreadings}

\subsection{Partial Observability}

Depending on the task, it may be difficult to learn the observed dynamics directly.
For example, when learning an artificial player for a computer game based only on the games' visual interface it may be difficult to predict the next frame in pixel space.
Instead, a common approach is to treat the visual interface as an observation in a POMDP with a hidden latent space (cf. \cref{sec:mdp:partial_observability}), and to learn the dynamics within the latent space and the observation model separately.

An example of this approach is the \midx{deep planning network} (PlaNet) algorithm which learns such a POMDP via variational inference and uses the cross-entropy method for closed-loop planning of action sequences \citep{hafner2019learning}.
The \midx{Dreamer} algorithm replaces the closed-loop planning of PlaNet by open-loop planning with entropy regularization \citep{hafner2019dream,hafner2020mastering}.
Notably, neither PlaNet nor Dreamer explicitly take into account epistemic model uncertainty during planning but rather use point estimates.

\section{Exploration}\label{sec:mbarl:exploration}\pidx{exploration-exploitation dilemma}%

Exploration is critical when interacting with unknown environments, such as in reinforcement learning.
We first encountered the exploration-exploitation dilemma in our discussion of Bayesian optimization, where we aimed at maximizing an unknown function in as little time as possible by making noisy observations.\footnote{see \cref{sec:bayesian_optimization}}
Within the framework of Bayesian optimization, we used so-called acquisition functions\pidx{acquisition function} for selecting the next point at which to observe the unknown function.
Observe that these acquisition functions are analogous to policies in the setting of reinforcement learning.
In particular, the policy that uses greedy exploitation like we have seen in the previous section is analogous to simply picking the point that maximizes the mean of the posterior distribution.
In the context of Bayesian optimization, we have already seen that this is insufficient for exploration and can easily get stuck in locally optimal solutions.
As reinforcement learning is a strict generalization of Bayesian optimization, there is no reason why such a strategy should be sufficient now.

Recall from our discussion of Bayesian optimization that we ``solved'' this problem by using the epistemic uncertainty in our model of the unknown function to pick the next point to explore.
This is what we will now explore in the context of reinforcement learning.

One simple strategy that we already investigated is the addition of some \midx{exploration noise}.
In other words, one follows a greedy exploitative strategy, but every once in a while, one chooses a random action (like in $\varepsilon$-greedy\pidx{$\varepsilon$-greedy}); or one adds additional noise to the selected actions (known as Gaussian noise ``dithering''\pidx{Gaussian noise ``dithering''}[idxpagebf]).
We have already seen that in difficult exploration tasks, these strategies are not systematic enough.

Two other approaches that we have considered before are Thompson sampling (cf. \cref{sec:bayesian_optimization:acquisition_functions:thompson_sampling}) and, more generally, the principle of \midx{optimism in the face of uncertainty} (cf. \cref{sec:bayesian_optimization:online_learning:mab}).

\subsection{Thompson Sampling}\pidx{Thompson sampling}

Recall from \cref{sec:bayesian_optimization:acquisition_functions:thompson_sampling} the main idea behind Thompson sampling: namely, that the randomness in the realizations of $f$ from the posterior distribution is already enough to drive exploration.
That is, instead of picking the action that performs best on average across all realizations of $f$, Thompson sampling picks the action that performs best for a \emph{single realization} of $f$.
The epistemic uncertainty in the realizations of~$f$ leads to variance in the picked actions and provides an additional incentive for exploration.
This yields \cref{alg:thompson_sampling} which is an immediate adaptation of greedy exploitation and straightforward to implement.

\begin{algorithm}
  \caption{Thompson sampling}\label{alg:thompson_sampling}
  start with (possibly empty) data $\spD = \emptyset$ and a prior $p(f) = p(f \mid \spD)$\;
  \For{several episodes}{
    sample a model $f \sim p(\cdot \mid \spD)$\;
    plan a new policy $\pi$ to (approximately) maximize, \vspace{-8pt}\begin{align}
      \max_\pi \J{\pi; f}[H]
    \end{align}\vspace{-20pt}\;
    roll out policy $\pi$ to collect more data\;
    update posterior $p(f \mid \spD)$\;
  }
\end{algorithm}

\subsection{Optimistic Exploration}\label{sec:mbarl:exploration:optimistic}\pidx{optimism in the face of uncertainty}

We have already seen in the context of Bayesian optimization and tabular reinforcement learning that optimism is a central pillar for exploration.
But how can we explore optimistically in model-based reinforcement learning?

Let us consider a set $\spM(\spD)$ of \midx<plausible models>{plausible model} given some data $\spD$.
Optimistic exploration would then optimize for the most advantageous model among all models that are plausible given the seen data.

\begin{ex}{Plausible conditional Gaussians}{}
  In the context of conditional Gaussians, we can consider the set of all models such that the prediction of a single dimension $i$ lies in some confidence interval, \begin{align}
    f_i(\vx, \va) \in \spC_{i} \defeq \begin{multlined}[t]
      [\mu_i(\vx,\va\mid\spD)-\beta_i\sigma_i(\vx,\va\mid\spD), \\ \mu_i(\vx,\va\mid\spD)+\beta_i\sigma_i(\vx,\va\mid\spD)],
    \end{multlined} \label{eq:plausible_gaussian_confidence_intervals}
  \end{align} where $\beta_i$ tunes the width of the confidence interval, analogously to \cref{sec:bayesian_optimization:acquisition_functions:ucb}.
  The set of plausible models is then given as \begin{align}
    \spM(\spD) \defeq \{f \mid \forall \vx \in \spX, \va \in \spA, i \in [d] : f_i(\vx, \va) \in \spC_{i}\}.
  \end{align}
\end{ex}

When compared to greedy exploitation, instead of taking the optimal step on average with respect to all realizations of the transition model~$f$, optimistic exploration as shown in \cref{alg:optimistic_exploration} takes the optimal step with respect to the most optimistic model among all transition models that are consistent with the data.

\begin{algorithm}
  \caption{Optimistic exploration}\pidx{optimistic exploration}\label{alg:optimistic_exploration}
  start with (possibly empty) data $\spD = \emptyset$ and a prior $p(f) = p(f \mid \spD)$\;
  \For{several episodes}{
    plan a new policy $\pi$ to (approximately) maximize, \vspace{-8pt}\begin{align}
      \max_\pi \max_{f \in \spM(\spD)} \J{\pi; f}[H]
    \end{align}\vspace{-20pt}\;
    roll out policy $\pi$ to collect more data\;
    update posterior $p(f \mid \spD)$\;
  }
\end{algorithm}

In general, the joint maximization over $\pi$ and $f$ is very difficult to solve computationally.
Yet, remarkably, it turns out that this complex optimization problem can be reduced to standard model-based reinforcement learning with a fixed model.
The key idea is to consider an agent that can control its ``luck''.
In other words, we assume that the agent believes it can control the outcome of its actions --- or rather choose which of the plausible dynamics it follows.
The ``luck'' of the agent can be considered as additional decision variables.
Consider the optimization problem, \begin{align}
  \pi_{t+1} \defeq \argmax_\pi \max_{\veta(\cdot) \in [-1,1]^d} \J{\pi; \widehat{f}_t}[H]
\end{align} with ``optimistic'' dynamics \begin{align}
  \widehat{f}_{t,i}(\vx, \va) \defeq \mu_{t, i}(\vx, \va) + \beta_{t, i} \eta_i(\vx, \va) \sigma_{t, i}(\vx, \va). \label{eq:dynamics_with_luck}
\end{align}
Here the decision variables $\eta_i$ control the variance of an action.
That is, within the confidence bounds of the transition model, the agent can freely choose the state that is reached by playing an action $\va$ from state $\vx$.
Essentially, this corresponds to maximizing expected reward in an augmented (optimistic) MDP with \emph{known} dynamics $\widehat{f}$ and with a larger action space that also includes the decision variables $\veta$.
This is a known MDP for which we can use our toolbox for planning which we developed in \cref{sec:mbarl:planning}.

The algorithm that maximizes expected reward in this augmented MDP is called \midx{hallucinated upper confidence reinforcement learning} (H-UCRL) \citep{curi2020efficient,treven2023efficient}.
H-UCRL can be seen as the natural extension of the UCB acquisition function from Bayesian optimization to reinforcement learning.
An illustration of the algorithm is given in \cref{fig:hucrl}.

\begin{figure}
  \incplt{hucrl}
  \caption{Illustration of H-UCRL in a one-dimensional state space.
  The agent ``hallucinates'' that it takes the black trajectory when, in reality, the outcomes of its actions are as shown in blue.
  The agent can hallucinate to land anywhere within the gray confidence regions (i.e., the epistemic uncertainty in the model) using the luck decision variables $\veta$.
  This allows agents to discover long sequences of actions leading to sparse rewards.}\label{fig:hucrl}
\end{figure}

Intuitively, the agent has the tendency to believe that it can achieve much more than it actually can.
As more data is collected, the confidence bounds shrink and the optimistic policy rewards converge to the actual rewards.
Yet, crucially, we only collect data in regions of the state-action space that are more promising than the regions that we have already explored.
That is, we only collect data in the most promising regions.

Optimistic exploration yields the strongest effects for hard exploration tasks, for example, settings with large penalties associated with performing certain actions and settings with sparse rewards.\footnote{Action penalties are often used to discourage the agent from exhibiting unwanted behavior.
However, increasing the action penalty increases the difficulty of exploration.
Therefore, optimistic exploration is especially useful in settings where we want to practically disallow many actions by attributing large penalties to them.}
In those settings, most other strategies (i.e., those that are not sufficiently explorative), learn not to act at all.
However, even in settings of ``ordinary rewards'', optimistic exploration often learns good policies faster.

\subsection{Constrained Exploration}\label{sec:mbarl:exploration:constrained}

Besides making exploration more efficient, another use of uncertainty is to make exploration more safe.
Today, reinforcement learning is still far away from being deployed directly to the real world.
In practice, reinforcement learning is almost always used together with a simulator, in which the agent can ``safely'' train and explore.
Yet, in many domains, it is not possible to simulate the training process, either because we are lacking a perfect model of the environment, or because simulating such a model is too computationally inefficient.
This is where we can make use of uncertainty estimates of our model to avoid unsafe states.

\begin{marginfigure}
  \incfig{planning_with_conf_bounds}
  \caption{Illustration of planning with confidence bounds.
  The unsafe set of states is shown as the red region.
  The agent starts at the position denoted by the black dot and plans a sequence of actions.
  The confidence bounds on the subsets of states that are reached by this action sequence are shown in gray.}
\end{marginfigure}

Let us denote by $\spX_\mathrm{unsafe}$ the subset of unsafe states of our state space~$\spX$.
A natural idea is to perform planning using confidence bounds of our epistemic uncertainty.
This allows us to pessimistically forecast the plausible consequences of our actions, given what we have already learned about the transition model.
As we collect more data, the confidence bounds will shrink, requiring us to be less conservative over time.
This idea is also at the heart of fields like \midx{robust control}.

A general formalism for planning under constraints is the notion of \midx{constrained Markov decision processes} \citep{altman1999constrained}.
Given dynamics $f$, planning in constrained MDPs amounts to the following optimization problem: \begin{subequations}\begin{align}
  \max_\pi \quad&J_{\mu}(\pi; f) \defeq \E[\vx_0 \sim \mu, \vx_{1:\infty} \mid \pi, f]{\sum_{t=0}^\infty \gamma^t r_t} \\
  \text{subject to} \quad&J_{\mu}^{c}(\pi; f) \leq \delta
\end{align}\label{eq:cmdp_opt}\end{subequations} where $\mu$ is a distribution over the initial state and \begin{align}
  J_{\mu}^{c}(\pi; f) \defeq \E[\vx_0 \sim \mu, \vx_{1:\infty} \mid \pi, f]{\sum_{t=0}^\infty \gamma^t c(\vx_t)}
\end{align} are expected discounted costs with respect to a cost function ${c : \spX \to \Rzero}$.\footnote{It is straightforward to extend this framework to allow for multiple constraints.}
Observe that for the cost function $c(\vx) \defeq \Ind{\vx \in \spX_\mathrm{unsafe}}$, the value $J_{\mu}^c(\pi; f)$ can be interpreted as an upper bound on the (discounted) probability of visiting unsafe states under dynamics $f$,\footnote{This follows from a simple union bound \eqref{eq:union_bound}.} and hence, the constraint $J_{\mu}^c(\pi; f) \leq \delta$ bounds the probability of visiting an unsafe state when following policy $\pi$.\looseness=-1

The \midx{augmented Lagrangian method} can be used to solve the optimization problem of \cref{eq:cmdp_opt}.\footnote{For an introduction, read chapter 17 of \icite{wright1999numerical}.}
Thereby, one solves \begin{align}
  &\max_\pi \min_{\lambda \geq 0} \quad J_\mu(\pi; f) - \lambda (J_\mu^{c}(\pi; f) - \delta) \\
  &= \max_\pi \begin{cases}
    J_\mu(\pi; f) & \text{if $\pi$ is feasible} \\
    -\infty & \text{otherwise}.
  \end{cases}
\end{align}
Observe that if $\pi$ is feasible, then $J_\mu^{c}(\pi; f) \leq \delta$ and so the minimum over $\lambda$ is satisfied if $\lambda = 0$.
Conversely, if $\pi$ is infeasible, $\lambda$ can be made arbitrarily large to solve the optimization problem.
In practice, an additional penalty term is added to smooth the objective when transitioning between feasible and infeasible policies.

Note that solving constrained optimization problems such as \cref{eq:cmdp_opt} yields an optimal safe policy.
However, it is not ensured that constraints are not violated during the search for the optimal policy.
Generally, exterior penalty methods such as the augmented Lagrangian method allow for generating infeasible points during the search, and are therefore not suitable when constraints have to be strictly enforced at all times.
Thus, this method is more applicable in the episodic setting (e.g., when an agent is first ``trained'' in a simulated environment and then ``deployed'' to the actual task) rather than in the continuous setting where the agent has to operate in the ``real world'' from the beginning and cannot easily be reset.

\begin{rmk}{Barrier functions}{}
  The augmented Lagrangian method is merely one example of optimizing a joint objective of maximizing rewards and minimizing costs to find a policy with bounded costs.
  Another example of this approach are so-called \midx<Barrier functions>{Barrier function} which augment the reward objective with a smoothed approximation of the boundary of a set $\spX_\mathrm{unsafe}$.
  That is, one solves \begin{align}
    \max_\pi J_\mu(\pi; f) - \lambda \cdot B_\mu^c(\pi; f)
  \end{align} for some $\lambda > 0$ where the barrier function $B_\mu^c(\pi; f)$ goes to infinity as a state on the boundary of $\spX_\mathrm{unsafe}$ is approached.
  Examples for barrier functions are $- \log c(\vx)$ and $\frac{1}{c(\vx)}$.

  Barrier functions are an interior penalty method which ensure that points are feasible during the search for the optimal solution.
\end{rmk}

So far, we have assumed knowledge of the true dynamics to solve the optimization problem of \cref{eq:cmdp_opt}.
If we do not know the true dynamics but instead have access to a set of plausible models $\spM(\spD)$ given data $\spD$ (cf. \cref{sec:mbarl:exploration:optimistic}) which encompasses the true dynamics, then a natural strategy is to be \g{optimistic} with respect to future rewards and \r{pessimistic} with respect to future constraint violations.
More specifically, we solve the optimization problem, \begin{subequations}\begin{align}
  \max_\pi \g{\max_{f \in \spM(\spD)}} \quad&J_{\mu}(\pi; f) \margintag{\emph{optimistic}} \\
  \text{subject to} \quad&\r{\max_{f \in \spM(\spD)}} J_{\mu}^{c}(\pi; f) \leq \delta. \margintag{\emph{pessimistic}}
\end{align}\label{eq:lambda}\end{subequations}
Intuitively, jointly maximizing $J_\mu(\pi; f)$ with respect to $\pi$ and (plausible) $f$ can lead the agent to try behaviors with potentially high reward due to \emph{optimism} (i.e., the agent ``dreams'' about potential outcomes).
On the other hand, being \emph{pessimistic} with respect to constraint violations enforces the safety constraints (i.e., the agent has ``nightmares'' about potential outcomes).

If the policy values $J_\mu(\pi; f)$ and $J_\mu^{c}(\pi; f)$ are modeled as a distribution (e.g., using Bayesian deep learning), then the inner maximization over plausible dynamics can be approximated using samples from the posterior distributions.
Thus, the augmented Lagrangian method can also be used to solve the general optimization problem of \cref{eq:lambda}.
The resulting algorithm is known as \midx{Lagrangian model-based agent} (LAMBDA) \citep{as2022constrained}.

\subsection{Safe Exploration}\label{sec:mbarl:exploration:safe}

As noted in the previous section, in many settings we do not only want to eventually find a safe policy, but we also want to ensure that we act safely while searching for an optimal policy.
To this end, recall the general approach outlined in the beginning of the previous section wherein we plan using (pessimistic) confidence bounds of the plausible consequences of our actions.

One key challenge of this approach is that we need to forecast plausible trajectories.
The confidence bounds of such trajectories cannot be nicely represented anymore.
One approach is to over-approximate the confidence bounds over reachable states from trajectories.

\begin{thm}[informal, \cite{koller2018learning}]
  For conditional Gaussian dynamics, the reachable states of trajectories can be over-approximated with high probability.\looseness=-1
\end{thm}

\begin{marginfigure}
  \incfig{planning_long_term_consequences}
  \caption{Illustration of long-term consequence when planning a finite number of steps.
  Green dots are to denote safe states and the red dot is to denote an unsafe state.
  After performing the first action, the agent is still able to return to the previous state.
  Yet, after reaching the third state, the agent is already guaranteed to end in an unsafe state.
  When using only a finite horizon of $H=2$ for planning, the agent might make this transition regardless.}
\end{marginfigure}

Another key challenge is that actions might have consequences that exceed the time horizon used for planning.
In other words, by performing an action now, our agent might put itself into a state that is not yet unsafe, but out of which it cannot escape and which will eventually lead to an unsafe state.
You may think of a car driving towards a wall.
When a crash with the wall is designated as an unsafe state, there are quite a few states in advance at which it is already impossible to avoid the crash.
Thus, looking ahead a finite number of steps is not sufficient to prevent entering unsafe states.

It turns out that it is still possible to use the epistemic uncertainty about the model and short-term plausible behaviors to make guarantees about certain long-term consequences.
One idea is to also consider a set of safe states $\spX_\mathrm{safe}$ alongside the set of unsafe states $\spX_\mathrm{unsafe}$, for which our agent knows how to stay inside (i.e., remain safe).
In other words, for states $\vx \in \spX_\mathrm{safe}$, we know that our agent can always behave in such a way that it will not reach an unsafe state.\footnote{In the example of driving a car, the set of safe states corresponds to those states where we know that we can still safely brake before hitting the wall.}
An illustration of this approach is shown in \cref{fig:safe_learning_based_mpc}.

\begin{figure}
  \incfig{safe_learning_based_mpc}
  \caption{Illustration of long-term consequence when planning a finite number of steps.
  The unsafe set of states is shown in red and the safe set of states is shown in blue.
  The confidence intervals corresponding to actions of the performance plan and safety plan are shown in orange and green, respectively.}\label{fig:safe_learning_based_mpc}
\end{figure}

The problem is that this set of safe states might be very conservative.
That is to say, it is likely that rewards are mostly attained \emph{outside} of the set of safe states.
The key idea is to plan two sequences of actions, instead of only one.
``Plan A'' (the \midx{performance plan}) is planned with the objective to solve the task, that is, attain maximum reward.
``Plan B'' (the \midx{safety plan}) is planned with the objective to return to the set of safe states $\spX_\mathrm{safe}$.
In addition, we enforce that both plans must agree on the first action to be played.

Under the assumption that the confidence bounds are conservative estimates, this guarantees that after playing this action, our agent will still be in a state of which it can return to the set of safe states.
In this way, we can gradually increase the set of states that are safe to explore.
It can be shown that under suitable conditions, returning to the safe set can be guaranteed \citep{koller2018learning}.

\subsection{Safety Filters}

An alternative approach is to (slightly) adjust a potentially unsafe policy $\pi$ to obtain a policy $\hat{\pi}$ which avoids entering unsafe states with high probability.

Following our interpretation of constrained policy optimization in terms of optimism and pessimism from \cref{sec:mbarl:exploration:constrained}, to pessimistically evaluate the safety of a policy with respect to a cost function $c$ given a set of plausible models $\spM(\spD)$, we can use \begin{align}
  C^\pi(\vx) \defeq \max_{f \in \spM(\spD)} J_{\delta_\vx}^c(\pi; f) \margintag{$\delta_\vx$ is the point density at $\vx$} \label{eq:pessimistic_cost_estimate}
\end{align} where the initial-state distribution $\delta_\vx$ is to denote that the initial state is $\vx$.
Observe that \cref{eq:pessimistic_cost_estimate} permits a reparameterization in terms of additional decision variables $\veta$ which is analogous to our discussion in \cref{sec:mbarl:exploration:optimistic}.
Concretely, we have \begin{align}
  C^\pi(\vx) = \max_\veta J_{\delta_\vx}^c(\pi; \widehat{f})
\end{align} where $\widehat{f}$ are the adjusted dynamics \eqref{eq:dynamics_with_luck} which are based on the ``luck'' variables $\veta$.
In the context of estimating costs which we aim to minimize (as opposed to rewards which we aim to maximize), $\veta$ can be interpreted as the ``bad luck'' of the agent.

When our only objective is to act safely, that is, we only aim to minimize cost and are indifferent to rewards, then this reparameterization allows us to find a ``maximally safe'' policy, \begin{align}
  \pi_{\mathrm{safe}} \defeq \argmin_\pi \E[\vx \sim \mu]{C^\pi(\vx)} = \argmin_\pi \max_\veta J_{\mu}^c(\pi; \widehat{f}).
\end{align}
Under some conditions $\pi_{\mathrm{safe}}$ can be shown to satisfy $\pi_{\mathrm{safe}}(\vx) = \pi_{\vx}(\vx)$ for any $\vx$ where $\pi_{\vx} \defeq \argmin_\pi C^\pi(\vx)$ \citep[proposition 4.2]{curi2022safe}.
On its own, the policy $\pi_{\mathrm{safe}}$ is rather useless for exploring the state space.
In particular, when in a state that we already deem safe, following policy $\pi_{\mathrm{safe}}$, the agent aims simply to ``stay'' within the set of safe states which means that it has no incentive to explore / maximize reward.\looseness=-1

Instead, one can interpret $\pi_{\mathrm{safe}}$ as a ``backup'' policy in case we realize upon exploring that a certain trajectory is too dangerous, akin to our notion of the ``safety plan'' in \cref{sec:mbarl:exploration:safe}.
That is, given any (potentially explorative and dangerous) policy $\pi$, we can find the adjusted policy,\looseness=-1 \begin{subequations}\begin{align}
  \hat{\pi}(\vx) \defeq \argmin_{\va \in \spA} \quad&d(\pi(\vx), \va) \\
  \text{subject to} \quad&\max_\veta C^{\pi_{\mathrm{safe}}}(\widehat{f}(\vx, \va)) \leq \delta
\end{align}\end{subequations} for some metric $d(\cdot, \cdot)$ on $\spA$.
The constraint ensures that the pessimistic next state $\widehat{f}(\vx, \va)$ is recoverable by following policy $\pi_{\mathrm{safe}}$.
In this way, \begin{align}\begin{split}
  \tilde{\pi}(\vx) \defeq \begin{cases}
    \hat{\pi}(\vx) & \text{if $C^{\pi_{\mathrm{safe}}}(\vx) \leq \delta$} \\
    \pi_{\mathrm{safe}}(\vx) & \text{if $C^{\pi_{\mathrm{safe}}}(\vx) > \delta$} \\
  \end{cases}
\end{split}\end{align} is the policy ``closest'' to $\pi$ which is $\delta$-safe with respect to the pessimistic cost estimates $C^{\pi_{\mathrm{safe}}}$ \citep{curi2022safe}.\footnote{The policy $\tilde{\pi}$ is required as, a priori, it is not guaranteed that the state $\vx_t$ will satisfy $C^{\pi_{\mathrm{safe}}}(\vx_t) \leq \delta$ for all $t$ unless $\hat{\pi}$ is replanned after every step.}
This is also called a \midx{safety filter}.
Similar approaches using backup policies also allow safe exploration in the model-free setting \citep{sukhija2022scalable,widmer2023tuning}.

\begin{oreadings}
  \begin{itemize}
    \item \pcite{curi2020efficient}
    \item \pcite{berkenkamp2017safe}
    \item \pcite{koller2018learning}
    \item \pcite{as2022constrained}
    \item \pcite{curi2022safe}
    \item \pcite{turchetta2019safe}
  \end{itemize}
\end{oreadings}

\section*{Discussion}

In this final chapter, we learned that leveraging a learned ``world model'' for planning can be dramatically more sample-efficient than model-free reinforcement learning.
Additionally, such world models allow for effective multitask learning since they can be reused across tasks and only the reward function needs to be swapped out.\footnote{A similar idea, though often classified as model-free, is to learn a goal-conditioned policy which is studied extensively in \midx{goal-conditioned reinforcement learning}~\citep{andrychowicz2017hindsight,plappert2018multi,park2024ogbench}.}
Finally, we explored how to use probabilistic inference of our world model to drive exploration more effectively and safely.

\excheading

\begin{nexercise}{Bounding the regret of H-UCRL}{hucrl_regret}
  Analogously to our analysis of the UCB acquisition function for Bayesian optimization, we can use optimism to bound the regret of H-UCRL.
  We assume for simplicity that $d = 1$ and drop the index $i$ in the following.
  Let us denote by $x_{t,k}$ the $k$-th state visited during episode $t$ when following policy $\pi_t$.
  We denote by $\widehat{x}_{t,k}$ the corresponding state but under the optimistic dynamics $\widehat{f}$ rather than the true dynamics $f$.
  The instantaneous regret during episode $t$ is given by \begin{align}
    r_t = J_H(\pis; f) - J_H(\pi_t; f)
  \end{align} where we take the objective to be $J_H(\pi; f) = \sum_{k=0}^{H-1} r(x_{t,k}, \pi(x_{t,k}))$.

  You may assume that $f(x, \pi(x))$, $r(x, \pi(x))$, and $\sigma(x, \pi(x))$ are Lipschitz continuous in $x$.
  \begin{enumerate}
    \item Show by induction that for any $k \geq 0$, with high probability, \begin{align}
      \norm{\widehat{x}_{t,k} - x_{t,k}} \leq 2 \beta_t \sum_{l=0}^{k-1} \alpha_t^{k-1-l} \sigma_{t-1}(x_{t,l}, \pi_t(x_{t,l})) \label{eq:hucrl_regret:induction}
    \end{align} where $\alpha_t$ depends on the Lipschitz constants and $\beta_t$.

    \item Assuming w.l.o.g. that $\alpha_t \geq 1$, show that with high probability, \begin{align}
      r_t \leq \BigO{\beta_t H \alpha_t^{H-1} \sum_{k=0}^{H-1} \sigma_{t-1}(x_{t,k}, \pi_t(x_{t,k}))}
    \end{align}

    \item Let $\Gamma_T \defeq \max_{\pi_1, \dots \pi_T} \sum_{t=1}^T \sum_{k=0}^{H-1} \sigma_{t-1}^2(x_{t,k}, \pi_t(x_{t,k}))$.
    Analogously to \cref{exercise:bayesian_regret_for_gp_ucb}, it can be derived that $\Gamma_T \leq \BigO{H \gamma_T}$ if the dynamics are modeled by a Gaussian process.\safefootnote{For details, see appendix A of \cite{treven2023efficient}.}
    Bound the cumulative regret $R_T = \sum_{t=1}^T r_t \leq \BigO{\beta_{T} H^{\frac{3}{2}} \alpha_T^{H-1} \sqrt{T \Gamma_T}}$.
  \end{enumerate}
  Thus, if the dynamics are modeled by a Gaussian process with kernel such that $\gamma_T$ is sublinear, the regret of H-UCRL is sublinear.
\end{nexercise}
