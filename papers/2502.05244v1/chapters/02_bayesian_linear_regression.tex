\chapter{Linear Regression}\label{sec:blr}\pidx{linear regression}

As a first example of probabilistic inference, we will study linear models for regression\footnote{As we have discussed in \cref{sec:fundamentals:supervised_learning}, regression models can also be used for classification. The canonical example of a linear model for classification is logistic regression, which we will discuss in \cref{sec:approximate_inference:bayesian_logistic_regression}.} which assume that the output $y \in \R$ is a linear function of the input $\vx \in \R^d$: \begin{align*}
  y \approx \transpose{\vw}\vx + w_0
\end{align*} where $\vw \in \R^d$ are the weights and $w_0 \in \R$ is the intercept.
Observe that if we define the extended inputs $\vx' \defeq (\vx, 1)$ and $\vw' \defeq (\vw, w_0)$, then $\transpose{\vw'}\vx' = \transpose{\vw}\vx~+~w_0$, implying that without loss of generality it suffices to study linear functions without the intercept term $w_0$.
We will therefore consider the following function class of linear models \begin{align*}
  f(\vx; \vw) \defeq \transpose{\vw} \vx.
\end{align*}

\begin{marginfigure}
  \incplt{linear_regression}
  \caption{Example of linear regression with the least squares estimator (shown in blue).}\label{fig:lr}
\end{marginfigure}

We will consider the supervised learning task of learning weights $\vw$ from labeled training data $\{(\vx_i, y_i)\}_{i=1}^n$.
We define the \midx{design matrix}, \begin{align}
  \mX \defeq \begin{bmatrix}
    \transpose{\vx_1} \\
    \vdots \\
    \transpose{\vx_n} \\
  \end{bmatrix} \in \R^{n \times d},
\end{align} as the collection of inputs and the vector $\vy \defeq \transpose{[y_1 \cdots y_n]} \in \R^n$ as the collection of labels.
For each noisy observation $(\vx_i, y_i)$, we define the value of the approximation of our model, $f_i \defeq \transpose{\vw} \vx_i$.
Our model at the inputs $\mX$ is described by the vector $\vf \defeq \transpose{[f_1 \; \cdots \; f_n]}$ which can be expressed succinctly as $\vf = \mX \vw$.

The most common way of estimating $\vw$ from data is the \midx{least squares estimator},\looseness=-1 \begin{align}
  \vwhat_{\ls} \defeq \argmin_{\vw \in \R^d} \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2 = \argmin_{\vw \in \R^d} \norm{\vy - \mX\vw}_2^2, \label{eq:least_squares}
\end{align} minimizing the squared difference between the labels and predictions of the model.
A slightly different estimator is used for \midx{ridge regression}, \begin{align}
  \vwhat_{\ridge} \defeq \argmin_{\vw \in \R^d} \norm{\vy - \mX\vw}_2^2 + \lambda \norm{\vw}_2^2
\end{align} where $\lambda > 0$.
The squared $L_2$ regularization term $\lambda \norm{\vw}_2^2$ penalizes large $\vw$ and thus reduces the ``complexity'' of the resulting model.\footnote[][-\baselineskip]{Ridge regression is more robust to multicollinearity than standard linear regression. \idx<Multicollinearity>{multicollinearity} occurs when multiple independent inputs are highly correlated. In this case, their individual effects on the predicted variable cannot be estimated well. Classical linear regression is highly volatile to small input changes. The regularization of ridge regression reduces this volatility by introducing a bias on the weights towards $0$.}
It can be shown that the unique solutions to least squares and ridge regression are given by \begin{align}
  \vwhat_{\ls} &= \inv{(\transpose{\mX} \mX)} \transpose{\mX} \vy \label{eq:linear_regression} \qquad\text{and} \\
  \vwhat_{\ridge} &= \inv{(\transpose{\mX} \mX + \lambda \mI)} \transpose{\mX} \vy, \label{eq:ridge_regression}
\end{align} respectively if the Hessian of the loss is positive definite (i.e., the loss is strictly convex) \exerciserefmark{closed_form_linear_regression}[1] which is the case as long as the columns of $\mX$ are not linearly dependent.
Least squares regression can be seen as finding the orthogonal projection of $\vy$ onto the column space of $\mX$, as is illustrated in \cref{fig:linear_regression_projection} \exerciserefmark{closed_form_linear_regression}[2].

\begin{marginfigure}[9\baselineskip]
  \incplt{linear_regression_projection}
  \caption{Least squares regression finds the orthogonal projection of $\vy$ onto $\mathrm{span}\{\mX\}$ (here illustrated as the plane).}
  \label{fig:linear_regression_projection}
\end{marginfigure}

\subsection{Maximum Likelihood Estimation}\label{sec:least_squares_as_mle}

Since our function class comprises linear functions of the form $\transpose{\vw} \vx$, the observation model from \cref{eq:data} simplifies to \begin{align}
  y_i = \transpose{{\opt{\vw}}} \vx_i + \varepsilon_i \label{eq:linear_data}
\end{align} for some weight vector $\vw$, where for the purpose of this chapter we will additionally assume that $\varepsilon_i \sim \N{0}{\sigman^2}$ is homoscedastic Gaussian noise.\footnote[][0.5\baselineskip]{$\varepsilon_i$ is called \idx{additive white Gaussian noise}.}
This observation model is equivalently characterized by the Gaussian likelihood, \begin{align}
  y_i \mid \vx_i, \vw \sim \N{\transpose{\vw} \vx_i}{\sigman^2}. \label{eq:blr_likelihood} \margintag{using \cref{eq:cond_linear_gaussian}}
\end{align}
Based on this likelihood we can compute the MLE \eqref{eq:mle} of the weights: \begin{align*}
  \vwhat_\MLE &= \argmax_{\vw \in \R^d} \sum_{i=1}^n \log p(y_i \mid \vx_i, \vw) = \argmin_{\vw \in \R^d} \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2. \margintag{plugging in the Gaussian likelihood and simplifying}
\end{align*}
Note that therefore $\vwhat_\MLE = \vwhat_{\ls}$.

In practice, the noise variance $\sigman^2$ is typically unknown and also has to be determined, for example, through maximum likelihood estimation. It is a straightforward exercise to check that the MLE of $\sigman^2$ given fixed weights $\vw$ is $\hat{\sigma}_{\mathrm{n}}^2 = \frac{1}{n} \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2$ \exerciserefmark{noise_variance_mle}.

\section{Weight-space View}\pidx{weight-space view}\label{sec:blr:weight_space_view}

The most immediate and natural probabilistic interpretation of linear regression is to quantify uncertainty about the weights $\vw$.
Recall that probabilistic inference requires specification of a generative model comprised of prior and likelihood.
Throughout this chapter, we will use the Gaussian prior, \begin{align}
  \vw \sim \N{\vzero}{\sigmap^2 \mI},
\end{align} and the Gaussian likelihood from \cref{eq:blr_likelihood}.
We will discuss possible (probabilistic) strategies for choosing hyperparameters such as the prior variance $\sigmap^2$ and the noise variance $\sigman^2$ in \cref{sec:gp:model_selection}.

\begin{marginfigure}
  \incfig{blr_dgm}
  \caption{Directed graphical model of Bayesian linear regression in plate notation.}\label{fig:blr}
\end{marginfigure}

\begin{rmk}{Why a Gaussian prior?}{max_entropy_principle}
  The choice of using a Gaussian prior may seem somewhat arbitrary at first sight, except perhaps for the nice analytical properties of Gaussians that we have seen in \cref{sec:fundamentals:gaussians} and which will prove useful.
  The maximum entropy principle (cf. \cref{sec:fundamentals:inference:priors}) provides a more fundamental justification for Gaussian priors since turns out that $\mathcal{N}$ has the maximum entropy among all distributions on $\R^d$ with known mean and variance \exerciserefmark{maximum_entropy_principle}.
\end{rmk}

Next, let us derive the posterior distribution over the weights. \begin{align}
  &\log p(\vw \mid \vx_{1:n}, y_{1:n}) \nonumber \\[3pt]
  &= \log p(\vw) + \log p(y_{1:n} \mid \vx_{1:n}, \vw) + \const \margintag{by Bayes' rule \eqref{eq:bayes_rule}} \nonumber \\
  &= \log p(\vw) + \sum_{i=1}^n \log p(y_i \mid \vx_i, \vw) + \const \margintag{using independence of the samples} \nonumber \\
  &= -\frac{1}{2} \brackets*{\sigmap^{-2} \norm{\vw}_2^2 + \sigman^{-2} \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2} + \const \margintag{using the Gaussian prior and likelihood} \nonumber \\
  &= -\frac{1}{2} \brackets*{\sigmap^{-2} \norm{\vw}_2^2 + \sigman^{-2} \norm{\vy - \mX\vw}_2^2} + \const \margintag{using $\sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2 = \norm{\vy - \mX\vw}_2^2$} \nonumber \\
  &= -\frac{1}{2} \brackets*{\sigmap^{-2} \transpose{\vw}\vw + \sigman^{-2} \parentheses*{\transpose{\vw}\transpose{\mX}\mX\vw - 2\transpose{\vy}\mX\vw + \transpose{\vy}\vy}} + \const \nonumber \\
  &= -\frac{1}{2} \brackets*{\transpose{\vw}(\sigman^{-2}\transpose{\mX}\mX + \sigmap^{-2}\mI)\vw - 2\sigman^{-2}\transpose{\vy}\mX\vw} + \const.
\end{align}
Observe that the log-posterior is a quadratic form in $\vw$, so the posterior distribution must be Gaussian: \begin{subequations}\begin{align}
  \vw &\mid \vx_{1:n}, y_{1:n} \sim \N{\vmu}{\mSigma} \margintag{see \cref{eq:gaussian_propto}}
  \intertext{where we can read off the mean and variance to be}
  \vmu &\defeq \sigman^{-2} \mSigma \transpose{\mX} \vy, \\
  \mSigma &\defeq \inv{\parentheses*{\sigman^{-2} \transpose{\mX} \mX + \sigmap^{-2} \mI}}. \label{eq:blr_posterior_var}
\end{align}\label{eq:blr_posterior}\end{subequations}
This also shows that Gaussians with known variance and linear likelihood are self-conjugate, a property that we had hinted at in \cref{sec:fundamentals:bayesian:conjugacy}.
It can be shown more generally that Gaussians with known variance are self-conjugate to any Gaussian likelihood \citep{murphy2007conjugate}.
For other generative models, the posterior can typically not be expressed in closed-form --- this is a very special property of Gaussians!



\subsection{Maximum a Posteriori Estimation}

Computing the MAP estimate for the weights, \begin{align}
  \vwhat_\MAP &= \argmax_\vw \log p(y_{1:n} \mid \vx_{1:n}, \vw) + \log p(\vw) \nonumber \\
  &= \argmin_\vw \norm{\vy - \mX \vw}_2^2 + \frac{\sigman^2}{\sigmap^2} \norm{\vw}_2^2, \margintag{using that the likelihood and prior are Gaussian} \label{eq:blr_as_ridge}
\end{align}
we observe that this is identical to ridge regression with weight decay ${\lambda \defeq \sigman^2 / \sigmap^2}$: $\vwhat_\MAP = \vwhat_{\ridge}$.
\Cref{eq:blr_as_ridge} is simply the MLE loss with an additional $L_2$-regularization (originating from the prior) that encourages keeping weights small.
Recall that the MAP estimate corresponds to the mode of the posterior distribution, which in the case of a Gaussian is simply its mean $\vmu$.
As to be expected, $\vmu$ coincides with the analytical solution to ridge regression from \cref{eq:ridge_regression}.

\begin{marginfigure}[2\baselineskip]
  \incplt{regularization}
  \caption{Level sets of $L_2$- (\b{blue}) and $L_1$-regularization (\r{red}), corresponding to Gaussian and Laplace priors, respectively. It can be seen that $L_1$-regularization is more effective in encouraging sparse solutions (that is, solutions where many components are set to exactly $0$).}
\end{marginfigure}

\begin{ex}{Lasso as the MAP estimate with a Laplace prior}{}
  One problem with ridge regression is that the contribution of nearly-zero weights to the $L_2$-regularization term is negligible.
  Thus, $L_2$-regularization is typically not sufficient to perform variable selection (that is, set some weights to zero entirely), which is often desirable for interpretability of the model.

  A commonly used alternative to ridge regression is the \emph{least absolute shrinkage and selection operator} (or \midx{lasso}), which regularizes with the $L_1$-norm: \begin{align}
    \vwhat_{\lasso} \defeq \argmin_{\vw \in \R^d} \norm{\vy - \mX\vw}_2^2 + \lambda \norm{\vw}_1.
  \end{align}
  It turns out that lasso can also be viewed as probabilistic inference, using a Laplace prior ${\vw \sim \Laplace{\vzero}{h}}$ with length scale $h$ instead of a Gaussian prior.

  Computing the MAP estimate for the weights yields, \begin{align}
    \vwhat_\MAP &= \argmax_\vw \log p(y_{1:n} \mid \vx_{1:n}, \vw) + \log p(\vw) \nonumber \\
    &= \argmin_\vw \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2 + \frac{\sigman^2}{h} \norm{\vw}_1 \margintag{using that the likelihood is Gaussian and the prior is Laplacian}
  \end{align} which coincides with the lasso with weight decay ${\lambda \defeq \sigman^2 / h}$.
\end{ex}

To make predictions at a test point $\vxs$, we define the {(model-)}predicted point $\fs \defeq \transpose{\vwhat_{\MAP}} \vxs$ and obtain the label prediction \begin{align}
  \ys \mid \vxs, \vx_{1:n}, y_{1:n} \sim \N{\fs}{\sigman^2}.
\end{align}
Here we observe that using point estimates such as the MAP estimate does not quantify uncertainty in the weights.
The MAP estimate simply collapses all mass of the posterior around its mode.
This can be harmful when we are highly unsure about the best model, e.g., because we have observed insufficient data.

\subsection{Probabilistic Inference}

Rather than selecting a single weight vector $\vwhat$ to make predictions, we can use the full posterior distribution.
This is known as \midx{Bayesian linear regression} (BLR) and illustrated with an example in \cref{fig:blr_comparison}.

\begin{figure}
  \incplt{blr}
  \caption{Comparison of \b{\textbf{linear regression (MLE)}}, \r{\textbf{ridge regression (MAP estimate)}}, and \textbf{Bayesian linear regression} when the data is generated according to \begin{align*}
    y \mid \vw, \vx \sim \N{\transpose{\vw}\vx}{\sigman^2}.
  \end{align*} The \textbf{true mean} is shown in black, the MLE in blue, and the MAP estimate in red. The dark gray area denotes the epistemic uncertainty of Bayesian linear regression and the light gray area the additional homoscedastic noise.
  On the left, $\sigman = 0.15$. On the right, $\sigman = 0.7$.}
  \label{fig:blr_comparison}
\end{figure}

To make predictions at a test point $\vxs$, we let $\fs \defeq \transpose{\vw} \vxs$ which has the distribution \begin{align}
  \fs \mid \vxs, \vx_{1:n}, y_{1:n} \sim \N{\transpose{\vmu} \vxs}{\transpose{\vxs} \mSigma \vxs}. \margintag{using the closedness of Gaussians under linear transformations \eqref{eq:gaussian_lin_trans}}
\end{align}
Note that this does not take into account the noise in the labels $\sigman^2$.
For the label prediction $\ys$, we obtain \begin{align}
  \ys \mid \vxs, \vx_{1:n}, y_{1:n} \sim \N{\transpose{\vmu} \vxs}{\transpose{\vxs} \mSigma \vxs + \sigman^2}. \label{eq:blr_pred_posterior} \margintag{using additivity of Gaussians \eqref{eq:gaussian_additivity}}
\end{align}

\subsection{Recursive Probabilistic Inference}\label{sec:blr:online}

We have already discussed the recursive properties of probabilistic inference in \cref{sec:fundamentals:recursive_inference}.
For Bayesian linear regression with a Gaussian prior and likelihood, this principle can be used to derive an efficient online algorithm since also the posterior is a Gaussian, \begin{align}
  p^{(t)}(\vw) = \N[\vw]{\vmu^{(t)}}{\mSigma^{(t)}},
\end{align} which can be stored efficiently using only $\BigO{d^2}$ parameters.
This leads to an efficient online algorithm for Bayesian linear regression with time-independent(!) memory complexity $\BigO{d}$ and round complexity $\BigO{d^2}$ \exerciserefmark{online_blr}.
The interpretation of Bayesian linear regression as an online algorithm also highlights similarities to other sequential models such as Kalman filters, which we discuss in \cref{sec:kf}.
In \cref{ex:blr_as_kf}, we will learn that online Bayesian linear regression is, in fact, an example of a Kalman filter.

\section{Aleatoric and Epistemic Uncertainty}\label{sec:blr:uncertainty}

The predictive posterior distribution from \cref{eq:blr_pred_posterior} highlights a decomposition of uncertainty wherein $\transpose{\vxs} \mSigma \vxs$ corresponds to the uncertainty about our model due to the lack of data (commonly referred to as the \midx{epistemic uncertainty}) and $\sigman^2$ corresponds to the uncertainty about the labels that cannot be explained by the inputs and any model from the model class (commonly referred to as the \midx{aleatoric uncertainty}, ``irreducible noise'', or simply ``(label) noise'') \exerciserefmark{aleatoric_and_epistemic_uncertainty}.

A natural probabilistic approach is to represent epistemic uncertainty with a probability distribution over models.
Intuitively, the variance of this distribution measures our uncertainty about the model and its mode corresponds to our current best (point) estimate.
The distribution over weights of a linear model is one example, and we will continue to explore this approach for other models in the following chapters.\looseness=-1

It is a practical modeling choice how much inaccuracy to attribute to epistemic or aleatoric uncertainty.
Generally, when a poor model is used to explain a process, more inaccuracy has to be attributed to irreducible noise.
For example, when a linear model is used to ``explain'' a nonlinear process, most uncertainty is aleatoric as the model cannot explain the data well.
As we use more expressive models, a larger portion of the uncertainty can be explained by the data.

Epistemic and aleatoric uncertainty can be formally defined in terms of the law of total variance \eqref{eq:lotv}, \begin{align}
  \Var{\ys \mid \vxs} = \b{\underbrace{\E[\vtheta]{\Var[\ys]{\ys \mid \vxs, \vtheta}}}_{\text{aleatoric uncertainty}}} + \r{\underbrace{\Var[\vtheta]{\E[\ys]{\ys \mid \vxs, \vtheta}}}_{\text{epistemic uncertainty}}}. \label{eq:lotv_interpretation}
\end{align}
Here, the mean variability of predictions $\ys$ averaged across all models $\vtheta$ is the estimated \midx{aleatoric uncertainty}.
In contrast, the variability of the mean prediction $\ys$ under each model $\vtheta$ is the estimated \midx{epistemic uncertainty}.
This decomposition of uncertainty will appear frequently throughout this manuscript.\looseness=-1

\section{Non-linear Regression}\label{sec:blr:non_linear}

We can use linear regression not only to learn linear functions.
The trick is to apply a nonlinear transformation $\vphi : \R^d \to \R^e$ to the features $\vx_i$, where $d$ is the dimension of the input space and $e$ is the dimension of the designed \midx{feature space}.
We denote the design matrix comprised of transformed features by $\mPhi \in \R^{n \times e}$.
Note that if the feature transformation $\vphi$ is the identity function then $\mPhi = \mX$.

\begin{marginfigure}
  \incplt{poly_linear_regression}
  \caption{Applying linear regression with a feature space of polynomials of degree 10. The \b{\textbf{least squares estimate}} is shown in blue, \r{\textbf{ridge regression}} in red, and \g{\textbf{lasso}} in green.}
\end{marginfigure}

\begin{ex}{Polynomial regression}{polynomial_regression}
  Let ${\vphi(x) \defeq [x^2, x, 1]}$ and ${\vw \defeq [a, b, c]}$.
  Then the function that our model learns is given as \begin{align*}
    f = a x^2 + b x + c.
  \end{align*}
  Thus, our model can exactly represent all polynomials up to degree 2.

  However, to learn polynomials of degree $m$ in $d$ input dimensions, we need to apply the nonlinear transformation \begin{align*}
    \vphi(\vx) = [&1, x_1, \dots, x_d, x_1^2, \dots, x_d^2, x_1 \cdot x_2, \dots, x_{d-1} \cdot x_d, \\
    &\dots, \\
    &x_{d-m+1} \cdot \dots \cdot x_d].
  \end{align*}

  Note that the feature dimension $e$ is $\sum_{i=0}^m {d+i-1 \choose i} = \Theta(d^m)$.\safefootnote{Observe that the vector contains ${d+i-1 \choose i}$ monomials of degree $i$ as this is the number of ways to choose $i$ times from $d$ items with replacement and without consideration of order. To see this, consider the following encoding: We take a sequence of $d+i-1$ spots. Selecting any subset of $i$ spots, we interpret the remaining $d-1$ spots as ``barriers'' separating each of the $d$ items. The selected spots correspond to the number of times each item has been selected. For example, if $2$ items are to be selected out of a total of $4$ items with replacement, one possible configuration is ``$\circ\mid\mid\circ\mid$'' where $\circ$ denotes a selected spot and $\mid$ denotes a barrier. This configuration encodes that the first and third item have each been chosen once. The number of possible configurations --- each encoding a unique outcome --- is therefore ${d+i-1 \choose i}$.}
  Thus, the dimension of the feature space grows exponentially in the degree of polynomials and input dimensions.
  Even for relatively small $m$ and $d$, this becomes completely unmanageable.
\end{ex}

The example of polynomials highlights that it may be inefficient to keep track of the weights $\vw \in \R^e$ when $e$ is large, and that it may be useful to instead consider a reparameterization which is of dimension~$n$ rather than of the feature dimension.

\section{Function-space View}\label{sec:blr:function_space_view}

Let us now look at Bayesian linear regression through a different lens.
Previously, we have been interpreting it as a distribution over the weights $\vw$ of a linear function $\vf = \mPhi \vw$.
The key idea is that for a finite set of inputs (ensuring that the design matrix is well-defined), we can equivalently consider a distribution directly over the estimated function values $\vf$.
We call this the \midx{function-space view} of Bayesian linear regression.\looseness=-1

Instead of considering a prior over the weights $\vw \sim \N{\vzero}{\sigmap^2 \mI}$ as we have done previously, we now impose a prior directly on the values of our model at the observations.
Using that Gaussians are closed under linear maps \eqref{eq:gaussian_lin_trans}, we obtain the equivalent prior \begin{align}
  \vf \mid \mX \sim \N{\mPhi \E{\vw}}{\mPhi \Var{\vw} \transpose{\mPhi}} = \N{\vzero} {\underbrace{\sigmap^2 \mPhi \transpose{\mPhi}}_{\mK}} \label{eq:kernel_matrix_defn}
\end{align} where $\mK \in \R^{n \times n}$ is the so-called \midx{kernel matrix}.
Observe that the entries of the kernel matrix can be expressed as $\mK(i,j) = \sigmap^2 \cdot \transpose{\vphi(\vx_i)} \vphi(\vx_j)$.

\begin{marginfigure}
  \incplt{function_space_view}
  \caption{An illustration of the function-space view. The model is described by the points $(x_i, f_i)$.}
\end{marginfigure}

You may say that nothing has changed, and you would be right --- that is precisely the point.
Note, however, that the shape of the kernel matrix is $n \times n$ rather than the $e \times e$ covariance matrix over weights, which becomes unmanageable when $e$ is large.
The kernel matrix $\mK$ has entries only for the finite set of observed inputs.
However, in principle, we could have observed any input, and this motivates the definition of the \midx{kernel function} \begin{align}
  k(\vx, \vxp) \defeq \sigmap^2 \cdot \transpose{\vphi(\vx)} \vphi(\vxp) \label{eq:blr_kernel}
\end{align} for arbitrary inputs $\vx$ and $\vxp$.
A kernel matrix is simply a finite ``view'' of the kernel function, \begin{align}
  \mK = \begin{bmatrix}
    k(\vx_1, \vx_1) & \cdots & k(\vx_1, \vx_n) \\
    \vdots & \ddots & \vdots \\
    k(\vx_n, \vx_1) & \cdots & k(\vx_n, \vx_n) \\
  \end{bmatrix}
\end{align}

Observe that by definition of the kernel matrix in \cref{eq:kernel_matrix_defn}, the kernel matrix is a covariance matrix and the kernel function measures the covariance of the function values $f(\vx)$ and $f(\vxp)$ given inputs~$\vx$ and~$\vxp$: \begin{align}
  k(\vx, \vxp) = \Cov{f(\vx), f(\vxp)}.
\end{align}

Moreover, note that we have reformulated\footnote{we often say ``kernelized''\pidx{kernelization}} the learning algorithm such that the feature space is now \emph{implicit} in the choice of kernel, and the kernel is defined by inner products of (nonlinearly transformed) inputs.
In other words, the choice of kernel implicitly determines the class of functions that $\vf$ is sampled from (without expressing the functions explicitly in closed-form), which encodes our prior beliefs.
This is known as the \midx{kernel trick}.

\subsection{Learning and Predictions}\label{sec:blr:learning_and_inference}

We have already kernelized the Bayesian linear regression prior.
The posterior distribution $\vf \mid \mX, \vy$ is again Gaussian due to the closedness properties of Gaussians, analogously to our derivation of the prior kernel matrix in \cref{eq:kernel_matrix_defn}.

It remains to show that we can also rely on the kernel trick for predictions.
Given the test point $\vxs$, we define \begin{align*}
  \Tilde{\mPhi} \defeq \begin{bmatrix}
    \mPhi \\
    \transpose{\vphi(\vxs)} \\
  \end{bmatrix}, \quad \Tilde{\vy} \defeq \begin{bmatrix}
    \vy \\
    \ys \\
  \end{bmatrix}, \quad \Tilde{\vf} \defeq \begin{bmatrix}
    \vf \\
    \fs \\
  \end{bmatrix}.
\end{align*}
We immediately obtain $\Tilde{\vf} = \Tilde{\mPhi} \vw$.
Analogously to our analysis of predictions from the weight-space view, we add the label noise to obtain the estimate $\Tilde{\vy} = \Tilde{\vf} + \Tilde{\vvarepsilon}$ where $\Tilde{\vvarepsilon} \defeq \transpose{[\varepsilon_1 \; \cdots \; \varepsilon_n \; \opt{\varepsilon}]} \sim \N{\vzero}{\sigman^2 \mI}$ is the independent label noise.
Applying the same reasoning as we did for the prior, we obtain \begin{align}
  \Tilde{\vf} \mid \mX, \vxs \sim \N{\vzero}{\Tilde{\mK}}
\end{align} where $\Tilde{\mK} \defeq \sigmap^2 \Tilde{\mPhi} \transpose{\Tilde{\mPhi}}$.
Adding the label noise yields \begin{align}
  \Tilde{\vy} \mid \mX, \vxs \sim \N{\vzero}{\Tilde{\mK} + \sigman^2 \mI}.
\end{align}
Finally, we can conclude from the closedness of Gaussian random vectors under conditional distributions \eqref{eq:cond_gaussian} that the predictive posterior ${\ys \mid \vxs, \mX, \vy}$ follows again a normal distribution.
We will do a full derivation of the posterior and predictive posterior in \cref{sec:gp:learning_and_inference}.

\subsection{Efficient Polynomial Regression}

But how does the kernel trick address our concerns about efficiency raised in \cref{sec:blr:non_linear}?
After all, computing the kernel for a feature space of dimension $e$ still requires computing sums of length $e$ which is prohibitive when $e$ is large.
The kernel trick opens up a couple of new doors for us: \begin{enumerate}
  \item For certain feature transformations $\vphi$, we may be able to find an easier to compute expression equivalent to $\transpose{\vphi(\vx)} \vphi(\vxp)$.
  \item If this is not possible, we could approximate the inner product by an easier to compute expression.
  \item Or, alternatively, we may decide not to care very much about the exact feature transformation and simply experiment with kernels that induce \emph{some} feature space (which may even be infinitely dimensional).
\end{enumerate}
We will explore the third approach when we revisit kernels in \cref{sec:gp:kernel_functions}.
A polynomial feature transformation can be computed efficiently in closed-form.

\begin{fct}
  For the polynomial feature transformation $\vphi$ up to degree $m$ from \cref{ex:polynomial_regression}, it can be shown that up to constant factors, \begin{align}
    \transpose{\vphi(\vx)} \vphi(\vxp) = (1 + \transpose{\vx} \vxp)^m.
  \end{align}
\end{fct}
For example, for input dimension $2$, the kernel $(1 + \transpose{\vx}\vxp)^2$ corresponds to the feature vector $\vphi(\vx) = \transpose{[1 \;\; \sqrt{2} x_1 \;\; \sqrt{2} x_2 \;\; \sqrt{2} x_1 x_2 \;\; x_1^2 \;\; x_2^2]}$.

\section*{Discussion}

We have explored a probabilistic perspective on linear models, and seen that classical approaches such as least squares and ridge regression can be interpreted as approximate probabilistic inference.
We then saw that we can even perform \emph{exact} probabilistic inference efficiently if we adopt a Gaussian prior and Gaussian noise assumption.
These are already powerful tools, which are often applied also to nonlinear models if we treat the latent feature space --- which was either human-designed or learned via deep learning --- as fixed.
In the next chapter, we will digress briefly from the storyline on ``learning'' to see how we can adopt a similar probabilistic perspective to track latent states over time.
Then, in \cref{sec:gp}, we will see how we can use the function-space view and kernel trick to learn flexible nonlinear models with exact probabilistic inference, without ever explicitly representing the feature space.

\excheading

\begin{nexercise}{Closed-form linear regression}{closed_form_linear_regression}
  \begin{enumerate}
    \item Derive the unique solutions to least squares and ridge regression from \cref{eq:linear_regression,eq:ridge_regression}.

    \item For an $n \times m$ matrix $\mA$ and vector $\vx \in \R^m$, we call $\mPi_{\mA} \vx$ the orthogonal projection of $\vx$ onto $\mathrm{span}\{\mA\} = \{\mA \vxp \mid \vxp \in \R^m\}$.
    In particular, an orthogonal projection satisfies $\vx - \mPi_{\mA} \vx \perp \mA \vxp$ for all $\vxp \in \R^m$.\par
    Show that $\vwhat_{\ls}$ from \cref{eq:linear_regression} is such that $\mX \vwhat_{\ls}$ is the unique closest point to $\vy$ on $\mathrm{span}\{\mX\}$, i.e., it satisfies $\mX \vwhat_{\ls} = \mPi_{\mX} \vy$.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{MLE of noise variance}{noise_variance_mle}
  Show that the MLE of $\sigman^2$ given fixed weights $\vw$ is \begin{align}
    \hat{\sigma}_n^2 = \frac{1}{n} \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2.
  \end{align}
\end{nexercise}

\begin{nexercise}{Variance of least squares around training data}{variance_around_training_data}
  Show that the variance of a prediction at the point $\transpose{[1 \; \xs]}$ is smallest when $\xs$ is the mean of the training data.
  More formally, show that if inputs are of the form $\vx_i = \transpose{[1 \; x_i]}$ where $x_i \in \R$ and $\vwhat_{\ls}$ is the least squares estimate, then $\Var{\ys \mid \transpose{[1 \; \xs]}, \vwhat_{\ls}}$ is minimized for $\xs = \frac{1}{n} \sum_{i=1}^n x_i$.
\end{nexercise}

\begin{nexercise}{Bayesian linear regression}{blr}
  Suppose you are given the following observations \begin{align*}
    \mX = \begin{bmatrix}
      1 & 1 \\
      1 & 2 \\
      2 & 1 \\
      2 & 2 \\
    \end{bmatrix}, \quad \vy = \begin{bmatrix}
      2.4 \\
      4.3 \\
      3.1 \\
      4.9 \\
    \end{bmatrix}
  \end{align*} and assume the data follows a linear model with homoscedastic noise $\N{0}{\sigman^2}$ where $\sigman^2 = 0.1$.

  \begin{enumerate}
    \item Find the maximum likelihood estimate $\vwhat_\MLE$ given the data.
    \item Now assume that we have a prior $p(\vw) = \N[\vw]{\vzero}{\sigmap^2 \mI}$ with ${\sigmap^2 = 0.05}$.
    Find the MAP estimate $\vwhat_\MAP$ given the data and the prior.
    \item Use the posterior $p(\vw \mid \mX, \vy)$ to get a posterior prediction for the label $\ys$ at $\vxs = \transpose{[3 \quad 3]}$.
    Report the mean and the variance of this prediction.
    \item How would you have to change the prior $p(\vw)$ such that \begin{align*}
      \vwhat_\MAP \to \vwhat_\MLE?
    \end{align*}
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Online Bayesian linear regression}{online_blr}
  \begin{enumerate}
    \item Can you design an algorithm that updates the posterior (as opposed to recalculating it from scratch using \cref{eq:blr_posterior}) in a smarter way?
    The requirement is that the memory should not grow as $\BigO{t}$.
    \item If $d$ is large, computing the inverse every round is very expensive.
    Can you use the recursive structure you found in the previous question to bring down the computational complexity of every round to $\BigO{d^2}$?\par
  \end{enumerate} The resulting efficient online algorithm is known as \midx{online Bayesian linear regression}.
\end{nexercise}

\begin{nexercise}{Aleatoric and epistemic uncertainty of BLR}{aleatoric_and_epistemic_uncertainty}
  Prove for Bayesian linear regression that $\transpose{\vxs}\mSigma\vxs$ is the epistemic uncertainty and $\sigman^2$ the aleatoric uncertainty in $\ys$ under the decomposition of \cref{eq:lotv_interpretation}.
\end{nexercise}

\begin{nexercise}{Hyperpriors}{hyperpriors}
  We consider a dataset \(\{(\vx_i,y_i)\}_{i=1}^n\) of size \(n\), where \(\vx_i \in \R^d\) denotes the feature vector and \(y_i \in \R\) denotes the label of the \(i\)-th data point.
  Let \(\varepsilon_i\) be i.i.d.~samples from the Gaussian distribution \(\N{0}{\lambda^{-1}}\) for a given \(\lambda > 0\).
  We collect the labels in a vector \(\vy \in \R^n\), the features in a matrix \(\mX \in \R^{n \times d}\), and the noise in a vector \(\vvarepsilon \in \R^n\).
  The labels are generated according to \(\vy = \mX \vw + \vvarepsilon.\)\par
  To perform Bayesian Linear Regression, we consider the prior distribution over the parameter vector \(\vw\) to be \( \N{\vmu}{\lambda^{-1} \mI_d}\), where \(\mI_d\) denotes the \(d\)-dimensional identity matrix and \(\vmu \in \R^d\) is a hyperparameter.

  \begin{enumerate}
    \item Given this Bayesian data model, what is the conditional covariance matrix \( \mSigma_{\vy} \defeq \Var{\vy}[\mX, \vmu, \lambda] \)?

    \item Calculate the maximum likelihood estimate of the hyperparameter~\( \vmu \).

    \item Since we are unsure about the hyperparameter \( \vmu \), we decide to model our uncertainty about \( \vmu \) by placing the ``\midx{hyperprior}'' \({\vmu \sim \N{\vzero}{\mI_d}}\). Is the posterior distribution \(p(\vmu \mid \mX, \vy, \lambda)\) a Gaussian distribution? If yes, what are its mean vector and covariance matrix?

    \item What is the posterior distribution \( p(\lambda \mid \mX, \vy, \vmu) \)?\par
    \textit{Hint: For any \( a \in \R, \mA \in \R^{n \times n} \) it holds that \( \det{a \mA} = a^n \det{\mA} \).}
  \end{enumerate}
\end{nexercise}
