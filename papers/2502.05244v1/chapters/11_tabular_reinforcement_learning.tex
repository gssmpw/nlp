\chapter{Tabular Reinforcement Learning}\label{sec:tabular_rl}

\section{The Reinforcement Learning Problem}\pidx{reinforcement learning}

Reinforcement learning is concerned with probabilistic planning in unknown environments.\pidx{planning}
This extends our study of known environments in the previous chapter.
Those environments are still modeled by Markov decision processes, but in reinforcement learning, we do not know the dynamics $p$ and rewards $r$ in advance.
Hence, reinforcement learning is at the intersection of the theories of probabilistic planning (i.e., Markov decision processes) and learning (e.g., multi-armed bandits), which we covered extensively in the previous chapters.

\begin{marginfigure}
  \incfig{reinforcement_learning}
  \caption{In reinforcement learning, an agent interacts with its environment in a sequence of rounds.
  After playing an action~$a_t$, it observes rewards~$r_t$ and its new state~$x_{t+1}$.
  The agent then uses this information to learn how to act to maximize reward.\looseness=-1}\label{fig:rl}
\end{marginfigure}

We will continue to focus on the fully observed setting, where the agent knows its current state.
As we have seen in the previous section, the partially observed setting corresponds to a fully observed setting with an enlarged state space.
In this chapter, we will begin by considering reinforcement learning with small state and action spaces.
This setting is often called the \midx{tabular setting}, as the value functions can be computed exhaustively for all states and stored in a table.

\pidx{exploration-exploitation dilemma}%

Clearly, the agent needs to trade exploring and learning about the environment with exploiting its knowledge to maximize rewards.
Thus, the exploration-exploitation dilemma, which was at the core of Bayesian optimization (see \cref{sec:bayesian_optimization:exploration_exploitation}), also plays a crucial role in reinforcement learning.
In fact, Bayesian optimization can be viewed as reinforcement learning with a fixed state:
In each round, the agent plays an action, aiming to find the action that maximizes the reward.
However, playing the same action multiple times yields the same reward, implying that we remain in a single state.
In the context of Bayesian optimization, we used ``regret'' as performance metric: in the jargon of planning, minimizing regret corresponds to maximizing the cumulative reward.

Another key challenge of reinforcement learning is that the observed data is dependent on the played actions.
This is in contrast to the setting of supervised learning that we have been considering in earlier chapters, where the data is sampled independently.

\subsection{Trajectories}\label{sec:tabular_rl:rl:trajectories}

The data that the agent collects is modeled using so-called trajectories.

\begin{defn}[Trajectory]\pidx{trajectory}
  A \emph{trajectory} $\tau$ is a (possibly infinite) sequence,\looseness=-1 \begin{align}
    \tau \defeq (\tau_0, \tau_1, \tau_2, \dots),
  \end{align} of \midx<transitions>{transition}, \begin{align}
    \tau_i \defeq (x_i, a_i, r_i, x_{i+1}),
  \end{align} where $x_i \in \sX$ is the starting state, $a_i \in \sA$ is the played action, $r_i \in \R$ is the attained reward, and $x_{i+1} \in \sX$ is the ending state.
\end{defn}

In the context of learning a dynamics and rewards model, $x_i$ and $a_i$ can be understood as inputs, and $r_i$ and $x_{i+1}$ can be understood as labels of a regression problem.

Crucially, the newly observed states $x_{t+1}$ and the rewards $r_t$ (across multiple transitions) are conditionally independent given the previous states $x_t$ and actions $a_t$.
This follows directly from the Markovian structure of the underlying Markov decision process.\footnote{Recall the Markov property \eqref{eq:markov_property}, which assumes that in the underlying Markov decision process (i.e., in our environment) the future state of an agent is independent of past states given the agent's current state. This is commonly called a Markovian structure. From this Markovian structure, we gather that repeated encounters of state-action pairs result in independent trials of the transition model and rewards.}
Formally, we have,\looseness=-1 \begin{subequations}\label{eq:rl_cond_indep}\begin{align}
  X_{t+1} &\perp X_{t'+1} \mid X_t, X_{t'}, A_t, A_{t'}, \\
  R_t &\perp R_{t'} \mid X_t, X_{t'}, A_t, A_{t'},
\end{align}\end{subequations} for any $t, t' \in \Nat_0$.
In particular, if $x_t = x_{t'}$ and $a_t = a_{t'}$, then $x_{t+1}$ and $x_{t'+1}$ are independent samples according to the transition model $p(X_{t+1} \mid x_t, a_t)$.
Analogously, if $x_t = x_{t'}$ and $a_t = a_{t'}$, then~$r_t$ and~$r_{t'}$ are independent samples of the reward model $r(x_t, a_t)$.
As we will see later in this chapter and especially in \cref{sec:mbarl}, this independence property is crucial for being able to learn about the underlying Markov decision process.
Notably, this implies that we can apply the law of large numbers \eqref{eq:slln} and Hoeffding's inequality \eqref{eq:hoeffdings_inequality} to our estimators of both quantities.

The collection of data is commonly classified into two settings.
In the \midx{episodic setting}, the agent performs a sequence of ``training'' rounds~(called \midx<episodes>{episode}).
In the beginning of each episode, the agent is reset to some initial state.
In contrast, in the \midx{continuous setting}~(or non-episodic / online setting), the agent learns online.
Especially, every action, every reward, and every state transition counts.

The episodic setting is more applicable to an agent playing a computer game.
That is, the agent is performing in a simulated environment that is easy to reset.
The continuous setting is akin to an agent that is deployed to the ``real world''.
In principle, real-world agents can be trained in simulated environments before being deployed.
However, this bears the risk of learning to exploit or rely on features of the simulated environment that are not present in the real environment. Sometimes, using a simulated environment for training is downright impossible, as the real environment is too complex.

\subsection{On-policy and Off-policy Methods}

Another important distinction in how data is collected, is the distinction between on-policy and off-policy methods.
As the names suggest, \midx{on-policy} methods are used when the agent has control over its own actions, in other words, the agent can freely choose to follow any policy.
Being able to follow a policy is helpful, for example because it allows the agent to experiment with trading exploration and exploitation.

In contrast, \midx{off-policy} methods can be used even when the agent cannot freely choose its actions.
Off-policy methods are therefore able to make use of purely observational data.
This might be data that was collected by another agent, a fixed policy, or during a previous episode.
Off-policy methods are therefore more \emph{sample-efficient} than on-policy methods.
This is crucial, especially in settings where conducting experiments (i.e., collecting new data) is expensive.

\section{Model-based Approaches}

Approaches to reinforcement learning are largely categorized into two classes.
\midx<Model-based>{model-based reinforcement learning}[idxpagebf] approaches aim to learn the underlying Markov decision process.
More concretely, they learn models of the dynamics~$p$ and rewards~$r$.
They then use these models to perform planning~(i.e., policy optimization) in the underlying Markov decision process.
In contrast, \midx<model-free>{model-free reinforcement learning} approaches learn the value function directly.
We begin by discussing model-based approaches to the tabular setting.
In \cref{sec:tabular_rl:model_free}, we cover model-free approaches.

\subsection{Learning the Underlying Markov Decision Process}\label{sec:rl:learning_mdp}

Recall that the underlying Markov decision process was specified by its dynamics $p(x' \mid x, a)$ that correspond to the probability of entering state $x' \in \sX$ when playing action $a \in \sA$ from state $x \in \sX$, and its rewards $r(x, a)$ for playing action $a \in \sA$ in state $x \in \sX$.
A natural first idea is to use maximum likelihood estimation to approximate these quantities.\looseness=-1

We can think of each transition $x' \mid x, a$ as sampling from a categorical random variable of which we want to estimate the success probabilities for landing in each of the states.
Therefore, as we have seen in \cref{ex:mle_bern}, the MLE of the dynamics model coincides with the sample mean,\looseness=-1 \begin{align}
\hat{p}(x' \mid x, a) = \frac{N(x' \mid x, a)}{N(a \mid x)}
\end{align} where $N(x' \mid x, a)$ counts the number of transitions from state $x$ to state $x'$ when playing action $a$ and $N(a \mid x)$ counts the number of transitions that start in state $x$ and play action $a$ (regardless of the next state).
Similarly, for the rewards model, we obtain the following maximum likelihood estimate (i.e., sample mean), \begin{align}
  \hat{r}(x, a) = \frac{1}{N(a \mid x)} \sum_{\substack{t = 0 \\ x_t = x \\ a_t = a}}^\infty r_t.
\end{align}
It is immediate that both estimates are unbiased as both correspond to a sample mean.

Still, for the models of our environment to become accurate, our agent needs to visit \emph{each} state-action pair $(x,a)$ numerous times.
Note that our estimators for dynamics and rewards are only well-defined when we visit the corresponding state-action pair at least once.
However, in a stochastic environment, a single visit will likely not result in an accurate model.
We can use Hoeffding's inequality \eqref{eq:hoeffdings_inequality} to gauge how accurate the estimates are after only a limited number of visits.

\section{Balancing Exploration and Exploitation}\label{sec:tabular_rl:exploration_exploitation}

The next natural question is how to use our current model of the environment to pick actions such that exploration and exploitation are traded effectively.
This is what we will consider next.

Given the estimated MDP given by $\hat{p}$ and $\hat{r}$, we can compute the optimal policy using either policy iteration or value iteration.
For example, using value iteration, we can compute the optimal state-action value function $\opt{\fnQ}$ within the \emph{estimated} MDP, and then employ the greedy policy\looseness=-1 \begin{align}
  \pi(x) = \argmax_{a \in \sA} \Q*{x}{a}.
\end{align}
Recall from \cref{eq:bop1} that this corresponds to always picking the best action under the \emph{current} model (that is, $\pi$ is the optimal policy).
But since the model is inaccurate, while potentially quickly generating some reward, we will likely get stuck in a suboptimal state.

\subsection{$\varepsilon$-greedy}\label{sec:tabular_rl:mb:epsilon_greedy}

Consider the other extreme:
If we always pick a random action, we will eventually(!) estimate the dynamics and rewards correctly, yet we will do extremely poorly in terms of maximizing rewards along the way.
To trade exploration and exploitation, a natural idea is to balance these two extremes.

Arguably, the simplest idea is the following: At each time step, throw a biased coin.
If this coin lands heads, we pick an action uniformly at random among all actions.
If the coin lands tails, we pick the best action under our current model.
This algorithm is called \emph{$\varepsilon$-greedy}, where the probability of a coin landing heads at time $t$ is $\varepsilon_t$.

\begin{algorithm}
  \caption{$\varepsilon$-greedy}\pidx{$\varepsilon$-greedy}[idxpagebf]
  \For{$t = 0$ \KwTo $\infty$}{
    sample $u \in \Unif{[0,1]}$\;
    \lIf{$u \leq \varepsilon_t$}{pick action uniformly at random among all actions}
    \lElse{pick best action under the current model}
  }
\end{algorithm}

The $\varepsilon$-greedy algorithm provides a general framework for addressing the exploration-exploitation dilemma.
When the underlying MDP is learned using Monte Carlo estimation as we discussed in \cref{sec:rl:learning_mdp}, the resulting algorithm is known as \midx{Monte Carlo control}.
However, the same framework can also be used in the model-free setting where we pick the best action without estimating the full underlying MDP.
We discuss this approach in greater detail in \cref{sec:tabular_rl:model_free}.

Amazingly, this simple algorithm already works quite well.
Nevertheless, it can clearly be improved.
The key problem of $\varepsilon$-greedy is that it explores the state space in an uninformed manner.
In other words, it explores ignoring all past experience.
It thus does not eliminate clearly suboptimal actions.
This is a problem, especially as we typically have many state-action pairs and recalling that we have to explore each such pair many times to learn an accurate model.

\begin{rmk}{Asymptotic convergence}{glie}
  It can be shown that Monte Carlo control converges to an optimal policy (albeit slowly) almost surely when the learned policy is ``greedy in the limit with infinite exploration''.

  \begin{defn}[Greedy in the limit with infinite exploration, GLIE]
    A sequence of policies $\pi_t$ is said to be \midx{greedy in the limit with infinite exploration} if \begin{enumerate}
      \item all state-action pairs are explored infinitely many times,\safefootnote{That all state-action pairs are chosen is a fundamental requirement. There is no reason why any algorithm would converge to the true value function for \emph{all} states when it only sees some state-action pairs finitely many times, or even not at all.} \begin{align}
        \lim_{t \to \infty} N_t(x, a) = \infty \quad\text{and}
      \end{align}

      \item the policy converges to a greedy policy, \begin{align}
        \lim_{t \to \infty} \pi_t(a \mid x) = \Ind{a = \argmax_{a' \in \sA} \opt{\fnQ}_t(x, a')}
      \end{align}
    \end{enumerate} where we denote by $N_t(x, a)$ the number of transitions from state $x$ playing action $a$ until time $t$, and $\opt{\fnQ}_t$ is the optimal state-action value function in the estimated MDP at time $t$.
  \end{defn}

  Note that $\varepsilon$-greedy is GLIE with probability $1$ if the sequence $(\varepsilon_t)_{t\in\Nat_0}$ satisfies the Robbins-Monro~(RM) conditions~\eqref{eq:rm_conditions}, \begin{align*}
    \varepsilon_t \geq 0 \quad \forall t, \quad \sum_{t=0}^\infty \varepsilon_t = \infty \quad\text{and}\quad \sum_{t=0}^\infty \varepsilon_t^2 < \infty.
  \end{align*}
  The RM-conditions are satisfied, for example, if $\varepsilon_t = \nicefrac{1}{t}$.

  \begin{thm}[Convergence of Monte Carlo control]
    GLIE Monte Carlo control converges to an optimal policy with probability $1$.
  \end{thm}

  Intuitively, the probability of exploration converges to zero, and hence, the policy will ``eventually coincide'' with the greedy policy.
  Moreover, the greedy policy will ``eventually coincide'' with the optimal policy due to an argument akin to the convergence of policy iteration,\safefootnote{see \cref{lem:policy_iteration_mon_impr,thm:policy_iteration_convergence}} and using that each state-action pair is visited infinitely often.
\end{rmk}

\subsection{Softmax Exploration}

An alternative to using $\varepsilon$-greedy for trading between greedy exploitation and uniform exploration is the so-called \midx{softmax exploration} or \midx{Boltzmann exploration}.
Given the agent is in state $x$, we pick action~$a$ with probability, \begin{align}
  \pi_\lambda(a \mid x) \propto \exp\parentheses*{\frac{1}{\lambda} \Q*{x}{a}}, \label{eq:softmax_exploration}
\end{align} which is the Gibbs distribution with temperature parameter $\lambda > 0$.
Observe that for $\lambda \to 0$, softmax exploration corresponds to greedily maximizing the Q-function (i.e., greedy exploitation), whereas for~${\lambda\to\infty}$, softmax exploration explores uniformly at random.
This can outperform $\varepsilon$-greedy as the exploration is directed towards actions with larger estimated value.

\subsection{Optimism}

Recall from our discussion of multi-armed bandits in \cref{sec:bayesian_optimization:online_learning:mab} that a key principle in effectively trading exploration and exploitation is \midx{optimism in the face of uncertainty}.
Let us apply this principle to the reinforcement learning setting.
The key idea is to assume that the dynamics and rewards model ``work in our favor'' until we have learned ``good estimates'' of the true dynamics and rewards.

\begin{marginfigure}
  \incfig{rmax}
  \caption{Illustration of the fairy-tale state of $R_\mathrm{max}$.
  If in doubt, the agent believes actions from the state $x$ to lead to the fairy-tale state $\xs$ with maximal rewards.
  This encourages the exploration of unknown states.}
\end{marginfigure}

More formally, if $r(x, a)$ is unknown, we set $\hat{r}(x, a) = R_\mathrm{max}$, where $R_\mathrm{max}$ is the maximum reward our agent can attain during a single transition.
Similarly, if $p(x' \mid x, a)$ is unknown, we set $\hat{p}(\xs \mid x, a) = 1$, where $\xs$ is a ``fairy-tale state''.
The fairy-tale state corresponds to everything our agent could wish for, that is, \begin{align}
  \hat{p}(\xs \mid \xs, a) &= 1 \quad &&\forall a \in \sA, \\
  \hat{r}(\xs, a)          &= R_\mathrm{max} \quad &&\forall a \in \sA.
\end{align}
In practice, the decision of when to assume that the learned dynamics and reward models are ``good enough'' has to be tuned.

In using these optimistic estimates of $p$ and $r$, we obtain an optimistic underlying Markov decision process that exhibits a bias towards exploration.
In particular, the rewards attained in this MDP, are an upper bound of the true reward.
The resulting algorithm is known as the \emph{$R_\mathrm{max}$ algorithm}.

\begin{algorithm}
  \caption{$R_\mathrm{max}$ algorithm}\pidx{$R_\mathrm{max}$ algorithm}
  add the fairy-tale state $\xs$ to the Markov decision process\;
  set $\hat{r}(x, a) = R_\mathrm{max}$ for all $x \in \sX$ and $a \in \sA$\;
  set $\hat{p}(\xs \mid x, a) = 1$ for all $x \in \sX$ and $a \in \sA$\;
  compute the optimal policy $\hat{\pi}$ for $\hat{r}$ and $\hat{p}$\;
  \For{$t = 0$ \KwTo $\infty$}{
    execute policy $\hat{\pi}$ (for some number of steps)\;
    for each visited state-action pair $(x, a)$, update $\hat{r}(x, a)$\;
    estimate transition probabilities $\hat{p}(x' \mid x, a)$\;
    after observing ``enough'' transitions and rewards, recompute the optimal policy $\hat{\pi}$ according the current model $\hat{p}$ and $\hat{r}$.
  }
\end{algorithm}

How many transitions are ``enough''?
We can use Hoeffding's inequality \eqref{eq:hoeffdings_inequality} to get a rough idea!
The key here, is our observation from \cref{eq:rl_cond_indep} that the transitions and rewards are conditionally independent given the state-action pairs since, as we have discussed in \cref{sec:approximate_inference:mcmc:ergodic_theorem} on the ergodic theorem, Hoeffding's inequality does not hold for dependent samples.
In this case, Hoeffding's inequality tells us that for the absolute approximation error to be below $\epsilon$ with probability at least $1-\delta$, we need \begin{align}
  N(a \mid x) \geq \frac{R_\mathrm{max}^2}{2 \epsilon^2} \log \frac{2}{\delta}. \margintag{see \eqref{eq:eq:hoeffdings_inequality_sample_size}}
\end{align}

\begin{lem}[Exploration and exploitation of $R_\mathrm{max}$]
  Every $T$ time steps, with high probability, $R_\mathrm{max}$ either \begin{itemize}
    \item obtains near-optimal reward; or
    \item visits at least one unknown state-action pair.\footnote{Note that in the tabular setting, there are ``only'' polynomially many state-action pairs.}
  \end{itemize}
  Here, $T$ depends on the mixing time of the Markov chain induced by the optimal policy.
\end{lem}

\begin{thm}[Convergence of $R_\mathrm{max}$, \cite{brafman2002r}]
  With probability at least $1-\delta$, $R_\mathrm{max}$ reaches an $\epsilon$-optimal policy in a number of steps that is polynomial in $\card{\sX}$, $\card{\sA}$, $T$, $\nicefrac{1}{\epsilon}$, $\nicefrac{1}{\delta}$, and $R_\mathrm{max}$.
\end{thm}

\subsection{Challenges of Model-based Approaches}

We have seen that the $R_\mathrm{max}$ algorithm performs remarkably well in the tabular setting.
However, there are important computational limitations to the model-based approaches that we discussed so far.

First, observe that the (tabular) model-based approach requires us to store $\hat{p}(x' \mid x, a)$ and $\hat{r}(x, a)$ in a table.
This table already has $\BigO{n^2 m}$ entries.
Even though polynomial in the size of the state and action spaces, this quickly becomes unmanageable.

Second, the model-based approach requires us to ``solve'' the learned Markov decision processes to obtain the optimal policy (using policy or value iteration).
As we continue to learn over time, we need to find the optimal policy many times.
$R_\mathrm{max}$ recomputes the policy after each state-action pair is observed sufficiently often, so $\BigO{n m}$ times.

\section{Model-free Approaches}\label{sec:tabular_rl:model_free}

In the previous section, we have seen that learning and remembering the model as well as planning within the estimated model can potentially be quite expensive in the model-based approach.
We therefore turn to model-free methods that estimate the value function directly.
Thus, they require neither remembering the full model nor planning~(i.e., policy optimization) in the underlying Markov decision process.
We will, however, return to model-based methods in \cref{sec:mbarl} to see that promise lies in combining methods from model-based reinforcement learning with methods from model-free reinforcement learning.

A significant benefit to model-based reinforcement learning is that it is inherently off-policy.
That is, any trajectory regardless of the policy used to obtain it can be used to improve the model of the underlying Markov decision process.
In the model-free setting, this not necessarily true.
By default, estimating the value function according to the data from a trajectory, will yield an estimate of the value function corresponding to the policy that was used to sample the data.

We will start by discussing on-policy methods and later see how the value function can be estimated off-policy.

\subsection{On-policy Value Estimation}\label{sec:tabular_rl:model_free:on_policy_value_estimation}

Let us suppose, our agent follows a fixed policy $\pi$.
Then, the corresponding value function $\fnv[\pi]$ is given as \begin{align}
  \v[\pi]{x} &= r(x, \pi(x)) + \gamma \sum_{x' \in \sX} p(x' \mid x, \pi(x)) \cdot \v[\pi]{x'} \margintag{using the definition of the value function \eqref{eq:value_function}} \nonumber \\
  &= \E[R_0, X_1]{R_0 + \gamma \v[\pi]{X_1} \mid X_0 = x, A_0 = \pi(x)} \label{eq:td_learning_mc} \margintag{interpreting the above expression as an expectation over the random quantities $R_0$ and $X_1$}
  \intertext{Our first instinct might be to use a Monte Carlo estimate of this expectation. Due to the conditional independence of the transitions \eqref{eq:rl_cond_indep}, Monte Carlo approximation does yield an unbiased estimate,}
  &\approx r + \gamma \v[\pi]{x'}, \label{eq:bootstrapping_estimate}
\end{align} where the agent observed the transition $(x, a, r, x')$.
Note that to estimate this expectation we use a single(!) sample,\footnote{The idea is that we will use this approximation repeatedly as our agent collects new data to achieve the same effect as initially averaging over multiple samples.} unlike our previous applications of Monte Carlo sampling where we usually averaged over $m$ samples.
However, there is one significant problem in this approximation.
Our approximation of $\fnv[\pi]$ does in turn depend on the (unknown) true value of $\fnv[\pi]$!

The key idea is to use a bootstrapping estimate of the value function instead.
That is, in place of the true value function $\fnv[\pi]$, we will use a ``running estimate'' $\fnV[\pi]$.
In other words, whenever observing a new transition, we use our previous best estimate of $\fnv[\pi]$ to obtain a new estimate $\fnV[\pi]$. We already encountered bootstrapping briefly in \cref{sec:bdl:approximate_inference:probabilistic_ensembles} in the context of probabilistic ensembles in Bayesian deep learning.
More generally, \midx{bootstrapping}[idxpagebf] refers to approximating a true quantity (e.g., $\fnv[\pi]$) by using an empirical quantity (e.g., $\fnV[\pi]$), which itself is constructed using samples from the true quantity that is to be approximated.\looseness=-1

Due to its use in estimating the value function, bootstrapping is a core concept to model-free reinforcement learning.
Crucially, using a bootstrapping estimate generally results in biased estimates of the value function. Moreover, due to relying on a single sample, the estimates from \cref{eq:bootstrapping_estimate} tend to have very large variance.

The variance of the estimate is typically reduced by mixing new estimates of the value function with previous estimates using a learning rate $\alpha_t$.
This yields the \emph{temporal-difference learning} algorithm.

\begin{algorithm}
  \caption{Temporal-difference (TD) learning}\pidx{temporal-difference learning}
  initialize $\fnV[\pi]$ arbitrarily (e.g., as $\vzero$)\;
  \For{$t = 0$ \KwTo $\infty$}{
    follow policy $\pi$ to obtain the transition $(x, a, r, x')$\;
    $\V[\pi]{x} \gets (1-\alpha_t)\V[\pi]{x} + \alpha_t(r + \gamma \V[\pi]{x'})$ \algeq{eq:td_learning}
  }
\end{algorithm}

The update rule is sometimes written equivalently as \begin{align}
  \V[\pi]{x} \gets \V[\pi]{x} + \alpha_t(r + \gamma \V[\pi]{x'} - \V[\pi]{x}).
\end{align}
Thus, the update to $\V[\pi]{x}$ is proportional to the learning rate and the difference between the previous estimate and the renewed estimate using the new observation.

\begin{thm}[Convergence of TD-learning, \cite{jaakkola1993convergence}]
  If $(\alpha_t)_{t\in\Nat_0}$ satisfies the RM-conditions \eqref{eq:rm_conditions} and all state-action pairs are chosen infinitely often, then $\fnV[\pi]$ converges to $\fnv[\pi]$ with probability $1$.
\end{thm}

Importantly, note that due to the Monte Carlo approximation of \cref{eq:td_learning_mc} with respect to transitions attained by following policy $\pi$, TD-learning is fundamentally on-policy.
That is, for the estimates $\fnV[\pi]$ to converge to the true value function $\fnv[\pi]$, the transitions that are used for the estimation must follow policy $\pi$.

\subsection{SARSA: On-policy Control}

TD-learning merely estimates the value function of a fixed policy $\pi$.
To find the optimal policy $\pis$, we can use an analogue of policy iteration~(see \cref{alg:policy_iteration}).
Here, it is more convenient to use an estimate of the state-action value function $\fnq[\pi]$ which can be obtained analogously to the bootstrapping estimate of $\fnv[\pi]$ \eqref{eq:bootstrapping_estimate}, \begin{align}
  \q[\pi]{x}{a} &= r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \sum_{a' \in \sA} \pi(a' \mid x') \q[\pi]{x'}{a'} \margintag{using Bellman's expectation equation \eqref{eq:q_function3}} \nonumber \\
  &= \E[R_0, X_1, A_1]{R_0 + \gamma \q[\pi]{X_1}{A_1} \mid X_0 = x, A_0 = a} \margintag{interpreting the above expression as an expectation over $R_0, X_1$ and $A_1$} \\[5pt]
  &\approx r + \gamma \q[\pi]{x'}{a'}, \label{eq:sarsa_bootstrapping} \margintag{Monte Carlo approximation with a single sample}
\end{align} where the agent observed transitions $(x, a, r, x')$ and $(x', a', r', x'')$.

The update rule from TD-learning is therefore adapted to\safefootnote{Note that for deterministic policies $\pi$, $\Q[\pi]{x'}{a'} = \Q[\pi]{x'}{\pi(x')} = \V[\pi]{x'}$ if the transitions are obtained by following policy $\pi$.} \begin{align}
  \Q[\pi]{x}{a} \gets (1-\alpha_t)\Q[\pi]{x}{a} + \alpha_t(r + \gamma \Q[\pi]{x'}{a'}). \label{eq:sarsa_update}
\end{align}

This algorithm is known as \midx{SARSA} (short for \emph{state-action-reward-state-action}).
Similar convergence guarantees to those of TD-learning can also be derived for SARSA.

\begin{thm}[Convergence of SARSA, \cite{singh2000convergence}]
  If $(\alpha_t)_{t\in\Nat_0}$ satisfies the RM-conditions \eqref{eq:rm_conditions} and all state-action pairs are chosen infinitely often, then $\fnQ[\pi]$ converges to $\fnq[\pi]$ with probability $1$.
\end{thm}

The policy iteration scheme to identify the optimal policy can be outlined as follows:
In each iteration $t$, we estimate the value function~$\fnq[\pi_t]$ of policy~$\pi_t$ with the estimate $\fnQ[\pi_t]$ obtained from SARSA.
We then choose the greedy policy with respect to $\fnQ[\pi_t]$ as the next policy $\pi_{t+1}$.
However, due to the on-policy nature of SARSA, we cannot reuse any data between the iterations.
Moreover, it turns out that in practice, when using only finitely many samples, this form of greedily optimizing Markov decision processes does not explore enough.
At least partially, this can be compensated for by injecting noise when choosing the next action, e.g., by following an $\varepsilon$-greedy policy or using softmax exploration.\looseness=-1

\subsection{Off-policy Value Estimation}\label{sec:tabular_rl:model_free:off_policy_value_estimation}

Consider the following slight adaptation of the derivation of SARSA~\eqref{eq:sarsa_bootstrapping},\looseness=-1 \begin{align}
  \q[\pi]{x}{a} &= r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \sum_{a' \in \sA} \pi(a' \mid x') \q[\pi]{x'}{a'} \margintag{using Bellman's expectation equation \eqref{eq:q_function3}} \nonumber \\
  &= \E[R_0, X_1]{R_0 + \gamma \sum_{a' \in \sA} \pi(a' \mid X_1) \q[\pi]{X_1}{a'}}[X_0 = x, A_0 = a] \margintag{interpreting the above expression as an expectation over $R_0$ and $X_1$} \\
  &\approx r + \gamma \sum_{a' \in \sA} \pi(a' \mid x') \q[\pi]{x'}{a'}, \margintag{Monte Carlo approximation with a single sample}
\end{align} where the agent observed the transition $(x, a, r, x')$.
This yields the update rule, \begin{align}
  \Q[\pi]{x}{a} \gets (1-\alpha_t)\Q[\pi]{x}{a} + \alpha_t\parentheses*{r + \gamma \sum_{a' \in \sA} \pi(a' \mid x') \Q[\pi]{x'}{a'}}.
\end{align}
This adapted update rule \emph{explicitly} chooses the subsequent action $a'$ according to policy $\pi$ whereas SARSA absorbs this choice into the Monte Carlo approximation.
The algorithm has analogous convergence guarantees to those of SARSA.

Crucially, this algorithm is off-policy.
That is, we can use transitions that were obtained according to \emph{any} policy to estimate the value of a fixed policy $\pi$, which we may have never used!
Perhaps this seems contradictory at first, but it is not.
As noted, the key difference to the on-policy TD-learning and SARSA is that our estimate of the Q-function explicitly keeps track of the next-performed action.
It does so for any action in any state.
Moreover, note that the transitions that are due to the dynamics model and rewards are unaffected by the used policy.
They merely depend on the originating state-action pair.
We can therefore use the instances where other policies played action $\pi(x)$ in state $x$ to estimate the performance of $\pi$.

\subsection{Q-learning: Off-policy Control}\label{sec:tabular_rl:model_free:q_learning}

It turns out that there is a way to estimate the value function of the optimal policy directly.
Recall from Bellman's theorem \eqref{eq:bop1} that the optimal policy $\pis$ can be characterized in terms of the optimal state-action value function $\fnq[\star]$, \begin{align*}
  \pis(x) = \argmax_{a \in \sA} \q*{x}{a}.
\end{align*} $\pis$ corresponds to greedily maximizing the value function.

Analogously to our derivation of SARSA \eqref{eq:sarsa_bootstrapping}, only using Bellman's theorem \eqref{eq:bop2} in place of Bellman's expectation equation \eqref{eq:q_function3}, we obtain,\looseness=-1 \begin{align}
  \q*{x}{a} &= r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \max_{a' \in \sA} \q*{x'}{a'} \margintag{using that the Q-function is a fixed-point of the Bellman update, see Bellman's theorem \eqref{eq:bop2}} \nonumber \\
  &= \E[R_0, X_1]{R_0 + \gamma \max_{a' \in \sA} \q*{X_1}{a'}}[X_0 = x, A_0 = a] \label{eq:q_learning_mc} \margintag{interpreting the above expression as an expectation over $R_0$ and $X_1$} \\
  &\approx r + \gamma \max_{a' \in \sA} \q*{x'}{a'}, \label{eq:q_learning_bootstrapping} \margintag{Monte Carlo approximation with a single sample}
\end{align} where the agent observed the transition $(x, a, r, x')$.
Using a bootstrapping estimate $\fnQ[\star]$ for $\fnq[\star]$, we obtain a structurally similar algorithm to TD-learning and SARSA --- only for estimating the optimal Q-function directly!
This algorithm is known as \emph{Q-learning}.
Whereas we have seen that the optimal policy can be found using SARSA in a policy-iteration-like scheme, Q-learning is conceptually similar to value iteration.

\begin{algorithm}[H]
  \caption{Q-learning}\pidx{Q-learning}
  initialize $\Q*{x}{a}$ arbitrarily (e.g., as $\vzero$)\;
  \For{$t = 0$ \KwTo $\infty$}{
    observe the transition $(x, a, r, x')$\;
    $\Q*{x}{a} \gets (1-\alpha_t)\Q*{x}{a} + \alpha_t(r + \gamma \max_{a' \in \sA} \Q*{x'}{a'})$ \algeq{eq:q_learning}
  }
\end{algorithm}

Similarly to TD-learning, the update rule can also be expressed as \begin{align}
  \Q*{x}{a} \gets \Q*{x}{a} + \alpha_t\parentheses*{r + \gamma \max_{a' \in \sA} \Q*{x'}{a'} - \Q*{x}{a}}.
\end{align}
Crucially, the Monte Carlo approximation of \cref{eq:q_learning_mc} does not depend on the policy.
Thus, Q-learning is an off-policy method.

\begin{thm}[Convergence of Q-learning, \cite{jaakkola1993convergence}]\label{thm:q_learning_convergence}
  If $(\alpha_t)_{t\in\Nat_0}$ satisfies the RM-conditions \eqref{eq:rm_conditions} and all state-action pairs are chosen infinitely often (that is, the sequence of policies used to obtain the transitions is GLIE), then $\fnQ[\star]$ converges to $\fnq[\star]$ with probability $1$.
\end{thm}

It can be shown that with probability at least $1-\delta$, Q-learning converges to an $\epsilon$-optimal policy in a number of steps that is polynomial in $\log |X|$, $\log |A|$, $\nicefrac{1}{\epsilon}$ and $\log \nicefrac{1}{\delta}$ \citep{even2003learning}.

\subsection{Optimistic Q-learning}

The next natural question is how to effectively trade exploration and exploitation to both visit all state-action pairs many times, but also attain a high reward.

However, as we have seen in \cref{sec:tabular_rl:exploration_exploitation}, random ``uninformed'' exploration like $\varepsilon$-greedy and softmax exploration explores the state space very slowly.
We therefore return to the principle of \midx{optimism in the face of uncertainty}, which already led us to the $R_\mathrm{max}$ algorithm in the model-based setting.
We will now additionally assume that the rewards are non-negative, that is, $0 \leq r(x, a) \leq R_\mathrm{max} \; (\forall x \in \sX, a \in \sA)$.
It turns out that a similar algorithm to $R_\mathrm{max}$ also exists for (model-free) Q-learning: it is called \emph{optimistic Q-learning} and shown in \cref{alg:opt_q_learning}.

\begin{algorithm}[H]
  \caption{Optimistic Q-learning}\label{alg:opt_q_learning}\pidx{optimistic Q-learning}
  initialize $\Q*{x}{a} = V_\mathrm{max} \prod_{t=1}^{T_\mathrm{init}} (1 - \alpha_t)^{-1}$\;
  \For{$t = 0$ \KwTo $\infty$}{
    pick action $a_t = \argmax_{a \in \sA} \Q*{x}{a}$ and observe the transition $(x, a_t, r, x')$\;
    $\Q*{x}{a_t} \gets (1-\alpha_t)\Q*{x}{a_t} + \alpha_t(r + \gamma \max_{a' \in \sA} \Q*{x'}{a'})$ \algeq{eq:optimistic_q_learning}
  }
\end{algorithm}

Here, \begin{align*}
  V_\mathrm{max} \defeq \frac{R_\mathrm{max}}{1-\gamma} \geq \max_{x \in \sX, a \in \sA} \q*{x}{a},
\end{align*} is an upper bound on the discounted return and $T_\mathrm{init}$ is some initialization time.
Intuitively, the initialization of $\fnQ[\star]$ corresponds to the best-case long-term reward, assuming that all individual rewards are upper bounded by $R_\mathrm{max}$. This is shown by the following lemma.

\begin{lem}\label{lem:optimistic_q_learning}
  Denote by $\fnQ[\star][t]$, the approximation of $\fnq[\star]$ attained in the $t$-th iteration of optimistic Q-learning.
  Then, for any state-action pair $(x,a)$ and iteration $t$ such that $N(a \mid x) \leq T_\mathrm{init}$,\footnote{$N(a \mid x)$ is the number of times action $a$ is performed in state $x$.} \begin{align}
    \Q*{x}{a}[t] \geq V_\mathrm{max} \geq \q*{x}{a}.
  \end{align}
\end{lem}
\begin{proof}
  We write $\beta_\tau \defeq \prod_{i=1}^\tau (1-\alpha_i)$ and $\eta_{i,\tau} \defeq \alpha_i \prod_{j=i+1}^\tau (1-\alpha_j)$.
  Using the update rule of optimistic Q-learning \eqref{eq:optimistic_q_learning}, we have \begin{align}
    \Q*{x}{a}[t] = \beta_{N(a \mid x)} \Q*{x}{a}[0] + \sum_{i=1}^{N(a \mid x)} \eta_{i,N(a \mid x)} (r + \gamma \max_{a_i \in \sA} \Q*{x_i}{a_i}[t_i]) \label{eq:opt_q_learning_char2}
  \end{align} where $x_i$ is the next state arrived at time $t_i$ when action $a$ is performed the $i$-th time in state $x$.

  Using the assumption that the rewards are non-negative, from \cref{eq:opt_q_learning_char2} and $\Q*{x}{a}[0] = \nicefrac{V_\mathrm{max}}{\beta_{T_\mathrm{init}}}$, we immediately have \begin{align*}
    \Q*{x}{a}[t] &\geq \frac{\beta_{N(a \mid x)}}{\beta_{T_\mathrm{init}}} V_\mathrm{max} \\
    &\geq V_\mathrm{max}. \margintag{using $N(a \mid x) \leq T_\mathrm{init}$} \qedhere
  \end{align*}
\end{proof}

Now, if $T_\mathrm{init}$ is chosen large enough, it can be shown that optimistic Q-learning converges quickly to an optimal policy.

\begin{thm}[Convergence of optimistic Q-learning, \cite{even2001convergence}]\label{thm:optimistic_q_learning}
  With probability at least $1-\delta$, optimistic Q-learning obtains an $\epsilon$-optimal policy after a number of steps that is polynomial in $\card{\sX}$, $\card{\sA}$, $\nicefrac{1}{\epsilon}$, $\log \nicefrac{1}{\delta}$, and $R_\mathrm{max}$ where the initialization time $T_\mathrm{init}$ is upper bounded by a polynomial in the same coefficients.
\end{thm}

Note that for Q-learning, we still need to store $\Q*{x}{a}$ for any state-action pair in memory.
Thus, Q-learning requires $\BigO{n m}$ memory. During each transition, we need to compute \begin{align*}
  \max_{a \in \sA} \Q*{x'}{a'}
\end{align*} once.
If we run Q-learning for $T$ iterations, this yields a time complexity of $\BigO{T m}$.
Crucially, for sparse Markov decision processes where, in most states, only few actions are permitted, each iteration of Q-learning can be performed in (virtually) constant time.
This is a big improvement of the quadratic (in the number of states) performance of the model-based $R_\mathrm{max}$ algorithm.

\section*{Discussion}

We have seen that both the model-based $R_\mathrm{max}$ algorithm and the model-free Q-learning take time polynomial in the number of states $\card{\sX}$ and the number of actions $\card{\sA}$ to converge.
While this is acceptable in small grid worlds, this is completely unacceptable for large state and action spaces.\looseness=-1

Often, domains are continuous, for example when modeling beliefs about states in a partially observable environment.
Also, in many structured domains (e.g., chess or multiagent planning), the size of the state and action space is exponential in the size of the input.
In the final two chapters, we will therefore explore how model-free and model-based methods can be used (approximately) in such large domains.


\excheading

\begin{nexercise}{Q-learning}{q_learning}
  Assume the following grid world, where from state $A$ the agent can go to the right and down, and from state $B$ to the left and down. From states $G_1$ and $G_2$ the only action is to exit.
  The agent receives a reward ($+10$ or $+1$) only when exiting.

  \begin{center}
    \begin{tabular}{|c|c|}
      \multicolumn{2}{c}{Rewards} \\
      \hline
      $0$ & $0$ \\
      \hline
      $+10$ & $+1$ \\
      \hline
    \end{tabular}
    \begin{tabular}{|c|c|}
      \multicolumn{2}{c}{States} \\
      \hline
      $A$ & $B$ \\
      \hline
      $G_1$ & $G_2$ \\
      \hline
    \end{tabular}
  \end{center}

  We assume the discount factor $\gamma = 1$ and that all actions are deterministic.\looseness=-1

  \begin{enumerate}
    \item We observe the following two episodes:

    \vspace{5pt}
    \begin{center}
      \begin{tabular}{|cccc|}
        \multicolumn{4}{c}{Episode 1} \\
        \hline
        $x$ & $a$ & $x'$ & $r$ \\
        \hline
        $A$ & $\downarrow$ & $G_1$ & $0$ \\
        $G_1$ & exit & & $10$ \\
        \hline
      \end{tabular}
      \begin{tabular}{|cccc|}
        \multicolumn{4}{c}{Episode 2} \\
        \hline
        $x$ & $a$ & $x'$ & $r$ \\
        \hline
        $B$ & $\leftarrow$ & $A$ & $0$ \\
        $A$ & $\downarrow$ & $G_1$ & $0$ \\
        $G_1$ & exit & & $10$ \\
        \hline
      \end{tabular}
      \vspace{5pt}
    \end{center}
    \vspace{5pt}

    Assume $\alpha = 0.3$, and that Q-values of all non-terminal states are initialized to $0.5$.
    What are the Q-values \begin{align*}
      \Q*{A}{\downarrow}, \quad \Q*{G_1}{\text{exit}}, \quad \Q*{G_2}{\text{exit}}
    \end{align*} learned by executing Q-learning with the above episodes?

    \item Will Q-learning converge to $\fnq[\star]$ for all state-action pairs $(x, a)$ if we repeat episode 1 and episode 2 infinitely often?
    If not, design a sequence of episodes that leads to convergence.

    \item How does the choice of initial Q-values influence the convergence of the Q-learning algorithm when episodes are obtained off-policy?

    \item Determine $\fnv[\star]$ for all states.
  \end{enumerate}
\end{nexercise}
