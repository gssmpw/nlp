\chapter{Model-free Reinforcement Learning}\label{sec:mfarl}

In the previous chapter, we have seen methods for tabular settings.
Our goal now is to extend the model-free methods like TD-learning and Q-learning to large state-action spaces $\spX$ and $\spA$.
We have seen that a crucial bottleneck of these methods is the parameterization of the value function.
If we want to store the value function in a table, we need at least $\BigO{\card{\spX}}$ space.
If we learn the Q function, we even need $\BigO{\card{\spX} \cdot \card{\spA}}$ space.
Also, for large state-action spaces, the time required to compute the value function for every state-action pair exactly will grow polynomially in the size of the state-action space. Hence, a natural idea is to learn \emph{approximations} of these functions with a low-dimensional parameterization.
Such approximations were the focus of the first few chapters and are, in fact, the key idea behind methods for reinforcement learning in large state-action spaces.

\section{Tabular Reinforcement Learning as Optimization}\label{sec:mfarl:tabular_rl_as_optimization}

To begin with, let us reinterpret the model-free methods from the previous section, TD-learning and Q-learning, as solving an optimization problem, where each iteration corresponds to a single gradient update.
We will focus on TD-learning here, but the same interpretation applies to Q-learning.
Recall the update rule of TD-learning \eqref{eq:td_learning}, \begin{align*}
  \V[\pi]{x} \gets (1-\alpha_t) \V[\pi]{x} + \alpha_t (r + \gamma \V[\pi]{x'}).
\end{align*}
Note that this looks just like the update rule of an optimization algorithm!
We can parameterize our estimates $\fnV[\pi]$ with parameters $\vtheta$ that are updated according to the gradient of some loss function, assuming fixed bootstrapping estimates.
In particular, in the tabular setting (i.e., over a finite domain), we can parameterize the value function exactly by learning a separate parameter for each state, \begin{align}
  \vtheta \defeq [\vtheta(1), \dots, \vtheta(n)], \quad \V[\pi]{x; \vtheta} \defeq \vtheta(x).
\end{align}

To re-derive the above update rule as a gradient update, let us consider the following loss function, \begin{align}
  \mean{\ell}(\vtheta; x, r) &\defeq \frac{1}{2}\parentheses*{\v[\pi]{x} - \vtheta(x)}^2 \\
  &= \frac{1}{2}\parentheses*{r + \gamma \E[x' \mid x, \pi(x)]{\v[\pi]{x'}} - \vtheta(x)}^2 \margintag{using Bellman's expectation equation \eqref{eq:bellman_expectation_equation_short}}
\end{align}
Note that this loss corresponds to a standard squared loss of the difference between the parameter $\vtheta(x)$ and the label $\v[\pi]{x}$ we want to learn.\looseness=-1

We can now find the gradient of this loss. Elementary calculations yield,\looseness=-1 \begin{align}
  \grad_{\vtheta(x)} \mean{\ell}(\vtheta; x, r) = \vtheta(x) - \parentheses*{r + \gamma \E[x' \mid x, \pi(x)]{\v[\pi]{x'}}}. \label{eq:l_2_bar_gradient}
\end{align}
Now, we cannot compute this derivative because we cannot compute the expectation.
Firstly, the expectation is over the true value function which is unknown to us.
Secondly, the expectation is over the transition model which we are trying to avoid in model-free methods.

To resolve the first issue, analogously to TD-learning, instead of learning the true value function $\fnv[\pi]$ which is unknown, we learn the bootstrapping estimate $\fnV[\pi]$.
Recall that the core principle behind bootstrapping as discussed in \cref{sec:tabular_rl:model_free:on_policy_value_estimation} is that this bootstrapping estimate~$\fnV[\pi]$ is \emph{treated} as if it were independent of the current estimate of the value function $\vtheta$.
To emphasize this, we write $\V[\pi]{x; \old{\vtheta}} \approx \v[\pi]{x}$ where $\old{\vtheta} = \vtheta$ but $\old{\vtheta}$ is treated as a constant with respect to $\vtheta$.\footnote{That is, the bootstrapping estimate $\V[\pi]{x; \old{\vtheta}}$ is assumed to be constant with respect to $\vtheta(x)$ in the same way that $\v[\pi]{x}$ is constant with respect to $\vtheta(x)$.

If we were not using the bootstrapping estimate, the following derivation of the gradient of the loss would not be this simple.}

To resolve the second issue, analogously to the introduction of TD-learning in the previous chapter, we will use a Monte Carlo estimate using a single sample.
Recall that this is only possible because the transitions are conditionally independent given the state-action pair.

\begin{rmk}{Sample (in)efficiency of model-free methods}{}
  Taking these two shortcuts are two of the main reasons why model-free methods such as TD-learning and Q-learning are usually \emph{sample inefficient}.
  This is because using a bootstrapping estimate leads to ``(initially) incorrect'' and ``unstable'' targets of the optimization problem,\safefootnote{We explore this in some more capacity in \cref{sec:mfarl:value_function_approximation:heuristics} where we cover heuristic approaches to alleviate this problem to some degree.} and Monte Carlo estimation with a single sample leads to a large variance.
  Recall that the theoretical guarantees for model-free methods in the tabular setting therefore required that all state-action pairs are visited infinitely often.
\end{rmk}

Using the aforementioned shortcuts, let us define the loss $\ell$ after observing the single transition $(x, a, r, x')$, \begin{align}
  \ell(\vtheta; x, r, x') \defeq \frac{1}{2}\parentheses*{r + \gamma \old{\vtheta}(x') - \vtheta(x)}^2.
\end{align}
We define the gradient of this loss with respect to $\vtheta(x)$ as \begin{align}
  \delta_\mathrm{TD} &\defeq \grad_{\vtheta(x)} \ell(\vtheta; x, r, x') \nonumber \\
  &= \vtheta(x) - \parentheses*{r + \gamma \old{\vtheta}(x')}. \label{eq:td_error}
\end{align}
This error term is also called \midx<temporal-difference (TD) error>{temporal-difference error}.
The temporal difference error compares the previous estimate of the value function to the bootstrapping estimate of the value function.
We know from the law of large numbers \eqref{eq:slln} that Monte Carlo averages are unbiased.\footnote{Crucially, the samples are unbiased with respect to the approximate label in terms of the bootstrapping estimate only.
Due to bootstrapping the value function, the estimates are not unbiased with respect to the true value function.
Moreover, the variance of each individual estimation of the gradient is large, as we only consider a single transition.}
We therefore have, \begin{align}
  \E[x' \mid x, \pi(x)]{\delta_\mathrm{TD}} \approx \grad_{\vtheta(x)} \mean{\ell}(\vtheta; x, r). \label{eq:td_error_approx}
\end{align}

Naturally, we can use these unbiased gradient estimates with respect to the loss $\mean{\ell}$ to perform stochastic gradient descent.
This yields the update rule, \begin{align}
  \V[\pi]{x; \vtheta} = \vtheta(x) &\gets \vtheta(x) - \alpha_t \delta_\mathrm{TD} \label{eq:td_learning_gradient_update} \margintag{using stochastic gradient descent with learning rate $\alpha_t$, see \cref{alg:sgd}} \\[5pt]
  &= (1 - \alpha_t) \vtheta(x) + \alpha_t \parentheses*{r + \gamma \old{\vtheta}(x')} \margintag{using the definition of the temporal difference error \eqref{eq:td_error}} \nonumber \\
  &= (1-\alpha_t) \V[\pi]{x; \vtheta} + \alpha_t \parentheses*{r + \gamma \V[\pi]{x'; \old{\vtheta}}}. \margintag{substituting $\V[\pi]{x; \vtheta}$ for $\vtheta(x)$} \label{eq:td_learning_as_optimization}
\end{align}
Observe that this gradient update coincides with the update rule of TD-learning \eqref{eq:td_learning}.
Therefore, TD-learning is essentially performing stochastic gradient descent using the TD-error as an unbiased gradient estimate.\footnote{An alternative interpretation is that TD-learning performs gradient descent with respect to the loss $\ell$.}
Crucially, TD-learning performs stochastic gradient descent with respect to the bootstrapping estimate of the value function $\fnV[\pi]$ and not the true value function $\fnv[\pi]$!
Stochastic gradient descent with a bootstrapping estimate is also called \midx{stochastic semi-gradient descent}.
Importantly, the optimization target $r + \gamma \V[\pi]{x'; \old{\vtheta}}$ from the loss $\ell$ is now \emph{moving} between iterations which introduces some practical challenges we will discuss in \cref{sec:mfarl:value_function_approximation:heuristics}.
We have seen in the previous chapter that using a bootstrapping estimate still guarantees (asymptotic) convergence to the true value function.

\section{Value Function Approximation}\label{sec:mfarl:value_function_approximation}

To scale to large state spaces, it is natural to approximate the value function using a parameterized model, $\V{\vx; \vtheta}$ or $\Q{\vx}{\va; \vtheta}$.
You may think of this as a regression problem where we map state(-action) pairs to a real number.
Recall from the previous section that this is a strict generalization of the tabular setting, as we could use a separate parameter to learn the value function for each individual state-action pair.
Our goal for large state-action spaces is to exploit the smoothness properties\footnote{That is, the value function takes similar values in ``similar'' states.} of the value function to condense the representation.

A straightforward approach is to use a linear function approximation with the hand-designed feature map $\vphi$, \begin{align}
  \Q{\vx}{\va; \vtheta} \defeq \transpose{\vtheta} \vphi(\vx, \va). \label{eq:q_linear_approx}
\end{align}
A common alternative is to use a deep neural network to learn these features instead.
Doing so is also known as \midx{deep reinforcement learning}.\footnote{Note that often non-Bayesian deep learning (i.e., point estimates of the weights of a neural network) is applied. In the final chapter, \cref{sec:mbarl}, we will explore the benefits of using Bayesian deep learning.}

We will now apply the derivation from the previous section directly to Q-learning.
For Q-learning, after observing the transition $(\vx, \va, r, \vxp)$, the loss function is given as \begin{align}
  \ell(\vtheta; \vx, \va, r, \vxp) \defeq \frac{1}{2}\parentheses*{r + \gamma \max_{\vap \in \spA} \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}}^2. \label{eq:q_loss}
\end{align}
Here, we simply use Bellman's optimality equation \eqref{eq:bop2} to estimate $\q*{\vx}{\va}$, instead of the estimation of $\v[\pi]{\vx}$ using Bellman's expectation equation for TD-learning.
The difference between the current approximation and the optimization target, \begin{align}
  \delta_\mathrm{B} \defeq r + \gamma \max_{\vap \in \spA} \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}, \label{eq:bellman_error}
\end{align} is called the \midx{Bellman error}[idxpagebf].
Analogously to TD-learning,\footnote{compare to \cref{eq:td_learning_gradient_update}} we obtain the gradient update, \begin{align}
  \vtheta &\gets \vtheta - \alpha_t \grad_\vtheta \ell(\vtheta; \vx, \va, r, \vxp) \\
  &= \vtheta - \alpha_t \grad_\vtheta \frac{1}{2}\parentheses*{r + \gamma \max_{\vap \in \spA} \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}}^2 \margintag{using the definition of $\ell$ \eqref{eq:q_loss}} \nonumber \\
  &= \vtheta + \alpha_t \delta_\mathrm{B} \grad_\vtheta \Q*{\vx}{\va; \vtheta}. \label{eq:q_learning_gradient_update_general} \margintag{using the chain rule}
  \intertext{When using a neural network to learn $\fnQ[\star]$, we can use automatic differentiation to obtain unbiased gradient estimates. In the case of linear function approximation, we can compute the gradient exactly,}
  &= \vtheta + \alpha_t \delta_\mathrm{B} \grad_\vtheta \transpose{\vtheta} \vphi(\vx, \va) \margintag{using the linear approximation of $\fnQ[\star]$ \eqref{eq:q_linear_approx}} \nonumber \\
  &= \vtheta + \alpha_t \delta_\mathrm{B} \vphi(\vx, \va). \label{eq:q_learning_gradient_update}
\end{align}

In the tabular setting, this algorithm is identical to Q-learning and, in particular, converges to the true Q-function $\fnq[\star]$.\footnote{see \cref{thm:q_learning_convergence}}
There are few such results in the approximate setting. Usage in practice indicates that using an approximation of the value function ``should be fine'' when a ``rich-enough'' class of functions is used.

\subsection{Heuristics}\label{sec:mfarl:value_function_approximation:heuristics}

The vanilla stochastic semi-gradient descent is very slow.
In this subsection, we will discuss some ``tricks of the trade'' to improve its performance.\looseness=-1

\paragraph{Stabilizing optimization targets:}

There are mainly two problems.
The first problem is that, as mentioned previously, the bootstrapping estimate changes after each iteration.
As we are trying to learn an approximate value function that depends on the bootstrapping estimate, this means that the optimization target is ``moving'' between iterations.
In practice, moving targets lead to stability issues.
The first family of techniques we discuss here aim to ``stabilize'' the optimization targets.

One such technique is called \midx{neural fitted Q-iteration} or \midx{deep Q-networks} (DQN) \citep{mnih2015human}.
DQN updates the neural network used for the approximate bootstrapping estimate infrequently to maintain a constant optimization target across multiple episodes.
How this is implemented exactly varies.
One approach is to clone the neural network and maintain one changing neural network (``online network'') for the most recent estimate of the Q-function which is parameterized by $\vtheta$, and one fixed neural network (``target network'') used as the target which is parameterized by $\old{\vtheta}$ and which is updated infrequently.

This can be implemented by maintaining a data set $\spD$ of observed transitions (the so-called \midx{replay buffer}) and then ``every once in a while'' (e.g., once $\card{\spD}$ is large enough) solving a regression problem, where the labels are determined by the target network.
This yields a loss term where the target is fixed across all transitions in the replay buffer $\spD$, \begin{align}
  \ell_{\mathrm{DQN}}(\vtheta; \spD) \defeq \frac{1}{2} \sum_{(\vx, \va, r, \vxp) \in \spD} \parentheses*{r + \gamma \max_{\vap \in \spA} \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}}^2. \label{eq:dqn} \margintag{compare to the Q-learning loss \eqref{eq:q_loss}}
\end{align}
The loss can also be interpreted (in an online sense) as performing regular Q-learning with the modification that the target network $\old{\vtheta}$ is not updated to $\vtheta$ after every observed transition, but instead only after observing $\card{\spD}$-many transitions.
This technique is known as \midx{experience replay}.
Another approach is \midx{Polyak averaging} where the target network is gradually ``nudged'' by the neural network used to estimate the Q-function.\looseness=-1

\paragraph{Maximization bias:}

Now, observe that the estimates $\fnQ[\star]$ are noisy estimates of $\fnq[\star]$ and consider the term, \begin{align*}
  \max_{\vap \in \spA} \q*{\vxp}{\vap} \approx \max_{\vap \in \spA} \Q*{\vxp}{\vap; \old{\vtheta}},
\end{align*} from the loss function \eqref{eq:dqn}.
This term maximizes a \emph{noisy} estimate of~$\fnq[\star]$, which leads to a \emph{biased} estimate of $\max \fnq[\star]$ as can be seen in \cref{fig:maximization_bias}.
The fact that the update rules of Q-learning and DQN are affected by inaccuracies (i.e., noise in the estimates) of the learned Q-function is known as the ``\midx{maximization bias}''.

\begin{figure*}
  \incplt{maximization_bias}
  \caption{Illustration of overestimation during learning. In each state (x-axis), there are 10 actions. The left column shows the true values $\v[\star]{x}$ (purple line). All true action values are defined by $\q[\star]{x}{a} \defeq \v[\star]{x}$. The green line shows estimated values $\Q[\star]{x}{a}$ for one action as a function of state, fitted to the true value at several sampled states (green dots). The middle column plots show all the estimated values (green), and the maximum of these values (dashed black). The maximum is higher than the true value (purple, left plot) almost everywhere. The right column plots show the difference in red. The blue line in the right plots is the estimate used by Double Q-learning with a second set of samples for each state. The blue line is much closer to zero, indicating less bias. The three rows correspond to different true functions (left, purple) or capacities of the fitted function (left, green). Reproduced with permission from \icite{van2016deep}.}\label{fig:maximization_bias}
\end{figure*}

\midx<Double DQN>{double DQN} (DDQN) is an algorithm that addresses this maximization bias \citep{van2016deep}.
Instead of picking the optimal action with respect to the old network, it picks the optimal action with respect to the new network, \begin{align}
  &\ell_{\mathrm{DDQN}}(\vtheta; \spD) \defeq \frac{1}{2} \sum_{(\vx, \va, r, \vxp) \in \spD} \parentheses*{r + \gamma \Q*{\vxp}{\vas(\vxp; \vtheta); \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}}^2 \label{eq:ddqn} \\
  &\text{where} \quad \vas(\vxp; \vtheta) \defeq \argmax_{\vap \in \spA} \Q*{\vxp}{\vap; \vtheta}.
\end{align}
Intuitively, this change ensures that the evaluation of the target network is consistent with the updated Q-function, which makes the algorithm more robust to noise.

Similarly to DQN, this can also be interpreted as the online update, \begin{align}
  \vtheta \gets \vtheta + \alpha_t \parentheses*{r + \gamma \Q*{\vxp}{\vas(\vxp; \vtheta); \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}} \grad_\vtheta \Q*{\vx}{\va; \vtheta} \label{eq:ddqn_single_update}
\end{align} after observing a single transition $(\vx, \va, r, \vxp)$ where while differentiating, $\vas(\vxp; \vtheta)$ is treated as constant with respect to $\vtheta$.
$\old{\vtheta}$ is then updated to $\vtheta$ after observing $\card{\spD}$-many transitions.


\section{Policy Approximation}\label{sec:mfarl:policy_approximation}

Q-learning defines a policy implicitly by \begin{align}
  \opt{\vpi}(\vx) \defeq \argmax_{\va \in \spA} \Q*{\vx}{\va}. \label{eq:q_learning_policy}
\end{align}
Q-learning also maximizes over the set of all actions in its update step while learning the Q-function.
This is intractable for large and, in particular, continuous action spaces.
A natural idea to escape this limitation is to immediately learn an approximate parameterized policy, \begin{align}
  \opt{\vpi}(\vx) \approx \vpi(\vx; \vvarphi) \eqdef \vpi_\vvarphi(\vx).
\end{align}
Methods that find an approximate policy are also called \midx<policy search methods>{policy search method} or \midx<policy gradient methods>{policy gradient method}.

Whereas with Q-learning, exploration can be encouraged by using an $\varepsilon$-greedy policy, softmax exploration, or an optimistic initialization, we will see later that policy gradient methods fundamentally rely on randomized policies for exploration.

\begin{rmk}{Notation}{}
  We refer to deterministic policies $\vpi$ in bold, as they can be interpreted as vector-valued functions from $\spX$ to $\spA$.
  We still refer to randomized policies by $\pi$, as for each state $\vx \in \spX$ they are represented as a PDF over actions $\spA$.

  In particular, we denote by $\pi_\vvarphi(\va \mid \vx)$ the probability (density) of playing action $\va$ when in state $\vx$ according to $\pi_\vvarphi$.
\end{rmk}

\subsection{Estimating Policy Values}

We will begin by attributing a ``value'' to a policy.
Recall the definition of the discounted payoff $G_t$ from time $t$, which we are aiming to maximize,\looseness=-1 \begin{align*}
  G_t = \sum_{m=0}^\infty \gamma^m R_{t+m}. \margintag{see \cref{eq:discounted_payoff}}
\end{align*}
We define $G_{t:T}$ to be the \midx{bounded discounted payoff} until time $T$, \begin{align}
  G_{t:T} \defeq \sum_{m=0}^{T-1-t} \gamma^m R_{t+m}. \label{eq:bounded_discounted_payoff}
\end{align}
Based on these two random variables, we can define the policy value function:\looseness=-1

\begin{defn}[Policy value function]\pidx{policy value function}
  The \emph{policy value function}, \begin{align}
    \j{\pi} \defeq \E[\pi]{G_0} = \E[\pi]{\sum_{t=0}^\infty \gamma^t R_t},
  \end{align} measures the expected discounted payoff of policy $\pi$.\footnote{We neglect here that implicitly one also averages over the initial state if this state is not fixed.}
  We also define the bounded variant, \begin{align}
    \j{\pi}[T] \defeq \E[\pi]{G_{0:T}} = \E[\pi]{\sum_{t=0}^{T-1} \gamma^t R_t}.
  \end{align}
\end{defn}
For simplicity, we will abbreviate $\j{\vvarphi} \defeq \j{\pi_\vvarphi}$.

\begin{rmk}{Notation of policy value function}{}
  We adopt the more common notation $\j{\pi}$ for the policy value function, as opposed to $j(\pi)$, which would be consistent with our notation of the (true) value functions $\fnv[\pi], \fnq[\pi]$.
  So don't be confused by this: just like the value functions $\fnv[\pi], \fnq[\pi]$, the policy value function $\j{\pi}$ is a deterministic object, measuring the mean discounted payoff.
  We will use $\J{\pi}$ to refer to our estimates of the policy value function.
\end{rmk}

Naturally, we want to maximize $\j{\vvarphi}$. That is, we want to solve \begin{align}
  \opt{\vvarphi} \defeq \argmax_\vvarphi \j{\vvarphi} \label{eq:policy_optimization}
\end{align} which is a non-convex optimization problem.
Let us see how $\j{\vvarphi}$ can be evaluated to understand the optimization problem better.
We will again use a Monte Carlo estimate.
Recall that a fixed $\vvarphi$ induces a unique Markov chain, which can be simulated.
In the episodic setting, each episode (also called \midx{rollout}) of length $T$ yields an independently sampled trajectory, \begin{align}
  \tau^{(i)} \defeq ((\vx_0^{(i)}, \va_0^{(i)}, r_0^{(i)}, \vx_1^{(i)}), (\vx_1^{(i)}, \va_1^{(i)}, r_1^{(i)}, \vx_2^{(i)}), \dots)
\end{align}
Simulating $m$ rollouts yields the samples $\tau^{(1)}, \dots, \tau^{(m)} \sim \altpi_\vvarphi$, where $\altpi_\vvarphi$ is the distribution of all trajectories (i.e., rollouts) of the Markov chain induced by policy $\pi_\vvarphi$.
We denote the (bounded) discounted payoff\pidx{discounted payoff} of the $i$-th rollout by \begin{align}
  g_{0:T}^{(i)} \defeq \sum_{t=0}^{T-1} \gamma^t r_t^{(i)} \label{eq:bounded_discounted_reward_sample}
\end{align} where $r_t^{(i)}$ is the reward at time $t$ of the $i$-th rollout.
Using a Monte Carlo approximation, we can then estimate $\j{\vvarphi}[T]$.
Moreover, due to the exponential discounting of future rewards, it is reasonable to approximate the policy value function using bounded trajectories, \begin{align}
  \j{\vvarphi} \approx \j{\vvarphi}[T] \approx \J{\vvarphi}[T] \defeq \frac{1}{m} \sum_{i=1}^m g_{0:T}^{(i)}.
\end{align}

\subsection{Policy Gradient}\label{sec:mfarl:policy_approximation:reinforce}

Policy gradient methods solve the above optimization problem~\eqref{eq:policy_optimization} by stochastic gradient ascent on the policy parameter $\vvarphi$: \begin{align}
  \vvarphi \gets \vvarphi + \eta \grad_\vvarphi \j{\vvarphi}. \label{eq:policy_gradient_ascent}
\end{align}

How can we compute the policy gradient?
Let us first formally define the distribution over trajectories $\altpi_\vvarphi$ that we introduced in the previous section.
We can specify the probability of a specific trajectory $\tau$ under a policy $\pi_\vvarphi$ by \begin{align}
  \altpi_\vvarphi(\tau)  = p(\vx_0) \prod_{t=0}^{T-1} \pi_\vvarphi(\va_t \mid \vx_t) p(\vx_{t+1} \mid \vx_t, \va_t). \label{eq:trajectory_distr}
\end{align}

For optimizing $\j{\vvarphi}$ we need to obtain unbiased gradient estimates: \begin{align}
  \grad_\vvarphi \j{\vvarphi} \approx \grad_\vvarphi \j{\vvarphi}[T] = \grad_\vvarphi \E[\tau \sim \altpi_\vvarphi]{G_{0:T}}.
\end{align}
Note that the expectation integrates over the measure $\altpi_\vvarphi$, which depends on the parameter $\vvarphi$.
Thus, we cannot move the gradient operator inside the expectation as we have often done previously~(cf.~\cref{sec:background:probability:gradients_of_expectations}).
This should remind you of the reparameterization trick (see \cref{eq:reparameterization_trick1}) that we used to solve a similar gradient in the context of variational inference.
In this context, however, we cannot apply the reparameterization trick.\footnote{This is because the distribution $\altpi_\vvarphi$ is generally not reparameterizable.
We will, however, see that reparameterization gradients are also useful in reinforcement learning. See, e.g., \cref{sec:mfarl:actor_critic_methods:randomized_policies,sec:mbarl:planning:stochastic_dynamics}.}
Fortunately, there is another way of estimating this gradient.

\begin{thm}[Score gradient estimator]\pidx{score gradient estimator}[idxpagebf]
  Under some regularity assumptions, we have \begin{align}
    \grad_\vvarphi \E[\tau \sim \altpi_\vvarphi]{G_0} = \E[\tau \sim \altpi_\vvarphi]{G_0 \grad_\vvarphi \log \altpi_\vvarphi(\tau)}. \label{eq:score_gradient_estimator}
  \end{align}
  This estimator of the gradient is called the \emph{score gradient estimator}.
\end{thm}
\begin{proof}
  To begin with, let us look at the so-called \midx{score function} of the distribution $\altpi_\vvarphi$, $\grad_\vvarphi \log \altpi_\vvarphi(\tau)$.
  Using the chain rule, the score function can be expressed as \begin{align}
    \grad_\vvarphi \log \altpi_\vvarphi(\tau) = \frac{\grad_\vvarphi \altpi_\vvarphi(\tau)}{\altpi_\vvarphi(\tau)} \label{eq:score_function}
  \end{align} and by rearranging the terms, we obtain \begin{align}
    \grad_\vvarphi \altpi_\vvarphi(\tau) = \altpi_\vvarphi(\tau) \grad_\vvarphi \log \altpi_\vvarphi(\tau). \label{eq:score_function_trick}
  \end{align}
  This is called the \midx{score function trick}.\footnote{We have already applied this ``trick'' in \cref{exercise:langevin_dynamics_convergence}.}

  Now, assuming that state and action spaces are continuous, we obtain \begin{align*}
    \grad_\vvarphi \E[\tau \sim \altpi_\vvarphi]{G_0} &= \grad_\vvarphi \int \altpi_\vvarphi(\tau) \cdot G_0 \,d\tau \margintag{using the definition of expectation \eqref{eq:expectation}} \\
    &= \int \grad_\vvarphi \altpi_\vvarphi(\tau) \cdot G_0 \,d\tau \margintag{using the regularity assumptions to swap gradient and integral} \\
    &= \int G_0 \cdot \altpi_\vvarphi(\tau) \grad_\vvarphi \log \altpi_\vvarphi(\tau) \,d\tau \margintag{using the score function trick \eqref{eq:score_function_trick}} \\
    &= \E[\tau \sim \altpi_\vvarphi]{G_0 \grad_\vvarphi \log \altpi_\vvarphi(\tau)}. \qedhere \margintag{interpreting the integral as an expectation over $\altpi_\vvarphi$}
  \end{align*}
\end{proof}

Intuitively, maximizing $\j{\vvarphi}$ increases the probability of policies with high returns and decreases the probability of policies with low returns.

To use the score gradient estimator for estimating the gradient, we need to compute $\grad_\vvarphi \log \altpi_\vvarphi(\tau)$. \begin{align}
  &\grad_\vvarphi \log \altpi_\vvarphi(\tau) \nonumber \\
  &= \grad_\vvarphi \parentheses*{\log p(\vx_0) + \sum_{t=0}^{T-1} \log \pi_\vvarphi(\va_t \mid \vx_t) + \sum_{t=0}^{T-1} \log p(\vx_{t+1} \mid \vx_t, \va_t)} \margintag{using the definition of the distribution over trajectories $\altpi_\vvarphi$} \nonumber \\
  &= \grad_\vvarphi \log p(\vx_0) + \sum_{t=0}^{T-1} \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t) + \sum_{t=0}^{T-1} \grad_\vvarphi \log p(\vx_{t+1} \mid \vx_t, \va_t) \nonumber \\
  &= \sum_{t=0}^{T-1} \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t). \margintag{using that the first and third term are independent of $\vvarphi$}
\end{align}
When using a neural network for the parameterization of the policy $\pi$, we can use automatic differentiation to compute the gradients.

The expectation of the score gradient estimator \eqref{eq:score_gradient_estimator} can be approximated using Monte Carlo sampling, \begin{align}
  \grad_\vvarphi \j{\vvarphi}[T] \approx \grad_\vvarphi \J{\vvarphi}[T] \defeq \frac{1}{m} \sum_{i=1}^m g_{0:T}^{(i)} \sum_{t=0}^{T-1} \grad_\vvarphi \log \pi_\vvarphi(\va_t^{(i)} \mid \vx_t^{(i)}). \label{eq:score_gradient_estimator_approx}
\end{align}
However, typically the variance of these estimates is very large.
Using so-called \midx<baselines>{baseline}, we can reduce the variance dramatically \exerciserefmark{score_gradients_with_baselines_variance}.

\begin{lem}[Score gradients with baselines]\label{lem:score_gradients_with_baselines}
  We have, \begin{align}
    \E[\tau \sim \altpi_\vvarphi]{G_0 \grad_\vvarphi \log \altpi_\vvarphi(\tau)} = \E[\tau \sim \altpi_\vvarphi]{(G_0 - b) \grad_\vvarphi \log \altpi_\vvarphi(\tau)}.
  \end{align}
  Here, $b \in \R$ is called a \emph{baseline}.
\end{lem}
\begin{proof}
  For the term to the right, we have due to linearity of expectation~\eqref{eq:linearity_expectation}, \begin{align*}
    \E[\tau \sim \altpi_\vvarphi]{(G_0 - b) \grad_\vvarphi \log \altpi_\vvarphi(\tau)} = \begin{multlined}[t]
      \E[\tau \sim \altpi_\vvarphi]{G_0 \grad_\vvarphi \log \altpi_\vvarphi(\tau)} \\ - \E[\tau \sim \altpi_\vvarphi]{b \cdot \grad_\vvarphi \log \altpi_\vvarphi(\tau)}.
    \end{multlined}
  \end{align*}
  Thus, it remains to show that the second term is zero, \begin{align*}
    \E[\tau \sim \altpi_\vvarphi]{b \cdot \grad_\vvarphi \log \altpi_\vvarphi(\tau)} &= b \cdot \int \altpi_\vvarphi(\tau) \grad_\vvarphi \log \altpi_\vvarphi(\tau) \,d\tau \margintag{using the definition of expectation \eqref{eq:expectation}} \\
    &= b \cdot \int \altpi_\vvarphi(\tau) \frac{\grad_\vvarphi \altpi_\vvarphi(\tau)}{\altpi_\vvarphi(\tau)} \,d\tau \margintag{substituting the score function \eqref{eq:score_function}, ``undoing the score function trick''} \\
    &= b \cdot \int \grad_\vvarphi \altpi_\vvarphi(\tau) \,d\tau \margintag{$\altpi_\vvarphi(\tau)$ cancels} \\
    &= b \cdot \grad_\vvarphi \int \altpi_\vvarphi(\tau) \,d\tau \\
    &= b \cdot \grad_\vvarphi 1 = 0. \qedhere \margintag{integrating a PDF over its domain is $1$ and the derivative of a constant is $0$}
  \end{align*}
\end{proof}

One can even show, that we can subtract arbitrary baselines depending on \emph{previous} states \exerciserefmark{score_gradients_state_dep_baselines}.

\begin{ex}{Downstream returns}{downstream_returns}
  A commonly used state-dependent baseline is \begin{align}
    b(\tau_{0:t-1}) \defeq \sum_{m=0}^{t-1} \gamma^m r_m.
  \end{align}
  This baseline subtracts the returns of all actions before time $t$.
  Intuitively, using this baseline, the score gradient only considers downstream returns.
  Recall from \cref{eq:bounded_discounted_payoff} that we defined $G_{t:T}$ as the bounded discounted payoff from time $t$.
  It is also commonly called the (bounded) \midx{downstream return} (or \midx{reward to go}) beginning at time $t$.

  For a fixed trajectory $\tau$ that is bounded at time $T$, we have \begin{align}
    G_0 - b(\tau_{0:t-1}) = \gamma^t G_{t:T},
  \end{align} yielding the gradient estimator, \begin{align}
    \grad_\vvarphi \j{\vvarphi} \approx \grad_\vvarphi \j{\vvarphi}[T] &= \E[\tau \sim \altpi_\vvarphi]{G_0 \grad_\vvarphi \log \altpi_\vvarphi(\tau)} \margintag{using the score gradient estimator \eqref{eq:score_gradient_estimator}} \nonumber \\
    &= \E[\tau \sim \altpi_\vvarphi]{\sum_{t=0}^{T-1} \gamma^t G_{t:T} \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}. \margintag{using a state-dependent baseline \eqref{eq:score_gradient_estimator_state_dep_baseline}} \label{eq:score_gradient_estimator_downstream_returns}
  \end{align}
\end{ex}

Performing stochastic gradient descent with the score gradient estimator and downstream returns is known as the \emph{REINFORCE algorithm}~\citep{williams1992simple} which is shown in~\cref{alg:reinforce}.

\begin{algorithm}[H]
  \caption{REINFORCE algorithm}\label{alg:reinforce}\pidx{REINFORCE algorithm}
  initialize policy weights $\vvarphi$\;
  \Repeat{converged}{
    generate an episode (i.e., rollout) to obtain trajectory $\tau$\;
    \For{$t = 0$ \KwTo $T-1$}{
      set $g_{t:T}$ to the downstream return from time $t$\;
      $\vvarphi \gets \vvarphi + \eta \gamma^t g_{t:T} \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)$ \algeq{eq:reinforce}
    }
  }
\end{algorithm}

The variance of REINFORCE can be reduced further.
A common technique is to subtract a term $b_t$ from the downstream returns, \begin{align}
  \grad_\vvarphi \j{\vvarphi} = \E[\tau \sim \altpi_\vvarphi]{\sum_{t=0}^T \gamma^t (G_{t:T} - b_t) \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}.
\end{align}
For example, we can subtract the $t$-independent mean reward to go, \begin{align}
  b_t \defeq b = \frac{1}{T} \sum_{t'=0}^{T-1} G_{t':T}.
\end{align}

The main advantage of policy gradient methods such as REINFORCE is that they can be used in continuous action spaces.
However, REINFORCE is not guaranteed to find an optimal policy.
Even when operating in very small domains, REINFORCE can get stuck in local optima.\looseness=-1

Typically, policy gradient methods are slow due to the large variance in the score gradient estimates.
Because of this, they need to take small steps and require many rollouts of a Markov chain.
Moreover, we cannot reuse data from previous rollouts, as policy gradient methods are fundamentally on-policy.\footnote{This is because the score gradient estimator is used to obtain gradients of the policy value function with respect to the \emph{current} policy.}

Next, we will combine value approximation techniques like Q-learning and policy gradient methods, leading to an often more practical family of methods called actor-critic methods.


\section{On-policy Actor-Critics}\label{sec:mfarl:actor_critic_methods}

Actor-Critic methods reduce the variance of policy gradient estimates by using ideas from value function approximation.
They use function approximation both to approximate value functions \emph{and} to approximate policies.
The goal for these algorithms is to scale to reinforcement learning problems, where we both have large state spaces and large action spaces.

\subsection{Advantage Function}

A key concept of actor-critic methods is the advantage function.

\begin{defn}[Advantage function]\pidx{advantage function}
  Given a policy $\pi$, the \emph{advantage function},\looseness=-1 \begin{align}
    \a[\pi]{\vx}{\va} &\defeq \q[\pi]{\vx}{\va} - \v[\pi]{\vx} \label{eq:advantage_function1} \\
    &= \q[\pi]{\vx}{\va} - \E[\vap \sim \pi(\vx)]{\q[\pi]{\vx}{\vap}}, \label{eq:advantage_function2} \margintag{using \cref{eq:v_as_q}}
  \end{align} measures the advantage of picking action $\va \in \spA$ when in state $\vx \in \spX$ over simply following policy $\pi$.
\end{defn}

It follows immediately from \cref{eq:advantage_function2} that for any policy $\pi$ and state~${\vx \in \spX}$, there exists an action $\va \in \spA$ such that $\a[\pi]{\vx}{\va}$ is non-negative, \begin{align}
  \max_{\va \in \spA} \a[\pi]{\vx}{\va} \geq 0.
\end{align}
Moreover, it follows directly from Bellman's theorem \eqref{eq:bop1} that \begin{align}
  \text{$\pi$ is optimal} \iff \forall \vx \in \spX, \va \in \spA : \a[\pi]{\vx}{\va} \leq 0.
\end{align}
In other words, quite intuitively, $\pi$ is optimal if and only if there is no action that has an advantage in any state over the action that is played by~$\pi$.\looseness=-1

Finally, we can re-define the greedy policy $\pi_{\fnq}$ with respect to the state-action value function $\fnq$ as \begin{align}
  \pi_{\fnq}(\vx) \defeq \argmax_{\va \in \spA} \a{\vx}{\va}
\end{align} since \begin{align*}
  \argmax_{\va \in \spA} \a{\vx}{\va} = \argmax_{\va \in \spA} \q{\vx}{\va} - \v{\vx} = \argmax_{\va \in \spA} \q{\vx}{\va},
\end{align*} as $\v{\vx}$ is independent of $\va$.
This coincides with our initial definition of greedy policies in \cref{eq:greedy_policy1}.
Intuitively, the advantage function is a shifted version of the state-action value function $\fnq$ that is relative to~$0$.
Using this quantity rather than $\fnq$ often has numerical advantages.

\subsection{Policy Gradient Theorem}\label{sec:mfarl:actor_critic_methods:policy_gradients}

Recall the score gradient estimator \eqref{eq:score_gradient_estimator_downstream_returns} that we had introduced in the previous section, \begin{align*}
  \grad_\vvarphi \j{\vvarphi}[T] = \E[\tau \sim \altpi_\vvarphi]{\sum_{t=0}^{T-1} \gamma^t G_{t:T} \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}.
\end{align*}
Previously, we have approximated the policy value function $\j{\vvarphi}$ by the bounded policy value function $\j{\vvarphi}[T]$.
We said that this approximation was ``reasonable'' due to the diminishing returns.
Essentially, we have ``cut off the tails'' of the policy value function.
Let us now reinterpret score gradients while taking into account the tails of $\j{\vvarphi}$. \begin{align}
  \grad_\vvarphi \j{\vvarphi} &= \lim_{T\to\infty} \grad_\vvarphi \j{\vvarphi}[T] \\
  &= \sum_{t=0}^\infty \E[\tau \sim \altpi_\vvarphi]{\gamma^t G_t \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}. \margintag{substituting the score gradient estimator with downstream returns \eqref{eq:score_gradient_estimator_downstream_returns} and using linearity of expectation \eqref{eq:linearity_expectation}} \nonumber
  \intertext{Observe that because the expectations only consider downstream returns, we can disregard all data from the trajectory prior to time $t$. Let us define \begin{align}
    \tau_{t:\infty} \defeq ((\vx_t, \va_t, r_t, \vx_{t+1}), (\vx_{t+1}, \va_{t+1}, r_{t+1}, \vx_{t+2}), \dots),
  \end{align} as the trajectory from time step $t$. Then,}
  &= \sum_{t=0}^\infty \E[\tau_{t:\infty} \sim \altpi_\vvarphi]{\gamma^t G_t \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}. \nonumber
  \intertext{We now condition on $\vx_t$ and $\va_t$,}
  &= \sum_{t=0}^\infty \E[\vx_t, \va_t]{\gamma^t \E[r_t, \tau_{t+1:\infty}]{G_t}[\vx_t, \va_t] \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}. \margintag{using that $\pi_\vvarphi(a_t \mid x_t)$ is a constant given $x_t$ and $a_t$} \nonumber
  \intertext{Observe that averaging over the trajectories $\E[\tau \sim \altpi_\vvarphi]{\cdot}$ that are sampled according to policy $\pi_\vvarphi$ is equivalent to our shorthand notation $\E[\pi_\vvarphi]{\cdot}$ from \cref{eq:expectation_over_policy},}
  &= \sum_{t=0}^\infty \E[\vx_t, \va_t]{\gamma^t \E[\pi_\vvarphi]{G_t}[\vx_t, \va_t] \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)} \nonumber \\
  &= \sum_{t=0}^\infty \E[\vx_t, \va_t]{\gamma^t \q[\pi_\vvarphi]{\vx_t}{\va_t} \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}. \margintag{using the definition of the Q-function \eqref{eq:q_function1}} \label{eq:policy_gradient_thm_proof}
\end{align}
It turns out that $\E[\vx_t, \va_t]{\q[\pi_\vvarphi]{\vx_t}{\va_t}}$ exhibits much less variance than our previous estimator $\E*[\vx_t, \va_t]{\E[\pi_\vvarphi]{G_t}[\vx_t, \va_t]}$.
\Cref{eq:policy_gradient_thm_proof} is known as the \midx{policy gradient theorem}.

Often, the policy gradient theorem is stated in a slightly rephrased form in terms of the \midx{discounted state occupancy measure},\footnote{Depending on the reward setting, there exist various variations of the policy gradient theorem. We derived the variant for infinite-horizon discounted payoffs. \icite{sutton2018reinforcement} derive the variant for undiscounted average rewards.} \begin{align}
  \rho_\vvarphi^\infty(\vx) \defeq (1-\gamma) \sum_{t=0}^\infty \gamma^t p_{\rX_t}\!(\vx).
\end{align}
The factor $(1-\gamma)$ ensures that $\rho_\vvarphi^\infty$ is a probability density as \begin{align*}
  \int \rho_\vvarphi^\infty(\vx) \,d\vx = (1-\gamma) \sum_{t=0}^\infty \gamma^t \int p_{\rX_t}\!(\vx) \,d\vx = (1-\gamma) \sum_{t=0}^\infty \gamma^t = 1.
\end{align*}
Intuitively, $\rho_\vvarphi^\infty(\vx)$ measures how often we visit state $\vx$ when following policy $\pi_\vvarphi$.
It can be thought of as a ``discounted frequency''.

\begin{thm}[Policy gradient theorem in terms of $\rho_\vvarphi^\infty$]
  Policy gradients can be represented in terms of the Q-function, \begin{align}
    \grad_\vvarphi \j{\vvarphi} &\propto \E*[\vx \sim \rho_\vvarphi^\infty]{\E[\va \sim \pi_\vvarphi(\cdot \mid \vx)]{\q[\pi_\vvarphi]{\vx}{\va} \grad_\vvarphi \log \pi_\vvarphi(\va \mid \vx)}}. \label{eq:policy_gradient_thm}
  \end{align}
\end{thm}
\begin{proof}
  The right hand side of \cref{eq:policy_gradient_thm_proof} can be expressed as \begin{align*}
    &\sum_{t=0}^\infty \int p_{\rX_t}\!(\vx) \cdot \E[\va_t \sim \pi_\vvarphi(\cdot \mid \vx)]{\gamma^t \q[\pi_\vvarphi]{\vx}{\va_t} \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx)} \,d\vx \\
    &= \frac{1}{1-\gamma} \int \rho_\vvarphi^\infty(\vx) \cdot \E[\va \sim \pi_\vvarphi(\cdot \mid \vx)]{\q[\pi_\vvarphi]{\vx}{\va} \grad_\vvarphi \log \pi_\vvarphi(\va \mid \vx)} \,d\vx
  \end{align*} where we swapped the order of sum and integral and reorganized terms.\looseness=-1
\end{proof}

Matching our intuition, according to the policy gradient theorem, maximizing $\j{\vvarphi}$ corresponds to increasing the probability of actions with a large value and decreasing the probability of actions with a small value, taking into account how often the resulting policy visits certain states.\looseness=-1

Observe that we cannot use the policy gradient to calculate the gradient exactly, as we do not know $\fnq[\pi_\vvarphi]$.
Instead, we will use bootstrapping estimates $\fnQ[\pi_\vvarphi]$ of $\fnq[\pi_\vvarphi]$.

\subsection{A First Actor-Critic}

\begin{marginfigure}
  \incfig{actor_critic}
  \caption{Illustration of one iteration of actor-critic methods.
  The dependencies between the actors and critics are shown as arrows.
  Methods differ in the exact order in which actor and critic are updated.}
\end{marginfigure}

Actor-Critic methods\pidx{actor-critic method} consist of two components: \begin{itemize}
  \item a parameterized policy, $\pi(\va \mid \vx; \vvarphi) \eqdef \pi_\vvarphi$, which is called \embeq{eq:actor} \midx{actor}; and
  \item a value function approximation, $\q[\pi_\vvarphi]{\vx}{\va} \approx \Q[\pi_\vvarphi]{\vx}{\va; \vtheta}$, which is called \midx{critic}.
  In the following, we will abbreviate $\fnQ[\pi_\vvarphi]$ by $\fnQ$. \embeq{eq:critic}
\end{itemize}
In deep reinforcement learning, neural networks are used to parameterize both actor and critic.
Therefore, in principle, the actor-critic framework allows scaling to both large state spaces and large action spaces.
We begin by discussing on-policy actor-critics.

One approach in the online setting (i.e., non-episodic setting), is to simply use SARSA for learning the critic.
To learn the actor, we use stochastic gradient descent with gradients obtained using single samples from \begin{align}
  \grad_\vvarphi \j{\vvarphi} \approx \grad_\vvarphi \J{\vvarphi} \defeq \sum_{t=0} ^\infty\E[(\vx_t,\va_t) \sim \pi_\vvarphi]{\gamma^t \Q{\vx_t}{\va_t; \vtheta} \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)} \label{eq:policy_gradient_bootstrapping_estimate} \margintag{see \cref{eq:policy_gradient_thm_proof}}
\end{align} where $\fnQ$ is a bootstrapping estimate of $\fnq[\pi_\vvarphi]$.
This algorithm is known as \emph{online actor-critic} or \midx{Q actor-critic} and shown in \cref{alg:oac}.

\begin{algorithm}
  \caption{Online actor-critic}\pidx{online actor-critic}\label{alg:oac}
  initialize parameters $\vvarphi$ and $\vtheta$\;
  \Repeat{converged}{
    use $\pi_\vvarphi$ to obtain transition $(\vx, \va, r, \vxp)$\;
    $\delta = r + \gamma \Q{\vxp}{\pi_\vvarphi(\vxp); \vtheta} - \Q{\vx}{\va; \vtheta}$\;
    \Comment{actor update}
    $\vvarphi \gets \vvarphi + \eta \gamma^t \Q{\vx}{\va; \vtheta} \grad_{\vvarphi} \log \pi_\vvarphi(\va \mid \vx)$ \algeq{eq:online_actor_critic1}
    \Comment{critic update}
    $\vtheta \gets \vtheta + \eta \delta \grad_{\vtheta} \Q{\vx}{\va; \vtheta}$ \algeq{eq:online_actor_critic2}
  }
\end{algorithm}

Comparing to the derivation for TD-learning from \cref{eq:td_learning_gradient_update}, we observe that \cref{eq:online_actor_critic2} corresponds to the SARSA update rule.\footnote{The gradient with respect to $\vtheta_Q$ appears analogously to our derivation of approximate Q-learning \eqref{eq:q_learning_gradient_update}.}
Due to the use of SARSA for learning the critic, this algorithm is fundamentally on-policy.

Crucially, by neglecting the dependence of the bootstrapping estimate $\fnQ$ on the policy parameters $\vvarphi$, we introduce bias in the gradient estimates.
In other words, using the bootstrapping estimate $\fnQ$ means that the resulting gradient direction might not be a valid ascent direction.
In particular, the actor is not guaranteed to improve.
Still, it turns out that under strong so-called ``compatibility conditions'' that are rarely satisfied in practice, a valid ascent direction can be guaranteed.

\subsection{Improved Actor-Critics}\label{sec:mfarl:actor_critc_methods:improved}

\paragraph{Reducing variance:}

To further reduce the variance of the gradient estimates, it turns out that a similar approach to the baselines we discussed in the previous section on policy gradient methods is useful.
A common approach is to subtract the state value function from estimates of the Q-function, \begin{align}
  \vvarphi &\gets \vvarphi + \eta_t \gamma^t (\Q{\vx}{\va; \vtheta} - \V{\vx; \vtheta}) \grad_{\vvarphi} \log \pi_\vvarphi(\va \mid \vx) \\
  &= \vvarphi + \eta_t \gamma^t \A{\vx}{\va; \vtheta} \grad_{\vvarphi} \log \pi_\vvarphi(\va \mid \vx) \margintag{using the definition of the advantage function \eqref{eq:advantage_function1}}
\end{align} where $\A{\vx}{\va; \vtheta}$ is a bootstrapped estimate of the advantage function~$\fna[\pi_\vvarphi]$.
This algorithm is known as \midx{advantage actor-critic} (A2C) \citep{mnih2016asynchronous}.
Recall that the Q-function is an absolute quantity, whereas the advantage function is a relative quantity, where the sign is informative for the gradient direction.
Intuitively, an absolute value is harder to estimate than the sign.
Actor-Critic methods are therefore often implemented with respect to the advantage function rather than the Q-function.\looseness=-1

Taking a step back, observe that policy gradient methods such as REINFORCE generally have \emph{high variance} in their gradient estimates.
However, due to using Monte Carlo estimates of $G_t$, the gradient estimates are \emph{unbiased}.
In contrast, using a bootstrapped Q-function to obtain gradient estimates yields estimates with a \emph{smaller variance}, but those estimates are \emph{biased}.
We are faced with a \midx{bias-variance tradeoff}.
A natural approach is therefore to blend both gradient estimates to allow for effectively trading bias and variance.
This leads to algorithms such as \midx{generalized advantage estimation} (GAE) \citep{schulman2015high}.

\paragraph{Exploration:}

Similarly to REINFORCE, actor-critic methods typically rely on randomization in the policy to encourage exploration, the idea being that if the policy is stochastic, then the agent will visit a diverse set of states.
The inherent stochasticity of the policy is, however, often insufficient.
A common problem is that the policy quickly ``collapses'' to a deterministic policy since the objective function is greedily exploitative.
A common workaround is to use an $\varepsilon$-greedy policy (cf. \cref{sec:tabular_rl:mb:epsilon_greedy}) or to explicitly encourage the policy to exhibit uncertainty by adding an entropy term to the objective function (more on this in \cref{sec:mfarl:actor_critic_methods:entropy_regularization}).
However, note that for on-policy methods, changing the policy also changes the value function learned by the critic.

\paragraph{Improving sample efficiency:}

Actor-Critic methods typically suffer from low sample efficiency.
When additionally using an on-policy method, actor-critics often need an extremely large number of interactions before learning a near-optimal policy, because they cannot reuse past data.
Allowing to reuse past data is a major advantage of off-policy methods like Q-learning.

One well-known variant that slightly improves the sample efficiency is \midx{trust-region policy optimization} (TRPO) \citep{schulman2015trust}.
TRPO uses multiple iterations, where in each iteration a fixed critic is used to optimize the policy.\footnote{Intuitively, each iteration performs a collection of gradient ascent steps.}
During iteration $k$, we select \begin{align}
  \vvarphi_{k+1} \gets \argmax_{\vvarphi} \J{\vvarphi} \quad \text{subject to}\ \E*[\vx \sim \rho_{\vvarphi_k}^\infty]{\KL{\pi_{\vvarphi_k}(\cdot \mid \vx)}{\pi_\vvarphi(\cdot \mid \vx)}} \leq \delta \label{eq:trpo} \\[-35pt] \nonumber
\end{align} for some fixed $\delta > 0$ and where \begin{align}
  \J{\vvarphi} \defeq \E[\vx \sim \rho_{\vvarphi_k}^\infty, \va \sim \pi_{\vvarphi_k}(\cdot \mid \vx)]{w_k(\vvarphi; \vx, \va) \A[\pi_{\vvarphi_k}]{\vx}{\va}}.
\end{align}
Notably, $\fnJ$ is an expectation with respect to the \emph{previous} policy $\pi_{\vvarphi_k}$ and the previous critic $\fnA[\pi_{\vvarphi_k}]$.
TRPO uses \midx{importance sampling} where the importance weights (called ``likelihood ratios''), \begin{align*}
  w_k(\vvarphi; \vx, \va) \defeq \frac{\pi_\vvarphi(\va \mid \vx)}{\pi_{\vvarphi_k}(\va \mid \vx)},
\end{align*} are used to correct for taking the expectation over the previous policy.
When $w_k \approx 1$ the policies $\pi_{\vvarphi}$ and $\pi_{\vvarphi_k}$ are similar, whereas when $w_k~\ll~1$ or $w_k~\gg~1$, the policies differ significantly.
To be able to assume that the fixed critic is a good approximation within a certain ``trust region'' (i.e., one iteration), we impose the constraint \begin{align*}
  \E*[\vx \sim \rho_{\vvarphi_k}^\infty]{\KL{\pi_{\vvarphi_k}(\cdot \mid \vx)}{\pi_\vvarphi(\cdot \mid \vx)}} \leq \delta
\end{align*} to optimize only in the ``neighborhood'' of the current policy.
This constraint is also necessary for the importance weights not to blow up.

\begin{rmk}{Estimating the KL-divergence}{}
  Instead of naive computation with $\E[\va \sim \pi_{\vvarphi_k}(\cdot \mid \vx)]{- \log w_k(\vvarphi; \vx, \va)}$, the KL-divergence is commonly estimated by Monte Carlo samples of \begin{align*}
    \begin{multlined}[t]
      \KL{\pi_{\vvarphi_k}(\cdot \mid \vx)}{\pi_\vvarphi(\cdot \mid \vx)} \\ = \E[\va \sim \pi_{\vvarphi_k}(\cdot \mid \vx)]{w_k(\vvarphi; \vx, \va) - 1 - \log w_k(\vvarphi; \vx, \va)},
    \end{multlined}
  \end{align*} which adds the ``baseline'' $w_k(\vvarphi; \vx, \va) - 1$ with mean $0$.
  Observe that this estimator is unbiased, always non-negative since $\log(x) \leq x - 1$ for all $x$, while having a lower variance than the naive estimator.
\end{rmk}

Taking the expectation with respect to the previous policy $\pi_{\vvarphi_k}$ means that we can reuse data from rollouts within the same iteration.
That is, TRPO allows reusing past data as long as it can still be ``trusted''.
This makes TRPO ``somewhat'' off-policy.
Fundamentally, though, TRPO is still an on-policy method.

\midx<Proximal policy optimization>{proximal policy optimization} (PPO) is a family of heuristic variants of TRPO which replace the constrained optimization problem of \cref{eq:trpo} by the unconstrained optimization of a regularized objective \citep{schulman2017proximal,wang2020truly}.
PPO algorithms often work well in practice.
One canonical PPO method uses the modified objective \begin{align}
  \vvarphi_{k+1} \gets \argmax_{\vvarphi} \J{\vvarphi} - \lambda \E*[\vx \sim \rho_{\vvarphi_k}^\infty]{\KL{\pi_{\vvarphi_k}(\cdot \mid \vx)}{\pi_\vvarphi(\cdot \mid \vx)}} \label{eq:ppo}
\end{align} with some $\lambda > 0$, which regularizes towards the trust region.
Another common variant of PPO is based on controlling the importance weights directly rather than regularizing by the KL-divergence.
PPO is used, for example, to train large-scale language models such as GPT \citep{stiennon2020learning,openai2023gpt4} which we will discuss in more detail in \cref{sec:mfarl:preference_learning}.
There we will also see that the objective from \cref{eq:ppo} can be cast as performing probabilistic inference.

\paragraph{Improving computational efficiency:}
A practical problem with the above methods is that the estimation of the advantage function $\fnA(\vx, \va; \vtheta)$ requires training a separate critic, next to the policy (i.e., the actor) parameterized by $\vvarphi$.
This can be computationally expensive.
In particular, when both models are large neural networks (think multiple billions of parameters each), training both models is computationally prohibitive.
Now, recall that we introduced critics in the first place to reduce the variance of the policy gradient estimates.\footnote{That is, we moved from the REINFORCE policy update~\eqref{eq:reinforce} to the actor-critic policy update~\eqref{eq:online_actor_critic1}.}
\midx<Group relative policy optimization>{group relative policy optimization} (GRPO) replaces the critic in PPO with simple Monte Carlo estimates of the advantage function~\citep{shao2024deepseekmath}: \begin{align}
  \begin{multlined}[t]
    \J{\vvarphi} \defeq \E[\{\tau^{(i)}\}_{i=1}^m \sim \altpi_{\vvarphi_k}(\cdot \mid \vx)]{\frac{1}{m} \sum_{i=1}^m \sum_{t=1}^T w_k(\vvarphi; \va^{(i)}) \fnAhat_{t,i}^{\pi_{\vvarphi_k}}}, \\
    \text{where}\quad \fnAhat_{t,i}^{\pi_{\vvarphi_k}} \defeq \text{\small$\frac{g_{t:T}^{(i)} - \mathrm{mean}(\{\tau^{(i)}\})}{\mathrm{std}(\{\tau^{(i)}\})}$}
  \end{multlined}
\end{align} estimates the advantage of action $\va^{(i)}$ at time $t$ by comparing to the mean reward and normalizing by the standard deviation of rewards from all trajectories $\tau^{(i)}$.\footnote{In \cref{eq:score_gradient_estimator_approx}, we have already seen that using a Monte Carlo estimate of returns is a simple approach to reduce variance without needing to learn a critic.}
GRPO combines Monte Carlo sampling and baselines for variance reduction with the trust-region optimization of PPO, leading to a method that is more sample efficient than naive REINFORCE while being computationally more efficient than PPO.

\section{Off-policy Actor-Critics}\label{sec:mfarl:off_policy_actor_critic_methods}

In many applications, sample efficiency is crucial.
Either because requiring too many interactions is computationally prohibitive or because obtaining sufficiently many samples for learning a near-optimal policy is simply impossible.
We therefore now want to look at a separate family of actor-critic methods, which are off-policy, and hence, allow for the reuse of past data. These algorithms use the reparameterization gradient estimates, which we encountered before in the context of variational inference,\footnote{see \cref{sec:approximate_inference:variational_inference:gradient_of_elbo}} instead of score gradient estimators.

The on-policy methods that we discussed in the previous section can be understood as performing a variant of \midx{policy iteration}, where we use an estimate of the state-action value function of the current policy and then try to improve that policy by acting greedily with respect to this estimate.
They mostly vary in how improving the policy is traded with improving the estimate of its value.
Fundamentally, these methods rely on policy evaluation.\footnote{Policy evaluation is at the core of policy iteration. See \cref{alg:policy_iteration} for the definition of policy iteration and \cref{sec:mdp:policy_evaluation} for a summary of policy evaluation in the context of Markov decision processes.}

The techniques that we will introduce in this section are much more closely related to value iteration, essentially making use of Bellman's optimality principle to learn the optimal value function directly which characterizes the optimal policy.

To begin with, let us assume that the policy $\vpi$ is deterministic.
We will later lift this restriction in \cref{sec:mfarl:actor_critic_methods:randomized_policies}.
Recall that our initial motivation to consider policy gradient methods and then actor-critic methods was the intractability of the DQN loss \begin{align*}
  \ell_{\mathrm{DQN}}(\vtheta; \spD) = \frac{1}{2} \sum_{(\vx, \va, r, \vxp) \in \spD} \parentheses*{r + \gamma \max_{\vap \in \spA} \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}}^2 \margintag{see \cref{eq:dqn}}
\end{align*} when the action space $\spA$ is large.
What if we simply replace the exact maximum over actions by a parameterized policy? \begin{align}
  \ell_{\mathrm{DQN}}(\vtheta; \spD) \approx \frac{1}{2} \sum_{(\vx, \va, r, \vxp) \in \spD} \parentheses*{r + \gamma \Q*{\vxp}{\vpi_{\vvarphi}(\vxp); \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}}^2.
\end{align}
We want to train our parameterized policy to learn the maximization over actions, that is, to approximate the greedy policy\footnote{Here, we already apply the improvement of DDQN to use the most-recent estimate of the Q-function for action selection (see \cref{eq:ddqn}).} \begin{align}
  \vpi_{\vvarphi}(\vx) \approx \opt{\vpi}(\vx) = \argmax_{\va \in \spA} \Q*{\vx}{\va; \vtheta}.
\end{align}
The key idea is that if we use a ``rich-enough'' parameterization of policies, selecting the greedy policy with respect to $\opt{\fnQ}$ is equivalent to \begin{align}
  \opt{\vvarphi} &= \argmax_{\vvarphi} \E[\vx \sim \mu]{\Q*{\vx}{\vpi_\vvarphi(\vx); \vtheta}} \label{eq:off_policy_actor_critics_optimization}
\end{align} where $\mu(\vx) > 0$ is an \midx{exploration distribution} over states with full support.\safefootnote{We require full support to ensure that all states are explored.}
We refer to this expectation by \begin{align}
  \J{\vvarphi; \vtheta}[\mu] \defeq \E[\vx \sim \mu]{\Q*{\vx}{\vpi_\vvarphi(\vx); \vtheta}}.
\end{align}
Commonly, the exploration distribution $\mu$ is taken to be the distribution that samples states uniformly from a replay buffer.
Note that we can easily obtain unbiased gradient estimates of $\fnJ[\mu]$ with respect to $\vvarphi$: \begin{align}
  \grad_{\vvarphi} \J{\vvarphi; \vtheta}[\mu] &= \E[\vx \sim \mu]{\grad_{\vvarphi} \Q*{\vx}{\vpi_\vvarphi(\vx); \vtheta}} \margintag{see \cref{sec:background:probability:gradients_of_expectations}}.
\end{align}

Analogously to on-policy actor-critics (see \cref{eq:policy_gradient_bootstrapping_estimate}), we use a bootstrapping estimate of $\opt{\fnQ}$.
That is, we neglect the dependence of the critic $\opt{\fnQ}$ on the actor $\vpi_\vvarphi$, and in particular, the policy parameters $\vvarphi$.
We have seen that bootstrapping works with Q-learning, so there is reason to hope that it will work in this context too.
This then allows us to use the chain rule to compute the gradient, \begin{align}
  \grad_{\vvarphi} \Q*{\vx}{\vpi_\vvarphi(\vx); \vtheta} = \jac_{\vvarphi} \vpi_\vvarphi(\vx) \cdot \grad_\va \Q*{\vx}{\va; \vtheta}\big\rvert_{\va=\vpi_\vvarphi(\vx)}. \label{eq:off_policy_acator_critic_actor_gradient}
\end{align}
This corresponds to evaluating the bootstrapping estimate of the Q-function at $\vpi_\vvarphi(\vx)$ and obtaining a gradient estimate of the policy estimate (e.g., through automatic differentiation).
Note that as $\vpi_\vvarphi$ is vector-valued, $\jac_{\vvarphi} \vpi_\vvarphi(\vx)$ is the Jacobian of $\vpi_\vvarphi$ evaluated at $\vx$.

\paragraph{Exploration:}

Now that we have estimates of the gradient of our optimization target $\fnJ[\mu]$, it is natural to ask how we should select actions (based on $\vpi_\vvarphi$) to trade exploration and exploitation.
As we have seen, policy gradient techniques rely on the randomness in the policy to explore, but here we consider deterministic policies.
As our method is off-policy, a simple idea in continuous action spaces is to add Gaussian noise to the action selected by $\vpi_\vvarphi$ --- also known as \midx{Gaussian noise ``dithering''}.\footnote{Intuitively, this adds ``additional randomness'' to the policy $\vpi_\vvarphi$.}
This corresponds to an algorithm called \emph{deep deterministic policy gradients} \citep{lillicrap2015continuous} shown in \cref{alg:ddpg}.
This algorithm is essentially equivalent to Q-learning with function approximation (e.g., DQN),\footnote{see \cref{eq:dqn}} with the only exception that we replace the maximization over actions with the learned policy $\vpi_\vvarphi$.

\begin{algorithm}
  \caption{Deep deterministic policy gradients, DDPG}\pidx{deep deterministic policy gradients}[idxpagebf]\label{alg:ddpg}
  initialize $\vvarphi, \vtheta$, a (possibly non-empty) replay buffer $\spD = \emptyset$\;
  set $\old{\vvarphi} = \vvarphi$ and $\old{\vtheta} = \vtheta$\;
  \For{$t=0$ \KwTo $\infty$}{
    observe state $\vx$, pick action $\va = \vpi_\vvarphi(\vx) + \vvarepsilon$ for $\vvarepsilon \sim \N{\vzero, \lambda \mI}$\;
    execute $\va$, observe $r$ and $\vxp$\;
    add $(\vx, \va, r, \vxp)$ to the replay buffer $\spD$\;
    \If{collected ``enough'' data}{
      \Comment{policy improvement step}
      \For{some iterations}{
        sample a mini-batch $\sB$ of $\spD$\;
        for each transition in $\sB$, compute the label $y = r + \gamma \Q*{\vxp}{\vpi(\vxp; \old{\vvarphi}); \old{\vtheta}}$\;
        \Comment{critic update}
        $\vtheta \gets \vtheta - \eta \grad_{\vtheta} \frac{1}{B} \sum_{(\vx, \va, r, \vxp, y) \in B} (y - \Q*{\vx}{\va; \vtheta})^2$\;
        \Comment{actor update}
        $\vvarphi \gets \vvarphi + \eta \grad_{\vvarphi} \frac{1}{B} \sum_{(\vx, \va, r, \vxp, y) \in B} \Q*{\vx}{\vpi(\vx; \vvarphi); \vtheta}$\;
        $\old{\vtheta} \gets (1 - \rho)\old{\vtheta} + \rho\vtheta$\;
        $\old{\vvarphi} \gets (1 - \rho)\old{\vvarphi} + \rho\vvarphi$\;
      }
    }
  }
\end{algorithm}

\midx<Twin delayed DDPG>{twin delayed DDPG} (TD3) is an extension of DDPG that uses two separate critic networks for predicting the maximum action and evaluating the policy \citep{fujimoto2018addressing}.
This addresses the maximization bias akin to Double-DQN. TD3 also applies delayed updates to the actor network, which increases stability.

\subsection{Randomized Policies}\label{sec:mfarl:actor_critic_methods:randomized_policies}

We have seen that randomized policies naturally encourage exploration.
With deterministic actor-critic methods like DDPG, we had to inject Gaussian noise to enforce sufficient exploration.
A natural question is therefore whether we can also handle randomized policies in this framework of off-policy actor-critics.

The key idea is to replace the squared loss of the critic, \begin{align*}
  \ell_{\mathrm{DQN}}(\vtheta; \spD) \approx \frac{1}{2} \parentheses*{r + \gamma \Q*{\vxp}{\vpi(\vxp; \vvarphi); \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}}^2, \margintag{refer to the squared loss of Q-learning \eqref{eq:q_loss}}
\end{align*} which only considers the fixed action $\vpi(\vxp; \old{\vvarphi})$ with an expected squared loss, \begin{align}
  \ell_{\mathrm{DQN}}(\vtheta; \spD) \approx \E[\vap \sim \pi(\vxp; \vvarphi)]{\frac{1}{2} \parentheses*{r + \gamma \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va;\vtheta}}^2},
\end{align} which considers a distribution over actions.

It turns out that we can still compute gradients of this expectation.
\begin{align}
  &\grad_{\vtheta} \E[\vap \sim \pi(\vxp; \vvarphi)]{\frac{1}{2} \parentheses*{r + \gamma \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va;\vtheta}}^2} \nonumber \\
  &= \E[\vap \sim \pi(\vxp; \vvarphi)]{\grad_{\vtheta} \frac{1}{2} \parentheses*{r + \gamma \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va;\vtheta}}^2}. \margintag{see \cref{sec:background:probability:gradients_of_expectations}} \nonumber
  \intertext{Similarly to our definition of the Bellman error \eqref{eq:bellman_error}, we define by \begin{align}
    \delta_\mathrm{B}(\vap) \defeq r + \gamma \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va;\vtheta},
  \end{align} the \midx{Bellman error} for a fixed action $\vap$. Using the chain rule, we obtain}
  &= \E[\vap \sim \pi(\vxp; \vvarphi)]{\delta_\mathrm{B}(\vap) \grad_{\vtheta} \Q*{\vx}{\va;\vtheta}}. \label{eq:svg_critic}
\end{align}
Note that this is identical to the gradient in DQN \eqref{eq:q_learning_gradient_update_general}, except that now we have an expectation over actions.
As we have done many times already, we can use automatic differentiation to obtain gradient estimates of $\grad_{\vtheta} \Q*{\vx}{\va;\vtheta}$.
This provides us with a method of obtaining unbiased gradient estimates for the critic.

We also need to reconsider the actor update.
When using a randomized policy, the objective function changes to \begin{align*}
  \J{\vvarphi; \vtheta}[\mu] \defeq \E*[\vx \sim \mu]{\E[\va \sim \pi(\vx; \vvarphi)]{\Q*{\vx}{\va; \vtheta}}}.
\end{align*} of which we can obtain gradients via \begin{align}
  \grad_{\vvarphi} \J{\vvarphi; \vtheta}[\mu] = \E*[\vx \sim \mu]{\grad_{\vvarphi} \E[\va \sim \pi(\vx; \vvarphi)]{\Q*{\vx}{\va; \vtheta}}}. \margintag{see \cref{sec:background:probability:gradients_of_expectations}}
\end{align}
Note that the inner expectation is with respect to a measure that depends on the parameters $\vvarphi$, which we are trying to optimize.
We therefore cannot move the gradient operator inside the expectation.
This is a problem that we have already encountered several times.
In the previous section on policy gradients, we used the score gradient estimator.\footnote{see \cref{eq:score_gradient_estimator}}
Earlier, in \cref{sec:approximate_inference} on variational inference we have already seen reparameterization gradients.\footnote{see \eqref{eq:reparameterization_trick2}}
Here, if our policy is reparameterizable, we can use the \midx{reparameterization trick} from \cref{thm:reparameterization_trick}!

\begin{ex}{Reparameterization gradients for Gaussians}{reparameterization_gradients_gaussian}
  Suppose we use a Gaussian parameterization of policies, \begin{align*}
    \pi(\vx; \vvarphi) \defeq \N{\vmu(\vx; \vvarphi)}{\mSigma(\vx; \vvarphi)}.
  \end{align*}
  Then, using conditional linear Gaussians, our action $\va$ is given by \begin{align}
    \va = \vg(\vvarepsilon; \vx, \vvarphi) \defeq \msqrt{\mSigma}(\vx; \vvarphi) \vvarepsilon + \vmu(\vx; \vvarphi), \quad \vvarepsilon \sim \SN
  \end{align} where $\msqrt{\mSigma}(\vx; \vvarphi)$ is the square root of $\mSigma(\vx; \vvarphi)$.
  This coincides with our earlier application of the reparameterization trick to Gaussians in \cref{ex:reparameterization_trick_gaussian}.
\end{ex}

As we have seen, not only Gaussians are reparameterizable.
In general, we called a distribution (in this context, a policy) reparameterizable iff $\va \sim \pi(\vx; \vvarphi)$ is such that $\va = \vg(\vvarepsilon; \vx, \vvarphi)$, where $\vvarepsilon \sim \phi$ is an independent random variable.

Then, we have, \begin{align}
  &\grad_{\vvarphi} \E[\va \sim \pi(\vx; \vvarphi)]{\Q*{\vx}{\va; \vtheta}} \nonumber \\
  &= \E[\vvarepsilon \sim \phi]{\grad_{\vvarphi} \Q*{\vx}{\vg(\vvarepsilon; \vx, \vvarphi); \vtheta}} \label{eq:reparameterization_gradients} \margintag{using the reparameterization trick \eqref{eq:reparameterization_trick2}} \\
  &= \E[\vvarepsilon \sim \phi]{\grad_\va \Q*{\vx}{\va; \vtheta}\big\rvert_{\va=\vg(\vvarepsilon; \vx, \vvarphi)} \cdot \jac_{\vvarphi} \vg(\vvarepsilon; \vx, \vvarphi)}. \margintag{using the chain rule analogously to \cref{eq:off_policy_acator_critic_actor_gradient}}
\end{align}
In this way, we can obtain unbiased gradient estimates for reparameterizable policies.
This general technique does not only apply to continuous action spaces.
For discrete action spaces, there is the analogous so-called \midx{Gumbel-max trick}, which we will not discuss in greater detail here.\looseness=-1

The algorithm that uses \cref{eq:svg_critic} to obtain gradients for the critic and reparameterization gradients for the actor is called \midx{stochastic value gradients}[idxpagebf] (SVG) \citep{heess2015learning}.


\section{Maximum Entropy Reinforcement Learning}\label{sec:mfarl:actor_critic_methods:entropy_regularization}

In practice, algorithms like SVG often do not explore enough.
A key issue with relying on randomized policies for exploration is that they might collapse to deterministic policies.
That is, the algorithm might quickly reach a local optimum, where all mass is placed on a single action.\looseness=-1

A simple trick that encourages a little bit of extra exploration is to regularize the randomized policies ``away'' from putting all mass on a single action.
In other words, we want to encourage the policies to exhibit some uncertainty.
A natural measure of uncertainty is entropy, which we have already seen several times.\footnote{see \cref{sec:approximate_inference:information_theory}}
This approach is known as \midx{entropy regularization}[idxpagebf] or \midx{maximum entropy reinforcement learning} (MERL).
Canonically, entropy regularization is applied to finite-horizon rewards (cf. \cref{rmk:other_reward_models}), yielding the optimization problem of maximizing \begin{align}
  \j{\vvarphi}[\lambda] &\defeq \j{\vvarphi} + \lambda \H{\altpi_\vvarphi} \\
  &= \sum_{t=1}^{T} \E[(\vx_t,\va_t) \sim \altpi_\vvarphi]{r(\vx_t, \va_t) + \lambda \H{\pi_\vvarphi(\cdot \mid \vx_t)}}, \label{eq:entropy_reg_rl}
\end{align} where we have a preference for entropy in the actor distribution to encourage exploration which is regulated by the temperature parameter $\lambda$.
As $\lambda \to 0$, we recover the ``standard'' reinforcement learning objective (here for finite-horizon rewards): \begin{align}
  \j{\vvarphi} = \sum_{t=1}^{T} \E[(\vx_t,\va_t) \sim \altpi_\vvarphi]{r(\vx_t, \va_t)}.
\end{align}
Here, for notational convenience, we begin the sum with $t=1$ rather than $t=0$.

\subsection{Entropy Regularization as Probabilistic Inference}\label{sec:mfarl:entropy_regularization_as_probabilistic_inference}

The entropy-regularized objective from \cref{eq:entropy_reg_rl} leads us to a remarkable interpretation of reinforcement learning and, more generally, decision-making under uncertainty as solving an inference problem akin to variational inference.
The framing of ``control as inference'' will lead us to contemporary algorithms for reinforcement learning as well as paint a path for decision-making under uncertainty beyond stationary MDPs.

\begin{marginfigure}
  \incfig{merl}
  \caption{Directed graphical model of the underlying hidden Markov model with hidden states $X_t$, optimality variables $O_t$, and actions $A_t$.}
\end{marginfigure}

Let us denote by $\altpi_\star$ the distribution over trajectories $\tau$ under the optimal policy $\pis$.
By framing the problem of optimal control as an inference problem in a hidden Markov model with hidden ``optimality variables'' $O_t \in \{0, 1\}$ indicating whether the played action $\va_t$ was optimal we can derive $\altpi_\star$ analytically.
That is to say, when $O_t = 1$ and $O_{t+1:T} \equiv 1$ the policy from time $t$ onwards was optimal.
To simplify the notation, we will denote the event $O_t = 1$ by $\spO_t$.

We consider the HMM defined by the Gibbs distribution \begin{align}
  p(\spO_t \mid \vx_t, \va_t) \propto \exp\parentheses*{\frac{1}{\lambda} r(\vx_t, \va_t)}, \quad\text{with $\lambda > 0$} \label{eq:merl_optimality}
\end{align} which is a natural choice as we have seen in \cref{exercise:maximum_entropy_property_of_gibbs_distribution} that the Gibbs distribution maximizes entropy subject to $\E{O_t \cdot r(\vx_t, \va_t)}[\vx_t, \va_t]~<~\infty$.

The distribution over trajectories conditioned on optimality of actions~(i.e., conditioned on $\spO_{1:T}$) is given by \begin{align}
  \altpi_\star(\tau) \defeq p(\tau \mid \spO_{1:T}) = p(\vx_1) \prod_{t=1}^{T} p(\va_t \mid \vx_t, \spO_t) p(\vx_{t+1} \mid \vx_t, \va_t). \margintag{Using \cref{eq:trajectory_distr}. We assume here that the dynamics and initial state distribution are ``fixed'', that is, we assume $p(\vx_1 \mid \spO_{1:T}) = p(\vx_1)$ and $p(\vx_{t+1} \mid \vx_t, \va_t, \spO_{1:T}) = p(\vx_{t+1} \mid \vx_t, \va_t)$.} \label{eq:distr_over_optimal_trajectories}
\end{align}
It remains to determine $p(\va_t \mid \vx_t, \spO_t)$ which corresponds to the optimal policy $\opt{\pi}(\va_t \mid \vx_t)$.
It is generally useful to think of the situation where the prior policy $p(\va_t \mid \vx_t)$ is uniform on $\spA$,\footnote{This is not a restriction as any informative prior can be pushed into \cref{eq:merl_optimality}.} in which case by Bayes' rule \eqref{eq:bayes_rule}, $p(\va_t \mid \vx_t, \spO_t) \propto p(\spO_t \mid \vx_t, \va_t)$, so \begin{align}
  \altpi_\star(\tau) &\propto \brackets*{p(\vx_1) \prod_{t=1}^{T} p(\vx_{t+1} \mid \vx_t, \va_t)} \exp\parentheses*{\frac{1}{\lambda} \sum_{t=1}^{T} r(\vx_t, \va_t)}. \label{eq:opt_trajectory_distr}
\end{align}

Recall that our fundamental goal is to approximate $\altpi_\star$ with a distribution over trajectories $\altpi_\vvarphi$ under the parameterized policy $\pi_\vvarphi$.
It is therefore a natural idea to minimize $\KL{\altpi_\vvarphi}{\altpi_\star}$:\safefootnote[-2\baselineskip]{Observe that we cannot easily minimize forward-KL as we cannot sample from $\altpi_\star$. In the context of RL, it can be argued that the mode-seeking behavior of reverse-KL is preferable over the moment-matching behavior of forward-KL \citep{levine2018reinforcement}.} \begin{align}
  &\argmin_\vvarphi \KL{\altpi_\vvarphi}{\altpi_\star} \nonumber \\
  &= \argmin_\vvarphi \crH{\altpi_\vvarphi}{\altpi_\star} - \H{\altpi_\vvarphi} \margintag{using the definition of KL-divergence \eqref{eq:kl}} \nonumber \\
  &= \argmax_\vvarphi \E[\tau \sim \altpi_\vvarphi]{\log \altpi_\star(\tau) - \log \altpi_\vvarphi(\tau)} \margintag{using the definition of cross-entropy \eqref{eq:cross_entropy} and entropy \eqref{eq:entropy}} \nonumber \\
  &= \argmax_\vvarphi \E[\tau \sim \altpi_\vvarphi]{\sum_{t=1}^{T} r(\vx_t, \va_t) - \lambda \log \pi_\vvarphi(\va_t \mid \vx_t)} \margintag{using \cref{eq:opt_trajectory_distr,eq:trajectory_distr} and simplifying} \nonumber \\
  &= \argmax_\vvarphi \sum_{t=1}^{T} \E[(\vx_t, \va_t) \sim \altpi_\vvarphi]{r(\vx_t, \va_t) + \lambda \H{\pi_\vvarphi(\cdot \mid \vx_t)}}. \margintag{using the definition of entropy \eqref{eq:entropy} and linearity of expectation \eqref{eq:linearity_expectation}} \label{eq:entropy_reg_rl2}
\end{align}
That is, entropy regularization is equivalent to minimizing the KL-divergence from $\altpi_\star$ to $\altpi_\vvarphi$.
This highlights a very natural tradeoff between exploration and exploitation, wherein $\crH{\altpi_\vvarphi}{\altpi_\star}$ encourages exploitation and $\H{\altpi_\vvarphi}$ encourages exploration.

It can be shown that a ``softmax'' version of the Bellman optimality equation \eqref{eq:bop2_q} can be obtained for \cref{eq:entropy_reg_rl2} \exerciserefmark{soft_value_function}: \begin{align}
  \q*{\vx}{\va} = \frac{1}{\lambda} r(\vx, \va) + \E[\vxp \sim \vx, \va]{\log \int_{\spA} \exp\parentheses*{\q*{\vxp}{\vap}} \,d \vap} \label{eq:soft_value_function}
\end{align} with the convention that $\q*{\vx_T}{\va} = 0$ for all $\va$.\footnote{Note that \cref{eq:bop2_q} was derived in the infinite horizon setting with discounted rewards, whereas here we study the finite horizon setting. \Cref{eq:soft_value_function} is the natural extension of the standard Bellman optimality equation in the finite horizon setting where the downstream rewards are measured by a softmax rather than the greedy policy.}
Here, $\opt{\fnq}$ is called a \midx{soft value function}.
As we will see in \cref{exercise:soft_value_function}, the optimal policy has the form $\pis(\va \mid \vx) \propto \exp(\q*{\vx}{\va})$, that is, it simply corresponds to performing softmax exploration \eqref{eq:softmax_exploration} with the soft value function.
The second term of \cref{eq:soft_value_function} quantifies downstream rewards.
In comparison to the ``standard'' Bellman optimality equation \eqref{eq:bop2_q}, the soft value function is less greedy which tends to encourage robustness.

Analogously to Q-learning, the soft value function $\opt{\fnq}$ can be approximated via a bootstrapped ``critic'' $\opt{\fnQ}$ which is called \midx{soft Q-learning} \citep{levine2018reinforcement}.
Note that computing the optimal policy requires computing an integral over the action space, which is typically intractable for continuous action spaces.
As discussed in \cref{sec:mfarl:policy_approximation,sec:mfarl:actor_critic_methods,sec:mfarl:off_policy_actor_critic_methods} and analogously to actor-critic methods such as DDPG and SVG, we can learn a parameterized policy (i.e., an ``actor'') $\pi_\vvarphi$ to approximate the optimal policy $\pis$.
The resulting algorithm, \midx{soft actor critic}[idxpagebf] (SAC) \citep{haarnoja2018soft,haarnoja2018soft2}, is widely used.
Due to its off-policy nature, it is also relatively sample efficient.

\begin{figure*}
  \includegraphics[width=\textwidth]{figures/sac.png}
  \caption{Comparison of training curves of a selection of on-policy and off-policy policy gradient methods.
  Reproduced with permission from \icite{haarnoja2018soft2}.}
\end{figure*}

We can also express this optimization in terms of an evidence lower bound.
The evidence lower bound for the observations $\spO_{1:T}$ is \begin{align}
  L(\altpi_\vvarphi, \altpi_\star; \spO_{1:T}) &= \E[\tau \sim \altpi_\vvarphi]{\log p(\spO_{1:T} \mid \tau) + \log \altpi(\tau) - \log \altpi_\vvarphi(\tau)} \margintag{using the definition of the ELBO \eqref{eq:elbo_expanded}}
\end{align} where $\altpi$ denotes the distribution over trajectories \eqref{eq:trajectory_distr} under the prior policy $p(\va_t \mid \vx_t)$.
This is commonly written as the variational free energy \begin{align}
  - L(\altpi_\vvarphi, \altpi_\star; \spO_{1:T}) &= \E[\tau \sim \altpi_\vvarphi]{\S{p(\spO_{1:T} \mid \tau)}} + \KL{\altpi_\vvarphi}{\altpi} \\
  &= \underbrace{\S{p(\spO_{1:T})}}_{\text{``extrinsic'' value}} + \underbrace{\KL{\altpi_\vvarphi}{\altpi_\star}}_{\text{``epistemic'' value}}. \margintag{using that $\altpi_\star(\tau) = p(\tau \mid \spO_{1:T})$}
\end{align} which we already encountered in \cref{sec:free_energy} in the context of variational inference.
Here, the ``extrinsic'' value is independent of the variational distribution $\altpi_\vvarphi$ and can be thought of as a fixed ``problem cost'', whereas the ``epistemic'' value can be interpreted as the approximation error or ``solution cost''.
To summarize, we have seen that \begin{align*}
  \argmax_\vvarphi \j{\vvarphi}[\lambda] = \argmin_\vvarphi \KL{\altpi_\vvarphi}{\altpi_\star} = \argmax_\vvarphi L(\altpi_\vvarphi, \altpi_\star; \spO_{1:T}).
\end{align*}

Recall that the free energy $-L(\altpi_\vvarphi, \altpi_\star; \spO_{1:T})$ is a variational upper bound to the surprise about observations $\S{\altpi_\star(\spO_{1:T})}$ when following an optimal policy.\footnote{see \cref{sec:free_energy}}
For example, undesirable states incur a low reward while desirable states yield a high reward, and thus, if we expect optimality of actions (i.e., $\spO_{1:T}$) paths leading to such states have high and low surprise, respectively.\safefootnote{This is because, a path leading to a low reward state will include sub-optimal actions.}

An agent which acts to minimize free energy with $\spO_{1:T}$ can be thought of as hallucinating to perform optimally, and acting to minimize the surprise about having played suboptimal actions.
Think, for example, about the robotics task of moving an arm to a new position.
Intuitively, minimizing free energy solves this task by ``hallucinating that the arm is at the goal position'', and then minimizing the surprise with respect to this perturbed world model.
In this way, MERL can be understood as identifying paths of least surprise akin to the \midx{free energy principle}.


\begin{rmk}{Towards active inference}{}
  Maximum entropy reinforcement learning makes one fundamental assumption, namely, that the ``biased'' distribution about observations specifying the underlying HMM is \begin{align*}
    p(\spO_t \mid \vx_t, \va_t) \propto \exp\parentheses*{\frac{1}{\lambda} r(\vx_t, \va_t)}.
  \end{align*}
  This assumption is key to the framing of optimal control (in an unknown MDP) with a certain reward function as an inference problem.
  One could conceive other HMMs.
  For example, \cite{fellows2019virel} propose an HMM defined in terms of the current value function of the agent: \begin{align*}
    p(\spO_t \mid \vx_t, \va_t) \propto \exp\parentheses*{\frac{1}{\lambda} \Q{\vx_t}{\va_t; \vtheta}}.
  \end{align*}

  Crucially, the choice of $p(\spO_t \mid \vx_t, \va_t)$ is the only place where the reward enters the inference problem, and one can conceive of settings where a ``stationary'' (i.e., time-independent) reward is not assumed to exist.
  The general approach to decision-making as probabilistic inference presented in this section (but for possibly reward-independent and non-stationary HMMs) is known as \midx{active inference} \citep{friston2015active,millidge2020relationship,millidge2021whence,parr2022active}.\looseness=-1
\end{rmk}

\section{Learning from Preferences}\label{sec:mfarl:preference_learning}

So far, we have been assuming that the agent is presented with a reward signal after every played action.
This is a natural assumption in domains such as games and robotics --- even though it often requires substantial ``reward engineering'' to break down complex tasks with sparse rewards to more manageable tasks (cf. \cref{sec:mbarl:exploration}).
In many other domains such as an agent learning to drive a car or a chatbot, it is unclear how one can even quantify the reward associated with an action or a sequence of actions.
For example, in the context of autonomous driving it is typically desired that agents behave ``human-like'' even though a different driving behavior may also reach the destination safely.

\begin{marginfigure}[-10\baselineskip]
  \incfig{learning_from_preferences}
  \caption{We generalize the perspective of reinforcement learning from \cref{fig:rl} by allowing the feedback to come from either the environment or an evaluation by other agents (e.g., humans), and by allowing the feedback to come in other forms than a numerical reward.}\label{fig:learning_from_preferences}
\end{marginfigure}

The task of ``aligning'' the behavior of an agent to human expectations is difficult in complex domains such as the physical world and language, yet crucial for their practical use.
To this end, one can conceive of alternative ways for presenting ``feedback'' to the agent: \begin{itemize}

  \item The classical feedback in reinforcement learning is a numerical score.
  Consider, for example, a recommender system for movies.
  The feedback is obtained after a movie was recommended to a user by a user-rating on a given scale (often $1$ to $10$).
  This rating is informative as it corresponds to an \emph{absolute value assessment}, allowing to place the recommendation in a complete ranking of all previous recommendations.
  However, numerical feedback of this type can be error-prone as it is scale-dependent (different users may ascribe different value to a recommendation rated a $7$).

  \item An alternative feedback mechanism is comparison-based.
  The user is presented with $k$ alternative actions and selects their preferred action (or alternatively returns a ranking of actions).
  This feedback is typically easy to obtain as humans are fast in making ``this-or-that'' decisions.
  However, in contrast to numerical rewards, the feedback provides information only on the user's \emph{relative preferences}.
  That is, such preference feedback encodes fewer bits of information than score feedback, and it therefore often takes longer to learn complex behavior from this feedback.
\end{itemize}

\begin{rmk}{Context-dependent feedback}{}
  We neglect here that feedback is often context-dependent \citep{lindner2022humans,casper2023open}.
  For example, if someone is asked whether they prefer ``ice cream'' over ``pizza'' the answer may depend on whether they are hungry and the weather.
\end{rmk}

\subsection{Language Models as Agents}

In the following, we discuss approaches to learning from preference feedback in the context of autoregressive\footnote{An autoregressive model predicts/generates the next ``token'' as a function of previous tokens.} large language models and chatbots.
A chatbot is an agent (often based on a transformer architecture, \cite{vaswani2017attention}) parameterized by $\vvarphi$ that given a \emph{prompt} $\vx$ returns a (stochastic) \emph{response} $\vy$.
The autoregressive generation of the response can be understood in terms of a policy $\pi_{\vvarphi}(y_{t+1} \mid \vx, y_{1:t})$ which generates the next token given the prompt and all previous tokens.\footnote{In large language models, a ``token'' is usually taken to be a letter, word, or something in-between. A special token is used to terminate the response.}
We denote the policy over complete responses (i.e., the chatbot) by\looseness=-1 \begin{align}
  \altpi_{\vvarphi}(\vy \mid \vx) = \prod_{t=0}^{T-1} \pi_{\vvarphi}(y_{t+1} \mid \vx, y_{1:t}). \label{eq:lm_trajectory_distr}
\end{align}
In RL jargon, the agents action corresponds to the choice of next token $y_{t+1}$ and the deterministic dynamics add this token to the current (incomplete) response $y_{1:t}$.
A full trajectory $\vy$ consists sequentially of all tokens $y_{1:T}$ comprising a response, and the prompt $\vx$ can be interpreted as a \midx{context} to this trajectory.
Observe that \cref{eq:lm_trajectory_distr} is derived from the general representation of the distribution over trajectories \eqref{eq:trajectory_distr}, noting that prior and dynamics are deterministic.

The standard pipeline for applying pre-trained\footnote[][-3\baselineskip]{The pre-trained language model is usually obtained by self-supervised training on a large corpus of text. \midx<Self-supervised learning>{self-supervised learning} generates labeled training data from an unlabeled data source by selectively ``masking-out'' parts of the data. When training an autoregressive language model, labeled training data can be obtained by repeatedly ``masking-out'' the next word in a sentence, and training the language model to predict this word. Such large models that can be fine-tuned and ``post-trained'' to various downstream tasks are also called \midx<foundation models>{foundation model}.} large language models such as GPT \citep{openai2023gpt4} to downstream tasks consists of two main steps which are illustrated in \cref{fig:llm_learning_pipeline} \citep{stiennon2020learning}: (1) supervised fine-tuning and (2) post-training using preference feedback.\looseness=-1

\begin{figure}
  \incfig{llm_post_training}
  \caption{Illustration of the learning pipeline of a large language model.}
  \label{fig:llm_learning_pipeline}
\end{figure}

The first step is to fine-tune the language model with supervised learning on high-quality data for the downstream task of interest.
For example, when the goal is to build a chatbot, this data may consist of desirable responses to some exemplary prompts.
We will denote the parameters of the fine-tuned language model by $\vvarphi^{\init}$, and its associated policy by $\altpi_{\init}$.

The second step is then to ``post-train'' the language model $\altpi_{\init}$ from the first step using human feedback.
Here, it is important that $\altpi_{\init}$ is already capable of producing sensible output (i.e., with correct spelling and grammar).
Learning this from scratch using only preference feedback would take far too long.
Instead, the post-training step is used to align the agent to the task and user preferences.

In each iteration of post-training, the model is prompted with prompts $\vx$ to produce pairs of answers $(\vy_A, \vy_B) \sim \altpi_{\vvarphi}(\cdot \mid \vx)$.
These answers are presented to human labelers who express their preference for one of the answers, denoted $\vy_A \succ \vy_B \mid \vx$.
A popular choice for modeling preferences is the \midx{Bradley-Terry model} which stipulates that the human preference distribution is given by \begin{align}
  p(\vy_A \succ \vy_B \mid \vx, r) &= \frac{\exp(r(\vy_A \mid \vx))}{\exp(r(\vy_A \mid \vx)) + \exp(r(\vy_B \mid \vx))}
  \intertext{for some unknown latent reward model $r(\vy \mid \vx)$ \citep{bradley1952rank}. This can be written in terms of the logistic function $\sigma$ \eqref{eq:logistic_function}:}
  &= \sigma(r(\vy_A \mid \vx) - r(\vy_B \mid \vx)). \margintag{as seen in \cref{exercise:softmax_and_logistic_function} this is the Gibb's distribution with energy $-r(\vy \mid \vx)$ in a binary classification problem} \label{eq:bradley_terry_model}
\end{align}

\begin{rmk}{Outcome rewards}{}
  Note that the Bradley-Terry model attributes reward only to ``complete'' responses.
  We call such a reward an \midx{outcome reward}.\safefootnote{Framing this in terms of an individual ``per-step'' rewards which we have seen so far, this corresponds to a (sparse) reward which is zero until the final action.}
  While the following discussion is on outcome rewards (which is most common in the context of language models), everything translates to individual per-step rewards.
\end{rmk}

The aggregated human feedback $\spD = \{\vy_A^{(i)} \succ \vy_B^{(i)} \mid \vx^{(i)}\}_{i=1}^n$ across $n$ different prompts is then used to update the language model $\pi_{\vvarphi}$.
In the next two sections, we discuss two standard approaches to post-training: reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO).

\subsection{Reinforcement Learning from Human Feedback}\pidx{reinforcement learning from human feedback}%

RLHF separates the post-training step into two stages \citep{stiennon2020learning}.
First, the human feedback is used to learn an approximate reward model $r_{\vtheta}$.
This reward model is then used in the second stage to determine a refined policy $\altpi_{\vvarphi}$.

\begin{figure}
  \incfig{rlhf}
  \caption{Illustration of the post-training process of RLHF.}
  \label{fig:rlhf}
\end{figure}

\paragraph{Learning a reward model:}

During the first stage, the initial policy obtained by supervised fine-tuning $\altpi_{\init}$ is used to generate proposals~${(\vy_A, \vy_B)}$ for some exemplary prompts $\vx$, which are then ranked according to the preference of human labelers.
This preference data can be used to learn a reward model $r_{\vtheta}$ by maximum likelihood estimation (or equivalently minimizing cross-entropy): \begin{align}\begin{split}
  &\argmax_{\vtheta} p(\spD \mid r_\vtheta) \\
  &= \argmax_{\vtheta} \E[(\vy_A \succ \vy_B \mid \vx) \sim \spD]{\log \sigma\parentheses*{r_{\vtheta}(\vy_A \mid \vx) - r_{\vtheta}(\vy_B \mid \vx)}} \margintag{using \cref{eq:bradley_terry_model} and SGD} \label{eq:rlhf_reward_model}
\end{split}\end{align}%
This is analogous to the standard maximum likelihood estimation of the reward model in model-based RL with score feedback which we discussed in \cref{sec:rl:learning_mdp}.
The reward model $r_{\vtheta}$ is often initialized from the initial policy $\pi_{\init}$ by placing a linear layer producing a scalar output on top of the final transformer layer.

\paragraph{Learning an optimal policy:}

One can now employ the methods from this and previous chapters to determine the optimal policy for the approximate reward $r_{\vtheta}$.
Due to the use of an \emph{approximate} reward, however, simply maximizing $r_{\vtheta}$ surfaces the so-called ``reward gaming'' problem which is illustrated in \cref{fig:reward_gaming}.
As the responses generated by the learned policy $\pi_{\vvarphi}$ deviate from the distribution of $\spD$ (i.e., the distribution induced by the initial policy $\pi_{\init}$), the approximate reward model becomes inaccurate.
The approximate reward may severely overestimate the true reward in regions far away from the training data.
A common approach to address this problem is to regularize the policy search towards policies whose distribution does not stray away ``too far'' from the training distribution.

\begin{marginfigure}
  \incplt{reward_gaming}
  \caption{Illustration of ``reward gaming''. Shown in black is the true reward $r(y \mid x)$ for a fixed prompt $x$. Shown in blue is the approximation based on the feedback $\spD$ to the responses shown in red. The yellow region symbolizes responses $y$ where the approximate reward can still be ``trusted''.}
  \label{fig:reward_gaming}
\end{marginfigure}

Analogously to the trust-region methods we discussed in \cref{sec:mfarl:actor_critc_methods:improved}, the deviation from the initial policy is typically controlled by maximizing the regularized objective \begin{align}
  \j{\vvarphi; \vvarphi^{\init} \mid \vx}[\lambda] &\defeq \E[\vy \sim \altpi_{\vvarphi}(\cdot \mid \vx)]{r(\vy \mid \vx)} - \lambda \KLsm{\altpi_{\vvarphi}(\cdot \mid \vx)}{\altpi_{\init}(\cdot \mid \vx)} \nonumber
  \intertext{in expectation over prompts $\vx$ sampled uniformly at random from the dataset $\spD$.
  Note that this coincides with the PPO objective from \cref{eq:ppo} with an outcome reward. We can expand the regularization term to obtain}
  &= \begin{multlined}[t]
    \underbrace{\E[\vy \sim \altpi_{\vvarphi}(\cdot \mid \vx)]{r(\vy \mid \vx)} + \lambda \Hsm{\altpi_{\vvarphi}(\cdot \mid \vx)}}_{\text{entropy-regularized RL}} \\ - \lambda \crHsm{\altpi_{\vvarphi}(\cdot \mid \vx)}{\altpi_{\init}(\cdot \mid \vx)}
  \end{multlined} \margintag{using the definition of KL-divergence \eqref{eq:kl}}
\end{align} which indicates an intimate relationship to entropy-regularized RL.\footnote{compare to \cref{eq:entropy_reg_rl}}

The optimal policy maximizing $\j{\vvarphi; \vvarphi^{\init} \mid \vx}[\lambda]$ is \exerciserefmark{ppo_as_probabilistic_inference} \begin{align}
  \altpi_\star(\vy \mid \vx) \propto \altpi_{\init}(\vy \mid \vx) \exp\parentheses*{\frac{1}{\lambda} r(\vy \mid \vx)} \label{eq:optimal_regularized_policy_rlhf}
\end{align} and can be interpreted as a probabilistic update to the prior $\altpi_{\init}$ where $\exp\parentheses{\frac{1}{\lambda} r(\vy \mid \vx)}$ corresponds to the ``likelihood of optimality''.
As $\lambda \to \infty$, $\altpi_\star \to \altpi_{\init}$, and as $\lambda \to 0$, the optimal policy reduces to deterministically picking the response with the highest reward.

In practice, sampling from $\altpi_\star$ explicitly is intractable, but note that any of the approximate inference methods discussed in \cref{part1} are applicable here.
In the context of chatbots, it is important that the sampling from the resulting policy is efficient (i.e., the chatbot should respond quickly to prompts).
Most commonly, the optimization problem of maximizing $j{\vvarphi; \vvarphi^{\init} \mid \vx}[\lambda]$ (with the estimated reward $r_{\vtheta}$) is solved approximately within a parameterized family of policies.
This is typically done using policy gradient methods such as PPO~\citep{stiennon2020learning} or GRPO~\citep{guo2025deepseek}.

\begin{rmk}{Other (non-preference) reward models}{}
  Note that the post-training pipeline we described here is agnostic to the choice of reward model.
  That is, instead of post-training our language model to comply with human preferences, we could have also post-trained it to maximize any other reward signal.
  More recently, works have explored the use of reward models for challenging ``reasoning problems'' such as mathematical calculations, where the reward is usually based on the correctness of the answer.\safefootnote{On training data where the answer to problem $\vx$ is known to be $\opt{y}(\vx)$, the reward is simply $r(\vy \mid \vx) = \Ind{\opt{y}(\vx) \in \vy}$. That is, the reward is $1$ if the response $\vy$ contains the correct answer and $0$ otherwise.}
  One prominent example for this are ``reasoning'' models such as the DeepSeek-R1 model~\citep{guo2025deepseek}, which was trained with GRPO.
\end{rmk}

\subsection{Direct Preference Optimization}\pidx{direct preference optimization}%

Observe that the reward model can be expressed in terms of its associated optimal policy: \begin{align}
  r(\vy \mid \vx) = \lambda \log\frac{\altpi_\star(\vy \mid \vx)}{\altpi_{\init}(\vy \mid \vx)} + \const. \margintag{by reorganizing the terms of \cref{eq:optimal_regularized_policy_rlhf}} \label{eq:dpo_reward_policy_relationship}
\end{align}
In particular, it follows that given a fixed prior $\altpi_{\init}$, \emph{any} policy $\altpi_\vvarphi$ has a family of associated reward models with respect to which it is optimal!
We denote by \begin{align}
  r_{[\vvarphi]}(\vy \mid \vx) \defeq \lambda \log\frac{\altpi_\vvarphi(\vy \mid \vx)}{\altpi_{\init}(\vy \mid \vx)}
\end{align} the ``simplest'' of these reward models.
Remembering the characterization of the optimal policy from \cref{eq:optimal_regularized_policy_rlhf} it follows immediately that $\altpi_\vvarphi$ is optimal with respect to $r_{[\vvarphi]}$.

Instead of first learning an approximate reward model and then finding the associated optimal policy, DPO exploits the relationship of \cref{eq:dpo_reward_policy_relationship} to learn the optimal policy directly \citep{rafailov2023direct}.
Substituting $r_\vtheta$ in the maximum likelihood estimation from \cref{eq:rlhf_reward_model}, yields the objective \begin{align}
  &\E[(\vy_A \succ \vy_B \mid \vx) \sim \spD]{\log \sigma\parentheses*{r_{[\vvarphi]}(\vy_A \mid \vx) - r_{[\vvarphi]}(\vy_B \mid \vx)}} \nonumber \\
  &= \E[(\vy_A \succ \vy_B \mid \vx) \sim \spD]{\log \sigma\parentheses*{\lambda \log\frac{\altpi_\vvarphi(\vy_A \mid \vx)}{\altpi_{\init}(\vy_A \mid \vx)} - \lambda \log\frac{\altpi_\vvarphi(\vy_B \mid \vx)}{\altpi_{\init}(\vy_B \mid \vx)}}}. \margintag{using \cref{eq:dpo_reward_policy_relationship}}
\end{align}
Gradients can be computed via automatic differentiation: \begin{align}
  \begin{multlined}
    \lambda \E*[(\vy_A \succ \vy_B \mid \vx) \sim \spD]{\Biggl[\underbrace{\sigma\parentheses*{r_{[\vvarphi]}(\vy_B \mid \vx) - r_{[\vvarphi]}(\vy_A \mid \vx)}}_{\text{weight according to error of reward estimate}} \\ \Bigl[\underbrace{\grad_\vvarphi \log \altpi_\vvarphi(\vy_A \mid \vx)}_{\text{increase likelihood of $\vy_A$}} - \underbrace{\grad_\vvarphi \log \altpi_\vvarphi(\vy_B \mid \vx)}_{\text{decrease likelihood of $\vy_B$}}\Bigr]\Biggr]}.
  \end{multlined}
\end{align}
Intuitively, DPO successively increases the likelihood of preferred responses $\vy_A$ and decreases the likelihood of dispreferred responses $\vy_B$.
Examples $(\vy_A \succ \vy_B \mid \vx)$ are weighted by the strength of regularization~$\lambda$ and by the degree to which the implicit reward model incorrectly orders the responses.

\section*{Discussion}\label{sec:mfarl:discussion}

In this chapter, we studied central ideas in actor-critic methods.
We have seen two main approaches to use policy-gradient methods.
We began, in \cref{sec:mfarl:policy_approximation}, by introducing the REINFORCE algorithm which uses policy gradients and Monte Carlo estimation, but suffered from large variance in the gradient estimates of the policy value function.
In \cref{sec:mfarl:actor_critic_methods}, we have then seen a number of actor-critic methods such as A2C and GAE behaving similarly to policy iteration that exhibit less variance, but are very sample inefficient due to their on-policy nature.
TRPO improves the sample efficiency slightly, but not fundamentally.

In \cref{sec:mfarl:off_policy_actor_critic_methods}, we discussed a second family of policy gradient techniques that generalize Q-learning and are akin to value iteration.
For reparameterizable policies, this led us to algorithms such as DDPG, TD3, SVG.
Importantly, these algorithms are significantly more sample efficient than on-policy policy gradient methods, which often results in much faster learning of a near-optimal policy.
In \cref{sec:mfarl:actor_critic_methods:entropy_regularization}, we discussed entropy regularization which frames reinforcement learning as probabilistic inference, and we derived the SAC algorithm which is widely used and works quite well in practice.

Finally, in \cref{sec:mfarl:preference_learning}, we studied two canonical approaches to learning from preference feedback: RLHF which separately learns reward model and policy and DPO which learns a policy directly.
We have seen that RLHF is akin to model-based RL as it explicitly learns a reward model through maximum likelihood estimation.
In contrast, DPO is more closely related to model-free RL and policy gradient methods as it learns the optimal policy directly.

\begin{oreadings}
  \begin{itemize}
    \item \textbf{A3C:} \pcite{mnih2016asynchronous}
    \item \textbf{GAE:} \pcite{schulman2015high}
    \item \textbf{TRPO:} \pcite{schulman2015trust}
    \item \textbf{PPO:} \pcite{schulman2017proximal}
    \item \textbf{DDPG:} \pcite{lillicrap2015continuous}
    \item \textbf{TD3:} \pcite{fujimoto2018addressing}
    \item \textbf{SVG:} \pcite{heess2015learning}
    \item \textbf{SAC:} \pcite{haarnoja2018soft}
    \item \textbf{DPO:} \pcite{rafailov2023direct}
  \end{itemize}
\end{oreadings}

\excheading

\begin{nexercise}{Q-learning and function approximation}{q_learning_func_approx}
  Consider the MDP of \cref{fig:q_learning_func_approx} and set $\gamma = 1$.

  \begin{enumerate}
    \item Using Bellman's theorem, prove that $\v*{x} = -\abs{x-4}$ is the optimal value function.

    \item Suppose we observe the following episode:
    \begin{center}
      \vspace{5pt}
      \begin{tabular}{cccc}
        \toprule
        $x$ & $a$ & $x'$ & $r$ \\
        \midrule
        $3$ & $-1$ & $2$ & $-1$ \\
        $2$ & $1$ & $3$ & $-1$ \\
        $3$ & $1$ & $4$ & $-1$ \\
        $4$ & $1$ & $4$ & $0$ \\
        \bottomrule
      \end{tabular}
      \vspace{5pt}
    \end{center}
    We initialize all Q-values to $0$.
    Compute the updated Q-values using Q-learning with learning rate $\alpha = \nicefrac{1}{2}$.

    \item We will now approximate the Q-function with a linear function.
    We let \begin{align*}
      \Q{x}{a; \vw} \defeq x w_0 + a w_1 + w_2
    \end{align*} where $\vw = \transpose{[w_0 \; w_1 \; w_2]} \in \R^3$.

    Suppose we have $\old{\vw} = \transpose{[1 \; -1 \; -2]}$ and $\vw = \transpose{[-1 \; 1 \; 1]}$, and we observe the transition $\tau = (2, -1, -1, 1)$.
    Use the learning rate $\alpha = \nicefrac{1}{2}$ to compute $\grad_\vw \ell(\vw; \tau)$ and the updated weights ${\vw' = \vw - \alpha \grad_\vw \ell(\vw; \tau)}$.
  \end{enumerate}
\end{nexercise}

\begin{figure}
  \incfig{q_learning_example}
  \caption{MDP studied in \cref{exercise:q_learning_func_approx}. Each arrow marks a (deterministic) transition and is labeled with $(\text{action},\text{reward})$.}\label{fig:q_learning_func_approx}
\end{figure}

\begin{nexercise}{Eligibility vector}{eligibility_vector}
  The vector $\grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)$ is commonly called \midx{eligibility vector}.
  In the following, we assume that the action space $\spA$ is finite and denote it by $\sA$.

  If we parameterize $\pi_\vvarphi$ as a softmax distribution \begin{align}
    \pi_\vvarphi(a \mid \vx) \defeq \frac{\exp(h(\vx, a, \vvarphi))}{\sum_{b \in \sA} \exp(h(\vx, b, \vvarphi))}
  \end{align} with linear preferences $h(\vx, a, \vvarphi) \defeq \transpose{\vvarphi} \vphi(\vx, a)$ where $\vphi(\vx, a)$ is some feature vector, what is the form of the eligibility vector?
\end{nexercise}

\begin{nexercise}{Variance of score gradients with baselines}{score_gradients_with_baselines_variance}
  In this exercise, we will see a sufficient condition for baselines to reduce the variance of score gradient estimators.

  \begin{enumerate}
    \item Suppose for a random vector $\rX$, we want to estimate $\E{f(\rX)}$ for some function $f$.
    Assume that you are given a function $g$ and also its expectation $\E{g(\rX)}$.
    Instead of estimating $\E{f(\rX)}$ directly, we will instead estimate $\E{f(\rX) - g(\rX)}$ as we know from linearity of expectation \eqref{eq:linearity_expectation} that \begin{align*}
      \E{f(\rX)} = \E{f(\rX) - g(\rX)} + \E{g(\rX)}.
    \end{align*}
    Prove that if $\frac{1}{2} \Var{g(\rX)} \leq \Cov{f(\rX), g(\rX)}$, then \begin{align}
      \Var{f(\rX) - g(\rX)} \leq \Var{f(\rX)}. \label{eq:variance_reduction}
    \end{align}

    \item Consider estimating $\grad_\vvarphi \j{\vvarphi}$.
    Prove that if $b^2 \leq 2 b \cdot r(\vx, \va)$ for every state $\vx \in \spX$ and action $\va \in \spA$, then \begin{align}
      \Var[\tau \sim \altpi_\vvarphi]{(G_0 - b) \grad_\vvarphi \log \altpi_\vvarphi(\tau)} \leq \Var[\tau \sim \altpi_\vvarphi]{G_0 \grad_\vvarphi \log \altpi_\vvarphi(\tau)}.
    \end{align}
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Score gradients with state-dependent baselines}{score_gradients_state_dep_baselines}
  For a sequence of \midx<state-dependent baselines>{state-dependent baseline} $\{b(\tau_{0:t-1})\}_{t=1}^T$ where \begin{align*}
    \tau_{0:t-1} \defeq (\tau_0, \tau_1, \dots, \tau_{t-1}),
  \end{align*} show that \begin{align}\begin{split}
    &\E[\tau \sim \altpi_\vvarphi]{G_0 \grad_\vvarphi \log \altpi_\vvarphi(\tau)} \\
    &= \E[\tau \sim \altpi_\vvarphi]{\sum_{t=0}^{T-1} (G_0 - b(\tau_{0:t-1})) \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}
  \end{split}\label{eq:score_gradient_estimator_state_dep_baseline}\end{align} where we write $b(\tau_{0:-1}) = 0$.
\end{nexercise}

\begin{nexercise}{Policy gradients with downstream returns}{policy_gradients_with_downstream_returns}
  Suppose we are training an agent to solve a computer game.
  There are only two possible actions, specifically: \begin{enumerate}
    \item \emph{do nothing}; and
    \item \emph{move}.
  \end{enumerate}
  Each episode lasts for four ($T = 3$) time steps.
  The policy $\pi_\theta$ is completely determined by the parameter $\theta \in [0,1]$.
  Here, for simplicity, we have assumed that the policy is independent of the current state.
  The probability of moving (action $2$) is equal to $\theta$ and the probability of doing nothing (action $1$) is $1 - \theta$.

  Initially, $\theta = 0.5$.
  One episode is played with this initial policy and the results are \begin{align*}
    \text{actions} = (1, 0, 1, 0), \quad \text{rewards} = (1, 0, 1, 1).
  \end{align*}
  Compute the policy gradient estimate with downstream returns, discount factor $\gamma = \nicefrac{1}{2}$, and the provided \emph{single} sample $\tau \sim \altpi_\theta$.
\end{nexercise}

\begin{nexercise}{Policy gradient with an exponential family}{policy_gradient_with_exponential_family}
  \begin{enumerate}
    \item Suppose, we can choose between two actions $a \in \{0, 1\}$ in each state.
    A natural stochastic policy is induced by a Bernoulli distribution, \begin{align}
      a \sim \Bern{\sigma(f_\vvarphi(\vx))},
    \end{align} where $\sigma$ is the logistic function from \cref{eq:logistic_function}.
    First, write down the expression for $\pi_\vvarphi(a \mid \vx)$.
    Then, derive the expression for $\grad_\vvarphi \j{\vvarphi}$ in terms of $\fnq[\pi_\vvarphi]$, $\sigma(f_\vvarphi(\vx))$, and $\grad_\vvarphi f_\vvarphi(\vx)$ using the policy gradient theorem.

    \item The Bernoulli distribution is part of a family of distributions that allows for a much simpler derivation of the gradient than was necessary in (1).
    A univariate \midx{exponential family} is a family of distributions whose PDF (or PMF) can be expressed in canonical form as\looseness=-1 \begin{align}
      \pi_\vvarphi(a \mid \vx) = h(a) \exp(a f_\vvarphi(\vx) - A(f_\vvarphi(\vx)))
    \end{align} where $h$, $f$, and $A$ are known functions.\safefootnote{Observe that this form is equivalent to the form introduced in \cref{eq:exponential_family_of_distributions} where $f_\vvarphi(\vx)$ is the natural parameter, and we let $A(f) = \log Z(f)$.}
    Derive the expression of the policy gradient $\grad_\vvarphi \j{\vvarphi}$ for such a distribution.

    \item Can you relate the results of the previous two exercises (1) and (2)?
    What are $h$ and $A$ in case of the Bernoulli distribution?

    \item The Gaussian distribution with unit variance $\N{f_\vvarphi(\vx)}{1}$ is of the same canonical form with \begin{align}
      A(f_\vvarphi(\vx)) = \frac{f_\vvarphi(\vx)^2}{2}.
    \end{align}
    Determine the policy gradient $\grad_\vvarphi \j{\vvarphi}$.

    \item For a Gaussian policy, can we instead apply the reparameterization trick \eqref{eq:reparameterization_trick_gaussian} that we have seen in the context of variational inference? If yes, how? If not, why?
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Soft value function}{soft_value_function}
  In this exercise, we derive the optimal policy solving \cref{eq:entropy_reg_rl2} and the soft value function from \cref{eq:soft_value_function}.
  \begin{enumerate}
    \item We let $\beta(\va_t \mid \vx_t) \defeq \exp(\frac{1}{\lambda} r(\vx_t, \va_t))$, $Z(\vx) \defeq \int_\spA \beta(\va \mid \vx) \, d \va$, and denote by $\hat{\pi}(\cdot \mid \vx)$ the policy $\beta(\cdot \mid \vx) / Z(\vx)$.
    Show that \begin{align}
      \begin{multlined}
        \KL{\altpi_\vvarphi}{\altpi_\star} \\ = \sum_{t=1}^T \E[\vx_t \sim \altpi_\vvarphi]{\KL{\pi_\vvarphi(\cdot \mid \vx_t)}{\hat{\pi}(\cdot \mid \vx_t)} - \log Z(\vx_t)}.
      \end{multlined} \label{eq:entropy_reg_rl3}
    \end{align}

    \item Show that if the space of policies parameterized by $\vvarphi$ is sufficiently expressive, $\pis(\va \mid \vx) \propto \exp(\q*{\vx}{\va})$ solves \cref{eq:entropy_reg_rl2}.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{PPO as probabilistic inference}{ppo_as_probabilistic_inference}
  \begin{enumerate}
    \item Consider the same generative model as was introduced in \cref{sec:mfarl:entropy_regularization_as_probabilistic_inference} when we interpreted entropy-regularized RL as probabilistic inference.
    Only now, assume that $T=1$ \safefootnote{since, in this section, we have been considering outcome rewards} and suppose that the prior over actions is $p(\vy \mid \vx) = \altpi_{\init}(\vy \mid \vx)$ rather than uniform.
    Define the distribution over optimal trajectories $\altpi_\star(\vy \mid \vx)$ as before in \cref{eq:distr_over_optimal_trajectories}.
    Show that for any context $\vx$, \begin{align}
      \argmin_{\vvarphi} \KL{\altpi_{\vvarphi}(\cdot \mid \vx)}{\altpi_\star(\cdot \mid \vx)} = \argmax_{\vvarphi} \j{\vvarphi; \vvarphi^{\init} \mid \vx}[\lambda].
    \end{align}

    \item Conclude that the policy maximizing $\j{\vvarphi; \vvarphi^{\init} \mid \vx}[\lambda]$ is \begin{align}
      \altpi_\star(\vy \mid \vx) \propto \altpi_{\init}(\vy \mid \vx) \exp\parentheses*{\frac{1}{\lambda} r(\vy \mid \vx)}.
    \end{align}
  \end{enumerate}
\end{nexercise}
