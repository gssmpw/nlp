\chapter{Mathematical Background}\label{sec:mathematical_background}

\section{Probability}









\subsection{Common discrete distributions}\label{sec:background:probability:common_discrete_distributions}

\begin{itemize}
  \item \midx<Bernoulli>{Bernoulli distribution} \quad $\Bern{p}$ describes (biased) coin flips.
  Its domain is ${\Omega = \{0,1\}}$ where $1$ corresponds to heads and $0$ corresponds to tails. $p \in [0,1]$ is the probability of the coin landing heads, that is, ${\Pr{X = 1} = p}$ and ${\Pr{X = 0} = 1 - p}$.
  \item \midx<Binomial>{binomial distribution} \quad $\Bin{n}{p}$ counts the number of heads in $n$ independent Bernoulli trials, each with probability of heads $p$.
  \item \midx<Categorical>{categorical distribution} \quad $\mathrm{Cat}(m, p_1, \dots, p_m)$ is a generalization of the Bernoulli distribution and represents throwing a (biased) $m$-sided die.
  Its domain is ${\Omega = [m]}$ and we have ${\Pr{X = i} = p_i}$. We require $p_i \geq 0$ and $\sum_{i \in [m]} p_i = 1$.
  \item \midx<Multinomial>{multinomial distribution} \quad $\mathrm{Mult}(n, m, p_1, \dots, p_m)$ counts the number of outcomes of each side in $n$ independent Categorical trials.
  \item \midx<Uniform>{uniform distribution} \quad $\Unif{\sS}$ assigns identical probability mass to all values in the set $\sS$. That is, $\Pr{X = x} = \frac{1}{|\sS|} \; (\forall x \in \sS)$.
\end{itemize}

\subsection{Probability simplex}\label{sec:background:probability:probability_simplex}

We denote by $\Delta^m$ the set of all categorical distributions on $m$ classes. Observe that $\Delta^m$ is an ${(m-1)}$-dimensional convex polytope.

To see this, let us consider the $m$-dimensional space of probabilities~$[0,1]^m$.
It follows from our characterization of the categorical distribution in \cref{sec:background:probability:common_discrete_distributions} that there is a one-to-one correspondence between probability distributions over $m$ classes and points in the space~$[0,1]^m$ where all coordinates sum to one.
This $(m-1)$-dimensional subspace of~$[0,1]^m$ is also known as the \midx{probability simplex}[idxpagebf].

\subsection{Universality of the Uniform and Sampling}\label{sec:inverse_transform_sampling}

Sampling from a continuous random variable is a common and non-trivial problem.
A family of techniques is motivated by the following general property of the uniform distribution, colloquially known as the ``universality of the uniform''.

\begin{marginbox}{Uniform distribution}
  The (continuous) \midx{uniform distribution}[idxpagebf] $\Unif{[a,b]}$ is the only distribution that assigns constant density to all points in the support $[a,b]$. That is, it has PDF \begin{align*}
    p(u) = \begin{cases}
      \frac{1}{b - a} & \text{if $u \in [a,b]$} \\
      0 & \text{otherwise} \\
    \end{cases}
  \end{align*} and CDF \begin{align*}
    P(u) = \begin{cases}
      \frac{u - a}{b - a} & \text{if $u \in [a,b]$} \\
      0 & \text{otherwise}. \\
    \end{cases}
  \end{align*}
\end{marginbox}

First, given a random variable $X \sim P$, we call \begin{align}
  \inv{P}(u) \defeq \min\{x \mid P(x) \geq u\} \quad\text{for all $0 < u < 1$}.
\end{align} the \midx{quantile function} of $X$.
That is, $\inv{P}(u)$ corresponds to the value $x$ such that the probability of $X$ being at most $x$ is $u$.
If the CDF $P$ is invertible, then $\inv{P}$ coincides with the inverse of $P$.

\begin{thm}[Universality of the uniform]\pidx{universality of the uniform}[idxpagebf]
  If $U \sim \Unif{[0,1]}$ and $P$ is invertible, then $\inv{P}(U) \sim P$.
\end{thm}
\begin{proof}
  Let $U \sim \Unif{[0,1]}$. Then, \begin{align*}
    \Pr{\inv{P}(U) \leq x} = \Pr{U \leq P(x)} = P(x). \qedhere \margintag{using $\Pr{U \leq u} = u$ if $U \sim \Unif{[0,1]}$}
  \end{align*}
\end{proof}

This implies that if we are able to sample from $\Unif{[0,1]}$,\safefootnote{This is done in practice using so-called \midx{pseudo-random number generators}.} then we are able to sample from any distribution with invertible CDF.
This method is known as \midx{inverse transform sampling}.

In the case of Gaussians, we learned that the CDF cannot be expressed in closed-form (and hence, is not invertible), however, for practical purposes, the quantile function of Gaussians can be approximated well.

\subsection{Point densities and Dirac's delta function}\label{sec:background:probability:dirac_delta}

The \midx{Dirac delta function} $\delta_\alpha$ is a function satisfying $\delta_\alpha(x) = 0$ for any~${x \neq \alpha}$ and $\int \delta_\alpha(x) \,dx = 1$.
$\delta_\alpha$ is also called a \midx{point density} at $\alpha$.

As an example, let us consider the random variable $Y = g(X)$, which is defined in terms of another random variable $X$ and a continuously differentiable function $g : \R \to \R$.
Using the sum rule and product rule, we can express the PDF of $Y$ as \begin{align}
  p(y) = \int p(y \mid x) \cdot p(x) \,d x = \int \delta_{g(x)}(y) \cdot p(x) \,d x.
\end{align}

In \cref{sec:fundamentals:probability:cov}, we discuss how one can obtain an explicit representation of $p_Y$ using the ``change of variables'' formula.

\subsection{Gradients of expectations}\label{sec:background:probability:gradients_of_expectations}

We often encounter gradients of expectations, $\grad_\vtheta \E[\rX]{f(\rX,\vtheta)}$.

\begin{marginbox}{Gradient}
  The \idx{gradient} of a function $f : \R^n \to \R$ at a point $\vx \in \R^n$ is \begin{align}
    \grad f(\vx) \defeq \transpose{\begin{bmatrix}
      \pdv{f(\vx)}{\vx(1)} & \cdots & \pdv{f(\vx)}{\vx(n)} \\
   \end{bmatrix}}.
  \end{align}
\end{marginbox}

\begin{fct}[Differentiation under the integral sign]\pidx{differentiation under the integral sign}
  Let $f(\vx, t)$ be differentiable in $\vx$, integrable in $t$, and be such that \begin{align*}
    \abs{\pdv{f(\vx, t)}{t}} \leq g(\vx)
  \end{align*} for some integrable function $g$.
  Then, if the distribution of $\rX$ is not parameterized by $t$,\safefootnote{That is, $\E{f(\rX, t)} = \int p(\vx) \cdot f(\vx, t) \,d\vx$ and $p(\vx)$ does not depend on $t$.} \begin{align}
    \pdv{\E{f(\rX, t)}}{t} = \E{\pdv{f(\rX, t)}{t}}.
  \end{align}
\end{fct}

Therefore, if the distribution of $\rX$ is not parameterized by $\vtheta$, \begin{align}
  \grad_\vtheta \E{f(\rX,\vtheta)} = \E{\grad_\vtheta f(\rX,\vtheta)}. \label{eq:swap_grad_exp_order}
\end{align}

\section{Quadratic Forms and Gaussians}\label{sec:fundamentals:qf}

\begin{defn}[Quadratic form]\pidx{quadratic form}
  Given a symmetric matrix $\mA \in \R^{n \times n}$, the \emph{quadratic form} induced by $\mA$ is defined as \begin{align}
    f_\mA : \R^n \to \R, \quad \vx \mapsto \transpose{\vx} \mA \vx = \sum_{i=1}^n \sum_{j=1}^n \mA(i,j) \cdot \vx(i) \cdot \vx(j). \label{eq:quadratic_form}
  \end{align}
\end{defn}

We call $\mA$ a \midx{positive definite} matrix if all eigenvalues of $\mA$ are positive.
Equivalently, we have $f_\mA(\vx) > 0$ for all $\vx \in \R^n \setminus \{\vzero\}$ and $f_\mA(\vzero) = 0$.
Similarly, $\mA$ is called \midx{positive semi-definite} if all eigenvalues of $\mA$ are non-negative, or equivalently, if $f_\mA(\vx) \geq 0$ for all $\vx \in \R^n$.
In particular, if $\mA$ is positive definite then $\sqrt{f_\mA(\vx)}$ is a norm (called the \midx{Mahalanobis norm} induced by $\mA$), and is often denoted by $\norm{\vx}_\mA$.

If $\mA$ is positive definite, then the sublevel sets of its induced quadratic form $f_\mA$ are convex ellipsoids.
Not coincidentally, the same is true for the sublevel sets of the PDF of a normal distribution $\N{\vmu}{\mSigma}$, which we have seen an example of in \cref{fig:multivariate_normal}. Hereby, the axes of the ellipsoid and their corresponding squared lengths are the eigenvectors and eigenvalues of $\mSigma$, respectively.

\begin{rmk}{Correspondence of quadratic forms and Gaussians}{qf_and_gaussians}
  Observe that the PDF of a zero-mean multivariate Gaussian $\N{\vzero}{\mSigma}$ is an exponentiated and appropriately scaled quadratic form induced by the positive definite precision matrix $\inv{\mSigma}$.
  The constant factor is chosen such that the resulting function is a valid probability density function, that is, sums to one.
\end{rmk}

One important property of positive definiteness of $\mSigma$ is that $\mSigma$ can be decomposed into the product of a lower-triangular matrix with its transpose.
This is known as the \emph{Cholesky decomposition}.

\begin{fct}[Cholesky decomposition, symmetric matrix-form]\pidx{Cholesky decomposition}
  For any symmetric and positive (semi-)definite matrix $\mA \in \R^{n \times n}$, there is a decomposition of the form \begin{align}
    \mA = \mCalL\transpose{\mCalL} \label{eq:cholesky}
  \end{align} where $\mCalL \in \R^{n \times n}$ is lower triangular and positive (semi-)definite.
\end{fct}

We will not prove this fact, but it is not hard to see that a decomposition exists (it takes more work to show that $\mCalL$ is lower-triangular).

Let $\mA$ be a symmetric and positive (semi-)definite matrix.
By the spectral theorem of symmetric matrices, $\mA = \mV\mLambda\transpose{\mV}$, where $\mLambda$ is a diagonal matrix of eigenvalues and $\mV$ is an orthonormal matrix of corresponding eigenvectors.
Consider the matrix \begin{align}
  \msqrt{\mA} \defeq \mV \msqrt{\mLambda} \transpose{\mV} \label{eq:matrix_sqrt}
\end{align} where $\msqrt{\mLambda} \defeq \diag{\sqrt{\mLambda(i, i)}}$, also called the \midx{square root} of $\mA$.
Then, \begin{align}
  \msqrt{\mA} \msqrt{\mA} &= \mV \msqrt{\mLambda} \transpose{\mV} \mV \msqrt{\mLambda} \transpose{\mV} \nonumber \\
  &= \mV \msqrt{\mLambda} \msqrt{\mLambda} \transpose{\mV} \nonumber \\
  &= \mV \mLambda \transpose{\mV} = \mA.
\end{align}
It is immediately clear from the definition that $\msqrt{\mA}$ is also symmetric and positive (semi-)definite.

Quadratic forms of positive semi-definite matrices are a generalization of the Euclidean norm, as \begin{align}
  \norm{\vx}_{\mA}^2 = \transpose{\vx} \mA \vx = \transpose{\vx} \msqrt{\mA} \msqrt{\mA} \vx = \transpose{(\msqrt{\mA} \vx)} \msqrt{\mA} \vx = \norm*{\msqrt{\mA} \vx}_2^2,
\end{align} and in particular, \begin{align}
  \log \N[\vx]{\vmu}{\mSigma} &= -\frac{1}{2}\norm*{\vx-\vmu}_{\inv{\mSigma}}^2 + \const \\
  &= -\frac{1}{2} \brackets*{\transpose{\vx}\inv{\mSigma}\vx - 2 \transpose{\vmu}\inv{\mSigma}\vx} + \const, \label{eq:gaussian_propto} \\
  \log \SN[\vx] &= -\frac{1}{2}\norm{\vx}_2^2 + \const. \label{eq:isotropic_gaussian_propto}
\end{align}

\section{Parameter Estimation}\label{sec:background:parameter_estimation}

In this section, we provide a more detailed account of parameter estimation as outlined in \cref{sec:fundamentals:supervised_learning}.

\subsection{Estimators}\label{sec:fundamentals:parameter_esitmation:estimators}

Suppose we are given a collection of independent samples $\vx_1,\dots,\vx_n$ from some random vector $\rX$.
Often, the exact distribution of $\rX$ is unknown to us, but we still want to ``estimate'' some property of this distribution, for example its mean.
We denote the property that we aim to estimate from our sample by $\opt{\vtheta}$.
For example, if our goal is estimating the mean of $\rX$, then $\opt{\vtheta} \defeq \E{\rX}$.

An \midx{estimator} for a parameter $\opt{\vtheta}$ is a random vector $\vthetahat_n$ that is a function of $n$ sample variables $\rX_1,\dots,\rX_n$ whose distribution is identical to the distribution of $\rX$.
Any concrete sample $\{\vx_i \sim \rX_i\}_{i=1}^n$ yields a concrete estimate of $\opt{\vtheta}$.

\begin{ex}{Estimating expectations}{estimating_expectations}
  The most common estimator for $\E{\rX}$ is the \midx{sample mean} \begin{align}
    \mean{\rX}_n \defeq \frac{1}{n} \sum_{i=1}^n \rX_i \label{eq:sample_mean}
  \end{align} where $\rX_i \iid \rX$.
  Using a sample mean to estimate an expectation is often also called \emph{Monte Carlo sampling}, and the resulting estimate is referred to as a \emph{Monte Carlo average}.
\end{ex}

\begin{ex}{MLE of Bernoulli random variables}{mle_bern}
  Let us consider an i.i.d. sample $x_{1:n}$ of $\Bern{p}$ distributed random variables.
  We want to estimate the parameter $p$ using a MLE.
  We have, \begin{align}
    \hat{p}_\MLE &= \argmax_p \Pr{x_{1:n} \mid p} \nonumber \\
    &= \argmax_p \sum_{i=1}^n \log \Pr{x_i \mid p} \nonumber \\
    &= \argmax_p \sum_{i=1}^n \log p^{x_i} (1-p)^{1-x_i} \margintag{using the Bernoulli PMF, see \cref{sec:background:probability:common_discrete_distributions}} \nonumber \\
    &= \argmax_p \sum_{i=1}^n x_i \log p + (1-x_i) \log(1-p). \nonumber
  \intertext{Computing the first derivative with respect to $p$, we see that the objective is maximized by}
    &= \frac{1}{n} \sum_{i=1}^n x_i.
  \end{align}
  Thus, the maximum likelihood estimate for the parameter $p$ of $X \sim \Bern{p}$ coincides with the sample mean $\mean{X}_n$.
\end{ex}

What does it mean for $\vthetahat_n$ to be a ``good estimate'' of $\opt{\vtheta}$?
There are two canonical measures of goodness of an estimator: its bias and its variance.

Clearly, we want $\E{\vthetahat_n} = \opt{\vtheta}$.
Estimators that satisfy this property are called \midx<unbiased>{unbiased estimator}.
The \midx{bias}, $\E[\vthetahat_n]{\vthetahat_n-\opt{\vtheta}}$, of an unbiased estimator is $\vzero$.
It follows directly from linearity of expectation \eqref{eq:linearity_expectation} that the sample mean is unbiased.
In \cref{sec:fundamentals:mc_approx}, we will see that the variance of the sample mean is small for reasonably large $n$ for ``light-tailed'' distributions.

\begin{ex}{Estimating variances}{estimating_variances}
  Analogously to the sample mean, the most common estimator for the covariance matrix $\Var{\rX}$ is the \midx{sample variance} (also called \midx{sample covariance matrix}) \begin{align}
    \mS_n^2 &\defeq \frac{1}{n-1} \sum_{i=1}^n (\rX_i - \mean{\rX}_n) \transpose{(\rX_i - \mean{\rX}_n)} \label{eq:sample_variance} \\
    &= \frac{n}{n-1} \parentheses*{\frac{1}{n} \sum_{i=1}^n \rX_i \transpose{{\rX_i}} - \mean{\rX}_n \transpose{\mean{\rX}_n}} \label{eq:sample_variance2}
  \end{align} where $\rX_i \iid \rX$.
  It can be shown that the sample variance is unbiased \exerciserefmark{sample_variance}.
  Intuitively, the normalizing factor is $\nicefrac{1}{n-1}$ because $\mS_n^2$ depends on the sample mean $\mean{\rX}_n$, which is obtained using the same samples.
  For this reason, $\mS_n^2$ has $n-1$ degrees of freedom.\safefootnote{That is, any sample $\rX_i$ can be recovered using all other samples and the sample mean.}
\end{ex}

The second desirable property of an estimator $\vthetahat_n$ is that its variance is small.\footnote{The variance of estimators vector-valued estimators $\vthetahat_n$ is typically studied component wise.}
A common measure for the variance of an estimator of $\theta$ is the \midx{mean squared error}[idxpagebf], \begin{align}
  \mathrm{MSE}(\hat{\theta}_n) \defeq \E[\hat{\theta}_n]{(\hat{\theta}_n-\opt{\theta})^2}. \label{eq:mse}
\end{align}
The mean squared error can be decomposed into the estimator's bias and variance: \begin{align}
  \mathrm{MSE}(\hat{\theta}_n) &= \E[\hat{\theta}_n]{(\hat{\theta}_n - \opt{\theta})^2} = \Var{\hat{\theta}_n} + (\E{\hat{\theta}_n} - \opt{\theta})^2, \margintag{using \eqref{eq:variance2} and $\Var[\hat{\theta}_n]{\hat{\theta}_n - \opt{\theta}} = \Var{\hat{\theta}_n}$} \label{eq:mse_bias_variance}
\end{align} the mean squared error of an estimator can be written as the sum of its variance and its squared bias.

A desirable property is for $\hat{\theta}_n$ to converge to $\opt{\theta}$ in probability \eqref{eq:p_convergence}: \begin{align*}
  \forall \epsilon > 0: \quad \lim_{n\to\infty} \Pr{\abs{\hat{\theta}_n - \opt{\theta}} > \epsilon} = 0.
\end{align*}
Such estimators are called \midx<consistent>{consistent estimator}, and a sufficient condition for consistency is that the mean squared error converges to zero as $n \to \infty$.\footnote{It follows from Chebyshev's inequality \eqref{eq:chebyshev_inequality} that $\Pr{\abs{\hat{\theta}_n - \opt{\theta}} > \epsilon} \leq \frac{\mathrm{MSE}(\hat{\theta}_n)}{\epsilon^2}$.}

Consistency is an asymptotic property.
In practice, one would want to know ``how quickly'' the estimator converges as $n$ grows.
To this end, an estimator $\hat{\theta}_n$ is said to be \midx{sharply concentrated} around $\opt{\theta}$ if \begin{align}
  \forall \epsilon > 0: \quad \Pr{\abs{\hat{\theta}_n - \opt{\theta}} > \epsilon} \leq \exp(-\Omega(\epsilon)),
\end{align} where $\Omega(\epsilon)$ denotes the class of functions that grow at least linearly in~$\epsilon$.\footnote{That is, $h \in \Omega(\epsilon)$ if and only if $\lim_{\epsilon \to \infty} \frac{h(\epsilon)}{\epsilon} > 0$. With slight abuse of notation, we force $h$ to be positive (so as to ensure that the argument to the exponential function is negative) whereas in the traditional definition of Landau symbols, $h$ is only required to grow linearly in absolute value.}
Thus, if an estimator is sharply concentrated, its absolute error is bounded by an exponentially quickly decaying error probability.

\subsection{Heavy Tails}\label{sec:fundamentals:parameter_esitmation:heavy_tails}

It is often said that a sharply concentrated estimator $\hat{\theta}_n$ has \midx{small tails}, where ``tails'' refer to the ``ends'' of a PDF.
Let us examine the difference between a \midx<light-tailed>{light-tailed distribution} and a \midx<heavy-tailed>{heavy-tailed distribution} distribution more closely.

\begin{marginfigure}
  \incplt{heavy_tails}
	\caption{Shown are the right tails of the PDFs of a \textbf{Gaussian} with mean $1$ and variance $1$, a \textbf{\b{exponential distribution}} with mean $1$ and parameter $\lambda=1$, and a \textbf{\r{log-normal distribution}} with mean $1$ and variance $1$ on a log-scale.}
\end{marginfigure}

\begin{defn}
  A distribution $P_X$ is said to have a \emph{heavy (right) tail} if its \midx{tail distribution function} \begin{align}
    \mean{P}_X(x) \defeq 1 - P_X(x) = \Pr{X > x}
  \end{align} decays slower than that of the exponential distribution, that is, \begin{align}
    \limsup_{x\to\infty} \frac{\mean{P}_X(x)}{e^{-\lambda x}} = \infty
  \end{align} for all $\lambda > 0$.
  When $\limsup_{x\to\infty} \frac{\mean{P}_X(x)}{\mean{P}_Y(x)} > 1$, the (right) tail of $X$ is said to be \emph{heavier} than the (right) tail of $Y$.
\end{defn}

It is immediate from the definitions that the distribution of an unbiased estimator is light-tailed if and only if the estimator is sharply concentrated, so both notions are equivalent.

\begin{marginfigure}[13\baselineskip]
  \incplt{laplace_vs_normal_tails}
	\caption{Shown are the right tails of the PDFs of a \textbf{\r{Laplace distribution}} with mean $0$ and length scale $1$ and a \textbf{\b{Gaussian}} with mean $0$ and variance $1$.}
\end{marginfigure}

\begin{ex}{Light- and heavy-tailed distributions}{}
  \begin{itemize}
    \item A Gaussian $X \sim \N{0}{1}$ is light-tailed since its tail distribution function is bounded by \begin{align}
      \mean{P}_X(x) = \int_x^\infty \frac{1}{\sqrt{2 \pi}} e^{- \nicefrac{t^2}{2}} \,d t \leq \int_x^\infty \frac{t}{x} \frac{1}{\sqrt{2 \pi}} e^{- \nicefrac{t^2}{2}} \,d t = \frac{e^{-\nicefrac{x^2}{2}}}{\sqrt{2\pi}x}. \margintag{using $\frac{t}{x} \geq 1$}
    \end{align}

    \item The \midx{Laplace distribution} with PDF \begin{align}
      \Laplace[x]{\mu}{h} \propto \exp\parentheses*{-\frac{\abs{x - \mu}}{h}}
    \end{align} is light-tailed, but its tails decay slower than those of the Gaussian.
    It is also ``more sharply peaked'' at its mean than the Gaussian.\looseness=-1
  \end{itemize}

  Heavy-tailed distributions frequently occur in many domains, the following just serving as a few examples.
  \begin{itemize}
    \item A well-known example of a heavy-tailed distribution is the \midx{log-normal distribution}.
    A random variable $X$ is logarithmically normal distributed with parameters $\mu$ and $\sigma^2$ if $\log X \sim \N{\mu}{\sigma^2}$.
    The log-normal arises when modeling natural growth phenomena or stock prices which are multiplicative, and hence, become additive on a logarithmic scale.

    \item The \midx{Pareto distribution} was originally used to model the distribution of wealth in a society, but it is also used to model many other phenomena such as the size of cities, the frequency of words, and the returns on stocks.
    Formally, the Pareto distribution is defined by the following PDF, \begin{align}
      \Pareto[x]{\alpha}{c} = \frac{\alpha c^\alpha}{x^{\alpha+1}} \Ind{x \geq c}, \quad x \in \R \label{eq:pareto_distr}
    \end{align} where the \midx{tail index} $\alpha > 0$ models the ``weight'' of the right tail of the distribution (larger $\alpha$ means lighter tail) and $c > 0$ corresponds to a cutoff threshold.
    The distribution is supported on $[c, \infty)$, and as $\alpha \to \infty$ it approaches a point density at $c$.
    The Pareto's right tail is $\mean{P}(x) = (\frac{c}{x})^{\alpha}$ for all $x \geq c$.

    \item The \midx{Cauchy distribution} arises in the description of numerous physical phenomena.
    The CDF of a Cauchy distribution is \begin{align}
      P_X(x) \defeq \frac{1}{\pi} \arctan\parentheses*{\frac{x - m}{\tau}} + \frac{1}{2}
    \end{align} where $X \sim \Cauchy{m}{\tau}$ with location $m$ and scale $\tau > 0$.
  \end{itemize}
\end{ex}

Light- and heavy-tailed distributions exhibit dramatically different behavior.
For example, consider the sample mean $\mean{X}_{n}$ of $n$ i.i.d. random variables $X_i$ with mean $1$, and we are told that the sample mean is $5$.
What does this tell us about the (conditional) distribution of $X_1$?
There are many possible explanations for this observation, but they can be largely grouped into two categories: \begin{enumerate}
  \item either \emph{many} $X_i$ are \emph{slightly} larger than $1$,
  \item or \emph{very few} $X_i$ are \emph{much} larger than $1$.
\end{enumerate}
\cite{nair2022fundamentals} term interpretation (1) the \midx{conspiracy principle} and interpretation (2) the \midx{catastrophe principle}.
It turns out that which of the two principles applies depends on the tails of the distribution of $X^{(1)}$.
If the tails are light, then the conspiracy principle applies, and if the tails are heavy, then the catastrophe principle applies as is illustrated in \cref{fig:conspiracy_vs_catastrophe}.

\begin{figure}
  \incplt{conspiracy_vs_catastrophe}
	\caption{Shown are the conditional distributions of $X_1$ given the event that the sample mean across two samples is surprisingly large: $\mean{X}_2 = 5$. We plot the examples where $X$ is a \textbf{\b{light-tailed}} Gaussian with mean $1$, $X$ is a \textbf{\r{heavy-tailed}} log-normal with mean $1$, and $X$ is \textbf{exponential} with mean $1$. When $X$ is \emph{light-tailed}, the large mean is explained by ``conspiratory'' samples $X_1$ and $X_2$. In contrast, when $X$ is \emph{heavy-tailed}, the large mean is explained by a single ``catastrophic'' event. See also \cite{nair2022fundamentals}.}
  \label{fig:conspiracy_vs_catastrophe}
\end{figure}

When working with light-tailed distributions, the conspiracy principle tells us that outliers are rare, and hence, we can usually ignore them.
In contrast, when working with heavy-tailed distributions, the catastrophe principle tells us that outliers are not just common but a defining feature, and hence, we need to be careful to not ignore them.

\begin{readings}
  For more details and examples of heavy-tailed distributions, refer to \icite{nair2022fundamentals}.
\end{readings}

\subsection{Mean Estimation and Concentration}\label{sec:fundamentals:mc_approx}%

We have seen that we desire two properties in estimators: namely, (1) that they are unbiased; and (2) that their variance is small.\footnote{That is, they are consistent and, ideally, their variance converges quickly.}
For estimating expectations with the sample mean $\mean{X}_n$ which is also known as a \midx{Monte Carlo approximation}[idxpagebf], we will see that both properties follow from standard results in statistics.

We have already concluded that the sample mean \eqref{eq:sample_mean} is an unbiased estimator for $\E*{X}$.
We will now see that the sample mean is also consistent, that its limiting distribution can usually be determined explicitly, and in some cases it is even sharply concentrated.
First, let us recall the common notions of convergence of sequences of random variables.

\begin{defn}[Convergence of random variables]\label{def:rv_convergence}
  Let $\{X_n\}_{n\in\Nat}$ be a sequence of random variables and $X$ another random variable. We say that, \begin{enumerate}
    \item $X_n$ converges to $X$ \midx<almost surely>{almost sure convergence} (also called \midx{convergence with probability $1$}) if \begin{align}
      \Pr{\braces*{\omega \in \Omega : \lim_{n\to\infty} X_n(\omega) = X(\omega)}} = 1, \label{eq:as_convergence}
    \end{align} and we write $X_n \almostsurely X$ as $n\to\infty$.

    \item $X_n$ converges to $X$ \midx<in probability>{convergence in probability} if for any $\epsilon > 0$, \begin{align}
      \lim_{n\to\infty} \Pr{\abs{X_n - X} > \epsilon} = 0, \label{eq:p_convergence}
    \end{align} and we write $X_n \convp X$ as $n\to\infty$.

    \item $X_n$ converges to $X$ \midx<in distribution>{convergence in distribution} if for all points $x \in X(\Omega)$ at which $P_X$ is continuous, \begin{align}
      \lim_{n\to\infty} P_{X_n}(x) = P_X(x), \label{eq:d_convergence}
    \end{align} and we write $X_n \convd X$ as $n\to\infty$.
  \end{enumerate}

  It can be shown that as $n\to\infty$, \begin{align}
    X_n \almostsurely X \implies X_n \convp X \implies X_n \convd X.
  \end{align}
\end{defn}

\begin{rmk}{Convergence in mean square}{mean_square_convergence}
  Mean square continuity and mean square differentiability are a generalization of continuity and differentiation to random processes, where the limit is generalized to a limit in the ``mean square sense''.

  A sequence of random variables $\{X_n\}_{n \in \Nat}$ is said to converge to the random variable $X$ in \midx<mean square>{mean square convergence} if \begin{align}
    \lim_{n\to\infty} \E{(X_n - X)^2} = 0.
  \end{align}
  Using Markov's inequality \eqref{eq:markov_inequality} it can be seen that convergence in mean square implies convergence in probability.
  Moreover, convergence in mean square implies \begin{align}
    \lim_{n\to\infty} \E*{X_n} &= \E*{X} \quad\text{and} \\
    \lim_{n\to\infty} \E*{X_n^2} &= \E*{X^2}.
  \end{align}

  Whereas a deterministic function $f(\vx)$ is said to be continuous at a point $\vxs$ if $\lim_{\vx\to\vxs} f(\vx) = f(\vxs)$, a random process $f(\vx)$ is \midx<mean square continuous>{mean square continuity} at $\vxs$ if \begin{align}
    \lim_{\vx\to\vxs} \E{(f(\vx) - f(\vxs))^2} = 0.
  \end{align}
  It can be shown that a random process is mean square continuous at $\vxs$ iff its kernel function $k(\vx, \vxp)$ is continuous at $\vx = \vxp = \vxs$.

  Similarly, a random process $f(\vx)$ is \midx<mean square differentiable>{mean square differentiability} at a point $\vx$ in direction $i$ if $(f(\vx + \delta \ve_i) - f(\vx)) / \delta$ converges in mean square as $\delta \to 0$ where $\ve_i$ is the unit vector in direction $i$.
  This notion can be extended to higher-order derivatives.

  For the precise notions of mean square continuity and mean square differentiability in the context of Gaussian processes, refer to section 4.1.1 of \icite{gpml}.
\end{rmk}

Given a random variable $X : \Omega \to \R$ with finite variance, it can be shown that \begin{align}
  \mean{X}_n \convp \E*{X} \label{eq:wlln}
\end{align} which is known as the \midx{weak law of large numbers} (WLLN) and which establishes consistency of the sample mean \exerciserefmark{concentration_inequalities}.
Using more advanced tools, it is possible to show almost sure convergence of the sample mean even when the variance is infinite:

\begin{thmb}
  \begin{fct}[Strong law of large numbers, SLLN]\pidx{law of large numbers}
    Given the random variable $X : \Omega \to \R$ with finite mean.
    Then, as $n\to\infty$, \begin{align}
      \mean{X}_n \almostsurely \E*{X}. \label{eq:slln}
    \end{align}
  \end{fct}
\end{thmb}

To get an idea of how quickly the sample mean converges, we can look at its variance: \begin{align}
  \Var{\mean{X}_n} = \Var{\frac{1}{n} \sum_{i=1}^n X_i} = \frac{\Var{X}}{n}.
\end{align}
Remarkably, one cannot only compute its variance, but also its limiting distribution.
This is known as the \midx{central limit theorem} (CLT) which states that the prediction error of the sample mean tends to a normal distribution as the sample size goes to infinity (even if the samples themselves are not normally distributed).

\begin{thmb}
  \begin{fct}[Central limit theorem by Lindeberg-Lévy]
    Given the random variable $X : \Omega \to \R$ with finite mean and finite variance.
    Then, \begin{align}
      \mean{X}_n \convd \N*{\E*{X}}{\frac{\Var{X}}{n}}
    \end{align} as $n\to\infty$.
  \end{fct}
\end{thmb}

The central limit theorem makes the critical assumption that the variance of $X$ is finite.
This is not the case for many heavy-tailed distributions such as the Pareto or Cauchy distributions.
One can generalize the central limit theorem to distributions with infinite variance, with the important distinction that their limiting distribution is no longer a Gaussian but also a heavy-tailed distribution with infinite variance~\citep{nair2022fundamentals}.

For a subclass of light-tailed distributions it is also possible to show that the sample mean $\mean{X}_n$ is sharply concentrated, which is a much stronger and much more practical property than consistency.
We will consider the class of sub-Gaussian random variables which encompass random variables whose tail probabilities decay at least as fast as those of a Gaussian.

\begin{defn}[Sub-Gaussian random variable]\pidx{sub-Gaussian random variable}
  A random variable $X : \Omega \to \R$ is called $\sigma$-\emph{sub-Gaussian} for $\sigma > 0$ if for all $\lambda \in \R$, \begin{align}
    \E{e^{\lambda (X - \E*{X})}} \leq \exp\parentheses*{\frac{\sigma^2 \lambda^2}{2}}. \label{eq:sub_gaussian}
  \end{align}
\end{defn}

\begin{ex}{Examples of sub-Gaussian random variables}{sub_gaussian_examples}
  \leavevmode\begin{itemize}
    \item If a random variable $X$ is $\sigma$-sub-Gaussian, then so is $-X$.

    \item If $X \sim \N{\mu}{\sigma^2}$ then a simple calculation shows that \begin{align}
      \varphi_X(\lambda) \defeq \E{e^{\lambda X}} = \exp\parentheses*{\mu \lambda + \frac{\sigma^2 \lambda^2}{2}} \qquad\text{for any $\lambda \in \R$} \label{eq:mgf_univ_gaussian}
    \end{align} which is called the ``moment-generating function'' of the normal distribution, and implying that $X$ is $\sigma$-sub-Gaussian.

    \item If the random variable $X : \Omega \to \R$ satisfies $a \leq X \leq b$ with probability $1$ then $X$ is $\frac{b-a}{2}$-sub-Gaussian.
    This is also known as \midx{Hoeffding's lemma}.
  \end{itemize}
\end{ex}

\begin{thmb}
  \begin{thm}[Hoeffding's inequality]\pidx{Hoeffding's inequality}\label{thm:hoeffdings_inequality}
    Let $X : \Omega \to \R$ be a $\sigma$-sub-Gaussian random variable.
    Then, for any $\epsilon > 0$, \begin{align}
      \Pr{\abs{\mean{X}_n - \E*{X}} \geq \epsilon} \leq 2 \exp\parentheses*{-\frac{n \epsilon^2}{2 \sigma^2}}. \label{eq:hoeffdings_inequality}
    \end{align}
  \end{thm}
\end{thmb}

In words, the absolute error of the sample mean is bounded by an exponentially quickly decaying error probability $\delta$.
Solving for $n$, we obtain that for \begin{align}
  n \geq \frac{2 \sigma^2}{\epsilon^2} \log \frac{2}{\delta} \label{eq:eq:hoeffdings_inequality_sample_size}
\end{align} the probability that the absolute error is greater than $\epsilon$ is at most $\delta$.

\begin{proof}[Proof of \cref{thm:hoeffdings_inequality}]
  Let $S_n \defeq n \mean{X}_n = X_1 + \dots + X_n$.
  We have for any $\lambda, \epsilon > 0$ that \begin{align*}
    \Pr{\mean{X}_n - \E*{X} \geq \epsilon} &= \Pr{S_n - \E*{S_n} \geq n \epsilon} \\
    &= \Pr{e^{\lambda(S_n - \E*{S_n})} \geq e^{n \epsilon \lambda}} \margintag{using that $z \mapsto e^{\lambda z}$ is increasing} \\
    &\leq e^{-n\epsilon\lambda} \E*{[e^{\lambda(S_n - \E*{S_n})}]} \margintag{using Markov's inequality \eqref{eq:markov_inequality}} \\
    &= e^{-n\epsilon\lambda} \prod_{i=1}^n \E*{[e^{\lambda(X_i - \E*{X})}]} \margintag{using independence of the $X_i$} \\
    &\leq e^{-n\epsilon\lambda} \prod_{i=1}^n e^{\sigma^2 \lambda^2 / 2} \margintag{using the characterizing property of a $\sigma$-sub-Gaussian random variable \eqref{eq:sub_gaussian}} \\
    &= \exp\parentheses*{-n\epsilon\lambda + \frac{n \sigma^2 \lambda^2}{2}}.
  \end{align*}
  Minimizing the expression with respect to $\lambda$, we set $\lambda = \epsilon /\sigma^2$, and obtain \begin{align*}
    \Pr{\mean{X}_n - \E*{X} \geq \epsilon} \leq \min_{\lambda > 0}\braces*{\exp\parentheses*{-n\epsilon\lambda + \frac{n \sigma^2 \lambda^2}{2}}} = \exp\parentheses*{-\frac{n \epsilon^2}{2 \sigma^2}}.
  \end{align*}
  The theorem then follows from \begin{align*}
    \Pr{\abs{\mean{X}_n - \E*{X}} \geq \epsilon} = \Pr{\mean{X}_n - \E*{X} \geq \epsilon} + \Pr{\mean{X}_n - \E*{X} \leq -\epsilon}
  \end{align*} and noting that the second term can be bounded analogously to the first term by considering the random variables $-X_1, \dots, -X_n$.
\end{proof}

The law of large numbers and Hoeffding's inequality tell us that when~$X$ is light-tailed, we can estimate $\E*{X}$ very precisely with ``few'' samples using a sample mean.
Crucially, the sample mean requires \emph{independent} samples $x_i$ from $X$ which are often hard to obtain.

\begin{rmk}{When the sample mean fails}{}
  While we have seen that the sample mean works well for light-tailed distributions, it fails for heavy-tailed distributions.
  For example, if we only assume that the variance of $X$ is finite, then the best known error rate is obtained by applying Chebyshev's inequality \eqref{eq:chebyshev_inequality}, \begin{align}
    \Pr{\abs{\mean{X}_n - \E*{X}} \geq \epsilon} \leq \frac{\Var{X}}{n \epsilon^2}
  \end{align} which decays only linearly in $n$.
  This is a result of the catastrophe principle, namely, that outliers may be likely and therefore need to be accounted for.

  Robust methods of estimation in the presence of outliers or corruptions have been studied extensively in \midx{robust statistics}.
  On the subject of mean estimation, approaches such as trimming outliers before taking the sample mean (called the \midx{truncated sample mean}) or using the \midx{median-of-means} which selects the median among $k$ sample means --- each computed on a subset of the data --- have been shown to yield sharply concentrated estimates, even when the variance of $X$ is infinite \citep{bubeck2013bandits}.
\end{rmk}

\subsection{Asymptotic Efficiency of Maximum Likelihood Estimation}\label{sec:fundamentals:parameter_estimation:asymptotic_efficiency}

In \cref{sec:fundamentals:parameter_estimation:mle}, we briefly discussed the asymptotic behavior of the MLE, and we mentioned that the MLE can be shown to be ``asymptotically efficient''.

Let us first address the question of how the asymptotic covariance matrix $\mS_n$ looks like?
It can be shown that $\mS_n = \inv{\mI_n(\vtheta)}$ where \begin{align}
  \mI_n(\vtheta) \defeq \E[\spD_n]{\hes_\vtheta \ell_\mathrm{nll}(\vtheta; \spD_n)}
\end{align} is the so-called \midx{Fisher information} which captures the curvature of the negative log-likelihood around $\vtheta$.
The Fisher information $\mI_n(\vtheta)$ can be used to measure the ``difficulty'' of estimating $\vtheta$ as shown by the \midx{Cramér-Rao lower bound}:

\begin{fct}[Cramér-Rao lower bound]
  Let $\vthetahat_n$ be an unbiased estimator of~$\vtheta$.
  Then,\safefootnote{We use $\mA \succeq \mB$ as shorthand for $\mA - \mB$ being positive semi-definite. This partial ordering of positive semi-definite matrices is called the \midx{Loewner order}.} \begin{align}
    \Var{\vthetahat_n} \succeq \inv{\mI_n(\vtheta)}.
  \end{align}
\end{fct}

An estimator is called \midx{efficient} if it achieves equality in the Cramér-Rao lower bound, and the MLE is therefore \emph{asymptotically efficient}.

\subsection{Population Risk and Empirical Risk}\label{sec:fundamentals:supervised_learning:risk}

The notion of ``error'' mentioned in \cref{sec:fundamentals:supervised_learning} is typically captured by a loss function $\ell(\hat{y}; y) \in \R$ which is small when the prediction $\hat{y} = \hat{f}(\vx)$ is ``close'' to the true label $y = \opt{f}(\vx)$ and large otherwise.
Let us fix a distribution $\spP_\spX$ over inputs, which together with the likelihood from \cref{eq:data} induces an unknown joint distribution over input-label pairs $(\vx, y)$ by $\spP$.
The canonical objective is to best-approximate the mappings $(\vx,y) \sim \spP$, that is, to minimize \begin{align}
  \E[(\vx, y) \sim \spP]{\ell(\hat{f}(\vx); y)}.
\end{align}
This quantity is also called the \midx{population risk}.
However, the underlying distribution $\spP$ is unknown to us.
All that we can work with is the training data for which we assume $\spD_n \iid \spP$.
It is therefore natural to consider minimizing \begin{align}
  \frac{1}{n} \sum_{i=1}^n \ell(\hat{f}(\vx_i); y_i), \quad \spD_n = \{(\vx_i, y_i)\}_{i=1}^n,
\end{align} which is known as the \midx{empirical risk}.

However, selecting $\hat{f}$ by minimizing the empirical risk can be problematic.
The reason is that in this case the model $\hat{f}$ and the empirical risk depend on the same data $\spD_n$, implying that the empirical risk will not be an unbiased estimator of the population risk.
This can result in a model which fits the training data too closely, and which is failing to generalize to unseen data --- a problem called \midx{overfitting}[idxpagebf].
We will discuss some (probabilistic) solutions to this problem in \cref{sec:gp:model_selection} when covering model selection.

\section{Optimization}\label{sec:fundamentals:optimization}

Finding parameter estimates is one of the many examples where we seek to minimize some function $\ell$.\footnote{W.l.o.g. we assume that we want to minimize $\ell$. If we wanted to maximize the objective, we can simply minimize its negation.}
The field of optimization has a rich history, which we will not explore in much detail here.
What will be important for us is that given that the function to be optimized (called the \midx<objective>{objective function} or \midx<loss>{loss function}) fulfills certain smoothness properties, optimization is a well-understood problem and can often be solved exactly (e.g., when the objective is convex) or ``approximately'' when the objective is non-convex.
In fact, we will see that it is often advantageous to frame problems as optimization problems when suitable because the machinery to solve these problems is so extensive.

\begin{readings}
  For a more thorough reminder of optimization methods, read chapter 7 of \icite{mml}.
\end{readings}

\subsection{Stationary Points}

In this section, we derive some basic facts about unconstrained optimization problems.
Given some function $f : \R^n \to \R$, we want to find\looseness=-1 \begin{align}
  \min_{\vx \in \R^n} f(\vx).
\end{align}
We say that a point $\vxs \in \R^n$ is a \midx<(global) optimum>{global optimum} of $f$ if $f(\vxs) \leq f(\vx)$ for any $\vx \in \R^n$.

Consider the more general problem of minimizing $f$ over some subset $\sS \subseteq \R^n$, that is, to minimize the function $f_\sS : \sS \to \R, \vx \mapsto f(\vx)$.
If there exists some open subset $\sS \subseteq \R^n$ including $\vxs$ such that $\vxs$ is optimal with respect to the function $f_\sS$, then $\vxs$ is called a \midx<local optimum>{local optimum} of $f$.

\begin{rmk}{Differentiability}{}
  We will generally assume that $f$ is continuously (Fréchet) differentiable on $\R^n$.
  That is, at any point $\vx \in \R^n$, there exists $\grad f(\vx)$ such that for any $\vdelta \in \R^n$, \begin{align}
    f(\vx + \vdelta) = f(\vx) + \transpose{\grad f(\vx)}\vdelta + o(\norm{\vdelta}_2), \label{eq:first_order_exp}
  \end{align} where $\lim_{\vdelta \to \vzero} \frac{o(\norm{\vdelta}_2)}{\norm{\vdelta}_2} = 0$, and $\grad f(\vx)$ is continuous on $\R^n$.

  \Cref{eq:first_order_exp} is also called a \midx{first-order expansion} of $f$ at $\vx$.
\end{rmk}

\begin{defn}[Stationary point]
  Given a function $f : \R^n \to \R$, a point $\vx \in \R^n$ where $\grad f(\vx) = \vzero$ is called a \midx{stationary point} of $f$.
\end{defn}

\begin{marginfigure}
  \incplt{saddle_point}
  \caption{Example of a saddle point at $x = 0$.}
\end{marginfigure}

Being a stationary point is not sufficient for optimality.
Take for example the point $x \defeq 0$ of $f(x) \defeq x^3$.
Such a point that is stationary but not (locally) optimal is called a \midx{saddle point}.

\begin{thm}[First-order optimality condition]\pidx{first-order optimality condition}
  If $\vx \in \R^n$ is a local extremum of a differentiable function $f : \R^n \to \R$, then $\grad f(\vx) = \vzero$.
\end{thm}
\begin{proof}
  Assume $\vx$ is a local minimum of $f$.
  Then, for all $\vd \in \R^n$ and for all small enough $\lambda \in \R$, we have $f(\vx) \leq f(\vx + \lambda\vd)$, so \begin{align*}
    0 &\leq f(\vx + \lambda\vd) - f(\vx) \\
    &= \lambda \transpose{\grad f(\vx)}\vd + o(\lambda\norm{\vd}_2). \margintag{using a first-order expansion of $f$ around $\vx$}
  \end{align*}
  Dividing by $\lambda$ and taking the limit $\lambda \to 0$, we obtain \begin{align*}
    0 \leq \transpose{\grad f(\vx)}\vd + \lim_{\lambda \to 0}\frac{o(\lambda\norm{\vd}_2)}{\lambda} = \transpose{\grad f(\vx)}\vd.
  \end{align*}
  Take $\vd \defeq - \grad f(\vx)$.
  Then, $0 \leq - \norm{\grad f(\vx)}_2^2$, so $\grad f(\vx) = \vzero$.
\end{proof}

\subsection{Convexity}

Convex functions are a subclass of functions where finding global minima is substantially easier than for general functions.

\begin{marginfigure}
  \incplt{convexity_0}
  \caption{Example of a convex function.
  Any line between two points on $f$ lies ``above'' $f$.}
\end{marginfigure}

\begin{defn}[Convex function]
  A function $f : \R^n \to \R$ is \midx<convex>{convex function} if \begin{align}
    \forall \vx, \vy \in \R^n : \forall \theta \in [0,1] : f(\theta\vx + (1-\theta)\vy) \leq \theta f(\vx) + (1-\theta) f(\vy). \label{eq:convexity}
  \end{align}
  That is, any line between two points on $f$ lies ``above'' $f$.
  If the inequality of \cref{eq:convexity} is strict, we say that $f$ is \midx<strictly convex>{strict convexity}.
\end{defn}

If the function $f$ is convex, we say that the function $-f$ is \midx<concave>{concave function}.
The above is also known as the \emph{$0$th-order characterization of convexity}.

\begin{thm}[First-order characterization of convexity]\label{thm:fo-characterization-convexity}\pidx{first-order characterization of convexity}
  Suppose that $f : \R^n \to \R$ is differentiable, then $f$ is convex if and only if \begin{align}
    \forall \vx, \vy \in \R^n : f(\vy) \geq f(\vx) + \transpose{\grad f(\vx)}(\vy - \vx).
  \end{align}
\end{thm}

\begin{marginfigure}
  \incplt{convexity_1}
  \caption{The first-order characterization characterizes convexity in terms of affine lower bounds.
  Shown is an affine lower bound based at $x = -2$.}
\end{marginfigure}

Observe that the right-hand side of the inequality is an affine function with slope $\grad f(\vx)$ based at $f(\vx)$.

\begin{proof}
  In the following, we will make use of directional derivatives.
  In particular, we will use that given a function $f : \R^n \to \R$ that is differentiable at $\vx \in \R^n$, \begin{align}
    \lim_{\lambda \to 0} \frac{f(\vx + \lambda\vd) - f(\vx)}{\lambda} = \transpose{\grad f(\vx)} \vd
  \end{align} for any direction $\vd \in \R^n$ \exerciserefmark{directional_derivatives}.

  \vspace{5pt}\begin{itemize}
    \item ``$\Rightarrow$'':
    Fix any $\vx, \vy \in \R^n$.
    As $f$ is convex, \begin{align*}
      f((1-\theta)\vx + \theta\vy) \leq (1-\theta)f(\vx) + \theta f(\vy),
    \end{align*} for all $\theta \in [0,1]$.
    We can rearrange to \begin{align*}
      f(\underbrace{(1-\theta)\vx + \theta\vy}_{\vx + \theta(\vy - \vx)}) - f(\vx) \leq \theta(f(\vy) - f(\vx)).
    \end{align*}
    Dividing by $\theta$ yields, \begin{align*}
      \frac{f(\vx + \theta(\vy - \vx)) - f(\vx)}{\theta} \leq f(\vy) - f(\vx).
    \end{align*}
    Taking the limit $\theta \to 0$ on both sides gives the directional derivative at $\vx$ in direction $\vy - \vx$, \begin{align*}
      \transpose{\grad f(\vx)}(\vy - \vx) = D f(\vx)[\vy - \vx] \leq f(\vy) - f(\vx).
    \end{align*}

    \item ``$\Leftarrow$'':
    Fix any $\vx, \vy \in \R^n$ and let $\vz \defeq \theta\vy + (1-\theta)\vx$.
    We have, \begin{align*}
      f(\vy) &\geq f(\vz) + \transpose{\grad f(\vz)}(\vy - \vz), \quad\text{and} \\
      f(\vx) &\geq f(\vz) + \transpose{\grad f(\vz)}(\vx - \vz).
    \end{align*}
    We also have $\vy - \vz = (1-\theta)(\vy-\vx)$ and $\vx - \vz = \theta(\vx - \vy)$.
    Hence, \begin{align*}
      \theta f(\vy) + (1-\theta) f(\vx) &\geq f(\vz) + \transpose{\grad f(\vz)}(\underbrace{\theta(\vy - \vz) + (1-\theta)(\vx - \vz)}_{\vzero}) \\
      &= f(\theta\vy + (1-\theta)\vx). \qedhere
    \end{align*}
  \end{itemize}
\end{proof}

\begin{thmb}
  \begin{thm}
    Let $f : \R^n \to R$ be a convex and differentiable function.
    Then, if $\vxs \in \R^n$ is a stationary point of $f$, $\vxs$ is a global minimum of $f$.
  \end{thm}
\end{thmb}
\begin{proof}
  By the first-order characterization of convexity, we have for any $\vy \in \R^n$, \begin{align*}
    f(\vy) \geq f(\vxs) + \underbrace{\transpose{\grad f(\vxs)}}_{\vzero}(\vy - \vxs) = f(\vxs). &\qedhere
  \end{align*}
\end{proof}

Generally, the main difficulty in solving convex optimization problems lies therefore in finding stationary points (or points that are sufficiently close to stationary points).

\begin{rmk}{Second-order characterization of convexity}{second_order_convexity}
  We say that $f : \R^n \to \R$ is twice continuously (Fréchet) differentiable if for any point $\vx \in \R^n$, there exists $\grad f(\vx)$ and $\hes_f(\vx)$ such that for any $\vdelta \in \R^n$, \begin{align}
    f(\vx + \vdelta) = f(\vx) + \transpose{\grad f(\vx)}\vdelta + \frac{1}{2}\transpose{\vdelta}\hes_f(\vx)\vdelta + o(\norm{\vdelta}^2_2), \label{eq:second_order_exp}
  \end{align} where $\lim_{\vdelta \to \vzero} \frac{o(\norm{\vdelta}^2_2)}{\norm{\vdelta}^2_2} = 0$, and $\grad f(\vx)$ and $\hes_f(\vx)$ are continuous on $\R^n$.
  \Cref{eq:second_order_exp} is also called a \midx{second-order expansion} of $f$ at $\vx$.

  Twice continuously differentiable functions admit a natural characterization of convexity in terms of the Hessian.

  \begin{fct}[Second-order characterization of convexity]\pidx{second-order characterization of convexity}
    Consider a twice continuously differentiable function $f : \R^n \to R$. Then, $f$ is convex if and only if $\hes_f(\vx)$ is positive semi-definite for all $\vx \in \R^n$.
  \end{fct}

  It follows that $f$ is concave if and only if $\hes_f(\vx)$ is negative semi-definite for all $\vx \in \R^n$.
\end{rmk}

\begin{marginbox}[-30\baselineskip]{Jacobian}
  Given a vector-valued function, \begin{align*}
    \vg : \R^n \to \R^m,\quad \vx \mapsto \begin{bmatrix}
      g_1(\vx) \\
      \vdots \\
      g_m(\vx) \\
   \end{bmatrix},
  \end{align*} where $g_i : \R^n \to \R$, the \idx{Jacobian} of $\vg$ at $\vx \in \R^n$ is \begin{align}
    \jac{\vg(\vx)} \defeq \begin{bmatrix}
      \pdv{g_1(\vx)}{\vx(1)} & \cdots & \pdv{g_1(\vx)}{\vx(n)} \\
      \vdots & \ddots & \vdots \\
      \pdv{g_m(\vx)}{\vx(1)} & \cdots & \pdv{g_m(\vx)}{\vx(n)} \\
    \end{bmatrix}.
  \end{align}
  Observe that for a function $f : \R^n \to \R$, \begin{align}
    \jac f(\vx) = \transpose{\grad f(\vx)}.
  \end{align}
\end{marginbox}

\begin{marginbox}[-5\baselineskip]{Hessian}
  The \idx{Hessian} of a function $f : \R^n \to \R$ at a point $\vx \in \R^n$ is \begin{align}
    \hes_f(\vx) &\defeq \hes f(\vx) \nonumber \\
    &\defeq \begin{bmatrix}
      \pdv{f(\vx)}{\vx(1),\vx(1)} & \cdots & \pdv{f(\vx)}{\vx(1),\vx(n)} \\
      \vdots & \ddots & \vdots \\
      \pdv{f(\vx)}{\vx(n),\vx(1)} & \cdots & \pdv{f(\vx)}{\vx(n),\vx(n)} \\
   \end{bmatrix} \\
   &= \transpose{(\jac\grad f(\vx))} \label{eq:hessian} \\
   &\in \R^{n \times n}. \nonumber
  \end{align}
  Thus, a Hessian captures the curvature of $f$.
  If the Hessian of $f$ is positive definite at a point $\vx$, then $f$ is ``curved up around $\vx$''.

  Hessians are symmetric when the second partial derivatives are continuous, due to Schwartz's theorem.
\end{marginbox}

\subsection{Stochastic Gradient Descent}

In this \course, we primarily employ so-called first-order methods, which rely on (estimates of) the gradient of the objective function to determine a direction of local improvement.
The main idea behind these methods is to repeatedly take a step in the opposite direction of the gradient scaled by a learning rate $\eta_t$, which may depend on the current iteration $t$.

We will often want to minimize a stochastic optimization objective \begin{align}
  L(\vtheta) \defeq \E[\vx \sim p]{\ell(\vtheta; \vx)}
\end{align} where $\ell$ and its gradient $\grad \ell$ are known.

Based on our discussion in previous subsections, it is a natural first step to look for stationary points of $L$, that is, the roots of $\grad L$.

\begin{thmb}
  \begin{fct}[Robbins-Monro (RM) algorithm]
    Let $\vg : \R^n \to \R^m$ be an unknown function of which we want to find a root and suppose that we have access to unbiased noisy observations $\rG(\vtheta)$ of $\vg(\vtheta)$.
    The scheme \begin{align}
      \vtheta_{t+1} \defeq \vtheta_t - \eta_t \vg^{(t)}(\vtheta_t),
    \end{align} where $\vg^{(t)}(\vtheta_t) \sim \rG(\vtheta_t)$ are independent and unbiased estimates of $\vg(\vtheta_t)$, is known as the \midx{Robbins-Monro algorithm}.

    It can be shown that if the sequence of learning rates $\{\eta_t\}_{t=0}^\infty$ is chosen such that the \midx{Robbins-Monro conditions},\safefootnote{$\eta_t = \nicefrac{1}{t}$ is an example of a learning rate satisfying the RM-conditions.} \begin{align}
      \eta_t \geq 0 \quad \forall t, \quad \sum_{t=0}^\infty \eta_t = \infty \quad\text{and}\quad \sum_{t=0}^\infty \eta_t^2 < \infty, \label{eq:rm_conditions}
    \end{align} and additional regularity assumptions are satisfied,\safefootnote{For more details, refer to \icite{bottou1998online}.} then we have that \begin{align}
      \vg(\vtheta_t) \almostsurely \vzero \quad\text{as $t \to \infty$}.
    \end{align}
    That is, the RM-algorithm converges to a root almost surely.\safefootnote{See \cref{eq:as_convergence} for a definition of almost sure convergence of a sequence of random variables.}
  \end{fct}
\end{thmb}

Using Robbins-Monro to find a root of $\grad L$ is known as \emph{stochastic gradient descent}.
In particular, when $\ell$ is convex and the RM-conditions are satisfied, Robbins-Monro converges to a stationary point (and hence, a global minimum) of $L$ almost surely.
Moreover, it can be shown that for general $\ell$ and satisfied RM-conditions, stochastic gradient descent converges to a local minimum almost surely.
Intuitively, the randomness in the gradient estimates allows the algorithm to ``jump past'' saddle points.

A commonly used strategy to obtain unbiased gradient estimates is to take the sample mean of the gradient with respect to some set of samples $\sB$ (also called a \midx{batch}) as is shown in \cref{alg:sgd}.

\begin{algorithm}
  \caption{Stochastic gradient descent, SGD}\pidx{stochastic gradient descent}\label{alg:sgd}
  initialize $\vtheta$\;
  \Repeat{converged}{
    draw mini-batch $\sB \defeq \{\vx^{(1)}, \dots, \vx^{(m)}\}, \vx^{(i)} \sim p$\;
    $\vtheta \gets \vtheta - \eta_t \frac{1}{m} \sum_{i=1}^m \grad_{\vtheta} \ell(\vtheta; \vx^{(i)})$\;
  }
\end{algorithm}

\begin{ex}{Minimizing training loss}{}
  A common application of SGD in the context of machine learning is the following: $p \defeq \Unif{\{\vx_1, \dots, \vx_n\}}$ is a uniform distribution over the training inputs, yielding the objective \begin{align}
    L(\vtheta) = \frac{1}{n} \sum_{i=1}^n \ell(\vtheta; \vx_i),
  \end{align} where $\ell(\vtheta; \vx_i)$ is the loss of a model parameterized by $\vtheta$ for training input $\vx_i$ and $m$ is a fixed \midx{batch size}.
  Here, computing the gradients exactly (i.e., choosing $m = n$) is expensive when the size of the training set is large.

  A commonly used alternative to sampling each batch independently, is to split the training set into equally sized batches and perform a gradient descent step with respect to each of them.
  Typically, the optimum is not found after a single pass through the training data, so the same procedure is repeated multiple times. One such iteration is called an \midx{epoch}.
\end{ex}

\begin{rmk}{Regularization via weight decay}{weight_decay}
  A common technique to improve the ``stability'' of minima found through gradient-based methods is to ``regularize'' the loss by adding an explicit bias favoring ``simpler'' solutions.
  That is, given a loss $\ell$ measuring the quality of fit, we consider the regularized loss\looseness=-1 \begin{align}
    \ell'(\vtheta; \vx) \defeq \underbrace{\ell(\vtheta; \vx)}_{\text{quality of fit}} + \underbrace{r(\vtheta)}_{\text{regularization}}
  \end{align} where $r(\vtheta)$ is large for ``complex'' and small for ``simple'' choices of $\vtheta$, respectively.
  A common choice is $r(\vtheta) = \frac{\lambda}{2} \norm{\vtheta}_2^2$ for some $\lambda > 0$, which is known as \midx{$L_2$-regularization}.

  Recall from \cref{sec:fundamentals:parameter_esitmation:map} that in the context of likelihood maximization, this choice of $r$ corresponds to imposing the Gaussian prior $\N{\vzero}{\lambda\mI}$, and finding the MAP estimate.
  Imposing, for example, a Laplace prior leads to \midx{$L_1$-regularization}.

  Using $L_2$-regularization, we obtain for the gradient, \begin{align}
    \grad_\vtheta \ell'(\vtheta; \vx) = \grad_\vtheta \ell(\vtheta; \vx) + \lambda \vtheta.
  \end{align}
  Thus, a gradient descent step changes to \begin{align}
    \vtheta \gets (1 - \lambda \eta_t) \vtheta - \eta_t \grad_\vtheta \ell(\vtheta; \vx).
  \end{align}
  That is, in each step, $\vtheta$ decays towards zero at the rate $\lambda \eta_t$.
  This regularization method is also called \midx{weight decay}.
\end{rmk}

\begin{rmk}{Adaptive learning rates and momentum}{}
  Commonly, in optimization, constant learning rates are used to accelerate mixing.
  The performance can be further improved by taking an adaptive gradient step with respect to the geometry of the cost function, which is done by commonly used algorithms such as \midx{Adagrad} and \midx{Adam}.
  The methods employed by these algorithms are known as \midx<adaptive learning rates>{adaptive learning rate} and \midx{momentum}.

  For more details on momentum read section 7.1.2 of \icite{mml} and for an overview of the aforementioned optimization algorithms refer to \icite{ruder2016overview}.
\end{rmk}

\section{Useful Matrix Identities and Inequalities}\label{sec:background:identities_and_inequalities}

\begin{itemize}
  \item \midx<Woodbury's matrix identity>{Woodbury matrix identity} states that for any matrices $\mA \in \R^{n \times n}$, $\mC \in \R^{m \times m}$, and $\mU, \transpose{\mV} \in \R^{n \times m}$ where $\mA$ and $\mC$ are invertible, \begin{align}
    \inv{(\mA + \mU\mC\mV)} = \inv{\mA} - \inv{\mA}\mU\inv{(\inv{\mC} + \mV\inv{\mA}\mU)}\mV\inv{\mA}. \label{eq:general_woodbury}
  \end{align}
  The following identity, called the \midx{Sherman-Morrison formula}, is a direct consequence: \begin{subequations}\begin{align}
    \inv{(\mA + \vx\transpose{\vx})} &= \inv{\mA} - \inv{\mA}\vx\inv{(1 + \transpose{\vx}\inv{\mA}\vx)}\transpose{\vx}\inv{\mA} \\
    &= \inv{\mA} - \frac{(\inv{\mA}\vx)\transpose{(\inv{\mA}\vx)}}{1 + \transpose{\vx}\inv{\mA}\vx}
  \end{align}\label{eq:woodbury}\end{subequations} for any symmetric and invertible matrix $\mA \in \R^{n \times n}$ and $\vx \in \R^n$.

  \item The \midx{matrix inversion lemma} states that for matrices $\mA, \mB \in \R^{n \times n}$, \begin{align}
    \inv{(\mA + \mB)} = \inv{\mA} - \inv{\mA}\inv{(\inv{\mA} + \inv{\mB})} \inv{\mA}. \label{eq:matrix_inversion_lemma}
  \end{align}

  \item \midx{Hadamard's inequality} states that \begin{align}
    \det{\mM} \leq \prod_{i=0}^d \mM(i,i) \label{eq:hadamard}
  \end{align} for any positive definite matrix $\mM \in \R^{d \times d}$.

  \item The \midx{Weinstein-Aronszajn identity} for positive definite matrices $\mA \in \R^{d \times n}$ and $\mB \in \R^{n \times d}$ states that \begin{align}
    \det{\mI_{d \times d} + \mA\mB} = \det{\mI_{n \times n} + \mB\mA}. \label{eq:weinstein_aronszajn}
  \end{align}
\end{itemize}

\excheading

\begin{nexercise}{The sample variance is unbiased}{sample_variance}
  Let $X$ be a zero-mean random variable.
  Confirm that the sample variance of $X$ is indeed unbiased.
\end{nexercise}

\begin{nexercise}{Consistency of the sample mean}{concentration_inequalities}
  We will now show that the sample mean is consistent.
  To this end, let us first derive two classical concentration inequalities. \begin{enumerate}
    \item Prove \midx{Markov's inequality} which says that if $X$ is a non-negative random variable, then for any $\epsilon > 0$, \begin{align}
      \Pr{X \geq \epsilon} \leq \frac{\E*{X}}{\epsilon}. \label{eq:markov_inequality}
    \end{align}

    \item Prove \midx{Chebyshev's inequality} which says that if $X$ is a random variable with finite and non-zero variance, then for any $\epsilon > 0$, \begin{align}
      \Pr{\abs{X - \E*{X}} \geq \epsilon} \leq \frac{\Var*{X}}{\epsilon^2}. \label{eq:chebyshev_inequality}
    \end{align}

    \item Prove the weak law of large numbers from \cref{eq:wlln}.
  \end{enumerate}
\end{nexercise}

\begin{nexercise}{Directional derivatives}{directional_derivatives}
  Given a function $f : \R^n \to \R$ that is differentiable at $\vx \in \R^n$, the \midx{directional derivative} of $f$ at $\vx$ in the direction $\vd \in \R^n$ is \begin{align}
    D f(\vx)[\vd] \defeq \lim_{\lambda \to 0} \frac{f(\vx + \lambda\vd) - f(\vx)}{\lambda}.
  \end{align}

  Show that \begin{align}
    D f(\vx)[\vd] = \transpose{\grad f(\vx)} \vd.
  \end{align}
  \textit{Hint: Consider a first-order expansion of $f$ at $\vx$ in direction $\lambda\vd$.}
\end{nexercise}
