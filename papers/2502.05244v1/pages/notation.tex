\setlength\LTleft{0pt}
\setlength\LTright{0pt}

\chapter{Summary of Notation}

We follow these general rules: \begin{itemize}
  \item uppercase italic for constants $N$
  \item lowercase italic for indices $i$ and scalar variables $x$
  \item lowercase italic bold for vectors $\vx$, entries are denoted $\vx(i)$
  \item uppercase italic bold for matrices $\mM$, entries are denoted $\mM(i,j)$
  \item uppercase italic for random variables $X$
  \item uppercase bold for random vectors $\rX$, entries are denoted $\rX(i)$
  \item uppercase italic for sets $\sA$
  \item uppercase calligraphy for spaces (usually infinite sets) $\spA$
\end{itemize}

\vspace{16ex}

\begin{longtable}{@{}p{2.5cm}l@{\extracolsep{\fill}}}
  $\defeq$ & equality by definition \\
  $\approx$ & approximately equals \\
  $\propto$ & proportional to (up to multiplicative constants), $f \propto g$ iff $\exists k.\ \forall x.\ f(x) = k \cdot g(x)$ \\
  $\const$ & an (additive) constant \\
  $\Nat$ & set of natural numbers $\{1, 2, \dots\}$ \\
  $\Nat_0$ & set of natural numbers, including $0$, $\Nat \cup \{0\}$ \\
  $\R$ & set of real numbers \\
  $[m]$ & set of natural numbers from $1$ to $m$, $\{1, 2, \dots, m-1, m\}$ \\
  $i:j$ & subset of natural numbers between $i$ and $j$, $\{i, i+1, \dots, j-1, j\}$ \\
  $(a,b]$ & real interval between $a$ and $b$ including $b$ but not including $a$ \\
  $f : \sA \to \sB$ & function $f$ from elements of set $\sA$ to elements of set $\sB$ \\
  $f \circ g$ & function composition, $f(g(\cdot))$ \\
  $(\cdot)_+$ & $\max\{0, \cdot\}$ \\
  $\log$ & logarithm with base $e$ \\
  $\pset{\sA}$ & power set (set of all subsets) of $\sA$ \\
  $\Ind{predicate}$ & indicator function ($\Ind{predicate} \defeq 1$ if the $predicate$ is true, else $0$) \\
  $\odot$ & Hadamard (element-wise) product \\
  $\gets$ & assignment \\
  \vspace{1ex} \\
  \toprule
  \caption*{\smallcaps{Analysis}} \\
  $\grad f(\vx) \in \R^n$ & gradient of a function $f : \R^n \to \R$ at a point $\vx \in \R^n$ \\
  $\jac \vg(\vx) \in \R^{m \times n}$ & Jacobian of a function $\vg : \R^n \to \R^m$ at a point $\vx \in \R^n$ \\
  $\hes f(\vx) \in \R^{n \times n}$ & Hessian of a function $f : \R^n \to \R$ at a point $\vx \in \R^n$ \\
  \addlinespace
  $\dive \vF$ & divergence operation on vector field $\vF$ \\
  $\lapl f$ & Laplacian of a scalar field $f : \R^n \to \R$ \\
  \addlinespace
  $f \in \BigO{g}$ & $f$ grows at most as fast as $g$ (up to constant factors), $0 \leq \limsup_{n\to\infty} \abs{\frac{f(n)}{g(n)}} < \infty$ \\
  $f \in \BigOTil{g}$ & $f$ grows at most as fast as $g$ up to constant and logarithmic factors \\
  \addlinespace
  $\norm{\cdot}_\alpha$ & $\alpha$-norm \\
  $\norm{\cdot}_\mA$ & Mahalanobis norm induced by matrix $\mA$ \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Linear Algebra}} \\
  $\mI$ & identity matrix \\
  \addlinespace
  $\transpose{\mA}$ & transpose of matrix $\mA$ \\
  $\inv{\mA}$ & inverse of invertible matrix $\mA$ \\
  $\mA^{\nicefrac{1}{2}}$ & \makecell[tl]{square root of a symmetric and positive semi-definite matrix $\mA$} \\
  \addlinespace
  $\det{\mA}$ & determinant of $\mA$ \\
  $\tr{\mA}$ & trace of $\mA$, $\sum_i \mA(i,i)$ \\
  $\diag[i\in\sI]{a_i}$ & diagonal matrix with elements $a_i$, indexed according to the set $\sI$ \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Probability}} \\
  $\Omega$ & sample space \\
  $\spA$ & event space \\
  $\fnPr$ & probability measure \\
  \addlinespace
  $X \sim P$ & random variable $X$ follows the distribution $P$ \\
  $X_{1:n} \iid P$ & \makecell[tl]{random variables $X_{1:n}$ are independent and identically distributed according to \\ distribution $P$} \\
  $x \sim P$ & value $x$ is sampled according to distribution $P$ \\
  $P_X$ & cumulative distribution function of a random variable $X$ \\
  $\overline{P}_X$ & tail distribution function of a random variable $X$ \\
  $\inv{P}_X$ & quantile function of a random variable $X$ \\
  $p_X$ & \makecell[tl]{probability mass function (if discrete) or probability density function \\ (if continuous) of a random variable $X$} \\
  $\Delta^{\spA}$ & set of all probability distributions over the set $\spA$ \\
  $\delta_\alpha$ & Dirac delta function, point density at $\alpha$ \\
  $\pf{\vg}{p}$ & pushforward of a density $p$ under perturbation $\vg$ \\
  \addlinespace
  $X \perp Y$ & random variable $X$ is independent of random variable $Y$ \\
  $X \perp Y \mid Z$ & \makecell[tl]{random variable $X$ is conditionally independent of random variable $Y$ \\ given random variable $Z$} \\
  \addlinespace
  $\E{X}$ & expected value of random variable $X$ \\
  $\E[x \sim X]{f(x)}$ & expected value of the random variable $f(X)$, $\E{f(X)}$ \\
  $\E{X \mid Y}$ & conditional expectation of random variable $X$ given random variable $Y$ \\
  $\Cov{X,Y}$ & covariance of random variable $X$ and random variable $Y$ \\
  $\Cor{X,Y}$ & correlation of random variable $X$ and random variable $Y$ \\
  $\Var{X}$ & variance of random variable $X$ \\
  $\Var{X \mid Y}$ & conditional variance of random variable $X$ given random variable $Y$ \\
  $\mSigma_\rX$ & covariance matrix of random vector $\rX$ \\
  $\mLambda_\rX$ & precision matrix of random vector $\rX$ \\
  \addlinespace
  $\mathrm{MSE}(X)$ & mean squared error of random variable $X$ \\
  $\mean{X}_n$ & sample mean of random variable $X$ with $n$ samples \\
  $S_n^2$ & sample variance of random variable $X$ with $n$ samples \\
  \addlinespace
  $X_n \almostsurely X$ & the sequence of random variables $X_n$ converges almost surely to $X$ \\
  $X_n \convp X$ & the sequence of random variables $X_n$ converges to $X$ in probability \\
  $X_n \convd X$ & the sequence of random variables $X_n$ converges to $X$ in distribution \\
  \addlinespace
  $\S{u}$ & surprise associated with an event of probability $u$ \\
  $\H{p}, \H{X}$ & entropy of distribution $p$ (or random variable $X$) \\
  $\crH{p}{q}$ & cross-entropy of distribution $q$ with respect to distribution $p$ \\
  $\KL{p}{q}$ & KL-divergence of distribution $q$ with respect to distribution $p$ \\
  $\Fisher{p}{q}$ & relative Fisher information of distribution $q$ with respect to distribution $p$ \\
  $\H{X \mid Y}$ & conditional entropy of random variable $X$ given random variable $Y$ \\
  $\H{X, Y}$ & joint entropy of random variables $X$ and $Y$ \\
  $\I{X}{Y}$ & mutual information of random variables $X$ and $Y$ \\
  $\I{X}{Y}[Z]$ & \makecell[tl]{conditional mutual information of random variables $X$ and $Y$ given random \\ variable $Z$} \\
  \addlinespace
  $\N{\vmu}{\mSigma}$ & normal distribution with mean $\vmu$ covariance $\mSigma$ \\
  $\Laplace{\vmu}{h}$ & Laplace distribution with mean $\vmu$ scale $h$ \\
  $\Unif{\sS}$ & uniform distribution on the set $\sS$ \\
  $\Bern{p}$ & Bernoulli distribution with success probability $p$ \\
  $\Bin{n}{p}$ & binomial distribution with $n$ trials and success probability $p$ \\
  $\Beta{\alpha}{\beta}$ & beta distribution with shape parameters $\alpha$ and $\beta$ \\
  $\GammaDistr{\alpha}{\beta}$ & gamma distribution with shape $\alpha$ and rate $\beta$ \\
  $\Cauchy{m}{\tau}$ & Cauchy distribution with location $m$ and scale $\tau$ \\
  $\Pareto{c}{\alpha}$ & Pareto distribution with cutoff threshold $c$ and shape $\alpha$ \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Supervised Learning}} \\
  $\vtheta$ & parameterization of a model \\
  \addlinespace
  $\spX$ & input space \\
  $\spY$ & label space \\
  $\vx \in \spX$ & input \\
  $\epsilon(\vx)$ & zero-mean noise, sometimes assumed to be independent of $\vx$ \\
  $y \in \spY$ & (noisy) label, $f(\vx) + \epsilon(\vx)$ where $f$ is unknown \\
  $\spD \subseteq \spX \times \spY$ & labeled training data, $\{(\vx_i, y_i)\}_{i=1}^n$ \\
  $\mX \in \R^{n \times d}$ & design matrix when $\spX = \R^d$ \\
  $\mPhi \in \R^{n \times e}$ & design matrix in feature space $\R^e$ \\
  $\vy \in \R^n$ & label vector when $\spY = \R$ \\
  \addlinespace
  $p(\vtheta)$ & prior belief about $\vtheta$ \\
  $p(\vtheta \mid \vx_{1:n}, y_{1:n})$ & posterior belief about $\vtheta$ given training data \\
  $p(y_{1:n} \mid \vx_{1:n}, \vtheta)$ & likelihood of training data under the model parameterized by $\vtheta$ \\
  $p(y_{1:n} \mid \vx_{1:n})$ & marginal likelihood of training data \\
  \addlinespace
  $\vthetahat_\MLE$ & maximum likelihood estimate of $\vtheta$ \\
  $\vthetahat_\MAP$ & maximum a posteriori estimate of $\vtheta$ \\
  \addlinespace
  $\ell_\mathrm{nll}(\vtheta; \spD)$ & negative log-likelihood of the training data $\spD$ under model $\vtheta$ \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Bayesian Linear Models}} \\
  $\vw \in \R^d$ & weights of linear function $f(\vx; \vw) = \transpose{\vw} \vx$ \\
  \addlinespace
  $\vwhat_\ls$ & least squares estimate of $\vw$ \\
  $\vwhat_\ridge$ & ridge estimate of $\vw$ \\
  $\vwhat_\lasso$ & lasso estimate of $\vw$ \\
  \addlinespace
  $\N{\vzero}{\sigmap^2 \mI}$ & prior \\
  $\N{\transpose{\vw} \vx}{\sigman^2}$ & likelihood \\
  $\vmu \in \R^d$ & posterior mean, $\sigman^{-2} \mSigma \transpose{\mX} \vy$ \\
  $\mSigma \in \R^{d \times d}$ & posterior covariance matrix, $\inv{\parentheses{\sigman^{-2} \transpose{\mX} \mX + \sigmap^{-2} \mI}}$ \\
  \addlinespace
  $\mK \in \R^{n \times n}$ & kernel matrix, $\sigmap^2 \mX \transpose{\mX}$ \\
  \addlinespace
  $\sigma$ & logistic function \\
  $\Bern{\sigma(\transpose{\vw} \vx)}$ & logistic likelihood \\
  $\ell_\mathrm{log}(\cdot; \vx, y)$ & logistic loss of a single training example $(\vx, y)$ \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Kalman Filters}} \\
  $\rX_t$ & sequence of hidden states in $\R^d$ \\
  $\rY_t$ & sequence of observations in $\R^m$ \\
  \addlinespace
  $\mF \in \R^{d \times d}$ & motion model \\
  $\mH \in \R^{m \times d}$ & sensor model \\
  $\vvarepsilon_t$ & zero-mean motion noise with covariance matrix $\mSigma_x$ \\
  $\veta_t$ & zero-mean sensor noise with covariance matrix $\mSigma_y$ \\
  \addlinespace
  $\mK_t \in \R^{d \times m}$ & Kalman gain \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Gaussian Processes}} \\
  $\mu : \spX \to \R$ & mean function \\
  $k : \spX \times \spX \to \R$ & kernel function / covariance function \\
  $f \sim \GP{\mu}{k}$ & $f$ is a Gaussian process with mean function $\mu$ and kernel function $k$ \\
  \addlinespace
  $\spH_k(\spX)$ & reproducing kernel Hilbert space associated with kernel function $k : \spX \times \spX \to \R$ \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Deep Models}} \\
  $\mW_l \in \R^{n_l \times n_{l-1}}$ & weight matrix of layer $l$ \\
  $\vnu^{(l)} \in \R^{n_l}$ & activations of layer $l$ \\
  \addlinespace
  $\varphi$ & activation function \\
  $\mathrm{Tanh}$ & hyperbolic tangent activation function \\
  $\mathrm{ReLU}$ & rectified linear unit activation function \\
  \addlinespace
  $\sigma_i(\vf)$ & softmax function computing the probability mass of class $i$ given outputs $\vf$ \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Variational Inference}} \\
  $\spQ$ & variational family \\
  $\vlambda \in \Lambda$ & variational parameters \\
  $q_\vlambda$ & variational posterior parameterized by $\vlambda$ \\
  $L(q, p; \spD)$ & evidence lower bound for data $\spD$ of variational posterior $q$ and true posterior $p(\cdot \mid \spD)$ \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Markov Chains}} \\
  $\sS$ & set of $n$ states \\
  $X_t$ & sequence of states \\
  $p(x' \mid x)$ & transition function, probability of going from state $x$ to state $x'$ \\
  $p^{(t)}(x' \mid x)$ & probability of reaching $x'$ from $x$ in exactly $t$ steps \\
  $\mP \in \R^{n \times n}$ & transition matrix \\
  \addlinespace
  $q_t$ & distribution over states at time $t$ \\
  $\pi$ & stationary distribution \\
  \addlinespace
  $\norm{\mu - \nu}_\mathrm{TV}$ & total variation distance between two distributions $\mu$ and $\nu$ \\
  $\tau_\mathrm{TV}$ & mixing time with respect to total variation distance \\
  \vspace{7.9ex} \\
  \toprule
  \caption*{\smallcaps{Markov Chain Monte Carlo Methods}} \\
  $r(\vxp \mid \vx)$ & proposal distribution, probability of proposing $\vxp$ when in $\vx$ \\
  $\alpha(\vxp \mid \vx)$ & acceptance distribution, probability of accepting the proposal $\vxp$ when in $\vx$ \\
  \addlinespace
  $f$ & energy function \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Active Learning}} \\
  $\sS \subseteq \spX$ & set of observations \\
  $I(\sS)$ & maximization objective, quantifying the ``information value'' of $\sS$ \\
  $\Delta_I(\vx \mid \sA)$ & marginal gain of observation $\vx$ with respect to objective $I$ given prior observations $\sA$ \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Bayesian Optimization}} \\
  $R_T$ & cumulative regret for time horizon $T$ \\
  $F(\vx; \mu, \sigma)$ & acquisition function \\
  $\spC_t(\vx)$ & confidence interval of $\opt{f}(\vx)$ after round $t$ \\
  $\beta_t(\delta)$ & scale of confidence interval to achieve confidence level $\delta$ \\
  $\gamma_T$ & maximum information gain after $T$ rounds \\
  \vspace{2ex} \\
  \toprule
  \caption*{\smallcaps{Reinforcement Learning}} \\
  $X, \spX$ & set of states \\
  $A, \spA$ & set of actions \\
  $p(x' \mid x, a)$ & \makecell[tl]{dynamics model, probability of transitioning from state $x$ to state $x'$ when playing \\ action $a$} \\
  $r$ & reward function \\
  $X_t$ & sequence of states \\
  $A_t$ & sequence of actions \\
  $R_t$ & sequence of rewards \\
  $\pi(a \mid x)$ & policy, probability of playing action $a$ when in state $x$ \\
  $G_t$ & discounted payoff from time $t$ \\
  $\gamma$ & discount factor \\
  \addlinespace
  $\v[\pi]{x}[t]$ & state value function, average discounted payoff from time $t$ starting from state $x$ \\
  $\q[\pi]{x}{a}[t]$ & \makecell[tl]{state-action value function, average discounted payoff from time $t$ starting from state $x$ \\ playing action $a$} \\
  $\a[\pi]{x}{a}[t]$ & advantage function, $\q[\pi]{x}{a}[t] - \v[\pi]{x}[t]$ \\
  $\j{\pi}$ & policy value function, expected reward of policy $\pi$ \\
\end{longtable}
