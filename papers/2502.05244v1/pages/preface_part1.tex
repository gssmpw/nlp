\chapter*{Preface to \Cref{part1}}

As humans, we constantly learn about the world around us.
We learn to interact with our physical surroundings.
We deepen our understanding of the world by establishing relationships between actors, objects, and events.
And we learn about ourselves by observing how we interact with the world and with ourselves.
We then continuously use this knowledge to make inferences and predictions, be it about the weather, the movement of a ball, or the behavior of a friend.

With limited computational resources, limited genetic information, and limited life experience, we are not able to learn everything about the world to complete certainty.
We saw in \cref{sec:fundamentals} that probability theory is the mathematical framework for reasoning with uncertainty in the same way that logic is the mathematical framework for reasoning with certainty.
We will discuss two kinds of uncertainty: ``aleatoric'' uncertainty which cannot be reduced under computational constraints, and ``epistemic'' uncertainty which can be reduced by observing more data.\looseness=-1

An important aspect of learning is that we do not just learn once, but continually.
Bayes' rule allows us to update our beliefs and reduce our uncertainty as we observe new data --- a process that is called \midx{probabilistic inference}.
By taking the former posterior as the new prior, probabilistic inference can be performed continuously and repeated indefinitely as we observe more and more data.

\begin{marginfigure}
  \incfig{perception_diagram}
	\caption{A schematic illustration of probabilistic inference in the context of the (supervised) learning of a model $\vtheta$ from perceived data $\spD$. The prior model $p(\vtheta)$ can equip the model with anything from substantial, to little, to no prior knowledge.\looseness=-1}
  \label{fig:perception_diagram}
\end{marginfigure}

Our sensory information is often noisy and imperfect, which is another source of uncertainty.
The same is true for machines, even if they can sometimes sense aspects of the world more accurately than humans.
We discuss how one can infer latent structure of the world from sensed data, such as the state of a dynamical system like a car, in a process that is called \midx{filtering}.

In this first part of the \course, we examine how we can build machines that are capable of (continual) learning and inference.
First, we introduce probabilistic inference in the context of linear models which make predictions based on fixed (often hand-designed) features.
We then discuss how probabilistic inference can be scaled to kernel methods and Gaussian processes which use a large (potentially infinite) number of features, and to deep neural networks which learn features dynamically from data.
In these models, exact inference is typically intractable, and we discuss modern methods for approximate inference such as variational inference and Markov chain Monte Carlo.
We highlight a tradeoff between curiosity (i.e., extrapolating beyond the given data) and conformity (i.e., fitting the given data), which surfaces as a fundamental principle of probabilistic inference in the regime where the data and our computational resources are limited.
