\chapter*{Preface to \Cref{part2}}

In the first part of the \course, we have learned about how we can build machines that are capable of updating their beliefs and reducing their epistemic uncertainty through probabilistic inference.
We have also discussed ways of keeping track of the world through noisy sensory information by filtering.
An important aspect of intelligence is to use this acquired knowledge for making decisions and taking actions that have a positive impact on the world.

Already today, we are surrounded by machines that make decisions and take actions; that is, exhibit some degree of agency.
Be it a search engine producing a list of search results, a chatbot answering a question, or a driving-assistance system steering a car: these systems are all perceiving the world, making decisions, and then taking actions that in turn have an effect on the world.
\Cref{fig:preception_action_loop} illustrates this \midx{perception-action loop}.

\begin{figure}
  \incfig{perception_action_diagram}
	\caption{An illustration of the perception-action loop. This is a straightforward extension of our view of probabilistic inference from \cref{fig:perception_diagram} with the addition of an ``action'' component which is capable of ``adaptively'' interacting with the outside world and the internal world model.}
  \label{fig:preception_action_loop}
\end{figure}

In the second part of this \course, we will discuss the underpinning principles of building machines that are capable of making sequential decisions.
We will see that decision-making itself can be cast as probabilistic inference, obeying the same mechanisms that we used in the first part to build learning systems.

We discuss various ways of addressing the question: \begin{center}
  \emph{How to act, given that computational resources and time are limited?}
\end{center}
One approach is to act with the aim to reduce epistemic uncertainty, which is the topic of active learning.
Another approach is to act with the aim to maximize some reward signal, which is the topic of bandits, Bayesian optimization, and reinforcement learning.

This surfaces the exploration-exploitation dilemma where the agent has to prioritize either maximizing its immediate rewards or reducing its uncertainty about the world which might pay off in the future.
We discuss that this dilemma is, in fact, in direct correspondence to the principle of curiosity and conformity which we discussed extensively throughout \cref{part1}.

Since time is limited, it is critical to be sample-efficient when learning the most important aspects of the world.
At the same time, interactions with the world are often complex, and some interactions might even be harmful.
We discuss how an agent can use its epistemic uncertainty to guide the exploration of its environment while mitigating risks and reasoning about safety.
