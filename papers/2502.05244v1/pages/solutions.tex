\chapter{Solutions}

\section*{\nameref{sec:fundamentals}}

\begin{solution}{properties_of_probability}
  \begin{enumerate}[beginpenalty=10000]
    \item By the third axiom and $B = A \cup (B \setminus A)$, \begin{align*}
      \Pr{B} = \Pr{A} + \Pr{B \setminus A}.
    \end{align*}
    Noting from the first axiom that $\Pr{B \setminus A} \geq 0$ completes the proof.

    \item By the second axiom, \begin{align*}
      \Pr{A \cup \compl{A}} = \Pr{\Omega} = 1
    \end{align*} and by the third axiom, \begin{align*}
      \Pr{A \cup \compl{A}} = \Pr{A} + \Pr{\compl{A}}.
    \end{align*}
    Reorganizing the equations completes the proof.

    \item Define the countable sequence of events \begin{align*}
      A'_i \defeq A_i \setminus \parentheses*{\bigcup_{j=1}^{i-1} A_j}.
    \end{align*}
    Note that the sequence of events $\{A'_i\}_i$ is disjoint. Thus, we have by the third axiom and then using (1) that \begin{align*}
      \Pr{\bigcup_{i=1}^\infty A_i} = \Pr{\bigcup_{i=1}^\infty A'_i} = \sum_{i=1}^\infty \Pr{A'_i} \leq \sum_{i=1}^\infty \Pr{A_i}.
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{graph_random_walk}
  We show that any vertex $v$ is visited eventually with probability $1$.

  We denote by $w \to v$ the event that the random walk starting at vertex $w$ visits the vertex $v$ eventually, we denote by $\Gamma(w)$ the neighborhood of $w$, and we write $\deg(w) \defeq |\Gamma(w)|$.
  We have, \begin{align*}
    \Pr{w \to v} &= \sum_{w' \in \Gamma(w)} \Pr{\text{the random walk first visits $w'$}} \cdot \Pr{w' \to v} \margintag{using the law of total probability \eqref{eq:lotp}} \\
    &= \frac{1}{\deg(w)} \sum_{w' \in \Gamma(w)} \Pr{w' \to v}. \margintag{using that the random walk moves to a neighbor uniformly at random}
  \end{align*}

  Take $u$ to be the vertex such that $\Pr{u \to v}$ is minimized. Then, \begin{align*}
    \Pr{u \to v} = \frac{1}{\deg(u)} \sum_{u' \in \Gamma(u)} \underbrace{\Pr{u' \to v}}_{\geq \Pr{u \to v}} \geq \Pr{u \to v}.
  \end{align*}
  That is, for all neighbors $u'$ of $u$, $\Pr{u \to v} = \Pr{u' \to v}$.
  Using that the graph is connected and finite, we conclude $\Pr{u \to v} = \Pr{w \to v}$ for any vertex $w$.
  Finally, note that $\Pr{v \to v} = 1$, and hence, the random walk starting at any vertex $u$ visits the vertex $v$ eventually with probability $1$.
\end{solution}

\begin{solution}{lote}
  Let $\Ind{\sA_i}$ be the indicator random variable for the event $\sA_i$.
  Then, \begin{align*}
    \E{\rX \cdot \Ind{\sA_i}} &= \E{\E{\rX \cdot \Ind{\sA_i} \mid \Ind{\sA_i}}} \margintag{by the tower rule \eqref{eq:tower_rule}} \\
    &= \begin{multlined}[t]
      \E{\rX \cdot \Ind{\sA_i} \mid \Ind{\sA_i} = 1} \cdot \Pr{\Ind{\sA_i} = 1} \\ + 0 \cdot \Pr{\Ind{\sA_i} = 0}
    \end{multlined} \margintag{expanding the outer expectation} \\
    &= \E{\rX \mid \sA_i} \cdot \Pr{\sA_i}. \margintag{the event $\Ind{\sA_i} = 1$ is equivalent to $\sA_i$}
  \end{align*}
  Summing up for all $i$, the left-hand side becomes \begin{align*}
    \E{\rX} = \sum_{i=1}^k \E{\rX \cdot \Ind{\sA_i}}.
  \end{align*}
\end{solution}

\begin{solution}{cov_mat_pos_sd}
  Let $\mSigma \defeq \Var{\rX}$ be a covariance matrix of the random vector $\rX$ and fix any $\vz \in \R^n$.
  Then, \begin{align*}
    \transpose{\vz} \mSigma \vz &= \transpose{\vz} \E{(\rX - \E{\rX}) \transpose{(\rX - \E{\rX})}} \vz \margintag{using the definitiion of variance \eqref{eq:variance}} \\
    &= \E{\transpose{\vz} (\rX - \E{\rX}) \transpose{(\rX - \E{\rX})} \vz}. \margintag{using linearity of expectation \eqref{eq:linearity_expectation}}
  \intertext{Define the random variable $Z \defeq \transpose{\vz} (\rX - \E{\rX})$. Then,}
    &= \E{Z^2} \geq 0.
  \end{align*}
\end{solution}

\begin{solution}{bayes_rule}
  Let us start by defining some events that we will reason about later.
  For ease of writing, let us call the person in question X. \begin{align*}
    D &= \text{X has the disease}, \\
    P &= \text{The test shows a positive response}.
  \end{align*}
  Now we can translate the information in the question to formal statements in terms of $D$ and $P$, \begin{align*}
    \Pr{D} &= 10^{-4} \margintag{the disease is rare} \\
    \Pr{P \mid D} = \Pr{\compl{P} \mid \compl{D}} &= 0.99. \margintag{the test is accurate}
  \end{align*}
  We want to determine $\Pr{D \mid P}$. One can find this probability by using Bayes' rule \eqref{eq:bayes_rule}, \begin{align*}
    \Pr{D \mid P} = \frac{\Pr{P \mid D} \cdot \Pr{D}}{\Pr{P}}.
  \end{align*}
  From the quantities above, we have everything except for $\Pr{P}$. This, however, we can compute using the law of total probability, \begin{align*}
    \Pr{P} &= \Pr{P \mid D} \cdot \Pr{D} + \Pr{P \mid \compl{D}} \cdot \Pr{\compl{D}} \\
    &= 0.99 \cdot 10^{-4} + 0.01 \cdot (1 - 10^{-4}) \\
    &= 0.010098.
  \end{align*}
  Hence, $\Pr{D \mid P} = 0.99 \cdot 10^{-4} / 0.010098 \approx 0.0098 = 0.98\%$.
\end{solution}

\begin{solution}{zero_ev_of_cov_mats}
  \begin{enumerate}[beginpenalty=10000]
    \item Suppose that $\rX$ is not linearly independent.
    Then, there exists some $\valpha \in \R^n \setminus \{\vzero\}$ such that $\transpose{\valpha} \rX = 0$.
    We have that $\valpha$ is an eigenvector of $\Var{\rX}$ with zero eigenvalue since \begin{align*}
      \transpose{\valpha} \Var{\rX} \valpha = \Var{\transpose{\valpha} \rX} = \Var{0} = 0. \margintag{using \cref{eq:linear_map_variance}}
    \end{align*}

    \item Suppose that $\Var{\rX}$ has a zero eigenvalue.
    Let $\vlambda$ be the corresponding eigenvector and consider the suggested linear combination $Y \defeq \transpose{\vlambda} \rX$.
    We have \begin{align*}
      \Var{Y} &= \Var{\transpose{\vlambda} \rX} \\
      &= \transpose{\vlambda} \Var{\rX} \vlambda \margintag{using \cref{eq:linear_map_variance}} \\
      &= \sum_{i=1}^n \vlambda(i) \cdot 0 \margintag{using the property of a zero eigenvalue: $\transpose{\vlambda} \Var{\rX} \vlambda = 0$} \\
      &= 0
    \end{align*} which implies that $Y$ must be a constant.
  \end{enumerate}
\end{solution}

\begin{solution}{gaussian_pdf_prod}
  We need to find $\vmu \in \R^n$ and $\mSigma \in \R^{n \times n}$ such that for all $\vx \in \R^n$, \begin{align}
    \begin{multlined}[t]
      \transpose{(\vx - \vmu)} \inv{\mSigma} (\vx - \vmu) \nonumber \\ = \transpose{(\vx - \vmu_1)} \inv{\mSigma_1} (\vx - \vmu_1) + \transpose{(\vx - \vmu_2)} \inv{\mSigma_2} (\vx - \vmu_2) + \const.
    \end{multlined}\label{eq:gaussian_pdf_prod_goal}
  \end{align}

  The left-hand side of \cref{eq:gaussian_pdf_prod_goal} is equal to \begin{align*}
    \transpose{\vx} \inv{\mSigma} \vx - 2 \transpose{\vx} \inv{\mSigma} \vmu + \transpose{\vmu} \inv{\mSigma} \vmu.
  \end{align*}
  The right-hand side of \cref{eq:gaussian_pdf_prod_goal} is equal to \begin{align*}
    &\begin{multlined}[t]
      \parentheses*{\transpose{\vx} \inv{\mSigma_1} \vx + \transpose{\vx} \inv{\mSigma_2} \vx} - 2 \parentheses*{\transpose{\vx} \inv{\mSigma_1} \vmu_1 + \transpose{\vx} \inv{\mSigma_2} \vmu_2} \\ + \parentheses*{\transpose{\vmu_1} \inv{\mSigma_1} \vmu_1 + \transpose{\vmu_2} \inv{\mSigma_2} \vmu_2}
    \end{multlined} \\
    &= \begin{multlined}[t]
      \transpose{\vx} \parentheses*{\inv{\mSigma_1} + \inv{\mSigma_2}} \vx - 2 \transpose{\vx} \parentheses*{\inv{\mSigma_1} \vmu_1 + \inv{\mSigma_2} \vmu_2} \\ + \parentheses*{\transpose{\vmu_1} \inv{\mSigma_1} \vmu_1 + \transpose{\vmu_2} \inv{\mSigma_2} \vmu_2}.
    \end{multlined}
  \end{align*}
  We observe that both sides are equal up to constant terms if \begin{align*}
    \inv{\mSigma} = \inv{\mSigma_1} + \inv{\mSigma_2} \quad\text{and}\quad \inv{\mSigma} \vmu = \inv{\mSigma_1} \vmu_1 + \inv{\mSigma_2} \vmu_2.
  \end{align*}
\end{solution}

\begin{solution}{grv_uncor_indep}
  Recall that independence of any random vectors $\rX$ and $\rY$ implies that they are uncorrelated,\footnote{This is because the expectation of their product factorizes, see \cref{eq:expectation_product}.} the converse implication is a special property of Gaussian random vectors.

  Consider the joint Gaussian random vector $\rZ \defeq [\rX \; \rY] \sim \N{\vmu}{\mSigma}$ and assume that $\rX \sim \N{\vmu_{\rX}}{\mSigma_{\rX}}$ and $\rY \sim \N{\vmu_{\rY}}{\mSigma_{\rY}}$ are uncorrelated. Then, $\mSigma$ can be expressed as \begin{align*}
    \mSigma = \begin{bmatrix}
      \mSigma_{\rX} & \mzero \\
      \mzero & \mSigma_{\rY} \\
    \end{bmatrix},
  \end{align*} implying that the PDF of $\rZ$ factorizes, \begin{align*}
    &\N[[\vx \; \vy]]{\vmu}{\mSigma} \\
    &= \begin{multlined}[t]
      \frac{1}{\sqrt{\det{2\pi\mSigma_{\rX}} \cdot \det{2\pi\mSigma_{\rY}}}} \exp\left(-\frac{1}{2}\transpose{(\vx-\vmu_{\rX})}\inv{\mSigma_{\rX}}(\vx-\vmu_{\rX}) \right. \\ \left. -\frac{1}{2}\transpose{(\vy-\vmu_{\rY})}\inv{\mSigma_{\rY}}(\vy-\vmu_{\rY})\right)
    \end{multlined} \\
    &= \N[\vx]{\vmu_{\rX}}{\mSigma_{\rX}} \cdot \N[\vy]{\vmu_{\rY}}{\mSigma_{\rY}}. \qedhere
  \end{align*}
\end{solution}

\begin{solution}{marginal_and_cond_gaussian}
  Let $\vx \sim \rX$.
  The joint distribution can be expressed as \begin{align*}
    p(\vx) &= p(\vx_A, \vx_B) \\
    &= \frac{1}{Z} \exp\parentheses*{-\frac{1}{2} \transpose{\begin{bmatrix}
      \vx_A - \vmu_A \\
      \vx_B - \vmu_B
    \end{bmatrix}} \begin{bmatrix}
      \mLambda_{AA} & \mLambda_{AB} \\
      \mLambda_{BA} & \mLambda_{BB}
    \end{bmatrix} \begin{bmatrix}
      \vx_A - \vmu_A \\
      \vx_B - \vmu_B
    \end{bmatrix}}
  \end{align*} where $Z$ denotes the normalizing constant.
  To simplify the notation, we write \begin{align*}
    \begin{bmatrix}
      \vDelta_A \\
      \vDelta_B
    \end{bmatrix} \defeq \begin{bmatrix}
      \vx_A - \vmu_A \\
      \vx_B - \vmu_B
    \end{bmatrix}.
  \end{align*}

  \begin{enumerate}
    \item We obtain \begin{align*}
      &p(\vx_A) \\
      &= \frac{1}{Z} \int \exp\parentheses*{-\frac{1}{2} \transpose{\begin{bmatrix}
        \vDelta_A \\
        \vDelta_B
      \end{bmatrix}} \begin{bmatrix}
        \mLambda_{AA} & \mLambda_{AB} \\
        \mLambda_{BA} & \mLambda_{BB}
      \end{bmatrix} \begin{bmatrix}
        \vDelta_A \\
        \vDelta_B
      \end{bmatrix}} \,d\vx_B \margintag{using the sum rule \eqref{eq:sum_rule}} \\
      &= \begin{multlined}[t]
        \frac{1}{Z} \exp\parentheses*{-\frac{1}{2} \brackets*{\transpose{\vDelta_A} (\mLambda_{AA} - \mLambda_{AB} \inv{\mLambda_{BB}} \mLambda_{BA}) \vDelta_A}} \\ \cdot \int \exp\parentheses*{-\frac{1}{2} \brackets*{\transpose{(\vDelta_\sB + \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}\vDelta_\sA)}\mLambda_{\sB\sB}(\vDelta_\sB + \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}\vDelta_\sA)}} \,d\vx_B.
      \end{multlined} \margintag{using the first hint} \\
    \intertext{Observe that the integrand is an unnormalized Gaussian PDF, and hence, the integral evaluates to $\sqrt{\det{2 \pi \inv{\mLambda_{BB}}}}$ (so is constant with respect to $\vx_A$). We can therefore simplify to}
      &= \frac{1}{Z'} \exp\parentheses*{-\frac{1}{2} \brackets*{\transpose{\vDelta_A} (\mLambda_{AA} - \mLambda_{AB} \inv{\mLambda_{BB}} \mLambda_{BA}) \vDelta_A}} \\
      &= \frac{1}{Z'} \exp\parentheses*{-\frac{1}{2} \brackets*{\transpose{\vDelta_A} \inv{\mSigma_{AA}} \vDelta_A}}. \margintag{using the second hint}
    \end{align*}

    \item We obtain \begin{align*}
      &p(\vx_B \mid \vx_A) \\
      &= \frac{p(\vx_A, \vx_B)}{p(\vx_A)} \margintag{using the definition of conditional distributions \eqref{eq:cond_distr}} \\
      &= \frac{1}{Z'} \exp\parentheses*{-\frac{1}{2} \transpose{\begin{bmatrix}
        \vDelta_A \\
        \vDelta_B
      \end{bmatrix}} \begin{bmatrix}
        \mLambda_{AA} & \mLambda_{AB} \\
        \mLambda_{BA} & \mLambda_{BB}
      \end{bmatrix} \begin{bmatrix}
        \vDelta_A \\
        \vDelta_B
      \end{bmatrix}} \margintag{noting that $p(\vx_A)$ is constant with respect to $\vx_B$} \\
      &= \begin{multlined}[t]
        \frac{1}{Z'} \exp\parentheses*{-\frac{1}{2} \brackets*{\transpose{\vDelta_A} (\mLambda_{AA} - \mLambda_{AB} \inv{\mLambda_{BB}} \mLambda_{BA}) \vDelta_A}} \\ \cdot \exp\parentheses*{-\frac{1}{2} \brackets*{\transpose{(\vDelta_\sB + \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}\vDelta_\sA)}\mLambda_{\sB\sB}(\vDelta_\sB + \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}\vDelta_\sA)}}
      \end{multlined} \margintag{using the first hint} \\
      &= \frac{1}{Z''} \exp\parentheses*{-\frac{1}{2} \brackets*{\transpose{(\vDelta_\sB + \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}\vDelta_\sA)}\mLambda_{\sB\sB}(\vDelta_\sB + \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}\vDelta_\sA)}}. \margintag{observing that the first exponential is constant with respect to $\vx_B$}
    \end{align*}
    Finally, observe that \begin{align*}
      \vDelta_\sB + \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}\vDelta_\sA &= \vmu_B - \inv{\mLambda_{\sB\sB}}\mLambda_{\sB\sA}(\vx_\sA - \vmu_\sA) = \vmu_{\sB\mid\sA} \quad\text{and} \margintag{using the third hint} \\
      \inv{\mLambda_{\sB\sB}} &= \mSigma_{\sB\sB} - \mSigma_{\sB\sA} \inv{\mSigma_{\sA\sA}} \mSigma_{\sA\sB} = \mSigma_{\sB\mid\sA}. \margintag{using the second hint}
    \end{align*}
    Thus, $\vx_B \mid \vx_A \sim \N{\vmu_{\sB\mid\sA}}{\mSigma_{\sB\mid\sA}}$.
  \end{enumerate}
\end{solution}

\begin{solution}{gaussian_closedness}
  \begin{enumerate}[beginpenalty=10000]
    \item Let $\rY \defeq \mA \rX + \vb$ and define $\vs \defeq \transpose{\mA} \vt$.
    We have for the MGF of $\rY$ that \begin{align*}
      \varphi_\rY(\vt) &= \E*{\exp\parentheses*{\transpose{\vt} \rY}} \\
      &= \E*{\exp\parentheses*{\transpose{\vt} \mA \rX} \cdot \exp\parentheses*{\transpose{\vt} \vb}} \\
      &= \E*{\exp\parentheses*{\transpose{\vs} \rX}} \cdot \exp\parentheses*{\transpose{\vt} \vb} \\
      &= \varphi_\rX(\vs) \cdot \exp\parentheses*{\transpose{\vt} \vb} \\
      &= \exp\parentheses*{\transpose{\vs} \vmu + \frac{1}{2}\transpose{\vs}\mSigma\vs} \cdot \exp\parentheses*{\transpose{\vt} \vb} \\
      &= \exp\parentheses*{\transpose{\vt} (\mA\vmu + \vb) + \frac{1}{2}\transpose{\vt} \mA \mSigma \transpose{\mA} \vt},
    \end{align*} which implies that $\rY \sim \N{\mA \vmu + \vb}{\mA \mSigma \transpose{\mA}}$.

    \item We have for the MGF of $\rY \defeq \rX + \rX'$ that \begin{align*}
      \varphi_\rY(\vt) &= \E*{\exp\parentheses*{\transpose{\vt} \rY}} \\
      &= \E*{\exp\parentheses*{\transpose{\vt} \rX}} \cdot \E*{\exp\parentheses*{\transpose{\vt} \rX'}} \margintag{using the independence of $\rX$ and $\rX'$} \\
      &= \varphi_\rX(\vt) \cdot \varphi_{\rX'}(\vt) \\
      &= \exp\parentheses*{\transpose{\vt} \vmu + \frac{1}{2}\transpose{\vt}\mSigma\vt} \cdot \exp\parentheses*{\transpose{\vt} \vmu' + \frac{1}{2}\transpose{\vt}\mSigma'\vt} \\
      &= \exp\parentheses*{\transpose{\vt} (\vmu + \vmu') + \frac{1}{2}\transpose{\vt} (\mSigma + \mSigma') \vt},
    \end{align*} which implies that $\rY \sim \N{\vmu + \vmu'}{\mSigma + \mSigma'}$.
  \end{enumerate}
\end{solution}

\begin{solution}{expectation_and_variance_of_gaussians}
  Note that if $Y \sim \N{0}{1}$ is a (univariate) standard normal random variable, then \begin{align*}
    \E{Y} &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty y \cdot \exp\parentheses*{-\frac{1}{2} y^2} \,d y \margintag{using the PDF of the standard normal distribution \eqref{eq:univ_normal}} \\
    &= \frac{1}{\sqrt{2\pi}} \parentheses*{\int_{-\infty}^0 y \cdot \exp\parentheses*{-\frac{1}{2} y^2} \,d y + \int_0^\infty y \cdot \exp\parentheses*{-\frac{1}{2} y^2} \,d y} \\
    &= \frac{1}{\sqrt{2\pi}} \parentheses*{\left[ - \exp\parentheses*{-\frac{1}{2} y^2} \right]_{-\infty}^0 + \left[ - \exp\parentheses*{-\frac{1}{2} y^2} \right]_0^\infty} \\
    &= \frac{1}{\sqrt{2\pi}} ([-1 + 0] + [0 + 1]) = 0, \\
    \Var{Y} &= \E{Y^2} - \underbrace{\E{Y}^2}_{0} \margintag{using the definition of variance \eqref{eq:variance2}} \\
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty y^2 \cdot \exp\parentheses*{-\frac{1}{2} y^2} \,d y \margintag{using the PDF of the standard normal distribution \eqref{eq:univ_normal}} \\
    &= \frac{1}{\sqrt{2\pi}} \parentheses*{\int_{-\infty}^0 y^2 \cdot \exp\parentheses*{-\frac{1}{2} y^2} \,d y + \int_0^\infty y^2 \cdot \exp\parentheses*{-\frac{1}{2} y^2} \,d y} \\
    &= \begin{multlined}[t]
      \frac{1}{\sqrt{2\pi}} \left(\left[- y \cdot \exp\parentheses*{-\frac{1}{2} y^2} \right]_{-\infty}^0 + \int_{-\infty}^0 \exp\parentheses*{-\frac{1}{2} y^2} \,d y \right. \\ \left. + \left[- y \cdot \exp\parentheses*{-\frac{1}{2} y^2} \right]_0^\infty + \int_0^\infty \exp\parentheses*{-\frac{1}{2} y^2} \,d y\right)
    \end{multlined} \margintag{integrating by parts} \\
    &= \frac{1}{\sqrt{2\pi}} \parentheses*{\int_{-\infty}^0 \exp\parentheses*{-\frac{1}{2} y^2} \,d y + \int_0^\infty \exp\parentheses*{-\frac{1}{2} y^2} \,d y} \\
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty \exp\parentheses*{-\frac{1}{2} y^2} \,d y = 1. \margintag{a PDF integrates to $1$}
  \end{align*}

  Recall from \cref{eq:gaussian_affine_transformation} that we can express $\rX \sim \N{\vmu}{\mSigma}$ as \begin{align*}
    \rX = \msqrt{\mSigma} \rY + \vmu
  \end{align*} where $\rY \sim \SN$.
  Using that $\rY$ is a vector of independent univariate standard normal random variables, we conclude that $\E{\rY} = \vzero$ and $\Var{\rY} = \mI$.
  We obtain \begin{align*}
    \E{\rX} &= \E{\msqrt{\mSigma} \rY + \vmu} = \msqrt{\mSigma} \underbrace{\E{\rY}}_{\vzero} + \vmu = \vmu, \quad\text{and} \margintag{using linearity of expectation \eqref{eq:linearity_expectation}} \\
    \Var{\rX} &= \Var{\msqrt{\mSigma} \rY + \vmu} = \msqrt{\mSigma} \underbrace{\Var{\rY}}_{\mI} \transpose{{\msqrt{\mSigma}}} = \mSigma. \margintag{using \cref{eq:linear_map_variance}}
  \end{align*}
\end{solution}

\begin{solution}{non_affine_transformations_of_gaussians}
  \begin{enumerate}[beginpenalty=10000]
    \item \textbf{Yes}. In the following, we give one example of a non-affine transformation that preserves the Gaussianity of a random vector.

    Let $\rX \sim \N{\vmu}{\mSigma}$ be a Gaussian random vector in $\R^d$.
    Define the coordinate-wise transformation $\vphi : \R^d \to \R^d$ as $(\vphi(\vx))_i = \phi_i(x_i)$ with \begin{align*}
      \phi_i(x_i) \defeq \begin{cases}
        - (x_i - \mu_i) + \mu_i & \abs{x_i - \mu_i} < 1 \\
        x_i & \text{otherwise}.
      \end{cases}
    \end{align*}
    Intuitively, $\phi_i$ simply flips all the ``mass'' in a neighborhood of $\mu_i$ on the $i$-th coordinate.
    Due to the symmetry of the Gaussian distribution around its mean, you could easily imagine that the transformation still preserves the Gaussian property.
    Also, this function cannot be an affine transformation since it is not continuous.

    To prove Gaussianity, we can use the change of variables formula \eqref{eq:change_of_variables}.
    Let \begin{align*}
      \rY \defeq \vphi(\rX), \quad p_{\rY}(\vy) = p_{\rX}(\vphi^{-1}(\vy)) \cdot \abs{\det{\jac \vphi^{-1}(\vy)}} \quad (\forall \vy \in \R^d).
    \end{align*}
    There are two cases for $\vy$: \begin{enumerate}
      \item If $\abs{y_i - \mu_i} < 1$, then $x_i = \phi_i^{-1}(y_i) = -(y_i - \mu_i) + \mu_i$.
      Also note that in this case, the Jacobian matrix is simply a diagonal matrix with $-1$ in the $i$-th position and $1$ elsewhere.
      Hence, its determinant is $-1$.
      We have \begin{align*}
        p_{\rY}(\vy) &= p_{\rX}(\vphi^{-1}(\vy)) \cdot \abs{\det{\jac \vphi^{-1}(\vy)}} \\
        &= p_{\rX}(\vphi^{-1}(\vy)) \\
        &= p_{\rX}\parentheses*{\begin{bmatrix}
          \vdots \\
          -(y_i - \mu_i) + \mu_i \\
          \vdots
        \end{bmatrix}} \\
        &= p_{\rX}\parentheses*{\begin{bmatrix}
          \vdots \\
          y_i \\
          \vdots
        \end{bmatrix}} \margintag{since $p_{\rX}$ is symmetric w.r.t. $\mu_i$} \\
        &= p_{\rX}(\vy).
      \end{align*}

      \item If $\abs{y_i - \mu_i} \geq 1$, then $p_{\rY}(\vy) = p_{\rX}(\vy)$ since $\vphi$ is the identity function in this case.
    \end{enumerate}\vspace{1ex}
    Thus, $\rY$ has an identical PDF to $\rX$ and is therefore Gaussian.

    \item \textbf{Yes}.
    It is difficult to calculate the distribution of $W \defeq \frac{X + YZ}{\sqrt{1 + Z^2}}$ directly since $Z$ is in the denominator.
    The trick here is to first condition $W$ on $Z$, and then integrate over the distribution of $Z$.
    For all $z \in \R$, we have \begin{align*}
      p(W \mid Z = z) &= p\left(\frac{X + YZ}{\sqrt{1 + Z^2}} \middle| Z = z\right) \\
      &= p\left(\frac{X + Yz}{\sqrt{1 + z^2}}\right) \margintag{since $X, Y \perp Z$} \\
      &= \N[W]{0}{1} \margintag{as $X + Yz \sim \N{0}{1 + z^2}$}
    \end{align*} which means that $W \mid Z = z \sim \N{0}{1}$ which is \emph{independent} of $z$!
    Using the law of total probability~\eqref{eq:lotp}, we can now write \begin{align*}
      p(W) &= \int p(W \mid Z = z) \cdot p_Z(z) \,d z \\
      &= \N[W]{0}{1} \int p_Z(z) \,d z \\
      &= \N[W]{0}{1}.
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{decision_theory}
  \begin{enumerate}[beginpenalty=10000]
    \item We have \begin{align*}
      \E*[y]{[(y - a)^2 \mid \vx]} &= a^2 - 2 a \E[y]{y}[\vx] + \const,
    \end{align*} which is easily seen to be minimized when $a = \E[y]{y}[\vx]$.

    \item We have \begin{align*}
      \E[y]{r(y, a)}[\vx] = c_1 \int_a^\infty (y-a) p(y \mid \vx) \,d y + c_2 \int_{-\infty}^a (a-y) p(y \mid \vx) \,d y.
    \end{align*}
    We can differentiate this expression with respect to $a$: \begin{align*}
      \odv{}{a} \E[y]{r(y, a)}[\vx] &= - c_1 \int_a^\infty p(y \mid \vx) \,d y + c_2 \int_{-\infty}^a p(y \mid \vx) \,d y \\
      &= - c_1 \Pr{y \geq a \mid \vx} + c_2 \Pr{y \leq a \mid \vx} \\
      &= - c_1 (1 - \Pr{y \leq a \mid \vx}) + c_2 \Pr{y \leq a \mid \vx} \\
      &= - c_1 + (c_1 + c_2) \Pr{y \leq a \mid \vx}.
    \end{align*}
    Setting this equal to zero, we find the critical point condition \begin{align*}
      \Pr{y \leq a \mid \vx} \overset{!}{=} \frac{c_1}{c_1 + c_2}.
    \end{align*}
    We obtain $\opt{a}(\vx) = \mu_\vx + \sigma_\vx \cdot \inv{\Phi}\parentheses*{\frac{c_1}{c_1 + c_2}}$ by transforming to a standard normal random variable.
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:blr}}

\begin{solution}{closed_form_linear_regression}
  \begin{enumerate}[beginpenalty=10000]
    \item We begin by deriving the gradient and Hessian of the least squares and ridge losses.
    For least squares, \begin{align*}
      \grad_{\vw} \norm{\vy - \mX \vw}_2^2 &= \grad_{\vw} (\transpose{\vw} \transpose{\mX} \mX \vw - 2 \transpose{\vw} \transpose{\mX} \vy + \norm{\vy}_2^2) \\
      &= 2 (\transpose{\mX} \mX \vw - \transpose{\mX} \vy), \\
      \hes_{\vw} \norm{\vy - \mX \vw}_2^2 &= 2 \transpose{\mX} \mX.
    \end{align*}
    Similarly, for ridge regression, \begin{align*}
      \grad_{\vw} \norm{\vy - \mX \vw}_2^2 + \lambda \norm{\vw}_2^2 &= 2 (\transpose{\mX} \mX \vw - \transpose{\mX} \vy) + 2 \lambda \vw, \\
      \hes_{\vw} \norm{\vy - \mX \vw}_2^2 + \lambda \norm{\vw}_2^2 &= 2 \transpose{\mX} \mX + 2 \lambda \mI.
    \end{align*}
    From the assumption that the Hessians are positive definite, we know that any minimizer is a unique globally optimal solution (due to strict convexity), and that $\transpose{\mX} \mX$ and $\transpose{\mX} \mX + \lambda \mI$ are invertible.

    Using the first-order optimality condition for convex functions, we attain the solutions to least squares and ridge regression by setting the respective gradient to $\vzero$.

    \item We choose $\vwhat_{\ls}$ such that $\mX \vwhat_{\ls} = \mPi_{\mX} \vy$.
    This implies that $\vy - \mX \vwhat_{\ls} \perp \mX \vw$ for all $\vw \in \R^d$.
    In other words, $\transpose{(\vy - \mX \vwhat_{\ls})} \mX \vw = 0$ for all $\vw$, which implies that $\transpose{(\vy - \mX \vwhat_{\ls})} \mX = \vzero$.
    By simple algebraic manipulation it can be seen that this condition is equivalent to the gradient condition of the previous problem.
  \end{enumerate}
\end{solution}

\begin{solution}{noise_variance_mle}
  To prevent confusion with the length of the data set $n$, we drop the subscript from the noise variance $\sigman^2$ in the following.
  We have \begin{align*}
    \hat{\sigma} &= \argmax_{\sigma} \sum_{i=1}^n \log p(y_i \mid \vx_i, \sigma) \\
    &= \argmin_{\sigma} \sum_{i=1}^n \frac{1}{2} \log(2 \sigma^2 \pi) + \frac{1}{2 \sigma^2} (y_i - \transpose{\vw} \vx_i)^2 \\
    &= \argmin_{\sigma} \frac{n}{2} \log \sigma^2 + \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2.
  \end{align*}
  We can solve this optimization problem by differentiating and setting to zero: \begin{align*}
    \pdv{}{\sigma^2} \brackets*{\frac{n}{2} \log \sigma^2 + \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2} &= 0 \\
    \frac{n}{2 \sigma^2} - \frac{1}{2 \sigma^4} \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2 &= 0 \\
    n - \frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \transpose{\vw} \vx_i)^2 &= 0 \margintag{multiplying through by $2 \sigma^2$}.
  \end{align*}
  The desired result follows by solving for $\sigma^2$.
\end{solution}

\begin{solution}{variance_around_training_data}
  Let us first derive the variance of the least squares estimate: \begin{align}
    \Var{\vwhat_{\ls}}[\mX] &= \Var{\inv{(\transpose{\mX} \mX)} \transpose{\mX} \vy}[\mX] \margintag{using \cref{eq:linear_regression}} \nonumber\\
    &= \inv{(\transpose{\mX} \mX)} \transpose{\mX} \Var{\vy}[\mX] \transpose{(\inv{(\transpose{\mX} \mX)} \transpose{\mX})} \margintag{using \cref{eq:linear_map_variance}} \nonumber\\
    &= \inv{(\transpose{\mX} \mX)} \transpose{\mX} \Var{\vy}[\mX] \mX \inv{(\transpose{\mX} \mX)}. \margintag{using $\inv{(\transpose{\mA})} = \transpose{(\inv{\mA})}$}
    \intertext{Due to the Gaussian likelihood \eqref{eq:blr_likelihood}, $\Var{\vy}[\mX] = \sigman^2\mI$, so}
    &= \sigman^2 \inv{(\transpose{\mX} \mX)}. \label{eq:least_squares_variance}
  \end{align}
  In the two-dimensional setting, i.e., data is of the form $\vx_i = \transpose{[1 \; x_i]}\ (x_i \in \R)$, we have \begin{align*}
    \transpose{\mX} \mX = \begin{bmatrix}
      n & \sum x_i \\
      \sum x_i & \sum x_i^2 \\
    \end{bmatrix}.
  \end{align*}
  Thus, \begin{align*}
    \Var{\vwhat_{\ls}}[\mX] = \sigman^2 \inv{(\transpose{\mX} \mX)} = \frac{\sigman^2}{Z} \begin{bmatrix}
      \sum x_i^2 & -\sum x_i \\
      -\sum x_i & n \\
    \end{bmatrix} \margintag{using \cref{eq:least_squares_variance}}
  \end{align*} where $Z \defeq n (\sum x_i^2) - \parentheses*{\sum x_i}^2$.

  Therefore, the predictive variance at a point $\transpose{[1 \; \xs]}$ is \begin{align*}
    \begin{bmatrix}
      1 & \xs
    \end{bmatrix} \Var{\vwhat_{\ls}} \begin{bmatrix}
      1 \\
      \xs \\
    \end{bmatrix} = \frac{\sigman^2}{Z} \sum x_i^2 - 2 x_i \xs + (\xs)^2 = \frac{\sigman^2}{Z} \sum (x_i - \xs)^2.
  \end{align*}
  Thus, the predictive variance is minimized for $\xs = \frac{1}{n} \sum x_i$.
\end{solution}

\begin{solution}{blr}
  \begin{enumerate}[beginpenalty=10000]
    \item Recall from \cref{sec:least_squares_as_mle} that the MLE and least squares estimate coincide if the noise is zero-mean Gaussian. We therefore have, \begin{align*}
      \vwhat_\MLE = \vwhat_{\ls} = \inv{(\transpose{\mX}\mX)}\transpose{\mX}\vy = \begin{bmatrix}
        0.63 \\
        1.83
      \end{bmatrix}.
    \end{align*}

    \item Recall from \cref{eq:blr_posterior} that $\vw \mid \mX, \vy \sim \N{\vmu}{\mSigma}$ with \begin{align*}
      \mSigma &= \inv{\parentheses*{\sigman^{-2} \transpose{\mX}\mX + \sigmap^{-2} \mI}} \quad\text{and} \\
      \vmu &= \sigman^{-2} \mSigma \transpose{\mX} \vy.
    \end{align*}
    Inserting $\sigman^2 = 0.1$ and $\sigmap^2 = 0.05$ yields, \begin{align*}
        \mSigma = \begin{bmatrix}
        0.019 & -0.014 \\
        -0.014 & 0.019
      \end{bmatrix}.
    \end{align*}
    Then, \begin{align*}
      \vwhat_\MAP = \vmu = \begin{bmatrix}
        0.91 \\
        1.31
      \end{bmatrix}.
    \end{align*}

    \item Recall from \cref{eq:blr_pred_posterior} that $\ys \mid \mX, \vy, \vxs \sim \N{\mu_\ys}{\sigma_{\ys}^2}$ with \begin{align*}
      \mu_\ys = \transpose{\vmu} \vxs \quad\text{and}\quad \sigma_{\ys}^2 = \transpose{\vxs} \mSigma \vxs + \sigman^2.
    \end{align*}
    Inserting $\vxs = \transpose{[3 \; 3]}$, and $\sigman^2, \vmu$ and $\mSigma$ from above yields, \begin{align*}
      \mu_\ys = 6.66 \quad\text{and}\quad \sigma_{\ys}^2 = 0.186.
    \end{align*}

    \item One would have to let $\sigmap^2 \to \infty$.
  \end{enumerate}
\end{solution}

\begin{solution}{online_blr}
  We denote by $\mX_t$ the design matrix and by $\vy_t$ the vector of observations including the first $t$ data points.
  \begin{enumerate}
    \item Note that \begin{align*}
      \transpose{\mX_t} \mX_t = \sum_{i=1}^t \vx_i \transpose{\vx_i} \quad\text{and}\quad \transpose{\mX_t} \vy_t = \sum_{i=1}^t y_i \vx_i.
    \end{align*}
    This means that after observing the $(t+1)$-st data point, we have that \begin{align*}
      \transpose{\mX_{t+1}} \mX_{t+1} &= \transpose{\mX_t} \mX_t + \vx_{t+1} \transpose{\vx_{t+1}} \quad\text{and} \\
      \transpose{\mX_{t+1}} \vy_{t+1} &= \transpose{\mX_t} \vy_t + y_{t+1} \vx_{t+1}.
    \end{align*}
    Hence, by just keeping $\transpose{\mX_t} \mX_t$ (which is a $d \times d$ matrix) and $\transpose{\mX_t} \vy_t$ (which is a vector in $\R^d$) in memory, and updating them as above, we do not need to keep the whole data in memory.

    \item One has to compute $\inv{(\sigman^{-2}\transpose{\mX_t}\mX_t + \sigmap^{-2}\mI)}$ for finding $\vmu$ and $\mSigma$ in every round.
    We can write \begin{align*}
      \inv{(\sigman^{-2}\transpose{\mX_{t+1}}\mX_{t+1} + \sigmap^{-2}\mI)} &= \sigman^2\inv{(\transpose{\mX_{t+1}}\mX_{t+1} + \sigman^2\sigmap^{-2}\mI)} \\
      &= \sigman^2\inv{(\underbrace{\transpose{\mX_t}\mX_t + \sigman^2\sigmap^{-2}\mI}_{\mA_t} + \vx_{t+1} \transpose{\vx_{t+1}})}
    \end{align*} where $\mA_t \in \R^{d \times d}$.
    Using the Woodbury matrix identity \eqref{eq:woodbury} and that we know the inverse of $\mA_t$ (from the previous iteration), the computation of the inverse of $(\mA_t + \vx_{t+1} \transpose{\vx_{t+1}})$ is of $\BigO{d^2}$, which is much better than computing the inverse of $(\mA_t + \vx_{t+1} \transpose{\vx_{t+1}})$ from scratch.
  \end{enumerate}
\end{solution}

\begin{solution}{aleatoric_and_epistemic_uncertainty}
  The law of total variance \eqref{eq:lotv_interpretation} yields the following decomposition of the predictive variance, \begin{align*}
    \Var[\ys]{\ys}[\vxs, \vx_{1:n}, y_{1:n}] = \begin{multlined}[t]
      \b{\E[\vw]{\Var[\ys]{\ys}[\vxs, \vw]}[\vx_{1:n}, y_{1:n}]} \\ + \r{\Var[\vw]{\E[\ys]{\ys}[\vxs, \vw]}[\vx_{1:n}, y_{1:n}]}
    \end{multlined}
  \end{align*} wherein the first term corresponds to the \b{aleatoric uncertainty} and the second term corresponds to the \r{epistemic uncertainty}.

  The aleatoric uncertainty is given by \begin{align*}
    \E[\vw]{\Var[\ys]{\ys}[\vxs, \vw]}[\vx_{1:n}, y_{1:n}] = \E[\vw]{\sigman^2}[\vx_{1:n}, y_{1:n}] = \sigman^2 \margintag{using the definition of $\sigman^2$. \eqref{eq:blr_likelihood}}
  \end{align*}
  For the epistemic uncertainty, \begin{align*}
    \Var[\vw]{\E[\ys]{\ys}[\vxs, \vw]}[\vx_{1:n}, y_{1:n}] &= \Var[\vw]{\transpose{\vw} \vxs}[\vx_{1:n}, y_{1:n}] \margintag{using that $\ys = \transpose{\vw} \vxs + \varepsilon$ where $\varepsilon$ is zero-mean noise} \\[5pt]
    &= \transpose{\vxs} \Var[\vw]{\vw}[\vx_{1:n}, y_{1:n}] \vxs \margintag{using \cref{eq:linear_map_variance}} \\
    &= \transpose{\vxs} \mSigma \vxs \margintag{using \cref{eq:blr_posterior}}
  \end{align*} where $\mSigma$ is the posterior covariance matrix.
\end{solution}

\begin{solution}{hyperpriors}
  \begin{enumerate}[beginpenalty=10000]
    \item Let \( \vf \defeq \mX \vw \).
    Since \( \vw \mid \vmu, \lambda \sim \N{\vmu}{\lambda^{-1} \mI_d} \), we have \[
      \vf \mid \mX, \vmu, \lambda \sim \N{\mX \vmu}{\lambda^{-1} \mX \mX^\top}. \margintag{using \cref{eq:linear_map_variance}}
    \]
    Thus, \[
      \vy \mid \mX, \vmu, \lambda \sim \N{\mX \vmu}{\lambda^{-1} \mX \mX^\top + \lambda^{-1} \mI_n}.
    \]

    \item We will denote the MLE by \( \widehat{\vmu}_1 \).
    We have \[
      \widehat{\vmu}_1 = \argmax_{\vmu} p(\vy \mid \mX, \vmu, \lambda) = \argmax_{\vmu} \log \N[\vy]{\mX \vmu}{\mSigma_{\vy}}.
    \]
    We can simplify \begin{align*}
      - \log \N[\vy]{\mX \vmu}{\mSigma_{\vy}} &= \frac{1}{2} (\vy - \mX \vmu)^\top \mSigma_{\vy}^{-1} (\vy - \mX \vmu) + \const \\
      &= \frac{1}{2} \vmu^\top \mX^\top \mSigma_{\vy}^{-1} \mX \vmu - \vmu^\top \mX^\top \mSigma_{\vy}^{-1} \vy + \const.
    \end{align*}
    Taking the gradient with respect to \( \vmu \), we obtain \[
      \grad_{\vmu} \left( \frac{1}{2} \vmu^\top \mX^\top \mSigma_{\vy}^{-1} \mX \vmu - \vy^\top \mSigma_{\vy}^{-1} \mX \vmu + \const \right) = \mX^\top \mSigma_{\vy}^{-1} \mX \vmu - \mX^\top \mSigma_{\vy}^{-1} \vy.
    \]
    This is zero iff \( \vmu = (\mX^\top \mSigma_{\vy}^{-1} \mX)^{-1} \mX^\top \mSigma_{\vy}^{-1} \vy \).

    \item By Bayes' rule~\eqref{eq:bayes_rule}, \[
      p(\vmu \mid \mX, \vy, \lambda) \propto p(\vy \mid \mX, \vmu, \lambda) \cdot p(\vmu).
    \]
    Taking the negative logarithm, \begin{align*}
      - \log p(\vmu \mid \mX, \vy, \lambda) &= - \log p(\vy \mid \mX, \vmu, \lambda) - \log p(\vmu) + \const.
      \intertext{Analogously to the previous question, we can simplify}
      &= \frac{1}{2} (\vy - \mX \vmu)^\top \mSigma_{\vy}^{-1} (\vy - \mX \vmu) + \frac{1}{2} \vmu^\top \vmu + \const \\
      &= \frac{1}{2} \vmu^\top (\mX^\top \mSigma_{\vy}^{-1} \mX + \mI_d) \vmu - \vmu^\top \mX^\top \mSigma_{\vy}^{-1} \vy + \const.
    \end{align*}
    This is a quadratic in \( \vmu \), so the posterior distribution must be Gaussian.
    By matching the terms with \[ \log \N[\vx]{\vmu'}{\mSigma'} = - \frac{1}{2} \vx^\top \mSigma'^{-1} \vx + \vx^\top \mSigma'^{-1} \vmu' + \const,\] we obtain the covariance matrix \( (\mX^\top \mSigma_{\vy}^{-1} \mX + \mI_d)^{-1} \) and the mean vector \( \mSigma_{\vmu} \mX^\top \mSigma_{\vy}^{-1} \vy \).

    \item Let \( \widetilde{\mSigma}_{\vy} \defeq \lambda \mSigma_{\vy} \).
    By Bayes' rule~\eqref{eq:bayes_rule}, \begin{align*}
      p(\lambda \mid \mX, \vy, \vmu) &\propto p(\lambda) \cdot p(\vy \mid \mX, \vmu, \lambda) \\
      &= e^{-\lambda} \cdot \N[\vy]{\mX \vmu}{\mSigma_{\vy}} \\
      &\propto e^{-\lambda} \cdot \det{\mSigma_{\vy}}^{-1/2} \exp\left( -\frac{1}{2} (\vy - \mX \vmu)^\top \mSigma_{\vy}^{-1} (\vy - \mX \vmu) \right) \\
      &= e^{-\lambda} \cdot \det{\lambda^{-1} \widetilde{\mSigma}_{\vy}}^{-1/2} \exp\left( -\frac{1}{2} (\vy - \mX \vmu)^\top (\lambda^{-1} \widetilde{\mSigma}_{\vy})^{-1} (\vy - \mX \vmu) \right) \\
      &\propto e^{-\lambda} \cdot \lambda^{n / 2} \exp\left( -\frac{\lambda}{2} (\vy - \mX \vmu)^\top \widetilde{\mSigma}_{\vy}^{-1} (\vy - \mX \vmu) \right) \margintag{using that $\inv{\lambda}\widetilde{\mSigma}_{\vy}$ is independent of $\lambda$} \\
      &= \lambda^{n / 2} \exp\left( -\lambda \left[1 + \frac{1}{2} (\vy - \mX \vmu)^\top \widetilde{\mSigma}_{\vy}^{-1} (\vy - \mX \vmu) \right] \right) \\
      &\propto \GammaDistr{\alpha}{\beta}
    \end{align*} with \( \alpha = 1 + \frac{n}{2} \) and \( \beta = 1 + \frac{1}{2} (\vy - \mX \vmu)^\top \widetilde{\mSigma}_{\vy}^{-1} (\vy - \mX \vmu) \).
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:kf}}

\begin{solution}{kf_predictive_distr}
  Recall from \cref{eq:bf_conditioning} that \begin{align}
    p(x_{t+1} \mid y_{1:t+1}) &= \frac{1}{Z} p(x_{t+1} \mid y_{1:t}) p(y_{t+1} \mid x_{t+1}). \label{eq:kf_predictive_distr_start}
  \end{align}

  Using the sensor model \eqref{eq:kf_1d_sensor_model}, \begin{align*}
    p(y_{t+1} \mid x_{t+1}) &= \frac{1}{Z'} \exp\parentheses*{-\frac{1}{2} \frac{(y_{t+1} - x_{t+1})^2}{\sigma_y^2}}.
  \end{align*}

  It remains to compute the predictive distribution, \begin{align*}
    p(x_{t+1} \mid y_{1:t}) &= \int p(x_{t+1} \mid x_t) p(x_t \mid y_{1:t}) \,d x_t \margintag{using \cref{eq:bf_prediction}} \\
    &= \frac{1}{Z''} \int \exp\parentheses*{-\frac{1}{2} \brackets*{\frac{(x_{t+1} - x_t)^2}{\sigma_x^2} + \frac{(x_t - \mu_t)^2}{\sigma_t^2}}} \,d x_t \margintag{using the motion model \eqref{eq:kf_1d_motion_model} and previous update} \\
    &= \frac{1}{Z''} \int \exp\parentheses*{-\frac{1}{2} \brackets*{\frac{\sigma_t^2 (x_{t+1} - x_t)^2 + \sigma_x^2 (x_t - \mu_t)^2}{\sigma_t^2 \sigma_x^2}}} \,d x_t.
  \intertext{The exponent is the sum of two expressions that are quadratic in $x_t$. \midx<Completing the square>{completing the square} allows rewriting any quadratic $a x^2 + b x + c$ as the sum of a squared term $a (x + \frac{b}{2 a})^2$ and a residual term $c - \frac{b^2}{4 a}$ that is independent of $x$. In this case, we have $a = (\sigma_t^2 + \sigma_x^2) / (\sigma_t^2 \sigma_x^2)$, $b = -2 (\sigma_t^2 x_{t+1} + \sigma_x^2 \mu_t) / (\sigma_t^2 \sigma_x^2)$, and $c = (\sigma_t^2 x_{t+1}^2 + \sigma_x^2 \mu_t^2) / (\sigma_t^2 \sigma_x^2)$. The residual term can be taken outside the integral, giving}
    &= \frac{1}{Z''} \exp\parentheses*{-\frac{1}{2} \brackets*{c - \frac{b^2}{4 a}}} \int \exp\parentheses*{-\frac{a}{2} \brackets*{x_t + \frac{b}{2 a}}^2} \,d x_t.
  \intertext{The integral is simply the integral of a Gaussian over its entire support, and thus evaluates to $1$. We are therefore left with only the residual term from the quadratic. Plugging back in the expressions for $a, b$, and $c$ and simplifying, we obtain}
    &= \frac{1}{Z''} \exp\parentheses*{-\frac{1}{2} \frac{(x_{t+1} - \mu_t)^2}{\sigma_t^2 + \sigma_x^2}}.
  \end{align*}
  That is, $X_{t+1} \mid y_{1:t} \sim \N{\mu_t}{\sigma_t^2 + \sigma_x^2}$.

  Plugging our results back into \cref{eq:kf_predictive_distr_start}, we obtain \begin{align*}
    p(x_{t+1} \mid y_{1:t+1}) &= \frac{1}{Z'''}  \exp\parentheses*{-\frac{1}{2} \brackets*{\frac{(x_{t+1} - \mu_t)^2}{\sigma_t^2 + \sigma_x^2} + \frac{(y_{t+1} - x_{t+1})^2}{\sigma_y^2}}}.
  \intertext{Completing the square analogously to our derivation of the predictive distribution yields,}
    &= \frac{1}{Z'''} \exp\parentheses*{-\frac{1}{2} \frac{\parentheses*{x_{t+1} - \frac{(\sigma_t^2 + \sigma_x^2) y_{t+1} + \sigma_y^2 \mu_t}{\sigma_t^2 + \sigma_x^2 + \sigma_y^2}}^2}{\frac{(\sigma_t^2 + \sigma_x^2) \sigma_y^2}{\sigma_t^2 + \sigma_x^2 + \sigma_y^2}}}.
  \end{align*}
  Hence, $X_{t+1} \mid y_{t+1} \sim \N{\mu_{t+1}}{\sigma_{t+1}^2}$ as defined in \cref{eq:kalman_posterior}.
\end{solution}

\begin{solution}{blr_as_kf}
  We prove the equivalence by induction.
  Note that the base case is satisfied trivially since the priors are identical.

  Assume after $t-1$ steps that $(\vmu_{t-1}, \mSigma_{t-1})$ coincide with the BLR posterior for the first $t-1$ data points.
  We will show that the Kalman filter update equations yield the BLR posterior for the first $t$ data points.

  \paragraph{Covariance update:} \begin{align*}
    \mSigma_t &= \mSigma_{t-1} - \vk_t \transpose{\vx_t} \mSigma_{t-1} \\
    &= \mSigma_{t-1} - \frac{(\mSigma_{t-1} \vx_t) \transpose{(\mSigma_{t-1} \vx_t)}}{\transpose{\vx_t}\mSigma_{t-1}\vx_t + 1} \margintag{using the symmetry of $\mSigma_{t-1}$} \\
    &= \inv{\parentheses*{\inv{\mSigma_{t-1}} + \vx_t \transpose{\vx_t}}} \margintag{using the Sherman-Morrison formula~\eqref{eq:woodbury} with $\inv{\mA} = \mSigma_{t-1}$} \\
    &= \inv{\parentheses*{\transpose{\mX_{1:t-1}} \mX_{1:t-1} + \vx_t \transpose{\vx_t} + \mI}} \margintag{by the inductive hypothesis and using \cref{eq:blr_posterior}} \\
    &= \inv{\parentheses*{\transpose{\mX_{1:t}} \mX_{1:t} + \mI}} \\
    &= \mSigma_t^{\mathrm{BLR}}. \margintag{using \cref{eq:blr_posterior}}
  \end{align*}

  \paragraph{Mean update:} \begin{align*}
    \inv{\mSigma_t} \vmu_t &= \inv{\mSigma_t} \vmu_{t-1} + \inv{\mSigma_t} \vk_t (y_t - \transpose{\vx_t} \vmu_{t-1}) \\
    &= \inv{\mSigma_{t-1}} \vmu_{t-1} + \vx_t \transpose{\vx_t} \vmu_{t-1} + \inv{\mSigma_t} \vk_t (y_t - \transpose{\vx_t} \vmu_{t-1}) \margintag{using ${\inv{\mSigma_t} = \inv{\mSigma_{t-1}} + \vx_t \transpose{\vx_t}}$} \\
    &= \inv{\mSigma_{t-1}} \vmu_{t-1} + \vx_t \transpose{\vx_t} \vmu_{t-1} + \vx_t (y_t - \transpose{\vx_t} \vmu_{t-1}) \margintag{using ${\inv{\mSigma_t} \vk_t = \vx_t}$} \\
    &= \inv{\mSigma_{t-1}} \vmu_{t-1} + \vx_t y_t \margintag{canceling terms} \\
    &= \transpose{\mX_{1:t-1}} \vy_{1:t-1} + \vx_t y_t \margintag{by the inductive hypothesis and using \cref{eq:blr_posterior}} \\
    &= \transpose{\mX_{1:t}} \vy_{1:t} \\
    &= \inv{\mSigma_t} \mSigma_t \transpose{\mX_{1:t}} \vy_{1:t} \\
    &= \inv{\mSigma_t} \mSigma_t^{\mathrm{BLR}} \transpose{\mX_{1:t}} \vy_{1:t} \margintag{using the covariance update above} \\
    &= \inv{\mSigma_t} \vmu_t^{\mathrm{BLR}}. \margintag{using \cref{eq:blr_posterior}}
  \end{align*}
  This completes the induction.
\end{solution}

\begin{solution}{parameter_estimation_with_kf}
  \begin{enumerate}[beginpenalty=10000]
    \item The given parameter estimation problem can be formulated as a Kalman filter in the following way: \begin{align*}
      x_t &= \pi &&\forall t, \\
      y_t &= x_t + \eta_t &&\eta_t \sim \N{0}{\sigma_y^2}.
    \end{align*}
    Thus, in terms of Kalman filters, this yields $f = h = 1, \varepsilon_t = \sigma_x^2 = 0$.
    Using \cref{eq:kalman_gain_1d}, the \emph{Kalman gain} is given by \begin{align*}
      k_{t+1} = \frac{\sigma_t^2}{\sigma_t^2 + \sigma_y^2},
    \end{align*} whereas the \emph{variance of the estimation error} $\sigma_t^2$ satisfies \begin{align*}
      \sigma_{t+1}^2 = \sigma_y^2 k_{t+1} = \frac{\sigma_t^2 \sigma_y^2}{\sigma_t^2 + \sigma_y^2}. \margintag{using \cref{eq:kalman_variance_1d}}
    \end{align*}
    To get the closed form, observe that \begin{align*}
      \frac{1}{\sigma_{t+1}^2} = \frac{1}{\sigma_t^2} + \frac{1}{\sigma_y^2} = \frac{1}{\sigma_{t-1}^2} + \frac{2}{\sigma_y^2} = \cdots = \frac{1}{\sigma_0^2} + \frac{t+1}{\sigma_y^2},
    \end{align*} yielding, \begin{align*}
      \sigma_{t+1}^2 = \frac{\sigma_0^2 \sigma_y^2}{(t+1)\sigma_0^2 + \sigma_y^2} \quad\text{and}\quad k_{t+1} = \frac{\sigma_0^2}{(t+1)\sigma_0^2 + \sigma_y^2}.
    \end{align*}

    \item When $t \to \infty$, we get $k_{t+1} \to 0$ and $\sigma_{t+1}^2 \to 0$, giving \begin{align*}
      \mu_{t+1} = \mu_t + k_{t+1} (y_{t+1} - \mu_t) = \mu_t, \margintag{using \cref{eq:kf_update_1d}}
    \end{align*} thus resulting in a stationary sequence.

    \item Observe that $\sigma_0^2 \to \infty$ implies $k_{t+1} = \frac{1}{t+1}$.
    Therefore, \begin{align*}
      \mu_{t+1} &= \mu_t + \frac{1}{t+1}(y_{t+1} - \mu_t) \margintag{using \cref{eq:kf_update_1d}} \\
      &= \frac{t}{t+1}\mu_t + \frac{y_{t+1}}{t+1} \\
      &= \frac{t}{t+1}\parentheses*{\frac{t-1}{t}\mu_{t-1} + \frac{y_t}{t}} + \frac{y_{t+1}}{t+1} \margintag{using \cref{eq:kf_update_1d}} \\
      &= \frac{t-1}{t+1}\mu_{t-1} + \frac{y_t + y_{t+1}}{t+1} \\
      &\;\;\vdots \\
      &= \frac{y_1 + \cdots + y_{t+1}}{t+1},
    \end{align*} which is simply the \emph{sample mean}.
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:gp}}

\begin{solution}{gaussian_kernel_feature_space}
  \begin{enumerate}[beginpenalty=10000]
    \item We have for any $j \in \NatZ$ and $x, y \in \R$, \begin{align*}
      \frac{1}{\sqrt{j!}} e^{-\frac{x^2}{2}} x^j \cdot \frac{1}{\sqrt{j!}} e^{-\frac{y^2}{2}} y^j &= e^{-\frac{x^2 + y^2}{2}} \frac{(x y)^j}{j!}.
    \end{align*}
    Summing over all $j$, we obtain \begin{align*}
      \transpose{\vphi(x)} \vphi(y) &= \sum_{j=0}^\infty \phi_j(x) \phi_j(y) \\
      &= e^{-\frac{x^2 + y^2}{2}} \sum_{j=0}^\infty \frac{(x y)^j}{j!} \\
      &= e^{-\frac{x^2 + y^2}{2}} e^{x y} \margintag{using the Taylor series expansion for the exponential function} \\
      &= e^{-\frac{(x - y)^2}{2}} \\
      &= k(x, y).
    \end{align*}

    \item As we have seen in \cref{sec:blr:function_space_view}, the effective dimension is $n$.
    The crucial difference of kernelized regression (e.g., Gaussian processes) to linear regression is that the effective dimension \emph{grows} with the sample size, whereas it is fixed for linear regression.
    Models where the effective dimension may depend on the sample size are called \emph{non-parametric} models, and models where the effective dimension is fixed are called \emph{parametric} models.
  \end{enumerate}
\end{solution}

\begin{solution}{kernels_on_the_circle}
  \begin{enumerate}[beginpenalty=10000]
    \item \textbf{Yes}. This is merely a restriction of the standard Gaussian kernel in $\R^2$ onto its subset $\mathbb{S}$.

    \item We have \[
      \begin{multlined}[t]
        \underbrace{
      \begin{pmatrix}
      1 & e^{-\nicefrac{\pi^2}{32}} & e^{-\nicefrac{\pi^2}{8}} & e^{-\nicefrac{\pi^2}{32}} \\
      e^{-\nicefrac{\pi^2}{32}} & 1 & e^{-\nicefrac{\pi^2}{32}} & e^{-\nicefrac{\pi^2}{8}} \\
      e^{-\nicefrac{\pi^2}{8}} & e^{-\nicefrac{\pi^2}{32}} & 1 & e^{-\nicefrac{\pi^2}{32}} \\
      e^{-\nicefrac{\pi^2}{32}} & e^{-\nicefrac{\pi^2}{8}} & e^{-\nicefrac{\pi^2}{32}} & 1
      \end{pmatrix}
      }_{\boldsymbol{K}}
      \begin{pmatrix}
      -1 \\
      1 \\
      -1 \\
      1
      \end{pmatrix} \\
      =
      \underbrace{
      \left(
      1 + e^{-\nicefrac{\pi^2}{8}} - 2 e^{-\nicefrac{\pi^2}{32}}
      \right)}_{\approx -0.18}
      \begin{pmatrix}
      -1 \\
      1 \\
      -1 \\
      1
      \end{pmatrix}
      \end{multlined}
    \]

    \item \textbf{No}. (2) gives an example of a negative eigenvalue in a covariance matrix.
    This implies that $k_{\text{i}}$ cannot be positive semi-definite.

    \item \textbf{Yes}. We have $k_h(\theta, \theta') = \langle \vphi(\theta), \vphi(\theta') \rangle_{\mathbb{R}^{L}}$ with \[
      \vphi(\theta) \defeq \frac{1}{\sqrt{C_{\kappa}}} \begin{pmatrix}
      1 \\
      e^{-\frac{\kappa^2}{4}} \sqrt{2} \cos(\theta) \\
      e^{-\frac{\kappa^2}{4}} \sqrt{2} \sin(\theta) \\
      \vdots \\
      e^{-\frac{\kappa^2}{4} (L-1)^2} \sqrt{2} \cos((L-1) \theta) \\
      e^{-\frac{\kappa^2}{4} (L-1)^2} \sqrt{2} \sin((L-1) \theta)
      \end{pmatrix}.
    \]
  \end{enumerate}
\end{solution}

\begin{solution}{kf_as_gp}
  First we look at the mean, \begin{align*}
    \mu(t) = \E{X_t} = \E{X_{t-1} + \varepsilon_{t-1}} = \E{X_{t-1}} = \mu(t-1).
  \end{align*}
  Knowing that $\mu(0) = 0$, we can derive that $\mu(t) = 0 \; (\forall t)$.

  Now we look at the variance of $X_t$, \begin{align*}
    \Var{X_t} = \Var{X_0 + \sum_{\tau=0}^{t-1} \varepsilon_{\tau}} = \sigma_0^2 + t \sigma_x^2. \margintag{using that the noise is independent}
  \end{align*}
  Finally, we look at the distribution of $\transpose{[f(t) \; f(t')]}$ for arbitrary $t \leq t'$. \begin{align*}
    \begin{bmatrix}
      X_t \\
      X_{t'}
    \end{bmatrix} = \begin{bmatrix}
      X_t \\
      X_t
    \end{bmatrix} + \sum_{\tau = t}^{t' - 1} \begin{bmatrix}
      0 \\
      \varepsilon_{\tau}
    \end{bmatrix}.
  \end{align*}
  Therefore, we get that \begin{align*}
    \begin{bmatrix}
      X_t \\
      X_{t'}
    \end{bmatrix} &\sim \N*{\begin{bmatrix}
      0 \\
      0
    \end{bmatrix}}{\Var{X_t} \begin{bmatrix}
      1 & 1 \\
      1 & 1
    \end{bmatrix} + (t' - t) \begin{bmatrix}
      0 & 0 \\
      0 & \sigma_x^2
    \end{bmatrix}} \\
    &= \N*{\begin{bmatrix}
      0 \\
      0
    \end{bmatrix}}{\begin{bmatrix}
      \Var{X_t} & \Var{X_t} \\
      \Var{X_t} & \Var{X_t} + (t' - t) \sigma_x^2
    \end{bmatrix}}.
  \end{align*}
  We take the kernel $k_{\mathrm{KF}}(t, t')$ to be the covariance between $f(t)$ and $f(t')$, which is $\Var{X_t} = \sigma_0^2 + \sigma_x^2 t$.
  Notice, however, that we assumed $t \leq t'$.
  Thus, overall, the kernel is described by \begin{align*}
    k_{\mathrm{KF}}(t, t') = \sigma_0^2 + \sigma_x^2 \min\{t, t'\}.
  \end{align*}
\end{solution}

\begin{solution}{reproducing_kernel_hilbert_space_properties}
  \begin{enumerate}[beginpenalty=10000]
    \item As $f \in \spH_k(\spX)$ we can express $f(\vx)$ for some $\beta_i \in \R$ and $\vx_i \in \spX$ as \begin{align*}
      f(\vx) &= \sum_{i=1}^n \beta_i k(\vx, \vx_i) \\
      &= \sum_{i=1}^n \beta_i \ip{k(\vx_i,\cdot), k(\vx,\cdot)}_k \\
      &= \ip{\sum_{i=1}^n \beta_i k(\vx_i,\cdot), k(\vx,\cdot)}_k \\
      &= \ip{f(\cdot), k(\vx,\cdot)}_k.
    \end{align*}

    \item By applying Cauchy-Schwarz, \begin{align*}
      |f(\vx) - f(\vy)| &= |\ip{f, k(\vx,\cdot) - k(\vy,\cdot)}_k| \\
      &\leq \norm{f}_k \norm{k(\vx,\cdot) - k(\vy,\cdot)}_k
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{representer_theorem}
  We denote by $f_{\sA} \defeq \Pi_{\sA} f$ the orthogonal projection of $f$ onto $\mathrm{span}\{k(\vx_1, \cdot), \dots, k(\vx_n, \cdot)\}$ which implies that $\hat{f}_{\sA}$ is a linear combination of $k(\vx_1, \cdot), \dots, k(\vx_n, \cdot)$.

  We then have that $f_{\sA}^\perp \defeq f - f_{\sA}$ is orthogonal to $\mathrm{span}\{k(\vx_1, \cdot), \dots, k(\vx_n, \cdot)\}$.
  Therefore, for any $i \in [n]$, \begin{align*}
    f(\vx) = \ip{f, k(\vx_i, \cdot)}_k = \ip{f_{\sA} + f_{\sA}^\perp, k(\vx_i, \cdot)}_k = \ip{f_{\sA}, k(\vx_i, \cdot)}_k = f_{\sA}(\vx)
  \end{align*} which implies \begin{align*}
    \spL(f(\vx_1), \dots, f(\vx_n)) = \spL(f_{\sA}(\vx_1), \dots, f_{\sA}(\vx_n)).
  \end{align*}

  Denoting the objective of \cref{eq:representer_theorem_obj} by $j(f)$ and noting that ${\norm{f_{\sA}}_k \leq \norm{f}_k}$, we have that $j(f_{\sA}) \leq j(f)$.
  Therefore, if $\hat{f}$ minimizes $j(f)$, then ${\hat{f}_{\sA} \defeq \Pi_{\sA} \hat{f}}$ is also a minimizer since $j(\hat{f}_{\sA}) \leq j(\hat{f})$.
  Thus, we conclude that there exists some $\valphahat \in \R^n$ such that $\hat{f}_{\sA}(\vx) = \sum_{i=1}^n \hat{\alpha}_i k(\vx_i, \vx)$ minimizes $j(f)$.
\end{solution}

\begin{solution}{mle_and_map_of_gps}
  \begin{enumerate}[beginpenalty=10000]
    \item By the representer theorem~\eqref{eq:representer_theorem}, $\hat{f}(\vx) = \transpose{\valphahat} \vk_{\vx,A}$.
    In particular, we have $\vf = \mK \valphahat$ and therefore \begin{align*}
      - \log p(y_{1:n} \mid \vx_{1:n}, f) &= \frac{1}{2 \sigman^2} \norm{\vy - \vf}_2^2 + \const \\
      &= \frac{1}{2 \sigman^2} \norm{\vy - \mK \valphahat}_2^2 + \const.
    \end{align*}
    The regularization term simplifies to \begin{align*}
      \norm{f}_k^2 = \ip{f,f}_k = \transpose{\valphahat} \mK \valphahat = \norm{\valphahat}_{\mK}^2.
    \end{align*}
    Combining, we have that \begin{align*}
      \valphahat = \argmin_{\valpha \in \R^n} \frac{1}{2 \sigman^2} \norm{\vy - \mK \valpha}_2^2 + \frac{1}{2} \norm{\valpha}_{\mK}^2
    \end{align*} as desired.
    It follows by multiplying through with $2 \sigman^2$ that $\lambda = \sigman^2$.

    \item Expanding the objective determined in (1), we are looking for the minimizer of \begin{align*}
      \transpose{\valpha} (\sigman^2 \mK + \mK^2) \valpha - 2 \transpose{\vy} \mK \valpha + \transpose{\vy} \vy.
    \end{align*}
    Differentiating with respect to the coefficients $\valpha$, we obtain the minimizer $\valphahat = \inv{(\mK + \sigman^2 \mI)} \vy$.
    Thus, the prediction at a point $\vxs$ is $\transpose{\vk_{\vxs,A}} \inv{(\mK + \sigman^2 \mI)} \vy$ which coincides with the MAP estimate.
  \end{enumerate}
\end{solution}

\begin{solution}{gradient_of_mll}
  \begin{enumerate}[beginpenalty=10000]
    \item Applying the two hints \cref{eq:gradient_of_mll_hint1,eq:gradient_of_mll_hint2} to \cref{eq:gp_mle_2} yields, \begin{align*}
      \pdv{}{\theta_j} \log p(\vy \mid \mX, \vtheta) &= \frac{1}{2} \transpose{\vy} \inv{\mK_{\vy,\vtheta}} \pdv{\mK_{\vy,\vtheta}}{\theta_j} \inv{\mK_{\vy,\vtheta}} \vy - \frac{1}{2} \tr{\inv{\mK_{\vy,\vtheta}} \pdv{\mK_{\vy,\vtheta}}{\theta_j}}.
    \intertext{We can simplify to}
      &= \frac{1}{2} \tr{\transpose{\vy} \inv{\mK_{\vy,\vtheta}} \pdv{\mK_{\vy,\vtheta}}{\theta_j} \inv{\mK_{\vy,\vtheta}} \vy} - \frac{1}{2} \tr{\inv{\mK_{\vy,\vtheta}} \pdv{\mK_{\vy,\vtheta}}{\theta_j}} \margintag{using that $\transpose{\vy} \inv{\mK_{\vy,\vtheta}} \pdv{\mK_{\vy,\vtheta}}{\theta_j} \inv{\mK_{\vy,\vtheta}} \vy$ is a scalar} \\
      &= \frac{1}{2} \tr{\inv{\mK_{\vy,\vtheta}} \vy \transpose{\vy} \inv{\mK_{\vy,\vtheta}} \pdv{\mK_{\vy,\vtheta}}{\theta_j} - \inv{\mK_{\vy,\vtheta}} \pdv{\mK_{\vy,\vtheta}}{\theta_j}} \margintag{using the cyclic property and linearity of the trace} \\
      &= \frac{1}{2} \tr{\inv{\mK_{\vy,\vtheta}} \vy \transpose{(\inv{\mK_{\vy,\vtheta}} \vy)} \pdv{\mK_{\vy,\vtheta}}{\theta_j} - \inv{\mK_{\vy,\vtheta}} \pdv{\mK_{\vy,\vtheta}}{\theta_j}} \margintag{using that $\inv{\mK_{\vy,\vtheta}}$ is symmetric} \\
      &= \frac{1}{2} \tr{(\alpha \transpose{\alpha} - \inv{\mK_{\vy,\vtheta}}) \pdv{\mK_{\vy,\vtheta}}{\theta_j}}.
    \end{align*}

    \item We denote by $\tilde{\mK}$ the covariance matrix of $\vy$ for the covariance function $\tilde{k}$, so $\mK_{\vy,\vtheta} = \theta_0 \tilde{\mK}$.
    Then, \begin{align*}
      \pdv{}{\theta_0} \log p(\vy \mid \mX, \vtheta) &= \frac{1}{2} \tr{(\theta_0^{-2} \inv{\tilde{\mK}} \vy \transpose{(\inv{\tilde{\mK}} \vy)} - \inv{\theta_0} \inv{\tilde{\mK}}) \tilde{\mK}}. \margintag{using \cref{eq:gradient_mll}}
    \end{align*}
    Simplifying the terms and using linearity of the trace, we obtain that \begin{align*}
      \pdv{}{\theta_0} \log p(\vy \mid \mX, \vtheta) = 0 \quad\iff\quad \theta_0 = \frac{1}{n} \tr{\vy \transpose{\vy} \inv{\tilde{\mK}}}.
    \end{align*}
    If we define $\tilde{\mLambda} \defeq \inv{\tilde{\mK}}$ as the precision matrix associated to $\vy$ for the covariance function $\tilde{k}$, we can express $\opt{\theta_0}$ in closed form as \begin{align}
      \opt{\theta_0} = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n \tilde{\mLambda}(i, j) y_i y_j. \label{eq:gradient_of_mll_theta_0}
    \end{align}

    \item We immediately see from \cref{eq:gradient_of_mll_theta_0} that $\opt{\theta_0}$ scales by $s^2$ if $\vy$ is scaled by $s$.
  \end{enumerate}
\end{solution}

\begin{solution}{uniform_convergence_of_fourier_features}
  \begin{enumerate}[beginpenalty=10000]
    \item We have that $s(\cdot) \in [-\sqrt{2}, \sqrt{2}]$, and hence, $s(\vDelta_i)$ is $\sqrt{2}$-sub-Gaussian.\footnote{see \cref{ex:sub_gaussian_examples}}
    It then follows from Hoeffding's inequality \eqref{eq:hoeffdings_inequality} that \begin{align*}
      \Pr{|f(\vDelta_i)| \geq \epsilon} \leq 2 \exp\parentheses*{-\frac{m \epsilon^2}{4}}.
    \end{align*}

    \item We can apply Markov's inequality \eqref{eq:markov_inequality} to obtain \begin{align*}
      \Pr{\norm{\grad f(\opt{\vDelta})}_2 \geq \frac{\epsilon}{2 r}} &= \Pr{\norm{\grad f(\opt{\vDelta})}_2^2 \geq \parentheses*{\frac{\epsilon}{2 r}}^2} \\
      &\leq \parentheses*{\frac{2 r \E*{\norm{\grad f(\opt{\vDelta})}_2}}{\epsilon}}^2.
    \end{align*}
    It remains to bound the expectation.
    We have \begin{align*}
      \E*{\norm{\grad f(\opt{\vDelta})}_2^2} &= \E*{\norm{\grad s(\opt{\vDelta}) - \grad k(\opt{\vDelta})}_2^2} \\
      &= \E*{\norm{\grad s(\opt{\vDelta})}_2^2} - 2 \transpose{\grad k(\opt{\vDelta})} \E*{\grad s(\opt{\vDelta})} + \E*{\norm{\grad k(\opt{\vDelta})}_2^2}. \margintag{using linearity of expectation \eqref{eq:linearity_expectation}}
      \intertext{Note that $\E*{\grad s(\vDelta)} = \grad k(\vDelta)$ using \cref{eq:swap_grad_exp_order} and using that $s(\vDelta)$ is an unbiased estimator of $k(\vDelta)$. Therefore,}
      &= \E*{\norm{\grad s(\opt{\vDelta})}_2^2} - \E*{\norm{\grad k(\opt{\vDelta})}_2^2} \\
      &\leq \E*{\norm{\grad s(\opt{\vDelta})}_2^2} \\
      &\leq \E*[\vomega \sim p]{\norm{\vomega}_2^2} = \sigmap^2. \margintag{using that $s$ is the $\cos$ of a linear function in $\vomega$}
    \end{align*}

    \item Using a union bound \eqref{eq:union_bound} and then the result of (1), \begin{align*}
      \Pr{\bigcup_{i=1}^T |f(\vDelta_i)| \geq \frac{\epsilon}{2}} \leq \sum_{i=1}^T \Pr{|f(\vDelta_i)| \geq \frac{\epsilon}{2}} \leq 2 T \exp\parentheses*{-\frac{m \epsilon^2}{16}}.
    \end{align*}

    \item First, note that by contraposition, \begin{align*}
      \sup_{\vDelta \in \spM_\Delta} |f(\vDelta)| \geq \epsilon \implies \text{$\exists i$. $|f(\vDelta_i)| \geq \frac{\epsilon}{2}$ or $\norm{\grad f(\opt{\vDelta})}_2 \geq \frac{\epsilon}{2 r}$},
    \end{align*} and therefore, \begin{align*}
      \Pr{\sup_{\vDelta \in \spM_\Delta} |f(\vDelta)| \geq \epsilon} &\leq \Pr{\bigcup_{i=1}^T |f(\vDelta_i)| \geq \frac{\epsilon}{2} \cup \norm{\grad f(\opt{\vDelta})}_2 \geq \frac{\epsilon}{2 r}} \\
      &\leq \Pr{\bigcup_{i=1}^T |f(\vDelta_i)| \geq \frac{\epsilon}{2}} + \Pr{\norm{\grad f(\opt{\vDelta})}_2 \geq \frac{\epsilon}{2 r}} \margintag{using a union bound \eqref{eq:union_bound}} \\
      &\leq 2 T \exp\parentheses*{-\frac{m \epsilon^2}{2^4}} + \parentheses*{\frac{2 r \sigmap}{\epsilon}}^2 \margintag{using the results from (2) and (3)} \\
      &\leq \alpha r^{-d} + \beta r^2 \margintag{using $T \leq (4 \; \mathrm{diam}(\spM) / r)^d$}
      \intertext{with $\alpha = 2 (4 \; \mathrm{diam}(\spM))^d \exp\parentheses*{-\frac{m \epsilon^2}{2^4}}$ and $\beta = \parentheses*{\frac{2 \sigmap}{\epsilon}}^2$. Using the hint, we obtain}
      &= 2 \beta^{\frac{d}{d+2}} \alpha^{\frac{2}{d+2}} \\
      &= 2 \parentheses*{2 \parentheses*{\frac{2^3 \sigmap \mathrm{diag}(\spM)}{\epsilon}}^{2 d}}^{\frac{1}{d+2}} \exp\parentheses*{- \frac{m \epsilon^2}{2^3 (d+2)}} \\
      &\leq 2^8 \parentheses*{\frac{\sigmap \mathrm{diag}(\spM)}{\epsilon}}^2 \exp\parentheses*{- \frac{m \epsilon^2}{2^3 (d+2)}} \margintag{using $\frac{\sigmap \mathrm{diam}(\spM)}{\epsilon} \geq 1$}
    \end{align*}

    \item We have \begin{align*}
      \sigmap^2 &= \E[\vomega \sim p]{\transpose{\vomega} \vomega} \\
      &= \int \transpose{\vomega} \vomega \cdot p(\vomega) \,d\vomega \\
      &= \int \transpose{\vomega} \vomega \cdot e^{i \transpose{\vomega} \vzero} p(\vomega) \,d\vomega.
      \intertext{Now observe that $\pdv[order=2]{}{\Delta_j} \int e^{i \transpose{\vomega} \vDelta} p(\vomega) \,d\vomega = - \int \omega_j^2 e^{i \transpose{\vomega} \vDelta} p(\vomega) \,d\vomega$. Thus,}
      &= - \tr{\hes_{\vDelta} \left. \int p(\vomega) e^{i \transpose{\vomega} \vDelta} \,d\vomega \right|_{\vDelta = \vzero}} \\
      &= - \tr{\hes_{\vDelta} k(\vzero)}. \margintag{using that $p$ is the Fourier transform of $k$ \eqref{eq:spectral_density}}
    \end{align*}
    Finally, we have for the Gaussian kernel that \begin{align*}
      \pdv[order=2]{}{\Delta_j} \left. \exp\parentheses*{- \frac{\transpose{\vDelta} \vDelta}{2 h^2}} \right|_{\Delta_j = 0} = - \frac{1}{h^2}.
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{sor}
  \begin{enumerate}[beginpenalty=10000]
    \item We write $\tilde{\vf} \defeq \transpose{[\vf \; \fs]}$.
    From the definition of SoR \eqref{eq:sor}, we gather \begin{align}
      q_{\mathrm{SoR}}(\tilde{\vf} \mid \vu) = \N*[\tilde{\vf}]{\begin{bmatrix}
        \mK_{\sA\sU} \inv{\mK_{\sU\sU}} \\
        \mK_{\star\sU} \inv{\mK_{\sU\sU}}
      \end{bmatrix} \vu}{\mzero}. \label{eq:sor_helper}
    \end{align}
    We know that $\tilde{\vf}$ and $\vu$ are jointly Gaussian, and hence, the marginal distribution of $\tilde{\vf}$ is also Gaussian.
    We have for the mean and variance that \begin{align*}
      \E{\tilde{\vf}} &= \E{\E{\tilde{\vf}}[\vu]} \margintag{using the tower rule \eqref{eq:tower_rule}} \\
      &= \E[\vu]{\begin{bmatrix}
        \mK_{\sA\sU} \inv{\mK_{\sU\sU}} \\
        \mK_{\star\sU} \inv{\mK_{\sU\sU}}
      \end{bmatrix} \vu} \margintag{using \cref{eq:sor_helper}} \\
      &= \begin{bmatrix}
        \mK_{\sA\sU} \inv{\mK_{\sU\sU}} \\
        \mK_{\star\sU} \inv{\mK_{\sU\sU}}
      \end{bmatrix} \E{\vu} = \vzero \margintag{using linearity of expectation \eqref{eq:linearity_expectation}} \\
      \Var{\tilde{\vf}} &= \E*{\underbrace{\Var{\tilde{\vf}}[\vu]}_{\mzero}} + \Var*{\,\E{\tilde{\vf}}[\vu]} \margintag{using the law of total variance \eqref{eq:lotv}} \\
      &= \Var[\vu]{\begin{bmatrix}
        \mK_{\sA\sU} \inv{\mK_{\sU\sU}} \\
        \mK_{\star\sU} \inv{\mK_{\sU\sU}}
      \end{bmatrix} \vu} \margintag{using \cref{eq:sor_helper}} \\
      &= \begin{bmatrix}
        \mK_{\sA\sU} \inv{\mK_{\sU\sU}} \\
        \mK_{\star\sU} \inv{\mK_{\sU\sU}}
      \end{bmatrix} \underbrace{\Var{\vu}}_{\mK_{\sU\sU}} \transpose{\begin{bmatrix}
        \mK_{\sA\sU} \inv{\mK_{\sU\sU}} \\
        \mK_{\star\sU} \inv{\mK_{\sU\sU}}
      \end{bmatrix}} \margintag{using \cref{eq:linear_map_variance}} \\
      &= \begin{bmatrix}
        \mQ_{\sA\sA} & \mQ_{\sA\star} \\
        \mQ_{\star\sA} & \mQ_{\star\star}
      \end{bmatrix}.
    \end{align*}
    Having determined $q_{\mathrm{SoR}}(\vf, \fs)$, $q_{\mathrm{SoR}}(\fs \mid \vy)$ follows directly using the formulas for finding the Gaussian process predictive posterior \eqref{eq:gp_posterior}.

    \item The given covariance function follows directly from inspecting the derived covariance matrix, $\Var{\vf} = \mQ_{\sA\sA}$.
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:approximate_inference}}

\begin{solution}{logistic_loss_gradient}
  \begin{enumerate}[beginpenalty=10000]
    \item We have \begin{align*}
      \grad_\vw \ell_\mathrm{log}(\transpose{\vw} \vx; y) &= \grad_\vw \log(1 + \exp(-y \transpose{\vw} \vx)) \margintag{using the definition of the logistic loss \eqref{eq:logistic_loss}} \\
      &= \grad_\vw \log \frac{1 + \exp(y \transpose{\vw} \vx)}{\exp(y \transpose{\vw} \vx)} \\
      &= \grad_\vw \log(1 + \exp(y \transpose{\vw} \vx)) - y \transpose{\vw} \vx \\
      &= \frac{1}{1 + \exp(-y \transpose{\vw} \vx)} \cdot \exp(-y \transpose{\vw} \vx) \cdot y \vx - y \vx \margintag{using the chain rule} \\
      &= - y \vx \cdot \parentheses*{1 - \frac{\exp(y \transpose{\vw} \vx)}{1 + \exp(y \transpose{\vw} \vx)}} \\
      &= - y \vx \cdot \sigma(-y \transpose{\vw} \vx). \margintag{using the definition of the logistic function \eqref{eq:logistic_function}}
    \end{align*}

    \item As suggested in the hint, we compute the first derivative of $\sigma$, \begin{align*}
      \sigma'(z) \defeq \odv{}{z} \sigma(z) &= \odv{}{z} \frac{1}{1 + \exp(-z)} \margintag{using the definition of the logistic function \eqref{eq:logistic_function}} \\
      &= \frac{\exp(-z)}{(1 + \exp(-z))^2} \margintag{using the quotient rule} \\
      &= \sigma(z) \cdot (1 - \sigma(z)). \margintag{using the definition of the logistic function \eqref{eq:logistic_function}}
    \end{align*}
    We get for the Hessian of $\ell_\mathrm{log}$, \begin{align*}
      \hes_\vw \ell_\mathrm{log}(\transpose{\vw} \vx; y) &= \mD_\vw \grad_\vw \ell_\mathrm{log}(\transpose{\vw} \vx; y) \margintag{using the definition of a Hessian \eqref{eq:hessian} and symmetry} \\[5pt]
      &= - y \vx \; \mD_\vw \sigma(-y \transpose{\vw} \vx). \margintag{using the gradient of the logistic loss from (1)}
    \intertext{We have $\mD_\vw -y \transpose{\vw} \vx = -y \transpose{\vx}$ and $\mD_z \sigma(z) = \sigma'(z)$, and therefore, using the chain rule of multivariate calculus \eqref{eq:chain_rule},}
      &= -y \vx \cdot \sigma'(-y \transpose{\vw} \vx) \cdot (-y \transpose{\vx}) \\
      &= \vx \transpose{\vx} \cdot \sigma'(-y \transpose{\vw} \vx). \margintag{using $y^2 = 1$}
    \end{align*}
    Finally, recall that \begin{align*}
      \sigma(\transpose{\vw} \vx) &= \Pr{y = +1 \mid \vx, \vw} \quad\text{and} \\
      \sigma(-\transpose{\vw} \vx) &= \Pr{y = -1 \mid \vx, \vw}.
    \end{align*}
    Thus, \begin{align*}
      \sigma'(-y \transpose{\vw} \vx) &= \Pr{Y \neq y \mid \vx, \vw} \cdot (1 - \Pr{Y \neq y \mid \vx, \vw}) \\
      &= \Pr{Y \neq y \mid \vx, \vw} \cdot \Pr{Y = y \mid \vx, \vw} \\
      &= (1 - \Pr{Y = y \mid \vx, \vw}) \cdot \Pr{Y = y \mid \vx, \vw} \\
      &= \sigma'(\transpose{\vw} \vx).
    \end{align*}

    \item By the second-order characterization of convexity (cf. \cref{rmk:second_order_convexity}), a twice differentiable function $f$ is convex if and only if its Hessian is positive semi-definite.
    Writing $c \defeq \sigma'(\transpose{\vw} \vx)$, we have for any $\vdelta \in \R^n$ that \begin{align*}
      \transpose{\vdelta} \hes_\vw \ell_\mathrm{log}(\transpose{\vw} \vx; y) \vdelta &= \transpose{\vdelta} \parentheses*{c \cdot \vx \transpose{\vx}} \vdelta = c (\transpose{\vdelta} \vx)^2 \geq 0, \margintag{using $c \geq 0$ and $(\cdot)^2 \geq 0$}
    \end{align*} and hence, the logistic loss is convex in $\vw$.
  \end{enumerate}
\end{solution}

\begin{solution}{gpc}
  \begin{enumerate}[beginpenalty=10000]
    \item Using the law of total probability \eqref{eq:lotp}, \begin{align*}
      &p(\ys = +1 \mid \vx_{1:n}, y_{1:n}, \vxs) \\
      &= \int p(\ys = +1 \mid \fs) p(\fs \mid \vx_{1:n}, y_{1:n}, \vxs) \,d\fs \\
      &= \int \sigma(\fs) p(\fs \mid \vx_{1:n}, y_{1:n}, \vxs) \,d\fs. \margintag{using $\ys \sim \Bern{\sigma(\fs)}$}
    \end{align*}
    Due to the non-Gaussian likelihood, the integral is analytically intractable.
    However, as the integral is one-dimensional, numerical approximations such as the Gauss-Legendre quadrature can be used.

    \item \begin{enumerate}
      \item According to Bayes' rule \eqref{eq:bayes_rule}, we know that \begin{align*}
        \psi(\vf) &= \log p(\vf \mid \vx_{1:n}, y_{1:n}) \margintag{using \cref{eq:log_posterior}} \\
        &= \log p(y_{1:n} \mid \vf) + \log p(\vf \mid \vx_{1:n}) - \log p(y_{1:n} \mid \vx_{1:n}) \margintag{using Bayes' rule \eqref{eq:bayes_rule}} \\
        &= \log p(y_{1:n} \mid \vf) + \log p(\vf \mid \vx_{1:n}) + \const.
      \intertext{Note that $p(\vf \mid \vx_{1:n}) = \N[\vf]{\vzero}{\mK_{AA}}$. Plugging in this closed-form Gaussian distribution of the GP prior gives}
        &= \log p(y_{1:n} \mid \vf) - \frac{1}{2} \transpose{\vf} \inv{\mK_{\sA\sA}} \vf + \const
      \end{align*}
      Differentiating with respect to $\vf$ yields \begin{align*}
        \grad \psi(\vf) &= \grad \log p(y_{1:n} \mid \vf) - \inv{\mK_{\sA\sA}} \vf \\
        \hes \psi(\vf) &= \hes_\vf \log p(y_{1:n} \mid \vf) - \inv{\mK_{\sA\sA}}.
      \end{align*}
      Hence, $\mLambda = \inv{\mK_{\sA\sA}} + \mW$ where $\mW \defeq \left. -\hes_\vf \log p(y_{1:n} \mid \vf) \right\rvert_{\vf = \vfhat}$.

      It remains to derive $\mW$ for the probit likelihood.
      Using independence of the training examples, \begin{align*}
        \log p(y_{1:n} \mid \vf) = \sum_{i=1}^n \log p(y_i \mid f_i),
      \end{align*} and hence, the Hessian of this expression is diagonal.
      Using the symmetry of $\Phi(z; 0, \sigman^2)$ around zero, we can write \begin{align*}
        \log p(y_i \mid f_i) = \log \Phi(y_i f_i; 0, \sigman^2).
      \end{align*}
      In the following, we write $\mathcal{N}(z) \defeq \N[z]{0}{\sigman^2}$ and $\Phi(z) \defeq \Phi(z; 0, \sigman^2)$ to simplify the notation.
      Differentiating with respect to $f_i$, we obtain \begin{align*}
        \pdv{}{f_i} \log \Phi(y_i f_i) &= \frac{y_i \mathcal{N}(f_i)}{\Phi(y_i f_i)} \margintag{using $\mathcal{N}(y_i f_i) = \mathcal{N}(f_i)$ since $\mathcal{N}$ is an even function and $y_i \in \{\pm 1\}$} \\
        \pdv[order=2]{}{f_i} \log \Phi(y_i f_i) &= - \frac{\mathcal{N}(f_i)^2}{\Phi(y_i f_i)^2} - \frac{y_i f_i \mathcal{N}(f_i)}{\sigman^2 \Phi(y_i f_i)},
      \end{align*} and $\mW = \left. -\diag{\pdv[order=2]{}{f_i} \log \Phi(y_i f_i)}_{i=1}^n \right\rvert_{\vf = \vfhat}$.

      \item Note that $\mLambda'$ is a precision matrix over weights $\vw$ and $\vf = \mX \vw$, so by \cref{eq:linear_map_variance} the corresponding variance over latent values $\vf$ is $\mX \inv{\mLambda'} \transpose{\mX}$.
      The two precision matrices are therefore equivalent if $\inv{\mLambda} = \mX \inv{\mLambda'}\transpose{\mX}$.

      Analogously to \cref{ex:bayesian_logistic_regression,exercise:logistic_loss_gradient}, we have that \begin{align*}
        \mW &= \left. -\hes_\vf \log p(y_{1:n} \mid \vf) \right\rvert_{\vf = \vfhat} \\
        &= \diag[i \in [n]]{\sigma(\hat{f}_i) (1-\sigma(\hat{f}_i))} \\
        &= \diag[i \in [n]]{\pi_i (1-\pi_i)}.
      \end{align*}
      By Woodbury's matrix identity \eqref{eq:general_woodbury}, \begin{align*}
        \inv{\mLambda'} &= \inv{(\mI + \transpose{\mX} \mW \mX)} \\
        &= \mI - \transpose{\mX} \inv{(\inv{\mW} + \mX \transpose{\mX})} \mX.
      \end{align*}
      Thus, \begin{align*}
        \mX \inv{\mLambda'} \transpose{\mX} &= \mX \transpose{\mX} - \mX \transpose{\mX} \inv{(\inv{\mW} + \mX \transpose{\mX})} \mX \transpose{\mX} \\
        &= \mK_{\sA\sA} - \mK_{\sA\sA} \inv{(\inv{\mW} + \mK_{\sA\sA})} \mK_{\sA\sA} \margintag{using that $\mK_{\sA\sA} = \mX \transpose{\mX}$} \\
        &= \inv{(\inv{\mK_{\sA\sA}} + \mW)} \margintag{by the matrix inversion lemma \eqref{eq:matrix_inversion_lemma}} \\
        &= \inv{\mLambda}.
      \end{align*}

      \item Using the formulas for the conditional distribution of a Gaussian \eqref{eq:cond_gaussian}, evaluating a conditional GP at a test point $\vxs$ yields, \begin{subequations}\begin{align}
        \fs \mid \vxs, \vf &\sim \N{\opt{\mu}}{\opt{k}}, \quad\text{where} \\
        \opt{\mu} &\defeq \transpose{\vk_{\vxs,\sA}} \inv{\mK_{\sA\sA}} \vf, \label{eq:gpc_helper_mean} \\
        \opt{k} &\defeq k(\vxs, \vxs) - \transpose{\vk_{\vxs,\sA}} \inv{\mK_{\sA\sA}} \vk_{\vxs,\sA}. \label{eq:gpc_helper_var}
      \end{align}\end{subequations}
      Then, using the tower rule \eqref{eq:tower_rule}, \begin{align}
        \E[q]{\fs}[\vxs, \vx_{1:n}, y_{1:n}] &= \E[\vf \sim q]{\E[\fs \sim p]{\fs}[\vxs, \vf]}[\vx_{1:n}, y_{1:n}] \nonumber \\
        &= \transpose{\vk_{\vxs,\sA}} \inv{\mK_{\sA\sA}} \E[q]{\vf}[\vx_{1:n}, y_{1:n}] \margintag{using \cref{eq:gpc_helper_mean} and linearity of expectation \eqref{eq:linearity_expectation}} \nonumber \\
        &= \transpose{\vk_{\vxs,\sA}} \inv{\mK_{\sA\sA}} \vfhat. \label{eq:gpc_helper_cond_mean}
      \intertext{Observe that any maximum $\vfhat$ of $\psi(\vf)$ needs to satisfy $\grad \psi(\vf) = \vzero$.
      Hence, $\vfhat = \mK_{\sA\sA} (\grad_\vf \log p(y_{1:n} \mid \vf))$, and the expectation simplifies to}
        &= \transpose{\vk_{\vxs,\sA}} (\grad_\vf \log p(y_{1:n} \mid \vf)). \nonumber
      \end{align}

      Using the law of total variance \eqref{eq:lotv}, \begin{align}
        &\Var[q]{\fs}[\vxs, \vx_{1:n}, y_{1:n}] \nonumber \\
        &= \begin{multlined}[t]
          \E[\vf \sim q]{\Var[\fs \sim p]{\fs \mid \vxs, \vx_{1:n}, \vf}}[\vx_{1:n}, y_{1:n}] \\ + \Var[\vf \sim q]{\E[\fs \sim p]{\fs \mid \vxs, \vx_{1:n}, \vf}}[\vx_{1:n}, y_{1:n}]
        \end{multlined} \nonumber \\
        &= \E[q]{\opt{k}}[\vx_{1:n}, y_{1:n}] + \Var[q]{\transpose{\vk_{\vxs,\sA}} \inv{\mK_{\sA\sA}} \vf}[\vx_{1:n}, y_{1:n}] \margintag{using \cref{eq:gpc_helper_var,eq:gpc_helper_mean}} \nonumber \\
        &= \opt{k} + \transpose{\vk_{\vxs,\sA}} \inv{\mK_{\sA\sA}} \Var[q]{\vf}[\vx_{1:n}, y_{1:n}] \inv{\mK_{\sA\sA}} \vk_{\vxs,\sA}. \margintag{using that $\opt{k}$ is independent of $\vf$, \cref{eq:linear_map_variance}, and symmetry of $\inv{\mK_{\sA\sA}}$} \nonumber
      \intertext{Recall from (a) that $\Var[q]{\vf}[\vx_{1:n}, y_{1:n}] = \inv{(\inv{\mK_{\sA\sA} + \mW})}$, so}
        &= \begin{multlined}[t]
          k(\vxs, \vxs) - \transpose{\vk_{\vxs,\sA}} \inv{\mK_{\sA\sA}} \vk_{\vxs,\sA} \\ + \transpose{\vk_{\vxs,\sA}} \inv{\mK_{\sA\sA}} \inv{(\inv{\mK_{\sA\sA} + \mW})} \inv{\mK_{\sA\sA}} \vk_{\vxs,\sA}
        \end{multlined} \margintag{plugging in the expression for $\opt{k}$ \eqref{eq:gpc_helper_var}} \nonumber \\
        &= k(\vxs, \vxs) - \transpose{\vk_{\vxs,\sA}} \inv{(\mK_{\sA\sA} + \inv{\mW})} \vk_{\vxs,\sA}. \margintag{using the matrix inversion lemma \eqref{eq:matrix_inversion_lemma}} \label{eq:gpc_helper_cond_variance}
      \end{align}
      Notice the similarity of \cref{eq:gpc_helper_cond_mean,eq:gpc_helper_cond_variance} to \cref{eq:gpc_helper_mean,eq:gpc_helper_var}.
      The latter is the conditional mean and variance if $\vf$ is known whereas the former is the conditional mean and variance given the noisy observation $y_{1:n}$ of $\vf$.
      The matrix $\mW$ quantifies the noise in the observations.

      \item Recall that \begin{align*}
        p(\ys = +1 \mid \vx_{1:n}, y_{1:n}, \vxs) &\approx \int \sigma(\fs) q(\fs \mid \vx_{1:n}, y_{1:n}, \vxs) \,d\fs \margintag{using the Laplace-approximated latent predictive posterior} \\[5pt]
        &= \E[q]{\sigma(\fs)}. \margintag{using LOTUS \eqref{eq:lotus}}
      \end{align*}
      This quantity can be interpreted as the \emph{averaged prediction} over all latent predictions $\fs$.
      In contrast, $\sigma(\E[q]{\fs})$ can be understood as the \emph{``MAP'' prediction}, which is obtained using the MAP estimate of $\fs$.\footnote{As $q$ is a Gaussian, its mode (i.e., the MAP estimate) and its mean coincide.}
      As $\sigma$ is nonlinear, the two quantities are not identical, and generally the averaged prediction is preferred.
    \end{enumerate}

    \item We have \begin{align*}
      \E*[\varepsilon]{\Ind{f(\vx) + \varepsilon \geq 0}} &= \Pr[\varepsilon]{f(\vx) + \varepsilon \geq 0} \margintag{using $\E{X} = p$ if $X \sim \Bern{p}$} \\
      &= \Pr[\varepsilon]{-\varepsilon \leq f(\vx)} \\
      &= \Pr[\varepsilon]{\varepsilon \leq f(\vx)} \margintag{using that the distribution of $\varepsilon$ is symmetric around $0$} \\
      &= \Phi(f(\vx); 0, \sigman^2).
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{jensen}
  \begin{enumerate}[beginpenalty=10000]
    \item Recall that as $f$ is convex, \begin{align*}
      \forall \vx_1, \vx_2, \forall \lambda \in [0,1] :\quad f(\lambda \vx_1 + (1-\lambda) \vx_2) \leq \lambda f(\vx_1) + (1-\lambda) f(\vx_2).
    \end{align*}
    We prove the statement by induction on $k$.
    The base case, $k=2$, follows trivially from the convexity of $f$.
    For the induction step, suppose that the statement holds for some fixed $k \geq 2$ and assume w.l.o.g. that $\theta_{k+1} \in (0,1)$. We then have, \begin{align*}
      \sum_{i=1}^{k+1} \theta_i f(\vx_i) &= (1-\theta_{k+1}) \parentheses*{\sum_{i=1}^k \frac{\theta_i}{1 - \theta_{k+1}} f(\vx_i)} + \theta_{k+1} f(\vx_{k+1}) \\
      &\geq (1-\theta_{k+1}) \cdot f\left(\sum_{i=1}^k \frac{\theta_i}{1 - \theta_{k+1}} \vx_i\right) + \theta_{k+1} f(\vx_{k+1}) \margintag{using the induction hypothesis} \\
      &\geq f\left(\sum_{i=1}^{k+1} \theta_i \vx_i\right). \margintag{using the convexity of $f$}
    \end{align*}

    \item Noting that $\log_2$ is concave, we have by Jensen's inequality, \begin{align*}
      \H{p} &= \E[x \sim p]{\log_2\parentheses*{\frac{1}{p(x)}}} \margintag{by definition of entropy \eqref{eq:entropy_discrete}} \\
      &\leq \log_2 \E[x \sim p]{\frac{1}{p(x)}} \margintag{by Jensen's inequality \eqref{eq:jensen}} \\
      &= \log_2 n.
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{bce_loss}
  If $y = 1$ then \begin{align*}
    \ell_{\mathrm{bce}}(\hat{y}; y) = - \log \hat{y} = \log(1 + e^{-\hat{f}}) = \ell_{\mathrm{log}}(\hat{F}; y).
  \end{align*}
  If $y = -1$ then \begin{align*}
    \ell_{\mathrm{bce}}(\hat{y}; y) = - \log(1 - \hat{y}) = \log(1 + e^{\hat{f}}) = \ell_{\mathrm{log}}(\hat{f}; y).
  \end{align*}
  Here the second equality follows from the simple algebraic fact \begin{align*}
    1 - \frac{1}{1 + e^{-z}} = 1 - \frac{e^z}{e^z + 1} = \frac{1}{1 + e^z}. \margintag{multiplying by $\frac{e^z}{e^z}$}
  \end{align*}
\end{solution}

\begin{solution}{gibbs_ineq}
  \begin{enumerate}[beginpenalty=10000]
    \item Let $p$ and $q$ be two (continuous) distributions.
    The KL-divergence between $p$ and $q$ is \begin{align*}
      \KL{p}{q} &= \E[\vx \sim p]{\log \frac{p(\vx)}{q(\vx)}} \margintag{using the definition of KL-divergence \eqref{eq:kl}} \\
      &= \E[\vx \sim p]{\S{\frac{q(\vx)}{p(\vx)}}}.
    \intertext{Note that the surprise $\S{u} = - \log u$ is a convex function in $u$, and hence,}
      &\geq \S{\E[\vx \sim p]{\frac{q(\vx)}{p(\vx)}}} \margintag{using Jensen's inequality \eqref{eq:jensen}} \\
      &= \S{\int q(\vx) \,d\vx} \\
      &= \S{1} = 0. \margintag{a probability density integrates to $1$}
    \end{align*}

    \item We observe from the derivation of (1) that $\KL{p}{q} = 0$ iff equality holds for Jensen's inequality.
    Now, if $p$ and $q$ are discrete with final and identical support, we can follow from the hint that Jensen's inequality degenerates to an equality iff $p$ and $q$ are point wise identical.
  \end{enumerate}
\end{solution}

\begin{solution}{maximum_entropy_principle}
  \begin{enumerate}[beginpenalty=10000]
    \item We have \begin{align*}
      \crH{f}{g} &= - \int_\R f(x) \log g(x) \,d x \margintag{using the definition of cross-entropy \eqref{eq:cross_entropy}} \\
      &= - \int_\R f(x) \cdot \parentheses*{\log\parentheses*{\frac{1}{\sqrt{2 \pi \sigma^2}}} - \frac{(x - \mu)^2}{2 \sigma^2}} \,d x \margintag{using that $g(x) = \N[x]{\mu}{\sigma^2}$} \\
      &= - \log\parentheses*{\frac{1}{\sqrt{2 \pi \sigma^2}}} \underbrace{\int_\R f(x) \,d x}_{1} + \frac{1}{2 \sigma^2} \int_\R f(x) (x - \mu)^2 \,d x \\
      &= \log(\sigma\sqrt{2\pi}) + \frac{1}{2 \sigma^2} \underbrace{\E[x \sim f]{(x - \mu)^2}}_{\sigma^2} \\
      &= \log(\sigma\sqrt{2\pi}) + \frac{1}{2} \\
      &= \H{g}. \margintag{using the entropy of Gaussians \eqref{eq:entropy_gaussian_univ}}
    \end{align*}

    \item We have shown that \begin{align*}
      \H{g} - \H{f} = \KL{f}{g} \geq 0,
    \end{align*} and hence, $\H{g} \geq \H{f}$.
    That is, for a fixed mean $\mu$ and variance $\sigma^2$, the distribution that has maximum entropy among all distributions on $\R$ is the normal distribution.
  \end{enumerate}
\end{solution}

\begin{solution}{mep_and_posteriors}
  \begin{enumerate}[beginpenalty=10000]
    \item This sample solution follows the works of \cite{caticha2006updating,caticha2021entropy}.
    Writing down the Lagrangian with dual variables $\lambda_0$ and $\lambda_1(\vy)$ for the normalization and data constraints yields \begin{align*}
      L(q, \lambda_0, \lambda_1) &= \begin{multlined}[t]
        \int q(\vx,\vy) \log\frac{q(\vx,\vy)}{p(\vx,\vy)} \,d\vx \,d\vy \margintag{the objective, using \eqref{eq:kl}} \\ + \lambda_0 \parentheses*{1 - \int q(\vx,\vy) \,d\vx \,d\vy} \margintag{the normalization constraint} \\ + \int \lambda_1(\vy) \brackets*{\delta_{\vyp}(\vy) - \int q(\vx,\vy) \,d\vx} \,d\vy \margintag{the data constraint}
      \end{multlined} \\
      &= \int q(\vx,\vy) \brackets*{\log\frac{q(\vx,\vy)}{p(\vx,\vy)} - \lambda_0 - \lambda_1(\vy)} \,d\vx \,d\vy + \const.
    \end{align*}
    Note that $L(q, \lambda_0, \lambda_1) = \KL{q_{\rX,\rY}}{p_{\rX,\rY}}$ if the constraints are satisfied.
    Thus, we simply need to solve the (dual) optimization problem \begin{align*}
      \min_{q(\cdot,\cdot) \geq 0} \max_{\lambda_0, \lambda_1(\cdot) \in \R} L(q, \lambda_0, \lambda_1).
    \end{align*}
    We have \begin{align*}
      \pdv{L(q, \lambda_0, \lambda_1)}{{q(\vx,\vy)}} = \log\frac{q(\vx,\vy)}{p(\vx,\vy)} - \lambda_0 - \lambda_1(\vy) + 1. \margintag{using the product rule of differentiation}
    \end{align*}
    Setting the partial derivatives to zero, we obtain \begin{align*}
      q(\vx,\vy) = \exp(\lambda_0 + \lambda_1(\vy) - 1) p(\vx,\vy) = \frac{1}{Z} \exp(\lambda_1(\vy)) p(\vx,\vy)
    \end{align*} where $Z \defeq \exp(\lambda_0 - 1)$ denotes the normalizing constant.
    We can determine $\lambda_1(\vy)$ from the data constraint: \begin{align*}
      \int q(\vx,\vy) \,d\vx &= \frac{1}{Z} \exp(\lambda_1(\vy)) \int p(\vx,\vy) \,d\vx \\
      &= \frac{1}{Z} \exp(\lambda_1(\vy)) \cdot p(\vy) \margintag{using the sum rule \eqref{eq:sum_rule}} \\
      &\overset{!}{=} \delta_{\vyp}(\vy).
    \end{align*}
    It follows that $q(\vx,\vy) = \delta_{\vyp}(\vy) \cdot \frac{p(\vx,\vy)}{p(\vy)} = \delta_{\vyp}(\vy) \cdot p(\vx \mid \vy)$.\margintag{using the definition of conditional distributions \eqref{eq:cond_distr}}

    \item From the sum rule \eqref{eq:sum_rule}, we obtain \begin{align*}
      q(\vx) = \int q(\vx,\vy) \,d\vy = \int \delta_{\vyp}(\vy) \cdot p(\vx \mid \vy) \,d\vy = p(\vx \mid \vyp).
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{kl_div_of_gaussians}
  We can rewrite the KL-divergence as \begin{align*}
    \KL{p}{q} &= \E[\vx \sim p]{\log p(\vx) - \log q(\vx)} \margintag{using the definition of KL-divergence \eqref{eq:kl}} \\
    &= \begin{multlined}[t]
      \E*[\vx \sim p]{\left[\frac{1}{2} \log \frac{\det{\mSigma_q}}{\det{\mSigma_p}} - \frac{1}{2} \transpose{(\vx - \vmu_p)} \inv{\mSigma_p} (\vx - \vmu_p) \right. \\ + \left. \frac{1}{2} \transpose{(\vx - \vmu_q)} \inv{\mSigma_q} (\vx - \vmu_q)\right]}
    \end{multlined} \margintag{using the Gaussian PDF \eqref{eq:normal}} \\
    &= \begin{multlined}[t]
      \frac{1}{2} \log \frac{\det{\mSigma_q}}{\det{\mSigma_p}} - \frac{1}{2} \E[\vx \sim p]{\transpose{(\vx - \vmu_p)} \inv{\mSigma_p} (\vx - \vmu_p)} \\ + \frac{1}{2} \E[\vx \sim p]{\transpose{(\vx - \vmu_q)} \inv{\mSigma_q} (\vx - \vmu_q)}
    \end{multlined} \margintag{using linearity of expectation \eqref{eq:linearity_expectation}}
  \end{align*}
  As $\transpose{(\vx - \vmu_p)} \inv{\mSigma_p} (\vx - \vmu_p) \in \R$, we can rewrite the second term as \begin{align*}
    &\frac{1}{2} \E[\vx \sim p]{\tr{\transpose{(\vx - \vmu_p)} \inv{\mSigma_p} (\vx - \vmu_p)}} \\
    &= \frac{1}{2} \E[\vx \sim p]{\tr{(\vx - \vmu_p) \transpose{(\vx - \vmu_p)} \inv{\mSigma_p}}} \margintag{using the cyclic property of the trace} \\
    &= \frac{1}{2} \tr{\E[\vx \sim p]{(\vx - \vmu_p) \transpose{(\vx - \vmu_p)}} \inv{\mSigma_p}} \margintag{using linearity of the trace and linearity of expectation \eqref{eq:linearity_expectation}} \\
    &= \frac{1}{2} \tr{\mSigma_p \inv{\mSigma_p}} \margintag{using the definition of the covariance matrix \eqref{eq:variance}} \\
    &= \frac{1}{2} \tr{\mI} = \frac{d}{2}.
  \end{align*}
  For the third term, we use the hint \eqref{eq:kl_div_of_gaussians_hint} to obtain \begin{align*}
    &\frac{1}{2} \E[\vx \sim p]{\transpose{(\vx - \vmu_q)} \inv{\mSigma_q} (\vx - \vmu_q)} \\
    &= \frac{1}{2} \parentheses*{\transpose{(\vmu_p - \vmu_q)} \inv{\mSigma_q} (\vmu_p - \vmu_q) + \tr{\inv{\mSigma_q} \mSigma_p}}.
  \end{align*}
  Putting all terms together we get \begin{align*}
    \KL{p}{q} = \begin{multlined}[t]
      \frac{1}{2} \left(\log \frac{\det{\mSigma_q}}{\det{\mSigma_p}} - d + \transpose{(\vmu_p - \vmu_q)} \inv{\mSigma_q} (\vmu_p - \vmu_q) \right. \\ \left. + \tr{\inv{\mSigma_q} \mSigma_p}\right).
    \end{multlined}
  \end{align*}
\end{solution}

\begin{solution}{forward_vs_reverse_kl}
  \begin{enumerate}[beginpenalty=10000]
    \item Let $p$ and $q$ be discrete distributions.
    The derivation is analogous if $p$ and $q$ are taken to be continuous.
    First, we write the KL-divergence between $p$ and $q$ as \begin{align*}
      &\KL{p}{q} \\
      &= \sum_x \sum_y p(x, y) \log_2 \frac{p(x, y)}{q(x) q(y)} \margintag{using the definition of KL-divergence \eqref{eq:kl}} \\
      &= \begin{multlined}[t]
        \sum_x \sum_y p(x, y) \log_2 p(x, y) - \sum_x \sum_y p(x, y) \log_2 q(x) \\ - \sum_x \sum_y p(x, y) \log_2 q(y)
      \end{multlined} \\
      &= \sum_x \sum_y p(x, y) \log_2 p(x, y) - \sum_x p(x) \log_2 q(x) - \sum_y p(y) \log_2 q(y) \margintag{using the sum rule \eqref{eq:sum_rule}} \\
      &= - \H{p(x, y)} + \crH{p(x)}{q(x)} + \crH{p(y)}{q(y)} \margintag{using the definitions of entropy \eqref{eq:entropy} and cross-entropy \eqref{eq:cross_entropy}} \\[5pt]
      &= \begin{multlined}[t]
        - \H{p(x, y)} + \H{p(x)} + \H{p(y)} + \KL{p(x)}{q(x)} \\ + \KL{p(y)}{q(y)}
      \end{multlined} \margintag{using \cref{eq:cross_entropy_decomp}} \\
      &= \KL{p(x)}{q(x)} + \KL{p(y)}{q(y)} + \const.
    \end{align*}
    Hence, to minimize $\KL{p}{q}$ with respect to the variational distributions $q(x)$ and $q(y)$ we should set $\KL{p(x)}{q(x)} = 0$ and $\KL{p(y)}{q(y)} = 0$, respectively.
    This is obtained when \begin{align*}
      q(x) = p(x) \qquad\text{and}\qquad q(y) = p(y).
    \end{align*}

    \item The reverse KL-divergence $\KL{q}{p}$ on the finite domain $x, y \in \{1, 2, 3, 4\}$ is defined as \begin{align*}
      \KL{q}{p} = \sum_x \sum_y q(x) q(y) \log_2 \frac{q(x) q(y)}{p(x, y)}.
    \end{align*}
    We can easily observe from the above formula that the support of $q$ must be a subset of the support of $p$.
    In other words, if $q(x, y)$ is positive outside the support of $p$ (i.e., when $p(x, y) = 0$) then $\KL{q}{p} = \infty$.
    Hence, the reverse KL-divergence has an infinite value except when the support of $q$ is either $\{1,2\} \times \{1,2\}$ or $\{(3,3)\}$ or $\{(4,4)\}$.
    Thus, it has three local minima.

    For the first case, the minimum is achieved when $q(x) = q(y) = (\frac{1}{2}, \frac{1}{2}, 0, 0)$.
    The corresponding KL-divergence is $\KL{q}{p} = \log_2 2 = 1$.
    For the second case and the third case, $q(x) = q(y) = (0, 0, 1, 0)$ and $q(x) = q(y) = (0, 0, 0, 1)$, respectively.
    The KL-divergence in both cases is $\KL{q}{p} = \log_2 4 = 2$.

    \item Let us compute $p(x = 4)$ and $p(y = 1)$: \begin{align*}
      p(x = 4) &= \sum_y p(x = 4, y) = \frac{1}{4}, \\
      p(y = 1) &= \sum_x p(x, y = 1) = \frac{1}{4}.
    \end{align*}
    Hence, $q(x = 4, y = 1) = p(x = 4) p(y = 1) = \frac{1}{16}$, however, $p(x = 4, y = 1) = 0$.
    We therefore have for the reverse KL-divergence that $\KL{q}{p} = \infty$.
  \end{enumerate}
\end{solution}

\begin{solution}{gaussian_vi_vs_laplace}
  \begin{enumerate}[beginpenalty=10000]
    \item Recall from \cref{sec:vi:kl:forward} that $\opt{q}$ matches the first and second \emph{moments} of $p$.
    In contrast, the Laplace approximation matches the \emph{mode} of $p$ and the second derivative of $- \log p$.
    In general, the mean is different from the mode, and so is the second moment from the second derivative.

    \item We have \begin{align*}
      &\argmin_{q \in \spQ} \KL{q}{p(\cdot \mid \spD)} \\
      &= \argmax_{q \in \spQ} L(q, p; \spD) \margintag{using \cref{eq:elbo_reverse_kl_relationship}} \\
      &= \argmax_{q \in \spQ} \E[\vtheta \sim q]{\log p(y_{1:n}, \vtheta \mid \vx_{1:n})} + \H{q} \margintag{using \cref{eq:elbo}} \\
      &= \argmax_{q \in \spQ} \E[\vtheta \sim q]{\log p(y_{1:n}, \vtheta \mid \vx_{1:n})} + \frac{n}{2} \log(2 \pi e) + \frac{1}{2} \log\det{\mSigma}. \margintag{using \cref{eq:entropy_gaussian}}
    \end{align*}
    Differentiating with respect to $\vmu$ and $\mSigma$, we have that $\tilde{q}$ must satisfy \begin{align*}
      \vzero &= \grad_{\vmu} \E[\vtheta \sim \tilde{q}]{\log p(y_{1:n}, \vtheta \mid \vx_{1:n})} \\
      \inv{\mSigma} &= -2 \grad_{\mSigma} \E[\vtheta \sim \tilde{q}]{\log p(y_{1:n}, \vtheta \mid \vx_{1:n})}. \margintag{using the first hint}
    \end{align*}
    The result follows by \cref{eq:gradient_of_gaussian_parameters}.
  \end{enumerate}
\end{solution}

\begin{solution}{gradient_of_reverse_kl}
  To simplify the notation, we write $\mSigma \defeq \diag{\sigma_1^2, \dots, \sigma_d^2}$.
  The reverse KL-divergence can be expressed as \begin{align*}
    \KL{q_\vlambda}{p(\cdot)} &= \frac{1}{2} \parentheses*{\tr{\sigmap^{-2} \mSigma} + \sigmap^{-2} \transpose{\vmu} \vmu - d + \log \frac{(\sigmap^2)^d}{\det{\mSigma}}} \margintag{using the expression for the KL-divergence of Gaussians \eqref{eq:kl_gaussian}} \\
    &= \frac{1}{2} \parentheses*{\sigmap^{-2} \sum_{i=1}^d \sigma_i^2 + \sigmap^{-2} \transpose{\vmu} \vmu - d + d \log \sigmap^2 - \sum_{i=1}^d \log \sigma_i^2}.
  \end{align*}
  It follows immediately that $\grad_\vmu \KL{q_\vlambda}{p(\cdot)} = \sigmap^{-2} \vmu$.
  Moreover, \begin{align*}
    \pdv{}{\sigma_i} \KL{q_\vlambda}{p(\cdot)} = \frac{1}{2} \bigg(\sigmap^{-2} \underbrace{\pdv{}{\sigma_i} \sigma_i^2}_{2 \sigma_i} - \underbrace{\pdv{}{\sigma_i} \log \sigma_i^2}_{\nicefrac{2}{\sigma_i}}\bigg) = \frac{\sigma_i}{\sigmap^2} - \frac{1}{\sigma_i}.
  \end{align*}
\end{solution}

\begin{solution}{reparameterizable_distributions}
  \begin{enumerate}[beginpenalty=10000]
    \item Let $Y \sim \Unif{[0,1]}$.
    Then, using the two hints, \begin{align*}
      (b - a) Y + a \sim \Unif{[a,b]}.
    \end{align*}

    \item Let $Z_1 \sim \N{\mu}{\sigma^2}$ and $Z_2 \sim \N{0}{1}$.
    We have, $X = e^{Z_1}$.
    Recall from \cref{eq:gaussian_affine_transformation} that $Z_1$ can equivalently be expressed in terms of $Z_2$ as $Z_1 = \sigma Z_2 + \mu$.
    This yields, \begin{align*}
      X = e^{Z_1} = e^{\sigma Z_2 + \mu}.
    \end{align*}

    \item Denote by $F$ the CDF of $\Cauchy{0}{1}$.
    Observe that $F$ is invertible with inverse $\inv{F}(y) = \tan(\pi (y - \frac{1}{2}))$.
    Let $Y \sim \Unif{[0,1]}$ and write $X = \inv{F}(Y)$.
    Then, \begin{align*}
        P_X(x) &= \Pr{X \leq x} \\
        &= \Pr{\inv{F}(Y) \leq x} \\
        &= \Pr{Y \leq F(x)} \\
        &= F(x).
    \end{align*}
    This reparameterization works for any distribution with invertible CDF (not just Cauchy) and is known as the \midx{universality of the uniform} (cf. \cref{sec:inverse_transform_sampling}).
    The universality of the uniform is commonly used in pseudo-random number generators as it allows ``lifting'' samples from a uniform distribution to countless other well-known distributions.

    \item The derivative of $\mathrm{ReLU}(z)$ is $\Ind{z > 0}$.
    Applying the reparameterization trick gives \begin{align*}
      \odv{}{\mu} \E*[x \sim \N{\mu}{1}]{\mathrm{ReLU}(w x)} &= \E*[\varepsilon \sim \N{0}{1}]{\odv{}{\mu} \mathrm{ReLU}(w (\mu + \varepsilon))} \\
      &= w \E*[\varepsilon]{\Ind{w(\mu + \varepsilon) > 0}} \margintag{using the chain rule} \\
      &= w \E*[\varepsilon]{\Ind{\mu + \varepsilon > 0}} \\
      &= w \Pr[\varepsilon]{\mu + \varepsilon > 0} \\
      &= w \Pr[\varepsilon]{\varepsilon < \mu} \\
      &= w \Pr[\varepsilon]{\varepsilon \leq \mu} \\
      &= w \Phi(\mu).
    \end{align*}
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:approximate_inference:mcmc}}

\begin{solution}{mc_update}
  We have \begin{align*}
    q_{t+1}(x') = \Pr{X_{t+1} = x'} = \sum_x \underbrace{\Pr{X_t = x}}_{q_t(x)} \underbrace{\Pr{X_{t+1} = x' \mid X_t = x}}_{p(x' \mid x)}. \margintag{using the sum rule \eqref{eq:sum_rule}}
  \end{align*}
  Noting that $p(x' \mid x) = \mP(x, x')$, we conclude $\vq_{t+1} = \vq_t \mP$.
\end{solution}

\begin{solution}{mc_multi_step_transitions}
  It follows directly from the definition of matrix multiplication that \begin{align*}
    \mP^k(x, x') &= \sum_{x_1, \dots, x_{k-1}} \mP(x, x_1) \cdot \mP(x_1, x_2) \cdots \mP(x_{k-1}, x') \\
    &= \sum_{x_1, \dots, x_{k-1}} \Pr{X_1 = x_1 \mid X_0 = x} \cdots \Pr{X_k = x' \mid X_{k-1} = x_{k-1}} \margintag{using the definition of the transition matrix \eqref{eq:transition_matrix}} \\
    &= \sum_{x_1, \dots, x_{k-1}} \Pr{X_1 = x_1, \cdots, X_{k-1} = x_{k-1}, X_k = x' \mid X_0 = x} \margintag{using the product rule \eqref{eq:product_rule}} \\
    &= \Pr{X_k = x' \mid X_0 = x}. \margintag{using the sum rule \eqref{eq:sum_rule}}
  \end{align*}
\end{solution}

\begin{solution}{finding_stationary_distributions}
  We consider the transition matrix \begin{align*}
    \mP = \begin{bmatrix}
      0.60 & 0.30 & 0.10 \\
      0.50 & 0.25 & 0.25 \\
      0.20 & 0.40 & 0.40 \\
    \end{bmatrix}.
  \end{align*}
  We note that the entries of $\mP$ are all different from $0$, thus the Markov chain corresponding to this transition matrix is ergodic.\footnote{All elements of the transition matrix being strictly greater than $0$ is a sufficient, but not necessary, condition for ergodicity.}
  Thus, there exists a unique stationary distribution $\pi$ to which the Markov chain converges irrespectively of the distribution over initial states $q_0$.

  We know that $\transpose{\mP}\vpi = \vpi$ (where we write $\vpi$ as a column vector), therefore, to find the stationary distribution $\pi$, we need to find the normalized eigenvector associated with eigenvalue $1$ of the matrix $\transpose{\mP}$.
  That is, we want to solve $(\transpose{\mP} - \mI) \vpi = \vzero$ for $\vpi$.
  We obtain the linear system of equations, \begin{align*}
    -0.40 \pi_1 + 0.50 \pi_2 + 0.20 \pi_3 &= 0 \\
    0.30 \pi_1 - 0.75 \pi_2 + 0.40 \pi_3 &= 0 \\
    0.10 \pi_1 + 0.25 \pi_2 - 0.60 \pi_3 &= 0.
  \end{align*}
  Note that the left hand side of equation $i$ corresponds to the probability of entering state $i$ at stationarity minus $\pi_i$.
  Quite intuitively, this difference should be $0$, that is, after one iteration the random walk is at state $i$ with the same probability as before the iteration.

  Solving this system of equations, for example, using the Gaussian elimination algorithm, we obtain the normalized eigenvector \begin{align*}
    \vpi = \frac{1}{72} \begin{bmatrix}
      35 \\ 22 \\ 15
    \end{bmatrix}.
  \end{align*}
  Thus, we conclude that in the long run, the percentage of news days that will be classified as ``good'' is $\nicefrac{35}{72}$.
\end{solution}

\begin{solution}{metropolis_hastings}
  Observe that the described proposal distribution is symmetric.
  Therefore, the acceptance probability simplifies to \begin{align*}
    \alpha(x' \mid x) = \min\braces*{1, \frac{p(x')}{p(x)}}.
  \end{align*}
  If we denote the number of $1$s in a bit string by $w(x)$, we have the requirement that $p(x) \propto w(x)$.
  Therefore, the acceptance probability becomes \begin{align*}
    \alpha(x' \mid x) = \begin{cases}
      \min\braces*{1, \frac{w(x')}{w(x)}} & \text{if $w(x) \neq 0$} \\
      1 & \text{otherwise}.
    \end{cases}
  \end{align*}
\end{solution}

\begin{solution}{gibbs_sampling}
  \begin{enumerate}[beginpenalty=10000]
    \item We have to compute the conditional distributions.
    Notice that for $x \in \{0, \dots, n\}$ and $y \in [0,1]$, \begin{align*}
      p(x, y) = {n \choose x} y^x (1-y)^{n-x} \cdot y^{\alpha-1}(1-y)^{\beta - 1} = \Bin[x]{n}{y} \cdot C_y,
    \end{align*}
    where $\Bin{n}{y}$ is the PMF of the binomial distribution \eqref{eq:binomial_distr} with $n$ trials and success probability $y$, and $C_y$ is a constant depending on $y$.
    It is clear that \begin{align*}
      p(x \mid y) &= \frac{p(x,y)}{p(y)} \margintag{using the definition of conditional probability \eqref{eq:cond_prob}} \\
      &= \frac{\Bin[x]{n}{y} \cdot C_y}{p(y)} \\
      &= \Bin[x]{n}{y}. \margintag{using that $p(x \mid y)$ is a probability distribution over $x$ and $\Bin[x]{n}{y}$ already sums to $1$, so $C_y = p(y)$}
    \end{align*}
    So in short, sampling from $p(x \mid y)$ is equivalent to sampling from a binomial distribution, which amounts to $n$ times throwing a coin with bias $y$, and outputting the number of heads.

    For the other conditional distribution, recall the PMF of the beta distribution with parameters $\alpha, \beta$, \begin{align*}
      \Beta[y]{\alpha}{\beta} = C \cdot y^{\alpha-1} (1-y)^{\beta-1}
    \end{align*} where $C$ is some constant depending on $\alpha$ and $\beta$ only.
    We then have \begin{align*}
      p(x, y) = \Beta[y]{x + \alpha}{n - x + \beta} \cdot C_x,
    \end{align*} where $C_x$ is some constant depending on $x,\alpha,\beta$.
    This shows (analogously to above), that \begin{align*}
      p(y \mid x) = \Beta[y]{x + \alpha}{n - x + \beta}.
    \end{align*}
    So for sampling $y$ given $x$, one can sample from the beta distribution.
    There are several methods for sampling from a beta distribution, and we refer the reader to the corresponding \href{https://en.wikipedia.org/wiki/Beta_distribution}{Wikipedia page}.

    \item We first derive the posterior distribution of $\mu$.
    We have \begin{align*}
      \log p(\mu \mid \lambda, x_{1:n}) &= \log p(\mu) + \log p(x_{1:n} \mid \mu, \lambda) + \const \\
      &= - \frac{\lambda_0}{2} (\mu - \mu_0)^2 - \frac{\lambda}{2} \sum_{i=1}^n (x_i - \mu)^2 + \const \\
      &= - \frac{1}{2} (\underbrace{\lambda_0 + n \lambda}_{l_\lambda}) \mu^2 + \Big(\underbrace{\lambda_0 \mu_0 + \lambda \sum_{i=1}^n x_i}_{m_\lambda l_\lambda}\Big) \mu + \const. \margintag{by expanding the squares}
    \end{align*}
    It follows that $l_\lambda = \lambda_0 + n \lambda, m_\lambda = (\lambda_0 \mu_0 + \lambda \sum_{i=1}^n x_i) / l_\lambda$, and $\mu \mid \lambda, x_{1:n} \sim \N{\mu_\lambda}{\inv{l_\lambda}}$.
    That is, the posterior precision is the sum of prior precision and the precisions of each observation, and the posterior mean is a weighted average of prior mean and observations (where the weights are the precisions).

    For the posterior distribution of $\lambda$, we have \begin{align*}
      p(\lambda \mid \mu, x_{1:n}) &\propto p(\lambda) \cdot p(x_{1:n} \mid \mu, \lambda) \\
      &= \lambda^{\alpha-1} e^{- \beta \lambda} \cdot \lambda^{\frac{n}{2}} e^{- \frac{\lambda}{2} \sum_{i=1}^n (x_i - \mu)^2} \\
      &= \lambda^{\alpha + \frac{n}{2} - 1} e^{-\lambda \parentheses*{\beta + \frac{1}{2} \sum_{i=1}^n (x_i - \mu)^2}} \\
      &= \GammaDistr[\lambda]{a_\mu}{b_\mu}
    \end{align*} where $a_\mu = \alpha + \frac{n}{2}$ and $b_\mu = \beta + \frac{1}{2} \sum_{i=1}^n (x_i - \mu)^2$.

    \item We have \begin{align*}
      p(\alpha, c \mid x_{1:n}) &\propto p(\alpha, c) \cdot p(x_{1:n} \mid \alpha, c) \\
      &\propto \Ind{\alpha, c > 0} \prod_{i=1}^n \frac{\alpha c^\alpha}{x_i^{\alpha+1}} \Ind{x_i \geq c} \\
      &= \frac{\alpha^n c^{n \alpha}}{(\prod_{i=1}^n x_i)^{\alpha+1}} \Ind{c < \opt{x}} \Ind{\alpha, c > 0}
    \end{align*} where $\opt{x} \defeq \min\{x_1, \dots, x_n\}$.
    It is not clear how one could sample from this distribution directly.
    Instead, we use Gibbs sampling.

    For the posterior distribution of $\alpha$, we have \begin{align*}
      p(\alpha \mid c, x_{1:n}) &\propto p(\alpha, c \mid x_{1:n}) \\
      &\propto \frac{\alpha^n c^{n \alpha}}{(\prod_{i=1}^n x_i)^{\alpha+1}} \Ind{\alpha > 0} \\
      &= \alpha^n \exp\parentheses*{- \alpha \parentheses*{\sum_{i=1}^n \log x_i - n \log c}} \Ind{\alpha > 0} \\
      &\propto \GammaDistr[\alpha]{a}{b}
    \end{align*} where $a \defeq n+1$ and $b \defeq \sum_{i=1}^n \log x_i - n \log c$.
    On the other hand, \begin{align*}
      p(c \mid \alpha, x_{1:n}) &\propto p(\alpha, c \mid x_{1:n}) \propto c^{n \alpha} \Ind{0 < c < \opt{x}}.
    \end{align*}
    It remains to show that it is easy to sample from a random variable $X$ with $p_X(x) \propto x^a \Ind{0 < x < b}$ for $a,b > 0$.
    We have that the normalizing constant is given by $\int_0^b x^a \,d x = \frac{b^{a+1}}{a+1}$.
    Therefore, the CDF of $X$ is \begin{align*}
      P_X(x) &= \int_0^x p(y) \,d y \\
      &= \frac{a+1}{b^{a+1}} \int_0^x y^a \,d y \\
      &= \frac{a+1}{b^{a+1}} \frac{x^{a+1}}{a+1} \\
      &= \parentheses*{\frac{x}{b}}^{a+1}.
    \end{align*}
    The inverse CDF is given by $\inv{P_X}(y) = b y^{\frac{1}{a+1}}$.
    Therefore, we can sample from $X$ using inverse transform sampling (cf. \cref{sec:inverse_transform_sampling}) by sampling $Y \sim \Unif{[0,1]}$ and setting $X = b Y^{\frac{1}{a+1}}$.
  \end{enumerate}
\end{solution}

\begin{solution}{bayesian_logistic_regression_energy_function}
  First, note that the sum of convex functions is convex, hence, we consider each term individually.

  The Hessian of the regularization term is $\lambda \mI$, and thus, by the second-order characterization of convexity, this term is convex in $\vw$.

  Finally, note that the second term is a sum of logistic losses $\ell_\mathrm{log}$ \eqref{eq:logistic_loss}, and we have seen in \cref{exercise:logistic_loss_gradient} that $\ell_\mathrm{log}$ is convex in $\vw$.
\end{solution}

\begin{solution}{maximum_entropy_property_of_gibbs_distribution}
  \begin{enumerate}[beginpenalty=10000]
    \item Our goal is to solve the optimization problem \begin{align}\begin{split}
      &\max_{p \in \Delta^{\spT}} - \sum_{x \in \spT} p(x) \log_2 p(x) \\
      &\text{subject to} \sum_{x \in \spT} p(x) f(x) = \mu \label{eq:maximum_entropy_property_of_gibbs_distribution}
    \end{split}\end{align}
    for some $\mu \in \R$.
    The Lagrangian with dual variables $\lambda_0$ and $\lambda_1$ is given by \begin{align*}
      L(p, \lambda_0, \lambda_1) &= \begin{multlined}[t]
        - \sum_{x \in \spT} p(x) \log_2 p(x) + \lambda_0 \parentheses*{1 - \sum_{x \in \spT} p(x)} + \\ \lambda_1 \parentheses*{\mu - \sum_{x \in \spT} p(x) f(x)}
      \end{multlined} \\
      &= - \sum_{x \in \spT} p(x) \parentheses*{\log_2 p(x) + \lambda_0 + \lambda_1 f(x)} + \const.
    \end{align*}
    Note that $L(p, \lambda_0, \lambda_1) = \H{p}$ if the constraints are satisfied.
    Thus, we simply need to solve the (dual) optimization problem \begin{align*}
      \max_{\vp \geq \vzero} \min_{\lambda_0, \lambda_1 \in \R} L(\vp, \lambda_0, \lambda_1).
    \end{align*}
    We have \begin{align*}
      \pdv{}{p(x)} L(p, \lambda_0, \lambda_1) = - \log_2 p(x) - \lambda_0 - \lambda_1 f(x) - 1.
    \end{align*}
    Setting the partial derivatives to zero, we obtain \begin{align*}
      p(x) &= 2 \exp(- \lambda_0 - \lambda_1 f(x) - 1) \margintag{using $\log_2(\cdot) = \frac{\log(\cdot)}{\log(2)}$} \\
      &\propto \exp(- \lambda_1 f(x)).
    \end{align*}
    Clearly, $p$ is a valid probability mass function when normalized (i.e., for an appropriate choice of $\lambda_0$).
    We complete the proof by setting $T \defeq \frac{1}{\lambda_1}$.

    \item As $T \to \infty$ (and $\lambda_1 \to 0$), the optimization problem reduces to picking the maximum entropy distribution without the first-moment constraint.
    This distribution is the uniform distribution over $\spT$.
    Conversely, as $T \to 0$ (and $\lambda_1 \to \infty$), the Gibbs distribution reduces to a point density around its mode.
  \end{enumerate}
\end{solution}

\begin{solution}{gibbs_sampling_energy_reduction}
  Recall the following two facts: \begin{enumerate}
    \item Gibbs sampling is an instance of Metropolis-Hastings with proposal distribution \begin{align*}
      r(\vxp \mid \vx) = \begin{cases}
        p(x'_i \mid \vxp_{-i}) & \text{if $\vxp$ differs from $\vx$ only in entry $i$} \\
        0 & \text{otherwise} \\
      \end{cases}
    \end{align*} and acceptance distribution $\alpha(\vxp \mid \vx) = 1$.

    \item The acceptance distribution of Metropolis-Hastings where the stationary distribution $p$ is a Gibbs distribution with energy function $f$ is \begin{align*}
      \alpha(\vxp \mid \vx) = \min\left\{1, \frac{r(\vx \mid \vxp)}{r(\vxp \mid \vx)} \exp(f(\vx) - f(\vxp))\right\}.
    \end{align*}
  \end{enumerate}

  We therefore know that \begin{align*}
    \frac{r(\vx \mid \vxp)}{r(\vxp \mid \vx)} \exp(f(\vx) - f(\vxp)) \geq 1.
  \end{align*}
  We remark that this inequality even holds with equality using our derivation of \cref{thm:gibbs_sampling_as_mh}.
  Taking the logarithm and reorganizing the terms, we obtain \begin{align}
    f(\vxp) \leq f(\vx) + \log r(\vx \mid \vxp) - \log r(\vxp \mid \vx). \label{eq:gibbs_sampling_energy_reduction_helper}
  \end{align}

  By the definition of the proposal distribution of Gibbs sampling, \begin{align*}
    r(\vxp \mid \vx) = p(x'_i \mid \vx_{-i}) \quad\text{and}\quad r(\vx \mid \vxp) = p(x_i \mid \vx_{-i}). \margintag{using $\vxp_{-i} = \vx_{-i}$}
  \end{align*}

  Taking the expectation of \cref{eq:gibbs_sampling_energy_reduction_helper}, \begin{align*}
    \E[x'_i \sim p(\cdot \mid \vx_{-i})]{f(\vxp)} &\leq f(\vx) + \log p(x_i \mid \vx_{-i}) - \E[x'_i \sim p(\cdot \mid \vx_{-i})]{\log p(x'_i \mid \vx_{-i})} \\
    &= f(\vx) - \S{p(x_i \mid \vx_{-i})} + \H{p(\cdot \mid \vx_{-i})}.
  \end{align*}

  That is, the energy is expected to decrease if the expected surprise of the new sample $x'_i \mid \vx_{-i}$ is smaller than the surprise of the current sample $x_i \mid \vx_{-i}$.
\end{solution}

\begin{solution}{langevin_dynamics_convergence}
  \begin{enumerate}[beginpenalty=10000]
    \item We take the minimum of \cref{eq:strongly_convex} with respect to $\vy$ on both sides.
    The minimum of the left-hand side is $f(\vzero) = 0$.
    To find the minimum of the right-hand side, we differentiate with respect to $\vy$: \begin{align*}
      \pdv{}{\vy} \brackets*{f(\vx) + \transpose{\grad f(\vx)}(\vy - \vx) + \frac{\alpha}{2} \norm{\vy - \vx}_2^2} = \grad f(\vx) + \alpha (\vy - \vx).
    \end{align*}
    Setting the partial derivative to zero, we obtain \begin{align*}
      \vy = \vx - \frac{1}{\alpha} \grad f(\vx).
    \end{align*}
    Plugging this $\vy$ into \cref{eq:strongly_convex}, we have \begin{align*}
      0 &\geq f(\vx) - \frac{1}{\alpha} \norm{\grad f(\vx)}_2^2 + \frac{1}{2 \alpha} \norm{\grad f(\vx)}_2^2 \\
      &= f(\vx) - \frac{1}{2 \alpha} \norm{\grad f(\vx)}_2^2.
    \end{align*}

    \item Using the chain rule, \begin{align*}
      \odv{}{t} f(\vx_t) &= \transpose{\grad f(\vx_t)} \odv{}{t} \vx_t.
      \intertext{Note that $\odv{}{t} \vx_t = - \grad f(\vx_t)$ by \cref{eq:gradient_flow}, so}
      &= -\norm{\grad f(\vx_t)}_2^2 \\
      &\leq - 2 \alpha f(\vx_t)
    \end{align*} where the last step follows from the PL-inequality \eqref{eq:polyak_lojasiewicz}.

    \item Follows directly from (2) and Grnwall's inequality \eqref{eq:groenwall} by letting $g(t) = f(\vx_t)$ and noting that $- \int_0^t 2 \alpha \,d s = - 2 \alpha t$.

    \item It suffices to show $\grad q_t = q_t \grad \log q_t$.
    By the chain rule, \begin{align*}
      \grad \log q_t = \frac{\grad q_t}{q_t}.
    \end{align*}
    We obtain the desired result by rearranging the terms.

    \item We have \begin{align*}
      \odv{}{t} \KL{q_t}{p} &= \odv{}{t} \int q_t \log \frac{q_t}{p} \,d \vtheta.
      \intertext{By the chain rule,}
      &= \int \pdv{q_t}{t} \log \frac{q_t}{p} \,d \vtheta + \int q_t \pdv{}{t} \log \frac{q_t}{p} \,d \vtheta.
      \intertext{For the second term, we have \begin{align*}
        \int q_t \pdv{}{t} \log \frac{q_t}{p} \,d \vtheta = \int \pdv{q_t}{t} \,d \vtheta = \odv{}{t} \underbrace{\int q_t \,d \vtheta}_1 = 0.
      \end{align*} Plugging in the Fokker-Planck equation \eqref{eq:simp_fokker_planck} into the first term, we obtain}
      &= \int \dive \parentheses*{q_t \grad \log \frac{q_t}{p}} \log \frac{q_t}{p} \,d \vtheta.
      \intertext{Letting $\varphi \defeq \log \frac{q_t}{p}$ and $\vF \defeq \grad \varphi$, and then applying the hint, we have}
      &= \int (\dive q_t \vF) \varphi \,d \vtheta \\
      &= - \int q_t \norm{\grad \varphi}_2^2 \,d \vtheta \\
      &= - \E[\vtheta \sim q_t]{\norm{\grad \log \frac{q_t(\vtheta)}{p(\vtheta)}}_2^2} \\
      &= - \Fisher{q_t}{p}.
    \end{align*}

    \item Noting that $p$ satisfies the LSI with constant $\alpha$ and combining with (5), we have that $\odv{}{t} \KL{q_t}{p} \leq -2 \alpha \KL{q_t}{p}$.
    Observe that this result is analogous to the result derived in (2) and the LSI can intuitively be seen as the PL-inequality, but in the space of distributions.
    Analogously to (3), we obtain the desired convergence result by applying Grnwall's inequality \eqref{eq:groenwall}.

    \item By Pinsker's inequality \eqref{eq:pinsker}, $\norm{q_t - p}_{\mathrm{TV}} \leq e^{-\alpha t} \sqrt{2 \KL{q_0}{p}}$.
    It follows from elementary algebra that $\norm{q_t - p}_{\mathrm{TV}} \leq \epsilon$ if $t \geq \frac{1}{\alpha} \log
    \frac{C}{\epsilon}$ where $C \defeq \sqrt{2 \KL{q_0}{p}}$.
    Thus, $\tau_{\mathrm{TV}}(\epsilon) \in \BigO{\log(1/\epsilon)}$.
  \end{enumerate}
\end{solution}

\begin{solution}{hmc}
  \begin{enumerate}[beginpenalty=10000]
    \item We show that under the dynamics \eqref{eq:hamiltonian_dynamics}, the Hamiltonian $H(\vx, \vy)$ is a constant.
    In particular, $H(\vxp, \vyp) = H(\vx, \vy)$.
    This directly implies that \begin{align*}
      \alpha((\vxp,\vyp) \mid (\vx,\vy)) = \min\{1, \exp(H(\vxp, \vyp) - H(\vx, \vy))\} = 1.
    \end{align*}

    To see why $H(\vx, \vy)$ is constant, we compute \begin{align*}
      \odv{}{t} H(\vx, \vy) &= \grad_\vx H \cdot \odv{\vx}{t} + \grad_\vy H \cdot \odv{\vy}{t} \margintag{using the chain rule} \\
      &= \grad_\vx H \cdot \grad_\vy H - \grad_\vx H \cdot \grad_\vy H \margintag{using the Hamiltonian dynamics \eqref{eq:hamiltonian_dynamics}} \\
      &= 0.
    \end{align*}

    \item By applying one Leapfrog step, we have for $\vx$, \begin{align*}
      \vx(t + \tau) &= \vx(t) + \tau \vy(t + \nicefrac{\tau}{2}) \margintag{using \cref{eq:leapfrog_x}} \\
      &= \vx(t) + \tau \parentheses*{\vy(t) - \frac{\tau}{2} \grad_\vx f(\vx(t))} \margintag{using \cref{eq:leapfrog_y1}} \\
      &= \vx(t) - \frac{\tau^2}{2} \grad_\vx f(\vx(t)) + \tau \vy(t).
    \end{align*}
    Now observe that $\vy(t)$ is a Gaussian random variable, independent of $\vx$ (because we sample $\vy$ freshly at the beginning of the $L$ Leapfrog steps, and we are doing just one Leapfrog step).
    By renaming $\tau' \defeq \nicefrac{\tau^2}{2}$ and $\epsilon \defeq \vy(t)$, we get \begin{align*}
      \vx_{t+1} = \vx_t - \tau' \grad_\vx f(\vx_t) + \sqrt{2 \tau'} \epsilon
    \end{align*} which coincides with the proposal distribution of Langevin Monte Carlo \eqref{eq:mala}.
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:bdl}}

\begin{solution}{softmax_and_logistic_function}
  We have \begin{align*}
    \sigma_1(\vf) &= \frac{\exp(f_1)}{\exp(f_0) + \exp(f_1)} \margintag{using the definition of the softmax function \eqref{eq:softmax_function}} \\
    &= \frac{1}{\exp(f_0 - f_1) + 1} \margintag{multiplying by $\frac{\exp(-f_1)}{\exp(-f_1)}$} \\
    &= \sigma(-(f_0 - f_1)). \margintag{using the definition of the logistic function \eqref{eq:logistic_function}}
  \end{align*}
  $\sigma_0(\vf) = 1 - \sigma(f)$ follows from the fact that $\sigma_0(\vf) + \sigma_1(\vf) = 1$.
\end{solution}

\section*{\nameref{sec:active_learning}}

\begin{solution}{mi_and_kl}
  We have \begin{align*}
    \I{\rX}{\rY} &= \H{\rX} -  \H{\rX \mid \rY} \margintag{using the definition of mutual information \eqref{eq:mi}} \\[5pt]
    &= \E[\vx]{-\log p(\vx)} - \E[(\vx,\vy)]{-\log p(\vx \mid \vy)} \margintag{using the definitions of entropy \eqref{eq:entropy} and conditional entropy \eqref{eq:cond_entropy}} \\[5pt]
    &= \E[(\vx, \vy)]{-\log p(\vx)} - \E[(\vx,\vy)]{-\log p(\vx \mid \vy)} \margintag{using the law of total expectation \eqref{eq:lotp}} \\
    &= \E[(\vx, \vy)]{\log \frac{p(\vx \mid \vy)}{p(\vx)}}.
  \end{align*}

  From this we get directly that \begin{align*}
    \I{\rX}{\rY} &= \E*[\vy]{\E[\vx \mid \vy]{\log \frac{p(\vx \mid \vy)}{p(\vx)}}} \\
    &= \E[\vy]{\KL{p(\vx \mid \vy)}{p(\vx)}}, \margintag{using the definition of KL-divergence \eqref{eq:kl}}
  \end{align*} and we also conclude \begin{align*}
    \I{\rX}{\rY} &= \E[(\vx, \vy)]{\log \frac{p(\vx, \vy)}{p(\vx) p(\vy)}} \margintag{using the definition of conditional probability \eqref{eq:cond_prob}} \\
    &= \KL{p(\vx, \vy)}{p(\vx) p(\vy)}. \margintag{using the definition of KL-divergence \eqref{eq:kl}}
  \end{align*}
\end{solution}

\begin{solution}{cond_mi_non_monotonicity}
  Symmetry of conditional mutual information \eqref{eq:cond_mi_symmetry} and \cref{eq:cond_mi_joint_mi} give the following relationship, \begin{align}
    \I{\rX}{\rY,\rZ} = \I{\rX}{\rY} + \I{\rX}{\rZ \mid \rY} = \I{\rX}{\rZ} + \I{\rX}{\rY \mid \rZ}. \label{eq:cond_mi_non_monotonicity_helper}
  \end{align}
  \begin{enumerate}
    \item $\rX \perp \rZ$ implies $\I{\rX}{\rZ} = 0$.
    Thus, \cref{eq:cond_mi_non_monotonicity_helper} simplifies to \begin{align*}
      \I{\rX}{\rY} + \I{\rX}{\rZ \mid \rY} = \I{\rX}{\rY \mid \rZ}.
    \end{align*}
    Using that $\I{\rX}{\rZ \mid \rY} \geq 0$, we conclude $\I{\rX}{\rY \mid \rZ} \geq \I{\rX}{\rY}$.

    \item $\rX \perp \rZ \mid \rY$ implies $\I{\rX}{\rZ \mid \rY} = 0$.
    \Cref{eq:cond_mi_non_monotonicity_helper} simplifies to \begin{align*}
      \I{\rX}{\rY} = \I{\rX}{\rZ} + \I{\rX}{\rY \mid \rZ}.
    \end{align*}
    Using that $\I{\rX}{\rZ} \geq 0$, we conclude $\I{\rX}{\rY \mid \rZ} \leq \I{\rX}{\rY}$.

    \item Again, \cref{eq:cond_mi_non_monotonicity_helper} simplifies to \begin{align*}
      \I{\rX}{\rY} = \I{\rX}{\rZ} + \I{\rX}{\rY \mid \rZ}.
    \end{align*}
    Using that $\I{\rX}{\rY \mid \rZ} \geq 0$, we conclude $\I{\rX}{\rZ} \leq \I{\rX}{\rY}$.
  \end{enumerate}
\end{solution}

\begin{solution}{interaction_information}
  \begin{enumerate}[beginpenalty=10000]
    \item Expanding the definition of interaction information, one obtains \begin{align*}
      \I{\rX}{\rY ; \rZ} = \; &(\H{\rX} + \H{\rY} + \H{\rZ}) \\
      &- (\H{\rX,\rY} + \H{\rX,\rZ} + \H{\rY,\rZ}) \\
      &+ \H{\rX,\rY,\rZ},
    \end{align*} and hence, interaction information is symmetric.

    \item Conditional on either one of $X_1$ or $X_2$, the distribution of $Y$ remains unchanged, and hence $\I{Y}{X_1 \mid X_2} = \I{Y}{X_2 \mid X_1} = 0$.
    Conversely, conditional on both $X_1$ and $X_2$, $Y$ is fully determined, and hence $\I{Y}{X_1,X_2} = 1$ noting that $Y$ encodes one bit worth of information.
    Thus, $\I{Y ; X_1}{X_2} = -1$ meaning that there is synergy between $X_1$ and $X_2$ with respect to $Y$.
  \end{enumerate}
\end{solution}

\begin{solution}{marginal_gain_mi}
  As suggested, we derive the result in two steps.
  \begin{enumerate}
    \item First, we have \begin{align*}
      \Delta_I(\vx \mid \sA) &= I(\sA \cup \{\vx\}) - I(\sA) \margintag{using the definition of marginal gain \eqref{eq:marginal_gain}} \\[5pt]
      &= \Ism{\vf_{\sA \cup \{\vx\}}}{\vy_\sA, y_\vx} - \I{\vf_\sA}{\vy_\sA} \margintag{using the definition of $I$ \eqref{eq:mi_optimization}} \\
      &= \Ism{\vf_{\sA \cup \{\vx\}}}{\vy_\sA, y_\vx} - \Ism{\vf_{\sA \cup \{\vx\}}}{\vy_\sA} \margintag{using $\vy_\sA \perp f_\vx \mid \vf_\sA$} \\
      &= \Hsm{\vf_{\sA \cup \{\vx\}} \mid \vy_\sA} - \Hsm{\vf_{\sA \cup \{\vx\}} \mid \vy_\sA, y_\vx} \margintag{using the definition of MI \eqref{eq:mi}} \\
      &= \Ism{\vf_{\sA \cup \{\vx\}}}{y_\vx}[\vy_\sA] \margintag{using the definition of cond. MI \eqref{eq:cond_mi}} \\
      &= \Ism{\vf_\sA}{y_\vx}[f_\vx, \vy_\sA] + \Ism{f_\vx}{y_\vx}[\vy_\sA] \margintag{using \cref{eq:cond_mi_joint_mi}} \\
      &= \Ism{f_\vx}{y_\vx}[\vy_\sA]. \margintag{using $\Ism{\vf_\sA}{y_\vx}[f_\vx, \vy_\sA] = 0$ as $y_\vx \perp \vf_\sA \mid f_\vx$}
    \end{align*}

    \item For the second part, we get \begin{align*}
      \Ism{f_\vx}{y_\vx}[\vy_\sA] &= \Ism{y_\vx}{f_\vx}[\vy_\sA] \margintag{using symmetry of conditional MI \eqref{eq:cond_mi_symmetry}} \\
      &= \H{y_{\vx} \mid \vy_\sA} - \Hsm{y_{\vx} \mid f_\vx, \vy_\sA} \margintag{using the definition of cond. MI \eqref{eq:cond_mi}} \\
      &= \H{y_{\vx} \mid \vy_\sA} - \H{y_{\vx} \mid f_\vx} \margintag{using that $y_\vx \perp \vvarepsilon_\sA$ so $y_\vx \perp \vy_\sA \mid f_\vx$} \\
      &= \H{y_{\vx} \mid \vy_\sA} - \H{\varepsilon_\vx}. \margintag{given $f_\vx$, the only randomness in $y_{\vx}$ originates from $\varepsilon_\vx$}
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{submodularity_and_no_synergy}
  We have \begin{align*}
    \Ism{f_\vx}{y_\vx ; \vy_{\sB \setminus \sA}}[\vy_\sA] &= \Ism{f_\vx}{y_\vx}[\vy_\sA] - \Ism{f_\vx}{y_\vx}[\vy_\sB] \margintag{using the definitiion of interaction information \eqref{eq:interaction_information}} \\[5pt]
    &= \Delta_I(\vx \mid \sA) - \Delta_I(\vx \mid \sB) \margintag{using \cref{eq:mg_mi}} \\
    &\geq 0
  \end{align*} where the final inequality follows from submodularity of $I$.
\end{solution}

\begin{solution}{transductive_active_learning}
  \begin{enumerate}[beginpenalty=10000]
    \item \textbf{Yes}.
    We have \[
      \Delta(j \mid S) = F(S \cup \{j\}) - F(S) = \Hsm{Z}[Y_S] - \Hsm{Z}[Y_{S\cup\{j\}}].
    \]
    This is non-negative iff \(\Hsm{Z}[Y_S] \geq \Hsm{Z}[Y_{S\cup\{j\}}]\) which is the ``information never hurts'' property \eqref{eq:information_never_hurts} of conditional entropy.

    \item \textbf{No}, our acquisition function $F$ is not equivalent to uncertainty sampling.
    Note that all \(X_i\) have identical prior variance.
    It suffices to show that not all \(i_1 \in \{1, \dots, 100\}\) maximize the marginal information gain \(\Delta(i_1 \mid \emptyset)\).

    The prior variance of \(Z\) is \(\Var{Z} = \sum_{i=1}^{100} i^2\) since all \(X_i\) are independent.
    Since the \(x_{1:100}\) and \(Z\) are jointly Gaussian, the posterior variance of \(Z\) after observing \(Y_1\) is \[
        \Var{Z}[Y_1] = \Var{Z} - \frac{\Cov{Z, Y_1}^2}{\Var{Y_1}}.
    \]
    The marginal information gain \(\Delta(i_1 \mid \emptyset)\) is equal up to constants to \(-\frac{1}{2} \log \Var{Z \mid Y_1}\).
    The variance of the observation is \(\Var{Y_1} = \Var{X_{i_1} + \varepsilon_1} = 2\) and the covariance is \(\Cov{Z, Y_1} = i_1\).
    Therefore, \[
        \Var{Z}[Y_1] = \sum_{i=1}^{100} i^2 - \frac{1}{2} i_1,
    \] which is uniquely minimized for \(i_1 = 100\).

    \item The acquisition function is submodular if the previous observation of an input \(X_i\) does not imply that the information conveyed by observing another input \(X_j\) about the value of \(Z\) is larger than it would have been without having previously observed \(X_i\).
    Clearly, if \(i = j\) then the information conveyed by the second observation is always $0$ bit, since the first observation was noiseless.
    However, depending on \(Z\) previous observations of inputs other than \(X_j\) may be crucial for the informativeness of observing \(X_j\).
    \begin{enumerate}
        \item \textbf{Yes}. If \(X_j = 0\) then we know that \(Z = 0\) also, no matter the previously observed inputs. If \(X_j = 1\) then it becomes slightly more probable that \(Z = 1\), however, this does not depend on which other points have been observed previously.
        \item \textbf{Yes}. The case of OR is symmetric to AND. If \(X_j = 1\) then we know that \(Z = 1\) also, no matter the previously observed inputs. If \(X_j = 0\) then it becomes slightly more probable that \(Z = 0\), however, this does not depend on which other points have been observed previously.
        \item \textbf{No}. Consider the case where we have observed \(X_{1:99}\) and are now observing \(X_{100}\). The marginal information gain of all previous observations was $0$ bit, since the observed values for \(X_{1:99}\) did not affect the distribution of \(Z\): the prior of \(Z\) was \(\Bern{0.5}\) and the current posterior is still the same Bernoulli distribution.
        Now, observing \(X_{100}\) will deterministically determine the value of \(f\), therefore conveying $1$ bit of information.
        However, just observing \(X_{100}\) alone, without any prior observations, conveys no information.
        This shows that the marginal information gain can increase after conditioning on more observations, and therefore the acquisition function \(F\) is not submodular with this choice of \(Z\).
    \end{enumerate}\vspace{1ex}
    The key intuition behind the above arguments is that AND and OR are monotonic functions whereas XOR is not.
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:bayesian_optimization}}

\begin{solution}{convergence_to_static_opt}
  First, observe that \begin{align}
    \lim_{T \to \infty} \frac{R_T}{T} &= \max_{\vx} \opt{f}(\vx) - \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T \opt{f}(\vx_t) \margintag{using the definition of regret \eqref{eq:regret2}} \nonumber \\
    &= \max_{\vx} \opt{f}(\vx) - \lim_{t \to \infty} \opt{f}(\vx_t). \label{eq:sublinear_regret2} \margintag{using the Cesro mean \eqref{eq:cesaro_mean}}
  \end{align}

  Now, suppose that the algorithm converges to the static optimum, \begin{align*}
    \lim_{t \to \infty} \opt{f}(\vx_t) = \max_{\vx} \opt{f}(\vx).
  \end{align*}
  Together with \cref{eq:sublinear_regret2} we conclude that the algorithm achieves sublinear regret.

  For the other direction, we prove the contrapositive.
  That is, we assume that the algorithm does not converge to the static optimum and show that it has (super-)linear regret.
  We distinguish between two cases.
  Our assumption is formalized by \begin{align*}
    \lim_{t \to \infty} \opt{f}(\vx_t) < \max_{\vx} \opt{f}(\vx).
  \end{align*}
  Together with \cref{eq:sublinear_regret2} we conclude $\lim_{T \to \infty} \nicefrac{R_T}{T} > 0$.
\end{solution}

\begin{solution}{bayesian_confidence_intervals}
  \begin{enumerate}[beginpenalty=10000]
    \item As suggested by the hint, we let $Z \sim \N{0}{1}$ and $c > 0$, and bound \begin{align}
      \Pr{Z > c} &= \Pr{Z - c > 0} \nonumber \\
      &= \int_0^\infty \frac{1}{\sqrt{2\pi}} e^{-(z+c)^2 / 2} \,d z. \margintag{using the PDF of the univariate normal distribution \eqref{eq:univ_normal}} \nonumber
      \intertext{Note that for $z \geq 0$, \begin{align*}
        \frac{(z+c)^2}{2} = \frac{z^2}{2} + z c + \frac{c^2}{2} \geq \frac{z^2}{2} + \frac{c^2}{2}.
      \end{align*}
      Thus,}
      &\leq e^{-c^2 / 2} \int_0^\infty \frac{1}{\sqrt{2\pi}} e^{-z^2 / 2} \,d z \nonumber \\
      &= e^{-c^2 / 2} \Pr{Z > 0} \nonumber \\
      &= \frac{1}{2} e^{-c^2 / 2}. \margintag{using symmetry of the standard normal distribution around $0$} \label{eq:bayesian_confidence_intervals_helper}
    \end{align}
    Since we made the Bayesian assumption $\opt{f}(\vx) \sim \N{\mu_{0}(\vx)}{\sigma_{0}^2(\vx)}$ and assumed Gaussian observation noise $y_t \sim \N{\opt{f}(\vx_t)}{\sigman^2}$, the posterior is also Gaussian: \begin{align*}
      \opt{f}(\vx) \mid \vx_{1:t}, y_{1:t} \sim \N{\mu_{t}(\vx)}{\sigma_{t}^2(\vx)}.
    \end{align*}
    Hence, writing $\fnPr_t(\cdot) \defeq \fnPr(\cdot \mid \vx_{1:t}, y_{1:t})$, \begin{align*}
      \fnPr_{t-1}(\opt{f}(\vx) \not\in \spC_t(\vx)) &= \fnPr_{t-1}(\abs{\opt{f}(\vx) - \mu_{t-1}(\vx)} > \beta_t \sigma_{t-1}(\vx)) \\
      &= 2 \fnPr_{t-1}\parentheses*{\frac{\opt{f}(\vx) - \mu_{t-1}(\vx)}{\sigma_{t-1}(\vx)} > \beta_t} \margintag{using symmetry of the Gaussian distribution} \\
      &\leq e^{- \beta_t^2 / 2}. \margintag{using \cref{eq:bayesian_confidence_intervals_helper} with $c = \beta_t$}
    \end{align*}

    \item We have \begin{align*}
      \Pr{\bigcup_{\vx \in \spX} \bigcup_{t \geq 1} \opt{f}(\vx) \not\in \spC_t(\vx)} &\leq \sum_{\vx \in \spX} \sum_{t \geq 1} \Pr{\opt{f}(\vx) \not\in \spC_t(\vx)} \margintag{using a union bound \eqref{eq:union_bound}} \\
      &\leq \card{\spX} \sum_{t \geq 1} e^{- \beta_t^2 / 2} \margintag{using (1)}.
      \intertext{Letting $\beta_t^2 \defeq 2 \log(\card{\spX} (\pi t)^2 / (6 \delta))$, we get}
      &= \frac{6 \delta}{\pi^2} \sum_{t \geq 1} \frac{1}{t^2} \\
      &= \delta. \margintag{using $\sum_{t \geq 1} \frac{1}{t^2} = \frac{\pi^2}{6}$}
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{bayesian_regret_for_gp_ucb}
  \begin{enumerate}[beginpenalty=10000]
    \item We denote the static optimum by $\vxs$.
    By the definition of $\vx_t$, \begin{align*}
      \mu_{t-1}(\vx_t) + \beta_t \sigma_{t-1}(\vx_t) &\geq \mu_{t-1}(\vxs) + \beta_t \sigma_{t-1}(\vxs) \\
      &\geq \opt{f}(\vxs). \margintag{using \cref{eq:gp_ucb_beta_calibration}}
    \end{align*}
    Thus, \begin{align*}
      r_t &= \opt{f}(\vxs) - \opt{f}(\vx_t) \\
      &\leq \beta_t \sigma_{t-1}(\vx_t) + \mu_{t-1}(\vx_t) - \opt{f}(\vx_t) \\
      & \leq 2 \beta_t \sigma_{t-1}(\vx_t). \margintag{again using \cref{eq:gp_ucb_beta_calibration}}
    \end{align*}

    \item We have for any fixed $T$, \begin{align*}
      \I{\vf_{T+1}}{\vy_{T+1}} &= \H{\vy_{T+1}} - \H{\vvarepsilon_{T+1}} \margintag{analogously to \cref{eq:mg_decomp}} \\
      &= \H{\vy_T} - \H{\vvarepsilon_T} + \H{y_{\vx_{T+1}} \mid \vy_T} - \H{\varepsilon_{\vx_{T+1}}} \margintag{using the chain rule for entropy \eqref{eq:chain_rule_entropy} and the mutual independence of $\vvarepsilon_{T+1}$} \\[5pt]
      &= \I{\vf_T}{\vy_T} + \I{f_{\vx_{T+1}}}{y_{\vx_{T+1}} \mid \vy_T} \margintag{using the definition of MI \eqref{eq:mi}} \\
      &= \I{\vf_T}{\vy_T} + \frac{1}{2}\log\parentheses*{1 + \frac{\sigma_T^2(\vx_{T+1})}{\sigman^2}}. \margintag{using \cref{eq:mi_cond_linear_gaussians}}
    \end{align*}
    Note that $\I{\vf_0}{\vy_0} = 0$.
    The result then follows by induction.

    \item By Cauchy-Schwarz, $R_T^2 \leq T \sum_{t=1}^T r_t^2$, and hence, it suffices to show $\sum_{t=1}^T r_t^2 \leq \BigO{\beta_T^2 \gamma_T}$.
    We have \begin{align*}
      \sum_{t=1}^T r_t^2 &\leq 4 \beta_T^2 \sum_{t=1}^T \sigma_{t-1}^2(\vx_t) \margintag{using part (1)} \\
      &= 4 \sigman^2 \beta_T^2 \sum_{t=1}^T \frac{\sigma_{t-1}^2(\vx_t)}{\sigman^2}.
      \intertext{Observe that $\sigma_{t-1}^2(\vx_t) /\sigman^2$ is bounded by $M \defeq \max_{\vx \in \spX}\sigma_0^2(\vx) / \sigman^2$ as variance is monotonically decreasing (cf. \cref{sec:fundamentals:gaussians}). Applying the hint, we obtain}
      &\leq 4 C \sigman^2 \beta_T^2 \sum_{t=1}^T \log\parentheses*{1 + \frac{\sigma_{t-1}^2(\vx_t)}{\sigman^2}} \\
      &= 8 C \sigman^2 \beta_T^2 \I{\vf_T}{\vy_T} \margintag{using part (2)} \\
      &\leq 8 C \sigman^2 \beta_T^2 \gamma_T. \margintag{using the definition of $\gamma_T$ \eqref{eq:gamma_t}}
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{sublinear_regret_for_linear_kernel}
  \begin{enumerate}[beginpenalty=10000]
    \item Let $\sS \subseteq \spX$ be such that $\card{\sS} \leq T$.
    Recall from \cref{eq:mi_cond_linear_gaussians} that $\I{\vf_\sS}{\vy_\sS} = \frac{1}{2} \log \det{\mI + \sigman^{-2}\mK_{\sS\sS}}$.
    Using that the kernel is linear we can rewrite $\mK_{\sS\sS} = \transpose{\mX_\sS} \mX_\sS$.
    Using Weinstein-Aronszajn's identity \eqref{eq:weinstein_aronszajn} we have \begin{align*}
      \I{\vf_\sS}{\vy_\sS} = \frac{1}{2}\log\det{\mI + \sigman^{-2} \transpose{\mX_\sS} \mX_\sS} = \frac{1}{2}\log\det{\mI + \sigman^{-2} \mX_\sS \transpose{\mX_\sS}}.
    \end{align*}
    If we define $\mM \defeq \mI + \sigman^{-2} \mX_\sS \transpose{\mX_\sS}$ as a sum of symmetric positive definite matrices, $\mM$ itself is symmetric positive definite.
    Thus, we have from Hadamard's inequality \eqref{eq:hadamard}, \begin{align*}
      \det{\mM} &\leq \det{\diag{\mI + \sigman^{-2} \mX_\sS \transpose{\mX_\sS}}} \margintag{$\diag{\mA}$ refers to the diagonal matrix whose elements are those of $\mA$} \\
      &= \det{\mI + \sigman^{-2} \diag{\mX_\sS \transpose{\mX_\sS}}}.
    \end{align*}
    Note that \begin{align*}
      \diag{\mX_\sS \transpose{\mX_\sS}}(i,i) &= \sum_{t=1}^{\card{\sS}} \vx_t(i)^2 \\
      &\leq \sum_{i=1}^d \sum_{t=1}^{\card{\sS}} \vx_t(i)^2 = \sum_{t=1}^{\card{\sS}} \underbrace{\norm{\vx_t}_2^2}_{\leq 1} \leq \card{\sS} \leq T.
    \end{align*}
    If we denote by $\lambda \leq T$ the largest term of $\diag{\mX_\sS \transpose{\mX_\sS}}$ then we have \begin{align*}
      \det{\mM} \leq (1 + \sigman^{-2} \lambda)^d \leq (1 + \sigman^{-2} T)^d,
    \end{align*} yielding, \begin{align*}
      \I{\vf_\sS}{\vy_{\sS}} \leq \frac{d}{2} \log(1 + \sigman^{-2} T)
    \end{align*} implying that $\gamma_T = \BigO{d \log T}$.

    \item Using the regret bound (cf. \cref{thm:bayesian_regret_for_gp_ucb}) and the Bayesian confidence intervals (cf. \cref{thm:bayesian_confidence_intervals}), and then $\gamma_T = \BigO{d \log T}$, we have \begin{align*}
      R_T = \BigO{\beta_T \sqrt{\gamma_T T}} = \BigOTil{\sqrt{d T}},
    \end{align*} and hence, $\lim_{T\to\infty} \frac{R_T}{T} = 0$.
  \end{enumerate}
\end{solution}

\begin{solution}{closed_form_ei}
  \begin{enumerate}[beginpenalty=10000]
    \item Note that $f$ is a Gaussian process, and hence, our posterior distribution after round $t$ is entirely defined by the mean function $\mu_{t}$ and the covariance function $k_{t}$.
    Reparameterizing the posterior distribution using a standard Gaussian \eqref{eq:gaussian_affine_transformation}, we obtain \begin{align*}
      f(\vx) \mid \vx_{1:t}, y_{1:t} = \mu_{t}(\vx) + \sigma_{t}(\vx) \varepsilon
    \end{align*} for $\varepsilon \sim \N{0}{1}$.
    We get \begin{align*}
      \mathrm{EI}_t(\vx) &= \E[f(\vx) \sim \N{\mu_{t}(\vx)}{\sigma_{t}^2(\vx)}]{I_t(\vx)} \margintag{using the definition of expected improvement \eqref{eq:ei}} \\[5pt]
      &= \E[f(\vx) \sim \N{\mu_{t}(\vx)}{\sigma_{t}^2(\vx)}]{(f(\vx) - \hat{f}_t)_+} \margintag{using the definition of improvement \eqref{eq:improvement}} \\[5pt]
      &= \E[\varepsilon \sim \N{0}{1}]{(\mu_{t}(\vx) + \sigma_{t}(\vx) \varepsilon - \hat{f}_t)_+} \margintag{using the reparameterization} \\
      &= \int_{-\infty}^{+\infty} (\mu_{t}(\vx) + \sigma_{t}(\vx) \varepsilon - \hat{f}_t)_+ \cdot \phi(\varepsilon) \,d\varepsilon.
    \end{align*}
    For $\varepsilon < \frac{\hat{f}_t - \mu_{t}(\vx)}{\sigma_{t}(\vx)} \eqdef z_t(\vx)$ we have $(\mu_{t}(\vx) + \sigma_{t}(\vx) \varepsilon - \hat{f}_t)_+ = 0$.
    Thus, we obtain \begin{align}
      \mathrm{EI}_t(\vx) &= \int_{z_t(\vx)}^{+\infty} (\mu_{t}(\vx) + \sigma_{t}(\vx) \varepsilon - \hat{f}_t) \cdot \phi(\varepsilon) \,d\varepsilon. \label{eq:ei_closed_form_helper}
    \end{align}

    \item By splitting the integral from \cref{eq:ei_closed_form_helper} into two distinct terms, we obtain \begin{align*}
      \mathrm{EI}_t(\vx) &= \begin{multlined}[t]
        (\mu_{t}(\vx) - \hat{f}_t) \int_{z_t(\vx)}^{+\infty} \phi(\varepsilon) \,d\varepsilon \\ - \sigma_{t}(\vx) \int_{z_t(\vx)}^{+\infty} (-\varepsilon) \cdot \phi(\varepsilon) \,d\varepsilon.
      \end{multlined}
    \intertext{For the first term, we use the symmetry of $\N{0}{1}$ around $0$ to write the integral in terms of the CDF.
    For the second term, we notice that $(-\varepsilon) \cdot \phi(\varepsilon) = \frac{1}{\sqrt{2\pi}} \odv{}{\varepsilon} e^{\nicefrac{-\varepsilon^2}{2}}$.
    Thus, we can derive this integral directly,}
      &= \begin{multlined}[t]
        (\mu_{t}(\vx) - \hat{f}_t) \Phi\parentheses*{-z_t(\vx)} \\ - \sigma_{t}(\vx) \parentheses*{\lim_{\varepsilon\to\infty}\phi(\varepsilon) - \phi\parentheses*{z_t(\vx)}}.
      \end{multlined}
    \end{align*}
    Using the symmetry of $\phi$ around $0$, we obtain \begin{align*}
      \mathrm{EI}_t(\vx) = (\mu_{t}(\vx) - \hat{f}_t) \Phi\parentheses*{\frac{\mu_{t}(\vx) - \hat{f}_t}{\sigma_{t}(\vx)}} + \sigma_{t}(\vx) \phi\parentheses*{\frac{\mu_{t}(\vx) - \hat{f}_t}{\sigma_{t}(\vx)}}.
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{regret_of_ids}
  \begin{enumerate}[beginpenalty=10000]
    \item As $\vx_t^{\mathrm{UCB}}$ is the UCB action, \begin{align*}
      \hat{\Delta}_t(\vx_t^{\mathrm{UCB}}) = u_t(\vx_t^{\mathrm{UCB}}) - l_t(\vx_t^{\mathrm{UCB}}) = 2 \beta_{t+1} \sigma_t(\vx_t^{\mathrm{UCB}}).
    \end{align*}

    \item We first bound $I_t(\vx_t^{\mathrm{UCB}})$: \begin{align*}
      I_t(\vx_t^{\mathrm{UCB}}) &= \I{f_{\vx_t^{\mathrm{UCB}}}}{y_{\vx_t^{\mathrm{UCB}}}}[\vx_{1:t}, y_{1:t}] \\
      &= \frac{1}{2} \log\parentheses*{1 + \frac{\sigma_t^2(\vx_t^{\mathrm{UCB}})}{\sigman^2}}. \margintag{using \cref{eq:mi_cond_linear_gaussians}}
      \intertext{Note that $\sigma_t^2(\vx) / \sigman^2 \leq C$ for some constant $C$ since variance is decreasing monotonically. So, applying the hint,}
      &\geq \frac{\sigma_t^2(\vx_t^{\mathrm{UCB}})}{2 C \sigman^2}.
    \end{align*}
    Combining this with (1), we obtain \begin{align*}
      \widehat{\Psi}_t(\vx_t^{\mathrm{UCB}}) &= \frac{\hat{\Delta}_t(\vx_t^{\mathrm{UCB}})^2}{I_t(\vx_t^{\mathrm{UCB}})} \leq 8 C \sigman^2 \beta_{t+1}^2.
    \end{align*}

    \item With high probability, \begin{align*}
      \Psi_t(\vx_{t+1}) \leq \widehat{\Psi}_t(\vx_{t+1}) \leq \widehat{\Psi}_t(\vx_t^{\mathrm{UCB}}) \leq 8 C \sigman^2 \beta_{t+1}^2,
    \end{align*} where the first inequality is due to $\Delta(\vx) \leq \hat{\Delta}_t(\vx)$ with high probability, the second inequality due to the definition of the IDS algorithm \eqref{eq:ids}, and the third inequality is from (2).
    Invoking \cref{thm:regret_information_ratio} with $\overline{\Psi}_T \defeq 8 C \sigman^2 \beta_T^2$, we obtain that \begin{align*}
      R_T &\leq \sqrt{\gamma_T \overline{\Psi}_T T} = \beta_T \sqrt{8 C \sigman^2 \gamma_T T}
    \end{align*} with high probability.
  \end{enumerate}
\end{solution}

\begin{solution}{lite}
  First notice that by the definition of $\fnS'$, we obtain the simpler objective:
  \begin{equation*}
    \spW(\pi) = \sum_{x \in \spX} \pi(x) \parentheses*{ \mu_t(x) + \phi(\Phi^{-1}(\pi(x))) \sigma_t(x) }.
  \end{equation*}
  Next, we show that \(\spW(\pi)\) is concave by computing the Hessian:
  \begin{align*}
    \frac{\partial}{\partial \pi(x)} \spW(\pi) & = \mu_t(x) - \sigma_t(x) \Phi^{-1}(\pi(x)) \phi(\Phi^{-1}(\pi(x))) \frac{d}{d \pi(x)} \Phi^{-1}(\pi(x))\\
    & = \mu_t(x) - \sigma_t(x) \Phi^{-1}(\pi(x))\nonumber\\
    \frac{\partial^2}{\partial \pi(x) \partial \pi(z)} \spW(r) & = - \sigma_t(x) \Ind{x=z} \frac{1}{\phi(\Phi^{-1}(\pi(x)))} \begin{cases}
      < 0 & x = z\\
      = 0 & x \not = z
    \end{cases},
  \end{align*}
  where we used the inverse function rule twice.
  From negative definiteness of the Hessian, it follows that $\spW(\cdot)$ is strictly concave.

  We show next that the optimum lies in the relative interior of the probability simplex, \(\pi^* \in \text{relint}(\Delta^{\spX})\).
  Indeed, at the border of the probability simplex the partial derivatives explode:
  \begin{equation*}
    \frac{\partial}{\partial \pi(x)} \spW(r) = \mu_t(x) - \sigma_t(x) \Phi^{-1}(\pi(x)) = \begin{cases}
      \infty & \pi(x) \to 0^+\\
      \text{finite} & \pi(x) \in (0,1)\\
      -\infty & \pi(x) \to 1^-
    \end{cases}.
  \end{equation*}
  Together with the concavity of \(\spW(\cdot)\) this ensures that \(r^* \in \text{relint}(\Delta^{\spX})\).
  Hence, \(\pi^*\) is a local optimizer of \(\spW(r)\) on the plane defined by \({\sum_{x \in \spX} \pi(x) = 1}\).
  Consequently, we obtain the Lagrangian
  \begin{equation*}
    \spL(\pi, \kappa) : (0,1)^{|\spX|} \times \R \to \R,\quad \pi \mapsto \spW(\pi) + \kappa \parentheses*{1-\sum_{x\in\spX} \pi(x)}.
  \end{equation*}
  Setting its partial derivatives equal to zero, we derive the closed-form solution:
  \begin{align*}
    0 = \mu_t(x) - \sigma_t(x) \Phi^{-1}(\pi^*(x)) - \kappa^* \iff \pi^*(x)  = \Phi\parentheses*{\frac{\mu_t(x) - \kappa^*}{\sigma_t(x)}},
  \end{align*}
  where \(\kappa^*\) ensures a valid distribution, i.e., \(\sum_{x\in \spX} \pi^*(x) = 1\).
\end{solution}

\begin{solution}{recall}
  Since $R_1$ and $R_2$ are independent, we have \begin{align*}
    \Pr{\max\{R_1, R_2\} \leq x} &= \Pr{\{R_1 \leq x\} \cap \{R_2 \leq x\}} \\
    &= \Pr{R_1 \leq x} \cdot \Pr{R_2 \leq x} \margintag{using independence} \\
    &= F_{R_1}(x) \cdot F_{R_2}(x) \\
    &= \Phi^2\parentheses*{\frac{x}{10}}
  \end{align*} where $\Phi$ is the CDF of the standard Gaussian.
  We are looking for the probability that either $R_1$ or $R_2$ are larger than $S = 1$: \begin{align*}
    \Pr{\max\{R_1, R_2\} > 1} = 1 - \Pr{\max\{R_1, R_2\} \leq 1} = 1 - \Phi^2\parentheses*{\frac{1}{10}}.
  \end{align*}
  We have $\Phi^2(\frac{1}{10}) \approx 0.29$. Thus, the probability that either $R_1$ or $R_2$ are larger than $S = 1$ is approximately $0.71$.
  Due to the symmetry in the problem, we know \begin{align*}
    \Pr{R_1 > 1} = \Pr{R_2 > 1} \approx 0.35.
  \end{align*}
\end{solution}

\section*{\nameref{sec:mdp}}

\begin{solution}{value_functions}
  We can use \cref{eq:q_function3} to write the state-action values as a linear system of equations (i.e., as a ``table'').
  This linear system can be solved, for example, using Gaussian elimination to yield the desired result.
\end{solution}

\begin{solution}{v_and_q_functions}
  It follows directly from the definition of the state-action value function \eqref{eq:q_function2} that \begin{align*}
    \argmax_{a \in \sA} \q{x}{a} = \argmax_{a \in \sA} r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \cdot \v{x'}.
  \end{align*}
\end{solution}

\begin{solution}{optimal_policies}
  \begin{enumerate}[beginpenalty=10000]
    \item Recall from Bellman's theorem \eqref{eq:bop1} that a policy is optimal iff it is greedy with respect to its state-action value function.
    Now, observe that in the ``poor, unknown'' state, the policy $\pi$ is not greedy.

    \item Analogously to \cref{exercise:value_functions}, we write the state-action values as a linear system of equations and solve the system using, e.g., Gaussian elimination.

    \item Observe from the result of (2) that $\pi'$ is greedy with respect to its state-action value function, and hence, it follows from Bellman's theorem that $\pi'$ is optimal.
  \end{enumerate}
\end{solution}

\begin{solution}{policy_iteration_linear_convergence}
  Using the hint and $\vvs \geq \vv^{\pi}$ for any policy $\pi$, \begin{align*}
    \norm{\vv^{\pi_t} - \vvs}_\infty &\leq \norm{\mBs \vv^{\pi_{t-1}} - \vvs}_\infty \\
    &= \norm{\mBs \vv^{\pi_{t-1}} - \mBs\vvs}_\infty \margintag{using that $\vvs$ is a fixed-point of $\mBs$, that is, $\mBs\vvs = \vvs$} \\[5pt]
    &\leq \gamma \norm{\vv^{\pi_{t-1}} - \vvs}_\infty \margintag{using that $\mBs$ is a contraction, see \cref{thm:vi_convergence}} \\[5pt]
    &\leq \gamma^t \norm{\vv^{\pi_0} - \vvs}_\infty. \margintag{by induction}
  \end{align*}
\end{solution}

\begin{solution}{reward_modification}
  \begin{enumerate}[beginpenalty=10000]
    \item Recall that the value function $\fnv[\pi][\sM]$ for an MDP $\sM$ is defined as $\v[\pi]{x}[\sM] = \E[\pi]{\sum_{t=0}^\infty \gamma^t R_t}[X_0 = x]$.
    Given an optimal policy $\pis$ for $\sM$ and any policy $\pi$, we know that for any $x \in \sX$, \begin{align*}
      &&\v[\pis]{x}[\sM] &\geq \v[\pi]{x}[\sM] \\
      \iff&&\qquad \E[\pis]{\textstyle\sum_{t=0}^\infty \gamma^t R_t}[X_0 = x] &\geq \E[\pi]{\textstyle\sum_{t=0}^\infty \gamma^t R_t}[X_0 = x] \\
      \iff&&\qquad \E[\pis]{\textstyle\sum_{t=0}^\infty \gamma^t \alpha R_t}[X_0 = x] &\geq \E[\pi]{\textstyle\sum_{t=0}^\infty \gamma^t \alpha R_t}[X_0 = x] \margintag{multiplying both sides by $\alpha$} \\
      \iff&&\qquad \E[\pis]{\textstyle\sum_{t=0}^\infty \gamma^t R'_t}[X_0 = x] &\geq \E[\pi]{\textstyle\sum_{t=0}^\infty \gamma^t R'_t}[X_0 = x] \\
      \iff&&\qquad \v[\pis]{x}[\sM'] &\geq \v[\pi]{x}[\sM'].
    \end{align*}
    Thus, $\pis$ is an optimal policy for $\sM'$.

    \item We give an example where the optimal policies differ when rewards are shifted.

    Consider an MDP with three states $\{1,2,3\}$ where $1$ is the initial state and $3$ is a terminal state.
    If one plays action $A$ in states $1$ or $2$ one transitions directly to the terminal state.
    Additionally, in state $1$ one can play action $B$ which leads to state $2$.
    Let every transition give a deterministic reward of $r \defeq -1$.
    Then it is optimal to traverse the shortest path to the terminal state, in particular, to choose action $A$ when in state $1$.

    If we consider the reward $r' \defeq r + 2 = 1$, then it is optimal to traverse the longest path to the terminal state, in particular, to choose action $B$ when in state $1$.

    \item For an MDP $\sM$, we know that is optimal state-action value function satisfies Bellman's optimality equation \eqref{eq:bop2_q}, \begin{align*}
      \q*{x}{a}[\sM] &= \E[x' \mid x, a]{r(x, x') + \gamma \max_{a' \in \sA} \q*{x'}{a'}[\sM]}.
    \end{align*}
    For the MDP $\sM'$, we have \begin{align*}
      \q*{x}{a}[\sM'] &= \E[x' \mid x, a]{r'(x, x') + \gamma \max_{a' \in \sA} \q*{x'}{a'}[\sM']} \\
      &= \E[x' \mid x, a]{r(x, x') + f(x, x') + \gamma \max_{a' \in \sA} \q*{x'}{a'}[\sM']} \\
      &= \E[x' \mid x, a]{r(x, x') + \gamma \phi(x') - \phi(x) + \gamma \max_{a' \in \sA} \q*{x'}{a'}[\sM']}.
    \end{align*}
    Reorganizing the terms, we obtain \begin{align*}
      \q*{x}{a}[\sM'] + \phi(x) = \E[x' \mid x, a]{r(x, x') + \gamma \max_{a' \in \sA} \parentheses*{\q*{x'}{a'}[\sM'] + \phi(x')}}.
    \end{align*}
    If we now define $\q{x}{a} \defeq \q*{x}{a}[\sM'] + \phi(x)$, we have \begin{align*}
      \q{x}{a} &= \E[x' \mid x, a]{r(x, x') + \gamma \max_{a' \in \sA} \q{x'}{a'}}.
    \end{align*}
    This is exactly Bellman's optimality equation for the MDP $\sM$ with reward function $r$, and hence, $\fnq \equiv \fnq[\star][\sM]$.

    If we take $\pis$ to be an optimal policy for $\sM$, then it satisfies \begin{align*}
      \pis(x) &\in \argmax_{a \in \sA} \q*{x}{a}[\sM] \\
      &= \argmax_{a \in \sA} \q*{x}{a}[\sM'] + \phi(x) \margintag{using the above characterization of $\fnq[\star][\sM]$} \\
      &= \argmax_{a \in \sA} \q*{x}{a}[\sM']. \margintag{using that $\phi(x)$ is independent of $a$}
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{fishing_problem_pomdb}
  \begin{enumerate}[beginpenalty=10000]
    \item We compute the answer in three steps: \begin{enumerate}
      \item \textbf{Predict step}: We compute the predicted belief after action $W$ but before observing $o_1$. \begin{align*}
        b'_0(F) &= b_0(F) \cdot 0.6 + b_0(\overline{F}) \cdot 0.5 = 0.55 \\
        b'_0(\overline{F}) &= b_0(F) \cdot 0.4 + b_0(\overline{F}) \cdot 0.5 = 0.45.
      \end{align*}
      \item \textbf{Update step}: Using the observation $o_1$ and the observation model, we update the belief. \begin{align*}
        b_1(F) &= \frac{1}{Z} b'_0(F) \Pr{o_1 \mid F} = \frac{1}{Z} \cdot 0.55 \cdot 0.8 \\
        b_1(\overline{F}) &= \frac{1}{Z} b'_0(\overline{F}) \Pr{o_1 \mid \overline{F}} = \frac{1}{Z} \cdot 0.45 \cdot 0.3.
      \end{align*}
      \item \textbf{Normalization}: We compute the normalization constant $Z$. \begin{align*}
        Z = 0.55 \cdot 0.8 + 0.45 \cdot 0.3 = 0.575.
      \end{align*}
    \end{enumerate}
    Therefore, \begin{align*}
      b_1(F) &= \frac{0.55 \cdot 0.8}{0.575} \approx 0.765, \\
      b_1(\overline{F}) &= \frac{0.45 \cdot 0.3}{0.575} \approx 0.235.
    \end{align*}

    \item We observe that if $A_1 = P$, then $\Pr{X_2 = F} = 0$ given any starting state, which will result in a zero in the belief update formula.
    Therefore, \begin{itemize}
      \item $b_2 = (0, 1)$ given $A_1 = P$ and $O_1 = o_1$,
      \item $b_2 = (0, 1)$ given $A_1 = P$ and $O_1 = o_2$.
    \end{itemize}

    \textbf{Case with $A_1 = W$ and $O_1 = o_1$}:
    Using the belief update formula~\eqref{eq:belief}, \begin{align*}
      b_2(F) &\propto \Pr{o_1 \mid F} \cdot \parentheses*{b_1(F) \cdot p(F \mid F, W) + b_1(\overline{F}) \cdot p(F \mid \overline{F}, W)} \approx 0.461 \\
      b_2(\overline{F}) &\propto \Pr{o_1 \mid \overline{F}} \cdot \parentheses*{b_1(F) \cdot p(\overline{F} \mid F, W) + b_1(\overline{F}) \cdot p(\overline{F} \mid \overline{F}, W)} \approx 0.127.
    \end{align*}
    By rescaling the probabilities, we get $b_2 \approx (0.784, 0.216)$.

    \textbf{Case with $A_1 = W$ and $O_1 = o_2$}:
    Using the belief update formula~\eqref{eq:belief}, \begin{align*}
      b_2(F) &\propto \Pr{o_2 \mid F} \cdot \parentheses*{b_1(F) \cdot p(F \mid F, W) + b_1(\overline{F}) \cdot p(F \mid \overline{F}, W)} \approx 0.115 \\
      b_2(\overline{F}) &\propto \Pr{o_2 \mid \overline{F}} \cdot \parentheses*{b_1(F) \cdot p(\overline{F} \mid F, W) + b_1(\overline{F}) \cdot p(\overline{F} \mid \overline{F}, W)} \approx 0.296.
    \end{align*}
    By rescaling the probabilities, we get $b_2 \approx (0.279, 0.721)$.
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:tabular_rl}}

\begin{solution}{q_learning}
  \begin{enumerate}[beginpenalty=10000]
    \item $\Q*{A}{\downarrow} = 1.355, \Q*{G_1}{\text{exit}} = 5.345, \Q*{G_2}{\text{exit}} = 0.5$

    \item Repeating the given episodes infinitely often will not lead to convergence to the optimal Q-function because not all state-action pairs are visited infinitely often.

    Let us assume we observe the following episode instead of the first episode.
    \begin{center}
      \begin{tabular}{|cccc|}
        \multicolumn{4}{c}{Episode 3} \\
        \hline
        $x$ & $a$ & $x'$ & $r$ \\
        \hline
        $A$ & $\rightarrow$ & $B$ & $0$ \\
        $B$ & $\downarrow$ & $G_2$ & $0$ \\
        $G_2$ & exit & & $1$ \\
        \hline
      \end{tabular}
      \vspace{5pt}
    \end{center}
    If we repeat episodes 2 and 3 infinitely often, Q-learning will converge to the optimal Q-function as all state-action pairs will be visited infinitely often.

    \item First, recall that Q-learning is an off-policy algorithm, and hence, even if episodes are obtained off-policy, Q-learning will still converge to the optimal Q-function (if the other convergence conditions are met).
    Note that it only matters which state-action pairs are observed and not which policies were followed to obtain these observations.

    The ``closer'' the initial Q-values are to the optimal Q-function, the faster the convergence of Q-learning.
    However, if the convergence conditions are met, Q-learning will converge to the optimal Q-function regardless of the initial Q-values.

    \item $\v*{A} = 10$, $\v*{B} = 10$, $\v*{G_1} = 10$, $\v*{G_2} = 1$
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:mfarl}}

\begin{solution}{q_learning_func_approx}
  \begin{enumerate}[beginpenalty=10000]
    \item We have to show that \begin{align*}
      \v*{x} = \max_{a \in \sA} r(x, a) + \gamma \E[x' \mid x, a]{\v*{x'}}
    \end{align*} for every $x \in \{1, 2, \dots, 7\}$.
    We give a derivation here for $x = 1$ and $x = 2$.
    \begin{itemize}
      \item For $x = 1$, \begin{align*}
        \v*{1} &= -3 \\
        \max_{a \in \sA} r(1, a) + \gamma \E[x' \mid 1, a]{\v*{x'}} &= -3
      \end{align*} since \begin{align*}
        r(1, a) + \gamma \E[x' \mid 1, a]{\v*{x'}} = \begin{cases}
          -1 + -2 = -3 & \text{if $a = 1$} \\
          -1 + -3 = -4 & \text{if $a = -1$}.
        \end{cases}
      \end{align*}
      \item Likewise, for $x = 2$, \begin{align*}
        \v*{2} &= -2 \\
        \max_{a \in \sA} r(2, a) + \gamma \E[x' \mid 2, a]{\v*{x'}} &= -2
      \end{align*} since \begin{align*}
        r(2, a) + \gamma \E[x' \mid 2, a]{\v*{x'}} = \begin{cases}
          -1 + -1 = -2 & \text{if $a = 1$} \\
          -1 + -3 = -4 & \text{if $a = -1$}.
        \end{cases}
      \end{align*}
    \end{itemize}

    \item We have \begin{align*}
      \Q{3}{-1} &= 0 + \frac{1}{2}\parentheses*{-1 + \max_{a' \in \sA} \Q{2}{a'}} = \frac{1}{2} (-1 + 0) = - \frac{1}{2} \\
      \Q{2}{1} &= 0 + \frac{1}{2}\parentheses*{-1 + \max_{a' \in \sA} \Q{3}{a'}} = \frac{1}{2} (-1 + 0) = - \frac{1}{2} \\
      \Q{3}{1} &= 0 + \frac{1}{2}\parentheses*{-1 + \max_{a' \in \sA} \Q{4}{a'}} = \frac{1}{2} (-1 + 0) = - \frac{1}{2} \\
      \Q{4}{1} &= 0 + \frac{1}{2}\parentheses*{0 + \max_{a' \in \sA} \Q{4}{a'}} = \frac{1}{2} (0 + 0) = 0.
    \end{align*}

    \item We compute \begin{align*}
      \grad_\vw \ell(\vw; \tau) &= - \parentheses*{r + \gamma \max_{a' \in \sA} \Q{x'}{a'; \old{\vw}} - \Q{x}{a; \vw}} \begin{bmatrix}
        x \\ a \\ 1
      \end{bmatrix} \margintag{using the derivation of \cref{eq:q_learning_gradient_update}} \\
      &= - \parentheses*{-1 + \max_{a' \in \sA}\{1 - a' -2\} - (-2 - 1 + 1)} \begin{bmatrix}
        2 \\ -1 \\ 1
      \end{bmatrix} \\
      &= \begin{bmatrix}
        -2 \\ 1 \\ -1
      \end{bmatrix}.
    \end{align*}

    This gives \begin{align*}
      \vw' &= \vw - \alpha \grad_\vw \ell(\vw; \tau) \\
      &= \begin{bmatrix}
        -1 \\ 1 \\ 1
      \end{bmatrix} - \frac{1}{2} \begin{bmatrix}
        -2 \\ 1 \\ -1
      \end{bmatrix} = \begin{bmatrix}
        0 \\ \nicefrac{1}{2} \\ \nicefrac{3}{2}
      \end{bmatrix}.
    \end{align*}
  \end{enumerate}
\end{solution}

\begin{solution}{eligibility_vector}
  We have \begin{align*}
    \grad_\vvarphi \log \pi_\vvarphi(a \mid \vx) &= \frac{\grad_\vvarphi \pi_\vvarphi(a \mid \vx)}{\pi_\vvarphi(a \mid \vx)} \margintag{using the chain rule} \\
    &= \vphi(\vx, a) - \frac{\sum_{b \in \sA} \vphi(\vx, b) \exp(\transpose{\vvarphi} \vphi(\vx, b))}{\sum_{b \in \sA} \exp(\transpose{\vvarphi} \vphi(\vx, b))} \margintag{using elementary calculus} \\
    &= \vphi(\vx, a) - \sum_{b \in \sA} \pi_\vvarphi(b \mid \vx) \cdot \vphi(\vx, b).
  \end{align*}
\end{solution}

\begin{solution}{score_gradients_with_baselines_variance}
  \begin{enumerate}[beginpenalty=10000]
    \item The result follows directly using that \begin{align*}
      \Var{f(\rX) - g(\rX)} = \Var{f(\rX)} + \Var{g(\rX)} - 2 \Cov{f(\rX), g(\rX)}. \margintag{using \cref{eq:sum_variance}}
    \end{align*}

    \item Denote by $r(\tau)$ the discounted rewards attained by trajectory $\tau$.
    Let $f(\tau) \defeq r(\tau) \grad_\vvarphi \altpi_\vvarphi(\tau)$ and $g(\tau) \defeq b \grad_\vvarphi \altpi_\vvarphi(\tau)$.
    Recall that $\E[\tau \sim \altpi_\vvarphi]{g(\tau)} = 0$, implying that \begin{align*}
      \Var{g(\tau)} &= \Var{b \grad_\vvarphi \altpi_\vvarphi(\tau)} \\
      &= \E{\parentheses*{b \grad_\vvarphi \altpi_\vvarphi(\tau)}^2}. \margintag{using the definition of variance \eqref{eq:variance}}
    \end{align*}
    On the other hand, \begin{align*}
      \Cov{f(\tau), g(\tau)} &= \E{(f(\tau) - \E{f(\tau)}) g(\tau)} \margintag{using the definition of covariance \eqref{eq:covariance}} \\
      &= \E{f(\tau) g(\tau)} - \E{f(\tau)} \underbrace{\E{g(\tau)}}_{0} \margintag{using linearity of expectation \eqref{eq:linearity_expectation}} \\
      &= \E{b \cdot r(\tau) \cdot \parentheses*{\grad_\vvarphi \altpi_\vvarphi(\tau)}^2}.
    \end{align*}
    Therefore, if $b^2 \leq 2 b \cdot r(\vx, \va)$ for every state $\vx \in \spX$ and action $\va \in \spA$, then the result follows from \cref{eq:variance_reduction}.
  \end{enumerate}
\end{solution}

\begin{solution}{score_gradients_state_dep_baselines}
  First, observe that \begin{align*}
    \E[\tau \sim \altpi_\vvarphi]{G_0 \grad_\vvarphi \log \altpi_\vvarphi(\tau)} = \E[\tau \sim \altpi_\vvarphi]{\sum_{t=0}^{T-1} G_0 \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}, \margintag{using \cref{eq:trajectory_distr}}
  \end{align*} and hence, it suffices to show \begin{align}\begin{split}
    &\E[\tau \sim \altpi_\vvarphi]{\sum_{t=0}^{T-1} G_0 \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)} \\
    &= \E[\tau \sim \altpi_\vvarphi]{\sum_{t=0}^{T-1} (G_0 - b(\tau_{0:t-1})) \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}.
  \end{split}\label{eq:score_gradient_estimator_state_dep_baseline_interm}\end{align}

  We prove \cref{eq:score_gradient_estimator_state_dep_baseline_interm} with an induction on $T$.
  The base case ($T = 0$) is satisfied trivially.
  Fixing any $T$ and assuming \cref{eq:score_gradient_estimator_state_dep_baseline_interm} holds for $T$, we have, \begin{align*}
    &\E[\tau \sim \altpi_\vvarphi]{\sum_{t=0}^{T} (G_0 - b(\tau_{0:t-1})) \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)} \\
    &= \E[\tau_{0:T-1}]{\E[\tau_{T}]{\sum_{t=0}^{T} (G_0 - b(\tau_{0:t-1})) \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}[\tau_{0:T-1}]} \margintag{using the tower rule \eqref{eq:tower_rule}} \\
    &= \begin{multlined}[t]
      \mathbb{E}_{\tau_{0:T-1}}\Bigg[\sum_{t=0}^{T-1} G_0 \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t) \\
      + \E[\tau_{T}]{(G_0 - b(\tau_{0:T-1})) \grad_\vvarphi \log \pi_\vvarphi(\va_{T} \mid \vx_{T})}[\tau_{0:T-1}]\Bigg].
    \end{multlined} \margintag{using the induction hypothesis} \\
    \intertext{Using the score function trick for the score function $\grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)$ analogously to the proof of \cref{lem:score_gradients_with_baselines}, we have, \begin{align*}
      &\E[\tau_{T}]{b(\tau_{0:T-1}) \grad_\vvarphi \log \pi_\vvarphi(\va_{T} \mid \vx_{T})}[\tau_{0:T-1}] \\[3pt]
      &= \E[\va_{T}]{b(\tau_{0:T-1}) \grad_\vvarphi \log \pi_\vvarphi(\va_{T} \mid \vx_{T})}[\tau_{0:T-1}] \\
      &= b(\tau_{0:T-1}) \int \pi_\vvarphi(\va_{T} \mid \vx_{T}) \grad_\vvarphi \log \pi_\vvarphi(\va_{T} \mid \vx_{T}) \,d\va_{T} \\
      &= b(\tau_{0:T-1}) \int \grad_\vvarphi \pi_\vvarphi(\va_{T} \mid \vx_{T}) \,d\va_{T} \margintag{using the score function trick, $\grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t) = \nicefrac{\grad_\vvarphi \pi_\vvarphi(\va_t \mid \vx_t)}{\pi_\vvarphi(\va_t \mid \vx_t)}$} \\
      &= 0.
    \end{align*}
    Thus,}
    &= \E[\tau_{0:T-1}]{\E[\tau_{T}]{\sum_{t=0}^{T} G_0 \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}[\tau_{0:T-1}]} \\
    &= \E[\tau \sim \altpi_\vvarphi]{\sum_{t=0}^{T} G_0 \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}. \margintag{using the tower rule \eqref{eq:tower_rule} again} \qedhere
  \end{align*}
\end{solution}

\begin{solution}{policy_gradients_with_downstream_returns}
  Each trajectory $\tau$ is described by four transitions, \begin{align*}
    \tau = (x_0, a_0, r_0, x_1, a_1, r_1, x_2, a_2, r_2, x_3, a_3, r_3, x_4).
  \end{align*}
  Moreover, we have given \begin{align*}
    \pi_\theta(2 \mid x) &= \theta, & \pdv{\pi_\theta(2 \mid x)}{\theta} &= +1 \\
    \pi_\theta(1 \mid x) &= 1 - \theta, & \pdv{\pi_\theta(1 \mid x)}{\theta} &= -1. \\
  \end{align*}

  We first compute the downstream returns for the given episode, \begin{align*}
    G_{0:4} &= r_0 + \gamma r_1 + \gamma^2 r_2 + \gamma^3 r^3 = 1 + \frac{1}{2} \cdot 0 + \frac{1}{4} \cdot 1 + \frac{1}{8} \cdot 1 = \frac{11}{8} \\
    G_{1:4} &= r_1 + \gamma r_2 + \gamma^2 r_3 = 0 + \frac{1}{2} \cdot 1 + \frac{1}{4} \cdot 1 = \frac{3}{4} \\
    G_{2:4} &= r_2 + \gamma r_3 = 1 + \frac{1}{2} \cdot 1 = \frac{3}{2} \\
    G_{3:4} &= r_3 = 1.
  \end{align*}
  Lastly, we can combine them to compute the policy gradient, \begin{align*}
    \grad_\theta j(\theta) &\approx \sum_{t=0}^3 \gamma^t G_{t:4} \grad_\theta \log\pi_\theta(a_t \mid x_t) \margintag{using Monte Carlo approximation of \cref{eq:score_gradient_estimator_downstream_returns} with a single sample} \\
    &= 1 \cdot \frac{11}{8} - 1 \cdot \frac{1}{2} \cdot \frac{3}{4} + 1 \cdot \frac{1}{4} \cdot \frac{3}{2} - 1 \cdot \frac{1}{8} \cdot 1 = \frac{5}{4}.
  \end{align*}
\end{solution}

\begin{solution}{policy_gradient_with_exponential_family}
  \begin{enumerate}[beginpenalty=10000]
    \item The distribution on actions is given by \begin{align*}
      \pi_\vvarphi(a \mid x) = \sigma(f_\vvarphi(x))^a \cdot (1 - \sigma(f_\vvarphi(x)))^{1-a}.
    \end{align*}
    To simplify the notation, we write $\mathbb{E}$ for $\mathbb{E}_{(\vx, a) \sim \pi_\vvarphi}$ and $\fnq$ for $\fnq[\pi_\vvarphi]$.
    We get \begin{align*}
      &\grad_\vvarphi j(\vvarphi) \\
      &= \E{\q{\vx}{a} \grad_\vvarphi (a \log \sigma(f_\vvarphi(\vx)) + (1-a) \log(1 - \sigma(f_\vvarphi(\vx))))} \margintag{using the policy gradient theorem \eqref{eq:policy_gradient_thm}} \\[5pt]
      &= \E{\q{\vx}{a} \grad_f (a \log \sigma(f) + (1-a) \log(1 - \sigma(f))) \grad_\vvarphi f_\vvarphi(\vx)} \margintag{using the chain rule} \\
      &= \E{\q{\vx}{a} \grad_f \parentheses*{-a \log(1 + e^{-f}) + (1-a) \log\parentheses*{\frac{e^{-f}}{1 + e^{-f}}}} \grad_\vvarphi f_\vvarphi(\vx)} \margintag{using the definition of the logistic function \eqref{eq:logistic_function}} \\
      &= \E{\q{\vx}{a} \grad_f \parentheses*{-f + a f - \log(1 + e^{-f})} \grad_\vvarphi f_\vvarphi(\vx)} \\
      &= \E{\q{\vx}{a} \parentheses*{a - 1 + \frac{e^{-f}}{1 + e^{-f}}} \grad_\vvarphi f_\vvarphi(\vx)} \\
      &= \E{\q{\vx}{a} \parentheses*{a - \sigma(f)} \grad_\vvarphi f_\vvarphi(\vx)}. \margintag{using the definition of the logistic function \eqref{eq:logistic_function}}
    \end{align*}
    The term $a - \sigma(f)$ can be understood as a residual as it corresponds to the difference between the target action $a$ and the expected action $\sigma(f)$.

    \item We have \begin{align*}
      \grad_\vvarphi j(\vvarphi) &= \E{\q{\vx}{a} \grad_f \log \pi_f(a \mid \vx) \grad_\vvarphi f(\vx)} \margintag{using \cref{eq:policy_gradient_thm} and the chain rule} \\
      &= \E{\q{\vx}{a} \grad_f (\log h(a) + a f - A(f)) \grad_\vvarphi f(\vx)} \\
      &= \E{\q{\vx}{a} (a - \grad_f A(f)) \grad_\vvarphi f(\vx)}.
    \end{align*}

    \item We have $\grad_f A(f) = \sigma(f)$.
    We are therefore looking for a function $A(f)$ whose derivative is $\sigma(f) = \frac{1}{1 + e^{-f}} = \frac{e^f}{1 + e^f}$.
    With this equality of the sigmoid we can find the integral, and we have $A(f) = \log(1 + e^f) + c$.
    Let us confirm that this gives us the Bernoulli distribution with $c = 0$: \begin{align*}
      \pi_f(a \mid \vx) &= h(a) \exp(a f - \log(1 + e^f)) \\
      &= h(a) \frac{e^{a f}}{1 + e^f} \\
      &= h(a) \begin{cases}
        \sigma(f) & \text{if $a = 1$} \\
        1 - \sigma(f) & \text{if $a = 0$}.
      \end{cases}
    \end{align*}
    This is the Bernoulli distribution with parameter $\sigma(f)$ where we have $h(a) = 1$.

    \item Using that $\grad_f A(f) = f$, we immediately get \begin{align*}
      \grad_\vvarphi j(\vvarphi) &= \E{\q{\vx}{a} (a - f) \grad_\vvarphi f(\vx)}.
    \end{align*}

    \item No, we cannot use the reparameterization trick since we do not know how the states $\vx$ depend on action $a$.
    These dependencies are determined by the unknown dynamics of the environment.
    Nonetheless, we can apply it after sampling an episode according to a policy and then updating policy parameters in hindsight.
    This is for example done by the soft actor-critic (SAC) algorithm (cf. \cref{sec:mfarl:actor_critic_methods:entropy_regularization}).
  \end{enumerate}
\end{solution}

\begin{solution}{soft_value_function}
  \begin{enumerate}[beginpenalty=10000]
    \item \Cref{eq:entropy_reg_rl2} can be written as \begin{align*}
      &\KL{\altpi_\vvarphi}{\altpi_\star} \\
      &= \sum_{t=1}^T \E[(\vx_t, \va_t) \sim \altpi_\vvarphi]{- \frac{1}{\lambda} r(\vx_t, \va_t) - \H{\pi_\vvarphi(\cdot \mid \vx_t)}} \\
      &= \sum_{t=1}^T \E[(\vx_t, \va_t) \sim \altpi_\vvarphi]{- \log \beta(\va_t \mid \vx_t) - \H{\pi_\vvarphi(\cdot \mid \vx_t)}}.
      \intertext{Adding and subtracting $\log Z(\vx_t)$ gives}
      &= \sum_{t=1}^T \E[(\vx_t, \va_t) \sim \altpi_\vvarphi]{\S{\hat{\pi}(\va_t \mid \vx_t)} - \log Z(\vx_t) - \H{\pi_\vvarphi(\cdot \mid \vx_t)}} \\
      &= \sum_{t=1}^T \E[\vx_t \sim \altpi_\vvarphi]{\crH{\pi_\vvarphi(\cdot \mid \vx_t)}{\hat{\pi}(\cdot \mid \vx_t)} - \log Z(\vx_t) - \H{\pi_\vvarphi(\cdot \mid \vx_t)}} \margintag{using the definition of cross-entropy \eqref{eq:cross_entropy}} \\
      &= \sum_{t=1}^T \E[\vx_t \sim \altpi_\vvarphi]{\KL{\pi_\vvarphi(\cdot \mid \vx_t)}{\hat{\pi}(\cdot \mid \vx_t)} - \log Z(\vx_t)}. \margintag{using the definition of KL-divergence \eqref{eq:kl}}
    \end{align*}

    \item We prove the statement by (reverse) induction on $t$.
    For the base case, note that the term \begin{align*}
      \E[\vx_T \sim \altpi_{\vvarphi}]{\KL{\pi_{\vvarphi}(\cdot \mid \vx_T)}{\hat{\pi}(\cdot \mid \vx_T)} - \log Z(\vx_T)}
    \end{align*} is minimized for $\pi_{\vvarphi} \equiv \hat{\pi}$.
    The KL-divergence then evaluates to zero, and we are left only with the $\log Z(\vx_T)$ term.

    For the inductive step, fix any $1 \leq t < T$ and $\pis(\va_t \mid \vx_t)$ must minimize the two terms \begin{align*}
      &\begin{multlined}[t]
        \E[\vx_t \sim \altpi_{\vvarphi}]{\KL{\pi_{\vvarphi}(\cdot \mid \vx_t)}{\hat{\pi}(\cdot \mid \vx_t)} - \log Z(\vx_t)} \\ + \E[(\vx_t, \va_t) \sim \altpi_{\vvarphi}]{\E[\vx_{t+1} \sim p(\cdot \mid \vx_t, \va_t)]{- \log Z(\vx_{t+1})}}
      \end{multlined}
      \intertext{where the first term stems directly from the objective \eqref{eq:entropy_reg_rl3} and the second term represents the contribution of $\pi_{\vvarphi}(\va_t \mid \vx_t)$ to all subsequent terms.
      Letting $\opt{\beta}(\va \mid \vx) \defeq \exp(\q*{\vx}{\va})$, $\opt{Z}(\vx) \defeq \int_\spA \opt{\beta}(\va \mid \vx) \, d \va$, and recalling that we denote by $\opt{\pi}(\cdot \mid \vx)$ the policy $\opt{\beta}(\cdot \mid \vx) / \opt{Z}(\vx)$, this objective can be reexpressed as}
      &= \E[\vx_t \sim \altpi_{\vvarphi}]{\KL{\pi_{\vvarphi}(\cdot \mid \vx_t)}{\opt{\pi}(\cdot \mid \vx_t)} - \log \opt{Z}(\vx_t)}
    \end{align*} which is minimized for $\pi_{\vvarphi} \equiv \opt{\pi}$, leaving only the $\log \opt{Z}(\vx_t)$ term.

    It remains only to observe that $\opt{\beta}(\va_T \mid \vx_T) = \beta(\va_T \mid \vx_T)$, so in the final state $\vx_T$, $\log \opt{Z}(\vx_T) = \log Z(\vx_T)$ and the policies $\pis$ and $\hat{\pi}$ coincide.
  \end{enumerate}
\end{solution}

\begin{solution}{ppo_as_probabilistic_inference}
  \begin{enumerate}[beginpenalty=10000]
    \item Let $\spO$ denote the event that the response $\vy$ is optimal.
    Since the prior over actions (i.e., responses) is not uniform, we have \begin{align}
      \altpi_\star(\vy \mid \vx) &= p(\vy \mid \vx, \spO) \nonumber \\
      &\propto p(\vy \mid \vx) \cdot p(\spO \mid \vx, \vy) \margintag{using Bayes' rule \eqref{eq:bayes_rule}} \nonumber \\
      &= \altpi_{\vvarphi^{\mathrm{init}}}(\vy \mid \vx) \exp\parentheses*{\frac{1}{\lambda} r(\vy \mid \vx)}. \label{eq:ppo_as_probabilistic_inference_helper}
    \end{align}
    The derivation is then analogous to \cref{eq:entropy_reg_rl2}: \begin{align*}
      &\argmin_\vvarphi \KL{\altpi_\vvarphi(\cdot \mid \vx)}{\altpi_\star(\cdot \mid \vx)} \\
      &= \argmin_\vvarphi \crH{\altpi_\vvarphi(\cdot \mid \vx)}{\altpi_\star(\cdot \mid \vx)} - \H{\altpi_\vvarphi(\cdot \mid \vx)} \margintag{using the definition of KL-divergence \eqref{eq:kl}} \\
      &= \argmax_\vvarphi \E[\vy \sim \altpi_\vvarphi(\cdot \mid \vx)]{\log \altpi_\star(\vy \mid \vx) - \log \altpi_\vvarphi(\vy \mid \vx)} \margintag{using the definition of cross-entropy \eqref{eq:cross_entropy} and entropy \eqref{eq:entropy}} \\
      &= \argmax_\vvarphi \E[\vy \sim \altpi_\vvarphi(\cdot \mid \vx)]{r(\vy \mid \vx) + \lambda \log \altpi_{\vvarphi^{\mathrm{init}}}(\vy \mid \vx) - \lambda \log \altpi_\vvarphi(\vy \mid \vx)} \margintag{using \cref{eq:ppo_as_probabilistic_inference_helper} and simplifying} \\
      &= \argmax_\vvarphi \j{\vvarphi; \vvarphi^{\init} \mid \vx}[\lambda]. \margintag{using the definition of cross-entropy \eqref{eq:cross_entropy} and entropy \eqref{eq:entropy}}
    \end{align*}

    \item Recall that the KL-divergence is minimized at $0$ if and only if the two distributions are identical.
    The desired result follows from (1) and \cref{eq:ppo_as_probabilistic_inference_helper}.
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:mbarl}}

\begin{solution}{hucrl_regret}
  \begin{enumerate}[beginpenalty=10000]
    \item To simplify the notation, we use $z_{t,k}$ as shorthand for $(x_{t,k}, \pi_t(x_{t,k}))$ (and similarly $\widehat{z}_{t,k}$ for $(\widehat{x}_{t,k}, \pi_t(\widehat{x}_{t,k}))$).
    The base case is implied trivially. For the induction step, assume that \cref{eq:hucrl_regret:induction} holds at iteration $k$.
    We have \begin{align*}
      \norm{\widehat{x}_{t,k+1} - x_{t,k+1}} &= \norm{\widehat{f}_{t-1}(\widehat{z}_{t,k}) - f(z_{t,k})} \\
      & \le \begin{multlined}[t]
        \norm{\widehat{f}_{n-1}(\widehat{z}_{t,k}) - f(\widehat{z}_{n,k})} + \norm{f(\widehat{z}_{t,k}) - f(z_{t,k})}
      \end{multlined} \margintag{adding and subtracting $f(\widehat{z}_{n,k})$ and using Cauchy-Schwarz} \\
      &\le 2 \beta_{t} \sigma_{t-1}(\widehat{z}_{t,k}) + L_1 \norm{\widehat{x}_{t,k} - x_{t,k}}
      \intertext{where the final inequality follows with high probability from the assumption that the confidence intervals are well-calibrated (cf. \cref{eq:plausible_gaussian_confidence_intervals}) and the assumed Lipschitzness.}
      &= \begin{multlined}[t]
        2 \beta_{t} \brackets*{\sigma_{t-1}(z_{t,k}) + \sigma_{t-1}(\widehat{z}_{t,k}) - \sigma_{t-1}(z_{t,k})} \\ + L_1 \norm{\widehat{x}_{t,k} - x_{t,k}}
      \end{multlined}
      \intertext{Once more using Lipschitz continuity, we obtain}
      &\leq \begin{multlined}[t]
        2 \beta_{t} \brackets*{\sigma_{t-1}(z_{t,k}) + L_2 \norm{\widehat{x}_{t,k} - x_{t,k}}} \\ + L_1 \norm{\widehat{x}_{t,k} - x_{t,k}}
      \end{multlined} \\
      &= 2 \beta_{t} \sigma_{t-1}(z_{t,k}) + \alpha_t \norm{\widehat{x}_{t,k} - x_{t,k}}
      \intertext{where $\alpha_t \defeq (L_1 + 2 \beta_t L_2)$. By the induction hypothesis,}
      &\le 2 \beta_t \sum_{l=0}^{k} \alpha_t^{k-l} \sigma_{t-1}(z_{t,l}).
    \end{align*}
    This is identical to the analysis of UCB from \cref{exercise:bayesian_regret_for_gp_ucb}, only that here errors compound along the trajectory.

    \item The assumption that $\alpha_t \geq 1$ implies that \begin{align}
      \norm{\widehat{x}_{t,k} - x_{t,k}} \leq 2 \beta_t \alpha_t^{H-1} \sum_{l=0}^{k-1} \sigma_{t-1}(z_{t,l}). \label{eq:hucrl_regret:induction_result_helper}
    \end{align}
    Moreover, by definition of $\pi_t$, we have with high probability that $J_H(\pi_t; \widehat{f}) \geq J_H(\pis; f)$.
    This is because $\pi_t$ maximizes reward under \emph{optimistic} dynamics.
    Thus, \begin{align*}
      r_t &= \sum_{k=0}^{H-1} r(x_{t,k}, \pis(x_{t,k})) - r(x_{t,k}, \pi_t(x_{t,k})) \\
      &\leq \sum_{k=0}^{H-1} r(\widehat{x}_{t,k}, \pi_t(\widehat{x}_{t,k})) - r(x_{t,k}, \pi_t(x_{t,k})) \\
      &\leq L_3 \sum_{k=0}^{H-1} \norm{\widehat{x}_{t,k} - x_{t,k}} \margintag{using Lipschitz-continuity of $r$} \\
      &\leq 2 \beta_t \alpha_t^{H-1} L_3 \sum_{k=0}^{H-1} \sum_{l=0}^{k-1} \sigma_{t-1}(z_{t,l}) \margintag{using \cref{eq:hucrl_regret:induction_result_helper}} \\
      &\leq 2 \beta_t H \alpha_t^{H-1} L_3 \sum_{k=0}^{H-1} \sigma_{t-1}(z_{t,k}).
    \end{align*}

    \item Let us first bound $R_T^2$.
    By the Cauchy-Schwarz inequality,
    \begin{align*}
        R_T^2 &\le T \sum_{t=1}^T r_t^2 \\
        &\le \BigO{T \sum_{t=1}^T \beta_{t}^2 H^2 \alpha_t^{2(H-1)} \parentheses*{\sum_{k=0}^{H-1} \sigma_{t-1}(z_{t,k})}^2} \margintag{using (2)} \\
        &\le \BigO{T \beta_{T}^2 H^3 \alpha_T^{2(H-1)} \sum_{t=1}^T \sum_{k=0}^{H-1} \sigma_{t-1}^2(z_{t,k})}. \margintag{using Cauchy-Schwarz and assuming w.l.o.g. that $\beta_t$ is monotonically increasing}
    \end{align*}
    Taking the square root, we obtain
    \begin{align*}
        R_T &\le \BigO{\beta_{T} H^{\frac{3}{2}} \alpha_T^{H-1} \sqrt{T \sum_{t=1}^T \sum_{k=0}^{H-1} \sigma_{t-1}^2(z_{t,k})}} \\
        &\le \BigO{\beta_{T} H^{\frac{3}{2}} \alpha_T^{H-1} \sqrt{T \Gamma_T}}.
    \end{align*}
  \end{enumerate}
\end{solution}

\section*{\nameref{sec:mathematical_background}}

\begin{solution}{sample_variance}
  Using that $X$ is zero-mean, we have that \begin{align*}
    \E{\mean{X}_n^2} &= \Var{\mean{X}_n} = \frac{1}{n} \Var{X} \qquad\text{and} \\
    \E{\frac{1}{n} \sum_{i=1}^n {X_i}^2} &= \frac{1}{n} \sum_{i=1}^n \E{{X_i}^2} = \Var{X}.
  \end{align*}
  Thus, \begin{align*}
    \E{S_n^2} = \frac{n}{n-1} \parentheses*{\E{\frac{1}{n} \sum_{i=1}^n {X_i}^2} - \E{\mean{X}_n^2}} = \Var{X}.
  \end{align*}
\end{solution}

\begin{solution}{concentration_inequalities}
  \begin{enumerate}[beginpenalty=10000]
    \item W.l.o.g. we assume that $X$ is continuous.
    We have \begin{align*}
      \E*{X} &= \int_0^\infty x \cdot p(x) \,d x \margintag{using the definition of expectation \eqref{eq:expectation}} \\
      &= \underbrace{\int_0^\epsilon x \cdot p(x) \,d x}_{\geq 0} + \int_\epsilon^\infty x \cdot p(x) \,d x \geq \epsilon \underbrace{\int_\epsilon^\infty \cdot p(x) \,d x}_{\Pr{X \geq \epsilon}}.
    \end{align*}

    \item Consider the non-negative random variable $Y \defeq (X - \E*{X})^2$.
    We have \begin{align*}
      \Pr{\abs{X - \E*{X}} \geq \epsilon} &= \Pr{(X - \E*{X})^2 \geq \epsilon^2} \\
      &\leq \frac{\E{(X - \E*{X})^2}}{\epsilon^2} \margintag{using Markov's inequality \eqref{eq:markov_inequality}} \\
      &= \frac{\Var*{X}}{\epsilon^2}. \margintag{using the definition of variance \eqref{eq:variance}}
    \end{align*}

    \item Fix any $\epsilon > 0$.
    Applying Chebyshev's inequality and noting that $\E*{\mean{X}_n} = \E*{X}$, we obtain \begin{align*}
      \Pr{\abs{\mean{X}_n - \E*{X}} \geq \epsilon} \leq \frac{\Var*{\mean{X}_n}}{\epsilon^2}.
    \end{align*}
    We further have for the variance of the sample mean that \begin{align*}
      \Var*{\mean{X}_n} = \Var{\frac{1}{n} \sum_{i=1}^n X_i} = \frac{1}{n^2} \sum_{i=1}^n \Var{X_i} = \frac{\Var*{X}}{n}.
    \end{align*}
    Thus, \begin{align*}
      \lim_{n\to\infty} \Pr{\abs{\mean{X}_n - \E*{X}} \geq \epsilon} \leq \lim_{n\to\infty} \frac{\Var*{X}}{\epsilon^2 n} = 0
    \end{align*} which is precisely the definition of $\mean{X}_n \convp \E*{X}$.
  \end{enumerate}
\end{solution}

\begin{solution}{directional_derivatives}
  Fix a $\lambda > 0$.
  Using a first-order expansion, we have \begin{align*}
    f(\vx + \lambda \vd) = f(\vx) + \lambda \transpose{\grad f(\vx)} \vd + o(\lambda \norm{\vd}_2).
  \end{align*}
  Dividing by $\lambda$ yields, \begin{align*}
    \frac{f(\vx + \lambda \vd) - f(\vx)}{\lambda} = \transpose{\grad f(\vx)} \vd + \underbrace{\frac{o(\lambda \norm{\vd}_2)}{\lambda}}_{\to 0}.
  \end{align*}
  Taking the limit $\lambda \to 0$ gives the desired result.
\end{solution}
