\section{\texttt{UCBVI}}

In this section, we consider the \ucbvi algorithm, introduced in~\citep{azar2017minimax}. First, we provide a more compact (but equivalent) pseudocode of the algorithm in Algorithm~\ref{alg:ucbvi}. 

We start by initializing the counters needed in order to run the algorithm. Then, we start the continuous interaction procedure for every episode $k\in\dsb{K}$. For every episode, before starting the interaction, the algorithm computes the optimistic estimate of the value function. Such estimate starts by computing all the transition probabilities $\Pest_k (y | x, a)$ for every state, action and next state as:
\begin{align}
    \Pest_k (y | x, a) = \frac{N_k(x, a, y)}{\max\{1,N_k(x, a)\}},
\end{align}
where $N_k(x, a)$ is the number of times we play action $a\in\As$ in state $x\in\Ss$, and $N_k(x, a, y)$ is the number of times we do so and observe the next state $y\in\Ss$. Then, we compute the optimistic value iteration for finite horizon MDPs, starting from stage $H$ backward, where the optimistic $Q_{k,h} (x,a)$ is defined as:
\begin{align*}
    Q_{k,h} (x,a) = \min \{ Q_{k-1,h} (x,a), R (x,a) + \! \sum_{y \in \mathcal{S}} \! \Pest_k (y | x, a) \Vest{k,h+1}(y) + b_{k,h} (x, a) \}.
\end{align*}
This procedure mimics value iteration with an additive exploration term $b_{k,h} (x, a)$ which will be further characterized later in Section~\ref{sec:analysis}. Then, term $V_{k,h} (x)$ is computed as usual for value iteration:
\begin{align*}
    \Vest{k,h}(x) = \max_{a \in \mathcal{A}} Q_{k,h}(x, a).
\end{align*}
After that, we can start interacting with the environment and play greedily. More in detail, at every stage $h\in\dsb{H}$ of episode $k\in\dsb{K}$, we observe state $x_{k,h}$ and play the most promising action according to the optimistic estimate:
\begin{align*}
a_{k,h} \in \argmax_{a \in \mathcal{A}} Q_{k,h} (x_{k,h}, a).
\end{align*}
After having played $a_{k,h}$, we observe the reward $r_{k,h}$ and the next state $x_{k,h+1}$. Finally, we use the collected information to properly update counters.

\RestyleAlgo{ruled}
\LinesNumbered
\begin{algorithm}[t!]
\caption{\ucbvi.}\label{alg:ucbvi}
{\small
\textbf{Initialize}: $N_0(x, a, y) = 0$, $N_0 (x, a) = 0$, $N'_{0,h} (x) = 0, \ \forall (x, a, y) \in \mathcal{S} \times \mathcal{A} \times \mathcal{S}$ \\ 
\phantom{\textbf{Initialize}:} $Q_{0,h} (x,a) = H - h + 1, \ \forall (x, a, h) \in \mathcal{S} \times \mathcal{A} \times \dsb{H}$

\For{$k \in \dsb{K}$}{

    \vspace{.1cm}

    \textcolor{black!40!white}{\texttt{// Update the optimistic estimates for episode} $k$}
    
    Estimate $\Pest_k (y | x, a) = N_k(x, a, y)/ \max\{1,N_k(x, a)\}$ \label{alg:ucbvi:pest}
    
    Initialize $\Vest{k,H+1}(x) = 0$, $\forall x \in \mathcal{S}$
    
    \For{$h = \{ H, H-1, \ldots, 1 \}$}{
    
        \For{$x \in \mathcal{S}$}{

            \For{$a \in \mathcal{A}$}{
                $Q_{k,h} (x,a) = \min \{ Q_{k-1,h} (x,a), R (x,a) + \! \sum_{y \in \mathcal{S}} \! \Pest_k (y | x, a) \Vest{k,h+1}(y) + b_{k,h} (x, a) \}$ \label{alg:ucbvi:qvals}
            }
    
            $\Vest{k,h}(x) = \max_{a \in \mathcal{A}} Q_{k,h}(x, a)$ \label{alg:ucbvi:vvals}
                        
        }
    }

    \textcolor{black!40!white}{\texttt{// Interact with the environment for episode} $k$}
    
    Agent observes state $x_{k,1}$
    
    \For{$h \in \dsb{H}$}{

        Agent plays action $a_{k,h} \in \argmax_{a \in \mathcal{A}} Q_{k,h} (x_{k,h}, a)$ \label{alg:ucbvi:play}

        Environment returns reward $r_{k,h}$ and next state $x_{k,h+1}$ \label{alg:ucbvi:observe}

        Update for every $(x, a, y) \in \mathcal{S} \times \mathcal{A} \times \mathcal{S}$: \label{alg:ucbvi:updatefirst}

        \phantom{Update} $N_k(x, a, y) = N_{k-1}(x, a, y) + \mathds{1}\{x=x_{k,h}, a=a_{k,h}, y = x_{k,h+1}\}$ 

        \phantom{Update} $N_k(x, a) = N_{k-1}(x, a) + \mathds{1}\{x=x_{k,h}, a=a_{k,h}\}$

        \phantom{Update} $N_{k,h}'(x) = N_{k-1,h}'(x) + \mathds{1}\{x=x_{k,h}\}$ \label{alg:ucbvi:updatelast}
    }
}
}
\end{algorithm}
