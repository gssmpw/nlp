\section{Numerical Validation}
\label{sec:experiments}

In this section, we numerically compare the performances of \ucbvi, both with the Chernoff-Hoeffding and Bernstein-Freedman bonuses of~\citep{azar2017minimax} and with the improved Bernstein-Freedman bonus of this paper, against the \mvp algorithm.

In order to fairly compare to the \mvp algorithm, all the $N_h(x,a)$ terms are considered as $N(x,a)$, removing the discriminant of the stage from the algorithm, and the $c_2$ constant (which refers to the uncertainty in the estimation of the rewards) is set to $0$, to remove the exploration factor needed due to the stochasticity of the reward in the original paper.

\subsection{Illustrative Environments}

As a first experimental evaluation, we consider a set of illustrative environments. We consider an MDP with parameters $S=3$, $A=3$, $H \in \{5, 10\}$, and we consider a number of episodes $K \in \{10^5, 10^6\}$. We evaluate each experiment by averaging over 10 runs. In each run, the rewards and transition probabilities of the MDP are randomly generated. Then, the clairvoyant optimum is calculated for the purpose of regret computation, and the algorithms are evaluated.

\begin{figure*}[t!]
    \centering

    \hspace{0.8cm}
    \subfloat[$H=5, \ K=10^5$.]{\resizebox{0.37\linewidth}{!}{\includegraphics{img/h5k1e5_standalone.pdf}}}
    \hfill
    \subfloat[$H=5, \ K=10^6$.]{\resizebox{0.37\linewidth}{!}{\includegraphics{img/h5k1e6_standalone.pdf}}}
    \hspace{1cm}

    \vspace{0.3cm}

    \hspace{0.8cm}
    \subfloat[$H=10, \ K=10^5$.]{\resizebox{0.37\linewidth}{!}{\includegraphics{img/h10k1e5_standalone.pdf}}}
    \hfill
    \subfloat[$H=10, \ K=10^6$.]{\resizebox{0.37\linewidth}{!}{\includegraphics{img/h10k1e6_standalone.pdf}}}
    \hspace{1cm}
    
    \caption{Performances in terms of cumulative regret in toy environments with $S=3$ states and $A=3$ actions ($10$ runs, mean $\pm$ $95\%$ C.I.).}
    \label{fig:TODO2}
\end{figure*}


% \begin{figure}[t!]
% \centering

% %\hspace{0.6cm}
% \begin{subfigure}[b]{0.37\textwidth}
% \resizebox{.4\linewidth}{!}{\input{img/h5k1e5}}
% \caption{$H=5, \ K=10^5$.}
% \end{subfigure}
% %\hfill
% \begin{subfigure}[b]{0.37\textwidth}
% \resizebox{.4\linewidth}{!}{\input{img/h5k1e6}}
% \caption{$H=5, \ K=10^6$.}
% \end{subfigure}
% %\hspace{1cm}


% %\hspace{0.6cm}
% \begin{subfigure}[b]{0.37\textwidth}
% \resizebox{.4\linewidth}{!}{\input{img/h10k1e5}}
% \caption{$H=10, \ K=10^5$.}
% \end{subfigure}
% %\hfill
% \begin{subfigure}[b]{0.37\textwidth}
% \resizebox{.4\linewidth}{!}{\input{img/h10k1e6}}
% \caption{$H=10, \ K=10^6$.}
% \end{subfigure}
% %\hspace{1cm}
% \caption{Performances in terms of cumulative regret in toy environments with $S=3$ states and $A=3$ actions ($10$ runs, mean $\pm$ $95\%$ C.I.).}
% \label{fig:TODO2}
% \end{figure}

\paragraph{Results.}
Figure~\ref{fig:TODO2} represents the cumulative regret of the evaluated algorithms in the first experimental evaluation for different values of $H$ and $K$. From the figures, we can observe that \ucbvi with the Chernoff-Hoeffding bonus and \mvp begin to show a sub-linear regret for $K=10^6$, whereas both versions of \ucbvi with the Bernstein-Freedman bonus greatly outperform the other algorithms in all the evaluated scenarios. In particular, the use of a tighter Bernstein-Freedman bonus (\ucbvibfi) translates into a cumulative regret that is, although of the same order, lower than with the usage of a larger bonus (\ucbvibf), highlighting the importance of lower order terms and constants in empirical performance.

\subsection{RiverSwim}
We now consider the RiverSwim environment~\citep{strehl2008analysis}. This environment emulates a swimmer that has to swim against the current, where the agent has 2 options: \emph{(i)} to try to swim to the other side or \emph{(ii)} to turn back. In this scenario, the rewards and the transition probabilities are designed such that the optimal policy corresponds to trying to swim and reach the other side of the \quotes{river}. This is considered a challenging benchmark for exploration. We consider the scenario with $S=5$ and $H=10$. The reward model and the transition probability are designed such that the suboptimality gap between the optimal action and the other one in the initial state is very low ($\sim\!0.1$, with a scale of the problem in the order of $H=10$).

\paragraph{Results.}
Figure~\ref{fig:river} compares the results when using \mvp and \ucbvi in its original version (\ucbvibf) and the one we propose with tighter bounds (\ucbvibfi). \mvp confirms its poor empirical performance, failing to deliver a sublinear trend for the considered horizon. Instead, \ucbvi, in both versions, shows a clear sublinear trend, with the improved version (\ucbvibfi) with a cumulative regret approximately half of the original one (\ucbvibf).

\begin{figure}[t!]
\centering
\resizebox{0.37\linewidth}{!}{\includegraphics{img/riverswim_s5h10_standalone.pdf}}
\caption{Performances in terms of cumulative regret in the RiverSwim environment with $S=5$ states and horizon $H=10$ ($4$ runs, mean $\pm$ $95\%$ C.I.).}
\label{fig:river}
\end{figure}

\begin{table}[t!]
    \centering
    \begin{tabular}{c|ccc}
        \hline 
        & Bonus ratio & Regret upper bound ratio &  Empirical regret ratio\\ \hline
        \texttt{CH} & $7/2$ & $10$ & - \\ \hline
        \texttt{BF} & $\sqrt{2}$ & $5/4$ & $1.87 \pm 0.03$ \\ \hline
    \end{tabular}
    \caption{Improvement ratios in the bonuses, regret upper bounds, and empirical regret between our analysis and the original of~\citep{azar2017minimax}.}
    \label{tab:ratios}
\end{table}

