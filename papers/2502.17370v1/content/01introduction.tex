\section{Introduction}
\label{sec:intro}

We focus on the problem of \emph{finite-horizon tabular RL}, where the statistical complexity is characterized by a well-established lower bound of the order $\Omega ( \sqrt{HSAT} )$, where $S$ is the number of states, $A$ is the number of actions, $H$ is the horizon of the episode, and $T=HK$ where $K$ is the number of episodes~\citep{domingues2021episodic}. State-of-the-art learning algorithms match the lower bound up to logarithmic factors. \texttt{UCBVI}~\citep{azar2017minimax} matches this lower bound by combining the classical value iteration approach with the \emph{optimism in the face of uncertainty} mechanism, achieving $\widetilde{\BigO}(\sqrt{HSAT})$ under the condition $T \ge \Omega (H^3 S^3 A)$. Recently, \texttt{MVP}~\citep{zhang2024settling} overcome this limitation by employing the so-called \emph{doubling trick}, ensuring order-optimal regret (up to logarithmic factors) for every time horizon. However, this improvement comes at the expense of significantly larger constant factors, which may imply poor empirical performances (see Section~\ref{sec:experiments}). Consequently, \ucbvi remains a competitive and practical solution for finite-horizon tabular RL.

In this work, we improve both the bound and the analysis of the \ucbvi algorithm, with particular attention to its advanced form with Bernstein-Freedman optimistic bonus. Our goal is to design an exploration bonus that is as tight as possible and to conduct a regret upper bound analysis that minimizes also constants. 