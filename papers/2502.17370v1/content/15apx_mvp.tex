\section{Adaptation of Monotonic Value Propagation}
\label{apx:mvp}
In this appendix, we present the pseudocode for \mvp~\citep{zhang2023settling,zhang2024settling} adapted to handle the scenario of stage-independent transitions and known deterministic rewards. The pseudocode is provide in Algorithm~\ref{alg:mvp}.

\RestyleAlgo{ruled}
\LinesNumbered
\begin{algorithm}[h!]
\caption{\mvp with Stage-independent Transitions and Known Rewards}\label{alg:mvp}
\SetKwInOut{Input}{Input}

\textbf{Initialize}: $N(s, a, s') = 0$, $N(s, a) = 0$, $N_{k,h}' (s, a) = 0, \ \forall (s, a, s') \in \mathcal{S} \times \mathcal{A} \times \mathcal{S}$ \\ 
\phantom{\textbf{Initialize}:} $Q_{h} (s,a) = H, \ \forall (s, a, h) \in \mathcal{S} \times \mathcal{A} \times \dsb{H}$


\For{$k \in \dsb{K}$}{

    \texttt{// Update the optimistic estimates for episode} $k$
    
    Estimate $\hat{P}_k (s' | s, a) = N_k(s, a, s')/N_k(s, a)$
    
    Initialize $V_{k, H+1} (s) = 0$, $\forall s \in \mathcal{S}$
    
    \For{$h = \{ H, H-1, \ldots, 1 \}$}{
    
        \For{$s \in \mathcal{S}$}{

            \For{$a \in \mathcal{A}$}{
                $Q_{k,h} (s,a) = \min ( Q_{k,h}^{\text{\texttt{OLD}}} (s,a), R_h (s,a) + \sum_{s' \in \mathcal{S}} \hat{P}_{k,h}(s' | s, a) V_{k, h+1}(s') + b_{k,h} (s, a) )$
            }
    
            $V_{k,h}(s) = \max_{a \in \mathcal{A}} Q_{k,h}(s, a)$ 
                        
        }
    }

    \texttt{// Interact with the environment for episode} $k$
    
    Agent observes state $s_{k,1}$
    
    \For{$h \in \dsb{H}$}{

        Agent plays action $a_{k,h} \in \argmax_{a \in \mathcal{A}} Q_{k,h} (s_{k,h}, a)$

        Environment returns reward $r_{k,h}$ and next state $s_{k,h+1}$

        Update $N_k(s_{k,h}, a_{k,h}, s_{k,h+1}) = N_k(s_{k,h}, a_{k,h}, s_{k,h+1}) + 1$  \\
        \phantom{Update} $N_k(s_{k,h}, a_{k,h}) = N_k(s_{k,h}, a_{k,h}) + 1$ \\
        \phantom{Update} $N_{k,h}'(s_{k,h}, a_{k,h}) = N_{k,h}'(s_{k,h}, a_{k,h}) + 1$
    }
}
\end{algorithm}