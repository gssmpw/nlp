\section{Proof of Theorem~\ref{thr:ucbvichUB}}
\label{apx:proof_CH}

\ucbvichUB*

We begin the proof by demonstrating optimism under the \ucbvich algorithm (\ie every optimistic value function is an upper bound of the true optimal value function), which requires us to show that, with high probability, the event $\Omega \coloneqq \{ \Vest{k,h}(x) \ge \Vstar{h}(x), \forall k \in \dsb{K}, h \in \dsb{H}, x \in \Ss \}$ holds.

\begin{lemma}[Optimism under Chernoff-Hoeffding bonus]
\label{lem:CH_opt}
    Let the optimistic bonus be defined as:
    \begin{equation*}
        \bonus_{k,h}(x,a) = \frac{2HL}{\sqrt{N_k(x,a)}}.
    \end{equation*}
    Then, under event $\mathcal{E}$, the following event holds:

    \begin{equation*}
        \Omega \coloneqq \{ \Vest{k,h}(x) \ge \Vstar{h}(x), \forall k \in \dsb{K}, h \in \dsb{H}, x \in \Ss \}.
    \end{equation*}
\end{lemma}

\begin{proof}
    We demonstrate the result by induction. Let $\Vest{k,h}$ be the optimistic value function at stage $h$ computed using the history up to the end of episode $k-1$, and let $\Vstar{h}$ be the true optimal value function at stage $h$.

    By definition, $\Vest{k,H+1}(x) = \Vstar{H+1}(x) = 0$ for every $x \in \Ss$, and thus the inequality $\Vest{k,H+1} \ge \Vstar{H+1}$ trivially holds. To prove the inductive step, we need to demonstrate that, if $\Vest{k, h+1} \ge \Vstar{h+1}$ holds, then it also holds that $\Vest{k,h} \ge \Vstar{h}$. We can derive this result as follows:

    \begin{align}
        \Vest{k,h}(x) - \Vstar{h} &= \max_{a\in\As} \Qest{k,h}(x,a) - \Vstar{h}(x) \nonumber \\
        &\ge \Qest{k,h}(x, a_{k,h}^{\pi^*}) - \Vstar{h}(x) \nonumber \\
        &= \sum_{y\in\Ss} \Pest_{k}(y | x, a_{k,h}^{\pi^*})\Vest{k,h+1}(y) + \bonus_{k,h}(x, a_{k,h}^{\pi^*}) - \sum_{y\in\Ss} P(y | x, a_{k,h}^{\pi^*})\Vstar{h+1}(y) \nonumber \\
        &\ge \sum_{y \in \Ss} \left[ \Pest_{k}(y | x, a_{k,h}^{\pi^*}) - P(y | x, a_{k,h}^{\pi^*})\right] \Vstar{h+1}(y)  + \bonus_{k,h}(x, a_{k,h}^{\pi^*}) \label{lem:CH_opt:1} \\
        &\ge \bonus_{k,h}(x, a_{k,h}^{\pi^*}) - 2\sqrt{\frac{H^2 L}{N_k(x_{k,h}, a_{k,h}^{\pi^*})}} \label{lem:CH_opt:2} \\
        &\ge 2\frac{H L}{\sqrt{N_k (x_{k,h}, a_{k,h}^{\pi^*})}} - 2\sqrt{\frac{H^2 L}{N_k (x_{k,h}, a_{k,h}^{\pi^*})}} \nonumber \\
        &\ge 0, \nonumber
    \end{align}

    where Equation~\eqref{lem:CH_opt:1} follows by the inductive hypothesis, and Equation~\eqref{lem:CH_opt:2} is obtained because, under $\mathcal{E}$, we can bound $|\Pest_{k}(y | x, a_{k,h}^{\pi^*}) - P(y | x, a_{k,h}^{\pi^*}) \Vstar{h+1}(y)|$ by applying Azuma-Hoeffding's inequality, allowing us to simplify terms and show optimism.
\end{proof}

Our objective is to bound the regret after $K$ episodes (\ie $\regch{K}$). We can observe that, under event $\Omega$, it holds that:

\begin{align*}
    \regch{K} &= \sum_{k \in \dsb{K}} \Vstar{1}(x_{k,1}) - \Vpi{1}(x_{k,1}) \\
    &\le \sum_{k \in \dsb{K}} \Vest{k,1}(x_{k,1}) - \Vpi{1}(x_{k,1}) \\
    &= \sum_{k \in \dsb{K}} \Vdifftil{k,1}(x_{k,1}) \\
    &= \regchtilde{K}.
\end{align*}

As such, we can now focus on finding an upper bound to $\regchtilde{K}$. By applying Lemma~\ref{lem:regr_dec}, we can write:

\begin{align}
    \regchtilde{K} &= \sum_{k \in \dsb{K}} \Vdifftil{k,1}(x_{k,1}) \nonumber \\
    \begin{split}
    \label{lem:CH_opt:3}
    &\le e \sum_{i=1}^K\sum_{j=1}^{H-1} \bigg[ \varepsilon_{i,j} + 2 \sqrt{L} \overline{\varepsilon}_{i,j} + \bonus_{i,j}(x_{i,j}, a_{i,j}^{\pi_i}) \\
    &\quad + \xi_{i,j} (x_{i,j}, a_{i,j}^{\pi_i}) + \frac{8H^2SL}{3 N_i (x_{i,j}, a_{i,j}^{\pi_i})} \bigg].
    \end{split}
\end{align}

To find an upper bound to the regret, we can thus bound the summation of each of the terms individually.

By applying Lemma~\ref{lem:sum_eps}, we obtain the following bounds:

\begin{align*}
    \sum_{i=1}^K \sum_{j=1}^{H-1} \varepsilon_{i,j} &\le 2 \sqrt{H^2 T L}, \\
    \sum_{i=1}^K \sum_{j=1}^{H-1} 2 \sqrt{L} \overline{\varepsilon}_{i,j} &\le 4 \sqrt{T L}. \\
\end{align*}

Then, we can derive the following bound:

\begin{align}
    \sum_{i=1}^K \sum_{j=1}^H \frac{8H^2SL}{3N_i(x_{i,j},a_{i,j}^{\pi_i})} &= \frac{8}{3}H^2SL \sum_{x \in \Ss} \sum_{a \in \As} \sum_{n=1}^{N_K(x,a)} n^{-1} \label{lem:CH_opt:5} \\
    &\le \frac{8}{3}H^2SL \sum_{x \in \Ss} \sum_{a \in \As} \sum_{n=1}^{\frac{KH}{SA}} n^{-1} \label{lem:CH_opt:6} \\
    &\le \frac{8}{3} H^2 S^2 A L^2 \nonumber
\end{align}

where Equation~\eqref{lem:CH_opt:5} is obtained by rearranging the terms to isolate the summation of $n^{-1}$ for $n$ from $1$ to $N_K(x,a)$ (\ie the total number of times each state-action pair has been observed up to the end of episode $K$), and Equation~\eqref{lem:CH_opt:6} derives from the observation that the summation can be upper bounded by considering a uniform state-action visit distribution. This derivation produces the same result as applying the well-known pigeonhole principle.

By applying a similar reasoning, we bound the remaining summations over the bonus terms:

\begin{align}
    \sum_{i=1}^K \sum_{j=1}^H \bonus_{i,j}(x_{i,j}) &= \sum_{i=1}^K \sum_{j=1}^H 2H \sqrt{\frac{L}{N_i (x_{i,j}, a_{i,j}^{\pi_i})}} \nonumber \\
    &= 2H \sqrt{L} \sum_{x\in\Ss}\sum_{a\in\As} \sum_{n=1}^{N_K(x,a)} n^{-1/2} \nonumber \\
    &\le 2H \sqrt{L} \sum_{x\in\Ss}\sum_{a\in\As} \sum_{n=1}^{\frac{KH}{SA}} n^{-1/2} \nonumber \\
    &\le 2 \sqrt{H^2 S A T L}, \nonumber
\end{align}

and over the model error terms:

\begin{align}
    \sum_{i=1}^K \sum_{j=1}^H \xi_{i,j} (x_{i,j}, a_{i,j}^{\pi_i}) &\le \sum_{i=1}^K \sum_{j=1}^H 2 H \sqrt{\frac{L}{N_i (x_{i,j}, a_{i,j}^{\pi_i})}} \label{lem:CH_opt:4} \\
    &= 2 H \sqrt{L} \sum_{x\in\Ss}\sum_{a\in\As} \sum_{n=1}^{N_K(x,a)} n^{-1/2} \nonumber \\
    &\le 2H \sqrt{L} \sum_{x\in\Ss}\sum_{a\in\As} \sum_{n=1}^{\frac{KH}{SA}} n^{-1/2} \nonumber \\
    &\le 2 \sqrt{H^2 S A T L}, \nonumber
\end{align}

where Equation~\eqref{lem:CH_opt:4} is obtained by bounding $\xi_{i,j} (x_{i,j}, a_{i,j}^{\pi_i})$ using the Chernoff-Hoeffding inequality.
Finally, we can put all the bounds together and rewrite Equation~\eqref{lem:CH_opt:3} as:

\begin{align*}
    \regchtilde{K} &\le e \left[ 2 \sqrt{H^2 T L} + 4 \sqrt{TL} + 2 \sqrt{H^2 S A T L} + 2 \sqrt{H^2 S A T L} + \frac{8}{3} H^2 S^2 A L^2 \right] \\
    &\le e \left[ 10 \sqrt{H^2 S A T L} + \frac{8}{3} H^2 S^2 A L^2 \right],
\end{align*}

thus completing the proof.