\section{Setting}
\label{sec:preliminaries}

In this section, we introduce the notation and the setting we consider in the rest of the work.

\paragraph{Notation.}
Given a measurable set $\mathcal{X}$, we denote with $\Delta(\mathcal{X})$ the set of probability measures over $\mathcal{X}$, and with $|\mathcal{X}|$ it cardinality. For $n \in \mathbb{N}$, we denote the set $\{1, \ldots, n\}$ as $\dsb{n}$. We denote the L1 norm of a vector as $\| \cdot \|_1$. We denote the indicator function of event $x$ as $\mathbb{I}\{x\}$.

\paragraph{Markov Decision Processes.}
An undiscounted, episodic Markov Decision Process \citep[MDP, ][]{puterman1990} is a tuple $\mathcal{M} \coloneqq (\Ss, \As, P, R, H)$. In this tuple, $\Ss$ is the state space, $\As$ is the action space, $P : \Ss \times \As \to \Delta(\Ss)$ represents the state transition probability, $R : \Ss \times \As \to \mathbb{R}$ represents the reward function, and $H \in \mathbb{N}_{\geq 1}$ is the length of each episode.\footnote{Let $x, y \in \Ss$ and $a \in \As$, we denote as $P(y|x,a)$ the probability of observing $y$ as the next state after playing action $a$ in state $x$, and $R(x,a)$ the reward obtained after playing action $a$ in state $x$.}

We assume the state space and the action space are finite sets, and we denote their cardinalities as $|\Ss| = S < + \infty$ and $|\As| = A < + \infty$. We assume the state transition probability and the reward do not depend on the stage. Moreover, we assume the reward to be deterministic, known, and bounded in $[0,1]$.

\paragraph{Interaction with the Environment.}
The agent interacts with the environment in a sequence of $K$ episodes. Denote as $x_{k,h}$ the state occupied by the agent at stage $h \in \dsb{H}$ of episode $k \in \dsb{K}$, and as $a_{k,h}^{\pi_k}$ the action played by the agent at stage $h$ of episode $k$ according to the policy $\pi_k$. We assume policies to be deterministic and stage-dependent, \ie $\pi : \Ss \times \dsb{H} \to \As$.

The interaction of the $k$-th episode is defined as follows. Starting from state $x_{k,1} \in \Ss$, the agent selects which action to play as $a_{k,h}^{\pi_k} = \pi_k (x_{k,h}, h)$ for every $h \in \dsb{H}$, and observes a sequence of next-states and rewards, until the end of the episode.

The function $\Vpi[]{h} : \Ss \to \mathbb{R}$ denotes the value function at stage $h \in \dsb{H}$, such that $\Vpi[]{h}(x)$ represents the expected sum of the $H-h$ returns received under policy $\pi$ starting from state $x\in\Ss$. Under the assumptions stated above, there exists a deterministic policy $\pi^*$ which attains the best possible value function $\Vstar{h}(x) \coloneqq \sup_{\pi} \Vpi[]{h}(x)$ for every state $x \in \Ss$.
We measure the performance of a learning algorithm $\mathfrak{A}$ after $K$ episodes by means of the \emph{cumulative regret}:
\begin{equation*}
    \textnormal{Reg}(\mathfrak{A}, K) \coloneqq \sum_{i=1}^K \Vstar{1}(x_{i,1}) - \Vpi[i]{1}(x_{i,1}).
\end{equation*}
We denote as $T=KH$ the total number of interactions.