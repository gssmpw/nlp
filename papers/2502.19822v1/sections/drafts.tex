// RELATED WORK

% beyond the small handful of prior work on AI in mental health and ADS risk-assessment systems. While AI tools have quickly become widely adopted in many industries, they can be considered a relative novelty in social work, where AI has begun to gain traction albeit at a sluggish pace \cite{lehtiniemi_contextual_2023, molala_social_2023}. Due to the nascence of AI technologies and the social work sector’s lag in adopting such systems, there is thus a latent gap in the field of applied AI in social work practices \cite{wykman_artificial_2023}, precipitating a need for further exploration. 

% Discuss broadly the background of AI in social work. Machine-assisted decision making can be a subset of this.

% Digital systems that aid social work decision-making generally take the form of predictive risk modelling (PRMs). These are statistical models that predict risks of various outcomes, most notably child abuse or welfare (e.g. \cite{gillingham2019can, van2017predicting}). There is much discussion over the risks involved with using such tools \cite{oak2016minority, drake2020practical}. In a field where decisions are not merely a product of emprical factors \cite{oak2016minority}, decision-aid tools oversimplify cases \cite{fitzgibbon2007risk, broadhurst2010risk} , fail to accurately weigh qualitative factors \cite{shlonsky2005next}, and create ethical dilemmas when tool and worker differ in judgement \cite{dare2012vulnerable}. Their use may also erode practitioners' critical thinking and professional judgement skills \cite{littlechild2008child} and relational competencies in interacting with clients \cite{oak2016minority}. 

% Further, present AI systems often rely on generic models \cite{fitzpatrick2017delivering} and data \cite{gillingham2019can, van2017predicting} that omit specific cultural and individual needs, such as societal norms, localized stigmas, and the unique resources available within one’s social network \cite{rooney2017direct, zhang2021designing}. These factors are crucial to a complete and holistic understanding of a client; existing tools are therefore unable to fully meet the needs of social workers, in providing adequately precise and accurate judgements about their clients.

% The emergence of new AI technologies such as LLMs perpetuate and even exacerbate these problems. By providing more human-like outputs and explanations, they may create a false sense of knowledge and correctness \textit{[cite CASA/related stuff here]}. The new widespread accessibility of such technologies compels a thorough investigation into how they affect the social work profession. At the same time, the opportunities afforded by such technological advancements warrants their use, and therefore an understanding of how they may be implemented responsibly and sustainably.

% The presentation of a singular number can give a sense of false confidence in users. Simply knowing that there is a high statistical risk of something happening to a client does not help them make a clearer judgement. 

% \subsection{AI Assistant Tools?}

% Process-oriented design?

% Why are there some things we did not catch in multiple earlier rounds of PD?

% \subsection{Understanding Use of AI in the Social Sector}
% \label{sec:relatedunderstanding}

% Despite the immense potential of AI systems to augment social work practice, past work has found multiple challenges with integrating such systems into real-life practice. Numerous studies have investigated the use of PRMs to help practitioners decide on a course of action for their clients. Practitioners often experience tension trying to weigh the system's suggestions against their own judgement \cite{kawakami2022improving, saxena2021framework}, being uncertain of how far they should rely on the machine. Despite often being instructed to use the tool as part of evaluating a client, practitioners remain reluctant to fully embrace AI due to its inability to adequately account for the full context of a case \cite{kawakami2022improving, gambrill2001need}, as well as practitioner uncertainty of how the system functioned and what its limitations were \cite{kawakami2022improving}. Brown et al. \cite{brown2019toward} conducted workshops with hypothetical algorithmic tools to understand service providers' comfort levels with using such tools in their work, and found similar issues with mistrust and perceived unreliability. In one instance, the introduction of AI even created new problems of its own, causing contention between confused participants over the inner workings of the tool and how much it should be trusted \cite{kawakami2022improving}. Such factors are critical barriers to the acceptance and effective use of AI in the sector.

% Other studies find a more positive outlook. De-Arteaga et al. \cite{de2020case} investigated how workers responded to a technical glitch resulting in incorrect AI outputs. They found workers "able to appropriately override the tool in many such cases" and make appropriate judgements of their own accord. They reference the spectrum of algorithm aversion and algorithm bias \cite{jones2023people}, finding workers in their study to fall somewhere in the middle of the spectrum.

% \subsection{Understanding Use of AI in Healthcare}
% \label{sec:relatedhealthcare}

% The healthcare sector shares crucial similarities to the social work sector, particularly that 1) practitioners need to make critical decisions that significantly impact a patient's or client's well-being and 2) that practitioners rely on their expert training, judgement, and experience to make the best decision. In healthcare, Decision Support Tools (DST) and their effectiveness in the field are commonly researched in HCI. Past work suggests that many DSTs, once implemented, are rarely used by practitioners, due to a range of factors: a lack of practitioner confidence in system accuracy and disruptions to regular workflows \cite{elwyn2013many}, and lack of agreement with and negative attitudes towards the system \cite{devaraj2014barriers}. Yang et al. \cite{yang2019unremarkable} suggest these challenges can be overcome with the notion of "fairly unremarkable" design: a system that can "be easily passed over when it agrees with [the clinician's] current decision", and "naturally augment the current activities of decision making, rather than pulling clinicians away from doing their routine."

% Many of the issues impeding adoption of clinical DSTs also exist with social work PRMs, specifically that practitioners are unsure of whether, when, and where they should adopt AI recommendations into their work. With the changing landscape of accessible and ubiquitous AI technologies, we re-visit these questions to better understand the challenges and opportunities of modern AI use.

% Find similar applications to ours, but in healthcare
% AI diagnosis - trusted by doctors and patients

% The most commonly-studied tool is the Allegheny Family Screening Tool (AFST) \cite{vaithianathan2017developing}. This outputs a numerical score indicating risk of child rehoming given some parameters input by a human worker. 
% In \citeauthor{kawakami2022improving} study, while workers were expected to use this score as part of their final evaluation of the client, value misalignments and the limitations of the tool's contextual understanding caused workers to rely little on the given score, and instead seek to adjust it according to other factors of the case that the tool does not account for. Further complicating matters, "it was unclear to workers how exactly they were expected to take the AFST’s assessments of long-term risk into account in ways that complemented their own judgment." The inner workings of the tool also became a point of contention. Workers, with their lack of knowledge of the inner workings of the AFST, ended up frequently discussing how the tool arrived at certain judgements, "improvising ways to learn more about the tool themselves". The introduction of this system created a new level of meta-discussion, causing confusion counter-productive to the actual work of assessing and helping clients. Studying the same tool, De-Arteaga et al. \cite{de2020case} investigated how workers responded to a technical glitch in the system, resulting in a subset of its outputs being incorrect. They present a more positive outlook, finding workers "able to appropriately override the tool in many such cases" and make appropriate judgements of their own accord. They reference the spectrum of algorithm aversion and algorithm bias \cite{jones2023people}, finding workers in their study to fall somewhere in the middle of the spectrum. Similar issues plagued other efforts to integrate predictive analytic into child welfare systems \cite{brown2019toward}, with perceptions of unreliable predictions and the uncertainty of how the tool functioned resulting in the termination of two active analytics programs. Brown et al. \cite{brown2019toward} took a different route with hypothetical algorithmic tools and conducted workshops to understand service providers' comfort levels with using such tools in their work. They found data bias and the inability of computers to understand human context to be some of the major factors behind discomfort towards such tools. 

// RESULTS

which are broadly summarized across three high-level thematic categories, which each consist of 3–4 subthemes respectively. \textit{Values} represent the underlying core principles and values that social workers hold in relation to AI use in the social work profession, \textit{Requirements} reflect the explicit translation and manifestation of these principles in concrete design terms and real-world applications of AI, and \textit{Attitudes} are the culmination of the sentiments, thoughts and feelings of social workers that are important for system designers to keep in mind while designing for the acceptance of such systems.

be managed with discretion and nuance. Respect for the client entails providing high quality care to the best ability of the social work practitioner, keeping in mind their individuality and independence while respecting their right to privacy and confidentiality.

While formulating a clear view of a client's situation from these notes is imperative for effective interventions, cleaning up such overwhelming amounts of raw data and formulating them into assessments this process can be difficult and "time-consuming" (W3). This is one area where LLMs and AI, with its superior data processing and generative potential, can "definitely help" with in terms of helping social workers to "streamline [their processes] and save...time" (W1). 

Yet, the use of AI here presents an ethical conundrum. Participants mentioned multiple times how risk and safety concerns (i.e. any potential threats to the physical safety of the client)
Yet, herein lies the ethical conundrum of utilizing AI in social work: it can help social workers to reduce the mental and manual burdens of data cleaning and processing, but before doing so, they need to first and foremost ensure that it is in line with "

These statements show that social workers recognize and value the tremendous time-saving benefits that AI can offer, but are hesitant and cautious about using them due to potential privacy concerns (e.g., whether the AI tool would retain clients' information, or whether the data would be used to train the AI model). , and with the technological advancements and accelerated data processing powers brought about by big data and AI technologies, "[t]oday's social workers face issues involving values and ethics that their predecessors in the profession could not possibly have imagined", including concerns of monitoring, surveillance, data privacy and confidentiality \cite{reamer2018social}. Our interviews therefore underscore the importance of data privacy as a core value of social workers and the exigency of designing and developing ethical AI systems that can safeguard and protect the confidentiality and personal information of clients in the context of social work.

C1's choice of intervention modality "really depends on, like, the case". S6's preparation for a session "depends on what is the goal I want to actually work on with the clients".

This illustrates how different workers may have very different processes and workflows for approaching the same problem (in this case, crafting an intervention plan for a client). Ultimately, however, it is the worker's skills and understanding of how to conceptualize a case using multi-faceted approaches that results in personalized, high-quality care.
This highlights a core \textit{theoretical} competency of social workers and principle behind the profession. Similar to how practitioners respect the individuality of each client \cite{rooney2017direct}, the ability to craft an approach to understanding a client's plight and situation that best fits their situation is a skill that must be accounted for.

\subsubsection{Pedagogy: Training and Learning}

The final theme relates to the ongoing development and growth of social workers, in line with the principle of always seeking to provide the best care possible \cite{rooney2017direct}. Naturally, workers newer to the field do not have all the skills, knowledge, and experience needed to perform parts of their job optimally. W7 had "never attend[ed] CBT training". \textit{[Cite more here...]} As a result, supervision was a theme that came up regularly.

S4 talked about "learning and development"

Proficiency - junior workers lack some skills. Link this to control?

W7 had "never attend[ed] CBT training", and was curious 

While formulating a clear view of a client's situation from these notes is imperative for effective interventions, cleaning up such overwhelming amounts of raw data and formulating them into assessments this process can be difficult and "time-consuming" (W3). This is one area where LLMs and AI, with its superior data processing and generative potential, can "definitely help" with in terms of helping social workers to "streamline [their processes] and save...time" (W1). 

// RESULTS: ATTITUDES

% Across the board, participants expressed surprisingly positive sentiments towards AI, reflecting \textbf{attitudes of openness, amazement and acceptance towards embracing algorithmic technologies in the social service sector}. Most of their interactions with the AI tool and comments revealed a \textit{sense of openness, curiosity, and willingness to experiment with AI}. D1, the director of the center noted that he was "excited [about] this particular project", due to the potential benefits it could bring to the agency and sector. While they acknowledged and were aware of the various limitations and shortcomings of AI, they nonetheless were open towards how they could incorporate it into their existing work practices, and even provide feedback to iteratively test and improve the model. This "willingness to try" spirit was mirrored throughout the testings and FGD with other participants, who were similarly displayed an open-mindedness in their testing the AI tool, gamely trialling the new technology and how they could incorporate it into their existing work practices. The final question in the above quote by D1, "If it's here to stay, how can we make the best of it?" is quite telling of the SSPs' attitude, aptly summarising and reflecting the way participants felt about AI in general: acknowledging the inevitability of AI without rejection or resignation, but rather, with a sense of optimism, acceptance, and positivity in embracing and making the most of it in spite of its challenges. This was similarly paralleled in how other SSPs referred to or talked about AI, stating that "I think there's no harm to use something that makes your life easier" (W5) or that "There's really no harm in terms, it expands the worker's perspective" (S4), viewing AI as a constructive technology that could bring about positive benefits like enhancing their workflows and even helping to broaden their perspectives to thinking and approaching their cases in a new light.

// DISCUSSION

% Human-AI collaboration emerged as a salient theme in the context of digital systems being unable to fully replace human workers in social service. The tendency of GAI to produce information in quantity over quality complements the focused and precise nature of the work normally produced by human workers. Generating multiple possible interventions gave workers new ideas for helping their clients (W11, W12); producing comprehensive overviews of cases helped remind participants of points they might have missed (S5) or expanded their existing perspectives beyond what they might have usually considered (S4); quickly generating summaries and questions about a case provided talking points for supervisors to expand on with junior workers (C1).

% We find this final theme particularly noteworthy because it emphasises the collaborative nature of human-AI interactions, as well as the importance of open dialogue between system designers and users. In our FGDs, we frequently encouraged participants to identify areas in which our system was lacking, even as we also asked them to envision using it in their daily lives. We believe this step to be crucial as it frames participants as the experts, and creates a sense of openness in allowing our users to imagine their own use cases rather than us dictating system design from the perspective of computer scientists.

% \textbf{Where do our findings differ from past work?}

% Going into the sessions, we were uncertain of how our system could help in this regard, since supervision largely involves human-to-human discussion. However, it 

% They conclude by recommending designing tools that "support workers in understanding the boundaries of [an AI system's] capabilities", and implementing design procedures that "support open cultures for critical discussion around AI decision making". The authors outline critical challenges of implementing AI systems, elucidating factors that may hinder their effectiveness and even negatively affect operations within the organisation.

% \subsection{explain possible reasons for results}
% May not be relevant to our paper