\subsection{Problem definition}
In this report, we will focus on an inverse problem in model-based elastography, which is governed by a PDE and boundary conditions
\begin{align}
    \nabla \cdot \sig(\s) &= 0, \quad \s \in \Omega \subset \mathbb{R}^d, \label{eqn:PDE} \\
    \uv(\s) &= \ve g, \quad \s \in \partial\Omega \subset \mathbb{R}^{d-1} \\
    \sig \cdot \ve n &= \ve f , \quad \s \in \partial\Omega \subset \mathbb{R}^{d-1}
\end{align}
where $\sig$ is the Cauchy stress tensor as a function of space $\s$, $\ve n$ is the outward normal vector on the boundary, and $\ve g$ and $\ve f$ are the respective prescribed values on the boundary. These equations are generally designed to model the physical reality and are general and well-established. We will refer to them as conservation laws and assume that these equations are reliable. We further have a point-wise (local) constitutive law 
\begin{equation}
    \sig(\s) = \sig(\nabla \uv (\s); ~m(\s)), \label{eqn:constitutive}
\end{equation}
where $\uv$ is the displacement field and $m$ is the material parameter (field), linking those quantities with the stresses $\sig$. These governing equations are valid only for specific material classes and rely on assumptions about the material, which may be incorrect. The equations become unreliable if the assumed constitutive model fails to represent the actual material behavior (e.g., assuming linearity for a nonlinear material). In these cases, results might be errorous or even misleading. We will refer to this as unreliable constitutive law.

With this set of equations, one can typically solve a forward model calculating an output $\uv$ from a known input $m$ plus boundary conditions and abstractly denote this as $\uv(m)$. In the inverse problem, noisy displacement observations $\hu = \{ \hat{u}_i \}_{i=1}^{N_{\hu}}$ at locations $\hs=\{ \hat{s}_i \}_{i=1}^{N_{\hu}}$ are available, where each measurement relates to the solution of the forward problem by
\begin{equation}
    \hat{u}_i(\hat{s}_i) = u_i(m, \hat{s}_i) + \tau^{-0.5}  \epsilon_i, \qquad \epsilon_i \sim \mathcal{N}(\ve 0, \ve 1), \label{eqn:obs}
\end{equation}
where the additive term accounts for the stochastic observation noise, which is assumed to be Gaussian distributed with variance $\tau^{-1}$. We can score how likely the observations $\hu$ arose from a given material field $m$ in probabilistic terms in the likelihood and can then use Bayes theorem by introducing a prior $p(m)$ to yield the posterior
\begin{equation}
    \begin{array}{ll}
        p(m | \hat{\ve u}) &  \propto p(\hat{\ve u}| m) ~p(m) \\
         & \propto \prod_{i=1}^{N_{\hat{\ve u}}} \mathcal{N} (\hat{\ve u}_i~|~\ve u_i(m), ~\tau^{-1}) ~~p(m).
    \end{array}
    \label{eqn:posterior_unnormed} 
\end{equation}


\subsection{Main idea}
We propose to reformulate a PDE-based inverse problem where, in addition to inferring the desired input quantity, one should be able to identify the validity of unreliable equations and provide quantitative indicators of model errors over the  problem domain. For this, we distinguish between reliable equations (e.g., conservation law, which is typically well-known, physics-based governing equations) and unreliable equations (e.g., constitutive law, which might be selected as part of a design choice). We will propose a scheme based on the ideas presented in \cite{scholz2025weak}, which includes reformulating the inverse problem that obviates the need for a forward model while retaining the valuable information the governing PDE provides. However, in addition, we consider \textit{all} state variables as random variables to be able to enforce reliable and unreliable equations in two separate sets of residuals.

We use a finite-dimensional representation of the material field $m$, the displacement field $\uv$ and the stress field $\sig$
% while we postulate a displacement field $\uv$ parameterized by a neural network
\begin{equation}
    m(\x, \s) = \sum_{i=1}^{d_{\x}} x_i \eta^{\x}_i(\s), \sig(\chi, \s) = \sum_{i=1}^{d_{\ve \chi}} \chi_i \ve{\eta}^{\ve \chi}_i(\s), \quad \mathrm{and} \quad \uv = \sum_{i=1}^{d_{\uv}} z_i \ve{\eta}^{\uv}_i(\s) \label{eqn:fields} % \NN_{\xi}\left( \z, \s \right), 
\end{equation}
where $\s$ are the spacial coordinates, $\z = \{ z_i \}_{i=0}^{d_{\z}}$, $\ve \chi = \{ \chi_i \}_{i=0}^{d_{\ve \chi}}$ and $\x = \{ x_i \}_{i=0}^{d_{\x}}$ are the respective (input) parameters, 
% $\ve \xi$ are the neural network weights and biases, 
and $\eta^{\x}_i$, $\ve{\eta}^{\uv}_i(\s)$ and $\ve{\eta}^{\ve \chi}_i(\s)$ are given basis functions dependent on space.

As in \cite{scholz2025weak}, we will make use of the weighted residuals (based on the weak form of the PDE in Eq. \ref{eqn:PDE}) to enforce the conservation law
\begin{equation}
    \re_{\ve w}(\ve \chi) = \int_{\Omega} \sigma_{ij} w_{i,j} \diff\Omega - \int_{\Gamma_N}  g_i w_i \diff\Gamma_N, \label{eqn:rc} %- \int_{\Omega} \rho b \cdot w \diff\Omega 
\end{equation}
where each residual differs in the selection of its weight function $\ve w$. As in previous works, the weighted residuals serve as data sources rather than as a means to derive a conventional discretized system of equations. Will further introduce a second set of residuals to score the validity of the constitutive law 
\begin{equation}
    \rc(\x, \z,\ve \chi, \s) = \sig(\ve \chi, \s) - \tsig(\x, \z, \s),
\end{equation}
where we distinguish between $\sig$ directly constructed by the random variable $\ve \chi$ (see Eq. \ref{eqn:fields}) and $\tsig$ calculated using the proposed (potentially erroneous) constitutive law in Eq. \ref{eqn:constitutive}. Further, the conservation law residual is calculated as an integral over a domain, while the constitutive law residual is evaluated at collocation points $\tilde{\s}$.

Alike in \cite{scholz2025weak, kaltenbach2020incorporating}, we will now treat \textit{both} sets of residuals as \textit{virtual} observations $\hrc = \hre = 0$, handling them similarly to the displacement observations in Eq. \ref{eqn:obs}, which relates to the actual residuals as:
\begin{align}
    0=\hre_{j} &= \re_{\ve w^{(j)} }(\ve \chi)+\left(\lame\right)^{-1}\epsilon_j, \qquad \epsilon_j \sim \mathcal{N}(0,1), \quad \mathrm{and} \\
    0=\hrc_k &= \rc(\z,\x, \ve \chi, \s)+\left(\lamc_k \right)^{-1}\epsilon_k, \qquad \epsilon_k \sim \mathcal{N}(0,1).
\end{align}
If we denote separately all the {\em virtual} observables  $\hRe=\{ \hre_k=0 \}_{k=1}^{N_e}$ and $\hRc=\{ \hrc_j=0 \}_{j=1}^{N_e}$, the equation above gives rise to the {\em virtual} likelihoods:
\begin{align}
    p(\hRe | \ve \chi) & =\prod_{j=1}^{N_e} p(\hre_j=0 | ~\ve \chi) \\
 & \propto \prod_{j=1}^{N_e} \sqrt{\lame}  \exp\left(  - \frac{\lame}{2} ~\left(\re_{\ve w^{(j)}}\right)^2(\ve \chi) \right), \quad \mathrm{and} \\
     p(\hRc | \z,\x, \ve \chi, \vlamc) & =\prod_{h=1}^{N_c} p(\hrc_j=0 | ~\z,\x, \ve \chi, \vlamc) \\
 & \propto \prod_{j=1}^{N_c} \sqrt{\lamc_j}  \exp\left(  - \frac{\lamc_j}{2} ~\left(\rc\right)^2(\z,\x, \ve \chi) \right).
\end{align}
where both hyper-parameter  $\{\lame, \vlamc\}>0$ penalize the deviation of the residuals from $0$, i.e. the value they would attain for any solution tuple $\{ \z,\x, \ve \chi \}$ (or equivalently $m$, $\sig$ and $\ve u$). Note that $\lame$ describes how tightly we want to enforce the PDE, similar to the numerical tolerance of a deterministic iterative solver. In contrast, $\vlamc$ scores the discrepancy (in residual form) between the probabilistic stresses $\sig$ and the constitutive-law predicted stresses $\tsig$. Thus, the random variables $\vlamc$ are central in this framework, as they connect the probabilistic threads between the reliable and unreliable equations of the model. Once the model is converged, a low $\left(\lamc_i\right)^{-1}$ indicates a part of the domain where the selected constitutive law is incorrect and vice versa.

As in \cite{scholz2025weak}, the \textit{actual} likelihood is now given by
\begin{equation}
    \begin{array}{ll}
p(\hat{\ve u}|\z)  & =\prod_{i=1}^{N_{\hat{\ve u}}} \mathcal{N} (\hat{ u}_i~|~ u_i( \z), ~\tau^{-1}) \\
& \propto \prod_{i=1}^{N_{\hat{\ve u}}} \sqrt{ \tau}  e^{-\frac{\tau}{2} (\hat{ u}_i-u_i( \z))^2},
\end{array}
\label{actual_like}
\end{equation}
which we can implement in the Bayes rule to yield the joint posterior
\begin{equation}
    p\left( \x, \z, \ve \chi, \vlamc | \hRc, \hRe, \hu \right) = \frac{p \left( \hu|\z \right) p\left(\hRe | \ve \chi\right) p\left( \hRc | \z,\x, \ve \chi, \vlamc\right) p\left( \x, \z, \ve \chi, \vlamc\right) }{p \left( \hRc, \hRe, \hu \right) },
\end{equation}
with joint prior $p\left( \x, \z, \ve \chi, \vlamc\right)$ and model evidence $p \left( \hRc, \hRe, \hu \right)$. In the following, we will adopt a mean field prior for simplicity.

\begin{Remarks}
    \item As the validity of the corresponding constitutive model equations is unknown a priori, the parameters $\vlamc$ must be learned from the data.
    \item If we consider collocation-type residuals, then large values of $\left(\lamc_j\right)^{-1}$indicate locations where the model error is high and vice versa. In such a case, nonzero values of $\rc$ do not arise due to variability in the material parameters $\x$ but rather due to the inadequacy of the constitutive model to provide sufficient closure to  the governing equations.
    \item Selecting values for $\left( \lame \right)^{-1}$ similar to the numerical tolerance of a deterministic iterative solver has proven to be a good strategy.
    \item The integration over the problem domain can either be carried out deterministically or by Monte Carl.
\end{Remarks}

\subsection{Probabilistic inference}
As the posterior is not available in closed form due to the residual terms, we opt for a numerical method, specifically Variational Inference. Here, the goal is to fit a parameterized family of densities $q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)$ with tuneable parameters $\ve \xi$ to the exact posterior by minimizing the KL divergence. Therefore, we construct the ELBO as
\begin{equation}
    \begin{array}{ll}
        \log p(\hRe, \hRc, \hu) & = \log \int p \left( \hu|\z \right) p\left(\hRe | \ve \chi\right) p\left( \hRc | \z,\x, \ve \chi, \vlamc\right) p\left( \x, \z, \ve \chi, \vlamc\right) \diff\z \diff\x \diff\ve \chi \diff \vlamc \\
        &\geq \left< \log \frac{p \left( \hu|\z \right) p\left(\hRe | \ve \chi\right) p\left( \hRc | \z,\x, \ve \chi, \vlamc\right) p\left( \x, \z, \ve \chi, \vlamc\right)}{q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)} \right>_{q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)} \\ %\quad \textrm{(\person{Jensen}'s inequality)}\\
        & = -\frac{\lame}{2} \sum_{j=1}^{N_e} \left< \left(\re_{\ve w^{(j)}} \right)^2 (\ve \chi) \right>_{q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)} \\
        &- \sum_{k=1}^{N_c} \left< \frac{\lamc_k}{2} \left(\rc_k \right)^2 (\z, \x, \ve \chi) \right>_{q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)} \\
        &- \frac{\tau}{2}  \sum_{i=1}^{N_{\hat{\ve u}}} \left< (\hat{ u}_i -  u_i(\z))^2 \right>_{q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)} \\
        & + \left< \log \frac{p(\z, \x, \ve \chi, \vlamc)}{q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)} \right>_{q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)} \\
        & = \mathcal{L}(\ve \xi),
    \end{array}
\label{eqn:ELBO}
\end{equation}
where $\left< \cdot \right>_{q_{\ve \xi}}$ denotes the expectation with respect to $q_{\ve{\xi}}$. The interpretation of the ELBO terms can be done as follows
\begin{itemize}
    \item The first term of the ELBO promotes the minimization of the conservation law residuals, satisfying the PDE.
    \item The second term encourages minimizing the constitutive law residuals, linking the stresses $\tsig$ that satisfy the PDE to match the stresses $\sig$ that satisfy the chosen constitutive equation.
    \item The third term minimizes the discrepancy between solution observations $\hu$ and the predicted solution $\uv$. 
    \item The fourth term provides regularization by minimizing the KL divergence with the prior. 
\end{itemize}

We follow the works in \cite{scholz2025weak} and perform a Monte Carlo approximation of the first ELBO term by subsampling a set of $K \ll N$ weight functions in each iteration to improve computational efficiency:
\begin{align}
    \sum_{j=1}^{N_e} \left< \left(\re_{\ve w^{(j)}} \right)^2 (\ve \chi) \right>_{q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)} \approx & \frac{{N_e}}{K} \sum_{k=1}^K \left< \left(\re_{\ve w^{(j_k)}} \right)^2 (\ve \chi) \right>_{q_{\ve \xi}(\z, \x, \ve \chi, \vlamc)}, \qquad j_k \sim Cat\left({N_e}, \frac{1}{{N_e}} \right). 
    \label{eqn:approx_virt_like}
\end{align}

To ensure a comprehensive assessment of model validity within the second term of the ELBO across the domain, we have to select the number of residuals $N_c$ required and their spatial distribution. There are three possible strategies:
\begin{enumerate}
    \item We preselect a set of constitutive-law residuals on a fixed grid, e.g., the nodes of the finite element grid used in Eq. \ref{eqn:fields}. At each iteration, one could either incorporate all preselected residuals or employ a subsampling strategy, similar to the first term of ELBO, to balance computational efficiency with accuracy, or
    \item We preselect a set of fixed, randomly distributed residuals in the domain and can again employ subsampling, or
    \item We can employ an adaptive strategies, beginning with a uniform distribution and progressively refining residual selection in regions where $\left(\lamc_j\right)^{-1}$ is found to be large, thereby focusing computational effort on areas with higher model uncertainty.
\end{enumerate}
We selected the first option without subsampling for this report, as it is the easiest to implement, and the problems considered are not limited by the number of $\rc$ considered due to their low dimensionality. We do not claim that this strategy is the best in accuracy or computational efficiency for other problems or even those considered in this report.

\subsection{Approximate posterior} \label{subsec:q}
\begin{figure}
    \centering
    \tikzstyle{observe} = [rectangle, 
    minimum width=1cm, 
    minimum height=1cm, 
    text centered, 
    text width=1cm, 
    draw=black, 
    fill=gray!30]
    
    \tikzstyle{latent} = [circle, 
    minimum width=1cm, 
    minimum height=1cm, 
    text centered, 
    draw=black]
    \tikzstyle{arrow} = [thick,->,>=stealth]
    
    \begin{tikzpicture}[node distance=2cm]
    \node (x) [latent] {$\x$};
    \node (z) [latent, below left of=x] {$\z$};
    \node (re) [observe, right of=x] {$\rc$};
    \node (rc) [observe, above of=re] {$\re$};
    \node (u) [observe, below of=re] {$\hu$};
    \node (chi) [latent, left of=rc] {$\ve \chi$};
    \node (lame) [latent, right of=re] {$\vlamc$};
    
    \draw [arrow] (z) |- (u);
    \draw [arrow] (z) -- (re);
    \draw [arrow] (z) |- (x);
    \draw [arrow] (z) |- (chi);
    \draw [arrow] (chi) -- (rc);
    \draw [arrow] (chi) -- (re);
    \draw [arrow] (x) -- (re);
    \draw [arrow] (lame) -- (re);
    
    \end{tikzpicture}
    \caption{Connection between latents (white circles) and observables (grey boxes).}
    \label{fig:chart}
\end{figure}

The approximate posterior $q_{\ve\xi}$ is critical for accuracy and efficiency, requiring a balance between expressiveness and computational simplicity. To motivate our choice, it is to note that solution field $\uv$, material field $m$, and stress field $\sig$ are strongly dependent as they must jointly ensure that the weighted residuals $\re$ and constitutive law residuals $\rc$ in the virtual likelihoods are in the vicinity of zero (given that the select constitutive law is valid). The general structure of the posterior is thus
\begin{equation}
    \q(\z, \x, \ve \chi, \vlamc) = \q(\x | \z) q(\ve \chi | \z) q(\z) q(\vlamc)
\end{equation}

To capture this connection, we use a latent variable $\z$ to represent the inner state of the system and connect its samples to the quantities mentioned above (see Figure \ref{fig:chart}):
\begin{equation}
    \q (\z) = \mathcal{N}\left( \z | \ve \mu_{\z}, \ve S_{\z} \right) 
\end{equation}
Analogously to \cite{scholz2025weak}, we link the mean $\ve{\mu}_{\x;\ve \xi_x}$ and $\ve{\mu}_{\ve \chi;\ve \xi_\chi}$ to $\z$ via neural networks with parameters $\ve \xi$
\begin{equation}
    \q(\x | \z) = \mathcal{N}\left( \x | ~\ve{\mu}_{\x;\ve \xi_x}\left( \z \right), ~\ve{S}_x \right) 
    \quad \mathrm{and} \quad 
    \q(\ve \chi | \z) = \mathcal{N}\left( \ve \chi | ~\ve{\mu}_{\ve \chi;\ve \xi_\chi}\left( \z \right), ~\ve{S}_{\chi} \right),
\end{equation}
where the conditional covariance $\ve{S}_{x}$ and $\ve{S}_{\chi}$ are assumed to be independent of $\z$ and of the form
\begin{equation}
    \ve{S}_x=\ve L_x \ve L_x^T + \mathrm{diag}\left( \ve \sigma_x^2 \right) 
    \quad \mathrm{and} \quad 
    \ve{S}_\chi=\ve L_\chi \ve L_\chi^T + \mathrm{diag}\left( \ve \sigma_\chi^2 \right),
\end{equation}
where $\ve{L}_x$ (or similarly $ \ve L_\chi$) is a matrix of dimension $d_{\ve x} \times d_{\tilde{\ve x}}$ (or $d_{\ve \chi} \times d_{\tilde{\ve \chi}}$) that captures the principal directions along which (conditional) variance is larger whereas  $\ve{\sigma}_x^2$ (or $\ve{\sigma}_\chi^2$) is a vector of dimension $d_x$ (or $d_\chi$) that captures the residual (conditional) variance along the $\x-$ dimensions (or $\ve \chi-$ dimensions).
In contrast to a full covariance matrix, the form adopted for $\ve{S}_x$ (or $\ve{S}_\chi$) ensures linear scaling of the unknown parameters with $d_x$ (or $d_\chi$), which for most problems can be high.

Further, the approximate posterior for $\vlamc$ is independent and given via
\begin{equation}
    \q(\vlamc) = \prod_{j=1}^{d_{\vlamc}} \mathrm{Gamma}\left(\lamc_i | a_i, b_i \right),
\end{equation}
to be able to update in closed form. 

% Finally, to evaluate all terms of the ELBO, one has to sample the solution field $\uv$, which includes a trainable neural network 
% \begin{equation}
%     \uv(\z) = \sum_{i=1}^{d_{\z}} z_i \NN_{\ve \xi_{u}}(\s).
% \end{equation} 

The resulting vector of parameters $\ve \xi$ that has to be optimized by maximizing the ELBO is given by 
\begin{equation}
    \ve \xi = \{ \ve \mu_z, \ve S_z, \ve \xi_x, \ve L_x, \ve \sigma_x^2, \ve \xi_\chi, \ve L_\chi, \ve \sigma_\chi^2, \ve a, \ve b \}. % ,  \ve \xi_u
\end{equation}

\begin{Remarks}
    \item There is an alternative formulation for $\uv$, where the representation does not rely on known basis functions but rather on learning the basis function using a neural network. The details, as well as the advantages, are discussed in Appendix \ref{app:latent}.
\end{Remarks}

\subsection{SVI}
The ELBO $\mathcal{L}(\ve \xi)$ is maximized using Stochastic Variational Inference (SVI), which relies on Monte Carlo estimates and Stochastic Gradient Ascent. The reparameterization trick is employed to generate samples from the approximate posterior $\q$. Given the structure of $\q$, this process begins by drawing $\z$-samples from $\q(\z)$
\begin{equation}
    \z = \ve \mu_z + \ve{S}_z \ve \varepsilon_1 \quad  \ve \varepsilon_1 \sim \mathcal{N}\left(\ve 0, \ve I_{d_{\ve z}}\right) ,
\end{equation}
which can be used as an input for the neural networks that approximate the means of $\x$ and $\ve \chi$ to then generate random samples as
\begin{align}
    \x =\ve{\mu}_{\x;\ve \xi_x}\left( \z \right) + \ve{L}_x \ve \varepsilon_2+ \ve \sigma_x  \odot \ve \varepsilon_3, \quad  & \ve \varepsilon_2 \sim \mathcal{N}\left(\ve 0, \ve I_{d_{\tilde{\ve x}}}\right) \mathrm{ \ and \ } \ve \varepsilon_3 \sim \mathcal{N}\left(\ve 0, \ve I_{d_{\ve x}}\right) \quad \mathrm{and} \\
    \ve \chi =\ve{\mu}_{\ve \chi;\ve \xi_\chi}\left( \z \right) + \ve{L}_\chi \ve \varepsilon_4+ \ve \sigma_\chi  \odot \ve \varepsilon_5, \quad  & \ve \varepsilon_4 \sim \mathcal{N}\left(\ve 0, \ve I_{d_{\tilde{\ve \chi}}}\right) \mathrm{ \ and \ } \ve \varepsilon_5 \sim \mathcal{N}\left(\ve 0, \ve I_{d_{\ve \chi}}\right),
\end{align}
respectively. Lastly, sampling from $\q(\vlamc)$ is straightforward, or when we follow the iterative updating scheme as in Appendix \ref{app:lame}, the expectancy can be evaluated in closed form by $\left< \lamc_i \right> = \frac{a_i}{b_i}$.

We sample $K$ tuples of $\{\ve \chi, \x, \z\}$ to approximate the weighted residuals $\re$ in Eq. \ref{eqn:rc}, and $J$ samples of $\vlamc$ to then evaluate the constitutive law residual $\rc$ at $J$ locations times $K$ samples. The gradients are computed using PyTorch's automatic differentiation. Parameter updating is performed via ADAM with standard parameters and learning rate $\rho = 10^{-4}$. The pseudo-code is given in Algorithm \ref{alg:SVI}.

\begin{algorithm}[t]
\caption{SVI Training Algorithm}\label{alg:SVI}
\begin{algorithmic}
\State Select $\lame$, $\tau$, $K$, $L$; Initialize $\ve \xi \leftarrow \ve \xi_0$, $t \leftarrow 0$
\While{$\mathcal{L}$ not converged}
\State Generate $K$ weight functions $\ve{w}^{(j_k)}$ 
\For{$\ell=1$ to  $L$}
\State Draw $\z \leftarrow \ve \mu_z + \ve{S}_z \ve \varepsilon_1 \quad  \ve \varepsilon_1 \sim \mathcal{N}\left(\ve 0, \ve I_{d_{\ve z}}\right)$
\State $\ve{\mu}_{n,\xi_x}, \ve{\mu}_{n,\xi_\chi}   \leftarrow NN_x(\ve z_\ell), NN_\chi(\ve z_\ell)$
\State Draw $\ve x_\ell \leftarrow \ve{\mu}_{x;\xi_x}\left( \ve z_\ell \right) + \ve{L}_x \ve \varepsilon_2 + \ve \sigma_x \ve \varepsilon_3, \  \ve \varepsilon_2 \sim \mathcal{N}\left(\ve 0, \ve I_{d_{\tilde{\ve x}}}\right), \ve \varepsilon_3\sim \mathcal{N}\left(\ve 0, \ve I_{d_{\ve x}}\right)$ 
\State Draw $\ve \chi_\ell \leftarrow \ve{\mu}_{\chi;\xi_\chi}\left( \ve z_\ell \right) + \ve{L}_\chi \ve \varepsilon_4 + \ve \sigma_\chi \ve \varepsilon_5, \  \ve \varepsilon_4 \sim \mathcal{N}\left(\ve 0, \ve I_{d_{\tilde{\ve \chi}}}\right), \ve \varepsilon_5\sim \mathcal{N}\left(\ve 0, \ve I_{d_{\ve \chi}}\right)$ 
\EndFor
\State Estimate $\mathcal{L}_{\ve \xi}(\ve x_\ell, \ve z_\ell, w^{(j_k)}, \ve \chi_\ell)$ 
\State Estimate $\nabla_{\ve \xi} \mathcal{L}_{\ve \xi}$
\State Update $\ve \xi_{t + 1} \leftarrow \ve \xi_{t} + \ve \rho^{(t)} \odot \nabla_{\ve \xi} \mathcal{L}_{\ve \xi}$ 
\State $t \leftarrow t + 1$
\EndWhile 
\end{algorithmic}
\end{algorithm}

\subsection{Important quantities for the results}
After convergence, we can sample $B=1000$ tuples $\{ \z, \x, \ve \chi, \vlamc \}$ from the approximate posterior $\q$ and combine them with the basis functions (for $\{ \x, \ve \chi \}$) to receive material fields $m$ and stress fields $\tsig$, evaluate the neural network at grid locations to receive a solution field $\uv$ or order the parameters directly to their positions in the case of $\vlamc$. In all cases, we are then able to estimate the posterior mean and variance via a location-point-wise Monte Carlo estimate of a quantity $(\cdot)$
\begin{align}
    \label{eqn:mean}
    \mathbb{E}[(\cdot)(\ve s) | \hu, \hRc, \hRe] &\approx \frac{1}{B} \sum_{b=1}^B (\cdot)_{b}(\ve s) \quad \mathrm{and} \\
    \mathrm{Var}[ (\cdot)(\ve s) | \hu, \hRc, \hRe ] &\approx \frac{1}{B } \sum_{b=1}^B \left( (\cdot)_{b}(\ve s) - \hat{(\cdot)}(\ve s) \right)^2. \label{eqn:variance}
\end{align}
or estimate the credibility intervals (e.g. $2.5 \%$ and $97.5 \%$) at each location as
\begin{align}
   Q_{0.025} ((\cdot)(\ve s)) %= lb_m(\ve s) 
   &= \mathrm{quantile}((\cdot)_b(\ve s), 0.025) \quad \mathrm{and} \label{eqn:lower_bound}\\
    Q_{0.975} ((\cdot)(\ve s)) %=ub_m(\ve s) 
    &= \mathrm{quantile}((\cdot)_b(\ve s), 0.975).  \label{eqn:upper_bound}
\end{align}