This alternative formulation focuses on how the fields are constructed from random variables, with particular emphasis on how the latent variable $\z$ generates the displacement field $\uv$. Instead of relying on predefined basis functions $\ve{\eta}^{\uv}_i(\s)$, we propose learning a set of basis functions via a neural network with parameters $\xi_{\uv}$:
\begin{equation}
    \uv = \sum_{i=1}^{d_{\z}} \z_i \NN_{\xi_{\uv}}(\s).
\end{equation}
Here, the neural network $\NN_{\xi_{\uv}}(\s)$ takes the position $\s$ as input and, through a dot product with the latent variable $\z$, generates the function value $\uv(\s)$. A key advantage of this approach is its computational efficiency, as it enables capturing the statistical variations in the displacement field $\uv$ with a lower-dimensional representation of $\z$ compared to the standard formulation. This efficiency arises from the enhanced expressiveness of the learned basis functions relative to traditional finite element shape functions.

This reformulation also alters the role of the random variable $\z$ within the framework. It now serves as a latent representation of the systemâ€™s internal state, influencing the tuple $\{ m, \uv, \sigma \}$. Unlike the approach presented in Section 2, where $\z$ primarily contributes to field construction, the task of forming $\uv$ is now shifted toward the neural network representing the basis functions. Additionally, as $\z$ serves as an input to the neural networks computing the means of $\sigma$ and $x$, its lower dimensionality results in a reduced number of required parameters in these networks.

While other fields, such as $m$ and $\sigma$, can also be parameterized using neural networks to further reduce parameter count, this introduces additional challenges. In particular, handling sharp inclusion boundaries necessitates an appropriate prior, as conventional finite element-based penalties for abrupt jumps between neighboring elements are no longer applicable. These challenges do not arise for the displacement field $\uv$, which is inherently smooth.

A limitation of this formulation is that domain integration is no longer feasible in closed form, as was possible with predefined basis functions. Consequently, Monte Carlo integration must be employed. To assess its impact, we conducted a study on how the number of Monte Carlo integration points influences noise in residual calculations (for 10 different weight functions and 10 random realizations of $\x, \z, \ve \chi$), summarized in Table \ref{tab:mc_study}. In some first studies, we thus use $500$ MC integration points to approximate the integrals, as it balances efficiency and accuracy.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
        \# MC Points & $|| \Delta \rc ||_2^2$ \\
        \hline
        10,000 & 0.0 (ground truth) \\
        5,000 & 0.87 \\
        1,000 & 2.81 \\
        500 & 2.89 \\
        100 & 10.05 \\
        50 & 11.38 \\
        10 & 30.71 \\
    \end{tabular}
    \caption{Effect of Monte Carlo integration points on residual noise.}
    \label{tab:mc_study}
\end{table}

