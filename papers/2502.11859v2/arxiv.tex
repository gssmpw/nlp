% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage[table,xcdraw]{xcolor}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \usepackage{imagex}
\usepackage{graphicx}
\usepackage{array} % required for text wrapping in tables
\usepackage{tabularx}
\usepackage{amssymb}
\usepackage{float}


\title{Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
Wenrui Xu\textsuperscript{1*}, Dalin Lyu\textsuperscript{1*}, Weihang Wang\textsuperscript{1*}, Jie Feng\textsuperscript{2}, Chen Gao\textsuperscript{3}, Yong Li\textsuperscript{2}\\
\textsuperscript{1}School of Architecture, Tsinghua University\\
\textsuperscript{2}Department of Electronic Engineering, Tsinghua University\\
\textsuperscript{3}BNRist, Tsinghua University\\
\texttt{\{xwr23,lvdl24,wang-wh22\}@mails.tsinghua.edu.cn}\\
\texttt{\{fengjie,chgao96,liyong07\}@tsinghua.edu.cn}\\
}

% 分工
% 周五11.13：results+discussion的数据分析图+边写作，相当于基本写全了
% 周六11.14：results+discussion之外的其他图
% 周日11.15：查漏补缺+appendix

% intro+related work: xwr
% methodology+Settings: darling
% Results-1：xwr
% Results-2：苇航
% Results-3：darling
% Discussion-1：darling
% Discussion-2：苇航
% Discussion-3：xwr
% Conclusion：xwr

% 1. 现有工作不足：不全、没测对/没有依据、没有和人类的比较
% 2. 我们的结论有insight、合理


\begin{document}

\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{These authors contributed equally to this work.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract} % xwr已润色

% 背景-gap-aim-方法-结果-贡献

The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): \textit{Spatial Perception}, \textit{Spatial Relation}, \textit{Spatial Orientation}, \textit{Mental Rotation}, and \textit{Spatial Visualization}. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans (average score 24.95 vs. 68.38), with three key findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation, weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading (30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought (0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from architectural constraints. Identified barriers include weak geometry encoding and missing dynamic simulation. By linking psychometric BSAs to VLM capabilities, we provide a diagnostic toolkit for spatial intelligence evaluation, methodological foundations for embodied AI development, and a cognitive science-informed roadmap for achieving human-like spatial intelligence.

\end{abstract}

\renewcommand{\thefootnote}{\arabic{footnote}}
\begin{figure}[t!]

    \centering
    \includegraphics[width=1\linewidth]{framework.jpg}
    \caption{BSA measuring framework for VLMs\footnotemark[1].}
    \label{fig:framework}
    
\end{figure}
\footnotetext[1]{Image source: \citet{vandenbergMentalRotationsGroup1978}.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} % xwr已润色
% 背景-gap-aim

Visual Language Models (VLMs) excel in a wide range of specific tasks \cite{hong3DLLMInjecting3D2023}. However, achieving human-like spatial intelligence for embodied AI applications such as visual navigation and embodied Q\&A remains a challenge \cite{duranteInteractiveAgentFoundation2024,duanSurveyEmbodiedAI2022}. Recent studies reveal that even advanced models like GPT-4o fail basic 2D spatial reasoning tasks that humans solve effortlessly \cite{tangSparkleMasteringBasic2024}.

Gardner's Theory of Multiple Intelligences \cite{bornsteinFramesMindTheory1986}, which is widely accepted across disciplines, posits that human intelligence is hierarchical, with general intelligence (\textit{g}) supported by subordinate intelligences including spatial intelligence. Spatial intelligence itself is also structured hierarchically, including five Basic Spatial Abilities (BSAs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. These abilities are crucial for advanced intelligence and provide a structured framework for evaluation.

However, existing studies assessing the spatial abilities of VLMs lack a solid theoretical foundation and typically focus on isolated abilities without a comprehensive framework, making it challenging to compare results across different studies or uncover potential interconnections between these abilities. Furthermore, most research omits human performance benchmarks, leaving the gap between VLMs and human largely unexplored.

To address these gaps, we propose a psychometric framework using standardized human spatial tests to systematically evaluate VLMs' BSAs, thereby establishing benchmarks and pathways for enhancing spatial intelligence in AI systems.

%Large language models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable performance in a wide range of specific tasks \cite{hong3DLLMInjecting3D2023}. Recently, an increasing number of researchers have envisioned AI possessing human-like Spatial Intelligence, enabling it to interact with real-world environments and achieve complex embodied AI agent applications \cite{duranteInteractiveAgentFoundation2024}, including visual exploration, visual navigation, and embodied Q\&A \cite{duanSurveyEmbodiedAI2022}. However, despite the impressive performance of these models in many specific tasks, studies have revealed significant limitations in their basic spatial understanding and reasoning abilities. For instance,\citet{tangSparkleMasteringBasic2024} found that GPT-4o and InternVL2-Pro produced implausible responses to simple 2D spatial reasoning tasks that humans could solve at a glance.

%\citet{bornsteinFramesMindTheory1986}'s "Theory of Multiple Intelligences" defines human intelligence as comprising multiple hierarchical types. This theory, widely recognized by psychologists, neuroscientists, and scholars from diverse fields, identifies General Intelligence (\textit{g}) as the highest level, beneath which lie eight distinct types of intelligence: linguistic, logical-mathematical, spatial, musical, naturalist, bodily-kinesthetic, interpersonal, and intra-personal. Spatial Intelligence, in particular, has a complex structure comprising a variety of cognitive abilities ranging from basic to advanced, including five Basic Spatial Abilities (BSAs): Spatial Perception (SP), Spatial Relation (SR), Spatial Orientation (SO), Mental Rotation (MR), and Spatial Visualization (SV). Moreover, evolutionary and developmental psychology studies suggest that spatial abilities can be nurtured and developed progressively. Thus, BSAs are vital for higher-level spatial intelligence.

%However, existing studies on assessing the spatial abilities of VLMs often lack a solid theoretical foundation for the tests they adopted. These studies also tend to focus on specific spatial abilities without a comprehensive framework that covers all basic spatial abilities, making it difficult to compare results across different studies and reveal potential interconnections between various dimensions of these abilities. Additionally, most research does not include human results for comparison, leaving the gap between the spatial abilities of LLMs and humans unexplored.

%Therefore, this study aims to comprehensively define the dimensions of BSAs based on psychometric theories and evaluate the BSAs of VLMs using human spatial ability tests. This approach will establish a foundational benchmark for evaluating VLMs' basic spatial intelligence and potentially provide a "step-by-step" framework for enhancing their more advanced intelligence capabilities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related works} % xwr润色中
% 1. 人类空间智能的定义、层级、维度（g、小g到五个维度的基础空间能力）和测试。
% 2. 已有大模型的空间能力的研究。——没关注基础空间能力，没我们全。


\subsection{Psychometric Studies on Human Spatial Intelligence}

Human spatial intelligence, defined as the ability to mentally model and manipulate spatial environments \cite{bornsteinFramesMindTheory1986}, has been studied since Spearman first identified it as an independent domain in 1904 \cite{spearmanGeneralIntelligenceObjectively1904}. Research diverges into two complementary streams \cite{poratSpatialAbilityUnderstanding2023}:

\textbf{Psychometric Classification.} The first stream establishes hierarchical intelligence models, from general intelligence to domain-specific intelligence like spatial intelligence \cite{gearySpatialAbilityDistinct2022, spearmanGeneralIntelligenceObjectively1904}. These frameworks enable systematic measurement of spatial abilities through standardized tests, continually refining ability subtypes and assessment methods.

\textbf{Interdisciplinary Mechanisms.} The second stream integrates evolutionary psychology, developmental studies, and cognitive neuroscience to explore the origins and mechanisms of spatial abilities, complementing psychometric taxonomies.

Psychometric consensus defines spatial intelligence through three hierarchical levels \cite{caemmererIndividualIntelligenceTests2020, mcgrewCHCTheoryHuman2009, johnsonStructureHumanIntelligence2005, linnEmergenceCharacterizationSex1985, maccobyPsychologySexDifferences1974, michaelDescriptionSpatialvisualizationAbilities1957, thurstonePrimaryAbilitiesVisual1950}:

\begin{enumerate}

\item General Intelligence (\textit{g}): cross-domain cognitive processes, such as attentional control \cite{kaneRolePrefrontalCortex2002}, neural integration \cite{jungParietofrontalIntegrationTheory2007}, and cellular processes \cite{gearyInclassAttentionSpatial2021}, which impact cross-domain learning and performance.

\item Domain-Specific Intelligence: Abilities in particular domains that share common features, with spatial intelligence representing a distinct set of abilities \cite{vernonAbilityFactorsEnvironmental1965}, as formalized in models such as the CHC theory \cite{carrollHumanCognitiveAbilities1993, hornOrganizationAbilitiesDevelopment1968, cattellTheoryFluidCrystallized1963}.

\item Basic Spatial Abilities (BSAs): Decomposing spatial intelligence into measurable subskills \cite{johnsonSexDifferencesMental2007, hegartySpatialAbilitiesDifferent2006, maierSpatialGeometrySpatial1996, voyerMagnitudeSexDifferences1995, halpernSexDifferencesCognitive1992, pellegrinoUnderstandingSpatialAbility1984}.

\end{enumerate}

This study focuses on Level 3, adopting established human experimental paradigms to evaluate VLMs' basic spatial abilities.

%Human Spatial Intelligence is a set of abilities to form mental models of the spatial world and to manipulate and use these models \cite{bornsteinFramesMindTheory1986}. Research on human SI began in 1904 with the identification of SI as an independent intelligence domain \cite{spearmanGeneralIntelligenceObjectively1904}. Since then, the studies can be broadly categorized into two main research lines \cite{poratSpatialAbilityUnderstanding2023}.
%Firstly, Psychometrics introduced hierarchical classifications of human intelligence \cite{gearySpatialAbilityDistinct2022} ranging from broad General Intelligence (\textit{g}) to subdomains (such as verbal and spatial intelligence) to more specific and basic abilities \cite{spearmanGeneralIntelligenceObjectively1904}. In addition, a substantial body of psychometric research has been devoted to developing tests and experiments for spatial abilities. This line of research continues to this day, refining the distinctions between subtypes of spatial ability and identifying the best methods for their measurement.
%Secondly, interdisciplinary research in evolutionary and developmental psychology, as well as cognitive neuroscience, has complemented traditional psychometric studies. These disciplines explore the influencing factors and internal mechanisms underlying spatial abilities, enriching our understanding of their development.
%Psychometric studies have established a hierarchical framework for defining human spatial intelligence, identifying at least three levels of abilities \cite{caemmererIndividualIntelligenceTests2020, mcgrewCHCTheoryHuman2009, johnsonStructureHumanIntelligence2005, linnEmergenceCharacterizationSex1985, maccobyPsychologySexDifferences1974, michaelDescriptionSpatialvisualizationAbilities1957, thurstonePrimaryAbilitiesVisual1950}. At the broadest level lies general intelligence (\textit{g}), which represents a system of multiple cognitive abilities, including factors such as attentional control \cite{kaneRolePrefrontalCortex2002}, neural networks \cite{jungParietofrontalIntegrationTheory2007}, and cellular processes \cite{gearyInclassAttentionSpatial2021} that impact cross-domain learning and performance. The second level captures the commonalities within specific domains of ability, with Spatial Intelligence being one of these domains \cite{vernonAbilityFactorsEnvironmental1965}. For instance, the integration of models by Cattell, Horn, and Carroll (CHC model) incorporates the \textit{g} factor alongside multiple broad cognitive domains, such as fluid reasoning, short-term memory, and quantitative knowledge \cite{carrollHumanCognitiveAbilities1993,hornOrganizationAbilitiesDevelopment1968,cattellTheoryFluidCrystallized1963}. The third level involves further decomposition of spatial abilities into subdomains \cite{johnsonSexDifferencesMental2007, hegartySpatialAbilitiesDifferent2006, maierSpatialGeometrySpatial1996, voyerMagnitudeSexDifferences1995, halpernSexDifferencesCognitive1992, pellegrinoUnderstandingSpatialAbility1984}, with different scholars offering varying interpretations and definitions.

%This study focuses specifically on these basic spatial abilities at the third level, utilizing established testing methods from prior human experiments to evaluate VLMs.


\subsection{Evaluation of LLMs and VLMs' Spatial Abilities}
% 用表格整理一下现有研究测定的能力

Recent advances in Visual Language Models (VLMs) have spurred evaluations of their spatial abilities, yet existing studies remain fragmented (Table \ref{tab:existing_studies}). Most prior work focuses on text-based LLMs, assessing abstract spatial relations through verbal descriptions \cite{yamadaEvaluatingSpatialUnderstanding2024}, inherently neglecting visual-spatial processing. Emerging VLM evaluations primarily target specialized 3D tasks (e.g., robotic trajectory labeling \cite{sharmaExploringImprovingSpatial2023}, indoor scene captioning \cite{fuSceneLLMExtendingLanguage2024}), which partially engage Spatial Perception, Spatial Relation, and Spatial Orientation. However, critical gaps persist: 

\begin{enumerate}

\item Theoretical Disconnect: Tasks lack grounding in theoretical frameworks, preventing direct comparison with human cognition. For instance, robotic trajectory tests \cite{sharmaExploringImprovingSpatial2023} conflate spatial reasoning with action planning.
\item Limited Scope: Most studies omit Mental Rotation and Spatial Visualization (Table \ref{tab:existing_studies}, MR/SV columns).
\item Benchmark Absence: No study systematically maps VLM performance to hierarchical BSAs or provides human baselines.

\end{enumerate}

This study addresses these limitations through a psychometric evaluation framework. Unlike prior studies testing subsets of BSAs (e.g., \citet{fuSceneLLMExtendingLanguage2024}: SP+SR+SO), we assess all five BSAs using standardized human experiments, enabling cross-model and human-AI comparisons.

%The development of Visual Large Language Models (Visual LLMs or VLMs) has been advancing rapidly. Recently, several studies have explored different dimensions of spatial abilities through targeted evaluations. As shown in Table \ref{tab:existing_studies}, most existing research focuses on assessing LLMs' understanding and reasoning of abstract spatial relations, often overlooking their ability to process spatial information from visual inputs. This limitation stems from the fact that most studies test text-in, text-out LLMs, relying primarily on text-based descriptions of spatial problems for evaluation \cite{yamadaEvaluatingSpatialUnderstanding2024}.

%More recently, some studies have begun evaluating Visual LLMs, but these efforts primarily focus on specific 3D spatial data and specialized tasks like interactive planning or 3D captioning. Within our framework, these studies partially test VLMs' spatial perception, spatial relation, and spatial orientation abilities, with limited attention given to other basic abilities like mental rotation and spatial visualization. Additionally, their experimental designs lack a solid theoretical foundation and fail to ensure comparability with similar human cognitive abilities. For example, \citet{sharmaExploringImprovingSpatial2023} investigated VLMs' capabilities in spatial orientation and spatial relation. Their evaluation included 2D path labeling tasks for directions (e.g., "up," "left") and shapes, 3D trajectory labeling tasks involving motions like "lifting," "rotating," and "sliding" (as well as cleaned versions), and identifying relationships between blocks in imagined setups using 3D robotic trajectory data. Similarly, \citet{fuSceneLLMExtendingLanguage2024} focused on VLMs' spatial abilities in indoor scene application tasks, such as describing scene details (dense captioning), identifying and describing objects (object captioning), and interactive planning.

\begin{table}[t]
\centering
\caption{Existing studies testing LLMs and VLMs' spatial abilities.}
\label{tab:existing_studies}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc}
\hline
\multirow{2}{*}{Related Work} & \multirow{2}{*}{VLM} & \multicolumn{5}{c}{Tested Spatial Abilities} \\ \cline{3-7} 
 &  & SP & SR & SO & MR & SV \\ \hline
\citet{fuSceneLLMExtendingLanguage2024}          & Yes & \checkmark & \checkmark & \checkmark &            &  \\
\citet{tangSparkleMasteringBasic2024}            & Yes & \checkmark & \checkmark & \checkmark &            &  \\
\citet{sharmaExploringImprovingSpatial2023}      & Yes &            & \checkmark & \checkmark &            &  \\
\citet{hong3DLLMInjecting3D2023}                 & Yes & \checkmark & \checkmark & \checkmark &            &  \\
\citet{bangMultitaskMultilingualMultimodal2023}  & Yes &            & \checkmark & \checkmark &            &  \\
\citet{yamadaEvaluatingSpatialUnderstanding2024} & No  &            & \checkmark &            &            &  \\
\citet{momennejadEvaluatingCognitiveMaps2023}    & No  &            & \checkmark & \checkmark &            &  \\
\hyperref[section:bib]{Cohn et al. (2023)}       & No  &            & \checkmark &            & \checkmark &  \\
\hline
This study                                       & Yes & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\hline
\end{tabular}%
}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{tests_table.jpg}
    \caption{Examples of classic BSA Tests\footnotemark[2].}
    \label{fig:Classic_Tests}
\end{figure}
\footnotetext[2]{Image source: \citet{erkekRelationshipPreserviceTeachers2011, ben-chaimDevelopmentAnalysisSpatial1986, pawlak-jakubowskaEvaluationSTEMStudents2023, katsioloudisComparativeAnalysisSpatial2014, Bennett2012TheDA, fehringerRCubeSRTestNew2023, friedmanComputerizedSpatialOrientation2020, vingerhoetsAnalysisMoneyRoadmap1996, vandenbergMentalRotationsGroup1978, petersRedrawnVandenbergKuse1995, bodnerPurdueVisualizationRotations1997, maedaPsychometricPropertiesRevised2013, hegartyHowSpatialAbilities2009, fehringerSupplementaryMaterialsImplementation2021}.}

\subsection{Definition and Composition of Basic Spatial Abilities}
Based on Psychometric theories \cite{maierSpatialGeometrySpatial1996} and existing VLM studies, this study identified five key dimensions of Basic Spatial Abilities \cite{pawlak-jakubowskaEvaluationSTEMStudents2023}, decomposing the concept as a comprehensive whole, as shown in Table \ref{tab:Decomposed BSA and the corresponding tests}.
To carry out a complete spatial ability evaluation of VLMs, the study selected nine specific classic psychometric tests, as shown in Figure \ref{fig:Classic_Tests}, designed to cover all five aspects of BSAs, enhancing the persuasiveness of the test. Human experiment results from existing studies were included as benchmarks for comparison.

\begin{table}[t]
\caption{Decomposed BSA and the corresponding tests.}
\label{tab:Decomposed BSA and the corresponding tests}
\resizebox{1\linewidth}{!}{
\begin{tabular}{ccc}
\hline
\textbf{Type} &
  \textbf{Definition} &
  \textbf{Tests} \\ \hline
\begin{tabular}[c]{@{}c@{}}Spatial \\ Perception\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}The ability to perceive \\ horizontal and vertical \\ orientations without \\ interference from \\ miscellaneous information.\end{tabular} &
  SVT \\ \hline
\begin{tabular}[c]{@{}c@{}}Spatial \\ Relation\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}The ability of recognizing \\ relationships between \\ parts of an entity.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}NCIT \\ DAT:SR \\ R-Cube-SR\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Spatial \\ Orientation\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}The ability to navigate \\ or enter a given \\ spatial state.\end{tabular} &
  MRMT \\ \hline
\begin{tabular}[c]{@{}c@{}}Mental \\ Rotation\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}The ability to mentally \\ rotate 3D objects.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MRT \\ PSVT:R\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Spatial \\ Visualization\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}The ability to mentally \\ manipulate and transform \\ 2D and 3D objects.\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}SBST \\ R-Cube-Vis\end{tabular} \\ \hline
\end{tabular}%
}
\end{table}

\subsection{Tests for Basic Spatial Abilities}
\label{section:Tests_for_Basic_Spatial_Abilities}

The goal of the employed tests, on the one hand, is to evaluate the spatial abilities of different VLMs and to compare them with those of humans. On the other hand, by breaking down the basic abilities of spatial intelligence, we aim to identify the deficiencies in current VLMs and provide a necessary foundation for future research aimed at enhancing these abilities. To achieve this goal, the nine selected test questions sets were carefully screened and are all currently available that unambiguously demonstrate a certain BSA, have complete questions, answers, and reproducible human experiment results, which are widely recognized. These questions were not created by the researchers but are grounded in solid psychometric theoretical foundations. The study posits that using as many classic tests as possible can more authentically and accurately reflect the BSAs of VLMs. Therefore, arbitrarily deleting tests to balance the weights of the five core spatial competencies was deemed inappropriate.

Tests adopted are presented in the format of multiple-choice or true/false questions. In each specific test, after briefly explaining the test's content and the ways of answering, we provide the questions in the form of images.


\subsubsection{Spatial Perception Tests}
\textbf{MGMP Spatial Visualization Test (SVT)}. 
SVT, originally developed for the Middle Grades Mathematics Project (MGMP), is also known as the "Lappan Test" and is composed of 32 multiple choice items, each with five options \cite{erkekRelationshipPreserviceTeachers2011,ben-chaimDevelopmentAnalysisSpatial1986}. The test utilizes the combination and transformation of square-cube buildings. The subject is expected to imagine the 2D flat view, the 3D corner view, and the "map plan", which is a numeric cube description of the base of the building. Questions include imagining the conversion between 2D and 3D views, the final appearance when some cubes are altered, and calculating the number of cubes used in a building.


\subsubsection{Spatial Relation Tests}
\textbf{Net Cube Imagination Test (NCIT)}. 
NCIT is based on the conversion between 2D and 3D cubes with lines drawn on inner or outer faces \cite{pawlak-jakubowskaEvaluationSTEMStudents2023}. Each of the 16 tasks has three options. The first eight items require expanding the cube into a flat shape, while the rest involve the reverse development of the cube. 

\textbf{Differential Aptitude Test: Space Relation (DAT:SR)}. 
DAT:SR is part of the Differential Aptitude Test, measuring the ability to relate two and three-dimensional worlds \cite{katsioloudisComparativeAnalysisSpatial2014,Bennett2012TheDA}. The test consists of 40 items and focuses on folding and unfolding 3D geometric shapes. When provided 2D flat unfolded figures, subjects are required to choose the 3D restored form from four options and vice versa.

\textbf{R-Cube-Spatial Relation Test (R-Cube-SR)}. 
R-Cube-SR uses Rubik's cubes with six different colors on each side as rotated visual materials \cite{fehringerRCubeSRTestNew2023}. For each of the 48 items, two cubes are shown in the corner view, the right of which may be the possible rotated result of the left cube. The subjects are required to give a true/false answer with a limited view of the colored sides. The test was created in plain and pattern versions.


\subsubsection{Spatial Orientation Tests}
\textbf{Money Road-Map Test (MRMT)}. 
The Standardized Road-Map Test of Direction Sense \cite{friedmanComputerizedSpatialOrientation2020,vingerhoetsAnalysisMoneyRoadmap1996} commonly known as MRMT, requires allocentric to egocentric right/left discrimination. Subjects follow a dashed path with 32 right/left turns on a map of an abstract city and are required to indicate the direction taken at each turn according to the facing direction. The turn types include the ones that require no rotation, a standard rotation of approximately 90°, and an irregular rotation between 90°-180°.


\subsubsection{Mental Rotation Tests}
\textbf{Mental Rotation Test (MRT)}. 
A classic test developed by \citet{vandenbergMentalRotationsGroup1978} takes the direct way of rotating 3D geometric figures made by cubes. Subjects are required to choose from the four options, two correct rotated reproductions of the target figure. The revised version of 24 items \cite{petersRedrawnVandenbergKuse1995} is utilized.

\textbf{Purdue Spatial Visualization Tests: Visualization of Rotations (PSVT:R)}. 
Developed by \citet{bodnerPurdueVisualizationRotations1997}, PSVT:R is an independent extended version of the Purdue Spatial Visualization Test, which requires matching between the target and the rotated objects. For each item, a geometric shape is rotated first to show the target rotation pattern. Subjects are required to choose from the five rotated views the correct rotated result of another shape under the same pattern. \citet{maedaPsychometricPropertiesRevised2013} further advanced the test to 30 test items, 17 of which involve asymmetrical shapes.


\subsubsection{Spatial Visualization Tests}
\textbf{Santa Barbara Solids Test (SBST)}. 
SBST involves geometric solids intersected by a cutting plane, and the task is to imagine the 2D cross section of the solids and choose from the four options, the two correct answers \cite{hegartyHowSpatialAbilities2009}. Two varying parameters result in the different difficulty of the 30 items: geometric complexity and cutting plane orientation. Simple solids, joined solids, and embedded solids take up one third of the items respectively, while orthogonal and oblique cutting planes divide the items at the same time.

\textbf{R-Cube-Visualization Short Test (R-Cube-Vis)}. 
Similar to the R-Cube-SR test, R-Cube-Vis items consist of two juxtaposed Rubik's cubes \cite{fehringerSupplementaryMaterialsImplementation2021}. However, instead of rotating as a whole, the composing cubes can be rotated as well. Subjects are required to decide the possibility of the left cube rotated into the right one in the 60 items. Based on the size of the cube, the number of the rotated elements, and the ways they are rotated, the items are divided into six difficulty levels.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{test_sample.jpg}
    \caption{An example of different VLMs' answers to a particular Santa Barbara Solids Test question\footnotemark[3].}
    \label{fig:tests_used}
\end{figure*}


\subsection{Settings}

\textbf{Models and APIs}. 
For BSA evaluation, we tested 13 mainstream open-source and commercial models, showcasing the full spectrum of the current major models' BSAs. For commercial VLMs, we used GPT-4o, GPT-4o mini, and GPT-4 Turbo from OpenAI \cite{yangDawnLMMsPreliminary2023} and Gemini-1.5-pro, Gemini-1.5-flash, and Gemini-1.5-flash-8b from Google \cite{geminiteamGeminiFamilyHighly2024}. For open-source models, we tested Qwen2-VL-72B, Qwen2-VL-7B \cite{baiQwenVLFrontierLarge2023}, InternVL2-Llama3, InternVL2-26B, InternVL2-8B \cite{chenHowFarAre2024}, Llama-3.2-11B, Llama-3.2-90B \cite{llamateamai@metaLlama3Herd2024} (the Qwen2 and Llama models are instruct variant). To carry out large-scale automatic tests, we used APIs from four platforms, including SiliconFlow, DeepInfra, OpenAI, and Google.

\textbf{Data Processing and Evaluation Metrics}. 
A total of 312 questions are fed to each of the models. Among the questions, spatial perception accounts for 10.26\% (32 questions); spatial relation accounts for 33.33\% (104 questions); spatial orientation accounted for 10.26\% (32 questions); mental rotation accounts for 17.31\% (54 questions); and spatial visualization accounts for 28.84\% (90 questions). For the assessment of individual abilities, a greater number of questions can increase the credibility of the results. For the evaluation of overall ability, the study equates different individual abilities, so that no single ability will have a greater weight due to a larger number of questions, nor will it have a biased impact on the overall ability.

Using zero-shot or few-shot experiments, we designed the following prompt for VLMs: "You are taking a spatial ability test. Please output the result in pure text format as: question number, answer." Each question is presented to the VLMs separately, while the question and options are displayed in a single image, because the case study indicates negligible difference of model performance with and without explicit separation of answer choices, and the ability to properly recognize option indices are also considered an elementary ability beneath spatial abilities. To ensure comparability with human benchmarks, we retained the instruction phase from the original human experiments when testing VLMs. The ground truth sets come from the original tests or human analysis.

For all employed tests, we take accuracy as the evaluation metric and consider the answer correct when it matches the ground-truth set exactly, and partly correct when it includes a correct option but does not contain false ones. The score for each question is calculated as the number of correct options selected divided by the total number of correct options. Specifically, scores are not counted to avoid distorted results if the models guess the same answers for all items due to the failure to understand the task.

The maximum score for each test is 100, and the model's score is determined by the number of correct answers divided by the total number of questions. The score for each BSA is calculated as the average score of all tests under that ability, while the Overall Ability Score is the average of the scores for all BSAs.


\footnotetext[3]{Image source: \citet{hegartyHowSpatialAbilities2009}.}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{ability_results_boxplot.png}
    \caption{Comparison of VLMs' and humans' five basic spatial abilities and overall ability.}
    \label{fig:ability_results}
\end{figure*}


\subsection{Results}
\label{section:results}
As shown in Table \ref{tab:test_result}, each cell in the table represents a model's score on a specific test. To evaluate the stability of model performance, each test was repeated three times for every model. The numbers in parentheses indicate the standard deviation of these three scores, serving as a measure of performance stability. The final rows of the table present the average scores and standard deviations for all models across each test, along with the corresponding human experiment results.


\subsubsection{Human vs VLM}

As illustrated in Figure \ref{fig:ability_results}, the overall ability scores of the 13 VLMs are relatively close, ranging from 16.31 to 31.22, with an average of 24.95. This is significantly lower than the human average of 68.38. When analyzed across the five BSAs, human performance consistently surpasses that of VLMs. In terms of averages, both human and VLMs show the same performance ranking across the abilities, with spatial orientation being the best and mental rotation the worst. 


\subsubsection{VLM vs VLM}
% 这一部分也是用Table 2和fig 4来叙述
% 不同LLM拆开比较
%\textbf{Model Dimension}. % 模型尺寸
%LLMs generally exhibit the scaling law in their spatial abilities as well %\cite{kaplanScalingLawsNeural2020}.
%\textbf{Model Brand}.% 模型厂家

From Figure \ref{fig:ability_results}, it can be observed that the performance of VLMs show consistency in spatial perception and mental rotation. In contrast, spatial relation and spatial visualization exhibit more significant variability. Most models perform relatively well in spatial orientation, yet the discrepancy is also considerable, with three models (Intern-VL2-76B, Intern-VL2-8B, and Gemini-1.5-flash) failing completely. In terms of overall ability, the performance differences among models are not particularly significant, though some models (e.g. Qwen2-VL-7B and GPT-4o), stand out with relatively strong results. 

\subsubsection{Correlation of Basic Spatial Abilities}
% 这个显著性的解释可能要着重确认一下
% 不同模型内可以看出不同的能力之间的相关性

For the test results of the models' BSAs, we performed a correlation analysis between the abilities to examine their associations. We used the Pearson correlation test, as shown in Figure \ref{fig:Correlation of BSAs.}. The results show that Pearson's correlation coefficients (r) between any two variables are less than 0.4, indicating that all of the ability combinations show very weak or no correlation. Since the events satisfied the two-dimensional normal distribution, no correlation is equivalent to independence. Thus, it can be considered that the spatial abilities do not show a correlation with each other, meaning that each spatial ability is sufficiently "basic" and proves the credibility of the BSA framework.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{BSA_cor.png}
    \caption{Pearson's correlation coefficients (r) between five basic spatial abilities.}
    \label{fig:Correlation of BSAs.}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{manufacturer.png}
    \caption{Comparison of different series of VLMs' basic spatial abilities and overall ability.}
    \label{fig:manufacturer}
\end{figure}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{parameters.png}
    \caption{Comparison of the basic spatial abilities of VLMs with different sizes and manufacturers.}
    \label{fig:model parameters}
\end{figure*}

\subsection{Discussion}
\subsubsection{VLMs' Overall Performance}
% 结果的总结共性、启发

Our evaluation confirms a significant gap between VLMs’ basic spatial abilities and human benchmarks across all dimensions, aligning with prior studies on individual abilities (e.g., spatial relations \cite{yamadaEvaluatingSpatialUnderstanding2024, cohnDialecticalLanguageModel2023}, spatial orientation \cite{momennejadEvaluatingCognitiveMaps2023}). This raises fundamental questions about whether VLMs operate as programmable pattern recognizers or genuinely emulate human-like spatial intelligence.

Notably, VLMs mirror human performance rankings across BSAs: highest in spatial orientation, followed by spatial visualization, and lowest in mental rotation (Figure \ref{fig:ability_results}). This correlates with task complexity that 2D tasks require simpler coordinate reasoning, while 3D demands dynamic mental manipulation, suggesting VLMs struggle disproportionately with higher-dimensional transformations just like humans. This performance gradient implies a developmental pathway: strengthening basic 2D spatial reasoning may scaffold advanced 3D capabilities \cite{tangSparkleMasteringBasic2024}. Such hierarchical training strategies could bridge current limitations.

%Through the evaluation, we can clearly summarize that the basic spatial abilities of VLMs still have a significant gap compared to humans. Although previous studies have not comprehensively covered the five basic dimensions of spatial ability, such as \citet{yamadaEvaluatingSpatialUnderstanding2024} and \citet{cohnDialecticalLanguageModel2023} on spatial relation and \citet{momennejadEvaluatingCognitiveMaps2023} on spatial orientation, the results are highly consistent, indicating that the BSAs of VLMs are significantly lower than the human benchmark, leaving much room for improvement. This brings us back to a widely discussed question: Are VLMs programmable machines or agents with spatial intelligence? Further research may be needed to answer this.

%Interestingly, the performance ranking of VLMs in the five BSAs perfectly matches human results, with the best performance in spatial orientation, followed by spatial visualization, and the lowest score in mental rotation. This may be related to the difficulty levels of the tests assessing different spatial abilities. However, it may also suggest that these abilities have varying degrees of difficulty for the human brain to process and that VLMs' cognitive processing of space has a certain similarity to the human brain. From relatively simple two-dimensional spatial orientation questions to more complex three-dimensional mental rotation queries, we may suggest a trend: VLMs perform better in spatial abilities involving two-dimensional shapes than three-dimensional ones. This insight may inspire the development of strategies where enhancing simpler spatial abilities could serve as a foundation for fostering more advanced spatial capabilities \cite{tangSparkleMasteringBasic2024}.


\subsubsection{Impact of VLM Manufacturer and Size}

As shown in Figure \ref{fig:manufacturer}, models from different manufacturers exhibit noticeable performance differences. The Qwen series demonstrates clear superiority with an overall score of 30.82, outperforming competitors across multiple assessment dimensions. Mid-tier performers including Gemini, GPT, and Llama series cluster around 25, while InternVL2 trails notably at 19.6.

Notably, individual spatial ability performance shows distinct patterns from aggregate scores. Gemini-1.5-flash achieves peak spatial relation performance (43.78) but demonstrates marked deficiencies in mental rotation (16.67) and complete absence of spatial orientation capability. Conversely, the Llama series excels in spatial orientation while underperforming in spatial visualization. These disparities likely originate from heterogeneous training data distributions across different BSAs.

Turning to model size analysis (Figure \ref{fig:model parameters}), our findings challenge conventional expectations. Contrary to typical scaling laws, we observe no positive correlation between parameter count and BSA performance. Smaller models (leftward positioned in manufacturer groupings) frequently outperform their larger counterparts, a trend particularly evident in the Qwen and Llama series where around 10B-parameter models surpass larger variants. The Gemini-1.5-flash-8B matches the performance of its substantially larger Gemini-1.5-pro counterpart, with similar scaling anomalies observed in InternVL2 models. This suggests that architectural optimization and training methodologies may outweigh sheer parameter count for spatial reasoning tasks, with compact models offering favorable efficiency-performance tradeoffs.

To clarify, model parameter counts for Qwen, InternVL2, and Llama series derive from official specifications, while GPT series estimates follow Microsoft's disclosed information. Gemini series parameters remain undisclosed, though relative comparisons within the series remain valid.

% As shown in Figure \ref{fig:manufacturer}, there are noticeable performance differences among models from different manufacturers. Qwen's overall ability has scored 30.82, showing an absolute advantage and achieving the best scores across multiple dimensions. The Gemini, GPT and Llama series exhibit moderate performance, with scores all around 25. The InternVL2 series lags behind, with only 19.6 points.

%The performance of individual spatial abilities may differ from the overall ability scores. For instance, Gemini-1.5-flash achieves the highest score (43.78) in spatial relation but performs comparatively weaker in mental rotation (16.67) and scores 0 in spatial orientation; the Llama series has an exceptionally low score in spatial visualization, yet ranks first in spatial orientation, highlighting significant performance variations across BSAs. This discrepancy may stems from differences in the volume and distribution of training data corresponding to various BSAs.

%Regarding model size, our findings lead to some interesting conclusions. It is necessary to clarify that, the model sizes of the Qwen, InternVL2, and Llama series are based on official data; the model sizes of the GPT series are disclosed by the Microsoft team and are widely accepted estimates; for the Gemini series, there is no official disclosure or credible data, but it is clear that the three models can be compared in terms of size. The models in \ref{fig:model parameters} are arranged by manufacturer, with smaller models placed on the left side. 

%As can be seen, there is no positive correlation between model size and the model's BSAs. On the contrary, larger models may exhibit relatively weaker performance, while some smaller models perform surprisingly well. Many models with parameters around 10B actually perform better than larger models from the same manufacturer, such as Qwen and Llama; the performance of the Gemini-1.5-flash-8B model is similar to that of the much larger Gemini-1.5-pro, and the InternVL2 series models also show a similar conclusion. This implies that finely tuned smaller models may be better suited for tasks involving spatial abilities and more economically efficient, and model size is definitely not a measure of spatial ability. Training methodologies and model architectural design may play a crucial role in determining a model's BSA.


\subsubsection{Impact of CoT and Example-Based Training}
% 离谱的错误（有啥写啥）
% 在测试开始前用几个例题进行训练，是否能在一定程度上提升模型表现？提升程度大吗-需要定量百分比？（需要用几个模型做一下小测试）
% 让模型在回答中分步骤输出思考过程，是否能在一定程度上提升模型表现？提升程度大吗-需要定量百分比？（需要用几个模型做一下小测试）

To assess intervention strategies, we implemented a five-step chain-of-thought (CoT) protocol for spatial reasoning (understanding the 3D shape, analyzing the plane, determining the cross-section, matching cross-section to options, giving the answer, as shown in Figure \ref{fig:CoT}) and conducted example-based training on the SBST geometric cutting test.

As shown in Figure \ref{fig:Training Accuracy}, implementing CoT alone boosted baseline accuracy by 0.100, demonstrating its capacity to enhance visual-semantic alignment and multi-step reasoning. When combined with one-shot example, accuracy further increased by 0.017, suggesting synergistic effects between structured reasoning and pattern recognition.

Additionally, incremental example exposure from one to ten shots initially improved accuracy by 0.159 at 1 shot and 0.209 at 3 shots, but gains plateaued beyond 5 examples (Figure \ref{fig:Training Accuracy}). While examples help VLMs recognize recurring spatial patterns such as prism cross-sections, they fail to address fundamental limitations in dynamic 3D mental simulation, as VLMs overly relied on the general patterns learnt during pre-training process.

%We selected the SBST test to examine the impact of the chain-of-thought (CoT) method and example-based training on accuracy. For the geometric cutting problems in the SBST test, we first designed a five-step reasoning chain:understanding the 3D shape, analyzing the plane, determining the cross-section, matching cross-section to options, giving the answer. We also pre-established an example question. As shown in Figure \ref{fig:Training Accuracy}, the CoT method significantly improved the accuracy of VLMs in handling spatial problems, improving the accuracy by 10\% without the example question and by 6.7\% with the example question. Through this reasoning process, VLMs were able to extend the patterns observed in single-object cutting to more complex cutting scenarios presented in the test questions. Additionally, the models analyzed the similarity between their inferred results and the provided answer choices, thereby improving their accuracy. This finding suggests that logical reasoning can enhance VLMs' ability to comprehend visual information and improve their spatial imagination.

%At the same time, example-based training also contributed to the improvement of VLMs' accuracy to a certain extent. In another experiment, we pre-trained VLMs with 1, 3, 5, and 10 example questions, respectively, and observed their accuracy performance. The results illustrated in Figure \ref{fig:Training Accuracy} indicate that as the number of examples increased, accuracy initially improved rapidly but eventually plateaued. We infer that example-based training helps VLMs better understand the common spatial patterns underlying problem descriptions, thereby enhancing accuracy. However, despite its benefits, example training does not fundamentally improve the model's adaptability to specific spatial tasks, as VLMs remain overly reliant on the general patterns learned during pre-training. According to VLMs' reasoning outputs, employing example-based training while incorporating CoT-based example analysis allows the models to better internalize the CoT approach and enhance performance. As shown in Figure \ref{fig:Training Accuracy}, Example-based training combined with the chain-of-thought (CoT) method improves accuracy more significantly than using either the CoT method or example-based training alone.

\subsubsection{Constraints in VLMs' Basic Spatial Abilities}

Our analysis reveals four fundamental constraints in VLMs’ BSAs. Firstly, VLMs struggle to distinguish subtle shape variations such as hexagon/octagon cross-sections and misinterpret spatial relationships such as interior/exterior boundaries, indicating weak metric encoding in visual representations. This limitation may stem from constraints in training data or a lack of prior knowledge such as geometric principles, which example-based training can partially mitigate. Secondly, even with CoT prompting, VLMs exhibit shallow reasoning chains and cannot dynamically simulate 3D transformations such as mental rotation trajectories, contrasting human parietal lobe-driven simulation mechanisms \cite{jungParietofrontalIntegrationTheory2007}. Thirdly, erratic behaviors like multi-answer selection in single-choice 3D tasks expose poor cross-modal grounding, which is a critical gap between textual instruction parsing and visual feature extraction. Lastly, overreliance on pre-training patterns limits adaptability to novel spatial configurations.

These limitations underscore architectural deficiencies beyond data scarcity. Unlike humans who integrate dorsal and ventral visual streams for spatial processing, VLMs lack dedicated modules for dynamic spatial simulation. Bridging this gap may require hybrid architectures embedding geometric priors and neurosymbolic reasoning.

%Analysis of the VLMs’ reasoning process further reveals that they may struggle to distinguish similar geometric shapes, making it difficult to differentiate subtle variations. Additionally, their understanding of the internal and external relationships between figures often exhibits biases. More notably, even when utilizing the CoT method, VLMs' logical reasoning remains relatively simplistic, making them unable to accurately interpret highly complex or atypical geometric scenarios. This limitation may stem from constraints in training data or a lack of prior knowledge, such as geometric principles, which example-based training can partially mitigate. Furthermore, VLMs' spatial cognition mechanisms exhibit fundamental deficiencies, as they are unable to dynamically simulate spatial transformations in the same way humans do. Their reasoning chains also remain relatively shallow. Addressing these shortcomings is a crucial direction for improving VLMs' spatial reasoning capabilities.

%In specific tests, we observed instances where the models failed to understand the questions correctly. For example, some models selected multiple answers in single-choice questions or chose the same answer consecutively. This phenomenon was particularly pronounced in more complex three-dimensional spatial tasks. These observations suggest that VLMs exhibit deficiencies in the integrated understanding of textual and visual information. Specifically, there remains a significant gap in VLMs' ability to comprehend and respond to visual information through textual instructions.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{accuracy.png}
    \caption{The impact of CoT method and example-based training on the accuracy of VLMs' answers.}
    \label{fig:Training Accuracy}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
% Findings+contributions
% 还要补充findings：不同LLM之间的比较；BSA之间的关系

This study establishes a psychometric framework for evaluating five basic spatial abilities (BSAs) in visual language models (VLMs), benchmarking 13 mainstream models across nine rigorously designed experiments. Our findings reveal three key insights:

First, VLMs exhibit a significant performance gap compared to humans (average score: 24.95 vs. 68.38) while paradoxically mirroring human performance hierarchies across BSAs, excelling in 2D spatial orientation but struggling with 3D mental rotation. The statistical independence of these BSAs validates our framework's discriminative capacity.

Second, model performance varies significantly by manufacturer, reflecting architectural priorities. Qwen-series models demonstrate superior cross-BSA integration, while InternVL2's fragmented performance suggests biases in training data. Contrary to scaling laws, compact models like Qwen2-VL-7B and Gemini-1.5-flash-8B frequently outperform larger counterparts, suggesting that spatial reasoning depends more on architectural optimization than parameter size, with profound implications for efficient model design.

Third, intervention strategies yield measurable but limited improvements. Chain-of-thought (CoT) prompting enhances visual-semantic alignment, improving baseline accuracy by 0.100, while 5-shot training boosts accuracy by 0.259 through pattern recognition. However, these methods plateau at certain limits that VLMs fail to simulate dynamic 3D transformations even with CoT training, and struggle with metric encoding, highlighting architectural constraints beyond training data limitations.

Additionally, our analysis identifies four critical barriers to spatial intelligence: 1) Weak geometric priors in visual representations; 2) Lack of dynamic simulation mechanisms akin to human parietal processing; 3) Disjointed cross-modal grounding; 4) Over-reliance on static pre-training patterns. Overcoming these may require a paradigm shift toward hybrid architectures integrating neurosymbolic reasoning and biologically inspired spatial processing.

In conclusion, by establishing the first quantitative linkage between psychometric BSAs and VLM capabilities, this study provides both a diagnostic toolkit for evaluating spatial intelligence and a roadmap for advancing embodied AI systems. Future progress depends on close collaboration between machine learning and cognitive science communities.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Limitations}

%This study revealed certain limitations. For example, spatial orientation ability was assessed using only one test due to the accessibility to the data from existing studies. Many models performed poorly on this test, making it difficult to discern subtle differences between them. Additionally, three tests involved choosing the correct answer from two options, which increased the likelihood of correct guesses. To avoid distortion of results, models that clearly failed to understand the questions or consistently guessed the same answer were assigned a score of zero for these tests. Future research can build upon the ability framework proposed in this study to curate or design new tests for assessing the basic spatial abilities of VLMs, addressing potential reliability issues of individual tests. It is also essential to include human experimental results as a benchmark.

%The models tested in this study are all VLMs released in recent months, and the number of multimodal models with visual capabilities remains limited. Future research can use the nine tests employed in this study to evaluate the basic spatial abilities of newly developed VLMs and compare their performance to the models tested here, highlighting progress over time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Ethics Statement}

This study utilized the API service provided by OpenAl, DeepInfra, Google, and SiliconFlow in full compliance with their terms of service and usage policies.

\bibliography{anthology,custom,xwr_references}
\label{section:bib}
\bibliographystyle{acl_natbib}


\appendix
\onecolumn


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Statistics}
As a necessary complement to the results in Section \ref{section:results}, this section provides the original statistics of the data from VLMs' BSA evaluation. The complete results are listed in Table \ref{tab:test_result}.

\begin{table}[H]
\centering
\caption{Test results of 13 VLMs and human recipients.}
\label{tab:test_result}
\resizebox{1\linewidth}{!}{%
\begin{tabular}{
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}l }

\toprule[1pt]

\textbf{} &
  \textbf{SVT} &
  \textbf{NCIT} &
  \textbf{DATSR} &
  \textbf{RCSR} &
  \textbf{MRMT} &
  \textbf{MRT} &
  \textbf{PSVTR} &
  \textbf{SBST} &
  \textbf{RCVis} \\

\midrule[0.5pt]

{\color[HTML]{4527A0} \textbf{Qwen2-VL-72B-Instruct}} &
  \begin{tabular}[c]{@{}l@{}}15.63\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}18.75\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}10.57\\ (1.67)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}38.19\\ (0.98)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}41.67\\ (1.47)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}29.17\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}20.00\\ (2.72)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}47.78\\ (1.57)\end{tabular} &
  / \\

\midrule[0.5pt]

{\color[HTML]{9575CD} \textbf{Qwen2-VL-7B-Instruct}} &
  \begin{tabular}[c]{@{}l@{}}30.21\\ (1.47)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}37.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}11.46\\ (0.75)\end{tabular} &
  / &
  \begin{tabular}[c]{@{}l@{}}47.92\\ (1.47)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}20.83\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}18.89\\ (1.57)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}25.56\\ (1.57)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}41.67\\ (0)\end{tabular} \\
\midrule[0.5pt]

{\color[HTML]{1976D2} \textbf{InternVL2-Llama3-76B}} &
  \begin{tabular}[c]{@{}l@{}}21.88\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}18.75\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}17.82\\ (1.19)\end{tabular} &
  / &
  / &
  \begin{tabular}[c]{@{}l@{}}25.00\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}26.67\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}15.56\\ (1.57)\end{tabular} &
  / \\
\midrule[0.5pt]

{\color[HTML]{2196F3} \textbf{InternVL2-26B}} &
  \begin{tabular}[c]{@{}l@{}}12.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}37.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}12.15\\ (1.18)\end{tabular} &
  / &
  \begin{tabular}[c]{@{}l@{}}53.13\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}12.50\\ (3.40)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}21.11\\ (1.57)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}23.33\\ (7.20)\end{tabular} &
  / \\
\midrule[0.5pt]

{\color[HTML]{64B5F6} \textbf{InternVL2-8B}} &
  \begin{tabular}[c]{@{}l@{}}12.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}43.75\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}9.00\\ (0)\end{tabular} &
  / &
  / &
  \begin{tabular}[c]{@{}l@{}}12.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}20.00\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}26.67\\ (2.72)\end{tabular} &
  / \\
\midrule[0.5pt]

{\color[HTML]{00695C} \textbf{Llama-3.2-90B-Vision-Instruct}} &
  \begin{tabular}[c]{@{}l@{}}9.38\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}12.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}8.50\\ (0)\end{tabular} &
  / &
  \begin{tabular}[c]{@{}l@{}}53.13\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}29.17\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}20.00\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}6.67\\ (0)\end{tabular} &
  / \\
\midrule[0.5pt]

{\color[HTML]{4DB6AC} \textbf{Llama-3.2-11B-Vision-Instruct}} &
  \begin{tabular}[c]{@{}l@{}}21.88\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}50.00\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}26.45\\ (0)\end{tabular} &
  / &
  \begin{tabular}[c]{@{}l@{}}50.00\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}22.22\\ (1.96)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}10.00\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}18.89\\ (1.57)\end{tabular} &
  / \\
\midrule[0.5pt]

{\color[HTML]{5D4037} \textbf{GPT-4o}} &
  \begin{tabular}[c]{@{}l@{}}20.83\\ (5.31)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}37.50\\ (5.10)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}10.10\\ (3.12)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}46.53\\ (1.96)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}44.79\\ (8.20)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}12.50\\ (5.89)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}21.11\\ (5.67)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}36.67\\ (7.20)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}40.56\\ (2.83)\end{tabular} \\
\midrule[0.5pt]

{\color[HTML]{795548} \textbf{GPT-4o mini}} &
  \begin{tabular}[c]{@{}l@{}}17.71\\ (7.80)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}18.75\\ (10.21)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}8.19\\ (1.09)\end{tabular} &
  / &
  \begin{tabular}[c]{@{}l@{}}42.71\\ (1.47)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}2.78\\ (1.96)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}15.56\\ (3.14)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}21.11\\ (1.57)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}46.11\\ (1.57)\end{tabular} \\
\midrule[0.5pt]

{\color[HTML]{A1887F} \textbf{GPT-4 Turbo}} &
  \begin{tabular}[c]{@{}l@{}}23.96\\ (3.90)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}18.75\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}16.08\\ (3.78)\end{tabular} &
  / &
  \begin{tabular}[c]{@{}l@{}}42.71\\ (1.47)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}12.50\\ (3.40)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}14.44\\ (6.85)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}17.78\\ (4.16)\end{tabular} &
  / \\
\midrule[0.5pt]

{\color[HTML]{F4511E} \textbf{Gemini-1.5-pro}} &
  \begin{tabular}[c]{@{}l@{}}21.88\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}18.75\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}23.93\\ (0.98)\end{tabular} &
  / &
  \begin{tabular}[c]{@{}l@{}}54.17\\ (2.95)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}4.17\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}18.89\\ (1.57)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}36.67\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}40.00\\ (1.36)\end{tabular} \\
\midrule[0.5pt]

{\color[HTML]{FF7043} \textbf{Gemini-1.5-flash}} &
  \begin{tabular}[c]{@{}l@{}}12.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}62.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}25.05\\ (0)\end{tabular} &
  / &
  / &
  \begin{tabular}[c]{@{}l@{}}16.67\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}16.67\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}30.00\\ (0)\end{tabular} &
  / \\
\midrule[0.5pt]

{\color[HTML]{FFAB91} \textbf{Gemini-1.5-flash-8B}} &
  \begin{tabular}[c]{@{}l@{}}15.63\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}37.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}16.72\\ (0.59)\end{tabular} &
  / &
  \begin{tabular}[c]{@{}l@{}}56.25\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}12.50\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}10.00\\ (0)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}26.67\\ (0)\end{tabular} &
  / \\
\midrule[0.5pt]

\textit{\textbf{Average of 13 VLMs}} &
  \textit{18.19} &
  \textit{31.37} &
  \textit{15.08} &
  \textit{5.90} &
  \textit{37.42} &
  \textit{16.35} &
  \textit{17.95} &
  \textit{25.64} &
  \textit{12.95} \\
\midrule[0.5pt]

Standard Deviation of 13 VLMs &
  5.87 &
  15.19 &
  6.54 &
  16.00 &
  21.84 &
  8.49 &
  4.62 &
  10.60 &
  20.26 \\
\midrule[0.5pt]

\textit{\textbf{Average of human recipients}} &
  \textit{58.47} &
  \textit{55.00} &
  \textit{55.33} &
  \textit{89.00} &
  \textit{84.69} &
  \textit{45.00} &
  \textit{63.60} &
  \textit{68.00} &
  \textit{88.00} \\
\midrule[0.5pt]

Standard Deviation of human &
  19.72 &
  19.13 &
  / &
  9.00 &
  14.41 &
  20.83 &
  20.53 &
  23.00 &
  7.00 \\
\midrule[0.5pt]

Number of human recipients&
  1007 &
  105 &
  1480 &
  51 &
  61 &
  636 &
  1022 &
  223 &
  52 \\
\bottomrule[1pt]
\end{tabular}
}
\end{table}

\begin{table}[H]
\centering
\caption{VLMs' performance in Santa Barbara Solids Test with and without explicit separation of answer choices.}
\label{tab:option_results}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c }
\toprule[1pt]
\textbf{Model} & \textbf{Original Experiment} & \textbf{Supplementary Experiment} \\ \midrule[0.5pt]
Qwen2-VL-72B-Instruct & 47.78 & 23.33 \\
Qwen2-VL-7B-Instruct & 25.56 & 26.67 \\
GPT-4o & 36.67 & 33.33 \\
GPT-4o mini & 21.11 & 26.67 \\
GPT-4 Turbo & 17.78 & 13.33 \\ \bottomrule[1pt]
\end{tabular}%
}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prompts for VLMs}
In this section, we provide the designed prompts for the nine BSA tests to receive results from VLMs.

\subsection{MGMP Spatial Visualization Test (SVT)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{prompts-05.jpg}
    \label{fig:prompts_SVT}
\end{figure}


\subsection{Net Cube Imagination Test (NCIT)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{prompts-06.jpg}
    \label{fig:prompts_NCIT}
\end{figure}


\subsection{Differential Aptitude Test: Space Relation (DAT:SR)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{prompts-07.jpg}
    \label{fig:prompts_DATSR}
\end{figure}


\subsection{R-Cube-Spatial Relation Test (R-Cube-SR)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{prompts-08.jpg}
    \label{fig:prompts_RCUBESR}
\end{figure}


\subsection{Money Road-Map Test (MRMT)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{prompts-09.jpg}
    \label{fig:prompts_MRMT}
\end{figure}


\subsection{Mental Rotation Test (MRT)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{prompts-10.jpg}
    \label{fig:prompts_MRT}
\end{figure}

\clearpage

\subsection{Purdue Spatial Visualization Tests: Visualization of Rotations (PSVT:R)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{prompts-11.jpg}
    \label{fig:prompts_PSVTR}
\end{figure}


\subsection{Santa Barbara Solids Test (SBST)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{prompts-12.jpg}
    \label{fig:prompts_SBST}
\end{figure}

\clearpage

\subsection{R-Cube-Visualization Short Test (R-Cube-Vis)}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{prompts-13.jpg}
    \label{fig:RCUBEVIS}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CoT Method and Example-based Training Demonstration}
In this section, we selected the SBST test, using GPT-4o to evaluate the extent to which the chain-of-thought (CoT) method and example-based training enhance the model's ability to solve spatial problems.

In the first experiment, we designed a structured CoT reasoning process (understanding the 3D shape, analyzing the plane, determining the cross-section, matching cross-section to options, giving the answer) along with a single example question. We then assessed the model's accuracy under four different conditions: (a) directly answering without additional guidance, (b) using only example-based training, (c) employing only the CoT method, and (d) combining both CoT and example-based training, where the example question was analyzed using the CoT approach.

In the second experiment, we examined the impact of varying the number of example questions (1, 3, 5, and 10) on response accuracy, without incorporating the CoT method.

To enhance the reliability of the findings, both experiments were conducted multiple times to take the average result. The experimental framework is illustrated in Figure \ref{fig:CoT}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{cot-05-05.png}
    \caption{Framework of CoT Method and Example-based Training}
    \label{fig:CoT}
\end{figure}

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sample Data Demonstration}
In this section, we take Qwen2-VL-7B, the model that acquired the highest score as an example, to demonstrate its response to nine sample questions of different tests, as shown in Figure \ref{fig:Data Samples}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{tests_table_big.jpg}
    \caption{Data samples from tests for BSA\footnotemark[2].}
    \label{fig:Data Samples}
\end{figure}


\end{document}
