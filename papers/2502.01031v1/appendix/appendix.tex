\newcommand{\smallsection}[1]{\noindent\underline{\smash{\textbf{#1:}}}}

\clearpage
\appendix
\vspace{1mm}
\begin{center}
    \Large{\bf \method: APPENDIX} \\
    \vspace{-1mm}
\end{center}

\section{Proofs}\label{sec:pfs_A}
% We provide proofs for our theoretical claims.
\subsection{Proof of Theorem~\ref{thm:np_hard}}\label{app:proof:np_hard}
\begin{proof}
    We consider an NP-hard problem called minimum $k$-union~\citep{vinterbo2002note}, and
    we shall show that given each instance of minimum $k$-union, we can construct an instance of influence minimization, such that if the instance of influence minimization is our problem is solved, then the original instance of minimum $k$-union is solved.
    Given a collection of $m$ set $S_1, S_2, \ldots, S_m$ and a positive integer $k$, the minimum $k$-union problem aims to find $k$ sets ($S_{i_1}, S_{i_2}, \ldots, S_{i_k}$) such that $\vert \bigcup_{j} S_{i_j} \vert$ is minimized.
    Let 
    \[X = \{x_1, x_2, \ldots, x_n\} = \bigcup_{j \in [m]}S_j.\]
    Given any instance of minimum $k$-union, we construct the following instance of influence minimization:
    we have a single seed node $S = \{v_{init}\}$,
    $m$ nodes $v^{(S)}_{j}$ for $j \in [m]$, and
    $n$ nodes $v^{(X)}_{i}$ for $i \in [n]$.
    We have $m$ edges from $v_{init}$ to each $v^{(S)}_{j}$, 
    and edge from $v^{(S)}_{j}$ to $v^{(X)}_{i}$ if and only if $x_i \in S_j$, for each $(i, j)$ pair.
    Also, each edge $e$ has an activation probability $p(e) = 1$.
    Now, it is easy to see that the original instance of minimum $k$-union is equivalent to finding $m - k$ edges between $v_{init}$ and $v^{(S)}_{j}$ such that removing those $m - k$ edges will minimize the expected number of activated nodes in the whole process (i.e., the objective in influence minimization).
    Hence, it suffices to show that you cannot do better by removing other edges.
    Indeed, if you remove an edge from $v^{(S)}_{j}$ to $v^{(X)}_{i}$, then replacing it by removing the edge from $v_{init}$ to $v^{(S)}_{j}$ will give at least the same, or better, minimization performance.
\end{proof}

\subsection{Proof of Lemma~\ref{lem:gnn_diff}}\label{app:proof:gnn_diff}
\begin{proof}
We have
\[
\frac{\partial \GNN_{\theta}(v; G, \Tilde{p}_{\Tilde{r}}, S)}{\partial \Tilde{r}} = \frac{\partial \GNN_{\theta}(v; G, \Tilde{p}_{\Tilde{r}}, S)}{\partial \Tilde{p}_{\Tilde{r}}} \frac{\partial \Tilde{p}_{\Tilde{r}}}{\partial \Tilde{r}},
\]
    where    
    \[
    \frac{\partial \Tilde{p}_{\Tilde{r}}(v, u)}{\partial \Tilde{r}(v', u')} = p(v, u) \mathbb{1}(v = v' \land u = u').
    \]    
    As mentioned in Sec.~\ref{subsec:method:ours_greedy}, $p$ is used as edge weights in the input graph of GNN, and thus $\frac{\partial \GNN_{\theta}(v; G, \Tilde{p}_{\Tilde{r}}, S)}{\partial \Tilde{p}_{\Tilde{r}}}$ is also well-defined.
    Hence, $\frac{\partial \GNN_{\theta}(v; G, \Tilde{p}_{\Tilde{r}}, S)}{\partial \Tilde{r}}$ is well-defined.
\end{proof}

%\subsection{Proof of Lemma~\ref{lem:relax_good}}\label{app:proof:relax_good}
\subsection{Proof of the meaningfulness of the loss Function}\label{app:proof:relax_good}
%First, let us provide a formal statement of Lemma~\ref{lem:relax_good}:
\begin{lemma}\label{lem:relax_good}
    Given a test instance $\calT = (G = (V, E), p, S, b)$, let 
    \[
    \sigma^* \coloneqq \min_{\calE \subseteq E, \Abs{\calE} = b} \sigma(S; G_{\setminus \calE}, p_{\setminus \calE}).
    \]
    Assume that 
    (1) $\GNN_\theta$ is well trained, i.e.,
    \[
    \Abs{\GNN_\theta(v; G, p, S) - \pi(v; G, p, S)} \leq \epsilon, \forall v
    \]
    for some $\epsilon > 0$ and
    (2) $\alpha$ and $\beta$ are sufficiently large,
    then each $r^* \in \arg \min_{\Tilde{r}} \calL_O(\Tilde{r})$ satisfies that
    (1) $r^*$ is discrete, 
    i.e., 
    \[
    r^*(e) \in \Set{0, 1}, \forall e \in E,
    \]
    (2) $r^*$ satisfies the budget constraint,
    i.e.,
    \[
    \sum_{e \in E} r^*(e) = \Abs{E} - b,
    \]
    and
    (3) $r^*$ is a good solution, i.e., 
    \[
    \sigma(S; G_{\setminus \calE^*}, p_{\setminus \calE^*}) - \sigma^* \leq 2\Abs{V}\epsilon,
    \]
    where 
    \[
    \calE^* = \calE^*(r^*) = \Set{e \in E \colon r^*(e) = 0}.
    \]
\end{lemma}

\begin{proof}
    Since $\alpha$ and $\beta$ are sufficiently large and there exists
    $\tilde{r}: E \to [0, 1]$ such that 
    (1) $\calL_{\text{budget}}$ is minimized, i.e.,
    \[
    \calL_{\text{budget}}(\tilde{r}) = 0,
    \]
    and
    (2) $\calL_{\text{certainty}}$ is minimized, i.e.,
    \[
    \calL_{\text{certainty}}(\tilde{r}) = 0,
    \]
    $r^*$ must satisfy that $\calL_{\text{budget}}(r^*) = \calL_{\text{certainty}}(r^*) = 0$, which is equivalent to $r^*(e) \in \Set{0, 1}, \forall e \in E$ and $\sum_{e \in E} r^*(e) = \Abs{E} - b$.
    In fact, all the discrete and budget-satisfying probabilistic decisions $\Tilde{r}$ satisfy that     
    $\calL_{\text{budget}}(\Tilde{r}) = \calL_{\text{certainty}}(\Tilde{r}) = 0$,
    and we have 
    \[
    r^* \in \arg \min_{\Tilde{r}: E \to \Set{0, 1}, \sum_{e \in E} \tilde{r}(e) = \Abs{E} - b} \calL_{\text{obj}},
    \]
    which is equivalent to 
    \[
    r^* \in \arg \min_{\Tilde{r}: E \to \Set{0, 1}, \sum_{e \in E} \tilde{r}(e) = \Abs{E} - b} \sum_{v \in V} \GNN_\theta(v; G, \Tilde{p}_{\Tilde{r}}, S).
    \]    
    Define $\calE_{r} \coloneqq \Set{e \in E \colon r(e)}$ for each $r: E \to \Set{0, 1}$, and 
    define
    \[    
    \Tilde{r}_{\calE}(e) = \mathbb{1}(e \notin \calE), \forall e \in E.
    \]
    Let
    \[
    \calE_{min} \in \arg \min_{\calE \subseteq E, \Abs{\calE} = b} \sigma(S; G_{\setminus \calE}, p_{\setminus \calE}),
    \]    
    for each $v \in V$, we have
    \begin{align*}
     \pi(v; G_{\setminus \calE_{r^*}}, p_{\setminus \calE_{r^*}}, S) 
     &\leq \GNN_\theta(v; G, \Tilde{p}_{r^*}, S) + \epsilon \\
     &\leq \GNN_\theta(v; G, \Tilde{p}_{\Tilde{r}_{\calE_{min}}}, S) + \epsilon \\
    & \leq \pi(v; G_{\setminus \calE_{min}}, p_{\setminus \calE_{min}}, S) + 2\epsilon.
    \end{align*}
    Taking the summation over $v \in V$ completes the proof.
\end{proof}

\begin{remark}\label{rem:regularization}
    %In practice, however, using too large $\alpha$ and $\beta$ would make $\calL_{\text{budget}}$ and $\calL_{\text{certainty}}$ dominant and thus impair the optimization performance w.r.t. the main objective $\calL_{\text{obj}}$.    
    {In practice, however, using too large $\alpha$ and $\beta$ would make $\calL_{\text{budget}}$ and $\calL_{\text{certainty}}$ dominant and thus impair the optimization performance w.r.t. the main objective $\calL_{\text{obj}}$.
    See Sec.~\ref{subsec:exp:ablation} for the ablation studies on $\alpha$ and $\beta$.} %$\calL_\text{budget}$ and $\calL_\text{certainty}$.
\end{remark}


\section{Extension to other spread models}\label{sec:diff_model}

We performed additional experiments under different realistic influence spread models: the linear threshold (LT) model~\citep{kempe2003maximizing} and the general Markov chain susceptible-infected-recovered (G-SIR) model~\citep{yi2022edge}.

\smallsection{Model definition}
Below, we describe the definitions of the LT model and the G-SIR model.

\begin{definition}[Linear threshold (LT) model]\label{def:LT}
    Given 
    (1) a graph $G = (V, E)$ and
    % (2) activation probabilities $p: E \rightarrow [0,1]$,
    (2) a seed set $S \subseteq V$,
    $\operatorname{LT}(G, S)$ is a stochastic process as follows:
    % \vspace{-2.5mm}
    \begin{itemize}[leftmargin=*,topsep=0pt]
        \item \textbf{Initialization:}         
        At time step $t = 0$, each seed node $v_S \in S$ is activated, and each non-seed node remains inactive. For each $v \in V$, activation threshold $p_v$, drawn from continuous uniform distribution $U(0, 1)$, is assigned to $E$.
        \item \textbf{Diffusion steps:}
        At each step $t \geq 1$,
        each inactive node $v$ until the previous step is newly activated when the ratio of its active in-neighbors exceeds the activation threshold $p_v$.
        Each activated node remains active for the whole process, and the process terminates when no node is activated in the previous step.
    \end{itemize}
\end{definition}

\begin{definition}[General Markov chain susceptible-infected-recovered (G-SIR) model]\label{def:GSIR}
    Given 
    (1) a graph $G = (V, E)$,
    (2) activation probabilities $p: E \rightarrow [0,1]$,
    (3) a recovery probability $r$,
    and
    (4) a seed set $S \subseteq V$,
    $\operatorname{G-SIR}(G, S)$ is a stochastic process as follows:
    % \vspace{-2.5mm}
    \begin{itemize}[leftmargin=*,topsep=0pt]
        \item \textbf{Initialization:}         
        At time step $t = 0$, each seed node $v_S \in S$ is activated, and each non-seed node remains inactive.
        \item \textbf{Diffusion steps:}
        At each step $t \geq 1$,
        each active node $v$ at the previous step becomes inactive again with the recovery probability $r$ and activates each of its inactive out-neighbor $u$ with activation probability $p(v, u)$. 
        Each recovered node remains inactive for the whole process.
        As long as each activated node stays activated, other non-recovered nodes can be activated during the process. The process terminates when no node is activated in the previous step.
    \end{itemize}
\end{definition}

In the G-SIR model, each edge has its individual (and possibly different) propagation probability, while in the original SIR model, the propagation probability is the same for all edges. 
In our experiments, we set the recovery probability $r = 0.5$.

\smallsection{Extensions}
Each considered method (see Sec.~\ref{subsec:exp:setting}) was straightforwardly extended for the two additional influence spread models.
For example, for extending \method, we modified the influence-estimation component (i.e., the surrogate-model GNN; see Alg.~\ref{algo:gnn}) to use the corresponding influence spread model.
% the IC model used for influence estimation in Alg.~\ref{algo:gnn} with each model.

\smallsection{Experimental results}
As in Sec.~\ref{sec:exp_perf}, we measured the average reduced ratio of influence and the average running time across all the test seed sets. 
We used the same process described in Sec.~\ref{subsec:exp:setting} to generate the training and test data for the LT and G-SIR models. 
As shown in Fig.~\ref{fig:app_perf_others}, the results followed a similar trend presented in Sec. \ref{sec:exp_perf}. 
That is, \advp was significantly faster than the most effective baseline, while achieving a similar reduced ratio.
For \WL, \RIS-$\epsilon$ ran out of time for every $\epsilon \in \{0.2, 0.4, 0.6\}$, leaving no reasonably comparable baselines.

\begin{figure}[t!]
    % \vspace{-1mm}
    \centering
    \begin{subfigure}{0.95\linewidth}
        \includegraphics[width=\linewidth]{appendix/figures/performance_LT_5.pdf}
        \caption{LT, $b=5$}
    \end{subfigure}
    \begin{subfigure}{0.95\linewidth}
            \includegraphics[width=\linewidth]{appendix/figures/performance_LT_10.pdf}
        \caption{LT, $b=10$}
    \end{subfigure}
    \begin{subfigure}{0.95\linewidth}
        \includegraphics[width=\linewidth]{appendix/figures/performance_SIR_5.pdf}
        \caption{G-SIR, $b=5$}
    \end{subfigure}
    \begin{subfigure}{0.95\linewidth}
        \includegraphics[width=\linewidth]{appendix/figures/performance_SIR_10.pdf}
        \caption{G-SIR, $b=10$}
    \end{subfigure}
    \begin{subfigure}{0.95\linewidth}
        \includegraphics[width=\linewidth]{appendix/figures/legend_baselines.pdf}
    \end{subfigure}
    
    \caption{The effectiveness (the reduced ratio of influence) and running time of each method, with budget $b \in \Set{5, 10}$ on LT and G-SIR models. We compared the running time of the best baseline to one of our methods with the most similar reduced ratio. Except for (d) on \CL and \EL, all \method versions were Pareto-optimal, i.e., no baseline is faster \textit{and} more effective than any version.
    }
    % \vspace{-2mm}
    \label{fig:app_perf_others}
\end{figure}

%\clearpage

\input{appendix/algos/greedy}
\input{appendix/algos/gnn_train}

\section{Pseudo-codes of algorithms}\label{sec:alg}
We provide the pseudo-codes of the naive incremental greedy algorithm (Alg.~\ref{algo:greedy}; see Sec.~\ref{subsec:method:naive_greedy}) and GNN training (Alg.~\ref{algo:gnn}; see Sec.~\ref{subsec:method:ours_greedy}).



\section{Additional related work}\label{app:rel_wk_detail}
Here, we provide more details of the related work on influence minimization.

\smallsection{Node removal}
The following works considered influence minimization with node removal.
\citet{wang2013negative} employed an incremental greedy algorithm under the IC model.
\citet{chang2016guarantee} focused on the outbreak phenomenon under the IC model and addressed the problem of maximizing the probability that the influence remains below a certain threshold, employing a greedy algorithm based on Monte Carlo simulation.
\citet{zheng2018least} employed a greedy algorithm to remove bridge end nodes to prevent propagation to other communities.
\citet{zhu2021misinformation} removed node groups from given candidate node groups in an acyclic graph under a propagation model that extends the IC model by incorporating ECE (echo chamber effect).
\citet{xie2023minimizing} accelerated the influence estimation in the greedy algorithm using a dominator tree.
\citet{ni2023misinformation} considered protecting specific nodes in multi-social networks by forcing them not to be activated.

\smallsection{Edge removal}
The following works considered influence minimization with edge removal.
\citet{Kimura2009blocking} minimized the average (or maximum) influence when each node is a seed set under the IC model using a greedy algorithm and accelerated the process with the bond percolation method.
\citet{tong2012gelling} minimized the eigenvalue of the graph under the susceptible-infectious-susceptible (SIS) model.
\citet{khalil2014scalable} minimized the influence when one of the nodes in the given source set became a seed node under the LT model and employed a greedy algorithm that quickly calculates marginal loss using a live-edge tree.
\citet{yao2015minimizing} applied a greedy algorithm to the influence minimization problem identical to the one considered by us (i.e., Problem~\ref{problem:rumor_blocking}).
\citet{yan2019rumor} addressed a problem identical to ours but with the condition of acyclic graphs. They analyzed the problem from the perspective of marginal decrement and proposed a heuristic algorithm based on propagation ability.
\citet{yi2022edge} accelerated the greedy algorithm in the G-SIR model by using reverse influence sampling.
\citet{zareie2022rumour} minimized the spreading ability of the graph when the seed set is not given.
\citet{wang2020efficient} proposed a robust sampling-based greedy algorithm to protect a given target node in the LT model.
\citet{jiang2022rumordecay} protected given target nodes in the SIR model, by identifying critical edges through sampling paths from the seed nodes to the target nodes

\smallsection{Active defense}
The following works proposed active defense to counter negative propagation.
\citet{budak2011limiting} used a Monte Carlo simulation-based greedy algorithm in the IC model and compared it with some heuristics.
\citet{he2012influence} estimated the influence of each node by using a locally directed acyclic graph in the LT model.
\citet{fan2013least} selected bridge ends to protect other communities from propagation using a greedy algorithm in the opportunistic one-activate-one model and deterministic one-activate-many model.
\citet{luo2014time} employed a Monte Carlo simulation-based greedy algorithm in the continuous-time multiple campaign diffusion model.
\citet{zhu2016minimum} efficiently calculated single-hop spread to estimate influence in the IC model.
\citet{tong2019beyond} developed a hybrid sampling process in the IC model that attaches high weights to the users vulnerable to misinformation.
\citet{hosni2019darim} used a greedy algorithm that simultaneously performs node blocking and active defense.

%\clearpage

\input{appendix/algos/mds}
\input{appendix/algos/mbpm}

\section{Additional details of baseline methods}\label{sec:app_base}
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt]
    \item \textbf{\MDS}~\cite{yan2019rumor} estimates the incremental change in the influence when each edge is removed, and greedily chooses $b$ edges w.r.t. the incremental changes. In Alg.~\ref{algo:MDS}, we provide the pseudo-code of MDS. 
    The edges are chosen according to the influenced probability and rumor-spread ability of their endpoints, with incremental updates. 
    \item \textbf{BPM}~\cite{Kimura2009blocking} uses the bond percolation method (BPM) to estimate the importance of edges, and removes the top-$b$ edges w.r.t. importance scores. BPM does not consider specific seed nodes.
    \item \textbf{Modified \BPM (\MBPM)} is a modified version of \BPM that considers specific seed nodes.
    % The original BPM method does not consider specific seed nodes (but the average influence of all the nodes), and thus we modified BPM to consider specific seed nodes as in Problem~\ref{problem:rumor_blocking}.
    Specifically, we count the number of nodes reachable from the seed set $S$, in a sampled graph according to the activation probabilities $p$. We provide its pseudo-code in Alg.~\ref{algo:MBPM}.
    %Its time complexity is $O(bd|E|)$ where $b$ is the number of edges that will be deleted, and $d$ is a sampling number in a single BPM. It still needs enough number of sampling in a BPM. we tested BPM with sampling number of 100, 1000, 10,000 and 100,000 times
    \item \textbf{\RIS}~\cite{yi2022edge} randomly samples a node and an activated graph each round, checks for each edge whether the node is reachable from the set of seeds without traversing the edge, and removes bottom-$b$ edges w.r.t. the success rate. Let $\tilde{\sigma}(S; G, p)$ be the number of infected nodes (including recovered nodes for the G-SIR model~\cite{yi2022edge}) except for the seed set. 
    According to Proposition 8.1 in~\cite{yi2022edge}, a $(1\pm \epsilon)$-approximation of $\tilde{\sigma}(S; G, p)$ is obtained with $O(\epsilon^{-2}n\log n)$ rounds of sampling an activated graph and a node. Therefore, as the error term $\epsilon$ decreases, a larger number of rounds is required.
    In our experiments, we conduct $0.1\cdot\epsilon^{-2}n\log n$ rounds for $\epsilon=0.2$, $0.4$, and $0.6$, respectively.
\end{enumerate}

\section{Additional details of experimental settings}\label{sec:app_setting}

\smallsection{\method}
We implemented \method in Python with the PyG library~\citep{fey2019fast} using the Adam optimizer~\cite{kingma2014adam}.
The initial node features were one-dimensional binary: $1$ if the node is a seed node, and $0$ otherwise.
For GNN training (Alg.~\ref{algo:gnn}), the learning rate and its decaying rate were optimized by Optuna~\cite{optuna_2019} with 2000 epochs.

\smallsection{Hardware}
All the versions of \method were run on a machine with 
2.10GHz Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} Silver 4210R processors and RTX2080Ti GPUs.
A single GPU was used for each experiment.
The baseline methods did not use GPUs, and they were run on a machine with more powerful CPUs, 3.70GHz Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i9-10900KF processors.

\section{Additional experimental results}\label{sec:app_exp}

\subsection{Additional results for Q1. Performances}\label{sec:app_perf}
We reported the effectiveness (the reduced ratio of influence) and the running time, along with the standard deviations, for each method, for each budget $b \in \Set{3,5,7,10}$, in Fig.~\ref{fig:app_perf} and Table~\ref{tab:app_perf}.
The standard deviations were computed over different seed sets, which could be high since the seed sets could be highly different. 
% Overall, the results showed a fair trade-off between time and the reduction ratio except for \advp. 
% It's important to note that the standard deviation can be substantial due to variations in influence resulting from different seed sets.

%\subsection{Time for training and running GNN}\label{app:GNN_time}
%We reported the training time for GCN in Table~\ref{tab:GNN_time_training} and compared the running time of GCN with MC simulation in Table~\ref{tab:GNN_time_running}. 
%Overall, the running time of GCN was at least 100 times shorter than the time taken by the MC simulation.

\subsection{Budget scalability}\label{app:scal} 
We conducted experiments with all the considered methods with budget $b$ increasing from $1$ to $10$.
In Table~\ref{tab:budget_min}, for each method and each dataset, we reported the minimum value of $b$ with which the method ran out of time (i.e., took more than one hour on a single seed set), where ``.''  indicates that the method did not run out of time even with $b = 10$.
The methods whose outputs do not depend on the seed set were not included.
Among the versions of \method, only \naive failed on the largest dataset \WL.
Among the baseline methods, MBPM had the lowest budget scalability, and \Greedy did not scale well on large graphs.

\subsection{Variants of \method}\label{app:together1by1} 
For \adv, one can optimize $\Tilde{r}$ and then choose the bottom-$b$ edges with the lowest $\Tilde{r}$ values together instead of removing edges one by one for each $n_{ep}$ epochs.
In Fig.~\ref{fig:together1by1}, we reported the effectiveness of the variant (selecting edges together after $bn_{ep}$ epochs) compared to original \adv (selecting edges one by one) with different budgets $b\in [10]$ for each dataset.
The effectiveness of the variant of \adv was nearly the same as the original \adv.

For \advp, one can choose the top-$b$ edges with the largest gradient together.
In Fig.~\ref{fig:together1by1_I}, we reported the effectiveness of the variant (selecting edges together instantly) compared to the original \advp (selecting edges one by one).
The effectiveness of the variant of \advp was noticeably lower than that of original \advp, except on the CL dataset.

\subsection{Estimation errors along training}\label{app:estim_error}
As shown in Fig.~\ref{fig:val_error_6layer}, the estimation errors on the validation set decreased as the training proceeded.
Overall, the trained GCNs achieved good estimation quality, with errors at most 4.5\% of the ground-truth influence.




\subsection{Performance on large-scale datasets}\label{sec:larger_dataset}

We additionally conducted experiments on three large-scale datasets: cit-HepTh~\citep{leskovec2005graphs, gehrke2003overview}, email-EuAll~\citep{leskovec2007graph}, and twitter~\citep{leskovec2012learning}. All these datasets are available at SNAP~\citep{snapnets}. We provided their basic statistics in Table~\ref{tab:larger_data}.
Due to the absence of realistic activation probabilities, for these datasets, we used the weighted cascade model~\citep{kempe2003maximizing}, a special case of the IC model where the activation probability of each edge from node $u$ to $v$ is $1$ divided by the in-degree of $v$.

The results with a budget of $b=5$ and $b=10$ are presented in Figure~\ref{fig:app_perf_larger}. The proposed method consistently and significantly outperformed the baseline methods that completed within the time and memory limits. Here, \BPM, \naive(naive), \RIS, and \MBPM-100000 ran out of time.






%\clearpage

\input{appendix/tabs/full_res}

%\FloatBarrier

\input{appendix/figures/full_res}

\FloatBarrier
% \clearpage



% \begin{table}[p]
%   \centering
%   \vspace{2mm}
%   \caption{Average training time (in seconds) for training each GCN model.}
%   \scalebox{1.0}{%
%     \begin{tabular}{r|r|r}
%     % \toprule
%     \hline
%     \multicolumn{1}{c|}{\WL}   & \multicolumn{1}{c|}{\CL}   & \multicolumn{1}{c}{\EL} \bigstrut \\
%     % \midrule
%     \hline
%     17,227 & 7,390 & 6,818 \bigstrut \\
%     % \bottomrule
%     \hline
%     \end{tabular}
%   }
%   \label{tab:GNN_time_training}
% \end{table}




\begin{table}[h!]
  \centering
  % \vspace{1mm}
  \scalebox{0.8}{%
    \begin{tabular}{l||c|c|c}
        \hline
        methods & \WL & \CL & \EL  \bigstrut \\
        \hline
        \hline        
        \MDS & .  &. &. \bigstrut \\
        \hline
        % \MBPM-10 & . & . & . \bigstrut[t] \\
        % \MBPM-100 & . & . & . \\
        % \MBPM-1000 & .  &. &.\\
        \MBPM-10000 & .  &. &.  \bigstrut[t] \\
        \MBPM-100000 & 3 & 5 & 7 \bigstrut[b] \\
        \hline
        \Greedy-10 & . & . & . \bigstrut[t] \\
        \Greedy-100 & 2  &. &. \bigstrut[b] \\
        \hline
        \RIS-0.6 & 5 & . & . \bigstrut[t]\\
        \RIS-0.4 & 2 & . & . \\
        \RIS-0.2 & 1 & 6 & 10 \bigstrut[b] \\
        \hline
        \naive & 6  &. &. \bigstrut[t] \\
        \adv & .  &. &. \\
        \advp & .  &. &. \bigstrut[b]\\
    \hline
    \end{tabular}
    }
    \caption{The minimum budget $b \leq 10$ with which each method runs out of time. Each cell with ``.'' implies that the method did not run out of time even with $b = 10$.}
    \label{tab:budget_min}
\end{table}


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{appendix/figures/together1by1_O.pdf}
    \includegraphics[width=0.7\linewidth]{appendix/figures/legend_together1by1_O.pdf}
    \caption{The effectiveness of the variant (selecting edges together after $bn_{ep}$ epochs) of \adv compared to original \adv (selecting edges one by one) with respect to budgets $b \in [10]$ for each dataset. The effectiveness of the variant was nearly the same as the original \adv.}
    \label{fig:together1by1}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{appendix/figures/together1by1_I.pdf}
    \includegraphics[width=0.7\linewidth]{appendix/figures/legend_together1by1_I.pdf}
    \caption{The effectiveness of the variant (selecting edges together) of \advp compared to original \advp (selecting edges one by one) with respect to budgets $b \in [10]$ for each dataset. The effectiveness of the variant is lower than that of the original \adv, except on the CL dataset.}
    \label{fig:together1by1_I}
\end{figure}

\begin{figure}[h!]
   \centering
   \includegraphics[width=0.75\linewidth]{appendix/figures/loss2_6layer.pdf}
   \caption{The average validation estimation errors in estimating influence decreased as GCN training proceeds.}
   \label{fig:val_error_6layer}
\end{figure}% 


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{appendix/figures/performance_larger_5and10.pdf}
    \caption{The effectiveness (the reduced ratio of influence) and running time of each method, with budget $b=5$ (top) and $b=10$ (bottom) on the large-scale datasets
    }
    \label{fig:app_perf_larger}
\end{figure}

\begin{table}[h!]
    \centering
    \begin{tabular}{l|l|r|r}
        \hline
        dataset &abbr. &\textbf{$|V|$}&\textbf{$|E|$} \bigstrut\\
        \hline
        cit-HepTh   & HT & $27,770$ & $352,807$\\
        email-EuAll & EA & $265,214$ & $420,045$\\ 
        twitter     & TW & $81,306$ & $1,768,149$\\
        \hline
    \end{tabular}
    \caption{The basic statistics of the large-scale datasets.}
    \label{tab:larger_data}
\end{table}