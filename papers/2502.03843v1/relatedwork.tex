\section{Related Work}
\noindent \textbf{Generative Natural Language Understanding}
In the field of natural language understanding, mainstream tasks encompass information extraction (NER, RE, EE, OpenIE etc.), text classification (topic classification, sentiment analysis, text similarity, natural language inference, etc.), and machine reading comprehension.
For information extraction, the UIE~\cite{DBLP:UIE} framework pioneered a generation-based unified approach, effectively addressing the challenges associated with redundant models and data construction. Building on this, InstructUIE~\cite{DBLP:InstructUIE} and YAYI-UIE~\cite{DBLP:yayiuie} developed a suite of information extraction instructions, implementing an instruction-based extraction framework through fine-tuning of large language models. To further enhance generalization beyond previous extraction instructions, OneKE~\cite{DBLP:OneKE} has introduced a more comprehensive and diverse set of information extraction instructions.
In the realm of text classification, ~\citet{DBLP:zeroshotTC} and ~\citet{DBLP:GARP} have innovatively utilized different prompting methods to facilitate zeroshot text classification and natural language inference, respectively. For machine reading comprehension, ~\citet{DBLP:AdaptLLM} achieved significant performance improvements by converting extensive amounts of raw text into QA pairs before fine-tuning.

\noindent \textbf{Instruction Synthesis} 
% Recent technical reports on the open-source large language models Llama 3.1~\cite{llama3.1} and Qwen2~\cite{qwen2} highlight that generating high-quality instructions is vital for training large models during both the pre-training and alignment stages. ~\citet{DBLP:MAGPIE} generates numerous queries through various templates, allowing current large language models to produce responses. ~\citet{DBLP:InstructPT} has developed an instruction synthesis method that converts raw pre-training text into instructional formats, substantially improving the performance of pre-trained models. Additionally, ~\citet{DBLP:WizardLM} has introduced an end-to-end framework that uses large language models to create evolving synthesis instruction datasets.
Recent technical reports on the open-source large language models Llama 3.1~\cite{llama3.1} and Qwen2~\cite{qwen2} highlight that generating high-quality instructions is vital for training large models during both the pre-training and alignment stages. ~\citet{DBLP:Self-Instruct} proposes a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. ~\citet{DBLP:GLAN} exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Additionally, ~\citet{DBLP:AUTOIF} transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions. There are also efforts to argument instructions for IE tasks. By annotation guidelines, GoLLIE~\cite{DBLP:GoLLIE} improves zero-shot information extraction, while ADELIE~\cite{DBLP:ADELIE} constructs a high-quality alignment corpus for IE instructions.