\section{Related Work}
\noindent \textbf{Generative Natural Language Understanding}
In the field of natural language understanding, mainstream tasks encompass information extraction (NER, RE, EE, OpenIE etc.), text classification (topic classification, sentiment analysis, text similarity, natural language inference, etc.), and machine reading comprehension.
For information extraction, the UIE____ framework pioneered a generation-based unified approach, effectively addressing the challenges associated with redundant models and data construction. Building on this, InstructUIE____ and YAYI-UIE____ developed a suite of information extraction instructions, implementing an instruction-based extraction framework through fine-tuning of large language models. To further enhance generalization beyond previous extraction instructions, OneKE____ has introduced a more comprehensive and diverse set of information extraction instructions.
In the realm of text classification, ____ and ____ have innovatively utilized different prompting methods to facilitate zeroshot text classification and natural language inference, respectively. For machine reading comprehension, ____ achieved significant performance improvements by converting extensive amounts of raw text into QA pairs before fine-tuning.

\noindent \textbf{Instruction Synthesis} 
% Recent technical reports on the open-source large language models Llama 3.1____ and Qwen2____ highlight that generating high-quality instructions is vital for training large models during both the pre-training and alignment stages. ____ generates numerous queries through various templates, allowing current large language models to produce responses. ____ has developed an instruction synthesis method that converts raw pre-training text into instructional formats, substantially improving the performance of pre-trained models. Additionally, ____ has introduced an end-to-end framework that uses large language models to create evolving synthesis instruction datasets.
Recent technical reports on the open-source large language models Llama 3.1____ and Qwen2____ highlight that generating high-quality instructions is vital for training large models during both the pre-training and alignment stages. ____ proposes a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. ____ exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Additionally, ____ transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions. There are also efforts to argument instructions for IE tasks. By annotation guidelines, GoLLIE____ improves zero-shot information extraction, while ADELIE____ constructs a high-quality alignment corpus for IE instructions.