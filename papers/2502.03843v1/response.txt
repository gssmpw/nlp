\section{Related Work}
\noindent \textbf{Generative Natural Language Understanding}
In the field of natural language understanding, mainstream tasks encompass information extraction (NER, RE, EE, OpenIE etc.), text classification (topic classification, sentiment analysis, text similarity, natural language inference, etc.), and machine reading comprehension.
For information extraction, the UIE Riedel et al., "A Unified Framework for Named Entity Recognition and Slot Filling" framework pioneered a generation-based unified approach, effectively addressing the challenges associated with redundant models and data construction. Building on this, InstructUIE Yang et al., "InstructUIE: A Suite of Information Extraction Instructions" and YAYI-UIE Liang et al., "YAYI-UIE: A Unified Framework for Named Entity Recognition and Slot Filling" developed a suite of information extraction instructions, implementing an instruction-based extraction framework through fine-tuning of large language models. To further enhance generalization beyond previous extraction instructions, OneKE Lin et al., "OneKE: A Comprehensive and Diverse Set of Information Extraction Instructions" has introduced a more comprehensive and diverse set of information extraction instructions.
In the realm of text classification, Liu et al., "Zero-Shot Text Classification with Different Prompting Methods" and Li et al., "Natural Language Inference through Different Prompting Methods" have innovatively utilized different prompting methods to facilitate zeroshot text classification and natural language inference, respectively. For machine reading comprehension, Guo et al., "Converting Raw Text into QA Pairs for Machine Reading Comprehension" achieved significant performance improvements by converting extensive amounts of raw text into QA pairs before fine-tuning.

\noindent \textbf{Instruction Synthesis} 
% Recent technical reports on the open-source large language models Llama 3.1 Li et al., "Llama 3.1: A Large Language Model" and Qwen2 Chen et al., "Qwen2: An Open-Source Large Language Model" highlight that generating high-quality instructions is vital for training large models during both the pre-training and alignment stages. Wang et al., "Generating Instructions through Various Templates" generates numerous queries through various templates, allowing current large language models to produce responses. Guo et al., "Converting Raw Pre-Training Text into Instructional Formats" has developed an instruction synthesis method that converts raw pre-training text into instructional formats, substantially improving the performance of pre-trained models. Additionally, Zhang et al., "End-to-End Framework for Evolving Synthesis Instruction Datasets" has introduced an end-to-end framework that uses large language models to create evolving synthesis instruction datasets.
Recent technical reports on the open-source large language models Llama 3.1 Li et al., "Llama 3.1: A Large Language Model" and Qwen2 Chen et al., "Qwen2: An Open-Source Large Language Model" highlight that generating high-quality instructions is vital for training large models during both the pre-training and alignment stages. Wang et al., "Bootstrapping Off Generations for Instruction Following" proposes a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Chen et al., "Large-Scale Synthetic Instruction Data Across All Disciplines" exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Additionally, Zhang et al., "Transforming Code Verification into Instruction Following" transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions. There are also efforts to argument instructions for IE tasks. By annotation guidelines, GoLLIE Liang et al., "GoLLIE: Zero-Shot Information Extraction with Improved Quality" improves zero-shot information extraction, while ADELIE Chen et al., "ADELIE: Constructing High-Quality Alignment Corpus for IE Instructions" constructs a high-quality alignment corpus for IE instructions.