
\begin{figure}[th]
  \vspace{-9mm}
  \centering
  % \includegraphics[width=1.0\textwidth]{figs/rebuttal/rfig1.jpg}
  \includegraphics[width=0.95\textwidth]{fig/rfig1.pdf}
  % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
  % \vspace{-6mm}
  \caption{Comparison between the existing benchmark and MMKE-Bench with a detailed example. In this example, the texts in red represent the edited counterfactual content. T/I-Rel represents text and image reliability, T/I-Gen represents text and image generalization and Port represents portability. Previous benchmarks mainly focus on entity recognition editing using a triplet-based knowledge representation format, which does not align with actual scenarios. MMKE-Bench focuses on evaluating diverse semantic editing in realistic scenarios in a natural language format.
% MMKE-Bench consider text modality and image modality semantic editing under the user-specific and user-agnostic scenario and construct three evaluation parts, namely entity-centric editing, visual semantic editing, and user-specific editing.
}
  \label{fig:exam}
  \vspace{-9mm}
\end{figure}

\section{Introduction}
Large language models (LLMs) and multimodal models (LMMs) have demonstrated remarkable success across various tasks due to their powerful understanding and reasoning abilities, grounded in vast amounts of knowledge~\citep{brown2020language,zhao2023survey,liu2024visual}. However, the knowledge within these models can become outdated or inaccurate over time due to evolving real-world information and changes in factual data. To address this, knowledge editing techniques have been developed to correct inaccuracies and inject new knowledge into pre-trained models with minimal cost, without affecting unrelated content~\citep{mitchell2022memory,yao2023editing}. In recent years, several datasets have been introduced to benchmark the progress of knowledge editing methods in both the textual~\citep{yao2023editing,onoe2023can,decao2021editing,li2023evaluating} and multimodal domains~\citep{mmedit2023,vlkeb2024,mike2024,mcmke2024}.

However, most existing benchmarks focus on editing \textit{entity-level} knowledge, typically formatted as a triplet (\textit{subject, relation, object}). While effective in certain tasks, this format lacks the complexity required for real-world applications, particularly in multimodal domains where visual knowledge must also encompass actions, body gestures, and object relationships. Furthermore, knowledge editing techniques have quickly saturated on these benchmarks, achieving near-perfect performance. For example, simply fine-tuning the LLaVA model achieved 99.59\%, 99.43\%, and 95.48\% accuracies for reliability, text generalization, and image generalization, respectively, on the VLKEB benchmark~\cite{vlkeb2024}. This highlights the urgent need for a more challenging benchmark to foster the development of multimodal knowledge editing techniques.

To address these issues, we introduce MMKE-Bench, a comprehensive multimodal knowledge editing benchmark designed to \textbf{evaluate diverse semantic editing in real-world scenarios}. MMKE-Bench represents multimodal knowledge using free-form natural language descriptions paired with images, providing a richer and more flexible expression of interconnected information. Reflecting real-world needs, MMKE-Bench includes three types of editing: visual entity editing, visual semantic editing, and user-specific editing. Visual entity editing updates entity-centric visual knowledge, while visual semantic editing targets complex object behaviors and relationships, such as referee gestures and traffic signals. Lastly, user-specific editing evaluates the modelâ€™s ability to integrate individualized knowledge. The first two types modify existing knowledge, while the third adds new knowledge. Comparisons with existing benchmarks are shown in Fig.\ref{fig:exam} and Tab.\ref{tab:com}.

To construct MMKE-Bench, we first collect original knowledge from various images and knowledge sources (e.g., multimodal knowledge graphs, demo videos, Google, and LLM generation). Next, we create editing knowledge by applying \textit{counterfactual editing for the text modality} and \textit{image replacement for the image modality}. User-specific editing involves adding entirely new, personalized knowledge to the model and does not need counterfactual editing. Following previous works~\citep{zheng2023can,vlkeb2024}, we adhere to four evaluation principles: \textit{reliability, locality, generalization, and portability}, generating evaluation questions and answers automatically. Finally, all questions and answers undergo human verification and are revised where necessary. The resulting benchmark contains 2,940 pieces of knowledge and 8,363 images across 33 broad categories.

We evaluate five of the most prominent multimodal knowledge editing methods on three representative LMMs, assessing their performance in both single and sequential editing tasks. Empirically, we find that (i) no single editing method excels across all evaluation criteria; (ii) visual knowledge and user-specific knowledge are more difficult for LMMs to edit; (iii) modern LMMs excel in producing and applying edited knowledge; and (iv) the proposed benchmark proves more challenging than previous benchmarks.

To sum up, our contribution can be summarized as follows:
\begin{itemize}
\item We propose MMKE-Bench, a challenging benchmark for evaluating diverse semantic editing in real-world scenarios. It adopts free-form natural language-based knowledge representation and includes three types of editing aligned with real-world contexts.
\item We introduce a novel pipeline for benchmark construction that collects original knowledge, generates editing knowledge, and produces evaluation questions guided by four principles.
\item Extensive experiments with various baseline methods and LMMs in both single and sequential editing settings are conducted, revealing several limitations in existing knowledge editing approaches.
\end{itemize}

\begin{table}[tbp]
\vspace{-9mm}
\centering
\renewcommand{\arraystretch}{1.3} 
\caption{Overall comparison with existing multimodal knowledge editing benchmarks.}
\label{tab:com}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Benchmark} & \textbf{Knowledge Representation} & \textbf{\makecell{Visual Entity \\ Editing}} & \textbf{\makecell{Visual Semantic \\Editing}} & \textbf{\makecell{User-Specific \\Editing}} & \textbf{Evaluation Principle} \\
\midrule
\textbf{MMEdit}  &   Short-Text     & \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}}  &  \makecell{Reliability, Locality,  and Generalization} \\
\textbf{MIKE}    & Triplet & \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} & \makecell{Reliability, Locality, and Generalization}\\
\textbf{MC-MKE}  & Triplet & \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}} &  \textcolor{red}{\ding{55}} & \makecell{Reliability, Locality, and Generalization}\\
\textbf{VLKEB}   & Triplet & \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & Reliability, Locality, Generalization, and Portability \\
\textbf{MMKE-Bench}    & Free-Form Natural Language & \textcolor{green}{\ding{51}} & \textcolor{green}{\ding{51}}  & \textcolor{green}{\ding{51}} & Reliability, Locality, Generalization, and Portability \\
\bottomrule
\end{tabular}
}
\vspace{-6mm}
\end{table}


% KE in LLM
% KE in VLM
% problem
% Our work
% COntribution


%%% 
% need to unify
% 1. the title
% 2. the benchmark name
% 3. problem
% focuses on entity recognition, editing with a triplet knowledge format. 
% 4. the evaluation perspective (motivation)
% complex semantic editing towards realistic scenarios
% 5. name of three type of subset.  three part
%    entity-centric editing,  visual semantic editing, user-specific editing
% 6. how to cover motivation.text modality
% and image modality semantic editing under the user-specific and user-agnostic scenario



% . While over time, both the meaning of these hand signals and the rules of the sports may change, so it is necessary to edit both the text and vision modality knowledge and the synergistic effects of both modalities of LMMs to make LMMs a helpful sports assistant.   

% It is crucial to understand these decisions and to reason about the game's progression based on previously learned rule knowledge.

% For instance, traffic police hand signals and referee sports are commonly used in traffic and sports fields to communicate to the outside. As hand signals and rules may evolve over time, it becomes necessary to update both text and visual modality knowledge in LMMs to maintain their effectiveness as helpful AI assistants.
% in sports, referees use hand signals to communicate decisions. As the meanings of these hand signals and the rules of the sports may evolve over time, it is necessary to edit both text and visual modality knowledge and their interactions in LMMs, to ensure their effectiveness as AI assistants for sports.

%~\citep{DBLP:journals/corr/abs-2406-13219,mike2024,vlkeb,mmedit2023}

% With the developments of the large language models (LLM), these models have been successfully applied in various tasks due to their enhanced comprehension and reasoning abilities, supported by extensive knowledge~\cite {DBLP:conf/cvpr/GuptaK23,zhao2023survey}. However, a significant challenge faced by LLMs is the tendency for the embedded knowledge to become inaccurate and outdated. Knowledge editing emerges as a promising method, focusing on making rapid and data-efficient updates to the behavior of a pre-trained LLM within a specific domain, without affecting unrelated content~\citep{mitchell2022memory,yao2023editing}.





% In contrast to editing single-modality models, Editing large multimodal models (LMMs) is crucial but challenging due to inherent modality diversity and their synergistic effects~\citep{mmedit2023}. 
% For example, traffic police hand signals and referee gestures in sports are widely used to communicate in their respective fields. As hand signals and rules may evolve, it becomes essential to update both text and visual modality knowledge in LMMs to ensure their continued effectiveness as AI assistants.
% To advance multimodal knowledge editing research, there is a need for benchmarks that effectively and comprehensively reflect editing performance. Several benchmarks for multimodal knowledge editing in LMMs have been proposed in previous studies. MMEdit~\citep{mmedit2023} establishes innovative evaluation metrics based on visual question answering and caption tasks. Subsequent works primarily focus on visual entity recognition editing, using a triple knowledge representation format: (\textit{subject}, \textit{relationship}, \textit{object}). Notably, MIKE~\citep{mike2024} and VLKEB~\citep{vlkeb} focus on fine-grained visual entity recognition, while MC-MKE~\citep{DBLP:journals/corr/abs-2406-13219} extents fine-grained visual entity recognition by emphasizing modality consistency.








% Editing large multimodal models (LMMs), in contrast to single-modality models, is equally important but significantly more challenging due to the inherent diversity of modalities and their synergistic effects~\citep{DBLP/emnlp/0008TL0WC023}. For instance, traffic police hand signals and referee gestures in sports are critical for communication in their respective domains. As these hand signals and rules evolve, it becomes essential to update both textual and visual modality knowledge in LMMs to ensure they remain effective as AI assistants.

% To advance research in multimodal knowledge editing, there is a pressing need for benchmarks that effectively capture and evaluate editing performance. Several benchmarks have been proposed for multimodal knowledge editing in LMMs. For example, MMEdit~\citep{DBLP/emnlp/0008TL0WC023} introduces innovative evaluation metrics based on visual question answering and captioning tasks. Subsequent works, such as MIKE~\citep{DBLP/acl/LiDZ0HQJ0T24} and VLKEB~\citep{vlkeb}, focus on fine-grained visual entity recognition editing, adopting a triplet-based knowledge representation format (\textit{subject}, \textit{relationship}, \textit{object}). MC-MKE~\citep{DBLP/corr/abs-2406-13219} extends this approach by emphasizing modality consistency alongside fine-grained entity recognition.








% showing that while current approaches provide some support for multimodal editing, the results are still not fully satisfactory.

% \newpage


% However, several key issues persist in existing benchmarks. First, triplet representation does \textbf{ not align with real-world applications} and conveys limited information. For example, when editing a visual entity such as a university, multiple aspects of the entity(e.g., location, founded year, president) cannot be edited simultaneously in a single triplet. Second, current benchmarks overlook \textbf{complex visual semantic knowledge} necessary for multimodal understanding. When analyzing images, we not only recognize entities but also focus on object behavior(e.g. gesture, action, emotion, and style) and the relationship between objects. 
% Third, some previous benchmarks allow \textbf{little room for improvement}, and even the simple baseline (i.e., Fine-tuning) could achieve high performance. For instance, fine-tuning the LLaVA model has resulted in accuracies of 99.59\%, 99.43\%, and 95.48\% for reliability, text generalization, and image generalization on the VLKEB benchmark, highlighting the necessity for more complex evaluation benchmarks.  Lastly, as for real-world scenario, there is an increasing \textbf{demand for personalized LMMs} to serve as AI assistants that remember the user preferences, or belongings for various applications, including health, education, and entertainment~\citep{alaluf2024myvlm,nguyen2024yo}, which is not considered in existing benchmark.





% a novel benchmark that could both conform to the actual scenario and incorporate complex visual knowledge for multimodal large language model editing evaluation (named MMKE-Bench).


%%%% previous

% To address these issues, we propose MMKE-Bench, a comprehensive multimodal knowledge editing benchmark designed to evaluate complex semantic editing in realistic scenarios using free-form natural language representation. The comparison between MMKE-Bench and the existing benchmark is illustrated in Fig.~\ref{fig:exam} and Tab.~\ref{tab:com}. First, we utilize free-form natural language descriptions combined with an image to represent multimodal knowledge, allowing for the effective and conventional expression of multiple relevant and interconnected information. Second, we propose complex visual semantic editing, covering various object behaviors and relationships, to
%  enhance visual modality understanding of LLM.  Finally,  we incorporate user personalized information as the editing knowledge to assess the integration of user-specific knowledge into the models.



% Considering real-world scenarios, MMKE-Bench encompasses three types of editing: visual entity editing, visual semantic editing, and user-specific editing. Similar to previous benchmarks, visual entity editing focuses on entity recognition editing. Visual semantic editing emphasizes complex visual content editing,  which requires a high level of content understanding. And user-specific editing focuses on incorporating personalized user information into LMMs. The first two types could be regarded as editing existing knowledge, and the third type could be regraded as inserting new knowledge.

%%%% newly

% To address these issues, we propose MMKE-Bench, a comprehensive multimodal knowledge editing benchmark designed to \textbf{evaluate diverse semantic editing in real-world scenarios}. On the one hand, MMKE-Bench employs free-form natural language descriptions paired with images to represent multimodal knowledge, allowing for a richer and flexible expression of interconnected information. On the other hand, considering real-world scenarios, MMKE-Bench introduces three types of editing: visual entity editing, visual semantic editing, and user-specific editing. Visual entity editing focuses on the modification of visual entity-centric knowledge. Visual semantic editing focuses on complex object behaviors and relationships, improving the model's understanding of high-level visual semantics. Finally, user-specific editing incorporates personalized user information to evaluate the integration of new, individualized knowledge into the models. The first two types of editing reflect the modification of existing knowledge, while the third involves adding new knowledge. Comparison with the existing benchmark is shown in Fig.~\ref{fig:exam} and Tab.~\ref{tab:com}.

% To address these issues, we introduce MMKE-Bench, a comprehensive multimodal knowledge editing benchmark designed to \textbf{evaluate diverse semantic editing in real-world scenarios}. MMKE-Bench utilizes free-form natural language descriptions paired with images to represent multimodal knowledge, providing a richer and more flexible expression of interconnected information. In line with real-world needs, MMKE-Bench includes three types of editing: visual entity editing, visual semantic editing, and user-specific editing. Visual entity editing focuses on updating entity-centric visual knowledge, while visual semantic editing targets complex object behaviors and relationships such as referee and traffic police gestures. Lastly, user-specific editing evaluates the model's ability to integrate individualized knowledge. The first two editing types modify existing knowledge, whereas the third involves adding new knowledge. Comparisons with existing benchmarks are presented in Fig.~\ref{fig:exam} and Tab.~\ref{tab:com}.





%%%%%%%%%%%%%%
% Additionally, by utilizing natural language descriptions, visual entity editing could edit multiple aspects of information about the entity alongside entity recognition. 


% The associated description includes both the action description and the rule or the meaning of this action.


% is user-centric and is achieved by injecting the user's personalized information into the model, such as personalized belongings, education and working experience, and personalized preference.


% where entry-level knowledge and user-specific knowledge mainly focus on actual application scenarios and visual knowledge focus on complex visual knowledge. 
%
% In the entry-level knowledge dataset, the natural language description is composed of multi-aspect information of an entity. 

% for entry-level knowledge, we get the entity name from the existing MMKG or larger-scale entity dataset, and crawl the Wikipedia summary as the description. For visual knowledge, we collect part of the samples manually from the demo videos and Google images collect the remaining samples from existing datasets, and write the original description manually or by LLM. For the user-specific datasets, we collect the images from MMKG,  and generate the description by LLM.


% As for constructing an editing dataset, considering that the entity knowledge and the visual knowledge may have been learned by the models, we build both datasets by counterfactual editing, which is widely used in existing LLM and LMM editing benchmarks~\citep{}. The (original?) user-specific dataset sets make up some users and their personalized information, which is all new knowledge to the model, so we do not make any editing and treat this as new knowledge insert~\citep{} for LMMs. 

% state how the description is collected. and image!!!
% state how to get edited knowledge 
% To construct this benchmark, we adopt \textit{counterfactual content} as the editing knowledge for three types of editing. To achieve this, we first collect original knowledge from various sources (e.g., multimodal knowledge graph~\citep{wu2023mmpedia}, demo video, Google images, and LLM generation) in the form of an image associated with the corresponding natural language description. Then, we generate editing knowledge by i) generating counterfactual content with LLM and ii) changing the visual content with another image from the same type, where both strategies are widely used in existing benchmarks. In this step, we do not operate on user-specific editing, as the user's personalized information is all new knowledge to the model. Next, we then generate evaluation questions and answers in an automated manner combined with human checks. We follow previous works~\citep{zheng2023can,vlkeb}, and consider four evaluation principles, namely \textbf{reliability}, \textbf{locality},

% we adopt \textit{counterfactual content} as the editing knowledge for the three types of editing.
% generating counterfactual content with LLM and changing the visual content with another image from the same type, where both text and image modalities are edited for evaluation. 

% designing the corresponding questions and answers with LLM.

% To construct this benchmark,  we begin with collecting original knowledge from various images and knowledge sources (e.g., multimodal knowledge graphs, demo videos, Google, and LLM generation). Next, we generate editing knowledge by \textit{counterfactual content editing for text modality and image replacement for image modality}.  At this stage, we do not operate on user-specific editing, as the user's personalized information is entirely new for the model. Subsequently,  we follow prior works~\citep{zheng2023can,vlkeb}, and follow four evaluation principles: \textit{reliability, locality, generalization, and portability},  to generate evaluation questions and answers in an automated manner supplemented by human checks.
% Finally, all the questions and answers undergo human verification and are rewritten as needed. The final benchmark comprises 2,940 pieces of knowledge, along with 7,229 images from 110 fine-grained types. 







% in the following way:  i) Reliability refers to the ability of the post-edit model to generate the target answer for a given case. We consider both text reliability and image reliability to develop the questions and the questions by LLM only based on the edited content in the description.
% ii) Generalization requires the model, after editing, to understand equivalent similar sentences, such as rephrased ones. We consider image generalization and randomly choose another image from multiple images for each entity, visual knowledge for assessing the generalization.
% iii) Locality highlights the focus on localized edits, ensuring that the output of unrelated knowledge remains unchanged. We consider both text locality and image locality and construct the question from existing benchmark~\citep{vlkeb}.
% iv) Portability assesses the model's ability to apply the edited knowledge to relevant contexts effectively. For the entity dataset, we consider text portability text- we sample relational triples $(s, r, o)$ of entities edited from Wikipedia to construct test examples with the edited entities serving as the subjects of these sampled triples. For visual knowledge and user-specific dataset, we consider image portability and combine the content image with an image of the same type, then ask about image-related questions such as color, shape, and count to form the Portability question.
% % state how to check.



% 
% state what evaluation quota is considered.
% state how to generate questions and answers.
% After generating editing content, we then generate evaluation questions and answers in an automated way combined with human checks. We follow previous works~\citep{zheng2023can,vlkeb}, and consider four evaluation principles, namely \textbf{reliability}, \textbf{locality},
% \textbf{generalization}, and \textbf{portability}, and design the corresponding questions and answers in the following way:  i) Reliability refers to the ability of the post-edit model to generate the target answer for a given case. We consider both text reliability and image reliability to develop the questions and the questions by LLM only based on the edited content in the description.
% ii) Generalization requires the model, after editing, to understand equivalent similar sentences, such as rephrased ones. We consider image generalization and randomly choose another image from multiple images for each entity, visual knowledge for assessing the generalization.
% iii) Locality highlights the focus on localized edits, ensuring that the output of unrelated knowledge remains unchanged. We consider both text locality and image locality and construct the question from existing benchmark~\citep{vlkeb}.
% iv) Portability assesses the model's ability to apply the edited knowledge to relevant contexts effectively. For the entity dataset, we consider text portability text- we sample relational triples $(s, r, o)$ of entities edited from Wikipedia to construct test examples with the edited entities serving as the subjects of these sampled triples. For visual knowledge and user-specific dataset, we consider image portability and combine the content image with an image of the same type, then ask about image-related questions such as color, shape, and count to form the Portability question.
% % state how to check.
% All the questions and answers have been checked by humans and inappropriate questions and incorrect answers are rewritten by humans.


% 
% state the final benchmark.






% We evaluate five of the most renowned multimodal knowledge editing methods on three representative LMMs, assessing their performance under both single and sequential editing. Empirically, we find that {\color{red} (i)
% (ii)
% (iii)
% }

% 11137 2028 5894

% (1000?), (500?), and (600?) samples for entry dataset, visual knowledge dataset, and user-specific dataset, respectively. 
% }.

% state how to conduct and experiments, and some important results.
% We conduct extensive experiments on existing models. the following are some findings:

    % We adopt free-from natural language-based knowledge representation and collect three types of editing, aligning with real-world scenarios. 

% showing that while current approaches provide some support for multimodal editing, the results are still not fully satisfactory.