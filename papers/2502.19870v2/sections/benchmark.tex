% \vspace{-3mm}
\section{Benchmark}
% \vspace{-2mm}
% Our benchmark is composed of three types of editing: visual entity editing, visual semantic editing, and user-specific editing. In the next subsections, we introduce how to construct the benchmark with four evaluation principles.



As shown in Fig.~\ref{fig:pip}, we construct the benchmark through four steps: i) Original Knowledge Collection;  ii) Editing Knowledge Generation; iii) Evaluation Question Generation; and iv) Human Verification.



\begin{figure}[tbp]
  \vspace{-9mm}
  \centering
  % \includegraphics[width=0.95\textwidth]{figs/rebuttal/fig2-new.jpg}
   \includegraphics[width=0.95\textwidth]{fig/fig2-new.pdf}
  \vspace{-2mm}
  % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
  \caption{ The construction pipeline of MMKE-Bench.}
  \label{fig:pip}
  \vspace{-6mm}
\end{figure}

\subsection{Original Knowledge Collection}

% As discussed before, we use an image and free-from natural language description to represent original knowledge. 

%%% !!!!!!!!!

In gathering original knowledge, we first list candidate fine-grained entities, visual semantics, or user-specific items, and then collect their corresponding images and descriptions.


For visual entity editing, we source candidates from two datasets: the multimodal knowledge graph, MMpedia~\cite{wu2023mmpedia}, and the visual entity recognition dataset, OVEN~\cite{hu2023open}. For each entity selected from the existing dataset, we get their images from the datasets and then manually review the images by removing the entities that cannot uniquely identify the main entity from images and noise images. For entities with less than two images, we recollect additional images by crawling from Google. Next, we retrieve entity descriptions from the Wikipedia summary dumps\footnote{\url{https://dumps.wikimedia.org/enwiki/20240620/}} and summarize the description by an LLM to generate the final descriptions. As shown in Fig.~\ref{fig:type}, this type covers 10 broad categories.




% For visual entity editing, we source candidates from two key datasets: the multimodal knowledge graph MMpedia~\cite{wu2023mmpedia} and the visual entity recognition dataset OVEN~\cite{hu2023open}. For each entity selected from these datasets, we obtain their corresponding images. These images are then manually reviewed, removing those that fail to uniquely represent the main entity. If an entity has fewer than two valid images, we recollect additional images by crawling from Google. Next, we retrieve entity descriptions from Wikipedia summary dumps\footnote{\url{https://dumps.wikimedia.org/enwiki/20240620/}}, and refine the content using a large language model (LLM) to generate the final descriptions. As illustrated in Fig.~\ref{fig
% }, this approach spans 10 broad categories and 70 subclasses.


% we consider four classes of semantic knowledge: single-person behavior, single-object behavior or attribute, object relationship, and global structure.

% For visual semantic editing, as shown in Fig.~\ref{fig:type}, we consider four broad classes of semantic knowledge: 
% For some visual knowledge with existing available datasets, e.g., object relationship, texture, and art style, we collect the images of visual semantic content from the corresponding datasets. 
% After that, we generate the description by depicting the visual semantic action and the rule or the meaning conveyed by this visual behavior with the help of LLM. The image source of each type of knowledge is shown in the appendix.


\begin{wrapfigure}{r}{0.55\textwidth} % r: 右侧, l: 左侧
    \vspace{-6mm}
    \centering
\includegraphics[width=0.52\textwidth]{fig/sunburst_chart_v1.pdf}
    \caption{The types of samples in MMKE-Bench.}
        \label{fig:type}
    \vspace{-6mm}
\end{wrapfigure}

% For visual semantic editing, as illustrated in Fig.~\ref{fig:type}, the candidate comes from predefined 14 broad classes of semantic knowledge by ourselves, including
% single-person behavior, single-object behavior or attributes, object relationship, and global structure, encompassing 25 subclasses. For some visual knowledge with existing datasets, such as object relationships, textures, and art styles, we collect the candidate semantic and images from the corresponding datasets.
% For others, we extract images from demo videos or crawl Google, applying human filters for quality control. Finally, we generate descriptions that depict the visual semantic actions and the rules or meanings conveyed by these behaviors, assisted by an LLM or human writing.  The image sources are detailed in the appendix.




For visual semantic editing, as shown in Fig.~\ref{fig:type}, we define the candidates across 14 broad categories of semantic knowledge, including single-person behaviors, single-object behaviors or attributes, object relationships, and global structures. For certain types of visual knowledge that have corresponding datasets, such as object relationships, textures, and art styles, we collect both the candidate semantics and associated images from these datasets. For other cases, we extract images from demonstration videos or gather them via Google, applying human verification for quality control. Descriptions of the visual semantic actions, along with the rules or meanings conveyed by these behaviors, are generated with the assistance of LLM or human writers. Details of the image sources are provided in the appendix.



For user-specific editing, we consider 9 broad categories of personalized information sources, such as favorite singers, owned pets, and alma maters. For personal items and pets, we gather candidates and images from the existing personalized research works~\cite{nguyen2024yo,alaluf2024myvlm}. For singers, actors, and cartoon characters, we first generate a candidate list and then crawl images from Google. For other categories, including company, university, sports club, and organization, we source candidates from MMpedia, manually verifying and removing noise images. Finally, we employ an LLM to generate personalized relationships and experiences between the user and these objects. 
 % The candidate relationships are shown in the appendix.


% For user-specific editing, we consider nine broad categories of personalized information sources, such as favorite singers, owned pets, and alma maters. For personalized items and pets, we gather candidates and images from existing personalized research works~\cite{nguyen2024yo,alaluf2024myvlm}. For singers, actors, and cartoon characters, we generate a candidate list and crawl images from Google. For other categories, including companies, universities, sports clubs, and organizations, we source candidates from MMpedia, manually verifying and filtering out noise images. Finally, we use a large language model (LLM) to generate personalized relationships and experiences between the user and these objects.



\subsection{Editing Knowledge Generation}
 % Considering the multimodal characteristics of LMMs, we propose to edit from both text and visual content when constructing the benchmark. Note that we only edit visual entity and visual semantic knowledge while keeping the user-specific editing unchanged, where the former is regarded as knowledge editing and the latter as knowledge insertion. For the visual modality, we follow previous work~\cite{vlkeb2024}, and randomly select another image from the same type to replace the original visual content. For example, as shown in Fig.~\ref{fig:exam}, for the offside penalty gesture of the assistant referee, we may select the substitution gesture of the assistant referee as edited visual content. For the text modality, we edit some key information of an entity for visual entity editing. Besides, we edit the rule or meaning into counterfactual content for visual semantic editing and replace the behavior description with the visual content. For the example of the offside penalty gesture, we replace the gesture action description with the gesture action of substitution gesture and edit the serve location as the penalty spot.


% Considering the multimodal characteristics of LMMs, we propose to edit both text and visual modalities when constructing the benchmark. We only edit visual entity and visual semantic knowledge while keeping user-specific knowledge unchanged, with the former regarded as knowledge editing and the latter as knowledge insertion. For the visual modality, we follow previous work~\cite{vlkeb2024} and adopt image-replacement-based editing, where we randomly select another image of the entity or semantic action of the same type to replace the original visual content. For example, as shown in Fig.~\ref{fig:exam} and Fig.~\ref{fig:pip}, the offside penalty gesture of the assistant referee is replaced by that of the substitution gesture as the edited visual content. In the text modality, we modify key information of an entity and rule or meaning into counterfactual content for visual entity editing and visual semantic editing, respectively. Besides, we also replace the action description of the visual content to align with the image. For the example of an offside gesture, we replace the original action description with that of the substitution gesture and edit the kick-off location from foul position to the penalty spot.



Considering the multimodal nature of large multimodal models (LMMs), we propose editing both text and visual modalities when constructing the benchmark. Specifically, we focus on editing visual entities and visual semantic knowledge while leaving user-specific knowledge unchanged. The former is treated as knowledge editing, while the latter is regarded as knowledge insertion.

For the visual modality, we follow the image-replacement-based editing approach from previous work~\cite{vlkeb2024}, where an image of the entity or semantic action is randomly replaced with another of the same type. For example, as illustrated in Fig.~\ref{fig:exam} and Fig.~\ref{fig:pip}, the assistant referee’s offside penalty gesture is replaced with a substitution gesture in the edited visual content.  In the text modality, we modify key information about the entity and the rule or meaning into counterfactual content for visual entity editing and visual semantic editing, respectively. Additionally, we update the action description to align with the new visual content. In the example of the offside gesture, the original action description is replaced with that of the substitution gesture, and the kick-off location is edited from the foul position to the penalty spot.



% \begin{figure}
%     \vspace{-6mm}
%     \centering
% \includegraphics[width=0.45\linewidth]{figs/sunburst_chart_v1.png}
%     \caption{Enter Caption}
%     \label{fig:type}
%     \vspace{-6mm}
% \end{figure}



\vspace{-2mm}
\subsection{Evaluation Question Generation}
\vspace{-1mm}
We adhere to four key evaluation principles to generate both the questions and answers. The reliability and portability questions are generated by prompting LLM and we show the prompts in the appendix.

\vspace{-2mm}
\paragraph{\textbf{Reliability Question Generation}}

% The reliability criterion is used to assess whether the edited knowledge can be produced correctly after editing.  When generating the questions and answers, we prompt LLM with the requirement that the question must ask something about the edited counterfactual contents(e.g., the kick-off location  of the offside penalty.) For this evaluation, instead of only considering image-reliability, we design both text-reliability and image-reliability to measure the ability of LMMs to edit text and visual modalities. The text reliability questions are designed to be answered entirely without the need for images while the image reliability question adopts the format of `{the type in the image}' to refer to the main object, behavior, or personalized item. An example is shown in Fig.~\ref{fig:pip}. We denote the reliability question sets as $Q_{rel} = (i_e, q_r, a_r)$, where $i_e$ is the edited image, $q_r$ is the question, and $a_r$ is the answer. Let $M_\theta$ and $M_\theta'$ denote original and edited LMMs, reliability is assessed as:


The reliability criterion assesses whether the edited knowledge is correctly produced after the editing process. When generating questions and answers, we prompt the LLM with a requirement that the question must ask one aspect of the edited counterfactual content (e.g., the kick-off location of the offside penalty). To evaluate this, we consider both text reliability and image reliability, measuring the LMM's ability to edit across text and visual modalities. Text reliability questions are crafted to be answerable without images, while image reliability questions use the format \{the type in the image\} to reference the main object, behavior, or personalized item. An example is provided in Fig.~\ref{fig:pip}. We denote the reliability question sets as $Q_{rel} = (i_e, q_r, a_r)$, where $i_e$ represents the edited image, $q_r$ the question, and $a_r$ the answer. Let $M_\theta$ and $M_\theta'$ denote the original and edited LMMs, respectively, and $\mathbb{I}[\cdot]$ denoted indicator function, reliability is then evaluated as:
\begin{equation}
    \mathbb{E}_{(i_e, q_r, a_r) \sim Q_{rel}}  \mathbb{I}\left[ M_{\theta}'(i_e, q_r) = a_r \right]
    \vspace{-1mm}
\end{equation}

\vspace{-2mm}
\paragraph{\textbf{Locality Question Generation}}

% The locality criterion assesses how much unrelated knowledge remains unchanged in the edited model by comparing the model's outputs before and after the editing. For locality, we evaluate both text and image locality, which tests the model’s stability when handling out-of-scope knowledge from both modalities. Following previous work, we gather the locality questions and answers from VLKEB benchmark~\cite{vlkeb2024}, where the text questions are chosen from the NQ dataset~\cite{kwiatkowski2019natural} and the image questions are constructed are designed by VLKEB. We denote the locality question as $Q_{loc} = (i_e, q_l)$, locality is assessed as:


The locality criterion evaluates how much unrelated knowledge remains unchanged in the edited model by comparing its outputs before and after the editing process. For locality, we assess both text and image locality, which tests the model’s stability when dealing with out-of-scope knowledge from each modality. Following prior work, we source locality questions and answers from the VLKEB benchmark~\cite{vlkeb2024}, where the text questions are drawn from the NQ dataset~\cite{kwiatkowski2019natural}, and the image questions are specifically designed by VLKEB. We represent the locality question set as $Q_{loc} = (i_l, q_l)$, and locality is evaluated as:
\begin{equation}
     \mathbb{E}_{(i_l, q_l) \sim Q_{loc}}\mathbb{I}\left[{M_\theta}(i_l, q_l) = M_{\theta}'(i_l, q_l) \right] 
     \vspace{-1mm}
\end{equation}

\vspace{-2mm}
\paragraph{\textbf{Generalization Question Generation}} 
% The generalization criterion evaluates how well the model responds to neighboring samples. Unlike triplet-based knowledge editing, we focus solely on image generalization, as text generalization does not apply to MMKE-Bench due to its free-form knowledge format. For image generalization, we randomly select another image $i_e^{g}$ from multiple images of an entity, visual behavior, or a personalized item, and reuse the same question and the answer from the image reliability question. Denote the generalization question as $Q_{gen} = (i_e^{g}, q_g, a_g)$, where $q_g = q_r$, and $a_g = a_r$ for the same object, generalization is evaluated as:

The generalization criterion evaluates how effectively the model responds to neighboring samples. Unlike triplet-based knowledge editing, we focus exclusively on image generalization, as text generalization is not considered due to the free-form knowledge format. For image generalization, we randomly select another image $i_e^{g}$ from the multiple available images of an entity, visual behavior, or personalized item, and reuse the same question and answer from the image reliability, with an example shown in Fig.~\ref{fig:pip}. We define the generalization question as $Q_{gen} = (i_e^{g}, q_g, a_g)$, where $q_g = q_r$ and $a_g = a_r$ for the same object. Generalization is evaluated as:
\begin{equation}
    \mathbb{E}_{(i_e^{g}, q_g, a_g) \sim Q_{gen}}\mathbb{I}\left[M_{\theta}'(i_e^{g}, q_g) = a_g\right]
    \vspace{-1mm}
\end{equation}

\vspace{-2mm}
\paragraph{\textbf{Portability Question Generation}}
% The portability criterion evaluates whether the edited knowledge can be successfully applied to related content. Follow prior work~\cite{vlkeb2024},
% We adopt text portability evaluation for visual entity editing and image modality portability for visual semantic editing and user-specific editing, to enhance the evaluation of visual moidality understanding. 

% For the former,  we ask for information about the edited content with extra Wikipedia information provided for question generation. For example, assume the current entity is the Eiffel Tower,  and the edited content $e'$ in the description is the designer of the building, we might generate a question like ``Who is the designer of the Eiffel Tower?". Then, we generate another question about the edited content such as the birth year. Finally, by combining these two questions, we will get the final probability question ``In which year was the builder of the Eiffel Tower born?". 

% For visual semantic and user-specific editing,  we first combine the image of the main behavior or item with another image of the same type into a new image $i_e^p$, and then we pose a question focusing on differences between two images such as hair color, or object shape. By combining this question with one related to edited content, we formulate the final portability question. As shown in Fig~\ref{fig:pip}, given an image containing the offside penalty gesture and corner-kick gesture made by two assistant referees, we may ask ``What color is the shoe of the assistant referee who is making the offside penalty gesture?".



The portability criterion evaluates whether the edited knowledge can be successfully applied to related content. Following prior work~\cite{vlkeb2024}, we adopt text portability evaluation for visual entity editing and image modality portability for visual semantic and user-specific editing to enhance visual modality evaluation.

For visual entity editing, we generate questions about the edited content, utilizing supplementary information from Wikipedia for question generation. For example, if the current entity is the Eiffel Tower and the edited content refers to the building's designer, we might create a question like, ``Who is the designer of the Eiffel Tower?" We can then generate another question about the edited content, such as asking for the designer's birth year. By combining these two questions, we can formulate the final probability question: ``In which year was the builder of the Eiffel Tower born?"

In the case of visual semantic and user-specific editing, we first combine the image of the main behavior or item with another image of the same type to create a new image, denoted as $i_e^p$. We then pose a question focusing on the differences between the two images, such as hair color or object shape. By integrating this question with one related to the edited content, we derive the final portability question. For instance, as shown in Fig.~\ref{fig:pip}, given an image that includes the offside penalty gesture and the corner-kick gesture made by two assistant referees, we might ask, ``What color is the
tops of the referee who is making the offside gesture in the image?". Denote the  portability question as $Q_{port} = (i_e^{p}, q_p, a_p)$, portability is evaluated as:
\begin{equation}
    \mathbb{E}_{(i_e^{p}, q_p, a_p) \sim Q_{port}}\mathbb{I}\left[M_{\theta}'(i_e^{p}, q_p) = a_p\right] 
\end{equation}


% we first select an edited entity  $e'$ in the description and generate a question about the entity, with the selected entity as the answer. Next, we generate a question based on wiki summary information related to the selected entity. Finally, we form the one-hop portability question by combining these questions.  For example, assume the current entity is the Eiffel Tower,  and the edited entity $e'$ in the description is the designer of the building, we might generate a question like ``Who is the designer of the building?". Then, we generate another question about the selected entity $e'$ such as birth year. Finally, by combining these two questions, we will get the final one-hop question such as ``In which year was the builder of the Eiffel Tower born?". 

% For visual semantic and user-specific editing, we consider Thus, we combine the image of the main behavior or object with another image of the same type into a new image $i_e^p$, and then we pose a question focusing on differences between two images such as hair color, or object shape. By combining this question with one related to edited content, we formulate the final one-hop question. For example, given an image of an offside penalty gesture and substitution gesture made by an assistant referee, we may ask ``what color is the shoe of the assistant referee who is making the offside penalty gesture?".

 


% We generate all questions by prompting an LLM or an LMM 





\vspace{-3mm}
\subsection{Human Check \& Benchmark Statistics}
\vspace{-3mm}
% During benchmark construction, we manually check and filter the sample many times. At the original dataset collection stage, we manually check the image of each entity, behavior, and object, which ensures the quality of the collected images. Especially for visual knowledge, the referee gestures of various sports, the traffic cop sign, and so on are done manually by ourselves. Furthermore, after counterfactual editing and question generation by LLM and LMM, we check each question and correct incorrect answers or rewrite unsuitable questions. We hope such human effort could offer a high-quality evaluation benchmark for the community.

% The overall statistics of MMKE-Bench are shown in Tab.\ref{tab:statis}. Eventually, MMKE-Bench includes three classes of editing knowledge and consists of a total of 2,940 pieces of knowledge and 7,229 images. The associated knowledge involves 110 types of knowledge, which indicates the diversity of MMKE-Bench. Following previous work~\cite{DBLP:conf/emnlp/0008TL0WC023}, we split the dataset into train and val datasets with the ratio of 4:6, where the train set is only used for training of some knowledge editing methods(e.g., SERAC~\cite{mitchell2022memory} and MEND~\cite{mitchell2021fast}). 




% During the construction of the benchmark, we manually collected, reviewed, and filtered the samples multiple times. At the original knowledge collection stage, we conducted a manual review of the images associated with each entity, behavior, and object to ensure the quality of the collected images. This process was particularly critical for visual knowledge, including referee gestures in various sports and traffic cop signals, all of which were manually curated. Additionally, after counterfactual editing and question generation by LLMs and LMMs, we manually reviewed each question, corrected any incorrect answers, and rewrote unsuitable questions. We believe that this human effort provides a high-quality evaluation benchmark for the community.



During benchmark construction, we manually collected, reviewed, and filtered the samples multiple times. In the original knowledge collection stage, we conducted a thorough manual review of the images associated with each entity, behavior, and object to ensure the quality of the collected visuals. 
% This process was particularly critical for visual knowledge, such as referee gestures in various sports and traffic cop signals, all of which were meticulously curated. 
Furthermore, after counterfactual editing and question generation, we manually reviewed the questions, revised unsuitable questions, and corrected wrong answers. 
% We believe that this human effort contributes to the creation of a high-quality evaluation benchmark for the community.

\begin{wraptable}{r}{0.47\textwidth}
\vspace{-3mm}
\centering % 这会使得整个表格水平居中
% \renewcommand{\arraystretch}{1.1}  % 调整行高
\caption{The statistics of MMKE-Bench.}
\label{tab:statis}
\resizebox{0.97\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
 & \textbf{Types} & \textbf{Train} & \textbf{Test} & \textbf{Images} \\
\midrule
\textbf{{Visual Entity Editing}}        &  {76}  & 636 & 955 & {3,534} \\
\textbf{Visual Semantic Editing}    &  {65}  & 214 & 293 & {3,201} \\
\textbf{User-Specific Editing}       &  {34}  & 331 & 511 & {1,628} \\
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\end{wraptable}


% 7229 3182  1521  2526

The statistics of MMKE-Bench are shown in Tab.\ref{tab:statis}. MMKE-Bench encompasses three classes of edited knowledge, totaling 2,940 knowledge pieces and 8,363 images. The knowledge spans {175} fine-grained types, highlighting the diversity of MMKE-Bench. We split the dataset into training and validation sets at 4:6, with the training set reserved solely for specific knowledge editing methods (e.g., SERAC~\cite{mitchell2022memory}).