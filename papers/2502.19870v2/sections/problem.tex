% \vspace{-3mm}
\section{Problem Definition}


% \vspace{-2mm}
\subsection{Knowledge Representation and Editing}


% which comprises multiple aspects of information about an entity or visual content.  

MMKE-Bench is distinctive in evaluating diverse semantic editing in realistic scenarios, leveraging natural language-based knowledge representation. It includes three types of editing: visual entity editing, visual semantic editing, and user-specific editing. Each piece of knowledge is represented in a unified format, $k= (i,d)$, where $i$ refers to the image and $d$ represents the natural language description of the main object, visual content, or a user-personalized item.  For example, in the case of a referee’s gesture, the image captures the action performed by the referee, while the description explains how the gesture is executed and its impact on the match.  During knowledge editing,   the original knowledge is transformed into $k_e= (i_e, d_e)$ in both visual entity and visual semantic editing, while it remains $k_e= (i, d)$ for user-specific editing. This is because user-specific editing introduces entirely new personalized knowledge into LMMs without needing to alter the image or description.

% MMKE-Bench stands out by evaluating diverse semantic editing in realistic scenarios, leveraging natural language-based knowledge representation. It encompasses three types of editing: visual entity editing, visual semantic editing, and user-specific editing. In each case, knowledge is represented in a unified format, $k = (i, d)$, where $i$ refers to the image and $d$ represents the natural language description of the main object, visual content, or a user-personalized item. For knowledge editing, the original knowledge transforms into $k_e = (i_e, d_e)$ in both visual entity and visual semantic editing, while it remains $k_e = (i, d)$ for user-specific editing. This is because user-specific editing introduces entirely new personalized knowledge into LMMs without altering the image or description.


% with all original user data being novel to these models.

% From the editing perspective, both the text and image modality are edited.


% On one hand, since the knowledge representation and editing in the form of triples can only convey limited information and does not conform to human usage habits, In this benchmark, we adopt free-form language expressions to define a piece of knowledge as well as corresponding editing input. The natural language format is closer to what humans typically produce or what can be found in news outlets, forums, and web pages. Additionally, similar to previous work~\citepp{}, it offers a unified approach to the editing problem, addressing a wide range of contexts and needs. On the other hand, complex visual semantic understanding is important for MLLms to understand the high-level semantics in an image, making them more practical in dynamic real-life scenes.

% In this benchmark, we construct three types of datasets: entity-level, visual semantic, and user-specific. In these datasets, the knowledge is represented in a unified format. A piece of knowledge is represented as $K= (I, D)$, where $I$ is the image, and $D$ is the description of the related information about the main object or the main visual content. The original information description comprises multiple aspects of information about an entity or visual content. From the editing perspective, a piece of original knowledge is modified as $K'= (I', D')$ for both entity-level dataset and visual semantic dataset, and as $K'= (I, D)$ for the user-specific dataset, as this dataset focuses on new knowledge insertion for LMMs.

% When evaluating the editing performance, we consider four evaluation principles and adopt LLM to generate the reliability, and generalization questions, where only the edited description is used for question generation.  The whole pipeline is shown in Fig.\ref{fig:pip}.

% \vspace{-2mm}
\subsection{Editing Type of MMKE-Bench}
% \vspace{-1mm}
% Visual semantic editing focuses on semantic-centric editing, and a piece of knowledge includes an image depicting the semantic content and the description is composed of a semantic action description and the rule or the meaning converted or associated with the semantic action.

Considering real-world needs, MMKE-Bench includes three types of editing as follows.

\paragraph{Visual Entity Editing} This type targets entity-centric modifications and the description covers multiple aspects of an entity. In realistic scenarios,  models may misidentify or retain incorrect or outdated information about the entity. Visual entity editing addresses this issue by allowing for simultaneous correction of all related content. To simulate such scenarios, we propose replacing the original image of the entity with that of another entity of the same type and modifying key information into counterfactual content. As shown in Fig.\ref{fig:exam}, Zlatan Ibrahimović's image is replaced with that of Wayne Rooney, and related information (e.g., nationality, club) is altered to counterfactual details. 

% This type of editing focuses on entity-centric modifications, where the description covers multiple aspects of the entity. In real-world scenarios, models may misidentify entities or retain incorrect or outdated information. Visual entity editing addresses these issues by allowing simultaneous corrections across all related content. To simulate such scenarios, we propose replacing the original image of the entity with that of another entity of the same type and modifying key information to counterfactual content. For instance, as illustrated in Fig.\ref{fig
% }, Zlatan Ibrahimović's image is replaced with Wayne Rooney's, and related details, such as nationality and club affiliation, are updated with counterfactual information.

% Visual semantic editing centers on semantic-focused modifications, where each piece of knowledge consists of an image representing the semantic content, accompanied by a description that depicts a semantic action and the corresponding rule or meaning associated with that action. The visual semantic includes various complex semantics like various gestures, actions, object attributes, and object interaction.
\paragraph{Visual Semantic Editing}

% In real-world scenarios, the LMMs might misrecognize complex visual semantics such as the traffic police hand gestures, human actions, or social actions, and misunderstand the implicit information associated with these actions. 

This type focuses on complex visual semantics-centric modifications, encompassing body gestures, actions, object relationships, and so on. The description provides detailed information about the semantic action and its rules or meanings. The LMMs may misrecognize and misunderstand these semantics, but visual semantic editing can address this issue by modifying both actions images, and meanings simultaneously. To simulate this, this type of editing also involves replacing the image of one semantic action with that of another action of the same type and altering the rule or meaning to counterfactual content. As shown in Fig.\ref{fig:exam}, the offside gesture in soccer is replaced with that of substitution, and the associated rule (e.g. kick-off location) is modified to counterfactual contents.



\paragraph{User-Specific Editing} This type focuses on injecting personalized user information into LMMs, and the description details the relationship between the user and the object, as well as their experiences. As there is a growing demand for LMMs to function as personalized AI assistants that can remember relevant user information, user-specific editing is designed to meet this need. Pre-trained LMMs serve as general models, so all user-specific information is treated as new knowledge for LMM. Thus, counterfactual editing is unnecessary, and original knowledge is used as editing knowledge. For example, Fig.\ref{fig:exam} describes the relationship between the toy puppet and the user's habits.

