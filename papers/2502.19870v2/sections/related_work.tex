\vspace{-6mm}
\section{Related Work}

\subsection{Large Multimodal Model}
Large multimodal models have achieved excellent performance in various multimodal understanding tasks due to vast knowledge and effective cross-modality alignment. Typically, such models integrate a vision encoder with a pertained large language model, linking the two components by an alignment module. Notably, BLIP-2~\citep{li2023blip} adopts Q-Former, a lightweight Transformer, as the alignment module. Inspired by the instruction tuning in LMM, MiniGPT-4~\citep{zhu2023minigpt} and InstructBLIP~\citep{instructblip} enhance this structure with multimodal instruction tuning. In contrast, LLaVA~\citep{liu2024visual} utilizes an MLP layer for alignment and proposes to generate an instruction-tuning dataset by self-instruct strategy~\citep{wang2022self}. Qwen-VL~\citep{bai2023qwen} introduces a novel module, the visual receptor, as its alignment module and proposes a three-stage training pipeline, achieving excellent performance across various multimodal tasks. Besides, several notable LMMs, such as mPLUG-DocOw 1.5~\citep{hu2024mplug}, InternVL-2 ~\citep{chen2024far}, and MiniCPM-V 2.5~\citep{yao2024minicpm}, have also achieved comparable or even superior results compared with GPT-4o.

\subsection{Knowledge editing for large language model}


% Knowledge editing presents a promising solution for effectively and economically updating the knowledge of LLM without retraining.
% Because LLMs capabilities and human cognitive processes are very similar, especially in learning and acquiring knowledge. Therefore, 
Existing methods for LLM can be divided into three categories: resorting to external knowledge, incorporating knowledge into the model, and editing internal knowledge. Resorting to external knowledge typically involves maintaining memory and retrieving the most relevant cases for each input. For instance, IKE \cite{zheng2023can}   provides in-context learning example support by building three types of demo examples: copy, update, and retain.  SERAC \cite{mitchell2022memory} builds a new counterfactual model by keeping the base model and using a scope classifier to determine whether to answer with a counterfactual model. The category of merging the knowledge into the model aims to learn representations of the new knowledge and incorporate this information into the model. Eva-KELLM  \cite{Wulora} employs LoRA for knowledge editing, while GRACE~\citep{DBLP:conf/nips/HartvigsenSPKG23} adopts a novel approach by maintaining a discrete codebook functioning as an adapter. Lastly, editing intrinsic knowledge works on directly modifying the model's weight using knowledge-specific methods through meta-learning and localization editing. The meta-learning method trains a hypernetwork to learn how to adjust the model. KE \cite{de2021editing} utilizes new knowledge representations directly to train the model to update the matrix, while MEND \cite{mitchell2021fast} applies rank-one decomposition to divide the model into two rank matrices. Additionally, localization approaches, like ROME \cite{rome22} and MEMIT, \cite{MEMIT23} employ a causal analysis method to detect which parts of the hidden state are more important by treating editing as minimal optimization, ensuring its reliability and non-circumvention. 


\subsection{Knowledge editing for large multimodal model}

Recently, several benchmarks have been proposed to evaluate the performance of editing LMMs. The MMEdit benchmark~\citep{mmedit2023} systematically defines the first evaluation framework for multimodal knowledge editing based on visual question answering and image caption tasks. As the MMEdit could not assess fine-grained entity knowledge, subsequent evaluation benchmarks focus on fine-grained entity recognition editing. MIKE~\citep{mike2024} evaluates recognizing new entities while VLKEB~\citep{vlkeb2024} targets editing known entities and introduces a portability evaluation principle. MC-MKE~\citep{mcmke2024} further extends fine-grained entity recognition by emphasizing modality consistency. However, these benchmarks mainly represent editing knowledge through triples and overlook diverse semantic editing in realistic scenarios. 
% MMKE-Bench overcomes these issues by evaluating complex semantic editing in realistic scenarios in natural language format, thereby advancing multimodal editing research.

