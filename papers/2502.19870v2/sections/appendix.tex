
\section{Benchmark construction}

\subsection{Original knowledge collection}
% \subsubsection{Image source}

In our process of gathering original knowledge, we begin by listing candidate fine-grained entities, visual semantics, or user-specific items, and subsequently collect their corresponding images.


For visual entity editing, we source candidates from two datasets: The multimodal knowledge graph, MMpedia~\citep{wu2023mmpedia}, and the visual entity recognition dataset, OVEN~\citep{hu2023open}. 
Given the extensive size of MMpedia, we filter entities with Wikipedia summaries of fewer than 40 words and eliminate candidates that cannot uniquely identify the main entity through images. Using the Wikipedia API, we retrieve the entity type and select the most popular 10\% within each type. We further apply optical character recognition (OCR) to exclude images containing entity names, such as university logos. After this, we gather images from the relevant datasets and manually remove any noisy images, or crawl additional images from Google for entities with fewer than two images. The same process is applied to the OVEN dataset, except without sampling.


 
\begin{table}[tbp]
\centering % 这会使得整个表格水平居中
\renewcommand{\arraystretch}{1.0}  % 调整行高
\caption{The image source of visual semantic knowledge in MMKE-Bench.}
\label{tab:vis_source}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{ll}
\toprule
Type & Source  \\
\midrule
Human Action & Crawling from google\\ 
Life Gesture & Crawling from google \\
Emotion & \makecell[l]{LFW-emotion dataset \\ 
\url{https://huggingface.co/datasets/TrainingDataPro/facial-emotion-recognition-dataset}}\\
Referee Gesture & Demo videos from Youtube and Bilibili \\ 
Traffic Cop Sign & Crawling from google \\
Traffic Sign  & \makecell[l]{TSRD  dataset \\ \url{https://nlpr.ia.ac.cn/PAL/TRAFFICDATA/recognition.html}}\\
Texture & DTD dataset~\citep{cimpoi2014describing}\\
Color &  Crawling from google \\
Shape & Crawling from google \\
Animal Body Language & Crawling from google \\
Relationship & Siwg-HOI~\citep{wang2021discovering} and  \\
Social  action & Crawling from google \\
Layout & Crawling from google \\
Art Style & \makecell[l]{Wiki-art dataset~\citep{wiki-art_dataset} \\ 
\url{https://huggingface.co/datasets/keremberke/painting-style-classification} }\\
\bottomrule
\end{tabular}
}
\end{table}



% As the number of MMpedia is very large, we filter the entities of which the number of Wikipedia summaries is less than 40 words and remove the candidates that can not uniquely determine the main entity from the images. Then, we get the entity type from Wikipedia API and we sample the most popular 10\% from each type of the remaining candidate entities. Next, we adopt OCR to remove the images with the entity name in the images like the university logo. Lastly, we get their images from corresponding datasets, and manually remove noise images. As for the OVEN dataset, we adopt the same process except for sampling.


% For visual semantic editing, we first list semantic candidates from four broad categories: single-person behavior, single-object behavior or attributes, object relationships, and global structure. The single-person behavior category includes human actions, life gestures, referee signals, traffic cop signs, and emotions. Single-object behavior or attributes cover animal body language, traffic signs, colors, shapes, and textures. Object relationships involve human-object interactions and social actions, while global structure encompasses layout and art styles. W
% images for human actions, gestures, traffic signs, colors, shapes, social actions, animal body language, and layout are sourced from Google. Images for traffic signs, textures, relationships, emotions, and art styles come from existing datasets. Referee gesture images are obtained by extracting frames from demo videos on YouTube and Bilibili.
% If there are available datasets like texture, we collect the entities and their images from existing datasets. Otherwise, we manually list the candidate according to domain knowledge and then collect images from various sources. We list the source for each type in Tab.\ref{tab:vis_source}. Among them, 

For visual semantic editing, we first list the semantic candidates from four broad categories: single-person behavior, single-object behavior or attributes, object relationship, and global structure. The single-person behavior includes human action, life gestures, referee gestures, traffic cop signs, and emotion. The single-object behavior or attribute covers animal body language,  traffic signs, color, shape, and texture. The object relationship involves human-object 
interactive relationship and social actions, while global structure encompasses layout and art style.
Where datasets exist, such as for texture, we gather the entities and images from existing sources. Otherwise, we manually curate the candidates using domain expertise and collect images from various sources. The sources for each type are listed in Tab.\ref{tab:vis_source}. Specifically, images for human action,  life gestures, traffic cop signs, color, shape, social action, animal body language, and layout are crawling from Google. Images for traffic signs, textures, relationships, emotions, and art styles come from existing datasets. Referee gesture images are collected by extracting frames from demo videos on YouTube and Bilibili.



As for user-specific editing, we consider nine types of personal information, including items, pets, actors, singers, cartoon characters, organizations, universities, sports clubs, and companies. The candidate relationships between users and these objects are outlined in Tab.\ref{tab:user_relation}, including examples like "employed at," "exchanged at," "studied at," and "favorite" for universities. We collect images for these items from various sources. For items and pets, candidates and images are sourced from existing datasets used for personalized large multimodal research~\citep{nguyen2024yo,alaluf2024myvlm}. For organizations, universities, sports clubs, and companies, we follow the same process as in visual entity editing, using data from MMpedia. For actors, singers, and cartoon characters, images are collected from Google.

To sum up, this benchmark covers a total of 2,940 pieces of knowledge, along with 8,363 images from 33 broad categories, and detailed type names are shown in Tab.\ref{tab:data_type}.

% After collecting images, we generate the description for each entity, visual semantic, and user-specific item in a natural language form. As for visual entity, we collect the description from Wikipedia summary and if the summary is too long, we adopt LLM to shorten it to less than 100.00 words.
% For visual semantic editing, the description includes the language description of the action and the meaning or the rule of the action. We collect them from the corresponding domain knowledge by humans or explain by LLM.  For user-specific editing, we select one relationship from the candidate and use LLM to make up a description about user personal information.


% \begin{table}[tbp]
% \centering % 这会使得整个表格水平居中
% \renewcommand{\arraystretch}{0.9}  % 调整行高
% \caption{The data type in MMKE-Bench.}
% \label{tab:data_type}
% \resizebox{0.9\linewidth}{!}{
% \begin{tabular}{clp{6.4cm}}
% \toprule
% \multicolumn{1}{l}{}   & \textbf{Broad Categories}      & \textbf{Types}    \\
% \midrule
% & Person&  Human\\
% & Aerial Animals       &  Bird, Dragonfly, Fly, Butterfly, Grasshopper, Wasp, Insect, Animal   \\
% & Marine Animals       &  Jellyfish, Turtle, Sea Star, Fish, Crab, Sea Lion  \\
% & Terrestrial Animals  & Bear,  Monkey, Amphibian, Mammal, Rodent, Wild Boar,Squirrel, Dog Breed, Fox, Wolf, Tick, Rabbit, Rhinoceros, Arthropod, Animal, Salamander, Spider, Mollusc, Crustacean,  Toad, Cat Breed, Deer, Beetle, Sloth, Frog, Mollusk, Snail, Hedgehog, Cat, Leopard,  Pangolin, Dog, Cattle, Millipede,Moth, Snake, Lizard, Antelope \\
% & Virtual Character    &   Animated Character, Anime Character, Comics Character   \\
% & Plant &  Fruit, Tree, Flower, Mushroom, Orchid,  Vegetable, Fungus, Plant \\
% & Building     &  Building, Church Building, Monument,  Tower, Sculpture, Statue    \\
% & Musical Group&  Musical Group\\
% & Vehicle     &  Car, Aircraft Model, Aircraft, Vehicle    \\
% \multirow{-20}{*}{\textbf{\makecell{Visual Entity \\ Editing}}}     & Others&  Instrument, Ball   \\
% \midrule
% & Human Action & Body Posture Adjustments, Head Adjustments, Hand Actions, Leg Actions, Whole-Body Actions, Eye Expressions, Facial Expressions, Water Sports, Sound Actions, Object Actions, Repair or Construction Actions, Cleaning, Hunting , Crushing, Human Body Actions, Stabbing, Sticking or Connecting Actions, Tools or Weapons Actions, Cutting, Packaging or Storage Actions, Whole Body Actions, Pinching, Inspection or Observation Actions, Head Adjustments, Object Actions\\
% & Life Gesture & Life Gesture Number, Life Gesture    \\
% & Emotion      & Emotion Sign \\
% & Referee Gesture      & Soccer Linesman, Soccer, Basketball, Volleyball, Volleyball Card, Baseball, Puck, Fencing, Handball, Badminton, Table Tennis  \\
% & Traffic Cop Sign     & Traffic Cop Sign     \\
% & Traffic Sign & Traffic Sign Forbidden, Traffic Sign Allow, Traffic Sign Point    \\
% & Texture      & Texture      \\
% & Color & Color \\
% & Animal Body Language & Monkey Body Language, Cat Body Language, Dog Body Language, Animal Actions\\
% & Shape & Circular Shapes, Triangles, Special Plane Shapes, Common Polyhedrons, Solids of Revolution, Special Shapes     \\
% & Social Action& Social Action, Agriculture, Cooking Actions, Using Tools, Communication or Giving Actions, Painting Depicting\\
% & Art Style    & Art Style    \\
% & Layout & Layout \\
% \multirow{-24}{*}{\textbf{\makecell{Visual Semantic \\ Editing}}} & Relationship &  Relationship, Burning Scalding , Containers or Liquids Actions, Striking, Impacting, Solids of Revolution, Protection\\
% \midrule
% & Item  & Cup, Toy Puppet, Statue, Toy, Plush Doll, Toy Doll,  Puppet Cow, Cat Figurine, Bean Bag, Saving Pot, Shoes, Pillow, Pen Container, Throw Pillow Doll\\
% & Actor & Actor\\
% & Singer & Singer       \\
% & Cartoon Character    & Cartoon Character   \\
% & Organization & Nonprofit Organization, Organization\\
% & University   & University   \\
% & Sports Club  & Baseball Team, Basketball Team, Sports Club, Sports Team, Association Football Team, Canadian Football Club, Futsal Team, Field Hockey Club  \\
% \multirow{-10}{*}{\textbf{\makecell{User-Specific \\ Editing}}}     & Pet   & Pet dog, Pet cat     \\
% \multicolumn{1}{l}{}   & Company      & Airline, Enterprise, Company, Public Company, Dot-Com Company, Media Company \\  
% \bottomrule
% \end{tabular}
% }
% \end{table}

\begin{table}[tbp]
\centering % 使表格水平居中
\renewcommand{\arraystretch}{0.9}  % 调整行高
\caption{The data type in MMKE-Bench.}
\label{tab:data_type}
\resizebox{0.9\linewidth}{!}{ % 调整宽度到 95%
\begin{tabular}{clp{7.2cm}} % 调整最后一列的宽度以匹配表格扩展
\toprule
\multicolumn{1}{l}{}   & \textbf{Broad Categories}      & \textbf{Types}    \\
\midrule
& Person&  Human\\
& Aerial Animals       &  Bird, Dragonfly, Fly, Butterfly, Grasshopper, Wasp, Insect, Animal   \\
& Marine Animals       &  Jellyfish, Turtle, Sea Star, Fish, Crab, Sea Lion  \\
& Terrestrial Animals  & Bear,  Monkey, Amphibian, Mammal, Rodent, Wild Boar, Squirrel, Dog Breed, Fox, Wolf, Tick, Rabbit, Rhinoceros, Arthropod, Salamander, Spider, Mollusc, Crustacean,  Toad, Cat Breed, Deer, Beetle, Sloth, Frog, Mollusk, Snail, Hedgehog, Cat, Leopard,  Pangolin, Dog, Cattle, Millipede, Moth, Snake, Lizard, Antelope \\
& Virtual Character    &   Animated Character, Anime Character, Comics Character   \\
& Plant &  Fruit, Tree, Flower, Mushroom, Orchid,  Vegetable, Fungus, Plant \\
& Building     &  Building, Church Building, Monument,  Tower, Sculpture, Statue    \\
& Musical Group&  Musical Group\\
& Vehicle     &  Car, Aircraft Model, Aircraft, Vehicle    \\
\multirow{-20}{*}{\textbf{\makecell{Visual Entity \\ Editing}}}     & Others&  Instrument, Ball   \\
\midrule
& Human Action & Body Posture Adjustments, Head Adjustments, Hand Actions, Leg Actions, Whole-Body Actions, Eye Expressions, Facial Expressions, Water Sports, Sound Actions, Object Actions, Repair or Construction Actions, Cleaning, Hunting, Crushing, Human Body Actions, Stabbing, Sticking or Connecting Actions, Tools or Weapons Actions, Cutting, Packaging or Storage Actions, Pinching, Inspection or Observation Actions\\
& Life Gesture & Life Gesture Number, Life Gesture    \\
& Emotion      & Emotion Sign \\
& Referee Gesture      & Soccer Linesman, Soccer, Basketball, Volleyball, Volleyball Card, Baseball, Puck, Fencing, Handball, Badminton, Table Tennis  \\
& Traffic Cop Sign     & Traffic Cop Sign     \\
& Traffic Sign & Traffic Sign Forbidden, Traffic Sign Allow, Traffic Sign Point    \\
& Texture      & Texture      \\
& Color & Color \\
& Animal Body Language & Monkey Body Language, Cat Body Language, Dog Body Language, Animal Actions\\
& Shape & Circular Shapes, Triangles, Special Plane Shapes, Common Polyhedrons, Solids of Revolution, Special Shapes     \\
& Social Action& Social Action, Agriculture, Cooking Actions, Using Tools, Communication or Giving Actions, Painting Depicting\\
& Art Style    & Art Style    \\
& Layout & Layout \\
\multirow{-24}{*}{\textbf{\makecell{Visual Semantic \\ Editing}}} & Relationship &   Burning Scalding, Containers or Liquids Actions, Striking, Impacting, Solids of Revolution, Protection\\
\midrule
& Item  & Cup, Toy Puppet, Statue, Toy, Plush Doll, Toy Doll,  Puppet Cow, Cat Figurine, Bean Bag, Saving Pot, Shoes, Pillow, Pen Container, Throw Pillow Doll\\
& Actor & Actor\\
& Singer & Singer       \\
& Cartoon Character    & Cartoon Character   \\
& Organization & Nonprofit Organization, Organization\\
& University   & University, Private University  \\
& Sports Club  & Baseball Team, Basketball Team, Sports Club, Sports Team, Futsal Team ,Football Club \\
\multirow{-10}{*}{\textbf{\makecell{User-Specific \\ Editing}}}     & Pet   & Pet dog, Pet cat     \\
\multicolumn{1}{l}{}   & Company      & Airline, Company, Public Company, Dot-Com Company, Media Company \\  
\bottomrule
\end{tabular}
}
\end{table}


After collecting the images, we generate natural language descriptions for each entity, visual semantic, and user-specific item. For visual entities, we retrieve descriptions from the Wikipedia summary, and if the summary is too lengthy, we use a large language model (LLM) to condense it to fewer than 100 words. For visual semantic editing, the description includes both a language description of the action and an explanation of its meaning or rule. These are gathered either from relevant domain knowledge by ourselves or generated with the help of an LLM. For user-specific editing, we select one relationship from the candidate list and use an LLM to craft a personalized description of the user's personal information.

% We then list the candidate relationship between the user and the object in Tab.\ref{tab:user_relation}, such as employed at, exchanged at, studied at, and favorite for university. We also collect their images from various sources. For items and pets, we collect the candidate and images from existing datasets for personalized large multimodal research~\citep {}. For organizations, universities, sports clubs,  and companies, we collect them from MMpeida and adopt the same process as visual entity editing. For actors, singers, and cartoon characters, we collect their images from Google.


% For user-specific editing, we consider nine categories of personal information, including items, pets, actors, singers, cartoon characters, organizations, universities, sports clubs, and companies. T





\begin{table}[tbp]
\centering % 这会使得整个表格水平居中
\renewcommand{\arraystretch}{1.0}  % 调整行高
\caption{The relationship between humans and the objects and data source of user-specific data in MMKE-Bench.}
\label{tab:user_relation}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lll}
\toprule
Categories & Relationship & Image Source \\
\midrule
Company & Employed at, Interned at, collaborated with, Favorite & MMpedia\\
Organization & Employed at, Interned at, Helped by, Favorite & MMpedia\\
University & Employed at, Exchanged at, Studied at, Traveled to, Favorite & MMpedia \\
Club & Employed at, Visited, Favorite & MMpedia\\
Cartoon character & Favorite & Crawling from Google \\
Actor & Favorite, Admire most & Crawling from Google \\
Singer & Favorite, Admire most & Crawling from Google\\
Pet & Owned & MyVLM~\citep{alaluf2024myvlm} and YoLLaVA~\citep{nguyen2024yo} \\
Item & Owned & MyVLM~\citep{alaluf2024myvlm} and YoLLaVA~\citep{nguyen2024yo}\\
\bottomrule
\end{tabular}
}
\end{table}




\subsection{Editing knowledge generation}


After collecting the original knowledge, we perform \textbf{counterfactual editing} to generate alternative knowledge for both visual entity and visual semantic editing. To achieve this, we prompt a large language model (LLM) with in-context examples. For visual entity editing, we modify key details, such as nationality, alma mater, and occupation of a person, into counterfactual variations. For visual semantic knowledge, we alter the rules or meanings, such as the location where a free kick is taken, into counterfactual scenarios. The specific prompt used is shown in Tab.\ref{fig:prompt_edit}.

In addition to text-based editing, we also perform image modality editing by replacing the image of an entity or action with one from another entity or action of the same type. This replacement strategy is consistent with existing benchmarks~\citep{vlkeb2024}.


\subsection{Evaluation question generation}


When generating evaluation questions, we adhere to four key principles: reliability, locality, generalization, and portability. For locality questions, we source them from existing benchmarks. For reliability, we generate questions by prompting a large language model (LLM) with in-context examples, ensuring that each question is related to one of the edited contents. In image reliability, we refer to the main object in the image using its type, such as ``the person in the image." For portability, during visual entity editing, we follow previous benchmarks by providing additional information about the edited content to ensure text portability. In visual semantic editing and user-specific editing, we focus on image portability by combining the current object’s image with another object of the same type. We then create a final one-hop question by merging the counterfactual content-related question with an easier, image-based question, such as asking about the color of shoes. After generating the questions and answers, we conduct a human review to verify the accuracy, rewriting any incorrect questions or answers. The prompts used for question generation are shown in Tab.\ref{fig:prompt_rel} and Tab.\ref{fig:prompt_port}.


% \subsection{More examples}


\section{Experiments}


We conduct experiments using the VLKEB library\footnote{\url{https://github.com/VLKEB/VLKEB}}, which employs PyTorch and integrates several knowledge editing methods and large multimodal models. The experiments are performed on NVIDIA A100/A800 80GB GPUs. The knowledge editing methods, and large multimodal models adopted in this study are listed below, with their hyper-parameters detailed in Tab.\ref{tab:hyper1}, Tab.\ref{tab:hyper2}, and Tab.\ref{tab:hyper3}.

 % we conduct experiments based on VLKEB library\footnote{\url{https://github.com/VLKEB/VLKEB}}, which employes PyTorch and integrates several knowledge editing methods and large multimodal models. In this study, we conduct experiments with NVIDIA A100.00/A800 80GB GPUs. The adopted knowledge editing methods and large multimodal models are shown in the following and The parameters are shown in Tab.\ref{}.



\begin{figure}[t] % r: 右侧, l: 左侧
    \vspace{-9mm}
    \centering
% \includegraphics[width=0.36\textwidth]{figs/blip_v1.pdf}
\includegraphics[width=0.36\textwidth]{fig/blip_v1.pdf}
    \caption{Evaluation comparison of IKE for BLIP2 with existing benchmarks. I-Gen and Port for MMEdit, along with Port for MIKE, is set 1, as they ignore the relevant criteria. }
\label{fig:res_com_blip2}
    \vspace{-6mm}
\end{figure}


\paragraph{MLLMs.} To evaluate our benchmark, we conduct experiments on three representative MLLMs.
\begin{itemize}
    \item \textbf{BLIP-2}~\citep{li2023blip}: BLIP2 effectively leverages both frozen pre-trained image models and language models by bootstrapping vision-language pre-training, and bridges the modality gap with a lightweight Querying Transformer. We follow previous work~\citep{vlkeb2024,mmedit2023}, and select BLIP-2 OPT as the basic edit model, where the vision model is ViT-L and the LLM is OPT model. 
    \item \textbf{MiniGPT-4}~\citep{bai2023qwen}: MiniGPT-4 aligns a frozen visual encoder module with a frozen advanced LLM using one projection layer. The LLM is Vicuna and the vision model is ViT.
    \item \textbf{LLaVA-1.5}~\citep{liu2024visual}: LLaVA-1.5 is an improved version of LLaVA, which is an end-to-end trained large multimodal model that connects a vision encoder and an LLM with an MLP projector for visual and language understanding. We select LLaVA-1.5 7B as the base model where CLIP-ViT-L-336px is the vision model and Vicuna-7B is the LLM.
\end{itemize}

\paragraph{Editing Methods.} Following the previous benchmarks~\citep{vlkeb2024}, we select five representative multimodal knowledge editing methods to conduct experiments.

\begin{itemize}
    \item \textbf{Fine-tuning (FT)}: Fine-tuning has become a widely used strategy for adapting pre-train models to specific tasks. We focus on finetuning two parts: the LLM and the vision-language alignment module, where only the last layer of the LLM is fine-tuned.
    \item \textbf{Knowledge Editor (KE)}~\citep{de2021editing}: KE is a method that can be used to edit this knowledge in the base model without the need for expensive retraining or fine-tuning. It uses a hyper-network with constrained optimization to predict the weight update at test time.
    \item \textbf{MEND}~\citep{mitchell2021fast}: MEND makes fast, local edits to a pre-trained model’s behavior using a single desired input-output pair. It learns to transform the gradient of standard fine-tuning, using a low-rank decomposition of the gradient.
    \item \textbf{SERAC}~\citep{mitchell2022memory}: SERAC is a memory-based method and it stores edits in explicit memory. It also introduces a scope classifier and counterfactual model, where the scope classifier is to determine whether the memory contains inputs relevant to processing them. If determined, the input is combined with the most relevant cache item into the counterfactual model for prediction.
    \item \textbf{In-context Knowledge Editing (IKE)}~\citep{zheng2023can}: IKE is inspired by in-context learning, and a new demonstration formatting and organization strategies are to construct suitable in-context learning demonstrations for guiding knowledge editing.
\end{itemize}


\vspace{-3mm}
\section{More results}


\paragraph{Comparison of evaluation results with existing benchmarks for BLIP2} The Comparison of evaluation results with existing benchmarks of IKE for BLIP2 is shown in Fig.~\ref{fig:res_com_blip2}. As we can see, IKE achieves high results in existing benchmarks, while it performs worse in our benchmark, indicating the proposed benchmark is more challenging.

\paragraph{ Results of sequential editing for BLIP-2} We additionally report the results of sequential editing for BLIP-2 on MMKE-Bench, as shown in Tab.\ref{res_seq_edit_blip}. As we can see, FT-LLM and FT-Alignment tend to forget previous knowledge while SERAC is better at keeping edited knowledge.


\begin{table}[tbp]
\centering
\renewcommand{\arraystretch}{0.95}
\caption{The hyper-parameters of knowledge editing methods and LMMs on the visual entity editing.}
\label{tab:hyper1}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{lllll}
\toprule
\multicolumn{5}{c}{ \textbf{FT-LLM}}  \\
\midrule
\textbf{Models} & \textbf{Steps} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{Edit LR} \\
\midrule
BLIP2-OPT & 30 & $31^{st}$ layer of Transformer Module & AdamW & $2 \mathrm{e}-4$ \\
MiniGPT-4 & 40 & $31^{st}$ layer of Transformer Module & AdamW & $1 \mathrm{e}-4$ \\
LLaVA-1.5 & 40 & $31^{st}$ layer of Transformer Module & AdamW & $1 \mathrm{e}-4$ \\
\midrule
\multicolumn{5}{c}{ \textbf{FT-Alignment}} \\
\midrule
\textbf{Models} & \textbf{Steps} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{Edit LR} \\
\midrule
BLIP2-OPT & 30 & Qformer & AdamW & $2 \mathrm{e}-4$ \\
MiniGPT-4 & 30 & Qformer & AdamW & $1 \mathrm{e}-4$ \\
LLaVA-1.5 & 30 & mm\_projector & AdamW & $1 \mathrm{e}-4$ \\
\midrule
\multicolumn{5}{c}{ \textbf{ MEND }} \\
\midrule
\textbf{Models} & \textbf{MaxIter} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{LR} \\
\midrule
BLIP2-OPT & 10,000 & layer $29,30,31$ of Transformer Module & Adam & $1 \mathrm{e}-6$ \\
MiniGPT-4 & 30,000 & layer $29,30,31$ of Transformer Module & Adam & $1 \mathrm{e}-6$ \\
LLaVA-1.5 & 10,000 & layer $29,30,31$ of Transformer Module & Adam & $1 \mathrm{e}-6$ \\
\midrule
\multicolumn{5}{c}{ \textbf{ SERAC }} \\
\midrule
\textbf{Models} & \textbf{MaxIter} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{LR} \\
\midrule
BLIP2-OPT & 10,000 & all layers of OPT-125M & Adam & $1 \mathrm{e}-5$ \\
MiniGPT-4 & 20,000 & $31^{st}$ layer of Vicuna-7B & Adam & $5 \mathrm{e}-5$ \\
LLaVA-1.5 & 10,000 & $31^{st}$ layer of Vicuna-7B-v1.5 & Adam & $1 \mathrm{e}-5$ \\
\midrule
\multicolumn{5}{c}{ \textbf{ KE }} \\
\midrule
\textbf{Models} & \textbf{MaxIter} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{LR} \\
\midrule
BLIP2-OPT & 10,000 & layer $29,30,31$ of Transformer Module & RMSprop & $3 \mathrm{e}-4$ \\
MiniGPT-4 & 10,000 & layer $29,30,31$ of Transformer Module & RMSprop & $3 \mathrm{e}-4$ \\
LLaVA-1.5 & 10,000 & layer $29,30,31$ of Transformer Module & RMSprop & $3 \mathrm{e}-4$ \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[tbp]
\centering
\renewcommand{\arraystretch}{0.95}
\caption{The hyper-parameters of knowledge editing methods and LMMs on visual semantic
editing.}
\label{tab:hyper2}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{lllll}
\toprule
\multicolumn{5}{c}{ \textbf{FT-LLM}}  \\
\midrule
\textbf{Models} & \textbf{Steps} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{Edit LR} \\
\midrule
BLIP2-OPT & 30 & $31^{st}$ layer of Transformer Module & AdamW & $2 \mathrm{e}-4$ \\
MiniGPT-4 & 40 & $31^{st}$ layer of Transformer Module & AdamW & $1 \mathrm{e}-4$ \\
LLaVA-1.5 & 40 & $31^{st}$ layer of Transformer Module & AdamW & $1 \mathrm{e}-4$ \\
\midrule
\multicolumn{5}{c}{ \textbf{FT-Alignment}} \\
\midrule
\textbf{Models} & \textbf{Steps} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{Edit LR} \\
\midrule
BLIP2-OPT & 30 & Qformer & AdamW & $2 \mathrm{e}-4$ \\
MiniGPT-4 & 30 & Qformer & AdamW & $1 \mathrm{e}-4$ \\
LLaVA-1.5 & 30 & mm\_projector & AdamW & $1 \mathrm{e}-4$ \\
\midrule
\multicolumn{5}{c}{ \textbf{ MEND }} \\
\midrule
\textbf{Models} & \textbf{MaxIter} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{LR} \\
\midrule
BLIP2-OPT & 20,000 & layer $29,30,31$ of Transformer Module & Adam & $1 \mathrm{e}-6$ \\
MiniGPT-4 & 30,000 & layer $29,30,31$ of Transformer Module & Adam & $1 \mathrm{e}-6$ \\
LLaVA-1.5 & 20,000 & layer $29,30,31$ of Transformer Module & Adam & $1 \mathrm{e}-6$ \\
\midrule
\multicolumn{5}{c}{ \textbf{ SERAC }} \\
\midrule
\textbf{Models} & \textbf{MaxIter} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{LR} \\
\midrule
BLIP2-OPT & 20,000 & all layers of OPT-125M & Adam & $1 \mathrm{e}-5$ \\
MiniGPT-4 & 20,000 & $31^{st}$ layer of Vicuna-7B & Adam & $5 \mathrm{e}-5$ \\
LLaVA-1.5 & 20,000 & $31^{st}$ layer of Vicuna-7B-v1.5 & Adam & $1 \mathrm{e}-5$ \\
\midrule
\multicolumn{5}{c}{ \textbf{ KE }} \\
\midrule
\textbf{Models} & \textbf{MaxIter} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{LR} \\
\midrule
BLIP2-OPT & 10,000 & layer $29,30,31$ of Transformer Module & RMSprop & $3 \mathrm{e}-4$ \\
MiniGPT-4 & 10,000 & layer $29,30,31$ of Transformer Module & RMSprop & $3 \mathrm{e}-4$ \\
LLaVA-1.5 & 10,000 & layer $29,30,31$ of Transformer Module & RMSprop & $3 \mathrm{e}-4$ \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[tbp]
\centering
\renewcommand{\arraystretch}{0.95}
\caption{The hyper-parameters of knowledge editing methods and LMMs on user-specific
editing.}
\label{tab:hyper3}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{lllll}
\toprule
\multicolumn{5}{c}{ \textbf{FT-LLM}}  \\
\midrule
\textbf{Models} & \textbf{Steps} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{Edit LR} \\
\midrule
BLIP2-OPT & 30 & $31^{st}$ layer of Transformer Module & AdamW & $2 \mathrm{e}-4$ \\
MiniGPT-4 & 40 & $31^{st}$ layer of Transformer Module & AdamW & $1 \mathrm{e}-4$ \\
LLaVA-1.5 & 40 & $31^{st}$ layer of Transformer Module & AdamW & $1 \mathrm{e}-4$ \\
\midrule
\multicolumn{5}{c}{ \textbf{FT-Alignment}} \\
\midrule
\textbf{Models} & \textbf{Steps} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{Edit LR} \\
\midrule
BLIP2-OPT & 30 & Qformer & AdamW & $2 \mathrm{e}-4$ \\
MiniGPT-4 & 30 & Qformer & AdamW & $1 \mathrm{e}-4$ \\
LLaVA-1.5 & 20 & mm\_projector & AdamW & $1 \mathrm{e}-4$ \\
\midrule
\multicolumn{5}{c}{ \textbf{ MEND }} \\
\midrule
\textbf{Models} & \textbf{MaxIter} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{LR} \\
\midrule
BLIP2-OPT & 10,000 & layer $29,30,31$ of Transformer Module & Adam & $1 \mathrm{e}-6$ \\
MiniGPT-4 & 30,000 & layer $29,30,31$ of Transformer Module & Adam & $1 \mathrm{e}-6$ \\
LLaVA-1.5 & 10,000 & layer $29,30,31$ of Transformer Module & Adam & $1 \mathrm{e}-6$ \\
\midrule
\multicolumn{5}{c}{ \textbf{ SERAC }} \\
\midrule
\textbf{Models} & \textbf{MaxIter} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{LR} \\
\midrule
BLIP2-OPT & 10,000 & all layers of OPT-125M & Adam & $1 \mathrm{e}-5$ \\
MiniGPT-4 & 20,000 & $31^{st}$ layer of Vicuna-7B & Adam & $5 \mathrm{e}-5$ \\
LLaVA-1.5 & 10,000 & $31^{st}$ layer of Vicuna-7B-v1.5 & Adam & $1 \mathrm{e}-5$ \\
\midrule
\multicolumn{5}{c}{ \textbf{ KE }} \\
\midrule
\textbf{Models} & \textbf{MaxIter} & \textbf{Edit Layer} & \textbf{Optimizer} & \textbf{LR} \\
\midrule
BLIP2-OPT & 10,000 & layer $29,30,31$ of Transformer Module & RMSprop & $3 \mathrm{e}-4$ \\
MiniGPT-4 & 10,000 & layer $29,30,31$ of Transformer Module & RMSprop & $3 \mathrm{e}-4$ \\
LLaVA-1.5 & 10,000 & layer $29,30,31$ of Transformer Module & RMSprop & $3 \mathrm{e}-4$ \\
\bottomrule
\end{tabular}}
\end{table}








% \begin{table}[tbp]
% \vspace{-3mm}
% \centering
% \renewcommand{\arraystretch}{0.95}
% % \vspace{-3mm}
% \caption{The results of sequential editing for BLIP2 on MMKE-Bench.}
% \label{res_seq_edit_blip}
% \resizebox{0.95\linewidth}{!}{
% \begin{tabular}{ccccccccc}
% \toprule
% \multicolumn{1}{l}{}    & \textbf{Method}    & \textbf{Gap/Num} & \textbf{T-Loc}  & \textbf{I-Loc}  & \textbf{T-Rel}  & \textbf{I-Rel}  & \textbf{I-Gen}  & \textbf{Port}   \\
% \midrule
% \multirow{12}{*}{\textbf{\makecell{Visual Entity \\ Editing}}}   & \multirow{4}{*}{\textbf{FT-LLM}} & - & 76.76 & 16.87 & 45.78 & 41.72 & 41.55 & 47.36 \\
%     &   & \textbf{3}  & 56.03 & 8.31  & 44.62 & 39.34 & 40.18 & 35.59 \\
%     &   & \textbf{6}  & 54.99 & 8.15  & 43.75 & 39.55 & 39.67 & 35.56 \\
%     &   & \textbf{10} & 54.75 & 8.08  & 42.76 & 38.01 & 38.55 & 36.08 \\
%     \cmidrule(r){2-9} 
%     & \multirow{4}{*}{\textbf{FT-Alignment}} & - & 100.00   & 8.22  & 36.37 & 35.03 & 37.53 & 36.23 \\

%     &   & \textbf{3}  & 100.00   & 1.05  & 36.37 & 32.54 & 29.89 & 34.82 \\
%     &   & \textbf{6}  & 100.00   & 1.54  & 36.37 & 29.16 & 27.7  & 35.11 \\
%     &   & \textbf{10} & 100.00   & 1.05  & 36.37 & 28.75 & 27.59 & 33.69 \\
%     \cmidrule(r){2-9} 
%     & \multirow{4}{*}{\textbf{SERAC}}   & - & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
%     &   & \textbf{3}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
%     &   & \textbf{6}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
%     &   & \textbf{10} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
% \midrule
% \multirow{12}{*}{\textbf{\makecell{Visual Semantic\\ Editing}}} & \multirow{4}{*}{\textbf{FT-LLM}} & - & 76.89 & 15.88 & 49    & 49.44 & 49.04 & 10.67 \\
%     &   & \textbf{3}  & 50.33 & 7.45  & 42.86 & 46.73 & 45.02 & 8.29  \\
%     &   & \textbf{6}  & 49.09 & 7.25  & 41.49 & 45.58 & 43.52 & 7.25  \\
%     &   & \textbf{10} & 48.23 & 7.06  & 41.51 & 45.09 & 42.08 & 7.63  \\
%  \cmidrule(r){2-9} 
%     & \multirow{4}{*}{\textbf{FT-Alignment}} & - & 100.00   & 11.33 & 27.83 & 44.5  & 35.37 & 15    \\
%     &   & \textbf{3}  & 100.00   & 1.53  & 28    & 34.06 & 24.57 & 6.51  \\
%     &   & \textbf{6}  & 100.00   & 1.46  & 27.83 & 31.62 & 23.54 & 6.96  \\
%     &   & \textbf{10} & 100.00   & 1.45  & 27.83 & 29.79 & 23.92 & 7.25  \\
% \cmidrule(r){2-9} 
%     & \multirow{4}{*}{\textbf{SERAC}}   & - & 100.00   & 34.31 & 27.83 & 41.09 & 41.82 & 11.29 \\
%     &   & \textbf{3}  & 99.93 & 13.75 & 27.99 & 29.71 & 30.7  & 11.17 \\
%     &   & \textbf{6}  & 99.93 & 13.84 & 27.92 & 29.91 & 31.09 & 11.34 \\
%     &   & \textbf{10} & 99.93 & 13.98 & 27.88 & 29.93 & 31.13 & 11.23 \\
% \midrule
% \multirow{12}{*}{\textbf{\makecell{User-Specific \\ Editing}}}   & \multirow{4}{*}{\textbf{FT-LLM}} & \textbf{-}  & 75.68 & 20.13 & 57.82 & 48.04 & 48.66 & 12.63 \\
%     &   & \textbf{1}  & 69.12 & 17.22 & 52.06 & 44.36 & 44.14 & 8.67  \\
%     &   & \textbf{3}  & 66.6  & 16.49 & 49.79 & 41.87 & 41.85 & 6.16  \\
%     &   & \textbf{5}  & 66.7  & 17.41 & 49.43 & 40.78 & 40.29 & 5.88  \\
%      \cmidrule(r){2-9} 
%     & \multirow{4}{*}{\textbf{FT-Alignment}} & \textbf{-}  & 100.00   & 10.95 & 41.41 & 41.01 & 43.72 & \multicolumn{1}{l}{} \\
%     &   & \textbf{1}  & 100.00   & 15.26 & 41.39 & 30.15 & 30.02 & 7.66  \\
%     &   & \textbf{3}  & 100.00   & 16.12 & 41.39 & 30.81 & 29.52 & 8.67  \\
%     &   & \textbf{5}  & 100.00   & 18.3  & 41.39 & 29.77 & 28.09 & 7.37  \\
%      \cmidrule(r){2-9} 
%     & \multirow{4}{*}{\textbf{SERAC}}   & \textbf{-}  & 99.97 & 97.24 & 41.76 & 37.49 & 37.67 & 13.23 \\
%     &   & \textbf{1}  & 99.92 & 97.62 & 41.45 & 38.09 & 37.98 & 12.79 \\
%     &   & \textbf{3}  & 99.92 & 97.58 & 41.39 & 37.93 & 37.98 & 12.79 \\
%     &   & \textbf{5}  & 99.93 & 97.57 & 41.33 & 37.9  & 37.98 & 12.79 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}




% \begin{table}[tbp]
% \vspace{-3mm}
% \centering
% \renewcommand{\arraystretch}{1.1}
% % \vspace{-3mm}
% \caption{The results of sequential editing for BLIP2 on MMKE-Bench.}
% \label{res_seq_edit_blip}
% \resizebox{0.98\linewidth}{!}{
% \begin{tabular}{clccccccc}
% \toprule
% \multicolumn{1}{l}{\textbf{}}      & \textbf{Method}   & \textbf{Gap / User Num} & \textbf{T-Loc} & \textbf{I-Loc} & \textbf{T-Rel} & \textbf{I-Rel} & \textbf{I-Gen} & \textbf{Port} \\
% \midrule
% \multirow{12}{*}{\textbf{\makecell{Visual Entity \\ Editing}}}   & \multirow{4}{*}{\textbf{FT-LLM}} & \textbf{-} & 68.83  & 20.2   & 29.13  & 29.47  & 29.83  & 22.60  \\
%    &   & \textbf{3} & 32.42  & 5.33   & 28.12  & 24.14  & 24.54  & 21.61 \\
%    &   & \textbf{6} & 31.26  & 5.13   & 26.20   & 22.60   & 23.89  & 22.18 \\
%    &   & \textbf{10}& 31.59  & 5.03   & 25.03  & 22.41  & 22.65  & 20.97 \\
%    \cmidrule(r){2-9} 
%    & \multirow{4}{*}{\textbf{FT-Alignment}} & \textbf{-} & 100.00    & 8.74   & 19.67  & 23.53  & 22.47  & 17.36 \\
%    &   & \textbf{3} & 100.00    & 3.51   & 19.67  & 15.88  & 15.89  & 14.71 \\
%    &   & \textbf{6} & 100.00    & 3.52   & 19.67  & 16.84  & 16.86  & 15.32 \\
%    &   & \textbf{10}& 100.00    & 3.62   & 19.67  & 15.95  & 15.94  & 16.19 \\
%    \cmidrule(r){2-9} 
%    & \multirow{4}{*}{\textbf{SERAC}}   & \textbf{-} & 99.97  & 64.34  & 19.67  & 23.30   & 23.21  & 15.1  \\
%    &   & \textbf{3} & 99.97  & 55.92  & 19.67  & 19.47  & 19.6   & 14.54 \\
%    &   & \textbf{6} & 99.97  & 55.93  & 19.67  & 19.53  & 19.63  & 14.28 \\
%    &   & \textbf{10}& 99.97  & 55.91  & 19.67  & 19.71  & 19.74  & 14.43 \\
% \midrule
% \multirow{12}{*}{\textbf{\makecell{Visual Semantic \\ Editing}}} & \multirow{4}{*}{\textbf{FT-LLM}} & \textbf{-} & 64.75  & 20.13  & 32.08  & 31.40   & 31.90   & 2.88  \\
%    &   & \textbf{3} & 25.92  & 5.07   & 27.56  & 25.76  & 25.29  & 1.08  \\
%    &   & \textbf{6} & 25.42  & 4.98   & 25.21  & 24.53  & 23.31  & 0.96  \\
%    &   & \textbf{10}& 24.35  & 4.64   & 23.57  & 22.05  & 21.03  & 1.63  \\
%    \cmidrule(r){2-9} 
%    & \multirow{4}{*}{\textbf{FT-Alignment}} & \textbf{-} & 100.00    & 9.7    & 15.97  & 31.73  & 28.27  & 4.54  \\
%    &   & \textbf{3} & 100.00    & 4.15   & 15.97  & 11.42  & 11.42  & 4.15  \\
%    &   & \textbf{6} & 100.00    & 4.17   & 15.97  & 12.01  & 12.33  & 3.13  \\
%    &   & \textbf{10}& 100.00    & 4.09   & 15.97  & 10.46  & 10.46  & 4.09  \\
%    \cmidrule(r){2-9} 
%    & \multirow{4}{*}{\textbf{SERAC}}   & \textbf{-} & 100.00    & 77.42  & 16.22  & 17.77  & 19.77  & 3.79  \\
%    &   & \textbf{3} & 100.00    & 77.5   & 15.97  & 12.37  & 12.82  & 3.79  \\
%    &   & \textbf{6} & 100.00    & 77.47  & 15.97  & 12.58  & 13.00     & 3.79  \\
%    &   & \textbf{10}& 100.00    & 77.62  & 15.97  & 12.22  & 12.82  & 3.79  \\
% \midrule
% \multirow{12}{*}{\textbf{\makecell{User-Specific \\ Editing}}}   & \multirow{4}{*}{\textbf{FT-LLM}} & \textbf{-} & 63.18  & 21.19  & 13.10   & 27.00     & 27.14  & 4.83  \\
%    &   & \textbf{1} & 47.51  & 10.29  & 10.65  & 17.05  & 17.09  & 0.70   \\
%    &   & \textbf{3} & 46.51  & 10.51  & 10.10   & 14.32  & 13.90   & 0.54  \\
%    &   & \textbf{5} & 45.74  & 10.60   & 9.45   & 13.68  & 13.53  & 0.84  \\
%    \cmidrule(r){2-9} 
%    & \multirow{4}{*}{\textbf{FT-Alignment}} & \textbf{-} & 100.00    & 8.83   & 7.81   & 18.15  & 17.8   & 6.19  \\
%    &   & \textbf{1} & 100.00    & 16.14  & 8.31   & 6.79   & 6.59   & 0.75  \\
%    &   & \textbf{3} & 100.00    & 18.82  & 8.31   & 6.90   & 6.37   & 1.17  \\
%    &   & \textbf{5} & 100.00    & 18.26  & 8.31   & 7.93   & 8.08   & 2.23  \\
%    \cmidrule(r){2-9} 
%    & \multirow{4}{*}{\textbf{SERAC}}   & \textbf{-} & 99.97  & 93.4   & 7.81   & 15.18  & 15.53  & 4.91  \\
%    &   & \textbf{1} & 99.94  & 93.73  & 8.31   & 14.89  & 14.90   & 4.16  \\
%    &   & \textbf{3} & 99.92  & 93.71  & 8.31   & 14.89  & 14.90   & 4.16  \\
%    &   & \textbf{5} & 99.90   & 93.64  & 8.31   & 14.89  & 14.90   & 4.16 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}


\begin{table}[tbp]
\vspace{-3mm}
\centering
\renewcommand{\arraystretch}{1.1}
% \vspace{-3mm}
\caption{The results of sequential editing for BLIP2 on MMKE-Bench.}
\label{res_seq_edit_blip}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{clccccccc}
\toprule
\multicolumn{1}{l}{\textbf{}}      & \textbf{Method}   & \textbf{Gap / User Num} & \textbf{T-Loc} & \textbf{I-Loc} & \textbf{T-Rel} & \textbf{I-Rel} & \textbf{I-Gen} & \textbf{Port} \\
\midrule
\multirow{12}{*}{\textbf{\makecell{Visual Entity \\ Editing}}}   & \multirow{4}{*}{\textbf{FT-LLM}} & \textbf{-} & 70.91  & 21.63   & 37.3  & 36.56  & 36.84  & 18.70  \\
   &   & \textbf{3} & 33.91  & 5.24   & 34.18  & 30.65  & 31.18  & 14.64 \\
   &   & \textbf{6} & 34.56  & 5.17   & 32.33   & 28.55   & 28.67  & 12.84 \\
   &   & \textbf{10}& 33.85  & 5.10   & 31.24  & 28.08  & 27.68  & 13.18 \\
   \cmidrule(r){2-9} 
   & \multirow{4}{*}{\textbf{FT-Alignment}} & \textbf{-} & 100.00    & 9.04   & 20.09  & 28.9  & 28.39  & 17.05 \\
   &   & \textbf{3} & 100.00    & 2.01   & 20.09  & 13.62  & 13.47  & 13.62 \\
   &   & \textbf{6} & 100.00    & 2.04  & 20.09  & 12.54  & 12.56  & 13.48 \\
   &   & \textbf{10}& 100.00    & 1.99   & 20.09  & 14.37  & 14.44  & 13.85 \\
   \cmidrule(r){2-9} 
   & \multirow{4}{*}{\textbf{SERAC}}   & \textbf{-} & 99.99  & 99.68  & 20.90  & 20.30   & 20.48  & 19.81  \\
   &   & \textbf{3} & 99.99  & 99.69  & 20.09  & 20.60  & 20.82   & 17.93 \\
   &   & \textbf{6} & 99.99  & 99.69  & 20.01  & 20.34  & 20.65  & 17.66 \\
   &   & \textbf{10}& 99.99  & 99.68  & 20.09  & 20.56  & 20.68  & 17.92 \\
\midrule
\multirow{12}{*}{\textbf{\makecell{Visual Semantic \\ Editing}}} & \multirow{4}{*}{\textbf{FT-LLM}} & \textbf{-} & 64.01  & 19.53  & 34.67  & 31.74   & 32.04   & 3.38  \\
   &   & \textbf{3} & 27.52  & 5.09   & 28.92  & 27.21  & 25.96  & 2.75  \\
   &   & \textbf{6} & 26.28  & 5.05   & 28.35  & 25.61  & 24.32  & 1.54  \\
   &   & \textbf{10}& 25.95  & 4.55   & 24.74  & 23.58  & 22.75  & 2.13  \\
   \cmidrule(r){2-9} 
   & \multirow{4}{*}{\textbf{FT-Alignment}} & \textbf{-} & 100.00    & 9.59    & 18.34  & 35.86  & 35.84  & 5.92  \\
   &   & \textbf{3} & 100.00    & 1.69   & 18.34  & 12.42  & 12.09  & 2.75  \\
   &   & \textbf{6} & 100.00    & 1.67   & 18.34  & 12.18  & 13.18  & 3.46  \\
   &   & \textbf{10}& 100.00    & 1.64   & 18.34  & 11.49  & 11.57  & 3.04  \\
   \cmidrule(r){2-9} 
   & \multirow{4}{*}{\textbf{SERAC}}   & \textbf{-} & 100.00    & 99.97  & 28.97  & 30.39  & 30.23  & 19.04  \\
   &   & \textbf{3} & 99.92    & 98.91   & 18.34  & 17.37  & 17.17  & 4.25  \\
   &   & \textbf{6} & 99.92    & 98.90  & 18.34  & 17.44  & 17.17     & 4.33  \\
   &   & \textbf{10}& 99.92    & 98.91  & 18.34  & 17.19  & 17.17  & 4.17  \\
\midrule
\multirow{12}{*}{\textbf{\makecell{User-Specific \\ Editing}}}   & \multirow{4}{*}{\textbf{FT-LLM}} & \textbf{-} & 61.77 & 20.19  & 13.24   & 27.61     & 27.82  & 5.53  \\
   &   & \textbf{1} & 48.33  & 10.25  & 10.92  & 17.80  & 17.99  & 0.78   \\
   &   & \textbf{3} & 44.55  & 10.61  & 10.20   & 15.09  & 14.70   & 1.14  \\
   &   & \textbf{5} & 43.30  & 10.51   & 9.31   & 14.20  & 14.22  & 1.10  \\
   \cmidrule(r){2-9} 
   & \multirow{4}{*}{\textbf{FT-Alignment}} & \textbf{-} & 100.00    & 8.61   & 7.92   & 17.17  & 17.18   & 6.82  \\
   &   & \textbf{1} & 100.00    & 14.70  & 7.53   & 6.69   & 6.98   & 1.46  \\
   &   & \textbf{3} & 100.00    & 18.13  & 7.53   & 6.31   & 5.83   & 2.08  \\
   &   & \textbf{5} & 100.00    & 12.45  & 7.53   & 5.37   & 5.79   & 1.35  \\
   \cmidrule(r){2-9} 
   & \multirow{4}{*}{\textbf{SERAC}}   & \textbf{-} & 100.00  & 99.78   & 7.92  & 15.38  & 15.73  & 5.33  \\
   &   & \textbf{1} & 100.00  & 99.76  & 7.53   & 14.34  & 14.30   & 4.98  \\
   &   & \textbf{3} & 100.00  & 99.76  & 7.53   & 14.37  & 14.30   & 4.98  \\
   &   & \textbf{5} & 100.00   & 99.76  & 7.53   & 14.37  & 14.30   & 4.98 \\
\bottomrule
\end{tabular}
}
\end{table}





\clearpage
\begin{table}[tbp]
\vspace{-5mm} 
\centering
\renewcommand{\arraystretch}{1.0} 
\caption{The results of  Visual Semantic Sequential Editing for LLaVA-1.5 on MMKE-Bench.}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{clccccccc}
\toprule
\multicolumn{1}{l}{\textbf{}}& \textbf{Method}  & \textbf{GAP} & \textbf{T-Loc} & \textbf{I-Loc}   & \textbf{T-Rel} & \textbf{I-Rel} & \textbf{I-Gen} & \textbf{Port} \\
\midrule

\multirow{21}{*}{\textbf{ \makecell{Visual Semantic  \\ Editing}}} 
& \multirow{7}{*}{\textbf{FT-LLM}} & \textbf{-}& 76.89 & 16.14   & 49.00 & 49.44 & 49.04 & 10.67\\
   && \textbf{3}& 50.33 & 7.36& 42.86 & 46.73 & 45.02 & 8.29 \\
   && \textbf{6}& 49.09 & 7.25& 41.49 & 45.58 & 43.52 & 7.25 \\
   && \textbf{10}& 48.23 & 7.02& 41.51 & 45.09 & 42.08 & 7.63 \\
   && \textbf{{40}} & {45.40} & {6.23} & {36.83} & {41.85} & {40.53} & {7.83} \\
   && \textbf{{60}} & {43.88} & {5.82} & {36.01} & {39.18} & {38.69} & {7.04} \\
   && \textbf{{80}} & {42.99} & {5.58} & {33.67} & {38.27} & {36.79} & {6.83} \\
    \cmidrule(r){2-9} 
   & \multirow{7}{*}{\textbf{FT-Alignment}} & \textbf{-}& 100.00   & 19.41   & 27.83 & 44.5  & 35.37 & 15.00   \\
   && \textbf{3}& 100.00   & 1.44& 28& 34.06 & 24.57 & 6.51 \\
   && \textbf{6}& 100.00   & 1.38& 27.83 & 31.62 & 23.54 & 6.96 \\
   && \textbf{10}& 100.00   & 1.38& 27.83 & 29.79 & 23.92 & 7.25 \\
   && \textbf{{40}} & {100.00} & {1.22} & {27.83} & {25.4} & {21.63} & {8.58} \\
   && \textbf{{60}} & {100.00} & {1.17} & {27.83} & {26.12} & {22.11} & {8.08} \\
   && \textbf{{80}} & {100.00} & {0.94} & {27.83} & {27.31} & {23.81} & {6.75} \\
    \cmidrule(r){2-9} 
   & \multirow{7}{*}{\textbf{SERAC}}   & \textbf{-}& 100.00   & 34.53   & 27.83 & 41.09 & 41.82 & 11.29\\
   && \textbf{3}& 99.93 & 13.56   & 27.99 & 29.71 & 30.70  & 11.17\\
   && \textbf{6}& 99.93 & 13.54   & 27.92 & 29.91 & 31.09 & 11.34\\
   && \textbf{10}& 99.93 & 13.52   & 27.88 & 29.93 & 31.13 & 11.23\\
   && \textbf{{40}} & {99.93} & {13.37} & {27.92} & {28.23} & {29.23} & {11.25} \\
   && \textbf{{60}} & {99.93} & {13.35} & {27.92} & {28.45} & {29.41} & {11.25} \\
   && \textbf{{80}} & {99.96} & {13.32} & {27.92} & {28.20} & {28.41} & {11.25} \\
\midrule
\end{tabular}
}
\vspace{-5mm} 
\end{table}


\begin{figure}[htbp]
   \vspace{-3mm}
\centering
% \includegraphics[width=0.95\linewidth]{figs/prompt_edit.pdf}
\includegraphics[width=0.95\linewidth]{fig/prompt_edit.pdf}
\caption{Prompt for editing knowledge.}
\label{fig:prompt_edit}
\vspace{-3mm}
\end{figure}



\begin{figure}[htbp]
   \vspace{-3mm}
\centering
% \includegraphics[width=0.95\linewidth]{figs/prompt_rel.pdf}
\includegraphics[width=0.95\linewidth]{fig/prompt_rel.pdf}
\caption{Prompt for editing generating reliability question.}
\label{fig:prompt_rel}
\vspace{-3mm}
\end{figure}
















\begin{figure}[htbp]
   \vspace{-3mm}
\centering
% \includegraphics[width=0.95\linewidth]{figs/prompt_port.pdf}
\includegraphics[width=0.95\linewidth]{fig/prompt_port.pdf}
\caption{Prompt for generating portability question.}
\label{fig:prompt_port}
\vspace{-3mm}
\end{figure}

\begin{figure}[htbp]
   \vspace{-3mm}
\centering
% \includegraphics[width=0.8\linewidth]{figs/rebuttal/1.jpg}
\includegraphics[width=0.8\linewidth]{fig/1.pdf}
% \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
\caption{In Fig.11 (a), the single editing takes one edit at a time and evaluates immediately, while in Fig.11 (b) and (c) the sequential editing involves continuous edits and tests after several other edits.}
\label{fig:1}
\vspace{-3mm}
\end{figure}






% \begin{figure}[htbp]
%    \vspace{-3mm}
% \centering
% \includegraphics[width=0.95\linewidth]{figs/fig2_rebuttal.jpg}
% \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
% \caption {The construction pipeline of MMKE-Bench.}
% \label{fig:prompt_port}
% \vspace{-3mm}
% \end{figure}



\begin{figure}[htbp]
   \vspace{-3mm}
\centering
\includegraphics[width=0.95\linewidth]{fig/ES.pdf}
% \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
\caption {There is a difference between Visual Entity Knowledge and Visual Semantic Knowledge. Visual Entity Knowledge focuses on entity objects, such as people, things, etc. Visual Semantic Knowledge focuses on the knowledge abstracted from images,  such as gestures, traffic signs, facial expressions, etc. For example, for Visual Entity Knowledge, in Figure 12 (a), the training knowledge needs a reference to the entity, such as "Donald John Trump", focusing on the information of the entity object; However, in (b) of Figure 12, for Visual Semantic Knowledge, entity reference, such as "The man", is not needed, but the gesture of the person in the image is emphasized.}
\label{fig:ES}
\vspace{-3mm}
\end{figure}


\begin{figure}[htbp]
    \vspace{-3mm}
    \centering
    % \includegraphics[width=0.95\linewidth]{figs/rebuttal/SERAC_loss_plot.png}
    \includegraphics[width=0.95\linewidth]{fig/SERAC_loss_plot.pdf}
    % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
    \caption[Loss Iteration Graph]{
        Loss iteration graph trained by SERAC method on Visual Semantic Knowledge data. Through the analysis of images, we can find that the SERAC method can normally achieve the convergence of loss on this data amount, and the loss value will approach 0 at last.
    }
    \label{fig:SERAC_loss_plot}
    \vspace{-3mm}
\end{figure}


\begin{figure}[htbp]
    \vspace{-3mm}
    \centering
    % \includegraphics[width=0.95\linewidth]{figs/rebuttal/MEND_loss_plot.png}
    \includegraphics[width=0.95\linewidth]{fig/MEND_loss_plot.pdf}
    % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
    \caption[Loss Iteration Graph]{
        Loss iteration graph trained by MEND method on Visual Semantic Knowledge data. Through the analysis of images, we can find that the MEND method can normally achieve the convergence of loss on this data amount, and the loss value will approach 0 at last.
    }
    \label{fig:MEND_loss_plot}
    \vspace{-3mm}
\end{figure}

 



% Data Example of Visual Entity Editing
\clearpage 
\begin{figure}[htbp]
   \vspace{-3mm}
   \centering
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data1.jpg}
        \includegraphics[width=\linewidth]{fig/data1.pdf}
       % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
       \caption{Data Example-1 of Visual Entity Editing in MMKE-Bench.}
       \label{fig:fig1}
   \end{minipage}
   \hspace{0.05\linewidth}
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data2.jpg}
       \includegraphics[width=\linewidth]{fig/data2.pdf}
       % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
       \caption{Data Example-2 of Visual Entity Editing in MMKE-Bench.} 
       \label{fig:fig2}
   \end{minipage}
   \vspace{5mm} 
\end{figure}

% \begin{figure}[htbp]
%    \vspace{-3mm}
%    \centering
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data3.jpg}
%        \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
%        \caption{Data Example-3 of Visual Entity Editing in MMKE-Bench.}
%        \label{fig:fig3}
%    \end{minipage}
%    \hspace{0.05\linewidth} 
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data4.jpg}
%        \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
%        \caption{Data Example-4 of Visual Entity Editing in MMKE-Bench.} 
%        \label{fig:fig4}
%    \end{minipage}
%    \vspace{5mm} 
% \end{figure}


%Data Example of Visual Semantic Editing
\begin{figure}[htbp]
   \vspace{-3mm}
   \centering
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data5.jpg}
        \includegraphics[width=\linewidth]{fig/data5.pdf}
       % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
       \caption{Data Example-1 of Visual Semantic Editing in MMKE-Bench.}
       \label{fig:fig5}
   \end{minipage}
   \hspace{0.05\linewidth}
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data6.jpg}
        \includegraphics[width=\linewidth]{fig/data6.pdf}
       % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
       \caption{Data Example-2 of Visual Semantic Editing in MMKE-Bench.} 
       \label{fig:fig6}
   \end{minipage}
   \vspace{5mm} 
\end{figure}

% \begin{figure}[htbp]
%    \vspace{-3mm}
%    \centering
%    \begin{minipage}[c]{0.46\linewidth}
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data7.jpg}
%        \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
%        \caption{Data Example-3 of Visual Semantic Editing in MMKE-Bench.}
%        \label{fig:fig7}
%    \end{minipage}
%    \hspace{0.05\linewidth} 
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data8.jpg}
%        \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
%        \caption{Data Example-4 of Visual Semantic Editing in MMKE-Bench.} 
%        \label{fig:fig8}
%    \end{minipage}
%    \vspace{5mm} 
% \end{figure}



%Data Example of User-Specific Editing
\begin{figure}[htbp]
   \vspace{-3mm}
   \centering
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data9.jpg}
        \includegraphics[width=\linewidth]{fig/data9.pdf}
       % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
       \caption{Data Example-1 of User-Specific Editing in MMKE-Bench.}
       \label{fig:data9}
   \end{minipage}
   \hspace{0.05\linewidth}
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data10.jpg}
        \includegraphics[width=\linewidth]{fig/data10.pdf}
       % \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
       \caption{Data Example-2 of User-Specific Editing in MMKE-Bench.} 
       \label{fig:data10}
   \end{minipage}
   \vspace{5mm} 
\end{figure}

% \begin{figure}[htbp]
%    \vspace{-3mm}
%    \centering
%    \begin{minipage}[c]{0.46\linewidth}
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data11.jpg}
%        \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
%        \caption{Data Example-3 of User-Specific Editing in MMKE-Bench.}
%        \label{fig:fig7}
%    \end{minipage}
%    \hspace{0.05\linewidth} 
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/data_example/data12.jpg}
%        \captionsetup{labelfont={color=blue}, textfont={color=blue}} 
%        \caption{Data Example-4 of User-Specific Editing in MMKE-Bench.} 
%        \label{fig:fig8}
%    \end{minipage}
%    \vspace{5mm} 
% \end{figure}




% Qualitative Example of Visual Entity Editing
\begin{figure}[htbp]
   \vspace{-3mm}
   \centering
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr2.jpg}
        \includegraphics[width=\linewidth]{fig/cr2.pdf}
       % \captionsetup{labelfont={color=red}, textfont={color=red}} 
       \caption{Case Study on Visual Entity Editing Example-1 in MMKE-Bench.}
       \label{fig:cr2}
   \end{minipage}
   \hspace{0.05\linewidth} 
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr4.jpg}
        \includegraphics[width=\linewidth]{fig/cr4.pdf}
       % \captionsetup{labelfont={color=red}, textfont={color=red}} 
       \caption{Case Study on Visual Entity Editing Example-2 in MMKE-Bench.} 
       \label{fig:cr4}
   \end{minipage}
   \vspace{5mm} 
\end{figure}

% \begin{figure}[htbp]
%    \vspace{-3mm}
%    \centering
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr3.jpg}
%        \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study on Visual Entity Editing Example-3 in MMKE-Bench.}
%        \label{fig:fig3}
%    \end{minipage}
%    \hspace{0.05\linewidth} 
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr4.jpg}
%        \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study on Visual Entity Editing Example-4 in MMKE-Bench.} 

%        \label{fig:fig4}
%    \end{minipage}
%    \vspace{5mm} 
% \end{figure}


% Qualitative Example of Visual Semantic Editing
% \begin{figure}[htbp]
%    \vspace{-3mm}
%    \centering
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr5.jpg}
%        \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study on Visual Semantic Editing Example-1 in MMKE-Bench.}
%        \label{fig:fig1}
%    \end{minipage}
%    \hspace{0.05\linewidth}
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr6.jpg}
%        \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study on Visual Semantic Editing Example-2 in MMKE-Bench.} 
%        \label{fig:fig2}
%    \end{minipage}
%    \vspace{5mm} 
% \end{figure}

\begin{figure}[htbp]
   \vspace{-3mm}
   \centering
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       \includegraphics[width=\linewidth]{fig/cr7.pdf}
        % \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr7.jpg}
       % \captionsetup{labelfont={color=red}, textfont={color=red}} 
       \caption{Case Study on Visual Semantic Editing Example-1 in MMKE-Bench.}
       \label{fig:cr7}
   \end{minipage}
   \hspace{0.05\linewidth} 
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr8.jpg}
        \includegraphics[width=\linewidth]{fig/cr8.pdf}
       % \captionsetup{labelfont={color=red}, textfont={color=red}} 
       \caption{Case Study on Visual Semantic Editing Example-2 in MMKE-Bench.} 
       \label{fig:cr8}
   \end{minipage}
   \vspace{5mm} 
\end{figure}


% Qualitative Example of User-Specific Editing
% \begin{figure}[htbp]
%    \vspace{-3mm}
%    \centering
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr9.jpg}
%        \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study on User-Specific Editing Example-1 in MMKE-Bench.}
%        \label{fig:fig1}
%    \end{minipage}
%    \hspace{0.05\linewidth}
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr10.jpg}
%        \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study on User-Specific Editing Example-2 in MMKE-Bench.} 
%        \label{fig:fig2}
%    \end{minipage}
%    \vspace{5mm} 
% \end{figure}

\begin{figure}[htbp]
   \vspace{-3mm}
   \centering
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr11.jpg}
        \includegraphics[width=\linewidth]{fig/cr11.pdf}
       % \captionsetup{labelfont={color=red}, textfont={color=red}} 
       \caption{Case Study on User-Specific Editing Example-1 in MMKE-Bench.}
       \label{fig:cr11}
   \end{minipage}
   \hspace{0.05\linewidth} 
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/qualitative_example/cr12.jpg}
        \includegraphics[width=\linewidth]{fig/cr12.pdf}
       % \captionsetup{labelfont={color=red}, textfont={color=red}} 
       \caption{Case Study on User-Specific Editing Example-2 in MMKE-Bench.} 
       \label{fig:cr12}
   \end{minipage}
   \vspace{5mm} 
\end{figure}




% task generalization Example of Visual Entity Editing
% \begin{figure}[htbp]
%    \vspace{-3mm}
%    \centering
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/task/r3.jpg}
%        % \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study of Reliability Example-1 of Visual Entity Editing in MMKE-Bench. The texts in brown indicate the same content as the editing knowledge.}
%        \label{fig:fig3}
%    \end{minipage}
%    \hspace{0.05\linewidth} r
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/task/r4.jpg}
%        % \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study of Reliability Example-2 of Visual Entity Editing in MMKE-Bench. The texts in brown indicate the same content as the editing knowledge.} 
%        \label{fig:fig4}
%    \end{minipage}
%    \vspace{5mm} 
% \end{figure}


% task generalization Example of Visual Semantic Editing
\begin{figure}[htbp]
   \vspace{-3mm}
   \centering
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/task/r1.jpg}
        \includegraphics[width=\linewidth]{fig/r1.pdf}
       % \captionsetup{labelfont={color=red}, textfont={color=red}} 
       \caption{Case Study of Question Answer Example-1 of Visual Semantic Editing in MMKE-Bench. The texts in brown indicate the same content as the editing knowledge.}
       \label{fig:r1}
   \end{minipage}
   \hspace{0.05\linewidth} 
   \begin{minipage}[c]{0.46\linewidth} 
       \centering
       % \includegraphics[width=\linewidth]{figs/rebuttal/task/r2.jpg}
        \includegraphics[width=\linewidth]{fig/r2.pdf}
       % \captionsetup{labelfont={color=red}, textfont={color=red}} 
       \caption{Case Study of Question Answer Example-2 of Visual Semantic Editing in MMKE-Bench. The texts in brown indicate the same content as the editing knowledge.} 
       \label{fig:r2}
   \end{minipage}
   \vspace{5mm} 
\end{figure}


% task generalization Example of User-Specific Editing
% \begin{figure}[htbp]
%    \vspace{-3mm}
%    \centering
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/task/r6.jpg}
%        % \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study of Reliability Example-1 of User-Specific Editing in MMKE-Bench. The texts in brown indicate the same content as the editing knowledge.}
%        \label{fig:fig3}
%    \end{minipage}
%    \hspace{0.05\linewidth} 
%    \begin{minipage}[c]{0.46\linewidth} 
%        \centering
%        \includegraphics[width=\linewidth]{figs/rebuttal/task/r5.jpg}
%        % \captionsetup{labelfont={color=red}, textfont={color=red}} 
%        \caption{Case Study of Reliability Example-2 of User-Specific Editing in MMKE-Bench. The texts in brown indicate the same content as the editing knowledge.} 
%        \label{fig:fig4}
%    \end{minipage}
%    \vspace{5mm} 
% \end{figure}