
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}




\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{longtable}

\usepackage{bbm}
\usepackage{bbding}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{tcolorbox}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{pifont}
\definecolor{color1}{RGB}{200,230,240}
\definecolor{color2}{RGB}{235,245,255}
\definecolor{lavender}{RGB}{220,220,250}


\usepackage{booktabs}

\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{array}  % 导入 array 宏包
% \usepackage{wraptable}

% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}



% \title{MMKE-Bench: A Comprehensive Multimodal Knowledge Editing Benchmark}
\title{MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Knowledge editing techniques have emerged as essential tools for updating the factual knowledge of large language models (LLMs) and multimodal models (LMMs), allowing them to correct outdated or inaccurate information without retraining from scratch. However, existing benchmarks for multimodal knowledge editing primarily focus on entity-level knowledge represented as simple triplets, which fail to capture the complexity of real-world multimodal information. To address this issue, we introduce MMKE-Bench, a comprehensive \textbf{M}ulti\textbf{M}odal \textbf{K}nowledge \textbf{E}diting Benchmark, designed to evaluate the ability of LMMs to edit diverse visual knowledge in real-world scenarios. MMKE-Bench addresses these limitations by incorporating three types of editing tasks: visual entity editing, visual semantic editing, and user-specific editing.  Besides, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering a more flexible and effective format.
The benchmark consists of 2,940 pieces of knowledge and 7,229 images across 110 fine-grained types, with evaluation questions automatically generated and human-verified. We assess five state-of-the-art knowledge editing methods on three prominent LMMs, revealing that no method excels across all criteria, and that visual and user-specific edits are particularly challenging. MMKE-Bench sets a new standard for evaluating the robustness of multimodal knowledge editing techniques, driving progress in this rapidly evolving field.
\end{abstract}




% Knowledge editing has become a powerful and cost-effective method for updating or correcting factual information in large language models. performing knowledge editing in large multimodal models is significantly more challenging due to the complexity of integrating diverse modalities and the interaction between them. While several evaluation benchmarks have been developed, they mainly focus on entity recognition editing using triplet-based knowledge representation. This format, however, falls short in real-world applications, as it conveys limited information. Moreover, the rich visual data inherent in multimodal models is often neglected in these benchmarks, despite its importance for advanced multimodal understanding. To address these gaps, we introduce a comprehensive \textbf{M}ulti\textbf{M}odal \textbf{K}nowledge \textbf{E}diting benchmark (MMKE-Bench) that evaluates diverse semantic editing tasks in real-world contexts. Unlike the triplet-based approach, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering a more flexible and effective format. Additionally, the benchmark includes three types of editing: visual entity editing, visual semantic editing, and user-specific editing, to better reflect the complexity of multimodal editing and personalized multimodal needs.
% MMKE-Bench consists of 2,940 knowledge pieces and 7,229 images from 110 different types, offering a more diverse and challenging dataset compared to previous benchmarks. We conducted extensive experiments on three leading multimodal models using five prominent knowledge editing methods. The results highlight the urgent need for innovative solutions in this field, and we hope our work will provide valuable insights for the multimodal research community.




% However, the triplet form does not align with real-world scenarios, as it conveys limited information. Furthermore, the visual modality of large multimodal models contains rich visual information, editing such information is important for high-level multimodal understanding,

 % which is overlooked by existing benchmarks.

%   To address these issues, we propose a comprehensive \textbf{M}ulti\textbf{M}odal \textbf{K}nowledge \textbf{E}diting benchmark (MMKE-Bench) 
% to evaluate diverse semantic editing for real-world scenarios. On the one hand, Different from the triplet-based format, free-form natural language is adopted to represent and edit pieces of knowledge, providing a more convenient and effective representation format. On the other hand, MMKE-Bench is constructed with three types of editing, namely visual entity editing, visual semantic editing, and user-specific editing, to evaluate complex text and visual semantic editing and meet personlized multimodal modal requirement.
% With a proposed pipeline, MMKE-Bench comprises 2,940 pieces of knowledge, along with 7,229 images from 110 types, which is more diverse and difficult than previous benchmarks. We conduct extensive experiments on three representative large multimodal models using five representative knowledge editing methods. Our findings highlight an urgent need for novel approaches in this field, and we hope that our work will offer useful insights to the multimodal community.

% However, performing knowledge editing in large multimodal models is significantly more challenging due to the complexity of integrating diverse modalities and the interaction between them. 





% To address these gaps, we introduce a comprehensive \textbf{M}ulti\textbf{M}odal \textbf{K}nowledge \textbf{E}diting benchmark (MMKE-Bench) that evaluates diverse semantic editing tasks in real-world contexts. Unlike the triplet-based approach, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering a more flexible and effective format. Additionally, the benchmark includes three types of editing: visual entity editing, visual semantic editing, and user-specific editing, to better reflect the complexity of multimodal editing and personalized multimodal needs.







% with free-form natural language representation. 
% Secondly, we focus on complex visual semantic editing covering various object behaviors and relationships. Thirdly, to meet the realistic requirements for personalized models, we assess the performance of injecting user-specific knowledge into the models. 


\input{sections/introduction}
\input{sections/related_work}
% \newpage
\input{sections/problem}
% \newpage
\input{sections/benchmark}
% \newpage
\input{sections/experiment}
% \newpage
\input{sections/conclusion}


% \newpage
\bibliography{reference_header,iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\input{sections/appendix}



\end{document}
