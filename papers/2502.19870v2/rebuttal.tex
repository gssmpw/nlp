
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}




\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{longtable}

\usepackage{bbm}
\usepackage{bbding}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{tcolorbox}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{pifont}
\definecolor{color1}{RGB}{200,230,240}
\definecolor{color2}{RGB}{235,245,255}
\definecolor{lavender}{RGB}{220,220,250}


\usepackage{booktabs}

\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{array}  % 导入 array 宏包
% \usepackage{wraptable}





% \title{MMKE-Bench: A Comprehensive Multimodal Knowledge Editing Benchmark}
\title{MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle



\newpage

\section{Revision Summary}

We thank all reviewers for their constructive thoughtful feedback. We are deeply encouraged by these positive comments by realistic and diverse benchmarks, an insightful data collection pipeline, extensive experimental evaluation, and systematic experimental analysis. And we appreciate all reviewers for their recognition of our work.

Additionally, we have addressed each reviewer's questions individually. Specifically, we have provided detailed descriptions of computational resources and extensibility, documentation and checkpoints (available in our Github repository), and the insights into the PPT metric.

Regarding the experiments, we have included statistics on the FLOPs metric, which further enhances our V-PETL benchmark.

Thank you again for your time and effort in reviewing our paper!



\newpage

\section{Response to Reviewer 1STo}

\textbf{W1:} I’m not sure if the workload is sufficient. 

A: Firstly, there is a separate Primary Area, namely datasets and benchmarks, in ICLR 2025, which we have selected for our submission. Therefore, we believe our work is appropriate for consideration at ICLR 2025. Secondly, beyond the introduced benchmark, we also propose a novel pipeline for benchmark construction, which we believe could inspire further. Besides, our results highlight the limitations of existing knowledge editing methods and show a need for more advanced multi-modal knowledge editing methods. Thirdly, we discuss the experiment results as follows.  A major difference between our benchmark and existing ones lies in that the training and evaluation questions are the same as the evaluation question (reliability) as there are multiple editing facts in one sample, However, in our benchmark, the training and evaluation questions differ. Thus, the editing method on our benchmark is more challenging. Furthermore, the evaluation task in our benchmark emphasizes semantic understanding, which is inherently more difficult than the entity recognition focus of the previous benchmarks.


\textbf{W2:} I didn’t find evidence in the experimental or case study sections demonstrating the necessity of this benchmark.

A: The case studies in experiments are primarily used to demonstrate the effectiveness of knowledge editing methods. To better illustrate the ‘differences’ and ‘necessity’, we have added more examples in the appendix (Figure 15-26, lines 1404-1560). Based on these samples, the ‘differences’ and ‘necessity’ can be summarized as follows: \textbf{1) Necessity in real-world scenarios:} Taking daily hand gestures as an example, the same gesture may mean friendly in one country, but it may mean unfriendly in another country. Thus, it is necessary to edit knowledge about the meaning of certain hand gestures to avoid cultural conflicts. Similarly, when we enter an unfamiliar country or region, people there may frequently use hand gestures that we do not recognize. In such cases, recognizing and then using these gestures could help us better communicate with them. Both scenarios need to edit the models about the recognition of hand gestures, which aligns with the motivation of visual semantic editing.  2) \textbf{Difference from other benchmarks:} Previous multi-modal benchmarks mainly focus on entity-level knowledge editing with a triple knowledge presentation format. In contrast, our benchmark focuses on complex visual semantic scenarios with natural language-based knowledge representation. To sum up, the key differences from other benchmarks are 1) the emphasis on complex visual semantic editing, and 2) the use of a flexible natural language-based knowledge representation format.

\textbf{Q1:} In line 776, “counterfactual editing” is mentioned. According to the description in lines 776-781, modifying certain facts constitutes counterfactual editing. Is this definition accurate?

A: Your understanding is correct. The strategy of counterfactual editing or counterfactual editing description is widely employed in the generation of knowledge editing datasets for LLM-based or LMM-based benchmarks[1-3]. In this approach, factual knowledge is modified into counterfactual content to create editing datasets. In our benchmark, we follow this strategy and edit certain facts into counterfactual content as counterfactual editing.

\textbf{Q2:}  The case study in Section 5.3 could include more analysis. Figure 5 and Figure 6 only present a relatively simple entity editing example (person-related information) and do not showcase complex cases of visual semantic editing or user-specific knowledge editing. It’s recommended to select representative and challenging cases or specific types of cases in visual semantic editing or user-specific knowledge editing.

A: Firstly, there seems to be some misunderstanding. While Figure 6 is person-related, its purpose is to illustrate a case of visual semantic editing. Specifically, we focus on understanding the meaning of the gesture performed by the basketball referee (visual semantic understanding) rather than identifying who the person is (entity recognition).  Secondly, following your advice, we have added additional analysis examples (Figure 27-38, line 1579-1766)  and task generalization examples (Figure 39-44, line 1760-1830)  in the appendix to provide better visualization in the revised version.


\textbf{Q3:} It is suggested to provide a detailed analysis of how IKE's performance on this new benchmark differs from its performance on previous benchmarks, and what these differences reveal about the nature of multimodal knowledge editing tasks.

A: We compare IKE's performance with existing benchmarks, such as VLKEB, as follows:  1) The results of editing methods on our benchmark align with those of existing benchmarks, where IKE achieves the best or the second-best results in terms of reliability and generality. However, IKE achieves much improvement over FT-LLM in terms of portability for VLKEB on all models (BLIP2, MiniGPT4, and LLaVA). In our benchmark, the results of IKE have limited improvement or are even worse than FT-LLM on BLIP2 and MiniGPT4, but it achieves more improvement on LLaVA 1.5. We attribute this to the fact that longer natural language descriptions are more difficult to understand for base models, and modern models, i.e., LLaVA 1.5, could understand better due to a better training strategy, leading to better performance. 2) The results also show that IKE could achieve better results in complex visual semantic and user-specific editing scenarios in terms of portability, which shows that semantic editing and user-specific editing are more difficult to transfer. 3) In-context examples in IKE are important for models to understand the editing task. Since we adopt a natural language format, which is longer than the triple-based knowledge format, the number of in-context examples in our benchmark is smaller than that of other benchmarks when the maximum input length is fixed for the base model, which is another difference from existing benchmarks.    



\textbf{Q4:} Could you provide some examples or data to demonstrate that your benchmark is indeed more challenging and more valuable for improving models compared to other benchmarks?

 A: We have discussed the differences in the section introduction and the section experiment. To better illustrate the comparison, we have added more data examples from our benchmark in the appendix (Figure 15-26, lines 1404-1560) of the revision, and made more discussion in W2.

[1] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and
Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. Findings of
EMNLP, 2023.

[2] Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu
Zhang. Can we edit multimodal large language models? In EMNLP, pp. 13877–13888, 2023.

[3] Han Huang, Haitian Zhong, Tao Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. Vlkeb: A
large vision-language model knowledge editing benchmark. NeurIPS, 2024.



































\newpage
\section{Response to Reviewer 5HUG}

\textbf{W1: Figures 1 and 2 Could Benefit from Improved Clarity and Accessibility} 

A: We have improved Figure 1 and Figure 2 in the revised version to help readers better understand our work. For Figure 1, we have replaced the example of a soccer referee gesture with that of a common life gesture, which is more familiar to most people. For Figure 2, we replaced the original figure with an updated version, enhanced by adding four bounding boxes and serial numbers, as well as correcting typos to clarify the data generation steps.

\textbf{W2: Limitations of Counterfactual Data for Real-World Applications} 

A: We agree that counterfactual data may have a different distribution from real-world data. While real-world data is difficult to collect, particularly when it comes to capturing genuine changes in visual semantic editing. Therefore, we adopt the widely used strategy in LLM and LMMs and construct the benchmark with counterfactual data generation [1-3]. Regarding the experiments on real-world data, to the best of our knowledge, there is currently no dataset or benchmark based on real-world data built on real-world data for multimodal knowledge editing. This makes it infeasible to conduct such experiments at this time. Nevertheless, we believe that collecting real-world data for multimodal knowledge editing is a valuable research direction, and we are considering exploring this in the future.

% In constructing the evaluation benchmark, both the real-world data and counterfactual data are considered. Both strategies have strengths and shortcomings. Real-world data follows the same distribution as our real life, but the collected data at the current time may be used as the training data of the next modern large multimodal model, in such cases, the collected data are unsuitable as knowledge editing data as they have been in model training. Counterfactual data would be used continuously regardless of future time, while the shortcoming is that it may follow a different data distribution from the real-world data. 

% As shown in previous LLM-based or LMM-based knowledge editing[1-3], most work adopts the second strategy, counterfactual data, 

\textbf{W3: Lack of Empirical Analysis on the Impact of Human Verification} 

A: Human verification is incorporated into several steps of the benchmark construction process, including candidate item generation, image filtering, check of counterfactual editing, remove or rewrite evaluation question generation, and overall check. Among these steps,  image filtering needs many human efforts as the image downloaded from Google often includes noisy or irrelevant ones, and some images need to be manually extracted from videos. After a human filter, the quality of images could be improved. As for checking counterfactual editing, we found that the majority of the counterfactual content generation is good, with a successful rate of nearly 100\%. As a result, no human verification was needed for this step. For evaluation question generation, we utilized LLMs to generate questions and then conducted human verification, we observed that the quality was lower than that of counterfactual editing. To show a quantitative comparison, we select 100 questions generated by LLM, among them, 81 questions and answers are good, achieving a successful rate of 81. After human verification and rewriting, noisy questions were either corrected or removed.


\textbf{Q1: Would it be more beneficial to construct knowledge editing samples using incremental, real-world updates? }

A: We agree that constructing real-world editing data for evaluation is a valuable direction. We are considering exploring this as part of our ongoing work.

\textbf{Q2: What impact did human verification have on the LLM-generated descriptions?}

A: We have explained the impact of human verification in W3, where analysis and quantitative comparisons are provided. We believe this step significantly improves the quality of our benchmark.

\textbf{Q3: Is there a comparison of Visual Entity Editing data quality between MMKE-Bench and prior datasets (e.g., MC-MKE, VLKEB)?} 

A: As explained earlier, the quality of our benchmark is ensured by automated generation steps complemented by extensive human verification, especially on visual entity editing data. As for comparison, the data collection and question generation steps are similar to the previous benchmark. As for entity and image collection, we first collect entity items from existing datasets, get the images of entities from Google, and then remove noisy images by human verification. In contrast, MC-MKE requires additional recognition of the subject in one sentence. The quality of data collection is mainly improved by human verification, where substantial human effort is invested to ensure image quality. For the knowledge representation, MC-MKE and VLKEB adopt triplet knowledge, while our benchmark adopts natural language-based knowledge representation. Descriptions are sourced from Wikipedia to ensure the quality of original descriptive information, followed by counterfactual editing performed by LLMs. We believe that both previous benchmarks and our benchmark achieve a high level of quality.

[1] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and
Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. Findings of
EMNLP, 2023.

[2] Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu
Zhang. Can we edit multimodal large language models? In EMNLP, pp. 13877–13888, 2023.

[3] Han Huang, Haitian Zhong, Tao Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. Vlkeb: A
large vision-language model knowledge editing benchmark. NeurIPS, 2024.

































\newpage
\section{Response to Reviewer Bskj}

\textbf{W1\&Q1:} There is some overlap between visual entity editing and visual semantic editing, as both tasks involve understanding image content, which could blur the distinction between these editing types. 
% {[\color{red} need figure number, need page number, explanation in rebuttal.]}

A: We have included a new figure in the appendix (Figure 12, Line 1322) of revision to show the differences between editing visual entity and semantic editing. In summary, visual entity editing focuses on entity recognition, namely who/what the entity is, and semantic editing focuses on semantic behavior understanding, such as gesture, emotion, and action, without relying on who/what the entity is.


\textbf{W2\&Q1:} Additionally, the user-specific editing scenario may lack practicality. In real-world applications, database or memory-based search might be more effective than training user-specific information for each user to achieve personalization in LLMs or LMMs. 

A: We agree that adopting an external memory or database is another viable approach to achieve user-specific applications. However, we would like to emphasize that the need for a user-specific scenario remains realistic and essential for users, regardless of the specific method employed. Moreover, as the amount of user-specified data grows, the required storage space would increase, and the retrieval performance from memory would likely degrade, which in turn could negatively impact performance in real-world applications. Therefore, we believe that both adopting additional memory and injecting user-specific knowledge directly into the model can be viable solutions for achieving a personalized model in real-world scenarios, with each approach having its respective strengths and weaknesses. Given this, we propose exploring knowledge editing as an alternative method for personalization. Lastly, it is worth noting that some knowledge editing methods, such as SERAC, also incorporate memory-based strategies, meaning that memory-based approaches are already partially considered in the benchmark evaluations.



\textbf{W3\&Q1:} Regarding the T-Loc test, there’s room for improvement. The results are near 100, suggesting that the randomly sampled T-Loc questions are relatively easy for methods such as SERAC, FT-Alignment, and MEND. Introducing more challenging cases could enhance the evaluation’s robustness. For instance, collect similar but harder test cases, either through web crawling or using LLM-generated content (e.g., from GPT) and verify them. This could improve the test’s effectiveness.

A: Regarding T-LOC, methods such as FT-Alignment and SERAC achieve nearly 100\%, which is attributed to their editing mechanisms. Specifically, FT-Alignment only updates the projection model between the vision and language models, while SERAC updates only the scope classifier and counterfactual module, without modifying the LLM itself. As a result, for a given text-only question, the outputs generated by the LLM before and after editing are nearly identical, leading to a T-LOC close to 100\%.To better understand the impact of questions on T-LOC, we generated harder T-LOC questions using the LLM for visual semantic editing. We provided the original knowledge (e.g., "This is a fist in a life gesture. The thumb and pinky finger are extended while the other fingers are clenched into a fist. It signifies strength or readiness to strike.") and item type (e.g., "life gesture") to the LLM and instructed it to generate type-related questions (e.g., "What does it mean when someone waves their hand up and down enthusiastically?"). Experiments were conducted on visual semantic editing data, and the results are shown in the table. As the results indicate, most editing methods perform worse under harder T-LOC questions, while FT-Alignment and SERAC remain nearly unaffected. This demonstrates that both methods can achieve consistently high T-LOC performance across different types of questions.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[h]
\centering
\begin{tabular}{clccl}
\toprule
\textbf{Model}                      & \textbf{Method} & \textbf{T-Loc (new)} & \textbf{T-Loc (origin)} &  \\
\midrule
\multirow{6}{*}{\textbf{LLaVA-1.5}} & FT-LLM         & 59.21     & 79.62              &  \\
                                    & FT-Alignment    & 100.00    & 100.00             &  \\
                                    & IKE             & 50.74     & 61.10               &  \\
                                    & SERAC           & 99.97     & 99.99              &  \\
                                    & MEND            & 95.65     & 98.15              &  \\
                                    & KE              & 63.53     & 71.39              &  \\
\bottomrule
\end{tabular}
\end{table}


\textbf{Q2:}  Given the modest amount of training data, is the dataset volume adequate for methods that require training, like serac or mend? 2) Additionally, how is the IKE method adapted to LMM editing? Which examples are used in context? Is target knowledge incorporated within the context, and do you include image-based examples in this context?

A: 1) On the one hand, we aim to show that our benchmark is of a similar order of magnitude as existing benchmarks, such as VLKEB and MIKE. On the other hand, to demonstrate the impact of training data, we plot the training loss of SERAC and MEND in the appendix (Figure 13–14, Lines 1355–1399) of the revision. As shown, during training, the loss decreases continuously and eventually converges to a small value, indicating that the training data is sufficient for training the model. 2) Regarding the IKE method, we follow its original implementation as described in MMedit and VLKEB. The most similar examples, calculated using cosine similarity, are adopted as in-context examples, and the target knowledge is incorporated into the context. For images, we follow MMedit and VLKEB, excluding images from the context.

\textbf{MMedit}:  \url{https://github.com/zjunlp/EasyEdit}

\textbf{VLKEB}: \url{https://github.com/VLKEB/VLKEB/tree/main}

\textbf{Q3:}  This raises two questions: 1) First, is LLaVA-1.5 indeed larger than MiniGPT4, as both use a 7B LLM, and MiniGPT4's vision encoder appears larger? 2) Second, this statement is not directly related to the benchmark’s core focus, which is to compare editing methods rather than models.

A: 1) We apologize for the confusing statement. The LLaVA 1.5 model is of a similar size to MiniGPT4, and its better performance compared to MiniGPT4 may be attributed to a better visual model design and instruction tuning strategy. Regarding Blip2, its superior performance may stem from a larger model size and better training strategy. We apologize again for the confusion caused by combining these points in one sentence, and we have now explained each separately in the revision.  2) Your understanding is correct. This analysis is a comparison of editing methods. We included this comparison to help readers better understand the factors that may affect knowledge editing performance.

\textbf{Q4:} 1) In Section 5.2.2, further clarification is needed regarding the meaning of “user number” and “allowed maximum items.” 2) Additionally, what is the precise gap between editing and testing in user-specific editing, and 3) why is the gap in the two visual editing tasks similar (1, 3, 6, 10) without a larger gap?
 % [need to figure number and line number, explanation in rebuttal}] 
 
A: 1\&2) The terms "user num" and "gap" indeed have different meanings, and we have added a figure in the appendix (Figure 11, Line 1297) to clarify the distinction. In summary, the "gap" refers to the number of editing examples in a sequence, while the "user num" represents the number of users in a sequence, where each user may have a different amount of user-specific data (at least one and at most the "allowed maximum items"). The "allowed maximum items" refers to the maximum amount of data for a single user, which is set to 9 in our benchmark. 3) We also conducted experiments to analyze how performance changes with a larger gap on the visual semantic editing dataset. The results, presented in Table 14 (Line 1204) in the appendix, show that as the gap increases, the performance of FT-LLM and FT-Alignment methods decreases, while the SERAC method shows minimal change.

\textbf{Q4:} Some Typos in this paper

A: Thanks for pointing out the typos in our paper, we have revised them in the version.











































\newpage
\section{Response to Reviewer PUQh}

\textbf{W1:} The training dataset size seems to be small, which might not be enough for mend/ serac that needs training. This could potentially lead to lower performance of such methods.

A: On the one hand, we aim to show that our benchmark is of a similar order of magnitude as existing benchmarks, such as VLKEB and MIKE. On the other hand, to illustrate the impact of training data, we plot the training loss of SERAC and MEND in the appendix (Figure 13–14, Lines 1355–1399) of the revision. As shown, during the training process, the loss consistently decreases and eventually converges to a small value, indicating that the training data is sufficient to train the model.

\textbf{W2:} There is potential to enhance the T-Loc test. 

A: As for T-LOC, some methods such as FT-alignment and SERAC are nearly 100\%,  which is attributed to the editing method. As FT-alignment only updates the projection model between the vision and language model and SERAC only updates the scope classifier and counterfactual mode, without updating the LLM model, so for a given text-only question, the output before and after by LLM is nearly the same, with the T-LOC nearly 100\%. Besides, to make a better understanding of the effect of questions for T-LOC, we further generate harder T-LOC questions by LLM on visual semantic editing. We give the original knowledge (e.g. This is a fist in a life gesture. The thumb and pinky finger are extended while the other fingers are clenched into a fist. It signifies strength or readiness to strike.) and item type (e.g. life gesture) to LLM and make it generate type-related questions (e.g., What does it mean when someone waves their hand up and down enthusiastically?). We conduct experiments on visual semantic editing data, and the results are shown in this table. As we can see, the results of most editing methods decrease under harder T-LOC questions, while FT-Alignment and SERAC are nearly unchanged. This also verifies that both methods could achieve high T-LOC in different questions. 


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[h]
\centering
\begin{tabular}{clccl}
\toprule
\textbf{Model}                      & \textbf{Method} & \textbf{T-Loc (new)} & \textbf{T-Loc (origin)} &  \\
\midrule
\multirow{6}{*}{\textbf{LLaVA-1.5}} & FT-LLM         & 59.21     & 79.62              &  \\
                                    & FT-Alignment    & 100.00    & 100.00             &  \\
                                    & IKE             & 50.74     & 61.10               &  \\
                                    & SERAC           & 99.97     & 99.99              &  \\
                                    & MEND            & 95.65     & 98.15              &  \\
                                    & KE              & 63.53     & 71.39              &  \\
\bottomrule
\end{tabular}
\end{table}

\textbf{W3:} The division of the three tasks lacks clarity. 

A: We have added a figure in the appendix (Figure 12, Line 1322) of the revision to illustrate the differences between visual entity editing and semantic editing. In summary, visual entity editing focuses on entity recognition, specifically identifying who or what the entity is, while semantic editing emphasizes understanding semantic behaviors, such as gestures, emotions, and actions, without relying on who or what the entity is. User-specific editing aims to inject user-related information into the models.


\textbf{W4:} Additionally, I question about if this user-specific editing approach is practical. 

A: We agree that adopting an external memory or database is another approach to achieve user-specific applications. However, we would like to emphasize that the need for a user-specific scenario remains realistic for users regardless of the method employed. Furthermore, as the amount of user-specified data increases, the required storage space grows, and the retrieval performance from memory declines, leading to degraded performance in real-world applications. Thus, we believe that both adopting external memory and injecting user-specific knowledge into the model are viable solutions for achieving a personalized model in real-world scenarios, each with its own strengths and weaknesses. Therefore, we propose exploring knowledge editing as an alternative method to achieve this. Lastly, among knowledge editing methods, some also adopt memory-based strategies, such as SERAC, meaning that memory-based approaches are already included in the benchmark evaluation. 



\textbf{W5:} And the task generalization might not be persuasive using only one case. If you want to prove this, more comprehensive experiments and evaluations should be conducted.

A: We have added additional task generalization cases in the appendix (Figure 39–44, Lines 1760–1830) to help readers better understand the editing results.


\textbf{Q1:} From Figure 1, there is no T-Gen test in MMKE-Bench, and you add a T-Rel test. Why do you make this change?

A: 1) The reason T-GEN is not tested in our benchmark is that it is not necessary for this benchmark. In triplet-based evaluation, the reliability question is the same as the training question, requiring a text generalization question that conveys the same meaning as the original question but with different phrasing. In our benchmark, however, the training question differs from the text reliability question, so rewriting the text reliability question is unnecessary. 2) The reason for using T-rel for evaluation is that the descriptions in our benchmark contain multiple facts, and T-rel reflects how well these edited facts are learned in a text-only format.

\textbf{Q2:} Additionally, how do you adapt each method to the lmm editing?

A: We follow the implementation of MMedit and VLKEB to adapt each method, with VLKEB also built on MMedit. For FT-LLM, FT-Alignment, KE, and MEND, we update the LLM module in a large multi-modal model. For SERAC, we update the scope classifier and counterfactual model during training. For IKE, we append the current editing examples with selected examples as in-context examples.

\textbf{MMedit}:  https://github.com/zjunlp/EasyEdit

\textbf{VLKEB}: https://github.com/VLKEB/VLKEB/tree/main


\textbf{Q3:} Figure 4: why I-Gen not evaluated for MMEdit?

A: We have rechecked the results of MMedit and found the I-Gen results in Figure 5. Since there is no explicit number in this figure, we estimate the I-Gen results to be 98.5\%. Based on this, we have re-plotted Figure 4 in the revision and replaced the original. As shown, the conclusion remains unchanged, confirming that our benchmark is more challenging than previous benchmarks.

\textbf{Q4:} Section 5.2.2 requires additional clarification on the terms “user number” and “allowed maximum items.” Further, what about the mend method in your sequential editing setting? And are the gaps substantial enough for the test? In LLM knowledge editing studies and the referenced VLKEB work, this gap can be larger, reaching 100 or more.

A: The terms "user num" and "gap" indeed have different meanings, and we have added a figure in the appendix (Figure 11, Line 1297) to illustrate the difference. In summary, the "gap" refers to the number of editing examples in a sequence, while the "user num" represents the number of users in a sequence, where each user may have a different amount of user-specific data (at least one and at most the "allowed maximum items"). The "allowed maximum items" refers to the maximum amount of data for a single user, which is 9 in our benchmark. In addition, we follow the implementation of VLKEB and have not tested MEND in our benchmark. Lastly, we conducted experiments to analyze how performance changes with a larger gap in the visual semantic editing dataset. The results, shown in Table 14 (Line 1204) in the appendix, indicate that as the gap increases, the performance of FT-LLM and FT-Alignment methods decreases, while the SERAC method shows little change.


\textbf{Q5:} The analysis in lines 425-426 only explains the possible reason for serac, leaving out mend.

A: We apologize for the confusing statement. SERAC is a memory-based method, while MEND is a parameter-based method. The performance of SERAC may be attributed to the use of in-context examples, while MEND's performance may result from fewer updates to the original model parameters.


\textbf{Q6:} Line 450, “visual knowledge” can be ambiguous, and the reliability seems to be not lower as stated.

A: We apologize that the words ‘visual knowledge’ should be ‘visual semantic knowledge’ in line  450.  We focused on portability and found that the results of visual semantic editing and user-specific editing is lower than than of visual entity editing. 


\textbf{Q7:} Line 466, how do you draw the conclusion that “parameter-based methods are better at applying edited knowledge to new contexts”?

A: For this conclusion, we mainly focus on the results of portability in Table 3-5. As we can see, KE achieves the best result in five of nine tasks, and the second best best in four of nine tasks, so we think KE is the best method for applying editing knowledge to new/related content. KE is one of the parameter-based methods, so we draw this conclusion.


\textbf{Q8:} Some Writing issues in this paper

A: Thanks for pointing out the writing issues of our work.  For Figure 1, we add a sentence "original knowledge = editing knowledge" in user-specific editing to help the reader better understand the editing; For Figure 2, we have replaced "hint" with "explanation" to avoid confusion. Besides, we have fixed all other issues in the revision.



\end{document}


