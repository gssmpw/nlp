
\begin{figure}
\centering
    \includegraphics[width=0.5\linewidth]{img/pdeexp.pdf}
  \vskip -0.1in
  \caption{PDE expression building and discovery architecture. The expression is $u_t = \tilde{u}^\epsilon \tilde{u}_x$ with parameter $\epsilon$ as the exponent.}
  \label{fig:pdeexp}
  \vskip -0.1in
\end{figure}

\section{Mechanistic Networks for PDE Discovery}
The \emph{goal} of discovery is to find the equations governing spatio-temporal data representing some physical phenomenon. 
The \emph{result} of governing equation discovery is a set of partial differential equations that, ideally, completely describe the given data.
Further restrictions may be added to the form of the discovered equations so that the equations may be interpreted by human scientists. 
One such restriction is that the resulting equations should be composed only of \emph{simple} functions. 
Another natural restriction is that the equations should \emph{concise}, or sparse, with as few terms as possible \cite{brunton2016discovering}.

In this section we present a Mechanistic PDE Network discovery architecture that allows discovery of simple and concise governing partial differential equations from spatio-temporal data.
With Mechanistic PDE Networks the PDE representations can contain arbitrary differentiable expressions over inputs parameterized by neural networks as in Figure \ref{fig:pdeexp}.
This allows the design to model complex nonlinear PDE expressions, going beyond linear combinations of fixed basis functions.



\subsection{Discovery Model}
\looseness=-1Given spatio-temporal data $u_{\text{data}}(t,x)$, where we assume two dimensions for simplicity, the discovery method generates a parameterized PDE \emph{expression} along with initial and boundary conditions.
$\mathcal{P}_u := \mathcal{P}_u(u, u_t, u_x, u_{tt}, u_{tx}, u_{tt}; \Theta)$.
In general the form of the PDE may include arbitrary differentiable functions of any complexity, such as deep neural networks.
However to satisfy the twin goals of simplicity and conciseness we restrict the allowed forms of the expressions appearing in PDEs to simple tree structures where each node is an elementary function with some weighting parameter. 
One common example of a tree expression structure in PDE modeling is a finite set of simple basis functions such as polynomials of a fixed maximum degree.
Mechanistic networks, however, are not constrained to linear combinations of fixed basis functions and can represent more complex expressions by combining the data $u$ and its derivatives with parameters in a differentiable way (Figure \ref{fig:pdeexp}).

Once the form of the expression is specified the aim of the discovery procedure is to find the appropriate parameters of the PDE expression given data.
One could also imagine learning the structure of the expression itself; however, that is not a direction we take, and in this paper, we use fixed forms of PDE expressions.

\textbf{Parameterized PDE Expressions.} Given the restriction of conciseness, there must only be a small number of expression parameters that describe complex data. 
Furthermore, the initial values of these parameters might be far from the true parameters. 
If the PDE solution corresponding to the initial parameters is far from the true $u_\text{data}$ the iterative learning can become brittle and non-smooth due the small number of parameters available to the optimization process. 

To make the learning smoother and more flexible, we parameterize the input to the PDE expression with a neural network.
We do this by transforming the data $u_\text{data}$ to $\tilde{u} = \text{NN}(u_\text{data})$ with a neural network and compute the expression on the transformed data $\tilde{u}$ instead of $u_\text{data}$.
This allows the learning to adapt both parameters and basis input to get to the target solution, leading to a more flexible learning procedure.
A further advantage of parameterizing the expression input is to make it robust to noise in the input data.
To ensure that the final learned equation is correct for $u$, we also add a loss term $loss(u,\tilde{u})$ so that  $\tilde{u}$ is close to $u$ at convergence.

\begingroup
\setlength\abovedisplayskip{1.5pt}
\setlength\belowdisplayskip{1.5pt}
\textbf{Discovery Example.} As an informative example, we illustrate the discovery method for Burger's equation: $u_t = uu_x  + 0.1u_{xx}$, where the diffusion term, $u_{xx}$, has the fixed coefficient 0.1. 
Given $u_\text{data}$, we first compute a transformation $\tilde{u} = \text{NN}(u_\text{data})$. 
Choosing degree two polynomials as the expression functions we obtain the coefficients for $u_x$ and $u_{xx}$ respectively as $p_1(\tilde{u}) = \theta_{0} + \theta_{1}\tilde{u} + \theta_2 \tilde{u}^2 $ and $p_2(\tilde{u}) = \phi_0 + \phi_1 \tilde{u} + \phi_2 \tilde{u}^2$, with $\theta_i$ and $\phi_i$ as learnable parameters.
Choosing second order spatial and first order temporal derivatives we get a PDE expression of the form
\begin{equation}
u_t = p_1(\tilde{u}; \theta)u_x + p_2(\tilde{u}; \phi)u_{xx}. \label{eq:burgers-example}
\end{equation}
The true equation corresponds to the case where $\theta_1=1$, $\phi_0 = 0.1$ and the remaining parameters are 0.
The initial and boundary conditions are obtained from $\tilde{u}$.
In each iteration we solve Equation \ref{eq:burgers-example} using the NeuRLP-PDE solver obtaining solution $u$.
We then compute the loss  as a sum of the losses (either $L_1$ or $L_2$) between $u_\text{data}$, $u$ and $u$, $\tilde{u}$.
We also include an $L_1$ loss over the parameters for sparsity. 
The total loss is computed as $l(u_\text{data},u,\tilde{u}, \theta,\phi) = loss(u_\text{data},u) + loss(u,\tilde{u}) + l_1(\theta, \phi)$.
\endgroup

The loss is iteratively minimized with gradient descent.
Finally a concise PDE is generated by thresholding the parameters, with the ones below the threshold set to 0.


