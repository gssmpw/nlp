Partial differential equations (PDEs) enjoy widespread use as interpretable, analytical models of spatio-temporal dynamics in all areas of science \cite{temam2024navier, turing1952chemical}.
The \emph{development} of PDE models still, however, is largely a manual task that is performed by domain experts examining experimental data.
The modern explosion of experimental and simulation data has underscored the need for data-driven machine learning models for discovery of governing equations \cite{brunton2020machine}. 
In spite of progress \cite{brunton2016discovering, raissi2019physics, dodeformer}, the development of machine learning discovery models that are flexible, handle complex nonlinear data and account for missing physics, while being robust to noise and missing data, is very much an open problem.

A recent discovery architecture for \emph{ordinary} differential equations (ODEs) was demonstrated by Mechanistic Neural Networks (MechNN) \cite{pervezmechanistic}, enabling a flexible and expressive network design for modeling explicit, interpretable nonlinear ODEs.
MechNNs embed a specialized, parallel ODE solver in the network, allowing explicit differential equation representations in neural networks, which can be employed for ODE discovery and prediction, both relevant in applications including general time series and numerical simulations~\cite{pervezmechanistic} and in climate science~\cite{chen2024scalable,yao2024marrying}.

In this paper we focus on \emph{governing equation discovery} and demonstrate Mechanistic Neural Networks for discovery of \emph{partial differential equations} from data.
We develop a PDE solver, NeuRLP-PDE, for parallel native differentiable solution of linear partial differential equations over multiple spatial dimensions (in addition to the time dimension). 
In contrast to ODEs, scaling is significantly more difficult in PDE solving, with memory requirements increasing exponentially with dimension and quickly becoming excessive even for mid-sized grids.
With NeuRLP-PDE, \emph{sparse computation} in all phases of the PDE solver ensures that the memory requirement of the solver over batches of PDEs remains limited for sizeable grids.
We build the solver using constrained optimization and a sparse \emph{multigrid} preconditioned iterative linear solver \cite{saad2003iterative} for solving the large and sparse linear systems that appear in the constrained optimization.
The NeuRLP-PDE solver is then incorporated into the PDE discovery architecture enabling the learning of governing PDEs directly from data.

The MechNN-PDE \emph{discovery} architecture couples the NeuRLP-PDE solver with deep neural networks that parameterize partial differential equations.
Two natural constraints on discovered differential equations are that they should be 1) \emph{simple}: i.e., built from elementary functions and 2) \emph{concise}: containing as few terms as possible. 
The MechNN-PDE architecture enables simplicity and conciseness by building PDE expressions using a family of elementary basis functions, each with a learnable parameter.
Since elementary functions are restrictive, for expressive modeling the basis functions inputs are parameterized by neural networks.
The built PDE is then solved by NeuRLP-PDE to obtain the solution $u$.
Loss terms ensure consistency so that the parameterized basis functions equal the true basis functions at convergence.
A sparsity loss ensures that the discovered expressions are concise and the entire pipeline is optimized by gradient descent.

The proposed synthesis of neural networks and PDEs for discovery has the following advantages.
\begin{itemize}
\item Derivatives never have to be directly on data and are only computed internally by the solver.
\item PDEs can contain arbitrary differentiable functions, going beyond generalized linear functions.
\item Numerical solving together with neural networks provide robustness to noise and missing data.
\item Memory-efficient batch-parallel PDE solving on GPU with a sparse multigrid-preconditioned iterative solver.
\end{itemize}

