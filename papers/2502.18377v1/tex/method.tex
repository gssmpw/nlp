A Mechanistic PDE Network consists of a \emph{mechanistic encoder}, $f_{Enc}$, a \emph{differentiable solver}, NeuRLP-PDE, specialized for linear PDEs, and an optional \emph{decoder}, $f_{Dec}$.
The input to the encoder are spatio-temporal data over time and one or more spatial dimensions, denoted $u_{\text{data}}(t,x_1, x_2, \ldots, x_{n_d})$. 
Given input $u_{\text{data}}$ the mechanistic encoder produces a set, of predefined size $n_{eq}$, of linear partial differential equations which we denote as $\{\mathcal{P}_{u_i}\}_{i\in [n_{eq}]}$ (using the notation $[n] := \{1,\ldots,n\}$).
Each $\mathcal{P}_{u_i}$ is a linear PDE for a spatio-temporal function $u_i(t,x_1, x_2,\ldots)$. 

All functions $u_i$ are defined over a domain $\mathcal{D}$ which we assume to be a Cartesian grid, i.e., $\mathcal{D}=\mathcal{T}\times\mathcal{X}_1\times\mathcal{X}_2\ldots\mathcal{X}_{n_d}$, where $\mathcal{T}$, $\mathcal{X}_i$ are intervals.
For conciseness, we collect the partial derivative indices appearing in PDE $\mathcal{P}_{u_i}$ into a multi-index set denoted $\mathcal{M}$.
As an example $\mathcal{M} = \{t, x_1, x_2, x_1x_1\}$, $\{u_m\}_{m\in \mathcal{M}}$ denotes the set of partial derivatives $\{u_t, u_{x_1}, u_{x_2}, u_{x_1x_2}\}$.
We also use the empty index $\phi$ for $u$, i.e., $u_\phi := u$.
\begingroup
\setlength\abovedisplayskip{1.5pt}
\setlength\belowdisplayskip{1.5pt}
For simplicity of exposition we assume a set with a single PDE denoted $\mathcal{P}_u$ for a function $u(t,x)$ over a spatial dimensions $x:=(x_1,\ldots, x_{n_d})$ with $t \in[0, t_f]$ and $x_i \in [0, x_{i,f}]$. 

With these simplifications $\mathcal{P}_u$ has the following general form:
\begin{equation}
\mathcal{P}_{u}: \sum_{m\in\mathcal{M}}c_m(t,x; u_\text{data})u_m = b(t,x; u_\text{data}), \label{eq:linear-pde-model}
\end{equation}
where $\mathcal{M}$ is a multi-index set, $u_m$ are partial derivatives and $c_m, b$ are time-space varying coefficients over $\mathcal{D}$.
Given a set $\mathcal{B}$ of boundary coordinates we complete the PDE specification by specifying the initial and boundary conditions with a function $\omega$ defined over $\mathcal{B}$ as 
\[
u(i) = \omega(i), \;\; \forall i \in \mathcal{B},
\]
In general mechanistic PDE networks allow flexibility in the specification of initial and boundary conditions which may be of the Dirichlet (i.e., boundary conditions on $u$), Neumann (i.e., boundary conditions on partial derivatives) or mixed types and may be arbitrary linear constraints (subject to well-posedness).
The time and space varying coefficients allow the PDE representations of mechanistic PDE networks to be very expressive and allow for modeling of complex dynamical phenomena.
In this paper we assume that PDE representations are well-posed and that the PDEs together with the boundary conditions have a unique solution.

\textbf{Discretization.}
To implement the PDE in a neural network we discretize the rectangular domain $\mathcal{D}$ over time and space.
We assume one time and one spatial dimension.
A given time interval $[0,t_f]$ is discretized into $n_t$ intervals $[t_i, t_{i+1}]$ with step size $s_{t_i} = t_{i+1}-t_i$.
Similarly a spatial interval $[0,x_{i,f}]$ is discretized into $n_{x_i}$ intervals $[x_{i,k}, x_{i,k+1}]$ with steps $s_{x_{i,k}}$.

All functions $c_m,b, \omega$ produced by the mechanistic encoder are now assumed to be defined over the discretized spatio-temporal grid.
With mechanistic networks the steps sizes can also learned and are then produced by the mechanistic encoder. 
All together the mechanistic encoder parameterizes the PDE by producing the coefficients, boundary parameters and the step sizes defined over the spatio-temporal grid for a given multi-index set $\mathcal{M}$.
\begin{equation}
[\{c_m\}_{m\in\mathcal{M}}, b, \omega, \{s_{t_i}\}, \{s_{x_i}\}] = f_{Enc}(u_{data})
\end{equation}

Next the generated PDE is fed to the NeuRLP-PDE solver which produces the solution and all partial derivatives specified in $\mathcal{M}$ as $\{u_m\}_{m\in\mathcal{M}}$.
The output of the solver is fed to a task-specific decoder or directly used in a loss.
All aspects of the PDE model in equation \ref{eq:linear-pde-model} including coefficients, initial and boundary conditions and step sizes are differentiable.

\textbf{Nonlinear PDEs.} The PDE representations generated by MechNN-PDE are linear, time-space varying.
However, the model is not restricted to learning linear PDEs. 
Nonlinear PDEs can be represented by using nonlinear basis functions, either directly over the input data $u_\text{data}$ or by learning nonlinear functions of $u$.
We illustrate this further in the section on discovery.
\endgroup


\section{Linear PDE Solving With Differentiable Optimization}
The workhorse of mechanistic PDE networks is NeuRLP-PDE -- a specialized, parallel and differentiable solver for linear PDEs.
NeuRLP-PDE solves linear PDEs, with partial derivatives of arbitrary order, over a discretized spatio-temporal grid by reducing PDE solution to a relaxed differentiable constrained optimization \cite{young1961linear}. 

Briefly, the constrained problem specifies that 1) the discretized PDE holds over all points of the grid, 2) that the initial or boundary conditions hold and 3) that the computed solution is a smooth function over the grid, and the objective is to minimize approximation error.
We then solve a relaxation of the optimization problem so that we can differentiate through the PDE solution to the PDE data using techniques from differentiable optimization. 

To fully specify the constrained optimization, we  specify the optimization variables and constraints.
We consider the discretized Cartesian grid with $t$ and $x$ discretization $\{t_i\}_{i\in [n_t]}$, $\{x_j\}_{j\in [n_x]}$.
Let $\mathcal{I}$ and $\mathcal{B}$ denote the set of grid and boundary indices, respectively. 
We also use the shorthand $\texttt{next}_{c}(i)$ and $\texttt{prev}_{c}(i)$ where $i\in\mathcal{I}$ and $c$ is a dimension for the next and previous adjacent grid points to point $i$ for coordinate $c$.

\emph{Variables.} We are given a multi-index set $\mathcal{M}$ with $\phi \in \mathcal{M}$.
We create variables $u_{\phi,i}$, $i\in\mathcal{I}$ for the solution $u$ at each grid point.
We also create variables for each partial derivative for each $i\in\mathcal{I}$, 
obtaining variables denoted $u_{t,i}$ and $u_{xx,i}$ for partial derivatives $u_t$ and $u_{xx}$ and similarly for all other partial derivatives from $\mathcal{M}$.
We denote the number of optimization variables by $n_v$.

The solver has three types of constraints: equation, smoothness and initial and boundary constraints.

\begingroup
\setlength\abovedisplayskip{1.5pt}
\setlength\belowdisplayskip{1.5pt}
\emph{Equation} constraints specify that the left and right hand sides of the given PDE are equal over all grid points.
\begin{equation}
\sum_{m\in\mathcal{M}}c_{m,i}u_{m,i} = b_i, \;\; \forall i \in \mathcal{I}
\end{equation}

\emph{Initial and boundary} constraints are specified for points in $\mathcal{B}$, where the form of the constraint can depend on the problem.
Here we give Dirichlet-type conditions which specify the values of $u$ at points in $\mathcal{B}$.
\begin{equation}
u_{\phi,i} = \omega_i, \;\; \forall i \in \mathcal{B}
\end{equation}

We use central difference constraints to compute partial derivatives. 
We illustrate for a 3 point central difference approximation for a second order partial derivative $u_t$, with derivative multi-index $m$. 
\begin{equation}
s_{t}^2u_{m,i} =u_{\texttt{prev(i)}} -2u_i + u_{\texttt{next(i)}}  \;\; \forall i \in \mathcal{I\setminus\mathcal{B}}
\end{equation}
In practice we use 5 point central differences and use forward, backward finite differences at the edges of the grid.

\emph{Forward and backward smoothness} constraints specify that the solution $u$ is smooth over the grid in each dimension in both directions.
This is achieved by computing a 1D Taylor expansion at each grid point for each dimension and approximating the $u$ value at the next and previous adjacent grids point for that dimension.
The constraint specifies that the Taylor approximation and the solution variables at the next point should be close.
We illustrate with a second-order Taylor approximation over the $t$ dimension.
\begin{align}
u_{\phi,i} + s_t u_{m_1,i} + \frac{1}{2}s_t^2 u_{m_2,i} &= u_{\texttt{next}_{t}(i)}\\
u_{\phi,i} - s_t u_{m_1,i} + \frac{1}{2}s_t^2 u_{m_2,i} &= u_{\texttt{prev}_{t}(i)},
\end{align}
where $m_1,m_2$ are multi-indices corresponding to the first and second partial derivatives of $t$.

We collect the variables $u_{m,i}$, $m\in\mathcal{M}, i\in I$ into a vector $z$. 
We can then write the constraints in matrix form as 
\begin{equation}
Az=d,\label{eq:matrix-form}
\end{equation}
with $A\in\mathrm{R}^{n_c\times n_v}$ being the coefficient matrix with $n_c$ constraints, and $d$ is the vector of constant coefficients.

We can solve eq~\eqref{eq:matrix-form} in a least squares sense by solving the corresponding normal equations.
\begin{equation}
A^\intercal Az= A^\intercal d.\label{eq:normal-eq-form}
\end{equation}
Solving these equations gives a solution $z$ of the optimization problem and thus of the PDE. 
However, we also require the dual variables for the differentiating through the least squares optimization.
For this we consider the following saddle-point formulation of least squares \cite{saad2003iterative}.
\begin{equation}
\begin{bmatrix}
I & A\\
A^\intercal& 0
\end{bmatrix}
\begin{bmatrix}
\lambda^*\\
z^*
\end{bmatrix}
= 
\begin{bmatrix}
d\\
0
\end{bmatrix}\label{eq:kkt-forward}
\end{equation}

\looseness=-1where $\lambda$ are the dual variables.
This formulation can also be seen as the KKT constraints of the convex quadratic programming formulation of least-squares.
Given the saddle-point formulation we can apply techniques from differentiable optimization for differentiating through the solution of constrained optimization problems \cite{amos2017optnet}.

\textbf{Forward pass.} In the forward pass we solve the normal equations and compute the dual variables from \ref{eq:kkt-forward} to obtain $z^*$ and $\lambda^*$.

\textbf{Backward pass.} Give scalar loss $l$ we consider the gradient, $g_z := \partial_{z^*} l$, relative to $z^*$. 
We require the gradient of $l$ relative to the matrix $A$ and vector $d$ which contain the optimization data.
Following \citet{amos2017optnet}, we can achieve this by solving the following saddle-point system:
\begin{equation}
\begin{bmatrix}
I & A \\ 
A^\intercal & 0
\end{bmatrix}
\begin{bmatrix}
d_\lambda\\
d_z
\end{bmatrix}
= 
\begin{bmatrix}
0\\
-g_z
\end{bmatrix}\label{eq:kkt-back}
\end{equation}
\looseness=-1The gradient of the loss $l$ relative to $A$ and $d$ is $\partial_A l = d_\lambda {z^*}^\intercal + \lambda^* d_z^\intercal$ and $\partial_d l  = -d_\lambda$ \cite{amos2017optnet}.

\textbf{Scaling PDE Solving.}
The linear systems in equations \eqref{eq:kkt-forward}, \eqref{eq:kkt-back}, \eqref{eq:normal-eq-form} can become very large with increasing dimension of the PDEs which results in very large grids over which the PDE is defined.
As an example for a PDE defined over 32x32 grids we have 5120 variables and 9212 constraints requiring 377 MB of double precision dense storage.
However for 32x32x32 (220k variables and 420k constraints)  we would need 780GB and for 64x64x64 we would need 50k GB.
For such large grids the linear systems cannot be solved by dense methods.
For the experiments we solve 1D PDEs with a dense solver, since for these for which the memory requirement is reasonably bounded for small grids and batch sizes.
However, for 2D or higher PDEs with midsize grids (say, 32 to 64 per dimension), the memory required for dense solvers is too great for GPU storage.
The matrices, however, are highly sparse with 99.9\% zero values for a 32x32 matrix and 99.999\% zeros for a 64x64x64 matrix.
For such problems we develop a multigrid preconditioner for use with a sparse iterative solver.
\endgroup

\subsection{An Iterative Sparse Multigrid Solver}
To make PDE solving feasible for more than two dimensions (including time) and for large grids, we build an iterative sparse solver for solving the normal equations.
We use the FGMRES linear solver, the flexible variant of the Krylov-subspace linear solver GMRES \cite{saad1986gmres}, as the base linear solver.
As is well-known from numerical linear algebra, standalone iterative solvers can lead to poor solutions for complex problems and require \emph{preconditioning} for accurate solution.
General purpose preconditioning methods such as incomplete LU or Cholesky factorization are not feasible for the large systems we encounter due to their high memory use and lack of GPU implementations.
Instead, we build a multigrid \emph{V-cycle} preconditioner for FGMRES. 
We briefly describe the multigrid V-cycle here and in Algorithm \ref{alg:v-cycle} in the appendix. 
More details on the multigrid methods and sparse iterative solvers can be found in \citet{saad2003iterative} and \citet{briggs2000multigrid}.

\textbf{Multigrid V-cycle}. The multigrid \emph{V-cycle} approach solves a linear system for a PDE on a succession of fine to coarse grids.
Given a square linear system $Mx = b$ with solution $x^*$ and current solution $x_0$  we can compute the error as $\epsilon = x^* - x_0$ and residual as $r_0 = b - Mx_0$.
We note that we can rewrite the linear system as $M\epsilon_0 = r_0$, where we wish to solve for the error $\epsilon_0$.
Given an estimate for $\epsilon_0$ we can add it to $x_0$ to obtain an improved solution.
Next we assume that we have a sequence of $n_g$ successive coarse-grained versions of $M$ (with the dimension of $M_{i+1}$, being, say, half the dimension of $M_i$) given as $M_1=M, M_2, \ldots, M_{n_g}$.

In the first half of the V-cycle, at the $k$th grid the linear system $M_{k}\epsilon_{k} = r_{k}$ is partially solved by taking a few steps of a simple iterative \emph{relaxation} method such as the Jacobi or Gauss-Seidel method, i.e., $\epsilon_{k} := \texttt{relax}(M_k, r_k)$.
Next the new residual $r'_k:= r_k - M_k \epsilon_k$ in the linear system at grid $k$ is computed  and transferred to the next coarser grid $k+1$ by a \emph{restriction} operator designed for grid transfer, i.e., $r_{k+1} := \texttt{restrict}(r'_k)$. 
The linear system at the coarsest grid $n_g$ is small and we solve for the exact error $\epsilon_{n_g}$ for the given residual $r_{n_g}$.

In the second half of the V-cycle the error at grid $k$ is transferred to the next finer grid $k-1$, starting from the coarsest grid, by a \emph{prolongation} operator, i.e., $\tilde{\epsilon}_{k-1} = \texttt{prolong}(\epsilon_k)$.
The prolonged error is added to the error at the finer grid, to obtain the new error at grid $k-1$, i.e, $\epsilon_{k-1} :=\epsilon_{k-1} + \tilde{\epsilon}_{k-1}$.
The error is smoothed by relaxation $\epsilon_{k-1} := \texttt{relax}(M_{k-1}, \epsilon_{k-1}, r_{k-1})$
The V-cycle ends when we return to the finest grid and we improve the solution $x_0$ by adding the estimated error, i.e, $x_0 := x_0 + \epsilon_1.$

In our solver we use the multigrid V-cycle as a preconditioner for FGMRES, which we found led to more accurate solutions compared with conjugate gradient methods and standalone V-cycle.
We use Gauss-Seidel as the relaxation method and linear interpolation as the restriction and prolongation methods.
To construct coarse grid matrices we use grid sizes with powers of 2 and successively halve each dimension whlist simultaneously doubling the step size in each dimension.
We use size 8 for each dimension for the coarsest grid.
The PDE coefficients and boundary conditions are coarsened by linear interpolation and the multigrid preconditioned FGMRES is applied to the matrix $A^\intercal A$ from the normal equations.

The resulting iterative solver is entirely sparse and batch-parallel on GPU for a batch of PDEs and is useful for higher dimensional PDEs where GPU memory cannot hold the full dense constraint matrices.

\input{tex/discovery_method}

