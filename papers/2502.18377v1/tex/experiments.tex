
\looseness=-1We demonstrate PDE discovery on a number of PDEs in two and three dimensions (including time), including complex dynamical data from the 2D Navier Stokes and reaction diffusion PDEs. 
We examine the robustness of the method to noise.
We also consider PDE parameter discovery in the case where the PDE cannot be expressed as a linear combination of fixed basis functions.
Table \ref{tab:pdes} lists the PDEs that we use to demonstrate our method, including their general form.

\begin{figure}[h]
  \vskip -0.05in
  \centering
    \includegraphics[height=0.30\linewidth]{img/data/burgers_inviscid.pdf}
    \includegraphics[height=0.30\linewidth]{img/data/vorticity.png}
  \vskip -0.1in
  \caption{Inviscid Burger's Equation (discontinuous shocks are visible) (left) and the vorticity field data $\nabla\times U$ from an incompressible Navier-Stokes equation.}
  \label{fig:fluid-data}
  \vskip -0.1in
\end{figure}

\textbf{Robustness to Noisy Data.} We test discovery in noisy settings by adding Gaussian noise of variance $\sigma^2$ to the data.
The noise standard deviation $\sigma$ is chosen to be a ratio $\sigma_{NR}$ of the data root mean squared value i.e., $\sigma := \sigma_{NR}||U||_{RMS}$, where $U$ denotes the dataset and $\sigma_{NR} \in [0,1]$ is the noise ratio \cite{messenger2021weak}.
In the experiments, we report the noise level as a percentage as $\sigma_{NR}. 100\%$ noise. 

\textbf{Evaluation Metrics.} 
We use two metrics for evaluating the quality of the discovered equation.
The \emph{true positivity ratio} is defined as $\text{TPR} = \frac{\text{TP}}{\text{TP}+\text{FN}+\text{FP}}$ \cite{lagergren2020learning} and measures how many terms have been correctly identified. 
We measure the maximum relative error of the true coefficients as $E_\infty(\xi) = \max_{j:\hat{\xi}_j^*!=0}  \frac{|\xi_j -\hat{\xi}^*_j|}{|\xi_j|}$, with $\xi,\hat{\xi}$ being the discovered and true coefficients \cite{messenger2021weak}.  
$E_\infty(\xi)$ serves as the measure of accuracy of discovered coefficients.


\begin{table}[t] %
  \centering
  \vskip -0.1in
  \caption{PDEs for Discovery}
  \begin{footnotesize}
  \begin{tabular}{lc}
    \toprule
    Name & Expression\\
    \midrule
    Diffusion & $u_t = \nu u_{xx}$\\
    Viscous Burger's & $u_t + u u_x = \nu u_{xx}$\\
    Inviscid Burger's & $u_t + u u_x =0$\\
    Porous Medium & $u_t = \nabla^2 (u^m)$\\
    Ginzburg-Landau & $\partial_t A = A + \nabla^2 A - (1+i\beta)|A|^2A $\\
    Navier-Stokes &$\partial_t U + U\cdot \nabla + \nabla p = \nu \nabla^2 U$\\
    \bottomrule
  \end{tabular}
  \label{tab:pdes}
  \end{footnotesize}%
  \vskip -0.15in
\end{table}
\begin{figure}
\centering
  \vskip -0.05in
    \includegraphics[width=0.35\linewidth]{img/data/rdiff.jpg}
    \includegraphics[width=0.3\linewidth]{img/data/react_diff_gl.jpg}
  \label{fig:img-reactdiff}
  \vskip -0.15in
  \caption{Easy case, showing $A^i$ (left) and harder, showing the norm $|A|^2$ (right) at a single time-step for reaction-diffusion data.}
  \vskip -0.1in
\end{figure}
\textbf{Datasets, experiments, baselines.}
We perform experiments on a number of 1D and 2D (in space) PDEs chosen to cover a range of complexity.
For the 1D equation, we demonstrate discovery with the diffusion equation and Burger's equation (both viscous and inviscid (i.e., without viscosity)).
The inviscid Burger's equation is known to develop \emph{shocks}, which appear as a discontinuity in wave propagation.
We show that our proposed method can perform discovery in the presence of this form of discontinuity.
The \emph{porous medium} equation is a nonlinear equation that cannot be written in the form of linear combinations of fixed basis functions because of a real exponent.
We show how the mechanistic network framework easily extends to such cases while many other methods cannot.

\looseness=-1For 2D PDEs, we work with reaction-diffusion and incompressible Navier-Stokes equations. 
Ginzburg-Landau reaction-diffusion equations are frequently employed for modeling biological pattern development.
We demonstrate discovery in two settings: 1) a relatively easy spiral dataset with a single spiral and 2) a harder setting with significantly more complex patterns where the baseline methods fail to recover the equations even in the absence of noise.
We also demonstrate discovery for an incompressible Navier-Stokes example with a small viscosity term \cite{pyro_joss}.
We set the viscosity term $\nu$ to 0.001 and show that our method is able to recover the equation while the baselines cannot.
For baselines, we compare with PDEFIND \cite{rudy2017data} and WeakSINDy \cite{reinbold2020using}, with implementations by \citet{de2020pysindy}, which are known to be robust to noise in many settings.
We especially focus on comparing the robustness under noise of our method with the baselines.

\begin{figure}[t]
  \vskip -0.05in
    \includegraphics[width=0.48\linewidth]{img/gl_simple_tpr.pdf}
    \includegraphics[width=0.48\linewidth]{img/gl_tpr.pdf}
  \vskip -0.1in
  \caption{TPR for easy (left) and harder (right) reaction-diffusion experiments with varying amounts of noise.}
  \vskip -0.05in
  \label{fig:rdiff-tpr}
\end{figure}
\begin{figure}
  \vskip -0.1in
    \includegraphics[width=0.48\linewidth]{img/gl_simple_error_max.pdf}
    \includegraphics[width=0.48\linewidth]{img/gl_error_max.pdf}
  \vskip -0.1in
  \caption{Log $E_\infty$ errors for the easy (left) and harder (right) reaction-diffusion experiment with varying amounts of noise.}
  \label{fig:rdiff-infinity-error}
  \vskip -0.1in
\end{figure}

\subsection{Solver Unit Test}
First, we briefly verify that our solver is capable of solving some given linear PDEs. 
We test on the 2D Laplace PDE, $\nabla^2 u = 0$ on 32x32, 64x64, and 128x128 grids using the multigrid solver.
We compare against a reference finite-difference implementation from the \texttt{py-pde} \cite{pypde} package with sinusoidal boundary conditions obtaining $L_2$ errors of orders $10^{-1}, 5\times 10^{-2}$ and  $10^{-2}$, respectively, showing the errors decreasing with increasing grid size. This simple evaluation shows that our solver is indeed capable of solving these equations to an accuracy comparable with traditional libraries while also being able to parallelize solving multiple equations on GPUs.


\subsection{PDE Discovery}
In this section, we demonstrate PDE discovery for equations in one and two spatial dimensions in addition to time.



\textbf{2D PDEs.} For two dimensional PDEs, we test on Ginzburg-Landau reaction diffusion equations and the incompressible Navier-Stokes equations.
For 2D PDEs the PDEs are a set of coupled equations.
For ease of training we demonstrate discovery on one equation from each of the coupled PDEs.

\begingroup
\setlength\abovedisplayskip{1.5pt}
\setlength\belowdisplayskip{1.5pt}
\textbf{2D Reaction-Diffusion Equations.} The Ginzburg-Landau reaction-diffusion equations are shown in Table \ref{tab:pdes} in complex form in the variable $A$.
For ease, we write the complex equations as a system of two equations, where we take the first equation in our experiments.
\begin{align}
A^r_t &= D_1\nabla^2 + A^r(1-|A|^2) + |A|^2 \beta A^i\\
A^i_t &= D_2\nabla^2 + A^i(1-|A|^2) - |A|^2 \beta A^r,
\end{align}
where $A^r$ and $A^i$ are the real and imaginary parts of $A$ and $|A|^2 = {A^r}^2 + {A^i}^2$ and $D_1$, $D_2$, $\beta$ are parameters.

\emph{Easy Case.} For the reaction-diffusion equations, we first test on an \emph{easy} dataset with a single spiral pattern for which the baselines also perform well \cite{reinbold2020using}. 
For this example, the spatial domain has a length of 20 with equal spatial step sizes and a spatial resolution of 64x64 in the $x$ and $y$ dimensions.
For time we use 128 time steps with a step size of 0.05.
An example of the data at a single time step is shown in Figure \ref{fig:img-reactdiff} (left).
The data was generated by a spectral method following the implementation in \citet{de2020pysindy}, setting $D1=D2=0.1$  and $\beta=1$.
\endgroup
\begin{figure}[t]
    \includegraphics[width=0.48\linewidth]{img/ns_tpr.pdf}
    \includegraphics[width=0.48\linewidth]{img/ns_error_max.pdf}
  \vskip -0.1in
  \caption{TPR and $E_\infty$ error for the Navier-Stokes dataset.}
  \vskip -0.2in
  \label{fig:ns-plots}
\end{figure}


\looseness=-1\emph{A Harder Case.} For reaction diffusion equations we also test on a more difficult dataset with more complex patterns, for which the baselines are unable to discover the correct equations even on noise-free data.
The dataset has a spatial resolution of 256x256 over a Cartesian domain of size 10 with equal step size.
The data was generated by using the Basilisk solver \cite{kenneally2020basilisk} starting from a random initial condition and running with a maximum time step size of 0.05. 
We set $D1=D2=1$  and $\beta=1.5$.
We take only 128 time steps from the data for training.
An example of the data at a single time step is shown in Figure \ref{fig:img-reactdiff} (right).

\emph{Training Data and Parameterization.} We use the same training setup for both kinds of reaction-diffusion examples.
For our model, the training data is mini-batched with a mini-batch size of 8. 
Each data example in the minibatch is of size 32x32x32, where the first dimension is time.
The data is parameterized by 10-layer 2D ResNets, where we consider the time dimension as a batch dimension.
The data consists of two scalar fields $A^r, A^i$, which are separately parameterized by neural networks before being used to build the PDE expression as $\tilde{u} = \text{NN}(A^r)$ and $\tilde{v} = \text{NN}(A^i)$.

\emph{PDE Expression.} The PDE expression model uses third degree polynomial functions built from the parameterized inputs $\tilde{u}, \tilde{v}$ and second order derivatives in $x$ and $y$.
We use four separate polynomial expressions in the PDE: one serving as the coefficients of the $u$ term, another two for the $u_{xx}$ and $u_{yy}$ terms, and one as the right hand side $b$.
The polynomial basis parameters for discovery are computed by a two-layer MLP with a learnable input, which we find to improve training over a flat parameter vector. 
The boundary conditions are learned Dirichlet boundary conditions and are simply chosen to be the boundaries of $\tilde{u}$.
Given the PDE expression and the boundary conditions, the PDE expression is fed to the solver to get solution $u$.

\looseness=-1\emph{Loss.} The final loss is an $L_1$ loss, which we find improves discovery.
The loss is composed of four terms. 
The first is a loss $l_1(u, A^r)$, minimizing the loss between solution and data.  
The other two terms are losses for the parameterizations $\tilde{u}$ and $\tilde{v}$ as $l_2(\tilde{u}, u)$ and $l_3(\tilde{v}, A^i)$.
Since we only solve one equation at a time, we do not have a learned solution to the second coupled equation and $\tilde{v}$ minimizes the loss directly with the data $A^i$.
The final term in the loss is a sparsity term, also $L_1$ for the basis polynomial parameters.
We use a weight of $10^{-4}$ for the sparsity term.
In particular, we note that we do not threshold the parameters or repeat the optimization in our experiments with thresholded parameters, using only a single training cycle.
Repeating the optimization can and does improve the accuracy of the learned coefficients, but also increases training time, which is why we do not repeat the optimization for the results in this paper.

The model is trained with Adam with a learning rate of $10^{-5}$, which we fix for all experiments.
We do not use learning rate scheduling for the experiments for simplicity and use a fixed learning rate throughout.
We repeat the training with up to $80\%$ noise added to $A^r$ and $A^i$.

\emph{Results.} The TPR results are shown in Figure \ref{fig:rdiff-tpr} and the $E_\infty$ errors are shown in Figure \ref{fig:rdiff-infinity-error} for the various quantities of noise for both the easy and hard experiments for MechNN-PDE, WeakSINDy and PDEFIND.
For clean data in the easy example, we see that all methods recover the non-zeros terms with a TPR of 1 with small error in the coefficients.

Upon increasing noise levels, we find that PDEFIND worsens the most, and the TPR drops significantly to 0-0.2 for the easy case while the error becomes very large.
WeakSINDy performs much better with noise on the easy dataset, and MechNN-PDE and WeakSINDy have similar TPR and error with noise except for 80\% noise, where WeakSINDy has better error for similar TPR as MechNN-PDE. 

For the harder case, neither PDEFIND nor WeakSINDy is able to find any significant non-zeros terms even with clean data.
The TPR seems to increase later on for PDEFIND, however, this is spurious due to the large error as seen in the plots.
WeakSINDy also performs no better than random in this setting.
MechNN-PDE, however, is able to maintain high TPR with noise for up to 40\% noise, with the TPR dropping to 0.6 at 80\% noise, showing consistent performance in TPR and error with noise.

\looseness=-1\textbf{Incompressible Navier-Stokes.} We use the first component of the velocity $U_1$ from the coupled equations for discovery given as $  {U_1}_t + U\cdot \nabla U_1 + p_x = \nu \nabla^2 U_1,$ where the pressure gradient $\nabla p$ is given.
We use a similar setup for the reaction-diffusion equations, except that we use degree 1 polynomials. We include interaction terms with the first derivatives.
We use a small viscosity $\nu=0.001$ for the data (Figure \ref{fig:fluid-data}).
A comparison of the results with WeakSINDy is shown in \ref{fig:ns-plots}, where WeakSINDy is unable to discover the system.
For MechNN-PDE, the TPR is still lower than 1 with low noise.
On inspection of the coefficients, it appears that this is due to the presence of zero terms with small coefficients that are of the order of the viscosity which appears as false positives, even though the viscosity and non-zeros terms are accurately recovered.

\textbf{1D PDEs.} We demonstrate discovery on the 1D diffusion equation and inviscid and viscous Burger's equations. 
We use degree 4 polynomials for the coefficients of the $u_x$ and $u_{xx}$ terms.
For all cases in the noise-free case the method discovers the true equation. 
For Burger's equation we experiment with noisy data and compare with WeakSINDy and find both methods to be on-par with each other.
The results are shown in Appendix \ref{sec:app:1d-experiments}.

\subsubsection{Discovery Beyond Generalized Linear}
MechNN-PDE can represent complex differentiable expression in PDEs, going beyond expressions representable as linear combinations of fixed basis functions.
We demonstrate with the task of parameter discovering for the Porous Medium equation for which the expression cannot be written as a linear combination of fixed basis functions.
The porous medium equation is given as $u_t = \nabla^2 (u^m),  m > 0$, and we want to recover the parameter $m$ given data.
We set $m=2.675$ in the data and do not use any other basis functions.
We note that $m$ is a positive real value which precludes fixed polynomial basis functions.
The task is to discover the exponent of $u$.
We build the PDE expression with an exponent for the parameterized data $\tilde{u}$.
For this example we also parameterize the gradient with a neural network.
With clean data we recover the coefficient 2.64$\pm$0.03 close the true value of 2.675.

