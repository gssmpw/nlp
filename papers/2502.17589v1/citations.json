[
  {
    "index": 0,
    "papers": [
      {
        "key": "TemplateChartSum",
        "author": "Farahani, Ali Mazraeh and Adibi, Peyman and Ehsani, Mohammad Saeed and Hutter, Hans-Peter and Darvishy, Alireza",
        "title": "Automatic chart understanding: a review"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "OCRChartSum",
        "author": "Parida, Shantipriya",
        "title": "Extractive Odia Text Summarization System: An OCR Based Approach"
      },
      {
        "key": "StructChartSum",
        "author": "Falke, Tobias",
        "title": "Automatic structured text summarization with concept maps"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "OCRChart2text",
        "author": "Kantharaj, Shankar and Leong, Rixie Tiffany Ko and Lin, Xiang and Masry, Ahmed and Thakkar, Megh and Hoque, Enamul and Joty, Shafiq",
        "title": "Chart-to-text: A large-scale benchmark for chart summarization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ChartAssistant2024",
        "author": "Meng, Fanqing and Shao, Wenqi and Lu, Quanfeng and Gao, Peng and Zhang, Kaipeng and Qiao, Yu and Luo, Ping",
        "title": "Chartassisstant: A universal chart multimodal language model via chart-to-table pre-training and multitask instruction tuning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ChartThinker2024",
        "author": "Mengsha Liu and\nDaoyuan Chen and\nYaliang Li and\nGuian Fang and\nYing Shen",
        "title": "ChartThinker: {A} Contextual Chain-of-Thought Approach to Optimized\nChart Summarization"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhou2024visual",
        "author": "Yucheng Zhou and\nXiang Li and\nQianning Wang and\nJianbing Shen",
        "title": "Visual In-Context Learning for Large Vision-Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ChartAdapter2024",
        "author": "Peixin Xu and\nYujuan Ding and\nWenqi Fan",
        "title": "ChartAdapter: Large Vision-Language Model for Chart Summarization"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhou2024less",
        "author": "Zhou, Yucheng and Zhang, Jihai and Chen, Guanjie and Shen, Jianbing and Cheng, Yu",
        "title": "Less Is More: Vision Representation Compression for Efficient Video Generation with Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "LLMTasks",
        "author": "Michael Wornow and\nYizhe Xu and\nRahul Thapa and\nBirju S. Patel and\nEthan Steinberg and\nScott L. Fleming and\nMichael A. Pfeffer and\nJason A. Fries and\nNigam H. Shah",
        "title": "The Shaky Foundations of Clinical Foundation Models: {A} Survey of\nLarge Language Models and Foundation Models for EMRs"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhou2025weak",
        "author": "Yucheng Zhou and Jianbing Shen and Yu Cheng",
        "title": "Weak to Strong Generalization for Large Language Models with Multi-capabilities"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Transformer",
        "author": "Xiaopeng Zhang and\nHaoyu Yang and\nEvangeline F. Y. Young",
        "title": "Attentional Transfer is All You Need: Technology-aware Layout Pattern\nGeneration"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "Transformer",
        "author": "Xiaopeng Zhang and\nHaoyu Yang and\nEvangeline F. Y. Young",
        "title": "Attentional Transfer is All You Need: Technology-aware Layout Pattern\nGeneration"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "BERT",
        "author": "Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova",
        "title": "{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding"
      },
      {
        "key": "GPT",
        "author": "Zhenhailong Wang and\nManling Li and\nRuochen Xu and\nLuowei Zhou and\nJie Lei and\nXudong Lin and\nShuohang Wang and\nZiyi Yang and\nChenguang Zhu and\nDerek Hoiem and\nShih{-}Fu Chang and\nMohit Bansal and\nHeng Ji",
        "title": "Language Models with Image Descriptors are Strong Few-Shot Video-Language\nLearners"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "BERT",
        "author": "Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova",
        "title": "{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wang2024diffusion",
        "author": "Wang, Chenglin and Zhou, Yucheng and Zhai, Zijie and Shen, Jianbing and Zhang, Kai",
        "title": "Diffusion Model with Representation Alignment for Protein Inverse Folding"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "LLMTasks",
        "author": "Michael Wornow and\nYizhe Xu and\nRahul Thapa and\nBirju S. Patel and\nEthan Steinberg and\nScott L. Fleming and\nMichael A. Pfeffer and\nJason A. Fries and\nNigam H. Shah",
        "title": "The Shaky Foundations of Clinical Foundation Models: {A} Survey of\nLarge Language Models and Foundation Models for EMRs"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "EmergentAbilities",
        "author": "Flavio Petruzzellis and\nAlberto Testolin and\nAlessandro Sperduti",
        "title": "Assessing the Emergent Symbolic Reasoning Abilities of Llama Large\nLanguage Models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "LLMChallenges",
        "author": "Emily M. Bender and\nTimnit Gebru and\nAngelina McMillan{-}Major and\nShmargaret Shmitchell",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "wang2024memorymamba",
        "author": "Wang, Qianning and Hu, He and Zhou, Yucheng",
        "title": "Memorymamba: Memory-augmented state space model for defect recognition"
      }
    ]
  }
]