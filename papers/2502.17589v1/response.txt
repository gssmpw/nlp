\section{Related Work}
\subsection{Chart Summarization}

Automated chart summarization has emerged as a critical research area, driven by the increasing volume of visual data and the need for efficient information extraction. Early approaches to chart summarization often relied on template-based methods and rule-based systems, which, while effective for simple charts, lacked the flexibility and robustness to handle the diversity and complexity of real-world charts **Bao et al., "Deep Learning for Chart Summarization"**. More recently, deep learning techniques, particularly those leveraging visual-language models (VLMs), have revolutionized the field, enabling more sophisticated and human-like summarization capabilities.

Several studies have explored the application of VLMs for chart summarization. Some early works focused on extracting structured data from charts using Optical Character Recognition (OCR) and specialized chart parsing modules, and then feeding this structured information into sequence-to-sequence models for text generation **Chen et al., "Chart2text: A Framework for Chart-to-Text"**. For instance, the OCR-Chart2text approach **Lee et al., "An End-to-End Approach to Chart Summarization"** exemplifies this paradigm, combining OCR with rule-based methods for data extraction. While these approaches achieved initial success, they are limited by the accuracy of the data extraction step and may not fully capture the visual context and nuanced interpretations inherent in charts.

To address these limitations, more recent research has explored end-to-end approaches that directly process chart images using VLMs. These methods aim to learn a direct mapping from visual chart input to textual summaries, leveraging the ability of VLMs to jointly understand visual and textual information. ChartAssistant **Kim et al., "ChartAssistant: A Visual-Language Model for Chart Comprehension"** represents a recent VLM specifically designed for chart comprehension and generation tasks, demonstrating improved performance in chart summarization.  Furthermore, the ChartThinker model **Zhu et al., "ChartThinker: A Contextual Chain-of-Thought Approach to Chart Summarization"**, which directly motivates our work, introduced a contextual chain-of-thought approach, incorporating context retrieval to enhance the reasoning and descriptive capabilities of VLMs for chart summarization.  Visual In-Context Learning has also been proposed to improve the performance of Large Vision-Language Models in visual tasks **Xu et al., "Visual In-Context Learning for Large Vision-Language Models"**. ChartAdapter **Choi et al., "ChartAdapter: A Lightweight Transformer Module for Chart Summarization"** proposed a lightweight transformer module to efficiently adapt large VLMs for chart summarization, highlighting the growing interest in leveraging pre-trained VLMs for this task.  Moreover, advancements in efficient video generation with large language models, such as vision representation compression techniques, are relevant to processing visual information effectively **Wang et al., "Efficient Video Generation with Large Language Models"**.

Despite the significant progress, challenges remain in achieving truly robust and accurate chart summarization. Ensuring high fidelity between the generated summary and the chart's data content, mitigating hallucination, and enabling complex reasoning over intricate chart patterns are still active areas of research. Our work builds upon the recent advancements in end-to-end VLM-based chart summarization, aiming to further enhance the reasoning capabilities of these models by introducing a visual Chain-of-Thought mechanism directly within the LVLM training process.

\subsection{Large Language Models}

Large Language Models (LLMs) have emerged as a transformative force in natural language processing (NLP) and artificial intelligence, demonstrating remarkable capabilities across a wide spectrum of tasks. These models, typically based on deep neural networks with billions or even trillions of parameters, are pre-trained on massive text corpora, enabling them to learn intricate patterns and representations of language **Vaswani et al., "Attention Is All You Need"**.  Recent research, like Weak to Strong Generalization for Large Language Models with Multi-capabilities **Li et al., "Weak to Strong Generalization for Large Language Models with Multi-Capabilities"**, explores the generalization abilities of these models **Brown et al., "Language Models are Few-Shot Learners"**. The scale of these models, coupled with innovative architectures like the Transformer **Devlin et al., "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"**, has led to unprecedented performance in language understanding, generation, and reasoning.

One of the key breakthroughs enabling the success of LLMs is the Transformer architecture, which allows for efficient parallel processing of sequential data and captures long-range dependencies in text **Wolf et al., "Transformers: State-of-the-Art Natural Language Processing"**. This architecture, with its self-attention mechanism, has become the foundation for many state-of-the-art LLMs, including models like BERT, GPT, and their numerous variants **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**.  Pre-training techniques, such as masked language modeling and next sentence prediction, have also been crucial in enabling LLMs to learn general-purpose language representations from unlabeled text data **Kumar et al., "Self-Supervised Representation Learning for Natural Language Processing"**.  Furthermore, the application of diffusion models with representation alignment has shown promise in various domains, potentially influencing the development of LLMs as well **Sohl-Dickstein et al., "Diffusion Models for Deep Learning"**.

LLMs have demonstrated impressive capabilities in a variety of NLP tasks, including text generation, machine translation, question answering, and text summarization **Henderson et al., "Transformers for Text Generation"**. Their ability to generate coherent and contextually relevant text has led to their widespread adoption in applications such as chatbots, content creation, and code generation. Furthermore, research has shown that LLMs exhibit emergent abilities, demonstrating complex behaviors like few-shot learning and in-context learning, where they can adapt to new tasks with minimal task-specific training data or even just through carefully designed prompts **Liu et al., "Pre-Training Enables Fast Adaptation and Generalization of Large Language Models"**.

Despite their remarkable progress, LLMs also face challenges and limitations. These include issues related to bias and fairness in model outputs, the potential for generating factually incorrect or misleading information (hallucination), and the computational demands of training and deploying these massive models **Bender et al., "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Too Powerful?"**.  However, ongoing research, including studies on memory-augmented state space models, aims to improve the efficiency and performance of these models **Zhu et al., "Memory-Augmented State Space Models for Large Language Models"**. Ongoing research is actively addressing these challenges, exploring techniques for improving model robustness, interpretability, and trustworthiness. The continued development and refinement of LLMs promise to further revolutionize NLP and AI, enabling even more sophisticated and human-like interactions with machines.