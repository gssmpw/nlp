\section{Introduction}
\label{sec:intro}
Sketching has long been a key form of visual expression that rapidly communicates ideas and expresses concepts.
Even people with little experience can easily sketch simple objects and ideas.
However, creating sketches that depict complex concepts and scenes remains a significant challenge for many.
Typically, individuals begin sketching by creating a rough partial sketch but often struggle to turn this into a final complex sketch that maintains a unique style.
One common challenge individuals face is the difficulty of vividly illustrating the interactions and compositions between the objects or subjects in the desired scene.

Although ShadowDraw~\cite{lee2011shadowdraw} provides real-time guidance when sketching simple objects, it does not adequately address the above challenges that individuals encounter when trying to portray elaborate scenes in a consistent style.
More recent sketch generation methods~\cite{vinker2023clipascene,xing2023diffsketcher} make it easier to generate intricate sketches from scratch using user-provided text prompts or reference images.
However, these methods lack the capacity to consider the user-provided partial sketches, thus creating two major issues: redundant strokes and style inconsistency.
First, such methods tend to generate strokes that duplicate elements of the user-provided partial sketch.
Second, all strokes are of the same style; the styles of the generated strokes are not adapted to match those of the user-provided partial sketch.
These methods thus do not automatically complete partial sketches provided by users.

To address these issues, we propose \methodName, a novel style-aware vector sketch completion method that accepts both a text prompt and a partial sketch as input.
The method completes the partial sketch by generating strokes that illustrate missing elements or concepts, while preventing the creation of redundant strokes and ensuring that the style aligns with that of the input partial sketch.
Following~\cite{xing2023diffsketcher}, we begin by optimizing strokes based on a guidance image generated by a pretrained ControlNet model conditioned on the input partial sketch.
We introduce a mask penalty to ensure that the generated strokes do not overlap with those of the input partial sketch, so there are no redundant strokes.

However, stroke optimization alone does not ensure that the completed sketch is satisfactory, because the style of the input partial sketch is not considered.
This raises two main issues.
First, the styles of the guidance images generated by ControlNet often do not match those of the partial sketches.
Second, the styles of the generated strokes may not align with those of the input partial sketch.
This underscores the importance of two tasks, \ie~``adding style descriptions to the input prompt before inputting it to the ControlNet'' and ``adjusting the styles of the generated strokes to ensure alignment with these descriptions''.


Based on these observations, we utilized a pretrained vision-language model (VLM) in conjunction with the stroke optimization process.
First, we leverage the VLM to extract style descriptions from the input partial sketch and then incorporate these descriptions into the input prompt.
This enables the ControlNet to generate guidance images that are very similar to the input partial sketch, improving the completed sketch.
Second, we employ the VLM to generate an executable code that adjusts the strokes in SVG format, thus enhancing the style coherence of the final completed sketch.
The main reason for using the VLM for style adjustment is that the variety of sketch styles complicates the task of defining appropriate parameterizations to capture all potential styles effectively.
Consequently, adjusting the styles using an optimization-based method becomes challenging.
Although it is possible to use the VLM to directly adjust strokes, this often results in stroke loss.
Moreover, given the token limitations, existing VLMs typically handle only a small number of strokes.
The use of the VLM to generate an executable style adjustment code overcomes these challenges, resulting in a more stylistically consistent sketch without losing content.

We compare our results with those of existing methods across various sketch styles and prompts.
Extensive quantitative and qualitative evaluations revealed that the completed sketches generated by our method better preserve the styles of the input partial sketches and more accurately represent the contents specified in the prompts.







