\section{Experiment}
\input{fig_files/comparison_fig}
\subsection{Implementation Details and Performance}
In this work, we use the GPT-4o model~\cite{hurst2024gpt} as the VLM, which extracts style descriptions and generates style adjustment codes.
We implement the first stage of our method using PyTorch~\cite{pytorch}.
The Adam~\cite{adam} optimizer is used to optimize the strokes.
The first stage, consisting of $1,000$ iterations, takes approximately $5$ minutes to complete, while the second stage requires around $3$ minutes when a sketch contains $512$ strokes.
For all computations, we used a PC with an Intel i7-12700 CPU and an NVIDIA RTX 4080 GPU.

\subsection{Comparison with Existing Methods}
\label{sec:result_comparison}
We qualitatively and quantitatively compare our method to conditional T2I models used to generate sketches and line drawings, namely ControlNet LineArt\footnote{\url{https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart}} and ControlNet Scribble\footnote{\url{https://huggingface.co/lllyasviel/sd-controlnet-scribble}} both qualitatively and quantitatively.
In~\cref{fig:comparison}, we show the results generated by our method and the two methods using identical user-provided partial sketch and stylized prompts.
The partial sketches were prepared by re-tracing publicly shared sketches and clipart, or were generated by other sketch generation methods, such as CLIPasso~\cite{vinker2022clipasso}.
The results of the Control-based methods (\cref{fig:comparison}(b,c)) often exhibit incomplete or inconsistent content.
Additionally, these methods tend to apply style transformations that deviate significantly from those of the provided sketches, sometimes entirely altering the styles. 
In contrast, our method consistently generates completed sketches that faithfully represent the contents of the text prompts.
Also, the styles of the generated strokes and the provided sketches are consistent.

To further validate the effectiveness of our method in terms of preserving the sketch styles and completing the content, we gather an evaluation set containing $10$ sketches and perform two types of quantitative evaluation.
First, we use commonly use visual and text metrics to evaluate the performance of our method.
However, since these metrics are typically not used for evaluating the sketch completion task and have their own limitation, we additionally conduct an user evaluation which further validate our method.
\paragraph{Evaluation using existing metrics.}
In terms of visual metrics, we used LPIPS~\cite{zhang2018perceptual}, DINO~\cite{caron2021emerging}, and DreamSim~\cite{fu2023dreamsim}.
These metrics were used to measure the style consistencies and image similarities between the input partial sketches and the generated completed sketches.
However, visual metrics alone cannot be used to sufficiently evaluate performance because input partial sketches that do not receive additional strokes tend to achieve the best scores.
Therefore, we also assess the alignment between the content of each completed sketch and the input prompt using the VQA score~\cite{lin2024evaluating} to eliminate the bias associated with visual metrics.
The VQA score measures prompt-image alignment on compositional prompts more effectively than the CLIP score~\cite{radford2021learning} and is more closely aligned with human judgement.
As shown in~\cref{tab:quan_result}, our method significantly outperforms the other methods across all visual metrics and achieves a comparable score on the text metric.
\input{tab_files/compare_quan_tab}
\paragraph{User evaluation.}
We conducted a user evaluation to further validate that our method generates sketches whose styles match those in user-provided partial sketches and depict complete content in the input prompt.
We use the same evaluation set used in~\cref{sec:result_comparison} generated by our method, ControlNet LineArt, and ControlNet Scribble.
Participants evaluated the quality of the generated completed sketches by conducting pairwise comparisons.
For each input sketch and prompt, we created two comparative pairs, ``Ours vs. ControlNet LineArt'' and ``Ours vs. ControlNet Scribble'', resulting in $20$ pairs for comparison.
During each comparison, two completed sketches were shown side by side in random order, along with their inputs.
Participants were asked to judge the sketches based on two criteria: ``How well they preserved the \textit{styles} of the input partial sketch'' and ``How effectively they depicted the \textit{content} of the input prompt''.
Each comparison was evaluated by $25$ different participants.
As shown in~\mbox{\cref{tab:study_result}}, the participants preferred our method for both criteria.
\input{tab_files/user_evaluation_tab}






\subsection{Diverse Sketch Scenario}
\paragraph{Iterative sketch completion.}
\input{fig_files/iter_edits_fig}
Sketching is often an iterative process, where users may want to introduce new details by adding new strokes or modifying the original prompt.
Our method enables users to achieve iterative sketch completion by retaining some strokes from the completed sketch and incorporating new ones (\cref{fig:teaser}(b) and \cref{fig:iter_edit}(b)), or by updating the input prompt (\cref{fig:iter_edit}(a)).
\paragraph{Sketches with different prompts, or distinct sketches}
Users may seek to employ a variety of partial sketches when generating sketches that depict the same content in the input prompt.
As shown in~\cref{fig:diff_promp_sketch}(a), the completed sketches represent similar content but in different styles.
Additionally, as shown in~\cref{fig:diff_promp_sketch}(b), the completed sketches created using different input prompts can represent distinct contents but share a similar style.
\input{fig_files/diff_prompt_sketch_fig}




\subsection{Ablation Study}
\subsubsection{The effectiveness of the style adjustment stage}
To demonstrate the effectiveness of the style adjustment stage, we compared the results generated by only the first stage to those of our full method.
We show the results after the first stage in~\cref{fig:comparison}(d) and those of our full method in~\cref{fig:comparison}(e).
Although the results of the first stage are both visually appealing and adequately represent the content of the input prompt, the sketch styles do not align well with those of the user-provided partial sketches.
In contrast, after the style adjustment stage, the sketch styles of the generated strokes, namely the stroke width, spacing, and curvatures, are better aligned with those of the user-provided sketch.

\subsubsection{The effectiveness of stylized prompt.}
\input{fig_files/prompt_stylization_ablation_fig}
To key feature of our method is the extraction the style descriptions from the user-provided partial sketch, and the use thereof to generate both the guidance image and the style adjustment code.
To further validate the effectiveness of the stylized prompt, we compared the results generated using the stylized prompt to those generated using the original input prompt.
As shown in~\cref{fig:prompt_stylization_ablation}, use of the stylized prompt yields result with more of the desired content while better matching the style of the user-provided partial sketch.
Additionally, the quantitative results on the alignment with input prompt demonstrate similar results (\cref{tab:ablation_result}(a)). 


\input{tab_files/ablation_quan_tab}
\input{fig_files/code_adjust_ablation_fig}
\subsubsection{The effectiveness of style adjustment code generation.}
Compared to directly asking the VLM to edit the SVG code, we found that requesting the VLM to generate style adjustment code results in sketches that were more consistent in terms of styles and had more complete content.
In~\cref{fig:code_ablation}, we show that although our stroke optimization method generates style-agnostic sketches that represent the overall content, some details may be lacking if we ask the VLM to directly adjust the sketch styles.
In contrast, the style adjustment code effectively preserves the content when adjusting the styles to match those of the user-provided sketch.
We can also observe similar quantitative results in~\cref{tab:ablation_result}(b).


\subsubsection{Generalization of VLMs.}
\input{fig_files/vlm_ablation_fig}
Our method can utilize different VLMs to stylize the input prompt and generate style adjustment codes.
To demonstrate the generality of our method, we show an example of a completed sketch in~\cref{fig:vlm_ablation}, which was generated using Gemini 2.0~\cite{team2023gemini} as the VLM.
The result demonstrates that our method can complete sketches that convey the same content while preserving similar styles, regardless of the VLM used.






