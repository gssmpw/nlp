\section{Related Work}
\label{sec:related}
\subsection{Vector Sketch Generation}
Previous studies~\cite{eitz2012hdhso,ha2017neural,sketchy2016} have collected sketch datasets of amateur sketches that sought to realistically depict everyday objects, while OpenSketch~\cite{gryaditskaya2019opensketch} contains professional sketches of product designs.
Existing studies used these sketch datasets and various deep learning models~\cite{ha2017neural,lin2020sketch,ribeiro2020sketchformer,zhou2018learning} to generate sketch sequences.
However, given their reliance on these sketch datasets, such methods generally generate sketches of only simple objects.

Recently, with the development of differentiable rasterizers~\cite{Li:2020:DVG}, novel methods~\cite{vinker2022clipasso,vinker2023clipascene, xing2023diffsketcher,sketchVideo24} that employ the ``synthesis through optimization'' paradigm, have emerged.
Such methods typically optimize stroke geometry and appearance using priors obtained from large pretrained models such as CLIP~\cite{radford2021learning}, and text-to-image~\cite{rombach2022high} and -video~\cite{wang2023modelscope} models.
However, such methods are usually generate sketches from scratch based solely on prompts; they do not complete partial sketches.

\subsection{Visual Content Completion}
Given the challenges associated with visual content creation, it would be useful to prepare only some partial content and then apply a method that automatically or semi-automatically completes the rest of the work.
Previous works developed autocompletion systems for various visual content creation tasks using repetitive elements and the editing history, such as in 3D sculpting~\cite{peng2018autocomplete} and animation sculpting~\cite{peng2020autocomplete}.
Other methods aim to complete sketches~\cite{liu2019sketchgan} or afford real-time guidance during freehand drawing~\cite{lee2011shadowdraw}.
Such approaches typically use category-specific priors learned from sketch datasets or edge maps of real-world photographs.
However, these methods either require the editing history of the user or are limited to relatively simple objects.
In contrast, our method uses diffusion priors to complete large missing regions and complex concepts in a partial sketch, and ensures that the style of the completed sketch aligns with that of the original partial sketch.



\subsection{LLM-based Sketch and SVG Editing}
Recent advancements in large language models (LLMs) have enabled extensive research on vector graphic generation and editing~\cite{nishina2024svgeditbench,zou2024vgbench,cai2023leveraging}.
This progress has led to the development of new benchmarks and frameworks aimed at evaluating enhancing the capabilities of LLMs.
For example, SketchAgent~\cite{vinker2024sketchagent} leverages an LLM to iteratively generate sketch strokes based on text prompts, while StarVector~\cite{rodriguez2023starvector} presents a multimodal LLM designed to vectorize raster images.
Other previous works~\cite{wu2023iconshop,tang2024strokenuwa,xing2024empowering} incorporate specialized tokenization methods or modular architectures to improve LLMsâ€™ understanding of SVG structures, enabling advanced tasks such as text-guided icon synthesis and SVG manipulation. 

Despite their successes, existing methods mainly focus on generating or editing vector graphics from scratch and fail to maintain style consistency between existing strokes and newly generated ones. 
Also, they typically focus on depicting simple objects or concepts, such as individual man-made objects or animals. 
In contrast, our approach completes partial sketches for complex scenes and concepts that contains better object interactions and compositions in a coherent style.