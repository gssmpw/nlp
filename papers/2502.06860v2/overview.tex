\input{fig_files/overview_fig}




\section{Overview}
In~\cref{fig:overview}, we illustrate the overview of our method.
Our method takes a text prompt $\inputprompt$ and a partial sketch $\inputsketch$ as inputs.
The prompt describes the content to be illustrated in the completed sketch, but the user-provided partial sketch represents only some of the content described in the prompt.
The output is a completed sketch $\completesketch=\inputsketch \cup \optsketch$ that fully represents the content of $\inputprompt$.
Our method has two stages: \textit{style-agnostic sketch completion} and \textit{sketch style adjustment}.

In the first stage, the goal is to optimize a set of parametric strokes that, when combined with the user-provided partial sketch, ensure that the complete sketch represents the content of $\inputprompt$ without consideration of sketch styles.
First, we stylize $\inputprompt$ by leveraging a large vision-language model (VLM) to produce style descriptions $\augprompt$ of the given partial sketch $\inputsketch$, \ie~$\finalprompt=\{\inputprompt \cup \augprompt\}$ (\cref{fig:overview}(a)).
Then, we optimize the parameters of $\optsketch$ using a diffusion prior conditioned on the stylized text prompt $\finalprompt$ (\cref{fig:overview}(b)) and obtains $\intermediatesketch$.

In the second stage, the goal is to adjust the styles of $\intermediatesketch$ to ensure a coherent style across the final sketch. 
We task the VLM using a carefully crafted prompt that contains the completed sketch of the first stage in SVG format and the text prompt $\finalprompt$.
The VLM then generates executable code that adjusts the styles of the new strokes in $\intermediatesketch$ to the style of the original partial sketch. 
