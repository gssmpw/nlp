\newpage
\chapter{Constrained Optimization with Specific Constraints}
\begingroup
\hypersetup{
linkcolor=structurecolor,
linktoc=page,  % page: only the page will be colored; section, all, none etc
}
\minitoc \newpage
\endgroup


\section{Constrained Optimization}
Sections~\ref{section:pgd} and \ref{section:cond_gd} introduce projected gradient descent and conditional gradient method for solving the constrained optimization problem (P2) (see Definition~\ref{definition:opt_probs_all}).
Unlike unconstrained optimization problems, in constrained optimization problems, the variable $\bx$ cannot assume any arbitrary value. 
This means that many algorithms designed for unconstrained optimization are not applicable. 
For instance, in gradient descent methods, moving along the negative gradient direction may lead to points that are not feasible, and the gradient of the objective function at the optimal solution is not necessarily a zero vector. 
Consequently, constrained optimization problems are significantly more complex than their unconstrained counterparts. 

In this chapter, we will discuss several penalty function methods that incorporate constraints into the objective function as penalty terms, transforming these problems into more familiar unconstrained optimization problems. 


When the set $\sS$ in the problem (P2)  takes the following form, we deal with a constrained optimization problem involving both equality and inequality constraints:
\begin{subequations}
\begin{equation}\label{equation:pr_in_constchap}
\begin{aligned}
\textbf{(P4)}:\qquad  
&\mathop{\min}_{\bx} &f(\bx)&, \\
&\text{s.t.} & c_i(\bx)&=0, \,\,i\in\mathcalE,\,\,\text{with $\abs{\mathcalE}=p$},\\
&  & c_i(\bx)&\leq 0, \,\,i\in\mathcalI,\,\,\text{with $\abs{\mathcalI}=q$},
\end{aligned}
\end{equation}
where the variables $\bx \in \real^n$, $\mathcalE$ is the index set of equality constraints, $\mathcalI$ is the index set of inequality constraints,  and $c_i(\bx)$ are continuous functions.
The optimality conditions for (P4) are discussed in Sections~\ref{section:constrain_opt}, \ref{section:constr_convset}, \ref{section:convex_optimiz}, and \ref{section:gen_kkt_cond}.

In the previous chapter, various methods for solving unconstrained optimization problems were introduced. We want to transform problem (P4) into an unconstrained optimization problem. In the following sections, we also start by considering a simplified scenario where only equality constraints are present:
\begin{equation}\label{equation:pr_in_constchap_equ}
\begin{aligned}
\textbf{(P4.1)}:\qquad  	& \mathopmin{\bx} & f(\bx)&, \\
& \text{s.t.} & c_i(\bx)& = 0, \,\, i \in \mathcalE,\,\,\text{with $\abs{\mathcalE}=p$}.\\
\end{aligned}
\end{equation}
\end{subequations}
n certain special cases, directly solving the (nonlinear) system of equations $c_i(\bx) = 0$ can eliminate some variables, thereby converting the problem into an unconstrained one. However, for general functions $c_i(\bx)$, eliminating variables through this approach is not feasible, necessitating alternative methods to handle such problems.

\section{Penalty Function Method}\label{section:pen_func_me}
The \textit{penalty function method} aims to convert the constrained optimization problem (P4) into an unconstrained one. To ensure the quality of the approximate solution, the objective function of the new unconstrained problem includes the original objective function plus a penalty term related to the constraints. For points outside the feasible region, this penalty term is positive, effectively penalizing those points; for points within the feasible region, the penalty term is zero, meaning no penalty is applied. Consequently, the penalty term drives the solution of the unconstrained optimization problem towards the feasible region.

\subsection{Quadratic Penalty Function Method for Equality Constraints}\label{section:pen_equa}

For problems with equality constraints, various penalty terms can be selected, the simplest being quadratic. Here is the definition of the quadratic penalty function:
\begin{definition}[Quadratic Penalty Function for Equality Constraints]
For the equality-constrained optimization problem (P4.1) in \eqref{equation:pr_in_constchap_equ}, define the quadratic penalty function
\begin{equation}
f_{\sigma}(\bx) \triangleq f(\bx) + \frac{1}{2} \sigma \sum_{i \in \mathcalE} c_i^2(\bx)
\triangleq f(\bx) + \frac{1}{2} \sigma \normtwo{\bc(\bx)}^2,
\end{equation}
where $\bc(\bx)$ is the vector function $ \bc: \real^n \rightarrow \real^p $, with its $ i $-th component being the $ i $-th constraint function $ c_i $.
The term $\frac{1}{2} \sigma \normtwo{\bc(\bx)}^2$ is referred to as the \textit{penalty term}, and $\sigma>0$ is called the \textit{penalty factor}.
\end{definition}


Since this penalty function penalizes points that violate the constraints, during the iterative process, these points typically lie outside the feasible region. 
For these infeasible points, increasing $\sigma$ increases the weight of the penalty term, which forces its minimum point closer to the feasible region. 
On the other hand, within the feasible region, the global minimum point of $f_{\sigma}(\bx)$ aligns with the optimal solution of the equality-constrained optimization problem (P4.1).



\begin{example}[Penalty Function Method]\label{example:penal_func}
Consider the optimization problem
$$
\begin{aligned}
& \min & x + \sqrt{8} y\quad \text{s.t.} \quad  & x^2 + y^2 = 1.
\end{aligned}
$$
Two points satisfy the KKT conditions: $\big[-\frac{1}{3}, -\frac{\sqrt{8}}{3}\big]^\top$ and $\big[\frac{1}{3}, \frac{\sqrt{8}}{3}\big]^\top$.
It is easy to verify that the optimal solution is $\big[-\frac{1}{3}, -\frac{\sqrt{8}}{3}\big]^\top$. Consider the quadratic penalty function
$$
f_{\sigma}(x, y) = x + \sqrt{8} y + \frac{\sigma}{2} (x^2 + y^2 - 1)^2,
$$
where the contour lines of the penalty function for $\sigma = 1$ and $\sigma = 30$ are shown in Figure~\ref{fig:penalty_examp}. It can be seen that as $\sigma$ increases, the minimum value of the quadratic penalty function $f_{\sigma}(x, y)$ approaches the minimum value (indicated by asterisks in the figures) of the original problem. 
However, as $\sigma$ increases, the contour lines near the optimal point also become flatter, which complicates solving the unconstrained optimization problem.
\end{example}
\begin{figure}[h]
\centering       
\vspace{-0.25cm}                 
\subfigtopskip=2pt               
\subfigbottomskip=-2pt         
\subfigcapskip=-10pt      
\includegraphics[width=0.98\textwidth]{imgs/penalty_examp.pdf}
\caption{The contour plot of $f_{\sigma}(x, y) = x + \sqrt{8} y + \frac{\sigma}{2} (x^2 + y^2 - 1)^2$ for $\sigma=1$ and $\sigma=20$.}
\label{fig:penalty_examp}
\end{figure}


From the preceding example, we understand that given a penalty factor $\sigma$, we can approximate the solution to the original problem by minimizing $f_{\sigma}(\bx)$. However, this approach has limitations in practice. The following example demonstrates that when $\sigma$ is too small, the penalty function might become unbounded.
\begin{example}[Failure of Penalty Function Method]
Consider the optimization problem
$$
\begin{aligned}
& \min & -2x^2 + 9y^2 \quad
\text{s.t.} \quad  x = 1.
\end{aligned}
$$
By eliminating the variable $x$, it's straightforward to determine that the optimal solution is $[1, 0]^\top$. However, considering the penalty function
$$
f_{\sigma}(x, y) = -2x^2 + 9y^2 + \frac{\sigma}{2}(x - 1)^2,
$$
for any $\sigma \leq 4$, the penalty function is unbounded.
\end{example}

This phenomenon occurs because if the penalty factor is too small, the decrease in the objective function value at infeasible points may counterbalance the penalty imposed for constraint violations. S the initial choice of $\sigma$ should not be too small.



\begin{algorithm}[h] 
\caption{Quadratic Penalty Function Method for (P4.1) in \eqref{equation:pr_in_constchap_equ}}
\label{alg:quad_pen_eq}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$ and a set of constraints $\{c_i(\bx)\}$; 
\State {\bfseries Input:}  Choose the penalty factor growth coefficient $\rho > 1$;
\State {\bfseries Input:}  Initialize $\bx^{(1)}$ and $\sigma_1 > 0$;
\For{$t=1,2,\ldots$}
\State With $\bx^\toptzero$ as the initial point, solve $\bx^\toptone = \mathop{\argmin}_{\bx} f_{\sigma_t}(\bx)$; 
\State Choose $\sigma_{t+1} = \rho \sigma_t$;
\EndFor
\State {\bfseries Return:} final  $\bx^\toptzero$;
\end{algorithmic} 
\end{algorithm}


The execution process of Algorithm~\ref{alg:quad_pen_eq} is straightforward: first, select a series of exponentially increasing penalty factors $\sigma_t$, then for each penalty factor, solve the quadratic penalty function $f_{\sigma_t}(\bx)$ to find its minimum (or local minimum). 
Any of the (iterative) unconstrained optimization algorithms introduced in Chapter~\ref{chapter:gd_convg} or the Newton-type methods in Chapter~\ref{chapter:second_order} can be used to solve the subproblem in the Step 4 of the algorithm.
For different problems, the meaning of $\arg \min$ in the Step 4 of the algorithm can be different:
\begin{itemize}
\item $\bx^\toptone$ is the global minimum of the penalty function $f_{\sigma_t}(\bx)$;
\item $\bx^\toptone$ is a local minimum of the penalty function $f_{\sigma_t}(\bx)$;
\item $\bx^\toptone$ is not a strict minimum of the penalty function $f_{\sigma_t}(\bx)$, but approximately satisfies the first-order optimality condition $\nabla f_{\sigma_t}(\bx^\toptone) \approx \bzero$.
\end{itemize}
Based on the above description, three key points need attention in Algorithm~\ref{alg:quad_pen_eq} \citep{liu2020optimization}: 
\begin{itemize}
\item Careful selection of the parameter 
$\sigma_t$ is crucial. If $\sigma_t$ increases too quickly, solving the subproblem becomes challenging. Conversely, if $\sigma_t$ increases too slowly, the number of  iterations will increase, resulting in a slow convergence. 
A more reasonable approach is to adjust the increment of $\sigma_t$ based on the difficulty of solving the current $f_{\sigma_t}(\bx)$. If the previous subproblem converges quickly, a larger $\sigma_{t+1}$ can be chosen; otherwise, $\sigma_t$ should not be increased excessively. 
\item As mentioned in the previous example, when $\sigma_t$ is small, $f_{\sigma_t}(\bx)$ may become unbounded, leading to divergence. When solving the subproblem, if divergence is detected, the iteration should be stopped immediately and the penalty factor should be increased. 
\item The precision of solving the subproblem must be sufficiently high. To ensure convergence, the error in solving the subproblem needs to approach zero.
\end{itemize}


\subsection*{Convergence Analysis}

This subsection explores the convergence properties of the quadratic penalty function method for equality-constrained problems. For simplicity, we assume that for each $\sigma_t$, there exists a minimum point of $f_{\sigma_t}(\bx)$.
Note that this assumption may not hold for some optimization problems since the quadratic penalty function might not sufficiently penalize constraint violations. Therefore, we will not apply the quadratic penalty function method to optimization problems that do not meet this assumption.


\begin{theoremHigh}[Penalty Function Method with Optimal Subproblem]\label{theorem:pen_cong_glo}
Let $\bx^\toptone$ be a global minimum of $f_{\sigma_t}(\bx)$, and let $\sigma_t$ monotonically increase to infinity in Algorithm~\ref{alg:quad_pen_eq}. Then every limit point $\bx^*$ of the sequence $\{\bx^\toptzero\}_{t>0}$ is a global minimum of the original problem (P4.1) as defined in \eqref{equation:pr_in_constchap_equ}.
\end{theoremHigh}

\begin{proof}[of Theorem~\ref{theorem:pen_cong_glo}]
Let $\widehatbx$ be a global minimum of the original problem (P4), i.e.,
$ f(\widehatbx) \leq f(\bx)$, for all $ \bx$ satisfies $c_i(\bx) = 0$, $i \in \mathcalE$.
Since $\bx^\toptone$ is a global minimum of $f_{\sigma_t}(\bx)$, we have $ f_{\sigma_t}(\bx^\toptone) \leq f_{\sigma_t}(\widehatbx), $
i.e.,
\begin{align}
&f(\bx^\toptone) + \frac{\sigma_t}{2} \sum_{i \in \mathcalE} c_i(\bx^\toptone)^2 \leq f(\widehatbx) + \frac{\sigma_t}{2} \sum_{i \in \mathcalE} c_i(\widehatbx)^2 \label{equation:pen_cong_glo1}\\
&\quad\implies\quad 
\sum_{i \in \mathcalE} c_i(\bx^\toptone)^2 \leq \frac{2}{\sigma_t} \big(f(\widehatbx) - f(\bx^\toptone)\big). \label{equation:pen_cong_glo2}
\end{align}
Let $\bx^*$ be a limit point of $\{\bx^\toptzero\}$.  In \eqref{equation:pen_cong_glo2}, taking $t \rightarrow\infty$, based on the continuity of $c_i(\bx)$ and $f(\bx)$ as well as $\sigma_t \rightarrow +\infty$, we have
$ \sum_{i \in \mathcalE} c_i(\bx^*)^2 = 0. $
This shows that $\bx^*$ is a feasible solution of the original problem. From~\eqref{equation:pen_cong_glo1}, we get
$ f(\bx^\toptone) \leq f(\widehatbx). $
Taking the limit on both sides yields $f(\bx^*) \leq f(\widehatbx)$. By the optimality of $\widehatbx$, we conclude that  $f(\bx^*) = f(\widehatbx)$, implying that $\bx^*$ is also a global minimum.
\end{proof}

The above theorem suggests that if a global minimum of the subproblem (Step 4 of Algorithm~\ref{alg:quad_pen_eq}) can be found, then its limit points are global minima of the original problem. However, in practical applications, finding a global minimum of $f_{\sigma_t}(\bx)$ is challenging, so we typically solve the subproblem to a certain level of precision. Consequently, the applicability of Theorem~\ref{theorem:pen_cong_glo} is limited. The following theorem provides an alternative convergence result based on the KKT conditions.

\begin{theoremHigh}[Penalty Function Method with Suboptimal Subproblem]\label{theorem:pen_cong_glo_full}
Let $f(\bx)$ and $c_i(\bx)$, $i \in \mathcalE$ be continuously differentiable, and let $\sigma_t \rightarrow+\infty$ in Algorithm~\ref{alg:quad_pen_eq}. If the solution $\bx^\toptone$ of the subproblem satisfies
$ \normtwobig{\nabla f_{\sigma_t}(\bx^\toptone)} \leq \varepsilon_t$ where $\varepsilon_t \rightarrow0$, 
and for any limit point $\bx^*$ of $\{\bx^\toptzero\}$, the gradients $\{ \nabla c_i(\bx^*) \}_{i \in \mathcalE}$ are linearly independent, then $\bx^*$ is a KKT point (Definition~\ref{definition:kkt_point}) of the equality-constrained optimization problem (P4.1) in \eqref{equation:pr_in_constchap_equ}, and
$$
\lim_{t \rightarrow\infty} (\sigma_t c_i(\bx^\toptone)) = \lambda_i^*, \quad \forall i \in \mathcalE, 
$$
where $\lambda_i^*$ is the Lagrange multiplier corresponding to the constraint $c_i(\bx^*) = 0$.
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:pen_cong_glo_full}]
Given the stopping criterion for solving the subproblem, for $ \bx^\toptone $, we have:
\begin{equation}\label{equation:pen_cong_glo_full1}
\normtwobig{\nabla f_{\sigma_t}(\bx^\toptone)} =\normtwo{\nabla f(\bx^\toptone) + \sum_{i \in \mathcalE} \sigma_t c_i(\bx^\toptone) \nabla c_i(\bx^\toptone)} \leq \varepsilon_t.
\end{equation}
Using the triangle inequality $ \normtwo{\ba} - \normtwo{\bb} \leq \normtwo{\ba + \bb} $, the above inequality indicates that
$$
\normtwo{\sum_{i \in \mathcalE} c_i(\bx^\toptone) \nabla c_i(\bx^\toptone)} \leq \frac{1}{\sigma_t} \big(\varepsilon_t + \normtwobig{\nabla f(\bx^\toptone)}\big).
$$
Assuming  $ \{\bx^\toptzero\} $ converges to $ \bx^* $, as $ t \rightarrow\infty $, based on the continuity of $ f(\bx) $ and $ c_i(\bx) $, we have:
$$
\sum_{i \in \mathcalE} c_i(\bx^*) \nabla c_i(\bx^*) = 0.
$$
Since $ \nabla c_i(\bx^*), \forall i $ are linearly independent, it must hold that $ c_i(\bx^*) = 0, \,\forall i \in \mathcalE $. This indicates that $ \bx^* $ is a feasible point.

Next, we show that $ \bx^* $ satisfies the KKT conditions. For this, we need to construct the Lagrange multiplier vector $ \blambda^* = [\lambda_1^*, \lambda_2^*, \cdots, \lambda_{p}^*]^\top $, where $ p\triangleq \abs{\mathcalE} $ denotes the number of elements in $ \mathcalE $. Define:
$$
\nabla \bc(\bx) \triangleq [\nabla c_i(\bx)^\top]_{i \in \mathcalE} \in\real^{p\times n},\quad \text{with $n\geq p$},
$$
and
$$
\lambda_i^\toptzero \triangleq -\sigma_t c_i(\bx^\toptone),\,\forall i, \qquad \blambda^\toptzero \triangleq \big[\lambda_1^\toptzero, \lambda_2^\toptzero, \cdots, \lambda_{p}^\toptzero\big]^\top\in\real^p.
$$
Then the gradient of $\nabla f_{\sigma_t}(\bx^\toptone)$ in \eqref{equation:pen_cong_glo_full1} can be rewritten as:
$$
\nabla \bc(\bx^\toptone)^\top \blambda^\toptzero = \nabla f(\bx^\toptone) - \nabla f_{\sigma_t}(\bx^\toptone).
$$
Since $ \nabla c_i(\bx^*), \forall i $ are linearly independent,  $ \nabla \bc(\bx^*) $ is a full-rank matrix. It holds that $ \bx^\toptzero \rightarrow\bx^* $, when $ t $ is sufficiently large, $ \nabla c(\bx^\toptone) $ should also be a full-rank matrix. Therefore, we can use the generalized inverse of $ \nabla c(\bx^\toptone) $ to express $ \blambda^\toptzero $:
$$
\blambda^\toptzero = \big[\nabla \bc(\bx^\toptone) \nabla \bc(\bx^\toptone)^\top\big]^{-1} \nabla \bc(\bx^\toptone) 
\big[\nabla f(\bx^\toptone) - \nabla f_{\sigma_t}(\bx^\toptone)\big].
$$
Taking the limit of both sides as $ t \rightarrow\infty $ and noting that $ \nabla f_{\sigma_t}(\bx^\toptone) \rightarrow \bzero $, we have:
$$
\blambda^* \triangleq \lim_{t \rightarrow \infty} \blambda^\toptzero = \big(\nabla \bc(\bx^*)^\top \nabla \bc(\bx^*)\big)^{-1} \nabla \bc(\bx^*)^\top \nabla f(\bx^*).
$$
Substituting $ t \rightarrow \infty $ into the gradient expression \eqref{equation:pen_cong_glo_full1}, we get:
$$
\nabla f(\bx^*) - \nabla \bc(\bx^*) \blambda^* = \bzero .
$$
This indicates that the gradient condition in the KKT conditions is satisfied (see, for example, Theorem~\ref{theorem:kkt_licq}), and $ \blambda^* $ is the Lagrange multiplier corresponding to the optimal point $ \bx^* $.
\end{proof}


Although Theorem~\ref{theorem:pen_cong_glo_full} does not require each subproblem to be solved exactly, to obtain a solution to the original problem, the precision of the subproblem solutions must increase progressively. 

\subsection*{Numerical Stability}
We briefly analyze the numerical difficulties of Algorithm~\ref{alg:quad_pen_eq}. We know that to obtain a solution to the original problem, the penalty factor $ \sigma_t $ must tend to positive infinity. From the perspective of matrix condition numbers, as $ \sigma_t $ tends to positive infinity, the difficulty of solving the subproblem increases significantly. Consider the Hessian matrix of the penalty function $ f_{\sigma}(\bx) $:
\begin{equation}\label{equation:penl_hess}
\nabla^2 f_{\sigma}(\bx) = \nabla^2 f(\bx) + \sum_{i \in \mathcalE} \sigma \nabla c_i(\bx) \nabla^2 c_i(\bx) + \sigma \nabla \bc(\bx)^\top \nabla \bc(\bx)\in\real^{n\times n}.
\end{equation}
As $ \bx $ approaches the optimal point, according to the proof of Theorem~\ref{theorem:pen_cong_glo_full}, when $ \bx \approx \bx^* $, it should hold that 
$$ 
-\sigma \nabla c_i(\bx) \approx \lambda_i^*.
$$ 
Based on this approximation, we can use the Hessian matrix of the Lagrangian function $ L(\bx, \blambda^*) $ to approximate the first two terms on the right-hand side of \eqref{equation:penl_hess}:
\begin{equation}\label{equation:penl_hess2}
\nabla^2 f_{\sigma}(\bx) \approx \nabla^2 L(\bx, \blambda^*) + \sigma \nabla \bc(\bx)^\top \nabla \bc(\bx), 
\end{equation}
where $ \nabla \bc(\bx)^\top \nabla \bc(\bx) $ is  positive semidefinite  and singular, with $ (n - p) $ eigenvalues equal to 0. 
Note that the right-hand side of \eqref{equation:penl_hess2} contains two matrices: a constant matrix and a singular matrix whose largest eigenvalue tends to positive infinity. Intuitively, the condition number of the Hessian matrix $ \nabla^2 f_{\sigma}(\bx) $ will become increasingly larger, which means that the contour lines of the subproblem will become more dense, making it very difficult to solve using gradient-based algorithms. If Newton's method (Chapter~\ref{chapter:second_order}) is used, solving the Newton equations themselves becomes a very challenging problem. Therefore, in practical applications, we cannot allow the penalty factor to tend to positive infinity.


\subsection{Quadratic Penalty Function Method for General Constraints}

The previous section focused solely on optimization problems with equality constraints. We briefly discuss how to  design a quadratic penalty function for problems that include both equality and inequality constraints (i.e., general constrained optimization problems as in (P4) from \eqref{equation:pr_in_constchap}).

The primary distinction between equality-constrained optimization and general constrained optimization lies in the allowance for inequality constraints to be less than zero ($ c_i(\bx) < 0 $, $i\in\mathcalI$). 
If we were to use the traditional penalty function defined as $ \normtwo{\bc(\bx)}^2 $, it would penalize feasible points where $ c_i(\bx) < 0 $, which is undesirable. 
For problem (P4), we need to modify the existing quadratic penalty function to ensure it only penalizes points where  $ c_i(\bx) > 0 $, $i\in\mathcalI$, without penalizing feasible points.


\begin{definition}[Quadratic Penalty Function for Equality and Inequality Constraints]
For the general constrained optimization problem (P4), define the quadratic penalty function:
$$
f_{\sigma}(\bx) \triangleq f(\bx) + \frac{1}{2} \sigma \left[ \sum_{i \in \mathcalE} c_i^2(\bx) + \sum_{i \in \mathcalI} \widetildec_i^2(\bx) \right],
$$
where the second term on the right-hand side represents  the penalty term, $ \widetildec_i(\bx) $ is defined as
\begin{equation}\label{equation:pen_ineq_widetil}
\widetildec_i(\bx) \triangleq \max\{c_i(\bx), 0\},
\end{equation}
and $ \sigma > 0 $ is  the penalty factor.
\end{definition}

Note that the function $ h(t) \triangleq (\max\{t, 0\})^2 $ is differentiable with respect to $ t $. 
Therefore, the gradient of $f_{\sigma} $ exists, allowing the use of gradient-based methods discussed in Chapters~\ref{chapter:gradient-descent} and \ref{chapter:gd_convg} to solve the subproblem.
However, generally speaking, $ f_{\sigma} $ is not twice differentiable, making it unsuitable for direct application of second-order methods (such as Newton's method; see Chapter~\ref{chapter:second_order}). This limitation is a notable drawback of the quadratic penalty function for general constrained problems. The structure and algorithm of the penalty function method for solving general constrained problems are identical to Algorithm~\ref{alg:quad_pen_eq}, so detailed explanations are omitted here.




\section{Augmented Lagrangian Method}\label{section:aug_lag_method}


In the quadratic penalty function method, when $t\rightarrow \infty$, according to Theorem~\ref{theorem:pen_cong_glo_full}, we have:
\begin{equation}
c_i(\bx^\toptone) \approx -\frac{\lambda_i^*}{\sigma_t}, \quad \forall i \in \mathcalE.
\end{equation}
Therefore, to ensure feasibility, the penalty factor $\sigma_t$ must tend to positive infinity. 
However, this leads to difficulties in solving the subproblem due to the explosion of the condition number.
Therefore, there is a need to improve the quadratic penalty function method to achieve a feasible approximate optimal solution using a finite penalty factor. The augmented Lagrangian method is one such approach that addresses these limitations.

\subsection{Augmented Lagrangian Method for Equality Constraints}

\subsection*{Construction of the Augmented Lagrangian Function}

The \textit{augmented Lagrangian method} constructs an \textit{augmented Lagrangian function (ALF)} at each iteration, combining elements of both the Lagrangian function and the quadratic penalty function. Specifically, for the equality-constrained optimization problem (P4.1) given in \eqref{equation:pr_in_constchap_equ}, the augmented Lagrangian function is defined as:
\begin{equation}\label{equation:auglag_func1}
\begin{aligned}
L_\sigma(\bx, \blambda) 
&\triangleq f(\bx) + \sum_{i \in \mathcalE} \lambda_i c_i(\bx) + \frac{1}{2} \sigma \sum_{i \in \mathcalE} c_i^2(\bx)
= f(\bx) + \bc(\bx)^\top\blambda + \frac{1}{2} \sigma \bc(\bx)^\top\bc(\bx),
\end{aligned}
\end{equation}~\footnote{Since $\blambda\in\real^p$, it can be alternatively stated as $L_\sigma(\bx, \blambda) \triangleq f(\bx) - \bc(\bx)^\top\blambda + \frac{1}{2} \sigma \bc(\bx)^\top\bc(\bx)$ with a reverse sign of $\blambda$.}
which adds the quadratic penalty term to the Lagrangian function.
Here, $\bc(\bx)=\{c_i(\bx)\}_{i=1}^p\in\real^p$  is the vector function $ \bc: \real^n \rightarrow \real^p $, whose $ i $-th component is the $ i $-th constraint function $ c_i $. 
Note that the Jacobian $\bJ \triangleq\nabla \bc(\bx)\in\real^{p\times n}$.




\begin{remark}[Gradient and Hessian of ALF for Equation Constraints]
The gradient and Hessian of the augmented Lagrangian function \eqref{equation:auglag_func1} w.r.t. $\bx$ are given, respectively, by 
\begin{subequations}\label{equation:aug_lag_gra_hess}
\begin{align}
\nabla_{\bx} L_{\sigma}(\bx, \blambda) 
&= \nabla f(\bx) + \sum_{i \in \mathcalE} \big[\lambda_i + \sigma c_i(\bx)\big] \nabla c_i(\bx) \\
&= \nabla f(\bx) + \nabla \bc(\bx)^\top \blambda +  \sigma\nabla \bc(\bx)^\top \bc(\bx)  \qquad\qquad\qquad\qquad\qquad \\
&= \nabla_{\bx} L(\bx, \blambda)  +  \sigma \nabla \bc(\bx)^\top \bc(\bx);
\end{align}
\begin{align}
\nabla_{\bx}^2 L_{\sigma}(\bx, \blambda) 
&=\nabla^2 f(\bx) + \sum_{i \in \mathcalE} \lambda_i\nabla^2 c_i(\bx) 
+  \sum_{i \in \mathcalE}\sigma \left\{\nabla c_i(\bx) \cdot \nabla c_i(\bx)^\top + c_i(\bx)\nabla^2 c_i(\bx)  \right\} \nonumber\\
&=\nabla_{\bx}^2 L(\bx, \blambda) + \sigma \nabla \bc(\bx)^\top \nabla \bc(\bx) 
+\sum_{i \in \mathcalE}\sigma \left\{ c_i(\bx)\nabla^2 c_i(\bx)  \right\},
\end{align}
where $L(\bx,\blambda) =f(\bx) + \sum_{i \in \mathcalE} \lambda_i c_i(\bx)$ is the Lagrangian function.
\end{subequations}
\end{remark}

\paragrapharrow{Properties.}
Notice that the discrepancy introduced by the quadratic penalty term can be relaxed: If $ \blambda = \blambda^* $ is an optimal point of the Lagrangian function, then the first-order conditions in Theorem~\ref{theorem:kkt_licq} and the fact that $ \bc(\bx^*) = \bzero $ implies that $ \bx^* $ is a stationary point of $ L_{\sigma} $:
$$
\nabla_{\bx} L_{\sigma}(\bx^*, \blambda^*) = \bzero.
$$

On the other hand, as mentioned at the beginning of this section, while the penalty function method struggles with the explosion of the condition number as the penalty factor tends to positive infinity, the augmented Lagrangian function method offers a solution. As shown by  \citet{fletcher2000practical}, there exists a finite value $ \widehat{\sigma} $ such that if $ \sigma > \widehat{\sigma} $, then $ \bx^* $ is an unconstrained local minimizer of $ L_{\sigma}(\bx, \blambda^*) $, i.e.,
\begin{equation}\label{equation:finite_alf_sigma}
\text{if }\quad
\bx_{\blambda, \sigma} \triangleq \mathop{\argmin}_{\bx \in \real^n} L_{\sigma}(\bx, \blambda),
\quad\text{ then }\quad
\bx_{\blambda^*, \sigma} = \bx^* \text{ for all } \sigma > \widehat{\sigma};
\end{equation}~\footnote{In case of several local minimizers, $ \mathop{\argmin}_{\bx \in \real^n} $ is interpreted as the local unconstrained minimizer in the valley around $ \bx^* $.} see Theorem~\ref{theorem:finite_sigma_auglag}.
This means that the penalty parameter $ \sigma $ does not have to go to infinity. If $ \sigma $ is sufficiently large and if we insert $ \blambda^* $ (the vector of Lagrangian multipliers at the solution $ \bx^* $), then the unconstrained minimizer of the augmented Lagrangian function solves the constrained problem. Thus, the problem of finding $ \bx^* $ has been reduced  to that of finding $ \blambda^* $.




\paragrapharrow{Iterative method.}
The preceding discussion outlines an \textit{iterative method} for solving the augmented Lagrangian function problem. The core idea is to initially use the penalty term to approach the solution $ \bx^* $, followed by using the Lagrangian term to achieve final convergence as $ \blambda $ approaches  $ \blambda^* $. A rough sketch of the algorithm is provided below:
\begin{tcolorbox}[colback=white,colframe=black]
	\begin{minipage}{0.6\textwidth}
	\qquad\qquad\qquad
\begin{enumerate}
	\item Choose initial values for $ \blambda $ and $ \sigma $;
	\item repeat until stopping criteria satisfied
	\begin{itemize}
		\item Compute $ \bx_{\blambda, \sigma} = \mathop{\argmin}_{\bx \in \real^n} L_{\sigma}(\bx, \blambda)$;
		\item Update $ \blambda $ and $ \sigma $;
	\end{itemize}
\end{enumerate}
	\end{minipage}
\end{tcolorbox}

Next, we introduce two algorithms designed for solving the augmented Lagrangian method applied to problem (P4.1) in \eqref{equation:pr_in_constchap_equ}. The first algorithm aligns the optimality conditions at each iteration with those of the optimal point. In contrast, the second algorithm updates the multipliers using \textit{steepest ascent directions}.



\begin{algorithm}[h] 
\caption{Augmented Lagrangian Method for (P4.1) in \eqref{equation:pr_in_constchap_equ}}
\label{alg:aug_lag_method}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$ and a set of equality constraints $\{c_i(\bx)\}, i\in\mathcalE$; 
\State {\bfseries Input:}  Choose initial point $ \bx^{(1)} $, multiplier $ \blambda^{(1)} $, penalty factor $ \sigma_1 > 0 $, penalty factor update constant $ \rho \in [2, 10] $, constraint violation tolerance $ \varepsilon > 0 $, and precision requirement $ \gamma_t > 0 $. 
\For{$t=1,2,\ldots$}
\State   \algoalign{With $ \bx^\toptzero $ as the initial point, solve $ \mathopmin{\bx}  L_{\sigma_t}(\bx, \blambda^\toptzero)$,
to obtain a solution $ \bx^\toptone $ satisfying the precision condition 
$\normtwobig{\nabla_{\bx} L_{\sigma_t}(\bx^\toptone, \blambda^\toptzero)} \leq \gamma_t$.
}
\If{$ \normtwobig{\bc(\bx^\toptone)} \leq \varepsilon $}
\State Return approximate solution $ \bx^\toptone, \blambda^\toptzero $, terminate iteration.
\EndIf
\State Update multipliers: $ \blambda^\toptone \leftarrow \blambda^\toptzero + \sigma_t \bc(\bx^\toptone) $.
\State Update penalty factor: $ \sigma_{t+1} \leftarrow \rho \sigma_t $.
\EndFor
\State {\bfseries Return:}  final $\bx \leftarrow \bx^\toptzero$;
\end{algorithmic} 
\end{algorithm}
\paragrapharrow{First algorithm.}
In the initial iteration steps we keep $ \blambda $ constant (e.g., $ \blambda = \bzero $) and let the penalty factor $ \sigma $ increase. 
This approach should bring us close to $ \bx^* $, as described in Section~\ref{section:pen_equa} for penalty function methods.


Specifically, in the $t$-th iteration, given the penalty factor $\sigma_t$ and the multiplier $\blambda^\toptzero$, the minimum point $\bx^\toptone$ of the augmented Lagrangian function $L_{\sigma_t}(\bx, \blambda^\toptzero)$ satisfies
\begin{equation}\label{equation:alf_ite1}
\nabla_{\bx} L_{\sigma_t}(\bx^\toptone, \blambda^\toptzero) = \nabla f(\bx^\toptone) 
+ \sum_{i \in \mathcalE} \big[\lambda_i^\toptzero + \sigma_t c_i(\bx^\toptone)\big] \nabla c_i(\bx^\toptone) = \bzero.
\end{equation}
By the KKT conditions (Theorem~\ref{theorem:kkt_licq}), for the optimization problem (P4.1), its optimal solution $\bx^*$ and the corresponding multipliers $\blambda^*$ must satisfy
\begin{equation}\label{equation:alf_ite2}
\nabla f(\bx^*) + \sum_{i \in \mathcalE} \lambda_i^* \nabla c_i(\bx^*) = \bzero.
\end{equation}

By the assumption that $\bx^\toptzero$ and $\bx^\toptone$ are close to $\bx^*$ using penalty function method in the beginning iterations,
comparing \eqref{equation:alf_ite1} and \eqref{equation:alf_ite2} and matching the optimality condition,
to ensure that the iterates generated by the augmented Lagrangian method converge to $ \bx^* $, we need to ensure consistency at the optimal solution. Therefore, for sufficiently large $ t $,
\begin{equation}\label{equation:aug_lag_upd1}
\lambda_i^* \approx \lambda_i^\toptzero + \sigma_t c_i(\bx^\toptone) 
\quad\iff\quad 
c_i(\bx^\toptone) \approx \frac{1}{\sigma_t} (\lambda_i^* - \lambda_i^\toptzero),\quad \forall i \in \mathcalE.
\end{equation}
Thus, when $ \lambda_i^\toptzero $ is sufficiently close to $ \lambda_i^* $, the constraint violation at $ \bx^\toptone $ will be much smaller than $ \frac{1}{\sigma_t} $. The augmented Lagrangian method can effectively reduce the constraint violation by updating the multipliers. Equation~\eqref{equation:aug_lag_upd1} suggests an effective update rule for the multipliers at the $t$-th iteration:
\begin{equation}\label{equation:aug_lag_upd_multi}
\lambda_i^\toptone \leftarrow  \lambda_i^\toptzero + \sigma_t c_i(\bx^\toptone), \quad \forall i \in \mathcalE.
\end{equation}
The augmented Lagrangian method for problem (P4.1) is thus presented in Algorithm~\ref{alg:aug_lag_method}. 
Note that $ \bc(\bx) = [c_i(\bx)]_{i \in \mathcalE} \in\real^p$ and 
$
\nabla \bc(\bx) = [\nabla c_i(\bx)]_{i \in \mathcalE} \in\real^{p\times n}.
$

As the penalty factor $\sigma_t$ increases, the condition number of the Hessian matrix $L_{\sigma_t}(\bx, \blambda^\toptzero)$ with respect to $\bx$ also increases, thereby making it more difficult to solve for the iterate point $\bx^\toptone$. However, when $\sigma_t$ and $\sigma_{t+1}$ are close, $\bx^\toptzero$ can serve as an initial point for solving $\bx^\toptone$, thus accelerating convergence. Therefore, the penalty factor $\sigma_t$ cannot increase too quickly. But if $\sigma_t$ increases too slowly, the sequence of iterates $\{\bx^\toptzero\}$ will converge to the solution of the original problem more slowly. In practice, we need to pay attention to the choice of the parameter $\rho$, and a common empirical value is $\rho \in [2, 10]$.

\paragrapharrow{Second algorithm: steepest ascent or Newton's methods.}
As discussed previously, in the initial iteration steps, we keep $ \blambda $ constant (e.g., $ \blambda = \bzero $) and let the penalty factor $ \sigma $ to increase. This should lead us close to $ \bx^* $ as described for penalty function methods in Section~\ref{section:pen_equa}.

Next, we fix $\sigma=\sigma_{\text{fix}}$ and vary the multipliers $\blambda$. Then, for this fixed penalty factor,
defining 
\begin{subequations}
\begin{align}
\bx_{\blambda} &\triangleq \arg\min_{\bx \in \real^n} L_{\sigma_{\text{fix}}}(\bx, \blambda);\\
\psi(\blambda) &\triangleq L_{\sigma_{\text{fix}}}(\bx_{\blambda}, \blambda) = \min_{\bx \in \real^n} L_{\sigma_{\text{fix}}}(\bx, \blambda),
\end{align}
\end{subequations}
where the latter is a function of $ \blambda $ alone. 
As discussed previously, for the finite value $ \widehat{\sigma}$, we assume $ \sigma_{\text{fix}} > \widehat{\sigma} $. Since
\begin{enumerate}[(i)]
\item $ \psi(\blambda) $ is the minimal value of $ L_{\sigma_{\text{fix}}} $,
\item the definition of the augmented Lagrangian function \eqref{equation:auglag_func1} combined with $ \bc(\bx^*) = \bzero $ shows that $ L_{\sigma}(\bx^*, \blambda) = f(\bx^*) $ for any $ (\blambda, \sigma) $,
\item \eqref{equation:finite_alf_sigma} implies $ \bx_{\blambda^*} = \bx^* $, where $\bx_{\blambda^*} = \arg\min_{\bx \in \real^n} L_{\sigma_{\text{fix}}}(\bx, \blambda^*),$
\end{enumerate}
it follows that for any $ \blambda $
$$
\psi(\blambda) \leq L_{\sigma_{\text{fix}}}(\bx^*, \blambda) = L_{\sigma_{\text{fix}}}(\bx^*, \blambda^*) = \psi(\blambda^*). 
$$
Thus, the Lagrangian multipliers at the solution is a local maximizer for $ \psi $; that is, 
$$
\blambda^* = \arg\max_{\blambda} \psi(\blambda). 
$$

Therefore, using the function $\psi(\blambda)$, in the $t$-th iteration, starting from the current $ \blambda^\toptzero $, we seek an (ascending) step $ \bs^\toptzero $ such that $ \blambda^\toptzero + \bs^\toptzero \approx \blambda^* $.
Similar to the steep descent method in Section~\ref{section:gradient-descent-all}, we can obtain the \textit{steepest ascent step} (the positive gradient direction) as 
\begin{equation}\label{equation:aug_lag_up_secondupdate}
\bs^\toptzero = \eta_t \nabla \psi(\blambda^\toptzero) =\eta_t \bc(\bx_{\blambda^\toptzero}), \quad \eta_t > 0.
\end{equation}
This is equivalent to the update from the first algorithm in \eqref{equation:aug_lag_upd1}, except that we use a learning rate $\eta_t$, rather than the penalty factor $\sigma_t$ to guide the step.
\citet{fletcher2000practical} shows that under certain regularity assumptions, \eqref{equation:aug_lag_upd1} or \eqref{equation:aug_lag_up_secondupdate}   provides linear convergence. Faster convergence can be achieved by applying Newton's method (Chapter~\ref{chapter:second_order}) to the nonlinear problem $ \nabla \psi(\blambda^\toptzero) = \bzero $,
\begin{equation}\label{equation:aug_lag_up_thirddate}
\blambda^* \approx \blambda + \bs^\toptzero, \quad \text{where } \nabla^2\psi(\blambda^\toptzero) \bs^\toptzero = \nabla \psi(\blambda^\toptzero).
\end{equation}
The procedures are analogous to those in Algorithm~\ref{alg:aug_lag_method}, and we shall not repeat the details.











\subsection*{Convergence}

The augmented Lagrangian function, as a type of penalty function, naturally raises questions about the relationship between its local minimizers and those of the original problem (P4.1). 
Suppose $\bx^*$, $\blambda^*$ are the local minimizer and corresponding multiplier of the equality-constrained problem (P4.1), and the second-order sufficient conditions hold ($\nabla^2 f(\bx^*) \succ \bzero$ by Theorem~\ref{theorem:second_suff_loca_int}). 
It can be shown that, given $\blambda^*$, for sufficiently large $\sigma$, $\bx^*$ is also a strict local minimizer of the augmented Lagrangian function $L_\sigma(\bx, \blambda^*)$. When $\blambda^*$ is unknown, for $\blambda$ sufficiently close to $\blambda^*$ and sufficiently large $\sigma$, the local minimizer of the augmented Lagrangian function $L_\sigma(\bx, \blambda)$ will be sufficiently close to $\bx^*$. This also shows that the augmented Lagrangian function is an  penalty function under certain conditions.

\begin{theoremHigh}\label{theorem:finite_sigma_auglag}
Let $\bx^*, \blambda^*$ be a local minimizer and the corresponding multiplier of problem (P4.1), and suppose the linear independence constraint qualification (LICQ, Definition~\ref{definition:licq}) and second-order sufficient conditions hold at $\bx^*$ ($\nabla^2 f(\bx^*) \succ \bzero$ by Theorem~\ref{theorem:second_suff_loca_int}). Then, there exists a finite constant $\widehat{\sigma}$ such that for any $\sigma \geq \widehat{\sigma}$, $\bx^*$ is a strict local minimizer of $L_\sigma(\bx, \blambda^*)$. Conversely, if $\bx^*$ is a local minimizer of $L_\sigma(\bx, \blambda^*)$ and satisfies $c_i(\bx^*) = 0$, $i \in \mathcalE$, then $\bx^*$ is a local minimizer of problem (P4.1).
\end{theoremHigh}
\begin{proof}
Since $\bx^*$ is a local minimizer of problem (P4.1) and the second-order sufficient conditions hold, we have
\begin{equation}\label{equation:opt_aug_theo1}
\small
\begin{aligned}
\nabla_{\bx} L(\bx^*, \blambda^*) &= \nabla f(\bx^*) + \sum_{i \in \mathcalE} \lambda_i^* \nabla c_i(\bx^*) = \bzero,\\
\bu^\top \nabla_{\bx}^2 L(\bx^*, \blambda^*) \bu &= \bu^\top \left( \nabla^2 f(\bx^*) + \sum_{i \in \mathcalE} \lambda_i^* \nabla^2 c_i(\bx^*) \right) \bu > 0, \quad \forall \bu \text{ satisfying } \nabla \bc(\bx^*) \bu = \bzero.
\end{aligned}
\end{equation}
Based on these conditions, we prove that $\bx^*$ is optimal for $L_\sigma(\bx, \blambda^*)$. Since $c_i(\bx^*) = 0$, $i \in \mathcalE$, by \eqref{equation:aug_lag_gra_hess}, we have
$$
\begin{aligned}
\nabla_{\bx} L_\sigma(\bx^*, \blambda^*) &= \nabla_{\bx} L(\bx^*, \blambda^*) = \bzero,\\
\nabla_{\bx}^2 L_\sigma(\bx^*, \blambda^*) &= \nabla_{\bx}^2 L(\bx^*, \blambda^*) + \sigma \nabla \bc(\bx^*)^\top \nabla \bc(\bx^*).
\end{aligned}
$$
For sufficiently large $\sigma$, it can be shown that
$$
\nabla_{\bx}^2 L_\sigma(\bx^*, \blambda^*) \succ \bzero.
$$
In fact, letting the penalty factor be positive integers $\sigma=t$, $t =1, 2, \ldots$, there exists $\bu^\toptzero$ such that $\normtwo{\bu^\toptzero} = 1$ and
$$
\bu^\toptzeroTOP \nabla_{\bx}^2 L_\sigma(\bx^*, \blambda^*) \bu^\toptzero = \bu^\toptzeroTOP \nabla_{\bx}^2 L(\bx^*, \blambda^*) \bu^\toptzero + t \normtwo{\nabla c(\bx^*) \bu^\toptzero}^2 \leq 0,
$$
then
$$
\normtwo{\nabla c(\bx^*) \bu^\toptzero}^2 \leq -\frac{1}{t} \bu^\toptzeroTOP \nabla_{\bx}^2 L(\bx^*, \blambda^*) \bu^\toptzero \to 0, \quad t \to \infty.
$$
Since $\{\bu^\toptzero\}$ is a bounded sequence, there must exist a limit point, denoted by $\bu$. Then,
$$
\nabla \bc(\bx^*) \bu = \bzero, \quad \bu^\top \nabla_{\bx}^2 L(\bx^*, \blambda^*) \bu \leq 0,
$$
which contradicts \eqref{equation:opt_aug_theo1}. Therefore, there exists a finite $\widehat{\sigma}$ such that when $\sigma \geq \widehat{\sigma}$,
$$
\nabla_{\bx}^2 L_\sigma(\bx^*, \blambda^*) \succ 0,
$$
and thus $\bx^*$ is a strict local minimizer of $L_\sigma(\bx, \blambda^*)$ by Theorem~\ref{theorem:second_suff_loca_int}.

Conversely, if $\bx^*$ satisfies $c_i(\bx^*) = 0$ and is a local minimizer of $L_\sigma(\bx, \blambda^*)$, then for any feasible point $\bx$ sufficiently close to $\bx^*$, we have
$$
f(\bx^*) = L_\sigma(\bx^*, \blambda^*) \leq L_\sigma(\bx, \blambda^*) = f(\bx).
$$
Therefore, $\bx^*$ is a local minimizer of problem (P4.1).
\end{proof}

For Algorithm~\ref{alg:aug_lag_method}, by further assuming the boundedness of the multiplier sequence and the constraint qualifications at the limit points, we can prove that the sequence $\{\bx^\toptzero\}$ generated by the algorithm has a subsequence converging to a first-order stable point of problem (P4.1).

\begin{theoremHigh}[Convergence of the Augmented Lagrangian Method \citep{liu2020optimization}]\label{theorem:conv_aug_lag_bounded}
Assume the multiplier sequence $\{\blambda^\toptzero\}$ is bounded, the penalty factor $\sigma_t \to +\infty$ as $t \to \infty$, and the precision requirement $\gamma_t \to 0$ in Algorithm~\ref{alg:aug_lag_method}. Suppose a subsequence $\{\bx^{(t_j)}\}$ of $\{\bx^{(t)}\}_{t>0}$ converges to $\bx^*$, and the linear independence constraint qualification (LICQ) holds at $\bx^*$. Then there exists $\blambda^*$ such that
$$
\blambda^{(t_j+1)} \to \blambda^*, \quad j \to \infty,
$$
$$
\nabla f(\bx^*) + \nabla \bc(\bx^*)^\top \blambda^* = \bzero, \quad \bc(\bx^*) = \bzero.
$$

\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:conv_aug_lag_bounded}]
By \eqref{equation:aug_lag_gra_hess} and the update rule of the multipliers in \eqref{equation:aug_lag_upd_multi}, the augmented Lagrangian function $ L_{\sigma_t}(\bx, \blambda^\toptzero) $ admits the gradient
$$
\begin{aligned}
	\nabla_{\bx} L_{\sigma_t}(\bx^\toptone, \blambda^\toptzero) 
	&= \nabla f(\bx^\toptone) + \big(\nabla \bc(\bx^\toptone)\big)^\top\big(\blambda^\toptzero + \sigma_t \bc(\bx^\toptone)\big)\\
	&= \nabla f(\bx^\toptone) + \big(\nabla \bc(\bx^\toptone)\big)^\top \blambda^\toptone = \nabla_{\bx} L(\bx^\toptone, \blambda^\toptone),
\end{aligned}
$$
where  $\nabla \bc(\bx)\in\real^{p\times n}$ for any $\bx\in\real^n$.
Then, for any $ t_j $ such that $ \rank(\nabla \bc(\bx^{(t_j+1)})) = p = |\mathcalE| $ (which holds when $ \bx^{(t_j+1)} $ is sufficiently close to $ \bx^* $; by assumption that the LICQ is satisfied at the local optimal point $ \bx^* $), we have 
$$
\blambda^{(t_j+1)} = \big[\nabla \bc(\bx^{(t_j+1)}) \nabla \bc(\bx^{(t_j+1)})^\top\big]^{-1} \nabla \bc(\bx^{(t_j+1)})
\left( \nabla_{\bx} L_{\sigma_t}(\bx^{(t_j+1)}, \blambda^{(t_j)}) - \nabla f(\bx^{(t_j+1)}) \right).
$$
Since $ \normtwo{\nabla_{\bx} L_{\sigma_t}(\bx^{(t_j+1)}, \blambda^{(t_j)})} \leq \gamma_{t_j} \to 0 $ by assumption, we have
$$
\blambda^{(t_j+1)} \to \blambda^* \triangleq - (\nabla \bc(\bx^*)^\top \nabla \bc(\bx^*))^{-1} \nabla \bc(\bx^*)^\top \nabla f(\bx^*)
$$
and
$
\nabla_{\bx} L(\bx^*, \blambda^*) = \bzero.
$
Since $ \{ \blambda^\toptzero \} $ is bounded by assumption and $ \blambda^{(t_j)} + \sigma_{t_j} c(\bx^{(t_j+1)}) \to \blambda^* $, it follows that
$
\{ \sigma_{t_j} \bc(\bx^{(t_j+1)}) \}
$   is bounded.
Since  $ \sigma_t \to +\infty $, then
$
\bc(\bx^*) = \bzero,
$ which completes the proof.
\end{proof}




\subsection{Augmented Lagrangian Method for General Constraints}
For a general constrained optimization problem (P4) in \eqref{equation:pr_in_constchap},
we can also define its augmented Lagrangian function and design the corresponding augmented Lagrangian method. 
We introduce two solutions: the activate set method and the method using slack variables.


%In the definition of the Lagrangian function, it is often preferred to keep simple constraints (such as non-negativity constraints, box constraints, etc.) and introduce multipliers for complex constraints. Here, for optimization problems with inequality constraints, we first introduce slack variables to convert inequality constraints into equality constraints and simple non-negativity constraints, and then add a quadratic penalty function for the remaining non-negativity constraint form of the Lagrangian function to construct the augmented Lagrangian function.


\subsection*{An Easy Solution: Active Set Method}
A straightforward approach to solving this problem involves using the method just described: Employ a penalty function method to bring us close to a solution, then consider only the active or nearly active constraints as equality constraints. Discard the remaining constraints (though continue monitoring them to ensure they remain inactive) and use one of two methods for updating the vector of Lagrange multipliers $ \blambda $ discussed in the previous section.

The augmented Lagrangian function can be expressed as:
$$
\widetilde{L}_{\sigma}(\bx, \blambda) = f(\bx) + \blambda^\top \bp(\bx) + \frac{1}{2} \sigma \bp(\bx)^\top \bp(\bx),
$$
where $ \bp(\bx) $ is defined as follows
$$
p_i(\bx) \triangleq 
\begin{cases} 
	c_i(\bx), & \text{if } i \in \sA_\delta(\bx); \\
	0, & \text{otherwise},
\end{cases}
$$
where the approximate active set $ \sA_{\delta}(\bx) $ is given by
$$
\sA_{\delta}(\bx) \triangleq \mathcalE \cup \{i \mid i\in \mathcalI \text{ and } c_{i}(\bx) \leq \delta\}, 
$$
where $ \delta $ is a small positive number. Initially we could keep $ \blambda = \bzero $ and increase $ \sigma_t $ until the approximate active set seems to have stabilized (e.g., remains constant for two consecutive iterations). As long as $ \sA_\delta(\bx) $ remains unchanged, we update $ \blambda $ using \eqref{equation:aug_lag_upd1} or \eqref{equation:aug_lag_up_thirddate} (discarding inactive constraints and assuming that the active inequality constraints are numbered first). Otherwise, $ \blambda $ is set to $ \bzero$ and $ \sigma_t $ is increased.

Various  alternatives for defining the active set could be considered; for example, we can determine the activate set  based on the values of $ \abs{c_{i}(\bx)}, i = 1, \ldots, m $. 
However, one drawback of this approach is that it requires specifying a threshold value, such as $ \delta $, which must be provided by the user.
The algorithm is outlined in Algorithm~\ref{alg:aug_lag_method_easy}.
\begin{algorithm}[h]
\caption{Augmented Lagrangian Method for (P4) in \eqref{equation:pr_in_constchap} (An Easy Solution)} 
\label{alg:aug_lag_method_easy}
\begin{algorithmic}[1]
\Require A function $f(\bx)$ and a set of equality constraints $\{c_i(\bx)\}, i\in\mathcalE \cup\mathcalI$; 
\State {\bfseries Input:}  Choose initial point $ \bx^{(1)} $, multiplier $ \blambda^{(1)} =\bzero $, penalty factor $ \sigma_1 > 0 $, penalty factor update constant $ \rho \in [2, 10] $, activate set tolerance $\delta$. 
\For{$t=1,2,\ldots$}
\State $ \bx^\toptone \leftarrow \argmin_{\bx} \widetilde{L}_{\sigma_t}(\bx, \blambda^\toptzero) $
\If{the active set $ \sA_{\delta}(\bx^\toptone) $ is stable compared to $ \sA_{\delta}(\bx^\toptzero) $}
\State $ \blambda^\toptone \leftarrow $  using \eqref{equation:aug_lag_upd1} or \eqref{equation:aug_lag_up_thirddate};
\State $ \sigma_{t+1} \leftarrow   \sigma_t $;
\Else
\State $ \blambda^\toptone \leftarrow \bzero $;
\State $ \sigma_{t+1} \leftarrow \rho  \sigma_t $;
\EndIf
\EndFor
\State {\bfseries Return:} final  $\bx \leftarrow \bx^\toptzero$;
\end{algorithmic}
\end{algorithm}



\subsection*{A Better Solution with Slack Variables}

Alternatively, for problem (P4) in \eqref{equation:pr_in_constchap}, by introducing slack variables, we can obtain the following equivalent form:
\begin{equation}\label{equation:pr_in_constchap_augfull}
\textbf{(P4.2)}:\qquad 
\begin{aligned}
& \underset{\bx\in\real^n,\bs \geq \bzero}{\min} & & f(\bx), \\
& \text{s.t.} & & c_i(\bx) = 0, \quad \,\,&i&\in\mathcalE,\,\,\text{with $\abs{\mathcalE}=p$}, \\
& & & c_i(\bx) + s_i = 0,  \,\,&i&\in\mathcalI,\,\,\text{with $\abs{\mathcalI}=q$}, \\
& & & s_i \geq 0, \quad &i& \in \mathcalI.
\end{aligned}
\end{equation}
Keeping the nonnegativity constraints, we can construct the Lagrangian function as follows:
$$
L(\bx, \bs, \blambda, \bmu) = f(\bx) + \sum_{i \in \mathcalE} \lambda_i c_i(\bx) + \sum_{i \in \mathcalI} \mu_i (c_i(\bx) + s_i), \quad s_i \geq 0, i \in \mathcalI.
$$
Let the quadratic penalty function for the equality constraints in problem \eqref{equation:pr_in_constchap_augfull} be $ g(\bx, \bs) $, then
$$
g(\bx, \bs) = \sum_{i \in \mathcalE} c_i^2(\bx) + \sum_{i \in \mathcalI} (c_i(\bx) + s_i)^2.
$$
We construct the augmented Lagrangian function as follows:
\begin{equation}\label{equation:pr_in_constchap_augfull2}
L_\sigma(\bx, \bs, \blambda, \bmu) = f(\bx) + \sum_{i \in \mathcalE} \lambda_i c_i(\bx) + \sum_{i \in \mathcalI} \mu_i (c_i(\bx) + s_i) + \frac{\sigma}{2} g(\bx, \bs), \; s_i \geq 0, i \in \mathcalI,
\end{equation}
where $\sigma$ is the penalty factor.
For optimizing the augmented Lagrangian function in \eqref{equation:pr_in_constchap_augfull2}, we again consider the alternating descent framework: alternately update $\bx,\bs$ and $\blambda$.
\paragrapharrow{Given multipliers, optimizing primal variables.}
To be more specific, in the $t$-th iteration, given multipliers $\blambda^\toptzero$, $\bmu^\toptzero$ and penalty factor $\sigma_t$, we need to solve the following problem:
\begin{equation}\label{equation:aug_lag_method_full}
\bx^\toptone, \bs^\toptone = \mathop{\argmin}_{\bx\in\real^n,\bs \geq \bzero} \quad L_{\sigma_t}(\bx, \bs, \blambda^\toptzero, \bmu^\toptzero), \quad \text{s.t.} \quad \bs \geq \bzero
\end{equation}
to obtain $\bx^\toptone, \bs^\toptone$. An effective method to solve problem \eqref{equation:aug_lag_method_full} is the projected gradient method (see Section~\ref{section:pgd}). Another method is to eliminate $\bs$ and solve the optimization problem only in terms of $\bx$. Specifically, fixing $\bx$, the subproblem for $\bs$ can be expressed as
$$
\bs^\toptone = \mathop{\argmin}_{\bs \geq \bzero} \quad \sum_{i \in \mathcalI} \mu_i^\toptzero (c_i(\bx) + s_i) + \frac{\sigma_t}{2} \sum_{i \in \mathcalI} (c_i(\bx) + s_i)^2.
$$
According to the property of quadratic functions, $\bs^\toptone$ is a global optimal solution to the above problem if and only if
\begin{equation}\label{equation:aug_lag_slacksol}
s_i^\toptone = \max \left\{ -\frac{\mu_i^\toptzero}{\sigma_t} - c_i(\bx), 0 \right\}, \quad i \in \mathcalI.
\end{equation}
Substituting the expression for $s_i$ into $L_{\sigma_t}$, we have
\begin{equation}\label{equation:auglag_gen_itet}
\small
\begin{aligned}
L_{\sigma_t}(\bx, \blambda^\toptzero, \bmu^\toptzero) 
%&=  f(\bx) + \sum_{i \in \mathcalE} \lambda_i c_i(\bx) + \frac{\sigma_t}{2} \sum_{i \in \mathcalE} c_i^2(\bx) 
%+ \frac{\sigma_t}{2} \sum_{i \in \mathcalI} \bigg( \max \bigg\{ \frac{\mu_i}{\sigma_t} + c_i(\bx), 0 \bigg\}^2 - \frac{\mu_i^2}{\sigma_t^2} \bigg)\\
&=  f(\bx) + \sum_{i \in \mathcalE} \Big(\lambda_i c_i(\bx) + \frac{\sigma_t}{2} c_i^2(\bx) \Big)
+ \frac{\sigma_t}{2} \sum_{i \in \mathcalI} \Big( \max \Big\{ \frac{\mu_i^\toptzero}{\sigma_t} + c_i(\bx), 0 \Big\}^2 - \frac{(\mu_i^\toptzero)^2}{\sigma_t^2} \Big)
\end{aligned}
\end{equation}
which is obtained by $\max\Big\{L_{\sigma_t}(\bx, s_i=0, \blambda^\toptzero, \bmu^\toptzero), L_{\sigma_t}(\bx, s_i=-\frac{\mu_i^\toptzero}{\sigma_t} - c_i(\bx), \blambda^\toptzero, \bmu^\toptzero) \Big\}$ and is a continuously differentiable function of $\bx$ (if $f(\bx), c_i(\bx), i \in \mathcalI \cup \mathcalE$ are continuously differentiable). Therefore, problem \eqref{equation:aug_lag_method_full} is equivalent to
$$
\bx^\toptone=\mathop{\argmin}_{\bx \in \real^n} \quad L_{\sigma_t}(\bx, \blambda^\toptzero, \bmu^\toptzero),
$$
and can be solved using  gradient-based methods (Chapters~\ref{chapter:gradient-descent} and \ref{chapter:gd_convg}). The advantage of this approach is that we eliminate the variable $\bs$, thereby solving the problem in a lower-dimensional space $\real^n$ (the decision space of problem \eqref{equation:aug_lag_method_full} is $\real^{n + \abs{\mathcalI}}$).

\paragrapharrow{Given primal variables, optimizing multipliers.}
For problem (P4.2) in \eqref{equation:pr_in_constchap_augfull}, its optimal solution $\bx^*, \bs^*$ and multipliers $\blambda^*, \bmu^*$ must satisfy the KKT conditions:
\begin{equation}\label{equation:aug_lag_full_kkt_pri}
\text{(KKT of \eqref{equation:pr_in_constchap_augfull})}\qquad 	
\begin{aligned}
\bzero &= \nabla f(\bx^*) + \sum_{i \in \mathcalE} \lambda_i^* \nabla c_i(\bx^*) + \sum_{i \in \mathcalI} \mu_i^* \nabla c_i(\bx^*),\\
\mu_i^* &\geq 0, \quad i \in \mathcalI,\\
s_i^* &\geq 0, \quad i \in \mathcalI.
\end{aligned}
\end{equation}
The optimal solution $\bx^\toptone, \bs^\toptone$ of problem \eqref{equation:aug_lag_method_full} satisfies:
\begin{equation}\label{equation:aug_lag_full_kkt_aug}
\text{(KKT of \eqref{equation:aug_lag_method_full})}\qquad
\begin{aligned}
\bzero 
&= \nabla f(\bx^\toptone) + \sum_{i \in \mathcalE} \left( \lambda_i^\toptzero + \sigma_t c_i(\bx^\toptone) \right) \nabla c_i(\bx^\toptone) \\
&\quad + \sum_{i \in \mathcalI} \left( \mu_i^\toptzero + \sigma_t \big(c_i(\bx^\toptone) + s_i^\toptone\big) \right) \nabla c_i(\bx^\toptone),\\
s_i^\toptone &= \max \Big\{ -\frac{\mu_i^\toptzero}{\sigma_t} - c_i(\bx^\toptone), 0 \Big\}, \quad i \in \mathcalI.
\end{aligned}
\end{equation}
To ensure that the iterates generated by the augmented Lagrangian method converge to $\bx^*, \bs^*$, we need to ensure consistency at the optimal solution. Therefore, for sufficiently large $t$, 
by comparing the KKT conditions of problems \eqref{equation:pr_in_constchap_augfull} and \eqref{equation:aug_lag_method_full} in \eqref{equation:aug_lag_full_kkt_pri} and \eqref{equation:aug_lag_full_kkt_aug}, respectively, it is easy to derive the update rules for the multipliers:
$$
\begin{aligned}
\lambda_i^* &\approx \lambda_i^\toptzero + \sigma_t c_i(\bx^\toptone) 
\;&\implies&  \; 
\lambda_i^\toptone \leftarrow \lambda_i^\toptzero + \sigma_t c_i(\bx^\toptone), \quad i \in \mathcalE,\\
\mu_i^* &\approx\mu_i^\toptzero + \sigma_t \big(c_i(\bx^\toptone) + s_i^\toptone\big)
\;&\implies&  \; 
\mu_i^\toptone \leftarrow \max \left\{ \mu_i^\toptzero + \sigma_t c_i(\bx^\toptone), 0 \right\}, \quad i \in \mathcalI.
\end{aligned}
$$
For equality constraints, the constraint violation could be  defined as:
$$
\nu_t(\bx^\toptone) \triangleq \sqrt{\sum_{i \in \mathcalE} c_i^2(\bx^\toptone) + \sum_{i \in \mathcalI} \left( c_i(\bx^\toptone) + s_i^\toptone \right)^2}.
$$
Based on \eqref{equation:aug_lag_slacksol}, eliminating $\bs$, the constraint violation becomes:
$$
\nu_t(\bx^\toptone) \triangleq \sqrt{\sum_{i \in \mathcalE} c_i^2(\bx^\toptone) + \sum_{i \in \mathcalI} \max \left\{ c_i(\bx^\toptone) , \frac{\mu_i^\toptzero}{\sigma_t} \right\}^2}.
$$

Using these definitions and update rules, we present the augmented Lagrangian method for constrained optimization problem \eqref{equation:pr_in_constchap_augfull} in Algorithm~\ref{alg:aug_lag_method_full}. This algorithm is similar to Algorithm~\ref{alg:aug_lag_method}. After each iteration, the algorithm checks if the constraint violation $\nu_t(\bx^\toptone)$ meets the precision requirement. If it does, the multipliers are updated and the subproblem solution accuracy is increased, keeping the penalty factor unchanged. If it does not, the multipliers are not updated, and the penalty factor is appropriately increased to obtain a smaller constraint violation.

\begin{algorithm}[h] 
\caption{Augmented Lagrangian Method for (P4.2) in \eqref{equation:pr_in_constchap_augfull}}
\label{alg:aug_lag_method_full}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$ and a set of constraints $\{c_i(\bx)\}, i\in\mathcalI\cup\mathcalE$; 
\State {\bfseries Input:}  Choose initial point $ \bx^{(1)} $, multiplier $ \blambda^{(1)}, \bmu^{(1)} $, penalty factor $ \sigma_1 > 0 $, constants $ 0 < \alpha \leq \beta \leq 1 $,
 penalty factor update constant $ \rho > 1 $, the global constraint violation tolerance $ \varepsilon > 0 $ and the initial local constraint violation tolerance $\varepsilon_1 = \frac{1}{\sigma_1^\alpha}$, and the global precision requirement $\gamma$ and the initial  local precision requirement $ \gamma_1 = \frac{1}{\sigma_1} $. 
\For{$t=1,2,\ldots$}
\State   \algoalign{With $ \bx^\toptzero $ as the initial point, solve
$
\min_{\bx}  L_{\sigma_t}(\bx, \blambda^\toptzero, \bmu^\toptzero),
$
to obtain a solution $ \bx^\toptone $ that satisfies the local precision condition
$
\normtwobig{\nabla_{\bx} L_{\sigma_t}(\bx^\toptone, \blambda^\toptzero, \bmu^\toptzero)} \leq \gamma_t.
$
}
\If{$ \nu_t(\bx^\toptone) \leq \varepsilon_t $}
\If{$ \nu_t(\bx^\toptone) \leq \varepsilon $ and $\normtwobig{\nabla_{\bx} L_{\sigma_t}(\bx^\toptone, \blambda^\toptzero, \bmu^\toptzero)} \leq \gamma$}
\State \algoalign{The global precision requirement for the gradient and the constraint violation \\are satisfied: return approximate solution $ \bx^\toptone, \blambda^\toptzero, \bmu^\toptzero $, terminate iteration.}
\EndIf

\State Update the multipliers:
$
\left\{
\begin{aligned}
	\lambda_i^\toptone \leftarrow \lambda_i^\toptzero + \sigma_t c_i(\bx^\toptone), \quad i \in \mathcalE,\\
	\mu_i^\toptone \leftarrow \max \{ \mu_i^\toptzero + \sigma_t c_i(\bx^\toptone), 0 \}, \quad i \in \mathcalI.
\end{aligned}\right.
$
%\State Update multipliers: $ \blambda^\toptone = \blambda^\toptzero + \sigma_t \bc(\bx^\toptone) $.

\State Keep the penalty factor unchanged: $ \sigma_{t+1} \leftarrow  \sigma_t $.
\State  Reduce local precision  and constraint violations: $ \gamma_{t+1} \leftarrow \frac{\gamma_t}{\sigma_{t+1}} $, $ \varepsilon_{t+1} \leftarrow \frac{\varepsilon_t}{\sigma_{t+1}^\beta} $.
\Else
\State Keep the multipliers unchanged: $ \blambda^\toptone \leftarrow \blambda^\toptzero $.
\State Update penalty factor: $ \sigma_{t+1} \leftarrow \rho \sigma_t $.
\State Adjust local precision  and constraint violations: $ \gamma_{t+1} \leftarrow \frac{1}{\sigma_{t+1}} $, $ \varepsilon_{t+1} \leftarrow \frac{1}{\sigma_{t+1}^\alpha} $.
\EndIf
\EndFor
\State {\bfseries Return:}  final $\bx \leftarrow \bx^\toptzero$;
\end{algorithmic} 
\end{algorithm}






\section{Alternating Direction Methods of Multipliers}

\subsection{Alternating Direction Methods of Multipliers (ADMM)}\label{section:admm_all}
We provide a brief introduction to the \textit{alternating direction methods of multipliers (ADMM)} and  subsequently discuss its applications in matrix factorization and nonnegative matrix factorization (NMF).

\paragrapharrow{Fixed Augmented Lagrangian Method.}
Consider the following convex optimization problem:
\begin{subequations}\label{equation:admm_subequs}
\begin{equation}\label{equation:admm_prob}
	\textbf{(P5)}:\qquad \mathopmin{\bx, \by} F(\bx)\triangleq f(\bx)+g(\by) \gap \text{s.t.}\gap \bD\bx+\bE\by=\bff,
\end{equation}
where $\bD\in\real^{m\times n}$, $\bE\in\real^{m\times p}$, and $\bff\in\real^m$.
The Lagrangian function is 
\begin{equation}
	L (\bx, \by, \blambda) = f(\bx)+g(\by) + \innerproduct{\blambda, \bD\bx+\bE\by-\bff}.
\end{equation}
The dual objective function is obtained by minimizing the Lagrangian with respect to the primal variables $\bx, \by$ (see Section~\ref{section:sub_conjug}):
$$
Q(\blambda) = \min_{\bx, \by\in \real^n} L(\bx, \by, \blambda) = -f^*(-\bD^\top\blambda) - g^*(-\bE^\top\blambda) - \innerproduct{\blambda, \bff}.
$$
Thus, the dual problem becomes
\begin{equation}\label{equation:admm_dualopt}
	\textbf{(PD)}:\qquad \min_{\blambda \in \real^n} f^*(-\bD^\top\blambda) + g^*(-\bE^\top\blambda) + \innerproduct{\blambda, \bff}.
\end{equation}
\end{subequations}
If we assume $f$ and $g$ are proper closed and convex, the dual problem can be solved using the proximal point method (Section~\ref{section:proximal_point}) due to the properness, closedness, and convexity of the conjugate functions (Exercise~\ref{exercise:closedconv_conj}, Exercise~\ref{exercise:proper_conj}).
Given a parameter $\sigma>0$, the update at the $t$-th iteration is 
$$
\blambda^\toptone \leftarrow 
\argmin_{\blambda \in \real^n} 
\left\{G(\blambda)\triangleq f^*(-\bD^\top\blambda) + g^*(-\bE^\top\blambda) + \innerproduct{\blambda, \bff} + \frac{1}{2\sigma } \normtwobig{\blambda - \blambda^\toptzero}^2\right\}.
$$
By the first-order necessary and sufficient conditions of convex functions (Theorem~\ref{theorem:fetmat_opt}), it follows that 
$$
\bzero \in \partial G(\blambda^\toptone) =
-\bD\partial f^*(-\bD^\top\blambda^\toptone)  -\bE\partial g^*(-\bE^\top\blambda^\toptone) +  \bff + \frac{{\blambda^\toptone - \blambda^\toptzero}}{\sigma }.
$$
%where 
%$$
%f^*(-\bD^\top\blambda^\toptone) = \max_{\bu} \innerproduct{\bu, -\bD^\top\blambda^\toptone} - f(\bv) ;\\
%g^*(-\bE^\top\blambda^\toptone) = \max_{\bv} \innerproduct{\bv, -\bE^\top\blambda^\toptone} - g(\bv)
%$$
The definition of conjugate functions (Definition~\ref{definition:conjug_func}) implies that 
$$
\begin{aligned}
 \min_{\bx\in\real^n} \innerproduct{\bD^\top\blambda^\toptone, \bx} + f(\bx)=  f^*(-\bD^\top\blambda^\toptone);\\
\min_{\by\in\real^n} \innerproduct{\bE^\top\blambda^\toptone, \by} + g(\by)= g^*(-\bE^\top\blambda^\toptone).
\end{aligned}
$$
By the conjugate subgradient theorem (Theorem~\ref{theorem:conju_subgra}), this is equivalent to update $\bx$, $\by$, and $\blambda$ by
$$
\begin{aligned}
\bx^\toptone&\in \argmin_{\bx\in\real^n} \innerproduct{\bD^\top\blambda^\toptone, \bx} + f(\bx)=\partial f^*(-\bD^\top\blambda^\toptone);\\
\by^\toptone&\in \argmin_{\by\in\real^n} \innerproduct{\bE^\top\blambda^\toptone, \by} + g(\by)=\partial g^*(-\bE^\top\blambda^\toptone);\\
\blambda^\toptone &= \blambda^\toptzero + \sigma\left( \bD\bx^\toptone  + \bE \by^\toptone  - \bff \right).
\end{aligned}
$$
Substituting the expression of $\blambda^\toptone$ into the first two equalities and using the first-order necessary and sufficient conditions of convex functions (Theorem~\ref{theorem:fetmat_opt}) again (since $f$ and $g$ are proper closed and convex), it follows that 
\begin{subequations}\label{equation:aug_ad_raw}
\begin{align}
	\bzero&\in  \bD^\top\left(\blambda^\toptzero + \sigma\big( \bD\bx^\toptone  + \bE \by^\toptone  - \bff \big)\right) + \partial f(\bx^\toptone);\label{equation:aug_ad_raw1}\\
	\bzero&\in  \bE^\top\left(\blambda^\toptzero + \sigma\big( \bD\bx^\toptone  + \bE \by^\toptone  - \bff \big)\right) + \partial g(\by^\toptone);\label{equation:aug_ad_raw2}\\
	\blambda^\toptone &= \blambda^\toptzero + \sigma\left( \bD\bx^\toptone  + \bE \by^\toptone  - \bff \right).\label{equation:aug_ad_raw3}
\end{align}
\end{subequations}
Given $\blambda^\toptzero$, the quantities \eqref{equation:aug_ad_raw2} and \eqref{equation:aug_ad_raw3} are satisfied as the optimality condition of 
\begin{equation}
L_\sigma (\bx, \by, \blambda^\toptzero) = f(\bx)+g(\by) + \innerproduct{\blambda^\toptzero, \bD\bx+\bE\by-\bff} + \frac{\sigma}{2}\normtwo{\bD\bx+\bE\by-\bff}^2,
\end{equation}
which is known as the \textit{augmented Lagrangian function} of \eqref{equation:admm_prob} as we have discussed in Section~\ref{section:aug_lag_method}.
When $\sigma=0$, the augmented Lagrangian function reduces to the standard Lagrangian function; when $\sigma>0$, it  acts as  a penalized version of the Lagrangian function.
The \textit{augmented Lagrangian method}  proceeds by iteratively solving (at the $t$-th iteration):
$$
\text{augmented Lagrangian:}
\gap 
\left\{
\begin{aligned}
	(\bx^\toptone, \by^\toptone) &\in \mathop{\argmin}_{\bx, \by} L_\sigma (\bx, \by, \blambda^\toptzero);\\
	\blambda^\toptone&=\blambda^\toptzero + \sigma(\bD\bx^\toptone+\bE\by^\toptone -\bff ).
\end{aligned}
\right.
$$
However, in this case, due to the convergence of the proximal point method (Theorem~\ref{theorem:prox_point}), the parameter $\sigma$ can remain fixed rather than increasing over iterations, unlike in standard augmented Lagrangian methods (Algorithm~\ref{alg:aug_lag_method}).

\paragrapharrow{ADMM.}
One significant challenge is the coupling term between the $\bx$ and  $\by$ variables, which takes the form  $\sigma(\bx^\top\bD^\top\bE\by)$.
ADMM addresses this issue by substituting exact minimization of $(\bx,\by)$ with one iteration of an alternating minimization method. Specifically, for the  $t$-iteration, the ADMM solution updates $\bx, \by$, and $\blambda$ iteratively:
\begin{tcolorbox}[colback=white,colframe=black]
\begin{minipage}{1\textwidth}
\begin{subequations}
\begin{equation}\label{equation:upda_admm1}
\text{ADMM:}\gap
\left\{
\small
\begin{aligned}
\bx^\toptone&\in\mathop{\argmin}_{\bx} \left\{ f(\bx)+ \frac{\sigma}{2}\normtwo{\bD\bx+\bE\by^\toptzero -\bff +\frac{1}{\sigma}\blambda^\toptzero}^2 \right\};\\
\by^\toptone&\in\mathop{\argmin}_{\by} \left\{ g(\by)+ \frac{\sigma}{2}\normtwo{\bD\bx^\toptone+\bE\by -\bff +\frac{1}{\sigma}\blambda^\toptzero}^2 \right\};\\
\blambda^\toptone&=\blambda^\toptzero + \sigma(\bD\bx^\toptone+\bE\by^\toptone -\bff ).
\end{aligned}
\right.
\end{equation}
Let $\widetildebl\triangleq\frac{1}{\sigma}\blambda$, this can be equivalently stated as (the one we will use in the sequel):
\begin{equation}\label{equation:admm_gen_up}
\text{ADMM:}\gap
\left\{
\small
\begin{aligned}
\bx^\toptone&\in\mathop{\argmin}_{\bx} \left\{ f(\bx)+ \frac{\sigma}{2}\normtwo{\bD\bx+\bE\by^\toptzero -\bff +\widetildebl^\toptzero}^2 \right\};\\
\by^\toptone&\in\mathop{\argmin}_{\by} \left\{ g(\by)+ \frac{\sigma}{2}\normtwo{\bD\bx^\toptone+\bE\by -\bff +\widetildebl^\toptzero}^2 \right\};\\
\widetildebl^\toptone&=\widetildebl^\toptzero + (\bD\bx^\toptone+\bE\by^\toptone -\bff ).
\end{aligned}
\right.
\end{equation}
\end{subequations}
\end{minipage}
\end{tcolorbox}



ADMM is widely utilized in solving problems such as LASSO, sparse logistic regression, and basis pursuit. These applications benefit from ADMM's capability to efficiently handle sparsity constraints. Additionally, ADMM can be employed to address optimization challenges related to training support vector machines (SVMs), especially when dealing with large-scale datasets \citep{boyd2011distributed}. 
Here, we provide a brief overview of its application in low-rank matrix factorization problems \citep{lu2021numerical}.


\index{Matrix factorization}
\paragrapharrow{ADMM applied to matrix factorization.}

In the context of low-rank matrix factorization, ADMM iteratively optimizes two subproblems to approximate $\bA\approx\bW\bZ$:
$$
\mathopmin{\bZ} \frac{1}{2}\normf{\bA-\bW\bZ}^2+r(\bZ)
\qquad\text{and}\qquad
\mathopmin{\bW} \frac{1}{2}\normf{\bA-\bW\bZ}^2+r(\bW)
$$
where $\bA\in\real^{M\times N}$, $\bW\in\real^{M\times K}$, and $\bZ\in\real^{K\times N}$ ($K<\min\{M,N\}$).
Given that these two problems are duals of each other, we focus on the first one. This problem can be equivalently formulated using an auxiliary variable $\widetildebZ\in\real^{K\times N}$:
\begin{equation}\label{equation:mf_admm_prob1}
	\mathopmin{\bZ} \frac{1}{2}\normf{\bA-\bW\bZ}^2+r(\widetildebZ), 
	\gap 
	\text{s.t.}
	\gap 
	\bZ=\widetildebZ.
\end{equation}
Following the general ADMM update rule in \eqref{equation:admm_gen_up}, let (a). \{$\bx\leftarrow \bZ$, $\by\leftarrow \widetildebZ$, $\widetildebl\leftarrow \bL$, $\bD=-\bI$,  $\bE=\bI$\} or (b). \{$\bx\leftarrow \bZ$, $\by\leftarrow \widetildebZ$, $\widetildebl\leftarrow \bL$, $\bD=\bI$,  $\bE=-\bI$\}, 
the ADMM updates  for \eqref{equation:mf_admm_prob1} are then given by: 
\begin{equation}\label{equation:admm_gen_als}
	%\text{ADMM:}\gap
	\left\{
	\begin{aligned}
		\bZ 
		&\stackrel{(a)}{\leftarrow} (\bW^\top\bW+\sigma \bI)^{-1} \left[ \bW^\top\bA +\sigma(\widetildebZ+\bL) \right]
		&\stackrel{(b)}{\leftarrow}& (\bW^\top\bW+\sigma \bI)^{-1} \left[ \bW^\top\bA +\sigma(\widetildebZ-\bL) \right];\\
		\widetildebZ
		&\stackrel{(a)}{\leftarrow}\mathop{\argmin}_{\widetildebZ} r(\widetildebZ) + \frac{\sigma}{2}\normf{-\bZ+\widetildebZ + \bL}^2
		&\stackrel{(b)}{\leftarrow}&\mathop{\argmin}_{\widetildebZ} r(\widetildebZ) + \frac{\sigma}{2}\normf{\bZ-\widetildebZ + \bL}^2\\
		\bL&\stackrel{(a)}{\leftarrow}\bL -\bZ+\widetildebZ &\stackrel{(b)}{\leftarrow}& \bL +\bZ-\widetildebZ.
	\end{aligned}
	\right.
\end{equation}
For practical implementation, the Cholesky decomposition of $(\bW^\top\bW+\sigma \bI)$ can be computed to facilitate updates through forward and backward substitutions. The update for $\bW$ follows similarly due to symmetry considerations. In the following discussion, we will consider setting (a) from the above equations.



%\begin{algorithm}[h] 
%\caption{Matrix Factorization via ADMM}
%\label{alg:mf_admm}
%\begin{algorithmic}[1] 
%\Require Matrix $\bA\in \real^{M\times N}$;
%\State Initialize $\bW\in \real^{M\times K}$, $\bZ\in \real^{K\times N}$ randomly;
%\State Choose a stoping criterion on the approximation error $\delta$;
%\State Choose maximal number of iterations $C$;
%\State $iter=0$; \Comment{Count for the number of iterations}
%\While{$\normf{\bA- (\bW\bZ)}^2>\delta $ and $iter<C$}
%\State $iter=iter+1$;  
%\For{$k=1$ to $K$}
%\State $
%\bZ[k,:]\leftarrow 
%\max\left(
%\bzero, 
%\frac{\bW[:,k]^\top\bA_k}{\normtwo{\bW[:,k]}^2}
%\right)$; \Comment{$\bA_k\triangleq\big(\bA-\sum_{p\neq k}^{K} \bW[:,p]\bZ[p,:]\big)$}
%
%\State $
%\bW[:, k]\leftarrow 
%\max\left(
%\bzero, 
%\frac{\bA_k\bZ[k,:]^\top}{\normtwo{\bZ[k,:]}^2}
%\right)$;
%\EndFor
%\EndWhile
%\State Output $\bW,\bZ$;
%\end{algorithmic} 
%\end{algorithm}

The function $r(\bZ)$ is quite general, allowing us to use various forms. Below are some examples:
\paragraph{ADMM applied to $\ell_1$ regularization.}
For  $\ell_1$ regularization, where  $r(\widetildebZ)=\lambda \normonebig{\widetildebZ}$, the update for each element $(k,n)$ of $\widetildebZ$ is given by
\begin{equation}\label{equation:admm_mf_l1}
\widetildez_{kn}\leftarrow \max(0, 1-\frac{\lambda}{\sigma} \abs{h_{kn}}^{-1}) h_{kn}
\end{equation} 
for all $k\in\{1,2,\ldots,K\}$ and $n\in\{1,2,\ldots,N\}$, where $h_{kn} = z_{kn}-l_{kn}$ (i.e., the elements of $\bH=\bZ-\bL$). 

\paragraph{ADMM applied to smoothness/denoising regularization.}
Smoothness regularization on $\bZ$ can be defined as $r(\widetildebZ)=\frac{\lambda}{2} \Vert\bT\widetildebZ^\top\Vert_F^2$, where $\bT$ is an $N\times N $ tridiagonal matrix with 2 on the main diagonal and $-1$ on the superdiagonal and subdiagonal. This regularization ensures the proximal components in each row of $\widetildebZ$ are smooth (see Problem~\ref{prob:denoise_rls}). The update  of $\widetildebZ$ becomes 
\begin{equation}\label{equation:admm_mf_smooth}
\widetildebZ\leftarrow \sigma\bZ(\lambda \bT^\top\bT +\sigma\bI)^{-1}.
\end{equation} 

\paragraph{ADMM applied to NMF.}
In the case of low-rank nonnegative matrix factorization (NMF) using ADMM, we replace $r(\bZ)$with an indicator function: $r(z_{kn})=0$ if $z_{kn}\geq 0$ and $\infty$ otherwise.
The update for $\widetildebZ$ becomes 
\begin{equation}\label{equation:admm_nmf}
\max\left(\bzero, \bZ-\bL\right),
\end{equation} 
where the max operator is applied  componentwise.

\subsection{Alternating Direction Proximal Method of Multipliers (ADPMM)}\label{section:adpmm}

Following \citet{beck2017first}, we also discuss the \textit{alternating direction proximal method of multipliers (ADPMM)}, which is a slight generalization of ADMM.
For the same problem (P5) in \eqref{equation:admm_prob}, the $t$-th iteration of ADPMM is derived from ADMM in \eqref{equation:upda_admm1}:
\begin{tcolorbox}[colback=white,colframe=black]
\begin{minipage}{1\textwidth}
\begin{equation}
\text{ADPMM:}\quad
\left\{
\small
\begin{aligned}
\bx^\toptone&\in\mathop{\argmin}_{\bx} \left\{ f(\bx)+ \frac{\sigma}{2}\normtwo{\bD\bx+\bE\by^\toptzero -\bff +\frac{1}{\sigma}\blambda^\toptzero}^2  +\textcolor{mylightbluetext}{\frac{1}{2}\norm{\bx-\bx^\toptzero}_{\bA}^2}  \right\};\\
\by^\toptone&\in\mathop{\argmin}_{\by} \left\{ g(\by)+ \frac{\sigma}{2}\normtwo{\bD\bx^\toptone+\bE\by -\bff +\frac{1}{\sigma}\blambda^\toptzero}^2   + \textcolor{mylightbluetext}{\frac{1}{2}\norm{\by-\by^\toptzero}_{\bB}^2} \right\} ;\\
\blambda^\toptone&=\blambda^\toptzero + \sigma(\bD\bx^\toptone+\bE\by^\toptone -\bff ).
\end{aligned}
\right.
\end{equation}
\end{minipage}
\end{tcolorbox}
\noindent 
Here, $\bA\in\real^{n\times n}$ and $\bB\in\real^{p\times p}$ are positive semidefintie matrices. The notation $\norm{\bx}_{\bQ}= (\bx^\top\bQ\bx)^{1/2}$ represents the $\bQ$-norm (see \eqref{equation:q_norm}).
When $\bA=\bzero $ and $\bB=\bzero$, ADPMM simplifies  to ADMM.

One possible choice for $\bA$ and $\bB$ is
\begin{equation}\label{equation:adlpmm_ab}
\bA\triangleq  \mu\bI -\sigma \bD^\top\bD
\qquad \text{and}\qquad 
\bB\triangleq \gamma \bI -\sigma \bE^\top\bE,
\end{equation}
where $\mu\geq \sigma \lambda_{\max}(\bD^\top\bD)$ and $ \gamma  \geq \sigma \lambda_{\max}(\bE^\top\bE)$ such that $\bA$ and $\bB$ are positive semidefinite according to Theorem~\ref{theorem:eigen_charac}.







\begin{assumption}[ADPMM]\label{assumption:ADPMM}
For the ADPMM, we assume
\begin{enumerate}[(i)]
\item $f : \real^n \to (-\infty, \infty]$ and $g : \real^p \to (-\infty, \infty]$ are proper closed convex functions.
\item $\bD \in \real^{m \times n}$, $\bE \in \real^{m \times p}$, $\bff \in \real^m$, $\sigma > 0$.
\item $\bA \in \real^{n\times n}$ and $\bB\in\real^{p\times p}$ are positive semidefinite.
\item For any $\bu \in \real^n$ and $\bv \in \real^p$, the optimal sets of the problems
$$
\min_{\bx \in \real^n} \left\{ f(\bx) + \frac{\sigma}{2} \normtwo{\bD \bx}^2 + \frac{1}{2} \normA{\bx}^2 + \langle \bu, \bx \rangle \right\}
$$
and
$$
\min_{\by \in \real^p} \left\{ g(\by) + \frac{\sigma}{2} \normtwo{\bE \by}^2 + \frac{1}{2} \normB{\by}^2 + \langle \bv, \by \rangle \right\}
$$
are nonempty.
\item There exists $\widetilde{\bx} \in \domain(f)$ and $\widetilde{\by} \in \domain(g)$ for which $\bD \widetilde{\bx} + \bE \widetilde{\by} = \bff$.
\end{enumerate}
\end{assumption}

Property (iv) ensures that the ADPMM method is well-defined. The following theorem demonstrates an
 $\mathcalO(1/T)$ rate of convergence for the sequence generated by ADPMM.

\begin{theoremHigh}[Convergence of ADPMM, $\mathcalO(1/T)$]\label{theorem:ratconv_adpmm}
Suppose that Assumption~\ref{assumption:ADPMM} holds. Let $\big\{(\bx^\toptzero, \by^\toptzero)\big\}_{t > 0}$ be the sequence generated by ADPMM for solving problem (P5) in \eqref{equation:admm_prob}. Let $(\bx^*, \by^*)$ be an optimal solution of problem (P5) and $\blambda^*$ be an optimal solution of the dual problem \eqref{equation:admm_dualopt}. Suppose that $\zeta > 0$ is any constant satisfying $\zeta \geq 2 \normtwo{\blambda^*}$.
Then for all $T >0$,
\begin{align}
F(\overline{\bx}^{(T)}, \overline{\by}^{(T)}) - F(\bx^*, \by^*) &\leq \frac{\normA{\bx^* - \bx^\topone}^2 + \normC{\by^* - \by^\topone}^2 + \frac{1}{\sigma} \big(\zeta + \normtwobig{\blambda^\topone}\big)^2}{2T},
\end{align}
where $\bC \triangleq \sigma \bE^\top \bE + \bB$ and
$$
\overline{\bx}^{(T)} = \frac{1}{T} \sum_{t=1}^T \bx^\toptone
\qquad \text{and}\qquad 
\overline{\by}^{(T)} = \frac{1}{T} \sum_{t=1}^T \by^\toptone.
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:ratconv_adpmm}]
By the first-order optimality condition of convex functions (Theorem~\ref{theorem:fetmat_opt}) and the update rules of ADPMM, it follows that $\bx^\toptone$ and $\by^\toptone$ satisfy
\begin{align}
- \sigma \bD^\top \left( \bD\bx^\toptone + \bE \by^\toptzero - \bff + \frac{1}{\sigma} \blambda^\toptzero \right) - \bA(\bx^\toptone - \bx^\toptzero) &\in \partial f(\bx^\toptone), \\
- \sigma \bE^\top \left( \bD\bx^\toptone + \bE \by^\toptone - \bff + \frac{1}{\sigma} \blambda^\toptzero \right) - \bB(\by^\toptone - \by^\toptzero) 
&\in \partial g(\by^\toptone).
\end{align}
Letting
$$
\begin{aligned}
\widetildebx^\toptzero &\triangleq \bx^\toptone, 
\qquad
\widetildeby^\toptzero &\triangleq \by^\toptone, 
\qquad
\widetildeblambda^\toptzero &\triangleq \blambda^\toptzero + \sigma (\bD \bx^\toptone + \bE \by^\toptzero - \bff),
\end{aligned}
$$
using the above optimality conditions and the subgradient inequality (Definition~\ref{definition:subgrad}),  we obtain that for any $\bx \in \domain(f)$ and $\by \in \domain(g)$,
$$
\begin{aligned}
	f(\bx) - f(\widetildebx^\toptzero) + \left\langle \sigma \bD^\top \left( \bD \widetildebx^\toptzero + \bE \by^\toptzero - \bff + \frac{1}{\sigma} \blambda^\toptzero \right) + \bA (\widetildebx^\toptzero - \bx^\toptzero), \bx - \widetildebx^\toptzero \right\rangle &\geq 0, \\
	g(\by) - g(\widetildeby^\toptzero) + \left\langle \sigma \bE^\top \left( \bD \widetildebx^\toptzero + \bE \widetildeby^\toptzero - \bff + \frac{1}{\sigma} \blambda^\toptzero \right) + \bB (\widetildeby^\toptzero - \by^\toptzero), \by - \widetildeby^\toptzero \right\rangle &\geq 0.
\end{aligned}
$$
Using the definition of $\widetildeblambda^\toptzero$, the above two inequalities can be equivalently expressed as
$$
\begin{aligned}
	f(\bx) - f(\widetildebx^\toptzero) + \left\langle \bD^\top \widetildeblambda^\toptzero + \bA (\widetildebx^\toptzero - \bx^\toptzero), \bx - \widetildebx^\toptzero \right\rangle &\geq 0, \\
	g(\by) - g(\widetildeby^\toptzero) + \left\langle \bE^\top \widetildeblambda^\toptzero + (\sigma \bE^\top \bE + \bB) (\widetildeby^\toptzero - \by^\toptzero), \by - \widetildeby^\toptzero \right\rangle &\geq 0.
\end{aligned}
$$
Adding the preceding two inequalities and using the  update rule for $\blambda^\toptone$:
$
\blambda^\toptone - \blambda^\toptzero = \sigma (\bD \widetildebx^\toptzero + \bE \widetildeby^\toptzero - \bff),
$
we can conclude that for any $\bx \in \domain(f)$, $\by \in \domain(g)$, and $\blambda \in \real^m$,
\begin{equation}\label{equation:adpmm_prof00}
\footnotesize
\begin{aligned}
F(\bx, \by) - F(\widetildebx^\toptzero, \widetildeby^\toptzero) + 
\innerproduct{\begin{bmatrix} 
\bx - \widetildebx^\toptzero \\ 
\by - \widetildeby^\toptzero \\ 
\blambda - \widetildeblambda^\toptzero 
\end{bmatrix}, 
\begin{bmatrix} 
\bD^\top \widetildeblambda^\toptzero \\ %+ \bA (\widetildebx^\toptzero - \bx^\toptzero) \\ 
\bE^\top \widetildeblambda^\toptzero \\% + (\sigma \bE^\top \bE + \bB) (\widetildeby^\toptzero - \by^\toptzero) \\ 
- \bD \widetildebx^\toptzero - \bE \widetildeby^\toptzero + \bff 
\end{bmatrix} 
- 
\begin{bmatrix} \bA (\bx^\toptzero - \widetildebx^\toptzero) \\ 
\bC (\by^\toptzero - \widetildeby^\toptzero) \\ 
\frac{1}{\sigma} \big(\blambda^\toptzero - \blambda^\toptone \big)
\end{bmatrix} }
\geq 0,
\end{aligned}
\end{equation}
where $\bC = \sigma \bE^\top \bE + \bB$. For any vectors $\ba,\bb,\bc,\bd$, and a positive semidefinite matrix $\bH$, we have:
$$
(\ba - \bb)^\top \bH (\bc - \bd) 
= 
\frac{1}{2} \left( \normH{\ba - \bd}^2 - \normH{\ba - \bc }^2 + \normH{\bb - \bc}^2 - \normH{\bb - \bd}^2 \right).
$$
This concludes that 
\begin{equation}\label{equation:adpmm_prof1}
\begin{aligned}
	(\bx - \widetildebx^\toptzero)^\top \bA (\bx^\toptzero - \widetildebx^\toptzero) &= \frac{1}{2} \left( \normAbig{\bx - \bx^\toptzero}^2 - \normAbig{\bx - \bx^\toptzero}^2 + \normAbig{\widetildebx^\toptzero - \bx^\toptzero}^2 \right) \\
	&\geq \frac{1}{2} \normAbig{\bx - \bx^\toptzero}^2 - \frac{1}{2} \normAbig{\bx - \bx^\toptzero}^2,
\end{aligned}
\end{equation}
and
\begin{equation}\label{equation:adpmm_prof2}
\begin{aligned}
	(\by - \widetildeby^\toptzero)^\top \bC (\by^\toptzero - \widetildeby^\toptzero) &= \frac{1}{2} \normC{\by - \by^\toptzero }^2 - \frac{1}{2} \normC{\by - \by^\toptzero}^2 + \frac{1}{2} \normC{\widetildeby^\toptzero - \by^\toptzero}^2.
\end{aligned}
\end{equation}
Using the expressions of $\widetildeblambda^\toptzero$ and $\blambda^\toptone$, we have 
$$
\small
\begin{aligned}
&2 (\blambda - \widetildeblambda^\toptzero)^\top (\blambda^\toptzero - \blambda^\toptone) 
= \normtwobig{\blambda - \blambda^\toptone}^2 - \normtwobig{\blambda - \blambda^\toptzero}^2 +\normtwobig{\widetildeblambda^\toptzero - \blambda^\toptzero}^2 - \normtwobig{ \widetildeblambda^\toptzero - \blambda^\toptone}^2\\
&= \normtwobig{\blambda - \blambda^\toptone}^2 
- \normtwobig{\blambda - \blambda^\toptzero}^2 
+ \sigma^2\normtwo{\bD \widetildebx^\toptzero + \bE \by^\toptzero - \bff}^2 \\
&\qquad - \normtwo{\blambda^\toptzero + \sigma (\bD \widetildebx^\toptzero + \bE \by^\toptzero - \bff) - \blambda^\toptzero - \sigma (\bD \widetildebx^\toptzero + \bE \widetildeby^\toptzero - \bff)}^2\\
&= \normtwobig{\blambda - \blambda^\toptone}^2 - \normtwobig{\blambda - \blambda^\toptzero}^2 
+ \sigma^2 \normtwo{\bD \widetildebx^\toptzero + \bE \by^\toptzero - \bff}^2 
 - \sigma^2 \normtwo{\bE (\by^\toptzero - \widetildeby^\toptzero)}^2.
\end{aligned}
$$
Therefore,
\begin{equation}\label{equation:adpmm_prof3}
	\small
\begin{aligned}
\frac{1}{\sigma} (\blambda - \widetildeblambda^\toptzero)^\top (\blambda^\toptzero - \blambda^\toptone) \geq \frac{1}{2\sigma} \left( \normtwobig{ \blambda - \blambda^\toptone}^2 - \normtwobig{ \blambda - \blambda^\toptzero}^2 \right) - \frac{\sigma}{2} \normtwobig{\bE (\by^\toptzero - \widetildeby^\toptzero)}^2.
\end{aligned}
\end{equation}
Moreover, we denote
$$
\bZ \triangleq
\small
\begin{bmatrix}
	\bA & 0 & 0 \\
	0 & \bC & 0 \\
	0 & 0 & \frac{1}{\sigma} \mathbf{I}
\end{bmatrix}, 
\normalsize
\qquad
\bu \triangleq 
\small
\begin{bmatrix}
	\bx \\
	\by \\
	\blambda
\end{bmatrix}, 
\normalsize
\qquad
\bu^\toptzero \triangleq 
\small
\begin{bmatrix}
	\bx^\toptzero \\
	\by^\toptzero \\
	\blambda^\toptzero
\end{bmatrix}, 
\normalsize
\quad\text{and}\quad 
\widetildebu^\toptzero \triangleq 
\small
\begin{bmatrix}
	\widetildebx^\toptzero \\
	\widetildeby^\toptzero \\
	\widetildeblambda^\toptzero
\end{bmatrix}.
\normalsize
$$
Combining \eqref{equation:adpmm_prof1}, \eqref{equation:adpmm_prof2}, and \eqref{equation:adpmm_prof3} yields that 
$$
\left\langle
\small
\begin{bmatrix}
	\bx - \widetildebx^\toptzero \\
	\by - \widetildeby^\toptzero \\
	\blambda - \widetildeblambda^\toptzero
\end{bmatrix}, 
\begin{bmatrix}
	\bA (\bx^\toptzero - \widetildebx^\toptzero) \\
	\bC (\by^\toptzero - \widetildeby^\toptzero) \\
	\frac{1}{\sigma} (\blambda^\toptzero - \blambda^\toptone)
\end{bmatrix} 
\normalsize
\right\rangle \geq \frac{1}{2} \normZ{\bu - \bu^\toptone}^2 - \frac{1}{2} \normZ{\bu - \bu^\toptzero}^2.
$$
Combining the preceding inequality with \eqref{equation:adpmm_prof00}, we obtain that for any $\bx \in \domain(f)$, $\by \in \domain(g)$, and $\blambda \in \real^m$,
\begin{equation}\label{equation:adpmm_prof4}
F(\bx, \by) - F(\widetildebx^\toptzero, \widetildeby^\toptzero) +\innerproduct{\bu - \widetildebu^\toptzero, \bG \widetildebu^\toptzero + \widetilde{\bff}} \geq \frac{1}{2} \normZbig{\bu - \bu^\toptone}^2 - \frac{1}{2} \normZbig{\bu - \bu^\toptzero}^2, 
\end{equation}
where
$$
\bG \triangleq 
\small
\begin{bmatrix}
	\bzero & \bzero & \bD^\top \\
	\bzero & \bzero & \bE^\top \\
	-\bD & -\bE & \bzero
\end{bmatrix}
\normalsize
\qquad \text{and}\qquad 
\widetilde{\bff} \small
\triangleq 
\begin{bmatrix}
	\bzero \\
	\bzero \\
	\bff
\end{bmatrix}.
$$
Note that
$$
\innerproduct{\bu - \widetildebu^\toptzero, \bG \widetildebu^\toptzero + \widetilde{\bff}}
= \innerproduct{\bu - \widetildebu^\toptzero, \bG (\widetildebu^\toptzero - \bu) + \bG \bu + \widetilde{\bff}}
= \innerproduct{\bu - \widetildebu^\toptzero, \bG \bu + \widetilde{\bff}},
$$
where the second equality follows from the fact that $\bG$ is skew-symmetric (i.e., $\bG^\top = -\bG$). We can thus conclude that \eqref{equation:adpmm_prof4} can be rewritten as
$$
F(\bx, \by) - F(\widetildebx^\toptzero, \widetildeby^\toptzero) + 
\innerproduct{\bu - \widetildebu^\toptzero, \bG \bu + \widetilde{\bff} }
\geq \frac{1}{2} \normZbig{\bu - \bu^\toptone}^2 - \frac{1}{2} \normZbig{\bu - \bu^\toptzero}^2.
$$
Performing sum of the above inequality over $t = 1, 2, \ldots, T$ yields the inequality
$$
T\cdot  F(\bx, \by) - \sum_{t=1}^T F(\widetildebx^\toptzero, \widetildeby^\toptzero) + 
\innerproduct{T \cdot \bu - \sum_{t=1}^T \widetildebu^\toptzero, \bG \bu + \widetilde{\bff}}
 \geq -\frac{1}{2} \normZbig{\bu - \bu^\topone}^2.
$$
Defining
$$
\overline{\bu}^{(T)} \triangleq \frac{1}{T} \sum_{t=1}^T \widetildebu^\toptzero, 
\qquad 
\overline{\bx}^{(T)} \triangleq \frac{1}{T} \sum_{t=1}^T \bx^\toptone,
\qquad 
\overline{\by}^{(T)} \triangleq \frac{1}{T} \sum_{t=1}^T \by^\toptone,
$$
and using the convexity of $F$, we obtain that
$$
\begin{aligned}
&F(\bx, \by) - F(\overline{\bx}^{(T)}, \overline{\by}^{(T)}) + \innerproduct{\bu - \overline{\bu}^{(T)}, \bG \bu + \widetilde{\bff}} + \frac{1}{2T} \normZbig{\bu - \bu^\topone}^2 \geq 0 \\
&\implies 
F(\bx, \by) - F(\overline{\bx}^{(T)}, \overline{\by}^{(T)}) + \innerproduct{\bu - \overline{\bu}^{(T)}, \bG \overline{\bu}^{(T)} + \widetilde{\bff}} + \frac{1}{2T} \normZbig{\bu - \bu^\topone}^2 \geq 0,
\end{aligned}
$$
where the implication follows from  the skew-symmetry of $\bG$.
In other words, for any $\bx \in \domain(f)$ and $\by \in \domain(g)$,
\begin{equation}\label{equation:adpmm_prof5}
F(\overline{\bx}^{(T)}, \overline{\by}^{(T)}) - F(\bx, \by) + \innerproduct{\overline{\bu}^{(T)} - \bu, \bG \overline{\bu}^{(T)} + \widetilde{\bff}} \leq \frac{1}{2T} \normZbig{\bu - \bu^\topone}^2. 
\end{equation}

Let $(\bx^*, \by^*)$ be an optimal solution of problem (P5). Then $\bD \bx^* + \bE \by^* = \bff$. 
Let $\overline{\blambda}^{(T)} \triangleq \frac{1}{T} \sum_{t=1}^T \blambda^\toptzero$. Plugging $\bx = \bx^*$, $\by = \by^*$, and the expressions for $\overline{\bu}^{(T)}$, $\bu$, $\bu^\topone$, $\bG$, $\bZ$, $\widetilde{\bff}$ into \eqref{equation:adpmm_prof5}, we obtain
$$
\begin{aligned}
& F(\overline{\bx}^{(T)}, \overline{\by}^{(T)}) - F(\bx^*, \by^*) +\innerproduct{\overline{\bx}^{(T)} - \bx^*, \bD^\top \overline{\blambda}^{(T)}} + \innerproduct{\overline{\by}^{(T)} - \by^*, \bE^\top \overline{\blambda}^{(T)}} \\
& \quad + \innerproduct{\overline{\blambda}^{(T)} - \blambda, -\bD \overline{\bx}^{(T)} - \bE \overline{\by}^{(T)} + \bff} \\
& \quad \leq \frac{1}{2T} \left\{ \normAbig{\bx^* - \bx^\topone}^2 + \normCbig{\by^* - \by^\topone}^2 + \frac{1}{\sigma} \normtwobig{ \blambda - \blambda^\topone}^2 \right\}.
\end{aligned}
$$
Cancelling terms and using the fact that $\bD \bx^* + \bE \by^* = \bff$, the above inequality reduces to
$$
\begin{aligned}
&F(\overline{\bx}^{(T)}, \overline{\by}^{(T)}) - F(\bx^*, \by^*) + \innerproduct{\blambda, \bD \overline{\bx}^{(T)} + \bE \overline{\by}^{(T)} - \bff} \\
&\leq \frac{\normA{\bx^* - \bx^\topone}^2 + \normC{\by^* - \by^\topone}^2 + \frac{1}{\sigma} \normtwobig{\blambda - \blambda^\topone }^2}{2T}.
\end{aligned}
$$
Taking $\blambda \in \sB[\bzero, \zeta]$ as the maximum of both sides obtains
$$
\begin{aligned}
&F(\overline{\bx}^{(T)}, \overline{\by}^{(T)}) - F(\bx^*, \by^*) + 
\zeta\normtwo{\bD \overline{\bx}^{(T)} + \bE \overline{\by}^{(T)} - \bff} \\
&\leq \frac{\normA{ \bx^* - \bx^\topone}^2 + \normC{\by^* - \by^\topone}^2 + \frac{1}{\sigma} (\zeta + \normtwobig{\blambda^\topone})^2}{2T}.
\end{aligned}
$$
This concludes the result.
Since $\zeta \geq 2 \normtwo{\blambda^*}$ 


\end{proof}

In fact, using the strong duality of optimal values of the primal and dual problems (P5) and (PD) in \eqref{equation:admm_subequs}, it can also be shown that 
$$
\normtwo{\bD \overline{\bx}^{(T)} + \bE \overline{\by}^{(T)} - \bff} 
\leq \frac{\normA{\bx^* - \bx^\topone}^2 + \normC{\by^* - \by^\topone}^2 + \frac{1}{\sigma} \big(\zeta + \normtwobig{\blambda^\topone }\big)^2}{\zeta T},
$$
See \citet{beck2017first} for more details.


\begin{problemset}
\item \textbf{ADLPMM.} Considering  ADPMM discussed in Section~\ref{section:adpmm}, let $\bA\triangleq  \mu\bI -\sigma \bD^\top\bD$ and 
$\bB\triangleq \gamma \bI -\sigma \bE^\top\bE$, as defined in \eqref{equation:adlpmm_ab}. Show that the  update of ADPMM becomes
\begin{align*}
\bx^\toptone &\leftarrow \prox_{\frac{1}{\mu} f} \left[ \bx^\toptzero - \frac{\sigma}{\mu} \bD^\top \left( \bD \bx^\toptzero + \bE \by^\toptzero - \bff + \frac{1}{\sigma} \blambda^\toptzero \right) \right]; \\
\by^\toptone &\leftarrow \prox_{\frac{1}{\gamma} g} \left[ \by^\toptzero - \frac{\sigma}{\gamma} \bE^\top \left( \bD \bx^\toptone + \bE \by^\toptzero - \bff + \frac{1}{\sigma} \blambda^\toptzero \right) \right]; \\
\blambda^\toptone &\leftarrow \blambda^\toptzero + \sigma \left( \bD \bx^\toptone + \bE \by^\toptone - \bff \right),
\end{align*}
where $\prox_{\cdot}(\cdot)$ denotes the proximal operator (Definition~\ref{definition:projec_prox_opt}).
This variant is referred to as the \textit{alternating direction linearized proximal method of multipliers (ADLPMM).}

\item Prove the update formulations of ADMM applied to the $\ell_1$ regularization in \eqref{equation:admm_mf_l1}, the smoothness regularization in \eqref{equation:admm_mf_smooth}, and the nonnegative regularization in \eqref{equation:admm_nmf}.
\end{problemset}

