\newpage
\chapter{Stochastic Optimization}\label{chapter:stochastic_opt}
\begingroup
\hypersetup{linkcolor=structurecolor,
linktoc=page,  % page: only the page will be colored; section, all, none etc
}
\minitoc \newpage
\endgroup
\index{Stochastic Optimization}

\section{Stochastic Optimization}
This chapter concludes with an exploration of stochastic optimization algorithms (a.k.a., stochastic optimizers), a critical approach for tackling large-scale and data-intensive problems. It covers key topics such as \textit{stochastic gradient descent (SGD)}, \textit{variance reduction methods like SAG, SAGA, and SVRG}, and adaptive optimizers including \textit{Adam, RMSProp, AdaGrad, and Nadam}. The discussion also includes learning rate strategies, such as \textit{annealing, warmup, and cyclical learning rates}, offering valuable insights into practical deep learning optimization.


Over time, stochastic gradient-based optimization has become a cornerstone in numerous scientific and engineering disciplines, particularly in areas like computer vision, large language models (LLM), and automatic speech recognition \citep{krizhevsky2012imagenet, hinton2012deep, graves2013speech, vaswani2017attention}. 
SGD, alongside deep neural networks or transformer architectures, plays a pivotal role in optimizing stochastic objective functions. 
When developing a new deep neural network or a transformer structure for a specific task, one must often select hyperparameters related to training through heuristic methods. For each combination of structural hyperparameters, a fresh network is usually trained from scratch and evaluated repeatedly. Although significant advancements in hardware (e.g., GPUs) and software (e.g., cuDNN) have accelerated the training process for individual network or transformer structures, exploring a vast array of potential configurations remains time-consuming. Therefore, there is a need for stochastic optimizers that are robust to hyperparameter settings. Efficient stochastic optimizers are thus essential for training deep neural networks and transformers effectively.


\begin{table}[h]
\begin{tabular}{llll|lll}
\hline
Method         & Year & Papers \gap \gap  &  & Method                  & Year & Papers \\ \hline\hline
Adam           & 2014 & 22,319   &  & AdamW                   & 2017 & 180     \\
SGD            & 1951 & 1916   &  & Local SGD               & 2018 & 65     \\
RMSProp        & 2013 & 495    &  & Gravity                 & 2021 & 177     \\
Adafactor      & 2018 & 691    &  & AMSGrad                 & 2019 & 49     \\
Momentum       & 1999 & 141    &  & LARS                    & 2017 & 74     \\
LAMB           & 2019 & 191    &  & MAS                     & 2020 & 135     \\
AdaGrad        & 2011 & 181    &  & DFA                     & 2016 & 48     \\
Deep Ensembles & 2016 & 163     &  & Nesterov momentum    & 1983 & 32     \\
ADOPT          & 2024 & 286     &  & AO    & 2000 & 58     \\
RAdam          & 2019 & 56     &  & Apollo    & 2020 & 46     \\
FA             & 2014 & 112     &  & Gradient Sparsification & 2017 & 36     \\ \hline
\end{tabular}
\caption{Data retrieved on February 9th, 2025 via https://paperswithcode.com/.}
\label{table:stochastic-optimizers}
\end{table}

Several adaptations of SGD aim to estimate an optimal learning rate at each iteration, either by accelerating progress when feasible or decelerating it near local minima. In this section, we present some stochastic optimizers from these two categories. Table~\ref{table:stochastic-optimizers} offers an overview of how frequently these stochastic optimization algorithms have been utilized in research papers for specific tasks and their publication dates. For more comprehensive reviews, readers may consult sources such as \citet{zeiler2012adadelta}, \citet{ruder2016overview},  \citet{goodfellow2016deep}, and \citet{bottou2018optimization}.




\begin{algorithm}[h] 
\caption{Stochastic Projected Gradient Descent Method}
\label{alg:stopgd_gen}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$ and a set $\sS$; 
\State {\bfseries Input:} Initialize $\bx^{(1)}\in\real^n$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t$ and a stochastic vector $\bg^\toptzero\in\real^n$;
\State $\by^{(t+1)} \leftarrow \bx^{(t)} - \eta_t \bg^\toptzero$;
\State $\bx^{(t+1)} \leftarrow \mathcalP_{\sS}(\by^{(t+1)})$;
\EndFor
\State (Output Option 1) Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\State (Output Option 2) Output  $\bx_{\text{avg}}\leftarrow \frac{1}{T}(\sum_{t=1}^{t}\bx^{(t)})$ or $\sum_{t=1}^{T} \frac{2t}{T(T+1)} \bx^{(t)}$;
\State (Output Option 3) Output  $\bx_{\text{best}}\leftarrow \argmin_{t\in\{1,2,\ldots,T\}} f(\bx^{(t)})$;
\end{algorithmic} 
\end{algorithm}
\section{Stochastic (Projected) Gradient Descent}
\subsection{Results of Stochastic Projected Gradient Descent}
We discussed the convergence results of the projected gradient descent  method in Section~\ref{section:pgd}. The \textit{stochastic projected gradient descent  (SPGD)} method (Algorithm~\ref{alg:stopgd_gen}) yields similar outcomes. 
As its name implies, SPGD employs a stochastic gradient vector instead of the true gradient vector used in gradient-based algorithms.
Let us begin by outlining the assumptions required for this method.
\begin{assumption}[Stochastic Optimizer Assumption]\label{assumption:sto_assump}
Given a convex function $f:\sS\rightarrow \real^n$, for the analysis of stochastic optimizer presented in Algorithm~\ref{alg:stopgd_gen}, we assume the following 
\begin{enumerate}[(A)]
\item \textit{Unbiasedness of random subgradient.} For any iteration  $ t > 0 $, $ \Exp[\bg^\toptzero \mid \bx^\toptzero] \in \partial f(\bx^\toptzero) $, i.e., $
f(\bz) \geq f(\bx^\toptzero) + \innerproduct{\Exp[\bg^\toptzero \mid \bx^\toptzero], \bz - \bx^\toptzero}$ for all $\bz \in \dom(f).
$
\item \textit{Boundedness of second moment.} There exists a constant $ \widetilde{L} > 0 $ such that for any $ t > 0 $, $ \Exp\big[\normtwobig{\bg^\toptzero}^2 \mid \bx^\toptzero\big] \leq \widetilde{L}^2 $.
\end{enumerate}
\end{assumption}

Part (A) of the assumption indicates  that $ \bg^\toptzero $ is an unbiased estimator of a subgradient at $ \bx^\toptzero $. 
The constant $ \widetilde{L} $ from part (B) of Assumption~\ref{assumption:sto_assump} does not necessarily serve as a Lipschitz constant for $ f $, unlike in deterministic cases.


The analysis of the stochastic projected subgradient closely mirrors that of its deterministic counterpart. We summarize the key findings in the following theorem. 
Note that for any iteration $t>0$, we define the best achieved function value as 
\begin{equation}
\fbest^\toptzero \triangleq \min\left\{f(\bx^\topone), f(\bx^{(2)}), \ldots, f(\bx^\toptzero)\right\}.
\end{equation}
\begin{theoremHigh}[SPGD for Convex and ``Lipschitz" Functions]\label{theorem:conv_sgpg}
Let $f: \sS \rightarrow \real$ be a proper closed and convex function, where $\sS$ is a closed and convex set. Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the stochastic projected subgradient method (Algorithm~\ref{alg:stopgd_gen}) with positive stepsizes $ \{\eta_t\}_{t > 0} $, and let $\left\{ \fbest^\toptzero\right\}_{t > 0}$ be the sequence of best achieved values.
Given the assumptions and vectors in Assumption~\ref{assumption:sto_assump}, we have:
\begin{enumerate}[(i)]
\item If $ \frac{\sum_{t=1}^T \eta_t^2}{\sum_{t=1}^T \eta_t} \rightarrow 0 $ as $ T \rightarrow \infty $, then $ \Exp[\fbest^{(T)}] \rightarrow f(\bx^*) $ as $ T \rightarrow \infty $, where $\bx^*$ is an optimizer of $f$.
\item Assuming that $ \sS $ is \textbf{compact}, and letting $ \Omega $ be an upper bound on the half-squared diameter of $ \sS $:
$
\Omega \geq \max_{\bx,\by \in \sS} \frac{1}{2} \normtwo{\bx-\by}^2.
$
\begin{enumerate}[(a)]
\begin{subequations}
\item If $ \eta_t \triangleq \frac{\sqrt{2\Omega}}{\widetilde{L} \sqrt{t+1}} $, then for all $ T \geq 2 $,
\begin{equation}
\Exp[\fbest^{(T)} - f(\bx^*)] \leq \frac{2(1 + \ln(3)) \widetilde{L} \sqrt{2\Omega}}{\sqrt{T+2}}.
\end{equation}
\item If $\{\eta_t\}_{t>0}$ is a nonincreasing sequence, then for all $ T \geq 2 $,
\begin{equation}
\Exp\big[f(\widebarbx^{(T)})\big] - f(\bx^*)\leq \frac{\Omega}{T\eta_T} + \frac{1}{2T}\sum_{t=1}^{T}\eta_t \widetilde{L}^2,
\end{equation}
where $\{\widebarbx^{(t)} \triangleq\frac{1}{t} \sum_{j=1}^{t}\bx^{(j)}\}_{t>0}$ denotes the sequence of average iterates for each $t$.
Specifically, it $\eta_t \triangleq \frac{\sqrt{2\Omega}}{\widetilde{L}\sqrt{t}}$. Then the above inequality reduces to 
\begin{equation}
\Exp\big[f(\widebarbx^{(T)})\big] - f(\bx^*)\leq \frac{3 \sqrt{2\Omega}\widetilde{L}}{2\sqrt{T}}.
\end{equation}
\end{subequations}
\end{enumerate}
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:conv_sgpg}]
\textbf{(i).} For any $ t > 0 $, by the update rule of SPGD, we have
$$
\small
\begin{aligned}
\Exp\big[\normtwobig{\bx^\toptone - \bx^*}^2 \mid \bx^\toptzero\big] 
&= \Exp\big[\normtwobig{\projectS(\bx^\toptzero - \eta_t \bg^\toptzero) - \projectS(\bx^*)}^2 \mid \bx^\toptzero\big] 
\stackrel{\dag}{\leq}  \Exp\big[\normtwobig{\bx^\toptzero - \eta_t \bg^\toptzero - \bx^*}^2 \mid \bx^\toptzero\big]  \\
&= \normtwobig{\bx^\toptzero - \bx^*}^2 - 2\eta_t \innerproduct{\Exp[\bg^\toptzero \mid \bx^\toptzero], \bx^\toptzero - \bx^*} + \eta_t^2 \Exp\big[\normtwobig{\bg^\toptzero}^2 \mid \bx^\toptzero\big] \\
&\stackrel{\ddag}{\leq} \normtwobig{\bx^\toptzero - \bx^*}^2 - 2\eta_t (f(\bx^\toptzero) - f(\bx^*)) + \eta_t^2 \widetilde{L}^2,
\end{aligned}
$$
where the inequality ($\dag$) follows from the nonexpansiveness property of the  projection operator (Theorem~\ref{theorem:proj_nonexpan}), and the inequality  ($\ddag$)  follows by Assumption~\ref{assumption:sto_assump} since $f$ is convex.
By the fact that $\Exp[\Exp[\rX \mid \rY]] = \Exp[\rX]$ for any random variables $\rX$ and $\rY$, the above inequality implies that
\begin{equation}\label{equation:conv_sgpg0}
\Exp\big[\normtwobig{\bx^\toptone - \bx^*}^2\big] \leq \Exp\big[\normtwobig{\bx^\toptzero - \bx^*}^2\big] - 2\eta_t \big(\Exp\big[f(\bx^\toptzero)\big] - f(\bx^*)\big) + \eta_t^2 \widetilde{L}^2.
\end{equation}
Telescoping the sum over $ t = j, j + 1, \ldots, T $ (where $ j $ is an integer satisfying $ j \leq T $),
$$
\begin{aligned}
&\Exp\big[\normtwobig{\bx^{(T+1)} - \bx^*}^2\big] \leq \Exp\big[\normtwobig{\bx^{(j)} - \bx^*}^2\big] - 2 \sum_{t=j}^T \eta_t \big(\Exp[f(\bx^\toptzero)] - f(\bx^*)\big) + \widetilde{L}^2 \sum_{t=j}^T \eta_t^2\\
&\quad\implies\quad 
\sum_{t=j}^T \eta_t \big(\Exp[f(\bx^\toptzero)] - f(\bx^*)\big) \leq \frac{1}{2} \Exp\big[\normtwobig{\bx^{(j)} - \bx^*}^2\big] + \frac{\widetilde{L}^2}{2} \sum_{t=j}^T \eta_t^2.
\end{aligned}
$$
This implies
$$
\left( \sum_{t=j}^T \eta_t \right) \left( \min_{t\in\{j, j+1, \ldots, T\}} \Exp[f(\bx^\toptzero)] - f(\bx^*) \right) \leq \frac{1}{2} \Exp\big[\normtwobig{\bx^{(j)} - \bx^*}^2\big] + \frac{\widetilde{L}^2}{2} \sum_{t=j}^T \eta_t^2.
$$
Since $
\Exp[\fbest^{(T)}] \leq \Exp \left[ \min_{t\in\{j, j+1, \ldots, T\}} f(\bx^\toptzero)\right] \leq \min_{t\in\{j, j+1, \ldots, T\}} \Exp[f(\bx^\toptzero)]
$,
we can conclude that
\begin{equation}\label{equation:conv_sgpg}
\Exp[\fbest^{(T)} - f(\bx^*)] \leq \frac{\Exp\big[\normtwobig{\bx^{(j)} - \bx^*}^2\big] + \widetilde{L}^2 \sum_{t=j}^T \eta_t^2}{2 \sum_{t=j}^T \eta_t}.
\end{equation}
When $j=1$, we obtain
$
\Exp\big[\fbest^{(T)} - f(\bx^*)\big] \leq \frac{\normtwo{\bx^{(1)} - \bx^*}^2 + \widetilde{L}^2 \sum_{t=1}^T \eta_t^2}{2 \sum_{t=1}^T \eta_t}.
$
Therefore, if $\frac{\sum_{t=1}^{T} \eta_t^{2}}{\sum_{t=1}^{T} \eta_t} \rightarrow 0$, then $\Exp\big[\fbest^{(T)}\big] \rightarrow f(\bx^*)$ as $T \rightarrow \infty$, proving the first claim.

\paragraph{(ii).(a).} Setting $j = \left\lceil \frac{T}{2} \right\rceil$ in \eqref{equation:conv_sgpg} and using the definition of the half-squared diameter $\Omega$, we obtain
$$
\Exp\big[\fbest^{(T)}\big] - f(\bx^*) \leq \frac{\Omega + \frac{\widetilde{L}^{2}}{2} \sum_{t=\left\lceil \frac{T}{2} \right\rceil}^{T} \eta_t^{2}}{\sum_{t=\left\lceil \frac{T}{2} \right\rceil}^{T} \eta_t}
\,\,\,\,\longeqtextfour{\eta_t \triangleq \frac{\sqrt{2\Omega}}{\widetilde{L} \sqrt{t+1}}}\,\,\,\,
\frac{\widetilde{L}\sqrt{2\Omega}}{2} 
\frac{1 +  \sum_{t=\left\lceil \frac{T}{2} \right\rceil}^{T} \frac{1}{t+1}}{\sum_{t=\left\lceil \frac{T}{2} \right\rceil}^{T} \frac{1}{\sqrt{t+1}}}.
$$
The result follows by invoking Lemma~\ref{lemma:pgd_lem2}\ref{pgd_lem2_v2} with $\Phi\triangleq 1$. 

\paragraph{(ii).(b).}
Since $f$ is convex and $\{\eta_t\}_{t>0}$ is a nonincreasing sequence, \eqref{equation:conv_sgpg0} also shows that 
$$
\small
\begin{aligned}
\Exp&\big[f(\widebarbx^{(T)})\big] - f(\bx^*)
\leq 
\frac{1}{T}\sum_{t=1}^{T} \big(\Exp\big[f(\bx^\toptzero)\big] - f(\bx^*)\big) \\
&\leq 
\frac{1}{2T}  
\sum_{t=1}^{T}\left\{\frac{1}{\eta_t}\Exp\big[\normtwobig{\bx^\toptzero - \bx^*}^2\big] -\frac{1}{\eta_t}\Exp\big[\normtwobig{\bx^\toptone - \bx^*}^2\big]  + \eta_t \widetilde{L}^2 \right\}\\
&\leq 
\frac{1}{2T} 
\left\{\frac{1}{\eta_1}\Exp\big[\normtwobig{\bx^{(1)} - \bx^*}^2\big]+ \sum_{t=2}^{T}(\frac{1}{\eta_t}-\frac{1}{\eta_{t-1}})\Exp\big[\normtwobig{\bx^\toptzero - \bx^*}^2\big]  + \sum_{t=1}^{T}\eta_t \widetilde{L}^2 \right\}
\leq \frac{\Omega}{T\eta_T} + \frac{1}{2T}\sum_{t=1}^{T}\eta_t \widetilde{L}^2.
\end{aligned}
$$
Using Lemma~\ref{lemma:pgd_lem1}, we have $\sum_{t=1}^{T} \frac{1}{\sqrt{t}} \leq \int_{0}^T \frac{1}{\sqrt{x}} \,dx = 2\sqrt{T}$.
Plugging $\eta_t \triangleq \frac{\sqrt{2\Omega}}{\widetilde{L}\sqrt{t}}$ into the above inequality we have
$
\Exp\big[f(\widebarbx^{(T)})\big] - f(\bx^*)
\leq 
\frac{3\sqrt{2\Omega}\widetilde{L}}{2\sqrt{T}}.
$
This completes the proof.
\end{proof}


	
For strongly convex functions, we also have the following result.

\begin{theoremHigh}[SPGD for SC and ``Lipschitz" Functions]\label{theorem:conv_sgpg_sc}
Let $f: \sS \rightarrow \real$ be a proper closed and $\alpha$-strongly convex function, where $\sS$ is a closed and convex set. Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the stochastic projected subgradient method (Algorithm~\ref{alg:stopgd_gen}) with stepsizes $ \eta_t = \frac{2}{\alpha (t+1)} $ for the $t$-th iteration, and let $\left\{ \fbest^\toptzero \right\}_{t > 0}$ be the sequence of best achieved values.
Given the assumptions and vectors in Assumption~\ref{assumption:sto_assump}, we have:
\begin{enumerate}[(i)]
\item For any $ T > 0 $,
$
\Exp[\fbest^{(T)}] - f(\bx^*) \leq \frac{2 \widetilde{L}^2}{\alpha (T + 1)}
$, where $\bx^*$ is an optimizer of $f$.
\item For any $ t > 0 $, define the sequence of averages $\{\widetildebx^{(t)} \triangleq \sum_{i=1}^t \alpha_i^t \bx^{(t)}\}_{t>0}$,
where $ \alpha_i^t = \frac{2i}{t(t+1)} $ and $\sum_{i=1}^{t}\alpha_i^t=1$. Then for any $ T > 0 $,
\[
\Exp[f(\widetildebx^{(T)})] - f(\bx^*) \leq \frac{2 \widetilde{L}^2}{\alpha (T + 1)}.
\]
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:conv_sgpg_sc}]
\textbf{(i).} For any optimizer $ \bx^* $ and $ t > 0$,
$$
\small
\begin{aligned}
\Exp\big[\normtwobig{\bx^\toptone - \bx^*}^2 \mid \bx^\toptzero\big] 
&= \Exp\big[\normtwobig{\projectS(\bx^\toptzero - \eta_t \bg^\toptzero) - \projectS(\bx^*)}^2 \mid \bx^\toptzero\big] 
\stackrel{\dag}{\leq}  \Exp\big[\normtwobig{\bx^\toptzero - \eta_t \bg^\toptzero - \bx^*}^2 \mid \bx^\toptzero\big]  \\
&= \normtwobig{\bx^\toptzero - \bx^*}^2 - 2\eta_t \innerproduct{\Exp[\bg^\toptzero \mid \bx^\toptzero], \bx^\toptzero - \bx^*} + \eta_t^2 \Exp\big[\normtwobig{\bg^\toptzero}^2 \mid \bx^\toptzero\big] \\
&\stackrel{\ddag}{\leq} (1 - \alpha \eta_t)\normtwobig{\bx^\toptzero - \bx^*}^2 - 2\eta_t (f(\bx^\toptzero) - f(\bx^*)) + \eta_t^2 \widetilde{L}^2,
\end{aligned}
$$
where the inequality ($\dag$) follows from the nonexpansiveness property of the  projection operator (Theorem~\ref{theorem:proj_nonexpan}), and the inequality  ($\ddag$)  follows from the assumption in  Assumption~\ref{assumption:sto_assump} and the strong convexity of $f$ (Definition~\ref{definition:scss_func}):
$$
f(\bx^*) \geq f(\bx^\toptzero) + \innerproduct{\Exp[\bg^\toptzero \mid \bx^\toptzero], \bx^* - \bx^\toptzero} + \frac{\alpha}{2} \normtwo{\bx^\toptzero - \bx^*}^2.
$$
Rearranging terms and noting that $ \eta_t = \frac{2}{\alpha(t+1)} $, this implies
$$
f(\bx^\toptzero) - f(\bx^*) \leq \frac{\alpha(t-1)}{4} \normtwo{\bx^\toptzero - \bx^*}^2 - \frac{\alpha(t+1)}{4} \Exp\big[\normtwobig{\bx^\toptone - \bx^*}^2 \mid \bx^\toptzero\big] + \frac{1}{\alpha(t+1)} \widetilde{L}^2.
$$
Given that fact that  $\Exp[\Exp[\rX \mid \rY]] = \Exp[\rX]$ for any random variables $\rX$ and $\rY$, multiplying the above inequality  by $ t $ and taking expectation with respect to $ \bx^\toptzero $ yields that  
$$
\small
\begin{aligned}
	t\big(\Exp[f(\bx^\toptzero)] - f(\bx^*)\big)\leq & \frac{\alpha t(t-1)}{4} \Exp\big[\normtwobig{\bx^\toptzero - \bx^*}^2\big] - \frac{\alpha t(t+1)}{4} \Exp\big[\normtwobig{\bx^\toptone - \bx^*}^2\big] 
	+ \frac{t}{\alpha(t+1)} \widetilde{L}^2.
\end{aligned}
$$
Telescoping the sum over $ t = 1,2, \ldots,T$,
\begin{equation}\label{equation:conv_sgpg_sc1}
\small
\begin{aligned}
&\sum_{t=1}^{T} t (\Exp[f(\bx^\toptzero)] - f(\bx^*)) \leq  - \frac{\alpha}{4} T(T+1) \Exp\big[\normtwobig{\bx^{(T+1)} - \bx^*}^2\big] + \frac{\widetilde{L}^2}{\alpha} \sum_{t=1}^{T} \frac{t}{t+1} \leq \frac{\widetilde{L}^2 T}{\alpha}\\
&\quad\implies\quad 
\left( \sum_{t=1}^{T} t \right) (\Exp[\fbest^\toptzero] - f(\bx^*)) \leq \frac{\widetilde{L}^2 T}{\alpha}
\quad\implies\quad 
\Exp[\fbest^\toptzero] - f(\bx^*) \leq \frac{2 \widetilde{L}^2}{\alpha(T+1)},
\end{aligned}
\end{equation}
where the last two inequalities follows from the fact that $ \Exp[f(\bx^\toptzero)] \geq \Exp[\fbest^\toptzero] $ for all $ t \in \{1,2, \ldots,T\} $ and $ \sum_{t=1}^{T} t = \frac{T(T+1)}{2} $.

\paragraph{(ii).}  Using Jensen's inequality (Theorem~\ref{theorem:jensens_ineq}) and dividing \eqref{equation:conv_sgpg_sc1} by $ \frac{T(T+1)}{2} $, we have
$$
\Exp[f(\widetildebx^{(T)})] - f(\bx^*) = \Exp\bigg[f\bigg(\sum_{t=1}^{T} \alpha_t^T \bx^\toptzero\bigg)\bigg] - f(\bx^*) 
\leq \sum_{t=1}^T \alpha_t^T \big(\Exp[f(\bx^\toptzero)] - f(\bx^*)\big) \leq \frac{2 \widetilde{L}^2}{\alpha(T+1)}.
$$
This completes the proof.
\end{proof}

\subsection{Results of Stochastic Gradient Descent}




Recall that in many scenarios, the function $f(\bx)$ is denoted by a dataset $\mathcalD=\{\bs_1, \bs_2, \ldots, \bs_D\}$, allowing us to express both  $f(\bx)$ and  its gradient $\nabla f(\bx)$ as:
\begin{equation}
	f(\bx)\triangleq f(\mathcalD; \bx) = \frac{1}{D} \sum_{d=1}^{D} f(\bs_d; \bx)\triangleq\frac{1}{D} \sum_{d=1}^{D} f_d(\bx)
\quad \text{and}\quad 
\nabla f(\bx) \triangleq \frac{1}{D}  \sum_{d=1}^{D}\nabla f_d(\bx),
\end{equation} 
respectively.
When we follow the negative gradient direction of a single sample or a batch of samples iteratively, we obtain an estimate of the direction, a method known as \textit{stochastic gradient descent} (SGD) \citep{robbins1951stochastic}. SGD can be categorized into two types:
\begin{itemize}
\item \textbf{The strict SGD:} Computes the gradient using only one randomly selected data point per iteration, i.e., the the gradient is approximated as $\nabla f(\bx) \approx \nabla f_d(\bx)$.
\item \textbf{The mini-batch SGD:} Represents a compromise between GD and strict SGD, computing the gradient over a subset (mini-batch) of the dataset, i.e., the the gradient is approximated as  $\nabla f(\bx) \approx \frac{1}{\abs{\mathcalD}}  \sum_{d\in\mathcalD}\nabla f_d(\bx)$.
\end{itemize}
SGD is particularly useful when dealing with a large number of training entries (i.e., the data used for updating the model, while the data used for final evaluation is called the \textit{test entries or test data}).
In such cases, gradients from different input samples may cancel out, resulting in small updates. Within the SGD framework, the objective function is stochastic, consisting of a sum of subfunctions evaluated at various subsamples of the data. However, both GD and  SGD have the limitation of potentially getting trapped in local minima \citep{rutishauser1959theory}.


We then prove the convergence of  strict SGD, where for each iteration $t$, an index $s_t$ is selected ($s_t \in \mathcalD = \{1, 2, \ldots, D\}$) under the following assumption. The analysis for mini-batch SGD follows similarly.
\begin{assumption}[SGD Assumption]\label{assumption:sgd_assump}
Let $f:\real^n\rightarrow \real$ be a $\alpha$-strongly, and $\beta$-smooth convex function. We assume further that the stochastic second moment is uniformly bounded. That is, there exists a constant $B>0$ such that for any $\bx\in\real^n$ and $s_t$, it follows that 
$$
\Exp_{s_t}\big[ \normtwo{\nabla f_{s_t}(\bx)}^2 \big] \leq B^2.
$$
\end{assumption}


\begin{theoremHigh}[SGD for SC, SS, and ``Lipschitz" Functions: $\mathcalO({1}/{T})$]\label{theorem:conv_sgpg_sc_ss}
Let $f: \real^n \rightarrow \real$ be a proper closed, $\alpha$-strongly, and $\beta$-smooth convex function, where $\sS$ is a closed and convex set. Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the stochastic  gradient descent (SGD) method. 
Consider the assumptions and vectors in Assumption~\ref{assumption:sgd_assump}. 
Denoting $e_t \triangleq\normtwo{\bx^\toptzero - \bx^*}$, then for any $T>1$,
\begin{itemize}
\item If the stepsizes are constant  $\eta_t \triangleq \eta \in  (0, \frac{1}{2\alpha}) $ for all iterations,
$$
\Exp\big[f(\bx^{(T+1)}) - f(\bx^*)\big] \leq \frac{\beta}{2} \Exp[e_{T+1}^2] \leq \frac{\beta}{2} \left[ (1 - 2\eta\alpha)^T e_1^2 + \frac{\eta B^2}{2\alpha} \right],
$$
where the expectation is taken over all the sample indices $s_1, s_2, \ldots,s_T$.
\item If we use a decreasing stepsize
$
\eta_t = \frac{\sigma}{t + \gamma},
$
where $\sigma > \frac{1}{2\alpha}$, $\gamma > 0$, such that $\eta_1 \leq \frac{1}{2\alpha}$, then,
$$
\Exp\big[f(\bx^{(T)}) - f(\bx^*)\big] \leq \frac{\beta}{2} \Exp[e_T^2] \leq \frac{\beta}{2} \frac{\zeta}{\gamma + T},
$$
where  $\zeta  \triangleq \max \left\{ \frac{\sigma^2 B^2}{2\sigma\alpha - 1}, (\gamma + 1) e_1^2 \right\}$
and $e_1 \triangleq\normtwo{\bx^\topone - \bx^*}$.
\end{itemize}

\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:conv_sgpg_sc_ss}]
\textbf{(i).}
By the update formula of the stochastic gradient descent method,
$$
\begin{aligned}
e_{t+1}^2 &\triangleq\normtwo{\bx^\toptone - \bx^*}^2 \\
&= \normtwo{\bx^\toptzero - \eta \nabla f_{s_t}(\bx^\toptzero) - \bx^*}^2\\
&= e_{t}^2  - 2\eta \langle \nabla f_{s_t}(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle + \eta^2 \normtwo{\nabla f_{s_t}(\bx^\toptzero)}^2 \\
&= \normtwo{\bx^\toptzero - \bx^*}^2 - 2\eta \langle \nabla f_{s_t}(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle + \eta^2 \normtwo{\nabla f_{s_t}(\bx^\toptzero)}^2,
\end{aligned}
$$
where the second term $\langle \nabla f_{s_t}(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle$ is challenging to handle because both $s_t$ and $\bx^\toptzero$ are random variables.
By the property of conditional expectation $\Exp[\rX] = \Exp[\Exp[\rX\mid \rY]]$ for any random variables $\rX$ and $\rY$, we have
\begin{equation}\label{equation:sgd_gd_xtxstar_exp}
\begin{aligned}
&\Exp_{s_1, s_2, \ldots, s_t}[\langle \nabla f_{s_t}(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle]\\
&= \Exp_{s_1, s_2, \ldots, s_{t-1}}\big[\Exp_{s_t}[\langle \nabla f_{s_t}(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle \mid s_1, s_2 \ldots, s_{t-1}]\big]\\
&= \Exp_{s_1, s_2, \ldots, s_{t-1}}
\big[\langle \Exp_{s_t}[\nabla f_{s_t}(\bx^\toptzero) \mid s_1, s_2, \ldots, s_{t-1}], \bx^\toptzero - \bx^* \rangle\big]\\
&= \Exp_{s_1, s_2, \ldots, s_{t-1}}\big[\langle \nabla f(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle\big]
= \Exp_{s_1, s_2, \ldots, s_t}\big[\langle \nabla f(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle\big],
\end{aligned}
\end{equation}
where we use the fact that $\bx^\toptzero$ depends only on $s_1, s_2, \ldots, s_{t-1}$. Therefore, when fixing $s_1, s_2, \ldots, s_{t-1}$, $\bx^\toptzero$ becomes a constant variable. By taking expectations, we replace $\nabla f_{s_t}(\bx^\toptzero)$ with $\nabla f(\bx^\toptzero)$.
By the characterization theorem of SC (Theorem~\ref{theorem:charac_stronconv}) and the fact that $\nabla f(\bx^*)=\bzero$, 
$$
\langle \nabla f(\bx^\toptzero), \bx^\toptzero -\bx^* \rangle = \langle \nabla f(\bx^\toptzero) - \nabla f(\bx^*), \bx^\toptzero -\bx^* \rangle \geq \alpha \normtwo{\bx^\toptzero - \bx^*}^2.
$$
Combining the above results, using the uniform boundedness of the second moment of the stochastic gradient and the bound on $\langle \nabla f(\bx^\toptzero), \bx^\toptzero -\bx^* \rangle$, we obtain
\begin{equation}\label{equation:conv_sgpg_sc_ss_pe1}
\Exp_{s_1, s_2, \ldots, s_t}[e_{t+1}^2] \leq (1 - 2\eta\alpha) \Exp_{s_1, s_2, \ldots, s_t}\left[e_t^2\right] + \eta^2 B^2. 
\end{equation}
Applying induction on $t=1,2,\ldots,T+1$, we obtain
$$
\Exp_{s_1, s_2, \ldots, s_T}[e_{T+1}^2] \leq (1 - 2\eta\alpha)^T e_1^2 + \sum_{t=1}^{T-1} (1 - 2\eta\alpha)^t \eta^2 B^2. 
$$
Given that $ \eta \in  (0, \frac{1}{2\alpha}) $ (i.e., $2\eta\alpha<1$), it follows that for any $T>1$
$$
\sum_{t=1}^{T-1} (1 - 2\eta\alpha)^t < \sum_{t=1}^{\infty} (1 - 2\eta\alpha)^t = \frac{1}{2\eta\alpha}-1,
$$
whence we have 
$$
\Exp_{s_1, s_2, \ldots, s_T}[e_{T+1}^2] \leq (1 - 2\eta\alpha)^T e_1^2 + \frac{\eta B^2}{2\alpha}. 
$$
By the smoothness of $f$ and the fact that $\nabla f(\bx^*) = \bzero$,
$$
f(\bx^{(T+1)}) - f(\bx^*) \leq \innerproduct{\nabla f(\bx^*), \bx^{(T+1)} -\bx^*} + \frac{\beta}{2} \normtwobig{\bx^{(T+1)} - \bx^*}^2
=\frac{\beta}{2}e_{T+1}^2.
$$
Taking expectations, we obtain the desired result.
%$$
%\Exp\big[f(\bx^{(T+1)}) - f(\bx^*)\big] \leq \frac{\beta}{2} \Exp[e_{T+1}^2] \leq \frac{\beta}{2} \left[ (1 - 2\eta\alpha)^T e_1^2 + \frac{\eta B^2}{2\alpha} \right].
%$$
\paragraph{(ii).} If using a decreasing stepsize $\eta_t = \frac{\sigma}{t + \gamma}$, by \eqref{equation:conv_sgpg_sc_ss_pe1}, for any $t>0$:
\begin{equation}\label{equation:conv_sgpg_sc_ss_pe2}
	\Exp_{s_1, s_2, \ldots, s_t}[e_{t+1}^2] \leq (1 - 2\eta_t \alpha) \Exp_{s_1, s_2, \ldots, s_t}[e_t^2] + \eta_t^2 B^2.
\end{equation}
We prove the result by induction. When $T = 1$, by the definition of $\zeta$, the claim holds trivially. Assume that the formula holds for $t$. For simplicity, define $\widetildet \triangleq \gamma + t$, then $\eta_t = \frac{\sigma}{\widetildet}$. By the induction hypothesis and \eqref{equation:conv_sgpg_sc_ss_pe2},
$$
\begin{aligned}
\Exp[e_{t+1}^2] 
&\leq \left(1 - \frac{2\sigma\alpha}{\widetildet}\right) \frac{\zeta}{\widetildet} + \frac{\sigma^2 B^2}{\widetildet^2}
= \frac{\zeta }{\widetildet}  + \frac{\sigma^2 B^2 - 2\sigma\alpha\zeta}{\widetildet^2}
\leq \frac{\zeta}{\widetildet + 1}
%&= \frac{\widetildet - 1}{\widetildet^2} \zeta - \frac{2\sigma\alpha - 1}{\widetildet^2} \zeta + \frac{\sigma^2 B^2}{\widetildet^2}
%\leq \frac{\zeta}{\widetildet + 1},
\end{aligned}
$$
where the last inequality follows from the definition of $\zeta$ such that $\sigma^2 B^2 - 2\sigma\alpha\zeta<0$. Therefore, the claim also holds for $t + 1$. This completes the proof.
\end{proof}



It can be observed that with a fixed stepsize, the algorithm does not guarantee convergence because the right-hand side lacks a term that decreases with $T$. 
However, by employing a decreasing stepsize, the convergence rate can achieve $\mathcalO\left(\frac{1}{T}\right)$.




\section{Variance/Noise Reduction Techniques}




Consider a constant stepsize $\eta_t\triangleq\eta$ for all iterations. We compare the pure gradient descent method with the stochastic gradient descent method.
Under the conditions specified in Assumption~\ref{assumption:sgd_assump}, by the characterization theorem of SC (Theorem~\ref{theorem:charac_stronconv})  and SS Property-O (Theorem~\ref{theorem:equi_gradsch_smoo}), for the gradient descent method, we have:
\begin{align*}
e_{t+1}^2 &\triangleq \normtwobig{\bx^\toptone - \bx^*}^2 = \normtwo{\bx^\toptzero - \eta \nabla f(\bx^\toptzero) - \bx^*}^2 \\
&= e_t^2 - 2\eta \innerproduct{\nabla f(\bx^\toptzero), \bx^\toptzero - \bx^*} + \eta^2 \normtwobig{\nabla f(\bx^\toptzero)}^2 \\
&\leq (1 - 2\eta \alpha) e_t^2 + \eta^2 \normtwobig{\nabla f(\bx^\toptzero)}^2  \\
&\leq (1 - 2\eta \alpha + \eta^2 \beta^2) e_t^2. 
\end{align*}
While for the stochastic gradient descent method, by leveraging properties of conditional expectations:
\begin{align*}
\Exp[e_{t+1}^2] 
&= \Exp\big[\normtwobig{\bx^\toptone - \bx^*}^2\big] = \Exp\big[\normtwobig{\bx^\toptzero - \eta \nabla f_{s_t}(\bx^\toptzero) - \bx^*}^2\big] \\
&= \Exp[e_t^2] - 2\eta \Exp\big[\langle \nabla f_{s_t}(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle\big] + \eta^2 \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero)}^2\big] \\
&\stackrel{\dag}{=} \Exp[e_t^2] - 2\eta \Exp\big[\langle \nabla f(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle\big] + \eta^2  \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \nabla f(\bx^\toptzero) + \nabla f(\bx^\toptzero) }^2\big] \\
%&\leq (1 - 2\eta \alpha) \Exp[e_t^2] + \eta^2 \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \nabla f(\bx^\toptzero)}^2\big] \\
&\stackrel{\ddag}{\leq} (1 - 2\eta \alpha) \Exp[e_t^2] + \eta^2 \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \nabla f(\bx^\toptzero) + \nabla f(\bx^\toptzero) }^2\big] \\
&\stackrel{*}{\leq} \underbrace{(1 - 2\eta \alpha + \eta^2 \beta^2) \Exp[e_t^2]}_{\triangleq A} + \underbrace{\eta^2 \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \nabla f(\bx^\toptzero)}^2\big]}_{\triangleq B}.
\end{align*}
where the equality ($\dag$) follows from \eqref{equation:sgd_gd_xtxstar_exp}, the inequality ($\ddag$) follows from the the characterization theorem of SC (Theorem~\ref{theorem:charac_stronconv}), and the inequality ($*$) follows from the SS Property-O (Theorem~\ref{theorem:equi_gradsch_smoo}).


Therefore, the main difference between the two methods  lies in the term $B$, which represents the variance in gradient estimation.  
This results in the convergence rate of the stochastic gradient algorithm being approximately $\mathcalO\left(\frac{1}{T}\right)$ (refer to  Theorem~\ref{theorem:gd_sc_ss} for GD and Theorem~\ref{theorem:conv_sgpg_sc_ss} for SGD). 
However, in many machine learning applications, the actual convergence rate of the stochastic gradient algorithm may be faster due to lower precision requirements for solutions and relatively smaller variance in early stages, i.e., $B \ll A$, leading to an approximate linear convergence rate initially; as iterations increase, variance grows, eventually resulting in a $\mathcalO\left(\frac{1}{T}\right)$ convergence rate. 
Therefore, to achieve faster convergence rates, our main objective is to reduce the variance term $B$. 
Below are three algorithms designed to reduce variance in SGD:
\begin{itemize}
	\item SAG (stochastic average gradient) \citep{schmidt2017minimizing}.
	\item SAGA \citep{defazio2014saga}.
	\item SVRG (stochastic variance reduced gradient) \citep{johnson2013accelerating}.
\end{itemize}
Compared to traditional stochastic gradient methods, these algorithms utilize previously computed information to decrease variance, ultimately achieving a linear convergence rate \citep{bottou2018optimization, liu2020optimization}. Below is a comparison among the update rules for SAG, SAGA, and SVRG, where notations will be clarified in the following sections:
\begin{subequations}
\begin{align}
\textbf{(SAG)}: \quad \bx^\toptone &\leftarrow \bx^\toptzero - \eta_t \bigg( \frac{1}{D} \big(\nabla f_{s_t}(\bx^\toptzero) - \bg_{s_t}^{(t-1)}\big) + \frac{1}{D} \sum_{i=1}^D \bg_i^{(t-1)} \bigg);\\
\textbf{(SAGA)}: \quad 	\bx^\toptone &\leftarrow \bx^\toptzero - \eta_t \bigg( \nabla f_{s_t}(\bx^\toptzero) - \bg_{s_t}^{(t-1)} + \frac{1}{D} \sum_{i=1}^D \bg_i^{(t-1)} \bigg);\\
\textbf{(SVRG)}: \quad 	\bx^\toptone&\leftarrow \bx^\toptzero - \eta_t \bigg( \nabla f_{s_t}(\bx^\toptzero) - \nabla f_{s_t}(\widetildebx^{(j)}) + \frac{1}{D} \sum_{i=1}^D \nabla f_i(\widetildebx^{(j)})\bigg).
\end{align}
\end{subequations}
In the following, we only discuss the results for strict SGD; the results for mini-batch SGD follow analogously.
\subsection{SAG Algorithm}

In the stochastic gradient descent  method, each iteration only uses the random gradient of the current point, discarding all previously computed gradients immediately after use.
However, as iterations approach convergence, the random gradient from the previous step can serve as a good estimate for the gradient at the current iteration point. The \textit{stochastic average gradient (SAG)} algorithm leverages this concept by recording all previously calculated random gradients and averaging them with the newly computed gradient to form an estimate for the next step.
Specifically, the SAG algorithm reserves memory space to store $D = \abs{\mathcalD}$ random gradients:
$$
[\bg_1^\toptzero, \bg_2^\toptzero, \ldots, \bg_D^\toptzero],
$$
which are used to record the latest random gradients for each sample. At the $t$-th update, if the sampled index is $s_t$, 
then after calculating the new random gradient,  $\bg_{s_t}^\toptzero$ is updated to this new value, while all other  $\bg_i^\toptzero$ remain unchanged from their previous values. 
Each time, the SAG algorithm updates the gradient direction using the average of all $\bg_i^\toptzero$. 
Formally, its mathematical iterative formula is given by:
\begin{equation}
	\bx^\toptone \leftarrow \bx^\toptzero - \frac{\eta_t}{D} \sum_{i=1}^D \bg_i^\toptzero, 
	\quad
	\text{ where }\bg_i^\toptzero = 
	\begin{cases} 
		\nabla f_{s_t}(\bx^\toptzero), & i = s_t, \\
		\bg_i^{(t-1)}, & \text{otherwise}.
	\end{cases} 
\end{equation}
That is, only update the random gradient for the $s_t$-th sample, with others directly carrying over from the previous step. Therefore, the SAG iterative formula can also be expressed as:
\begin{equation}\label{equation:sag}
\textbf{(SAG)}: \quad \bx^\toptone \leftarrow \bx^\toptzero - \eta_t \bigg( \frac{1}{D} \big(\nabla f_{s_t}(\bx^\toptzero) - \bg_{s_t}^{(t-1)}\big) + \frac{1}{D} \sum_{i=1}^D \bg_i^{(t-1)} \bigg). 
\end{equation}
For the SAG algorithm, the initial values of $\{\bg_i^\toptzero\}$ can be set to zero vectors or random gradient vectors with a mean of zero.
We present the convergence analysis of the SAG algorithm:
\begin{theoremHigh}[Convergence of the SAG Algorithm]
Under Assumption~\ref{assumption:sgd_assump}, taking a constant stepsize $\eta_t \triangleq\eta\triangleq\frac{1}{16\beta}$, and initializing   $\bg_i^\toptzero$, $i\in\{1,2,\ldots,D\}$ as zero vectors,  for any $t>0$, we have
\begin{equation}
	\Exp\big[f(\bx^\toptzero)\big] - f(\bx^*) \leq \left(1 - \min\left\{\frac{\alpha}{16\beta}, \frac{1}{8D}\right\}\right)^t C_0,
\end{equation}
where $C_0 \triangleq f(\bx^{(1)}) - f(\bx^*) + \frac{4\beta}{D}\normtwo{\bx^{(1)} - \bx^*}^2 + \frac{\sigma^2}{16\beta}$, $\sigma^2 = \frac{1}{D}\sum_{i=1}^{D} \normtwo{\nabla f_i(\bx^*)}^2$, and $\bx^*$ is an optimal point.
Additionally, if the function is not strongly convex under  Assumption~\ref{assumption:sgd_assump}, then 
\begin{equation}
\Exp\left[f\Big(\frac{1}{t} \sum_{i=1}^{t}\bx^{(i)}\Big)\right] - f(\bx^*) \leq  \frac{32D}{t}C_0.
\end{equation}
\end{theoremHigh}
The detailed proof of this theorem can be found in \citet{schmidt2017minimizing}, which involves finding a Lyapunov function for a nonlinear stochastic dynamical system.
The theorem demonstrates that the SAG algorithm indeed achieves a linear convergence rate. However, a limitation of the SAG algorithm is its requirement to store $D$ gradient vectors, which can be prohibitively expensive when the sample size $D$  is large. 
Consequently, the SAG algorithm is rarely used in practice, especially in the era of large language models. Nonetheless, its core idea has inspired the development of many practical algorithms.



\subsection{SAGA Algorithm}

The \textit{SAGA} algorithm builds upon the SAG method. One known limitation of the SAG algorithm is that the conditional expectation of the random gradient at each step does not equal the true gradient. To address this, the SAGA algorithm employs an unbiased gradient vector for its update direction. Its iterative formula is given by:
\begin{equation}
\textbf{(SAGA)}: \quad 	\bx^\toptone \leftarrow \bx^\toptzero - \eta_t \bigg( \nabla f_{s_t}(\bx^\toptzero) - \bg_{s_t}^{(t-1)} + \frac{1}{D} \sum_{i=1}^D \bg_i^{(t-1)} \bigg). 
\end{equation}
It can be demonstrated that the gradient direction used in each iteration is unbiased, i.e.,
\begin{equation}
	\Exp \left[ \nabla f_{s_t}(\bx^\toptzero) - \bg_{s_t}^{(t-1)} + \frac{1}{D} \sum_{i=1}^D \bg_i^{(t-1)} \mid \bx^\toptzero \right] = \nabla f(\bx^\toptzero).
\end{equation}
TSimilar to SAG, the SAGA algorithm also achieves a linear convergence rate. Specifically, we have the following result:
\begin{theoremHigh}[Convergence of the SAGA Algorithm]
Under Assumption~\ref{assumption:sgd_assump}, taking a constant stepsize $\eta_t \triangleq\eta\triangleq \frac{1}{2(\alpha D + \beta)}$. Letting $e_t \triangleq \normtwo{\bx^\toptzero - \bx^*}$,  for any $t > 0$, we have
\begin{equation}
\Exp[e_t^2] \leq \left(1 - \frac{\alpha}{2(\alpha D + \beta)}\right)^t 
\left( e_1^2 + \frac{D\big(f(\bx^{(1)}) - f(\bx^*) - \langle \nabla f(\bx^*), \bx^{(1)} - \bx^* \rangle\big)}{\alpha D + \beta} \right). 
\end{equation}
%Additionally, if the function is not strongly convex under  Assumption~\ref{assumption:sgd_assump}, taking a constant stepsize $\eta_t \triangleq\eta\triangleq  \frac{1}{3\beta}$ can also be taken, and similar convergence results will be obtained.
\end{theoremHigh}
The proof of this theorem also involves finding a Lyapunov function for a nonlinear stochastic dynamical system, details of which can be found in \citet{defazio2014saga}. For brevity, we will not delve into these specifics here.


\subsection{SVRG Algorithm}

Unlike the SAG and SAGA algorithms, the \textit{stochastic variance reduced gradient (SVRG)} algorithm reduces variance by periodically caching the full gradient. 
Specifically, in the stochastic gradient descent method, a checkpoint is established every $m$ iterations to compute the full gradient. During the subsequent $m$ iterations, this full gradient serves as a reference point to reduce variance. 
Let $\widetildebx^{(j)}$ be the $j$-th checkpoint, then we need to compute the full gradient at point $\widetildebx^{(j)}$
\begin{equation}
\nabla f(\widetildebx^{(j)}) = \frac{1}{D} \sum_{i=1}^D \nabla f_i(\widetildebx^{(j)}), 
\quad \text{ with }\quad 
\widetildebx^{(j)} \triangleq \frac{1}{m} \sum_{t=j-m}^{j-1} \bx^{(t)}
\end{equation}
And in the subsequent iterations (i.e., $t\in\{j, j+1, \ldots, j+m-1\}$), we can use the direction $\bp^\toptzero$ as the update direction:
\begin{equation}
\bp^\toptzero \triangleq \nabla f_{s_t}(\bx^\toptzero) - \big(\nabla f_{s_t}(\widetildebx^{(j)}) - \nabla f(\widetildebx^{(j)})\big), \quad t\in\{j, j+1, \ldots, j+m-1\},
\end{equation}
where $s_t \in \{1, 2, \ldots, D\}$ is a randomly selected sample. To see why we define the vector $\bp^\toptzero$, note that given $s_1, s_2, \ldots, s_{t-1}$, $\bx^\toptzero$ and $\widetildebx^{(j)}$ are both fixed values, whence we have 
\begin{equation}
\begin{aligned}
&\Exp[\bp^\toptzero \mid s_1, s_2, \ldots, s_{t-1}] \\
=& \Exp\big[\nabla f_{s_t}(\bx^\toptzero) \mid \bx^\toptzero\big] - \Exp\big[\nabla f_{s_t}(\widetildebx^{(j)}) - \nabla f(\widetildebx^{(j)}) \mid s_1, s_2, \ldots, s_{t-1}\big] \\
=& \nabla f(\bx^\toptzero) - \bzero = \nabla f(\bx^\toptzero).
\end{aligned}
\end{equation}
Thus, $\bp^\toptzero$ is an unbiased estimate of $\nabla f(\bx^\toptzero)$. Therefore, if we use $\nabla f_{s_t}(\widetildebx^{(j)})$ to estimate $\nabla f(\widetildebx^{(j)})$,  the term $\nabla f_{s_t}(\widetildebx^{(j)}) - \nabla f(\widetildebx^{(j)})$ can be regarded as the error of the gradient estimation. At each step of the stochastic gradient iteration, this term is used to correct $\nabla f_{s_t}(\bx^\toptzero)$.
Consequently, the SVRG update becomes:
\begin{equation}\label{equation:svrg_update}
	\textbf{(SVRG)}: \quad 
\begin{aligned}
\bx^\toptone 
&\leftarrow\bx^\toptzero - \eta_t\bp^\toptzero\\
&= \bx^\toptzero - \eta_t \bigg( \nabla f_{s_t}(\bx^\toptzero) - \big(\nabla f_{s_t}(\widetildebx^{(j)}) - \nabla f(\widetildebx^{(j)})\big)\bigg).
\end{aligned} 
\end{equation}
Unlike the SAG  and  SAGA algorithms, the SVRG algorithm does not require storage space for recording $D$ gradient vectors, but it requires calculating the full gradient every $m$ steps and an additional gradient $\nabla f_{s_t}(\widetildebx^{(j)})$ at each iteration.

To analyze the variance, an additional assumption is needed here:
\begin{equation}
\normtwo{\nabla f_i(\bx) - \nabla f_i(\by)} \leq \beta\normtwo{\bx - \by}, \quad i = 1, 2, \ldots, D,
\end{equation}
which means that each sub-function of $f(\bx)$ is  $\beta$-smooth. 
Let $\bx^*$ denote the minimum point of $f(\bx)$ and $e_t \triangleq \normtwo{\bx^\toptzero - \bx^*}$ represent  the distance between $\bx^\toptzero$ and $\bx^*$. Then,
\begin{equation}\label{equation:svag_ineqclo}
\begin{aligned}
& \Exp\big[\normtwobig{\bp^\toptzero}^2\big] = \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \big(\nabla f_{s_t}(\widetildebx^{(j)}) - \nabla f(\widetildebx^{(j)})\big)}^2\big] \\
= & \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \nabla f_{s_t}(\widetildebx^{(j)}) + \nabla f(\widetildebx^{(j)}) + \nabla f_{s_t}(\bx^*) - \nabla f_{s_t}(\bx^*)}^2\big] \\
\leq & 2 \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \nabla f_{s_t}(\bx^*)}^2\big] + 2 \Exp\big[\normtwobig{\nabla f_{s_t}(\widetildebx^{(j)}) - \nabla f(\widetildebx^{(j)}) - \nabla f_{s_t}(\bx^*)}^2\big] \\
\leq & 2\beta^2 \Exp[e_t^2] + 2 \Exp\big[\normtwobig{\nabla f_{s_t}(\widetildebx^{(j)}) - \nabla f_{s_t}(\bx^*)}^2\big] \\
\leq & 2\beta^2 \Exp[e_t^2] + 2\beta^2 \Exp\big[\normtwobig{\widetildebx^{(j)} - \bx^*}^2\big],
\end{aligned}
\end{equation}
where the first inequality follows from the fact that $\normtwo{\ba + \bb}^2 \leq 2\normtwo{\ba}^2 + 2\normtwo{\bb}^2$ for any vectors $\ba,\bb$.

The inequality \eqref{equation:svag_ineqclo} shows that  if $\bx^\toptzero$ and $\widetildebx^{(j)}$ are very close to $\bx^*$, the variance of the gradient estimation is  small. Obviously, frequently updating $\widetildebx^{(j)}$ can decrease the variance, but this also increases the number of times the full gradient must be calculated.
Below is the analysis of the convergence of the SVRG algorithm.
\begin{theoremHigh}[Convergence of the SVRG Algorithm]\label{theorem:conv_svrg}
Let each sub-function $f_i(\bx)$ be a differentiable convex and  $\beta$-smooth function for $i\in\{1,2,\ldots,D\}$, and let the function $f(\bx)=\sum_{i=1}^{D} f_i(\bx)$ be $\alpha$-strongly convex. Consider the SGD algorithm with an SVRG update using a constant stepsize $\eta_t\triangleq\eta \in \left(0, \frac{1}{2\beta}\right]$, and suppose $m$ is sufficiently large such that
\begin{equation}
\rho = \frac{1}{\alpha \eta (1 - 2\beta \eta) m} + \frac{2\beta \eta}{1 - 2\beta \eta} < 1.
\end{equation}
Let  $\bx^*$ be any optimal point of $f = \tfrac{1}{D} \sum_{i=1}^{D} f_i$.
Then, for $j>m$:
\begin{equation}
\Exp \big[f(\widetildebx^{(j)}) - f(\bx^*)\big] \leq \rho \Exp \big[f(\widetildebx^{(j-m)}) - f(\bx^*)\big],
\end{equation}
i.e., a linear rate of convergence.
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:conv_svrg}]
Let $e_t \triangleq \normtwo{\bx^\toptzero - \bx^*}$ be the distance between $\bx^\toptzero$ and $\bx^*$, and let $\widetildebx^{(j)}$ be the $j$-th checkpoint ($j$ is a scalar multiple of $m$). By the update rule of SVRG and the convexity of $f$, for $t\in\{j, j+1, \ldots, j+m-1\}$,
\begin{equation}\label{equation:conv_svrg_et1et}
\begin{aligned}
\Exp[e_{t+1}^2] 
&= \Exp\big[\normtwobig{\bx^\toptone - \bx^*}^2\big] = \Exp\big[\normtwobig{\bx^\toptzero - \eta \bp^\toptzero - \bx^*}^2\big] \\
&= \Exp[e_t^2] - 2\eta \Exp\big[\langle \bp^\toptzero, \bx^\toptzero - \bx^* \rangle\big] + \eta^2 \Exp\big[\normtwobig{\bp^\toptzero}^2\big] \\
&= \Exp[e_t^2] - 2\eta \Exp\big[\langle \nabla f(\bx^\toptzero), \bx^\toptzero - \bx^* \rangle\big] + \eta^2 \Exp\big[\normtwobig{\bp^\toptzero}^2\big] \\
&\leq \Exp[e_t^2] - 2\eta \Exp\big[(f(\bx^\toptzero) - f(\bx^*))\big] + \eta^2 \Exp\big[\normtwobig{\bp^\toptzero}^2\big].
\end{aligned}
\end{equation}
Construct the following auxiliary function for each index $i\in\{1,2,\ldots, D\}$:
\begin{equation}
	\phi_i(\bx) \triangleq f_i(\bx) - f_i(\bx^*) - \innerproduct{\nabla f_i(\bx^*), \bx - \bx^*},
\end{equation}
Notice that $\phi_i(\bx)$ is also a convex  and $\beta$-smooth function. 
Since $\nabla \phi_i(\bx) - \nabla \phi_i(\bx^*) = \nabla f_i(\bx) -  \nabla f_i(\bx^*) $ (noting that $\bx^*$ is an optimal point of $\phi_i$ or $f$, but may not be an optimal point of $f_i$), invoking  Theorem~\ref{theorem:smoo_prop2_bound} on $\phi_i$, we have
\begin{equation}
\frac{1}{2\beta}\normtwo{\nabla f_i(\bx) - \nabla f_i(\bx^*)}^2 = \frac{1}{2\beta} \normtwo{\nabla \phi_i(\bx)}^2 \leq  \phi_i(\bx) - \phi_i(\bx^*).
\end{equation}
Taking the sum over $i=1,2,\ldots, D$, and noting that $\nabla f(\bx^*) = \frac{1}{D} \sum_{i=1}^{D} \nabla f_i(\bx^*)= \bzero$, we obtain
\begin{equation}\label{equation:svrg_smoo_sin}
\frac{1}{D} \sum_{i=1}^D \normtwo{\nabla f_i(\bx) - \nabla f_i(\bx^*)}^2 \leq 2\beta \left[ f(\bx) - f(\bx^*) \right], \quad \forall \bx\in\real^n.
\end{equation}
Using the same procedure as \eqref{equation:svag_ineqclo}, for $t\in\{j, j+1, \ldots, j+m-1\}$, we have the upper bound expression for the second moment of $\bp^\toptzero$:
\begin{equation}\label{equation:svrg_decom}
\Exp\big[\normtwobig{\bp^\toptzero}^2\big] \leq 2 \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \nabla f_{s_t}(\bx^*)}^2\big] + 2 \Exp\big[\normtwobig{\nabla f_{s_t}(\widetildebx^{(j)}) - \nabla f_{s_t}(\bx^*)}^2\big].
\end{equation}
For the first term on the right-hand side, we have
\begin{equation}
\begin{aligned}
&\Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \nabla f_{s_t}(\bx^*)}^2\big] 	
= \Exp\left[ \Exp\big[\normtwobig{\nabla f_{s_t}(\bx^\toptzero) - \nabla f_{s_t}(\bx^*)}^2 \mid s_1, s_2, \ldots, s_{t-1}\big] \right] \\
=& \Exp\left[ \frac{1}{D} \sum_{i=1}^D \normtwo{\nabla f_i(\bx^\toptzero) - \nabla f_i(\bx^*)}^2 \right]
\leq 2\beta \Exp\big[f(\bx^\toptzero) - f(\bx^*)\big],
\end{aligned}
\end{equation}
where the first equality follows from the property of conditional expectation $\Exp[\rX] = \Exp[\Exp[\rX\mid \rY]]$, 
the second equality follows from the definition of expectation, and the last inequality uses inequality \eqref{equation:svrg_smoo_sin}. Similarly, for the second term on the right-hand side of \eqref{equation:svrg_decom}, invoking  Theorem~\ref{theorem:smoo_prop2_bound} on $f_{s_t}$, we have
\begin{equation}
\Exp\big[\normtwobig{\nabla f_{s_t}(\widetildebx^{(j)}) - \nabla f_{s_t}(\bx^*)}^2\big] \leq 2\beta \Exp[f(\widetildebx^{(j)}) - f(\bx^*)].
\end{equation}
Combining the preceding two inequalities, \eqref{equation:svrg_decom} becomes 
\begin{equation}
\Exp\big[\normtwobig{\bp^\toptzero}^2\big] \leq 4\beta \left( \Exp[f(\bx^\toptzero) - f(\bx^*)] + \Exp[f(\widetildebx^{(j)}) - f(\bx^*)] \right).
\end{equation}
Substituting the upper bound of $\Exp\big[\normtwobig{\bp^\toptzero}^2\big]$ into the estimate of $\Exp[e_{t+1}^2]$ in \eqref{equation:conv_svrg_et1et}, we have
\begin{equation}
\begin{aligned}
\Exp[e_{t+1}^2] \leq& \Exp[e_t^2] - 2\eta \Exp\big[f(\bx^\toptzero) - f(\bx^*)\big] + \eta^2 \Exp\big[\normtwobig{\bp^\toptzero}^2\big] \\
\leq& \Exp[e_t^2] - 2\eta(1 - 2\eta \beta) \Exp\big[f(\bx^\toptzero) - f(\bx^*)\big] 
+ 4\beta\eta^2 \Exp[f(\widetildebx^{(j)}) - f(\bx^*)].
\end{aligned}
\end{equation}
Taking the sum over  $t\in\{j, j+1, \ldots, j+m-1\}$, and noting that $\bx^{(j)} = \widetildebx^{(j)}$ (an initialization for each subroutine, i.e., initialization every $m$ steps), we can obtain
\begin{equation}
\begin{aligned}
&\Exp[e_{j+m}^2] + 2\eta(1 - 2\eta \beta) \sum_{t=j}^{j+m-1} \Exp\big[f(\bx^\toptzero) - f(\bx^*)\big] \\
\leq& \Exp\big[\normtwobig{\widetildebx^{(j)} - \bx^*}^2\big] + 4\beta\eta^2 m \Exp\big[f(\widetildebx^{(j)}) - f(\bx^*)\big] \\
\leq& \frac{2}{\alpha} \Exp\big[f(\widetildebx^{(j)}) - f(\bx^*)\big] + 4\beta\eta^2 m \Exp\big[f(\widetildebx^{(j)}) - f(\bx^*)\big],
%=(\frac{2}{\alpha}  + 4\beta\eta^2 m) \Exp\big[f(\widetildebx^{(j)}) - f(\bx^*)\big],
\end{aligned}
\end{equation}
where the last inequality follows from the SC Property-II (Theorem~\ref{theorem:exi_close_sc}).
Since $\widetildebx^{(j+m)} = \frac{1}{m} \sum_{t=j}^{j+m-1} \bx^{(t)}$, using the convexity of $f$, the above inequality becomes
\begin{equation}
\begin{aligned}
\Exp\big[f(\widetildebx^{(j+m)}) - f(\bx^*)\big] 
&\leq \frac{1}{m} \sum_{t=j}^{j+m-1} \Exp\big[f(\bx^\toptzero) - f(\bx^*)\big]\\
&\leq \frac{1}{2\eta(1 - 2\eta \beta)m} \left( \frac{2}{\alpha} + 4\beta\eta^2m \right) \Exp\big[f(\widetildebx^{(j)}) - f(\bx^*)\big].
%&= \rho \Exp\big[f(\widetildebx^{(j)}) - f(\bx^*)\big].
\end{aligned}
\end{equation}
This completes the proof.
\end{proof}





\section{Stochastic Optimization Algorithms}
We provided an overview of the number of papers utilizing stochastic optimization algorithms in Table~\ref{table:stochastic-optimizers}. This section introduces the development of these algorithms.
Note that for all algorithms, at iteration $\bx^\toptzero$, our goal is to determine the step $\bd^\toptzero$ leading to the update:
$$
\bx^\toptone \leftarrow \bx^\toptzero + \bd^\toptzero.
$$
We provide the update formulas for various stochastic optimization algorithms; for an illustrative comparison of these algorithms in the context of deep neural networks, refer to \citet{hinton2012neural2, zeiler2012adadelta, lu2022adasmooth, lu2022gradient}.
For convenience, unless specified otherwise, the gradient notations used in this section, $\bg^\toptzero \triangleq \nabla f(\bx^\toptzero)$, can either represent a strict gradient (using only one sample) or a mini-batch gradient (using a subset of samples).
In Section~\ref{section:learning-rate-annealing}, we will discuss learning rate (the term ``learning rate" is used more commonly than ``stepsize" in the machine learning and deep learning communities) annealing or warmup strategies for training deep neural networks or transformers; however, for the purposes of this section, we assume the learning rate remains constant: $\eta_t=\eta$ for all iterations.

\subsection{Momentum }\label{section:sgd_momentum}
If the cost surface is non-spherical, learning can be significantly slowed down because the learning rate must be kept small to avoid divergence along directions with steep curvature \citep{polyak1964some, rumelhart1986learning, qian1999momentum, sutskever2013importance}. 
SGD with momentum (that can be applied to full batch or mini-batch learning) attempts to accelerate learning by leveraging previous steps when suitable, thereby improving convergence rates in deep networks or transformers. The core idea behind momentum is to speed up learning along dimensions where the gradient consistently points in the same direction, while slowing down along dimensions where the gradient's sign frequently changes.

Figure~\ref{fig:momentum_gd} illustrates updates for basic GD (or SGD), showing consistent updates along dimension $x_1$ and zigzag movements along dimension $x_2$ continues to change in a zigzag pattern.  
Momentum-based GD  (or SGD) keeps track of past parameter updates with exponential decay, and its update rule from iteration $t$ to iteration $t+1$  is as follows:
\begin{equation}\label{equation:momen_sgd}
\begin{aligned}
\bd^\toptzero &\leftarrow \rho \bd^\toptminus - \eta  \nabla f(\bx^\toptzero);\\
\bx^\toptone &\leftarrow \bx^\toptzero + \bd^\toptzero,
\end{aligned}
\end{equation}
where the algorithm blends the current update with the past update using a parameter $\rho$, called the \textit{momentum parameter}.
This mechanism ensures that the change in parameters is proportional to the negative gradient plus the previous weight change, effectively smoothing and accelerating the updates; the added \textit{momentum term} acts as both a smoother and an accelerator.
The momentum parameter $\rho<1$ acts  as a \textit{decay constant}, meaning that although $\bd^{(1)}$ might influence $\bd^{(100)}$, its effect diminishes over time. Typically, $\rho$ is set to  0.9 by default.
Momentum mimics inertia in physics, indicating that each update depends not just on the gradient descent (\textit{dynamic term}) but also retains a component related to the previous update's direction (\textit{momentum}).

As mentioned, we previously introduced GD with momentum in Equation~\eqref{equation:gd_with_momentum} (along with its application in quadratic models in Section~\ref{section:quadratic-in-momentum}). The difference here is that the gradient is computed using a single sample or a batch of samples in SGD with momentum.


\begin{figure}[h]
\centering  
\vspace{-0.15cm}  
\subfigtopskip=2pt  
\subfigbottomskip=2pt  
\subfigcapskip=-5pt 
\subfigure[
A two-dimensional surface plot for quadratic convex function.
]{\label{fig:alsgd12}
\includegraphics[width=0.47\linewidth]{./imgs/momentum_surface.pdf}}
\subfigure[The contour plot of $f(\bx)$. The red dot is the optimal point.]{\label{fig:alsgd22}
\includegraphics[width=0.44\linewidth]{./imgs/momentum_contour.pdf}}
\caption{Figure~\ref{fig:alsgd12} shows a function surface and its contour plot (\textcolor{mylightbluetext}{blue}=low, \textcolor{mydarkyellow}{yellow}=high), where the upper graph is the surface, and the lower one is its projection (i.e., contour). The quadratic function $f(\bx) = \frac{1}{2} \bx^\top \bA \bx - \bb^\top \bx + c$ is from parameters $\bA=\scriptsize\begin{bmatrix}
4 & 0\\ 0 & 40
\end{bmatrix}$, $\bb=[12,80]^\top $, and $c=103$. Or equivalently, $f(\bx)=2(x_1-3)^2 + 20(x_2-2)^2+5$ and $\nabla f(\bx)=[4x_1-12, 8x_2-16]^\top$.}
\label{fig:momentum-contour}
\end{figure}

\begin{figure}[h]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt  
\subfigcapskip=-5pt  
\subfigure[Optimization without momentum. A higher learning rate
may result in larger parameter updates in the dimension across the valley (direction of $x_2$) which could lead to oscillations back and forth across the valley.]{\label{fig:momentum_gd}
	\includegraphics[width=0.47\linewidth]{./imgs/mom_surface_lrate40_gd-0_xy-2-5.pdf}}
\subfigure[Optimization with momentum. Though the gradients along the valley (direction of $x_1$) are much smaller than the gradients across the valley (direction of $x_2$), they are
typically in the same direction. Thus, the momentum term
accumulates to speed up movement, dampens oscillations, and causes us to barrel through narrow valleys, small humps and (local) minima.]{\label{fig:momentum_mum}
	\includegraphics[width=0.47\linewidth]{./imgs/mom_surface_lrate40_gd-1_xy-2-5_mom-2.pdf}}
\caption{The loss function is shown in Figure~\ref{fig:momentum-contour}. The starting point is $[-2, 5]^\top$. After 5 iterations, the squared loss from basic GD is 42.72, and the loss from GD with momentum is 35.41 in this simple case. The learning rates $\eta$ are set to be 0.04 in both cases.}
\label{fig:momentum_gd_compare}
\end{figure}

Momentum excels especially in scenarios with ravine-shaped loss curves, characterized by significantly steeper slopes in one dimension than another (see Figure~\ref{fig:momentum-contour}, which  are common near local minima in deep neural networks). In such areas, basic GD or SGD struggles due to oscillations across the valley. Momentum helps by accelerating gradients towards the optimal direction and reducing oscillations, as shown in Figure~\ref{fig:momentum_gd_compare}.

As shown by the toy example in Figure~\ref{fig:momentum_gd}, GD tends to oscillate across the narrow ravine since the negative gradient will point down one of the steep sides rather than along the ravine towards the optimum. 
By incorporating a fraction $\rho$ of the previous update vector into the current update, momentum accelerates progress when consecutive updates align (e.g., the blue arrow regions in Figure~\ref{fig:momentum_mum}). Conversely, if updates oppose, the algorithm tends to follow the predominant direction of recent updates. Thus, momentum accumulates contributions along the long axis while averaging out oscillations along the short axis, ultimately facilitating smoother navigation through narrow valleys and local minima.


%Momentum exhibits superior performance, particularly in the presence of a ravine-shaped loss curve. 
%A ravine refers to an area where the surface curves are significantly steeper in one dimension than in another (see the surface and contour curve in Figure~\ref{fig:momentum-contour}, i.e., a long, narrow valley). 
%Ravines are common near local minima in deep neural networks, and basic GD or SGD has trouble navigating them. As shown by the toy example in Figure~\ref{fig:momentum_gd}, GD tends to oscillate across the narrow ravine since the negative gradient will point down one of the steep sides rather than along the ravine towards the optimum. Momentum helps accelerate gradients in the correct direction and dampens oscillations, as evident in the example shown in  Figure~\ref{fig:momentum_mum}. 



%As mentioned earlier, the momentum method is achieved by incorporating a fraction $\rho$ of the update vector from the previous time step into the current update vector. When $\bd^\toptzero$ and $\bd^\toptminus$ are in the same direction, the momentum accelerates the update step (e.g., the \textcolor{mylightbluetext}{blue} arrow regions in Figure~\ref{fig:momentum_mum}).
%Conversely, if they are in the opposite directions, the algorithm tends to update in the former direction if $\bx$ has been updated in this direction for many iterations. 
%To be more concrete, in Figure~\ref{fig:momentum_gd},  considering the \textcolor{mylightbluetext}{blue} starting point, and then looking at the \textcolor{cyan}{cyan} point we get to after one step in the step of the update without momentum, they have gradients that are pretty much equal and opposite. As a result, the gradient across the ravine has been canceled out. But the gradient along the ravine has not canceled out. 
%Therefore, along the ravine, we're going to keep
%building up speed, and so, after the momentum method has settled down, it'll
%tend to go along the bottom of the ravine.
%
%From this figure, the problem with the basic GD is that the gradient is big in the direction in which we only want to travel a small distance; and the gradient is small in the direction in which we want to travel a large distance. However, one can easily find that the momentum term helps average out the oscillation along the short axis while at the same time adds up contributions along the long axis. In other words, although it starts off by following the gradient, however, when it has velocity, it no longer does steepest descent. We call this \textit{momentum}, which makes it keep going in the previous direction.











\subsection{Nesterov Momentum}
\textit{Nesterov momentum}, also referred to as \textit{Nesterov accelerated gradient (NAG)}, introduces a variant of the traditional momentum update method and has been increasingly adopted in recent years. The fundamental concept of Nesterov momentum involves predicting the future position of the parameter vector $\bx^\toptzero$ based on the momentum term discussed previously.
Given that the momentum alone (ignoring the gradient term) will adjust the parameter vector by $\rho \bd^\toptminus$, it becomes logical to compute the gradient at this predicted future position $\bx^\toptzero + \rho \bd^\toptminus$. 
This lookahead approach considers where the parameters are likely to be updated next, rather than their current position $\bx^\toptzero$.
Consequently, the gradient is calculated at this anticipated position, resulting in the following update rule:
$$
\begin{aligned}
\bd^\toptzero &\leftarrow \rho \bd^\toptminus - \eta \nabla f(\bx^\toptzero + \rho \bd^\toptminus);\\
\bx^\toptone &\leftarrow \bx^\toptzero + \bd^\toptzero,
\end{aligned}
$$



\begin{figure}[h]
\centering   
\vspace{-0.35cm}  
\subfigtopskip=2pt  
\subfigbottomskip=2pt  
\subfigcapskip=-5pt  
\subfigure[Momentum: evaluate gradient at the current position $\bx^\toptzero$, and the  momentum is about to carry us to the tip of the green arrow.]{\label{fig:nesterov1}
\includegraphics[width=0.47\linewidth]{./imgs/momentum.pdf}}
\subfigure[Nesterov momentum: evaluate the gradient at this ``looked-ahead" position.]{\label{fig:nesterov2}
\includegraphics[width=0.47\linewidth]{./imgs/momentum-nesterov.pdf}}
\caption{Comparison of momentum and Nesterov momentum.}
\label{fig:momentum_nesterov_comp}
\end{figure}


Figure~\ref{fig:momentum_nesterov_comp} illustrates the distinction between standard momentum and Nesterov momentum. In standard momentum, the gradient is evaluated at the current position $\bx^\toptzero$, with the momentum carrying the parameters to the tip of the green arrow. In contrast, Nesterov momentum evaluates the gradient at the ``lookahead" position, providing an opportunity to correct the direction before making the actual step.
This ``peeking ahead" feature is designed to mitigate excessive velocities by assessing the objective function values in the proposed search direction. Essentially, Nesterov momentum first makes a large jump in the direction of the accumulated gradient, then measures the gradient at the new location and adjusts accordingly. Conversely, standard momentum initially moves according to the current gradient, followed by a significant leap in the direction of the previous gradient accumulation.

To use an analogy, if you're going to make a speculative move (gamble), it's more effective to make the move first and then correct based on the outcome, rather than correcting first and then gambling  \citep{hinton2012neural2}. According to \citet{sutskever2013importance}, Nesterov momentum offers a theoretically superior performance bound compared to standard gradient descent in convex, non-stochastic settings.




\subsection{AdaGrad}
The learning rate annealing procedure (will be introduced in Section~\ref{section:learning-rate-anneal}) adjusts a single global learning rate applied uniformly across all parameter dimensions. To address this limitation, \citet{duchi2011adaptive} introduced \textit{AdaGrad}, which updates the learning rate on a per-dimension basis. In AdaGrad, each parameter's learning rate is influenced by its update history, allowing parameters with fewer past updates to be adjusted more aggressively through a higher learning rate. Specifically, parameters that have received fewer updates in the past are assigned higher learning rates currently.
Formally, AdaGrad's update rule is given by:
\begin{equation}\label{equation:update:adagrad}
\begin{aligned}
\bd^\toptzero &= -  \frac{ \eta}{ \sqrt{\sum_{\tau=1}^t (\bg^{(\tau)})^2 +\epsilon} }\hadaprod \bg^\toptzero ,
\end{aligned}
\end{equation}
where $\hadaprod$ denotes the Hadamard product, $\epsilon$ serves as a smoothing term to stabilize division, $\eta$ represents a shared global learning rate, $(\bg^{(\tau)})^2$ denotes the element-wise square $\bg^{(\tau)}\hadaprod \bg^{(\tau)}$, and the denominator computes the $\ell_2$ norm of the sum of all previous squared gradients on a per-dimension basis. 
Though the global learning rate $\eta$ is used across all dimensions, each dimension has its own dynamic learning rate regulated by the $\ell_2$ norm of accumulated gradient magnitudes. Since this dynamic learning rate grows with the inverse of the accumulated gradient magnitudes, larger gradient magnitudes correspond to smaller learning rates, while smaller gradient magnitudes result in larger learning rates. 
Consequently, the aggregated squared magnitude of partial derivatives for each parameter throughout the algorithm acts similarly to learning rate annealing; see Section~\ref{section:learning-rate-anneal}.


AdaGrad's simplicity makes it straightforward to implement, as shown in the following Python snippet:
\begin{python}
# Assume the gradient dx and parameter vector x 
cache += dx**2
x += - learning_rate * dx / np.sqrt(cache + 1e-8)
\end{python}
One advantage of AdaGrad is its ability to reduce the need for tuning the learning rate, which is controlled by accumulated gradient magnitudes. However, AdaGrad suffers from a significant drawback due to the unbounded accumulation of squared gradients in the denominator. Since every added term is positive, the accumulated sum grows continuously during training, causing the per-dimension learning rate to diminish over time and eventually approach zero, halting further training.

Moreover, AdaGrad can be sensitive to the initialization of parameters and their corresponding gradients. If initial gradient magnitudes are large, per-dimension learning rates will remain low throughout training. While increasing the global learning rate may help, this makes AdaGrad sensitive to the choice of the global learning rate.

Furthermore, AdaGrad assumes that parameters with fewer updates should have larger learning rates and those with more updates should have smaller learning rates. This approach considers only information from squared gradients or the absolute values of gradients, neglecting total movement (i.e., the sum of updates; see Section~\ref{section:adaer} for a further discussion).

To summarize, AdaGrad's main limitations include:
\begin{itemize}
\item Continuous decay of learning rates throughout training.
\item The necessity for manually selecting a global learning rate.
\item  Consideration of only the absolute value of gradients.
\end{itemize}


%On the other hand, AdaGrad partly eliminates the need to tune the learning rate %which is 
%controlled by the accumulated gradient magnitude. 
%However, AdaGrad faces a significant drawback related to the unbounded accumulation of squared gradients in the denominator. Since every added term is positive, the accumulated sum 
%keeps growing or exploding during every training step. This in turn causes the per-dimension learning rate to shrink and eventually decrease throughout training and become infinitesimally small, eventually falling to zero and stopping training any more. 
%Moreover, since the magnitudes of gradients are factored out in
%AdaGrad, this method can be sensitive to the initialization
%of the parameters and the corresponding gradients. If the initial magnitudes of the gradients are large or infinitesimally huge, the per-dimension learning rates will be low for the remainder of training. 
%This can be partly combated by increasing the global learning rate, making the AdaGrad method sensitive to the choice of learning rate. 
%Further, AdaGrad assumes the parameter with fewer updates should favor a larger learning rate; and one with more movement should employ a smaller learning rate. This makes it
%consider only  the information from squared gradients or the absolute value of the gradients. And thus AdaGrad does not include information from the total move (i.e., the sum of updates; in contrast to the sum of absolute updates).
%
%To be more succinct, AdaGrad exhibits  the following primary drawbacks:
%1) the continual decay of learning rates throughout training;
%2) the need for a manually selected global learning rate;
%3) considering only the absolute value of gradients.

\subsection{RMSProp}

\textit{RMSProp} is an optimization algorithm that builds upon AdaGrad, aiming to address its primary limitation---AdaGrad's tendency to aggressively reduce the learning rate over time \citep{hinton2012neural, zeiler2012adadelta}. The core idea behind RMSProp is straightforward: it limits the influence of past gradients by considering only the most recent ones instead of accumulating all past gradients up to the current time step $t$.
However, storing a fixed number $w$ of previous squared gradients can be inefficient. To circumvent this, RMSProp employs an exponentially decaying average of the squared gradients, similar to how momentum accumulates past gradients but with a focus on their magnitude.

We first delve into the specifics of RMSProp's formulation. At any given iteration $t$, the running average of the squared $\Exp[\bg^2]^\toptzero$ is calculated as:
%RMSProp is an extension of AdaGrad designed to overcome the main weakness of AdaGrad \citep{hinton2012neural, zeiler2012adadelta}. The original idea of RMSProp is simple: it restricts the window of accumulated past gradients to some fixed size $w$ rather than $t$ (i.e., current time step). 
%However, since storing $w$ previous squared gradients is inefficient, the RMSProp introduced in \citet{hinton2012neural, zeiler2012adadelta} implements this accumulation as an exponentially decaying average of the squared gradients. This is very similar to the idea of momentum term (or decay constant).
%
%
%
%We first discuss the specific formulation of RMSProp.
%Assume at time $t$,  the running average, denoted by $\Exp[\bg^2]^\toptzero$, is computed as follows:
\begin{equation}\label{equation:adagradwin}
\Exp[\bg^2]^\toptzero = \rho \Exp[\bg^2]^\toptminus + (1 - \rho) (\bg^\toptzero)^2,
\end{equation}
where $\rho$ is a decay constant similar to that used in the momentum method, and $(\bg^\toptzero)^2$ indicates the element-wise square $\bg^\toptzero\hadaprod \bg^\toptzero$. 
This formula effectively blends the existing estimate with the latest gradient information. Initially, the running average is set to $\bzero$, which might introduce bias early on; however, this bias diminishes over time. Notably, older gradients' impact decreases exponentially throughout the training process.


Let $\rms[\bg]^\toptzero \triangleq \sqrt{\Exp[\bg^2]^\toptzero + \epsilon}$, where again a constant $\epsilon$ is added for numerical stability. Similar to the AdaGrad update in \eqref{equation:update:adagrad},  the resulting stepsize can be obtained as follows:
\begin{equation}\label{equation:rmsprop_update}
\bd^\toptzero=- \frac{\eta}{\rms[\bg]^\toptzero}  \hadaprod \bg^\toptzero.
\end{equation}
%where again $\hadaprod$ is the element-wise vector multiplication. 
%Similar to momentum, $\rho$ in AdaGradWin is also set to 0.9 by default. 

The \textit{exponential moving average (EMA)} concept underpins the above formulation. In EMA, $1-\rho$ is also known as the smoothing constant (SC),which can be approximated by $\frac{2}{N+1}$, where $N$ represents the number of past values considered in the average \citep{lu2022adasmooth}:
\begin{equation}\label{equation:ema_smooting_constant}
\text{SC}=1-\rho \approx \frac{2}{N+1},
\end{equation}
which establishes a relationship among  different variables: the decay constant $\rho$, the smoothing constant (SC), and the period $N$.
For instance , if $\rho=0.9$, then $N=19$. That is, roughly speaking, $\Exp[\bg^2]^\toptzero $ at iteration $t$ is roughly equal to the moving average of the past 19 squared gradients and the current one (i.e., the moving average of a total of 20 squared gradients).
The relationship in \eqref{equation:ema_smooting_constant} though is not discussed in the original paper of \citet{zeiler2012adadelta}, it is important to decide the lower bound of the decay constant $\rho$. Typically, a time period of $N=3$ or 7 is thought to be a relatively small frame making the lower bound of decay constant $\rho=0.5$ or 0.75; as $N\rightarrow \infty$, the decay constant $\rho$ approaches $1$.


While AdaGrad excels in convex settings, RMSProp demonstrates superior performance in non-convex scenarios. When applied to a non-convex function, e.g., to train a deep neural network, the learning trajectory can pass through many different structures and eventually arrives at a region that is a locally convex bowl. AdaGrad shrinks the learning rate according to the entire history of the squared partial derivative leading to an infinitesimally small learning rate before arriving at such a convex structure. While RMSProp discards ancient squared gradients to address this problem.

Despite these advantages, RMSProp has limitations---it only considers the magnitude of gradients and does not account for their direction. This characteristic can result in suboptimal learning rates near local minima.

RMSProp was independently developed by \citet{hinton2012neural} and \citet{zeiler2012adadelta}, both addressing AdaGrad's rapid decrease in learning rates. Hinton suggests setting $\rho=0.9$ and the global learning rate  $\eta=0.001$. Furthermore, RMSProp can be integrated with the Nesterov momentum method \citep{goodfellow2016deep}, enhancing its capabilities for deep learning applications; the comparison between the two is presented in Algorithm~\ref{alg:rmsprop} and Algorithm~\ref{alg:rmsprop_nesterov}..


%AdaGrad is designed to converge rapidly when applied to a convex function; while RMSProp performs better in non-convex settings. When applied to a non-convex function to train a neural network, the learning trajectory can pass through many different structures and eventually arrives at a region that is a locally convex bowl. AdaGrad shrinks the learning rate according to the entire history of the squared partial derivative leading to an infinitesimally small learning rate before arriving at such a convex structure. While RMSProp discards ancient squared gradients to address this problem.
%
%However, we can find that the RMSProp still only considers the absolute value of gradients and a fixed number of past squared gradients is not flexible. 
%This limitation can cause a small learning rate near (local) minima as we will discuss in the sequel. 
%
%The RMSProp is developed independently by  \citet{hinton2012neural} and \citet{zeiler2012adadelta}, both of which are stemming from the need to resolve AdaGrad's radically diminishing per-dimension learning rates. 
%\citet{hinton2012neural} suggest setting $\rho$ to 0.9 and the global learning rate $\eta$ to default to $0.001$. The RMSProp further can be combined into the Nesterov momentum method \citep{goodfellow2016deep}, where the comparison between the two is presented in Algorithm~\ref{alg:rmsprop} and Algorithm~\ref{alg:rmsprop_nesterov}.

%\noindent
%\begin{minipage}[t]{0.495\linewidth}
\begin{algorithm}[h] 
\caption{RMSProp}
\label{alg:rmsprop}
\begin{algorithmic}[1]
\State {\bfseries Input:} Initial parameter $\bx^{(1)}$, constant $\epsilon$;
\State {\bfseries Input:} Global learning rate $\eta$, by default $\eta=0.001$;
\State {\bfseries Input:} Decay constant $\rho$;
\State {\bfseries Input:} Initial accumulated squared gradients $\Exp[\bg^2]^{(0)} = \bzero $;
\For{$t=1:T$ } 
\State Compute gradient $\bg^\toptzero \leftarrow \nabla f(\bx^\toptzero)$;
\State Compute running estimate $	\Exp[\bg^2]^\toptzero \leftarrow \rho \Exp[\bg^2]^\toptminus + (1 - \rho) (\bg^\toptzero)^2;$
\State Compute step $\bd^\toptzero \leftarrow- \frac{\eta}{\sqrt{\Exp[\bg^2]^\toptzero+\epsilon }}  \hadaprod \bg^\toptzero$;
\State Apply update $\bx^\toptone \leftarrow \bx^\toptzero + \bd^\toptzero$;
\EndFor
\State {\bfseries Return:} resulting parameters $\bx^\toptzero$, and the loss $f(\bx^\toptzero)$.
\end{algorithmic}
\end{algorithm}
%\end{minipage}%
%\hfil 
%\begin{minipage}[t]{0.495\linewidth}
\begin{algorithm}[h] 
\caption{RMSProp with Nesterov Momentum}
\label{alg:rmsprop_nesterov}
\begin{algorithmic}[1]
\State {\bfseries Input:} Initial parameter $\bx^{(1)}$, constant $\epsilon$;
\State {\bfseries Input:} Global learning rate $\eta$, by default $\eta=0.001$;
\State {\bfseries Input:} Decay constant $\rho$, \textcolor{mylightbluetext}{momentum constant $\alpha$};
\State {\bfseries Input:} Initial accumulated squared gradients $\Exp[\bg^2]^{(0)} = \bzero $, and update step $\bd^{(0)}=\bzero$;
\For{$t=1:T$ } 
\State Compute interim update $\widetilde{\bx}^\toptzero \leftarrow \bx^\toptzero + \alpha \bd^\toptminus$;
\State Compute interim gradient $\bg^\toptzero \leftarrow \nabla f(\textcolor{mylightbluetext}{\widetilde{\bx}^\toptzero})$;
\State Compute running estimate $	\Exp[\bg^2]^\toptzero \leftarrow \rho \Exp[\bg^2]^\toptminus + (1 - \rho) (\bg^\toptzero)^2;$
\State Compute step $\bd^\toptzero \leftarrow\textcolor{mylightbluetext}{\alpha \bd^\toptminus}- \frac{\eta}{\sqrt{\Exp[\bg^2]^\toptzero +\epsilon }}  \hadaprod \bg^\toptzero$;
\State Apply update $\bx^\toptone \leftarrow \bx^\toptzero + \bd^\toptzero$;
\EndFor
\State {\bfseries Return:} resulting parameters $\bx^\toptzero$, and the loss $f(\bx^\toptzero)$.
\end{algorithmic}
\end{algorithm}
%\end{minipage}

\subsection{AdaDelta}\label{section:adadelta}

\citet{zeiler2012adadelta} pointed out an inconsistency in the units of the learning rate used in RMSProp (as well as in basic SGD, momentum, and AdaGrad). To address this issue and leverage insights from second-order methods (discussed in Section~\ref{section:new_methods}), the author proposed rearranging the Hessian matrix to adjust the involved quantities.
Although calculating or approximating the Hessian matrix is computationally intensive, it provides valuable curvature information that aids optimization. Additionally, the units in Newton's method are consistent when using the Hessian. Given the Hessian matrix $\bH$, the update rule in Newton's method can be expressed as follows:
\begin{equation}
\bd^\toptzero = - \bH^{-1} \bg^\toptzero \sim \frac{\text{units}(\nabla f(\bx^\toptzero))}{\text{units}(\nabla^2 f(\bx^\toptzero))},
\end{equation}
where $\text{units}(\cdot)$ denotes the units of the matrix or vector.
This implies 
\begin{equation}
\frac{1}{\text{units}(\nabla^2 f(\bx^\toptzero))} = \frac{\bd^\toptzero}{\text{units}(\nabla f(\bx^\toptzero))},
\end{equation}
i.e., the units of the inverse Hessian matrix can be approximated by the right-hand side term of the above equation. Since the RMSProp update in \eqref{equation:rmsprop_update} already includes $\rms[\bg]^\toptzero$ in the denominator, i.e., the units of the gradients. Introducing an additional  unit of the order of $\bd^\toptzero$ in the numerator can match the same order as Newton's method. 
Therefore, we define another exponentially decaying average of the update steps:
\begin{equation}
\begin{aligned}
\rms[\bd]^\toptzero &\triangleq 	\sqrt{\Exp[\bd^2]^\toptzero } 
= \sqrt{\rho \Exp[\bd^2]^\toptminus + (1 - \rho) (\bd^\toptzero)^2  }.
\end{aligned}
\end{equation}
Given that the value of $\bd^\toptzero$ for the current iteration is unknown but the curvature can be locally smoothed,  $\rms[\bd]^\toptzero$ can be approximated by $\rms[\bd]^\toptminus$. 
So we can use an estimation of $\frac{1}{\text{units}(\nabla^2 f(\bx^\toptzero))} $ to replace  the computationally expensive $\bH^{-1}$:
\begin{equation}
\bH^{-1}\sim \frac{1}{\text{units}(\nabla^2 f(\bx^\toptzero))} 
=\frac{\bd^\toptzero}{\text{units}(\nabla f(\bx^\toptzero))} 
\sim \frac{\rms[\bd]^\toptminus}{\rms[\bg]^\toptzero}.
\end{equation}
This approximation of the inverse Hessian uses only RMS measures of $\bg$ and $\bd$, leading to an update step with matched units:
%This presents an approximation to the diagonal Hessian, using only RMS measures of $\bg$ and $\bd$, and results in the update step whose units are matched:
\begin{equation}
\bd^\toptzero = -\frac{\rms[\bd]^\toptminus}{\rms[\bg]^\toptzero} \hadaprod \bg^\toptzero.
\end{equation}
The idea behind AdaDelta, derived from second-order methods, alleviates the need to manually select a learning rate.
Andrej Karpathy has developed a web demo that allows users to compare the convergence rates among SGD, SGD with momentum, AdaGrad, and AdaDelta \footnote{see https://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html.}.

When using RMSProp or AdaDelta, it is important to note that although the accumulated squared gradients in the denominator help balance per-dimension learning rates, restarting training from saved checkpoints may lead to poor performance in the first few batches due to insufficient gradient data smoothing the denominator. For example, Figure~\ref{fig:er-rmsprop_epochstart} shows loss deterioration after each epoch when weights are loaded from checkpoints. While this does not significantly impact overall training progress, a more effective strategy involves saving $\Exp[\bg^2]^\toptzero$ alongside the neural network weights.


%The idea of AdaDelta, derived  from the second-order method, alleviates the annoying choosing of learning rate. 
%Meanwhile, a web demo developed by Andrej Karpathy can be explored to find the convergence rates among SGD, SGD with momentum, AdaGrad, and AdaDelta \footnote{see https://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html.}.
%
%A crucial consideration when employing the RMSProp or AdaDelta method is to carefully notice that though the accumulated squared gradients in the denominator can compensate for the per-dimension learning rates, if we save the checkpoint of the neural networks at the end of some epochs and want to re-tune the parameter by loading the weights from the checkpoint, the first few batches of the re-tuning can perform poorly since there are not enough squared gradients to smooth the denominator. 
%A particular example is shown in Figure~\ref{fig:er-rmsprop_epochstart}, where we save the weights and load them after each epoch; loss deterioration is observed after each epoch.
%While this doesn't significantly impact overall training progress as the loss can still go down from that point, a more effective  choice involves saving the $\Exp[\bg^2]^\toptzero$ along with the weights of the neural networks. 
\begin{figure}[h]
%\begin{SCfigure}%[H]
\centering
\includegraphics[width=0.6\textwidth]{imgs/rmsprop_epochstart.pdf}
\caption{Illustration  of tuning parameter after each epoch by loading the weights. 
Loss deterioration is observed at the start of each retraining phase following weight loading.
%We save the weights and load them after each epoch such that there are step-points while re-training after each epoch. That is, loss deterioration is observed after each epoch.
	}
\label{fig:er-rmsprop_epochstart}
%\end{SCfigure}
\end{figure}
\index{Loss deterioration}



\index{Effective ratio}
\index{Exponential moving average}
\subsection{AdaSmooth}\label{section:adaer}

In this section, we will discuss the effective ratio, derived from previous updates in the stochastic optimization process. We will explain how it can be used to achieve adaptive learning rates per dimension through a flexible smoothing constant, hence the name AdaSmooth. This concept builds upon the RMSProp method to address two primary limitations: 1) focusing solely on the absolute value of gradients rather than overall movement in each dimension; and 2) the necessity for manual selection of hyperparameters.
\paragrapharrow{Effective Ratio (ER).}
\citet{kaufman2013trading, kaufman1995smarter} suggested replacing the smoothing constant in the exponential moving average (EMA) formula with a constant based on the \textit{efficiency ratio} (ER).  
This indicator is designed to measure the \textit{strength of a trend}, ranging from $-1.0$ to $+1.0$, with larger magnitudes indicating stronger upward or downward trends.
\citet{lu2022reducing} demonstrated that the ER can also reduce overestimation and underestimation in time series forecasting.
Given the window size $M$ and a series $\{h^{(1)}, h^{(2)}, \ldots, h^{(T)}\}$, the ER is calculated as follows:
\noindent
\begin{equation}
\begin{aligned}
e^\toptzero  &\triangleq \frac{s^\toptzero}{n^\toptzero}\triangleq \frac{h^\toptzero - h^{(t-M)}}{\sum_{i=0}^{M-1} \abs{h^{(t-i)} - h^{(t-1-i)}}}= \frac{\text{Total move for a period}}{\text{Sum of absolute move for each bar}},
\end{aligned}
\end{equation}
where $e^\toptzero$ is the ER of the series at time $t$. 
In strong trends, the ER approaches 1 in absolute value (i.e., the input series is moving in a certain direction, either up or down); if there is no directed movement, it remains close to 0.
Instead of calculating the ER base on the closing price of the underlying asset (the time series problem), we want to calculate the ER of the moving direction in the update methods for each parameter.
Specifically, we focus on how much each parameter moves from its initial point in each period, regardless of direction. Therefore, only the absolute value of the ER is considered. For parameters in the method, the ER is computed as:
\begin{equation}\label{eqution:signoiase-er-delta}
\begin{aligned}
\be^\toptzero  \triangleq \frac{\bs^\toptzero}{\bn^\toptzero}&\triangleq \frac{| \bx^\toptzero -  \bx^{(t-M)}|}{\sum_{i=0}^{M-1} \abs{\bx^{(t-i)} -  \bx^{(t-1-i)}}}= \frac{\abs{ \sum_{i=0}^{M-1} \bd^{(t-1-i)}}}{\sum_{i=0}^{M-1} \abs{\bd^{(t-1-i)}}},
\end{aligned}
\end{equation}
where $\be^\toptzero \in \real^n$, and its $i$-th element $e_{i}^\toptzero$ is in the range of $ [0, 1]$ for all $i$ in $[1,2,\ldots, n]$. A large value of $e_{i}^\toptzero$ indicates the descent method in the $i$-th dimension is moving consistently in a certain direction; while a small value approaching 0 means the parameter in the $i$-th dimension is moving in a zigzag pattern, alternating between positive and negative movements. In practice,   $M$ is chosen based on the batch index within each epoch. 
That is, $M=1$ if the training is in the first batch of each epoch; and $M=M_{\text{max}}$ if the training is in the last batch of the epoch, where $M_{\text{max}}$ is the maximal number of batches per epoch. In other words, $M$ ranges in $[1, M_{\text{max}}]$ for each epoch. Therefore, the value of $e_{i}^\toptzero$ indicates the movement of the $i$-th parameter in the most recent epoch. Or even more aggressively, the window can range from 0 to the total number of batches seen during the entire training progress. The adoption of the adaptive window size $M$ rather than a fixed one has the benefit that we do not need to keep the past $M+1$ steps $\{ \bx^{(t-M)},  \bx^{(t-M+1)}, \ldots,  \bx^\toptzero\}$ to calculate the signal and noise vectors $\{\bs^\toptzero,\bn^\toptzero\}$ in Equation~\eqref{eqution:signoiase-er-delta} since they can be obtained in an accumulated fashion.

\paragrapharrow{AdaSmooth.}\label{section:adaer-after-er}
The AdaSmooth method adjusts the smoothing constant dynamically based on the magnitude of the ER. If the magnitude of ER is small (close to 0), indicating zigzag movements, AdaSmooth uses a longer averaging period to slow down parameter updates. 
Conversely, when the magnitude of ER  is large (near 1), 
the path in that dimension is moving in a certain direction (not zigzag), and the learning actually is happening and the descent is moving in a correct direction, where the learning rate should be assigned to a relatively large value for that dimension; thus, the AdaSmooth method tends to choose a small period, which leads to a small compensation in the denominator (of, for example, \eqref{equation:rmsprop_update}); since the gradients in the closer periods are small in magnitude when it's near the (local) minima. 

A particular example is shown in Figure~\ref{fig:er-explain}, where the descent is moving in a certain direction, and the gradient in the closer periods is small in magnitude; if we choose a larger period to compensate for the denominator, the descent will be slower due to the large factored denominator.
In short, we want a smaller period to calculate the exponential average of the squared gradients in \eqref{equation:adagradwin} if the update is moving in a certain direction without a zigzag pattern; while when the parameter is updated in a zigzag fashion, the period for the exponential average should be larger \citep{lu2022adasmooth}.



%If the ER in magnitude of each parameter is small (approaching 0), the movement in this dimension is zigzag, the  \textit{AdaSmooth} method tends to use a long period average as the scaling constant to slow down the movement in that dimension. When the absolute ER per-dimension is large (tend to 1), the path in that dimension is moving in a certain direction (not zigzag), and the learning actually is happening and the descent is moving in a correct direction, where the learning rate should be assigned to a relatively large value for that dimension. Thus, the AdaSmooth tends to choose a small period, which leads to a small compensation in the denominator; since the gradients in the closer periods are small in magnitude when it's near the (local) minima. A particular example is shown in Figure~\ref{fig:er-explain}, where the descent is moving in a certain direction, and the gradient in the near periods is small in magnitude; if we choose a larger period to compensate for the denominator, the descent will be slower due to the large factored denominator.
%In short, we want a smaller period to calculate the exponential average of the squared gradients in \eqref{equation:adagradwin} if the update is moving in a certain direction without a zigzag pattern; while when the parameter is updated in a zigzag fashion, the period for the exponential average should be larger \citep{lu2022adasmooth}.

\begin{figure}[h]
%\begin{SCfigure}%[H]
\centering
\includegraphics[width=0.6\textwidth]{imgs/alsgd_specialcase.pdf}
\caption{Demonstration of how the effective ratio works. Stochastic optimization tends to move a large step when it is far from the (local) minima; and a relatively small step when it is close to the (local) minima.}
\label{fig:er-explain}
%\end{SCfigure}
\end{figure}

The obtained value of ER is incorporated into the exponential smoothing formula.  
To enhance our approach, we aim to dynamically adjust the time period $N$ discussed in \eqref{equation:ema_smooting_constant} to be a smaller value when the magnitude of ER tends to 1; or to a larger value when the ER moves towards 0. When $N$ is small, $\text{SC} $ is known as a ``\textit{fast SC};" otherwise, $\text{SC} $ is known as a ``\textit{slow SC}." 

For example, let the small time period be $N_1=3$, and the large time period be $N_2=199$.
The smoothing ratio for the fast movement must align with that of  EMA with period $N_1$ (``fast SC" = $\frac{2}{N_1+1}$ = 0.5); and for the period of no trend, the EMA period must be equal to $N_2$ (``slow SC" = $\frac{2}{N_2+1}$ = 0.01). 
Thus the new changing smoothing constant is introduced, called the ``\textit{scaled smoothing constant}" (SSC), denoted by a vector $\bc^\toptzero\in \real^n$:
$$
\bc^\toptzero =  ( \text{fast SC} - \text{slow SC}) \times \be^\toptzero   + \text{slow SC}.
$$
By \eqref{equation:ema_smooting_constant}, we can define the \textit{fast decay constant} $\rho_1=1-\frac{2}{N_1+1}$, and the \textit{slow decay constant} $\rho_2 = 1-\frac{2}{N_2+1}$. Then the scaled smoothing constant vector can be obtained by:
$$
\bc^\toptzero =  ( \rho_2- \rho_1) \times \be^\toptzero   + (1-\rho_2),
$$
where the smaller $\be^\toptzero$, the smaller $\bc^\toptzero$.
For a more efficient influence of the obtained smoothing constant on the averaging period, Kaufman recommended squaring it.
The final calculation formula then follows:
\begin{equation}\label{equation:squared-ssc}
\Exp[\bg^2]^\toptzero  =  (\bc^\toptzero)^2 \hadaprod (\bg^\toptzero)^2  +  \left(1-(\bc^\toptzero)^2 \right)\hadaprod \Exp[\bg^2]^\toptminus.
\end{equation}
or after rearrangement:
$$
\Exp[\bg^2]^\toptzero = \Exp[\bg^2]^\toptminus+ (\bc^\toptzero)^2 \hadaprod \left((\bg^\toptzero)^2 -  \Exp[\bg^2]^\toptminus\right).
$$
We notice that $N_1=3$ is a small period to calculate the average (i.e., $\rho_1=1-\frac{2}{N_1+1}=0.5$) such that the EMA sequence will be noisy if $N_1$ is less than 3. Therefore, the minimum value of $\rho_1$ in practice is set to be greater than 0.5 by default. While $N_2=199$ is a large period to compute the average (i.e., $\rho_2=1-\frac{2}{N_2+1}=0.99$) such that the EMA sequence almost depends only on the previous value, leading to the default value of $\rho_2$ no larger than 0.99. Experimental study  reveals that the AdaSmooth update is insensitive to the hyperparameters  \citet{lu2022adasmooth}. We also carefully notice that when $\rho_1=\rho_2$, the AdaSmooth algorithm recovers to the RMSProp algorithm with decay constant $\rho=1-(1-\rho_2)^2$ since we square it in \eqref{equation:squared-ssc}. After developing the AdaSmooth method, we realize the main idea behind it is similar to that of SGD with momentum: to speed up (compensate less in the denominator) the learning along dimensions where the gradient consistently points in the same direction; and to slow the pace (compensate more in the denominator) along dimensions in which the sign of the gradient continues to change.

\index{Saddle point}
As will be discussed in the cyclical learning rate section (Section~\ref{section:cyclical-lr}), \citet{dauphin2014identifying, dauphin2015equilibrated} argue that the primary challenge in minimizing loss arises from saddle points rather than poor local minima. Saddle points, characterized by small gradients, can impede the learning process. However, an adaptive smoothing procedure for learning rates per dimension can naturally identify these saddle points and compensate less in the denominator---or effectively ``increase" the learning rates---when optimization occurs in these areas. This allows for more rapid traversal of saddle point plateaus. When applied to a non-convex function for training a deep neural network, the learning trajectory may pass through various structures before arriving at a locally convex bowl. AdaGrad shrinks the learning rate based on the entire history of squared partial derivatives, which might make the learning rate too small before reaching such convex structures. RMSProp partially addresses this issue by using an exponentially decaying average to discard older squared gradients, making it more robust in non-convex settings compared to AdaGrad. AdaSmooth advances this approach in two ways: 1) when close to a saddle point, a smaller compensation in the denominator helps escape the saddle point; 2) when near a locally convex bowl, the smaller compensation accelerates convergence.





Empirical evidence shows that the ER used in a simple moving average (SMA) with a fixed window size $M$ can also reflect trends in series/movements in quantitative strategies \citep{lu2022adasmooth}. However, this method again requires storing $M$ previous squared gradients in the AdaSmooth case, making it inefficient, so we do not adopt this extension.
%Empirical evidence shows the ER used in the simple moving average (SMA) with a fixed windows size $M$ can also reflect the trend of the series/movement in quantitative strategies \citep{lu2022adasmooth}. However, this again needs to store $M$ previous squared gradients in the AdaSmooth case, making it inefficient and we shall not adopt this extension.

\paragrapharrow{AdaSmoothDelta.}\label{section:adasmoothdelta}
We observe that the ER can also be applied to the AdaDelta setting:
\begin{equation}\label{equation:adasmoothdelta}
\bd^\toptzero = -\frac{\sqrt{\Exp[\bd^2]^\toptminus}}{\sqrt{\Exp[\bg^2]^\toptzero+\epsilon}} \hadaprod \bg^\toptzero,
\end{equation}
where 
\begin{equation}\label{equation:adasmoothdelta111}
\Exp[\bg^2]^\toptzero  =  (\bc^\toptzero)^2 \hadaprod (\bg^\toptzero)^2  +  \left(1-(\bc^\toptzero)^2 \right)\hadaprod \Exp[\bg^2]^\toptminus ,
\end{equation}
and 
\begin{equation}\label{equation:adasmoothdelta222}
\Exp[\bd^2]^\toptminus = \left(1-(\bc^\toptzero)^2\right) \hadaprod (\bd^\toptminus)^2+ (\bc^\toptzero)^2 \hadaprod \Exp[\bd^2]^{(t-2)},
\end{equation}
In this context, $\Exp[\bd^2]^\toptminus$ selects a larger period when the ER is small. 
This is reasonable in the sense that $\Exp[\bd^2]^\toptminus$ appears in the numerator, while $\Exp[\bg^2]^\toptzero$ is in the denominator of \eqref{equation:adasmoothdelta}, making their compensation towards different directions. Alternatively, a fixed decay constant can be applied for $\Exp[\bd^2]^\toptminus$:
$$
\Exp[\bd^2]^\toptminus = (1-\rho_2)  (\bd^\toptminus)^2+ \rho_2  \Exp[\bd^2]^{(t-2)},
$$
The AdaSmoothDelta optimizer introduced above further alleviates the need for a hand specified global learning rate, which is conventionally set to $\eta=1$ in the Hessian context. However, due to the adaptive smoothing constants in \eqref{equation:adasmoothdelta111} and \eqref{equation:adasmoothdelta222}, the $\Exp[\bg^2]^\toptzero $ and $\Exp[\bd^2]^\toptminus$ are less locally smooth, making it less insensitive to the global learning rate than the AdaDelta method. Therefore, a smaller global learning rate, e.g., $\eta=0.5$ is favored in AdaSmoothDelta. The full procedure for computing AdaSmooth is then formulated in Algorithm~\ref{algo:adasmooth}.




\begin{algorithm}[tb]
\caption{AdaSmooth algorithm. All operations on vectors are element-wise. Good default settings for the tested tasks are $\rho_1=0.5, \rho_2=0.99, \epsilon=1e-6, \eta=0.001$; see Section~\ref{section:adaer-after-er} or \eqref{equation:ema_smooting_constant} for a detailed discussion on the explanation of the decay constants' default values. 
%	Empirical study in Section~\ref{section:ader_experiments} shows that the AdaSmooth algorithm is not sensitive to the hypermarater $\rho_2$, while $\rho_1=0.5$ is relatively a lower bound in this setting. 
The AdaSmoothDelta iteration can be calculated in a similar way.
}
\label{alg:computer-adaer}
\begin{algorithmic}[1]
\State {\bfseries Input:} Initial parameter $\bx_1$, constant $\epsilon$;
\State {\bfseries Input:} Global learning rate $\eta$, by default $\eta=0.001$;
\State {\bfseries Input:} Fast decay constant $\rho_1$, slow decay constant $\rho_2$;
\State {\bfseries Input:} Assert $\rho_2>\rho_1$, by default $\rho_1=0.5$, $\rho_2=0.99$;
\For{$t=1:T$ } 
\State Compute gradient $\bg^\toptzero \leftarrow \nabla f(\bx^\toptzero)$;
\State Compute ER $\be^\toptzero  \leftarrow\frac{\abs{\bx^\toptzero -  \bx^{(t-M)}}}{\sum_{i=0}^{M-1} \abs{ \bd^{(t-1-i)}}}$ ;
\State Compute scaled smoothing vector $\bc^\toptzero \leftarrow  ( \rho_2- \rho_1) \times \be^\toptzero   + (1-\rho_2)$;
\State Compute normalization term $\Exp[\bg^2]^\toptzero  \leftarrow  (\bc^\toptzero)^2 \hadaprod (\bg^\toptzero)^2  +  \left(1-(\bc^\toptzero)^2 \right)\hadaprod \Exp[\bg^2]^\toptminus ;$
\State Compute step $\bd^\toptzero \leftarrow- \frac{\eta}{\sqrt{\Exp[\bg^2]^\toptzero+\epsilon}}  \hadaprod \bg^\toptzero$;
\State Apply update $\bx^\toptone \leftarrow \bx^\toptzero + \bd^\toptzero$;
\EndFor
\State {\bfseries Return:} resulting parameters $\bx^\toptzero$, and the loss $f(\bx^\toptzero)$.
\end{algorithmic}\label{algo:adasmooth}
\end{algorithm}

We have previously discussed the loss deterioration problem encountered when reloading weights from checkpoints using methods like RMSProp or AdaDelta (Figure~\ref{fig:er-rmsprop_epochstart}). However, this issue is less severe in the AdaSmooth setting. As illustrated in Figure~\ref{fig:er-rmsprop_epochstart22}, the loss deterioration observed after reloading weights is smaller in the AdaSmooth example compared to the RMSProp case.
\begin{figure}[h]
%\begin{SCfigure}%[H]
\centering
\includegraphics[width=0.6\textwidth]{imgs/rmsprop_epochstart22.pdf}
\caption{Illustration  of parameter tuning after each epoch by loading the weights. We save the weights and reload them after each new epoch such that there are step-points while re-training after each epoch. This issue is less sever in the AdaSmooth method than in  RMSProp. A smaller loss deterioration is observed in the AdaSmooth example than that of the RMSProp case.}
\label{fig:er-rmsprop_epochstart22}
%\end{SCfigure}
\end{figure}
\index{Loss deterioration}




%\begin{figure}[!h]
%\centering
%\subfigure[MNIST training Loss]{\includegraphics[width=0.47\textwidth, ]{imgs/MNIST_MLP_loss_Train.pdf} \label{fig:mnist_loss_train_mlp}}
%\subfigure[Census Income training loss]{\includegraphics[width=0.47\textwidth]{imgs/Census_MLP_loss_Train.pdf} \label{fig:census_loss_train_mlp}}
%\caption{\textbf{MLP:} Comparison of descent methods on MNIST digit and Census Income data sets for 60 and 200 epochs with MLP.}
%\label{fig:mnist_census_loss_MLP}
%\end{figure}

%\paragrapharrow{Example: multi-layer perceptron.}
%
%
%To see the difference between the discussed algorithms by far,
%we conduct experiments with different machine learning models; and different data sets including 
%real handwritten digit classification task, MNIST \citep{lecun1998mnist} \footnote{It has a training set of 60,000 examples, and a test set of 10,000 examples.}, and Census Income \footnote{Census income data has 48842 number of samples and 70\% of them are used as the training set in our case: https://archive.ics.uci.edu/ml/datasets/Census+Income.} data sets are used.
%In all scenarios, the same parameter initialization is adopted when training with different stochastic optimization algorithms. We compare the results in terms of convergence speed and generalization. 
%
%Multi-layer perceptrons (MLP, a.k.a., multi-layer neural networks) are powerful tools for solving machine learning tasks, finding internal linear and nonlinear features behind the model inputs and outputs. We adopt the simplest MLP structure: an input layer, a hidden layer, and an output layer. We notice that rectified linear unit (Relu) outperforms Tanh, Sigmoid, and other nonlinear units in practice, making it the default nonlinear function in our structures. Since dropout has become a core tool in training neural networks \citep{srivastava2014dropout}, we adopt 50\% dropout noise to the network architecture during training to prevent overfitting.   
%To be more concrete, the detailed architecture for each fully connected layer is described by F$(\langle \textit{num outputs} \rangle:\langle \textit{activation function} \rangle)$; and for a dropout layer is described by
%DP$(\langle \textit{rate} \rangle)$. Then the network structure we use can be described as follows:
%\begin{equation}
%\text{F(128:Relu)DP(0.5)F(\text{num of classes}:Softmax)}.
%\end{equation}
%All methods are trained on mini-batches of 64 images per batch for 60 or 200 epochs through the training set. Setting the hyperparameter to $\epsilon=1e-6$.
%If not especially mentioned, the global learning rates are set to $\eta=0.001$ in all scenarios. While a relatively large learning rate ($\eta=0.01$) is used for AdaGrad method due to  its accumulated decaying effect; learning rate for the AdaDelta method is set to 1 as suggested by \citet{zeiler2012adadelta} and for the AdaSmoothDelta method is set to 0.5 as discussed in Section~\ref{section:adasmoothdelta}.
%%while we will show in the sequel the learning rate for AdaSmoothDelta will not influence the result significantly.
%In Figure~\ref{fig:mnist_loss_train_mlp} and ~\ref{fig:census_loss_train_mlp}, we compare SGD with momentum, AdaGrad, RMSProp, AdaDelta, AdaSmooth, and AdaSmoothDelta in optimizing the training set losses for MNIST and Census Income data sets, respectively. The SGD with momentum method does the worst in this case. AdaSmooth performs slightly better than AdaGrad and RMSProp in the MNIST case and much better than the latters in the Census Income case. AdaSmooth shows fast convergence from the initial epochs while continuing to reduce the training losses in both the two experiments. We here show two sets of slow decay constant for AdaSmooth, i.e., ``$\rho_2=0.9$" and ``$\rho_2=0.95$." Since we square the scaled smoothing constant in \eqref{equation:squared-ssc}, when $\rho_1=\rho_2=0.9$, the AdaSmooth recovers to RMSProp with $\rho=0.99$ (so as the AdaSmoothDelta and AdaDelta case). In all cases, the AdaSmooth results perform better while there is almost no difference between the results of AdaSmooth with various hyperparameters in the MLP model. Table~\ref{fig:mlp_table_perform} shows the best training set accuracy for different algorithms.
%%indicating the superiority of AdaSmooth. 
%While we notice the best test set accuracies for various algorithms are very close; we only present the best ones for the first 5 epochs in Table~\ref{fig:mlp_table_perform-test}. 
%In all scenarios, the AdaSmooth method converges slightly faster than other optimization methods in terms of the test accuracy for this toy example.
%
%\begin{table}[!h]
%\centering
%\begin{tabular}{lll}
%\hline
%Method &\gap MNIST &\gap  Census \\ \hline
%SGD with Momentum ($\rho=0.9$) &\gap 98.64\% &\gap 85.65\%\\
%AdaGrad ($\eta$=0.01) &\gap 98.55\%&\gap 86.02\%\\
%RMSProp ($\rho=0.99$) &\gap 99.15\%&\gap 85.90\%\\
%AdaDelta ($\rho=0.99$) &\gap 99.15\%&\gap 86.89\%\\
%AdaSmooth ($\rho_1=0.5, \rho_2=0.9$) &\gap \textbf{99.34}\%&\gap \textbf{86.94}\%\\
%AdaSmooth ($\rho_1=0.5, \rho_2=0.95$) &\gap \textbf{99.45}\%&\gap \textbf{87.10}\%\\
%AdaSmoothDelta ($\rho_1=0.5, \rho_2=0.9$) &\gap \textbf{99.60}\%&\gap {86.86}\%\\
%\hline
%\end{tabular}
%\caption{\textbf{MLP}: Best in-sample evaluation in training accuracy (\%).}
%\label{fig:mlp_table_perform}
%\end{table}
%
%\begin{table}[!h]
%\centering
%\begin{tabular}{lll}
%\hline
%Method &\gap MNIST &\gap  Census \\ \hline
%SGD with Momentum ($\rho=0.9$) &\gap 94.38\%&\gap  83.13\%\\
%AdaGrad ($\eta$=0.01) &\gap 96.21\%& \gap84.40\%\\
%RMSProp ($\rho=0.99$) &\gap 97.14\%& \gap84.43\%\\
%AdaDelta ($\rho=0.99$) &\gap 97.06\%&\gap84.41\%\\
%AdaSmooth ($\rho_1=0.5, \rho_2=0.9$) &\gap 97.26\%&\gap 84.46\%\\
%AdaSmooth ($\rho_1=0.5, \rho_2=0.95$) &\gap 97.34\%&\gap 84.48\%\\
%AdaSmoothDelta ($\rho_1=0.5, \rho_2=0.9$) &\gap 97.24\% &\gap \textbf{84.51}\%\\
%\hline
%\end{tabular}
%\caption{\textbf{MLP}: Best out-of-sample evaluation in test accuracy for the first 5 epochs. }
%\label{fig:mlp_table_perform-test}
%\end{table}


\subsection{Adam}
\textit{Adaptive moment estimation (Adam)} is yet another adaptive learning rate optimization algorithm \citep{kingma2014adam}. 
Adam utilizes both first-order and second-order information in its updates. Similar to RMSProp, AdaDelta, and AdaSmooth, Adam maintains an exponential moving average of past squared gradients (the second moment). However, it also keeps track of an exponentially decaying average of past gradients (the first moment):
\begin{equation}\label{equation:adam-updates}
\begin{aligned}
\bmm^\toptzero &=  \rho_1 \bmm^\toptminus + (1-\rho_1)\bg^\toptzero; \\
\bv^\toptzero &= \rho_2 \bv^\toptminus +(1-\rho_2)(\bg^\toptzero)^2,
\end{aligned}
\end{equation}
where $\bmm^\toptzero$ and $\bv^\toptzero$ are running estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients, respectively. The drawback of RMSProp is that the running estimate $\Exp[\bg^2]$ can be biased towards zero during the initial steps because it starts from zero, particularly when the decay constant is large (when $\rho$ is close to 1 in RMSProp). Observing the biases towards zero in \eqref{equation:adam-updates} as well, Adam counteracts these biases by computing the bias-free moment estimates:
$$
\begin{aligned}
\widehat{\bmm}^\toptzero &= \frac{\bmm^\toptzero}{1-\rho_1^t}
\qquad\text{and}\qquad
\widehat{\bv}^\toptzero = \frac{\bv^\toptzero}{1-\rho_2^t}.
\end{aligned}
$$ 
The first and second moment estimates are then incorporated into the update step:
$$
\begin{aligned}
\bd^\toptzero &\leftarrow - \frac{\eta}{\sqrt{\widehat{\bv}^\toptzero}+\epsilon} \hadaprod  \widehat{\bmm}^\toptzero;\\
\bx^\toptone &\leftarrow \bx^\toptzero - \bd^\toptzero.
\end{aligned}
$$
In practice, \citet{kingma2014adam} suggest to use $\rho_1=0.9$, $\rho_2=0.999$, and $\epsilon=1e-8$ for the parameters by default.

\subsection{AdaMax}
Building on Adam, \citet{kingma2014adam} explored the use of higher-order moments for optimization updates. Specifically, they considered:
$$
\bv^\toptzero = \rho_2^p \bv^\toptminus + (1-\rho_2^p) \absbig{\bg^\toptzero}^p,
$$
which can become numerically unstable for large values of  $p$. 
This instability makes the $\ell_1$ and $\ell_2$ norms common choices for update calculations. 
However, as $p\rightarrow \infty$, the $\ell_{\infty}$ norm also shows stable behavior. Based on this observation, AdaMax employs the following moment update rule:
$$
\bu^\toptzero = \rho_2^\infty \bu_{t-1} + (1-\rho_2^\infty) \absbig{\bg^\toptzero}^\infty = \max (\rho_2\bu^{(t-1)}, \absbig{\bg^\toptzero}),
$$
where $\bu^\toptzero$ does not require initialization bias correction. The parameter update step in AdaMax is then given by:
$$
\bd^\toptzero = - \frac{\eta}{\bu^\toptzero} \hadaprod  \widehat{\bmm}^\toptzero.
$$
In practice, \citet{kingma2014adam} suggest to use $\eta=0.002$, $\rho_1=0.9$, and $\rho_2=0.999$ as the default parameters.

%\subsection{Nadam}
%Nadam (Nesterov-accelerated Adam) combines the ideas of Adam and Nesterov momentum \citep{dozat2016incorporating}. We recall the momentum and NAG updates as follows:
%$$
%\boxed{
%\begin{aligned}
%&\text{Momentum:}\\
%&\bg^\toptzero = \nabla f(\bx^\toptzero);\\
%&\bd^\toptzero = \rho\bd^\toptminus - \eta \bg^\toptzero; \\
%&\bx^\toptone = \bx^\toptzero +\bd^\toptzero,
%\end{aligned}
%}
%\gap 
%\boxed{
%\begin{aligned}
%&\text{NAG:}\\
%&\bg^\toptzero = \nabla f(\bx^\toptzero + \rho \bd^\toptminus);\\
%&\bd^\toptzero =\rho\bd^\toptminus - \eta\bg^\toptzero; \\
%&\bx^\toptone = \bx^\toptzero +\bd^\toptzero,
%\end{aligned}
%}
%$$
%\citet{dozat2016incorporating} first proposes a modification of NAG by using the current momentum vector to look ahead, which we call NAG$^\prime$ here. That is, applying the momentum update twice for each update:
%$$
%\boxed{
%\begin{aligned}
%&\text{NAG}^\prime:\\
%&\bg^\toptzero = \nabla f(\bx^\toptzero);\\
%&\bd^\toptzero = \rho\bd^\toptminus - \eta \bg^\toptzero; \\
%&\bd^\toptzeroPRIME = \rho\bd^\toptzero - \eta \bg^\toptzero; \\
%&\bx^\toptone = \bx^\toptzero +\bd^\toptzeroPRIME .
%\end{aligned}
%}
%$$
%By rewriting the Adam in the following form, where a similar modification according to NAG$^\prime$ leads to the Nadam update:
%$$
%\boxed{
%\begin{aligned}
%&\text{Adam}:\\
%&\bmm^\toptzero =  \rho_1 \bmm^\toptminus + (1-\rho_1)\bg^\toptzero;\\
%&\widehat{\bmm}^\toptzero = \rho_1 \frac{\bmm^\toptminus }{1-\rho_1^t} +\frac{1-\rho_1}{1-\rho_1^t} \bg^\toptzero; \\
%&\bd^\toptzero = - \frac{\eta}{\sqrt{\widehat{\bv}^\toptzero}+\epsilon} \hadaprod  \widehat{\bmm}^\toptzero;\\
%&\bx^\toptone = \bx^\toptzero +\bd^\toptzero,
%\end{aligned}
%}
%\leadto 
%\boxed{
%\begin{aligned}
%&\text{Nadam}:\\
%&\bmm^\toptzero =  \rho_1 \bmm^\toptminus + (1-\rho_1)\bg^\toptzero;\\
%&\widehat{\bmm}^\toptzero = \rho_1 \frac{\bmm_{\textcolor{mylightbluetext}{t}} }{1-\rho_1^{\textcolor{mylightbluetext}{t+1}}} +\frac{1-\rho_1}{1-\rho_1^t} \bg^\toptzero; \\
%&\bd^\toptzero = - \frac{\eta}{\sqrt{\widehat{\bv}^\toptzero}+\epsilon} \hadaprod  \widehat{\bmm}^\toptzero;\\
%&\bx^\toptone = \bx^\toptzero +\bd^\toptzero.
%\end{aligned}
%}
%$$
%However, the $\rho_1 \frac{\bmm^\toptminus }{1-\rho_1^t}$ in $\widehat{\bmm}^\toptzero$ of the Adam method can be replaced in a momentum fashion; by applying the same modification on NAG$^\prime$, the second version of Nadam has the following form (though it's not originally presented in \citet{dozat2016incorporating}):
%$$
%\boxed{
%\begin{aligned}
%&\text{Adam}^\prime:\\
%&\bmm^\toptzero =  \rho_1 \bmm^\toptminus + (1-\rho_1)\bg^\toptzero;\\
%&\widehat{\bmm}^\toptzero = \rho_1 \textcolor{mylightbluetext}{\widehat{\bmm}_{t-1}} +\frac{1-\rho_1}{1-\rho_1^t} \bg^\toptzero; \\
%&\bd^\toptzero = - \frac{\eta}{\sqrt{\widehat{\bv}^\toptzero}+\epsilon} \hadaprod  \widehat{\bmm}^\toptzero;\\
%&\bx^\toptone = \bx^\toptzero +\bd^\toptzeroPRIME,
%\end{aligned}
%}
%\leadto 
%\boxed{
%\begin{aligned}
%&\text{Nadam}^\prime:\\
%&\widehat{\bmm}^\toptzero = \rho_1 \frac{\bmm^\toptminus }{1-\rho_1^t} +\frac{1-\rho_1}{1-\rho_1^t} \bg^\toptzero; \\
%&\widehat{\bmm}^\toptzeroPRIME = \rho_1 \textcolor{mylightbluetext}{\widehat{\bmm}_{t}} +\frac{1-\rho_1}{1-\rho_1^t} \bg^\toptzero; \\
%&\bd^\toptzero = - \frac{\eta}{\sqrt{\widehat{\bv}^\toptzero}+\epsilon} \hadaprod  \widehat{\bmm}^\toptzeroPRIME;\\
%&\bx^\toptone = \bx^\toptzero +\bd^\toptzeroPRIME.
%\end{aligned}
%}
%$$


\index{Saddle point}
\subsection{Problems in SGD}\label{section:c-problem}
The previous introduced optimization algorithms for stochastic gradient descent  are  widely used in training deep learning models. However, they come with their own set of challenges and potential issues. Here are some common problems associated with SGD:
\paragrapharrow{Saddle points.}
When the Hessian of loss function is positive definite, then the optimal point $\bx^*$ with vanishing gradient must be a local minimum (Theorem~\ref{theorem:second_nec_nonstrict_loca}). 
Similarly, when the Hessian is negative definite, the point is a local maximum; when the Hessian has both positive and negative eigenvalues, the point is a saddle point (Definition~\ref{definition:stat_point}). 
The stochastic optimization algorithms discussed above in practice are first-order optimization algorithms: they only look at the gradient information, and never explicitly compute the Hessian. Such algorithms may get stuck at saddle points (see toy example in Figure~\ref{fig:quadratic_saddle}). In the algorithms presented earlier, including basic update, AdaGrad, AdaDelta, RMSprop, and others, this issue may arise. 
AdaSmooth, as discussed in Section~\ref{section:adaer}, may have mechanisms to escape saddle points. Momentum and Nesterov momentum can also help navigate past saddle points due to their incorporation of previous step sizes; however, these techniques can make convergence more difficult, particularly when the momentum term $\rho$ is large. 


\paragrapharrow{Low speed in SGD.}
Despite claims by Rong Ge's post \footnote{http://www.offconvex.org/2016/03/22/saddlepoints/} that SGD might converge to saddle points with a high error rate, \citet{dauphin2014identifying} and Benjamin Recht's post \footnote{http://www.offconvex.org/2016/03/24/saddles-again/} argue that it is actually quite challenging for SGD to converge to saddle points from random initial points. This difficulty arises because local minima and saddle points are often surrounded by plateaus with small curvature on the error surface. In SGD, algorithms tend to be repelled away from saddle points towards regions of lower error along directions of negative curvature. Therefore, while there isn't a true ``saddle point problem" in SGD, escaping these areas can be slow due to the low curvature plateaus. Second-order methods like Newton's method  handle saddle points well, as they aim to rapidly descend through plateaus by scaling gradient steps using the inverse eigenvalues of the Hessian matrix.

As noted by \citet{dauphin2014identifying}, random Gaussian error functions over large $n$ dimensions are increasingly likely to have saddle points rather than local minima as $n$ increases. And the ratio of the number of saddle points to local minima increases exponentially with the dimensionality $n$. The authors also argue that it is saddle points rather than local minima that provide a fundamental impediment to rapid high dimensional non-convex optimization. In this sense, local minima with high errors are exponentially rare compared to saddle points, making computation slow in SGD when navigating through regions of small curvature.





%\paragrapharrow{First-order method to escape from saddle point.}
%The post \footnote{http://www.offconvex.org/2016/03/22/saddlepoints/} by Rong Ge introduces a first-order method to escape from saddle point. 
%He explains that saddle points are inherently unstable: if you place a ball on a saddle point and give it a slight nudge, the ball is likely to roll towards a local minimum, especially when the second-order term $\frac{1}{2}  \bd^\top \bH  \bd$ is significantly smaller than 0 (i.e., there is a steep direction where the function value decreases, and assume we are looking for a local minimum), which is called a \textit{strict saddle function} in Rong Ge's post. In this case, we can use \textit{noisy gradient descent}:
%$$
%\bx^\toptone = \bx^\toptzero + \bd^\toptzero + \bepsilon,
%$$
%where $\bepsilon$ is a noise vector that has zero mean $\bzero$. Actually, this is the basic idea of SGD, which uses the gradient of a mini-batch rather than the true gradient. However, the drawback of the stochastic gradient descent is not the direction, but the size of the step along each eigenvector. However, in second-order methods, the step, along any eigen-direction $\bq_i$, is given by something like $-\lambda_i \Delta {v_i}$, where $\lambda_i$ is an eigenvalue of the Hessian $\bH$; steps taken in directions with small absolute values of eigenvalues tend to be very small; and conversely, steps taken in directions with large absolute values of eigenvalues tend to be very large. 
%To be more concrete, as an example where the curvature of the error surface may not be the same in all directions. If there is a long and narrow valley in the error surface, the component of the gradient in the direction that points along the base of the valley is very small; while the component perpendicular to the valley walls is quite large even though we have to move a long distance along the base and a small distance perpendicular to the walls. This phenomenon can be seen in Figure~\ref{fig:momentum_gd} (though it's partly solved by SGD with momentum).
%
%
%We normally move by making a step that is some constant times the negative gradient rather than a step of constant length in the direction of the negative gradient. This means that in steep regions (where we have to be careful not to make our steps too large), we move quickly; and in shallow regions (where we need to move in big steps), we move slowly. This phenomenon again contributes to the slower convergence of SGD methods compared to second-order methods.




\index{Learning rate annealing}
\index{Learning rate warmup}
\section{Learning Rate Annealing and Warmup}\label{section:learning-rate-annealing}
We have discussed in \eqref{equation:momen_sgd} that the learning rate $\eta$ controls how large of a step to take in the direction of the negative gradient so that we can reach a (local) minimum. In a wide range of applications, a fixed learning rate works well in practice. While there are alternative learning rate schedules that change the learning rate during learning, and it is most often changed between epochs, especially for training deep neural  or transformer structures. 
We've seen that per-dimension optimizers like AdaGrad, AdaDelta, RMSProp, and AdaSmooth can adaptively change the learning rate in each dimension  \citep{duchi2011adaptive, hinton2012neural, zeiler2012adadelta, lu2022adasmooth}. 
This section focuses on strategies for decaying or annealing the global learning rate, i.e., adjusting the value of $\eta$ in \eqref{equation:momen_sgd}.


Using a constant learning rate poses a dilemma: a small learning rate can make the algorithm take too long to reach an optimal solution, whereas a large initial learning rate might initially bring the algorithm close to a good (local) minimum but cause it to oscillate around this point indefinitely. To mitigate these issues, decreasing the learning rate over time can help by slowing down parameter updates. This can be done manually when validation accuracy plateaus, or automatically through decay functions based on epoch count.
Common decay functions include \textit{step decay}, \textit{inverse decay}, and \textit{exponential decay}. 
The following sections explore the mathematical formulations of various learning rate annealing schemes.



\index{Learning rate annealing}
\subsection{Learning Rate Annealing}\label{section:learning-rate-anneal}
\paragrapharrow{Step decay.} The \textit{step decay} scheduler drops the learning rate by a factor every epoch or after a set number of epochs. 
For  iteration $t$, number of iterations to drop $n$, initial learning rate $\eta_0$, and decay factor $d<1$, the form of step decay is given by
$$
\eta_t = \eta_0 \cdot  d^{\floor{\frac{t}{n}} }=\eta_0 \cdot  d^s,
$$
where $s\triangleq{\floor{\frac{t}{n}} }$ represents the \textit{step stage} at which the decay occurs. Therefore, the step decay policy decays the learning rate every $n$ iterations.

\paragrapharrow{Multi-step decay.}
The \textit{multi-step decay} scheduler is a slightly different version of the step decay, wherein the step stage is the index where the iteration $t$ falls in the milestone vector $\bmm=[m_1,m_2,\ldots, m_k]^\top$ with $0\leq m_1\leq m_2\leq\ldots\leq m_k\leq T$ and $T$ being the total number of iterations (or epochs) \footnote{When $T$ is the total number of iterations, it can be obtained by the product of the number of epochs and the number of steps per epoch.}. To be more concrete, the step stage $s$ at iteration $t$ is obtained by
$$
s = 
\left\{
\begin{aligned}
	&0, \gap &t<m_1;\\
	&1, \gap &m_1\leq t < m_2; \\
	&\ldots\\
	&k, \gap &m_k\leq t \leq T. 
\end{aligned}
\right.
$$
As a result, given the iteration $t$, initial learning rate $\eta_0$, and decay factor $d<1$, the learning rate at iteration $t$ is calculated as 
$$
\eta_t = \eta_0 \cdot  d^s.  
$$


\begin{figure}[h]
	%\begin{SCfigure}%[H]
	\centering
	\includegraphics[width=0.6\textwidth]{imgs/lr_step_decay.pdf}
	\caption{Demonstration of step decay, multi-step decay, annealing polynomial, inverse decay, inverse square root, and exponential decay schedulers. One may find that among the six, exponential decay exhibits the smoothest behavior, while multi-step decay is characterized by the least smoothness.}
	\label{fig:lr_step_decay}
	%\end{SCfigure}
\end{figure}

\paragrapharrow{Exponential decay.}
Given the iteration $t$, the initial learning rate $\eta_0$, and the exponential decay factor $k$, the form of the \textit{exponential decay} is given by
$$
\eta_t = \eta_0 \cdot \exp(-k \cdot t),
$$
where the parameter $k$ controls the rate of the decay.

%\textcolor{red}{TO delete}
%\paragrapharrow{Polynomial decay.} The learning rate $\eta_t$ can be expressed in terms of the initial rate $\eta_0$, the end rate $\eta_T$, total steps $T$ (i.e., number of epochs times number of steps per epoch), warm up steps $w$, and power rate $p$, the learning rate $\eta_t$ at time $t$ can be expressed as
%$$
%\eta_t = (\eta_0-\eta_T) \cdot  \left(  \frac{T - t}{T-w} \right)^p + \eta_T.
%$$
%In practice, the default values for the parameters are: initial rate $\eta_0=0.001$, end rate $\eta_T=1e-10$, the warm up steps $s=100$, power rate $p=2$. 

\paragrapharrow{Inverse decay.}
The \textit{inverse decay} scheduler is a variant of exponential decay in that the decaying effect is applied by the inverse function. Given the iteration number $t$, the initial learning rate $\eta_0$, and the decay factor $k$, the form of the inverse decay is obtained by
$$
\eta_t = \frac{\eta_0}{1+ k\cdot t},
$$
where, again, the parameter $k$ controls the rate of the decay.


\paragrapharrow{Inverse square root.} 
The \textit{inverse square root} scheduler is a learning rate schedule 
$$
\eta_t = \eta_0 \cdot \sqrt{w} \cdot \frac{1}{\sqrt{\max(t, w)}},
$$
where $t$ represents the current training iteration,  $w$ is the number of warm-up steps, and $\eta_0$ is the initial learning rate. This scheduler maintains a constant learning rate for the initial  steps, then exponentially decays the learning rate until the pre-training phase concludes.

\paragrapharrow{Annealing polynomial decay.}
Given the iteration $t$, the \textit{max decay iteration} $M$, the power factor $p$, the initial learning rate $\eta_0$, and the final learning rate $\eta_T$, the \textit{annealing polynomial decay} at iteration $t$ can be obtained by 
\begin{equation}\label{equation:annealing_polynomial}
	\begin{aligned}
		& decay\_batch = \min(t, M) ;\\
		& \eta_t = (\eta_0-\eta_T)\cdot \left(1-\frac{t}{decay\_batch}\right)^{p}+\eta_T.
	\end{aligned}
\end{equation}
In practice, the default values for the parameters are: initial rate $\eta_0=0.001$, end rate $\eta_T=1e-10$, the warm up steps $M=T/2$, where $T$ is the maximal iteration number, and power rate $p=2$. 


Figure~\ref{fig:lr_step_decay} compares step decay, multi-step decay, annealing polynomial decay, inverse decay, inverse square root, and exponential decay with a specified  set of parameters. 
The smoothness varies among these methods, with exponential decay exhibiting the smoothest behavior and multi-step decay being the least smooth.  
In  annealing polynomial decay, the max decay iteration parameter $M$ determines the decay rate:
\begin{itemize}
	\item When $M$ is small, the decay gets closer to that of the exponential scheduler or the step decay; however, the exponential decay has a longer tail. That is, the exponential scheduler decays slightly faster in the beginning iterations but slows down in the last few iterations.
	\item When $M$ is large, the decay gets closer to that of the multi-step decay; however, the multi-step scheduler exhibits a more aggressive behavior.
\end{itemize}

\begin{figure}[h]
	\centering  
	\vspace{-0.35cm} 
	\subfigtopskip=2pt 
	\subfigbottomskip=2pt 
	\subfigcapskip=-5pt 
	\subfigure[Training loss.]{\label{fig:mnist_scheduler_1}
		\includegraphics[width=0.47\linewidth]{imgs/mnist_scheduler_loss_Train.pdf}}
	\subfigure[Training accuracy.]{\label{fig:mnist_scheduler_2}
		\includegraphics[width=0.47\linewidth]{imgs/mnist_scheduler_acc_Train.pdf}}
	\subfigure[Test loss.]{\label{fig:mnist_scheduler_3}
		\includegraphics[width=0.47\linewidth]{imgs/mnist_scheduler_loss_Test.pdf}}
	\subfigure[Test accuracy.]{\label{fig:mnist_scheduler_4}
		\includegraphics[width=0.47\linewidth]{imgs/mnist_scheduler_acc_Test.pdf}}
	\caption{Training and test performance with different learning rate schemes.}
	\label{fig:mnist_scheduler_1234}
\end{figure}

\paragrapharrow{Toy example.}
To assess the impact of different schedulers, we utilize a toy example involving the training of a multi-layer perceptron (MLP) on the MNIST digit classification set  \citep{lecun1998mnist} \footnote{It has a training set of 60,000 examples, and a test set of 10,000 examples.}. Figure~\ref{fig:mnist_scheduler_1234} presents the training and test performance in terms of \textit{negative log-likelihood loss}. 
The parameters for various schedulers are detailed in Figure~\ref{fig:lr_step_decay} (for 100 epochs). We observe that the stochastic gradient descent method with fixed learning rate may lead to a continued reduction in test loss; however, its test accuracy may get stuck at a certain point. The toy example shows learning rate annealing schemes,  in general, can enhance optimization methods by guiding them towards better local minima with improved performance.

\index{Learning rate warmup}
\subsection{Learning Rate Warmup}
The concept of warmup in training neural networks receive attention in recent years \citep{he2016deep, goyal2017accurate, smith2019super}. Further insights into the efficacy of the warmup scheduler in neural machine translation (NML) can be found in the comprehensive discussion by  \citet{popel2018training}. 
Learning rate annealing schedulers can be applied on both an epoch- and step-basis. However, learning rate warmup schemes are typically implemented on a step-by-step basis, where the total number of steps is the product of the number of epochs and the number of steps per epoch, as mentioned earlier \citep{vaswani2017attention, howard2018universal}. Note that with this scheduler, early stopping should generally be avoided. In the rest of this section, we will explore two commonly used warmup policies: the slanted triangular learning rates (STLR) and the Noam methods.

\paragrapharrow{Slanted Triangular Learning Rates (STLR).} STLR is a learning rate schedule that initially  increases the learning rate over some number of epochs linearly, and then  decays it over the remaining epochs linearly. The rate at iteration $t$ is computed as follows: 
$$
\begin{aligned}
	cut &= \ceil{T\cdot frac} ;\\
	p &= 
	\left\{
	\begin{aligned}
		&t/cut , \gap &\text{if }t<cut;\\
		& 1 - \frac{t-cut }{cut\cdot (1/frac-1)}, \gap &\text{otherwise};
	\end{aligned} 
	\right.\\
	\eta_t &= \eta_{\text{max}} \cdot \frac{1+p\cdot (ratio - 1)}{ratio },\\
\end{aligned}
$$
where $T$ is the number of training iterations (the product of the number of epochs and the number of updates per epoch), $frac$ is the fraction of iterations we want to increase the learning rate, $cut$ is the iteration when we switch from increasing to decreasing the learning rate, $p$ is the fraction of the number of iterations we have increased or decreased the learning rate respectively, $ratio$ specifies how much smaller the lowest learning rate is from the maximum learning rate $\eta_{\text{max}}$. In practice, the default values are $frac=0.1$, $ratio=32$, and $\eta_{\text{max}}=0.01$ \citep{howard2018universal}. 



\paragrapharrow{Noam.} The Noam scheduler is originally used in neural machine translation (NML) tasks and is proposed in \citet{vaswani2017attention}. This corresponds to increasing the learning rate linearly for the first ``warmup\_steps" 
training steps and decreasing it thereafter proportionally to the inverse square root of the
step number, scaled by the inverse square root of the dimensionality of the model (linear warmup for a given number of steps followed by exponential decay). Given the warmup steps $w$ and the model size $d_{\text{model}}$ (representing the hidden size parameter which dominates the number of parameters in the model), the learning rate $\eta_t$ at step $t$ can be calculated by 
$$
\eta_t = \alpha \cdot  \frac{1}{\sqrt{d_{\text{model}}}}\cdot  \min \left(\frac{1}{\sqrt{t}} ,  \frac{t}{w^{3/2}}\right),
$$
where $\alpha$ is a smoothing factor. In the original paper, the warmup step $w$ is set to $w=4000$. While in practice, $w=25000$ can be a good choice. 


\begin{figure}[h]
	%\begin{SCfigure}%[H]
	\centering
	\includegraphics[width=0.6\textwidth]{imgs/lr_noam.pdf}
	\caption{Comparison of Noam and STLR schedulers.}
	\label{fig:lr_noam}
	%\end{SCfigure}
\end{figure}
Moreover, in rare cases, the model size is occasionally  set to be the same as the warmup steps, resulting in what is known as the \textit{warmup Noam scheduler}:
$$
\eta_t = \alpha \cdot \frac{1}{\sqrt{w}} \cdot  \min \left(\frac{1}{\sqrt{t}} ,  \frac{t}{w^{3/2}}\right).
$$
Figure~\ref{fig:lr_noam} compares the STLR and Noam schedulers with various parameters. Generally, the Noam scheduler decays more slowly after the warmup phase compared to STLR.




\subsection{Cyclical Learning Rate (CLR) Policy}\label{section:cyclical-lr}

The cyclical learning rate policy generalizes the concepts of warmup and decay policies, such as the Noam scheme or STLR, which typically involve only one cycle. The core idea behind this policy is that a temporary increase in the learning rate can have a short-term negative effect but ultimately contribute to long-term benefits. This observation leads to the strategy of varying the learning rate within a range rather than using a fixed or exponentially decreasing value, with minimum and maximum boundaries set for variation.
One simple method to implement this idea is through a triangular window function, which linearly increases and then decreases the learning rate \citep{smith2017cyclical}.

\index{Learning rate range test}
\citet{dauphin2014identifying, dauphin2015equilibrated} suggest that the challenge in minimizing loss often stems from saddle points (illustrated by a toy example in Figure~\ref{fig:quadratic_saddle}) rather than poor local minima. Saddle points feature small gradients that slow down the learning process. Increasing the learning rate helps navigate these plateaus more swiftly. Therefore, a cyclical learning rate policy with periodic increases and decreases between minimum and maximum boundaries is reasonable. These boundaries are problem-specific and are typically identified through a \textit{learning rate range test}, where the model is run for several epochs with different learning rates. Plotting accuracy versus learning rate helps identify suitable minimum and maximum boundaries: when accuracy starts to increase and when it begins to slow, become erratic, or decline, the two of which constitute good choices for the minimum and maximum boundaries. 

Cyclical learning rate policies can be categorized into those based on iterations and those based on epochs~\footnote{Again, the total number of iterations equals the product of the number of epochs and the number of updates per epoch.}. The former adjusts the learning rate at each iteration, while the latter does so on an epoch basis. However, there's no significant difference between the two; any policy can be adapted to either approach. Below, we discuss the update policies according to their original proposals.


\paragrapharrow{Triangular, Triangular2, and Exp Range.} 
The \textit{triangular} policy involves a linear increase and decrease of the learning rate. 
Given the initial learning rate $\eta_0$ (the lower boundary in the cycle), the maximum learning rate $\eta_{\max}$, and the stepsize $s$ (number of training iterations per half cycle), the learning rate $\eta_t$ at iteration $t$ can be obtained by:
$$
triangular: \gap 
\left\{
\begin{aligned}
	{cycle}&= \floor{1+\frac{t}{2s}};\\
	x &= \text{abs}\left(\frac{t}{s} - 2\times {cycle}+1\right);\\
	\eta_t &=\eta_0 + (\eta_{\max}-\eta_0) \cdot \max(0, 1-x),\\
\end{aligned}
\right.
$$
where the calculated \textit{cycle} indicates which cycle iteration $t$ is in.
The same as the \textit{triangular} policy, the \textit{triangular2} policy cuts in half at the end of each cycle:
$$
triangular2:\gap \eta_t =\eta_0 + (\eta_{\max}-\eta_0) \cdot \max(0, 1-x) \cdot \frac{1}{2^{\text{cycle}-1}}.
$$
Less aggressive than the \textit{triangular2} policy, the amplitude of a cycle in \textit{exp\_range} policy is scaled exponentially based on $\gamma^t$, where $\gamma<1$ is the scaling constant:
$$
exp\_range:\gap \eta_t =\eta_0 + (\eta_{\max}-\eta_0) \cdot \max(0, 1-x) \cdot \gamma^{t}.
$$ 
A comparison of these three policies is presented in Figure~\ref{fig:lr_triangular}. In practice, the stepsize $s$  is typically set to $2\sim 10$ times the number of iterations in an epoch \citep{smith2017cyclical}.

\begin{figure}[h]
	%\begin{SCfigure}%[H]
	\centering
	\includegraphics[width=0.6\textwidth]{imgs/lr_triangular.pdf}
	\caption{Demonstration of \textit{triangular}, \textit{triangular2}, and \textit{exp\_range} schedulers.}
	\label{fig:lr_triangular}
	%\end{SCfigure}
\end{figure}


\paragrapharrow{Cyclical cosine.} 

The \textit{Cyclical cosine} is a type of learning rate scheduler that initiates with a high learning rate, rapidly decreases it to a minimum value, and then quickly increases it again.
The resetting of the learning rate acts as a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a ``warm restart" in contrast to a ``cold restart," where a new set of small random numbers may be used as a starting point \citep{loshchilov2016sgdr, huang2017snapshot}. The learning rate $\eta_t$ at iteration $t$ is calculated as follows:
$$
\eta_t = \frac{\eta_0}{2} \left( \cos \left(  \frac{\pi \,\, \text{mod}(t-1,\ceil{T/M} ) }{\ceil{T/M}}  \right)+1\right),
$$
where $T$ is the total number of training iterations (note the original paper takes the iterations as epochs in this sense \citep{loshchilov2016sgdr}), $M$ is the number of cycles, and $\eta_0$ is the initial learning rate. The scheduler anneals the learning rate from its initial value $\eta_0$ to a small learning rate approaching 0 over the course of a cycle. That is, we split the training process into $M$ cycles as shown in Figure~\ref{fig:lr_cosine}, each of which starts with a large learning rate $\eta_0$ and then gets annealed to a small learning rate.
The provided equation facilitates a rapid decrease in the learning rate, encouraging the model to converge towards its first local minimum after a few epochs. The optimization then continues at a larger learning rate that can perturb the model and dislodge it from the minimum \footnote{The goal of the procedure is similar to the perturbed SGD that can help escape from saddle points \citep{jin2017escape, du2017gradient}.}. The iterative procedure is then repeated several times to achieve multiple convergences. 
%In practice, the iteration $t$ usually refers to the $t$-th epoch. 
More generally, any learning rate with general function $f$ in the following form can have a similar effect:
$$
\eta_t = f(\text{mod}(t-1, \ceil{T/M})).
$$





\begin{figure}[h]
	\centering  
	\vspace{-0.35cm} 
	\subfigtopskip=2pt 
	\subfigbottomskip=2pt 
	\subfigcapskip=-5pt 
	\subfigure[Cyclical cosine.]{\label{fig:lr_cosine}
		\includegraphics[width=0.48\linewidth]{imgs/lr_cosine.pdf}}
	\subfigure[Cyclical step.]{\label{fig:lr_cyclical_step}
		\includegraphics[width=0.48\linewidth]{imgs/lr_cyclical_step.pdf}}
	\caption{Cyclical cosine and cyclical step learning rate policies.}
	\label{fig:lr_cosine_cyclical_step}
\end{figure}


\paragrapharrow{Cyclical step.} Similar to the cyclical cosine scheme, the \textit{cyclical step learning rate policy} combines a linear learning rate decay with warm restarts \citep{mehta2019espnetv2}:
$$
\eta_t  =\eta_{\text{max}} - (t \,\, \text{mod}\,\, M) \cdot \eta_{\text{min}},
$$
where in the original paper, $t$ refers to the epoch count, $\eta_{\text{min}}$ and $\eta_{\text{max}}$ are the ranges for the learning rate, and $M$ is the cycle length after which the learning rate will restart. The learning rate scheme can be seen as a variant of the cosine learning policy as discussed above and the comparison between the two policies is shown in Figure~\ref{fig:lr_cosine_cyclical_step}. In practice, $\eta_{\text{min}}=0.1$, $\eta_{\text{max}}=0.5$, and $M=5$ are set as default values in the original paper. 

\paragrapharrow{Cyclical polynomial.}
The \textit{cyclical polynomial} is a variant of the \textit{annealing polynomial decay} (Equation~\eqref{equation:annealing_polynomial}) scheme, where the difference is that the cyclical polynomial scheme employs a cyclical warmup similar to the $exp\_range$ policy. Given the iteration number $t$, the initial learning rate $\eta_0$, the final learning rate, $\eta_T$, and the maximal decay number $M<T$, the rate can be calculated by:
$$
\begin{aligned}
	& decay\_batch = M\cdot \ceil{\frac{t}{M}} \\
	& \eta_t= (\eta_0-\eta_T)\cdot \left(1-\frac{t}{decay\_batch+\epsilon}\right)^{p}+\eta_T,
\end{aligned}
$$
where $\epsilon=1e-10$ is applied for better conditioning when $t=0$. 
Figure~\ref{fig:lr_cyclic_polynomial_decay} presents the cyclical polynomial scheme with various parameters.

\begin{figure}[h]
	%\begin{SCfigure}%[H]
	\centering
	\includegraphics[width=0.6\textwidth]{imgs/lr_cyclic_polynomial_decay.pdf}
	\caption{Demonstration of cyclical polynomial scheduler with various parameters.}
	\label{fig:lr_cyclic_polynomial_decay}
	%\end{SCfigure}
\end{figure}








