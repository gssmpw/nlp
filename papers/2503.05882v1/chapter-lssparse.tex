

\chapter{Least Squares: Linear, Nonlinear, Sparse Problems}
\begingroup
\hypersetup{
	linkcolor=structurecolor,
	linktoc=page,  % page: only the page will be colored; section, all, none etc
}
\minitoc \newpage
\endgroup
Least squares problems are fundamental to optimization, especially in regression analysis and data fitting. This chapter delves into linear and nonlinear least squares, Levenberg-Marquardt methods, and sparse optimization techniques such as L1-regularized optimization and iterative hard-thresholding methods. These approaches are particularly pertinent for compressed sensing and high-dimensional data analysis.

\index{Least squares}
\index{Linear model}
\section{Linear Model and Least Squares}\label{section:pre_ls}

The linear model is a cornerstone technique in regression analysis, employing the least squares approximation as its primary tool to minimize the sum of squared errors. This method is naturally suited for finding the regression function that minimizes the expected squared error. Over recent decades, linear models have been extensively applied across various fields, including decision-making  \citep{dawes1974linear}, time series analysis \citep{christensen1991linear, lu2017machine}, quantitative finance \citep{menchero2011barra}, and numerous other disciplines like production science, social science, and soil science \citep{fox1997applied, lane2002generalized, schaeffer2004application, mrode2014linear}.


To illustrate, consider an overdetermined system represented by $\bb = \bA\bx $, where $\bA\in \real^{m\times n}$ denotes  the \textit{input data matrix} (also referred to the \textit{predictor variables}), $\bb\in \real^m$ represents  the \textit{observation vector} (or \textit{target/response vector}), and the  number of samples $m$ exceeds  the dimensionality $n$. 
The vector $\bx$ contains the weights of the linear model. 
Typically, $\bA$ will have full column rank since real-world data is often uncorrelated or can be processed to achieve this.
In practical applications, a \textit{bias term} (a.k.a., an \textit{intercept}) is added to the first column of $\bA$ to enable the least squares method to solve:
\begin{equation}\label{equation:ls-bias}
	\widetildebA \widetildebx = 
	[\bm{1} ,\bA ] 
	\begin{bmatrix}
		x_0\\
		\bx
	\end{bmatrix}
	= \bb .
\end{equation}

However, it is common for the equation $\bb = \bA\bx$ not to have an exact solution (the system is \textit{inconsistent}) due to the  being overdetermined---that is, there are more equations than unknowns.
Define the column space of $\bA$ as $\{\bA\bgamma: \,\, \forall \bgamma \in \real^n\}$, denoted by $\cspace(\bA)$.
In essence, when we say $\bb = \bA\bx$ has no solution, it implies that $\bb$ lies outside the column space of $\bA$. 
In other words, the error $\be = \bb -\bA\bx$ cannot be reduced to zero. 
In these cases, the objective shifts to minimizing the error, typically measured by the mean squared error.
The resulting solution $\bx_{LS}$, which minimizes $\normtwo{\bb-\bA\bx_{LS}}^2$, is called the \textit{least squares solution}. The least squares method is a cornerstone of mathematical sciences, and there are numerous resources dedicated entirely to this topic, including works by   \citet{trefethen1997numerical, strang2019linear, strang2021every,  lu2021rigorous}.



\paragrapharrow{Least squares by calculus.}
When $\normtwo{\bb-\bA\bx}^2$ is differentiable and the parameter space of $\bx$ spans the entire space $\real^n$ (i.e., unconstrained optimization), 
the least squares estimate corresponds to the root of the gradient of $\normtwo{\bb-\bA\bx}^2$. 
This leads us to the following lemma.~\footnote{Variants of the least squares problem are explored in Problems~\ref{problem:rls}$\sim$\ref{problem:twls2}.}

\index{Optimality condition}
\begin{lemma}[Least Squares by Calculus]\label{lemma:ols}
Let  $\bA \in \real^{m\times n}$ be a  fixed data matrix with full rank  and $m\geq n$ (i.e., its columns  are linearly independent)~\footnote{See Problem~\ref{prob:als_pseudo1}$\sim$\ref{prob:als_pseudon} for a relaxation of this condition using the pseudo-inverse.}. 
For the overdetermined system $\bb = \bA\bx$, the least squares solution is obtained by  setting the partial derivatives of $\normtwo{\bb-\bA\bx}^2$ in every direction to zero (i.e., the gradient vanishes by Theorem~\ref{theorem:fetmat_opt}, since $f(\bx)=\normtwo{\bb-\bA\bx}^2$ is a convex function by Exercise~\ref{exercise:conv_quad}). This yields the solution: 
$$
\bx_{LS} = (\bA^\top\bA)^{-1}\bA^\top\bb.
$$ 
The value, $\bx_{LS} = (\bA^\top\bA)^{-1}\bA^\top\bb$, is commonly referred to as the \textit{ordinary least squares (OLS)} estimate or simply the \textit{least squares (LS)} estimate of $\bx$.
\end{lemma}
In the lemma, we use the fact $\bA^\top\bA \in \real^{n\times n}$ is invertible since we assume that $\bA$ has full rank and $m\geq n$, . 
%\begin{proof}[of Lemma \ref{lemma:ols}]
%Since $f(\bx)=\normtwo{\bb-\bA\bx}^2$ is a convex function by Exercise~\ref{exercise:conv_quad}, by Theorem~\ref{theorem:fetmat_opt},  
%the function $f(\bx)$ attains a minimum  at  $\bx_{LS}$ when the gradient $\nabla f(\bx)=\bzero$. The gradient of $\normtwo{\bb-\bA\bx}^2$ is given by $2\bA^\top\bA\bx -2\bA^\top\bb$. $\bA^\top\bA$ is invertible since we assume $\bA$ is fixed and has full rank with $m\geq n$. 
%Consequently, the OLS solution for $\bx$ is $\bx_{LS} = (\bA^\top\bA)^{-1}\bA^\top\bb$, which completes the proof.
%\end{proof}


\begin{definition}[Normal Equation]\label{definition:normal-equation-als}
The condition for the gradient of $\normtwo{\bb-\bA\bx}^2$ to be zero can be expressed as $\bA^\top\bA \bx_{LS} = \bA^\top\bb$. The equation is also known as the \textit{normal equation}. 
Under the assumption that $\bA$ has full rank with $m\geq n$, it follows that $\bA^\top\bA$ is invertible, implying $\bx_{LS} = (\bA^\top\bA)^{-1}\bA^\top\bb$.
\end{definition}

%\index{Convex functions}
%\begin{figure}[h!]
%	\centering  
%	\vspace{-0.35cm} 
%	\subfigtopskip=2pt 
%	\subfigbottomskip=2pt 
%	\subfigcapskip=-5pt 
%	\subfigure[A convex function.]{\label{fig:convex-1}
%		\includegraphics[width=0.26\linewidth]{./imgs/convex.pdf}}
%	\subfigure[A concave function.]{\label{fig:convex-2}
%		\includegraphics[width=0.26\linewidth]{./imgs/concave.pdf}}
%	\subfigure[A random function.]{\label{fig:convex-3}
%		\includegraphics[width=0.26\linewidth]{./imgs/convex-none.pdf}}
%	\caption{Three functions.}
%	\label{fig:convex-concave-none}
%\end{figure}
%However, it is not certain whether the least squares estimate obtained in Lemma~\ref{lemma:ols} is the smallest, largest, or neither.
%An example illustrating this ambiguity is presented in Figure~\ref{fig:convex-concave-none}. 
%What we can confidently assert is the existence of at least one root for the gradient of the function  $f(\bx)=\normtwo{\bb-\bA\bx}^2$, and this root is a necessary condition for the minimum point rather than a sufficient condition.
%The following remark provides clarification on this matter.
%\begin{remark}[Verification of Least Squares Solution]
%	Why does a zero gradient imply the least mean squared error? 
%	The typical explanation stems from convex analysis, as we will see shortly.  
%	However, here we directly confirm that the OLS solution indeed minimizes the mean squared error.  
%	For any $\bx \neq \bx_{LS}$, we have 
%	\begin{equation}
%		\begin{aligned}
%			\normtwo{\bb - \bA\bx}^2 &= \normtwo{\bb - \bA\bx_{LS} + \bA\bx_{LS} - \bA\bx}^2
%			= \normtwo{\bb-\bA\bx_{LS} + \bA (\bx_{LS} - \bx)}^2 \\
%			&=\normtwo{\bb-\bA\bx_{LS}}^2 + \normtwo{\bA(\bx_{LS} - \bx)}^2 + 2\left(\bA(\bx_{LS} - \bx)\right)^\top(\bb-\bA\bx_{LS}) \\ 
%			&=\normtwo{\bb-\bA\bx_{LS}}^2 + \normtwo{\bA(\bx_{LS} - \bx)}^2 + 2(\bx_{LS} - \bx)^\top(\bA^\top\bb - \bA^\top\bA\bx_{LS}), \nonumber
%		\end{aligned} 
%	\end{equation}
%	where the third term is zero due to the normal equation, and $\normtwo{\bA(\bx_{LS} - \bx)}^2 \geq 0$. Therefore,
%	$
%	\normtwo{\bb - \bA\bx}^2 \geq \normtwo{\bb-\bA\bx_{LS}}^2. 
%	$
%	Thus, we demonstrate that the OLS estimate indeed corresponds to the minimum, rather than the maximum or a saddle point~\footnote{
%		A \textit{saddle point} is a point at which the gradient vanishes (a \textit{stationary point}), and  there exists a direction where the objective function decreases and another direction where it increases.
%	}, using the calculus approach.
%	As a matter of fact, this condition from the least squares estimate is also known as the \textit{sufficiency of stationarity under convexity}. When $\bx$ is defined over the entire space $\real^n$, this condition is also known as the \textit{necessity of stationarity under convexity}.
%\end{remark}
%\index{Saddle point}

Another question that arises: Why does the normal equation seemingly ``magically" yield solutions for  $\bx$?
A simple example can help illustrate the answer. The equation $x^2=-1$ has no real solution. 
However, $x\cdot x^2 = x\cdot (-1)$ does have a real solution $\hat{x} = 0$, in which case, $\hat{x}$ minimizes the difference between $x^2$ and $-1$, making them as close as possible.

\begin{example}[Altering the Solution Set by Left Multiplication]
Consider the  data matrix and target vector:
$\tiny
\bA=
\begin{bmatrix}
-3 & -4 \\
4 & 6  \\
1 & 1
\end{bmatrix}
$
and
$
\bb=
\tiny
\begin{bmatrix}
1  \\
-1   \\
0
\end{bmatrix}
.
$
It can be easily verified that the system $\bA\bx = \bb$ has no solution for $\bx$. 
However, if we multiply both sides on the left by
$
\bB=
\scriptsize
\begin{bmatrix}
0 & -1 & 6\\
0 & 1  & -4
\end{bmatrix},
$
then  $\bx_{LS} = [1/2, -1/2]^\top$ becomes the solution to $\bB\bA\bx= \bB\bb$. 
This example illustrates why the normal equation can lead to the least squares solution. Multiplying a linear system on the left alters the solution set, effectively projecting the problem into a different space where the least squares solution can be computed.
\end{example}


\paragrapharrow{Rank-deficiency.}
In this discussion, we assume that $\bA\in \real^{m\times n}$ has full rank with $m\geq n$, ensuring that $\bA^\top\bA$ is invertible. 
However, if two or more columns of $\bA$ are perfectly correlated, the matrix $\bA$ becomes deficient, and $\bA^\top\bA$ becomes singular. 
To address this issue, one approach is to choose $\bx$ that minimizes $\normtwo{\bx_{LS}}^2$ while satisfying the normal equation. 
That is, we select the least squares solution with the smallest magnitude.
\citet{lu2021numerical} discussed how to use UTV decomposition and singular value decomposition (SVD) to address this rank-deficient least squares problems.
See Problems~\ref{prob:als_pseudo1}$\sim$\ref{prob:als_pseudon} or the following paragraph for more insights.

\index{Contion number}
\index{Tikhonov regularization}
\index{$\ell_2$ regularization}
\paragrapharrow{Regularizations and stability.}
A common  problem that arise in the ordinary least square solution is the near-singularity of $\bA$.
Let the SVD of $\bA$ be $\bA=\bU\bSigma\bV^\top\in\real^{m\times n}$, where $\bU\in\real^{m\times m}$ and $\bV\in\real^{n\times n}$ are orthogonal, and the main diagonal of $\bSigma\in\real^{m\times n}$ contains the singular values (Theorem~\ref{theorem:full_svd_rectangular}). Consequently, $\bA^\top\bA = \bV(\bSigma^\top\bSigma)\bV^\top \triangleq \bV\bS\bV^\top$, where $\bS\triangleq \bSigma^\top\bSigma  = \diag(\sigma_1^2, \sigma_2^2, \ldots,\sigma_n^2)\in\real^{n\times n}$ contains the squared singular values of $\bA$. When $\bA$ is nearly singular, $\sigma_n^2\approx 0$, making the inverse operation $(\bA^\top\bA)^{-1} = \bV\bS^{-1}\bV^\top$ numerically unstable. 
As a result, the solution $\bx_{LS} =(\bA^\top\bA)^{-1}\bA^\top\bb $ may diverge.
To address this issue, we typically  add an $\ell_2$ regularization term to obtain the solution for the following optimization problem:
\begin{equation}
	\bx_{Tik} = \mathop{\argmin}_{\bx} \normtwo{\bb-\bA\bx}^2 +\lambda\normtwo{\bx}^2.
\end{equation}
This method is known as the  \textit{Tikhonov regularization method} (or simply the $\ell_2$ regularized method) \citep{tikhonov1963solution}.
The gradient of the problem is $2(\bA^\top\bA+\lambda\bI)\bx-2\bA^\top\bb$. Thus, the least squares solution is given by 
$
\bx_{Tik} = (\bA^\top\bA+\lambda\bI)^{-1}\bA^\top\bb.
$
The inverse operation becomes $(\bA^\top\bA+\lambda\bI)^{-1} = \bV(\bS+\lambda\bI)^{-1}\bV^\top$, where $\widetildebS\triangleq(\bS+\lambda\bI)=\diag(\sigma_1^2+\lambda, \sigma_2^2+\lambda, \ldots,\sigma_n^2+\lambda)$. 
The solutions for OLS and Tikhonov regularized LS are given, respectively, by 
\begin{equation}
	\begin{aligned}
		\bx_{LS} &= (\bA^\top\bA)^{-1}\bA^\top\bb = \bV\left(\bS^{-1}\bSigma\right)\bU^\top\bb;\\
		\bx_{Tik} &= (\bA^\top\bA+\lambda\bI)^{-1}\bA^\top\bb = \bV\left((\bS+\lambda\bI)^{-1}\bSigma\right)\bU^\top\bb,\\
	\end{aligned}
\end{equation}
where the main diagonals of $\left(\bS^{-1}\bSigma\right)$ are $\diag(\frac{1}{\sigma_1}, \frac{1}{\sigma_2}, \ldots, \frac{1}{\sigma_n})$; and the main diagonals of $\left((\bS+\lambda\bI)^{-1}\bSigma\right)$ are $\diag(\frac{\sigma_1}{\sigma_1^2+\lambda}, \frac{\sigma_2}{\sigma_2^2+\lambda}, \ldots, \frac{\sigma_n}{\sigma_n^2+\lambda})$. The latter solution is more stable if $\lambda$ is greater than the   smallest nonzero squared singular value.
The condition number becomes smaller if  the smallest singular value $\sigma_n$ is close to zero (see \eqref{equation:cond_pd_ineq} or \citet{lu2021numerical} for more details):
$$
\kappa(\bA^\top\bA) = \frac{\sigma_1^2}{\sigma_n^2}
\qquad \rightarrow \qquad
\kappa(\bA^\top\bA+\lambda\bI) = \frac{\lambda+\sigma_1^2}{\lambda+\sigma_n^2}.
$$
Tikhonov regularization effectively prevents  divergence  in the least squares solution 
$\bx_{LS} = (\bA^\top\bA)^{-1} \bA^\top \bb$ when the matrix $\bA$ is nearly singular or even rank-deficient. This improvement enhances the convergence properties of both the LS algorithm and its variants, such as alternating least squares,  while addressing identifiability issues  in various settings (see, for example, \citet{gillis2020nonnegative, lu2021numerical}). As a result, Tikhonov regularization has become a widely applied technique.

\index{Data least squares}
\index{Total least squares}
\paragrapharrow{Data least squares.}
While the OLS method accounts for errors in the response variable $\bb$, the \textit{data least sqaures (DLS)} method considers errors in the predictor variables:
\begin{equation}
	\bx_{DLS} = \mathop{\argmin}_{\bx, \widetildebA} \normfbig{\widetildebA}^2 \quad \text{s.t.}\quad \bb\in\cspace(\bA+\widetildebA),
\end{equation}
where $\widetildebA$ represents a perturbation of $\bA$ (i.e., a noise in the predictor variables).
That is, $(\bA+\widetildebA) \bx_{DLS} = \bb$, assuming the measured response $\bb$ is noise-free.
The Lagrangian function and its gradient w.r.t. $\bx$ are, respectively, given by
$$
\begin{aligned}
	L(\bx, \widetildebA, \blambda) &= \trace(\widetildebA\widetildebA^\top) +\blambda^\top (\bA\bx+\widetildebA\bx-\bb);\\
	\nabla_{\widetildebA} L(\bx, \widetildebA,\blambda) &= \widetildebA+\blambda\bx^\top = \bzero \quad\implies\quad \widetildebA=-\blambda\bx^\top,
\end{aligned}
$$
where $\blambda\in\real^m$  is a vector of Lagrange multipliers.
Substituting the value of the vanishing gradient into $(\bA+\widetildebA) \bx = \bb$ yields $\blambda = \frac{\bA\bx-\bb}{\bx^\top\bx}$ and $\widetildebA=-\frac{(\bA\bx-\bb)\bx^\top}{\bx^\top\bx} $.
Therefore, using the invariance of cyclic permutation of factors in trace,  the objective function becomes 
$$
\mathop{\argmin}_{\bx}
\frac{(\bA\bx-\bb)^\top (\bA\bx-\bb)}{\bx^\top\bx} .
$$

\paragrapharrow{Total least squares.} Similar to  data least squares, the \textit{total least squares (TLS)} method considers errors in both the predictor variables and the response variables. The TLS problem can be formulated as:
\begin{equation}
	\bx_{TLS} = \mathop{\argmin}_{\bx, \widetildebA, \widetildebb} \normf{[\widetildebA, \widetildebb]}^2, 
	\quad \text{s.t.}\quad (\bb+\widetildebb)\in\cspace(\bA+\widetildebA), 
\end{equation}
where $\widetilde{\bA}$ and $\widetilde{\bb}$ are perturbations in the predictor variables and the response variable, respectively.
Let $\bC\triangleq[\bA,\bb]\in\real^{m\times (n+1)}$,  $\bD\triangleq[\widetildebA, \widetildebb]\in\real^{m\times (n+1)}$, and $\by\triangleq\footnotesize\begin{bmatrix}
	\bx\\
	-1
\end{bmatrix}$, the problem can be equivalently stated as
\begin{equation}
	\bx_{TLS} = \mathop{\argmin}_{\by, \bD} \normf{\bD}^2, 
	\quad \text{s.t.}\quad \bD\by = -\bC\by, 
\end{equation}


\section{Nonlinear Least Squares Problem}\label{section:nonlinear_l2}
Building upon the linear least squares problem, we now consider the general form of the nonlinear least squares problem: 
\begin{equation}\label{equation:nonlinear_ols}
	f(\bx) = \frac{1}{2} \sum_{j=1}^{m} r_j^2(\bx)= \frac{1}{2} \normtwo{\br(\bx)}^2,
\end{equation}
where $r_j : \real^n \rightarrow \real$ are differentiable functions, and we assume $m \geq n$. The functions $r_j$ are referred to as \textit{residuals}.
 For convenience, we define the residual vector $\br : \real^n \rightarrow \real^m$ as
$ \br(\bx) \triangleq [r_1(\bx), r_2(\bx), \ldots, r_m(\bx)]^\top. $
Using this definition, the function $f(\bx)$ can be written as $f(\bx) = \frac{1}{2} \normtwo{\br(\bx)}^2$.
When $\br(\bx) = \bA\bx-\bb$, the problem becomes the linear least squares case discussed in the preceding section.

Now, consider the case where $m=n$. If we have a good initial estimate $\bx^{(1)}$, and the Jacobian $\bJ(\bx^{(t)})$ of the residual vector $\br(\bx)$ is nonsingular, then the \textit{Newton-Raphson} method provides a fast and accurate solution. 
A simple approach is then to combine the  Newton-Raphson method with line
search. The typical update rule at $t$-th iteration is
\begin{subequations}\label{equation:newton_raphson_nonlin}
\begin{align}
\bd^\toptzero &\leftarrow  \text{ solve }\bJ(\bx^\toptzero) \bd = -\br(\bx^\toptzero);\\
\bx^\toptone&\leftarrow \bx^\toptzero + \eta_t  \bd^\toptzero;
\end{align}
\end{subequations}
The search direction $\bd^\toptzero$ is a descent direction such that $\nabla f(\bx^\toptzero)^\top \bd^\toptzero \leq 0$.

More generally,
problem~\eqref{equation:nonlinear_ols} is an unconstrained optimization problem that can be solved using any of the algorithms discussed earlier. Here, we directly provide the gradient and Hessian matrix of $f(\bx)$:
\begin{subequations}
\begin{align}
\nabla f(\bx) &= \bJ(\bx)^\top \br(\bx),  \label{equation:gaus_new_grad} \\
\nabla^2 f(\bx) &= \bJ(\bx)^\top \bJ(\bx) + \sum_{i=1}^{m} r_i(\bx) \nabla^2 r_i(\bx), \label{equation:gaus_new_hess}
\end{align}
\end{subequations}
where $\bJ(\bx) \in \real^{m \times n}$ is the Jacobian matrix of the residual vector $\br(\bx)$ at point $\bx$. 
Note that $\nabla^2 f(\bx)$ consists of two components:  $\bJ(\bx)^\top \bJ(\bx)$ and $\sum_{i=1}^{m} r_i(\bx) \nabla^2 r_i(\bx)$. 
The computational difficulty of these two components differs: while calculating $\nabla f(\bx)$ naturally provides the Jacobian matrix of $\br(\bx)$ without extra effort, computing the second term of the Hessian matrix requires evaluating each $\nabla^2 r_i(\bx)$, which incurs additional computational cost. Many nonlinear least squares algorithms are designed to leverage this distinction.



\begin{algorithm}[h] 
\caption{Gauss-Newton Method}
\label{alg:gauss_newton}
\begin{algorithmic}[1] 
\Require A function $f(\bx) = \frac{1}{2} \sum_{j=1}^{m} r_j^2(\bx)=\frac{1}{2} \br(\bx)^\top\br(\bx)$; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}\in\real^n$;
\For{$t=1,2,\ldots$}
\State Compute the residual vector $ \br^\toptzero $ and the Jacobian matrix $ \bJ^\toptzero $;
\State \algoalign{Compute the QR decomposition of $ \bJ^\toptzero $: $ \bJ^\toptzero = \bQ^\toptzero \bR^\toptzero $, where $ \bQ^\toptzero \in \real^{m \times n}, \bR^\toptzero \in \real^{n \times n} $;}
\State Solve the equation $ \bR^\toptzero \bd = -\bQ^\toptzeroTOP \br^\toptzero $ to obtain the descent direction $ \bd^\toptzero $.
\State Use line search criteria to compute the step size $ \eta_t $.
\State Update: $ \bx^\toptone \leftarrow \bx^\toptzero + \eta_t \bd^\toptzero $.
\EndFor
\State {\bfseries Return:}  $\bx\leftarrow \bx^\toptzero$;
\end{algorithmic} 
\end{algorithm}

\index{Gauss-Newton method}
\subsection{Gauss-Newton Method}
The \textit{Gauss-Newton} method forms the foundation for highly efficient techniques we will discuss in the upcoming sections, aimed at solving nonlinear least squares problems. It can be considered a variant of Newton's method that incorporates line search. This approach relies on using first derivatives of the vector function components or approximates the Hessian matrix by utilizing only the Jacobian matrix. In certain cases, it achieves quadratic convergence similar to Newton's method applied to general optimization problems.

\paragrapharrow{Gauss-Newton by affine approximation.}
The Gauss-Newton method employs an affine approximation of the components of $\br$ (an \textit{affine model} of $\br$) around the iteration point $ \bx^\toptzero $. 
Assuming that  $\br$ is twice continuously differentiable, according to the linear approximation theorem (Theorem~\ref{theorem:linear_approx}),
\begin{equation}\label{equation:gauss_new_affinemodel0}
\br(\bx^\toptzero + \bd) = \br(\bx^\toptzero) + \bJ(\bx^\toptzero) \bd + \mathcalO(\normtwo{\bd}^2) .
\end{equation}
This implies that
\begin{equation}\label{equation:gauss_new_affinemodel}
\ell_t(\bd) \triangleq \br(\bx^\toptzero) + \bJ(\bx^\toptzero) \bd
\end{equation}
is a good approximation  when $\bd$ is sufficiently small. Substituting this into the definition  of $f$ in   \eqref{equation:nonlinear_ols}, we obtain
$$
f(\bx^\toptzero + \bd) \approx \psi_t(\bd) \triangleq \tfrac{1}{2} \ell_t(\bd)^\top \ell_t(\bd) 
= f(\bx^\toptzero) + \bd^\top \bJ^\toptzeroTOP \br^\toptzero + \tfrac{1}{2} \bd^\top \bJ^\toptzeroTOP \bJ^\toptzero \bd ,
$$
where $\br^\toptzero \triangleq \br(\bx^\toptzero)$ and $\bJ^\toptzero \triangleq \bJ(\bx^\toptzero)$. The so-called \textit{Gauss-Newton step} $\bd_{\text{gn}}^\toptzero$ minimizes $\psi_t(\bd)$,
$$
\bd_{\text{gn}}^\toptzero = \argmin_{\bd} \left\{ \psi_t(\bd) \triangleq\tfrac{1}{2}\normtwo{\br^\toptzero + \bJ(\bx^\toptzero) \bd}^2\right\} .
$$
The approximation $\psi_t$ models the behavior of $f$ near the current iterate, and the gradient of $\psi_t$ is 
\begin{equation}\label{equation:gaun_nabla_psit}
\nabla \psi_t(\bd) = \bJ^\toptzeroTOP \br^\toptzero + ( \bJ^\toptzeroTOP \bJ^\toptzero ) \bd .
\end{equation}
By the first-order optimality condition (Theorem~\ref{theorem:fermat_fist_opt}),
the gradient is zero at a minimizer for $\psi_t$,  leading to the \textit{Gauss-Newton equation}:
\begin{equation}\label{equation:gn_first_equa}
	( \bJ^\toptzeroTOP \bJ^\toptzero ) \bd_{\text{gn}}^\toptzero = -\nabla f(\bx^\toptzero) = -\bJ^\toptzeroTOP \br^\toptzero .
\end{equation}
This shows that \eqref{equation:gn_first_equa} is the normal equation (Definition~\ref{definition:normal-equation-als})
for the overdetermined  linear system
$
\br(\bx^\toptzero) + \bJ(\bx^\toptzero) \bd = \bzero.
$
As mentioned previously, from linear algebra, we know that regardless of whether $\bJ^\toptzero$ has full rank, Equation~\eqref{equation:gn_first_equa} always has a solution, effectively representing a linear least squares problem (see Problem~\ref{prob:als_pseudo1}$\sim$\ref{prob:als_pseudon}). 
The optimality condition for the problem is:
\begin{equation}\label{equation:gaus_newton_ls}
\bd_{\text{gn}}^\toptzero =\argmin_{\bd} \quad \tfrac{1}{2} \normtwobig{\bJ^\toptzero \bd + \br^\toptzero}^2.
\end{equation}
In solving the linear least squares problem, we only need to perform the QR decomposition on $ \bJ^\toptzero $ (Theorem~\ref{theorem:qr-decomposition}), so the matrix $ \bJ^\toptzeroTOP \bJ^\toptzero $ does not need to be computed explicitly; see, for example, \citet{lu2021numerical} for more details.

\paragrapharrow{Gauss-Newton by approximated Hessian.}
We have outlined the Gauss-Newton method using an affine approximation of the components of $\br$.
Alternatively, since second derivative terms related to $r_i(\bx)$ in the Hessian matrix are challenging to compute (see Equation~\eqref{equation:gaus_new_hess}), the Gauss-Newton method simplifies this by ignoring these terms and directly employing $\bJ(\bx)^\top \bJ(\bx)$ as the approximate Hessian matrix to solve the Newton equation (\eqref{equation:newton_secon_approx_eq3} in Newton's method).
The descent direction $\bd_{\text{gn}}^\toptzero$ generated by the Gauss-Newton method satisfies (derived from the Newton equation in \eqref{equation:newton_secon_approx_eq3})
\begin{equation}\label{equation:gaus_new_normal}
	\begin{aligned}
		\nabla^2 f(\bx^\toptzero)\bd_{\text{gn}}^\toptzero &= -\nabla f(\bx^\toptzero);\\
		\underrightarrow{ \text{approximated by} }\quad
		\bJ^\toptzeroTOP \bJ^\toptzero \bd_{\text{gn}}^\toptzero &= -\bJ^\toptzeroTOP \br^\toptzero.
	\end{aligned}
\end{equation}


\paragrapharrow{The algorithm.}






If $\bJ^\toptzero$ has full column rank, then the matrix $\bA^\toptzero \triangleq \bJ^\toptzeroTOP \bJ^\toptzero$ also has full column rank and hence is positive definite. This ensures that $\psi_t(\bd)$ has a unique minimizer, which can be found by solving the Gauss-Newton equation \eqref{equation:gn_first_equa}. 
However, this does not guarantee that $\bx^\toptzero + \bd_{\text{gn}}^\toptzero$ is a minimizer of $f$, since $\psi_t(\bd)$  only approximates  $f(\bx^\toptzero + \bd)$. 
Nonetheless, Theorem~\ref{theorem:uncons_des_dir} guarantees that the solution $\bd_{\text{gn}}^\toptzero = -( \bJ^\toptzeroTOP \bJ^\toptzero )^{-1} \nabla f(\bx^\toptzero)$ is a descent direction. Consequently, $\bd_{\text{gn}}^\toptzero$ can be utilized as $\bd^\toptzero$ in descent methods (Algorithm~\ref{alg:struc_gd_gen}). 
An advantage of the Gauss-Newton method over pure Newton's method is that it does not always guarantee that $ \bd_{\text{n}}^\toptzero $ from the Newton equation is a descent direction. In contrast, the Gauss-Newton method employs a positive semidefinite approximation of the Hessian matrix, potentially yielding a more effective descent direction.

The typical step at $t$-th iteration is
$$
\begin{aligned}
\bd_{\text{gn}}^\toptzero&\leftarrow \text{solution of } ( \bJ^\toptzeroTOP \bJ^\toptzero ) \bd = -\bJ^\toptzeroTOP \br^\toptzero;  \\
\bx^\toptone &\leftarrow \bx^\toptzero + \eta_t \bd_{\text{gn}}^\toptzero.\\
\end{aligned}
$$
where $\eta_t$ is determined through line search. 
While the \textit{classical Gauss-Newton method (a.k.a., pure Gauss-Newton method)} uses a constant stepsize ($\eta_t = 1$), incorporating line search ensures convergence under certain conditions (Theorem~\ref{theorem:gaus_new_glob_conv}), specifically if:
\begin{enumerate}
\item The level set $\lev[f, \bx^{(1)}]\triangleq\{\bx \mid f(\bx) \leq \bx^{(1)}\}$ is bounded.
\item the Jacobian $\bJ(\bx^\toptzero)$ has full rank in all steps.
\end{enumerate}
The framework for the Gauss-Newton method is detailed in Algorithm~\ref{alg:gauss_newton}. 




\paragrapharrow{Approximation issues.}
A natural question arises: Given that the Gauss-Newton method employs an approximate matrix to solve the Newton equation, under what circumstances is this approximation valid?
Intuitively, based on the expression for the Hessian matrix in  \eqref{equation:gaus_new_hess}, the approximation  $ \bJ^\toptzeroTOP \bJ^\toptzero $ is meaningful when it plays a dominant role.  
A sufficient condition for this is that at the optimal point $ \bx^* $, the residuals $ r_i(\bx^*) $ are very small. 
In such cases, the Gauss-Newton method closely resembles Newton's method and shares many of its properties. Conversely, if the residual vector $ \br(\bx^*) $ has a large norm,  using $ \bJ^\toptzeroTOP \bJ^\toptzero $ alone may not adequately approximate $ \nabla^2 f(\bx^\toptzero) $, potentially leading to slow convergence or even divergence of the Gauss-Newton method. See Section~\ref{section:quasi_largeresi} for a remedy.


Newton's method for optimization typically exhibits quadratic final convergence  (Theorem~\ref{theorem:conv_classNewton}). This level of convergence is generally not achieved by the Gauss-Newton method. 
However, if   $\br(\bx^*) = \bzero$, then near $\bx^*$, we have  $\nabla^2 \psi_t(\bzero) \approx \nabla^2 f(\bx^\toptzero)$,  which can also result in quadratic convergence with the Gauss-Newton method. 
Superlinear convergence can be expected if the functions $\{r_i\}$ exhibit small curvatures or if the values $\{\abs{r_i(\bx^*)}\}$ are small. Nonetheless, in general, one should anticipate linear convergence. 
Since $f(\bx^*) = \frac{1}{2} \normtwo{\br(\bx^*)}^2$, 
it is noteworthy that the value of $f(\bx^*) $ also influences the speed of convergence.

\begin{example}[Failure of Classical Gauss-Newton Methods \citep{madsen2010and}]
Consider the simple problem with $n=1$ and $m=2$:
$$
\br(x) = \begin{bmatrix} x + 1 \\ \lambda x^2 + x - 1 \end{bmatrix}, \quad f(x) = \frac{1}{2}(x+1)^2 + \frac{1}{2}(\lambda x^2 + x - 1)^2 .
$$
It follows that
$
f'(x) = 2\lambda^2 x^3 + 3\lambda x^2 - 2(\lambda - 1)x ,
$
indicating that  $x=0$ is a stationary point for $f$. 
Additionally, the second derivative is given by
$
f''(x) = 6\lambda^2 x^2 + 6\lambda x - 2(\lambda - 1) .
$
This shows that if $\lambda < 1$, then $f''(0) > 0$, so $x=0$ is a local minimizer---and indeed, it is the global minimizer (Remark~\ref{remark:charac_statpoint}).
The Jacobian is
$$
\bJ(x) = \begin{bmatrix} 1 \\ 2\lambda x + 1 \end{bmatrix} ,
$$
and the classical Gauss-Newton method ($\eta_t=1$) yields
$$
x_{\text{new}} = x - \frac{2\lambda^2 x^3 + 3\lambda x^2 - 2(\lambda - 1)x}{2 + 4\lambda x + 4\lambda^2 x^2} .
$$
Now, if $\lambda \neq 0$ and $x$ is close to zero, then
$$
x_{\text{new}} = x + (\lambda - 1)x + \mathcalO(x^2) = \lambda x + \mathcalO(x^2) .
$$
Thus, if $\abs{\lambda}< 1$, we observe  linear convergence. If $\lambda < -1$, then the classical Gauss-Newton method  fails to find the minimizer. For instance with $\lambda = -2$ and $x^{(1)} = 0.1$, the iterates display seemingly chaotic behavior:
$$
x^{(1)} = 0.1         \;\implies\;
x^{(2)} = -0.3029     \;\implies\;
x^{(3)} = 0.1368      \;\implies\;
x^{(4)} = -0.4680.
$$
Finally, if $\lambda = 0$, then $x_{\text{new}} = x - x = 0$, meaning the solution is found in one step. 
This is because $\br$  becomes an affine function in this case.
\end{example}




\paragrapharrow{Convergence analysis.}
Next, we present the convergence properties of the Gauss-Newton method. 
When $ \bJ^\toptzeroTOP \bJ^\toptzero $ dominates the Hessian matrix \eqref{equation:gaus_new_hess}, the Gauss-Newton method can achieve faster convergence rates.
To be more specific, similar to Newton's method, we also provide the local convergence results for the Gauss-Newton method.


\begin{theoremHigh}[Local Convergence  of Gauss-Newton]\label{theorem:gaus_new_local_conv}
Suppose $ r_i(\bx), i=1,2,\ldots,m $ are twice continuously differentiable, and $ \bx^* $ is the optimal solution to the nonlinear least squares problem \eqref{equation:nonlinear_ols}. If both the Hessian matrix $ \nabla^2 f(\bx) $ and its approximation $ \bJ(\bx)^\top \bJ(\bx) $ are  Lipschitz continuous in a neighborhood $\sB(\bx^*, \varepsilon)$ of $ \bx^* $. 
Given that the initial point is sufficiently close to $\bx^*$, the sequence $\{\bx^\toptzero\}_{t>0}$ generated by the Gauss-Newton method (Algorithm~\ref{alg:gauss_newton}) with a constant stepsize  $ \eta_t = 1 $ satisfies
$$
\normtwobig{\bx^\toptone - \bx^*} \leq C \normtwo{\big[(\bJ^*)^\top \bJ^*\big]^{-1} \bH^*} \normtwo{\bx^\toptzero - \bx^*} + \mathcalO(\normtwobig{\bx^\toptzero - \bx^*}^2),
$$
where $ \bH^*\triangleq \sum_{i=1}^m r_i(\bx^*) \nabla^2 r_i(\bx^*) $ is the part of the Hessian matrix $ \nabla^2 f(\bx^*) $ that remains after removing $ \bJ(\bx^*)^\top \bJ(\bx^*) $, $ C > 0 $ is a constant.
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:gaus_new_local_conv}]
By the update rule and the fact that $\nabla f(\bx^*)=\bzero$, we have 
$$
\bx^\toptone - \bx^* = \bx^\toptzero + \bd_{\text{gn}}^\toptzero - \bx^*
= \big[\bJ^\toptzeroTOP \bJ^\toptzero\big]^{-1} 
\Big\{\bJ^\toptzeroTOP \bJ^\toptzero (\bx^\toptzero - \bx^*) + \nabla f(\bx^*) - \nabla f(\bx^\toptzero)\Big\}.
$$
By the fundamental theorem of calculus (Theorem~\ref{theorem:fund_theo_calculu}), we have 
$$
\begin{aligned}
&\nabla f(\bx^\toptzero) - \nabla f(\bx^*)
= \int_0^1 \nabla^2 f\big(\bx^* + \mu(\bx^\toptzero - \bx^*)\big) (\bx^\toptzero - \bx^*) d\mu\\
&= \int_0^1 \bJ^\top \bJ\big(\bx^* + \mu(\bx^\toptzero - \bx^*)\big) (\bx^\toptzero - \bx^*) d\mu +
\int_0^1 \bH\big(\bx^* + \mu(\bx^\toptzero - \bx^*)\big) (\bx^\toptzero - \bx^*) d\mu,
\end{aligned}
$$
where $ \bJ^\top \bJ(\bx) \triangleq \bJ^\top(\bx) \bJ(\bx) $, and $ \bH(\bx) \triangleq \nabla^2 f(\bx) - \bJ^\top \bJ(\bx) $ denotes the remaining part of the Hessian matrix. Combining the two equalities and taking norms, we have
$$
\begin{aligned}
&\normtwo{\big[\bJ^\toptzeroTOP \bJ^\toptzero\big] (\bx^\toptone - \bx^*) }=
\normtwo{\bJ^\toptzeroTOP \bJ^\toptzero (\bx^\toptzero - \bx^*) - \big(\nabla f(\bx^\toptzero) - \nabla f(\bx^*) \big)}\\
&\leq \int_0^1 \normtwo{\big[\bJ^\top \bJ(\bx^\toptzero) - \bJ^\top \bJ\big(\bx^* + \mu(\bx^\toptzero - \bx^*)\big)\big] (\bx^\toptzero - \bx^*)} d\mu \\
&\quad +
\int_0^1 \normtwo{\bH\big(\bx^* + \mu(\bx^\toptzero - \bx^*)\big) (\bx^\toptzero - \bx^*)} d\mu\\
&\leq \frac{L}{2} \normtwobig{\bx^\toptzero - \bx^*}^2 + C \normtwo{\bH^*} \normtwobig{\bx^\toptzero - \bx^*},
\end{aligned}
$$
where $ L $ is the Lipschitz constant of $ \bJ^\top \bJ(\bx) $. The last inequality holds because we use $ \bH^* $ to approximate $ \bH(\bx^* + \mu(\bx^\toptzero - \bx^*)) $. By continuity, there exists $ C > 0 $ and a neighborhood $ \sB(\bx^*, \varepsilon) $ of $ \bx^* $ such that for any $ \bx \in \sB(\bx^*, \varepsilon) $, $ \normtwo{\bH(\bx)} \leq C \normtwo{\bH(\bx^*)} $, which establishes the result.
\end{proof}
Theorem~\ref{theorem:gaus_new_local_conv} indicates that if $ \normtwo{\bH(\bx^*)} $ is sufficiently small, the Gauss-Newton method can achieve a linear rate of convergence; and when $ \normtwo{\bH(\bx^*)} = 0 $, the convergence rate is quadratic. However, if $ \normtwo{\bH(\bx^*)} $ is large, the Gauss-Newton method may fail to converge effectively.

From the preceding discussion, it is evident that the nonsingularity of the Jacobian matrix $ \bJ^\toptzero $ plays a crucial role. Thus, we establish the global convergence under this condition.
For the global convergence of the Gauss-Newton method, we impose the following uniform full-rank (uniform eigenvalues) assumption in the neighborhood of the level set \citep{sun2006optimization, liu2020optimization}.
\begin{assumption}[Restricted Strong Convexity]\label{assumption:uniform_sing_jac}
Assume that the Jacobian matrix $ \bJ(\bx) $ has eigenvalues uniformly greater than 0, i.e., there exists $ \alpha > 0 $ such that
\begin{equation}
\normtwo{\bJ(\bx) \bz} \geq \alpha \normtwo{\bz}, \quad \forall \bx \in \sB, \quad \forall \bz \in \real^n,
\end{equation}
where we assume $ \lev[f, f(\bx^{(1)})] $ is bounded, and $ \sB $ is a neighborhood set of the level  set
$
\sL\triangleq\lev[f, f(\bx^{(1)})] \triangleq \{ \bx \mid f(\bx) \leq f(\bx^{(1)}) \}
$
of the initial point $ \bx^{(1)} $.
Specifically,
$\sB(\sL, R)  \triangleq \{\bx \mid  \normtwo{\bx-\by} < R \text{ for some } \by\in \sL\}$ for some $R > 0$.
The assumption is related to the restricted strong convexity as defined in Definition~\ref{definition:res_scss_mat}.
\end{assumption}

Under the above assumptions, we have the following convergence theorem:

\begin{theoremHigh}[Global Convergence of Gauss-Newton]\label{theorem:gaus_new_glob_conv}
Let each residual function $ r_j $ be Lipschitz continuous on a bounded level set $ \sL \triangleq \lev[f, f(\bx^{(1)})] \triangleq \{ \bx \mid f(\bx) \leq f(\bx^{(1)}) \} $, and let the Jacobian matrix $ \bJ(\bx) $ satisfies the uniform full-rank condition in Assumption~\ref{assumption:uniform_sing_jac}. 
If the stepsizes $\eta_t$ satisfies the Wolfe conditions (Definition~\ref{definition:wolfe_cond}), then for the Gauss-Newton method (Algorithm~\ref{alg:gauss_newton}), the sequence $ \{ \bx^\toptzero \} $ satisfies
$$
\lim_{t \to \infty} \left\{\nabla f(\bx^\toptzero)=\bJ^\toptzeroTOP \br^\toptzero\right\} = \bzero.
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:gaus_new_glob_conv}]
Here we directly verify the Zoutendijk condition (Theorem~\ref{theorem:zoutendijk_cond}, the convergence under the Zoutendijk condition is guaranteed by Theorem~\ref{theorem:conv_line_search}). First, choose a bounded level set $ \lev[f, f(\bx^{(1)})] $ and its neighborhood $ \sB(\sL, R)$ sufficiently small so that there exist $ L > 0 $ and $ B > 0 $ such that for any $ \bx, \widetildebx\in \sB(\sL, R) $ and any $ j = 1, 2, \ldots, m $, the following conditions are satisfied:
\begin{align*}
\abs{r_j(\bx)} &\leq B, & \normtwo{\nabla r_j(\bx)} &\leq B, \\
\abs{r_j(\bx) - r_j(\widetildebx)} &\leq L \normtwo{\bx - \widetildebx}, & \normtwo{\nabla r_j(\bx) - \nabla r_j(\widetildebx)} &\leq L \normtwo{\bx - \widetildebx}.
\end{align*}
It follows that for any $ \bx \in \lev[f, f(\bx^{(1)})] $, there exists $ \widetildeB $ such that $ \normtwo{\bJ(\bx)} = \normtwo{\bJ(\bx)^\top} \leq \widetildeB $, and $ \nabla f(\bx) = \bJ(\bx)^\top \br(\bx) $ is a Lipschitz continuous function. Let $ \theta_t $ be the angle between the Gauss-Newton direction $ \bd_{\text{gn}}^\toptzero $ and the negative gradient direction, then
$$
\cos (\theta_t) = - \frac{ \nabla f(\bx^\toptzero)^\top \bd_{\text{gn}}^\toptzero }{ \normtwobig{\bd_{\text{gn}}^\toptzero} \normtwo{\nabla f(\bx^\toptzero)} } = \frac{ \normtwobig{\bJ^\toptzero \bd_{\text{gn}}^\toptzero}^2 }{ \normtwobig{\bd_{\text{gn}}^\toptzero} \normtwo{ \bJ^\toptzeroTOP \bJ^\toptzero \bd_{\text{gn}}^\toptzero} } \geq \frac{ \alpha^2 \normtwobig{\bd_{\text{gn}}^\toptzero}^2 }{ \widetildeB^2 \normtwobig{\bd_{\text{gn}}^\toptzero}^2 } = \frac{ \alpha^2 }{ \widetildeB^2 } > 0.
$$
By Theorem~\ref{theorem:zoutendijk_cond} and Theorem~\ref{theorem:conv_line_search}, it follows that $ \nabla f(\bx^\toptzero) \to \bzero $.
\end{proof}
We observe that the key assumption of Theorem~\ref{theorem:gaus_new_glob_conv} is the uniform full-rank condition in Assumption~\ref{assumption:uniform_sing_jac}. In fact, if $ \bJ^\toptzero $ does not have a  full rank, then the linear system \eqref{equation:gaus_new_normal} has infinitely many solutions. If no additional requirements are imposed on the solution properties, it cannot be concluded that $ \cos (\theta_t) $ is uniformly greater than zero. In this case, convergence may not hold.





\subsection{Levenberg-Marquardt Method}\label{section:nonlinearls_lmmethod}
Levenberg and later Marquardt proposed using a damped Gauss-Newton method (commonly referred to as the LM method, Section~\ref{section:modified_damp_new}) to solve nonlinear least squares problems \citep{levenberg1944method, marquardt1963algorithm}. 
In the \textit{Levenberg-Marquardt (LM) Gauss-Newton method} (or simply the LM method),
the step $\bd_{\text{lm}}^\toptzero$ at iteration point $\bx^\toptzero$ is defined by modifying the Gauss-Newton equation \eqref{equation:gn_first_equa} as follows: 
\begin{equation}\label{equation:damped_gauss_newt_eq}
\begin{aligned}
( \bJ^\toptzeroTOP \bJ^\toptzero + \mu_t \bI) \bd_{\text{lm}}^\toptzero &= -\bJ^\toptzeroTOP \br^\toptzero \\
\text{with } \bJ^\toptzero &\triangleq \bJ(\bx^\toptzero), \quad \br^\toptzero \triangleq \br(\bx^\toptzero), \quad \mu_t \geq 0 .
\end{aligned}
\end{equation}
The \textit{damping parameter} $\mu_t$ serves several purposes:
\begin{enumerate}
\item For all $\mu_t > 0$ the coefficient matrix $\bJ^\toptzeroTOP \bJ^\toptzero + \mu_t \bI$ is positive definite, ensuring that  $\bd_{\text{lm}}^\toptzero$ is a descent direction by Theorem~\ref{theorem:uncons_des_dir}.
\item For large values of $\mu_t$, we have 
$
\bd_{\text{lm}}^\toptzero \approx -\frac{1}{\mu_t} \bJ^\toptzeroTOP \br^\toptzero = -\frac{1}{\mu_t} \nabla f(\bx^\toptzero) ,
$
which is  a short step in the steepest descent direction. This is appropriate if the current iterate is far from the solution.
\item If $\mu_t$ is very small, then $\bd_{\text{lm}}^\toptzero \approx \bd_{\text{gn}}^\toptzero$, which is advantageous in the final stages of iteration when  $\bx^\toptzero$ is close to $\bx^*$. If $f(\bx^*) = \frac{1}{2} \normtwo{\br(\bx^*)}^2= \bzero$ (or very small), this can lead to (almost) quadratic final convergence.
\end{enumerate}

Therefore, the damping parameter affects both the direction and size of the step, allowing for an effective method without specific line search requirements. 
The initial value of $\mu_1$ should be related to the magnitude of the elements in  $\bA^{(1)} = \bJ^{(1)\top} \bJ^{(1)}$, such as
\begin{equation}
	\mu_1 = \tau \cdot \max_i \{ a_{ii}^{(1)} \} ,
\end{equation}
where $\tau$ is a hyperparameter. 
Although the algorithm is not highly sensitive to the choice of $\tau$, a small value like $\tau = 10^{-6}$ is recommended if $\bx^{(1)}$ is believed to be a good approximation of $\bx^*$. 
Otherwise, use $\tau = 10^{-3}$ or even $\tau = 1$ \citep{madsen2010and}. 
During iterations, the size of $\mu_t$ can be updated as described in the damped Newton method (Section~\ref{section:modified_damp_new}). The updating is controlled by the gain ratio
\begin{equation}\label{equation:nonlinearls_gainfactor}
\nu_t = \frac{f(\bx^\toptzero) - f(\bx^\toptone)}{\psi_t(\bzero) - \psi_t(\bd_{\text{lm}}^\toptzero)} ,
\quad
\text{where}\quad \bx^\toptone = \bx^\toptzero + \bd_{\text{lm}}^\toptzero,
\end{equation}
where $\psi_t(\bd) \triangleq 
f(\bx^\toptzero) + \bd^\top \bJ^\toptzeroTOP \br^\toptzero + \tfrac{1}{2} \big(\bd^\top \bJ^\toptzeroTOP +\textcolor{mylightbluetext}{\mu_t\bI}\big) \bJ^\toptzero \bd $.
A high  $\nu_t$ suggests  that $\psi_t(\bd_{\text{lm}}^\toptzero)$ is a good approximation of $f(\bx^\toptzero + \bd_{\text{lm}}^\toptzero)$, prompting a decrease in $\mu_t$ so that the next Levenberg-Marquardt step is closer to the Gauss-Newton step. 
Conversely, if $\nu_t$ is small or  even negative, indicating a poor approximation, $\mu_t$ should be increased to approach the steepest descent direction and reduce the step size.




As previously noted with the Gauss-Newton equation \eqref{equation:gn_first_equa}, the Gauss-Newton step  $\bd_{\text{gn}}^\toptzero$ solves the least squares problem
$\tfrac{1}{2} \normtwobig{\bJ^\toptzero \bd + \br^\toptzero}^2$.
Similarly, the LM damped Gauss-Newton equation in \eqref{equation:damped_gauss_newt_eq} corresponds to the normal equation for the augmented linear system
\begin{equation}\label{equation:damped_gsnew}
\bd_{\text{lm}}^\toptzero
=
\argmin_{\bd} \quad 
\frac{1}{2}
\normtwo{
	\begin{bmatrix} \bJ^\toptzero \\ 
		\sqrt{\mu_t} \bI \end{bmatrix} 
	\bd 
	+ 
	\begin{bmatrix} \br^\toptzero \\ \bzero \end{bmatrix}}^2.
\end{equation}
The most accurate solution can be found via orthogonal transformations using QR decomposition; see, for example, \citet{lu2021numerical}. However, since the solution
$\bd_{\text{lm}}^\toptzero$ is part of an iterative process, it does not need to be computed with high precision, making the normal equations approach in \eqref{equation:damped_gauss_newt_eq} preferable due to lower computational cost.
 

The complete  \textit{Levenberg-Marquardt (LM) Gauss-Newton method} is similar to the LM damped Newton method in Algorithm~\ref{algorithm:leve_dam_new}, except that it employs the damped Gauss-Newton equation \eqref{equation:damped_gauss_newt_eq} instead of the damped Newton equation \eqref{equation:damped_newton_eq}.





\subsection{Trust Region Method}
We introduced the trust region method in Section~\ref{section:des_trust_reg} for solving general optimization problems. Clearly, this method can be applied to nonlinear least squares problems as well. Recall that we established a connection between the trust region method and the damped Newton method in Theorem~\ref{theorem:damped_trust}. Specifically, for nonlinear least squares problems, this connection is made through the damped Gauss-Newton equation \eqref{equation:damped_gauss_newt_eq}. In each step, the trust region method for nonlinear least squares solves the following problem, called the nonlinear least squares subproblem (NLS):
\begin{equation}\label{equation:lms}
\textbf{(NLS)}:\qquad \min_{\bd} \quad \frac{1}{2} \normtwo{\bJ^\toptzero \bd + \br^\toptzero}^2 \quad \text{s.t.} \quad \normtwo{\bd} \leq \Delta_t,
\end{equation}
where $\Delta_t$ is the trust region radius at the $t$-th iteration.
In fact, NLS treats the above quadratic approximation as the trust region subproblem for $\psi_t(\bd)$ (see also \eqref{equation:trs_approx}):
\begin{equation}
\textbf{(NLS$'$)}:\qquad\begin{aligned}
\psi_t(\bd) 
&\triangleq f(\bx^\toptzero) + \nabla f(\bx^\toptzero)^\top \bd + \frac{1}{2} \bd^\top \bB^\toptzero \bd\\
&= \frac{1}{2} \normtwobig{\br^\toptzero}^2 + \bd^\top \bJ^\toptzeroTOP \br^\toptzero + \frac{1}{2} \bd^\top \bJ^\toptzeroTOP \bJ^\toptzero \bd\\
\implies 
\bd_{\text{tr}}^\toptzero &= \argmin_{\bd} \big\{ \psi_t(\bd)  \; \quad \text{s.t.} \quad \normtwo{\bd} \leq \Delta_t\big\}.
\end{aligned}
\end{equation}
This method uses $\bB^\toptzero = \bJ^\toptzeroTOP \bJ^\toptzero$ to approximate the Hessian matrix, which is derived from the Gauss-Newton method. For convenience, we omit the iteration index $t$. 
Subproblem \eqref{equation:lms} represents a trust region subproblem, and its properties were discussed in  Section~\ref{section:des_trust_reg}. Based on the optimality condition for TRS (Theorem~\ref{theorem:trs_kkt}), we can derive the following corollary:
\begin{corollary}[Optimality Condition of TRS for Nonlinear LS]
	The vector $\bd^*$ is a solution to the trust region subproblem
	\begin{equation}
		\min_{\bd} \quad \frac{1}{2} \normtwo{\bJ \bd + \br}^2 \quad \text{s.t.} \quad \normtwo{\bd} \leq \Delta
	\end{equation}
	if and only if $\bd^*$ is feasible and there exists a number $\lambda \geq 0$ such that
	\begin{subequations}
		\begin{align}
			(\bJ^\top \bJ + \lambda \bI) \bd^* &= -\bJ^\top \br, \\
			\lambda (\Delta - \normtwo{\bd^*}) &= 0\\
			(\bB + \lambda \bI) &\succeq \bzero. \label{equation:lms_kkt4}
		\end{align}
	\end{subequations}
	Note that $\bJ^\top \bJ$ is  positive semidefinite, so the condition \eqref{equation:lms_kkt4} is naturally satisfied.
\end{corollary}


To solve the (NLS) problem \eqref{equation:lms}, we follow a procedure similar to that used for trust region subproblems. First, determine $\lambda$ by solving a root-finding problem (see Section~\ref{section:solve_trs}), then obtain the search direction directly from the trust region equation.
Due to the special structure of the NLS, QR decomposition can be utilized, thus avoiding the need to compute $\bJ^\top \bJ + \lambda \bI$. Note that $(\bJ^\top \bJ + \lambda \bI)\bd = -\bJ^\top \br$ is actually a linear least squares problem:
\begin{equation}\label{equation:lm_ls}
	\min_{\bd} \quad \frac{1}{2}\normtwo{
		\begin{bmatrix} \bJ \\ 
			\sqrt{\lambda} \bI \end{bmatrix} 
		\bd 
		+ 
		\begin{bmatrix} \br \\ \bzero \end{bmatrix}}^2.
\end{equation}
The connection between NLS and the LM Gauss-Newton method become clear by \eqref{equation:damped_gsnew} and \eqref{equation:lm_ls}.

The coefficient matrix of this problem has a certain structure. Each time $\lambda$ is changed during trials, the block related to $\bJ$ remains unchanged, so there is no need to repeatedly perform QR decompositions on $\bJ$. Specifically, let $\bJ = \bQ\bR$ be the QR decomposition of $\bJ$, where $\bQ \in \real^{m \times n}$ and $\bR \in \real^{n \times n}$ (Theorem~\ref{theorem:qr-decomposition}).
A decomposition $\footnotesize\begin{bmatrix}
	\bJ \\
	\sqrt{\lambda} \bI
\end{bmatrix}$ of can be obtained by 
$$
\begin{bmatrix}
	\bJ \\
	\sqrt{\lambda} \bI
\end{bmatrix}
=
\begin{bmatrix}
	\bQ\bR \\
	\sqrt{\lambda} \bI
\end{bmatrix}
=
\begin{bmatrix}
	\bQ & \bzero \\
	\bzero & \bI
\end{bmatrix}
\begin{bmatrix}
	\bR \\
	\sqrt{\lambda} \bI
\end{bmatrix}.
$$
The matrix 
$
\footnotesize 
\begin{bmatrix}
	\bR \\
	\sqrt{\lambda} \bI
\end{bmatrix}
$
contains many zero elements. Using this feature, we can employ Householder transformations or Givens rotations to complete the QR decomposition of this matrix; see \citet{lu2021numerical} for more details. 
Thus, the linear least squares problem in \eqref{equation:lm_ls} can be solved using this QR decomposition \citep{lu2021numerical}.
The convergence of the trust region method can also be directly inferred from the convergence properties of the trust region method (Theorem~\ref{theorem:global_conv_trs_eta0} and Theorem~\ref{theorem:glo_conv_trs_othe}); therefore, we will not repeat these details here.

\index{QR decomposition}

\subsection{Powell's Dog Leg Method}\label{section:powell_dog_leg1}
An effective algorithm for addressing the trust region subproblem in nonlinear least squares problems is the Powell's Dog Leg method. This method, proposed by Powell, provides an approach to approximate the solution to the trust region subproblem $\bd_{\text{tr}}^\toptzero$, defined by \eqref{equation:trs_subpro}. The name ``Dog Leg" originates from golf, drawing a parallel between the step construction in the algorithm and the shape of a dogleg hole in golf \citep{powell1970new}.
Similar to the Levenberg-Marquardt Gauss-Newton method, this technique employs a combination of the Gauss-Newton and steepest descent directions but controls them explicitly through the radius of a trust region (refer to Section~\ref{section:des_trust_reg}):
$$
\bd_{\text{tr}}^\toptzero = \argmin_{\bd} \big\{ \psi_t(\bd)  \; \quad \text{s.t.} \quad \normtwo{\bd} \leq \Delta_t\big\}.
$$
where $\psi_t(\bd)\triangleq\tfrac{1}{2} \normtwobig{\br^\toptzero}^2 + \bd^\top \bJ^\toptzeroTOP \br^\toptzero + \frac{1}{2} \bd^\top \bJ^\toptzeroTOP \bJ^\toptzero \bd$ is an approximation to $f(\bx^\toptzero + \bd)$.

Given $\br : \real^n \rightarrow \real^m$ and the iteration point $\bx^\toptzero$, the Gauss-Newton step $\bd_{\text{gn}}^\toptzero$ is a least squares solution to the linear system
$
\bJ^\toptzero \bd +\br^\toptzero =\bzero
$
(see \eqref{equation:gaus_newton_ls}).
Meanwhile, the steepest descent direction is defined as the negative gradient:
$$
\bd_{\text{sd}}^\toptzero = -\nabla f(\bx^\toptzero) = -\bJ^\toptzeroTOP \br^\toptzero.
$$
However, the steepest descent direction is not properly scaled.
To scale it accordingly using the information from the Jacobian matrix, we consider the affine model
$$
\begin{aligned}
\br(\bx^\toptzero + \gamma \bd_{\text{sd}}^\toptzero) 
&\approx \br^\toptzero + \gamma \bJ^\toptzero \bd_{\text{sd}}^\toptzero\\
&\Downarrow\\
f(\bx^\toptzero + \gamma \bd_{\text{sd}}^\toptzero) 
&\approx \frac{1}{2} \normtwo{ \br^\toptzero + \gamma \bJ^\toptzero \bd_{\text{sd}}^\toptzero}^2\\
&= f(\bx^\toptzero) + \gamma  \br^\toptzeroTOP \bJ^\toptzero\bd_{\text{sd}}^\toptzero + \frac{1}{2} \gamma^2 \normtwo{\bJ^\toptzero \bd_{\text{sd}}^\toptzero}^2 .
\end{aligned}
$$
A closed-form scaling factor $\gamma_t$ can improve the solution at each iteration:
\begin{equation}
\gamma_t = - \frac{\br^\toptzeroTOP \bJ^\toptzero\bd_{\text{sd}}^\toptzero}{\normtwo{\bJ^\toptzero \bd_{\text{sd}}^\toptzero}^2} 
= \frac{\normtwobig{\bd_{\text{sd}}^\toptzero}^2}{\normtwo{\bJ^\toptzero \bd_{\text{sd}}^\toptzero}^2} .
\end{equation}

This yields two potential steps from the current point $\bx^\toptzero$: $\gamma_t \bd_{\text{sd}}^\toptzero$ and $\bd_{\text{gn}}^\toptzero$.
Given the trust region has radius $\Delta_t$, Powell suggested to use the following strategy for choosing the step; see Figure~\ref{fig:dog_leg} for an illustration for the third case:
\noindent
\begin{figure}[H]
\centering
\begin{minipage}{0.415\textwidth}
\begin{figure}[H]
\centering
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\includegraphics[width=0.9\textwidth]{./imgs/powell.pdf}
\caption{Dog Leg step, the third case.}
\label{fig:dog_leg}
\end{figure}
\end{minipage}\hfill
\framebox{
\begin{minipage}{0.55\textwidth}
\begin{equation}\label{equation:dog_leg_step}
\begin{aligned}
&\text{if } \normtwobig{\bd_{\text{gn}}^\toptzero} \leq \Delta_t \\
&\qquad \bd_{\text{dl}} ^\toptzero\leftarrow \bd_{\text{gn}}^\toptzero; \\
&\text{elseif } \normtwobig{\gamma_t \bd_{\text{sd}}^\toptzero} \geq \Delta_t \\
&\qquad \bd_{\text{dl}}^\toptzero \leftarrow (\Delta_t / \normtwobig{\bd_{\text{sd}}^\toptzero}) \bd_{\text{sd}}^\toptzero ;\\
&\text{else} \\
&\qquad \bd_{\text{dl}}^\toptzero \leftarrow \gamma_t \bd_{\text{sd}}^\toptzero + \beta (\bd_{\text{gn}}^\toptzero - \gamma_t \bd_{\text{sd}}^\toptzero) \\
&\qquad \text{with } \beta \text{ chosen so that } \normtwobig{\bd_{\text{dl}}^\toptzero} = \Delta_t .
\end{aligned}
\end{equation}
\end{minipage}
}
\end{figure}

Letting
$\bu \triangleq \gamma_t \bd_{\text{sd}}^\toptzero$, 
$\bv \triangleq \bd_{\text{gn}}^\toptzero$ (such that $\normtwo{\bu}<\Delta_t$ and $\normtwo{\bv}>\Delta_t$), and defining $w \triangleq \bu^\top (\bv - \bu)$, we can express the third case as
$$
h(\beta) \triangleq \normtwo{\bu + \beta (\bv - \bu)}^2 - \Delta_t^2 = \normtwo{\bv - \bu}^2 \beta^2 + 2 w \beta + \normtwo{\bu}^2 - \Delta_t^2 .
$$
We seek a root for this second degree polynomial.
Since $h(0) = \normtwo{\bu}^2 - \Delta_t^2 < 0$ and $h(1) = \normtwo{\bv}^2 - \Delta_t^2 > 0$, $h$ has a root in $[0, 1]$; this is also observed in Figure~\ref{fig:dog_leg}. 
The choice of $\beta$ is then the root $\Big( -w \pm \sqrt{w^2 + \normtwo{\bv - \bu}^2 (\Delta_t^2 - \normtwo{\bu}^2)} \Big) / \normtwo{\bv - \bu}^2$
that lies in $[0,1]$.
As in the LM Gauss-Newton or damped Newton methods, we can use the gain factor
$$
\nu_t = \frac{f(\bx^\toptzero) - f(\bx^\toptzero + \bd_{\text{dl}}^\toptzero)}{\psi_t(\bzero) - \psi_t(\bd_{\text{dl}}^\toptzero)}
$$
to monitor the iteration. 

In the LM Gauss-Newton method (similar to Algorithm~\ref{algorithm:leve_dam_new}), we used the gain factor $\nu_t$ to control the size of the damping parameter. Similar to the decent method with trust region (Algorithm~\ref{alg:trust_region1}), here, we use it to control the radius $\Delta_t$ of the trust region. A large value of $\nu_t$ indicates that the linear model is good. We can increase $\Delta_t$ and thereby take longer steps, and they will be closer to the Gauss-Newton direction. 
Conversely, if $\nu_t$ is small or even negative, it indicates that the model may not be reliable, prompting us to decrease $\Delta_t$, leading to smaller steps that are closer to the steepest descent direction. 
The procedure is similar to the descent method with trust region in Algorithm~\ref{alg:trust_region1}, except we use the Dog Leg step in \eqref{equation:dog_leg_step} rather than the solution of trust region subproblem in \eqref{equation:trs_subpro}.







\subsection{A Secant Version of the LM Method}
We introduced the LM  method in Section~\ref{section:nonlinearls_lmmethod} for solving nonlinear least squares problems.
The methods discussed  assume that the vector function $\br$ is differentiable, i.e., the Jacobian
$
\bJ(\bx) = \left\{\frac{\partial r_i}{\partial x_j}\right\}_{ij}
$
exists. 
However, in many practical optimization scenarios, we may not have explicit formulas for the elements of $\bJ$, such as when $\br$ is treated as a ``black box''. 
For these cases, the secant version of the LM method is particularly useful.

The simplest remedy is to replace $\bJ^\toptzero \triangleq \bJ(\bx^\toptzero)$ by a matrix $\bG^\toptzero$ obtained through  numerical differentiation: The $(i,j)$-th element can be approximated  using finite differences:
\begin{equation}\label{equation:lmseca_fini_diffG}
	\frac{\partial r_i}{\partial x_j}(\bx^\toptzero) \approx g_{ij}^\toptzero \triangleq \frac{r_i(\bx^\toptzero + \delta \be_j) - r_i(\bx^\toptzero)}{\delta} ,
\end{equation}
where $\be_j$ is the unit vector in the $j$-th coordinate direction and $\delta$ is an appropriately small real number.  This strategy requires  $n+1$ evaluations of $\br$ for each iteration $\bx^\toptzero$. 
Since $\delta$ is probably much smaller than the distance $\normtwo{\bx^\toptzero - \bx^*}$, this approach provides limited additional information about the global behavior of  $\br$ than we would get from just evaluating $\br(\bx^\toptzero)$. To improve efficiency, alternative strategies are needed.




Now, consider the linear model from the Gauss-Newton method for $ \br : \real^n \rightarrow \real^m $,
$
\br(\bx^\toptzero +\bd)\approx \ell_t(\bd) \triangleq \br^\toptzero + \bJ^\toptzero \bd
$, where $\br^\toptzero\triangleq \br(\bx^\toptzero)$.
We will replace it by
$$
\br(\bx^\toptzero + \bd) \approx h_t(\bd) \triangleq \br^\toptzero + \bG^\toptzero \bd  , 
$$
where $ \bG^\toptzero $ approximates  $ \bJ^\toptzero $. 
Letting $\bd^\toptzero =\argmin_{\bd} h_t(\bd)$, 
we need $ \bG^\toptone $ for the next iteration so that
$$ 
\br(\bx^\toptone + \bd) \approx  h_{t+1}(\bd) \triangleq \br^\toptone + \bG^\toptone \bd , 
\quad \text{with }\bx^\toptone \triangleq\bx^\toptzero + \bd^\toptzero.
$$
Especially, we want this model $h_{t+1}(\bd)$ to hold with equality for $ \bd = - \bd^\toptzero = \bx^\toptzero - \bx^\toptone $, establishing the equality
\begin{equation}
	\br^\toptzero = \br^\toptone + \bG^\toptone (\bx^\toptzero - \bx^\toptone)  .
\end{equation}
This equation  gives us $ m $ equations in the $ mn $ unknown elements of $ \bG^\toptone $, necessitating additional conditions.
Similar to the symmetric rank-one update quasi-Newton method (Section~\ref{section:quasi_new_update}), \citet{broyden1965class} suggested supplementing the above equality with
\begin{equation}
	\bG^\toptone \bv = \bG^\toptzero \bv \quad \text{for all} \quad \bv \perp (\bx^\toptzero - \bx^\toptone)  .
\end{equation}
This yields \textit{Broyden's rank-one update} for the Jacobian approximation.
\begin{definition}[Broyden's Rank-One Update\index{Broyden's rank-one update}]\label{definition:broyd_roulm}
Let $\bG^\toptzero$ be the approximation of the Jacobian matrix $\bJ^\toptzero$, and let $\bd^\toptzero =\argmin_{\bd} h_t(\bd)$ such that $\bx^\toptone = \bx^\toptzero + \bd^\toptzero$.
Then the Broyden's rank-one update for the next Jacobian matrix is
$$
\bG^\toptone = \bG^\toptzero+ \ba \bd^\toptzeroTOP,
$$
where
$ \ba \triangleq \frac{1}{\bd^\toptzeroTOP \bd^\toptzero} \left( \br^\toptone - \br^\toptzero - \bG^\toptzero\bd^\toptzero \right) .$
\end{definition}

A brief sketch of the central part of the LM damped Newton method in Algorithm~\ref{algorithm:leve_dam_new} with this modification has the form
\begin{subequations}
\begin{align}
\bd_{\text{slm}}^\toptzero &\leftarrow \text{solve }  (\bG^\toptzeroTOP \bG^\toptzero + \mu_t \bI) \bd_{\text{slm}}^\toptzero = -\bG^\toptzeroTOP \br^\toptzero;\\
\bx^\toptone &\leftarrow \bx^\toptzero + \bd_{\text{slm}}^\toptzero;\\
\bG^\toptone &\leftarrow \bG^\toptzero+ \ba \bd^\toptzeroTOP \text{ by Definition~\ref{definition:broyd_roulm}};\\
\text{Update }& \mu_{t+1} \text{ and } \bx^\toptone \text{ as in Algorithm~\ref{algorithm:leve_dam_new}}.\
\end{align}
\end{subequations}

\citet{powell1970new} has shown that if the sequence of iterate $  \{\bx^{(t)}\}_{t>0} $ converges to $ \bx^* $ and if the set of steps $ \{ \bd^\toptzero \triangleq \bx^\toptone - \bx^\toptzero \}_{t>0} $ satisfy the condition that $ \{ \bd^{(t-n+1)}, \ldots, \bd^{(t-1)}, \bd^\toptzero \} $ are linearly independent (they span the whole of $ \real^n $) for each $ t \geq n $, then the set of approximations $ \{ \bG^\toptzero \} $ converges to $ \bJ(\bx^*) $, irrespective of the choice of $ \bG^{(1)} $.

In practice, however, it often happens that the previous $ n $ steps do not span the whole of $ \real^n $, and there is a risk that after some iterations, the current $ \bG^\toptzero $ is such a poor approximation to the true Jacobian, that $ \nabla f(\bx^\toptzero) \approx -\bG^\toptzeroTOP \br^\toptzero $ might not even be a descent direction. 
In such cases,  $ \bx^\toptzero $ remains unchanged,  and the damping parameter $ \mu_t $ is increased (see Algorithm~\ref{algorithm:leve_dam_new}). 
The approximation $ \bG^\toptzero $ is changed, but may still be a poor approximation, leading to a further increase in $ \mu_t $. Eventually the process is stopped by $ \bd_{\text{slm}}^\toptzero $ being so small that the stopping criteria is satisfied (e.g., the (ST3) in \eqref{equation:des_stopcri1}), although $ \bx^\toptzero $ may be far from $ \bx^* $ \citep{madsen2010and}.

Several strategies have been proposed to address this issue, including occasional recomputations of $ \bG^\toptzero $ using finite differences at certain iterations.
\citet{madsen2010and} suggested employ the rank-one update determined by Definition~\ref{definition:broyd_roulm} with a cyclic, coordinate-wise series of updatings. 
Specifically, let $ \bd^\toptzero $ denote the current step, and let $ j $ be the current coordinate number. If the pseudo angle $ \theta_t $ between $ \bd^\toptzero $ and $ \be_j $ is ``large", then we compute a
finite difference approximation to the $j$-th column of $\bJ^\toptzero$. More precisely, this occurs when
\begin{equation}
\cos (\theta_t) = \frac{\abs{\be_j^\top \bd^\toptzero}}{\normtwo{\bd^\toptzero} \cdot \normtwo{\be_j}} < \zeta 
\qquad \iff \qquad 
\abs{\bd^\toptzero_j} < \zeta \normtwo{\bd^\toptzero} .
\end{equation}
Experiments indicate that choosing $\zeta=0.8$ yields good performance.


\subsection{A Secant Version of the Dog Leg Method}

The idea of using a secant approximation to the Jacobian can, of course, also be used in connection with the Dog Leg method from Section~\ref{section:powell_dog_leg1}. 
In this section, we will focus on the special case where  $m = n$ (the Newton-Raphson scenario in \eqref{equation:newton_raphson_nonlin}).
Similar to the relationship between BFGS  and DFP formulas in  quasi-Newton methods (Section~\ref{section:quasi_new_update}),
\citet{broyden1965class} not only provided the formula from Definition~\ref{definition:broyd_roulm} for updating the approximate Jacobian but also introduced a formula for updating an approximate inverse of the Jacobian:
$$
\bP(\bx) \approx \bJ(\bx)^{-1}.
$$
For iteration point $\bx^\toptzero$, the two formulas are
\begin{subequations}\label{equation:secan_dogleg}
\begin{align}
\bG^\toptone &= \bG^\toptzero + \frac{1}{\bd^\toptzeroTOP \bd^\toptzero} (\ba - \bG^\toptzero \bd^\toptzero) \bd^\toptzeroTOP , \\
\bP^\toptone &= \bP^\toptzero + \frac{1}{\bb^\top \ba} (\bd^\toptzero - \bP^\toptzero \ba) \bb^\top ,
\end{align}
\end{subequations}
where
$
\bd^\toptzero = \bx^\toptone - \bx^\toptzero =  \argmin_{\bd} h_t(\bd)$,  $\ba \triangleq \br^\toptone - \br^\toptzero$ ,  $ \bb \triangleq \bP^\toptzero \bd^\toptzero
$, and $\bP^\toptzero \triangleq\bP(\bx^\toptzero)$.
It is easy to verify that then the updates $\bG^\toptone$ and $\bP^\toptone$ satisfy $\bG^\toptone \bP^\toptone = \bI$.

Using these matrices, the steepest descent direction $\bd^\toptzero_{\text{sd}}$ and the Gauss-Newton step $\bd^\toptzero_{\text{gn}}$ (see Section~\ref{section:powell_dog_leg1}) are approximated by
\begin{equation}
\bd^\toptzero_{\text{ssd}} = -\bG^\toptzeroTOP \br^\toptzero 
\qquad \text{and} \qquad 
\bd^\toptzero_{\text{sgn}} = -\bP^\toptzero \br^\toptzero.
\end{equation}
The updates for Powell's Dog Let method in \eqref{equation:dog_leg_step}  can be  easily adapted  to use these approximations. The initial $\bG = \bG^{(1)}$ can be obtained using the difference approximation \eqref{equation:lmseca_fini_diffG}, while  $\bP^{(1)}$ can be computed as $(\bG^{(1)})^{-1}$. 









\subsection{Quasi-Newton Methods for Large Residual Problems}\label{section:quasi_largeresi}
In the previous sections, we introduced the Gauss-Newton method, the LM method, and the trust region method (along with some of their variants).
Essentially, for the iteration point $\bx^\toptzero$, these methods approximate the Hessian matrix $\nabla^2 f(\bx^\toptzero)$ by $\bJ^\toptzeroTOP\bJ^\toptzero$ and ignore the the residual term $\sum_{i=1}^{m} r_i(\bx^\toptzero) \nabla^2 r_i(\bx^\toptzero)$ (see \eqref{equation:gaus_new_hess}). 
Alternatively stated, they use the affine model in \eqref{equation:gauss_new_affinemodel}.
These approximations are highly effective for small residual least squares problems. However, in large residual problems, the second part of the Hessian matrix $\nabla^2 f(\bx^\toptzero)$ cannot be neglected. 
Simply considering $\bJ^\toptzeroTOP \bJ^\toptzero$ as the $t$-th step Hessian matrix approximation can lead to significant errors. In such cases, the aforementioned methods may fail.

Nevertheless, we can treat the nonlinear least squares problem \eqref{equation:nonlinear_ols} as an unconstrained problem and solve it using the previously discussed Newton's method and quasi-Newton methods (\S~\ref{section:new_methods}, \S~\ref{section:quasi_newton_method}). 
However, for many problems, calculating the Hessian matrices $\nabla^2 r_i(\bx^\toptzero)$ for each residual component is challenging, making  Newton's method very expensive. Directly applying the quasi-Newton method to approximate the Hessian matrix $\nabla^2 f(\bx^\toptzero)$ seems to overlook the special structure of the nonlinear least squares problem.

Given this, we introduce a hybrid approach: continue using the aforementioned methods for calculating the first term in the Hessian and employ a quasi-Hessian method for estimating the residual term \citep{liu2020optimization}.

The expression for the Hessian matrix \eqref{equation:gaus_new_hess} shows that  $\nabla^2 f(\bx^\toptzero)$ consists of two parts: one is easy to obtain but not enough to approximate the Hessian well; the other is more difficult to obtain but necessary for accurate calculations. 
For the easy part, we can directly retain the Gauss-Newton matrix $\bJ^\toptzeroTOP \bJ^\toptzero$, and for the more challenging part, we apply the quasi-Newton method for approximation:
$$
\nabla^2 f(\bx^\toptzero) = 
\underbrace{\bJ(\bx^\toptzero)^\top \bJ(\bx^\toptzero)}_{\text{Gauss-Newton}}
 +
\underbrace{\sum_{i=1}^{m} r_i(\bx^\toptzero) \nabla^2 r_i(\bx^\toptzero)}_{\text{quasi-Newton}}. 
$$
This forms the basis of our strategy for solving large residual problems, balancing the structure of the Hessian matrix and computational cost through a hybrid approximation method.

Specifically, we denote $\bH^\toptzero$ as the approximate Hessian matrix $\nabla^2 f(\bx^\toptzero)$, i.e.,
$ \bH^\toptzero = \bJ^\toptzeroTOP \bJ^\toptzero + \bR^\toptzero, $
where $\bR^\toptzero$ is the approximation of the second part of the Hessian matrix $\sum_{j=1}^{m} r_j(\bx^\toptzero) \nabla^2 r_j(\bx^\toptzero)$.
The key challenge lies in constructing the matrix $\bR^\toptzero$. Recall that when constructing the quasi-Newton method, the process mainly involves two steps: finding the quasi-Newton conditions and updating the low-rank matrix based on these conditions. Here, we follow a similar process but note that $\bR^\toptzero$ is only a part of the quasi-Newton matrix $\bH^\toptzero$ and may not satisfy the secant conditions \eqref{equation:secant1} (replacing $\bH^\toptone$ with $\bR^\toptone$ due to the symmetry of $\{\bR^\toptzero\}$). Our goal is to make $\bR^\toptone$ as close as possible to the second part of the Hessian matrix, i.e.,
$ \bR^\toptone \approx \sum_{j=1}^{m} r_j(\bx^\toptone) \nabla^2 r_j(\bx^\toptone). $

%From a first-order Taylor expansion, it is known that $ \bR^\toptone $ should retain as much as possible the properties of the original Hessian matrix. 
According to linear approximation theorem (Theorem~\ref{theorem:linear_approx}), the gradient function $\nabla r_i(\bx)$ for each $i\in\{1,2,\ldots,m\}$ can be approximated near $\bx^\toptone$ as:
$$
\nabla r_i(\bx) = \nabla r_i(\bx^\toptone) + \nabla^2 r_i(\bx^\toptone)(\bx - \bx^\toptone) + \mathcalO\big(\normtwobig{\bx - \bx^\toptone}^2\big).
$$
Let $\bx \triangleq \bx^\toptzero$, $\bh^\toptzero \triangleq \bx^\toptone - \bx^\toptzero$, and $\by_i^\toptzero \triangleq \nabla r_i(\bx^\toptone) - \nabla r_i(\bx^\toptzero)$.
Then,
$$
\nabla^2 r_i(\bx^\toptone) \bh^\toptzero + \mathcalO(\normtwobig{\bh^\toptzero}^2) = \by_i^\toptzero.
$$
This establishes the secant equation for the second term of $\nabla^2 f(\bx^\toptone)$:
$$
\small
\begin{aligned}
	\bR^\toptone \bh^\toptzero
	&= \bigg(\sum_{j=1}^{m} r_j(\bx^\toptone) \nabla^2 r_j(\bx^\toptone) \bigg) \bh^\toptzero
	= \sum_{j=1}^{m} r_j(\bx^\toptone) \left( \nabla^2 r_j(\bx^\toptone) \right) \bh^\toptzero \\
	&\approx \sum_{j=1}^{m} r_j(\bx^\toptone) \left( \nabla r_j(\bx^\toptone) - \nabla r_j(\bx^\toptzero) \right)
	= (\bJ^\toptone)^\top \br^\toptone - (\bJ^\toptzero)^\top  \br^\toptone
	\triangleq\by^\toptzero.
\end{aligned}
$$
Since $\bR^\toptone$ is also symmetric, the matrix updating methods in Section~\ref{section:quasi_new_update} can be applied using the above secant equation; and we shall not repeat the details.


\index{Linear programming}
\section{Nonlinear Least Squares Problem with Other Norms}
In the previous section, we discussed methods for nonlinear least squares, where the objective function is the $\ell_2$ norm of a vector of nonlinear functions; see \eqref{equation:nonlinear_ols}.
In this section, we explore methods for parameter estimation using other norms. Specifically, given a vector function $\br : \real^n \rightarrow \real^m$, our goal is to find $\bx^*$ that minimizes certain measures of $\br(\bx)$, such as $\normone{\br(\bx)}$ or $\norminf{\br(\bx)}$. 


The function $\br$ may depend nonlinearly on $\bx$. Similar to the Gauss-Newton method with affine approximation in \eqref{equation:gauss_new_affinemodel0}, provided that $\br$ is twice continuously differentiable, we approximate $\br$ at iteration $\bx^\toptzero$ by 
\begin{equation}\label{equation:nonlinear_affapprox_eq1}
\br(\bx^\toptzero + \bd) \approx {\ell_t}(\bd) \triangleq \br(\bx^\toptzero) + \bJ(\bx^\toptzero) \bd ,
\end{equation}
where $\nabla r_i \in \real^n$ is the gradient of $r_i$ and $\bJ(\bx)=[\nabla r_1, \nabla r_2, \ldots, \nabla r_m]^\top \in \real^{m \times n}$ is the Jacobian matriux of $\br$. 
We then provide details of the trust region method (Algorithm~\ref{alg:trs_diffnorms}; see also Section~\ref{section:des_trust_reg}) for fitting $\ell_{\infty}$ and $\ell_1$ norms \citep{hald1981combined, hald1985combined}.
At each iteration $t$, the the trust region subprblem is 
$$
\bd_{\text{tr}}^{\toptzero} = \argmin_{\norminf{\bd} \leq \Delta_t} \left\{\psi_t(\bd) \triangleq \normbig{ \br^\toptzero + \bJ^\toptzero \bd}\right\},
$$
where $\norm{\cdot}$ denotes some norm,  $\br^\toptzero\triangleq\br(\bx^\toptzero)$, and  $\bJ^\toptzero \triangleq\bJ(\bx^\toptzero)$.
The functions $ f $ and $ \psi_t $ are defined by $ f(\bx) = \norm{\br(\bx)}_p $ and $ \psi_t(\bd) = \norm{\ell_t(\bd)}_p $, respectively for different norms. 
Note that the trust region is bounded using the $\ell_\infty$ norm: $\norminf{\bd} \leq \Delta_t$, thus transforming the problems into linear programming problems; see the following sections.

\begin{algorithm}[h] 
\caption{Trust Region Method for Fitting Different Norms}
\label{alg:trs_diffnorms}
\begin{algorithmic}[1] 
\Require A function $f(\bx) = \norminf{\br(\bx)}$ or  $f(\bx) = \normone{\br(\bx)}$; 
\State {\bfseries Input:}  Set the maximum radius $ \Delta_{\max} $, initial radius $ \Delta_1 $, initial point $ \bx^{(1)} $, accept radius $\gamma\in[0,\frac{1}{4})$;
\For{$t=1,2,\ldots$}
\State $\bd_{\text{tr}}^{\toptzero} \leftarrow \argmin_{\norminf{\bd} \leq \Delta} \psi_t(\bd)$; \Comment{see Sections~\ref{section:nonlinear_infnorm} and \ref{section:nonlinear_lonenorm}}
\State $\nu_t \leftarrow \big(f(\bx^\toptzero) - f(\bx^\toptzero + \bd_{\text{tr}}^{\toptzero})\big) / \big(\psi_t(\bzero) - \psi_t(\bd_{\text{tr}}^{\toptzero})\big) $; \Comment{gain factor}
\If{$\nu_t > 0.75$ and $\normtwobig{\bd^\toptzero} =\Delta_t$} \Comment{very good step, and the step is at the border}
\State $\Delta_{t+1} \leftarrow \min\{2 \Delta_t, \Delta_{\max}\}$; \Comment{larger trust region}
\EndIf
\If{$\nu_t < 0.25$} \Comment{poor step}
\State $\Delta_{t+1} \leftarrow \Delta_t / 3$; \Comment{smaller trust region}
\EndIf
\If{$\nu_t > \gamma$} \Comment{reject step if $\nu_t \leq \gamma$}
\State $\bx^\toptone \leftarrow \bx^\toptzero + \bd_{\text{tr}}^{\toptzero}$;
\EndIf
\EndFor
\State {\bfseries Return:}  $\bx\leftarrow \bx^{(t)}$;
\end{algorithmic} 
\end{algorithm}






\subsection{Fitting in the $ \ell_\infty $ norm}\label{section:nonlinear_infnorm}
When the underlying norm is the $\ell_\infty$ norm, the trust region subproblem can be solved equivalently by a linear programming (LP) problem (Example~\ref{example:linear_program}; see, for example, \citet{boyd2004convex, bertsekas2015convex} for some algorithms of this problem).  
Omitting the superscripts,
we seek a minimizer $ \widetildebx $ of the function
$ f(\bx) = \norminf{\br(\bx)} = \max_i \abs{r_i(\bx)}   $.
The linearized problem
$$ 
\widetildebx = \argmin_{\norminf{\bd} \leq \Delta} \left\{ \psi(\bd) \triangleq \norminf{\br(\bx) + \bJ(\bx) \bd} \right\} 
$$
is solved by a linear programming  algorithm. 
To formulate the LP problem, we introduce the variable $ d_{n+1} = \psi(\bd) $ and the extended vector
$ \widetildebd = \small\begin{bmatrix} \bd \\ d_{n+1} \end{bmatrix} \normalsize\in \real^{n+1}  $. Using this auxiliary variable, $ \widetildebx $ can be found by solving the following linear programming problem
$$
\begin{aligned}
	& \min & & d_{n+1} \\
	& \text{s.t.} & & -\Delta \leq d_i \leq \Delta &i = 1,2 \ldots, m; \\
	& & & -d_{n+1} \leq r_i(\bx) + \nabla r_i(\bx)^\top \bd \leq d_{n+1}, \quad &i = 1,2 \ldots, m. \\
\end{aligned}
$$


\subsection{Fitting in the $\ell_1 $ norm}\label{section:nonlinear_lonenorm}
Similarly, when the underlying norm is the $\ell_1$ norm, the trust region subproblem can also be equivalently stated as an LP problem.
Again, omitting the superscripts, we seek a minimizer $ \widetildebx $ of the function
$ f(\bx) = \normone{\br(\bx)} = \sum_{i=1}^m  \abs{r_i(\bx)}  $. 
For the linearized problem
$$ 
\widetildebx = \argmin_{\norminf{\bd} \leq \Delta} \left\{ \psi(\bd) \triangleq \normone{\br(\bx) + \bJ(\bx) \bd} \right\},
$$
we introduce auxiliary variables $ d_{n+i} $, $ i=1,2\ldots,m $, and the extended vector
$ \widetildebd = \scriptsize\begin{bmatrix} \bd \\ d_{n+1} \\ \vdots \\ d_{n+m} \end{bmatrix}  \normalsize\in \real^{n+m}  . $
Using these auxiliary variables, $ \widetildebx $ can be found by solving the problem
$$
\begin{aligned}
	& \min & & \sum_{i=1}^m d_{n+i} \\
	& \text{s.t.} & & -\Delta \leq d_i \leq \Delta        &i = 1,2 \ldots, m; \\
	& & & -d_{n+i} \leq r_i(\bx) + \nabla r_i(\bx)^\top \bd \leq d_{n+i}, \quad &i = 1,2 \ldots, m. \\
\end{aligned}
$$



\subsection{Huber Estimation}\label{section:huber_estima}
As mentioned in Chapter~\ref{chapter_introduction}, estimation using $\ell_1$ norms can produce more robust estimates, particularly in the presence of outliers  \citep{zoubir2012robust}.
\textit{Huber estimation}, on the other hand, combines the smoothness of the least squares estimator with the robustness of the $ \ell_1 $-estimator.
For a function $ \br : \real^n \rightarrow \real^m $, the Huber estimator $ \bx_\delta $ is defined by
\begin{equation}\label{equation:huberloss_eq1}
	\bx_\delta = \argmin_{\bx} \left\{ f_\delta(\bx) \triangleq \sum_{i=1}^m h_\delta(r_i(\bx)) \right\} ,
\end{equation}
where $ h_\delta $ is the \textit{Huber loss function},
\begin{equation}
	h_\delta(x) = 
	\begin{cases} 
		\frac{1}{2\delta}x^2, & \abs{x} < \delta, \\
		\abs{x} - \frac{\delta}{2}, & \text{otherwise}.
	\end{cases}
\end{equation}
When $\delta \to 0$, the smooth function $h_\delta(x)$ approaches the absolute value function $\abs{x}$. Figure~\ref{fig:huber} shows the graph of $h_\delta(x)$ for different values of $\delta$.

\begin{SCfigure}%[h]
	\centering
	\includegraphics[width=0.5\textwidth]{./imgs/huber.pdf} 
	\caption{Illustration of Huber loss function $h_\delta(x)$ for different values of $\delta$.}
	\label{fig:huber}
\end{SCfigure}





The threshold $ \delta $ is used to distinguish between ``small'' and ``large'' function values (residuals). Based on the values of the $ r_i(\bx) $ we define a generalized sign vector $ \bs \triangleq \bs_\delta(\bx) $ and an ``activity matrix'' $ \bW \triangleq \bW_\delta(\bx) = \text{diag}(w_1, w_2 \ldots, w_m) $. Note that $ w_i = 1 - s_i^2 $ and
\begin{table}[H]
	\centering
	\caption{Parameters using Huber loss functions.}
	\label{table:huber_sw}
	\begin{tabular}{|c|ccc|}
		\hline
		& $ r_i(\bx) < -\delta $ & $ \abs{r_i(\bx)} \leq \delta $ & $ r_i(\bx) > \delta $ \\
		\hline
		$ s_i(x) $ & $-1$ & $0$ & $1$ \\\hline
		$ w_i(x) $ & $0$ & $1$ & $0$ \\
		\hline
	\end{tabular}
\end{table}

Using these definitions, the Huber objective function in \eqref{equation:huberloss_eq1} can be expressed as
\begin{equation}\label{equation:huber_objfunc}
	f_\delta(\bx) = \frac{1}{2\delta} \br^\top \bW \br + \br^\top \bs - \frac{1}{2} \delta \bs^\top \bs ,
\end{equation}
where we have omitted the argument $ \bx $ and the Huber parameter $ \delta $. The gradient is
\begin{equation}
	\nabla f_\delta (\bx) = \frac{1}{\delta} \bJ^\top (\bW \br + \delta \bs) .
\end{equation}
By Theorem~\ref{theorem:fermat_fist_opt}, a necessary condition for $ \bx_\delta $ being a minimizer of $ f_\delta $ is that it satisfies the equation
\begin{equation}
\nabla f_\delta (\bx) =\bzero  
\quad\implies\quad
\bJ(\bx)^\top \big(\bW_\delta(\bx) \br(\bx) + \delta \bs_\delta(\bx)\big) = \bzero.
\end{equation}
Before delving into the nonlinear Huber estimation, we first describe the linear Huber estimation.
\paragrapharrow{Linear Huber estimation.}
Consider the linear function
\begin{equation}
\br(\bx) = \bb - \bA \bx ,
\end{equation}
where $ \bA \in \real^{m \times n} $ and $ \bb \in \real^m $ and are known variables. 
The gradient is
\begin{equation}
	\nabla f_\delta(\bx) = -\frac{1}{\delta} \bA^\top \big(\bW(\bx) (\bb - \bA \bx) + \delta \bs(\bx)\big) .
\end{equation}
While the Hessian is positive semidefinite:
\begin{equation}
 \bH(\bx)  \triangleq \nabla^2 f_\delta(\bx) = \frac{1}{\delta} \bA^\top \bW(\bx) \bA.
\end{equation}
By Theorem~\ref{theorem:second_nec_nonstrict_loca}, the positive semidefiniteness implies that if we find a stationary point $ \bx^* $ satisfying $ \nabla f_\delta(\bx^*) = \bzero $, then it is also a minimizer for $ f_\delta $. 
We use Newton's method with line search to find a stationary point.
The typical step at $t$-th iteration is
\begin{subequations}
\begin{align}
	\bd_{\text{lh}}^\toptzero&\leftarrow \text{solution of } \bH(\bx^\toptzero) \bd = -\nabla f_\delta(\bx^\toptzero);  \\
	\eta_t &\leftarrow \argmin_{\eta} f_\delta(\bx^\toptzero + \eta \bd_{\text{lh}}^\toptzero);\\
	\bx^\toptone &\leftarrow \bx^\toptzero + \eta_t \bd_{\text{lh}}^\toptzero.
\end{align}
\end{subequations}
The line search is very simple: $ g(\eta) \triangleq f_\delta(\bx^\toptzero + \eta \bd_{\text{lh}}^\toptzero) $ is a piecewise quadratic in the scalar variable $ \eta $, so $ \eta_t $ is a root for $ g' $, which is piecewise linear.



\paragrapharrow{Nonlinear Huber estimation.}
When the  function $\br$  depends nonlinearly on $\bx$, we must use approximation methods to find the minimizer $\bx^*$ of $f_\delta$. 
Following the affine approximation in \eqref{equation:nonlinear_affapprox_eq1}, at the point $\bx$ we use the affine approximation $\br(\bx+\bd) \approx \bl \triangleq \br(\bx) + \bJ(\bx) \bd$ and the corresponding approximation to the objective function from \eqref{equation:huber_objfunc}
$$
f_\delta(\bx + \bd) \approx \psi(\bd)
= \frac{1}{2\delta} \bl^\top \bW \bl + \bl^\top \bs - \frac{1}{2} \delta \bs^\top \bs ,
$$
where $\bs$ and $\bW$ are given by Table~\ref{table:huber_sw} with $\br(\bx)$ replaced by $\bl$. Therefore, the typical step for the descent step at $t$-th iteration is 
$$
\bd_{\text{nh}}^\toptzero \leftarrow \argmin_{\bd} \left\{f_\delta(\bx^\toptzero + \bd) \approx \psi_t(\bd)\right\},
$$
where $\psi_t(\bd)$ represents the corresponding approximation at $\bx^\toptzero$.
For nonlinear Huber estimation, instead of using trust region methods, \citet{hald1981combined, madsen2010and} suggested using a Levenberg-Marquardt like algorithm (Algorithm~\ref{alg:nonlinear_huber_est}); see also Section~\ref{section:modified_damp_new} and Algorithm~\ref{algorithm:leve_dam_new} for reference.



\begin{algorithm}[h]
\caption{Nonlinear Huber Estimation Method}
\label{alg:nonlinear_huber_est}
\begin{algorithmic}[1]
\Require A twice continuously differentiable function $f(\bx)$; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}, \mu_1$ (by default $\mu_1=1$),  $ \sigma$ (by default $\sigma\triangleq 10^{-3}$), Huber parameter  $\delta$;
\For{$t=1,2,\ldots$}
\While{$\nabla^2 \psi_t(\bd) + \mu_t \bI$ is not positive definite}
\State $\mu_t \leftarrow 2\mu_t$
\EndWhile
\State $\bd_{\text{nh}}^\toptzero \leftarrow \argmin_{\bd} \left\{ \psi_t(\bd)\right\}$;
\State Gain factor $\nu_t \leftarrow \big(f_\delta(\bx^\toptzero) - f_\delta(\bx^\toptzero + \bd_{\text{nh}}^\toptzero)\big) / \big(\psi_t(\bzero) - \psi_t(\bd_{\text{nh}}^\toptzero)\big)$;
\If{$\nu_t > \sigma$} \Comment{$f$ decreases}
\State $\bx^\toptone \leftarrow \bx^\toptzero + \bd_{\text{dn}}^\toptzero$ \Comment{new iterate}
\State $\mu_{t+1} \leftarrow  \max\left\{\frac{1}{3}, 1 - (2\nu_t - 1)^3\right\} \cdot \mu_t$ 
\Else
\State $\bx^\toptone \leftarrow \bx^\toptzero $;  \Comment{old iterate}
\State $\mu_{t+1} \leftarrow 2\cdot \mu_t $;
\EndIf
\EndFor
\State {\bfseries Return:}  $\bx\leftarrow \bx^{(t)}$;
\end{algorithmic}
\end{algorithm}







\section{Separable Nonlinear Least Squares Problem}
In many data fitting problems, the model includes parameters that appear both linearly and nonlinearly. Such problems are referred to as \textit{separable}. This property can be leveraged in least squares estimation, and various specialized algorithms have been proposed, see e.g. \citet{ruhe1980algorithms, niel2000separable}. In this section, we discuss how to reformulate the problem to exploit this property using a standard program for nonlinear least squares.

\subsection{Full Parameter Set}
Let $\mathcalX = \left\{(\zeta_1, y_1), (\zeta_2, y_2), \ldots, (\zeta_m, y_m)\right\}$ be the given data points, and consider the model
\begin{equation}
y_i = G(\bx, \bmu, \zeta_i) \triangleq \mu_1 g_1(\bx, \zeta_i) + \ldots + \mu_p g_p(\bx, \zeta_i) , \quad i\in\{1,2,\ldots,m\},
\end{equation}
where  $g_i : \real^{n+1} \rightarrow \real$ are nonlinear functions that depend on $\bx \in \real^n$ and  $\bzeta$. The coefficients $\mu_1, \mu_2, \ldots, \mu_p$ are parameters that occur linearly in the model.
Collecting all the unknown parameters into a single vector  $\bz \triangleq \footnotesize\begin{bmatrix} \bx \\ \bmu \end{bmatrix} \in \real^{n+p}$, the least squares estimate is defined as
\begin{equation}
\bz^* = \argmin_{\bz \in \real^{n+p}} \left\{ f(\bz) \triangleq \tfrac{1}{2} \br(\bz)^\top \br(\bz) \right\} ,
\end{equation}
where
\begin{equation}\label{equation:sepe_rg}
\br(\bz) = \by - \bG(\bx) \bmu \in\real^m 
\qquad \text{and}\qquad
\left\{\bG(\bx)\right\}_{ij} = \{g_j(\bx, \zeta_i)\}_{ij} \in\real^{m\times p}  
\end{equation}
The least squares estimate satisfies
\begin{equation}
\nabla f(\bz^*) = \bzero , \quad \text{where} \quad \nabla f(\bz) \triangleq \bJ(\bz)^\top \br(\bz) ,
\end{equation}
with the Jacobian $\bJ(\bz) \triangleq\begin{bmatrix} \bJ_g(\bz) & -\bG(\bx) \end{bmatrix} \in \real^{m \times (n+p)}$, where $\left( \bJ \right)_{ij} = \dfrac{\partial r_i}{\partial z_j}$.
The last $p$ columns correspond to the linear parameters $\bmu$, while each element in the first $n$ columns of $\bJ$ are given by
\begin{equation}
\left( \bJ_g(\bx) \right)_{ij} = \frac{\partial r_i}{\partial x_j}(\bz) 
= - \sum_{k=1}^p \mu_k \frac{\partial g_k}{\partial x_j}(\bx, \zeta_i)  .
\end{equation}
We shall use the following notation for the $j$-th ($j\in\{1,2,\ldots,n\}$) column in $\bJ_g$,
\begin{equation}\label{equation:sep_jaco_nablag}
\left( \bJ_g(\bz) \right)_{:,j} 
= \left(\nabla\br(\bz)\right)_{:,j}
= - \bG'_j(\bx) \bmu 
\quad \text{with} \quad 
\left\{\bG'_j(\bx)\right\}_{ik} = \left\{\frac{\partial g_k}{\partial x_j}(\bx, \zeta_i)\right\} \in\real^{m\times p},
\end{equation}
where $\left(\nabla\br(\bz)\right)_{:,j} 
= 
\frac{\partial \br(\bz)}{\partial z_j}
= 
\frac{\partial \br(\bz)}{\partial x_j}
\in\real^m
$ for $j\in\{1,2,\ldots,n\}$.

\paragrapharrow{Gauss-Newton method.}
As mentioned previously, the \textit{Gauss-Newton method} is widely used for solving nonlinear least squares problems: Given an approximation $\bz^\toptzero$ to $\bz^*$ at the $t$-th iteration, the next approximation is found as $\bz^\toptzero + \bd_{\text{gn}}^\toptzero$, where
\begin{equation}
\bd_{\text{gn}}^\toptzero = \argmin_{\bd \in \real^{n+p}} \left\{ \psi_t(\bd) \triangleq \tfrac{1}{2} \normtwo{\bJ(\bz^\toptzero) \bd +\br(\bz^\toptzero)}^2 \right\} ,
\end{equation}
leading to the \textit{Gauss-Newton} equation
\begin{equation}\label{equation:sep_gaussnew}
\textbf{(Gauss-Newton)}:\qquad \left( \bJ(\bz^\toptzero)^\top \bJ(\bz^\toptzero) \right) \bd_{\text{gn}}^\toptzero = \bJ(\bz^\toptzero)^\top \br(\bz^\toptzero)  .
\end{equation}
This equation  is often modified by the \emph{Levenberg-Marquardt} strategy, where $\bd_{\text{gn}}^\toptzero$ is replaced by $\bd_{\text{lm}}^\toptzero$, determined by
\begin{equation}\label{equation:sep_levmarg}
\textbf{(LM)}:\qquad \left( \bJ(\bz^\toptzero)^\top \bJ(\bz^\toptzero) + \mu_t \bI \right) \bd_{\text{lm}}^\toptzero = \bJ(\bz^\toptzero)^\top \br(\bz^\toptzero)  .
\end{equation}
The \textit{damping parameter} $\mu_t$ is adjusted during iteration according to how well $\psi_t(\bd_{\text{lm}}^\toptzero)$ approximates $f(\bz^\toptzero + \bd_{\text{lm}}^\toptzero)$ using the gain factor; see \eqref{equation:trs_reduc_ratio}. 


\subsection{Reduced Parameter Set}

Instead of treating the elements in $\bmu$ as nonlinear variables, we replace \eqref{equation:sepe_rg} by
\begin{equation}\label{equation:sep_red_resid}
\br(\bz) = \by - \bG(\bx) \bmu 
\qquad\implies\qquad 
\br(\bx) = \by - \bG(\bx) \bmu(\bx) ,
\end{equation}
with $\bmu(\bx)\in\real^p$ computed as the least squares solution to $\bG(\bx) \bmu(\bx) = \by$. We can express it as the solution to the normal equation (Definition~\ref{definition:normal-equation-als}):
\begin{equation}
\bA(\bx) \bmu(\bx) = \bb(\bx) ,
\quad \text{where}\quad 
\bA(\bx) \triangleq \bG(\bx)^\top \bG(\bx) \in\real^{p\times p} , 
\quad \bb(\bx) \triangleq \bG(\bx)^\top \by  \in\real^p.
\end{equation}
With the notation from \eqref{equation:sep_jaco_nablag}, the corresponding $j$-th column of the Jacobian matrix $\bJ(\bx)=\nabla\br(\bx)\in\real^{m\times n}$ is given by
\begin{equation}\label{equation:sep_jaco_j}
\left( \bJ(\bx) \right)_{:,j} =  \left(\nabla\br(\bx)\right)_{:,j} %\br_j'(\bx) 
= - \bG_j'(\bx) \bmu(\bx) - \bG(\bx) \bmu_j'(\bx) 
\triangleq \left( \bJ_g(\bx) \right)_{:,j} + \left( \bH(\bx) \right)_{:,j} .
\end{equation}
From the above notation, for any $j \in\{1,2, \ldots, n\}$, we have
$$
\bA(\bx) \bmu(\bx) = \bb(\bx)
\qquad\implies\qquad 
\bA_j'(\bx) \bmu(\bx) + \bA(\bx) \bmu_j'(\bx) = \bb_j'(\bx) ,
$$
where $\bA_j'(\bx) \triangleq \frac{\partial \bA(\bx)}{\partial x_j}  = \bG_j'(\bx)^\top \bG(\bx) + \bG(\bx)^\top \bG_j'(\bx) \in\real^{p\times p}$, 
$\bmu_j'(\bx)\triangleq \frac{\partial \bmu(\bx)}{\partial x_j}\in\real^p$, and $\bb_j'(\bx) \triangleq\bG_j'(\bx)^\top \by \in\real^p$.
Omitting the argument $\bx$, we have 
\begin{equation}\label{equation:sep_redu_parAmuj}
\begin{aligned}
	\bA \bmu_j' &= \bb_j' - \bA_j' \bmu 
	= \bG_j'^\top \by - \left( \bG_j'^\top \bG + \bG^\top \bG_j' \right) \bmu  \\
	&= \bG_j'^\top \br - \bG^\top \bG_j' \bmu  
	= \bG_j'^\top \br + \bG^\top \left( \bJ_g(\bx) \right)_{:,j} , \quad j = 1,2, \ldots, n  .
\end{aligned}
\end{equation}
From this system of equations, we can find $\bmu_j'$ and plug into \eqref{equation:sep_jaco_j}, for which we can apply the Gauss-Newton method \eqref{equation:sep_gaussnew} or the Levenberg-Marquardt method \eqref{equation:sep_levmarg} accordingly.
Therefore, the matrix $\bJ(\bz) \in \real^{m \times (n+p)}$ in the full parameter set model is replaced by the smaller matrix $\bJ(\bx) \in \real^{m \times n}$. 

Given the normal equation $\bA(\bx) \bmu(\bx) = \bb(\bx) $, it follows that $\bG(\bx)^\top (\bG(\bx) \bmu(\bx) -  \by)=\bzero $.
This implies the residual $\br(\bx)$ in \eqref{equation:sep_red_resid} is orthogonal to the column space of $\bG(\bx)$: $\bG(\bx)^\top\br(\bx)=\bzero$. This implies that, in the right-hand side of the Gauss-Newton method \eqref{equation:sep_gaussnew} or the Levenberg-Marquardt method \eqref{equation:sep_levmarg}, (omitting the superscript) the $j$-th element is
$$
\left( \bJ(\bx)^\top \br(\bx) \right)_{j} = \left( \left( \bJ(\bx) \right)_{:,j}\right)^\top \br(\bx)
%= - \left( \bG_j'(\bx) \bmu(\bx) + \bG(\bx) \bmu_j'(\bx) \right)^\top \br(\bx)
= - \left( \bG_j'(\bx) \bmu(\bx) \right)^\top \br(\bx)  .
$$
This shows that the right-hand side in \eqref{equation:sep_gaussnew}-\eqref{equation:sep_levmarg} is independent of $\bmu_j'(\bx)$. To get the correct matrix on the left-hand side, however, we have to include the term $\bG(\bx) \bmu_j'(\bx)$.

Some special cases that lead to simple expressions for the Jacobian warrant attention:
\begin{itemize}
\item \textbf{$p=1$.} In this case $\bG(\bx), \bG'_j(\bx) \in \real^{m }$ are vectors and $A(\bx)$ and $\mu(\bx)$ are scalars. Additionally, 
since $\bG(\bx)^\top \bG'_j(\bx) =  \bG'_j(\bx)^\top\bG(\bx)$,
it also follows that
$$ 
A \mu_j' = \bG_j'^\top \br - \bG^\top \bG_j' \mu = \bG_j'^\top (\br - \bG \mu), \quad j = 1, 2,\ldots, n  . 
$$
Introducing the matrix $\bG' = [\bG_1',\bG_2',\ldots,  \bG_n']\in\real^{m\times n}$ and the $\real^n$ vector $\bmu'=[\mu_1', \mu_2', \ldots, \mu_n']^\top\in\real^n$, we get
$$
\bmu' = \frac{1}{A} \bG'^\top (\br - \mu \bG) \in\real^n, 
\qquad 
\bJ = - \mu \bG' - \bG (\bmu')^\top  \in\real^{m\times n}.
$$
This reduces to the nonlinear least squares problem in \eqref{equation:nonlinear_ols} in some sense.
\item \textbf{$p=n$ and $g_j(\bx, \zeta) = g_j(x_j, \zeta)$ for $j\in\{1,2,\ldots,n\}$}. That is, each of the functions $g_j$ depends only on $x_j$ and $\zeta$. In this case, all columns in $\bG_j'(\bx)\in\real^{m\times n}$ are zero except for the $j$-th column $\big( \bG_j'(\bx) \big)_{:,j}$. Therefore, the $j$-th element $\big( \bG_j'^\top \br \big)_j$ is the only nonzero element in $\bG_j'^\top \br$, and the other contribution $ \bG^\top \left( \bJ_g(\bx) \right)_{:,j} = - \bG^\top \left(\bG_j'(\bx) \bmu(\bx)\right)$ to the right-hand side in \eqref{equation:sep_redu_parAmuj} is $\bG^\top \big( \mu_j (\bG_j')_{:,j} \big)$. Let
$$
\bG' \triangleq \left[ (\bG_1')_{:,1},\;(\bG_2')_{:,2}, \;\ldots,\;  (\bG_n')_{:,n} \right] \in\real^{m\times n}, 
\qquad 
\bU' \triangleq \left[ \bmu_1', \bmu_2', \ldots, \bmu_n' \right] \in\real^{n\times n}.
$$
For \eqref{equation:sep_redu_parAmuj} and $j\in\{1,2,\ldots,n\}$, we have  
$$
\bA \bmu_j'=
\bG_j'^\top \br + \bG^\top \left( \bJ_g(\bx) \right)_{:,j}
=
\begin{bmatrix}
0, \ldots, \big( \bG_j'^\top \br \big)_j, 0, \ldots,0
\end{bmatrix}^\top 
- \bG^\top \big( \mu_j (\bG_j')_{:,j} \big)
\in\real^n.
$$
Then we can write \eqref{equation:sep_redu_parAmuj} as a matrix equation
$$
\bA \bU' = \diag(\bG^\top \br) - \bG^\top \bG'\diag(\bmu), %\quad \text{with} \quad \bP = \bG'\diag(\bmu) (= -\bJ_g) ,
$$
where $\diag(\cdot)$ is a diagonal matrix concerning the given vector.
And similarly, the Jacobian matrix in \eqref{equation:sep_jaco_j} takes the form
$$
\bJ = -\bG'\diag(\bmu) - \bG \bU'  .
$$
\end{itemize}







\section{Sparse Optimization Problem: $\ell_0$ Norm}

We have introduced several algorithms for solving the constrained optimization problem (P2) as defined in Definition~\ref{definition:opt_probs_all}, including projected gradient descent, conditional gradient method, and mirror descent method. These algorithms are applicable when the feasible set $\sS$ is closed and convex.
However, in this section, we focus on sparse optimization problems where the constraint set is non-convex. Consequently, none of the previously discussed results can be directly applied to this class of problems. Despite some similarities between convex and non-convex cases, there are significant differences in the types of results that can be obtained.

\section*{Definition and Notation}

In this section, we address the challenge of minimizing a continuously differentiable objective function subject to a sparsity constraint. Specifically, we consider the following problem:
\begin{equation}\label{equation:sparse_ellzero}
\textbf{(S0)}\qquad 
\begin{aligned}
	& \min_{\bx}
	& & f(\bx) \\
	& \text{s.t.}
	& & \normzero{\bx} \leq s,
\end{aligned}
\end{equation}
where $f : \real^n \to \real$, with its gradient being Lipschitz continuous with constant $\beta$, $s > 0$ is an integer smaller than $n$, and $\normzero{\bx}$ is the so-called $\ell_0$ norm of $\bx$, which counts the number of nonzero components in $\bx$:
$$
\normzero{\bx} = \absbig{\{i \mid x_i \neq 0, \forall i\in\{1,2,\ldots,n\}\}}.
$$

It is important to note that the $\ell_0$ is not actually a norm because it does not satisfy the homogeneity property; specifically, $\normzero{\lambda \bx} =  \normzero{\bx}$ for $\lambda \neq 0$ (Definition~\ref{definition:matrix-norm}). We do not assume that $f$ is a convex function, and the constraint set is inherently non-convex. As such, much of the analysis presented thus far does not apply to problem (S0). Nevertheless, this type of problem is crucial and has applications in areas like compressed sensing, making it valuable to study and understand how classical results over convex sets can be adapted for problem (S0).


To begin, let's define some notation. For a given vector $\bx \in \real^n$, we introduce the support set and the concept of $k$-sparse set.
\begin{definition}[Support  Set and $k$-Sparse Vectors]\label{definition:supposrt_supp_set}
The \textit{support set} of a vector $ \bx $ and its complement are  denoted, respectively, by 
$$ 
\sI_1(\bx)\triangleq\supp(\bx) \triangleq \{i \mid x_i \neq 0\}
\qquad\text{and}\qquad 
\sI_0(\bx) \triangleq \{i \mid x_i = 0\}.
$$
A vector $ \bx $ is referred to as \textit{$ k $-sparse} if $ \abs{\supp(\bx)} \leq k $, i.e., $\bx\in \sB_0[k]$ (the closed $\ell_0$ ball with a zero center: $\sB_0[k]=\sB_0[\bzero, k]$ for brevity; Definition~\ref{definition:open_closed_ball}), meaning that they have at most $k$ nonzero elements.
\end{definition}


Using this notation, problem (S0) can be restated as:
$$
\min \; f(\bx) \; \text{ s.t. }\;\bx \in \sB_0[s] .
$$
For a vector $\bx \in \real^n$ and $i \in \{1, 2, \ldots, n\}$, the \textit{$i$-th largest absolute value component} in $\bx$ is denoted by $[\bx]_i$, satisfying:
\begin{equation}
[\bx]_1 \geq [\bx]_2 \geq \ldots \geq [\bx]_n,
\end{equation}
where 
\begin{equation}
[\bx]_1 = \max_{i=1,\ldots,n} \abs{x_i}
\quad\text{and}\qquad 
[\bx]_n = \min_{i=1,\ldots,n} \abs{x_i}.
\end{equation}

Our goal now is to explore necessary optimality conditions for problem (S0), which share similarities with stationarity properties. Given the non-convexity of the constraint in (S0), traditional stationarity conditions may not hold. However, alternative optimality conditions can be formulated.

\subsection{Optimity Condition under $L$-Stationarity}

We begin by extending the concept of stationarity to problem (S0), utilizing the characterization of stationarity through the projection operator. Recall that $\bx^*$ is a stationary point of the problem of minimizing a continuously differentiable function over a closed and convex set $\sS$ if and only if
\begin{equation}\label{equation:lstat_proje}
\bx^* = \projectS(\bx^* - \eta \nabla f(\bx^*)),
\end{equation}
where $\eta$ is an arbitrary positive scalar (Theorem~\ref{theorem:stat_point_uncons_convset_proj}). 
Note that condition \eqref{equation:lstat_proje}---although expressed in terms of the parameter $\eta$---does not actually depend on $\eta$. However, this condition does not apply when $\sS = \sB_0[s]$, since the orthogonal projection onto non-convex sets like $\sB_0[s]$ is not unique; instead, it represents a multivalued mapping defined as:
$$
\project_{\sB_0[s]}(\bx) = \mathop{\argmin}_{\by} \{\normtwo{\by - \bx} \mid \by \in \sB_0[s]\}.
$$

By Weierstrass theorem (Theorem~\ref{theorem:weierstrass_them}\ref{weier2_prop_close_v3}),
the existence of vectors in $\project_{\sB_0[s]}(\bx)$ follows from the closedness of $\sB_0[s]$ and the coerciveness of the function $\normtwo{\by - \bx}$ (with respect to $\by$). To extend \eqref{equation:lstat_proje} to the sparsity constrained problem (S0), we introduce the notion of ``$L$-stationarity.''

\begin{definition}[$L$-Stationarity]\label{definition:l_stat}
A vector $\bx^* \in \sB_0[s]$ is called an $L$-stationary point of (S0) if it satisfies the relation
\begin{equation}\label{equation:l_stationary}
\textbf{($L$-stationarity)}:\qquad \bx^* \in \project_{\sB_0[s]}\left(\bx^* - \frac{1}{L} \nabla f(\bx^*)\right).
\end{equation}
\end{definition}

Given that the orthogonal projection operator $\project_{\sB_0[s]}(\cdot)$ is not single-valued, its output consists of all vectors comprising the 
$s$ components of $\bx$ with the largest absolute values, with zeros elsewhere. For example,
$$
\project_{\sB_0[2]}([3, 1, 1]^\top) = \big\{[3, 1, 0]^\top, [3, 0, 1]^\top\big\}.
$$

Below, we demonstrate that under the assumption that $f $ is $\beta$-smooth, $L$-stationarity is a necessary condition for optimality whenever $L > \beta$.
Before proving this assertion, let's present a more explicit representation of  $L$-stationarity.
\begin{lemma}[$L$-Stationarity]\label{lemma:l_stationary}
For any $L > 0$, a vector  $\bx^* \in \sB_0[s]$ satisfies $L$-stationarity if and only if
\begin{equation}\label{equation:l_stationarylem}
\abs{\frac{\partial f}{\partial x_i}(\bx^*)}
\begin{cases}
	\leq L \cdot [\bx^*]_s, & \text{if } i \in \sI_0(\bx^*), \\
	= 0, & \text{if } i \in \sI_1(\bx^*).
\end{cases}
\end{equation}
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:l_stationary}]
Suppose that $\bx^*$ satisfies $L$-stationarity. Note that for any index $j \in \{1, 2, \ldots, n\}$, the $j$-th component of any vector in $\project_{\sB_0[s]}(\bx^* - \frac{1}{L} \nabla f(\bx^*))$ is either zero or equal to $x_j^* - \frac{1}{L} \frac{\partial f}{\partial x_j}(\bx^*)$. On the other hand, since $\bx^* \in \project_{\sB_0[s]}(\bx^* - \frac{1}{L} \nabla f(\bx^*))$, it follows that if $i \in \sI_1(\bx^*)$, then $x_i^* = x_i^* - \frac{1}{L} \frac{\partial f}{\partial x_i}(\bx^*)$, so that $\frac{\partial f}{\partial x_i}(\bx^*) = 0$. If $i \in \sI_0(\bx^*)$, then $\abs{x_i^* - \frac{1}{L} \frac{\partial f}{\partial x_i}(\bx^*)} \leq [\bx^*]_s$ and $x_i^* = 0$, implying $\abs{ \frac{\partial f}{\partial x_i}(\bx^*)} \leq L \cdot [\bx^*]_s$. Therefore,  condition in \eqref{equation:l_stationarylem} holds.

Conversely, suppose that $\bx^*$ satisfies \eqref{equation:l_stationarylem}. If $\normzero{\bx^*} < s$, then $[\bx^*]_s = 0$, and by \eqref{equation:l_stationarylem} it follows that $\nabla f(\bx^*) = \bzero$; therefore, in this case, $\project_{\sB_0[s]}\left(\bx^* - \frac{1}{L} \nabla f(\bx^*)\right) = \project_{\sB_0[s]}(\bx^*)$ is the set $\{\bx^*\}$, which apparently satisfies the $L$-stationarity. 
On the other hand, if $\normzero{\bx^*} = s$, then $[\bx^*]_s \neq 0$ and $\abs{\sI_1(\bx^*)} = s$. By \eqref{equation:l_stationarylem},
$$
\abs{x_i^* - \frac{1}{L} \frac{\partial f}{\partial x_i}(\bx^*)}
\begin{cases}
\leq [\bx^*]_s, & i \in \sI_0(\bx^*), \\
= \abs{x_i^*}, & i \in \sI_1(\bx^*).
\end{cases}
$$
Therefore, the vector $\bx^* - \frac{1}{L} \nabla f(\bx^*)$ contains the $s$ components of $\bx^*$ with the largest absolute values and all other components are smaller or equal  to them in magnitude. This also implies the $L$-stationary and completes the proof.
\end{proof}


Clearly, condition \eqref{equation:l_stationarylem} depends on the constant $L$; it becomes more restrictive as $L$ decreases. 
This highlights that the situation in the non-convex case differs from the convex one. Before exploring under which conditions $L$-stationarity serves as a necessary optimality condition, we will first prove a technical yet useful result. 

%Our analysis heavily relies on the descent lemma, which stems from the definition of smoothness (Definition~\ref{definition:scss_func}): if $f $ is $\beta$-smooth and $L \geq \beta$, then
%$$
%f(\by) \leq f(\bx) + \innerproduct{\nabla f(\bx), \by - \bx} + \frac{L}{2} \normtwo{\by - \bx}^2.
%$$

\begin{lemma}[Descent Lemma for Sparse Projection under SS]\label{lemma:des_lem_lstat}
Let  $f: \real^n\rightarrow \real$  be a $\beta$-smooth function, and let $L > \beta$. Then for any $\bx \in \sB_0[s]$ and $\by \in \real^n$ satisfying
$
\by \in \project_{\sB_0[s]}\left( \bx - \frac{1}{L} \nabla f(\bx) \right),
$
we have
$$
f(\bx) - f(\by) \geq \frac{L - \beta}{2} \normtwo{\bx - \by}^2.
$$
\end{lemma}
\begin{proof}
Since $\by \in \project_{\sB_0[s]}\left( \bx - \frac{1}{L} \nabla f(\bx) \right)$, we have 
$$
\small
\begin{aligned}
\by 
&\in \mathop{\argmin}_{\bu \in \sB_0[s]} \normtwo{\bu - \left( \bx - \frac{1}{L} \nabla f(\bx) \right)}^2\\
&=\mathop{\argmin}_{\bu \in \sB_0[s]}  \frac{L}{2} \normtwo{\bu - \left( \bx - \frac{1}{L} \nabla f(\bx) \right)}^2  
+\underbrace{ f(\bx) -\frac{1}{2L} \normtwo{\nabla f(\bx)}^2}_{\text{constant w.r.t. } \bu}\\
&=\mathop{\argmin}_{\bu \in \sB_0[s]} f(\bx) + \langle \nabla f(\bx), \bu - \bx \rangle + \frac{L}{2} \normtwo{\bu - \bx}^2
\end{aligned}
$$
Let 
\begin{equation}\label{equation:des_lem_lstatpsi}
\Psi_L(\bu,\bx)\triangleq f(\bx) + \langle \nabla f(\bx), \bu - \bx \rangle + \frac{L}{2} \normtwo{\bu - \bx}^2.
\end{equation}
This implies that
$$
\Psi_L(\by, \bx) \leq \Psi_L(\bx, \bx) = f(\bx).
$$
By the $\beta$-smoothness of $f$ (Definition~\ref{definition:scss_func}), we have 
$$
f(\bx) - f(\by) \geq f(\bx) - \Psi_{\beta}(\by, \bx).
$$
Combining the preceding properties and the fact that $
\Psi_{\beta}(\by, \bx) = \Psi_L(\by, \bx) - \frac{L - \beta}{2} \normtwo{\bx - \by}^2
$ yields the desired result.
\end{proof}

We are now ready to demonstrate the necessity of  $L$-stationarity under the condition $L > \beta$ for $\beta$-smooth functions.


\begin{theoremHigh}[Necessity under $L$-Stationarity and Smoothness]\label{theorem:necess_lstat}
Let  $f: \real^n\rightarrow \real$ be a $\beta$-smooth function, and let $L > \beta$. Let $\bx^*$ be an optimal solution of (S0). Then
\begin{enumerate}[(i)]
\item $\bx^*$ is an $L$-stationary point.
\item The set $\project_{\sB_0[s]}\left(\bx^* - \frac{1}{L} \nabla f(\bx^*)\right)$ is a singleton.
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:necess_lstat}]
Assume, for contradiction, the set is not a singleton; that is, that there exists a vector
$
\by \in \project_{\sB_0[s]}\left(\bx^* - \frac{1}{L} \nabla f(\bx^*)\right),
$
which is different from $\bx^*$ ($\by \neq \bx^*$). By Lemma~\ref{lemma:des_lem_lstat}, we have 
$
f(\bx^*) - f(\by) \geq \frac{L - \beta}{2} \normtwo{\bx^* - \by}^2,
$
contradicting the optimality of $\bx^*$. 
Thus, $\bx^*$ is the only vector in the set $\project_{\sB_0[s]}\left(\bx^* - \frac{1}{L} \nabla f(\bx^*)\right)$, and $\bx^*$ is $L$-stationary.
\end{proof}

In summary, we have demonstrated that if the function is $\beta$-smooth, $L$-stationarity for any $L > \beta$ is a necessary optimality condition for problem (S0).


\subsection{Restricted Isometry and Other Design Properties}

In many applications, sparse optimization problems involve minimizing the function $f(\bx) = \frac{1}{2m}\normtwo{\bb - \bA \bx}$, where $\bA\in\real^{m\times n}$ is referred to as the \textit{design matrix} and $\bb\in\real^m$.
Apparently, when the design matrix is an identity,  it preserves the geometric properties of signals or models, enabling the recovery of sparse signals. 
We will now further formalize this concept and establish specific conditions on $\bA$ that guarantee universal recovery, meaning that for every $\bx \in \sB_0[s]$, $\bx$ can be uniquely recovered from the measurements $\bA\bx$.



To be more specific, a design matrix capable of identifying sparse vectors does not necessarily ensure universal recovery. For instance, consider a design matrix $\bA \in \real^{m\times n}$ such that for some distinct $\bx_1, \bx_2 \in \sB_0[s]$ and $\bx_1 \neq \bx_2$, we have $\by_{1} = \by_{2}$, where $\by_{1} = \bA \bx_1$ and $\by_{2} = \bA \bx_2$. 
In this scenario, it becomes theoretically impossible to distinguish between $\bx_1$ and $\bx_2$ based on measurements made using $\bA$, i.e., using $\by_{1}$ (or $\by_{2}$). 
Consequently, this design matrix cannot support universal recovery because it fails to differentiate between sparse vectors. It's important to note that such a matrix would fail to uniquely identify not just one pair but an infinite set of pairs---or even an entire subspace---of sparse vectors.


Therefore, it is crucial that the design matrix maintains the geometry of sparse vectors when projecting them from a high-dimensional space ($n$ dimensions) to a lower-dimensional space ($m$ dimensions), especially in scenarios where $m \ll n$. The \textit{nullspace property}, among others, encapsulates this requirement. 
To see this, for any subset of coordinates $\sS \subseteq \{1, 2,\ldots, n\}$,  define the sets:
\begin{align}
	\sC[\sS] &\triangleq \{\bx \in \real^{n} \mid  \normone{\bx_{\comple{\sS}}} \leq \normone{\bx_{\sS}} \};\\
	\sC[\sS; \gamma] &\triangleq \{\bx \in \real^{n} \mid  \normone{\bx_{\comple{\sS}}} \leq \gamma\normone{\bx_{\sS}} \};\\
	\sC[k] &\triangleq \bigcup_{\sS: \abs{\sS} = k} \sC[\sS],
\end{align}
where $\comple{\sS}$ is the complement of $\sS$ (Definition~\ref{definition:matlabnotation}).
Here, square brackets are used instead of parentheses to indicate inclusion of equalities in the definitions. 
Thus, $\sC[\sS]=\sC[\sS; 1]$ represents the convex set of points placing most of their weight on coordinates within the set $\sS$.
Meanwhile, $\sC[k]$ constitutes the non-convex set of points concentrating most of their weight on some $k$ coordinates.
Note that $ \sB_0[k] \subset \sC[k] $~\footnote{Note that $\sB_0[k]=\sB[\bzero,  k]$ denotes the closed ball using the $\ell_0$ norm.} since $k$-sparse vectors assign all their weight to $k$ coordinates, and that $\sC[\sS; \gamma_1] \subseteq \sC[\sS; \gamma_2]$ if $\gamma_1\leq \gamma_2$.




We now introduce certain desirable properties of the design matrix, which are extensively covered in literature, e.g.,  \citet{candes2005decoding, cohen2009compressed, raskutti2010restricted, jain2014iterative, hastle2015statistical, jain2017non}.
\begin{definition}[Nullspace Property]\label{definition:nullspace_prop}
A matrix $\bA \in \real^{m\times n}$ is said to satisfy the \textit{nullspace property of order $k$} if $\nspace(\bA) \cap \sC[k] = \{\bzero\}$, where $\nspace(\bA) \triangleq \{\bx \in \real^{n}: \bA \bx = \bzero\}$ denotes the nullspace of $\bA$ (Definition~\ref{definition:nullspace}).
\end{definition}

If a design matrix satisfies this property, vectors in its nullspace cannot concentrate most of their weight on any $k$ coordinates. Consequently, no $k$-sparse vector can reside within the nullspace. If a design matrix has the nullspace property of order $2s$, it ensures that two distinct  $s$-sparse vectors cannot be identified, which is crucial for ensuring global recovery. An enhanced version of the nullspace property leads to the restricted eigenvalue property.

\begin{definition}[Restricted Eigenvalue Property]\label{definition:res_eig}
A matrix $\bA \in \real^{m\times n}$ is said to satisfy the \textit{restricted eigenvalue (RE) property with constant $\alpha$} over $\sS$ if for all $\bx \in \sS$, we have 
$$
\alpha\leq \frac{\frac{1}{m} \bx^\top\bA^\top\bA\bx}{\normtwo{\bx}^{2}}
\qquad\iff\qquad 
\alpha \cdot \normtwo{\bx}^{2} \leq \frac{1}{m} \normtwo{\bA\bx}^{2},
$$
where the right-hand side of the first inequality represents  the Rayleigh quotient of the vector $\bx$ associated with the matrix $\frac{1}{m} \bA^\top\bA$, and $\frac{1}{m} \bA^\top\bA$ is the Hessian of the function $f(\bx) = \frac{1}{2m}\normtwo{\bA\bx-\bb}^2$.
\end{definition}

This property implies that not only are $k$-sparse vectors absent from the nullspace, they actually retain a significant  fraction of their length after projection as well. This means that if $k=2s$, then 
\begin{equation}
\bx_1, \bx_2 \in \sB_0[s] 
\quad\implies\quad 
\frac{1}{m} \normtwo{\bA(\bx_1 - \bx_2)}^{2} \geq \alpha \cdot \normtwo{\bx_1 - \bx_2}^{2}
\end{equation}
%$\bx_1, \bx_2 \in \sB_0[s]$, we have $\frac{1}{m} \normtwo{\bA(\bx_1 - \bx_2)}^{2} \geq \alpha \cdot \normtwo{\bx_1 - \bx_2}^{2}$.
Thus, the distance between any two sparse vectors does not significantly decrease after projection. 




The next property further explicates this concept and is known as the restricted isometry property.

\begin{definition}[Restricted Isometry Property]\label{definition:rip}
A matrix $\bA \in \real^{m\times n}$ is said to satisfy the \textit{restricted isometry property (RIP) of order $k$ with constant $\delta_{k} \in [0,1)$} if for all $\bx \in \sB_0[k] $, we have
$$
(1-\delta_{k}) \cdot \normtwo{\bx}^{2} \leq \frac{1}{m} \normtwo{\bA\bx}^{2} \leq (1+\delta_{k}) \cdot \normtwo{\bx}^{2}.
$$
\end{definition}

This property is widely used in analyzing sparse recovery and compressive sensing algorithms. However, it can be somewhat restrictive since it requires distortion parameters to be of the form $(1 \pm \delta)$ for $\delta \in [0,1)$. 


\begin{exercise}[Subset of Restricted Isometry Property]\label{exercise:sub_rip}
Suppose the matrix $ \bA \in \real^{m\times n} $ satisfies RIP at order $ s $ with constant $ \delta_s $ over $\sB_0[s]$. Show that for any set $ \sI \subset \{1,2,\ldots,n\}, \abs{\sI} \leq s $, the smallest eigenvalue of the matrix $ \bA_{\sI}^\top \bA_{\sI} $ is lower bounded by $ (1 - \delta_s) $, where $\bA_{\sI}\triangleq\bA[:,\sI]$.
\end{exercise}
%\begin{proof}[of Lemma~\ref{lemma:sub_rip}]
%The smallest eigenvalue $\lambda_{\min}$ of $\bA_{\sI}^\top \bA_{\sI}$ is defined as:
%$$
%\lambda_{\min} = \min_{\bx_{\sI} \neq 0} \frac{\bx_{\sI}^\top \bA_{\sI}^\top \bA_{\sI} \bx_{\sI}}{\bx_{\sI}^\top \bx_{\sI}}
%\geq 1 - \delta_s
%$$
%\end{proof}



%Using the RIP inequality:
%$$
%\bx_{\sI}^\top \bA_{\sI}^\top \bA_{\sI} \bx_{\sI} = \normtwo{\bA_{\sI} \bx_{\sI}}^2 \geq (1 - \delta_s) \normtwo{\bx_{\sI}}^2.
%$$
%
%Therefore, for any nonzero $\bx_{\sI}$:
%$$
%\frac{\bx_{\sI}^\top \bA_{\sI}^\top \bA_{\sI} \bx_{\sI}}{\bx_{\sI}^\top \bx_{\sI}} \geq 1 - \delta_s.
%$$
%
%The smallest eigenvalue $\lambda_{\min}$ of $\bA_{\sI}^\top \bA_{\sI}$ is thus lower bounded by $1 - \delta_s$:
%$$
%\lambda_{\min}(\bA_{\sI}^\top \bA_{\sI}) \geq 1 - \delta_s.
%$$

A useful generalization of this property, particularly relevant in scenarios where the properties of the design matrix are not strictly controlled---such as in gene expression analysis---is the concept of restricted strong convexity and smoothness.


\begin{definition}[Restricted Strong Convexity/Smoothness Property]\label{definition:res_scss_mat}
A matrix $\bA \in \real^{m\times n}$ is said to satisfy the \textit{$\alpha$-restricted strong convexity (RSC) property} and the \textit{$\beta$-restricted smoothness (RSS) property of order $k$} if for all $\bx \in \sB_0[k]$, we have
$$
\alpha \cdot \normtwo{\bx}^{2} \leq \frac{1}{m} \normtwo{\bA\bx}^{2} \leq \beta \cdot \normtwo{\bx}^{2}.
$$
\end{definition}

The key difference between RIP and RSC/RSS properties lies in their constants. RIP requires these constants to be of the form $1 \pm \delta_{k}$, whereas RSC/RSS does not impose such constraints. Readers will notice similarities between the definitions of restricted strong convexity and smoothness provided here and Definition~\ref{definition:res_scss_func}, where we defined these concepts for general functions. It's encouraged that readers verify the relationship between these two definitions. Indeed, Definition~\ref{definition:res_scss_func} can be viewed as a generalization of Definition~\ref{definition:res_scss_mat} to general functions \citep{jain2014iterative}. For twice-differentiable functions, both definitions essentially impose restrictions on the (restricted) eigenvalues of the function's Hessian.



\subsection{The Iterative Hard-Thresholding (IHT) Method}
\begin{algorithm}[h] 
\caption{Iterative Hard-Thresholding (IHT) Method}
\label{alg:pgd_iht}
\begin{algorithmic}[1] 
\Require Sparsity level $k$; 
\State {\bfseries Input (a).} A differential function $f$; 
\State {\bfseries Input (b).} $f(\bx) = \frac{1}{2m}\normtwo{\bb - \bA \bx}$: design matrix $\bA\in\real^{m\times n}$ and  observation vector $\bb\in\real^m$;
\State {\bfseries Input:}  Initialize $\bx^{(1)} \in \sB_0[k]$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t$;
\State $\by^{(t+1)} \stackrel{(a)}{\leftarrow} \bx^{(t)} - \eta_t \nabla f(\bx^{(t)}) \quad \text{ or }\quad \stackrel{(b)}{\leftarrow} \bx^{(t)} - \eta_t  \frac{1}{m} \bA^\top (\bA\bx^\toptzero - \bb)$;
\State $\bx^{(t+1)} \in \mathcalP_{\sB_0[k]}(\by^{(t+1)})$; \Comment{Selecting the $k$ largest elements in magnitude of $\by^{(t+1)}$}
\EndFor
\State {\bfseries Return:}  final $\bx\leftarrow \bx^{(t)}$;
\end{algorithmic} 
\end{algorithm}

One approach to solving problem (S0) involves using a natural generalization of the projected gradient descent algorithm, which can be described as follows:
\begin{subequations}
\begin{equation}
\bx^\toptone \in \project_{\sB_0[s]}\left(\bx^\toptzero - \frac{1}{L} \nabla f(\bx^\toptzero)\right), \quad t = 1, 2, \ldots.
\end{equation}
The method is known in the literature as the \textit{iterative hard-thresholding (IHT) method} (Algorithm~\ref{alg:pgd_iht}), and we will adopt this terminology.
Note that the algorithm can be  applied  both to a general function $f$ (with properties outlined in Lemma~\ref{lemma:conv_iht_smoot})  and a least squares objective function $f(\bx) = \frac{1}{2m}\normtwo{\bb - \bA \bx}$ (which satisfies certain design properties as previously discussed).
It has been shown that the general step of the IHT method is equivalent to the relation
\begin{equation}
\bx^\toptone \in \mathop{\argmin}_{\bx \in \sB_0[s]} \Psi_L(\bx, \bx^\toptzero), \quad t = 1, 2, \ldots,
\end{equation}
\end{subequations}
where $\Psi_L(\bx, \by)$ is defined by \eqref{equation:des_lem_lstatpsi}.
When the projected set $\sS$ is closed convex, the projected gradient descent method (Algorithm~\ref{alg:pgd_gen}) can be considered  a fixed-point method for solving $
\bx^* =\mathcalP_{\sS}\big(\bx^* - \eta\nabla f(\bx^*)\big)
$ (Theorem~\ref{theorem:stat_point_uncons_convset_proj}).
Therefore, the IHT method can also be viewed as a fixed-point method aimed at ``enforcing'' the $L$-stationary condition (Definition~\ref{definition:l_stat}, Theorem~\ref{theorem:necess_lstat}) and solving $\bx^* =\mathcalP_{\sS}\big(\bx^* - \frac{1}{L}\nabla f(\bx^*)\big)$ for $L>\beta$,  assuming the underlying function is $\beta$-smooth.

Several fundamental properties of the IHT method are summarized in the following lemma:
\begin{lemma}[Properties of IHT under Smoothness]\label{lemma:conv_iht_smoot}
Let  $f: \real^n\rightarrow \real$ be a $\beta$-smooth and lower bounded function.
Suppose $\{\bx^\toptzero\}_{t > 0}$ is the sequence generated by the IHT method (Algorithm~\ref{alg:pgd_iht}(a)) with a constant stepsize $\eta\triangleq\eta_t\triangleq\frac{1}{L}$, where $L > \beta$, and a projection sparsity level $k=s$. Then,
\begin{enumerate}[(i)]
\item $f(\bx^\toptzero) - f(\bx^\toptone) \geq \frac{L - \beta}{2} \normtwo{\bx^\toptzero - \bx^\toptone}^2$,
\item $\{f(\bx^\toptzero)\}_{t > 0}$ is a nonincreasing sequence,
\item $\normtwo{\bx^\toptzero - \bx^\toptone} \to 0$,
\item For every $t = 1, 2, \ldots$, if $\bx^\toptzero \neq \bx^\toptone$, then $f(\bx^\toptone) < f(\bx^\toptzero)$.
\end{enumerate}
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:conv_iht_smoot}]
Part (i) follows from Lemma~\ref{lemma:des_lem_lstat} by substituting $\bx = \bx^\toptzero, \by = \bx^\toptone$. Part (ii) follows immediately from part (i). To prove (iii), note that since $\{f(\bx^\toptzero)\}_{t > 0}$ is a nonincreasing sequence, which is also lower bounded, it follows that it converges to some limit $B$ and hence $f(\bx^\toptzero)) - f(\bx^\toptone) \to B - B = 0$ as $t \to \infty$. Therefore, by part (i) and the fact that $L > \beta$, the limit $\normtwo{\bx^\toptzero - \bx^\toptone} \to 0$ holds. Finally, (iv) is a direct consequence of (i).
\end{proof}

As previously mentioned, the IHT algorithm can be considered a fixed-point method for solving the $L$-stationarity condition. 
The following theorem states that all accumulation points of the sequence generated by the IHT method with a constant  stepsize $\frac{1}{L}$ are indeed $L$-stationary points.
\begin{theoremHigh}[Convergence of IHT \citep{beck2014introduction}]\label{theorem:acc_iht_conv}
Consider the same conditions  as Lemma~\ref{lemma:conv_iht_smoot}.
Let $\{\bx^\toptzero\}_{t > 0}$ be the sequence generated by the IHT method (Algorithm~\ref{alg:pgd_iht}(a)) with a constant stepsize $\eta\triangleq\eta_t \triangleq \frac{1}{L}$, where $L > \beta$, and a projection sparsity level $k=s$. Then, any accumulation point of $\{\bx^\toptzero\}_{t > 0}$ is an $L$-stationary point.
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:acc_iht_conv}]
Suppose that $\bx^*$ is an accumulation point of the sequence. 
Thus, there exists a subsequence $\{\bx^{(t_j)}\}_{j > 0}$ that converges to $\bx^*$. By Lemma~\ref{lemma:conv_iht_smoot}, we have 
\begin{equation}\label{equation:acc_iht_conv1}
f(\bx^{(t_j)}) - f(\bx^{(t_j + 1)}) \geq \frac{L - \beta}{2} \normtwo{\bx^{(t_j)} - \bx^{(t_j + 1)}}^2.
\end{equation}
Since $\{f(\bx^{(t_j)})\}_{j > 0}$ and $\{f(\bx^{(t_j + 1)})\}_{j > 0}$, as nonincreasing and lower bounded sequences,
they both converge to the same limit.
Consequently, $f(\bx^{(t_j)}) - f(\bx^{(t_j + 1)}) \to 0$ as $j \to \infty$, which combined with \eqref{equation:acc_iht_conv1} yields that
$
\bx^{(t_j + 1)} \to \bx^* \text{ as } j \to \infty.
$
Recall that for all $j > 0$
$
\bx^{(t_j + 1)} \in \project_{\sB_0[s]}\left(\bx^{(t_j)} - \frac{1}{L} \nabla f(\bx^{(t_j)})\right).
$
We then consider the following two cases.

Let $i \in \sI_1(\bx^*)$ (i.e., in the suppose set; Definition~\ref{definition:supposrt_supp_set}). By the convergence of $\bx^{(t_j)}$ and $\bx^{(t_j + 1)}$ to $\bx^*$, it follows that there exists an integer $J$ such that
$$
x_i^{(t_j)}, x_i^{(t_j+1)} \neq 0 \text{ for all } j > J,
$$
and therefore, for $j > J$,
$$
x_i^{(t_j+1)} = x_i^{(t_j)} - \frac{1}{L} \frac{\partial f}{\partial x_i}(\bx^{(t_j)}).
$$
Taking the limit as  $j\rightarrow \infty$ and using the continuity of $f$, we obtain that
$
\frac{\partial f}{\partial x_i}(\bx^*) = 0.
$

Now let $i \in \sI_0(\bx^*)$. If there exist an infinite number of indices $t_j$ for which $x_i^{(t_j+1)} \neq 0$, then as in the previous case, we obtain that $x_i^{(t_j+1)} = x_i^{(t_j)} - \frac{1}{L} \frac{\partial f}{\partial x_i}(\bx^{(t_j)})$ for these indices, implying (by taking the limit) that $\frac{\partial f}{\partial x_i}(\bx^*) = 0$. In particular, $\left| \frac{\partial f}{\partial x_i}(\bx^*) \right| \leq L [\bx^*]_s$. On the other hand, if there exists an $M > 0$ such that for all $j > M$, $x_i^{(t_j+1)} = 0$, then
$$
\left| x_i^{(t_j)} - \frac{1}{L} \frac{\partial f}{\partial x_i}(\bx^{(t_j)}) \right| \leq [\bx^{(t_j)} - \frac{1}{L} \nabla f(\bx^{(t_j)}) ]_s = [\bx^{(t_j + 1)}]_s.
$$

Thus, taking $j$ to infinity, while exploiting the continuity of the function $[\cdot]_s$, we obtain that
$$
\left| \frac{\partial f}{\partial x_i}(\bx^*) \right| \leq L [\bx^*]_s,
$$
and hence, by Lemma~\ref{lemma:l_stationary}, the desired result is established.
\end{proof}








We will now establish an important result: if the design matrix satisfies the RIP condition with suitable constants, then the IHT algorithm guarantees universal sparse recovery for the least squares problem: $f(\bx) = \frac{1}{2m}\normtwo{\bb - \bA \bx}$.
\begin{theoremHigh}[Rate of Convergence of IHT \citep{jain2014iterative, jain2017non}]\label{theorem:conv_iht_lasso}

Suppose $\bA \in \real^{m \times n}$ is a design matrix that satisfies the RIP property of order $3s$ with parameter $\delta_{3s} < \frac{1}{2}$. Let $\bx^* \in \sB_0[s] \subset \real^n$ be any arbitrary sparse vector and let $\bb = \bA \bx^*$. Then the IHT algorithm (Algorithm~\ref{alg:pgd_iht}(b)), when executed with a constant stepsize $\eta_t = 1$ and a projection sparsity level $k = s$, ensures $\normtwo{\bx^\toptzero - \bx^*} \leq \epsilon$ after at most $t = \mathcalO\left(\ln \frac{\normtwo{\bx^*}}{\epsilon}\right)$ iterations of the algorithm.
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:conv_iht_lasso}]
Let $\sS^* \triangleq \text{supp}(\bx^*)$ and $\sS^t \triangleq \text{supp}(\bx^\toptzero)$. 
Define $\sI^t \triangleq \sS^t \cup \sS^{t+1} \cup \sS^*$ as the union of the supports of  two consecutive iterates and the optimal model. 
This definition ensures that while analyzing this update step, the error vectors $\bx^\toptzero - \bx^*$ and $\bx^\toptone - \bx^*$, which will be the focal point of the analysis, have support within $\sI^t$. Note that $\abs{\sI^t} \leq 3s$. 



With $\eta_t = 1$,  $\by^\toptone = \bx^\toptzero - \frac{1}{m} \bA^\top (\bA \bx^\toptzero - \bb)$ by Algorithm~\ref{alg:pgd_iht}. 
Since the set $\sB_0[k]$ is non-convex, we can only apply the Projection Property-O (Lemma~\ref{lemma:proj_prop0}) for the projection step $\bx^\toptone = \mathcalP_{\sB_0[k]}(\by^\toptone)$. Along with the fact that $\normtwo{\bu}^2 = \normtwo{\bu_{\sI}}^2 + \normtwo{\bu_{\comple{\sI}}}^2$ for any vector $\bu\in\real^n$ (where again $\comple{\sI}$ denotes the complement of $\sI$), we have
$$
\begin{aligned}
&\normtwo{\bx^\toptone - \by^\toptone}^2 \leq \normtwo{\bx^* - \by^\toptone}^2\\
&\implies \quad \normtwo{\bx_{\sI}^\toptone - \by_{\sI}^\toptone}^2 + \normtwo{\bx_{\comple{\sI}}^\toptone - \by_{\comple{\sI}}^\toptone}^2 \leq \normtwo{\bx_{\sI}^* - \by_{\sI}^\toptone}^2 + \normtwo{\bx_{\comple{\sI}}^* - \by_{\comple{\sI}}^\toptone}^2,
\end{aligned}
$$
where, for brevity, we denote $\sI \triangleq\sI^t$ such that $\bx_{\comple{\sI}}^\toptone = \bx_{\comple{\sI}}^* = \bzero$. 
Using the fact that $\bb = \bA \bx^*$, and denoting $\widetildebA \triangleq \frac{1}{\sqrt{m}} \bA$ and $\widetildebA_{\sI} \triangleq\widetildebA[:,\sI]$, we have
$$
\begin{aligned}
&\normtwo{\bx_{\sI}^\toptone - \by_{\sI}^\toptone} \leq \normtwo{\bx_{\sI}^* - \by_{\sI}^\toptone}\\
&\Rightarrow
\normtwo{\bx_{\sI}^\toptone - \bx_{\sI}^* + \bx_{\sI}^* - \left( \bx_{\sI}^\toptzero - \widetildebA_{\sI}^\top \widetildebA (\bx^\toptzero - \bx^*) \right)} 
\leq 
\normtwo{\bx_{\sI}^* - \left( \bx_{\sI}^\toptzero - \widetildebA_{\sI}^\top \widetildebA (\bx^\toptzero - \bx^*) \right)}\\
&\Rightarrow 
\normtwo{\bx_{\sI}^\toptone - \bx_{\sI}^*} 
\leq 
2 \normtwo{\left( \bx_{\sI}^\toptzero - \bx_{\sI}^* \right) - \widetildebA_{\sI}^\top \widetildebA (\bx^\toptzero - \bx^*)},
\end{aligned}
$$
where the last inequality follows from the triangle inequality for any norm: $\abs{\norm{\bu}-\norm{\bv}} \leq \norm{\bu-\bv}$ for any $\bu$ and $\bv$, and $\by_{\sI}^\toptone = \left(\bx^{(t)} -  \widetildebA^\top\widetildebA (\bx^\toptzero - \bx^*)\right)_{\sI} = \bx^{(t)}_{\sI} -  \widetildebA_{\sI}^\top\widetildebA (\bx^\toptzero - \bx^*)$. 
Given that $\bx_{\comple{\sI}}^\toptzero = \bx_{\comple{\sI}}^* = \bzero$, it holds that 
$$
\widetildebA_{\comple{\sI}} (\bx^\toptzero_{\comple{\sI}} - \bx^*_{\comple{\sI}}) = \bzero
\,\,\, \implies\,\,\, 
\widetildebA (\bx^\toptzero - \bx^*) = 
\widetildebA_{\comple{\sI}} (\bx^\toptzero_{\comple{\sI}} - \bx^*_{\comple{\sI}}) +  \widetildebA_{{\sI}} (\bx^\toptzero_{{\sI}} - \bx^*_{{\sI}})
= \widetildebA_{\sI} (\bx^\toptzero_{\sI} - \bx^*_{\sI}),
$$
whence we have 
\begin{equation}
\begin{aligned}
\normtwo{\bx^\toptone - \bx^* } 
&\leq 2 \normtwo{(\bI - \widetildebA_{\sI}^\top \widetildebA_{\sI}) (\bx_{\sI}^\toptzero - \bx_{\sI}^*)}\\
&\leq 2 \left( \normtwo{\bx_{\sI}^\toptzero - \bx_{\sI}^*} - \normtwo{\widetildebA_{\sI}^\top \widetildebA_{\sI} (\bx_{\sI}^\toptzero - \bx_{\sI}^*)} \right)
\leq 2 \delta_{3s} \normtwo{\bx_{\sI}^\toptzero - \bx_{\sI}^*}.
\end{aligned}
\end{equation}
This completes the proof.
\end{proof}


For further proving the rate of convergence for a general function, we need the following lemma.

\begin{lemma}[Sparsity Projection Property]\label{lemma:spar_proj}
Let $\bx \in \real^n$ and  $\widetildebx \triangleq \mathcal\project_{\sB_0[s]}(\bx)$. Then for any $\bx^* \in \sB_0[s^*]\subseteq \real^n$ such that $\normzero{\bx^*} \leq s^*$, where $s^*\leq s\leq n$, we have
$$
\frac{\normtwo{\widetildebx - \bx}^2}{n-s} \leq  \frac{\normtwo{\bx^* - \bx}^2}{n-s^*}
\qquad\implies\qquad 
\normtwo{\widetildebx - \bx}^2 \leq \frac{n - s}{n - s^*} \normtwo{\bx^* - \bx}^2.
$$
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:spar_proj}]
Without loss of generality, assume that we have reordered coordinates such that $\abs{x_1} \geq \abs{x_2} \geq \ldots \geq \abs{x_n}$. Since the projection operator $\mathcal\project_{\sB_0[s]}(\cdot)$ operates by selecting the largest elements by magnitude, we have $\widetildex_1 = x_1, \widetildex_2 = x_2 \ldots, \widetildex_s = x_s$ and $\widetildex_{s+1} = \widetildex_{s+2} = \ldots = \widetildex_{n} = 0$.

Let $\widehatbx \triangleq \mathcalP_{\sB_0[s^*]}(\bx)$. By the above argument, we have $\widehatx_1 = x_1, \widehatx_2 = x_2, \ldots, \widehatx_{s^*} = x_{s^*}$ and $\widehatx_{s^*+1} = \widehatx_{s^*+2} = \ldots = \widehatx_{n} = 0$. 
Therefore, assuming $s^*\leq s$, we have 
\begin{equation}
\begin{aligned}
(n-s)&(n-s^*)\left(\frac{\normtwo{\widehatbx - \bx}^2}{n - s^*} - \frac{\normtwo{\widetildebx - \bx}^2}{n - s} \right)
=(n-s) \sum_{i=s^*+1}^{n} x_i^2 - (n-s^*)\sum_{i=s+1}^{n} x_i^2\\
&=(n-s) \sum_{i=s^*+1}^{\textcolor{mylightbluetext}{s}} x_i^2 + \big((n-s)-(n-s^*)\big)\sum_{i=s+1}^{n} x_i^2\\
&\geq (n-s)(s-s^*)  x_{s}^2 +  (s^*-s)(n-s) x_{s+1}^2 \geq 0.\\
\end{aligned}
\end{equation}
Given the Projection Property-O (Lemma~\ref{lemma:proj_prop0}), it is valid that $ \normtwo{\widehatbx - \bx} \leq \normtwo{\bx^* - \bx} $ for any $\bx^* \in \sB_0[s^*]$, which establishes the desired result when combining the above inequality.
\end{proof}



For a more general function $f$ with an optimal $s^*$-sparse parameter $\bx^*$, other than $f(\bx) = \frac{1}{2m}\normtwo{\bb - \bA \bx}$,
our analysis combines the above observation with the RSC/RSS properties of $f$ to provide geometric convergence rates for the IHT procedure below.




\begin{theoremHigh}[Rate of Convergence of IHT \citep{jain2014iterative}]\label{theorem:conv_iht_genfunc}
Let $f$ have RSC and RSS parameters given by $\alpha_{2s+s^*}(f) = \alpha$ and $\beta_{2s+s^*}(f) = \beta$, respectively. Let Algorithm~\ref{alg:pgd_iht} be invoked with $f$, $k\triangleq s \geq 32 \left(\frac{\beta}{\alpha}\right)^2 s^*$, and a constant stepsize $\eta_t = \frac{2}{3\beta}$. Also let $\bx^* = \argmin_{\normzero{\bx} \leq s^*} f(\bx)$. Then, the $T$-th iterate of Algorithm~\ref{alg:pgd_iht}(a), for $T = \mathcalO\left(\frac{\beta}{\alpha} \cdot \ln\left(\frac{f(\bx^\topone)}{\epsilon}\right)\right)$ satisfies:
$ f(\bx^{(T)}) - f(\bx^*) \leq \epsilon.
$
\end{theoremHigh}

\begin{proof}[of Theorem~\ref{theorem:conv_iht_genfunc}]
Denote $\bg^\toptzero \triangleq \nabla f(\bx^\toptzero)$, recall that $\bx^\toptone = \mathcal\project_{\sB_0[s]}(\bx^\toptzero - \frac{\gamma}{\beta} \bg^\toptzero)$ where $\gamma = \frac{2}{3} < 1$. Let $\sS^t \triangleq \supp(\bx^\toptzero)$, $\sS^* \triangleq \supp(\bx^*)$, and $\sS^{t+1} \triangleq \supp(\bx^\toptone)$. Also, let $\sI \triangleq \sS^* \cup \sS^t \cup \sS^{t+1}$.
Given the RSS property (Definition~\ref{definition:res_scss_func}) and the fact that $\supp(\bx^\toptzero) \subseteq \sI$ and $\supp(\bx^\toptone) \subseteq \sI$ (implying that $\bx_{\comple{\sI}}^\toptone = \bx_{\comple{\sI}}^\toptzero = \bzero$), along with the fact that $\normtwo{\bu}^2 = \normtwo{\bu_{\sI}}^2 + \normtwo{\bu_{\comple{\sI}}}^2$ for any vector $\bu\in\real^n$, we have:
\begin{equation}\label{equation:conv_iht_genfunc1}
\begin{aligned}
&f(\bx^\toptone) - f(\bx^\toptzero) \leq \innerproduct{\bg^\toptzero, \bx^\toptone - \bx^\toptzero} + \frac{\beta}{2} \normtwo{\bx^\toptone - \bx^\toptzero}^2, \\
&= \frac{\beta}{2} \normtwo{\bx^\toptone_{\sI} - \bx^\toptzero_{\sI} + \frac{\gamma}{\beta} \cdot  \bg^\toptzero_{\sI}}^2 - \frac{\gamma^2}{2\beta} \normtwo{\bg^\toptzero_{\sI}}^2 + (1 - \gamma) \innerproduct{\bx^\toptone - \bx^\toptzero, \bg^\toptzero}.
\end{aligned}
\end{equation}
Since $\sS^t \setminus \sS^{t+1}$ and $\sS^{t+1}$ are disjoint, we have:
\begin{equation}\label{equation:conv_iht_genfunc2}
\small
\begin{aligned}
\innerproduct{\bx^\toptone - \bx^\toptzero, \bg^\toptzero} 
&= \innerproduct{\,\cancel{\bx^\toptone_{\sS^t \setminus \sS^{t+1}}} - \bx^\toptzero_{\sS^t \setminus \sS^{t+1}}, \bg^\toptzero_{\sS^t \setminus \sS^{t+1}}} 
+ \innerproduct{\bx^\toptone_{\sS^{t+1}} - \bx^\toptzero_{\sS^{t+1}}, \bg^\toptzero_{\sS^{t+1}}} \\
&\overset{\dag}{=} - \innerproduct{\bx^\toptzero_{\sS^t \setminus \sS^{t+1}}, \bg^\toptzero_{\sS^t \setminus \sS^{t+1}}} - \frac{\gamma}{\beta} \normtwo{\bg^\toptzero_{\sS^{t+1}}}^2 \\
&\overset{\ddag}{\leq} \frac{\gamma}{2\beta} \normtwo{\bg^\toptzero_{\sS^{t+1} \setminus \sS^t}}^2 - \frac{\gamma}{2\beta} \normtwo{\bg^\toptzero_{\sS^t \setminus \sS^{t+1}}}^2 - \frac{\gamma}{\beta} \normtwo{\bg^\toptzero_{\sS^{t+1}}}^2   \cancel{-\left( \frac{\beta}{2\gamma} \normtwo{\bx_{\sS^t \setminus \sS^{t+1}}}^2\right)} \\
&\overset{*}{=} - \frac{\gamma}{2\beta} \normtwo{\bg^\toptzero_{\sS^{t+1} \setminus \sS^t}}^2 - \frac{\gamma}{2\beta} \normtwo{\bg^\toptzero_{\sS^t \setminus \sS^{t+1}}}^2 - \frac{\gamma}{\beta} \normtwo{\bg^\toptzero_{\sS^t \cap \sS^{t+1}}}^2 \\
&\leq - \frac{\gamma}{2\beta} \normtwo{\bg^\toptzero_{\sS^t \cup \sS^{t+1}}}^2,
\end{aligned}
\end{equation}
where the equality $(\dag)$ follows from the gradient step, i.e., $\bx^\toptone_{\sS^{t+1}} = \bx^\toptzero_{\sS^{t+1}} - \frac{\gamma}{\beta} \bg^\toptzero_{\sS^{t+1}}$, the inequality $(\ddag)$ follows using the fact that $\bx^\toptone$ is obtained using hard-thresholding ($\sS^{t+1}$ contains the indices for largest $\abs{\sS^{t+1}}=\abs{\sS^{t}}$ magnitudes) and the fact that $\abs{\sS^t \setminus \sS^{t+1}} = \abs{\sS^{t+1} \setminus \sS^t}$, such that 
$
\normtwo{\bx_{\sS^t \setminus \sS^{t+1}}^\toptzero - \frac{\gamma}{\beta} \bg_{\sS^t \setminus \sS^{t+1}}^\toptzero}^2 
\leq 
\normtwo{\bx_{\sS^{t+1} \setminus \sS^t}^\toptone}^2 
= \frac{\gamma^2}{\beta^2} \normtwo{\bg_{\sS^{t+1} \setminus \sS^t}^\toptzero}^2,
$
and the equality $(*)$ follows from $\normtwo{\bg_{\sS^{t+1}}^\toptzero}^2 = \normtwo{\bg_{\sS^{t+1} \setminus \sS^t}^\toptzero}^2 + \normtwo{\bg_{\sS^t \cap \sS^{t+1}}^\toptzero}^2$.
Combining \eqref{equation:conv_iht_genfunc1} and \eqref{equation:conv_iht_genfunc2} yields
\begin{equation}\label{equation:conv_iht_genfunc3}
\small
\begin{aligned}
&f(\bx^\toptone) - f(\bx^\toptzero) \leq \frac{\beta}{2} \normtwo{\bx_{\sI}^\toptone - \bx_{\sI}^\toptzero + \frac{\gamma}{\beta} \bg_{\sI}^\toptzero}^2 - \frac{\gamma^2}{2\beta} \normtwo{\bg_{\sI}^\toptzero}^2 - \frac{\gamma(1-\gamma)}{2\beta} \normtwo{\bg_{\sS^t \cup \sS^{t+1}}^\toptzero}^2 \\
&= \frac{\beta}{2} \normtwo{\bx_{\sI}^\toptone - \bx_{\sI}^\toptzero + \frac{\gamma}{\beta} \bg_{\sI}^\toptzero}^2 
- \frac{\gamma^2}{2\beta} \normtwo{\bg_{\sI \setminus (\sS^t \cup \sS^*)}^\toptzero}^2 
- \frac{\gamma^2}{2\beta} \normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero}^2
- \frac{\gamma(1-\gamma)}{2\beta} \normtwo{\bg_{\sS^t \cup \sS^{t+1}}^\toptzero}^2.
\end{aligned}
\end{equation}
Next, we will upper bound the first two terms on the right-hand side  of the above inequality. Since $\sI \setminus (\sS^t \cup \sS^*) = \sS^{t+1} \setminus (\sS^t \cup \sS^*) \subseteq \sS^{t+1}$, we have $\bx_{\sI \setminus (\sS^t \cup \sS^*)}^\toptone = \bx_{\sI \setminus (\sS^t \cup \sS^*)}^\toptzero - \frac{\gamma}{\beta} \bg_{\sI \setminus (\sS^t \cup \sS^*)}^\toptzero$. 
However, since $\bx_{\sI \setminus \sS^t}^\toptzero = \bzero$, the preceding equality reduces to $\bx_{\sI \setminus (\sS^t \cup \sS^*)}^\toptone = -\frac{\gamma}{\beta} \bg_{\sI \setminus (\sS^t \cup \sS^*)}^\toptzero$. 
Let $\sT \subseteq \sS^t \setminus \sS^{t+1}$ such that $\abs{\sT} = \abs{\sS^{t+1} \setminus (\sS^t \cup \sS^*)}$. Such a choice is possible since $\abs{\sS^{t+1} \setminus (\sS^t \cup \sS^*)} = \abs{\sS^t \setminus \sS^{t+1}} - \abs{(\sS^{t+1} \cap \sS^*) \setminus \sS^t}$ (which itself is a consequence of the fact that $\abs{\sS^{t+1}} = \abs{\sS^t}$ such that $\abs{\sS^t \setminus \sS^{t+1}} = \abs{\sS^{t+1} \setminus \sS^t}$). Moreover, since $\bx^\toptone$ is obtained by hard-thresholding $\left(\bx^\toptzero - \frac{\gamma}{\beta} \bg^\toptzero\right)$, for any choice of $\sT$ made above, we have:
\begin{equation}
\frac{\gamma^2}{\beta^2} \normtwo{\bg_{\sS^{t+1} \setminus (\sS^t \cup \sS^*)}^\toptzero}^2 
= \normtwo{\bx_{\sS^{t+1} \setminus (\sS^t \cup \sS^*)}^\toptone}^2 \geq \normtwo{\bx_\sT^\toptzero - \frac{\gamma}{\beta} \bg_\sT^\toptzero}^2.
\end{equation}
Using the above equation,  the fact that $\bx_\sT^\toptone = \bzero$ (since $\sT \nsubseteq \sS^{t+1}$), and the fact that $\sI \setminus (\sS^t \cup \sS^*) = \sS^{t+1} \setminus (\sS^t \cup \sS^*)$, the first two terms of equality~\eqref{equation:conv_iht_genfunc3} becomes 
\begin{equation}\label{equation:conv_iht_genfunc4}
\small
\begin{aligned}
&\quad \frac{\beta}{2} \normtwo{\bx_{\sI}^\toptone - \bx_{\sI}^\toptzero + \frac{\gamma}{\beta} \bg_{\sI}^\toptzero}^2 - \frac{\gamma^2}{2\beta} \normtwo{\bg_{\sI \setminus (\sS^t \cup \sS^*)}^\toptzero}^2 \\
&\leq \frac{\beta}{2} \normtwo{\bx_{\sI}^\toptone - \bx_{\sI}^\toptzero + \frac{\gamma}{\beta} \bg_{\sI}^\toptzero}^2 - \frac{\beta}{2} \normtwo{\bx_\sT^\toptone - \bx_\sT^\toptzero + \frac{\gamma}{\beta} \bg_\sT^\toptzero}^2 \\
&= \frac{\beta}{2} \normtwo{\bx_{\sI \setminus \sT}^\toptone - \bx_{\sI \setminus \sT}^\toptzero + \frac{\gamma}{\beta} \bg_{\sI \setminus \sT}^\toptzero}^2.
\end{aligned}
\end{equation}
Since we construct the set $\sT$ such that $\sT \subseteq \sS^t \setminus \sS^{t+1}$ satisfying $\abs{\sT} = \abs{\sS^{t+1} \setminus (\sS^t \cup \sS^*)} = \abs{\sI \setminus (\sS^t \cup \sS^*)}$, we can bound the size of $\sI \setminus \sT$ as $\abs{\sI \setminus \sT} \leq \abs{\sS^{t}} + \abs{(\sS^t \setminus \sS^{t+1}) \setminus \sT} + \abs{\sS^*} \leq s + \abs{(\sS^{t+1} \cap \sS^*) \setminus \sS^t} + s^* \leq s + 2s^*$. Also, since $\sS^{t+1} \subseteq (\sI \setminus \sT)$, we have $\bx_{\sI \setminus \sT}^\toptone = \project_{\sB_0[s]} \left(\bx_{\sI \setminus \sT}^\toptzero - \frac{\gamma}{\beta} \bg_{\sI \setminus \sT}^\toptzero\right)$.

Given the above observation with \eqref{equation:conv_iht_genfunc4} and the assumption that $s \geq 32 \left(\frac{\beta}{\alpha}\right)^2 s^*$, Lemma~\ref{lemma:spar_proj} is valid with $\widetildebx \triangleq \bx_{\sI \setminus \sT}^\toptone + \frac{\gamma}{\beta} \bg_{\sI \setminus \sT}^\toptzero$ and $\bx \triangleq\bx_{\sI \setminus \sT}^\toptzero $, whence we reduce the first two terms of equality~\eqref{equation:conv_iht_genfunc3} to
\begin{equation}\label{equation:conv_iht_genfunc5}
\small
\begin{aligned}
&\frac{\beta}{2} \normtwo{\bx_{\sI}^\toptone - \bx_{\sI}^\toptzero + \frac{\gamma}{\beta} \bg_{\sI}^\toptzero}^2 - \frac{\gamma^2}{2\beta} \normtwo{\bg_{\sI \setminus (\sS^t \cup \sS^*)}^\toptzero}^2 
\leq \frac{\beta}{2} \cdot \frac{\abs{\sI \setminus \sT} - s}{\abs{\sI \setminus \sT} - s^*} \normtwo{\bx_{\sI \setminus \sT}^* - \bx_{\sI \setminus \sT}^\toptzero + \frac{\gamma}{\beta} \bg_{\sI \setminus \sT}^\toptzero}^2 \\
&\overset{\dag}{\leq} \frac{\beta}{2} \cdot \frac{2s^*}{s + s^*} \normtwo{\bx_{\sI}^* - \bx_{\sI}^\toptzero + \frac{\gamma}{\beta} \bg_{\sI}^\toptzero}^2 
= \frac{2s^*}{s + s^*} \cdot \left(\gamma \langle \bx^* - \bx^\toptzero, \bg^\toptzero \rangle + \frac{\beta}{2} \normtwo{\bx^* - \bx^\toptzero}^2 + \frac{\gamma^2}{2\beta} \normtwo{\bg_{\sI}^\toptzero}^2\right) \\
&\overset{\ddag}{\leq} \frac{2s^*}{s + s^*} \cdot \left(\gamma f(\bx^*) - \gamma f(\bx^\toptzero) + \frac{\beta - \gamma \alpha}{2} \normtwo{\bx^* - \bx^\toptzero}^2 + \frac{\gamma^2}{2\beta} \normtwo{\bg_{\sI}^\toptzero}^2\right),
\end{aligned}
\end{equation}
where the inequality $(\dag)$ follows from $|\sI \setminus \sT| \leq s + 2s^*$ as shown earlier and the observation that $\frac{z-a}{z-b}$ is a positive and increasing function on the interval $z \geq a$ if $a \geq b \geq 0$. Note that since we already have $\sS^{t+1} \subseteq (\sI \setminus \sT)$, we get $|\sI \setminus \sT| \geq s$. The inequality $(\ddag)$ follows by the RSC property of functions (Definition~\ref{definition:res_scss_func}).

Plugging \eqref{equation:conv_iht_genfunc5} into the first two terms of equality~\eqref{equation:conv_iht_genfunc3}, and using the fact that $\sS^{t+1} \setminus (\sS^t \cup \sS^*) \subseteq (\sS^{t+1} \cup \sS^t )$, we get:
\begin{equation}
\small
\begin{aligned}
f(\bx^\toptone) - f(\bx^\toptzero) 
&\leq \frac{2s^*}{s + s^*} \cdot \left( \gamma f(\bx^*) - \gamma f(\bx^\toptzero) + \frac{\beta - \gamma \alpha}{2} \normtwo{\bx^* - \bx^\toptzero}^2 + \frac{\gamma^2}{2\beta} \normtwo{\bg_\sI^\toptzero}^2 \right) \\
&\quad - \frac{\gamma^2}{2\beta} \normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero}^2 - \frac{\gamma(1-\gamma)}{2\beta} \normtwo{\bg_{\sS^{t+1} \setminus (\sS^t \cup \sS^*)}^\toptzero}^2.
\end{aligned}
\end{equation}
Since $\gamma = 2/3$ and $s \geq 32 \left( \frac{\beta}{\alpha} \right)^2 s^*$, so that we have $\frac{2s^*}{s + s^*} \leq \frac{\alpha^2}{16\beta(\beta - \gamma \alpha)}$. Since $\beta \geq \alpha$, we also have $\frac{\alpha^2}{16\beta(\beta - \gamma \alpha)} \leq \frac{3}{16}$. Using these inequalities, rearrange the terms in \eqref{equation:conv_iht_genfunc5} above:
\begin{equation}
\begin{aligned}
f(\bx^\toptone) - f(\bx^\toptzero) &\leq \frac{2s^*}{s + s^*} \cdot \gamma \cdot \left( f(\bx^*) - f(\bx^\toptzero) \right) + \frac{\alpha^2}{32\beta} \normtwo{\bx^* - \bx^\toptzero}^2 + \frac{1}{24\beta} \normtwo{\bg_\sI^\toptzero}^2 \\
&\quad - \frac{2}{9\beta} \normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero}^2 - \frac{1}{9\beta} \normtwo{\bg_{\sS^{t+1} \setminus (\sS^t \cup \sS^*)}^\toptzero}^2.
\end{aligned}
\end{equation}
Decomposing $\normtwo{\bg_\sI^\toptzero}^2 = \normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero}^2 + \normtwo{\bg_{\sS^{t+1} \setminus (\sS^t \cup \sS^*)}^\toptzero}^2$ implies
\begin{equation}\label{equation:conv_iht_genfunc6}
\small
\begin{aligned}
f(\bx^\toptone) - f(\bx^\toptzero) &\leq \frac{2s^*}{s + s^*} \cdot \gamma \cdot \left( f(\bx^*) - f(\bx^\toptzero) \right) - \frac{1}{2\beta} \left( \frac{13}{36} \normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero}^2 - \frac{\alpha^2}{16} \normtwo{\bx^* - \bx^\toptzero}^2 \right) \\
&\quad - \frac{1}{2\beta} \cdot \left( \frac{4}{9} - \frac{1}{12} \right) \normtwo{\bg_{\sS^{t+1} \setminus (\sS^t \cup \sS^*)}^\toptzero}^2 \\
&\leq \frac{2s^*}{s + s^*} \cdot \gamma \cdot \left( f(\bx^*) - f(\bx^\toptzero) \right) - \frac{13}{72\beta} \left( \normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero}^2 - \frac{\alpha^2}{4} \normtwo{\bx^* - \bx^\toptzero}^2 \right). \\
%&\leq \frac{2s^*}{s + s^*} \cdot \gamma \cdot \left( f(\bx^*) - f(\bx^\toptzero) \right) - \frac{\alpha}{12\beta} \left( f(\bx^\toptzero) - f(\bx^*) \right),
\end{aligned}
\end{equation}
Using the RSC property, we have:
$$
\small
\begin{aligned}
f(\bx^\toptzero) - f(\bx^*) 
&\leq \innerproduct{\bg^\toptzero, \bx^\toptzero - \bx^*}- \frac{\alpha}{2} \normtwo{\bx^* - \bx^\toptzero}^2 
= \innerproduct{\bg^\toptzero_{\sS^t \cup \sS^*}, \bx^\toptzero_{\sS^t \cup \sS^*} - \bx^*_{\sS^t \cup \sS^*}} - \frac{\alpha}{2} \normtwo{\bx^* - \bx^\toptzero}^2 \\
&\leq \normtwo{\bg^\toptzero_{\sS^t \cup \sS^*}} \normtwo{\bx^\toptzero - \bx^*} - \frac{\alpha}{2} \normtwo{\bx^* - \bx^\toptzero}^2.
\end{aligned}
$$
This shows 
$$
\small
\begin{aligned}
\normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero}^2 &- \frac{\alpha^2}{4} \normtwo{\bx^* - \bx^\toptzero}^2
= \left( \normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero} - \frac{\alpha}{2} {\normtwo{\bx^* - \bx^\toptzero }} \right) \left( \normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero} + \frac{\alpha}{2} \normtwo{\bx^* - \bx^\toptzero} \right) \\
&\geq \frac{(f(\bx^\toptzero) - f(\bx^*))}{\normtwo{\bx^\toptzero - \bx^*}} \cdot \left( \normtwo{\bg_{\sS^t \cup \sS^*}^\toptzero} + \frac{\alpha}{2} \normtwo{\bx^* - \bx^\toptzero} \right)
\geq \frac{\alpha}{2} \cdot (f(\bx^\toptzero) - f(\bx^*)).
\end{aligned}
$$
Plugging this inequality into \eqref{equation:conv_iht_genfunc6} yields 
\begin{equation}\label{equation:conv_iht_genfunc7}
\small
\begin{aligned}
f(\bx^\toptone) - f(\bx^\toptzero) 
&\leq \frac{2s^*}{s + s^*} \cdot \gamma \cdot \left( f(\bx^*) - f(\bx^\toptzero) \right) - \frac{\alpha}{12\beta} \left( f(\bx^\toptzero) - f(\bx^*) \right)\\
&=-\left(\frac{2s^*}{s + s^*} \cdot \gamma  + \frac{\alpha}{12\beta}\right) \left(  f(\bx^\toptzero) - f(\bx^*)  \right) .
\end{aligned}
\end{equation}
The result  follows by observing that $\frac{2s^*}{s + s^*} \geq 0$.
\end{proof}


\index{Convex relaxqtion}
\index{LASSO}
\index{Constrained LASSO}
\index{Lagrangian LASSO}
\section{Sparse Optimization Problem: $\ell_1$ Relaxation}
From the above sparse optimization analysis, although under very specific constraints, the convergence of the sparse optimization using IHT can be guaranteed,  we may find sparse optimization using the $\ell_0$ norm (or hard-thresholding), which counts the number of nonzero elements in a vector, poses several significant challenges and drawbacks:
\begin{itemize}
\item Non-convexity: The  $\ell_0$ norm is inherently non-convex. This means that optimization problems involving the  $\ell_0$ norm are typically NP-hard. Finding the global minimum for such problems can be computationally prohibitive, especially as the dimensionality of the problem increases.
\item 	Combinatorial nature: Minimizing the  $\ell_0$ norm involves searching through all possible combinations of the elements to find the subset that provides the sparsest solution while satisfying the constraints of the problem. This combinatorial search is impractical for high-dimensional data sets due to its exponential complexity.
\item Instability: Solutions obtained by directly minimizing the  $\ell_0$ norm can be highly sensitive to small changes in the data. This instability makes it difficult to achieve robust solutions, particularly in noisy environments or when dealing with real-world data that may contain errors or outliers.
\item Lack of continuity and differentiability: The  $\ell_0$ norm is neither continuous nor differentiable, making it incompatible with many standard optimization techniques that rely on gradient information. This lack of smoothness complicates the application of gradient-based methods and other iterative algorithms designed for continuous functions.
\item Difficulty in incorporating into optimization algorithms: Due to its discrete nature, incorporating the  $\ell_0$ norm directly into optimization frameworks is challenging. Many algorithms require some form of relaxation or approximation (e.g., using the  $\ell_1$ norm as a convex surrogate) to make the problem tractable.
\end{itemize}
Similar to the $\ell_0$ norm, the $\ell_1$ norm encourages solutions where many coefficients are exactly zero. This is due to its geometry; in high dimensions, the corners of the  $\ell_1$ ball (a polytope) align with axes, promoting sparse solutions (see Figure~\ref{fig:p-norm-comparison-3d}).
The  $\ell_1$ norm is convex, meaning that any local minimum found will also be a global minimum. This property significantly simplifies optimization problems compared to using the non-convex $\ell_0$  norm.

To be more specific, consider the standard linear regression model in matrix-vector form
\begin{equation}
\bb = \bA \bx^* + \bepsilon,
\end{equation}
where $\bA \in \real^{m\times n}$ is the model (design) matrix, $\bepsilon \in \real^m$ is a vector of noise variables, and $\bx^* \in \real^n$ is the unknown coefficient vector. By employing convex relaxation via $\ell_1$ norms,  this section develops practical algorithms and theoretical guarantees for both the \textit{constrained form} of the \textit{LASSO (least
absolute selection and shrinkage operator)} and its Lagrangian version:
\begin{subequations}\label{equation:lassos_all}
\begin{align}
(\textbf{Constrained LASSO}):\qquad &\mathopmin{\normone{\bx}\leq R}f(\bx)\triangleq \frac{1}{2m}\normtwo{\bb - \bA \bx}^2; \label{equation:loss_cons_lasso}\\
(\textbf{Lagrangian LASSO}):\qquad &\mathopmin{\bx \in \real^n}F(\bx)\triangleq \left\{ \frac{1}{2m} \normtwo{\bb - \bA \bx}^2 + \lambda_m \normone{\bx} \right\}. \label{equation:loss_lagrang_lasso}
\end{align}
\end{subequations}
Through Lagrangian duality, there exists a correspondence between these two families of quadratic programs, where  $\lambda_m$ can be interpreted as the Lagrange multiplier associated with the constraint $\normone{\bx} \leq R$ (see Section~\ref{section:geom_int_regu} for a geometrical interpretation for this kind of correspondence).



\subsection{Sparse Optimization Algorithms}

The original form of the Lagrangian LASSO is:
\begin{equation}\label{equation:lag_lasso_alg}
\min_{\bx\in \real^n} \quad F(\bx) \triangleq f(\bx) +g(\bx)\triangleq \frac{1}{2m} \normtwo{\bb - \bA \bx}^2 + \lambda_m \normone{\bx},
\end{equation}
where $g(\bx)\triangleq\lambda_m \normone{\bx}$. Since $f$ and $g$ are both convex (Exercise~\ref{exercise:conv_quad}), one might consider using the gradient descent method (Algorithm~\ref{alg:gd_gen}) as a straightforward solution. 
However, it is observed that  the objective function $F(\bx)$ of the LASSO problem is not smooth at certain points, and thus the gradient cannot be directly obtained for the original problem. 
To address this issue, one can employ subgradient methods instead.
Although $g(\bx)$ is Lipschitz and $f(\bx)$ is Lipschitz continuously differentiable ($f$ is $\frac{1}{m}\normtwo{\bA^\top\bA}$-smooth; Example~\ref{example:lipschitz_spar}), their sum does not yield any of these properties.
Therefore, the convergence is not guaranteed.

\subsection*{Smoothing with Huber Loss}
An alternative approach involves recognizing that the nonsmooth term in the objective function,  $\normone{\bx}$, consists of the sum of absolute values of each component of  $\bx$. If a smooth function could approximate the absolute value function, then gradient-based methods could be applied to solve the LASSO problem. To achieve this, we utilize the Huber loss function (refer to Section~\ref{section:huber_estima}):
\begin{equation}
h_\delta(x) = 
\begin{cases} 
	\frac{1}{2\delta}x^2, & \abs{x} < \delta, \\
	\abs{x} - \frac{\delta}{2}, & \text{otherwise}.
\end{cases}
\end{equation}
When $\delta \to 0$, the smooth function $h_\delta(x)$ approaches the absolute value function $\abs{x}$. Figure~\ref{fig:huber} illustrates  the graph of $h_\delta(x)$ for various  values of $\delta$.

By substituting the absolute value terms with the Huber loss, the smoothed LASSO problem can be formulated as:
\begin{equation}
	\min_{\bx\in\real^n} \quad F_\delta(\bx) = \frac{1}{2m}\normtwo{\bA\bx-\bb}^2 + \lambda_m H_\delta(\bx), 
	\quad \text{with }H_\delta(\bx) \triangleq \sum_{i=1}^{n} h_\delta(x_i),
\end{equation}
where $\delta$ is the given smoothing parameter. The gradient of $F_\delta(\bx)$ can be computed as
$ \nabla F_\delta(\bx) = \frac{1}{m}\bA^\top(\bA\bx - \bb) + \lambda_m \nabla H_\delta(\bx), $
where $\nabla H_\delta(\bx)$ is defined component-wise by:
$ (\nabla H_\delta(\bx))_i = 
\footnotesize
\begin{cases} 
	\sign(x_i), & \abs{x_i} > \delta; \\
	{x_i}/{\delta}, & \abs{x_i} \leq \delta.
\end{cases}
$
Given that the gradient of $F_\delta(\bx)$ is convex and Lipschitz continuously differentiable with a constant $\beta = \frac{1}{m}\normtwo{\bA^\top \bA} + \frac{\lambda_m}{\delta}$ (see Example~\ref{example:lipschitz_spar} and Exercise~\ref{exercise:sum_sc_conv}, i.e., $\beta$-smooth), according to Theorem~\ref{theorem:pgd_smooth}, the gradient descent method with a constant stepsize $\eta=\frac{1}{\beta}$ can guarantee the convergence of the algorithm.
However, if $\delta$ is very small, it is crucial to select sufficiently small stepsizes $\eta$ to ensure the convergence of the gradient method.





\subsection*{Proximal Gradient Method and FISTA}
Given that both $f$ and $g$ functions  in the LASSO problem \eqref{equation:lag_lasso_alg} are convex, where $f$ is differentiable and $g$ is non-differentiable, problem \eqref{equation:lag_lasso_alg} can be considered  a composite model. 
We have introduced proximal gradient, generalized conditional gradient, and generalized mirror descent methods to address such composite models (Chapter~\ref{chapter:gd_convg}). 
Recalling that the latter two algorithms both involve an optimization of  $\mathop{\argmin}_{\bx\in {\real^n}} \innerproduct{\nabla f(\bx^{(t)}), \bx} + g(\bx)$ at each step, which is the same as the original LASSO problem \eqref{equation:lag_lasso_alg} since $\nabla f(\bx) = \frac{1}{m}\bA^\top (\bA\bx - \bb)$.
Therefore, the most suitable choice is the proximal gradient method.

Following Algorithm~\ref{alg:prox_gd_gen}, the update rule for the proximal gradient method applied to the LASSO problem \eqref{equation:lag_lasso_alg} at the $t$-th iteration is:
\begin{subequations}
\begin{align}
\by^\toptone  &\leftarrow \bx^\toptzero - \eta_t \frac{1}{m}\bA^\top (\bA\bx^\toptzero - \bb);\\
\bx^\toptone &\leftarrow \prox_{\eta_t g}(\by^\toptone)= \mathcalT_{\eta_t\lambda_m} (\by^\toptone)= [|\by^\toptone| - \eta_t \lambda_m \bone]_+ \hadaprod \sign(\by^\toptone),
\end{align}
\end{subequations}
where $\mathcalT_\lambda(\cdot)$ denotes the soft thresholding function (Example~\ref{example:soft_thres}).
That is, the first step performs a gradient descent, and the second step performs a shrinkage, ensuring sparsity in the solution during iterations. 
This explains why the proximal gradient algorithm is effective.
Convergence is guaranteed by Theorem~\ref{theorem:prox_conv_ss_cvx} with a constant stepsize $\eta_t = \frac{m}{\normtwo{\bA^\top\bA}}$ since $f$ is $\frac{1}{m}\normtwo{\bA^\top\bA}$-smooth (Example~\ref{example:lipschitz_spar}).

Additionally, noting that the FISTA method (Algorithm~\ref{alg:fistav1} and Lemma~\ref{lemma:seq_gamma_zeta}) is a minor  variant of the proximal gradient method. 
Thus, the update steps for FISTA applied to the LASSO problem are:
\begin{subequations}
\begin{align}
\by^\toptone &\leftarrow \mathcalT_{{\eta_t\lambda_m}} \left( \bx^\toptzero - \eta_t \frac{1}{m} \bA^\top (\bA \bx^\toptzero - \bb) \right);\\
\zeta_{t+1} &\leftarrow \frac{1 + \sqrt{1 + 4 \zeta_t^2}}{2};\\
\bx^\toptone &\leftarrow \by^\toptone + \left( \frac{\zeta_t - 1}{\zeta_{t+1}} \right) (\by^\toptone - \by^\toptzero).
\end{align}
\end{subequations}






\index{Penalty function}
\index{Basis pursuit}
\subsection*{Penalty Function Method}
Recall the Lagrangian LASSO problem:
$$
\min_{\bx\in\real^n}  \quad \frac{1}{2m} \normtwo{\bA\bx-\bb}^2 + \lambda_m \normone{\bx},
$$
where $\lambda_m > 0$ is a regularization parameter. Solving the LASSO problem ultimately aims to solve the following \textit{basis pursuit (BP)} problem:
$$
\begin{aligned}
& \min_{\bx\in\real^n}  \quad \normone{\bx}\quad \text{s.t.} \quad \bA\bx = \bb,
\end{aligned}
$$
The BP problem involves a non-smooth optimization problem with equality constraints. By applying a quadratic penalty function to the equality constraint $\bA\bx = \bb$ (Section~\ref{section:pen_func_me}), we obtain:
$$
\min_{\bx\in\real^n}  \quad \normone{\bx} + \frac{\sigma}{2} \normtwo{\bA\bx-\bb}^2.
$$
Let $m \lambda_m \triangleq \frac{1}{\sigma}$, it becomes clear that using $\frac{1}{\lambda_m}$ as the quadratic penalty factor makes the penalty function subproblem of the BP problem equivalent to the LASSO problem. This observation highlights two points:
\begin{itemize}
\item The solutions of the LASSO and BP problems are not identical; however, as $\lambda_m$ approaches zero, the solution of the LASSO problem converges to the solution of the BP problem. 
\item When $m\lambda_m$ is relatively small, based on previous discussions, the penalty function of the BP problem becomes ill-conditioned (Section~\ref{section:pen_equa}), potentially leading to slow convergence if solved directly.
\end{itemize}
According to the penalty function approach, the penalty factor should gradually increase towards infinity, analogous to initially setting a larger 
$\lambda_m$ in the LASSO problem and then continuously reducing it until reaching the desired solution. The specific algorithm is detailed in Algorithm~\ref{alg:lass_penal}.

\begin{algorithm}[h]
\caption{Lagrangian LASSO Problem  via Penalty Function Method}
\label{alg:lass_penal}
\begin{algorithmic}[1]
\State {\bfseries Input:}  Given initial value $\bx^{(1)}$, regularized parameter $\lambda_m$, initial parameter $\zeta_1\gg \lambda_m$, reducing factor $\gamma \in (0, 1)$.
%\While{$\zeta_t \geqslant \lambda_m$}
\For{$t=1,2,\ldots$}
\State \algoalign{Using $\bx^\toptzero$ from last iteration as the initial guess, solve the problem $\bx^\toptone = \argmin_{\bx} \left\{ \frac{1}{2m} \normtwo{\bA\bx-\bb}^2 + \zeta_t \normone{\bx} \right\}$ by subgradient descent method or other unconstrained optimization method;}
\If{$\zeta_t = \lambda_m$}
\State Stop iteration, output $\bx^\toptone$;
\Else
\State Update/reducing the penalty factor $\zeta_{t+1} = \max\{\lambda_m, \gamma \zeta_t\}$;
\EndIf
\EndFor
\State {\bfseries Return:} final $\bx\leftarrow \bx^\toptzero $;
\end{algorithmic}
\end{algorithm}





\index{ADMM}
\subsection*{ADMM}
Note that the Lagrangian LASSO problem \eqref{equation:lag_lasso_alg} can be equivalently expressed using an auxiliary variable as follows:
$$
\min_{\bx\in\real^n} F(\bx)\triangleq f(\bx)+g(\by) 
\triangleq   
\frac{1}{2m} \normtwo{\bA\bx-\bb}^2 + \lambda_m \normone{\by} \quad \text{ s.t. }\bx=\by.
$$
This formulation is in the standard form suitable for ADMM (Section~\ref{section:admm_all}). 
Given a smoothing parameter $\sigma>0$, the updates at the $t$-th  iteration are:
$$
\begin{aligned}
\bx^\toptone 
&\leftarrow \argmin_{\bx} \left\{ \frac{1}{2m} \normtwo{\bA\bx-\bb}^2 + \frac{\sigma}{2} \normtwo{\bx - \by^\toptzero + \frac{1}{\sigma} \blambda^\toptzero}^2 \right\}\\
&= \Big(\frac{1}{m}\bA^\top \bA + \sigma \bI\Big)^{-1} \Big(\frac{1}{m}\bA^\top \bb + \sigma \by^\toptzero - \blambda^\toptzero\Big);\\
\by^\toptone 
&\leftarrow \argmin_{\by} \left\{ \lambda_m \normone{\by} + \frac{\sigma}{2} \normtwo{\bx^\toptone - \by + \frac{1}{\sigma} \blambda^\toptzero}^2 \right\}\\
&= \prox_{(\lambda_m/\sigma) \normone{\cdot}} \left( \bx^\toptone + \frac{1}{\sigma} \blambda^\toptzero \right) 
=\mathcalT_{(\lambda_m/\sigma)}(\bx);\\
\blambda^\toptone 
&\leftarrow \blambda^\toptzero +  \sigma (\bx^\toptone - \by^\toptone),
\end{aligned}
$$
where $\mathcalT_\lambda(\cdot)$ denotes the soft thresholding function (Example~\ref{example:soft_thres}).

Since $ \sigma > 0 $, $ \bA^\top \bA + \sigma \bI $ is always invertible. The update for $ \bx $  essentially solves  a ridge regression problem (an $ \ell_2 $ norm squared regularization least squares problem); while the update for $ \by $ involves  an $ \ell_1 $ norm proximal operator, which also has an explicit solution. 
When solving the $ \bx $ iteration, if a fixed penalty parameter $ \sigma $ is used, we can precompute the decomposition of the matrix $ \bA^\top \bA + \sigma \bI $. This allows us to cache this initial decomposition, thereby reducing the computational load in subsequent iterations.



\subsection*{Block Coordinate Descent}



We briefly introduce how to solve the LASSO problem using the \textit{block coordinate descent (BCD)} method.
Since the  $\normone{\bx}$ term in the objective function is separable, each block variable corresponds to an individual component of  $\bx$. For convenience, when updating the $i$-th block, we represent $\bx$ as:
$$
\bx = 
\begin{bmatrix}
x_i \\
\bx_{-i}
\end{bmatrix}
$$
where $x_i$ denotes the $i$-th component of $\bx$, and $\bx_{-i}$ represents the vector $\bx$ excluding its $i$-th component. 
Similarly, for matrix $\bA$, during the $i$-th block update, it can be written as:
$$
\bA = \begin{bmatrix}
\ba_i & \bA_{-i}
\end{bmatrix},
$$
where $\bA_{-i}$ is the matrix formed by removing the $i$-th column from $\bA$. 
This reordering of components in $\bx$ and columns in $\bA$ maintains the equivalence of the modified problem to the original one.

Subsequently, during the update of the $i$-th block, the optimization problem becomes:
$$
\min_{x_i} \quad \lambda_m \abs{x_i} + \lambda_m \normone{\bx_{-i}} + \frac{1}{2m} \normtwo{\ba_i x_i - (\bb - \bA_{-i} \bx_{-i})}^2.
$$
Let $ \bc_i \triangleq \bb - \bA_{-i} \bx_{-i} $, noting that terms involving only $\bx_{-i}$ are constants. The problem is thus equivalent to:
\begin{equation}\label{equation:bcd_lasso}
\min_{x_i} \quad f_i(x_i) \triangleq \lambda_m \abs{x_i} + \frac{1}{2m} \normtwo{\ba_i}^2 x_i^2 - \ba_i^\top \bc_i x_i.
\end{equation}
The minimizer for \eqref{equation:bcd_lasso} can be directly obtained as:
\begin{equation}\label{equation:bcd_lasso_upd}
x_i^\toptzero \leftarrow \argmin_{x_i} f_i(x_i) = 
\begin{cases}
m\cdot	\frac{\ba_i^\top \bc_i - \lambda_m}{\normtwo{\ba_i}^2}, & \ba_i^\top \bc_i > \lambda_m, \\
m\cdot	\frac{\ba_i^\top \bc_i + \lambda_m}{\normtwo{\ba_i}^2}, & \ba_i^\top \bc_i < -\lambda_m, \\
	0, & \text{otherwise}.
\end{cases}
\end{equation}
Therefore, the block coordinate descent method for solving the LASSO problem can be summarized in Algorithm~\ref{alg:bcd_lasso}.

\begin{algorithm}[H]
\caption{Block Coordinate Descent Method for LASSO}
\label{alg:bcd_lasso}
\begin{algorithmic}[1]
\State {\bfseries Input:}   $ \bA, \bb $, parameter $\lambda_m$. Initialize $ \bx^\topone $;
\For{$t=1,2,\ldots$}
\For{$i = 1, 2, \ldots, n$}
\State Compute $\bx_{-i}, \bc_i$ according to definitions;
\State Compute $ x_i^\toptzero $ by \eqref{equation:bcd_lasso_upd};
\EndFor
\EndFor
\State {\bfseries Return:} final $\bx\leftarrow \bx^\toptzero$; 
\end{algorithmic}
\end{algorithm}


\index{Group LASSO}
\paragrapharrow{Group LASSO.}
When we believe that the columns of $\bA$ can be grouped into subsets $\sG$, where each subset represents a coherent group of columns, the goal of \textit{group LASSO} is to enforce sparsity at the group level rather than at the individual element level. Specifically, the union of all groups covers all columns of $\bA$, i.e.,  $\cup_{\scriptsize g\in\sG}g=\{1,2,\ldots,n\}$. 
To achieve this group-level sparsity, we can apply the  $\ell_1$ vector norm to a vector in $\real^{\abs{\sG}}$, where each component of this vector corresponds to the  $\ell_1$ or $\ell_2$ matrix norm of the columns within each group $g\in\sG$ (the $\ell_2$ matrix norm within each group does not promote sparsity on its own). This approach avoids direct penalization of individual elements and focuses on the aggregated norms of each group.
This method is particularly useful when the underlying structure of the data suggests that features or variables naturally form meaningful groups, allowing us to select entire groups of features rather than individual features.


To be more specific, let matrix $ \bA \in \real^{m\times n} $ and vector $ \bb \in \real^m $ be composed of $ m $  observations from the independent variables and response variables in the above model. The parameter $ \bx = [\bx_1; \bx_2; \ldots; \bx_G] \in \real^n $ can be divided into $ G = \abs{\sG} $ groups, and among $ \{\bx_i\}_{i=1}^G $, only a few are nonzero vectors. The optimization problem corresponding to the group LASSO can be formulated as:
$$
\min_{\bx\in\real^m}  \quad \frac{1}{2m} \normtwo{\bb - \bA\bx}^2 + \lambda_m \sum_{i=1}^G \sqrt{p_i} \normtwo{\bx_i}.
$$
In this formulation, there are $G$ blocks of variables to be optimized, with each block representing one group. Here, $p_i$ denotes the number of elements (or dimensions) in group $i$, and $\lambda_m$ controls the level of regularization applied to encourage sparsity at the group level. This setup allows for efficient selection of relevant feature groups while maintaining model simplicity and interpretability.

\subsection{Other Sparse Optimization Formulations}
\index{Generalized LASSO}

Apart from the standard LASSO problem, we also briefly introduce the generalized LASSO and sparse noise LASSO problems.
\subsection*{Generalized LASSO Problem}
The \textit{generalized LASSO problem} is defined as:
\begin{equation}\label{equation:gen_lasso}
\min_{\bx\in\real^n}  \quad  \frac{1}{2m} \normtwo{\bA\bx-\bb}^2 +\lambda_m \normone{\bB\bx},
\end{equation}
where $\bA\in\real^{m\times n}$, $\bB\in\real^{p\times n}$, and $\bb\in\real^m$.
In the LASSO problem, adding the $\normone{\bx}$ term ensures the sparsity of $\bx$. For many problems, $\bx$ itself may not be sparse, but it becomes sparse under certain transformations. An important example is when $\bB \in \real^{(n-1) \times n}$ is a first-order difference matrix:
$$
b_{ij} = 
\begin{cases} 
	1, & j = i + 1, \\
	-1, & j = i, \\
	0, & \text{otherwise},
\end{cases}
$$
and $\bA = \bI$, the generalized LASSO problem simplifies to:
$$
\min_{\bx\in\real^n}  \quad \frac{1}{2m} \normtwo{\bx - \bb}^2 + \lambda_m \sum_{i=1}^{n-1} \abs{x_{i+1} - x_i},
$$
This formulation is known as the \textit{total variation (TV)} model for image denoising; when $\bA = \bI$ and $\bB$ is a second-order difference matrix, problem \eqref{equation:gen_lasso} is known as \textit{trend filtering} \citep{rudin1992nonlinear, kim2009ell_1}~\footnote{See Problem~\ref{prob:denoise_rls} for its denoising effect.}. 

Below, we explain how to use ADMM to solve this problem.
To solve problem \eqref{equation:gen_lasso}, by introducing the constraint $\bB\bx = \by$, we rewrite the problem in a form suitable for ADMM:
$$
\min_{\bx, \by} \quad \frac{1}{2m} \normtwo{\bA\bx - \bb}^2 + \lambda_m \normone{\by}
\quad \text{s.t.} \quad \bB\bx - \by = \bzero.
$$
Introducing the Lagrange multiplier $\blambda\in\real^p$ (assume $\bB\in\real^{p\times n}$), the augmented Lagrangian function is:
$$
L_\sigma(\bx, \by, \blambda) = \frac{1}{2m} \normtwo{\bA\bx - \bb}^2 + \lambda_m \normone{\by} + \blambda^\top (\bB\bx - \by) + \frac{\sigma}{2} \normtwo{\bB\bx - \by}^2,
$$
where $\sigma$ is the penalty factor. 
Similar to the ADMM approach for a standard LASSO problem, the update for $\bx$   involves solving the linear system:
$$
\Big(\frac{1}{m}\bA^\top \bA + \sigma \bB^\top \bB\Big)\bx = \frac{1}{m}\bA^\top \bb + \sigma \bB^\top \Big( \by^\toptzero - \frac{1}{\sigma}\blambda^\toptzero \Big),
$$
while the $\by$ update is performed using the $\ell_1$ norm proximal operator. Therefore, the ADMM iterations are:
\begin{subequations}
\begin{align}
\bx^\toptone &\leftarrow \Big(\frac{1}{m}\bA^\top \bA + \sigma \bB^\top \bB\Big)^{-1} \Big( \frac{1}{m}\bA^\top \bb + \sigma \bB^\top \Big( \by^\toptzero - \frac{1}{\sigma}\blambda^\toptzero \Big) \Big);\\
\by^\toptone &\leftarrow \prox_{(\lambda_m/\sigma) \normone{\cdot}} \Big( \bB \bx^\toptone + \frac{1}{\sigma}\blambda^\toptzero \Big);\\
\blambda^{k+1} &\leftarrow \blambda^\toptzero +  \sigma (\bB\bx^\toptone - \by^\toptone).
\end{align}
\end{subequations}




\subsection*{Sparse Noise}
In the original LASSO problem \eqref{equation:lassos_all}, the noise term $\bb-\bA\bx$ is not sparse. 
The following problem considers a sparse noise term:
\begin{equation}\label{equation:spar_noi_lass}
\min_{\bx \in \real^n} \left\{ F(\bx) \triangleq \frac{1}{m} \normone{\bA \bx - \bb} + \lambda_m \normone{\bx} \right\},
\end{equation}
where $\bA \in \real^{m \times n}$, $\bb \in \real^m$, and $\lambda_m > 0$. We will consider two possible methods to solve the problem:

\paragrapharrow{Subgradient descent.} When applying the subgradient descent method to problem \eqref{equation:spar_noi_lass}, the method takes the form (choosing the subgradient of  $\normone{\by}$ as $\sign(\by)$; see Exercise~\ref{exercise:sub_norms}):
$$
\bx^\toptone \leftarrow \bx^\toptzero - \eta_t \big(\frac{1}{m}\bA^\top \sign(\bA \bx^\toptzero - \bb) + \lambda_m \sign(\bx)\big).
$$
The stepsize $\eta_t$ is chosen according to Theorem~\ref{theorem:pgd_lipschitz} as $\eta_t = \frac{1}{\normtwo{F'(\bx^\toptzero)} \sqrt{t+1}}$. That is, this method can be viewed as a projected subgradient descent with $\sS=\real^n$.
\paragrapharrow{Proximal subgradient.} For \eqref{equation:spar_noi_lass}, let $f(\bx) \triangleq \normone{\bA \bx - \bb}$ and $g(\bx) \triangleq \lambda_m \normone{ \bx}$, so that $F = f + g$. The proximal subgradient method then takes the form
$$
\bx^\toptone \leftarrow \prox_{\eta_t g}\big(\bx^\toptzero - \eta_t\frac{1}{m} \bA^\top \sign(\bA \bx^\toptzero - \bb)\big).
$$
Since $g(\bx) = \lambda_m \normone{\bx}$, it follows that $\prox_{\eta_t g}$ is a soft thresholding operator. Specifically, by Example~\ref{example:soft_thres}, $\prox_{\eta_t g} = \mathcal{T}_{\lambda_m \eta_t}$,  and thus the general update rule becomes
$$
\bx^\toptone \leftarrow \mathcal{T}_{\lambda_m \eta_t}\big(\bx^\toptzero - \eta_t\frac{1}{m} \bA^\top \sign(\bA \bx^\toptzero - \bb)\big).
$$
The stepsize $\eta_t$ can be chosen, for example, as $\eta_t = \frac{1}{\normtwo{f'(\bx^\toptzero)} \sqrt{t+1}}$.


\subsection{Theoretical Results for LASSO}
We conclude this section by providing several theoretical results for the LASSO problem. 
Given a LASSO estimate  $\widehatbx \in \real^n$, we can evaluate its quality in various ways. 
Depending on the application, different types of loss functions may be relevant.
In some scenarios, we are interested in the predictive performance of $\widehatbx$, leading us to compute a \textit{prediction loss function}:
\begin{subequations}
\begin{equation}
	\mathcalL_{\text{pred}}(\widehatbx; \bx^*) = \frac{1}{m} \normtwo{\bA \widehatbx - \bA \bx^*}^2,
\end{equation}
which represents the mean-squared error of $\widehatbx$ over the samples provided by $\bA$. 
In other applications---such as medical imaging, remote sensing, and compressed sensing---the primary interest lies in the unknown vector $\bx^*$ itself.
Therefore, it is more appropriate to consider loss functions such as the $\ell_2$ distance between the estimator and the true parameter:
\begin{equation}
	\mathcalL_2(\widehatbx; \bx^*) = \normtwo{\widehatbx - \bx^*}^2,
\end{equation}
which we refer to as the  \textit{parameter estimation loss}. 
Finally, if variable selection or support recovery is the goal, one might use the following loss function:
\begin{equation}
	\mathcalL_{\text{vs}}(\widehatbx; \bx^*) =
	\begin{cases}
		0 & \text{if } \sign(\widehatx_i) = \sign(x_i^*) \text{ for all } i \in \{1,2, \ldots, n\}, \\
		1 & \text{otherwise}.
	\end{cases}
\end{equation}
\end{subequations}
This evaluates whether the estimated vector $\widehatbx$ shares the same signed support as $\bx^*$. 

In this subsection, for brevity, we present theoretical results for the first two cases. To understand these results, we need the following technical lemma.
%\begin{lemma}
%Suppose that $\lambda_m \geq 2 \norminf{\frac{\bA^\top \bepsilon}{m}} > 0$. Then the error $\widehatbe \triangleq \widehatbx - \bx^*$ associated with any LASSO solution $\widehatbx$ belongs to the cone set $\sC[\sS; 3]$.
%\end{lemma}
%\begin{proof}Since $\frac{\norminf{\bA^\top \bepsilon}}{m} \leq \frac{\lambda_m}{2}$, \textcolor{red}{inequality~\eqref{equation:bound_lassl2error6}} implies that
%$$
%0 \leq \frac{\lambda_m}{2} \normone{\widehatbe} + \lambda_m \left\{ \normone{\widehatbe_{\sS}} - \normone{\widehatbe_{\comple{\sS}}} \right\};
%$$
%Rearranging and then dividing out by $\lambda_m > 0$ yields that $\normone{\widehatbe_{\comple{\sS}}} \leq 3 \normone{\widehatbe_{\sS}}$ as claimed.
%\end{proof}

\begin{lemma}\label{lemma:sparse_lem1}
Let $\bx, \by\in\real^n$  with $\normone{\bx}\leq \normone{\by}$, where $\supp(\by) \triangleq\sS\subseteq \{1,2,\ldots,n\}$.
Denote $\be \triangleq \bx-\by$.  Then $\normone{\be_{\comple{\sS}}} \leq \normone{\be_{\sS}}$, i.e., $\be \in \sC[\sS; 1]$, where $\comple{\sS}$ denotes the complement of $\sS$.
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:sparse_lem1}]
Let $\sT\triangleq\supp(\bx)$. The assumption  $\normone{\bx}\leq \normone{\by}$ indicates that 
$$
\begin{aligned}
&\sum_{i\in\sT} \abs{x_i} \leq  \sum_{i\in\sS} \abs{y_i} 
\quad\implies\quad 
\sum_{i\in\sT\setminus \sS} \abs{x_i}+ \sum_{i\in\sS \cap\sT} \abs{x_i}   \leq  \sum_{i\in\sS \setminus\sT} \abs{y_i} + \sum_{i\in\sS \cap\sT} \abs{y_i}\\
&\implies\quad  
\sum_{i\in\sT\setminus \sS} \abs{x_i}    \leq  \sum_{i\in\sS \setminus\sT} \abs{y_i} + \sum_{i\in\sS \cap\sT} \abs{y_i - x_i},
\end{aligned}
$$
which obtains the desired result.
\end{proof}

With this intuition in place, we now state a result that provides a bound on the parameter estimation error $\normtwo{\widehatbx - \bx^*}$ based on the linear observation model $\bb = \bA\bx^* + \bepsilon$, where $\bx^*$ is $k$-sparse and supported on the subset $\sS$.

\begin{theoremHigh}[Bounds of Optimizer Error for LASSO \citep{hastle2015statistical}]\label{theorem:bound_lassl2error}
Let the observations follow from the linear observation model $\bb = \bA\bx^* + \bepsilon$, where $\bA\in\real^{m\times n}$, and  $\bx^*$ is $k$-sparse, supported on the subset $\sS\subseteq \{1, 2,\ldots, n\}$ (i.e., $\abs{\sS}=k$).
Suppose that the model matrix $\bA$ satisfies the restricted eigenvalue (RE) property (Definition~\ref{definition:res_eig}) with parameter $\alpha > 0$ over $\sC[\sS;3]$.~\footnote{Note that the first result only requires the RE property holds over the subset $\sC[\sS;1]$.}
Then,
\begin{enumerate}[(i)]
\item \textit{Constrained LASSO.} Any estimate $\widehatbx$ based on the constrained LASSO \eqref{equation:loss_cons_lasso} with $\normone{\bx^*}$ satisfies the bound
$$
\normtwo{\widehatbx - \bx^*} \leq \frac{4\sqrt{k}}{\alpha m} \norminf{\bA^\top \bepsilon}.
$$

\item \textit{Lagrangian LASSO.} Given a regularization parameter $\lambda_m \geq \frac{2}{m} \norminf{\bA^\top \bepsilon} > 0$, any estimate $\widehatbx$ from the regularized LASSO \eqref{equation:loss_lagrang_lasso} satisfies the bound
$$
\normtwo{\widehatbx - \bx^*} \leq \frac{3}{\alpha} \sqrt{{k}}  \lambda_m.
$$
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:bound_lassl2error}]
\textbf{(i) Constrained LASSO.} Since $\bx^*$ is feasible and $\widehatbx$ is optimal, we have the inequality $\normtwo{\bb - \bA\widehatbx}^2 \leq \normtwo{\bb - \bA\bx^*}^2$. Defining the error vector $\widehatbe \triangleq \widehatbx - \bx^*$, substituting in the relation $\bb = \bA\bx^* + \bepsilon$ yields
\begin{equation}\label{equation:bound_lassl2error1}
\text{(Constrained LASSO)}: \qquad \frac{\normtwo{\bA\widehatbe}^2}{m} \leq \frac{2}{m} \bepsilon^\top \bA \widehatbe.
\end{equation}
Applying  H\"older's inequality (Theorem~\ref{theorem:holder-inequality}) to the right-hand side yields the upper bound $\frac{2}{m} \abs{\bepsilon^\top \bA \widehatbe} \leq \frac{2}{m} \norminf{\bA^\top \bepsilon} \normone{\widehatbe}$. The inequality $\normone{\widehatbx} \leq R = \normone{\bx^*}$ implies that $\widehatbe \in \sC[\sS; 1]$ (Lemma~\ref{lemma:sparse_lem1}):
\begin{equation}\label{equation:bound_lassl2error2}
\normone{\widehatbx} \leq \normone{\bx^*}\quad\implies\quad 
\normone{\widehatbe} = \normone{\widehatbe_{\sS}} + \normone{\widehatbe_{\comple{\sS}}} \leq 2 \normone{\widehatbe_{\sS}} 
\stackrel{\dag}{\leq} 2 \sqrt{k} \normtwo{\widehatbe_{\sS}}\leq 2 \sqrt{k} \normtwo{\widehatbe},
\end{equation}
where the inequality ($\dag$) follows from the Cauchy-Schwarz inequality (Equation~\eqref{equation:vector_form_cauchyschwarz}): 
\begin{equation}\label{equation:bound_lassl2error2_v2}
\normone{\widehatbe_{\sS}}=\abs{\widehatbe_{\sS}}^\top\bone \leq \normtwo{\widehatbe_{\sS}}\normtwo{\bone} = \sqrt{k}\normtwo{\widehatbe_{\sS}} 
\leq  \sqrt{k}\normtwo{\widehatbe}.
\end{equation}
Note that \eqref{equation:bound_lassl2error2} holds under the condition $\normone{\widehatbx} \leq \normone{\bx^*}$, while \eqref{equation:bound_lassl2error2_v2} is valid for any $\sS$ and $\widehatbe$.
On the other hand, applying the RE property to the left-hand side of the inequality \eqref{equation:bound_lassl2error1} yields $ \alpha \normtwo{\widehatbe}^2 \leq \frac{1}{m} \normtwo{\bA \widehatbe}^2$. Combining these inequalities yields the desired result.

\paragraph{(ii) Lagrangian LASSO.} Define the function
\begin{equation}
G(\be) \triangleq \frac{1}{2m} \normtwo{\bb - \bA (\bx^* + \be)}^2 + \lambda_m \normone{\bx^* + \be}.
\end{equation}
Noting that $\widehatbe \triangleq \widehatbx - \bx^*$ minimizes $G$ by construction, we have $G(\widehatbe) \leq G(\bzero)$. 
Substituting in the relation $\bb = \bA\bx^* + \bepsilon$ yields
\begin{equation}\label{equation:bound_lassl2error3}
\text{(Lagrangian LASSO)}:\qquad \frac{\normtwo{\bA \widehatbe}^2}{m} \leq \frac{2}{m} \bepsilon^\top \bA \widehatbe + 2\lambda_m \left\{ \normone{\bx^*} - \normone{\bx^* + \widehatbe} \right\}.
\end{equation}
Since $\bx_{\comple{\sS}}^* = \bzero$, we have $\normone{\bx^*} = \normone{\bx_{\sS}^*}$, and
$$
\normone{\bx^* + \widehatbe} = \normone{\bx_{\sS}^* + \widehatbe_{\sS}} + \normone{\widehatbe_{\comple{\sS}}} \geq \normone{\bx_{\sS}^*} - \normone{\widehatbe_{\sS}} + \normone{\widehatbe_{\comple{\sS}}}.
$$
Substituting these relations into the inequality \eqref{equation:bound_lassl2error3} and applying  H\"older's inequality (Theorem~\ref{theorem:holder-inequality}) yield
\begin{equation}\label{equation:bound_lassl2error5}
\begin{aligned}
\frac{\normtwo{\bA \widehatbe}^2}{m} 
&\leq \frac{2 }{m}\bepsilon^\top \bA \widehatbe + 2\lambda_m \left\{ \normone{\widehatbe_{\sS}} - \normone{\widehatbe_{\comple{\sS}}} \right\} \\ 
&\leq \frac{2 }{m} \norminf{\bA^\top \bepsilon}\normone{\widehatbe} + 2\lambda_m \left\{ \normone{\widehatbe_{\sS}} - \normone{\widehatbe_{\comple{\sS}}} \right\}.
\end{aligned}
\end{equation}
Since $\frac{2}{m} \norminf{\bA^\top \bepsilon} \leq \lambda_m$ by assumption, using the fact that $\normone{\widehatbe_{\sS}} \leq \sqrt{k} \normtwo{\widehatbe}$ from \eqref{equation:bound_lassl2error2_v2}, we have:
\begin{equation}\label{equation:bound_lassl2error6}
0\leq \frac{\normtwo{\bA \widehatbe}^2}{m} \leq {\lambda_m} \left\{ \normone{\widehatbe_{\sS}} + \normone{\widehatbe_{\comple{\sS}}} \right\} + 2\lambda_m \left\{ \normone{\widehatbe_{\sS}} - \normone{\widehatbe_{\comple{\sS}}} \right\} \leq {3} \sqrt{k} \lambda_m \normtwo{\widehatbe}.
\end{equation}
The inequality \eqref{equation:bound_lassl2error6} also implies that $\normone{\widehatbe_{\comple{\sS}}} \leq 3 \normone{\widehatbe_{\sS}}$ such that $\widehatbe\in \sC[\sS; 3]$.
Applying the $\alpha$-RE condition to $\widehatbe$ ensures that $\alpha \normtwo{\widehatbe}^2 \leq \frac{1}{m} \normtwo{\bA \widehatbe}^2$. 
Combining this lower bound with the inequality \eqref{equation:bound_lassl2error6} yields
$
{\alpha} \normtwo{\widehatbe}^2 \leq {3}  \sqrt{k} \lambda_m \normtwo{\widehatbe},
$
which implies the desired result.
\end{proof}



\begin{theoremHigh}[Bounds of Prediction Error for LASSO \citep{hastle2015statistical}]\label{theorem:lassobound_predict_error}
Let the observations follow from the linear observation model $\bb = \bA\bx^* + \bepsilon$, where $\bA\in\real^{m\times n}$.
Consider the Lagrangian LASSO \eqref{equation:loss_lagrang_lasso} with a regularization parameter $\lambda_m \geq \frac{2}{m} \norminf{\bA^\top \bepsilon}$.
\begin{enumerate}[(i)]
\item If $\normone{\bx^*} < R_1$, then any optimal solution $\widehatbx$ satisfies
$$
\frac{\normtwo{\bA (\widehatbx - \bx^*)}^2}{m} \leq 12 \, R_1 \, \lambda_m.
$$
\item Suppose further that $\bx^*$ is $k$-sparse, supported on the subset $\sS\subseteq \{1, 2,\ldots, n\}$ (i.e., $\abs{\sS}=k$).
If the model matrix $\bA$ satisfies the restricted eigenvalue property with parameter $\alpha > 0$ over $\sC[\sS;3]$, then any optimal solution $\widehatbx$ satisfies
$$
\frac{\normtwo{\bA (\widehatbx - \bx^*)}^2}{m} \leq 9\frac{ k \lambda_m^2}{\alpha}.
$$
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:lassobound_predict_error}]
The proof is a slight adaptation based on \citet{hastle2015statistical}.
\paragraph{(i).} Once again, we define the error vector $\widehatbe \triangleq \widehatbx - \bx^*$. Following the inequality \eqref{equation:bound_lassl2error3}, we have
\begin{align}
0 &\leq\frac{\normtwo{\bA \widehatbe}^2}{m}  \leq \frac{2}{m} \norminf{\bA^\top \bepsilon} \normone{\widehatbe} + 2\lambda_m \big\{ \normone{\bx^*} - \normone{\bx^* + \widehatbe} \big\} \label{equation:lassobound_predict_error_ineq1}\\
&\overset{\dag}{\leq} 2\left\{ \frac{\norminf{\bA^\top \bepsilon}}{m} - \lambda_m \right\} \normone{\widehatbe} + 4 \lambda_m \normone{\bx^*} 
\overset{\ddag}{\leq}  \lambda_m \Big\{ - \normone{\widehatbe} + 4 \normone{\bx^*} \Big\}, 
\end{align}
where the inequality $(\dag)$ follows from the triangular inequality 
$-(\normone{\bx^*} - \normone{\widehatbe} ) \leq \normone{\bx^* - (-\widehatbe)} 
$~\footnote{$\abs{\norm{\bx}-\norm{\by}} \leq \norm{\bx-\by}$ for any vector norm.}, and  the inequality $(\ddag)$ follows from the assumption that $\lambda_m \geq \frac{2}{m} \norminf{\bA^\top \bepsilon}$.
This indicates that $\normone{\widehatbe} \leq 4 \normone{\bx^*} \leq 4 R_1$. Combining these inequalities and applying the triangle inequality $\normone{\bx^*} - \normone{\bx^* + \widehatbe} \leq \normone{\bx^* -(\bx^* + \widehatbe)}=\normone{\widehatbe}$ in \eqref{equation:lassobound_predict_error_ineq1} yield that
\begin{equation}\label{equation:lassobound_predict_error_part1fin}
\frac{\normtwo{\bA \widehatbe}^2}{m} \leq 2\left\{ \frac{\norminf{\bA^\top \bepsilon}}{m} + \lambda_m \right\} \normone{\widehatbe} \leq 12 \lambda_m R_1,
\end{equation}
which establishes the desired result.

\paragraph{(ii).} Following \eqref{equation:bound_lassl2error6}, we have
$
\frac{\normtwo{\bA \widehatbe}^2}{m}   \leq 3  \sqrt{k}\lambda_m \normtwo{\widehatbe}.
$~\footnote{
Note that \citet{hastle2015statistical} use \eqref{equation:lassobound_predict_error_part1fin} and the fact $\normone{\widehatbe} = \normone{\widehatbe_{\sS}} + \normone{\widehatbe_{\comple{\sS}}} \leq 4 \normone{\widehatbe_{\sS}} 
\leq 4 \sqrt{k} \normtwo{\widehatbe_{\sS}}\leq 4 \sqrt{k} \normtwo{\widehatbe}$ to obtain 
$\frac{\normtwo{\bA \widehatbe}^2}{m} \leq 12\sqrt{k}\normtwo{\widehatbe}$, which results in a looser bound.
}
By the proof of Theorem~\ref{theorem:bound_lassl2error}, the error vector $\widehatbe$ belongs to the cone $\sC[\sS;3]$, so that the $\alpha$-RE condition guarantees that $\normtwo{\widehatbe}^2 \leq \frac{1}{m \alpha} \normtwo{\bA \widehatbe}^2$. Combining these inequalities yields the desired result.
\end{proof}


\citet{hastle2015statistical} demonstrate that for various statistical models, choosing $\lambda_m = \mathcalO\big(\sqrt{\frac{\ln(n)}{m}}\big)$ is valid for Theorem~\ref{theorem:lassobound_predict_error} with high probability. Consequently, the two bounds take the form:
\begin{subequations}
\begin{align}
\frac{\normtwo{\bA (\widehatbx - \bx^*)}^2}{m} &\leq \mathcalO\big(R_1 \, \sqrt{\frac{\ln(n)}{m}}\big); \label{equation:lassobound_predict_error1} \\
\frac{\normtwo{\bA (\widehatbx - \bx^*)}^2}{m} &\leq \mathcalO\big(\frac{ \abs{\sS} \, \ln(n)}{m}\big). \label{equation:lassobound_predict_error2}
\end{align}
\end{subequations}
The bound \eqref{equation:lassobound_predict_error1}, which depends on the $\ell_1$-ball radius $R_1$, is known as the ``slow rate'' for the LASSO problem, since the squared prediction error decays as $1/\sqrt{m}$. On the other hand, the bound \eqref{equation:lassobound_predict_error2} is known as the ``fast rate,'' since it decays as $1/m$. 
Note that the latter relies on stronger assumptions: specifically, the hard sparsity condition that $\bx^*$ is supported on a small subset $\sS$, and more critically, the $\alpha$-RE condition on the design matrix $\bA$.
In principle, prediction performance should not require an RE condition, suggesting that this requirement might be an artifact of our proof technique.


\begin{problemset}

\item \label{prob:als_pseudo1} \textbf{Least squares for rank-deficiency.} Let $\bA\in\real^{m\times n}$ and $\bb\in\real^m$. Show that the least squares problem $L(\bx)=\normtwo{\bA\bx-\bb}^2$ has a minimizer $\bx^*\in\real^n$ if and only if there exists a vector $\by\in\real^n$ such that $\bx^*=\bA^+\bb+(\bI-\bA^+\bA)\by$, where $\bA^+$ is the pseudo-inverse of $\bA$. 
\begin{itemize}
\item This shows that the least squares has a \textbf{unique} minimizer of $\bx^*=\bA^+\bb$ only when $\bA^+$ is a left inverse of $\bA$ (i.e., $\bA^+\bA = \bI$). The solution in Lemma~\ref{lemma:ols} is a special case.
\item The optimal value is $L(\bx^*)=\bb^\top(\bI-\bA\bA^+)\bb$.
\item If $\by\neq \bzero$: $\normtwo{\bA^+\bb}\leq \normtwo{\bA^+\bb+(\bI-\bA^+\bA)\by}$.
\end{itemize}
\textit{Hint: using singular value decomposition.}

\item \label{prob:als_pseudo2} \textbf{Least squares for rank-deficiency.} Let  $\bA\in\real^{m\times n}$ and $\bB\in\real^{m\times p}$. Show that the least squares problem $L(\bX) = \normf{\bA\bX-\bB}^2$ has a minimizer $\bX^*=\bA^+\bB\in\real^{n\times p}$. Determine all the minimizers using Problem~\ref{prob:als_pseudo1}.

\item \label{prob:als_pseudon} \textbf{Least squares for rank-deficiency.}  Let  $\bA\in\real^{m\times n}$ and $\bB\in\real^{p\times n}$. Show that the least squares problem $L(\bX) = \normf{\bX\bA-\bB}^2$ has a minimizer $\bX^*=\bB\bA^+\in\real^{p\times m}$.







\item \label{problem:rls} \textbf{Regularized least squares (RLS).} 
Given $\bA\in\real^{m\times n}, \bb\in\real^{m}, \bB\in\real^{p\times n}$, and $\lambda\in\real_{++}$, we consider the constrained least squares problem:
$$
\mathop{\min}_{\bx\in\real^n} \normtwo{\bA\bx-\bb}^2 + \lambda\normtwo{\bB\bx}^2.
$$
Show that the constrained least squares (RLS) problem has a unique solution if and only if $\nspace(\bA)\cap \nspace(\bB) = \{\bzero\}$.

\item \label{prob:denoise_rls} \textbf{Denoising via RLS.} Consider a noisy measurement of a signal $\bx\in\real^n$:
$
\by = \bx+\be,
$
where $\by$ is the observed measurement, and $\be$ is the noise vector. We want to find an estimate $\bx$ of the observed measurement $\by$ such that $\bx \approx \by$:
$
\min \normtwo{\bx-\by}^2.
$
Apparently, the optimal solution of this optimization is given by $\bx=\by$; however, it is meaningless.
To improve the estimate,  we can add a penalty term for the differences between  consecutive observations:
$
R(\bx) = \sum_{i=1}^{n-1} (x_i - x_{i+1})^2.
$
Then, 
\begin{itemize}
\item Find the constrained least squares representation for this problem and derive the constrained least squares solution.
\item Find some applications of this denoising problem. For example, when we model the profit and loss signal of a financial asset, the two observations over consecutive days of the underlying asset should exhibit smooth transitions rather than abrupt changes.
\end{itemize}



\item \textbf{Weighted least squares (WLS).}
Building upon the assumptions in Lemma~\ref{lemma:ols}, we consider further that each data point $i\in\{1,2,\ldots, m\}$ (i.e., each row of $\bA$) has a weight $w_i$. 
This means some  data points may carry greater significance than others, and we can produce approximate minimizers that reflect this.
Show that the value $\bx_{WLS} = (\bA^\top\bW^2\bA)^{-1}\bA^\top\bW^2\bb$ serves as the \textit{weighted least squares (WLS)}  estimate of $\bx$, where $\bW=\diag(w_1, w_2, \ldots, w_m)\in\real^{m\times m}$. \textit{Hint: Find the normal equation for this problem.}

\item \textbf{Positive definite  weighted least squares (PDWLS).}
Building upon the assumptions in Lemma~\ref{lemma:ols}, we consider further  the matrix equation $\bA\bx + \be =\bb$, where $\be$ is an error vector. Define the weighted error squared sum $E_w = \be^\top \bW \be$, where the weighting matrix $\bW$ is  positive definite. 
Show that the positive definite weighted least squares solution is $\bx^* = (\bA^\top\bW\bA)^{-1}\bA^\top\bW\bb$. \textit{Hint: Compute the gradient of $E_w = (\bb-\bA\bx)^\top\bW(\bb-\bA\bx)$.}


\item \label{problem:tls} \textbf{Transformed least squares (TLS).}
Building upon the assumptions in Lemma~\ref{lemma:ols}, we consider further the restriction $\bx=\bC\bgamma+\bc$, where $\bC\in\real^{n\times k}$ is a known matrix such that $\bA\bC$ has full rank, $\bc$ is a known vector, and $\bgamma$ is an unknown vector.
Show that the value $\bx_{TLS}=\bC(\bC^\top\bA^\top\bA\bC)^{-1}(\bC^\top\bA^\top)(\bb-\bA\bc) +\bc$ serves as the \textit{transformed least squares (TLS)} estimate of $\bx$.



\item \label{problem:twls2} Find the transformed weighted least squares estimate.


\item Verify that $\bG^\toptone \bP^\toptone = \bI$ in \eqref{equation:secan_dogleg}.

\item Prove that the set $\sC[k] \triangleq \bigcup_{\sS: \abs{\sS} = k} \sC[\sS]$ is non-convex.

\item Let $ \bA $ be a design matrix that satisfies the nullspace property of order $ 2s $ (Definition~\ref{definition:nullspace_prop}). Show that for any two distinct $ s $-sparse vectors $ \bu_1, \bu_2 \in \sB_0[s] $, $ \bu_1 \neq \bu_2 $, it follows that  $ \bA \bu_1 \neq \bA \bu_2 $.

\end{problemset}