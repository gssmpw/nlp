\newpage
\chapter{Descent Methods}\label{chapter:gradient-descent} 
\begingroup
\hypersetup{linkcolor=structurecolor,
linktoc=page,  % page: only the page will be colored; section, all, none etc
}
\minitoc \newpage
\endgroup

\index{Gradient descent}
\section{Descent Methods and Gradient Descent Method}\label{section:gradient-descent-all} 
The \textit{gradient descent} (GD) method is a specific type of descent method used to find the (local or global) minimum of a differentiable function, whether convex or non-convex. This function is commonly referred to as the \textit{cost function} (also known as the \textit{loss function} or \textit{objective function}).
It stands out as one of the most popular algorithms to perform optimization and by far the
most common way to optimize machine learning, deep learning, and various optimization problems. 
This is especially true for optimizing neural networks and transformer networks \citep{lecun2015deep, goodfellow2016deep, vaswani2017attention}. 
In the context of machine learning, the cost function measures the difference between a modelâ€™s predicted output and the actual output. Neural networks, transformer networks, and machine learning models in general seek to find a set of parameters $\bx\in \real^n$ (also known as weights) that optimize an objective function $f(\bx)$. This is expressed as the unconstrained optimization problem (P1) in Definition~\ref{definition:opt_probs_all}:
$$
\textbf{(P1)}:\qquad \text{Find}\quad \bx^* = \mathop{\argmin}_{\bx} f(\bx).
$$
In some special cases, (P1) can be solved analytically by finding the stationary points of the function using the first-order optimality condition in Theorem~\ref{theorem:fermat_fist_opt}: $\nabla f(\bx)=\bzero$. However, in most cases, the problem must be solved using iterative methods.
All the optimization methods discussed in this book, including gradient descent, fall under the category of \textit{iterative methods}. Denoting $t=1,2,\ldots$ as the iteration number, these methods generate a sequence of vectors:
\begin{equation}
\bx^{(1)}, \bx^{(2)}, \ldots, \bx^{(T)} \in \dom(f)
\end{equation}
\footnote{Some texts denote the starting point as $\bx^{(0)}$, but in this book, we use $\bx^{(1)}$.}
such that as $T\rightarrow \infty$, the sequence converges to the optimal solution $\bx^*$, and the objective function value $f(\bx^{(T)})$ approaches the optimal minimum $f(\bx^*)$, under certain mild conditions.
At each iteration $t$, an \textit{update step (or a descent step)} $\bh^\toptzero$ is applied to update the parameters. Denoting the parameters at the $t$-th iteration as $\bx^\toptzero$,  the update rule  is given by:
\begin{equation}\label{equation:sg_update_rule}
(\textbf{GD update}):\qquad 
\bx^\toptone \leftarrow \bx^\toptzero + \bh^\toptzero.
\end{equation}


\index{Learning rate}
\index{Strict SGD}
\index{Mini-batch SGD}
\index{Local minima}
\paragrapharrow{Gradient descent.}
The most basic form of gradient descent is the \textit{vanilla update}, where the parameters move in the opposite direction of the gradient. This follows the \textit{steepest descent direction} since gradients are orthogonal to level curves (also known as level surfaces, see Lemma~\ref{lemm:direction-gradients}):
\begin{equation}\label{equation:gd-equaa-gene}
\bh^\toptzero = -\eta_t \bg^\toptzero\triangleq -\eta_t \nabla f(\bx^\toptzero),
\end{equation}
where the positive value $\eta_t$ denotes the \textit{learning rate  (or stepsize, step length, step size)} that depends on specific problems.
The term $\bg^\toptzero\triangleq\nabla f(\bx^\toptzero) \in \real^n$ represents the gradient of the parameters.
The {learning rate} $\eta_t$ controls how large of a step to take in the direction of negative gradient so that we can reach a (local) minimum.
The method that follows the negative gradient direction (i.e., $\bd^\toptzero \triangleq -\nabla f(\bx^\toptzero)$ in Algorithm~\ref{alg:struc_gd_gen}) is called the \textit{steepest descent method (or gradient method)}. 
The choice of descent direction is ``the best" (locally; see \eqref{equation:steep_des}) and we could combine it with an exact line search to determine the learning rate  (Section~\ref{section:exact_line_search}). A method like this converges, but the final convergence is linear and often very slow. 

Examples in \citet{madsen2010and, boyd2004convex} show how the gradient descent method with exact line search and finite computer precision can fail to find the minimizer of a second degree polynomial. However, for many problems, it performs well in the early stages of the iterative process.
Considerations like this has lead to the so-called \textit{hybrid methods}, which---as the name suggests---are based on two different methods. One which is good in the initial stage, like the \textit{gradient method}, and another method which is good in the final stage, like \textit{Newton's method} (Section~\ref{section:new_methods}). A key challenge with hybrid methods is designing an effective mechanism to switch between the two approaches at the appropriate time.

The descent property of such a step in \eqref{equation:gd-equaa-gene} is guaranteed by Definition~\ref{definition:uncons_des_direct} and Lemma~\ref{lemma:descent_property} or  (by setting $\bB\triangleq\bI$ in Theorem~\ref{theorem:uncons_des_dir}).
In \eqref{equation:sg_update_rule}, $\bh^\toptzero$ is referred to as a \textit{descent step}. While a direction $\bd^\toptzero$ in Definition~\ref{definition:uncons_des_direct} satisfying $\innerproduct{\bg^\toptzero,\bd^\toptzero}<0$ is called a \textit{descent direction}. In most cases, the relationship between the descent step and descent direction follows a scale by the learning rate:
\begin{equation}\label{equation:desce_step_direc}
\begin{aligned}
	&\textbf{(Descent direction)}: \qquad&&\bd^\toptzero = -\bg^\toptzero;\\
	&\textbf{(Descent step)}:\qquad &&\bh^\toptzero=\eta_t\bd^\toptzero.
\end{aligned}
\end{equation}
In many cases, when the learning rate is equal to 1, the above two terms are used \textbf{interchangeably},  then the descent direction and the descent step are the same; for example, Newton's method in Chapter~\ref{chapter:second_order}.



\paragrapharrow{Gradient descent by calculus}
An intuitive analogy to understand gradient descent is to imagine the path of a river starting from a mountain peak and flowing downhill to reach the lowest point at its base.
Similarly, the goal of gradient descent is to find the lowest point in the landscape defined by the objective function $f(\bx)$, where $\bx$ represents  a $n$-dimensional input variable. Our task is to use an algorithm that guides us to a (local) minimum of $f(\bx)$.
To better understand this process, consider moving a ball a small distance $d_1$ along the $x_1$ axis, a small amount $d_2$ along the $x_2$ axis, and so on up to $d_n$ along the $x_n$ axis. 
Calculus informs us of the variation in the objective function  $f(\bx)$ as follows:
$$
\Delta f(\bx) \approx \frac{\partial f}{\partial x_1}d_1 + \frac{\partial f}{\partial x_2}d_2 + \ldots + \frac{\partial f}{\partial x_n}d_n.
$$
Our challenge is to choose $d_1, d_2, \ldots, d_n$ such that they cause $\Delta f(\bx)$ to be negative, thereby decreasing the objective function towards minimization.
Let $\bd=[d_1,d_2, \ldots , d_n]^\top$ denote  the vector of  changes in $\bx$, and let $\nabla f(\bx)=[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2}, \ldots , \frac{\partial f}{\partial x_n}]^\top$ denotes the gradient vector of $f(\bx)$ \footnote{Note the difference between $\Delta f(\bx)$ and $\nabla f(\bx)$.}. Then it follows that
$$
\Delta f(\bx) \approx \frac{\partial f}{\partial x_1}d_1 + \frac{\partial f}{\partial x_2}d_2 +\ldots  + \frac{\partial f}{\partial x_n}d_n = \innerproduct{\nabla f(\bx), \bd}.
$$ 
In the context of descending the function, our aim is to ensure that $\Delta f(\bx)$ is negative. 
This ensures that moving from $\bx^\toptzero$ to $\bx^\toptone = \bx^\toptzero+\bd^\toptzero$  (from $t$-th iteration to $(t+1)$-th iteration) results in a reduction of the loss function $f(\bx^\toptone) = f(\bx^\toptzero) + \Delta f(\bx^\toptzero)$, given that $\Delta f(\bx^\toptzero) \leq 0$.
It can be demonstrated that if the update step is defined as $\bd^\toptzero=-\eta_t \nabla f(\bx^\toptzero)$, where $\eta_t$ is the learning rate, the following relationship holds:
$$
\Delta  f(\bx^\toptzero) \approx -\eta_t \nabla f(\bx^\toptzero)^\top\nabla f(\bx^\toptzero) = -\eta_t\normtwobig{\nabla f(\bx^\toptzero)}^2 \leq 0. 
$$
Specifically,  $\Delta  f(\bx^\toptzero) < 0$ unless we are already at the optimal point with zero gradients.
This analysis validates the approach of gradient descent. 
%The update rule for the next parameter $\bx^\toptone$ is:
%$$
%\bx^\toptone \leftarrow \bx^\toptzero - \eta_t \nabla f(\bx^\toptzero).
%$$


%\index{Descent condition}
%\index{Descent direction}
%\index{Search direction}
%\index{Taylor's formula}
%\begin{remark}[Descent Condition]\label{remark:descent_condition}
%In the above construction, we define $\bd^\toptzero = -\eta \nabla f(\bx^\toptzero)$, where $-\nabla f(\bx^\toptzero)$ is the \textit{descent direction} such that $\Delta f \approx -\eta \nabla f(\bx^\toptzero)^\top \nabla f(\bx^\toptzero) < 0$ (assuming $\nabla f(\bx^\toptzero) \neq \bzero $). More generally, any \textit{search direction} $\bd^\toptzero \in \real^n{\setminus}\{\bzero\}$ that satisfies the \textit{descent condition} can be chosen as the descent direction:
%$$
%\frac{d f(\bx^\toptzero + \eta \bd^\toptzero)}{d \eta}\bigg|_{\eta=0} = \nabla f(\bx^\toptzero)^\top \bd^\toptzero <0.
%$$
%In other words, according to  Taylor's formula (Section~\ref{section:differ_calc}),
%$$
%f(\bx^\toptzero+\eta\bd^\toptzero) \approx f(\bx^\toptzero) + \eta \nabla f(\bx^\toptzero)^\top\bd^\toptzero 
%$$
%implies $f(\bx^\toptzero+\eta\bd^\toptzero)  < f(\bx^\toptzero)$ when $\eta$ is sufficiently small. When $\bd^\toptzero = -\nabla f(\bx^\toptzero)$, the descent direction is known as the \textit{steepest descent direction} (see \eqref{equation:steep_des}).
%\textcolor{red}{TODO}
%When the learning rate $\eta$ is not fixed and decided by exact line search, the method is called \textit{steepest descent method} (see Section~\ref{section:quadratic-in-steepestdescent}).
%\end{remark}


\index{Convex functions}
\paragrapharrow{Gradient descent for convex functions.}
We further explore the application of gradient descent in (unconstrained) convex problems (see Section~\ref{section:convex_opt_uncons} for optimality conditions of this problem).
If the objective function $f(\bx)$ is (continuously differentiable) convex, then the relationship $\innerproduct{\nabla f(\bx^\toptzero), (\bx^\toptone-\bx^\toptzero)}\geq 0$ implies $f(\bx^\toptone) \geq f(\bx^\toptzero)$. This can be derived from the gradient inequality of a continuously differentiable convex function, i.e., $f(\bx^\toptone)- f(\bx^\toptzero)\geq \innerproduct{\nabla f(\bx^\toptzero), (\bx^\toptone-\bx^\toptzero)}$; see Theorem~\ref{theorem:conv_gradient_ineq}. 

In this sense, to ensure a reduction in the objective function from the point $\bx^\toptzero$ to $\bx^\toptone$, it is imperative to ensure  $\innerproduct{\nabla f(\bx^\toptzero), (\bx^\toptone-\bx^\toptzero)}\leq 0$. 
In the context of  gradient descent, the choice of $\eta_t\bd^\toptzero = \bx^\toptone-\bx^\toptzero$ aligns with the negative gradient $-\nabla f(\bx^\toptzero)$. However, there are many other descent methods, such as \textit{(non-Euclidean) greedy descent}, \textit{normalized steepest descent}, \textit{Newton step}, and so on. The core principle of these methods is to ensure that $\innerproduct{\nabla f(\bx^\toptzero), (\bx^\toptone-\bx^\toptzero)}= \innerproduct{\nabla f(\bx^\toptzero), \bd^\toptzero} \leq 0$, provided the objective function is convex.


\paragrapharrow{Gradient descent with momentum.}
\textit{Gradient descent with momentum} is an improvement over basic gradient descent, frequently used in machine learning and deep learning to minimize the loss function and update model parameters (Section~\ref{section:sgd_momentum}). While standard gradient descent updates parameters solely based on the current gradient, momentum-based gradient descent introduces a \textit{momentum} term to accelerate convergence and smooth the optimization path.

In this approach, the momentum term enables the algorithm to build velocity in directions with a steady but small gradient, helping it overcome local minima and saddle points. By incorporating a fraction of the previous update into the current one, this technique mimics inertia, allowing the algorithm to continue moving in the same direction despite minor fluctuations in the gradient. Consequently, this method not only speeds up convergence but also reduces oscillations, particularly in regions where the surface curvature varies significantly across different dimensions.
At each iteration $t$, the process involves two key steps:
\begin{subequations}\label{equation:gd_with_momentum}
\begin{align}
\textbf{(Velocity update)}:\qquad \bd^\toptzero &\leftarrow \rho \bd^\toptminus - \eta_t \nabla f(\bx^\toptzero);\\
\textbf{(Parameter update)}:\qquad \bx^\toptone &\leftarrow \bx^\toptzero + \bd^\toptzero.
\end{align}
\end{subequations}
By incorporating past gradients into the update rule, gradient descent with momentum enables more efficient traversal across the error surface, particularly in complex landscapes, leading to faster convergence and improved performance.
In summary, The gradient descent with momentum approach is advantageous for the following reasons:
\begin{itemize}
\item At saddle points, the gradient of the cost function becomes nearly zero or entirely negligible. This results in minimal or no updates to the weights, causing the network's learning process to stagnate and effectively halt.
\item The trajectory taken by the gradient descent method tends to be quite erratic, even when employing mini-batch processing. This jittery path can impede efficient convergence towards the minimum (Section~\ref{section:sgd_momentum}).
\end{itemize}
We will analyze the behaviors of standard gradient descent and momentum-based gradient descent for quadratic models in Sections~\ref{section:quadratic_vanilla_GD}, \ref{section:quadratic-in-steepestdescent}, and \ref{section:quadratic-in-momentum}.

\paragrapharrow{Steepest descent.}
The linear approximation theorem (Theorem~\ref{theorem:linear_approx}) states that 
\begin{equation}\label{equation:gd_gree_tay11}
	f(\bx^\toptzero + \eta \bd) = f(\bx^\toptzero)+ \eta \bd^\top \nabla  f(\bx^\toptzero )+ \mathcalO(\normtwo{\eta \bd}^2).
\end{equation}
From \eqref{equation:gd_gree_tay11} and by the definition of directional derivative, we observe that when taking  a step $\eta \bd$ with a positive stepsize $\eta$,  the relative reduction in function value satisfies
$$
\lim_{\eta \rightarrow 0} \frac{f(\bx^\toptzero) - f(\bx^\toptzero + \eta \bd)}{\eta \normtwo{\bd}} = -\frac{1}{\normtwo{\bd}} \bd^{\top} \nabla f(\bx^\toptzero) = \normtwobig{\nabla f(\bx^\toptzero)} \cos (\theta),
$$
where $\theta$ is the angle between the vectors $\bd$ and $-\nabla f(\bx^\toptzero)$. This equation indicates that we get the greatest gain rate if $\theta = 0$, meaning the optimal descent direction is the steepest descent direction $\bd_{\text{sd}}^\toptzero$, given by
\begin{equation}\label{equation:steep_des}
\bd_{\text{sd}}^\toptzero = -\nabla f(\bx^\toptzero).
\end{equation}
That is, the steepest descent method coincides with the gradient descent method.


\paragrapharrow{Stochastic gradient descent.} 
In many cases, the function $f(\bx)$ is defined over a datasets $\mathcalD=\{\bs_1, \bs_2, \ldots, \bs_D\}$ such that $f(\bx)$ and its gradient $\nabla f(\bx)$ can be expressed as 
\begin{equation}
f(\bx)\triangleq f(\mathcalD; \bx) = \frac{1}{D} \sum_{d=1}^{D} f(\bs_d; \bx)
\qquad \text{and}\qquad 
\nabla f(\bx) \triangleq \frac{1}{D}  \sum_{d=1}^{D}\nabla f(\bs_d; \bx),
\end{equation} 
respectively.
While if we follow the negative gradient of a single sample or a batch of samples iteratively, the local estimate of the direction can be obtained and is known as the \textit{stochastic gradient descent} (SGD) \citep{robbins1951stochastic}.
The SGD method can be categorized  into two types:
\begin{itemize}
\item \textbf{The strict SGD:} Computes the gradient using only one randomly selected data point per iteration: $\nabla f(\bx^\toptzero) \approx \nabla f(\bs_d; \bx^\toptzero)$.
\item \textbf{The mini-batch SGD:} A compromise between full gradient descent and strict SGD, where a small subset (mini-batch) of the dataset is used to compute an estimate of the gradient: $\nabla f(\bx^\toptzero) \approx \frac{1}{\abs{\sS}}  \sum_{d\in\sS}\nabla f(\bs_d; \bx^\toptzero)$.
\end{itemize}
The SGD method is particular useful when the number of \textit{training entries} (i.e., the data used for updating/training the model, while the data used for final evaluation is called the \textit{test entries or test data}) are substantial, as computing the full gradient can be computationally expensive or even resulting in that the gradients from different input samples may cancel out and the final update is small.
However, since the gradient is estimated using only a subset of the data, the updates can be noisy.
In the SGD framework, the objective function is stochastic, composed of a sum of subfunctions evaluated at different subsamples of the data (see Chapter~\ref{chapter:stochastic_opt}). However, a drawback of the vanilla update (both GD and SGD) lies in its susceptibility to getting trapped in local minima \citep{rutishauser1959theory}.




\paragrapharrow{Choice of stepsize.}
For a small stepsize, gradient descent ensures a monotonic improvement at every iteration, guaranteeing convergence, albeit to a local minimum. However, the speed of the vanilla gradient descent method is generally slow, and it can exhibit a linear rate in case of poor curvature conditions. 
While choosing a stepsize larger than an optimal threshold may cause divergence in terms of the objective function.
Determining an optimal learning rate (whether global or per-dimension) becomes more of an art than science for many problems. 
Previous work has attempted to alleviate the need for manually selecting a global learning rate \citep{zeiler2012adadelta}, though such methods remain sensitive to other hyperparameters. We will introduce \textit{(exact or inexact) line search strategies} to determine the stepsize more systematically in Section~\ref{section:line_search}.




\paragrapharrow{Descending Property.}
Most (if not all) optimization methods incorporate mechanisms to enforce the descending property:
\begin{equation}\label{equation:des_prob1}
f(\bx^\toptone) < f(\bx^\toptzero). 
\end{equation}
This prevents convergence to a maximizer and also makes it less probable that we get convergence to a saddle point (Definition~\ref{definition:stat_point}). 
If the objective function has several minimizers, the final solution depends on the starting point $\bx^{(1)}$. We do not know which of the minimizers that will be found; the specific minimizer found is not necessarily the one closest to $\bx^{(1)}$.

As mentioned previously, in many cases the method produces vectors which converge towards the minimizer in two clearly different stages: the ``global stage" where $\bx^{(1)}$ is far from the solution and we want the method to produce iterates which move steadily towards the optimizer $\bx^*$, and the ``final stage" where $\bx^\toptzero$ is close to $\bx^*$ and seek faster convergence.

The global convergence properties of a method describe its behavior when initialization occurs at a point $\bx^{(1)}$, which is not close to a (local) minimizer $\bx^{*}$. 
Ideally, the iterates should move steadily toward a neighborhood of  $\bx^{*}$. For instance, there are methods for which it is possible to prove that any accumulation point (i.e., limit of a subseries) of $\{\bx^\toptzero\}_{t>0}$ is a stationary point (Definition~\ref{definition:stat_point}), meaning the gradient vanishes:
$$
\nabla f(\bx^\toptzero) \rightarrow \bzero \qquad \text{ for } \qquad t \rightarrow \infty. 
$$
While this does not eliminate the possibility of convergence to a saddle point or maximizer, the descending property \eqref{equation:des_prob1} typically prevents such cases in practice.
In this ``global phase", our primary concern is ensuring that losses do not increase (except for possibly the initial steps).
To analyze convergence in terms of iterates rather than function values, a natural potential function is 
$$
\text{$e_t\triangleq\normtwobig{\be^\toptzero},\quad$ where $\be^\toptzero\triangleq \bx^\toptzero - \bx^*$.}
$$
Let $\{\be^\toptzero\}_{t>0}$ denote the error sequence. The requirement for progress is:
$$
\normtwobig{\be^\toptone} < \normtwobig{\be^\toptzero}\qquad  \text{ for }\qquad t > t'.
$$

In the final stages of the iteration where the $\bx^\toptzero$ are close to $\bx^{*}$, we expect faster convergence. 
Local convergence analysis describes how quickly the iterates approach $\bx^{*}$ to a desired accuracy. Some methods exhibit  linear convergence (Definition~\ref{definition:linear-convergence}):
$$
\normtwobig{\be^\toptone}\leq c_{1}\normtwobig{\be^\toptzero}, \quad \text{with } 0 < c_{1} < 1 \text{ and } \bx^\toptzero \text{ close to } \bx^{*}.  
$$
However, higher-order convergence is preferable. For instance, quadratic convergence (Definition~\ref{definition:quadratic-convergence}) satisfies:
$$
\normtwobig{\be^\toptone}\leq c_{2}\normtwobig{\be^\toptzero}^{2}, \quad \text{with } c_{2} > 0 \text{ and } \bx^\toptzero \text{ close to } \bx^{*}. 
$$
Few practical methods achieve quadratic convergence, but superlinear convergence (Definition~\ref{definition:superlinear_convergence}) is a common goal:
$$
\frac{\normtwobig{\be^\toptone}}{\normtwobig{\be^\toptzero}} \rightarrow 0 \quad\text{ for } t \rightarrow \infty.
$$
Superlinear convergence is faster than linear convergence, though typically not as rapid as quadratic convergence.


\paragrapharrow{Framework of a descent method.}

\begin{algorithm}[H] 
	\caption{Structure of  Descent Methods}
	\label{alg:struc_gd_gen}
	\begin{algorithmic}[1] 
		\Require A function $f(\bx)$; 
		\State {\bfseries Input:}  Initialize $\bx^{(1)}$;
		\For{$t=1,2,\ldots$}
		\State Find a descent direction $\bd^\toptzero$ such that $\innerproduct{\bd^\toptzero, \bg^\toptzero}<0$;
		\State Pick a stepsize $\eta_t$;
		\State $\bx^{(t+1)} \leftarrow \bx^\toptzero + \eta_t \bd^\toptzero$;
		\EndFor
		\State (Output Option 1) Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
		\State (Output Option 2) Output  $\bx_{\text{avg}}\leftarrow \frac{1}{T}(\sum_{t=1}^{t}\bx^\toptzero)$ or $\sum_{t=1}^{T} \frac{2t}{T(T+1)} \bx^\toptzero$;
		\State (Output Option 3) Output  $\bx_{\text{best}}\leftarrow \argmin_{t\in\{1,2,\ldots,T\}} f(\bx^\toptzero)$;
	\end{algorithmic} 
\end{algorithm}


The methods presented in this book are descent methods, meaning they satisfy the descending condition \eqref{equation:des_prob1} at each iteration. Each iteration consists of:
\begin{itemize}
\item Finding a descent direction $\bd^\toptzero$ at the $t$-th iteration.
\item Determining a stepsize $\eta_t$ giving a good decrease in the function value.
\end{itemize}
This sequence of operations forms the foundation of descent algorithms, see Algorithm~\ref{alg:struc_gd_gen}.
The search direction $ \bd^\toptzero $ at each iteration must be a descent direction (Definition~\ref{definition:uncons_des_direct}, Theorem~\ref{theorem:uncons_des_dir}). 
This ensures that we can reduce $ f(\bx) $ by choosing an appropriate walking distance, and thus we can satisfy the descending condition \eqref{equation:des_prob1}. 

\paragrapharrow{Stopping criteria.}
Ideally, a stopping criterion should indicate when the current error is sufficiently small:
$$
\textbf{(ST1)}:\qquad \normtwobig{\be^{\toptzero}} < \delta_{1}.
$$
Another ideal condition would be when the current function value is close enough to the minimum:
$$
\textbf{(ST2)}:\qquad f(\bx^\toptzero ) - f(\bx^{*}) < \delta_{2}.
$$
Both conditions reflect the convergence $ \bx^\toptzero  \rightarrow \bx^{*} $. 
However, they are impractical because $ \bx^{*} $ and $ f(\bx^{*}) $ are (in most cases) unknown~\footnote{In some cases, the value $f(\bx^*)$ is known. For example, in the convex feasibility problem, we seek feasible points within a convex set, where $f(\bx^*)$ is zero.}. 
Instead, we rely on approximations:
\begin{equation}\label{equation:des_stopcri1}
\textbf{(ST3)}:\qquad \normtwobig{\bx^\toptone - \bx^\toptzero} < \varepsilon_{1} \qquad \text{or} \qquad f(\bx^\toptzero ) - f(\bx^\toptone) < \varepsilon_{2}.
\end{equation}
We must emphasize that even if \eqref{equation:des_stopcri1} is fulfilled with small $ \varepsilon_{1} $ and $ \varepsilon_{2} $, it does not guarantee that  $ \normtwobig{\be^{\toptzero}} $ or $ f(\bx^\toptzero ) - f(\bx^{*}) $ are small.

Another form of convergence, mentioned earlier in this chapter, is $ \nabla f (\bx^\toptzero ) \rightarrow \bzero $ for $ t \rightarrow \infty $ (Theorem~\ref{theorem:fermat_fist_opt}). This leads to another commonly used stopping criterion:
\begin{equation}\label{equation:des_stopcri2}
\textbf{(ST4)}:\qquad \normtwobig{\nabla f(\bx^\toptzero )} < \varepsilon_{3},
\end{equation}
which is included in many implementations of descent methods.

Another useful approach involves leveraging the property of converging function values. 
The quadratic approximation (Theorem~\ref{theorem:quad_app_theo}) of $ f $ at $ \bx^{*} $ is
$$
f(\bx^\toptzero ) \approx f(\bx^{*}) + (\bx^\toptzero  - \bx^{*})^{\top} \nabla f(\bx^{*}) + \frac{1}{2}(\bx^\toptzero  - \bx^{*})^{\top} \nabla^2 f(\bx^{*})(\bx^\toptzero - \bx^{*}).
$$
Since $ \bx^{*} $ is a local minimizer, we have $ \nabla f(\bx^{*}) = \bzero $ and $ \bH^{*} \triangleq \nabla^2 f(\bx^{*}) $ is positive semidefinite (Theorem~\ref{theorem:second_nec_loca}). This simplifies to:
$
f(\bx^\toptzero ) - f(\bx^{*}) \approx \frac{1}{2}(\bx^\toptzero  - \bx^{*})^{\top} \bH^{*}(\bx^\toptzero  - \bx^{*}).
$
Thus, another stopping criterion can be defined as:
$$
\textbf{(ST5)}:\qquad \frac{1}{2}(\bx^\toptone - \bx^\toptzero )^{\top} \bH^\toptzero(\bx^\toptone - \bx^\toptzero ) < \varepsilon_{4} \quad \text{with} \quad \bx^\toptzero  \approx \bx^{*}. 
$$
Here, $ \bx^\toptzero  - \bx^{*} $ is approximated by $ \bx^\toptone - \bx^\toptzero  $ and $ \bH^{*} $ is approximated by
$
\bH^\toptzero \triangleq \nabla^2 f(\bx^\toptzero ).
$


In the following sections, we delve into a detailed exploration of the gradient descent method, examining its variations and adaptations from different perspectives. This comprehensive analysis aims to provide a deeper understanding of the algorithm, its formulations, challenges, and practical applications.








%Considering the condition $\normtwo{\bd}=1$ for positive $\eta_t$, we formulate the descent search as:
%$$
%\bd=\mathop{\arg \min}_{\normtwo{\bd}=1} f(\bx^\toptzero + \eta_t \bd) \approx\mathop{\arg \min}_{\normtwo{\bd}=1}
%\left\{f(\bx^\toptzero ) + \eta_t \bd^\top \nabla  f(\bx^\toptzero )\right\}.
%$$
%This is known as the \textit{greedy search}. This process leads to the optimal $\bd$ determined by
%$$
%\bd = -\frac{\nabla f(\bx^\toptzero )}{\normtwobig{\nabla f(\bx^\toptzero )}},
%$$
%i.e., $\bd$ lies in the opposite direction of $\nabla f(\bx^\toptzero )$. Consequently, the update for $\bx^\toptone$ is reasonably expressed as:
%$$
%\bx^\toptone =\bx^\toptzero + \eta_t \bd = \bx^\toptzero - \eta_t \frac{\nabla f(\bx^\toptzero)}{\normtwobig{\nabla f(\bx^\toptzero )}},
%$$
%which is usually called the \textit{gradient descent}, as aforementioned. If we further absorb the denominator into the stepsize $\eta_t$, the gradient descent can be simplified to the trivial way:
%$$
%\bx^\toptone = \bx^\toptzero - \eta_t {\nabla f(\bx^\toptzero)}.
%$$
%Suppose we want to approximate $\bx^\toptone$ by a linear update on $\bx^\toptzero$, the expression takes the following form:
%$$
%\bx^\toptone = \bx^\toptzero + \eta_t \bd.
%$$
%The problem now revolves around finding a solution for $\bd$ to minimize the expression:
%$$
%\bd=\mathop{\arg \min}_{\bd} f(\bx^\toptzero + \eta_t \bd) .
%$$



\index{Greedy search}
\index{Non-Euclidean gradient descent}
\index{Dual norm}
\section{Gradient Descent by Greedy Search and Variants}\label{section:als-gradie-descent-taylor}

We now consider  the \textit{greedy search} method such that $\bx^\toptone    \leftarrow \mathop{\arg \min}_{\bx^\toptzero} f(\bx^\toptzero)$ under some mild conditions. 
The linear approximation theorem (Theorem~\ref{theorem:linear_approx}) shows that 
\begin{equation}\label{equation:gd_gree_tay}
f(\bx^\toptzero + \eta \bd) = f(\bx^\toptzero)+ \eta \bd^\top \nabla  f(\bx^\toptzero )+ \mathcalO(\normtwo{\eta \bd}^2).
\end{equation}
For small values of  $\eta$, the term $\mathcalO(\normtwo{\eta \bd}^2)$ becomes negligible compared to the middle term. Therefore, we can approximate $f(\bx^\toptzero  + \eta \bd)$ as
\begin{equation}\label{equation:gd_gree_approx}
f(\bx^\toptzero + \eta \bd) \approx f(\bx^\toptzero ) + \eta \bd^\top \nabla  f(\bx^\toptzero ),
\end{equation}
when $\eta$ is sufficiently small. 
The second term on the right-hand side, $ \bd^\top \nabla  f(\bx^\toptzero ) $, is the \textit{directional derivative} of $ f $ at $ \bx^\toptzero $ in the direction $ \bd $. 
To reiterate, it indicates the approximate change in $ f $ for a small step $ \bd $. The step $ \bd $ is a descent direction if the directional derivative is negative.



To address how to choose $\bd$ to make the directional derivative as negative as possible, note that since directional derivative $ \bd^\top \nabla  f(\bx^\toptzero ) $ is linear in $ \bd $, it can be made arbitrarily negative by increasing $ \bd $ (provided $ \bd $ is a descent direction, i.e., $ \bd^\top \nabla  f(\bx^\toptzero ) < 0 $). To make this question meaningful, we must limit the size of $ \bd $, or normalize by its length.

Let $ \norm{\cdot} $ be any norm on $ \real^n $. We define a \textit{normalized greedy descent direction} (with respect to the norm $ \norm{\cdot} $) as
\begin{equation}\label{equation:norma_greedes}
\bd_{\text{ngd}}^\toptzero \in \argmin_{\bd} \left\{ \bd^\top \nabla  f(\bx^\toptzero ) \text{ s.t. } \norm{\bd} = 1 \right\}.
\end{equation}
(Note that there may be multiple minimizers.) A normalized greedy descent direction $ \bd_{\text{ngd}}^\toptzero $ is a step of unit norm that provides the largest decrease in the linear approximation of $ f $.
By the definition of the dual norm \eqref{equation:dual_norm_equa}, it follows that $\innerproduct{\bd_{\text{ngd}}^\toptzero, \nabla  f(\bx^\toptzero )} =-\norm{\nabla  f(\bx^\toptzero )}_{*}$, where $\norm{\cdot}_{*}$ denotes the dual norm.

Since the problem in \eqref{equation:norma_greedes} can  equivalently be stated using the constraint $\norm{\bd}\leq 1$, $\bd_{\text{ngd}}^\toptzero$ also lies within the set of primal counterparts of $\nabla  f(\bx^\toptzero ) $, whose existence is shown in Definition~\ref{definition:set_primal}.


It is also convenient to consider an unnormalized greedy descent step $\bd_{\text{ugd}}^\toptzero$ by scaling the normalized greedy descent direction in a particular way:
\begin{equation}\label{equation:unnorma_greedes}
\bd_{\text{ugd}}^\toptzero \triangleq \norm{\nabla f(\bx^\toptzero)}_{*} \bd_{\text{ngd}}^\toptzero.
\end{equation}
The reason for this particular unnormalization is that it aligns with the negative gradient (the steepest descent direction) when the underlying norm is the $\ell_2$ norm.
 Note that for the greedy descent step, we have
$$
\nabla f(\bx^\toptzero)^\top \bd_{\text{ugd}}^\toptzero = \norm{\nabla f(\bx^\toptzero)}_{*} \nabla f(\bx^\toptzero)^\top \bd_{\text{ngd}}^\toptzero = -\norm{\nabla f(\bx^\toptzero)}_{*}^2.
$$
When exact line search is used (see Section~\ref{section:line_search}),  scale factors in the descent direction do not affect the outcome, so either the normalized or unnormalized direction can be used.


\paragrapharrow{Greedy search for $\ell_2$ norm.}
If we take the norm $\norm{\cdot}$ to be the $\ell_2$ norm, we find that the greedy descent direction in \eqref{equation:norma_greedes} is simply the negative gradient, i.e., 
\begin{equation}\label{equation:greedy_l2_ngdugd}
\bd_{\text{ngd}}^\toptzero = - \frac{\nabla f(\bx^\toptzero )}{\normtwobig{\nabla f(\bx^\toptzero )}}
\qquad \text{and}\qquad 
\bd_{\text{ugd}}^\toptzero = -\nabla f(\bx^\toptzero ).
\end{equation}
The greedy descent method for the $\ell_2$ norm coincides with the gradient descent method (or the steepest descent method).
The above equality also shows that the unnormalized greedy search direction  corresponds to the negative gradient direction or the steepest descent direction.

\paragrapharrow{Greedy search for $\ell_1$ norm.}
As another example,  consider the greedy descent method for the $\ell_1$ norm. A normalized greedy descent direction can be characterized as
$$
\bd_{\text{ngd}}^\toptzero \in \argmin_{\bd} \left\{ \bd^\top \nabla  f(\bx^\toptzero ) \text{ s.t. } \normone{\bd} \leq 1 \right\}.
$$
We use `$\in$' since the solution of the problem may not be unique.
Let $i$ be any index for which $\norminf{\nabla f(\bx^\toptzero)} = \abs{(\nabla f(\bx^\toptzero))_i}$. 
By the definition of the $\ell_\infty$ norm and the dual norm (\eqref{equation:l_p_norm} and \eqref{equation:dual_norm_equa}), a normalized greedy descent direction $\bd_{\text{ngd}}^\toptzero$ for the $\ell_1$ norm is given by
$$
\bd_{\text{ngd}}^\toptzero = -\sign\left(\frac{\partial f}{\partial x_i}(\bx^\toptzero)\right) \be_i,
$$
where $\be_i$ is the $i$-th  unit basis vector (see Example~\ref{example:set_primal_count}). An unnormalized greedy descent step is then
$$
\bd_{\text{ugd}}^\toptzero = \bd_{\text{ngd}}^\toptzero \norminf{\nabla f(\bx^\toptzero)} = -\frac{\partial f}{\partial x_i} (\bx^\toptzero) \be_i.
$$
which is a descent direction since $\innerproduct{\bd_{\text{ugd}}^\toptzero, \nabla f(\bx^\toptzero)}<0$ (assuming $\nabla f(\bx^\toptzero) \neq\bzero$).
Thus, the normalized greedy descent step in the $\ell_1$ norm can always be chosen to be a (positive or negative) standard basis vector, representing the coordinate axis direction along which the approximate decrease in $f$ is greatest.
Note that the index for which $\norminf{\nabla f(\bx^\toptzero)} = \abs{(\nabla f(\bx^\toptzero))_i}$ may not be unique (see Example~\ref{example:set_primal_count}). In such cases, a convex combination of these descent directions can be used as the final descent direction.

The greedy descent algorithm in the $\ell_1$ norm has a  natural interpretation: At each iteration we select a component of $\nabla f(\bx^\toptzero)$ with maximum absolute value (though the component may not be unique), and then decrease or increase the corresponding component of $\bx^\toptzero$, according to the sign of $(\nabla f(\bx^\toptzero))_i$. The algorithm is sometimes called a \textit{coordinate-descent algorithm} because only one component of the variable $\bx$ is updated at each iteration, potentially simplifying or even trivializing the line search.

\paragrapharrow{Greedy search for $\bQ$-norm.}
We further consider the $\bQ$-norm in \eqref{equation:q_norm}: 
$
\norm{\bx}_{\bQ} = (\bx^\top \bQ \bx)^{1/2} = \normtwo{\bQ^{1/2} \bx}
$
for any $\bx\in\real^n$,
where $\bQ$ is positive definite. The normalized greedy descent direction is given by
$$
\begin{aligned}
\bd_{\text{ngd}}^\toptzero 
&=\argmin_{\bd} \left\{ \bd^\top \nabla  f(\bx^\toptzero ) \text{ s.t. } \norm{\bd}_{\bQ} \leq 1 \right\}\\
&=-\normtwo{\bQ^{-1/2} \nabla f(\bx^\toptzero)}^{-1/2}\bQ^{-1} \nabla f(\bx^\toptzero). %\left[\nabla f(\bx^\toptzero)^\top \bQ^{-1} \nabla f(\bx^\toptzero)\right]^{-1/2}  
\end{aligned}
$$
This can be solved using the KKT conditions or the definition of the dual norm.
The dual norm is given by $\norm{\bx}_{*} = \normtwo{\bQ^{-1/2} \bx} =\norm{\bx}_{\bQ^{-1}}$ for any $\bx\in\real^n$, so the greedy descent step with respect to $\norm{\cdot}_{\bQ}$ is given by
\begin{equation}\label{equation:qnorm_ugd}
	\bd_{\text{ugd}}^\toptzero = -\bQ^{-1} \nabla f(\bx^\toptzero).
\end{equation}
This is a descent direction by Theorem~\ref{theorem:uncons_des_dir}.


\paragrapharrow{Change of variables in $\bQ$-norm.}

An interesting alternative interpretation of the greedy descent direction $\bd_{\text{ugd}}^\toptzero $ is as the gradient search direction after applying a change of coordinates to the problem. Let $\widetildebx \triangleq \bQ^{1/2} \bx$; thus, $\norm{\bx}_{\bQ} = \normtwo{\widetildebx}$. Using this change of coordinates, we can solve the original problem of minimizing $f$ by solving the equivalent problem of minimizing the function $\widetildef : \real^n \rightarrow \real$, given by
$$
\widetildef(\widetildebx) \triangleq f(\bQ^{-1/2} \widetildebx) = f(\bx).
$$
If we apply the gradient method to $\widetildef$, the search direction at a point $\widetildebx^\toptzero$ (which corresponds to the point $\bx^\toptzero = \bQ^{-1/2} \widetildebx^\toptzero$ for the original problem) is
$$
\widetildebd^\toptzero = -\nabla \widetildef(\widetildebx^\toptzero) = -\bQ^{-1/2} \nabla f(\bQ^{-1/2} \widetildebx^\toptzero) = -\bQ^{-1/2} \nabla f(\bx^\toptzero).
$$
Since $\widetildebx = \bQ^{1/2} \bx$ by definition, the search direction in the original space is obtained by mapping $\widetildebd^\toptzero$ back using $\bQ^{-1/2}$: 
$$
\bd^\toptzero = \bQ^{-1/2} \widetildebd^\toptzero = -\bQ^{-1} \nabla f(\bx^\toptzero)
$$
which corresponds to the unnormalized greedy search direction in \eqref{equation:qnorm_ugd}. In other words, the greedy descent method in the $\bQ$-norm $\norm{\cdot}_{\bQ}$ can be thought of as the gradient method applied to the problem after the change of variables $\widetildebx^\toptzero = \bQ^{1/2} \bx^\toptzero$ for each iteration $t$.







\section{Geometrical Interpretation of Gradient Descent} 
\begin{lemma}[Direction of Gradients]\label{lemm:direction-gradients}
An important fact is that gradients are orthogonal to level curves (also known as level surfaces).
\end{lemma}
\begin{proof}[of Lemma~\ref{lemm:direction-gradients}: Informal]
To prove this, we need to show that the gradient is orthogonal to the tangent of the level curve. Let's start with the two-dimensional case. Suppose the level curve has the form $f(x,y)=c$. 
This equation implicitly defines a relationship  between $x$ and $y$, such that $y=y(x)$, where $y$ can be considered as a function of $x$. Therefore, the level curve can be expressed as:
$$
f(x, y(x)) = c.
$$
Applying the chain rule gives us:
$$
\frac{\partial f}{\partial x} \underbrace{\frac{dx}{dx}}_{=1} + \frac{\partial f}{\partial y} \frac{dy}{dx}=0.
$$
This implies that the gradient is perpendicular to the tangent vector:
$$
\left\langle \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right\rangle
\cdot 
\left\langle \frac{dx}{dx}, \frac{dy}{dx}\right\rangle=0.
$$
Now, let's generalize this to higher dimensions. Consider a level set defined by a vector $\bx\in \real^n$: $f(\bx) = f(x_1, x_2, \ldots, x_n)=c$. Each variable $x_i$ can be regarded as a function of a parameter $t$ along the level set $f(\bx)=c$: $f(x_1(t), x_2(t), \ldots, x_n(t))=c$. Differentiating both sides with respect to $t$ using the chain rule yields:
$$
\frac{\partial f}{\partial x_1} \frac{dx_1}{dt} + \frac{\partial f}{\partial x_2} \frac{dx_2}{dt}
+\ldots + \frac{\partial f}{\partial x_n} \frac{dx_n}{dt}
=0.
$$
Therefore, the gradients is perpendicular to the tangent in the $n$-dimensional case:
$$
\left\langle \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right\rangle
\cdot 
\left\langle \frac{dx_1}{dt}, \frac{dx_2}{dt}, \ldots, \frac{dx_n}{dt}\right\rangle=0.
$$
This completes the proof.
\end{proof}
The lemma above provides a profound geometric interpretation of gradient descent. In the process of minimizing a (convex) function $f(\bx)$, gradient descent strategically moves in the direction opposite to the gradient, which reduces the loss. Figure~\ref{fig:alsgd-geometrical} illustrates a two-dimensional scenario where  $-\nabla f(\bx)$ guides the decrease in loss for a (convex) function $f(\bx)$. 

\begin{figure}[h]
\centering   
\vspace{-0.25cm}  
\subfigtopskip=2pt  
\subfigbottomskip=2pt  
\subfigcapskip=-5pt  
\subfigure[A two-dimensional convex function $f(\bx)$.]{\label{fig:alsgd1}
\includegraphics[width=0.47\linewidth]{./imgs/quadratic_singular.pdf}}
\subfigure[$f(\bx)=c$ is a constant.]{\label{fig:alsgd2}
\includegraphics[width=0.44\linewidth]{./imgs/alsgd2.pdf}}
\caption{Figure~\ref{fig:alsgd1} shows a convex function surface plot and its contour plot (\textcolor{mylightbluetext}{blue}=low, \textcolor{mydarkyellow}{yellow}=high), where the upper graph represents  the surface plot, and the lower one is its projection (i.e., contour). Figure~\ref{fig:alsgd2}: $-\nabla f(\bx)$ directs the reduction in loss for the convex function $f(\bx)$.}
\label{fig:alsgd-geometrical}
\end{figure}



\index{Regularization}
\section{Geometrical Interpretation of Regularization}\label{section:geom_int_regu}
\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{./imgs/alsgd3.pdf}
\caption{Constrained gradient descent with $\bx^\top\bx\leq C$. The \textcolor{mydarkgreen}{green} vector $\bw$ is the projection of $\bv_1$ into $\bx^\top\bx\leq C$, where $\bv_1$ is the component of $-\nabla f(\bx)$ perpendicular to $\bx_1$. The right picture shows the next step after the update in the left picture. $\bx^*$ denotes the optimal solution of \{$\min f(\bx)$\}.}
\label{fig:alsgd3}
\end{figure}
\textit{Regularization} is a machine learning technique employed to prevent overfitting and improve model generalization. Overfitting occurs when a model is overly complex and fits the training data too closely, resulting in poor performance on  unseen data. 
To mitigate this issue, regularization introduces a constraint or a penalty term into the loss function used for model optimization, discouraging the development of overly complex models. 
This creates  a trade-off between having a simple, generalizable model and fitting the training data well. 
Common types of regularization include $\ell_1$ regularization, $\ell_2$ regularization (Tikhonov regularization), and elastic net regularization (a combination of $\ell_1$ and $\ell_2$ regularizations). 
Regularization finds extensive applications in machine learning algorithms such as linear regression, logistic regression, and neural networks.


Gradient descent also reveals the geometric significance of regularization. To avoid confusion, we denote the loss function without regularization by $f(\bz)$ and the loss with the $\ell_2$ regularization by $F(\bx) \triangleq f(\bx)+\lambda \normtwo{\bx}^2$, where $f(\bx): \real^n \rightarrow \real$ (this notation is exclusive to this subsection). When minimizing $f(\bx)$, the descent method searches for a solution in $\real^n$. 
However, in machine learning, an exhaustive search across the entire space may lead to overfitting. A partial remedy involves searching within a subset of the vector space, such as searching in $\bx^\top\bx < C$ for some constant $C$. That is,
$$
\argmin_{\bx} \, \big\{f(\bx) \gap  \text{s.t.} \gap \bx^\top\bx\leq C\big\}.
$$
This constrained search helps prevent overfitting by introducing regularization through the addition of a penalty term in the optimization process.
In the previous discussion, a basic gradient descent approach proceeds in the direction of $-\nabla f(\bx)$,  updating $\bx$ by $\bx^+\leftarrow \bx-\eta \nabla f(\bx)$ for a small stepsize $\eta$. 
When the level curve is $f(\bx)=c_1$ and the descent approach is situated at $\bx=\bx_1$, where $\bx_1$ is the intersection of $\bx^\top\bx=C$ and $f(\bx)=c_1$, the descent direction $-\nabla f(\bx_1)$ will be perpendicular to the level curve of $f(\bx_1)=c_1$, as shown in the left picture of Figure~\ref{fig:alsgd3}. 
However, if we further restrict that the optimal value can only be in the subspace $\bx^\top\bx\leq C$, the trivial descent direction $-\nabla f(\bx_1)$ will lead $\bx_2=\bx_1-\eta \nabla f(\bx_1)$ outside of $\bx^\top\bx\leq C$. 

To address this,  the step $-\nabla f(\bx_1)$ is decomposed  into 
$$
-\nabla f(\bx_1) = a\bx_1 + \bv_1,
$$ 
where $a\bx_1$ is the component perpendicular to the curve of $\bx^\top\bx=C$, and $\bv_1$ is the component parallel to the curve of $\bx^\top\bx=C$. Keeping only the step $\bv_1$, then the update 
$$
\bx_2 = \text{project}(\bx_1+\eta \bv_1) = \text{project}\left(\bx_1 + \eta
\underbrace{(-\nabla f(\bx_1) -a\bx_1)}_{\bv_1}\right)\footnote{where the project($\bx$) operator
will project the vector $\bx$ to the closest point inside $\bx^\top\bx\leq C$. Notice here the direct update $\bx_2 = \bx_1+\eta \bv_1$ can still make $\bx_2$ outside the curve of $\bx^\top\bx\leq C$.}
$$ 
will lead to a smaller loss from $f(\bx_1)$ to $f(\bx_2)$ while still satisfying the constraint $\bx^\top\bx\leq C$. 
This technique is known as the \textit{projected gradient descent} (Section~\ref{section:pgd}). It is not hard to see that the update $\bx_2 = \text{project}(\bx_1+\eta \bv_1)$ is equivalent to finding a vector $\bw$ (depicted by the \textcolor{mydarkgreen}{green} vector in the left panel of Figure~\ref{fig:alsgd3}) such that $\bx_2=\bx_1+\bw$ lies inside the curve of $\bx^\top\bx\leq C$. Mathematically, the vector $\bw$ can be obtained as $-\nabla f(\bx_1) -2\lambda \bx_1$ for some $\lambda$, as shown in the middle panel of Figure~\ref{fig:alsgd3}. This aligns with the negative gradient of $F(\bx)=f(\bx)+\lambda\normtwo{\bx}^2$ such that 
$$
-\nabla F(\bx_1) = -\nabla f(\bx_1) - 2\lambda \bx_1,
$$
and 
$$
\begin{aligned}
\bw &= -\nabla F(\bx_1) 
\qquad \implies \qquad 
\bx_2 = \bx_1+ \bw =\bx_1 -  \nabla F(\bx_1).
\end{aligned}
$$
In practice, a small stepsize $\eta$ can be applied to prevent crossing  the curve boundary of $\bx^\top\bx\leq C$:
$$
\bx_2  =\bx_1 -  \eta\nabla F(\bx_1).
$$
%which is exactly what we have discussed in Section~\ref{section:regularization-extention-general}, the regularization term.




\index{Line search}
\index{Steepest descent}
\index{Exact line search method} 
\index{Inexact line search method} 
\index{Soft line search method}
\index{Greedy search}
\section{Descent Methods with Line Search}\label{section:line_search}
%An intuitive analogy to understand gradient descent is to imagine the path of a river starting from a mountain peak and flowing downhill to reach the lowest point at its base.
%Similarly, the goal of gradient descent is to find the lowest point in the landscape defined by the objective function $f(\bx)$, where $\bx$ represents  a $n$-dimensional input variable.


For the unconstrained optimization problem (P1) in Definition~\ref{definition:opt_probs_all}, we will use the process of finding the minimum value of  $f(\bx)$ as an analogy to going downhill. Imagine a person standing at some point $\bx$, where $f(\bx)$ represents the height at that location. To find the lowest point, two things need to be determined at point $\bx$: first, which direction to take for the next step; second, how far to walk along that direction before choosing the next downhill direction. Once these factors are determined, the process can be repeated until reaching the minimum value of $f(\bx)$.

The second phase is then solved using line search strategies.
The mathematical description of the line search method is as follows: Given the current iteration point $\bx^\toptzero$, first determine the direction vector $\bd^\toptzero$ using some method, then determine the positive scalar $\eta_t$. The next iteration point can be written as:
$ \bx^\toptone \leftarrow \bx^\toptzero + \eta_t \bd^\toptzero.  $
We call $\bd^\toptzero$ the \textit{search direction} or \textit{descent direction} at the iteration point $\bx^\toptzero$, and $\eta_t$ the corresponding stepsize. 
As mention in the previous sections, we require that $\bd^\toptzero$ is a descent direction, i.e., $\innerproduct{\bd^\toptzero, \nabla f(\bx^\toptzero)} < 0$ (Definition~\ref{definition:uncons_des_direct}, assume $\nabla f(\bx^\toptzero)\neq \bzero$). This descent property ensures that the function value decreases along this direction. The key to the line search method is determining a good direction $\bd^\toptzero \in \real^n$ and an appropriate stepsize $\eta_t$.

Gradient descent, steepest descent, greedy search methods are some examples of how to select the search direction $\bd^\toptzero$.
In this section, we will focus on selecting $\eta_t$.  
While methods for selecting $\bd^\toptzero$ vary widely, techniques for choosing $\eta_t$ are similar across different algorithms. First, construct the auxiliary function
$ \phi(\eta) = f(\bx^\toptzero + \eta \bd^\toptzero), $
where $\bd^\toptzero$ is the given descent direction, and $\eta > 0$ is the independent variable of this auxiliary function. The geometric meaning of $\phi(\eta)$ is straightforward: it is the restriction of the objective function $f(\bx)$ to the ray $\{\bx^\toptzero + \eta \bd^\toptzero \mid \eta > 0\}$. Note that $\phi(\eta)$ is a univariate function, making it relatively easy to study.

The goal of line search is to select an appropriate $\eta_t$ such that $\phi(\eta_t)$ is minimized as much as possible. However, this task is still challenging: $\eta_t$ should ensure sufficient decrease in $f$, while also minimizing computational effort. 
Balancing these aspects is crucial. A natural approach is to find $\eta_t$ such that
$$ 
\eta_t = \arg\min_{\eta > 0} \phi(\eta), 
$$
i.e., $\eta_t$ is the optimal stepsize. This line search algorithm is called the \textit{exact line search method}. Although the exact line search method often leads to solutions under most circumstances, it typically requires significant computation to determine $\eta_t$, making it impractical for many applications.
The \textit{inexact line search method} or \textit{soft line search method}, on the other hand, does not require $\eta_t$ to be the minimum point of $\phi(\eta)$, but only requires $\phi(\eta_t)$ to satisfy certain inequality conditions. Due to its simpler structure, the inexact line search method is more commonly used in practice. We will now introduce the details of these algorithms.

\subsection{Exact Line Search Conditions}\label{section:exact_line_search}
In the preceding sections, we derived the gradient descent, where the update step at step $t$ is $ -\eta_t\bg^\toptzero\triangleq-\eta_t \nabla  f(\bx^\toptzero)$, and the learning rate $\eta_t$ controls how large of a step to take in the direction of negative gradient. 
Exact line search is a method that directly determines the optimal learning rate to achieve the most significant improvement in the objective function. Formally, at the $t$-th step of gradient descent, exact line search solves the following problem:
$$
\eta_t = \underset{\eta}{\arg\min}\,\,   f(\bx^\toptzero - \eta \bg^\toptzero).
$$
After performing the gradient update $\bx^\toptone \leftarrow \bx^\toptzero - \eta_t \bg^\toptzero$, the gradient is computed at $\bx^\toptone$ for the next step $t+1$. More generally, let $\bd^\toptzero$ be the descent direction; then, the descent method with line search can be described by:
$$
\eta_t = \underset{\eta}{\arg\min}\,\,   f(\bx^\toptzero + \eta \bd^\toptzero).
$$
In line search methods, the loss function at iteration $t$ can be  expressed in terms of $\eta$ as
$
\phi(\eta) \triangleq f(\bx^\toptzero + \eta \bd^\toptzero).
$
Consequently, the problem can be formulated as finding
$$
\eta_t = \underset{\eta}{\arg\min} \,\, f(\bx^\toptzero + \eta \bd^\toptzero)=\underset{\eta}{\arg\min} \,\,  \phi(\eta).
$$
A necessary condition on $\eta_t$ is $\phi^\prime(\eta_t)=0$, as stated by Theorem~\ref{theorem:fermat_fist_opt}.
Since the derivative of $\phi$ is  $\phi^\prime(\eta) = \innerproduct{\nabla f(\bx^\toptzero + \eta \bd^\toptzero), \bd^\toptzero}$, this shows that either $\nabla f(\bx^\toptzero + \eta_t \bd^\toptzero) = \bzero$, , indicating that we have found a stationary point for $f$, or if $\nabla f(\bx^\toptzero + \eta_t \bd^\toptzero) \neq \bzero$, then  $\phi^\prime(\eta_t)=0$ indicates 
$$
\innerproduct{\nabla f(\bx^\toptone),  \bd^\toptzero} = 0.
$$
The two cases mean that the exact line search will stop at a point where the local gradient is orthogonal to the search direction. We formalize this finding in the following lemma, which will be crucial in the development of conjugate gradient methods (Section~\ref{section:conjugate-descent}).
\begin{lemma}[Orthogonality in Exact Line Search]\label{lemm:linear-search-orghonal}
The gradient of optimal point $\bx^\toptone=\bx^\toptzero + \eta_t \bd^\toptzero $ of an exact line search is orthogonal to the current update direction $\bd^\toptzero$:
$$
\innerproduct{\nabla f(\bx^\toptone), \bd^\toptzero} = 0.
$$
\end{lemma}
%\begin{proof}[of Lemma~\ref{lemm:linear-search-orghonal}]
%Suppose $\nabla f(\bx^\toptone)^\top \bd^\toptzero \neq 0$, then there exists a $\delta$ and it follows by Taylor's formula (Section~\ref{section:differ_calc}) that
%\begin{equation}\label{equation:orthogonal-line-search}
%f(\bx^\toptzero +\eta_t \bd^\toptzero \pm \delta \bd^\toptzero) \approx f(\bx^\toptzero +\eta_t \bd^\toptzero)\pm \delta \bd^\toptzeroTOP \nabla f(\bx^\toptzero +\eta_t \bd^\toptzero).
%\end{equation}
%Since $\bx^\toptzero +\eta_t \bd^\toptzero$ is the optimal move such that $ f(\bx^\toptzero +\eta_t \bd^\toptzero) \leq f(\bx^\toptzero +\eta_t \bd^\toptzero \pm \delta \bd^\toptzero)$ and $\delta \neq 0$. This leads to the claim 
%$$
%\bd^\toptzeroTOP \nabla f(\bx^\toptzero +\eta_t \bd^\toptzero)=0.$$
%We complete the proof.
%\end{proof}




When $\eta=0$, we have
\begin{equation}\label{equation:linesearc-eta0}
	\phi^\prime (0) = \innerproduct{\bd^\toptzero,  \bg^\toptzero} \leq 0.
\end{equation}
A crucial property in typical line search settings is that the loss function
$\phi(\eta)$, when expressed in terms of $\eta$, is often a unimodal function. 
If a value $\eta_{\max}$ is identified such that $\phi^\prime(\eta_{\max}) > 0$, the optimal learning rate is then in the range of $[0, \eta_{\max}]$. Line search methods are used to find the optimal $\eta_t$ within this range, satisfying the optimal condition $\phi^\prime(\eta_t)=0$.
Following this rule, we then introduce conditions for soft line search algorithms and we will introduce some prominent line search approaches: polynomial interpolation line search, bisection line search, golden-section line search, and Armijo condition search in the following sections.



\subsection{Soft Line Search Conditions}

In the early days of optimization, exact line search was dominant. Now, soft line search is used more frequently, and new methods rarely require exact line search.
One advantage of soft line search over exact line search is its speed. If the initial guess for the step length is a rough approximation to the minimizer in the given direction, the line search can terminate immediately if certain mild criteria are met. While the result of exact line search is typically a good approximation, it often requires additional function evaluations, which can make descent methods with exact line search less efficient overall. Despite this, exact line search can sometimes find local minima in fewer iterations compared to soft line search.

At the start of an iteration with a descent method, where $\bx^\toptzero$ is far
from the solution $\bx^*$, the imprecision of soft line search results is less critical. This further supports the preference for soft line search in practice. 

In the soft line search method, the selection of $\eta_t$ must meet certain requirements known as line search conditions. The appropriateness of these conditions directly affects the convergence of the algorithm. Choosing inappropriate line search conditions can lead to failure in convergence. We illustrate this with an example.



\begin{example}[Divergence of Descent Methods]
Consider a one-dimensional unconstrained optimization problem
$$
\min_{x} \quad f(x) = x^2,
$$
with the initial point $ x^{(1)} = 1 $. Since the problem is one-dimensional, the descent direction only has two possibilities: $\{-1, 1\}$. We choose $ d^\toptzero = -\sign(x^\toptzero) $ and require that the stepsize satisfies the condition of monotonic decrease of the function value at the iteration point, i.e., $ f(x^\toptzero + \eta_t d^\toptzero) < f(x^\toptzero) $. Consider the following two stepsizes:
$$
\eta_{t,1} = \frac{1}{3^{t+1}}
\qquad \text{and}\qquad
\eta_{t,2} = 1 + \frac{2}{3^{t+1}},
$$
Performing some algebra yields the sequences:
$$
x_1^\toptzero = \frac{1}{2} \left(1 + \frac{1}{3^t}\right)
\qquad \text{and}\qquad
\qquad x_2^\toptzero = \frac{(-1)^t}{2} \left(1 + \frac{1}{3^t}\right).
$$
Clearly, both sequences $\{f(x_1^\toptzero)\}$ and $\{f(x_2^\toptzero)\}$ are  monotonically decreasing. However, the sequence $\{x_1^\toptzero\}$ converges to a point that is not a minimum point, while the sequence $\{x_2^\toptzero\}$ oscillates around the origin and does not converge.
\end{example}

The reason for the divergence in the above example is insufficient decrease in the function value $ f(x^\toptzero) $ during the iteration process, preventing the algorithm from converging to the minimum point. To avoid this issue, more reasonable soft line search conditions must be introduced to ensure convergence.


\subsection*{Armijo Condition and Backtracking Methods}

We first introduce the \textit{Armijo condition}, which is a commonly used soft line search condition.
The purpose of the Armijo condition is to ensure that each iteration sufficiently decreases the function value.
\begin{definition}[Armijo Condition/Sufficient Decrease Condition \citep{armijo1966minimization}]\label{definition:arminjo_condition}
Let $f:\real^n\rightarrow\real$ be a differentiable function, and let $ \bd^\toptzero $ denote  a  descent direction at the point $ \bx^\toptzero $. 
Then the stepsize $\eta=\eta_t$ satisfies the \textit{Armijo condition (a.k.a., the sufficient decrease condition)} if
\begin{equation}\label{equation:arminjo_condition}
f(\bx^\toptzero + \eta \bd^\toptzero) \leq f(\bx^\toptzero) + c_1 \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}, 
\end{equation}
where $c_1 \in (0, 1)$ is a fixed constant.
The existence of such a constant $c_1$ is guaranteed by Lemma~\ref{lemma:valid_suff_des_armi}.
\end{definition}

The Armijo condition \eqref{equation:arminjo_condition} has a very intuitive geometric meaning, indicating that the point $(\eta, \phi(\eta))$ must lie below the line
$$
\rho_1(\eta) \triangleq \phi(0) + c_1 \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}, 
\quad
\text{where }\phi(\eta) \triangleq f(\bx^\toptzero + \eta \bd^\toptzero).
$$
As shown in Figure~\ref{fig:descent_armijo}, all points in the shaded area satisfy the Armijo condition. Note that $\bd^\toptzero$ is the descent direction, which means the slope of $\rho_1(\eta)$ is negative. Choosing $\eta$ that satisfies condition \eqref{equation:arminjo_condition} indeed ensures the decrease of the function value. In practical applications, the parameter $c_1$ is usually chosen to be a very small positive number, such as $c_1 = 10^{-4}$, making the Armijo condition  easy to satisfy. However, using only the Armijo condition alone cannot guarantee the convergence of the iteration. This is because $\eta = 0$ trivially satisfies condition \eqref{equation:arminjo_condition}, which means the sequence of iterations does not change. Studying such a stepsize is meaningless. Therefore, the Armijo condition needs to be used in conjunction with other conditions.

In the implementation of optimization algorithms, finding a stepsize that satisfies the Armijo condition is relatively easy. A commonly used algorithm is the \textit{backtracking method}. Given an initial guess $\widehat{\eta}$, the backtracking method continuously reduces the trial stepsize by a factor until it finds the first point that satisfies the Armijo condition  \eqref{equation:arminjo_condition}. Specifically, the backtracking method selects
$$
\eta_t = \gamma^{j} \widehat{\eta},
$$
where
$
j = \min \left\{ i = 0, 1, \cdots \mid f(\bx^\toptzero + \gamma^i \widehat{\eta} \bd^\toptzero) \leq f(\bx^\toptzero) + c_1 \gamma^i \widehat{\eta} \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero} \right\},
$
and $\gamma \in (0, 1)$ is a given constant. The basic process of the backtracking method is shown in Algorithm~\ref{alg:gd_line_search}.

\begin{figure}[h!]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[Armijo condition.]{\label{fig:descent_armijo}
\includegraphics[width=0.48\linewidth]{./imgs/descent_armijo.pdf}}
\subfigure[Goldstein condition. Same as Figure~\ref{fig:descent_goldstein2}.]{\label{fig:descent_goldstein}
\includegraphics[width=0.48\linewidth]{./imgs/descent_goldstein.pdf}}
\caption{The Armijo condition ensures that any point $(\eta, \phi(\eta))$ must lie below the line $\rho_1(\eta)$, while the Goldstein condition ensures that any point must lie between the lines $\rho_1(\eta)$ and $\rho_2(\eta)$.}
\label{fig:descent_armijo_goldstein}
\end{figure}


\begin{algorithm}[h] 
\caption{Descent Method with Backtracking Line Search at Iteration $t$}
\label{alg:gd_line_search}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$ and $t$-th iteration $\bx^\toptzero$; 
\State {\bfseries Input:}  Choose an initial stepsize $\widehat{\eta}$, parameters $\gamma, c_1 \in (0, 1)$;
\State {\bfseries Input:}  Initialize $\eta \leftarrow \widehat{\eta}$;
\While{$f(\bx^\toptzero + \eta \bd^\toptzero) > f(\bx^\toptzero) + c_1 \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}$}
\State Let $\eta \leftarrow \gamma \eta$;
\EndWhile
\State {\bfseries Return:}  $\eta_t \leftarrow \eta$;
\end{algorithmic} 
\end{algorithm}

This algorithm is called the backtracking method because the trial values of $\eta$ are reduced from large to small, ensuring that the output $\eta_t$ is as large as possible while still satisfying the Armijo condition. Additionally, Algorithm~\ref{alg:gd_line_search} will not run indefinitely because $\bd^\toptzero$ is a descent direction.



\subsection*{Goldstein Condition}

When $\eta$ is sufficiently small, the Armijo condition always holds. In practical applications,
to overcome the shortcomings of the Armijo condition, we usually set a lower bound for $\eta$ to prevent the stepsize from being too small. 
Since the Armijo condition only requires that the point $(\eta, \phi(\eta))$ must lie below a certain line, we can also require that this point lies above another line. This is the   \textit{Goldstein condition}.

\begin{definition}[Goldstein Condition \citep{goldstein1965steepest}]\label{definition:goldstein_cond}
Let $f:\real^n\rightarrow\real$ be a differentiable function, and let $ \bd^\toptzero $ denote  a  descent direction at the point $ \bx^\toptzero $. 
Then the stepsize $\eta=\eta_t$ satisfies the \textit{Goldstein condition} if
\begin{subequations}\label{equation:goldsteins}
\begin{align}
f(\bx^\toptzero + \eta \bd^\toptzero) &\leq f(\bx^\toptzero) + c_1 \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}, \label{equation:goldstein1} \\
f(\bx^\toptzero + \eta \bd^\toptzero) &\geq f(\bx^\toptzero) + (1 - c_1) \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero},\label{equation:goldstein2}
\end{align}
\end{subequations}
where $c_1 \in \left(0, \frac{1}{2}\right)$  is a fixed constant. Let 
$
\phi(\eta) \triangleq f(\bx^\toptzero + \eta \bd^\toptzero)$. 
Then the Goldstein condition can  be equivalently stated as
\begin{subequations}\label{equation:goldsteins_all2}
\begin{align}
\phi(\eta) &\leq \phi(0) + c_1 \eta \phi^\prime(0)\triangleq\rho_1(\eta), \label{equation:goldsteinv2_1} \\
\phi(\eta) &\geq \phi(0) + (1 - c_1) \eta \phi^\prime(0)\triangleq\rho_2(\eta).\label{equation:goldsteinv2_2}
	\end{align}
\end{subequations}
\end{definition}

Similarly, the Goldstein condition \eqref{equation:goldsteins} also has an  intuitive geometric interpretation, indicating that the point $(\eta, \phi(\eta))$ must lie between two lines:
$$
\begin{aligned}
\rho_1(\eta) &\triangleq \phi(0) + c_1 \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero},\\
\rho_2(\eta) &\triangleq \phi(0) + (1 - c_1) \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}.
\end{aligned}
$$
As shown in Figure~\ref{fig:descent_goldstein}, all points in the shaded area satisfy the Goldstein condition. We also note that the Goldstein condition effectively prevents excessively small stepsizes $\eta$.
Note that the restriction $c_1 < \frac{1}{2}$ is necessary so that $\rho_2$ lies below $\rho_1$. 
%In fact, if $\phi(\eta)$ is a quadratic function satisfying $\phi^\prime(0) < 0$ and $\phi^{\prime\prime}(0) > 0$, then the global minimizer $\eta^*$ of $\phi$ satisfies
%$$
%\phi(\eta^*) = \phi(0) + \frac{1}{2} \eta^* \phi^\prime(0).
%$$
%Thus, $\eta^*$ satisfies \eqref{equation:goldsteinv2_1} if and only if $c_1 < \frac{1}{2}$. 
%The restriction $c_1 < \frac{1}{2}$ will also finally permit $\eta = 1$ for Newton's method and quasi-Newton methods. 


\begin{figure}[h!]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[Goldstein condition, where $c_1<1$ and $c_2=1-c_1$; same as Figure~\ref{fig:descent_goldstein}.]{\label{fig:descent_goldstein2}
	\includegraphics[width=0.48\linewidth]{./imgs/descent_goldstein.pdf}}
\subfigure[Wolfe condition.]{\label{fig:descent_wolfe}
	\includegraphics[width=0.48\linewidth]{./imgs/descent_wolfe.pdf}}
\caption{The Goldstein condition ensures that any point must lie between the lines $\rho_1(\eta)$ and $\rho_2(\eta)$, while the Wolfe condition ensures that any point must lie below the line $\rho_1(\eta)$ and the slope must be greater than $c_2$ times the initial slope $\phi^\prime(0)$.}
\label{fig:descent_goldstein_wolfe}
\end{figure}


\subsection*{Wolfe Condition}

The Goldstein condition ensures that the function value decreases sufficiently, but it may exclude the optimal function value. As shown in Figure~\ref{fig:descent_goldstein}, the minimum point of the one-dimensional function $\phi(\eta)$ does not necessarily lie within the shaded area that satisfies the Goldstein condition. Therefore, we introduce the \textit{Wolfe condition}.
\begin{definition}[Wolfe Condition]\label{definition:wolfe_cond}
Let $f:\real^n\rightarrow\real$ be a differentiable function, and let $ \bd^\toptzero $ denote  a  descent direction at the point $ \bx^\toptzero $. 
Then the stepsize $\eta=\eta_t$ satisfies the \textit{Wolfe condition} if
\begin{subequations}\label{equation:wolfe_all}
\begin{align}
f(\bx^\toptzero + \eta \bd^\toptzero) &\leq f(\bx^\toptzero) + c_1 \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}, \label{equation:wolfe_1} \\
\innerproduct{\nabla f(\bx^\toptzero + \eta \bd^\toptzero),  \bd^\toptzero} &\geq c_2 \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}, \label{equation:wolfe_2}
\end{align}
\end{subequations}
where $0<c_1 < c_2<1$ are given constants.
Alternatively, let 
$\phi(\eta) \triangleq f(\bx^\toptzero + \eta \bd^\toptzero)$.
%= f(\bx^\toptzero) + \eta\bd^\toptzero \nabla f(\bx^\toptzero) + \frac{1}{2} \bd^\toptzeroTOP\nabla^2 f(\bx^\toptzero) \bd^\toptzero +\mathcalO(\eta^3).
%$$
%Then $\phi^\prime(0) = \bd^\toptzeroTOP \nabla f(\bx^\toptzero) $.
Then the Wolfe condition \eqref{equation:wolfe_all} can be equivalently stated as 
\begin{subequations}\label{equation:wolfe_var}
\begin{align}
\phi(\eta) &\leq \phi(0)+ c_1 \eta \phi^\prime(0)  \triangleq\rho_1(\eta), \label{equation:wolfevar_1} \\
\phi^\prime(\eta) &\geq c_2 \phi^\prime(0). \label{equation:wolfevar_2}
\end{align}
\end{subequations}
\end{definition}

In the Wolfe condition,
\begin{itemize}
\item The first inequality \eqref{equation:wolfe_1} is the Armijo condition, which requires that $\eta_t$ must be sufficiently small to provide a useful decrease in the objective function.
\item The second inequality \eqref{equation:wolfe_2} is the essential requirement of the Wolfe condition. Note that $\innerproduct{\nabla f(\bx^\toptzero + \eta \bd^\toptzero),  \bd^\toptzero}$ is  the derivative of $\phi(\eta)$. The Wolfe condition requires that the slope of the tangent line to $\phi(\eta)$ at $\eta=\eta_t$ cannot be less than $c_2$ times the slope at $\eta = 0$, i.e., $\eta_t$ also needs to be sufficiently large so that we have moved away from the starting tangent of the curve $y=\phi(\eta)$ for $\eta\geq 0$. 
\end{itemize}
As shown in Figure~\ref{fig:descent_wolfe}, all points in the interval shaded area satisfy the Wolfe condition. Note that at the minimum point $\eta^*$ of $\phi(\eta)$, $\phi^\prime(\eta^*) = \innerproduct{\nabla f(\bx^\toptzero + \eta^* \bd^\toptzero),  \bd^\toptzero} = 0$, so $\eta^*$ always satisfies condition \eqref{equation:wolfe_2}. Choosing a smaller $c_1$ can make $\eta^*$ also satisfy condition \eqref{equation:wolfe_1}, i.e., the Wolfe condition includes the exact solution of the line search problem in most cases. In practical applications, the parameter $c_2$ is usually set to 0.9.


%\begin{figure}[h!]
%\centering  
%\vspace{-0.35cm} 
%\subfigtopskip=2pt 
%\subfigbottomskip=2pt 
%\subfigcapskip=-5pt 
%\subfigure[Wolfe condition; same as Figure~\ref{fig:descent_goldstein}.]{\label{fig:strong_descent_goldstein2}
%\includegraphics[width=0.48\linewidth]{./imgs/descent_goldstein.pdf}}
%\subfigure[Wolfe condition.]{\label{fig:strong_descent_wolfe}
%\includegraphics[width=0.48\linewidth]{./imgs/descent_wolfe.pdf}}
%\caption{\textcolor{red}{text} The Goldstein condition ensures that any point must lie between the lines $\rho_1(\eta)$ and $\rho_2(\eta)$, while the Wolfe condition ensures that any point must lie below the line $\rho_1(\eta)$ and the slope must be greater than $c_2$ times the initial slope $\phi^\prime(0)$.}
%\label{fig:strong_descent_goldstein_wolfe}
%\end{figure}
\subsection*{Strong Wolfe Condition}
In addition, a stepsize may satisfy the Wolfe condition without being close to a minimizer of $\phi(\eta)$ (see Figure~\ref{fig:descent_wolfe}). 
The \textit{strong Wolfe condition} modifies the curvature condition to force $\eta_t$ to lie in at least a broad neighborhood of a local minimizer or stationary point of $\phi$. 
\begin{definition}[Strong Wolfe Condition]\label{definition:strong_wolfe_cond}
Let $f:\real^n\rightarrow\real$ be a differentiable function, and let $ \bd^\toptzero $ denote  a  descent direction at the point $ \bx^\toptzero $. 
Then the stepsize $\eta=\eta_t$ satisfies the \textit{strong Wolfe condition} if
\begin{subequations}\label{equation:strong_wolfe_all}
\begin{align}
f(\bx^\toptzero + \eta \bd^\toptzero) &\leq f(\bx^\toptzero) + c_1 \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}, \label{equation:strong_wolfe_1} \\
\abs{\innerproduct{\nabla f(\bx^\toptzero + \eta \bd^\toptzero),  \bd^\toptzero}} &\leq c_2 \abs{\innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}}, \label{equation:strong_wolfe_2}
\end{align}
\end{subequations}
where $0<c_1 < c_2<1$ are given constants.
Alternatively, let 
$\phi(\eta) \triangleq f(\bx^\toptzero + \eta \bd^\toptzero)$.
Then the strong Wolfe condition \eqref{equation:strong_wolfe_all} can be equivalently stated as 
\begin{subequations}\label{equation:strong_wolfe_var}
\begin{align}
\phi(\eta) &\leq \phi(0)+ c_1 \eta \phi^\prime(0)  \triangleq\rho_1(\eta), \label{equation:strong_wolfevar_1} \\
\abs{\phi^\prime(\eta)} &\geq c_2 \abs{\phi^\prime(0)}. \label{equation:strong_wolfevar_2}
\end{align}
\end{subequations}
\end{definition}


\subsection*{Nonmonotone Line Search Conditions}

The three conditions introduced above have a common feature: they generate a monotonic iteration sequence. In practical applications, nonmonotone algorithms sometimes perform better. This requires the use of nonmonotone line search conditions.

\begin{definition}[Grippo Condition \citep{grippo1986nonmonotone}]\label{definition:grippo_condition}
Let $f:\real^n\rightarrow\real$ be a differentiable function, and let $ \bd^\toptzero $ denote  a  descent direction at the point $ \bx^\toptzero $. 
Given a positive integer $ k > 0 $, then the stepsize $\eta=\eta_t$ satisfies the \textit{Grippo condition} if
\begin{equation}
f(\bx^\toptzero + \eta \bd^\toptzero) \leq \max_{0 \leq j \leq \min(t, k)} f(\bx^{(t-j)}) + c_1 \eta \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero},
\end{equation}
where $ c_1 \in (0, 1) $ and $k\geq 0$ are fixed constants. When $k=0$, this reduces to the Armijo condition.
\end{definition}
The Grippo condition is similar to the Armijo condition, but with a key difference. The Armijo condition requires the function value $ f(\bx^\toptone) $ of the next iteration to decrease sufficiently compared to the current iteration's function value $ f(\bx^\toptzero) $. 
In contrast, the Grippo condition only requires the function value of the next step to be lower than the maximum of the previous $ k $ steps' function values. This makes the Grippo condition less restrictive than the Armijo condition and does not require the sequence  $ \{f(\bx^\toptzero)\}_{t>0} $ to be monotonic.



\subsection{Line Search Algorithm}

This section introduces line search algorithms used in practical applications. Previous discussions have introduced the backtracking method (Algorithm~\ref{alg:gd_line_search}) and noted that it can be used to find a stepsize that satisfies the Armijo condition \eqref{equation:arminjo_condition}.
In fact, by simply modifying the termination condition of the algorithm, the backtracking method can be applied to other line search conditions, such as the  nonmonotone line search conditions in Definitions~\ref{definition:grippo_condition}. The implementation of the backtracking method is simple and straightforward, making it one of the most commonly used line search algorithms. 
However, it has some drawbacks: first, it cannot guarantee finding a stepsize that satisfies the Wolfe condition \eqref{equation:wolfe_2}, which is essential for some optimization algorithms; 
second, the backtracking method reduces the stepsize exponentially, making it sensitive to the initial guess $ \widehat{\eta} $ and parameters $ \gamma $ and $ c_1 $ in Algorithm~\ref{alg:gd_line_search}.


The selection of parameter $\gamma$ is relatively sensitive. 
If $\gamma$ is too large, each reduction in step length during the trial is very small, resulting in low backtracking efficiency. 
Conversely, if $\gamma$ is too small, the backtracking is overly aggressive, leading to a final step length that is too small, potentially missing opportunities to select a larger stepsize. 
Below is a brief introduction to other types of line search algorithms. See also \citet{madsen2010and, aggarwal2020linear,  lu2022gradient} for a reference.

\subsection*{Polynomial Interpolation Line Search}
To improve the efficiency of the backtracking method, we use a \textit{polynomial interpolation-based line search} algorithm. Assuming an initial step length $\widehat{\eta}_1$ is given, if this step length does not satisfy the Armijo condition, we reduce the trial step length using polynomial interpolation rather than reducing $\widehat{\eta}_1$ by a constant multiple. 
Specifically, we construct a quadratic interpolation function $q(\eta)$ using $\phi(0)$, $\phi^\prime(0)$, $\phi(\widehat{\eta}_1)$ to, i.e., we find a quadratic function $q(\eta)$ satisfying:
$$
q(0) = \phi(0), \qquad q^\prime(0) = \phi^\prime(0), \qquad q(\widehat{\eta}_1) = \phi(\widehat{\eta}_1).
$$
Since a quadratic function has only three parameters, these three conditions uniquely determine $q(\eta)$. 
It is easy to verify that the minimum value point of $q(\eta)$ lies within $(0, \widehat{\eta}_1)$. 
We then take this minimum value point, $\widehat{\eta}_2$,  as the next trial point and repeat the process until the Armijo condition is satisfied.


\begin{figure}[ht]
\centering
\begin{minipage}{1\textwidth}
\begin{algorithm}[H]
\caption{Soft Line Search with Wolfe Condition  at Iteration $t$}
\label{alg:soft_line_search_wolfe}
\begin{algorithmic}[1]
\Require A function $f(\bx)$ and $t$-th iteration $\bx^\toptzero$; 
\State {\bfseries Input:}  Choose the maximum of step length $\eta_{\max}$ and the maximum number of trials $k_{\max}$;
\If{$\phi^\prime(0) = 0$} \Comment{(i)}
\State $\eta \leftarrow 0$;
\Else
\State $k \leftarrow 0; \quad \gamma \leftarrow c_2  \phi^\prime(0);$
\State $[a, b] \leftarrow \big[0, \min\{1, \eta_{\max}\}\big]$;  \Comment{(ii)}
\State $//$ $b$ is sufficiently small but not large enough $\implies$ increase the range;
\While{$(\phi(b) \leq \rho_1(b))$ and $(\phi^\prime(b) \leq \gamma)$ and $(b < \eta_{\max})$ and $(k < k_{\max})$} \Comment{(iii)}
\State $k \leftarrow k + 1$; 
\State $[a, b] \leftarrow \big[b, \min\{2b, \eta_{\max}\}\big]$;
\EndWhile
\State $\eta \leftarrow b$;
\State $//$ $b$ is too large, and $a$ is either 0 or too small $\implies$ $[a,b]$ contains acceptable points;
\While{$((\phi(\eta) > \rho_1(\eta))$ or $(\phi^\prime(\eta) < \gamma))$ and $(k < k_{\max})$}
\State $k \leftarrow k + 1$
\State Refine $\eta$ and $[a, b]$ by Algorithm~\ref{alg:soft_line_search_wolfe_refine}; \Comment{(iv)}
\EndWhile
\If{$\phi(\eta) \geq \phi(0)$} \Comment{(v)}
\State $\eta \leftarrow 0$;
\EndIf
\EndIf
\State {\bfseries Return:}  Final $\eta$;
\end{algorithmic}
\end{algorithm}
\end{minipage}\hfill
\begin{minipage}{1\textwidth}
\begin{algorithm}[H]
\caption{Find the Stepsize by Interpolation}
\label{alg:soft_line_search_wolfe_refine}
\begin{algorithmic}[1]
\Require A function $f(\bx)$, the interval $[a,b]$ where $a$ is small enough and $b$ is large enough; 
\State $D \leftarrow b - a; \quad c \leftarrow \left( \phi(b) - \phi(a) - D \cdot \phi^\prime(a) \right) / D^2$ 
\If{$c > 0$}  \Comment{Minimization of the polynomial $q(\zeta)$ in \eqref{equation:wolfe_poly}}
\State $\eta \leftarrow a - \phi^\prime(a) / (2c)$
\State $\eta \leftarrow \min\left\{\max\{\eta, a + 0.1D\}, b - 0.1D\right\}$ \Comment{Make  $\eta$  in the middle 80\% of $[a,b]$}
\Else  \Comment{Take $\eta$ as the midpoint of $[a, b]$, i.e., bisection}
\State $\eta \leftarrow (a + b) / 2$
\If{$\phi(\eta) < \rho_1(\eta)$} \Comment{$\eta$ is sufficiently small}
\State $[a,b] \leftarrow [a, \eta]$;
\Else
\State $[a,b]\leftarrow [\eta, b]$;
\EndIf
\EndIf
\State {\bfseries Return:}  Final candidate $\eta$ and refined interval $[a,b]$;
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure}



The interpolation-based line search algorithm effectively reduces the number of trials but still cannot guarantee that the stepsize satisfies the Wolfe condition. To address this, Algorithm~\ref{alg:soft_line_search_wolfe} can be used to find the stepsize iteratively \citep{fletcher2000practical, frandsen1999unconstrained, madsen2010and}.
Algorithm~\ref{alg:soft_line_search_wolfe} maintains an interval $[a,b]$ until $b$ satisfies the upper bound of the Wolfe condition by \eqref{equation:wolfevar_1}:
\begin{enumerate}[(i)]
\item The step shows $\bx^\toptzero$ is a stationary point ($\nabla f(\bx^\toptzero) = \bzero \Rightarrow \phi^\prime(0) = 0$) or $\bd^\toptzero$ is not downhill, then we do nothing.

\item The initial choice $b=1$ is used because in many optimization methods (e.g., Newton's method in Chapter~\ref{chapter:second_order}) $\eta=1$ is a very good guess in the final steps of the iteration. The upper bound $\eta_{\max}$ must be provided by the user to prevent  infinite loop if $f$ is unbounded.

\item The condition shows that $b$ is sufficiently small, but not large enough. So we increase the range from $[a, b]$ to $\big[b, \min\{2b, \eta_{\max}\}\big]$


\item The interval $[a,b]$ contains acceptable points, and use Algorithm~\ref{alg:soft_line_search_wolfe_refine} to find the optimal stepsize $\eta$ or decrease the range of the interval.

\item The algorithm may have stopped abnormally, e.g., by exceeding the permitted number $k_{\max}$ of function evaluations. If the current value of $\eta$ does not decrease the objective function, then we return $\eta = 0$.
\end{enumerate}
Sub-Algorithm~\ref{alg:soft_line_search_wolfe_refine} receives an interval $[a,b]$ where $a$ is the lower bound and $b$ is the upper bound.
We first construct a  second-order polynomial
\begin{equation}\label{equation:wolfe_poly}
q(\zeta) = \phi(a) + \phi^\prime(a) \cdot (\zeta - a) + c \cdot (\zeta - a)^2,
\end{equation}
where  $D \triangleq b - a$ and $c \triangleq \left( \phi(b) - \phi(a) - D \cdot \phi^\prime(a) \right) / D^2$ 
satisfying 
$$
q(a) = \phi(a), \quad q^\prime(a) = \phi^\prime(a), \quad \text{and}\quad q(b) = \phi(b).
$$
If $c > 0$, then the polynomial $q(\cdot)$ has a minimum, and we let $\eta$ be the minimizer. Otherwise we take $\eta$ as the midpoint of $[a, b]$.
In this case, if $\phi(\eta)$ is sufficiently small, then the right-hand part of $[a, b]$ contains points that satisfy both  constraints of the Wolfe condition \eqref{equation:wolfe_var}. Otherwise, $[\eta, b]$ is sure to contain acceptable points.







\index{Bisection line search}
\subsection*{Bisection Line Search}
As mentioned previously, the loss function $\phi(\eta)$, when expressed in terms of $\eta$, is often a unimodal function. 
In the \textit{bisection line search} method, we start by setting the interval $[a,b]$ as $[\eta_{\min}, \eta_{\max}]$, where $\eta_{\min}$ and $ \eta_{\max}$ serve as the lower and upper bounds, respectively, for the learning rate $\eta$ ($\eta_{\min}$ can be set to 0 as specified by \eqref{equation:linesearc-eta0}). The bisection line search involves evaluating the loss function $\phi(\eta)$ at the midpoint $\frac{a+b}{2}$. Given that $\phi^\prime(a)<0$ and $\phi^\prime(b)>0$, the bisection line search follows that
$$
\left\{
\begin{aligned}
\text{set } a &\triangleq \frac{a+b}{2} \text{, \gap if $\phi^\prime\left(\frac{a+b}{2}\right)<0$}; \\
\text{set } b &\triangleq \frac{a+b}{2} \text{, \gap if $\phi^\prime\left(\frac{a+b}{2} \right)>0$}.
\end{aligned}
\right.
$$
The procedure is repeated until the interval between $a$ and $b$ becomes sufficiently small.

The bisection line search is also known as the \textit{binary line search}. And in some cases, the derivative of $\phi(\eta)$ cannot be easily obtained; then the interval is narrowed by evaluating the objective function at two closely spaced
points around $\frac{a+b}{2} $. To be more concrete, assume $\phi(\eta)$ is convex (since we are in the descent setting), we evaluate the loss function at $\frac{a+b}{2}$ and $\frac{a+b}{2}+\epsilon$, where $\epsilon$ is a  small numerical value, e.g.,  $\epsilon=1e-8$.  
This allows us to evaluate whether the function is increasing or decreasing at $\frac{a+b}{2}$ by determining which of the two evaluations is larger. If the function is increasing
at $\frac{a+b}{2}$, the interval is narrowed to $[a,\frac{a+b}{2}+\epsilon]$; otherwise, it is narrowed to
$[\frac{a+b}{2}, b]$.
$$
\left\{
\begin{aligned}
\text{set } b &= \frac{a+b}{2}+\epsilon,  &\text{ \gap if increasing at $\frac{a+b}{2}$};\\
\text{set } a &= \frac{a+b}{2}, &\text{ \gap otherwise}. \\
\end{aligned}
\right.
$$
This iterative process continues until the range is sufficiently small or the required level of accuracy is achieved in the interval.



\index{Golden-section line search}
\subsection*{Golden-Section Line Search}
Similar to the bisection line search, the \textit{golden-section line search} also identifies the best learning rate $\eta$ for a unimodal function $\phi(\eta)$. 
It starts with the interval $[a,b]$ as $[0, \eta_{\max}]$. However, instead of selecting a midpoint, the golden-section search designates two points of $c_1$ and $c_2$ such that $a<c_1<c_2<b$. 
The procedure is as follows:  
\begin{itemize}
\item If $\eta=a$ results in the minimum value for $\phi(\eta)$ (among the four values $\phi(a), \phi(c_1), \phi(c_2)$, and $\phi(b)$), we can exclude the interval $(c_1, b]$. 
\item If $\eta=c_1$ yields the minimum value, we can exclude the interval $(c_2, b]$. 
\item If $\eta=c_2$ yields the minimum value, we can exclude the interval $[a, c_1)$. 
\item If $\eta=b$ yields the minimum value, we can exclude the interval $[a, c_2)$.
\end{itemize}
These four situations are illustrated in Figure~\ref{fig:conjguatecy-golden_1234}.
In other words, at least one of the intervals $[a,c_1]$ and $[c_2, b]$ can be discarded  in the golden-section search
method.
By excluding one of the four intervals, the new bounds $[a, b]$ are adjusted accordingly, and the process iterates until the range is sufficiently small.
\begin{figure}[h]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[$\eta=a$ yields the minimum.]{\label{fig:golden_1}
\includegraphics[width=0.23\linewidth]{./imgs/golden_1.pdf}}
\subfigure[$\eta=c_1$ yields the minimum.]{\label{fig:golden_2}
\includegraphics[width=0.23\linewidth]{./imgs/golden_2.pdf}}
\subfigure[$\eta=c_2$ yields the minimum.]{\label{fig:golden_3}
\includegraphics[width=0.23\linewidth]{./imgs/golden_3.pdf}}
\subfigure[$\eta=b$ yields the minimum.]{\label{fig:golden_4}
\includegraphics[width=0.23\linewidth]{./imgs/golden_4.pdf}}
\caption{Demonstration of four different update ways in golden-section line search.}
\label{fig:conjguatecy-golden_1234}
\end{figure}


%\begin{figure}[h]
%%\begin{SCfigure}%[H]
%\centering
%\includegraphics[width=0.6\textwidth]{imgs/armijo_convex.pdf}
%\caption{Demonstration of Armijo line search in a convex setting.}
%\label{fig:armijo_convex}
%%\end{SCfigure}
%\end{figure}
%\index{Armijo condition}
%\subsection*{Armijo Line Search}
%The \textit{Armijo line search} method is yet another approach to obtain the stepsize that satisfies the Armijo condition while preventing the stepsize from being too small.
%Using linear approximation theorem (Theorem~\ref{theorem:linear_approx}), we have
%$$
%\phi(\eta)=f(\bx^\toptzero +\eta \bd^\toptzero ) \approx f(\bx^\toptzero )+ \eta  \innerproduct{\bd^\toptzero,  \nabla f(\bx^\toptzero )}.
%$$
%Since $\phi'(0) = \innerproduct{\bd^\toptzero,  \nabla f(\bx^\toptzero )} \leq 0$, it follows that 
%\begin{equation}\label{equation:armijo_step_approx}
%f(\bx^\toptzero + \eta \bd^\toptzero) \leq f(\bx^\toptzero) + \alpha \eta \cdot  \innerproduct{\bd^\toptzero, \nabla f(\bx^\toptzero)},  \gap \alpha \in (0,1).
%\end{equation}
%Let 
%$\widetilde{\phi}(\eta) = \phi(0)+  \phi^\prime(0) \cdot  \eta$ \footnote{The tangent of $\phi(\eta)$ at $\eta=0$.}
%and 
%$\widehat{\phi}(\eta) = \phi(0)+ \alpha \phi^\prime(0) \cdot  \eta$, the relationship between the two functions is depicted in Figure~\ref{fig:armijo_convex} for the case where $\phi(\eta)$ is a convex function; and we note that $\widehat{\phi}(\eta) > \widetilde{\phi}(\eta)$ when $\eta>0$.
%
%The Armijo line search states that an acceptable $\widehat{\eta}$ should satisfy $\phi(\widehat{\eta}) \leq \widehat{\phi}(\widehat{\eta})$ to ensure sufficient decrease and $\phi( \widehat{\eta}/\beta) > \widehat{\phi}(\widehat{\eta}/\beta)$ to prevent the stepsize from being too small, where $\beta\in (0,1)$. 
%This ensures that  the (local) optimal learning rate is in the range of $[\widehat{\eta}, \widehat{\eta}/\beta)$. By Equation~\eqref{equation:armijo_step_approx}, the two criteria above can also be described by:
%$$
%\left\{
%\begin{aligned}
%\phi(\widehat{\eta}) &\leq \widehat{\phi}(\widehat{\eta}); \\
%\phi( \widehat{\eta}/\beta) &> \widehat{\phi}(\widehat{\eta}/\beta),
%\end{aligned}
%\right.
%\Longrightarrow \gap 
%\left\{
%\begin{aligned}
%f(\bx^\toptzero + \widehat{\eta} \bd^\toptzero) -  f(\bx^\toptzero)&\leq \alpha \widehat{\eta} \cdot \innerproduct{\bd^\toptzero, \nabla f(\bx^\toptzero)};  \\
%f(\bx^\toptzero + \widehat{\eta}/\beta \bd^\toptzero) -  f(\bx^\toptzero)&> \alpha \widehat{\eta}/\beta \cdot \innerproduct{ \bd^\toptzero, \nabla f(\bx^\toptzero)} .
%\end{aligned}
%\right.
%$$
%which is closely related to the Goldstein condition (Definition~\ref{definition:goldstein_cond}).
%The complete algorithm for calculating the learning rate at the $t$-th iteration is outlined in Algorithm~\ref{alg:als_armijo}. In practice, the parameters   are typically set as $\beta \in [0.2, 0.5]$ and $\alpha\in [1e-5, 0.5]$. Additionally, it's worth noting that the Armijo line search is inexact, and it works even when $\phi(\eta)$ is not unimodal.
%
%After developing the Armijo algorithm, the underlying concept of the Armijo condition becomes clear: the descent direction $\phi^\prime(0)$ at that starting point $\eta=0$ often deteriorates in terms of rate of improvement since it moves further along this direction. However, a fraction $\alpha\in [1e-5, 0.5]$ of this improvement is acceptable. By Equation~\eqref{equation:armijo_step_approx}, the descent update at $(t+1)$-th iteration $f(\bx^\toptzero + \eta \bd^\toptzero)$ is at least $\alpha \eta \cdot  \innerproduct{\bd^\toptzero,  \nabla f(\bx^\toptzero)}$ smaller than that of $t$-th iteration.
%
%\begin{algorithm}[h] 
%\caption{Armijo Line Search at $t$-th Iteration}
%\label{alg:als_armijo}
%\begin{algorithmic}[1] 
%\Require Start with $\eta_t=s$, $0<\beta<1$, and $0<\alpha<1$;
%\State isStop = False;
%\While{isStop is False} 
%\If{$f(\bx^\toptzero + \eta_t \bd^\toptzero) - f(\bx^\toptzero) \leq \alpha \eta_t \cdot \innerproduct{\bd^\toptzero,  \nabla f(\bx^\toptzero)}$}
%\State isStop = True;
%\Else
%\State $\eta_t \leftarrow \beta \eta_t$;
%\EndIf
%\EndWhile
%\State {\bfseries Return:}  $\eta_t$;
%\end{algorithmic} 
%\end{algorithm}













\subsection{Convergence Analysis}\label{section:gd_conv_line_search}

This subsection discusses the convergence of the algorithm derived using different line search criteria. While the conclusions are relatively weak because they are established in the framework of general line search algorithms, they provide essential insights into the requirements for the convergence of these algorithms.
\begin{theorem}[Descent Lemma and Zoutendijk  under SS and Wolfe]\label{theorem:zoutendijk_cond}
Let $f:\real^n\rightarrow\real$ be a differentiable and $\beta$-smooth function that is bounded below. 	
Consider the general update rule of  descent methods in Algorithm~\ref{alg:struc_gd_gen}, where $\bd^\toptzero$ is a descent direction at the point $ \bx^\toptzero $ (not necessarily the negative gradient), $\eta_t$ is the step length, and  the Wolfe condition (Definition~\ref{definition:wolfe_cond}) is satisfied in each iteration. 
Then, the descent lemma holds for each iteration $t$:
$$
f(\bx^\toptone) -f(\bx^\toptzero) \leq  c_1 \frac{c_2 - 1}{\beta} \cos^2 (\theta_t) \normtwobig{\nabla f(\bx^\toptzero)}^2,
$$
and the following \textit{Zoutendijk condition} holds:
\begin{equation}\label{equation:zoutendijk_cond}
\sum_{t=1}^{\infty} \cos^2 (\theta_t) \normtwobig{\nabla f(\bx^\toptzero)}^2 < +\infty.~\footnote{This implies that $\cos^2 (\theta_t) \normtwobig{\nabla f(\bx^\toptzero)}^2\rightarrow 0$ as $t\rightarrow \infty$.}
\end{equation}
where $\cos (\theta_t)$ is the cosine of the angle between the negative gradient $-\nabla f(\bx^\toptzero)$ and the descent direction $\bd^\toptzero$, i.e.,
$
\cos (\theta_t) = \frac{-\innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}}{\normtwobig{\nabla f(\bx^\toptzero)} \normtwobig{\bd^\toptzero}}.
$
\end{theorem}
\begin{proof}[of Theorem~\ref{theorem:zoutendijk_cond}]
Since $\bx^\toptone = \bx^\toptzero +\eta_t\bd^\toptzero$, by the Wolfe condition \eqref{equation:wolfe_2},
$$
\innerproduct{ \nabla f(\bx^\toptone) - \nabla f(\bx^\toptzero) ,  \bd^\toptzero} \geq (c_2 - 1) \innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}.
$$
By the Cauchy-Schwarz inequality (Proposition~\ref{proposition:cauchy-schwarz-inequ}) and the  $\beta$-smoothness,
$$
\innerproduct{ \nabla f(\bx^\toptone) - \nabla f(\bx^\toptzero) ,  \bd^\toptzero}
\leq \normtwo{\nabla f(\bx^\toptone) - \nabla f(\bx^\toptzero)} \normtwobig{\bd^\toptzero} \leq \eta_t \beta \normtwobig{\bd^\toptzero}^2.
$$
Combining these  two inequalities yields that 
$$
\eta_t \geq \frac{c_2 - 1}{\beta} \frac{\innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}}{\normtwobig{\bd^\toptzero}^2}.
$$
Since $\bd^\toptzero$ is a descent direction, implying that $\innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero} < 0$, substituting the above into the Wolfe condition \eqref{equation:wolfe_1}, whence we have the following \textit{descent inequality}~\footnote{Such inequalities are called descent lemma for descent methods and play a core role in proving the convergence results for gradient descent methods and their variants in Chapter~\ref{chapter:gd_convg}.}:
$$
f(\bx^\toptone) -f(\bx^\toptzero) \leq  c_1 \frac{c_2 - 1}{\beta} \frac{\big(\innerproduct{\nabla f(\bx^\toptzero),  \bd^\toptzero}\big)^2}{\normtwobig{\bd^\toptzero}^2}
= c_1 \frac{c_2 - 1}{\beta} \cos^2 (\theta_t) \normtwobig{\nabla f(\bx^\toptzero)}^2,
$$
where the last equality follows from the definition of $\theta_t$.
Performing telescopic cancellations over $t\in\{1,2,\ldots,T\}$, we have
$$
f(\bx^{(T+1)}) \leq f(\bx^{(1)}) + c_1 \frac{c_2-1}{\beta} \sum_{t=1}^T \cos^2 (\theta_t) \normtwobig{\nabla f(\bx^\toptzero)}^2.
$$
Since the function $f$ is bounded below and $0 < c_1 < c_2 < 1$, it follows that $c_1 (c_2-1) < 0$. Therefore, as $t \to \infty$,
$
\sum_{t=1}^\infty \cos^2 (\theta_t) \normtwobig{\nabla f(\bx^\toptzero)}^2 < +\infty,
$
establishing the desired result.
\end{proof}

Theorem~\ref{theorem:zoutendijk_cond} indicates that as long as the iterations satisfy the Wolfe condition, for a smooth and bounded below function, the Zoutendijk condition always holds. In fact, using the Goldstein condition can also lead to similar conclusions \citep{nocedal1999numerical, sun2006optimization}. The Zoutendijk condition characterizes the nature of line search conditions, and together with the selection of the descent direction $\bd^\toptzero$, we can obtain convergence results for descent methods.


In the aforementioned theorem, given a descent direction $\bd$ at point $\bx$, we introduce the angle between $\bd$ and $-\nabla f(\bx)$:
$$
\cos (\theta) = \frac{-\bd^\top \nabla f(\bx)}{\normtwo{\bd}\normtwo{\nabla f(\bx)}}.
$$
The existence of the angle is guaranteed by the Cauchy-Schwartz inequality in Definition~\ref{definition:angle_bet_vec_ineq} and provide a type of descent method known as the \textit{absolute descent method}.

\begin{definition}[Absolute Descent Method]\label{definition:absolute_descent_direction}
Let $f:\real^n\rightarrow\real$ be a differentiable function.
Consider the general update rule of  descent methods in Algorithm~\ref{alg:struc_gd_gen}, where $\bd^\toptzero$ is a descent direction at the point $ \bx^\toptzero $ (not necessarily the negative gradient). 
Let $\theta_t$ be the angle between the negative gradient $-\nabla f(\bx^\toptzero)$ and the descent direction $\bd^\toptzero$: 
$\cos(\theta_t) = \frac{-\innerproduct{\bd^\toptzero \nabla f(\bx^\toptzero)}}{\normtwo{\bd^\toptzero}\normtwo{\nabla f(\bx^\toptzero)}}$, and assume for any $t>0$, there exists a constant $\zeta > 0$ such that
\begin{equation}\label{equation:abs_des_dire2}
\theta_t < \frac{\pi}{2} - \zeta,\quad \forall t>0, \quad\text{ with } \zeta>0 \text{ independent of $ t $},
\end{equation}
ensuring $\theta_t\in(0, \frac{\pi}{2})$.
That is, the angle $\theta_t$ between $\bd^\toptzero$ and $-\nabla f(\bx^\toptzero)$ is \textit{uniformly bounded away} from $90^{\circ}$. Note that if $\theta=90^{\circ}$, then $\bd^\toptzero$ is almost not a descent direction.
\end{definition}
The restriction that $\zeta$ must be constant in all the steps is necessary for the global convergence result for iterative methods.
Then we have the following convergence results for descent methods.
\begin{theoremHigh}[Convergence of Descent Method under Zoutendijk]\label{theorem:conv_line_search}
Under the same conditions as Theorem~\ref{theorem:zoutendijk_cond}, let $\bd^\toptzero$ satisfy the absolute descent direction in \eqref{equation:abs_des_dire2}.
Then the sequence $\{\bx^\toptzero\}_{t>0}$ converges to a stationary point of $f$.
That is, if  the Zoutendijk condition $
\sum_{t=1}^{\infty} \cos^2 (\theta_t) \normtwobig{\nabla f(\bx^\toptzero)}^2 < +\infty
$ is satisfied~\footnote{We should also note that the Zoutendijk condition is derived from the Wolfe condition; More generally, if  exact line search is applicable, the Zoutendijk condition is also satisfied since  exact line search represents a special case of the Wolfe condition.}, it follows that
$$
\lim_{t \to \infty} \nabla f(\bx^\toptzero) = \bzero.
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:conv_line_search}]
Assume, for the sake of contradiction, that the conclusion is false. That is, there exists a subsequence $\{t_k\}_{k>0}$ and a positive constant $\delta > 0$ such that
$$
\normtwo{\nabla f(\bx^{(t_k)})} \geq \delta, \quad k = 1, 2, \ldots.
$$
Based on the assumption of $\theta_t$, for any $t>0$,
$
\cos (\theta_t) > \cos \left( \frac{\pi}{2} - \zeta \right) = \sin (\zeta) > 0.
$
Focusing only on the terms corresponding to$t_k$ in Equation~\eqref{equation:zoutendijk_cond}, and we have
$$
\sum_{t=1}^\infty \cos^2 (\theta_t) \normtwobig{\nabla f(\bx^\toptzero)}^2 \geq \sum_{k=1}^\infty \cos^2 (\theta_{t_k}) \normtwo{\nabla f(\bx^{(t_k)})}^2
> \sum_{l=1}^\infty\sin^2 (\zeta) \cdot \delta^2 \to +\infty.
$$
This leads to a contradiction to Theorem~\ref{theorem:zoutendijk_cond}. Therefore, it must hold that 
$
\lim_{t \to \infty} \nabla f(\bx^\toptzero) = \bzero,
$
which completes the proof.
\end{proof}

Theorem~\ref{theorem:conv_line_search} is derived from the Zoutendijk condition, which essentially imposes a constraint on the absolute descent direction. Specifically, it ensures that each descent direction $\bd^\toptzero$ does not become asymptotically orthogonal to the negative gradient direction.  
This condition is geometrically intuitive: when the descent direction $\bd^\toptzero$ and the gradient are nearly orthogonal, according to  Taylor's expansion, the objective function value $f(\bx^\toptzero)$ would experience little to no change. 
Consequently, we require $\bd^\toptzero$ to maintain a consistent lower bound on the angle relative to the gradient's orthogonal direction. 
Apparently, the steepest descent direction and the greedy search directions we introduced in the previous sections satisfy this condition.

Under the Goldstein condition, we can also obtain a convergence result under mild assumptions.
\begin{theoremHigh}[Convergence of Descent Method under Goldstein or Wolfe]
Let $f:\real^n\rightarrow\real$ be a differentiable function. 	
Consider the general update rule of  descent methods in Algorithm~\ref{alg:struc_gd_gen}, where $\bd^\toptzero$ is a descent direction at the point $ \bx^\toptzero $ (not necessarily the negative gradient), and let $\eta_t$ in Algorithm~\ref{alg:struc_gd_gen} be defined by the Goldstein condition (Definition~\ref{definition:goldstein_cond}) or Wolfe condition (Definition~\ref{definition:wolfe_cond}).
Suppose that  the descent direction $\bd^\toptzero$ satisfy  the absolute descent direction in \eqref{equation:abs_des_dire2} for each iteration. If $\nabla f$ exists and is uniformly continuous on the level set $\sL\triangleq\lev[f, f(\bx^{(1)})]\triangleq\{\bx \mid f(\bx) \leq f(\bx^{(1)})\}$, then either 
$$
\text{$\nabla f(\bx^\toptzero) = \bzero$ for some $t$, $\quad$ or $f(\bx^\toptzero) \to -\infty$,$\quad$ or $\nabla f(\bx^\toptzero) \to \bzero$.}
$$
\end{theoremHigh}
\begin{proof}
Let $\eta_t$ be defined  by the Goldstein condition (Definition~\ref{definition:goldstein_cond}). 
Suppose that, for all $t$, $\bg^\toptzero \triangleq \nabla f(\bx^\toptzero) \neq \bzero$ (whence $\bh^\toptzero \triangleq \eta_t \bd^\toptzero \neq \bzero$) and $f(\bx^\toptzero)$ is bounded below, it follows that $f(\bx^\toptzero) - f(\bx^\toptone) \to 0$, hence $-\bg^\toptzeroTOP \bh^\toptzero \to 0$ from the first inequality of the Goldstein condition (or the first inequality of the  Wolfe condition) in \eqref{equation:goldstein1}.

Now, assume that $\bg^\toptzero \to \bzero$ does not hold. Then there exist a constant $\varepsilon > 0$ and a subsequence $\sT$ such that $\normtwobig{\bg^\toptzero} \geq \varepsilon$  and $\normtwobig{\bh^\toptzero} \to 0$ for $t\in\sT$. Based on the assumption of $\theta_t$, for any $t>0$,
$
\cos (\theta_t) > \cos \left( \frac{\pi}{2} - \zeta \right) = \sin (\zeta) > 0,
$
whence we have
$$
-\bg^\toptzeroTOP \bh^\toptzero \geq \sin (\zeta) \normtwobig{\bg^\toptzero} \normtwobig{\bh^\toptzero} \geq \varepsilon \sin (\zeta) \normtwobig{\bh^\toptzero}.
$$
By the mean value theorem (Theorem~\ref{theorem:mean_approx}), there exist a vector $\bxi^\toptzero$  on the line segment $(\bx^\toptzero, \bx^\toptone)$ such that 
$$
f(\bx^\toptone) = f(\bx^\toptzero) + \nabla f(\bxi^\toptzero)^\top \bh^\toptzero,
$$
By the uniform continuity of $\nabla f$, we have $\nabla f(\bxi^\toptzero) \to \bg^\toptzero$ when $\bh^\toptzero \to 0$. So
$$
f(\bx^\toptone) = f(\bx^\toptzero) + \bg^\toptzeroTOP \bh^\toptzero + o(\normtwobig{\bh^\toptzero}).
$$
Therefore,  it follows that 
$$
\frac{f(\bx^\toptone) - f(\bx^\toptzero) }{\bg^\toptzeroTOP \bh^\toptzero} \to 1 \quad \text{ as }\quad \normtwobig{\bh^\toptzero} \to 0,
$$
which contradicts the second inequality of the Goldstein condition in \eqref{equation:goldstein2}. Hence, we conclude that $ \bg^\toptzero \to \bzero $, completing the proof.

Similarly, for the Wolfe condition, we rearrange its second inequality in \eqref{equation:wolfe_2} as follows:
$$
-\innerproduct{\bg^\toptzero, \bd^\toptzero} 
\leq  
\frac{\innerproduct{(\bg^\toptone - \bg^\toptzero), \bd^\toptzero}}{1-c_2}
\leq 
\frac{\normtwo{\bg^\toptone - \bg^\toptzero} \normtwo{\bd^\toptzero}}{1-c_2}
$$
where $c_2\in(0,1)$.
Since the uniform continuity $\nabla f(\bx) $ implies that $\normtwo{\bg^\toptone - \bg^\toptzero} = o(1)$ for $t\in\sT$ (Remark~\ref{remark:conse_uniform_cont}) and given the assumption that $\normtwobig{\bg^\toptzero} \geq \varepsilon$ for $t\in \sT$, the above inequality also implies that 
$$
\cos(\theta_t) =\frac{-\innerproduct{\bg^\toptzero, \bd^\toptzero} }{\normtwo{\bg^\toptzero}\normtwo{\bd^\toptzero}}
\leq \frac{o(1)}{(1-c_2)\normtwo{\bg^\toptzero}}
\leq o(1),
\quad \text{for }t\in \sT,
$$
which contradicts the assumption of the absolute descent direction in \eqref{equation:abs_des_dire2}. Thus, $\bg^\toptzero \rightarrow \bzero$.
This completes the proof.
\end{proof}







\section{Descent Methods with Trust Region: An Overview}\label{section:trs_intro}
Equation~\eqref{equation:gd_gree_approx}, based on linear approximation theorem (Theorem~\ref{theorem:linear_approx}), provides us with a linear approximation to $ f $ near a given point $ \bx^\toptzero $:
\begin{subequations}\label{equation:trs_first_all}
\begin{align}
	&f(\bx^\toptzero+\bd) \approx \psi_t(\bd)   \label{equation:trs_first1} \\
	&\text{with } \psi_t(\bd) \triangleq f(\bx^\toptzero) + \bd^\top \nabla f(\bx^\toptzero).  \label{equation:trs_first2}
\end{align}
\end{subequations}
Similarly, we obtain a quadratic approximation (Theorem~\ref{theorem:quad_app_theo}) to $ f $
\begin{subequations}\label{equation:trs_second_all}
\begin{align}
	& f(\bx^\toptzero+\bd) \approx \psi_t(\bd) \label{equation:trs_second1} \\
	& \text{with } \psi_t(\bd) \triangleq f(\bx^\toptzero) + \bd^\top \nabla f(\bx^\toptzero) + \frac{1}{2} \bd^\top \nabla^2 f(\bx^\toptzero) \bd. \label{equation:trs_second2}
\end{align}
\end{subequations}
In both cases, $ \psi_t(\bd) $ provides a good approximation to $ f(\bx^\toptzero+\bd) $ only when $ \bd $ is sufficiently small. 
These approximations motivate the following iterative step, which is determined by solving the following model problem:
\begin{equation}\label{equation:trs_subpro_gd}
\begin{aligned}
	& \bd_{\text{tr}}^\toptzero = \mathop{\argmin}_{\bd \in \sS}\,\psi_t(\bd) , \\
	& \text{where } \sS \triangleq \{ \bd \mid \normtwo{\bd} \leq \Delta \}, \, \Delta > 0.
\end{aligned}
\end{equation}
The region $ \sS $ is referred to as the \textit{trust region} and $ \psi_t(\bd) $ is given by \eqref{equation:trs_first2} or \eqref{equation:trs_second2}.
Since \eqref{equation:trs_second2} is used extensively in the literature \citep{nocedal1999numerical}, we defer its analysis to Section~\ref{section:des_trust_reg}, where we introduce second-order methods.

We use $ \bd = \bd_{\text{tr}}^\toptzero $ as a candidate to our next step, and reject it if $ f(\bx^\toptzero+\bd) \geq f(\bx^\toptzero) $. 
The reduction in the objective function value determines the size of the trust region for the next step. 
Specifically, the actual reduction is compared to the predicted reduction from the approximation function, leading to the definition of the \textit{gain factor}:
\begin{equation}\label{equation:gain_fact_gd}
\textbf{(Gain factor)}: \qquad \nu_t \triangleq \frac{f(\bx^\toptzero) - f(\bx^\toptzero+\bd_{\text{tr}}^\toptzero)}{\psi_t(\bzero) - \psi_t(\bd_{\text{tr}}^\toptzero)}. 
\end{equation}
When $\nu_t$ is small,  the approximation poorly reflects $f$, whereas a large $\nu_t$ indicates good agreement. 
Consequently, the gain factor regulates the trust region's size for the next step (or, if
$\nu_t\leq  0$,  $\bd_{\text{tr}}^\toptzero$ is rejected and it triggers a re-evaluation of $\bd_{\text{tr}}^\toptzero$).
These ideas are summarized in Algorithm~\ref{alg:trust_region0}.
The numerical values in the algorithm, 0.75, 2, 0.25, and 1/3, are chosen based on practical experience; while $\gamma$ is selected for convergence issues (Theorem~\ref{theorem:global_conv_trs_eta0} and Theorem~\ref{theorem:glo_conv_trs_othe}). Although minor variations in these values do not significantly impact performance, the parameters $p_1$ and $p_2$ in the expressions $\Delta_{t+1} \leftarrow p_1 \Delta_t$ and $\Delta_{t+1} \leftarrow \Delta_t / p_2$ must be chosen carefully to prevent oscillations in the trust region radius.

\begin{algorithm}
\caption{Descent Method with Trust Region}
\label{alg:trust_region0}
\begin{algorithmic}[1]
\Require A first  or twice  differentiable function $f(\bx)$ for \eqref{equation:trs_first_all} or \eqref{equation:trs_second_all}, respectively; 
\State {\bfseries Input:}   Set the maximum radius $ \Delta_{\max} $, initial radius $ \Delta_1 $, initial point $ \bx^{(1)} $, accept radius $\gamma\in[0,\frac{1}{4})$;
\For{$t=1,2,\ldots$}
\State $\bd_{\text{tr}}^{\toptzero} \leftarrow \text{Solution of trust region subproblem \eqref{equation:trs_subpro_gd}}$;
\State $\nu_t \leftarrow \text{gain factor \eqref{equation:gain_fact_gd}}$;
\If{$\nu_t > 0.75$ and $\normtwobig{\bd^\toptzero} =\Delta_t$} \Comment{very good step, and the step is at the border}
\State $\Delta_{t+1} \leftarrow \min\{2 \Delta_t, \Delta_{\max}\}$; \Comment{larger trust region}
\EndIf
\If{$\nu_t < 0.25$} \Comment{poor step}
\State $\Delta_{t+1} \leftarrow \Delta_t / 3$; \Comment{smaller trust region}
\EndIf
\If{$\nu_t > \gamma$} \Comment{reject step if $\nu_t \leq \gamma$}
\State $\bx^\toptone \leftarrow \bx^\toptzero + \bd_{\text{tr}}^{\toptzero}$;
\EndIf
\EndFor
\State {\bfseries Return:} final $\bx\leftarrow \bx^\toptzero$;
\end{algorithmic}
\end{algorithm}


\index{Quadratic form}
\index{Fisher information matrix}
\index{Positive definite}
\index{Positive semidefinite}
\index{Symmetry}
\section{Quadratic Model using Gradient Descent with Fixed Stepsize}\label{section:quadratic_vanilla_GD}


We further explore gradient descent with a fixed stepsize applied to the simplest model: the  quadratic function,
\begin{equation}\label{equation:quadratic-form-general-form}
f(\bx) = \frac{1}{2} \bx^\top \bA \bx - \bb^\top \bx + c, \gap \bx\in \real^n,
\end{equation}
where $\bA\in \real^{n\times n}$, $\bb \in \real^n$, and $c$ is a scalar constant.
The function is convex (resp. strictly convex) if $\bA$ is positive semidefinite (resp. positive definite); see Exercise~\ref{exercise:conv_quad}. Although the quadratic form in \eqref{equation:quadratic-form-general-form} is a highly simplified model, it is versatile enough to approximate many other functions, such as the Fisher information matrix \citep{amari1998natural}, while also capturing key characteristics of pathological curvature. The gradient of $f(\bx)$ at a given point $\bx$ is given by 
\begin{equation}\label{equation:unsymmetric_gd_gradient}
\nabla f(\bx) = \frac{1}{2} (\bA^\top +\bA) \bx - \bb.
\end{equation}
The stationary point (if exists) of the function is the solution of the linear system $\frac{1}{2} (\bA^\top +\bA) \bx=  \bb $:
\begin{equation}\label{equation:gd_solution_unsymmetric}
\bx^* = 2(\bA^\top +\bA)^{-1}\bb.
\end{equation}
If $\bA$ is symmetric (for most of our discussions, we will restrict to be symmetric $\bA$ or even {positive definite}), the equation reduces to 
\begin{equation}\label{equation:symmetric_gd_gradient}
\nabla f(\bx) = \bA \bx - \bb.
\end{equation}
Then the unique minimum of the function is the solution of the linear system $\bA\bx=\bb$ (Theorem~\ref{theorem:fermat_fist_opt}); and the optimal point of $\bx$ is thus given by 
$$
\bx^* = \bA^{-1}\bb
$$
if $\bA$ is nonsingular.







\begin{figure}[h!]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[Positive definite matrix: $\bA = \begin{bmatrix}
	200 & 0 \\ 0 & 200
\end{bmatrix}$.]{\label{fig:quadratic_PD}
	\includegraphics[width=0.485\linewidth]{imgs/quadratic_PD.pdf}}
\subfigure[Negative definite matrix: $\bA = \begin{bmatrix}
	-200 & 0 \\ 0 & -200
\end{bmatrix}$.]{\label{fig:quadratic_ND}
	\includegraphics[width=0.485\linewidth]{imgs/quadratic_ND.pdf}}
\subfigure[Semidefinite matrix: $\bA = \begin{bmatrix}
	200 & 0 \\ 0 & 0
\end{bmatrix}$. The set of solutions forms a line along the bottom of the valley.]{\label{fig:quadratic_singular}
	\includegraphics[width=0.485\linewidth]{imgs/quadratic_singular.pdf}}
\subfigure[Indefinte matrix: $\bA = \begin{bmatrix}
	200 & 0 \\ 0 & -200
\end{bmatrix}$.]{\label{fig:quadratic_saddle}
	\includegraphics[width=0.485\linewidth]{imgs/quadratic_saddle.pdf}}
\caption{Loss surfaces for different quadratic forms.}
\label{fig:different_quadratics}
\end{figure}
For different types of matrix $\bA$, the loss surface of $f(\bx)$ will be different, as illustrated in Figure~\ref{fig:different_quadratics}. When $\bA$ is positive definite, the surface forms a convex bowl; when $\bA$ is negative definite, on the contrary, the surface becomes a concave bowl. $\bA$ also could be singular, in which case $\bA\bx-\bb=\bzero$ has more than one solution, and the set of solutions is a line (in the two-dimensional case ) or a hyperplane (in the high-dimensional case). This behavior is similar to that of a semidefinite quadratic form, as shown in Figure~\ref{fig:quadratic_singular}. If $\bA$ does not fall into any of these categories, a saddle point emerges (see Figure~\ref{fig:quadratic_saddle}), posing  challenges for gradient descent.  
In such cases, alternative methods, e.g., perturbed GD \citep{jin2017escape, du2017gradient}, can be employed to navigate away from saddle points.

\begin{figure}[htp]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[Contour and the descent direction. The red dot is the optimal point.]{\label{fig:quadratic_vanillegd_contour}
	\includegraphics[width=0.31\linewidth]{imgs/quadratic_steepest_contour_tilt.pdf}}
\subfigure[GD, $\eta=0.02$.]{\label{fig:quadratic_vanillegd_contour2}
	\includegraphics[width=0.31\linewidth]{./imgs/steepest_gd_mom-0_lrate-2.pdf}}
\subfigure[GD, $\eta=0.08$.]{\label{fig:quadratic_vanillegd_contour8}
	\includegraphics[width=0.31\linewidth]{./imgs/steepest_gd_mom-0_lrate-8.pdf}}
\caption{Illustration of  gradient descent with fixed stepsize applied to a quadratic form with $\bA=\scriptsize\begin{bmatrix}
		20 & 7 \\ 5 & 5
	\end{bmatrix}$, $\bb=\bzero$, and $c=0$. The procedure is at $\bx^\toptzero=[-3,3.5]^\top$ for the $t$-th iteration.}
\label{fig:quadratic_vanillegd}
\end{figure}

\index{Quadratic form}
\index{Saddle point}

In this context, gradient descent is not strictly necessary, as the minimum can be computed directly if the data matrix $\bA$ is nonsingular and its inverse can be determined efficiently. However, our focus is on the iterative updates of the convex quadratic function.
Suppose we initialize the process at $\bx^{(1)}\in \real^n$. At each time step $t$, the simplest update rule involves fixing the learning rate $\eta$ and selecting a descent direction $\bd^\toptzero$, leading to the update:
$$
\bx^\toptone \leftarrow  \bx^\toptzero + \eta \bd^\toptzero. 
$$
This results in a monotonically decreasing sequence of function values $\{f(\bx^\toptzero)\}$. 
Specifically, when the descent direction is chosen to be the negative gradient $\bd^\toptzero = -\bg^\toptzero=-(\bA\bx^\toptzero-\bb)$ ($\bA$ is positive definite), the update becomes 
\begin{equation}\label{equation:vanilla-gd-update}
\text{(GD with fixed stepsize)}:\quad \bx^\toptone = \bx^\toptzero - \eta (\bA\bx^\toptzero-\bb).
\end{equation}
A concrete example is given in Figure~\ref{fig:quadratic_vanillegd}, where $\bA=\scriptsize\begin{bmatrix}
20 & 7 \\ 5 & 5
\end{bmatrix}$, $\bb=\bzero$, and $c=0$. Suppose at $t$-th iteration, $\bx^\toptzero=[-3,3.5]^\top$. Figure~\ref{fig:quadratic_vanillegd_contour} shows the descent direction given by the negative gradient at  $\bx^\toptzero$; Figure~\ref{fig:quadratic_vanillegd_contour2} and Figure~\ref{fig:quadratic_vanillegd_contour8} illustrate  10 iterations of gradient descent with learning rates $\eta=0.02$ and $\eta=0.08$, respectively.


\index{Spectral decomposition}
\paragrapharrow{Closed-form for GD.}
When $\bA$ is positive definite, it admits a spectral decomposition (Theorem~\ref{theorem:spectral_theorem}):
$$
\bA=\bQ\bLambda\bQ^\top \in \real^{n\times n} 
\quad\implies\quad 
\bA^{-1} = \bQ\bLambda^{-1}\bQ^\top,
$$ 
where $\bQ = [\bq_1, \bq_2, \ldots , \bq_n]$ consists of mutually orthonormal eigenvectors of $\bA$, and $\bLambda = \diag(\lambda_1, \lambda_2, \ldots , \lambda_n)$ contains the corresponding real eigenvalues.  
Since $\bA$ is positive definite,  the eigenvalues are all positive (Theorem~\ref{theorem:eigen_charac}). By convention, we order the eigenvalues such that $\lambda_1\geq \lambda_2\geq \ldots \geq \lambda_n$. Define the following iterate vector at iteration $t$ as
\begin{equation}\label{equation:vanilla-yt}
\by^\toptzero \triangleq \bQ^\top(\bx^\toptzero - \bx^*),
\end{equation}
where $\bx^* = \bA^{-1}\bb$.
By the update rule $\bx^\toptone \leftarrow \bx^\toptzero-\eta\nabla f(\bx^\toptzero)$,
it then follows that
$$
\small
\begin{aligned}
\by^\toptone
&\triangleq \bQ^\top(\bx^\toptone - \bx^*) = \bQ^\top(\bx^\toptzero - \eta(\bA\bx^\toptzero-\bb) - \bx^*)\\
%=\bQ^\top(\bx^\toptzero - \bx^*) - \eta \bQ^\top (\bA\bx^\toptzero-\bb) \\
&= \by^\toptzero - \eta \bQ^\top (\bQ\bLambda\bQ^\top\bx^\toptzero-\bb) 
= \by^\toptzero - \eta  (\bLambda\bQ^\top\bx^\toptzero-\bQ^\top\bb) \\
&= \by^\toptzero - \eta \bLambda\bQ^\top (\bx^\toptzero-\bx^*) = \by^\toptzero - \eta \bLambda \by^\toptzero \\
&= (\bI - \eta \bLambda)\by^\toptzero  = (\bI - \eta \bLambda)^t\by^{(1)}. \\
\end{aligned}
$$
From this, the error at each iteration follows:
\begin{equation}\label{equation:vanilla-gd-closedform}
\normtwo{\bx^\toptone - \bx^*}^2 = \normtwo{\bQ\by^\toptone}^2 = \normtwo{\bQ(\bI - \eta \bLambda)^t\by^{(1)}}^2 = \normtwo{\sum_{i=1}^{n} y_{i}^{(1)} \cdot (1-\eta \lambda_i)^t \bq_i}^2,
\end{equation}
where $\by^{(1)}$ depends on the initial parameter $\bx^{(1)}$, and $y_{i}^{(1)}$ is the $i$-th element of $\by^{(1)}$. 
Intuitively, $\by^\toptone$ represents  the error in the $\bQ$-basis at iteration $t+1$. 
Equation~\eqref{equation:vanilla-gd-closedform} highlights that the learning rate should satisfy:
\begin{equation}\label{equation:vanillagd-quandr-rate-chgoices}
\abs{1-\eta\lambda_i} < 1, \gap \forall \,\, i\in \{1,2,\dots, n\}.
\end{equation}
The error consists of $n$ terms, each with its own convergence rate, governed by $\abs{1-\eta\lambda_i}$. 
The closer this value is to 1, the slower the convergence in that dimension \citep{shewchuk1994introduction, o2015adaptive, goh2017momentum}.

\begin{figure}[h]
%\begin{SCfigure}%[H]
\centering
\includegraphics[width=0.5\textwidth]{imgs/rate_convergen_steepest.pdf}
\caption{Rate of convergence (per iteration) in  GD with fixed stepsize. The $y$-axis is $\frac{\kappa-1}{\kappa+1}$.}
\label{fig:rate_convergen_vanillaGD}
%\end{SCfigure}
\end{figure}
\index{Rate of convergence}

To ensure convergence, the learning rate must satisfy that $\abs{1-\eta\lambda_i} < 1$, implying $0<\eta\lambda_i <2$ for $i$ in $\{1,2,\ldots, n\}$. Therefore, the overall rate of convergence is determined by the slowest component:
$$
\text{rate}(\eta) = \max \{\abs{1-\eta\lambda_1},  \abs{1-\eta\lambda_n}\},
$$
since $\lambda_1\geq \lambda_2\geq \ldots \geq \lambda_n$. The optimal learning rate is obtained when the first and last eigenvectors converge at the same rate, i.e., $\eta\lambda_1-1 =1- \eta\lambda_n$:
\begin{equation}\label{equation:eta-vanilla-gd}
\text{optimal } \eta = \underset{\eta}{\arg\min} \text{ rate}(\eta) = \frac{2}{\lambda_1+\lambda_n},
\end{equation}
and 
\begin{equation}\label{equation:vanialla-gd-rate}
\text{optimal rate}  = \underset{\eta}{\min} \text{ rate}(\eta) =
\frac{\lambda_1/\lambda_n - 1}{\lambda_1/\lambda_n + 1}
\triangleq\frac{\kappa - 1}{\kappa + 1},
\end{equation}
where $\kappa \triangleq \frac{\lambda_1}{\lambda_n} \geq 1$ is known as the \textit{condition number} (see, for example, \citet{lu2021numerical} for more details). When $\kappa=1$, convergence occurs in a single step. However, as the condition number increases, gradient descent slows down. Figure~\ref{fig:rate_convergen_vanillaGD} illustrates this effect: poorly conditioned matrices $(\kappa\gg 1)$ significantly hinder the convergence of GD.




\index{Quadratic form}
\section{Quadratic Model using Gradient Descent with Line Search}\label{section:quadratic-in-steepestdescent}
Following the discussion of the quadratic form in  gradient descent with a fixed stepsize, we now consider  the quadratic form in gradient descent with exact line search. 
Assuming that  $\bA$ is positive definite, we define $\phi(\eta)$ as follows:
$$
\begin{aligned}
\phi(\eta) \triangleq f(\bx^\toptzero+\eta \bd^\toptzero) &= \frac{1}{2} (\bx^\toptzero+\eta \bd^\toptzero)^\top \bA (\bx^\toptzero+\eta \bd^\toptzero) - \bb^\top (\bx^\toptzero+\eta \bd^\toptzero) +c\\
&= f(\bx^\toptzero) + \eta \bd^\toptzeroTOP \underbrace{\left(\bA\bx^\toptzero - \bb \right)}_{=\nabla f(\bx^\toptzero)=\bg^\toptzero} +\frac{1}{2} \eta^2 \bd^\toptzeroTOP \bA\bd^\toptzero,
\end{aligned}
$$
which is a quadratic function of $\eta$.
A closed-form solution for the line search exists (since $\bd^\toptzeroTOP \bA\bd^\toptzero>0$ for $\bd^\toptzero \neq 0 $ due to positive definiteness):
\begin{equation}\label{equation:eta-gd-steepest}
\eta_t = - \frac{\bd^\toptzeroTOP \bg^\toptzero}{ \bd^\toptzeroTOP \bA\bd^\toptzero }.
\end{equation}
When the search direction is the negative gradient $\bd^\toptzero=-\bg^\toptzero$, the descent update becomes
\begin{equation}\label{equation:steepest-quadratic}
\begin{aligned}
\text{(GD with line search)}: \quad \bx^\toptone 
&=  \bx^\toptzero + \eta_t \bd^\toptzero
= \bx^\toptzero  -  \frac{\bd^\toptzeroTOP \bg^\toptzero}{ \bd^\toptzeroTOP \bA\bd^\toptzero } \bd^\toptzero \\
& =\bx^\toptzero   - \frac{\bg^\toptzeroTOP \bg^\toptzero}{ \bg^\toptzeroTOP \bA\bg^\toptzero } \bg^\toptzero.
\end{aligned}
\end{equation}


\begin{figure}[h]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[Contour plot and the descent direction. The red dot represents the optimal point.]{\label{fig:quadratic_steepest_contour_tilt}
\includegraphics[width=0.485\linewidth]{imgs/quadratic_steepest_contour_tilt.pdf}}
\subfigure[Intersection of the loss surface and the vertical plane through the descent direction.]{\label{fig:quadratic_steepest_surface_tilt}
\includegraphics[width=0.485\linewidth]{imgs/quadratic_steepest_surface_tilt.pdf}}
\subfigure[Intersection of the loss surface and the vertical plane through the descent direction in two-dimensional space.]{\label{fig:quadratic_steepest_tilt_intersection}
\includegraphics[width=0.485\linewidth]{imgs/quadratic_steepest_tilt_intersection.pdf}}
\subfigure[Various gradients along the line through the descent direction, where the gradient at the center point is orthogonal to the gradient of the previous step.]{\label{fig:quadratic_steepest_tilt_gradient_direction}
\includegraphics[width=0.485\linewidth]{imgs/quadratic_steepest_tilt_gradient_direction.pdf}}
\caption{Illustration of GD with line search applied to a quadratic form with $\bA=\scriptsize\begin{bmatrix}
20 & 7 \\ 5 & 5
\end{bmatrix}$, $\bb=\bzero$, and $c=0$. The procedure is at $\bx^\toptzero=[-3,3.5]^\top$ for the $t$-th iteration.
}
\label{fig:quadratic_steepest_tilt}
\end{figure}

\begin{figure}[h]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[GD, $\eta=0.02$.]{\label{fig:momentum_gd_conjugate2}
\includegraphics[width=0.31\linewidth]{./imgs/steepest_gd_mom-0_lrate-2.pdf}}
\subfigure[GD, $\eta=0.08$.]{\label{fig:momentum_gd_conjugate8}
\includegraphics[width=0.31\linewidth]{./imgs/steepest_gd_mom-0_lrate-8.pdf}}
\subfigure[GD with line search.]{\label{fig:conjguatecy_zigzag2}
\includegraphics[width=0.31\linewidth]{./imgs/steepest_gd_bisection.pdf}}
\caption{Illustration of GD with line search applied to a quadratic form with $\bA=\scriptsize\begin{bmatrix}
20 & 7 \\ 5 & 5
\end{bmatrix}$, $\bb=\bzero$, and $c=0$. The procedure is at $\bx^\toptzero=[-3,3.5]^\top$ for the $t$-th iteration. The example is executed for 10 iterations by  GD with $\eta=0.02$,  GD with $\eta=0.08$, and GD with line search, respectively. We observe the cumbersome choice of learning rates in vanilla GD and the zigzag trajectory in GD with line search due to the orthogonality between each gradient and the previous gradient (Lemma~\ref{lemm:linear-search-orghonal}).
}
\label{fig:quadratic_steepest_tilt22}
\end{figure}

A concrete example is presented in Figure~\ref{fig:quadratic_steepest_tilt}, where $\bA=\scriptsize\begin{bmatrix}
20 & 7 \\ 5 & 5
\end{bmatrix}$, $\bb=\bzero$, and $c=0$. 
Suppose at the $t$-th iteration, the parameter is positioned at $\bx^\toptzero=[-3,3.5]^\top$. 
Additionally, Figure~\ref{fig:quadratic_steepest_contour_tilt} shows  the descent direction using the negative gradient. 
The GD with line search involves selecting the learning rate $\eta_t$ by minimizing $\phi(\eta) = f(\bx^\toptzero + \eta\bd^\toptzero)$, which is equivalent to determining the point on the intersection of the vertical plane through the descent direction and the paraboloid defined by the loss function $f(\bx)$, as shown in Figure~\ref{fig:quadratic_steepest_surface_tilt}. Figure~\ref{fig:quadratic_steepest_tilt_intersection} further illustrates the parabola resulting from the intersection of the two surfaces. 
In Figure~\ref{fig:quadratic_steepest_tilt_gradient_direction}, various gradients along the line through the descent direction are displayed, where the gradient at the center point is orthogonal to the gradient of the previous step $\nabla f(\bx^\toptone)^\top \bd^\toptzero$, as proved in Lemma~\ref{lemm:linear-search-orghonal}. The black arrows represent the gradients, and the blue arrows are the projection of these gradients along $\bd^\toptzero = -\nabla f(\bx^\toptzero)$. 
An intuitive explanation for this orthogonality at the minimum is as follows: the slope of the parabola (Figure~\ref{fig:quadratic_steepest_tilt_intersection}) at any point is equal to the magnitude of the projection of the gradients onto the search direction (Figure~\ref{fig:quadratic_steepest_tilt_gradient_direction}) \citep{shewchuk1994introduction}. These projections represent the rate of increase of the loss function $f(\bx)$ as the point traverses the search line; and the minimum of $f(\bx)$ occurs where the projection is zero, which corresponds to the point where the gradient is orthogonal to the search line.

The example is executed for 10 iterations in Figure~\ref{fig:momentum_gd_conjugate2}, Figure~\ref{fig:momentum_gd_conjugate8}, and Figure~\ref{fig:conjguatecy_zigzag2} using  GD with $\eta=0.02$,  GD with $\eta=0.08$, and GD with (exact) line search, respectively. It is evident that  GD involves cumbersome selection of learning rates; and the zigzag trajectory is observed in the  GD with line search, resulting from the orthogonality between each gradient and the previous gradient (Lemma~\ref{lemm:linear-search-orghonal}). However, this limitation will be addressed in conjugate descent (Section~\ref{section:conjugate-descent}).



\index{Spectral decomposition}
\index{Quadratic form}
\subsubsection{Special Case:  Error Vector as an Eigenvector}
To analyze the convergence behavior of GD with line search, we examine specific scenarios. Following \citet{shewchuk1994introduction}, we first introduce two key definitions: the \textit{error vector}, which represents the difference between the parameter $\bx^\toptzero$ at the $t$-th iteration and the optimal parameter $\bx^*$, and the \textit{residual vector}, which measures the difference between the target vector $\bb$ and the predicted value $\bA\bx^\toptzero$ at iteration $t$.

\begin{definition}[Error and Residual Vector]\label{definition:error-gd-}
At iteration $t$, the \textit{error vector} is defined as $\be^\toptzero \triangleq \bx^\toptzero - \bx^*$, which quantifies the distance of the iterate from the optimal solution, where $\bx^* =\bA^{-1}\bb$ when $\bA$ is positive definite. 
Substituting this into Equation~\eqref{equation:steepest-quadratic}, the update for the error vector become
\begin{equation}\label{equation:steepest-quadratic-error}
\be^\toptone = \be^\toptzero - \frac{\bg^\toptzeroTOP \bg^\toptzero}{ \bg^\toptzeroTOP \bA\bg^\toptzero } \bg^\toptzero.
\end{equation}
Additionally, the \textit{residual vector} is defined as $\br^\toptzero \triangleq \bb - \bA\bx^\toptzero$, which indicates how far the iterate is from the correct value of $\bb$. 
Since $\bA$ is positive definite, the residual is equal to the negative gradient and the descent direction, i.e., $\br^\toptzero = \bd^\toptzero = -\bg^\toptzero$ (we may use $-\bg^\toptzero$ and $\br^\toptzero$ interchangeably when $\bA$ is positive definite). 
\end{definition} 


We first consider the case where the error vector $\be^\toptzero$ at iteration $t$ is an eigenvector of $\bA$ corresponding to eigenvalue $\lambda_t$, i.e., $\bA\be^\toptzero = \lambda_t\be^\toptzero $. Then, the gradient vector (for positive definite $\bA$ by \eqref{equation:symmetric_gd_gradient}) becomes
$$
\bg^\toptzero = \bA\bx^\toptzero-\bb = \bA\bigg(\bx^\toptzero-\underbrace{\bA^{-1}\bb}_{=\bx^*}\bigg) =\bA\be^\toptzero = \lambda_t\be^\toptzero,
$$ 
which is also an eigenvector of $\bA$ corresponding to the eigenvalue  $\lambda_t$, i.e., $\bA\bg^\toptzero = \lambda_t \bg^\toptzero$.
By \eqref{equation:steepest-quadratic-error}, the update for the $(t+1)$-th iteration is 
$$
\begin{aligned}
\be^\toptone &= \be^\toptzero   - \frac{\bg^\toptzeroTOP \bg^\toptzero}{ \bg^\toptzeroTOP \bA\bg^\toptzero } \bg^\toptzero
=\be^\toptzero - \frac{\bg^\toptzeroTOP \bg^\toptzero}{ \lambda_t \bg^\toptzeroTOP \bg^\toptzero } (\lambda_t\be^\toptzero) = \bzero .
\end{aligned}
$$
Thus, when $\be^\toptzero$ is an eigenvector of $\bA$, the GD with line search converges to the solution in just one additional step. 
A concrete example is shown in Figure~\ref{fig:steepest_gd_bisection_eigenvector}, where $\bA=\scriptsize\begin{bmatrix}
20 & 5 \\ 5 & 5
\end{bmatrix}$, $\bb=\bzero$, and $c=0$.

\begin{figure}[h]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[GD with line search, $\bA\be^\toptzero=\lambda_t\be^\toptzero$.]{\label{fig:steepest_gd_bisection_eigenvector}
\includegraphics[width=0.481\linewidth]{./imgs/steepest_gd_bisection_eigenvector.pdf}}
\subfigure[GD with line search, $\lambda_1=\lambda_2$.]{\label{fig:steepest_gd_bisection_eigenvector_sameeigenvalue}
\includegraphics[width=0.481\linewidth]{./imgs/steepest_gd_bisection_sameeigenvalue.pdf}}
\caption{Illustration of special cases for GD with line search applied to quadratic forms. $\bA=\scriptsize\begin{bmatrix}
20 & 5 \\ 5 & 5
\end{bmatrix}$, $\bb=\bzero$, $c=0$, and starting point to descent is $\bx^\toptzero=[-1.3, 4.3]^\top$ for Figure~\ref{fig:steepest_gd_bisection_eigenvector}. $\bA=\scriptsize\begin{bmatrix}
20 & 0 \\ 0 & 20
\end{bmatrix}$, $\bb=\bzero$, $c=0$, and starting point to descent is $\bx^\toptzero=[-1, 2]^\top$ for Figure~\ref{fig:steepest_gd_bisection_eigenvector_sameeigenvalue}.}
\label{fig:steepest_specialcases}
\end{figure}

\subsubsection{Special Case: Spherical}
When $\bA$ is positive definite, it admits a spectral decomposition (Theorem~\ref{theorem:spectral_theorem}):
$$
\bA=\bQ\bLambda\bQ^\top \in \real^{n\times n} 
\qquad\implies\qquad
\bA^{-1} = \bQ\bLambda^{-1}\bQ^\top,
$$ 
where the columns of $\bQ = [\bq_1, \bq_2, \ldots , \bq_n]$ are mutually orthonormal eigenvectors of $\bA$, and the entries of $\bLambda = \diag(\lambda_1, \lambda_2, \ldots , \lambda_n)$ are the  corresponding eigenvalues of $\bA$, which are positive real and ordered as $\lambda_1\geq \lambda_2\geq \ldots \geq \lambda_n$. Since the eigenvectors are chosen to be mutually orthonormal, they satisfy:
$$
\bq_i^\top \bq_j =
\left\{
\footnotesize
\begin{aligned}
&1, \gap i=j;\\
&0, \gap i\neq j.
\end{aligned}
\right.
$$
Moreover, the eigenvectors form a complete basis for $\real^n$, allowing any error vector $\be^\toptzero  \in \real^n$ to be expressed as a linear combination of these eigenvectors:
\begin{equation}\label{equation:steepest-et-eigen-decom}
\be^\toptzero = \sum_{i=1}^{n} \alpha_i \bq_i,
\end{equation}
where $\alpha_i$ represents  the component of $\be^\toptzero$ along the direction of $\bq_i$.
Then the gradient vector  can be obtained by  
\begin{equation}\label{equation:steepest-eigen-decom-part}
\begin{aligned}
\bg^\toptzero = \bA\bx^\toptzero-\bb = \bA\bigg(\bx^\toptzero-\underbrace{\bA^{-1}\bb}_{=\bx^*}\bigg) =\bA\be^\toptzero
 = 
\bA  \sum_{i=1}^{n} \alpha_i \bq_i
= 
\sum_{i=1}^{n} \alpha_i \lambda_i\bq_i,
\end{aligned}
\end{equation} 
which is also a linear combination of eigenvectors, with each component scaled by the corresponding eigenvalue $\lambda_i$.
Again, by \eqref{equation:steepest-quadratic-error}, the update for the $(t+1)$-th iteration is  computed as follows:
$$
\begin{aligned}
\be^\toptone &= \be^\toptzero   - \frac{\bg^\toptzeroTOP \bg^\toptzero}{ \bg^\toptzeroTOP \bA\bg^\toptzero } \bg^\toptzero
=\be^\toptzero - \frac{\sum_{i=1}^{n}\alpha_i^2\lambda_i^2}{ \sum_{i=1}^{n}\alpha_i^2\lambda_i^3 } \sum_{i=1}^{n} \alpha_i \lambda_i\bq_i 
 .
\end{aligned}
$$
The above equation indicates  that when only one component of $\alpha_i$'s is nonzero (i.e., $\be^\toptzero = \alpha_i\bq_i$), the convergence can be  achieved in only one additional step as well, as illustrated in Figure~\ref{fig:steepest_gd_bisection_eigenvector}.

More specially, when $\lambda_1=\lambda_2=\ldots =\lambda_n=\lambda$, i.e., all the eigenvalues are the same, it then follows that 
$$
\begin{aligned}
\be^\toptone &= \be^\toptzero   - \frac{\bg^\toptzeroTOP \bg^\toptzero}{ \bg^\toptzeroTOP \bA\bg^\toptzero } \bg^\toptzero
=\be^\toptzero - \frac{\sum_{i=1}^{n}\alpha_i^2}{ \sum_{i=1}^{n}\alpha_i^2 }  \be^\toptzero 
= \bzero .
\end{aligned}
$$
Thus, for any arbitrary initial error vector $\be^\toptzero$ (which is always an eigenvector of $\bA$), the algorithm converges to the solution in a single step as well. A specific example is shown in Figure~\ref{fig:steepest_gd_bisection_eigenvector_sameeigenvalue}, where $\bA=\scriptsize\begin{bmatrix}
20 & 0 \\ 0 & 20
\end{bmatrix}$, $\bb=\bzero$, and $c=0$.

\subsubsection{General Convergence Analysis for Symmetric PD Quadratic}\label{section:general-converg-steepest}
To explore general convergence results, we introduce the \textit{energy norm}~\footnote{Also known as the $\bQ$-norm in \eqref{equation:q_norm}.} for the error vector, defined as $\norm{\be}_{\bA} = (\be^\top\bA\be)^{1/2}$. It can be shown that minimizing $\normbig{\be^\toptzero}_{\bA}$ is equivalent to minimizing $f(\bx^\toptzero)$ due to the following relation:
\begin{equation}\label{equation:energy-norm-equivalent}
\norm{\be}_{\bA}^2  = 2f(\bx^\toptzero) \underbrace{- 2f(\bx^*) -2\bb^\top\bx^*}_{\text{constant}}.
\end{equation}
Using  the definition of the energy norm, Equation~\eqref{equation:steepest-quadratic-error}, and the  positive definiteness of $\bA$, we express the update of the energy norm sequence as follows:
\begin{equation}\label{equation:energy-norm-leq}
\small
\begin{aligned}
\normbig{\be^\toptone}_{\bA}^2 
&= (\be^\toptone)^\top \bA\be^\toptone 
= \bigg(\be^\toptzero - \frac{\bg^\toptzeroTOP \bg^\toptzero}{ \bg^\toptzeroTOP \bA\bg^\toptzero } \bg^\toptzero\bigg)^\top \bA \bigg(\be^\toptzero - \frac{\bg^\toptzeroTOP \bg^\toptzero}{ \bg^\toptzeroTOP \bA\bg^\toptzero } \bg^\toptzero\bigg)\\
&=\normbig{\be^\toptzero}_{\bA}^2 + \left(\frac{\bg^\toptzeroTOP \bg^\toptzero}{ \bg^\toptzeroTOP \bA\bg^\toptzero }\right)^2 \bg^\toptzeroTOP \bA\bg^\toptzero 
- 2 \frac{\bg^\toptzeroTOP \bg^\toptzero}{ \bg^\toptzeroTOP \bA\bg^\toptzero } \bg^\toptzeroTOP \bA \be^\toptzero   \\
&\stackrel{\dag}{=}\normbig{\be^\toptzero}_{\bA}^2 - \frac{(\bg^\toptzeroTOP \bg^\toptzero)^2}{ \bg^\toptzeroTOP \bA\bg^\toptzero }   
=\normbig{\be^\toptzero}_{\bA}^2 \cdot\bigg(1- \frac{(\bg^\toptzeroTOP \bg^\toptzero)^2}{ \bg^\toptzeroTOP \bA\bg^\toptzero \cdot \be^\toptzeroTOP \bA\be^\toptzero}\bigg)\\
&\stackrel{\ddag}{=}\normbig{\be^\toptzero}_{\bA}^2  \cdot\left(1- \frac{(\sum_{i=1}^{n}\alpha_i^2\lambda_i^2)^2}{(\sum_{i=1}^{n}\alpha_i^2\lambda_i^3) \cdot (\sum_{i=1}^{n}\alpha_i^2\lambda_i)}\right)
\triangleq\normbig{\be^\toptzero}_{\bA}^2 \cdot r^2, 
\end{aligned}
\end{equation}
where the equality ($\dag$) follows from $\bA\be^\toptzero = \bg^\toptzero$, and   the equality ($\ddag$) follows from \eqref{equation:steepest-et-eigen-decom} and \eqref{equation:steepest-eigen-decom-part}.
The value $r^2 \triangleq\left(1- \frac{(\sum_{i=1}^{n}\alpha_i^2\lambda_i^2)^2}{(\sum_{i=1}^{n}\alpha_i^2\lambda_i^3) \cdot (\sum_{i=1}^{n}\alpha_i^2\lambda_i)}\right)$
determines the rate of convergence. 
By convention, we assume the eigenvalues of $\bA$ are ordered in descending magnitude and they are all positive, i.e.,  $\lambda_1 \geq \lambda_2\geq \ldots \geq \lambda_n>0$, since $\bA$ is positive definite. 
Then the condition number is defined as $\kappa \triangleq \frac{\lambda_1}{\lambda_n}$. 
Additionally, let $\kappa_i \triangleq \frac{\lambda_i}{\lambda_n}$ and $\sigma_i \triangleq \frac{\alpha_i}{\alpha_1}$. 
Then it follows that:
$$
r^2 = 
1- 
\frac{ (\kappa^2+ \sum_{i=\textcolor{mylightbluetext}{2}}^{n}\sigma_i^2\kappa_i^2)^2}
{ (\kappa^3 + \sum_{i=\textcolor{mylightbluetext}{2}}^{n}\sigma_i^2\kappa_i^3) \cdot 
( \kappa +\sum_{i=\textcolor{mylightbluetext}{2}}^{n}\sigma_i^2\kappa_i)}
.
$$
Thus, the rate of convergence is influenced by $\kappa$, $\sigma_i$'s, and $\kappa_i$'s, where $|\kappa_i|\geq  1$ for $i\in \{2,3,\ldots,n\}$. 

\paragrapharrow{Two-dimensional case.} Specifically, when $n=2$, we have 
\begin{equation}\label{equation:2d-rate-steepest}
r^2=
1- 
\frac{ (\kappa^2+ \sigma_2^2)^2}
{ (\kappa^3 + \sigma_2^2) \cdot 
( \kappa +\sigma_2^2)} .
\end{equation}
Figure~\ref{fig:converge_contour_steepest} illustrates  the value $r^2$ as a function of $\kappa$ and $\sigma_2$. When $n=2$, from \eqref{equation:steepest-et-eigen-decom}, we have 
\begin{equation}\label{equation:steepest-et-eigen-decom-2d}
\be^\toptzero = \alpha_1 \bq_1+\alpha_2\bq_2. % = \alpha_1 (\bq_1 + \sigma_2 \bq_2).
\end{equation}
This confirms the two special examples shown in Figure~\ref{fig:steepest_specialcases}: when $\be^\toptzero$ is an eigenvector of $\bA$, it follows that:
$$
\begin{aligned}
\text{case 1: }	&\alpha_2=0 \qquad\implies\qquad  \sigma_2 = \alpha_2/\alpha_1 \rightarrow 0;\\
\text{case 2: }	&\alpha_1=0 \qquad\implies\qquad  \sigma_2 = \alpha_2/\alpha_1 \rightarrow \infty.
\end{aligned}
$$
That is, the slope of $\sigma_2$ is either zero or infinite, the rate of convergence approaches  zero and it converges instantly in just one step (example in Figure~\ref{fig:steepest_gd_bisection_eigenvector}). Similarly, when the eigenvalues are identical ($\kappa=1$),  the rate of convergence is also zero (example in Figure~\ref{fig:steepest_gd_bisection_eigenvector_sameeigenvalue}).

\begin{figure}[h!]
\centering                       
\vspace{-0.15cm}                 
\subfigtopskip=2pt               
\subfigbottomskip=2pt             
\subfigcapskip=-5pt              
\subfigure[Demonstration of the rate of convergence in GD with line search in the two-dimensional case. When $\sigma_2=0, \infty$, or $\kappa=1$, the rate of convergence is 0, resulting in immediate convergence within a single step. 
These two cases correspond to $\be^\toptzero$ being an eigenvector of $\bA$ or the eigenvalues being identical, respectively, as illustrated  in Figure~\ref{fig:steepest_specialcases}.]{\label{fig:converge_contour_steepest}
	\includegraphics[width=0.48\linewidth]{./imgs/converge_contour_steepest.pdf}}
\subfigure[Upper bound on the rate of convergence (per iteration) for GD with line search in the two-dimensional case. The $y$-axis represents $\frac{\kappa-1}{\kappa+1}$.]{\label{fig:rate_convergen_steepest}
	\includegraphics[width=0.48\linewidth]{./imgs/rate_convergen_steepest.pdf}}
\caption{Rate of convergence for GD with line search, the lower the better.}
\label{fig:caaa}
\end{figure}



%\begin{figure}[h]
%%\begin{SCfigure}
%\centering
%\includegraphics[width=0.6\textwidth]{imgs/converge_contour_steepest.pdf}
%\caption{Demonstration of the rate of convergence in GD with line search in two-dimensional case. When $\sigma_2=0, \infty$, or $\kappa=1$, the rate of convergence is 0, resulting in immediate convergence within a single step. 
%These two cases correspond to $\be^\toptzero$ being an eigenvector of $\bA$ or the eigenvalues being identical, respectively, as illustrated  in Figure~\ref{fig:steepest_specialcases}.}
%\label{fig:converge_contour_steepest}
%%\end{SCfigure}
%\end{figure}
%\begin{figure}[h]
%%\begin{SCfigure}
%\centering
%\includegraphics[width=0.5\textwidth]{imgs/rate_convergen_steepest.pdf}
%\caption{Upper bound on the rate of convergence (per iteration) in GD with line search with two-dimensional parameter. The $y$-axis is $\frac{\kappa-1}{\kappa+1}$.}
%\label{fig:rate_convergen_steepest}
%%\end{SCfigure}
%\end{figure}
\index{Rate of convergence}

\paragrapharrow{Worst case.} We recall that $\sigma_2 = \alpha_2/\alpha_1$ determines the error vector $\be^\toptzero$ (Equation~\eqref{equation:steepest-et-eigen-decom} or Equation~\eqref{equation:steepest-et-eigen-decom-2d}), which, in turn, determines the point $\bx^\toptzero$ in the two-dimensional case. 
A natural question is: what is the worst possible starting point for convergence?
For a fixed $\kappa$ (i.e., $\bA$ and the loss function $f(\bx)$ are fixed), assuming $t=1$,  we seek the initial point $\bx^{(1)}$ that results in the slowest descent. 
It can be shown that the rate of convergence in \eqref{equation:2d-rate-steepest} is maximized when $\sigma_2=\pm \kappa$:
$$
\begin{aligned}
r^2 &\leq 1-\frac{4 \kappa^2}{\kappa^5+2\kappa^4+\kappa^3}= \frac{(\kappa-1)^2}{(\kappa+1)^2}.
\end{aligned}
$$
Substituting into \eqref{equation:energy-norm-leq}, we obtain 
$$
\begin{aligned}
\normbig{\be^\toptone}_{\bA}^2 &\leq \normbig{\be^\toptzero}_{\bA}^2 \cdot \frac{(\kappa-1)^2}{(\kappa+1)^2}
\quad\implies\quad
\normbig{\be^\toptone}_{\bA} \leq \normbig{\be^{(1)}}_{\bA} \cdot\left(\frac{\kappa-1}{\kappa+1}\right)^t.
\end{aligned}
$$
The upper bound on the convergence rate per iteration is shown in Figure~\ref{fig:rate_convergen_steepest}. 
As expected, the more \textit{ill-conditioned} the matrix, the slower the convergence of gradient descent with line search. 
Notably, the upper bound on the convergence rate matches that of  GD with a fixed stepsize; see \eqref{equation:vanialla-gd-rate}. 
%However, a key difference is that the rate for GD with a fixed stepsize is derived in terms of the $\by^\toptzero$ vector in \eqref{equation:vanilla-yt}; while the rate of GD with line search is presented in terms of the energy norm. 
However, a key difference is that the rate of GD with a fixed stepsize in \eqref{equation:vanialla-gd-rate} is obtained by selecting a specific learning rate, as shown in \eqref{equation:eta-vanilla-gd}. 
This makes the rate of GD with a fixed stepsize more of  a tight bound. In practice, GD with a fixed stepsize converges slower than GD with line search, as evident in the examples shown in Figure~\ref{fig:momentum_gd_conjugate2}, Figure~\ref{fig:momentum_gd_conjugate8}, and Figure~\ref{fig:conjguatecy_zigzag2}.



\index{Spectral decomposition}
\index{Eigenvalue decomposition}
\index{Quadratic form}
\section{Quadratic Model using Momentum}\label{section:quadratic-in-momentum}
We have discussed the gradient descent method with momentum in \eqref{equation:gd_with_momentum}.
Following the discussions of the quadratic form in GD with a fixed stepsize (Section~\ref{section:quadratic_vanilla_GD})  and GD with line search (Section~\ref{section:quadratic-in-steepestdescent}), we now examine  the quadratic form in GD with momentum. The general update is:
$$
\begin{aligned}
\bd^\toptzero &= \rho\bd^\toptminus - \eta_t\nabla f(\bx^\toptzero); \\
\bx^\toptone &= \bx^\toptzero + \bd^\toptzero,
\end{aligned}
$$
where $\nabla f(\bx^\toptzero) = \bA\bx^\toptzero - \bb$ under the assumption that  $\bA$ is positive definite for the quadratic form. 
Fixing the learning rate $\eta_t = \eta$, the update simplifies to:
$$
\begin{aligned}
\bd^\toptzero &= \rho\bd^\toptminus - \eta(\bA\bx^\toptzero - \bb); \\
\bx^\toptone &= \bx^\toptzero + \bd^\toptzero.
\end{aligned}
$$
Again, define the iterate vectors as follows:
$$
\left\{
\begin{aligned}
\by^\toptzero &\triangleq \bQ^\top(\bx^\toptzero - \bx^*);\\
\bz^\toptzero &\triangleq \bQ^\top  \bd^\toptzero,
\end{aligned}
\right.
$$
where $\bx^* = \bA^{-1}\bb$ under the assumption that $\bA$ is positive definite, and $\bA=\bQ\bLambda\bQ^\top$ is the spectral decomposition of matrix $\bA$. This construction leads to the following update rule:
$$
\begin{aligned}
\bz^\toptzero &= \rho \bz^\toptminus - \eta\bLambda \by^\toptzero; \\
\by^\toptone &= \by^\toptzero + \bz^\toptzero,
\end{aligned}
\quad\implies\quad
\begin{bmatrix}
\bz^\toptzero \\
\by^\toptone
\end{bmatrix}
= 
\begin{bmatrix}
\rho \bI & -\eta \bLambda \\
\rho \bI & -\eta \bLambda + \bI
\end{bmatrix}
\begin{bmatrix}
\bz^\toptminus \\
\by^\toptzero
\end{bmatrix}.
$$
Alternatively, this results in the per-dimension update:
\begin{equation}\label{equation:momentum-quadra-generalformula}
\begin{bmatrix}
z_{i}^\toptzero \\
y_{i}^\toptone
\end{bmatrix}
= 
\begin{bmatrix}
\rho  & -\eta\lambda_i \\
\rho  & 1-\eta \lambda_i 
\end{bmatrix}^t
\begin{bmatrix}
z_{i}^{(0)} \\
y_{i}^{(1)}
\end{bmatrix}
\triangleq
\bB^t
\begin{bmatrix}
z_{i}^{(0)} \\
y_{i}^{(1)}
\end{bmatrix},
\end{equation}
where $z_{i}^\toptzero$ and $y_{i}^\toptzero$ are $i$-th element of $\bz^\toptzero$ and $\by^\toptzero$, respectively, and 
$\bB\triangleq
\footnotesize
\begin{bmatrix}
\rho  & -\eta\lambda_i \\
\rho  & 1-\eta \lambda_i 
\end{bmatrix}$.
Note here, $\bz^{(0)}$ is initialized as a zero vector, and $\by^{(1)}$ is initialized as $\bQ^\top (\bx^{(1)}-\bx^*)$, where $\bx^{(1)}$ represents the initial parameter.
Suppose the eigenvalue decomposition (Theorem~\ref{theorem:eigenvalue-decomposition}) of $\bB$ admits
$$
\bB = \bC\bD \bC^{-1},
$$
where the columns of $\bC$ contain eigenvectors of $\bB$, and $\bD=\diag(\alpha,\beta)$ is a diagonal matrix  containing the eigenvalues of $\bB$. Then $\bB^t = \bC\bD^t\bC^{-1}$ (Remark~\ref{remark:power-eigenvalue-decom}). 
Alternatively, the eigenvalues of $\bB$ can be computed by solving $\det(\bB-\alpha\bI)=0$:
$$
\alpha, \beta = \frac{(\rho+1-\eta\lambda_i) \pm \sqrt{(\rho+1-\eta\lambda_i)^2 -4\rho}}{2}.
$$
We then have by \citet{williams1992n} that
$$
\bB^t=
\left\{
\begin{aligned}
&\alpha^t \frac{\bB-\beta\bI}{\alpha-\beta} - \beta^t \frac{\bB-\alpha\bI}{\alpha-\beta}, \gap &\text{if $\alpha\neq \beta$};\\
&\alpha^{t-1}(t\bB - (t-1)\alpha\bI), \gap &\text{if $\alpha=\beta$}.
\end{aligned}
\right.
$$
Substituting this into \eqref{equation:momentum-quadra-generalformula} yields the following expression:
$$
\begin{bmatrix}
z_{i}^\toptzero \\
y_{i}^\toptone
\end{bmatrix}
=
\bB^{t}
\begin{bmatrix}
z_{i}^{(0)} \\
y_{i}^{(1)}
\end{bmatrix},
$$
where the rate of convergence is controlled by the slower one, $\max\{\abs{\alpha}, \abs{\beta}\}$. When we have $\max\{\abs{\alpha}, \abs{\beta}\}<1$, the GD with momentum is guaranteed to converge. 
In the case of  $\rho=0$, the momentum reduces to the basic GD, the condition for convergence becomes
$$
\max\{\abs{\alpha}, \abs{\beta}\} = \abs{1-\eta\lambda_i} <1, \gap \forall \,\, i\in \{1,2,\ldots, n\},
$$
which aligns with  that in \eqref{equation:vanillagd-quandr-rate-chgoices}.

As an example, consider the quadratic form with $\bA=\footnotesize\begin{bmatrix}
4 & 0\\ 0 & 40
\end{bmatrix}$, which has eigenvalues  $\lambda_1=4$ and $\lambda_2=40$, respectively. Then, the corresponding matrix $\bB$ in \eqref{equation:momentum-quadra-generalformula} is 
$$
\bB_1 = 
\begin{bmatrix}
\rho & -4 \eta\\
\rho & 1-4\eta
\end{bmatrix}
\gap \text{or}\gap 
\bB_2 = 
\begin{bmatrix}
\rho & -40\eta \\
\rho & 1-40\eta
\end{bmatrix}.
$$
It can be shown that when $\eta=0.04, \rho=0.8$, the rate of convergence is approximately  $0.89$; Figure~\ref{fig:momentum_rho8} displays the updates for 20 iterations. 
Although the trajectory exhibits a zigzag pattern, the method still converges. 
However, when $\eta=0.04, \rho=1$, the rate of convergence is equal to 1, indicating no actual progress toward convergence; Figure~\ref{fig:momentum_rho10} shows the updates for 20 iterations, where the trajectory diverges, even though it intermittently passes through the optimal point.




\begin{figure}[h]
\centering   
\vspace{-0.35cm}  
\subfigtopskip=2pt  
\subfigbottomskip=2pt  
\subfigcapskip=-5pt  
\subfigure[Momentum $\rho=0.2$, convergence rate$\approx0.79$.]{\label{fig:momentum_rho2}
\includegraphics[width=0.31\linewidth]{./imgs/mom_surface_lrate40_gd-1_xy-2-5_mom-2.pdf}}
\subfigure[Momentum $\rho=0.8$, convergence rate$\approx0.89$.]{\label{fig:momentum_rho8}
\includegraphics[width=0.31\linewidth]{./imgs/mom_surface_lrate40_gd-1_xy-2-5_mom-8.pdf}}
\subfigure[Momentum $\rho=1$, convergence rate=1.]{\label{fig:momentum_rho10}
\includegraphics[width=0.31\linewidth]{./imgs/mom_surface_lrate40_gd-1_xy-2-5_mom-10.pdf}}
\caption{Momentum creates its own oscillations. The learning rates $\eta$ are set to be 0.04 for all scenarios.}
\label{fig:momentum_own_oscilation}
\end{figure}



\begin{problemset}
\item \textbf{Constructing symmetric form.} Let $P(\bA)\triangleq\frac{1}{2}(\bA+\bA^\top)$ if $\bA\in\real^{n\times n}$. Show that 
\begin{itemize}
	\item \textit{Null space.} $\nspace(\bA)\subset \nspace(P(\bA))$ and $\nspace(\bA^\top)\subset \nspace(P(\bA))$ such that $\rank(P(\bA))\leq \rank(\bA)$.
	\item When $\rank(P(\bA))= \rank(\bA)$, it follows that $\bA$, $\bA^\top$, and $P(\bA)$ have the same null space.
\end{itemize}
\textit{Hint: Consider the quadratic form $\bx^\top\bA\bx$ and $\bx^\top\bA^\top\bx$.}

\item Let $\bA,\bB\in\real^{n\times n}$ be  positive semidefinite matrices. Show that $\bA+\bB$ is also positive semidefinite.
\item Let $\bA\in\real^{n\times n}$ and $\bB\in\real^{m\times m}$ be  two symmetric matrices. Show that the following two claims are equivalent:
\begin{enumerate}
	\item $\bA$ and $\bB$ are positive semidefinite.
	\item $\scriptsize\begin{bmatrix}
		\bA& \bzero \\
		\bzero & \bB
	\end{bmatrix}$ is positive semidefinite.
\end{enumerate}
\item 
Given a matrix $\bB\in\real^{n\times k}$ and let $\bA=\bB\bB^\top$. Show that 
\begin{enumerate}
	\item $\bA$ is positive semidefinite.
	\item $\bA$ is positive definite if and only if $\bB$ has full row rank.
\end{enumerate}
\item Show that if $\bA$ is positive semidefinite and nonsingular, then $\bA^{-1}$ is positive definite.
%\item Given a positive semidefinite matrix $\bA$, show that its eigenvalue $\lambda$ is nonnegative.
\end{problemset}



