\newpage
\chapter{First-Order Methods}\label{chapter:gd_convg}
\begingroup
\hypersetup{linkcolor=structurecolor,
	linktoc=page,  % page: only the page will be colored; section, all, none etc
}
\minitoc \newpage
\endgroup

\section{Background and Mathematical Tools}

First-order methods are essential in large-scale optimization, particularly in the fields of machine learning and deep learning. This chapter explores advanced gradient-based techniques, including projected gradient descent, proximal gradient methods, and mirror descent. Additionally, it delves into acceleration strategies such as Nesterovâ€™s accelerated gradient and adaptive approaches like the conditional gradient method. These methodologies form the backbone of modern optimization techniques.


Our primary focus will be on the convergence results of algorithms for problems defined in Definition~\ref{definition:opt_probs_all}. Specifically, we will examine:
\begin{subequations}\label{equation:p1p2}
\begin{align}
\text{(P1)}:\qquad &\min \{f(\bx)\};\\
\text{(P2)}:\qquad &\min \{f(\bx)\} \quad\text{s.t.}\quad \bx\in\sS;\\
\text{(P3)}:\qquad &\min \left\{F(\bx)\triangleq f(\bx)+g(\bx) \right\}.
\end{align}
\end{subequations}
To be more specific, we will consider the \textit{gradient descent (GD) method} for (P1); the \textit{projected gradient descent (PGD)}, \textit{mirror descent}, and \textit{conditional gradient (CG) descent} methods for (P2); the \textit{proximal gradient}, the \textit{G-mirror descent}, and the \textit{generalized conditional gradient (GCG)} methods   for (P3):

When seeking a maximizer of a function, one can simply find a minimizer of the function with opposite sign. In this context, problem (P1) is referred to \textit{unconstrained optimization}, (P2) as \textit{constrained optimization}, and (P3) as a non-convex composite problem. 
The function $f$ or $F$ is called the \textit{objective function} (a.k.a.,  \textit{cost function} or \textit{loss function}), while we use  $\bx^*$ to represent  the \textit{minimum point (or simply minimizer)}.
For (P3), different forms of $g$ lead to various types of optimization problems:
\begin{itemize}
	\item When $g(\bx)=0$,  problem (P3) reduces to an unconstrained problem (P1).
	\item When $g(\bx)=\indicatorS(\bx)$,  problem (P3) becomes a constrained problem (P2).
	\item When $g(\bx)=\lambda\normone{\bx}$,  problem (P3) turns into an $\ell_1$ regularized problem. 
\end{itemize}

\begin{figure}[H]
\centering
\begin{widepage}
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tikzpicture}[>=latex]

\tikzstyle{state} = [draw, very thick, fill=white, rectangle, minimum height=3em, minimum width=7.8em, node distance=3em, font={\sffamily\bfseries}]
\tikzstyle{stateEdgePortion} = [black,thick];
\tikzstyle{stateEdge} = [stateEdgePortion,->];
\tikzstyle{stateEdge2} = [stateEdgePortion,<->];
\tikzstyle{edgeLabel} = [pos=0.5, text centered, font={\sffamily\small}];


\node[state, name=cg, node distance=7em, xshift=-9em, yshift=-15em, fill={colorlu}] {CG (P2)};
\node[state, name=gcg, fill={colorlu}, right=of cg, xshift=18em] {GCG (P3)};
\node[state, name=gd, fill={colorlu}, above=of cg, yshift=0.1em, xshift=0em] {\parbox{5.8em}{GD $\;\;$(P1)\\Line Search}};
\node[state, name=pgd, fill={colorlu}, above=of cg, yshift=0.1em, xshift=14em] {PGD (P2)};
\node[state, name=prox, fill={colorlu}, right=of gd, xshift=18em] {\parbox{6.6em}{Proximal $\;\;$(P3)\\FISTA/Nesterov}};
\node[state, name=mirror, draw, fill={colorlu}, above=of gd, yshift=0.2em, xshift=0em] {Mirror (P2)};
\node[state, name=gmirror, draw, fill={colorlu}, right=of mirror, xshift=18em] {G-Mirror (P3)};



\draw (mirror.east) 
edge[stateEdge] node[edgeLabel, xshift=0.5em, yshift=-0.8em]{Composite} 
($(gmirror.west) + (0em,0em)$);

\draw (pgd.east)
edge[stateEdge] node[edgeLabel, xshift=-0.1em, yshift=0.5em,]{Composite} 
(prox.west) ;

\draw ($(cg.east)$)
edge[stateEdge] node[edgeLabel, xshift=0.5em, yshift=0.5em]{Composite} 
(gcg.west) ;


\draw ($(pgd.west) + (0em,0em)$)
edge[stateEdge, bend left=0] node[edgeLabel, xshift=-0.0em,yshift=0.5em]{Unconstrained} 
(gd.east) ;
\draw ($(pgd.north) + (0em,0em)$)
edge[stateEdge, bend left=-12.5] node[edgeLabel, xshift=1.5em,yshift=-0em]{Non-Euclidean Distance} 
($(mirror.south) + (2.5em,0em)$) ;
\draw ($(pgd.south)$)
edge[stateEdge, bend left=+12.5] node[edgeLabel, xshift=1em, yshift=0em]{Convex Combination Update} 
($(cg.north) + (2.5em,0em)$) ;

\draw ($(prox.north) + (0em,0em)$)
edge[stateEdge] node[edgeLabel, xshift=-1.5em,yshift=0em]{Non-Euclidean Distance} 
(gmirror.south) ;
\draw ($(prox.south)$)
edge[stateEdge] node[edgeLabel, xshift=-1em, yshift=0em]{Convex Combination Update} 
(gcg.north) ;

% draw dotted lines around passive and active closes
\begin{pgfonlayer}{background}
\draw [join=round,cyan,dotted,fill={colormiddle}] ($(cg.south west -| mirror.west) + (-0.5em, -0.5em)$) rectangle ($(mirror.north east -| prox.north east) + (0.6em, 0.5em)$);		%original (0.6,0.5), two-orthogonal-prox
\end{pgfonlayer}


\end{tikzpicture}
}
\end{widepage}
\caption{
Relationship and classification of first-order optimization methods can be understood in the context of different problem types, specifically (P1), (P2), and (P3). The diagram depicts an evolution from fundamental techniques, such as PGD, which are suitable for simpler problem formulations, to more sophisticated approaches like proximal gradient methods and GCG, designed for handling composite and constrained problems.
At the core of these methodologies lies PGD, which serves as a foundational algorithm. Advanced methods build upon it by incorporating concepts such as convex combination updates and non-Euclidean metrics, which enhance their ability to address complex optimization challenges.
}
\label{fig:first_order_opt_map}
\end{figure}

Part of the relationship among these methods is illustrated in Figure~\ref{fig:first_order_opt_map}. The diagram shows an evolution from fundamental techniques, such as PGD, which are suitable for simpler problem formulations, to more sophisticated approaches like proximal gradient methods and GCG, designed for handling composite and constrained problems.
At the core of these methodologies lies PGD, serving as a foundational algorithm. Advanced methods build upon PGD by incorporating concepts such as convex combination updates and non-Euclidean metrics, enhancing their ability to address complex optimization challenges. For this reason, we begin our analysis with PGD and its unconstrained counterpart, GD.



\section*{Descent Lemmas}
We start by proving the descent lemmas for GD and PGD.
Previously, we introduced the descent inequality under the Zoutendijk condition in the proof of Theorem~\ref{theorem:zoutendijk_cond}: $f(\bx^\toptone) -f(\bx^\toptzero) \leq  c_1 \frac{c_2 - 1}{\beta} \cos^2 (\theta_t) \normtwobig{\nabla f(\bx^\toptzero)}^2$, where $0 < c_1 < c_2 < 1$.
This kind of descent inequality is crucial for proving the convergence or the convergence rate of the underlying algorithm.
Analogously, definitions of smoothness and strong convexity lead to descent lemmas for GD or PGD updates: 
$
\bx^+\leftarrow \bx-\eta\nabla f(\bx)
 \text{ or } 
\bx^+ \leftarrow \projectS\left(\bx - \eta \nabla f(\bx)\right),
$
where $\sS$ denotes a set and $\projectS(\cdot)$ denotes the projection operation (Definition~\ref{definition:projec_prox_opt}).
\noindent
\begin{lemma}[Descent Lemmas for Problems (P1) and (P2)]\label{lemma:gdupd_sm_sconv}
For a differentiable function $f$, we  consider the gradient descent update rule $\bx^+\leftarrow \bx-\eta\nabla f(\bx)$ or the projected gradient descent update rule $\bx^+ \leftarrow \projectS\left(\bx - \eta \nabla f(\bx)\right)$ with a constant stepsize $\eta$. 
Then,
\begin{enumerate}[(i)]
\item \label{descent_lem_gdss} \textit{GD for SS.}  Let $f: \real^n \rightarrow \real$ be a differentiable  and $\beta$-smooth function. Then the gradient descent update $\bx^+\leftarrow \bx-\frac{1}{\beta}\nabla f(\bx)$  ensures that:
$$
\textbf{(SS)}:\qquad  f(\bx^+)-f(\bx) \leq -\frac{1}{2\beta} \normtwo{\nabla f(\bx)}^2.
$$
This indicates that the gradient update decreases the function value by an amount proportional to the squared norm of the gradient.~\footnote{See Theorem~\ref{theorem:pgd_smooth} for more insights.}
\item \label{descent_lem_gdscss} \textit{GD for SC and SS.} Let $f: \real^n \rightarrow \real$ be a differentiable, $\alpha$-strongly convex, and $\beta$-smooth function. Then for any $\bx, \by \in \real^n$ and an update of the form $\bx^+ \leftarrow \bx - \frac{1}{\beta} \nabla f(\bx)$:
$$
\textbf{(SC+SS)}:\qquad f(\bx^+) - f(\by) \leq \nabla f(\bx)^\top (\bx - \by) - \frac{1}{2\beta} \normtwo{\nabla f(\bx)}^2 - \frac{\alpha}{2} \normtwo{\bx - \by}^2.~\footnote{In practice, $\by$ can be set to the minimizer $\bx^*$ of the function; see Theorem~\ref{theorem:gd_sc_ss} for more insights.}
$$


\item \label{descent_lem_pgdss} \textit{PGD for  SS.} Let $f: \sS\rightarrow \real$ be a differentiable and $\beta$-smooth function, where $\sS\subseteq\real^n$ is a convex set. Then for any $ \bx, \by \in \sS$, an update $\bx^+ \in \sS$ of the form  $\bx^+ \leftarrow \projectS\left(\bx - \frac{1}{\beta} \nabla f(\bx)\right)$~\footnote{See Algorithm~\ref{alg:pgd_gen}.}, and the function $g: \sS \rightarrow \real^n$ defined as $g(\bx) \triangleq \beta(\bx - \bx^+)$ (i.e., $\bx^+ =\bx-\frac{1}{\beta}g(\bx)$):
$$
\textbf{(PGD+SS)}:\qquad f(\bx^+) - f(\by) \leq g(\bx)^\top (\bx - \by) - \frac{1}{2\beta} \normtwo{g(\bx)}^2.
$$

\item \label{descent_lem_pgdscss} \textit{PGD for SC and SS.} Let $f: \sS\rightarrow \real$ be a differentiable, $\alpha$-strongly convex, and $\beta$-smooth function, where $\sS\subseteq\real^n$ is a convex set. Then for any $ \bx, \by \in \sS$, an update $\bx^+ \in \sS$ of the form $\bx^+ \leftarrow \projectS\left(\bx - \frac{1}{\beta} \nabla f(\bx)\right)$, and the function $g: \sS \rightarrow \real^n$ defined as $g(\bx) \triangleq \beta(\bx - \bx^+)$ (i.e., $\bx^+ =\bx-\frac{1}{\beta}g(\bx)$):
$$
\textbf{(PGD+SC+SS)}:\qquad f(\bx^+) - f(\by) \leq g(\bx)^\top (\bx - \by) - \frac{1}{2\beta} \normtwo{g(\bx)}^2 - \frac{\alpha}{2} \normtwo{\bx - \by}^2.
$$
When $\sS=\real^n$, we have $g(\bx)=\nabla f(\bx)$ and this reduces to (ii); when $\alpha=0$, it reduces to (iii); when $\sS=\real^n$, $\by=\bx$, and $\alpha=0$, this reduces to (i).
\end{enumerate}
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:gdupd_sm_sconv}]
Note that the first part follows directly from the definition of smoothness. 
Since the first three parts are special cases of \ref{descent_lem_pgdscss}, we only prove \ref{descent_lem_pgdscss} here.
%\paragraph{(ii).} By the smoothness and strong convexity, 
%$$
%\begin{aligned}
%f(\bx^+)  - f(\by)  &= \left(f(\bx^+) - f(\bx)\right) + \left(f(\bx) - f(\by)\right) \\
%&\leq \left(\nabla f(\bx)^\top (\bx^+ - \bx) + \frac{\beta}{2} \normtwo{\bx^+ - \bx}^2\right)  + 
%\left(\nabla f(\bx)^\top (\bx - \by) - \frac{\alpha}{2} \normtwo{\bx - \by}^2\right) \\
%&= \nabla f(\bx)^\top (\bx^+ - \by) + \frac{1}{2\beta} \normtwo{\nabla f(\bx)}^2 - \frac{\alpha}{2} \normtwo{\bx - \by}^2 \\
%&= \nabla f(\bx)^\top (\bx - \by) - \frac{1}{2\beta} \normtwo{\nabla f(\bx)}^2 - \frac{\alpha}{2} \normtwo{\bx - \by}^2. \\
%\end{aligned}
%$$
By Projection Property-I (Lemma~\ref{lemma:proj_prop1}), it follows that $\innerproduct{\bx^+-(\bx-\frac{1}{\beta}\nabla f(\bx)), \bx^+-\by}\leq 0$. This implies
\begin{equation}\label{equation:gdupd_sm_sconv1}
\nabla f(\bx)^\top (\bx^+ - \by) \leq g(\bx)^\top (\bx^+ - \by).
\end{equation}
Thus, by the smoothness and strong convexity (Definition~\ref{definition:scss_func}), 
$$
\begin{aligned}
f(\bx^+)  - f(\by)  
&= \left(f(\bx^+) - f(\bx)\right) + \left(f(\bx) - f(\by)\right) \\
&\leq \left(\nabla f(\bx)^\top (\bx^+ - \bx) + \frac{\beta}{2} \normtwo{\bx^+ - \bx}^2\right)  + 
\left(\nabla f(\bx)^\top (\bx - \by) - \frac{\alpha}{2} \normtwo{\bx - \by}^2\right) \\
&\leq   g(\bx)^\top (\bx^+ - \by) + \frac{1}{2\beta} \normtwo{g(\bx)}^2 - \frac{\alpha}{2} \normtwo{\bx - \by}^2 \\
&=  g(\bx)^\top (\bx - \by) - \frac{1}{2\beta} \normtwo{ g(\bx)}^2 - \frac{\alpha}{2} \normtwo{\bx - \by}^2.
\end{aligned}
$$
This completes the proof.
\end{proof}
In the above proof, since $\bx\in\sS$,  by Projection Property-II (Lemma~\ref{lemma:proj_prop2}) and letting $\bz\triangleq \bx-\frac{1}{\beta}\nabla f(\bx)$, we can also show that  that
\begin{equation}\label{equation:gd_des_cor}
\normtwo{\bx^+ - \bx} \leq \normtwo{\bz-\bx}
\quad\implies\quad 
\normtwo{g(\bx)} \leq \normtwo{\nabla f(\bx)}.
\end{equation}
%&\leq \nabla f(\bx)^\top (\bx^+ - \by) + \frac{1}{2\beta} \normtwo{\nabla f(\bx)}^2 - \frac{\alpha}{2} \normtwo{\bx - \by}^2 \\





\begin{algorithm}[h] 
\caption{Gradient Descent Method}
\label{alg:gd_gen}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t$;
\State $\bx^{(t+1)} \leftarrow \bx^{(t)} - \eta_t \nabla f(\bx^{(t)})$ or $\bx^{(t+1)} \leftarrow \bx^{(t)} - \eta_t f^\prime(\bx^{(t)})$, where $ f^\prime(\bx^{(t)})\in \partial f(\bx^{(t)})$;
\EndFor
\State (Output Option 1) Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\State (Output Option 2) Output  $\bx_{\text{avg}}\leftarrow \frac{1}{T}(\sum_{t=1}^{t}\bx^{(t)})$ or $\sum_{t=1}^{T} \frac{2t}{T(T+1)} \bx^{(t)}$;
\State (Output Option 3) Output  $\bx_{\text{best}}\leftarrow \argmin_{t\in\{1,2,\ldots,T\}} f(\bx^{(t)})$;
\end{algorithmic} 
\end{algorithm}

\section{Gradient Descent}\label{section:gd_basic}
For the unconstrained problem (P1) given in \eqref{equation:p1p2}, we consider the gradient descent method as described in Algorithm~\ref{alg:gd_gen}~\footnote{Although convergence results are also applicable to non-differentiable functions using subgradient descent, for brevity, we will focus solely on differentiable cases in this section.}, i.e., using negative gradient direction (steepest descent direction) for the descent method in Algorithm~\ref{alg:struc_gd_gen}.
For convex and smooth functions, the gradient descent method achieves a convergence rate of $\mathcalO(1/T)$.
\begin{theoremHigh}[GD for Convex and SS Functions: $\mathcalO(1/T)$]\label{theorem:pgd_smooth}
Let $f:\real^n\rightarrow \real$ be a  differentiable, convex, and  $\beta$-smooth function defined on $\real^n$. 
Suppose $\{\bx^\toptzero\}_{t > 0}$ is the sequence generated by the gradient descent method (Algorithm~\ref{alg:gd_gen}) for solving  problem (P1)
with a constant stepsize $\eta = \frac{1}{\beta}$, and let $\bx^*$ be an optimizer of $f$. 
Then, for any $T>0$,
$$
f(\bx^{(T)}) - f(\bx^*) \leq \frac{2\beta \normtwo{\bx^{(1)} -\bx^*}^2}{T-1}.
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:pgd_smooth}]
Let $\delta_t \triangleq f(\bx^{(t)}) - f(\bx^*)$. 
By convexity (Theorem~\ref{theorem:conv_gradient_ineq}), it follows that
$$
\delta_t \leq \nabla f(\bx^{(t)})^\top (\bx^{(t)} -\bx^*) \leq \normtwobig{\bx^{(t)} -\bx^*} \cdot \normtwobig{\nabla f(\bx^{(t)})}.
$$
By the update rule and the definition of smoothness (Lemma~\ref{lemma:gdupd_sm_sconv}), we have
$$
f(\bx^{(t+1)}) - f(\bx^{(t)}) \leq -\frac{1}{2\beta} \normtwobig{\nabla f(\bx^{(t)})}^2
\quad\implies\quad 
\delta_{t+1} \leq \delta_t - \frac{1}{2\beta} \normtwobig{\nabla f(\bx^{(t)})}^2.
$$
Combining the above two inequalities yields 
\begin{equation}\label{equation:pgd_smooth1}
\delta_{t+1} \leq \delta_t - \frac{\delta_t^2}{2\beta\normtwobig{\bx^{(t)} -\bx^*}^2}.
\end{equation}
Since the gradient at stationary points vanishes: $\nabla f(\bx^*) = \bzero$, we have
$$
%\small
\begin{aligned}
\normtwobig{\bx^{(t+1)} -\bx^*}^2 
&= \normtwo{\bx^{(t)} - \frac{1}{\beta} \nabla f(\bx^{(t)}) -\bx^*}^2 \\
&= \normtwobig{\bx^{(t)} -\bx^*}^2 - \frac{2}{\beta} \nabla f(\bx^{(t)})^\top (\bx^{(t)} -\bx^*) + \frac{1}{\beta^2} \normtwobig{\nabla f(\bx^{(t)})}^2 \\
&\stackrel{\dag}{\leq} \normtwobig{\bx^{(t)} -\bx^*}^2 - \frac{1}{\beta^2} \normtwobig{\nabla f(\bx^{(t)})}^2 
\leq \normtwobig{\bx^{(t)} -\bx^*}^2.
\end{aligned}
$$
where the inequality ($\dag$) follows from \eqref{equ:charac_smoo_3} in Theorem~\ref{theorem:charac_smoo}.
This shows $\normtwobig{\bx^{(t)} -\bx^*}$ is decreasing with $t$, which with \eqref{equation:pgd_smooth1} implies
$
\delta_{t+1} \leq \delta_t - \frac{\delta_t^2}{2\beta \normtwo{\bx^{(1)} - \bx^*}^2} .
$
Let $z \triangleq \frac{1}{2\beta \normtwo{\bx^{(1)} -\bx^*}^2}$. Then,
$$
z\delta_t^2 + \delta_{t+1} \leq \delta_t 
\iff 
z\frac{\delta_t}{\delta_{t+1}} + \frac{1}{\delta_t} \leq \frac{1}{\delta_{t+1}} 
\implies 
z  \leq \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t} 
\implies 
z(T-1) \leq \frac{1}{\delta_T},
$$
where the last inequality follows by performing telescopic cancellations.
\end{proof}

\begin{theoremHigh}[GD for SC and SS Functions]\label{theorem:gd_sc_ss}
Let $f: \real^n \rightarrow \real$ be a differentiable, $\alpha$-strongly convex, and $\beta$-smooth function. 
Suppose $\{\bx^\toptzero\}_{t > 0}$ is the sequence generated by the gradient descent method (Algorithm~\ref{alg:gd_gen}) for solving  problem (P1),
and let $\bx^*$ be an optimizer of $f$. If the constant stepsize is $\eta=\frac{1}{\beta}$, then,
$$
\normtwo{\bx^{(t+1)} - \bx^*}^2 \leq \exp\left(-t\frac{\alpha}{\beta}\right) \normtwo{\bx^{(1)} - \bx^*}^2.
$$
For the function value, if the constant stepsize is  $\eta = \frac{2}{\alpha + \beta}$, then
$$
\begin{aligned}
\normtwo{\bx^{(t+1)} - \bx^*}^2 &\leq\exp\left(-\frac{4t}{\alpha + 1}\right) \normtwo{\bx^{(1)} - \bx^*}^2;\\
f(\bx^{(t+1)}) - f(\bx^*)& \leq \frac{\beta}{2} \exp\left(-\frac{4t}{\alpha + 1}\right) \normtwo{\bx^{(1)} - \bx^*}^2.
\end{aligned}
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:gd_sc_ss}]
\textbf{When $\eta=\frac{1}{\beta}$.}
Using the update rule,
$$
\small
\begin{aligned}
&\normtwo{\bx^{(t+1)} - \bx^*}^2 
= \normtwo{\bx^{(t)} - \frac{1}{\beta} \nabla f(\bx^{(t)}) - \bx^*}^2\\
&= \normtwo{\bx^{(t)} - \bx^*}^2 - \frac{2}{\beta} \nabla f(\bx^{(t)})^\top (\bx^{(t)} - \bx^*) + \frac{1}{\beta^2} \normtwobig{\nabla f(\bx^{(t)})}^2 \\
&\stackrel{\dag}{\leq} \left(1 - \frac{\alpha}{\beta}\right) \normtwo{\bx^{(t)} - \bx^*}^2 
\leq \left(1 - \frac{\alpha}{\beta}\right)^t \normtwo{\bx^{(1)} - \bx^*}^2 
\leq \exp\left(-t\frac{\alpha}{\beta}\right) \normtwo{\bx^{(1)} - \bx^*}^2,
\end{aligned}
$$
where inequality ($\dag$) follows from  the descent lemma (\ref{descent_lem_gdscss} in Lemma~\ref{lemma:gdupd_sm_sconv}), and we have used the fact that $1 - r \leq \exp(-r)$ for all $r \in \real$.

\paragraph{When $\eta = \frac{2}{\alpha + \beta}$.}
By the smoothness and $\nabla f(\bx^*) = \bzero$, we have 
$
f(\bx^{(t)}) - f(\bx^*) \leq \frac{\beta}{2} \normtwo{\bx^{(t)} - \bx^*}^2
$
(Theorem~\ref{theorem:charac_smoo}).
Thus, by the characterization theorem of SC and SS (Theorem~\ref{theorem:charac_smoo_n_stronconv}),
$$
\small
\begin{aligned}
\normtwo{\bx^{(t+1)} - \bx^*}^2 &= \normtwo{\bx^{(t)} - \eta \nabla f(\bx^{(t)}) - \bx^*}^2 \\
&= \normtwobig{\bx^{(t)} -\bx^*}^2 - 2\eta \innerproduct{\nabla f(\bx^{(t)}), (\bx^{(t)} - \bx^*)} + \eta^2 \normtwobig{\nabla f(\bx^{(t)})}^2 \\
&\leq \left(1 - 2\frac{\eta \alpha \beta}{\beta + \alpha}\right) \normtwo{\bx^{(t)} - \bx^*}^2 + \left(\eta^2 - 2\frac{\eta}{\beta + \alpha}\right) \normtwobig{\nabla f(\bx^{(t)})}^2 \\
&= \left(\frac{\alpha - 1}{\alpha + 1}\right)^2 \normtwo{\bx^{(t)} - \bx^*}^2 
\leq \exp\left(-\frac{4t}{\alpha + 1}\right) \normtwo{\bx^{(1)} - \bx^*}^2.
\end{aligned}
$$
This completes the proof.
\end{proof}

\index{Condition number}
\paragrapharrow{Condition number.}
We observe that the convergence rate of the gradient descent algorithm for an $\alpha$-SC and $\beta$-SS function is of the form 
$$
\normtwo{\bx^{(t+1)} - \bx^*}^2 \leq \exp\left(-t\frac{\alpha}{\beta}\right) \normtwo{\bx^{(1)} - \bx^*}^2.
$$
The number $\kappa \triangleq \frac{\beta}{\alpha}$ is known as the \textit{condition number} 
of the optimization problem.
The condition number of the objective function significantly affects the convergence rate of algorithms.
Indeed, if $\kappa = \frac{\beta}{\alpha}$ is small, then $\exp\left(-\frac{\alpha}{\beta}\right) = \exp\left(-\frac{1}{\kappa}\right)$ will be small, ensuring rapid convergence. However, if $\kappa \gg 1$, then $\exp\left(-\frac{1}{\kappa}\right) \approx 1$, potentially leading to slow convergence.




The concept of the condition number is central to numerical optimization. Below, we provide an informal definition of this concept. In subsequent sections, we will see how the condition number appears repeatedly in the context of the convergence of various optimization algorithms for both convex and non-convex problems. 
The exact numerical form of the condition number (for instance, here it is $\beta/\alpha$) will vary depending on the application~\footnote{In a linear system, the condition number can be defined as the ratio of the largest eigenvalue to the smallest eigenvalue of the underlying matrix; see, for example, \citet{lu2021numerical} for more details.}. 
However, in general, all these definitions of condition number will satisfy the following property.


\begin{definition}[Condition Number---Informal]
The condition number of a function $f: \sS \rightarrow \real$ is a scalar $\kappa \in \real$ that bounds how much the function value can change relative to a perturbation of the input.
\end{definition}

Functions with a small condition number are stable, meaning changes to their input do not significantly affect the function output values. Conversely, functions with a large condition number may exhibit abrupt changes in output values even with slight changes to the input. To better understand this concept, consider a differentiable function $f$ that is $\alpha$-SC and $\beta$-SS. 
Suppose $\bx^*$  is a stationary point of $f$, i.e., $\nabla f(\bx^*) = \bzero$. 
For a general function, such a point could be a local optimum or a saddle point. However, since $f$ is strongly convex, $\bx^*$ represents the unique global minimum of $f$. Therefore, for any other point  $\by\neq \bx^*$,
$$
\frac{\alpha}{2} \normtwo{\bx^* - \by}^2 \leq f(\by) - f(\bx^*) \leq \frac{\beta}{2} \normtwo{\bx^* - \by}^2.
$$
Dividing throughout by $\frac{\alpha}{2} \normtwo{\bx^* - \by}^{2}$ yields
$$
\frac{f(\by) - f(\bx^*)}{\frac{\alpha}{2} \normtwo{\bx^* - \by}^2} \in [1, \frac{\beta}{\alpha}] \triangleq [1, \kappa].
$$
Thus, upon perturbing the input from the global minimum $\bx^*$ to a point $\by$ at a distance $\normtwo{\bx^* - \by} = \epsilon$  away, the function value changes moderately---it increases by at least $\frac{\alpha \epsilon^2}{2}$ but no more than $\kappa \cdot \frac{\alpha \epsilon^2}{2}$. Such predictable behavior in response to perturbations makes it easier for optimization algorithms to achieve fast convergence.



\paragrapharrow{Change of variables and choice of $\bQ$-norm for greedy descent.}
In the next section, we will establish convergence results for non-Euclidean gradient descent methods. For now, let's focus on the $\bQ$-norm.
The choice of  $\bQ$ used to define the greedy descent direction can dramatically affect the convergence rate.
In Section~\ref{section:als-gradie-descent-taylor}, we showed that the greedy descent method with  $\bQ$-norm is the same as the gradient method applied to the problem after the change of variables $\widetildebx^\toptzero = \bQ^{1/2} \bx^\toptzero$. We know that the gradient method works well when the condition numbers $\frac{\beta}{\alpha}$ are moderate, and works poorly when the condition numbers are large. 
It follows that if the condition number is moderate after the change of variables $\widetildebx^\toptzero = \bQ^{1/2} \bx^\toptzero$, the greedy descent method will perform well according to Theorem~\ref{theorem:gd_sc_ss}.

This observation suggests a strategy for choosing $\bQ$: It should be selected so that the condition number of the underlying function, transformed by $\bQ^{-1/2}$, is well-conditioned. 
For example, if an approximation $\widetildebB$ of the Hessian at the optimal point $\nabla^2 f(\bx^*)$ were known, a very good choice for  $\bQ$ would be $\bQ = \widetildebB$, because  the Hessian of $f$ at the optimum would then satisfy
\begin{equation}\label{equation:gree_cond_opt}
\widetildebB^{-1/2} \nabla^2 f(\bx^*) \widetildebB^{-1/2} \approx \bI,
\end{equation}
indicating a likely low condition number.



\begin{algorithm}[H] 
\caption{Non-Euclidean Gradient  Descent Method}
\label{alg:non_euclidean_gd_gen}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t$;
\State Pick $\bd_{\text{ngd}}^\toptzero \in \Lambda_{\nabla f(\bx^\toptzero)}$ and let $\bd_{\text{ugd}}^\toptzero \triangleq \norm{\nabla f(\bx^\toptzero)}_{*} \bd_{\text{ngd}}^\toptzero$;
\State Update $\bx^\toptone \leftarrow \bx^\toptzero - \eta_t\bd_{\text{ugd}}^\toptzero$;
\EndFor
\State {\bfseries Return:}   $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
%\State (Output Option 2) Output  $\bx_{\text{avg}}\leftarrow \frac{1}{T}(\sum_{t=1}^{t}\bx^\toptzero)$ or $\sum_{t=1}^{T} \frac{2t}{T(T+1)} \bx^\toptzero$;
%\State (Output Option 3) Output  $\bx_{\text{best}}\leftarrow \argmin_{t\in\{1,2,\ldots,T\}} f(\bx^\toptzero)$;
\end{algorithmic} 
\end{algorithm}

\section{Non-Euclidean Gradient Descent}\label{section:noneucli_gd}
For the unconstrained problem (P1) in \eqref{equation:p1p2}, we consider  the \textit{non-Euclidean gradient descent} method in Algorithm~\ref{alg:non_euclidean_gd_gen}, which is derived  from greedy searching using different norms (Section~\ref{section:als-gradie-descent-taylor}).
For any gradient $\nabla f(\bx^\toptzero)$ at $\bx^\toptzero$, Equations~\eqref{equation:norma_greedes} and \eqref{equation:unnorma_greedes} provide normalized and unnormalized greedy search directions, respectively, where we use the latter for illustration since the unnormalized greedy search directions in the $\ell_2$ Euclidean case is equivalent to the negative gradient direction, establishing guidance for the selection of  stepsizes; see Theorem~\ref{theorem:noneu_conv_ss}.



Letting $ \norm{\cdot} $ be any norm on $ \real^n $, by the conjugate subgradient theorem (Theorem~\ref{theorem:conju_subgra}), the set of primal counterparts (Definition~\ref{definition:set_primal}) can be equivalently stated as 
$$
\Lambda_{\ba} = \partial h(\ba), \text{ where } h(\cdot) \triangleq \norm{\cdot}_*,  \text{ for any }\ba\in\real^n,
$$
where  $\norm{\cdot}_*$ denotes the dual norm.
Using these definitions, the update rule for the non-Euclidean gradient descent method at the $t$-th iteration can be stated as
\begin{enumerate}
\item Pick $\bd_{\text{ngd}}^\toptzero \in \Lambda_{\nabla f(\bx^\toptzero)}$ and let $\bd_{\text{ugd}}^\toptzero \triangleq \norm{\nabla f(\bx^\toptzero)}_{*} \bd_{\text{ngd}}^\toptzero$;
\item Update $\bx^\toptone \leftarrow  \bx^\toptzero - \eta_t\bd_{\text{ugd}}^\toptzero$;
\end{enumerate}


Note that smoothness was defined under the $\ell_2$ norm in Definition~\ref{definition:scss_func}. In this section, letting $ \norm{\cdot} $ be any norm on $ \real^n $, we assume that $f$ is $\beta$-smooth w.r.t. the underlying norm:
\begin{subequations}
\begin{equation}\label{equation:smooth_noneul}
f(\by)-f(\bx)-\nabla f(\bx)^\top (\by-\bx)
\leq \frac{\beta}{2} \norm{\bx-\by}^2, \text{ for any }\bx,\by\in\real^n.
\end{equation}
Similar to the SS Property-O (Theorem~\ref{theorem:equi_gradsch_smoo}), this also shows 
\begin{equation}\label{equation:smooth_noneu2}
\norm{\nabla f(\bx) - \nabla f(\by)}_* \leq \beta  \norm{\bx-\by}.
\end{equation}
\end{subequations}
We are now ready to present the convergence results of the non-Euclidean gradient method.
\begin{theoremHigh}[Non-Euclidean Method for SS]\label{theorem:noneu_conv_ss}
Let $f:\real^n\rightarrow \real$ be a $\beta$-smooth function w.r.t. to the norm $\norm{\cdot}$. Let $\{\bx^\toptzero\}_{t > 0}$ be the sequence generated by the non-Euclidean gradient method (Algorithm~\ref{alg:non_euclidean_gd_gen}) for solving  problem (P1)
with  a constant stepsize corresponding to $\eta_t\triangleq\eta \in\left(0, \frac{1}{\beta}\right)$. Then,
\begin{enumerate}[(i)]
\item It satisfies that $  f(\bx^\toptzero) - f(\bx^\toptone) \geq \eta(1-\frac{\beta\eta}{2})\norm{\nabla f(\bx^\toptzero)}_*^2$.
\item The sequence $\{ f(\bx^\toptzero) \}_{t > 0}$ is nonincreasing; in addition, $f(\bx^\toptone) < f(\bx^\toptzero)$ if and only if $\nabla f(\bx^\toptzero) \neq \bzero$.
\item If the sequence $\{ f(\bx^\toptzero) \}_{t > 0}$ is bounded below, then $\nabla f(\bx^\toptzero) \to \bzero$ as $t \to \infty$;
\item If the optimal value of (P1) is finite and let $\bx^*$ be any optimal point. Then, for any $T>0$,
\begin{equation}
\min_{t=1,2,\ldots,T} \norm{\nabla f(\bx^\toptzero)}_* \leq 
\frac{\sqrt{f(\bx^{(1)}) - f(\bx^*)}}{\sqrt{\eta(1-\frac{\beta\eta}{2})T}}.
\end{equation}
\item All limit points of the sequence $\{ \bx^\toptzero \}_{t > 0}$ are stationary points of problem (P1).
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:noneu_conv_ss}]
\textbf{(i, ii).}
By the smoothness \eqref{equation:smooth_noneul} w.r.t. the norm $\norm{\cdot}$, the definition of the stepsize $\eta\in\left(0, \frac{1}{\beta}\right)$, and the update rule of Algorithm~\ref{alg:non_euclidean_gd_gen}, we have
\begin{equation}\label{equation:noneu_conv_ss1}
\small
\begin{aligned}
f(\bx^\toptone) &\leq f(\bx^\toptzero) + \innerproduct{\nabla f(\bx^\toptzero), \bx^\toptone - \bx^\toptzero} 
+ \frac{\beta}{2} \norm{\bx^\toptone - \bx^\toptzero}^2 \\
&= f(\bx^\toptzero) - \eta{\norm{\nabla f(\bx^\toptzero)}_*} \innerproduct{\nabla f(\bx^\toptzero), \bd_{\text{ngd}}^\toptzero} 
+\frac{\beta\eta^2}{2} \norm{\nabla f(\bx^\toptzero)}_*^2 \\
&\stackrel{\dag}{=} f(\bx^\toptzero) - \eta{\norm{\nabla f(\bx^\toptzero)}_*^2} +\frac{\beta\eta^2}{2} \norm{\nabla f(\bx^\toptzero)}_*^2 \\
&= f(\bx^\toptzero) - \eta(1-\frac{\beta\eta}{2})\norm{\nabla f(\bx^\toptzero)}_*^2,
\end{aligned}
\end{equation}
where the equality ($\dag$) follows from the definition of the dual norm. This inequality readily implies that $f(\bx^\toptzero) \geq f(\bx^\toptone)$ since $\eta\in\left(0, \frac{1}{\beta}\right)$ and that if $\nabla f(\bx^\toptzero) \neq \bzero$, then $f(\bx^\toptone) < f(\bx^\toptzero)$.

\paragraph{(iii).} Since the sequence $\{ f(\bx^\toptzero) \}_{t > 0}$ is nonincreasing and bounded below, it converges. Thus, in particular $f(\bx^\toptzero) - f(\bx^\toptone) \to 0$ as $t \to \infty$, which, combined with \eqref{equation:noneu_conv_ss1}, implies that $\nabla f(\bx^\toptzero) \to \bzero$ as $t \to \infty$.


\paragraph{(iv).} Let $B\triangleq\eta(1-\frac{\beta\eta}{2})$. By (i), for any $t > 0$,
$ f(\bx^\toptzero) - f(\bx^\toptone) \geq B \norm{\nabla f(\bx^\toptzero)}_*^2. $
Performing telescopic sum over $t = 1,2, \ldots, T$, we obtain
$$ 
f(\bx^{(1)}) - f(\bx^{(T)}) \geq B \sum_{t=1}^T \norm{\nabla f(\bx^\toptzero)}_*^2 \geq T B \min_{t=1,2\ldots,T} \norm{\nabla f(\bx^\toptzero)}_*^2. 
$$
Since $f(\bx^{(T)}) \geq f(\bx^*)$, the result follows.

\paragraph{(v).} Let $\widetildebx$ be a limit point of $\{ \bx^\toptzero \}_{t > 0}$. Then there exists a subsequence $\{ \bx^{(t_j)} \}_{j \geq 0}$ converging to $\widetildebx$. By the triangle inequality and the smoothness in \eqref{equation:smooth_noneu2},  for any $j \geq 0$
$$
\norm{\nabla f(\widetildebx)}_* \leq \norm{\nabla f(\bx^{(t_j)}) - \nabla f(\widetildebx)}_* + \norm{\nabla f(\bx^{(t_j)}) }_* \leq \beta \norm{\bx^{(t_j)} - \widetildebx} + \norm{\nabla f(\bx^{(t_j)})}_*. 
$$
Since the right-hand side of the above inequality goes to $0$ as $j \to \infty$, it follows that $\nabla f(\widetildebx) = \bzero$.
\end{proof}

\begin{remark}[Smoothness Parameter]
Note that to prove the result, we require the function to be smooth. In many cases, the smoothness parameter $\beta$ might not be known in advance. In such scenarios, similar results can be obtained using backtracking (Algorithm~\ref{alg:gd_line_search}) or exact line search as long as these methods satisfy:
$$
f(\bx^\toptone) \leq f(\bx^\toptzero) + \innerproduct{\nabla f(\bx^\toptzero), \bx^\toptone - \bx^\toptzero} + \frac{1}{2\eta_t} \norm{\bx^\toptone - \bx^\toptzero}^2
$$ 
for each iteration.
This discussion applies to many results in this section; thus, we will not repeat the details for them subsequently.
\end{remark}

Similar to Theorem~\ref{theorem:pgd_smooth}, we can establish an $\mathcalO(1/T)$ rate of convergence for convex functions using non-Euclidean gradient descent methods.
To demonstrate this, we require an additional boundedness assumption:
\begin{itemize}
\item For the initial point $\bx^{(1)}$ and any optimal point $\bx^*$, there exists $R > 0$ such that
$
\max_{\bx, \bx^*} \{ \norm{\bx^* - \bx} \mid f(\bx) \leq f(\bx^{(1)})  \} \leq R
$.
\end{itemize}
The proof of the convergence rate is based on the following lemma.
\begin{lemma}\label{lemma:noneu_cvx_sequence}
Let $\{a_t\}_{t > 0}$ be a sequence of nonnegative real numbers satisfying for any $t > 0$:
$$ a_t - a_{t+1} \geq \frac{1}{\gamma} a_t^2 $$
for some $\gamma > 0$. Then, for any $T \geq 1$,
\begin{equation}
a_T \leq \frac{\gamma}{T-1}.
\end{equation}
\end{lemma}
\begin{proof}
Let $t$ be a positive integer. If $a_t = 0$, then the result holds trivially. We then assume that $a_t > 0$. By the monotonicity of $\{a_t\}_{t > 0}$, we have that $a_1, a_2, \ldots, a_T > 0$. For any $t = 1, 2, \ldots, T$,
$$
\frac{1}{a_{t+1}} - \frac{1}{a_{t}} = \frac{a_{t} - a_{t+1}}{a_{t} a_{t+1}} \geq \frac{1}{\gamma} \frac{a_{t}^2}{a_{t} a_{t+1}} = \frac{1}{\gamma} \frac{a_{t}}{a_{t+1}} \geq \frac{1}{\gamma}.
$$
where the last inequality follows from the monotonicity of the sequence. Telescoping the sum over $t = 1, 2, \ldots, T$ yields that 
$
\frac{1}{a_T} \geq \frac{1}{a_1} + \frac{T-1}{\gamma} \geq \frac{T-1}{\gamma}
$.
\end{proof}

\begin{theoremHigh}[Non-Euclidean Method for Convex and SS: $\mathcalO(1/T)$]\label{theorem:noneu_conv_sscvx}
Let $f:\real^n\rightarrow \real$ be a $\beta$-smooth and \textbf{convex} function w.r.t. to the norm $\norm{\cdot}$. Let $\{\bx^\toptzero\}_{t > 0}$ be the sequence generated by the non-Euclidean gradient method (Algorithm~\ref{alg:non_euclidean_gd_gen}) for solving  problem (P1)
with  a constant stepsize corresponding to $\eta_t\triangleq\eta \in\left(0, \frac{1}{\beta}\right)$~\footnote{Similar results can be obtained using backtracking or exact line search as long as it satisfies $f(\bx^\toptone) \leq f(\bx^\toptzero) + \innerproduct{\nabla f(\bx^\toptzero), \bx^\toptone - \bx^\toptzero} + \frac{1}{2\eta_t} \norm{\bx^\toptone - \bx^\toptzero}^2$ for each iteration.}.
Suppose further that for the initial point $\bx^{(1)}$ and any optimal point $\bx^*$, there exists $R > 0$ such that
$
\max_{\bx, \bx^*} \{ \norm{\bx^* - \bx} \mid f(\bx) \leq f(\bx^{(1)}) \} \leq R
$.
Then,
\begin{itemize}
\item It follows that 
$
f(\bx^\toptzero) - f(\bx^\toptone) \geq \frac{1}{C} \big(f(\bx^\toptzero) - f(\bx^*)\big)^2, 
$
where $C\triangleq\frac{R^2}{\eta(1-\frac{\beta\eta}{2})}$ is a positive constant. 
\item For any $T \geq 1$,
$
f(\bx^{(T)}) - f(\bx^*) \leq \frac{C}{T-1}
$.
\end{itemize}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:noneu_conv_sscvx}]
By Theorem~\ref{theorem:noneu_conv_ss}(ii), $\{f(\bx^\toptzero)\}_{t > 0}$ is nonincreasing, and in particular for any $t > 0$ it holds that $f(\bx^\toptzero) \leq f(\bx^{(1)})$. Therefore, for any $\bx^* $ and $t > 0$,
$
\norm{\bx^\toptzero - \bx^*} \leq R
$. 
By Theorem~\ref{theorem:noneu_conv_ss}(i), we have 
\begin{equation}
f(\bx^\toptzero) - f(\bx^\toptone) \geq \eta\big(1-\frac{\beta\eta}{2}\big) \norm{\nabla f(\bx^\toptzero)}_*^2.
\end{equation}
This establishes the first claim.
By the convexity of $f$ (Theorem~\ref{theorem:conv_gradient_ineq}) and the \holders inequality (Theorem~\ref{theorem:holder-inequality}), for any optimal point $\bx^*$,
\begin{equation}
\small
\begin{aligned}
f(\bx^\toptzero) - f(\bx^*) 
\leq \innerproduct{\nabla f(\bx^\toptzero), \bx^\toptzero - \bx^*} 
\leq \norm{\nabla f(\bx^\toptzero)}_* \norm{\bx^\toptzero - \bx^*}
\leq R \norm{\nabla f(\bx^\toptzero)}_*. 
\end{aligned}
\end{equation}
Combining the above two inequalities yields that 
$$ 
f(\bx^\toptzero) - f(\bx^\toptone) \geq \eta(1-\frac{\beta\eta}{2}) \norm{\nabla f(\bx^\toptzero)}_*^2 \geq \frac{\eta(1-\frac{\beta\eta}{2})}{R^2} \big(f(\bx^\toptzero) - f(\bx^*)\big)^2. 
$$
Applying Lemma~\ref{lemma:noneu_cvx_sequence} proves the second claim.
\end{proof} 








































\begin{algorithm}[h] 
\caption{Projected (Sub)Gradient Descent Method}
\label{alg:pgd_gen}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$ and a set $\sS$; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t$;
\State $\by^{(t+1)} \leftarrow \bx^{(t)} - \eta_t \nabla f(\bx^{(t)})$ or $\by^{(t+1)} \leftarrow \bx^{(t)} - \eta_t f^\prime(\bx^{(t)})$, where $ f^\prime(\bx^{(t)})\in \partial f(\bx^{(t)})$;
\State $\bx^{(t+1)} \in \mathcalP_{\sS}(\by^{(t+1)})$;
\EndFor
\State (Output Option 1) Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\State (Output Option 2) Output  $\bx_{\text{avg}}\leftarrow \frac{1}{T}(\sum_{t=1}^{t}\bx^{(t)})$ or $\sum_{t=1}^{T} \frac{2t}{T(T+1)} \bx^{(t)}$;
\State (Output Option 3) Output  $\bx_{\text{best}}\leftarrow \argmin_{t\in\{1,2,\ldots,T\}} f(\bx^{(t)})$;
\end{algorithmic} 
\end{algorithm}

%\begin{table}[t]
%\centering
%\label{tab:bounds_on_error}
%\begin{tabular}{|c|c|c|}
%\hline
%& Convex & Strongly convex \\
%\hline
%Lipschitz & $\epsilon \leq \mathcalO(1/\sqrt{T})$ & $\epsilon \leq \mathcalO(1/T)$ \\
%\hline
%Smooth & $\epsilon \leq \mathcalO(1/T)$ & $\epsilon \leq e^{-\Omega(T)}$ \\
%\hline
%\end{tabular}
%\caption{Bounds on error $\epsilon$ as a function of number of steps taken $T$ for gradient descent applied to various classes of functions.}
%\end{table}

\section{Projected Gradient Descent}\label{section:pgd}
Standard gradient descent is typically used to solve unconstrained optimization problems. Given a set $\sS$, the \textit{projected gradient descent (PGD)} method  (Algorithm~\ref{alg:pgd_gen}) addresses constrained optimization problems of the form:
\begin{equation}\label{equation:p2_pgd}
\text{(P2)}: \qquad \min_{\bx\in\real^n } f(\bx) \quad \text{s.t.}\quad \bx\in\sS.
\end{equation}
Sections~\ref{section:constrain_opt} and \ref{section:constr_convset} discuss optimality conditions for this type of constrained optimization problem.
The PGD method represents the simplest approach to tackling the constrained optimization problem (P2).
As previously mentioned, the PGD method forms the cornerstone of first-order optimization algorithms, serving as a fundamental technique; refer to Figure~\ref{fig:first_order_opt_map}.
Numerous algorithms, such as the proximal gradient method, are derived from this basic algorithm.
This algorithm can be viewed as a fixed-point iteration for solving the stationarity condition
$$
\bx^* =\mathcalP_{\sS}(\bx^* - \eta\nabla f(\bx^*))
$$
if the set $\sS$ is closed convex (Theorem~\ref{theorem:stat_point_uncons_convset_proj}, assuming the stepsize is constant).


To illustrate further, in PGD, the update rule from the $t$-th to the $(t+1)$-th iteration can be equivalently expressed as:
\begin{tcolorbox}[colback=white,colframe=black]
\begin{minipage}{1\textwidth}
\begin{equation}\label{equation:pgd_decom_raw}
\small
\textbf{(PGD)}:\quad 
\begin{aligned}
\bx^\toptone 
&\leftarrow \mathcalP_{\sS}(\bx^{(t)} - \eta_t\nabla f(\bx^{(t)})) \\
&= \mathop{\argmin}_{\bx\in\sS} \normtwo{\bx - \left(\bx^{(t)} - \eta_t \nabla f(\bx^{(t)})\right)}^2\\
&=\mathop{\argmin}_{\bx\in\sS} 
\left\{
\frac{1}{2\eta_t} \normtwo{\bx-\bx^\toptzero}^2 + f(\bx^\toptzero)+ \innerproduct{\nabla f(\bx^\toptzero), \bx-\bx^\toptzero} 
\right\},
\end{aligned}
\end{equation}
\end{minipage}
\end{tcolorbox}
\noindent
where we add a constant $f(\bx^\toptzero)$ term in the third form since the optimization is over $\bx$.
Therefore, the update rule in PGD can be interpreted  as a minimization of the sum of a linearization of the smooth component  around the current iterate plus a quadratic term. The optimization of the linearization ensures that we can make a significant progress, while the quadratic term acts as a regularization mechanism, ensuring the update remains within a neighborhood of the current iterate.
Understanding this decomposition and the intuition behind the PGD updates is crucial for developing other methods, such as the proximal gradient method, conditional gradient method, and mirror descent method.



We start  by proving an $\mathcalO(1/\sqrt{T})$ rate of convergence for convex and Lipschitz functions (Definition~\ref{definition:lipschi_funs}) with a constant stepsize. 
This result depends on an upper bound on the distance between the initial guess $\bx^{(1)}$ and any optimal point $\bx^*$.
Additionally, we assume the total number of iterations $T$ is known in advance.
Note that Theorem~\ref{theorem:pgd_lipschitz_dyna} provides an alternative convergence result  when dynamic stepsizes are employed.
\begin{theoremHigh}[PGD for Convex and Lipschitz Functions: $\mathcalO(1/\sqrt{T})$]\label{theorem:pgd_lipschitz}\footnote{Theorem~\ref{theorem:pgd_lipschitz_dyna} provides an alternative convergence result with dynamic stepsizes.}
Let $f:\sS\rightarrow \real$ be a proper convex,  differentiable, and $L$-Lipschitz function defined over the closed and convex domain $\sS\subseteq\real^n$. 
Let $\bx^{(1)}, \bx^{(2)}, \ldots, \bx^{(T)}$ be the sequence of $T$ steps generated by the projected gradient descent method (Algorithm~\ref{alg:pgd_gen}) for solving  problem (P2)
with  a constant stepsize corresponding to  $\eta = \frac{R}{L\sqrt{T}}$, where $R$ is the upper bound on the distance $\normtwo{\bx^{(1)}-\bx^{*}}$ from the initial point $\bx^{(1)}$ to an optimal point $\bx^{*} \in \arg\min_{\bx\in\sS} f(\bx)$.~\footnote{Under the assumption that  $ \sS $ is \textbf{compact}.}
%Let $\bx^{(1)},\bx^{(2)} \ldots, \bx^{(T)}$ be the sequence of iterates computed by $T$ steps of projected gradient descent.
Then, for any $T>0$,
$$
\small
\begin{aligned}
f\left(\frac{1}{T}\sum_{t=1}^{T}\bx^{(t)}\right) - f\left(\bx^{*}\right) \leqslant \frac{RL}{\sqrt{T}}.
\end{aligned}
$$
This implies that for any integer $T$ satisfying $T\geq \frac{R^2L^2}{\epsilon^2}$, it follows that 
$$
\small
\begin{aligned}
f\left(\frac{1}{T}\sum_{t=1}^{T}\bx^{(t)}\right) - f\left(\bx^{*}\right) \leq \epsilon.~\footnote{If $R$ is not known, similar result can be obtained that $f\left(\frac{1}{T}\sum_{t=1}^{T}\bx^{(t)}\right) - f\left(\bx^{*}\right) \leq \epsilon$ for $T=\mathcalO(\frac{1}{\epsilon^2})$ and $\eta = \frac{1}{\sqrt{T}}$.}
\end{aligned}
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:pgd_lipschitz}]
The proof begins by  bounding the difference in function values
$
f(\bx^{(t)}) - f(\bx^*)
$ (called the \textit{sub-optimality} at the $t$-th iterate):
\begin{equation}\label{equation:pgd_lipschitz1}
\small
\begin{aligned}
f(\bx^{(t)}) - f(\bx^*) &\stackrel{\dag}{\leq} \innerproduct{\nabla f(\bx^{(t)}),  (\bx^{(t)} - \bx^*)}
\stackrel{*}{=} \frac{1}{\eta} \innerproduct{(\bx^{(t)} - \by^{(t+1)}), (\bx^{(t)} - \bx^*)}\\
&\stackrel{\ddag}{=} \frac{1}{2\eta} \left(\normtwo{\bx^{(t)} - \by^{(t+1)}}^2 + \normtwo{\bx^{(t)} - \bx^*}^2  - \normtwo{\by^{(t+1)} - \bx^*}^2 \right) \\
&\stackrel{*}{=} \frac{1}{2\eta} \left( \normtwo{\bx^{(t)} - \bx^*}^2 - \normtwo{\by^{(t+1)} - \bx^*}^2 \right) + \frac{\eta}{2} \normtwobig{\nabla f(\bx^{(t)})}^2\\
&\stackrel{+}{\leq} \frac{1}{2\eta} \left( \normtwo{\bx^{(t)} - \bx^*}^2 - \normtwo{\bx^{(t+1)} - \bx^*}^2 \right) + \frac{\eta L^2}{2},
\end{aligned}
\end{equation}
where the inequality ($\dag$) follows from convexity (Theorem~\ref{theorem:conv_gradient_ineq}),  the equalities ($*$) follow from the update rule of PGD, 
the equality ($\ddag$) follows from Theorem~\ref{theorem:funda_opt}, and  the inequality ($+$) follows from the Lipschitz condition (Theorem~\ref{theorem:lipsc_equiv}) and Projection Property-II (Lemma~\ref{lemma:proj_prop2}).

Now, sum these differences from $t=1$ to $t=T$:
$$
\small
\begin{aligned}
\sum_{t=1}^{T} \left(f(\bx^{(t)}) - f(\bx^*)\right) 
&\leq \frac{1}{2\eta} \sum_{t=1}^{T} \left( \normtwo{\bx^{(t)} - \bx^*}^2 - \normtwo{\bx^{(t+1)} - \bx^*}^2 \right) + \frac{\eta L^2 T}{2}\\
%&= \frac{1}{2\eta} \left( \normtwo{\bx^{(1)} - \bx^*}^2 - \normtwo{\bx^{(T)} - \bx^*}^2 \right) + \frac{\eta L^2 T}{2}\\
&\leq \frac{1}{2\eta} \normtwo{\bx^{(1)} - \bx^*}^2 + \frac{\eta L^2 T}{2}
\leq \frac{R^2}{2\eta} + \frac{\eta L^2 T}{2}.
\end{aligned}
$$~\footnote{If $R$ is not known, we can set $\bx^{(1)}=\bzero$ and $\eta=\frac{1}{\sqrt{T}}$, obtaining a similar complexity.}
Finally, by convexity and $\eta = \frac{R}{L\sqrt{T}} $,
$$
\small
\begin{aligned}
f\left(\frac{1}{T} \sum_{t=1}^T \bx^{(t)}\right) - f(\bx^*) \leq \left(\frac{1}{T} \sum_{t=1}^T f(\bx^{(t)})\right) - f(\bx^*) 
\leq \frac{R^2}{2\eta T} + \frac{\eta L^2}{2}
= \frac{R L}{\sqrt{T}}.
\end{aligned}
$$
This completes the proof.
\end{proof}

Let $e_t \triangleq f(\bx^{(t)}) - f(\bx^*)$. 
The equality (+) in \eqref{equation:pgd_lipschitz1} also tells us that the sub-optimality $e_t$ is small  if the consecutive iterates $\bx^\toptzero$ and $\bx^\toptone$ are close to each other, since $\frac{\eta L^2}{2}$ is small due to $\eta = \mathcalO(\frac{1}{\sqrt{T}})$.
This observation is quite useful since it tells us that once PGD stops making a lot of progress, it actually converges to the optimum.



\begin{figure}[h]
\centering       
\vspace{-0.25cm}                 
\subfigtopskip=2pt               
\subfigbottomskip=-2pt         
%\subfigcapskip=-10pt      
\subfigure[$f(a) + f(a+1) + \cdots + f(b) \leq \int_{a-1}^b f(t) \, dt$. ]{\label{fig:pgd_sumlem1}
\includegraphics[width=0.48\linewidth]{imgs/pgd_sumlem1.pdf}}
\subfigure[$\int_a^{b+1} f(t) \, dt \leq f(a) + f(a+1) + \cdots + f(b)$.]{\label{fig:pgd_sumlem2}
\includegraphics[width=0.48\linewidth]{imgs/pgd_sumlem2.pdf}}
\caption{An example illustrating Lemma~\ref{lemma:pgd_lem1}, where we set $a\triangleq 1$ and $b\triangleq 8$ in the lemma.}
\label{fig:pgd_sumlem}
\end{figure}


Theorem~\ref{theorem:pgd_lipschitz} provides the convergence result for convex and Lipschitz function using predefined stepsizes for each iteration.
Alternative, we provide convergence results using dynamic stepsizes. To see this, we need the following lemmas.
\begin{lemma}\label{lemma:pgd_lem1}
Let $ f: [a-1, b+1] \rightarrow \real $ be a continuous nonincreasing function over $[a-1, b+1]$, where $a$ and $b$ are integer numbers satisfying $a \leq b$. Then,
$$
\int_a^{b+1} f(t) \, dt \leq f(a) + f(a+1) + \cdots + f(b) \leq \int_{a-1}^b f(t) \, dt.
$$
See Figure~\ref{fig:pgd_sumlem} for an illustrating example.
\end{lemma}

\begin{lemma}\label{lemma:pgd_lem2}
Let $ \Phi \in \real $. Then,
\begin{enumerate}[(i)]
\item \label{pgd_lem2_v1} For any $ T \geq 5 $,
$
\frac{\Phi + \sum_{t=1}^T \frac{1}{t+1}}{\sum_{t=1}^T \frac{1}{\sqrt{t+1}}} \leq \frac{\Phi  + \ln(T+1)}{\sqrt{T+1}}.
$

\item \label{pgd_lem2_v2} For any $ T \geq 2 $,
$
\frac{\Phi + \sum_{t=\lceil T/2 \rceil}^T \frac{1}{t+1}}{\sum_{t=\lceil T/2 \rceil}^T \frac{1}{\sqrt{t+1}}} \leq \frac{4(\Phi + \ln(3))}{\sqrt{T+2}}.
$
\end{enumerate}
\end{lemma}
\begin{proof}
\textbf{(i).} Using Lemma~\ref{lemma:pgd_lem1}, we obtain the following inequalities:
$$
\begin{aligned}
\sum_{t=1}^T \frac{1}{t+1} &\leq  \int_0^T \frac{1}{x+1} \, dx =  \ln(T+1);\\
\sum_{t=1}^T \frac{1}{\sqrt{t+1}} &\geq \int_1^{T+1} \frac{1}{\sqrt{x+1}} \, dx = 2\sqrt{T+2} - 2\sqrt{2} \geq \sqrt{T+1},
\end{aligned}
$$
where the last inequality holds for all $ T \geq 5 $. This yields the desired result in (i).
\paragraph{(ii).} Using Lemma~\ref{lemma:pgd_lem1}, we can also obtain the following inequalities for any $ T \geq 2 $:
$$
\begin{aligned}
\sum_{t=\lceil T/2 \rceil}^T \frac{1}{t+1} &\leq \int_{\lceil T/2 \rceil - 1}^T \frac{1}{x+1}\, dx  
= \ln\left(\frac{T+1}{\lceil  T/2 \rceil}\right) 
%&\leq \ln\left(\frac{T+1}{T/2}\right) 
\leq \ln\left(2 + \frac{2}{T}\right) 
\leq \ln(3)
\end{aligned}
$$
and
$$
\begin{aligned}
\sum_{t=\lceil T/2 \rceil}^T \frac{1}{\sqrt{t+1}} &\geq \int_{\lceil T/2 \rceil}^{T+1} \frac{1}{\sqrt{x+1}}\,dx = 2\sqrt{T+2} - 2\sqrt{\lceil T/2 \rceil + 1} \\
&\geq 2\sqrt{T+2} - 2\sqrt{T/2 + 2} = \frac{4(T+2) - 4(T/2 + 2)}{2\sqrt{T+2} + 2\sqrt{T/2 + 2}} \\
&= \frac{T}{\sqrt{T+2} + \sqrt{T/2 + 2}} \geq \frac{T}{2\sqrt{T+2}}
\geq \frac{1}{4}\sqrt{T+2},
\end{aligned}
$$
where the last inequality holds since $ T \geq 2 $. This yields the desired result in (ii).
\end{proof}




The descent lemma of GD and PGD (Lemma~\ref{lemma:gdupd_sm_sconv}) demonstrates the function value inequality for GD or PGD updates. 
The following lemma illustrates the relationship between iterations of PGD.
\begin{lemma}[Iterate Inequality for PGD]\label{lemma:iterate_ineq_pgd}
Let $f: \sS\rightarrow \real$ be a  proper closed and  \textbf{convex} function, where $\sS\subseteq\real^n$ is a closed and convex set such that $\sS\subseteq \interior(\dom(f))$~\footnote{This ensures the nonemptness of the subdifferential by Theorem~\ref{theorem:nonemp_relint_conv}.}. Define $\bx^\toptone  \in \sS$ as $\bx^\toptone  \leftarrow \projectS\left(\bx^\toptzero - \eta_t f^\prime(\bx^\toptzero)\right)$, where $f^\prime(\bx^\toptzero)\in \partial f(\bx^\toptzero)$ denotes any subgradient. 
Let $\bx^*$ be any optimizer. Then,
\begin{equation}\label{equation:iterate_ineq_pgd1}
\normtwo{\bx^\toptone  - \bx^*}^2 \leq \normtwo{\bx^\toptzero - \bx^*}^2 - 2 \eta_t  \left(f(\bx^\toptzero) - f(\bx^*)\right) + \eta_t^2 \normtwo{f^\prime(\bx^\toptzero)}^2. 
\end{equation}
Let $\{\bx^\toptzero\}_{t> 0}$ be the sequence generated by this update rule with positive stepsizes $\{\eta_t\}_{t > 0}$. Then, for any   nonnegative integer $T>0$, performing telescopic cancellations using \eqref{equation:iterate_ineq_pgd1} shows that
\begin{equation}\label{equation:iterate_ineq_pgd2}
\sum_{t=1}^{T} \eta_t \big(f(\bx^\toptzero) - f(\bx^*)\big) \leq \frac{1}{2} \normtwo{\bx^{(1)} - \bx^*}^2 + \frac{1}{2} \sum_{t=1}^{T} \eta_t^2 \normtwo{f^\prime(\bx^\toptzero)}^2. 
\end{equation}
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:iterate_ineq_pgd}]
In this case, a minimum subgradient should be
$$
\begin{aligned}
\normtwo{\bx^\toptone - \bx^*}^2 &= \normtwo{\projectS\big(\bx^\toptzero - \eta_t f^\prime(\bx^\toptzero)\big) - \projectS(\bx^*)}^2 
\stackrel{\dag}{\leq} \normtwo{\bx^\toptzero - \eta_t f^\prime(\bx^\toptzero) - \bx^*}^2 \\
&= \normtwo{\bx^\toptzero - \bx^*}^2 - 2 \eta_t \innerproduct{f^\prime(\bx^\toptzero), \bx^\toptzero - \bx^*} + \eta_t^2 \normtwo{f^\prime(\bx^\toptzero)}^2 \\
&\stackrel{\ddag}{\leq} \normtwo{\bx^\toptzero - \bx^*}^2 - 2 \eta_t \left(f(\bx^\toptzero) - f(\bx^*)\right) + \eta_t^2 \normtwo{f^\prime(\bx^\toptzero)}^2,
\end{aligned}
$$
where the inequality ($\dag$) follows from the nonexpansiveness of the  projection operator (Theorem~\ref{theorem:proj_nonexpan}), and the inequality ($\ddag$) follows from the subgradient inequality (Definition~\ref{definition:subgrad}, or from the gradient inequality if $f$ is differentiable; Theorem~\ref{theorem:conv_gradient_ineq}).
\end{proof}
We are now ready to prove the convergence result.



\begin{theoremHigh}[PGD for Convex and Lipschitz Functions with Dynamic Stepsizes: $ \mathcalO(\ln(T)/\sqrt{T}) $]\label{theorem:pgd_lipschitz_dyna}
Let $f: \sS \rightarrow \real$ be a proper closed,  convex, and  $L$-Lipschitz function, where $\sS\subseteq\real^n$ is a closed and  convex set. 
Let $\{\bx^\toptzero\}_{t > 0}$ be the sequence generated by the projected subgradient method (Algorithm~\ref{alg:pgd_gen}) for solving  problem (P2).
Suppose further that  $\left\{\fbest^\toptzero \triangleq \min\left\{f(\bx^{(1)}), f(\bx^{(2)}), \ldots, f(\bx^{(t)})\right\}\right\}_{t>0}$ is the sequence of best achieved values.
Then,
\begin{enumerate}[(i)]
\item If $\frac{\sum_{t=1}^{T} \eta_t^2}{\sum_{t=1}^{T} \eta_t} \rightarrow 0  $ as $T \rightarrow \infty$, then $\fbest^{(T)} - f(\bx^*) \rightarrow 0$ as $T \rightarrow \infty$.
\item If the stepsize for each $t$-th iteration is $\eta_t = \frac{1}{\normtwo{f^\prime(\bx^\toptzero)}\sqrt{t+1}}$ if $f^\prime(\bx^\toptzero) \neq \bzero$ and $\eta_t = \frac{1}{L}$ otherwise. Then, for any $ T \geq 5 $,
$$
\max\left\{ \fbest^{(T)} - f(\bx^*) , f(\widetildebx^{(T)}) - f(\bx^*) \right\} \leq \frac{L}{2} \frac{\normtwo{\bx^{(1)} - \bx^*}^2 + \ln(T+1)}{\sqrt{T+1}},
$$
where $
\widetildebx^{(T)} = \frac{1}{\sum_{t=1}^{T} \eta_t} \sum_{t=1}^{T} \eta_t \bx^\toptzero
$,~\footnote{Let $\Phi_T \triangleq \sum_{t=1}^{T} \eta_t $. We note that $\widetildebx^{(T+1)} = \frac{\Phi_T}{\Phi_{T+1}} \widetildebx^{(T)} + \frac{\eta_{T+1}}{\Phi_{T+1}} \bx^{(T+1)}$, such that the sequence $\{\widetildebx^{(t)}\}_{t>0}$ can be computed recursively.}
i.e., a weighted average of the iterates.
\end{enumerate}



\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:pgd_lipschitz_dyna}]
\textbf{(i).}  Using \eqref{equation:iterate_ineq_pgd2} along with the inequality $f(\bx^\toptzero) \geq \fbest^{(T)}$ for any $t=\{1,2,\ldots,T\}$, we obtain
\begin{equation}\label{equation:pgd_lipschitz_dyna_1}
\fbest^{(T)} - f(\bx^*) \leq \frac{1}{2} \frac{\normtwo{\bx^{(1)} - \bx^*}^2 + \sum_{t=1}^{T} \eta_t^2 \normtwo{f^\prime(\bx^\toptzero)}^2}{\sum_{t=1}^{T} \eta_t}. 
\end{equation}
Since
$\sum_{t=1}^{T} \eta_t \rightarrow \infty$ as  $T \rightarrow \infty$ and $\normtwo{f^\prime\left(\bx^\toptzero\right)} \leq L$, the desired result in (i) follows directly from \eqref{equation:pgd_lipschitz_dyna_1}.



\paragraph{(ii).}
%Using \eqref{equation:iterate_ineq_pgd2} along with the inequality $f(\bx^\toptzero) \geq \fbest^{(T)}$ for any $t=\{1,2,\ldots,T\}$, we obtain
%$$
%\fbest^{(T)} - f(\bx^*) \leq \frac{1}{2} \frac{\normtwo{\bx^{(1)} - \bx^*}^2 + \sum_{t=1}^{T} \eta_t^2 \normtwo{f^\prime(\bx^\toptzero)}^2}{\sum_{t=1}^{T} \eta_t}. 
%$$
By Jensen's inequality (Theorem~\ref{theorem:jensens_ineq})
$
f(\widetildebx^{(T)}) \leq \frac{1}{\sum_{t=1}^{T} \eta_t} \sum_{t=1}^{T} \eta_t f(\bx^\toptzero)
$
and \eqref{equation:iterate_ineq_pgd2}, we have
$$
f(\widetildebx^{(T)}) - f(\bx^*) \leq \frac{1}{2} \frac{\normtwo{\bx^{(1)} - \bx^*}^{2} + \sum_{t=1}^{T} \eta_t^{2} \normtwo{f^\prime\left(\bx^\toptzero\right)}^{2}}{\sum_{t=1}^{T} \eta_t}. 
$$
The above two inequalities indicate
$$
\max\left\{\fbest^{(T)} - f(\bx^*), f(\widetildebx^{(T)}) - f(\bx^*)\right\} \leq \frac{1}{2} \frac{\normtwo{\bx^{(1)} - \bx^*}^{2} + \sum_{t=1}^{T} \eta_t^{2} \normtwo{f^\prime\left(\bx^\toptzero\right)}^{2}}{\sum_{t=1}^{T} \eta_t}.
$$
By the definition of $ \eta_t $, $ \eta_t^{2} \normtwo{f^\prime\left(\bx^\toptzero\right)}^{2} \leq \frac{1}{t+1} $ (satisfied as an equality when $ f^\prime\left(\bx^\toptzero\right) \neq \bzero $ and as a strict inequality when $ f^\prime\left(\bx^\toptzero\right) = \bzero $); in addition, since $ \normtwo{f^\prime\left(\bx^\toptzero\right)} \leq L $, we have $ \eta_t \geq \frac{1}{L \sqrt{t+1}} $. Therefore,
$$
\max\left\{\fbest^{(T)} - f(\bx^*), f(\widetildebx^{(T)}) - f(\bx^*)\right\} \leq \frac{L}{2} \frac{\normtwo{\bx^{(1)} - \bx^*}^{2} + \sum_{t=1}^{T} \frac{1}{t+1}}{\sum_{t=1}^{T} \frac{1}{\sqrt{t+1}}}.
$$
Invoking Lemma~\ref{lemma:pgd_lem2}\ref{pgd_lem2_v1} with $ \Phi \triangleq \normtwo{\bx^{(1)} - \bx^*}^{2} $ obtains the desired result.
\end{proof}

\begin{figure}[h]
\centering       
\vspace{-0.25cm}                 
\subfigtopskip=2pt               
\subfigbottomskip=-2pt         
%\subfigcapskip=-10pt      
\subfigure[$\eta_t=\frac{1}{\sqrt{t+1}}$.]{\label{fig:gd_conv_stepsizes_1}
\includegraphics[width=0.48\linewidth]{imgs/gd_conv_stepsizes_1.pdf}}
\subfigure[$\eta_t=\frac{1}{t}$.]{\label{fig:gd_conv_stepsizes_2}
\includegraphics[width=0.48\linewidth]{imgs/gd_conv_stepsizes_2.pdf}}
\caption{Illustration of the ratio  $\text{Ratio}(T)={(\sum_{t=1}^{T} \eta_t^2)}/{(\sum_{t=1}^{T} \eta_t)}$ as a function of the total number of iterations $T$ for $\eta_t\triangleq\frac{1}{\sqrt{t+1}}$ and $\eta_t\triangleq\frac{1}{t}$.}
\label{fig:gd_conv_stepsizes}
\end{figure}

According to Theorem~\ref{theorem:pgd_lipschitz_dyna}, we can choose, for example, the stepsizes as
$\eta_t \triangleq \frac{1}{\sqrt{t+1}}$ for each iteration.
This ensures convergence of function values to $ f(\bx^*) $,  since  $\sum_{t=1}^{T} \frac{1}{\sqrt{t+1}}$ is of the order of $\sqrt{T}$, whereas  $\sum_{t=1}^{T} \frac{1}{t+1}$ grows logarithmically with $T$.
Figure~\ref{fig:gd_conv_stepsizes_1} illustrates the ratio of $\frac{\sum_{t=1}^{T} \eta_t^2}{\sum_{t=1}^{T} \eta_t}$ as a function of the total number of iterations $T$ (note that Lemma~\ref{lemma:pgd_lem2}\ref{pgd_lem2_v1} demonstrates 
$
\frac{ \sum_{t=1}^T \frac{1}{t+1}}{\sum_{t=1}^T \frac{1}{\sqrt{t+1}}} \leq \frac{ \ln(T+1)}{\sqrt{T+1}}
$ in this scenario).
On the other hand, when  $ \eta_t \triangleq \frac{1}{t} $, the numerator becomes
$
\sum_{t=1}^{T} \eta_t^2  = \sum_{t=1}^{T} \frac{1}{t^2}
$.
The series $ \sum_{t=1}^{\infty} \frac{1}{t^2} $ converges (it is a $p$-series with $ p = 2 > 1 $), and its partial sums grow logarithmically.
The denominator is
$
\sum_{t=1}^{T} \eta_t = \sum_{t=1}^{T} \frac{1}{t}
$.
The harmonic series $ \sum_{t=1}^{\infty} \frac{1}{t} $ diverges, but its partial sums grow like $ \ln(T) $.
Therefore, the ratio also converges to 0 as $T\rightarrow\infty$ (Figure~\ref{fig:gd_conv_stepsizes_2}).






The $ \mathcalO(\ln(T)/\sqrt{T}) $ rate of convergence proved in Theorem~\ref{theorem:pgd_lipschitz_dyna} is less favorable compared to the $ \mathcalO(1/\sqrt{T}) $ rate established in Theorem~\ref{theorem:pgd_lipschitz} for the projected gradient descent method using a constant stepsize, where the stepsize $\eta = \frac{R}{L\sqrt{T}}$ depends on the total number of steps $T$. 
However, under the  assumption that the feasible set $ \sS $ is compact, it is also possible to achieve an $ \mathcalO(1/\sqrt{T}) $ convergence rate with dynamic stepsizes.


\begin{theoremHigh}[PGD for Convex and Lipschitz Functions with Dynamic Stepsizes: $ (\mathcalO(1/\sqrt{T}) $]\label{theorem:pggd_conv_lip_dyn}
Let $f: \sS \rightarrow \real$ be a proper closed,  convex, and  $L$-Lipschitz function, where $\sS\subseteq\real^n$ is a closed and  convex set, and assume that $ \sS $ is \textbf{compact}. Let $ \Omega $ be an upper bound on the half-squared diameter of $ \sS $:
$
\Omega \geq \max_{\bx,\by \in \sS} \frac{1}{2} \normtwo{\bx - \by}^2.
$
Let $\{\bx^\toptzero\}_{t > 0}$ sequence generated by the projected subgradient method (Algorithm~\ref{alg:pgd_gen}) for solving  problem (P2) with stepsizes chosen as either
$$
\eta_t = \frac{\sqrt{2\Omega}}{L \sqrt{t+1}} 
\qquad\text{or}\qquad
\eta_t = \begin{cases} 
\frac{\sqrt{2\Omega}}{\normtwo{f^\prime(\bx^\toptzero)}\sqrt{t+1}}, & f^\prime(\bx^\toptzero) \neq \bzero, \\
\frac{\sqrt{2\Omega}}{L \sqrt{t+1}}, & f^\prime(\bx^\toptzero) = \bzero.
\end{cases}
$$
Then, for all $T \geq 2$, it holds that 
$$
\fbest^{(T)} - f(\bx^*) \leq \frac{2(1 + \ln(3)) L \sqrt{2\Omega}}{\sqrt{T+2}},
$$
where $\left\{\fbest^\toptzero \triangleq \min\left\{f(\bx^{(1)}), f(\bx^{(2)}), \ldots, f(\bx^{(t)})\right\}\right\}_{t>0}$  is the sequence of best achieved values.
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:pggd_conv_lip_dyn}]
By the iterate inequality in \eqref{equation:iterate_ineq_pgd1},  for any $t>0$,
$$
\frac{1}{2} \normtwo{\bx^\toptone - \bx^*}^2 \leq \frac{1}{2} \normtwo{\bx^\toptzero - \bx^*}^2 - \eta_t \big(f(\bx^\toptzero) - f(\bx^*)\big) + \frac{\eta_t^2}{2} \normtwo{f^\prime(\bx^\toptzero)}^2.
$$
Summing this inequality from $t = \lceil T/2 \rceil, \lceil T/2 \rceil + 1, \ldots, T$, we obtain
\begin{align*}
\sum_{t=\lceil T/2 \rceil}^{T} \eta_t \big(f(\bx^\toptzero) - f(\bx^*)\big) &\leq \frac{1}{2} \normtwo{\bx^{(\lceil T/2 \rceil)} - \bx^*}^2 - \frac{1}{2} \normtwo{\bx^{(T+1)} - \bx^*}^2 + \sum_{t=\lceil T/2 \rceil}^{T} \frac{\eta_t^2}{2} \normtwo{f^\prime(\bx^\toptzero)}^2 \\
&\leq \Omega + \sum_{t=\lceil T/2 \rceil}^{T} \frac{\eta_t^2}{2} \normtwo{f^\prime(\bx^\toptzero)}^2 
\leq \Omega + \Omega \sum_{t=\lceil T/2 \rceil}^{T} \frac{1}{t+1},
\end{align*}
where the last inequality follows from the definition of the stepsizes and the boundedness of the subgradients.
Since $\eta_t \geq \frac{\sqrt{2\Omega}}{L \sqrt{t+1}}$ and $f(\bx^\toptzero) \geq \fbest^{(T)}$ for all $t \leq T$, it follows that
$$
\sum_{t=\lceil T/2 \rceil}^{T} \eta_t \big(f(\bx^\toptzero) - f(\bx^*)\big) \geq \left( \sum_{t=\lceil T/2 \rceil}^{T} \frac{\sqrt{2\Omega}}{L \sqrt{t+1}} \right) \big(\fbest^{(T)} - f(\bx^*)\big). \
$$
Therefore, combining the above two inequalities and  invoking Lemma~\ref{lemma:pgd_lem2}\ref{pgd_lem2_v2} yields
$$
\fbest^{(T)} - f(\bx^*) \leq  \frac{L \sqrt{\Omega}}{\sqrt{2}} \frac{ \left(1 + \sum_{t=\lceil T/2 \rceil}^{T} \frac{1}{t+1}\right)}{\sum_{t=\lceil T/2 \rceil}^{T} \frac{1}{\sqrt{t+1}}}
\leq  \frac{L \sqrt{\Omega}}{\sqrt{2}} \frac{4(1 + \ln(3))}{\sqrt{T+2}}.
$$
This completes the proof.
\end{proof}

Based on   from Theorem~\ref{theorem:pggd_conv_lip_dyn}, assume further that $f$ is strongly convex.
The following theorem demonstrates that the convergence rate of the projected gradient descent for $\alpha$-strongly convex functions is similar to that of $\beta$-smooth functions, achieving an error bound of $\epsilon \leq \mathcalO(1/T)$, which is faster compared to the case involving only Lipschitz continuity.
\begin{theoremHigh}[PGD for SC and Lipschitz Functions: $\mathcalO(1/T)$]\label{theorem:pgd_strongconvex}
Let $f: \sS \rightarrow \real$ be a proper, differentiable, $\alpha$-strongly convex, and  $L$-Lipschitz function, where $\sS\subseteq\real^n$ is a closed and  convex set.
Let $\{\bx^\toptzero\}_{t > 0}$ be the sequence generated by the projected gradient descent method (Algorithm~\ref{alg:pgd_gen}) for solving  problem (P2) with an adaptive stepsize $\eta_t = \frac{2}{\alpha(t+1)}$ for each iteration $t$.
Assume  the function has a minimizer  $\bx^*$. Then,~\footnote{Note the result also holds for projected subgradient descent with non-differentiable functions.}
\begin{enumerate}[(i)]
\item  Define the sequence $\{\widetildebx^\toptzero \triangleq \sum_{i=1}^{t} \frac{2i}{t(t+1)} \bx^{(i)}\}$ (a convex combination of iterates). Then, for any $T>0$,
$$
\small
\begin{aligned}
f\left(\widetildebx^{(T)}\right) - f(\bx^*) \leq \frac{2L^2}{\alpha(T+1)}.
\end{aligned}
$$
\item Define  $\left\{\fbest^\toptzero \triangleq \min\left\{f(\bx^{(1)}), f(\bx^{(2)}), \ldots, f(\bx^{(t)})\right\}\right\}$ as the sequence of best achieved values. Then, for any $T>0$,
$$
\small
\begin{aligned}
\fbest^{(T)} -f(\bx^*) &\leq \frac{2L^2}{\alpha(T+1)}.
\end{aligned}
$$
It also follows that 
$$
\small
\begin{aligned}
\normtwo{\bx^{(i_T)} - \bx^{*}} \leq \frac{2 L}{\alpha\sqrt{T+1}},
\end{aligned}
$$
where $i_T\in\mathop{\argmin}_{t=1,2,\ldots,T} f(\bx^\toptzero)$.
\end{enumerate}
\noindent
This implies for any integer $T$ satisfying $T\geq \frac{2L^2}{\alpha\epsilon} -1$, it follows that 
$$
f\left(\widetildebx^{(T)}\right) - f(\bx^*)  \leq \epsilon 
\qquad\text{and}\qquad 
\fbest^{(T)} -f(\bx^*) \leq \epsilon.
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:pgd_strongconvex}]
\textbf{(i).}
To prove this part, we first establish bounds on the difference in function values
$
f(\bx^{(t)}) - f(\bx^*)
$  using the strong convexity (Definition~\ref{definition:scss_func}):
$$
\small
\begin{aligned}
f(\bx^{(t)}) &- f(\bx^*) \leq \nabla f(\bx^{(t)})^\top (\bx^{(t)} -\bx^*) - \frac{\alpha}{2} \normtwobig{\bx^{(t)} -\bx^*}^2 \\
%&= \frac{1}{\eta_t} (\bx^{(t)} - \by^{(t+1)})^\top (\bx^{(t)} -\bx^*) - \frac{\alpha}{2} \normtwobig{\bx^{(t)} -\bx^*}^2 \\
&\stackrel{\dag}{=} \frac{1}{2\eta_t} \left( \normtwobig{\bx^{(t)} -\bx^*}^2 + \normtwo{\bx^{(t)} - \by^{(t+1)}}^2 - \normtwo{\by^{(t+1)} -\bx^*}^2 \right) - \frac{\alpha}{2} \normtwobig{\bx^{(t)} -\bx^*}^2 \\
&= \frac{1}{2\eta_t} \left( \normtwobig{\bx^{(t)} -\bx^*}^2 - \normtwo{\by^{(t+1)} -\bx^*}^2 \right) + \frac{\eta_t}{2} \normtwobig{\nabla f(\bx^{(t)})}^2 - \frac{\alpha}{2} \normtwobig{\bx^{(t)} -\bx^*}^2 \\
&\stackrel{+}{\leq} \frac{1}{2\eta_t} \left( \normtwobig{\bx^{(t)} -\bx^*}^2 - \normtwobig{\bx^{(t+1)} -\bx^*}^2 \right) + \frac{\eta_t}{2} \normtwobig{\nabla f(\bx^{(t)})}^2 - \frac{\alpha}{2} \normtwobig{\bx^{(t)} -\bx^*}^2 \\
&\leq \left( \frac{1}{2\eta_t} - \frac{\alpha}{2} \right) \normtwobig{\bx^{(t)} -\bx^*}^2 - \frac{1}{2\eta_t} \normtwobig{\bx^{(t+1)} -\bx^*}^2 + \frac{\eta_t L^2}{2},
\end{aligned}
$$
where the equality ($\dag$) follows from Theorem~\ref{theorem:funda_opt} and the update of PGD, the inequality ($+$) follows from Projection Property-II (Lemma~\ref{lemma:proj_prop2}), and the last inequality follows from Theorem~\ref{theorem:lipsc_equiv}.
By multiplying $t$ on both sides and substituting the stepsize $\eta_t$ by $\frac{2}{\alpha(t+1)}$, we get
$$
\small
\begin{aligned}
t\big(f(\bx^{(t)}) - f(\bx^*)\big) \leq \frac{L^2}{\alpha} + \frac{\alpha}{4} \left( t(t-1) \normtwobig{\bx^{(t)} -\bx^*}^2 - t(t+1) \normtwobig{\bx^{(t+1)} -\bx^*}^2 \right).
\end{aligned}
$$
Finally, we can find the upper bound of the function value by telescoping the sum of  $T$ steps:
$$
\small
\begin{aligned}
f&\Big( \sum_{t=1}^T \frac{2t}{T(T+1)} \bx^{(t)} \Big) 
\leq \sum_{t=1}^T \frac{2t}{T(T+1)} f(\bx^{(t)}) \\
&\leq \frac{2}{T(T+1)} \sum_{t=1}^T \left( t f(\bx^*) + \frac{L^2}{\alpha} + \frac{\alpha}{4} \left( t(t-1) \normtwobig{\bx^{(t)} -\bx^*}^2 - t(t+1) \normtwobig{\bx^{(t+1)} -\bx^*}^2 \right) \right) \\
&= \frac{2}{T(T+1)} \sum_{t=1}^T t f(\bx^*) + \frac{2L^2}{\alpha(T+1)} - \frac{\alpha}{2} \normtwo{\bx^{(T+1)} -\bx^*}^2 
\leq f(\bx^*) + \frac{2L^2}{\alpha(T+1)}.
\end{aligned}
$$


\paragraph{(ii).} Using the update rule,
$$
\small
\begin{aligned}
\normtwo{\bx^\toptone - \bx^*}^2 &= \normtwo{\projectS\left(\bx^\toptzero - \eta_t \nabla f(\bx^\toptzero)\right) - \projectS\left(\bx^*\right)}^2 
\stackrel{\dag}{\leq} \normtwo{\bx^\toptzero - \eta_t \nabla f(\bx^\toptzero) - \bx^*}^2 \\
&= \normtwo{\bx^\toptzero - \bx^*}^2 - 2\eta_t \innerproduct{\nabla f(\bx^\toptzero), \bx^\toptzero - \bx^*} + \eta_t^2 \normtwobig{\nabla f(\bx^\toptzero)}^2\\
&\stackrel{\ddag}{\leq} \left(1 - \alpha \eta_t\right) \normtwo{\bx^\toptzero - \bx^*}^2 - 2 \eta_t \left(f(\bx^\toptzero) - f(\bx^*)\right) + \eta_t^2 L^2.
\end{aligned}
$$
where the inequality ($\dag$) follows from the nonexpansiveness of the  projection operator (Theorem~\ref{theorem:proj_nonexpan}), and the inequality ($\ddag$) follows from the definition of Lipschitzness and  strong convexity  (Theorem~\ref{theorem:lipsc_equiv}, Definition~\ref{definition:scss_func}).
Plugging $\eta_t = \frac{2}{\alpha(t+1)}$ into the inequality and multiplying the above by $t$, this implies
$$
t\left(f(\bx^\toptzero) - f(\bx^*)\right) \leq \frac{\alpha t(t-1)}{4} \normtwo{\bx^\toptzero - \bx^*}^2 - \frac{\alpha t(t+1)}{4} \normtwo{\bx^\toptone - \bx^*}^2 + \frac{t}{\alpha(t+1)} L^2.
$$
Since $f(\bx^\toptzero) \geq \fbest^{(T)}$ for all $t = \{1,2,\ldots,T\}$, telescoping the sum yields
$$
\left.
\begin{aligned}
&\left(\sum_{t=1}^T t\right)\left(\fbest^{(t)} - f(\bx^*)\right) 
\leq \sum_{t=1}^T t\left(f(\bx^\toptzero) - f(\bx^*)\right) \\
&\leq - \frac{\alpha}{4}T(T+1)\normtwo{\bx^{T+1} - \bx^*}^2 + \frac{L^2}{\alpha} \sum_{t=1}^T \frac{t}{t+1} \leq \frac{L^2 T}{\alpha}
\end{aligned}
\right\}
\,\,\implies\,\, 
\fbest^{(T)} - f(\bx^*) \leq \frac{2L^2}{\alpha(T+1)}.
$$
Note that $\fbest^{(T)} = f(\bx^{(i_T)})$. 
Since $f$ is strongly convex, SC Property-II (Theorem~\ref{theorem:exi_close_sc}) implies that 
%Since $\bx\in\interior(\sS)$, we have $\nabla f(\bx^*)=\bzero$, and strong convexity implies that
$$
\frac{\alpha}{2} \normtwo{\bx^{(i_T)} - \bx^{*}}^{2} \leq \fbest^{(T)} - f(\bx^*) \leq \frac{2 L^{2}}{\alpha(T+1)}
\quad\implies\quad 
\normtwo{\bx^{(i_T)} - \bx^*} \leq \frac{2 L}{\alpha\sqrt{T+1}}.
$$
This completes the proof.
\end{proof}




Similar to Theorem~\ref{theorem:gd_sc_ss}, we can also demonstrate the same result for the constrained scenario using projected gradient descent for strongly convex and smooth functions.
\begin{theoremHigh}[PGD for SC and SS Functions]\label{theorem:pgd_sc_ss}
Let $f: \sS \rightarrow \real$ be a proper differentiable, $\alpha$-strongly convex, and $\beta$-smooth function, where $\sS\subseteq\real^n$ is a closed and convex set. 
Let $\{\bx^\toptzero\}_{t > 0}$ be the sequence generated by the projected gradient descent method (Algorithm~\ref{alg:pgd_gen}) for solving  problem (P2) with a constant stepsize $\eta=\frac{1}{\beta}$.
Assume  the function has a minimizer  $\bx^*$. Then,
$$
\normtwo{\bx^{(T+1)} - \bx^*}^2 \leq \exp\left(-T\frac{\alpha}{\beta}\right) \normtwo{\bx^{(1)} - \bx^*}^2.
$$
This also shows  that $\normtwo{\bx^{(T)} -\bx^*}^2 \leq \epsilon$ after at most $T = \mathcalO\left(\frac{\beta}{\alpha} \ln \frac{1}{\epsilon}\right)$ steps.
\end{theoremHigh}
The proof of Theorem~\ref{theorem:pgd_sc_ss} follows exactly as in Theorem~\ref{theorem:gd_sc_ss}, substituting the appropriate projected gradient descent update for the standard gradient descent update using the descent lemma (Lemma~\ref{lemma:gdupd_sm_sconv}).
\begin{proof}[of Theorem~\ref{theorem:pgd_sc_ss}]
Let $g(\bx) \triangleq \beta\left(\bx - \projectS\left(\bx - \frac{1}{\beta} \nabla f(\bx)\right)\right)$. Then,
$$
\small
\begin{aligned}
\normtwo{\bx^{(t+1)} - \bx^*}^2 
&= \normtwo{\bx^{(t)} -\frac{1}{\beta}g(\bx^{(t)}) - \bx^*}^2 
= \normtwo{\bx^{(t)} - \bx^*}^2 - \frac{2}{\beta} g(\bx^{(t)})^\top (\bx^{(t)} - \bx^*) + \frac{1}{\beta^2} \normtwo{g(\bx^{(t)})}^2 \\
&\stackrel{\dag}{\leq} \left(1 - \frac{\alpha}{\beta}\right) \normtwo{\bx^{(t)} - \bx^*}^2 
\leq \left(1 - \frac{\alpha}{\beta}\right)^t \normtwo{\bx^{(1)} - \bx^*}^2 
\leq \exp\left(-t\frac{\alpha}{\beta}\right) \normtwo{\bx^{(1)} - \bx^*}^2,
\end{aligned}
$$
where inequality ($\dag$)  follows from  the descent lemma (\ref{descent_lem_pgdscss} in Lemma~\ref{lemma:gdupd_sm_sconv}).
\end{proof}



Following \citet{jain2017non}, we provide an alternative proof for the convergence result of PGD under SC and SS. 
This proof also highlights the roles of SC and SS in optimization algorithms.
\begin{proof}[of Theorem~\ref{theorem:pgd_sc_ss}: Alternative Way] Alternatively, we may prove the theorem in the following steps.
\paragraph{Strong smoothness: the gradient does not vanish too fast.} We use it to show that PGD always makes significant progress in each iteration:
$$
\begin{aligned}
& f(\bx^\toptone) - f(\bx^\toptzero) \leq \innerproduct{\nabla f(\bx^\toptzero), \bx^\toptone - \bx^\toptzero} + \frac{\beta}{2} \normtwo{\bx^\toptzero - \bx^\toptone}^{2} \\
&= \innerproduct{\nabla f(\bx^\toptzero), \bx^\toptone - \bx^{*}} + \innerproduct{\nabla f(\bx^\toptzero), \bx^{*} - \bx^\toptzero} + \frac{\beta}{2} \normtwo{\bx^\toptzero - \bx^\toptone}^{2} \\
&= \frac{1}{\eta} \innerproduct{\bx^\toptzero - \by^\toptone, \bx^\toptone - \bx^*} + \innerproduct{\nabla f(\bx^\toptzero), \bx^{*} - \bx^\toptzero} + \frac{\beta}{2} \normtwo{\bx^\toptzero - \bx^\toptone}^{2},
\end{aligned}
$$
where the last equality follows from the update $\by^{(t+1)} \leftarrow \bx^{(t)} - \eta \nabla f(\bx^{(t)})$.


\paragraph{Projection step.}To simplify the expression, which contains the term $\by^\toptone$, we  apply Projection Property-I (Lemma~\ref{lemma:proj_prop1}) to eliminate it, yielding $\innerproduct{\bx^\toptone-\bx^*, \bx^\toptone-\by^\toptone}\leq 0$:
$$
\begin{aligned}
\innerproduct{\bx^\toptzero - \by^\toptone, \bx^\toptone -\bx^*} &\leq \innerproduct{\bx^\toptzero - \bx^\toptone, \bx^\toptone -\bx^*} 
=-\innerproduct{\bx^\toptone-\bx^\toptzero, \bx^\toptone -\bx^*}\\
&= \frac{1}{2} \left(\normtwo{\bx^\toptzero -\bx^*}^2 - \normtwo{\bx^\toptzero - \bx^\toptone}^2 - \normtwo{\bx^\toptone -\bx^*}^2\right).
\end{aligned}
$$
where the last equality follows from the fundamental theorem of optimization (Theorem~\ref{theorem:funda_opt}).
Using $\eta = 1/\beta$ and combining the above results, we get
$$
\begin{aligned}
& f(\bx^\toptone) - f(\bx^\toptzero) \leq \innerproduct{\nabla f(\bx^\toptzero), \bx^{*} - \bx^\toptzero} + \frac{\beta}{2} \left(\normtwo{\bx^\toptzero - \bx^{*}}^{2} - \normtwo{\bx^\toptone - \bx^{*}}^{2}\right).
\end{aligned}
$$

\paragraph{Strong convexity.} The above expression is suitable  for a telescoping sum, except for the inner product term. Fortunately, this can be eliminated using strong convexity.
$$
\innerproduct{\nabla f(\bx^\toptzero), \bx^{*} - \bx^\toptzero} \leq f\left(\bx^{*}\right) - f(\bx^\toptzero) - \frac{\alpha}{2} \normtwo{\bx^\toptzero - \bx^{*}}^{2}
$$
Combining this with the previous results gives us
$$
\begin{aligned}
&0\leq   f(\bx^\toptone) - f(\bx^*) \leq \frac{\beta - \alpha}{2} \normtwo{\bx^\toptzero -\bx^*}^2 - \frac{\beta}{2} \normtwo{\bx^\toptone -\bx^*}^2\\
&\implies \ \normtwo{\bx^\toptone -\bx^*}^2 \leq (1-\frac{\alpha}{\beta}) \normtwo{\bx^\toptzero -\bx^*}^2 
\leq 
\exp\left(-t\frac{\alpha}{\beta}\right) \normtwo{\bx^{(1)} -\bx^*}^2.
\end{aligned}
$$
This completes the proof.
\end{proof}


All the above convergence results for PGD are established under the assumption that $f$  is convex and the set $\sS$ is also convex. Analysis of PGD for non-convex functions is more restricted. Several approaches address this limitation, the simplest being to transform the constraint set into a convex one, potentially by taking its convex hull, which is a common approach in relaxation methods. However, a less drastic alternative widely used in non-convex optimization literature exists. Note that the convergence results for the PGD algorithm in Theorem~\ref{theorem:pgd_sc_ss} do not require the objective function to be convex (or strongly convex/strongly smooth) over all of $\real^n$, but only over a subset of the space. When the function is restricted strongly convex and smooth (Definition~\ref{definition:res_scss_func}), convergence is also guaranteed.


\begin{theoremHigh}[PGD for RSC and RSS Functions (Non-Convex)]\label{theorem:pgd_rsc_rss}
Let $ f:\sS\subseteq\real^n\rightarrow \real $ be a proper closed (possibly non-convex) function satisfying the $\alpha$-RSC and $\beta$-RSS properties (Definition~\ref{definition:res_scss_func}) over a (possibly non-convex) constraint set $\sS$ with $\beta / \alpha < 2$.
Suppose $\bx^*$ is an optimizer such that $\nabla f(\bx^*)=\bzero$.
Let Algorithm~\ref{alg:pgd_gen} be executed with a constant stepsize $\eta = \frac{1}{\beta}$. 
Then,
$$
f(\bx^{(T+1)}) - f(\bx^*)\leq \exp\left(-T\frac{2\alpha-\beta}{\alpha}\right) \big(f(\bx^{(1)}) - f(\bx^*)\big)
$$
This also shows that $ f(\bx^{(T+1)}) -f(\bx^*)  \leq  \epsilon $ after at most  $ T = \mathcalO\left( \frac{\alpha}{2\alpha - \beta} \ln \frac{1}{\epsilon} \right) $ steps.
\end{theoremHigh}

This result holds even when the step length is chosen within a range that is large enough but still smaller than $1/\beta$. However, setting $\eta = \frac{1}{\beta}$  simplifies the proof and focuses attention on key concepts.

\begin{proof}[of Theorem~\ref{theorem:pgd_rsc_rss}]
We will use RSS to track the global convergence of the algorithm and RSC to locally assess the progress made by the algorithm in each iteration.
%We will use $\Phi_t = f(\bx^\toptone) - f(\bx^*)$ as the potential function.

\paragraph{Restricted strong smoothness.}
Since both $\bx^\toptzero, \bx^\toptone \in \sS$ due to the projection steps and $\eta = 1/\beta$, we apply the $\beta$-RSS property (Definition~\ref{definition:res_scss_func}):
$$
\begin{aligned}
f(\bx^\toptone) - f(\bx^\toptzero) 
&\leq \innerproduct{\nabla f(\bx^\toptzero), \bx^\toptone - \bx^\toptzero} + \frac{\beta}{2} \normtwo{ \bx^\toptzero - \bx^\toptone}^2\\
&= \frac{1}{\eta} \innerproduct{\bx^\toptzero - \by^\toptone, \bx^\toptone - \bx^\toptzero} + \frac{\beta}{2} \normtwo{\bx^\toptzero - \bx^\toptone }^2\\
&= \frac{\beta}{2} \left( \normtwo{\bx^\toptone - \by^\toptone}^2 - \normtwo{\bx^\toptzero - \by^\toptone}^2 \right),
\end{aligned}
$$
where the last equality follows from the fundamental theorem of optimization (Theorem~\ref{theorem:funda_opt}) with $\ba\triangleq \bx^\toptzero - \by^\toptone$ and $\bb\triangleq\bx^\toptzero-\bx^\toptone$.

\paragraph{Projection update rule.}
To handle the term $\by^\toptone$, unlike before, we cannot apply Projection Properties I, II, or III as non-convex projections may not satisfy them. Instead, we use Projection Property-O (Lemma~\ref{lemma:proj_prop0}), which applies to all projections:
\begin{equation}\label{equation:pgd_rsc_rss1}
\begin{aligned}
	f(\bx^\toptone) - f(\bx^\toptzero) 
	&\leq \frac{\beta}{2} \left( \normtwo{\bx^* - \by^\toptone}^2 - \normtwo{\bx^\toptzero - \by^\toptone}^2 \right)\\
	&\stackrel{\dag}{=} \frac{\beta}{2} \left( \normtwo{\bx^* - \bx^\toptzero}^2 + 2 \innerproduct{ \bx^* - \bx^\toptzero, \bx^\toptzero - \by^\toptone} \right)\\
	&= \frac{\beta}{2} \normtwo{\bx^* - \bx^\toptzero}^2 + \innerproduct{\bx^* - \bx^\toptzero, \nabla f(\bx^\toptzero)},
\end{aligned}
\end{equation}
where the  equality ($\dag$) follows from the fundamental theorem of optimization (Theorem~\ref{theorem:funda_opt}) with $\ba\triangleq \bx^* - \bx^\toptzero$ and $\bb\triangleq\by^\toptone - \bx^\toptzero$.

\paragraph{Restricted strong convexity.}
Since both $\bx^\toptzero, \bx^* \in \sS$, we apply the $\alpha$-RSC property (Definition~\ref{definition:res_scss_func}) to them. However, we do so in two ways:
$$
\begin{aligned}
f(\bx^*) - f(\bx^\toptzero) 
&\geq \innerproduct{\nabla f(\bx^\toptzero), \bx^* - \bx^\toptzero } + \frac{\alpha}{2} \normtwo{\bx^\toptzero - \bx^*}^2;\\
f(\bx^\toptzero) - f(\bx^*) 
&\geq \innerproduct{\nabla f(\bx^*), \bx^\toptzero - \bx^*} + \frac{\alpha}{2} \normtwo{\bx^\toptzero - \bx^* }^2 = \frac{\alpha}{2} \normtwo{\bx^\toptzero - \bx^*}^2,
\end{aligned}
$$
where in the second inequality, we use $\nabla f(\bx^*) = \bzero$. 
By manipulating these equations, we get:
$$
\innerproduct{\nabla f(\bx^\toptzero), \bx^* - \bx^\toptzero} + \frac{\beta}{2} \normtwo{\bx^* - \bx^\toptzero }^2 \leq \left( 2 - \frac{\beta}{\alpha} \right) \left( f(\bx^*) - f(\bx^\toptzero) \right).
$$
Plugging this into \eqref{equation:pgd_rsc_rss1}:
\begin{equation}\label{equation:pgd_rsc_rss2}
f(\bx^\toptone) - f(\bx^\toptzero) \leq \left( 2 - \frac{\beta}{\alpha} \right) \left( f(\bx^*) - f(\bx^\toptzero) \right).
\end{equation}
Equation~\eqref{equation:pgd_rsc_rss2} indicates that the larger the gap between $f(\bx^*)$ and $f(\bx^\toptzero)$, the greater the decrease in the objective value when moving from $\bx^\toptzero$ to $\bx^\toptone$. The form of the result is also quite fortunate as it assures us that we will cover a constant fraction $\left( 2 - \frac{\beta}{\alpha} \right)$ of the remaining ``distance'' to $\bx^*$ at each step. Rearranging this gives
$$
f(\bx^\toptone) - f(\bx^*)\leq (\kappa - 1) \big(f(\bx^\toptzero) - f(\bx^*)\big),
$$
where $\kappa \triangleq \beta / \alpha$. Note that we always have $\kappa \geq 1$ and by assumption $\kappa = \beta / \alpha < 2$, so that we always have $\kappa - 1 \in [0, 1)$. This proves the result using the fact that $1 - r \leq \exp(-r)$ for all $r \in \real$.
\end{proof}


We observe that the condition number once again plays a crucial role in determining the convergence rate of the algorithm, this time for a non-convex problem. However, it is noted that the condition number is defined differently here, utilizing the RSC and RSS  constants instead of the standard SC  and SS  constants used in Section~\ref{section:gd_basic}.

Readers will notice that while there was no restriction on the condition number $\kappa$ in the analysis of the GD or PGD algorithms (refer to Theorem~\ref{theorem:gd_sc_ss} or Theorem~\ref{theorem:pgd_sc_ss}), the analysis for PGD with RSC and RSS functions does require $\kappa < 2$. It is worth noting that this restriction can be relaxed for specific problems; however, doing so significantly complicates the analysis. Addressing this issue in a general context is beyond the scope of this book; see, for example, \citet{jain2017non} for more details.









\begin{algorithm}[h] 
\caption{Proximal Gradient Method}
\label{alg:prox_gd_gen}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$ and a closed convex function $g$ (usually non-smooth) satisfying (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t$;
\State $\by^{(t+1)} \leftarrow \bx^{(t)} - \eta_t \nabla f(\bx^{(t)})$;
\State $\bx^{(t+1)} \leftarrow \prox_{\eta_t g}(\by^{(t+1)}) \triangleq \mathcalT_{L_t}^{f,g}(\bx^\toptzero)$;
\EndFor
\State (Output Option 1) Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\State (Output Option 2) Output  $\bx_{\text{avg}}\leftarrow \frac{1}{T}(\sum_{t=1}^{t}\bx^{(t)})$ or $\sum_{t=1}^{T} \frac{2t}{T(T+1)} \bx^{(t)}$;
\State (Output Option 3) Output  $\bx_{\text{best}}\leftarrow \argmin_{t\in\{1,2,\ldots,T\}} f(\bx^{(t)})$;
\end{algorithmic} 
\end{algorithm}
\section{Proximal Gradient Method}\label{section:prox_gd}
We have introduced similarity between  projection and proximal operators in Section~\ref{section:proj_prox_sep}.
Noting the equivalence $\prox_{\indicatorS}(\by) = \projectS(\by)$, we can replace the projection step in Algorithm~\ref{alg:pgd_gen} with 
$$
\left\{\bx^{(t+1)} \leftarrow \mathcalP_{\sS}(\by^{(t+1)})\right\}
\qquad\leadsto \qquad
\left\{\bx^{(t+1)} \leftarrow \prox_{\indicatorS}(\by^{(t+1)})\right\}.
$$
When discussing the convergence results for the PGD approach, we require the underlying set $\sS$ to be closed convex~\footnote{Again, we should note that the notion of projection operator is only interesting for closed sets. Unless otherwise stated, when we discuss the projection operator, we assume inherently that the set is closed. Similarly, we assume the function is proper when we discuss the proximal operator.} such that the projection operator satisfies Projection Properties-O, I, II, III (Lemmas~\ref{lemma:proj_prop0}$\sim$\ref{lemma:proj_prop3}), which are essential for deriving convergence results for PGD.
The \textit{proximal gradient descent method} (Algorithm~\ref{alg:prox_gd_gen}) extends the projection step by 
\begin{equation}\label{equation:prox_obj1}
\left\{\bx^{(t+1)} \leftarrow \prox_{\indicatorS}(\by^{(t+1)})\right\}
\qquad\leadsto \qquad
\left\{\bx^{(t+1)} \leftarrow \prox_{\textcolor{mylightbluetext}{\eta_t g}}(\by^{(t+1)})\right\},
\end{equation}
where $\eta_t$ is the stepsize at $t$-th iteration (we will see its role shortly).
Similarly, we require the function $g$ to be closed and convex such that the proximal steps satisfy  Proximal Properties-O, I, II, III, IV (Lemmas~\ref{lemma:prox_prop0}$\sim$\ref{lemma:prox_prop3}).
Apparently, when $g=\indicatorS$, the method reduces to a PGD approach.

In PGD, the update rule from $t$-th to $(t+1)$-th iteration is 
\begin{center}
\framebox{
\begin{minipage}{0.95\textwidth}
\begin{equation}\label{equation:pgd_decom}
\small
\textbf{(PGD)}:\quad 
\begin{aligned}
\bx^\toptone 
&\leftarrow \mathop{\argmin}_{\bx\in\sS} \normtwo{\bx - \left(\bx^{(t)} - \eta_t \nabla f(\bx^{(t)})\right)}^2\\
&=\mathop{\argmin}_{\bx\in\sS} 
\left\{
\frac{1}{2\eta_t} \normtwo{\bx-\bx^\toptzero}^2 + f(\bx^\toptzero)+ \innerproduct{\nabla f(\bx^\toptzero), \bx-\bx^\toptzero} 
\right\},
\end{aligned}
\end{equation}
\end{minipage}
}
\end{center}
where we add a constant $f(\bx^\toptzero)$ term in the second form since the optimization is over $\bx$.
Therefore, the update rule in PGD can be seen as a minimization of the sum of the linearization of the smooth part around the current iterate plus a quadratic term. The optimization of the linearization ensures significant progress, while the quadratic term acts as regularization, ensuring the update stays within a neighborhood of the current iterate $\bx^\toptzero$.
The proximal gradient method add a non-smooth term $g$ into the objective function in \eqref{equation:pgd_decom} while consider a unconstrained problem:
\begin{subequations}\label{equation:prox_decom}
\begin{tcolorbox}[colback=white,colframe=black]
\begin{minipage}{1\textwidth}
\small
\begin{align}
\bx^\toptone 
&\leftarrow\mathop{\argmin}_{\bx\in\textcolor{mylightbluetext}{\real^n}} 
\left\{
\frac{1}{2\eta_t} \normtwo{\bx-\bx^\toptzero}^2 + f(\bx^\toptzero)+ \innerproduct{\nabla f(\bx^\toptzero), \bx-\bx^\toptzero }
+\textcolor{mylightbluetext}{g(\bx)}
\right\}\\
\textbf{(Prox)}:\qquad\,\,\,&=\mathop{\argmin}_{\bx\in\textcolor{mylightbluetext}{\real^n}} \eta_t g(\bx) +\frac{1}{2}\normtwo{\bx-\left( \bx^\toptzero - \eta_t\nabla f(\bx^\toptzero) \right)}\\
&=\prox_{\eta_t g}\left(\bx^\toptzero - \eta_t\nabla f(\bx^\toptzero)\right)
\triangleq \mathcalT_{L_t}^{f,g}(\bx^\toptzero), \label{equation:prox_decom3}
\end{align}
\end{minipage}
\end{tcolorbox}
\noindent where $L_t \triangleq\frac{1}{\eta_t}$, and $\mathcalT_{L}^{f,g}(\bx)\triangleq\prox_{\frac{1}{L} g}\left(\bx - \frac{1}{L}\nabla f(\bx)\right)$ denotes the \textit{prox-grad operator}. 
Define 
$$
\mathcalG_{L}^{f,g}(\bx)\triangleq L\big(\bx-\mathcalT_{L}^{f,g}(\bx)\big).
$$
Then the update in \eqref{equation:prox_decom3} can be equivalently denoted as 
\begin{equation}
\bx^\toptone 
\leftarrow 
\prox_{\eta_t g}\left(\bx^\toptzero - \eta_t\nabla f(\bx^\toptzero)\right)
\triangleq \mathcalT_{L_t}^{f,g}(\bx^\toptzero)
\triangleq \bx^\toptzero - \frac{1}{L_t} \mathcalG_{L_t}^{f,g}(\bx^\toptzero).
\end{equation}
\end{subequations}
The last form resembles a standard gradient descent update. 
Therefore, the term $\mathcalG_{L_t}^{f,g}(\bx^\toptzero)$ is called a \textit{gradient mapping}. 
When the identities of $f$ and $g$ are clear from the context, we will omit the superscripts and write $\mathcalT_{L_t}(\cdot)$ or $\mathcalG_{L_t}(\cdot)$ instead of $\mathcalT_{L_t}^{f,g}(\cdot)$ or $\mathcalG_{L_t}^{f,g}(\cdot)$.

\index{Gradient mapping}\index{Prox-grad operator}

Equation~\eqref{equation:prox_decom} explains the scaling term in \eqref{equation:prox_obj1}.
This demonstrates that the proximal gradient method aims to optimize the composite function:
$$
\text{(P3)}:\qquad \min \{F(\bx) \triangleq f(\bx)+g(\bx)\},
$$
where $f$ and $g$ satisfy (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}.


\begin{assumption}[Proximal Gradient Method]\label{assumption:proximal_grad}
In problem (P3), we often assume 
\begin{itemize}
\item[(A1)] $ g: \real^n \rightarrow (-\infty, \infty] $ is proper, closed, and convex.~\footnote{A few examples are discussed in the paragraph below Definition~\ref{definition:opt_probs_all}.}
\item[(A2)] $ f: \real^n \rightarrow (-\infty, \infty] $ is proper and closed, $ \dom(f) $ is convex, $ \dom(g) \subseteq \textcolor{mylightbluetext}{\interior}(\dom(f)) $, and $ f $ is $ \beta $-smooth over $ \textcolor{mylightbluetext}{\interior}(\dom(f)) $.
\end{itemize}
The iterative methods introduced will use the function $g$ to generate a ``candidate" update. Therefore, we assume the function $g$ is proper closed and convex.
We do not assume the function $g$ has a compact domain; since the proximal operator has a quadratic term such that the function is strongly convex. This ensures the update has a minimum (Lemma~\ref{lemma:prox_prop3}, Theorem~\ref{theorem:exi_close_sc}).
\end{assumption}


Assumption (A1) ensures that $ g(\bx) $ is proper, closed, and convex, which guarantees that the proximal step has a unique solution (Lemma~\ref{lemma:prox_prop3}).
The domain compatibility  $ \dom(g) \subseteq \interior(\dom(f)) $ in (A2) ensures that the proximal operator is applied within the region where $ f(\bx) $ is differentiable, avoiding issues at the boundary of $ \dom(f) $. The closedness of $f$ and $g$ ensures that $F\triangleq f+g$ is also closed such that iterative algorithms can find stationary points (see discussion in Figure~\ref{fig:low_nonlowersemis}).


Several stopping criteria for gradient descent were discussed in Section~\ref{section:gradient-descent-all}. The descent lemma for the proximal gradient method shows that the gradient mapping can serve as a stopping criterion. The following theorem establishes that the stationary point of (P3) corresponds to the condition $\mathcalG_{L}^{f,g}(\bx)=\bzero$ (Theorem~\ref{theorem:opt_cond_p3}).


\begin{theoremHigh}[Optimality Condition using Gradient Mapping]\label{theorem:opt_gdmap}
Let $ f $ be a proper \textcolor{black}{closed} and $\beta$-smooth function,  and let $ g $ be a proper closed and convex function satisfying assumption (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}.
Let $L\triangleq\beta$.
Then,
\begin{enumerate}[(i)]
\item When  $ g_0(\bx) = 0 $ for all $\bx\in\real^n$, $ \mathcalG_L^{f,g_0}(\bx) = \nabla f(\bx) $ for any $ \bx \in \interior(\dom(f)) $.
\item \label{opt_gdmap_item2} For $ \bx^* \in \interior(\dom(f)) $, it holds that $ \mathcalG_L^{f,g}(\bx^*) = \bzero $ if and only if $ \bx^* $ is a stationary point of problem (P3).
\item  Suppose additionally that $ f $ is convex. Then, for $ \bx^* \in \dom(g) $, $ \mathcalG_L^{f,g}(\bx^*) = \bzero $ if and only if $ \bx^* $ is an optimal solution of problem (P3).
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:opt_gdmap}]
\textbf{(i).} Since $ \prox_{\frac{1}{L}g_0}(\by) = \by $ for all $ \by \in \real^n $ if $g_0(\bx)=0$ for all $\bx\in\real^n$, it follows that
$
\mathcalG_L^{f,g_0}(\bx) = L(\bx - \mathcalT_L^{f,g_0}(\bx)) %= L\left(\bx - \prox_{\frac{1}{L}g_0}\left(\bx - \frac{1}{L}\nabla f(\bx)\right)\right) 
= L\left(\bx - \left(\bx - \frac{1}{L}\nabla f(\bx)\right)\right) = \nabla f(\bx).
$
\paragraph{(ii).} $ \mathcalG_L^{f,g}(\bx^*) = \bzero $ if and only if $ \bx^* = \prox_{\frac{1}{L}g}\left(\bx^* - \frac{1}{L}\nabla f(\bx^*)\right) $. By  Proximal Property-I (Lemma~\ref{lemma:prox_prop1}), the latter relation holds if and only if
$$
\bx^* - \frac{1}{L}\nabla f(\bx^*) - \bx^* \in \frac{1}{L}\partial g(\bx^*)
\qquad \iff\qquad 
-\nabla f(\bx^*) \in \partial g(\bx^*),
$$
which is exactly the condition for stationarity (Theorem~\ref{theorem:opt_cond_p3}).
\paragraph{(iii).} When $f$ is convex,  Theorem~\ref{theorem:opt_cond_p3} shows $\bx^*$ is an optimal solution of (P3).
\end{proof}


Previously, we observed that descent lemmas (e.g., Lemma~\ref{lemma:gdupd_sm_sconv}) are crucial for proving convergence results. For the proximal gradient descent method, there is a similar form of the descent lemma.
\begin{lemma}[Descent Lemma for Proximal Gradient Method]\label{lemma:des_lem_prox}
Let $ f $ be a proper \textcolor{black}{closed} and $\beta$-smooth function, and let  $ g $ be a proper closed and convex function satisfying assumption (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}. Consider the composite function $ F \triangleq f + g $ and the prox-grad operator $ \mathcalT_{L} \triangleq \mathcalT_{L}^{f,g} $. 
Then, for any $ \bx \in \interior(\dom(f)) $ and $ L \in \left(\frac{\beta}{2}, \infty\right) $, the following inequality holds:
\begin{equation}\label{equation:des_lem_prox_res1}
F(\mathcalT_{L}(\bx)) -F(\bx) \leq \frac{\frac{\beta}{2} -L}{L^2} \normtwo{\mathcalG_{L}^{f,g}(\bx)}^2 \leq  0, 
\end{equation}
where $ \mathcalG_{L}^{f,g} : \interior(\dom(f)) \rightarrow \real^n $ is the operator defined by $\mathcalG_{L}^{f,g}(\bx)\triangleq L\big(\bx-\mathcalT_{L}(\bx)\big) $ for all $ \bx \in \interior(\dom(f)) $.

In addition, for any $\by\in\real^n$, $\bx \in \interior(\dom(f))$, consider the \textit{proximal gap} $\mathcalD_f(\by, \bx) \triangleq f(\by) - f(\bx) - \innerproduct{\nabla f(\bx), \by - \bx }$~\footnote{When $f$ is convex, this is also known as the Bregman distance (Definition~\ref{definition:breg_dist}).} and  $L\triangleq\beta$. Then it holds that 
\begin{equation}\label{equation:des_lem_prox_res2}
F(\mathcalT_{L}(\bx)) - F(\by)  \leq   \frac{L}{2} \normtwo{\by-\bx}^2 -\frac{L}{2}\normtwo{\by - \mathcalT_{L}(\bx)}^2 - \mathcalD_f(\by, \bx).~\footnote{Similar to \ref{descent_lem_gdscss} and \ref{descent_lem_pgdscss} in the descent lemma for GD and PGD (Lemma~\ref{lemma:gdupd_sm_sconv}), we can set $\by$ to the optimal point $\by\triangleq\bx^*$ to obtain convergence results.}
\end{equation}
When $\by=\bx$, \eqref{equation:des_lem_prox_res2} reduces to \eqref{equation:des_lem_prox_res1} by letting $L\triangleq\beta$.
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:des_lem_prox}]
\textbf{Equation~\eqref{equation:des_lem_prox_res1}.}
For simplicity, denote $ \bx^+ \triangleq \mathcalT_{L}(\bx) $. By the smoothness of $f$ (Definition~\ref{definition:scss_func}),
$$
f(\bx^+) \leq f(\bx) + \innerproduct{\nabla f(\bx), \bx^+ - \bx} + \frac{\beta}{2} \normtwo{\bx - \bx^+}^2. 
$$
By the Proximal Property-I (Lemma~\ref{lemma:prox_prop1}), since $ \bx^+ = \prox_{\frac{1}{L}g}(\bx - \frac{1}{L}\nabla f(\bx)) $, we have
$$
\begin{aligned}
&\innerproduct{\bx - \frac{1}{L}\nabla f(\bx) - \bx^+, \bx - \bx^+} \leq \frac{1}{L} g(\bx) - \frac{1}{L} g(\bx^+)\\
&\quad\implies\quad  g(\bx^+)    \leq  g(\bx)-L \normtwo{\bx^+ - \bx}^2  -\innerproduct{\nabla f(\bx), \bx^+ - \bx} .
\end{aligned}
$$
Combining the two inequalities yields
$$
f(\bx^+) + g(\bx^+) \leq f(\bx) + g(\bx) + \left(-L + \frac{\beta}{2}\right) \normtwo{\bx^+ - \bx}^2.
$$
Hence, taking into account the definitions of $ \bx^+ $, $ \mathcalG_{L}^{f,g}(\bx) $, and the identities $ F(\bx) = f(\bx) + g(\bx) $, $ F(\bx^+) = f(\bx^+) + g(\bx^+) $, the desired result follows.

\paragraph{Equation~\eqref{equation:des_lem_prox_res2}.} For the second part, consider the function
$$
\varphi(\bu) \triangleq f(\bx) + \innerproduct{\nabla f(\bx), \bu - \bx} + g(\bu) + \frac{L}{2} \normtwo{\bu - \bx}^2. 
$$
which is the first objective function in \eqref{equation:prox_decom} for each update of the proximal gradient method.
Since $\varphi$ is an $L$-strongly convex function and $ \mathcalT_{L}(\bx) = \arg\min_{\bu \in \real^n} \varphi(\bu) $, it follows by Theorem~\ref{theorem:exi_close_sc}(ii) that
\begin{equation}\label{equation:des_lem_prox1}
\varphi(\by) - \varphi(\mathcalT_L(\bx)) \geq \frac{L}{2} \normtwo{\by - \mathcalT_L(\bx)}^2. 
\end{equation}
This implies, by the smoothness of $f$, that
\begin{equation}\label{equation:des_lem_prox2}
\begin{aligned}
\varphi(\mathcalT_L(\bx)) &= f(\bx) + \innerproduct{\nabla f(\bx), \mathcalT_L(\bx) - \bx} + \frac{L}{2} \normtwo{\mathcalT_L(\bx) - \bx}^2 + g(\mathcalT_L(\bx)) \\
&\geq f(\mathcalT_L(\bx)) + g(\mathcalT_L(\bx)) = F(\mathcalT_L(\bx)).
\end{aligned}
\end{equation}
Thus, combining \eqref{equation:des_lem_prox1} and \eqref{equation:des_lem_prox2} yields that 
$$
\begin{aligned}
%\varphi(\by) - F(\mathcalT_L(\bx)) &\geq \frac{L}{2} \normtwo{\by - \mathcalT_L(\bx)}^2 \quad \text{for any $\by \in \real^n $};\\
\underbrace{f(\bx) + \innerproduct{\nabla f(\bx), \by - \bx} + g(\by) + \frac{L}{2} \normtwo{\by - \bx}^2}_{=\varphi(\by)} - F(\mathcalT_L(\bx)) &\geq \frac{L}{2} \normtwo{\by - \mathcalT_L(\bx)}^2, \quad \forall \text{$\by \in \real^n $}.
\end{aligned}
$$
This proves the desired result.
\end{proof}

A direct consequence of Lemma~\ref{lemma:des_lem_prox} is the monotonicity of  function values and  iterates in proximal gradient methods.
\begin{lemma}[Monotonicity and Sufficient Decrease of Proximal Gradient]\label{lemma:mont_prox}
Let $ f $ be a proper \textcolor{black}{closed} and $\beta$-smooth function, and let $ g $ be a proper closed and convex function satisfying assumption (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}.
Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the proximal gradient method (Algorithm~\ref{alg:prox_gd_gen}) for solving problem (P3). Then, for any $L_t\triangleq L \in\left(\frac{\beta}{2}, \infty\right)\,\forall t>0$, \eqref{equation:des_lem_prox_res1} shows that
\begin{equation}\label{equation:mont_prox_1}
\left(
\begin{aligned}
	\textbf{Monotonicity and }\\
	\textbf{Sufficient Decrease}
\end{aligned}
\right): \qquad
F(\bx^\toptone) - F(\bx^\toptzero) \leq\frac{\frac{\beta}{2} -L}{L^2} \normtwo{\mathcalG_{L}^{f,g}(\bx^\toptzero)}^2 \leq 0.
\end{equation}
Since $\mathcalG_{L}^{f,g}(\bx)\neq  \bzero$ by Theorem~\ref{theorem:opt_gdmap}\ref{opt_gdmap_item2} and equals to $\bzero$ only for stationary points, this also shows that 
\begin{itemize}
	\item $F(\bx^\toptzero) - F(\bx^\toptone) \geq 0$.
	\item $F(\bx^\toptzero) > F(\bx^\toptone)$  if $\bx^\toptzero$  is not a stationary point of problem (P3).
\end{itemize}

Suppose additionally  that $f$ is convex and $L_t \triangleq L \triangleq \beta$. Then, for any optimal point $\bx^*$ of $F=f+g$, it holds that 
$$
\begin{aligned}
\textbf{(\textbf{\fejer} \text{monotonicity})}:\qquad &\normtwo{\bx^\toptone - \bx^*} \leq \normtwo{\bx^\toptzero - \bx^*};\\
\textbf{(Gradient mapping)}:\qquad & \normtwo{\mathcalG_{L}(\bx^\toptone)} \leq \normtwo{\mathcalG_{L}(\bx^\toptzero)},
\end{aligned}
$$
where $ \mathcalG_{L} \triangleq \mathcalG_{L}^{f, g} $ and $ \mathcalT_{L} \triangleq \mathcalT_{L}^{f, g} $.
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:mont_prox}]
$\textbf{\textbf{\fejer} \text{monotonicity}}.$
Since $f$ is convex, $\mathcalD_f$ qualifies  as a Bregman distance such that $\mathcalD_f(\bx^*, \bx^\toptzero)\geq 0$ (Remark~\ref{remark:bregnan_dist}).
For any $ t > 0 $, using the descent lemma in \eqref{equation:des_lem_prox_res2}, we obtain
$$
\begin{aligned}
\frac{2}{\beta} \left(  F(\bx^\toptone) - F(\bx^*) \right) &\leq   \normtwo{\bx^* - \bx^\toptzero}^2 -\normtwo{\bx^* - \bx^\toptone}^2 - \frac{2}{\beta} \mathcalD_f(\bx^*, \bx^\toptzero) \\
&\leq  \normtwo{\bx^* - \bx^\toptzero}^2 - \normtwo{\bx^* - \bx^\toptone}^2.
\end{aligned}
$$

\paragraph{Gradient mapping.}
Let $ \ba \triangleq \nabla f(\bx^\toptone) - \nabla f(\bx^\toptzero) $ and $ \bb \triangleq \bx^\toptone - \bx^\toptzero $. By the convexity and smoothness of $f$, Theorem~\ref{theorem:charac_smoo} shows that
$$
\normtwo{\ba}^2 \leq L \innerproduct{\ba, \bb}
\quad\implies \quad
\normtwo{\ba - \frac{L}{2} \bb}^2 \leq \frac{L^2}{4} \normtwo{\bb}^2
\quad\implies \quad
\normtwo{\frac{1}{L} \ba - \frac{1}{2} \bb} \leq \frac{1}{2} \normtwo{\bb}.
$$
Using the triangle inequality,
$$
\normtwo{\frac{1}{L} \ba - \bb } \leq \normtwo{ \frac{1}{L} \ba - \bb + \frac{1}{2} \bb} + \frac{1}{2} \normtwo{\bb} \leq \normtwo{\bb}.
$$
Plugging the expressions for $ \ba $ and $ \bb $ into the above inequality, we obtain that
$$
\normtwo{\bx^\toptzero - \frac{1}{L} \nabla f(\bx^\toptzero) - \bx^\toptone + \frac{1}{L} \nabla f(\bx^\toptone)} \leq \normtwo{\bx^\toptone - \bx^\toptzero}.
$$
Since $g$ is convex, using Proximal Property-IV (Lemma~\ref{theorem:proj_nonexpan}),
$$
\begin{aligned}
&\normtwo{ \mathcalG_{L}(\bx^\toptone)} =  L \normtwo{\bx^\toptone - \mathcalT_{L}(\bx^\toptone)} 
\\
&= L \normtwo{\prox_{\frac{1}{L} g} \left( \bx^\toptzero - \frac{1}{L} \nabla f(\bx^\toptzero) \right) - \prox_{\frac{1}{L} g} \left( \bx^\toptone - \frac{1}{L} \nabla f(\bx^\toptone) \right)} \\
&\leq L \normtwo{\bx^\toptzero - \frac{1}{L} \nabla f(\bx^\toptzero) - \bx^\toptone + \frac{1}{L} \nabla f(\bx^\toptone)} 
\leq L \normtwo{\bx^\toptone - \bx^\toptzero}  = \normtwo{\mathcalG_{L}(\bx^\toptzero)}.
\end{aligned}
$$
This completes the proof.
\end{proof}



We then prove the convergence results for the proximal gradient methods with $F$ being smooth, convex and smooth, and strongly convex and smooth, respectively.
\begin{theoremHigh}[Proximal Gradient for SS $f$]\label{theorem:prox_conv_ss}
Let $ f $ be a proper \textcolor{black}{closed} and $\beta$-smooth function, and let  $ g $ be a proper closed and convex function satisfying assumption (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}.
Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the proximal gradient method (Algorithm~\ref{alg:prox_gd_gen}) for solving problem (P3)  with a constant stepsize defined by $ L_t \triangleq L \in \left(\frac{\beta}{2}, \infty\right) $. Then,
\begin{enumerate}[(i)]
\item $ \mathcalG_L(\bx^\toptzero) \rightarrow \bzero $ as $ t \rightarrow \infty $.
\item 
$
\min_{t=1,\ldots,T} \normtwo{\mathcalG_L(\bx^\toptzero)} \leq \frac{\sqrt{F(\bx^{(1)}) - F(\bx^*)}}{\sqrt{CT}},
$
where $ C\triangleq -\frac{\frac{\beta}{2} -L}{L^2}$;
\item All limit points of the sequence $ \{\bx^\toptzero\}_{t > 0} $ are stationary points of problem (P3).
\end{enumerate}
\end{theoremHigh}

\begin{proof}[of Theorem~\ref{theorem:prox_conv_ss}]
\textbf{(i).} Since the sequence $ \{F(\bx^\toptzero)\}_{t > 0} $ is nonincreasing and bounded below, and is equal to zero only if $\bx^\toptzero$ is a stationary point (Lemma~\ref{lemma:mont_prox}), it converges. Thus, in particular, $ F(\bx^\toptzero) - F(\bx^\toptone) \rightarrow 0 $ as $ t \rightarrow \infty $, which, combined with \eqref{equation:mont_prox_1}, implies that $ \normtwo{\mathcalG_L(\bx^\toptzero)} \rightarrow \bzero $ as $ t \rightarrow \infty $.

\paragraph{(ii).} Performing telescopic cancellations on \eqref{equation:mont_prox_1} over $t=\{1,2,\ldots,T\}$, we have
$$
F(\bx^{(1)})-F(\bx^*) 
\geq 
F(\bx^{(1)}) - F(\bx^{(T+1)}) \geq C \sum_{t=1}^{T} \normtwo{\mathcalG_L(\bx^\toptzero)}^2 \geq CT \min_{t\in\{1,2,\ldots,T\}} \normtwo{\mathcalG_L(\bx^\toptzero)}^2.
$$
This concludes the result.

\paragraph{(iii).} Suppose that $ \widetildebx $ is a limit point of $ \{\bx^\toptzero\}_{t > 0} $. Then there exists a subsequence $ \{\bx^{(t_j)}\}_{j > 0} $ converging to $ \widetildebx $. For any $ j > 0 $,
$$
\normtwo{\mathcalG_L(\widetildebx)} \leq \normtwo{\mathcalG_L(\bx^{(t_j)}) - \mathcalG_L(\widetildebx)} + \normtwo{\mathcalG_L(\bx^{(t_j)})} \leq (2L + \beta) \normtwo{\bx^{(t_j)} - \widetildebx} + \normtwo{\mathcalG_L(\bx^{(t_j)})},
$$
where the second inequality follows from the Lipschitzness of $\mathcalG_L(\cdot)$ (Problem~\ref{prob:grad_map_lipschitz}). Since $\mathcalG_L(\bx^{(t_j)})$ tends to $ \bzero $ as $ j \rightarrow \infty $, it follows that $ \mathcalG_L(\widetildebx) = \bzero $, which by Theorem~\ref{theorem:opt_gdmap} implies that $ \widetildebx $ is a stationary point of problem (P3).
\end{proof}



\begin{theoremHigh}[Proximal Gradient for Convex and SS $f$: $\mathcalO(1/T)$]\label{theorem:prox_conv_ss_cvx}
Let $ f $ be a proper \textcolor{black}{closed} and $\beta$-smooth function, and let   $ g $ be a proper closed and convex function satisfying assumption (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}.
Additionally, assume  that $ f $ is \textbf{convex}. 
Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the proximal gradient method (Algorithm~\ref{alg:prox_gd_gen}) for solving problem (P3)  with a constant stepsize rule in which $ L_t  \triangleq \beta $ for all $ t > 0 $. 
Then, for any optimizer $ \bx^*$ of $F\triangleq f+g$ and $ T > 1 $, it follows that
$$
F(\bx^{(T)}) - F(\bx^*) \leq \frac{ \beta \normtwo{\bx^{(1)} - \bx^*}^2}{2(T-1)}.
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:prox_conv_ss_cvx}]
Since $f$ is convex, $\mathcalD_f$ qualifies a Bregman distance such that $\mathcalD_f(\bx^*, \bx^\toptzero)\geq 0$ (Remark~\ref{remark:bregnan_dist}).
For any $ t > 0 $, using the descent lemma in \eqref{equation:des_lem_prox_res2}, we obtain
$$
\begin{aligned}
\frac{2}{\beta} \left(  F(\bx^\toptone) - F(\bx^*) \right) &\leq   \normtwo{\bx^* - \bx^\toptzero}^2 -\normtwo{\bx^* - \bx^\toptone}^2 - \frac{2}{\beta} \mathcalD_f(\bx^*, \bx^\toptzero) \\
&\leq  \normtwo{\bx^* - \bx^\toptzero}^2 - \normtwo{\bx^* - \bx^\toptone}^2.
\end{aligned}
$$
Performing telescopic cancellations over $ t = \{1, 2, \ldots, T-1\} $, we obtain
$$
\sum_{t=1}^{T-1} \left(  F(\bx^\toptone) - F(\bx^*) \right) \leq   \frac{ \beta}{2}\normtwo{\bx^* - \bx^{(1)}}^2 - \frac{ \beta}{2}\normtwo{\bx^* - \bx^{(T)}}^2
\leq  
\frac{ \beta}{2}\normtwo{\bx^* - \bx^{(1)}}^2.
$$
By the monotonicity of $ \{ F(\bx^\toptzero) \}_{t > 0} $ (Equation~\eqref{equation:des_lem_prox_res1} and Lemma~\ref{lemma:mont_prox}), we can conclude that
$$
\begin{aligned}
(T-1) \left( F(\bx^{(T)}) - F(\bx^*) \right) &\leq \sum_{t=1}^{T-1} \left( F(\bx^\toptone) - F(\bx^*) \right) \leq \frac{ \beta}{2} \normtwo{\bx^* - \bx^{(1)}}^2,
\end{aligned}
$$
which implies the desired result.
\end{proof}



\begin{theoremHigh}[Proximal Gradient for SC and SS $f$]\label{theorem:prox_conv_ss_sc}
Let $ f $ be a proper \textcolor{black}{closed} and $\beta$-smooth function, and let    $ g $ be a proper closed and convex function satisfying assumption (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}. 
Additionally, assume that $ f $ is \textbf{$ \alpha $-strongly convex}.  
Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the proximal gradient method (Algorithm~\ref{alg:prox_gd_gen}) for solving problem (P3)  with a constant stepsize rule in which $ L_t  \triangleq \beta $ for all $ t > 0 $.
Then, for any optimizer $ \bx^*$ of $F\triangleq f+g$ and $ t >1 $,  it follows that
$$
\begin{aligned}
\normtwo{\bx^\toptone - \bx^*}^2 &\leq \exp\left(-t\frac{\alpha}{\beta}\right)  \normtwo{\bx^{(1)} - \bx^*}^2;\\
F(\bx^\toptone) - F(\bx^*) &\leq \frac{ \beta}{2} \exp\left(-t\frac{\alpha}{\beta}\right)  \normtwo{\bx^{(1)} - \bx^*}^2.
\end{aligned}
$$
This also shows  that $\normtwo{\bx^{(T)} -\bx^*}^2 \leq \epsilon$ after at most $T = \mathcalO\left(\frac{\beta}{\alpha} \ln \frac{1}{\epsilon}\right)$ steps.
\end{theoremHigh}

\begin{proof}[of Theorem~\ref{theorem:prox_conv_ss_sc}]
Since $f$ is $\alpha$-strongly convex, $\mathcalD_f$ qualifies a Bregman distance such that $\mathcalD_f(\bx^*, \bx^\toptzero)\geq 0$ (Remark~\ref{remark:bregnan_dist}), and  it follows by the definition of strong convexity (Definition~\ref{definition:scss_func}) that
$$
\mathcalD_f(\bx^*, \bx^\toptzero) = f(\bx^*) - f(\bx^\toptzero) - \innerproduct{\nabla f(\bx^\toptzero), \bx^* - \bx^\toptzero} \geq \frac{\alpha}{2} \normtwo{\bx^\toptzero - \bx^*}^2.
$$
For any $ t > 0 $, using the descent lemma in \eqref{equation:des_lem_prox_res2}, we obtain
\begin{equation}\label{equation:prox_conv_ss_sc}
\begin{aligned}
	F(\bx^\toptone) - F(\bx^*) &\leq  \frac{\beta}{2} \normtwo{\bx^* - \bx^\toptzero}^2 -\frac{\beta}{2}\normtwo{\bx^* - \bx^\toptone}^2 - \mathcalD_f(\bx^*, \bx^\toptzero)\\
&\leq   \frac{\beta-\alpha}{2} \normtwo{\bx^* - \bx^\toptzero}^2 -\frac{\beta}{2}\normtwo{\bx^* - \bx^\toptone}^2.
\end{aligned}
\end{equation}
Since $ \bx^* $ is a minimizer of $ F $, $  F(\bx^\toptone) -F(\bx^*) \geq 0 $, and hence, the above inequality implies that
$$
\normtwo{\bx^\toptone - \bx^*}^2  \leq \left(1 - \frac{\alpha}{ \beta}\right) \normtwo{\bx^\toptzero - \bx^*}^2 
\leq \left(1 - \frac{\alpha}{ \beta}\right)^t \normtwo{\bx^{(1)} - \bx^*}^2.
$$
On the other hand, \eqref{equation:prox_conv_ss_sc} also shows 
$$
\begin{aligned}
	F(\bx^\toptone) - F(\bx^*) &\leq \frac{\beta - \alpha}{2} \normtwo{\bx^\toptzero - \bx^*}^2 - \frac{\beta}{2} \normtwo{\bx^\toptone - \bx^*}^2 
	\leq \frac{ \beta - \alpha}{2} \normtwo{\bx^\toptzero - \bx^*}^2 \\
	&= \frac{ \beta}{2} \left(1 - \frac{\alpha}{ \beta}\right) \normtwo{\bx^\toptzero - \bx^*}^2 
	\leq \frac{ \beta}{2} \left(1 - \frac{\alpha}{ \beta}\right)^{t} \normtwo{\bx^{(1)} - \bx^*}^2,
\end{aligned}
$$
which completes the proof.
\end{proof}

Once again, we observe that the condition number $\kappa\triangleq\frac{\beta}{\alpha}$ plays a crucial role in determining the convergence rate of the algorithm. The rate of convergence for the proximal gradient method is identical to those described in Theorems \ref{theorem:gd_sc_ss} and \ref{theorem:pgd_sc_ss}. However, in this case, the algorithm is applied to solve the composite problem (P3).

\begin{algorithm}[h] 
\caption{Proximal Point Method}
\label{alg:prox_point_gen}
\begin{algorithmic}[1] 
\Require A proper closed convex function $g$ ; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t$;
\State $\bx^{(t+1)} \leftarrow \prox_{\eta_t g}(\bx^{(t)})$;
\EndFor
\State Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\end{algorithmic} 
\end{algorithm}

\begin{figure}[h]
\centering  
\vspace{-0.15cm} 
%\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\includegraphics[width=0.78\textwidth]{./imgs/prox_point.pdf}
\caption{Geometric view of the proximal point method. The minimum of the function 
$
g(\bx) + \frac{1}{2\eta_t} \normtwo{\bx - \bx^\toptzero}^2
$
is attained at a unique point $ \bx^\toptone $. The value $ \zeta_t $ in the quadratic term represents the scalar adjustment needed to raise the graph of
$
-\frac{1}{2\eta_t} \normtwo{\bx - \bx^\toptzero}^2
$
so that it just touches the graph of $ g $. The slope shown in the figure,
$
\frac{\bx^\toptzero - \bx^\toptone}{\eta_t},
$
is the common gradient of $ g(\bx) $ and 
$
-\frac{1}{2\eta_t} \normtwo{\bx - \bx^\toptzero}^2
$
at the minimizing point $ \bx^\toptone $.
}
\label{fig:prox_point}
\end{figure}
\section{Proximal Point Method}\label{section:proximal_point}

Consider the problem
\begin{equation}\label{equation:prob_prox_point}
\min_{\bx \in \real^n} g(\bx),
\end{equation}
where $ g : \real^n \to (-\infty, \infty] $ is a proper closed and convex function. Problem~\eqref{equation:prob_prox_point} is actually a special case of the composite problem (P3) with $ f \triangleq 0 $. The update step of the proximal gradient method in this scenario  takes the form
$$
\bx^\toptone = \prox_{\eta_t g}(\bx^\toptzero).
$$
Taking $ \eta_t = \eta$ for some $ \eta > 0 $, we obtain the \textit{proximal point method} (Algorithm~\ref{alg:prox_point_gen}).
See Figure~\ref{fig:prox_point} for an illustration of the optimization procedure at each step.



The proximal point method is generally not practical because its general step requires minimizing the function  $ g(\bx) + \frac{1}{2\eta} \normtwo{ \bx - \bx^\toptzero}^2 $ at the $t$-th iteration, which is typically as challenging as solving the original problem of minimizing $ g $. 
However, since the proximal point method is a special case of the proximal gradient method, we can derive its main convergence results from the corresponding results on the proximal gradient method. Specifically, given that the smooth part $ f = 0 $ is 0-smooth, we can use any constant stepsize to ensure convergence. Consequently, Theorem~\ref{theorem:prox_conv_ss_cvx} implies the following result.




\begin{theoremHigh}[Proximal Point Method for Convex]\label{theorem:prox_point}
Let $ g : \real^n \to (-\infty, \infty] $ be a proper closed and convex function. Assume that problem
$$
\min_{\bx \in \real^n} g(\bx)
$$
has a nonempty optimal set. Let $\bx^*$ be an optimal point of $g$, and let $ \{ \bx^\toptzero \}_{t>0} $ be the sequence generated by the proximal point method (Algorithm~\ref{alg:prox_point_gen}) with parameter $ \eta > 0 $. Then,
\begin{enumerate}[(a)]
\item $ g(\bx^\toptzero) - g(\bx^*) \leq \frac{\normtwo{\bx^\topone - \bx^*}^2}{2\eta t} $  for $ t>0 $;
\item The sequence $ \{ \bx^\toptzero \}_{t>0} $ converges to some optimal point $\bx^*$ of $g$.
\end{enumerate}
\end{theoremHigh}





%\begin{algorithm}[h] 
%\caption{Fast Proximal Gradient Descent (FISTA) Method}
%\label{alg:fixta}
%\begin{algorithmic}[1] 
%\Require A function $f(\bx)$ and a closed convex function $g$ (usually non-smooth) satisfying (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}; 
%\State {\bfseries Input:}  Initialize $\bx^{(1)}$ and $\gamma_1=1$;
%\For{$t=1,2,\ldots$}
%\State Pick a stepsize $\eta_t$;
%\State $\by^{(t+1)} \leftarrow \bx^{(t)} - \eta_t \nabla f(\bx^{(t)})$;
%\State $\bz^{(t+1)} \leftarrow \prox_{\eta_t g}(\by^{(t+1)}) \triangleq \mathcalT_{L_t}^{f,g}(\bx^\toptzero)$;
%\State Set  $\gamma_{t+1} = \frac{1 +\sqrt{1+4\gamma_t^2}}{2}$
%\State $\bx^{(t+1)} \leftarrow \bz^{(t+1)} + \frac{\gamma_t - 1}{\gamma_{t+1}} (\bz^{(t+1)} - \bz^{(t)} )$;
%\EndFor
%\State (Output Option 1) Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
%\State (Output Option 2) Output  $\bx_{\text{avg}}\leftarrow \frac{1}{T}(\sum_{t=1}^{t}\bx^{(t)})$ or $\sum_{t=1}^{T} \frac{2t}{T(T+1)} \bx^{(t)}$;
%\State (Output Option 3) Output  $\bx_{\text{best}}\leftarrow \argmin_{t\in\{1,2,\ldots,T\}} f(\bx^{(t)})$;
%\end{algorithmic} 
%\end{algorithm}

\noindent
\begin{minipage}[t]{0.505\linewidth}
\begin{algorithm}[H]
\caption{FISTA-V1}
\label{alg:fistav1}
\begin{algorithmic}[1]
\State $\by^{(1)} = \bx^{(1)} \in \real^n$, $\gamma_1=\frac{1}{\zeta_1}=1$;
\For{$t=1,2,\ldots$}
\State Choose $\eta_{t}$ and $\zeta_t=\frac{1}{\gamma_t}$;
\State $\bx^{(t+1)} \leftarrow \prox_{\eta_{t} g}(\by^{(t)} - \eta_{t} \nabla f(\by^{(t)}))$;
\State $\small\begin{aligned}
&\by^{(t+1)} \leftarrow \bx^{(t+1)} + \frac{\zeta_t-1}{\zeta_{t+1}} (\bx^{(t+1)} - \bx^{(t)});\\
&= \bx^{(t+1)} + (\frac{\gamma_{t+1}}{\gamma_t}-\gamma_{t+1}) (\bx^{(t+1)} - \bx^{(t)});
\end{aligned}$
\EndFor
\State Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\end{algorithmic}
\end{algorithm}
\end{minipage}%
\hfil 
\begin{minipage}[t]{0.495\linewidth}
\begin{algorithm}[H]
\caption{FISTA-V2}
\label{alg:fistav2}
\begin{algorithmic}[1]
\State $\bu^{(1)} =\by^{(1)} = \bx^{(1)} \in \real^n$, $\gamma_1=1$;
\For{$t=1,2,\ldots$}
\State Choose $\eta_{t}$ and $\gamma_t$;
\State $\bx^{(t+1)} \leftarrow \prox_{\eta_{t} g}(\by^{(t)} - \eta_{t} \nabla f(\by^{(t)}))$;
\State $\bu^{(t+1)} \leftarrow \bx^{(t)} + \frac{1}{\gamma_{t}}(\bx^{(t+1)} - \bx^{(t)})$;
\State $\by^{(t+1)} \leftarrow (1 - \gamma_{t+1})\bx^{(t+1)} + \gamma_{t+1} \bu^{(t+1)}$;
\EndFor
\State Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\end{algorithmic}
\end{algorithm}
\end{minipage}
\section{Fast Proximal Gradient Method---FISTA}\label{section:fista}
The \textit{fast proximal gradient} method, also known as the \textit{fast iterative shrinkage-thresholding algorithm (FISTA)} (Algorithm~\ref{alg:fistav2}) is a variant of the proximal gradient method \citep{beck2009fast}. 
Recall that the proximal gradient method achieves a convergence rate of  $\mathcalO\left(\frac{1}{T}\right)$ for a convex and smooth function (Theorem~\ref{theorem:prox_conv_ss_cvx}). Naturally, we aim to accelerate this method, leading us to the introduction of FISTA in this section.

FISTA comprises two steps: first, it calculates a new point based on the direction from the previous two iterations; second, it performs a proximal gradient iteration at this new point. Specifically,
\begin{subequations}\label{equation:fista_intui}
\begin{align}
\by^{(t)}   &\leftarrow \bx^{(t)} + \frac{t-2}{t+1}(\bx^{(t)} - \bx^{(t-1)});\\
\bx^{(t+1)} &\leftarrow \prox_{\eta_t g}(\by^{(t)} - \eta_t \nabla f(\by^{(t)})).
\end{align}
\end{subequations}
The key difference from the standard proximal gradient method is the inclusion of a  \textit{momentum term}, $\frac{t-2}{t+1}(\bx^{(t)} - \bx^{(t-1)})$. 
This momentum term accelerates convergence by leveraging the difference between the current and previous iterates. 
Importantly, this approach has minimal impact on the computational cost per iteration but significantly enhances performance. 
If $\eta_t$ is chosen as a fixed stepsize  less than or equal to $\frac{1}{\beta}$ ($f$ is assumed to be $\beta$-smooth), the convergence rate improves to $\mathcalO\left(\frac{1}{T^2}\right)$. We will detail the derivation in the convergence analysis section.
With this intuition, the complete FISTA algorithm is shown in Algorithm~\ref{alg:fistav1}, where we let $\zeta_t=\frac{1}{\gamma_t}$ for all $t>0$. When $\frac{1}{\gamma_t}=\zeta_t \triangleq \frac{t+1}{2}$, Algorithm~\ref{alg:fistav1} is equivalent to the update in \eqref{equation:fista_intui}.


To facilitate convergence analysis, an equivalent formulation of the FISTA algorithm can be presented. This reformulation splits the second step of the original algorithm into two separate operations, as shown in Algorithm~\ref{alg:fistav2}. While Algorithm~\ref{alg:fistav1} highlights the momentum term explicitly, making it more explanatory, Algorithm~\ref{alg:fistav2} simplifies the convergence analysis (see Theorem~\ref{theorem:fista_conv_ssf}). 

%For better promotion of the convergence analysis, an equivalent statement of the FISTA algorithm can be given, which only breaks the first step of the original algorithm into two iterations, see Algorithm~\ref{alg:fistav2}. 
%Algorithm~\ref{alg:fistav1} is explanatory since it shows the momentum term explicitly; while Algorithm~\ref{alg:fistav2} is straightforward for the convergence analysis (see Theorem~\ref{theorem:fista_conv_ssf}).


%\begin{algorithm}[H]
%\caption{FISTA-V1}
%\label{alg:fistav0}
%\begin{algorithmic}[1]
%\State $\bx^{(1)} = \bx^{(0)} \in \real^n$
%\For{$t=1,2,\ldots$}
%\State $\by^{(t+1)} \leftarrow \bx^{(t)} + \frac{t-2}{t+1}(\bx^{(t)} - \bx^{(t-1)})$.
%\State Choose $\eta_{t+1} \in (0, \frac{1}{L}]$;
%\State $\bz^{(t+1)} \leftarrow \by^{(t+1)} - \eta_{t+1} \nabla f(\by^{(t+1)})$;
%\State $\bx^{(t+1)} \leftarrow \prox_{\eta_{t+1} g}(\bz^{(t+1)})$;
%\EndFor
%\end{algorithmic}
%\end{algorithm}
%\begin{algorithm}[H]
%\caption{FISTA-V2}
%\label{alg:fistav2}
%\begin{algorithmic}[1]
%	\State $\bu^{(1)} = \bx^{(1)} \in \real^n$
%	\For{$t=1,2,\ldots$}
%	\State $\by^{(t+1)} \leftarrow (1 - \gamma_{t+1})\bx^{(t)} + \gamma_{t+1} \bu^{(t)}$.
%	\State Choose $\eta_{t+1}$;
%	\State $\bz^{(t+1)} \leftarrow \by^{(t+1)} - \eta_{t+1} \nabla f(\by^{(t+1)})$
%	\State $\bx^{(t+1)} \leftarrow \prox_{\eta_{t+1} g}(\bz^{(t+1)})$.
%	\State $\bu^{(t+1)} \leftarrow \bx^{(t)} + \frac{1}{\gamma_{t+1}}(\bx^{(t+1)} - \bx^{(t)})$.
%	\EndFor
%\end{algorithmic}
%\end{algorithm}
%\begin{algorithm}[H]
%\caption{FISTA-V2}
%\label{alg:fistav2}
%\begin{algorithmic}[1]
%	\State $\bu^{(1)} = \bx^{(1)} \in \real^n$
%	\For{$t=1,2,\ldots$}
%	\State $\by^{(t+1)} \leftarrow (1 - \gamma_{t+1})\bx^{(t)} + \gamma_{t+1} \bu^{(t)}$.
%	\State Choose $\eta_{t+1}$;
%	\State $\bz^{(t+1)} \leftarrow \by^{(t+1)} - \eta_{t+1} \nabla f(\by^{(t+1)})$
%	\State $\bx^{(t+1)} \leftarrow \prox_{\eta_{t+1} g}(\bz^{(t+1)})$.
%	\State $\bu^{(t+1)} \leftarrow \bx^{(t)} + \frac{1}{\gamma_{t+1}}(\bx^{(t+1)} - \bx^{(t)})$.
%	\EndFor
%\end{algorithmic}
%\end{algorithm}

\paragrapharrow{Convergence requirement.}
For this algorithm framework, we need to determine how to choose the stepsizes $\eta_t$ and $\gamma_t = \frac{1}{\zeta_t}$, which influence the convergence rate of the algorithm. 
First, we outline the conditions under which Algorithm~\ref{alg:fistav2} achieves a convergence rate of $\mathcalO\left(\frac{1}{T^2}\right)$ (the detailed proof will follow):
\begin{subequations}\label{equation:fista_require}
\small
\begin{align}
&f(\bx^\toptzero) \leq f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \bx^\toptzero - \by^\toptzero} + \frac{1}{2\eta_t} \normtwo{\bx^\toptzero - \by^\toptzero}^2, \text{ i.e., $\eta_t \leq \frac{1}{\beta}$};  \label{equation:fista_require1}\\
&\gamma_1 = 1, \quad \frac{(1 - \gamma_{t+1})\eta_{t+1}}{\gamma_{t+1}^2} \leq \frac{\eta_{t}}{\gamma_{t}^2}, \quad t > 1; \label{equation:fista_require2}\\
&\frac{\gamma_t^2}{\eta_t} = \mathcalO\left(\frac{1}{t^2}\right). \label{equation:fista_require3}
\end{align}
\end{subequations}
\paragrapharrow{Choice of $\gamma_t = \frac{1}{\zeta_t}$.}
It can be observed  that when $\eta_t = \frac{1}{\beta}$ and $\frac{1}{\zeta_t}=\gamma_t = \frac{2}{t+1}$ for $t>0$, the above conditions are satisfied. Moreover, the choice of $\gamma_t$ is not unique; for example, we can take
$$
\small
\begin{aligned}
\gamma_1 = 1, \quad \frac{1}{\gamma_{t+1}} = \frac{1}{2} \left( 1 + \sqrt{1 + \frac{4}{\gamma_{t}^2}} \right)
\iff 
\zeta_{t+1} = \frac{1 + \sqrt{1 + 4\zeta_t^2}}{2},
\quad t>0.
\end{aligned}
$$
to obtain the sequence $\{\gamma_t\} = \{\frac{1}{\zeta_t}\}$, and the derived algorithm still converges at a rate of $\mathcalO\left(\frac{1}{T^2}\right)$.
Using the following lemma, it can also be shown that the above choice of $\{\gamma_t\} = \{\frac{1}{\zeta_t}\}$ also satisfies the requirements in specified  \eqref{equation:fista_require}.

\begin{lemma}\label{lemma:seq_gamma_zeta}
Let $\{\frac{1}{\gamma_t}\}_{t>0}=\{\zeta_t\}_{t> 0}$ be the sequence (see Algorithm~\ref{alg:fistav1}) defined by
$ \zeta_1 = 1, \; \zeta_{t+1} = \frac{1 + \sqrt{1 + 4\zeta_t^2}}{2},  t \geq 0. $
Then, $\zeta_t \geq \frac{t+1}{2}$ for all $t > 0$.
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:seq_gamma_zeta}]
The proof is by induction. Obviously, for $t = 1$, $\zeta_1 = 1 \geq \frac{1+1}{2}$. Suppose that the claim holds for $t$ such that $\zeta_t \geq \frac{t+1}{2}$. We will prove that $\zeta_{t+1} \geq \frac{t+2}{2}$. By the recursive relation defining the sequence and the induction assumption,
$ \zeta_{t+1} = \frac{1 + \sqrt{1 + 4\zeta_t^2}}{2} \geq \frac{1 + \sqrt{1 + (t+1)^2}}{2} \geq \frac{1 + \sqrt{(t+1)^2}}{2} = \frac{t+2}{2}$, which completes the proof.
\end{proof}

\paragrapharrow{Monotone issue.}
The original FISTA algorithm is not a descent algorithm. Here, we present a descent variant of FISTA that only requires modifying a single line of Algorithm~\ref{alg:fistav1}: $\bx^\toptone \in \arg\min\{F(\bx) : \bx = \bx^\toptzero, \bz^\toptzero\}$ to ensure the condition $F(\bx^\toptone) \leq \min\{F(\bz^\toptzero), F(\bx^\toptzero)\}$ (Algorithm~\ref{alg:fistav_monotone}). After calculating the proximal operator, instead of immediately updating the iterate point, we check if the function value has decreased. Only if there is a reduction in the function value do we proceed to update the iterate point.
More compactly,  Steps 6 to 10 in Algorithm~\ref{alg:fistav_monotone} can be equivalently stated as
$$
\by^\toptone = \bx^\toptone + \frac{\zeta_t}{\zeta_{t+1}} (\bz^\toptzero - \bx^\toptone) + \frac{\zeta_t - 1}{\zeta_{t+1}} (\bx^\toptone - \bx^\toptzero).
$$


\noindent
\begin{minipage}[h]{1\linewidth}
\begin{algorithm}[H]
\caption{Monotine FISTA \citep{beck2009fastgradient}, Compare to Algorithm~\ref{alg:fistav1}}
\label{alg:fistav_monotone}
\begin{algorithmic}[1]
\Require A function $f(\bx)$ and a closed convex function $g$ (usually non-smooth) satisfying (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}; 
\State {\bfseries Input:} Initialize $\by^{(1)} = \bx^{(1)} \in \real^n$, $\gamma_1=\frac{1}{\zeta_1}=1$;
\For{$t=1,2,\ldots$}
\State Choose $\eta_{t}$ and $\zeta_t=\frac{1}{\gamma_t}$;
\State $\bz^\toptzero \leftarrow \prox_{\eta_{t} g}(\by^{(t)} - \eta_{t} \nabla f(\by^{(t)}))$;
\State choose $\bx^\toptone$ such that $F(\bx^\toptone) \leq \min\{F(\bz^\toptzero), F(\bx^\toptzero)\}$;
\If{$\bx^\toptone$ is $\bz^\toptzero$}
\State $\small\begin{aligned}
\by^{(t+1)} \leftarrow \bx^{(t+1)} + \frac{\zeta_t-1}{\zeta_{t+1}} (\bz^{(t)} - \bx^{(t)})
\end{aligned}$ \Comment{Same as FISTA}
\ElsIf{$\bx^\toptone$ is $\bx^\toptzero$}
\State $\small\begin{aligned}
\by^{(t+1)} \leftarrow \bx^\toptzero + \frac{\zeta_t}{\zeta_{t+1}} (\bz^\toptzero - \bx^\toptzero);
\end{aligned}$
\EndIf
\EndFor
\State {\bfseries Return:}   $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\end{algorithmic}
\end{algorithm}
\end{minipage}%
\hfil 
\begin{minipage}[t]{1\linewidth}
\begin{algorithm}[H]
\caption{Line Search Algorithm at $t$-th Iteration}
\label{alg:fistav_line}
\begin{algorithmic}[1]
\State {\bfseries Input:}  $\eta_t = \eta_{t-1} > 0$, $\rho < 1$. Reference point $\by^\toptzero$ and its gradient $\nabla f(\by^\toptzero)$;
\State Calculate candidate update $\bx^\toptone \leftarrow \prox_{\eta_t g}(\by^\toptzero - \eta_t \nabla f(\by^\toptzero))$;
\While {condition \eqref{equation:fista_require1} is not satisfied for $\bx^\toptone, \by^\toptzero$}
\State Reducing stepsize $\eta_t \gets \rho \eta_t$;
\State Recalculate $\bx^\toptone \leftarrow \prox_{\eta_t g}(\by^\toptzero - \eta_t \nabla f(\by^\toptzero))$;
\EndWhile
\State {\bfseries Return:}  update $\bx^\toptone$, stepsize $\eta_t$;
\end{algorithmic}
\end{algorithm}
\end{minipage}



\paragrapharrow{Line search methods.}
In Algorithms~\ref{alg:fistav1} and \ref{alg:fistav2}, the stepsize must satisfy $\eta_t \leq \frac{1}{\beta}$, under which the condition \eqref{equation:fista_require1} is satisfied. However, for most problems, we do not know the Lipschitz constant of the function $\nabla f$. In this case, condition \eqref{equation:fista_require1} can still be satisfied by employing a line search to determine an appropriate $\eta_t$, while choosing $\gamma_t$ such that conditions \eqref{equation:fista_require2} and \eqref{equation:fista_require3} are simultaneously satisfied, making the algorithm achieve a convergence rate of $\mathcalO\left(\frac{1}{T^2}\right)$.
The line search Algorithm~\ref{alg:fistav_line} is one such method used to determine the stepsize $\eta_t$ dynamically. 
For each iteration $t$, the initial stepsize $\eta_t$ is taken as the previous stepsize $\eta_{t-1}$. 
By progressively  reducing the stepsize $\eta_t$, condition \eqref{equation:fista_require1} is satisfied. Note that when $\eta_t$ is sufficiently small, this condition will always be met, thus preventing the line search from failing to terminate. 
It is straightforward  to verify that the other two conditions \eqref{equation:fista_require2} and \eqref{equation:fista_require3} are also satisfied during the iterative  process.





The following theorem provides the convergence rate of the FISTA algorithm with a fixed stepsize. The convergence with a dynamic stepsize using line search algorithms can be proved analogously and we will not repeat the details.
\begin{theoremHigh}[FISTA for  Convex and SS $f$, $\mathcalO(1/T^2)$:  Compare to Theorem~\ref{theorem:prox_conv_ss_cvx}]\label{theorem:fista_conv_ssf}
Let $ f $ be a proper \textcolor{black}{closed} and $\beta$-smooth function  and  $ g $ be a proper closed and convex function satisfying assumption (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}. 
Additionally, assume  that $f$ is \textbf{convex}. 
Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the FISTA method (Algorithm~\ref{alg:fistav2}) for solving problem (P3)  with a constant stepsize rule in which $\eta_t \triangleq \frac{1}{\beta}$ for all $ t > 0 $. Then, for any optimizer $ \bx^*$ of $F\triangleq f+g$ and $ T > 1 $, it follows that
$$
F(\bx^{(T)}) - F(\bx^*) \leq \frac{2\beta}{T^2} \normtwo{\bx^{(1)} - \bx^*}^2.~\footnote{If the Lipschitz constant $\beta$ is not known beforehand, line search Algorithm~\ref{alg:fistav_line} can be applied to achieve a same rate of convergence.}
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:fista_conv_ssf}]
Since $\bx^\toptone = \prox_{\eta_t g}(\by^\toptzero - \eta_t \nabla f(\by^\toptzero))$, by the Proximal Property-I (Lemma~\ref{lemma:prox_prop1}), we have
$$
-\bx^\toptone + \by^\toptzero - \eta_t \nabla f(\by^\toptzero) \in \eta_t \partial g(\bx^\toptone).
$$
By the subgradient inequality (Definition~\ref{definition:subgrad}), for any $\bx\in\real^n$, we have
$$
\eta_t g(\bx) \geq \eta_t g(\bx^\toptone) + \innerproduct{-\bx^\toptone + \by^\toptzero - \eta_t \nabla f(\by^\toptzero), \bx - \bx^\toptone}.
$$
By the $\beta$-smoothness of $f$ (Definition~\ref{definition:scss_func}) and $\eta_t = \frac{1}{\beta}$, we obtain
$$
f(\bx^\toptone) \leq f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \bx^\toptone - \by^\toptzero} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptone - \by^\toptzero}^2.
$$
Combining the above two inequalities, for any $\bx$, we have
\begin{equation}\label{equation:fista_conv_ssf1}
\small
\begin{aligned}
&F(\bx^\toptone) = f(\bx^\toptone) + g(\bx^\toptone)\\
&\leq g(\bx) + f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \bx - \by^\toptzero} + \frac{1}{\eta_t} \innerproduct{\bx^\toptone - \by^\toptzero, \bx - \bx^\toptone} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptone - \by^\toptzero}^2\\
&\stackrel{\dag}{\leq} g(\bx) + f(\bx) + \frac{1}{\eta_t} \innerproduct{\bx^\toptone - \by^\toptzero, \bx - \bx^\toptone} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptone - \by^\toptzero}^2\\
&= F(\bx) + \frac{1}{\eta_t} \innerproduct{\bx^\toptone - \by^\toptzero, \bx - \bx^\toptone} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptone - \by^\toptzero}^2,
\end{aligned}
\end{equation}
where the inequality ($\dag$) follows from the convexity of $f$.
Invoking \eqref{equation:fista_conv_ssf1} with $\bx \triangleq \bx^{(t)}$ and $\bx \triangleq \bx^*$, for which we multiply by $1 - \gamma_t$ and $\gamma_t$, respectively, and add them up to get
\begin{equation}\label{equation:fista_conv_ssf2}
\small
\begin{aligned}
&F(\bx^\toptone) - F(\bx^*) - (1 - \gamma_t)\big(F(\bx^{(t)}) - F(\bx^*)\big)\\
&\leq \frac{1}{\eta_t} \innerproduct{\bx^\toptone - \by^\toptzero, (1 - \gamma_t)\bx^{(t)} + \gamma_t \bx^* - \bx^\toptone} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptone - \by^\toptzero}^2\\
&\stackrel{\dag}{=} \frac{1}{2\eta_t} \left\{\normtwo{\by^\toptzero - (1 - \gamma_t)\bx^{(t)} - \gamma_t \bx^*}^2 - \normtwo{\bx^\toptone - (1 - \gamma_t)\bx^{(t)} - \gamma_t \bx^*}^2\right\}\\
&= \frac{\gamma_t^2}{2\eta_t} \left\{\normtwo{\bu^{(t)} - \bx^*}^2 - \normtwo{\bu^{(t+1)} - \bx^*}^2\right\},
\end{aligned}
\end{equation}
where the equality ($\dag$) follows from the fact that $2\innerproduct{\ba-\bb,\bc-\ba} +\normtwo{\ba-\bb}^2 = \normtwo{\bb-\bc}^2-\normtwo{\ba-\bc}^2$ for any $\ba,\bb,\bc\in\real^n$, and the last equality follows from the update rules:
$$
\begin{aligned}
\bu^{(t+1)} &= \bx^{(t)} + \frac{1}{\gamma_{t}}(\bx^{(t+1)} - \bx^{(t)})
\qquad\text{and}\qquad
\by^{(t)} = (1 - \gamma_t)\bx^{(t)} + \gamma_t \bu^{(t)}.
\end{aligned}
$$
Recalling that $\eta_t$ and $\gamma_t$ are chosen such that
$
\frac{1 - \gamma_t}{\gamma_t^2} \eta_t \leq \frac{1}{\gamma_{t-1}^2} \eta_{t-1},
$
we obtain an iterate inequality for consecutive iterations
$$
\small
\begin{aligned}
&\frac{\eta_t}{\gamma_t^2} \big(F(\bx^\toptone) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\bu^{(t+1)} - \bx^*}^2 
\leq (1-\gamma_t)\frac{\eta_t}{\gamma_t^2} \big(F(\bx^{(t)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\bu^{(t)} - \bx^*}^2\\
&\leq \frac{\eta_{t-1}}{\gamma_{t-1}^2} \big(F(\bx^{(t)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\bu^{(t)} - \bx^*}^2
\leq \frac{\eta_1}{\gamma_1^2} \big(F(\bx^{(2)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\bu^{(2)} - \bx^*}^2\\
&\leq \frac{(1 - \gamma_1)\eta_1}{\gamma_1^2} \big(F(\bx^{(1)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\bu^{(1)} - \bx^*}^2
=\frac{1}{2} \normtwo{\bx^{(1)} - \bx^*}^2,
\end{aligned}
$$
where the last equality follows from the fact that  $\gamma_1 = 1$ and $\bu^{(1)} = \bx^{(1)}$.
Using Lemma~\ref{lemma:seq_gamma_zeta} concludes the result.
\end{proof}
%\begin{proof}[of Theorem~\ref{theorem:fista_conv_ssf}]
%Since $\bx^\toptzero = \prox_{\eta_t g}(\by^\toptzero - \eta_t \nabla f(\by^\toptzero))$, by the Proximal Property-I (Lemma~\ref{lemma:prox_prop1}), we have
%$$
%-\bx^\toptzero + \by^\toptzero - \eta_t \nabla f(\by^\toptzero) \in \eta_t \partial g(\bx^\toptzero).
%$$
%By the subgradient inequality (Definition~\ref{definition:subgrad}), for any $\bx\in\real^n$, we have
%$$
%\eta_t g(\bx) \geq \eta_t g(\bx^\toptzero) + \innerproduct{-\bx^\toptzero + \by^\toptzero - \eta_t \nabla f(\by^\toptzero), \bx - \bx^\toptzero}.
%$$
%By the $\beta$-smoothness of $f$ and $\eta_t = \frac{1}{\beta}$, we obtain
%$$
%f(\bx^\toptzero) \leq f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \bx^\toptzero - \by^\toptzero} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptzero - \by^\toptzero}^2.
%$$
%Combining the above two inequalities, for any $\bx$, we have
%\begin{equation}\label{equation:fista_conv_ssf1}
%	\small
%\begin{aligned}
%	&F(\bx^\toptzero) = f(\bx^\toptzero) + g(\bx^\toptzero)\\
%	&\leq g(\bx) + f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \bx - \by^\toptzero} + \frac{1}{\eta_t} \innerproduct{\bx^\toptzero - \by^\toptzero, \bx - \bx^\toptzero} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptzero - \by^\toptzero}^2\\
%	&\stackrel{\dag}{\leq} g(\bx) + f(\bx) + \frac{1}{\eta_t} \innerproduct{\bx^\toptzero - \by^\toptzero, \bx - \bx^\toptzero} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptzero - \by^\toptzero}^2\\
%	&= F(\bx) + \frac{1}{\eta_t} \innerproduct{\bx^\toptzero - \by^\toptzero, \bx - \bx^\toptzero} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptzero - \by^\toptzero}^2,
%\end{aligned}
%\end{equation}
%where the inequality ($\dag$) follows from the convexity of $f$.
%Invoking \eqref{equation:fista_conv_ssf1} with $\bx \triangleq \bx^{(t-1)}$ and $\bx \triangleq \bx^*$, for which we multiply by $1 - \gamma_t$ and $\gamma_t$, respectively, and add them up to get
%\begin{equation}\label{equation:fista_conv_ssf2}
%\begin{aligned}
%&F(\bx^\toptzero) - F(\bx^*) - (1 - \gamma_t)\big(F(\bx^{(t-1)}) - F(\bx^*)\big)\\
%&\leq \frac{1}{\eta_t} \innerproduct{\bx^\toptzero - \by^\toptzero, (1 - \gamma_t)\bx^{(t-1)} + \gamma_t \bx^* - \bx^\toptzero} + \frac{1}{2\eta_t} \normtwobig{\bx^\toptzero - \by^\toptzero}^2\\
%&\stackrel{\dag}{=} \frac{1}{2\eta_t} \left\{\normtwo{\by^\toptzero - (1 - \gamma_t)\bx^{(t-1)} - \gamma_t \bx^*}^2 - \normtwo{\bx^\toptzero - (1 - \gamma_t)\bx^{(t-1)} - \gamma_t \bx^*}^2\right\}\\
%&= \frac{\gamma_t^2}{2\eta_t} \left\{\normtwo{\bu^{(t-1)} - \bx^*}^2 - \normtwo{\bu^{(t)} - \bx^*}^2\right\},
%\end{aligned}
%\end{equation}
%where the equality ($\dag$) follows from the fact that $2\innerproduct{\ba-\bb,\bc-\ba} +\normtwo{\ba-\bb}^2 = \normtwo{\bb-\bc}^2-\normtwo{\ba-\bc}^2$ for any $\ba,\bb,\bc\in\real^n$, and the last equality follows from the update rules:
%$$
%\begin{aligned}
%\bu^{(t)} &= \bx^{(t-1)} + \frac{1}{\gamma_t}(\bx^\toptzero - \bx^{(t-1)});\\
%\by^{(t)} &= (1 - \gamma_t)\bx^{(t-1)} + \gamma_t \bu^{(t-1)}.
%\end{aligned}
%%\implies
%%\by^{(t)}=
%%\frac{\gamma_{t-1}+ \gamma_t - \gamma_t \gamma_{t-1}}{\gamma_{t-1}} \bx^{(t-1)}
%%+
%%\frac{ \gamma_t\gamma_{t-1} - \gamma_t }{\gamma_{t-1}}\bx^{(t-2)}.
%$$
%Recalling that $\eta_t, \gamma_t$ are chosen such that
%$$
%\frac{1 - \gamma_t}{\gamma_t^2} \eta_t \leq \frac{1}{\gamma_{t-1}^2} \eta_{t-1},
%$$
%we obtain a relevant inequality for adjacent iterations
%$$
%\begin{aligned}
%\frac{\eta_t}{\gamma_t^2} \big(F(\bx^\toptzero) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\bu^{(t)} - \bx^*}^2 
%&\leq \frac{\eta_{t-1}}{\gamma_{t-1}^2} \big(F(\bx^{(t-1)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\bu^{(t-1)} - \bx^*}^2\\
%&\leq \frac{\eta_2}{\gamma_2^2} \big(F(\bx^{(2)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\bu^{(2)} - \bx^*}^2\\
%&\stackrel{\dag}{\leq} \frac{(1 - \gamma_2)\eta_2}{\gamma_2^2} \big(F(\bx^{(1)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\bu^{(1)} - \bx^*}^2\\
%&=\frac{1}{2} \normtwo{\bx^{(1)} - \bx^*}^2
%\end{aligned}
%$$
%\end{proof}


\noindent
\begin{minipage}[t]{0.495\linewidth}
\begin{algorithm}[H]
\caption{FISTA-V2, Same as Algo~\ref{alg:fistav2}}
\label{alg:fistav2_comp}
\begin{algorithmic}[1]
	\State $\bu^{(1)} =\by^{(1)} = \bx^{(1)} \in \real^n$, $\gamma_1=1$;
	\For{$t=1,2,\ldots$}
	\State Choose $\eta_{t}$ and $\gamma_t$;
	\State $\bx^{(t+1)} \leftarrow \prox_{\eta_{t} g}(\by^{(t)} - \eta_{t} \nabla f(\by^{(t)}))$;
	\State $\bu^{(t+1)} \leftarrow \bx^{(t)} + \frac{1}{\gamma_{t}}(\bx^{(t+1)} - \bx^{(t)})$;
	\State $\by^{(t+1)} \leftarrow (1 - \gamma_{t+1})\bx^{(t+1)} + \gamma_{t+1} \bu^{(t+1)}$;
	\EndFor
	\State Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\end{algorithmic}
\end{algorithm}
\end{minipage}%
\hfil 
\begin{minipage}[t]{0.505\linewidth}
\begin{algorithm}[H]
\caption{Nesterov Accelerated Method}
\label{alg:nesterov}
\begin{algorithmic}[1]
\State $\bx^{(1)} =\by^{(1)} = \widetildebx^{(1)} \in \real^n$, $\gamma_1=1$;
\For{$t=1,2,\ldots$}
\State Choose $\eta_{t}$ and $\gamma_{t+1}$;
\State  $\widetildebx^\toptone \leftarrow \prox_{\frac{\eta_t}{\gamma_t}g}\left(\widetildebx^{(t)} - \frac{\eta_t}{\gamma_t} \nabla f(\by^{(t)})\right)$.
\State  $\bx^\toptone \leftarrow (1 - \gamma_t)\bx^{(t)} + \gamma_t \widetildebx^\toptone$.
\State  $\by^\toptone \leftarrow (1 - \gamma_{t+1})\bx^{(t+1)} + \gamma_{t+1} \widetildebx^{(t+1)}$.
\EndFor
\State Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
\end{algorithmic}
\end{algorithm}
\end{minipage}




\section{Nesterov Accelerated Method}

This section will introduce  another acceleration algorithm besides the FISTA algorithm, which is an extension of Nesterov's algorithms \citep{nesterov1988approach, nesterov2005smooth}. 
The Nesterov accelerated method (Algorithm~\ref{alg:nesterov}) differs from the FISTA algorithm in that the sequences $\{\widetildebx^\toptzero\}$, $\{\by^\toptzero\}$, and $\{\bx^\toptzero\}$ are guaranteed to remain within the domain of definition (since all the updates are convex combinations of old iterates, which bears a resemblance to the conditional gradient method in Section~\ref{section:cond_gd}).
In contrast, the sequence $\{\by^\toptzero\}$ in the FISTA algorithm (Algorithm~\ref{alg:fistav2_comp}) does not necessarily stay within the domain.
Note that  the FISTA method modifies both the base iterate and the descent direction in the proximal operation; while the Nesterov accelerated method only updates the descent direction: 
$$
\bx^{(t+1)} \leftarrow \prox_{\eta_{t} g}(\by^{(t)} - \eta_{t} \nabla f(\by^{(t)}))
\qquad\leadsto \qquad
\widetildebx^\toptone \leftarrow \prox_{\frac{\eta_t}{\gamma_t}g}\left(\widetildebx^{(t)} - \frac{\eta_t}{\gamma_t} \nabla f(\by^{(t)})\right).
$$
Same as the FISTA method,  the Nesterov accelerated method uses $\gamma_t = \frac{2}{t+1}$ (or the recursive sequence $\{\gamma_t\}$ from Lemma~\ref{lemma:seq_gamma_zeta}) and 
$\eta_t = \frac{1}{\beta}$ to achieve a convergence rate of $\mathcalO\left(\frac{1}{T^2}\right)$.
The convergence analysis of the  Nesterov accelerated method can employ techniques analogous to those used for the FISTA method.

\begin{theoremHigh}[Nesterov for  Convex and SS $f$, Compare to Theorem~\ref{theorem:fista_conv_ssf}]\label{theorem:nesterov_acc}
Let $ f $ be a proper \textcolor{black}{closed} and $\beta$-smooth function  and  $ g $ be a proper closed and convex function satisfying assumption (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}. 
Additionally, assume that $f$ is \textbf{convex}. 
Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the Nesterov accelerated method (Algorithm~\ref{alg:nesterov}) for solving problem (P3)  with a constant stepsize rule in which $\eta_t \triangleq \frac{1}{\beta}$ and $\gamma_t = \frac{2}{t+1}$ (or the recursive sequence $\{\gamma_t\}$ from Lemma~\ref{lemma:seq_gamma_zeta}) for all $ t > 0 $. Then, for any optimizer $ \bx^*$ of $F\triangleq f+g$ and $ T > 1 $, it follows that
$$
F(\bx^\toptzero) - F(\bx^*) \leq \frac{2\beta}{T^2} \normtwo{\bx^{(1)} - \bx^*}^2.
$$
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:nesterov_acc}]
Since $\widetildebx^\toptone = \prox_{\frac{\eta_t}{\gamma_t}g}\left(\widetildebx^{(t)} - \frac{\eta_t}{\gamma_t} \nabla f(\by^{(t)})\right)$, by the Proximal Property-I (Lemma~\ref{lemma:prox_prop1}), we have
$$
\widetildebx^{(t)} - \frac{\eta_t}{\gamma_t} \nabla f(\by^{(t)}) - \widetildebx^\toptone 
\in \frac{\eta_t}{\gamma_t} \partial g(\widetildebx^\toptone),
$$
By the subgradient inequality (Definition~\ref{definition:subgrad}), for any $\bx\in\real^n$, we have
$$
g(\bx) \geq  g(\widetildebx^\toptone) + 
\frac{\gamma_t}{\eta_t} \innerproduct{\widetildebx^{(t)} - \frac{\eta_t}{\gamma_t} \nabla f(\by^{(t)}) - \widetildebx^\toptone , \bx - \widetildebx^\toptone}.
$$
By the convexity of $g$ and the update rule $\bx^\toptone = (1 - \gamma_t)\bx^{(t)} + \gamma_t \widetildebx^\toptone$, 
$$
g(\bx^\toptone) \leq (1 - \gamma_t) g(\bx^{(t)} ) + \gamma_t g(\widetildebx^\toptone).
$$
Substituting  the expression for $g(\widetildebx^\toptone)$ into the preceding inequality obtains
$$
g(\bx^\toptone) \leq (1 - \gamma_t) g(\bx^{(t)})
+ \gamma_t \left[ g(\bx) - \innerproduct{\frac{\gamma_t}{\eta_t} (\widetildebx^{(t)} - \widetildebx^{(t+1)}) - \nabla f(\by^\toptzero), \bx - \widetildebx^{(t+1)}} \right].
$$
Given that $\eta_t = \frac{1}{\beta}$, by the $\beta$-smoothness of $f$ (Definition~\ref{definition:scss_func}), we obtain
$$
f(\bx^\toptone) \leq 
f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \bx^\toptone - \by^\toptzero} + \frac{1}{2\eta_t} \normtwo{\bx^\toptone - \by^\toptzero}^2.
$$
Subtracting the two updare rules:
$\bx^\toptone = (1 - \gamma_t)\bx^{(t)} + \gamma_t \widetildebx^\toptone$ and
$\by^\toptzero = (1 - \gamma_t)\bx^{(t)} + \gamma_t \widetildebx^{(t)}$, 
we have 
$\bx^{(t+1)} - \by^\toptzero = \gamma_t (\widetildebx^{(t+1)} - \widetildebx^{(t)})$. 
Substituting this equality into the above inequality, using the update rule for $\bx^\toptone$ again, and letting $\be^{(t)} \triangleq \widetildebx^{(t+1)} - \widetildebx^{(t)}$, 
we get
$$
\footnotesize
\begin{aligned}
&f(\bx^\toptone) \leq 
f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), (1 - \gamma_t)\bx^{(t)} + \gamma_t \widetildebx^\toptone - \by^\toptzero} + \frac{\gamma_t^2}{2\eta_t} \normtwobig{\be^{(t)}}^2\\
&= (1 - \gamma_t) \Big\{f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \bx^{(t)} - \by^\toptzero}\Big\}
+ \gamma_t \Big\{f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \widetildebx^\toptone - \by^\toptzero }\Big\} 
+ \frac{\gamma_t^2}{2\eta_t} \normtwobig{\be^{(t)}}^2\\
&\leq (1 - \gamma_t) f(\bx^{(t)}) + \gamma_t \left\{f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \widetildebx^\toptone - \by^\toptzero}\right\}
+ \frac{\gamma_t^2}{2\eta_t} \normtwobig{\be^{(t)}}^2,
\end{aligned}
$$
where the last inequality follows from the convexity of $f$ (Theorem~\ref{theorem:conv_gradient_ineq}).


Combining the inequalities for $f(\bx^\toptone)$ and $g(\bx^\toptone)$ with $\bx\triangleq\bx^*$, along with the convexity of $f$ such that $
f(\bx^*) \geq f(\by^\toptzero) + \innerproduct{\nabla f(\by^\toptzero), \bx^* - \by^\toptzero}
$, yields that 
$$
\small
\begin{aligned}
&F(\bx^\toptone) - F(\bx^*) - (1 - \gamma_t)\big(F(\bx^{(t)}) - F(\bx^*)\big)\\
&\leq 
\frac{\gamma_t^2}{\eta_t}\innerproduct{\widetildebx^\toptone -\widetildebx^\toptzero, \bx^*-\widetildebx^\toptone}  + \frac{\gamma_t^2}{2\eta_t} \normtwobig{\widetildebx^{(t+1)} - \widetildebx^{(t)}}^2\\
&\stackrel{\dag}{=} \frac{\gamma_t^2}{2\eta_t} \left\{ \normtwo{\widetildebx^\toptzero-\bx^*}^2 - \normtwo{\widetildebx^\toptone-\bx^*}^2 \right\}
\end{aligned}
$$
where the equality ($\dag$) follows from the fact that $2\innerproduct{\ba-\bb,\bc-\ba} +\normtwo{\ba-\bb}^2 = \normtwo{\bb-\bc}^2-\normtwo{\ba-\bc}^2$ for any $\ba,\bb,\bc\in\real^n$.
Recalling that $\eta_t$ and $\gamma_t$ are chosen such that
$
\frac{1 - \gamma_t}{\gamma_t^2} \eta_t \leq \frac{1}{\gamma_{t-1}^2} \eta_{t-1}
$ (same as the FISTA method),
we obtain an iterate inequality for consecutive iterations
$$
\small
\begin{aligned}
	&\frac{\eta_t}{\gamma_t^2} \big(F(\bx^\toptone) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\widetildebx^{(t+1)} - \bx^*}^2 
	\leq (1-\gamma_t)\frac{\eta_t}{\gamma_t^2} \big(F(\bx^{(t)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\widetildebx^{(t)} - \bx^*}^2\\
	&\leq \frac{\eta_{t-1}}{\gamma_{t-1}^2} \big(F(\bx^{(t)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\widetildebx^{(t)} - \bx^*}^2
	\leq \frac{\eta_1}{\gamma_1^2} \big(F(\bx^{(2)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\widetildebx^{(2)} - \bx^*}^2\\
	&\stackrel{\dag}{\leq} \frac{(1 - \gamma_1)\eta_1}{\gamma_1^2} \big(F(\bx^{(1)}) - F(\bx^*)\big) + \frac{1}{2} \normtwo{\widetildebx^{(1)} - \bx^*}^2
	=\frac{1}{2} \normtwo{\bx^{(1)} - \bx^*}^2,
\end{aligned}
$$
where the last equality follows from fact that  $\gamma_1 = 1$ and $\bx^{(1)} = \widetildebx^{(1)}$.
Using Lemma~\ref{lemma:seq_gamma_zeta} concludes the result.
\end{proof}

Similarly, note that the key steps in deriving Theorem~\ref{theorem:nesterov_acc} rely on conditions in  \eqref{equation:fista_require}. Therefore, if the smoothness constant is not known beforehand, the line search Algorithm~\ref{alg:fistav_line} can guarantee the same convergence results.







\begin{algorithm}[h] 
\caption{Conditional Gradient  Method}
\label{alg:cond_gen}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$ and a set $\sS$; 
\State {\bfseries Input:}  Initialize $\bx^{(0)}$;
\For{$t=0,1,2,\ldots$}
\State Pick a stepsize $\eta_t\in[0,1]$;
\State $\widetildebx^{(t)} \leftarrow \mathop{\argmin}_{\bx\in \sS} \innerproduct{\nabla f(\bx^{(t)}), \bx}$; \Comment{linear optimization}
\State  $\bx^{(t+1)} \leftarrow \bx^{(t)} + \eta_t (\widetildebx^{(t)} - \bx^{(t)}) = (1-\eta_t)\bx^{(t)} +  \eta_t \widetildebx^{(t)} $;  \Comment{``convex" update}
\EndFor
\State {\bfseries Return:}   $\bx_{\text{final}} \leftarrow \bx^\toptzero$;
\end{algorithmic} 
\end{algorithm}
\section{Conditional Gradient  (Frank-Wolfe) Method}\label{section:cond_gd}
We investigate  the \textit{conditional gradient method}, commonly referred to as the \textit{Frank-Wolfe (FW) algorithm}. 
This method provides an appealing alternative to constrained optimization problems, particularly by addressing potential computational inefficiencies associated with the projection step in projected gradient descent methods.
Consider the following optimization problem:
\begin{equation}\label{equation:cgd_prob}
\text{(P2)}:\qquad \mathop{\min}_{\bx} f(\bx)\gap \text{s.t.}\gap \bx\in\sS.
\end{equation}
In the conditional gradient Method, the next step is computed as a convex combination of the current iterate and a minimizer of a linearized form of the objective function over the feasible set $\sS$ (Algorithm~\ref{alg:cond_gen}).

Recall that in the PGD method, the update rule from the $t$-th to the $(t+1)$-th iteration can be expressed as follows:
\begin{center}
\framebox{
\begin{minipage}{0.95\textwidth}
\begin{equation}\label{equation:pgd_decom_cg}
\small
\textbf{(PGD)}:\qquad 
\begin{aligned}
\bx^\toptone 
&\leftarrow \mathop{\argmin}_{\bx\in\sS} \normtwo{\bx - \left(\bx^{(t)} - \eta_t \nabla f(\bx^{(t)})\right)}^2\\
&=\mathop{\argmin}_{\bx\in\sS} 
\left\{
\frac{1}{2} \normtwobig{\bx-\bx^\toptzero}^2+ \eta_t\innerproduct{\nabla f(\bx^\toptzero), \bx} 
\right\},
\end{aligned}
\end{equation}
\end{minipage}
}
\end{center}
where the quadratic term acts as a regularization ensuring the update remains within a neighborhood of the current iterate $\bx^\toptzero$.
In contrast, the CG method employs a convex combination of the new update found by linear search  $\mathop{\argmin}_{\bx\in\sS}\innerproduct{\nabla f(\bx^{(t)}), \bx}$ and the current iterate $\bx^\toptzero$,  avoiding the use of quadratic regularization. This sum can also be viewed as a form of regularization, ensuring the new iterate does not stray too far from the current iterate:
\begin{tcolorbox}[colback=white,colframe=black]
\begin{minipage}{1\textwidth}
\begin{equation}\label{equation:cgmethod}
\small
\textbf{(CG)}:\qquad 
\begin{aligned}
\bx^\toptone 
&\leftarrow (1-\eta_t)\bx^{(t)} + \eta_t\mathop{\argmin}_{\bx\in\sS}\innerproduct{\nabla f(\bx^{(t)}), \bx} .
\end{aligned}
\end{equation}
\end{minipage}
\end{tcolorbox}
Similarly, CG can achieve a rate of $\mathcalO(T)$ for convex and smooth functions.
\begin{theoremHigh}[FW for Convex and SS: $\mathcalO(T)$]\label{theorem:fw_under_smoo}
Let $f: \sS \rightarrow \real$ be a proper, differentiable, convex, and $\beta$-smooth function defined over the convex domain $\sS\subseteq\real^n$. 
Suppose that $f$ attains its global minimum at a point $\bx^* \in \sS$, and that $D$ is the diameter of $\sS$, defined as $D = \max_{\bx, \by \in \sS} \normtwo{\bx - \by}$.
Let $ \{\bx^\toptzero\}_{t \geq 0} $~\footnote{For simplicity, note that we suppose the iterate starts by index 0 for the CG method.} be the sequence generated by the Frank-Wolfe method (Algorithm~\ref{alg:cond_gen}) for solving problem (P2)  with a dynamic stepsize rule in which $\eta_t = \frac{2}{t+2}$ at the $t$-th iteration. 
Then, for any $T\geq0$,
$$
f(\bx^{(T)}) - f(\bx^*) \leq \frac{2\beta D^2}{T+2}.
$$
\end{theoremHigh}

\begin{proof}[of Theorem~\ref{theorem:fw_under_smoo}]
By smoothness, convexity, and the progress rule of conditional gradient method, we have
$$
\begin{aligned}
f(\bx^{(t+1)}) 
&\leq f(\bx^{(t)}) + \nabla f(\bx^{(t)})^\top (\bx^{(t+1)} - \bx^{(t)}) + \frac{\beta}{2} \normtwo{ \bx^{(t+1)} - \bx^{(t)}}^2\quad &&(\text{smoothness})\\
&=f(\bx^{(t)}) + \eta_t \nabla f(\bx^{(t)})^\top (\widetildebx^{(t)} - \bx^{(t)}) + \frac{\eta_t^2 \beta}{2} \normtwo{ \widetildebx^{(t)} - \bx^{(t)} }^2\quad &&\text{(update rule)}\\
&\leq f(\bx^{(t)}) + \eta_t \nabla f(\bx^{(t)})^\top (\bx^* - \bx^{(t)}) + \frac{\eta_t^2 \beta D^2}{2}. &&\text{(linear update rule)}
\end{aligned}
$$
Due to convexity, we also have:
$
\nabla f(\bx^{(t)})^\top (\bx^* - \bx^{(t)}) \leq f(\bx^*) - f(\bx^{(t)}).
$
Combining the two inequalities yields
\begin{equation}\label{equation:fw_under_smoo1}
f(\bx^{(t+1)}) - f(\bx^*) \leq (1 - \eta_t)(f(\bx^{(t)}) - f(\bx^*)) + \frac{\eta_t^2 \beta D^2}{2}.
\end{equation}
We use induction in order to prove $f(\bx^{(t)}) - f(\bx^*) \leq \frac{2\beta D^2}{t+2}$ based on the above equation.

\paragraph{Base case $t = 0$.} When $t = 0$, we have $\eta_0 = \frac{2}{2} = 1$, and \eqref{equation:fw_under_smoo1} reduces to
$$
\begin{aligned}
f(\bx^{(1)}) - f(\bx^*) 
&\leq (1 - \eta_0)(f(\bx_0) - f(\bx^*)) + \frac{\beta}{2} \normtwo{\bx^{(1)} -\bx^*}^2 
=  \frac{\beta D^2}{2}
\leq \frac{2\beta D^2}{2}.
\end{aligned}
$$
\paragraph{Inductive step.} Proceeding by induction, we assume that 
$
f(\bx^{(t)}) - f(\bx^*) \leq \frac{2\beta D^2}{t+2}
$
holds for all integers up to $t$ and we show the claim for $t+1$. By \eqref{equation:fw_under_smoo1},
\begin{align*}
f(\bx^{(t+1)}) - f(\bx^*) &\leq \left(1 - \frac{2}{t+2}\right) \left(f(\bx^{(t)}) - f(\bx^*)\right) + \frac{2\beta D^2}{(t+2)^2} \\
&\leq \left(1 - \frac{2}{t+2}\right) \frac{2\beta D^2}{t+2} + \frac{2\beta D^2}{(t+2)^2} 
%= \beta D^2 \left(\frac{2t}{(t+2)^2} + \frac{2}{(t+2)^2}\right) 
= 2\beta D^2 \frac{t+1}{(t+2)^2} \\
&= 2\beta D^2 \frac{t+1}{t+2} \cdot \frac{1}{t+2} 
\leq 2\beta D^2 \frac{t+2}{t+3} \cdot \frac{1}{t+2} 
= 2\beta D^2 \frac{1}{t+3}.
\end{align*}
Thus, the inequality also holds for the $t+1$ case.
\end{proof}

The CG method is useful for understanding the \textit{power method} for computing the eigenpair of a matrix.
\begin{example}[Power Method]
Consider the specific problem 
$$
\mathop{\min}_{\bx} f(\bx)=-\frac{1}{2} \bx^\top\bA\bx,\gap \text{s.t.}\gap \sS=\{\norm{\bx}_2\leq 1\},
$$
where $\bA$ is positive semidefinite. 
Then, $\bt^{(t)} = \argmin_{\bt\in\sS} \innerproduct{\bt, \nabla f(\bx^{(t)})}$ is given by $\frac{\bA\bx^{(t)}}{\norm{\bA\bx^{(t)}}_2}$. The conditional gradient update is:
$$
\bx^{(t+1)} = (1-s_t)\bx^{(t)} + s_t \frac{\bA\bx^{(t)}}{\norm{\bA\bx^{(t)}}_2}.
$$
If we set the stepsizes using an exact line search strategy, then 
$$
s_t =\mathop{\argmin}_{{s\in[0,1]}} f(\bx^{(t)}+s_t(\bt^{(t)} - \bx^{(t)})).
$$
When $\bx^{(t)}$ is not the optimal solution of the problem, and since $f(\bx)$ is concave, we can choose $s_t=1$. Therefore, the update becomes 
$$
\bx^{(t+1)} = \frac{\bA\bx^{(t)}}{\norm{\bA\bx^{(t)}}_2}.
$$
This is equivalent to the power method for finding the eigenvector corresponding to the maximal eigenvalue. The sequence for the eigenvalue is
$$
\lambda^{{(t+1)}} = \bx^{(t+1)\top}\bA\bx^{(t+1)}.
$$
See, for example, \citet{lu2021numerical} for more details.
\end{example}

\begin{algorithm}[h] 
\caption{Generalized Conditional Gradient  Method}
\label{alg:gen_cond_gen}
\begin{algorithmic}[1] 
\Require A function $f(\bx)$ and a convex function $g(\bx)$; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t\in[0,1]$;
\State $\widetildebx^{(t)} \leftarrow \mathop{\argmin}_{\bx\in \textcolor{mylightbluetext}{\real^n}} \innerproduct{\nabla f(\bx^{(t)}), \bx} + g(\bx)$; \Comment{``linear" optimization}
\State  $\bx^{(t+1)} \leftarrow \bx^{(t)} + \eta_t (\widetildebx^{(t)} - \bx^{(t)}) = (1-\eta_t)\bx^{(t)} +  \eta_t \widetildebx^{(t)} $;  \Comment{``convex" update}
\EndFor
\State {\bfseries Return:}   $\bx_{\text{final}} \leftarrow\bx^\toptzero$;
\end{algorithmic} 
\end{algorithm}
\section{Generalized Conditional Gradient Method}

We have shown that the conditional gradient method addresses the same constrained problem, \eqref{equation:cgd_prob}, as the projected gradient descent approach. However, the \textit{generalized conditional gradient (GCG)} method (Algorithm~\ref{alg:gen_cond_gen}) focuses on solving an unconstrained composite problem:
\begin{equation}\label{equation:gene_cgd_prob}
\text{(P3)}:\qquad \min \{F(\bx) \triangleq f(\bx)+g(\bx)\}.
\end{equation}
In this context, we assume that $f$ and $g$ satisfy conditions (B1) and (B2) in Assumption~\ref{assumption:gcg}:
\begin{assumption}[GCG]\label{assumption:gcg}
For problem (P3), we assume
\begin{itemize}
	\item[(B1)] $ g: \real^n \rightarrow (-\infty, \infty] $ is proper, closed, and convex and \textcolor{mylightbluetext}{$\dom(g)$ is compact}.
	\item[(B2)] $ f: \real^n \rightarrow (-\infty, \infty] $ is proper, closed, and  $ \beta $-smooth over $ \dom(f) $ ($ \beta > 0 $), which is assumed to be an open and convex set satisfying $ \dom(g) \subseteq \dom(f) $.
\end{itemize}
\end{assumption}
The rationale behind these assumptions for the GCG method includes:
\begin{itemize}
\item \textit{Compactness of $\dom(g)$.}  (B1) ensures that the subproblem
$
\underset{\by \in \dom(g)}{\arg\min} \nabla f(\bx)^\top \by  + g(\bx)
$
has a solution. The compactness of $ \dom(g) $ ensures boundedness and avoids unbounded solutions in the subproblem.


\item \textit{Open and convex $\dom(f)$.} The openness of $ \dom(f) $ in (B2) allows the linear subproblem to remain well-defined and ensures that iterates stay in the feasible region.

\item \textit{Domain compatibility.}  The inclusion $ \dom(g) \subseteq \dom(f) $ ensures compatibility, i.e., the linear subproblem remains within the domain of $ f $, and $ f(\bx) $ can be evaluated and differentiated.
\end{itemize}
The  generalized conditional gradient method shares similarities with the standard conditional gradient method, but differs by computing a minimizer of the sum of the linearized smooth part of $f$ and the convex function $g$. Since the objective consists of a sum of convex and affine functions, the problem remains relatively straightforward to solve \citep{beck2017first}.


A natural optimality measure in proximal gradient methods is the gradient mapping. However, the analysis of the conditional gradient method relies on a different optimality measure, which we will refer to as the \textit{conditional gradient norm}.

\begin{definition}[Conditional Gradient Norm]\label{definition:cond_grad_norm}
	Suppose that $f$ is $\beta$-smooth and $g$ is \textcolor{black}{closed} convex. Then, the conditional gradient norm is the function $\mathcalW : \dom(f) \rightarrow \real$ defined by
	$$
	\begin{aligned}
		\mathcalW(\bx) &\triangleq \innerproduct{\nabla f(\bx), \bx - \widetildebx} + g(\bx) - g(\widetildebx)\\
		&= \max_{\bv \in \real^n} \{\innerproduct{\nabla f(\bx), \bx - \bv} + g(\bx) - g(\bv)\}\\
		&= \innerproduct{\nabla f(\bx), \bx} + g(\bx) + g^*(-\nabla f(\bx)).
	\end{aligned}
	$$
where $\widetildebx\in\mathop{\argmin}_{\bv\in\real^n} \innerproduct{\nabla f(\bx), \bv} + g(\bv)$, and $g^*$ represents the conjugate function of $g$ (Definition~\ref{definition:conjug_func}).
\end{definition}




\begin{theorem}[Conditional Gradient Norm as an Optimality Measure]\label{theorem:cgn_optmea}
Suppose that $f$ is $\beta$-smooth and $g$ is \textcolor{black}{closed} convex, satisfying assumption (B1) and (B2) in Assumption~\ref{assumption:gcg}. Then,
\begin{enumerate}[(i)]
\item $\mathcalW(\bx) \geq 0$ for any $\bx \in \dom(f)$;
\item $\mathcalW(\bx^*) = 0$ if and only if $- \nabla f(\bx^*) \in \partial g(\bx^*)$, i.e., if and only if $\bx^*$ is a stationary point of problem (P3); see Theorem~\ref{theorem:opt_cond_p3} and Definition~\ref{definition:stat_opt_cond_p3}.
\end{enumerate}
\end{theorem}

\begin{proof}
\textbf{(i).} This follows from the third expression of the conditional gradient norm in Definition~\ref{definition:cond_grad_norm}  and Fenchel's inequality (Theorem~\ref{theorem:fenchel_ineq}).

\paragraph{(ii).} By part (i), it follows that $\mathcalW(\bx^*) = 0$ if and only if $\mathcalW(\bx^*) \leq 0$, which is the same as the relation (using the second expression of conditional gradient norm in Definition~\ref{definition:cond_grad_norm} for $\mathcalW(\bx^*)$)
$$
\begin{aligned}
&\innerproduct{ \nabla f(\bx^*), \bx^* - \bv} + g(\bx^*) - g(\bv) \leq 0 \quad \text{for all } \bv \in \real^n\\
&\qquad\qquad\implies\qquad  g(\bv) \geq g(\bx^*) + \innerproduct{-\nabla f(\bx^*), \bv - \bx^*},
\end{aligned}
$$
which is equivalent to the relation $-\nabla f(\bx^*) \in \partial g(\bx^*)$, namely, to stationarity (Definition~\ref{definition:stat_opt_cond_p3}).
\end{proof}

Similar to descent lemma for GD and PGD (Lemma~\ref{lemma:gdupd_sm_sconv}),
the basic inequality that will be used in the analysis of the GCG method is the following recursive inequality.


\begin{lemma}[Descent Lemma for GCG]\label{lemma:desc_gcg}
Suppose that $f$ is $\beta$-smooth and $g$ is \textcolor{black}{closed} convex, satisfying assumption (B1) and (B2) in Assumption~\ref{assumption:gcg}. Let $\bx \in \dom(g)$ and $\eta \in [0, 1]$. 
Consider problem (P3).
Then,
\begin{equation}
F(\bx + \eta(\widetildebx - \bx)) \leq F(\bx) - \eta \mathcalW(\bx) + \frac{\eta^2 \beta}{2} \normtwo{\widetildebx - \bx}^2. 
\end{equation}
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:desc_gcg}]
By the smoothness of $f$ and  the convexity of $g$, we can write the following:
$$
\small
\begin{aligned}
F(\bx + &\eta(\widetildebx - \bx)) 
= f(\bx + \eta(\widetildebx - \bx)) + g(\bx + \eta(\widetildebx - \bx)) \\
%&\leq f(\bx) + \eta\innerproduct{\nabla f(\bx),  \widetildebx -\bx} + \frac{\eta^2 \beta}{2} \normtwo{\widetildebx - \bx}^2 + g((1 - \eta)\bx + \eta \widetildebx) \\
&\leq f(\bx) + \eta \innerproduct{\nabla f(\bx),  \widetildebx -\bx } + \frac{\eta^2 \beta}{2} \normtwo{\widetildebx - \bx}^2 + (1 - \eta)g(\bx) + \eta g(\widetildebx) \\
&= F(\bx) + \eta\big(\innerproduct{\nabla f(\bx), \widetildebx -\bx } - g(\bx) + g(\widetildebx)\big) + \frac{\eta^2 \beta}{2} \normtwo{\widetildebx - \bx}^2 \\
&= F(\bx) - \eta \mathcalW(\bx) + \frac{\eta^2 \beta}{2} \normtwo{\widetildebx - \bx}^2.
\end{aligned}
$$
This completes the proof.
\end{proof}

\begin{lemma}[Monotonicity and Sufficient Decrease for GCG]\label{lemma:mono_suff_gcg}
Suppose that $f$ is $\beta$-smooth and $g$ is \textcolor{black}{closed} convex, satisfying assumption (B1) and (B2) in Assumption~\ref{assumption:gcg}. 
Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the generalized conditional gradient method (Algorithm~\ref{alg:gen_cond_gen}) for solving problem (P3) with adaptive stepsizes $\eta_t \triangleq \left\{ 1, \frac{\mathcalW(\bx^\toptzero)}{\beta \normtwo{\widetildebx^\toptzero -\bx^\toptzero}^2} \right\}$  for each $t$-th iteration. Then, for any $ t > 0 $,
$$
F(\bx^\toptzero) - F(\bx^\toptone) \geq \frac{1}{2} \min \left\{ \mathcalW(\bx^\toptzero), \frac{\mathcalW^2(\bx^\toptzero)}{\beta D^2} \right\}
$$
where $ D $ is an upper bound on the diameter of $ \dom(g) $:
$
D \geq \max_{\bx,\by \in \dom(g)} \normtwo{\bx - \by}.
$
Since $\mathcalW(\bx^\toptzero)\geq 0$ by Theorem~\ref{theorem:cgn_optmea} and equals to 0 only for stationary points, this also shows that 
\begin{itemize}
\item $F(\bx^\toptzero) - F(\bx^\toptone) \geq 0$.
\item $F(\bx^\toptzero) > F(\bx^\toptone)$  if $\bx^\toptzero$  is not a stationary point of problem (P3).
\end{itemize}
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:mono_suff_gcg}]
For any iteration $ t > 0 $ and the update rule in Algorithm~\ref{alg:gen_cond_gen}, it holds that $ \bx^\toptone = \bx^\toptzero + \eta_t (\widetildebx^\toptzero - \bx^\toptzero) $, where
$
\eta_t = \min \left\{ 1, \frac{\mathcalW(\bx^\toptzero)}{\beta \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2} \right\}.
$
By the descent lemma for GCG (Lemma~\ref{lemma:desc_gcg}), we have
\begin{equation}\label{equation:mono_suff_gcg}
F(\bx^\toptzero) - F(\bx^\toptone) \geq \eta_t \mathcalW(\bx^\toptzero) - \frac{\eta_t^2 \beta}{2} \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2.
\end{equation}
Then,
\begin{itemize}
\item \textit{Case 1: $\frac{\mathcalW(\bx^\toptzero)}{\beta \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2} < 1$.}
We have 
$
F(\bx^\toptzero) - F(\bx^\toptone) \geq \frac{\mathcalW^2(\bx^\toptzero)}{2\beta \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2} \geq \frac{\mathcalW^2(\bx^\toptzero)}{2\beta D^2}.
$

\item \textit{Case 2: $\frac{\mathcalW(\bx^\toptzero)}{\beta \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2} \geq 1$, $\eta_t=1$.}
We have
$
F(\bx^\toptzero) - F(\bx^\toptone) \geq \mathcalW(\bx^\toptzero) - \frac{\beta}{2} \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2 \geq \frac{1}{2} \mathcalW(\bx^\toptzero).
$
\end{itemize}
Combining the two cases obtains the desired result.
\end{proof}

When $ f $ is convex, the conditional gradient norm is lower bounded by the distance to optimality in terms of function values.
\begin{lemma}[Conditional Gradient Norm for Convex Functions]\label{lemma:con_gcg_cgnorm}
Suppose that $f$ is $\beta$-smooth and $g$ is \textcolor{black}{closed} convex, satisfying assumption (B1) and (B2) in Assumption~\ref{assumption:gcg}, and suppose additionally that $ f $ is convex. Then, for any $ \bx \in \dom(g) $ and any optimizer $\bx^*$ of $F$,
$$
\mathcalW(\bx) \geq F(\bx) - F(\bx^*).
$$
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:con_gcg_cgnorm}]
For any $ \bx \in \dom(g) $,
$$
\small
\begin{aligned}
\mathcalW(\bx) 
&= \innerproduct{\nabla f(\bx), \bx - \widetildebx} + g(\bx) - g(\widetildebx) 
= \innerproduct{\nabla f(\bx), \bx} + g(\bx) - \big(\innerproduct{\nabla f(\bx), \widetildebx} + g(\widetildebx)\big) \\
&\geq \innerproduct{\nabla f(\bx), \bx} + g(\bx) - \big(\innerproduct{\nabla f(\bx), \bx^*} + g(\bx^*)\big) 
= \innerproduct{\nabla f(\bx), \bx - \bx^*} + g(\bx) - g(\bx^*) \\
&\geq f(\bx) - f(\bx^*) + g(\bx) - g(\bx^*)  
= F(\bx) - F(\bx^*).
\end{aligned}
$$
where the last inequality follows from the convexity of $f$. This completes the proof.
\end{proof}

Using these lemmas, we then prove the convergence of GCG for smooth functions.

\begin{theoremHigh}[GCG for SS $f$]\label{theorem:gcg_ss}
Suppose that $f$ is $\beta$-smooth and $g$ is \textcolor{black}{closed} convex, satisfying assumption (B1) and (B2) in Assumption~\ref{assumption:gcg}. 
Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the generalized conditional gradient method (Algorithm~\ref{alg:gen_cond_gen}) for solving problem (P3) with adaptive stepsizes $\eta_t \triangleq \left\{ 1, \frac{\mathcalW(\bx^\toptzero)}{\beta \normtwo{\widetildebx^\toptzero -\bx^\toptzero}^2} \right\}$  for each $t$-th iteration.
Then,
\begin{enumerate}[(i)]
\item $\mathcalW(\bx^\toptzero) \rightarrow 0$ { as } $t \rightarrow \infty$.
\item For any $t > 0$, $\underset{t=\{1,\ldots,T\}}{\min}\mathcalW(\bx^\toptzero) \leq \max \left\{ \frac{2(F(\bx^{(1)}) - F(\bx^*))}{T}, \frac{\sqrt{2\beta D^2 (F(\bx^{(1)}) - F(\bx^*))}}{\sqrt{T}} \right\}$, where $ D $ is an upper bound on the diameter of $ \dom(g) $.
\item All limit points of the sequence $ \{\bx^\toptzero\}_{t > 0} $ are stationary points of problem (P3).
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:gcg_ss}]
\textbf{(i).} Since $ \{F(\bx^\toptzero)\}_{t>0} $ is nonincreasing and bounded below by $ F(\bx^*) $ (Lemma~\ref{lemma:mono_suff_gcg}), it follows that it is convergent, and in particular, $ F(\bx^\toptzero) - F(\bx^\toptone) \rightarrow 0 $ as $ t \rightarrow \infty $. Therefore, by the sufficient decrease in Lemma~\ref{lemma:mono_suff_gcg}, it follows that $ \min \left\{ \mathcalW(\bx^\toptzero), \frac{\mathcalW^2(\bx^\toptzero)}{\beta D^2} \right\} \rightarrow 0 $ as $ t \rightarrow \infty $, implying that $ \mathcalW(\bx^\toptzero) \rightarrow 0 $ as $ t \rightarrow \infty $.

\paragraph{(ii).} By the sufficient decrease in Lemma~\ref{lemma:mono_suff_gcg}, for all $t  \geq 0 $,
$$
F(\bx^\toptzero) - F(\bx^\toptone) \geq \frac{1}{2} \min \left\{ \mathcalW(\bx^\toptzero), \frac{\mathcalW^2(\bx^\toptzero)}{\beta D^2} \right\}.
$$
Telescoping the sum over $ t = 1,2,\ldots,T  $,
\begin{equation*}
\begin{aligned}
F(\bx^{(1)}) - F(\bx^*)  &\geq F(\bx^{(1)}) - F(\bx^{(T+1)}) \geq \frac{1}{2} \sum_{t=1}^{T} \min \left\{ \mathcalW(\bx^\toptzero), \frac{\mathcalW^2(\bx^\toptzero)}{\beta D^2} \right\}\\
&\geq
\frac{T}{2} \min_{t \in \{1, 2, \ldots, T\}} \left[ \min \left\{ \mathcalW(\bx^\toptzero), \frac{\mathcalW^2(\bx^\toptzero)}{\beta D^2} \right\} \right].
\end{aligned}
\end{equation*}
This implies  that there exists an $ t \in \{1, 2, \ldots, T\} $ for which
$$
\min \left\{ \mathcalW(\bx^\toptzero), \frac{\mathcalW^2(\bx^\toptzero)}{\beta D^2} \right\} \leq \frac{2\big(F(\bx^{(1)}) - F(\bx^*)\big)}{T}.
$$
That is,
$$
\mathcalW(\bx^\toptzero) \leq \max \left\{ \frac{2(F(\bx^{(1)}) - F(\bx^*))}{T}, \frac{\sqrt{2\beta D^2 (F(\bx^{(1)}) - F(\bx^*))}}{\sqrt{T}} \right\}.
$$


\paragraph{(iii).} Suppose that $ \bx^+ $ is a limit point of $ \{\bx^\toptzero\}_{t>0} $. Then there exists a subsequence $ \{\bx^{(t_j)}\}_{j > 0} $ that converges to $ \bx^+ $. By the definition of the conditional gradient norm $ \mathcalW(\cdot) $, it follows that for any $ \bv\in\real^n $,
$$
\mathcalW(\bx^{(t_j)}) \geq \innerproduct{\nabla f(\bx^{(t_j)}), \bx^{(t_j)} - \bv} + g(\bx^{(t_j)}) - g(\bv).
$$
Taking the limit as $ j \rightarrow \infty $ and using the fact that $ \mathcalW(\bx^{(t_j)}) \rightarrow 0 $ as $ j \rightarrow \infty $, along with the continuity of $ \nabla f $ and the closedness of $ g $, we obtain that
$$
0 \geq \innerproduct{\nabla f(\bx^+), \bx^+ - \bv} + g(\bx^+) - g(\bv) \text{ for any } \bv \in \real^n,
$$
which is the same as the relation $ -\nabla f(\bx^+) \in \partial g(\bx^+) $,  indicating stationarity.
\end{proof}


We will demonstrate that the iterates of function values in the GCG approach for convex and smooth functions follow a quadratic inequality form. The following lemma summarizes the convergence results for such a form.
\begin{lemma}\label{lemma:gcg_conv_ss_usg}
Let $B$ be a positive integer, and let $\{a_t\}_{t > 0}$ and $\{b_t\}_{t > 0}$ be nonnegative sequences satisfying for any $t > 0$
\begin{equation}\label{equation:ineq_gcg_conv_ss_usg}
a_{t+1} \leq a_t - \gamma_t b_t + \frac{A}{2} \gamma_t^2,
\end{equation}
where $\gamma_t = \frac{2}{t + 2B}$ and $A$ is a positive number. Suppose that $a_t \leq b_t$ for all $t>0$. Then,
\begin{enumerate}[(i)]
\item For any $T \geq 1$, $a_T \leq \frac{2\max\{A, a_1(B-1)\}}{T + 2B - 2}$.
\item For any $T \geq 3$,
$
\min_{t=\lfloor T/2 \rfloor + 2, \ldots, T} b_t \leq \frac{8\max\{A, a_1(B-1)\}}{T - 2}.
$
\end{enumerate}
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:gcg_conv_ss_usg}]
\textbf{(i).} 
Since $a_{t+1} \leq a_t - \gamma_t b_t + \frac{A}{2} \gamma_t^2$ and $a_t \leq b_t$ for all $t>0$, it follows that
$a_{t+1} \leq (1 - \gamma_t) a_t + \frac{A}{2} \gamma_t^2$.
Therefore,
$$
\begin{aligned}
a_2 &\leq (1 - \gamma_1) a_1 + \frac{A}{2} \gamma_1^2; \\
a_3 &\leq (1 - \gamma_2) a_2 + \frac{A}{2} \gamma_2^2 = (1 - \gamma_2)(1 - \gamma_1) a_1 + (1 - \gamma_2) \frac{A}{2} \gamma_1^2 + \frac{A}{2} \gamma_2^2; \\
a_4 &\leq (1 - \gamma_3) a_3 + \frac{A}{2} \gamma_3^2 = a_1 \prod_{t=1}^{3} (1 - \gamma_t)
+ \frac{A}{2} \big((1 - \gamma_3)(1 - \gamma_2)  \gamma_1^2 + (1 - \gamma_3)  \gamma_2^2 +  \gamma_3^2\big);\\
\vdots &\leq  \vdots.\\
\end{aligned}
$$
That is, for any $T>1$, the general term formula for $a_T$ is 
\begin{equation}\label{equation:gcg_conv_ss_usg1}
a_T \leq a_1 \prod_{t=1}^{T-1} (1 - \gamma_t) + \frac{A}{2} \sum_{i=1}^{T-1} \left[ \prod_{t=i+1}^{T-1} (1 - \gamma_t) \right] \gamma_i^2.
\end{equation}
For the first term of \eqref{equation:gcg_conv_ss_usg1}, we have
$$
a_1 \prod_{t=1}^{T-1} (1 - \gamma_t) = a_1 \prod_{t=1}^{T-1} \frac{t + 2B - 2}{t + 2B} = a_1 \frac{(2B - 1)2B }{(T + 2B - 2)(T + 2B - 1)}. 
$$
For the second term of \eqref{equation:gcg_conv_ss_usg1}, since $\gamma_t = \frac{2}{t + 2B}$, it follows that
$$
\begin{aligned}
& \sum_{i=1}^{T-1} \left[ \prod_{t=i+1}^{T-1} (1 - \gamma_t) \right] \gamma_i^2 
=  \sum_{i=1}^{T-1} \left[ \prod_{t=i+1}^{T-1} \frac{t + 2B - 2}{t + 2B} \right] \frac{4}{(i + 2B)^2} \\
&=  \sum_{i=1}^{T-1} \frac{(i + 2B - 1)(i + 2B)}{(T + 2B - 2)(T + 2B - 1)}  \frac{4}{(i + 2B)^2} 
= \sum_{i=1}^{T-1} \frac{4(i + 2B - 1)}{(T + 2B - 2)(T + 2B - 1)(i + 2B)}   \\
&\leq \frac{4(T-1)}{(T + 2B - 2)(T + 2B - 1)}. 
\end{aligned}
$$
Substituting these into \eqref{equation:gcg_conv_ss_usg1} yields
$$
\begin{aligned}
a_T &\leq   \frac{\big(a_1(2B - 2)2B\big) + \big(2A(T-1)\big)}{(T + 2B - 2)(T + 2B - 1)} 
=2\frac{a_1(B - 1)2B + A(T-1)}{(T + 2B - 2)(T + 2B - 1)} 
\\
&\leq \frac{2 \max\{A, a_1(B - 1)\}(T + 2B - 1)}{(T + 2B - 2)(T + 2B - 1)} 
= \frac{2 \max\{A, a_1(B - 1)\}}{T + 2B - 2}.
\end{aligned}
$$
\paragraph{(ii).} 
Summing the inequality~\eqref{equation:ineq_gcg_conv_ss_usg} over $t = j, j+1, \ldots, T$, we obtain that
$a_{T+1} \leq a_j - \sum_{t=j}^{T} \gamma_t b_t + \frac{A}{2} \sum_{t=j}^{T} \gamma_t^2. $
Let $\widetildeb\triangleq\min_{t \in \{j, j+1, \ldots, T\}} b_t$, where $j\geq 1$. Then, using the result of part (i),
$$
\begin{aligned}
\widetildeb\bigg( \sum_{t=j}^{T} \gamma_t \bigg)  
&\leq a_j + \frac{A}{2} \sum_{t=j}^{T} \gamma_t^2 
\leq \frac{2 \max\{A, a_1(B-1)\}}{j+2B-2 } + 2A \sum_{t=j}^{T} \frac{1}{(t+2B)^2}\\
&\leq \frac{2 \max\{A, a_1(B-1)\}}{j+2B-2 } + 2A \sum_{t=j}^{T} \left(\frac{1}{t+2B-1} - \frac{1}{t+2B}\right)\\
&\leq \frac{2 \max\{A, a_1(B-1)\}}{j+2B-2 } + 2A  \left(\frac{1}{j+2B-1} - \frac{1}{T+2B}\right)
\leq \frac{4 \max\{A, a_1(B-1)\}}{j+2B-2 }.
\end{aligned}
$$
Since  $\sum_{t=j}^{T} \gamma_t =  \sum_{t=j}^{T} \frac{2}{t+2B} \geq 2 \frac{T-j+1}{T+2B}$,
which, combined with the above inequality and letting $j \triangleq \lfloor T/2 \rfloor + 2$,  concludes that for any $T \geq 3$:
$$ 
\begin{aligned}
\widetildeb=\min_{t \in \{\lfloor T/2 \rfloor + 2, \ldots, T\}} b_t 
&\leq \frac{2 \max\{A, a_1(B-1)\}(T+2B)}{(\lfloor T/2 \rfloor + 2B)(T - \lfloor T/2 \rfloor - 1)}
\leq \frac{2 \max\{A, a_1(B-1)\}(T+2B)}{( T/2  + 2B-0.5)(T - \lfloor T/2 \rfloor - 1)}\\
&=\frac{4 \max\{A, a_1(B-1)\}(T+2B)}{( T  + 4B-1)(T - \lfloor T/2 \rfloor - 1)}
\leq \frac{4 \max\{A, a_1(B-1)\}}{T/2 - 1},
\end{aligned} 
$$
which yields the desired result.
\end{proof}

Similar to the proximal gradient method for solving a composite problem (P3) with $f$ being convex and smooth (Theorem~\ref{theorem:prox_conv_ss_cvx}), using the above lemma, the GCG can also achieves a rate of $\mathcalO(1/T)$.

\begin{theoremHigh}[GCG for Convex and SS $f$: $\mathcalO(1/T)$]\label{theorem:gcg_conv_ss}
Suppose that $f$ is $\beta$-smooth and $g$ is \textcolor{black}{closed} convex, satisfying assumption (B1) and (B2) in Assumption~\ref{assumption:gcg}, and suppose additionally that $ f $ is convex.
Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the generalized conditional gradient method (Algorithm~\ref{alg:gen_cond_gen}) for solving problem (P3) with either a predefined stepsize $ \eta_t \triangleq \gamma_t\triangleq \frac{2}{t+2} $ or an adaptive stepsize $\eta_t\triangleq \theta_t \triangleq \min\left\{ 1, \frac{\mathcalW(\bx^\toptzero)}{\beta \normtwo{\widetildebx^\toptzero -\bx^\toptzero}^2} \right\}$  for each $t$-th iteration. Let $ D $ be an upper bound on the diameter of $ \dom(g) $: $ D \geq \max_{\bx,\by \in \dom(g)} \normtwo{\bx - \by}.$
Then,
\begin{enumerate}[(i)]
\item For  any $T\geq 1 $,  $F(\bx^{(T)}) - F(\bx^*) \leq \frac{2\beta D^2}{T}$.
\item For any $T \geq 3$, $\min_{t=\lfloor T/2 \rfloor + 2, \ldots, T} \mathcalW(\bx^\toptzero) \leq \frac{8\beta D^2}{T-2}$.
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:gcg_conv_ss}]
By the descent lemma for GCG (Lemma~\ref{lemma:desc_gcg}),
$$
F\big(\bx^\toptzero + \eta_t (\widetildebx^\toptzero - \bx^\toptzero)\big) - F(\bx^*) \leq F(\bx^\toptzero) - F(\bx^*) - \eta_t \mathcalW(\bx^\toptzero) + \frac{\eta_t^2 \beta}{2} \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2,
$$
where $ \widetildebx^\toptzero =\mathop{\argmin}_{\bx\in {\real^n}} \innerproduct{\nabla f(\bx^{(t)}), \bx} + g(\bx) $. Specifically, if a predefined stepsize $ \gamma_t \triangleq \frac{2}{t+2} $ is used, then
$$
F\big(\bx^\toptzero + \gamma_t (\widetildebx^\toptzero - \bx^\toptzero)\big) - F(\bx^*) \leq F(\bx^\toptzero) - F(\bx^*) - \gamma_t \mathcalW(\bx^\toptzero) + \frac{\gamma_t^2 \beta}{2} \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2.
$$
In the adaptive stepsize strategy, $  \theta_t \triangleq \min \left\{ 1, \frac{\mathcalW(\bx^\toptzero)}{\beta \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2} \right\} $, where $ \theta_t $ satisfies
$
\theta_t = \mathop{\argmin}_{\theta \in [0,1]} \left\{ -\theta \mathcalW(\bx^\toptzero) + \frac{\theta^2 \beta}{2} \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2 \right\}. 
$
This implies
\begin{align*}
F\big(\bx^\toptzero + \theta_t (\widetildebx^\toptzero - \bx^\toptzero)\big) - F(\bx^*) &\leq F(\bx^\toptzero) - F(\bx^*) - \theta_t \mathcalW(\bx^\toptzero) + \frac{\theta_t^2 \beta}{2} \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2 \\
&\leq F(\bx^\toptzero) - F(\bx^*) - \gamma_t \mathcalW(\bx^\toptzero) + \frac{\gamma_t^2 \beta}{2} \normtwo{\widetildebx^\toptzero - \bx^\toptzero}^2,
\end{align*}
Thus, both stepsize strategies satisfy
$$
F(\bx^\toptone) - F(\bx^*) \leq F(\bx^\toptzero) - F(\bx^*) - \gamma_t \mathcalW(\bx^\toptzero) + \frac{\gamma_t^2 \beta D^2}{2}.
$$
Invoking Lemma~\ref{lemma:gcg_conv_ss_usg} with $ a_t \triangleq F(\bx^\toptzero) - F(\bx^*), b_t \triangleq \mathcalW(\bx^\toptzero), A \triangleq \beta D^2 $, and $ B \triangleq 1 $, and noting that $ a_t \leq b_t $ by Lemma~\ref{lemma:con_gcg_cgnorm}, both parts (i) and (ii) follow.
\end{proof}







\section{Mirror Descent Method}\label{section:mirror}
\begin{algorithm}[h] 
\caption{Mirror Descent Method}
\label{alg:mirror_des}
\begin{algorithmic}[1] 
\Require A  function $f(\bx)$, a convex function $\phi(\bx)$, and a set $\sS$; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t$;
\State $\bx^{(t+1)} \leftarrow \mathop{\argmin}_{\bx\in\sS} 
\left\{  \innerproduct{\eta_t f'(\bx^\toptzero)-\nabla \phi(\bx^\toptzero), \bx}    + \phi(\bx) \right\}$; \Comment{$f'(\bx^\toptzero)\in \partial f(\bx^\toptzero)$}
\EndFor
\State (Output Option 1) Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
%\State (Output Option 2) Output  $\bx_{\text{avg}}\leftarrow \frac{1}{T}(\sum_{t=1}^{t}\bx^{(t)})$ or $\sum_{t=1}^{T} \frac{2t}{T(T+1)} \bx^{(t)}$;
\State (Output Option 2) Output  $\bx_{\text{best}}\leftarrow \argmin_{t\in\{1,2,\ldots,T\}} f(\bx^{(t)})$;
\end{algorithmic} 
\end{algorithm}

We showed in \eqref{equation:pgd_decom_raw} that the PGD method is equivalent to a minimization of the sum of the linearization of the smooth part around the current iterate plus a quadratic term, over which we derived the proximal gradient descent for the unconstrained composite optimization problem (P3); see \eqref{equation:prox_decom}. We reiterate the equivalence  as follows:
\begin{center}
\framebox{
\begin{minipage}{0.95\textwidth}
\begin{equation}\label{equation:pgd_decom_mirr}
\small
\textbf{(PGD)}:\quad 
\begin{aligned}
\bx^\toptone 
&\leftarrow\mathop{\argmin}_{\bx\in\sS} \normtwo{\bx - \left(\bx^{(t)} - \eta_t  f'(\bx^{(t)})\right)}^2\\
&=\mathop{\argmin}_{\bx\in\sS} 
\left\{
\frac{1}{2\eta_t} \normtwo{\bx-\bx^\toptzero}^2 + f(\bx^\toptzero)+ \innerproduct{f'(\bx^\toptzero), \bx-\bx^\toptzero} 
\right\}.
\end{aligned}
\end{equation}
\end{minipage}
}
\end{center}
This expression represents the sum of a linearization term and an Euclidean distance term between $\bx$ and the current iterate $\bx^\toptzero$. Here, $f'(\bx^\toptzero)\in \partial f(\bx^\toptzero)$ denotes any subgradient of $f$ at $\bx^\toptzero$; when $f$ is differentiable, this reduces to the gradient.
The \textit{mirror descent method} extends this idea from using Euclidean distance to employing a non-Euclidean distance defined by the Bregman distance (Definition~\ref{definition:breg_dist}):
\begin{tcolorbox}[colback=white,colframe=black]
\begin{minipage}{1\textwidth}
\begin{subequations}\label{equation:mirr_rule}
\small
%\textbf{(Mirror)}:\quad 
\begin{align}
\bx^\toptone 
&\leftarrow\mathop{\argmin}_{\bx\in\sS} 
\left\{
\frac{1}{\eta_t} \mathcalD_{\phi}(\bx,\bx^\toptzero) + \cancel{f(\bx^\toptzero)}+ \innerproduct{f'(\bx^\toptzero), \bx-\cancel{\bx^\toptzero}} 
\right\}             \\
\textbf{(Mirror)}:\qquad\qquad  &=\mathop{\argmin}_{\bx\in\sS} 
\left\{  \innerproduct{\eta_t f'(\bx^\toptzero)-\nabla \phi(\bx^\toptzero), \bx}    + \phi(\bx) \right\}   \\
&=\mathop{\argmin}_{\bx\in\textcolor{mylightbluetext}{\real^n}} 
\left\{  \innerproduct{\eta_t f'(\bx^\toptzero)-\nabla \phi(\bx^\toptzero), \bx}    + \widetilde{\phi}(\bx) \right\}   \\
&=\mathop{\argmin}_{\bx\in\textcolor{mylightbluetext}{\real^n}} 
\left\{  \widetildef(\bx)    + \mathcalD_{\phi}(\bx,\bx^\toptzero) \right\} \triangleq \bproxtphi(\bx), \label{equation:mirr_rule_v4}
\end{align}
\end{subequations}
\end{minipage}
\end{tcolorbox}
\noindent
where we replace $\frac{1}{2}\normtwo{\bx-\bx^\toptzero}^2$ term in PGD with a Bregman distance $\big( \mathcalD_{\phi}(\bx,\bx^\toptzero) \triangleq \phi(\bx) - \phi(\bx^\toptzero) - \innerproduct{\nabla \phi(\bx^\toptzero), \bx - \bx^\toptzero }\big)$ and remove constant terms. The function $\phi(\bx)$ needs to be a convex function by Definition~\ref{definition:breg_dist}, and:
$$
\begin{aligned}
\widetildef(\bx) &\triangleq \innerproduct{\eta_t f'(\bx^\toptzero), \bx} + \indicatorS(\bx)
\quad &\implies&\quad \text{closed convex if $\sS$ is closed convex};\\
\widetilde{\phi}(\bx) &\triangleq \phi(\bx)+\indicatorS(\bx) 
\quad &\implies&\quad \text{$\widetilde{\phi}(\bx) = \infty$ if $\bx\notin\sS$},
\end{aligned}
$$
where $\indicatorS(\bx)$ is closed convex if and only if $\sS$ is closed convex (Exercise~\ref{exercise_closed_indica} and Exercise~\ref{exercise_convex_indica}).
The operator $\bproxtphi(\cdot)$ in \eqref{equation:mirr_rule_v4} denotes the Bregman-proximal operator (Definition~\ref{definition:projec_prox_opt}), which is an extension of the proximal operator.
When $\phi(\bx) = \frac{1}{2}\normtwo{\bx}^2$, $\bproxtphi(\bx) = \prox_{\widetildef}(\bx)$ for any $\bx\in\real^n$; then the mirror descent is equivalent  to PGD (Section~\ref{section:pgd}).

\begin{assumption}[Mirror Descent]\label{assumption:mirror}
For the analysis of the mirror descent method, we assume that 
\begin{itemize}
	\item  $f: \sS\rightarrow \real$ is a  proper closed and  \textbf{convex} function, where $\sS\subseteq\real^n$ is a closed and convex set such that $\sS\subseteq \interior(\dom(f))$~\footnote{This ensures the nonemptness of the subdifferential by Theorem~\ref{theorem:nonemp_relint_conv}.}.
	\item  $\phi$ is a differentiable, proper closed and \textbf{convex} function such that $\sS\subseteq \dom(\phi)$ and $\phi+\indicatorS$ is $\alpha$-strongly convex ($\alpha>0$).  
\end{itemize}
\noindent Under this assumption, the update of the mirror descent method in \eqref{equation:mirr_rule_v4} has a \textbf{unique} minimizer in  $\sS \cap \dom(\partial \phi)$ by invoking the Bregman-Proximal Property-O (Lemma~\ref{lemma:breg_prox_propo}) with $f(\bx) \triangleq \widetildef(\bx)$.
\end{assumption}

We will thus discuss the convergence of the mirror descent method under this assumption.
Using the Bregman-Proximal Property-I (Lemma~\ref{lemma:breg_prox_prop1}, under the convexity and closeness of $\widetildef(\bx)$) and the three-point property of the Bregman distance (Remark~\ref{remark:bregnan_dist}, under the convexity of $\phi(\bx)$), we can now establish an iterate inequality satisfied by the sequence generated by the mirror descent method. The inequality can be seen as a generalization of Lemma~\ref{lemma:iterate_ineq_pgd} for PGD approaches.
\begin{lemma}[Iterate Inequality for Mirror Descent]\label{lemma:itera_ineq_mirr}
Assume that Assumption~\ref{assumption:mirror} holds.
Let $\{\bx^\toptzero\}_{t > 0}$ is the sequence generated by the mirror descent method (Algorithm~\ref{alg:mirror_des}) with positive stepsizes $\{\eta_t\}_{t > 0}$ (Algorithm~\ref{alg:mirror_des}). Then, for   any optimizer $\bx^*$ of $f$ and any $t > 0$,
\begin{equation}\label{equation:itera_ineq_mirre1}
\mathcalD_\phi(\bx^*, \bx^\toptone)  \leq \mathcalD_\phi(\bx^*, \bx^\toptzero) - \eta_t \big(f(\bx^\toptzero) - f(\bx^*)\big) + \frac{\eta_t^2}{2\alpha} \normtwo{f'(\bx^\toptzero)}^2,~
\footnote{Note that when $\phi(\bx)=\frac{1}{2}\normtwo{\bx}^2$, $\mathcalD_{\phi}(\bx,\by)=\frac{1}{2}\normtwo{\bx-\by}^2$ by Example~\ref{example:breg_examp} and $\phi$ is $1$-SC; the inequality reduces to \eqref{equation:iterate_ineq_pgd1} for PGD.}
\end{equation}
where $f^\prime(\bx^\toptzero)\in \partial f(\bx^\toptzero)$ denotes any subgradient. 
For any nonnegative integer $T>0$, performing telescopic cancellations using \eqref{equation:itera_ineq_mirre1} shows that 
\begin{equation}\label{equation:itera_ineq_mirre2}
\sum_{t=1}^{T} \eta_t \big(f(\bx^\toptzero) - f(\bx^*)\big) \leq \mathcalD_\phi(\bx^*, \bx^{(1)})  + \frac{1}{2\alpha} \sum_{t=1}^{T} \eta_t^2 \normtwo{f^\prime(\bx^\toptzero)}^2. 
\end{equation}
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:itera_ineq_mirr}]
By the update formula of the mirror descent method in \eqref{equation:mirr_rule} for $\bx^\toptone$ and the Bregman-Proximal Property-I (Lemma~\ref{lemma:breg_prox_prop1}) invoked with $\by \triangleq \bx^\toptzero$ and the underlying function $\widetildef(\bx) \triangleq \eta_t \innerproduct{f'(\bx^\toptzero), \bx} + \indicatorS(\bx)$ (and hence $\widebarby \triangleq \bx^\toptone$), we have for any $\bz \in \sS$,
$$
\innerproduct{\nabla \phi(\bx^\toptzero) - \nabla \phi(\bx^\toptone), \bz - \bx^\toptone} 
\leq \eta_t \innerproduct{f'(\bx^\toptzero), \bz - \bx^\toptone}.
$$
By the three-point property (Remark~\ref{remark:bregnan_dist}, with $\bx \triangleq \bz$, $\by \triangleq \bx^\toptone$, and $\bz \triangleq \bx^\toptzero$),
$$
\mathcalD_\phi(\bz, \bx^\toptone) + \mathcalD_\phi(\bx^\toptone, \bx^\toptzero) - \mathcalD_\phi(\bz, \bx^\toptzero)
=
\innerproduct{\nabla \phi(\bx^\toptzero) - \nabla \phi(\bx^\toptone), \bz - \bx^\toptone}.
$$
Combining the preceding two quantities, using the subgradient inequality (Definition~\ref{definition:subgrad}), and letting $\bz\triangleq\bx^*$,  we have 
$$
\small
\begin{aligned}
&\eta_t (f(\bx^\toptzero) - f(\bx^*))\stackrel{*}{\leq} \eta_t \innerproduct{f'(\bx^\toptzero), \bx^\toptzero - \bx^*} \\
&\quad\leq \mathcalD_\phi(\bx^*, \bx^\toptzero) - \mathcalD_\phi(\bx^*, \bx^\toptone) - \mathcalD_\phi(\bx^\toptone, \bx^\toptzero) + \eta_t \innerproduct{ f'(\bx^\toptzero), \bx^\toptzero - \bx^\toptone }\\
%&\quad \stackrel{\dag}{\leq} \mathcalD_\phi(\bx^*, \bx^\toptzero) - \mathcalD_\phi(\bx^*, \bx^\toptone) - \frac{\alpha}{2} \normtwobig{\bx^\toptone - \bx^\toptzero}^2 + \eta_t \innerproduct{f'(\bx^\toptzero), \bx^\toptzero - \bx^\toptone}\\
&\quad \stackrel{\dag}{\leq} \mathcalD_\phi(\bx^*, \bx^\toptzero) - \mathcalD_\phi(\bx^*, \bx^\toptone) - \frac{\alpha}{2} \normtwobig{\bx^\toptone - \bx^\toptzero}^2 + \innerproduct{\frac{\eta_t}{\sqrt{\alpha}} f'(\bx^\toptzero), \sqrt{\alpha} (\bx^\toptzero - \bx^\toptone)} \\
&\quad \stackrel{\ddag}{\leq} \mathcalD_\phi(\bx^*, \bx^\toptzero) - \mathcalD_\phi(\bx^*, \bx^\toptone) - \frac{\alpha}{2} \normtwobig{\bx^\toptone - \bx^\toptzero}^2 + \frac{\eta_t^2}{2\alpha} \normtwobig{f'(\bx^\toptzero)}^2 + \frac{\alpha}{2} \normtwobig{\bx^\toptone - \bx^\toptzero}^2 \\
&\quad = \mathcalD_\phi(\bx^*, \bx^\toptzero) - \mathcalD_\phi(\bx^*, \bx^\toptone) + \frac{\eta_t^2}{2\alpha} \normtwobig{f'(\bx^\toptzero)}^2,
\end{aligned}
$$
where the inequality ($*$) follows from the subgradient inequality of $f$, the inequality $(\dag)$ follows from the $\alpha$-SC of $\phi+\indicatorS$, and the inequality $(\ddag)$ follows from the fundamental theorem of optimization (Theorem~\ref{theorem:funda_opt}). 
This completes the proof.
\end{proof}

Under a boundedness assumption on $\mathcalD_\phi(\bx, \bx^{(1)})$ over $\sS$, we can deduce a useful bound on the sequence of best achieved function values defined by
\begin{equation}
\fbest^\toptzero \triangleq \min\left\{f(\bx^{(1)}), f(\bx^{(2)}), \ldots, f(\bx^{(t)})\right\}.
\end{equation}

Similar to the convergence of PGD for convex functions in Theorem~\ref{theorem:pgd_lipschitz_dyna}, we then obtain the convergence results for mirror descent methods.
\begin{theoremHigh}[Mirror Descent for Convex and Lipschitz, $\mathcalO(\ln (T) / \sqrt{T})$]\label{theorem:mirr_best_bound}
Considering the same setting as Lemma~\ref{lemma:itera_ineq_mirr},
assume that Assumption~\ref{assumption:mirror} holds.
Let $\{\bx^\toptzero\}_{t > 0}$ be the sequence generated by the mirror descent method (Algorithm~\ref{alg:mirror_des}) with positive stepsizes $\{\eta_t\}_{t > 0}$ and $\bx^*$ be any optimal point. 
Suppose that $\normtwo{f'(\bx)} \leq L$ for all $\bx \in \sS$, where $L > 0$. 
Then, for any optimal point $ \bx^*$,
\begin{enumerate}[(i)]
\item  If $\mathcalD_\phi(\bx, \bx^{(1)})$ is bounded over $\sS$ such that 
$
\Omega(\bx^{(1)}) \geq \max_{\bx \in \sS} \mathcalD_\phi(\bx, \bx^{(1)})
$, then for any $T > 0$,
$$
\fbest^{(T)} - f(\bx^*) \leq \frac{\Omega(\bx^{(1)}) + \frac{L^2}{2\alpha} \sum_{t=1}^T \eta_t^2}{\sum_{t=1}^T \eta_t}.
$$

\item If $ \frac{\sum_{t=1}^{T} \eta_t^2}{\sum_{t=1}^{T} \eta_t} \to 0 $ as $ T \to \infty $, then $ \fbest^{(T)} \to f(\bx^*) $ as $ T \to \infty $.

\item If $
\eta_t = \frac{\sqrt{2\alpha}}{L \sqrt{t+1}}
$ or 
$\eta_t = \scriptsize\begin{cases} 
	\sqrt{2\alpha}/{(\normtwo{f'(\bx^\toptzero)} \sqrt{t+1})}, & f'(\bx^\toptzero) \neq \bzero; \\
	{\sqrt{2\alpha}}/{(L \sqrt{t+1})}, & f'(\bx^\toptzero) = \bzero,
\end{cases}$
then for all $ T\geq 5 $,
$$
\fbest^{(T)} - f(\bx^*) \leq \frac{L}{\sqrt{2\alpha}} \frac{\mathcalD_{\phi}(\bx^*, \bx^{(1)}) + \ln(T+1)}{\sqrt{T+1}}.
$$
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:mirr_best_bound}]
\textbf{(i).}
By Lemma~\ref{lemma:itera_ineq_mirr},  it follows that for any $T > 0$,
\begin{equation}
\begin{aligned}
\sum_{t=1}^{T} \eta_t \big(\fbest^{(T)} - f(\bx^*)\big) 
&\leq 	
\sum_{t=1}^{T} \eta_t \big(f(\bx^\toptzero) - f(\bx^*)\big) \\
&\leq \mathcalD_\phi(\bx^*, \bx^{(1)})  + \frac{1}{2\alpha} \sum_{t=1}^{T} \eta_t^2 \normtwo{f^\prime(\bx^\toptzero)}^2
\leq \Omega(\bx^{(1)}) + \frac{L^2}{2\alpha} \sum_{t=1}^T \eta_t^2,
\end{aligned}
\end{equation}
which obtains the desired result in (i).
\paragraph{(ii).}
By (i), we have 
$$
\fbest^{(T)} - f(\bx^*)
\leq 
\frac{\mathcalD_\phi(\bx^*, \bx^{(1)})  + \frac{L}{2\alpha} \sum_{t=1}^{T} \eta_t^2}{\sum_{t=1}^{T} \eta_t}.
$$
The claim in (ii) follows immediately.
\paragraph{(iii).}
Note that in either case, it follows that  $ \eta_t^2 \normtwo{f'(\bx^\toptzero)}^2 \leq \frac{2\alpha}{t+1} $ and $ \eta_t \geq \frac{\sqrt{2\alpha}}{L \sqrt{t+1}} $, whence we have
$$
\fbest^{(T)} - f(\bx^*) \leq \frac{L}{\sqrt{2\alpha}} \frac{\mathcalD_{\phi}(\bx^*, \bx^{(1)}) + \sum_{t=1}^{T} \frac{1}{{t+1}}}{\sum_{t=1}^{T} \frac{1}{\sqrt{t+1}}}.
$$
Using Lemma~\ref{lemma:pgd_lem2} concludes the result in (iii).
\end{proof}





\begin{algorithm}[h] 
\caption{Generalized Mirror Descent Method}
\label{alg:gen_mirror_des}
\begin{algorithmic}[1] 
\Require Functions $f(\bx), g(\bx)$, a convex function $\phi(\bx)$; 
\State {\bfseries Input:}  Initialize $\bx^{(1)}$;
\For{$t=1,2,\ldots$}
\State Pick a stepsize $\eta_t$;
\State $\bx^{(t+1)} \leftarrow \mathop{\argmin}_{\bx\in\real^n} 
\left\{  \innerproduct{\eta_t f'(\bx^\toptzero)-\nabla \phi(\bx^\toptzero), \bx}+ \textcolor{mylightbluetext}{\eta_t g(\bx)}  + \phi(\bx) \right\}$;
\EndFor
\State (Output Option 1) Output  $\bx_{\text{final}}\leftarrow \bx^{(T)}$;
%\State (Output Option 2) Output  $\bx_{\text{avg}}\leftarrow \frac{1}{T}(\sum_{t=1}^{t}\bx^{(t)})$ or $\sum_{t=1}^{T} \frac{2t}{T(T+1)} \bx^{(t)}$;
\State (Output Option 2) Output  $\bx_{\text{best}}\leftarrow \argmin_{t\in\{1,2,\ldots,T\}} f(\bx^{(t)})$;
\end{algorithmic} 
\end{algorithm}
\section{Generalized Mirror Descent Method}\label{section:gmirror}

We have shown that the mirror descent method addresses the same constrained problem \eqref{equation:p2_pgd} as the PGD approach. 
The \textit{generalized mirror (G-mirror) descent method (a.k.a., Bregman proximal gradient method)} (Algorithm~\ref{alg:gen_mirror_des}) tackles the unconstrained composite problem:
\begin{equation}\label{equation:gene_mirror_prob}
\text{(P3)}:\qquad \min \{F(\bx) \triangleq f(\bx)+g(\bx)\}.
\end{equation}

Similar to proximal gradient and generalized conditional gradient methods, by incorporating an  objective function  $\eta_t g(\bx)$ into the update rule  of the mirror descent, we obtain:
\begin{center}
\framebox{
\begin{minipage}{0.95\textwidth}
\begin{subequations}\label{equation:gmirr_rule}
\small
%\textbf{(Mirror)}:\quad 
\begin{align}
\textbf{(G-Mirror)}:\qquad\bx^\toptone 
&\leftarrow\mathop{\argmin}_{\bx\in\textcolor{mylightbluetext}{\real^n}} 
\left\{  \innerproduct{\eta_t f'(\bx^\toptzero)-\nabla \phi(\bx^\toptzero), \bx} + \textcolor{mylightbluetext}{\eta_t g(\bx)}   + \phi(\bx) \right\}   \\
&=\mathop{\argmin}_{\bx\in\textcolor{black}{\real^n}} 
\left\{  \widehatf(\bx)    + \mathcalD_{\phi}(\bx,\bx^\toptzero) \right\} \triangleq \bproxhphi(\bx), \label{equation:gmirr_rule_v4}
\end{align}
\end{subequations}
\end{minipage}
}
\end{center}
where we remove the constraint over $\sS$ and add a $\eta_t g(\bx)$ term to the mirror descent update in \eqref{equation:mirr_rule}. The function $\phi(\bx)$ must be a convex function by Definition~\ref{definition:breg_dist}, and:
$$
\begin{aligned}
\widehatf(\bx) &\triangleq \innerproduct{\eta_t f'(\bx^\toptzero), \bx} + \eta_t g(\bx)
\quad &\implies&\quad \text{closed convex if $g(\bx)$ is closed convex}.\\
\end{aligned}
$$
Again, the operator $\bproxhphi(\cdot)$ in \eqref{equation:gmirr_rule_v4} denotes the Bregman-proximal operator (Definition~\ref{definition:projec_prox_opt}), which is an extension of the proximal operator.
When $\phi(\bx) = \frac{1}{2}\normtwo{\bx}^2$, $\bproxhphi(\bx) = \prox_{\widehatf}(\bx)$ for any $\bx\in\real^n$; then the G-mirror descent method is equivalent  to the proximal gradient method (Section~\ref{section:prox_gd}), and the update rule is: 
$$
\bx^\toptone \leftarrow \prox_{\eta_t g}(\bx^\toptzero - \eta_t f'(\bx^\toptzero)).
$$




\begin{assumption}[G-Mirror Descent]\label{assumption:gen_mirror}
For the analysis of the G-mirror descent method, we assume that 
\begin{itemize}
\item  $f, g: \real^n\rightarrow \real$ are proper closed and \textbf{convex} functions, where $\dom(g)\subseteq\interior(\dom(f))$~\footnote{This ensures the nonemptness of the subdifferential by Theorem~\ref{theorem:nonemp_relint_conv}.}. And $\normtwo{f'(\bx)}\leq L$ for any $\bx\in\dom(g)$, where $L>0$ and $f'(\bx)\in\partial f(\bx)$ denotes a subgradient.
\item  $\phi$ is a differentiable, proper closed and \textbf{convex} function such that $\dom(g)\subseteq \dom(\phi)$ and $\phi+\indicatorG_{\dom(g)}$ is $\alpha$-strongly convex ($\alpha>0$).  
\end{itemize}
\noindent Under this assumption, the update of the G-mirror descent method in Algorithm~\ref{alg:gen_mirror_des} has a \textbf{unique} minimizer in  $\dom(g) \cap \dom(\partial \phi)$ by invoking the Bregman-Proximal Property-O (Lemma~\ref{lemma:breg_prox_propo}) with $f(\bx) \triangleq\widehatf(\bx)$.
\end{assumption}


The analysis of the G-mirror method is based on arguments similar to those used in Section~\ref{section:mirror} for analyzing the mirror descent method. 
We start  by proving a technical lemma establishing an inequality similar to the one derived in Lemma~\ref{lemma:itera_ineq_mirr}. 
Note that in addition to our basic assumptions, we assume that $ g $ is a nonnegative function and that the stepsizes are nonincreasing.
We again define best achieved function value at each iteration as: 
\begin{equation}
\Fbest^\toptzero \triangleq  \min\left\{F(\bx^{(1)}), F(\bx^{(2)}), \ldots, F(\bx^{(t)})\right\}.
\end{equation}
\begin{lemma}[Iterate Inequality for G-Mirror Descent]\label{lemma:itera_ineq_gmirr}
Assume that Assumption~\ref{assumption:gen_mirror} holds, and suppose  that $ g $ is a nonnegative function. Let $ \{\bx^\toptzero\}_{t > 0} $ be the sequence generated by the G-mirror method (Algorithm~\ref{alg:gen_mirror_des}) with positive nonincreasing stepsizes $ \{\eta_t\}_{t > 0} $. Then, for any optimal point $ \bx^*$ and $ T > 0 $,
\begin{equation}\label{equation:itera_ineq_gmirre1}
\small
\begin{aligned}
\mathcalD_\phi(\bx^*, \bx^\toptone)  \leq \mathcalD_\phi(\bx^*, \bx^\toptzero) 
- \eta_t \big(f(\bx^\toptzero) +  g(\bx^\toptone)- F(\bx^*)\big) + \frac{\eta_t^2}{2\alpha} \normtwo{f'(\bx^\toptzero)}^2,
\end{aligned}
\end{equation}
where $f^\prime(\bx^\toptzero)\in \partial f(\bx^\toptzero)$ denotes any subgradient. 
For any nonnegative integer $T>0$, performing telescopic cancellations using \eqref{equation:itera_ineq_gmirre1} shows that 
\begin{equation}\label{equation:itera_ineq_gmirre2}
\Fbest^{(T)} - F(\bx^*) \leq \frac{\eta_1 g(\bx^{(1)}) + \mathcalD_\phi(\bx^*, \bx^{(1)}) + \frac{1}{2\alpha} \sum_{t=1}^T \eta_t^2 \normtwo{f'(\bx^\toptzero)}^2}{\sum_{t=1}^T \eta_t}.
\end{equation}
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:itera_ineq_gmirr}]
By the update formula of the G-mirror descent method in \eqref{equation:gmirr_rule} for $\bx^\toptone$ and the Bregman-Proximal Property-I (Lemma~\ref{lemma:breg_prox_prop1}) invoked with $\by \triangleq \bx^\toptzero$ and the underlying function $\widehatf(\bx) \triangleq \innerproduct{\eta_t f'(\bx^\toptzero), \bx} + \eta_t g(\bx)$ (and hence $\widebarby \triangleq \bx^\toptone$), we have for any $\bz \in \real^n$,
$$
\innerproduct{\nabla \phi(\bx^\toptzero) - \nabla \phi(\bx^\toptone), \bz - \bx^\toptone} 
\leq \eta_t \innerproduct{f'(\bx^\toptzero), \bz - \bx^\toptone} + \eta_t g(\bz) - \eta_t g(\bx^\toptone).
$$
By the three-point property (Remark~\ref{remark:bregnan_dist}, with $\bx \triangleq \bz$, $\by \triangleq \bx^\toptone$, and $\bz \triangleq \bx^\toptzero$),
$$
\mathcalD_\phi(\bz, \bx^\toptone) + \mathcalD_\phi(\bx^\toptone, \bx^\toptzero) - \mathcalD_\phi(\bz, \bx^\toptzero)
=\innerproduct{\nabla \phi(\bx^\toptzero) - \nabla \phi(\bx^\toptone), \bz - \bx^\toptone}.
$$
Combining the preceding two quantities, using the subgradient inequality (Definition~\ref{definition:subgrad}), and letting $\bz\triangleq\bx^*$,  we have 
$$
\begin{aligned}
 &\eta_t \left[ f(\bx^\toptzero) + g(\bx^\toptone) - F(\bx^*) \right] \stackrel{*}{\leq}\eta_t \innerproduct{f'(\bx^\toptzero), \bx^\toptzero - \bx^*} + \eta_t g(\bx^\toptone) - \eta_t g(\bx^*)\\
&\leq \mathcalD_\phi(\bx^*, \bx^\toptzero) - \mathcalD_\phi(\bx^*, \bx^\toptone) - \mathcalD_\phi(\bx^\toptone, \bx^\toptzero) + \eta_t \innerproduct{ f'(\bx^\toptzero), \bx^\toptzero - \bx^\toptone}\\
&\stackrel{\dag}{\leq} \mathcalD_\phi(\bx^*, \bx^\toptzero) - \mathcalD_\phi(\bx^*, \bx^\toptone) - \frac{\alpha}{2} \normtwo{\bx^\toptone - \bx^\toptzero}^2 + \innerproduct{ \frac{\eta_t}{\sqrt{\alpha}} f'(\bx^\toptzero), \sqrt{\alpha} (\bx^\toptzero - \bx^\toptone)}\\
&\stackrel{\ddag}{\leq} \mathcalD_\phi(\bx^*, \bx^\toptzero) - \mathcalD_\phi(\bx^*, \bx^\toptone) - \frac{\alpha}{2} \normtwo{\bx^\toptone - \bx^\toptzero}^2 + \frac{\eta_t^2}{2\alpha} \normtwo{f'(\bx^\toptzero)}^2 + \frac{\alpha}{2} \normtwo{\bx^\toptone - \bx^\toptzero}^2\\
&= \mathcalD_\phi(\bx^*, \bx^\toptzero) - \mathcalD_\phi(\bx^*, \bx^\toptone) + \frac{\eta_t^2}{2\alpha} \normtwo{f'(\bx^\toptzero)}^2.
\end{aligned}
$$
where the inequality ($*$) follows from the subgradient inequality of $f$, the inequality $(\dag)$ follows from the $\alpha$-SC of $\phi+\indicatorG_{\dom(g)}$, and the inequality $(\ddag)$ follows from the fundamental theorem of optimization (Theorem~\ref{theorem:funda_opt}). 
This establishes \eqref{equation:itera_ineq_gmirre1}.
Performing telescopic cancellations using \eqref{equation:itera_ineq_gmirre1} shows that 
$$
\sum_{t=1}^T \eta_t \left[ f(\bx^\toptzero) + g(\bx^\toptone) - F(\bx^*) \right] 
\leq \mathcalD_\phi(\bx^*, \bx^{(1)}) - \mathcalD_\phi(\bx^*, \bx^{(T+1)}) + \frac{1}{2\alpha} \sum_{t=1}^T \eta_t^2 \normtwo{f'(\bx^\toptzero)}^2.
$$
Since $\{\eta_t\}$ is nonincreasing and $g$ is nonnegative, adding the term $ \eta_1 g(\bx^{(1)}) - \eta_T g(\bx^{(T+1)}) $ to both sides and using the nonnegativity of the Bregman distance, we get
$$
\small
\begin{aligned}
\bigg( \sum_{t=1}^T \eta_t \bigg) \big( \Fbest^{(T)} - F(\bx^*) \big)
&\leq \sum_{t=1}^T \eta_t \left[ F(\bx^\toptzero) - F(\bx^*) \right] \\
&\leq\eta_1 (F(\bx^{(1)}) - F(\bx^*)) + \sum_{t=2}^T \left[ \eta_t f(\bx^\toptzero) + \eta_{t-1} g(\bx^\toptzero) - \eta_t F(\bx^*) \right]\\
&\leq \eta_1 g(\bx^{(1)}) - \eta_T g(\bx^{(T+1)}) + \mathcalD_\phi(\bx^*, \bx^{(1)}) + \frac{1}{2\alpha} \sum_{t=1}^T \eta_t^2 \normtwo{f'(\bx^\toptzero)}^2\\
&\leq\eta_1 g(\bx^{(1)}) + \mathcalD_\phi(\bx^*, \bx^{(1)}) + \frac{1}{2\alpha} \sum_{t=1}^T \eta_t^2 \normtwo{f'(\bx^\toptzero)}^2,
\end{aligned}
$$
which concludes the result.
\end{proof}



Using Lemma~\ref{lemma:itera_ineq_gmirr}, we can also establish the rate of convergence of the G-mirror method with a dynamic stepsize rule.
\begin{theoremHigh}[G-Mirror Descent for Convex: $\mathcalO(\ln T / \sqrt{T})$]\label{theorem:gmirror_dyn}
Assume that Assumption~\ref{assumption:gen_mirror} holds, and suppose  that $ g $ is a nonnegative function. 
Let $\{\bx^\toptzero\}_{t > 0}$ be the sequence generated by the G-mirror method (Algorithm~\ref{alg:gen_mirror_des}) with positive nonincreasing stepsizes $ \{\eta_t\}_{t > 0} $.
Suppose that $\normtwo{f'(\bx)} \leq L$ for all $\bx \in \real^n$, where $L > 0$. 
Then, for any optimal point $ \bx^*$,
\begin{enumerate}[(i)]
\item  Let
$
\Omega(\bx^{(1)}) \geq \max_{\bx \in \sS} \mathcalD_\phi(\bx, \bx^{(1)})
$. Then, for any $T > 0$,
$$
\Fbest^{(T)} - F(\bx^*) \leq \frac{\eta_1 g(\bx^{(1)}) + \Omega(\bx^{(1)}) + \frac{L^2}{2\alpha} \sum_{t=1}^T \eta_t^2}{\sum_{t=1}^T \eta_t}.
$$
Note that we can initialize $\bx^{(1)}$ such that $g(\bx^{(1)})=0$ to simplify the result.
\item If $ \frac{\sum_{t=1}^{T} \eta_t^2}{\sum_{t=1}^{T} \eta_t} \to 0 $ as $ T \to \infty $, then $ \Fbest^{(T)} \to F(\bx^*) $ as $ T \to \infty $.

\item If $
\eta_t = \frac{\sqrt{2\alpha}}{L \sqrt{t+1}}
$,~\footnote{Note that the second choice of the stepsize in (iii) of Theorem~\ref{theorem:mirr_best_bound} can not be applied since it might not be a nonincreasing sequence.}
then for all $ T\geq 5 $,
\begin{equation}\label{eq:dynamic_rate}
\Fbest^{(T)} - F(\bx^*) 
\leq 
\frac{L}{\sqrt{2\alpha}} \frac{\frac{\sqrt{\alpha}}{L} g(\bx^{(1)}) + \mathcalD_\phi(\bx^*, \bx^{(1)}) + \ln(T+1)}{\sqrt{T+1}}.
\end{equation}
\end{enumerate}
\end{theoremHigh}
\begin{proof}[of Theorem~\ref{theorem:gmirror_dyn}]
\textbf{(i, ii).}
The result in (i) follows immediately by Lemma~\ref{lemma:itera_ineq_gmirr}, which in turn concludes the claim in (ii).
\paragraph{(iii).}
By Lemma~\ref{lemma:itera_ineq_gmirr} and the choice of the stepsizes, we have 
$$
\Fbest^{(T)} - F(\bx^*) 
\leq 
\frac{ \frac{\sqrt{\alpha}}{L} g(\bx^{(1)}) + \mathcalD_\phi(\bx^*, \bx^{(1)}) + \frac{1}{2\alpha} \sum_{t=1}^T \eta_t^2 \normtwo{f'(\bx^\toptzero)}^2}{\sum_{t=1}^T \eta_t}.
$$
Since $\normtwo{f'(\bx)}\leq L$ by assumption, we have $\eta_t^2 \normtwo{f'(\bx^\toptzero)}^2 \leq \frac{2\alpha}{t+1}$ and $\eta_t = \frac{\sqrt{2\alpha}}{L \sqrt{t+1}}$. Plugging into the above inequality yields that
$$
\Fbest^{(T)} - F(\bx^*) 
\leq \frac{L}{\sqrt{2\alpha}} \frac{ \frac{\sqrt{\alpha}}{L} g(\bx^{(1)}) + \mathcalD_\phi(\bx^*, \bx^{(1)}) + \sum_{t=1}^T \frac{1}{t+1}}{\sum_{t=1}^T \frac{1}{\sqrt{t+1}}}.
$$
Using Lemma~\ref{lemma:pgd_lem2} concludes the result in (iii).
\end{proof}









\begin{problemset}
\item Prove the equivalence of the three definitions of conditional gradient norm in Definition~\ref{definition:cond_grad_norm}.

\item \label{prob:grad_map_lipschitz} \textbf{Lipschitzness of gradient mapping.} Let $ f $ be a proper \textcolor{black}{closed} and $\beta$-smooth function  and  $ g $ be a proper closed and convex function satisfying assumption (A1) and (A2) in Assumption~\ref{assumption:proximal_grad}. Let further $L\in \left(\frac{\beta}{2}, \infty\right)$. 
Show that $\mathcalG_L(\bx)$ is $(2L+\beta)$-Lipschitz: $\normtwo{\mathcalG_L(\bx) - \mathcalG_L(\by)} \leq (2L+\beta) \normtwo{\bx-\by}$ for any $\bx,\by\in\interior(\dom(f))$.

\item Prove \eqref{equation:gd_des_cor} rigorously.

\item Prove the relation in \eqref{equation:smooth_noneu2}.

\item Verify the equivalent update rules for PGD in \eqref{equation:pgd_decom_raw}.
\end{problemset}

