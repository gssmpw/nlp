\newpage
\chapter{Introduction and Mathematical Tools}\label{chapter_introduction}
\begingroup
\hypersetup{
linkcolor=structurecolor,
linktoc=page,  % page: only the page will be colored; section, all, none etc
}
\minitoc \newpage
\endgroup
\section*{Introduction and Background}
\addcontentsline{toc}{section}{Introduction and Background}


In today's era, where data-driven decisions and computational efficiency are paramount, optimization techniques have become an indispensable cornerstone across various fields including mathematics, computer science, operations research, and machine learning. These methods play a pivotal role in solving complex problems, whether it involves enhancing machine learning models, improving resource allocation, or designing effective algorithms. Aimed at providing a comprehensive yet accessible reference, this book is designed to equip readers with the essential knowledge for understanding and applying optimization methods within their respective domains.

Our primary goal is to unveil the inner workings of optimization algorithms, ranging from black-box optimizers to stochastic ones, through both formal and intuitive explanations. Starting from fundamental mathematical principles, we derive key results to ensure that readers not only learn how these techniques work but also understand when and where to apply them effectively. We strike a careful balance between theoretical depth and practical application to cater to a broad audience, seeking robust optimization strategies.

The book begins by laying down the necessary mathematical foundations, such as linear algebra, inequalities, norms, and matrix decompositions, which form the backbone for discussing optimality conditions, convexity, and various optimization techniques. As the discussion progresses, we delve deeper into first-order and second-order methods, constrained optimization strategies, and stochastic optimization approaches. Each topic is treated with clarity and rigor, supplemented by detailed explanations, examples, and formal proofs to enhance comprehension.

Special emphasis is placed on gradient descent---a quintessential optimization algorithm widely used in machine learning. The text meticulously examines basic gradient descent, momentum-based methods, conjugate gradient techniques, and stochastic optimization strategies, highlighting their historical development and modern applications. Particular attention is given to stochastic gradient descent (SGD) and its crucial role in optimizing deep neural network or transformer structures, especially concerning overcoming computational challenges and escaping saddle points.

At the heart of the book also lies the exploration of constrained optimization, covering methods for handling equality and inequality constraints, augmented Lagrangian methods, alternating direction method of multipliers (ADMM), and trust region methods. Additionally, second-order methods such as Newton's method, quasi-Newton methods, and conjugate gradient methods are discussed, along with insights into least squares problems (both linear and nonlinear) and sparse optimization problems. The final section delves into stochastic optimization, encompassing topics like SGD, variance reduction techniques, and learning rate annealing and warm-up strategies.

Throughout the book, compactness, clarity, and mathematical rigor are prioritized, with detailed explanations, examples, and proofs provided to ensure a profound understanding of optimization principles and their applications. 
The primary aim of this book is to provide a self-contained introduction to the concepts
and mathematical tools used in optimization analysis and complexity, and based on them
to present major optimization algorithms and their applications to the reader.
We clearly realize our inability to cover all the useful and interesting topics concerning
optimization methods. Therefore, we recommend additional literature for more detailed introductions to related areas. Some excellent references include \citet{beck2017first, boyd2004convex, sun2006optimization, bertsekas2015convex}.



In the following sections of this chapter, we provide a concise overview of key concepts in linear algebra and calculus. Further important ideas will be introduced as needed to enhance understanding. It is important to note that this chapter does not aim for an exhaustive exploration of these subjects. Those interested in deeper study should refer to advanced texts on linear algebra and calculus.

\section{Linear Algebra}
\index{Vector space}
\index{Matrix space}
%\paragraph{The vector space $\real^n$ and matrix space $\real^{m\times n}$.}
The vector space $\real^n$ consists of all $n$-dimensional column vectors with real components. Our main focus in this book will be problems within the $\real^n$ vector space, though we will occasionally examine other vector spaces, such as the nonnegative vector space. Similarly, the matrix space $\real^{m\times n}$ comprises all real-valued matrices of dimensions $m\times n$.

Scalars will be represented using non-bold font, possibly with subscripts (e.g., $a$, $\alpha$, $\alpha_i$). Vectors will be denoted by \textbf{bold} lowercase letters, potentially with subscripts (e.g., $\bmu$, $\bx$, $\bx_n$, $\bz$), while matrices will be indicated by \textbf{bold} uppercase letters, also potentially with subscripts (e.g., $\bA$, $\bL_j$). 
The $i$-th element of a vector $\bz$ will be written as $z_i$ in  non-bold font.
While a matrix $\bA\in\real^{m\times n}$, with $(i,j)$-th element being $a_{ij}$, can be denoted as $\bA=\{a_{ij}\}$ or $\bA=\{a_{ij}\}_{i,j=1}^{m,n}$.


When we fix a subset of indices, we form subarrays. Specifically, the entry at the $i$-th row and $j$-th column value of matrix $\bA$ (entry ($i,j$) of $\bA$) is denoted by  $a_{ij}$. 
We also adopt \textbf{Matlab-style notation}: the submatrix of   $\bA$ spanning from the $i$-th row to the $j$-th row and from the $k$-th column to the $m$-th column is written as $\bA_{i:j,k:m}=\bA[i:j,k:m]$. 
A colon indicates all elements along a dimension; for example,  $\bA_{:,k:m}=\bA[:,k:m]$ represents columns column $k$ through $m$  of  $\bA$, and $\bA_{:,k}$ denotes the $k$-th column of $\bA$. Alternatively, the $k$-th column of  $\bA$ can be more compactly written as $\ba_k$. 


\index{Matlab-style notation}

When the indices are non-continuous, given ordered subindex sets $\sI$ and $\sJ$, $\bA[\sI, \sJ]$ denotes the submatrix of $\bA$ formed by selecting the rows and columns of $\bA$ indexed by $\sI$ and $\sJ$, respectively.
Similarly, $\bA[:, \sJ]$ indicates the submatrix of $\bA$ obtained by extracting all rows from $\bA$ and only the columns specified by $\sJ$, where again the colon operator signifies that all indices along that dimension are included. 
%, and  the $[:, \sJ]$ syntax in this expression selects all rows from $\bA$ and only the columns specified by the indices in $\sJ$.

\begin{definition}[Matlab Notation]\label{definition:matlabnotation}
Let $\bA\in \real^{m\times n}$, and $\sI=\{i_1, i_2, \ldots, i_k\}$ and $\sJ=\{j_1, j_2, \ldots, j_l\}$ be two index sets. Then, $\bA[\sI,\sJ]$ represents   the $k\times l$ submatrix
$$
\bA[\sI,\sJ]=
\begin{bmatrix}
a_{i_1,j_1} & a_{i_1,j_2} &\ldots & a_{i_1,j_l}\\
a_{i_2,j_1} & a_{i_2,j_2} &\ldots & a_{i_2,j_l}\\
\vdots & \vdots&\ddots & \vdots\\
a_{i_k,j_1} & a_{i_k,j_2} &\ldots & a_{i_k,j_l}\\
\end{bmatrix}.
$$
Similarly, $\bA[\sI,:]$ denotes a $k\times n$ submatrix, and $\bA[:,\sJ]$ denotes a $m\times l$ submatrix.
For a vector $\bv\in\real^m$, $\bv_{\sI}\triangleq\bv[\sI]$ denotes a $\real^k$ subvector, and $\bv_{\comple{\sI}}\triangleq\bv[\comple{\sI}]$ denotes a $\real^{m-k}$ subvector, where $\comple{\sI}$ is the complement of  set $\sI$: $\comple{\sI} \triangleq\{1,2,\ldots,m\}\backslash\sI$.

Note that it does not matter whether the index sets $\sI$ and $\sJ$ are row vectors or column vectors; what's crucial is which axis they index  (either rows  or columns of $\bA$).
Additionally, the ranges of the indices are given as follows:
$$
\left\{
\begin{aligned}
0&\leq \min(\sI) \leq \max(\sI)\leq m;\\
0&\leq \min(\sJ) \leq \max(\sJ)\leq n.
\end{aligned}
\right.
$$
\end{definition}


In all cases, vectors are presented in column form rather than as rows. A row vector will be denoted by the transpose of a column vector, such as $\ba^\top$. 
A column vector with specific values is separated by the semicolon symbol $``;"$, for instance, $\bx=[1;2;3]$ is a column vector in $\real^3$. 
Similarly, a  row vector with specific values is split by the comma symbol $``,"$, e.g., $\by=[1,2,3]$ is a row vector with 3 values. 
Additionally, a column vector can also be represented as the transpose of a row vector, e.g., $\by=[1,2,3]^\top$ is also a column vector.

The transpose of a matrix $\bA$ will be denoted as $\bA^\top$, and its inverse as $\bA^{-1}$. 
We  denote the $p \times p$ identity matrix by $\bI_p$ (or simply by $\bI$ when the size is clear from context). A vector or matrix of all zeros will be denoted by a {boldface} zero, $\bzero$, with the size inferred from context, or we denote $\bzero_p$ to be the vector of all zeros with $p$ entries.
Similarly, a vector or matrix of all ones will be denoted by a {boldface} one, $\bone$, whose size is inferred  from  context, or we denote $\bone_p$ to be the vector of all ones with $p$ entries.
We  frequently omit the subscripts of these matrices when the dimensions are clear from  context.

We   use $\be_1, \be_2, \ldots, \be_n$ to represent the standard (unit) basis of $\real^n$, where $\be_i$ is the vector whose $i$-th component is one while all  others are zero.

\begin{definition}[Nonnegative Orthant, Positive Orthant, and Unit-Simplex]\label{definition:simplex}
The \textit{nonnegative orthant} is a subset of $\real^n$ that consists of all vectors in $\real^n$ with nonnegative components and is denoted by $\real_+^n$: 
$$
\real_+^n=\{[x_1, x_2, \ldots, x_n]^\top\in\real^n: x_1, x_2, \ldots, x_n\geq 0\}.
$$
Similarly, the \textit{positive orthant} comprises all vectors in $\real^n$ with strictly positive components and is denoted by $\real_{++}^n$:
$$
\real_{++}^n=\{[x_1, x_2, \ldots, x_n]^\top\in\real^n: x_1, x_2, \ldots, x_n> 0\}.
$$
The \textit{unit-simplex} (or simply simplex) is a subset of $\real^n$ comprising all nonnegative vectors whose components sum to one:
$$
\Delta_n=\{\bx=[x_1, x_2, \ldots, x_n]^\top\in\real^n:  x_1, x_2, \ldots, x_n\geq  0,
\,\, \sum_{i=1}^{n}x_i=1 \}.
$$
\end{definition}




\index{Eigenvalue}
\index{Eigenvector}
\begin{definition}[Eigenvalue, Eigenvector]
Given any vector space $\sF$ and a linear map $\bA: \sF \rightarrow \sF$ (or simply a real matrix $\bA\in\real^{n\times n}$), a scalar $\lambda \in \sK$ is called an \textit{eigenvalue, or proper value, or characteristic value} of $\bA$, if there exists some nonzero vector $\bu \in \sF$ such that
\begin{equation*}
\bA \bu = \lambda \bu.
\end{equation*}
The vector $\bu$ is then called an \textit{eigenvector} of $\bA$ associated with $\lambda$.
\end{definition}
The pair $(\lambda, \bu)$ is termed an \textit{eigenpair}. Intuitively, these definitions imply that multiplying matrix $\bA$ by the vector $\bu$ results in a new vector that is in the same direction as $\bu$, but its length is scaled by $\lambda$ (an eigenvector $\bu$ of a matrix $\bA$ represents a direction that remains unchanged when transformed into the coordinate system defined by the columns of $\bA$.). Any eigenvector $\bu$ can be scaled by a scalar $s$ such that $s\bu$ remains an eigenvector of $\bA$. To avoid ambiguity, it is common practice to normalize eigenvectors to have unit length and ensure the first entry is positive (or negative), since both $\bu$ and $-\bu$ are valid eigenvectors.
Note that real-valued matrices can have complex eigenvalues, but all eigenvalues of symmetric matrices are real (see Theorem~\ref{theorem:spectral_theorem}).

In linear algebra, every vector space has a basis, and any vector within the space can be expressed as a linear combination of the basis vectors. We define the span and dimension of a subspace using the basis.


\index{Subspace}
\begin{definition}[Subspace]
A nonempty subset $\mathcalV$ of $\real^n$ is called a \textit{subspace} if $x\ba+y\ba\in \mathcalV$ for every $\ba,\bb\in \mathcalV$ and every $x,y\in \real$.
\end{definition}


\index{Span}
\begin{definition}[Span]
If every vector $\bv$ in subspace $\mathcalV$ can be expressed as a linear combination of $\{\ba_1, \ba_2, \ldots,$ $\ba_m\}$, then $\{\ba_1, \ba_2, \ldots, \ba_m\}$ is said to \textit{span} $\mathcalV$.
\end{definition}


\index{Linearly independent}
In this context, we frequently use the concept of linear independence for a set of vectors. Two equivalent definitions are given below.
\begin{definition}[Linearly Independent]
A set of vectors $\{\ba_1, \ba_2, \ldots, \ba_m\}$ is said to be \textit{linearly independent} if there is no combination that  can yield $x_1\ba_1+x_2\ba_2+\ldots+x_m\ba_m=0$ unless all $x_i$'s are equal to  zero. An equivalent definition states that $\ba_1\neq \bzero$, and for every $k>1$, the vector $\ba_k$ does not belong to the span of $\{\ba_1, \ba_2, \ldots, \ba_{k-1}\}$.
\end{definition}


\index{Basis}
\index{Dimension}
\begin{definition}[Basis and Dimension]
A set of vectors $\{\ba_1, \ba_2, \ldots, \ba_m\}$ forms a \textit{basis} of a subspace $\mathcalV$ if they are linearly independent and span $\mathcalV$. Every basis of a given subspace contains the same number of vectors, and this number is called the \textit{dimension} of the subspace $\mathcalV$. By convention, the subspace $\{\bzero\}$ is defined to have a dimension of zero. Additionally, every subspace of nonzero dimension has an orthogonal basis, meaning that a basis for the subspace can always be chosen to be orthogonal.
\end{definition}

\index{Column space}
\begin{definition}[Column Space (Range)]
If $\bA$ is an $m \times n$ real matrix, its \textit{column space (or range)} is the set of all linear combinations of its columns, formally defined as:
\begin{equation*}
\mathcalC (\bA) = \{ \by\in \real^m: \exists\, \bx \in \real^n, \, \by = \bA \bx \}.
\end{equation*}
Similarly, the \textit{row space} of $\bA$ is the set spanned by its rows, which is equivalent to the column space of $\bA^\top$:
\begin{equation*}
\mathcalC (\bA^\top) = \{ \bx\in \real^n: \exists\, \by \in \real^m, \, \bx = \bA^\top \by \}.
\end{equation*}
\end{definition}

\index{Null space}
\begin{definition}[Null Space (Nullspace, Kernel)]\label{definition:nullspace}
If $\bA$ is an $m \times n$ real matrix, its \textit{null space} (also called the \textit{kernel} or \textit{nullspace}) is the set of all vectors that are mapped to zero by $\bA$:
\begin{equation*}
\nspace (\bA) = \{\by \in \real^n:  \, \bA \by = \bzero \}.
\end{equation*}
Similarly, the null space of $\bA^\top$ is defined as 	
\begin{equation*}
\nspace (\bA^\top) = \{\bx \in \real^m:  \, \bA^\top \bx = \bzero \}.
\end{equation*}
\end{definition}

Both the column space of $\bA$ and the null space of $\bA^\top$ are subspaces of $\real^m$. In fact, every vector in $\nspace(\bA^\top)$ is orthogonal  to every vector in $\cspace(\bA)$, and vice versa. Similarly, every vector in $\nspace(\bA)$ is also perpendicular to every vector in $\cspace(\bA^\top)$, and vice versa.

\index{Rank}
\begin{definition}[Rank]
The \textit{rank} of a matrix $\bA\in \real^{m\times n}$ is the dimension of its column space. 
In other words, the rank of $\bA$ is the maximum number of linearly independent columns of $\bA$, which is also equal to the maximum number of linearly independent rows of $\bA$. 
A fundamental property of matrices is that $\bA$ and its transpose $\bA^\top$ always have the same rank. 
A matrix $\bA$ is considered to have \textit{full rank} if its rank is equal to $min\{m,n\}$.

A specific example of rank computation arises when considering the outer product of two vectors. Given a vector $\bu \in \real^m$ and a vector $\bv \in \real^n$, then the $m\times n$ matrix $\bu\bv^\top$ formed by their outer product always has rank 1. In short, the rank of a matrix is equal to:
\begin{itemize}
\item The number of linearly independent columns.
\item The number of linearly independent rows.
\end{itemize}
These two quantities are always equal (see Theorem~\ref{theorem:fundamental-linear-algebra}).
\end{definition}

\index{Orthogonal complement}
\begin{definition}[Orthogonal Complement in General]
The \textit{orthogonal complement} of a subspace $\mathcalV\subseteq\real^m$, denoted as $\mathcalV^\perp\subseteq \real^m$, consists of all vectors in $\real^m$ that are perpendicular to every vector in $\mathcalV$. Formally,
$$
\mathcalV^\perp = \{\bv\in \real^m : \bv^\top\bu=0, \,\,\, \forall \bu\in \mathcalV  \}.
$$
These two subspaces are mutually exclusive yet collectively span the entire space.
The dimensions of $\mathcalV$ and $\mathcalV^\perp$ sum up to the dimension of the entire space. Furthermore, it holds that $(\mathcalV^\perp)^\perp=\mathcalV$.
\end{definition}

\index{Orthogonal complement}
\begin{definition}[Orthogonal Complement of Column Space]
For an  $m \times n$ real matrix $\bA$, the orthogonal complement of its column space $\mathcalC(\bA)$, denoted as $\mathcalC^{\bot}(\bA)$, is the subspace defined as:
\begin{equation*}
\begin{aligned}
\mathcalC^{\bot}(\bA) &= \{\by\in \real^m: \, \by^\top \bA \bx=\bzero, \, \forall \bx \in \real^n \} \\
&=\{\by\in \real^m: \, \by^\top \bv = \bzero, \, \forall \bv \in \mathcalC(\bA) \}.
\end{aligned}
\end{equation*}
\end{definition}
We can then identify the four fundamental subspaces associated with any matrix $\bA\in \real^{m\times n}$ of rank $r$:
\begin{itemize}
\item $\cspace(\bA)$: Column space of $\bA$, i.e., linear combinations of columns, with dimension $r$.

\item $\nspace(\bA)$: Null space of $\bA$, i.e., all $\bx$ satisfying $\bA\bx=\bzero$, with dimension $n-r$.

\item  $\cspace(\bA^\top)$: Row space of $\bA$, i.e., linear combinations of rows, with dimension $r$.

\item  $\nspace(\bA^\top)$: Left null space of $\bA$, i.e., all $\by$ satisfying $\bA^\top \by=\bzero$, with dimension $m-r$.
\end{itemize}
Furthermore, $\nspace(\bA)$ is the orthogonal complement of $\cspace(\bA^\top)$, and $\cspace(\bA)$ is the orthogonal complement of $\nspace(\bA^\top)$; see Theorem~\ref{theorem:fundamental-linear-algebra}.

\index{Fundamental subspace}

\index{Orthogonal matrix}
\begin{definition}[Orthogonal Matrix, Semi-Orthogonal Matrix]
A real square matrix $\bQ\in\real^{n\times n}$ is called an \textit{orthogonal matrix} if its inverse is equal to its  transpose, that is, $\bQ^{-1}=\bQ^\top$ and $\bQ\bQ^\top = \bQ^\top\bQ = \bI$. 
In other words, suppose $\bQ=[\bq_1, \bq_2, \ldots, \bq_n]$, where $\bq_i \in \real^n$ for all $i \in \{1, 2, \ldots, n\}$, then $\bq_i^\top \bq_j = \delta(i,j)$, where $\delta(i,j)$ is the Kronecker delta function. 
One important property of an orthogonal matrix is that it preserves vector norms: $\norm{\bQ\bx}= \norm{\bx}$ for all $\bx\in\real^n$.

If $\bQ$ contains only $\gamma$ of these columns with $\gamma<n$, then $\bQ^\top\bQ = \bI_\gamma$ stills holds, where $\bI_\gamma$ is the $\gamma\times \gamma$ identity matrix. 
But $\bQ\bQ^\top=\bI$ will not hold. 
In this case, $\bQ$ is called \textit{semi-orthogonal}.
\end{definition}





From an introductory course on linear algebra, we have the following remark regarding  the equivalent claims of nonsingular matrices.
\begin{remark}[List of Equivalence of Nonsingularity for a Matrix]
For a square matrix $\bA\in \real^{n\times n}$, the following claims are equivalent:
\begin{itemize}
\item $\bA$ is nonsingular;~\footnote{The source of the name is a result of the singular value decomposition (SVD); see, for example, \citet{lu2021numerical}.}
\item $\bA$ is invertible, i.e., $\bA^{-1}$ exists; 
\item $\bA\bx=\bb$ has a unique solution $\bx = \bA^{-1}\bb$;
\item $\bA\bx = \bzero$ has a unique, trivial solution: $\bx=\bzero$;
\item Columns of $\bA$ are linearly independent;
\item Rows of $\bA$ are linearly independent;
\item Determinant $\det(\bA) \neq 0$; 
\item Dimension $\dim(\nspace(\bA))=0$;
\item $\nspace(\bA) = \{\bzero\}$, i.e., the null space is trivial;
\item $\cspace(\bA)=\cspace(\bA^\top) = \real^n$, i.e., the column space or row space spans the entire $\real^n$;
\item $\bA$ has full rank $r=n$;
\item The reduced row echelon form is $\bR=\bI$;
\item $\bA^\top\bA$ is symmetric positive definite;
\item $\bA$ has $n$ nonzero (positive) singular values;
\item All eigenvalues are nonzero.
\end{itemize}
\end{remark}

These equivalences are fundamental in linear algebra and will be useful in various contexts. Conversely, the following remark establishes the equivalent conditions for a singular matrix.
\begin{remark}[List of Equivalence of Singularity for a Matrix]
For a square matrix $\bA\in \real^{n\times n}$ with eigenpair $(\lambda, \bu)$, the following claims are equivalent:
\begin{itemize}
\item $(\bA-\lambda\bI)$ is singular; 
\item $(\bA-\lambda\bI)$ is not invertible;
\item $(\bA-\lambda\bI)\bx = \bzero$ has  nonzero  solutions $\bx\neq \bzero$, and $\bx=\bu$ is one of such solutions;
\item $(\bA-\lambda\bI)$ has linearly dependent columns;
\item Determinant $\det(\bA-\lambda\bI) = 0$; 
\item Dimension $\dim(\nspace(\bA-\lambda\bI))>0$;
\item Null space of $(\bA-\lambda\bI)$ is nontrivial;
\item Columns of $(\bA-\lambda\bI)$ are linearly dependent;
\item Rows of $(\bA-\lambda\bI)$ are linearly dependent;
\item $(\bA-\lambda\bI)$ has rank $r<n$;
\item Dimension of column space = dimension of row space = $r<n$;
\item $(\bA-\lambda\bI)^\top(\bA-\lambda\bI)$ is symmetric semidefinite;
\item $(\bA-\lambda\bI)$ has $r<n$ nonzero (positive) singular values;
\item Zero is an eigenvalue of $(\bA-\lambda\bI)$.
\end{itemize}
\end{remark}

%\index{Linear system}
%\begin{remark}[Linear System Solution]
%Given a linear system $\bA\bx = \bb$ with $\bA\in\real^{m\times n}$.
%\begin{itemize}
%	\item When $\bA$ has full column rank $n$, the linear system has a  unique solution: $\widehat{\bx} = (\bA^\top\bA)^{-1}\bA^\top \bb$. Refer to its description in the left inverse (Proposition~\ref{proposition:unique-linear-system-solution}, p.~\pageref{proposition:unique-linear-system-solution}).
%	\item When $\bA$ has full row rank $m$, the linear system has at least one solution: $\widehat{\bx} = \bA_R^{-1}\bb$, where $\bA_R^{-1}$ is a right inverse of $\bA$. Refer to its description in the right inverse (Proposition~\ref{proposition:always-have-solution-right-inverse}, p.~\pageref{proposition:always-have-solution-right-inverse}).
%\end{itemize}
%\end{remark}

%\index{Vector norm}
%\index{Matrix norm}
%\begin{definition}[Vector $\ell_p$ Norm]\label{definition:vec_l2_norm}
%For a vector $\bx\in\real^n$, the \textit{$\ell_p$ vector norm} is defined as $\norm{\bx}_p = \sqrt[p]{\abs{x_1}^p+\abs{x_2}^p+\ldots+\abs{x_n}^p} $.
%\end{definition}
%
%For a matrix $\bA\in\real^{m\times n}$, we  define the (matrix) Frobenius norm as follows.
%\begin{definition}[Matrix Frobenius Norm\index{Frobenius norm}]\label{definition:frobernius-in-svd}
%The \textit{Frobenius norm} of a matrix $\bA\in \real^{m\times n}$ is defined as 
%$$
%\norm{\bA}_F = \sqrt{\sum_{i=1,j=1}^{m,n} (a_{ij})^2}=\sqrt{\trace(\bA\bA^\top)}=\sqrt{\trace(\bA^\top\bA)} = \sqrt{\sigma_1^2+\sigma_2^2+\ldots+\sigma_r^2}, 
%$$
%where $\sigma_1, \sigma_2, \ldots, \sigma_r$ are nonzero singular values of $\bA$.
%\end{definition}
%
%The spectral norm is defined as follows.
%\begin{definition}[Matrix Spectral Norm]\label{definition:spectral_norm}
%The \textit{spectral norm} of a matrix $\bA\in \real^{m\times n}$ is defined as 
%$$
%\normtwo{\bA} = \mathop{\max}_{\bx\neq\bzero} \frac{\normtwo{\bA\bx}}{\normtwo{\bx}}  =\mathop{\max}_{\bu\in \real^n: \norm{\bu}_2=1}  \normtwo{\bA\bx} ,
%$$
%which is also the maximal singular value of $\bA$, i.e., $\normtwo{\bA} = \sigma_1(\bA)$.
%\end{definition}
%
%We note that the Frobenius norm serves as the matrix counterpart of vector $\ell_2$ norm.
%For simplicity, we do not give the full subscript of the norm for the vector $\ell_2$ norm or Frobenius norm when it is clear from the context which one we are referring to: $\norm{\bA}=\norm{\bA}_F$ and $\norm{\bx}=\normtwo{\bx}$.
%However, for the spectral norm, the subscript $\normtwo{\bA}$ should \textbf{not} be omitted.

\section{Well-Known Inequalities}\label{section:inequalities}
In this section, we introduce several well-known inequalities that will be frequently used throughout the discussion.

The AM-GM inequality is a fundamental tool in competitive mathematics, particularly useful for determining the maximum or minimum values of multivariable functions or expressions. It establishes a relationship between  the \textit{arithmetic mean (AM)} and the \textit{geometric mean (GM)}. \index{AM-GM inequality}
\begin{theorem}[AM-GM Inequality]
For  any nonnegative real numbers $x_1,x_2,\ldots,x_n$,  the following inequality holds:
$$
\frac{\sum_{i=1}^{n} x_i}{n} \geq \sqrt[n]{\prod_{i=1}^{n}x_i}.
$$ 
That is, the \textit{geometric mean} of a set of nonnegative numbers does not exceed their \textit{arithmetic mean}.
Equality holds if and only if all the numbers are equal.
\end{theorem}

When $n=2$, the AM-GM inequality can be expressed as:
\begin{equation}\label{equation:amgm_ineq}
a^2+b^2\geq 2\sqrt{a^2b^2} = 2\abs{ab}\geq ab.
\end{equation}


\begin{proposition}[Weighted AM-GM Inequality]\label{proposition:weighted_amgm}
For any nonnegative real numbers $x_1,x_2,\ldots,x_n$ and nonnegative weights $w_1,w_2,\ldots, w_n$,  the following inequality holds:
$$
\frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n}w_i} \geq
\sqrt[(\sum w_i)]{\prod_{i=1}^{n}x_i^{w_i}}.
$$ 
When $w_1=w_2=\ldots=w_n=1$, this reduces to  the standard AM-GM inequality. Alternatively, when $\bw\in \Delta_n$, i.e., $\bw$ is a unit-simplex with $\sum_{i=1}^{n}w_i=1$, it follows that 
$$
\sum_{i=1}^{n} w_i x_i \geq
\prod_{i=1}^{n}x_i^{w_i},
\quad\text{where}\quad \bw\in \Delta_n.
$$
\end{proposition}
\begin{proof}[of Proposition~\ref{proposition:weighted_amgm}]
For simplicity, we will only prove the second part, as the first part can be established similarly.
Applying  Jensen's inequality (Theorem~\ref{theorem:jensens_ineq}) to the convex function $f(x)=-\ln (x)$ with $x_1, x_2,  \ldots, x_n>0$ and $\bw \in \Delta_n$, we have 
$
-\ln\left( \sum_{i=1}^{n} w_i x_i\right) \leq -\sum_{i=1}^{n} w_i \ln(\bx_i).
$
Taking the exponent of both sides, we obtain 
$
\sum_{i=1}^{n}w_i x_i \geq \exp^{\sum_{i=1}^{n} w_i \ln(x_i)}
=\prod_{i=1}^{n}x_i^{w_i}.
$
This completes the proof.
\end{proof}









\subsection{Cauchy-Schwarz Inequality}
\index{Cauchy–Schwarz inequality}
The \textit{Cauchy–Schwarz inequality} is one of the most important and widely used inequalities in mathematics.
\begin{proposition}[Cauchy-Schwarz Matrix (Vector) Inequality]\label{proposition:cauchy-schwarz-inequ}
For any $m\times n$ matrices $\bX$ and $\bY$, we have 
$$
\normbig{\bX^\top \bY} \leq \Vert\bX\Vert \cdot \Vert\bY\Vert.
$$
This is a special form of the Cauchy-Schwarz inequality, where the inner product is defined as $\langle\bX,\bY\rangle = \norm{\bX^\top \bY}$.
Similarly, for any vectors $\bu$ and $\bv$, we have 
\begin{equation}\label{equation:vector_form_cauchyschwarz}
\absbig{\bu^\top \bv} \leq \normtwo{\bu} \cdot  \normtwo{\bv},
\end{equation}
where the equality holds if and only if $\bu$ and $\bv$ are linearly dependent.
In the two-dimensional case, it becomes
$$
(ac+bd)^2 \leq (a^2 +b^2)(c^2+d^2).
$$
\end{proposition}
The vector form of the Cauchy-Schwarz inequality plays an important role in various branches of modern mathematics, including Hilbert space theory and numerical analysis \citep{wu2009various}. For simplicity, we provide only the proof for the vector form of the Cauchy-Schwarz inequality. 
To see this, given two vectors $\bu,\bv\in \real^n$, we have 
$$
\begin{aligned}
&0\leq \sum_{i=1}^{n}\sum_{j=1}^{n} (u_i v_j - u_j v_i)^2 = 
\sum_{i=1}^{n}\sum_{j=1}^{n} u_i^2v_j^2 + \sum_{i=1}^{n}\sum_{j=1}^{n} v_i^2 u_j^2 - 2\sum_{i=1}^{n}\sum_{j=1}^{n} u_iu_j v_iv_j\\
&=\left(\sum_{i=1}^{n} u_i^2\right) \left(\sum_{j=1}^{n} v_j^2\right) +
\left(\sum_{i=1}^{n} v_i^2\right) \left(\sum_{j=1}^{n} u_j^2\right) - 
2\left(\sum_{i=1}^{n} u_iv_i \right)^2
=2 \normtwo{\bu}^2 \cdot \normtwo{\bv}^2 -2 \abs{\bu^\top\bv}^2,
\end{aligned}
$$
from which the result follows.
The equality holds if and only if $\bu = k\bv$ for some constant $k\in \real$, i.e., $\bu$ and $\bv$ are linearly dependent.



\paragraph{Angle between two vectors.} From Equation~\eqref{equation:vector_form_cauchyschwarz}, given two vectors $\bx,\by$, we note that 
$$
-1\leq 
\frac{\bx^\top\by}{\Vert\bx\Vert_2 \Vert\by\Vert_2} 
\leq 1.
$$
This two-side inequality illustrates the concept of the angle between two vectors.
\begin{definition}[Angle Between Vectors]\label{definition:angle_bet_vec_ineq}
The angle between two vectors $\bx$ and $\by$ is the number $\theta\in [0,\pi]$ such that 
$$
\cos (\theta) = \frac{\bx^\top\by}{\Vert\bx\Vert_2 \Vert\by\Vert_2} .
$$
\end{definition}
The definition of the angle between vectors will be useful in the discussion of line search strategies (Section~\ref{section:gd_conv_line_search}).

%\subsection{Generalized Cauchy-Schwarz}
Starting from the vector form of the Cauchy-Schwarz inequality, let $x_i\triangleq u_i^2$, $y_i\triangleq v_i^2$ for all $i=\{1,2,\ldots,n\}$, where $\bu,\bv\in\real^n$ in Equation~\eqref{equation:vector_form_cauchyschwarz}. We then obtain:
\begin{equation}\label{equation:gen_ca_sc_1}
\left( \sum_{i=1}^{n} x_i \right)^{1/2} \left( \sum_{i=1}^{n} y_i \right)^{1/2}
\geq  \sum_{i=1}^{n} (x_iy_i)^{1/2} .
\end{equation}
More generally, we have the generalized Cauchy-Schwarz inequality.
\begin{theorem}[Generalized Cauchy-Schwarz Inequality]
Given a set of vectors $\ba,\bb,\ldots, \bz\in \real^n$, and weights $\lambda_1, \lambda_2, \ldots, \lambda_z$ with $\lambda_1+\lambda_2+\ldots+\lambda_z=1$, it follows that 
$$
\small
\begin{aligned}
& \left( \sum_{i=1}^{n} a_i \right)^{\lambda_a}
\left( \sum_{i=1}^{n} b_i \right)^{\lambda_b}
\ldots
\left( \sum_{i=1}^{n} z_i \right)^{\lambda_z}
\geq 
(a_1^{\lambda_a} b_1^{\lambda_b} \ldots z_1^{\lambda_z})+
(a_2^{\lambda_a} b_2^{\lambda_b} \ldots z_2^{\lambda_z})+
\ldots +
(a_n^{\lambda_a} b_n^{\lambda_b} \ldots z_n^{\lambda_z}).
\end{aligned}
$$
The equality holds if $a_i=b_i=\ldots=z_i$ for all $i\in\{1,2,\ldots,n\}$.
\end{theorem}



%\index{Schur's inequality}
%\subsection{Schur's Inequality}
%Schur's inequality relates three nonnegative real numbers.
%\begin{theorem}[Schur's Inequality]\label{theorem:schurs_real_inequality}
%Given nonnegative real numbers $x, y, z$, and a positive real number $t$, it follows that 
%\begin{equation}\label{equation:sschurs_real_inequality}
%x^t(x-y)(x-z) + y^t(y-x)(y-z) +z^t(z-x)(z-y) \geq 0.
%\end{equation}
%The equality holds if and only if $x=y=z$, or if two of $x,y,z$ are equal and the third is 0.
%When $t=1$, it becomes 
%$$
%x^3+y^3+z^3 + 3xyz \geq xy(x+y) + xz(x+z) + yz(y+z).
%$$
%\end{theorem}
%\begin{proof}[of Theorem~\ref{theorem:schurs_real_inequality}]
%We note the inequality is symmetric in the variables $x,y,z$. Without loss of generality, suppose $x\geq y\geq z$. We factor the left-hand side of Equation~\eqref{equation:sschurs_real_inequality} as 
%$
%(x-y)\bigg(x^t (x-z) - y^t(y-z) \bigg) + z^t (z-x)(z-y).
%$
%We note that $(x-y)\geq 0 $, $\big(x^t (x-z) - y^t(y-z) \big) \geq 0$, and $z^t (z-x)(z-y)\geq 0$. This completes the proof.
%\end{proof}

\subsection{Young's Inequality}
Young's inequality is a special case of the weighted AM-GM inequality (Proposition~\ref{proposition:weighted_amgm}) and has a wide range of applications.
\begin{theorem}[Young's Inequality]\label{theorem:holder-inequality1}
For nonnegative numbers $x,y \geq 0$, and positive real numbers $p,q>1$ with $\frac{1}{p}+\frac{1}{q} = 1$, it follows that
\begin{equation}\label{equation:holder-v1}
xy \leq  \frac{1}{p} x^p +\frac{1}{q} y^q.
\end{equation}
\end{theorem}

\begin{figure}[h]
\centering
\vspace{-0.35cm}
\subfigtopskip=2pt
\subfigbottomskip=2pt
\subfigcapskip=-5pt
\subfigure[Case 1: $x< y^{1/(p-1)}$ and $p\geq 2$.]{\label{fig:holder1}
\includegraphics[width=0.38\linewidth]{./imgs/holder1.pdf}}
\quad 
\subfigure[Case 2: $x\geq  y^{1/(p-1)}$ and $p\geq 2$.]{\label{fig:holder2}
\includegraphics[width=0.38\linewidth]{./imgs/holder2.pdf}}
\subfigure[Case 3: $x< y^{1/(p-1)}$ and $1<p< 2$.]{\label{fig:holder3}
\includegraphics[width=0.38\linewidth]{./imgs/holder3.pdf}}
\quad 
\subfigure[Case 4: $x\geq  y^{1/(p-1)}$ and $1<p< 2$.]{\label{fig:holder4}
\includegraphics[width=0.38\linewidth]{./imgs/holder4.pdf}}
\caption{Demonstration of Young's inequality for different cases.}
\label{fig:holderineqd}
\end{figure}
\begin{proof}[of Theorem~\ref{theorem:holder-inequality1}]
The area $xy$ is bounded above by the sum of the areas of the two trapezoids with curved edges (the shaded regions $A$ and $B$) as shown in Figure~\ref{fig:holderineqd}:
$$
\mathrm{area } \,\,\,B =\int_0^x x^{p-1} dx, \qquad \mathrm{area } \,\,\,A=\int_{0}^y y^{1/(p-1)}dy.
$$
That is, 
$$
\small
\begin{aligned}
xy &\leq \int_0^x x^{p-1} dx + \int_{0}^y y^{1/(p-1)}dy 
= \frac{1}{p} x^p + \int_{0}^y y^{q/p}dy 
= \frac{1}{p} x^p +  (\frac{q}{p}+1)^{-1} y^{q/p+1} 
= \frac{1}{p} x^p +\frac{1}{q} y^q.
\end{aligned}
$$
This completes the proof.
\end{proof}

There are several conceptually different ways to prove Young's inequality. As an alternative, we can use the \textit{interpolation inequality} as follows.
\begin{lemma}[Interpolation Inequality for Exponential Functions]\label{lemma:interpolation_inequality}
For $t\in [0,1]$, we have 
$$
e^{tx+(1-t)y} \leq t e^x + (1-t)e^y. 
$$
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:interpolation_inequality}]
The function of the scant line through the points $(x,e^x)$ and $(y, e^y)$ on the graph of $f(a)=e^a$ is (see Figure~\ref{fig:older_interpolation}):
$$
f(t) = (tx+(1-t)y, te^x+(1-t)e^y).
$$
Since the function $f(a)=e^a$ is convex, we have 
$$
e^{tx+(1-t)y}\leq t e^x + (1-t)e^y. 
$$
This completes the proof.
\end{proof}


%\begin{figure}[h!]
%\centering
%\includegraphics[width=0.6\textwidth]{./imgs/holder_interpolation.pdf}
%\caption{Demonstration of interpolation inequality for $e^x$.}
%\label{fig:older_interpolation}
%\end{figure}

\begin{SCfigure}%[h]
\centering
\includegraphics[width=0.48\textwidth]{./imgs/holder_interpolation.pdf}
\caption{Demonstration of interpolation inequality for $e^x$.}
\label{fig:older_interpolation}
\end{SCfigure}

Using the interpolation inequality for the exponential function, we provide an alternative proof of Theorem~\ref{theorem:holder-inequality1}.
\begin{proof}[of Theorem~\ref{theorem:holder-inequality1}, an alternative way]
We observe that
$$
\begin{aligned}
xy &= \exp\{\log x+\log y\} 
= \exp\left \{ \frac{q-1}{q} \frac{q}{q-1} \log x + \frac{1}{q} q \log y \right\}.
\end{aligned}
$$
Applying Lemma~\ref{lemma:interpolation_inequality}, we obtain
$$
\begin{aligned}
xy&\leq \frac{q-1}{q} e^{\frac{q}{q-1} \log x} + \frac{1}{q} e^{q\log y} 
= \frac{q-1}{q} x^{\frac{q}{q-1}} + \frac{1}{q} y^q
= \frac{1}{p} x^p +\frac{1}{q}y^q,
\end{aligned}
$$
from which the result follows.
\end{proof}

%We realize that  Young's inequality is a far-reaching generalization of Cauchy's inequality


\subsection{H{\"o}lder's Inequality}

\holders inequality, named after \textit{Otto  H{\"o}lder}, is widely used in optimization, machine learning, and many other fields. 
It is also a generalization of the (vector) Cauchy-Schwarz inequality.

\begin{theorem}[\holders  Inequality]\label{theorem:holder-inequality}
Suppose $p,q>1$ such that $\frac{1}{p}+\frac{1}{q} = 1$. Then,  for any vector $\bx,\by\in \real^n$, we have
$$
\sum_{i=1}^{n}x_i y_i
\leq 
\abs{\sum_{i=1}^{n}x_i y_i}
\leq \sum_{i=1}^{n}\abs{x_i} \abs{y_i} \leq \left(\sum_{i=1}^{n}  \abs{x_i}^p\right)^{1/p}  \left(\sum_{i=1}^{n} \abs{y_i}^q\right)^{1/q}=\norm{\bx}_p\norm{\by}_q,
$$
where $\norm{\bx}_p = \left(\sum_{i=1}^{n}  \abs{x_i}^p\right)^{1/p} $ is known as the \textbf{$\ell_p$ norm} or \textbf{$p$ norm} of the vector $\bx$ (see Section~\ref{section:vector-norm}). The equality holds if the two sequences $\{\abs{x_i}^p\}$ and $\{\abs{y_i}^q\}$ are linearly dependent~\footnote{To be more concrete, the equality attains if and only if $\abs{\bx^\top\by}=\abs{\bx}^\top\abs{\by}$ and 
$$
\left\{
\begin{aligned}
&\abs{\bx}\hadaprod\abs{\by}=\norminf{\by}\abs{\bx}, &\text{if }& p=1; \\ 
&\abs{\bx}\hadaprod\abs{\by}=\norminf{\bx}\abs{\by}, &\text{if }& p=\infty;\\ 
&\norm{\by}_q^{1/p} \abs{\bx}^{\hadaprod1/q} = \norm{\bx}_p^{1/q}\abs{\by}^{\hadaprod1/p}, &\text{if }&1<p<\infty, \\ 
\end{aligned}
\right.
$$
where $\hadaprod$ denotes Hadamard product/power; see \citet{bernstein2008matrix} and the references therein.
}.
When $p=q=2$, this reduces to the vector Cauchy-Schwarz inequality \eqref{equation:vector_form_cauchyschwarz}.
\end{theorem}
\begin{proof}[of Theorem~\ref{theorem:holder-inequality}]
Let $ u \triangleq \frac{\abs{x_i}}{\Vert\bx\Vert_p}$ and $ v \triangleq \frac{\abs{y_i}}{\Vert\by\Vert_q}$. From Equation~\eqref{equation:holder-v1}, it follows that
$$
uv = \frac{\abs{x_i}\abs{y_i}}{\norm{\bx}_p\norm{\by}_q} \leq \frac{1}{p} \frac{\abs{x_i}^p}{\norm{\bx}_p^p} + \frac{1}{q}\frac{\abs{y_i}^q}{\norm{\by}_q^q}, \qquad \forall i \in \{1,2,\ldots,n\}.
$$
Therefore,
$$
\sum_{i=1}^{n} \frac{\abs{x_i}\abs{y_i}}{\norm{\bx}_p\norm{\by}_q} \leq \frac{1}{p\norm{\bx}_p^p} \sum_{i=1}^{n}\abs{x_i}^p  +
\frac{1}{q\norm{\by}_q^q}   \sum_{i=1}^{n}\abs{y_i}^q = \frac{1}{p}+\frac{1}{q} = 1.
$$
That is, 
$
\sum_{i=1}^{n} \abs{x_i}\abs{y_i}  \leq  \norm{\bx}_p\norm{\by}_q.
$
It is trivial that $\sum_{i=1}^{n}x_i y_i\leq \sum_{i=1}^{n}\abs{x_i} \abs{y_i} $,
from which the result follows.
\end{proof}

%\begin{corollary}[Cauchy Inequality]
%When  $p=q=2$ in \holders inequality, we recover the \textit{Cauchy inequality}:
%$$
%\sum_{i=1}^{n} x_i y_i \leq
%\sum_{i=1}^{n} \abs{x_i} \abs{y_i} \leq 
%\sqrt{\sum_{i=1}^{n} \abs{x_i}^2}
%\sqrt{\sum_{i=1}^{n} \abs{y_i}^2}.
%$$
%\end{corollary}

Minkowski's inequality follows immediately from the \holders inequality.
\begin{theorem}[Minkowski's Inequality]\label{theorem:minkoski}
Given nonnegative reals $x_1, x_2, \ldots, x_n\geq 0$ and $y_1, y_2, \ldots, y_n\geq 0$, and $p\geq 1$, we have 
$$
\left( \sum_{i=1}^{n} (x_i+y_i)^p  \right)^{1/p} \leq  
\left( \sum_{i=1}^{n}x_i^p \right)^{1/p}+
\left( \sum_{i=1}^{n}y_i^p \right)^{1/p}.
$$
The equality holds if and only if the sequence $x_1, x_2, \ldots, x_n$ and $y_1, y_2, \ldots, y_n$ are proportional. If $p<1$, the inequality sign is reversed.
\end{theorem}
Minkowski's inequality is essentially  the triangle inequality of $\ell_p$ norms (Section~\ref{section:vector-norm}).
\begin{proof}[of Theorem~\ref{theorem:minkoski}]
If $p=1$, the inequality is immediate. We then assume $p>1$.
Observe that
$$
\sum_{i=1}^{n} (x_i+y_i)^p = 
\sum_{i=1}^{n} x_i (x_i+y_i)^{p-1}
+ 
\sum_{i=1}^{n} y_i(x_i+y_i)^{p-1}.
$$
Using \holders inequality, given $p,q$ such that $p>1$ and $\frac{1}{p}+\frac{1}{q}=1$, we have 
$$
\footnotesize
\begin{aligned}
\sum_{i=1}^{n} x_i (x_i+y_i)^{p-1}
&\leq 
\bigg( \sum_{i=1}^{n} x_i^p \bigg)^{1/p}
\bigg( \sum_{i=1}^{n} (x_i+y_i)^{(p-1)q} \bigg)^{1/q}
=\bigg( \sum_{i=1}^{n} x_i^p \bigg)^{1/p}
\bigg( \sum_{i=1}^{n} (x_i+y_i)^{p} \bigg)^{1/q},\\
\sum_{i=1}^{n} y_i (x_i+y_i)^{p-1}
&\leq 
\bigg( \sum_{i=1}^{n} y_i^p \bigg)^{1/p}
\bigg( \sum_{i=1}^{n} (x_i+y_i)^{(p-1)q} \bigg)^{1/q}
=\bigg( \sum_{i=1}^{n} y_i^p \bigg)^{1/p}
\bigg( \sum_{i=1}^{n} (x_i+y_i)^{p} \bigg)^{1/q}.
\end{aligned}
$$
Combining these results, we obtain
$$
\sum_{i=1}^{n} (x_i+y_i)^p
\leq 
\bigg(
\bigg( \sum_{i=1}^{n} x_i^p \bigg)^{1/p}
+
\bigg( \sum_{i=1}^{n} y_i^p \bigg)^{1/p}
\bigg)
\bigg( \sum_{i=1}^{n} (x_i+y_i)^{p} \bigg)^{1/q}.
$$
This completes the proof.
\end{proof}


\section{Norm}\label{appendix:matrix-norm}
The concept of a norm is essential for evaluating the magnitude of vectors and, consequently, allows for the definition of certain metrics on linear spaces equipped with norms. Norms provide a measure of the magnitude of a vector or matrix, which is useful in many applications, such as determining the length of a vector in Euclidean space or the size of a matrix in a multidimensional setting.
Additionally, norms enable us to define distances between vectors or matrices. 
The distance between two vectors $\bu$ and $\bv$ can be computed using the norm of their difference $\norm{\bu-\bv}$. This is critical for tasks involving proximity measures, such as clustering algorithms in machine learning.~\footnote{We only discuss the norms (and inner products) for real vector or matrix spaces. Most results can be applied directly to complex cases.}
\begin{definition}[Vector Norm and Matrix Nrom\index{Matrix norm}\index{Vector norm}]\label{definition:matrix-norm}
Given a norm $\norm{\cdot}: \real^n\rightarrow \real$ on vectors or a norm $\norm{\cdot}: \real^{m\times n}\rightarrow \real$ on matrices, for any vector $\bx \in \real^{n}$ and  any matrix $\bA \in \real^{m\times n}$, we have~\footnote{When $\norm{\bx}=\bzero$ for some nonzero vector $\bx$, the norm is called a \textit{semi-norm}.}~\footnote{When the vector has a single element, the norm can be understood as the absolute value operation.}~\footnote{When $\norm{\bI}=1$ for a matrix norm, the matrix norm is said to be \textit{normalized}.}
\begin{enumerate}
\item \textit{Positive homogeneity}. $\norm{\lambda \bA} = \abs{\lambda} \cdot \norm{\bA}$ or $\norm{\lambda \bx} = \abs{\lambda} \cdot \norm{\bx}$ for any $\lambda \in \real$.
\item \textit{Triangle inequality, a.k.a., subadditivity}. $\norm{\bA+\bB} \leq \norm{\bA}+\norm{\bB}$, or $\norm{\bx+\by} \leq \norm{\bx}+\norm{\by}$ for any matrices $\bA, \bB\in \real^{m\times n}$ or vectors $\bx,\by\in \real^n$.
\begin{enumerate}
\item The triangle inequality also indicates~\footnote{
$\big| \, \norm{\bx} - \norm{\by}\, \big| \leq \norm{\bx-\by}$ since $$
\left.
\begin{aligned}
\norm{\bx} &= \norm{\bx-\by+\by} \leq \norm{\bx-\by} + \norm{\by};\\
\norm{\by} &= \norm{\by-\bx+\bx} \leq \norm{\by-\bx} + \norm{\bx},\\
\end{aligned}
\right\}
\implies 
\begin{aligned}
\norm{\bx}-\norm{\by}  &\leq \norm{\bx-\by}; \\
\norm{\by} -\norm{\bx} &\leq  \norm{\by-\bx},
\end{aligned}
$$
}
\begin{equation}
\big| \, \norm{\bA} - \norm{\bB}\, \big| \leq \norm{\bA-\bB}
\quad\text{or}\quad
\big| \, \norm{\bx} - \norm{\by}\, \big| \leq \norm{\bx-\by}.
\end{equation}

\end{enumerate}
\item \textit{Nonnegativity}. $\norm{\bA} \geq 0$ or $\norm{\bx}\geq 0$.
\begin{enumerate}
\item  the equality holds if and only if $\bA=\bzero $ or $\bx=\bzero$. 
\end{enumerate}
\end{enumerate}
\end{definition}
\index{Vector norm}
\index{Matrix norm}
The vector space $\real^n$, together with a given norm $\norm{\cdot}$, is called a \textit{normed vector space}.
On the other hand, one way to define norms for matrices is by viewing a matrix $\bA\in\real^{m\times n}$ as a vector in $\real^{mn}$ through its vectorization.
What distinguishes a matrix norm is a property called \textit{submultiplicativity}: $\norm{\bA\bB}\leq \norm{\bA}\norm{\bB}$ if $\norm{\cdot}$ is a submultiplicative matrix norm (see discussions below). Almost all of the matrix norms we discuss are submultiplicative (Frobenius norm in Proposition~\ref{propo:submul_frob} and spectral norm in Proposition~\ref{propo:submul_spec}, both of which are special cases of the \textit{Schatten $p$-norm}, as a consequence of the singular value decomposition \citep{lu2021numerical}).

\index{Orthogonally invariant}
\index{Initarily invariant}
\begin{definition}[Orthogonally Invariant Norms]\label{definition:unitarily_invaria}
A matrix norm on $\real^{m\times n}$ is \textit{orthogonally invariant} if $\norm{\bU\bA\bV} =\norm{\bA}$ for all orthogonal $\bU\in\real^{m\times m}$ and $\bV\in\real^{n\times n}$ and for all $\bA\in\real^{m \times n}$; and  it is \textit{weakly orthogonally invariant} if  $\norm{\bU\bA\bU^\top} =\norm{\bA}$ for all orthogonal $\bU\in\real^{m\times m}$ and $\bA\in\real^{m\times m}$ is square.
Similarly, a vector norm on $\real^{n}$ is \textit{orthogonally invariant} if $\norm{\bQ\bx}=\norm{\bx}$ for all orthogonal $\bQ\in\real^{n\times n}$ and for all $\bx\in\real^n$.~\footnote{The term \textit{unitarily invariant} is used more frequently in the literature for complex matrices.}
\end{definition}






%From the above definition on the norm, we have the following property, 
%\begin{equation}
%\big|||\bx|| - ||\by||\big| \leq ||\bx-\by||.
%\end{equation}
%This can be seen that 
%$$
%||\bx|| = ||\bx-\by+\by||\leq ||\bx-\by||+||\by||\leadto ||\bx|| - ||\by|| \leq ||\bx-\by||
%$$
%and 
%$$
%\begin{aligned}
%	&||\by|| = ||\by-\bx+\bx||\leq ||\by-\bx|| + ||\bx|| \leadto ||\by|| - ||\bx||  \leq  ||\by-\bx||.
%\end{aligned}
%$$


\index{Semi-norm}
\index{Semi-inner product}
\index{Inner product}
\begin{definition}[Inner Product]\label{definition:inner_prod}
In most cases, the norm can be derived from the vector \textit{inner product} $\langle\cdot, \cdot\rangle: \real^n\times \real^n\rightarrow \real$ (the inner product of vectors $\bx,\by\in\real^n$ is given by $\langle\bx,\by\rangle$), which satisfies the following three axioms:~\footnote{When $\inner{\bx}{\bx}=0$ for some nonzero $\bx$, the inner product is called a \textit{semi-inner product}.}
\begin{enumerate}
\item \textit{Commutativity}. $\langle\bx,\by\rangle = \langle\by,\bx\rangle$ for any $\bx,\by\in \real^n$. 
\item \textit{Linearity}. $\langle\lambda_1 \bx_1+\lambda_2\bx_2, \by\rangle = \lambda_1\langle\bx,\by\rangle+\lambda_2\langle\bx_2,\by\rangle$ for any $\lambda_1,\lambda_2 \in \real$ and $\bx,\by\in \real^n$.
\item \textit{Nonnegativity}. $\langle \bx,\bx \rangle \geq 0$ for any $\bx \in \real^n$.
\begin{enumerate}
\item $\langle \bx,\bx \rangle = 0$ if and only if $\bx=\bzero$.
\end{enumerate}
\end{enumerate}
Similarly, an inner product for matrices can be defined as a function $\langle\cdot, \cdot\rangle: \real^{m\times n}\times \real^{m\times n}\rightarrow \real$.
\end{definition}

For example, the \textit{Euclidean inner product}, defined as $\inner{\bu}{\bv}=\bu^\top\bv, \forall\bu,\bv\in\real^n$, is an inner product on vectors; the \textit{Frobenius inner product}, defined as $\inner{\bA}{\bB} = \trace(\bA^\top\bB)$, is an inner product on matrices.
Unless stated otherwise, we will use the Euclidean inner product throughout this book.

\begin{exercise}[Cauchy-Schwarz Inequality for Inner Product]
Given an inner product $\langle\cdot, \cdot\rangle: \real^n\times \real^n\rightarrow \real$, show that 
$$
\abs{\langle \bu, \bv\rangle}^2 \leq \inner{\bu}{\bu}\inner{\bv}{\bv} 
\gap 
\text{for all }
\gap 
\bu,\bv\in\real^n.
$$
\textit{Hint: Consider the vector $\bx \triangleq \inner{\bv}{\bv}\bu - \inner{\bu}{\bv}\bv$ and analyze $\inner{\bx}{\bx}\geq 0$.}
\end{exercise}

\begin{exercise}[Norm Derived from Inner Product, and its Property]\label{exercise:norm_innr_pro}
Let $\langle\cdot, \cdot \rangle$ be an inner product on $\real^n\times \real^n$. Then the function $\norm{\cdot}=\langle\cdot, \cdot\rangle^{1/2}:  \real^n\rightarrow [0, \infty)$ is a norm on $\real^n$.
Show that the function satisfies 
\begin{equation}\label{equation:norm_inn_pro}
\textbf{(Parallelogram identity):}\,\, \norm{\bu+\bv}^2+\norm{\bu-\bv}^2 = 2(\norm{\bu}^2+\norm{\bv}^2), \,\, \forall \bu,\bv\in\real^n.
\end{equation} 
(This is known as the \textit{parallelogram identity}: consider a parallelogram in a vector space where the sides are represented by the vectors $\bu$ and $\bv$, the diagonals of this parallelogram are represented by the vectors $\bu+\bv$ and $\bu-\bv$.)
Prove the following \textit{polarization identity}:
\begin{equation}\label{equation:norm_pol_pro}
\textbf{(Polarization identity):} \gap \inner{\bu}{\bv} = \frac{1}{4}(\norm{\bu+\bv}^2-\norm{\bu-\bv}^2).
\end{equation}
\end{exercise}
Therefore, the vector space $\real^n$, when equipped with an inner product $\inner{\cdot}{\cdot}$, is called an \textit{inner product space}, which is also a normed vector space with the derived norm.


\index{$\ell_p$ norm}
\index{$\ell_2$ norm}
\index{$\ell_1$ norm}
\index{$\ell_\infty$ norm}
\subsection{Vector Norm}\label{section:vector-norm}
The vector norm is derived from the definition of the inner product. In most cases, the inner product in $\real^n$ is implicitly  referred to as  the  \textit{dot product (a.k.a., Euclidean inner product)}, defined by 
$
\langle\bx,\by\rangle = \sum_{i=1}^{n} x_i y_i.
$
The $\ell_2$ \textit{norm} (also called the $\ell_2$ vector norm) is induced by the dot product and is given by
$
\normtwo{\bx} = \sqrt{\langle\bx,\bx\rangle} = \sqrt{\sum_{i=1}^{n}x_i^2}.
$
More generally, for any $p\geq 1$ \footnote{When $p<1$, the $\ell_p$ function does not satisfy the third axiom of Definition~\ref{definition:matrix-norm}, meaning it is not a valid norm.}, the $\ell_p$ \textit{norm} (a.k.a., the $\ell_p$ vector norm or \holders norm) is given by 
\begin{equation}\label{equation:l_p_norm}
\textbf{($\ell_p$ {norm})}:\qquad \norm{\bx}_p = \sqrt[p]{ \sum_{i=1}^{n}\abs{x_i}^p  },
\quad 
p\geq 1.
\end{equation}
This norm satisfies positive homogeneity and nonnegativity by definition, while the triangle inequality follows from Minkowski's inequality (Theorem~\ref{theorem:minkoski}; alternatively, we can prove the triangle inequality for the $\ell_p$ norm using H{\"o}lder's inequality; see Exercise~\ref{exercise:triangle_ineq}).
From the general definition of the $\ell_p$ norm,  the $\ell_1$ and $\ell_\infty$ \textit{norms}  can be obtained by, respectively, 
$$
\norm{\bx}_1 = \sum_{i=1}^{n} \abs{x_i}
\gap \text{and}\gap
\norm{\bx}_\infty = \mathop{\max}_{i=1,2,\ldots,n} \abs{x_i} .
$$


Following the definition of the $\ell_p$ norm, we can obtain the famous H{\"o}lder's inequality in Theorem~\ref{theorem:holder-inequality}. Thus, the $\ell_p$ norm is sometimes referred to as  \textit{H{\"o}lder's norm}. 
Conversely, H{\"o}lder's inequality can be used to prove the validity of the  $\ell_p$ norm. This proof is left as an exercise; see, for example, \citet{lu2021numerical}.
\begin{exercise}[Triangle Inequality]\label{exercise:triangle_ineq}
Prove the triangle inequality for the $\ell_p$ norm using H{\"o}lder's inequality.
For the special case $p=2$, use the Cauchy-Schwartz inequlaity (Proposition~\ref{proposition:cauchy-schwarz-inequ}) to prove the triangle inequality.
\end{exercise}
%\begin{proof}[Triangle Inequality of $\ell_p$ Norm] 
%\textbf{Case $p>1$ for $\norm{\bx}_p$.}
%From  H{\"o}lder's inequality (requires $p,q>1$; Theorem~\ref{theorem:holder-inequality}), we can prove the validity of the $\ell_p$ norm. Let $\frac{1}{p}+\frac{1}{q} = 1$, it follows that 
%$$
%\begin{aligned}
%\sum_{i=1}^{n}& (\abs{x_i} + \abs{y_i})^p = \sum_{i=1}^{n}\abs{x_i} (\abs{x_i}+\abs{y_i})^{p-1} + \sum_{i=1}^{n}\abs{y_i} (\abs{x_i}+\abs{y_i})^{p-1} \\
%&\leq \left(\sum_{i=1}^{n}  \abs{x_i}^p\right)^{1/p} \left( \sum_{i=1}^{n} (\abs{x_i} + \abs{y_i})^{(p-1)q}  \right)^{1/q}
%\left(\sum_{i=1}^{n} \abs{y_i}^p\right)^{1/p}  \left( \sum_{i=1}^{n} (\abs{x_i} + \abs{y_i})^{(p-1)q}  \right)^{1/q} \\
%&= \left[\left(\sum_{i=1}^{n}  \abs{x_i}^p   \right)^{1/p} +\left(\sum_{i=1}^{n} \abs{y_i}^p\right)^{1/p} \right]  \left( \sum_{i=1}^{n} (\abs{x_i} + \abs{y_i})^{(p-1)q}  \right)^{1/q} ,\\
%\end{aligned}
%$$
%Since $(p-1)q=p$, the above inequality implies 
%$$
%\left( \sum_{i=1}^{n} (\abs{x_i} + \abs{y_i})^{p}  \right)^{1/p} \leq  \left(\sum_{i=1}^{n}  \abs{x_i}^p   \right)^{1/p} +\left(\sum_{i=1}^{n} \abs{y_i}^p\right)^{1/p} = \norm{\bx}_p + \norm{\by}_p.
%$$
%Therefore, it follows that 
%$$
%\norm{\bx+\by}_p = \left( \sum_{i=1}^{n} (|x_i + y_i|)^{p}  \right)^{1/p}  \leq  \left( \sum_{i=1}^{n} (\abs{x_i} + \abs{y_i})^{p}  \right)^{1/p} 
%\leq \norm{\bx}_p + \norm{\by}_p.
%$$
%
%\paragraph{Case $p=1$ for $\norm{\bx}_1$.}
%It is straightforward that $\norm{\bx+\by}_1 \leq \norm{\bx}_1 + \norm{\by}_1$.
%
%
%\paragraph{Case $p<1$ for $\norm{\bx}_p$.}
%However, when $p<1$, the triangle inequality condition for vector norms is not satisfied. For example, when $p=1/3$, and $\bx=[0,1]^\top$ and $\by=[1,0]^\top$. It follows that 
%$$
%\begin{aligned}
%\norm{\bx+\by}_{1/3}= 8, \gap \norm{\bx}_{1/3} = 1, \gap \norm{\by}_{1/3}=1
%\gap \implies 
%\norm{\bx+\by}_{1/3}> \norm{\bx}_{1/3}+\norm{\by}_{1/3}.
%\end{aligned}
%$$
%This proves the validity of the $\ell_p$ norm.
%\end{proof}

%[hidealllines=\mdframehideline,backgroundcolor=\mdframecolor,frametitle={Triangle Inequality of $\ell_2$  Norm by Cauchy-Schwarz Inequality}]

%Alternatively, the triangle inequality for $\ell_2$ norm can be proved using the Cauchy-Schwarz inequality.
%\begin{proof}[Triangle Inequality of $\ell_2$  Norm by Cauchy-Schwarz Inequality]
%By the Cauchy-Schwarz inequality (Proposition~\ref{proposition:cauchy-schwarz-inequ}), for any vectors $\bx$ and $\by$, we have $\bx^\top \by \leq \normtwo{\bx} \cdot \normtwo{\by}$. Therefore,
%$$
%\begin{aligned}
%\normtwo{\bx+\by}^2 &= \normtwo{\bx}^2 + \normtwo{\by}^2 + 2\bx^\top\by
%\leq  \normtwo{\bx}^2 + \normtwo{\by}^2 + 2 \normtwo{\bx} \cdot \normtwo{\by} 
%= (\normtwo{\bx} + \normtwo{\by})^2.
%\end{aligned}
%$$
%This results in $\normtwo{\bx+\by} \leq \normtwo{\bx}+\normtwo{\by}$.
%\end{proof}


\begin{exercise}[Orthogonal Invariance of $\ell_2$]\label{exercise:orthogo_ell2}
Show that the $\ell_2$ norm is orthogonally invariant: $\norm{\bQ\bx}_2=\normtwo{\bx}$ for all $\bx\in\real^n$ if $\bQ$ is orthogonal.
Investigate whether this property holds for other  $\ell_p$ norms.
\end{exercise}







\section*{Properties}
For any vector norm, we also have the following properties.
\begin{proposition}[$\ell_p$ Norm Inequalities]\label{prop:lp_norm_ineqs}
Given any $\bx\in\real^n$, the function $f(p)=\norm{\bx}_p$ is nonincreasing for $p=[1,\infty)$. Therefore, 
\begin{equation}
\norm{\bx}_\infty\leq \norm{\bx}_q \leq \norm{\bx}_p\leq \normtwo{\bx} \leq \normone{\bx},
\quad \text{if }  \infty \geq q \geq p\geq 2\geq 1.
\end{equation}
If $\bx$ has at least two nonzero elements, the inequalities can be strict.
This behavior can also be observed in the figures of unit balls (Figure~\ref{fig:p-norm-comparison-3d}).
Moreover, for any $\infty\geq p \geq 1$, we also have the bound
\begin{equation}
\norm{\bx}_\infty \leq \norm{\bx}_p \leq  n^{1/p} \norm{\bx}_\infty,
\quad \text{if }  \infty \geq  p \geq 1.
\end{equation}
\end{proposition}
\begin{proof}[of Proposition~\ref{prop:lp_norm_ineqs}]
Let $S(p) \triangleq \norm{\bx}_p^p$. Taking the logarithm of $f(p)$ gives
$\ln f(p) = \frac{1}{p} \ln S(p). $
Differentiate both sides with respect to $p$:
$$\frac{f'(p)}{f(p)} = \frac{d}{dp} \left( \frac{1}{p} \ln S(p) \right)
= -\frac{1}{p^2} \ln S(p) + \frac{1}{p} \cdot \frac{S'(p)}{S(p)}
. 
$$
This shows
$$f'(p) = f(p) \left( -\frac{1}{p^2} \ln S(p) + \frac{1}{p} \cdot \frac{S'(p)}{S(p)} \right)
=
\frac{1}{p}\norm{\bx}_p^{1-p}\sum_{i=1}^{n} \alpha_i,
$$
where 
$$
S'(p)=\sum_{i=1}^{n}\abs{x_i}^p \ln\abs{x_i}
\quad \text{and} \quad 
\alpha_i =
\left\{
\begin{aligned}
&\abs{x_i}^p (\ln \abs{x_i} - \ln \norm{\bx}_p), &\text{if } x_i\neq 0;\\
&0, &\text{if } x_i=0.
\end{aligned}
\right.
$$
This shows $f'(p)\leq 0$ since $\ln \abs{x_i} - \ln \norm{\bx}_p\leq 0$.
If $\bx$ has at least two nonzero elements, $f'(p)< 0$ and $f(p)$ is decreasing.

For the second part, following the definition of the $\ell_p$ norm, we notice that 
$$
(\norm{\bx}_\infty )^p = \left(\mathop{\max}_{i=1,2,\ldots,n} \abs{x_i}\right)^p \leq \sum_{i=1}^{n} \abs{x_i}^p \leq n \mathop{\max}_{i=1,2,\ldots,n} \abs{x_i}^p = n (\norm{\bx}_\infty )^p,
$$
from which the result follows.
\end{proof}




\begin{proposition}[Orthogonally Invariant Vector Norm]\label{proposition:orinv_vecnorm}
Let $\norm{\cdot}$ be an orthogonally invariant norm on $\real^n$, and let $\norm{\cdot}_2$ be the $\ell_2$ norm. Then, for any $\bx\in\real^n$, we have $\norm{\bx}=\normtwo{\bx}\norm{\be_1}$.
\end{proposition}
\begin{proof}[of Proposition~\ref{proposition:orinv_vecnorm}]
Define $\gamma\triangleq\normtwo{\bx}$ and let $\widehat{\bx}\triangleq\bx/\gamma$, so that $\bx=\gamma \widehat{\bx}$. 
Consider  an orthogonal matrix $\bU$  whose first column is $\widehat{\bx}$.
By the definition of a norm and the orthogonally invariant property, we have $\norm{\bx}=\gamma\norm{\widehat{\bx}}=\gamma\norm{\bU\be_1}=\gamma\norm{\be_1}$. 
\end{proof}
The result also implies that the $\ell_2$ norm is the only orthogonally invariant norm satisfying $\norm{\be_1}=1$.






We conclude this section by introducing an important property of vector norms that  is frequently useful. 
The following theorem on the equivalence of vector norms states that if a vector is small in one norm, it is also small in another norm, and vice versa.
\begin{theorem}[Equivalence of Vector Norms]\label{theorem:equivalence-vector-norm}
Let $\norm{\cdot}_a$ and $\norm{\cdot}_b$ be two different vector norms, where $\norm{\cdot}_a$, $\norm{\cdot}_b$: $\real^n\rightarrow \real$. Then, there exist positive scalars $\alpha$ and $\beta$ such that for all $\bx \in \real^n$, the following inequality holds:
$$
\alpha\norm{\bx}_a \leq \norm{\bx}_b \leq \beta\norm{\bx}_a.
$$
This implies 
$$
\frac{1}{\beta}\norm{\bx}_b  \leq \norm{\bx}_a \leq \frac{1}{\alpha}\norm{\bx}_b.
$$
This justifies the term ``equivalence" in the theorem.
%The proof of the above theorem relies on the continuity properties of functions and can be found in \citet{van2020advanced}. We shall not discuss it here for simplicity.
\end{theorem}
\begin{proof}[of Theorem~\ref{theorem:equivalence-vector-norm}]
In advanced calculus, it is stated that $\mathop{\sup}_{\bx\in \sS} f(\bx)$ is attained for some vector $\bx\in \sS$ as long as $f(\cdot)$ is continuous and $\sS$ is a compact set (closed and bounded); see Theorem~\ref{theorem:weierstrass_them}. When the supremum is an element in the set $\sS$, this supremum is known as the maximum such that $\mathop{\sup}_{\bx\in\sS} f(\bx) =\mathop{\max}_{\bx\in\sS} f(\bx)$.
Without loss of generality, we assume $\bx\neq \bzero$. Then we have:
$$
\begin{aligned}
\norm{\bx}_b &= \frac{\norm{\bx}_b}{\norm{\bx}_a} \norm{\bx}_a 
\leq \mathop{\sup}_{\bz\neq 0} \frac{\norm{\bz}_b}{\norm{\bz}_a} \norm{\bx}_a
= \mathop{\sup}_{\norm{\by}_a=1} \norm{\by}_b \norm{\bx}_a
=\norm{\bx}_a \mathop{\max}_{\norm{\by}_a=1} \norm{\by}_b.
\end{aligned}
$$
The last equality holds since $\{\by: \norm{\by}_a=1\}$ is a compact set.
By setting $\beta\triangleq \mathop{\max}_{\norm{\by}_a=1} \norm{\by}_b  $, we have $\norm{\bx}_b \leq \beta\norm{\bx}_a$.
From the above argument, there exists a $\tau$ such that 
$
\norm{\bx}_a \leq \tau \norm{\bx}_b.
$
Letting $\alpha \triangleq \frac{1}{\tau}$, we obtain $\alpha\norm{\bx}_a \leq \norm{\bx}_b$, from which the result follows.
\end{proof}

\begin{example}[Equivalence of Vector Norms]
The following inequalities hold for all $\bx\in\real^n$:
$$
\begin{aligned}
\norm{\bx}_\infty &\leq \norm{\bx}_1 \leq n\norm{\bx}_\infty; \\
\norm{\bx}_\infty &\leq \normtwo{\bx} \leq \sqrt{n}\norm{\bx}_\infty; \\
\normtwo{\bx} &\leq \norm{\bx}_1 \leq \sqrt{n}\normtwo{\bx}. \\
\end{aligned}
$$
This demonstrates the equivalence of the $\ell_1, \ell_2$, and $\ell_\infty$ vector norms.
\end{example}


\section*{Dual Norm}
Consider the $\ell_p$ vector norm. From \holders inequality (Theorem~\ref{theorem:holder-inequality}), we have 
$
\bx^\top\by \leq \norm{\bx}_p \norm{\by}_q,
$
where $p,q>1$ satisfy $\frac{1}{p}+\frac{1}{q}=1$, and $\bx,\by\in \real^n$. Equality holds if the two sequences $\{\abs{x_i}^p\}$ and $\{\abs{y_i}^q\}$ are linearly dependent. This implies
\begin{equation}\label{equation:dual_norm_equa}
\mathop{\max}_{\norm{\by}_q=1} \bx^\top\by = \norm{\bx}_p.
\end{equation}
For this reason, $\norm{\cdot}_q$ is called the \textit{dual norm} of $\norm{\cdot}_p$.
On the other hand, for each $\bx\in \real^n$ with $\norm{\bx}_p=1$, there exists a vector $\by\in \real^n$ such that $\norm{\by}_q=1$ and $\bx^\top\by=1$.
Notably, the $\ell_2$ norm is self-dual, while the $\ell_1$ and $\ell_\infty$ norms are dual to each other.


\begin{definition}[Set of Primal Counterparts]\label{definition:set_primal}
Let $ \norm{\cdot} $ be any norm on $ \real^n $. Then the \textit{set of primal counterparts of $\ba$} is defined as 
\begin{equation}
\Lambda_{\ba} = \argmax_{\bu \in \real^n} \left\{ \innerproduct{\ba, \bu} \mid  \norm{\bu} \leq 1 \right\}.
\end{equation}
That is, $\innerproduct{\ba, \ba^\dagger} = \norm{\ba}_*$ for any $\ba^\dagger\in \Lambda_{\ba}$, where $\norm{\cdot}_{*}$ denotes the dual norm.
It follows that 
\begin{enumerate}[(i)]
\item If $\ba \neq \bzero$, then $\norm{\ba^\dagger} = 1$ for any $\ba^\dagger \in \Lambda_{\ba}$.
\item If $\ba = \bzero$, then $\Lambda_{\ba} = \{\bx\in\real^n\mid  \norm{\bx} \leq 1\}$.
%\item $\innerproduct{\ba, \ba^\dagger} = \norm{\ba}_*$ for any $\ba^\dagger \in \Lambda_{\ba}$.
\end{enumerate}
\end{definition}

\begin{example}[Set of Primal Counterparts]\label{example:set_primal_count}
A few examples for the sets of primal counterparts are shown below:
\begin{itemize}
\item If the norm is the  $\ell_2$ norm, then for any $\ba \neq \bzero$,
$
\Lambda_{\ba} = \left\{ {\ba}/{\normtwo{\ba}} \right\}.
$

\item If the norm is the  $\ell_1$ norm, then for any $\ba \neq \bzero$,
$$
\Lambda_{\ba} = \left\{ \sum_{i \in \sI(\ba)} \lambda_i \sign(a_i) \be_i \mid \sum_{i \in \sI(\ba)} \lambda_i = 1, \lambda_j \geq 0, j \in \sI(\ba) \right\},
$$
where $\sI(\ba) \triangleq \argmax_{i=1,2,\ldots,n} |a_i|$.

\item If the norm is the  $\ell_\infty$ norm, then for any $\ba \neq \bzero$,
$$
\Lambda_{\ba} = \left\{ \bx \in \real^n \mid x_i = \sign(a_i), i \in \sI_{\neq}(\ba), |x_j| \leq 1, j \in \sI_0(\ba) \right\},
$$
where
$
\sI_{\neq}(\ba) \triangleq \left\{ i \in \{1, 2, \ldots, n\} \mid a_i \neq 0 \right\} $ and $ \sI_0(\ba) \triangleq \left\{ i \in \{1, 2, \ldots, n\} \mid a_i = 0 \right\}.
$
\end{itemize}
These examples play a crucial role in the development of non-Euclidean gradient descent methods, which will be discussed in Sections~\ref{section:als-gradie-descent-taylor} and \ref{section:noneucli_gd}.
\end{example}

\index{Unit ball}
\section*{Unit Ball}
The \textit{unit ball} of a norm is the set of all points whose distance from the origin (i.e., the zero vector) equals 1.
If the distance is defined by the $\ell_p$ norm,  the unit ball is the collection of 
$$
\sB_p = \{\bx: \norm{\bx}_p=1\}.
$$
The comparison of the  $\ell_p$ norm in three-dimensional space with different values of $p$ is depicted in  Figure~\ref{fig:p-norm-comparison-3d}.
%\begin{SCfigure}%[h!]
%\centering
%\includegraphics[width=0.5\textwidth]{./imgs/p-norm-2d_2.pdf}
%\caption{Unit ball of $\ell_p$ norms in two-dimensional space. When $p<1$, the metric is not a norm since it does not satisfy the third axiom of the norm in Definition~\ref{definition:matrix-norm}.}
%\label{fig:p-norm-2d}
%\end{SCfigure}



\begin{figure}[h]
\centering  
\vspace{-0.25cm} 
%\subfigtopskip=2pt
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[$p=\infty$.]{\label{fig:p-norm-3d1}
\includegraphics[width=0.18\linewidth]{./imgs/p-norm-3d-p-1111.pdf}}
\subfigure[$p=2$.]{\label{fig:p-norm-3d2}
\includegraphics[width=0.18\linewidth]{./imgs/p-norm-3d-p-2.pdf}}
\subfigure[$p=1$.]{\label{fig:p-norm-3d3}
\includegraphics[width=0.18\linewidth]{./imgs/p-norm-3d-p-1.pdf}}
\subfigure[$p=0.5$.]{\label{fig:p-norm-3d4}
\includegraphics[width=0.18\linewidth]{./imgs/p-norm-3d-p-05.pdf}}
\subfigure[$p=0$.]{\label{fig:p-norm-3d5}
\includegraphics[width=0.18\linewidth]{./imgs/p-norm-3d-p-0.pdf}}
\caption{Unit ball of $\ell_p$ norms in three-dimensional space. When $p<1$, the metric does not qualify as a norm since it does not satisfy the third axiom of the norm in Definition~\ref{definition:matrix-norm}.}
\label{fig:p-norm-comparison-3d}
\end{figure}

Vector norms are fundamental in machine learning. In Section~\ref{section:pre_ls}, we will discuss how least squares aim to minimize the squared distance between the observed value $\bb$ and the predicted value $\bA\bx$: $\normtwo{\bA\bx-\bb}$, which is the $\ell_2$ norm of $\bA\bx-\bb$.
Alternatively, minimizing the $\ell_1$ norm of the difference between the observed and predicted values can yield a more robust estimation of $\bx$, particularly in the presence of outliers  \citep{zoubir2012robust}.

\section*{$\bQ$-Inner Product, $\bQ$-Norm, $\bS$-Norm}
The standard dot product is not the only possible inner product that can be defined on $\real^n$. Given a positive definite $n\times n$ matrix $\bQ$, a $\bQ$\textit{-dot product} can be defined as~\footnote{When $\bQ$ is positive semidefinite, the $\bQ$-dot product is a semi-inner product.}
$$
\langle \bx,\by \rangle_{\bQ} = \bx^\top\bQ\by.
$$
One can verify that the $\bQ$-dot product defined above satisfies the three axioms of an inner product, as discussed at the beginning of this section. When $\bQ=\bI$, this reduces to the standard Euclidean inner product (dot product). From the $\bQ$-dot product, we can define the corresponding \textit{$\bQ$-$\ell_2$-norm} as 
$
\norm{\bx}_{\bQ,2} = \sqrt{\bx^\top\bQ\bx}.
$
More generally, given a positive definite matrix $\bQ\in\real^{n\times n}$ and  a general norm $\norm{\cdot}$ on $\real^n$, we define the \textit{$\bQ$-norm} as:
\begin{equation}\label{equation:q_norm}
\textbf{$\bQ$-norm:} \qquad\norm{\bx}_{\bQ} = \norm{\sqrt{\bQ}\bx}
\end{equation}	
Similarly, given a matrix $\bS\in\real^{m\times n}$ with full column rank and a general norm $\norm{\cdot}$ on $\real^m$,  the \textit{$\bS$-norm} is defined as:
\begin{equation}\label{equation:s_norm}
\textbf{$\bS$-norm:} \qquad \norm{\bx}_{\bS}=\norm{\bS\bx},
\end{equation}	
which is a norm on $\real^n$.
To establish this rigorously, we introduce the following lemma.
\begin{lemma}[Construct Norms from Other Norms]\label{lemma:construct_norm}
Let $\norm{\cdot}$ be a norm on $\real^m$. Given a matrix $\bS\in \real^{m\times n}$, then $\norm{\bS(\cdot)}$ defines a semi-norm~\footnote{It may be zero for nonzero vectors in $\real^n$.} on $\real^n$. 
If $\bS$ has full column rank $n$, then $\norm{\bS(\cdot)}$ is a norm on $\real^m$.
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:construct_norm}]
For any vectors $\bx, \by\in \real^m$, following from Definition~\ref{definition:matrix-norm}, we have 
$$
\norm{\bx}\geq 0, \gap \norm{\lambda \bx} = \abs{\lambda}\cdot  \norm{\bx}, \gap \norm{\bx+\by} \leq \norm{\bx} + \norm{\by}.
$$
Now, for any $\bc, \bd \in \real^n$, it follows that:
$$
\norm{\bS\bc}\geq 0, \gap \norm{\lambda \bS\bc} = \abs{\lambda}\cdot  \norm{\bS\bc}, \gap \norm{\bS\bc+\bS\bd} \leq \norm{\bS\bc} + \norm{\bS\bb}.
$$
Therefore, $\norm{\bS(\cdot)}$ is a semi-norm.
If $\bS$ has full column rank, $\bS\bc=\bzero$ only if $\bc=\bzero$, ensuring that $\norm{\bS(\cdot)}$ satisfies the norm definiteness property. Hence, it is a norm. This completes the proof.
\end{proof}

%\begin{exercise}[$\bQ$-Norm, $\bS$-Norm]\label{exercise:qnorm}
%Show that the $\bQ$-norm and $\bS$-norm defined above satisfy the three axioms of norms given in Definition~\ref{definition:matrix-norm}.
%Show that when $\norm{\cdot}$ is the $\ell_2$ norm, the $\bS$-norm reduces to the $\bQ$-$\ell_2$-norm. 
%\textit{Hint: Use Cholesky and spectral decompositions (Theorems~\ref{theorem:cholesky-factor-exist} and \ref{theorem:spectral_theorem}).}
%\end{exercise}


\begin{exercise}[Weighted $\ell_p$ Norm]
Let $w_1, w_2, \ldots,w_p$ be positive real numbers, and let $p\geq 1$. Show that  the weighted $\ell_p$ norm $\norm{\bx}=\sqrt[p]{ \sum_{i=1}^{n}w_i\abs{x_i}^p  }$  is a valid norm on $\real^n$. \textit{Hint: Show that it is a norm of the form $\norm{\bS\bx}_p$.} 
\end{exercise}

%From the above lemma, we can then prove the correctness of the $\bQ$-norm.
%\begin{proof}[of Corollary~\ref{corollary:qnorm}]
%By Theorem~\ref{theorem:unique-factor-pd}, any positive definite matrix $\bQ$ admits  a unique decomposition $\bQ=\bB^2=\bB^\top\bB$ with positive definite $\bB$. Therefore, we have 
%$$
%\norm{\bx}_{\bQ} = \sqrt{\bx^\top\bQ\bx} = \sqrt{\bx^\top\bB^\top \bB \bx} = \norm{\bB\bx}_2.
%$$
%Therefore, $\norm{\bx}_{\bQ}$ is equivalent to the $\ell_2$ norm of $\bB\bx$.
%The second part can be proved using spectral decomposition (Section~\ref{section:conc_pd}).
%\end{proof}



\subsection{Matrix Norm}\label{appendix:matrix-norm-sect2}

\index{Submultiplicativity}
\index{Submultiplicative matrix norms}
\subsection*{Submultiplicativity of Matrix Norms}
In some texts, a matrix norm that is not \textit{submultiplicative} is referred to as a \textit{vector norm on matrices} or a \textit{generalized matrix norm}.
The submultiplicativity of a matrix norm plays a crucial role in the analysis of square matrices. However, it's important to note that the definition of a matrix norm applies to both square and rectangular matrices.
For a submultiplicative matrix norm $\norm{\cdot}$ satisfying $\norm{\bA\bB}\leq \norm{\bA}\norm{\bB}$, considering $\bA\in\real^{n\times n}$, it follows that 
\begin{equation}\label{equation:power_subm}
\norm{\bA^2} \leq \norm{\bA}^2
\quad\implies\quad
\norm{\bA^k} \leq \norm{\bA}^k, \forall k\in\{1,2,\ldots,\}.
\end{equation}
Therefore, if the matrix is idempotent, i.e., $\bA^2=\bA$, we have $\norm{\bA}\geq 1$. This also indicates:
\begin{equation}\label{equation:power_subm2}
\norm{\bI}\geq 1,
\quad\text{if}\quad
\norm{\cdot} \text{ is submultiplicative}.
\end{equation}
On the other hand, if $\bA$ is nonsingular, then for submultiplicative norms, we have the inequality:
$$
1\leq \norm{\bI}=\norm{\bA\bA^{-1}}\leq \norm{\bA}\norm{\bA^{-1}}.
$$  
This means that for a submultiplicative norm, $\norm{\bI}\geq 1$, and it is considered \textit{normalized} if and only if $\norm{\bI}=1$.

\paragraph{Bounds on the spectral radius.}
The property of submultiplicativity can be utilized to establish bounds on the \textit{spectral radius} of a matrix  (i.e., the largest magnitude of the eigenvalues of a matrix).
Given an eigenpair $(\lambda,\bx)$ of a matrix $\bA\in\real^{n\times n}$, and consider the matrix $\bX=\bx\bone^\top=[\bx,\bx,\ldots,\bx]$. 
For a submultiplicative matrix norm $\norm{\cdot}$ on $\real^{n\times n}$, it holds that 
$
\abs{\lambda}\norm{\bX}
=
\norm{\lambda\bX}
=
\norm{\bA\bX}
\leq \norm{\bA}\norm{\bX}.
$
Thus, the spectral radius is bounded by the matrix norm $\norm{\bA}$.
Similarly, we can prove that $\abs{\lambda^{-1}}\leq \norm{\bA^{-1}}$.
Consequently, we obtain lower and upper bounds on the spectral radius $\rho(\bA)$:
\begin{equation}\label{equation:bounds_submulnorm}
\textbf{Bounds on spectral radius:} \qquad \frac{1}{\norm{\bA^{-1}}}\leq \rho(\bA)\leq \norm{\bA}.
\end{equation}
Additionally, the submultiplicative property aids in understanding bounds on the spectral radius of the product of matrices. 
For instance, given matrices $\bA$ and $\bB$ (provided  the matrix product $\bA\bB$ is defined), we have
\begin{equation}
\rho(\bA\bB)\leq \norm{\bA\bB} \leq \norm{\bA}\norm{\bB}.
\end{equation}

\index{Convergent matrices}
\paragraph{Power of square matrices.}
When $\norm{\bA}\leq 1$ for a square matrix $\bA\in\real^{n\times n}$, \eqref{equation:power_subm} shows that $\norm{\bA^k} \leq \norm{\bA}^k\stackrel{k\rightarrow \infty}{=}0$.
This holds true regardless of the specific norm used, provided it is submultiplicative. Thus, if $\bA^k$ tends to the zero matrix when $k\rightarrow \infty$ if $\norm{\bA}\leq 1$.
A matrix with this property is called \textit{convergent}.
The convergence of a matrix can also be characterized by its spectral radius.
\begin{exercise}[Convergence Matrices]\label{exer:conv_mat}
Let $\bA\in\real^{n\times n}$. Show that $\mathoplim{k\rightarrow \infty}\bA^k=\bzero$ if and only if the spectral radius $\rho(\bA)<1$.
\textit{Hint: Examine $\bA^k\bx=\lambda^k\bx$.}
\end{exercise}

\begin{exercise}[Convergence Matrices]
Let $\bA\in\real^{n\times n}$ and  $\epsilon>0$. Show that there is a constant $C=C(\bA,\epsilon)$ such that $\abs{(\bA^k)_{ij}} \leq C(\rho(\bA)+\epsilon)^k$ for $k\in\{1,2,\ldots\}, i,j\in\{1,2,\ldots,n\}$.
\textit{Hint: Use Exercise~\ref{exer:conv_mat} and examine $\bB=\frac{1}{\rho(\bA)+\epsilon}\bA$, whose spectral radius is strictly less than 1.}
\end{exercise}

\index{Gelfand formula}
The Gelfand formula offers a method to estimate the spectral radius of a matrix using submultiplicative norms.
\begin{exercise}[Gelfand Formula]\label{exercise:gelfand_formula}
Let $\bA\in\real^{n\times n}$ and let $\norm{\cdot}$ be a submultiplicative matrix norm on $\real^{n\times n}$. 
Show that $\rho(\bA)=\mathoplim{k\rightarrow \infty}\norm{\bA^k}^{1/k}$.
\textit{Hint: Consider $\rho(\bA^k)=\rho(\bA)^k\leq \norm{\bA}$ and examine $\bB=\frac{1}{\rho(\bA)+\epsilon}\bA$, whose spectral radius is strictly less than 1.}
\end{exercise}

\index{Frobenius}
\subsection*{Frobenius Norm}
The norm of a matrix serves a similar purpose to the norm of a vector.
One important matrix norm is the Frobenius norm, which can be considered the matrix equivalent of the  $\ell_2$ vector norm.
\begin{definition}[Frobenius Norm]\label{definition:frobenius}
The Frobenius norm of a matrix $\bA\in \real^{m\times n}$ is defined as 
$$
\norm{\bA}_F = \sqrt{\sum_{i=1,j=1}^{m,n} (a_{ij})^2}=\sqrt{\trace(\bA\bA^\top)}=\sqrt{\trace(\bA^\top\bA)} = \sqrt{\sigma_1^2+\sigma_2^2+\ldots+\sigma_r^2},
$$
where the values of $\sigma_i$ are the singular values of $\bA$, and $r$ is the rank of $\bA$. 
This represents the square root of the sum of the squares of all elements in $\bA$. 
\end{definition}
The equivalence of $\sqrt{\sum_{i=1,j=1}^{m,n} (a_{ij})^2}$, $\sqrt{\trace(\bA\bA^\top)}$, and $\sqrt{\trace(\bA^\top\bA)}$ is straightforward. 
The equivalence between $\sqrt{\trace(\bA\bA^\top)}$ and $\sqrt{\sigma_1^2+\sigma_2^2+\ldots+\sigma_r^2}$ can be shown using the singular value decomposition (SVD). Suppose $\bA$ admits the SVD $\bA = \bU\bSigma\bV^\top$, then:
$$
\sqrt{\trace(\bA\bA^\top)} = \sqrt{\trace(\bU\bSigma\bV^\top \bV\bSigma\bU^\top)} = \sqrt{\trace(\bSigma^2)}=\sqrt{\sigma_1^2+\sigma_2^2+\ldots+\sigma_r^2}.
$$
Apparently, the Frobenius norm can  also be defined using the  vector $\ell_2$ norm such that $\norm{\bA}_F = \sqrt{\sum_{i=1}^{n} \norm{\ba_i}^2}$, where $\ba_i$ for all $i \in \{1,2,\ldots, n\}$ are the columns of $\bA$.

\index{Submultiplicativity}
\begin{proposition}[Submultiplicativity of Frobenius]\label{propo:submul_frob}
The Frobnenius norm is submultiplicative. That is, $\norm{\bA\bB}_F\leq \norm{\bA}_F\norm{\bB}_F$.
\end{proposition}
\begin{proof}[of Proposition~\ref{propo:submul_frob}]
Suppose $\bA\in\real^{m\times n}$ and $\bB\in\real^{n\times p}$. We have 
$$
\norm{\bA\bB}_F = 
\bigg(\sum_{i,j=1}^{m,p} \big(\sum_{k=1}^{n}a_{ik}b_{kj} \big)^2 \bigg)^{1/2}
\leq 
\bigg(\sum_{i,j=1}^{m,p} \big(\sum_{k=1}^{n}a_{ik}^2 \big)\big(\sum_{k=1}^{n}b_{kj}^2 \big) \bigg)^{1/2}
=
\norm{\bA}_F\norm{\bB}_F.
$$
This completes the proof.
\end{proof}


\index{Orthogonally invariance}
\begin{proposition}[Orthogonally Invariance of Frobenius]\label{proposition:frobenius-orthogonal-equi}
Let $\bA\in \real^{m\times n}$ be given, and let $\bU\in \real^{m\times m}$ and $\bV\in \real^{n\times n}$ be orthogonal matrices. Then,
$
\norm{\bA }_F =  \norm{\bU\bA\bV }_F.
$
\end{proposition}
\begin{proof}[of Proposition~\ref{proposition:frobenius-orthogonal-equi}]
We observe that 
$$
\begin{aligned}
\norm{\bU\bA\bV}_F  &=\sqrt{\trace((\bU\bA\bV)(\bU\bA\bV)^\top)} = \sqrt{\trace(\bU\bA\bA^\top\bU^\top)}\\
&= \sqrt{\trace(\bA\bA^\top\bU^\top\bU)} = \sqrt{\trace(\bA\bA^\top)}=
\norm{\bA}_F ,
\end{aligned}
$$
where the third equality holds due to the cyclic property of the trace.
\end{proof}

\index{Schur inequality}
The Frobenius norm is defined as the square root of the sum of the squares of the matrix elements. Additionally, we have the well-known \textit{Schur inequality}, which follows from the definition of the Frobenius norm.
\begin{theorem}[Schur Inequality]\label{theorem:schur_inequality}
Let $\lambda_1, \lambda_2, \ldots, \lambda_n$ be  real eigenvalues of the matrix $\bA\in \real^{n\times n}$. Then,
$
\sum_{i=1}^{n} \abs{\lambda_i}^2 \leq \sum_{i=1,j=1}^{n,n} \abs{a_{ij}}^2 = \norm{\bA}_F^2,
$
which means the sum of the squared absolute values of the eigenvalues is bounded by the Frobenius norm of the matrix.
\end{theorem}
\begin{proof}[of Theorem~\ref{theorem:schur_inequality}]
Suppose the Schur decomposition of $\bA$ is given by $\bA=\bQ\bU\bQ^\top$ (see, for example, \citet{lu2021numerical}), where the diagonal of $\bU$ contains the eigenvalues of $\bA$. By the orthogonal invariance Proposition~\ref{proposition:frobenius-orthogonal-equi}, we have
$
\norm{\bU}_F = \norm{\bQ\bU\bQ^\top}_F = \norm{\bA}_F. 
$
Therefore,
$$
\sum_{i=1}^{n} \abs{\lambda_i}^2 = \sum_{i=1}^{n} u_{ii}^2 \leq \sum_{i=1}^{n} u_{ii}^2 + \sum_{ i\neq j} u_{ij}^2 = \norm{\bU}_F^2
\quad\implies \quad
\sum_{i=1}^{n} \abs{\lambda_i}^2  \leq \norm{\bA}_F^2.
$$
This completes the proof.
\end{proof}
\begin{exercise}[Schur Inequality]
Prove the Schur inequality for general matrices that do not necessarily have real eigenvalues. Under what conditions does equality hold?
\end{exercise}




%\subsection*{The $\ell_p$ Matrix Norm: Generalizing Frobenius Norm}
%Similar to the $\ell_p$ vector norm, we can  define the $\ell_p$ matrix norm as follows.
%\begin{definition}[$\ell_p$ Matrix Norm]\label{definition:lp-matrix_norm_app}
%Given a  matrix $\bA\in \real^{m\times n}$, for  $p\geq 1$, the $\ell_p$ \textit{matrix norm} is defined as:
%$$
%\norm{\bA}_{m_p} = \sqrt[p]{ \sum_{j=1}^{n} \sum_{i=1}^{m}|a_{ij}|^p  }.
%$$	
%From this definition, we have the specific cases of the $\ell_1, \ell_2$, and $\ell_\infty$ matrix norms:
%$$
%\begin{aligned}
%\norm{\bA}_{m_1} &= \sum_{j=1}^{n} \sum_{i=1}^{m} |a_{ij}|; \qquad
%\norm{\bA}_{m_2} = \left(\sum_{j=1}^{n} \sum_{i=1}^{m} |a_{ij}|^2 \right)^{1/2}; \\
%\norm{\bA}_{m_\infty} &= \mathop{\max}_{i,j} |a_{ij}|, \gap \forall i\in\{1,2,\ldots,m\}, j\in \{1,2,\ldots,n\},\\
%\end{aligned}
%$$
%where the subscript $m_p$ is used to distinguish the  $\ell_p$  matrix norm  from the  $\ell_p$ vector norm and the induced matrix norms (discussed later). When $p=2$, the $\ell_2$ matrix norm reduces to the Frobenius norm.
%Since $\norm{\bA}_{m_p} = \norm{vec(\bA)}_p$, it follows from Propositrion~\ref{prop:lp_norm_ineqs} that
%\begin{equation}
%\norm{\bA}_{m_\infty}\leq \norm{\bA}_{m_q} \leq \norm{\bA}_{m_p}\leq \normtwo{\bA} \leq \norm{\bA}_{m_1},
%\quad \text{if }  \infty \geq q \geq p\geq 2\geq 1;
%\end{equation}
%\begin{equation}
%\norm{\bA}_{m_\infty} \leq \norm{\bA}_{m_p} \leq  n^{1/p} \norm{\bA}_{m_\infty},
%\quad \text{if }  \infty \geq  p \geq 1.
%\end{equation}
%\end{definition}
%
%\begin{exercise}[$\ell_1$, $\ell_\infty$ Matrix Norm]
%Show that the $\ell_1$ matrix norm is submultiplicative and the $\ell_\infty$ matrix norm is not submultiplicative.
%How about the matrix norm $\norm{\bA}=n\norm{\bA}_{m_\infty}$ if $\bA\in\real^{n\times n}$?
%\end{exercise}

\subsection*{Spectral Norm}
Another important matrix norm that is extensively used is the \textit{spectral norm}.
\begin{definition}[Spectral Norm]\label{definition:spectral_norm_app}
The spectral norm of a matrix $\bA\in \real^{m\times n}$ is defined as 
$$
\normtwo{\bA} = \mathop{\max}_{\bx\neq\bzero} \frac{\normtwo{\bA\bx}}{\normtwo{\bx}}  =\mathop{\max}_{\bu\in \real^n: \norm{\bu}_2=1}  \norm{\bA\bu}_2 ,
$$
which corresponds to the largest singular value of $\bA$, i.e., $\normtwo{\bA} = \sigma_{\max}(\bA)$~\footnote{When $\bA$ is an $n\times n$  positive semidefinite matrix, $\normtwo{\bA}=\sqrt{\lambda_{\max}(\bA^2)}=\lambda_{\max}(\bA)$, i.e., the largest eigenvalue of $\bA$.}. The second equality holds because scaling $\bx$ by a nonzero scalar does not affect the ratio:
$
\frac{\norm{\lambda \cdot \bA\by}_2 }{\norm{\lambda \cdot \by}_2 }  =  \norm{\bA\by}_2.
$
The definition also indicates the matrix-vector inequality:
$$
\text{ $\normtwo{\bA\bx} \leq \normtwo{\bA}\normtwo{\bx}$ for all vectors $\bx\in\real^n$.}
$$
\end{definition}
To see why the spectral norm of a matrix is equal to its largest singular value, consider the singular value decomposition $\bA=\bU\bSigma\bV^\top$. We have:
$$
\begin{aligned}
\norm{\bU\bSigma\bV^\top}_2 &= \max_{\bx \neq \bzero} \frac{\norm{\bU\bSigma\bV^\top \bx}_2}{\normtwo{\bx}} \stackrel{*}{=} \max_{\bV^\top\bx \neq \bzero} \frac{\norm{\bSigma\bV^\top \bx}_2}{\normtwo{\bx}}\\
&= \max_{\bV^\top\bx \neq \bzero} \frac{\norm{\bSigma\bV^\top \bx}_2}{\norm{\bV^\top\bx}_2} \frac{\norm{\bV^\top\bx}_2}{\normtwo{\bx}} 
=\max_{\by \neq \bzero} \frac{\norm{\bSigma \by}_2}{\norm{\by}_2}
\leq \sigma_{\max}(\bA),
\end{aligned}
$$
where the equality ($*$) holds since $\bV$ is orthogonal and the $\ell_2$ vector norm is orthogonally invariant. The inequality holds because the largest singular value of $\bA$ is the maximum value of $\frac{\norm{\bSigma \by}_2}{\norm{\by}_2}$ over all nonzero vectors $\by$.
Alternatively, this can be shown by noting that: $\normtwo{\bA\bx}^2=\abs{\bx^\top\bA^\top\bA\bx}\leq \sigma_{\max}^2(\bA)$.




\index{Submultiplicativity}
\begin{proposition}[Submultiplicativity of Spectral]\label{propo:submul_spec}
The spectral norm is submultiplicative. That is, $\norm{\bA\bB}_2\leq \normtwo{\bA}\norm{\bB}_2$.
\end{proposition}
\begin{proof}[of Proposition~\ref{propo:submul_spec}]
The definition of the spectral norm shows that
$$
\begin{aligned}
\norm{\bA\bB}_2 &= \max_{\bx \neq \bzero} \frac{\norm{\bA\bB \bx}_2}{\normtwo{\bx}} \stackrel{*}{=} \max_{\bB\bx \neq \bzero} \frac{\norm{\bA\bB \bx}_2}{\normtwo{\bx}} 
= \max_{\bB\bx \neq \bzero} \frac{\norm{\bA\bB \bx}_2}{\norm{\bB\bx}_2} \frac{\norm{\bB\bx}_2}{\normtwo{\bx}} \\
&\leq \max_{\by \neq \bzero} \frac{\norm{\bA \by}_2}{\norm{\by}_2} \max_{\bx \neq \bzero} \frac{\norm{\bB\bx}_2}{\normtwo{\bx}}
=\normtwo{\bA}\norm{\bB}_2,
\end{aligned}
$$
where the equality ($*$) holds because if the maximum is obtained when $\bx\neq \bzero$ and $\bB\bx=\bzero$, the norm is $\norm{\bA\bB}=\bzero$. This holds only when $\bA\bB=\bzero$, and the submultiplicativity holds obviously.

Alternatively, we have 
$
\norm{\bA\bB}_2 = \max_{\norm{\bu}=1}\norm{\bA\bB \bu}_2,
$
where $\norm{\bA\bB \bu}_2\leq \normtwo{\bA}\norm{\bB\bu}_2\leq\normtwo{\bA}\norm{\bB}_2 $. This again indicates $\norm{\bA\bB}_2\leq \normtwo{\bA}\norm{\bB}_2 $ implicitly.
\end{proof}


\index{Orthogonally invariance}
\begin{proposition}[Orthogonally Invariance of Spectral]\label{proposition:submul_spec}
Let $\bA\in \real^{m\times n}$ be given, and let $\bU\in \real^{m\times m}$ and $\bV\in \real^{n\times n}$ be orthogonal matrices. Then,
$
\norm{\bA }_2 =  \norm{\bU\bA\bV }_2.
$
\end{proposition}
\begin{proof}[of Proposition~\ref{proposition:submul_spec}]
We notice that 
$$
\begin{aligned}
\normtwo{\bU\bA\bV}  &=\mathop{\max}_{\bx\neq\bzero} \frac{\norm{\bU\bA\bV\bx}_2}{\normtwo{\bx}}
=\mathop{\max}_{\bx\neq\bzero} \frac{\norm{\bA\bV\bx}_2}{\normtwo{\bx}}
=\mathop{\max}_{\bx\neq\bzero} \frac{\norm{\bA\bV\bx}_2}{\norm{\bV\bx}_2}
=\mathop{\max}_{\by\neq\bzero} \frac{\norm{\bA\by}_2}{\norm{\by}_2}
=\normtwo{\bA} ,
\end{aligned}
$$
This completes the proof.
\end{proof}

We conclude that the spectral norm is a normalized matrix norm.
\begin{proposition}[Normalized Spectral]\label{proposition:norma_spec}
%\begin{remark}[Properties of the Spectral Norm]\label{remark:2norm-properties}
The spectral norm is normalized such that:
\begin{enumerate}
\item \textit{Normalization}. When $\bA\neq \bzero$, we have $\norm{\frac{1}{\normtwo{\bA}} \bA}_2=1$.
\item \textit{Normalized}. $\norm{\bI}_2=1$.
\end{enumerate}
\end{proposition}



%\subsection*{Induced Matrix Norm: Generalizing Spectral Norm}
%More generally, many matrix norms can be generated  using the concept of induced norms.
%
%\index{Induced norm}
%\begin{definition}[Induced Matrix Norm: General Matrix Norm]\label{definition:induced_norm_app}
%Given a matrix $\bA\in \real^{m\times n}$, and two vector norms $\norm{\cdot}_a$ and $\norm{\cdot}_b$ on $\real^n$ and $\real^m$ respectively, the \textit{induced matrix norm} $\norm{\bA}_{a,b}$ is defined by 
%$$
%\norm{\bA}_{a,b} =  \mathop{\max}_{\norm{\bx}_a \neq  0 } \frac{\norm{\bA\bx}_b}{\norm{\bx}_a}  
%=  \mathop{\max}_{\norm{\bx}_a = 1} \norm{\bA\bx}_b .~\footnote{Although the norms  $\norm{\cdot}_a$ and $\norm{\cdot}_b$ can be chosen from a variety of options, $\ell_p$ norms are most commonly discussed in this context. Therefore, we may also refer to it as the  \textit{\holders induced matrix norm}.}
%$$
%The \textit{matrix-vector product inequality} can be easily derived from the above definition:
%\begin{equation}\label{equation:induced_ineqy_intern}
%\norm{\bA\bx}_b \leq \norm{\bA}_{a,b} \cdot \norm{\bx}_a.
%\end{equation}
%The induced matrix norm can also be referred to as the $(a,b)$-norm. When $a=b$, we simply call it an \textit{$a$-norm} and use the notation $\norm{\bA}_{a}$ instead of $\norm{\bA}_{a,a}$. When $a=b=2$, we have the classic spectral norm. The induced norm is defined as the maximum factor by which $\bA$ magnifies the length of nonzero vectors, where the length of the vector $\bx$ is measured with norm $\norm{\cdot}_a$ and the length of the transformed vector $\bA\bx$ is measured with norm $\norm{\cdot}_b$. 
%\end{definition}
%
%From the definition of the induced norm, we can see that the spectral norm is a special induced norm when $a=b=2$ such that $\normtwo{\bA} = \norm{\bA}_{2,2}$. Similarly, \textit{matrix 1-norm} can be obtained by
%\begin{equation}\label{equation:mat_one_norm}
%\textbf{Matrix 1-norm: }\norm{\bA}_1 = \mathop{\max}_{j=1,2,\ldots,n} \sum_{i=1}^{m}|a_{ij}| 
%=\mathop{\max}_{j=1,2,\ldots,n} \norm{\ba_j}_1
%,
%\end{equation}
%which is also called the \textit{maximum absolute column sum norm}. 
%To see this, given $\bA\in\real^{m\times n}$, we have 
%$$
%\norm{\bA\bx}_1=\norm{\sum_{i=1}^{n} x_i\ba_i}_1
%\leq 
%%\sum_{i=1}^{n} \norm{x_i\ba_i}_1
%%=
%\sum_{i=1}^{n} \abs{x_i}\norm{\ba_i}_1
%\leq 
%\sum_{i=1}^{n} \abs{x_i} \left(\mathop{\max}_{j=1,2,\ldots,n}\norm{\ba_j}_1 \right)
%=
%\norm{\bx}_1 \norm{\bA}_1.
%$$
%Therefore, $ \mathop{\max}_{\norm{\bx}_1 = 1} \norm{\bA\bx}_1 \leq \norm{\bA}_1$.
%On the other hand, consider $\mathop{\max}_{\norm{\bx}_1 = 1}\norm{\bA\bx}_1$ for $\bx=\be_j,\,\forall j\in\{1,2,\ldots,n\}$, we have
%$
%\mathop{\max}_{\norm{\bx}_1 = 1}\norm{\bA\bx}_1\geq \mathopmax{j=1,2,\ldots,n}\norm{\ba_j}_1=\norm{\bA}_1.
%$
%By ``sandwiching", we have $\mathop{\max}_{\norm{\bx}_1 = 1}\norm{\bA\bx}_1=\norm{\bA}_1$.
%Similarly, the \textit{matrix $\infty$-norm} can be obtained by 
%\begin{equation}\label{equation:mat_inf_norm}
%\textbf{Matrix $\infty$-norm: }\norm{\bA}_\infty = \mathop{\max}_{i=1,2,\ldots,m} \sum_{j=1}^{n}|a_{ij}| ,
%\end{equation}
%which is also called the \textit{maximum absolute row sum norm}.
%\begin{exercise}[Maximum Absolute Row Sum Norm]
%Prove that $\norm{\bA}_\infty$ is induced from the $\ell_\infty$ vector norm.
%\end{exercise}
%\index{Equi-induced matrix norm}
%\index{Normalized matrix norm}
%\begin{exercise}[Equi-Induced and Normalized Norm]
%Let $a=b$ and let $\bA\in\real^{n\times n}$ be square in Definition~\ref{definition:induced_norm_app}. Then the induced norm is also called a \textbf{equi-induced norm}. Show that the equi-induced norm is normalized: $\norm{\bI_n}=1$ and $\norm{\bA}\geq 1$.
%\end{exercise}
%
%\index{Submultiplicativity}
%We have shown that the spectral norm, which is a special induced norm, is submultiplicative.
%In fact, all induced norms satisfy the submultiplicative property.
%\begin{theorem}[Submultiplicativity of Induced Norms]\label{theorem:Submultiplicativity_induced}
%For $a,b,c\geq 1$ and matrices $\bA,\bB$ (provided the matrix product $\bA\bB$ is defined), we have 
%$$
%\norm{\bA\bB}_{a,b} \leq \norm{\bA}_{c,b}\norm{\bB}_{a,c}.
%$$
%That is, the norm of a
%product is bounded by the product of the norms.
%\end{theorem}
%\begin{proof}[of Theorem~\ref{theorem:Submultiplicativity_induced}]
%From the definition of the induced norm, we have
%$$
%\begin{aligned}
%\norm{\bA\bB}_{a,b} &= \mathop{\max}_{\norm{\bx}_a=1} \norm{\bA\bB\bx}_b
%\leq \mathop{\max}_{\norm{\bx}_a=1}  \norm{\bA}_{c, b} \norm{\bB\bc}_{c} 
%\leq \mathop{\max}_{\norm{\bx}_a=1}\norm{\bA}_{c,b} \norm{\bB}_{a,c} \norm{\bx}_a .
%\end{aligned}
%$$
%This completes the proof.
%\end{proof}
%
%\begin{remark}[Condition Number]
%For a nonsingular $\bA\in\real^{n\times n}$,
%we note that $1 = \norm{\bI}_{b,b} \leq \norm{\bA}_{a,b} \norm{\bA^{-1}}_{b,a}$. The value $\kappa_{a,b}=\norm{\bA}_{a,b} \norm{\bA^{-1}}_{b,a}$ is normally known as the \textit{condition number} with respect to the induced vector norms $\norm{\cdot}_a$ and $\norm{\cdot}_b$. See Appendix~\ref{appendix:condition_number}  for a further discussion.
%\end{remark}
%
%
%
%The following properties of the induced norm are useful for evaluating the theory of condition numbers  (Appendix~\ref{appendix:condition_number}).
%\begin{lemma}[Induced Norm Properties]\label{lemma:induced_norm_property}
%\begin{enumerate}
%\item
%Given $\by\in \real^m$, $\bx\in \real^n$, and $a,b\geq 1$ with $\frac{1}{a}+\frac{1}{a^\star}=1$ and $\frac{1}{b}+\frac{1}{b^\star}=1$, it follows that 
%$$
%\norm{\by\bx^\top}_{a,b} = \norm{\by}_b \norm{\bx}_{a^\star}.
%$$
%\item 
%Given $\by\in \real^m$ and $\bx\in \real^n$ with $\norm{\bx}_a =\norm{\by}_b=1$, then there exists a matrix $\bA$ such that $\norm{\bA}_{a,b}=1$ and $\bA\bx=\by$.
%\item 
%Particularly, we have $\norm{\bA^\top}_{a,b} = \norm{\bA}_{b^\star, a^\star}$.
%\end{enumerate}
%\end{lemma}
%\begin{proof}[of Lemma~\ref{lemma:induced_norm_property}]
%For (1), from the definition of the induced norm, we have 
%$$
%\norm{\by\bx^\top}_{a,b} =\mathop{\max}_{\norm{\bz}_a= 1} \norm{\by\bx^\top\bz}_b
%= \norm{\by}_b \mathop{\max}_{\norm{\bz}_a= 1} |\bx^\top\bz| =  \norm{\by}_b  \norm{\bx}_{a^\star},
%$$
%where the last equality follows from Equation~\eqref{equation:dual_norm_equa}.
%
%For (2), again from Equation~\eqref{equation:dual_norm_equa}, there exists  a vector $\bz\in \real^n$ such that $\norm{\bz}_{a^\star}=1$ and $\bz^\top\bx=1$. Therefore, $\bA=\by\bz^\top$ satisfies $\bA\bx=\by$. And from (1), we have $\norm{\bA}_{a,b}=\norm{\by}_b \norm{\bz}_{a^\star}=1.$ 
%
%For (3), again from the definition of the induced norm, we have 
%$$
%\begin{aligned}
%\norm{\bA^\top}_{a,b} &= \mathop{\max}_{\norm{\bx}_a=1} \norm{\bA^\top\bx}_b
%= \mathop{\max}_{\norm{\bx}_a=1} \mathop{\max}_{\norm{\bz}_{b^\star}=1} \bx^\top \bA\bz
%=\mathop{\max}_{\norm{\bz}_{b^\star}} \norm{\bA\bz}_{a^\star} 
%= \norm{\bA}_{b^\star, a^\star},
%\end{aligned}
%$$
%where the second and third equalities follow from Equation~\eqref{equation:dual_norm_equa}.
%This completes the proof.
%\end{proof}
%
%
%Similarly to the vector norms in Theorem~\ref{theorem:equivalence-vector-norm},  matrix norms also admit an equivalence statement.
%\begin{theorem}[Equivalence of Matrix Norms]\label{theorem:equiv_mat_norm}
%Let $\norm{\cdot}_a$ and $\norm{\cdot}_b$ be two different matrix norms: $\real^{m\times n}\rightarrow \real$. Then there exist positive scalars $\alpha$ and $\beta$ such that for all $\bA \in \real^{m\times n}$,
%$$
%\alpha\norm{\bA}_a \leq \norm{\bA}_b \leq \beta\norm{\bA}_a.
%$$
%\end{theorem}
%The proof is similar to that of Theorem~\ref{theorem:equivalence-vector-norm}, and we shall not repeat the details. 
%The equivalence theorem again states that if a matrix is small in one norm, it is also small in other norms, and vice versa.
%
%
%
%We have shown in Lemma~\ref{lemma:construct_norm} how to construct a norm from another norm.
%The induced norm can also be constructed using another induced norm (when the underlying matrix is square).
%\begin{proposition}[Construct Induced Norms from Induced Norms]\label{propo:cons_ind}
%Let $\norm{\cdot}$ be a matrix norm (resp. a submultiplicative matrix norm) on $\real^{n\times n}$ and $\bS\in\real^{n\times n}$ be nonsingular. Then, the following function on matrix $\bA\in\real^{n\times n}$ is also a matrix norm (resp. a submultiplicative matrix norm):
%$$
%\norm{\bA}_{\bS} = \norm{\bS\bA\bS^{-1}}.
%$$
%Moreover, if $\norm{\cdot }$ is induced from the vector norm $\norm{\cdot}_v$ on $\real^n$, the matrix norm $\norm{\bA}_{\bS}$ is induced from the vector norm $\norm{\bx}_{\bS}=\norm{\bS\bx}_v$ on $\real^n$ (the $\bS$-norm in \eqref{equation:s_norm} with nonsingular $\bS$).
%\end{proposition}
%\begin{proof}[of Proposition~\ref{propo:cons_ind}]
%The proof for the first part is left as an exercise.
%And we note that the submultiplicativity of $\norm{\bA}_{\bS}$ follows from the submultiplicativity of $\norm{\bA}$.
%For the second part, we have
%$$
%\mathopmax{\norm{\bx}_{\bS}=1} \norm{\bA\bx}_{\bS}
%=
%\mathopmax{\norm{{\bS}\bx}=1} \norm{\bS\bA\bx}_v
%=
%\mathopmax{\norm{\by}=1} \norm{\bS\bA\bS^{-1}\by}_v
%=
%\norm{\bS\bA\bS^{-1}},
%$$
%from which the result follows.
%\end{proof}
%
%
%\index{Subordinate}
%\subsection*{Subordinate Property}
%
%We note in the induced norm that $	\norm{\bA\bx}_b \leq \norm{\bB}_{a,b} \cdot \norm{\bx}_a$ (Equation~\eqref{equation:induced_ineqy_intern}). 
%Recall that the matrix norm measures how much a vector is ``stretched" in this context. 
%This property is known as the \textit{subordinate property} of a matrix norm with respect to given vector norms.
%\begin{definition}[Subordinate Matrix Norm]\label{definition:subordinate_matrix_norm}
%A matrix norm $\norm{\cdot}: \real^{m\times n} \rightarrow \real$ is said to be subordinate to vector norms $\norm{\cdot}_a: \real^n\rightarrow \real$ and  $\norm{\cdot}_b: \real^m\rightarrow \real$ if for all vectors $\bx\in\real^n$ and matrices $\bA\in \real^{m\times n}$,
%$$
%\norm{\bA\bx}_b \leq \norm{\bA} \cdot \norm{\bx}_a.
%$$
%\end{definition}
%
%The following corollary  follows immediately from the definition of the induced norm.
%\begin{corollary}[Subordinate Induced Norm]
%The induced matrix norm $\norm{\cdot}_{a,b}$ is subordinate to the vector norms $\norm{\cdot}_a$ and $\norm{\cdot}_b$ that induce it.
%\end{corollary}
%
%
%
%\begin{corollary}[Subordinate Frobenius Norm]\label{corollary:subordinate_frobenius}
%The Frobenius norm is subordinate to the $\ell_2$ vector norm.
%\end{corollary}
%\begin{proof}[of Corollary~\ref{corollary:subordinate_frobenius}]
%Without loss of generality, we assume $\bx\neq \bzero \in\real^n $. 
%Given a matrix $\bA\in \real^{m\times n}$, we have 
%$$
%\begin{aligned}
%\normtwo{\bA\bx} = \frac{\normtwo{\bA\bx}}{\normtwo{\bx}} \normtwo{\bx} \leq
%\mathop{\max }_{\by\neq \bzero } \frac{\norm{\bA\by}_2}{\norm{\by}_2} \normtwo{\bx} 
%= \mathop{\max }_{\norm{\by}_2=1 }  \norm{\bA\by}_2 \normtwo{\bx}   = 
%\normtwo{\bA} \normtwo{\bx} .
%\end{aligned}
%$$
%Since $\norm{\bA}_F =\sqrt{\sigma_1^2+\sigma_2^2+\ldots+\sigma_r^2}$ and $\normtwo{\bA} = \sigma_1$ ($\sigma_1$ is the largest singular value), we also have $\normtwo{\bA} \leq \norm{\bA}_F$. Combining the two results, the claim follows.
%\end{proof}





%We provide an inequality involving subordinate norms, which will be useful in the analysis of condition numbers.
%To see this, we  need to use the spectral lemma of norms (given submultiplicativity). We prove in Theorem~\ref{theorem:schur_inequality} that the sum of squared eigenvalues is bounded by the Frobenius norm. The spectral radius (Definition~\ref{definition:spectrum}) of any square matrix is bounded for any (submultiplicative) matrix norm (Equation~\eqref{equation:bounds_submulnorm}).
%
%\begin{proposition}[Subordinate Inequality]\label{proposition:subordinate_ineq1}
%Let $\norm{\cdot}:\real^{n\times n}\rightarrow \real$ be any submultiplicative induced matrix norm. Then,
%\begin{enumerate}
%\item If $\norm{\cdot}$ is a subordinate matrix norm,  and  $\bA\in\real^{n\times n}$ is any square matrix satisfying $\norm{\bA}<1$, then the matrix $\bI+\bA$ is invertible and 
%$$
%\frac{1}{1+\norm{\bA}}
%\leq 
%\norm{(\bI+\bA)^{-1}}
%\leq \frac{\norm{\bI}}{1-\norm{\bA}}.
%$$
%When $\norm{\cdot}$ is normalized, $\norm{\bI}=1$ in the right inequality~\footnote{Actually, the only subordinate unitarily invariant norm is the spectral norm, which is normalized: $\normtwo{\bI}=1$.
%}.
%\item If  the matrix $\bI+\bA$ is singular, then $\norm{\bA}\geq 1$ for every matrix norm (nor necessarily subordinate).
%\end{enumerate}
%\end{proposition}
%\begin{proof}[of Proposition~\ref{proposition:subordinate_ineq1}]
%For (1), since $(\bI+\bA)\bx = \bzero$ implies $\bA\bx = -\bx$, we have $\Vert\bx\Vert = \Vert\bA\bx\Vert$.
%By the subordinate property, if $(\bI+\bA)\bx = \bzero$, we get 
%$$
%\Vert\bx\Vert = \Vert\bA\bx\Vert \leq 
%\norm{\bA} \norm{\bx}.
%$$
%Since $\Vert\bA\Vert<1$, if $\bx\neq \bzero$, we have 
%$
%\Vert\bA\bx\Vert
%< \Vert\bx\Vert.
%$
%This leads to a contradiction to $\Vert\bx\Vert = \Vert\bA\bx\Vert$.
%Therefore, $\bx$ must be $\bzero$. Hence, the null space of  $\bI+\bA$ is of dimension 0, and  $\bI+\bA$ is nonsingular. Similarly, we can also show that $\bI-\bA$ is nonsingular.
%Then we get
%$$
%(\bI+\bA)^{-1} +\bA(\bI+\bA)^{-1} = (\bI+\bA)(\bI+\bA)^{-1} = \bI
%\quad\implies\quad 
%(\bI+\bA)^{-1} = \bI - \bA(\bI+\bA)^{-1}.
%$$
%By the triangle inequality and submultiplicative property, we have 
%$$
%\Vert (\bI+\bA)^{-1}\Vert 
%\leq 
%\norm{\bI}+ \Vert\bA\Vert \left\Vert (\bI+\bA)^{-1}\right\Vert
%\quad\implies\quad
%\norm{(\bI+\bA)^{-1}}
%\leq \frac{\norm{\bI}}{1-\Vert\bA\Vert}.
%$$
%On the other hand, since $\norm{\cdot}$ is submultiplicative, we have $\norm{\bI}\geq 1$ (Equation~\eqref{equation:power_subm2}). Since $\bI-\bA$ is nonsingular, we obtain the left inequality: 
%$$
%1\leq \norm{\bI}
%=\norm{(\bI-\bA)(\bI-\bA)^{-1}}
%\leq 
%\norm{(\bI-\bA)}\norm{(\bI-\bA)^{-1}}
%\leq 
%(1+\norm{\bA})\norm{(\bI-\bA)^{-1}}.
%$$
%
%For (2), if the norm is not subordinate and $\bI+\bA$ is singular, then $-1$ is an eigenvalue of $\bA$. By Equation~\eqref{equation:bounds_submulnorm}, it follows that 
%$$
%|-1|\leq \rho(\bA) \leq \Vert\bA\Vert,
%$$
%from which the result follows.
%\end{proof}






\section{Symmetry, Definiteness, and Quadratic Models}
In this section, we will introduce the fundamental concepts of symmetric matrices, positive definite matrices, and quadratic functions (models), which are essential in the modeling of many optimization problems.
\subsection{Symmetric Matrices}\label{section:symmetric_mats}
We introduce four important properties of symmetric matrices.
The following proposition states  that symmetric matrices have only real eigenvalues.
\begin{proposition}[Symmetric Property-I: Real Eigenvalues]\label{proposition:real-eigenvalues-spectral}
The eigenvalues of any symmetric matrix are all real. 
\end{proposition}
\begin{proof}[of Proposition~\ref{proposition:real-eigenvalues-spectral}]
Suppose  $\lambda$ is an eigenvalue of a symmetric matrix $\bA$, and let it be a complex number $\lambda=a+ib$, where $a,b$ are real. Its complex conjugate is $\bar{\lambda}=a-ib$. 
Similarly, let the corresponding complex eigenvector be $\bx = \bc+i\bd$ with its conjugate $\bar{\bx}=\bc-i\bd$, where $\bc, \bd$ are real vectors. Then, we have 
$$
\bA \bx = \lambda \bx\qquad   \underrightarrow{\text{ leads to }}\qquad  \bA \bar{\bx} = \bar{\lambda} \bar{\bx}\qquad   \underrightarrow{\text{ transpose to }}\qquad  \bar{\bx}^\top \bA =\bar{\lambda} \bar{\bx}^\top.
$$
We take the dot product of the first equation with $\bar{\bx}$ and the last equation with $\bx$:
$$
\bar{\bx}^\top \bA \bx = \lambda \bar{\bx}^\top \bx \qquad \text{and } \qquad \bar{\bx}^\top \bA \bx = \bar{\lambda}\bar{\bx}^\top \bx.
$$
Then we have the equality $\lambda\bar{\bx}^\top \bx = \bar{\lambda} \bar{\bx}^\top\bx$. Since $\bar{\bx}^\top\bx = (\bc-i\bd)^\top(\bc+i\bd) = \bc^\top\bc+\bd^\top\bd$ is a real number, we deduce that the imaginary part of $\lambda$ must be zero. Thus, $\lambda$ is real.
\end{proof}

\begin{proposition}[Symmetric Property-II: Orthogonal Eigenvectors]\label{proposition:orthogonal-eigenvectors}
The eigenvectors  corresponding to distinct eigenvalues of a symmetric matrix are orthogonal. 
Consequently, these eigenvectors can be normalized to form an orthonormal basis, since 
$$
\bA\bx = \lambda \bx 
\qquad \implies \qquad 
\bA\frac{\bx}{\normtwo{\bx}} = \lambda \frac{\bx}{\normtwo{\bx}},
$$ 
which preserves the eigenvalue.
\end{proposition}
\begin{proof}[of Proposition~\ref{proposition:orthogonal-eigenvectors}]
Let $\lambda_1, \lambda_2$ be distinct eigenvalues of $\bA$, with corresponding eigenvectors $\bx_1, \bx_2$, satisfying $\bA\bx_1=\lambda \bx_1$ and $\bA\bx_2 = \lambda_2\bx_2$. We have the following equalities:
$$
\bA\bx_1=\lambda_1 \bx_1 
\qquad \implies \qquad 
\bx_1^\top \bA =\lambda_1 \bx_1^\top 
\qquad \implies \qquad 
\bx_1^\top \bA \bx_2 =\lambda_1 \bx_1^\top\bx_2,
$$
and 
$$
\bA\bx_2 = \lambda_2\bx_2 
\qquad \implies \qquad 
\bx_1^\top\bA\bx_2 = \lambda_2\bx_1^\top\bx_2,
$$
implying $\lambda_1 \bx_1^\top\bx_2=\lambda_2\bx_1^\top\bx_2$. Since  $\lambda_1\neq \lambda_2$, the eigenvectors are orthogonal.
\end{proof}



%In the above Proposition~\ref{proposition:orthogonal-eigenvectors}, we demonstrate that  eigenvectors corresponding to distinct eigenvalues  of a symmetric matrix are orthogonal. 
%More generally, we now prove an important theorem stating that the eigenvectors corresponding to distinct eigenvalues of any matrix are linearly independent.
%\begin{theorem}[Independent Eigenvector Theorem]\label{theorem:independent-eigenvector-theorem}
%Let $\bA\in \real^{n\times n}$ be any matrix with $k$ distinct eigenvalues, then any set of $k$ corresponding (nonzero) eigenvectors are linearly independent.
%\end{theorem}
%\begin{proof}[of Theorem~\ref{theorem:independent-eigenvector-theorem}]
%We will prove this by induction. First, we will prove that any two eigenvectors corresponding to distinct eigenvalues are linearly independent. Suppose  $\bv_1$ and $\bv_2$ are eigenvectors corresponding to distinct eigenvalues $\lambda_1$ and $\lambda_2$, respectively. Assume there exists a nonzero vector $\bx=[x_1,x_2] \neq \bzero $ such that  
%\begin{equation}\label{equation:independent-eigenvector-eq1}
%	x_1\bv_1+x_2\bv_2=\bzero.
%\end{equation}
%That is, we assume $\bv_1$ and $\bv_2$ are linearly dependent.
%Premultiply Equation~\eqref{equation:independent-eigenvector-eq1}  by $\bA$, we obtain
%\begin{equation}\label{equation:independent-eigenvector-eq2}
%	x_1 \lambda_1\bv_1 + x_2\lambda_2\bv_2 = \bzero.
%\end{equation}
%Premultiply Equation~\eqref{equation:independent-eigenvector-eq1}  by $\lambda_2$, we get 
%\begin{equation}\label{equation:independent-eigenvector-eq3}
%	x_1\lambda_2\bv_1 + x_2\lambda_2\bv_2 = \bzero.
%\end{equation}
%Subtract Equation~\eqref{equation:independent-eigenvector-eq2} from Equation~\eqref{equation:independent-eigenvector-eq3}, we find
%$$
%x_1(\lambda_2-\lambda_1)\bv_1 = \bzero.
%$$
%Since $\lambda_2\neq \lambda_1$ and $\bv_1\neq \bzero$, we must have $x_1=0$. From Equation~\eqref{equation:independent-eigenvector-eq1} and $\bv_2\neq \bzero$, we must also have $x_2=0$, which arrives at a contradiction. Therefore, $\bv_1$ and $\bv_2$ are linearly independent.
%
%Now, assume that any $j<k$ eigenvectors corresponding to distinct eigenvalues are linearly independent. We aim to show that any $j+1$ eigenvectors are also linearly independent. Suppose $\bv_1, \bv_2, \ldots, \bv_j$ are linearly independent, and $\bv_{j+1}$ is dependent on the first $j$ eigenvectors. That is, there exists a nonzero vector $\bx=[x_1,x_2,\ldots, x_{j}]\neq \bzero$ satisfying  
%\begin{equation}\label{equation:independent-eigenvector-zero}
%	\bv_{j+1}=	x_1\bv_1+x_2\bv_2+\ldots+x_j\bv_j .
%\end{equation}
%Suppose the $j+1$ eigenvectors correspond to distinct eigenvalues $\lambda_1,\lambda_2,\ldots,\lambda_j,\lambda_{j+1}$.
%Premultiply Equation~\eqref{equation:independent-eigenvector-zero} by $\bA$, we obtain
%\begin{equation}\label{equation:independent-eigenvector-zero2}
%	\lambda_{j+1} \bv_{j+1} = x_1\lambda_1\bv_1+x_2\lambda_2\bv_2+\ldots+x_j \lambda_j\bv_j .
%\end{equation}
%Premultiply Equation~\eqref{equation:independent-eigenvector-zero} by $\lambda_{j+1}$, we get
%\begin{equation}\label{equation:independent-eigenvector-zero3}
%	\lambda_{j+1} \bv_{j+1} = x_1\lambda_{j+1}\bv_1+x_2\lambda_{j+1}\bv_2+\ldots+x_j \lambda_{j+1}\bv_j .
%\end{equation}
%Subtract Equation~\eqref{equation:independent-eigenvector-zero3} from Equation~\eqref{equation:independent-eigenvector-zero2}, we find
%$$
%x_1(\lambda_{j+1}-\lambda_1)\bv_1+x_2(\lambda_{j+1}-\lambda_2)\bv_2+\ldots+x_j (\lambda_{j+1}-\lambda_j)\bv_j = \bzero. 
%$$
%From the assumption, $\lambda_{j+1} \neq \lambda_i$ for all $i\in \{1,2,\ldots,j\}$, and $\bv_i\neq \bzero$ for all $i\in \{1,2,\ldots,j\}$. We must have $x_1=x_2=\ldots=x_j=0$, which leads to a contradiction. Therefore, the eigenvectors $\bv_1,\bv_2,\ldots,\bv_j,\bv_{j+1}$ are linearly independent. This completes the proof.
%\end{proof}
%
%
%A direct consequence of the  above theorem is as follows:
%\begin{corollary}[Independent Eigenvector Theorem, CNT.]\label{corollary:independent-eigenvector-theorem-basis}
%If a matrix $\bA\in \real^{n\times n}$ has $n$ distinct eigenvalues, then any set of $n$ corresponding eigenvectors form a basis for $\real^n$.
%\end{corollary}

\begin{proposition}[Symmetric Property-III: Orthonormal Eigenvectors for Duplicate Eigenvalue]\label{proposition:eigen-multiplicity}
Let $\bA\in\real^{n\times n}$ be symmetric.
If $\bA$ has a duplicate eigenvalue $\lambda_i$ with multiplicity  $k\geq 2$, then there exist $k$ orthonormal eigenvectors corresponding to $\lambda_i$.
\end{proposition}
\begin{proof}[of Proposition~\ref{proposition:eigen-multiplicity}]
We start by noting that there exists at least one unit-length eigenvector $\bx_{i1}$ corresponding to $\lambda_i$. 
Furthermore, for such an eigenvector $\bx_{i1}$, we can consistently find  $n-1$ additional orthonormal vectors $\by_2, \by_3, \ldots, \by_n$ such that $\{\bx_{i1}, \by_2, \by_3, \ldots, \by_n\}$ constitutes an orthonormal basis of $\real^n$. 
Define the matrices $\bY_1$ and $\bP_1$ as follows:
$$
\bY_1\triangleq[\by_2, \by_3, \ldots, \by_n] \qquad \text{and} \qquad \bP_1\triangleq[\bx_{i1}, \bY_1].
$$
Since $\bA$ is symmetric, we then have
$
\bP_1^\top\bA\bP_1 = \footnotesize\begin{bmatrix}
\lambda_i &\bzero \\
\bzero & \bY_1^\top \bA\bY_1
\end{bmatrix}.
$
Since $\bP_1$ is nonsingular and orthogonal, $\bA$ and $\bP_1^\top\bA\bP_1$ are similar matrices such that they have the same eigenvalues  (see, for example, \citet{lu2021numerical}).
Using the determinant of block matrices~\footnote{If matrix $\bM$ has a block formulation: $\bM=\scriptsize\begin{bmatrix}
		\bA & \bB \\
		\bC & \bD 
	\end{bmatrix}$, then $\det(\bM) = \det(\bA)\det(\bD-\bC\bA^{-1}\bB)$.}, we get: 
$$
\det(\bP_1^\top\bA\bP_1 - \lambda\bI_n) =
(\lambda_i - \lambda )\det(\bY_1^\top \bA\bY_1 - \lambda\bI_{n-1}).
$$
If $\lambda_i$ has a multiplicity of $k\geq 2$, then the term $(\lambda_i-\lambda)$ appears $k$ times in the characteristic  polynomial resulting from the determinant $\det(\bP_1^\top\bA\bP_1 - \lambda\bI_n)$, i.e., this term appears  $k-1$ times in the characteristic  polynomial from $\det(\bY_1^\top \bA\bY_1 - \lambda\bI_{n-1})$. In other words, $\det(\bY_1^\top \bA\bY_1 - \lambda_i\bI_{n-1})=0$, and $\lambda_i$ is an eigenvalue of $\bY_1^\top \bA\bY_1$ with multiplicity $k-1$. 



Let $\bB\triangleq\bY_1^\top \bA\bY_1$. Since $\det(\bB-\lambda_i\bI_{n-1})=0$, the null space of $\bB-\lambda_i\bI_{n-1}$ is nonempty. Suppose $(\bB-\lambda_i\bI_{n-1})\bn = \bzero$, i.e., $\bB\bn=\lambda_i\bn$, where $\bn$ is an eigenvector of $\bB$.

From $
\bP_1^\top\bA\bP_1 = 
\footnotesize
\begin{bmatrix}
	\lambda_i &\bzero \\
	\bzero & \bB
\end{bmatrix},
$
we have $
\bA\bP_1 
\footnotesize
\begin{bmatrix}
	z \\
	\bn 
\end{bmatrix} 
= 
\bP_1
\footnotesize
\begin{bmatrix}
	\lambda_i &\bzero \\
	\bzero & \bB
\end{bmatrix}
\begin{bmatrix}
	z \\
	\bn 
\end{bmatrix}$, where $z$ is any scalar. 
From the left side of this equation:
\begin{equation}\label{equation:spectral-pro4-right}
	\begin{aligned}
		\bA\bP_1 
		\begin{bmatrix}
			z \\
			\bn 
		\end{bmatrix} 
		&=
		\begin{bmatrix}
			\lambda_i\bx_{i1}, \bA\bY_1
		\end{bmatrix}
		\begin{bmatrix}
			z \\
			\bn 
		\end{bmatrix} 
		=\lambda_iz\bx_{i1} + \bA\bY_1\bn.
	\end{aligned}
\end{equation}
From the right side of the equation:
\begin{equation}\label{equation:spectral-pro4-left}
	\begin{aligned}
		\bP_1
		\begin{bmatrix}
			\lambda_i &\bzero \\
			\bzero & \bB
		\end{bmatrix}
		\begin{bmatrix}
			z \\
			\bn 
		\end{bmatrix}
		&=
		\begin{bmatrix}
			\bx_{i1} & \bY_1
		\end{bmatrix}
		\begin{bmatrix}
			\lambda_i &\bzero \\
			\bzero & \bB
		\end{bmatrix}
		\begin{bmatrix}
			z \\
			\bn 
		\end{bmatrix}
		=
		\begin{bmatrix}
			\lambda_i\bx_{i1} & \bY_1\bB 
		\end{bmatrix}
		\begin{bmatrix}
			z \\
			\bn 
		\end{bmatrix}\\
		&= \lambda_i z \bx_{i1} + \bY_1\bB \bn 
		=\lambda_i z \bx_{i1} + \lambda_i \bY_1 \bn,
	\end{aligned}
\end{equation}
where the last equality is due to  $\bB \bn=\lambda_i\bn$.
Combining \eqref{equation:spectral-pro4-left} and \eqref{equation:spectral-pro4-right}, we obtain 
$$
\bA\bY_1\bn = \lambda_i\bY_1 \bn,
$$
which means $\bY_1\bn$ is an eigenvector of $\bA$ corresponding to the eigenvalue $\lambda_i$ (the same eigenvalue corresponding to $\bx_{i1}$). Since $\bY_1\bn$ is a linear combination of $\by_2, \by_3, \ldots, \by_n$, which are orthonormal to $\bx_{i1}$, it can be chosen to be orthonormal to $\bx_{i1}$.

To conclude, if there exists an  eigenvector, $\bx_{i1}$, corresponding to the eigenvalue  $\lambda_i$ with a multiplicity  $k\geq 2$, we can construct a second eigenvector by choosing a vector from the null space of $(\bB-\lambda_i\bI_{n-1})$, as constructed above. 

Now, suppose  we have constructed the second eigenvector $\bx_{i2}$, which is orthonormal to $\bx_{i1}$.  
For such eigenvectors $\bx_{i1}$ and $\bx_{i2}$, we can always find  $n-2$ additional orthonormal vectors $\by_3, \by_4, \ldots, \by_n$ such  that $\{\bx_{i1},\bx_{i2}, \by_3, \by_4, \ldots, \by_n\}$ forms an orthonormal basis for $\real^n$. 
Place these vectors  $\by_3, \by_4, \ldots, \by_n$ into matrix $\bY_2$ and $\{\bx_{i1},\bx_{i2},  \by_3, \by_4, \ldots, \by_n\}$ into matrix $\bP_2$:
$$
\bY_2\triangleq[\by_3, \by_4, \ldots, \by_n] \qquad \text{and} \qquad \bP_2\triangleq[\bx_{i1}, \bx_{i2},\bY_1].
$$
Since $\bA$ is symmetric, we then have
$$
\bP_2^\top\bA\bP_2 = 
\small
\begin{bmatrix}
	\lambda_i & 0 &\bzero \\
	0& \lambda_i &\bzero \\
	\bzero & \bzero & \bY_2^\top \bA\bY_2
\end{bmatrix}
\triangleq
\begin{bmatrix}
	\lambda_i & 0 &\bzero \\
	0& \lambda_i &\bzero \\
	\bzero & \bzero & \bC
\end{bmatrix},
$$
where $\bC\triangleq\bY_2^\top \bA\bY_2$ such that $\det(\bP_2^\top\bA\bP_2 - \lambda\bI_n) = (\lambda_i-\lambda)^2 \det(\bC - \lambda\bI_{n-2})$. If the multiplicity of $\lambda_i$ is $k\geq 3$, then $\det(\bC - \lambda_i\bI_{n-2})=0$, and the null space of $\bC - \lambda_i\bI_{n-2}$ is not empty. Thus, we can still find a vector $\bn$ from the null space of $\bC - \lambda_i\bI_{n-2}$ such that  $\bC\bn = \lambda_i \bn$. Now we can construct a vector $\footnotesize\begin{bmatrix}
	z_1 \\
	z_2\\
	\bn
\end{bmatrix}\in \real^n $, where $z_1$ and $ z_2$ are any scalar values, such that 
$$
\bA\bP_2\small\begin{bmatrix}
	z_1 \\
	z_2\\
	\bn
\end{bmatrix} = \normalsize\bP_2 
\small
\begin{bmatrix}
	\lambda_i & 0 &\bzero \\
	0& \lambda_i &\bzero \\
	\bzero & \bzero & \bC
\end{bmatrix}
\begin{bmatrix}
	z_1 \\
	z_2\\
	\bn
\end{bmatrix}.
$$
Similarly, from the left side of the above equation, we will get $\lambda_iz_1\bx_{i1} +\lambda_iz_2\bx_{i2}+\bA\bY_2\bn$. From the right side of the above equation, we will get $\lambda_iz_1\bx_{i1} +\lambda_i z_2\bx_{i2}+\lambda_i\bY_2\bn$. As a result, 
$$
\bA\bY_2\bn = \lambda_i\bY_2\bn,
$$
where $\bY_2\bn$ is an eigenvector of $\bA$, orthogonal to both $\bx_{i1}$ and $\bx_{i2}$. This eigenvector can also be normalized to ensure orthonormality with the first two eigenvectors.


The process can continue, ultimately yielding a set of $k$ orthonormal eigenvectors corresponding to $\lambda_i$.

In fact, the dimension of the null space of $\bP_1^\top\bA\bP_1 -\lambda_i\bI_n$ is equal to the multiplicity $k$. It also follows that if the multiplicity of $\lambda_i$ is $k$, there cannot be more than $k$ orthogonal eigenvectors corresponding to $\lambda_i$. 
If there were more than $k$, it would lead to the conclusion that there are more than $n$  orthogonal eigenvectors in $\real^n$, which is a contradiction.
\end{proof}






For any matrix multiplication, the rank of the resulting matrix is at most the rank of the input matrices.
\begin{lemma}[Rank of $\bA\bB$]\label{lemma:rankAB}
Let $\bA\in \real^{m\times n}$ and  $\bB\in \real^{n\times k}$ be any matrices. Then, the rank of the product  $\bA\bB\in \real^{m\times k}$ satisfies $\rank$($\bA\bB$)$\leq$min($\rank$($\bA$), $\rank$($\bB$)).
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:rankAB}]
For the matrix product $\bA\bB$, we observe the following: 
\begin{itemize}
\item Every row of $\bA\bB$ is a linear combination of the rows of $\bB$. Hence, the row space of $\bA\bB$ is contained within the row space of $\bB$, implying that $\rank$($\bA\bB$)$\leq$$\rank$($\bB$).

\item Similarly, every column of $\bA\bB$ is a linear combination of the columns of $\bA$. Therefore, the column space of $\bA\bB$ is a subspace of the column space of $\bA$, which gives $\rank$($\bA\bB$)$\leq$$\rank$($\bA$).
\end{itemize}
Combining these two results, we conclude that $\rank$($\bA\bB$)$\leq$min($\rank$($\bA$), $\rank$($\bB$)).
\end{proof}
This lemma establishes that the rank of a symmetric matrix is equal to the number of its nonzero eigenvalues.
\begin{proposition}[Symmetric Property-IV: Rank of Symmetric Matrices]\label{proposition:rank-of-symmetric}
Let $\bA$ be an $n\times n$ real symmetric matrix. Then, 
$$
\text{rank($\bA$) =
	the total number of nonzero eigenvalues of $\bA$. }
$$
In particular, $\bA$ has full rank if and only if it is nonsingular. Moreover, the column space of $\bA$, denoted $\cspace(\bA)$, is spanned by the eigenvectors corresponding to its nonzero eigenvalues. 
\end{proposition}
\begin{proof}[of Proposition~\ref{proposition:rank-of-symmetric}]
For any symmetric matrix $\bA$, we have $\bA$, in spectral form, as $\bA = \bQ \bLambda\bQ^\top$ and also $\bLambda = \bQ^\top\bA\bQ$; see \eqref{equation:spec_decom}, which is a result of Symmetric Properties-I$\sim$III (Propositions~\ref{proposition:real-eigenvalues-spectral}$\sim$\ref{proposition:eigen-multiplicity}). 
By Lemma~\ref{lemma:rankAB}, we know that for any matrix multiplication, $\rank$($\bA\bB$)$\leq$min($\rank$($\bA$), $\rank$($\bB$)).
Applying this result:
\begin{itemize}
\item From $\bA = \bQ \bLambda\bQ^\top$, we have $\rank(\bA) \leq \rank(\bQ \bLambda) \leq \rank(\bLambda)$.
	
\item From $\bLambda = \bQ^\top\bA\bQ$, we have $\rank(\bLambda) \leq \rank(\bQ^\top\bA) \leq \rank(\bA)$.
\end{itemize}
Since both inequalities hold in opposite directions, we conclude that $\rank(\bA) = \rank(\bLambda)$, which is the total number of nonzero eigenvalues.

Furthermore, $\bA$ is nonsingular if and only if all  its eigenvalues are nonzero, which establishes that $\bA$ has full rank if and only if it is nonsingular.
\end{proof}

\subsection{Positive Definiteness}
A symmetric matrix can be  categorized into positive definite, positive semidefinite, negative definite, negative semidefinite, and indefinite types as follows.

\begin{definition}[Positive Definite and Positive Semidefinite\index{Positive definite}\index{Positive semidefinite}]\label{definition:psd-pd-defini}
A symmetric matrix $\bA\in \real^{n\times n}$ ($\bA\in\symmetric^n$) is considered \textit{positive definite (PD)} if $\bx^\top\bA\bx>0$ for all nonzero $\bx\in \real^n$, denoted by $\bA\succ \bzero$ or $\bA\in\pd^n$.
Similarly, a symmetric matrix $\bA\in \real^{n\times n}$ is called \textit{positive semidefinite (PSD)} if $\bx^\top\bA\bx \geq 0$ for all $\bx\in \real^n$,  denoted by $\bA\succeq\bzero$ or $\bA\in\psd^n$. 
%\footnote{
%In this book, a positive definite or a semidefinite matrix is always assumed to be symmetric, i.e., the notion of a positive definite matrix or semidefinite matrix is only interesting for symmetric matrices. And we only consider $\bx\in\real^n$; while the complex case for PD matrices are equivalent; see Problem~\ref{prob:pd_cmequiv}.
%}
%\footnote{Similarly, a complex matrix $\bA$ is said to be \textit{Hermitian positive definite (HSD)}  if $\bA$ is Hermitian and $\bz^\ast\bA\bz>0$ for all $\bz\in\complex^n$ with $\bz\neq \bzero$. However, the complex analog of  PD/PSD can be defined for non-Hermitian matrices. We will mostly consider PD/PSD for real symmetric matrices in this book.
%}
\footnote{A symmetric matrix $\bA\in\real^{n\times n}$ is called \textit{negative definite (ND) } if $\bx^\top\bA\bx<0$ for all nonzero $\bx\in\real^n$, denoted by $\bA\prec\bzero$ or $\bA\in\nd^n$; 
a symmetric matrix $\bA\in\real^{n\times n}$ is called \textit{negative semidefinite (NSD)}  if $\bx^\top\bA\bx\leq 0$ for all $\bx\in\real^n$, denoted by $\bA\preceq\bzero$ or $\bA\in\nsd^n$;
and a symmetric matrix $\bA\in\real^{n\times n}$ is called \textit{indefinite (ID)}  if there exist vectors $\bx$ and $\by\in\real^n$ such that $\bx^\top\bA\bx<0$ and $\by^\top\bA\by>0$.
}
\end{definition}

Given a negative definite matrix $\bA$, then $-\bA$ is a positive definite matrix; if $\bA$ is negative semidefintie matrix, then $-\bA$ is positive semidefinite.
The following property demonstrates that PD and PSD matrices have special forms of eigenvalues.
\begin{theorem}[PD Property-I: Eigenvalue Characterization Theorem]\label{theorem:eigen_charac}
A symmetric matrix $\bA$ is positive definite if and only if all its eigenvalues are positive. Similarly, a matrix $\bA$ is positive semidefinite if and only if all its eigenvalues are nonnegative.
Additionally,  a matrix is indefinite if and only if it possesses at least one positive eigenvalue and at least one negative eigenvalue.
%~\footnote{The trace, determinant, and principal minors of a positive (semi)definite matrix is discussed in Problem~\ref{prob:tr_de_pd}.}
Furthermore, we have the following implications:
\begin{itemize}
\item $\bA-\mu\bI\succeq \bzero$ if and only if $\lambda_{\min}(\bA) \geq \mu$;
\item  $\bA-\mu\bI\succ \bzero$ if and only if $\lambda_{\min}(\bA) > \mu$;
\item $\bA-\mu\bI\preceq \bzero$ if and only if $\lambda_{\max}(\bA) \leq \mu$;
\item $\bA-\mu\bI\prec \bzero$ if and only if $\lambda_{\max}(\bA) < \mu$;
\item $\lambda_{\min}(\bA)\bI\preceq \bA \preceq \lambda_{\max}(\bA)\bI$,
\end{itemize}
\noindent where $\lambda_{\min}(\bA)$ and $\lambda_{\max}(\bA)$ denote the minimum and maximum eigenvalues of $\bA$, respectively. 
The notation $\bB \prec \bC$ means that $\bC-\bB$ is PSD.
\end{theorem}
Given the eigenpair $(\lambda, \bx)$ of $\bA$, the forward implication can be shown that $\bx^\top\bA\bx=\lambda\bx^\top\bx>0$ such that $ \lambda=\frac{\bx^\top\bA\bx}{\bx^\top\bx}>0$ (resp. $\geq 0$) if $\bA$ is PD (resp. PSD).
The complete proof of this equivalence can be derived using the spectral theorem (Theorem~\ref{theorem:spectral_theorem}); the details can be found in \citet{lu2021numerical}.
This theorem provides an alternative definition of positive definiteness and positive semidefiniteness in terms of the eigenvalues of the matrix, which is fundamental for the Cholesky decomposition (Theorems~\ref{theorem:cholesky-factor-exist}).



Although not all components of a positive definite matrix are necessarily positive, the diagonal components of such a matrix are guaranteed to be positive, as stated in the following result.
\begin{theorem}[PD Property-II: Positive Diagonals of PD Matrices]\label{theorem:positive-in-pd}
The diagonal elements of a positive definite matrix $\bA\in\real^{n\times n}$ are all \textit{positive}. Similarly, the diagonal elements of a positive semidefinite matrix $\bB\in\real^{n\times n}$ are all \textit{nonnegative}.
Additionally,  the diagonal elements of an indefinite matrix $\bC\in\real^{n\times n}$ contains at least one positive diagonal and at least one negative diagonal.
\end{theorem}
\begin{proof}[of Theorem~\ref{theorem:positive-in-pd}]
By the definition of positive definite matrices, we have $\bx^\top\bA \bx >0$ for all nonzero vectors $\bx$. Consider the specific case where $\bx=\be_i$, with $\be_i$ being the $i$-th unit basis vector having the $i$-th entry equal to 1 and all other entries equal to 0. Then,
$$
\be_i^\top\bA \be_i = a_{ii}>0, \gapforall \forall i \in \{1, 2, \ldots, n\},
$$	
where $a_{ii}$ is the $i$-th diagonal component. The proofs for the second and the third parts follow a similar argument. This completes the proof.
\end{proof}

\subsection{Quadratic Functions and Quadratic Models}
%\paragraph{Quadratic form of positive definite matrices.}
\begin{figure}[h]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[Positive definite matrix: $\bA =\scriptsize \begin{bmatrix}
200 & 0 \\ 0 & 200
\end{bmatrix}$.]{\label{fig:quadratic_PD1}
\includegraphics[width=0.485\linewidth]{imgs/quadratic_PD.pdf}}
\subfigure[Negative definite matrix: $\bA = \scriptsize\begin{bmatrix}
-200 & 0 \\ 0 & -200
\end{bmatrix}$.]{\label{fig:quadratic_ND1}
\includegraphics[width=0.485\linewidth]{imgs/quadratic_ND.pdf}}
\subfigure[Semidefinite matrix: $\bA = \scriptsize\begin{bmatrix}
200 & 0 \\ 0 & 0
\end{bmatrix}$. A line runs through the bottom of the valley is the set of solutions.]{\label{fig:quadratic_singular1}
\includegraphics[width=0.485\linewidth]{imgs/quadratic_singular.pdf}}
\subfigure[Indefinte matrix: $\bA = \scriptsize\begin{bmatrix}
200 & 0 \\ 0 & -200
\end{bmatrix}$.]{\label{fig:quadratic_saddle1}
\includegraphics[width=0.485\linewidth]{imgs/quadratic_saddle.pdf}}
\caption{Loss surfaces for different quadratic forms, providing the surface plots and contour plots (\textcolor{mylightbluetext}{blue}=low,
\textcolor{mydarkyellow}{yellow}=high), where the upper graphs are the surface plots, and the lower ones are their projection (i.e., contours).}
\label{fig:different_quadratics1}
\end{figure}

We  further  discuss  linear systems with different types of matrices, the  quadratic form:
\begin{equation}\label{equation:quadratic-form-general-form1}
f(\bx) = \frac{1}{2} \bx^\top \bA \bx - \bb^\top \bx + c, \gap \bx\in \real^n,
\end{equation}
where $\bA\in \real^{n\times n}$, $\bb \in \real^n$, and $c$ is a scalar constant. Though the quadratic form in Equation~\eqref{equation:quadratic-form-general-form1} is an extremely simple model, it is rich enough to approximate many other functions, e.g., the Fisher information matrix \citep{amari1998natural}, and it captures key features of pathological curvature. The gradient of $f(\bx)$ at point $\bx$ is given by 
$$
\nabla f(\bx) = \frac{1}{2} (\bA^\top +\bA) \bx - \bb.
$$
Any optimum point of the function is the solution to the linear system $\frac{1}{2} (\bA^\top +\bA) \bx=  \bb $:
$$
\bx^* = 2(\bA^\top +\bA)^{-1}\bb.
$$
If $\bA$ is symmetric, the equation reduces to 
$
\nabla f(\bx) = \bA \bx - \bb.
$
Then the optimum point of the function is the solution of the linear system $\bA\bx=\bb$, where $\bA$ and $\bb$ are known matrix or vector, and $\bx$ is an unknown vector; and the optimum point of $\bx$ is thus given by 
$$
\bx^*  = \bA^{-1}\bb, \quad\text{if $\bA$ is nonsingular.}
$$
For different types of matrix $\bA$, the loss surface of $f(\bx)$ will vary as shown in Figure~\ref{fig:different_quadratics1}. When $\bA$ is positive definite, the surface is a \textit{convex bowl}; when $\bA$ is negative definite, on the contrary, the surface is a \textit{concave bowl}. $\bA$ also could be singular, in which case $\bA\bx-\bb=\bzero$ has more than one solution, and the set of solutions is a line (in the two-dimensional case) or a hyperplane (in the high-dimensional case).
This situation is similar to the case of a semidefinite quadratic form, as shown in Figure~\ref{fig:quadratic_singular1}.
Moreover, $\bA$ could be none of the above, then there exists a \textit{saddle point} (see Figure~\ref{fig:quadratic_saddle1}), where the gradient descent may fail (see discussions in the following chapters). In such senses, other methods, e.g., perturbed gradient descent \citep{jin2017escape}, can be applied to escape  saddle points. 

\paragraph{Diagonally dominant matrices.}
A specific form of diagonally dominant matrices constitutes a significant subset of positive semidefinite matrices.
\begin{definition}[Diagonally Dominant Matrices]
Given a symmetric matrix $\bA\in\real^{n\times n}$,  $\bA$ is called diagonally dominant if 
$$
\abs{a_{ii}} \geq \sum_{j\neq i} \abs{a_{ij}}, \qquad \forall i\in\{1,2,\ldots, n\};
$$
and $\bA$ is called strictly diagonally dominant if 
$$
\abs{a_{ii}} > \sum_{j\neq i} \abs{a_{ij}}, \qquad \forall i\in\{1,2,\ldots, n\}.
$$
\end{definition}


We  now show that \textit{diagonally dominant matrices} with nonnegative diagonal elements are positive semidefinite and that \textit{strictly diagonally dominant matrices} with positive diagonal elements are positive definite. 

\begin{theorem}[Positive Definiteness of Diagonally Dominant Matrices]\label{theorem:pd_diag_domi}
Given a symmetric matrix  $\bA\in\real^{n\times n}$, 
\begin{enumerate}[(i)]
\item If $\bA$ is diagonally dominant with nonnegative diagonals, then $\bA$ is positive semidefinite; 
\item If $\bA$ is strictly diagonally dominant with positive diagonals, then $\bA$ is positive definite.
\end{enumerate}

\end{theorem}
\begin{proof}[of Theorem~\ref{theorem:pd_diag_domi}]
\textbf{(i).} Suppose $\bA$ is not PSD and has a negative eigenvalue $\lambda$ associated with an eigenvector $\bv$ such that $\bA\bv=\lambda\bv$. 
Let $v_i$ be the element of $\bv$ with largest magnitude.
Consider the $i$-th element of $(\bA-\lambda\bI)\bv=\bzero$, we have
$$
\abs{a_{ii} - \lambda}\cdot \abs{v_i}
=
\abs{\sum_{j\neq i} a_{ij} v_j}
\leq 
\left( \sum_{ j\neq i} \abs{a_{ij}} \right) \abs{v_i}
\leq \abs{a_{ij}} \cdot \abs{v_i}.
$$
This implies $\abs{a_{ii} - \lambda} \leq \abs{a_{ij}}$ and leads to a contradiction.

\paragraph{(ii).}  From part (i), we known that $\bA$ is positive semidefinite. Suppose $\bA$ is not positive definite and has a zero eigenvalue $0$ associated with a nonzero eigenvector $\bv$ such that $\bA\bv=\bzero$.   Similarly, we have 
$$
\abs{a_{ii}}\cdot \abs{v_i}
=
\abs{\sum_{j\neq i} a_{ij} v_j}
\leq 
\left( \sum_{ j\neq i} \abs{a_{ij}} \right) \abs{v_i}
< \abs{a_{ij}} \cdot \abs{v_i},
$$
which is impossible and the result follows.
\end{proof}



\begin{exercise}[Quadratic Form]\label{exercise:quad_form_psd}
Consider the quadratic form $f(\bx) = \frac{1}{2} \bx^\top \bA \bx - \bb^\top \bx + c$, where $\bA\in\real^{n\times n}$ is symmetric, $ \bx\in \real^n$, and $c\in\real$. Show that the following two claims are equivalent~\footnote{This result finds its application in non-convex quadratic constrained quadratic problems (QCQPs) \citep{beck2014introduction}.}:
\begin{itemize}
\item $f(\bx) \geq 0$ for all $\bx\in\real^n$.
\item $\scriptsize\begin{bmatrix}
\bA & -\bb\\
-\bb^\top & 2c
\end{bmatrix}\succeq \bzero$.
\end{itemize} 
\textit{Hint: Apply the eigenvalue characterization theorem on $\bA$.}
\end{exercise}







\section{Differentiability and Differential Calculus}\label{section:differ_calc}
\index{Continuously differentiability}
\index{Second-order partial derivative}
\begin{definition}[Directional Derivative, Partial Derivative]\label{definition:partial_deri}
Given a function $f$ defined over a set $\sS\subseteq \real^n$ and a nonzero vector $\bd\in\real^n$, the \textit{directional derivative} of $f$ at $\bx$ with respect to the direction $\bd$ is given by, if the limit exists, 
$$
\mathop{\lim}_{\mu\rightarrow 0}
\frac{f(\bx+\mu\bd) - f(\bx)}{\mu}.
$$
And it is denoted by $f^\prime(\bx; \bd)$ or $D_{\bd}f(\bx)$. 
The directional derivative is sometimes referred to as the  \textit{G\^ateaux derivative}.

For any $i\in\{1,2,\ldots,n\}$, the directional derivative at $\bx$ with respect to the direction of the $i$-th standard basis vector  $\be_i$ (if it exists) is called the $i$-th \textit{partial derivative} and is denoted by $\frac{\partial f}{\partial x_i} (\bx)$, $D_{\be_i}f(\bx)$, or $\partial_i f(\bx)$.
\end{definition}

\begin{figure*}[h]
\centering  
%	\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=9pt 
\subfigcapskip=-5pt 
\includegraphics[width=0.75\textwidth]{imgs/directional_derivative.pdf}
\caption{Plot for the function $f(x, y) = \sqrt{x^2+y^2}$, in which case any directional derivative for the direction $\bd=[a,b]^\top$ with $a\neq 0$ and $b\neq 0$ at point $[0,0]^\top$ exists. However, the partial derivatives at this point do not exist.}
\label{fig:direc_deri_no_partial}
\end{figure*}

It's important to note that even if a function can have a directional derivative in every direction $\bd$ at some points, its partial derivatives may not exist.
For example, given the direction $\bd=[a,b]^\top$ with $a\neq 0$ and $b\neq 0$, the directional derivation of the function $f(x,y)=\sqrt{x^2+y^2}$ (see Figure~\ref{fig:direc_deri_no_partial}) at $[0,0]^\top$ can be obtained by 
$$
f^\prime(0,0; \bd) = \mathop{\lim}_{\mu\rightarrow 0}
\frac{\sqrt{(0+\mu a)^2+(0+\mu b)^2} -\sqrt{0^2+0^2} }{\mu} = \sqrt{a^2+b^2}.
$$
When $\bd$ is a unit vector, the directional derivative is 1.
However, it can be shown that the partial derivatives are 
$$
\begin{aligned}
\frac{\partial f}{\partial x}(0,0) &= \mathop{\lim}_{h\rightarrow 0} \frac{\sqrt{(0+h)^2+0^2}}{h} = \mathop{\lim}_{h\rightarrow 0} \frac{\abs{h}}{h},\\
\frac{\partial f}{\partial y}(0,0) &= \mathop{\lim}_{h\rightarrow 0} \frac{\sqrt{0^2+(0+h)^2}}{h} = \mathop{\lim}_{h\rightarrow 0} \frac{\abs{h}}{h}.
\end{aligned}
$$
When $h>0$, the partial derivatives are 1; when $h<0$, the partial derivaties are $-1$. Therefore, the partial derivatives do not exist.





If all the partial derivatives of a function $f$ exist at a point $\bx\in\real^n$, then the \textit{gradient} of $f$ at $\bx$ is defined as the column vector containing all the partial derivatives:
$$
\nabla f(\bx)=
\begin{bmatrix}
\frac{\partial f}{\partial x_1} (\bx),
\frac{\partial f}{\partial x_2} (\bx),
\ldots,
\frac{\partial f}{\partial x_n} (\bx)	
\end{bmatrix}^\top
\in \real^n.
$$


\begin{exercise}
Show that a function can have partial derivatives but is not necessarily continuous (Definition~\ref{definition:conti_funs}). 
Additionally, demonstrate that a function can be continuous but does not necessarily have partial derivatives.
\end{exercise}



A function $f$ defined over an open set $\sS\subseteq \real^n$ (Definition~\ref{definition:open_close_sets}) is called \textit{differentiable}
if all its partial derivatives exist (i.e., the derivatives exist in univariate cases, and the gradients exist in multivariate cases). This is actually the definition of \textit{Fr\'echet differentiability}.


\index{(Fr\'echet) differentiability}
\begin{definition}[(Fr\'echet) Differentiability]
Given a function $f:\real^n\rightarrow \real$, the function $f$ is said to be differentiable at $\bx$ if there exists a vector $\bg\in\real^n$ such that 
$$
\mathop{\lim}_{\bd\rightarrow\bzero} \frac{f(\bx+\bd)-f(\bx)-\bg^\top\bd}{\normtwo{\bd}}=0.
$$
The unique vector $\bg$ is equal to the gradient $\nabla f(\bx)$.
\end{definition}

\index{Continuously differentiable}
Moreover, a function $f$ defined over an open set $\sS\subseteq \real^n$ is called \textit{continuously differentiable} over $\sS$ if all its partial derivatives exist and are also continuous on $\sS$.

A differentiable function may not be continuously differentiable. For example, consider  the following function $f$:
$$
f(x) = 
\begin{cases}
x^2 \sin(\frac{1}{x}), & \text{if } x \neq 0, \\
0, & \text{if } x = 0.
\end{cases}
$$
This function is differentiable everywhere, including at $x=0$. To see this, for $x \neq 0$, $f(x)$ is a product of two differentiable functions ($x^2$ and $\sin(1/x)$), hence it is differentiable.
At $x = 0$, we can calculate the derivative using the limit definition:
$$
f^\prime(0) = \mathop{\lim}_{\mu\rightarrow 0}
\frac{f(\mu) - f(0) }{\mu}
=\mathop{\lim}_{\mu\rightarrow 0} \frac{\mu^2 \sin(\frac{1}{\mu}) - 0 }{\mu}
=\mu \sin(\frac{1}{\mu}) 
$$
Since $\abs{\sin(1/\mu)}\leq 1$ for all $\mu \neq 0$, the limit exists and is equal to 0. Thus, $f(x)$ is differentiable at $x = 0$ with $f^\prime(0) = 0$.
However,  $f(x)$ is not continuously differentiable at $x = 0$. The derivative of $f(x)$ when $x\neq 0$ is 
$$
f^\prime(x) = 2x \sin(\frac{1}{x}) - \cos(\frac{1}{x}).
$$
The limit $\mathop{\lim}_{x\rightarrow 0}f^\prime(x)$ does not exist because the sine and cosine  functions oscillate as $x$ approaches 0. 



In the setting of \textit{differentiability}, the directional derivative and gradient have the following relationship:
\begin{equation}\label{equation:direc_contdiff}
f^\prime(\bx; \bd) = \nabla f(\bx)^\top \bd, \gap \text{for all }\bx\in\sS \text{ and }\bd\in\real^n.
\end{equation} 
\begin{proof}
The formula is obviously correct for $\bd = \bzero$. We then assume that $\bd \neq \bzero$. The differentiability of $f$ implies that
$$
0 = \lim_{\mu \to 0^+} \frac{f(\bx + \mu \bd) - f(\bx) - \langle \nabla f(\bx), \mu \bd \rangle}{\normtwo{\mu \bd}} 
= \lim_{\mu \to 0^+} \left[ \frac{f(\bx + \mu \bd) - f(\bx)}{\mu \normtwo{\bd}} - \frac{\langle \nabla f(\bx), \bd \rangle}{\normtwo{\bd}} \right].
$$
Therefore,
$$
\begin{aligned}
&f'(\bx; \bd) = \lim_{\mu \to 0^+} \frac{f(\bx + \mu \bd) - f(\bx)}{\mu}\\
&= \lim_{\mu \to 0^+} \left\{ \normtwo{\bd} \left[ \frac{f(\bx + \mu \bd) - f(\bx)}{\mu \normtwo{\bd}} - \frac{\langle \nabla f(\bx), \bd \rangle}{\normtwo{\bd}} \right] + \langle \nabla f(\bx), \bd \rangle \right\}
&= \langle \nabla f(\bx), \bd \rangle.
\end{aligned}
$$
This proves \eqref{equation:direc_contdiff}.
\end{proof}

Recalling the definition of differentiability, we also have:
\begin{equation}
\mathop{\lim}_{\bd\rightarrow \bzero}
\frac{f(\bx+\bd) - f(\bx) - \nabla f(\bx)^\top \bd}{\normtwo{\bd}} = 0\gap 
\text{for all }\bx\in\sS,
\end{equation}
or 
\begin{equation}
f(\by) = f(\bx)+\nabla f(\bx)^\top (\by-\bx) + o(\normtwo{\by-\bx}),
\end{equation}
where the \textit{small-oh} function $o(\cdot): \real_+\rightarrow \real$ is a one-dimensional function satisfying $\frac{o(\mu)}{\mu}\rightarrow 0$ as $\mu\rightarrow 0^+$.~\footnote{Note that we also use the standard \textit{big-Oh} notation to describe the asymptotic behavior of functions.
Specifically, the notation $g(\bd) = \mathcalO(\normtwo{\bd}^p)$ means that there are positive numbers
$C_1$ and $\delta$ such that $\abs{g(\bd)} \leq  C_1 \normtwo{\bd}^p$ for all $\normtwo{\bd}\leq\delta$. In practice it is
often equivalent to $\abs{g(\bd)} \approx C_2\normtwo{\bd}^p$ for  sufficiently small $\bd$, where $C_2$ is  another positive constant.
The \textit{soft-Oh} notation is employed to hide poly-logarithmic factors i.e., $f = \widetilde{\mathcalO}(g)$ will
imply $f = \mathcalO(g \log^c(g))$ for some absolute constant $c$.}
Therefore, any differentiable function ensures that $\mathop{\lim}_{\bx\rightarrow \ba} f(\bx) = f(\ba)$. Hence, any differentiable function is continuous. 

On the other hand, in the setting of differentiability, although the partial derivatives may not be continuous, the partial derivatives exist for all differentiable points.

The partial derivative $\frac{\partial f}{\partial x_i} (\bx)$ is also a real-valued function of $\bx\in\sS$ that can be partially differentiated. The $j$-th partial derivative of $\frac{\partial f}{\partial x_i} (\bx)$ is defined as 
$$
\frac{\partial^2 f}{\partial x_j\partial x_i} (\bx)=
\frac{\partial \left(\frac{\partial f}{\partial x_i} (\bx)\right)}{\partial x_j} (\bx).
$$
This is called the ($j,i$)-th \textit{second-order partial derivative} of function $f$.
A function $f$ defined over an open set $\sS\subseteq$ is called \textit{twice continuously differentiable} over $\sS$ if all the second-order partial derivatives exist and are continuous over $\sS$. In the setting of twice continuously differentiability, the second-order partial derivative are symmetric:
$$
\frac{\partial^2 f}{\partial x_j\partial x_i} (\bx)=
\frac{\partial^2 f}{\partial x_i\partial x_j} (\bx).
$$
The \textit{Hessian} of the function $f$ at a point $\bx\in\sS$ is defined as the symmetric $n\times n$ matrix 
$$
\nabla^2f(\bx)=
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} (\bx) & 
\frac{\partial^2 f}{\partial x_1\partial x_2} (\bx) & \ldots &
\frac{\partial^2 f}{\partial x_1\partial x_n} (\bx)\\
\frac{\partial^2 f}{\partial x_2\partial x_1} (\bx) & 
\frac{\partial^2 f}{\partial x_2\partial x_2} (\bx) & \ldots &
\frac{\partial^2 f}{\partial x_2\partial x_n} (\bx)\\
\vdots & 
\vdots & \ddots &
\vdots\\
\frac{\partial^2 f}{\partial x_n\partial x_1} (\bx) & 
\frac{\partial^2 f}{\partial x_n\partial x_2} (\bx) & \ldots &
\frac{\partial^2 f}{\partial x_n^2} (\bx)
\end{bmatrix}.
$$

We then provide a simple proof of Taylor's expansion   for one-dimensional functions. 
\begin{theorem}[Taylor's Expansion with Lagrange Remainder]
Let $f(x): \real\rightarrow \real$ be $k$-times continuously differentiable on the closed interval $I$ with endpoints $x$ and $y$, for some $k\geq 0$. If $f^{(k+1)}$ exists on the interval $I$, then there exists a $\xi \in (x,y)$ such that 
$$
\begin{aligned}
f(x)& = f(y) + f^\prime(y)(x-y) +\ldots + \frac{f^{(k)}(y)}{k!}(x-y)^k
+ \frac{f^{(k+1)}(\xi)}{(k+1)!}(x-y)^{k+1}\\
&=\sum_{i=0}^{k} \frac{f^{(i)}(y)}{i!} (x-y)^i + \frac{f^{(k+1)}(\xi)}{(k+1)!}(x-y)^{k+1}.
\end{aligned}
$$ 
Taylor's expansion can be extended to a function of vector $f(\bx):\real^n\rightarrow \real$ or a function of matrix $f(\bX): \real^{m\times n}\rightarrow \real$.
\end{theorem}
Taylor's expansion, or also known as  \textit{Taylor's series}, approximates the function $f(x)$ around a value  $y$ using a polynomial in a single variable $x$. To understand the origin of this series, we recall from the elementary calculus course that the approximated function  of $\cos (\theta)$ around $\theta=0$ is given by 
$
\cos (\theta) \approx 1-\frac{\theta^2}{2}.
$
This means that $\cos(\theta)$ can be approximated by a second-degree polynomial. 
If we want to approximate $\cos(\theta)$ more generally with a second-degree polynomial $ f(\theta) = c_1+c_2 \theta+ c_3 \theta^2$, an intuitive approach is to match the function and its derivatives at $\theta=0$. That is,
$$\left\{
\begin{aligned}
\cos(0) &= f(0); \\
\cos^\prime(0) &= f^\prime(0);\\
\cos^{\prime\prime}(0) &= f^{\prime\prime}(0);\\
\end{aligned}
\right.
\quad\implies\quad 
\left\{
\begin{aligned}
1 &= c_1; \\
-\sin(0) &=0= c_2;\\
-\cos(0) &=-1= 2c_3.\\
\end{aligned}
\right.
$$
Solving these equations yields $f(\theta) = c_1+c_2 \theta+ c_3 \theta^2 = 1-\frac{\theta^2}{2}$, which matches our initial approximation $\cos (\theta) \approx 1-\frac{\theta^2}{2}$ around  $\theta=0$. 

For high-dimensional functions, we have the following approximation results.
\begin{theorem}[Mean Value Theorem]\label{theorem:mean_approx}
Let $f(\bx):\sS\rightarrow \real$ be a  continuously differentiable function over an open set $\sS\subseteq\real^n$, and given two points $\bx, \by\in\sS$. Then, there exists a point $\bxi\in[\bx,\by]$ such that 
$$
f(\by) = f(\bx)+ \nabla f(\bxi)^\top (\by-\bx).
$$

\end{theorem}


\begin{theorem}[Linear Approximation Theorem]\label{theorem:linear_approx}
Let $f(\bx):\sS\rightarrow \real$ be a twice continuously differentiable function over an open set $\sS\subseteq\real^n$, and let $\bx, \by\in\sS$. Then, there exists a point $\bxi\in[\bx,\by]$ such that 
$$
f(\by) = f(\bx)+ \nabla f(\bx)^\top (\by-\bx) + \frac{1}{2} (\by-\bx)^\top \nabla^2 f(\bxi) (\by-\bx),
$$ 
or 
$$
f(\by) = f(\bx)+\nabla f(\bx)^\top (\by-\bx) + o(\normtwo{\by-\bx}),
$$
or
$$
f(\by) = f(\bx)+\nabla f(\bx)^\top (\by-\bx) + \mathcalO(\normtwo{\by-\bx}^2).
$$
\end{theorem}

\begin{theorem}[Quadratic Approximation Theorem]\label{theorem:quad_app_theo}
Let $f(\bx):\sS\rightarrow \real$ be a twice continuously differentiable function over an open set $\sS\subseteq\real^n$, and let $\bx, \by\in\sS$. Then it follows that 
$$
f(\by) = f(\bx)+ \nabla f(\bx)^\top (\by-\bx) + \frac{1}{2} (\by-\bx)^\top \nabla^2 f(\bx) (\by-\bx)
+
o(\normtwo{\by-\bx}^2),
$$
or 
$$
f(\by) = f(\bx)+ \nabla f(\bx)^\top (\by-\bx) + \frac{1}{2} (\by-\bx)^\top \nabla^2 f(\bx) (\by-\bx)
+
\mathcalO(\normtwo{\by-\bx}^3).
$$
\end{theorem}

\section{Fundamental Theorems}
This section introduces fundamental theorems from optimization, linear algebra, and calculus that are frequently utilized in subsequent discussions.
\begin{theorem}[Fundamental Theorem of Optimization]\label{theorem:funda_opt}
Let $\ba,\bb\in\real^n$ be two vectors. The inner product of these two vectors can be expressed as a sum of norms:
$$
\ba^\top\bb = \frac{1}{2} \left( \normtwo{\ba}^2 + \normtwo{\bb}^2 - \normtwo{\ba-\bb}^2 \right).
$$
\end{theorem}

The following theorem provides an elementary proof that the row rank and column rank of any matrix $\bA\in \real^{m\times n}$ are equal, which also highlights the fundamental theorem of linear algebra. \index{Matrix rank}
\begin{theorem}[Row Rank Equals Column Rank\index{Matrix rank}]\label{theorem:equal-dimension-rank}
The dimension of the column space of a matrix $\bA\in \real^{m\times n}$ is equal to the dimension of its row space. 
In other words, the row rank and the column rank of a matrix $\bA$ are identical.
\end{theorem}

\begin{proof}[of Theorem~\ref{theorem:equal-dimension-rank}]
Firstly, observe that the null space of $\bA$ is orthogonal complementary to the row space of $\bA$: $\nspace(\bA) \bot \cspace(\bA^\top)$ (where the row space of $\bA$ corresponds to the column space of $\bA^\top$). 
This means that any vector in the null space of $\bA$ is orthogonal to every vector in the row space of $\bA$. 
To illustrate this, assume $\bA$ has rows $\ba_1^\top, \ba_2^\top, \ldots, \ba_m^\top$ and let $\bA=[\ba_1^\top; \ba_2^\top; \ldots; \ba_m^\top]$. For any vector $\bx\in \nspace(\bA)$, we have $\bA\bx = \bzero$, implying $[\ba_1^\top\bx; \ba_2^\top\bx; \ldots; \ba_m^\top\bx]=\bzero$. And since the row space of $\bA$ is spanned by $\ba_1^\top, \ba_2^\top, \ldots, \ba_m^\top$, it follows that $\bx$ is perpendicular to all vectors in $\cspace(\bA^\top)$, confirming $\nspace(\bA) \bot \cspace(\bA^\top)$.

Next, suppose the dimension of the row space of $\bA$ is $r$. \textcolor{mylightbluetext}{Let $\br_1, \br_2, \ldots, \br_r\in\real^n$ be a basis for the row space}. 
Consequently, the $r$ vectors $\bA\br_1, \bA\br_2, \ldots, \bA\br_r$ are in the column space of $\bA$ and are linearly independent. To see this, suppose we have a linear combination of the $r$ vectors: $x_1\bA\br_1 + x_2\bA\br_2+ \ldots+ x_r\bA\br_r=\bzero$, that is, $\bA(x_1\br_1 + x_2\br_2+ \ldots+ x_r\br_r)=\bzero$, and the vector $\bv\triangleq x_1\br_1 + x_2\br_2+ \ldots+ x_r\br_r$ is in null space of $\bA$. But since $\{\br_1, \br_2, \ldots, \br_r\}$ is a basis for the row space of $\bA$, $\bv$ must also lie in the row space of $\bA$. We have shown that vectors from the null space of $\bA$ are perpendicular to vectors from the row space of $\bA$; thus, it holds that $\bv^\top\bv=0$ and $x_1=x_2=\ldots=x_r=0$. Therefore, \textcolor{mylightbluetext}{$\bA\br_1, \bA\br_2, \ldots, \bA\br_r$ are in the column space of $\bA$, and they are linearly independent}. This implies that the dimension of the column space of $\bA$ is larger than $r$. Therefore, \textbf{the row rank of $\bA\leq $ column rank of $\bA$}. 

By applying the same argument to $\bA^\top$, we deduce that \textbf{column rank of $\bA\leq $ row rank of $\bA$}, completing the proof that the row and column ranks of $\bA$ are equal.
\end{proof}

From this proof, it follows that if $\{\br_1, \br_2, \ldots, \br_r\}$ is a set of vectors in $\real^n$ forming a basis for the row space, then \textcolor{black}{$\{\bA\br_1, \bA\br_2, \ldots, \bA\br_r\}$ constitutes a basis for the column space of $\bA$}. 
This observation is formalized  in the following lemma.



\begin{lemma}[Column Basis from Row Basis]\label{lemma:column-basis-from-row-basis}
Let $\bA\in \real^{m\times n}$ be a matrix, and suppose that $\{\br_1, \br_2, \ldots, \br_r\}$ is a set of vectors in $\real^n$, which forms a basis for the row space. Then, $\{\bA\br_1, \bA\br_2, \ldots, \bA\br_r\}$ constitutes a basis for the column space of $\bA$.
\end{lemma}




\index{Fundamental theorem}
%\subsection{The Fundamental Theorem of Linear Algebra}\label{appendix:fundamental-rank-nullity}
For any matrix $\bA\in \real^{m\times n}$, it is straightforward to verify that any vector in the row space of $\bA$ is orthogonal to any vector in the null space of $\bA$. Suppose $\bx_n \in \nspace(\bA)$, then $\bA\bx_n = \bzero$, meaning that $\bx_n$ is orthogonal to every row of $\bA$, thereby confirming this property.

Similarly, we can also demonstrate that any vector in the column space of $\bA$ is perpendicular to any vector in the null space of $\bA^\top$. 
Furthermore, the column space of $\bA$ together with the null space of $\bA^\top$ span the entire space $\real^m$, which is a key aspect of the fundamental theorem of linear algebra.

The fundamental theorem consists of two aspects: the dimensions of the subspaces and their orthogonality. 
The orthogonality can be easily verified as shown above. 
Additionally, when the row space has dimension $r$, the null space has dimension $n-r$. 
This is not immediately obvious and is proven in the following theorem.

\begin{figure}[h!]
\centering
\includegraphics[width=0.98\textwidth]{imgs/lafundamental.pdf}
\caption{Two pairs of orthogonal subspaces in $\real^n$ and $\real^m$. $\dim(\cspace(\bA^\top)) + \dim(\nspace(\bA))=n$ and $\dim(\nspace(\bA^\top)) + \dim(\cspace(\bA))=m$. The null space component maps to zero as $\bA\bx_n = \bzero \in \real^m$. The row space component maps to the column space as $\bA\bx_r = \bA(\bx_r+\bx_n)=\bb \in \cspace(\bA)$.}
\label{fig:lafundamental}
\end{figure}
\begin{theorem}[Fundamental Theorem of Linear Algebra]\label{theorem:fundamental-linear-algebra}
Orthogonal Complement and Rank-Nullity Theorem: Let $\bA\in \real^{m\times n}$ be a matrix. Then, 
\begin{itemize}
\item $\nspace(\bA)$ is the orthogonal complement to the row space $\cspace(\bA^\top)$ in $\real^n$: $\dim(\nspace(\bA))+\dim(\cspace(\bA^\top))=n$;
\item $\nspace(\bA^\top)$ is the orthogonal complement to the column space $\cspace(\bA)$ in $\real^m$: $\dim(\nspace(\bA^\top))+\dim(\cspace(\bA))=m$;
\item If $\bA$ has rank $r$, then $\dim(\cspace(\bA^\top)) = \dim(\cspace(\bA)) = r$, implying that $\dim(\nspace(\bA)) = n-r$ and $\dim(\nspace(\bA^\top))=m-r$.
\end{itemize}
\end{theorem}

\begin{proof}[of Theorem~\ref{theorem:fundamental-linear-algebra}]
Building upon the proof of Theorem~\ref{theorem:equal-dimension-rank}, consider a set of vectors $\br_1, \br_2, \ldots, \br_r$  in $\real^n$ forming a basis for the row space.
Consequently, \textcolor{black}{$\{\bA\br_1, \bA\br_2, \ldots, \bA\br_r\}$ constitutes a basis for the column space of $\bA$}. 
Let $\bn_1, \bn_2, \ldots, \bn_k \in \real^n$ form a basis for the null space of $\bA$. Following again from the proof of Theorem~\ref{theorem:equal-dimension-rank}, it follows that $\nspace(\bA) \bot \cspace(\bA^\top)$,  indicating the orthogonality between $\br_1, \br_2, \ldots, \br_r$ and $\bn_1, \bn_2, \ldots, \bn_k$. Consequently, the set   $\{\br_1, \br_2, \ldots, \br_r, \bn_1, \bn_2, \ldots, \bn_k\}$ is linearly independent in $\real^n$.

For any vector $\bx\in \real^n $, $\bA\bx$ is in the column space of $\bA$. Then it can be expressed as a linear combination of $\bA\br_1, \bA\br_2, \ldots, \bA\br_r$: $\bA\bx \triangleq \sum_{i=1}^{r}a_i\bA\br_i$, which states that $\bA(\bx-\sum_{i=1}^{r}a_i\br_i) = \bzero$ and $\bx-\sum_{i=1}^{r}a_i\br_i$ is thus in $\nspace(\bA)$. Since $\{\bn_1, \bn_2, \ldots, \bn_k\}$ is a basis for the null space of $\bA$, $\bx-\sum_{i=1}^{r}a_i\br_i$ can be expressed as a linear combination of $\bn_1, \bn_2, \ldots, \bn_k$: $\bx-\sum_{i=1}^{r}a_i\br_i = \sum_{j=1}^{k}b_j \bn_j$, i.e., $\bx=\sum_{i=1}^{r}a_i\br_i + \sum_{j=1}^{k}b_j \bn_j$. That is, any vector $\bx\in \real^n$ can be expressed as $\{\br_1, \br_2, \ldots, \br_r, \bn_1, \bn_2, \ldots, \bn_k\}$ and the set forms a basis for $\real^n$. Thus, the dimensions satisfy: $r+k=n$, i.e., $\dim(\nspace(\bA))+\dim(\cspace(\bA^\top))=n$. Similarly, we can prove that $\dim(\nspace(\bA^\top))+\dim(\cspace(\bA))=m$.
\end{proof}

Figure~\ref{fig:lafundamental} visually represents the orthogonality of these subspaces and illustrates how $\bA$ maps a vector $\bx$ into the column space. The row space and null space dimensions add up to $n$, while the dimensions of the column space of $\bA$ and the null space of $\bA^\top$ sum to $m$. The null space component maps to zero as $\bA\bx_{\bn} = \bzero \in \real^m$, which is the intersection of the column space of $\bA$ and the null space of $\bA^\top$. 
Conversely, the row space component transforms into the column space as $\bA\bx_{\br} = \bA(\bx_{\br} + \bx_{\bn})=\bb\in \real^m$. 



We provide the fundamental theorem of calculus below, which  plays a pivotal role in linking differential and integral calculus, offering profound insights into the behavior of functions. 
\begin{theorem}[Fundamental Theorem of Calculus]\label{theorem:fund_theo_calculu}
Let $f(\cdot):\real^n\rightarrow \real$. The fundamental theorem of calculus describes the difference between function values:
\begin{subequations}
\begin{equation}\label{equation:fund_theo_calculu3}
f(\by) - f(\bx) = \int_{0}^{1} \langle \nabla f(\bx+\mu(\by-\bx)), \by-\bx  \rangle d\mu, 
\end{equation}
or the difference between gradients:
\begin{align}
\nabla f(\by) -\nabla f(\bx) &= \left( \int_{0}^{1} \nabla^2f(\bx+\mu(\by-\bx))d\mu \right) \cdot (\by-\bx); \label{equation:fund_theo_calculu1}\\
\nabla f(\bx+\alpha\bd) - \nabla f(\bx) &= \int_{0}^{\alpha} \nabla^2 f(\bx+\mu\bd)\bd d\mu.
\end{align}
\end{subequations}
where $ \nabla f(\by) $ denotes the gradient of $ f $ evaluated at $ \by $ (see Section~\ref{section:differ_calc}), and $ \innerproduct{\cdot, \cdot} $ represents the inner product.
Furthermore, use directional derivative for twice continuously differentiable functions, we also have
\begin{equation}
f(\by) = f(\bx) + \innerproduct{\nabla f(\bx),  (\by-\bx)} + \int_{0}^{1} (1-\mu) \frac{\partial^2 f(\bx+\mu(\by-\bx))}{\partial \mu^2} d\mu.
\end{equation}
\end{theorem}
The first equality \eqref{equation:fund_theo_calculu3} can be derived by letting $g(t)\triangleq f(\bx+t(\by-\bx))$. Then we can obtain $f(\by)$ as 
$
f(\by) = g(1) = g(0) + \int_{0}^{1} g'(t)dt.
$
The other equalities  can be demonstrated similarly. 

\index{Implicit function theorem}
We present the implicit function theorem without a proof. Further  details can be found in \citet{krantz2002implicit}.
\begin{theorem}[Implicit Function Theorem]\label{theorem:implic_func_theorem}
Let $ \bh(\bx)=[h_i(\bx)]_{i\in\{1,2,\ldots,p\}} : \mathbb{R}^n \rightarrow \mathbb{R}^p $ ($p<n$) and $ \widetilde{\bx} = [\widetilde{y}_1, \ldots, \widetilde{y}_p, \widetilde{z}_1, \ldots, \widetilde{z}_{n-p}] = [\widetilde{\by}, \widetilde{\bz}] \in\real^n $ satisfy:
\begin{enumerate}[(i)]
\item $ \bh(\widetilde{\bx}) = \bzero $.
\item $ \bh(\bx) $ is continuously differentiable in a neighborhood of $ \widetilde{\bx} $.
\item The $ p \times p $ Jacobian matrix
$$
\small
\begin{bmatrix}
\frac{\partial h_1(\widetilde{\bx})}{\partial y_1} & \cdots & \frac{\partial h_1(\widetilde{\bx})}{\partial y_p} \\
\vdots & \ddots & \vdots \\
\frac{\partial h_p(\widetilde{\bx})}{\partial y_1} & \cdots & \frac{\partial h_p(\widetilde{\bx})}{\partial y_p}
\end{bmatrix}
$$
is nonsingular.
\end{enumerate}
Then, there exists $ \varepsilon > 0 $ along with functions $ \bs(\bz) = [s_1(\bz), \ldots, s_p(\bz)]^\top\in\real^p $ such that for all $ \bz \in \sB(\widetilde{\bz}, \varepsilon) $, $ \bh(\bs(\bz), \bz) = \bzero $. Moreover, each $ s_k(\bz) $ is continuously differentiable for $ k \in \{1,2, \ldots, p\} $. Furthermore, for all $ i \in \{1,2, \ldots, p\} $ and $ j \in \{1,2, \ldots, n-p\} $, we have:
$$
\sum_{k=1}^p \frac{\partial h_i(\by, \bz)}{\partial y_k} \frac{\partial s_k(\bz)}{\partial z_j} + \frac{\partial h_i(\by, \bz)}{\partial z_j} = 0.
$$
\end{theorem}

\index{Inverse function theorem}
\begin{theorem}[Inverse Function Theorem]
Let $ F: \mathbb{R}^n \to \mathbb{R}^n $ be a continuously differentiable function, and let $ \bx^* $ be a point in $ \mathbb{R}^n $. If the derivative of $ F $ at $ \bx^* $, denoted as $ \nabla F(\bx^*) $, is an invertible linear transformation (meaning the Jacobian matrix $ \bJ_F(\bx^*) $ is nonsingular), then there exists an open neighborhood $ \sS $ of $ \bx^* $ such that:
\begin{enumerate}[(i)]
\item $ F $ maps $ \sS $ bijectively onto an open set $ \sV = F(\sS) $,
\item The inverse function $ F^{-1}: \sV \to \sS $ is also continuously differentiable,
\item For each $ \by \in \sV $, the derivative of the inverse function at $ \by $ is the inverse of the derivative of $ F $ at $ \bx $, where $ F(\bx) = \by $; i.e.,
$$
\nabla (F^{-1})(\by) = [\nabla F(\bx)]^{-1}.
$$
\end{enumerate}
\end{theorem}
In simpler terms, if a function locally behaves like a linear transformation (i.e., its derivative is nonzero and invertible), then near that point, an inverse function exists that ``undoes" the original function. This inverse function is also smooth (continuously differentiable).
The inverse function theorem is a powerful tool that has wide-ranging applications in mathematics, physics, economics, and engineering. For example, the theorem can be used to solve systems of nonlinear equations by transforming them into a system of linear equations.




\begin{theorem}[Von-Neumann Lemma]\label{theorem:von_neumannlem}
Let $\norm{\cdot}$ be a submultiplicative matrix norm with $\norm{\bI} = 1$, and let $\bE \in \real^{n \times n}$. If $\norm{\bE} < 1$, then $\bI - \bE$ is nonsingular, and
\begin{equation}\label{equation:von_neumannlem1}
(\bI - \bE)^{-1} = \sum_{t=0}^{\infty} \bE^t,
\end{equation}
\begin{equation}\label{equation:von_neumannlem2}
\norm{(\bI - \bE)^{-1}} \leq \frac{1}{1 - \norm{\bE}}.
\end{equation}
If $\bA \in \real^{n \times n}$ is nonsingular and $\norm{\bA^{-1}(\bB - \bA)} < 1$, then $\bB$ is nonsingular and satisfies
\begin{equation}\label{equation:von_neumannlem3}
\bB^{-1} = \sum_{t=0}^{\infty} (\bI - \bA^{-1}\bB)^t \bA^{-1},
\end{equation}
and
\begin{equation}\label{equation:von_neumannlem4}
\norm{\bB^{-1}} \leq \frac{\norm{\bA^{-1}}}{1 - \norm{\bA^{-1}(\bB - \bA)}}.
\end{equation}
\end{theorem}
\begin{proof}[of Theorem~\ref{theorem:von_neumannlem}]
Since $\norm{\bE} < 1$, then
$ \bS_t \triangleq \bI + \bE + \bE^2 + \cdots + \bE^t $
defines a Cauchy sequence and thus $\bS_t$ converges. 
Taking the limit, we obtain
$ \sum_{t=0}^{\infty} \bE^t = \lim_{t \to \infty} \bS_t = (\bI - \bE)^{-1} $
which proves \eqref{equation:von_neumannlem1} and \eqref{equation:von_neumannlem2}.
For the second part, since $\bA$ is nonsingular and $\norm{\bA^{-1}(\bB - \bA)} = \norm{-(\bI - \bA^{-1}\bB)} < 1$, we set $\bE \triangleq \bI - \bA^{-1}\bB$ and apply \eqref{equation:von_neumannlem1} and \eqref{equation:von_neumannlem2}, immediately yielding \eqref{equation:von_neumannlem3} and \eqref{equation:von_neumannlem4}. 
\end{proof}


\begin{theorem}[Inverse Function Theorem: a Variant]\label{theorem:invfunctheo_var}
Let $ F : \sS \subset \mathbb{R}^n \to \mathbb{R}^n $ be continuously differentiable in a neighborhood of $ \bx^* \in \sS $, and suppose that $ \nabla F(\bx^*) $ is nonsingular. Then there exist $ \delta > 0 $, $ \xi > 0 $, and $ \epsilon > 0 $, such that when $ \normtwo{\by - \bx^*} < \delta $ and $ \by \in \sS $, $ \nabla F(\by) $ is nonsingular and
\begin{equation}\label{equation:invfunctheo_var1}
\normtwo{\nabla F(\by)^{-1}} \leq \xi,
\end{equation}
where $\normtwo{\cdot}$ for a matrix denotes the spectral norm.
Additionally, the inverse $ \nabla F(\by)^{-1} $ is continuous at $ \bx^* $, meaning
$$
\normtwo{\nabla F(\by)^{-1} - \nabla F(\bx^*)^{-1}} < \epsilon.
$$
\end{theorem}
\begin{proof}
Define $\alpha \triangleq \normtwo{\nabla F(\bx^*)^{-1}}$. Since $F$ is continuously differentiable, for a given $\beta < \alpha^{-1}$, choose $\delta$ such that when $\normtwo{\by - \bx^*} < \delta$ with $\by \in \sS$,
$$
\normtwo{\nabla F(\bx^*) - \nabla F(\by)} \leq \beta.
$$
It follows from Von-Neumann lemma (Theorem~\ref{theorem:von_neumannlem}) that $\nabla F(\by)$ is invertible, and \eqref{equation:von_neumannlem4} shows that 
$$
\normtwo{\nabla F(\by)^{-1}} \leq  \frac{\normtwo{\nabla F(\bx^*)^{-1}}}{1 - \normtwo{\nabla F(\bx^*)^{-1}(\nabla F(\by) - \nabla F(\bx^*))}}
\leq \alpha / (1 - \beta \alpha).
$$
Therefore, \eqref{equation:invfunctheo_var1} holds with $\xi \triangleq \alpha / (1 - \beta \alpha)$. Thus,
$$
\begin{aligned}
\normtwo{\nabla F(\bx^*)^{-1} - \nabla F(\by)^{-1}}
&= \normtwo{\nabla F(\bx^*)^{-1}(\nabla F(\by) - \nabla F(\bx^*))\nabla F(\by)^{-1}}
\leq \alpha \beta \xi 
\triangleq \epsilon,
\end{aligned}
$$
which shows that the continuity of $\nabla F$ guarantees the continuity of $(\nabla F)^{-1}$.
\end{proof}


\section{Matrix Decomposition}
This section provide several matrix decomposition approaches, which can be useful for the proofs in the optimization methods.
\subsection{Cholesky Decomposition}\label{section:choleskydecomp}
Positive definiteness or positive semidefiniteness is one of the most desirable properties a matrix can have. In this section, we introduce decomposition techniques for positive definite matrices, with a focus on the well-known \textit{Cholesky decomposition}.
The Cholesky decomposition is named after a French military officer and mathematician, Andr\'{e}-Louis Cholesky (1875{\textendash}1918), who developed this method in his surveying work. It is primarily used to solve linear systems involving positive definite matrices.

Here, we establish the existence of the Cholesky decomposition using an inductive approach. Alternative proofs also exist, such as those derived from the LU decomposition  \citep{lu2022matrix}.

\index{Cholesky decomposition}
\begin{theorem}[Cholesky Decomposition]\label{theorem:cholesky-factor-exist}
Every positive definite (PD) matrix $\bA\in \real^{n\times n}$ can be factored as 
$$
\bA = \bR^\top\bR,
$$
where $\bR \in \real^{n\times n}$ is an upper triangular matrix \textbf{with positive diagonal elements}. This decomposition is called the \textit{Cholesky decomposition}  of $\bA$, and $\bR$ is known as the \textit{Cholesky factor} or \textit{Cholesky triangle} of $\bA$.
Specifically, the Cholesky decomposition is unique (Corollary~\ref{corollary:unique-cholesky-main}).
\end{theorem}
Alternatively, $\bA$ can be factored as $\bA=\bL\bL^\top$, where $\bL=\bR^\top$ is a lower triangular matrix \textit{with positive diagonals}.
\begin{proof}[of Theorem~\ref{theorem:cholesky-factor-exist}]
We will prove by induction that every $n\times n$ positive definite matrix $\bA$ has a decomposition $\bA=\bR^\top\bR$. The $1\times 1$ case is trivial by setting $R\triangleq\sqrt{A}$, so that $A=R^2$. 

Suppose any $k\times k$ PD matrix $\bA_k$ has a Cholesky decomposition. We must show that any $(k+1)\times(k+1)$ PD matrix $\bA_{k+1}$ can also be factored as this Cholesky decomposition, then we complete the proof.

For any $(k+1)\times(k+1)$ PD matrix $\bA_{k+1}$, write $\bA_{k+1}$ as
$
\bA_{k+1} \triangleq \footnotesize\begin{bmatrix}
\bA_k & \bb \\
\bb^\top & d
\end{bmatrix}.
$
Since $\bA_k$ is PD, by the inductive hypothesis, it admits a Cholesky decomposition  $\bA_k = \bR_k^\top\bR_k$. Define the upper triangular matrix
$
\bR_{k+1}\triangleq\footnotesize\begin{bmatrix}
\bR_k & \br\\
0 & s
\end{bmatrix}.
$
Then,
$$
\bR_{k+1}^\top\bR_{k+1} = 
\begin{bmatrix}
\bR_k^\top\bR_k & \bR_k^\top \br\\
\br^\top \bR_k & \br^\top\br+s^2
\end{bmatrix}.
$$
Therefore, if we can prove $\bR_{k+1}^\top \bR_{k+1} = \bA_{k+1}$ is the Cholesky decomposition of $\bA_{k+1}$ (which requires the value $s$ to be positive), then we complete the proof. That is, we need to prove
$$
\begin{aligned}
\bb &= \bR_k^\top \br, \\
d &= \br^\top\br+s^2.
\end{aligned}
$$
Since $\bR_k$ is nonsingular, we can solve uniquely for $\br$ and $s$: 
$$
\begin{aligned}
\br &= \bR_k^{-\top}\bb, \\
s &= \sqrt{d - \br^\top\br} = \sqrt{d - \bb^\top\bA_k^{-1}\bb},
\end{aligned}
$$
where we assume $s$ is nonnegative. However, we need to further prove that $s$ is not only nonnegative, but also positive. Since $\bA_k$ is PD, from Sylvester's criterion (see \citet{lu2022matrix}), and the fact that if matrix $\bM$ has a block formulation: $\bM=\footnotesize\begin{bmatrix}
\bA & \bB \\
\bC & \bD 
\end{bmatrix}$, then $\det(\bM) = \det(\bA)\det(\bD-\bC\bA^{-1}\bB)$, we have
$$
\det(\bA_{k+1}) = \det(\bA_k)\det(d- \bb^\top\bA_k^{-1}\bb) =  \det(\bA_k)(d- \bb^\top\bA_k^{-1}\bb)>0.
$$
Because $ \det(\bA_k)>0$, we then obtain that $(d- \bb^\top\bA_k^{-1}\bb)>0$, and this implies $s>0$.
This completes the proof.
\end{proof}

\index{Uniqueness}
\begin{corollary}[Uniqueness of Cholesky Decomposition\index{Uniqueness}]\label{corollary:unique-cholesky-main}
The Cholesky decomposition $\bA=\bR^\top\bR$ for any positive definite matrix $\bA\in \real^{n\times n}$ is unique.
\end{corollary}
\begin{proof}[of Corollary~\ref{corollary:unique-cholesky-main}]
Suppose the Cholesky decomposition is not unique. Then, there exist two distinct decompositions such that $\bA=\bR_1^\top\bR_1 = \bR_2^\top\bR_2$.  Rearranging, we obtain
$$
\bR_1\bR_2^{-1}= \bR_1^{-\top} \bR_2^\top.
$$
Since the inverse of an upper triangular matrix is also upper triangular, and the product of two upper triangular matrices is upper triangular, \footnote{Similarly, the inverse of a lower triangular matrix is lower triangular, and the product of two lower triangular matrices is also lower triangular.} we conclude that the left-hand side of the above equation is an upper triangular matrix, while the right-hand side is a lower triangular matrix. Consequently, $\bR_1\bR_2^{-1}= \bR_1^{-\top} \bR_2^\top$ must be a diagonal matrix, and $\bR_1^{-\top} \bR_2^\top= (\bR_1^{-\top} \bR_2^\top)^\top = \bR_2\bR_1^{-1}$.
Let $\bLambda \triangleq \bR_1\bR_2^{-1}= \bR_2\bR_1^{-1}$ be the diagonal matrix. We notice that the diagonal value of $\bLambda$ is the product of the corresponding diagonal values of $\bR_1$ and $\bR_2^{-1}$ (or $\bR_2$ and $\bR_1^{-1}$). Explicitly, writing the matrices as
$$
\bR_1=
\small
\begin{bmatrix}
r_{11} & r_{12} & \ldots & r_{1n} \\
0 & r_{22} & \ldots & r_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \ldots & r_{nn}
\end{bmatrix},
\qquad 
\normalsize
\bR_2=
\small
\begin{bmatrix}
s_{11} & s_{12} & \ldots & s_{1n} \\
0 & s_{22} & \ldots & s_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \ldots & s_{nn}
\end{bmatrix},
$$
we find that
$$
\begin{aligned}
\bR_1\bR_2^{-1}=
\small
\begin{bmatrix}
\frac{r_{11}}{s_{11}} & 0 & \ldots & 0 \\
0 & \frac{r_{22}}{s_{22}} & \ldots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \ldots & \frac{r_{nn}}{s_{nn}}
\end{bmatrix}
=
\small
\begin{bmatrix}
\frac{s_{11}}{r_{11}} & 0 & \ldots & 0 \\
0 & \frac{s_{22}}{r_{22}} & \ldots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \ldots & \frac{s_{nn}}{r_{nn}}
\end{bmatrix}
\normalsize
=\bR_2\bR_1^{-1}.
\end{aligned}
$$ 
Since both $\bR_1$ and $\bR_2$ have positive diagonals, this implies $r_{11}=s_{11}, r_{22}=s_{22}, \ldots, r_{nn}=s_{nn}$. 
Thus, we conclude that $\bLambda = \bR_1\bR_2^{-1}= \bR_2\bR_1^{-1}  =\bI$, which implies $\bR_1=\bR_2$, contradicting our initial assumption that the decomposition was not unique. Therefore, the Cholesky decomposition is unique.
\end{proof}		

\paragraph{Computing Cholesky decomposition element-wise.}

It is common to compute the Cholesky decomposition using element-wise equations derived directly from solving the matrix equation $\bA=\bR^\top\bR$. Observing that the $(i,j)$-th entry of $\bA$ is given by $a_{ij} = \bR_{:,i}^\top \bR_{:,j} = \sum_{k=1}^{i} r_{ki}r_{kj}$ if $i<j$. This further implies the  following recurrence relation: if $i<j$, we have
$$
\begin{aligned}
a_{ij} &= \bR_{:,i}^\top \bR_{:,j} = \sum_{k=1}^{i} r_{ki}r_{kj} 
= \sum_{k=1}^{i-1} r_{ki}r_{kj} + r_{ii}r_{ij}
\implies
r_{ij} = (a_{ij} - \sum_{k=1}^{i-1} r_{ki}r_{kj})/r_{ii},
\gap 
\text{if }i<j.
\end{aligned}
$$
For the diagonal entries ($i=j$), we have:
$$
\begin{aligned}
a_{jj} &= \sum_{k=1}^{j} r_{kj}^2=\sum_{k=1}^{j-1} r_{kj}^2 + r_{jj}^2
&\implies
r_{jj} = \sqrt{a_{jj} - \sum_{k=1}^{j-1} r_{kj}^2}.
\end{aligned}
$$
If we equate the elements of $\bR$ by taking a column at a time and start with $r_{11} = \sqrt{a_{11}}$, the element-level algorithm is formulated in Algorithm~\ref{alg:compute-choklesky-element-level}.


\begin{algorithm}[H] 
\caption{Cholesky Decomposition Element-Wise: $\bA=\bR^\top\bR$} 
\label{alg:compute-choklesky-element-level} 
\begin{algorithmic}[1] 
\Require 
Positive definite matrix $\bA$ with size $n\times n$; 
\State Calculate first element of $\bR$ by $r_{11} \leftarrow \sqrt{a_{11}}$; 
\For{$j=1$ to $n$} \Comment{Compute the $j$-th column of $\bR$}
\For{$i=1$ to $j-1$} 
\State $r_{ij} \leftarrow (a_{ij} - \sum_{k=1}^{i-1} r_{ki}r_{kj})/r_{ii}$, since $i<j$;
\EndFor
\State $r_{jj} \leftarrow \sqrt{a_{jj}- \sum_{k=1}^{j-1}r_{kj}^2}$;
\EndFor
\State Output $\bA=\bR^\top\bR$.
\end{algorithmic} 
\end{algorithm}
On the other hand, Algorithm~\ref{alg:compute-choklesky-element-level}  can be modified to compute the Cholesky decomposition in the form $\bA=\bL\bD\bL^\top$, where $\bL$ is unit lower triangular and $\bD$ is diagonal, as outlined in Algorithm~\ref{alg:compute-choklesky-_ldl}, whose Step 3 and Step 5 are derived from (since $l_{ii}=1, \forall i\in\{1,2,\ldots,n\}$):
$$
\begin{aligned}
a_{jj}&=\sum_{k=1}^{j-1}d_{kk} l_{jk}^2 + d_{jj};\\
a_{ij}&= d_{jj} l_{ij}+ \sum_{k=1}^{j-1} d_{kk} l_{ik}l_{jk}, \gap \text{if }i>j.
\end{aligned}
$$
\begin{exercise}
Derive the complexity of Algorithm~\ref{alg:compute-choklesky-_ldl}.
\end{exercise}
This form of Cholesky decomposition is useful for determining the condition number of a PD matrix \citep{lu2021numerical}. 
In essence, the condition number of a function measures the sensitivity of the output value to small changes in the input; a smaller condition number indicates better numerical stability. 
For positive definite linear systems, the condition number is defined as the ratio of the largest eigenvalue to the smallest eigenvalue.
The condition number of a positive definite matrix is lower bounded by the diagonal matrix in the Cholesky decomposition (see Problem~\ref{problem:cond_pd}):
\begin{equation}\label{equation:cond_pd_ineq}
\cond(\bA) \geq \cond(\bD).
\end{equation}
This can be proven by showing that $\lambda_{\max}\geq d_{\max}$ and $\lambda_{\min}\leq d_{\min}$, where $\lambda_{\max}$ and $\lambda_{\min}$ are the largest and smallest eigenvalue of $\bA$, and $d_{\max}$ and $d_{\min}$ are the largest and smallest diagonals of $\bD$.
Therefore, this form of the Cholesky decomposition can be utilized  to modify Newton's method; see \S~\ref{section:modified_damp_new}.


\begin{algorithm}[h] 
\caption{Cholesky Decomposition Element-Wise: $\bA=\bL\bD\bL^\top$}  
\label{alg:compute-choklesky-_ldl} 
\begin{algorithmic}[1] 
\Require 
Positive definite matrix $\bA$ with size $n\times n$; 
\For{$j=1$ to $n$} \Comment{Compute the $j$-th column of $\bL$}
\State $l_{jj}\leftarrow1$;
\State $c_{jj}\leftarrow a_{jj}-\sum_{k=1}^{j-1}d_{kk} l_{jk}^2$;
\State $d_{jj}\leftarrow c_{jj}$
\For{$i=j+1$ to $n$} 
\State $c_{ij}\leftarrow a_{ij}-\sum_{k=1}^{j-1}d_{kk} l_{ik}l_{jk}$, since $i>j$;
\State $l_{ij}\leftarrow \frac{c_{ij}}{d_{jj}}$;
\EndFor
\EndFor
\State Output $\bA=\bL\bD\bL^\top$, where $\bD=\diag(d_{11}, d_{22},\ldots,d_{nn})$.
\end{algorithmic} 
\end{algorithm}


\subsection{Eigenvalue and Spectral Decomposition}\label{section:eigendecomp}\label{section:spectraldecomp}\label{section:existence-of-spectral}
\textit{Eigenvalue decomposition} is also known as to diagonalize the matrix. If a matrix $\bA\in\real^{n\times n}$ has distinct eigenvalues, its corresponding eigenvectors are guaranteed to be linearly independent, allowing $\bA$ to be diagonalized. It is important to note that without $n$ linearly independent eigenvectors, diagonalization is not possible.
\index{Eigenvalue decomposition}
\begin{theorem}[Eigenvalue Decomposition]\label{theorem:eigenvalue-decomposition}
Any square matrix $\bA\in \real^{n\times n}$ with linearly independent eigenvectors can be factored as 
$$
\bA = \bX\bLambda\bX^{-1},
$$
where $\bX\in\real^{n\times n}$ is a matrix whose columns are the eigenvectors of $\bA$, and $\bLambda\in\real^{n\times n}$ is a diagonal matrix given by $\diag(\lambda_1, $ $\lambda_2, \ldots, \lambda_n)$ with $\lambda_1, \lambda_2, \ldots, \lambda_n$ denoting the eigenvalues of $\bA$.
\end{theorem}
\begin{proof}[of Theorem~\ref{theorem:eigenvalue-decomposition}]
Let $\bX=[\bx_1, \bx_2, \ldots, \bx_n]$ be the linearly independent eigenvectors of $\bA$. Clearly, we have
$$
\bA\bx_1=\lambda_1\bx_1,\qquad \bA\bx_2=\lambda_2\bx_2, \qquad \ldots, \qquad\bA\bx_n=\lambda_n\bx_n.
$$
Expressing this in matrix form, we obtain
$$
\bA\bX = [\bA\bx_1, \bA\bx_2, \ldots, \bA\bx_n] = [\lambda_1\bx_1, \lambda_2\bx_2, \ldots, \lambda_n\bx_n] = \bX\bLambda.
$$
Since the eigenvectors are linearly independent, $\bX$ is invertible, leading to
$
\bA = \bX\bLambda \bX^{-1}.
$
This completes the proof.
\end{proof}

An important advantage of the decomposition $\bA =\bX\bLambda\bX^{-1}$ is that it allows efficient computation of matrix powers.
\index{$m$-th Power}
\begin{remark}[$m$-th Power]\label{remark:power-eigenvalue-decom}
If $\bA$ admits an eigenvalue decomposition $\bA =\bX\bLambda\bX^{-1}$, then its $m$-th power  can be computed as  $\bA^m = \bX\bLambda^m\bX^{-1}$.
\end{remark}

The existence of eigenvalue decomposition depends on the linear independence of the eigenvectors of $\bA$. This condition is automatically satisfied in specific cases.
\begin{lemma}[Different Eigenvalues]\label{lemma:diff-eigenvec-decompo}
If the eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$ of $\bA\in \real^{n\times n}$ are all distinct, then the associated eigenvectors are necessarily linearly independent. Consequently, any square matrix with unique eigenvalues is diagonalizable. 
\end{lemma}
\begin{proof}[of Lemma~\ref{lemma:diff-eigenvec-decompo}]
Assume that $\bA$ has distinct eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$ and that the eigenvectors $\bx_1,\bx_2, \ldots, \bx_n$ are linearly dependent. Then, there exists a nonzero vector $\bc = [c_1,c_2,\ldots,c_{n-1}]^\top$ such that
$
\bx_n = \sum_{i=1}^{n-1} c_i\bx_{i}. 
$
Applying $\bA$ to both sides, we get
$$
\begin{aligned}
\bA \bx_n &= \bA (\sum_{i=1}^{n-1} c_i\bx_{i}) 
=c_1\lambda_1 \bx_1 + c_2\lambda_2 \bx_2 + \ldots + c_{n-1}\lambda_{n-1}\bx_{n-1}.
\end{aligned}
$$
and 
$$
\begin{aligned}
\bA \bx_n &= \lambda_n\bx_n
=\lambda_n (c_1\bx_1 +c_2\bx_2+\ldots +c_{n-1} \bx_{n-1}).
\end{aligned}
$$
Equating both expressions, we obtain
$
\sum_{i=1}^{n-1} (\lambda_n - \lambda_i)c_i \bx_i = \bzero .
$
This leads to a contradiction since $\lambda_n \neq \lambda_i$ for all $i\in \{1,2,\ldots,n-1\}$, from which the result follows.
\end{proof}



%\subsection{Spectral Decomposition}
The \textit{spectral theorem}, also referred to as  the \textit{spectral decomposition} for symmetric matrices, states that every symmetric matrix has real eigenvalues and can be diagonalized using a (real) orthonormal basis \footnote{Note that the spectral decomposition for \textit{Hermitian matrices} states that Hermitian matrices also have real eigenvalues but are diagonalizable using a complex orthonormal basis.}. 
\index{Spectral decomposition}
\begin{theorem}[Spectral Decomposition]\label{theorem:spectral_theorem}
A real matrix $\bA \in \real^{n\times n}$ is symmetric if and only if there exists an orthogonal matrix $\bQ$ and a diagonal matrix $\bLambda$ such that
\begin{equation}\label{equation:spec_decom}
\bA = \bQ \bLambda \bQ^\top,
\end{equation}
where the columns of $\bQ = [\bq_1, \bq_2, \ldots, \bq_n]$ are eigenvectors of $\bA$ and are mutually orthonormal, and the entries of $\bLambda=\diag(\lambda_1, \lambda_2, \ldots, \lambda_n)$ are the corresponding \textbf{real} eigenvalues of $\bA$. 
Moreover, the rank of $\bA$ is equal to the number of nonzero eigenvalues. 
This result is known as the \textit{spectral decomposition} or the \textit{spectral theorem} for real symmetric matrices. Specifically, the following properties hold:
\begin{enumerate}[(i)]
\item A symmetric matrix has only \textbf{real eigenvalues}.
\item The eigenvectors are orthogonal and can be chosen to be \textbf{orthonormal} by normalization.
\item The rank of $\bA$ is equal to the number of nonzero eigenvalues (Proposition~\ref{proposition:rank-of-symmetric}).
\item If the eigenvalues are distinct, the eigenvectors are necessarily linearly independent.
\end{enumerate}
\end{theorem}

The existence of the spectral decomposition follows directly from Symmetric Properties-I$\sim$III (Propositions~\ref{proposition:real-eigenvalues-spectral}$\sim$\ref{proposition:eigen-multiplicity}). Note that the Symmetric Property-IV (Proposition~\ref{proposition:rank-of-symmetric}) is proved using the main result in the spectral decomposition.
Similar to the eigenvalue decomposition, spectral decomposition allows for efficient computation of the $m$-th power of a matrix.
\index{$m$-th Power}
\begin{remark}[$m$-th Power]\label{remark:power-spectral}
If a matrix $\bA$ admits a spectral decomposition $\bA=\bQ\bLambda\bQ^\top$, then its $m$-th power can be computed as  $\bA^m = \bQ\bLambda^m\bQ^\top$.
\end{remark}

\subsection{QR Decomposition}
In many applications, we are interested in the column space of a matrix $\bA=[\ba_1, \ba_2, \ldots, \ba_n] \in \real^{m\times n}$. The successive subspaces spanned by the columns $\ba_1, \ba_2, \ldots$ of $\bA$ plays a crucial role in understanding the structure and properties of the matrix:
$$
\cspace([\ba_1])\,\,\,\, \subseteq\,\,\,\, \cspace([\ba_1, \ba_2]) \,\,\,\,\subseteq\,\,\,\, \cspace([\ba_1, \ba_2, \ba_3])\,\,\,\, \subseteq\,\,\,\, \ldots,
$$
where $\cspace([\ldots])$ denotes the subspace spanned by the enclosed vectors, alternatively expressed as $\cspace([\ldots])=\spn\{\ldots\}$.
The concept of orthogonal or orthonormal bases within the column space is fundamental to many algorithms, enabling efficient computations and interpretations. \textit{QR factorization} is a widely used technique for analyzing and decomposing matrices in a way that explicitly reveals their column space structure.

QR decomposition constructs a sequence of orthonormal vectors $\bq_1, \bq_2, \ldots$ that span the same successive subspaces:
$$
\big\{\cspace([\bq_1])=\cspace([\ba_1])\big\}\subseteq 
\big\{\cspace([\bq_1, \bq_2])=\cspace([\ba_1, \ba_2])\big\}\subseteq
\big\{\cspace([\bq_1, \bq_2, \bq_3])=\cspace([\ba_1, \ba_2, \ba_3])\big\} 
\subseteq \ldots.
$$
\begin{theorem}[QR Decomposition]\label{theorem:qr-decomposition}
Let $\bA=[\ba_1, \ba_2, \ldots, \ba_n] \in\real^{m\times n}$ be any matrix (regardless of whether its columns are linearly independent or dependent) with $m\geq n$. Then, it can be factored as 
$$
\bA = \bQ\bR,~\footnote{If $\bA$ is complex, then $\bQ$ is unitary (or semi-unitary with orthonormal columns) and $\bR$ is complex upper triangular.}
$$
where 
\begin{enumerate}
\item  \textbf{Reduced}: $\bQ$ is $m\times n$ with orthonormal columns, and $\bR$ is an $n\times n$ upper triangular matrix, known as the \textbf{reduced QR decomposition};
\item \textbf{Full}: $\bQ$ is $m\times m$ with orthonormal columns, and $\bR$ is an $m\times n$ upper triangular matrix, known as the \textbf{full QR decomposition}. If we further restrict the upper triangular matrix to be a square matrix, the full QR decomposition takes the form
$$
\bA = \bQ\begin{bmatrix}
\bR_0\\
\bzero
\end{bmatrix},
$$
where $\bR_0$ is an $n\times n$ upper triangular matrix. 
\end{enumerate}

Specifically, when $\bA$ has full rank, i.e., $\bA$  has linearly independent columns, then $\bR$ also exhibits linearly independent columns, and $\bR$ is nonsingular in the \textit{reduced} case, meaning its diagonal elements are nonzero.
Under this condition, when we further restrict elements on the diagonal of $\bR$ to be  \textbf{positive} and $\rank(\bA)=n$, the \textit{reduced} QR decomposition is \textbf{unique} \citep{lu2022matrix}. However, the \textit{full} QR decomposition is generally not unique since the rightmost $(m-n)$ columns in $\bQ$ can be arranged in any order.
\end{theorem}

%The QR decomposition is a method of factorizing a matrix into an orthogonal matrix  $\bQ$ and an upper triangular matrix $\bR$. This technique is particularly useful in linear algebra and has a variety of significant applications. For example, QR decomposition can be employed to solve linear systems of equations $\bA\bx=\bb$. Since $\bQ$ is orthogonal ($\bQ\bQ^\top=\bI$), the system can be transformed into $\bR\bx=\bQ^\top\bb$, which is easier to solve due to the upper triangular nature of $\bR$.
%
%In the field of statistics and data fitting, QR decomposition is used to find the least squares solution for an overdetermined system, where there are more equations than unknowns. This provides a method to minimize the sum of the squares of the residuals. Moreover, QR decomposition can be utilized to compute the eigenvalues and eigenvectors of a matrix, especially within algorithms like the \textit{QR algorithm} \citep{lu2021numerical}.
%
%If a matrix $\bA$ can be decomposed into $\bQ$ and $\bR$, then its inverse $\bA^{-1}$ can be calculated as $\bR^{-1}\bQ^\top$. This approach is more stable and efficient than direct inversion for certain types of matrices.  The QR decomposition is also numerically stable, meaning it is less susceptible to round-off errors compared to some other matrix decompositions, making it ideal for computational tasks. In certain applications, the structure of the matrix, like sparsity, can be exploited during the QR decomposition process to reduce computational complexity.

\paragraph{Project vector $\ba$ onto vector $\bb$.}
The projection of a vector $\ba$ onto another vector $\bb$ finds the closest vector to $\ba$ that lies along the line spanned by $\bb$.
%The process of projecting  a vector $\ba$ onto a vector $\bb$ involves finding the vector closest to $\ba$ along the line determined by $\bb$. 
The projected vector, denoted by $\widehat{\ba}$, is a scalar multiple of $\bb$. 
Let $\widehat{\ba} \triangleq \widehat{x} \bb$. By construction, $\ba-\widehat{\ba}$ is perpendicular to $\bb$, leading to the following result:
$$
\text{$\ba^\perp \triangleq(\ba-\widehat{\ba}) \perp \bb 
\quad\implies\quad 
(\ba-\widehat{x}\bb)^\top\bb=0
\quad\implies\quad 
\widehat{x}$ = $\frac{\ba^\top\bb}{\bb^\top\bb}$ and $\widehat{\ba} = \frac{\ba^\top\bb}{\bb^\top\bb}\bb = \frac{\bb\bb^\top}{\bb^\top\bb}\ba$}.
$$
%~\footnote{The result can be generalized to a norm derived from an inner product (Definition~\ref{definition:inner_prod}, Exercise~\ref{exercise:norm_innr_pro}). Let $\norm{\cdot}$ be a norm on $\real^n$ that is derived from an inner product. Then, $\widehat{x}$ that minimizes the value $\norm{\ba-\widehat{x}\bb}$ is $\widehat{x}=\frac{\inner{\ba}{\bb}}{\norm{\bb}^2}$. When the inner product is the Euclidean inner product, it reduces to the main result shown above.}
The above discussion leads to the following lemma.
\begin{lemma}[Finding an Orthogonal Vector]
Given two unit vectors $\bu$ and $\bv$ (i.e., $\normtwo{\bu}=\normtwo{\bv}=1$), then $\bw\triangleq\bv-\bu\bu^\top\bv$ is orthogonal to $\bu$.
If $\bu$ and $\bv$ are not unit vectors, they can be normalized $\bu:=\frac{\bu}{\normtwo{\bu}}$ and $\bv:=\frac{\bv}{\normtwo{\bv}}$ to achieve the same result.
\end{lemma}

Given three linearly independent vectors $\{\ba_1, \ba_2, \ba_3\}$ and the space spanned by the three linearly independent vectors $\cspace{([\ba_1, \ba_2, \ba_3])}$, i.e., the column space of the matrix $[\ba_1, \ba_2, \ba_3]$, we aim to construct three orthogonal vectors $\{\bb_1, \bb_2, \bb_3\}$ such that $\cspace{([\bb_1, \bb_2, \bb_3])}$ = $\cspace{([\ba_1, \ba_2, \ba_3])}$, i.e., the column space remains unchanged. 
We then normalize these orthogonal vectors to obtain an orthonormal set. This process produces three mutually orthonormal vectors $\bq_1 \triangleq \frac{\bb_1}{\normtwo{\bb_1}}$, $\bq_2 \triangleq \frac{\bb_2}{\normtwo{\bb_2}}$, and $\bq_2 \triangleq \frac{\bb_2}{\normtwo{\bb_2}}$.

For the first vector, we simply set $\bb_1 \triangleq \ba_1$. The second vector $\bb_2$ must be perpendicular to the first one. This is achieved by considering the vector $\ba_2$ and subtracting its projection along $\bb_1$:
\begin{equation}
\begin{aligned}
\bb_2 &
%= \ba_2- \frac{\bb_1 \bb_1^\top}{\bb_1^\top\bb_1} \ba_2 
= \left(\bI- \frac{\bb_1 \bb_1^\top}{\bb_1^\top\bb_1} \right)\ba_2  
= \ba_2-  \underbrace{\frac{ \bb_1^\top \ba_2}{\bb_1^\top\bb_1} \bb_1}_{\triangleq\widehat{\ba}_2},
\end{aligned}
\end{equation}
where the first equation shows that $\bb_2$ results from the multiplication of the matrix $\big(\bI- \frac{\bb_1 \bb_1^\top}{\bb_1^\top\bb_1} \big)$ and the vector $\ba_2$, signifying the projection of $\ba_2$ onto the orthogonal complement space of $\cspace{([\bb_1])}$. The second equality in the above equation shows that $\ba_2$ is a linear combination of $\bb_1$ and $\bb_2$.
Clearly, the space spanned by $\bb_1, \bb_2$ coincides with the space spanned by $\ba_1, \ba_2$. 
Figure~\ref{fig:gram-schmidt1} illustrates this process, where we designate \textbf{the direction of $\bb_1$ as the $x$-axis in the Cartesian coordinate system}, and $\widehat{\ba}_2$ represents the projection of $\ba_2$ onto the line defined by $\bb_1$. 
The figure clearly demonstrates that  the component of $\ba_2$ that is perpendicular to $\bb_1$ is $\bb_2 = \ba_2 - \widehat{\ba}_2$, as derived from the Pythagorean theorem.


For the third vector $\bb_3$, it must be perpendicular to both  $\bb_1$ and $\bb_2$, which corresponds to the vector $\ba_3$ subtracting its projection along the plane spanned by $\bb_1$ and $\bb_2$:
\begin{equation}\label{equation:gram-schdt-eq2}
\begin{aligned}
\bb_3 &
%= \ba_3- \frac{\bb_1 \bb_1^\top}{\bb_1^\top\bb_1} \ba_3 - \frac{\bb_2 \bb_2^\top}{\bb_2^\top\bb_2} \ba_3 
= \left(\bI- \frac{\bb_1 \bb_1^\top}{\bb_1^\top\bb_1}  - \frac{\bb_2 \bb_2^\top}{\bb_2^\top\bb_2} \right)\ba_3  
= \ba_3- \underbrace{\frac{ \bb_1^\top\ba_3}{\bb_1^\top\bb_1} \bb_1}_{\triangleq\widehat{\ba}_3} - \underbrace{\frac{ \bb_2^\top\ba_3}{\bb_2^\top\bb_2}  \bb_2}_{\triangleq\bar{\ba}_3},   
\end{aligned}
\end{equation}
where the first equation shows that $\bb_3$ is a multiplication of the matrix $\left(\bI- \frac{\bb_1 \bb_1^\top}{\bb_1^\top\bb_1}  - \frac{\bb_2 \bb_2^\top}{\bb_2^\top\bb_2} \right)$ and the vector $\ba_3$, signifying the projection of $\ba_3$ onto the orthogonal complement space of $\cspace{([\bb_1, \bb_2])}$. The second equality in the above equation shows that $\ba_3$ is a linear combination of $\bb_1, \bb_2, $ and $\bb_3$. This property is essential in the idea of the QR decomposition.
Once again, it can be shown that the space spanned by $\bb_1, \bb_2, \bb_3$ is identical to the space spanned by $\ba_1, \ba_2, \ba_3$. 
Figure~\ref{fig:gram-schmidt2} illustrates this step, where we designate \textbf{the direction of $\bb_2$ as the $y$-axis in the Cartesian coordinate system}. Here, $\widehat{\ba}_3$ is the projection of $\ba_3$ onto line $\bb_1$, while $\bar{\ba}_3$ represents the projection of $\ba_3$ onto line $\bb_2$. 
It can be shown that the component of $\ba_3$ perpendicular to both $\bb_1$ and $\bb_2$ is $\bb_3=\ba_3-\widehat{\ba}_3-\bar{\ba}_3$ from the figure.


Finally, we normalize each vector by dividing its  length, resulting in three orthonormal vectors $\bq_1 \triangleq \frac{\bb_1}{\normtwo{\bb_1}}$, $\bq_2 \triangleq \frac{\bb_2}{\normtwo{\bb_2}}$, and $\bq_2 \triangleq \frac{\bb_2}{\normtwo{\bb_2}}$.


\begin{figure}[H]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[Project $\ba_2$ onto the space perpendicular to $\bb_1$.]{\label{fig:gram-schmidt1}
\includegraphics[width=0.4\linewidth]{./imgs/gram-schmidt1.pdf}}
\quad 
\subfigure[Project $\ba_3$ onto the space perpendicular to $\bb_1, \bb_2$.]{\label{fig:gram-schmidt2}
\includegraphics[width=0.4\linewidth]{./imgs/gram-schmidt2.pdf}}
\caption{The Gram-Schmidt process.}
\label{fig:gram-schmidt-12}
\end{figure}

This procedure can be extended beyond three vectors to an arbitrary set and is known as the \textit{Gram-Schmidt process}. After applying this process, the matrix $\bA$ is transformed into an upper triangular form. The method is named after \textit{Jørgen Pedersen Gram} and \textit{Erhard Schmidt}, though  it appeared earlier in the work of \textit{Pierre-Simon Laplace} within the context of Lie group decomposition theory.

As previously mentioned, the key idea behind QR decomposition is to construct a sequence of orthonormal vectors $\bq_1, \bq_2, \ldots$, which successively span the same subspaces as the original vectors:
$$
\big\{\cspace([\bq_1])=\cspace([\ba_1])\big\} \subseteq
\big\{\cspace([\bq_1, \bq_2])=\cspace([\ba_1, \ba_2])\big\} \subseteq
\big\{\cspace([\bq_1, \bq_2, \bq_3])=\cspace([\ba_1, \ba_2, \ba_3])\big\} 
\subseteq \ldots.
$$
This implies that  any vector $\ba_k$ resides in the space spanned by $\cspace([\bq_1, \bq_2, \ldots, \bq_k])$.~\footnote{Moreover, any vector $\bq_k$ resides in the space spanned by $\cspace([\ba_1, \ba_2, \ldots, \ba_k])$.} Once these orthonormal vectors are determined, the reconstruction of $\ba_i$'s from the orthogonal matrix $\bQ=[\bq_1, \bq_2, \ldots, \bq_n]$ requires an upper triangular matrix $\bR$,  such that $\bA = \bQ\bR$.

\index{Gram–Schmidt}
While the Gram-Schmidt process is a widely used algorithm for QR decomposition, it is not the only approach. Alternative methods, such as \textit{Householder reflections} and \textit{Givens rotations}, offer more numerical stability in the presence of round-off errors and may modify the order in which the columns of $\bA$ are processed \citep{lu2021numerical}.


\subsection{Singular Value Decomposition}
In eigenvalue decomposition, we factor the matrix into a diagonal matrix. However, this is not always possible. 
When $\bA$ lacks linearly independent eigenvectors, such diagonalization cannot be achieved.  
The \textit{singular value decomposition (SVD)} fills this gap. 
Instead of using an eigenvector matrix, SVD decomposes a matrix into two orthogonal matrices. Below, we state the theorem for SVD.






\index{Karhunen-Loewe expansion}
\index{Decomposition: SVD}
\begin{theoremHigh}[Reduced SVD for Rectangular Matrices]\label{theorem:reduced_svd_rectangular}
Let $\bA\in\real^{m\times n}$ be any real $m\times n$ matrix with rank $r$. Then, it can be factored as
$$
\bA = \bU \bSigma \bV^\top,
$$ 
where $\bSigma\in \real^{r\times r}$ is a real diagonal matrix, $\bSigma=\diag(\sigma_1, \sigma_2 \ldots, \sigma_r)$ with $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r>0$ and 
\footnote{Note when $\bA$ is complex,  both $\bU$ and $\bV$ become semi-unitary. However, $\bSigma$ remains real and diagonal because the eigenvalues of Hermitian matrices $\bA\bA^*$ or $\bA^*\bA$ are real.}
%\footnote{In image processing, the SVD is also known as the \textit{Karhunen-Loewe expansion}.}
\begin{itemize}
\item The $\sigma_i$'s are the nonzero \textit{singular values} of $\bA$, in the meantime, which are the positive square roots of the nonzero \textit{eigenvalues} of $\trans{\bA} \bA$ and $ \bA \trans{\bA}$.

\item The columns of $\bU\in \real^{m\times r}$ contain the $r$ eigenvectors of $\bA\bA^\top$ corresponding to the $r$ nonzero eigenvalues of $\bA\bA^\top$ (semi-orthogonal). 

\item The columns of $\bV\in \real^{n\times r}$ contain the $r$ eigenvectors of $\bA^\top\bA$ corresponding to the $r$ nonzero eigenvalues of $\bA^\top\bA$ (semi-orthogonal). 

%\item If $\sigma_r > 0$ is the smallest singular value greater than zero then the matrix $\bA$ has rank $r$.

\item The columns of $\bU$ and $\bV$ are called the \textit{left and right singular vectors} of $\bA$, respectively. 

\item Furthermore, the columns of both $\bU$ and $\bV$ are mutually orthonormal (by spectral theorem~\ref{theorem:spectral_theorem}). 
\end{itemize}

In particular, the matrix decomposition can be expressed as the sum of outer products of vectors:
$$
\bA = \bU \bSigma \bV^\top = \sum_{i=1}^r \sigma_i \bu_i \bv_i^\top,
$$ which is a sum of $r$ rank-one matrices.



\end{theoremHigh}



By appending $m-r$ additional orthonormal columns to the  $r$  eigenvectors of   $\bA\bA^\top$, we obtain an orthogonal matrix $\bU\in \real^{m\times m}$. 
The same process applies to the columns of $\bV$.
We then present the full SVD  in the following theorem. 
Differences between the reduced and full SVD are highlighted in \textcolor{mylightbluetext}{blue} text.
\begin{theoremHigh}[Full SVD for Rectangular Matrices]\label{theorem:full_svd_rectangular}
Let $\bA\in\real^{m\times n}$ be any real $m\times n$ matrix with rank $r$. Then, it can be factored as
$$
\bA = \bU \bSigma \bV^\top,
$$ 
where the  top-left part of $\bSigma\in $\textcolor{mylightbluetext}{$\real^{m\times n}$} is a real diagonal matrix, that is $\bSigma=\footnotesize\begin{bmatrix}
\bSigma_1 & \bzero \\
\bzero & \bzero
\end{bmatrix}$, where $\bSigma_1=\diag(\sigma_1, \sigma_2 \ldots, \sigma_r)\in \real^{r\times r}$ with $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r>0$ and 
\begin{itemize}
\item The $\sigma_i$'s are the nonzero \textit{singular values} of matrix $\bA$, which are the positive square roots of the nonzero \textit{eigenvalues} of $\trans{\bA} \bA$ and $ \bA \trans{\bA}$. 

\item  The columns of $\bU\in \textcolor{mylightbluetext}{\real^{m\times m}}$ contain the $r$ eigenvectors of $\bA\bA^\top$ corresponding to the $r$ nonzero eigenvalues of $\bA\bA^\top$ \textcolor{mylightbluetext}{and $m-r$ extra orthonormal vectors from $\nspace(\bA^\top)$}. 

\item  The columns of $\bV\in \textcolor{mylightbluetext}{\real^{n\times n}}$ contain the $r$ eigenvectors of $\bA^\top\bA$ corresponding to the $r$ nonzero eigenvalues of $\bA^\top\bA$ \textcolor{mylightbluetext}{and $n-r$ extra orthonormal vectors from $\nspace(\bA)$}.

\item  The columns of  $\bU$ and $\bV$ are called the \textit{left and right singular vectors} of $\bA$, respectively. 

\item Furthermore, the columns of $\bU$ and $\bV$ are mutually orthonormal (by spectral theorem~\ref{theorem:spectral_theorem}), and \textcolor{mylightbluetext}{$\bU$ and $\bV$ are orthogonal matrices}. 
\end{itemize}

In particular, the matrix decomposition can be expressed as the sum of outer products of vectors: $ \bA = \bU \bSigma \bV^\top = \sum_{i=1}^r \sigma_i \bu_i \bv_i^\top$, which is a sum of $r$ rank-one matrices.
\end{theoremHigh}


In image processing,  SVD is also known as the \textit{Karhunen-Loewe expansion}.
The comparison between the reduced and the full SVD is shown in Figure~\ref{fig:svd-comparison}, where white entries are zero, and \textcolor{mylightbluetext}{blue} entries are not necessarily zero.
\begin{figure}[H]
\centering  
\vspace{-0.35cm} 
\subfigtopskip=2pt 
\subfigbottomskip=2pt 
\subfigcapskip=-5pt 
\subfigure[Reduced SVD decomposition.]{\label{fig:svdhalf}
\includegraphics[width=0.47\linewidth]{./imgs/svdreduced.pdf}}
\quad 
\subfigure[Full SVD decomposition.]{\label{fig:svdall}
\includegraphics[width=0.47\linewidth]{./imgs/svdfull.pdf}}
\caption{Comparison between the reduced and full SVD.}
\label{fig:svd-comparison}
\end{figure}


%\begin{remark}
%We clarify some terminologies and observations in the following.
%\begin{itemize}
%%\item The \textit{multiplicity} of a singular value $\sigma_i$ of $\bA$ is the multiplicity of $\sigma_i^2$ as an eigenvalue of $\bA^\top\bA$ or, equivalently, of $\bA\bA^\top$. Therefore, a singular value $\sigma_i$ of $\bA$ is said to be \textbf{simple} if $\sigma_i^2$ is a simple eigenvalue (Definition~\ref{definition:simple_eig}) of $\bA^\top\bA$ or, equivalently, of $\bA\bA^\top$.
%
%\item Complex Hermitian matrices has only real eigenvalues (Problem~\ref{problem:real_herm}). Therefore, for a complex matrix $\bA\in\complex^{m\times n}$, the singular matrix $\bSigma$ remains real, while the orthogonal matrices $\bU$ and $\bV$ become unitary (resp. semi-unitary) for the full SVD (resp. reduced SVD).
%
%\item We have shown that product and sum of eigenvalues are equal to the determinant and trace of the given matrix, respectively (Theorem~\ref{theorem:eigen_trace}, \ref{theorem:eigen_trace2}). Therefore, the product of singular values of $\bA$ is equal to $\abs{\det(\bA)}$; and the sum of squared singular values is equal to $\trace(\bA^\top\bA)=\trace(\bA\bA^\top)$.
%
%\item The SVD of a matrix is not unique. For example, replacing $\bU$ by $-\bU$ and $\bV$ by $-\bV$ yields another valid SVD.
%
%\item When $\bA$ is symmetric, then $\bU=\bV$. The singular values are equal to the eigenvalues of $\bA$ (Theorem~\ref{theorem:spectral_theorem}, Corollary~\ref{corollary:spectral_normal_symmetricx}, Problem~\ref{problem:symm_square}).
%
%\item When $\bA$ is skew-symmetric, then $\bU=\bV$. The singular values are equal to the square roots of the eigenvalues of $-\bA^2$ (Corollary~\ref{corollary:spectral_normal_skew}, Problem~\ref{problem:skewsymm_square}, Problem~\ref{problem:skewsymm_square2}).
%\end{itemize}
%\end{remark}

\section{Classes of Rate of Convergence}
Before we delve into specific  algorithms for solving optimization problems, it is essential to establish criteria for evaluating the speed at which these algorithms converge, as most of them are iterative methods. We define the convergence of a sequence as follows. Note that the $t$-th element in a sequence is denoted by a superscript in parentheses. 
For instance, $\bA^{(t)}$ denotes the $t$-th matrix in a sequence, and $\ba^{(t)}$ represents the $t$-th vector in a sequence. \index{Rate of convergence}
\begin{definition}[Convergence of a Sequence]
Let $\mu^{(1)}, \mu^{(2)}, \ldots \in \real$ be an infinite sequence of scalars. The sequence $\{\mu^{(t)}\}_{t>0}$ is said to converge to $\mu^*$ if 
$$
\mathop{\lim}_{t\rightarrow \infty}  \abs{\mu^{(t)} - \mu^*} = 0.
$$
Similarly, let $\bmu^{(1)}, \bmu^{(2)}, \ldots \in \real^n$ be an infinite sequence of vectors. The sequence $\{\bmu^{(t)}\}_{t>0}$ is said to converge to $\bmu^\star$ if 
$$
\mathop{\lim}_{t\rightarrow \infty} \norm{\bmu^{(t)} - \bmu^\star} = 0.
$$
\end{definition}
The convergence of a sequence of vectors or matrices depends on the norm used. It's important to note that, according to the equivalence of vector or matrix norms (Theorem~\ref{theorem:equivalence-vector-norm}), if a sequence of vectors or matrices converges in one norm, it will also converge in all other norms. 

\begin{definition}[Linear Convergence]\label{definition:linear-convergence}
A sequence $\{\mu^{(t)}\}_{t>0}$ with limit $\mu^*$ is said to converge \textit{linearly} if there exists a constant \textcolor{mylightbluetext}{$c\in (0,1)$} such that 
$$
\abs{\mu^{(t+1)} - \mu^*} \leq c \abs{\mu^{(t)} - \mu^*}.
$$
In other words, the \textit{linearly convergent sequence} has the following property:
$$
\mathop{\lim}_{t\rightarrow \infty}  \frac{\abs{\mu^{(t+1)} - \mu^*}}{\abs{\mu^{(t)} - \mu^*}} = c \in (0,1).
$$
\end{definition}
For example, the sequence $\mu^{(t)} = 4+ (1/4)^t$ converges linearly to $\mu^* = 4$ since 
$$
\mathop{\lim}_{t\rightarrow \infty}  \frac{\abs{\mu^{(t+1)} - \mu^*}}{\abs{\mu^{(t)} - \mu^*}} = \frac{1}{4} \in (0,1).
$$
\begin{definition}[Superlinear Convergence]\label{definition:superlinear_convergence}
A sequence $\{\mu^{(t)}\}_{t>0}$ with limit $\mu^*$ is said to converge \textit{superlinearly} if there exists a constant \textcolor{mylightbluetext}{$c_t >0$ with $c_t \rightarrow 0$}  such that 
$$
\abs{\mu^{(t+1)} - \mu^*} \leq c_t \abs{\mu^{(t)} - \mu^*}.
$$
In other words, the \textit{superlinearly convergent sequence} has the following property:
$$
\mathop{\lim}_{t\rightarrow \infty}  \frac{\abs{\mu^{(t+1)} - \mu^*}}{\abs{\mu^{(t)} - \mu^*}} =0.
$$
\end{definition}
For example, the sequence $\mu^{(t)} = 4+\left(\frac{1}{t+4}\right)^{t+3}$ converges superlinearly to $\mu^* = 4$ since 
$$
\mathop{\lim}_{t\rightarrow \infty}  \frac{\abs{\mu^{(t+1)} - \mu^*}}{\abs{\mu^{(t)} - \mu^*}}= 
\left(\frac{k+4}{k+5}\right)^{k+3}  \frac{1}{k+5}
= 0.
$$


\begin{definition}[Quadratic Convergence]\label{definition:quadratic-convergence}
A sequence $\{\mu^{(t)}\}_{t>0}$ with limit $\mu^*$ is said to converge \textit{quadratically} if there exists a constant \textcolor{mylightbluetext}{$c>0$} such that 
$$
\abs{\mu^{(t+1)} - \mu^*} \leq c \abs{\mu^{(t)} - \mu^*}^2.
$$
In other words, the \textit{quadratically convergent sequence} has the following property:
$$
\mathop{\lim}_{t\rightarrow \infty}  \frac{\abs{\mu^{(t+1)} - \mu^*}}{\abs{\mu^{(t)} - \mu^*}^2} = c .
$$
\end{definition}
For example, the sequence $\mu^{(t)} = 4+ (1/4)^{2^t}$ converges quaeratically to $\mu^* = 4$ since 
$$
\mathop{\lim}_{t\rightarrow \infty}  \frac{\abs{\mu^{(t+1)} - \mu^*}}{\abs{\mu^{(t)} - \mu^*}^2} = 1.
$$

\begin{definition}[Cubic Convergence]
A sequence $\{\mu^{(t)}\}_{t>0}$ with limit $\mu^*$ is said to converge \textit{cubically} if there exists a constant \textcolor{mylightbluetext}{$c>0$} such that 
$$
\abs{\mu^{(t+1)} - \mu^*} \leq c \abs{\mu^{(t)} - \mu^*}^3.
$$
In other words, the \textit{cubically convergent sequence} has the following property:
$$
\mathop{\lim}_{t\rightarrow \infty}  \frac{\abs{\mu^{(t+1)} - \mu^*}}{\abs{\mu^{(t)} - \mu^*}^3} = c .
$$
\end{definition}
For example, the sequence $\mu^{(t)} = 4+ (1/4)^{3^t}$ converges cubically to $\mu^* = 4$ since 
$$
\mathop{\lim}_{t\rightarrow \infty}  \frac{\abs{\mu^{(t+1)} - \mu^*}}{\abs{\mu^{(t)} - \mu^*}^3} = 1.
$$


\begin{problemset}
\item Prove that the sum or maximum  of two norms is also a norm.
\item Prove that the $\ell_1$ and $\ell_\infty$ norms cannot be derived from an inner product.
\textit{Hint: Consider the standard basis vectors $\be_1$ and $\be_2$, and examine the parallelogram identity in \eqref{equation:norm_inn_pro} or the polarization identity in \eqref{equation:norm_pol_pro}.}
\item \textbf{$k$-Norm.}
Show that the $k$-norm $\norm{\bx}^{[k]}$ on $\bx\in\real^m$, defined as the sum of the largest $k$ absolute elements of a vector, is a valid norm. 
Specifically, verify that $\norm{\bx}_\infty = \norm{\bx}^{[1]}$ and $\norm{\bx}^{[n]}=\norm{\bx}_1$. 
Additionally, prove the following sequence of inequalities:
\begin{equation}
\norm{\bx}_\infty = \norm{\bx}^{[1]} 
\leq 
\norm{\bx}^{[2]}
\leq\ldots 
\leq 
\norm{\bx}^{[n]}=\norm{\bx}_1.
\end{equation}

\item \label{problem:cond_pd} Given the Cholesky decomposition of a PD matrix: $\bA=\bL\bD\bL^\top$, show that $\cond(\bA)\geq \cond(\bD)$.

\item In Proposition~\ref{proposition:orthogonal-eigenvectors}, we demonstrated that  eigenvectors corresponding to distinct eigenvalues  of a symmetric matrix are orthogonal. More generally, prove that the eigenvectors corresponding to distinct eigenvalues of any matrix are linearly independent.
%\begin{theorem}[Independent Eigenvector Theorem]\label{theorem:independent-eigenvector-theorem}
%Let $\bA\in \real^{n\times n}$ be any matrix with $k$ distinct eigenvalues, then any set of $k$ corresponding (nonzero) eigenvectors are linearly independent.



\item Show that if $\bA$ is positive semidefinite and nonsingular, then $\bA^{-1}$ is positive definite.
%\item Given a positive semidefinite matrix $\bA$, show that its eigenvalue $\lambda$ is nonnegative.


\item \textbf{Quadratic form.} Consider   the quadratic form $L(\bx) = \frac{1}{2} \bx^\top \bA \bx - \bb^\top \bx + c$ in Equation~\eqref{equation:quadratic-form-general-form1}, where $\bA\in\real^{n\times n}$, $ \bx\in \real^n$, and $c\in\real$. Suppose $\bA$ is positive semidefinite. 
Show that $L(\bx)$ is bounded below over $\real^n$ if and only if $\bb$ is in the column space of $\bA$.

\item \textbf{Quadratic form.} Consider  again  the quadratic form $L(\bx) = \frac{1}{2} \bx^\top \bA \bx - \bb^\top \bx + c$ in Equation~\eqref{equation:quadratic-form-general-form1}. Show that $L(\bx)$ is coercive~\footnote{A function $f(\bx):\real^n\rightarrow \real$ is called coercive if $\mathop{\lim}_{\bx\rightarrow \infty} f(x)=\infty$.} if and only if $\bA$ is PD.

\item  \label{problem:symm_square} \textbf{Square of symmetric matrices.} Let $\bA\in\real^{n\times n}$ be symmetric. Show that the eigenvalues of $\bA^2$ (can only be nonnegative) are the squares  of the eigenvalues of $\bA\in\real^{n\times n}$. What is the relationship between the eigenvectors?



\item \label{problem:part_ortho} Consider the partition of an orthogonal matrix
$
\bQ = 
\scriptsize
\begin{bmatrix}
	\underset{p\times p}{\bA} &\underset{p\times q}{\bB} \\
	\underset{q\times p}{\bC} & \underset{q\times q}{\bD}
\end{bmatrix}\in\real^{n\times n}.
$
Show that $\rank(\bB)=\rank(\bC)$ and $\rank(\bD)=n+\rank(\bA)-2p$.
\end{problemset}


