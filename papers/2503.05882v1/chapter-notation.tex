\chapter*{Notation}\label{notation}
\index{Notation}


%\typeout{START_CHAPTER "notation" \theabspage}

% Sometimes we have to include the following line to get this section
% included in the Table of Contents despite being a chapter*
%\addcontentsline{toc}{chapter}{Notation}
This section provides a concise reference describing notation used throughout this
book.
If you are unfamiliar with any of the corresponding mathematical concepts,
the book describes most of these ideas in Chapter~\ref{chapter_introduction}.


%\vspace{0.4in}
\noindent
\begin{minipage}{\textwidth}
\centerline{\bf General Notations}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cp{4.25in}}
$\displaystyle \triangleq$ & equals by definition \\ 
$\displaystyle :=, \leftarrow$ & equals by assignment \\ 
$\displaystyle \pi$ &	 3.141592$\ldots$ \\
$\displaystyle e, \exp$ &	 2.71828$\ldots$
\end{tabular}
\egroup
\end{minipage}


%\vspace{0.4in}
%\vspace{\notationgap}
% Need to use minipage to keep title of table on same page as table
\noindent
\begin{minipage}{\textwidth}
% This is a hack to put a little title over the table
% We cannot use "\section*", etc., they appear in the table of contents.
% tocdepth does not work on this chapter.
\centerline{\bf Numbers and Arrays}
\bgroup
% The \arraystretch definition here increases the space between rows in the table,
% so that \displaystyle math has more vertical space.
\def\arraystretch{1.5}
\begin{tabular}{cp{4.25in}}
$\displaystyle a$   & A scalar (integer or real)\\
$\displaystyle \ba$ & A vector\\
$\displaystyle \bone$ & A all-ones vector\\
$\displaystyle \bA$ & A matrix\\
%$\displaystyle \eA$ & A tensor\\
$\displaystyle \bI_n$ & Identity matrix with $n$ rows and $n$ columns\\
$\displaystyle \bI$   & Identity matrix with dimensionality implied by context\\
$\displaystyle \be_i$ & Standard basis/canonical orthonormal  vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
$\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\ba$\\
$\displaystyle \text{diag}(\bA, \bB,\ldots)$&  a block-diagonal matrix=($\bA\oplus \bB\oplus \ldots$), the direct sum notation\\
$\displaystyle \ra$   & A scalar random variable\\
$\displaystyle \rva$  & A vector-valued random variable\\
$\displaystyle \rmA$  & A matrix-valued random variable\\
\end{tabular}
\egroup
\index{Scalar}
\index{Vector}
\index{Matrix}
\index{Tensor}
\end{minipage}

\index{Sets}
\vspace{0.2in}
%\vspace{\notationgap}
\noindent
\begin{minipage}{\textwidth}
\centerline{\bf Sets}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cp{4.25in}}
$\displaystyle \sS, \abs{\sS}, \comple{\sS}$ & A set $\sS$, its cardinality, and its complement\\
$\displaystyle \varnothing$ & The null set \\
$\displaystyle \real, \real_+, \real_{++},\extreal $ & The set of real/nonnegative/positive/extended real numbers \\
$\displaystyle \naturalset, \integer$ & The set of natural/integer numbers \\
$\sB_p(\bc, r), \sB_p[\bc, r]$ & Open/close ball: $ \{\bx\in\real^n: \norm{\bx-\bc}_p <r \text{ or } \norm{\bx-\bc}_p \leq r \}$ \\
$\displaystyle \sF$ & The set of either real or complex numbers \\
% NOTE: do not use \R^+, because it is ambiguous whether:
% - It includes 0
% - It includes only real numbers, or also infinity.
% We usually do not include infinity, so we may explicitly write
% [0, \infty) to include 0
% (0, \infty) to not include 0
$\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
$\displaystyle \{0, 1, \ldots, n \}$ & The set of all integers between $0$ and $n$\\
$\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
$\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
$\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\

$\displaystyle \lev[f, \alpha]$  & Level set of a function: $\{ \bx \in \real^n \mid f(\bx) \leq \alpha \}$\\
%$\displaystyle \gG$ & A graph\\
%$\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
\end{tabular}
\egroup
\index{Scalar}
\index{Vector}
\index{Matrix}
\index{Tensor}




\vspace{0.4in}
%\vspace{\notationgap}
\noindent
\begin{minipage}{\textwidth}
\centerline{\bf Calculus}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cp{4.25in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
$\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
$\displaystyle  f^\prime(\bx; \bd)$ or $D_{\bd}f(\bx)$& Directional derivative of $f$\\
$\displaystyle \frac{\partial f}{\partial x_i} (\bx)$, $D_{\be_i}f(\bx)$, or $\partial_i f(\bx)$ & $i$-th partial derivative of $f$\\
$\displaystyle \nabla_{\bx} f(\bx), \nabla f(\bx), \text{ or } \bg$ & Gradient of $f$ with respect to $\bx$ \\
$\displaystyle \nabla_{\bX} y $ & Matrix derivatives of $y$ with respect to $\bX$ \\
%$\displaystyle \nabla_{\eX} y $ & Tensor containing derivatives of $y$ with respect to $\eX$ \\
$\displaystyle \frac{\partial f}{\partial \vx}, \nabla f(\bx), \bJ_f(\bx), \text{ or } \bJ(\bx) $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
$\displaystyle \nabla_\vx^2 f(\vx), \nabla^2 f(\vx), \bH, \text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
$\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
$\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
\end{tabular}
\egroup
\index{Derivative}
\index{Integral}
\index{Jacobian matrix}
\index{Hessian matrix}
\end{minipage}


\index{Set}
\end{minipage}

\index{Matrix indexing}
\vspace{0.2in}
%\vspace{\notationgap}
\noindent
\begin{minipage}{\textwidth}
\centerline{\bf Indexing}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cp{4.25in}}
	$\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
	$\displaystyle \ba_{-i}$ & All elements of vector $\va$ except for element $i$ \\
	$\displaystyle  \bA[i,j]=a_{ij}$ & Element $(i, j)$ of matrix $\mA$ \\
	$\displaystyle \mA_{i, :}=\bA[i,:]$ & Row $i$ of matrix $\mA$ \\
	$\displaystyle \mA_{:, i}=\bA[:,i]=\ba_i$ & Column $i$ of matrix $\mA$ \\
	%$\displaystyle \eA_{ijk}=\elA_{ijk}=a_{ijk}$ & Element $(i, j, k)$ of a 3-D tensor $\eA$\\
	%$\displaystyle \eA_{:, :, i}=\eA[:,:,i]$ & 2-D slice of a 3-D tensor $\eA$\\
	%$\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
\end{tabular}
\egroup
\end{minipage}


\vspace{0.4in}
%\vspace{\notationgap}
\noindent
\begin{minipage}{\textwidth}
\centerline{\bf Probability and Information Theory}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cp{4.25in}}
$\displaystyle \ra \bot \rb$ & The random variables $\ra$ and $\rb$ are independent\\
$\displaystyle \ra \bot \rb \mid \rc $ & They are conditionally independent given $\rc$\\
$\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
$\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
a variable whose type has not been specified\\
$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
$\displaystyle  \Exp_{\rx\sim P} [ f(x) ]\text{ or } \Exp [f(x)]$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
$\displaystyle \Var[f(x)] $ &  Variance of $f(x)$ under $P(\rx)$ \\
$\displaystyle \Cov[f(x),g(x)] $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
$\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
$\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
$\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
\end{tabular}
\egroup
\index{Independence}
\index{Conditional independence}
\index{Variance}
\index{Covariance}
\index{Kullback-Leibler divergence}
\index{Shannon entropy}
\end{minipage}

\vspace{0.4in}
%\vspace{\notationgap}
\noindent
\begin{minipage}{\textwidth}
\centerline{\bf Functions}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cp{3.25in}}
$\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
$\displaystyle f^\prime(\bx; \bd), \nabla f(\bx)$ & Directional derivative, gradient \\
$\displaystyle f'(\bx), \partial f(\bx)$ & Subgradient, subdifferential \\
$\displaystyle \indicatorS(\bx)$ & Indicator function, $\indicatorS(\bx)=0$ if $\bx\in\sS$, and $\indicatorS(\bx)=\infty$ otherwise\\
$\displaystyle \indicator\{\mathrm{condition}\}$ & is 1 if the condition is true, 0 otherwise\\
$\projectS(\bx), \proxf(\bx), \bproxfphi(\bx)$ & Projection/proximal/Bregman-proximal operator\\
$\proxgradL^{f,g}(\bx)\triangleq\prox_{\frac{1}{L} g}\left(\bx - \frac{1}{L}\nabla f(\bx)\right)$ & Prox-grad operator\\
$\gradmapL^{f,g}(\bx)\triangleq L\big(\bx-\proxgradL^{f,g}(\bx)\big)$ & Gradient mapping\\
$\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%$\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%(Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
$\displaystyle \log(x), \ln(x)$ & Natural logarithm of $x$ \\
$\displaystyle \sigma(x),\, \text{Sigmoid}(x)$ & Logistic sigmoid, i.e., $\displaystyle \frac{1} {1 + \exp\{-x\}}$ \\
$\displaystyle \phi(\eta)$ & $\phi(\eta)\triangleq f(\bx^\toptzero - \eta \bg^\toptzero)$ for line search algorithms\\
$\displaystyle \psi_t(\bd)$ &  $\psi_t(\bd) \triangleq f(\bx^\toptzero) + \bd^\top \bg^\toptzero + \frac{1}{2} \bd^\top \nabla^2 f(\bx^\toptzero) \bd$ for trust region methods\\
%$\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp\{x\})$ \\
$\displaystyle \norm{\bx}_p $ & $\ell_p$ norm of $\vx$ \\
$\displaystyle \norm{\bx}_1, \norm{\bx}_2, \norm{\bx}_\infty$ & $\ell_1$ norm, $\ell_2$ norm, $\ell_\infty$ norm\\
$\displaystyle \normf{\bA}, \normtwo{\bA}$ & Frobenius and spectral norms \\
%$\displaystyle \norm{\bA}_{[k]}, \norm{\bA}_{[k,p]}, \norm{\bA}_{[q,p]} $ & Ky Fan $k$-norm, \kyfanpknorm, Schatten $p$-norm \\
$\displaystyle  [x]_+$ & Positive part of $x$, i.e., $\max(0,x)$\\
%$\displaystyle u(x)$ & Step function with value 1 when $x\geq0$ and value 0 otherwise\\
%$\displaystyle \text{Re}(x), \text{Im}(x)$ & Real and imaginary part of $x$\\
\end{tabular}
\egroup
\index{Sigmoid}
\index{Softplus}
\index{Norm}
\end{minipage}
Sometimes we use a function $f$ whose argument is a scalar but apply
it to a vector or a matrix: $f(\vx)$ or $f(\mX)$.
This denotes the application of $f$ to the
array element-wise. For example, if $\by = [\bx]_+$, then $y_i = [x_i]_+$ for all valid values of $i$.


\vspace{0.2in}
%\vspace{\notationgap}
\noindent
\begin{minipage}{\textwidth}
\centerline{\bf Linear Algebra Operations}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cp{4.25in}}
$\displaystyle \abs{\bA}$ & Componentwise absolute matrix $\mA$ \\
$\displaystyle \frac{[\bA]}{[\bB]}$ &  Componentwise division between two matrices\\
$\displaystyle \bA^\top$ & Transpose of matrix $\mA$ \\
$\displaystyle \bA^{-1}$ & Inverse of $\mA$\\
$\displaystyle \bA^+$ & Moore-Penrose pseudo-inverse of $\mA$\\
$\displaystyle \bA \circledast \bB $ & Element-wise (Hadamard) product of $\mA$ and $\mB$ \\
%$\displaystyle \bA \kronecker \bB $ & Kronecker product of $\mA$ and $\mB$ \\
%$\displaystyle \bA \khatrirao \bB $ & Khatri-Rao product of $\mA$ and $\mB$ \\
% Wikipedia uses \circ for element-wise multiplication but this could be confused with function composition
%$\displaystyle \eX \circ \eY $ & Tensor outer product of $\eX$ and $\eY$ \\
$\displaystyle \det(\bA)$ & Determinant of $\bA$ \\
$\displaystyle \trace(\bA)$ & Trace of $\bA$ \\
$\displaystyle \mathrm{rref}(\bA)$ & Reduced row echelon form of $\bA$ \\
$\displaystyle \cspace(\bA)$ & Column space of $\bA$ \\
$\displaystyle \nspace(\bA)$ & Null space of $\bA$ \\
$\displaystyle \mathcalV, \mathcalW$ & A general subspace \\
$\displaystyle \dim(\mathcalV)$ & Dimension of the space $\mathcalV$ \\
$\displaystyle \defect(\bA)$ & Defect or nullity of $\bA$ \\
$\displaystyle \rank(\bA)$ & Rank of $\bA$ \\
$\displaystyle \trace(\bA)$ & Trace of $\bA$ \\
%$\displaystyle \adjugate(\bA)$ & Adjugate of $\bA$ \\
%$\displaystyle \bN_n$ & Standard nilpotent matrix $\in\real^{n\times n}$\\
%$\displaystyle \rho(\bA), \Lambda(\bA), \widehat{\Lambda}(\bA)$ & Spectral radius, spectrum, multispectrum of $\bA$\\
$\displaystyle \lambda_{\min}(\bA),\lambda_{\max}(\bA)$ & Smallest/largest eigenvalues of $\bA$\\
$\displaystyle \sigma_{\min}(\bA),\sigma_{\max}(\bA)$ & Smallest/largest singuar values of $\bA$\\
$\displaystyle d_{\min}(\bA),d_{\max}(\bA)$ & Smallest/largest diagonal values of $\bA$\\
$\displaystyle \bA\geq \bzero, \bA>\bzero$ & Nonnegative/positive matrix $\bA$\\
$\displaystyle \bA\succeq \bzero\, (\bA\in\psd^n), \bA\succ \bzero\, (\bA\in\pd^n)$ & Positive semidefinite/definite matrix $\bA$\\
subdiagonal/superdiagonal & Entries below/above the main diagonal
\end{tabular}
\egroup
\index{Transpose}
\index{Element-wise product|see {Hadamard product}}
\index{Hadamard product}
\index{Determinant}
\end{minipage}

%\vspace{0.4in}
%%\vspace{\notationgap}
%\noindent
%\begin{minipage}{\textwidth}
%\centerline{\bf Datasets and Distributions}
%\bgroup
%\def\arraystretch{1.5}
%\begin{tabular}{cp{4.25in}}
%$\displaystyle \pdata$ & The data generating distribution\\
%$\displaystyle \ptrain$ & The empirical distribution defined by the training set\\
%$\displaystyle \sX$ & A set of training examples\\
%$\displaystyle \vx^{(i)}$ & The $i$-th example (input) from a dataset\\
%$\displaystyle y^{(i)}\text{ or }\vy^{(i)}$ & The target associated with $\vx^{(i)}$ for supervised learning\\
%$\displaystyle \mX$ & The $m \times n$ matrix with input example $\vx^{(i)}$ in row $\mX_{i,:}$\\
%\end{tabular}
%\egroup
%\end{minipage}

\vspace{0.4in}
\noindent
\begin{minipage}{\textwidth}
\centerline{\bf Abbreviations}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{cp{4.25in}}
PD & Positive definite  \\
PSD & Positive semidefinite \\
SC, SS & Strongl convexity, Strong smoothness\\
RSC, RSS & Restricted strong convexity, Restricted strong smoothness\\
KKT & Karush-Kuhn-Tucker conditions\\
%CPQR & Column-pivoted QR decomposition \\
%CGS & Classical Gram-Schimdt process\\
%MGS & Modified Gram-Schmidt process\\
%CR & Column-row decomposition \\
%REF & Row echelon form\\
%RREF & Reduced row echelon form\\
%ID & Interpolative decomposition\\
%IID & Intervened interpolative decomposition \\
%BID & Bayesian interpolative decomposition \\
%MCMC & Markov chain Monte Carlo \\
%i.i.d. & Independently and identically distributed \\
%p.d.f. & Probability density function \\
%p.m.f. & Probability mass function \\
SVD &  Singular value decomposition \\
PCA & Principal component analysis \\
OLS & Ordinary least squares\\
%NG & Normal-Gamma distribution \\
%NIG & Normal-inverse-Gamma  distribution \\
%NIX & Normal-inverse-Chi-squared distribution\\
%TN & Truncated-normal distribution \\
%GTN & General-truncated-normal distribution\\
%RN & Rectified-normal distribution \\
%IW & Inverse-Wishart distribution \\
%NIW & Normal-inverse-Wishart distribution \\
LS & Least squares \\
GD & Gradient descent  method\\
PGD & Projected (sub)gradient descent  method\\
CG & Conditional gradient method \\
GCG & Generalized conditional gradient method \\
FISTA & Fast Proximal Gradient method\\
SGD & Stochastic gradient descent \\
LM & Levenberg-Marquardt method\\
%MU & Multiplicative update \\
%MSE & Mean squared error\\
%NMF & Nonnegative matrix factorization \\
ADMM & Alternating direction methods of multipliers\\
ADPMM & Alternating direction proximal methods of multipliers\\
IHT & Iterative hard-thresholding method \\
LASSO & Least absolute selection and shrinkage operator\\
%HOSVD & High-order SVD \\
%CP & Canonical Polyadic Decomposition\\
%TT & Tensor-train decomposition\\

\end{tabular}
\egroup
\end{minipage}


\clearpage

%\typeout{END_CHAPTER "notation" \theabspage}
