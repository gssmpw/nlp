\section{RELATED WORKS}
\noindent \textbf{Complex Reasoning over Knowledge Graph.} Complex Reasoning over Knowledge graph aims to provide answers to multi-hop natural language questions using knowledge graphs as their primary source of information~\cite{beamqa,chatkbqa,kgandllm}. Existing methods can be broadly categorized into semantic-parsing and retrieval-augmented methods. Semantic-parsing methods parse questions into the executable formal language (e.g., SPARQL) and perform precise queries on KGs to obtain answers~\cite{chatkbqa,spareser2}. Initial works~\cite{step,step2} utilize strategies of step-wise query graph generation and search for parsing. Subsequent works~\cite{beamqa} employ Seq2Seq models (e.g., T5~\cite{T5}) to generate SARSQL-expressions directly, which take advantage of the ability of pre-trained language models to enhance the semantic parsing process. More recently, ChatKBQA~\cite{chatkbqa} further fine-tunes large language models (e.g., LLaMA~\cite{llama}) to improve the accuracy of formal language generation. Despite these advancements, semantic-parsing methods heavily rely on the quality of generated queries, and no answers can be obtained if the query is not executable.

Retrieval-augmented methods~\cite{UniKGQA,Structgpt,Rog} retrieve the relevant factual triples from the KG and then feed them to the LLM to help generate the final answers. Some methods~\cite{Structgpt} develop specialized interfaces for gathering pertinent evidence from structured data, while others~\cite{UniKGQA,retrieval} retrieve facts by assessing semantic similarities between the question and associated facts. Meanwhile, certain approaches~\cite{chatrule,decom} utilize the LLM to decompose the question and then retrieve corresponding triples for generation, enhancing the precision of the retrieval process. Notably, ToG~\cite{ToG} adopts an explore-and-exploit strategy, allowing the LLM to traverse the KG for information gathering, achieving state-of-the-art performance. GoG~\cite{gog} further proposes the think-search-generate paradigm to address the incompleteness issue of KG. However, most of these approaches rely on capable closed-source LLM APIs (e.g., GPT4~\cite{gpt4}), resulting in significant performance degradation when using weak LLMs as backbones. 

% Furthermore, they all assume that KGs comprehensively contain the answers, overlooking the issue of KG incompleteness in real-world scenarios.


%With the surprising long-horizon planning and reasoning capabilities shown in LLMs~\cite{}, researchers have explored building LLM-based agent systems~\cite{} to unlock the door of Artificial General Intelligence. The most representative LLM agent, ReAct~\cite{}, proposes a prompting method to enable LLMs to interact with external environments and receive feedback. Subsequent works further focus on agent planning~\cite{}, function call~\cite{}, and code generation~\cite{}, improving the ability of LLMs on various complicated tasks. Recently, there has been an increasing focus on endowing open-source LLMs with agent capabilities through fine-tuning~\cite{} on expert data distilled from teacher models. However, these approaches heavily rely on prompts for customization, which makes it difficult to tailor the behavior. In this paper, we introduce a self-learning framework, enabling weak LLMs to iteratively improve by interacting with the environment.
\noindent\textbf{Large Language Model based Agents.}
With the surprising long-horizon planning and reasoning capabilities shown in LLMs \cite{plan}, researchers have explored building LLM-based agent systems \cite{Aglite} to unlock the door of Artificial General Intelligence. The most representative LLM agent, ReAct~\cite{react}, proposes a prompting method to enable LLMs to interact with external environments and receive feedback. Subsequent works further focus on agent planning~\cite{reflection}, function call~\cite{toolformer}, and code generation~\cite{pot}, improving the ability of LLMs on various complicated tasks. Recently, there has been an increasing focus on endowing open-source LLMs with agent capabilities through fine-tuning~\cite{ditill} on expert data distilled from teacher models. However, methods like AutoAct~\cite{autoact} and AgentGym~\cite{agentgym} propose self-interactive trajectory synthesis, demonstrating superior performance over distillation and showcasing significant potential. Furthermore, recent research emphasizes the significance of incorporating reinforcement learning techniques with LLMs to enhance decision-making in dynamic scenarios. Notably, studies like~\cite{rl} highlight how RL frameworks can enable LLMs to continuously adapt their strategies with meticulously designed prompts, thus significantly improving their performance in practical applications. 

% However, these approaches heavily rely on prompts for customization, which makes it difficult to tailor the behavior. In this paper, we introduce a self-learning framework, enabling weak LLMs to improve iteratively by interacting with the environment.

%引文[cite1]：
% @inproceedings{du2023guiding,
%   title={Guiding pretraining in reinforcement learning with large language models},
%   author={Du, Yuqing and Watkins, Olivia and Wang, Zihan and Colas, C{\'e}dric and Darrell, Trevor and Abbeel, Pieter and Gupta, Abhishek and Andreas, Jacob},
%   booktitle={International Conference on Machine Learning},
%   pages={8657--8677},
%   year={2023},
%   organization={PMLR}
% }





% struggle to deliver precise and reliable factual knowledge in responsese (i.e., hallucination issues)~\cite{}. 



 % kg subgraph contian many error information, we can selectively utilize these information. learning from the kg and perform retrieval. directy rerieval informaiton for complex reasoning is not good.

 \begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{symagent_framework.pdf}
    \caption{The overview of our proposed SymAgent. (a) the planner in SymAgent, which derives the symbolic rules from the KG to guide the reasoning; (b) the executor in SymAgent, which conducts the automatic action invocation to obtain the answer; (c) the self-learning framework to enhance the agent iteratively; and (d) an example of the synthesized action invoking trajectory.}
    \label{framework}
\end{figure*}