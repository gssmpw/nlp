\section{RELATED WORKS}
\noindent \textbf{Complex Reasoning over Knowledge Graph.} Complex Reasoning over Knowledge graph aims to provide answers to multi-hop natural language questions using knowledge graphs as their primary source of information____. Existing methods can be broadly categorized into semantic-parsing and retrieval-augmented methods. Semantic-parsing methods parse questions into the executable formal language (e.g., SPARQL) and perform precise queries on KGs to obtain answers____. Initial works____ utilize strategies of step-wise query graph generation and search for parsing. Subsequent works____ employ Seq2Seq models (e.g., T5____) to generate SARSQL-expressions directly, which take advantage of the ability of pre-trained language models to enhance the semantic parsing process. More recently, ChatKBQA____ further fine-tunes large language models (e.g., LLaMA____) to improve the accuracy of formal language generation. Despite these advancements, semantic-parsing methods heavily rely on the quality of generated queries, and no answers can be obtained if the query is not executable.

Retrieval-augmented methods____ retrieve the relevant factual triples from the KG and then feed them to the LLM to help generate the final answers. Some methods____ develop specialized interfaces for gathering pertinent evidence from structured data, while others____ retrieve facts by assessing semantic similarities between the question and associated facts. Meanwhile, certain approaches____ utilize the LLM to decompose the question and then retrieve corresponding triples for generation, enhancing the precision of the retrieval process. Notably, ToG____ adopts an explore-and-exploit strategy, allowing the LLM to traverse the KG for information gathering, achieving state-of-the-art performance. GoG____ further proposes the think-search-generate paradigm to address the incompleteness issue of KG. However, most of these approaches rely on capable closed-source LLM APIs (e.g., GPT4____), resulting in significant performance degradation when using weak LLMs as backbones. 

% Furthermore, they all assume that KGs comprehensively contain the answers, overlooking the issue of KG incompleteness in real-world scenarios.


%With the surprising long-horizon planning and reasoning capabilities shown in LLMs____, researchers have explored building LLM-based agent systems____ to unlock the door of Artificial General Intelligence. The most representative LLM agent, ReAct____, proposes a prompting method to enable LLMs to interact with external environments and receive feedback. Subsequent works further focus on agent planning____, function call____, and code generation____, improving the ability of LLMs on various complicated tasks. Recently, there has been an increasing focus on endowing open-source LLMs with agent capabilities through fine-tuning____ on expert data distilled from teacher models. However, these approaches heavily rely on prompts for customization, which makes it difficult to tailor the behavior. In this paper, we introduce a self-learning framework, enabling weak LLMs to iteratively improve by interacting with the environment.
\noindent\textbf{Large Language Model based Agents.}
With the surprising long-horizon planning and reasoning capabilities shown in LLMs ____, researchers have explored building LLM-based agent systems ____ to unlock the door of Artificial General Intelligence. The most representative LLM agent, ReAct____, proposes a prompting method to enable LLMs to interact with external environments and receive feedback. Subsequent works further focus on agent planning____, function call____, and code generation____, improving the ability of LLMs on various complicated tasks. Recently, there has been an increasing focus on endowing open-source LLMs with agent capabilities through fine-tuning____ on expert data distilled from teacher models. However, methods like AutoAct____ and AgentGym____ propose self-interactive trajectory synthesis, demonstrating superior performance over distillation and showcasing significant potential. Furthermore, recent research emphasizes the significance of incorporating reinforcement learning techniques with LLMs to enhance decision-making in dynamic scenarios. Notably, studies like____ highlight how RL frameworks can enable LLMs to continuously adapt their strategies with meticulously designed prompts, thus significantly improving their performance in practical applications. 

% However, these approaches heavily rely on prompts for customization, which makes it difficult to tailor the behavior. In this paper, we introduce a self-learning framework, enabling weak LLMs to improve iteratively by interacting with the environment.

%引文[cite1]：
% @inproceedings{du2023guiding,
%   title={Guiding pretraining in reinforcement learning with large language models},
%   author={Du, Yuqing and Watkins, Olivia and Wang, Zihan and Colas, C{\'e}dric and Darrell, Trevor and Abbeel, Pieter and Gupta, Abhishek and Andreas, Jacob},
%   booktitle={International Conference on Machine Learning},
%   pages={8657--8677},
%   year={2023},
%   organization={PMLR}
% }





% struggle to deliver precise and reliable factual knowledge in responsese (i.e., hallucination issues)____. 



 % kg subgraph contian many error information, we can selectively utilize these information. learning from the kg and perform retrieval. directy rerieval informaiton for complex reasoning is not good.

 \begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{symagent_framework.pdf}
    \caption{The overview of our proposed SymAgent. (a) the planner in SymAgent, which derives the symbolic rules from the KG to guide the reasoning; (b) the executor in SymAgent, which conducts the automatic action invocation to obtain the answer; (c) the self-learning framework to enhance the agent iteratively; and (d) an example of the synthesized action invoking trajectory.}
    \label{framework}
\end{figure*}