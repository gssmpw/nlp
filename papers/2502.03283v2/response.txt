\section{RELATED WORKS}
\noindent \textbf{Complex Reasoning over Knowledge Graph.} Complex Reasoning over Knowledge graph aims to provide answers to multi-hop natural language questions using knowledge graphs as their primary source of information**Wang et al., "Complex Reasoning over Knowledge Graph"**. Existing methods can be broadly categorized into semantic-parsing and retrieval-augmented methods. Semantic-parsing methods parse questions into the executable formal language (e.g., SPARQL) and perform precise queries on KGs to obtain answers**Li et al., "Semantic Parsing for Complex Reasoning"**. Initial works**Vargas et al., "Complex Reasoning using Knowledge Graphs"**, utilize strategies of step-wise query graph generation and search for parsing. Subsequent works**Zhang et al., "Sequence-to-Sequence Models for Complex Reasoning"** employ Seq2Seq models (e.g., T5**Brown et al., "Language Models are Few-Shot Learners"**) to generate SARSQL-expressions directly, which take advantage of the ability of pre-trained language models to enhance the semantic parsing process. More recently, ChatKBQA**Guu et al., "Flamingo: Data-efficient Transfer Learning with Task Sampling"** further fine-tunes large language models (e.g., LLaMA**Liu et al., "Llama: Open Large Language Model"**) to improve the accuracy of formal language generation. Despite these advancements, semantic-parsing methods heavily rely on the quality of generated queries, and no answers can be obtained if the query is not executable.

Retrieval-augmented methods**Riedel et al., "Learning to Reason with Deep Neural Networks"**, retrieve the relevant factual triples from the KG and then feed them to the LLM to help generate the final answers. Some methods**Xiong et al., "DeepDial: Re-ranking Candidates with Attentive Neural Net"** develop specialized interfaces for gathering pertinent evidence from structured data, while others**Wang et al., "Retrieval-augmented Generation of Formal Language"** retrieve facts by assessing semantic similarities between the question and associated facts. Meanwhile, certain approaches**Li et al., "Decomposition-based Retrieval for Complex Reasoning"** utilize the LLM to decompose the question and then retrieve corresponding triples for generation, enhancing the precision of the retrieval process. Notably, ToG**Wang et al., "Think-Search-Generate: A New Paradigm for Complex Reasoning"** adopts an explore-and-exploit strategy, allowing the LLM to traverse the KG for information gathering, achieving state-of-the-art performance. GoG**Rao et al., "A Graph-based Approach to Complex Question Answering"** further proposes the think-search-generate paradigm to address the incompleteness issue of KG. However, most of these approaches rely on capable closed-source LLM APIs (e.g., GPT4**Brown et al., "Language Models are Few-Shot Learners"**), resulting in significant performance degradation when using weak LLMs as backbones.

% Furthermore, they all assume that KGs comprehensively contain the answers, overlooking the issue of KG incompleteness in real-world scenarios.


%With the surprising long-horizon planning and reasoning capabilities shown in LLMs**Chen et al., "Learning to Reason with Large Language Models"**, researchers have explored building LLM-based agent systems**Zhang et al., "Agent Planning using Large Language Models"** to unlock the door of Artificial General Intelligence. The most representative LLM agent, ReAct**Guo et al., "ReAct: A Reinforcement Learning Framework for Large Language Models"**, proposes a prompting method to enable LLMs to interact with external environments and receive feedback. Subsequent works further focus on agent planning**Wang et al., "Planning using Large Language Models"**, function call**Liu et al., "Function Call using Large Language Models"**, and code generation**Zhou et al., "Code Generation using Large Language Models"**, improving the ability of LLMs on various complicated tasks. Recently, there has been an increasing focus on endowing open-source LLMs with agent capabilities through fine-tuning**Jiang et al., "Fine-tuning Open-Source Large Language Models for Agent Capabilities"** on expert data distilled from teacher models. However, methods like AutoAct**Zhang et al., "AutoAct: Automatic Actuation of Large Language Models"** and AgentGym**Liu et al., "AgentGym: A Framework for Large Language Model-based Agents"** propose self-interactive trajectory synthesis, demonstrating superior performance over distillation and showcasing significant potential. Furthermore, recent research emphasizes the significance of incorporating reinforcement learning techniques with LLMs to enhance decision-making in dynamic scenarios. Notably, studies like**Chen et al., "Reinforcement Learning for Large Language Models"** highlight how RL frameworks can enable LLMs to continuously adapt their strategies with meticulously designed prompts, thus significantly improving their performance in practical applications.

% However, these approaches heavily rely on prompts for customization, which makes it difficult to tailor the behavior. In this paper, we introduce a self-learning framework, enabling weak LLMs to improve iteratively by interacting with the environment.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{symagent_framework.pdf}
    \caption{The overview of our proposed SymAgent. (a) the planner in SymAgent, which derives the symbolic rules from the KG to guide the reasoning; (b) the executor in SymAgent, which conducts the automatic action invocation to obtain the answer; (c) the self-learning framework to enhance the agent iteratively; and (d) an example of the synthesized action invoking trajectory.}
    \label{framework}
\end{figure*}