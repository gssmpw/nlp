\section{Related Works}
Papers that relate to our work are briefly reviewed below.

\textbf{Continuous-time RL}. ____ models the noise or randomness in the environment dynamics as following an SDE, and incorporates an entropy-based regularizer into the objective function to facilitate the exploration-exploitation tradeoff. Follow-up works include designing model-free methods and algorithms under either finite horizon ____ or infinite horizon ____. 
% along with applications to portfolio optimization 
% ____.

\textbf{RL for fine-tuning T2I diffusion models}. DDPO ____ and DPOK ____ both discrete the time steps and fine-tune large pretrained T2I diffusion models through the reinforcement learning algorithms. Moreover, ____ introduces DPPO, a policy gradient-based RL framework for fine-tuning diffusion-based policies in continuous control and robotic tasks.

\textbf{Other Preference Optimizations for diffusion models}. ____ proposes an adaptation of Direct Preference Optimization (DPO) for aligning T2I diffusion models like Stable Diffusion XL to human preferences. 
% In a similar direction, ____ proposes to enhance text-to-audio generation by fine-tuning the Tango latent diffusion model using DPO, leverage Audio-Alpaca. D3PO ____ formulates the denoising process as a multi-step Markov Decision Process (MDP) and extend DPO in such a setting. 
____ proposes a novel fine-tuning method for diffusion models that iteratively improves model performance through self-play, where a model competes with its previous versions to enhance human preference alignment and visual appeal. 
See Section 4.5 in ____ for a review.
% ____ proposes to align T2I diffusion models with human preferences using per-sample binary feedback (likes/dislikes) instead of pairwise comparisons or reward models, enabling large-scale preference learning.

\textbf{Stochastic Control}. ____, which also formulated the diffusion models alignment as a continuous-time stochastic control problem with a different parameterization of the control; ____ also provides a more rigorous review and discussion. ____ proposes to use adjoint to solve a similar control problem. 
In a concurrent work to ours,  
____ uses $q$-learning ____ for inferring the score of diffusion models 
(instead of fine tuning a pretrained model),
which relies on an earlier version ____ of this paper.

The rest of the paper is organized as follows. In Section \ref{sc2}, we review the preliminaries of continuous-time RL and score-based diffusion models. Section \ref{sc3} presents our continuous-time  framework for fine-tuning diffusion models using RLHF, with the theory and algorithm for policy optimization detailed in Section \ref{sc4}, and the effectiveness of the algorithm illustrated in Section \ref{sc5}. Concluding remarks and discussions are presented in Section \ref{sc6}.