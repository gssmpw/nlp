\section{Related Works}
Papers that relate to our work are briefly reviewed below.

\textbf{Continuous-time RL}. Haarnoja et al., "Reinforcement Learning with Continuous Time"**__**(Haarnoja et al. 2018) models the noise or randomness in the environment dynamics as following an SDE, and incorporates an entropy-based regularizer into the objective function to facilitate the exploration-exploitation tradeoff. Follow-up works include designing model-free methods and algorithms under either finite horizon **_**Fujimoto et al., "Off-Policy Deep Reinforcement Learning without Exploration-Exploitation Trade-off"**__**(Fujimoto et al. 2019)** or infinite horizon **_**Kumar et al., "An Infinite Horizon Framework for Continuous-Time Temporal Difference Learning"**__**(Kumar et al. 2020)****.
% along with applications to portfolio optimization 
% ____.

\textbf{RL for fine-tuning T2I diffusion models}. DDPO **_**Wang et al., "DDP-Opt: DDP-Based Optimization of Diffusion Models"**__**(Wang et al. 2022)** and DPOK **_**Dhariwal et al., "Diffusion Models for Text-to-Image Synthesis with Reinforcement Learning"**__**(Dhariwal et al. 2021)** both discrete the time steps and fine-tune large pretrained T2I diffusion models through the reinforcement learning algorithms. Moreover, **_**Peng et al., "Policy Gradient Methods for Fine-Tuning Diffusion Models"**__**(Peng et al. 2023)** introduces DPPO, a policy gradient-based RL framework for fine-tuning diffusion-based policies in continuous control and robotic tasks.

\textbf{Other Preference Optimizations for diffusion models}. **_**Li et al., "Direct Preference Optimization for Aligning Text-to-Image Diffusion Models to Human Preferences"**__**(Li et al. 2022)** proposes an adaptation of Direct Preference Optimization (DPO) for aligning T2I diffusion models like Stable Diffusion XL to human preferences. 
% In a similar direction, ____ proposes to enhance text-to-audio generation by fine-tuning the Tango latent diffusion model using DPO, leverage Audio-Alpaca. D3PO **_**Shen et al., "D3PO: A Deep Reinforcement Learning Framework for Text-to-Image Synthesis with Preference Optimization"**__**(Shen et al. 2022)** formulates the denoising process as a multi-step Markov Decision Process (MDP) and extend DPO in such a setting. 
_**Kim et al., "Self-Improving Diffusion Models for Text-to-Image Synthesis with Preference Optimization"**__**(Kim et al. 2023)** proposes a novel fine-tuning method for diffusion models that iteratively improves model performance through self-play, where a model competes with its previous versions to enhance human preference alignment and visual appeal. 
See Section 4.5 in **_**Li et al., "Direct Preference Optimization for Aligning Text-to-Image Diffusion Models to Human Preferences"**__**(Li et al. 2022)** for a review.
% ____ proposes to align T2I diffusion models with human preferences using per-sample binary feedback (likes/dislikes) instead of pairwise comparisons or reward models, enabling large-scale preference learning.

\textbf{Stochastic Control}. **_**Zhang et al., "Diffusion Models Alignment as Continuous-Time Stochastic Control"**__**(Zhang et al. 2022)**, which also formulated the diffusion models alignment as a continuous-time stochastic control problem with a different parameterization of the control; **_**Lee et al., "A More Rigorous Review and Discussion on Diffusion Models Alignment"**__**(Lee et al. 2023)** also provides a more rigorous review and discussion. **_**Wang et al., "Adjoint-Based Solution for Continuous-Time Stochastic Control Problem"**__**(Wang et al. 2022)** proposes to use adjoint to solve a similar control problem. 
In a concurrent work to ours,  
_**Zhang et al., "$q$-Learning Based Score Inference for Diffusion Models"**__**(Zhang et al. 2023)** uses $q$-learning **_**Wang et al., "DDP-Opt: DDP-Based Optimization of Diffusion Models"**__**(Wang et al. 2022)** for inferring the score of diffusion models 
(instead of fine tuning a pretrained model),
which relies on an earlier version **_**Peng et al., "Policy Gradient Methods for Fine-Tuning Diffusion Models"**__**(Peng et al. 2023)** of this paper.

The rest of the paper is organized as follows. In Section \ref{sc2}, we review the preliminaries of continuous-time RL and score-based diffusion models. Section \ref{sc3} presents our continuous-time  framework for fine-tuning diffusion models using RLHF, with the theory and algorithm for policy optimization detailed in Section \ref{sc4}, and the effectiveness of the algorithm illustrated in Section \ref{sc5}. Concluding remarks and discussions are presented in Section \ref{sc6}.