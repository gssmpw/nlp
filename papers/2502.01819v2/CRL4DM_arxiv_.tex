%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{arydshln}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{wrapfig}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{xcolor}  
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}

\usepackage{scalerel}[2016/12/29]
\usepackage[textsize=tiny]{todonotes}

\newcommand\sla{\scaleobj{0.8}{\leftarrow}}
\newcommand{\mycomment}[1]{}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Score as Action: Fine-tuning Diffusion Models by Continuous-time Reinforcement Learning}

\begin{document}

\twocolumn[
\icmltitle{Score as Action: Fine-Tuning Diffusion Generative Models \\ by Continuous-time Reinforcement Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}

\icmlauthor{Hanyang Zhao}{CU}
\icmlauthor{Haoxian Chen}{CU}
\icmlauthor{Ji Zhang}{StonyBrook}
\icmlauthor{David D. Yao}{CU}
\icmlauthor{Wenpin Tang}{CU}

\end{icmlauthorlist}

\icmlaffiliation{CU}{Department of IEOR, Columbia University, New York, USA}
\icmlaffiliation{StonyBrook}{Department of CS, Stony Brook University, New York, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{}{hz2684, yao, wt2319@columbia.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Reinforcement learning from human feedback (RLHF), which aligns a diffusion model with input prompt, has become a crucial step in building reliable generative AI models. Most works in this area use a {\it discrete-time} formulation, which is prone to induced errors, and often not applicable to models with higher-order/black-box solvers. 
The objective of this study is to develop a disciplined approach to fine-tune diffusion models using {\it continuous-time} RL, formulated as a stochastic control problem with a reward function that aligns the end result (terminal state) with input prompt. The key idea is to treat score matching as controls or actions, and thereby making connections to policy optimization and regularization in continuous-time RL. To carry out this idea, we lay out a new policy optimization framework for continuous-time RL, and illustrate its potential in enhancing the value networks design space via leveraging the structural property of diffusion models. We validate the advantages of our method by experiments in downstream tasks of fine-tuning large-scale Text2Image models of Stable Diffusion v1.5. 

\end{abstract}

\section{Introduction} 
Diffusion models \cite{sohl2015deep}, with the capacity to turn a noisy/non-informative initial distribution into a desired target distribution through a well-designed denoising process \cite{Ho20DDPM,DDIM,Song20SGMbySDE}, have recently 
% and have shown the remarkable capability to capture intricate, high-dimensional distributions make them the leading framework for 
found applications in diverse areas such as high-quality and creative image generation
% both unconditionally \cite{dhariwal2021diffusion} or conditionally given additional text prompts 
\cite{DALLE2,DALLE3,Imagen,StableDiffusion}, 
% they are also rapidly finding use in other domains such as 
video synthesis \cite{ho2022imagen-video}, and drug design \cite{xu2022geodiff}.
% and continuous controls \cite{janner2022planning,wang2022diffusion}. However, existing models still have limited abilities for needs like multiple objective compositions \cite{feng2022training,gokhale2022benchmarking}, specific color and counts \cite{lee2023aligning}, and they may also suffer from sources of bias or fairness concern \cite{luccioni2023stable} and may fail to produce reliable visual text, even being distorted. As such, there is great interest in improving them further in terms of either generated distribution quality, or controllability. Specifically, 
And, the emergence of human-interactive platforms like ChatGPT \cite{ouyang2022training} and Stable Diffusion \cite{StableDiffusion} has further increased the demand for diffusion models to align with human preference or feedback.
% which seems to be infeasible for the current training process only targeted at maximizing the likelihood.

To meet such demands, \cite{hao2022optimizing} proposed a natural way to fine-tune diffusion models using reinforcement learning (RL, \cite{sutton2018reinforcement}). Indeed,  RL has already demonstrated empirical successes in enhancing the performance of LLM (large language models) using human feedback \cite{ christiano2017deep, ouyang2022training,bubeck2023sparks}, and \cite{fan2023optimizing} is among the first to utilize RL-like methods to train diffusion models for better image synthesis. Moreover, \cite{lee2023aligning,DPOK,DDPO} have improved the text-to-image (T2I) diffusion model performance by incorporating reward models to align with human preference (e.g., CLIP \cite{CLIP}, BLIP \cite{BLIP}, ImageReward \cite{ImageReward}).

%The RL method not only adapt to non-differentiable reward models, and are also shown in\cite{DDPO} to outperform benchmark supervised fine-tuning methods, like Reward Weighted Regression (RWR) \cite{lee2023aligning}, either with regularization or not \cite{DPOK}. 

Notably, all studies referenced above that combine diffusion models with RL are formulated as {\it discrete-time} sequential optimization problems, such as Markov decision processes (MDPs, \cite{puterman2014markov}), and solved by discrete-time RL algorithms such as REINFORCE \cite{sutton1999policy} or PPO \cite{schulman2017proximal}. 

Yet, diffusion models are intrinsically {\it continuous-time} as they were originally created to model the evolution of  thermodynamics \cite{sohl2015deep}.
% the  \cite{Song20SGMbySDE}.  as they are inspired by non-equilibrium thermodynamics \cite{sohl2015deep}. 
Notably, the continuous-time formalism of diffusion models provides a unified framework for various existing discrete-time algorithms as shown in \cite{Song20SGMbySDE}: the denoising steps in DDPM \cite{Ho20DDPM} can be viewed as a discrete approximation of a stochastic differential equation (SDE) and are implicitly {\it score-based} under a specific variance-preserving SDE \cite{Song20SGMbySDE}; and DDIM \cite{DDIM}, which underlies the success of Stable Diffusion \cite{StableDiffusion}, can also be seen as a numerical integrator of an ODE (ordinary differential equation) sampler \cite{salimans2022progressive}. 
Awareness of the continuous-time nature informs the design structure of the discrete-time SOTA large-scale T2I generative models (e.g.,\cite{dhariwal2021diffusion,StableDiffusion,StableDiffusionv3}), and enables simple controllable generations by classifier guidance to solve inverse problems \cite{Song20SGMbySDE,song2021solving}. 
It also motivates more efficient diffusion models with continuous-time samplers, including the ODE-governed probability (normalizing) flows \cite{papamakarios2021normalizing,Song20SGMbySDE} and rectified flows \cite{RectifiedFlow,InstaFlow} underpinning Stable Diffusion v3 \cite{StableDiffusionv3}. A discrete-time formulation of RL algorithms for fine-tuning diffusion models, if/when directly applied to continuous-time diffusion models via discretization, can {\it nullify} the models' continuous nature and fail to capture or utilize their structural properties. 

For fine-tuning diffusion models, discrete-time RL algorithms (such as DDPO) require a prior chosen time discretization in sampling. 
We thus examine the robustness of a fine-tuned model to the inference time discretization, and observe an ``overfitting" phenomenon as illustrated in Figure \ref{Fig: Discrete RL overfits when fine-tuning SD v1.4}. Specifically, improvements observed during inference at alternative discretization timesteps (25 and 100) are significantly smaller than that of sampling timestep (50) in RL. 
\vspace{-2pt}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/discrete_RL_value_difference.png}
    \caption{Reward curve of model checkpoints sampling under different timesteps (25, 50, 100): After training Stable Diffusion v1.4 for a fixed prompt with 60 training steps by DDPO \cite{DDPO} with 50 discretization steps, the average reward of images generated by the checkpoints obtained (under 50 discretization steps) evaluated by ImageReward \cite{ImageReward} increases by 0.046, while the average reward of images generated with 100 discretization steps only increases by less than 0.016.}
    \label{Fig: Discrete RL overfits when fine-tuning SD v1.4}
\end{figure}
\vspace{-10pt}

In addition, 
%when discrete-time diffusion models adopt black-box, the discrete-time formulations are also not applicable;  
for high-order solvers (such as 2$^{\text{nd}}$ order Heun in EDM \cite{karras2022elucidating}), discrete-time RL methods will require solving a high-dimension root-finding problem for each inference step, which is inefficient in practice.

\textbf{Main contributions.} 
To address the above issues, we develop a unified continuous-time RL framework to fine-tune score-based diffusion models. 

Our first contribution is a continuous-time RL framework for fine-tuning diffusion models by treating {\it score functions as actions}. This framework naturally accommodates discrete-time diffusion models with any solver as well as continuous-time diffusion models, and overcomes the afore-mentioned limitations of discrete-time RL methods. (See Section \ref{sc3}.)

Second, we illustrate the promise of leveraging the structural property of diffusion models to generate tractable optimization problems and to enhance the design space of value networks. This includes transforming the KL regularization to a tractable running reward over time, and a novel design of value networks that involves ``sample prediction" by sharing parameters with policy networks and fine-tuned diffusion models. Through experiments, we demonstrate the drastic improvements over naive value network designs.

Third, we provide a new theory for RL in continuous-time and space, which leads to the first scalable policy optimization algorithm (for continuous-time RL), along with estimates of policy gradients via generated samples. Compared with existing works in continuous-time PPO, we consider a special case of state-independent diffusion coefficients, which yields sharper bounds and closed-form advantage-rate functions instead of estimations. (See Section \ref{sc4}.)

\subsection{Related Works}
Papers that relate to our work are briefly reviewed below.

\textbf{Continuous-time RL}. \cite{wang2020reinforcement} models the noise or randomness in the environment dynamics as following an SDE, and incorporates an entropy-based regularizer into the objective function to facilitate the exploration-exploitation tradeoff. Follow-up works include designing model-free methods and algorithms under either finite horizon \cite{jia2022policy_evaluation, jia2022policy_gradient, jia2022q_learning} or infinite horizon \cite{zhao2024policy}. 
% along with applications to portfolio optimization 
% \cite{huang2022achieving,dai2023learning}.

\textbf{RL for fine-tuning T2I diffusion models}. DDPO \cite{DDPO} and DPOK \cite{DPOK} both discrete the time steps and fine-tune large pretrained T2I diffusion models through the reinforcement learning algorithms. Moreover, \cite{dppo} introduces DPPO, a policy gradient-based RL framework for fine-tuning diffusion-based policies in continuous control and robotic tasks.

\textbf{Other Preference Optimizations for diffusion models}. \cite{diffusiondpo} proposes an adaptation of Direct Preference Optimization (DPO) for aligning T2I diffusion models like Stable Diffusion XL to human preferences. 
% In a similar direction, \cite{tango2} proposes to enhance text-to-audio generation by fine-tuning the Tango latent diffusion model using DPO, leverage Audio-Alpaca. D3PO \cite{yang2024using} formulates the denoising process as a multi-step Markov Decision Process (MDP) and extend DPO in such a setting. 
\cite{yuan2024self} proposes a novel fine-tuning method for diffusion models that iteratively improves model performance through self-play, where a model competes with its previous versions to enhance human preference alignment and visual appeal. 
See Section 4.5 in \cite{winata2024preference} for a review.
% \cite{li2024aligning} proposes to align T2I diffusion models with human preferences using per-sample binary feedback (likes/dislikes) instead of pairwise comparisons or reward models, enabling large-scale preference learning.

\textbf{Stochastic Control}. \cite{uehara2024continuous-fine-tune}, which also formulated the diffusion models alignment as a continuous-time stochastic control problem with a different parameterization of the control; \cite{Tang24} also provides a more rigorous review and discussion. \cite{domingo2024adjoint} proposes to use adjoint to solve a similar control problem. 
In a concurrent work to ours,  
\cite{gao2024reward} uses $q$-learning \cite{jia2022q_learning} for inferring the score of diffusion models 
(instead of fine tuning a pretrained model),
which relies on an earlier version \cite{zhao2024scores} of this paper.

The rest of the paper is organized as follows. In Section \ref{sc2}, we review the preliminaries of continuous-time RL and score-based diffusion models. Section \ref{sc3} presents our continuous-time  framework for fine-tuning diffusion models using RLHF, with the theory and algorithm for policy optimization detailed in Section \ref{sc4}, and the effectiveness of the algorithm illustrated in Section \ref{sc5}. Concluding remarks and discussions are presented in Section \ref{sc6}.


\section{Preliminaries}
\label{sc2}
\subsection{Continuous-time RL}
\textbf{Diffusion Process.} We consider the state space $\mathbb{R}^d$, and denote by $\mathcal{A}$ the action space. 
Let $\pi(\cdot \mid t,x)$ be a {feedback} policy given $t\in [0,T]$ and $x \in \mathbb{R}^d$. 
The state dynamics $(X^\pi_t, \, 0 \leq t \leq T)$ is governed by the following SDE:
\begin{equation}
\label{SDE_dynamic}
\mathrm{d} X_t^\pi=b\left(t, X_t^\pi, a_t\right) \mathrm{d} t+\sigma(t) \mathrm{d} B_t,\quad X^\pi_0\sim \rho,
\end{equation}
where $(B_t, \, t \ge 0)$ is a $d$-dimensional Brownian motion;
$b: \mathbb{R}_+ \times \mathbb{R}^d \times \mathcal{A} \to \mathbb{R}^d$ and $\sigma: \mathbb{R}_+ \to \mathbb{R}_+$ \footnote{For our applications here we assume that the diffusion coefficient $\sigma(t)$ only depends on time $t$. 
Note, however, that the general continuous-time RL theory also holds for time-, state- and action-dependent $\sigma(t,x,a)$, see \cite{jia2022policy_evaluation,jia2022policy_gradient}.}
are given functions;
the action $a_t$ follows the distribution $\pi\left(\cdot \mid t, X^\pi_t\right)$ by external randomization;
and $\rho$ is the initial  distribution over the state space. 
%Unlike the general state-dependent diffusion coefficient (i.e. instead of $\sigma(t)$, it could be $\sigma(t,x)$) in \cite{jia2022policy_evaluation,jia2022policy_gradient}, our form in \eqref{SDE_dynamic} is motivated by and sufficient for the purpose of diffusion models.

\textbf{Performance Metric}. Our goal is to find the optimal feedback policy $\pi^*$ that maximizes the expected reward over a finite time horizon:
\begin{equation}
\label{Discounted Objective 2}
{V^*: =} \max_{\pi} \mathbb{E}\left[\int_0^{T}r\left(t, X_t^\pi, a_t^\pi\right) \mathrm{d} t +h(X^{\pi}_T)\mid X_0^\pi\sim \rho \right],
\end{equation}
where $r:\mathbb{R}_+ \times \mathbb{R}^d \times \mathcal{A} \to \mathbb{R}$ and $h: \mathbb{R}^d \to \mathbb{R}$ are the running and terminal rewards respectively. 
Given a policy $\pi(\cdot)$, let $\tilde{b}(t,x, \pi(\cdot)):=\int_{\mathcal{A}} b(t,x, a) \pi(a) \mathrm{d} a$.
We consider the following equivalent representation of \eqref{SDE_dynamic}:
\begin{equation}
\label{SDE_Dynamics_exp}
\mathrm{d} \tilde{X}_t = \tilde{b}\left(t,\tilde{X}_t, \pi(\cdot \mid  t,\tilde{X}_t)\right) \mathrm{d} t+\sigma(t) \mathrm{d} \tilde{B}_t, \quad \tilde{X}_0\sim \rho  ,
\end{equation}
in the sense that
there exists a probability measure $\tilde{\mathbb{P}}$ 
that supports a $d$-dimensional Brownian motion $(\tilde{B}_t, \, t \ge 0)$, 
and for each $t \geq 0$, the distribution of $\tilde{X}_t$ under $\tilde{\mathbb{P}}$ agrees with that of $X_t$ under $\mathbb{P}$ defined by \eqref{SDE_dynamic}. 
Note that the dynamics \eqref{SDE_Dynamics_exp} does not require external randomization. Accordingly, set
$\tilde{r}(t,x,\pi):=\int_{\mathcal{A}} r(t,x, a) \pi(a) \mathrm{d} a$.
% and $\tilde{R}(t,x,\pi):=\int_{\mathcal{A}} R(t,x, a,\pi) \pi(a) \mathrm{d} a$.

The value function associated with the feedback policy $\{\pi(\cdot \mid t, x): x \in \mathbb{R}^d\}$ is
% \begin{eqnarray}
% \label{Value function Definition}
% &V( t, x ; \pi):=
% \mathbb{E} \left[\int_t^{T} \left[r\left(s, X_s^\pi, a_s^\pi\right)+\gamma R\left(s, X_s^\pi, a_s^\pi, \pi\left(\cdot \mid X_s^\pi\right)\right)\right] \mathrm{d} s +h\left(X_T^{\pi}\right)\mid X_0^\pi = x \right] \nonumber\\
% &= \mathbb{E} \left[\int_t^{T}  \left[\tilde{r}\left(s,\tilde{X}_s^\pi, \pi(\cdot \mid \tilde{X}_s^\pi)\right)+\gamma \tilde{R}\left(s,\tilde{X}_s^\pi ,\pi(\cdot \mid \tilde{X}_s^\pi)\right)\right] \mathrm{d} s+ h\left(\tilde{X}_T^{\pi}\right)\mid \tilde{X}_0^\pi=x\right].
% \end{eqnarray}
\begin{align}
\label{Value function Definition}
&V( t, x ; \pi):=
\mathbb{E} \left[\int_t^{T} r\left(s, X_s^\pi, a_s^\pi\right) \mathrm{d} s +h\left(X_T^{\pi}\right)\mid X_t^\pi = x \right] \nonumber\\
&= \mathbb{E} \left[\int_t^{T}\tilde{r}(s,\tilde{X}_s^\pi, \pi(\cdot |s,\tilde{X}_s^\pi)) \mathrm{d} s+ h(\tilde{X}_T^{\pi})\mid \tilde{X}_t^\pi=x\right]
\end{align}
The performance metric is $V^\pi := \int_{\mathbb{R}^d} V(0, x; \pi) \rho(dx)$, and 
$V^* := \max_\pi V^\pi$. 
The task is to 
 construct a sequence of (feedback) policies $\pi_k$, $k = 1,2,\ldots$ recursively such that 
the performance metric is non-decreasing in $k$.

\textbf{$q$-Value}.
Following the definition in \cite{jia2022q_learning}, 
given a policy $\pi$ and $(t,x, a) \in [0,\infty)\times\mathbb{R}^n \times \mathcal{A}$, we construct a ``perturbed" policy, denoted by $\hat{\pi}$: It takes the action $a \in \mathcal{A}$ on $[t, t+\Delta t)$, and then follows $\pi$ on $[t+\Delta t, \infty)$. 
Specifically, the corresponding state process $X^{\hat{\pi}}$, given $X_t^{\hat{\pi}}=x$, 
breaks into two pieces: on $[t, t+\Delta t)$, it is $X^a$ following \eqref{SDE_dynamic} with $a_t\equiv a$ 
(i.e., $\pi(t,x,a)=1$);
% which is the solution to
%\begin{equation}
%\mathrm{d} X_s^a=b\left(X_s^a, a\right) \mathrm{d} s+\sigma\left(X_s^a, a\right) \mathrm{d} B_s, s \in[t, t+\Delta t) ; X_t^a=x,
%\end{equation}
while on $[t+\Delta t, \infty)$, it is $X^\pi$ following (\ref{SDE_Dynamics_exp}) 
but with the initial time-state pair $\left(t+\Delta t, X_{t+\Delta t}^a\right)$.
The $q$-value measures the rate of the performance difference between the two policies when $\Delta t\to 0$, and is shown in \cite{jia2022q_learning} to take the following form:
%It will be useful to introduce the $q$-value function (\cite{jia2022q_learning}):
%for a given policy $\pi \in \Pi$ and $(t, x, a) \in[0,T]\times\mathbb{R}^d \times \mathcal{A}$,
\begin{align}
\label{defqvalue}
q(t, x, a ;& \pi)=\frac{\partial V}{\partial t}\left(t, x ; \pi\right)+ \nonumber\\
&\mathcal{H}\left(t, x, a, \frac{\partial V}{\partial x}\left(t,x ; \pi\right), \frac{\partial^2 V}{\partial x^2}\left(t,x ; \pi\right)\right),
\end{align}
%for a given policy $\pi \in \Pi$ and $(t, x, a) \in[0,T]\times\mathbb{R}^d \times \mathcal{A}$, in which 
where $\mathcal{H}(t, x, a, y, A):=b(t, x, a) \cdot y+\frac{1}{2} \sigma^2(t) \sum_{i} A_{ii}+r(t, x, a)$ is the (generalized) Hamilton function in stochastic control theory \cite{yong1999stochastic}. 

\subsection{Score-Based Diffusion Models}

\textbf{Forward and Backward SDE}. 
%\textbf{Diffusion Models}. 
We follow the presentation in \cite{SBDM_tutorial}.
Consider the following SDE that governs the dynamics of a process $(X_t, \, 0 \le t \le T)$ in $\mathbb{R}^d$ \cite{Song20SGMbySDE},
\begin{equation}
\label{eq:SDE}
\mathrm{d}X_t = f(t, X_t) \mathrm{d}t + g(t) \mathrm{d}B_t, \quad X_0 \sim p_{\scalebox{0.7}{data}}(\cdot),
\end{equation}
where $(B_t, \, t \ge 0)$ is a $d$-dimensional Brownian motion, 
%$f: \mathbb{R}_+ \times \mathbb{R}^d \to \mathbb{R}^d$ 
%and $g: \mathbb{R}_+ \to \mathbb{R}_+$
$f: \mathbb{R}_+ \times \mathbb{R}^d \to \mathbb{R}^d$ and $g: \mathbb{R}_+ \to \mathbb{R}_+$ are two given functions (up to the designer to choose), 
and the initial state $X_0$ follows a distribution %(shaped by data) 
with density $p_{\scalebox{0.7}{data}}(\cdot)$, 
which is shaped by data yet unknown {\it a priori}.  
Denote by $p_t(\cdot)$ the probability density of $X_t$.

Run the SDE in \eqref{eq:SDE} until a given time $T>0$,  to obtain $X_T \sim p(T, \cdot)$. 
Next, consider the  ``time reversal'' of $X_t$, 
denoted $X^{\text{rev}}_t$, such that
%for all $t\in [0 , T]$ with a given $T>0$, 
%or more precisely, 
the distribution of 
$X^{\text{rev}}_t$ agrees with that of 
$X_{T-t}$ on $[0,T]$. 
Then, $(X^{\text{rev}}_t, \, 0 \le t \le T)$ satisfies the following SDE under mild conditions on $f$ and $g$:
%(refer to the proof in Appendix \ref{Proof of Backward SDE}):
\begin{align}
\label{eq:timerev_SDE}
\mathrm{d} X^{\text{rev}}_t = &\left(-f(T-t,X^{\text{rev}}_t) + g^2(T-t) \nabla \log p_{T-t} (X^{\text{rev}}_t) \right) \nonumber \\
&\mathrm{d} t + g(T-t) \mathrm{d} B_t,
\end{align}
where $\nabla\log p_t(x)$ is known as {\em Stein's score function}. 
Below we will refer to the two SDE's in \eqref{eq:SDE} and \eqref{eq:timerev_SDE}, respectively, as the forward and the backward SDE.

For sampling from the backward SDE, we replace $p_T( \cdot)$ with some $p_{\scalebox{0.7}{noise}}(\cdot)$ as an approximation. 
The initialization $p_{\scalebox{0.7}{noise}}(\cdot)$ is commonly independent of $p_{\scalebox{0.7}{data}}(\cdot)$, 
which is the reason why diffusion models are known for generating data from ``noise''.

\textbf{Score Matching}. 
Since the score function
$\nabla_x \log p_t(x)$ in \eqref{eq:timerev_SDE} is unknown, 
the idea is to learn the score $s_{\theta_{\text{pre}}}(t,x)\approx \nabla_x \log p_t(x)$, which is often referred to as {\em pretraining}.
It boils down to solving the following denoising score matching (DSM) problem \cite{vincent2011connection} \footnote{There are several existing score matching methods, among which the DSM is the most tractable one because $p_t(\cdot|x_0)$ is accessible for a wide class of diffusion processes; in particular, it is conditionally Gaussian if \eqref{eq:SDE} is a linear SDE \cite{Song20SGMbySDE}.}:
\begin{equation}
\label{eq:DSM objective}
%\theta^*=\arg
\mathcal{J}_{\text{DSM}}(\theta) =\mathbb{E}\left[\lambda(t)\left\|s_{\theta}(t,x_t)-\nabla \log p_t(x_t|x_0)\right\|_2^2\right],
\end{equation}
where $x_t \sim p_t(\cdot|x_0)$ and $\lambda:[0, T] \rightarrow \mathbb{R}_{>0}$ is a chosen positive-valued weight function. 
%The DSM is more tractable in the sense that $p_t(\cdot|x_0)$ is accessible for certain diffusion dynamics; in particular, it is conditionally Gaussian if \eqref{eq:SDE} is a linear SDE as in \cite{Song20SGMbySDE}.

\textbf{Inference Process}. Once the best approximation $s_{\theta_{\text{pre}}}$ is obtained, we use it to replace $\nabla \log p_t(x)$ in \eqref{eq:timerev_SDE}. The corresponding approximation to the reversed process $X^{\text{rev}}_t$, denoted as 
$X^{\sla}_t$, then follows the SDE:
\begin{align}
\label{eq:timerevapprox}
\mathrm{d} X^{\sla}_t = &\left(-f(T-t,X^{\sla}_t) + g^2(T-t) s_{\theta_{\text{pre}}}(T-t, X^{\sla}_t)  \right) \mathrm{d}t \nonumber\\
&+ g(T-t) \mathrm{d}B_t,
\end{align}
with $X^{\sla}_0\sim p_{\scalebox{0.7}{noise}}(\cdot)$. 
At time $t=T$,  the distribution of $X^{\sla}_T$ 
is expected to be close to $p_{\scalebox{0.7}{data}}(\cdot)$. 
The well-known DDPM \cite{Ho20DDPM}
% and DDIM\cite{DDIM}, 
can be viewed as a discretized version of the SDE in \eqref{eq:timerevapprox}.
% and its ODE version, repectively.
%in \eqref{eq:timerevapprox_ode}. 
This has been established in \cite{Song20SGMbySDE,salimans2022progressive,zhang2022fast,zhang2022gddim}; also refer to further discussions in Appendix \ref{app:discrete and continuous sampler connection}. 
Throughout the rest of the paper, we will focus on the continuous formalism (via SDE). 

%
%For the backward process following either \eqref{eq:timerevapprox} or \eqref{eq:timerevapprox_ode}, the approximate score function $s_{\theta^{*}}(T-t, X^{\sla}_t)$ determines the law of the time-reversed process
%(more precisely, its approximation)  %$\operatorname{law}
%$X^{\sla}_t$ for all $t\in(0,T]$. (Even though, for the purpose of generating $p_{\scalebox{0.7}{data}}(\cdot)$, the main object of interest is the distribution of $X^{\sla}_T$.)
%In this regard, $s_{\theta} (t,x_t)$  is reminiscent of the action (or control) taken by an agent in RL (at time $t$ and in state $x_t$), and the derivation of the optimized 
%in the RL/control theory and our formulation are thus motivated to treat this score function/approximation as a {\it control} over the backward process; for task of fine-tuning, 
%$s_{\theta^{*}}$ can be viewed as policy optimization in RL.
%seen as a pretrained control/policy we can both have access and we shall refer to.

\section{Continuous-time RL for Diffusion Models Fine Tuning}
\label{sc3}
Here we formulate the task of fine-tuning diffusion models as a continuous-time stochastic control problem.
The high-level idea it to treat the score function approximation as a control process applied to the backward SDE.

\textbf{Scores as Actions}. 
First, to broaden the application context of the diffusion model,
 we add a parameter $c$ to the score function, 
interpreted as a ``class'' index or label (e.g., for input prompts).
Then, the backward SDE in \eqref{eq:timerevapprox} becomes:
\begin{align}
\label{b-sde}
\mathrm{d} X^{\sla}_t = &\left(-f(T-t,X^{\sla}_t) + g^2(T-t) s_{\theta_{\text{pre}}}(T-t, X^{\sla}_t,c  )\right) \nonumber\\
&\mathrm{d}t + g(T-t) \mathrm{d}B_t.
\end{align}
Next, comparing the continuous RL process in \eqref{SDE_Dynamics_exp} and the inference process \eqref{b-sde}, 
we choose $b$ and $\sigma$ in the RL dynamics in \eqref{SDE_Dynamics_exp} as:
\begin{equation}
\label{drift and diffusion coefficient definition}
\left\{
\begin{array}{ll}
    \sigma(t) := g(T-t), \\[8pt]
    b\left(t, x, a\right) := -f(T-t,x) + g^2(T-t) a.
\end{array}
\right.
\end{equation}
In the sequel, we will stick to this definition of $b$ and $\sigma$.

Define a specific feedback control, $a^{\theta_\text{pre}}_t= s_{\theta_{\text{pre}}}(T-t,  X^{\sla}_t,c)$,
and the backward SDE in (\ref{b-sde}) is expressed as:
\begin{equation}
\label{eq:score as action}
\mathrm{d} X^{\sla}_t = b\left(t, X^{\sla}_t, a^{\theta_\text{pre}}_t\right)\mathrm{d}t + \sigma(t) \mathrm{d}B_t.
\end{equation}
%where $b$ and $\sigma$ are specified in \eqref{drift and diffusion coefficient definition}. 
This way, the score function is replaced by the action (or control/policy), and finding the optimal score becomes a policy optimization problem in RL.
Denote by $p^{\theta_{\text{pre}}}(t,\cdot,c)$ the probability density of $X^{\sla}_t$ in \eqref{eq:score as action}.

\textbf{Exploratory SDEs}. %Lies central in RL is the exploration. 
As we will deal with the time-reversed process $X^{\sla}_t$ exclusively from now on, the superscript $^{\sla}$ will be 
dropped to lighten the notation. 
To enhance exploration, 
%an important feature in RL, 
we will use a Gaussian control: 
\begin{equation}
\label{atht}
a^{\theta}_t \sim \pi^{\theta}(\cdot\mid t,X^{\theta}_t,c) = N(\mu^{\theta}(t,X^{\theta}_t,c),\Sigma_t ).
\end{equation}
Specifically, the dependence on $\theta$ is through that of the mean function $\mu^\theta$, while  
%in which the mean $\mu^{\theta}(t,X_t,c)$ is approximated by some function approximation parameterized by $\theta$ 
the covariance matrix $\Sigma_t$ only depends on time $t$, representing a chosen exploration level at $t$. 
For brevity, write $X^{\theta}_t$ for the (time-reversed) process $X^{\pi^{\theta}}_t$ driven by the policy $\pi^{\theta}$.
Then $(X^{\theta}_t, \, 0 \le t \le T)$ is governed by the SDE:
\begin{align}
\mathrm{d} X_t^{\theta}=&\left[-f(T-t,X_t^{\theta}) + g^2(T-t) \mu^{\theta}(t,X_t^{\theta},c)\right]\mathrm{d}t\nonumber\\
&+g(T-t)\mathrm{d}B_t,\quad X^{\theta}_0\sim\rho.
\end{align}
Denote by $p^{\theta}(t,\cdot,c)$ the probability density of $X_t^{\theta}$.

%\vskip 5 pt 
%We emphasize that using the (conditional) score matching parameterization as the same as the policy parameterization $\mu^{\theta^*}(t,x,c) = s^{\theta^*}(T-t,x,c)$ recovers the backward procedure \eqref{eq:timerevapprox} in the score-based diffusion models, that's why we refer our formulation enables `score function as action'. This perspective is also reminiscent to classifier guidance, as classifier guidance for diffusion models leverage the property of the conditional score as:
%$$\nabla \log p(t,x\mid c) = \nabla \log p(t,c\mid x)+\nabla \log p(t,x),$$
%in order to generate conditionally on an extra label $c$ in our contents. $\nabla \log p(t,x)$ can be seen as an old unconditional control before fine-tuned, and $\nabla \log p(t,x\mid y)$ can be seen as a new control for better conditional generation, and $ \nabla p(T,c\mid x)$ acts as a reward to guide the shift/difference from the old control to the new one. In more general cases when reward model is complicated, it's hard to design structures like the case of classifier guidance, for which $\nabla \log p(t,y\mid x)$ appears as a natural candidate for conditional likelihood maximization $ p(T,y\mid x)$, that's why we use the RL approach.


\textbf{Objective Function}. 
%\textbf{Regularization as Rewards}. 
The objective function of the RL problem consists of two parts. The first part is the terminal reward, i.e., a given
reward model (RM) that is a function of both $X_T$ and $c$. 
%(the generated distribution) such that it can output the reward $\text{RM}(x,c)$ given a generation $x\in\mathbb{R}^d$, which represents the human preference or any target we want to maximize of the current generation $X_T$ given input $c$. 
For instance, if the task is T2I generation, then $\text{RM}(X_T,c)$ represents how well the generated image $X_T$ aligns with the input prompt $c$. The second part is a penalty (i.e., regularization) term, which takes the form of the KL divergence between $p^{\theta}(T,\cdot,c)$ and its pretrained counterpart. 
This is similar in spirit to previous works on fine-tuning diffusion models by discrete-time RL, see e.g., \cite{ouyang2022training,DPOK}.
As for exploration, note that it has been represented by the Gaussian noise in $a^\theta_t$; refer to (\ref{atht}), and more on this below. So, here is the problem we want to solve:
%In addition, we also consider adding regularization: in our cases, there should be two sources of regularization: (i) regularization to encourage exploration as in our continuous RL formulation, like entropy regularization in \cite{wang2020reinforcement} or \cite{jia2022policy_evaluation}; (ii) regularization to prevent the model from overfitting to the reward or catastrophic forgetting, and thus failing to utilize the capabilities of the pre-trained diffusion models parameterized by $\theta_{pre}$. Notice that here since we already used Gaussian Exploration with fixed variance to encourage exploration, we will not use additional regularization of source (i); for source (ii), in the same essence of previous work of tuning diffusion models by discrete-time RL \cite{ouyang2022training,DPOK}, we still target at bounding the KL divergence of the final generation, i.e., our final optimization objective yields:
\begin{equation}
\label{objective with regularization}
\max_\theta \mathbb{E}\left[\text{RM}(c,X^{\theta}_T)-\beta \operatorname{KL}\left(p^{\theta}(T,\cdot,c)\| p^{\theta_{pre}}(T,\cdot,c)\right)\right],
\end{equation}
where $\beta>0$ is a (given) penalty cost.

To connect the problem in \eqref{objective with regularization}
to the objective function of the RL model in \eqref{Discounted Objective 2}, we need the following explicit expression for the KL divergence term in \eqref{objective with regularization}.
%often needed to tune separately. However, instead of adding KL divergence directly to the objective like DPOK\cite{DPOK}, we could `smartly' transform this term into an integration of expected $L^2$ penalty term between the mean of the current Gaussian policy and the pretrained/reference score along the path, thanks to our continuous-time formulation and the following theorem:
\begin{theorem}
\label{thm:Regularization as KL bound}
For any given $c$, the KL divergence between $p^{\theta}$ and $p^{\theta_{pre}}$ is:
\begin{align}
&\operatorname{KL}(p^{\theta}(T,\cdot,c)\|p^{\theta_{pre}}(T,\cdot,c))\nonumber\\
&= \mathbb{E}\int_{0}^{T} \frac{g^2(T-t)}{2}\|\mu^{\theta}(t,X_t^{\theta},c)-\mu^{\theta_{pre}}(t,X_t^{\theta},c)\|^2\mathrm{d}t.
\end{align}
\end{theorem}
{\it Proof Sketch}. The full proof is given in Appendix \ref{Proof of Regularization as KL bound}. 

As a remark, it is important to use the ``reverse''-KL divergence $\operatorname{KL}\left(p^{\theta}(T,\cdot,c)\| p^{\theta_{pre}}(T,\cdot,c)\right)$, because it yields the expectation under the current policy $\pi^{\theta}$ that can be estimated from sample trajectories.
% In addition, 
% Since
% \begin{equation}
% \label{sig}
% \mathbb{E}_{p^{\theta}(t,\cdot,c)}\|a_t^{\theta}-\mu^{\theta_{pre}}(t,X_t,c)\|^2=\mathbb{E}_{p^{\theta}(t,\cdot, c)} \|\mu^{\theta}(t,X_t^{\theta},c)-\mu^{\theta_{pre}}(t,X_t^{\theta},c)\|^2+ \Sigma_t,
% \end{equation}
By Theorem \ref{thm:Regularization as KL bound}, the objective function in \eqref{objective with regularization} is equivalent to the following:
\begin{align}
\label{continuous-time objective with regularization}
\eta^\theta:=&
\mathbb{E} \int_0^{T}\underbrace{-\frac{\beta}{2} g^2(T-t)\|\mu_t^{\theta}-\mu^{\theta_{pre}}_t\|^2}_{r (t,X^\theta_t,a_t^{\theta})} \mathrm{d} t \notag \\
& \quad +\mathbb{E}\underbrace{\text{RM}(X^{\theta}_T,c)}_{h(X^{\theta}_T,c)},
\end{align}
where we abbreviate $\mu^{\theta}(t,X_t^{\theta},c)$ and $\mu^{\theta_{pre}}(t,X_t^{\theta},c)$ by $\mu_t^{\theta}$ and $\mu^{\theta_{pre}}_t$ respectively.
%the two terms $r$ and $h$ map to the same terms in \eqref{Discounted Objective 2}. 
Thus, maximizing the objective function in \eqref{objective with regularization} aligns with the RL model formulated in \eqref{Discounted Objective 2}.
We can also define the corresponding value function as:
\begin{align}
\label{continuous-time value function with regularization}
V^\theta(t,x;c)=& 
\mathbb{E} \bigg[\int_t^{T}-\frac{\beta}{2} g^2(T-t)\|\mu_t^{\theta}-\mu^{\theta_{pre}}_t\|^2\mathrm{d} t  \nonumber\\
&\quad +\text{RM}(X^{\theta}_T,c)\mid X^{\theta}_t=x\bigg] , 
\end{align}

\textbf{Value Network Design}. 
We also adopt a function approximation to learn the value function (i.e., the critic). 
For the value function $V^{\theta}(t,x;c)$ associated with policy $\pi^{\theta}$, 
there is the boundary condition:
\begin{equation}
\label{eq:bdry}
V^\theta(T,x;c)=\mathbb{E} \left[\text{RM}(X^{\theta}_T,c)\mid X^{\theta}_T=x\right] = \text{RM}(x,c). 
\end{equation}
To meet this condition, we propose the following parametrization that leverages the structural property of diffusion models:
\begin{align}
V^\theta&(t,x;c)\approx \mathcal{V}^{\theta}_{\phi}(t,x;c) :=\nonumber\\ 
&\underbrace{c_{\text{skip}}(t)\cdot \text{RM}(\hat{x}_{\theta}(t,x,c))}_{\text{reward mean predictor}} + \underbrace{c_{\text{out}}(t)\cdot F_{\phi}(t,x, c)}_{\text{residual term corrector}},
\end{align}
where $\mathcal{V}^{\theta}_{\phi}$ denotes a family of functions parameterized by ($\theta, \phi)$, and 
%we denote as our function family , and $x_{\theta}$ is defined as:
\begin{equation}
\label{Denoised sample predict}
\hat{x}_{\theta}(t,x,c) = \frac{1}{\alpha_t}\left(\sigma^2_t s_{\theta}(t,x,c) + x\right),
\end{equation}
with $\alpha_t$ and $\sigma_t$ being noise schedules of diffusion models 
(see Appendix \ref{app:ddim} for details). 
When $\theta=\theta_{\text{pre}}$, $\hat{x}_{\theta}$ predicts a denoised sample given the current $x$ and the score estimate $s_{\theta}(t,x,c)$,
which is known as {\em Tweedie's formula}. 
To treat the second term in \eqref{continuous-time value function with regularization},
our intuition comes from that 
\begin{align}
\text{RM}(\mathbb{E}(X_T\mid X_t))\approx \mathbb{E}(\text{RM}(X_T)\mid X_t),
\end{align}
if we are allowed to exchange the conditional expectation and the reward model score (though generally it's not true).
%which is the second part in the value function in \eqref{continuous-time value function with regularization}. 
$F_{\phi}(t,x,c)$ are effectively approximations to the residual term,
which can be seen as a composition of the possible reward error and the 
first term in \eqref{continuous-time value function with regularization}. 

We refer these two parts to as {\it reward mean predictor} and {\it residual corrector}. 
There $c_{\text{skip}}(t)$ and $c_{\text{out}}(t)$ are differentiable functions such that $c_{\text{skip}}(T)=1$ and $c_{\text{out}}(T)=0$, so the boundary condition \eqref{eq:bdry} is satisfied. 
Notably, similar parametrization trick has also been used to train successful diffusion models such as
EDM \cite{karras2022elucidating} and consistency models \cite{song2023consistency}.

For learning the value function, we use trajectory-wise Monte Carlo estimation to update $\phi$ by minimizing the mean square error (MSE). 
In our experiments, we observe that choosing $c_{\text{skip}}(t)=\cos(\frac{\pi}{2T}t)$ and $c_{\text{out}}(t)=\sin(\frac{\pi}{2T}t)$ yields the smallest loss (see Table \ref{tab:architecture_comparison}). 
Also refer to Section \ref{Sec: experiment SD 1.5} for more architecture details.
\vspace{-5pt}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/VN_ablation.png}
    \caption{Pretraining Value Function with Different Architecture.}
    \label{fig: VN architecture ablation}
\end{figure}

\begin{table}[htbp]
    \centering
    \begin{minipage}{\columnwidth}
    \small % Reduce font size
    \setlength{\tabcolsep}{3pt} % Adjust column spacing
    \begin{tabular}{ccccc}
        \toprule
        \multicolumn{1}{c}{Architecture} & 
        \multicolumn{1}{c}{Predictor} & 
        \multicolumn{1}{c}{Corrector} & 
        \multicolumn{1}{c}{$c_{\text{out}}$} & 
        \multicolumn{1}{c}{MSE} \\
        \midrule
        Baseline & $\text{RM}(x,c)$ & $F(x,c)$ & 1-$\cos(\frac{\pi}{2T}t)$ & 2.63 \\
        Org+Denoised & $\text{RM}(x,c)$ & $F(\hat{x}_{\theta},c)$ & 1-$\cos(\frac{\pi}{2T}t)$ & 2.51 \\
        Denoised+Orig & $\text{RM}(\hat{x}_{\theta},c)$ & $F(x,c)$ & 1-$\cos(\frac{\pi}{2T}t)$ & 0.67 \\
        Denoised+Denoised & $\text{RM}(\hat{x}_{\theta},c)$ & $F(\hat{x}_{\theta},c)$ & 1-$\cos(\frac{\pi}{2T}t)$ & 0.66 \\
        \textbf{Denoised+Orig} & $\text{RM}(\hat{x}_{\theta},c)$ & $F(x,c)$ & $\sin(\frac{\pi}{2T}t)$ & \textbf{0.29} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of different architecture configurations: $\hat{x}_{\theta}$ is abbreviated for $\hat{x}_{\theta}(t,x,c)$.}
    \label{tab:architecture_comparison}
    \end{minipage}
\end{table}

\vspace{-5 pt}

Note that it is possible to learn the value function by either solving the associated Hamilton-Jacobi-Bellman equation, or minimizing Bellman's error rate (which can be seen as the continuous-time analog of temporal difference). However, both approaches will yield a supervised learning objective that contains the second-order derivative of the value function, 
which is hard to optimize. We leave for future work the investigation of policy evaluation methods that are based on partial differential equations or temporal difference rates.

\section{Continuous-time Policy Optimization}
\label{sc4}

To efficiently optimize the continuous-time RL problem raised above, 
we further develop the theory of policy optimization in continuous time and space for fine-tuning diffusion models. 
Different from the general formalism in the literature \cite{schulman2015trust,zhao2024policy},
we focus on the case of 
(1) KL regularized rewards,
and (2) state-independent diffusion coefficients in the continuous-time setup, which yield new results not only in the analysis but also in the resulting algorithms.

\textbf{Policy Gradient}. We first show that the continuous-time policy gradient can be directly computed 
without any prior discretization of the time variable.
%We first present the policy gradient formula for our finite horizon problem \eqref{Discounted Objective 2}, which simplifies the formula in \cite{jia2022policy_gradient} by utilizing the discussion in \cite{jia2022q_learning,zhao2024policy}. 
\begin{theorem}
\label{thm:PG formula}
%We have that the policy 
The gradient of an admissible policy $\pi^{\theta}$ parameterized by $\theta$ takes the form:
\begin{equation}
\nabla_{\theta} V^{\theta}= \mathbb{E}\left[\int _ { 0 } ^ { T }\nabla_{\theta} \log \pi^ { \theta} ( a _ { t } ^ {\theta} | t , X _ { t } ^ {\theta} ) q(t, X_t^{\theta}, a_t^{\theta} ; \pi^\theta)\mathrm{d} t\right],
\end{equation}
where $\pi^\theta$, $a^\theta_t$ and $q$ are as defined in \eqref{atht} and \eqref{defqvalue}.
\end{theorem}

{\it Proof Sketch}. The full proof is given in Appendix \ref{Proof of PG formula}. 

Note that the only terms in the $q$-value function that involve action $a$ are (the second order term is irrelevant to action $a$):
$$
g^2(T-t) a \frac{\partial V^{\theta}}{\partial x}(t,x) =: \tilde{q}^{\theta}(t,x,a).
% =r_{\eta}(t,x,a)+\left(-f(T-t,x) + \frac{1+\eta^2}{2}g^2(T-t) a\right) \frac{\partial V}{\partial x},
$$
%of which $r(t,x,a)$ can be observed, $b(t,x,a)$ can be computed given $(x,a)$.  %When the reward function is differentiable with respect to $x$, then 
In addition, the value function approximation can be computed by Monte Carlo or the martingale approach as in \citet{jia2022policy_evaluation}, and then $\frac{\partial V}{\partial x}$ can be evaluated by backward propagation. Since the reward can be non-differentiable, and also for the sake of efficient computation, 
we can approximate $\tilde{q}^{\theta}(t,x,a)\approx \left(V(t, x+\eta\, g^2(T-t) a)-V(t, x)\right)/\eta$, where $\eta$ is a scaling parameter. 
%$\tilde{q}^{\theta}(t,x,a)$ can be seen as a first order approximation of $\left(V(t, x+\sigma\cdot g^2(T-t) a)-V(t, x)\right)/\sigma$, and this approximation thus yields an efficient alternative.

\textbf{Continuous-time TRPO/PPO}. We also derive the continuous-time finite horizon analogies of TRPO and PPO for the discrete RL in the finite horizon setting \cite{schulman2015trust,schulman2017proximal}, and in the continuous-time infinite horizon setup \cite{zhao2024policy}. 
%Given a feedback policy $\hat{\pi} = \pi^{\hat{\theta}}$, 
The Performance Difference Lemma (PDL) is as follows.
\begin{lemma}
\label{lem:Continuous-time PDL}
We have that:
\begin{eqnarray}
\label{eqn: CT PDL}
V^{\hat{\theta}} - V^{\theta} =\mathbb{E}\int _ { 0 } ^ { T } q(t, X_t^{\hat{\theta}}, a_t^{\hat{\theta}} ; \pi^{\theta})\mathrm{d} t.
\end{eqnarray}
\end{lemma}
\vspace{-5 pt}
The proof is similar to Theorem 2 in \cite{zhao2024policy}. In the same essence of \cite{KakadeL02, schulman2015trust,zhao2024policy}, we define the {\em local approximation function} to $V^{\hat{\theta}}$ by
\begin{eqnarray}
\label{TRPO objective cont integral}
	L^{\theta}(\hat{\theta})=V^{\theta}+\mathbb{E}\int _ { 0 } ^ { T } \frac{\pi^ { \hat{\theta}}( a _ { t } ^ {\theta} | t , X _ { t } ^ {\theta} )}{\pi^ { \theta}( a _ { t } ^ {\theta} | t , X _ { t } ^ {\theta} )} q(t, X_t^{\theta}, a_t^{\theta} ; \pi^\theta)\mathrm{d} t.
\end{eqnarray}
Observe that
$$\text{(i) }L^{\theta}(\theta)=V^{\theta}, \quad\text{(ii) }\nabla_{\hat{\theta}}L^{\theta}(\hat{\theta})\mid_{\hat{\theta}=\theta}=\nabla_{\hat{\theta}}V^{\hat{\theta}}\mid_{\hat{\theta}=\theta},$$ 
i.e., the local approximation function and the true performance objective share the same value and the same gradient with respect to the policy parameters.
Thus, the local approximation function can be regarded as the first order approximation to the performance metric.

Now we provide analysis on the gap $V^{\hat{\theta}} - L^{\theta}(\hat{\theta})$, which guarantees the policy improvement
(similar to approaches for discounted/average reward MDP \cite{schulman2015trust,zhang2021policy}, and for continuous-time RL in the infinite horizon \cite{zhao2024policy}).
\begin{assumption}
\label{Difference Bound Assumptions} ({\it Bounded Reward and $q$ function}): There exists $M > 0$ such that for any $c$, $x$ and $a$, $|\text{RM}(x,c)|\leq M$ and $|q(t,x,a;\pi^{\theta})|\leq M$ for any $t$, $\pi^{\theta}$.
\end{assumption}
\begin{theorem}
\label{thm:TRPO/PPO}
Under Assumption \ref{Difference Bound Assumptions}, then for any policy $\hat{\theta}$ such that $\bigl|\ln \bigl(\tfrac{p^{\hat{\theta}}(x)}{p^{\theta_{\mathrm{pre}}}(x)}\bigr)\bigr|$ is bounded, and $\mathrm{KL}(p^{\theta} \,\|\, p^{\hat{\theta}})\leq 1$, there exists a constant $C > 0$ such that:
\begin{align}
|V^{\hat{\theta}} & - L^{\theta}(\hat{\theta})|\leq \nonumber\\
&C \, \left(\mathbb{E}\int_{0}^{T} \operatorname{KL}(\pi^{\theta}(\cdot | t , X _ { t } ^ {\theta} )\|\pi^{\hat{\theta}}( \cdot | t , X _ { t } ^ {\theta} ))\mathrm{d}t\right)^{\frac{1}{2}}.
\end{align}
\end{theorem}
\vspace{-10pt}
\textit{Proof Sketch}. Different from the proofs in \cite{schulman2015trust,zhao2024policy}, which bound the difference through PDL, our proof relies on the structural property of diffusion models by bounding: 
\begin{align*}
|V^{\hat{\theta}} & - V^{\theta}|\text{ and }|L^{\theta}(\hat{\theta}) - V^{\theta}|
\end{align*}
separately. The detailed proof is given in Appendix \ref{Proof of TRPO/PPO}. 

By Theorem \ref{thm:TRPO/PPO}, we can apply the same technique as in PPO \cite{schulman2017proximal} by clipping the ratio and replacing $q$ with $\tilde{q}$ (which is equivalent to adapting a baseline function).
This yields the policy update rule as:
\begin{equation}
\theta_{n+1} = \max_\theta \mathbb{E}\int_{0}^{T}\min \left(\rho^{\theta}_{t} q^{\theta_{n}}_{t}, \operatorname{clip}\left(\rho^{\theta}_{t}, \epsilon\right) q^{\theta_{n}}_{t}\right)\mathrm{d}t,
\end{equation}
where the advantage rate function and the likelihood ratio are defined by
$$
q^{\theta_n}_{t}=\tilde{q}(t, X_t^{\theta_n}, a_t^{\theta_n} ; \pi^\theta_n),\quad \rho^{\theta}_{t}=\frac{\pi^ { \theta}( a _ { t } ^ {\theta_n} | t , X _ { t } ^ {\theta_n} )}{\pi^ { \theta_n}( a _ { t } ^ {\theta_n} | t , X _ { t } ^ {\theta_n} )}.
$$
The surrogate objective can then be optimized by stochastic gradient descent. The pseudo-code of our algorithm is listed in Appendix \ref{app:alg pseudo-code}.

\section{Experiments}
\label{sc5}

% \subsection{Enhancing Diffusion Policies}

\subsection{Enhancing Small-Steps Diffusion Models}
% \blue{Haoxian is testing.}
\textbf{Setup}. We evaluate the ability of our proposed algorithm to train short-run diffusion models with significantly reduced generation steps $T$,
while maintaining high sample quality. 
In the experiment, we take $T=10$. Our experiments are conducted on the CIFAR-10 (3232) dataset \cite{cifar10}. We fine-tune pretrained diffusion model backbone using DDPM \cite{Ho20DDPM}. The primary evaluation metric is the Frchet Inception Distance (FID) \cite{fid}, which measures the quality of generated samples.

To benchmark our method, we compare it against DxMI \cite{dxmi}, which formulates the diffusion model training as an inverse reinforcement learning (IRL) problem. DxMI jointly trains a diffusion model and an energy-based model (EBM), where the EBM estimates the log data density and provides a reward signal to guide the diffusion process. To ensure a fair comparison, we replace the policy improvement step in DxMI with our continuous-time RL counterpart, maintaining consistency while evaluating the effectiveness of our approach. We set the learning rate of the value network to $2\times10^{-5}$ and U-net to $3\times 10^{-7}$.

\textbf{Result}. Figure \ref{fig:cifar10} shows our approach converges significantly faster than DxMI, and achieves consistently lower FID scores throughout training. The samples from the two fine-tuned models are shown in Figures \ref{fig:DxMI_sample} and \ref{fig:CTRL_sample}. In comparison, the samples generated from the model fine-tuned by continuous-time RL have clearer contours, better aligned with real-world features, and exhibit superior aesthetic quality.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/CIFAR10.png}
    \caption{Training curves of DxMI and continuous-time RL.}
    \label{fig:cifar10}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/DxMI_sample.png}
        \caption{DxMI samples at the $6000$-th step}
        \label{fig:DxMI_sample}
    \end{minipage}%
    \hspace{2mm}
    \begin{minipage}[t]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/CTRL_sample.png}
        \caption{Continuous-time RL samples at the $6000$-th step}
        \label{fig:CTRL_sample}
    \end{minipage}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/VN_architecture.png}
    \caption{We adopt the similar backbone of ImageReward for two parts in value network, both by adding an MLP layer over the BLIP encoded latents.}
    \label{fig:arch}
    \vspace{-5 pt}
\end{figure}


\subsection{Fine-Tuning Stable Diffusion}
\label{Sec: experiment SD 1.5}

\textbf{Setup}. We also validate our proposed algorithm for fine-tuning large-scale T2I diffusion models, Stable Diffusion v1.5 \footnote{https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5}. We adopt the pretrained ImageReward \cite{ImageReward} as the reward signal during RL, as it has been shown in previous studies to achieve better alignment with human preferences to other metrics such as aesthetic scores, CLIP and BLIP scores.

We train the value networks with full parameter tuning, while we use LoRA \cite{hu2021lora} for tuning the U-nets of diffusion models. We adopt a learning rate of $10^{-7}$ for optimizing the value network, $3\times 10^{-5}$ for optimizing the U-net and $\beta=5\times 10^{-5}$ for regularization. We train the models on 8 H200 GPUs with 128 effective batch sizes.

\textbf{Value Network Architecture.} Since we fix the reward model as ImageReward, we design the value network by using a similar backbone to the ImageReward model, which is composed of BLIP and a MLP header (see Figure \ref{fig:arch}).  
To ensure the boundary condition, we fix the parameters (i.e., BLIP and MLP) in the left part (skyblue) of the value network and only tune 30\% of the parameters of BLIP in the right part (green). 
The VAE Decoder on both parts is fixed for efficiency and stabilized training. 

As a remark, replacing $x$ with $x_{\theta}(t,x)$ in the ``residual corrector" leads to minimum gain, compared to the drastic improvement brought forth by using $x_{\theta}(t,x)$ as the input in the ``reward mean predictor".
See Figure \ref{fig: VN architecture ablation} and Table \ref{tab:architecture_comparison} for our ablation of network architecture and MSE statistics.

\textbf{Policies trained by Continuous-time RL are robust to time discretization}. 
We find that the policies trained by continuous-time RL achieve coherent performance in terms of the reward mean evaluated by ImageReward. 
In Figure \ref{fig:CTRLvsTimeSteps}, three line plots that correspond to 25, 50, and 100 steps almost always overlap after 20 epochs of training, which is consistent with our theoretical analyses.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/CTRLvsTimeSteps.png}
    \caption{Performance of continuous-time RL's checkpoints with respect to discretization timesteps.}
    \label{fig:CTRLvsTimeSteps}
\end{figure}

\vspace{-10 pt}
Qualitative examples with the same prompt of the base model, continuous-time RL training for 50 steps and 100 steps can be found in Figure \ref{Fig:CTRL_SD_sample}.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/CTRL_SD_samples.png}
    \caption{Model generations with prompt ``A unicorn in a clearing. it has a single shining horn. volumetric light." \quad a) \textbf{Top:} Base model Stable Diffusion v1.5; b) \textbf{Mid:} continuous-time RL after 50 training steps; c) \textbf{Bot:} continuous-time RL after 100 training steps.}
    \label{Fig:CTRL_SD_sample}
\end{figure}


\textbf{Continuous-time RL outperforms Discrete-time RL baseline methods in both efficiency and stability}. We also compare the reward curves of discrete-time RL with our continuous-time RL algorithms. 
In Figure \ref{Fig:CTRLvsDDPO.png}, the performance of the continuous-time RL is much more stable, and is more efficient in achieving a high average reward.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/CTRLvsDDPO.png}
    \caption{Performance of continuous-time RL against discrete-time RL under the same 50 discretization timesteps.}
    \label{Fig:CTRLvsDDPO.png}
    \vspace{-10 pt}
\end{figure}
Why continuous-time approaches show better performance? 
Here we provide a heuristic explanation. Discrete-time RL methods optimize the objective with a priori time-discretization, which induces an error such that the resulting optimal policy can be significantly away from the true optimum in continuous time. 
Continuous-time RL methods, on the other hand, only require time-discretization in estimating the policy gradient. The error caused by this discretization --- the gap between the resulting optimum and the true (continuous-time objective) optimum --- is bounded by a polynomial of the step size (in gradient estimation) under suitable regularity conditions.


\section{Discussion and Conclusion}
\label{sc6}

We have proposed in this study a continuous-time reinforcement learning (RL) framework for fine-tuning diffusion models. Our work introduces novel policy optimization theory for RL in continuous time and space, alongside a scalable and effective RL algorithm that enhances the generation quality of diffusion models, as validated by our experiments.  

In addition, our algorithm and network designs exhibit a striking versatility that allows us to incorporate and leverage some of the advantages of prior works in diffusion models design, so as to better exploit model structures  and to improve value network architectures. In view of this, we believe the continuous-time RL, in providing cross-pollination between diffusion models and RLHF, presents a highly promising direction for future research. 

%\section*{Impact Statement}
%This paper presents work whose goal is to advance the field of 
%Machine Learning. There are many potential societal consequences 
%of our work, none which we feel must be specifically highlighted here.

\section*{Acknowledgement}
Hanyang Zhao, Haoxian Chen and Wenpin Tang are supported by NSF grant DMS-2113779 and DMS-2206038.
Haoxian Chen is supported by the Amazon CAIT fellowship.
Wenpin Tang receives support from the Columbia Innovation Hub grant,
and the Tang Family Assistant Professorship.
The works of Haoxian Chen, Hanyang Zhao, 
and David Yao are part of a Columbia-CityU/HK collaborative project that is supported by InnoHK
Initiative, The Government of the HKSAR and the AIFT Lab. 


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Connection between discrete-time and continuous-time sampler}
In this section, we summarize the discussion of popular samplers like DDPM, DDIM, stochastic DDIM and their continuous-time limits being a Variance Preserving (VP) SDE.
\label{app:discrete and continuous sampler connection}
\subsection{DDPM sampler is the discretization of VP-SDE}
\label{app:ddpm}
We review the forward and backward process in DDPM, and its connection to the VP SDE following the discussion in \cite{Song20SGMbySDE,SBDM_tutorial}. DDPM considers a sequence of positive noise scales $0<\beta_1, \beta_2, \cdots, \beta_N<1$. For each training data point $x_0 \sim p_{\text {data }}(x)$, a discrete Markov chain $\left\{x_0, x_1, \cdots, x_N\right\}$ is constructed such that:
\begin{equation}
\label{DDPM forward}
x_i=\sqrt{1-\beta_i} x_{i-1}+\sqrt{\beta_i} z_{i-1}, \quad i=1, \cdots, N,
\end{equation}
where $z_{i-1} \sim \mathcal{N}(0, I)$, thus $p\left(x_i \mid x_{i-1}\right)=\mathcal{N}\left(x_i ; \sqrt{1-\beta_i} x_{i-1}, \beta_i I\right)$. We can further think of $x_i$ as the $i^{\text{th}}$ point of a uniform discretization of time interval $[0,T]$ with discretization stepsize $\Delta t=\frac{T}{N}$, i.e. $x_{i \Delta t}=x_i$; and also $z_{i \Delta t}=z_i$. To obtain the limit of the Markov chain when $N \rightarrow \infty$, we define a function $\beta:[0,T]\rightarrow \mathbb{R}^+$ assuming that the limit exists: $\beta(t)= \lim_{\Delta t\rightarrow 0}\beta_i /\Delta t$ with $i=t/\Delta t$. Then when $\Delta t$ is small, we get:
$$x_{t+\Delta t}\approx\sqrt{1-\beta(t) \Delta t} x_t+\sqrt{\beta(t) \Delta t} z_t \approx x_t-\frac{1}{2} \beta(t) x_t \Delta t+\sqrt{\beta(t) \Delta t} z_t.$$
Further taking the limit $\Delta t \rightarrow 0$, this leads to:
$$
d X_t=-\frac{1}{2} \beta(t) X_t d t+\sqrt{\beta(t)} d B_t, \quad 0 \leq t \leq T,
$$
and we have:
$$
f(t, x)=-\frac{1}{2} \beta(t) x, g(t)=\sqrt{\beta(t)}.
$$
Through reparameterization, we have $p_{\bar{\alpha}_i}\left(x_i \mid x_0\right)=\mathcal{N}\left(x_i ; \sqrt{\bar{\alpha}_i} x_0,\left(1-\bar{\alpha}_i\right) I\right)$, where $\bar{\alpha}_i:=\prod_{j=1}^i\left(1-\beta_j\right)$. For the backward process, a variational Markov chain in the reverse direction is parameterized with $p_{\theta}\left(x_{i-1} \mid x_i\right)=\mathcal{N}\left(x_{i-1} ; \frac{1}{\sqrt{1-\beta_i}}\left(x_i+\beta_i s_{\theta}\left(i,x_i\right)\right), \beta_i I\right)$, and trained with a re-weighted variant of the evidence lower bound (ELBO):
$$
\theta^*=\underset{\theta}{\arg \min } \sum_{i=1}^N\left(1-\bar{\alpha}_i\right) \mathbb{E}_{p_{\text {data }}(x)} \mathbb{E}_{p_{\bar{\alpha}_i}(\tilde{x} \mid x)}\left[\left\|s_{\theta}(i,\tilde{x})-\nabla_{\tilde{x}} \log p_{\bar{\alpha}_i}(\tilde{x} \mid x)\right\|_2^2\right] .
$$
After getting the optimal model $s_{\theta^*}(i,x)$, samples can be generated by starting from $x_N \sim \mathcal{N}(0, I)$ and following the estimated reverse Markov chain as:
\begin{equation}
\label{DDPM Backward Process}
x_{i-1}=\frac{1}{\sqrt{1-\beta_i}}\left(x_i+\beta_i s_{\theta^*}\left(i,x_i\right)\right)+\sqrt{\beta_i} z_i, \quad i=N, N-1, \cdots, 1 .
\end{equation}
Similar discussion as for the forward process, the equation \eqref{DDPM Backward Process} can further be rewritten as:
\begin{equation}
\begin{aligned}
x_{(i-1)\Delta t} &\approx \frac{1}{\sqrt{1-\beta_{i\Delta t}\Delta t}}\left(x_{i\Delta t}+\beta (i\Delta t)\Delta t \cdot s_{\theta^*} \left(i\Delta t,x_{i\Delta t}\right)\right)+\sqrt{\beta_i} z_i,\\
&\approx (1+\frac{1}{2}\beta_{i\Delta t}\Delta t)\left(x_{i\Delta t}+\beta (i\Delta t)\Delta t \cdot s_{\theta^*} \left(i\Delta t,x_{i\Delta t}\right)\right)+\sqrt{\beta_i} z_i,\\
&\approx (1+\frac{1}{2}\beta_{i\Delta t}\Delta t)x_{i\Delta t}+\beta (i\Delta t)\Delta t\cdot s_{\theta^*} \left(i\Delta t,x_{i\Delta t}\right)+\sqrt{\beta_i} z_i,
\end{aligned}
\end{equation}
when $\beta_{i\Delta t}$ is small. This is indeed the time discretization of the backward SDE:
\begin{equation}
\begin{aligned}
\mathrm{d} X^{\sla}_t &= (\frac{1}{2} \beta(T-t) X^{\sla}_t + \beta(T-t) s_{\theta^{*}}(T-t, X^{\sla}_t))\mathrm{d}t + \sqrt{\beta(t)} \mathrm{d}B_t,\\
&= \left(-f(T-t,X^{\sla}_t) + g^2(T-t) s_{\theta^{*}}(T-t, X^{\sla}_t)  \right) \mathrm{d}t + g(T-t) \mathrm{d}B_t.
\end{aligned}
\end{equation}

\subsection{DDIM sampler is the discretization of ODE}
\label{app:ddim}
We review the backward process in DDIM, and its connection to the probability flow ODE following the discussion in \cite{Song20SGMbySDE,kingma2021variationalDM,salimans2022progressive,zhang2022fast}.

(i) DDIM update rule: 
The concrete updated rule in DDIM paper (same as in the implementation) adopted the following rule (with $\sigma_t=0$ in Equation (12) of \cite{DDIM}):
\begin{equation}
\label{eq: DDIM update rule}
x_{t-1}=\sqrt{\bar{\alpha}_{t-1}} \underbrace{\left(\frac{x_t-\sqrt{1-\bar{\alpha}_t} \epsilon_\theta^{(t)}\left(x_t\right)}{\sqrt{\bar{\alpha}_t}}\right)}_{\text {``predicted } x_0 "}+\underbrace{\sqrt{1-\bar{\alpha}_{t-1}} \cdot \epsilon_\theta^{(t)}\left(x_t\right)}_{\text {``direction pointing to } x_t "}
\end{equation}
To show the correspondence between DDIM parameters and continuous-time SDE parameters, we follow one derivation in \cite{salimans2022progressive} by considering the ``predicted $x_0$'': note that define the predicted $x_0$ parameterization as: 
$$
\hat{x}_\theta\left(t,x\right) = \frac{x-\sqrt{1-\bar{\alpha}_t} \epsilon_\theta^{(t)}\left(x\right)}{\sqrt{\bar{\alpha}_t}},\text{ or , }\epsilon_\theta^{(t)}\left(x\right) = \frac{x- \sqrt{\bar{\alpha}_t}\hat{x}_\theta\left(t,x\right)}{\sqrt{1-\bar{\alpha}_t}},
$$
above \eqref{eq: DDIM update rule} can be rewritten as:
\begin{equation}
\label{DDIM rewritten}
x_{t-1}=\frac{\sqrt{1-\bar{\alpha}_{t-1}}}{\sqrt{1-\bar{\alpha}_t}} \left(x_t-\sqrt{\bar{\alpha}_t} \hat{x}_\theta\left(t,x\right)\right)+\sqrt{\bar{\alpha}_{t-1}} \cdot \hat{x}_\theta\left(t,x\right)
\end{equation}
Using parameterization $\sigma_t=\sqrt{1-\bar{\alpha}_{t}}$ and $\alpha_t = \sqrt{\bar{\alpha}_{t}}$, we have for $t-1 = s<t$:
\begin{equation}
\label{DDIM update rule continous}
X_s=\frac{\sigma_s}{\sigma_t}\left[X_t-\alpha_t \hat{x}_\theta\left(t,X_t\right)\right]+\alpha_s \hat{x}_\theta\left(t,X_t\right),
\end{equation}
which is the same as derived in \cite{kingma2021variationalDM,salimans2022progressive}.

\subsubsection{ODE explanation by analyzing the derivative}
We further assume a VP diffusion process with $\alpha_t^2=1-\sigma_t^2=\operatorname{sigmoid}\left(\lambda_t\right)$ for $\lambda_t=\log \left[\alpha_t^2 / \sigma_t^2\right]$, in which $\lambda_t$ is known as the signal-to-noise ratio. Taking the derivative of \eqref{DDIM update rule continous} with respect to $\lambda_s$, assuming again a variance preserving diffusion process, and using $\frac{d \alpha_\lambda}{d \lambda}=\frac{1}{2} \alpha_\lambda \sigma_\lambda^2$ and $\frac{d \sigma_\lambda}{d \lambda}=-\frac{1}{2} \sigma_\lambda \alpha_\lambda^2$, gives
$$
\begin{aligned}
\frac{X_{\lambda_s}}{d \lambda_s} & =\frac{d \sigma_{\lambda_s}}{d \lambda_s} \frac{1}{\sigma_t}\left[X_t-\alpha_t \hat{x}_\theta\left(t,X_t\right)\right]+\frac{d \alpha_{\lambda_s}}{d \lambda_s} \hat{x}_\theta\left(t,X_t\right) \\
& =-\frac{1}{2} \alpha_s^2 \frac{\sigma_s}{\sigma_t}\left[X_t-\alpha_t \hat{x}_\theta\left(t,X_t\right)\right]+\frac{1}{2} \alpha_s \sigma_s^2 \hat{x}_\theta\left(t,X_t\right) .
\end{aligned}
$$

Evaluating this derivative at $s=t$ then gives
\begin{equation}
\label{DDIM derivative}
\begin{aligned}
\left.\frac{X_{\lambda_s}}{d \lambda_s}\right|_{s=t} & =-\frac{1}{2} \alpha_\lambda^2\left[X_\lambda-\alpha_\lambda \hat{x}_\theta\left(t,X_\lambda\right)\right]+\frac{1}{2} \alpha_\lambda \sigma_\lambda^2 \hat{x}_\theta\left(t,X_\lambda\right) \\
& =-\frac{1}{2} \alpha_\lambda^2\left[X_\lambda-\alpha_\lambda \hat{x}_\theta\left(t,X_\lambda\right)\right]+\frac{1}{2} \alpha_\lambda\left(1-\alpha_\lambda^2\right) \hat{x}_\theta\left(t,X_\lambda\right) \\
& =\frac{1}{2}\left[\alpha_\lambda \hat{x}_\theta\left(t,X_\lambda\right)-\alpha_\lambda^2 X_\lambda\right] .
\end{aligned}
\end{equation}
Recall that the forward process in terms of an SDE is defined as:
$$
\mathrm{d} X_t=f(t,X_t) \mathrm{d} t+g(t) \mathrm{d} B_t,\quad t\in [0,T]
$$
and \cite{Song20SGMbySDE} shows that backward of this diffusion process is an SDE, but shares the same marginal probability density of an associated probability flow ODE  (by taking $t:=T-t$) :
$$
\mathrm{d} X_t=\left[f(t,X_t)-\frac{1}{2} g^2(t) \nabla_x \log p(t,X_t)\right] \mathrm{d} t,\quad t\in [T,0]
$$
where in practice $\nabla_x \log p(t,x)$ is approximated by a learned denoising model using
\begin{equation}
\label{score_parameterization}
\nabla_x \log p(t,x) \approx s_{\theta}(t,x)=\frac{\alpha_t \hat{x}_\theta\left(t,x\right)-x}{\sigma_t^2} = -\frac{\epsilon_\theta^{(t)}\left(x\right)}{\sigma_t}.
\end{equation}
with two chosen noise scheduling parameters $\alpha_t$ and $\sigma_t$, and corresponding drift term $f(t,x)=\frac{d \log \alpha_t}{d t} x_t$ and diffusion term $g^2(t)=\frac{d \sigma_t^2}{d t}-2 \frac{d \log \alpha_t}{d t} \sigma_t^2$. 

Further assuming a VP diffusion process with $\alpha_t^2=1-\sigma_t^2=\operatorname{sigmoid}\left(\lambda_t\right)$ for $\lambda_t=\log \left[\alpha_t^2 / \sigma_t^2\right]$, we get
$$
f(t,x)=\frac{d \log \alpha_t}{d t} x=\frac{1}{2} \frac{d \log \alpha_\lambda^2}{d \lambda} \frac{d \lambda}{d t} x=\frac{1}{2}\left(1-\alpha_t^2\right) \frac{d \lambda}{d t} x=\frac{1}{2} \sigma_t^2 \frac{d \lambda}{d t} x.
$$
Similarly, we get
$$
g^2(t)=\frac{d \sigma_t^2}{d t}-2 \frac{d \log \alpha_t}{d t} \sigma_t^2=\frac{d \sigma_\lambda^2}{d \lambda} \frac{d \lambda}{d t}-\sigma_t^4 \frac{d \lambda}{d t}=\left(\sigma_t^4-\sigma_t^2\right) \frac{d \lambda}{d t}-\sigma_t^4 \frac{d \lambda}{d t}=-\sigma_t^2 \frac{d \lambda}{d t}.
$$
Plugging these into the probability flow ODE then gives
\begin{equation}
\label{eq:DDIM ODE}
\begin{aligned}
\mathrm{d} X_t & =\left[f(t,X_t)-\frac{1}{2} g^2(t) \nabla_x \log p(t,x)\right] \mathrm{d} t \\
& =\frac{1}{2} \sigma_t^2\left[X_t+\nabla_x \log p(t,X_t)\right] \mathrm{d} \lambda_t .
\end{aligned}
\end{equation}
Plugging in our function approximation from Equation  \eqref{score_parameterization} gives
\begin{equation}
\label{eq:ODE_practical}
\begin{aligned}
\mathrm{d}  X_t & =\frac{1}{2} \sigma_t^2\left[X_t+\left(\frac{\alpha_t \hat{x}_\theta\left(t,X_t\right)-X_t}{\sigma_t^2}\right)\right] \mathrm{d} \lambda_t \\
& =\frac{1}{2}\left[\alpha_t \hat{x}_\theta\left(t,X_t\right)+\left(\sigma_t^2-1\right) X_t\right] \mathrm{d} \lambda_t \\
& =\frac{1}{2}\left[\alpha_t\hat{x}_\theta\left(t,X_t\right)-\alpha_t^2 X_t\right]\mathrm{d} \lambda_t .
\end{aligned}
\end{equation}
Comparison this with Equation \eqref{DDIM derivative} now shows that DDIM follows the probability flow ODE up to first order, and can thus be considered as an integration rule for this ODE. 

\subsubsection{Exponential Integrator Explanation}
In \cite{zhang2022fast} that the integration role above is referred as "exponential integrator" of \eqref{eq:ODE_practical}. We adopt two ways of derivations:

(a) Notice that, if we treat the $\hat{x}_\theta\left(t,X_t\right)$ as a constant in \eqref{eq:ODE_practical} (or assume that it does not change w.r.p. $t$ along the ODE trajectory), we have:
\begin{equation}
\begin{aligned}
\mathrm{d}  X_t + \frac{1}{2}\alpha_t^2 X_t\mathrm{d} \lambda_t & =\hat{x}_\theta\left(t,X_t\right)\cdot\frac{1}{2} \alpha_t \mathrm{d} \lambda_t .
\end{aligned}
\end{equation}
Both sides multiplied by $1/\sigma_t$ and integrate from $t$ to $s$ yields:
\begin{equation}
\begin{aligned}
\frac{X_s}{\sigma_s}-\frac{X_t}{\sigma_t}=\hat{x}_\theta\left(t,X_t\right)\cdot\left(\exp(\frac{1}{2}\lambda_s)-\exp(\frac{1}{2}\lambda_t)\right)=\hat{x}_\theta\left(t,X_t\right)\cdot\left(\frac{\alpha_s}{\sigma_s}-\frac{\alpha_t}{\sigma_t}\right).
\end{aligned}
\end{equation}
which is thus
\begin{equation}
\begin{aligned}
X_s 
&=  \frac{\sigma_s}{\sigma_t}X_t+\left[\alpha_s-\alpha_t \frac{\sigma_s}{\sigma_t} \right] \hat{x}_\theta\left(t,X_t\right),
\end{aligned}
\end{equation}
which is the same as DDIM continuous-time intepretation as in \eqref{DDIM update rule continous}. 

(b) We also notice that we can also simplify the whole proof by treating the scaled score (same as in \cite{zhang2022fast}):
\begin{equation}
\label{scaled_score}
\sigma_t\nabla_x \log p(t,x) \approx \sigma_t s_{\theta}(t,x)=\frac{\alpha_t \hat{x}_\theta\left(t,x\right)-x}{\sigma_t}
\end{equation}
as a constant in \eqref{eq:DDIM ODE} (or assume that it does not change w.r.p. $t$ along the ODE trajectory). Notice that from backward ODE, we have:
\begin{equation}
\begin{aligned}
\mathrm{d} X_t =\frac{1}{2} \sigma_t^2\left[X_t+\frac{1}{\sigma_t}\sigma_t\nabla_x \log p(t,X_t)\right] \mathrm{d} \lambda_t .
\end{aligned}
\end{equation}
Both sides multiplied by $1/\alpha_t$ and integrate from $t$ to $s$ yields:
\begin{equation}
\begin{aligned}
\frac{X_s}{\alpha_s}-\frac{X_t}{\alpha_t}=\left(\frac{\alpha_t \hat{x}_\theta\left(t,X_t\right)-X_t}{\sigma_t}\right)\cdot\left(-\frac{\sigma_s}{\alpha_s}+\frac{\sigma_t}{\alpha_t}\right).
\end{aligned}
\end{equation}
which is thus
\begin{equation}
\begin{aligned}
X_s 
&=  \frac{\sigma_s}{\sigma_t}X_t+\left[\alpha_s-\alpha_t \frac{\sigma_s}{\sigma_t} \right] \hat{x}_\theta\left(t,X_t\right),
\end{aligned}
\end{equation}
which is the same as DDIM continuous-time intepretation as in \eqref{DDIM update rule continous}. 

As a summary, treating the denoised mean or the noise predictor as the constants will both recovery the rule of DDIM. Usually, for ODE flows, the denoised mean assumption naturally holds; however, why the scaled score leads to the same integration rule remains to be an interesting question, probably comes from the design property of DDIM, see e.g. discussions in \cite{karras2022elucidating}.

\section{Theorem Proofs}
\subsection{Proof of Theorem \ref{thm:Regularization as KL bound}}
\label{Proof of Regularization as KL bound}
The main proof technique relies on Girsanov's Theorem, which is similar to the argument in \cite{chen2022sampling}. First, we recall a consequence of Girsanov's theorem that can be obtained by combining Pages 136-139, Theorem 5.22, and Theorem 4.13 of Le Gall (2016).
\begin{theorem} For $t \in[0, T]$, let $\mathcal{L}_t=\int_0^t b_s \mathrm{~d} B_s$ where $B$ is a $Q$-Brownian motion. Assume that $\mathbb{E}_Q \int_0^T\left\|b_s\right\|^2 \mathrm{~d} s<\infty$. Then, $\mathcal{L}$ is a $Q$-martingale in $L^2(Q)$. Moreover, if
\begin{equation}
\label{condition for Girsanov}
\mathbb{E}_Q \mathcal{E}(\mathcal{L})_T=1, \quad \text { where } \mathcal{E}(\mathcal{L})_t:=\exp \left(\int_0^t b_s \mathrm{~d} B_s-\frac{1}{2} \int_0^t\left\|b_s\right\|^2 \mathrm{~d} s\right),
\end{equation}
then $\mathcal{E}(\mathcal{L})$ is also a $Q$-martingale, and the process
\begin{equation}
t \mapsto B_t-\int_0^t b_s \mathrm{~d} s
\end{equation}
is a Brownian motion under $P:=\mathcal{E}(\mathcal{L})_T Q$, the probability distribution with density $\mathcal{E}(\mathcal{L})_T$ w.r.t. $Q$.
\end{theorem}
If the assumptions of Girsanov's theorem are satisfied (i.e., the condition \eqref{condition for Girsanov}), we can apply Girsanov's theorem to $Q$ as the law of the following reverse process (we omit $c$ for brevity),
\begin{equation}
\label{eqn:ReverseSDEapprox by pretrain}
\mathrm{d} \overline{X}_t = \left(-f(T-t,\overline{X}_t) + g^2(T-t) s_{\theta_{pre}}(T-t, \overline{X}_t)  \right) \mathrm{d}t + g(T-t) \mathrm{d}B_t,\ \overline{X}_0\sim p_\infty(\cdot)
\end{equation}
and
\begin{equation}
b_t=g(T-t)\left[s_{\theta}(T-t, \overline{X}_t)-s_{\theta_{pre}}(T-t, \overline{X}_t)\right],
\end{equation}
where $t \in[0,T]$. This tells us that under $P=\mathcal{E}(\mathcal{L})_T Q$, there exists a Brownian motion $\left(\beta_t\right)_{t \in[0, T]}$ s.t.
\begin{equation}
\label{new BM under new measure}
\mathrm{d} B_t=g(T-t)\left[s_{\theta}(T-t, \overline{X}_t)-s_{\theta_{pre}}(T-t, \overline{X}_t)\right] \mathrm{d} t+\mathrm{d} \beta_t.
\end{equation}
Plugging \eqref{new BM under new measure} into \eqref{eqn:ReverseSDEapprox by pretrain} we have $P$-a.s.,
\begin{equation}
\mathrm{d} \overline{X}_t = \left(-f(T-t,\overline{X}_t) + g^2(T-t) s_{\theta}(T-t, \overline{X}_t)  \right) \mathrm{d}t + g(T-t) \mathrm{d}\beta_t,\ \overline{X}_0\sim p_\infty(\cdot)
\end{equation}
In other words, under $P$, the distribution of $\overline{X}$ is the same as the distribution generated by current policy parameterized by $\theta$, i.e., $p_\theta(\cdot)=P_T=$ $\mathcal{E}(\mathcal{L})_T Q$. Therefore,
$$
\begin{aligned}
& D_{KL}\left(p_{\theta}\|p_{\theta_{pre}}\right)=\mathbb{E}_{P_T} \ln \frac{\mathrm{d} P_T}{\mathrm{d} Q_T}=\mathbb{E}_{P_T} \ln \mathcal{E}(\mathcal{L})_T \\
& =\mathbb{E}_{P_T}\left[\int_0^t b_s \mathrm{~d} B_s-\frac{1}{2} \int_0^t\left\|b_s\right\|^2\right] \\
& =\mathbb{E}_{P_T}\left[\int_0^t b_s \mathrm{~d} \beta_s+\frac{1}{2} \int_0^t\left\|b_s\right\|^2\right]\\
& =\frac{1}{2} \int_0^tg^2(T-t)\underbrace{\mathbb{E}_{P}\left\|s_{\theta}(T-t, \overline{X}_t)-s_{\theta_{pre}}(T-t, \overline{X}_t)\right\|^2}_{\epsilon_t^2}\mathrm{d}t\\
\end{aligned}
$$
Thus we can bound the discrepancy between distribution generated by the policy $\theta$ and the pretrained parameters $\theta_{pre}$ as 
\begin{equation}
D_{KL}(p_{\theta}\|p_{\theta_{pre}})\leq \frac{1}{2}\int_{0}^{T}g^2(T-t)\epsilon_t^2\mathrm{d}t
\end{equation}

\subsection{Proof of Theorem \ref{thm:PG formula}}
\label{Proof of PG formula}
First we include the policy gradient formula theorem for finite horizon in continuous time from \cite{jia2022policy_gradient}:
\begin{lemma}[Theorem 5 of \cite{jia2022policy_gradient} when $R\equiv 0$]
\label{thm:Jia&Zhou PG}
Under some regularity conditions, given an admissible parameterized policy $\pi_{\theta}$, the policy gradient of the value function $V\left(t, x ; \pi^\theta\right)$ admits the following representation:
\begin{equation}
\label{eqn:Jia&Zhou PG}
\begin{aligned}
\frac{\partial}{\partial \theta} V(t, x ; \pi^\theta)= & \mathbb{E}^{\mathbb{P}}\left[\int _ { t } ^ { T } e ^ { - \beta ( s - t ) } \left\{\frac { \partial } { \partial \theta} \operatorname { l o g } \pi^ { \theta} ( a _ { s } ^ { \boldsymbol { \pi } ^ {\theta} } | s , X _ { s } ^ { \boldsymbol { \pi } ^ {\theta} } ) \left(\mathrm{d} V(s, X_s^{\pi^\theta} ; \pi^\theta)\right.\right.\right. \\
& \left.\left.\left.+\left[r_R(s, X_s^{\pi^\theta}, a_s^{\pi^\theta})-\beta V(s, X_s^{\pi^\theta} ; \pi^\theta)\right] \mathrm{d} s\right) \right\} \mid X_t^{\pi^\theta}=x\right], \quad(t, x) \in[0, T] \times \mathbb{R}^d
\end{aligned}
\end{equation}
in which we denote the regularized reward
$$
r_R(t, X_t^{\pi^\theta}, a_t^{\pi^\theta}) = \gamma(t) \|a_t^{\pi^\theta}-s^{\theta^{*}}(t,X_t)\|^2.
$$
\end{lemma}
First, by applying It\^o's formula to $V(t,X_t)$, we have:
\begin{equation}
\mathrm{d}V(t,X_t) = \left[\frac{\partial V}{\partial t}(t,X_t)+\frac{1}{2}\sigma(t)^2\circ\frac{\partial^2 V}{\partial x^2}(t,X_t)\right]\mathrm{d}t+\frac{\partial V}{\partial x}(t,X_t)\mathrm{d}X_t.
\end{equation}
Further recall that:
\begin{equation}
q(t, x, a ; \pi)=\frac{\partial V}{\partial t}\left(t, x ; \pi\right)+\mathcal{H}\left(t, x, a, \frac{\partial V}{\partial x}\left(t,x ; \pi\right), \frac{\partial^2 V}{\partial x^2}\left(t,x ; \pi\right)\right)-\beta V\left(t,x ; \pi\right),
\end{equation}
this implies that (similar discussion also appeared in \cite{jia2022q_learning})
\begin{equation}
q\left(t, X_t^{\pi}, a_t^{\pi} ; \pi\right) \mathrm{d} t=\mathrm{d} J\left(t, X_t^{\pi} ; \pi\right)+r\left(t, X_t^{\pi}, a_t^{\pi}\right) \mathrm{d} t-\beta J\left(t, X_t^{\pi} ; \pi\right) \mathrm{d} t+\{\cdots\} \mathrm{d} B_t.
\end{equation}
Plug this equality back in \eqref{eqn:Jia&Zhou PG} yields:
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \theta} V(t, x ; \pi^\theta)= & \mathbb{E}^{\mathbb{P}}\left[\int _ { t } ^ { T } e ^ { - \beta ( s - t ) } \frac { \partial } { \partial \theta} \log \pi^ { \theta} ( a _ { s } ^ { \pi ^ {\theta} } | s , X _ { s } ^ {\pi^ {\theta} } ) q\left(t, X_t^{\pi}, a_t^{\pi} ; \pi\right)\mathrm{d} s \mid X_t^{\pi^\theta}=x\right],
\end{aligned}
\end{equation}
Let $t=0$, $\beta = -\alpha$ and further taking expectation to the initial distribution yields Theorem \ref{thm:PG formula}.

% \subsection{Proof of Lemma \ref{lem:Continuous-time PDL}}
% \label{Proof of PDL}

\subsection{Proof of Theorem \ref{thm:TRPO/PPO}}
\label{Proof of TRPO/PPO}
We recall the definition of terms and rewrite the bound in Theorem \ref{thm:TRPO/PPO} for convenience. It suffices to prove that there exists constant $C_1$ and $C_2$, such that
\begin{align}
\label{Direct Performance Difference}
|V^{\hat{\theta}} & - V^{\theta}|\leq C_1 \cdot \left(\mathbb{E}\int_{0}^{T} \operatorname{KL}(\pi^{\theta}(\cdot | t , X _ { t } ^ {\theta} )\|\pi^{\hat{\theta}}( \cdot | t , X _ { t } ^ {\theta} ))\mathrm{d}t\right)^{\frac{1}{2}},
\end{align}
and 
\begin{align}
\label{Local Approximation Difference}
|L^{\theta}(\hat{\theta}) - V^{\theta}|\leq C_2 \cdot \left(\mathbb{E}\int_{0}^{T} \operatorname{KL}(\pi^{\theta}(\cdot | t , X _ { t } ^ {\theta} )\|\pi^{\hat{\theta}}( \cdot | t , X _ { t } ^ {\theta} ))\mathrm{d}t\right)^{\frac{1}{2}}.
\end{align}

To prove \eqref{Direct Performance Difference}, we need to utilize the KL-regularized objective we study for this paper, as this might not hold under general cases. Explicitly writing the definition of $V^{\theta}$ and $V^{\hat{\theta}}$, we have (excluding contents $c$ for simplicity):
$$
|V^{\hat{\theta}} - V^{\theta}|\leq \underbrace{|\mathbb{E}(\text{RM}(X^{\hat{\theta}}_T))-\mathbb{E}(\text{RM}(X^{\theta}_T))|}_{\text{(i) reward difference}}+\beta \underbrace{|\operatorname{KL}\left(p^{\hat{\theta}}(T,\cdot)\| p^{\theta_{pre}}(T,\cdot)\right)-\operatorname{KL}\left(p^{\theta}(T,\cdot)\| p^{\theta_{pre}}(T,\cdot)\right)|}_{\text{(ii) KL difference}}.
$$
For bounding (i), we have:
$$
\text{(i)} =\int_{x}(p^{\hat{\theta}}(T,x)-p^{\theta}(T,x))\text{RM}(x) \mathrm{d} x|\leq |\int_{x}|p^{\hat{\theta}}(T,x)-p^{\theta}(T,x)||\text{RM}(x)| \mathrm{d} x \leq N\cdot\sqrt{\operatorname{KL}\left(p^{\theta}(T,\cdot)\| p^{\hat{\theta}}(T,\cdot)\right)},
$$
where the last equality is due to Pinsker's inequality. For bounding (ii), a standard identity for Kullback--Leibler divergences gives
\[
  \mathrm{KL}\bigl(p^{\hat{\theta}} \,\|\, p^{\theta_{\mathrm{pre}}}\bigr)
  \;-\;
  \mathrm{KL}\bigl(p^{\theta} \,\|\, p^{\theta_{\mathrm{pre}}}\bigr)
  \;=\;
  -\,\mathrm{KL}\bigl(p^{\theta}\,\|\,p^{\hat{\theta}}\bigr)
  \;+\;
  \int \bigl[p^{\hat{\theta}}(x) \;-\; p^{\theta}(x)\bigr]
        \,\ln\!\Bigl(\tfrac{p^{\hat{\theta}}(x)}{p^{\theta_{\mathrm{pre}}}(x)}\Bigr)\,\mathrm{d}x.
\]
Hence, by the triangle inequality,
\[
  \Bigl|
     \mathrm{KL}\bigl(p^{\hat{\theta}} \,\|\, p^{\theta_{\mathrm{pre}}}\bigr)
     -
     \mathrm{KL}\bigl(p^{\theta}\,\|\,p^{\theta_{\mathrm{pre}}}\bigr)
  \Bigr|
  \;\le\;
  \mathrm{KL}\bigl(p^{\theta}\,\|\,p^{\hat{\theta}}\bigr)
  \;+\;
  \biggl|\int (p^{\hat{\theta}} - p^{\theta})\,
              \ln\!\bigl(\tfrac{p^{\hat{\theta}}}{p^{\theta_{\mathrm{pre}}}}\bigr)\biggr|.
\]
By the bounded log-ratio assumption,
\[
  \bigl|\ln\!\bigl(\tfrac{p^{\hat{\theta}}(x)}{p^{\theta_{\mathrm{pre}}}(x)}\bigr)\bigr|
  \;\le\;
  C_3,
\]
so
\[
  \biggl|\int (p^{\hat{\theta}} - p^{\theta})\,
               \ln\!\bigl(\tfrac{p^{\hat{\theta}}}{p^{\theta_{\mathrm{pre}}}}\bigr)\biggr|
  \;\le\;
  C_3 \,\|p^{\hat{\theta}} - p^{\theta}\|_1.
\]
Finally, Pinskers inequality implies
\(\|p^{\hat{\theta}} - p^{\theta}\|_1 \le \sqrt{2\,\mathrm{KL}\!\bigl(p^{\theta}\,\|\,p^{\hat{\theta}}\bigr)}.\)
Combining these bounds yields
\[
  \Bigl|
     \mathrm{KL}\bigl(p^{\hat{\theta}} \,\|\, p^{\theta_{\mathrm{pre}}}\bigr)
     -
     \mathrm{KL}\bigl(p^{\theta}\,\|\,p^{\theta_{\mathrm{pre}}}\bigr)
  \Bigr|
  \;\le\;
  \mathrm{KL}\bigl(p^{\theta}\,\|\,p^{\hat{\theta}}\bigr)
  \;+\;
  C_4\,\sqrt{2\,\mathrm{KL}\bigl(p^{\theta}\,\|\,p^{\hat{\theta}}\bigr)}.
\]
Further given the assumption that $p^{\hat{\theta}}$ is close to $p^{\theta}$ such that $\mathrm{KL}(p^{\theta} \,\|\, p^{\hat{\theta}})\leq 1$, we have 
$$
|V^{\hat{\theta}} - V^{\theta}|\leq C_5\cdot \sqrt{\mathrm{KL}(p^{\theta} \,\|\, p^{\hat{\theta}})}.
$$
Further applying the same argument in Theorem \ref{thm:Regularization as KL bound} proves \eqref{Direct Performance Difference}. To prove \eqref{Local Approximation Difference}, it suffices to bound 
\begin{eqnarray}
\mathbb{E}\int _ { 0 } ^ { T } \frac{\pi^ { \hat{\theta}}( a _ { t } ^ {\theta} | t , X _ { t } ^ {\theta} )}{\pi^ { \theta}( a _ { t } ^ {\theta} | t , X _ { t } ^ {\theta} )} q(t, X_t^{\theta}, a_t^{\theta} ; \pi^\theta)\mathrm{d} t
\end{eqnarray}
By importance sampling, we have
\[
\mathbb{E}_{\theta}\!\Bigl[
  \frac{\pi^{\hat{\theta}}(a_{t}^{\theta}\mid t, X_{t}^{\theta})}%
       {\pi^{\theta}(a_{t}^{\theta}\mid t, X_{t}^{\theta})}
  \,q(t, X_t^{\theta}, a_t^{\theta} ; \pi^\theta)
\Bigr]
\;=\;
\mathbb{E}_{\hat{\theta}}[\,q(t, X_t^{\theta}, a_t^{\hat{\theta}} ; \pi^\theta)\,].
\]
Hence the left side equals
$\int_0^T \mathbb{E}_{\hat{\theta}}[\,q(\cdots)\,]\mathrm{d}t$.
Assume that $|q|\le M$, we get
\[
\bigl|\mathbb{E}_{\hat{\theta}}[q] - \mathbb{E}_\theta[q]\bigr|
\;\le\;
M\,\|\pi^{\hat{\theta}} - \pi^\theta\|_1
\;\le\;
M\,\sqrt{2\,\mathrm{KL}(\pi^\theta\,\|\,\pi^{\hat{\theta}})},
\]
by Pinskers inequality.  Therefore,
\[
\mathbb{E}_{\hat{\theta}}[q(\cdots)]
\;\le\;
\mathbb{E}_\theta[q(\cdots)]
\;+\;
M\,\sqrt{2\,\mathrm{KL}(\pi^\theta\,\|\,\pi^{\hat{\theta}})}.
\]
Integrate over $t$ from $0$ to $T$, and the result follows that:
\begin{align}
|L^{\theta}(\hat{\theta}) - V^{\theta}|\leq C_6 \cdot \mathbb{E}\int_{0}^{T} \sqrt{\operatorname{KL}(\pi^{\theta}(\cdot | t , X _ { t } ^ {\theta} )\|\pi^{\hat{\theta}}( \cdot | t , X _ { t } ^ {\theta} ))}\mathrm{d}t \leq C_7 \left(\mathbb{E}\int_{0}^{T} \operatorname{KL}(\pi^{\theta}(\cdot | t , X _ { t } ^ {\theta} )\|\pi^{\hat{\theta}}( \cdot | t , X _ { t } ^ {\theta} ))\mathrm{d}t\right)^{\frac{1}{2}}
\end{align}
where the last inequality follows from Jensen's ineqality and Cauchy-Schwarz inequality.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{More Experimental Details}
\subsection{Algorithm Pseudocode}
\label{app:alg pseudo-code}

We present the algorithm pseudocode as below.

\begin{algorithm}[!htbp]
   \caption{CTRL for Diffusion Models}
   \label{alg:ppo-diffusion}
\begin{algorithmic}
   \STATE {\bfseries Input:} Initial policy parameters $\theta_0$, value function parameters $\phi_0$ (after pretraining), clip parameter $\epsilon$, learning rates $\alpha_\theta$, $\alpha_\phi$, number of epochs $K$, number of trajectories $N$, a fixed exploration level $\sigma$, number of exploration actions $M$, scaling parameter $\eta$.
   \FOR{$n = 0, 1, 2, \ldots$}
   \STATE Collect $N$ trajectories $\{(t_i, X_{t_i}^{\theta_n}, a_{t_i}^{\theta_n}, r_{t_i}^{\theta_n})\}_{i=1}^N$ by running policy $\pi^{\theta_n}$
   \STATE Compute returns $\hat{R}_i = \sum_{t=t_i}^T r_t^{\theta_n}$ for each trajectory
   \STATE Initialize value function dataset $\mathcal{D}_V = \{(t_i, X_{t_i}^{\theta_n}, \hat{R}_i)\}$
   
   \STATE \textit{\# Train value function}
   \FOR{$k = 1, 2, \ldots, K$}
   \STATE Sample batch $\mathcal{B}_V \subset \mathcal{D}_V$
   \STATE Update $\phi_{n+1} \leftarrow \phi_n - \alpha_\phi \nabla_\phi \frac{1}{|\mathcal{B}_V|} \sum_{(t,X_t,\hat{R}) \in \mathcal{B}_V} (V^{\phi}(t, X_t) - \hat{R})^2$
   \ENDFOR
   
   \STATE \textit{\# Compute advantage rate function estimates}
   \FOR{each $(t_i, X_{t_i}^{\theta_n}, a_{t_i}^{\theta_n})$ in collected trajectories}
   \STATE Sample $M$ samples of random noise $\epsilon_j\sim\mathcal{N}(0,I)$.
   \STATE Compute $M$ pseudo samples $a_{t_i,j}^{\theta_n}=a_{t_i}^{\theta_n}+\sigma\epsilon_j$.
   \STATE Compute advantage $q_{t_i,j}^{\theta_n}=\left(V(t_i, X_{t_i}^{\theta_n}+\eta\, g^2(T-t) \epsilon_j)-V(t_i, X_{t_i}^{\theta_n})\right)/\eta$
   \ENDFOR
   
   \STATE Initialize policy optimization dataset $\mathcal{D}_\pi = \{(t_i, X_{t_i}^{\theta_n}, a_{t_i,j}^{\theta_n}, q_{t_i,j}^{\theta_n})\}$
   
   \STATE \textit{\# Update policy using PPO objective}
   \FOR{$k = 1, 2, \ldots, K$}
   \STATE Sample batch $\mathcal{B}_\pi \subset \mathcal{D}_\pi$
   \STATE Compute likelihood ratios $\rho_{t,j}^{\theta} = \frac{\pi^{\theta}(a_{t,j}^{\theta_n}|t, X_t^{\theta_n})}{\pi^{\theta_n}(a_{t,j}^{\theta_n}|t, X_t^{\theta_n})}$ for all $(t, X_t^{\theta_n}, a_{t,j}^{\theta_n}, q_{t,j}^{\theta_n}) \in \mathcal{B}_\pi$
   \STATE Compute clipped objective:
   \STATE $L(\theta) = \frac{1}{|\mathcal{B}_\pi|} \sum_{(t, X_t, a_{t,j}, q_{t,j}) \in \mathcal{B}_\pi} \min(\rho_{t,j}^{\theta} q_{t,j}^{\theta_n}, \text{clip}(\rho_{t,j}^{\theta}, 1-\epsilon, 1+\epsilon) q_{t,j}^{\theta_n})$
   \STATE Update $\theta \leftarrow \theta + \alpha_\theta \nabla_\theta L(\theta)$
   \ENDFOR
   \STATE $\theta_{n+1} \leftarrow \theta$
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
