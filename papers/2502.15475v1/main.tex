\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{amsmath,amsfonts}
\usepackage{booktabs}
\usepackage{enumerate}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Convolutional Neural Engine for Decoding Punctured Convolutional and Turbo Codes}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning Solution for Protocols Compliance}

\author{
  Yongli Yan \\
  Department of Electronic Engineering \\
  Tsinghua University \\
  \texttt{yanyongli@tsinghua.edu.cn} \\
  \And
  Linglong Dai \\
  Department of Electronic Engineering \\
  Tsinghua University \\
  \texttt{daill@tsinghua.edu.cn} \\
}

\newcounter{magicrownumbers}
\newcommand\rownumber{\stepcounter{magicrownumbers}\arabic{magicrownumbers}}

\begin{document}
\maketitle


\begin{abstract}
Neural network-based decoding methods have shown promise in enhancing error correction performance, but traditional approaches struggle with the challenges posed by punctured codes. In particular, these methods fail to address the complexities of variable code rates and the need for protocol compatibility. This paper presents a unified Long Short-Term Memory (LSTM)-based decoding architecture specifically designed to overcome these challenges. The proposed method unifies punctured convolutional and Turbo codes. A puncture embedding mechanism integrates puncturing patterns directly into the network, enabling seamless adaptation to varying code rates, while balanced bit error rate training ensures robustness across different code lengths, rates, and channels, maintaining protocol flexibility. Extensive simulations in Additive White Gaussian Noise and Rayleigh fading channels demonstrate that the proposed approach outperforms conventional decoding techniques, providing significant improvements in decoding accuracy and robustness. These results underscore the potential of LSTM-based decoding as a promising solution for next-generation artificial intelligence powered communication systems.
\end{abstract}

% \begin{IEEEkeywords}
% Deep Learning, Long Short-Term Memory, Error Correction Code, Convolutional Code, Turbo Code, IEEE 802.11, 3GPP TS 36.212.
% \end{IEEEkeywords}

\section{Introduction}
Artificial Intelligence (AI) has demonstrated exceptional capabilities in various fields, such as natural language processing (e.g., ChatGPT) \cite{chatgpt} and computer vision \cite{ml_for_cv}, driving breakthroughs in applications previously dominated by hand-engineered methods. These advances in AI have not only revolutionized traditional domains but also sparked interest in its application to the evolving telecommunications industry. With the ongoing transition towards 6th Generation (6G) networks, AI’s nonlinear modeling capabilities are being explored to enhance wireless communication systems. Recognizing its potential, the 3rd Generation Partnership Project (3GPP) has established the AI-for-RAN working group in 2022 \cite{3gpp_tr37817}, focusing on incorporating AI into the Radio Access Network (RAN) to improve key performance metrics such as efficiency and capacity.


\subsection{Prior Works}
In wireless communication, AI has shown promise in physical layer signal processing tasks such as channel estimation, signal detection, and channel decoding \cite{ml_ch_est0, ml_ch_est1, ml_mimo_det0, ml_mimo_det1, ml_ch_dec0, ml_ch_dec1}. Traditionally, signal processing algorithms were implemented on Central Processing Units (CPUs), Digital Signal Processors (DSPs), or Application-Specific Integrated Circuits (ASICs), which rely on \textit{serial processing}. However, the highly \textit{parallel nature} of AI algorithms requires more efficient architectures, driving the shift from CPU-based signal processing algorithms to Graphics Processing Units (GPUs), which are optimized for parallel computation and can efficiently handle large-scale, simultaneous operations.

As a pioneer in this shift and a leading designer of GPUs, NVIDIA has restructured cellular wireless network receivers using AI. For instance, NVIDIA developed the Sionna signal processing AI library \cite{sionna_lib} and used it to create multi-user, real-time Neural Network (NN) receivers compatible with the 5th Generation New Radio (5G NR) protocol \cite{mumimo_5gnr_sionna, std_5gnr_sionna}. While AI has been successfully applied to channel estimation, signal detection, and demodulation in NVIDIA’s work, integrating neural network-based decoders into the receiver presents significant challenges. One major issue is the generalization of neural network decoders, particularly when applied to varying code rates, which can limit their performance and flexibility in diverse scenarios \cite{6g_unify_decoder}.

Puncturing, which discards part of the encoded data to form different code rates and improve spectral efficiency, is essential in real-world wireless communication systems. Control channels typically employ lower code rates for high reliability, while data channels use more flexible and higher code rates to accommodate diverse transmission conditions and maximize throughput. Linear block codes (e.g., Low-Density Parity-Check (LDPC) and Polar codes) and sequential codes (e.g., convolutional and Turbo codes) are widely used in commercial communication protocols \cite{ldpc_code, polar_code, conv_code, turbo_code}. For example, Wi-Fi protocols \cite{ieee_80211_std} support four code rates for convolutional and LDPC codes, while cellular protocols \cite{3gpp_36212_std, 3gpp_38212_std} utilize Polar codes for control channels with dozens of code rates, and Turbo and LDPC codes for data channels with over a hundred code rates.

Recent studies have proposed neural network-based decoders to address puncturing in linear block codes, demonstrating improvements over traditional methods. For instance, \cite{lbc_ecct} employed a Transformer-based network to decode linear block codes with varying code rates, while \cite{lbc_uecct} introduced a unified Transformer decoder capable of simultaneously decoding multiple code rates of linear block codes with a single set of neural network parameters.

Other studies have focused on applying neural networks to decode sequential codes, such as convolutional and Turbo codes. For example, \cite{comm_alg_via_dl} explored the use of Recurrent Neural Networks (RNNs) for decoding convolutional codes, achieving performance comparable to the Viterbi algorithm \cite{viterbi_alg_1967} in both AWGN and non-AWGN channels with $t$-distributed noise. The DeepTurbo approach \cite{deep_turbo}, inspired by the iterative Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm \cite{bcjr_alg_1974}, targets Turbo codes but faces performance degradation when generalized to longer code lengths, requiring retraining. A notable study \cite{mind_model_independent} employed model-agnostic meta-learning to enhance the generalization of neural network-based decoders in unseen channel conditions. Additionally, Turbo Autoencoders \cite{turbo_autoencoder} introduced an end-to-end learning framework that jointly optimizes both the encoder and decoder, outperforming traditional BCJR decoders.

However, these studies on convolutional and Turbo codes have not addressed the issue of puncturing, limiting their applicability in real-world communication systems. In the context of NN-based decoders, models trained without considering puncturing may fail when exposed to such conditions. This is because the model's parameters are optimized for scenarios without punctured data, making it less robust in real-world applications where puncturing is common. Furthermore, training a separate neural network for each possible code rate leads to significant storage overhead, which is impractical for scalable deployment in dynamic environments.


\subsection{Contributions}
To address this gap of puncturing issues in sequential codes, we propose a unified, protocol-compatible Long Short-Term Memory (LSTM)-based neural decoder for punctured convolutional and Turbo codes, inspired by the inherent similarity between convolutional coding, which transmits information through state transitions in shift registers, and the information propagation across time steps in LSTM networks. The proposed architecture encodes puncturing patterns directly into the neural network's latent space and introduces a Balanced Bit Error Rate Training (BBT) method for efficient fine-tuning across different code rates, ensuring seamless compatibility with protocol flexibility \footnote{Simulation codes will be provided to reproduce the results in this paper: \url{http://oa.ee.tsinghua.edu.cn/dailinglong/publications/publications.html}}. The main contributions of this work are summarized as follows.

\begin{enumerate}
\item \textbf{Protocol-Compatible Neural Decoding}:
We propose a unified protocol-compatible neural decoding framework specifically designed for punctured convolutional and Turbo codes. This framework ensures compliance with Wi-Fi (IEEE 802.11) and cellular (3GPP TS 36.212) standards by supporting varying code lengths and rates, even under practical puncturing patterns. The protocol compatibility is achieved through \textit{puncture embedding} and \textit{balanced bit error rate training}, which enables the decoder to handle different code rates effectively. To the best of our knowledge, this is the first neural decoding approach capable of generalizing seamlessly across diverse protocol-compliant configurations while maintaining competitive performance.

\item \textbf{Puncture Embedding for Adaptability:}
We introduce a puncture embedding module that encodes puncturing patterns directly into the neural network’s latent space, enabling seamless adaptation to various code rates. By incorporating the puncture embedding as a gating mechanism, the flow of Log-Likelihood Ratio information is controlled. This design allows the decoder to support diverse code lengths and rates with a single set of network parameters, ensuring exceptional generalization and compatibility with dynamically changing protocol requirements, making it highly suitable for modern wireless communication systems where adaptability is critical.

\item \textbf{Balanced Bit Error Rate Training}:
To further enhance flexibility and prevent overfitting to any specific code rate, we propose a Balanced Bit Error Rate Training strategy. This method adjusts the Signal-to-Noise Ratio (SNR) for different code rates to maintain a consistent Bit Error Rate (BER) across all settings during training. As a result, each code rate contributes equally, helping the neural network generalize effectively without overfitting to any particular configuration.

\item \textbf{Superior Performance in Practical Channels}:
The proposed neural decoder demonstrates state-of-the-art performance in both Additive White Gaussian Noise and Rayleigh fading channels. Extensive simulations show that the model not only outperforms traditional decoding algorithms under practical Least Squares (LS) channel estimation but also exceeds the performance of conventional decoders with Perfect Channel State Information (PCSI).

\end{enumerate}

This paper is structured as follows. Section II offers foundational knowledge, covering the basics of convolutional codes, Turbo codes, and Long Short-Term Memory networks. Section III describes the proposed Convolutional Neural Engine in detail. Section IV discusses the outcomes of training and inference experiments. Section V evaluates the computational complexity and decoding latency of the proposed architecture. Finally, Section VI provides a conclusion.

\textit{Notations}: Bold lowercase and uppercase letters are used to represent vectors and matrices, respectively. The symbol $\mathbb{R}$ represents the set of real numbers, while $\mathbb{C}$ denotes the set of complex numbers. The transpose operation is denoted by $[\cdot]^T$, and $[\cdot]^H$ represents the Hermitian (conjugate transpose) operation. The notation $\mathcal{N}(\mu, {\sigma}^2)$ denotes a Gaussian distribution with mean $\mu$ and variance ${\sigma}^2$.



\section{Background}
To provide a comprehensive understanding of the proposed Convolutional Neural Engine, this section presents background knowledge on convolutional codes, Turbo codes, and Long Short-Term Memory neural networks, along with an overview of the simulation link architecture and channel model used in the system.


\subsection{Fundamentals of Convolutional and Turbo Codes}

\subsubsection{Convolutional Codes in IEEE 802.11}
\
\newline
\indent
IEEE 802.11 mandates convolutional coding as a required feature for both Access Points (APs) and Stations (STAs) in the physical layer (PHY), specifically within the Orthogonal Frequency Division Multiplexing (OFDM) modulation schemes. The standard supports convolutional codes with rates of 1/2, 2/3, 3/4, and 5/6 for Forward Error Correction (FEC) to mitigate bit errors caused by noise, fading, and interference.
\begin{equation}
\label{eq_bcc_gen_poly}
\begin{aligned}
G = \left[ {{g_0} = {{133}_8},{g_1} = {{171}_8}} \right]
\end{aligned}
\end{equation}

\begin{figure}[htb]
\centering
\includegraphics[width=252pt]{fig/fig_bcc_encoder.pdf}
\caption{Encoding structure of the convolutional code in IEEE 802.11.}
\label{fig_bcc_encoder}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=252pt]{fig/fig_bcc_puncture.pdf}
\caption{Example of the bit-stealing procedure in IEEE 802.11 (R = 2/3, 3/4, 5/6).}
\label{fig_bcc_puncture}
\end{figure}

The convolutional code is characterized by its encoder structure, where the input bit stream passes through a series of shift registers, generating encoded bits as output. Each output bit is determined by the current input bit and several previous input bits, as defined by the constraint length of the code. The constraint length represents the number of bits stored in the encoder's memory that influence the current output. In IEEE 802.11, the convolutional encoder operates with a mother code rate of 1/2 and a constraint length of 7. This setup produces two output bits for each input bit, with the constraint length indicating that the output depends on the current bit and the six preceding bits. Figure \ref{fig_bcc_encoder} provides a schematic illustration of the convolutional encoder used in IEEE 802.11, showcasing its encoding process and structure. The bit denoted as "A" shall be output from the encoder before the bit denoted as "B". The specific generator polynomials governing this encoding process, expressed in octal form, are provided in Eq. \ref{eq_bcc_gen_poly}. Higher code rates, such as 2/3, 3/4, and 5/6, are achieved using a technique known as puncturing. Puncturing selectively removes specific encoded bits to increase the effective code rate without altering the encoder's fundamental structure. Figure \ref{fig_bcc_puncture} illustrates how puncturing is applied in IEEE 802.11 to achieve these higher code rates.

% The Viterbi algorithm is commonly used for decoding convolutional codes in IEEE 802.11. This algorithm efficiently decodes the received sequence by finding the most likely transmitted sequence, based on the maximum likelihood criterion.


\subsubsection{Turbo Codes in 3GPP TS 36.212}
\
\newline
\indent
Turbo codes, introduced in the late 1990s, represent a significant advancement in error correction techniques. These codes are used in the 3rd Generation Partnership Project (3GPP) standards, specifically in the 3GPP TS 36.212 specification for Long-Term Evolution (LTE) and beyond.
\begin{equation}
\label{eq_turbo_gen_poly}
\begin{aligned}
G\left( D \right) = \left[ {1,\frac{{{g_1}\left( D \right)}}{{{g_0}\left( D \right)}}} \right]{,_{}}\left\{ {\begin{array}{*{20}{l}}
{{g_0}\left( D \right) = 1 + {D^2} + {D^3}}\\
{{g_1}\left( D \right) = 1 + D + {D^3}}
\end{array}} \right.
\end{aligned}
\end{equation}

\begin{figure}[htb]
\centering
\includegraphics[width=252pt]{fig/fig_turbo_encoder.pdf}
\caption{Encoding structure of the Turbo codes in 3GPP TS 36.212.}
\label{fig_turbo_encoder}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=252pt]{fig/fig_turbo_rate_matching.pdf}
\caption{Example of the rate matching procedure in 3GPP TS 36.212.}
\label{fig_turbo_rate_matching}
\end{figure}

In 3GPP TS 36.212, Turbo codes are used in the downlink and uplink channels for the physical layer, where they provide strong error correction capabilities. The standard adopts Parallel Concatenated Convolutional Codes (PCCC) as the foundation of Turbo coding. Figure \ref{fig_turbo_encoder} illustrates the encoding process of Turbo codes, which consists of two parallel convolutional encoders, each with a constraint length of 5. The first encoder processes the input bitstream directly, while the second operates on an interleaved version of the input, with the interleaver introducing randomness to the input sequence. The specific generator polynomials for these encoders are detailed in Equation \ref{eq_turbo_gen_poly}.

The mother code rate of Turbo codes is 1/3, producing three output bits for each input bit. To support higher data rates, the effective code rate can be increased up to 0.93 through puncturing, which selectively omits specific encoded bits. This enables flexibility in adapting to different data rate requirements while maintaining error correction capabilities. The intermediate code rates between the mother code rate and the maximum rate are achieved by adjusting the allocation of time-frequency physical resources based on Channel Quality Indicators (CQIs) reported by the user. This adaptive process, known as rate matching, enables efficient code rate adjustment to ensure optimal performance under varying channel conditions.

Figure \ref{fig_turbo_rate_matching} provides a comprehensive overview of the rate matching process, which adapts the encoded data to the available physical resources. This process begins with the Transport Block (TB) being divided into Code Blocks (CBs) after appending a 24-bit Cyclic Redundancy Check (CRC) using CRC24A for error detection. Each CB then adds another 24-bit CRC24B and is subsequently processed by the Turbo encoder. The encoded CBs undergo sub-block interleaving, which rearranges the data to improve resilience against burst errors. The interleaved code blocks are then processed through a circular buffer, which facilitates the rate matching procedure by selecting and concatenating bits as per the allocated resource block size. This process is crucial for aligning the data to the physical layer's constraints while ensuring efficient utilization of resources.

% The decoding of Turbo codes in 3GPP 36.212 is typically performed using the Turbo decoding algorithm, which is based on the Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm. This algorithm performs iterative decoding, where the decoder alternates between decoding the two parallel convolutional codes and exchanging soft information between the two decoders. This iterative process significantly improves the decoding performance compared to a non-iterative decoding approach.


\subsection{Long Short-Term Memory Neural Network}
Long Short-Term Memory networks \cite{lstm_network}, a variant of Recurrent Neural Networks \cite{rnn_network}, are specifically designed to model sequential data and capture long-term dependencies effectively. As shown in Figure \ref{fig_rnn_structure}, RNNs employ a cyclic structure that allows information to persist across time steps $t$, enabling sequential data processing. The update of the hidden state $h_t$ in an RNN is governed by:
\begin{equation}
\label{eq_rnn}
\begin{aligned}
\boldsymbol{h}_t = \tanh \left( \boldsymbol{W}_h \boldsymbol{h}_{t-1} + \boldsymbol{W}_x \boldsymbol{x}_t + \boldsymbol{b}_h \right)
\end{aligned}
\end{equation}
where $\boldsymbol{h}_t$ is the hidden state at time step $t$, $\boldsymbol{h}_{t-1}$ is the hidden state from the previous time step, $\boldsymbol{x}_t$ is the input at time step $t$, and $\boldsymbol{W}_h$, $\boldsymbol{W}_x$, $\boldsymbol{b}_h$ are learnable parameters. This cyclic nature of RNNs makes them well-suited for capturing temporal dependencies in sequential data. However, traditional RNNs encounter challenges during training, particularly vanishing and exploding gradient issues, which hinder their ability to capture long-range dependencies.


\begin{figure}[htb]
\centering
\includegraphics[width=252pt]{fig/fig_rnn_structure.pdf}
\caption{The unrolled recurrent neural network.}
\label{fig_rnn_structure}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=252pt]{fig/fig_lstm_structure.pdf}
\caption{The architecture of an LSTM cell.}
\label{fig_lstm_structure}
\end{figure}

LSTMs, introduced by Hochreiter and Schmidhuber in 1997 \cite{lstm_nn}, overcome these limitations through an innovative memory cell structure. Unlike the simpler architecture of RNNs, LSTMs incorporate a gating mechanism to dynamically regulate information flow, enabling the network to selectively retain, update, or discard information. The architecture of an LSTM cell, illustrated in Figure \ref{fig_lstm_structure}, includes three core gates: the forget gate, input gate, and output gate. These gates are mathematically defined as follows:
\begin{equation}
\label{eq_lstm}
\begin{aligned}
\begin{array}{l}
\boldsymbol{f}_t = \sigma \left( \boldsymbol{W}_f \cdot \begin{bmatrix} \boldsymbol{h}_{t-1} , \boldsymbol{x}_t \end{bmatrix} + \boldsymbol{b}_f \right) \\

\boldsymbol{i}_t = \sigma \left( \boldsymbol{W}_i \cdot \begin{bmatrix} \boldsymbol{h}_{t-1} , \boldsymbol{x}_t \end{bmatrix} + \boldsymbol{b}_i \right) \\

\boldsymbol{o}_t = \sigma \left( \boldsymbol{W}_o \cdot \begin{bmatrix} \boldsymbol{h}_{t-1} , \boldsymbol{x}_t \end{bmatrix} + \boldsymbol{b}_o \right) \\

\tilde{\boldsymbol{c}}_t = \tanh \left( \boldsymbol{W}_c \cdot \begin{bmatrix} \boldsymbol{h}_{t-1} , \boldsymbol{x}_t \end{bmatrix} + \boldsymbol{b}_c \right) \\

\boldsymbol{c}_t = \boldsymbol{f}_t \odot \boldsymbol{c}_{t-1} + \boldsymbol{i}_t \odot \tilde{\boldsymbol{c}}_t \\

\boldsymbol{h}_t = \boldsymbol{o}_t \odot \tanh \left( \boldsymbol{c}_t \right)
\end{array}
\end{aligned}
\end{equation}
where $\boldsymbol{f}_t$, $\boldsymbol{i}_t$, and $\boldsymbol{o}_t$ represent the forget, input, and output gate activations at time step $t$; $\boldsymbol{x}_t$ is the input;  $\tilde{\boldsymbol{c}}_t$ is the candidate memory cell; $\boldsymbol{c}_t$ is the cell state; and $\boldsymbol{h}_t$ is the hidden state.

Here, $\sigma$ denotes the sigmoid activation function, which plays a critical gating role by producing values in the range of $[0, 1]$. This enables $\sigma$ to act as a "soft switch" that determines the degree to which information should be passed through the network. For instance, in the forget gate $\boldsymbol{f}_t$, $\sigma$ dynamically adjusts the proportion of the previous cell state $\boldsymbol{c}_{t-1}$ that should be retained. Similarly, in the input gate $\boldsymbol{i}_t$ and output gate $\boldsymbol{o}_t$, $\sigma$ controls how much new information is incorporated into the cell state and how much of the cell state influences the hidden state $\boldsymbol{h}_t$, respectively. By gating these information flows, $\sigma$ ensures that the network can focus on relevant inputs and maintain stable gradients during training, allowing LSTMs to effectively capture both short- and long-term dependencies.

% The ability of LSTMs to capture long-term dependencies makes them particularly suitable for decoding tasks in communication systems, such as convolutional and Turbo codes. These systems generate encoded data that inherently exhibit temporal and structural correlations, aligning closely with LSTM's core strengths. Unlike traditional decoding algorithms, which may struggle to fully utilize these correlations, LSTMs can naturally model the sequential dependencies present in such data. For instance, LSTMs interpret convolutionally encoded sequences as time-series data, uncovering features that significantly enhance decoding performance.


\subsection{System Model}
The proposed system is evaluated under two distinct communication channel models: the AWGN channel and the Rayleigh fading channel. The AWGN channel serves as the training and inference environment, providing an idealized setting for model training. In contrast, the Rayleigh fading channel is used solely for inference, allowing us to assess how well the model, trained in the AWGN channel, generalizes to more realistic fading conditions.

In the AWGN channel, the communication link follows the sequence illustrated in Figure \ref{fig_awgn_rayleigh_channel_model}, where $K$ denotes the length of the original message, $N$ represents the length of the encoded message, and $E$ refers to the length of the message after rate-matching. The code rate $R$ of the encoded message is defined as the ratio of the message length to the rate-matched length, i.e., $R = \frac{K}{E}$.

\begin{figure*}[htb]
\centering
\includegraphics[width=1.0\textwidth]{fig/fig_awgn_rayleigh_channel_model.pdf}
\caption{The communication link of the AWGN and Rayleigh fading channel.}
\label{fig_awgn_rayleigh_channel_model}
\end{figure*}

% \begin{figure}[htb]
% \centering
% \includegraphics[width=252pt]{fig/fig_awgn_rayleigh_channel_model.pdf}
% \caption{The communication link of the AWGN and Rayleigh fading channel.}
% \label{fig_awgn_rayleigh_channel_model}
% \end{figure}

The transmitted signal $\boldsymbol{x}$ is modulated using Binary Phase Shift Keying (BPSK), and the received signal $\boldsymbol{y}$ is corrupted by additive Gaussian noise $\boldsymbol{n}$. The relationship between the transmitted and received signals is expressed as:
\begin{equation}
\label{eq_awgn_model}
\begin{aligned}
\boldsymbol{y} = \boldsymbol{x} + \boldsymbol{n}
\end{aligned}
\end{equation}
where $\boldsymbol{n} \sim \mathcal{N}(0, \sigma^2)$ represents the AWGN noise. After demodulation and de-rate matching, the signal is decoded to obtain the transmitted message.

In contrast, the Rayleigh fading channel is utilized for inference to simulate more realistic wireless communication conditions, where the transmitted signal experiences multipath fading. The communication link for the Rayleigh fading channel is detailed in Figure \ref{fig_awgn_rayleigh_channel_model}, which illustrates the multipath fading model and frequency-domain processing.

The received signal in the Rayleigh fading channel is modeled as:
\begin{equation}
\label{eq_rayleigh_model}
\begin{aligned}
\boldsymbol{y} = \boldsymbol{H} \boldsymbol{x} + \boldsymbol{n}
\end{aligned}
\end{equation}
where $\boldsymbol{H} \in \mathbb{C}^{N_R \times N_T}$ is the channel matrix representing the Rayleigh fading coefficients, with $N_R$ denoting the number of receive antennas and $N_T$ the number of transmit antennas. $\boldsymbol{x} \in \mathbb{C}^{N_T \times 1}$ is the transmitted signal vector in the frequency domain after modulation (e.g., OFDM subcarriers). $\boldsymbol{n} \in \mathbb{C}^{N_R \times 1}$ is the noise vector at the receiver, where each element is i.i.d. Gaussian noise with zero mean and variance $\sigma^2$.

The channel matrix $\boldsymbol{H}$ is constructed by generating multipath coefficients in the time domain, based on an $L$-tap delay profile determined by environmental characteristics and delay spread, and then transformed into the frequency domain via the Fast Fourier Transform (FFT) to be applied to $\boldsymbol{x}$. Each tap of $\boldsymbol{H}$ is modeled as an independent complex Gaussian random variable, and the total power of the $L$ taps is normalized to 1, i.e., $\sum_{l=0}^{L-1} \mathbb{E}[|h_l|^2] = 1$, where $h_l$ represents the coefficient of the $l$-th tap.

Channel estimation is performed to estimate $\boldsymbol{H}$, and Multiple Input Multiple Output (MIMO) detection is used to recover the transmitted signal $\boldsymbol{x}$ after it has been affected by multipath fading. The subsequent steps, including demodulation, de-rate matching, and decoding, are similar to those in the AWGN case, with the goal of recovering the transmitted message.

By training the model on the AWGN channel and testing it on the Rayleigh fading channel, we evaluate how well the decoder generalizes to real-world, challenging environments.
% The Rayleigh fading channel introduces significant multipath effects, making it a realistic test of the decoder's robustness.


\section{Proposed Convolutional Neural Engine}
In this section, we present the complete architecture and training process of the proposed \textbf{Convolutional Neural Engine (CNE)}.


\subsection{Overall Architecture}
The proposed Convolutional Neural Engine introduces a groundbreaking approach to decoding convolutional codes, particularly under punctured conditions, by effectively integrating domain-specific insights into a deep learning framework. Unlike conventional NN-based methods, which often treat punctured sequences as missing data, CNE incorporates an innovative \textbf{Puncture Embedding} mechanism that explicitly encodes puncturing patterns into the feature space. This approach enhances the decoder's adaptability, enabling it to generalize across varying code rates specified by protocols and ensuring compatibility for practical applications. The architecture of the proposed CNE is illustrated in Figure \ref{fig_bcc_decoder_structure}.

\begin{figure*}[htb]
\centering
\includegraphics[width=1.0\textwidth]{fig/fig_bcc_decoder_structure.pdf}
\caption{The architecture of the proposed Convolutional Neural Engine.}
\label{fig_bcc_decoder_structure}
\end{figure*}


\subsubsection{Convolutional Decoding}
\
\newline
\indent
At the core of the CNE architecture lies a sequence of operations that transform raw input data and puncturing patterns into actionable decoding outputs. The process begins with a projection of the input sequence, $\boldsymbol{x} \in \mathbb{R}^{K \times D_{\text{in}}}$, into a higher-dimensional embedding space:
\begin{equation}
\label{eq_lstm_input_embedding}
\begin{aligned}
\boldsymbol{e} = \boldsymbol{W}_{\text{proj}} \boldsymbol{x} + \boldsymbol{b}_{\text{proj}}
\end{aligned}
\end{equation}
where $\boldsymbol{W}_{\text{proj}} \in \mathbb{R}^{D_{\text{embed}} \times D_{\text{in}}}$ and $\boldsymbol{b}_{\text{proj}} \in \mathbb{R}^{D_{\text{embed}}}$ are learnable parameters, $K$ represents the sequence length, and $D_{\text{in}}$ denotes the dimensionality of each input vector in the sequence. Specifically, for convolutional codes as defined in IEEE 802.11, $D_{\text{in}} = 2$, corresponding to two input symbols per time step. Here, $\boldsymbol{x}$ represents the de-punctured data, with zeros inserted at the positions of punctured bits to restore the full sequence structure.

To incorporate puncturing information, a \textbf{Puncture Embedding} module maps the puncturing pattern $\boldsymbol{p} \in \mathbb{R}^{K \times D_{\text{in}}}$ into the same embedding space. This is achieved using:
\begin{equation}
\label{eq_lstm_puncture_embedding_0}
\begin{aligned}
\boldsymbol{e}_{\text{p}} = \sigma \left( \boldsymbol{W}_{\text{punc}} \boldsymbol{p} + \boldsymbol{b}_{\text{punc}} \right)
\end{aligned}
\end{equation}
where $\boldsymbol{W}_{\text{punc}} \in \mathbb{R}^{D_{\text{embed}} \times D_{\text{in}}}$, $\boldsymbol{b}_{\text{punc}} \in \mathbb{R}^{D_{\text{embed}}}$, and $\sigma(\cdot)$ is the sigmoid activation function, which ensures that the puncture information acts as a gate, controlling the flow of puncturing effects.

The input embedding $\boldsymbol{e}$ and puncture embedding $\boldsymbol{e}_{\text{p}}$ are then combined through element-wise multiplication:
\begin{equation}
\label{eq_lstm_puncture_embedding_1}
\begin{aligned}
\boldsymbol{e}_{\text{final}} = \boldsymbol{e} \odot \boldsymbol{e}_{\text{p}}
\end{aligned}
\end{equation}
allowing the model to dynamically adjust the input representation based on the puncturing pattern.

Following this, the combined embedding $\boldsymbol{e}_{\text{final}}$ undergoes batch normalization to stabilize training:
\begin{equation}
\label{eq_lstm_bn}
\begin{aligned}
\boldsymbol{e}_{\text{norm}} = \text{BN}(\boldsymbol{e}_{\text{final}})
\end{aligned}
\end{equation}

The normalized embedding is then processed by a multi-layer bidirectional LSTM, which captures both forward and backward temporal dependencies in the sequence:
\begin{equation}
\label{eq_lstm_core}
\begin{aligned}
\boldsymbol{h} = \text{LSTM}(\boldsymbol{e}_{\text{norm}})
\end{aligned}
\end{equation}
where $\boldsymbol{h} \in \mathbb{R}^{K \times 2D_{\text{hidden}}}$ represents the encoded context of the entire sequence, leveraging information from both directions.

Finally, the output from the LSTM is passed through a fully connected layer to produce the decoded bit likelihoods:
\begin{equation}
\label{eq_lstm_output}
\begin{aligned}
\boldsymbol{y} = \boldsymbol{W}_{\text{out}} \boldsymbol{h} + \boldsymbol{b}_{\text{out}}
\end{aligned}
\end{equation}
where $\boldsymbol{W}_{\text{out}} \in \mathbb{R}^{2D_{\text{hidden}} \times 1}$, and $\boldsymbol{y} \in \mathbb{R}^{K \times 1}$ represents the likelihoods of the estimated bits being 1.

The entire decoding process can be summarized as:
\begin{equation}
\label{eq_lstm_total_process}
\begin{aligned}
\boldsymbol{y} = \boldsymbol{W}_{\text{out}} \cdot \text{LSTM} \Big( \text{BN} \Big( \big( \boldsymbol{W}_{\text{proj}} \boldsymbol{x} + \boldsymbol{b}_{\text{proj}} \big) \odot \sigma \big( \boldsymbol{W}_{\text{punc}} \boldsymbol{p} + \boldsymbol{b}_{\text{punc}} \big) \Big) \Big) + \boldsymbol{b}_{\text{out}}
\end{aligned}
\end{equation}

This unified formulation captures the seamless interaction among input embedding, puncture information, temporal modeling, and decoding. The proposed architecture's explicit incorporation of puncturing patterns into the decoding process distinguishes it from traditional NN-based methods, demonstrating superior adaptability and performance in decoding convolutional codes.

\subsubsection{Turbo Decoding}
\
\newline
\indent
The proposed LSTM-based Turbo decoder builds upon the principles of traditional BCJR Turbo decoding, employing two identical CNEs as its core components. These CNEs are connected through interleaving and de-interleaving mechanisms, enabling iterative refinement of the decoding process. The architecture is specifically designed to capture both the sequential nature of convolutional codes and the iterative exchange of extrinsic information that characterizes Turbo decoding. This architecture is also visualized in Figure \ref{fig_turbo_decoder_structure}, which illustrates the interaction between the two component CNEs.

Before the iterative decoding process begins, the received sequence undergoes a rate-matching reversal procedure, which consists of de-puncturing and sub-block de-interleaving operations. During de-puncturing, zeros are inserted at the locations of punctured bits to restore the original codeword structure. These steps reconstruct the structure of the original transmitted codewords, ensuring compatibility with the decoder architecture. Additionally, this process generates the puncturing indicators $\boldsymbol{p}$, corresponding to the transmitted codeword and its parity sequences.

\begin{figure}[htb]
\centering
\includegraphics[width=252pt]{fig/fig_turbo_decoder_structure.pdf}
\caption{The architecture of the LSTM-based Turbo code decoder.}
\label{fig_turbo_decoder_structure}
\end{figure}

In our designed architecture \textbf{CNE 0} processes the systematic bit sequence $\boldsymbol{x}$ alongside the first parity sequence $\boldsymbol{z}$. \textbf{CNE 1} processes the interleaved systematic bit sequence and the second parity sequence $\boldsymbol{z}'$. These two CNEs are identical in architecture and parameterization, ensuring symmetry and simplifying training. Each CNE employs LSTM layers to extract temporal dependencies in the input sequence, followed by a fully connected layer to predict Log-Likelihood Ratios (LLRs).

The decoding process operates iteratively, exchanging extrinsic information between the two CNEs through interleaving and de-interleaving. At each iteration, the following steps occur:
\begin{equation}
\label{eq_turbo_total_process}
\begin{aligned}
&\text{Step 1.}\quad
\hat{\boldsymbol{x}}_0^{(t)} = f_{\text{CNE 0}}(\begin{bmatrix} \boldsymbol{x} + \hat{\boldsymbol{x}}_1^{\text{de-int}, (t-1)}, \boldsymbol{z} \end{bmatrix}, \boldsymbol{p}_0) \\
&\text{Step 2.}\quad
\hat{\boldsymbol{x}}_0^{\text{int}, (t)} = \boldsymbol{\pi}(\hat{\boldsymbol{x}}_0^{(t)}-\hat{\boldsymbol{x}}_1^{\text{de-int}, (t-1)}) \\
&\text{Step 3.}\quad
\hat{\boldsymbol{x}}_1^{(t)} = f_{\text{CNE 1}}( \begin{bmatrix} \hat{\boldsymbol{x}}_0^{\text{int}, (t)}, \boldsymbol{z}' \end{bmatrix}, \boldsymbol{p}_1) \\
&\text{Step 4.}\quad
\hat{\boldsymbol{x}}_1^{\text{de-int}, (t)} = \boldsymbol{\pi}^{-1}(\hat{\boldsymbol{x}}_1^{(t)}-\hat{\boldsymbol{x}}_0^{\text{int}, (t)})
\end{aligned}
\end{equation}

Here, $t$ denotes the iteration step, while $\pi$ and $\pi^{-1}$ represent the interleaving and de-interleaving operations, respectively. The vectors $\boldsymbol{p}_0$ and $\boldsymbol{p}_1$ indicate the puncturing information for $\begin{bmatrix} \boldsymbol{x}, \boldsymbol{z} \end{bmatrix}$ and $\begin{bmatrix} \boldsymbol{x}, \boldsymbol{z}' \end{bmatrix}$, respectively.

After $N_{\text{iter}}$ iterations, the final estimate of the systematic bit likelihood is obtained by applying the de-interleaving operation to the output of CNE 1:
\begin{equation}
\label{eq_turbo_final_output}
\begin{aligned}
\boldsymbol{y} = \boldsymbol{\pi}^{-1} \left( \hat{\boldsymbol{x}}_1^{(N_{\text{iter}})} \right)
\end{aligned}
\end{equation}

Additionally, regardless of whether convolutional codes or Turbo codes are employed, to enhance the model's generalization ability across different communication channels, it is essential to normalize the LLRs before feeding them into the neural network, while retaining the sign of the LLRs. The normalization procedure is expressed as follows:
\begin{equation}
\label{eq_llr_norm}
\begin{aligned}
\overline{\boldsymbol{\text{LLR}}} = \left\lvert \frac{\boldsymbol{\text{LLR}} - \mu}{\sqrt{\sigma^2 + \epsilon}} \right\rvert \cdot \text{sign}(\boldsymbol{\text{LLR}})
\end{aligned}
\end{equation}
where $\mu$ and $\sigma^2$ denote the mean and variance of the LLR values, respectively, and $\epsilon=10^{-6}$ is a small constant added to ensure numerical stability by preventing division by zero. The function $\text{sign}(\cdot)$ preserves the sign of the LLRs during normalization.


\subsection{Training Methodology}
To optimize the proposed CNE-based convolutional and Turbo decoder for diverse code rates and puncturing patterns, a two-stage training strategy is employed. The process begins with pretraining on non-punctured codes at a fixed Signal-to-Noise Ratio (SNR) of 0 dB. This pretraining step establishes a strong foundational model capable of decoding convolutional and Turbo codes under idealized conditions, serving as the basis for subsequent fine-tuning.

Building upon this pretrained model, fine-tuning is performed using a mixed-rate dataset containing samples from multiple code rates, each associated with distinct puncturing patterns. During fine-tuning, the SNR for codewords at different code rates $R$ is adjusted using the formula:
\begin{equation}
\label{eq_mix_train_snr}
\begin{aligned}
\text{SNR}_{\text{train}} = \text{SNR}_{\text{offset}} + 10 \log_{10}(2R)
\end{aligned}
\end{equation}
where $\text{SNR}_{\text{offset}}$ serves as a baseline SNR value calibrated to align the Bit Error Rate (BER) across code rates. This parameter can be selected based on empirical results from conventional decoders, such as the Viterbi or BCJR algorithms, to ensure approximate BER alignment across code rates. This \textbf{Balanced BER Training (BBT)} prevents the loss function from being dominated by specific code rates, maintaining stability during optimization.

The training loss is defined using Binary Cross-Entropy (BCE), which evaluates the accuracy of the predicted likelihoods against the true bit labels. Mathematically, the BCE loss is expressed as:
\begin{equation}
\label{eq_train_loss}
\begin{aligned}
\mathcal{L}_{\text{BCE}} = -\frac{1}{K} \sum_{k=1}^K \left( y_k \log(\hat{y}_k) + (1 - y_k) \log(1 - \hat{y}_k) \right)
\end{aligned}
\end{equation}
where $K$ is the number of bits per batch, $y_k$ denotes the true bit value, and $\hat{y}_k$ is the predicted likelihood for bit $k$. This loss function ensures that the model accurately predicts bit likelihoods across all code rates.

The training process leverages the Adam optimizer \cite{adam_opt} with an initial learning rate of $10^{-3}$ during pretraining and $10^{-4}$ during fine-tuning. The reduced learning rate in the fine-tuning phase helps preserve the knowledge acquired during pretraining and minimizes the risk of catastrophic forgetting. To further enhance optimization, a cosine decay scheduler \cite{consine_sch} is employed to progressively decrease the learning rate to $10^{-6}$ by the end of training. Both pretraining and fine-tuning are carried out over 1000 epochs with a batch size of 128. The training and inference processes are performed on an NVIDIA RTX 4090 GPU with 24GB of memory, complemented by an Intel(R) Xeon(R) Platinum 8468V CPU and 512GB of DDR5 memory, providing the computational power necessary to handle the complexity and diversity of the dataset.


\section{Experiments}
In the experimental setup, the information bit length during training is 120, while during inference, it is varied across 120, 240, 480, and 960 to evaluate the model's generalization to different input sizes. Training is performed in an AWGN channel, with inference conducted in both AWGN and Rayleigh fading channels to assess generalization across different channel conditions. Convolutional codes are assigned code rates of 1/2, 2/3, 3/4, and 5/6, in line with protocol specifications. For consistency, the Turbo codes are configured with code rates of 1/3, 1/2, 2/3, 3/4, and 5/6, as Turbo codes offer more flexibility in rate adaptation. The fine-tuning rates are 1/3, 1/2, 2/3, and 3/4, while the 5/6 code rate is used as an unseen rate only during inference to validate the model's generalization ability. The traditional convolutional decoder uses the Viterbi algorithm with a traceback depth of 120, while the Turbo code decoder employs a reduced-complexity BCJR algorithm called max-log-MAP \cite{max_log_map} with full traceback depth.

The Rayleigh fading channel is modeled with 3 independent taps and a MIMO configuration with 4 transmit and 4 receive antennas. Channel State Information (CSI) in the Rayleigh fading scenario is obtained using least squares channel estimation \cite{mimo_chest}. For a received signal  $\boldsymbol{y}$, the transmitted pilot matrix $\boldsymbol{P}$, and the channel matrix $\boldsymbol{H}$, the LS estimation is given by:
\begin{equation}
\label{eq_ls_chest}
\boldsymbol{\hat{H}} = \boldsymbol{y}\boldsymbol{P}^\dagger
\end{equation}
where $\boldsymbol{P}^\dagger = (\boldsymbol{P}^H\boldsymbol{P})^{-1}\boldsymbol{P}^H$ is the Moore-Penrose pseudoinverse of $\boldsymbol{P}$, and $\boldsymbol{P}^H$ represents the Hermitian transpose of $\boldsymbol{P}$. The pilot matrix $\boldsymbol{P} \in \mathbb{C}^{N_T \times N_T}$ is generated according to the IEEE 802.11 protocol \cite{ieee_80211_std} to facilitate accurate channel estimation.


For MIMO detection, the Minimum Mean Square Error (MMSE) algorithm is employed \cite{mimo_sigdet}, which minimizes the mean squared error between the estimated transmitted signal $\hat{\boldsymbol{x}}$ and the true transmitted signal $\boldsymbol{x}$. The MMSE filter $\boldsymbol{W} \in \mathbb{C}^{N_T \times N_R}$ is computed as:
\begin{equation}
\label{eq_mmse_cal_w}
\boldsymbol{W} = \left( \overline{\boldsymbol{H}}^H \overline{\boldsymbol{H}} + \lambda \boldsymbol{I} \right)^{-1} \overline{\boldsymbol{H}}^H
\end{equation}
where $\lambda = 10^{-6}$ is a regularization parameter ensuring numerical stability, and $\overline{\boldsymbol{H}} \in \mathbb{C}^{(N_R + N_T) \times N_T}$ is the extended matrix incorporating the noise variance $\sigma^2$. The extended channel matrix is defined as:
\begin{equation}
\label{eq_mmse_cal_hbar}
\overline{\boldsymbol{H}} = \begin{bmatrix} \boldsymbol{\hat{H}} \\ \sigma^2 \boldsymbol{I} \end{bmatrix}
\end{equation}
where $\boldsymbol{I} \in \mathbb{C}^{N_T \times N_T}$ is the identity matrix.

Once the MMSE filter is computed, the transmitted signal $\boldsymbol{x}$ is estimated as:
\begin{equation}
\label{eq_mmse_cal_x_hat}
\hat{\boldsymbol{x}} = \boldsymbol{W} \boldsymbol{y}
\end{equation}

This process yields the estimated transmitted signal \(\hat{\boldsymbol{x}}\), which is subsequently passed through the demodulation and decoding stages to recover the original message.

Specifically, in the demodulation stage, the LLR for the $i$-th bit of the detected signal is computed based on the Euclidean distance between the detected signal and the possible transmitted symbols. The LLR for the $i$-th bit $x_i$ is calculated as follows:
\begin{equation}
\begin{aligned}
\text{LLR}(x_i) = \log \left( \frac{P(x_i = 0 \mid \hat{x})}{P(x_i = 1 \mid \hat{x})} \right) = \min_{\boldsymbol{x}_0} \left( \| \hat{x} - \boldsymbol{x}_0 \| \right) - \min_{\boldsymbol{x}_1} \left( \| \hat{x} - \boldsymbol{x}_1 \| \right)
\end{aligned}
\end{equation}

Here, $\boldsymbol{x}_0$ and $\boldsymbol{x}_1$ represent the possible transmitted symbols corresponding to hypotheses $x_i = 0$ and $x_i = 1$, respectively, while $\hat{x}$ denotes the detected signal.

\begin{table*}[htb]
\centering
\caption{Detailed Parameters for CNE and Simulation}
\includegraphics[width=1.0\textwidth]{tab/tab_sim_para.pdf}
\label{tab_sim_para}
\end{table*}

% \begin{table}[htb]
% \centering
% \caption{Detailed Parameters for CNE and Simulation}
% \includegraphics[width=252pt]{tab/tab_sim_para.pdf}
% \label{tab_sim_para}
% \end{table}

Following the demodulation process, the LLRs are passed to the proposed CNE neural network, which is characterized by several key hyperparameters. These include the number of LSTM layers $N_{\text{layer}}$, the embedding dimension $D_{\text{embed}}$, the hidden layer size $D_{\text{hidden}}$, and the number of iterations $N_{\text{iter}}$. Specifically, in the LSTM cell shown in Eq. \ref{eq_lstm}, the weight matrices $\boldsymbol{W}_f, \boldsymbol{W}_i, \boldsymbol{W}_o, \boldsymbol{W}_c \in \mathbb{R}^{D_{\text{embed}} \times (D_{\text{embed}} + D_{\text{hidden}})}$. A detailed summary of the parameters for the CNE neural network architecture and the simulation setup is provided in Table \ref{tab_sim_para}.



\subsection{AWGN Channels: Benchmarking and Precision}
In this section, we compare the performance of the proposed LSTM-based CNE with traditional decoders in an AWGN channel using BPSK modulation. The information bit lengths varied from 120 to 960, and the code rates ranged from 1/3 to 5/6. The BER was then measured for each combination of information bit length and code rate. For each Signal-to-Noise Ratio point, $10^5$ code blocks were simulated to ensure statistical reliability.

The simulation results for convolutional codes are depicted in Figure \ref{fig_awgn_bcc_sim_result}. The proposed decoding method exhibits comparable performance to the conventional Viterbi algorithm when the information bit length is short, such as 120 bits. However, as the bit length increases, the proposed method significantly outperforms the Viterbi decoder, achieving State-Of-The-Art (SOTA) performance. Notably, at an information bit length of 960 and a BER of $10^{-4}$, the proposed algorithm achieves a performance gain of over 1.0 dB across all code rates. This performance advantage stems from the CNE's ability to exploit bidirectional LLR information, capturing both forward and backward dependencies among LLRs. In contrast, the Viterbi algorithm, constrained by limited traceback depth to minimize hardware complexity, fails to achieve a global optimum.

\begin{figure*}[htb]
\centering
\includegraphics[width=1.0\textwidth]{fig/fig_awgn_bcc_sim_result.pdf}
\caption{Performance comparison of the proposed CNE decoder and the conventional Viterbi algorithm for convolutional codes. The results demonstrate significant performance gains with increasing information bit lengths.}
\label{fig_awgn_bcc_sim_result}
\end{figure*}

Figures \ref{fig_awgn_turbo_sim_result_iter6} and \ref{fig_awgn_turbo_sim_result_iter3} present the Turbo code simulation results. In Figure \ref{fig_awgn_turbo_sim_result_iter6}, the performance of the CNE decoder with 3 iterations is nearly identical to that of the conventional BCJR decoder with 6 iterations, even when the BCJR algorithm employs full traceback. This demonstrates the efficiency of the proposed method in reducing computational complexity while maintaining high decoding accuracy.

\begin{figure*}[htb]
\centering
\includegraphics[width=1.0\textwidth]{fig/fig_awgn_turbo_sim_result_iter6.pdf}
\caption{BER performance of Turbo codes using the proposed CNE decoder with 3 iterations compared to the BCJR decoder with 6 iterations. Results indicate comparable performance with reduced computational complexity.}
\label{fig_awgn_turbo_sim_result_iter6}
\end{figure*}

Furthermore, Figure \ref{fig_awgn_turbo_sim_result_iter3} shows the performance comparison at an identical iteration count of 3. The CNE decoder consistently outperforms the BCJR decoder across all information bit lengths and code rates. This superiority is attributed to the ability of the LSTM-based CNE to effectively model temporal dependencies in the data and refine LLR estimates through its iterative architecture, resulting in enhanced decoding performance. 

Notably, due to the explicit embedding of puncturing information into the neural network, the proposed architecture effectively generalizes to the previously unseen 5/6 code rate for both convolutional and Turbo codes, demonstrating its generalization ability across various code rates.

\begin{figure*}[htb]
\centering
\includegraphics[width=1.0\textwidth]{fig/fig_awgn_turbo_sim_result_iter3.pdf}
\caption{BER performance of Turbo codes comparing the proposed CNE decoder and the BCJR decoder, both with 3 iterations. The CNE decoder consistently achieves superior performance across all information bit lengths and code rates.}
\label{fig_awgn_turbo_sim_result_iter3}
\end{figure*}



\subsection{Rayleigh Channels: Generalization and Robustness}
In this section, we evaluate the performance of the proposed LSTM-based CNE in a $4\times4$ MIMO Rayleigh fading channel characterized by 3-tap multipath coefficients. The simulation utilized 16-Quadrature Amplitude Modulation (16-QAM) with Gray-coded mapping for signal transmission. The evaluation covered a range of information bit lengths from 120 to 960 and code rates from 1/3 to 5/6, mirroring the configurations used in the AWGN scenario. For Turbo codes, the BCJR algorithm performs up to 6 iterations, while the proposed CNE decoder is fixed at 3 iterations. To assess the impact of channel estimation accuracy, we simulated the system's performance under both perfect channel state information and least squares channel estimation. To ensure statistical robustness, $10^5$ code blocks were simulated for each SNR point.

Tables \ref{tab_rayleigh_bcc_sim_result} and \ref{tab_rayleigh_turbo_sim_result} present the required $E_b/N_0$ values to achieve target BERs of $10^{-3}$ and $10^{-4}$, respectively, for convolutional and Turbo codes in Rayleigh fading channels. In the tables, $\overline{A}$ indicates that when $E_b/N_0 > A$, the decoding performance converges to a BER of 0 before reaching the target BER, and "$...$" signifies that even at $E_b/N_0 = 40$ dB, the target BER remains unattained. Smaller $E_b/N_0$ values reflect better performance, and the best results are highlighted in bold.

From the results in Table \ref{tab_rayleigh_bcc_sim_result} and \ref{tab_rayleigh_turbo_sim_result}, it is evident that the proposed CNE decoder achieves a significant performance improvement over traditional decoding algorithms in Rayleigh fading channels. Notably, under LS channel estimation, the CNE decoder demonstrates a substantial advantage, outperforming traditional algorithms by more than 5 dB in many scenarios. Furthermore, the CNE decoder even surpasses traditional decoders with perfect channel state information.

\begin{table*}[htb]
\centering
\caption{Simulation results for convolutional codes in a $4\times4$ Rayleigh fading channel with 16-QAM modulation}
\includegraphics[width=1.0\textwidth]{tab/tab_rayleigh_bcc_sim_result_8pt.pdf}
\label{tab_rayleigh_bcc_sim_result}
\end{table*}

\begin{table*}[htb]
\centering
\caption{Simulation results for Turbo codes in a $4\times4$ Rayleigh fading channel with 16-QAM modulation}
\includegraphics[width=1.0\textwidth]{tab/tab_rayleigh_turbo_sim_result_8pt.pdf}
\label{tab_rayleigh_turbo_sim_result}
\end{table*}

% \begin{table}[htb]
% \centering
% \caption{Simulation results of convolutional codes in a $4\times4$ Rayleigh fading channel}
% \includegraphics[width=252pt]{tab/tab_rayleigh_bcc_sim_result.pdf}
% \label{tab_rayleigh_bcc_sim_result}
% \end{table}

% \begin{table}[htb]
% \centering
% \caption{Simulation results of Turbo codes in a $4\times4$ Rayleigh fading channel}
% \includegraphics[width=252pt]{tab/tab_rayleigh_turbo_sim_result.pdf}
% \label{tab_rayleigh_turbo_sim_result}
% \end{table}

The simulation results under Rayleigh fading channels provide critical insights into the strengths of the proposed LSTM-based CNE decoder, particularly in terms of \textbf{generalization} and \textbf{robustness}. The proposed CNE decoder achieves consistent performance gains under both AWGN and Rayleigh fading channels. This demonstrates its ability to generalize across different channel conditions, effectively learning the underlying relationships between received LLRs and transmitted bits, irrespective of the noise or fading environment. Unlike traditional decoders that rely on predefined assumptions and fixed traceback mechanisms, the CNE decoder dynamically adapts to complex channel conditions. Its LSTM-based architecture efficiently models temporal dependencies and leverages bidirectional information flow, enabling it to handle multipath interference and Rayleigh fading robustly.



\section{Analysis}
In this section, we analyze the proposed LSTM-based CNE decoder in terms of computational complexity and decoding latency. These two metrics are critical for evaluating the feasibility and efficiency of decoding algorithms in practical communication systems.


\subsection{Computational Complexity Analysis}
The computational complexity of the proposed LSTM-based CNE decoder, as detailed in Eq. \eqref{eq_lstm_total_process}, arises from four key components: the input projection and gating operations, the batch normalization, the LSTM-based sequence processing, and the output projection.

The first step involves projecting the input features $\boldsymbol{x} \in \mathbb{R}^{K \times D_{\text{in}}}$ through a fully connected layer with weight matrix $\boldsymbol{W}_{\text{proj}} \in \mathbb{R}^{D_{\text{embed}} \times D_{\text{in}}}$. Simultaneously, a gating mechanism modulates the features using the puncturing input $\boldsymbol{p} \in \mathbb{R}^{K \times D_{\text{in}}}$, with the gating weights parameterized by $\boldsymbol{W}_{\text{punc}} \in \mathbb{R}^{D_{\text{embed}} \times D_{\text{in}}}$. The complexity of this step, which involves matrix multiplications and sigmoid activations, is $\mathcal{O} \big( 2K \cdot D_{\text{embed}} \cdot D_{\text{in}} + K \cdot D_{\text{embed}} \big)$.

Next, batch normalization is applied to the modulated features, which results in an output with dimension $\mathbb{R}^{K \times D_{\text{embed}}}$. Since batch normalization operates linearly over the embedding dimension $D_{\text{embed}}$, the computational complexity of this step is $\mathcal{O}(K \cdot D_{\text{embed}})$.

The batch-normalized features are then processed by the LSTM network, the core computational unit of the decoder. For a sequence length $K$, input size $D_{\text{embed}}$, and hidden state size $D_{\text{hidden}}$, the computational complexity of the bidirectional LSTM, accounting for the three gate operations (forget, input, and output gates) and the candidate memory cell, as well as the nonlinear activation functions, is given by:
\begin{equation}
\label{eq_lstm_complexity}
\begin{aligned}
\mathcal{O} \left( K \cdot \left( 8 D_{\text{hidden}}^2 + 8 D_{\text{hidden}} D_{\text{embed}} + 14 D_{\text{hidden}} \right) \right)
\end{aligned}
\end{equation}

This complexity arises from matrix multiplications for each gate and memory cell, along with the non-linear activations, with the factor of 8 accounting for the gates and candidate cell, and the factor of 14 for the activation operations. The bidirectional structure doubles this complexity.

Finally, the LSTM outputs, $\boldsymbol{h} \in \mathbb{R}^{K \times 2D_{\text{hidden}}}$, are projected to the output space using the weight matrix $\boldsymbol{W}_{\text{out}} \in \mathbb{R}^{2D_{\text{hidden}} \times 1}$, resulting in the final output $\boldsymbol{y} \in \mathbb{R}^{K \times 1}$. The computational complexity of this step is $\mathcal{O}(2K \cdot D_{\text{hidden}})$.

Combining the complexities of the input projection, gating mechanism, batch normalization, LSTM processing, and output projection, the total computational complexity of the proposed CNE is:
\begin{equation}
\label{eq_cne_total_complexity}
\begin{aligned}
\mathcal{O} \left( K \cdot \left( 
\begin{aligned}
8 D_{\text{hidden}}^2 + 8 D_{\text{hidden}} D_{\text{embed}} + 2 D_{\text{in}} D_{\text{embed}} + 2 D_{\text{embed}} + 16 D_{\text{hidden}}
\end{aligned}
\right) \right)
\end{aligned}
\end{equation}

% The complexity analysis shows that the LSTM operations, due to their sequential nature and reliance on temporal dependencies, dominate the overall computational cost. The total complexity scales linearly with the sequence length $K$ and quadratically with the hidden state size $D_{\text{hidden}}$. Therefore, optimizing $D_{\text{hidden}}$ and exploiting parallel computation can significantly enhance the efficiency of the CNE decoder.

The computational complexity of the Viterbi algorithm is determined by the number of states in the trellis and the number of transitions evaluated at each time step. For a convolutional code with constraint length $L$, the trellis contains $2^{L-1}$ states. At each time step, the algorithm computes the path metrics for all $2^{L-1}$ states, resulting in a complexity of:
\begin{equation}
\label{eq_viterbi_complexity}
\begin{aligned}
\mathcal{O}(K \cdot 2^{L-1})
\end{aligned}
\end{equation}

Similarly, the BCJR algorithm, which performs forward and backward recursions over the trellis, requires computations proportional to the number of states in the trellis for each time step. Since both forward and backward recursions process $2^{L-1}$ states, and the Turbo decoder involves two component codes with $N_{\text{iter}}$ iterations, the total computational complexity of the BCJR algorithm is:
\begin{equation}
\label{eq_bcjr_complexity}
\begin{aligned}
\mathcal{O}(N_{\text{iter}} \cdot K \cdot 2^{L+1})
\end{aligned}
\end{equation}

In addition, we evaluated the proposed CNE decoder using Thop \cite{flops_thop}, a tool designed to measure the Multiply-Accumulate Operations (MACs) of neural networks. The evaluation, conducted with an information bit length of 120, revealed that the CNE convolutional code decoder requires $2,245,632 \, \text{MACs/decoded bit}$, with a network containing $2,237,441$ trainable parameters. Similarly, for the CNE Turbo code decoder, the network contains $6,715,398$ trainable parameters and requires $16,168,550 \, \text{MACs/decoded bit}$.

Although the CNE decoder results in a higher computational cost compared to traditional Viterbi and BCJR algorithms, this overhead is offset by the reduced number of required decoding iterations and the substantial performance gains demonstrated in both AWGN and Rayleigh fading channels. Furthermore, the parallelizable nature of matrix operations in the LSTM and fully connected layers allows for efficient deployment on modern hardware accelerators, such as GPUs, thereby enhancing the overall computational efficiency.


\subsection{Decoding Latency Analysis}
Decoding latency is a key performance metric in practical communication systems, as it directly impacts throughput and real-time system performance. In this subsection, we analyze the decoding latency of the proposed LSTM-based CNE decoder and compare it with traditional Viterbi and BCJR algorithms, focusing on the computational delay of each module to provide a comprehensive understanding of their respective latency characteristics.

The decoding latency of the proposed CNE decoder stems from four primary components: the input projection and gating mechanism, the batch normalization, the LSTM network, and the output projection. The input features $\boldsymbol{x} \in \mathbb{R}^{K \times D_{\text{in}}}$ are first projected through a fully connected layer parameterized by $\boldsymbol{W}_{\text{proj}} \in \mathbb{R}^{D_{\text{embed}} \times D_{\text{in}}}$. Concurrently, a gating mechanism modulates these features based on the puncturing input $\boldsymbol{p} \in \mathbb{R}^{K \times D_{\text{in}}}$, using a gating weight matrix $\boldsymbol{W}_{\text{punc}} \in \mathbb{R}^{D_{\text{embed}} \times D_{\text{in}}}$. Both operations primarily involve matrix-vector multiplications, and assuming parallel processing across all $K$ time steps, the total latency of this stage can be expressed as:
\begin{equation}
\label{eq_input_delay}
\begin{aligned}
T_{\text{proj}} = t_{\text{mat}}(D_{\text{embed}}, D_{\text{in}})
\end{aligned}
\end{equation}
where $t_{\text{mat}}(m, n)$ represents the processing delay for a matrix-vector multiplication of size $m \times n$.  

The modulated features $\in \mathbb{R}^{K \times D_{\text{embed}}}$ then undergo batch normalization, which normalizes the features independently across the embedding dimension $D_{\text{embed}}$. Since batch normalization operates in parallel across all time steps and dimensions, the latency of this step is:
\begin{equation}
\label{eq_bn_delay}
\begin{aligned}
T_{\text{BN}} = t_{\text{bn}}(D_{\text{embed}})
\end{aligned}
\end{equation}
where $t_{\text{bn}}(d)$ denotes the processing time required to normalize a vector of length $d$. Given its simplicity and element-wise operation, batch normalization contributes minimally to the overall latency.  

The batch-normalized features are then passed through the LSTM network, which constitutes the core computational module of the CNE decoder. Due to the sequential nature of LSTMs, the latency of this stage depends on the sequence length $K$, the input embedding size $D_{\text{embed}}$, and the hidden state size $D_{\text{hidden}}$. For each time step, the LSTM processes input features, updates its hidden state, and computes gating activations. Assuming $t_{\text{lstm}}(D_{\text{hidden}}, D_{\text{embed}})$ is the processing delay for one LSTM cell, the total latency for the LSTM is:
\begin{equation}
\label{eq_lstm_delay}
\begin{aligned}
T_{\text{LSTM}} = K \cdot t_{\text{lstm}}(D_{\text{hidden}}, D_{\text{embed}})
\end{aligned}
\end{equation}

This sequential operation across $K$ time steps makes the LSTM the dominant factor in the total decoding latency.  

Finally, the LSTM outputs $\boldsymbol{h} \in \mathbb{R}^{K \times 2D_{\text{hidden}}}$ are projected to the output space via a fully connected layer parameterized by $\boldsymbol{W}_{\text{out}} \in \mathbb{R}^{2D_{\text{hidden}} \times 1}$. As this operation is parallelizable across all $K$ time steps, its latency can be expressed as:
\begin{equation}
\label{eq_output_delay}
\begin{aligned}
T_{\text{out}} = t_{\text{mat}}(1, 2D_{\text{hidden}})
\end{aligned}
\end{equation}

By combining the latencies of all these components, the total decoding latency of the CNE decoder can be expressed as:
\begin{equation}
\label{eq_cne_total_delay}
\begin{aligned}
T_{\text{CNE}} = T_{\text{proj}} + T_{\text{BN}} + T_{\text{LSTM}} + T_{\text{out}}
\end{aligned}
\end{equation}

% While the sequential nature of the LSTM dominates $T_{\text{CNE}}$, the highly parallelizable input projection, batch normalization, and output projection layers can be effectively accelerated using modern hardware, such as GPUs, significantly reducing the overall latency.

For comparison, the decoding latency of the Viterbi algorithm is determined by its forward recursion through the trellis structure. For a convolutional code with $2^{L-1}$ states, each time step requires updating path metrics for all states. Assuming $t_{\text{state}}(2^{L-1})$ represents the processing delay per state, the total latency of the Viterbi decoder is:
\begin{equation}
\label{eq_viterbi_delay}
\begin{aligned}
T_{\text{Viterbi}} = K \cdot t_{\text{state}}(2^{L-1})
\end{aligned}
\end{equation}

As the Viterbi algorithm operates sequentially across $K$ time steps, its latency scales linearly with the sequence length. Although efficient, its reliance on a fixed trellis structure limits its flexibility in adapting to varying channel conditions.

The BCJR algorithm, which forms the basis of Turbo decoding, incurs significantly higher latency due to its bidirectional nature and iterative processing. Each decoding iteration involves both forward and backward recursions over the trellis, with $2^{L-1}$ states per recursion. Assuming $t_{\text{state}}(2^{L-1})$ is the processing delay for each recursion, the latency for a single iteration is:
\begin{equation}
\label{eq_bcjr_single_iter_delay}
\begin{aligned}
T_{\text{recursion}} = 2 \cdot K \cdot t_{\text{state}}(2^{L-1})
\end{aligned}
\end{equation}
and considering $N_{\text{iter}}$ iterations for the Turbo decoder, the total latency of the BCJR-based Turbo decoder is:
\begin{equation}
\label{eq_bcjr_delay}
\begin{aligned}
T_{\text{BCJR}} = N_{\text{iter}} \cdot T_{\text{recursion}} = 2 \cdot N_{\text{iter}} \cdot K \cdot t_{\text{state}}(2^{L-1})
\end{aligned}
\end{equation}

In addition, we evaluated the proposed CNE decoder using Torch-TensorRT \cite{tensor_rt}, a tool designed to accelerate neural network inference on NVIDIA GPUs. The evaluation was performed with a floating-point precision of float32, based on 1000 trials with an information bit length of 120. The results show that the proposed CNE convolutional code decoder has an average inference latency of $0.116 \, \mu\text{s/decoded bit}$, while the Turbo code decoder exhibits an average inference latency of $0.867 \, \mu\text{s/decoded bit}$. It is expected that, with future quantization to float16 or int8, the inference latency will be further reduced.

In summary, the proposed LSTM-based CNE decoder strikes an effective balance between decoding performance and latency. While the sequential nature of the LSTM introduces higher latency compared to the Viterbi and BCJR algorithms, it outperforms under dynamic channel conditions and requires significantly fewer iterations than the BCJR-based Turbo decoder (e.g., 3 iterations versus 6 iterations in our experiments). Moreover, the CNE decoder benefits from parallelizable components such as input projection, batch normalization, and output projection, enabling efficient implementation on modern hardware accelerators. These advantages make the proposed CNE decoder highly suitable for real-time communication systems requiring both low latency and reliable performance.



\section{Conclusions}
This paper introduces a unified LSTM-based decoding architecture that enhances the performance of punctured convolutional and Turbo codes in practical communication scenarios. By leveraging deep learning techniques, the proposed method offers a flexible and code-agnostic solution that ensures robust decoding across a wide range of code rates and channel conditions. The results obtained from extensive simulations validate the efficacy of the approach, demonstrating notable improvements in decoding accuracy compared to traditional algorithms. Future work could focus on further optimizing the architecture to reduce decoding complexity and latency, as well as extending its application to more complex coding schemes and diverse communication environments, paving the way for more efficient and reliable decoding in next-generation AI-powered wireless systems.


% \section*{Acknowledgments}
% This was was supported in part by......

%Bibliography
% \bibliographystyle{unsrt}
\bibliographystyle{IEEEtran}
\bibliography{references}  


\end{document}
