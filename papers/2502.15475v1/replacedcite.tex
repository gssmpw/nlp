\section{Prior Works}
In wireless communication, AI has shown promise in physical layer signal processing tasks such as channel estimation, signal detection, and channel decoding ____. Traditionally, signal processing algorithms were implemented on Central Processing Units (CPUs), Digital Signal Processors (DSPs), or Application-Specific Integrated Circuits (ASICs), which rely on \textit{serial processing}. However, the highly \textit{parallel nature} of AI algorithms requires more efficient architectures, driving the shift from CPU-based signal processing algorithms to Graphics Processing Units (GPUs), which are optimized for parallel computation and can efficiently handle large-scale, simultaneous operations.

As a pioneer in this shift and a leading designer of GPUs, NVIDIA has restructured cellular wireless network receivers using AI. For instance, NVIDIA developed the Sionna signal processing AI library ____ and used it to create multi-user, real-time Neural Network (NN) receivers compatible with the 5th Generation New Radio (5G NR) protocol ____. While AI has been successfully applied to channel estimation, signal detection, and demodulation in NVIDIAâ€™s work, integrating neural network-based decoders into the receiver presents significant challenges. One major issue is the generalization of neural network decoders, particularly when applied to varying code rates, which can limit their performance and flexibility in diverse scenarios ____.

Puncturing, which discards part of the encoded data to form different code rates and improve spectral efficiency, is essential in real-world wireless communication systems. Control channels typically employ lower code rates for high reliability, while data channels use more flexible and higher code rates to accommodate diverse transmission conditions and maximize throughput. Linear block codes (e.g., Low-Density Parity-Check (LDPC) and Polar codes) and sequential codes (e.g., convolutional and Turbo codes) are widely used in commercial communication protocols ____. For example, Wi-Fi protocols ____ support four code rates for convolutional and LDPC codes, while cellular protocols ____ utilize Polar codes for control channels with dozens of code rates, and Turbo and LDPC codes for data channels with over a hundred code rates.

Recent studies have proposed neural network-based decoders to address puncturing in linear block codes, demonstrating improvements over traditional methods. For instance, ____ employed a Transformer-based network to decode linear block codes with varying code rates, while ____ introduced a unified Transformer decoder capable of simultaneously decoding multiple code rates of linear block codes with a single set of neural network parameters.

Other studies have focused on applying neural networks to decode sequential codes, such as convolutional and Turbo codes. For example, ____ explored the use of Recurrent Neural Networks (RNNs) for decoding convolutional codes, achieving performance comparable to the Viterbi algorithm ____ in both AWGN and non-AWGN channels with $t$-distributed noise. The DeepTurbo approach ____, inspired by the iterative Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm ____, targets Turbo codes but faces performance degradation when generalized to longer code lengths, requiring retraining. A notable study ____ employed model-agnostic meta-learning to enhance the generalization of neural network-based decoders in unseen channel conditions. Additionally, Turbo Autoencoders ____ introduced an end-to-end learning framework that jointly optimizes both the encoder and decoder, outperforming traditional BCJR decoders.

However, these studies on convolutional and Turbo codes have not addressed the issue of puncturing, limiting their applicability in real-world communication systems. In the context of NN-based decoders, models trained without considering puncturing may fail when exposed to such conditions. This is because the model's parameters are optimized for scenarios without punctured data, making it less robust in real-world applications where puncturing is common. Furthermore, training a separate neural network for each possible code rate leads to significant storage overhead, which is impractical for scalable deployment in dynamic environments.