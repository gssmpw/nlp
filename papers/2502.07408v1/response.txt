\section{Related Work on Bit-Flip Attacks}
\label{sec:related}
Early works such as \textit{Terminal Brain Damage} (TBD) **Chakraborty, "Terminal Brain Damage"** illustrated how manipulating exponent bits could severely harm floating-point networks. However, TBD excludes sign bits, which we find can be far more devastating to overall accuracy with fewer flips.  
Other methods, including **Chen, "Efficient Bit-Flip Attack"**, perform iterative gradient-based flips. For example, **Rakin, "Bit Flipping Attacks on Deep Neural Networks"** requires multiple samples to compute gradients and can disrupt ResNet-50â€™s accuracy by $\sim99.7\%$ using 11 bit flips. %(on INT8 networks).
**Jiang, "Semi-Supervised Bit Flip Attack"** similarly needs iterative optimization, reaching significant disruption at the cost of 23 flips.  

Recent variants have attempted to relax data requirements. For instance, **Niu, "Data-Independent Sign-Bit Flipping Attacks"** generate pseudo-samples or use partial data statistics to guide which bits to flip. Although they lessen the need for a large labeled dataset, they still rely on model feedback or approximate gradients. In contrast, our sign bit flipping approach is lightweight, data-agnostic, and can degrade a large variety of networks by by over $99.8\%$ with few flips. This distinction stems from focusing on sign bits, which, due to the abrupt change from $+$ to $-$, often exert a disproportionate influence on learned representations.

While prior research highlights the vulnerability of neural networks to parameter corruption, the striking simplicity and severity of sign flips merit closer scrutiny. As shown in \cref{tab:compared}, these flips can be carried out without data or optimization, and are straightforward to locate in memory, making them both feasible and devastating in real-world scenarios.