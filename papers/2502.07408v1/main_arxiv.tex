
\documentclass{article}
\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage{hyperref}

\usepackage{afterpage}

\usepackage{PRIMEarxiv}
\usepackage{natbib}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[capitalize,noabbrev]{cleveref}



\newcommand{\methodname}{DNL}
\newcommand{\emethodname}{1P-DNL}



% my imports
\usepackage{multirow}
\usepackage{multicol}
\usepackage{dblfloatfix}
\usepackage{enumitem}
\usepackage{lipsum} % for dummy text
\usepackage{wrapfig} % for wrapping text around figures

\usepackage{xcolor}         % colors
\usepackage{color, colortbl}
\usepackage{makecell}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{tcolorbox}



\definecolor{mygreen}{RGB}{17, 128, 41}
\definecolor{myred}{RGB}{185, 0, 27}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\title{No Data, No Optimization: A Lightweight Method To Disrupt Neural Networks With Sign-Flips}


\author{%
  Ido Galil* \\
  Technion, NVIDIA\\
\texttt{idogalil.ig@gmail.com, igalil@nvidia.com} \\
\And
  Moshe Kimhi* \\
  Technion \\
\texttt{moshekimhi@cs.technion.ac.il}\\
  \AND
  Ran El-Yaniv \\
  Technion, NVIDIA \\
  \texttt{rani@cs.technion.ac.il, relyaniv@nvidia.com} \\
}


\begin{document}
\maketitle

\afterpage{
\begin{figure*}[tp] %move to tp
\centering
\includegraphics[width=\linewidth]
{figs/dalmat.pdf}
%An example of 
\caption{\methodname{} applied to RegNetY-400MF’s \cite{Radosavovic_2020} first convolution layer. The original (Sobel-like) kernel, used for horizontal edge detection, is shown above the flipped version obtained by changing just one high-magnitude weight’s sign bit. Even this minimal alteration leads to a drastically different output feature map. This corrupted feature propagates through the model, undermining downstream representations and severely impairing the network’s overall ability to recognize the Dalmatian.}
\label{fig:dalmatian}
\end{figure*}
}


\begin{abstract}
\let\thefootnote\relax\footnotetext{*These authors contributed equally.}

Deep Neural Networks (DNNs) can be catastrophically disrupted by flipping only a handful of sign bits in their parameters. We introduce Deep Neural Lesion (\methodname), a data-free, lightweight method that locates these critical parameters and triggers massive accuracy drops. We validate its efficacy on a wide variety of computer vision models and datasets. The method requires no training data or optimization and can be carried out via common exploits software, firmware or hardware based attack vectors.  An enhanced variant that uses a single forward and backward pass further amplifies the damage beyond \methodname's zero-pass approach. Flipping just two sign bits in ResNet50 on ImageNet reduces accuracy by 99.8\%. We also show that selectively protecting a small fraction of vulnerable sign bits provides a practical defense against such attacks.
% -----------------------------
\end{abstract}





\section{Introduction}
\label{sec:intro}

Deep neural networks (DNNs) power a wide range of applications, including safety-critical tasks such as autonomous driving, unmanned aerial vehicle (UAV) navigation, medical diagnostics, and robotics, where real-time decision-making is essential. However, the increasing reliance on DNNs also raises concerns about their resilience to malicious attacks. Ensuring the robustness of DNNs is crucial to maintaining their reliability in such critical applications.


In this paper, we expose a critical vulnerability in DNNs that allows for severe disruption by flipping as few as one to ten sign bits, a tiny fraction of the model’s parameters. Our method demonstrates how a small number of bit flips, within models containing up to hundred millions of parameters, can cause catastrophic degradation in performance.

We systematically analyze and identify the parameters most susceptible to sign flips, which we term ``critical parameters.'' Our approach disrupts models by flipping the sign bits in their weights, achieving significant performance degradation with minimal computational effort across a wide range of model architectures. Crucially, our approach is data-agnostic: it requires only direct access to model weights, bypassing any need for training or validation data.

Our method is an extremely lightweight, \emph{Pass-free} Attack, which needs no additional computational passes and is driven by a magnitude-based heuristic, considering inductive bias on learnable features and the flow of information within the networks. We also propose an enhanced \emph{1-Pass} Attack that employs a single forward and backward pass (on random inputs) to refine the selection of critical parameters. Despite this small increase in computation, the attack remains highly efficient, preserves its data independence, and inflicts even greater damage on the model.



Malicious actors can exploit the identified parameter vulnerability through multiple system layers, including file-system intrusions, firmware compromises, direct memory access (DMA) from compromised peripherals, or memory-level exploits. In each case, once attackers gain access to the model’s parameters, they can flip a small number of high-magnitude sign bits and trigger severe model failures. This lightweight approach requires no iterative optimization, thereby reducing the attacker’s overhead while also increasing stealth.



For example, consider the implications for autonomous driving systems. Traditional adversarial attacks \cite{PGD,FGSM,AA} on such systems would involve manipulating the input pixels in real-time, requiring continuous communication with the vehicle and performing intensive gradient calculations to mislead the model. Physical adversarial attacks, such as placing adversarial stickers on street signs (e.g., as demonstrated in \citet{adversarial_stickers}), demand direct access to the environment and are vulnerable to countermeasures like sensors on street signs or validation through additional traffic inputs. These approaches also require physical intervention by the attacker, limiting their practicality.
An attacker, even with limited access, could exploit any of the aforementioned vulnerabilities and discretely flip a small number of sign bits. By altering only a handful of critical parameters, often just one or two, the attacker can critically degrade the model’s perception and decision-making, posing a far more potent threat to the reliability of autonomous driving systems.
The minimal computational footprint and high impact of this attack make it exceptionally challenging to detect and mitigate in real-world deployments.
All code will be made publicly available upon acceptance.



\textbf{Our main contributions are summarized as follows:}

\textbf{Fatal Vulnerability Exposure:} We reveal a severe vulnerability in DNNs by demonstrating that flipping a small number of specific sign bits can catastrophically degrade model performance. This attack is entirely data-agnostic, requiring no knowledge of the training data, domain-specific data, or synthetic inputs. Our method consists of two lightweight variants: the ``Pass-free'' Attack, which operates without any additional computational passes, and the ``1-Pass'' Attack, which uses a single forward and backward pass with random inputs to enhance the attack's impact.

\textbf{Definition of Model Vulnerability:} We formalize DNN vulnerability to parameter manipulations and characterize why the vast majority of parameters are robust to random bit flips, while a small subset is uniquely critical.

\textbf{Extensive Evaluation:} We validate our approach on 60 classifiers across diverse tasks and datasets, including 48 ImageNet models from the publicly available timm \cite{rw2019timm} and TorchVision \cite{torchvision2016} repositories. Flipping just a handful of bits— fewer than ten—is sufficient to significantly reduce accuracy, demonstrating the broad applicability and impact of this vulnerability.

\textbf{Defense Mechanisms:} We leverage the insight gained from identifying critical parameters to propose efficient defenses. By selectively protecting only these most vulnerable parameters, models can become substantially more resilient to sign-flip attacks.


\section{Problem Setup}
\label{sec:problem_setup}

Modern deep learning frameworks typically store parameters in the IEEE 754 32-bit floating-point format. Each float has a sign bit, eight exponent bits, and 23 mantissa bits: $(-1)^{s} \times 2^{(e - 127)} \times \Bigl(1 + \frac{m}{2^{23}}\Bigr).$
We focus on a standard supervised learning scenario where a model $f_\theta$, is trained on a dataset with distribution $\mathcal{D}$.
Let $\mathcal{X},\mathcal{Y}$ be the input and label spaces, $(X,Y) \sim \mathcal{D}$, where $X \in \mathcal{X}$ and $Y \in \mathcal{Y}$. A trained model $f_\theta$ seeks to minimize the expected risk
$\min_{\theta} \; \mathbb{E}_{(X,Y)\sim \mathcal{D}} \Bigl[\mathcal{L}\bigl(f_\theta(X), Y\bigr)\Bigr],$
where $\mathcal{L}$ is a loss function. Once trained, $\theta$ is deployed for inference.

An attacker, who lacks access to $\mathcal{D}$, $P(\mathcal{X})$, or $P(\mathcal{Y})$, nonetheless obtains %low level 
control over the model parameters $\theta$ and flips a small subset of $k$ bits in $\theta$ to produce $\theta'_{(k)}$. The adversary can gain access directly through software, firmware, or hardware-level exploits, namely Bit flip attacks \cite{10.5555/2823820}. 
%Following is a brief overview of a few of exploits that enable malicious bit flipping: 
Below, we outline several exploits that adversaries can leverage to execute malicious bit-flipping operations on model parameters.
\newline
A \emph{rootkit} \citep{Hoglund2006Rootkits,Sparks2005ShadowWalker,Rutkowska2007} is malicious software running with high-level (kernel or ring-0) privileges, allowing it to intercept or modify operations. Once installed, a rootkit can scan the system’s memory or storage for the model’s parameter files, then surgically flip bits in place. By concealing its processes and hooking system APIs, the rootkit can evade detection from common antivirus tools and monitoring systems, enabling stealthy, ongoing tampering with model parameters without triggering suspicious activity logs.\newline
\emph{Firmware exploits} \citep{FW} (e.g., SSD/HDD controllers, GPU firmware, BIOS, or microcode patches) can give attackers privileged memory access or the ability to inject custom commands that flip bits in system memory or on storage media. By compromising firmware updates or exploiting known bugs, attackers can precisely manipulate parameter bits.\newline  
\emph{DMA from untrustworthy peripherals} \citep{Markettos2019ThunderclapEV} can read and write system memory without involving the CPU or the operating system’s normal access controls. If attackers gain low-level access to a DMA device (e.g., via Thunderbolt or FireWire interfaces), they can directly overwrite targeted bits in protected memory regions.\newline  
\emph{Rowhammer} \citep{Rowhammer,Kim2014,Seaborn2015} exploits the electrical interference between neighboring rows in modern DRAM modules. By rapidly accessing (“hammering”) one row, an attacker causes bits in adjacent rows to flip, even without direct write permissions. Rowhammer attacks typically rely on high-frequency memory accesses that defeat standard refresh mechanisms; once carefully controlled, these flips can be directed at specific bit positions.\newline
\emph{GPU cache tampering} \citep{Nethammer,Throwhammer}, which exploits a compromised kernel driver or malicious GPU code, can manipulate cache management routines to induce bit flips in stored parameters. Similar to Rowhammer’s repeated DRAM accesses, continuously evicting and reloading specific cache lines may corrupt targeted parameters. Because GPU caches are often less scrutinized than CPU caches, this tampering can remain undetected, leading to stealthy yet severe degradation of model performance.\newline  
\emph{Voltage/frequency glitching} \citep{Plundervolt,tang2017clkscrew,TRRespass,GLitch} manipulates the operating voltage or clock frequencies to induce computational errors. Certain voltage ranges can systematically cause specific bits to flip in registers or memory segments. \newline

In all cases, the attacker’s objective is to significantly degrade performance with minimal bit flips for stealth and practicality.
$
\min_k \max \mathbb{E}_{(X,Y)\sim \mathcal{D}} \Bigl[\mathcal{L}\bigl(f_{\theta'_{(k)}}(X), Y\bigr)\Bigr],
$

where both finding minimal $k$ and flipping $k$ bits to produce $\theta'_{(k)}$ are discrete optimization problems.

In other words, the attacker’s goal is to induce a significant performance drop while flipping only a handful of bits, both for stealth and practical reasons, as fewer corruptions are less likely to be detected and can be exploited by the mentioned hardware attacks.
For instance, Rowhammer-based exploits~\citep{Rowhammer} typically induce only sporadic bit upsets in adjacent cells, making massive coordinated flips infeasible.
Notably, the attacker does \emph{not} have access to any training or validation data, nor do they conduct extensive inference passes or iterative gradient-based searches. Such lightweight attacks are realistic in settings where the attacker’s computational resources on the victim device are minimal, or where repeated forward/backward passes might raise suspicion.
We therefore distinguish two scenarios: a \emph{pass-free} attack, which uses no extra computation beyond reading the model weights, and a \emph{1-pass} attack, which uses only a single forward (and backward) pass on random inputs for additional scoring. Both settings stand in contrast to existing approaches that require data samples and multiple optimization steps (see \cref{sec:related} for more details).


Although the attacker’s objective can be framed as a discrete optimization problem—finding the smallest set of bits whose flips induce the greatest performance drop—exhaustive searches over millions of parameters are computationally infeasible in real-time and cannot scale with larger models. Instead, lightweight heuristics can pinpoint ``critical'' parameters while incurring minimal overhead. By leveraging inductive insights into how information flows through the network, an attacker can disrupt the most influential parameters without iterative optimization or a large dataset. This stands in contrast to data-driven or gradient-based methods, which demand multiple inference passes and raise both computational requirements and the risk of detection.



\subsection{Accuracy Reduction Metrics}
To measure the effect of bit flips, let $\theta'_{(k)}$ be the set of parameters obtained by flipping exactly $k$ sign bits in $\theta$. If $\text{Acc}(\theta)$ is the model’s original accuracy, we define:
\begin{equation*} \label{AR}
\text{AR}(k) \;=\; \frac{\text{Acc}(\theta) \;-\; \text{Acc}(\theta'_{(k)})}{\text{Acc}(\theta)} ,
\end{equation*}
which captures the drop in accuracy induced by $k$ flips. For a broader view, we also define:
\begin{equation} \label{MAR}
\text{mAR}(N) \;=\; \frac{1}{N} \sum_{k=1}^{N} \text{AR}(k),
\end{equation}
so that a single number can represent the model’s overall vulnerability across different flip counts. 
Because practical hardware attacks often manage only a handful of flips, we mainly focus on small $k$ (e.g., $k \le 10$). 



\section{Locating Models' Most Critical Parameters}
\label{sec:locating}

\begin{figure}[b!]
\centering
\includegraphics[width=0.55\linewidth]
{figs/random_boxplots_arrow.pdf}%\vspace{-8pt}
\caption{Impact of randomly flipping sign bits on model performance. The plot shows the distribution of $AR(\cdot)$ values across 48 Imagenet models when up to 100,000 sign bits are flipped at random.}
\label{fig:rand_box}
\end{figure}

Considering the FP32 representation, while exponent flips can alter a weight’s magnitude, flipping the most significant \emph{sign} bit instantly switches a parameter from positive to negative (or vice versa). This produces a drastic effect on the learned features, as shown in Figure \ref{fig:dalmatian}, motivating us to focus on those bits. Moreover, localizing the sign bit in memory is straightforward (e.g., always the MSB), making it appealing as a simple target for adversaries.
Various hardware-based studies show that repeated access patterns more reliably flip the \emph{same} bit position across different addresses than arbitrarily chosen bits \cite{Rowhammer,Seaborn2015}. 
%In practice, it is often easier to induce a single “bit offset” flip reliably rather than precisely manipulating arbitrary bits. 
Hence, focusing on sign bits aligns with how hardware attacks often achieve consistent flips in a specific bit offset across multiple weights, increasing the chance of our targeted attack success rate.



Flipping random sign bits in a network’s parameters typically has a negligible impact on performance. Indeed, our experiments (visualized in \cref{fig:rand_box}) show that for many architectures, flipping even up to $100,000$ bits (up to 8\% of the parameters of some models) does not  reduce the accuracy consistently—%especially in larger models—
indicating that most parameters are not ``critical.'' These findings motivate a more targeted strategy to identify and flip only the most sensitive parameters.


\begin{figure}[tb!]
\centering
\includegraphics[width=0.65\linewidth]
{figs/boxplots_arrow.pdf}%\vspace{-8pt}
\caption{Comparison of $mAR_{10}$ across different strategies applied to 48 ImageNet models. Magnitude-based sign-flips consistently exhibit fatal reductions in model accuracy, outperforming random flips. The proposed methods, \methodname{} and \emethodname{}, demonstrate even greater effectiveness by targeting critical parameters, achieving significant accuracy degradation with minimal computational overhead.}
\label{fig:box_simple}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=0.65\linewidth]
{figs/max_boxplots_arrow.pdf}%\vspace{-8pt}
\caption{Comparing the $AR(10)$ under different strategies across 48 ImageNet models. The figure highlights the superior performance of \emethodname{} in causing substantial accuracy drops with up to 10 sign flips.}
\label{fig:max_box}
\end{figure}

\textbf{Magnitude-Based Strategy}

Drawing inspiration from the pruning literature, we first examine magnitude-based strategies. Just as magnitude pruning removes low-magnitude weights to minimize the impact on final predictions \cite{frankle2018lottery}, we hypothesize that flipping the sign of \emph{high-magnitude} parameters causes significant disruption. Formally, the parameter score function is defined as follows
\begin{equation}
    \label{eq:magnitude}
    \mathcal{S}(\theta_i) \;=\; \,|\theta_i|\;
\end{equation}
As far as we are aware, this work is the first to evaluate the efficacy of a magnitude-based attack, a surprisingly simple yet powerful strategy that disrupts neural networks without data, optimization or prior knowledge.
In Figure~\ref{fig:box_simple}, the second boxplot from the left, shows that focusing on the top-$k$ largest weights (in absolute value) significantly disrupts most evaluated models. 


\textbf{One-Flip-Per-Kernel Constraint}

Empirical analyses of CNN filters \citep{ImageNetClassification,VisualizingAndUnderstanding} highlight the importance of early-stage kernels (e.g., Gabor-like or Sobel-like) in extracting fundamental visual features. These studies reveal that flipping a single sign bit in a kernel can completely disrupt its feature extraction capability, altering the information the model relies on (see \cref{fig:dalmatian} for the effect of sign flips on a real kernel). However, flipping multiple bits within the same kernel often merely changes its orientation or slightly modifies its functionality, rather than fully destroying the feature, as demonstrated in Figure~\ref{fig:sobel}.
We observe this phenomenon consistently across multiple architectures. Below are a few examples:

\textbf{MobileNetV3-Large} \citep{howard2019searching}: 
Applying our magnitude-based method for $k=2$ selects the second highest-magnitude weight for sign flipping, which in this case belongs to the third convolutional layer, and results in a significant accuracy drop to $AR(2) = 81.31$. 
Adding another magnitude-based weight results in a flip within the same kernel that reduces the degradation to $AR(3) = 46.97$, partially offsetting the attack. 
However, flipping the next highest-magnitude parameter from a different kernel instead raises the accuracy reduction dramatically to $AR(3) = 94.0$.

\textbf{RegNet-Y 16GF} \citep{radosavovic2020designing}: 
In the second convolutional layer, several of the top 10 highest-magnitude weights reside in the same kernel. 
Flipping the sixth highest weight yields $AR(6) = 74.2$, while also flipping the seventh highest weight (in the same kernel) improves accuracy to $AR(7) = 66.5$, rather than compounding the damage.

To maximize damage, we constrain the attack to flip exactly \emph{one} bit per kernel, ensuring the disruption does not offset itself and affects a broader range of features. Given our focus on a small number of flips, distributing them across more kernels also helps amplify the overall impact.


\begin{figure}[tb!]
\centering
\includegraphics[width=0.7\linewidth]
{figs/SobelY.pdf}%\vspace{-8pt}
\caption{Horizontal edge detection filter (based on the Sobel Y filter) with one or two sign flips and their corresponding extracted features. With a single sign flip, the filter is severely disrupted, rendering it unable to detect edges effectively. However, with two bit flips, the resulting errors may partially offset each other, allowing the filter to retain some edge-detection capability and produce features similar to the original.}
\label{fig:sobel}
%\vspace{-5pt}
\end{figure}




\subsection{Layer Selection}

Beyond which parameters to flip, we also investigate \emph{where} in the network to apply the attack. One might intuitively expect that targeting \emph{final} layers—being closer to the classifier—would cause greater damage. However, our experiments reveal that in many architectures, early-layer manipulations are disproportionately damaging. Drawing on an analogy from neuroscience, early lesions (e.g., in the retina or optic nerve) can cause severe or total blindness \citep{kandel2000principles,Stewart2020ARO,science}. Similarly, flipping a single parameter in a fundamental feature detector (e.g., Sobel and Gabor filters) sends erroneous signals throughout subsequent layers, often leading to compounding error. Figure~\ref{fig:dalmatian} illustrates this: a sign flip in a low-level ``edge-detection'' filter causes the network to misinterpret critical structural cues, compounding errors to later layers and severely degrading performance —more so than flips occurring in higher-level layers. 



% \begin{figure*}[tb!]
% \centering
% \begin{minipage}{0.5\textwidth}
%     \centering
%     \resizebox{\linewidth}{!}{%
%     \begin{tabular}{l c c c c}
%         \hline
%         \rowcolor{gray!20} \textbf{Targeted Layers} & \textbf{AR(1)} & \textbf{AR(2)} & \textbf{AR(3)} & \textbf{AR(5)} \\
%         \hline
%           All Layers& 0.133 & 0.15 & 0.24 &0.39  \\
%         First ($l=100$) &0.19 & 0.24 & 0.42 &  0.48 \\
%          First ($l=10$) & \textbf{93.93} & \textbf{99.58} &99.61  & \textbf{99.76}  \\
%           First ($l=5$) & \textbf{93.93} & \textbf{99.58} & \textbf{99.68} & 99.71  \\
%           First ($l=2$) &\textbf{93.93} & \textbf{99.58} & \textbf{99.68} & 99.71  \\
%           First ($l=1$) &59.46 & 53.69 & 82.42 & 88.51  \\
%            Last ($l=10$) & 0.01& 0.04 &0.06  & 0.17  \\
%             Last ($l=5$) &  0.03 & 0.19 &  0.46 & 1.14\\
%         \hline
%     \end{tabular}
%     }
%     \captionof{table}{$AR(\cdot)$ Targeting different layers of ShuffleNetV2 \citet{ma2018shufflenet} with \methodname{}}
%     \label{tab:shuffle_layer}
% \end{minipage}%
% \hfill
% \begin{minipage}{0.5\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figs/boxplots_k_layers.pdf}
%     \caption{Targeting only the first $l$ layers, x-axis report $mAP(10)$.}
%     \label{fig:boxplots_k_layers}
% \end{minipage}
% \end{figure*}


\begin{table}[t!]
\centering
    \caption{$AR(\cdot)$ Targeting different layers of ShuffleNetV2 \citet{ma2018shufflenet} with \methodname{} }
    \resizebox{.56\columnwidth}{!}{%
    \begin{tabular}{l c c c c}
        \hline
        \rowcolor{gray!20} \textbf{Targeted Layers} & \textbf{AR(1)} & \textbf{AR(2)} & \textbf{AR(3)} & \textbf{AR(5)} \\
        \hline
        
          All Layers& 0.133 & 0.15 & 0.24 &0.39  \\
        First ($l=100$) &0.19 & 0.24 & 0.42 &  0.48 \\
         First ($l=10$) & \textbf{93.93} & \textbf{99.58} &99.61  & \textbf{99.76}  \\
          First ($l=5$) & \textbf{93.93} & \textbf{99.58} & \textbf{99.68} & 99.71  \\
          First ($l=2$) &\textbf{93.93} & \textbf{99.58} & \textbf{99.68} & 99.71  \\
          First ($l=1$) &59.46 & 53.69 & 82.42 & 88.51  \\
           Last ($l=10$) & 0.01& 0.04 &0.06  & 0.17  \\
            Last ($l=5$) &  0.03 & 0.19 &  0.46 & 1.14\\
        \hline
    \end{tabular}
    }
    %\vspace{-5pt}
    \label{tab:shuffle_layer}
\end{table}

Interestingly, for most models evaluated, the largest parameters (\emph{in absolute value}) tend to concentrate in these early layers. However, many models such as ShuffleNetV2 \citep{ma2018shufflenet} exhibit a different pattern: their largest parameters are concentrated in later layers. As a result, naive attacks that always target the largest parameters—often located in the late layers of ShuffleNetV2—are less effective. Redirecting the attack to early layers, however, significantly amplifies the damage (see Table~\ref{tab:shuffle_layer} for quantitative details).  


Based on these observations, we explore a simple heuristic that flips bits only in the first $l$ layers of a network (with $1 \leq l \leq 10$). \cref{alg:dnl_pass_free} summarizes our Pass-free attack for a given model, number of sign flips $k$, and layers $l$. We find that any $l$ in this range consistently degrades accuracy more than random or purely magnitude-based strategies (Figure~\ref{fig:boxplots_k_layers}). We select $l = 10$ for simplicity and only consider the parameters of those layers as candidates for sign flips. 


\begin{figure}[tb!]
\centering
\includegraphics[width=0.6\linewidth]
{figs/boxplots_k_layers.pdf}%\vspace{-10pt}
\caption{Targeting only the first $l$ layers, x-axis report $mAP(10)$.}
\label{fig:boxplots_k_layers}
%\vspace{-5pt}
\end{figure}



\begin{algorithm}[ht]
\caption{Deep Neural Lesion (DNL) -- Pass-free Attack}
\label{alg:dnl_pass_free}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} Model parameters $\theta$, number of bits to flip $k$, number of layers $L$
\STATE $\theta_L \leftarrow \text{parameters in the first $L$ layers of } \theta$
\STATE Sort $\theta_L$ in descending order by $|\theta_i|$
\STATE $\mathcal{K} \leftarrow \text{top-$k$ entries of } \theta_L \text{ (one per kernel)}$
\FOR{each $\theta_i$ in $\mathcal{K}$}
  \STATE $\theta_i \leftarrow -\theta_i \quad \text{// flip sign bit}$
\ENDFOR
\STATE \textbf{Output:} Modified parameters $\theta$
\end{algorithmic}
\end{algorithm}



\subsection{Enhanced Attack Using a Single Forward Pass}

When a single forward (and backward) pass is within the attacker's budget, we propose an enhanced attack, called \emethodname{}, inspired by \emph{gradient-based pruning} methods~\citep{LeCun1989OptimalBD,skeletonization,SNIP,GRASP,synflow}. These methods typically assign a saliency or importance score to each parameter $\theta_i$ by measuring how altering that parameter (e.g., pruning or modifying it) would affect the network’s loss or outputs. Although pruning and sign-flip attacks differ in goal, the underlying idea of identifying the ``most critical'' weights is similar.

\textbf{Hybrid Importance Score}

We define a hybrid importance scoring function that combines magnitude-based saliency with second-order information. Specifically, we start from a second-order Taylor expansion of a scalar loss function $\mathcal{R}(\theta)$ around $\theta_i$ \citep{LeCun1989OptimalBD,OBS}. Let $\alpha$ and $\beta$ be tunable coefficients controlling the relative weight of magnitude- and gradient-based terms. For a given parameter~$\theta_i$,
\begin{equation}
\label{eq:score-hybrid}
\mathcal{S}(\theta_i) \;=\; \alpha \,|\theta_i|\;+\;\beta \,\Biggl|\,
\frac{\partial \mathcal{R}}{\partial \theta_i}\,\theta_i 
\;+\;\frac{1}{2}\,H_{ii}\,\theta_i^2
\;+\;\sum_{j\neq i}H_{ij}\,\theta_i\,\theta_j
\Biggr|,
\end{equation}
where $H$ is the Hessian of $\mathcal{R}$ with respect to $\theta$. In our case, we let $\alpha=\beta=1$, $\mathcal{R}(\theta)\!=\!\sum_i c_i$, where $c_i$ is the output logit (or class score) for a Gaussian input. Although the summation over $j\neq i$ captures inter-weight coupling, we approximate $H_{ij}=0$ for $j\neq i$ (a common diagonal approximation in second-order pruning~\citep{LeCun1989OptimalBD}), significantly reducing computation. Similarly, we replace $H_{ii}$ by $(\frac{\partial \mathcal{R}}{\partial \theta_i})^2$ (i.e., a Gauss-Newton like approximation), which further simplifies Hessian-based estimation.

- If $\frac{\partial \mathcal{R}}{\partial \theta_i}\!=\!0$ and $H_{ii}\!=\!0$, Eq.~\eqref{eq:score-hybrid} reduces to:
\begin{equation*}
\mathcal{S}(\theta_i) \;=\; \alpha\,|\theta_i|,
\end{equation*}
mirroring a simple magnitude-based saliency score (identical to Equation~\ref{eq:magnitude}).

- If $\alpha\!=\!0$, we recover a purely second-order (Optimal Brain Damage-like) approach:
\begin{equation*}
\mathcal{S}(\theta_i) \;=\; \beta\,\Bigl|\,
\frac{\partial \mathcal{R}}{\partial \theta_i}\,\theta_i 
\;+\;\tfrac{1}{2} H_{ii}\,\theta_i^2
\Bigr|,
\end{equation*}
which focuses on predicted changes in $\mathcal{R}$ under small parameter perturbations.

\begin{algorithm}
\caption{1P-DNL -- Single-Pass Attack}
\label{alg:1p_dnl}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} Model $f_{\theta}$, number of bits to flip $k$, number of layers $L$
\STATE $X \leftarrow \text{random input (e.g., Gaussian noise)}$
\STATE $\mathcal{R}(\theta) \leftarrow \sum_i f_{\theta}(X)[i] \quad \text{// e.g., sum of logits}$
\STATE $g \leftarrow \nabla_{\theta}\,\mathcal{R}(\theta) \quad \text{// one backward pass}$
\STATE $\theta_L \leftarrow \text{parameters in the first $L$ layers of } \theta$
\FOR{each $\theta_i$ in $\theta_L$}
  \STATE Approx.\ Hessian diagonal by Gauss–Newton: $H_{ii} \approx [g_i]^2$ 
  \STATE $\displaystyle \mathcal{S}(\theta_i) \leftarrow
         |\theta_i|
         + \Bigl|\theta_i \, g_i + \tfrac12\,\theta_i^2\,H_{ii}\Bigr| \quad$
\ENDFOR
\STATE Sort $\theta_L$ in descending order by $\mathcal{S}(\theta_i)$
\STATE $\mathcal{K} \leftarrow \text{top-$k$ entries of } \theta_L \text{ (one flip per kernel)}$
\FOR{each $\theta_i$ in $\mathcal{K}$}
  \STATE $\theta_i \leftarrow -\theta_i \quad \text{// flip sign bit}$
\ENDFOR
\STATE \textbf{Output:} Modified parameters $\theta$
\end{algorithmic}
\end{algorithm}

Although one forward/backward pass on random (Gaussian) data might be required to estimate $\mathcal{S}$, it remains significantly simpler than full data-driven optimization-based attacks (e.g., iterative gradient-based bit-flips). Figure~\ref{fig:box_simple} shows that incorporating second-order signals consistently amplifies the attack’s damage compared to purely magnitude-based methods.
Consequently, this hybrid scoring approach yields a more powerful single-pass sign-flip attack in scenarios where the attacker can run a forward and backward pass on the architecture, yet does not have access to the original training set.
To summarize \emethodname{}, we refer the reader to \cref{alg:1p_dnl}.
Figure~\ref{fig:max_box} shows the impact of all previously suggested methods with 10 sign flips. 
Both \methodname{} and \emethodname{} cause most models to collapse, with 43 out of 48 models exhibiting an accuracy reduction above 60\%.
Finally, in Appendix~\ref{apdx:ablate_score} we compare 1P-DNL with various other 1-pass methods from the weight pruning literature to find critical parameters and find 1P-DNL the most potent.

\section{Additional Analysis}\label{sec:additional_analysis} We further assess our sign-flip attack on various datasets to confirm its broad applicability beyond ImageNet. In particular, we evaluate DNL and \emethodname{} on DTD \citep{DTD}, FGVC-Aircraft \citep{FGVC}, Food101 \citep{FOOD101}, and Stanford Cars \citep{CARS}. In \cref{fig:datasets_avg_models} and \cref{fig:datasets_avg_models_1p}, we plot the average accuracy reduction across EfficientNetB0 \citep{tan2019efficientnet}, MobileNetV3-Large, and ResNet-50 \cite{he2015deep} after applying \methodname{} and \emethodname{} respectively. In all cases, flipping as few as one or two sign bits leads to a sudden collapse in model performance, reaffirming that this vulnerability is not tied to a specific dataset or image distribution. Most notably, applying \methodname{} disrupts all models across all datasets with $AR(5) \geq 85\%$, and \emethodname{} achieve $AR(4) \geq 90\%$ across all data and models. For additional evaluations on each dataset, see \cref{apdx:datasets}.

Taken together, these results underscore both the potency and the generality of our approach: across diverse vision tasks and architectures, zero or one-pass sign-flip attacks consistently induce dramatic failures with minimal computational overhead or data requirements.

\begin{figure*}[tb]
\centering
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/various_datasets_degradation_graph.pdf}
    \caption{Averaged $AR$ (\%) of \methodname{} over EfficientNetB0, MobileNetV3-Large, and ResNet-50 vs number of sign flips. Each color represents a different dataset, confirming the impact of our pass-free attack on DTD, FGVC-Aircraft, Food101, and Stanford Cars.}
    \label{fig:datasets_avg_models}
\end{minipage}%
\hfill
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/various_datasets_degradation_graph_111.pdf}
    \caption{Averaged $AR$ (\%) of \emethodname{} over EfficientNetB0, MobileNetV3-Large, and ResNet-50 vs number of sign flips. Each color represents a different dataset, confirming the fatality of our single-pass attack on DTD, FGVC-Aircraft, Food101, and Stanford Cars. } 
    \label{fig:datasets_avg_models_1p}
\end{minipage}
\end{figure*}


% \begin{figure}[tb]
% \centering
% \includegraphics[width=\linewidth]
% {figs/various_datasets_degradation_graph.pdf}%\vspace{-10pt}
% \caption{Averaged $AR$ (\%)  of \methodname{} over EfficientNetB0, MobileNetV3-Large, and ResNet-50 vs number of sign flips. Each color represents a different dataset, confirming the impact of our pass-free attack on DTD, FGVC-Aircraft, Food101, and Stanford Cars. } %EfficientNetB0, MobileNetV3-Large & ResNet-50
% \label{fig:datasets_avg_models}
% \end{figure}



% \begin{figure}[tb]
% \centering
% \includegraphics[width=\linewidth]
% {figs/various_datasets_degradation_graph_111.pdf}%\vspace{-10pt}
% \caption{Averaged $AR$ (\%) of \emethodname{} over EfficientNetB0, MobileNetV3-Large, and ResNet-50 vs number of sign flips. Each color represents a different dataset, confirming the fatality of our single-pass attack on DTD, FGVC-Aircraft, Food101, and Stanford Cars. } %EfficientNetB0, MobileNetV3-Large & ResNet-50
% \label{fig:datasets_avg_models_1p}
% \end{figure}

\textbf{Impact of Model Size on Attack Success}

To assess whether model size influences the effectiveness of our attacks, we evaluate both \methodname{} and \emethodname{}, across five families of architectures with varying parameter counts: ResNet, RegNet, EfficientNet, ConvNeXt \citep{liu2022convnet}, and ViT \citep{dosovitskiy2020image}. The results, summarized in \cref{fig:model_size_1p} and \cref{fig:model_size}, reveal that model size does not exhibit a clear correlation with attack susceptibility. Most models collapse at similar levels regardless of their scale, demonstrating that our attack effectively scales to model size and disrupts networks of all sizes.

\begin{figure}[tb!]
\centering
\includegraphics[width=0.85\linewidth]
{figs/model_size_vs_damage_1p.pdf}%\vspace{-8pt}
\caption{$AR$ reported across five model families of varying capacities under \emethodname{} attack. The similar vulnerability levels suggest that model size alone does not mitigate sign-flip attacks.} 
\label{fig:model_size_1p}
\end{figure}


\section{Selective Defense Against Sign-Flips}
\label{sec:defense}

A straightforward way to protect against sign-flip attacks is to maintain multiple copies of the model's sign bits and compare them at inference time. Flipping just a few bits in one or two copies would not suffice to degrade the predictions, since a majority vote of the copies will apply before prediction, forcing an attacker to corrupt majority of the copies simultaneously. Although effective, this strategy inflates memory usage and computational overhead.

A more memory-efficient approach is to employ \emph{error-correcting codes} (ECC) \citet{peterson1972error}, such as Hamming codes, which detect and correct single-bit errors per memory word. ECC helps safeguard against sign flips by automatically reversing small-scale corruptions. However, as the number of bits or parameters grows, stronger ECC schemes (e.g., multi-bit correction) can be required, further increasing memory overhead.

Instead of uniformly coding every sign bit, a key insight is that \emph{only a small fraction of sign bits are genuinely critical}. By identifying large-magnitude parameters (those whose sign flips cause catastrophic degradation) one can selectively apply ECC or bit-replication only to these ``critical'' sign bits. This selective defense still allows occasional bit flips in less vulnerable areas but maintains overall accuracy at a fraction of the cost of safeguarding all sign bits.

To quantify this selective defense, we tested it on 16 particularly vulnerable networks, each suffering at least a 50\% accuracy reduction ($AR(100,000)\!\geq\!50\%$) when 100K random parameters were flipped.
We use this large-scale random flip as a strong, non-specific stress test that is not tied to our own scoring method, ensuring that the defense remains robust to other score-based sign bit flips.
We then varied the fraction of protected sign bits from 1\% to 20\%, focusing on the largest weights in absolute value. As expected, even a modest level of protection dramatically reduced the damage inflicted by sign-flip attacks. Moreover, as illustrated in \cref{fig:defend_box}, selectively safeguarding this small subset of sign bits mitigates the impact from sign bit flip attacks, as reflected from the stress test proposed above, demonstrating that partial protection of critical parameters offers a practical and effective defense
In \cref{apdx:defence}, we show that naive defense mechanisms would have been unsuccessful in defending against sign flips.


\begin{figure}[tb!]
\centering
\includegraphics[width=0.65\linewidth]
{figs/defendd.pdf}
\caption{$AR(100,000)$ under 100k random sign flips, with selective protection on varying fractions of the most vulnerable parameters (ranked by 
\methodname{}). Even partial coverage of high-scoring parameters substantially improves robustness.
}
\label{fig:defend_box}
%\vspace%{-20pt}
\end{figure}


\section{Related Work on Bit-Flip Attacks}
\label{sec:related}
Early works such as \textit{Terminal Brain Damage} (TBD)~\citet{TBD} illustrated how manipulating exponent bits could severely harm floating-point networks. However, TBD excludes sign bits, which we find can be far more devastating to overall accuracy with fewer flips.  
Other methods, including \citep{BFA, DeepHammer}, perform iterative gradient-based flips. For example, \citet{BFA} requires multiple samples to compute gradients and can disrupt ResNet-50’s accuracy by $\sim99.7\%$ using 11 bit flips. %(on INT8 networks).
\citet{DeepHammer} similarly needs iterative optimization, reaching significant disruption at the cost of 23 flips.  

Recent variants have attempted to relax data requirements. For instance, \citep{ghavami2021bdfa, park2021zebra} generate pseudo-samples or use partial data statistics to guide which bits to flip. Although they lessen the need for a large labeled dataset, they still rely on model feedback or approximate gradients. In contrast, our sign bit flipping approach is lightweight, data-agnostic, and can degrade a large variety of networks by by over $99.8\%$ with few flips. This distinction stems from focusing on sign bits, which, due to the abrupt change from $+$ to $-$, often exert a disproportionate influence on learned representations.

While prior research highlights the vulnerability of neural networks to parameter corruption, the striking simplicity and severity of sign flips merit closer scrutiny. As shown in \cref{tab:compared}, these flips can be carried out without data or optimization, and are straightforward to locate in memory, making them both feasible and devastating in real-world scenarios.



\section{Concluding Remarks}
\label{sec:concluding}

We presented a previously unknown vulnerability that is critical for the security of DNNs. We also showed a simple way to make models robust to such vulnerabilities.
\newline Our findings highlight a pressing need to re-examine security and robustness in DNN deployments, particularly in safety-critical contexts. Beyond inspiring the development of targeted sign-flip defenses, our results also open questions about model architectures and training regimes that can inherently mitigate parameter vulnerabilities. We hope our work will encourage the research community to explore architectural, optimization, and hardware-level strategies to build DNNs more robust against sign-bit attacks.


% \section*{Impact Statement}
% Our work reveals a critical vulnerability in neural networks that, if exploited, could result in malicious disruptions in applications ranging from autonomous vehicles to healthcare applications. By demonstrating how few sign-bit flips can induce large-scale failures, we highlight the importance of developing robust defenses and secure deployment practices. We believe that this work will encourage further research on resilient models, memory protections, and fail-safe mechanisms. While there is potential for misuse, we hope this work ultimately spurs constructive efforts to harden deep learning systems against a variety of feasible threats and improves overall trust in machine learning deployments.

\section{Acknowledgments}
The research was partially supported by Israel Science Foundation, grant No 765/23

\nocite{langley00}

\bibliography{refrences}
\bibliographystyle{plainnat} %icml2025



\newpage
\appendix

\section{Compare to Bit Flip Attacks}
\label{apdx:compare}

Following \cref{sec:related}, \cref{tab:compared} compares bit-flip attacks on ImageNet1K and highlights how our approach differs from prior methods. BFA~\cite{BFA} and DeepHammer~\cite{DeepHammer} rely on iterative gradient-based optimization and require partial data or repeated inference steps. ZeBRA~\cite{park2021zebra} removes the need for real data but still employs optimization. In contrast, our methods (\methodname{} and \emethodname{}) are both data-agnostic and optimization-free, targeting sign bits with a lightweight, single-pass or pass-free strategy. We compare the complexity of prior art to ours in \cref{tab:comp_cost}.
Despite this simplicity, they match or surpass prior work in accuracy reduction (\textbf{AR}) while needing only a handful of bit flips as shown in \cref{tab:compared}; for instance, \emethodname{} degrades ResNet-50 by 99.8\% with just two sign flips. This balance of minimal computation and high impact underscores the severity of sign-bit vulnerabilities in modern DNNs.


\begin{table}[ht]
\centering
\caption{Approximate computational costs for different bit-flip attacks. 
Here, $\theta$ is the number of parameters in the model, 
$k$ is the number of flipped bits, 
$m$ is the (mini-)batch size used for gradient or scoring, 
and $B$ is the number of candidate bits evaluated in each iteration. 
All complexities assume that a forward/backward pass scales on the order of $\mathcal{O}(\theta \times m)$.}
\label{tab:comp_cost}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{l l c}
\toprule
\textbf{Method} & \textbf{Description} & \textbf{Complexity} \\
\midrule
\textbf{BFA} \cite{BFA} 
& Iterative gradient-based search; each flip requires scoring multiple bits
& $\mathcal{O}(k \times B \times \theta \times m)$\\[4pt]

\textbf{DeepHammer} \cite{DeepHammer}
& Chain-based iterative search for each flip 
& $\mathcal{O}(k \times B \times \theta \times m)$\\[4pt]

\textbf{ZeBRA} \cite{park2021zebra}
& Zero real-data, but still repeated forward/backward passes per flip
& $\mathcal{O}(k \times B \times \theta \times m)$\\[4pt]

\midrule
\textbf{DNL (Ours)}
& \textbf{Pass-free}; select bits by magnitude only 
& $\mathcal{O}(\theta) + \mathcal{O}(k)$\\[4pt]

\textbf{1P-DNL (Ours)}
& \textbf{Single-pass}; one forward/backward pass
& $\mathcal{O}(\theta) + \mathcal{O}(k)$\\
\bottomrule
\end{tabular}}
\end{table}



\begin{table}[h]
    \centering
    \caption{Comparison of different bit flip attack methods on the ImageNet1K benchmark. Our method is the only one that is both data-agnostic (\textbf{DA}) and optimization-free (\textbf{OF}). $^*$ denotes the best result out of 5 trials.}
    \resizebox{.7\columnwidth}{!}{
    \label{tab:compared}%
    \begin{tabular}{c l c c c c}
        \hline
        \rowcolor{gray!20} \textbf{Model} & \textbf{Method} & \textbf{OF} & \textbf{DA} & \textbf{AR} & \textbf{\# Flips}\\
        \hline
        
        \multirow{5}{*}{AlexNet} & BFA \cite{BFA} & \textcolor{myred}{\ding{55}} & \textcolor{myred}{\ding{55}} & 99.6 & 17 \\
        & \textbf{\methodname{}} & \textcolor{mygreen}{\checkmark} & \textcolor{mygreen}{\checkmark} & 88.5 & 10 \\
        & \textbf{\emethodname{}} & \textcolor{mygreen}{\checkmark} & \textcolor{mygreen}{\checkmark} & 93.2 & 10 \\
         & \textbf{\methodname{}} & \textcolor{mygreen}{\checkmark} & \textcolor{mygreen}{\checkmark} & 98.15 & 17 \\
         & \textbf{\methodname{}} & \textcolor{mygreen}{\checkmark} & \textcolor{mygreen}{\checkmark} & 98.15 & 17 \\
        \hline
        
        \multirow{4}{*}{VGG11} & BFA \cite{BFA} & \textcolor{myred}{\ding{55}} & \textcolor{myred}{\ding{55}} & 99.7 & 17  \\
        & ZeBRA \cite{park2021zebra} & \textcolor{myred}{\ding{55}} & \textcolor{mygreen}{\checkmark} & 99.7 & 9 \\
        & \textbf{\methodname{}} & \textcolor{mygreen}{\checkmark} &\textcolor{mygreen}{\checkmark} & 98.8 & 9 \\
        & \textbf{\methodname{}} & \textcolor{mygreen}{\checkmark} &\textcolor{mygreen}{\checkmark} & 99.37 & 17 \\
        \hline

        
        \multirow{5}{*}{ResNet-50} & BFA \cite{BFA} & \textcolor{myred}{\ding{55}} & \textcolor{myred}{\ding{55}} & 99.7 & 11 \\
        & DH \cite{DeepHammer} & \textcolor{myred}{\ding{55}} & \textcolor{myred}{\ding{55}} & 75.39 & $23^*$ \\
        & ZeBRA \cite{park2021zebra} & \textcolor{myred}{\ding{55}} & \textcolor{mygreen}{\checkmark} & 99.7 & 5 \\
        & \textbf{\methodname{}} & \textcolor{mygreen}{\checkmark} & \textcolor{mygreen}{\checkmark} & 51.9 & 10 \\
        & \textbf{\emethodname{}} & \textcolor{mygreen}{\checkmark} & \textcolor{mygreen}{\checkmark} & 99.8 & 2 \\
        \hline
        
        \multirow{4}{*}{MobileNet-V2} & DH \cite{DeepHammer} & \textcolor{myred}{\ding{55}} & \textcolor{myred}{\ding{55}} & xx & $2^*$ \\
        & ZeBRA \cite{park2021zebra} & \textcolor{myred}{\ding{55}} & \textcolor{mygreen}{\checkmark} & 99.8 & 2 \\
        & \textbf{\methodname{}} & \textcolor{mygreen}{\checkmark} & \textcolor{mygreen}{\checkmark} & 99.8 & 2 \\
        & \textbf{\emethodname{}} & \textcolor{mygreen}{\checkmark} & \textcolor{mygreen}{\checkmark} & 99.7 & 2 \\
        \hline
    \end{tabular}
    }
\end{table}


\newpage
\section{Weight Score Ablation}
\label{apdx:ablate_score}

We evaluate several parameter scoring functions from the pruning literature and compare their effectiveness in identifying high-impact weights for sign-flip attacks. As shown in \cref{fig:ablate_score_box}, we measure the mean accuracy reduction $\text{mAR}_{10}$ across 48 ImageNet models under the following scoring functions:

\begin{itemize}
\item \textbf{Magnitude-based:} $S(\theta_i) = |\theta_i|$.
\item \textbf{GraSP:} $S(\theta_i) = \bigl|\theta_i \odot Hg\bigr|$, following the gradient-flow preservation principle of \citet{GRASP} where $Hg$ is the hessian vector product.
\item \textbf{GraSP (Gauss-Newton Approx.):} Similar to GraSP but approximates the Hessian $H$ with the square of first-order gradients. \item \textbf{SynFlow:} $S(\theta_i) = \bigl|g \odot \theta_i \bigr|$, akin to gradient \emph{times} weight.
\item \textbf{Optimal Brain Damage (OBD):} $S(\theta_i) \approx \tfrac{1}{2} \theta_i^t H_{ii} \theta_i$ \citep{LeCun1989OptimalBD}.
\item \textbf{Hybrid (Ours):} As we define in \cref{eq:score-hybrid} with and without second order term. \end{itemize}

where $g=\tfrac{\partial \mathcal{R}}{\partial \theta_i}, H=\tfrac{\partial^2 \mathcal{R}}{\partial \theta_i^2}$.


We observe that certain models are vulnerable to second-order-based scores (e.g., OBD) even when they prove more resilient to pure magnitude-based attacks. Nevertheless, other architectures appear more robust against OBD or GraSP while showing larger drops under magnitude-based score. Motivated by these mixed results, our hybrid score combines both magnitude and gradient terms. This blend consistently identifies critical weights even in cases where either component alone fails to degrade accuracy. Overall, the hybrid approach delivers the most reliable performance drop across the tested models.


\begin{figure}[tb!]
\centering
\includegraphics[width=\linewidth]
{figs/ablate_boxplots.pdf}
\caption{Comparison of $mAR_{10}$ across different weight score functions for the model parameters applied to 48 ImageNet models.}
\label{fig:ablate_score_box}
\end{figure}



\section{Additional Datasets Evaluation}
\label{apdx:datasets}

Figures~\ref{fig:DTD}, \ref{fig:FGVC}, and \ref{fig:Food101} analyze individual dataset results on these three popular classifiers. Each shows a steep drop in accuracy with very few sign flips, highlighting the generality of the attack. Notably, although these models differ in architecture and capacity, they all exhibit severe degradation once our detected sign bits are flipped. This finding reinforces that our method targets fundamental weaknesses in DNN representations rather than exploiting quirks of a specific network or dataset.


\begin{figure*}[tb!]
\centering
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/DTD.pdf}
    \caption{$AR$ (\%) on DTD dataset \citep{DTD} with varying number of sign flips over popular image encoders.}
    \label{fig:DTD}
\end{minipage}%
\hfill
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/FGVC.pdf}
    \caption{$AR$ (\%) on FGVC Aircraft dataset \citep{FGVC} with varying number of sign flips over popular image encoders.}
    \label{fig:FGVC}
\end{minipage}
\vspace{10pt}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/FOOD.pdf}
    \caption{$AR$ (\%) on Food101 dataset \citep{FOOD101} with varying number of sign flips over popular image encoders.}
    \label{fig:Food101}
\end{minipage}%
\hfill
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/CARS.pdf}
    \caption{$AR$ (\%) on Stanford Cars dataset \citep{CARS} with varying number of sign flips over popular image encoders.}
    \label{fig:Stanford}
\end{minipage}
\end{figure*}



% \begin{figure}[tb!]
% \centering
% \includegraphics[width=0.7\linewidth]
% {figs/DTD.pdf}
% \caption{$AR$ (\%) on DTD dataset\citep{DTD} with varying number of sign flips over popular image encoders.} %
% \label{fig:DTD}
% \end{figure}



% \begin{figure}[tb!]
% \centering
% \includegraphics[width=0.7\linewidth]
% {figs/FGVC.pdf}
% \caption{$AR$ (\%) on FGVC Aircraft dataset \citep{FGVC} with varying number of sign flips over popular image encoders.} %%EfficientNetB0, MobileNetV3-Large & ResNet-50
% \label{fig:FGVC}
% \end{figure}

% \begin{figure}[tb!]
% \centering
% \includegraphics[width=0.7\linewidth]
% {figs/FOOD.pdf}
% \caption{$AR$ (\%) on Food101 dataset \citep{FOOD101} with varying number of sign flips over popular image encoders.} %%EfficientNetB0, MobileNetV3-Large & ResNet-50
% \label{fig:Food101}
% \end{figure}

% \begin{figure}[tb!]
% \centering
% \includegraphics[width=0.7\linewidth]
% {figs/CARS.pdf}
% \caption{$AR$ (\%) on Stanford Cars dataset \citep{CARS} with varying number of sign flips over popular image encoders.} %%EfficientNetB0, MobileNetV3-Large & ResNet-50
% \label{fig:Stanford}
% \end{figure}



\begin{figure}[tb!]
\centering
\includegraphics[width=\linewidth]
{figs/model_size_vs_damage.pdf}
\caption{$AR$ of 5 families of models with different sizes attacked with 10 sign-flips by \methodname{}. } %EfficientNetB0, MobileNetV3-Large & ResNet-50
\label{fig:model_size}
\end{figure}


\section{Defense Baseline}
\label{apdx:defence}

In addition to selectively protecting the most impactful sign bits, we also tested a baseline defense that shields a randomly chosen subset of bits at different coverage levels. \cref{fig:random_defend_box} shows that even when 20\% of the sign bits are randomly protected, the network remains highly vulnerable under 100k random sign flips. This stands in stark contrast to protecting only a small fraction of critical sign bits (e.g., the largest-magnitude weights), which can substantially preserve accuracy. The results underscore that which bits get protected is more important than how many.

\begin{figure}[tb!]
\centering
\includegraphics[width=\linewidth]
{figs/random_defend.pdf}%\vspace{-10pt}
\caption{$AR(100,000)$ under 100k random sign flips, with random subsets (1\%, 5\%, 10\%, and 20\% coverage) of sign bits protected. Unlike \cref{fig:defend_box}, where shielding the most vulnerable bits significantly reduces damage, uniform random selection offers little resilience, as even 20\% coverage barely mitigates the attack.}
\label{fig:random_defend_box}
%\vspace%{-20pt}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
