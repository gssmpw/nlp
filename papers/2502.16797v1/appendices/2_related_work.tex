\section{Extended related work}\label{app:related_work}


\textbf{LLMs \& scaling laws.} Language models are now used as general-purpose tools \citep{openai2024reasoning, anthropic2024claude3, team2024gemini}, quantitative reasoners \citep{llm-quant-reasoning, llm-lean}, coding assistants or agents \citep{nijkamp2023codegenopenlargelanguage, li2023starcodersourceyou}, and as zero-shot base predictors in scientific discovery \citep{llm-funcprotein, llm-chemtools, llm-mol}. 
This progress is partly predicted by language model scaling laws, which show that performance predictably scales with compute \citep{kaplan2020scaling, brown2020language, hoffmann2022training, wei2022emergent, pmlr-v162-borgeaud22a}. 


\textbf{Model Safety.} There are many documented risks of language models; see \citep{weidinger2021ethicalsocialrisksharm, bommasani2022opportunitiesrisksfoundationmodels, ji2024aialignmentcomprehensivesurvey, bengio2025international} for surveys. Some salient risks include spreading misinformation \citep{pmlr-v202-kandpal23a, EDTX2024}, amplifying social and political biases \cite{gallegos-etal-2024-bias, llm-bias}, use for cyber-offense \citep{ ncsc2025impact, metta2024generativeaicybersecurity}, and loss of control \citep{doi:10.1126/science.131.3410.1355, good1966speculations, 10.5555/1566174.1566226}, among others.