\section{Related Work}
There are two approaches when it comes to classifying network traffic using machine learning. One approach considers each packet individually and makes predictions based on the features extracted from the packet itself. The other approach classifies a flow of packets (a stream of packets transferred between a pair of source and destination for a specific application), which has the benefit of capturing the sequential dependencies that arise from features extracted from the flow. This approach can also be processed using methods inspired by the field of Natural Language Processing (NLP). With the advent of deep learning as one of the most effective machine learning methods, achieving high accuracy in many fields, many recent studies have employed different neural network architectures to classify network traffic datasets. In \cite{maghalebasecrnn}, Lopez-Martin et al. present a deep Convolutional Recurrent Neural Network (CRNN) architecture to classify network flows and tune the hyperparameters of the model to find the optimal settings and feature set. However, they have not addressed the imbalance problem in their dataset. Additionally, the scale of the dataset used in their study does not reflect the real-world scale of internet traffic in terms of volume and the number of classes. In \cite{springer_deep}, Rahul et al. also proposed using Convolutional Neural Networks (CNN) to classify network traffic, but their work only considers three classes of applications on a limited dataset. In \cite{sharifia}, a comparison between CNN and Stacked Auto-Encoders for classifying both types of traffic and applications in the network in a standard VPN/none-VPN dataset is presented. In this method, instead of flows, each packet is classified individually based on the features from both header and payload of packets which may not be available in some privacy-preserving datasets. In \cite{bayesianneural}, Auld et al. deploy a Bayesian neural network in the form of a multi-layer perception and classify their dataset accordingly. In this work, the proposed method tends to underperform when it comes to classes with the lowest number of data in the dataset. 
Transformers \cite{vaswani2017attention} are state-of-the-art deep learning models that use the self-attention mechanism to weigh the relevance of each input data element differently. They are primarily used in natural language processing (NLP) and computer vision. Recently, Transformers have also been widely used in Network Traffic Classification (NTC) tasks. PERT \cite{he2020pert} is a Transformer-based model that uses the payload of packets to classify encrypted HTTPS traffic data. The Multi-task Transformers (MTT) \cite{zheng2022mtt} is a multi-task learning approach that simultaneously classifies traffic characterization and application identification tasks. To extract features, the proposed model treats the input packet as a sequence of bytes and employs a multi-head attention mechanism. FlowFormers \cite{FlowFormers} enhances the FlowPrint representation with attention-based Transformer encoders. This model outperforms traditional deep learning models on NTC tasks such as application type and provider classification.

Some previous work has attempted to address the imbalance issue in network traffic datasets. In \cite{PGMMOORE}, Rotsos et al. introduced a method using Probabilistic Graphical Models for semi-supervised learning in a Naive Bayes model. For their learning, they assumed a Dirichlet distribution before the classes with a high $\alpha$ value. This assumption is based on the idea that some classes have a higher probability than others. In \cite{acganaug}, an augmentation method is proposed using an Auxiliary Classifier Generative Adversarial Network (AC-GAN). However, only two classes of network traffic are considered: SSH and non-SSH, and the dataset used does not extend to other network applications. Furthermore, the method is only evaluated on traditional machine learning algorithms such as Support Vector Machines, Random Forest, and Naive Bayes. Additionally, \cite{devide&conqueraug} presents a new feature extraction method using a divide-and-conquer approach for handling imbalanced datasets in network traffic.

For augmentation purposes, one needs to simulate the sequential patterns of flows through time series generation methods. LSTMs \cite{LSTM}, as one of the most popular methods in sequence representation learning, have also been utilized for generating sequential data. For example, \cite{LSTMGEN} introduced a method to generate data using LSTMs and evaluated the method to show that it can capture the temporal features in the dataset. LSTMs have also been used as an augmentation tool in works such as \cite{daskhataug} and \cite{LSTM_ae_aug} for generating handwriting and human movement data, respectively, and have proven to be effective in both cases.

Embedding methods for sequences of words are critical in transforming raw text data into numerical representations that machine learning models can effectively process. These embeddings, such as Word2Vec, GloVe, or transformer-based models like BERT, capture semantic relationships and contextual meaning between words, allowing models to understand the underlying structure of language. By converting words into dense vectors in a continuous vector space, embeddings reduce the dimensionality of the input data, leading to more efficient training and faster convergence. This transformation helps models learn more effectively, as they can focus on meaningful features instead of raw word counts or one-hot encodings. As a result, embedding methods not only speed up the convergence process but also improve the overall performance of models, enabling them to generalize better on unseen data and handle complex tasks with higher accuracy.

Flows in a network resemble sentences in a language. As a result, word embedding techniques can be useful when representing a network flow. In word embedding \cite{turian2010word}, words with similar meanings are represented similarly in a learned representation of text. One advantage of adopting dense, low-dimensional vectors is the reduction of computational costs. Most neural networks do not operate well with very high-dimensional, sparse vectors, such as one-hot encoding, especially when the number of categories is enormous \cite{li2018slim}. Hence, a neural network must perform computationally expensive operations to properly learn a representation that relates each word to another. An embedding layer \cite{gal2016theoretically} is a word embedding approach that is learned alongside a neural network model. The embedding layer allows us to convert each word into a fixed-length vector of a specific size. The resulting vector is dense, with real values, rather than just 0’s and 1’s. The fixed length of word vectors allows us to better represent words while reducing dimensionality. To the best of our knowledge, previous work in this area employs models that are either too computationally expensive or fail to consider performance on the sparse classes in the dataset. We aim to address these issues through our proposed augmentation scheme, which uses LSTM to generate traffic flow patterns that balance the dataset, as well as a flow packet embedding scheme that enhances both the speed and accuracy of the deep learning model.