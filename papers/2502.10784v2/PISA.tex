\documentclass[11pt,a4paper]{article}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry} 
 
\usepackage{amsmath,amssymb,amsthm,amsfonts,subcaption,caption}
\usepackage[colorlinks=True,
            linkcolor=blue,
            anchorcolor=green,
            citecolor=blue
            ]{hyperref} 
\usepackage[noabbrev,capitalise,nameinlink]{cleveref}
\usepackage{float}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{multirow} 
\usepackage{enumitem,url}
\usepackage{threeparttable}  

\usepackage[ruled,vlined]{algorithm2e}
\setlength{\algomargin}{8pt}
\usepackage{etoolbox}

 
 \newtheorem{theorem}{Theorem}[section]
\newtheorem{condition}{Condition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{scheme}{Scheme}[section]
\newtheorem{example}{Example}[section]
\newtheorem{setup}{Setup}[section]

\usepackage{hyperref}
\newcommand{\footremember}[2]{%
    \footnote{#2}
    \newcounter{#1}
    \setcounter{#1}{\value{footnote}}%
}
\newcommand{\footrecall}[1]{%
    \footnotemark[\value{#1}]%
}

 
\textwidth  6.5in 
\textheight 10in 
\topmargin -.75in
\addtolength{\evensidemargin}{-0.2cm}
\addtolength{\oddsidemargin}{-0.2cm}
\linespread{1.25} 
\parskip 1.5mm 
\setlength\parindent{0pt}

\graphicspath{ {./Figures/} }

\def\B{{\mathcal B}}
\def\BB{{\mathbb B}}
\def\N{{\mathbb N}}
\def\D{{\mathcal D}}
\def\L{{\mathcal L}}
\def\E{{\mathbb E}}
\def\R{{\mathbb R}}

\def\H{{\bf  H}}
\def\I{{\bf  I}}
\def\W{{\bf  W}}
\def\Q{{\bf  Q}}
\def\M{{\bf  M}}

\def\bu{{\bf u}}
\def\bv{{\bf v}} 
\def\bw{{\bf w}}
\def\bz{{\bf z}}
\def\bx{{\bf x}} 
\def\by{{\bf y}}
\def\br{{\bf r}}
\def\bm{{\bf m}}
\def\bg{{\bf g}}
\def\bd{{\bf d}}

 
\def\bpi{{\boldsymbol  \pi}} 
\def\P{{\boldsymbol  \Pi}}
\def\bsi{{\boldsymbol  \sigma}}
\def\bga{{\boldsymbol  \gamma}}
\def\eqspace{\arraycolsep=1.5pt\def\arraystretch}


\renewcommand{\abstractname}{}


\author{
  Shenglong Zhou\footremember{bjtu1}{School of Mathematics and Statistics, Beijing Jiaotong University, China (shlzhou@bjtu.edu.cn).}~
  Ouya Wang\footremember{SD}{ITP Lab, Department of EEE, Imperial College London, UK (ouya.wang20@imperial.ac.uk).}~ 
  Ziyan Luo\footremember{bjtu2}{School of Mathematics and Statistics, Beijing Jiaotong University, China (luozy@bjtu.edu.cn).}~
  Yongxu Zhu\footremember{seu}{National Mobile Communications Research Laboratory, Southeast University, China (yongxu.zhu@seu.edu.cn).}~
  Geoffrey Ye Li\footremember{icl1}{ITP Lab, Department of EEE, Imperial College London, UK (geoffrey.li@imperial.ac.uk).}
%  ITP Lab, Department of EEE\\  Imperial College London, United Kingdom \\
%  Emails:\{shenglong.zhou, geoffrey.li\}@imperial.ac.uk
}

\title{\vspace{-1.25cm}
Preconditioned Inexact Stochastic ADMM for Deep Models\thanks{Corresponding authors: Ouya Wang and and Shenglong Zhou.}
\vspace{-0.25cm}}

\date{}


\begin{document}
\flushbottom
 
\maketitle
 
\vspace{-1.3cm}



\begin{abstract}  
\noindent \textbf{Abstract:} The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA (\textbf{P}reconditioned \textbf{I}nexact \textbf{S}tochastic \textbf{A}lternating Direction Method of Multipliers), which enables scalable parallel computing and supports various second-moment schemes. Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables PISA to tackle the challenge of data heterogeneity effectively. Comprehensive experimental evaluations for training or fine-tuning diverse FMs, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate its superior numerical performance compared to various state-of-the-art optimizers. 

\vspace{0.3cm} 
 
\noindent{\textbf{Keywords}:} Foundation models, SGD, preconditioned inexact stochastic ADMM, global convergence, data heterogeneity, high numerical performance
\end{abstract}


\numberwithin{equation}{section}


 
 
%\maketitle

 

\section{Introduction}


 

Foundation models (FMs) have led to extensive applications across a variety of industries. For instance, fine-tuning large language models (LLMs), such as Gemma \cite{touvron2023llama2openfoundation}, GPT \cite{openai2024gpt4technicalreport}, and Llama \cite{gemmateam2024gemmaopenmodelsbased}, with user-specific data can significantly enhance the model performance. Diffusion models (DMs) are increasingly used to generate personalized content, including images, audio, and video, for entertainment. Vision models (VMs), like ResNet \cite{he2015deepresiduallearningimage} and vision transformers (ViT) \cite{dosovitskiy2020image}, are in high demand for tasks, such as image classification, segmentation, and object detection on large-scale image datasets. However, the growing complexity and scale of these tasks present considerable challenges for widely used stochastic gradient descent (SGD)-based optimizers due to their inherent limitations. In this section, we review a subset of these methods, sufficient to motivate the development of our proposed approach.
 

\subsection{SGD-based learning algorithms}
It is known that the plain SGD-based algorithms, such as the original stochastic approximation approaches \cite{robbins1951stochastic,chung1954stochastic}, the parallelized SGD \cite{zinkevich2010parallelized} for machine learning, and the newly developed FedAvg \cite{mcmahan2017communication, Li2020On} and LocalSGD \cite{stich2018local} algorithms for federated learning, are frequently prone to high sensitivity to poor conditioning \cite{novak2018sensitivity} and slow convergence in high-dimensional and non-convex landscapes \cite{chen2023symbolic}. To overcome these drawbacks, SGD with momentum and adaptive learning rates have been proposed to enhance robustness and accelerate convergence.  The former introduced first-order momentum to suppress the oscillation of SGD \cite{qian1999momentum}, and the latter updated the learning rate iteratively based on historical information. 
	For instance,  the adaptive gradient (AdaGrad  \cite{duchi2011adaptive}) interpolated the accumulated second-order moment of gradients to achieve the adaptive learning rate. 
	The root mean-squared propagation (RMSProp \cite{tieleman2012lecture}) exploited an exponential weighting technique to balance the distant historical information and the knowledge of the current second-order gradients, avoiding premature termination encountered by AdaGrad. 
	The adaptive moment (Adam  \cite{kingma2014adam}) combined the first-order moment and adaptive learning rates, thereby exhibiting robustness to hyperparameters. 
	AMSGrad \cite{jR2018on} took smaller learning rates than those of Adam by using a maximum value for normalizing the running average of the gradients, thereby fixing the convergence of Adam.
	 Padam \cite{chen2018closing} employed a similar strategy of AMSGrad with the difference of the rate power.
	AdaBound  \cite{luo2019adaptive} designed a clipping mechanism on Adam-type learning rates by clipping the gradients larger than
a threshold to avoid gradient explosion. 
	PRSGD   \cite{yu2019parallel} simply benefited from a decayed power learning rate. 
Lamb \cite{You2020Large} performed per dimension normalization with respect to the square root of the second moment used in Adam and set large batch sizes suggested by Lars \cite{you2017scaling}. For some other SGD-based algorithms, one can refer to  AdaDelta \cite{Zeiler2012ADADELTAAA}, Adamax  \cite{kingma2014adam}, AdamW \cite{loshchilov2017decoupled}, and those reviewed in nice surveys \cite{bottou2010large, ruder2016overview, bottou2018optimization}. 

The aforementioned algorithms primarily leveraged the heavy ball acceleration technique to estimate the first- and second-order moments of the gradient. An alternative approach for accelerating convergence is Nesterov acceleration, which has been theoretically proven to converge faster \cite{nesterov1983method,nesterov1988approach}. This has led to the development of several Nesterov-accelerated SGD algorithms. For example, NAdam \cite{dozat2016incorporating} integrated this technique into Adam to enhance convergence speed. More recently, Adan \cite{Adan24} adopted Nesterov momentum estimation to refine the estimation of the first- and second-order moments of the gradient, thereby improving adaptive learning rate adjustments for faster convergence.


\subsection{ADMM-based learning algorithms} 
The alternating direction method of multipliers  (ADMM) \cite{gabay1976dual,boyd2011distributed} is a promising framework for solving large-scale optimization problems because it decomposes complex problems into smaller and more manageable sub-problems. This characteristic makes ADMM particularly well-suited for various challenges in distributed learning. It has demonstrated substantial potential in applications, such as image compressive sensing \cite{yang2018admm}, federated learning \cite{feddp21, zhou2023federated, zhouli23}, reinforcement learning \cite{xu2023federated}, and few-shot learning \cite{wang2023new}.

 
When applied to deep model training, early studies first relaxed optimization models before developing ADMM-based algorithms. Therefore, these methods can be classified as model-driven/model-specific approaches. They targeted the relaxations rather than the original problems, enabling better handling of the challenges associated with highly non-convex optimization landscapes.  For instance,  an ADMM-based algorithm in \cite{taylor2016training} addressed a penalized formulation, avoiding pitfalls that hinder gradient-based methods in non-convex settings. Similarly, a deep learning ADMM algorithm \cite{wang2019admm} and its variant \cite{ebrahimi2024aa} were designed to enhance convergence by addressing penalization models as well. Moreover, the sigmoid-ADMM algorithm \cite{zeng2021admm} utilized the algorithmic gradient-free property to address saturation issues with sigmoid activation functions. Despite their advantages, model-driven ADMM approaches face two critical issues. Firstly, these algorithms exhibit high computational complexity because they require computing full gradients using the entire dataset and performing matrix inversions for weight updates, making them impractical for large-scale tasks. Additionally, the reliance on the specific structure of optimization models limits their general applicability, as different deep models often necessitate distinct formulations.
 
%To address these challenges, data-driven ADMM algorithms have garnered significant attention. For instance, deterministic ADMM methods  \cite{zhou2023federated,zhouli23}  have been developed for distributed learning problems. Due to a full batch of datasets used to compute gradients, these algorithms are computationally inefficient. To combat that, stochastic ADMM (SADMM) methods aim to replace full gradients with stochastic approximations. Representatives of plain SADMM are S-ADMM \cite{ouyang2013stochastic} and PS-ADMM \cite{ding2019stochastic}, which, however,  solve sub-problems exactly,  leading to high computational complexity. To further improve convergence, techniques like variance reduction and Nesterov acceleration have been integrated into SADMM algorithms. Examples include SAG-ADMM \cite{zhong2014fast}, SVRG-ADMM \cite{zheng2016fast}, and ASVRG-ADMM \cite{liu2017accelerated} for convex problems, as well as ASVRG-ADMM \cite{zeng2024accelerated} for non-convex problems. Nevertheless, these approaches require access to full gradients to reduce stochastic gradient variance, which still remains computationally demanding.

To address the aforementioned limitations, data-driven ADMM algorithms have emerged as a promising alternative, aiming to reduce computational burdens and enhance adaptability to diverse tasks. For instance, the deterministic ADMM \cite{zhou2023federated, zhouli23}, designed for distributed learning problems, achieved high accuracy but low computational efficiency due to the use of full dataset gradients. Stochastic ADMM (SADMM) methods tackled the inefficiency by replacing full gradients with stochastic approximations. Examples like S-ADMM \cite{ouyang2013stochastic} and PS-ADMM \cite{ding2019stochastic}  aimed to solve sub-problems exactly and thus still involved high computational costs. To further enhance convergence and reduce stochastic gradient variance, advanced techniques of variance reduction and Nesterov acceleration have been incorporated into SADMM. Representatives consist of SAG-ADMM \cite{zhong2014fast}, SPIDER-ADMM \cite{huang2019faster}, and ASVRG-ADMM  for convex \cite{liu2017accelerated} and non-convex problems \cite{zeng2024accelerated}. However, these enhanced methods still relied on access to full gradients for the aim of reducing stochastic gradient variance, posing challenges for large-scale applications.

%Prominent approaches include SAG-ADMM \cite{zhong2014fast}, SVRG-ADMM \cite{zheng2016fast}, and ASVRG-ADMM \cite{liu2017accelerated} for convex problems, as well as ASVRG-ADMM \cite{zeng2024accelerated} for non-convex problems. However, these enhanced methods still relied on access to full gradients for the aim of reducing stochastic gradient variance, posing challenges for large-scale applications.


 \begin{table}[!t]
\centering
\caption{Assumptions imposed by different stochastic algorithms for convergence: \textcircled{1}: Convexity; 
\textcircled{2}: Strong convexity;  
\textcircled{3}: Lipschitz continuity of the gradient; \textcircled{4}: Bounded (stochastic) gradient or the second-moment of the (stochastic) gradient; 
\textcircled{5}: Bounded variance; 
\textcircled{6}: Bounded sequence generated by the algorithm; \textcircled{7}: Unbiased gradient estimation; 
\textcircled{8}: Compact convex feasible region;
\textcircled{9}: Lipschitz continuity of the objective function;
\textcircled{10}: Lipschitz sub-minimization paths;
\textcircled{11}: Bounded dual variables.}\label{tab:comp-assumps}
\renewcommand{\arraystretch}{1.1}\addtolength{\tabcolsep}{2pt}
\begin{tabular}{lccccr}
\hline \hline 
  Algorithms&  Refs.  & Class & Data& Convergence rate $B$ & Assumptions \\ \hline\hline 
  \multicolumn{6}{c}{Type I convergence: $ F(\bw)-F^*= B$}\\\hline  
 
AdaGrad& \cite{duchi2011adaptive} &SGD&IID& $O(1/\sqrt{T})$ &\textcircled{1}\textcircled{4}\textcircled{6} \\ 

 Adam  & \cite{kingma2014adam} &SGD&IID& $O(1/\sqrt{T})$&\textcircled{1}\textcircled{4}\textcircled{6} \\
 
 RMSProp& \cite{mukkamala2017variants} &SGD&IID& $O(1/\sqrt{T})$&\textcircled{1}\textcircled{4}\textcircled{6}\\ 
  
   AMSGrad & \cite{jR2018on} &SGD&IID& $O(1/\sqrt{T})$ &\textcircled{1}\textcircled{4}\textcircled{8}  \\   
   
  AdaBound & \cite{luo2019adaptive} &SGD&IID& $O(1/\sqrt{T})$ &  \textcircled{1}\textcircled{4}\textcircled{8}\\
  
    FedAvg & \cite{Li2020On}  &SGD&IID/Non-IID& $O(1/T)$ &  \textcircled{2}\textcircled{3}\textcircled{4}\textcircled{5} \\
      
     LocalSGD & \cite{stich2018local} &SGD&IID& $O(1/T)$&\textcircled{2}\textcircled{3}\textcircled{4}\textcircled{5} \\
   
      S-ADMM & \cite{ouyang2013stochastic} &SADMM&IID&  $O(\log(T)/T)$ &  \textcircled{2}\textcircled{4}\textcircled{8}\\
      
    %  SAG-ADMM& \cite{zhong2014fast}&SADMM&&  $O(1/T)$ &  \textcircled{1}\textcircled{3}\textcircled{7}\\
      
 %     SVRG-ADMM& \cite{zheng2016fast}&SADMM&IID&  $O(1/T)$ &  \textcircled{1}\textcircled{3}\\
      SPIDER-ADMM &\cite{huang2019faster}&SADMM&IID&  $O(1/T)$ &   \textcircled{3}\textcircled{4}\\
      ASVRG-ADMM& \cite{liu2017accelerated}&SADMM&IID&  $O(1/T^2)$ &  \textcircled{1}\textcircled{3}\textcircled{8}\textcircled{11}\\
 %     ASVRG-ADMM & \cite{zeng2024accelerated}&SADMM&IID&  $O(1/T)$ &   \textcircled{3}\textcircled{10}\\
  PS-ADMM & \cite{ding2019stochastic} &SADMM&IID&  $O(1/T)$ &  \textcircled{2}\textcircled{3}\textcircled{9}\\\hline 
  
  \multicolumn{6}{c}{Type II convergence: $\|\nabla F(\bw)\|^2 = B$}\\\hline 
 Adam & \cite{zou2019sufficient} &SGD&IID& $O(\log(T)/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{7}\\
  Padam & \cite{chen2018closing} &SGD&IID& $O(1/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{7}\\
 PRSGD & \cite{yu2019parallel} &SGD&IID& $O(1/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{5}\textcircled{7}\\
%Lars & \cite{You2020Large} &SGD&IID& $O(1/{T})$ &  \textcircled{3}\textcircled{4}\textcircled{5}\\
Lamb & \cite{You2020Large} &SGD&IID& $O(1/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{5}\\
  Adan & \cite{Adan24} &SGD&IID& $O(1/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{5}\textcircled{7}\\
  PISA & Ours  &SADMM&IID/Non-IID& $O(1/T)$ &  \textcircled{3} \\
\hline \hline 
\end{tabular}
\label{table:compare-conditions}
\end{table}
 
 
\subsection{Contributions}

 
In this work, we propose a data-driven preconditioned inexact SADMM algorithm, termed as PISA. It distinguishes itself from prior approaches and aims to reduce computational costs, relax convergence assumptions, and enhance numerical performance. The key contributions are threefold.

{\textit{$a)$ A general algorithmic structure.}}
The algorithm is based on a preconditioned inexact SADMM, combining simplicity with high generality to handle a wide range of FM-based applications. The framework is inherently compatible with parallel computing, making it ideal for large-scale data settings. Additionally, the preconditioned approach enables the integration of multiple second-moment schemes to upgrade the training performance.

{\textit{$b)$ Strong convergence theory under a sole assumption.}}
%PISA is proven to converge under a single assumption: the Lipschitz continuity of the gradient. Despite using stochastic gradients, it avoids many assumptions typically required by stochastic algorithms, as highlighted in Table \ref{table:compare-conditions}, where all algorithms, except for PISA and FedAvg \cite{Li2020On}, benefited from identically and independently distributed (IID) samples to derive a stochastic gradient so as to obtain an unbiased estimation of the gradient.  However, data in real-world applications are usually heterogeneous (i.e., non-IID) \cite{ye2023heterogeneous},  challenging the convergence of these algorithms for IID data settings. Moreover, it is noted from the table that convexity is necessary for most of the algorithms to establish type I convergence, and PISA achieves the best type II convergence rate under the weakest assumption.  We successfully remove the conditions on the boundedness of the stochastic gradient, the second moment of the stochastic gradient, and the variance. This renders the method particularly well-suited for addressing challenges associated with data heterogeneity.
PISA is proven to converge under a single assumption: the Lipschitz continuity of the gradient. Despite relying on stochastic gradients, it avoids many of the assumptions typically required by stochastic algorithms. As highlighted in Table \ref{table:compare-conditions}, all algorithms, except for PISA and FedAvg \cite{Li2020On}, have drawn identically and independently distributed (IID) samples to derive stochastic gradients for unbiased gradient estimation. However, real-world data are often heterogeneous (i.e., non-IID), a phenomenon commonly referred to as statistical or data heterogeneity \cite{li2020federated, kairouz2021advances, li2022federated,  ye2023heterogeneous}, which poses significant challenges to the convergence of these algorithms for IID datasets. Moreover, the table reveals that most algorithms require convexity to establish type I convergence, whereas PISA achieves the best type II convergence rate under the weakest assumption. More importantly, we eliminate the need for conditions on the boundedness of the stochastic gradient, the second moment of the stochastic gradient, and variance. This makes PISA well-suited for addressing the challenges associated with data heterogeneity, an open problem in federated learning  \cite{kairouz2021advances, ye2023heterogeneous}.
 

{\textit{$c)$ A high numerical performance for various FM-based applications.}} The effectiveness of the proposed method is demonstrated through comparisons with a number of state-of-the-art optimizers across five FM-based tasks:  VMs, LLMs, reinforcement learning models (RLMs), generative adversarial networks (GANs), and recurrent neural networks (RNNs), highlighting its great potential for extensive applications.

%\begin{tabular}{lcccr}
%\hline \hline 
%  Algs.&  Refs.  & Gradient & Convergence rate $B$ & Assumptions \\ \hline\hline 
%  \multicolumn{5}{c}{Type I convergence: $ F(\bw)-F^*\leq B$}\\\hline  
% 
%AdaGrad& \cite{duchi2011adaptive}& Non-Stochastic & $O(1/\sqrt{T})$ &\textcircled{1}\textcircled{4}\textcircled{6} \\ 
%
% Adam  & \cite{kingma2014adam}& Non-Stochastic & $O(1/\sqrt{T})$&\textcircled{1}\textcircled{4}\textcircled{6} \\
% 
% RMSProp& \cite{mukkamala2017variants}& Non-Stochastic & $O(1/\sqrt{T})$&\textcircled{1}\textcircled{4}\textcircled{6} \\ 
%  
%   AMSGrad & \cite{jR2018on}& Non-Stochastic & $O(1/\sqrt{T})$ &\textcircled{1}\textcircled{4}\textcircled{8}  \\   
%
%    FedAvg & \cite{Li2020On} &  Stochastic & $O(1/T)$ &  \textcircled{2}\textcircled{3}\textcircled{4}\textcircled{5} \\
%      
%     LocalSGD & \cite{stich2018local}& Stochastic & $O(1/T)$&\textcircled{2}\textcircled{3}\textcircled{4}\textcircled{5} \\\hline 
%  
%\multicolumn{5}{c}{Type II convergence: $\|\nabla F(\bw)\|^2\leq B$}\\\hline 
% Adam & \cite{zou2019sufficient}& Stochastic & $O(\log(T)/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{7}\\
%  Padam & \cite{chen2018closing}& Stochastic & $O(1/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{7}\\
% PRSGD & \cite{yu2019parallel}& Stochastic & $O(1/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{5}\\
%LAMB & \cite{You2020Large}& Stochastic & $O(1/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{5}\\
%  Adan & \cite{Adan24}& Stochastic & $O(1/\sqrt{T})$ &  \textcircled{3}\textcircled{4}\textcircled{5}\textcircled{7}\\
%  PISA & Ours & Stochastic & $O(1/T)$ &  \textcircled{3} \\
%\hline \hline 
%\end{tabular}
%\label{table:compare-conditions}
%\end{table}
\subsection{Organization and Notation}

The paper is organized as follows: In the next section, we introduce the main model and develop the proposed algorithm, PISA. Section \ref{sec:convergence} provides rigorous proofs of the convergence. Section \ref{sec:SISA} specifies the pre-condition by the second moment to derive a variation of PISA, termed as SISA (second-moment-based inexact SADMM). Extensive experimental results validating the effectiveness of SISA are provided in Section \ref{sec:numerical}. Concluding remarks are discussed in the last section.

Throughout the paper, scalars are represented using plain letters, vectors are denoted with bold letters, and matrices are indicated with bold capital letters.  Let $[m]:=\{1,2,\ldots,m\}$, where `$:=$' means `define'. The cardinality of a set $\D$ is written as $|\D|$. 
For two vectors $\bw$ and $\bv$, their inner product is denoted  by $\langle\bw,\bv\rangle:=\sum_iw_iv_i$.  Let ${\|\cdot\|}$ be the Euclidean norm for vectors, namely $\|\bw\|=\sqrt{\langle\bw,\bw\rangle}$, and the Spectral norm for matrices. A ball with a positive radius $\sqrt{r}$ is written as ${\N(r):=\{\bw:\|\bw\|^2<r\}}$.
A symmetric positive semi-definite matrix $\Q$ is written as $\Q\succeq 0$. Then $\mathbf{P}\succeq \Q$ means that $ \mathbf{P}- \Q\succeq 0$. Denote the identity matrix by $\I$ and let $\textbf{1}$ be the vector with all entries being $1$.   We write 
\begin{eqnarray*} 
&&\P~=(\bpi_1,\bpi_2,\ldots,\bpi_m),\qquad \W~=(\bw_1,\bw_2,\ldots,\bw_m),\\
&&\M=(\bm_1,\bm_2,\ldots,\bm_m),\qquad \bsi=(\sigma_1,\sigma_2,\ldots,\sigma_m).
\end{eqnarray*} 
Similar rules are also employed for the definitions of $\P^\ell, \W^\ell,\M^\ell$, and $\bsi^\ell$.
\section{Preconditioned Inexact SADMM}
We begin this section by introducing the mathematical optimization model for general distributed learning. Then we go through the development of the algorithm. 

\subsection{Model description}
Suppose we are given a set of data as $\D:=\{\bx_t:t=1,2,\ldots,|\D|\}$, where $\bx_t$ is the $t${th} sample. Let $f(\bw; \bx_t)$ be a function (such as neural networks) parameterized by $\bw$ and sampled by $\bx_t$. The total loss function on $\D$ is defined by $ \sum_{\bx_t\in\D} f\left(\bw; \bx_t\right)/{|\D|}$. 
%\begin{eqnarray*} 
% \frac{1}{|\D|} \sum_{\bx_t\in\D} f_t\left(\bw; \bx_t\right),
%\end{eqnarray*}
%where $f_t(\bw; \bx_t)$ is a function (such as neural networks) parameterized by $\bw$ and sampled by $\bx_t$. 
We then divide data $\D$ into $m$ disjoint batches, namely,  $\D= \D_1\cup\D_2\cup\ldots\cup\D_m$ and $\D_i\cap\D_{i'}=\emptyset$ for any two distinct $i$ and $i'$.   Denote
\begin{eqnarray}  \label{def-Fbn}
H_{i}(\bw;\D_i):=  \frac{1 }{|\D_{i}|}  \sum_{\bx_t\in\D_{i}} f\left(\bw; \bx_t\right) \qquad\text{and}\qquad\alpha_i:=\frac{|\D_{i}|}{|\D|}.
\end{eqnarray} 
Clearly, $\sum_{i=1}^m \alpha_i=1$. Now, we can rewrite the total loss as follows,
\begin{eqnarray*}  
\frac{1}{|\D|} \sum_{\bx_t\in\D} f\left(\bw; \bx_t\right)
 = \frac{1}{|\D|} \sum_{i=1}^m \sum_{\bx_t\in\D_{i}} f\left(\bw; \bx_t\right)
 = \sum_{i=1}^m \alpha_i H_{i}(\bw;\D_i).
\end{eqnarray*}
The task is  to learn an optimal parameter to minimize the following regularized loss function,   
\begin{eqnarray}  \label{opt-prob}
 \min\limits_{\bw}~  \sum_{i=1}^m \alpha_i H_{i}(\bw;\D_i) + \frac{\mu}{2}\|\bw\|^2, 
\end{eqnarray}
where $\mu\geq0$ is a penalty constant and $\|\bw\|^2$ is a  regularization. 
\subsection{Main model}
Throughout the paper, we focus on the following  equivalent model of problem (\ref{opt-prob}),
\begin{eqnarray}  \label{opt-prob-distribute}
\begin{aligned}
F^* := \min\limits_{\bw,\W}~\sum_{i=1}^m \alpha_i   F_{i}(\bw_i) + \frac{\lambda}{2}\|\bw\|^2,~~~
{\rm s.t. }~\bw_{i} = \bw,~ i\in[m],
\end{aligned}
\end{eqnarray}
where $ \lambda\in[0,\mu]$ and
\begin{eqnarray}  \label{def-F_i-F}
\begin{aligned}
 F_{i}(\bw)&:= F_{i}(\bw;\D_i):= H_{i}(\bw;\D_i) +\frac{\mu-\lambda}{2}\|\bw\|^2, \\
F (\bw)&:=\sum_{i=1}^m \alpha_i  F_{i}(\bw), \qquad F_\lambda(\bw):=F (\bw)+ \frac{\lambda}{2}\|\bw\|^2.
\end{aligned}
\end{eqnarray}
In problem \eqref{opt-prob-distribute}, $m$ auxiliary variables $\bw_i$ are introduced in addition to the global parameter $\bw$. We emphasize that problems (\ref{opt-prob}) and (\ref{opt-prob-distribute}) are equivalent in terms of their optimal solutions but are expressed in different forms when ${\lambda\in[0,\mu)}$, and they are identical when ${\lambda=\mu}$. Throughout this work, we assume that optimal function value $F^*$ is bounded from below. It is worth noting that any stationary point $\bw$ of problem (\ref{opt-prob-distribute})  satisfies
\begin{eqnarray}\label{stationary-point}
\begin{aligned}
0\in  \nabla F_\lambda(\bw)= \sum_{i=1}^m \alpha_i   \nabla F_{i}(\bw)    + \lambda \bw,
\end{aligned}
\end{eqnarray}
where $ \nabla F_{i}(\bw)$ is the sub-differential \cite[Definition 8.3]{rockafellar2009variational} of $F_{i}(\bw)$. In our convergence analysis, we assume that $F_{i}$ is continuously differentiable, so sub-differential $ \nabla F_{i}(\bw)$ reduces to the gradient of $F_{i}(\bw)$. In this case,  inclusion `$\in$' reduces to `$=$'. Moreover, if each $F_{i}$ is convex (which is unnecessary in this paper), then the stationary points coincide with the optimal solutions to (\ref{opt-prob-distribute}). Since problems (\ref{opt-prob-distribute}) and (\ref{opt-prob}) are equivalent, their stationary points are also identical.

 
\subsection{The algorithmic design}
When employing ADMM to solve problem (\ref{opt-prob-distribute}), we need its associated augmented Lagrange function, which is defined as follows,
\begin{eqnarray}  \label{opt-prob-distribute-Lag}
\begin{aligned}
\L\left(\bw,\W,\P;\bsi\right)
&:= \sum_{i=1}^m \alpha_i L_{i}(\bw,\bw_i, \bpi_i;\sigma_i) + \frac{\lambda}{2}\|\bw\|^2, \\
L_{i}(\bw,\bw_i, \bpi_i;\sigma_i)&:=F_{i}(\bw_{i})+\langle \bpi_{i}, \bw_{i}- \bw\rangle + \frac{\sigma_i}{2} \| \bw_{i}- \bw\|^2,
\end{aligned}
\end{eqnarray}
where $\sigma_i>0$ and  $\bpi_{i}, i\in[m]$ are the Lagrange multipliers.  Based on the above augmented Lagrange function, the conventional ADMM updates each variable in $(\bw,\W,\P)$ iteratively. However, we modified the framework using a proximal term as follows. 
Given initial point $(\bw^0,\W^0,\P^0;\bsi^0)$, the proximal ADMM performs the following steps iteratively for $\ell=0,1,2,\ldots,$ 
\begin{subequations}\label{frame-ADMM}	
			\begin{alignat}{4}
			 \label{w-update}	
			\bw^{\ell+1} &= {\rm arg}\min_{\bw} \L\left(\bw,\W^\ell,\P^\ell;\bsi^{\ell} \right),\\[1ex]
 		%	\text{For each $i\in[m]$,}\\[1ex]
			\label{wb-update}	
			\bw_{i}^{\ell+1} &= {\rm arg}\min_{\bw_{i}}  L_i\left(\bw^{\ell+1},\bw_i, \bpi_i^{\ell}; \sigma_i^{\ell+1} \right)+\frac{\rho_i}{2}\left\langle\bw_i-\bw^{\ell+1}, \Q_i^{\ell+1}\left(\bw_i-\bw^{\ell+1}\right)\right\rangle,\\[1ex]
			 \label{pib-update}	
			\bpi_{i}^{\ell+1} &= \bpi_{i}^{\ell}  + \sigma_i^{\ell+1} ( \bw_{i}^{\ell+1}- \bw^{\ell+1}),
			 \end{alignat}
		\end{subequations}
 	for each $i\in[m]$, where ${\rho_i>0}$ and  both scalar $\sigma_i^{\ell+1}$ and matrix $\Q_i^{\ell+1}\succeq 0$ will be updated properly. The last term in the right-hand side in \eqref{wb-update} is known as the proximal term, while involved matrix $\Q_i^{\ell+1}$ is usually called (adaptively) pre-conditioning matrix in preconditioned gradient methods \cite{gupta2018shampoo,agarwal2019efficient,yong2023general}. 
    %Sub-problem (\ref{w-update}) can be solved by
 %\begin{eqnarray}\label{sub-w}
%\begin{aligned}\bw^{\ell+1}  %=   {\rm arg}\min\limits_{\bw} ~\sum_{i=1}^m \alpha_i \Big[   -\langle \bpi_{i}^\ell,  \bw\rangle + \frac{\sigma_i^{\ell}}{2} \|\bw_{i}^\ell- \bw\|^2 \Big]+ \frac{\lambda}{2}\|\bw\|^2 
%=\frac{\sum_{i=1}^m\alpha_i \left(\sigma_i^{\ell}\bw_{i}^{\ell}+\bpi_{i}^{\ell}  \right) }{ \sum_{i=1}^m\alpha_i\sigma_i^{\ell}+\lambda}.
%\end{aligned}\end{eqnarray}
One can check that sub-problem \eqref{w-update} admits a closed-form solution outlined in \eqref{sub-w-mini}. 
 For sub-problem (\ref{wb-update}), to accelerate the computational speed, we solve it inexactly by
  \begin{eqnarray}\label{sub-wbn}
\begin{aligned}
\bw_{i}^{\ell+1} &= {\rm arg}\min\limits_{\bw_{i}} ~ \langle    \bpi_{i}^{\ell}, \bw_{i} -\bw^{\ell+1}\rangle + \frac{ \sigma_i^{\ell+1} }{2} \|\bw_{i} -\bw^{\ell+1}\|^2 \\ 
&+  F_{i}( \bw^{\ell+1}) + \langle    \nabla F_{i}( \bw^{\ell+1};\B_i^{\ell+1})  , \bw_{i} - \bw^{\ell+1}\rangle   +\frac{\rho_i}{2}\left\langle\bw_i-\bw^{\ell+1}, \Q_i^{\ell+1}\left(\bw_i-\bw^{\ell+1}\right)\right\rangle  \\ 
%&=\Big(\sigma \I + \rho_i \Q_i^{\ell+1} \Big)^{-1}  \Big(\sigma \bw^{\ell+1} + \rho_i \Q_i^{\ell+1} \bw_i^{\ell}  -   \bpi_{i}^{\ell} -  \nabla F_{i}( \bw^{\ell+1})  \Big)\\
&=\bw^{\ell+1} - \Big(\sigma_i^{\ell+1}  \I + \rho_i \Q_i^{\ell+1} \Big)^{-1}  \Big(  \bpi_{i}^{\ell} +  \nabla F_{i}( \bw^{\ell+1};\B_i^{\ell+1})  \Big).
\end{aligned}
\end{eqnarray}
\begin{algorithm}[!t]
    \SetAlgoLined

Divide  $\D$ into $m$ disjoint  batches {$\{\D_1,\D_2,\ldots,\D_m\}$} and calculate $\alpha_i$ by (\ref{def-Fbn}). 

{{Initialize $\bw^0=\bw_i^0=\bpi_i^0=0$,}}  $\gamma_i\in[3/4,1), \sigma_i^0>0, \eta_i>0$  and $\rho_i>0$  for each $i\in[m]$.  

\For{$\ell=0,1,2,\ldots$}{
 \begin{eqnarray}  \label{sub-w-mini}
\bw^{\ell+1}  =  \frac{\sum_{i=1}^m\alpha_{i} \left(\sigma_i^\ell\bw_{i}^{\ell}+\bpi_{i}^{\ell}  \right) }{\sum_{i=1}^m\alpha_{i}  \sigma_i^\ell+\lambda}.
 \end{eqnarray}   
\For{$i=1,2,\ldots,m$}{  
%\text{Local update for sub-batch $\D_{i}$:}
 \begin{align}
 \label{sub-B-g}  &\text{Randomly draw a mini-batch $ {\B}_{i}^{\ell+1}\subseteq\D_{i}$ and calculate $\bg_i^{\ell+1}= \nabla F_{i}( \bw^{\ell+1}; {\B}_{i}^{\ell+1})$},\\[1ex]
 \label{sub-Q-eta}  &\text{Choose $\Q_i^{\ell+1} $ to satisfy $\eta_i\I\succeq \Q_i^{\ell+1} \succeq0$,}\\[1ex]
% &\bm_{i}^{\ell+1}= \min\left\{ \beta_{i}\bm_{i}^{\ell} + (1-\beta_{i})  \textcolor{blue}{\left(\bpi_{i}^{\ell}+\bg_i^{\ell+1}\right)} \odot  \textcolor{blue}{\left(\bpi_{i}^{\ell}+\bg_i^{\ell+1}\right)} , \eta_{i}^2  \textbf{1} \right\},\\
 \label{sub-sigma-mini} &\sigma_i^{\ell+1}  = \sigma_i^{\ell}/\gamma_i, \\[1ex]
 \label{sub-wbn-mini} &\bw_{i}^{\ell+1}  = \bw^{\ell+1} - \Big(\sigma_i^{\ell+1} \I + \rho_i \Q_i^{\ell+1} \Big)^{-1}  \Big(  \bpi_{i}^{\ell} + \bg_i^{\ell+1} \Big), \\[1ex]
\label{sub-pin} &\bpi_{i}^{\ell+1} =  \bpi_{i}^{\ell} +  \sigma_i^{\ell+1}(\bw_{i}^{\ell+1}- \bw^{\ell+1}). 
 \end{align} 
}
} 
\caption{\textbf{P}reconditioned \textbf{I}nexact \textbf{S}tochastic \textbf{A}DMM (PISA).  }\label{algorithm-ADMM-mini}
\end{algorithm}
This update admits three advantages. First, it solves problem (\ref{wb-update}) by a closed-form solution, namely, the second equation in \eqref{sub-wbn}, reducing the computational complexity.  Second,  we approximate $ F_{i}(\bw) $ using its second-order approximation at $\bw^{\ell+1}$ rather than $\bw_{i}^{\ell}$, which facilitates each batch parameter $\bw_{i}^{\ell+1}$ to tend to $\bw^{\ell+1}$ quickly, thereby accelerating the overall convergence. Finally, $\nabla F_{i}( \bw^{\ell+1};\B_i^{\ell+1})$ serves as a stochastic approximation of true gradient $\nabla F_{i}( \bw^{\ell+1})=\nabla F_{i}( \bw^{\ell+1};\D_i)$, as defined by \eqref{def-F_i-F}, where $\B_i^{\ell+1}$ is a mini-batch dataset randomly selected from data $\D_i$. By using mini-batch datasets $\{\B_1^{\ell+1},\ldots,\B_m^{\ell+1}\}$ in every iteration,  rather than full data $\D=\{\D_1,\ldots,\D_m\}$, the computational cost is significantly reduced. Overall, based on these observations, we name our algorithm PISA, which stands for Preconditioned Inexact SADMM, as described in Algorithm \ref{algorithm-ADMM-mini}.

Another advantageous property of PISA  is its ability to perform parallel computation, which stems from the parallelism used in solving sub-problems in ADMM. At each iteration,  $m$ nodes (i.e., $i=1,2,\cdots,m$) update their parameters by \eqref{sub-B-g}-\eqref{sub-pin} in parallel, thereby enabling the processing of large-scale datasets. Moreover, when specifying the preconditioning matrix, $\Q_i^{\ell+1}$, as a diagonal matrix (as outlined in Section \ref{sec:SISA}) and sampling ${\B}_{i}^{\ell+1}$ with small batch sizes, each node exhibits significantly low-computational complexity, facilitating fast computation.





\section{Convergence of PISA}\label{sec:convergence}
In this subsection, we aim to establish the convergence property of Algorithm \ref{algorithm-ADMM-mini}. To proceed with that, we first define some constants. Given $\gamma_i\in[3/4,1)$, we set $m$ positive constants $\tau_i$ as
 \begin{eqnarray} \label{def-tau}
\begin{aligned}
\tau_i\geq \log_{\gamma_i}(1-\gamma_i)-1,~~~i\in[m].
\end{aligned}
\end{eqnarray} 
For instance, when $\gamma_i=3/4$, one can take $\tau_i=4$. Moreover, we define 
 \begin{eqnarray} \label{def-eps}
\begin{aligned}
\triangle F &:= F(\bw^0)-F^*,\qquad \sigma^0:=\min \{\sigma_1^0,\sigma_2^0,\cdots,\sigma_m^0\},\qquad \delta   := \max_{i\in[m]} \frac{8(\triangle F+2)}{\sigma^0 \alpha_i\gamma_i^{\tau_i}},\\[1ex]
%\delta  &:= \max_{i\in[m]} \frac{8(\triangle F+2)}{\sigma^0 \alpha_i\gamma_i^{\tau_i}},\qquad\overline{\delta}:=10\delta + \frac{1}{\sigma^0},\\[1ex]
\varepsilon_i(r)&:= \sup_{\B_i,\B'_i\subseteq\D_i}\sup_{\bw\in\N(r)} 64\left\|\nabla F_i(\bw;\B_i
)- \nabla F_i(\bw;\B'_i)\right\|^2,~~\forall~i\in[m].
\end{aligned}
\end{eqnarray}
\noindent One can observe that ${\varepsilon_i(r)=0}$ for any ${r>0}$ if we take the full batch data in each step, namely, choosing ${\B_i^{\ell}=(\B_i^{\ell})'=\D_i}$ for every ${i\in[m]}$ and all ${\ell\geq1}$. However for  min-batch dataset ${\B_i^{\ell}\subset\D_i}$, this parameter is related to the bound of variance $\E_{\B_i}\left\|\nabla F_i(\bw;\B_i
)- \nabla F_i(\bw;\D_i)\right\|^2$, which is commonly assumed to be bounded \cite{stich2018local,yu2019parallel,Li2020On,CooperativeSGD21,Adan24}. However, in our analysis, we can verify that both generated sequences $\{\bw^\ell\}$ and $\{\bw_i^\ell\}$ fall into a bounded region $\N(\delta)$ for any $i\in[m]$, thereby leading to a finitely bounded $\varepsilon_i(\delta)$ naturally, as stated below.
\begin{lemma}\label{bound-varepsilon} $\varepsilon_i(r)<\infty$ for any given $r\in(0,\infty)$ and any $i\in[m]$. 
\end{lemma}
\begin{proof} As $F_i$ is continuously differentiable, $\nabla F_i$ is continuous. We note that $\N(r)$ is bounded due to $r\in(0,\infty)$, thereby gap $\|\nabla F_i(\bw;\B_i
)- \nabla F_i(\bw;\B_i')\|^2$ is bounded for fixed $\B_i$ and $\B'_i$. Then there are finitely many $\B_i$ and $\B'_i$ in $\D_i$, indicating $\varepsilon_i(r)$ is bounded.
\end{proof}
As a result of the above lemma, we no longer need to assume the boundedness of the variance, $\E_{\B_i}\left\|\nabla F_i(\bw;\B_i) - \nabla F_i(\bw;\D_i)\right\|^2$. This assumption is known to be somewhat restrictive, particularly for non-IID or heterogeneous datasets.  Therefore, the theorems we establish in the sequel effectively address this critical challenge  \cite{kairouz2021advances,  ye2023heterogeneous}. In other words, our algorithm demonstrates robust performance in settings with heterogeneous data.

%\begin{setup}\label{setup} For each batch $i\in[m]$, choose $\Q_i^{\ell}$ to satisfy  $\eta_i \I \succeq \Q_i^{\ell} \succeq 0 $ for every $\ell=1,2,\ldots$, where $\eta_i>0$ is a given constant. Moreover, let $\sigma$ satisfy
%\end{setup} 

\subsection{Global convergence}
To establish convergence, we need the assumption of the Lipschitz continuity of the gradient, which is a common assumption in convergence analysis for various optimization algorithms. We point out that gradient Lipschitz continuity is equivalent to the L-smoothness for convex functions.
\begin{assumption}\label{assumption}  Function $f(\cdot;\bx_t)$  has a Lipschitz continuous gradient $\nabla f(\cdot;\bx_t)$ with a constant $c(\bx_t)>0$ for each $t\in[|\D|]$. Denote $c_i:=\max_{\bx_t\in\D_i}c(\bx_t)$ and $ r_i:=c_i+\mu-\lambda$ for each $i\in[m]$. 
\end{assumption}
Moreover,  we  set $\bsi^0$ by
{{\begin{eqnarray}  \label{choice-of-sigma}
\sigma^0~\geq~ \max_{i\in[m]}\left\{8\rho_i\eta_i,~ 8r_i,~ \frac{16\gamma_i\varepsilon_i\left(\triangle F \right)}{1-\gamma_i},~\frac{80(\triangle F+3)}{\alpha_i\gamma_i^{\tau_i}\triangle F}\right\}.
\end{eqnarray}}}
According to Lemma \ref{bound-varepsilon}, ${\varepsilon_i\left(\triangle F \right)<\infty}$ due to the boundedness of $\triangle F$. Hence, $\sigma^0$ in \eqref{choice-of-sigma} is a well-defined constant. For notational simplicity, hereafter, we denote
\begin{eqnarray}\label{def-notation}
\begin{aligned}
  &\triangle\bw^{\ell}:= \bw^{\ell}-\bw^{\ell-1},~~~&&\triangle\bw_{i}^{\ell}:= \bw_{i}^{\ell}-\bw_{i}^{\ell-1}\\
  & \triangle\bpi_{i}^{\ell}:= \bpi_{i}^{\ell}-\bpi_{i}^{\ell-1},&&\triangle\overline{\bw}^{\ell}_i:= \bw^{\ell}_i-\bw^{\ell},\\ 
   & \triangle\bg_i^{\ell}:= \bg_i^{\ell}- \nabla F_i(\bw^{\ell}),&&\L^{\ell}:=\L(\bw^\ell,\W^\ell,\P^\ell;\bsi^\ell).
\end{aligned}\end{eqnarray}
 To make these notation well-defined when ${\ell=0}$, we let $\bw^{-1}=\bw^0,~\bw_{i}^{-1}=\bw_{i}^0,~\bpi_{i}^{-1}=\bpi_{i}^0$, and $\bg_i^{0}=\nabla F_i(\bw^0)$ for any $i\in[m]$. Our first result shows the descent property of a merit function associated with objective function $\L^{\ell}$.  
\begin{lemma}\label{descent-lemma-L} Let $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ be the sequence generated by Algorithm \ref{algorithm-ADMM-mini} with $\bsi^0$  chosen as (\ref{choice-of-sigma}). Then under Assumption \ref{assumption}, it holds that
$\bw^\ell,\bw_i^\ell\in\N(\delta)$ for any $i\in[m]$ and any $\ell\geq0$. Moreover,
\begin{eqnarray} \label{descent-L}
\widetilde{\L}^{\ell}- \widetilde{\L}^{\ell+1} \geq \sum_{i=1}^m  \alpha_i\left[ \frac{\sigma_i^{\ell+1}+4\lambda}{8}\left\|  \triangle\bw^{\ell+1} \right\|^2 +\frac{ \sigma_i^{\ell+1}}{8} \left\|\triangle \bw_{i}^{\ell+1}\right\|^2 \right],  
\end{eqnarray}
for any $\ell\geq0$, where
{{\begin{eqnarray}\label{def-tilde-L}
\begin{aligned}
\widetilde{\L}^{\ell} :=\L^\ell+\sum_{i=1}^m \alpha_i \left[ \frac{\sigma_i^\ell}{2}\left\| \triangle\overline{\bw}^{\ell}_i\right\|^2 + \gamma_i^\ell   \right].
\end{aligned}\end{eqnarray}}}
\end{lemma}
\begin{theorem}\label{main-convergence} Let $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ be the sequence generated by Algorithm \ref{algorithm-ADMM-mini} with  $\bsi^0$  chosen as (\ref{choice-of-sigma}). Then the following statements are valid under Assumption \ref{assumption}.
\begin{itemize}[leftmargin=17pt]
\item[1)] Sequences $\{ {\L}^\ell\}$ and $\{\widetilde{\L}^\ell\}$  converge and for any $i\in[m]$, 
\begin{eqnarray} \label{gap-all}
0=\lim_{\ell\to\infty}\left\|  \triangle\bw^{\ell+1} \right\| =\lim_{\ell\to\infty} \left\|\triangle \bw_{i}^{\ell+1}\right\|= \lim_{\ell\to\infty} \left\| \triangle \overline{\bw}^{\ell+1}_i\right\|=\lim_{\ell\to\infty} \left(\widetilde{\L}^\ell-\L^\ell\right). 
\end{eqnarray}
\item[2)] Sequence $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ is bounded, and any accumulating point $\bw^\infty$ of $\{\bw^\ell\}$ is a stationary point of (\ref{opt-prob}) in expectation. 
\end{itemize}
\end{theorem}
To establish the convergence of entire sequence $\{(\bw^\ell,\W^\ell,\P^\ell)\}$, one can assume that one of the following three conditions: (c1) $\bw^\infty$ is isolated \cite[Lemma 4.10]{more1983computing}; (c2) $\mu\textbf{I}+\textbf{U}^\infty$ is positive definite for any generalized Hessian \cite[Definition 2.1]{hiriart1984generalized} $\textbf{U}^\infty$ of $\sum_{i=1}^m \alpha_i H_{i}(\bw;\D_i)$; (c3) $H_{i}(\bw;\D_i)$ satisfies the  Kurdyka-Lojasiewicz property \cite{bolte2014proximal,zhouli23}. As the proof is straightforward, we omit the detailed results in this paper.

\subsection{Complexity analysis}
Besides the global convergence established above, the algorithm exhibits the following rate of convergence under the same assumption and  parameter setup.
\begin{lemma}\label{main-convergence-rate} Let $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ be the sequence generated by Algorithm \ref{algorithm-ADMM-mini} with $\bsi^0$ chosen as (\ref{choice-of-sigma}). Then  under Assumption \ref{assumption}, 
 \begin{eqnarray}  
\begin{aligned}\label{convergence-rate}
\min_{\ell=0,1,\ldots,T-1} \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2 
 \leq     \frac{\theta}{ T\min \{\gamma_1^T,\cdots,\gamma_m^T\}},  
\end{aligned}
\end{eqnarray}
where 
$\theta:=  10 ( \triangle F+2 ) \max \{\sigma_1^0,\cdots,\sigma_m^0\}$.
\end{lemma}
%We note that $\gamma_i$ can be any scalar in $[3/4,1)$. This means we can choose $\gamma_i$ sufficiently close to $1$ in Lemma \ref{main-convergence-rate}, for example $\gamma_i=1-\varrho$ where $\varrho$ is sufficiently close to $0$, then $\gamma_i^k=(1-\varrho)^k\approx 1-\varrho k$. This means for a give number $T$ of iterations, by letting $\varrho=(1-a)/T$ with any $a\in(0,1)$, we have  $\gamma_i^T=(1-\varrho)^T \approx (1-\varrho T)= a$. Therefore, under given $T$ and given $\gamma_i=1-(1-a)/T$, the convergence rate in (\ref{convergence-rate}) reduces to  
%\begin{eqnarray*}  
%\begin{aligned}\label{convergence-rate-1}
%\min_{\ell=1,2,\ldots,T} \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2 
% =  O\left( \frac{1}{T}\right).
%\end{aligned}
%\end{eqnarray*}
%The result can be stated as follows.
\begin{theorem}\label{main-convergence-rate-eps} Let $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ be the sequence generated by Algorithm \ref{algorithm-ADMM-mini} with $\bsi^0$  chosen as (\ref{choice-of-sigma}).   For any given tolerance $\epsilon\in (0,1/(2\theta+1)]$,  set $\gamma_i=1-\epsilon^{2}/4$ for each $i\in[m]$. Then under Assumption \ref{assumption},  
 \begin{eqnarray}  \label{rate-epsilon}
\begin{aligned} 
\min_{\ell=0,1,\ldots,T-1} \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2 
 \leq    \epsilon \qquad\text{when}\qquad T= \left\lceil \frac{{2\theta}}{\epsilon } \right\rceil,
\end{aligned}
\end{eqnarray}
where $\lceil a \rceil$ represents the ceiling of $a$.\end{theorem}
The above theorem shows that given a small tolerance $\epsilon$, by setting proper $\gamma_i, i\in[m]$, the complexity of the iteration to guarantee \eqref{rate-epsilon} is about $T=O(1/\epsilon).$ Therefore,
\begin{eqnarray*}  
\begin{aligned}\label{convergence-rate-1}
\min_{\ell=0,1,\ldots,T-1} \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2 
 =  O\left( \frac{1}{T}\right).
\end{aligned}
\end{eqnarray*}
 To achieve such a result, we only assume Assumption \ref{assumption} without imposing any other commonly used assumptions, such as those presented in Table \ref{tab:comp-assumps}. 
\subsection{Convergence of full-batched PISA}
Now we would like to see the convergence property of full-batched PISA, namely,  $\B_i^{\ell}=\D_i$ for every $i\in[m]$ and all $\ell\geq0$. For this setting, we can always choose $\bsi$ and $\bga$ by
\begin{eqnarray}  \label{choice-of-sigma-ell}
\gamma_i=1,~~~\sigma_i^\ell\equiv\sigma_i\geq\max\Big\{8\rho_i\eta_i,~ 8r_i\Big\},~~~\forall~i\in[m],\forall~\ell\geq 0.
\end{eqnarray}
\begin{theorem}\label{main-convergence-full} Let $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ be the sequence generated by full-batched PISA with parameters chosen as (\ref{choice-of-sigma-ell}). Then the following statements are valid under Assumption \ref{assumption}.
\begin{itemize}[leftmargin=17pt]
\item[1)] Sequences $\{ {\L}^\ell\}$ and $\{\widetilde{\L}^\ell\}$  converge and for any $i\in[m]$, 
\begin{eqnarray*} \label{gap-all-full}
0=\lim_{\ell\to\infty}\left\|  \triangle\bw^{\ell+1} \right\| =\lim_{\ell\to\infty} \left\|\triangle \bw_{i}^{\ell+1}\right\|= \lim_{\ell\to\infty} \left\| \triangle \overline{\bw}^{\ell+1}_i\right\|= \lim_{\ell\to\infty} \left\|\triangle \bpi_{i}^{\ell+1}\right\|=\lim_{\ell\to\infty} \left(\widetilde{\L}^\ell-\L^\ell\right). 
\end{eqnarray*}
\item[2)] Sequence $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ is bounded, and any accumulating point $\bw^\infty$ of $\{\bw^\ell\}$ is a stationary point of problem (\ref{opt-prob}). 
\item[3)]  Given a tolerance $\epsilon>0$,
\begin{eqnarray}  
\begin{aligned}\label{convergence-rate-full}
\min_{\ell=0,1,\ldots,T-1} \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2 \leq \epsilon~~~~\text{whenever}~~~~T= \left\lceil\frac{\theta}{\epsilon}\right\rceil.
\end{aligned}
\end{eqnarray}
%where $\theta:=  12  \left(\max_{i\in[m]} \sigma_i\right)\left( \widetilde{\L}^1 -  F^* + \sum_{i=1}^m\frac{\alpha_i\rho_i\eta_i}{3}\left\| \triangle\overline{\bw}^1_i\right\|^2 \right)$.
\end{itemize}
\end{theorem}

%\subsection{Some extensions}
%One can equivalently rewrite \eqref{opt-prob-distribute} as
%\begin{eqnarray}  \label{opt-prob-distribute-neq}
%\begin{aligned}
%\min\limits_{\bw,\W}~\sum_{i=1}^m \alpha_i    \left[F_{i}(\bw_{i}) + \frac{\omega\lambda}{2}\|\bw_i\|^2\right] +\frac{(1-\omega)\lambda}{2}\|\bw\|^2,~~~
%{\rm s.t. }~~\bw = \bw_{i},~ i\in[m].
%\end{aligned}
%\end{eqnarray}
%where $\omega\in[0,1]$. This model reduces to  \eqref{opt-prob-distribute} when $\omega=0$. 
\section{SISA: Second Moment-based inexact SADMM }\label{sec:SISA}
We note that the second moment to determine an adaptive learning rate enables the improvements of the learning performance of several popular algorithms, such as RMSProp \cite{tieleman2012lecture} and   
Adam  \cite{kingma2014adam}. Motivated by this, we specify preconditioned matrix $\Q_i^{\ell+1}$ by using the second moment as follows,
\begin{equation}
 \label{fact-4}
\begin{aligned} 
\Q_i^{\ell+1} = {\rm Diag}\left(\sqrt{\bm_i^{\ell+1}}\right),
\end{aligned}
\end{equation} 
where ${\rm Diag}(\bm)$ is the diagonal matrix with the diagonal entries formed by $\bm$ and $\bm_i^{\ell+1}$ can be chosen flexibly as long as it satisfies that $\|\bm_i^{\ell+1}\|_\infty\leq \eta_i^2$. Here, $\|\bm\|_\infty$ is the infinity norm of $\bm$. We can set $\bm_i^{\ell+1}$ as follows $$\bm_i^{\ell+1} = \min\left\{  \widetilde{\bm}_i^{\ell+1}, ~ \eta_i^2  \textbf{1} \right\},$$
where $\widetilde{\bm}_i^{\ell+1}$ can be updated by
\begin{equation}
\label{second_moment_update}
\begin{aligned}
&\text{Scheme I:}\qquad&&\widetilde{\bm}_i^{\ell+1}=  \widetilde{\bm}_i^{\ell} + \left( \bpi_{i}^{\ell} +   \bg_i^{\ell+1}   \right) \odot \left(  \bpi_{i}^{\ell} +   \bg_i^{\ell+1}  \right),\\
&\text{Scheme II:}\qquad&&\widetilde{\bm}_i^{\ell+1}=  \beta_i \widetilde{\bm}_i^{\ell} +  \beta_i  \left( \bpi_{i}^{\ell} +   \bg_i^{\ell+1}   \Big) \odot \Big(  \bpi_{i}^{\ell} +   \bg_i^{\ell+1}  \right),\\
&\text{Scheme III:}\qquad &&\mathbf{n}_i^{\ell+1}=  \beta_i \mathbf{n}_i^{\ell} +  \beta_i  \left( \bpi_{i}^{\ell} +   \bg_i^{\ell+1}   \Big) \odot \Big(  \bpi_{i}^{\ell} +   \bg_i^{\ell+1}  \right),\\
&&&\widetilde{\bm}_i^{\ell+1}=\mathbf{n}_i^{\ell+1}/(1- \beta_i^{\ell+1}),
\end{aligned}
\end{equation}
where  $\widetilde{\bm}_i^{0}$ and $\mathbf{n}_i^{0}$ are given,  $\beta_i\in(0,1)$,  and $\beta_i^\ell$ stands for power $\ell$ of $\beta_i$.
These three schemes resemble the ones used by  AdaGrad   \cite{duchi2011adaptive}, RMSProp   \cite{tieleman2012lecture}, and   
Adam  \cite{kingma2014adam}, respectively.    Employing \eqref{fact-4} into  Algorithm \ref{algorithm-ADMM-mini} gives rise to Algorithm \ref{algorithm-ADMM-SM}. We term it SISA, an abbreviation for the second moment-based inexact stochastic ADMM.  
 

\begin{algorithm}[!t]
    \SetAlgoLined
  
Divide  $\D$ into $m$ disjoint  batches {$\{\D_1,\D_2,\ldots,\D_m\}$} and calculate $\alpha_i$ by (\ref{def-Fbn}). 

Initialize $\bw^0=\bw_i^0=\bpi_i^0=0$,  $\gamma_i\in[3/4,1), \sigma_i^0>0, \eta_i>0$  and $\rho_i>0$  for each $i\in[m]$.  

%Divide  $\D$ into $m$ disjoint  batches {$\{\D_1,\D_2,\ldots,\D_m\}$}. Initialize {$(\bw^0,\W^0,\P^0)$}. Calculate $\alpha_i$ by (\ref{def-Fbn}) and initialize  $\sigma_i^0>0, \gamma_i\in[1/2,1), \eta_i>0$  and $\rho_i>0$  for each $i\in[m]$.  

\For{$\ell=0,1,2,\ldots$}{
 \begin{eqnarray*}  
\bw^{\ell+1}  =  \frac{\sum_{i=1}^m\alpha_{i} \left(\sigma_i^\ell\bw_{i}^{\ell}+\bpi_{i}^{\ell}  \right) }{\sum_{i=1}^m\alpha_{i}  \sigma_i^\ell+\lambda}.
 \end{eqnarray*}     
\For{$i=1,2,\ldots,m$}{  
%\text{Local update for sub-batch $\D_{i}$:}
 \begin{align}
 &\text{Randomly draw a mini-batch ${\B}_{i}^{\ell+1}\subseteq\D_{i}$ and calculate $\bg_i^{\ell+1}= \nabla F_{i}( \bw^{\ell+1}; {\B}_{i}^{\ell+1})$},~~~~~\nonumber\\[1ex] 
  &\text{Choose $\bm_i^{\ell+1}$ to satisfy $\|\bm_i^{\ell+1}\|_\infty\leq \eta_i^2$},~~~~~\nonumber\\[1ex] 
  &  \sigma_i^{\ell+1}  = \sigma_i^{\ell}/\gamma_i, \nonumber\\[1ex]
%&& \bm_i^{\ell+1}= \min\left\{ \beta_i\bm_i^{\ell} + (1-\beta_i) \Big( \bpi_{i}^{\ell} +  \bg_i^{\ell+1} \Big) \odot \Big(  \bpi_{i}^{\ell} +  \bg_i^{\ell+1}  \Big), \eta_i^2  \textbf{1} \right\},\\
 \label{wi-update-PISA-sm}
 & \bw_{i}^{\ell+1} = \bw^{\ell+1} -   \frac{1}{\sigma_i^{\ell+1}  + \rho_i \sqrt{\bm_i^{\ell+1}}  } \odot \Big(  \bpi_{i}^{\ell} +  \bg_i^{\ell+1} \Big), \\ 
&\bpi_i^{\ell+1} =  \bpi_{i}^{\ell} +\sigma_i^{\ell+1}(\bw_{i}^{\ell+1}- \bw^{\ell+1}).\nonumber
\end{align}
 } 
} 
\caption{\textbf{S}econd moment-based \textbf{I}nexact \textbf{S}tochastic \textbf{A}DMM (SISA). }\label{algorithm-ADMM-SM}
\end{algorithm} 

\subsection{Properties of SISA}
Compared to PISA in Algorithm \ref{algorithm-ADMM-mini}, SISA admits three advantages. 
\begin{itemize}[leftmargin=17pt]
\item[i)]
It is capable of incorporating various schemes of the second moment, which may enhance the numerical performance of SISA significantly. 
\item[ii)] One can easily check that $\eta_i \I \succeq \Q_i^{\ell+1} \succeq 0 $ for each batch $i\in[m]$ and all $\ell\geq1$. Therefore,  \eqref{fact-4} enables us to preserve the convergence property as follows.
\begin{theorem}\label{main-convergence-sm} Let $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ be the sequence generated by Algorithm \ref{algorithm-ADMM-SM} with $\bsi^0$ chosen as (\ref{choice-of-sigma}). Then under Assumption \ref{assumption}, all statements in Theorems \ref{main-convergence} and \ref{main-convergence-rate-eps} are valid. For the full-batched case with parameters chosen as (\ref{choice-of-sigma-ell}),  all statements in  Theorems \ref{main-convergence-full} are valid under Assumption \ref{assumption}.  
\end{theorem}
\item[iii)] Such a choice of $\Q_i^{\ell+1}$ enables the fast computation compared to  update $\bw_i^{\ell+1}$ by \eqref{sub-wbn}. This is because the complexity of computing \eqref{wi-update-PISA-sm} is $O(p)$, where $p$ is the dimension of $\bw_i^{\ell+1}$, whereas the complexity of computing \eqref{sub-wbn-mini} is about $O(p^3)$.
\end{itemize}


\subsection{A practical implementation of SISA}
A practical and effective random selection of sub-batch $\B_i$  from batch $\D_i$ is the periodic selection. To be more specific,  we first divide each batch data $\D_i$ into $n$ mini-batches as $\D_i= \D_{i1}\cup\D_{i2}\cup\ldots\cup\D_{in}$ and $\D_{ij}\cap\D_{ij'}=\emptyset$ for any two distinct $j$ and $j'$. Then at the $\ell$th iteration, we select 
\begin{equation}\label{Sampling-B}
\B_i^{\ell+1}=\D_{i,1+ (\ell~ {\rm mod}~ n)},
\end{equation}
 where $\ell~ {\rm mod}~ n$ returns the remainder after dividing $\ell$ by $n$. The specific implementation of SISA in Algorithm \ref{algorithm-ADMM-SM} can be found in Figure \ref{epoch-SISA}, where an epoch is a stage using all data once.
 
 \vspace{3mm}
\begin{figure}[!th]
\centering
%\includegraphics[scale=.425]{mpiam.png}
\includegraphics[scale=.425]{piam1.png}
\caption{From iteration $(\ell+1)$ to $(\ell+n)$ (i.e., 1 epoch) of SISA using the periodic selection.
\label{epoch-SISA}}
\end{figure}

% 
% 
%\begin{algorithm}[!t]
%    \SetAlgoLined 
%Divide  $\D$ into $m$ disjoint  batches {$\{\D_1,\D_2,\ldots,\D_m\}$}  and calculate $\alpha_i$ by (\ref{def-Fbn}).
%
%Divide $\D_i$ into $n$ disjoint mini-batches $\{\D_{i1},\D_{i2},\ldots,\D_{in}\}$ for each $i\in[m]$.
%
%Initialize $\bw^0, \bw_i^0=\bw^0,\bpi_i^0=0$,  $\gamma_i\in[3/4,1), \sigma_i^0>0, \eta_i>0$  and $\rho_i>0$  for each $i\in[m]$.   
% 
%\For{$\ell=0,1,2,\ldots$}{ 
% \begin{eqnarray*}  
%\bw^{\ell+1}  =  \frac{\sum_{i=1}^m\alpha_{i} \left(\sigma_i^\ell\bw_{i}^{\ell}+\bpi_{i}^{\ell}  \right) }{\sum_{i=1}^m\alpha_{i}  \sigma_i^\ell+\lambda}.
% \end{eqnarray*}   
%\For{$i=1,2,\ldots,m$}{  
%%\text{Local update for sub-batch $\D_{i}$:}
% \begin{eqnarray*}
% \begin{aligned}
%& \bg_i^{\ell+1} ~=  \nabla F_{i}( \bw^{\ell+1}; \D_{i,1+ (\ell~ {\rm mod}~ n)}) ,\\[1ex]
%&\text{Choose $\bm_i^{\ell+1}$ to satisfy $\|\bm_i^{\ell+1}\|_\infty\leq \eta_i^2$},\\[1ex] 
% &  \sigma_i^{\ell+1}  = \sigma_i^{\ell}/\gamma_i, \\[1ex] 
% & \bw_{i}^{\ell+1} = \bw^{\ell+1} -   \frac{1}{\sigma_i^{\ell+1}  + \rho_i \sqrt{\bm_i^{\ell+1}}  } \odot \Big(  \bpi_{i}^{\ell} +  \bg_i^{\ell+1} \Big), \\ 
%&\bpi_i^{\ell+1} =  \bpi_{i}^{\ell} +\sigma_i^{\ell+1}(\bw_{i}^{\ell+1}- \bw^{\ell+1}).
% \end{aligned}
%\end{eqnarray*} 
%}
%} 
%\caption{SISA with the periodic selection of mini-batches.  }\label{algorithm-ADMM-mini-spec}
%\end{algorithm} 


%\begin{figure}[!t]
%\centering
%\includegraphics[scale=.4]{piam.png}
%\caption{From $(\ell+1)$th to $(\ell+n)$th iteration (i.e., $n$ epochs) of PISA in Algorithm \ref{algorithm-ADMM-SM}.}
%\end{figure}




 
 \section{Numerical Experiments}
\label{sec:numerical}

%This section presents the evaluation of SISA with \eqref{Sampling-B} to select $\B_i$ in Algorithm \ref{algorithm-ADMM-SM} on various benchmarks.  We primarily compare it with the widely-used DL optimizers Adam and RMSProp by applying them to four different types of FMs: VMs, LLMs, RLMs, and GANs. For the hyperparameters of SISA, we fix $\beta_i = 0.999$ and update the second-moment $\widetilde{\bm}$ using Scheme III in \ref{second_moment_update} for all experiments in the sequel. Other hyperparameters vary across different experiments, with Table \ref{table:hyperparameter} providing detail settings.

This section evaluates the performance of SISA in Algorithm \ref{algorithm-ADMM-SM}. We use \eqref{Sampling-B} to select $\B_i$ and Scheme III in \eqref{second_moment_update} {with $\beta_i=0.999$ and $\eta_i=10^4$} to update the second moment. Other hyperparameters can be found in Table \ref{table:hyperparameter} in Appendix \ref{appedix-A}. We compare SISA with various excellent deep learning optimizers to process five types of FMs: VMs, LLMs, RLMs, GANs, and RNNs.


%\begin{table}[h!]
%\caption{Hyperparameters of SISA in the numerical experiments.}
%\centering
%\renewcommand{\arraystretch}{1}\addtolength{\tabcolsep}{1.5pt}
%% Create the table
%\begin{tabular}{cccccccccc}
%\hline
% & \multicolumn{2}{c}{VMs} && LLMs && RLMs&& \multicolumn{2}{c}{GANs} \\\cline{2-3} \cline{5-5}\cline{7-7}\cline{9-10}
% &ResNet-34&VGG-11&&GPT(335M)&&Humanoid&&WGAN&WGAN-GP\\\hline
%$\sigma_i^0$      &0.1& 6.5&&0.01&&0.1&&0.1& 5e-5\\  
%$\rho_i$      & 5e3& 5e4&& 1e4&& 1e5&& 1e3& 2e2 \\ 
%$\gamma_i$    & 0.97& 0.98&& $1-\frac{1}{E-\lfloor {\ell}/{n}\rfloor}$&& $1-\frac{1}{E-\lfloor {\ell}/{n}\rfloor}$&& 1& 1  \\ 
%$\mu$       & 5e-5& 5e-4&& 0&& 0&& 0 & 0 \\ 
%$\lambda$       & 0& 0&& 0&& 0&& 5e-5& 1e-4  \\  
%$E$       & 200& 200&& 150&& 100&& 100& 100  \\ \hline
%\end{tabular}
%
%\label{table:hyperparameter}
%\end{table}


\begin{figure}[!t]
\centering
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/ImageNet/ViT_imagenet.pdf} 
\end{subfigure}	 
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/ImageNet/AlexNet_imagenet.pdf} 
\end{subfigure}   
\\[2ex]
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/ImageNet/ResNet_imagenet.pdf} 
\end{subfigure}	 
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/ImageNet/DenseNet_imagenet.pdf} 
\end{subfigure} 
\caption{Training loss over epochs for four VMs on ImageNet.\label{fig:imagnet}}
\end{figure}

\subsection{Classification by VMs}
We perform classification tasks using two well-known datasets: ImageNet \cite{krizhevsky2012imagenet} and Cifar-10 \cite{krizhevsky2010cifar}  and evaluate the performance by reporting the convergence speed and testing accuracy.  

{\bf a) Comparison of Convergence Speed.} To validate this, we employ the ImageNet dataset and adopt a large-batch training strategy, which provides more stable gradient estimates and reduces variance in parameter updates. All experimental settings follow the official implementations of widely used model architectures on the ImageNet dataset\footnote{https://github.com/pytorch/examples/tree/main/imagenet}. 
We compare SISA with five optimizers: Adam, AdamW, Adabelief, Lamb, and SGD-M \cite{robbins1951stochastic} on training four VMs:  ViT \cite{dosovitskiy2020image}, AlexNet \cite{krizhevsky2012imagenet}, ResNet \cite{he2016identity}, and DenseNet \cite{huang2017densely}. 
To ensure fair comparison, all models are trained using identical training strategies and hyperparameters, such as batch size and weight decay. The only exception is the learning rate (which plays a role analogous to $\sigma$ and $\rho$ in SISA), which is tuned for each optimizer to achieve comparable performance. Figure \ref{fig:imagnet} illustrates the training loss over epochs, and SISA consistently achieves the fastest convergence and maintains the lowest training loss.




 


 {\bf b) Comparison of Testing Accuracy.}
We evaluate the testing accuracy on both the ImageNet and Cifar-10 datasets, following the experiment settings and hyperparameter configurations in \cite{zhuang2020adabelief}. In this numerical experiment, a small batch size (e.g., $|\D_i|=128$ for every $i\in[m]$) is used during training.  For Cifar-10, we train three VMs: VGG-11 \cite{simonyan2014very}, ResNet-34 \cite{he2016identity}, and DenseNet-121 \cite{huang2017densely}. As shown in Table \ref{table:cifar_sota}, SISA achieves the highest test accuracy among all baselines. Additionally, Figure \ref{fig:cifar} illustrates the training loss and test accuracy over training epochs for VGG-11 and ResNet-34, demonstrating the fast convergence and high accuracy of SISA. 
For ImageNet, we train a ResNet18 model and report the testing accuracy  in Table \ref{table:imagenet_sota}. From the table, SISA outperforms most of adaptive methods but is slightly worse than SGD-M and AdaBelief.



 \begin{table}[!th]
 \renewcommand{\arraystretch}{1.25}\addtolength{\tabcolsep}{-4pt}
\caption{Top-1 Acc. (\%) of ResNet-34, VGG-11, and DenseNet-121 on Cifar-10.}\vspace{-5mm}
\begin{center}
\begin{tabular}{cccccccccccc}
\hline 
  & SISA & SGD-M & AdaBound & Adam & Radam & MSVAG & AdamW & AdaBlief & Yogi &{LAMB}\\ 
  &   & \cite{robbins1951stochastic} & \cite{luo2019adaptive} &  \cite{kingma2014adam} &  \cite{liu2019variance} &  \cite{balles2018dissecting} &  \cite{loshchilov2017decoupled} &  \cite{zhuang2020adabelief} & \cite{zaheer2018adaptive} &\cite{You2020Large} \\
\hline
ResNet-34 &\textbf{95.04}& $94.65^*$ & $94.33^\Diamond$ &$94.69^*$&$93.02^*$ &$94.44^\Diamond$ &$94.28^*$ &$94.11^*$ & $94.52^*$ &$94.01^*$ \\
%\hline
VGG-11 &\textbf{90.83}& $90.11^\Diamond$ & $90.62^\Diamond$ &$88.40^\Diamond$&$89.30^\Diamond$ &$90.24^\Diamond$ &$89.39^\Diamond$ &90.07 & $90.67^\Diamond$& 87.48\\
%\hline
DensNet-121 &\textbf{95.05}& $94.50^\Diamond$ & $94.58^\Diamond$ &$93.35^\Diamond$&$94.81^\Diamond$ &$94.81^\Diamond$ &$94.55^\Diamond$ &94.78 & $94.76^\Diamond$ &94.53\\
\hline
\end{tabular}
\begin{tablenotes}
$*$ and $\Diamond$ are reported in \cite{Adan24} and \cite{zhuang2020adabelief}. {Higher} values indicate better performance.
\end{tablenotes}\vspace{-5mm}
\label{table:cifar_sota}
\end{center}
\end{table}


\begin{figure}[!t]
\centering
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.975\linewidth]{Figures/cifar10/vgg_train.pdf} 
\end{subfigure}	 
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/cifar10/vgg_test.pdf} 
\end{subfigure}   \\[2ex]
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.975\linewidth]{Figures/cifar10/resnet_train.pdf} 
\end{subfigure}   
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/cifar10/resnet_test.pdf} 
\end{subfigure} 
\caption{
Training loss and testing accuracy over epochs for VMs on Cifar-10.\label{fig:cifar}}
\end{figure}



\begin{table}[!th]
\renewcommand{\arraystretch}{1.25}\addtolength{\tabcolsep}{-1.6pt}
\caption{Top-1 Acc. (\%) of ResNet-18 on ImageNet.}
\vspace{-5mm}
\begin{center}
\begin{tabular}{ccccccccccc}
\hline 
   SISA & SGD-M & AdaBound & Adam & Radam & {MSVAG} & AdamW & AdaBelief & LAMB &Nadam\\ 
     & \cite{robbins1951stochastic} & \cite{luo2019adaptive} &  \cite{kingma2014adam} &  \cite{liu2019variance} &  \cite{balles2018dissecting} &  \cite{loshchilov2017decoupled} &  \cite{zhuang2020adabelief} & \cite{You2020Large} & \cite{dozat2016incorporating} &\\
\hline
70.03& $\textbf{70.23}^\Diamond$ & $68.13^\Diamond$ &$63.79^\Diamond$&$67.62^\Diamond$ &$65.99^\Diamond$ &$67.93^\Diamond$ &$70.08^\Diamond$ & $68.46^*$ & $68.82^*$\\
\hline
\end{tabular}
\begin{tablenotes}
 $*$ and $\Diamond$ are reported in \cite{Adan24} and \cite{zhuang2020adabelief}. {Higher} values indicate better performance.
\end{tablenotes}
\label{table:imagenet_sota}
\end{center} \vspace{-5mm}
\end{table}


\subsection{Fine-tuning LLMs}In this section, we apply SISA to fine-tune a LLM, the pre-trained GPT-2 model \cite{radford2019language}, which already possesses a foundational understanding of the English language, on a cooking recipe corpus. Fine-tuning enables the LLM to acquire task-specific knowledge while retaining its general language capabilities, allowing it to generate coherent and logically consistent recipes based on given input ingredients. Following the experimental setup and hyperparameter configurations outlined in \cite{tran2023finetuning}, we compare SISA against AdamW, using GPT-2 models with 124M and 335M parameters. {To prevent overfitting, we terminate SISA at the 25th epoch at which the testing loss begins to rise.}

\begin{figure}[!t]
\centering
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/GPT/gpt_small.pdf} 
\end{subfigure}	 
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/GPT/gpt_medium.pdf} 
\end{subfigure}   
\caption{
Training loss over epochs for fine-tuning GPT2. \label{fig:gpt}}\vspace{-2mm}
\end{figure}


To evaluate performance, we report both the training and testing loss, as well as the semantic similarity \cite{zhang2019bertscore}. It noted that lower training and testing loss and higher semantic similarity indicate better performance. Semantic similarity is computed using BERTScore \cite{zhang2019bertscore}, which compares the contextual word embeddings from a pre-trained BERT model to capture the deeper meaning of two texts, rather than relying on exact word matches, and calculates an overall similarity score. By incorporating semantic similarity alongside testing loss, we provide a more comprehensive evaluation of the generated recipes, considering both token-level accuracy and logical coherence. The training process over 25 epochs is shown in Figure \ref{fig:gpt}, where SISA consistently demonstrates faster convergence. Moreover, as illustrated in Table \ref{table:gpt_small}, SISA outperforms AdamW in both testing loss and semantic similarity during the early stages of training.

\begin{table}[!th]
\caption{Testing loss and semantic similarity for GPT2.}\vspace{-4mm}
\begin{center}
\renewcommand{\arraystretch}{1.0}\addtolength{\tabcolsep}{6.5pt}
% Create the table
\begin{tabular}{llccccccc}
\hline
&& \multicolumn{3}{c}{Testing Loss} && \multicolumn{3}{c}{Semantic Similarity(\%)} \\ \cline{3-5}\cline{7-9}
&Epoch      & 2& 5& 15&& 2& 5& 15\\ \hline
\multirow{2}{*}{GPT2(124M)}&AdamW                & 5.20& 3.45& 2.30&& 82.16 & 84.30& 84.73\\ 
&SISA                & 3.69& 2.73& 2.17&& 83.15 & 84.36& 84.82 \\ \hline
\multirow{2}{*}{GPT2(335M)}&AdamW                & 3.62& 2.73& 1.95&& 83.17 & 84.50& 84.19\\  
&SISA                & 2.31& 2.04& 1.93&& 84.76 & 85.24& 84.84 \\ \hline
\end{tabular}
\label{table:gpt_small}
\end{center}\vspace{-5mm}
\end{table}


\subsection{Games reward by RLMs}
  In this subsection, we employ SISA to train  RLMs, specifically focusing on two popular algorithms: Proximal Policy Optimization (PPO) and Advantage Actor-Critic (A2C) \cite{duan2016benchmarking}, which are Adam-based approaches. Therefore, we can directly substitute Adam with SISA in PPO and A2C to derive PPO-SISA and A2C-SISA, without requiring additional modification. We refer to the default configurations as PPO-Adam and A2C-Adam. We evaluate these algorithms on four continuous control tasks/games simulated in MuJoCo \cite{todorov2012mujoco}, a widely-used physics engine. The environments for the four tasks are Ant, Humanoid, Half-Cheetah, and Inverted Double Pendulum.

 
In each test, agents are rewarded at every step based on their actions. Following standard evaluation procedures, we run each task with the same random seeds and assess performance over 10 episodes every 30,000 steps. All experiments are conducted using the Tianshou framework \cite{weng2022tianshou} with its default hyperparameters as the baseline. The results are shown in Figure \ref{fig:rl}, where the solid line represents the average rewards and the shaded region indicates the standard deviation over episodes. Across all four games, SISA outperforms the Adam-based algorithms, achieving higher rewards.

\begin{figure}[!t]
\centering
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/RL/Ant.pdf} 
\end{subfigure}	 
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/RL/Humanoid.pdf} 
\end{subfigure}   
 \\[2ex]
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/RL/HalfCheetach.pdf} 
\end{subfigure}   
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/RL/InvertedDoublePendulum.pdf} 
\end{subfigure} 
\caption{Rewards over steps for RLMs under four environments.\label{fig:rl}}
\end{figure}

\subsection{Generation tasks by GANs}
 
It is well known that GANs involve alternating updates between the generator and discriminator in a minimax game, a training procedure that is notoriously unstable \cite{goodfellow2014generative}. This instability presents a significant challenge when optimizing GANs for real-world applications. To evaluate the stability of optimizers, we compare SISA with adaptive optimizers, such as Adam, RMSProp, and AdaBelief. These optimizers are commonly recommended to enhance both stability and efficiency in training GANs \cite{salimans2016improved, zhuang2020adabelief}. For this experiment, we use two popular GAN architectures: the Wasserstein GAN (WGAN) \cite{arjovsky2017wasserstein} and its improved version, the Wasserstein GAN with gradient penalty (WGAN-GP) \cite{salimans2016improved}, implemented with a small model and a vanilla CNN generator on the Cifar-10 dataset.

 To evaluate performance, we compute the Frchet Inception Distance (FID) \cite{heusel2017gans} every 10 training epochs, measuring the distance between 6,400 generated images and 60,000 real images. The FID score is a widely used metric for generative models, assessing both image quality and diversity. The lower FID values indicate better generated images. We follow the experimental setup from \cite{zhuang2020adabelief} and run each optimizer five times independently. The corresponding mean and standard deviation of training FID over epochs are presented in Figure \ref{fig:gan}. Clearly, SISA achieves the fastest convergence and the lowest FID. After training,  64,000 fake images are produced to compute the testing FID in Table \ref{table:wgan_sota}. Once again, SISA outperforms the other three optimizers, demonstrating its superior stability and effectiveness in training GANs.

\begin{figure}[!t]
\centering
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/GAN/WGAN.pdf} 
\end{subfigure}	 
\begin{subfigure}{.495 \textwidth}
	\centering
	\includegraphics[width=.95\linewidth]{Figures/GAN/WGAN_GP.pdf} 
\end{subfigure}   
\caption{Training FID over epochs for GANs on Cifar-10. \label{fig:gan}}
\end{figure}

\begin{table}[!th]
\renewcommand{\arraystretch}{1.25}\addtolength{\tabcolsep}{10.0pt}
\caption{Testing FID after training GANs on Cifar-10. }
\vspace{-5mm}
\begin{center}
\begin{tabular}{ccccc}
\hline 
 & SISA & Adam & RMSProp& AdaBelief\\\hline 
WGAN &$\textbf{85.07}\pm \textbf{5.49}$ & $95.06\pm2.97$ & $102.39\pm7.05$ &$92.37\pm11.13$\\
WGAN\_GP &$\textbf{53.34}\pm \textbf{3.87}$ & $71.38\pm5.04$ & $121.62\pm22.79$ &$65.71\pm3.36$\\
\hline
\end{tabular}
\label{table:wgan_sota}
\end{center} \vspace{-3mm}
\end{table}


\begin{table}[!th]
\renewcommand{\arraystretch}{1.25}\addtolength{\tabcolsep}{-3.0pt}
\caption{Testing perplexity on Penn Treebank for 1, 2, 3-layer LSTM. }
\vspace{-5mm}
\begin{center}
\begin{tabular}{ccccccccccc}
\hline 
 & SISA & SGD-M & AdaBound & Adam & Radam& MSVAG& AdamW & AdaBelief & Yogi &Padam\\ 
  &   & \cite{robbins1951stochastic} & \cite{luo2019adaptive} &  \cite{kingma2014adam} &  \cite{liu2019variance} &  \cite{balles2018dissecting} &  \cite{loshchilov2017decoupled} &  \cite{zhuang2020adabelief} & \cite{zaheer2018adaptive} & \cite{chen2018closing} \\\hline 
1-layer &\textbf{84.27}& 85.70 & 84.73 &$85.90^*$&$86.50^*$ &84.85 &88.39 &84.47 & 86.37 & $94.46^\Diamond$\\
%\hline
2-layer &\textbf{66.23}& 67.32 & 67.43 &$67.30^*$&$72.30^*$ &$68.82^*$ &$72.80^*$ &66.70 & 71.64 & $89.77^\Diamond$\\
%\hline
3-layer &\textbf{61.06}& 63.91 & 63.72 &$65.02^*$&$70.00^*$ &$64.32^*$ &$69.90^*$ &61.20 & 67.69 & $95.10^\Diamond$\\
\hline
\end{tabular}
\begin{tablenotes}
$*$ and $\Diamond$ are reported in \cite{Adan24} and \cite{zhuang2020adabelief}. {Lower} values indicate better performance. 
\end{tablenotes}
\label{table:lstm_sota}
\end{center} \vspace{-3mm}
\end{table}

\subsection{Text prediction with RNNs}
We extend our experiments to natural language processing tasks by applying SISA to long short-term memory (LSTM) networks \cite{graves2012long}, one of the most widely used RNNs. We adopt the same experimental setup and default hyperparameter configurations as in \cite{zhuang2020adabelief} since the provided code offers a broader range of baseline comparisons. Model performance is evaluated on the Penn TreeBank dataset \cite{marcus1993building}, with perplexity scores reported on the testing dataset. As shown in Table \ref{table:lstm_sota}, SISA consistently achieves the lowest perplexity across all three LSTM models, demonstrating its effectiveness in improving RNNs performance.


\section{Conclusion}
This paper introduces an ADMM-based algorithm that leverages stochastic gradients and solves sub-problems inexactly, significantly accelerating computation. The preconditioning framework allows it to incorporate popular second-moment schemes, enhancing training performance. Theoretical guarantees, based solely on the Lipschitz continuity of the gradients, make the algorithm suitable for heterogeneous datasets, effectively addressing an open problem in stochastic optimization for distributed learning. The algorithm demonstrates superior generalization across a wide range of architectures, datasets, and tasks, outperforming well-known deep learning optimizers.

\bibliographystyle{abbrv}
\bibliography{references}

\newpage
\appendix

\section{Appendix: Hyperparameters of All Experiments}\label{appedix-A}

\begin{table}[!th]
\caption{Hyperparameters of SISA in the numerical experiments.}\vspace{-5mm}
\begin{center}
\renewcommand{\arraystretch}{1.25}\addtolength{\tabcolsep}{-0.25pt}
\begin{tabular}{llccccccccc}
\hline
\multirow{2}{*}{} & model &$\sigma_i^0$ & $\rho_i$ & $\gamma_i$ & $\mu$ & $\lambda$ & $E$ &$m$ &$n$ &$|\D_{ij}|$\\ \hline
&ResNet-18& 0.005 & 2e3/3 & 1 & 0 & 0 & 303 &4&4&8e$4$ \\ 
             VMs        &ViT-B-16& 0.005 & 2e4 & 1 & 0 & 0 & 380 &4&4&8e$4$ \\ 
             Training        &AlexNet& 0.050 & 2e4 & 1 & 0 & 0 & 380 &4&4&8e$4$ \\
                     &DenseNet-121& 0.005 & 2e3/3 & 1 & 0 & 0 & 303 &4&4&8e$4$ \\\hline

&ResNet-34& 0.10 & 5e3 & $\gamma_\ell(0.80,15)^*$ & 5e-5 & 0 & 200&4&390&32 \\ 
             VMs        &VGG-11& 6.50 & 5e4 & $\gamma_\ell(0.90,10)$ & 2.5e-4 & 0 & 200&4&390&32 \\ 
              Testing       &DenseNet-121& 0.08 & 2e3 & $\gamma_\ell(0.25,50)$ & 2e-5 & 1e-5 & 200&4&390&64 \\
                     &ResNet-18& 1.00 & 2e3 & 1 & 1e-2 & 5e-6 & 100 &4&5000&64\\\hline
\multirow{2}{*}{LLMs} &GPT(124M)& 0.01 & 2.5e4 & $\gamma_\ell^{**}$ & 0 & 0 & 80 &4&4&$62$ \\
                      &GPT(335M)& 0.01 & 1e4 & $\gamma_\ell$ & 0 & 0 & 150 &4&4&$62$ \\ \hline
\multirow{4}{*}{RLMs} &Ant& 5.0 & 2e4 & $\gamma_\ell$ & 0 & 0 & 100 &4&112&64 \\
                      &Humanoid& 0.1 & 1e5 & $\gamma_\ell$ & 0 & 0 & 100 &4&112&64 \\ 
                      &Half-Cheetah& 0.5 & 200 & $\gamma_\ell$ & 0 & 0 & 100 &4&117&64 \\
                      &Pendulum& 0.5 & 200 & $\gamma_\ell$ & 0 & 0 & 100 &4&117&64\\\hline
\multirow{2}{*}{GANs} &WGAN& 1e-1 & 1e3 & 1 & 0 & 5e-5 & 100 &4&937&16 \\  
                     &WGAN-GP& 5e-5 & 2e2 & 1 & 0 & 1e-4 & 100 &2&937&32\\ \hline
\multirow{3}{*}{RNNs} &1 layer& 0.04 & 4e2 & 1 & 1.2e-6 & 0 & 200 &4&663&5 \\ 
                     &2 layer& 0.01 & 2e2/3 & 1 &  1.2e-6 & 0 & 200&4&663&5 \\ 
                     &3 layer& 0.01 & 2e2/3 & 1 &  1.2e-6 & 0 & 200&4&663&5 \\\hline
\end{tabular}
\begin{tablenotes}
      $^*~ \gamma_\ell(t,s)=t$ if $\ell$ is a multiple of $sn$ and $=1$ otherwise.
      
      $^{**} \gamma_\ell=1-\frac{1}{E-\lfloor {\ell}/{n}\rfloor}$, where $E$ is the number of total epochs and $\lfloor a\rfloor$ is the floor of $a$. 
    \end{tablenotes}
\label{table:hyperparameter}
\end{center}\vspace{-5mm}
\end{table}
\section{Appendix: Proof of All Theorems}

\subsection{Useful facts}
Before the main results, we first present some useful facts. 
\begin{itemize}[leftmargin=17pt]
\item For any $\bw$ and $\bv$, and $t>0$, 
\begin{eqnarray}\label{triangle-ineq}
\begin{aligned}
 2 \left\langle \bw, \bv \right\rangle &\leq t \left\| \bw  \right\|^2 +   \frac{1}{t}\left\| \bv \right\|^2\\
\left \| \bw + \bv  \right\|^2&\leq (1+t) \left\| \bw  \right\|^2 + \Big(1+\frac{1}{t}\Big) \left\| \bv  \right\|^2.
\end{aligned}
\end{eqnarray}
\item By update  (\ref{sub-w-mini}), it follows 
\begin{eqnarray}\label{optimality-condition-w}
\begin{aligned}
0=~&\sum_{i=1}^m \alpha_i \Big[   - \bpi_{i}^\ell  -  \sigma_i^{\ell}(\bw_{i}^\ell- \bw^{\ell+1})\Big] +\lambda \bw^{\ell+1}\\
=~&\sum_{i=1}^m \alpha_i \Big[   - \bpi_{i}^\ell  -  \sigma_i^{\ell}(\bw_{i}^\ell- \bw^{\ell+1}) +\lambda \bw^{\ell+1}\Big]. 
\end{aligned}
\end{eqnarray}
\item  By update  (\ref{sub-wbn-mini}), it follows
\begin{eqnarray}\label{optimality-condition}
\begin{aligned}
0=~&\bpi_{i}^{\ell} + \sigma_i^{\ell+1} \left(\bw_{i}^{\ell+1}  - \bw^{\ell+1} \right)  +  { \bg_{i}^{\ell+1}}+ \rho_i \Q_i^{\ell+1}  \left(\bw_{i}^{\ell+1}  -\bw^{\ell+1} \right)\\[1ex]
\overset{\eqref{sub-pin}}{=} & \bpi_{i}^{\ell+1} +   { \bg_{i}^{\ell+1}} +\rho_i \Q_i^{\ell+1}  \left(\bw_{i}^{\ell+1}  - \bw^{\ell+1}\right) .
\end{aligned}
\end{eqnarray}
\item The choice, $\eta_i\I\succeq \Q_i^{\ell+1}   \succeq0$, of $\Q_i^{\ell+1}$ indicates
\begin{eqnarray}\label{Q-upper-bd}
\begin{aligned}
\left\| \Q_i^{\ell+1} \right\| \leq \eta_i, ~~\forall i\in[m].
\end{aligned}
\end{eqnarray}
\item Hereafter, we define a useful constant:
 \begin{eqnarray} \label{def-eps-delta}
\begin{aligned}
\overline{\delta}:=10\delta + \frac{1}{\sigma^0}\overset{\eqref{def-eps} }{=}\max_{i\in[m]} \frac{80(\triangle F+2)}{\sigma^0 \alpha_i\gamma_i^{\tau_i}} + \frac{1}{\sigma^0}.
\end{aligned}
\end{eqnarray}
Since $\alpha_i\gamma_i^{\tau_i}\in(0,1)$, direct verification derives that
\begin{eqnarray*}   
\sigma^0 \overset{\eqref{choice-of-sigma}}{\geq}   \max_{i\in[m]}\frac{80(\triangle F+3)}{\alpha_i\gamma_i^{\tau_i}\triangle F} \geq  \max_{i\in[m]}\left(\frac{80(\triangle F+2)}{\alpha_i\gamma_i^{\tau_i} } +1\right)\frac{1}{\triangle F} \overset{\eqref{def-eps} }{=}\left(10\delta\sigma^0 +1\right)\frac{1}{\triangle F}. 
\end{eqnarray*}
The definition of $\varepsilon_i(\cdot)$ in \eqref{def-eps} indicates $\varepsilon_i(s)\geq \varepsilon_i(t)$ if $s\geq t\geq0$, thereby
\begin{eqnarray*} 
\begin{aligned}\varepsilon_i\left(\triangle F\right) \geq \varepsilon_i\left(10\delta+ \frac{1}{\sigma^0}\right)\overset{\eqref{def-eps-delta}}{=}\varepsilon_i(\overline{\delta}).
\end{aligned}\end{eqnarray*}
As a result, we obtain
\begin{eqnarray}\label{sigma-0-vareps}
\begin{aligned} \sigma^0\overset{\eqref{choice-of-sigma}}{\geq} \frac{16\gamma_i\varepsilon_i(\triangle F)}{1-\gamma_i}\geq \frac{16\gamma_i\varepsilon_i(\overline{\delta})}{1-\gamma_i}  \geq  \frac{16\gamma_i\varepsilon_i(\delta)}{1-\gamma_i},
\end{aligned}\end{eqnarray}   
where the last inequality is due to $\overline{\delta}\geq \delta.$  We note that the range of $\gamma_i$  is
\begin{eqnarray}\label{range-gamma}
\begin{aligned}
\frac{3}{4}\leq\gamma_i<1, ~~\forall i\in[m].
\end{aligned}
\end{eqnarray}
Together with \eqref{sigma-0-vareps} and \eqref{choice-of-sigma}, one can check the following order,
\begin{eqnarray}
\label{increase-sigma}&&\sigma_i^{\ell+1}>\sigma_i^{\ell}>\sigma_i^0\geq\sigma^0 \geq\max\left\{8\rho_i\eta_i,~ 8r_i,~\frac{16\gamma_i\varepsilon_i(\delta)}{1-\gamma_i}\right\}, ~~\forall i\in[m], \forall \ell\geq 1.
%\label{range-ti}&&t_i:=\frac{8-5\gamma_i+ \gamma_i^2 }{2} \in(2,3), ~~~  t_i\gamma_i\in(1,2),\qquad\forall i\in[m].
\end{eqnarray} 
Furthermore,   for any $\ell\geq0$, 
\begin{eqnarray} \label{upbd-sigma-ell-1}
\begin{aligned} 
\frac{16 \varepsilon_i(\delta)}{\sigma_i^{\ell+1}} \overset{\eqref{sub-sigma-mini}}{=}  \frac{ 16\gamma_i\varepsilon_i(\delta)}{1- \gamma_i} \left( \frac{1}{ {\sigma_i^{\ell}}} -\frac{1}{ {\sigma_i^{\ell+1}}} \right)\overset{\eqref{sigma-0-vareps}}{\leq}    \sigma_i^{0}\left( \frac{1}{ {\sigma_i^{\ell}}} -\frac{1}{ {\sigma_i^{\ell+1}}} \right)\overset{\eqref{sub-sigma-mini}}{=} \gamma_i^{\ell}-\gamma_i^{\ell+1}.
\end{aligned}
\end{eqnarray}
\item By letting $\textbf{P}_i^{\ell+1}:=(\sigma_i^{\ell+1}-\sigma_i^{\ell})\I+ \rho_i \Q_i^{\ell+1}$, it follows 
\begin{eqnarray} \label{P-upper-bd}
\begin{aligned} \|\textbf{P}_i^{\ell+1}\| \overset{\eqref{Q-upper-bd}}{\leq}  \sigma_i^{\ell+1}-\sigma_i^{\ell}+\rho_i\eta_i\overset{\eqref{increase-sigma}}{\leq} \sigma_i^{\ell+1}- \frac{7\sigma_i^{\ell}}{8} \overset{\eqref{sub-sigma-mini}}{=} \sigma_i^{\ell+1}- \frac{7\gamma_i\sigma_i^{\ell+1}}{8} \overset{\eqref{range-gamma}}{\leq} \frac{11\sigma_i^{\ell+1}}{32}.
\end{aligned}
\end{eqnarray}
\item Let $(\bw^*,\W^*)$ be the optimal solution to \eqref{opt-prob-distribute}. Since $F^*$ is the optimal function value and is lower bounded,  for any $\bw=\bw_i,i\in[m]$, it holds
\begin{eqnarray}  \label{opt-value}
\begin{aligned}
-\infty < F^*   &=  \sum_{i=1}^m \alpha_i   F_{i}(\bw_i^*) + \frac{\lambda}{2}\|\bw^*\|^2   
 \leq \sum_{i=1}^m \alpha_i   F_{i}(\bw_i) + \frac{\lambda}{2}\|\bw\|^2\\
& = \sum_{i=1}^m \alpha_i   F_{i}(\bw) + \frac{\lambda}{2}\|\bw\|^2 
  = F(\bw)+  \frac{\lambda}{2}\|\bw\|^2.
\end{aligned}
\end{eqnarray} 
\end{itemize}



\subsection{Key lemmas}
\begin{lemma} Under Assumption \ref{assumption}, for any $\bw,\bv$ and  any $\B_i\subseteq\D_i$,
\begin{eqnarray}\label{Lipschitz-continuity}
\begin{aligned}
  & \left\|\nabla  F_i(\bw;\B_i) - \nabla  F_i(\bv;\B_i) \right\|   \leq \frac{\sigma_i^\ell}{8} \left\| \bw- \bv \right\|,&&\forall i\in[m], ~\forall \ell\geq 0, \\
 & F_{i}(\bw) -F_{i}(\bv) \leq   \left\langle \nabla F_{i}(\bu), \bw-\bv \right\rangle  +\frac{ \sigma_i^{\ell}}{16} \|  \bw-\bv\|^2,&&\forall i\in[m], ~\forall \ell\geq 0, 
\end{aligned}\end{eqnarray} 
where $\bu=\bw$ or $\bu=\bv$.
\end{lemma}
\begin{proof} 
The Lipschitz continuity of $\nabla f(\cdot;\bx_t)$ with constant $c(\bx_t)>0, c_i=\max_{\bx_t\in\D_i}c(\bx_t)$, and $r_i=c_i+ \mu-\lambda$ imply that, for any $\B_i\subseteq\D_i$,
\begin{eqnarray*} 
\begin{aligned}
   \Big\|\nabla  F_i(\bw;\B_i) - \nabla  F_i(\bv;\B_i) \Big\|   &\overset{\eqref{def-F_i-F}}{=}&& \Big\|\nabla  H_i(\bw;\B_i) - \nabla  H_i(\bv;\B_i) + (\mu-\lambda)(\bw-\bv)\Big\|\\
 &~\leq&&  \Big\| \frac{1}{|\B_i|}\sum_{\bx_t\in\B_i} (\nabla  f(\bw;\bx_t) - \nabla  f(\bv;\bx_t) )\Big\|+(\mu-\lambda)\Big\| \bw-\bv  \Big\| \\
  &~\leq&&   \frac{1}{|\B_i|} \sum_{\bx_t\in\B_i}c(\bx_t)\Big\| \bw-\bv  \Big\|+(\mu-\lambda)\Big\| \bw-\bv  \Big\| \\
%  &~\leq&&   \frac{1}{|\B_i|}   \sum_{\bx_t\in\D_i}\Big\|\nabla  f(\bw;\bx_t) - \nabla  f(\bv;\bx_t) \Big\|+(\mu-\lambda)\| \bw- \bv\| \\
   &~\leq&& \frac{1}{|\B_i|}    \sum_{\bx_t\in\B_i} c_i \Big\| \bw-\bv  \Big\|+(\mu-\lambda)\Big\| \bw-\bv  \Big\| \\
  &~= &&   r_i \Big\| \bw-\bv  \Big\| \overset{\eqref{increase-sigma}}{\leq}\frac{\sigma_i^\ell}{8} \Big\| \bw- \bv \Big\|.
\end{aligned}\end{eqnarray*}
It follows from \cite[Eq. (37)]{zhouli23} and the above condition that for $\bu=\bw$ or $\bu=\bv$,
\begin{eqnarray*}
\begin{aligned}  
F_{i}(\bw) -F_{i}(\bv) ~\leq ~ &\left\langle \nabla F_{i}(\bu), \bw-\bv \right\rangle  +\frac{r_i}{2} \|  \bw-\bv\|^2\\
\overset{\eqref{increase-sigma}}{\leq} &\left\langle \nabla F_{i}(\bu), \bw-\bv \right\rangle  +\frac{ \sigma_i^{\ell}}{16} \|  \bw-\bv\|^2,
\end{aligned}
\end{eqnarray*} 
showing the desired result.
\end{proof}

\begin{lemma}\label{descent-lemma} Let $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ be the sequence generated by Algorithm \ref{algorithm-ADMM-mini} with $\bsi^0$  chosen as (\ref{choice-of-sigma}). Under Assumption \ref{assumption}, for any $i\in[m]$ and any $\ell\geq0$, if $\bw^\ell\in\N(\delta)$ then 
 \begin{eqnarray}
 \label{gap-pi-k}
     \left\|\triangle \bpi_{i}^{\ell+1}\right\|^2  \leq     \frac{15\varepsilon_i(\delta)}{2} +  \frac{ (\sigma_i^{\ell+1})^2}{32} \left\| \triangle \bw^{\ell+1} \right\|^2+ \frac{\sigma_i^{\ell+1}\varphi_i^{\ell+1}}{4},
\end{eqnarray}
where $\varphi_i^{\ell+1}$ is defined by
 \begin{eqnarray}
 \label{def-varphi}\varphi_i^{\ell+1}:=  \frac{ \sigma_i^{\ell}}{2}\left\|\triangle\overline{\bw}^{\ell}_i\right\|^2 -   \frac{ \sigma_i^{\ell+1}}{2} \left\|\triangle\overline{\bw}^{\ell+1}_i \right\|^2 .\end{eqnarray}
%Moreover, for any $k\geq1$,
% \begin{eqnarray}
%\label{gap-pi-k}
%\sum_{\ell=1}^{k} \left\|\triangle \bpi_{i}^{\ell+1}\right\|^2 
%   &\leq&      \sum_{\ell=1}^k \frac{5(\sigma_i^{\ell})^2}{24}   \left\| \triangle \bw^{\ell+1} \right\|^2    +   \frac{1}{5}\left\| \triangle \bpi_{i}^{1}\right\|^2+   5k\varepsilon_i,\\
%    \left\|\triangle \bpi_{i}^{\ell+1}\right\|^2 &\leq &   4\varepsilon_i  +  \frac{ (\sigma_i^{\ell})^2}{6} \left\| \triangle \bw^{\ell+1} \right\|^2+ \frac{4\sigma_i^{\ell}}{5} \varphi_i^{\ell+1}.
%\end{eqnarray}
\end{lemma}
\begin{proof} 
  First,  it follows from $\bw^\ell\in\N(\delta)$  that 
  \begin{eqnarray*}  
\begin{aligned}
\left\| \nabla F_i(\bw^{\ell};\B_i^{\ell+1})-\bg_{i}^{\ell}\right\|^2 &~=&&   \left\| \nabla F_i(\bw^{\ell};\B_i^{\ell+1})-\nabla F_i(\bw^{\ell};\D_i)+\nabla F_i(\bw^{\ell};\D_i)-\bg_{i}^{\ell}\right\|^2\\
&~\leq  &&  2\left\| \nabla F_i(\bw^{\ell};\B_i^{\ell+1})-\nabla F_i(\bw^{\ell};\D_i)\right\|^2+2\left\| \nabla F_i(\bw^{\ell};\D_i)-\bg_{i}^{\ell}\right\|^2\\
& \overset{\eqref{def-eps}}{\leq} &&  \frac{\varepsilon_i(\delta)}{16},  
\end{aligned}
\end{eqnarray*}
which immediately leads to
 \begin{eqnarray} \label{fact-33}
\begin{aligned}
\left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}\right\|^2
 &~~=&& \left\|\bg_{i}^{\ell+1}-\nabla F_i(\bw^{\ell};\B_i^{\ell+1})+\nabla F_i(\bw^{\ell};\B_i^{\ell+1})-\bg_{i}^{\ell}\right\|^2 \\
&\overset{\eqref{triangle-ineq}}{\leq}  &&64 \left\| \nabla F_i(\bw^{\ell};\B_i^{\ell+1})-\bg_{i}^{\ell}\right\|^2 +\frac{64}{63} \left\|\bg_{i}^{\ell+1}-\nabla F_i(\bw^{\ell};\B_i^{\ell+1}) \right\|^2   \\
& \overset{\eqref{Lipschitz-continuity}}{\leq}&& {4\varepsilon_i(\delta)}+ \frac{(\sigma_i^{\ell+1})^2}{63} \left\| \triangle \bw^{\ell+1} \right\|^2.
% \overset{\eqref{sub-sigma-mini}}{=}~&   4b \varepsilon_i+\frac{ \gamma_i\sigma_i^{\ell+1}\sigma_i^{\ell}}{63} \left\| \triangle \bw^{\ell+1} \right\|^2.
\end{aligned}
\end{eqnarray}
Moreover, one can check that
%\begin{eqnarray*}
%\begin{aligned}
% \left\|\rho_i \Q_i^{\ell} \triangle\overline{\bw}^{\ell}_i\right\|^2+ \left\| \rho_i\Q_i^{\ell+1}\triangle\overline{\bw}^{\ell+1}_i  \right\|^2
%   \overset{\eqref{Q-upper-bd}}{\leq}  ~& \rho_i^2\eta_i^2 \left\| \triangle\overline{\bw}^{\ell}_i\right\|^2+ \rho_i^2\eta_i^2 \left\|\triangle\overline{\bw}^{\ell+1}_i  \right\|^2\\ 
%   \overset{\eqref{increase-sigma}}{\leq}  ~&\frac{\rho_i\eta_i\sigma_i^{\ell+1}}{8}   \left\| \triangle\overline{\bw}^{\ell}_i\right\|^2+\frac{\rho_i\eta_i\sigma_i^{\ell+1}}{8}   \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2\\ 
%    =  ~~& \frac{ \sigma_i^{\ell+1}}{8}  \varphi_i^{\ell+1}+\frac{2\rho_i\eta_i\sigma_i^{\ell+1}}{8}   \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2\\ 
%      \overset{\eqref{sub-sigma-mini}}{=}  ~& \frac{ \sigma_i^{\ell+1}}{8}  \varphi_i^{\ell+1}+\frac{\rho_i\eta_i}{4\sigma_i^{\ell+1}}   \left\|  \bpi^{\ell+1}_i\right\|^2\\ 
%       \overset{\eqref{increase-sigma}}{\leq}  ~& \frac{ \sigma_i^{\ell+1}}{8}  \varphi_i^{\ell+1}+\frac{1}{32}   \left\| \bpi^{\ell+1}_i\right\|^2,
%\end{aligned}
%\end{eqnarray*}
\begin{eqnarray*}
\begin{aligned}
 \left\|\rho_i \Q_i^{\ell+1} \triangle\overline{\bw}^{\ell+1}_i\right\|^2+ \left\| \rho_i\Q_i^{\ell}\triangle\overline{\bw}^{\ell}_i  \right\|^2
   &\overset{\eqref{Q-upper-bd}}{\leq}  && \rho_i^2\eta_i^2 \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2+ \rho_i^2\eta_i^2 \left\|\triangle\overline{\bw}^{\ell}_i  \right\|^2\\ 
   & \overset{\eqref{sub-pin}}{=}  &&   \frac{\rho_i^2\eta_i^2}{(\sigma_i^{\ell+1})^2} \left\|\triangle\bpi^{\ell+1}_i  \right\|^2  +\frac{\rho_i^2\eta_i^2}{(\sigma_i^\ell)^2} \left\| \triangle\bpi^{\ell}_i\right\|^2\\ 
   &\overset{\eqref{increase-sigma}}{\leq}  &&\frac{1}{64}   \left\| \triangle\bpi^{\ell+1}_i\right\|^2+\frac{1}{64}   \left\| \triangle\bpi^{\ell}_i\right\|^2\\[1ex] 
 & ~~{=}  &&\frac{1}{32}   \left\|\triangle\bpi^{\ell+1}_i\right\|^2 +\frac{1}{64}   \left\| \triangle\bpi^{\ell}_i\right\|^2-\frac{1}{64}   \left\|\triangle\bpi^{\ell+1}_i\right\|^2 \\[1ex]
& \overset{\eqref{increase-sigma}}{\leq} &&\frac{1}{32}   \left\|\triangle\bpi^{\ell+1}_i\right\|^2 +\frac{\sigma_i^{\ell+1}}{64\sigma_i^{\ell}}   \left\| \triangle\bpi^{\ell}_i\right\|^2-\frac{\sigma_i^{\ell+1}}{64\sigma_i^{\ell+1}}   \left\|\triangle\bpi^{\ell+1}_i\right\|^2 \\ 
  & \overset{\eqref{sub-pin}}{=} && \frac{1}{32}   \left\|\triangle\bpi^{\ell+1}_i\right\|^2 +\frac{ \sigma_i^{\ell+1}\varphi_i^{\ell+1}}{32},
\end{aligned}
\end{eqnarray*}
which further results in
\begin{eqnarray*} 
\begin{aligned}
 \left\|\triangle \bpi_{i}^{\ell+1}\right\|^2  
 &~\overset{\eqref{optimality-condition}}{=} && \left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}+\rho_i \Q_i^{\ell+1} \triangle\overline{\bw}^{\ell+1}_i - \rho_i \Q_i^{\ell} \triangle\overline{\bw}^{\ell}_i \right\|^2\\
 &~\overset{\eqref{triangle-ineq}}{\leq}  &&   \frac{3}{2}\left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}\right\|^2 +  3   \left\|\rho_i \Q_i^{\ell+1} \triangle\overline{\bw}^{\ell+1}_i- \rho_i\Q_i^{\ell}\triangle\overline{\bw}^{\ell}_i  \right\|^2\\
&~\overset{\eqref{triangle-ineq}}{\leq}  &&   \frac{3}{2}\left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}\right\|^2 +  6  \left( \left\|\rho_i \Q_i^{\ell+1} \triangle\overline{\bw}^{\ell+1}_i\right\|^2+  \left\| \rho_i\Q_i^{\ell}\triangle\overline{\bw}^{\ell}_i  \right\|^2\right)\\
 &\overset{\eqref{fact-33}}{~\leq} && 6\varepsilon_i(\delta) +  \frac{ (\sigma_i^{\ell+1})^2}{42} \left\| \triangle \bw^{\ell+1} \right\|^2+ \frac{3\sigma_i^{\ell+1}\varphi_i^{\ell+1}}{16} + \frac{3}{16}   \left\| \triangle\bpi^{\ell+1}_i\right\|^2.
\end{aligned}
\end{eqnarray*}
 This enables showing \eqref{gap-pi-k}.  \end{proof}

% Next we prove \eqref{gap-pi}. 
% By the following condition
% \begin{eqnarray} \label{fact-44}
%\begin{aligned}
% \frac{3t_i\rho_i\eta_i}{5}  \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2 = ~~&-2\rho_i\eta_i   \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2+ \frac{(3t_i+10)\rho_i\eta_i}{5}  \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2\\[1ex]
% \overset{\eqref{sub-pin}}{=}  ~&-2\rho_i\eta_i   \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2+
%\frac{(3t_i+10)\rho_i\eta_i}{5(\sigma_i^{\ell+1})^2}  \left\| \triangle\bpi^{\ell+1}_i\right\|^2 \\
%  \overset{\eqref{choice-of-sigma}}{\leq}  ~&-2\rho_i\eta_i   \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2+\frac{3t_i+10}{25\sigma_i^{\ell+1}}  \left\| \triangle\bpi^{\ell+1}_i\right\|^2,
%\end{aligned}
%\end{eqnarray}
%we can derive the following chain of inequalities,
% \begin{eqnarray*} %\label{gap-pi-0}
%\begin{aligned}
% \frac{t_i}{\sigma_i^{\ell+1}}\left\|\triangle \bpi_{i}^{\ell+1}\right\|^2
% \overset{\eqref{optimality-condition}}{=} ~& \frac{t_i}{\sigma_i^{\ell+1}} \left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}+\rho_i \Q_i^{\ell+1} \triangle\overline{\bw}^{\ell+1}_i - \rho_i \Q_i^{\ell} \triangle\overline{\bw}^{\ell}_i \right\|^2\\ 
% \overset{\eqref{triangle-ineq}}{\leq}    ~&\frac{3t_i}{\sigma_i^{\ell+1}}\left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}\right\|^2+ \frac{3t_i}{\sigma_i^{\ell+1}}  \left\|\rho_i \Q_i^{\ell+1} \triangle\overline{\bw}^{\ell+1}_i\right\|^2+ \frac{3t_i}{\sigma_i^{\ell+1}}  \left\| \rho_i\Q_i^{\ell}\triangle\overline{\bw}^{\ell}_i  \right\|^2\\ 
%   \overset{\eqref{choice-of-sigma}}{\leq}  ~&\frac{3t_i}{\sigma_i^{\ell+1}}\left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}\right\|^2+ \frac{3t_i\rho_i\eta_i}{5}  \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2+ \frac{3t_i\rho_i\eta_i}{5}  \left\|\triangle\overline{\bw}^{\ell}_i  \right\|^2\\ 
%    \overset{\eqref{range-ti}}{\leq}  ~&\frac{3t_i}{\sigma_i^{\ell+1}}\left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}\right\|^2+ \frac{3t_i\rho_i\eta_i}{5}  \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2+ {2\rho_i\eta_i}   \left\|\triangle\overline{\bw}^{\ell}_i  \right\|^2\\ 
%    \overset{\eqref{fact-44}}{\leq}  ~&\frac{3t_i}{\sigma_i^{\ell+1}}\left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}\right\|^2  +\frac{3t_i+10}{25\sigma_i^{\ell+1}}  \left\| \triangle\bpi^{\ell+1}_i\right\|^2 + 2  \varphi_i^{\ell+1},
%\end{aligned}
%\end{eqnarray*}
%which after simple manipulations shows 
% \begin{eqnarray*} 
%\begin{aligned}
% \frac{2t_i}{3\sigma_i^{\ell+1}}\left\|\triangle \bpi_{i}^{\ell+1}\right\|^2 \leq \frac{22t_i-10}{25\sigma_i^{\ell+1}}\left\|\triangle \bpi_{i}^{\ell+1}\right\|^2 \leq \frac{3t_i}{\sigma_i^{\ell+1}}\left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}\right\|^2   + 2 \varphi_i^{\ell+1},
%\end{aligned}
%\end{eqnarray*}
%where the first inequality is due to $t_i\in(2,3)$ by \eqref{range-ti}. 
%As a result,
%  \begin{eqnarray*}
%\begin{aligned}
% \frac{t_i}{\sigma_i^{\ell+1}}\left\|\triangle \bpi_{i}^{\ell+1}\right\|^2 \leq ~~~&3 \varphi_i^{\ell+1} +\frac{9t_i}{2\sigma_i^{\ell+1}}\left\|\bg_{i}^{\ell+1}-\bg_{i}^{\ell}\right\|^2      \\
%\overset{\eqref{fact-33}}{\leq} ~&3 \varphi_i^{\ell+1}+   \frac{9t_i\varepsilon_i}{2\sigma_i^{\ell+1}} +\frac{t_i\gamma_i\sigma_i^{\ell}}{5}  \left\| \triangle \bw^{\ell+1} \right\|^2     \\
%\overset{\eqref{range-ti}}{\leq} ~&3 \varphi_i^{\ell+1}+   \frac{27\varepsilon_i}{2\sigma_i^{\ell+1}}+\frac{2\sigma_i^{\ell}}{5}  \left\| \triangle \bw^{\ell+1} \right\|^2, 
%\end{aligned}
%\end{eqnarray*}
%showing the desired result.  
%\end{proof}


%Summing the both sides of the above inequity for $\ell=1,2,\ldots,k$, we obtain
% \begin{eqnarray*} 
%\begin{aligned}
%22 \sum_{\ell=1}^k \left\|\triangle \bpi_{i}^{\ell+1}\right\|^2   
%   &\leq    \sum_{\ell=1}^k \frac{25(\sigma_i^{\ell})^2}{8}  \left\| \triangle \bw^{\ell+1} \right\|^2  +    3\sum_{\ell=1}^k \left\| \triangle \bpi_{i}^{\ell}\right\|^2+ 75k\varepsilon_i,
%\end{aligned}
%\end{eqnarray*}
%which further yields
% \begin{eqnarray*} 
%\begin{aligned}
%15\sum_{\ell=1}^{k} \left\|\triangle \bpi_{i}^{\ell+1}\right\|^2 &\leq 22 \left\|\triangle \bpi_{i}^{k+1}\right\|^2  + 19 \sum_{\ell=1}^{k-1} \left\|\triangle \bpi_{i}^{\ell+1}\right\|^2 \\  
%   &\leq       \sum_{\ell=1}^k \frac{25(\sigma_i^{\ell})^2}{8}   \left\| \triangle \bw^{\ell+1} \right\|^2    +   3\left\| \triangle \bpi_{i}^{1}\right\|^2+   75k\varepsilon_i.
%\end{aligned}
%\end{eqnarray*}
%This condition enables us to  derive \eqref{gap-pi-k}. 

%\end{proof}

{{ \begin{lemma}\label{descent-lemma} Let $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ be the sequence generated by Algorithm \ref{algorithm-ADMM-mini} with $\bsi^0$ chosen as (\ref{choice-of-sigma}). Suppose Assumption \ref{assumption} holds. If ${\bw^\ell, \bw^\ell_i\in\N(\delta)}$ for any ${i\in[m]}$, then  
\begin{eqnarray}  \label{L-lower-bd}
\begin{aligned}
\min\left\{\widetilde{\L}^{\ell}, \widetilde{\L}^{\ell+1}\right\}   \geq   F^* -1   > -\infty. %{\L}^{\ell}   \geq    F^* - \sum_{i=1}^m    \frac{\alpha_i\varepsilon_i}{\sigma_i^{0}}  > -\infty. 
\end{aligned}
\end{eqnarray}
\end{lemma}
\begin{proof} Let $c^{\ell}:=\sum_{i=1}^m\alpha_{i}  \sigma_i^\ell+\lambda.$ It follows from \eqref{sub-w-mini} that
 \begin{eqnarray*}  
 \begin{aligned}
 \triangle \bw^{\ell+1}  &~=  \frac{1}{c^{\ell}}\sum_{i=1}^m\alpha_{i} \left(\sigma_i^\ell \triangle \overline{\bw}_{i}^{\ell}+\bpi_{i}^{\ell} -\lambda \bw^{\ell}   \right)  \\
&\overset{\eqref{optimality-condition}}{=}   \frac{1}{c^{\ell}}\sum_{i=1}^m\alpha_{i} \left((\sigma_i^\ell\I-\rho_i \Q_i^{\ell}) \triangle \overline{\bw}_{i}^{\ell} -   { \bg_{i}^{\ell}} -\lambda \bw^{\ell}   \right).
 \end{aligned} 
 \end{eqnarray*} 
We note that $\sigma_i^\ell\I\succeq\sigma_i^\ell\I-\rho_i \Q_i^{\ell}\succeq  0$ and thus $\|\sigma_i^\ell\I-\rho_i \Q_i^{\ell}\|\leq \sigma_i^\ell$. Based on this, $\bw^\ell, \bw^\ell_i\in\N(\delta)$,  and the above condition, we obtain
 \begin{eqnarray*}  
 \begin{aligned}
\|\bw^{\ell+1}\| &~~\leq~ \|\bw^{\ell}\| +\|\triangle \bw^{\ell+1}\| \\
  &~\leq~ \sqrt{\delta} +\frac{1}{c^{\ell}} \sum_{i=1}^m\alpha_{i} \left( \sigma_i^\ell\|\triangle \overline{\bw}_{i}^{\ell}\| + \| { \bg_{i}^{\ell}}\| +\lambda \|\bw^{\ell} \|  \right)  \\
&~\leq~  \sqrt{\delta}+\frac{1}{c^{\ell}} \sum_{i=1}^m\alpha_{i} \left( \sigma_i^\ell\| \bw_{i}^{\ell}\|+\sigma_i^\ell\| \bw^{\ell}\| + \| { \bg_{i}^{\ell}}\| +\lambda \|\bw^{\ell} \|  \right)  \\
&~\leq~  \sqrt{\delta} +\frac{1}{c^\ell}\sum_{i=1}^m\alpha_{i} \left( (2\sigma_i^\ell +\lambda)\sqrt{\delta} + \| { \bg_{i}^{\ell}}\|   \right)\\
&~\leq~ 3\sqrt{\delta} +\frac{1}{c^\ell}\sum_{i=1}^m\alpha_{i}  \| \nabla F_{i}( \bw^\ell; {\B}_{i}^{\ell})\| \\
&\overset{\eqref{def-eps}}{\leq}  3\sqrt{\delta} + \sum_{i=1}^m \frac{\alpha_{i} \sqrt{\varepsilon_i(\delta)}}{8\sigma_i^\ell}    \\
&\overset{\eqref{sigma-0-vareps}}{\leq} 3\sqrt{\delta} + \sum_{i=1}^m  \frac{\alpha_{i}\sqrt{(1-\gamma_i)\sigma_i^0} }{24\sqrt{2\gamma_i} \sigma_i^\ell}  \\
&\overset{\eqref{choice-of-sigma}}{\leq} 3\sqrt{\delta} +  \frac{1}{24\sqrt{ \sigma^0}},
 \end{aligned} 
 \end{eqnarray*} 
 thereby leading to
  \begin{eqnarray*}  
 \begin{aligned}
\|\bw^{\ell+1}\|^2  \leq \left(3\sqrt{\delta} +  \frac{1}{24\sqrt{ \sigma^0}}\right)^2\overset{\eqref{triangle-ineq}}{\leq} 10\delta + \frac{1}{\sigma^0 } = {\overline{\delta}}.
 \end{aligned} 
 \end{eqnarray*}  
This shows ${\bw^{\ell+1}\in\N(\overline{\delta})}$.  As $\overline{\delta}>\delta$, it holds ${\bw^{\ell}\in\N(\overline{\delta})}$, thereby leading to
\begin{eqnarray}\label{fact-0006}
\max\left\{\left\|\triangle\bg_i^{\ell+1}\right\|^2, \left\|\triangle\bg_i^{\ell}\right\|^2\right\}\overset{\eqref{def-eps}}{\leq} \frac{\varepsilon_i(\overline{\delta})}{64}.
\end{eqnarray}
 One can verify that 
\begin{eqnarray} \label{fact-06}
\begin{aligned} 
   \left \langle -\bg_i^{\ell}, \triangle \overline{\bw}^{\ell}_i\right\rangle 
   \overset{(\ref{optimality-condition})}{=} ~&  \left\langle \bpi_{i}^{\ell} + \rho_i \Q_i^{\ell} \triangle \overline{\bw}^{\ell}_i, \triangle \overline{\bw}^{\ell}_i\right\rangle \\ 
\overset{\eqref{Q-upper-bd}}{\leq} ~&  \left\langle \bpi_{i}^{\ell}, \triangle \overline{\bw}^{\ell}_i\right\rangle +   \rho_i\eta_i \left\| \triangle \overline{\bw}^{\ell}_i \right\|^2\\ 
\overset{\eqref{increase-sigma}}{\leq} ~&  \left\langle \bpi_{i}^{\ell}, \triangle \overline{\bw}^{\ell}_i\right\rangle +   \frac{\sigma_i^{\ell}}{8} \left\| \triangle \overline{\bw}^{\ell}_i \right\|^2,\\
 \left\langle \triangle \bg_i^{\ell}, \triangle \overline{\bw}^{\ell}_i\right\rangle 
\overset{(\ref{triangle-ineq})}{\leq}~&  \frac{\sigma_i^{\ell}}{4}\left\| \triangle \overline{\bw}^{\ell}_i\right\|^2+\frac{1}{\sigma_i^{\ell}} \left\|\triangle \bg_i^{\ell}\right\|^2.
\end{aligned}
\end{eqnarray}
These two conditions lead to
\begin{eqnarray} \label{fact-6-w-wi}
\begin{aligned} 
F_{i}(\bw^{\ell}) 
&\overset{(\ref{Lipschitz-continuity})}{\leq}&&F_{i}(\bw_{i}^{\ell})+\left\langle -\nabla F_{i}(\bw ^{\ell}), \triangle \overline{\bw}^{\ell}_i\right\rangle + \frac{\sigma_i^{\ell}}{16}\left\|\triangle \overline{\bw}^{\ell}_i\right\|^2\\
&~\overset{(\ref{def-notation})}{=}&&F_{i}(\bw_{i}^{\ell})+\left\langle \triangle \bg_i^{\ell} -\bg_i^{\ell}, \triangle \overline{\bw}^{\ell}_i\right\rangle + \frac{\sigma_i^{\ell}}{16}\left\|\triangle \overline{\bw}^{\ell}_i\right\|^2\\
&~~\leq && F_{i}(\bw_{i}^{\ell})+ \left\langle \bpi_{i}^{\ell} , \triangle \overline{\bw}^{\ell}_i\right\rangle  + \sigma_i^{\ell}\left\|\triangle \overline{\bw}^{\ell}_i\right\|^2+\frac{1}{\sigma_i^{\ell}} \left\|\triangle \bg_i^{\ell}\right\|^2,
\end{aligned}
\end{eqnarray}
thereby bringing out
\begin{eqnarray*}% \label{decrease-L-F*-infty}
\begin{aligned}
\widetilde{\L}^{\ell}
&\overset{~(\ref{def-tilde-L})}{=}
&& \L^{\ell}+ \sum_{i=1}^m  \alpha_i \left(\frac{\sigma_i^{\ell}}{2 } \left\| \triangle \overline{\bw}_{i}^{\ell}\right\|^2    + \gamma_i^\ell \right) \\
&\overset{~(\ref{opt-prob-distribute-Lag})}{=}&&\sum_{i=1}^m \alpha_i \left(  F_{i}(\bw_{i}^{\ell})+\langle \bpi_{i}^{\ell}, \triangle \overline{\bw}^{\ell}_i\rangle +  \sigma_i^{\ell}  \|\triangle \overline{\bw}^{\ell}_i\|^2 + \gamma_i^\ell   \right)+ \frac{\lambda}{2}\|\bw^{\ell}\|^2  \\ 
&\overset{(\ref{fact-6-w-wi})}{\geq }&&\sum_{i=1}^m \alpha_i \left(  F_{i}(\bw^{\ell})- \frac{1}{\sigma_i^\ell}\left\|\triangle\bg_i^{\ell}\right\|^2 \right)+ \frac{\lambda}{2}\|\bw^{\ell}\|^2  \\ 
&\overset{(\ref{opt-value})}{\geq}&&    F^* -\sum_{i=1}^m  \frac{\alpha_i }{\sigma_i^\ell}\left\|\triangle\bg_i^{\ell}\right\|^2 \overset{(\ref{fact-0006})}{\geq}   F^* - \sum_{i=1}^m  \frac{\alpha_i \varepsilon_i(\overline{\delta}) }{64\sigma_i^0}\\
&\overset{~(\ref{sigma-0-vareps})}{\geq}&& F^* - \sum_{i=1}^m  \frac{\alpha_i  (1-\gamma_i) }{\gamma_i} \geq F^* -1 \overset{(\ref{opt-value})}{>}-\infty. 
\end{aligned}
\end{eqnarray*}
Since ${\bw^{\ell+1}\in\N(\overline{\delta})}$, the similar reasoning enables to show  $\widetilde{\L}^{\ell+1}\geq F^* -1 >-\infty$.
\end{proof} }}

\begin{lemma}\label{descent-lemma-gamma} Let $\gamma\in(0,1)$ and $\tau\geq \log_{\gamma}(1-\gamma)-1$. Then 
\begin{eqnarray}  \label{new-tangle-k}
\begin{aligned}
 \|\bw^k\|^2\leq \sum_{\ell=1}^k\frac{1}{\gamma^{\tau+\ell}}\|\triangle \bw^\ell\|^2.
\end{aligned}
\end{eqnarray}
\end{lemma}
\begin{proof}  By ${\tau\geq \log_{\gamma}(1-\gamma)-1}$, we have ${1-\gamma -  \gamma^{\tau+1}\geq0}$ and thus for any $\ell\geq 0$
\begin{eqnarray*}  
\begin{aligned}
 \frac{t_\ell}{\gamma^{\tau+\ell+1}}  =   \frac{1-\gamma -  {\gamma^{\tau+1}+\gamma^{\tau+\ell+1}}  }{(1-\gamma)\gamma^{\tau+\ell+1}} \geq \frac{1}{1-\gamma}>1,
\end{aligned}
\end{eqnarray*}
where  $t_\ell:=1-\gamma^{\tau+1} -\gamma^{\tau+2}-\cdots-\gamma^{\tau+\ell}$. The above condition enables the following inequalities,
\begin{eqnarray*}  
\begin{aligned}
\frac{1}{t_\ell}\| \bw^{k}-\bw^\ell\|^2  \overset{(\ref{triangle-ineq})}{\leq}  &\frac{1}{t_\ell}\left[ \frac{t_\ell}{\gamma^{\tau+\ell+1}}  \| \bw^{\ell+1}-\bw^\ell\|^2+ \left(1+\frac{1}{ \frac{t_\ell}{\gamma^{\tau+\ell+1}} -1}\right)\| \bw^{k}-\bw^{\ell+1}\|^2\right]\\
=~& \frac{1}{\gamma^{\tau+\ell+1}}  \| \bw^{\ell+1}-\bw^\ell\|^2+ \frac{1}{t_{\ell+1}}\| \bw^{k}-\bw^{\ell+1}\|^2,
\end{aligned}
\end{eqnarray*}
which allows us to derive that
\begin{eqnarray*}  
\begin{aligned}
 \|\bw^k\|^2 ~{=}~~& \|\bw^k-\bw^0\|^2=\|\triangle \bw^1+\bw^{k}-\bw^1\|^2\\
 % \overset{\eqref{triangle-ineq}}{\leq} &\frac{1}{\gamma^{\tau+1}}\|\triangle \bw^1\|^2 +  \left(1+\frac{1}{\frac{1}{\gamma^{\tau+1}}-1}\right)\|\bw^{k}-\bw^1\|^2\\
{\leq}~~&\frac{1}{\gamma^{\tau+1}}\|\triangle \bw^1\|^2 + \frac{1}{t_1}\|\bw^{k}-\bw^1\|^2\\
{\leq}~~&\frac{1}{\gamma^{\tau+1}}\|\triangle \bw^1\|^2 + \frac{1 }{\gamma^{\tau+2}}\|\triangle \bw^2\|^2 + \frac{1}{ t_2}  \|\bw^{k}-\bw^2\|^2\\
{\leq}~~&  \cdots\\
{\leq}~~& \frac{1}{\gamma^{\tau+1}}\|\triangle \bw^1\|^2 + \frac{1 }{\gamma^{\tau+2}}\|\triangle \bw^2\|^2 + \cdots+\frac{1}{t_{k-1}}  \|\triangle \bw^{k}\|^2\\
{=}~~& \frac{1}{\gamma^{\tau+1}}\|\triangle \bw^1\|^2 + \frac{1 }{\gamma^{\tau+2}}\|\triangle \bw^2\|^2 + \cdots+\frac{1-\gamma}{1-\gamma -\gamma^{\tau+1} + \gamma^{k+\tau}  }  \|\triangle \bw^{k}\|^2\\
%{\leq}~~& \frac{1}{\gamma^{\tau+1}}\|\triangle \bw^1\|^2 + \frac{1 }{\gamma^{\tau+2}}\|\triangle \bw^2\|^2 + \cdots+\frac{1-\gamma}{  \gamma^{k+\tau}  }  \|\triangle \bw^{k}\|^2\\
 {\leq}~~& \frac{1}{\gamma^{\tau+1}}\|\triangle \bw^1\|^2 + \frac{1 }{\gamma^{\tau+2}}\|\triangle \bw^2\|^2 + \cdots+\frac{1}{  \gamma^{k+\tau}  }  \|\triangle \bw^{k}\|^2\\
\end{aligned}
\end{eqnarray*}
where the last inequality results from $\tau\geq \log_{\gamma}(1-\gamma)-1$ and $\gamma\in(0,1)$. 
\end{proof}

%\begin{lemma}\label{bound-W1} Let $\{(\bw^\ell,\W^\ell,\P^\ell)\}$ be the sequence generated by Algorithm \ref{algorithm-ADMM-mini} with $\bsi^0$ chosen as (\ref{choice-of-sigma}). Then under Assumptions \ref{assumption} and \ref{assumption-varepsilon}, $\bw^1$ and $\bw_i^1\in\Omega$ for any $i\in[m]$.
%\end{lemma}
%\begin{proof}
%When $\ell=0$,  
% \begin{eqnarray*}
% \begin{aligned}
%\bw^{1}  \overset{\eqref{sub-w-mini}}{=} \frac{\sum_{i=1}^m\alpha_{i} \left(\sigma_i^0\bw_{i}^{0}+\bpi_{i}^{0}  \right) }{\sum_{i=1}^m\alpha_{i}  \sigma_i^0+\lambda}=\frac{\sum_{i=1}^m\alpha_{i} \sigma_i^0\bw^{0}}{\sum_{i=1}^m\alpha_{i}  \sigma_i^0+\lambda}=:\omega(\bw^{0}),  
%\end{aligned}
%  \end{eqnarray*}  
%  which results in
%   \begin{eqnarray*}
% \begin{aligned}
%\|\bw_{i}^{1}- \bw^{0}\| \overset{\eqref{sub-wbn-mini}}{=} &\left\|\bw^{1} - \Big(\sigma_i^{1} \I + \rho_i \Q_i^{1} \Big)^{-1}  \Big(  \bpi_{i}^{0} + \bg_i^{1} \Big)- \bw^{0}\right\|\\
%\leq~~ &\left\|\omega(\W^0,\P^0,\bsi^0)- \bw^{0} \right\|+ \left\|\Big(\sigma_i^{1} \I + \rho_i \Q_i^{1} \Big)^{-1}  \Big(  \bpi_{i}^{0} + \nabla F_{i}( \omega(\W^0,\P^0,\bsi^0); {\B}_{i}^{1}) \Big)\right\|\\
%\leq~~ &\left\|\omega(\W^0,\P^0,\bsi^0)- \bw^{0} \right\|+  \frac{1}{\sigma_i^1}\left(\left\|  \bpi_{i}^{0} \right\| + \sup_{{\B}_{i}\subseteq\D_i}\left\|\nabla F_{i}( \omega(\W^0,\P^0,\bsi^0); {\B}_{i}) \right\|\right)\\
%\leq~~ &\delta_0.
%  \end{aligned}
%  \end{eqnarray*}  
%  Therefore, $\bw_i^1\in\Omega$ for any $i\in[m]$.
%\end{proof}

\subsection{Proof of Lemma \ref{descent-lemma-L}}\label{proof-descent-lemms}
\begin{proof}  
We decompose 
\begin{eqnarray*}  
  \L^{\ell+1}- \L^\ell =G_0+G_1+G_2+G_3,
\end{eqnarray*}
where $G_0, G_1,G_2,$ and $G_3$ are defined by
\begin{eqnarray} 
\begin{aligned} 
G_0 &:=\L\left(\bw^{\ell+1},\W^{\ell+1},\P^{\ell+1};\bsi^{\ell+1}\right)- \L\left(\bw^{\ell+1},\W^{\ell+1},\P^{\ell+1};\bsi^{\ell}\right),\\
G_1 &:=\L\left(\bw^{\ell+1},\W^{\ell+1},\P^{\ell+1};\bsi^{\ell}\right)- \L\left(\bw^{\ell+1},\W^{\ell+1},\P^\ell;\bsi^{\ell}\right),\\
G_2 &:=\L\left(\bw^{\ell+1},\W^{\ell+1},\P^{\ell};\bsi^{\ell}\right)- \L\left(\bw^{\ell+1},\W^{\ell},\P^{\ell};\bsi^{\ell}\right),\\
G_3 &:= \L\left(\bw^{\ell+1},\W^{\ell},\P^{\ell};\bsi^{\ell}\right)- \L\left(\bw^\ell,\W^\ell,\P^\ell;\bsi^{\ell}\right).
%G_4 &:= \L\left(\bw^{\ell},\W^{\ell},\P^{\ell};\bsi^{\ell+1}\right)- \L\left(\bw^\ell,\W^\ell,\P^\ell;\bsi^{\ell}\right).
\end{aligned}
\end{eqnarray}
We  prove the results by induction. 

\noindent \textbf{Part 1: $\ell=0$.}  Since $\bw_i^0=\bw^0=0$ for any $i\in[m]$, we have that
\begin{eqnarray*} 
\begin{aligned} 
\bw^0\in\N(\delta ),~~\bw_i^0\in \N(\delta), ~\forall~i\in[m].
\end{aligned}
\end{eqnarray*}
 For $G_0$, it follows that
\begin{eqnarray*} 
\begin{aligned} 
G_0 =\sum_{i=1}^m  \frac{\alpha_i(\sigma_i^{1}-\sigma_i^{0})}{2} \| \triangle \overline{\bw}^{1}_i\|^2 \overset{(\ref{sub-sigma-mini},\ref{sub-pin})}{=}\sum_{i=1}^m  \frac{\alpha_i(1-\gamma_i)}{2\sigma_i^{1}} \|\triangle \bpi_{i}^{1}\|^2 \overset{(\ref{range-gamma})}{\leq }\sum_{i=1}^m  \frac{\alpha_i}{8\sigma_i^{1}} \|\triangle \bpi_{i}^{1}\|^2.
\end{aligned}
\end{eqnarray*}
For $G_1$, we have 
\begin{eqnarray*} 
\begin{aligned} 
G_1 %&=\L\left(\bw^{\ell+1},\W^{\ell+1},\P^{\ell+1}\right)- \L\left(\bw^{\ell+1},\W^{\ell+1},\P^\ell\right)\\ 
 = \sum_{i=1}^m \alpha_i  \langle \bpi_{i}^{1}- \bpi_{i}^{0}, \bw_{i}^{1}- \bw^{1}\rangle   \overset{(\ref{sub-pin})}{=} \sum_{i=1}^m   \frac{\alpha_i}{\sigma_i^{1}}\left\|\triangle \bpi_{i}^{1} \right\|^2. 
\end{aligned}
\end{eqnarray*}
For $G_2$, we need to claim several facts. The first one is derived as below,
\begin{eqnarray} \label{fact-1}
\begin{aligned}  
\frac{\sigma_i^{0}}{2} \left\| \bw_{i}^{1}- \bw^{1}\right\|^2 -  \frac{\sigma_i^{0}}{2} \left\| \bw_{i}^{0}- \bw^{1}\right\|^2
~= ~ &{\sigma_i^{0}}  \left\langle \bw_{i}^{1}- \bw^{1}, \triangle \bw_{i}^{1}\right\rangle   - \frac{\sigma_i^{0}}{2} \left\|\triangle \bw_{i}^{1}\right\|^2 \\
\overset{\eqref{sub-sigma-mini}}{=}  &{\sigma_i^{0}}  \left\langle \triangle\overline{\bw}^{1}_i, \triangle \bw_{i}^{1}\right\rangle   - \frac{\gamma_i\sigma_i^{1}}{2} \left\|\triangle \bw_{i}^{1}\right\|^2\\
\overset{\eqref{range-gamma}}{\leq}  &{\sigma_i^{0}}  \left\langle \triangle\overline{\bw}^{1}_i, \triangle \bw_{i}^{1}\right\rangle   - \frac{ 3\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2. 
\end{aligned}
\end{eqnarray}
Since $\bw^0\in \N(\delta)$, we can verify that  
\begin{eqnarray*} \label{fact-2}
\begin{aligned} 
&&& \left\langle \nabla F_{i}(\bw^{0})-\nabla F_{i}(\bw^{0};\B^{1}_i) +\nabla F_{i}(\bw^{0};\B^{1}_i)-\bg_i^{1}, \triangle \bw_{i}^{1}\right\rangle \\
&~\overset{\eqref{triangle-ineq}}{\leq}  &&  \frac{\sigma_i^{1}}{64} \left\|\triangle \bw_{i}^{1}\right\|^2+\frac{64}{4\sigma_i^{ 1}} \left\|  \nabla F_{i}(\bw^{0})-\nabla F_{i}(\bw^{0};\B^{1}_i)  \right\|^2  \\
&~~~{+}&& \frac{   \sigma_i^{1}}{64} \left\|\triangle \bw_{i}^{1}\right\|^2  + \frac{64}{4\sigma_i^{ 1}}\left\| \nabla F_{i}(\bw^{0};\B^{1}_i)-\bg_i^{1}\right\|^2\\
&~\overset{(\ref{def-eps})}{\leq}&&   \frac{\sigma_i^{ 1}}{32} \left\|\triangle \bw_{i}^{1}\right\|^2 + \frac{\varepsilon_i(\delta )}{4\sigma_i^{ 1}}   +\frac{64}{4\sigma_i^{ 1}} \left\| \nabla F_{i}(\bw^{0};\B^{1}_i)-\bg_i^{1}\right\|^2\\
&\overset{(\ref{Lipschitz-continuity})}{\leq} &&  \frac{\sigma_i^{ 1}}{32} \left\|\triangle \bw_{i}^{1}\right\|^2 + \frac{ \varepsilon_i(\delta )}{4\sigma_i^{ 1}}   +\frac{ (\sigma_i^{0})^2}{4\sigma_i^{ 1}} \left\| \triangle\bw^{1}\right\|^2\\
&\overset{~(\ref{sub-sigma-mini})}{\leq} &&  \frac{ \sigma_i^{ 1}}{32} \left\|\triangle \bw_{i}^{1}\right\|^2 + \frac{\varepsilon_i(\delta )}{ 4\sigma_i^{ 1}}  +\frac{ \sigma_i^{0}}{4} \left\| \triangle\bw^{1}\right\|^2,
\end{aligned}
\end{eqnarray*}
and the following condition,
\begin{eqnarray*} 
\begin{aligned} 
 \left\langle \nabla F_{i}(\bw_{i}^{0})-\nabla F_{i}(\bw^{0}), \triangle \bw_{i}^{1}\right\rangle 
 \overset{\eqref{triangle-ineq}}{\leq}~~&\frac{\sigma_i^{1}}{32}\left\|\triangle \bw_{i}^{1}\right\|^2+\frac{64}{8\sigma_i^{1}}\left\|\nabla F_{i}(\bw_{i}^{0})-\nabla F_{i}(\bw^{0})\right\|^2 \\
\overset{\eqref{Lipschitz-continuity}}{\leq} ~& \frac{\sigma_i^{1}}{32}\left\|\triangle \bw_{i}^{1}\right\|^2 + \frac{(\sigma_i^{0})^2}{8\sigma_i^{1}}\left\| \bw_{i}^{0}-\bw^{0}\right\|^2\\
\overset{\eqref{sub-pin}}{=}  ~&   \frac{\sigma_i^{1}}{32}\left\|\triangle \bw_{i}^{1}\right\|^2  +\frac{ (\sigma_i^{0})^2}{8(\sigma_i^{1})^3 } \left\| \triangle \bpi_{i}^{0}\right\|^2\\
\overset{\eqref{sub-sigma-mini}}{\leq}  ~&  \frac{\sigma_i^{1}}{32}\left\|\triangle \bw_{i}^{1}\right\|^2  +\frac{1}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{0}\right\|^2.
\end{aligned}
\end{eqnarray*}
The above two conditions result in
\begin{eqnarray} \label{fact-02}
\begin{aligned} 
 \left\langle   \nabla F_{i}(\bw_{i}^{0})-\bg_i^{1}, \triangle \bw_{i}^{1}\right\rangle &= \left\langle \nabla F_{i}(\bw_{i}^{0})-\nabla F_{i}(\bw^{0}), \triangle \bw_{i}^{1}\right\rangle\\[1.5ex]
 &+\left\langle \nabla F_{i}(\bw^{0})-\nabla F_{i}(\bw^{0};\B^{1}_i) +\nabla F_{i}(\bw^{0};\B^{1}_i)-\bg_i^{1}, \triangle \bw_{i}^{1}\right\rangle \\
 &\leq \frac{\sigma_i^{1}}{16}\left\|\triangle \bw_{i}^{1}\right\|^2 +\frac{ \varepsilon_i(\delta )}{4\sigma_i^{ 1}}  +\frac{ \sigma_i^{0} }{4} \left\| \triangle\bw^{1}\right\|^2+\frac{1}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{0}\right\|^2,
\end{aligned}
\end{eqnarray}
which further contributes to the second fact,
\begin{eqnarray} \label{fact-222}
\begin{aligned} 
F_{i}(\bw_{i}^{1}) - F_{i}(\bw_{i}^{0})
\overset{(\ref{Lipschitz-continuity})}{\leq }  & \left\langle \nabla F_{i}(\bw_{i}^{0}), \triangle \bw_{i}^{1} \right\rangle+ \frac{\sigma_i^{1}}{16}\left\|\triangle \bw_{i}^{1}\right\|^2\\ 
=~& \left\langle \nabla F_{i}(\bw_{i}^{0})-\bg_i^{1}, \triangle \bw_{i}^{1} \right\rangle +\left\langle  \bg_i^{1}, \triangle \bw_{i}^{1} \right\rangle + \frac{\sigma_i^{1}}{16}\left\|\triangle \bw_{i}^{1}\right\|^2 \\ 
%\overset{(\ref{range-gamma})~}{\leq }  &\left\langle \nabla F_{i}(\bw_{i}^{0})-\bg_i^{1}+\bg_i^{1}, \triangle \bw_{i}^{1} \right\rangle+ \frac{ \sigma_i^{1}}{16}\left\|\triangle \bw_{i}^{1}\right\|^2\\ 
\overset{(\ref{fact-02})}{\leq } &  \left\langle \bg_i^{1}, \triangle \bw_{i}^{1} \right\rangle+\frac{\sigma_i^{1}}{8}\left\|\triangle \bw_{i}^{1}\right\|^2 +\frac{\varepsilon_i(\delta )}{4\sigma_i^{ 1}}  +\frac{ \sigma_i^{0} }{4} \left\| \triangle\bw^{1}\right\|^2+\frac{1}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{0}\right\|^2 .
\end{aligned}
\end{eqnarray} 
%By letting $\textbf{P}_i^{\ell+1}:=(\sigma_i^{\ell+1}-\sigma_i^{\ell})\I+ \rho_i \Q_i^{\ell+1}$, it follows 
%\begin{eqnarray} \label{P-upper-bd}
%\begin{aligned} \|\textbf{P}_i^{\ell+1}\| \overset{\eqref{Q-upper-bd}}{\leq}  \sigma_i^{\ell+1}-\sigma_i^{\ell}+\rho_i\eta_i\overset{\eqref{increase-sigma}}{\leq} \sigma_i^{\ell+1}- \frac{7\sigma_i^{\ell}}{8} \overset{\eqref{sub-sigma-mini}}{\leq} \sigma_i^{\ell+1}- \frac{7\gamma_i\sigma_i^{\ell+1}}{8} \overset{\eqref{range-gamma}}{\leq} \frac{11\sigma_i^{\ell+1}}{32},
%\end{aligned}
%\end{eqnarray} 
% thereby resulting in 
%\begin{eqnarray} \label{fact-22}
%\begin{aligned} 
% -   \left \langle \textbf{P}_i^{\ell+1} \triangle\overline{\bw}^{\ell+1}_i, \triangle \bw_{i}^{\ell+1}\right\rangle %&= -  \frac{ \rho_i}{\sigma_i^{0}}\left \langle  \Q_i^{1} \triangle\bpi^{1}_i, \triangle \bw_{i}^{1}\right\rangle\\
%%~= ~~&- \left \langle  \textbf{P}_i^{1} \triangle\overline{\bw}^{1}_i,  \triangle \bw_{i}^{1}\right\rangle\\
%\overset{~\eqref{triangle-ineq}}{\leq} ~&  \frac{\sigma_i^{\ell+1}}{8}  \left\| \triangle \bw_{i}^{\ell+1}\right\|^2+ \frac{2}{ \sigma_i^{\ell+1}}\left\|  \textbf{P}_i^{\ell+1} \triangle\overline{\bw}^{\ell+1}_i\right\|^2\\
%\overset{\eqref{P-upper-bd}}{\leq} ~&  \frac{\sigma_i^{\ell+1}}{8}  \left\| \triangle \bw_{i}^{\ell+1}\right\|^2+\frac{242\sigma_i^{\ell+1}}{1024}\left\|  \triangle\overline{\bw}^{\ell+1}_i\right\|^2\\
%\overset{\eqref{sub-pin}}{\leq} ~&  \frac{\sigma_i^{\ell+1}}{8}  \left\|\triangle \bw_{i}^{\ell+1}\right\|^2+\frac{1}{4\sigma_i^{\ell+1}}\left\|\triangle\bpi^{\ell+1}_i\right\|^2.
%% {=} ~~~&  \frac{ \sigma_i^{\ell+1}}{16} \left\|\triangle \bw_{i}^{\ell+1}\right\|^2+\frac{4}{  \sigma_i^{\ell+1} } \left\|\triangle\bpi^{\ell+1}_i\right\|^2.
%\end{aligned}
%\end{eqnarray}
The third fact is obtained as follows,
\begin{eqnarray} \label{fact-22}
\begin{aligned} 
 -   \left \langle \textbf{P}_i^{1} \triangle\overline{\bw}^{1}_i, \triangle \bw_{i}^{1}\right\rangle 
\overset{~\eqref{triangle-ineq}}{\leq} ~&  \frac{\sigma_i^{1}}{8}  \left\| \triangle \bw_{i}^{1}\right\|^2+ \frac{2}{ \sigma_i^{1}}\left\|  \textbf{P}_i^{1} \triangle\overline{\bw}^{1}_i\right\|^2\\
\overset{\eqref{P-upper-bd}}{\leq} ~&  \frac{\sigma_i^{1}}{8}  \left\| \triangle \bw_{i}^{1}\right\|^2+\frac{242\sigma_i^{1}}{1024}\left\|  \triangle\overline{\bw}^{1}_i\right\|^2\\
\overset{\eqref{sub-pin}}{\leq} ~&  \frac{\sigma_i^{1}}{8}  \left\|\triangle \bw_{i}^{1}\right\|^2+\frac{1}{4\sigma_i^{1}}\left\|\triangle\bpi^{1}_i\right\|^2.
\end{aligned}
\end{eqnarray}
The above facts enable us to derive that
\begin{eqnarray*} 
\begin{aligned} 
p_i~  
{:=}~~~& \left[  F_{i}(\bw_{i}^{1})+\left\langle \bpi_{i}^{0}, \bw_{i}^{1}- \bw^{1}\right\rangle + \frac{\sigma_i^{0}}{2} \left\| \bw_{i}^{1}- \bw^{1}\right\|^2  \right]   \\  
{-}~~~&\left[  F_{i}(\bw_{i}^{0})+\left\langle \bpi_{i}^{0}, \bw_{i}^{0}- \bw^{1}\right\rangle + \frac{\sigma_i^{0}}{2} \left\| \bw_{i}^{0}- \bw^{1}\right\|^2 \right]  \\  
\overset{(\ref{fact-1})}{= } ~&
  F_{i}(\bw_{i}^{1}) - F_{i}(\bw_{i}^{0})   +   \left\langle  \bpi_{i}^{0} +\sigma_i^{0}  \triangle\overline{\bw}^{1}_i, \triangle \bw_{i}^{1}\right\rangle -  \frac{ 3 \sigma_i^{1}}{8}\left\|\triangle \bw_{i}^{1}\right\|^2\\ 
 \overset{(\ref{fact-222})}{\leq } ~&
\frac{\varepsilon_i(\delta )}{4\sigma_i^{ 1}} +  \frac{1}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{0}\right\|^2+\frac{ \sigma_i^{0} }{4} \left\| \triangle\bw^{1}\right\|^2 -  \frac{\sigma_i^{1}}{4} \left\|\triangle \bw_{i}^{1}\right\|^2  + \left\langle \bg_i^{1}+\bpi_{i}^{0}+\sigma_i^{0}  \triangle\overline{\bw}^{1}_i, \triangle \bw_{i}^{1}\right\rangle \\ 
\overset{(\ref{optimality-condition})~}{=} ~& \frac{\varepsilon_i(\delta )}{4\sigma_i^{ 1}}  +  \frac{1}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{0}\right\|^2+\frac{ \sigma_i^{0} }{4} \left\| \triangle\bw^{1}\right\|^2-  \frac{\sigma_i^{1}}{4} \left\|\triangle \bw_{i}^{1}\right\|^2  -  \left\langle  \textbf{P}_i^{1} \triangle\overline{\bw}^{1}_i, \triangle \bw_{i}^{1}\right\rangle   \\ 
\overset{(\ref{fact-22})}{\leq } ~&\frac{\varepsilon_i(\delta )}{ 4\sigma_i^{ 1}}  +  \frac{1}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{0}\right\|^2+\frac{ \sigma_i^{0} }{4} \left\| \triangle\bw^{1}\right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2 +\frac{1}{4\sigma_i^{1} } \left\| \triangle \bpi_{i}^{1}\right\|^2\\
\overset{(\ref{increase-sigma})~}{\leq } ~&\frac{\varepsilon_i(\delta )}{ 4\sigma_i^{ 1}}   +  \frac{1}{8\sigma_i^{0} } \left\| \triangle \bpi_{i}^{0}\right\|^2 - \frac{1}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{1}\right\|^2 +\frac{ \sigma_i^{0} }{4} \left\| \triangle\bw^{1}\right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2 +\frac{3}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{1}\right\|^2\\
\overset{(\ref{def-varphi})}{=} ~&\frac{\varepsilon_i(\delta )}{4\sigma_i^{ 1}}  +  \frac{\varphi_i^1}{4 }   +\frac{ \sigma_i^{0} }{4} \left\| \triangle\bw^{1}\right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2 +\frac{3}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{1}\right\|^2,
%\overset{(\ref{range-gamma})}{\leq}~ & \frac{ b \varepsilon_i}{ \sigma_i^{ 1}} +  \frac{5-4\gamma_i+\gamma_i^2}{2\sigma_i^{1} } \left\| \triangle \bpi_{i}^{1}\right\|^2- \frac{ \sigma_i^{1}}{10} \left\|\triangle \bw_{i}^{1}\right\|^2  ,
\end{aligned}
\end{eqnarray*}
which immediately leads to 
\begin{eqnarray*} 
\begin{aligned} 
G_2  
= \sum_{i=1}^m \alpha_i p_i  
\leq   \sum_{i=1}^m \alpha_i \left[\frac{\varepsilon_i(\delta )}{4\sigma_i^{ 1}}  +  \frac{\varphi_i^1}{4 }   +\frac{ \sigma_i^{0} }{4} \left\| \triangle\bw^{1}\right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2 +\frac{3}{8\sigma_i^{1} } \left\| \triangle \bpi_{i}^{1}\right\|^2 \right].
\end{aligned}
\end{eqnarray*}
For $G_3$, we first have two facts 
\begin{eqnarray*} 
\begin{aligned} 
  \left\| \bw_{i}^{0}- \bw^{1}\right\|^2 -  \left\| \bw_{i}^{0}- \bw^{0}\right\|^2  %&= \| \bw_{i}^{0}- \bw^{1}\|^2 -  \| \bw_{i}^{0}-\bw^{1}+\bw^{1}- \bw^{0}\|^2 \\
  &=  2 \left\langle \bw^{1}-\bw_{i}^{0}, \triangle\bw^{1} \right\rangle -  \left\|  \triangle\bw^{1} \right\|^2, \\
 \left\|\bw^{1}\right\|^2-   \left\|\bw^{0}\right\|^2 %&= \|\bw^{1}\|^2-   \|\bw^{0}-\bw^{1}+\bw^{1}\|^2 \\
&=   2\left\langle \bw^{1}, \triangle\bw^{1} \right\rangle -  \left\|  \triangle\bw^{1} \right\|^2,
\end{aligned}
\end{eqnarray*}
which lead to 
\begin{eqnarray*} 
\begin{aligned} 
G_3  &~~=~\sum_{i=1}^m \alpha_i \left[ \left\langle \bpi_{i}^{0},  - \triangle \bw^{1}\right\rangle + \frac{\sigma_i^{0}}{2} \left\| \bw_{i}^{0}- \bw^{1}\right\|^2 -\frac{\sigma_i^{0}}{2} \left\| \bw_{i}^{0}- \bw^{0}\right\|^2 \right] +  \frac{\lambda}{2}\left\|\bw^{1}\right\|^2-  \frac{\lambda}{2}\left\|\bw^{0}\right\|^2 \\
%&~~=~\sum_{i=1}^m \alpha_i \left[ \left\langle -\bpi_{i}^{0}+\sigma_i^{0}( \bw^{1}-\bw_{i}^{0}),   \triangle \bw^{1}\right\rangle  +  \lambda \left\langle \bw^{1}, \triangle\bw^{1} \right\rangle -  \frac{\sigma_i^{0}+\lambda}{2}\left\|  \triangle\bw^{1} \right\|^2 \right]\\
&~~=~\sum_{i=1}^m \alpha_i \left[ \left\langle -\bpi_{i}^{0}+\sigma_i^{0}( \bw^{1}-\bw_{i}^{0}) + \lambda \bw^{1},   \triangle \bw^{1}\right\rangle    -  \frac{\sigma_i^{0}+\lambda}{2}\left\|  \triangle\bw^{1} \right\|^2 \right] \\
&\overset{(\ref{optimality-condition-w})}{=}    \sum_{i=1}^m\alpha_i \left[- \frac{\sigma_i^{0}+\lambda}{2}\left \|  \triangle\bw^{1} \right\|^2\right].
\end{aligned}
\end{eqnarray*}
{{Overall, the upper bounds of $G_0, G_1, G_2,$ and $G_3$ yield that 
\begin{equation}\label{gap-LL-sum}
\begin{aligned} 
 \L^{1}- \L^0 
\leq~&\sum_{i=1}^m \alpha_i \left[\frac{\varepsilon_i(\delta )}{4\sigma_i^{ 1}}   +  \frac{ \varphi_i^1}{4}   -  \frac{\sigma_i^{0}+2\lambda}{4}\left\|  \triangle\bw^{1} \right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2 +\frac{3}{2\sigma_i^{1} } \left\| \triangle \bpi_{i}^{1}\right\|^2\right]\\
\overset{(\ref{sub-sigma-mini})}{\leq}&\sum_{i=1}^m \alpha_i \left[\frac{  \varepsilon_i(\delta )}{4\sigma_i^{ 1}}   +  \frac{ \varphi_i^1}{4}   -  \frac{3\sigma_i^{1}+8\lambda}{16}\left\|  \triangle\bw^{1} \right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2 +\frac{3}{2\sigma_i^{1} } \left\| \triangle \bpi_{i}^{1}\right\|^2\right]\\
\overset{(\ref{gap-pi-k})}{\leq} & \sum_{i=1}^m \alpha_i \left[\frac{  12\varepsilon_i(\delta )}{\sigma_i^{ 1}}  +   \varphi_i^1     -  \frac{\sigma_i^{1}+4\lambda}{8}\left\|  \triangle\bw^{1} \right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2  \right]\\
\overset{(\ref{upbd-sigma-ell-1})}{\leq} & \sum_{i=1}^m \alpha_i \left[ \gamma_i^{0}-\gamma_i^{1}   +   \varphi_i^1     -  \frac{\sigma_i^{1}+4\lambda}{8}\left\|  \triangle\bw^{1} \right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2  \right],
\end{aligned}
\end{equation}  
%Additionally, since $\bw^0\in\N(\bw^0,\delta )$, it follows
%$$  \frac{1}{2\sigma_i^0}\left\|\triangle\bg_i^{0}\right\|^2\overset{\eqref{def-eps}}{\leq} \frac{32\varepsilon_i(\delta )}{\sigma_i^0},$$
%which by \eqref{gap-LL-sum} implies that
%\begin{equation*}%\label{gap-LL-sum}
%\begin{aligned} 
% \L^{1}- \L^0 
% \leq  & \sum_{i=1}^m \alpha_i \left[\frac{48\varepsilon_i(\delta )}{ \sigma_i^{ 1}} -  \frac{1}{2\sigma_i^0}\left\|\triangle\bg_i^{\ell}\right\|^2 +   \varphi_i^1     -  \frac{\sigma_i^{1}+4\lambda}{8}\left\|  \triangle\bw^{1} \right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2  \right]\\
% \leq  & \sum_{i=1}^m \alpha_i \left[\frac{48\varepsilon_i(\delta )}{ \sigma_i^{ 1}} -  \frac{1}{2\sigma_i^0}\left\|\triangle\bg_i^{0}\right\|^2  +  \frac{1}{2\sigma_i^1}\left\|\triangle\bg_i^1\right\|^2 +   \varphi_i^1     -  \frac{\sigma_i^{1}+4\lambda}{8}\left\|  \triangle\bw^{1} \right\|^2-  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2  \right].
%\end{aligned}
%\end{equation*}  
% The above condition and 
%  \begin{eqnarray*} 
%\begin{aligned}
%\frac{16\varepsilon_i(\delta )}{\sigma_i^{1}} =  \frac{16\gamma_i\varepsilon_i(\delta )}{1- \gamma_i} \left( \frac{1}{ {\sigma_i^{0}}} -\frac{1}{ {\sigma_i^{1}}} \right)\overset{\eqref{sigma-0-vareps}}{\leq}   \sigma_i^{0} \left( \frac{1}{ {\sigma_i^{0}}} -\frac{1}{ {\sigma_i^{1}}} \right)=\gamma_i^0-\gamma_i^1
%\end{aligned}
%\end{eqnarray*}
sufficing to the following condition, i.e., condition \eqref{descent-L} with $\ell=0$, 
\begin{equation} \label{decreasing-ell-0}
\begin{aligned} 
 &\sum_{i=1}^m \alpha_i \left[   \frac{\sigma_i^{1}+4\lambda}{8}\left\|  \triangle\bw^{1} \right\|^2+  \frac{\sigma_i^{1}}{8} \left\|\triangle \bw_{i}^{1}\right\|^2 \right] \leq \widetilde{\L}^{0}- \widetilde{\L}^{1}.
% \leq &\left[\L^0+\sum_{i=1}^m  \alpha_i \left(\frac{\sigma_i^{0} }{2} \left\| \triangle \overline{\bw}_{i}^{0}\right\|^2   + \gamma_i^0 \right)\right] - \left[\L^1+\sum_{i=1}^m  \alpha_i \left(\frac{\sigma_i^{1}}{2 } \left\| \triangle \overline{\bw}_{i}^{1}\right\|^2   + \gamma_i^1\right)\right].
\end{aligned}
\end{equation} 
%Since $\bw^0,~\bw_i^0\in\N(\bw^0,\delta )$ for any $\forall~i\in[m],$  Lemma \ref{descent-lemma} results in 
%\begin{eqnarray*} 
%\begin{aligned} 
%\widetilde{\L}^1\geq L^*.
%\end{aligned}
%\end{eqnarray*}
\noindent \textbf{Part 2.} Suppose the conclusion holds for any $\ell=0,1,2,\ldots, k-1$, namely, 
\begin{align} \label{w-wi-in-N}
&\bw^{\ell}\in\N(\delta ),~~\bw_i^{\ell}\in\N(\delta ),~\forall~i\in[m]\\[1ex]
\label{ell-k-1}
 &\sum_{i=1}^m \alpha_i \left[   \frac{\sigma_i^{\ell+1}+4\lambda}{8}\left\|  \triangle\bw^{\ell+1} \right\|^2 + \frac{ \sigma_i^{\ell+1}}{8} \left\|\triangle \bw_{i}^{\ell+1}\right\|^2 \right] \leq \widetilde{\L}^{\ell}- \widetilde{\L}^{\ell+1}.
% = &\left[\L^{\ell}+ \sum_{i=1}^m  \alpha_i \left(\frac{\sigma_i^{\ell}}{2 } \left\| \triangle \overline{\bw}_{i}^{\ell}\right\|^2 + \frac{1}{2\sigma_i^\ell}\left\|\triangle\bg_i^{\ell}\right\|^2 + \frac{48\gamma_i\varepsilon_i}{(1- \gamma_i)\sigma_i^{\ell}} \right) \right]\\
%  -& \left[\L^{\ell+1}+ \sum_{i=1}^m  \alpha_i \left(\frac{\sigma_i^{\ell+1}}{2 } \left\| \triangle \overline{\bw}_{i}^{\ell+1}\right\|^2 + \frac{1}{2\sigma_i^{\ell+1}}\left\|\triangle\bg_i^{\ell+1}\right\|^2 + \frac{48\gamma_i\varepsilon_i}{(1- \gamma_i)\sigma_i^{\ell+1}} \right) \right],
\end{align}
Next, we prove the conclusion for $\ell=k$. Firstly, by Lemma \ref{descent-lemma}  and \eqref{w-wi-in-N}, we obtain 
\begin{eqnarray*} 
\begin{aligned} 
\widetilde{\L}^{k}\geq F^*-1.
\end{aligned}
\end{eqnarray*}
The initialization in Algorithm \ref{algorithm-ADMM-mini} implies $ \triangle \overline{\bw}_{i}^{0} =0$, thereby
\begin{equation*} 
\begin{aligned} 
\widetilde{\L}^0  = {\L}^0 + \sum_{i=1}^m \alpha_i\gamma_i^0 = F(\bw^0)+ 1.
\end{aligned}
\end{equation*} 
The above two conditions give rise to
\begin{equation} \label{L-0-F0}
\begin{aligned} 
 \widetilde{\L}^0-\widetilde{\L}^{k} 
 \leq  F(\bw^0)+ 1 -F^*+1 \overset{(\ref{def-eps})}{=} \triangle F   + 2. 
\end{aligned}
\end{equation} 
Direct verification enables the following chain of inequalities,
\begin{equation*} %\label{ell-k-11}
\begin{aligned} 
%&\sum_{\ell=0}^{k-1}\max_{i\in[m]}\left\{  \frac{\alpha_i \sigma_i^{0}}{8\gamma_i^{\ell+1}} \left\|  \triangle\bw^{\ell+1} \right\|^2,  \frac{\alpha_i \sigma_i^{0}}{8\gamma_i^{\ell+1}}\left\|\triangle \bw_{i}^{\ell+1}\right\|^2 \right\}\\
& \sum_{\ell=0}^{k-1}\sum_{i=1}^m \frac{\alpha_i \sigma_i^{0}}{8\gamma_i^{\ell+1}} \left[   \left\|  \triangle\bw^{\ell+1} \right\|^2+  \left\|\triangle \bw_{i}^{\ell+1}\right\|^2 \right]\\
\overset{\eqref{sub-sigma-mini}}{=}&\sum_{\ell=0}^{k-1}\sum_{i=1}^m \frac{\alpha_i \sigma_i^{\ell+1}}{8} \left[   \left\|  \triangle\bw^{\ell+1} \right\|^2+  \left\|\triangle \bw_{i}^{\ell+1}\right\|^2 \right]\\
 {\leq}~~&\sum_{\ell=0}^{k-1}\sum_{i=1}^m \alpha_i \left[   \frac{\sigma_i^{\ell+1}+4\lambda}{8}\left\|  \triangle\bw^{\ell+1} \right\|^2+  \frac{\sigma_i^{\ell+1}}{8} \left\|\triangle \bw_{i}^{\ell+1}\right\|^2 \right]\\
\overset{\eqref{ell-k-1}}{\leq}&\sum_{\ell=0}^{k-1} \left[ \widetilde{\L}^{\ell}- \widetilde{\L}^{\ell+1} \right] = \widetilde{\L}^0-\widetilde{\L}^{k} 
 \overset{(\ref{L-0-F0})}{\leq}  \triangle F   + 2, 
\end{aligned}
\end{equation*} 
  which contributes to
  \begin{equation*}% \label{ell-k-11}
\begin{aligned} 
\left\|  \bw^{k}\right\|^2 \overset{(\ref{new-tangle-k})}{\leq} \sum_{\ell=0}^{k-1} \frac{1}{\gamma_i^{{\tau_i}+\ell+1}}  \left\|  \triangle\bw^{\ell+1} \right\|^2 {\leq}~& \frac{8(\triangle F +2)}{\alpha_i \sigma_i^{0}\gamma_i^{\tau_i}}\overset{(\ref{def-eps})}{\leq}\delta.
%\left\|  \bw^{k}_i  \right\|^2 \overset{(\ref{new-tangle-k})}{\leq} \sum_{\ell=0}^{k-1} \frac{1}{\gamma_i^{{\tau_i}+\ell+1}}  \left\|  \triangle\bw^{\ell+1}_i \right\|^2 {\leq}~& \frac{8(\triangle F + 2)}{\alpha_i \sigma_i^{0}\gamma_i^{\tau_i}}\overset{(\ref{def-eps})}{\leq}\delta . 
\end{aligned}
\end{equation*} 
Similarly, $\left\|  \bw^{k}_i  \right\|^2\leq \delta$.  Therefore, we show that
 \begin{eqnarray*} %\label{w-wi-in-N}
\begin{aligned} 
\bw^{k}\in\N(\delta ),~~\bw_i^{k}\in\N(\delta ), ~\forall~i\in[m],
\end{aligned}
\end{eqnarray*}
which by the similar reasoning to show \eqref{decreasing-ell-0} enables showing \eqref{ell-k-1} for $\ell=k$. }}
\end{proof}

 

 
\subsection{Proof of Theorem \ref{main-convergence}}
\begin{proof}  {{1) By \eqref{descent-L} and \eqref{L-lower-bd}, sequence $\{\widetilde{\L}^\ell\}$ is non-increasing and lower bounded, so it  converges. Taking the limit of both sides of \eqref{descent-L} yields  $\lim_{\ell\to\infty}\sigma_i^{\ell}\left\| \triangle  {\bw}^{\ell}\right\|=\lim_{\ell\to\infty}\sigma_i^{\ell}\left\| \triangle  {\bw}^{\ell}_i\right\|=0$, thereby $\lim_{\ell\to\infty} \left\| \triangle  {\bw}^{\ell}\right\|=\lim_{\ell\to\infty} \left\| \triangle  {\bw}^{\ell}_i\right\|=0$, and 
 \begin{eqnarray*} 
\begin{aligned}
\lim_{\ell\to\infty}\sigma_i^{\ell}\left\|\triangle\overline{\bw}^{\ell+1}_i  \right\| %&= \left\|  {\bw}^{\ell+1}_i -{\bw}^{\ell}_i+{\bw}^{\ell}_i -{\bw}^{\ell}+{\bw}^{\ell}-{\bw}^{\ell+1} \right\|\\
= \lim_{\ell\to\infty}\left\| \sigma_i^{\ell}\triangle {\bw}^{\ell+1}_i  +\sigma_i^{\ell}\triangle\overline{\bw}^{\ell}_i-\sigma_i^{\ell}\triangle {\bw}^{\ell+1} \right\| =\lim_{\ell\to\infty} \sigma_i^{\ell}\left\| \triangle\overline{\bw}^{\ell}_i\right\|.
\end{aligned}
\end{eqnarray*}
Since $\bw^{k},\bw_i^{k}\in\N(\delta )$ for any $i\in[m]$ and $\ell\geq0$ from Lemma \ref{descent-lemma-L}, it holds
 \begin{eqnarray*} 
\begin{aligned}
\sigma_i^{\ell+1}\left\| \triangle \overline{\bw}^{\ell+1}_i\right\|^2 &\overset{\eqref{sub-pin}}{=}&& \frac{1}{\sigma_i^{\ell+1}} \left\|\triangle \bpi_{i}^{\ell+1}\right\|^2 \\
&\overset{\eqref{gap-pi-k}}{\leq} &&  \frac{15\varepsilon_i(\delta)}{2\sigma_i^{\ell+1}} +  \frac{\sigma_i^{\ell+1}}{32} \left\| \triangle \bw^{\ell+1} \right\|^2+ \frac{\varphi_i^{\ell+1}}{4}\\  
       &\overset{\eqref{def-varphi}}{=} && 
       \frac{15\varepsilon_i(\delta)}{2\sigma_i^{\ell+1}} +  \frac{ \sigma_i^{\ell+1}}{32} \left\| \triangle \bw^{\ell+1} \right\|^2+ \frac{ \sigma_i^{\ell}}{8}\left\|\triangle\overline{\bw}^{\ell}_i\right\|^2 -   \frac{ \sigma_i^{\ell+1}}{8} \left\|\triangle\overline{\bw}^{\ell+1}_i \right\|^2\\
            &\overset{~\eqref{upbd-sigma-ell-1}}{\leq} && 
       \frac{\gamma_i^{\ell} }{2} -    \frac{ \gamma_i^{\ell+1}}{2} +  \frac{ \sigma_i^{\ell+1}}{32} \left\| \triangle \bw^{\ell+1} \right\|^2+ \frac{ \sigma_i^{\ell}}{8}\left\|\triangle\overline{\bw}^{\ell}_i\right\|^2 -   \frac{ \sigma_i^{\ell+1}}{8} \left\|\triangle\overline{\bw}^{\ell+1}_i \right\|^2,   
\end{aligned}
\end{eqnarray*}
which leads to
 \begin{eqnarray*} 
\begin{aligned}
\frac{9\sigma_i^{\ell+1}}{8}\left\| \triangle \overline{\bw}^{\ell+1}_i\right\|^2 \leq 
       \frac{\gamma_i^{\ell} }{2} -    \frac{ \gamma_i^{\ell+1}}{2}  +  \frac{ \sigma_i^{\ell+1}}{32} \left\| \triangle \bw^{\ell+1} \right\|^2+ \frac{ \sigma_i^{\ell}}{8}\left\|\triangle\overline{\bw}^{\ell}_i\right\|^2.  
\end{aligned}
\end{eqnarray*}
Now taking the limit of both sides leads to
 \begin{eqnarray*} 
\begin{aligned}
\lim_{\ell\to\infty}\frac{9\sigma_i^{\ell+1}}{8}\left\| \triangle \overline{\bw}^{\ell}_i\right\|^2 \leq \lim_{\ell\to\infty} \frac{ \sigma_i^{\ell}}{8}\left\|\triangle\overline{\bw}^{\ell}_i\right\|^2=\lim_{\ell\to\infty} \frac{ \sigma_i^{\ell}}{8}\left\|\triangle\overline{\bw}^{\ell+1}_i\right\|^2\leq\lim_{\ell\to\infty} \frac{  \sigma_i^{\ell+1}}{8}\left\|\triangle\overline{\bw}^{\ell+1}_i\right\|^2, 
\end{aligned}
\end{eqnarray*}
where the first inequality is from   $\lim_{\ell\to\infty}\sigma_i^{\ell}\left\| \triangle  {\bw}^{\ell}\right\| =0$.  
Therefore, $\lim_{\ell\to\infty} \sigma_i^{\ell} \left\|\triangle\overline{\bw}^{\ell}_i\right\|^2=0$, which by   \eqref{def-tilde-L} 
  suffices to $\lim_{\ell\to\infty}(\widetilde{\L}^\ell-\L^\ell)=0$.}}
  

2) It follows from Lemma \ref{descent-lemma-L} that $\bw^\ell,\bw_i^\ell\in\N(\delta)$ for any $i\in[m]$ and $\ell\geq0$, thereby
\begin{eqnarray*} 
\begin{aligned}
\| \bpi_{i}^{\ell}\| &~~ \leq&&\| \bpi_{i}^{\ell} + \bg_{i}^0\| + \|  \bg_{i}^0\| \\ 
  &~\overset{\eqref{optimality-condition}}{=} &&\| \bg_{i}^0- \bg_{i}^\ell- \rho_i \Q_i^{\ell}  \overline{\bw}_{i}^{\ell}\| + \| \bg_{i}^0\|\\[1ex]
&~~=&&\|\triangle\bg_i^{0}+ \nabla F_i(\bw^0)- \nabla F_i(\bw^\ell)-\triangle\bg_i^{\ell}- \rho_i \Q_i^{\ell}  \overline{\bw}_{i}^{\ell}\| + \| \bg_{i}^0\|\\ 
 & \overset{\eqref{Lipschitz-continuity}}{\leq}&&\|\triangle\bg_i^{0} -\triangle\bg_i^{\ell}\| + r_i\|  \bw^0 - \bw^{\ell}\|   +\rho_i\eta_i \|\triangle \overline{\bw}_{i}^{\ell}\| + \|  \bg_{i}^0\|\\ 
&~\overset{\eqref{def-eps}}{\leq}&& \sqrt{\varepsilon_i(\delta)}/4+ r_i\|  \bw^0 - \bw^{\ell}\|   +\rho_i\eta_i \|\triangle \overline{\bw}_{i}^{\ell}\| + \|  \bg_{i}^0\|\\[1.25ex]
 &~~\leq&& \sqrt{\varepsilon_i(\delta)}/4+ r_i(\|  \bw^0 \|+\| \bw^{\ell}\|) + \rho_i\eta_i (\| \bw^{\ell}_i\|+\| \bw^{\ell}\|) + \| \bg_{i}^0\|  .
\end{aligned}
\end{eqnarray*}
As every term of the right-hand side of the above inequality is bounded, $\{ \bpi_{i}^{\ell}\}$ is bounded.  Finally, let $(\bw^\infty,\W^\infty,\P^\infty)$ be an accumulating point  of sequence $\{(\bw^\ell,\W^\ell,\P^\ell)\}$, namely there is a subsequence converges to $(\bw^\infty,\W^\infty,\P^\infty)$. Then using \eqref{gap-all} and taking the limit of the right sides of \eqref{optimality-condition-w} and \eqref{optimality-condition} (along with the convergent trace of the subsequence if necessary), we obtain
\begin{eqnarray*}\label{accumulating-point}
\begin{aligned}
0 \overset{ \eqref{optimality-condition-w}}{=}~&\sum_{i=1}^m - \alpha_i   \bpi_{i}^\infty    + \lambda \bw^\infty,\\ 
0  \overset{ \eqref{optimality-condition}}{=} ~&\lim_{\ell\to0} \E \left(\bpi_{i}^{\ell} +   { \bg_{i}^{\ell}} +\rho_i \Q_i^{\ell+1} \triangle\overline{\bw}^{\ell}_i \right)\\
  = ~~&\lim_{\ell\to0}   \left( \bpi_{i}^{\ell} +    \nabla F_{i}(\bw^\ell) +\rho_i \Q_i^{\ell+1} \triangle\overline{\bw}^{\ell}_i \right)\\
  = ~~&\bpi_{i}^\infty +    \nabla F_{i}(\bw^\infty) ,~~ \forall i\in[m].
\end{aligned}
\end{eqnarray*}
Consequently, $0=\sum_{i=1}^m \alpha_i  \nabla F_{i}(\bw^\infty)  + \lambda \bw^\infty$. This means that $\bw^\infty$ is a stationary point of \eqref{opt-prob-distribute} in expectation according to \eqref{stationary-point}. 
%Based on \cite[Lemma 4.10]{more1983computing}, whole sequence $\{\bw^{\ell+1}\}$   converges to $\bw^\infty$ because $\lim_{\ell\to\infty}\triangle \bw^{\ell+1}=0$ and $\bw^\infty$ is isolated. 
\end{proof}

\subsection{Proof of Lemma \ref{main-convergence-rate}}
\begin{proof} First, we denote
\begin{eqnarray}  \label{def-L-phi}
 \begin{aligned}
\phi_i^{\ell+1}&:=\frac{ \varepsilon_i(\delta )}{\sigma_i^{\ell+1}}+ \frac{ \sigma_i^{\ell+1} }{8} \Big\|\triangle \bw_{i}^ {\ell+1}  \Big\|^2 +   \frac{ \sigma_i^{\ell+1}}{8}   \left\| \triangle \bw^{\ell+1} \right\|^2+    \varphi_i^{\ell+1}.
\end{aligned}
\end{eqnarray}
One can check that
\begin{equation} \label{def-L-phi-11}
\begin{aligned} 
\frac{  12\varepsilon_i(\delta )}{\sigma_i^{\ell+1}} \leq \frac{  16\varepsilon_i(\delta )}{\sigma_i^{\ell+1}} -\frac{ \varepsilon_i(\delta )}{\sigma_i^{\ell+1}} 
\overset{\eqref{sigma-0-vareps}}{\leq}  \gamma_i^{\ell}-\gamma_i^{\ell+1}  -\frac{ \varepsilon_i(\delta )}{\sigma_i^{\ell+1}}. 
\end{aligned}
\end{equation}  
Based on the proof of  Lemma \ref{descent-lemma-L} in Section \ref{proof-descent-lemms}, similar to derive \eqref{gap-LL-sum}, we can prove that
{{\begin{equation*} 
\begin{aligned} 
 \L^{\ell+1}- \L^{\ell}
 ~\leq ~~ & \sum_{i=1}^m \alpha_i \left[\frac{  12\varepsilon_i(\delta )}{\sigma_i^{\ell+1}}  +   \varphi_i^{\ell+1}     -  \frac{\sigma_i^{\ell+1}+4\lambda}{8}\left\|  \triangle\bw^{\ell+1} \right\|^2-  \frac{\sigma_i^{\ell+1}}{8} \left\|\triangle \bw_{i}^{\ell+1}\right\|^2  \right]\\
\overset{\eqref{def-L-phi-11}}{\leq} & \sum_{i=1}^m \alpha_i \left[\gamma_i^{\ell}-\gamma_i^{\ell+1}  +2\varphi_i^{\ell+1}  -  \phi_i^{\ell+1}  \right]. 
\end{aligned}
\end{equation*}  
This further leads to
\begin{equation*} 
\begin{aligned} 
\sum_{i=1}^m \alpha_i   \phi_i^{\ell+1} \overset{\eqref{def-varphi}}{\leq}~&  \L^{\ell}+\sum_{i=1}^m \alpha_i\left[\gamma_i^{\ell}+ \sigma_i^{\ell} \left\| \triangle\overline{\bw}^{\ell}_i\right\|^2\right] -\L^{\ell+1}-\sum_{i=1}^m \alpha_i\left[\gamma_i^{\ell+1} + \sigma_i^{\ell+1} \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2\right]\\
\overset{\eqref{def-tilde-L}}{\leq}~&\widetilde{\L}^{\ell} -\widetilde{\L}^{\ell+1} +\sum_{i=1}^m  \alpha_i\left[  \frac{  \sigma_i^{\ell}}{2} \left\| \triangle\overline{\bw}^{\ell}_i\right\|^2 -\frac{  \sigma_i^{\ell+1}}{2} \left\| \triangle\overline{\bw}^{\ell+1}_i\right\|^2\right],
\end{aligned}
\end{equation*}  
which together with $\triangle\overline{\bw}^{0}_i=0$ results in,
for any $k\geq0$,  
\begin{eqnarray} \label{sum-wi-w-L-L}
 \begin{aligned}
 \sum_{\ell=0}^{k-1} \sum_{i=1}^m \alpha_i\phi_i^{\ell+1} 
 \leq  \widetilde{\L}^{0} -\widetilde{\L}^{k}   \overset{\eqref{L-0-F0}}{\leq} \triangle F  +2.
\end{aligned}
\end{eqnarray}}}
Secondly,  there is
 \begin{eqnarray} \label{Pw-w-bd}
\begin{aligned}
 \frac{16}{15}\Big\|  \sigma_{i}^{\ell}\triangle \bw_{i}^ {\ell+1} +\textbf{P}_i^{\ell+1}  \triangle \overline{\bw}_{i}^ {\ell+1}\Big\|^2 
  &\overset{\eqref{triangle-ineq}}{\leq}  &&    \frac{16}{15} \frac{9(\sigma_i^\ell)^2}{8} \Big\| \triangle \bw_{i}^ {\ell+1}  \Big\|^2 +  \frac{144}{15} \Big\| \textbf{P}_i^{\ell+1}  \triangle \overline{\bw}_{i}^ {\ell+1}\Big\|^2 \\[1ex]
  &\overset{\eqref{P-upper-bd}}{\leq} && \frac{6(\sigma_i^\ell)^2}{5} \Big\| \triangle \bw_{i}^ {\ell+1}  \Big\|^2+\frac{48}{5}\frac{(11\sigma_i^{\ell+1})^2}{32^2} \Big\|  \triangle \overline{\bw}_{i}^ {\ell+1}\Big\|^2    \\[1ex]
 &\overset{\eqref{sub-pin}}{\leq}  &&   \frac{6(\sigma_i^{\ell+1})^2}{5} \Big\|\triangle \bw_{i}^ {\ell+1}  \Big\|^2+\frac{6}{5}\Big\|  \triangle  \bpi_{i}^ {\ell+1}\Big\|^2. 
% \overset{\eqref{range-gamma}}{\leq} ~&   \frac{11(\sigma_i^\ell)^2}{10} \Big\|\triangle \bw_{i}^ {\ell+1}  \Big\|^2+\frac{99}{25}\Big\|  \triangle  \bpi_{i}^ {\ell+1}\Big\|^2.
\end{aligned}
\end{eqnarray}
 Combining two optimality conditions \eqref{optimality-condition-w} and \eqref{optimality-condition}, we have 
\begin{eqnarray} \label{optimality-condition-w-0}
\begin{aligned}
0&\overset{\eqref{optimality-condition-w}}{=}&&\sum_{i=1}^m \alpha_i \Big[   - \bpi_{i}^\ell  -  \sigma_{i}^\ell(\bw_{i}^\ell- \bw^{\ell+1})\Big] +\lambda \bw^{\ell+1}\\
& \overset{\eqref{sub-pin}}{=}&&\sum_{i=1}^m \alpha_i \Big[   - \bpi_{i}^{\ell+1} + \sigma_{i}^{\ell+1}(\bw_{i}^ {\ell+1}- \bw^{\ell+1}) -  \sigma_{i}^\ell(\bw_{i}^\ell- \bw^{\ell+1})\Big]  +\lambda \bw^{\ell+1}\\
&~~=&&\sum_{i=1}^m \alpha_i \Big[   - \bpi_{i}^{\ell+1} + (\sigma_{i}^{\ell+1}-\sigma_{i}^{\ell})\triangle \overline{\bw}_{i}^ {\ell+1}+ \sigma_{i}^{\ell}\triangle \bw_{i}^ {\ell+1}\Big]  +\lambda \bw^{\ell+1}\\
 &\overset{\eqref{optimality-condition}}{=}&& \sum_{i=1}^m \alpha_i \Big[  \bg_i^{\ell+1}+\rho_i \Q_i^{\ell+1}  \triangle \overline{\bw}_{i}^ {\ell+1}  + (\sigma_{i}^{\ell+1}-\sigma_{i}^{\ell})\triangle \overline{\bw}_{i}^ {\ell+1}+ \sigma_{i}^{\ell}\triangle \bw_{i}^ {\ell+1}\Big] +\lambda \bw^{\ell+1}\\
&~~=&& \sum_{i=1}^m \alpha_i \Big[  \bg_i^{\ell+1}+\textbf{P}_i^{\ell+1}  \triangle \overline{\bw}_{i}^ {\ell+1} + \sigma_{i}^{\ell}\triangle \bw_{i}^ {\ell+1}\Big] +\lambda \bw^{\ell+1}. 
\end{aligned}
\end{eqnarray}
%which results in
%\begin{eqnarray*} %{optimality-condition-w}
%\begin{aligned}
%  \lambda \bw^{\ell+1} = \sum_{i=1}^m \alpha_i    \lambda \bw^{\ell+1} =\sum_{i=1}^m \alpha_i \Big[ -\bg_i^{\ell+1}- \sigma_{i}^{\ell}\triangle \bw_{i}^ {\ell+1} -\textbf{P}_i^{\ell+1}  \triangle \overline{\bw}_{i}^ {\ell+1}   \Big].
%\end{aligned}
%\end{eqnarray*}
Thirdly, we can verify that
\begin{eqnarray} \label{third-fact}
\begin{aligned}
 &&&  \Big\| \nabla F_{i}(\bw^{\ell+1})-\bg_i^{\ell+1}-  \sigma_i^\ell \triangle \bw_{i}^ {\ell+1} -\textbf{P}_i^{\ell+1} \triangle \overline{\bw}_{i}^ {\ell+1}  \Big\|^2 \\[1ex] 
 &~\overset{\eqref{triangle-ineq}}{\leq}  &&16\Big\| \nabla F_{i}(\bw^{\ell+1})-\bg_i^{\ell+1}\Big\|^2+\frac{16}{15}\Big\|\sigma_i^\ell \triangle \bw_{i}^ {\ell+1} +\textbf{P}_i^{\ell+1} \triangle \overline{\bw}_{i}^ {\ell+1} \Big\|^2\\[1ex] 
 &~\overset{\eqref{def-eps}}{\leq}  && \frac{ \varepsilon_i(\delta )}{4}  +\frac{16}{15}\Big\|\sigma_i^\ell \triangle \bw_{i}^ {\ell+1} +\textbf{P}_i^{\ell+1} \triangle \overline{\bw}_{i}^ {\ell+1} \Big\|^2 \\[1ex]   
&\overset{\eqref{Pw-w-bd}}{\leq} &&\frac{ \varepsilon_i(\delta )}{4}  +   \frac{6(\sigma_i^{\ell+1})^2}{5} \Big\|\triangle \bw_{i}^ {\ell+1}  \Big\|^2+\frac{6}{5} \Big\|  \triangle  \bpi_{i}^ {\ell+1}\Big\|^2\\[1ex]
&\overset{\eqref{gap-pi-k} }{\leq} && 10\varepsilon_i(\delta ) +  \frac{6(\sigma_i^{\ell+1})^2}{5} \Big\|\triangle \bw_{i}^ {\ell+1}  \Big\|^2 +   \frac{(\sigma_i^{\ell+1})^2}{20}   \left\| \triangle \bw^{\ell+1} \right\|^2+  \frac{3\sigma_i^{\ell+1}\varphi_i^{\ell+1}}{10}    \\[1ex]
&\overset{\eqref{def-L-phi}}{\leq} && 10\sigma_i^{\ell+1}  \phi_i^{\ell+1}\overset{\eqref{sub-sigma-mini} }{=}   \frac{10  \sigma_i^0 \phi_i^{\ell+1}}{ \gamma_i^{\ell+1}}   \leq  t^{\ell+1} \phi_i^{\ell+1},
\end{aligned}
\end{eqnarray} 
where $t^{\ell+1}:=\max_{i\in[m]} {10 \sigma_i^0}/{ \gamma_i^{\ell+1}}$. 
Based on the above conditions, $\sum_{i=1}^m\alpha_i=1$, and the convexity of $\|\cdot\|^2$, we obtain
 \begin{eqnarray*} 
\begin{aligned}
 \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2
 &~\overset{\eqref{opt-prob}}{=} &&\Big\|\sum_{i=1}^m \alpha_i   \nabla F_{i}(\bw^{\ell+1})+ \lambda \bw^{\ell+1}  \Big\|^2\\ 
&\overset{\eqref{optimality-condition-w-0}}{=}&&\Big\|\sum_{i=1}^m \alpha_i   \left(\nabla F_{i}(\bw^{\ell+1})-\bg_i^{\ell+1}- \sigma_i^\ell \triangle \bw_{i}^ {\ell+1} -\textbf{P}_i^{\ell+1} \triangle \overline{\bw}_{i}^ {\ell+1}    \right) \Big\|^2 \\ 
 &~~\leq&&\sum_{i=1}^m \alpha_i  \Big\| \nabla F_{i}(\bw^{\ell+1})-\bg_i^{\ell+1}-  \sigma_i^\ell \triangle \bw_{i}^ {\ell+1} -\textbf{P}_i^{\ell+1} \triangle \overline{\bw}_{i}^ {\ell+1}  \Big\|^2 \\ 
&\overset{\eqref{third-fact}}{\leq}&&   \sum_{i=1}^m \alpha_i  t^{\ell+1} \phi_i^{\ell+1}.
\end{aligned}
\end{eqnarray*} 
Summing the both sides of the above inequity for $\ell=0,1,\ldots,T-1$ yields
 \begin{eqnarray*} \label{sum-F'}
\begin{aligned}
\sum_{\ell=0}^{T-1} \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2
 \leq~&
\sum_{\ell=0}^{T-1}   \sum_{i=1}^m \alpha_i  t^{\ell+1} \phi_i^{\ell+1}  
\leq
\sum_{\ell=0}^{T-1}  \sum_{i=1}^m \alpha_i t^{T} \phi_i^{\ell+1} \\
 \overset{\eqref{sum-wi-w-L-L}}{\leq} &
 t^{T} \left( \triangle {F}+2 \right)
 \leq     \frac{\theta}{ \min \{\gamma_1^T,\cdots,\gamma_m^T\}} ,
\end{aligned}
\end{eqnarray*}
which together with the following fact,
 \begin{eqnarray*}  
\begin{aligned}
 \min_{\ell=0,1, \ldots,T-1} \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2\leq \frac{1}{T}\sum_{\ell=1}^T \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2,
\end{aligned}
\end{eqnarray*}
immediately shows the desired result.
\end{proof}

\subsection{Proof of Theorem \ref{main-convergence-rate-eps}}
\begin{proof} The range ${\epsilon\in (0,1/(2\theta+1)]}$ implies ${\gamma_i=1-\epsilon^{2}/4\in[3/4,1)}$ for each ${i\in[m]}$. Consequently,   \eqref{convergence-rate} holds and thus enables the following condition,
 \begin{eqnarray}   \label{tau-T-gamma-T}
\begin{aligned} 
\min_{\ell=0,1,\ldots,T-1} \left\|\nabla F_{\lambda}(\bw^{\ell+1})\right\|^2 
 \leq     \frac{\theta}{ T(1-\epsilon^{2}/4)^T}.
\end{aligned}
\end{eqnarray}
Again, the range of $\epsilon$ results in
 \begin{eqnarray*}  
\begin{aligned} 
\frac{2-2\sqrt{1-\theta\epsilon}}{\epsilon^{2}}  \leq \frac{2\theta\epsilon}{\epsilon^2}= \frac{2\theta}{\epsilon} \leq T=\left\lceil\frac{2\theta}{\epsilon}\right\rceil  \leq \frac{2\theta}{\epsilon}+1 \leq \frac{2\theta\epsilon+1}{\epsilon^2} \leq \frac{2+2\sqrt{1-\theta\epsilon}}{\epsilon^{2}}.
\end{aligned}
\end{eqnarray*}
Based on the above range of $T$, one can check that 
 \begin{eqnarray*}  \begin{aligned} 
 T\left(1- \frac{\epsilon^{2}}{4}\right)^T  
\geq   T \Big(1- \frac{\epsilon^{2}T}{4}\Big)  
\geq   \frac{\theta}{\epsilon} ,
\end{aligned}
\end{eqnarray*}
which combing \eqref{tau-T-gamma-T} derives the conclusion.  
\end{proof}

\subsection{Proof of Theorem \ref{main-convergence-full}}
\begin{proof}  1) Similar to prove 1) in Theorem \ref{main-convergence}, we claim all statements  except for $\lim_{\ell\to\infty}\|\triangle \bpi_{i}^{\ell+1}\|= 0$. This conclusion holds due to 
$$\lim_{\ell\to\infty}\|\triangle \bpi_{i}^{\ell+1} \|=\lim_{\ell\to\infty}\sigma_i^\ell\|\triangle \overline{\bw}^{\ell+1}_i\|=\lim_{\ell\to\infty}\sigma_i\|\triangle \overline{\bw}^{\ell+1}_i\| = 0.$$
2) The conclusion follows from the proof of 2) in Theorem \ref{main-convergence}. \\
3) This is the direct result of \eqref{convergence-rate} with setting $\varepsilon_i(\delta )=0$ and $\gamma_i=1$ for any $i\in[m]$.
\end{proof}


\end{document}
  
