\section{Related Work}
\textbf{Single Neuron Models:} Single neuron models are essential in understanding the fundamental properties of neuronal dynamics and behavior. The Leaky Integrate-and-Fire model \cite{liu2001spike}, developed in the 1950s, is a minimalistic model that simulates the integration of synaptic inputs and the leakage of membrane potential over time. The Hodgkin-Huxley model \cite{nelson1995hodgkin}, introduced in 1952, provides a detailed description of action potential generation using complex equations to represent ionic currents across the neuronal membrane, offering deep insights into neuronal excitability. The FitzHugh-Nagumo model \cite{izhikevich2006fitzhugh}, proposed in 1961, simplifies the Hodgkin-Huxley model to focus on excitability and action potential dynamics while reducing computational complexity. Finally, the Izhikevich model, developed in 2003, combines simplicity with versatility, effectively capturing a wide range of neuronal firing patterns and balancing computational efficiency with biological realism. The hyperparameters set for these models can be viewed as prior assumptions about the inherent properties of the neurons. In the subsequent sections, we select the Izhikevich model to generate a synthetic dataset for testing , using the set hyperparameters as the true labels. 

\textbf{Neural Latent Representation Learning:} Neural Latent Representation Learning has been pivotal in transforming high-dimensional neuronal data into lower-dimensional embeddings that encapsulate instantaneous information \cite{bengio2013representation}. A remarkable impact has been made in neuroscience —from the linear dimensionality reduction techniques such as Principal Component Analysis (PCA) \cite{mackiewicz1993principal}—to the non-linear visualization methods like Uniform Manifold Approximation and Projection (UMAP) \cite{mcinnes2018umap} and t-Distributed Stochastic Neighbor Embedding (t-SNE) \cite{kobak2019art}—and most recently, to the advanced, data-driven deep learning strategies. The focus was first on reducing the dimensionality of neuronal data alone. It later expanded to include joint dimensionality reduction with behavioral information and external stimuli (e.g., pi-VAE \cite{prakash2024interpreting} and BLEND \cite{guo2025blendbehaviorguidedneuralpopulation}). CEBRA represents a culmination of these advancements, integrating various techniques into a unified framework. CEBRA is used in this study to integrate the stimuli information experienced by each neuron as input for . NeuPRINT is a method used to extract invariant information from neurons \cite{mi2023learning}; however, it still adheres to traditional neuron modeling approaches, which can only implicitly represent neurons. This results in challenging training processes and suboptimal representation performance.

\textbf{Contrastive Learning for Voice Representations:} The basic approach of the solution presented in this paper draws inspiration from similar tasks, such as extracting inherent representations of speakers from voice data \cite{torres2024singer}. This type of work has been implemented on song-singer datasets, where contrastive learning is used to bring together the voices of the same singer while pushing apart the voices of different singers. However, while we borrowed the underlying idea, the specific methods had to be uniquely designed to accommodate the characteristics of neural data.
% 本文解决方案的基本思路参考了类似任务：从声音中提取讲话人的固有表示。这类工作在歌曲-歌手数据集进行实现，借助对比学习使同一歌手的歌声彼此靠近，同时将不同歌手的歌声推开。但是我们只是借鉴了思想，具体方法还需针对神经数据特点另做设计