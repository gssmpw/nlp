
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{marvosym}
\usepackage{graphicx} 
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
\lstset{
    backgroundcolor=\color{white},
    basicstyle=\ttfamily,
    breaklines=true,
    frame=single,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
}



\title{Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{Wei Wu \\
Peking University\\
\texttt{weiwu@stu.pku.edu.cn} \\
\And
Can Liao \\
University of Georgia\\
\texttt{can.liao@uga.edu} \\
\And
Zizhen Deng\\
Chinese Academy of Sciences\\
Institute of Automation\\
\texttt{18803835618@163.com} \\
\And
Zhengrui Guo \\
The Hong Kong University of Science and Technology\\
Beijing Institute of Collaborative Innovation\\
\texttt{zguobc@connect.ust.hk} \\
\And
Jinzhuo Wang \textsuperscript{\Letter}\thanks{\Letter~: Corresponding Author}\\
Peking University\\
\texttt{wangjinzhuo@pku.edu.cn} \\
}
% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\renewcommand\footnotemark{}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
The Platonic Representation Hypothesis posits that behind different modalities of data (what we sense or detect), there exists a universal, modality-independent representation of reality. Inspired by this hypothesis, we treat each neuron as a system, where we can detect the neuron’s multi-segment activity data under different peripheral conditions. We believe that, similar to the Platonic idea, there exists a time-invariant representation behind different segments of the same neuron, which reflects the intrinsic properties of the neuron’s system. Intrinsic properties include the molecular profiles, location within brain regions and morphological structure, etc. The optimization objective for obtaining intrinsic neuronal representations should meet two criteria: (I) the representations of recording segments from the same neuron must exhibit higher similarity compared to those from different neurons; (II) the representations should generalize effectively to out-of-domain data. To this end, we propose the NeurPIR (Neuron Platonic Intrinsic Representation) framework, which leverages contrastive learning by treating segments from the same neuron as positive pairs and those from different neurons as negative pairs. In the implementation, we adopt VICReg, which only uses positive pairs while indirectly separating dissimilar samples through regularization terms. 
To validate the efficacy of our method, we first conducted tests on simulated neuronal population dynamics data generated by the Izhikevich model. The results confirmed that our approach accurately captured the neuron types as defined by the preset hyperparameters. Subsequently, we applied our method to two real - world neuron dynamics datasets, which included neuron type annotations derived from spatial transcriptomics and the location of each neuron within brain regions. The representations learned from our model not only accurately predicted neuron types and locations but also demonstrated robustness when tested on out-of-domain data (data from unseen animals). This finding underscores the potential of our approach in furthering the understanding of neuronal systems and offers valuable insights for future neuroscience research. Code is available at \href{https://github.com/ww20hust/NeurPIR}{https://github.com/ww20hust/NeurPIR}.
\end{abstract}
\section{Introduction}
Unraveling the intricacies of neuronal activity and the information encoded within neural dynamics stands as a monumental challenge in the field of neuroscience. Plato’s cave allegory and Platonic Representation Hypothesis \cite{huh2024platonicrepresentationhypothesis} suggests the existence of a universal, modality-independent representation of world that transcends the modalities through which we perceive it. Drawing inspiration from this philosophical concept, we propose a novel perspective on neuronal activity, treating each neuron as a distinct system capable of generating multi-segment activity data under various peripheral conditions, the multi-segment activity data of individual neuron as what we perceive.

Our approach, NeurPIR, is based on the premise that, similar to the Platonic idea, a time-invariant representation exists that underlies the diverse activity segments of the same neuron. This representation is hypothesized to encapsulate the intrinsic properties of the neuronal system, offering a stable and consistent framework for understanding neuronal function.

To extract intrinsic representations, we formulated an optimization objective based on two key criteria. First, activity segments from the same neuron should exhibit greater similarity than those from different neurons, ensuring that our method effectively captures the unique signatures of individual neurons. Second, the derived representations should demonstrate high generalizability, enabling their application to out-of-domain data, such as neuronal activity from different species or experimental conditions not included in the training set.

To achieve these objectives, we employed contrastive learning, a powerful machine learning technique that treats segments from the same neuron as positive pairs and those from different neurons as negative pairs. This approach leverages the contrast between similar and dissimilar samples to learn effective representations. For our implementation, we utilized VICReg, a variant of contrastive learning that focuses exclusively on positive pairs while incorporating regularization terms to indirectly separate dissimilar samples. This method ensures that the learned representations are both discriminative and robust to variations in the data. However, achieving the above requires the following two innovative designs for neuron data: Firstly, we use CEBRA \cite{schneider2023learnable} to integrate the single neuronal peripheral information (such as activity of neighboring neuronal populations and behavioral data for each segment of a single neuron). This process encodes the peripheral information associated with each segment of an individual neuron. While CEBRA has been widely recognized for its ability to produce high-performance learnable latent embeddings for jointly representing surrounding neuronal data and external information, it was previously applied to groups of neurons rather than focusing on individual neurons. Secondly, the approach aims to produce a consistent representation of a neuron's identity across varying experimental conditions, using these conditions as positive instances in a contrastive learning paradigm. While multi-session data naturally provides such instances, the reality is that many neurons are recorded in only a single session. To address this limitation, we designed a data augmentation method tailored to neuronal data. This method involves extracting segments of varying lengths from the same session to serve as positive sample pairs. In this way, the learned representation can capture intrinsic properties across different time scales. 

To rigorously assess the effectiveness of the representations learned by NeurPIR, we model neuron population dynamic data using the Izhikevich model  \cite{izhikevich2003simple}, where different neurons are assigned distinct hyperparameters representing  different firing modes as intrinsic properties. These neurons are randomly connected to form a network, and after stimulating the network, we obtain neuron population data. The representations learned by  through self-supervised learning on this neuron population data have been confirmed to contain hyperparameter information. In recognition of the noise and complexity in real-world neuronal datasets, we turn to a publicly available dataset of mouse brain neuron populations. These neurons, after recording dynamic activities, are separated and labeled with cell type annotations using transcriptomics. The representations learned by  on this dataset can be efficiently utilized for downstream cell type prediction tasks. In addition, using another real public dataset containing ten mice and 39 datasets, we demonstrate that the intrinsic information contained in the representation obtained by is consistent across animals, and that this representation has strong cross-domain capability when performing downstream tasks.

\section{Related Work}
\textbf{Single Neuron Models:} Single neuron models are essential in understanding the fundamental properties of neuronal dynamics and behavior. The Leaky Integrate-and-Fire model \cite{liu2001spike}, developed in the 1950s, is a minimalistic model that simulates the integration of synaptic inputs and the leakage of membrane potential over time. The Hodgkin-Huxley model \cite{nelson1995hodgkin}, introduced in 1952, provides a detailed description of action potential generation using complex equations to represent ionic currents across the neuronal membrane, offering deep insights into neuronal excitability. The FitzHugh-Nagumo model \cite{izhikevich2006fitzhugh}, proposed in 1961, simplifies the Hodgkin-Huxley model to focus on excitability and action potential dynamics while reducing computational complexity. Finally, the Izhikevich model, developed in 2003, combines simplicity with versatility, effectively capturing a wide range of neuronal firing patterns and balancing computational efficiency with biological realism. The hyperparameters set for these models can be viewed as prior assumptions about the inherent properties of the neurons. In the subsequent sections, we select the Izhikevich model to generate a synthetic dataset for testing , using the set hyperparameters as the true labels. 

\textbf{Neural Latent Representation Learning:} Neural Latent Representation Learning has been pivotal in transforming high-dimensional neuronal data into lower-dimensional embeddings that encapsulate instantaneous information \cite{bengio2013representation}. A remarkable impact has been made in neuroscience —from the linear dimensionality reduction techniques such as Principal Component Analysis (PCA) \cite{mackiewicz1993principal}—to the non-linear visualization methods like Uniform Manifold Approximation and Projection (UMAP) \cite{mcinnes2018umap} and t-Distributed Stochastic Neighbor Embedding (t-SNE) \cite{kobak2019art}—and most recently, to the advanced, data-driven deep learning strategies. The focus was first on reducing the dimensionality of neuronal data alone. It later expanded to include joint dimensionality reduction with behavioral information and external stimuli (e.g., pi-VAE \cite{prakash2024interpreting} and BLEND \cite{guo2025blendbehaviorguidedneuralpopulation}). CEBRA represents a culmination of these advancements, integrating various techniques into a unified framework. CEBRA is used in this study to integrate the stimuli information experienced by each neuron as input for . NeuPRINT is a method used to extract invariant information from neurons \cite{mi2023learning}; however, it still adheres to traditional neuron modeling approaches, which can only implicitly represent neurons. This results in challenging training processes and suboptimal representation performance.

\textbf{Contrastive Learning for Voice Representations:} The basic approach of the solution presented in this paper draws inspiration from similar tasks, such as extracting inherent representations of speakers from voice data \cite{torres2024singer}. This type of work has been implemented on song-singer datasets, where contrastive learning is used to bring together the voices of the same singer while pushing apart the voices of different singers. However, while we borrowed the underlying idea, the specific methods had to be uniquely designed to accommodate the characteristics of neural data.
% 本文解决方案的基本思路参考了类似任务：从声音中提取讲话人的固有表示。这类工作在歌曲-歌手数据集进行实现，借助对比学习使同一歌手的歌声彼此靠近，同时将不同歌手的歌声推开。但是我们只是借鉴了思想，具体方法还需针对神经数据特点另做设计

\section{Method}
\subsection{Goal} \label{sec:goal}
Our goal is to develop a method for learning intrinsic neuron representations from neuron population data. These representations should meet three key criteria: (i) neurons with similar functional roles should exhibit greater similarity in their representations compared to those with different roles; (ii) the learned representations should be robust to variations in neuronal activity patterns caused by different stimuli or environmental conditions; and (iii) the representations should be adaptable and generalizable to new and unseen neuronal activity patterns.
% \[
% \text{NeuronSystem}_{\Omega}(\text{Stimulus}) = \text{Response Activity}
% \]
% \[
% \text{}(\text{Stimulus}, \text{Response Activity}) = \Omega\text{ Embedding}
% \]
% \[
% \text{Similar } \Omega-\text{Similar } \Omega\text{ Embedding}
% \]
\subsection{ Architechture}\label{sec:overview}
The ideal embedding space for neuron representations should cluster recording segments of the same neuron while also ensuring semantic consistency by placing similar neurons close to each other within the space. In line with the criteria outlined in Section \ref{sec:goal}, We conducted advanced contrastive learning loss functions, VICReg \cite{bardes2021vicreg}. We also carefully designed the data sampling methods to generate multi-segments activity data for each neuron for training purposes.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figure1.png}
    \caption{Optimization objective for obtaining intrinsic representations of neurons as follows: clips(segments) from the same neuron should have a higher average similarity than clips from different neurons. Figure 1 illustrates how this objective can be achieved using a contrastive learning approach, where different recording segments from the same neuron are treated as positive pairs and segments from different neurons are treated as negative pairs. Each neuron is considered separately, and the Stimulus, Behavior and Session treated as surrounding information(SI), which acts as auxiliary variables. The surrounding information for each neuron is processed and encoded using CEBRA.}
    \label{fig:model}
\end{figure}





\textbf{Data sampling}:
Without loss of generality, we take bi-segment data as an example. Consider two scenarios: 1) If a neuron has recordings from multiple sessions, we randomly extract two segments $(X_{seg}, X'_{seg})$ from different sessions. 2) If a neuron has recordings from only a single session, we randomly extract two non-overlapping segments $(X_{seg}, X'_{seg})$ from that session. 
We repeat this process $B$ times for a batch size of $B$, obtaining a positive pair batch $(X^{(1)}_{seg}, X^{(2)}_{seg})$.

\textbf{Model}:
We should integrate surrounding information into each segment. 
In neurobiology, the stimulation received by the neuron corresponding to a segment (\( X_{st} \)), the animal's behavior at the corresponding time (\( X_{be} \)), and the session information (\( X_{se} \)) are all the surrounding information (SI) of this segment. CEBRA is a self-supervised learning algorithm designed to obtain interpretable and consistent embeddings of high-dimensional recordings using auxiliary variables. In simple terms, it serves as an encoder for the dynamic activity information of neuronal populations. Its advantage lies in its ability to encode both neuronal activity data (N*T) and corresponding auxiliary variables, such as behavior and external stimuli (M*T), into a lower-dimensional representation (D*T), where D represents the latent space dimensions. In CEBRA, D does not correspond to individual neurons, but rather provides a dimensionality reduction method that combines high-dimensional temporal activity data of neuronal populations with corresponding auxiliary variable information, aiming to uncover the relationships between neuronal activity and these variables.

Our work focuses on obtaining time-invariant representations for individual neurons. We innovatively use CEBRA as a preprocessing step for our input data from a new perspective. Specifically, from the viewpoint of individual neurons, we leverage CEBRA to integrate peripheral information—such as the activity of neighboring neuronal populations and behavioral data—associated with each segment of a single neuron. $X_{st}$ consists of two components: peripheral neuronal activity and visual stimulation. The dimensions of $X_{st}$ are $n+1$: The $n$ dimensions are the activity of the surrounding neurons relative to the neuron you're considering, because there will be coordinates for each neuron in the dataset, so for each neuron, we'll take the 47 nearest neurons in the experiment. The last dimension is visual stimuli, and we have a relatively simple way of dealing with visual stimuli: Blank visual stimuli are represented by 0, Drifting Grating visual stimuli by 1, Natural Scenes visual stimuli by 2, and each time point is represented by a fixed number. In the future, we could try to encode the video of visual stimuli so that it changes over time, but we currently only encode it according to the type of visual stimulus. The dimension of $X_{be}$ is 1: running speed each time point. The dimension of $X_{se}$ is 1: session number. The dimensions of $X_{si}$ are 10: The total dimension of $X_{be}$, $X_{se}$ and $X_{st}$ is 50, we use CEBBRA project them to the low dimension 10. 
% We then denote the pair \((\text{SI Embedding}, \text{Neuron Activity})\) for the same time segment as \(X\) in the following sections.
\begin{equation}
    X_{si} = \text{CEBRA}(X_{st},X_{be},X_{se})
\end{equation}
We then denote the pair \((X_{si}, X_{seg})\)for the same time as \(X\) in the following sections.
The encoder \( G(\cdot) \) maps the extracted input \( X \) into a latent representation \( H' \), which is then aggregated into time-invariant feature embeddings \( H \) using adaptive average pooling. These embeddings \( H \) are further mapped into a lower-dimensional space \( Z \) by a projection layer \( P(\cdot) \). The full model \( F(\cdot) \) combines feature extraction, encoding, and projection. During training, \( F(X) \) produces the projections \( Z \). After training, the projection layer is removed, retaining only the embeddings \( H \). The similarity between embeddings is measured using cosine similarity.

% 整体框架：我们先介绍的流程，然后再对其中细节逐个补充。在一个神经元群的数据集中，对于其中任一个神经元，可以取得它的多段活动时序数据和对应的刺激时序数据，方法介绍部分我们两个神经元（i，j）和各自两段活动与对应刺激时序数据（活动数据：Xim，Xin，Xjm，Xjn，刺激数据： Yim， Yin， Yjm， Yjn）
% \textbf{Contrastive Learning - SimCLR}:
% We employ the contrastive loss from SimCLR. The loss maximizes the agreement between positive samples and pushes all other embeddings of the batch (the negative parts) away in the representation space. It does so by maximizing the cosine similarity ($\text{sim}$) between positive samples and minimizing the sum of similarities for all other pairs formed in the batch:
%  \begin{equation}\label{eq:contrastive_loss_3}
% \mathcal{L}_{\text{cont}}(\mathbf{Z}) =
% - \sum_i \log \frac{\exp(\text{sim}(Z_i^{(1)}, Z_i^{(2)}) / \tau)}{\sum_{j \neq i} \exp(\text{sim}(Z_i^{(1)}, Z_j^{(2)}) / \tau)}.
% \end{equation}

\textbf{Contrastive Learning - VICReg}:
VICReg aims to enhance the quality of learned embeddings by incorporating three types of losses: variance loss, invariance loss, and covariance loss. 

The invariance loss, ensures that embeddings of segments from the same neuron are close:
\begin{equation}
\mathcal{L}_{\text {invar}}(Z^{(1)}, Z^{(2)}) = \frac{1}{N} \sum_i\left\| Z_{i}^{(1)} - Z_{i}^{(2)} \right\|^{2},
\end{equation}
The variance loss regularizes the standard deviation of the embeddings to be near a target value \(\mu\), which helps prevent embedding dimensions from becoming non-informative. Given \(\mathbf{d_j(z)} \in \mathbb{R}^B\), a vector of batch values at dimension \(j\), the variance loss is defined as:
\begin{equation}
    \mathcal{L}_{\text {var}} (\mathbf{z}) = \frac{1}{D} \sum_{j=1}^{D} \max \left(0, \mu - S\left(\mathbf{d_j(z)}, \epsilon\right)\right),
\end{equation}
where \(D\) represents the number of dimensions in \(\mathbf{z}\), and \(S\) denotes the regularized standard deviation, \(S(x, \epsilon) = \sqrt{\operatorname{Var}(x) + \epsilon}\).

The covariance loss promotes orthogonality among embedding dimensions by decorrelating them:

\begin{equation}
    \mathcal{L}_{\text {cov}}(\mathbf{z}) = \frac{1}{D_z} \sum_{i \neq j} (C(\mathbf{z}))_{i, j}^{2},
\end{equation}
where \(C(\mathbf{z}) = \frac{1}{N-1} \sum_{i=1}^{N} \left(\mathbf{z}_{i} - \bar{\mathbf{z}}\right) \left(\mathbf{z}_{i} - \bar{\mathbf{z}}\right)^{T}\) is the covariance matrix of \(\mathbf{z}\), and \(\bar{\mathbf{z}} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{z}_{i}\).

% We also introduced an additional commonly used loss function that disperses segments from different neurons across a unified spherical surface, commonly referred to as uniformity.
% \begin{equation}\label{eq:uniformity}
% \mathcal{L}_{\text {unif }}^k(Z^{(k)}) = \log \frac{1}{N} \sum_{i, j}\left(\exp(-t\| Z_{i}^{(k)} - Z_{j}^{(k)} \|^{2})\right),
% \end{equation}
% where \( t = 2 \) and \( \mathcal{L}_{\text {unif }} = \sum_{k=1,2} \mathcal{L}_{\text {unif }}^k / 2 \).
\section{Experiments}
\subsection{Data} \label{sec:data}
\textbf{Simulated Data: }Since the neuron intrinsic property is hardly available in vivo neuronal recordings, we applied  to synthetic data where we can access the ground-truth intrinsic property. To make the
synthetic data exhibit dynamics similar to that of real neurons, we simulated the data following the Izhikevich model. The Izhikevich model is a spiking neuron model that combines biological realism with computational efficiency. It is designed to capture the rich dynamics of real neurons while remaining computationally simple. The model is defined by the following differential equations:
\begin{equation}
    \frac{dV}{dt} = 0.04V^2 + 5V + 140 - u + I,
\end{equation}
\begin{equation}
    \frac{du}{dt} = a(bV - u),
\end{equation}
where \( V \) is the membrane potential of the neuron, \( u \) is a recovery variable, \( I \) is the input current, and \( a \), \( b \), and \( c \) are hyperparameters. The model also includes a spike-reset mechanism:
\begin{equation}
    \text{if } V \geq 30 \text{ mV, then } 
    \begin{cases}
        V \leftarrow c \\
        u \leftarrow u + d
    \end{cases}
\end{equation}
 \( a \) , \( b \) , \( c \) and \( d \) can be regarded as intrinsic properties of each neuron. 
% We simulated a population with 400 neurons for each of the five types (Regular Spiking, Fast Spiking, Chattering, Intrinsically Bursting, and Low-threshold Spiking) with random connections, generating dynamic activity data for a total of five sessions, each containing 1200 time points.

\textbf{Real Data - Bugeon: }We utilized a rare, real-world multimodal dataset \cite{bugeon2022transcriptomic} to test , which comprises two main components: 1) in vivo long-term recordings of multiple neurons in the mouse primary visual cortex using two-photon imaging technology, and 2) spatial transcriptomics of ex vivo slices from the recorded brain tissue to measure the expression levels of 72 selected genes in these neurons. Based on gene expression levels, neurons were assigned two main types of labels: (i) excitatory and inhibitory classification, and (ii) subclass labels (Lamp5, Pvalb, Vip, Sncg, and Sst) for some inhibitory neurons. This dataset contains recordings from four mice (Mouse A, B, C, and D). Mouse A has data from 6 sessions, Mouse B from 3 sessions, Mouse C from 5 sessions, and Mouse D from 3 sessions. During each session, the mice were exposed to three types of visual stimuli: Blank, Drifting Gratings, and Natural Scenes. A total of 9,278 unique neurons were recorded in the dataset.
% 这个数据集一共记录了四只老鼠（mouse A B C D）A有6个sessions B有3个session C有5个session D有3个session，每只老鼠在每一个session中会应对三种视觉刺激（Blank，Drifting Gratings，Natural Scenes）

\textbf{Real Data - Steinmetz: } The Steinmetz dataset comprises 39 Neuropixels recordings, capturing data from 400 to 700 neurons across various regions of the mouse brain of 10 mice during a visual behavior task \cite{steinmetz2019distributed}. This dataset is excellent for exploratory analyses and is well-supported by extensive tutorial resources \footnote{https://www.youtube.com/watch?v=WXn4-FpVaOo}, alongside a wealth of experimental and behavioral variables included within. 
\subsection{Evaluation}
\textbf{Evaluation on Simulated Data: }
The process is divided into two steps: (i) perform self-supervised contrastive learning on the dynamic data of all neurons to obtain a representation for each neuron; (ii) Employ a 5-fold cross-validation approach to use these neuron representations as input to a classifier for predicting the pre-defined neuron type labels from the simulation.

\textbf{Evaluation on Real Data - Bugeon: }
The process is divided into three steps: (i) perform self-supervised contrastive learning on the dynamic data of all neurons from four mouse in the real dataset to obtain a representation for each neuron; (ii) Based on neurobiological prior knowledge and spatial transcriptomic gene expression information, assign cell type labels to each neuron. The labels fall into two categories: (a) excitatory and inhibitory, and (b) Lamp5, Pvalb, Vip, Sncg, and Sst; (iii) Implement a 4-fold (with the folds based on the identity of the mice) cross-validation approach where the neuron representations are used as input to a classifier to predict the neuron’s type labels for both categories (a) and (b).

\textbf{Out-of-Domain Evaluation - Steinmetz dataset: }
The process is divided into three steps: (i) Perform self-supervised contrastive learning on the dynamic data of all neurons from all mice in the real dataset to obtain a representation for each neuron; (ii) As before, assign location within brain regions labels to the neurons; (iii) Implement a 10-fold (with the folds based on the identity of the mice) cross-validation approach where the neuron representations are used as input to a classifier to predict the neuron’s location. During neurodevelopment, where the position of a neuron is crucial for its differentiation, maturation, and connectivity. The location can influence the neuron's gene expression, synaptic connections, and ultimately its function \cite{PatelPoo1982}. In this sense, the location is an intrinsic property because it defines role within the nervous system.

\subsection{Comparison of Methods}
\textbf{LOLCAT: }This method \cite{schneider2023transcriptomic} follows a supervised learning paradigm. It directly uses activity data from a subset of neurons to train a classifier to predict neuron labels, and then validates on the remaining neurons. Consequently, the representations learned in the intermediate layers of the model contain only label information and do not fully capture the intrinsic properties of the neurons.

\textbf{PCA: }This method employs a self-supervised approach. Principal Component Analysis (PCA) reduces the dimensionality of each neuron's activity data by projecting it onto a lower-dimensional space, thereby providing a representation based on the most significant components of the activity data.

\textbf{UMAP: }Similar to PCA, Uniform Manifold Approximation and Projection (UMAP) is a self-supervised method that reduces the dimensionality of each neuron's activity data. UMAP preserves local and global structures in the data to create a meaningful lower-dimensional representation.

\textbf{NeurPrint:  }This self-supervised method involves implicit representation learning through backpropagation. Due to its implicit nature, the model can be difficult to converge and may require substantial training data and time to achieve effective results, and the representations may not align well with the Platonic Representation Hypothesis.


To demonstrate that the representations learned only from neuronal activity capture invariant properties, we designed a verification process using simulation data. Based on the Izhikevich model, we defined the neurons' time-invariant hyperparameters as their intrinsic properties and applied stimulation to generate activity data (Figure ~\ref{fig:sys_data1}). Our model relies solely on activity data to derive time-invariant representations of neurons and validates these representations by showing their ability to distinguish predefined intrinsic hyperparameters. To evaluate the effectiveness of our method, we classify intrinsic hyperparameters (five categories representing five neuronal firing modes) using the obtained intrinsic representations and compare its performance with four other algorithms: PCA, UMAP, NeuPRINT, and LOLCAT. The classification performance is assessed across multiple neuron types using three common metrics: Precision, Recall, and F1-score. Higher values of these metrics indicate better classification performance.

From the results presented in Table~\ref{table1}, we observe that: (i) our method consistently achieves the highest performance across most neuron firing modes, particularly in the Regular Spiking (RS) and Fast Spiking (FS) categories, where it significantly outperforms all baselines with F1-scores of 0.884 and 0.881, respectively; (ii) although some baselines, such as LOLCAT and NeuPRINT, demonstrate competitive results in certain neuron firing modes like Low-Threshold Spiking (LTS), their performance falls short compared to NeurPIR. For instance, in the case of Chattering (CH) neurons, NeurPIR achieves an F1-score of 0.671, surpassing LOLCAT's 0.652 and NeuPRINT's 0.611. These results underscore the robustness and accuracy of NeurPIR across varying neuron firing modes, establishing it as the most effective method in this comparison. Furthermore, these neuron firing modes are determined by preset hyperparameters, further validating that the intrinsic representations learned by NeurPIR effectively capture hyperparameter information.

\begin{table*}[ht]
\label{table1}
\centering
\begin{tabular}{lcccccc}
\hline
Firing Modes& Metric & PCA  & UMAP & NeuPRINT & LOLCAT & NeurPIR\\
\hline
  & Prec.  & 0.689 & 0.473 & 0.836 & 0.783 & \textbf{0.872} \\
regular spiking (RS) & Rec.   & \textbf{0.918} & 0.590 & 0.908 & 0.910 & 0.898 \\
  & F1.    & 0.786 & 0.525 & 0.870 & 0.841 & \textbf{0.884} \\
\hline
  & Prec.  & 0.534 & 0.332 & 0.646 & \textbf{0.681} & 0.678 \\
intrinsically bursting (IB) & Rec.   & 0.375 & 0.310 & 0.645 & 0.573 & \textbf{0.693} \\
  & F1.    & 0.440 & 0.320 & 0.644 & 0.620 & \textbf{0.684} \\
\hline
  & Prec.  & 0.506 & 0.335 & 0.648 & 0.616 & \textbf{0.722} \\
chattering (CH) & Rec.   & 0.603 & 0.248 & 0.580 & \textbf{0.698} & 0.630 \\
  & F1.    & 0.548 & 0.285 & 0.611 & 0.652 & \textbf{0.671} \\
\hline
  & Prec.  & 0.778 & 0.602 & 0.826 & 0.853 & \textbf{0.853} \\
fast spiking (FS) & Rec.   & 0.620 & 0.638 & 0.820 & 0.733 & \textbf{0.913} \\
  & F1.    & 0.689 & 0.618 & 0.823 & 0.787 & \textbf{0.881} \\
\hline
  & Prec.  & 0.970 & 0.944 & 0.968 & \textbf{0.993} & 0.990 \\
low-threshold spiking (LTS) & Rec.   & 0.935 & 0.965 & \textbf{0.993} & 0.983 & 0.990 \\
  & F1.    & 0.952 & 0.954 & 0.980 & 0.988 & \textbf{0.990} \\
\hline
\end{tabular}

\caption{This table presents the performance metrics—precision (Prec.), recall (Rec.), and F1 score—across five methods (PCA, UMAP, NeurPrint, LOLCAT, and NeurPIR) for different neuron types: regular spiking (RS), intrinsically bursting (IB), chattering (CH), fast spiking (FS), and low-threshold spiking (LTS). The results indicate that  and NeurPrint consistently achieve higher precision and F1 scores for most neuron types, while UMAP shows relatively lower performance, particularly for IB and CH neurons.}
\end{table*}
\subsection{Real Data - Neuron Platonic Intrinsic Representation Contains Molecular Information}


% \begin{table*}[ht]
% \label{table2}
% \centering
% \begin{tabular}{lcccc}
% \hline
% Neuron Type & Metric & NeuPRINT & LOLCAT &  NeurPIR\\
% \hline
% \hline
% Subclass\\
% \hline
%   & Prec.  & 0.457 & 0.348 & \textbf{0.509} \\
% Lamp5 & Rec.  & 0.800 & \textbf{0.850} & 0.830 \\
%   & F1.    & 0.582 & 0.494 & \textbf{0.631} \\
% \hline
%   & Prec.  & 0.815 & 0.395 & \textbf{0.833} \\
% Vip & Rec.   & 0.750 & 0.150 & \textbf{0.800} \\
%   & F1.    & 0.781 & 0.217 & \textbf{0.816} \\
% \hline
%   & Prec.  & 0.700 & 0.398 & \textbf{0.732} \\
% Pvalb & Rec.   & 0.700 & 0.470 & \textbf{0.710} \\
%   & F1.   & 0.700 & 0.431 & \textbf{0.721} \\
% \hline
%   & Prec.  & \textbf{1.000} & - & \textbf{1.000} \\
% Sst & Rec.   & 0.330 & 0.000 & \textbf{0.440} \\
%   & F1.    & 0.496 & - & \textbf{0.611} \\
% \hline
% \hline
% Class\\
% \hline
%   & Prec.  & 0.736 & 0.549 & \textbf{0.794} \\
% Ex & Rec.   & 0.810 &  \textbf{0.900} & 0.850 \\
%   & F1.    & 0.771 & 0.682 & \textbf{0.821} \\
% \hline
%   & Prec.  & 0.549 & 0.722 & \textbf{0.839} \\
% In & Rec.   & \textbf{0.900}&  0.260 & 0.780 \\
%   & F1.    & 0.682 & 0.383 & \textbf{0.808} \\
% \hline
% \end{tabular}



% \begin{table*}[ht]
% \label{table2}
% \centering
% \begin{tabular}{lcccc}
% \hline
% Neuron Type & Metric & NeuPRINT & LOLCAT &  NeurPIR\\
% \hline
% \hline
% Subclass\\
% \hline
%   & Prec.  & 0.4624 & 0.3010 & \textbf{0.4921} \\
% Lamp5 & Rec.  & 0.7444 & \textbf{0.7499} & 0.7711 \\
%   & F1.    & 0.5703 & 0.4296 & \textbf{0.6008} \\
% \hline
%   & Prec.  & 0.7121 & 0.4510 & \textbf{0.7800} \\
% Vip & Rec.   & 0.6982 & 0.1563 & \textbf{0.7545} \\
%   & F1.    & 0.7050 & 0.2319 & \textbf{0.7666} \\
% \hline
%   & Prec.  & 0.6433 & 0.4933 & \textbf{0.6852} \\
% Pvalb & Rec.   & 0.6591 & 0.4535 & \textbf{0.6686} \\
%   & F1.   & 0.6509 & 0.4723 & \textbf{0.6766} \\
% \hline
%   & Prec.  & 0.8569 & 0.4359 & \textbf{0.8684} \\
% Sst & Rec.   & 0.3274 & 0.1044 & \textbf{0.4243} \\
%   & F1.    & 0.4728 & 0.1680 & \textbf{0.5700} \\
% \hline
% \hline
% Class\\
% \hline
%   & Prec.  & 0.7285 & 0.5499 & \textbf{0.7744} \\
% Ex & Rec.   & 0.8035 & 0.8782 & \textbf{0.8220} \\
%   & F1.    & 0.7641 & 0.6763 & \textbf{0.7975} \\
% \hline
%   & Prec.  & 0.7810 & 0.6986 & \textbf{0.8104} \\
% In & Rec.   & 0.7005 & 0.2811 & \textbf{0.7605} \\
%   & F1.    & 0.7385 & 0.4007 & \textbf{0.7846} \\
% \hline
% \end{tabular}
% \caption{Performance metrics for neuron type classification across three methods—NeuPRINT, LOLCAT, and . This table delineates the precision (Prec.), recall (Rec.), and F1 score for various neuron types categorized into subclasses (Lamp5, Vip, Pvalb, Sst) and classes (Ex, In). The metrics demonstrate the comparative performance of these methods in identifying and classifying distinct neuronal subtypes. }
% \end{table*}



In this section, we utilized a public multimodal dataset, referred to as Bugeon, to train and evaluate our model. We compared the performance metrics of neuron type classification for three methodologies: NeuPRINT, LOLCAT, and the proposed NeurPIR (PCA and UMAP were excluded as they cannot process behavioral information). The evaluation metrics included precision (Prec.), recall (Rec.), and F1 score, which are essential for assessing the classification effectiveness across various neuron types, categorized into subclasses and classes.

The results for the subclasses Lamp5, Vip, Pvalb, and Sst are summarized in Table~\ref{table4}. For the Lamp5 subclass, NeurPIR achieved the highest F1 score (0.569 ± 0.014), demonstrating a balanced classification performance. Similarly, in the Vip subclass, NeurPIR outperformed others with an F1 score of 0.662 ± 0.035, highlighting its superior accuracy in classifying Vip neurons. For Pvalb neurons, NeurPIR again led with an F1 score of 0.604 ± 0.043, reflecting its effectiveness in this subclass. In the Sst subclass, although NeuPRINT achieved the highest precision (0.704 ± 0.168), its large standard deviation indicates variability. NeurPIR showed the highest F1 score (0.492 ± 0.080), demonstrating robustness despite the challenges in classifying Sst neurons.

Overall, the results demonstrate the effectiveness of NeurPIR in accurately classifying distinct neuronal subtypes, achieving improved precision and F1 scores compared to existing methodologies. These findings highlight that the neuronal representations generated by our method effectively encode neuron type information. Since neuron types are determined by molecular characteristics, this suggests that the intrinsic representations also capture molecular information.

\begin{table}[ht]
\label{table2}
\centering
\begin{tabular}{lcccc}
\hline
Neuron Type & Metric & LOLCAT & NeuPRINT & NeurPIR \\
\hline
\hline
Subclass & Prec. & $0.460 \pm 0.064$ & $0.590 \pm 0.073$ & $0.607 \pm 0.049$ \\
         & Rec.  & $0.418 \pm 0.084$ & $0.562 \pm 0.093$ & $0.587 \pm 0.067$ \\
         & F1.   & $0.401 \pm 0.056$ & $0.553 \pm 0.080$ & $0.582 \pm 0.059$ \\
\hline
Class    & Prec. & $0.619 \pm 0.015$ & $0.711 \pm 0.044$ & $0.755 \pm 0.023$ \\
         & Rec.  & $0.584 \pm 0.018$ & $0.707 \pm 0.047$ & $0.749 \pm 0.040$ \\
         & F1.   & $0.552 \pm 0.018$ & $0.706 \pm 0.047$ & $0.747 \pm 0.016$ \\
\hline
\end{tabular}
% \begin{tabular}{lcccc}
% \hline
% Neuron Type & Metric & NeuPRINT & LOLCAT & NeurPIR \\
% \hline
% \hline
% Subclass & Prec. & $0.590 \pm 0.073$ & $0.460 \pm 0.064$ & $0.607 \pm 0.049$ \\
%          & Rec.  & $0.562 \pm 0.093$ & $0.418 \pm 0.084$ & $0.587 \pm 0.067$ \\
%          & F1.   & $0.553 \pm 0.080$ & $0.401 \pm 0.056$ & $0.582 \pm 0.059$ \\
% \hline
% Class    & Prec. & $0.711 \pm 0.044$ & $0.619 \pm 0.015$ & $0.755 \pm 0.023$ \\
%          & Rec.  & $0.707 \pm 0.047$ & $0.584 \pm 0.018$ & $0.749 \pm 0.040$ \\
%          & F1.   & $0.706 \pm 0.047$ & $0.552 \pm 0.018$ & $0.747 \pm 0.016$ \\
% \hline
% \end{tabular}
\caption{Average performance metrics for neuron type classification. Subclass averages are calculated across Lamp5, Vip, Pvalb, and Sst, while Class averages are calculated across Ex and In. Metrics include precision (Prec.), recall (Rec.), and F1 score, with errors represented as standard deviations.}
\label{table:averages}
\end{table}






% \subsection{Neuron Platonic Intrinsic Representation Contains Brain Area Information}
% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{sys2.png}
%     \caption{a: Steinmetz dataset contains 39 subdataset from 10 mice. Each subdataset records 400-700 neurons each from across the mouse brain during a visual behavior task. Each neuron with its brain area label. b: show of part of subdataset3. c: show of part of subdataset26.}
%     \label{fig:sys_data2}
% \end{figure}


% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{outofdomain.png}
%     \caption{Accuracy comparison of three models (, NeuPRINT, and LOLCAT) in brain region classification across four brain areas (visual cortex, thalamus, hippocampus, and midbrain). The results show validation accuracy on in-domain data (dark bars) and test accuracy on out-of-domain data (light bars). }
%     \label{fig:outofdomain}
% \end{figure}





% \begin{table}[ht]
% \label{table3}
% \centering
% \begin{tabular}{l l c c c c c}
% \toprule
% \textbf{Region} & \textbf{Method} & \textbf{Precision(In/Out)} & \textbf{Recall(In/Out)} & \textbf{F1(In/Out)} \\
% \midrule
% \multirow{3}{*}{Vis}    & NeuPRINT & 0.923 ± 0.029 / 0.751 ± 0.047 & 0.614 ± 0.052 / 0.467 ± 0.105 & 0.736 ± 0.042 / 0.572 ± 0.086 \\
%                                & LOLCAT   & 0.836 ± 0.049 / 0.775 ± 0.014 & 0.450 ± 0.059 / 0.425 ± 0.053 & 0.584 ± 0.060 / 0.547 ± 0.043 \\
%                                & NeurPIR   & 0.900 ± 0.016 / 0.825 ± 0.017 & 0.652 ± 0.024 / 0.514 ± 0.012 & 0.756 ± 0.013 / 0.633 ± 0.013 \\

% \midrule
% \multirow{3}{*}{Thal}    & NeuPRINT & 0.758 ± 0.025 / 0.669 ± 0.025 & 0.762 ± 0.005 / 0.724 ± 0.019 & 0.760 ± 0.013 / 0.695 ± 0.016 \\
%                                & LOLCAT   & 0.717 ± 0.047 / 0.698 ± 0.073 & 0.738 ± 0.013 / 0.681 ± 0.010 & 0.726 ± 0.025 / 0.688 ± 0.040 \\
%                                & NeurPIR   & 0.765 ± 0.025 / 0.725 ± 0.049 & 0.772 ± 0.018 / 0.761 ± 0.027 & 0.768 ± 0.015 / 0.742 ± 0.034 \\
% \midrule
% \multirow{3}{*}{Hipp} & NeuPRINT & 0.708 ± 0.033 / 0.626 ± 0.026 & 0.785 ± 0.013 / 0.671 ± 0.022 & 0.744 ± 0.023 / 0.647 ± 0.016 \\
%                                & LOLCAT   & 0.668 ± 0.034 / 0.595 ± 0.028 & 0.742 ± 0.018 / 0.703 ± 0.026 & 0.703 ± 0.027 / 0.644 ± 0.020 \\
%                                & NeurPIR   & 0.723 ± 0.018 / 0.644 ± 0.032 & 0.782 ± 0.019 / 0.743 ± 0.019 & 0.751 ± 0.013 / 0.689 ± 0.017 \\
% \midrule
% \multirow{3}{*}{Mid} & NeuPRINT & 0.669 ± 0.026 / 0.611 ± 0.012 & 0.814 ± 0.005 / 0.748 ± 0.031 & 0.734 ± 0.017 / 0.672 ± 0.013 \\
%                                & LOLCAT   & 0.584 ± 0.018 / 0.569 ± 0.024 & 0.768 ± 0.010 / 0.726 ± 0.029 & 0.663 ± 0.013 / 0.637 ± 0.009 \\
%                                & NeurPIR   & 0.669 ± 0.028 / 0.634 ± 0.027 & 0.789 ± 0.018 / 0.740 ± 0.013 & 0.724 ± 0.021 / 0.683 ± 0.021 \\

% \bottomrule
% \end{tabular}
% \caption{Performance Comparison of Methods Across location within brain regions (In-Distribution vs Out-of-Distribution)}
% \end{table}
\begin{table}[ht]
\label{table3}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Domain} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
\multirow{2}{*}{LOLCAT}   & In-Domain & $0.701 \pm 0.093$ & $0.674 \pm 0.084$ & $0.694 \pm 0.072$ \\
                          & Out-of-Domain & $0.659 \pm 0.075$ & $0.644 \pm 0.092$ & $0.652 \pm 0.073$ \\
\midrule
\multirow{2}{*}{NeuPRINT} & In-Domain & $0.764 \pm 0.091$ & $0.744 \pm 0.081$ & $0.743 \pm 0.071$ \\
                          & Out-of-Domain & $0.665 \pm 0.079$ & $0.678 \pm 0.092$ & $0.662 \pm 0.072$ \\
\midrule
\multirow{2}{*}{NeurPIR}  & In-Domain & $0.764 \pm 0.093$ & $0.749 \pm 0.072$ & $0.758 \pm 0.066$ \\
                          & Out-of-Domain & $0.701 \pm 0.081$ & $0.672 \pm 0.074$ & $0.708 \pm 0.054$ \\
\bottomrule
\end{tabular}
\caption{Average performance metrics for In-Domain and Out-of-Domain data across all regions (Vis, Thal, Hipp, Mid) for each method. Metrics include precision, recall, and F1 score, with errors represented as standard deviations.}
\label{table:averages_ind_outd}
\end{table}





\subsection{Real Data -  Shows Robustness on Out-of-Domain (Unseen Animal) Data}
% 在这个章节中，我们主要是来验证模型获得的神经元固有表示在不同动物上的一致性，Steinmetz dataset包含来自十个老鼠的神经元活动数据，每个神经元包含所处脑区的标签，我们使用脑区标签分类任务来验证固有表示。我们使用NeurPIR对所有老鼠的神经元进行自监督训练获取固有表示，对于固有表示的下游脑区分类任务，我们从十只老鼠中任意五只老鼠的神经元表示来训练和测试分类器，余下老鼠的神经元表示用于验证表示的泛化性，即在across-animal上的一致性。

In this section, we focus on validating the consistency of the intrinsic representations generated by the model across different animals. The Steinmetz dataset, which includes neural activity data from ten rats, provides neuron labels based on their respective brain regions. It can be intuitively seen from Figure ~\ref{fig:sys_data2} that the response patterns of neurons in the same brain area are significantly different in different mice, but we hope that the representation obtained from the model still contains consistent information, like consistent brain area information. This corresponds to the evaluation of the generalizability of the representations obtained by the model on cross-modal (here, cross-animal) data in deep learning. We use a task of classifying location within brain regions to validate the intrinsic representations. We use NeurPIR to perform self-supervised training on all the neurons from the all mice to obtain the intrinsic representations. For the downstream task of brain region classification, we used 10-fold cross-validation (with folds based on the identity of the mice).

As shown in Figure~\ref{table3}, the model's generalizability is evaluated through brain region classification across different neural representations. In-domain validation results demonstrate that  consistently outperforms other methods like LOLCAT and NeuPRINT across all location within brain regions, including visual cortex (ViS), thalamus (Thal), hippocampus (Hipp), and midbrain (Mid). Specifically,  achieves validation Precision close to 0.80 in most regions, with NeuPRINT slightly trailing behind.

When examining out-of-domain performance (cross-animal), we notice a general drop in accuracy for all methods. However, NeurPIR still retains a higher degree of accuracy across all location within brain regions compared to NeuPRINT and LOLCAT, showing the model's ability to capture more robust intrinsic representations across different animals. This consistent outperformance across both in-domain and out-of-domain tests suggests that the representations obtained by  better generalize across animals while still preserving critical brain region information.

 
% \subsection{Ablation Studies }

\section{Conclusion and Discussion}
In this paper, we present a novel and scalable approach for extracting and leveraging the intrinsic properties of neurons. This approach holds significant potential for re-evaluating existing neuroscience data and enhancing our understanding of neural computation.
Future research may focus on further enhancing its ability to handle an even more diverse range of datasets and applying it to other domains where extracting intrinsic properties from complex systems is of great importance.
Limitations:
(i) The representation learned by our method can only distinguish neurons with substantial differences in essential attributes. For instance, when neurons are associated with more refined brain-area labels, it becomes challenging to differentiate them, necessitating more data for training support.
(ii) The learned neuron representations only support data collected from the same technology. The generalization of cross - platform data, such as two-photon data and neurpixel data, remains to be explored.
(iii) Over very long timescales, some of the short-term invariant properties of neurons may change. This can be exploited to study the changes in neuronal properties during the progression of diseases like Alzheimer's disease.

\section*{Reproducibility Statement}
To enhance the reproducibility of this study, we provide an Appendix section comprising 4 subsections that offer detailed supplementary information. Appendix A.3 presents the pseudo-code of Synthetic Data. Appendix A.4 presents python code for downloading and organizing the steinmetz dataset. Appendix A.5 presents the pseudo-code of sownstream task. Appendix A.6 presents Description and Function across firing types / neuron types / brain regions in this paper.

\section*{Acknowledgement}
This research was supported by National Natural Science Foundation of China (6220071694), National Key Research and Development Program of China (2024YFF0507404), and Young Elite Scientist Sponsorship Program by Beijing Association for Science and Technology (BYESS2023026).



\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}
\newpage



\subsection{Detail performance metrics for neuron type/location classification.}
\begin{table*}[ht]
\label{table4}
\centering
\begin{tabular}{lcccc}
\hline
Neuron Type & Metric & NeuPRINT & LOLCAT & NeurPIR\\
\hline
\hline
Subclass\\
\hline
  & Prec.  & 0.481 ± 0.035 & 0.344 ± 0.023 & 0.487 ± 0.007 \\
Lamp5 & Rec.  & 0.667 ± 0.037 & 0.694 ± 0.033 & 0.684 ± 0.028 \\
  & F1.    & 0.559 ± 0.036 & 0.460 ± 0.024 & 0.569 ± 0.014 \\
\hline
  & Prec.  & 0.614 ± 0.044 & 0.592 ± 0.078 & 0.657 ± 0.048 \\
Vip & Rec.   & 0.652 ± 0.034 & 0.406 ± 0.028 & 0.668 ± 0.027 \\
  & F1.    & 0.632 ± 0.034 & 0.480 ± 0.042 & 0.662 ± 0.035 \\
\hline
  & Prec.  & 0.559 ± 0.046 & 0.428 ± 0.026 & 0.602 ± 0.042 \\
Pvalb & Rec.   & 0.604 ± 0.017 & 0.403 ± 0.017 & 0.607 ± 0.055 \\
  & F1.   & 0.580 ± 0.032 & 0.415 ± 0.019 & 0.604 ± 0.043 \\
\hline
  & Prec.  & 0.704 ± 0.168 & 0.477 ± 0.130 & 0.681 ± 0.072 \\
Sst & Rec.   & 0.323 ± 0.084 & 0.170 ± 0.077 & 0.388 ± 0.080 \\
  & F1.    & 0.441 ± 0.105 & 0.248 ± 0.100 & 0.492 ± 0.080 \\
\hline
\hline
Class\\
\hline
  & Prec.  & 0.685 ± 0.050 & 0.555 ± 0.010 & 0.720 ± 0.025 \\
Ex & Rec.   & 0.774 ± 0.020 & 0.854 ± 0.009 & 0.817 ± 0.031 \\
  & F1.    & 0.726 ± 0.036 & 0.673 ± 0.008 & 0.765 ± 0.009 \\
\hline
  & Prec.  & 0.737 ± 0.037 & 0.682 ± 0.020 & 0.790 ± 0.020 \\
In & Rec.   & 0.640 ± 0.073 & 0.314 ± 0.027 & 0.680 ± 0.048 \\
  & F1.    & 0.685 ± 0.058 & 0.430 ± 0.028 & 0.729 ± 0.023 \\
\hline
\end{tabular}

\caption{Performance metrics for neuron type classification, 4-fold cross-validation was used, with the folds based on the identity of the mice This table delineates the precision (Prec.), recall (Rec.), and F1 score for various neuron types categorized into subclasses (Lamp5, Vip, Pvalb, Sst) and classes (Ex, In). The metrics demonstrate the comparative performance of these methods in identifying and classifying distinct neuronal subtypes. }
\end{table*}
\begin{table}[ht]
\label{table5}
\centering
\begin{tabular}{l l c c c c c}
\toprule
\textbf{Region} & \textbf{Method} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
\multirow{3}{*}{Vis}    & neu & 0.923 ± 0.029 & 0.614 ± 0.052 & 0.736 ± 0.042 \\
                               &          & 0.751 ± 0.047 & 0.467 ± 0.105 & 0.572 ± 0.086 \\
                               & LOLCAT   & 0.836 ± 0.049 & 0.450 ± 0.059 & 0.584 ± 0.060 \\
                               &          & 0.775 ± 0.014 & 0.425 ± 0.053 & 0.547 ± 0.043 \\
                               & NeurPIR  & 0.900 ± 0.016 & 0.652 ± 0.024 & 0.756 ± 0.013 \\
                               &          & 0.825 ± 0.017 & 0.514 ± 0.012 & 0.633 ± 0.013 \\
\midrule
\multirow{3}{*}{Thal}    & NeuPRINT & 0.758 ± 0.025 & 0.762 ± 0.005 & 0.760 ± 0.013 \\
                               &          & 0.669 ± 0.025 & 0.724 ± 0.019 & 0.695 ± 0.016 \\
                               & LOLCAT   & 0.717 ± 0.047 & 0.738 ± 0.013 & 0.726 ± 0.025 \\
                               &          & 0.698 ± 0.073 & 0.681 ± 0.010 & 0.688 ± 0.040 \\
                               & NeurPIR  & 0.765 ± 0.025 & 0.772 ± 0.018 & 0.768 ± 0.015 \\
                               &          & 0.725 ± 0.049 & 0.761 ± 0.027 & 0.742 ± 0.034 \\
\midrule
\multirow{3}{*}{Hipp}    & NeuPRINT & 0.708 ± 0.033 & 0.785 ± 0.013 & 0.744 ± 0.023 \\
                               &          & 0.626 ± 0.026 & 0.671 ± 0.022 & 0.647 ± 0.016 \\
                               & LOLCAT   & 0.668 ± 0.034 & 0.742 ± 0.018 & 0.703 ± 0.027 \\
                               &          & 0.595 ± 0.028 & 0.703 ± 0.026 & 0.644 ± 0.020 \\
                               & NeurPIR  & 0.723 ± 0.018 & 0.782 ± 0.019 & 0.751 ± 0.013 \\
                               &          & 0.644 ± 0.032 & 0.743 ± 0.019 & 0.689 ± 0.017 \\
\midrule
\multirow{3}{*}{Mid}     & NeuPRINT & 0.669 ± 0.026 & 0.814 ± 0.005 & 0.734 ± 0.017 \\
                               &          & 0.611 ± 0.012 & 0.748 ± 0.031 & 0.672 ± 0.013 \\
                               & LOLCAT   & 0.584 ± 0.018 & 0.768 ± 0.010 & 0.663 ± 0.013 \\
                               &          & 0.569 ± 0.024 & 0.726 ± 0.029 & 0.637 ± 0.009 \\
                               & NeurPIR  & 0.669 ± 0.028 & 0.789 ± 0.018 & 0.724 ± 0.021 \\
                               &          & 0.634 ± 0.027 & 0.740 ± 0.013 & 0.683 ± 0.021 \\
\bottomrule
\end{tabular}
\caption{Performance Comparison of Methods Across location within brain regions (In-Distribution vs Out-of-Distribution). The upper part of each indicator represents In Domain and the lower part represents Out of Domain.}
\end{table}
\newpage

\subsection{hyperparameters of five firing modes of neurons when simulating data}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figure2.png}
    \caption{a: the hyperparameters of five firing modes of neurons when simulating data; b: the neuron firing: the index of regular spiking (RS) is 0-400, the index of intrinsically bursting (IB) is 400-800, the index of chattering (CH) is 800-1200, the index of fast spiking (FS) is 1200-1600, the index of low-threshold spiking (LTS) is 1600-2000; c: the results of the visualization of neuronal activity data using UMAP.}
    \label{fig:sys_data1}
\end{figure}
\newpage


\subsection{Simulation of Neuronal Population Using the Izhikevich Model}
\begin{enumerate}
    \item Define the Izhikevich model parameters for each neuron type:
    \begin{itemize}
        \item Regular Spiking (RS)
        \item Intrinsically Bursting (IB)
        \item Chattering (CH)
        \item Fast Spiking (FS)
        \item Low-Threshold Spiking (LTS)
    \end{itemize}
    
    \item Initialize the total number of excitatory ($N_e$) and inhibitory ($N_i$) neurons.
    
    \item Assign neuron types to indices:
    \begin{itemize}
        \item 25\% Regular Spiking (RS)
        \item 25\% Intrinsically Bursting (IB)
        \item 25\% Chattering (CH)
        \item 25\% Fast Spiking (FS)
    \end{itemize}
    
    \item Initialize synaptic connection matrix $S$.
    
    \item Set initial values for membrane potential $v$ and recovery variable $u$.
    
    \item For each time step $t$ from 0 to $T$:
    \begin{itemize}
        \item Calculate input current $I$.
        \item If $t > 0$, add synaptic contributions from previously fired neurons.
        \item Update membrane potential $v$ and recovery variable $u$ using Euler's method.
        \item Check for spikes and record firing events.
        \item Reset membrane potential and increment recovery variable for fired neurons.
        \item Store current and potentials for analysis.
    \end{itemize}
    
    \item Update neuron activity data.
    
    \item Plot firing events.
    
    \item Print shapes of activity arrays for each neuron type.
\end{enumerate}


\subsection{python code for downloading and organizing the steinmetz dataset}
\begin{lstlisting}[language=Python]
# @title Data retrieval
import os, requests

fname = []
for j in range(3):
    fname.append('steinmetz_part%d.npz' % j)
url = ["https://osf.io/agvxh/download"]
url.append("https://osf.io/uv3mw/download")
url.append("https://osf.io/ehmw2/download")

for j in range(len(url)):
    if not os.path.isfile(fname[j]):
        try:
            r = requests.get(url[j])
        except requests.ConnectionError:
            print("!!! Failed to download data !!!")
        else:
            if r.status_code != requests.codes.ok:
                print("!!! Failed to download data !!!")
            else:
                with open(fname[j], "wb") as fid:
                    fid.write(r.content)
# @title Data loading
alldat = np.array([])
for j in range(len(fname)):
    alldat = np.hstack((alldat,
                        np.load('steinmetz_part%d.npz' % j,
                                allow_pickle=True)['dat']))
\end{lstlisting}
\newpage
\subsection{pseudo-code of downstream task}
\begin{algorithm}
\caption{Neural Network Classification with K-Fold Cross-Validation}
\begin{algorithmic}[1]
\State \textbf{Import necessary libraries:} NumPy, scikit-learn, Seaborn, Matplotlib
\State \textbf{Ignore warnings}
\State
\Procedure{Main}{}
    \State \textbf{Initialize labels} as NumPy array
    \State \textbf{Encode labels} using LabelEncoder
    \State \textbf{Standardize neuron features} using StandardScaler
    \State
    \State \textbf{Initialize StratifiedKFold} with 5 splits
    \State \textbf{Initialize statistics dictionaries} for precision, recall, F1-score
    \State \textbf{Initialize empty list for confusion matrices}
    \State
    \For{each fold in K-Fold}
        \State \textbf{Split data into training and test sets}
        \State \textbf{Create MLPClassifier model}
        \State \textbf{Fit model to training data}
        \State \textbf{Predict labels for test data}
        \State \textbf{Generate classification report and confusion matrix}
        \State \textbf{Append confusion matrix to list}
        \State
        \For{each cell type in classes}
            \State \textbf{Record precision, recall, and F1-score}
        \EndFor
    \EndFor
    \State
    \For{each cell type in classes}
        \State \textbf{Calculate and print average metrics}
    \EndFor
    \State
    \State \textbf{Compute cumulative confusion matrix}
    \State \textbf{Print cumulative confusion matrix}
    \State
    \State \textbf{Optional: Plot cumulative confusion matrix using Seaborn}
\EndProcedure
\end{algorithmic}
\end{algorithm}

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{appendix2.png}
%     \caption{Confusion matrices for different neuron classification models. Each heatmap illustrates the performance of the respective model in classifying neuron subclasses and excitatory/inhibitory neurons.}
%     \label{fig:appendix2}
% \end{figure}

% \subsection{Explanation of Izhikevich model hyperparameters}
% These hyperparameters characterize the neuron's unique dynamics and response characteristics:
% \begin{itemize}
%     \item \( a \): Defines the time scale of the recovery variable \( u \), indicating the speed at which the neuron recovers from spiking events.
%     \item \( b \): Influences the sensitivity of \( u \) to the membrane potential \( V \), affecting the neuron's firing patterns.
%     \item \( c \): Determines the reset value for \( V \) after a spike, specifying the neuron's post-spike membrane potential.
%     \item \( d \): Indicates the amount by which \( u \) is incremented after a spike, shaping the neuron's recovery process.
% \end{itemize}

\newpage
\subsection{Steinmetz dataset}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.85\textwidth]{sys2.png}
    \caption{a: Steinmetz dataset contains 39 subdataset from 10 mice. Each subdataset records 400-700 neurons each from across the mouse brain during a visual behavior task. Each neuron with its brain area label. b: show of part of subdataset3. c: show of part of subdataset26.}
    \label{fig:sys_data2}
\end{figure}

\subsection{Description and Function across firing types / neuron types / brain regions in this paper}
Firing Types:

1. Regular Spiking (RS) Neurons:

   - Description: Regular Spiking neurons are characterized by their ability to fire action potentials at a regular, predictable rate in response to a sustained depolarizing stimulus. These neurons typically exhibit a linear relationship between input and output, meaning they can respond to small inputs with a consistent firing pattern.
   

2. Intrinsically Bursting (IB) Neurons:

   - Description: Intrinsically Bursting neurons can produce bursts of action potentials in response to a depolarizing stimulus, in addition to regular single spikes. This type of neuron displays a unique pattern of activity where a series of action potentials is generated in quick succession followed by a period of quiescence.
   

3. Chattering (CH) Neurons:

   - Description: Chattering neurons exhibit a high-frequency, sustained firing pattern. These neurons are characterized by their ability to fire at a high rate in bursts, often exhibiting a very rapid oscillatory behavior with minimal latency between spikes.
   

4. Fast Spiking (FS) Neurons:

   - Description: Fast Spiking neurons are a type of inhibitory interneuron known for their ability to fire action potentials at very high frequencies (often greater than 100 Hz). They exhibit rapid and precise spiking in response to stimuli and are critical for the regulation of network activity.
   

5. Low-Threshold Spiking (LTS) Neurons:

   - Description: Low-Threshold Spiking neurons are characterized by their ability to fire action potentials at relatively low levels of depolarization. They are often described as having a "sensitive" or "easy-to-trigger" firing threshold, which enables them to respond to subtle changes in membrane potential.
   

Neuron Types:

1. Lamp5 Neurons (Lysosomal-Associated Membrane Protein 5):

   - Description: Lamp5 neurons are a type of GABAergic interneuron that expresses the Lamp5 protein, which is involved in cellular trafficking and autophagy. These neurons are often found in regions of the brain involved in cortical circuits, particularly in the cortex.
   

2. Vip Neurons (Vasoactive Intestinal Peptide-expressing neurons):

   - Description: Vip neurons are a type of inhibitory interneuron that express vasoactive intestinal peptide (VIP), a neuropeptide involved in the modulation of neural circuits. These neurons typically have broad inhibitory effects in cortical regions and influence the activity of other interneuron types.
   

3. Pvalb Neurons (Parvalbumin-expressing neurons):

   - Description: Pvalb neurons are a subtype of fast-spiking inhibitory interneurons that express the calcium-binding protein parvalbumin. These neurons are well known for their ability to fire action potentials at extremely high frequencies with minimal delay.
   

4. Sst Neurons (Somatostatin-expressing neurons):

   - Description: Sst neurons are another type of inhibitory interneuron that express somatostatin, a neuropeptide that inhibits neurotransmitter release. These neurons are typically involved in modulating cortical circuits and are especially important for regulating synaptic plasticity.
   

Brain Regions:

1. Vis (Visual Cortex): Occipital lobe, primarily in the calcarine sulcus and surrounding regions.

2. Hipp (Hippocampus): Medial temporal lobe, beneath the cerebral cortex.

3. Thal (Thalamus): Deep within the brain, just above the brainstem, near the center.

4. Mid (Midbrain): Between the forebrain and hindbrain, just above the pons and below the thalamus.


\end{document}
