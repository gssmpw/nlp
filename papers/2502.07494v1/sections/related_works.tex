\section{Related Works}

\subsection{Disentangled Representation Learning}
Disentangled representation learning has emerged as a critical area of study within the broader field of representation learning.
This can be categorized into dimension-wise and vector-wise approaches according to representation structure~(\cite{WangCTWZ22}).
Dimension-wise methods assign a single scalar dimension to represent fine-grained generative factors, 
making them suitable for synthetic and simple datasets~(\cite{ChenDHSSA16, JeonLPK21, LinKFO19, XiaoHM17}).
In contrast, vector-wise methods use multiple dimensions (vectors) to represent coarse-grained factors, 
making them more applicable to complex real-world tasks such as identity swapping, 
image classification and video understanding~(\cite{TranYL17, LiuLNXZ21, Denton17, WangCTWZ22}).
In this paper, we focus on vector-wise approaches 
since flexible approaches are suitable to code search as one of the most realistic tasks.

(\cite{WilesGSRKDC22, LeeJLPSHY24}) explore the disentangled representations and (\cite{LeeJLPSHY24}) highlights 
that minimum entropy problem ignores the influence of disentangled representations, 
which leads to unreliable prediction.
In addition, (\cite{WilesGSRKDC22}) assists that it is important 
for model to understand disentangled representations for distribution shift.
However, most methodologies for disentangled representation learning do not focus on 
the learning process itself which drives model to learn entanglement of fine-grained representations
In contrast, this paper analyzes adaptation as minimum entropy problem in terms of Lebesgue integral 
and proposes a new algorithm for clustering disentangled representations 
to reflect on the internal structure of representation

\subsection{Semantic Code Search}
Semantic Code Search~(\cite{GuZ018,CambroneroLKS019}) is a task to retrieve the most relevant code snippets 
in response to given natural language query.
This semantic code search is especially useful for hallucination of LLM with leverage for the result of 
retrieval~(\cite{ChenYJSKRJT24,ShiqiMJZTSJ24}). 
It has significantly evolved in accordance with the advancements in deep learning.
Some datasets such as CodeSearchNet~(\cite{HusainWGAB19}) and CoSQA~(\cite{HuangTSG0J0D20}) are introduced 
and other foundational works did significant efforts to advance the frontiers to the next level
~(\cite{EnshenYWLHSDH23, HaochenXAC23, Haochen24}).
These datasets not only support the development of code-specific pretrained language model 
~(\cite{GuoRLFT0ZDSFTDC21, FengGTDFGS0LJZ20, WangWJH21, GuoLDW0022, WangLGB0H23}) but also enable the fine-tuning of these models, 
leading to notable improvements in code search tasks~(\cite{GuoLDW0022, WangLGB0H23}). 
GraphCodeBERT~(\cite{FengGTDFGS0LJZ20}), UniXCoder~(\cite{GuoLDW0022}) and CodeT5+~(\cite{WangLGB0H23}) stand out 
as these programming language models that are the ones of the most successful models in code search task.

As long as we know, there is no single paper that 
treats the distribution shift in code search except for docprompting~(\cite{Zhou0XJN23}) 
which only partially treats code search under the setting of distribution shift 
since the authors focus on the code generation.
However, the retrieval module is enough deserved to be spotlighted, 
considering the sensitivity of LLMs to given information and the hallucination as the problem deep-seated into LLMs.
Disentangled representation learning is well-suited to the solution of this distribution shift, 
especially in the domain of programming language 
in that PL is structured language and pattern in this domain is sensitive to the structure of disentangled fragments.
This learning paradigm is out of sight of the researchers in code domain 
except for (\cite{ZhangHZWLS21}) which try to learn disentangled representations in code domain for multilingual generalization.
However, they do only focus on code translation not on code search 
in spite of the intimate connection between code search and the representation learning.
In contrast to these previous efforts, 
we focus on disentangled representation learning as an adaptation paradigm 
to diverse types of shifts in code search for the leverage of structure in disentangled representations.

\subsection{Minimum Entropy Problem}
Entropy is originally introduced as optimal length of code by (\cite{Shannon48, AnqiMY23}).
This entropy is formally defined as the expectation of self-information across different events.
The self-information can be interpreted as a measure for the amount of information 
since it is inversely proportional to the possibility of specific event.
If that event is likely to happen, the occurrence of that event does not update much references for decision.
In contrast, the occurrence of unlikely event updates many parts of codebooks.
These correlations makes self-information proportional to the amount of updates, 
which is the reason why we use this measure for information.

Minimum entropy problem is the problem to minimize the expectation 
of this amount of updates by exploring the space of parameters.
This problem is widely used in the field of machine learning for classification with cross entropy.
In addition, entropy minimization plays a crucial role in Test Time Adaptation 
that can improve the generalization to shifted distributions in unsupervised manner(\cite{WangSLOD21}). 
(\cite{OriRYM24}) provides experimental supports for the hypothesis that entropy minimization is related to clustering.
Unlike (\cite{WangSLOD21}) and (\cite{OriRYM24}), we theoretically shed on the light 
into the internal mechanism of minimum entropy problem 
which can be used to train model regardless of the existence of supervised signals.
This makes us realize the reason that shifted initialization results in the bad solution of clustering.

The minimum set cover problem is one of the most fundamental optimization problems, which is NP-complete.
Given universe $U$, a collection $T$ as a power set of $U$, and cost function $c$, 
this problem is defined to find a sub-collection of $T$ whose union of all elements covers $U$ whose cost is minimum.
Greedy algorithm selects subset $s$ whose price is minimum for each iteration until all elements of $U$ are covered.
It is well known that this algorithm becomes best approximation unless NP has polynomial time algorithm.
This means that the minimum set cover as solution constructed by greedy algorithm has tight upper bound 
for the cost, $c(OPT)(\ln{1 \over p}+\gamma)$ when $\gamma$ is Euler-Mascheroni constant 
and $OPT$ is the optimal solution of the minimum set cover problem.

According to (\cite{HalperinK05, CardinalFJ12}), the minimum set cover problem is dual form of the minimum entropy problem, 
which is called the minimum entropy set cover problem.
However, existing researches only focus on the superficial parts of this problem.
With the analysis of the minimum set cover problem in terms of Lebesgue integral, 
we extend the connection of the minimum set cover problem and minimum entropy problem into the unknowns.
We unveil another minimum set cover problem behind the minimum entropy set cover problem.