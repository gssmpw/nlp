\section{Proofs}
\subsection{Proof for Theorem 2.1}
The outline of proof is as follows. 
At first, we prove the base case of Theorem 2.1 from the relationship of self-information and the minimum set cover problem which is described by Lebesgue integral.
Base case means the probability of each element in universe is ${1 \over 2^k}$ and we extends the relationship 
to general case in terms of probability and the cardinality of universe.
At last, we illuminate the relationship between minimum entropy problem and the minimum set cover problem from self-information’s one.


\begin{proof} 
For the first, we explain the basics of the minimum set cover problem (\cite{HalimG16}).
The minimum set cover problem is one of the most fundamental optimization problems, which is NP-complete. 
Given universe $U_\alpha$ , a collection $T$ as a power set of $U_\alpha$($\alpha$ is the identifier for specific event) 
, and cost function $c$, this problem is defined to find a sub-collection of $T$ whose union of all elements covers $U_{\alpha}$ 
and the cost is minimum. 
Greedy algorithm selects subsets whose price is minimum for each iteration until all elements of $U_{\alpha}$ are covered.
According to (\cite{HalimG16}), this greedy algorithm constructs the minimum set cover and 
the tight upper bound of cost function for this solution is $\ln n$ 
when $n$ is the cardinality of universe $U_{\alpha}$.
Assume that $OPT$ is the fixed optimal set cover and $u^{\alpha}_1,u^{\alpha}_2,...,u^{\alpha}_{n}$ 
(elements of $U_{\alpha}$) are sorted in the covered order by greedy algorithm.
If we consider that $C_{k-1}=\{u^{\alpha}_1,u^{\alpha}_2,...,u^{\alpha}_{k-1}\}$ has been covered already, 
then the collection of optimal sets selected by greedy algorithm from now on is $OPT_k$=$\{O_1, O_2,...,O_r\}$. 
For each $O_i$ as a subset of $U_{\alpha}$, the lower bound of residual elements is described by equation (21),
which the equality holds true when all $O_i$s are disjoint to each other. %% 이거 disjoint 한지 다시 확인 
\begin{equation}
\label{lower_bound_msc}
\sum_{i=1}^r |O_i\cap(U_{\alpha}\backslash C_{k-1})| 
\ge |U_{\alpha}\backslash C_{k-1}|
\end{equation}

\begin{equation}
\label{cardinality_residual_candi}
|U_{\alpha}\backslash C_{k-1}| = n-k+1
\end{equation}

By definition, the $price\_per\_item$ (price for each element of universe) is as follows, 
for each $j \in \{1,2,...,r\}$.
\begin{equation}
\label{price_per_item_selection}
price\_per\_item(O_j)=\frac{c(O_j)}{|O_j \cap (U_{\alpha} \backslash C_{k-1})|}
\end{equation}

Since the optimal greedy algorithm selects the optimal set whose $price\_per\_item$ is minimum,
the upper bound of price of $e_k$ is as follows.  
\begin{align}
\label{up_bound_price_per_item}
price(u^{\alpha}_k) \le \frac{c(O_j)}{|O_j \cap (U_{\alpha} \backslash C_{k-1})|} 
\end{align}
The entire cost of $OPT_k$ is as follows.
\begin{equation}
\label{low_bound_cost_optimum}
\begin{split}
c(OPT_k) &= \sum_{j=1}^rc(O_j)\\
         &\ge price(u^{\alpha}_k) \cdot \sum_{j=1}^r|O_j \cap (U_{\alpha}\backslash C_{k-1})|\\
         &\ge price(u^{\alpha}_k) \cdot |U_{\alpha} \backslash C_{k-1}|\\
         &= price(u^{\alpha}_k) \cdot (n-k+1)
\end{split}
\end{equation}

Let $V$ be a collection of subsets selected by greedy algorithm. 
Then the upper bound for cost of $V$ is as follows.
\begin{equation}
\label{up_bound_cost}
\begin{split}
c(V) &= \sum_{k=1}^n price(u^{\alpha}_k) \\
     &\le \sum_{k=1}^n\frac{c(OPT_k)}{n-k+1} \\
     &\le \sum_{k=1}^n\frac{c(OPT)}{n-k+1} \\
     &\le c(OPT) \cdot \sum_{k=1}^n \frac{1}{n-k+1} \\
     &= c(OPT) \cdot \sum_{k=1}^n \frac{1}{k} \\
     &\approx c(OPT)(\ln n + \gamma)
% & = \sum_{k=1}^n \frac{1}{k} \\
% & \approx \ln n + \gamma
\end{split}
\end{equation} 

Now we will describe self-information in terms of Lebesgue integral like equation (1) in Section 2 
and connect the description with the minimum set cover problem.
\begin{align}
\label{lebesgue_integral_entropy}
\begin{split}
\ln {1 \over p(E_\alpha)} &=\int p(E_\alpha)d{1 \over p(E_\alpha)}\\
                          &=\sup\{\sum_{n=1}^{\lceil{1 \over p(E_{\alpha})}\rceil}p(E_{\alpha}) \cdot 1\}\\  
\end{split}
\end{align}
Based on equation (26) and details about minimum set cover, 
we interpret $\sum_{n=1}^{\lceil{1 \over p(E_\alpha)}\rceil}p(E_\alpha)\cdot 1$ as the cost of the minimum set cover problem.
Therefore, the minimum set cover problem corresponding to self-information configures $n$ to $\lceil\frac{1}{p(E_{\alpha})}\rceil$, 
which means that there are $\lceil\frac{1}{p(E_{\alpha})}\rceil$ elements in the universe $U_{\alpha}$ 
of minimum set cover problem by definition of cardinality.
Now, we will see the properties of each element of universe $U_{\alpha}$.
% First, we will see entropy as follows. 

% \begin{equation}
%     \label{ap_eq_7}
%     \begin{split}
%         \mathbb{E}_{E_\alpha}[\ln\lceil\frac{1}{p}\rceil ]
%         &=\sum_{E_\alpha \in E}p_{E_{\alpha}}\cdot\ln(\lceil\frac{1}{p_{E_{\alpha}}}\rceil)\\
%         &=\sum_{E_\alpha \in E}p_{E_{\alpha}}\cdot\mu(U_{\alpha})
%     \end{split}
% \end{equation}
Lebesgue integral for non-negative measure is defined as equation (28) and equation (29). 
\begin{equation}
\label{APP_D_def:lebesgue_integral}
\int f d\mu  = \sup_s\{\int_k a_k \cdot \mu(A_k)\}\ \ (0 \le h \le  f)
\end{equation}
\begin{equation}
    \label{def:lebesgue_measurable_set}
    \begin{split}
        A_k = \{ x | s(x)=a_k \}
    \end{split}
\end{equation}

Based on equation (27), we can match $a_k$ to $p(E_{\alpha})$ and $\mu(A_k)$ to $\mu(u)$ 
whose value is $1$ in equation (27).
\begin{align}
\label{connect_entropy_msc}
\begin{split}
\sup\{\sum_{n=1}^{\lceil{1 \over p(E_\alpha)}\rceil}p(E_\alpha) \cdot 1\} 
&= \sup\{\sum_{n=1}^{\lceil{1 \over p(E_\alpha)}\rceil}p(E_\alpha)\cdot \mu(u)\}
\end{split}
\end{align}
With the correspondence of equation (29) and (30), we can define $U_{\alpha}$ as follows. 
\begin{equation}
    \label{lebesgue_set_for_entropy}
    \begin{split}
        U_{\alpha} = \{ u | s(u)=p({E_{\alpha}}) \}
    \end{split}
\end{equation}
%% E_\alpha 부분이 x와 관련된 변수로 바뀌어야 함 

Now, we prove that there is another minimum set cover problem as predecessor linked with the problem which we have treated until now.
Since probability is Lebesgue measure and $p(r)$ is non-negative measurable function as unit probability, 
we can write $p({E_{\alpha}})$ as follows based on equation (28).
\begin{equation}
    \label{extension_msc}
    \begin{split}
        p({E_{\alpha}}) &= \int_e p(e) d\chi(e \in u)\  \textrm{s.t.}\ p(u)=p({E_{\alpha}})\\
                       &=\sup\{\int_{e}s(e)d\chi(e \in u) \}\ (0 \le s(e) \le p(e)) \\
                       &=\min\max\{ \int_{e}s(e) d\chi(e \in u) \} 
    \end{split}
\end{equation}
Since all outputs of $\max\{ \int_{e}s(e) d\chi(e \in u) \}$ should be bigger than 
or equal to $p({E_{\alpha}})$, 
we can rewrite $p(E_{\alpha})$ to equation (32) with inequality condition.
\begin{equation}
    \label{refined_extension_msc}
    \begin{split}
        p(E_{\alpha}) &= \min_{u \subset U}\{ \int_{e}s(e) d\chi(e \in u) \} \\
        &\textrm{s.t.}\ \ p({E_{\alpha}}) \le p(u)
    \end{split}
\end{equation}
We rewrite equation (33) to equation (34) in terms of arguments
and interpret $U_{\alpha}$ as the collection of $u^{\ast}$s.
\begin{equation}
    \label{element_predec_msc}
    \begin{split}
        u^{\ast} &= \arg\min_{u \subset U}\{ \int_{e}s(e) d\chi(e \in u) \} \\
        &\textrm{s.t.}\ \ p({E_{\alpha}}) \le p(u)
    \end{split}
\end{equation}

Based on this reformulation and the definition of Lebesgue integral, we can derive that each element of $U_{\alpha}$ is a set
since argument of min operation should be a set from the definition of Lebesgue integral.
To summarize, $U_\alpha$ is a collection of $\lceil \frac{1}{p(E_{\alpha})} \rceil$ arguments 
which are sets as solutions of equation (34).
We can also interpret $\min$ as greedy decision based on $\int_{e}s(e) d\chi(e \in u)$ and,
$U_{\alpha}$ is constructed with this greedy algorithm under the constraints that iteration should be repeated  
until unique $\lceil \frac{1}{p({E_{\alpha}})} \rceil$ elements are selected.

We think about the base case that $p(e)=({1 \over 2})^k$ and $p({E_{\alpha}})=(\frac{1}{2})^m\ (m \le k)$.
Since the number of elements in $U_{\alpha}$ is $2^m$ and probability of each element is $(\frac{1}{2})^m$, 
summation of all probabilities in $U_{\alpha}$ becomes 1. 
Therefore, if all elements are disjoint to each other, then $U_{\alpha}$ becomes set cover 
for given universe $U$ as a sample space which has all possible outcomes as elements.
The candidates of predecessor minimum set cover problem are subsets of given universe $U$ 
whose probability is bigger than or equal to $p_{E_{\alpha}}$ 
and cost function is simple summation across probabilities of all elements for each subset. 
Then, this $U_{\alpha}$ becomes greedy solution of the minimum set cover problem, 
since 1 becomes minimum cost of this problem for sample space.
In conclusion, universe of successor minimum set cover problem is solution of another minimum set cover problem 
whose cost function is simple summation of probabilities for all elements in that subset
since $U_{\alpha}$ is also the universe of successor minimum set cover problem.  
\end{proof}


Now, we extend the relationship to general probabilities ($p({E_{\alpha}})$) 
and general cardinalities($\ln\frac{1}{p({E_{\alpha}})}$).
\begin{proof}
Let's return to equation (28) to leverage the property of simple function. 
As a result of equation (28), we can decide $p({E_{\alpha}})$ as the simple function of Lebesgue integral 
$\int p({E_{\alpha}})d{1\over p({E_{\alpha}})}$.
According to (\cite{KwonY12}), every real function can be approximated with simple function 
whose value is $\frac{a}{2^n}$.
Since the probability of each element in $U$ is $\frac{1}{2^m}$, 
the probability of each element $u$ in $U_{\alpha}$ which is same as $p({E_{\alpha}})$ 
is the form of $\frac{a}{2^m}$ since $u$ is the set of elements in $U$.
Therefore, the greedy process can approximate every real function with the elements $e$ in $U$.
Eventually, the relationship between self-information and tight upper bound of minimum set cover 
constructed by greedy algorithm approximately holds true for all real probabilities.
In addition, it is well known that $\ln\lceil\frac{1}{p}\rceil$ becomes $\ln\frac{1}{p}$ in probability
by Asymptotic Equipartition Property(AEP) in information theory.
In conclusion, the relationship in base case also holds true for the case of general probability and cardinality 
in probability.
\end{proof}

As the last step of this proof, we show the weak duality of minimum entropy problem and the chain of 
two minimum set cover problem.

\begin{proof}
For all events $\alpha$, the following inequality about cost of minimum set cover 
($c_{E_\alpha}$) always holds true. 
\begin{equation}
\label{cost_up_bound_event}
c_{E_\alpha} \le \ln {1 \over p({E_\alpha})}
\end{equation}
Since $p_{E_\alpha}$ is positive, the multiplication of $p({E_{\alpha}})$ does not 
change the direction of inequality.
\begin{equation}
\label{weighted_cost_up_bound_event}
p({E_\alpha})c_{E_\alpha} \le p({E_\alpha})\ln {1 \over p({E_\alpha})} \\
\end{equation}
We can get the following inequality by summation across all $\alpha$s.
\begin{equation}
\label{cost_up_bound_event_space}
\sum_{\alpha} p({E_\alpha})c_{E_\alpha} \le \sum_{\alpha}p({E_\alpha})\ln {1 \over p({E_\alpha})} \\
\end{equation}
\begin{equation}
\label{cost_up_bound_expectation}
\mathbb{E}_{\alpha}[c_{E_\alpha}] \le \mathbb{E}_{\alpha}[\ln {1 \over p({E_\alpha})}]
\end{equation}
The RHS is entropy since definition of entropy is the expectation of self-information across all events. 
The LHS is expectation of cost for minimum set cover constructed by greedy algorithm. 
Therefore, the problem to minimize entropy is dual form of problem to maximize 
the expectation of cost for minimum set cover by weak duality theorem. 
Then we can extend the result to general probability and cardinalities same as the proof of Theorem 2.1.
\end{proof}


\subsection{Proof for Theorem 2.2}
Now, we prove by construction that the chain of two minimum set cover problems clusters elements of the universe 
for predecessor minimum set cover problem.
Likewise Theorem 2.1, we first prove Theorem 2.2 for the base case whose price for each element is ${1 \over 2^k}$ 
and probability of given event is ${1 \over 2^m}$ ($k \geq m$).
Then, the greedy algorithm for predecessor selects candidate with simple summation 
across the probabilities of elements until $U$ is covered. 
\begin{proof} We start with predecessor minimum set cover problem.  

\textbf{Predecessor Minimum Set Cover Problem: } If the universe for predecessor is $U=\{e_1,\ldots,e_{|U|}\}$, 
then the set of candidates becomes power set of $U$ whose element is bigger than or equal to probability corresponding to specific event $E_\alpha$. 
Then, the greedy algorithm for predecessor selects candidate with simple summation across the probabilities of elements 
for that candidate as subset of $U$ until $U$ is covered. 
We call this result as $U_\alpha$ which consists of subset whose price is ${1 \over 2^m}$ and $|U_\alpha|$ becomes $2^{k-m}$.
In this base case, each element of $|U_{\alpha}|$ is disjoint to each other since it minimizes the setcover's cost 
(This property approximately extends to general case because every real number can be approximated with ${1 \over 2^k}$).

Next we continues to successor minimum set cover problem.

\textbf{Successor Minimum Set Cover Problem: }
This $U_\alpha$ becomes universe of successor minimum set cover problem.
In this time, the candidates becomes power set of $U_\alpha$ with no constraints.
Then, greedy algorithm for successor selects candidate with optimal measure to each subset for the minimum set cover problem.
In this time, greedy algorithm selects the candidate which includes only one element of $U_{\alpha}$ due to the disjointness of each element.
Then the number of selections becomes $2^m$, which makes supremum of the cost about successor’s set cover $\ln 2^m$ 
(For the base case, we don’t have to think about the supremum. However, we can easily extend the result of base case to general case by doing this).
We define the result as $S_{\alpha}$.
Now, we think about the probability of each element in $S_{\alpha}$.
Since each element of $S_{\alpha}$ consists of the only subset whose has unique element of $U_{\alpha}$, 
the probability of each element in $S_{\alpha}$ is same as the probability of each element in $U_{\alpha}$ 
(This relationship is valid due to the definition of Lebesgue integral formulated as simple function).
Therefore, the probability of each element in $S_{\alpha}$ becomes ${1 \over 2^m}$ which is same as the given probability of event $\alpha$.

For the last, we think about the combination of results of the chain of the minimum set cover problems. 

\textbf{Combination: }
Now, we have constructions for all events $\alpha$.
Then, we select only one element of $S_{\alpha}$ for each $\alpha$ due to the property 
that all elements of $U_{\alpha}$ is equal to each other (This corresponds to each calculation of $p_{\alpha}\ln{1 \over p_\alpha}$). 
This condition usually meets in classification setting since there is implicit assumption for the disjointness across all events.
With disjointness of events,  we can say that minimum entropy problem according to the following definition according to (\cite{Soni12}).

\begin{definition}
\label{def:clustering}
Clustering  is the process of partitioning a dataset into subsets with some selected distance measure
\end{definition}
\end{proof}


\subsection{Example of Theorem 2.2}
We provide the example about the way how the chain of two minimum set cover problems works
and explains about the reason why it is important to reflect on the relationships betweeen disentangled representations.
We think about the situation that find the codebook about characters ($a$, $b$ and $c$)
and the probability of each character is $p(a)={1 \over 2}$ and $p(b)=p(c)={1 \over 4}$.
According to Theorem 3.2, the chain of two minimum set cover problems works for each character.  
Regardless of the characters, the universe of predecessor becomes $\{00, 01, 10, 11\}$.
However, the candidates becomes different due to the discrepancies of probabilities for each character.
These discrepancies make all following matters different. 

\textbf{Clustering for $a$: }
The set of candidates for a becomes 
$\{\{00, 01\}, \{00, 10\}, \{00, 11\}, ..., \{00, 01, 10, 11\}\}$.
The greedy algorithm of predecessor only selects one of the candidates based on the scale of probability 
for each set. 
Since all probabilities are bigger than ${1 \over 2}$ and there are more than 2 candidates set of candidates,
greedy algorithm selects only candidates whose probabilities are ${1 \over 2}$ 
among $\{\{00, 01\}, \{00, 10\}, \{00, 11\}, \{01, 10\}, \{01, 11\}, \{10, 11\}$).
We assume that greedy algorithm selects $\{00, 10\}$ at first and $\{01, 11\}$ for the next, like Figure 5 
(This is possible since greedy algorithm does not consider the relationship between elements of $U$ by itself).
Then, the set cover $U_a$ ($\{\{00, 10\},\{01, 11\}\}$) becomes new universe of successor minimum set cover problems. 
In the case of successor, the candidates becomes power set of $U_{\alpha}$ excluding $\emptyset$.
Then, the greedy algorithm which considers about the overlaps selects the subset 
which consists of the only element in $U_{\alpha}$ like Figure 5 for each iteration. 
Then, the output setcover of successor becomes $\{\{\{00,10\}\},\{\{01,11\}\}\}$ 
due to the disjointness of each element to the other elements in $U_a$.


\textbf{Clustering for $b$ and $c$: }
The same process also proceeds in the case of characters $b$ and $c$ for different probabilities and candidates. 
For the character $b$ and $c$, the candidates of predecessor becomes $\{\{00\}, \{01\}, \{10\}, \{11\},....,\{00,01,10,11\}\}$
$(P(\{00,01,10,11\})\backslash \{ \emptyset \})$ since probabilities for $b$ and $c$ are ${1 \over 4}$ and 
the probability for each candidate should be bigger than or equal to the probability of corresponding character.
Likewise character $a$, the greedy algorithm of predecessor selects the candidates whose probabilities are ${1 \over 4}$.
We also assume that greedy algorithm selects $\{00\}, \{01\}, \{10\}$ and $\{11\}$ for each iteration,
which leads to set cover $\{\{00\}, \{01\}, \{10\}, \{11\}\}$ as $U_{b}$ and $U_{c}$
These set covers becomes universe again for the next minimum set cover problem for character $b$ and $c$.
The candidates for successor become the power set of $U_b$ and $U_c$ without $\emptyset$. 
The successor's greedy algorithm also selects the subset of only one element in $U_b$ and $U_c$
in consideration with overlaps for each iteration like Figure 6 and Figure 7.
Then, the output setcover of successor becomes $\{\{\{00\}\},\{\{01\}\},\{\{10\}\},\{\{11\}\}\}$

\textbf{Selection of Clusters for $a$, $b$ and $c$: }
We have the following set covers for $a$, $b$, and $c$.
\begin{align}
\label{cluster_result}
    S(a)&=\{\{\{00,10\}\},\{\{01,11\}\}\}\\
    S(b)&=\{\{\{00\}\},\{\{01\}\},\{\{10\}\},\{\{11\}\}\}\\
    S(c)&=\{\{\{00\}\},\{\{01\}\},\{\{10\}\},\{\{11\}\}\}
\end{align}
Now, we select only one element in set cover for each character 
under the constraint that each element is disjoint to each other. 
For example, we select $\{\{00,10\}\}$ for a, $\{\{01\}\}$ for b and $\{\{11\}\}$ for c
like Figure 8.  

% In general, the optimal coding for this setting, is $\{\{00,01\}\}$ for $a$ 
% , $\{\{10\}\}$ for $b$ and $\{\{11\}\}$ for $c$.
% However, our constructed result is different from the optimal coding. 
% This is due to the absence of information about relationships between disentangled representations. 

% In the process of design for optimal coding, we already know the relationships between disentangled representations
% ($00, 01, 10, 11$). 
% In general, we set the distance between bit sequences as the number of different bits. 
% In addition, we assume that the starting point of read for bit encoding is left-most position of bit string. 
% These pieces of information inform us that $00$ is closer to $01$ than $10,11$ and $00$ is closer to $10$ than $11$. 
% These relationships hold true for bit strings other than $00$. 

\textbf{Analysis: }
Generally, an optimal coding book for bit strings is designed under the condition 
that the distance between bit strings and the context that bits are read 
from the leftmost position of the bit string are given.
As a result, the bit string 00 is closest to 01, followed by 10, and finally 11.
In contrast, the provided example corresponds to a situation 
to design the optimal coding book, given only minimum entropy.
As a result, the coding book constructed in the example differs from the optimal coding book.
In other words, an optimal coding book cannot be designed based solely on minimum entropy
(additional information about the relationships between bit strings is required to construct the optimal coding book).

In training a neural network, no information is provided about the relationships between their disentangled representations unlike designing an optimal coding book for bit strings.
While supervised signals generally provide information about the relationships 
between samples or between samples and labels, 
the absence of direct labels for the disentangled representations means that their relationships remain unknown.
This remains true even in InfoNCE, which has become the new standard.
In other words, without additional information about the relationships between disentangled representations, 
InfoNCE cannot leverage these relationships, even though it can utilize the relationships between samples.
However, the floating boundary problem causes the relationships between disentangled representations 
to vary based on the context (e.g., decoding from the front or back), 
making it difficult to provide direct supervised signals for these relationships.
To address this issues, we have developed URECA.

\newpage
\begin{figure}[!h]
\centering
\includegraphics[width=0.95\columnwidth]{sections/figures/examples/th2_example_a.png} 
\caption{Clustering behind Minimum Entropy Problem for a}
\label{fig_ex1}
\end{figure}

\begin{figure}[!]
\centering
\includegraphics[width=0.95\columnwidth]{sections/figures/examples/th2_example_b.png} 
\caption{Clustering behind Minimum Entropy Problem for b}
\label{fig_ex2}
\end{figure}

\newpage
\begin{figure}[!t]
\centering
\includegraphics[width=0.994\columnwidth]{sections/figures/examples/th2_example_c.png} 
\caption{Clustering behind Minimum Entropy Problem for c}
\label{fig_ex3}
\end{figure}

\begin{figure}[!]
\centering
\includegraphics[width=0.998\columnwidth]{sections/figures/examples/th2_example_d.png} 
\caption{Cluster Selection for a, b and c }
\label{fig_ex4}
\end{figure}

\newpage
\subsection{Proofs for Theorem 3.1 and Corollary 3.2}

To prove Theorem 3.1, we first define Usually Positive Limit which is widely used in our proofs. 
\begin{definition}
\label{def:up-limit}
For an sequence $a_n$, if there exists $N \in \mathbb{N}$ such that 
for $\forall n \geq N$, $|a_n-\alpha| < \epsilon$ ($\forall \epsilon \in \mathbb{R}^+$), 
then $a_n$ converges to constant $\alpha$ in terms of Universally Positive Limit (UP-Limit).
The notation is as follows. 
\begin{align}
    \lim^{UP}_{n \rightarrow \infty}a_n=\alpha  
\end{align}
\end{definition}

\begin{lemma}
\label{lem:adj_convergence}
if the UP-Limit of $p(C^t_{i,j})$ converges to ${1 \over |J|}$ (J is the set of cluster indexes ),
then the UP-Limit of $evid(C^{t+1}_{i,j})-evid(C^{t+1}_{a,b})$ converges to zero
($\forall i,j, a, b \in J$). 

\begin{align}
    \begin{split}
        \lim^{UP}_{t \rightarrow \infty}p(C^{t}_{i,j})={1 \over |J|} \implies
        \lim^{UP}_{t \rightarrow \infty}(evid(C^{t+1}_{i,j}-evid(C^{t}_{a,b})))=0
        \ \ (\forall i,j,a,b \in J)
    \end{split}
\end{align}
\end{lemma}
\begin{proof}
By definition of UP-Limit, the following statements holds true for the sequence $p(C_{t,j})\ (\forall j \in J)$.
There exists $T \in \mathbb{N}$ 
such that $|p(C_{t,j})-{1 \over |J|}| < \epsilon\ (\forall \epsilon \in \mathbb{R}^+)$.
\begin{align}
    \begin{split}
        &\exists T \in \mathbb{N}\\
        s.t. |p(&C^t_{i,j})-{1 \over |J|}| < \epsilon \\
        (\forall t &\geq T,\ \forall \epsilon \in \mathbb{R}^+) 
    \end{split}
\end{align}

Based on the equation (44), we can derive the following inequalities about $evid(C^t_{i,j})$. 
\begin{align}
           &{1 \over |J|} - \epsilon < p(C^t_{i,j})  <  {1 \over |J|} +\epsilon (\forall \epsilon \in \mathbb{R}^+)\\
&\implies   \ln{{1 \over |J|} - \epsilon \over p(y)} < \ln{p(C^t_{i,j})\over p(y)} < \ln{{1 \over |J|} + \epsilon \over p(y)}\\
&\implies   \ln{{1 \over |J|} - \epsilon \over p(y)} < evid(C^t_{i,j}) < \ln{{1 \over |J|} + \epsilon \over p(y)}\\
&
\end{align}
With these inequalities, we can obtain the following relationship about the difference $evid(C^{t+1}_{i,j})$ and $evid(C^t_{a,b})$.
\begin{align}
    -\ln{{1\over|J|} + \epsilon \over {1 \over |J|} - \epsilon} < evid(C^{t+1}_{i,j}) - evid(C^t_{a,b}) 
    < \ln{{1\over|J|}+\epsilon \over {1 \over |J|} -\epsilon}
\end{align}

We can derive following statement from equation (49) when, 
$\delta = \ln{{1 \over |J|}+\epsilon \over {1 \over |J|} -\epsilon} \ \  (\forall \epsilon \in \mathbb(R)^{+})$. 
\begin{align}
    \exists T &\in \mathbb{N}\\
    s.t.\ \ |evid(C^{t+1}_{i,j})-e&vid(C^t_{a,b})| < \delta \ \ (\forall \delta \in \mathbb(R)^+)
\end{align}
By the definition of universally positive limit, $evid(C^{t+1}_{i,j})-evid(C^{t}_{a,b})$ converges to zero as $t$ goes to $infty$
\begin{align}
    \lim_{t\rightarrow \infty}^{UP}(evid(C^{t+1}_{i,j})-evid(C^{t}_{a,b}))=0
\end{align}
Therefore, the only condition that the universally positive convergence of $P(C^t_{i,j})$ to ${1 \over |J|}$
implies that the sequence of $P(C^t_{i,j})$ is Uniform Cauchy.
\end{proof}

\begin{lemma}
\label{lem:adj_lip_continuous}
If the UP-Limit of $evid(C^{t+1}_{i,j})-evid(C^t_{a,b})$ converges to 0 ($\forall i,j,a,b \in J$),
then $evid(C^t_{i,j})$ is $\ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon}$-Lipschitz Continuous
($\forall \epsilon \in \mathbb{R}^+$).
\end{lemma}
\begin{proof}
Based on equation (49), we can extend the relationship between difference of evidences from adjacent timesteps to general timesteps
for all $i,j,a,b \in J$.  
\begin{align}
    \begin{split}
        -1\cdot(\ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon}) 
        < evid(C^{t+1}_{i,j}) &- evid(C^{t}_{a,b}) < 1\cdot(\ln{{1 \over |J|} + \epsilon \over {1 \over |J|}-\epsilon})\\
        -2\cdot(\ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon}) 
        < evid(C^{t+2}_{i,j}) &- evid(C^{t}_{a,b}) < 2\cdot(\ln{{1 \over |J|} + \epsilon \over {1 \over |J|}-\epsilon})\\
                              &\dots\\
        -L\cdot(\ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon}) 
        < evid(C^{t+L}_{i,j}) &- evid(C^{t}_{a,b}) < L\cdot(\ln{{1 \over |J|} + \epsilon \over {1 \over |J|}-\epsilon})\\        
    \end{split}
\end{align}
We can rewrite the last inequality in equation (53) as follows. 
\begin{align}
    |{evid(C^{t+L}_{i,j}) - evid(C^t_{a,b}) \over (t+L) - (t)}| < \ln {{1 \over |J|} +\epsilon \over {1 \over |J|} - \epsilon}
\end{align}
In conclusion, $evid(C^{t}_{i,j})$ is $\ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon}$ Lipschitz-continuous 
by the definition of Lipschitz-continuous~(\cite{KavoshDM18}).  
\end{proof}

However, the $\ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon}$ Lipschitz-continuity does not assure the uniform convergence. 
So we dervie the following lemma.

\begin{lemma}
\label{lem:uniform_cauchy}
if the UP-Limit of $P(C^t_{i,j})$ converges to ${1 \over |J|}$ ($\forall i,j,a,b \in J$) 
and $evid(C^t_{i,j})$ is $\alpha\cdot\ln{{1\over|J|}+\epsilon \over {1\over|J|}-\epsilon}$-Lipschitz continuous 
($\forall \alpha \in [0,{{1 \over |J|}-\epsilon \over {1 \over |J|}+\epsilon})$),
then $evid(C^t_{i,j})$ is uniform cauchy sequence of $t$.
\end{lemma}
\begin{proof}
By Lemma D.3, the following inequality holds true. 
\begin{align}
    -L\cdot(\ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon})
    < evid(C^{t+L}_{i,j})-evid(C^{t}_{a,b})
    <L\cdot(\ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon})\ \ (\forall i, j, a, b \in J)
\end{align}
In general, the following inequality holds true. 
\begin{align}
    {d \over dx}(\ln x)={1 \over x} < 1 = {d \over dL}(L)\ (x > 1)
\end{align}
Equation (56) implies $L\cdot \ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon}$ diverges to positive infinity.
\begin{align}
    \lim_{L\rightarrow \infty, \lim \epsilon \rightarrow 0} (L\cdot\ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon})=\infty 
\end{align}

However, if we multiply $\alpha \in [0, {{1 \over |J|}-\epsilon \over {1 \over |J|}+\epsilon}]$ 
with $L\cdot \ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon}$, 
then $(\alpha L)\cdot \ln{{1\over|J|}+\epsilon \over {1 \over ||J}-\epsilon}$ converges to 0 for universally positive limit. 
\begin{align}
    \lim^{UP}_{L \rightarrow \infty, \epsilon \rightarrow 0}|(\alpha L)
    \cdot \ln{{1 \over |J|} + \epsilon \over {1 \over |J|} - \epsilon}| = 0 
\end{align}
This is because the following relationship.
\begin{align}
    \begin{split}
       &{d \over d x}(\ln x) = |{1 \over x}| > \alpha = ({d \over dL}(\alpha \cdot L)) \\
        (\forall &x \in [0, {{1 \over |J|} +\epsilon \over {1 \over |J|}-\epsilon}),
        \forall \alpha \in [0, {{1 \over |J|}-\epsilon \over {1 \over |J|}+\epsilon}))
    \end{split}
\end{align}
Therefore, the conditions of universally positive convergence of $P(C^t_{i,j})$ and
$(\alpha L)\cdot \ln{{1 \over |J|}+\epsilon \over {1 \over |J|}-\epsilon}$-continuous imply that 
$|evid(C^{t+L}_{i,j})-evid(C^t_{a,b})| < \alpha\ (\forall \alpha \in \mathbb{R}^+)$.
By the definition of uniform cauchy condition, the sequence $evid(C^t_{i,j})$ is uniform cauchy. 

\end{proof}

Based on these Lemmas, we prove the Theorem 3.1 that $evid(C^t_{i,j})$ converges uniformly.
We first show the pointwise convergence of $evid(C^t_{i,j})$.
\begin{proof}
By Lemma D.4, $evid(C^t_{i,j})$ is uniform cauchy under the conditions of Theorem 3.1. 
The, for each $i, j \in J$, the real sequence $evid(C^t_{i,j})$ is Cauchy, 
so it converges by completeness of $mathbb{R}$.
We define $evid(C_{i,j})$ as follows, and then $evid(C^t_{i,j}) \rightarrow evid(C_{i,j})$ pointwise.
\begin{align}
    evid(C_{i,j}) = \lim_{t \rightarrow \infty} evid(C^{t}_{i,j})
\end{align}

Next, we prove the uniform convergence of $evid(C^t_{i,j})$.
If $\gamma > 0$, then we can choose $T \in \mathbb{N}$ (depending only on $\gamma$) such that 
$|evid(C^m_{i,j})-evid(C^n_{a,b})| < {\gamma \over 2}\ (for\ \forall i,j,a,b \in J and m, n > T)$.
In this case, we have the following inequality. 
\begin{align}
    \begin{split}
        |evid(C^m_{i,j})-evid(C_{i,j})| 
        &\leq |evid(C^m_{i,j})-evid(C^n_{a,b})| + |evid(C^n_{a,b})-evid(C_{i,j})|\\
        &< {\gamma \over 2} + |evid(C^n_{a,b})-evid(C_{i,j})|
    \end{split}
\end{align}
Since $evid(C^n_{a,b})\rightarrow evid(C_{a,b})$ as $m\rightarrow \infty$, 
we can choose $m > T$ for the following inequality. 
\begin{align}
    |evid(C^n_{a,b})-evid(C_{a,b})| < {\gamma \over 2}\ (\forall a, b \in J)
\end{align}
Since there is no constraint about $a, b$ of $C_{a,b}$ for the following condition,
\begin{align}
    \lim^{UP}_{t \rightarrow \infty}P(C^t_{a,b})={1 \over |J|} (\forall a,b \in J)
\end{align}
, we can rewrite the equation (62) as follows. 
\begin{align}
    |evid(C^n_{a,b})-evid(C_{i,j})| < {\gamma \over 2}\ (\forall i,j,a,b \in J)
\end{align}
Equation (61) derives the following,
\begin{align}
    |evid(C^{m}_{i,j})-evid(C_{i,j})| < \gamma\ (\forall j \in J)    
\end{align}
Equation (65) means that $evid(C^{m}_{i,j})$ uniformly converges to $evid(C_{i,j})$.


\end{proof}

At last, we prove the Corollary 3.2 as a refined version of Theorem 3.1. 
\begin{proof}
As we have done so far, We define $evid(C^{t+1}_{i,j})$ and $evid(C^t_{a,b})$ as follows.
\begin{align}
    evid(C^{t+1}_{i,j}) &= \ln{p(C^{t+1}_{i,j}) \over p(y)}\\
    evid(C^t_{a,b}) &= \ln{p(C^t_{a,b}) \over p(y)}
\end{align}
For $evid(C^{t+1}_{i,j})$ and $evid(C^t_{a,b})$, the following holds true by Jenson's Inequality. 
\begin{align}
    \begin{split}
        evid(C^{t+1}_{i,j}) &= \ln{p(C^{t+1}_{i,j}) \over p(y)}\\
                        &= \ln E_{(a,b) \sim p(C^{t+1}_{i,j}|C^t_{a,b})}[{p(C^t_{a,b}) \over p(y)}]\\
                        &\geq E_{(a,b) \sim p(C^{t+1}_{i,j}|C^{t}_{a,b})}[\ln{p(C^t_{a,b})\over p(y)}]
    \end{split}
\end{align}

The equality of equation (68) holds true when all arguments of convex function are same 
$(\forall a, b \in J)$. 
However, if $C^{t+1}_{i,j}$ and $C^{t}_{a,b}$ are mutually exclusive, then 
$p(C^{t+1}_{i,j}|C^{t}_{a,b})$ becomes zero. 
In this case, the term $p(C^{t+1}_{i,j}|C^{t}_{a,b})\ln {p(C^{t}_{a,b}) \over p(y)}$
is ceared out in the convex combination. 
Since these kinds of clusters do not impact on the equality condition after some threshold,
we conclude that we can distill the entire index set $J$ for all clusters
to $K$ like Corollary 3.2.
The definition of $K$ is as follows. 
\begin{align}
    K = \{j | \lim^{UP}_{t \rightarrow \infty} p_t(C^{t+1}_{i,j}|C^{t}_{a,b})=0 ,j \in J\} 
\end{align}
Since the only difference between Theorem 3.1 and Corollary 3.2 is replacement $J$ with $K$, 
the Corollary 3.2 also holds true. 
\end{proof}