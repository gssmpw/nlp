
\section{Problem Analysis}
At first, we analyze the minimum entropy problem in the lens of Lebesgue integral (Theorem 2.1).
After then, we show that model learns to cluster the elements by minimum entropy problem (Theorem 2.2).
These theorems drive us to notice that the ignorance of disentangled relationships leads to shifted initialization cascade. 

\begin{theorem}
\label{thm:min_entropy_min_setcover}
Minimum entropy problem is dual to the combination of chains of two minimum set cover problems with greedy algorithm for disentangled representations. 
\end{theorem}
\begin{align}
\label{eq:thm_min_entropy_min_setcover}
\ln {\frac{1}{p(E_\alpha)}} &=\int p(E_\alpha)d{\frac{1}{p(E_\alpha)}}\\
                          &=\sup\{\sum_{n=1}^{\lceil{\frac{1}{p(E_\alpha)}}\rceil}p(E_\alpha)\}\\  
\begin{split}
                          &=\sup\{\sum_{n=1}^{\lceil{\frac{1}{p(E_\alpha)}}\rceil}\min_{u \subset U}\int_e s(e)d\chi(e \in u)\}\\
                          &(s.t.\ p(E_\alpha) \leq p(u))
\end{split}
\end{align}
As we mentioned, we examine minimum entropy problem in the lens of Lebesgue integral.
The proof of Theorem 2.1 is provided in Appendix D.1.
$u$ is the subset of $U$ which is the universe of disentangled representations $e$ for the predecessor minimum set cover problem.
The predecessor minimum set cover problem is the first problem of the chain of two minimum set cover problems
(We call the last problem of the chain as successor minimum set cover problem).
$\chi(e)$ is the characteristic function which describes whether $e$ is the element of $u$ (Appendix B).
$s(e)$ is the simple function which measures the size of $e$.

The analysis starts from equation (2.1) which describes the self-information in terms of Lebesgue integral to probability.
We can rewrite the integral form of equation (2.1) to supremum form of equation (2.2) based on the definition of Lebesgue integral 
for non-negative measure (Appendix B).
With properties of probability as Lebesgue measure, this supremum form hints that self information becomes the tight upper bound 
for the cost of solution to the chain of minimum set cover problems equation (2.3). 
This self information linearly combines to form the entropy.
Linear combinations do not change the direction of inequality in equation (36) and (37).
As a result, minimum entropy problem is dual form of the combination of chains of two minimum set cover problems by weak duality theorem.

Theorem 2.2 relates minimum entropy problem to clustering based on the previous analysis.
We provide the details about proof and examples in Appendix D.2 and Appendix D.3.
\begin{theorem}
\label{thm:min_entropy_cluster}
Minimum entropy problem is equivalent to the problem which clusters the disentangled representations 
to minimize the expected cost of clustering given the probabilities for each event.
\end{theorem}
According to proof of Theorem 2.2, 
the chain of minimum set cover problems constructs the set of clusters for each event.
At first, the predecessor problem gathers subsets of disentangled representations with greedy algorithm and 
initializes the universe of successor problem with those subsets like $\{\{00,10\},\{01, 11\}\}$ from 1st MSC in Figure 5.
Then, the successor also constructs the solution with greedy algorithm like $\{\{\{00, 10\}\},\{\{01, 11\}\}\}$ from 2nd MSC in Figure 5.
This, in turn, initializes the combination process for solutions of each chain of problems.
For all events, minimum entropy problem does the same thing for different events.
In the process of combination, computation selects only one element in the solution of chained problems for each event
like $\{\{00, 10\}\}$ for event $a$ in Figure 8.
After then, it combines them to new family of subsets like $\{\{\{00, 10\}\}, \{\{01\}\}, \{\{11\}\}\}$, 
which can be interpreted as clusters of disentangled representation by the definition of clustering (Definition D.1 in Appendix D.2).

From Theorem 2.1 and Theorem 2.2, we can construct the minium entropy problems 
with the combination of the minimum set cover problems.
The construction unveils the internal mechanism of minimum entropy problem, 
which reveals the limitation of minimum entropy problem in terms of clustering.
In the process of clustering, predecessor makes a series of greedy decisions for the construction of solution according to the measure 
that simply sums all probabilities of elements in specific subset.
This simple summation of elements means that the decision criteria does not consider the relationships between elements 
(disentangled representations) but only considers about the size of each element itself 
unlike general clustering which considers about the distance between elements. 
% unlike successor whose greedy decision considers about the relationships between elements (subset of disentangled representations) by dividing the simple summation with the parts of those subsets overlapped with other subsets
%% 이거는 근데 element 사이의 관계를 고려하는 게 아니지 않나? -> subset 사이의 관계를 고려하는 거지? -> 이러면 successor로 부터 disentangled representation 사이의 관계를 고려하지 않는다라는 얘기를 유도하기는 어려울 거 같음 -> 일반적인 clustering과 연관지어 보자 -> 근데 clustering의 distance랑 simple sum이랑 연관지 잘 안됨 
%% -> 

The analysis also holds true for InfoNCE~(\cite{DenYO18}) since it considers the only relationships between given samples
which are mixtures of disentangled representations.
The coarse-grained relationships limit for model to capture the fine-grained relationships between disentangled representations.
This becomes apparent in the situation that fragments in same sample can sometimes be less relevant to each other than fragments of different sample.
For example, Code Fragment (C) in Figure 1 is less relevant to the Code Fragment (A) of same function than Code Fragment (C) of different function.  
Figure 3 demonstrates that shifted initialization cascade comes from the ignorance of relationships between disentangled representations 
as encoding of these fragments. 

