\section{Union-find based Recursive Evidence Clustering Algorithm (URECA)}

\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{sections/figures/methodology/figure2.png} 
% Reduce the figure size so that it is slightly narrower than the column.
% \includegraphics[width=1\textwidth,height=15cm]{figures/figure2.png} % Reduce the figure size so that it is slightly narrower than the column.
\caption{
Overview of URECA
}
\label{fig2}
\end{figure*}

URECA is a new algorithm to cluster disentangled representations based on the relationships between disentangled representations.
URECA implements the iterative steps of initialization, update and recursion in other clustering algorithms (k-means etc.) 
with the combination of calculations for numeric values based on simulation trick. 
Simulation trick is the trick to replace specific task to simple numeric computations. 
For example, a simulation implements moving a box only by changing its position 
from (1, 2, 3) to (4, 5, 6) with the addition of 3 for each element of (1, 2, 3).
Figure 2 illustrates the algorithm of URECA in terms of Real World Scenario and Simulation Scenario.  
We also provide pseudo code of URECA in Appendix C and details of analyses in Appendix E.


\subsection{Initializatoin Step}
At first, URECA calculates weight of evidence for initial cluster (logits) like equation (4).
$(q_i,c_i)$ is the given pair of (query, code) and $f$ is neural network.
\begin{align}
\label{initial_evidence_cluster}
\begin{split}
    evid_0(C^0_{i,j})&=\ln{ \frac{p(q_i,c_j)}{p(y)}}\\
                     &= f(q_i) \cdot f(c_j)
\end{split}
\end{align}
In the context of code search, the estimation of weight of clusters leads to Fragment Initialization and Cluster Initialization.
We can justify this statement since the estimation of logits leads to estimation of conditional probability and entropy, 
which respectively correspond to Fragment Initialization and Cluster Initialization.

\textbf{Fragment Initialization: }
We can apply the analysis of Theorem 2.1 and Theorem 2.2 to conditional probability as output of neural network
in the scenario of training neural network with minimum entropy problem.
Neural network calculates conditional probability based on logits and softmax.
Conditioning random variable in this conditional probability prerequisites the inverse image as subset of universe of fragments. 
The conditional probability configures this subset of universe of fragments as universe of the predecessor minimum set cover problem.
In other words, forward propagation and dot product adopt the only query-relevant fragments in code search
and do fragment initialization with them.
This conditional probability also leads to configuration of candidates and criteria 
for greedy decision to the chain of minimum set cover problems

\textbf{Cluster Initialization: }
Then, the calculation of entropy clusters the query-relevant fragments like Figure 5, 6, 7 and 8. 
The chain of two minimum set cover problems operate to cluster disentangled representations for each event.
Then, the calculation of weighted self-information($p\ln{\frac{1}{p}}$) selects only one cluster 
in the output of the chain for each event.
This statement can be justified since all elements of solution have probability equal to each other.  
At last, the cluster initialization completes by gathering the selected clusters across all events. 


\subsection{Update Step}
Update step consists of dynamics estimation and transport computation.
URECA esitmates dynamics of disentangled representations with attention scores of queries. 
After then, URECA estimtates the transported parts of logits through linear combination of logits with these dynamics.
Then, it subtracts the parts of logits from logits of dropped cluster and adds the value to logits of survived cluster.
The subtraction and addition simulate to transport evidences/fragments from dropped cluster to survived cluster.

Simulation trick enables URECA to simulate the transport of fragments with numeric calculations.
The following properties of Lebesgue measure enable simulation trick to operate with no errors (equation (3.2)).
\begin{align}
\label{lebesgue_property_union_diff}
\begin{split}
\mu(A \cup B) &= \mu(A)+\mu(B) - \mu(A \cap B)\\ 
\mu(A-&B)=\mu(A)-\mu(A \cap B)
\end{split}
\end{align}
These properties naturally connects the numeric calculations for Lebesgue measure to transport elements between measurable set $A$ and $B$.
% The connection enables to formulate the transport for semantic fragment $R_{s,t}$ from source cluster $C_s$ to target cluster $C_t$
% like equation (6).
We split the transport into two sub-units, separation from $C_s$ (subtraction) and clump into $C_t$ (addition).
To begin with, we can formalize the separation to separate semantic fragment $R_{s,t}$ from $C_s$ as follows.
\begin{align}
\label{lebesgue_property_diff_fragment}
\begin{split}
\mu(C_s-R_{s,t})  &= \mu(C_s)-\mu(C_s \cap R_{s,t})\\
                  &=\mu(C_s)-\mu(R_{s,t})
\end{split}
\end{align}
Then, we can also derive the following formula for addition to clump $R_{s,t}$ into $C_t$.
\begin{align}
\label{lebesgue_property_union_fragment}
\begin{split}
\mu(C_t \cup R_{s,t})=\mu(C_t)+\mu(R_{s,t})-\mu(C_t \cap R_{s,t})
\end{split}
\end{align}
The estimation of $\mu(R_{s,t})$ enables to subtract the estimation from $\mu(C_s)$ and add it to $\mu(C_t)$, 
which simulates the transport subset of semantic fragments($R_{s,t}$) from $C_s$ to $C_t$
(In this case, $\mu(C_t \cap R_{s,t}) $ should be zero by the assumption that specific fragment/evidence belongs 
to the only one cluster at specific time).
In other words, we can accurately implement the update of general clustering algorithm 
with the sequence of operations for Lebesgue measure.
Probability meets the basic properties in equation (3.2) since it is one of the Lebesgue measures.
Therefore, we can formulate the transport as follows with Bayesian theorem.
\begin{align}
\label{evidence_for_prob}
\begin{split}
evid(C^{t+1}_{i,i})&=E_{j\sim p(C^{t+1}_{i,i}|C^t_{j,j})}[p(C^t_{j,j})]
\end{split}
\end{align}
However, probability does not operate well for the training of neural network due to the regularized property in some settings, 
especially for few shot learning.
To avoid the excessive regularization, 
we replace the probabilities ($p(C^t_{j,j})$) to logits ($\ln{\frac{p(C^t_{j,j})}{p(y)}}$).
\begin{align}
\label{evidence_for_logits}
\begin{split}
evid(C^{t+1}_{i,i})=E_{j\sim p(C^{t+1}_{i,i}|C^t{j,j})}[\ln{\frac{p(C^t_{j,j})}{p(y)}}]
\end{split}
\end{align}
This naive replacement withdraws the theoretical guarantees about the accurate simulation to transport 
since logit is not the Lebesuge measure.
As a result of efforts to theoretical guarntees for this update rule (equation (3.6)), 
we specify the conditions for the expectation of logits to become unbiased estimator with Theorem 3.1 and Corollary 3.2 in the next sub-section.

In this update rule, the dynamics becomes $p(C^{t+1}_{i,i}|C^t_{j,j})$ since evidence is updated 
based on the conditional probability.
\begin{align}
\label{orig_dynamics}
\begin{split}
dyn(C^{t+1}_{i,i},C^t_{j,j})=p(C^{t+1}_{i,i}|C^t_{j,j})
\end{split}
\end{align}
We assume that the conditional probability as dynamics is stationary for each time step of update.
\begin{align}
\label{stationary_dynamics}
\begin{split}
dyn(C^{t+1}_{i,i},C^t_{j,j})&=p(C^{t+1}_{i,i}|C^t_{j,j})\\
                          &=p(C_{i,i}|C_{j,j})\ \ (\forall t \in N)
\end{split}
\end{align}
We also hypothesize that query $q_i$ is the centroid of cluster $C_{i,j}$ for all $j$.
As a result, we calculate the dynamics with attention scores across queries
since they represent the properties of corresponding clusters as centroids. 
\begin{align}
\label{dynamics_impl}
\begin{split}
p(C_{i,i}|C_{j,j})&=p(q_{i}|q_{j})\ \ (\forall t \in N)
\end{split}
\end{align}

For the next sub-section, we release the Stationary Assumption to Thresholdly Updatable Stationary Assumption. 
We model this stationary conditional probability with attention score for queries
since the softmax operation allows to estimate the conditional probability in the process of calculation for attention scores.
We provide the reason why we only focus on the query in Appendix E.1.


\subsection{Recursion Step} 
% URECA simulates iteration for initialization and update of other clustering algorithms in recursive manner.
% It recursively initializes the weights of clusters using only logits of the survived group 
% and repeatedly update these weights according to update rule of URECA.
% This process is recursive since URECA only focuses on the survived group 
% which is a subset of previous initialization. 
As mentioned earlier, we use logits instead of probabilities  
which impede training of neural network due to excessive regularization.
However, logits do not meet the properties of Lebesgue measure.
This unsatisfaction causes errors in the process of transport-based estimation for next time step’s logits.
Probability-based transport decides next time step's logits as follows. 
\begin{align}
\label{accurate_transport}
\begin{split}
\ln{p_{t+1}(C^{t+1}_{i,i}) \over p(y)}= \ln E_{j\sim p(C^{t+1}_{i,i}|C^t_{j,j})}[{p(C^t_{j,j})\over p(y)}]
\end{split}
\end{align}
In contrast, logit-based transport estimates next time step's logits as follows. 
\begin{align}
\label{estiamted_transport}
\begin{split}
evid(C^{t+1}_{i,i})=E_{j \sim p_{t+1}(C^{t+1}_{i,i}|C^t_{j,j})}[\ln{p_t(C^t_{j,j}) \over p(y)}]
\end{split}
\end{align}
According to Jensen’s Inequality, equation (3.10) and equation (3.11), the following inequality holds true 
between original transport and estimated transport.

\begin{align}
\label{estimation_error}
\begin{split}
\ln{p_{t+1}(C^{t+1}_{i,i}) \over p(y)}   \ge E_{j \sim p(C^{t+1}_{i,i}|C^t_{j,j})}[\ln{p_{t}(C^t_{j,j})\over p(y)}]
\end{split}
\end{align}
If we scrutinize the inequality, the estimated logit is smaller than original logit in general, 
which means there are positive errors for the estimation.
However, the error may disappear when all of the $p_t(C^t_{j,j})$s are equal to each other for all $j$s, 
since it satisfies the equality condition of Jensen’s inequality.
% This also makes all $\ln {p_t(C^t_{j,j}) \over p(y)}$s and $\ln{p_{t+1}(C^{t+1}_{i,i})\over p(y)}$s 
% equal to each other for all $i$ and $j$.
From this intuition, we derive Theorem 3.1 that describes the conditions for the uniform convergence of estimated logits to each other.
(UP-Limit is defined in Appendix D.4 as Definition D.2).
Uniform convergence assures that the error of estimation disappears as update proceeds.
The UP-Limit and Lipchitz continuity conditions for uniform convergence 
ensure that transported logits serves as unbiased estimator for the original logits.
We provide the proof of Theorem 3.1 in Appendix D.4. 
\begin{theorem}
\label{thm:uniform_convergence}
If the UP-Limit of $p_t(C^t_{j,j})$ converges to ${1 \over |J|}$ and 
$\ln {p_t(C^t_{j,j}) \over p(y)}$ is 
$\alpha\cdot\ln{{1 \over |J|} +\epsilon \over {1 \over |J|}-\epsilon}$-Lipschitz continuous 
($\forall \alpha \in [0,{{1 \over |J|} -\epsilon \over {1 \over |J|} +\epsilon})$ 
and $\forall \epsilon \in [0,\infty)$), 
then $\ln {p_t(C^t_{j,j}) \over p(y)}$ converges uniformly to each other as $t \rightarrow \infty$.
\end{theorem}
 
For the uniform convergence, we don’t have to care about the term whose $p_{t+1}(C^{t+1}_{i,i}|C^t_{j,j})$ 
becomes zero, since each term of RHS in equation (3.12) consists of $\ln {p_t(C^t_{j,j})\over p(y)}$s 
multiplied by $p_{t+1}(C^{t+1}_{i,i}|C^t_{j,j})$s. 
In this case, $p_t(C^t_{j,j})$ should converges to ${1 \over |K|}$ for uniform convergence like 
The definition of $K$ is provided with the proof of Corollary 3.1 in Appendix D.4
\begin{corollary}
\label{cor:restricted_uniform_convergence}
If $p_t(C^t_{j,j})$ converges to ${1 \over |K|}$ for UP-Limit and $\ln {p_t(C^t_{j,j}) \over p(y)}$  
is $\alpha\cdot\ln{{1 \over |J|} +\epsilon \over {1 \over |J|}-\epsilon}$-Lipschitz continuous 
($\forall \alpha \in [0,{{1 \over |J|} -\epsilon \over {1 \over |J|} +\epsilon})$ 
and $\forall \epsilon \in [0,\infty)$), then $\ln {p_t(C^t_{j,j}) \over p(y)}$ uniformly converges.
\end{corollary}
For the Corollary 3.2, $p_{t+1}(C^{t+1}_{i,i}|C^t_{j,j})=0$ implies that  $C^{t+1}_{i,i}$and $C^t_{j,j}$ 
are mutually exclusive (or disjoint) to each other.
Corollary 3.2 also assures that the mutual exclusion of $C^t_{i,i}$ and $C^{t}_{j,j}$.
The sole stationary assumption in previous sub-section cannot reflect on this property for uniform convergence of update.
Therefore, we introduce Threshold-Bounded Stationary Assumption which incorporates this property 
into the update rule.
This assumption means that dynamics are stationary until threshold $T$ 
but some $p_{t+1}(C^{t+1}_{i,i}|C^t_{j,j})$s reduce to zero once the threshold is crossed..

To implement this assumption, URECA estimates clusters mutually exclusive with the ground truth cluster 
and reduces the corresponding elements of dynamics to zero.
To estimate the disjoint clusters, 
we decide the sign of total divergence for clusters of all codes in terms of ground truth query.
Divergence is defined as the difference between intake and leak of elements for infinitesimal space. 
From this perspective, negative divergence means the leak of evidences to other clusters, 
which makes the cluster completely disjoint to other clusters as update progresses based on the current dynamics.
We provide the details about divergence and the analysis in Appendix E.2. 

In origin, total divergence of specific code $k$ in terms of query $i$ should consider 
about unit divergences for all code $j$s in equation (72) in Appendix E.2.
However, URECA narrows down the range of $j$ to ground truth code $i$ for computational efficiency.
In other words, URECA estimates the original total divergence as unit divergence of cluster $C^t_{i,k}$ with 
ground truth cluster $C^t_{i,i}$.
This unit divergence estimates the clusters disjoint to other clusters if unit divergence of $(q_i,d_k)$ is negative. 
This estimation results in the split of clusters into survived group (non-negative/possibly overlapped clusters) 
and dropped group (negative/disjoint clusters).
With the estimated groups, URECA clears out the elements of the dynamics that belong to the dropped group.
This process makes up of union find structure as binary tree, 
since URECA recursively iterates this process for each level which consists of survived node and dropped node 
which are disjoint to each other. 

URECA continues the recursion until transport is not possible anymore.
In terms of implementation, the transport becomes impossible when URECA cannot split the clusters into survived group 
and dropped group.
URECA cannot split the clusters when there is no negative divergence.
We naively assume that the no negative divergences come from the convergence of the joint probability distribution $p(C^t_{j,j})$ 
to uniform distribution as ${1 \over |K|}$, 
which is one of two conditions for uniform convergence of evidence transportation in the Corollary 3.2. 

However, $|K| > 1$ means that neural network does not know the difference between these clusters in $|K|$, 
since all $\ln {p_t(C^t_{j,j}) \over p(y)}$s are equal to each other for all $j$s.
This is the reason why we exclude the clusters for the calculation of loss ($L$ is the set of codes corresponding to dropped clusters).
\begin{align}
\label{URECA_loss}
\begin{split}
L_U= -{1 \over |B|}\sum_{b \in B}\ln{ exp(c_b \cdot c_b)\over |K| + \sum_{l \in L}exp(c_b\cdot c_l)}
\end{split}
\end{align}
While $L_U$ is derived from InfoNCE, it differs from the original InfoNCE in that it highlights the relationships between disentangled representations. 
It achieves this through the leverage of the estimated connections between codes, 
which align with the relationships between clusters estimated from URECA.
This $L_U$ is an auxiliary loss for $L_I$, the original InfoNCE loss between query and code,
which allows model to reflect on the structure between disentangled representations in addition to relationships between samples.
\begin{equation}
\label{combined_loss}
\begin{split}
L_{C}=L_{I} + L_{U} 
\end{split}
\end{equation}
