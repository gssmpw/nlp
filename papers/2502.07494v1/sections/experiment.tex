\section{Experiments}
In this section, we focus on the results and analyses of our experiments
for few shot adaptation to shifts in code search. 
To make the best use of the available space, we have included the details 
of the experimental setupâ€”such as the datasets, baselines, and metrics in Appendix F.1$\sim$F.4. 
Furthermore, Appendix F also provides a detailed explanation of the reasons 
behind the occurrence of specific types of shifts (e.g., query shift, code shift, and task shift) 
in each experimental setting.

\begin{table*}[h]
\def\arraystretch{1.0}
\setlength\tabcolsep{8pt} % default value: 6pt
\begin{tabular}{@{}lllcccccc@{}}

\toprule
Model                            & \multicolumn{1}{l}{Method}              & \multicolumn{1}{c}{40}           
& \multicolumn{1}{c}{80}         & \multicolumn{1}{c}{120}               & \multicolumn{1}{c}{160}     
& \multicolumn{1}{c}{200}        \\ \midrule

\multirow{2}{*}{CodeT5p-220M}      
& InfoNCE                   & \multicolumn{1}{c}{8.7}             & \multicolumn{1}{c}{13.9}          
                            & \multicolumn{1}{c}{18.8}             & \multicolumn{1}{c}{24.6}          
                            & \multicolumn{1}{c}{24.4}                  
                            \\ \cmidrule(l){2-7} 
& URECA                     & \multicolumn{1}{c}{11.2(+2.6)}          & \multicolumn{1}{c}{16.4(+2.5)}          
                            & \multicolumn{1}{c}{21(+2.2)}          & \multicolumn{1}{c}{26.9(+2.3)}          
                            & \multicolumn{1}{c}{25.6(+1.2)}                   
                            \\ \midrule

\multirow{2}{*}{UniXCoder} 
& InfoNCE                   & \multicolumn{1}{c}{46}             & \multicolumn{1}{c}{48.1}          
                            & \multicolumn{1}{c}{51.8}             & \multicolumn{1}{c}{50.2}          
                            & \multicolumn{1}{c}{50.6}                   
                            \\ \cmidrule(l){2-7} 
& URECA                     & \multicolumn{1}{c}{48.5(+2.4)}          & \multicolumn{1}{c}{51.2(+3.1)}          
                            & \multicolumn{1}{c}{45.1(+2.3)}          & \multicolumn{1}{c}{54.4(+4.2)}          
                            & \multicolumn{1}{c}{54.8(+4.3)}                  
                            \\ \midrule

\multirow{2}{*}{CoCoSoDA} 
& InfoNCE                   & \multicolumn{1}{c}{63.2}             & \multicolumn{1}{c}{63.6}          
                            & \multicolumn{1}{c}{64}             & \multicolumn{1}{c}{63.7}          
                            & \multicolumn{1}{c}{64.7}                    
                            \\ \cmidrule(l){2-7} 
& URECA                     & \multicolumn{1}{c}{64.9(+1.7)}          & \multicolumn{1}{c}{65.8(+2.2)}          
                            & \multicolumn{1}{c}{66.6(+2.6)}          & \multicolumn{1}{c}{67.2(+3.5)}          
                            & \multicolumn{1}{c}{67.7(+3)}                   
                            \\ \bottomrule 
\end{tabular}
\caption{Results of Python across different number of few shot examples (MRR).}
\label{CSN_Python}
\end{table*}

\begin{table*}[h]
\def\arraystretch{1.0}
\setlength\tabcolsep{8pt} % default value: 6pt
\begin{tabular}{@{}lllcccccc@{}}

\toprule
Model                            & \multicolumn{1}{l}{Method}              & \multicolumn{1}{c}{40}           
& \multicolumn{1}{c}{80}         & \multicolumn{1}{c}{120}               & \multicolumn{1}{c}{160}     
& \multicolumn{1}{c}{200}        \\ \midrule

\multirow{2}{*}{CodeT5p-220M}      
& InfoNCE                   & \multicolumn{1}{c}{19.7}          & \multicolumn{1}{c}{27}          
                            & \multicolumn{1}{c}{29}          & \multicolumn{1}{c}{33.9}          
                            & \multicolumn{1}{c}{37.7}                  
                            \\ \cmidrule(l){2-7} 
& URECA                     & \multicolumn{1}{c}{23.4(+3.7)}          & \multicolumn{1}{c}{28.2(+1.2)}          
                            & \multicolumn{1}{c}{32.1(+3.1)}          & \multicolumn{1}{c}{36.6(+2.7)}          
                            & \multicolumn{1}{c}{40(+2.3)}                   
                            \\ \midrule

\multirow{2}{*}{UniXCoder} 
& InfoNCE                   & \multicolumn{1}{c}{46.8}          & \multicolumn{1}{c}{45}          
                            & \multicolumn{1}{c}{48}          & \multicolumn{1}{c}{48.5}          
                            & \multicolumn{1}{c}{48.8}                   
                            \\ \cmidrule(l){2-7} 
& URECA                     & \multicolumn{1}{c}{48.2(+1.4)}          & \multicolumn{1}{c}{48.6(+3.6)}          
                            & \multicolumn{1}{c}{48.7(+0.7)}          & \multicolumn{1}{c}{50.3(+1.8)}          
                            & \multicolumn{1}{c}{50.1(+1.3)}                  
                            \\ \midrule

\multirow{2}{*}{CoCoSoDA} 
& InfoNCE                   & \multicolumn{1}{c}{48}          & \multicolumn{1}{c}{49.9}          
                            & \multicolumn{1}{c}{51.6}          & \multicolumn{1}{c}{54.1}          
                            & \multicolumn{1}{c}{54.4}                   
                            \\ \cmidrule(l){2-7} 
& URECA                     & \multicolumn{1}{c}{52(+4)}          & \multicolumn{1}{c}{54.6(+4.7)}          
                            & \multicolumn{1}{c}{54.6(+3)}          & \multicolumn{1}{c}{56.3(+2.2)}          
                            & \multicolumn{1}{c}{55.6(+1.2)}                                  
                            \\ \bottomrule 
\end{tabular}
\caption{Results of CoSQA across different number of few shot examples (MRR).}
\label{CoSQA}
\end{table*}

\subsection{Adaptation to Shifts}
Table 1, 2 and Appendix F.5's Tables show that URECA generally exhibits performance gains over InfoNCE.
This demonstrates the necessity of reflection on disentangled representations to address shifted initialization cascade
across diverse types of shifts.
Experiment section reports results which is average of three different seeds for CSN-Python and CoSQA, 
while results for the remaining programming languages in CSN-Ruby, CSN-Javascript, CSN-Java, CSN-Go and CSN-PHP 
are provided in Appendix F.5.

\textbf{Task Shift: }
UniXCoder has learned patterns of relevance between queries and code through cross-modal generation for generation tasks. 
However, it needs to appropriately adjust these patterns for the code search task, 
leading to the presence of task shift for UniXCoder (detailed information can be found 
in Appendix F.1 to F.4).
In this context, URECA consistently demonstrates performance gains over InfoNCE, 
which means the necessity of leverage for the relationships between disentangled representations.

\textbf{Code Shift: }
Since CodeT5p-220M is a specific version of CodeT5+ 
which has not been pretrained on CSN, both task shift and code shift exist for CSN 
(Details can be found in Appendix F.1 and CSN in Appendix F.2).
Since the current setting is the most harsh, it generally exhibits the lowest performance compared to other settings.
Even under these circumstances, URECA demonstrates consistent performance gains over InfoNCE 
across programming languages and different number of few-shot examples.
These results imply the necessity for leverage of the relationships between disentangled representations 


\textbf{Query Shift: }
In the case of CoCoSoDA, it has already been trained on the search task using CodeSearchNet data, 
and the codes in CoSQA also comes from CodeSearchNet. 
Therefore, CoCoSoDA is only related to query shifts for CoSQA.
In constrast to CSN, CoCoSoDA has not been trained on CoSQA (detailed information can be found in Appendix F.2).
Therefore, the performance of CoCoSoDA on CoSQA is meaningful itself unlike CSN in few shot learning  
, and URECA achieves state-of-the-art on CoSQA in the few-shot adaptation setting.
These results are especially important considering that query shifts are the most common type of shifts in real-world scenarios.

Since code of CoSQA is based on CSN-Python code, comparison of the performance improvements in CSN-Python with those in CoSQA allows 
clear understanding for the impact of query shifts. 
In fact, CSN-Python's MRR of URECA for UniXCoder increase by 3.1$\%$ with the increase of few-shot examples,
while  CoSQA's MRR of URECA for UniXCoder increases by only 1.8$\%$ on average. 
This means accurate estimation of dynamics leverages performance of URECA 
since UniXCoder's dynamics of CSN-Python is more accurate than CoSQA due to query shift.

\subsection{Shifted Initialization and Dynamics Estimation}
\begin{table}[h]
\centering
\def\arraystretch{0.8}
\setlength\tabcolsep{6pt} % default value: 6pt
\begin{tabular}{@{}lcc@{}}

\toprule
\multicolumn{1}{l}{Method}                      
& \multicolumn{1}{c}{80}          & \multicolumn{1}{c}{120}                \\ \midrule

     
CSN-Go                      & \multicolumn{1}{c}{43.1 / 46.9 (+3.8)}          
                            & \multicolumn{1}{c}{\textbf{42 / 59.1 (+17.1)}}                          
                            \\ \midrule 
CSN-PHP                     & \multicolumn{1}{c}{\textbf{11.4 / 18 (+6.6)}}          
                            & \multicolumn{1}{c}{15.8 / 17.5 (+1.7)}                             
                            \\ \bottomrule 
\end{tabular}
\caption{Shifted Initialization Cascade of CodeT5+ for CSN-Go/PHP (InfoNCE/URECA(DIFF))}
\label{Shifted Initialization CasCade Performance}
\end{table}
\begin{figure}[h]
\centering
\subfigure[CSN-Go~(Few Shot 120)]{
\includegraphics[width=.45\columnwidth]{sections/figures/experiment/go_120_codet5p.png}
\label{fig:nrGroup}
}
\subfigure[CSN-PHP~(Few Shot 80)]{
\includegraphics[width=.45\columnwidth]{sections/figures/experiment/php_80_codet5p.png}
\label{fig:overallResult}
}
\centering
\caption{
Shifted initialization cascade for CSN-Go and CSN-PHP
}
\end{figure}
\textbf{Shifted Initialization: } 
Figure 3 demonstrates that minimum entropy problem leads to bad solutions due to shifted initialization as mentioned in Section 2.
Figure 3 represents the results of training CodeT5p-220M for 100 epochs with 120 few-shot examples 
of CSN-Go and 80 few-shot examples of CSN-PHP.
For CSN-Go, InfoNCE shows extremely low performance for first 10 epochs (MRR: 0.1$\%$)
while URECA shows superior performance (MRR: 48.1$\%$). 
Although the performance gaps decrease in the process of training,
it is still significant even after 100 epochs for 59.1 $\%$ (InfoNCE) and 70.3$\%$ (URECA). 
For CSN-PHP in Figure 3, InfoNCE still shows little performance improvements again even after 100 epochs of training. 
Although URECA also exhibits little improvements during the first 10 epochs, the performance of URECA spikes after then.
These results demonstrate that 
URECA mitigates the negative effects of shifted initialization with reflection on the relationships between disentangled representations. 
Average performance (11.4 $\%$) in CSN-PHP on Table 3 is higher than performance (4.7 $\%$) for specific seed on Figure 3. 
This shows that the random sampling of few shot examples may release the effect of shifted initialization.
However, since we generally cannot control the suite of few shot examples due to the scarcity of data in real scenario, 
generalization should work robust against the random sampling.
URECA enables this robust generalization in spite of insufficient supervised signals 
based on the reflection of relationships between disentangled representations. 

\textbf{Dynamics Estimation: }
In the case of CoCoSoDA, it has already been trained with supervised signals of relevance based on data augmentation for CSN.
Therefore, CoCoSoDA more accurately estimates the dynamics between disentangled representations for CSN dataset,
which allows us to observe how the precision of estimation for dynamics influences on adaptation to shifts with URECA.
Overall, the performance improvements of URECA in CoCoSoDA are greater than that in UniXCoder, 
which implies that accurate estimation of dynamics is crucial for URECA's performance enhancement.
We also show that Thresholdly Updatable Stationary Assumption outperforms the performance of naive Stationary Assumption 
in Table 11 of Appendix F.6.
These result supports that estimated logits become unbiased estimator for the accurate logits, 
and our estimation based on unit divergence properly works to meets the conditions of Corollary 3.2 for uniform convergence.  

\begin{figure}[t]
\centering
\includegraphics[width=0.98\columnwidth]{sections/figures/experiment/figure3.png}
\caption{Efficiency of Simulation Trick}
\label{efficiency}
\end{figure}

\textbf{Simulation Trick: }We provide the comparison between simulation trick and the approaches of GraphCodeBERT 
(Appendix F.3) in terms of efficiency.
This is especially important since URECA and GraphCodeBERT should be done at every iteration. 
% as the relationship of disentangled representations depends on the construction of mini-batch.
% Likewise, the methodologies of GraphCodeBERT (especially for graph-guided attention) should be done for every iteration to guide 
% the relationships between data flow nodes and code tokens.
GraphCodeBERT's approaches need heavy computations to guide the relationships between code token 
and node of data flow.
In contrast, simulation trick just needs little computations, 
which leads to large gaps of second per iteration between URECA and GraphCodeBERT in Figure 4.
We also compare the effectiveness and scalability of URECA and GraphCodeBERT as methodologies to leverage semantic structure in Appendix F.7. 