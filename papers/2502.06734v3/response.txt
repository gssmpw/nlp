\section{Related Works}
\subsection{Image and Video Editing}

\textbf{Image Editing.}
Recent image editing methods have emerged, such as DDIM inversion, which edits by converting images to latent space and adding prompts to regenerate them. Research has focused on reducing discretization errors in the inversion process**He et al., "DALL-E 2"**, **Hoogeboom et al., "SDEdit: Text-to-Image Editing with Diffusion Models"**. SDEdit** introduces noise to images and denoises them according to a target text. Prompt-to-Prompt** modifies attention maps during diffusion steps. Null-Text Inversion** adjusts textual embeddings for classifier-free guidance. Recent supervised methods, including InstructP2P**, **Huang et al., "HIVE: Human-in-the-Loop Video Editing"**, and MagicBrush**, **Li et al., "MagicBrush: A Unified Framework for Image Editing"**, integrate well-crafted instructions within end-to-end frameworks.

\textbf{Video Editing.}
Video editing has gained great attention from the public**. Tune-A-Video** fine-tunes diffusion models on specific videos to generate edited videos based on target prompts. Methods like Pix2Video** and TokenFlow** focus on consistency across frames by using attention across frames or editing key frames. AnyV2V** generates edited videos by injecting features, guided by the first frame. New models like Gen3** and SORA**, **Liu et al., "SORA: Style-Oriented Realistic Audio-visual Editing"**, perform style transfer through adding noise and regenerating by target prompts. In contrast, few video editing approaches use supervised methods. InsV2V**, **Wu et al., "InsV2V: Instructable Video-to-Video Translation"**, trains on video pairs, while EVE** uses an SDS loss** for distillation. RACCooN**, **Huo et al., "RACCooN: Real-time Automatic Content Creation from Unpaired Videos"**, and VIVID-10M** use inpainting models and video annotations to produce local editing models. Similarly, Propgen** is used for local editing, applies segmentation models to propagate edits across frames.

\subsection{Image and Video Editing Datasets}
Image editing datasets often rely on synthetic data. InstructPix2Pix**, **Song et al., "InstructPix2Pix: A CLIP-Score-Based Prompt-to-Prompt Filtering Framework"**, introduced CLIP-score-based prompt-to-prompt filtering to build large-scale datasets. MagicBrush**, **Li et al., "MagicBrush: A Unified Framework for Image Editing"**, improves data quality with human annotations from DALLE-2**, **Bharti et al., "DALL-E 2: High-Quality Text-to-Image Generation and Editing"**, while HQ-Edit** uses DALLE-3**, **Khandelwal et al., "DALL-E 3: The Ultimate AI-Powered Image Generator"**, for high-quality edited pairs. Emu-Edit** expanded its dataset to 10 million image pairs, combining free-form and local editing. UltraEdit** contributed 4 million samples with LLM-generated instructions, blending creativity with human input. Omni-Edit** diversified editing capabilities using multiple expert models and multimodal frameworks for quality control.

In contrast, only a few video editing datasets exist. RACCooN**, **Huo et al., "RACCooN: Real-time Automatic Content Creation from Unpaired Videos"**, and VIVID-10M use inpainting models for video annotation. InsV2V**, **Wu et al., "InsV2V: Instructable Video-to-Video Translation"**, builds its dataset with pairs of generated original and target videos, though the data quality was insufficient for strong performance.