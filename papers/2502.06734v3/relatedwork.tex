\section{Related Works}
\subsection{Image and Video Editing}

\textbf{Image Editing.}
Recent image editing methods have emerged, such as DDIM inversion, which edits by converting images to latent space and adding prompts to regenerate them. Research has focused on reducing discretization errors in the inversion process~\cite{other_ddim_huberman2024edit, other_ddim_lu2022dpm, other_ddim_wallace2023edict}. SDEdit~\cite{sdedit_meng2021sdedit} introduces noise to images and denoises them according to a target text. Prompt-to-Prompt~\cite{prompt_to_prompt_hertz2022prompt} modifies attention maps during diffusion steps. Null-Text Inversion~\cite{null_text_inversion_mokady2023null} adjusts textual embeddings for classifier-free guidance. Recent supervised methods, including InstructP2P~\cite{instructpix2pix_brooks2023instructpix2pix}, HIVE~\cite{hive_zhang2024hive}, and MagicBrush~\cite{magic_brush_zhang2024magicbrush}, integrate well-crafted instructions within end-to-end frameworks.

\textbf{Video Editing.}
Video editing has gained great attention from the public~\cite{fatezero_qi2023fatezero, stablev2v_liu2024stablev2v}. Tune-A-Video~\cite{tuneavideo_wu2023tune} fine-tunes diffusion models on specific videos to generate edited videos based on target prompts. Methods like Pix2Video~\cite{pix2video_ceylan2023pix2video} and TokenFlow~\cite{tokenflow_geyer2023tokenflow} focus on consistency across frames by using attention across frames or editing key frames. AnyV2V~\cite{ku2024anyv2v} generates edited videos by injecting features, guided by the first frame. New models like Gen3~\cite{gen3} and SORA~\cite{sora} perform style transfer through adding noise and regenerating by target prompts. In contrast, few video editing approaches use supervised methods. InsV2V~\cite{insv2v_cheng2024consistent} trains on video pairs, while EVE~\cite{eve_singer2025video} uses an SDS loss~\cite{dreamfusion_poole2022dreamfusion} for distillation. RACCooN~\cite{raccon_yoon2024raccoonremoveaddchange} and VIVID-10M~\cite{vivid_10m_hu2024vivid} use inpainting models and video annotations to produce local editing models. Similarly, Propgen~\cite{propgen_liu2024generative} is used for local editing, applies segmentation models to propagate edits across frames.

\subsection{Image and Video Editing Datasets}
Image editing datasets often rely on synthetic data. InstructPix2Pix~\cite{instructpix2pix_brooks2023instructpix2pix} introduced CLIP-score-based prompt-to-prompt filtering to build large-scale datasets. MagicBrush~\cite{magic_brush_zhang2024magicbrush} improves data quality with human annotations from DALLE-2~\cite{dalle2_ramesh2022hierarchical}, while HQ-Edit~\cite{hq_edit_hui2024hq} uses DALLE-3~\cite{dalle3_betker2023improving} for high-quality edited pairs. Emu-Edit~\cite{emu_edit_sheynin2024emu} expanded its dataset to 10 million image pairs, combining free-form and local editing. UltraEdit~\cite{ultraedit_zhao2024ultraedit} contributed 4 million samples with LLM-generated instructions, blending creativity with human input. Omni-Edit~\cite{omniedit_wei2024omniedit} diversified editing capabilities using multiple expert models and multimodal frameworks for quality control.

In contrast, only a few video editing datasets exist. RACCooN~\cite{raccon_yoon2024raccoonremoveaddchange} and VIVID-10M use inpainting models for video annotation. InsV2V~\cite{insv2v_cheng2024consistent} builds its dataset with pairs of generated original and target videos, though the data quality was insufficient for strong performance.