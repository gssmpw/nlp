\section{Related Works}
\subsection{Image and Video Editing}

\textbf{Image Editing.}
Recent image editing methods have emerged, such as DDIM inversion, which edits by converting images to latent space and adding prompts to regenerate them. Research has focused on reducing discretization errors in the inversion process____. SDEdit____ introduces noise to images and denoises them according to a target text. Prompt-to-Prompt____ modifies attention maps during diffusion steps. Null-Text Inversion____ adjusts textual embeddings for classifier-free guidance. Recent supervised methods, including InstructP2P____, HIVE____, and MagicBrush____, integrate well-crafted instructions within end-to-end frameworks.

\textbf{Video Editing.}
Video editing has gained great attention from the public____. Tune-A-Video____ fine-tunes diffusion models on specific videos to generate edited videos based on target prompts. Methods like Pix2Video____ and TokenFlow____ focus on consistency across frames by using attention across frames or editing key frames. AnyV2V____ generates edited videos by injecting features, guided by the first frame. New models like Gen3____ and SORA____ perform style transfer through adding noise and regenerating by target prompts. In contrast, few video editing approaches use supervised methods. InsV2V____ trains on video pairs, while EVE____ uses an SDS loss____ for distillation. RACCooN____ and VIVID-10M____ use inpainting models and video annotations to produce local editing models. Similarly, Propgen____ is used for local editing, applies segmentation models to propagate edits across frames.

\subsection{Image and Video Editing Datasets}
Image editing datasets often rely on synthetic data. InstructPix2Pix____ introduced CLIP-score-based prompt-to-prompt filtering to build large-scale datasets. MagicBrush____ improves data quality with human annotations from DALLE-2____, while HQ-Edit____ uses DALLE-3____ for high-quality edited pairs. Emu-Edit____ expanded its dataset to 10 million image pairs, combining free-form and local editing. UltraEdit____ contributed 4 million samples with LLM-generated instructions, blending creativity with human input. Omni-Edit____ diversified editing capabilities using multiple expert models and multimodal frameworks for quality control.

In contrast, only a few video editing datasets exist. RACCooN____ and VIVID-10M use inpainting models for video annotation. InsV2V____ builds its dataset with pairs of generated original and target videos, though the data quality was insufficient for strong performance.