\section{Method}
\subsection{Preliminary of Font Generative Model}
\label{sec:fontgenerativemodel}
We use \textit{DG-Font}~\cite{XieDGFont2021} as our font generative model. 
This model takes a style image $I_{S}$ representing the desired font style, and a content image $I_{C}$ representing the desired character, as input.
It then generates the character image that represents the desired character in the desired font style as the output. 
As illustrated in \autoref{fig:dg-font}, the architecture of \textit{DG-Font} is an encoder-decoder model with two encoders: a style encoder $E_{S}$ and a content encoder $E_{C}$, along with a content decoder $G_{C}$.
The generation process starts by extracting the style latent vector from the style image using the style encoder and the content latent vector from the content image.
Then, the content decoder takes both the style and content latent vectors as input to generate the desired font image $\hat{I}$, which maintains a similar style to the style image while preserving the structure of the content image.
Overall, the generation process during the training process can be formulated as:
\begin{align}
\bm{z}_S = E_{S}(I_S),\quad \bm{z}_C = E_{C}(I_C), \quad 
\hat{I} = G_C(\bm{z}_S, \bm{z}_C).
\end{align}
In our work, we use an enhanced version of \textit{DG-Font}, which includes an additional content discriminator.
Notably, our system (\systemName) only needs the pretrained model to generate new fonts instead of training a model from the beginning.
We will provide the details of this additional model architecture and training process in the supplemental material.

In the rest of the paper, if we need to specify the content image $I_C$ or its latent vector $\bm{z}_C$ for a specific character such as ``A'', we will denote it as  $I_C[\text{``A''}]$ $\bm{z}_C[\text{``A''}]$.
Otherwise, we will use $I_C$ or $\bm{z}_C$ for abbreviation. 
This also applied to the generated image $\hat{I}$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures_pdf/dgfont_v8.pdf}
    \caption{   
    \textbf{Overview of \textit{DG-Font}.} \textit{DG-Font} is an encoder-decoder model that takes a character image representing style and a character image representing content as input, and outputs a character image that combines the content with the specified style.
    During font designing in our system, users use our human-in-the-loop optimization to explore the style latent space of the style encoder.
    Please find the detailed architecture of the encoder and decoder in the supplemental material.
    }
    \label{fig:dg-font}
\end{figure}


\subsection{Preliminary of FontCLIP}
To incorporate multimodal input when designing fonts, we use FontCLIP~\cite{tatsukawa2024fontclip} to extract typographical features from both text and image input.
FontCLIP is a visual-language model that
bridges the semantic understanding of a large vision-language model with typographical knowledge.
It consists of a text encoder and a visual encoder and builds a joint latent space that encodes typographical knowledge.
In this joint latent space, similar font images and text prompts will have similar latent vectors.
For example, a bold font will have a similar latent vector to the text prompt ``This is a \textit{strong} and \textit{thick} font'' compared to the text prompt ``This is a \textit{thin} font''.
Therefore, the FontCLIP latent vector can be used to retrieve similar fonts using text or image input.
In our system, we utilize both the FontCLIP text encoder and visual encoder to extract a latent vector from the multimodal input to customize the linear subspace.

\subsection{Preliminary of Human-in-the-Loop Bayesian Optimization}
\label{method:prelim_BO}
\subsubsection{Problem Formulation}
Human-in-the-loop optimization is a computational approach used to solve parameter optimization problems involving human evaluators in its iterative algorithm.
It is commonly used to support design tasks that involve generating visual content defined by a set of parameters $\bm{x}$, with the aim of achieving certain subjective design goals.
Specifically, we can formulate such an optimization problem as:
\begin{align}
\bm{x}^* = \argmax_{\bm{x}\in{\mathcal{X}}}f(\bm{x}),
\label{eq:opt_goal}
\end{align}
where $\mathcal{X}$ is the search space, and $f: \mathcal{X} \rightarrow \mathbb{R}$ is the goodness function to evaluate a subjective design goal (\eg~the aesthetics of the current design).
We aim to find the optimal value $\bm{x}^*$ with the fewest trials because evaluating $f(\cdot)$ is costly.
However, solving \autoref{eq:opt_goal} using traditional Bayesian optimization (BO) might not be suitable for many design tasks.
This is because it is often difficult to assign an exact value to a sample, whereas comparing a couple of samples and choosing the preferred one is more intuitive.
For example, it is hard for users to give a score to a font individually, but easier for them to choose the preferred font from a set of candidates.
Therefore, in this work, we choose to use preferential Bayesian optimization (PBO) \cite{Koyama2022}, which is a variant of Bayesian optimization (BO) that runs with relative preferential data.
In particular, we build our method upon Sequential Line Search (SLS) \cite{KoyamaSequential2017}, a PBO method that constructs a sequence of linear subspaces that leads to the optimal parameters that match the user's need.


\subsubsection{Sequential Line Search (SLS)}
\label{sec:sls}
With SLS, the user can search for his/her preference by adjusting a slider.
At each iteration of the optimization, SLS constructs a linear subspace using the current-best position $\bm{x}^{+}$ and the best-expected-improving position $\bm{x}^{\text{EI}}$.
Suppose we already have $t$ observed response so far, then the next linear subspace $\mathcal{S}_{t+1}$ is constructing by connecting:
\begin{align}
    \bm{x}^\text{EI}_t &= \argmax_{\bm{x}\in{\mathcal{X}}}a^\text{EI}(\bm{x};\mathcal{D}_t) \label{eq:build_subspace} \\ \nonumber
    \bm{x}^+_t &= \argmax_{\bm{x}\in{\left\{\bm{x}_i\right\}_{i=1}^{N_t}}}\mu_t(\bm{x})
\end{align}
where $\left\{\bm{x}_i\right\}_{i=1}^{N_t}$ denotes the set of points observed so far, $\mu_t$ and $a^\text{EI}$ are the predicted mean function and the acquisition function calculated from the current data.
We use the expected improvement (EI) criterion as the acquisition function to choose the next sampling point that is likely to optimize the function $f$ and at the same time its evaluation is more informative:
\begin{align} \label{equation:acquisition}
    a^{\text{EI}}(\bm{x} ; \mathcal{D})=\mathbb{E}\left[\max \left\{f(\bm{x})-f^{+}, 0\right\}\right].
\end{align}

After the $t$-th iteration, we suppose that we obtained a set of $t$ single slider responses, which is represented as
\begin{align}
    \mathcal{D}_t=\left\{\bm{x}_i^\text{chosen}>\left\{\bm{x}_{i-1}^{+}, \bm{x}_{i-1}^{\text{EI}}\right\}\right\}_{i=1}^t,
\label{eq:response_data}
\end{align}
where $\bm{x}_i^{\text{chosen}}$ represent the position chosen by the user at the $t$-th iteration.

Let $f_i$ be the goodness function value at a data point $\bm{x}_i$, i.e., $f_i = f(\bm{x}_i)$, and $\bm{f}$ be the concatenation of the goodness values of all data points:
\begin{align}
\bm{f} = [f_1, f_2, \ldots, f_N].
\end{align}
Under the assumption of Gaussian process (GP) prior on $f$, we use $\bm{\theta}$ to represent the hyperparameters of the multivariate Gaussian distribution.
Since the goodness values $\bm{f}$ and the hyperparameters $\bm{\theta}$ are correlated, we infer $\bm{f}$ and $\bm{\theta}$ jointly by using MAP estimation:
\begin{align}
(\bm{f}^{\text{MAP}}, \bm{\theta}^{\text{MAP}})&=\argmax_{(\bm{f},\bm{\theta})}p(\bm{f}, \bm{\theta} \mid \mathcal{D}) \nonumber \\
&= \argmax_{(\bm{f},\bm{\theta})}p(\mathcal{D} \mid \bm{f}, \bm{\theta}) p(\bm{f}\mid \bm{\theta}) p(\bm{\theta}).
\label{eq:map}
\end{align}

Once $\bm{f}^{\text{MAP}}$ and $\bm{\theta}^{\text{MAP}}$ have been estimated, we can compute $\mu(\cdot)$, $\sigma(\cdot)$, and $a^{\text{EI}}(\cdot)$ in order to construct the next slider subspace $\mathcal{S}_{t+1}$.
We only describe the minimum concept of how SLS constructs the linear subspace to optimize the function $f$ for understanding how we incorporate it in the style latent space of a font generative model and multimodal input.
For more details, please refer to the supplemental material.


\subsection{Multimodal Bayesian Optimization for Font Generation}
\label{sec:multiModalBayesianOptimization}
Specifically, following \autoref{eq:opt_goal}, the objective function for designing a character can be formulated as:
\begin{align}
\bm{z}^* = \argmax_{\bm{z}\in{\mathcal{Z}}}f(G(\bm{z})), \label{eq:argmax_latent_space}
\end{align}
where $\bm{z} \in\mathcal{Z}$ is the style latent vector of the font generative model, which is the search space $\mathcal{X}$ in our problem setting.
Moreover, $G$ is the decoder of the font generative model, and $f: \mathcal{Z} \rightarrow \mathbb{R}$ is the user preference function that measures how good the currently generated character is perceived by the user.
To solve \autoref{eq:argmax_latent_space} and obtain the desired font, users can perform three different actions: \textit{explore the font style latent space}, \textit{retract previous preferences}, and \textit{propagate style to other characters} at each iteration.

\subsubsection{Action 1: Explore the Font Style Latent Space}
\label{sec:explorationWithSingleSlider}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures_pdf/exploreWithBayesianOptimization_v7.pdf}
    \caption{
    \textbf{Exploration of the font style latent space using a single slider.} 
    (a) Users explore a one-dimensional search subspace within the font style latent space using a single slider.
    At each iteration, users choose a point in the latent subspace and submit it as their current preference $\bm{z}^{\text{chosen}}_t$.
    After a couple of iterations, users gradually converge to their desired font style.
    The overall exploration process, users can explore (b) BO subspace only, (c) multimodal-guided subspace only, and (d) combination of both. 
}
    \label{fig:ExploreWithBayesianOptimization}
\end{figure}

The primary task for users is to explore the one-dimensional font-style search subspace using a slider.
By repeating the slide manipulation, users gradually converge on a point that aligns with their desired font style as illustrated in \autoref{fig:ExploreWithBayesianOptimization}.
This subspace is constructed by the system in two ways:
one solely follows the SLS method and another utilizes multimodal references.
Note that, regardless of these two different approaches to constructing the subspace, the interaction (\ie~manipulating the slider and submitting a preferred point to the system) remains consistent.

\paragraph{Constructing a SLS subspace}
Using the SLS method outlined in \autoref{sec:sls}, our system constructs a linear subspace $\mathcal{S}_{t}$ by connecting the current best point $(\bm{z}^+_t)$ and the point that maximizes the acquisition function $(\bm{z}^\text{EI}_t)$ using \autoref{eq:build_subspace} at the $t$-th iteration.
Then, users can choose a style latent vector $\bm{z}$ using the slider within $\mathcal{S}_{t}$ and view the generated character $G(\bm{z})$.
Once satisfied, users submit their preferred point on the slider $\bm{z}^{\text{chosen}}_t$ by clicking the \textsc{Update} or \textsc{Update all} button and request the system to construct a new linear subspace $\mathcal{S}_{t+1}$ for the $(t+1)$-th iteration using \autoref{eq:map}.


\paragraph{Constructing a multimodal-guided subspace}
While exploration with a single slider is useful, the linear subspace determined solely by Bayesian optimization sometimes fails to capture user preferences effectively, which leads to an increasing number of iterations and potentially causes frustration and a diminished sense of agency.
To address this issue, we allow users to intervene in the linear subspace construction by providing multimodal references at any iteration.
At $(t+1)$-th iteration, instead of exploring the linear subspace $\mathcal{S}_{t+1}=(\bm{z}^+_t, \bm{z}^\text{EI}_t)$ constructed solely by Bayesian optimization, the user explores the multimodal-guided subspace: $\mathcal{S}^{mm}_{t+1} = (\bm{z}^+_t, \bm{z}^{mm}_t)$.
Here, $\bm{z}^{mm}_t$ is the style latent vector obtained by retrieving the most similar fonts to the multimodal reference provided at $(t+1)$-th iteration from a font database containing \num{1,169} Roman fonts collected by O'Donovan~\etal~\cite{o2014exploratory}.
Specifically, we retrieve $n$ fonts and use the mean of their latent vectors as $\bm{z}^{mm}_t$ (we use $n = 5$ in our implementation).
Once the user is satisfied with the current generated character, the slider response: ($\bm{z}_{t+1}^\text{chosen},\bm{z}_{t}^{+}, \bm{z}_{t}^{mm}$) will be recorded in $\mathcal{D}_{t+1}$ (\autoref{eq:response_data}) and used for constructing the subspace in the future iteration.
This means that all multimodal references provided until iteration $t$ will affect the subspace constructed at $(t+1)$-th iteration.
In our current implementation, at each iteration, users can provide only a single multimodal reference, and we construct a new search subspace by connecting the current chosen point and the latent vector of the provided multimodal reference.
At $0$-th iteration, we construct the initial linear subspace $\mathcal{S}_{0}=(\bm{z}^{\text{init}}, \bm{z}^{mm}_{0})$, where $\bm{z}^{\text{init}}$ represents the style latent vector of a commonly used font (we use ``IPAex gothic'' font in our implementation).
Note that the multimodal references are only used to construct the linear subspace for the user to explore, not being directly used to infer the user preferences.r

To construct a multimodal subspace with the user-provided multimodal reference, our system identifies a suitable font in our font database that corresponds to the input text or image. 
For text input, the system first extracts font attributes such as ``formal,'' ``italic,'' and ``happy'' using a Large Language Model (LLM). 
The LLM selects relevant font attributes based on the given text.
We utilize $37$ types of font attributes compiled by O'Donovan~\etal~\cite{o2014exploratory} (see the supplemental material for more details).
Once the font attributes are extracted, the system obtains the feature vector of these text attributes using the text encoder of FontCLIP~\cite{tatsukawa2024fontclip} and retrieves fonts from our font database based on feature similarity. 
The retrieved fonts are used as the most suitable options, and the mean of their style latent vectors $\bm{z}^{mm}$ is obtained using the style encoder $E_{S}$.

For the text-rendered image reference, our system retrieves its most similar font in the font database using the FontCLIP feature.
Similarly, we then project the retrieved font image into the style latent space using $E_{S}$ and obtain $\bm{z}^{mm}$.
Finally, for font file input, our system directly uses the provided font as the suitable font and projects it into the latent space, similar to the process for text and image inputs.
\autoref{fig:multi_modal_subspace}(c) illustrates how to obtain the style latent vector of the multimodal references.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figures_pdf/multi_modal_subspace_v6.pdf}
    \caption{
    \textbf{Constructing linear subspaces using multimodal references.} 
    (a) At the start of the font design process using our proposed method, the user inputs text, an image, or a font file. 
    The system encodes this input into a font style latent vector and initializes the line search space by connecting the latent vector and a fixed point predetermined by the system.
    (b) Additionally, the user can introduce multimodal inputs at any stage of the design process.
    When the user provides new input, the system generates a new line search subspace by connecting the last user preference point with the newly encoded point.
    (c) Our system encodes multimodal input into the style latent space by leveraging LLM and FontCLIP text and visual encoders.
}
    \label{fig:multi_modal_subspace}
\end{figure*}

\subsubsection{Action 2: Retract Previous Preferences}
At each iteration, if the user is not satisfied with the current design and the recommended candidates, they can choose to retract previous preferences.
Specifically, at $(t+1)$-th iteration, if the user wishes to retract the last two slider manipulations, then the last two slider responses stored in $\mathcal{D}_t$ will be discarded.
Then, if the user opts to explore a new subspace by providing a new multimodal reference, they will then explore the subspace $\mathcal{S}^{mm}_{t-1} = (\bm{z}^+_{t-2}, \bm{z}^{mm}_{t-2})$.
Otherwise, the user will explore a subspace constructed using SLS solely: $\mathcal{S}_{t-1} = (\bm{z}^+_{t-2}, \bm{z}^{\text{EI}}_{t-2})$.
By retracting previous preferences, users can update their preferences during the design process.

\subsubsection{Action 3: Propagate Style to Other Characters}
Once the user is satisfied with the design of the focused character (\eg~``A'') and obtains the style latent vector $\bm{z}_S^{*}$, our system can propagate the style to all other characters and finish designing a single character.
Specifically, to propagate the style vector to character ``B'':
\begin{align}
\hat{I}[\text{``B''}] = G_C(\bm{z}_S^{*}[\text{``A''}], \bm{z}_C[\text{``B''}]).
\end{align}
Next, users can check all characters with the propagated styles.
If they are unsatisfied with the result of another character, they can perform action 1 or action 2 to design that character.
If they are satisfied, the resulting font is exported as an outline font.
